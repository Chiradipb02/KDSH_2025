Group-wise oracle-efficient algorithms for
online multi-group learning
Samuel Deng
Department of Computer Science
Columbia University
samdeng@cs.columbia.eduDaniel Hsu
Department of Computer Science
Columbia University
djhsu@cs.columbia.edu
Jingwen Liu
Department of Computer Science
Columbia University
jingwenliu@cs.columbia.edu
Abstract
We study the problem of online multi-group learning, a learning model in which
an online learner must simultaneously achieve small prediction regret on a large
collection of (possibly overlapping) subsequences corresponding to a family of
groups . Groups are subsets of the context space, and in fairness applications, they
may correspond to subpopulations defined by expressive functions of demographic
attributes. In contrast to previous work on this learning model, we consider
scenarios in which the family of groups is too large to explicitly enumerate, and
hence we seek algorithms that only access groups via an optimization oracle. In
this paper, we design such oracle-efficient algorithms with sublinear regret under a
variety of settings, including: (i) the i.i.d. setting, (ii) the adversarial setting with
smoothed context distributions, and (iii) the adversarial transductive setting.
1 Introduction
We study the problem of online multi-group learning, originally introduced by [BL20] (adapting the
specialists/time-selection setup of [Blu97; Fre+97; BM07]). In this learning model, we consider a
collection of groups G, which are (possibly intersecting) subsets of a context space X, as well as
a hypothesis class Hof functions defined on X. Contexts x1, x2, . . . , x Tarrive one-by-one over a
sequence of Trounds, and the learner must make a prediction associated with each xt. The learner’s
goal is to perform well on every subsequence of rounds corresponding to each group g∈ G. Here,
performance is measured relative to the predictions of the best-in-hindsight hypothesis h∈ H for the
specific subsequence under consideration.
A common interpretation of multi-group learning—which is natural when considering fairness in
machine learning (ML)—identifies contexts x∈ X with individuals, each group g∈ G with a
subpopulation (perhaps defined by a combination of various demographic features such as age and
gender), and each hypothesis h∈ H with a classifier that makes predictions about individuals [RY21].
The goal of the learner, then, is to predict as well as the best subpopulation-specific hypothesis, for
all subpopulations simultaneously.
The standard benchmark in online learning is regret, which compares the performance of the learner
on all rounds to that of the single best hypothesis in hindsight. But such a benchmark is only
meaningful if there is a hypothesis that performs well in all contexts. At the other extreme, we
may hope that the learner performs as well as using the best context-specific hypothesis in every
38th Conference on Neural Information Processing Systems (NeurIPS 2024).round. However, this may be impossible if no context is ever repeated. The group-wise notion of
regret in online multi-group learning naturally interpolates between these two extremes: the former is
recovered when G={X}, and the latter when G={{x}:x∈ X} .
We are particularly interested in scenarios where Gmay be extremely large (and perhaps even infinite).
In such cases, it is too time-consuming to explicitly enumerate the groups in G, and this precludes the
use of the algorithmic solutions from prior works [BL20; Ach+23].
The use of highly expressive families of groups has been a recent focus in the ML fairness literature,
where fairness with respect to such rich families of groups is seen as compromise between coarse
notions of statistical fairness that ignore intersectionality, and individualized notions of fairness that
are typically difficult to ensure [Kea+18; Heb+18; KGZ19; GKR22; Glo+23]. For example, if groups
are defined by simple combinations of demographic attributes (e.g., linear threshold functions), then
the number of subsequences determined by these groups may grow exponentially with the number of
attributes. To deal with the intractability of explicit enumeration of groups, these prior works rely on
optimization oracles that implicitly search through the family of groups. In this work, we seek to do
the same, but for the online (as opposed to batch) problem at hand.
Another motivation for multi-group learning with rich families of groups comes from the literature
on “subgroup robustness” [Sag+20], where one is concerned with the test-time distribution shifting
from the training distribution by restricting to subsets (or “subgroups”) of the feature space. Such
scenarios have been practically motivated, for instance, in medical domains where training data sets
are constructed to include data from all patients (healthy and sick) as a matter of convenience, but the
population relevant to the application is only a subset of the sick patients for which an intervention is
potentially possible [Oak+20]. It may be difficult to anticipate which subgroup will ultimately be
relevant (or even to provide an explicit shortlist of such subgroups), but multi-group learning with a
very rich and expressive family of groups Gensures that, as long as a subgroup is well-approximated
or within the collection G, it obtains our theoretical guarantees. This motivates dealing with large or
potentially infinite Gto provide guarantees for as many subgroups as possible.
1.1 Summary of Results
We construct an end-to-end oracle-efficient online learning algorithm that is oracle-efficient in all
the problem parameters. Namely, it is oracle-efficient in both HandG. In the case of finite Hor
G, this admits an exponential computational speedup over the previous algorithms; in the case of
infinite G, this is the first algorithm that achieves multi-group online learning. With this in mind, we
can even think of Gas representing a binary-valued function class that takes in contexts and selects
subsequences S⊆[T]based on those contexts in possibly complex ways.
Previous work has shown that the basic goal of designing a computationally efficient algorithm (in all
problem parameters) to achieve o(T)regret for fully worst-case adversaries is impossible [HK16;
Blo+22]. Research has therefore focused on natural structural assumptions in which computational
efficiency and sublinear regret is possible, albeit in the traditional setting without groups [SKS16;
Hag+22; HRS22; Blo+22]. Because the multi-group online adversarial setting is strictly harder than
the standard setting in which this lower bound applies (simply consider multi-group learning with one
group: the entire sequence), we must also take similar assumptions to circumvent the computational
hardness result. In this work, we consider the same structural assumptions in the multi-group scenario.
Specifically, we make the following contributions (with ˜O(·)suppressing logfactors):
1.Group-wise oracle efficiency for the smoothed setting. We present an oracle-efficient algorithm
that achieves ˜O(p
dT/σ )regret on every group g∈ Gfor a binary-valued action space for the
smoothed online learning setting of [HRS22; Blo+22], where dis a bound on the VC dimension
ofHandG, and σis a parameter interpolating between the scenarios in which contexts are
generated by a worst-case adversary and a benign, fully i.i.d. adversary.
2.Group-wise oracle-efficiency through generalized follow-the-perturbed leader. If a sufficient
condition referred to as γ-approximability in previous literature ([Wan+22]) is met, a variant of our
oracle-efficient algorithm achieves, for each particular g∈ G, a regret of ˜O(p
NTglog|H||G| )
for finite HandG, where Ncorresponds roughly to a required number of perturbations, and Tg
is the number of rounds t∈[T]in which xt∈g. The dependence on Tgas opposed to Tis
preferable if some groups do not appear frequently over the Trounds. As a special case, our
2Work Setting Regret Computation Oracle-
efficient
inHOracle-
efficient
inG
[BL20] Adversarialp
Tglog|H||G| for all
g∈ GTime: O(|H||G| )
Space: O(|H||G| )No No
[Ach+23] Adversarialp
Tglog|H||G| for all
g∈ GTime: O(|G|)Space:
O(|G|)Yes No
[Blo+22] σ-Smoothq
dTlogT
σ(does not
handle multi-group set-
ting)Time: poly( T) calls
to optimization oracleYes N/A
[Hag+22] σ-Smoothq
dTlogT
σ1/2(does not
handle multi-group set-
ting)Time: poly( T) calls
to optimization oracleYes N/A
Ours (The-
orem 4.1)σ-Smoothq
dTlogT
σfor all g∈
GTime: poly( T) calls
to optimization oracleYes Yes
Ours (The-
orem 5.1)γ-approximablep
NTglog|H||G| for
allg∈ GTime: poly( T) calls
to optimization oracleYes Yes
Ours
(Corollary
5.1.1)Transductive N1/4p
Tglog|H||G|
for all g∈ GTime: poly( T) calls
to optimization oracleYes Yes
Table 1: In the table above, Tis the number of rounds of online learning, Tgis the number of
rounds for a particular group g,His the hypothesis class, Gis the collection of groups, and σis the
smoothness parameter defined in Definition 4.1. dis an upper bound on the VC dimension of H. In
ourσ-smooth result in the fifth row, dis also an upper bound on the VC dimension of a possibly
infinite collection of groups, G.In the γ-approximable setting of the sixth row, Nis the number of
perturbations.
algorithm also achieves ˜O(N1/4p
Tglog|H||G| )in the transductive setting of [BKM97; KK05],
where the adversary must reveal a set of Nfuture contexts before learning begins.
Table 1 summarizes our results in relation to existing work. Our algorithms follow a more general
algorithm design template based on the adversary moves first (AMF) framework of [Lee+22]. We ex-
tend this framework with the following technical enhancements: sparsifying the implicit distributions
over the hypothesis spaces used by follow-the-perturbed-leader (FTPL) algorithms, and simplifying
the min-max game that is solved in every iteration of AMF.
In particular, the AMF framework allows us to have low-regret with respect to all group-hypothesis
pairs; the multi-objective regret guarantees are useful for our purposes because we want to guarantee
simultaneous low regret over all g∈ G. However, naively applying AMF requires us to enumerate
these objectives, in turn enumerating G, the main issue we want to avoid. FTPL comes to the rescue
and allows us to have low-regret to all pairs implicitly without enumerating them. So AMF allows
us to compete with all g∈ G; FTPL ensures this is efficient. The algorithmic details and main
technical challenges can be found in Section 4.2. These techniques allow us to adapt this framework
to the scenario where the number of groups is too large to enumerate, and they may find use in other
multi-objective learning problems.
1.2 Related Work
[BL20] showed that it is possible to achieve sublinear multi-group regret by reducing to the specialists
framework of [Blu97; Fre+97; BM07] (a.k.a. sleeping experts ). The multi-group regret they achieve
scales logarithmically in both HandG; this recovers the minimax regret when specializing to the
standard online learning setting (with finite H) with only a single group. The paper focuses on regret
rather than computational considerations; a direct implementation of their algorithm uses time and
space linear in |H| × |G| , and there are no stated guarantees for infinite HorG.
3[Ach+23] show how to avoid enumeration of Husing an optimization oracle for H. They achieve
this by applying a meta-algorithm atop a black-box oracle-efficient online learning algorithm, but
this meta-algorithm ultimately requires explicit enumeration of G. Our work, in contrast, uses an
optimization oracle for G × H jointly and hence avoids explicit enumeration of either GorH.
Multi-group (agnostic) learning has also been studied in the batch setting [RY21; TH22; GKR22;
HJZ24]. In this setting, training data is drawn i.i.d. from a fixed distribution, and the learner’s goal
is to find a single hypothesis ˆh(possibly outside of H) that ensures small excess risk E[ℓ(ˆh(x), y)|
x∈g]−infh∈HE[ℓ(ˆh(x), y)|x∈g]for every group g∈ G simultaneously. The works of
[RY21; HJZ24] design algorithms for achieving this learning criterion under a certain “multi-PAC
compatibility” assumption on HandG. [GKR22; TH22] design multi-group learning algorithms that
remove the need for this assumption. One of the algorithms of [TH22], which enjoys near optimal
sample complexity for general but finite HandG, is based on the the online multi-group learning
approach of [BL20] combined with online-to-batch conversion.
The proof of our algorithm builds on two primary technical frameworks studied in previous literature:
theadversary moves first (AMF) framework of [Lee+22], and a line of work designing follow-
the-perturbed leader style algorithms [KV05] for adversarial online learning in the oracle-efficient
learning model [KK05; SKS16; Hag+22; Blo+22; Wan+22].
2 Preliminaries
2.1 Notation
Throughout, Xdenotes a context space , andYdenotes an action space . For example, in a typical
(online) supervised learning setup, Xis the feature space, and Yis the label space. A group gis a
subset of the context space X. We overload the notation gfor a group by using it as an indicator
function g(x) :=1{x∈g}for group membership. Let 2Xdenote all subsets of the context space X,
and let YXdenote all possible mappings from XtoY. For an integer n, denote [n] :={1,2, . . . , n }.
For simplicity of exposition, we will focus on the setting where Yis binary throughout, i.e. Y=
{−1,1}. We note that our techniques are more general, however, and may be adapted to the case of
finite, multi-class action spaces (see Appendix C for details).
2.2 Online Multi-Group Learning
We formally define the multi-group learning model as follows. Let G ⊂ 2Xbe a collection of
(possibly non-disjoint) groups, and let H ⊂ YXbe a hypothesis class of functions h:X → Y
mapping from contexts to actions. Let ℓ:Y × Y → [0,1]be a bounded loss function. In each round
t∈[T]:
1. Nature chooses (xt, yt)∈ X × Y and reveals xt.
2. The learner chooses an action ˆyt∈ Y.
3. Nature reveals yt∈ Y.
4. The learner incurs loss ℓ(ˆyt, yt)∈[0,1].
The choices of Nature and the learner may be randomized. In the standard online prediction setting,
the regret of the learner is the difference between the cumulative loss of the learner and that of the
best-in-hindsight hypothesis from H:RegT(H) :=PT
t=1ℓ(ˆyt, yt)−minh∈HPT
t=1ℓ(h(xt), yt).
The goal of the learner is to achieve sublinear (in T) expected regret.
In multi-group online learning, we consider the regret of the learner on subsequences of rounds
(t∈[T] :xt∈g)defined by the groups g∈ Gand the sequence of contexts x1, . . . , x T. Specifically,
the(multi-group) regret of the learner on group gis
RegT(H, g) :=TX
t=1g(xt)ℓ(ˆyt, yt)−min
h∈HTX
t=1g(xt)ℓ(h(xt), yt). (1)
Crucially, the best hypothesis for one group may differ from that of another group. Further, groups
may intersect, precluding the strategy of simply running a separate no-regret algorithm for each group.
The learner seeks to achieve achieve sublinear expected regret, on all groups g∈ Gsimultaneously.
42.3 Group Oracle-Efficiency
The main challenge posed in this work is to design computationally efficient algorithms that work
with both large hypothesis classes Hand large collections of groups G. The prior work of [Ach+23]
shows how to use the following optimization oracle to avoid explicitly enumerating the hypothesis
classH(but still require enumerating G).
Definition 2.1 (Optimization Oracle) .For some error parameter α≥0and function class F ∈ZX,
anα-approximate optimization oracle OPT takes a collection of pairs (x1, z1), . . . , (xm, zm)∈ X ×
Z, a sequence of weights w1, . . . , w m∈R, and a sequence of mloss functions ℓi:Z ×Z → [−1,1]
and outputs a function ˆf:= OPT( {(xi, zi, wi)}m
i=1)∈ F satisfying:
mX
i=1wiℓi(ˆf(xi), zi)≤inf
f∈FmX
i=1wiℓi(f(xi), zi) +α.
Instantiating (in Definition 2.1) Zas our action space Y, each ℓias the given loss ℓof our problem,
andFas our hypothesis class Hgives a standard empirical risk minimization (ERM) oracle over a
dataset {(xi, yi)}m
i=1. We present this more general definition to distinguish the action space Yof the
problem from the output space of the oracle (see Definition 2.2).
The optimization oracle is regarded as a natural computational primitive because, for many problems
in machine learning, various heuristic methods (e.g., stochastic gradient descent) appear to routinely
solve such problem instances despite the worst-case intractability of such problems.
The work of [Ach+23] still relies on explicit enumeration of G. We show how this can be avoided
using a joint optimization oracle for G × H , defined as follows.
Definition 2.2 ((G,H)-optimization oracle) .Fix an error parameter α≥0. For a collection
of groups G ∈ 2X, a collection of hypotheses H ⊆ YX, and a sequence of mloss functions
ℓi: ({0,1} × Y )×(Y × Y )→[−1,1], anα-approximate (G,H)-optimization oracle OPTα
(G,H)is
anα-approximation optimization oracle (Definition 2.1) that outputs a pair (˜g,˜h)∈ G×H satisfying:
mX
i=1wiℓi((˜g(xi),˜h(xi)),(yi, y′
i))≥ sup
(g∗,h∗)∈G×HmX
i=1wiℓi((g∗(xi), h∗(xi)),(yi, y′
i))−α. (2)
IfY={−1,1}(which we assume in the main paper body), this oracle outputs a group-hypothesis pair
(˜g,˜h)that maximizes the batch loss over mexamples (xi,(yi, y′
i))inX × {− 1,1}2. [GKR22] also
made such an assumption and gave two implementations: one based on cost-sensitive classification
oracles for GandHseparately, the other a heuristic algorithm that is empirically effective. Details of
these oracle instantiations are included in Appendix B.3.
We also require an optimization oracle for Hitself, defined similarly. This can be thought of simply
as (exact) empirical risk minimization over H.1
Definition 2.3 (H-optimization oracle) .For a collection of hypotheses H ⊆ YX, and a sequence
ofmloss functions ℓi:Y × Y → [−1,1], anH-optimization oracle OPT His a0-approximation
optimization oracle (Definition 2.1, with α= 0) that outputs a hypothesis h∈ H satisfyingPm
i=1wiℓi(h(xi), yi)≤infh∗∈HPm
i=1wiℓi(h∗(xi), yi).
3 Warm-up: I.I.D. Setting
In this section, as a warm-up, we consider a setting where Nature is stochastic and oblivious: the
(xt, yt)are drawn i.i.d. from a single fixed (but unknown) distribution µ, independent of any choices
of the learner. In a standard online prediction setting with i.i.d. data, it suffices to use a “follow-the-
leader” (FTL) strategy (see, e.g., [Haz22]), which can be easily implemented using an optimization
oracle for H. However, such a strategy only guarantees low regret on g=X. To achieve low regret
on all (possibly intersecting) groups g∈ Gsimultaneously, we need a multi-group analogue of FTL.
1In fact, it will suffice to have a substantially simpler oracle that, given a single (x, y)∈ X × Y , determines
if there exists h∈ H such that h(x) =y.
5What makes FTL work in the standard online prediction setting is the instantaneous expected regret
bound of empirical risk minimization (ERM) on i.i.d. data. Therefore, it is natural to replace ERM
with a batch multi-group algorithm [TH22; GKR22]; this will ensure the requisite instantaneous
guarantee on all groups g∈ G. We show how to use the oracle-efficient algorithm LISTUPDATE
of [GKR22] for the online multi-group problem.
Throughout this section, we assume ℓis the zero-one loss (for simplicity), and that HandGboth
have VC dimension at most d≥1. For any g∈ G, letP(g) :=E(x,y)∼µ[g(x)]be the probability
mass of group g.
Theorem 3.1 (Theorem 16 of [GKR22]) .For any δ∈(0,1), given ni.i.d. training samples
{(xi, yi)}n
i=1fromµ, the LISTUPDATE algorithm2returns a function f:X → Y such that, with
probability 1−δ, for any group g∈ G,
E[ℓ(f(x), y)|x∈g]≤min
h∈HE[ℓ(h(x), y)|x∈g] +1
P(g)·O dlogn+ log(1 /δ)
n1/3!
.
Moreover, LISTUPDATE makes poly( n, d,log(1/δ))calls to a (G,H)optimization oracle.
Our algorithm, O NLINE LISTUPDATE , forms its prediction ˆytin round tas follows:
• Run L ISTUPDATE on the samples from previous rounds (x1, y1), . . . , (xt−1, yt−1).
• Let ftdenote the function returned by L ISTUPDATE , and predict ˆyt:=ft(xt).
Using Theorem 3.1, we obtain the following multi-group regret bound for O NLINE LISTUPDATE .
Theorem 3.2. If(xt, yt)are drawn i.i.d. from a fixed distribution µoverX × Y ,ONLINE LISTUP-
DATE achieves the following expected multi-group regret bound: for all g∈ G,
E[RegT(H, g)] =O
(dlogT)1/3T2/3+p
dTlogT
.
The proof of Theorem 3.2 is given in Appendix A.
4 Group Oracle-Efficiency with Smooth Contexts
In this section, we first describe a natural problem setting in which oracle-efficient online multi-group
learning is possible: the smoothed online learning setting (Section 4.1), for which the i.i.d. setting of
Section 3 is a special case. We then present our main algorithm, Algorithm 1, for achieving oracle-
efficient online multi-group learning (Section 4.2). Easy modifications of this main algorithm will
admit oracle-efficient online multi-group learning for other common online learning specifications,
as described in Section 5.
4.1 Smoothed Online Learning
We now describe smoothed online learning , a prevalent model in recent literature in computation-
ally efficient online learning that formalizes the natural relaxation that Nature is not maximally
adversarial [RST11; HRS22; Hag+22; Blo+22]. The main assumption is that, instead of choosing
arbitrary (possibly worst-case) examples (xt, yt)∈ X × Y at every round, Nature adversarially fixes
a distribution µtoverXand draws xt∼µt, while still drawing ytadversarially. Formally, we restrict
such distributions to be σ-smooth , following the definitions of [Blo+22; HRS22].
Definition 4.1 (σ-smooth distribution) .Letµbe some probability measure on X, and let Bbe a base
measure on X.The distribution µisσ-smooth (with respect to B) ifµis absolutely continuous3with
respect to Band
ess supdµ
dB≤1
σ.
We denote the set of all σ-smooth distributions on Xwith respect to the measure BasSσ(X,B).IfB
is clear from context, we simply write Sσ(X).We assume that we have sample access to Bthroughout.
For simplicity, one may assume Bis uniform on X.
2Technically, we use the T RAIN BYOPTvariant of L ISTUPDATE from Theorem 16 of [GKR22].
3A probability measure µisabsolutely continuous to another measure Bif, for every B-measurable set A,
B(A) = 0 implies µ(A) = 0 .
6Definition 4.1 interpolates between the benign setting where xtare drawn i.i.d. from µwhen σ= 1,
and the fully adversarial setting when σapproaches 0.In this sense, the warm-up result of Section 3
is a special case of this setting when σ= 1andµis fixed for all rounds. Note that this definition
does not restrict the choice of ytat all; ytmay still be chosen adversarially.
With this definition in hand, consider the following specification of the learning game outlined in
Section 2.2, henceforth refered to as the σ-smooth online learning setting. For each round t∈[T]:
1.Nature fixes a distribution µt∈ Sσ(X)that may depend in any way on the entire history prior to
round t. Nature samples xt∼µtand chooses yt∈ Y adversarially; xtis revealed to the learner.
2. The learner (randomly) chooses an action ˆyt∈ Y.
3. Nature reveals yt∈ Y, and the learner incurs the loss ℓ(ˆyt, yt)∈[0,1].
We now depart from the previous literature that considers oracle-efficient algorithms in this setting
([HRS22; Blo+22]), as we focus on the more difficult objective of minimizing multi-group regret
over a collection G, as in Equation (1). This setting will allow us to employ our (G,H)-optimization
oracle in Definition 2.2; a full description of our algorithm is now in order in Section 4.2.
4.2 Algorithm for Smooth Contexts
In this section, we present Algorithm 1, our main algorithm for multi-group online learning for the
σ-smooth setting. At a high level, our algorithm takes inspiration from the very general adversary-
moves-first (AMF) framework for multiobjective online learning of [Lee+22]. Our algorithm can be
thought of as a sequential game between two competing players: an adversarial (G,H)-player and the
learner, referred to, in the context of Algorithm 1 as the H-player. On each round t, the(G,H)-player
employs a (G,H)-optimization oracle, OPTα
(G,H), to play a distribution over group-hypothesis pairs
that maximizes the misfortune of the H-player, based on the history up until t. Upon receiving
this distribution (and the context xtfrom Nature), the H-player chooses ˆytrandomly according to
a distribution obtained by solving a simple constant-size linear program, and then incurs the loss
ℓ(ˆyt, yt). The (G,H)-player, taking this new loss into account, can now adjust his strategy to foil
theH-player in the next round by putting mass on the groups on which the H-player performs
poorly. Crucially, neither GnorHis ever accessed directly, although our proofs need to maintain a
distribution over G × H .In order to do this, we make the crucial observation that FTPL maintains an
implicit distribution over G × H and we sparsely approximate that distribution through repeatedly
querying the OPTα
(G,H)oracle. (For clarity, we use the tilde decoration, ˜hand˜g, on hypotheses and
groups obtained by the (G,H)-player using OPTα
(G,H).)
Main algorithm. For any x∈ X, define ˜ℓx: ({0,1} × Y )×(Y × Y )→[−1,1]as:
˜ℓx((˜g,˜h),(y′, y)) := ˜ g(x)
ℓ(y′, y)−ℓ(˜h(x), y)
, (3)
where ℓ(·,·)is the loss given by the learning problem. The quantity ˜ℓxis the loss that the (G,H)-
player is maximizing; it corresponds to the single-round regret of the learner on group gto the
hypothesis hif the context on that round is x.
The(G,H)-player will employ the FTPL style strategy of [Blo+22], adapted to our setting. For each
round t,this requires generating nperturbation examples as extra input to OPTα
(G,H). To generate
these hallucinated perturbation examples, we independently draw zt,j∼ B andγt,j∼N(0,1),
samples j∈[n]from the base measure and the standard Gaussian, respectively. In this section,
Y={−1,1}andH ⊆ {− 1,1}X, so we use the perturbations in their FTPL variant for binary-valued
action spaces (outlined in Appendix B.6),
πbin
t,n(g, h, η ) :=nX
j=1ηγt,jg(zt,j)h(zt,j)√n. (4)
Remark. We focus on the setting where Y={−1,1}for ease of exposition, but settings in which Y
is a general finite set can be handled with easy modifications. See Appendix C for details.
Remark. Multi-group online learning settings other than the σ-smooth setting can be handled
appropriately simply by replacing the strategy of the (G,H)-player by an appropriate no-regret
7Algorithm 1 Algorithm for Group-wise Oracle Efficiency (for smoothed online learning)
Input: Perturbation strength η >0; perturbation count n∈N; number of oracle calls M∈N.
1:fort= 1,2,3, . . . , T do
2: Receive a context xt∼µtfrom Nature.
3: fori= 1,2,3, . . . , M do
4: (G,H)-player: Draw nhallucinated examples as in Equation (4) to construct πbin
t,n.
5: (G,H)-player: Using the entire history {(ˆys, ys)}t−1
s=1so far, call OPTα
(G,H)to obtain
(˜g(i)
t,˜h(i)
t)∈ G × H satisfying:
t−1X
s=1˜ℓxs((˜g(i)
t,˜h(i)
t),(ˆys, ys)) +πbin
t,n(˜g(i)
t,˜h(i)
t, η)
≥ sup
(g∗,h∗)∈G×Ht−1X
s=1˜ℓxs((g∗, h∗),(ˆys, ys)) +πbin
t,n(g∗, h∗, η)−α(5)
6: end for
7:H-player: CallOPT Htwice on the singleton datasets {(xt,1)}and{(xt,−1)}, with the 0-1
loss, obtaining:
h′
1∈arg min
h∗∈H1{h∗(xt)̸= 1}, h′
−1∈arg min
h∗∈H1{h∗(xt)̸=−1}.
8:H-player: Solve the linear program
min
p,λ∈Rλ
subj. toMX
i=1p˜ℓxt((˜g(i)
t,˜h(i)
t),(h′
1(xt), y)) + (1 −p)˜ℓxt((˜g(i)
t,˜h(i)
t),(h′
−1(xt), y))≤λ
∀y∈ {− 1,1}
0≤p≤1.
9: Sample b∼Ber(p)where b∈ {− 1,1}, letht=h′
b.
10: Learner commits to the action ˆyt=ht(xt); Nature reveals yt.
11: Learner incurs the loss ℓ(ˆyt, yt).
12:end for
algorithm with access to OPTα
(G,H). Examples of such variants are given in Section 5, and the
general framework for such modifications is given in Appendix B.
Remark. With appropriate modifications, one can instantiate the (G,H)-player with the FTPL style
strategy of [Hag+22] instead, inheriting the σ−1/4dependence summarized in Table 1. Our focus in
this paper is not on the dependence on σ, however, so our main exposition centers around the similar
algorithmic techniques of [Blo+22].
It is clear that GandHare never accessed except through OPTα
(G,H)andOPT H. We make Moracle
calls to OPTα
(G,H)and two oracle calls to OPT Hat each round.
Theorem 4.1. LetY={−1,1}be a binary action space, H ⊆ {− 1,1}Xbe a binary-valued
hypothesis class, G ⊆2Xbe a (possibly infinite) collection of groups, and ℓ:{−1,1} × {− 1,1} →
[0,1]be a bounded loss function. Let the VC dimensions of HandGboth be bounded by d.Let
α≥0be the approximation error of the oracle OPTα
(G,H). If we are in the σ-smooth online learning
setting, then, for M= poly( T), n= poly( T/σ),andη= poly( T/σ), Algorithm 1 achieves, for
eachg∈ G:
E[RegT(H, g)]≤O r
dTlogT
σ+αT!
,
8where the expectation is over all the randomness of the (G,H)-player’s perturbations and the
H-player’s Bernoulli choices. (See Corollary B.7.1 for precise settings of M,n, and η.)
Technical details. We solve three main difficulties toward ensuring that our algorithm achieves
diminishing multi-group regret while maintaining computational efficiency for large or infinite G,
which we outline here. The full proof of Theorem 4.1 is in Appendix B.
First, although the general framework of casting online learning problems with multiple objectives as
two-player games is not new ([Lee+22; HJZ24; HPY23]), previous works have employed a multi-
plicative weights algorithm to hedge against the multiple objectives, requiring explicit enumeration.
Departing from previous literature, however, our (G,H)-player uses a follow-the-perturbed leader
(FTPL) style algorithm (see, e.g., [KV05]) with OPT (G,H).The particular follow-the-perturbed
leader variant of [Blo+22] constructs “perturbations” via a set of fake examples drawn from the the
base measure BonX, and, is thus suitable for our oracle and problem setting.
Second, a key property needed by the proof of Algorithm 1 is that the H-player must receive a
distribution overG × H to reduce the complex multi-objective criterion of performing well against
all(g, h)∈ G × H to a scalar quantity. Previous work directly supplied this distribution through the
multiplicative weights algorithm. However, this would involve explicitly enumerating GandH. On
the other hand, using an FTPL algorithm as is would only output a single action from G × H , which
is insufficient. To remedy this, we make the crucial observation that FTPL algorithms implicitly
maintain a distribution over G × H through the randomness of their perturbations, and, thus, we
construct the empirical approximation of this distribution through repeatedly calling OPT (G,H).
Standard uniform convergence arguments are used to bound the number of oracle calls needed. An
argument employing the minimax theorem shows that the final regret guarantee of the entire algorithm
essentially inherits the regret of the FTPL algorithm, plus sublinear error terms.
Finally, the H-player chooses a distribution over Yby by solving an exceedingly simple linear
program (LP) with two optimization variables, pandλ. The value p∈[0,1]corresponds to the
parameter of a Bernoulli distribution from which we sample to choose ˆyt. This choice of action
corresponds exactly to choosing the minimax optimal strategy against the worst-case ythat Nature
could select. We employ similar techniques as [Lee+22], analyzing the value of this min-max game
as if Nature (the max in the min-max) had gone first instead. The two calls to OPT Hare used just to
find the actions achievable by Honxt. (Note that it is possible that h′
y′(xt)̸=y′for some y′∈ Y,
in which case the Learner will always play −y′, regardless of the value of p.)
5 Group Oracle-Efficiency in Other Settings
In the previous section, we presented an algorithm that achieves o(T)expected regret for all g∈ G,
satisfying our main desideratum from Section 2.2. However, in some cases, we may want something
more. Suppose that some groups are rarer than others; in this case, a natural extension would be to
ensure a stronger “adaptive” regret bound that instead scales with Tg:=PT
t=1g(xt), the number
of times group gappeared in the Trounds. We note that the algorithm of [BL20] achieves such a
multi-group regret guarantee (for finite HandG), but their algorithm is not oracle-efficient. So a
question that remains is whether such guarantees can be achieved in an oracle-efficient manner.
In this section, we are back in the general fully adversarial multi-group online learning setting of
Section 2.2 (without i.i.d. or smoothness assumptions). We discuss how to modify Algorithm 1 via
theGeneralized Follow-the Perturbed-Leader (GFTPL) framework of [Dud+20; Wan+22] to obtain
regret guarantees on group gwhere the dependence on Tis replaced (at least in part) with Tg. Due to
space limitations, we give a sketch here; the full details are in Appendix C.
We first make a simple observation that motivates our use of more advanced oracle-efficient online
learning techniques. Recall the ˜g-specific per-round regret to ˜hof playing h(xt)at round t∈[T]:
˜ℓxt((˜g,˜h),(h(xt), y)) = ˜g(xt)
ℓ(h(xt), y)−ℓ(˜h(xt), y)
.
The job of the (G,H)-player is to run a no-regret algorithm to maximize this quantity in aggregate, as
described in Section 4.2 and detailed in Appendix B.5. The online learning literature for small-loss
regret focuses on developing algorithms that have regret depending on cumulative loss in hindsight
instead of the number of rounds T[HP05; CL06; GSV14]; this has the advantage of giving a tighter
9regret bound when losses are small in magnitude. It is immediate that ˜ℓxt((˜g,˜h),(h(xt), y)) = 0
whenever ˜g(xt) = 0 , so a small-loss regret would immediately give a o(Tg)guarantee.
We focus on the case where GandHare finite, and G × H is the set of experts the (G,H)-player has
access to. Most small-loss regret algorithms would require prohibitive enumeration of G × H [HP05;
CL06; GSV14; LS15], but the GFTPL with small-loss bound algorithm of [Wan+22] has the property
that it is oracle-efficient andenjoys small-loss regret. This algorithm follows the GFTPL design
template of [Dud+20], which, similar to the classic FTPL algorithm of [KV05], generates a noise
vector to perturb each each decision of expert. However, whereas the classic FTPL algorithm
generates |G| × |H| independent random noise variables, GFTPL only generates N≪ |G| × |H|
independent random variables and uses a perturbation matrix (PM) Γ∈[−1,1]|G||H|× Nto translate
the noise vector back to |G| × |H| dependent perturbations.
The main challenge in instantiating a GFTPL algorithm is to construct a suitable Γfor the problem
at hand. [Wan+22] provide two sufficient conditions for Γthat, respectively, imbue the GFTPL
algorithm with oracle-efficiency and small-loss regret: implementability andapproximability. In our
setting, implementability requires that every column of Γcorrespond to a dataset of “fake examples”
suitable to OPTα
(G,H). Approximability with parameter γ >0guarantees the stability property that
the ratio P[(˜gt,˜ht) = ( g, h)]/P[(˜gt+1,˜ht+1) = ( g, h)]≤exp(γηt)for all (g, h)∈ G × H ,where
ηt>0is the per-round learning rate of GFTPL. If such a Γexists, then instantiating the (G,H)-player
in Algorithm 1 with GFTPL (instead of the algorithm of [Blo+22]) gives us the stronger o(Tg)regret
guarantee. Full definitions and the proof, with the precise setting of M, can be found in Appendix C
and Proposition C.3.1.
Theorem 5.1. Assume H,Gare finite and there exists a γ-approximable and implementable pertur-
bation matrix Γ∈[−1,1]|G||H|× N. Let α≥0be the approximation parameter of OPTα
(G,H).Let
the no-regret algorithm for the (G,H)-player in Algorithm 1 be the GFTPL algorithm of [Wan+22]
instantiated with Γ, with parameter M= poly( T). Then, for each g∈ G:
E[RegT(H, g)]≤Op
Tgmaxn
γ,log|H||G| ,p
Nlog|H||G|o
+αT
We give a particular setting in which one can easily construct an approximable and implementable Γ.
Transductive Setting. In the transductive setting of [SKS16; Dud+20], Nature reveals a set X⊂ X
to the Learner at the beginning of the learning process; then at each round t∈[T], Nature can only
choose xtfrom X. LetN:=|X|denote the number of different contexts that Nature chooses from.
For this setting, we can explicitly construct Γto get the following result.
Corollary 5.1.1 (Transductive setting) .In the transductive setting, there exists a perturbation matrix
Γ∈[−1,1]|G||H|× 4Nsuch that Algorithm 1 with GFTPL parameterized with M= poly( T)and
OPTα
(G,H)with error parameter α≥0satisfies:
E[RegT(H, g)]≤O 
p
Tgr
maxn
log|H||G| ,p
Nlog|H||G|o
+αT!
for all g∈ G.
Suppose that N≤T(which is the case in the transductive learning setting from [KK05]). Then
the regret bound (ignoring the dependence on log(|H||G| )) on group gisO(p
TgT1/4), which is
asymptotically smaller than√
Twhenever Tg=o(√
T). IfNis fixed independent of T, then the
regret bound is O(p
Tg).
6 Conclusion and Future Work
In this paper, we design algorithms for online multi-group learning that are oracle-efficient and
achieve diminishing o(T)expected regret for all groups g∈ Gsimultaneously, even when Gis too
large to explicitly enumerate. The most interesting future directions that we leave open in this work
include designing oracle-efficient algorithms that achieve o(Tg)group-specific regret for infinite H
andGand in more general settings.
10Acknowledgments
We are grateful to Abhishek Shetty for pointing out possible improvements to the dependence on σ.
We acknowledge funding support from a Google Faculty Research Award to Daniel Hsu, the NSF
under grants IIS-2040971 and CCF-2008733, and the ONR under grants N00014-24-1-2700 and
N00014-22-1-2713. Samuel Deng acknowledges funding from the Avanessians Doctoral Fellowship
for Engineering Thought Leaders and Innovators in Data Science.
References
[Ach+23] Krishna Acharya, Eshwar Ram Arunachaleswaran, Sampath Kannan, Aaron Roth, and
Juba Ziani. Oracle Efficient Algorithms for Groupwise Regret . arXiv:2310.04652 [cs].
Oct. 2023.
[BKM97] Shai Ben-David, Eyal Kushilevitz, and Yishay Mansour. “Online Learning versus Offline
Learning”. In: Machine Learning 29.1 (Oct. 1997), pp. 45–63.
[BL20] Avrim Blum and Thodoris Lykouris. “Advancing Subgroup Fairness via Sleeping Ex-
perts”. In: 11th Innovations in Theoretical Computer Science Conference (ITCS 2020) .
Ed. by Thomas Vidick. V ol. 151. Leibniz International Proceedings in Informatics
(LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl – Leibniz-Zentrum für Informatik, 2020,
55:1–55:24.
[Blo+22] Adam Block, Yuval Dagan, Noah Golowich, and Alexander Rakhlin. “Smoothed Online
Learning is as Easy as Statistical Learning”. In: Proceedings of Thirty Fifth Conference
on Learning Theory . Ed. by Po-Ling Loh and Maxim Raginsky. V ol. 178. Proceedings
of Machine Learning Research. PMLR, July 2022, pp. 1716–1786.
[Blu97] Avrim Blum. “Empirical support for winnow and weighted-majority algorithms: Results
on a calendar sched uling domain”. In: Machine Learning 26.1 (1997), pp. 5–23.
[BM07] Avrim Blum and Yishay Mansour. “From External to Internal Regret”. In: Journal of
Machine Learning Research 8.47 (2007), pp. 1307–1324.
[BT97] Dimitris Bertsimas and John N. Tsitsiklis. Introduction to linear optimization. V ol. 6.
Athena scientific optimization and computation series. Athena Scientific, 1997, pp. I–XV ,
1–587.
[CL06] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games . Cambridge
University Press, 2006.
[Dud+20] Miroslav Dudík, Nika Haghtalab, Haipeng Luo, Robert E. Schapire, Vasilis Syrgkanis,
and Jennifer Wortman Vaughan. “Oracle-efficient Online Learning and Auction Design”.
In:J. ACM 67.5 (Sept. 2020).
[Fre+97] Yoav Freund, Robert E Schapire, Yoram Singer, and Manfred K Warmuth. “Using and
combining predictors that specialize”. In: Proceedings of the twenty-ninth annual ACM
Symposium on Theory of Computing . 1997, pp. 334–343.
[GKR22] Ira Globus-Harris, Michael Kearns, and Aaron Roth. “An Algorithmic Framework for
Bias Bounties”. In: Proceedings of the 2022 ACM Conference on Fairness, Accountabil-
ity, and Transparency . FAccT ’22. New York, NY , USA: Association for Computing
Machinery, June 2022, pp. 1106–1124.
[Glo+23] Ira Globus-Harris, Declan Harrison, Michael Kearns, Aaron Roth, and Jessica Sorrell.
“Multicalibration as boosting for regression”. In: Proceedings of the 40th International
Conference on Machine Learning . ICML’23. JMLR.org, 2023.
[GSV14] Pierre Gaillard, Gilles Stoltz, and Tim Van Erven. “A second-order bound with excess
losses”. In: Conference on Learning Theory . PMLR. 2014, pp. 176–196.
[Hag+22] Nika Haghtalab, Yanjun Han, Abhishek Shetty, and Kunhe Yang. “Oracle-efficient online
learning for smoothed adversaries”. In: Advances in Neural Information Processing
Systems 35 (2022), pp. 4072–4084.
[Haz22] Elad Hazan. Introduction to Online Convex Optimization . 2nd ed. MIT Press, Sept. 2022.
[Heb+18] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. “Multicali-
bration: Calibration for the (Computationally-Identifiable) Masses”. In: Proceedings of
the 35th International Conference on Machine Learning . PMLR, July 2018, pp. 1939–
1948.
11[HJZ24] Nika Haghtalab, Michael Jordan, and Eric Zhao. “A unifying perspective on multi-
calibration: Game dynamics for multi-objective learning”. In: Advances in Neural Infor-
mation Processing Systems 36 (2024).
[HK16] Elad Hazan and Tomer Koren. “The computational power of optimization in online
learning”. In: Proceedings of the forty-eighth annual ACM symposium on Theory of
Computing . STOC ’16. New York, NY , USA: Association for Computing Machinery,
June 2016, pp. 128–141.
[HP05] Marcus Hutter and Jan Poland. “Adaptive Online Prediction by Following the Perturbed
Leader”. In: Journal of Machine Learning Research 6.22 (2005), pp. 639–660.
[HPY23] Nika Haghtalab, Chara Podimata, and Kunhe Yang. “Calibrated Stackelberg Games:
Learning Optimal Commitments Against Calibrated Agents”. In: Advances in Neural
Information Processing Systems 36 (Dec. 2023), pp. 61645–61677.
[HRS22] Nika Haghtalab, Tim Roughgarden, and Abhishek Shetty. “Smoothed Analysis with
Adaptive Adversaries”. In: 2021 IEEE 62nd Annual Symposium on Foundations of
Computer Science (FOCS) . Feb. 2022, pp. 942–953.
[Kea+18] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. “Preventing fairness ger-
rymandering: Auditing and learning for subgroup fairness”. In: International conference
on machine learning . PMLR. 2018, pp. 2564–2572.
[KGZ19] Michael P Kim, Amirata Ghorbani, and James Zou. “Multiaccuracy: Black-box post-
processing for fairness in classification”. In: Proceedings of the 2019 AAAI/ACM Con-
ference on AI, Ethics, and Society . 2019, pp. 247–254.
[KK05] Sham Kakade and Adam Tauman Kalai. “From Batch to Transductive Online Learning”.
In:Advances in Neural Information Processing Systems . V ol. 18. MIT Press, 2005.
[KV05] Adam Kalai and Santosh Vempala. “Efficient algorithms for online decision problems”.
In:Journal of Computer and System Sciences 71.3 (Oct. 2005), pp. 291–307.
[Lee+22] Daniel Lee, Georgy Noarov, Mallesh Pai, and Aaron Roth. “Online minimax multiob-
jective optimization: Multicalibeating and other applications”. In: Advances in Neural
Information Processing Systems 35 (2022), pp. 29051–29063.
[LS15] Haipeng Luo and Robert E Schapire. “Achieving all with no parameters: Adanormal-
hedge”. In: Conference on Learning Theory . PMLR. 2015, pp. 1286–1304.
[Nat89] B. K. Natarajan. “On learning sets and functions”. In: Machine Learning 4.1 (Oct. 1989),
pp. 67–97.
[NMR44] John von Neumann, Oskar Morgenstern, and Ariel Rubinstein. Theory of Games and
Economic Behavior (60th Anniversary Commemorative Edition) . Princeton University
Press, 1944.
[Oak+20] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. “Hidden
stratification causes clinically meaningful failures in machine learning for medical
imaging”. In: Proceedings of the ACM conference on health, inference, and learning .
2020, pp. 151–159.
[RST11] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. “Online Learning: Stochastic,
Constrained, and Smoothed Adversaries”. In: Advances in Neural Information Process-
ing Systems . V ol. 24. Curran Associates, Inc., 2011.
[RY21] Guy N. Rothblum and Gal Yona. “Multi-group Agnostic PAC Learnability”. In: Pro-
ceedings of the 38th International Conference on Machine Learning . PMLR, July 2021,
pp. 9107–9115.
[Sag+20] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. “Distribution-
ally robust neural networks for group shifts: On the importance of regularizat ion for
worst-case generalization”. In: International Conference on Learning Representations .
2020.
[SKS16] Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert Schapire. “Efficient Algorithms
for Adversarial Contextual Learning”. In: Proceedings of The 33rd International Con-
ference on Machine Learning . Ed. by Maria Florina Balcan and Kilian Q. Weinberger.
V ol. 48. Proceedings of Machine Learning Research. New York, New York, USA: PMLR,
June 2016, pp. 2159–2168.
[TH22] Christopher J. Tosh and Daniel Hsu. “Simple and near-optimal algorithms for hid-
den stratification and multi-group learning”. In: Proceedings of the 39th International
Conference on Machine Learning . PMLR, June 2022, pp. 21633–21657.
12[Wan+22] Guanghui Wang, Zihao Hu, Vidya Muthukumar, and Jacob D. Abernethy. “Adaptive
Oracle-Efficient Online Learning”. In: Advances in Neural Information Processing
Systems 35 (Dec. 2022), pp. 23398–23411.
13A Proof of Theorem 3.2
The goal is to prove that O NLINE LISTUPDATE satisfies, for all g∈ G,
E[RegT(H, g)]≤O
(dlogT)1/3T2/3+p
dTlogT
.
Fix any g∈ G. For each h∈ H, define
RegT(h, g) :=TX
t=1g(xt)(ℓ(ˆyt, yt)−ℓ(h(xt), yt)).
By linearity of expectation, Theorem 3.1, and an elementary integral bound,
E[RegT(h, g)] =TX
t=1E[g(xt)ℓ(ˆyt, yt)−E[g(xt)ℓ(h(xt), yt)]
≤1 +TX
t=2P(g)·(E[ℓ(ft(xt), yt)|xt∈g]−E[g(xt)ℓ(h(xt), yt)|xt∈g])
≤1 +TX
t=2 
(1−δ)·O dlog(t−1) + log(1 /δ)
t−11/3!
+δ!
= 1 + O
(dlogT+ log(1 /δ))1/3T2/3+δT
.
Plug-in δ= 1/Tto obtain
E[RegT(h, g)]≤O
(dlogT)1/3T2/3
.
It remains to relate max h∈HE[RegT(h, g)]toE[RegT(H, g)]. Define
hg∈arg min
h∈HE(x,y)∼µ[g(x)ℓ(h(x), y)]and ˆhg∈arg min
h∈HTX
t=1g(xt)ℓ(h(xt), yt).
Then
E[RegT(H, g)] = max
h∈HE[RegT(h, g)] +E"TX
t=1g(xt)(ℓ(hg(xt, yt)−ℓ(ˆhg(xt), yt))#
.
SinceHhas VC dimension at most d, a standard uniform convergence argument implies
E"
max
h∈HTE(x,y)∼µ[g(x)ℓ(h(x), y)]−TX
t=1g(xt)ℓ(h(xt), yt)#
≤Op
dTlogT
.
Using the definitions of hgandˆhg, we obtain
E"TX
t=1g(xt)(ℓ(hg(xt, yt)−ℓ(ˆhg(xt), yt))#
≤Op
dTlogT
.
Therefore, we conclude that
E[RegT(H, g)]≤O
(dlogT)1/3T2/3+p
dTlogT
.
This finishes the proof of Theorem 3.2.
14B Proof of Main Theorem 4.1
In this section, we prove the multi-group regret guarantee of our main algorithm, Algorithm 1. To
restate the theorem, we aim to show, for all g∈ G,
E[RegT(H, g)]≤O r
dTlogT
σ+αT!
.
More explicitly, we aim to show that:
TX
t=1E[g(xt) (ℓ(ht(xt), yt)−ℓ(h∗(xt), yt))]≤O r
dTlogT
σ+αT!
,
where h∗∈minh∈HPT
t=1g(xt)ℓ(h(xt), yt).We follow a generalization of the online minimax
multiobjective optimization framework of [Lee+22], with techniques inspired by [HPY23].
B.1 The AMF Algorithm Framework
We first restate their “adversary-moves-first” AMF algorithm of [Lee+22] and its main regret guarantee
for convenience, as we will need to adapt and generalize it to our setting. Let Atdenote a general
action space of the learner, and let Ztdenote the general action space of the adversary at round
t∈[T]. In full generality, AtandZtare allowed to change with the rounds t∈[T].We differentiate
this from the action space Yof the main body. For each round t= 1, . . . , T , consider the following
setting, which we refer to as the multiobjective online optimization problem :
1.The adversary selects a continuous, d-dimensional loss function rt:At× Zt→[−1,1]d. Each
component rj
t:At× Zt→[−1,1]is convex in Atand concave in Zt.
2. The learner selects an action at∈ At.
3. Nature observes the learner’s action atand responds with zt∈ Zt.
4. The learner incurs the d-dimensional loss rt(at, zt).
In this setting, the learner’s goal is to minimize the value of the maximum dimension of the accumu-
lated loss vector after Trounds:
max
j∈[d]TX
t=1rj
t(at, zt).
To benchmark the learner’s performance, we consider the following quantity, which we refer to as the
adversary-moves-first (AMF) value at round t.
Definition B.1 (Adversary-Moves-First (AMF) Value at Round t,[Lee+22]) .Theadversary-moves-
first (AMF) value at round tis the value:
vA
t:= max
zt∈Ztmin
at∈At
max
j∈[d]rj
t(at, zt)
. (6)
We conceive of the value vA
tin(6)as the aspirational smallest value of the maximium coordinate of
rtthe learner could guarantee ifthe adversary had to reveal ztfirst and the learner could best respond
withat.Per how the multiobjective online optimization problem is set up, however, the opposite is
true — the learner must commit to an action at∈ Atfirst, and then the adversary is allowed to play
zt∈ Ztin response to maximize the learner’s misfortune. Regardless, we can define a notion of
regret with respect to this particular benchmark.
Definition B.2 (Adversary-Moves-First (AMF) Regret) .Over Trounds of the above multiobjective
online optimization problem , the adversary-moves-first regret of the learner is:
AMFRegT:= max
j∈[d] TX
t=1rj
t(at, zt)−vA
t!
.
This notion measures the cumulative regret the learner has for not playing the action at∈ A t
achieving the aspirational value vA
tat round tover alldcoordinates of the loss vector.
15Algorithm 2 AMF Algorithm of [Lee+22]
1:fort= 1,2,3, . . . , T do
2: Receive adversarially chosen action spaces AtandZtand the d-dimensional loss function
rt:At× Zt→[−1,1]d.
3: Let
qj
t:=exp
ηPt−1
s=1rj
s(as, zs)
P
i∈[d]exp
ηPt−1
s=1ris(as, zs)forj∈[d].
Fort= 1, letqj
t= 1/dfor all j∈[d].
4: Solve the min-max optimization problem:
at∈arg min
a∈Atmax
z∈ZtX
j∈[d]qj
trj
t(a, z). (7)
5: Commit to the action at∈ At, observe Nature’s choice zt∈ Zt, and incur loss rt(at, zt).
6:end for
A natural question, then, is to wonder if such a regret can be made to diminish sublinearly. That is,
does there exist an algorithm such that AMFRegT≤o(T)? [Lee+22] answer this in the affirmative,
presenting Algorithm 2.
It is not immediately clear at first glance how Algorithm 2 translates to our multi-group online
learning setting. In the next section, we will show a reduction from our setting to the this AMF
framework. We will specialize Algorithm 2 to our setting and prove a “meta-theorem” similar to
the original guarantee of Algorithm 2 and proceed to control the regret via that meta-theorem in
subsequent sections.
B.2 Reduction of multi-group online learning to AMF Framework
Recall that, in this paper, we actually care about the online multi-group learning setting described in
Section 2.2. We restate the objective here for convenience.
In multi-group online learning, the learner has access to a hypothesis class Htaking contexts from
Xand outputting actions in Y, a common action space for the learner and Nature. There is a
common loss function for the problem, ℓ(·,·) :Y × Y → [0,1]. To allay confusion, we refer to the
adversary in the multi-group online setting of Section 2.2 as “Nature” and the adversary in the online
multiobjective optimization problem of Section B.1 as “the adversary.” The learners in both settings
correspond to one another, so we just use “the learner.” For clarity of exposition, we let Y={−1,1}
be a binary action space for the remainder of Appendix B. The generalization to the case where Y
takes Kdiscrete values is sketched in Appendix C.
We consider the regret of the learner on subsequences of rounds (t∈[T] :xt∈g)defined by the
groups g∈ Gand the sequence of contexts x1, . . . , x T. Specifically, the (multi-group) regret of the
learner on group gis
RegT(H, g) :=TX
t=1g(xt)ℓ(ˆyt, yt)−min
h∈HTX
t=1g(xt)ℓ(h(xt), yt). (8)
Crucially, the best hypothesis for one group may differ from that of another group. The learner seeks
to achieve achieve sublinear expected regret, on all groups g∈ Gsimultaneously.
We now show a reduction from the multi-group online learning setting to the general multiobjective
online optimization problem of the previous Section B.1. An important observation is that, when
Y={−1,1}, varying ht∈ H only affects the regret at round t∈[T]insofar as its behavior on xt.
That is, for a fixed xt∈ X andy∈ Y,ℓ(ht(xt), y)∈ {ℓ(−1, y), ℓ(1, y)}. Note that it is possible for
the set {h(xt) :h∈ H} ⊆ Y to be a singleton set, in which case the learner will always take this
unique action.
• LetAt, the learner’s action space for round t∈[T]in Section B.1, be the simplex ∆(Y).
16•LetZt, the adversary’s action space in Section B.1, be Zt= [0,1]for all t∈[T], which will
correspond to the parameter of a Bernoulli distribution over the binary-valued action space Yof
Nature in the multi-group online learning problem.
•Letrt:At× Z t→[−1,1]d, the adversarially chosen loss function, be G × H -dimensional,
with each coordinate corresponding to a pair (˜g,˜h)∈ G × H .Consider any p, γ∈[0,1], each
determining a Bernoulli distribution over Y={−1,1}. As in Section 4.2, for any x∈ X:
˜ℓx((˜g,˜h),(y′, y)) := ˜ g(x)
ℓ(y′, y)−ℓ(˜h(x), y)
.
Then, define:
r(˜g,˜h)
t(p, γ) :=Ey′∼pEy∼Ber(γ)h
˜ℓxt((˜g,˜h),(y′, y))i
. (9)
Above, the loss ℓ(·,·)is the fixed loss of the multi-group online learning problem, and xt∈ X is
the context chosen by Nature at round t, which indexes rt.
It may now be slightly clearer how Algorithm 1 maps to Algorithm 2, but we provide a high-level
overview here to prepare the reader for the subsequent sections.
•The distribution qt∈∆[d]in Line 3 of Algorithm 2 corresponds to the implicit distribution
formed by querying OPTα
(G,H)Mtimes to generate {(˜gi,˜hi)}M
i=1, i.e. the (G,H)-player.
•Solving the min-max optimization problem in Line 4, Equation (7)of Algorithm 2 corresponds to
the two calls to the simple optimization problem solved by the H-player and solving the simple
one-dimensional linear program in Line 8 of Algorithm 1.
We make these correspondences formal in the subsequent sections. To organize this, we first formally
show the correspondence between the (G,H)-player and the construction of qt, and the H-player and
Equation (7) in Algorithm 2.
B.3 Instantiations of the OPTα
(G,H)oracle
The computational primitive our algorithm assumes access to is a (G,H)-optimization oracle, defined
in Definition 2.2, and requoted here for ease of reference.
Definition B.3 ((G,H)-optimization oracle) .Fix an error parameter α≥0. For a collection
of groups G ∈ 2X, a collection of hypotheses H ⊆ YX, and a sequence of mloss functions
ℓi: ({0,1} × Y )×(Y × Y )→[−1,1], anα-approximate (G,H)-optimization oracle OPTα
(G,H)is
anα-approximation optimization oracle (Definition 2.1) that outputs a pair (˜g,˜h)∈ G×H satisfying:
mX
i=1wiℓi((˜g(xi),˜h(xi)),(yi, y′
i))≥ sup
(g∗,h∗)∈G×HmX
i=1wiℓi((g∗(xi), h∗(xi)),(yi, y′
i))−α. (10)
A common assumption in the literature on oracle-efficient online learning is positing the existence of
some reasonable optimization oracle, typically commensurate to the ability to solve ERM. Although
it is well-known that ERM is computationally hard in the worst-case, a bedrock of modern machine
learning is the assumption that ERM is at least heuristically and approximately solvable. Although,
for the purposes of our work, we assume access to this oracle as a black-box, it is natural to wonder if
such an oracle can be instantiated. [GKR22] gives two such instantiations which we quote here for
completeness.
We consider the specific instantiation of the (G,H)-oracle in Algorithm 1 for a specific round t∈[T],
which aims to solve the following optimization problem for some (˜gt,˜ht)∈ G × H :
t−1X
s=1˜gt(xs)
ℓ(ˆys, ys)−ℓ(˜ht(xs), ys)
≥OPT−α, (11)
where OPT := supg∗,h∗∈(G,H)Pt−1
s=1g∗(xs) (ℓ(ˆys, ys)−ℓ(h∗
t(xs), ys)).
In both instantiations of the oracle in [GKR22], the oracle aims to find a (g, h)∈(G,H)competitive
to some reference model, f:X → { 0,1}.For simplicity, as in the main body, we assume that
17Y={0,1}, and the “reference model” we compete with is given by the Learner’s history of actions
up to round t:(x1,ˆy1), . . . , (xt−1,ˆyt−1). That is, we compare with the function f:X → { 0,1}that
maps f(xs) = ˆysfor all s= 1, . . . , t −1.
Reduction to ternary classification. The first instantiation of a (G,H)oracle in [GKR22] reduces
the optimization oracle to the existence of a solver for a weighted ternary classification problem. The
exposition here is quoted directly from [GKR22].
Start with a class Kofternary valued functions p:X → { 0,1,?}. For each p∈ K, define the
p-derived group andp-derived hypothesis as:
gp(x) =1ifp(x)∈ {0,1}
0ifp(x) =?hp(x) =p(x)ifp(x)∈ {0,1}
0 ifp(x) =?
This class Kinduces a set of pairs (gp, hp)and a product class (G,H)K:={(gp, hp) :p∈ K} . We
may now define a cost-sensitive classification problem over Kas follows, given an existing model
f:X → { 0,1}, with the following costs:
cf((x, y), z) :=

0 ifz=?
1 iff(x) =y̸=z
−1ifz=y̸=f(x)
0 otherwise.
For any distribution µoverX × { 0,1}, the associated cost-sensitive classification problem for costs
cf((x, y), z)defined above is:
p∗∈arg min
p∈KE(x,y)∼µ[cf((x, y), p(x))]. (12)
Many efficient algorithms that heuristically solve such optimization problems exist.
The main theorem from [GKR22], restated here, is the following:
Theorem B.1. Fix any arbitrary distribution µoverX × { 0,1}. LetKbe a class of ternary-valued
functions p:X → { 0,1,?}and let f:X → { 0,1}be any binary-valued model. Let p∗be the
solution to the cost-sensitive classification problem in Equation (12). Then,
(g∗
p, h∗
p)∈arg max
(g,h)∈(G,H)KE(x,y)∼µ[g(x) (ℓ(f(x), y)−ℓ(h(x), y))].
When µis the empirical distribution over x1, . . . , x t−1, the solution (g∗
p, h∗
p)forms a solution to the
optimization problem in Equation (11) when (G,H)K=G × H .
We refer the reader to Section 4.2 in [GKR22] for a proof.
Reduction to alternating maximization. Another instantiation of the (G,H)-oracle in [GKR22] is
an alternating maximization approach. The reduction to ternary classification quoted above relies
on an oracle for the class Kand supplies guarantees for the derived class (G,H)K.However, if we
wish to begin with G × H , we can take an “EM-style” alternating maximization approach that only
requires ERM oracles for GandHseparately. This approach only guarantees a saddle point local
optimum.
The main idea is that, by holding gfixed and solving for h∗, and vice versa, the following optimization
problems are no harder than ERM over GandHindividually:
g∗∈arg max
g∗∈GE(x,y)∼µ[g∗(x) (ℓ(f(x), y)−ℓ(h(x), y))] (13)
h∗∈arg max
h∈HE(x,y)∼µ[g(x) (ℓ(f(x), y)−ℓ(h∗(x), y))]. (14)
In Equation (13),his fixed and we solve for g∗; in Equation (14),gis fixed, and we solve for h∗.
With appropriate modifications to the distribution µwe can construct ERM problems for g∗and
h∗comensurate to solving Equations (13) and(14). We refer the reader to Lemmas 21 and 22 in
[GKR22] for the proofs.
This allows us to state an alternating maximization algorithm for finding a saddle point (g, h)∈ G×H
that gives a local optimum to Equation (11) in Algorithm 3. The corresponding theorem, restated
from [GKR22] is:
18Theorem B.2. Letϵ >0. Fix any empirical distribution over (x1, y1), . . . , (xm, ym), letf:X →
{0,1}be an arbitrary model, and let GandHbe arbitrary group and hypothesis classes. After
solving at most 2/ϵERM problems over each of GandH(in Equations (13) and(14), respectively),
Algorithm 3 returns a pair (g∗, h∗)with the properties that:
1. For every h∈ H,
mX
i=1g∗(xi) (ℓ(f(xi), yi)−ℓ(h(xi), yi))≤mX
i=1g∗(xi) (ℓ(f(xi), yi)−ℓ(h∗(xi), yi)) +ϵ.
2. For every g∈ G,
mX
i=1g(xi) (ℓ(f(xi), yi)−ℓ(h∗(xi), yi))≤mX
i=1g∗(xi) (ℓ(f(xi), yi)−ℓ(h∗(xi), yi)) +ϵ.
Algorithm 3 Alternating Maximization for G × H Oracle
Input: Dataset {(xi, yi)}m
i=1, a model f:X → { 0,1}, error parameter ϵ.
1:Initialize (g∗, h∗)∈ G × H arbitrarily.
2:Let
VAL :=mX
i=1g∗(xi) (ℓ(f(xi), yi))−ℓ(h∗(xi), yi))
3:Use ERM oracle for Equations (13) and (14) to solve for:
g∗∈arg max
g∈GmX
i=1g(x) (ℓ(f(x), y)−ℓ(h∗(x), y))
h∗∈arg max
h∈HmX
i=1g∗(x) (ℓ(f(x), y)−ℓ(h(x), y))
4:whilePm
i=1g∗(x) (ℓ(f(x), y)−ℓ(h∗(x), y))≥VAL + ϵdo
5: Let
VAL :=mX
i=1g∗(xi) (ℓ(f(xi), yi))−ℓ(h∗(xi), yi))
6: Use ERM oracle for Equations (13) and (14) to solve for:
g∗∈arg max
g∈GmX
i=1g(x) (ℓ(f(x), y)−ℓ(h∗(x), y))
h∗∈arg max
h∈HmX
i=1g∗(x) (ℓ(f(x), y)−ℓ(h(x), y))
7:end while
8:Return (g∗, h∗)∈ G × H .
We believe that it is an interesting and worthwhile open question to develop more specific instantia-
tions of this (G,H)-oracle for more specific problem settings that are computationally efficient and
have provable optimization guarantees.
B.4 The group-hypothesis and hypothesis players
We formally define how the (G,H)-player corresponds to the weights qt∈∆[d]in Algorithm 2. The
crucial observation here is that the perturbations of the FTPL algorithm of [Blo+22] used in our
setting form an implicit distribution over G × H that can be approximated by calling the OPTα
(G,H)
oracle Mtimes.
In the proceeding sections, we denote ∆(G × H )as the (possibly infinite-dimensional) space of
measures over the functions G × H . However, we will always only access sparse distributions on this
19space, with a finite number of (g, h)pairs in G ×H obtaining nonzero mass. The following definition
should make this clear.
Definition B.4 (The distribution of the (G,H)-player) .For any round t∈[T], let{(˜gi,˜hi)}M
i=1
be the Msamples drawn from querying OPTα
(G,H)in Algorithm 1. Let the empirical distribution
˜qt∈∆(G × H )be the distribution of the (G,H)-player at round t.
It is easy to see that ˜qtis a valid distribution over (G,H), with measure over A⊆ G × H , defined by:
PM(A) :=1
MMX
i=1δ(˜gi,˜hi)(A),
where δ(˜gi,˜hi)is the Dirac measure of (˜gi,˜hi)falling into the set A.The stochasticity of (˜gi,˜hi)is
over the random perturbations described in Section 4.2, Equation (4). Equipped with this definition,
we can take empirical expectations over ˜qt∈∆(G × H )in the usual way. Observe that, in Algorithm
2, Equation (7), the optimization problem at round t, is equivalent to:
at∈arg min
a∈Amax
z∈ZEj∼qth
rj
t(a, z)i
.
Because the dobjectives in our reduction (Section B.2) correspond to each group-hypothesis pair
(g, h)∈ G × H ,Atcorresponds to ∆(Y), andZtalways corresponds to [0,1], we can equivalently
consider the min-max optimization problem at round t∈[T]:
pt∈arg min
p∈∆(Y)max
γ∈[0,1]E(˜g,˜h)∼˜qth
r(˜g,˜h)
t(p, γ)i
, (15)
where r(˜g,˜h)
t is defined in Equation 9. The next lemma relates the optimization procedure of the
H-player in Algorithm 1 to the min-max optimization procedure of Equation (7) in Algorithm 2.
Lemma B.3 (The optimization of the H-player) .For any round t∈[T], let{(˜gi,˜hi)}M
i=1denote
theMsamples obtained by calling OPTα
(G,H)Mtimes in Algorithm 1, and denote ˜qt∈∆(G × H )
denote the corresponding empirical distribution (Definition B.4). Then, pt∈∆(Y)defined in
Equation (15) above is equivalent to the distribution (p,1−p)∈∆({−1,1})obtained from solving
the linear program of the H-player in Algorithm 1.
Proof. Consider any round t∈[T]. Observe that Line 8 of Algorithm 1 is equivalent to solving the
linear program for λ∈Randp:= (p,1−p)∈R2:
minλ
s.t.X
pi= 1
p⊤˜Lei≤λ∀i∈[2]
pi≥0∀i∈[2]
where the payoff matrix ˜L∈[−1,1]2×2has the coordinates (y′, y):
˜L(y′,y):=1
MMX
i=1˜ℓxt((˜g(i)
t,˜h(i)
t),(y′, y)),
andeiis the ith coordinate vector of R2. Let∆(Y)denote the space of probability distributions over
Y. Let p= (p,1−p)andz= (γ,1−γ), and, as shorthand, denote p(−1) = 1 −p,p(1) = p,
z(−1) = 1 −γ, andz(1) = γ.
20By the equivalence of linear programs (LPs) to zero-sum min-max games (see, e.g., [BT97; Haz22]),
obtaining the optimal pfor this LP is the equivalent to solving:
min
p∈∆(Y)max
z∈∆(Y)p⊤˜Lz= min
p∈∆(Y)max
z∈∆(Y)1
MX
y∈YX
y′∈YMX
i=1p(y′)z(y)˜ℓxt((˜g(i)
t,˜h(i)
t),(y′, y))
= min
p∈∆(Y)max
z∈∆(Y)X
y∈YX
y′∈Yp(y′)z(y)E(˜g,˜h)∼˜qth
˜ℓxt((˜g,˜h),(y′, y))i
= min
p∈∆(Y)max
z∈∆(Y)Ey′∼pEy∼Ber(γ)h
E(˜g,˜h)∼˜qth
˜ℓxt((˜g,˜h),(y′, y)ii
= min
p∈∆(Y)max
γ∈[0,1]Ey′∼Ber(p)Ey∼Ber(γ)h
E(˜g,˜h)∼˜qth
˜ℓxt((˜g,˜h),(y′, y)ii
= min
p∈∆(Y)max
γ∈[0,1]E(˜g,˜h)∼˜qth
r(˜g,˜h)
t(p, γ)i
Above, the first equality just comes from definition of ˜L, the second equality is from the Definition
B.4 of the empirical distribution ˜qt, the third and fourth equalities are from the definition of pandz
in the previous paragraph. The final equality is just from interchanging the order of expectation and
the definition of r(˜g,˜h)
t(p, γ)in Equation (9). By this chain of inequalities, we see that obtaining the
optimal pfor the original LP corresponds exactly to the choice of ptin Equation (15).
Lemma B.3 tells us that the strategy of the H-player (i.e., Lines 8 and 9 in Algorithm 1) to obtain ht
is exactly the same as obtaining the minimizing ptin Equation (15). From the exposition above, this
corresponds to the min-max optimization problem in Equation (7)when qtis˜qt,the distribution over
G × H .We now proceed to prove a more general “meta-theorem” from which the regret guarantee of
Algorithm 1 will follow once we plug in a specific FTPL algorithm for the (G,H)-player.
B.5 Meta-algorithm for Online Multi-group Learning
We now present a meta-algorithm, Algorithm 4, and its corresponding Theorem B.4, the “meta-
theorem” for online multi-group learning from which Theorem 4.1 follows. This is more general,
however, than the guarantee in Theorem 4.1, and we emphasize that, through changing the no-regret
algorithm for the (G,H)-player, we can obtain regret guarantees for settings other than the smoothed
online learning setting of Theorem 4.1. Section 5 gives a couple of examples, which we elaborate in
Appendix C.
Note that approximating the distribution of the (G,H)-player is crucial to obtain computational
efficiency, as we cannot hope to enumerate GandHby explicitly representing qtin Algorithm 4.
Thus, sampling Mtimes is a crucial “sparsification” step that allows us to implicitly access the
distribution over G × H that the (G,H)-player maintains.
We prove Theorem B.4 through techniques similar to the regret guarantee proof of Algorithm 2
in [Lee+22]. Namely, we observe that by using the reduction outlined in Section B.2 with the
G × H -dimensional loss
r(˜g,˜h)
t(p, γ) =Eh(xt)∼pEy∼Ber(γ)h
˜g(xt)
ℓ(h(xt), y)−ℓ(˜h(xt), y)i
=Eh(xt)∼pEy∼Ber(γ)h
˜ℓ
(˜g,˜h),(h(xt), y)i
we may obtain a bound on the multi-group regret of our Algorithm 4 by obtaining an adversary-
moves-first regret guarantee (Definition B.2). For simplicity, we prove this for the case of binary
actions Y={−1,1}; we outline how to obtain a similar theorem for multi-class action spaces in
Appendix C.
Theorem B.4 (Meta-Theorem for Online Multi-group Learning) .LetXbe a context space, let
Y:={−1,1}be a binary action space, and let Hbe a hypothesis class of functions h:X → Y , and
letGbe a collection of groups, g:X → { 0,1}. Letℓ:Y × Y → [0,1]be a bounded loss function.
Suppose the (G,H)-player of Algorithm 4 is instantiated with a no-regret (maximization) algorithm
operating over H × G that has the guarantee that for any sequence of losses of length Tbounded
in[−1,1], by playing (a possibly implicit distribution) qt, it has regret at most R(T)in expectation.
21Algorithm 4 Meta-algorithm for Online Multi-group Learning ( Y={−1,1})
Input: M∈N, the number of samples to take from qtat each step.
1:fort= 1,2,3, . . . , T do
2: Receive a context xtfrom Nature.
3: For any y′, y∈ Y, construct the loss:
˜ℓxt((˜g,˜h),(y′, y)) := ˜ g(xt)
ℓ(y′, y)−ℓ(˜h(xt), y)
.
4: (G,H)-player: With access to the entire history of the past t−1rounds and xt, with access to
OPTα
(G,H), run a no-regret algorithm over the benchmark class G × H to output a distribution
qt∈∆(G × H ).
5: Construct an empirical approximation ˜qtofqtby sampling from qtMtimes, obtaining a
collection of pairs {(˜h(i)
t,˜g(i)
t)}M
i=1fromG × H .
6:H-player: CallOPT Htwice on the singleton datasets {(xt,−1)}and{(xt,1)}with the 0-1
loss, obtaining:
h′
1∈arg min
h∗∈H1{h∗(xt)̸= 1}, h′
−1∈arg min
h∗∈H1{h∗(xt)̸=−1}.
7:H-player: Solve the linear program
min
p,λ∈Rλ
subj. toMX
i=1p˜ℓxt((˜g(i)
t,˜h(i)
t),(h′
1(xt), y)) + (1 −p)˜ℓxt((˜g(i)
t,˜h(i)
t),(h′
−1(xt), y))≤λ
∀y∈ {− 1,1}
0≤p≤1.
8: Sample b∼Ber(p), where b∈ {− 1,1}, letht=h′
b.
9: Learner commits to the action ˆyt=ht(xt); Nature reveals yt.
10: Learner incurs the loss ℓ(ˆyt, yt).
11: The(G,H)-player draws (˜g,˜h)∼˜qtand incurs the loss ˜ℓxt((˜g,˜h),(ˆyt, yt)).
12:end for
Then, with expectation over any randomness of the (G,H)-player and Nature, Algorithm 1 obtains
the multi-group regret guarantee of:
E[RegT(H, g)]≤TX
t=1vA
t+TX
t=1E[ϵt(M)] +R(T), (16)
for all g∈ G, where
vA
t:= max
γ∈[0,1]min
pt∈∆(H)max
(˜g,˜h)∈G×HEh∼pt,y∼Ber(γ)h
˜g(xt)(ℓ(h(xt), y)−ℓ(˜h(xt), y))i
(17)
andϵt(M)is the error incurred from estimating qtwith˜qtwithMsamples at step t∈[T].
Proof. From the perspective of the (G,H)-player, who is running a regret maximization algorithm,
the following game is being played. For t= 1,2,3, . . . , T :
•Receive a context xt∈ X from Nature, possibly adversarially and depending on the past t−1
rounds.
•Play a pair (˜ht,˜gt)∈ H × G , possibly randomly and dependent on the last t−1rounds, where
pairs are functions
(h, g) :X → Y × { 0,1}.
• Commit to the action ˜yt:= (˜ht(xt),˜gt(xt))∈ Y × { 0,1}.
22•An adversary (the H-player’s prediction andNature) reveals (ˆyt, yt)∈ Y × Y , and we incur the
loss:
˜ℓ((˜g,˜h),(ˆyt, yt)) := ˜ g(xt)
ℓ(ˆyt, yt)−ℓ(˜ht(xt), yt)
.
Here, ˆyt=ht(xt), which is a random variable depending on sampling from pt, the Bernoulli
distribution of the H-player at round t.
Note that the (G,H)-player is attempting to maximize this loss.
Leth1, . . . , h Tbe the sequence of hypotheses chosen by the H-player, and let y1, . . . , y Tbe the
sequence of adversarially chosen outcomes. Then, by the regret guarantee of the (G,H)-player’s
no-regret algorithm algorithm, for any (h∗, g∗)∈ H × G :
TX
t=1Eh
˜ℓ((g∗, h∗),(ht(xt), yt))i
−TX
t=1E(˜gt,˜ht)∼qth
˜ℓ((˜gt,˜ht),(ht(xt), yt))i
≤R(T),
where the expectation is over the distributions qtover∆(H×G )that the (G,H)-player commits to at
each round andthe random choices of Nature and the H-player’s random choice of ˆytat each round.
To ease notation, we keep the subscript in the expectation over hthidden, noting that ht(xt)∼pt
is a random variable throughout. For instance, in the case of Theorem 4.1, this is a random process
determined by the (G,H)-player sampling nperturbation terms and calling the (G,H)-oracle to
obtain the random variable (˜gt,˜ht). Instantiating the regret bound for all (h∗, g∗)gives us:
max
(h∗,g∗)∈H×GTX
t=1Eh
˜ℓ((g∗, h∗),(ht(xt), yt))i
≤TX
t=1E(˜gt,˜ht)∼qth
˜ℓ((˜gt,˜ht),(ht(xt), yt))i
+R(T).
However, we do not have direct access to qt, the implicit distribution over G × H , but we have an
approximation ˜qt.In Algorithm 2, (˜gt,˜ht)is drawn according to ˜qt, the empirical distribution over
∆(H × G )formed by drawing Msamples {(˜h(i)
t,˜g(i)
t}M
i=1from ˜qt. Fixing xt, yt,andht(xt), let
ϵt(M)be the error we incur from replacing the true distribution qtby the empirical distribution ˜qt,
which we can handle through uniform convergence on the samples {(˜h(i)
t,˜g(i)
t}M
i=1(see Lemma B.6):
E(˜g,˜h)∼˜qt[˜ℓ((˜g,˜h),(ht(xt), yt))]−E(g,h)∼qt[˜ℓ((g, h),(ht(xt), yt))]≤ϵt(M).
Adding the estimation error at each t∈[T], our regret bound becomes:
max
(h∗,g∗)∈H×GTX
t=1Eh
˜ℓ((g∗, h∗),(ht(xt), yt))i
≤TX
t=1E(˜gt,˜ht)∼qth
˜ℓ((˜gt,˜ht),(ht(xt), yt))i
+R(T)
≤TX
t=1E(˜gt,˜ht)∼˜qth
˜ℓ((˜gt,˜ht),(ht(xt), yt))i
+TX
t=1ϵt(M) +R(T).
Now, we aim to bound the terms E(˜gt,˜ht)∼˜qth
˜ℓ((˜gt,˜ht),(ht(xt), yt))i
. At round t∈[T], Algorithm
4 chooses the best response ht∈ H by solving a linear program for a Bernoulli parameter pand
then sampling hfrom Ber(p). This is equivalent to sampling ht(xt)∼pt, where pt:= (p,1−p)on
∆(Y).We use sampling from this Bernoulli distribution and sampling from ptinterchangeably. By
Lemma B.3, this is equivalent to solving the min-max optimization problem in Equation (15)
pt∈arg min
p∈∆(Y)max
γ∈[0,1]E(˜gt,˜ht)∼˜qth
r(˜gt,˜ht)
t (p, γ)i
,
which, by definition of r(˜g,˜h)
t(p, γ), is equivalent to:
pt∈arg min
pt∈∆(Y)max
γ∈[0,1]E(˜gt,˜ht)∼˜qth
Eht(xt)∼ptEy∼Ber(γ)h
˜gt(xt)
ℓ(ht(xt), y)−ℓ(˜ht(xt), y))ii
.
(18)
The inner expectations are linear in ptandγ, and taking the outer expectation over ˜qtmaintains
linearity. Therefore, this is a convex and concave min-max optimization problem, and von Neumann’s
23minimax theorem [NMR44] applies, allowing us to swap the order of minimization and maximization.
Therefore
min
pt∈∆(Y)max
γ∈[0,1]E(˜gt,˜ht)∼˜qth
Eht(xt)∼ptEy∼Ber(γ)h
˜gt(xt)
ℓ(ht(xt), y)−ℓ(˜ht(xt), y))ii
= max
γ∈[0,1]min
pt∈∆(Y)E(˜gt,˜ht)∼˜qth
Eht(xt)∼ptEy∼Ber(γ)h
˜gt(xt)
ℓ(ht(xt), y)−ℓ(˜ht(xt), y))ii
≤max
γ∈[0,1]min
pt∈∆(Y)max
(˜g,˜h)∈G×HEht(xt)∼pt,y∼Ber(γ)h
˜g(xt)
ℓ(ht(xt), y)−ℓ(˜h(xt), y)i
=vA
t
where the first equality is from the minimax theorem and the second inequality is just because
averages are less than or equal to maximums. Combining all the inequalities, we obtain
max
(h∗,g∗)∈H×GTX
t=1˜ℓ((g∗, h∗),(ht(xt), yt))≤TX
t=1vA
t+TX
t=1ϵt(M) +R(T),
and our theorem follows from taking an expectation on both sides and substituting back the definition
of
˜ℓ((g∗, h∗),(h(xt), yt)) := g∗(xt) (ℓ(h(xt), yt)−ℓ(h∗(xt), yt)),
because each ˜ℓ((g∗, h∗),(h(xt), yt))is simply the per-round regret.
With Theorem B.4 in hand, it remains to make sure that the two termsPT
t=1vA
tandPT
t=1E[ϵt(M)]
are both o(T).Then, if we have a no-regret algorithm with o(T)regret while only accessing Gand
Hthrough the OPTα
(G,H)oracle, we will have a multi-group online learning algorithm. The next two
lemmas show that both terms are, indeed, o(T).
First, we bound the values vA
t, which are known as the “AMF values” in the framework of [Lee+22].
Lemma B.5. For any t∈[T], the AMF value of the game at round t, is nonpositive, i.e.
vA
t:= max
γ∈[0,1]min
pt∈∆(H)max
(h,g)∈H×GE[g(xt)(ℓ(ht(xt), y)−ℓ(h(xt), y))]≤0,
where the expectation is taken over ht∼ptandy∼Ber(γ).
Proof. Fix any parameter γ∈[0,1]for the max player. Then, for any (h, g)∈ H × G ,
E[g(xt)(ℓ(ht(xt), y)−ℓ(h(xt), y))] =γEht∼pt[g(xt)(ℓ(ht(xt),1)−ℓ(h(xt),1))]
+ (1−γ)Eht∼pt[g(xt)(ℓ(ht(xt),−1)−ℓ(h(xt),−1))].
Expanding the expectation over ht∼pt, this is equivalent to:
g(xt)X
h′∈Hpt
h′(γℓ(h′(xt),1) + (1 −γ)ℓ(h′(xt),0))−g(xt)(γℓ(h(xt),1) + (1 −γ)ℓ(h(xt),0)),
so it suffices to find pt∈∆(H)such that, for all (h, g)∈ H × G ,
g(xt)X
h′∈Hpt
h′(γℓ(h′(xt),1) + (1 −γ)ℓ(h′(xt),0))≤g(xt)(γℓ(h(xt),1) + (1 −γ)ℓ(h(xt),0)).
Theg(xt)value is the same for both sides, so it really suffices to find the pt∈∆(H)such that, for
allh∈ H,
pt
h′(γℓ(h′(xt),1) + (1 −γ)ℓ(h′(xt),0))≤γℓ(h(xt),1) + (1 −γ)ℓ(h(xt),0).
Because we know γ, the Bernoulli parameter for the true distribution of y|xt, we can choose
pt∈∆(H)to put all its mass on the h∗∈ H that minimizes this risk, i.e.
h∗∈arg min
h∈HEy[ℓ(h(xt), y)|xt].
This is, by definition, exactly the h∗such that, for any h∈ H,
γℓ(h∗(xt),1) + (1 −γ)ℓ(h∗(xt),−1)≤γℓ(h(xt),1) + (1 −γ)ℓ(h(xt),−1).
24Therefore, we have that:
vA
t= max
γ∈[0,1]min
pt∈∆(H)max
(h,g)∈H×GEh′∼pt,y∼Ber(γ)[g(xt)(ℓ(h′(xt), y)−ℓ(h(xt), y))]
≤ max
γ∈[0,1],(h,g)∈H×GEy∼Ber(γ)[g(xt)(ℓ(h∗(xt), y)−ℓ(h(xt), y))]
≤0.
The final inequality follows from our argument above.
Next, we bound the expected approximation error we incur from replacing qtwith the empirical
distribution ˜qtobtained from the Msamples {(˜g(i)
t,˜h(i)
t}M
i=1,which we denoted as ϵt(M).This
comes from a standard uniform convergence argument.
Lemma B.6. Lett∈[T]andxt∈ X be fixed, and consider the function ˜ℓxt((g, h),(y′, y)) :=
g(xt)(ℓ(y′, y)−ℓ(h(xt), y)). Let|Y|=k <∞.IfM≥T1+δ, where δ= Ω(log(log T+log k)
logT), then
over the randomness of drawing Msamples {(˜g(i)
t,˜h(i)
t}M
i=1to construct the empirical distribution
˜qtdescribed in Definition B.4, for all y′, y∈ Y, letϵt(M)be defined as the supremum
ϵt(M) := sup
(y′,y)∈Y×YE(˜g,˜h)∼˜qt[˜ℓxt((˜g,˜h),(y′, y))]−E(g,h)∼qt[˜ℓxt((g, h),(y′, y))],
and, over all Trounds,
E"TX
i=1ϵt(M)#
≤2√
T.
Proof. Fix any round t∈[T]. We use a standard uniform convergence argument to ensure that the
empirical distribution ˜qtand the true distribution qtare close for the function ˜ℓxt((g, h),(y′, y))for
ally′, y∈ Y.We assume that Yis finite, and denote k:=|Y|.
Let{(˜g(i)
t,˜h(i)
t)}M
i=1denote the Mrandom samples from G × H .We note that the expectation over
˜qtis the same as:
E(˜g,˜h)∼˜qth
˜ℓ((˜g,˜h),(y′, y))i
=1
MMX
i=1˜ℓ
(˜g(i)
t,˜h(i)
t),(y′, y)
.
Consider the empirical process
sup
(y′,y)∈Y21
MMX
i=1˜ℓ((˜g(i)
t,˜h(i)
t),(y′, y))−E(g,h)∼qth
˜ℓ((g, h),(y′, y))i
| {z }
Zi(y′,y).
For notational simplicity, let us refer to this empirical process as:
sup
(y′,y)∈Y21
MMX
i=1Zi(y′, y)=ϵt(M).
Because ℓ(·,·)∈[0,1], we know ˜ℓ∈[−1,1].Moreover, |Y2|=k2.We can now use Hoeffding’s
inequality and a union bound to obtain:
P"
sup
(y′,y)∈Y21
MMX
i=1Zi(y′, y)≥ε#
≤k2exp(−2Mε2)≤k2exp(−2T1+δε2).
25By the elementary integral inequality E[X]≤R∞
0P[X≥t]dt, we obtain:
E"
sup
(y′,y)∈Y21
MMX
i=1Zi(y′, y)#
≤Z∞
0P"
sup
(y′,y)∈Y21
MMX
i=1Zi(y′, y)≥t#
dt
≤Zw
0P"
sup
(y′,y)∈Y21
MMX
i=1Zi(y′, y)≥t#
dt+Z∞
wk2exp(−2T1+δt2)dt
≤w+Z∞
wk2exp(−2T1+δt2)dt
≤w+k2exp(−2T1+δw2),
where w >0is an arbitrary parameter. Set w=q
1
T. Ifδ≥log(2 log( k)+1
2log(T))
2 logT, then:
E"
sup
(y′,y)∈Y21
MMX
i=1Zi(y′, y)#
=E[ϵt(M)]≤2r
1
T.
Therefore, summing up over all Trounds, we obtainPT
t=1E[ϵt(M)]≤2√
T.
B.6 Instantiating the meta-algorithm for Theorem 4.1
Finally, we instantiate Theorem B.4 with a concrete no-regret algorithm for the (G,H)-player to
obtain Theorem 4.1. We employ the specific no-regret algorithm of [Blo+22] for our (G,H)-player,
restated here for reference.
Theorem B.7 (Smoothed FTPL of [Blo+22]) .LetF:X → [−1,1]be a function class and let ℓbe
a loss function that is L-Lipscchitz in both arguments. Suppose further that we are in the smoothed
online learning setting (see Section 4.1) where each xiare drawn from a distribution that is σ-smooth
with respect ot some base measure BonX.Let
πt,n(f) :=nX
i=1f(zt,i)γt,i√n,
where zt,i∼ B are independent and the γt,iare independent standard Gaussian variables. Suppose
thatα≥0and consider the algorithm which uses an α-approximate oracle for F(see Definition
2.1) to choose ftaccording to
t−1X
s=1ℓ(ft(xs), ys) +ηπt,n(ft)≤inf
f∗∈Ft−1X
s=1ℓ(f∗(xs), ys) +πt,n(f∗) +α,
and let ˆyt=ft(xt).IfFandytare binary valued, with the VC dimension of Fbounded by d≥1,
then for n=T/√σandη=q
Tlog(TL/σ )
σ
E[RegT(ft)]≤C r
dTlogT
σ+αT!
,
where C >0is some absolute constant.
This is precisely what the (G,H)-player does in Algorithm 1. Let G:={g⊆ X :g∈ G} be a
collection of groups, represented as Boolean functions g:X → { 0,1}. LetHbe a hypothesis
class of binary-valued functions h:X → {− 1,1}.To be clear, the we map Theorem B.7 to our
Algorithm 1 in the following way:
• LetFof Theorem B.7 be the class of functions in [−1,1]Xdefined by:
F:={x7→˜g(x)˜h(x) : ˜g∈ G,˜h∈ H} .
Note that each f∈ F maps to {−1,0,1}.
26• Let the loss function in Theorem B.7 be the loss of the (G,H)-player on x∈ X:
˜ℓ((˜g,˜h),(h(x), y)) := ˜ g(x)
ℓ(h(x), y)−ℓ(˜h(x), y)
.
In terms of Fabove, we can rewrite this as:
˜ℓ(f(x),(y′, y)) :=

0 iff(x) = 0
ℓ(y′, y)−ℓ(−1, y)iff(x) =−1
ℓ(y′, y)−ℓ(1, y) iff(x) = 1 .
This loss function has the signature ˜ℓ:{−1,0,1} × {− 1,1}2→[−1,1]because ℓ(·,·)∈[0,1].
It is also 2-Lipschitz in both arguments.
•It remains to make sure that ternary-valued functions taking values in {−1,0,1}do not break the
proof of Theorem B.7. In the proof of Theorem B.7 in [Blo+22], the binary-valued function case
where fhas range {−1,1}is handled by embedding {−1,1}into the real line. There are two
main parts of the proof that rely on this assumption that fhas range {−1,1}that easily maintain
when fhas range {−1,0,1}.
–First, in Lemma 34 of [Blo+22], the authors use L-Lipschitzness and the simple fact that
|f(x)−f′(x)| ≤(f(x)−f′(x))2when f∈ {− 1,1}.This still holds when f∈ {− 1,0,1}.
–Second, [Blo+22] also use the fact that ∥f∥L2= 1 for all f∈ F , which is also true for
f∈ {− 1,0,1}.
Finally, the rest of the proof in Lemma 35 of [Blo+22] relies only on the Lipschitzneess of ˜ℓto
employ standard smoothness arguments and Rademacher contraction, which we have already
established.
•Putting all this together, the (G,H)-player in Algorithm 1 essentially runs the algorithm of B.7,
with the caveat that it calls the OPTα
(G,H)oracle Mtimes to get the empirical approximation ˜qt
of the true implicit distribution qtoverG × H . This implicit distribution is defined by the random
process of drawing the nperturbations and calling the optimization oracle.
Therefore, by Lemmas B.6, B.5, and Theorem B.7 applied to the “meta-theorem” Theorem B.4, we
immediately obtain our main theorem, Theorem 4.1. We now restate Theorem 4.1 as Corollary B.7.1
with the specified choices of parameters.
Corollary B.7.1 (Theorem 4.1, with parameters specified) .LetY={−1,1}be a binary action
space, H ⊆ {− 1,1}Xbe a binary-valued hypothesis class, G ⊆2Xbe a (possibly infinite) collection
of groups, and ℓ:{−1,1}×{− 1,1} →[0,1]be a bounded loss function. Let the VC dimensions of H
andGboth be bounded by d.Letα≥0be the approximation error of the oracle OPTα
(G,H). If we are
in the σ-smooth online learning setting, then, for M≥T1+δ, δ≥log(2 log( k)+1
2log(T))
2 logT, n=T/√σ,
andη=q
Tlog(T/σ)
σ, Algorithm 1 achieves, for each g∈ G:
E[RegT(H, g)]≤r
dTlogT
σ+αT,
where the expectation is over all the randomness of the (G,H)-player’s perturbations and the
H-player’s Bernoulli choices.
C Other instantiations of the meta-algorithm
It is be clear from the statement of Theorem B.4 that our “meta-algorithm” Algorithm 4 straightfor-
wardly applies for other online learning settings as well, so long as we adopt an appropriate strategy
for the (G,H)-player. In this section, we give a few examples of this flexibility for discrete action
spaces in the smoothed online setting and the
C.1 Multi-class action spaces
Instead of |Y|= 2, we can let |Y|=K, more generally. In this case, a straightforward extension
of Theorem B.7 allows us to embed Y ∪ { 0,1}into the real line, and we generalize to considering
27the Natarajan dimension [Nat89] of Hinstead of the VC dimension. Rademacher contraction and
Lipschitzness still apply to ˜ℓ, so with just a difference in the absolute constant, we obtain the following
multi-class analogue of Theorem 4.1 as a corollary. Thus, for the (G,H)-player in Algorithm 1, we
can just use the same exact algorithm outlined in Theorem B.7.
We make a small change to the H-player in Algorithm 1. In the multi-class action space setting, we
need to make Kcalls to the OPT Horacle and solve a K×Ksize linear program for the H-player
at each step. For completeness, we present the algorithm for the K-class action spaces here, as
Algorithm 5.
Algorithm 5 Algorithm for Group Oracle Efficiency (multi-class)
Input: Perturbation strength η >0; number of OPTα
(G,H)callsM∈N.
1:fort= 1,2,3, . . . , T do
2: Receive a context xt∼µtfrom Nature.
3: fori= 1,2,3, . . . , M do
4: (G,H)-player: Draw nhallucinated examples as in Equation (4) to construct πbin
t,n.
5: (G,H)-player: Using the entire history {(ˆys, ys)}t−1
s=1so far, call OPTα
(G,H)to obtain
(˜gt(i),˜h(i)
t)∈ G × H satisfying:
t−1X
s=1˜ℓxs((˜g,˜h),(ˆys, ys)) +πbin
t,n(˜g,˜h, η)
≥ sup
(g∗,h∗)∈G×Ht−1X
s=1˜ℓxs((g∗, h∗),(ˆys, ys)) +πbin
t,n(g∗, h∗, η)−α(19)
6: end for
7:H-player: CallOPT HKtimes on the singleton datasets {(xt, k)}for action k∈[K]with
the 0-1 loss, obtaining:
h′
k∈arg min
h∗∈H1{h∗(xt)̸=k}
8:H-player: Using the Msamples {(˜g(i)
t,˜h(i)
t)}M
i=1, construct the K×Kpayoff matrix ˜L∈
[−1,1]K×Kindexed by (k, y)∈[K]×[K]:
˜Lk,y:=MX
i=1˜ℓxt((˜g(i)
t,˜h(i)
t),(h′
k(xt), y)).
9:H-player: Solve the linear program
min
p∈RK,λ∈Rλ
s.t.p⊤˜Ley≤λ∀y∈[K]
pk≥0∀k∈[K]
X
pi= 1
(where eyis the y-th coordinate vector in RK)
10: Sample k∼p, letht=h′
k.
11: Learner commits to the action ˆyt=ht(xt); Nature reveals yt.
12: Learner incurs the loss ℓ(ˆyt, yt).
13:end for
Theorem C.1. LetY={1, . . . , K }be aK-class action space, H ⊆ YXbe aK-valued hypothesis
class,G ⊆2Xbe a (possibly infinite) collection of groups, and ℓ:Y × Y → [0,1]be a bounded loss
function. Let the Natarajan dimension [Nat89] of Hand the VC dimension of Gboth be bounded by
d.Letα≥0be the approximation error of the oracle OPTα
(G,H). If we are in the σ-smooth online
learning setting, then, for appropriate choices of M∈N, n∈N,andη >0, Algorithm 5 achieves,
28for each g∈ G:
E[RegT(H, g)]≤O r
dTlogT
σ+αT!
,
where the expectation is over all the randomness of the (G,H)-player’s perturbations.
C.2 Group-dependent regret
In this section, we give the details on how to achieve the desired group-dependent regret guarantees
of Section 5. Throughout this section, for any g∈ G, letTg:=PT
t=1g(xt). The results in this
section hinge on the GFTPL with small-loss bound algorithm of [Wan+22]. The main idea will be to
instantiate the (G,H)-player in our meta-algorithm, Algorithm 4 using the GFTPL with small-loss
bound algorithm so Theorem B.4 allows us to directly inherit its regret guarantee. We quote the
algorithm here, adapted to our setting, for reference. Throughout this section, we use the familiar
notation from Section 4.2 of the main body:
˜ℓx((˜g,˜h),(y′, y)) := ˜ g(x)
ℓ(y′, y)−ℓ(˜h(x), y)
, (20)
where ℓ(·,·)is the fixed loss of the entire multi-group online learning setting.
Algorithm 6 GFTPL with small-loss bound
Input: Perturbation matrix Γ∈[−1,1]|G||H|× N
1:Draw i.i.d. vector ν= (ν(1), . . . , ν(N))∼Lap(1)N, i.e., p(ν(i)) =1
2exp(−|ν(i)|).
2:fort= 1,2,3, . . . , T do
3: Setνt←ν
ηtwhere ηt>0is a parameter computed online.
4: Using the entire history up to t−1so far, call OPTα
(G,H)to obtain (˜g,˜h)∈ G × H sastisying:
t−1X
s=1˜ℓxs((˜g,˜h),(ˆys, ys)) +⟨Γ(˜g,˜h), νt⟩
≥ sup
(g∗,h∗)∈G×Ht−1X
s=1˜ℓxs((g∗, h∗),(ˆys, ys)) +⟨Γ(˜g,˜h), νt⟩ −α, (21)
where Γ(˜g,˜h)is the (˜g,˜h)th row of Γ.
5:end for
In our setting, the classical FTPL algorithm of [KV05] draws |G| × |H| independent perturbations
at each round, which requires enumeration of both GandH.In order to remedy this, the GFTPL
algorithm of [Dud+20] uses a |G||H| × Nperturbation matrix Γto generate dependent “shared
randomness.” The transformation Γapplies to a perturbation vector ν∈RN, where Nis much
smaller than |G||H| .Running FTPL with these perturbations, then, results on an oracle-efficient
algorithm. [Wan+22] extends this by showing that, under certain conditions on Γ, this oracle-
efficient algorithm can also achieve a small-loss regret , where the regret diminishes based on the total
magnitude of the losses over the Trounds instead of the number of rounds.
Specifically, a small loss regret looks like the following. It is well-known that, in the worst-case,
a regret of Op
Tlog|G||H|
is minimax optimal [CL06]. However, stronger bounds have been
obtained for problems with “small losses” (see, e.g., [HP05; GSV14; LS15]), where, for a loss
function f: (G × H )× Y → [0,1], one can achieve:
O
vuutTX
t=1f((ht, gt), yt) log|H||G|
,
which is sharper than the Op
Tlog|H||G|
bound when f((ht, gt), yt)<1on some rounds.
We desire this property to achieve a regret bound in the multi-group online learning setting that is
sublinear in terms of the number of rounds a group appears Tg.
29Following [Wan+22], we require the perturbation matrix Γto have two sufficient conditions for
Algorithm 6 to obtain the desired small-loss regret. The first is γ-approximability, which is a condition
that ensures the stability choices of (˜gt,˜ht)and(˜gt+1,˜ht+1)across rounds. In particular, the stability
needed is a bound on the ratio:
P[(˜gt,˜ht) = (g, h)]
P[(˜gt+1,˜ht+1) = (g, h)]≤exp(γηt)
for all (g, h)∈ G × H ,where ηt>0is the per-round leraning rate of GFTPL. By Lemma 2
of [Wan+22], the following condition is sufficient to ensure this property. We restate it here, translated
to our setting.
Definition C.1 (γ-approximability [Wan+22]) .LetΓ∈[−1,1]|G||H|× N, where Nis the dimension
of the noise vector, νin Algorithm 6. Define B1
γ:={s∈RN:∥s∥1≤γ}.Γisγ-approximable if,
for all (g, h)∈ G × H and(x, y′, y)∈ X × Y × Y , there exists s∈RNwith∥s∥1≤γsuch that the
following holds for all (g′, h′)∈ G × H :
⟨Γ(g,h)−Γ(g′,h′), s⟩ ≥˜ℓx((g, h),(y′, y))−˜ℓx((g′, h′),(y′, y)).
The second property is implementability. This property actually allows us to use our optimization
oracle (in our case, OPTα
(G,H)) to access Γwithout explicitly representing it. In essence, it requires
that we can generate a small number of “fake examples” that effectively implement the perturbation
needed by Algorithm 6.
Definition C.2 (Implementability [Dud+20]) .A matrix Γ∈[−1,1]|G||H|× Nisimplementable with
complexity Mif for each j∈[N]there exists a dataset Sjwith|Sj| ≤Msuch that, for all pairs of
rows(g, h),(g′, h′)∈ G × H ,
Γ((g,h),j)−Γ((g′,h′),j)=X
(w,(x,y,y′))∈Sjw
˜ℓx((˜g,˜h),(y′, y))−˜ℓx((˜g′,˜h′),(y′, y))
.
In the above definition, the fake examples are tuples of a context x∈ X and two outcomes y, y′∈ Y.
Finally, with the sufficient conditions of implementability and approximability, we quote the main
regret guarantee of Algorithm 6in [Wan+22] here.
Theorem C.2 (Regret guarantee of 6 [Wan+22]) .Let{1, . . . , K }be the action space of the Learner
and let Zbe the action space of the adversary. Suppose that, at each round, the Learner chooses
action xt∈[K], the adversary chooses action zt∈ Z, and the loss function is f: [K]× Z → [0,1].
LetL∗
T= min k∈[K]PT
t=1f(k, yt). Then, if there exists a γ-approximable matrix Γ, Algorithm 6
instantiated with Γandηt:= min
1
γ,C√
L∗
t−1+1
achieves the following regret bound:
E[RegT]≤ 
4√
2 max{2 logK,√NlogK}
C+ 2γ
C+1
C!
p
L∗
T+ 1
+ 8γlog1
Cp
L∗
T+ 1 + γ
+ 2γ2+ 4√
2 max{2 logK,p
NlogK}γ.
With an appropriate choice of C >0, we may obtain the regret bound:
Op
L∗
Tmaxn
γ,logK,p
NlogKo
(22)
IfΓis also implementable with complexity M, then Algorithm 6 is oracle-efficient, making O(T+
NM)oracle calls per round, where Nis the number of columns of Γ.
Finally, we need one more lemma using a standard uniform convergence argument to bound the
approximation error from sampling with OPTα
(G,H)Mtimes. This is essentially the same as Lemma
B.6, but we obtain a sharper bound on EhPT
t=1ϵt(M)i
at the cost of making polynomially (in T)
more calls to the oracle.
30Lemma C.3. Lett∈[T]andxt∈ X be fixed, and consider the function ˜ℓxt((g, h),(y′, y)) :=
g(xt)(ℓ(y′, y)−ℓ(h(xt), y)). Let|Y|=k <∞.IfM≥T2log(k2T), then over the randomness of
drawing Msamples {(˜g(i)
t,˜h(i)
t}M
i=1to construct the empirical distribution ˜qtdescribed in Definition
B.4, for all y′, y∈ Y, letϵt(M)be defined as the supremum
ϵt(M) := sup
(y′,y)∈Y×YE(˜g,˜h)∼˜qt[˜ℓxt((˜g,˜h),(y′, y))]−E(g,h)∼qt[˜ℓxt((g, h),(y′, y))],
and, over all Trounds,
E"TX
i=1ϵt(M)#
≤2.
Proof. The proof of this lemma follows the proof of Lemma B.6 exactly, except for the choice of M.
Therefore, just using the exact same notation as Lemma B.6, we have:
P"
sup
(y′,y)∈Y21
MMX
i=1Zi(y′, y)≥ε#
≤k2exp(−2Mε2)
By the same exact argument using E[X]≤R∞
0P[X≥t]dt, we obtain
E"
sup
(y′,y)∈Y21
MMX
i=1Zi(y′, y)#
≤w+k2exp(−2Mw2),
where w >0is an arbitrary parameter. Set w=1
T2.Then, if M≥T2log(kT), we get
E[ϵt(M)] =E"
sup
(y′,y)∈Y21
MMX
i=1Zi(y′, y)#
≤2/T.
The lemma follows from summing over T.
Proof of Theorem 5.1. We can now prove Theorem 5.1. The main idea is that the small loss regret
translates directly into a o(Tg)regret due to how we defined our loss function, ˜ℓx, so we simply
instantiate the (G,H)-player in the general algorithm template of Algorithm 4 with Algorithm 6. This
allows us to directly inherit the o(Tg)regret guarantee. We restate it here, with parameters specified,
as Proposition C.3.1.
Proposition C.3.1 (Theorem 5.1, with parameters specified) .Assume H,Gare finite and there exists
aγ-approximable and implementable perturbation matrix Γ∈[−1,1]|G||H|× N. Let|Y|=k.Let
α≥0be the approximation parameter of OPTα
(G,H).Let the no-regret algorithm for the (G,H)-
player in Algorithm 1 be the GFTPL algorithm of [Wan+22] instantiated with Γ, with parameter
M=T2log(k2T). Then, for each g∈ G:
E[RegT(H, g)]≤Op
Tgmaxn
γ,log|H||G| ,p
Nlog|H||G|o
+αT
Proof. Armed with approximability and implementability, we are ready to prove Theorem 5.1. Sup-
pose that there exists a γ-approximable and implementable perturbation matrix Γ∈[−1,1]|G||H|× N.
In the setting of Theorem C.2, we instantiate K=|G||H| ,Z=X × Y × Y , and the loss function
f(·,·)as:
f((g, h),(x, y′, y)) := ˜ℓx((g, h),(y′, y)) =g(x) (ℓ(y′, y)−ℓ(h(x), y)).
31Observe that, with the loss instantiated as ˜ℓx, we have:
L∗
T= min
(g,h)∈G×HTX
t=1˜ℓx((g, h),(xt, y′
t, yt))
≤TX
t=1˜ℓx((g, h),(xt, y′
t, yt)) for all (g, h)∈ G × H
=TX
t=1g(xt)(ℓ(y′
t, yt)−ℓ(h(xt), yt)) for all (g, h)∈ G × H .
≤TX
t=1g(xt) =Tg,
for all g∈ G.The last inequality just comes from the fact that ℓ(·,·)∈[0,1].By directly applying
Theorem C.2, we obtain the regret guarantee for Algorithm 6 of
E[RegT]≤Op
L∗
Tmaxn
γ,log|G||H| ,p
Nlog|G||H|o
≤Op
Tgmaxn
γ,log|G||H| ,p
Nlog|G||H|o
.
However, this is just the regret guarantee of Algorithm 6, not the regret guarantee of our end-to-
end multi-group online learning algorithm, Algorithm 1. We replace the algorithm of [Blo+22] in
Algorithm 1 with Algorithm 6. That is, we use the Algorithm 6 for the (G,H)-player in Algorithm
4; a full description of this substitution is in Algorithm 7. By our meta-theorem, Theorem B.4, this
entire algorithm achieves the multi-group regret guarantee, for all g∈ G:
E[RegT(H, g)]≤TX
t=1vA
t+TX
t=1E[ϵt(M)] +R(T). (23)
≤TX
t=1E[ϵt(M)] +Op
Tgmaxn
γ,log|G||H| ,p
Nlog|G||H|o
. (24)
Equation (24) follows from applying Lemma B.5 and using the regret bound for Algorithm 6
established above. It remains to ensure thatPT
t=1E[ϵt(M)]≤o(Tg).Simply applying Lemma B.6
results inPT
t=1E[ϵt(M)] = 2√
T, which is insufficient for our purposes. Instead, we use Lemma C.3,
which ensures thatPT
t=1E[ϵt(M)]≤O(1)at the cost of increasing Mto be M≥T2log(k2T),
making polynomially more oracle calls to OPTα
(G,H)per-round. This gives us the final regret
guarantee of
E[RegT(H, g)]≤2 +Op
Tgmaxn
γ,log|G||H| ,p
Nlog|G||H|o
,
as desired.
One possible setting in which a Γmatrix is easily constructible is the transductive setting. Here, we
explicitly show how to construct Γto obtain Corollary 5.1.1.
Proof of Corollary 5.1.1. LetX⊆ X be the set the adversary fixes beforehand in the transductive set-
ting, where N:=|X|.We can construct a 1-approximable and implementable Γ∈[−1,1]|G||H|× 4N
by creating a row for each (g, h)∈ G ×H and a column for each (x, y, y′)∈X×Y ×Y , and setting
each entry as
Γ((g,h),(x,y,y′)):=˜ℓx((g, h),(y′, y)).
32Algorithm 7 Algorithm for Group Oracle Efficiency (with GFTPL)
Input: Perturbation matrix Γ∈[−1,1]|G||H|× N; number of OPTα
(G,H)callsM∈N.
1:fort= 1,2,3, . . . , T do
2: Receive a (possibly adversarial) context xt∼µtfrom Nature.
3: fori= 1,2,3, . . . , M do
4: (G,H)-player: Draw i.i.d. vector ν= (ν(1), . . . , ν(N))∼Lap(1)N, i.e., p(ν(i)) =
1
2exp(−|ν(i)|).
5: (G,H)-player: Setνt←ν
ηtwhere ηt:= min
1
γ,C√
L∗
t−1+1
.
6: (G,H)-player: Using the entire history {(ˆys, ys)}t−1
s=1so far, call OPTα
(G,H)to obtain
(˜g,˜h)∈ G × H satisfying:
t−1X
s=1˜ℓxs((˜g,˜h),(ˆys, ys)) +⟨Γ(˜g,˜h), νt⟩
≥ sup
(g∗,h∗)∈G×Ht−1X
s=1˜ℓxs((g∗, h∗),(ˆys, ys)) +⟨Γ(˜g,˜h), νt⟩ −α, (25)
where Γ(˜g,˜h)is the (˜g,˜h)th row of Γ.
7: end for
8:H-player: CallOPT Htwice on the singleton datasets {(xt,−1)}and{(xt,1)}with the 0-1
loss, obtaining:
h′
1∈arg min
h∗∈H1{h∗(xt)̸= 1}h′
−1∈arg min
h∗∈H1{h∗(xt)̸=−1}.
9:H-player: Solve the linear program
min
p,λ∈Rλ
s.t.MX
i=1p˜ℓxt((˜g(i)
t,˜h(i)
t),(h′
1(xt), y)) + (1 −p)˜ℓxt((˜g(i)
t,˜h(i)
t),(h′
−1(xt), y))≤λ∀y∈ {− 1,1}
0≤p≤1.
10: Sample b∼Ber(p)where b∈ {− 1,1}, letht=h′
b.
11: Learner commits to the action ˆyt=ht(xt); Nature reveals yt.
12: Learner incurs the loss ℓ(ˆyt, yt).
13:end for
33NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope?
Answer: [Yes]
Justification: The three main theorems of the paper and their respective problem settings are laid
out in the abstract and Section 1.1, the Summary of Results section, of the introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims made in the
paper.
•The abstract and/or introduction should clearly state the claims made, including the contribu-
tions made in the paper and important assumptions and limitations. A No or NA answer to
this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how much the
results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals are
not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the technical assumptions needed to design oracle-efficient algorithms
in Sections 3, 4, and 5 (respectively, i.i.d. assumption, smooth assumption, and existence of
perturbation matrix assumption). We also note that our work does not address group-dependent
o(Tg)regret for infinite GandH, as mentioned in Section 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that the
paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to vi-
olations of these assumptions (e.g., independence assumptions, noiseless settings, model
well-specification, asymptotic approximations only holding locally). The authors should
reflect on how these assumptions might be violated in practice and what the implications
would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was only
tested on a few datasets or with a few runs. In general, empirical results often depend on
implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low
or images are taken in low lighting. Or a speech-to-text system might not be used reliably to
provide closed captions for online lectures because it fails to handle technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms and how
they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by review-
ers as grounds for rejection, a worse outcome might be that reviewers discover limitations that
aren’t acknowledged in the paper. The authors should use their best judgment and recognize
that individual actions in favor of transparency play an important role in developing norms
that preserve the integrity of the community. Reviewers will be specifically instructed to not
penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and a
complete (and correct) proof?
Answer: [Yes]
34Justification: All theorems are formally stated in the main body and proved in the Appendix. Each
main theorem, Theorem 3.2, Theorem 4.1, and Theorem 5.1, are stated in the main body and
proven in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if they appear
in the supplemental material, the authors are encouraged to provide a short proof sketch to
provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented by
formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclusions of
the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: This is a theory paper, where the main contributions are towards novel algorithmic
design principles for a learning-theoretic model.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived well by
the reviewers: Making the paper reproducible is important, regardless of whether the code and
data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken to
make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways. For
example, if the contribution is a novel architecture, describing the architecture fully might
suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary
to either make it possible for others to replicate the model with the same dataset, or provide
access to the model. In general. releasing code and data is often one good way to accomplish
this, but reproducibility can also be provided via detailed instructions for how to replicate the
results, access to a hosted model (e.g., in the case of a large language model), releasing of a
model checkpoint, or other means that are appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submissions to
provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how to
reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe the
architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should either
be a way to access this model for reproducing the results or a way to reproduce the model
(e.g., with an open-source dataset or instructions for how to construct the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of
closed-source models, it may be that access to the model is limited in some way (e.g.,
to registered users), but it should be possible for other researchers to have some path to
reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [NA]
Justification: This is a theory paper (see above).
35Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be possible,
so “No” is an acceptable answer. Papers cannot be rejected simply for not including code,
unless this is central to the contribution (e.g., for a new open-source benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how to
access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state
which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized versions
(if applicable).
•Providing as much information as possible in supplemental material (appended to the paper)
is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [NA]
Justification: This is a theory paper (see above).
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: This is a theory paper (see above).
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main
claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for example,
train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
•The method for calculating the error bars should be explained (closed form formula, call to a
library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error of the
mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should preferably
report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality
of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or figures
symmetric error bars that would yield results that are out of range (e.g. negative error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how they
were calculated and reference the corresponding figures or tables in the text.
368.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experi-
ments?
Answer: [NA]
Justification: This is a theory paper (see above).
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual experi-
mental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute than the
experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it
into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS
Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: This paper mainly focuses on a theoretical problem that does not involve the use
of any real-world datasets. Insofar as the proofs go, we have attempted to make them clear and
easily checkable.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consideration
due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative societal
impacts of the work performed?
Answer: [NA]
Justification: The main contribution of this work is towards algorithmic techniques for a learning
theory problem. We see the societal impacts of such techniques as minimal.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal impact or
why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
ment of technologies that could make decisions that unfairly impact specific groups), privacy
considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied to
particular applications, let alone deployments. However, if there is a direct path to any
negative applications, the authors should point it out. For example, it is legitimate to point out
that an improvement in the quality of generative models could be used to generate deepfakes
for disinformation. On the other hand, it is not needed to point out that a generic algorithm
for optimizing neural networks could enable people to train models that generate Deepfakes
faster.
•The authors should consider possible harms that could arise when the technology is being
used as intended and functioning correctly, harms that could arise when the technology is
being used as intended but gives incorrect results, and harms following from (intentional or
unintentional) misuse of the technology.
37•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms
for monitoring misuse, mechanisms to monitor how a system learns from feedback over time,
improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible release
of data or models that have a high risk for misuse (e.g., pretrained language models, image
generators, or scraped datasets)?
Answer: [NA]
Justification: Our paper does not include any trained models or data.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere
to usage guidelines or restrictions to access the model or implementing safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do not
require this, but we encourage authors to take this into account and make a best faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the
paper, properly credited and are the license and terms of use explicitly mentioned and properly
respected?
Answer: [NA]
Justification: Our paper does not use any licensed code, data, or models.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of service of
that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has curated
licenses for some datasets. Their licensing guide can help determine the license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of the
derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to the asset’s
creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Our paper does not use any new assets (no experiments).
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their sub-
missions via structured templates. This includes details about training, license, limitations,
etc.
•The paper should discuss whether and how consent was obtained from people whose asset is
used.
•At submission time, remember to anonymize your assets (if applicable). You can either create
an anonymized URL or include an anonymized zip file.
3814.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as well as
details about compensation (if any)?
Answer: [NA]
Justification: Our paper does not conduct any experiments on human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
•Including this information in the supplemental material is fine, but if the main contribution of
the paper involves human subjects, then as much detail as possible should be included in the
main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or
other labor should be paid at least the minimum wage in the country of the data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals
(or an equivalent approval/review based on the requirements of your country or institution) were
obtained?
Answer: [NA]
Justification: Our paper does not conduct any experiments on human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent) may
be required for any human subjects research. If you obtained IRB approval, you should clearly
state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions and
locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines
for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
39