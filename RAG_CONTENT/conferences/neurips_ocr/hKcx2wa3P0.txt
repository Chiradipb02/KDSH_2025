On the Target-kernel Alignment: a Unified Analysis
with Kernel Complexity
Chao Wang†, Xin He†♯∗, Yuwen Wang‡∗, Junhui Wang‡
†School of Statistics and Management, Shanghai University of Finance and Economics
♯Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, Shanghai
‡Chinese University of Hong Kong
wang.chao@stu.sufe.edu.cn, he.xin17@mail.shufe.edu.cn
wangyw@link.cuhk.edu.hk, junhuiwang@cuhk.edu.hk
Abstract
This paper investigates the impact of alignment between the target function of
interest and the kernel matrix on a variety of kernel-based methods based on a
general loss belonging to a rich loss function family, which covers many commonly
used methods in regression and classification problems. We consider the truncated
kernel-based method (TKM) which is estimated within a reduced function space
constructed by using the spectral truncation of the kernel matrix and compare
its theoretical behavior to that of the standard kernel-based method (KM) under
various settings. By using the kernel complexity function that quantifies the
complexity of the induced function space, we derive the upper bounds for both
TKM and KM, and further reveal their dependencies on the degree of target-kernel
alignment. Specifically, for the alignment with polynomial decay, the established
results indicate that under the just-aligned and weakly-aligned regimes, TKM and
KM share the same learning rate. Yet, under the strongly-aligned regime, KM
suffers the saturation effect, while TKM can be continuously improved as the
alignment becomes stronger. This further implies that TKM has a strong ability
to capture the strong alignment and provide a theoretically guaranteed solution
to eliminate the phenomena of saturation effect. The minimax lower bound is
also established for the squared loss to confirm the optimality of TKM. Extensive
numerical experiments further support our theoretical findings. The Python code
for reproducing the numerical experiments is available on Github.
1 Introduction
Kernel-based methods have attracted increasing attention in recent years (Belkin et al., 2018; Liang
& Rakhlin, 2020; Ghorbani et al., 2020; Li et al., 2023), due to its close connection with some
cutting-edge research topics, including the understanding of over-parameterized neural network
through the neural tangent kernel (Jacot et al., 2018; Chizat et al., 2019) and large-scale kernel
learning with gradient descent (Lin & Zhou, 2018; Xu et al., 2021). It is of fundamental importance
to provide theoretical explanations of their behaviors under these research topics.
In literature, some recent works show that the learning rate of kernel-based methods is actually
affected by both the model complexity of the considered reproducing kernel Hilbert space (RKHS)
and the target-kernel alignment, a measure to quantify the similarity between the considered RKHS
(or kernel matrix from the empirical point of view) and the target function, which is also known
as the smoothness of a target function in the RKHS (Caponnetto & De Vito, 2007; Smale & Zhou,
∗Xin He and Yuwen Wang are the corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).2007). Particularly, the existing learning rate for the kernel ridge regression (KRR) is proved to be
O(n−2αγ
2αγ+1)for1
2≤γ≤1, where γis known as the source condition parameter (Cui et al., 2021)
and can be treated as a measure of target-kernel alignment at the population level, and αcontrols
the model complexity of the RKHS. This rate aligns with the intuitive sense that strong alignment
and small model complexity contribute to a faster learning rate. Yet, with the increasing in γsuch
thatγ >1which corresponds to a smoother target function, the best learning rate of KRR plateaus
atO(n−2α
2α+1)(Caponnetto & De Vito, 2007). This phenomenon is known as the saturation effect
(Bauer et al., 2007; Li et al., 2022) and is widely observed in many applications (Bauer et al., 2007;
Gerfo et al., 2008). It has been conjectured for decades that no matter what carefully chosen tuning
parameter, the learning rate of KRR plateaus when the smoothness of the target function exceeds
certain levels. Most recently, Li et al. (2022) establishes the rigorous saturation lower bound of KRR
that confirms practical conjecture. Amini (2021); Amini et al. (2022) propose a truncated KRR based
on the spectral-truncated kernel matrix, and further prove that as the alignment becomes stronger,
the truncated KRR can be consistently improved in terms of the expected mean squared error and
eventually tends to the parametric rate. Clearly, this improvement effectively tackles the saturation
effect for the KRR where the squared loss is specified. Yet, it is still unclear whether the saturation
effect can be solved for the general kernel-based methods where the specified loss function belongs
to a rich loss function family.
In this paper, motivated by this theoretical gap, we investigate the impact of target-kernel alignment
through kernel complexity from the kernel complexity perspective for various kernel-based methods
by considering a general loss function which belongs to a rich loss function family. Our established
results shed light on the statistical benefits of the truncated estimator and are also verified by extensive
numerical experiments. We want to emphasize that in contrast to the existing works that focus on
the KRR benefit from the analytical solution and thus their theoretical analysis heavily relies on the
closed form of the solution to establish some critical results (Cui et al., 2021; Amini et al., 2022), the
explicit solution does not exist anymore in this paper, which requires different technical treatments to
conduct the theoretical analysis. Specifically, our theoretical analysis crucially relies on the kernel
complexity which quantifies the complexity of the RKHS (Bousquet & Herrmann, 2002) and some
empirical process techniques. The established results successfully capture the trade-off between
the complexity of the truncated RKHS and approximation bias as presented in Theorem 4.2. A
simpler bound when considering the polynomial case in Corollary 4.3 further indicates that with a
carefully chosen truncated space, the truncated method can efficiently eliminate the saturation effect.
More importantly, we establish the minimax lower bound when the squared loss is specified, and
thus rigorously confirm the conjecture in Amini et al. (2022), stating that the truncated KRR attains
minimax optimality.
1.1 Contributions
The main contribution of this paper is to offer a unified analysis and a comprehensive understanding
of the impact of target-kernel alignment, and provide a theoretically guaranteed solution to eliminate
the phenomena of saturation effect. Some of its contributions are listed as follows.
(i) By leveraging the kernel complexity function, we establish the upper bounds for both the standard
kernel-based estimator and the truncated estimator for a general loss function belonging to a family
of Lipschitz loss functions. The established bounds indicate that with the variation of the alignment
level, the learning rates for these two estimators exhibit distinct trajectories. Specifically, under the
polynomial decay assumption, when alignment is at a lower level, the standard kernel-based estimator
and the truncated estimator share the same learning efficiency and improve with the rise in alignment
level. Yet, when the alignment level surpasses a threshold ( γ= 1in Assumption 3.2), the learning rate
of the kernel-based estimator plateaus with no improvement as γincreases — a phenomenon known
as the saturation effect in literature. As opposed, the learning rate of the truncated estimator exhibits
continuous improvement with the increasing alignment level, thus eliminating the saturation effect.
This indicates a significant improvement in the truncated estimator over the standard kernel-based
estimator.
(ii) By employing the standard Fano method, we establish minimax lower bound when the squared loss
is specified, indicating that for both the just-aligned and weakly-aligned regimes, both the standard
kernel-based estimator and the truncated estimator achieve minimax optimality. Furthermore, for the
strong-aligned regime, we demonstrate that the standard kernel-based estimator can only attain sub-
2optimality, while the truncated estimator is also minimax-optimal. Our minimax analysis significantly
extends the existing results presented in Yang et al. (2017), offering a unified perspective for realistic
scenarios where the true target is assumed to reside in the RKHS.
(iii) Various numerical experiments are conducted in the context of various regression and classifica-
tion problems to demonstrate the advantages of the truncated estimator, to support the established
theory substantially. More interestingly, we also empirically verify the existence of a trade-off arising
from the model complexity of the RKHS and target-kernel alignment.
1.2 Related Work
Kernel-based methods have been widely studied for the past few decades, and are known as the
time-proven efficient tools for statistical analysis. The theoretical behaviors of the kernel-based
estimator have been established in Caponnetto & De Vito (2007); Li et al. (2007); Smale & Zhou
(2007); Patle & Chouhan (2013). The concept of target-kernel alignment has been introduced in
Cristianini et al. (2001), where a classification algorithm is developed adapting to the target-kernel
alignment with a significant improvement in classification accuracy. Follow-up works have expanded
the concept of target-kernel alignment to some other learning tasks, including regression (Kandola
et al., 2002) and multi-class classification (Guermeur et al., 2004). The implications of target-kernel
alignment have also inspired some applications to spectral kernel learning (Hoi et al., 2006), and
feature selection (Wong & Burkowski, 2011).
Most recently, many works have attempted to provide a theoretical understanding of the kernel-based
method by considering the target-kernel alignment. Specifically, Canatar et al. (2021) investigates
the generalization error of KRR and derives an analytical framework for the generalization error
that captures the effect of the target-kernel alignment. Amini (2021) considers a spectrally truncated
KRR and demonstrates that with a carefully chosen truncation, the truncated KRR outperforms the
standard KRR. Li et al. (2022) verifies the saturation effect observed behind the KRR estimator by
establishing a lower bound that O(n−2α
2+α)whatever the smoothness degree of the target function
is. Motivated by these works, Amini et al. (2022) further demonstrates the non-monotonicity of the
regularization curve for the bandlimited alignment setting and further reveals that the learning rate of
the truncated KRR can be consistently enhanced as the degree of target-kernel alignment increases.
2 Preliminaries
Background on RKHS. LetHKdenote the reproducing kernel Hilbert space (RKHS) induced by a
positive semi-definite kernel function K(·,·) :X × X → R+, where X ⊂ Rp. The inner product
equipped with HKis denoted as ⟨·,·⟩Kwith the endowed norm ∥ · ∥2
K=⟨·,·⟩K. For each x∈ X, it
is well-known that Kx:=K(x,·)∈ H Kand the reproducing property holds that ⟨f, Kx⟩K=f(x)
for all f∈ H K. We assume that supx,x′∈XK(x,x′)≤κ2for some positive constant κ. This
condition is commonly assumed in literature and various popularly used kernel functions satisfy this
condition, including the Gaussian kernel and Laplacian kernel.
Problem setup. We consider a collection of pairs {(xi, Yi)}n
i=1where {xi}n
i=1is a collection of
covariates and the response Yiis independently drawn from a conditional distribution PY|xion
Y ⊂ R . Throughout this paper, we focus on the fixed design setting, where {xi}n
i=1are fixed,
otherwise we treat all the random quantities as conditioning on {xi}n
i=1. A similar treatment also
appears in Yang et al. (2017); Wei et al. (2017); Amini et al. (2022).
In the literature of machine learning, the learning task is often defined with some pre-specified
loss function. Specifically, we consider a loss function L(·,·) :R × R → R+, where L(y, f(x))
quantifies the inaccuracy for predictor f(x)when yis the true response. Then, the population risk
function can be defined as
E(f) :=EYnh1
nnX
i=1L 
Yi, f(xi))i
,
where EYndenotes the expectation taken over Y1, ..., Y n. In literature, the target function of interest
in the learning task is typically defined as the minimizer of the population risk f∗= argminfE(f).
In this paper, we assume f∗∈ H Kand consider a family of loss functions that Lis assumed to
be convex and locally Lipschitz continuous in the second argument (Wainwright, 2019; Dasgupta
3et al., 2019). Here, locally Lipschitz continuity is specified as that for any b >0, there exists some
positive constant ML,b2such that for any y∈ Y andx∈ X , and for any f, f′∈ H Ksatisfying
max{∥f∥K,∥f′∥K} ≤b, the following inequality holds:
|L(y, f(x))−L(y, f′(x))| ≤ML,b|f(x)−f′(x)|.
It is worth pointing out that the local Lipschitz continuity is satisfied for a variety of commonly used
loss functions, and some of them are listed in Table 1.
Table 1: Different losses with corresponding Lipschitz constant ML,b
Loss Squared Exponential Check Hinge Huber Logistic
ML,b 2U+ 2bκ31 1 1 τ41
Note that the choice of the loss function is task-specific based on the problem of interest and prior
knowledge on the data. For instance, under the regression setting, the squared loss can be specified for
mean regression and the check loss can be specified for quantile regression. Under the classification
setting, the hinge loss can be specified for margin-based classification and the logistic loss can be
specified for estimating the conditional probability.
3 Standard Kernel-based Method
In the rest of this paper, we use lowercase letters {yi}n
i=1to denote the observations of {Yi}n
i=1,
and denote the empirical measure of {x1, ...,xn}byPn. Given an estimator bf, its accuracy can be
evaluated by the L(Pn)-error which is defined as ∥bf−f∗∥2
n=1
nPn
i=1(bf(xi)−f∗(xi))2. We also
use the excess risk that E(bf)− E(f∗)as an evaluation measure. To estimate the underlying target
function f∗, we consider the following penalized empirical risk minimization problem that
bfλ= argmin
f∈HKbE(f) +λ∥f∥2
K	
, (1)
wherebE(f) =1
nPn
i=1L(yi, f(xi))andλis regularization parameter. We define a sample operator
Sx:HK→ RnasSx(f) :=1√n(f(x1), ..., f (xn))⊤,and its adjoint operator S⊤
x:Rn→ H Kis
defined as
S⊤
x(α) :=1√nnX
j=1αjK(·,xj),α= (α1, ..., α n)⊤∈ Rn.
Then, by the representer theorem (Kimeldorf & Wahba, 1971), the minimizer of the learning task (1)
must have a finite form that bfλ=S⊤
x(bα)wherebα∈ Rnis the solution to the following optimization
task
bα= argmin
α∈RnbE(S⊤
x(α)) +λα⊤Kα	
.
LetK=1
nK(xi,xj)	n
i,j=1be the empirical kernel matrix where the scaling is for analytical
simplicity. In the subsequent analysis, we further assume that Kis positive which is also required
in literature (Liang & Rakhlin, 2020; Amini et al., 2022). Then, the kernel matrix Kadmits an
eigen-decomposition that K=U D U⊤, where U= (u1, ...,un)∈ Rn×nis an orthonormal matrix
andD∈ Rn×nis a diagonal matrix with positive elements µ1, ..., µ narranging in a descending
ordering. Without of loss generality, we further require that µj→0asj→ ∞ .
Letξ∗=U⊤Sx(f∗). The elements of the vector ξ∗are referred to as target alignment (TA) scores
(Amini et al., 2022), which quantify the agreement level between f∗andK. Intuitively, a more
2ML,bis a constant with possible dependence on b.
3For squared loss, we assume that Y ⊂[−U, U], which is commonly adopted in literature of learning theory
(Bartlett et al., 2005; Smale & Zhou, 2005, 2007; Wei et al., 2017).
4τis the threshold parameter for Huber loss.
4favorable learning rate can be achieved if the target and kernel are strongly aligned corresponding
to fast decay TA scores. For example, the scenario that Sx(f∗)is predominantly situated in the
space generated by the eigenvectors corresponding to the first several eigenvalues of Kindicates a
strong alignment. In other words, ξ∗
jis expected to be as large as possible for small jand as small as
possible for large j. An ideal scenario is that Sx(f∗)exactly matches the eigenvector u1, leading
toξ∗= (1,0, ...,0)⊤with proper scaling such that ∥f∗∥2
n= 1. In this paper, we are devoted to
providing an analytic framework for the impact of alignment on the performance of the kernel-based
methods based on the kernel complexity of K.
3.1 Technical Assumptions
The following necessary assumption is needed in our theoretical analysis.
Assumption 3.1. There exist two constants 0< c0≤c′
0such that
c0∥f−f∗∥2
n≤ E(f)− E(f∗)≤c′
0∥f−f∗∥2
n,
for any f∈ H Ksatisfying ∥f−f∗∥2
n≤bwith some sufficiently small constant b >0.
The first inequality in Assumption 3.1 is a c0-locally strong convexity condition, and the second
inequality is a c′
0-local smooth condition of the loss function with respect to ∥ · ∥ n. Assumption
3.1 is commonly assumed in literature (Steinwart & Christmann, 2008; Wei et al., 2017; Li et al.,
2019; Farrell et al., 2021). Due to space limit, some brief discussions are provided below. For the
squared loss, Assumption 3.1 is satisfied with c0=c′
0= 1. For the check loss, the c0-locally strong
convexity condition is slightly more relaxing than the similar assumption in the literature (Lian, 2022)
that requires the conditional density function of the noise term to be uniformly bounded away from
zero. And, the c′
0-local smoothness condition holds if the conditional density function is uniformly
bounded. Other widely used loss functions including Huber loss, Logistic loss, Hinge loss, and
exponential loss also satisfy Assumption 3.1 under some mild conditions as discussed in Appendix F.
Assumption 3.2. There exist some constants γ≥1
2andu≥2such thatPn
j=1ξ∗
j2µj−2γ≤u2for
anyn.
Assumption 3.2 imposes the regularization on the TA scores ξ∗concerning K. Note that the parameter
γreflects the degree of target-kernel alignment, where a larger γindicates stronger alignment between
Kandf∗. Moreover, the parameter γin Assumption 3.2 can be considered analogous to the source
condition parameter under the random design setting (Caponnetto & De Vito, 2007; Cui et al., 2021;
Li et al., 2023). Further discussions on the extension to the random setting are deferred to Appendix
B. In our subsequent analysis, we consider the following three cases that
• Just-aligned regime: γ=1
2where we only assume f∗∈ H K.
• Weakly-aligned regime:1
2< γ≤1where f∗∈ H Kand is more aligned with K.
• Over-aligned regime: γ >1where f∗∈ H Kand has a strong alignment with K.
3.2 The Upper Bound for Standard Kernel-based Method
In the rest of this paper, we use Cto denote some constant independent of n, γ, α , which may hide
the constants such as u, c0, c′
0and whose values may vary from line to line. In literature, the empirical
kernel complexity function is defined as R(δ) :=q
1
nPn
j=1min{δ2, µj}(Bartlett et al., 2005). R(δ)
serves as a measure of complexity of HKand is closely connected to local Rademacher complexity
(Bartlett et al., 2005; Steinwart et al., 2009). It plays a crucial role in establishing our theoretical
results via the critical radius δn, defined as the smallest positive value δsuch that
Clogι−1R(δ)≤c0
2δ2η+1, (2)
where η= min {γ,1}andιis specified in the subsequent theorems and corollaries. The learning
rate of the kernel-based estimator defined in (1)highly depends on δn, and the existence and
uniqueness of δnare verified in Appendix B.1. As pointed out in Yang et al. (2017), the statistical
dimension is defined as the first index for which the associated eigenvalue µjdrops below δ2that
d(δ) := min {j∈[n] :µj≤δ2}, where [n] ={1,2, .., n}andd(δ) :=nif{j∈[n] :µj≤δ2}=∅.
5Note that the statistical dimension serves as a measure of the intrinsic dimension of the kernel matrix
K. Moreover, a kernel is regular if the tail sum of its eigenvalue sequence can be well bounded as
the formPn
d(δ)+1µj≲d(δ)δ2(Yang et al., 2017). Note that kernels in the kernel class with the
polynomial or exponential decay in their eigenvalues are regular. Then, the kernel complexity can be
well approximated byp
d(δ)δ2/n. The close connection between d(δ)andR(δ)enables us to find
the explicit formulation of δnin our theoretical analysis.
The following theorem provides theoretical guarantees of bfλdefined in (1)in terms of L(Pn)-error
and the excess risk which hold with high probability.
Theorem 3.3. Suppose that Assumptions 3.1 and 3.2 are satisfied and δ2
n≤λ≤15. Let η=
min{γ,1}. Then, for any ι∈(0,1), with probability at least 1−ι, there holds
max
∥bfλ−f∗∥2
n,E(bfλ)− E(f∗)	
≤C(δ4η
n+λ2η).
The proof of Theorem 3.3 is provided in Appendix C. To complete the proof of Theorem 3.3, we
only need to require the first inequality in Assumption 3.1. Note that the established bound for bfλ
consists of two terms that are related to the critical radius δnand the parameter λ. Compared to
the existing works (Yang et al., 2017; Amini et al., 2022) under the fixed setting where only the
squared loss is considered, Theorem 3.3 provides a comprehensive theoretical analysis on various
kernel-based estimators by considering a general loss function with the help of kernel complexity and
also considers the effect of the target-kernel alignment on the estimation performance under different
aligned regimes. Moreover, we notice that with the choice of λsatisfying λ≍δ2
n, an optimal rate
can be achieved that
E(bfλ)− E(f∗)≍ ∥bfλ−f∗∥2
n≍δ4η
n.
Note that Amini et al. (2022) provides some valuable insights into the learning rate of the kernel-based
estimator under the squared loss in terms of the expected L(Pn)-error where the following polynomial
decay condition is required.
Assumption 3.4. There exist some constants α >1andγ≥1
2such that the eigenvalues of Kand
the TA scores exhibit polynomial decay rate that
µj≍j−αand ξ∗
j2≍j−2γα−1.
In Assumption 3.4, the parameter αcontrols the complexity of HKin the sense that a decreasing α
results in the increasing compacity of the RKHS HK(Cui et al., 2021; Amini et al., 2022). Various
widely used kernels, including the Sobolev kernel and the Laplacian kernel, belong to this class. Note
that with slight modification by setting ξ∗
j2≍j−2γα−1(logj)−2, it can be verified that Assumption
3.4 directly leads to Assumption 3.2 if we ignore the logarithmic term.
By Assumption 3.4, it is clear that d(δ)≍δ−2/α, and consequently δ2
n≍ (logι−1)2
nα
2ηα+1. To
better understand the established results in Theorem 3.3, the following corollary is also provided.
Corollary 3.5. Suppose that Assumptions 3.1, 3.2 and 3.4 are satisfied. Let η= min {γ,1}. For any
ι∈(0,1), if we choose λ≍ (logι−1)2
nα
2ηα+1, then with probability at least 1−ι, there holds
∥bfλ−f∗∥2
n≍ E(bfλ)− E(f∗)≤C(logι−1)2
n2ηα
2ηα+1.
Under the just-aligned regime that γ=1
2, the learning rate turns to be 
(logι−1)2/nα
α+1, which
is consistent with that in literature (Wei et al., 2017) where merely assumes f∗∈ H K. Yet, under
the weakly-aligned regime that1
2< γ≤1, the learning rate exceeds 
(logι−1)2/nα
α+1due to the
stronger target-kernel alignment. More interestingly, under the over-aligned regime that γ >1, the
learning rate plateaus with no improvement as γincreases, which indicates a saturation effect for
the standard kernel-based method. It is also interesting to notice that no matter how the choice of
λ, the learning rate is always lower bounded by O(n−2α
2α+1)forγ≥1(Li et al., 2022). In the next
section, we will show that a careful choice of truncation allows us to construct an estimator based on
a reduced RKHS that achieves the best rate and mitigates the saturation effect for γ >1.
5Note that we assume λ≤1as the theoretical choice of λtypically depends on nand is close to zero for
sufficiently large n.
64 Truncated Kernel-based Method
To construct the reduced RKHS, we introduce a collection of functions {ψk}k∈[n]⊂ H K, defined as
ψk:= argmin
∥ψ∥K:ψ∈ H K, Sx(ψ) =uk	
.It can be verified that {ψk}k∈[n]is unique and by
the orthogonality of u1, ...,un, we also have ⟨ψi, ψj⟩n= 1fori=jand0otherwise (Amini et al.,
2022). Then, for a given r, we define a function space as
HKr:=nrX
k=1αkψk:α= (α1, ..., α r)⊤∈ Rro
.
LetHKrbe equipped with the norm ∥f∥2
Kr=⟨f, f⟩Kr, where the inner product is defined as
⟨f, g⟩Kr:=Pr
k=1αkβk/µkforf=Pr
i=1αkψkandg=Pr
i=1βkψk. The following lemma from
Amini et al. (2022) indicates that HKris also an RKHS associated with a different kernel function.
Lemma 4.1. HKr⊂ H Kis an r-dimensional RKHS with reproducing kernel Kr(x,x′) =Pr
k=1µkψk(x)ψk(x′).
Clearly, HKrcan be treated as a relatively smaller function space compared to the full RKHS HK.
Based on HKr, we can find a truncated kernel-based estimator by solving
bfλ,r= argmin
f∈HKrbE(f) +λ∥f∥2
Kr	
.
For the truncated RKHS HKr, we also define the operator S⊤
x,r:Rn→ H Kras
S⊤
x,r(α) :=1√nnX
j=1αjKr(·,xj),α= (α1, ..., α n)⊤∈ Rn.
Then, by the representer theorem (Kimeldorf & Wahba, 1971) again, bfλ,ralso has a finite solution
thatbfλ,r=S⊤
x,r(bαr)andbαrcan be obtained by solving the following optimization task
bαr= argmin
α∈RnbE(S⊤
x,r(α)) +λα⊤Krα	
.
where Kr=1
nKr(xi,xj)	n
i,j=1is the empirical kernel matrix w.r.t. Kr. Note that the truncated
method does not impose an additional computational cost compared to the standard kernel method,
and its detailed discussion will be provided in Appendix B.5. By the construction of {ψi}i∈[n], it is
easy to verify that Kr=U D rU⊤, where Dris diagonal matrix with elements µ1, ..., µ r,0, ...,0,
detailed proof can be seen in Appendix B.2. This further implies that Kr=Kwhen r=n, and thus
leads to bfλ(xi) =bfλ,n(xi)for each i∈[n].
4.1 The Upper Bound for Truncated Kernel-based Method
Given the truncated RKHS HKr, our theoretical results below rely on the truncated kernel complexity
function, defined as Rr(δ) :=q
1
nPr
j=1min{δ2, µj}. Moreover, the critical radius δn,rcan be
defined as the smallest positive value δsuch that
Clogι−1Rr(δ)≤c0
2δ2η+1. (3)
The existence and uniqueness of δn,rare verified in Appendix B.1. It can be verified that Rr(δ)≤
R(δ)and thus leads to δn,r≤δn. This observation indicates a potential improvement of the
truncated estimator bfλ,rand is the core of our theoretical analysis. The following theorem shows
thatbfλ,rconverges to the underlying target in terms of the L(Pn)-error and the excess risk with high
probability.
Theorem 4.2. Suppose that Assumptions 3.1 and 3.2 are satisfied and max{δ2
n,r,Pn
j=r+1ξ∗
j2} ≤
λ≤1. Letη= min {γ,1}. Then, for any ι∈(0,1), with probability at least 1−ι, there holds
max
∥bfλ,r−f∗∥2
n,E(bfλ,r)− E(f∗)	
≤C 
δ4η
n,r+λ2η+nX
j=r+1ξ∗
j2
.
7The proof of Theorem 4.2 is provided in Appendix D. The established upper bound of bfλ,rfirst
decomposes the total error into two components: estimation error (the first two terms), which is
controlled by complexity of the reduced RKHS HKr, and approximation bias (the last term), which
results from the dissimilarity between the truncated RKHS HKrand the full RKHS HKwhere f∗
belongs to. Specifically, a smaller value of rreduces the complexity of HKr, possibly leading to a
more favorable estimation error. Yet, it amplifies the gap between HKrandHKand may lead to an
extra approximation bias which may be significantly large. Clearly, rcan be regarded as a trade-off
parameter that balances the approximation biasPn
j=r+1ξ∗
j2and the estimation error δ4η
n,r+λ2η. It
is clear that if r=n, the approximation bias is zero and the upper bound of bfλ,nrecovers that of
bfλ. This implies that with careful choice of r, the truncated method at least performs as well as the
standard estimator. If Assumption 3.4 also holds, we can conclude thatPn
j=r+1ξ∗
j2≲r−2γαI{r<n}.
Then, the best choice of randλto achieve an optimal rate is given by r≍δ−2/(γα)
n,r ifγ >1;r=n
if1
2≤γ≤1, and λ≍δ2
n,r. Accordingly, there holds
E(bfλ,r)− E(f∗)≍ ∥bfλ,r−f∗∥2
n≍δ4η
n,r.
For the comparison of Corollary 3.5, we also establish the following corollary for bfλ,r.
Corollary 4.3. Suppose that Assumptions 3.1, 3.2 and 3.4 are satisfied. For any ι∈(0,1), if
we choose λ≍ (logι−1)2
nmax{γ,1}α
2γα+1andr≍(n
(logι−1)2)1
2γα+1I{γ>1}+nI{1
2≤γ≤1}, then with
probability at least 1−ι, there holds
E(bfλ,r)− E(f∗)≍ ∥bfλ,r−f∗∥2
n≤C(logι−1)2
n2γα
2γα+1.
Clearly, under the over-aligned regime that γ >1, the truncated estimator bfλ,rcan achieve a faster
learning rate compared to bfλ; for1
2≤γ≤1, the trivial choice of r=nis optimal and bfλ,rshares the
same learning rate as bfλ. More impressively, the learning rate of bfλ,rcan be continuously increased
with the enhancement of the target-kernel alignment, thus eliminating the saturation effect. Note
that as γ→ ∞ , the learning rate of bfλ,rapproaches1
n, meaning that the truncated estimator can
successfully capture the strong alignment to attain a comparable rate to the parametric rate.
The connection between randd(δ).Recall that for the regular kernel class, we have R(δ)≍p
n−1d(δ)δ2. It can also be shown by simple algebra that Rr(δ)≍p
n−1min{r, d(δ)}δ2(See
Appendix D.3 for details). Particularly, for the kernel class with polynomial decay, we have d(δ)≍
δ−2/α. Once the critical radius δn,ris determined for specified kernel matrix, we denote dn=
d(δn,r)≍δ−2/α
n,r and take r≍δ−2η/(γα)
n,r to balance δ4η
n,randr−2γα. Consequently, we obtain
r≍dη/γ
n. Such a relation between randdnis very reflective and provides theoretical insight into
why the truncated estimator is more efficient under a more aligned situation. Specifically, for the case
γ >1, it is clear that r≍d1/γ
n< dn, and we have
Rr(δn,r)≍q
n−1rδ2n,r<q
n−1dnδ2n,r≍R(δn,r).
As a result, the truncated kernel complexity is substantially reduced compared to R(δn,r), leading
to an improved learning rate. On the contrary, for the case that1
2≤γ≤1, we have r≍dn
andRr(δn,r)≍q
n−1dnδ2n,r≍R(δn,r), which indicates the truncated kernel complexity remains
invariant as rdecreases. To avoid introducing additional approximation bias, the best choice of
truncation level turns out to be r=n.
4.2 Minimax Lower Bound
In this section, we establish the minimax lower bound under squared loss based on the Fano method
(see Chapter 15 in Wainwright (2019) for more details). For γ≥1
2, we consider the space within a
ball as Hb
K=
f∈ H K:Pn
j=1ξj2µj−2γ≤u2	
, where ξj’s are the TA scores associated with f.
8Theorem 4.4. Suppose that the RKHS is induced by the regular kernel, and efis any estimator based
on the data {(xi, yi)}n
i=1. If1
2≤γ≤1, we have
inf
efsup
f∗∈Hb
KP 
∥ef−f∗∥2
n≥cδ4γ
n
≥1
2.
Ifγ >1, with the choice of rsatisfying r≍d(δ1/γ
n,r)≤d(δn,r), we have
inf
efsup
f∗∈Hb
KP 
∥ef−f∗∥2
n≥cδ4
n,r
≥1
2,
where c >0is a constant independent of nandγ.
The proof of Theorem 4.4 is provided in Appendix E. For γ >1, the condition d(δ1/γ
n,r)≤d(δn,r)
can be easily verified for the most popular polynomial decay case that µj≍j−α. Specifically, for
the kernel class with polynomial decay, we have d(δ)≍δ−2/α, which leads to
d(δ1/γ
n,r)≍δ−2/α
n,r≤δ−2/γα
n,r ≍d(δn,r).
Moreover, it can be seen that r≍δ−2/γα
n,r is the optimal choice, aligning with the optimal choice
in the upper bound. Note that it is common to establish the upper bound for the other loss function
and compare it to the lower bound established under the squared loss to check the optimality (Wei
et al., 2017; Lv et al., 2018; Li et al., 2019). By comparing the lower bounds in Theorem 4.4 with the
achievable rates from Theorems 3.3 and 4.2, we can conclude that under the case that1
2≤γ≤1,
both the standard kernel-based estimator bfλand the truncated estimator bfλ,ris minimax-optimal.
More importantly, under the more challenging case that γ >1,bfλcan only achieve a sub-optimal
rate, whereas bfλ,rcan attain the minimax rate as long as r≍d(δ1/γ
n,r)≤d(δn,r), suggesting that
the truncated kernel-based method can be treated as optimal tackling. It is also worthy pointing out
that under the just-aligned regime that γ=1
2, Yang et al. (2017) derives the minimax lower bound
by considering the regular kernel class, and Theorem 4.4 extends it to the more general setting by
allowing γ≥1
2.
5 Numerical Verification
Our established results indicate that a larger αcorresponding to a lower model complexity of the
RKHS leads to a better rate. As opposed, a smaller model with lower complexity simultaneously
may result in a potential mismatch between the model space and the target. This may weaken the
target-kernel alignment which undermines the learning efficiency. Consequently, a trade-off exists
between model capacity αand target-kernel alignment γ, with a preference for relatively lower model
complexity and stronger target-kernel alignment.
To illustrate this, we conduct some numerical experiments to study how the RKHS with varying
model complexities affect the numerical performance of KM and TKM. Specifically, we use the
spline kernel with order αthatKα(x,x′) =P∞
k=−∞e2πikxe−2πikx′|k|−α(Wahba, 1990), where α
controls the model complexity of the induced RKHS at the population level. Moreover, we consider
the nonparametric quantile regression that
Yi=f∗(xi) +σ(ϵi−Φ−1(τ)), i= 1, ..., n,
where f∗(x) =K3.5(x,0) sin( x),σ= 2,ϵi∼N(0,1),{xi}n
i=1are independently sampled from
the uniform distribution on (0,1)andΦdenotes CDF function of standard normal distribution. We
conduct the numerical experiments by varying τ∈ {0.3,0.5,0.7}andα∈ {2,4,6,8,10}with fixed
n= 300 . The data generating scheme is repeated for 50times and all the tuning parameters are tuned
to the best for both methods. The obtained results are presented in Figure 1.
From Figure 1, we can conclude that the smaller α, corresponding to richer RKHS and potentially
stronger alignment, results in significant improvement in TKM over KM. Conversely, the larger
α, corresponding to a smaller RKHS and potentially weaker alignment, results in a comparable
performance for these two methods. This observation precisely aligns with our theoretical results.
Clearly, based on our theoretical findings, the experiment results verify the existence of a trade-off
92 4 6 8 10
1.6
1.4
1.2
1.0
Log MSE=0.3
TKM 
KM 
2 4 6 8 10
1.6
1.4
1.2
Log MSE=0.5
TKM
KM
2 4 6 8 10
1.7
1.6
1.5
1.4
1.3
1.2
1.1
Log MSE=0.7
TKM
KM
2 4 6 8 10
2.0
1.9
1.8
1.7
Log empirical excess risk=0.3
TKM
KM
2 4 6 8 10
1.8
1.7
1.6
Log empirical excess risk=0.5
TKM
KM
2 4 6 8 10
2.00
1.95
1.90
1.85
1.80
1.75
1.70
Log empirical excess risk=0.7
TKM
KMFigure 1: Averaged log MSE and log empirical excess risk for KM and TKM versus αfor different τ.
between the model complexity and target-kernel alignment, indicating that a carefully data-driven
choice of the kernel may be necessary to achieve better learning efficiency. We defer the deeper
exploration of data-driven selection of an appropriate kernel to future research endeavors.
The real data analysis is deferred to Appendix A. Furthermore, a variety of additional experiments
are presented in Appendix H. The obtained results are discussed in detail, which further supports our
theoretical findings.
6 Discussions and Conclusion
6.1 Comparison and Discussions
Amini et al. (2022) studied how the target-kernel alignment affects both the standard KRR and
the truncated KRR. Although our work is motivated by Amini et al. (2022), especially for the
methodological aspect, there exist significant differences between our established results and those in
Amini et al. (2022), and some are summarized as follows: (a) Amini et al. (2022) only focused on
the upper bounds in terms of expected mean squared error, while our results provide more precise
high-probability upper bounds. (b) Beyond the polynomial decay condition considered in Amini et al.
(2022), we introduce a more general condition as stated in Assumption 3.2. This condition involves
γ, reflecting the degree of target-kernel alignment. (c) In Amini et al. (2022), both the standard KRR
and the truncated KRR have explicit solutions. This allows leveraging analytic solutions to establish
critical results, without requiring more advanced techniques. In contrast, no explicit solutions exist
in our case and our theoretical analysis adopts an alternative analytic treatment by utilizing kernel
complexity and empirical process techniques. (d) Last but not least, we rigorously confirm the
conjecture in Amini et al. (2022) asserting that the truncated KRR can achieve the minimax optimality
for all γ≥1
2.
6.2 Conclusion and Future Work
This paper provides a comprehensive theoretical understanding of the properties of the truncated
kernel-based method for a broad family of loss functions. By using kernel complexity and empirical
process techniques, the established results reveal some significant benefits from the truncated RKHS
and indicate that a carefully chosen truncation allows for an optimal trade-off between the model
complexity and approximation bias. Extensive numerical studies further justify our theoretical
findings, demonstrating a consistent improvement of the truncated estimator over the standard kernel-
based estimator. We also derived an algorithm-free minimax lower bound that matches the upper
bound on the truncated estimator and therefore confirmed its optimality. To some extent, our results
shed light on future research in statistical learning theory and real-world applications. This paper
also leaves several interesting open questions for future investigation, including the theoretical
explorations under the misspecified setting that 0< γ <1
2and how to develop a data-driven
algorithm for selecting a strongly-aligned kernel with lower model complexity.
10Acknowledgements
CW’s research is supported by the Fundamental Research Funds for the Central Universities. XH’s
research is sponsored by Natural Science Foundation of Shanghai (24ZR1421400), NSFC-11901375,
Shanghai Science and Technology Development Funds (23JC1402100), Program for Innovative
Research Team of Shanghai University of Finance and Economics and Shanghai Research Center
for Data Science and Decision Technology. JW’s research is supported in part by HK RGC Grant
GRF-14303424 and CUHK Startup Grant 4937091.
References
Amini, A., Baumgartner, R., & Feng, D. (2022). Target alignment in truncated kernel ridge regression.
InAdvances in Neural Information Processing Systems (pp. 21948–21960). Curran Associates,
Inc. volume 35.
Amini, A. A. (2021). Spectrally-truncated kernel ridge regression and its free lunch. Electronic
Journal of Statistics ,15, 3743–3761.
Bartlett, P. L., Bousquet, O., & Mendelson, S. (2005). Local rademacher complexities. The Annals of
Statistics ,33, 1497–1537.
Bauer, F., Pereverzev, S., & Rosasco, L. (2007). On regularization algorithms in learning theory.
Journal of Complexity ,23, 52–72.
Belkin, M., Ma, S., & Mandal, S. (2018). To understand deep learning we need to understand kernel
learning. In International Conference on Machine Learning (pp. 541–549). PMLR.
Belloni, A., & Chernozhukov, V . (2011). l1-penalized quantile regression in high dimensional sparse
models. The Annals of Statistics ,39, 82–130.
Bousquet, O. (2002). A Bennett concentration inequality and its application to suprema of empirical
processes. Comptes Rendus Mathematique ,334, 495–500.
Bousquet, O., & Herrmann, D. (2002). On the complexity of learning the kernel matrix. Advances in
Neural Information Processing Systems ,15.
Breiman, L. (2001). Random forests. Machine learning ,45, 5–32.
Canatar, A., Bordelon, B., & Pehlevan, C. (2021). Spectral bias and task-model alignment explain
generalization in kernel regression and infinitely wide neural networks. Nature Communications ,
12, 2914.
Caponnetto, A., & De Vito, E. (2007). Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics ,7, 331–368.
Chizat, L., Oyallon, E., & Bach, F. (2019). On lazy training in differentiable programming. Advances
in neural information processing systems ,32.
Cristianini, N., Shawe-Taylor, J., Elisseeff, A., & Kandola, J. (2001). On kernel-target alignment.
Advances in Neural Information Processing Systems ,14.
Cui, H., Loureiro, B., Krzakala, F., & Zdeborová, L. (2021). Generalization error rates in kernel
regression: The crossover from the noiseless to noisy regime. Advances in Neural Information
Processing Systems ,34, 10131–10143.
Dasgupta, S., Goldberg, Y ., & Kosorok, M. R. (2019). Feature elimination in kernel machines in
moderately high dimensions. The Annals of Statistics ,47, 497–526.
Farrell, M. H., Liang, T., & Misra, S. (2021). Deep neural networks for estimation and inference.
Econometrica ,89, 181–213.
Gerfo, L. L., Rosasco, L., Odone, F., Vito, E. D., & Verri, A. (2008). Spectral algorithms for
supervised learning. Neural Computation ,20, 1873–1897.
11Ghorbani, B., Mei, S., Misiakiewicz, T., & Montanari, A. (2020). When do neural networks
outperform kernel methods? Advances in Neural Information Processing Systems ,33, 14820–
14830.
Guermeur, Y ., Lifchitz, A., & Vert, R. (2004). A kernel for protein secondary structure prediction.
Hoi, S. C., Lyu, M. R., & Chang, E. Y . (2006). Learning the unified kernel machines for classification.
InProceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (pp. 187–196).
Jacot, A., Gabriel, F., & Hongler, C. (2018). Neural tangent kernel: Convergence and generalization
in neural networks. Advances in neural information processing systems ,31.
Kandola, J., Shawe-Taylor, J., & Cristianini, N. (2002). On the extensions of kernel alignment.
Technical report 120, Department of Computer Science, University of London , .
Kimeldorf, G., & Wahba, G. (1971). Some results on Tchebycheffian spline functions. Journal of
Mathematical Analysis and Applications ,33, 82–95.
Lai, J., Huang, D., Lin, Q. et al. (2024). The optimality of kernel classifiers in sobolev space. In The
Twelfth International Conference on Learning Representations .
Li, Y ., Liu, Y ., & Zhu, J. (2007). Quantile regression in reproducing kernel Hilbert spaces. Journal of
the American Statistical Association ,102, 255–268.
Li, Y ., Zhang, H., & Lin, Q. (2022). On the saturation effect of kernel ridge regression. In The
Eleventh International Conference on Learning Representations .
Li, Y ., Zhang, H., & Lin, Q. (2023). On the asymptotic learning curves of kernel ridge regression
under power-law decay. In Thirty-seventh Conference on Neural Information Processing Systems .
Li, Z., Ton, J.-F., Oglic, D., & Sejdinovic, D. (2019). Towards a unified analysis of random fourier
features. In International Conference on Machine Learning (pp. 3905–3914). PMLR.
Lian, H. (2022). Distributed learning of conditional quantiles in the reproducing kernel Hilbert space.
Advances in Neural Information Processing Systems ,35, 11686–11696.
Liang, T., & Rakhlin, A. (2020). Just interpolate: Kernel “ridgeless" regression can generalize. The
Annals of Statistics ,48, 1329–1347.
Lin, S.-B., & Zhou, D.-X. (2018). Distributed kernel-based gradient descent algorithms. Constructive
Approximation ,47, 249–276.
Lv, S., Lin, H., Lian, H., & Huang, J. (2018). Oracle inequalities for sparse additive quantile
regression in reproducing kernel hilbert space. The Annals of Statistics ,46, 781–813.
Ma, C., Pathak, R., & Wainwright, M. J. (2023). Optimally tackling covariate shift in RKHS-based
nonparametric regression. The Annals of Statistics ,in press , 1–38.
Patle, A., & Chouhan, D. S. (2013). SVM kernel functions for classification. In 2013 International
Conference on Advances in Technology and Engineering (ICATE) (pp. 1–9). IEEE.
Smale, S., & Zhou, D.-X. (2005). Shannon sampling II: Connections to learning theory. Applied and
Computational Harmonic Analysis ,19, 285–302.
Smale, S., & Zhou, D.-X. (2007). Learning theory estimates via integral operators and their approxi-
mations. Constructive Approximation ,26, 153–172.
Steinwart, I., & Christmann, A. (2008). Support Vector Machines . Springer Science & Business
Media.
Steinwart, I., Scovel, C. et al. (2009). Optimal rates for regularized least squares regression. In
Proceedings of the Annual Conference on Learning Theory, 2009 (pp. 79–93).
Wahba, G. (1990). Spline models for observational data. In CBMS-NSF Regional Conference Series
in Applied Mathematics . SIAM.
12Wainwright, M. J. (2019). High-dimensional Statistics: A Non-asymptotic Viewpoint volume 48.
Cambridge University Press.
Wei, Y ., Yang, F., & Wainwright, M. J. (2017). Early stopping for kernel boosting algorithms: A
general analysis with localized complexities. Advances in Neural Information Processing Systems ,
30.
Wong, W. W., & Burkowski, F. J. (2011). Using kernel alignment to select features of molecular de-
scriptors in a QSAR study. IEEE/ACM Transactions on Computational Biology and Bioinformatics ,
8, 1373–1384.
Xu, P., Wang, Y ., Chen, X., & Tian, Z. (2021). Coke: Communication-censored decentralized kernel
learning. Journal of Machine Learning Research ,22, 1–35.
Yang, Y ., Pilanci, M., & Wainwright, M. J. (2017). Randomized sketches for kernels: Fast and
optimal nonparametric regression. Annals of Statistics ,45, 991–1023.
Zhai, R., Pukdee, R., Jin, R., Balcan, M. F., & Ravikumar, P. K. (2024). Spectrally transformed
kernel regression. In The Twelfth International Conference on Learning Representations .
13Appendix
This appendix is organized as follows. In Section A, we conduct a real data analysis. Section B is
devoted to providing more discussions and future directions. In Section C, we provide the proof
for results in Section 3, including Theorem 3.3 and Corollary 3.5. In Section D, we provide the
proof for results in Section 4 in part, including Theorem 4.2 and Corollary 4.3. In Section E, we
complement the upper bounds by deriving the minimax lower bound. In Section G, we discuss the
locally strong convexity condition and local smoothness condition presented in Assumption 3.2 for
various loss functions in detail. In Section G, we list some useful lemmas utilized in our proofs,
including concentration inequality, symmetrization inequality, and Gaussian contraction inequality.
Section H provides additional experiments under various settings.
Notation. Denote the vector inner product ⟨α,β⟩2=Pn
i=1αiβiand the norm ∥α∥2=p
⟨α,α⟩2
forα,β∈ Rn. For any integer m, we use [m]to represent the set {1,2, ..., m}for short.
A Real Data Analysis
We apply both TKM and KM with check loss to the wine quality dataset, which is available in
the UCI Machine Learning Repository. Specifically, we first adopt the random forest method
(Breiman, 2001) to rank the feature importance and select the first three influential features: ‘Alcohol’,
‘Sulfates’, and ‘V olatile Acidity’ for analysis. Then, we randomly select 500samples for training
and another 500samples for testing. The above procedure is repeated 10times, where the Laplacian
kernel K(x,x′) = exp( −∥x−x′∥1)is adopted and the parameters γandrare tuned by 5-fold
cross-validation. The averaged MSE with different τ∈(0.3,0.5,0.7)is reported in Table 2.
Table 2: Averaged MSE for different methods
τ 0.3 0.5 0.7
KM 0.590±0.027 0.483±0.035 0.638±0.073
TKM 0.548±0.026 0.454±0.045 0.530±0.046
It is thus clear that the obtained results in the real application align with the results for synthetic data
and our theoretical findings in the main text, which further demonstrates the benefits of TKM.
B More Discussions and Future Directions
B.1 Verification of the Existence and Uniqueness of δnandδn,r
In this section, we provide a detailed verification of the existence and uniqueness of δnandδn,r,
where δnandδn,rare defined as the smallest solutions to (2) and (3), respectively.
For any ι∈(0,1)andγ≥1
2, define
g(δ) :=2Clogι−1R(δ)
c0δ2η+1onδ∈(0,∞),
where η= max {γ,1}. Here, we ignore the dependence of gonιandγfor ease of presentation.
The verifying argument is based on Lemma 3.2 in Bartlett et al. (2005), which states that if ψ:
[0,∞)→[0,∞)is a nontrivial sub-root function6, then it is continuous on [0,∞), and the equation
ψ(r) =rhas a unique positive solution.
Recall that we assume µ1, ..., µ n>0. From the definition of R(δ), it can be easily seen that R(√
δ)
is a nontrivial sub-root function. Then from Lemma 3.2 in Bartlett et al. (2005), R(√
δ)is continuous
on[0,∞), and thus g(δ)is also continuous on (0,∞). Note that g(δ)δ2ηis nonincreaing on (0,∞),
theng(δ)must be strictly decreasing (0,∞).
6A function ψis called nontrivial sub-root if ψ̸≡0, and it is nonnegative, nondecreasing and if ψ(r)/√ris
nonincreasing for r >0.
14Ifg(δ)is always larger than 1on(0,∞), we have
lim
δ→∞g(δ)δ2η=∞,
which is impossible since g(δ)δ2ηis nonincreasing on (0,∞). On the other hand, if g(δ)is always
smaller than 1on(0,∞), then
lim
δ→0g(δ)δ2η= 0,
implying g(δ)δ2η≡0since g(δ)δ2ηis nonincreasing. Since R(δ)is not trivial, this is also impossible.
Therefore, by the continuity of gon(0,1), the equation g(δ) = 1 has a positive solution δnthat
is unique by the strict monotonicity of g. Note that by the strict monotonicity of g,δncan be
equivalently defined as the smallest solution to g(δ)≤1. According to the definition of g, we
conclude the existence and uniqueness of δn. Forδn,r, repeat a similar argument, we can also verify
its existence and uniqueness.
B.2 Decomposition of the Reduced Kernel Matrix
Recall that Kr=1
nKr(xi,xj)	n
i,j=1is the empirical kernel matrix w.r.t. Kr, andK=U DU⊤,
where U= (u1, ...,un)∈ Rn×nis an orthonormal matrix and D∈ Rn×nis a diagonal matrix with
positive elements µ1, ..., µ narranging in a descending ordering. Denote uk= (uk1, ..., u kn)⊤. By
the definitions of Krand{ψk}i∈[n], we have
(Kr)ij=1
nrX
k=1µkψk(xi)ψk(xj) =rX
k=1µkukiukj.
On the other hand, recall that Dris diagonal matrix with elements µ1, ..., µ r,0, ...,0, we have
(U D rU⊤)ij=nX
k=1Dkkukiukj=rX
k=1µkukiukj= (Kr)ij.
Therefore, we conclude that Kr=U D rU⊤.
B.3 Extension to Random Design Setting
In this work, we investigate the effect of target-kernel alignment and provide the best solution to
overcome the well-known saturation effect. It is also worthy pointing out that although the obtained
results are derived under the fixed design setting, it may be possible to extend them to the random
design setting and we leave it to future exploration. Some key steps of the possible extensions are
discussed below.
Under the random design setting, we consider a random variable X∼ρ, where ρis a probability
measure supported on X ⊂ Rp. Let the covariates {xi}n
i=1be independently sampled from ρ. Denote
the space of square-integrable functions f:X → R with respect to ρasL(X, ρ), where X ⊂ Rp.
Recall that HKis an RKHS induced by a positive semi-definite kernel function K. By Mercer’s
theorem (see, for instance, Theorem 12.20 in Wainwright (2019)), if Xis compact and Kis a
continuous, combined with our bounded assumption that supx,x′∈XK(x,x′)≤κ2, the kernel
function admits an expansion of form
K(x,x′) =∞X
j=1˜µjϕj(x)ϕj(x′),
where ˜µj’s are the non-negative eigenvalues in descending ordering and ϕj’s are the corresponding
eigenfunctions in L(X, ρ). Given this expansion of K, the RKHS HKcan be written as
HK=n
f=∞X
j=1αjϕj:∞X
j=1α2
j
˜µj<∞o
,
equipped with inner product ⟨f, g⟩K=P∞
j=1αjβj
˜µjforf=P∞
j=1αjϕjandg=P∞
j=1βjϕj.
15Under the random design setting, the population risk function is defined as
E(f) :=E
L 
Y, f(X))
,
where Y|X=x∼PY|x. Then, the target function f∗is defined as
f∗:= argminfE(f).
If we assume f∗∈ H K, then f∗can be expanded as f∗=P∞
j=1α∗
jϕjwithα∗= (α∗
1, α∗
2, ...)⊤
satisfyingPn
j=1α∗
j2
˜µj<∞.
An assumption analogous to Assumption 3.2 under the random design setting is required thatP∞
j=1α∗
j2˜µ−2γ
j≤u2for some constants γ≥1
2andu. Here, γmeasures the target-kernel alignment
at the population level, and it is equivalent to smooth (or source) parameter in literature (Caponnetto
& De Vito, 2007; Cui et al., 2021; Li et al., 2023). Note that if γ=1
2, we merely assume that
the target function f∗belongs to the RKHS HK, and as γincreases, the target function becomes
smoother w.r.t. the RKHS HK.
Furthermore, the polynomial decay assumption analogous to Assumption 3.4 under the random
design setting can be made as
˜µj≍j−αand α∗
j2≍j−2γα−1(4)
with constants α >1andγ≥1
2. Note that the assumption (4)is equivalent to condition (8) in Cui
et al. (2021).
Grant these assumptions, the impact of the target-kernel alignment on both standard and truncated
kernel-based methods under the random design setting can be analyzed by using the population kernel
complexity, defined as
eR(δ) :=vuut1
n∞X
j=1min{δ2,˜µj}.
Since the theoretical derivation should be more deeply involved, we leave this promising topic to
future investigation.
B.4 Connection with Spectrally Transform Kernel Regression
The spectrally transformed kernel regression (SKRR, Zhai et al. (2024)) aims to use spectrally
transformation for constructing a new kernel that can leverage the information contained in unlabeled
data in an explicit way. Note that we have shown that the truncated kernel method can overcome the
saturation effect thanks to the reduced kernel complexity. We also believe that SKRR may be able to
overcome the saturation effect if the transformation function can be properly chosen. The possible
routine for establishing the theoretical results is briefly discussed below.
Recall that by Mercer’s theorem, we have
K(x,x′) =∞X
j=1˜µjϕj(x)ϕj(x′).
For SKRR, K(x,x′)is replaced with a new kernel that
K′(x,x′) :=∞X
j=1s(˜µj)ϕj(x)ϕj(x′),
where s(·) :R+→ R+is the general transformation function. Let bfλ,sbe the kernel-based estimator
via the RKHS HK′induced by K′. The primary goal is to study how the prediction error ∥bfλ,s−f∗∥ρ
depends on the choice of s.
The idea of deriving an upper bound on the prediction error is to separately bound the estimation
error∥bfλ,s−f♯
s∥ρand approximation bias ∥f♯
s−f∗∥ρ, where ∥ · ∥ ρdenotes the norm equipped
16withL(X, ρ), and f♯
s:=P∞
j=1s(α∗
j)ϕjis introduced as an immediate function belonging to HK′.
Following a similar technical treatment in Section D, the upper bound on estimation error can be
established. For the approximation bias, by writing f∗=P∞
j=1α∗
jϕj, we find that
f♯
s−f∗2
ρ=∞X
j=1 
s(α∗
j)−α∗
j2.
Clearly, the selection of s(·)is crucial and it is favorable if s(·)is close to the identity function for
small jand decays extremely rapidly as jtends to infinity, such as s(˜µj) = ˜µjI{j≤r}corresponding to
the truncated method. Then, SKRR with some proper choices of s(·)may achieve similar conclusions
about the upper bound as we provided in the main text. We leave such a promising topic as potential
future work.
B.5 Computational Complexity of Truncated Kernel Method
For simplicity, we focus only on the mean regression task where the squared loss is specified. Note
that the total computational complexity of the truncated KRR is composed of three parts. Specifically,
in the first part, spectrally decomposing the kernel matrix Khas the computational complexity of
O(n3). In the second part, the basis {ψk}1≤k≤rcan be simply calculated by ψk(x) =u⊤
kK−1Kx
with
Kx=1√n 
K(x,x1), ..., K (x,xn)⊤
for each k∈[n], which also has O(n3)computational complexity. In the last part, computing
the KRR via the r-dimensional RKHS HKrhas computational complexity of O(nr2). To sum
up, the overall computational complexity of TKM is O(n3). Solving the standard KRR also has
computational complexity of O(n3), and thus the truncated KRR does not impose an additional
computational cost.
C Proof of Results for Kernel-based Method
C.1 Error Analysis
For ease of presentation, without loss of generality, we assume µj≤1for all j∈[n]in the rest of
this paper7. We start the error analysis by noting that for any f=S⊤
x(α)∈ H Kwithα∈ Rn, we
have
∥f∥n=∥Kα∥2and∥f∥K=K1/2α
2. (5)
This transform from the L(Pn)-norm and the norm in RKHS to vector norm will be frequently
applied in our proof.
Denote
q∗:=√nSx(f∗) = (f∗(x1), ..., f∗(xn))⊤.
Recall that the TA scores are the elements of the vector
ξ∗=U⊤Sx(f∗) =1√nU⊤q∗.
Define an immediate function
f♯:=S⊤
x(α♯)∈ H Kwith α♯=U D−1ξ∗∈ Rn.
f♯can be viewed as the best approximation of f∗onto the n-dimensional function space Hn, defined
as
Hn:=
f=S⊤
x(α) :α∈ Rn	
.
7Otherwise, by the assumption µj→0, we always have µj≤1forjexceeding some constant j∗.
17To be more clear, by the orthogonality of U, we have
f∗−f♯2
n=1
nq∗−√nKα♯2
2
=n−1/2U⊤q∗−U⊤U D U⊤α♯2
2
=U⊤Sx(f∗)−U⊤U D U⊤α♯2
2
=ξ∗−D U⊤α♯2
2=ξ∗−D U⊤U D−1ξ∗2
2= 0, (6)
implying
f∗(xi) =f♯(xi)for each i∈[n].
Therefore, we obtainf−f∗
n=f−f♯
nfor all f∈ H K (7)
and
E(f)− E(f∗) =E(f)− E(f♯)for all f∈ H K. (8)
Furthermore, by applying (5), we have
f♯
K=K1/2U D−1ξ∗
K=U D1/2U⊤U D−1ξ∗
K=D−1/2ξ∗
K.
By our assumption that µj≤1for all j, one has
f♯2
K=D−1/2ξ∗2
2=nX
j=1µ−1
jξ∗
j2≤nX
j=1µ−2γ
jξ∗
j2≤u2, (9)
where the last inequality holds by Assumption 3.2 with γ≥1
2.
The construction of f♯plays a crucial role in our proofs. Intuitively, the kernel complexity function
R(δ)is defined at an empirical level (depends on the fixed points x1, ...,xn) and serves as a com-
plexity measure of finite space Hn. This poses a technical challenge as the true function f∗lies in an
infinite-dimensional function space, creating a mismatch with the empirical kernel complexity R(δ).
To solve this problem, we introduce the best approximation f♯off∗in finite-dimensional function
spaceHn. Our proof will first focus on deriving the upper bound on ∥bfλ−f♯∥n, and then move to
∥bfλ−f∗∥nby using the relation (6).
Another advantage to consider the best approximation of f∗instead of itself is that bfλlies in the
same space Hnasf♯, allowing us to express ∥bfλ−f♯∥2
2and∥bfλ−f♯∥2
Kin the term of the kernel
matrix Kaccording to (5), which is useful in the technical proof. To the best of our knowledge, this
is a novel treatment to establish theoretical results for the kernel-based estimator. In the proof for
the truncated estimator in Section D, we will adopt a similar proof strategy to construct the best
approximation f♯
roff∗in the reduced space HKr.
Based on the error analysis, we are ready to present the proof for Theorem 3.3.
C.2 Proof of Theorem 3.3
Define the localized function class
Hn,b:=
f:f∈ H n,∥f−f♯∥K≤b	
.
Here, bis a constant independent of n, γ, which will be specified in the proof. Without loss of
generality, we assume b >1.
For given ι∈(0,1)andδ >0, define the auxiliary event
V(ι, δ) :=(bE(f)−bE(f♯)−[E(f)− E(f♯)]
δ−1∥f−f♯∥n+∥f−f♯∥K≤Clogι−1R(δ)holds for any f∈ H n,b)
.
The following two lemmas are crucial for proving Theorem 3.3.
18Lemma C.1. For given ι∈(0,1)andδ >0, the event V(ι, δ)occurs with probability greater than
1−ι, i.e.
P(V(ι, δ))≥1−ι.
As demonstrated in the proof of Lemma C.1, bis incorporated into the constant Cof the upper bound
Clogι−1R(δ), meaning that Cdepends on b.
Recall that δnis the critical radius defined as the smallest solution to (2).
Lemma C.2. Letη= min {γ,1}. On the event V(ι, δn), with the choice of λsatisfying δ2
n≤λ≤1,
we havebfλ−f♯2
n≤C 
δ4η
n+λ2η
,
where Cis a constant independent of n, γ.
By applying Lemma C.1, we have P(V(ι, δn))≥1−ι. Together with Lemma C.2 and the relation
(6), it holds with probability at least 1−ιthat
bfλ−f∗2
n=bfλ−f♯2
n≤C 
δ4η
n+λ2η
,
which completes the proof for the L(Pn)-error.
For the excess risk, it immediately follows from Assumption 3.1. Accordingly, it remains to prove
Lemmas C.1 and C.2.
C.2.1 Proof of Lemma C.1
Denote
D:=bE(f)−bE(f♯)−
E(f)− E(f♯)
.
Then, our goal is to prove that for all f∈ H n,b
|D| ≤ Clogι−1R(δ) 
δ−1f−f♯
n+f−f♯
K
.
Ifδ−1∥f−f♯∥n+∥f−f♯∥K= 0, the above inequality is naturally satisfied. Therefore, without
loss of generality, we assume δ−1∥f−f♯∥n+∥f−f♯∥K>0for all f∈ H n,b. It is equivalent to
proving that
A:= sup
f∈Hn,b|D|
δ−1∥f−f♯∥n+∥f−f♯∥K≤Clogι−1R(δ).
By applying the triangle inequality, together with (9), we find that for any f∈ H n,b
∥f∥K≤f−f♯
K+f♯
K≤u+b.
Let˜b:=u+b. According to our assumption for the loss function in Section 2, L(y,·)satisfies the
Lipschitz continuity over the function class Hn,bwith Lipschitz constant ML,˜b. For simplifying
notation, we hide the dependence of the Lipschitz constant on L,˜bby writing M:=ML,˜b.
The remaining proof follows a standard procedure: first bound the expectation of Aand then bound
the deviation of Afrom its expectation. Finally, we combine these two bounds to obtain the desired
result.
Bounding E[A].Letw1, ..., w n∼N(0,1)denote the standard Gaussian variables, independent of
the data. To bound E[A], we employ the symmetrization technique in Lemma G.2. Specifically, we
have
E[A] =Eh
sup
f∈Hn,b|D|
δ−1∥f−f♯∥n+∥f−f♯∥Ki
(i)
≤√
2π
nEh
sup
f∈Hn,bPn
i=1wi 
L(yi, f(xi))−L(yi, f♯(xi))
δ−1∥f−f♯∥n+∥f−f♯∥Ki
(ii)
≤2√
2πM
nEh
sup
f∈Hn,bPn
i=1wi(f(xi)−f♯(xi))
δ−1∥f−f♯∥n+∥f−f♯∥Ki
, (10)
19where (i) follows from Lemma G.2, and (ii) follows from the fact that the loss function is M-Lipschitz
continuous and the Gaussian contraction inequality in Lemma G.3. To further derive the upper bound
for the right-hand side of (10), we consider the localized function class of the form
F(δ) :=n
f=S⊤
x(α) :f−f♯
K≤1,f−f♯
n≤δ,α∈ Rno
.
For any f∈ H n,b, there exists α∈ Rnsuch that f=S⊤
x(α). Recall that f♯=S⊤
x(α♯).
Define the vector β:=D U⊤(α−α♯). Then, by applying (5),f∈ F(δ)implies the constraints on
βthat∥D−1/2β∥2≤1and∥β∥2≤δ.
Further, note that any vector satisfying these two constraints must belong to the ellipse class
E:=n
β∈ Rn:nX
j=1β2
j
νj≤2withνj= min {δ2, µj}o
.
Denote w= (w1, ..., w n)⊤, we have
Eh
sup
f∈F(δ)nX
i=1wi(f(xi)−f♯(xi))i
≤Eh
sup
β∈E√n⟨w,Uβ⟩i
(i)=Eh
sup
β∈E√n⟨w,β⟩i
(ii)
≤Eh
sup
β∈E√nvuutnX
j=1β2
j
νjvuutnX
j=1νjw2
ji
≤√
2nEhvuutnX
j=1νjw2
ji(iii)
≤√
2nvuut1
nnX
j=1νj=√
2nR(δ),
(11)
where (i) follows ⟨w,Uβ⟩=⟨Uw,β⟩andUw∼N(0,In)sinceUis an orthogonal matrix, (ii)
follows from Cauchy-Schwarz inequality, and (iii) follows from Jensen’s inequality.
Note that (11) holds when considering the supremum over F(δ). For f∈ H n,b, by defining the
rescaled function
ef=f−f♯
δ−1∥f−f♯∥n+∥f−f♯∥K+f♯,
we have
ef−f♯
n=∥f−f♯∥n
δ−1∥f−f♯∥n+∥f−f♯∥K≤δ
and
ef−f♯
K=∥f−f♯∥K
δ−1∥f−f♯∥n+∥f−f♯∥K≤1.
As a result, ef∈ F(δ).
On the other hand,
EhPn
i=1wi(f(xi)−f♯(xi))
δ−1∥f−f♯∥n+∥f−f♯∥Ki
=EhnX
i=1wi(ef(xi)−f♯(xi))i
.
This result, combined with (10) and (11), implies
E[A]≤2√
2πM
nEh
sup
f∈Hn,bPn
i=1wi(f(xi)−f♯(xi))
δ−1∥f−f♯∥n+∥f−f♯∥Ki
≤2√
2πM
nEh
sup
f∈F(δ)nX
i=1wi(f(xi)−f♯(xi))i
≤4√πMR (δ).(12)
20Bounding A −E(A).We use the concentration inequality in Lemma G.1 to bound A −E(A). For
eachi∈[n]and any f∈ F(δ), define sj= sign( f(xi)−f♯(xi))ifj=iandsj= 0ifj̸=iand
lets= (s1, ..., s n)⊤, then we have
|f(xi)−f♯(xi)|=nX
j=1sj(f(xi)−f♯(xi))
=√n⟨s,Uβ⟩(i)
≤√nvuutnX
j=1β2
j
νjvuutnX
j=1νjs2
j≤√nvuutnX
j=1β2
j
νjvuutnX
j=1νj=√
2nR(δ),
where (i) follows from the fact that Uis orthogonal and Cauchy-Schwarz inequality. Consequently,
for each i∈[n], we have
L(yi, f(xi))−L(yi, f♯(xi))≤M|f(xi)−f♯(xi)| ≤M√
2nR(δ),
where the second inequality follows from that L(yi,·)isM-Lipschitz continuous. In addition, for
anyf∈ F(δ), we have
1
nnX
i=1Eh 
L(yi, f(xi))−L(yi, f♯(xi))2i
≤M2
nnX
i=1(f(xi)−f♯(xi))2
=M2∥f−f♯∥2
n
=M2⟨Uβ,Uβ⟩
=M2⟨β,β⟩
≤M2max
i∈[n]νjnX
i=1β2
j
νj≤2M2nX
i=1νj= 2M2nR2(δ).
By a similar rescaled method, we haveL(yi, f(xi))−L(yi, f♯(xi))
δ−1∥f−f♯∥n+∥f−f♯∥K≤M|f(xi)−f♯(xi)|
δ−1∥f−f♯∥n+∥f−f♯∥K
=M|ef(xi)−f♯(xi)|
≤√
2MnR (δ),
and
1
nnX
i=1EhL(yi, f(xi))−L(yi, f♯(xi))
δ−1∥f−f♯∥n+∥f−f♯∥K2i
≤M2
nnX
i=1f(xi)−f♯(xi)
δ−1∥f−f♯∥n+∥f−f♯∥K2
=M2
L,˜b
nnX
i=1(ef(xi)−f♯(xi))2
≤2M2nR2(δ).
Thus, the requirements in Lemma G.1 hold with η=√
2MnR (δ)andζ2= 2M2nR2(δ). Then, by
applying Lemma G.1, for any ι∈(0,1), lett=q
logι−1
n, it holds at with probability at least 1−ι
that
A −E(A)≤r
logι−1
n
4M2nR2(δ) + 4√
2MnR (δ)E(A)
+2√
2M
3R(δ) logι−1
(i)
≤eClogι−1R(δ),(13)
whereeC=q
(4M2+ 16√
2πM2) +2√
2M
3and(i)follows from (12).
Therefore, by combining (12) and (13), it holds with probability at least 1−ιthat
A ≤Clogι−1R(δ)
where C=q
(4M2+ 8√
2πM2) +2√
2M
3+ 4√πM. This completes the proof of Theorem 3.3. □
21C.2.2 Proof of Lemma C.2
For short, we write
∆ =c0
4δ4η
n+ 4u2λ2η.
On the event V(ι, δn), we claim that
bfλ−f♯2
n≤C∆.
According to the optimality of bfλand the feasibility of f♯, we obtain
bE(bfλ)−bE(f♯) +λ∥bfλ∥2
K−λ∥f♯∥2
K≤0. (14)
Then, proving ∥bfλ−f♯∥2
n≤C∆suffices to prove that if ∥bfλ−f♯∥2
n> C∆or∥bfλ−f♯∥K> b,
we have
bE(bfλ)−bE(f♯) +λ∥bfλ∥2
K−λ∥f♯∥2
K>0.
Below we are devoted to verifying this fact. Define the function class
G:=n
f∈ H n:f−f♯2
n≤C∆,f−f♯
K≤bo
.
Suppose that bfλ̸∈ G. Since both GandHnare convex class by the convexity of L(y,·)and Jensen’s
inequality, there exists a function ef=αbfλ+ (1−α)f♯withα∈(0,1]that sits on the boundary of
G(Ma et al., 2023). If we can show that
bE(ef)−bE(f♯) +λ∥ef∥2
K−λ∥f♯∥2
K>0, (15)
by the convexity of L(y,·)and Jensen’s inequality, we must have
bE(bfλ)−bE(f♯) +λ∥bfλ∥2
K−λ∥f♯∥2
K≥1
α
bE(ef)−bE(f♯) +λ∥ef∥2
K−λ∥f♯∥2
K
>0.
Then, let us focus on proving (15) on the event V(ι, δn).
Note that efbelongs to the the boundary of G, we can split the remaining proof into two cases: (i)
∥ef−f♯∥2
n=C∆and∥ef−f♯∥K≤b; and (ii) ∥ef−f♯∥2
n≤C∆and∥ef−f♯∥K=b.
Case (i): By applying (7) and (8), from Assumption 3.1, we have
c0ef−f♯2
n=c0ef−f∗2
n≤ E(ef)− E(f∗) =E(ef)− E(f♯).
so that the c0-strong convexity also holds for f♯.
On the event V(ι, δn), we have
bE(f♯)−bE(ef)
≤ E(f♯)− E(ef) +Clogι−1R(δn)
δ−1
nef−f♯
n+ef−f♯
K
≤ −c0ef−f♯2
n+Clogι−1R(δn)
δ−1
nef−f♯
n+ef−f♯
K
(i)=−c0C∆ +Clogι−1R(δn)
δ−1
n√
C∆ +ef−f♯
K
(ii)
≤ −c0C∆ +Cδ−1
nlogι−1R(δn)√
C∆ +C(logι−1R(δn))2λ−1+λ
4ef−f♯2
K
(iii)
≤ −c0C∆ +c0
2δ2η
n√
C∆ +c0
4δ4η+2
nλ−1+λ
4ef−f♯2
K,
where (i) follows from ∥ef−f♯∥2
n=C∆, (ii) uses the elementary inequality that 2ab≤a2+b2and
(iii) follows from the definition of δnsatisfying
Clogι−1R(δn)≤c0
2δ2η+1
n.
22Further with the choice of λ≥δ2
n, we have
bE(f♯)−bE(ef)≤ −c0C∆ +c0
2δ2η
n√
C∆ +c0
4δ4η
n+λ
4ef−f♯2
K.
On the other hand, we notice that
λf♯2
K−λef2
K=−2λ⟨f♯,ef−f♯⟩K−λef−f♯2
K. (16)
Note that ⟨f, g⟩K=⟨K1/2f,K1/2g⟩Kfor any f, g∈ H n.ef∈ H nso that it can be written as
ef=S⊤
x(eα)for some eα∈ Rn.
Therefore, for1
2≤γ≤1, there holds
λ⟨f♯,ef−f♯⟩K=λ⟨K1/2α♯,K1/2(eα−α♯)⟩2
=λ⟨K1−γα♯,Kγ(eα−α♯)⟩2
≤λK1−γα♯
2Kγ(eα−α♯)
2
=λD1−γD−1ξ∗
2Kγ(eα−α♯)
2
=λD−γξ∗
2Kγ(eα−α♯)
2
(i)
≤uλKγ(eα−α♯)
2,
where (i) follows from Assumption 3.2.
For1
2≤γ≤1, using a similar treatment as that in Lian (2022), we apply Young’s inequality
AB≤Ap
p+Bq
qfor two positive operators AandBwith1
p+1
q= 1andp, q≥1to obtain
λKγ(eα−α♯)
2
=λγq
λ2−2γK2γ−1K eα−α♯
,eα−α♯
2
≤λγq
 
(2−2γ)λ+ (2γ−1)K
K eα−α♯
,eα−α♯
2
(i)
≤λγmaxq
λK1/2 eα−α♯
,K1/2 eα−α♯
2,q
K 
α−α♯
,K 
α−α♯
2
(ii)
≤λγ+1
2ef−f♯
K+λγef−f♯
n, (17)
where (i) holds by taking γ=1
2andγ= 1, and (ii) follows from max{a, b} ≤a+b.
Forγ >1, we claim that ∥D−1ξ∗∥2≤u. To see this, we find that
D−1ξ∗2
2=∞X
j=1µ−2
jξ∗
j2(i)
≤∞X
j=1µ−2γ
jξ∗
j2≤u2,
where (i) holds by our assumption µj≤1for all j.
Then, similar to (17) with γ= 1, we have
λ⟨f♯,ef−f♯⟩K≤uλ3
2ef−f♯
K+uλef−f♯
n.
Combine these two case to obtain that for γ≥1
2
λ⟨f♯,ef−f♯⟩K≤uλη+1
2ef−f♯
K+uληef−f♯
n
=uλη+1
2ef−f♯
K+uλη√
C∆.(18)
Here, we recall that η= min {γ,1}.
23Putting the pieces together, for all γ≥1
2, we have
bE(f♯)−bE(ef) +λ∥f♯∥2
K−λ∥ef∥2
K
≤ −c0C∆ +c0
2δ2η
n+ 2uλη√
C∆ +c0
4δ4η
n+λ
4ef−f♯2
K+ 2uλη+1
2ef−f♯
K−λef−f♯2
K
(i)
≤ −c0C∆ +c0
2δ2η
n+ 2uλη√
C∆ +c0
4δ4η
n+λ
2ef−f♯2
K+ 4u2λ2η−λef−f♯2
K
≤ −c0C∆ +c0
2δ2η
n+ 2uλη√
C∆ +c0
4δ4η
n+ 4u2λ2η, (19)
where (i) uses the elementary inequality.
Below is devoted to proving that for a sufficiently large C, the RHS of (19) is less than 0. Precisely,
let
φ(x) =c0x2−c0
2δ2η
n+ 2uλη
x−c0
4δ4η
n−4u2λ2η.
Letx=√
C∆, note that
φ(x) =c0C∆−c0
2δ2η
n+ 2uλη√
C∆−c0
4δ4η
n−4u2λ2η
=(c0C−1)∆−c0
2δ2η
n+ 2uλη√
C∆
=√
C∆c0C−1√
C√
∆−c0
2δ2η
n+ 2uλη
(i)
≥√
C∆c0C−1√
2C√c0
2δ2η
n+ 2uλη
−c0
2δ2η
n+ 2uλη
,
where (i) follows from the basic inequalitya+b
2≤q
a2+b2
2. Sincec0C−1√
2Cis increasing in C, we can
select Csuch thatc0C−1√
2C≥max{√c0,1}, which leads to φ(x)>0.
In conclusion, for a sufficiently large Cin the definition of the function class G, on the event V(ι, δn),
for case (i), we have
bE(f♯)−bE(ef) +λ∥f♯∥2
K−λ∥ef∥2
K<0.
Case (ii) : Repeat the similar argument as that in Case (i), on the event V(ι, δn)and by Assumption
3.1, we have
bE(f♯)−bE(ef)
≤ −c0ef−f♯2
n+Clogι−1R(δn)
δ−1
nef−f♯
n+ef−f♯
K
≤Clogι−1R(δn)
δ−1
nef−f♯
n+ef−f♯
K
≤c0
2δ2η
n√
C∆ +c0
4δ4η+2
nλ−1+λ
4ef−f♯2
K.
Further with the choice of λ≥δ2
n, we have
bE(f♯)−bE(ef)≤c0
2δ2η
n√
C∆ +c0
4δ4η
n+λ
4ef−f♯2
K.
Combine with (16) and (18) to obtain
bE(f♯)−bE(ef) +λ∥f♯∥2
K−λ∥ef∥2
K
≤c0
2δ2η
n+ 2uλη√
C∆ +c0
4δ4η
n+λ
4ef−f♯2
K+ 2uλη+1
2ef−f♯
K−λef−f♯2
K
≤c0
2δ2η
n+ 2uλη√
C∆ +c0
4δ4η
n+ 4u2λ2η−λ
2ef−f♯2
K.
24Note that 0< λ≤1andλ≥δ2
ntogether implies
λ≥λ2ηand λ≥δ4η
n.
Moreover, in Case (ii), ∥ef−f♯∥K=b. Therefore,
bE(f♯)−bE(ef) +λ∥f♯∥2
K−λ∥ef∥2
K
≤c0
2δ2η
n+ 2uλη√
C∆ +c0
4δ4η
n+ 4u2λ2η−1
2b2δ4η
n−1
2b2λ2η
≤r
c2
0
2δ4η
n+ 8u2λ2η√
C∆ +c0
4δ4η
n+ 4u2λ2η−1
2b2δ4η
n−1
2b2λ2η
≤√
Cc0
4max{2c0,1}δ4η
n+ 8u2λ2η
+c0
4δ4η
n+ 4u2λ2η−1
2b2δ4η
n−1
2b2λ2η,
where the last line is less line is less than 0for sufficiently large constant b.
At last, by combining Cases (i) and (ii), we prove (15). Therefore, on the event V(ι, δn), we have
bfλ−f♯2
n≤C∆≲δ4η
n+λ2η,
which completes the proof. □
C.3 Proof of Corollary 3.5
Recall that the statistical dimension is defined as
d(δ) = min
j∈[n] :µj≤δ2	
.
From the definition of d(δ), we have
R(δ) =vuut1
nd(δ)δ2+1
nnX
d(δ)+1µj. (20)
Recalling thatPn
d(δ)+1µj≲d(δ)δ2for regular kernel and kernel with the polynomial in its eigen-
values is regular (Yang et al., 2017), the kernel complexity function satisfies
R(δ)≍r
1
nd(δ)δ2.
Therefore, the solution to the inequality (2) can be bounded from above by the solution to
Clogι−1r
1
nd(δ)δ2≤c0
2δ2η+1. (21)
Moreover, if the eigenvalues of Kexhibit α-polynomial decay that is µj≍j−α, then we have
d(δ)≍δ−2/α. Together with (21) leads to
δ2
n≤C(logι−1)2
nα
2ηα+1.
Then, with the choice of λ≍δ2
n, it holds with probability at least 1−ιthat
∥bfλ−f∗∥2
n≤C(δ4η
n+λ2η)≤C(logι−1)2
n2ηα
2ηα+1.
We conclude the upper bound in Corollary 3.5. □
25D Proof of Results for Truncated Kernel-based Method
D.1 Error Analysis
Recall that
q∗=√nSx(f∗) = (f∗(x1), ..., f∗(xn))⊤
and
ξ∗=U⊤Sx(f∗) =1√nU⊤q∗.
In the proof for the truncated kernel-based estimator, we partition ξ∗into two sub-vectors as
ξ∗⊤= (ξ∗
1⊤, ξ∗
2⊤)
withξ∗
1∈ Rrandξ∗
2∈ Rn−r.
Moreover, we partition Uinto two sub-matrixs
U= (U1,U2)
withU1∈ Rn×randU2∈ Rn×(n−r), and partition Dinto two blocks D1andD2, that is
D=
D1
D2
,
where D1∈ Rr×randD2∈ R(n−r)×(n−r). Since the last n−rdiagonal elements of Drare all
zero, for any α∈ Rn, the last n−relements of DrU⊤αare also all zero. Then, we have
DrU⊤α= 
(D1U⊤
1α)⊤,0⊤⊤for all α∈ Rn. (22)
Define
Hn,r:=
f=S⊤
x,r(α) :α∈ Rn	
.
For any f=S⊤
x,r(α)∈ H n,r, we have
∥f∥n=Krα
2=DrU⊤α
2=D1U⊤
1α
2,
where the last step holds by applying (22). We also observe
∥f∥Kr=K1/2
rα
2=D1/2
1U⊤
1α
2. (23)
Define an immediate function
f♯
r:=S⊤
x,r(α♯
r)∈ H n,r with α♯
r=U1D−1
1ξ∗
1∈ Rn.
From (23), we have
f♯
r2
Kr=D1/2
1U⊤
1U1D−1
1ξ∗
12
Kr
=D−1/2
1ξ∗
12
Kr=rX
j=1µ−1
jξ∗
j2(i)
≤rX
j=1µ−2γ
jξ∗
j2(ii)
≤u2,(24)
where (i) holds by our assumption µj≤1for all jand (ii) follows from Assumption 3.2.
The construction of f♯
rallows us to analyze the prediction error of the truncated kernel-based estimator
from two sources: the estimation error depending on the complexity of the truncated RKHS HKr,
and the approximation error arising from the dissimilarity between the truncated RKHS HKrand the
full RKHS HK. Specifically, we have the error decomposition as follows.
Error decomposition. By applying the elementary inequality that1
2(a+b)2≤a2+b2, the total
error∥bfλ,r−f∗∥2
ncan be decomposed as
1
2bfλ,r−f∗2
n≤bfλ,r−f♯
r2
n|{z}
Estimation error+f♯
r−f∗2
n|{z}
Approximation bias. (25)
26Note that both bfλ,randf♯
rbelong to Hn,r, allowing us to analyze the estimation error based on
the complexity of the reduced kernel matrix Kr. This decomposition successfully captures two
components of error: estimation error and approximation bias. The estimation error is controlled by
the model richness of the truncated space HKr, while approximation bias depends on the dissimilarity
between the truncated RKHS HKrand the full RKHS HKwhere the true target f∗is sitting in. A
larger ramplifies the space HKr, resulting in a larger estimation error. At the same time, it narrows
the gap between HKrandHK, thereby decreasing the approximation bias. Consequently, a trade-off
emerges, and an optimal choice of truncation raims to balance the estimation error and approximation
bias.
D.2 Proof of Theorem 4.2
We will separately bound from above each term appearing in the decomposition (25).
Bounding the approximation bias. Note that
f♯
r−f∗2
n=1
nq∗−√nKrα♯
r2
n
(i)=ξ∗−DrU⊤α♯
r2
2(ii)=ξ∗
1−D1U⊤
1α♯
r2
2+ξ∗
22
2,
where (i) follows from the eigen-expansion that Kr=UK rU⊤and (ii) follows from (22).
By the definition of α♯
r, we have
D1U⊤
1α♯
r=D1U⊤
1U1D−1
1ξ∗
1=ξ∗
1.
Therefore, we arrive at
f♯
r−f∗2
n=ξ∗
22
2=nX
j=r+1ξ∗
j2. (26)
Bounding the estimation error. Define the localized function class
Hn,r,b:=
f:f∈ H n,r,∥f−f♯∥K≤b	
,
where b >1is a constant independent of n, γ, which will be specified in the proof.
For given ι∈(0,1)andδ >0, define the auxiliary event
Vr(ι, δ) :=(bE(f)−bE(f♯
r)−[E(f)− E(f♯
r)]
δ−1∥f−f♯
r∥n+∥f−f♯
r∥Kr≤Clogι−1Rr(δ)holds for any f∈ H n,r,b)
.
To establish the bound for the estimation error, we need the following two lemmas.
Lemma D.1. For given ι∈(0,1)andδ >0, the event Vr(ι, δ)occurs with probability greater than
1−ι, i.e.
P(Vr(ι, δ))≥1−ι.
Recall that δn,ris the critical radius w.r.t. the truncated kernel complexity function defined as the
smallest solution to (3).
Lemma D.2. Letη= min {γ,1}. On the event Vr(ι, δn,r), with the choice of λsatisfying
max{δ2
n,r,Pn
j=r+1ξ∗
j2} ≤λ≤1, we have
bfλ,r−f♯
r2
n≤C
δ4η
n,r+λ2η+nX
j=r+1ξ∗
j2
,
where Cis a constant independent of n, γ.
27By applying Lemma D.1, we have
P(Vr(ι, δn,r))≥1−ι,
which, together with Lemma D.2, implies
bfλ,r−f♯
r2
n≤C
δ4η
n,r+λ2η+nX
j=r+1ξ∗
j2
.
holds with probability at least 1−ι.
Finally, by applying the error decomposition (25) and the equality (26), one has
bfλ,r−f∗2
n≤2bfλ,r−f♯
r2
n+ 2f♯
r−f∗2
n
≤C
δ4η
n,r+λ2η+nX
j=r+1ξ∗
j2
,
which completes the proof for the L(Pn)-error. For the excess risk, it immediately follows from
Assumption 3.1.
D.2.1 Proof of Lemma D.1
Denote
Dr=bE(f)−bE(f♯
r)−
E(f)− E(f♯
r)
.
Similar to the proof for Lemma C.2, it is equivalent to proving that
Ar:= sup
f∈Hn,r,b|Dr|
δ−1∥f−f♯
r∥n+∥f−f♯
r∥Kr≤Clogι−1Rr(δ).
From (24), we find that for any f∈ H n,r,b
∥f∥Kr≤f−f♯
r
Kr+f♯
r
Kr≤u+b.
Similar to that in Section C.2.1, we let ˜b:=u+b, and L(y,·)satisfies the Lipschitz continuity over
the function class Hn,r,b with Lipschitz constant ML,˜b. We write M:=ML,˜bfor short.
Bounding E[Ar].Following a similar treatment as that in (10), by using the Lemma G.2 and Lemma
G.3, we have
E[Ar] =Eh
sup
f∈Hn,r,b|Dr|
δ−1∥f−f♯
r∥n+∥f−f♯
r∥Kri
≤2√
2πM
nEh
sup
f∈Hn,r,bPn
i=1wi(f(xi)−f♯
r(xi))
δ−1∥f−f♯
r∥n+∥f−f♯
r∥Kri
.(27)
Let
Fr(δ) :=n
f=S⊤
x,r(α) :f−f♯
r
Kr≤1,f−f♯
r
n≤δ,α∈ Rno
.
For any f∈ H n,r,b, there exists α∈ Rnsuch that f=S⊤
x,r(α). Recall that f♯
r=S⊤
x,r(α♯
r).
Define the vector β:=DrU⊤(α−α♯
r), then f∈ F(δ)is equivalent to the constraints on βrthat
∥D−1/2
rβ∥2≤1and∥β∥2≤δ.
From (22), the last n−relements of βare all zero, then any vector satisfying these two constraints
must belong to the ellipse class
Er:=n
β∈ Rn:rX
j=1β2
j
νj≤2withνj= min {δ2, µj}o
.
28Denote w= (w1, ..., w n)⊤, we have
Eh
sup
f∈Fr(δ)nX
i=1wi(f(xi)−f♯
r(xi))i
=Eh
sup
β∈Er√n⟨w,Uβ⟩i
=Eh
sup
β∈Er√n⟨w,β⟩i
(i)
≤Eh
sup
β∈Er√nvuutrX
j=1β2
j
νjvuutrX
j=1νjw2
ji
≤√
2nvuut1
nrX
j=1νj=√
2nRr(δ),
where (i) follows from Cauchy-Schwarz inequality and the fact that the last n−relements of βare
all zero.
Similar to the argument in the proof for Lemma C.2, by appropriately scaling, we obtain
E
Ar
≤2√
2πM
nEh
sup
f∈Fr(δ)nX
i=1wi(f(xi)−f♯
r(xi))i
≤4√πMR r(δ). (28)
Bounding Ar−E[Ar].For each i∈[n]and any f∈ Fr(δ), define sj= sign( f(xi)−f♯
r(xi))if
j=iandsj= 0ifj̸=iand let s= (s1, ..., s n)⊤, then we have
|f(xi)−f♯
r(xi)|=nX
i=1si(f(xi)−f♯
r(xi))
=√n⟨s,Uβ⟩ ≤√nvuutrX
j=1β2
j
νjvuutrX
j=1νjs2
j≤√nvuutrX
j=1β2
j
νjvuutrX
j=1νj=√
2nRr(δ).
Consequently, for each i∈[n], we have
L(yi, f(xi))−L(yi, f♯
r(xi))≤M|f(xi)−f♯
r(xi)| ≤M√
2nRr(δ).
In addition, note that
1
nnX
i=1Eh 
L(yi, f(xi))−L(yi, f♯
r(xi))2i
≤M2
nnX
i=1(f(xi)−f♯
r(xi))2=M2∥f−f♯
r∥2
n
=M2⟨Uβ,Uβ⟩=M2⟨β,β⟩ ≤M2max
i∈[r]νjrX
i=1β2
j
νj≤2M2rX
i=1νj= 2M2nR2
r(δ).
By a similar rescaled method, we haveL(yi, f(xi))−L(yi, f♯
r(xi))
δ−1∥f−f♯
r∥n+∥f−f♯
r∥Kr≤M|f(xi)−f♯
r(xi)|
δ−1∥f−f♯
r∥n+∥f−f♯
r∥Kr
=M|ef(xi)−f♯
r(xi)| ≤M√
2nRr(δ),
and
1
nnX
i=1EhL(yi, f(xi))−L(yi, f♯
r(xi))
δ−1∥f−f♯
r∥n+∥f−f♯
r∥Kr2i
≤M2
nnX
i=1f(xi)−f♯
r(xi)
δ−1∥f−f♯
r∥n+∥f−f♯
r∥Kr2
=M2
nnX
i=1(ef(xi)−f♯
r(xi))2
= 2M2nR2
r(δ).
29Thus, the requirements in Lemma G.1 hold with η=M√
2nRr(δ)andζ2= 2M2nR2
r(δ). Then,
by applying Lemma G.1, for any ι∈(0,1), lett=q
1
nlogι−1, it holds at with probability at least
1−ιthat
Ar−E(Ar)≤r
logι−1
n
4M2nR2r(δ) + 4M√
2nRr(δ)E(Ar)
+2√
2M
3Rr(δ)logι−1
(i)
≤eClogι−1Rr(δ),(29)
whereeC=q
(4M2+ 16√
2πM2) +2√
2M
3and(i)follows from (28).
Therefore, by combining (28) and (29), it holds at with probability at least 1−ιthat
Ar≤Clogι−1Rr(δ), (30)
where C=q
(4M2+ 16√
2πM2) +2√
2M
3+ 4√πM.
This completes the proof of Theorem 4.2. □
D.2.2 Proof of Lemma D.2
Denote
∆r=c0
4δ4η
n,r+ 4u2λ2η+ (c′
0+c0)nX
j=r+1ξ∗
j2.
On the event Vr(ι, δn,r), we claim that
bfλ,r−f♯
r2
n≤C∆r.
By the optimality of bfλ,rand the feasibility of f♯
r, we have
bE(bfλ,r)−bE(f♯
r) +λ∥bfλ,r∥2
Kr−λ∥f♯
r∥2
Kr≤0.
Define the function class
Gr:=n
f∈ H n,r:f−f♯
r2
n≤C∆r,f−f♯
r
K≤bo
.
By following a similar argument as that in the proof of Lemma C.2, it suffices to prove that
bE(efr)−bE(f♯
r) +λ∥efr∥2
K−λ∥f♯
r∥2
K>0, (31)
whereefris some function belonging to the boundary of Gr.
It follows from Assumption 3.1 that
c0efr−f∗2
n≤ E(efr)− E(f∗)
and
E(f♯
r)− E(f∗)≤c′
0f♯
r−f∗2
n.
Then, we have
c0efr−f♯
r2
n−(2c′
0+ 2c0)f♯
r−f∗2
n
(i)
≤2c0efr−f∗2
n−2c′
0f♯
r−f∗2
n
≤2E(efr)− E(f∗)−2(E(f♯
r)− E(f∗))
= 2 
E(efr)− E(f♯
r)
, (32)
where (i) uses the elementary inequality.
Note that (32) establishes a connection between the excess risk and L(Pn)-norm at f♯
r, which allows
us to prove Lemma D.2 by using a similar argument as the proof for Lemma C.2.
30Below we separately consider two cases: (i) ∥efr−f♯
r∥2
n=C∆rand∥efr−f♯
r∥Kr≤b; and (ii)
∥efr−f♯
r∥2
n≤C∆rand∥efr−f♯
r∥Kr=b.
Case (i): On the event Vr(ι, δn,r), we have
bE(f♯
r)−bE(efr)
≤ E(f♯
r)− E(efr) +Clogι−1Rr(δn,r)
δ−1
n,refr−f♯
r
n+efr−f♯
r
Kr
(i)
≤ −c0
2efr−f♯
r2
n+ (c′
0+c0)f♯
r−f∗2
n+Clogι−1Rr(δn,r)
δ−1
n,refr−f♯
r
n+efr−f♯
r
Kr
(ii)=−c0
2C∆r+ (c′
0+c0)nX
j=r+1ξ∗
j2+Clogι−1Rr(δn,r)
δ−1
n,rp
C∆r+efr−f♯
r
Kr
,
where (i) follows from (32), and (ii) follows from ∥efr−f♯
r∥2
n=C∆rand (26).
Recall that δn,rsatisfies
Clogι−1Rr(δn,r)≤c0
2δ2η+1
n,r
and we choose λsatisfying λ≥δ2
n,r. Following a similar argument as the proof for Lemma C.2, we
have
bE(f♯
r)−bE(efr)≤ −c0
2C∆r+ (c′
0+c0)nX
j=r+1ξ∗
j2+c0
2δ2η
n,rp
C∆r+c0
4δ4η
n,r+λ
4efr−f♯
r2
Kr.
Sinceefr∈ H n,r, there exists eαr∈ Rnsuch that
efr=S⊤
x,r(eαr).
Note that
λf♯
r2
Kr−λefr2
Kr=−2λ⟨f♯
r,efr−f♯
r⟩Kr−λefr−f♯
r2
Kr. (33)
Following the similar treatment as that in the proof for Lemma C.2 with Kreplaced by Krandeα
replaced by eαr, we have
λ⟨f♯
r,efr−f♯
r⟩Kr≤uλη+1
2efr−f♯
r
Kr+uληefr−f♯
r
n
=uλη+1
2efr−f♯
r
Kr+uληp
C∆r. (34)
Put the pieces together, and repeat the similar argument as that in the proof for Lemma C.2, for all
γ≥1
2, we obtain
bE(f♯
r)−bE(efr) +λ∥f♯
r∥2
Kr−λ∥efr∥2
Kr
≤ −c0
2C∆r+ (c′
0+c0)nX
j=r+1ξ∗
j2+c0
2δ2η
n,r+ 2uληp
C∆r+c0
4δ4η
n,r+ 4u2λ2η
and for sufficiently large C, we have
bE(f♯
r)−bE(efr) +λ∥f♯
r∥2
Kr−λ∥efr∥2
Kr<0.
31Case (ii) : Repeat the similar argument as that in Case (i), on the event Vr(ι, δn,r)and by Assumption
3.1, we have
bE(f♯
r)−bE(efr)
≤ −c0
2efr−f♯
r2
n+ (c′
0+c0)nX
j=r+1ξ∗
j2+Clogι−1Rr(δn,r)
δ−1
n,refr−f♯
r
n+efr−f♯
r
Kr
≤(c′
0+c0)nX
j=r+1ξ∗
j2+Clogι−1Rr(δn,r)
δ−1
n,refr−f♯
r
n+efr−f♯
r
Kr
≤(c′
0+c0)nX
j=r+1ξ∗
j2+c0
2δ2η
n,rp
C∆r+c0
4δ4η+2
n,rλ−1+λ
4efr−f♯
r2
Kr
≤(c′
0+c0)nX
j=r+1ξ∗
j2+c0
2δ2η
n,rp
C∆r+c0
4δ4η
n,r+λ
4efr−f♯
r2
Kr,
where the last step holds with the choice of λsatisfying λ≥δ2
n,r.
Combine with (33) and (34) and by applying the elementary inequality to obtain
bE(f♯
r)−bE(efr) +λ∥f♯
r∥2
Kr−λ∥efr∥2
Kr
≤(c′
0+c0)nX
j=r+1ξ∗
j2+c0
2δ2η
n,r+ 2uληp
C∆r+c0
4δ4η
n,r+ 4u2λ2η−λ
2efr−f♯
r2
Kr.
By our choice that 0< λ≤1,λ≥max{δ2
n,r,Pn
j=r+1ξ∗
j2}, we have
λ≥λ2η, λ≥δ4η
n,r,and λ≥nX
j=r+1ξ∗
j2.
Note that in this case, ∥efr−f♯
r∥2
Kr=b2. Therefore, repeat the similar argument as that in the proof
of Lemma C.2, for a sufficiently large constant b, the RHS of the above inequality is less than 0.
By combining Cases (i) and (ii), on the event Vr(ι, δn,r), we have
bfλ,r−f♯
r2
n≤C∆r≲δ4η
n,r+λ2η+nX
j=r+1ξ∗
j2,
which completes the proof. □
D.3 Proof of Corollary 4.3
From (26)
f♯
r−f∗2
n=nX
j=r+1ξ∗
j2.
For the approximation bias, according to the polynomial assumption that ξ∗
j≍j−2γα−1, we have
∥f♯
r−f∗∥2
n=nX
j=r+1ξ∗
j2≤CnX
j=r+1j−2γα−1≤CZ∞
rt−2γα−1dt≤Cr−2γα.
For the estimation error, we control the truncated kernel function first. Similar to the equality (20) for
R(δ), ifr > d (δ), we have
Rr(δ) =vuut1
nd(δ)δ2+1
nrX
d(δ)+1µj.
32Moreover, for the regular kernel class, we have Rr(δ)≍p
d(δ)δ2/n. Ifr≤d(δ), we have
Rr(δ)≍p
rδ2/n. Combining these two results, we have
Rr(δ)≍r
1
nmin{r, d(δ)}δ2. (35)
Next, we split the remaining proof by considering two cases: (i)1
2≤γ≤1; and (ii) γ >1.
Case (i): Recall that for the eigenvalues of Kthat satisfy µj≍j−α, we have d(δ)≍δ−2/α.
In addition, we notice that in this case, the best truncation level rto balance δ4ηandr−2γαis
r≍d(δ). This means that whatever ris, we always have Rr(δ)≍q
1
nd(δ)δ2. Hence, the kernel
complexity remains the same, and to avoid introducing additional approximation bias, the best choice
of truncation level turns out to be r=n. Then, following a similar argument as that in Section C.3,
we have δ2
n,r≤C (logι−1)2
nα
2γα+1. Choosing the optimal parameter of λ≍δ2
n,ryields
E(bfλ,r)− E(f∗)≍bfλ,r−f∗2
n≤C(logι−1)2
n2γα
2γα+1.
Case (ii): In this case, the best truncation level rto balance δ4ηandr−2γαisr≍δ−2/(γα), which
implies r≲d(δ)and
Rr(δ)≍r
1
nrδ2≍r
1
nδ2αγ−2
αγ.
Therefore, the solution to the inequality (3) can be upper bounded by the solution to
Clogι−1r
1
nδ2αγ−2
αγ≤c0
2δ3.
Solving this inequality yields
δ2
n,r≤C(logι−1)2
nγα
2γα+1,
and we thus obtain
r≍n
(logι−1)2 1
2γα+1.
The desired upper bound in the case γ >1follows by choosing λ≍δ2
n,r≍r−2γα. By combining
these two cases, we complete the proof. □
E Proof of Theorem 4.4
We consider the special case that the data is generated according to the mean regression model
Yi=f∗(xi) +εiwith εi∼N(0,1)
for each i∈[n]. For this mean regression model, f∗is the minimizer of the population risk E(f)
with squared loss specified.
For any δ >0andγ≥1
2, define the ellipse class
Eγ(δ) :=

ξ= (ξ1, ..., ξ n)⊤∈ Rn:nX
j=1ξ2
j
(min{δ2, µj})2γ≤u2

.
Forξ∈ Rn, define the rescaled norm
∥ξ∥2
Eγ:=nX
j=1ξ2
j
(min{δ2, µj})2γ.
Then, it is equivalent to write
Eγ(δ) =n
ξ= (ξ1, ..., ξ n)⊤∈ Rn:∥ξ∥2
Eγ≤u2o
.
33Recall that the statistical dimension is defined as
d(δ) = min
j∈[n] :µj≤δ2	
.
Our main proof is based on the following lemma that states a result concerning metric entropy.
Lemma E.1. For any δ >0andγ≥1
2, there is a collection of1
2δ2γ-separated points {ξ1, ..., ξM}
inEγ(δ)such that logM≥1
32d(δ).
By using Lemma E.1, there exists a1
2δ2γ-separated collection of points {ξ1, ..., ξM}inEγ(δ)such
thatlogM≥1
32d(δ). Given {ξ1, ..., ξM}, we construct f1, ..., fMasfi=S⊤
x(U D−1ξi). Note
that the TA scores corresponding to fiare given by
U⊤Sx(fi) =U⊤K U D−1ξi=ξi.
Hence, {ξ1, ..., ξM} ⊂ E γ(δ)implies fi∈ Hb
Kfor each i∈[M]. Moreover, we have
∥fi−fj∥2
n=∥D U⊤(U D−1ξi−U D−1ξj)∥2
2=∥ξi−ξj∥2
2≥δ4γ
4,
which implies that {f1, ..., fM}is1
2δ2γ-separated in Hb
K.
Since ξi∈ Eγ(δ), we also have
∥fi∥2
n=∥ξi∥2
2=nX
k=1ξi
k2=δ4γnX
k=1ξi
k2
δ4γ≤u2δ4γ.
Therefore, by using the triangle inequality, we have
∥fi−fj∥2
n≤2u2δ4γfor each i, j∈[M].
Letρkbe the underlying distribution of the collected data {(xi, yi)}n
i=1corresponding to fk. Then,
there holds
KL(ρi∥ρj)(i)=nX
i=1KL(N(fi(xi),1)∥N(fj(xi),1))(ii)=n
2∥fi−fj∥2
n≤u2nδ4γ
where KL(·∥·)denotes the KL divergence between two distributions, (i) follows from the fact that
KL(P1⊗P2∥Q1⊗Q2) =KL(P1∥Q1) +KL(P2∥Q2)and⊗denoting the product measure, and (ii)
follows from the fact KL (N(µ1, σ2)∥N(µ2, σ2)) =(µ1−µ2)2
2σ2.
Below is devoted to establishing the minimax lower bound by applying the standard Fano’s method
(see, for instance, Proposition 15.12 in Wainwright (2019)). To be specific, for δ >0and for any
estimator efbased on the data {(xi, yi)}n
i=1, we have
inf
efsup
f∗∈Hb
KP
∥ef−f∗∥2
n≥δ4γ
4
≥1−max 1≤i,j≤MKL(ρi∥ρj) + log 2
logM
≥1−u2nδ4γ+ log 2
d(δ)/32. (36)
Below we separately consider two cases: i)1
2≤γ≤1; and ii) γ >1.
Case i: For1
2≤γ≤1, we take δ= (4c)1
4γδn, where δnis the critical radius defined as the smallest
solution to (2) in Section 3. Plugging into (36) yields
inf
efsup
f∗∈Hb
KP
∥ef−f∗∥2
n≥cδ4γ
n
≥1−4cu2nδ4γ
n+ log 2
d((4c)1
4γδn)/32
≥1−4cu2nδ4γ
n+ log 2
dn/32, (37)
34where dn=d(δn)and the last inequality holds since d(δ)is decreasing as δgrows and (4c)1
4γδn≤δn
for sufficiently small c.
Recall that for1
2≤γ≤1,δnis smallest solution to
Clogι−1R(δ)≤c0
2δ2γ+1.
Moreover, for the regular kernel, we have R(δ)≍q
1
nd(δ)δ2, which implies
δ2γ+1
n≲r
1
nd(δn)δ2n=r
1
ndnδ2n.
Then, dn≥c1nδ4γ
nfor some universal constant c1. Plugging this inequality into (37) yields
inf
efsup
f∗∈Hb
KP
∥ef−f∗∥2
n≥cδ4γ
n
≥1−4cu2nδ4γ
n+ log 2
c1nδ4γ
n≥1
2,
where the last step holds for sufficiently small c.
Case ii: Forγ >1, we take δ= (4c)1
4γδ1/γ
n,r, where δn,ris the critical radius defined as the smallest
solution to (3) in Section 4. It follows from (36) that
inf
efsup
f∗∈Hb
KP
∥ef−f∗∥2
n≥cδ4
n,r
≥1−4cu2nδ4
n,r+ log 2
d((4c)1
4γδ1/γ
n,r)/32
≥1−4cu2nδ4
n,r+ log 2
d(δ1/γ
n,r)/32.
Recall that for γ >1,δn,ris smallest solution to
Clogι−1Rr(δ)≤c0
2δ3.
According to (35), if r≤d(δ), we have
Rr(δ) =r
1
nrδ2.
Hence, we have
δ3
n,r≲r
1
nrδ2=r
1
nrδ2,
which leads to r≥c2nδ4
n,rfor some universal constant c2. Then, if we choose r≍d(δ1/γ
n,r)satisfying
d(δ1/γ
n,r)≤d(δn,r), we have
inf
efsup
f∗∈Hb
KP 
∥ef−f∗∥2
n≥cδ4
n,r
≥1−4u2cnδ4
n,r+ log 2
Cc2nδ4n,r/32≥1
2,
where the last step holds for sufficiently small c. This completes the proof of Theorem 4.4. □
Remark E.2.In the proof for the lower bound, we consider the Gaussian noise case. However, for
the upper bound, we require Y ⊂[−U, U]when the squared loss is specified. The bounded range
assumption essentially requires the random noise to be uniformly bounded. Nevertheless, the upper
bound established in this paper can be also extended to the sub-Gaussian noise case with a slight order
sacrifice of some logfactors in the upper bound. Specifically, suppose that {εi}n
i=1are i.i.d. sub-
Gaussian variables: that is, there exist positive constants c, σ2such that P 
|εi|> t
≤cexp(−σ2t2)
for all t≥0. Then, by the union bound, we have
P 
max
i=1,...,n|εi|> t
≤Pn[
i=1
|εi|> t	
≤nX
i=1cexp(−σ2t2) =cnexp(−σ2t2).
35Consequently, for any ι∈(0,1), by taking t=σ−1q
log cn
ι
, it holds with probability at least
1−ιthat
max
i=1,...,n|εi| ≤σ−1r
logcn
ι
≲r
log1
ι+p
logn.
Further note that by the reproducing kernel property and our assumption that supx,x′K(x,x′)≤κ2,
we have that for any x∈ X
|f∗(x)|=|⟨f∗, K(x,·)⟩K| ≤ ∥f∗∥K∥K(x,·)∥K≤κ∥f∗∥K. (38)
Therefore, for the sub-Gaussian noise case, we can immediately complete the proof by replacing U
withUι,n=κ∥f∗∥K+C(q
log1
ι+√logn). As a result, the upper bound for the sub-Gaussian
noise case will align with that for the uniform bounded noise case up to some logfactors.
E.1 Proof of Lemma E.1
Lemma E.1 states a result concerning metric entropy, and its proof is motivated by that of Lemma 4
in Yang et al. (2017), which only focuses on the just-aligned regime γ=1
2.
For each j∈[M], let
ξj= 
δ2γ
p
2d(δ)wj
1,δ2γ
p
2d(δ)wj
2, ...,δ2γ
p
2d(δ)wj
d(δ),0, ...,0!⊤
, (39)
where
w1= (w1
1, ..., w1
d(δ))⊤, ...,wM= (wM
1, ..., wM
d(δ))⊤∼N(0,Id(δ))
are a collection of independent standard Gaussian vectors. We claim that with a probability greater
0, we can find a set {ξ1, ..., ξM}generated in the above manner that are δ2γ-separated in E(δ)and
M≥e1
32d(δ).
On one hand, to show that {ξ1, ..., ξM} ⊂ E γ(δ), we need to equivalently prove ∥ξi∥2
Eγ≤u2for each
i∈[M]. Indeed, for each index i∈[M], since δ2≤µjfor each j≤d(δ), we have ∥ξi∥2
Eγ=∥wi∥2
2
2d(δ).
Note that ∥wi∥2
2∼χ2
d(δ). Then, by using the tail bound for chi-square distribution (Example 2.11 in
Wainwright (2019)), we have
P
∥ξi∥2
Eγ≤u2
=P1
d(δ)∥wi∥2
2−1≤2u2−1
≥P1
d(δ)∥wi∥2
2−1≤7
≥1−e−49d(δ)
8, (40)
where we use the assumption that u≥2. By applying the union bound, we have
P
ξi∥2
Eγ≤u2for all i∈[M]
≥1−Me−49d(δ)
8. (41)
On the other hand, note that ∥ξi−ξj∥2
2=δ4γ
2d(δ)∥wi−wj∥2
2. Since wiandwjare independent, we
have(wi−wj)/√
2∼N(0,Id(δ)). Then, similar to the inequality (40), we also have
P
∥ξi−ξj∥2
2≥δ4γ
4
=P1
2d(δ)∥wi−wj∥2
2≥1
4
=P1
2d(δ)∥wi−wj∥2
2−1≥ −3
4
≥1−e−9d(δ)
128,
and by applying the union bound, we have
P
∥ξi−ξj∥2
2≥δ4γ
4for all i, j∈[M]
≥1−M2e−9d(δ)
128. (42)
36Combining (41) and (42) yields
P
∥ξi∥2
Eγ≤u2and∥ξi−ξj∥2
2≥δ4γ
4for all i, j∈[M]
≥1−Me−49d(δ)
8−M2e−9d(δ)
128,
where the left side is positive by setting logM=d(δ)/32.
We thus conclude the statement in Lemma E.1. □
F More discussions on Assumption 3.1
As discussed in Section 3, Assumption 3.1 is a relatively mild condition for many widely used loss
functions. It is clear that the squared loss satisfies Assumption 3.1 with c0=c′
0= 1. For the Huber
lossLτ(y, f(x)) = ( y−f(x))2if|y−f(x)| ≤τ, and τ|y−f(x)| −1
2τ2otherwise, since it is
locally equivalent to the squared loss function, thus it satisfies Assumption 3.1 under some mild tail
conditions on the noise term Y−f∗(x)(Wainwright, 2019).
For the Hinge loss L(y, f(x)) = max {1−yf(x),0}that is designed for the margin-based classi-
fication problem, as mentioned in (Wainwright, 2019), whether Assumption 3.1 holds hinges on
the distribution of the covariates x, and the hypothesis function class F. We remark that for the
classification problem, the theoretical guarantee for 0-1loss is also crucial. Once the 0-1loss is
considered, one possible routine for establishing the theoretical results for 0-1loss is to follow a
similar technical treatment as that on Page 17 of Lai et al. (2024) with some slight modifications,
where the bridge between the excess risk w.r.t 0-1loss and mean squared error is established, and
based on the result in Lai et al. (2024), the excess risk only gets a slower rate compared to the rates
established in our paper.
For other loss functions, including the check loss, Logistic loss, and exponential loss, we provide
a more detailed discussion and deduce some sufficient conditions to ensure the satisfaction of
Assumption 3.1.
F.1 Check loss
Letρτ(t) =t(τ−I{t≤0})andLτ(y, f(x)) =ρτ(y−f(x)). We next verify Assumption 3.1 if the
following assumption holds.
Assumption F.1. Denote FY|X=xbe the conditional distribution on Ygiven X=x. We assume
that there exist two constants c0, c′
0such that
2c0|y| ≤FY|X=xi(f∗(xi) +y)−FY|X=xi(f∗(xi))≤2c′
0|y|.
Proposition F.2. Under Assumption F .1, for any b >0, both the local c0-strong convexity and the
local c′
0-smooth condition are satisfied.
Proof. For each i∈[n], denote w=Y−f∗(xi)andv=f(xi)−f∗(xi). By using Knight’s
identity (Equation B.3 in Belloni & Chernozhukov (2011)) that ρτ(w−v)−ρτ(w) =−v(τ−
1{w≤0}) +Rv
0(1{w≤t}−1{w≤0})dt, we have
E
ρτ(Y−f(xi))−ρτ(Y−f∗(xi))
=−E[(f(xi)−f∗(xi))(τ−1{Y≤f∗(xi)})] +EhZf(xi)−f∗(xi)
0(1{Y≤f∗(xi)+t}−1{Y≤f∗(xi)})dti
.
(43)
Recall the definition of f∗, we have
E[(f(xi)−f∗(xi))(τ−1{Y≤f∗(xi)})] = ( f(xi)−f∗(xi))E[(τ−1{Y≤f∗(xi)})] = 0 .
37Now we consider the second term in the right hand of (43). It follows from Fubini’s theorem that
EhZf(xi)−f∗(xi)
0(1{Y≤f∗(xi)+t}−1{Y≤f∗(xi)})dti
=Zf(xi)−f∗(xi)
0E[1{Y≤f∗(xi)+t}−1{Y≤f∗(xi)}]dt
=Zf(xi)−f∗(xi)
0 
FY|X=xi(f∗(xi) +t)−FY|X=xi(f∗(xi))
dt.
According to Assumption F.1, there holds
Zf(xi)−f∗(xi)
0 
FY|X=xi(f∗(xi) +t)−FY|X=xi(f∗(xi))
dt
≥Zf(xi)−f∗(xi)
02c0|t|dt=c0(f(xi)−f∗(xi))2,
and
Zf(xi)−f∗(xi)
0 
FY|X=xi(f∗(xi) +t)−FY|X=xi(f∗(xi))
dt
≤Zf(xi)−f∗(xi)
02c′
0|t|dt=c′
0(f(xi)−f∗(xi))2.
Then, we have
c0(f(xi)−f∗(xi))2≤EhZf(xi)−f∗(xi)
0(1{Y≤f∗(xi)+t}−1{Y≤f∗(xi)})dti
≤c′
0(f(xi)−f∗(xi))2.
The desired conclusion immediately follows by summing from 1ton. This completes the proof of
Proposition F.2.
F.2 Logistic loss
The Logistic loss L(y, f(x)) = log(1 + exp( −yf(x)))is specified for the binary classification
problem, where the response ytakes values in {−1,1}. Simple algebra yields the first and second
derivatives of L(y, θ)in the second argument that
∂L
∂θ=−yexp(−yθ)
1 + exp( −yθ)
and
∂2L
∂θ2=y2
(exp(−yθ/2) + exp( yθ)/2)2.
It is clear that for any θ∈ R, we have
∂L
∂θ≤1and∂2L
∂θ2≤1
4,
implying that L(y,·)is1-Lipschitz continuous and the c′
0-local smoothness condition holds with
c′
0=1
4.
Recall from (38). Forf∈ H Kandx∈ X satisfying |f(x)−f∗(x)| ≤D, denote B=κ∥f∗∥K+D,
we have
∂2L
∂θ2
θ=f(x)≥1
e−B+eB+ 2,
implying the locally strong convexity condition holds with c0=1
e−B+eB+2.
38F.3 Exponential loss
The Exponential loss L(y, f(x)) = exp( −yf(x))is used in the AdaBoost algorithm designed for
the classification problem, where y∈ {− 1,1}. Note that the first and second derivatives of L(y, θ)
in the second argument is given by
∂L
∂θ=−ye−yθand∂2L
∂θ2=e−yθ.
For any θ∈ R, we have∂L
∂θ≤1, which implies that Lis1-Lipschitz continuous. For the locally
strong convexity condition and local smoothness condition, with a similar argument as that for the
Logistic loss, we have that
e−B≤∂2L
∂θ2≤eB.
This ensures that the local strong convexity condition holds with c0=e−Band the local smoothness
condition holds with c′
0=eB.
G Supporting Lemmas.
The following lemma presents Talagrand’s concentration inequality for random elements taking
values in some space Z(Bousquet, 2002; Lv et al., 2018). Detailed proofs can be found in Bousquet
(2002).
Lemma G.1 (Talagrand’s concentration inequality) .LetZ1, . . . , Z nbe independent random elements
taking values in some space Zequipped with norm ∥ · ∥. LetFbe a class of real-valued measurable
functions acting on Z. If we have
max
i∈[n]∥f(Zi)∥ ≤ηand1
nnX
i=1Var(f(Zi))≤ζ2,∀f∈ F,
define the empirical process Z:= supf∈F|1
nPn
i=1(f(Zi)−Ef(Zi))|, then for any t >0
P
Z≥E(Z) +tp
2 (ζ2+ 2ηE(Z)) +2ηt2
3
≤exp(−nt2).
The following lemma is known as the symmetrization technique, which provides a fundamental
tool to bound from above the expectation of the empirical process (Wainwright, 2019). A typical
version of the symmetrization lemma is to consider the Rademacher variables {ε1, ..., ε n}, i.e.
P(εi= 1) = P(εi=−1) =1
2(Proposition 4.11 in Wainwright (2019)). In our proof, we consider a
sequence of standard Gaussian variables to utilize the rotation invariance of the Gaussian vector.
Lemma G.2 (Symmetrization) .LetX1, ..., X nbe a sequence of random variables and w1, ..., w n∼
N(0,1)denote the standard Gaussian variables independent of X1, ..., X n. For any measurable
function class F, we have
Eh1
nsup
f∈FnX
i=1(f(Xi)−E[f(Xi)])i
≤√
2πEh1
nsup
f∈FnX
i=1wif(Xi)i
Proof. By applying Proposition 4.11 in Wainwright (2019), we have
Eh1
nsup
f∈FnX
i=1(f(Xi)−E[f(Xi)])i
≤2Eh1
nsup
f∈FnX
i=1εif(Xi)i
.
Therefore, it remains to prove that
Eh1
nsup
f∈FnX
i=1εif(Xi)i
≤rπ
2Eh1
nsup
f∈FnX
i=1wif(Xi)i
39Indeed, we have
Eεh1
nsup
f∈FnX
i=1εif(Xi)i(i)=rπ
2Eεh1
nsup
f∈FEw nX
i=1|wi|εif(Xi))i
(ii)
≤rπ
2Eεh1
nEwsup
f∈FnX
i=1|wi|εif(Xi)i
=rπ
2Eh1
nsup
f∈FnX
i=1|wi|εif(Xi)i
(iii)=rπ
2Eh1
nsup
f∈FnX
i=1wif(Xi)i
,
where we use Ewto denote taking expectation with respect to w1, ..., w nand use a similar notation
Eεfor taking expectation with respect to ε1, ..., ε n, (i) follows from E[|wi|] =q
2
π, (ii) uses Jensen’s
inequality and (iii) is due to the fact that wihas the same distribution with εiwifor each i. This
completes the proof of Lemma G.2.
The following lemma can be found in Wainwright (2019), which allows us to utilize the symmetriza-
tion technique for the Lipschitz function family.
Lemma G.3 (Gaussian contraction inequality) .For any set T ∈ Rd, and let {ϕj:R → R , j=
1, ..., d}be any family of M-Lipschitz functions such that ϕj(0) = 0 forj∈[d], we have
E
sup
θ∈TdX
j=1wjϕj(θj)
≤2ME
sup
θ∈TdX
j=1wjθj
.
H Additional Simulations
H.1 Assessing the Performance of the Truncated Kernel Method
In this section, we conduct a numerical investigation to assess the performance of the truncated
kernel method (TKM) and validate our theoretical results in the main text. The experimental design
consists of four distinct examples, including kernel quantile regression, kernel ridge regression, kernel
support machine, and kernel logistic regression. In each example, we consider both the univariate and
multi-dimensional cases. In specific, we apply Sobolev kernel K(x, x′) = min {x, x′}for univariate
cases and Laplacian kernel K(x,x′) = exp( −∥x−x′∥1)for multi-dimension cases. The elements
of multi-dimensional covariates are independently sampled from the normal distribution, while in the
univariate case, the covariate is sampled from the uniform distribution on [0,1]. Aligned with the
previous notation, TKM means truncated kernel-based method, and KM means standard kernel-based
method. All the experiments are repeated 50 times and all the tuning parameters are tuned to the best
for both methods. All experiments were conducted on the same hardware setup: Intel i9 13900K
CPU @ 2.20GHz with 128 GB memory.
Example 1 (Kernel quantile regression) .In this illustrative example, we begin by conducting a
comprehensive analysis of multivariate kernel quantile regression and postpone the univariate case to
the subsequent discussion. We consider the data-generating scheme that y=f0(x) +σ(ϵ−Φ−1(τ)),
where σ= 3,ϵ∼N(0,1)andΦdenotes CDF function of standard normal distribution. Here,
we set f0(x) = sin 2( x1+x2+x3)withx= (x1, x2, x3)⊤and vary the quantile level τfrom
{0.3,0.5,0.7}.
We first compare the numerical performance between TKM and KM in estimating the true function
f0under different sample sizes n. The averaged numerical results in terms of logarithmic mean
square error (MSE) and empirical excess risk are illustrated in Figure 2.
It is clear that under different quantile levels, TKM always outperforms KM. More interestingly, the
decline rate of TKM is significantly faster than that of KM, which validates our theoretical findings
that under the over-aligned regime γ >1, TKM achieves a faster learning rate than KM as illustrated
in Corollary 4.3.
40200 500 1000 1500 2000
n4
3
2
1
Log MSE
=0.3
KM
TKM
200 500 1000 1500 2000
n4
3
2
1
Log MSE
=0.5
KM
TKM
200 500 1000 1500 2000
n4
3
2
1
Log MSE
=0.7
KM
TKM
200 500 1000 1500 2000
n4
3
2
1
Log empirical excess risk
=0.3
KM
TKM
200 500 1000 1500 2000
n4
3
2
1
Log empirical excess risk
=0.5
KM
TKM
200 500 1000 1500 2000
n4
3
2
1
Log empirical excess risk
=0.7
KM
TKMFigure 2: Averaged log MSE and log empirical excess risk for KM and TKM under check loss with
varying sample size n.
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log MSE=0.3
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log MSE=0.5
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log MSE=0.7
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log empirical excess risk=0.3
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log empirical excess risk=0.5
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log empirical excess risk=0.7
TKM
Figure 3: Averaged log MSE and log empirical excess risk for KM and TKM under check loss with
varying truncation level rl= log( r/n).
In the following study, we fix the sample size as n= 500 to investigate how the numerical perfor-
mance of estimating f0(x)is affected by the truncation level rby varying the logarithmic ratio of the
truncation level rto the sample size n,rl= log( r/n). The averaged numerical results in terms of
logarithmic MSE and empirical excess risk are illustrated in Figure 3.
From Figure 3, we can see that the curves in all the scenarios have a steep decrease at first, then
turn to a gradual increase, and finally become stabilizing with little vibration. This confirms our
theoretical findings on the truncation level rand illustrates that a properly chosen truncation level is
necessary to boost the estimation accuracy of the truncated kernel-based method.
Now we demonstrate univariate simulation for kernel quantile regression. We assume the true
function to be f0(x) = sin(10 x)and underlying model to be y=f0(x) +σ(ϵ−Φ−1(τ)), where
σ= 3, ϵ∼N(0,1)andΦis CDF function of standard normal distribution.
As shown in Figures 4 and 5, for the univariate kernel quantile regression, we observe that, across
different quantiles and various performance measures such as MSE or empirical excess risk, TKM
consistently outperforms KM. This observation also confirms the advantages of TKM compared to
KM.
41200 500 1000 1500 2000
n3.5
3.0
2.5
2.0
1.5
1.0
0.5
Log MSE
=0.3
KM
TKM
200 500 1000 1500 2000
n3.5
3.0
2.5
2.0
1.5
1.0
0.5
Log MSE
=0.5
KM
TKM
200 500 1000 1500 2000
n3.5
3.0
2.5
2.0
1.5
1.0
0.5
Log MSE
=0.7
KM
TKM
200 500 1000 1500 2000
n3.5
3.0
2.5
2.0
1.5
1.0
0.5
Log empirical excess risk
=0.3
KM
TKM
200 500 1000 1500 2000
n3.5
3.0
2.5
2.0
1.5
1.0
0.5
Log empirical excess risk
=0.5
KM
TKM
200 500 1000 1500 2000
n3.5
3.0
2.5
2.0
1.5
1.0
0.5
Log empirical excess risk
=0.7
KM
TKMFigure 4: Averaged MSE and empirical excess risk for KQR and truncated KQR in the multivariate
case.
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log MSE=0.3
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log MSE=0.5
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log MSE=0.7
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log empirical excess risk=0.3
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log empirical excess risk=0.5
TKM
4
 2
 0
rl3.0
2.5
2.0
1.5
1.0
Log empirical excess risk=0.7
TKM
Figure 5: Averaged MSE and empirical excess risk vs rl= log( r/n); the sample size is set to
n= 500 .
Example 2 (Kernel ridge regression) .In the kernel ridge regression, we consider the model y=
f0(x) +ϵ, where f0(x) = sin( x), ϵ∼N(0,1)for univariate x. While for multi-dimension scenario,
we assume dimension p= 3 and we generate data from y= sin(2( x1+x2+x3)) +ϵ, where
ϵ∼N(0,0.5). Another setting is similar to quantile regression.
As shown in Figure 6, it can be observed that in both the univariate and multivariate scenarios, TKM
outperforms KM significantly. Furthermore, in multi-dimensional cases, the advantage of TKM is
even more pronounced. From the right panel, it can be seen that although the optimal value of r
differs, they all reach their minimum at a certain point within the range of [0,1]. This verifies our
theoretical conclusion.
Example 3 (Kernel support vector machine) .In the kernel support vector machine, we denote
the sign of xassign(x)and generate data through the model y= sign( f0(x) +ϵ). In univariate
casef0(x) = sin(10 x)and for the multi-dimensional counterpart f0(x) = 3 sin( x1+x2+x3).
ϵ∼N(0,1.5)in both case.
As shown in Figure 7, a similar trend can be observed in both methods, where TKM outperforms
KM under different sample sizes. Furthermore, the advantage of TKM becomes more pronounced in
42200 500 1000 1500 2000
n4.00
3.25
2.50
1.75
1.00
Log MSE
(a)KM
TKM
6.0
 4.5
 3.0
 1.5
 0.0
 lr2.5
2.0
1.5
1.0
0.5
Log MSE
(b)TKM
200 500 1000 1500 2000
n2.75
2.50
2.25
2.00
1.75
1.50
Log MSE
(c)KM
TKM
4
 2
 0
 lr2.00
1.75
1.50
1.25
1.00
0.75
Log MSE
(d)TKMFigure 6: Simulation for kernel ridge regression. (a) univariate covariate case: average accuracy
vsn. (b) univariate covariate case: average accuracy vs rl= log( r/n); the sample size is set to
n= 500 . (c) multivariate covariate case: average accuracy vs n. (d) multivariate covariate case:
average accuracy vs rl= log( r/n); the sample size is set to n= 500 .
200 500 1000 1500 2000
n0.000.050.100.150.20Classification error rate
(a)KM
TKM
6.0
 4.5
 3.0
 1.5
 0.0
 lr0.00.10.20.30.40.5Classification error rate
(b)TKM
200 500 1000 1500 2000
n0.100.150.200.250.300.35Classification error rate
(c)KM
TKM
4
 2
 0
 lr0.150.200.250.300.35Classification error rate
(d)TKM
Figure 7: Simulation for kernel support vector machine. (a) univariate covariate case: average
accuracy vs n. (b) univariate covariate case: average accuracy vs rl= log( r/n); the sample size is
set to 200. (c) multivariate covariate case: average accuracy for vs n. (d) multivariate covariate case:
average accuracy vs rl= log( r/n); the sample size is set to 200.
43200 500 1000 1500 2000
n0.000.050.100.150.20Classification error rate
(a)KM
TKM
5.00
 3.75
 2.50
 1.25
 0.00
 lr0.00.20.40.6Classification error rate
(b)TKM
200 500 1000 1500 2000
n0.100.150.200.250.30Classification error rate
(c)KM
TKM
4
 3
 2
 1
 0
 lr0.20.30.4Classification error rate
(d)TKMFigure 8: Simulation for kernel logistic regression. (a) univariate covariate case: average accuracy
vsn. (b) univariate covariate case: average accuracy vs rl= log( r/n); the sample size is set to
n= 100 . (c) multivariate covariate case: average accuracy vs n. (d) multivariate covariate case:
average accuracy vs rl= log( r/n); the sample size is set to n= 100 .
2 4 6 8 10
0.150.200.250.300.350.400.450.50Classification error rateKSVM
KM
TKM
2 4 6 8 10
0.8
0.6
0.4
0.2
Log empirical excess riskKSVM
TKM
KM
Figure 9: Kernel SVM; averaged classification error rate and log excess risk for KM and TKM versus
α.
multi-dimensional cases. As for the error curves against rl, it can be seen that both univariate and
multivariate scenarios reach their minimum value within the range of [0,1].
Example 4 (Kernel logistic regression) .In kernel logistic regression, we generate data from y∼
Bernoulli (p), where p=1
1+exp( −f0(x)), where f0(x) = sin(15 x)in univariate case and f0(x) =
3 sin( x1+x2+x3)for the multi-dimensional case.
As shown in Figure 8, its exhibited curve trend is similar to that of kernel support machines. This
validates that under different model assumptions, if a specific ris chosen, TKM performs much better
than KM.
H.2 SVM with varying Model Complexities
In this part, we aim to investigate the problem how once the hinge loss is specified (corresponding to
SVM), how the RKHS with varying model complexities affect the numerical performance of KM and
TKM. Specifically, the experiment setup is the same as that in Section H.1, including the selection
of kernel, repeat times, and tuning method for λandrexcept that the underlying true function is
set as f∗(x) = sin(11 x)and(xi, yi)300
i=1is independently drawn from yi=sign(f∗(xi) +N(0,4))
withxi=i−1
300, i= 1, . . . , 300. The obtained numerical results are reported in Figure 9. It is thus
clear from Figure 9 that the error curves for the hinge loss align with those for the check loss, which
further confirms our theoretical findings and also empirically supports that our theoretical analysis
can apply to SVM.
443.50
 3.15
 2.80
 2.45
 2.10
 1.75
 1.40
 1.05
 0.70
 0.35
 0.00
rl2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
Log MSE=0.3
KM
TKM
3.50
 3.15
 2.80
 2.45
 2.10
 1.75
 1.40
 1.05
 0.70
 0.35
 0.00
rl2.0
1.8
1.6
1.4
Log empirical excess risk=0.3
TKM
KM
3.50
 3.15
 2.80
 2.45
 2.10
 1.75
 1.40
 1.05
 0.70
 0.35
 0.00
rl2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
Log MSE=0.5
KM
TKM
3.50
 3.15
 2.80
 2.45
 2.10
 1.75
 1.40
 1.05
 0.70
 0.35
 0.00
rl1.8
1.6
1.4
1.2
Log empirical excess risk=0.5
TKM
KM
3.50
 3.15
 2.80
 2.45
 2.10
 1.75
 1.40
 1.05
 0.70
 0.35
 0.00
rl2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
Log MSE=0.7
KM
TKM
3.50
 3.15
 2.80
 2.45
 2.10
 1.75
 1.40
 1.05
 0.70
 0.35
 0.00
rl2.0
1.8
1.6
1.4
Log empirical excess risk=0.7
TKM
KMFigure 10: Kernel quantile regression; averaged log MSE and log empirical excess risk for KM and
TKM versus log ratios ( rl= log( r/n)) of the truncation level rto the sample size nacross different
quantile levels.
H.3 Exponential Decay Case
Note that our technical analysis can also cover the exponential decay case that µj≍exp(−αj)and
ξ∗
j2≍exp(−(2γα+β)j)withα, β > 0. Precisely, under the exponential decay setting, the explicit
upper bound of the approximation bias term can be derived by
nX
j=r+1ξ∗
j2≤CZ∞
rexp(−(2γα+β)t)dt=C
2γα+βexp(−(2γα+β)r).
Note that if r≥logn
(2γα+β), we always havePn
j=r+1ξ∗
j2≲1
n. Consequently, we can also derive the
corresponding convergence rates under these scenarios, which suggests that both TKM and KM can
attain an optimal rate whatever γis ifris greater than a certain threshold. We also conduct some
numerical experiments to verify this finding.
Specifically, the experimental setup is the same as Example 1 in Section H.1 except that we set
f∗(x) = sin(6 x)and the Gaussian kernel is used. The experiment result, presented in Figure 10,
shows that TKM initially performs worse than KM for the small value of r. Whereas, as rsurpasses
a threshold, TKM maintains comparable performance to KM. This observation precisely aligns with
our theory for the exponential decay scenario.
H.4 Determining rvia Cross-validation
Previously, all the tuning parameters were tuned to the best for both competitors. In this part, we will
also provide the numerical experiment with the tuning parameters selected in a data-driven fashion.
Specifically, we consider the kernel quantile regression that the data is independently generated
from the model y=f∗(x) +√
2(ε−Φ−1(τ))withf∗(x) = sin(6 x),x= 0,1
n, . . . ,n−1
n, and
ε∼N(0,1). In this experiment, we use the Laplacian kernel K(x,x′) = exp( −||x−x′||1), and
45the parameters randλare tuned by 5-fold cross-validation. The obtained numerical results using the
data-driven choice of rare attached in the following tables. Clearly, it can be observed that TKM
consistently outperforms KM, which further confirms our theoretical findings that TKM can achieve
superior performance across various scenarios.
Table 3: Averaged MSE for different n(τ= 0.3).
n 100 200 300 400
KM 0.583±0.257 0.220±0.104 0.165±0.071 0.121±0.374
TKM 0.367±0.174 0.188±0.078 0.140±0.004 0.099±0.029
Table 4: Averaged Empirical excess risk for different n(τ= 0.3).
n 100 200 300 400
KM 0.323±0.039 0.208±0.040 0.175±0.036 0.155±0.059
TKM 0.289±0.066 0.192±0.060 0.161±0.021 0.128±0.018
Table 5: Averaged MSE for different n(τ= 0.5).
n 100 200 300 400
KM 0.246±0.137 0.177±0.129 0.096±0.032 0.114±0.069
TKM 0.195±0.087 0.153±0.133 0.075±0.033 0.079±0.042
Table 6: Averaged Empirical excess risk for different n(τ= 0.5).
n 100 200 300 400
KM 0.214±0.062 0.189±0.052 0.176±0.047 0.140±0.028
TKM 0.168±0.039 0.146±0.048 0.158±0.055 0.123±0.027
Table 7: Averaged MSE for different n(τ= 0.7).
n 100 200 300 400
KM 0.434±0.272 0.273±0.099 0.163±0.086 0.121±0.072
TKM 0.325±0.195 0.200±0.079 0.134±0.072 0.098±0.054
Table 8: Averaged Empirical excess risk for different n(τ= 0.7).
n 100 200 300 400
KM 0.178±0.053 0.201±0.050 0.145±0.032 0.119±0.026
TKM 0.159±0.053 0.155±0.053 0.122±0.018 0.106±0.023
46NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: See Abstract and Section 1.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See Section 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
47Justification: The assumptions are clearly stated in Assumptions 3.1,3.2, 3.4, and all proofs
are presented in Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: See Section 5 and Appendix H.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
48Answer: [Yes]
Justification: The code has been included in the supplemental material, and the data is
generated through simulation or is available in the UCI Machine Learning Repository.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See Section 5 and Appendix H.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: See Section 5 and Appendix H.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
49•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Our simulations were done on a regular laptop. The type of compute worker
and memory was described in Appendix H.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We are sure to preserve anonymity and there is no special consideration.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discussed the societal impacts of this paper in Section 6.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
50•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
51•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
52