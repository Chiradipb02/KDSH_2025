The Impact of Initialization
on LoRA Finetuning Dynamics
Soufiane Hayou
Simons Institute
UC Berkeley
hayou@berkeley.eduNikhil Ghosh
Dept of Statistics
UC Berkeley
nikhil_ghosh@berkeley.eduBin Yu
Dept of Statistics
UC Berkeley
binyu@berkeley.edu
Abstract
In this paper, we study the role of initialization in Low Rank Adaptation (LoRA)
as originally introduced in Hu et al. [19]. Essentially, to start from the pretrained
model as initialization for finetuning, one can either initialize Bto zero and A
to random (default initialization in PEFT package), or vice-versa. In both cases,
the product BA is equal to zero at initialization, which makes finetuning starts
from the pretrained model. These two initialization schemes are seemingly sim-
ilar. They should in-principle yield the same performance and share the same
optimal learning rate. We demonstrate that this is an incorrect intuition and that
the first scheme (initializing Bto zero and Ato random) on average yields better
performance compared to the other scheme. Our theoretical analysis shows that
the reason behind this might be that the first initialization allows the use of larger
learning rates (without causing output instability) compared to the second initial-
ization, resulting in more efficient learning of the first scheme. We validate our
results with extensive experiments on LLMs.
1 Introduction
The pretrain-finetune paradigm (e.g., [7, 9]) has revolutionized deep learning, replacing task-specific
models trained from scratch with finetuning of pretrained base models. These base models, trained
on generic unsupervised objectives, learn powerful features that can be rapidly adapted to down-
stream tasks. The most effective models are consistently the largest ones [14, 25], with state-of-
the-art models reaching hundreds of billions of parameters. While many such models are openly
available (e.g., Llama by Touvron et al. [38]), full finetuning remains computationally prohibitive
for most practitioners. This has led to parameter-efficient finetuning methods, including adapters
[11], prompt tuning [20], and (IA)3[24].
Low Rank Adaptation (LoRA) [19] has emerged as a leading parameter-efficient method, training
only low-rank adapter matrices added to pretrained weights, typically using Adam [3]. LoRA often
matches or exceeds full-finetuning performance [35, 39], though it may underperform on complex
generation tasks. While prior work has examined rank [31] and learning rate [44] hyperparame-
ters, initialization schemes remain understudied. This work provides experimental and theoretical
justification for choosing between seemingly equivalent initialization approaches.
In standard LoRA training, one of the two LoRA matrices is initialized with random values and the
other is initialized to zero (see Section 2.1). Recently, in Meng et al. [48] the authors proposed an al-
ternative initialization scheme to LoRA which uses the top singular vectors of the pretrained weights
as opposed to a random initialization and showed improved training on several tasks. To further im-
prove LoRA training with quantization, Li et al. [34] introduced a new method called LoftQ for
computing a better initialization for quantized training [27]. However, to the best of our knowledge,
38th Conference on Neural Information Processing Systems (NeurIPS 2024).there has not been any study concerning the random initialization in vanilla LoRA. Specifically, it is
not clear from prior work which of the two LoRA matrices should be initialized to be zero . Empirical
results by Zhu et al. [50] suggested that the two initialization schemes mentioned above yield simi-
lar performance, but it is not clear if the learning rate was well-tuned for each initialization scheme.
Our findings suggest that these two initialization schemes lead to fundamentally different finetuning
dynamics, and that one of these schemes generally yields better result compared to the other.
Pretrained
Weights
+Init[A]: Init[B]:InitOptimal
Learning Rate
Finetuning
DynamicsMore Efficient
Feature Learning  with
some instabilityStability with
suboptimal Feature
Learning
Init[A] (generally) leads to better  performance
Figure 1: Summary of our contributions in this paper: a de-
scription of the difference between the finetuning dynamics
when LoRA weights AandBare initialized with Init[A]
orInit[B] .LoRA Variations. Beyond alter-
ing the LoRA initialization scheme,
there have been a series of works
which try to address limitations of
vanilla LoRA using different varia-
tions. To further reduce the num-
ber of trainable parameters, LoRA-
FA [42] freezes the Amatrix which
leads to small performance loss while
reducing memory consumption by
up to 1.4 ×. The performance of
this training scheme is also investi-
gated in Zhu et al. [50]. VeRA [33]
freezes random weight tied adapters
and learns vector scalings of the in-
ternal adapter activations. LoRA-XS
[43] initializes the AandBmatri-
ces using the SVD of the pretrained
weights and trains a low-rank update
of the form BRA where Ris a train-
abler×rmatrix and B,Aare fixed.
NOLA [32] parametrizes the adapter
matrices to be linear combinations of
frozen random matrices and optimizes the linear coefficients of the mixtures. VB-LORA [46] shares
adapter parameters using a global vector bank. In order to improve the learning ability for more chal-
lenging finetuning tasks, Kalajdzievski [31] proposes a scaling rule for the scalar adapter multiplier
to unlock increased gains with higher adapter ranks. MoRA [45] learns high-rank updates while
still preserving parameter efficiency by applying hand-designed compress and decompress opera-
tions before and after a trainable adapter matrix. DoRA [47] decomposes the pretrained weight into
magnitude and direction components to allow for better training dynamics.
Contributions. We study the impact of Initialization in LoRA through a theory of large width
for neural networks. The core approach is to take the width of a neural network to infinity and
determine how the behavior of the limit depends on the choice of the hyperparameters, such as
the learning rate and initialization. This approach allows to derive principled scaling choices for
these hyperparameters such that desired properties (e.g. stable feature learning) are achieved as the
network size grows (see Appendix A.2 for more details). Examples of the infinite-width limit include
works on initialization (e.g. He et al. [4]), and training dynamics (e.g. [21]). Examples for the depth
limit include initialization strategies [6, 10, 30], and depth scaling (see e.g. [18, 23, 28, 29, 37,
41]). A similar strategy was used to derive scaling rules for the LoRA learning rate in Hayou et al.
[44] (LoRA +) that concluded that the learning rates for different LoRA matrices should be scaled
differently to ensure optimal feature learning. In this work we use the same approach to provide
a systematic comparison between two different random initialization schemes for vanilla LoRA
finetuning (using the same learning rate for the AandBmatrices). Using the notation Init[A] to
refer to the case where Ais initialized to random and Bto zero (as in [19]) and Init[B] for the
opposite, we show that Init[A] andInit[B] lead to fundamentally different training dynamics
(as shown in Figure 1):
1.Init[A] allows the use of larger learning rates compared to Init[B]
2.Init[A] leads to ‘internal instability’ where the features Az(for some input z) are large
but LoRA output BAz is small. This form of instability allows more efficient feature
learning. We identify a feature learning / stability tradeoff in this case.
23.Init[B] does not cause any instabilities but training is suboptimal ( Bis undertrained).
4. Empirical results confirm the theory and show that Init[A] generally leads to better per-
formance than Init[B] .
2 Setup and Definitions
We consider a general neural network model of the form


Yin(x) =Winx,
Yl(x) =Fl(Wl, Yl−1(x)), l∈[L],
Yout(x) =WoutYL(x),(1)
where x∈Rdis the input, L≥1is the network depth, (Fl)l∈[L]are mappings that define the layers,
andWl∈Rn×nare the hidden weights, where nis the network width , and Win, Woutare input
and output embedding weights.1This model will represent the pretrained model that will later be
finetuned on some new task.
To finetune a (large) pretrained model with a limited amount of computational resources, a popular
resource efficient approach is to use the LoRA finetuning method defined below.
Definition 1 (Low Rank Adapters (LoRA) from [19]) .To apply LoRA to a weight matrix W∈
Rn1×n2in the model, we constrain its update in the fine-tuning process by representing the latter
with a low-rank decomposition W=W∗+α
rBA. Here, only the weight matrices B∈Rn1×r,
A∈Rr×n2are trainable and the original pretrained weights W∗remain frozen. The rank r≪
min(n1, n2)andα∈Rare tunable constants.
As the width ngrows,2the network initialization scheme and the learning rate should be adapted to
avoid numerical instabilities and ensure efficient learning. For instance, the variance of the initial-
ization weights (in hidden layers) should scale like 1/nto prevent the pre-activations from blowing
up as we increase model width n(e.g., He initialization [4]). To derive proper scaling rules, a prin-
cipled approach consist of analyzing the statistical properties of key quantities in the model (e.g.
second moment of the pre-activations) as ngrows and then adjust the initialization variance, the
learning rate, and the architecture to achieve desirable properties in the limit n→ ∞ [5, 10, 13,
40]. We use this approach to study the effect of initialization on the feature learning dynamics of
LoRA in the infinite-width limit. For more details about the theory of scaling of neural networks,
see Appendix A.2.
Throughout the paper, we will be using asymptotic notation to describe the behaviour of several
quantities as the width ngrows. Note that the width nwill be the only scaling dimension of neural
network training which grows and all other scaling dimensions such as the LoRA rank r, number
of layers L, sequence length, number of training steps, etc., will be considered as fixed. We use the
following notation for the asymptotic analysis.
Notation. Given sequences cn∈Randdn∈R+, we write cn=O(dn), resp. cn= Ω( dn),
to refer to cn< κd n, resp. cn> κd n, for some constant κ > 0. We write cn= Θ( dn)if both
cn=O(dn)andcn= Ω( dn)are satisfied. For vector sequences cn= (ci
n)1≤i≤k∈Rk(for some
k >0), we write cn=O(dn)when ci
n=O(di
n)for all i∈[k], and same holds for other asymptotic
notations. Finally, when the sequence cnis a vector of random variables, convergence is understood
to be convergence in second moment ( L2norm).
2.1 Initialization of LoRA Adapters
The standard way to initialize trainable weights is to take an iid initialization of the entries Aij∼
N(0, σ2
A), Bij∼ N(0, σ2
B)for some σA, σB≥0(this includes initialization with zeros if σBorσA
1We use the same notation from Hayou et al. [44].
2The width in SOTA models is typically large, i.e. of width n >103.
3are set to 0).3. Due to the additive update structure of LoRA, we want to initialize the product BA
to be 0so that finetuning starts from the pretrained model [19]. This can be achieved by initializing
one of the weights AandBto0. If both are initialized to 0, no learning occurs in this case since this
is a saddle point and the parameter gradients will remain zero. Thus, we should initialize one of the
parameters AandBto be non-zero and the other to be zero. If we choose a non-zero initialization
forA, then following standard initialization schemes (e.g., He Init [4], LeCun Init [1]), one should
setσ2
A= Θ( n−1)to ensure Axdoes not explode for large n. This is justified by the Central Limit
Theorem (CLT). On the other hand, if we choose a non-zero initialization for B, one should make
sure that σ2
b= Θ( r−1) = Θ(1) . This leaves us with two possible initialization schemes:
•Init[A] :σ2
B= 0, σ2
A= Θ( n−1)(default initialization in LoRA [19]).
•Init[B] :σ2
B= Θ( r−1) = Θ(1) , σ2
A= 0.4
These two initialization achieve the goal of starting finetuning from the pretrained model. A priori,
it is unclear if there is a material difference between the two initialization schemes. Surprisingly,
as we will show later in this paper, these two initialization schemes lead to fundamentally different
training dynamics when model width is large.
2.2 LoRA Features
Notation. For a given LoRA layer in the network, we use Z to denote the input to that layer and ¯Z
for the output after adding the pretrained weights. More precisely, we can write the layer operation
as¯Z=W∗Z+α
rBAZ.
Our main analysis relies on a careful estimation of the magnitude of several quantities involving
LoRA features . Let us first give a formal definition.
Definition 2 (LoRA Features) .Given a general neural architecture and a LoRA layer (Definition 1),
we define LoRA features (ZA, ZB)asZA=AZ, and ZB=BZA=BAZ. At fine-tuning step t, we
use the superscript tto denote the value of LoRA features Zt
A, Zt
B, and the subscript tto denote the
weights At, Bt.
3 LoRA Finetuning Dynamics in the Large Width Limit
We fix the LoRA rank rthroughout the analysis and examine the finetuning dynamics in the limit
of large width. This setup aligns well with practical scenarios where the rank is much smaller
than the width (i.e., r≪n). Typically, for Llama models the rank ris generally of order 2kfor
k∈ {2, . . . , 6}, and model width nis generally larger than 212. We will refer to a layer of the
network to which LoRA is applied (see Definition 1) as a LoRA layer . For the theoretical analysis,
we adopt a simplified setting that facilitates a rigorous yet intuitive derivations of the results.
3.1 Simplified Setting
The following simplified setup was considered in Hayou et al. [44] to derive asymptotic results
concerning the learning rates in LoRA. We use the same setup in our analysis to investigate the
impact of initialization.
Finetuning Dataset. We assume that the dataset used for finetuning consists of a single datapoint
(x, y),5and the goal is to minimize the loss calculated with the model with adjusted weights W∗+
BAfor all LoRA layers (here θ={A, B, for all LoRA layers in the model }). Ztis the input to the
LoRA layer, computed with data input x. Similarly, we write d¯Ztto denote the gradient of the loss
function with respect to the layer output features ¯Zevaluated at data point (x, y).
3Gaussianity is not important and can be replaced by any zero-mean distribution with finite-variance for our
purposes.
4Here, we assumed that r= Θ(1) (in width), i.e. it doesn’t grow with width. In general, the right scaling
forInit[B] isσ2
B= Θ( r−1).
5Although this a simplifying assumption for our analysis, the results can be extended to mini-batched gra-
dients without affecting the conclusions. Such results will require additional assumptions to be fully rigorous.
4Single LoRA Module. Given a LoRA layer, LoRA feature updates are not only driven by the
change in the A, B weights, but also the changes in Z , d¯Zwhich are updated as we finetune the
model (assuming there are multiple LoRA layers). To isolate the contribution of individual LoRA
layers to feature learning, we assume that only a single LoRA layer is trainable and all other LoRA
layers are frozen.6For this LoRA layer the layer input Z is fixed and does not change with t, whereas
d¯Zchanges with step t(because ¯Zt= (W∗+α
rBtAt)Z). After step t,ZBis updated as follows
∆Zt
B=Bt−1∆Zt
A|{z}
δ1
t+ ∆BtZt−1
A|{z}
δ2
t+ ∆Bt∆Zt
A|{z}
δ3
t. (2)
As discussed in Hayou et al. [44], the terms δ1
t, δ2
trepresent ‘linear’ feature updates that we obtain
if we fix one weight matrix and only train the other. The third term δ3
trepresents the ‘multiplicative’
feature update which captures the compounded update due to updating both AandB.
3.2 Stability and Feature Learning
Hayou et al. [44] introduced the notion of stability of LoRA features as width grows. We introduce
here a slightly more relaxed notion of stability.
Definition 3 (Feature Stability) .We say that LoRA finetuning is stable if for all LoRA layers in the
model, and all training steps t, we have Z , ZB=O(1),as the width ngoes to infinity.
Here, feature stability implies that LoRA output ZBremains bounded (in L2norm) as width grows.
To achieve such stability, hyperparameters (initialization, learning rate) should be scaled as ngrows.
We will show that the dependence of the optimal learning rate on nis highly sensitive to the choice
of initialization ( Init[A] orInit[B] ).
Note that feature stability also requires that Z =O(1)which is directly related to pretraining dynam-
ics since it depends on some pretrained weights W∗. We assume that pretraining parameterization
(how initialization and learning rate are parametrized w.r.t width) ensures this kind of stability (see
Appendix A for more details).7
As discussed above, feature updates are driven by the terms (δi
t)i∈{1,2,3,}. Asngrows, these feature
updates might become trivial (i.e. vanish as n→ ∞ ) or unstable (i.e. grows unbounded). To avoid
such scenarios, we want to ensure that ∆ZB= Θ(1) . Such conditions are the main ideas behind
µP [26] and Depth- µP [41], which are network parametrizations that ensure stability and feature
learning in the large width and depth limits for pretraining. We recall this definition from [44].
Definition 4 (Feature Learning) .We say that LoRA finetuning induces stable feature learning in the
limit of large width if the dynamics are stable (Definition 3), and for all finetuning steps t, we have
∆Zt
Bdef=Zt+1
B−Zt
B= Θ(1) .
∆ZBis the sum of the terms δi
t’s (Equation (2)). To achieve optimal feature learning, we want to
ensure that δ1
t= Θ(1) andδ2
t= Θ(1) which means that both weight matrices AandBare efficiently
updated and contribute to the update in ZB. An intuitive explanation is provided in Appendix A.1.
This leads us to the following definition of efficient learning with LoRA.
Definition 5 (Efficient Learning with LoRA) .We say that LoRA fine-tuning is efficient if it is stable
(Definition 3), and for all LoRA layers in the model, and all fine-tuning steps t >1, we have
δi
t= Θ(1) , i∈ {1,2}.
Next, we introduce the γ-operator, an essential tool in our analysis of the large width dynamics.
6This is equivalent to having only a single LoRA layer in the model since LoRA layers are initialized to
zero.
7When taking the infinite width limit, we can for instance assume that pretraining parameterization is µP
[26]. This is a technicality for the infinite-width limit and does not have any implications on practical scenarios
where the width is finite. The most important implications of this assumption is that in the pretrained network
(before introducing LoRA layers), we have Z = Θ(1) ,¯Z= Θ(1) , which holds for a general input-output pair
(x, y).
53.3 Introduction to the γ-operator
In the theory of scaling, one usually tracks the asymptotic behavior of key quantities as we scale
some model ingredient. For instance, if we scale the width nof a neural network, we are interested
in quantifying how certain quantities in the network behave as ngrows. This is a standard approach
for (principled) model scaling and it has so far been used to derive scaling rules for initialization [5],
activation function [10], network parametrization [41], amongst other things.
With Init[A] andInit[B] , initialization weights are of order Θ(n−β)for some β≥0. Assuming
that the learning rate also scales polynomialy with n, it is straightforward that preactivations, gra-
dients, and weight updates are all asymptotically polynomial in n. Note that this is only possible
because all neural computations consists of sums of Θ(nα)terms, where typically α∈ {0,1}. For
instance, when calculating the features AZ, each entry is a sum of nterms, while when calculating
BZA, each entry is a sum of rterms ( rfixed as ngoes to infinity). This is true for general neural
computation that can be expressed as Tensor Programs [15].
Consequently, for some quantity vin the computation graph, it is natural to track the exponent that
determines the asymptotic behavior of vwith respect to n. We write v= Θ( γ[v])to capture this
polynomial dependence. Elementary operations with the γ-operator include:8
Zero. When v= 0, we write γ[v] =−∞ (as a limit of γ[n−β]when β→ ∞ ).
Multiplication. Given two real-valued variables v, v′, we have γ[v×v′] =γ[v] +γ[v′].
Addition. Given two real-valued variables v, v′, we generally haveγ[v+v′] = max( γ[v], γ[v′]).
The only case where this is violated is when v′=−v. This is generally a zero probability event if v
andv′are random variables that are not perfectly (negatively) correlated, which is the case in most
situations where we make use of this formula. See Appendix A.3 for discussion.
We have now introduced all required notions for the subsequent analysis. For better readability, we
defer all the proofs to the appendix.
3.4 Recursive formulas
Using the γ-operator, we can track the asymptotic behavior of the finetuning dynamics as model
width ngrows. At finetuning step t, the weights are updated as follows
At=At−1−ηgt−1
A, B t=Bt−1−ηgt−1
B,
where gA, gBare processed gradients (e.g. normalized gradients with momentum as in AdamW).
We assume that the gradients are processed in a way that makes their entries Θ(1) . This is gen-
erally satisfied in practice (with Adam for instance) and has been considered in [40] to derive the
µ-parametrization for general gradient processing functions. From this, we obtain the following
recursive formulas for γ[Zt
A]andγ[Bt], which characterizes their behavior in the large width limit.
Lemma 1 (Informal) .Fortfixed, the asymptotic dynamics of Zt
AandBtfollow the recursive
formula
γ[Zt
A] = max( γ[Zt−1
A], γ[η] + 1)
γ[Bt] = max( γ[Bt−1]], γ[η]).(3)
The formal proof of Lemma 1 is provided in Appendix A and relies on Assumption 1 which fairly
represents practical scenarios (see Appendix A for a detailed discussion). Lemma 1 captures the
change in asymptotic behavior of quantities Zt
AandBtas width grows. Naturally, the dynamics
depend on the the initialization scheme which lead to completely different behaviors as we show in
the next two results.
3.5 Init[A] leads to more efficient feature learning but suffers “internal” instability
Next, we provide a precise characterization of stability and feature learning when using Init[A] .
Theorem 1 (Informal) .Fortfixed, with Init[A] and learning rate η, we have
8Theγ-operator is a mapping from the set {v,s.t.v= Θ( nβ)forβ∈R∪ {−∞}} to the set R∪ {−∞} .
6•Stability :Zt
B=O(1)if and only if γ[η]≤ −1/2.
•Feature Learning :∆Zt
B= Θ(1) if and only if γ[η] =−1/2. In this case, we also have
δ1
t, δ2
t= Θ(1) (efficient feature learning, Definition 5).
Moreover, “internal” instability ( Zt
A= Ω(1) ) occurs when γ[η]∈(−1,1/2].
With Init[A] , the maximal learning rate9that does not lead to instability in ZBscales as Θ(n−1/2).
This can be seen as an asymptotic form of the edge of stability phenomenon [17] where if we increase
the learning rate beyond some level, instability occurs. Interestingly, in this case (i.e. with Θ(n−1/2)
learning rate) the features are efficiently updated (Definition 5). However, this comes with caveat:
the features Zt
Agrow as Θ(n1/2)which can potentially cause numerical instabilities. We call this
phenomenon internal instability : only the features ZA(internal LoRA features) grows, LoRA output
ZBremains Θ(1) in this case.
The fact that Θ(n−1/2)is the maximal learning rate that does not cause instability in ZBdoes not
mean it is the optimal learning rate. As the width ngrows, this internal instability in ZAwill become
more and more problematic. Intuitively, we expect that a trade-off appears in this case: the optimal
learning rate (found by grid search) to be larger than Θ(n−1)but smaller than Θ(n−1/2), i.e. the
network will try to achieve a balance between optimal feature learning ( γ[η] =−1/2) and internal
stability Zt
A= Θ(1) (γ[η] =−1). We verify this empirically in the next section.
3.6 Init[B] leads to suboptimal feature learning with internal stability
In the next result, we show that the maximal learning rate allowed with Init[B] is different from
that with Init[A] , leading to completely different dynamics.
Theorem 2 (Informal) .Fortfixed, with Init[B] , we have
•Stability :Zt
B=O(1)if and only if γ[η]≤ −1.
•Feature Learning :∆Zt
B= Θ(1) if and only if γ[η] =−1.
Moreover, efficient feature learning cannot be achieved with Init[B] for any choice of learning rate
scaling γ[η](that does not violate the stability condition). More precisely, with Θ(n−1)learning
rate, the limiting dynamics (when n→ ∞ ) are the same if Bwas not trained and Ais trained.
With Init[B] , the maximal learning rate (that does not violate stability) scales as Θ(n−1)(for any
ϵ >0, a learning rate of Θ(n−1+ϵ)leads to ZB= Ω(1) ).
Because of this bound on the maximal learning rate, no internal instability occurs with Init[B] . In
this case, feature learning is suboptimal since the Bweight matrix is undertrained in the large width
limit ( δ2
t→0).
Conclusions from Sections 3.5 and 3.6. The results of Theorem 1 and Theorem 2 suggest that
Init[A] allows the use of larger learning rates compared to Init[B] , which might lead to better
feature learning and hence better performance at the expense of some internal instability. Here,
‘larger’ learning rate should be interpreted in asymptotic terms: with Init[A] the maximal learning
rate that does not cause instability satisfies γ[η] =−1/2. With Init[B] , we have γ[η] =−1
instead. Note that because of the constants in Θ(nβ)learning rates (for some β) , the optimal
learning rate with Init[A] is not systematically larger than Init[B] forfinite width . However, as
width grows, we will see that it is case.
Another important insight from this analysis is that with both initializations, the dynamics are sub-
optimal in the limit: internal instability with Init[A] and undertraining of Bwith Init[B] .10We
will later discuss possible solutions to this behavior.
9Maximal γ[η]that does not cause instability in ZB
10More precisely, one can show that with Init[B] , for fixed t, in the limit n→ ∞ ,Btconverges to B0, i.e.
Bis untrained in this limit.
70.00.51.01.5Init[A]
---------------------------------
=4.2e-04, =1.6e-02
0.00.51.01.5Init[B]
---------------------------------
=5.3e-04, =8.1e-03
|Az|
|BAz|
0 50 100
step0.00.51.01.5=3.0e-04, =2.3e-02
0 50 100
step0.00.51.01.5=3.0e-04, =1.2e-02
Width = 128
0246Init[A]
---------------------------------
=2.4e-04, =1.4e-03
0246seed-1Init[B]
---------------------------------
=3.7e-04, =4.8e-04
|Az|
|BAz|
0 50 100
step024=2.9e-04, =1.2e-03
0 50 100
step024seed-2=3.6e-04, =9.8e-04
Width = 8192Figure 3: Evolution of the norms of the ZA, ZBfeatures, averaged over training data. We compute the average
ˆ|ZA|def=N−1PN
i=1∥ZA(xi)∥(and same for ZB), where the xi’s are the training data. The dynamics are
shown for widths n= 128 andn= 8192 , two seeds, and for both Init[A] andInit[B] . Train loss and the
(optimal) learning rate are shown on top of each plot. We observe that the magnitude of ZAis significantly
higher with Init[A] compared to Init[B] at large width ( n= 8192 ). Interestingly, the train loss is smaller
with Init[A] , as compared to Init[B] . Results with other seeds and widths are shown in Appendix B.
3.7 Toy Model
2829210211212213
n212
211
210
29
28
27
26
*
Stability <-> Feature Learning
Tradeoff
Init[A]
C1n1/2
Init[B]
C2n1
Figure 2: Optimal Learning rate for the fine-
tuning of synthetic model Equation (4) with
Init[A] andInit[B] as initialization. The
optimal LRs are shown as a function of width
n. Theoretical lines n−1andn−1/2are shown
as well (constants C1, C2are chosen to provide
suitable trend visualization). As model width n
grows, the optimal learning rate with Init[A]
becomes larger than the optimal learning rate
with Init[B] . This is in agreement with the the-
oretical results.To validate our theory in a controlled setting, we con-
sider the following simple model:

Yin=Winx,
Yh=Yin+ (Wh+BA)ϕ(Yin)
Yout=Woutϕ(Yh)(4)
where Win∈Rn×d, Wh∈Rn×n, Wout∈R1×n,
andB, A⊤∈Rr×n.
We generate synthetic data from the teacher model
using the following config: d= 5, rteacher =
20, n= 1000 , N = 1000 (train data size),
andNtest = 100 (test data size). The weight
Wteacher
in , Wteacher
out , Ateacher,andBteacherare ran-
domly initialized, and Wteacher
h = 0.11We train stu-
dent models with d= 5, r= 4,and varying widths
n∈ {2k, k= 7, . . . , 13}.12
Optimal Learning Rate. We finetune model (4) on
synthetic data generated from the teacher model. In
Figure 2, we show the optimal learning rate when us-
ing either Init[A] orInit[B] to initialize the fine-
tuning, as a function of width n. For n≫1(typically n≥29), the optimal learning rate with
Init[A] is larger than the optimal learning rate with Init[B] . This is in agreement with the the-
oretical results obtained in Theorem 1 and Theorem 2 which predict asymptotic maximal learning
rates (that satisfy the stability condition) of Θ(n−1/2)andΘ(n−1)respectively.
With Init[A] , we observe the stability/feature learning trade-off for large n. The optimal learning
rate with Init[A] in this regime (e.g. n= 213) is smaller than the maximal theoretical learning
11Here, the pretrained model is effectively given by Yout=Wteacher
out ϕ(Wteacher
in x), and the finetuning
dataset is simulated by injecting the LoRA weights Ateacher, Bteacher.
12In this setup, a student model can have larger width nthan the teacher model.
8raten−1/2that achieves optimal feature learning (Theorem 1). Here, the model seems to balance
the internal instability that occurs in the ZAfeatures with feature learning and thus favors smaller
learning rates: the optimal learning rates is smaller than Θ(n−1/2)and larger than Θ(n−1).
Internal Instability and Feature Learning. Figure 3 shows the average magnitude of ZAand
ZBforInit[A] andInit[B] for widths n∈ {128,8192}. With Init[A] , the magnitude of
ZAfeatures seem to grow with width, hence trading off internal stability for more efficient feature
learning. This behavior is consistent across random seeds as shown in the figure, and as further
confirmed by experiments in Appendix B. The train loss is consistently smaller with Init[A] , which
can be explained by the fact that Init[A] allows more efficient feature learning at the cost of some
internal instability. This flexibility cannot be achieved with Init[B] . Note also that ZBfeatures
tends to get smaller with nwith Init[A] as predicted by theory: the trade-off between internal
instability and feature learning implies that η∗=o(n−1/2), which implies that Zt
B=o(1), i.e. the
ZBfeatures vanish as width grows. While this might problematic, it only becomes an issue when
the width is extremely large: if the optimal learning rate scales as Θ(n−β)for some β∈(1/2,1)(so
that the learning rate is between Θ(n−1)andΘ(n−1/2), balancing internal instability and efficient
feature learning), the LoRA output feature scales as ZB=BtAtZ= Θ( n−β+1). Therefore, if
β≈0.7for instance, the vanishing rate of LoRA output feature is ZB≈Θ(n−0.3)which is slow
given the order of magnitude of width in practice (for n= 212, we have n−0.3≈0.08).
4 Experiments with Language Models
Our theoretical results from earlier provides a detailed asymptotic analysis of the finetuning dy-
namics when LoRA modules are initialized with Init[A] orInit[B] . The main conclusions are
thatInit[A] generally leads to more efficient feature learning (which can be justified by the fact
that optimal learning rate is larger when using Init[A] compared to when using Init[B] ). To
provide evidence of this claim on real-world tasks, we use LoRA to finetune a set of language
models on different benchmarks. Details about the experimental setup and more empirical re-
sults are provided in Appendix B. We use LoRA +code [44] for our experiments (available at
https://github.com/nikhil-ghosh-berkeley/loraplus ).
4.1 GLUE tasks with RoBERTa
The GLUE benchmark (General Language Understanding Evaluation) consists of several language
tasks that evaluate the understanding capabilities of langugage models [8]. Using LoRA, we fine-
tune Roberta-large from the RoBERTa family [12] on MNLI, SST2, and QNLI tasks with varying
learning rates ηand initialization schemes ( Init[A] orInit[B] ). We use the same experimental
setup of [19] for Roberta-Large to compare our results with theirs (see Appendix B for more details).
105
104
103
0.40.50.60.70.80.9T est AccMNLI
---------------------------------
Init[A]: Acc=90.69, *=8.0e-05
Init[B]: Acc=89.47, *=1.0e-05
Init[B]
Init[A]
105
104
103
0.50.60.70.80.9T est AccSST2
---------------------------------
Init[A]: Acc=96.67, *=1.6e-04
Init[B]: Acc=96.44, *=4.0e-05
Init[B]
Init[A]
105
104
103
0.8000.8250.8500.8750.9000.9250.950T est AccQNLI
---------------------------------
Init[A]: Acc=95.09, *=8.0e-05
Init[B]: Acc=93.61, *=8.0e-05
Init[B]
Init[A]
Figure 4: Test Accuracy for RoBERTa-Large finetuned on GLUE tasks. The results are shown after con-
vergence of finetuning with LoRA, initialized with either Init[A] orInit[B] . Models were finetuned using
LoRA rank r= 8 and FP16 precision. Optimal learning rate and corresponding accuracy are shown on top of
each panel for both initializations. The experimental setup is provided in Appendix B.
The results in Figure 4 are aligned with our theory: we observe that Init[A] generally leads to bet-
ter performance, and the optimal learning rate with Init[A] is generally larger than with Init[B] .
Models initialized with Init[A] match the performances reported in [19], while those initialized
with Init[B] generally underperform that baseline. For MNLI task (the hardest one amongst the
9three tasks), we observe a significant difference in the best test accuracy (over 3 random seeds) with
90.69with Init[A] and89.47with Init[B] . We also observe that for MNLI, the optimal learn-
ing rate with Init[A] (η∗= 8e-5) is much larger than the optimal learning rate with Init[B]
(η∗= 1e-5), which aligns with our theoretical predictions. However, note that for QNLI for in-
stance (an easier task), while the optimal test accuracy is significantly better with Init[A] , the
optimal learning rate (from the grid search) is the same for Init[A] andInit[B] . There are many
possible explanations for this: 1) the width is not large enough in this case to see the gap between
optimal learning rates (for RoBERTa-Large, the width is n= 210) 2) The constants in Θ(n−1)are
Θ(n−1/2)are significantly different in magnitude due to dependence on finetuning task. We notice
similar behaviour with LLama experiments below. A precise analysis of this observation is beyond
the scope of this paper, we leave it for future work.
4.2 Llama
105
104
103
7.27.47.67.88.08.28.4T est PerplexityTinyLlama on WikiT ext-2---------------------------------------------------
init[A]: PPL = 7.089, = 7.0e-04
init[B]: PPL = 7.151, = 4.0e-04
init[B]
init[A]
104
103
0.260.280.300.320.340.360.380.40MMLU AccuracyLlama-7b on Flan-v2---------------------------------------------------
init[A]: Acc = 0.410, = 4.0e-04
init[B]: Acc = 0.406, = 4.0e-04
init[B]
init[A]
105
104
103
0.120.140.160.180.200.220.240.26T est AccuracyLlama-7b on GSM8k---------------------------------------------------
init[A]: Acc = 0.260, = 1.0e-03
init[B]: Acc = 0.239, = 1.0e-03
init[B]
init[A]
Figure 5: (Left) Test perplexity (lower is better) of TinyLlama LoRA on WikiText-2 with Init[A] and
Init[B] .(Center) MMLU accuracy of Llama-7b LoRA finetuned on the Flan-v2 dataset. (Right) GSM8k
test accuracy of Llama-7b LoRA finetuned on the GSM8k dataset. More experimental details are provided in
Appendix B.
To further validate our theoretical findings on more modern models and datasets, we report the
results of finetuning the Llama-7b model [38] on the Flan-v2 dataset [36] and the GSM8k dataset
[16], and finetuning the TinyLlama model [49] on WikiText-2 using LoRA. Each trial is averaged
over two seeds and the shaded region indicates one standard error. In the left panel of Figure 5 we
see that when finetuning TinyLlama using LoRA the optimal learning rate using Init[A] is larger
than with Init[B] and the corresponding test perplexity is lower. Similarly, for the center panel of
Figure 5, when finetuning the Llama-7b model on Flan-v2, the optimal learning rates for Init[A]
andInit[B] are the same (for the learning rate grid we used), but the the optimal MMLU accuracy
forInit[A] is slightly higher than for Init[B] . For learning rates close to the optimal choice, the
accuracy using Init[A] is generally higher than for Init[B] . An analagous result holds for the
GSM8k dataset as shown in the rightmost panel of Figure 5. More details about this setting are
provided in Appendix B.
5 Conclusion and Limitations
We showed that LoRA dynamics are highly sensitive to initialization. Init[A] is associated with
larger optimal learning rates, compared to Init[B] . Larger learning rates typically result in better
performance, as confirmed by our empirical results. Note that this is a zero-cost adjustment with
LoRA finetuning: we simply recommend using Init[A] instead of Init[B] .
One limitation of our work is that we only define feature learning via the magnitude of feature
updates in the limit of large width. In this way, our definition of feature learning is data-agnostic
and therefore no conclusion about generalization can be obtained with this analysis. The constants
inΘ(.)asymptotic notation naturally depend on the data (the finetuning task) and therefore such
data-agnostic approach does not allow us to infer any information about the impact of the data on
the finetuning dynamics.
More importantly , our results indicate that both initialization schemes lead to suboptimal scenarios,
although Init[A] allows more efficient feature learning. In both cases, instability and/or suboptimal
feature learning present fundamental issues, which can potentially be mitigated by approaches such
as LoRA +[44]. Understanding the interaction of LoRA +and related efficiency methods with the
initialization scheme is an important question for future work.
106 Acknowledgement
We thank Gradient AI for cloud credits under the Gradient AI fellowship awarded to SH and thank
AWS for cloud credits under an Amazon Research Grant awarded to the Yu Group. We also grate-
fully acknowledge partial support from NSF grants DMS-2209975, 2015341, 20241842, NSF grant
2023505 on Collaborative Research: Foundations of Data Science Institute (FODSI), the NSF and
the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning
through awards DMS-2031883 and 814639, and NSF grant MC2378 to the Institute for Artificial
CyberThreat Intelligence and OperatioN (ACTION).
11References
[1] Y . LeCun, L. Bottou, G. B. Orr, and K.-R. Müller. “Efficient backprop”. In: Neural networks:
Tricks of the trade . Springer, 2002, pp. 9–50.
[2] L. Yang, S. Hanneke, and J. Carbonell. “A theory of transfer learning with applications to
active learning”. In: Machine learning 90 (2013), pp. 161–189.
[3] D. P. Kingma and J. Ba. “Adam: A method for stochastic optimization”. In: arXiv preprint
arXiv:1412.6980 (2014).
[4] K. He, X. Zhang, S. Ren, and J. Sun. “Deep residual learning for image recognition”. In: Pro-
ceedings of the IEEE conference on computer vision and pattern recognition . 2016, pp. 770–
778.
[5] S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. “Deep Information Propagation”.
In:International Conference on Learning Representations . 2017.
[6] S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep Information Propaga-
tion. 2017. arXiv: 1611.01232 [stat.ML] .
[7] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. “Improving language understand-
ing by generative pre-training”. In: (2018).
[8] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A Multi-Task
Benchmark and Analysis Platform for Natural Language Understanding . 2018. arXiv: 1804.
07461 [cs.CL] .
[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “BERT: Pre-training of Deep Bidi-
rectional Transformers for Language Understanding”. In: arXiv preprint arXiv:1810.04805
(2019).
[10] S. Hayou, A. Doucet, and J. Rousseau. “On the Impact of the Activation function on Deep
Neural Networks Training”. In: Proceedings of the 36th International Conference on Ma-
chine Learning . Ed. by K. Chaudhuri and R. Salakhutdinov. V ol. 97. Proceedings of Machine
Learning Research. PMLR, Sept. 2019, pp. 2672–2680.
[11] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M.
Attariyan, and S. Gelly. “Parameter-efficient transfer learning for NLP”. In: International
Conference on Machine Learning . PMLR. 2019, pp. 2790–2799.
[12] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,
and V . Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach . 2019. arXiv:
1907.11692 [cs.CL] .
[13] G. Yang. “Scaling limits of wide neural networks with weight sharing: Gaussian process
behavior, gradient independence, and neural tangent kernel derivation”. In: arXiv preprint
arXiv:1902.04760 (2019).
[14] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-
ford, J. Wu, and D. Amodei. “Scaling laws for neural language models”. In: arXiv preprint
arXiv:2001.08361 (2020).
[15] G. Yang. “Tensor programs iii: Neural matrix laws”. In: arXiv preprint arXiv:2009.10685
(2020).
[16] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J.
Hilton, R. Nakano, et al. “Training verifiers to solve math word problems”. In: arXiv preprint
arXiv:2110.14168 (2021).
[17] J. Cohen, S. Kaur, Y . Li, J. Z. Kolter, and A. Talwalkar. “Gradient Descent on Neural Net-
works Typically Occurs at the Edge of Stability”. In: International Conference on Learning
Representations . 2021.
[18] S. Hayou, E. Clerico, B. He, G. Deligiannidis, A. Doucet, and J. Rousseau. “Stable ResNet”.
In:Proceedings of The 24th International Conference on Artificial Intelligence and Statistics .
Ed. by A. Banerjee and K. Fukumizu. V ol. 130. Proceedings of Machine Learning Research.
PMLR, 13–15 Apr 2021, pp. 1324–1332.
[19] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. “LoRA:
Low-Rank Adaptation of Large Language Models”. In: arXiv preprint arXiv:2106.09685
(2021).
[20] B. Lester, R. Al-Rfou, and N. Constant. “The power of scale for parameter-efficient prompt
tuning”. In: arXiv preprint arXiv:2104.08691 (2021).
12[21] G. Yang and E. J. Hu. “Tensor programs iv: Feature learning in infinite-width neural net-
works”. In: International Conference on Machine Learning . PMLR. 2021, pp. 11727–11737.
[22] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las
Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den
Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals,
and L. Sifre. Training Compute-Optimal Large Language Models . 2022. arXiv: 2203.15556
[cs.CL] .
[23] M. Li, M. Nica, and D. Roy. “The Neural Covariance SDE: Shaped Infinite Depth-and-Width
Networks at Initialization”. In: Advances in Neural Information Processing Systems . Ed. by
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. V ol. 35. Curran Asso-
ciates, Inc., 2022, pp. 10795–10808.
[24] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. “Few-shot
parameter-efficient fine-tuning is better and cheaper than in-context learning”. In: Advances
in Neural Information Processing Systems 35 (2022), pp. 1950–1965.
[25] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma,
D. Zhou, D. Metzler, et al. “Emergent abilities of large language models”. In: arXiv preprint
arXiv:2206.07682 (2022).
[26] G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen,
and J. Gao. “Tensor programs v: Tuning large neural networks via zero-shot hyperparameter
transfer”. In: arXiv preprint arXiv:2203.03466 (2022).
[27] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. “QLoRA: Efficient Finetuning of
Quantized LLMs”. In: arXiv preprint arXiv:2305.14314 (2023).
[28] S. Hayou. “On the infinite-depth limit of finite-width neural networks”. In: Transactions on
Machine Learning Research (2023). ISSN : 2835-8856.
[29] S. Hayou and G. Yang. “Width and Depth Limits Commute in Residual Networks”. In: Pro-
ceedings of the 40th International Conference on Machine Learning . Ed. by A. Krause, E.
Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett. V ol. 202. Proceedings of Ma-
chine Learning Research. PMLR, 23–29 Jul 2023, pp. 12700–12723.
[30] B. He, J. Martens, G. Zhang, A. Botev, A. Brock, S. L. Smith, and Y . W. Teh. Deep Trans-
formers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation . 2023.
arXiv: 2302.10322 [cs.LG] .
[31] D. Kalajdzievski. “A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA”. In:
arXiv preprint arXiv:2312.03732 (2023).
[32] S. A. Koohpayegani, K. Navaneet, P. Nooralinejad, S. Kolouri, and H. Pirsiavash.
“NOLA: Networks as linear combination of low rank random basis”. In: arXiv preprint
arXiv:2310.02556 (2023).
[33] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. “VeRA: Vector-based Random Matrix
Adaptation”. In: arXiv preprint arXiv:2310.11454 (2023).
[34] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. “Loftq: Lora-fine-
tuning-aware quantization for large language models”. In: arXiv preprint arXiv:2310.08659
(2023).
[35] H. Liu, C. Li, Y . Li, and Y . J. Lee. “Improved baselines with visual instruction tuning”. In:
arXiv preprint arXiv:2310.03744 (2023).
[36] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J.
Wei, et al. “The flan collection: Designing data and methods for effective instruction tuning”.
In:arXiv preprint arXiv:2301.13688 (2023).
[37] L. Noci, C. Li, M. B. Li, B. He, T. Hofmann, C. Maddison, and D. M. Roy. The Shaped
Transformer: Attention Models in the Infinite Depth-and-Width Limit . 2023. arXiv: 2306.
17759 [stat.ML] .
[38] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P.
Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J.
Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn, S. Hosseini,
R. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,
M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y . Lu, Y . Mao, X. Martinet, T. Mihaylov, P.
Mishra, I. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R.
Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan,
13P. Xu, Z. Yan, I. Zarov, Y . Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,
S. Edunov, and T. Scialom. “Llama 2: Open Foundation and Fine-Tuned Chat Models”. In:
arXiv preprint arXiv:2307.09288 (2023).
[39] Y . Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu, D. Wadden, K. MacMillan,
N. A. Smith, I. Beltagy, et al. “How Far Can Camels Go? Exploring the State of Instruction
Tuning on Open Resources”. In: arXiv preprint arXiv:2306.04751 (2023).
[40] G. Yang and E. Littwin. “Tensor programs ivb: Adaptive optimization in the infinite-width
limit”. In: arXiv preprint arXiv:2308.01814 (2023).
[41] G. Yang, D. Yu, C. Zhu, and S. Hayou. “Tensor Programs VI: Feature Learning in Infinite-
Depth Neural Networks”. In: arXiv preprint arXiv:2310.02244 (2023).
[42] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. “Lora-fa: Memory-efficient low-rank adapta-
tion for large language models fine-tuning”. In: arXiv preprint arXiv:2308.03303 (2023).
[43] K. Bałazy, M. Banaei, K. Aberer, and J. Tabor. “LoRA-XS: Low-Rank Adaptation with Ex-
tremely Small Number of Parameters”. In: arXiv preprint arXiv:2405.17604 (2024).
[44] S. Hayou, N. Ghosh, and B. Yu. LoRA+: Efficient Low Rank Adaptation of Large Models .
2024. arXiv: 2402.12354 [cs.LG] .
[45] T. Jiang, S. Huang, S. Luo, Z. Zhang, H. Huang, F. Wei, W. Deng, F. Sun, Q. Zhang, D. Wang,
et al. “MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning”. In: arXiv preprint
arXiv:2405.12130 (2024).
[46] Y . Li, S. Han, and S. Ji. “VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector
Banks”. In: arXiv preprint arXiv:2405.15179 (2024).
[47] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen.
“DoRA: Weight-Decomposed Low-Rank Adaptation”. In: arXiv preprint arXiv:2402.09353
(2024).
[48] F. Meng, Z. Wang, and M. Zhang. “PiSSA: Principal Singular Values and Singular Vectors
Adaptation of Large Language Models”. In: arXiv preprint arXiv:2404.02948 (2024).
[49] P. Zhang, G. Zeng, T. Wang, and W. Lu. “Tinyllama: An open-source small language model”.
In:arXiv preprint arXiv:2401.02385 (2024).
[50] J. Zhu, K. Greenewald, K. Nadjahi, H. S. de OcÃ¡riz Borde, R. B. Gabrielsson, L. Choshen,
M. Ghassemi, M. Yurochkin, and J. Solomon. Asymmetry in Low-Rank Adapters of Founda-
tion Models . 2024. arXiv: 2402.16842 [cs.LG] .
14A Theory and Proofs
A.1 Role of A and B weight matrices
Recall the feature update decomposition
∆Zt
B=Bt−1∆Zt
A|{z}
δ1
t+ ∆BtZt−1
A|{z}
δ2
t+ ∆Bt∆Zt
A|{z}
δ3
t. (5)
To achieve optimal feature learning, we want to ensure that δ1
t= Θ(1) andδ2
t= Θ(1) which means
that both weight matrices AandBare efficiently updated and contribute to the update in ZB. To
justify why this is a desirable property, let us analyze how changes in matrices AandBaffect LoRA
feature ZB=BAZ.
Let(B:,i)1≤i≤rdenote the columns of B. We have the following decomposition of ZB:
ZB=rX
i=1(AZ)iB:,i,
where (AZ)iis the ithcoordinate of AZ. This decomposition suggests that the direction ofZBis a
weighted sum of the columns of B, and Amodulates the weights . With this, we can also write


δ1
t=Pr
i=1(∆AtZ)i(B:,i)t−1
δ2
t=Pr
i=1(At−1Z)i(∆B:,i)t−1,
where (B:,i)trefers to the columns of Bat time step t. Having both δ1
tandδ2
tof order Θ(1) means
that both AandBare ‘sufficiently’ updated to induce a change in weights (AZ)iand directions B:,i.
If one of the matrices A, B is not efficiently updated, we might end up with suboptimal finetuning,
leading to either non updated directions Bor direction weights (At−1Z). For instance, assuming
that the model is initialized with Init[B] , and that Bis not efficiently updated, the direction of ZB
will be mostly determined by the vector (sub)space of dimension rgenerated by the columns of B
at initialization.
This intuition was discussed in details in [44].
A.2 Scaling of Neural Networks
Scaling refers to the process of increasing the size of one of the ingredients in the model to improve
performance (see e.g. [22]). This includes model capacity which can be increased via width (em-
bedding dimension) or depth (number of layers) or both, compute (training data), number of training
steps etc. In this paper, we are interested in scaling model capacity via the width n. This is motivated
by the fact that most state-of-the-art language and vision models have large width.
It is well known that as the width ngrows, the network initialization scheme and the learning should
be adapted to avoid numerical instabilities and ensure efficient learning. For instance, the initial-
ization variance should scale 1/nto prevent arbitrarily large pre-activations as we increase model
width n(e.g. He init [4]). To derive such scaling rules, a principled approach consist of analyzing
statistical properties of key quantities in the model (e.g. pre-activations) as ngrows and then adjust
the initialization, the learning rate, and the architecture itself to achieve desirable properties in the
limitn→ ∞ [5, 10, 13].
In this context, Yang et al. [26] introduces the Maximal Update Parameterization (or µP), a set of
scaling rules for the initialization scheme, the learning rate, and the network architecture that ensure
stability and maximal feature learning in the infinite width limit. Stability is defined by Yi
l= Θ(1)
for all landiwhere the asymptotic notation ‘ Θ(.)’ is with respect to width n(see next paragraph
for a formal definition), and feature learning is defined by ∆Yl= Θ(1) , where ∆refers to the
feature update after taking a gradient step. µP guarantees that these two conditions are satisfied at
any training step t. Roughly speaking, µP specifies that hidden weights should be initialized with
Θ(n−1/2)random weights, and weight updates should be of order Θ(n−1). Input weights should be
initialized Θ(1) and the weights update should be Θ(1) as well. While the output weights should be
15initialized Θ(n−1)and updated with Θ(n−1). These rules ensure both stability and feature learning
in the infinite-width limit, in contrast to standard parameterization (exploding features if the learning
rate is well tuned), and kernel parameterizations (e.g. Neural Tangent Kernel parameterization where
∆Yl= Θ( n−1/2), i.e. no feature learning in the limit).
A.3 When does γ-Operator fail to capture asymptotic behavior?
When non-polynomial dependencies (in terms of n) appear in neural computations, then the γoper-
ator cannot capture asymptotic behavior of the learning dynamics. For instance, if one of the layers
has embedding dimension enorn×log(n), polynomial exponents are no longer sufficient to capture
the asymptotic dynamics. Fortunately, such cases are generally not considered in practice.
A.4 Proof of Lemma 1
In this section, we provide the formal proof of Lemma 1. The proof relies on the following assump-
tion on the processed gradient gA. This assumption was used in [44] to derive scaling rules for the
optimal learning rates for AandBweight matrices. Here, we use it to study the sensitivity of LoRA
dynamics to initialization. We provide an intuitive discussion that shows why this assumption is
realistic.
Assumption 1. With the same setup of Section 3, at training step t, we have Z , d¯Z= Θ(1) and
gt
AZ= Θ( n).
Assumption 1 consists of two parts: that 1) Z , d¯Z= Θ(1) and 2) gt
AZ= Θ( n). The first condition
is mainly related to pretraining paramterization which we assume satisfied such conditions.13The
second condition is less intuitive, so let us provide an argument to justify why it is sound in practice.
Let us study the product gt
AZin the simple case of Adam with no momentum, a.k.a SignSGD which
is given by
gA=sign∂L
∂A
,
where the sign function is applied element-wise. At training step t, we have
∂Lt
∂A=α
rB⊤
t−1d¯Zt−1⊗Z,
LetSt=α
rB⊤
t−1d¯Zt−1. Therefore we have
gA=sign(St⊗Z) = ( sign(St
iZj))1≤i,j≤n.
However, note that we also have
sign(St
iZj) =sign(St
i)sign(Zj),
and as a result
gt
A=sign(St)⊗sign(Z).
Hence, we obtain
gt
AZ= (sign(Z)⊤Z)sign(St) = Θ( n),
where we used the fact that sign (Z)⊤Z= Θ( n).
This intuition should in-principle hold for the general variant of Adam with momentum as long as
the gradient processing function (a notion introduced in [2]) roughly preserves the sign (Z)direction.
This reasoning can be made rigorous for general gradient processing function using the Tensor Pro-
gram framework and taking the infinite-width limit where the components of gA,Z, d¯Zall become
13There is a technical intricacy on this point. While Z depends only on pretraining, the Jacobian d¯Zdepends
on finetuning. However, under the stability conditions mentioned in Definition 3, if d¯Z= Θ(1) , it should
remain so during finetuning as well.
16iid. However this necessitates an intricate treatment of several quantities in the process, which we
believe is an unnecessary complication and does not serve the main purpose of this paper.
Lemma 1. Under Assumption 1, the asymptotic behaviour of Zt
AandBtfollow the recursive for-
mula
γ[Zt
A] = max( γ[Zt−1
A], γ[η] + 1)
γ[Bt] = max( γ[Bt−1]], γ[η]).
Proof. At finetuning step t, the weights are updated as follows
At=At−1−ηgt−1
A, B t=Bt−1−ηgt−1
B.
Using the elementary operations with the γ-operator, we obtain
γ[Zt
A] = max( γ[Zt−1
A], γ[ηgt−1
AZ]) = max( γ[Zt−1
A], γ[η] +γ[gt−1
AZ]).
We conclude for Zt
Ausing Assumption 1. The formula for γ[Bt]follows using the same techniques.
A.5 Proof of Theorem 1
Theorem 1. Under Assumption 1, For tfixed, with Init[A] and learning rate η, we have
•Stability :Zt
B=O(1)if and only if γ[η]≤ −1/2.
•Feature Learning :∆Zt
B= Θ(1) if and only if γ[η] =−1/2. In this case, we also have
δ1
t, δ2
t= Θ(1) (efficient feature learning, Definition 5).
Moreover, “internal” instability ( Zt
A= Ω(1) ) occurs when γ[η]∈(−1,1/2].
Proof. With Init[A] , we have γ[B0] =−∞ andγ[A0Z] = 0 . As a result, we have for all t
γ[AtZ] = max(0 , γ[η] + 1)
γ[Bt] =γ[η]
To achieve ZB=O(1), we should therefore have
γ[η] + max(0 , γ[η] + 1)≤0,
which is equivalent to γ[η]≤ −1/2.
This implies that the maximum learning rate that does not cause instability is Θ(n−1/2). Such
learning rate causes internal instability, i.e. the feature ZAexplodes with width. Why? Because,
with this learning rate, we have γ[AtZ] = 1 /2, i.e. AtZ= Θ( n1/2)which diverges as ngrows.
However, this growth is compensated with the fact that γ[Bt] =−1/2, i.e. Bt= Θ( n−1/2). This
analysis is valid for any γ[η]∈(−1,1/2].
In this case, feature learning is efficient in the sense of Definition 5: δ1
t= Θ(1) andδ2
t= Θ(1) . To
see this, recall that δ1
t=Bt−1∆Z1
Awhich yields γ[δ1
t] =γ[Bt−1]+γ[∆Zt
A] =γ[η]+γ[η]+1 = 0
andγ[δ2
t] =γ[∆Bt] +γ[Zt−1
A] =γ[η] + max( γ[η] + 1 ,0) = 0 . So both weights contribute
significantly to feature updates at the expense of benign exploding in Zt
A=AtZ.
17A.6 Proof of Theorem 2
Theorem 2. Under Assumption 1, for tfixed, with Init[B] and learning rate η, we have
•Stability :Zt
B=O(1)if and only if γ[η]≤ −1.
•Feature Learning :∆Zt
B= Θ(1) if and only if γ[η] =−1.
Moreover, efficient feature learning cannot be achieved with Init[B] for any choice of learning rate
scaling γ[η](that does not violate the stability condition). More precisely, with Θ(n−1)learning
rate, the limiting dynamics (when n→ ∞ ) are the same if Bwas not trained and Ais trained.
Proof. Here, we show that maximal learning rate that does not cause instability in LoRA output
features ZBisΘ(n−1)and no internal instability occurs in this scenario.
With Init[B] , we have that γ[B0] = 0 andγ[A0Z] =−∞. From Equation (3), we obtain that
γ[AtZ] =γ[η] + 1
γ[Bt] = max(0 , γ[η]).
As a result, LoRA output stability is achieved if and only if
γ[η] + 1 + max(0 , γ[η])≤0,
which is equivalent to having γ[η]≤ −1.
Moreover, with η= Θ( n−1)we have that γ[δ1
t] =γ[Bt−1] +γ[∆Zt
A] = 0 + γ[η] + 1 = 0 and
γ[δ2
t] =γ[∆Bt] +γ[Zt−1
A] =γ[η] + 0 = −1. As a result, feature learning is not efficient in this
case, and the learning dynamics are asymptotically equivalent to not training matrix B(because
δ2
t→0).
B Additional Experiments
This section complements the empirical results reported in the main text. We provide the details of
our experimental setup, and show the acc/loss heatmaps for several configurations.
B.1 Empirical Details
B.1.1 Toy Example
In Figure 2, we trained a simple model with LoRA layers to verify the results of the analysis in ??.
Here we provide the empirical details for these experiments.
Model. We consider a simple model given by
f(x) =Woutϕ(Winx+ (Wh+BA)ϕ(Winx)),
where Win∈Rn×d, Wout∈R1×n, A∈Rr×n, B∈Rn×rare the weights, and ϕis the ReLU
activation function.
Dataset. Here, we used d= 5,n= 1000 , and r= 20 to simulate synthetic data (the teacher
model). Synthetic dataset generated by X∼ N (0, Id), Y=f(X). The number of training
examples is Ntrain = 1000 , and the number of test examples is Ntest= 100 . the weights
Win, Wh, Wout, B, A are randomly sampled from a Gaussian distribution with normalized variance
(1/fan-in).
Training. We train the model with AdamW with β1= 0.9andβ2= 0.99for a range for values of
η. The weights are initialized as follows: Win∼ N(0,1/d), Wh∼ N(0,1/n), Wout∼ N(0,1/n)
and fixed. Only the weight matrices A, B are trainable.
18B.1.2 GLUE tasks with RoBERTa
For our experiments with RoBERTa models, finetuned on GLUE tasks, we use the following setup:
Training Alg Details
Model Roberta-Large
Learning Rates {2k×10−5,fork= 0,1,2, . . . , 10}
β1 0.9
β2 0.999
ε 1×10−8
LR Schedule Linear with Warmup Ratio 0.06
Weight Decay 0.0
Train Batch Size 4
Number of Epochs 10
LoRA Hyperparameters
LoRA Rank 8
LoRA α 16
LoRA Dropout 0.1
Target Modules ‘query, value’
Other Hyperparameters
Sequence Length Ttarget= 128
Random Seeds 3
Precision FP16
GPUs. Nvidia A10 with 24GB VRAM.
19B.1.3 TinyLlama WikiText-2
For our experiments using the TinyLlama model finetuned on Wikitext-2, we use the following setup
training with AdamW.
Training Algorithm Details
Learning Rates 1×10−5,5×10−5,1×10−4,2×10−4,4×10−4,7×10−4,1×10−3,2×10−3
β1 0.9
β2 0.999
ε 1×10−6
LR Schedule Linear with Warmup Ratio 0.03
Weight Decay 0.0
Train Batch Size 8
Number of Epochs 1
LoRA Hyperparameters
LoRA Rank 64
LoRA α 16
LoRA Dropout 0.0
Target Modules ‘q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj’
Other Hyperparameters
Sequence Length 1024
Random Seeds 2
Precision BF16
GPUs. Nvidia A10 with 24GB VRAM.
20B.1.4 Llama-7b Flan-v2
For our experiments using the Llama-7b model finetuned on a size 100k random subset of flan-v2,
we use following setup training with AdamW
Training Algorithm Details
Learning Rates 1×10−5,5×10−5,1×10−4,2×10−4,4×10−4,7×10−4,1×10−3
β1 0.9
β2 0.999
ε 1×10−6
LR Schedule Linear with Warmup Ratio 0.03
Weight Decay 0.0
Train Batch Size 16
Number of Epochs 1
LoRA Hyperparameters
LoRA Rank 64
LoRA α 16
LoRA Dropout 0.0
Target Modules ‘q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj’
Other Hyperparameters
Sequence Length Tsource = 1536 ,Ttarget= 512
Random Seeds 2
Precision BF16
MMLU Evaluation: We evaluate average accuracy on MMLU using 5-shot prompting.
GPUs: Nvidia A10 with 24GB VRAM.
21B.1.5 Llama-7b GSM8k
For our experiments using the Llama-7b model finetuned on the GSM8k training dataset, we use
following setup training with AdamW
Training Algorithm Details
Learning Rates 1×10−5,5×10−5,1×10−4,2×10−4,4×10−4,7×10−4,1×10−3
β1 0.9
β2 0.999
ε 1×10−6
LR Schedule Linear with Warmup Ratio 0.03
Weight Decay 0.0
Train Batch Size 16
Number of Epochs 1
LoRA Hyperparameters
LoRA Rank 64
LoRA α 16
LoRA Dropout 0.0
Target Modules ‘q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj’
Other Hyperparameters
Sequence Length Tsource = 1536 ,Ttarget= 512
Random Seeds 2
Precision BF16
GPUs: Nvidia A10 with 24GB VRAM.
B.2 Additional Exps
22012Init[A]
---------------------------------
=7.7e-04, =6.7e-02
012Init[B]
---------------------------------
=7.4e-04, =6.7e-02
|Az|
|BAz|
0 50 100
step012=5.6e-04, =5.6e-02
0 50 100
step012=6.5e-04, =4.7e-02
Width = 64
012Init[A]
---------------------------------
=4.9e-04, =1.4e-02
012Init[B]
---------------------------------
=5.5e-04, =9.7e-03
|Az|
|BAz|
0 50 100
step012=4.0e-04, =1.6e-02
0 50 100
step012=3.9e-04, =6.8e-03
Width = 256
0123Init[A]
---------------------------------
=2.1e-04, =1.2e-02
0123Init[B]
---------------------------------
=3.0e-04, =3.4e-03
|Az|
|BAz|
0 50 100
step0123=2.5e-04, =6.8e-03
0 50 100
step0123=4.4e-04, =1.4e-03
Width = 512
0123Init[A]
---------------------------------
=3.5e-04, =4.8e-03
0123Init[B]
---------------------------------
=4.5e-04, =2.0e-03
|Az|
|BAz|
0 50 100
step0123=2.5e-04, =4.0e-03
0 50 100
step0123=3.9e-04, =2.8e-03
Width = 1024
0246Init[A]
---------------------------------
=3.4e-04, =4.0e-03
0246Init[B]
---------------------------------
=3.1e-04, =2.0e-03
|Az|
|BAz|
0 50 100
step024=2.7e-04, =1.7e-03
0 50 100
step024=3.7e-04, =1.2e-03
Width = 2048
0246Init[A]
---------------------------------
=4.1e-04, =2.8e-03
0246Init[B]
---------------------------------
=4.7e-04, =8.2e-04
|Az|
|BAz|
0 50 100
step0123=1.8e-04, =1.7e-03
0 50 100
step0123=4.1e-04, =2.8e-04
Width = 4096Figure 6: Same as Figure 3 with differents widths.
23NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In the abstract and introduction we clearly state the main contribution of the
paper which is to compare the difference between the two natural initializations for LoRA
and to conclude using theoretical justification based on large width theory and extensive
experiments that one is the superior choice. In our theoretical sections we provide rigorous
statements with clearly stated assumptions which provide intuition for the main claims. We
provide experiments on synthetic tasks which give fine-grained support for the theoretical
claims and show that for several real-world LLM tasks the main claims are supported,
illustrating the generality and relevance of our results. Limitations are discussed in the
final section.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We provide a comprehensive discussion of the limitations in Section 5.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All assumptions are clearly stated and referenced in the theorem statement.
The main intuitions for the results are provided in the main paper and complete proofs are
provided in the appendix.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All details to fully reproduce the experiments are provided in the main pa-
per in combination with the Appendix. Experiments were performed on standard public
datasets using standard public libraries. The algorithmic complexity of the experiments is
fairly minimal as it involves just adjusting hyperparameters.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: The validity of the experiments should already be believable without code, but
we plan to fully release code soon. The main claims should already be easily reproducible
using standard libraries.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: This is provided in Appendix B.
7.Experiment Statistical Significance
24Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We average all results over multiple random seeds and plot both the average
and 1-sigma error bars shaded in the plots.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide the compute resource details in Appendix B.
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: This paper does not meet any of the concerns for potential harms.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The paper is concerned with an abstract problem of choosing an appropriate
initialization in an extremely general setting and is not closely tied to any societal concerns.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: There are no such risks for this paper.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: There are no licensing concerns.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justification: No new assets are created.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
25Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
26