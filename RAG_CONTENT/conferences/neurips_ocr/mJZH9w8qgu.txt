In-Trajectory Inverse Reinforcement Learning: Learn
Incrementally Before an Ongoing Trajectory
Terminates
Shicheng Liu & Minghui Zhu
School of Electrical Engineering and Computer Science
Pennsylvania State University
University Park, PA 16802, USA
{sfl5539,muz16}@psu.edu
Abstract
Inverse reinforcement learning (IRL) aims to learn a reward function and a corre-
sponding policy that best fit the demonstrated trajectories of an expert. However,
current IRL works cannot learn incrementally from an ongoing trajectory because
they have to wait to collect at least one complete trajectory to learn. To bridge
the gap, this paper considers the problem of learning a reward function and a
corresponding policy while observing the initial state-action pair of an ongoing
trajectory and keeping updating the learned reward and policy when new state-
action pairs of the ongoing trajectory are observed. We formulate this problem as
an online bi-level optimization problem where the upper level dynamically adjusts
the learned reward according to the newly observed state-action pairs with the
help of a meta-regularization term, and the lower level learns the corresponding
policy. We propose a novel algorithm to solve this problem and guarantee that
the algorithm achieves sub-linear local regret O(√
T+ log T+√
TlogT). If the
reward function is linear, we prove that the proposed algorithm achieves sub-linear
regret O(logT). Experiments are used to validate the proposed algorithm.
1 Introduction
Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy
that are consistent with the demonstrated trajectories of an expert. In recent years, several IRL
methods are proposed to help learn the reward and policy, including maximum margin methods [ 1,2],
maximum entropy methods [ 3,4], maximum likelihood methods [ 5,6], and Bayesian methods [ 7,8].
The aforementioned IRL works learn from pre-collected demonstration sets and do not improve
the learned model during deployment. Online IRL [ 9,10,11] instead can learn from sequentially
arrived demonstrated trajectories and continuously improve the learned reward and policy from the
newly observed complete trajectories. However, recent applications of IRL motivate the need to
learn incrementally from an ongoing trajectory before it terminates. For example, inferring a moving
shooter’s intention from its ongoing movement in order to evacuate the hiding victims [ 12] before the
shooter finds them. In this case, we need to quickly update the inference about the shooter’s intention
once a new movement of the shooter is observed, so that we can use the latest inference to plan a
rescue strategy as soon as possible. We cannot wait until the shooter trajectory ends, in case the
shooter has found the victims. Another example is learning a target customer’s investment preference
from its daily updated investment trajectory in a stock market [ 13] in order to recommend appropriate
stocks [ 14,15] before other competitors get this customer. However, current IRL works cannot learn
from an ongoing trajectory because they have to wait to collect at least one complete trajectory to
learn from. To bridge the gap, this paper proposes in-trajectory IRL, a new type of IRL that learns a
38th Conference on Neural Information Processing Systems (NeurIPS 2024).reward function and a corresponding policy at the initial state-action pair of an ongoing trajectory, and
keeps updating the learned reward and policy once a new state-action pair of the ongoing trajectory is
observed. We summarize our contributions as follows:
Contribution statement . This paper proposes the first in-trajectory IRL framework, termed meta-
regularized in-trajectory inverse reinforcement learning" (MERIT-IRL), to learn a reward function
and a corresponding policy from an ongoing trajectory. Our contributions are twofold.
First, we formulate this in-trajectory learning problem as an online bi-level optimization problem
where the upper level continuously updates the learned reward according to the newly observed
state-action pairs and the lower level computes the corresponding policy. We develop a novel online
learning algorithm (MERIT-IRL) to solve this problem. The major novelty of the proposed algorithm
is that we propose a novel reward update mechanism specially designed for the in-trajectory learning
setting. This special reward update not only aims to explain the expert trajectory observed so far, but
also aims to consider for the future. Moreover, since the data is lacking as there is only one ongoing
trajectory, we introduce a meta-regularization term to embed prior knowledge and avoid overfitting.
Second, we theoretically guarantee that MERIT-IRL achieves sub-linear local regret O(√
T+ log T+√
TlogT). If the reward function is linear, we prove that MERIT-IRL achieves sub-linear regret
O(logT). The major novelty of the theoretical analysis is to address the difficulty that the input data
is not identically independent distributed (i.i.d.) but temporally correlated, i.e., the data (st, at)at
each time tis affected by the data (st−1, at−1)at last time.
2 Related works
Due to the space limit, we only discuss the related works on learning from incomplete trajectories
here, and we include the discussion on more related works in Appendix A.
Papers [ 16,17] use partial trajectories to update the learned reward function by comparing the expert
trajectories after the expert states and the trajectories starting from those expert states rolled out by the
learned policy. However, given the current expert state, they use expert trajectory suffix (i.e., future
trajectory) to compare while we can only access expert trajectory prefix (i.e., previous trajectory)
from an ongoing trajectory. Papers [ 18,19,20] use imitation learning to learn from incomplete
demonstrations. In specific, paper [ 18] uses a discounted sum along the future trajectory as the
weight for weighted behavior cloning and works effectively even if only portions of trajectories are
observed. Papers [ 19,20] extend GAIL [ 21] to solve for the case where the action sequences are not
complete. However, these works all require a pre-collected set of demonstrations so that they are not
in-trajectory learning since the trajectory in their cases is not ongoing.
3 Problem Formulation
In this section, we formulate the problem of in-trajectory IRL. In in-trajectory IRL, there is an
expert whose decision-making is based on a Markov decision process (MDP). An MDP is a tuple
(S,A, γ, r E, P0, P)which consists of a state set S, an action set A, a discount factor γ∈[0,1),
a reward function rE:S × A → R, and the initial state distribution P0(·). The state transition
probability (density) function is denoted by P(·|·,·)such that P(s′|s, a)denotes the probability
(density) of state transition to s′from sby taking action a. The expert is using its policy πEto
demonstrate an ongoing trajectory ζE=SE
0, AE
0, SE
1, AE
1,···and at each time t, only the state-
action pair (SE
t, AE
t)is observed. We want to learn a reward function and a corresponding policy
from the ongoing trajectory and update the learned reward function and policy at each time t.
Many IRL algorithms [ 5,6,11,22,23,24] employ a bi-level learning structure. In this structure,
the upper level learns a reward function while the lower level aims to find an associated policy by
solving an RL problem under the current learned reward function. Inspired by their bi-level learning
structure, we formulate the in-trajectory IRL problem as an online non-convex bi-level optimization
problem. In specific, we aim to learn a reward function rθ(parameterized by θ) in the upper level and
a policy corresponding to rθin the lower level. The loss function at time tis defined as:
Lt(θ; (SE
t, AE
t))≜−γtlogπθ(AE
t|SE
t) +λγt
2||θ−¯θ||2, π θ= arg max
πJθ(π) +H(π).(1)
2Note that Lt(θ; (SE
t, AE
t))is defined using πθandθis the parameter of the reward function rθ, here
the policy πθis also parameterized by θbecause it is computed by solving an RL problem (in the
lower level) under the reward function rθ, and thus is indirectly parameterized by θ. Maximum
likelihood IRL (ML-IRL) [ 5] has a similar bi-level formulation with (1), however, ML-IRL only
solves an offline optimization problem and its analysis does not hold for non i.i.d. input data and
continuous state-action space. We discuss our distinctions from ML-IRL in Appendix A.1.
The upper-level loss function Lthas two terms. The first term −γtlogπθ(AE
t|SE
t)is the discounted
negative log-likelihood of the state-action pair (SE
t, AE
t)at time tand the second termλγt
2||θ−¯θ||2
is the discounted meta-regularization term [ 25] where λis a hyper-parameter. The likelihood function
is commonly used in IRL [ 5,6] to learn a reward function. Basically, the upper-level loss function at
timetencourages to find a reward function rθthat makes the observed state-action pair (SE
t, AE
t)
most likely and meanwhile, the reward parameter θshould not be too far from the prior experience,
i.e., the meta-prior ¯θ. Note that ¯θis a pre-trained meta-prior that embeds the information of “relevant
experience". We will introduce the training of ¯θin Subsection 4.3 and Appendix C.
The lower-level problem is used to compute πθusing the current reward function rθ. It proposes
to find a policy πθthat maximizes the entropy-regularized cumulative reward Jθ(π) +H(π). The
cumulative reward of a policy πunder the reward function rθisJθ(π)≜Eπ
S,A[P∞
t=0γtrθ(St, At)]
where the initial state is drawn from P0. The causal entropy of a policy πis defined as H(π)≜
Eπ
S,A[−P∞
t=0γtlogπ(At|St)].
Since the expert demonstrates {(SE
t, AE
t)}t≥0sequentially, we have a sequence of loss functions
{Lt(θ; (SE
t, AE
t))}t≥0. We use this sequence of loss functions (1)to formulate an online learning
problem. A typical online learning problem is to minimize the regret:PT−1
t=0Lt(θt; (SE
t, AE
t))−
minθPT−1
t=0Lt(θ; (SE
t, AE
t)). However, it is too challenging to minimize the regret in our case
because the loss function Ltcould be non-convex. Therefore, we aim to minimize the local regret
which is widely adopted in online non-convex optimization [ 26,27] and online IRL [ 11]. The local
regret quantifies the general stationarity of a sequence of loss functions under the learned parameters.
In specific, given a sequence of loss functions {ft(x)}t≥0, the local regret [ 11,26,27] at time t
is defined as ||1
t+1Pt
i=0∇fi(xt)||2which quantifies the gradient norms of the average of all the
previous loss functions under the current learned parameter xt. The total local regret is defined as
the sum of the local regret at each time t, i.e.,PT−1
t=0||1
t+1Pt
i=0∇fi(xt)||2. In our case, we replace
{ft}t≥0with the loss function {Lt}t≥0defined in (1)and thus formulate the local regret (2)-(3)
which has a bi-level formulation. We aim to minimize the following local regret:
E{(SE
t,AE
t)∼PπE
t(·,·)}t≥0T−1X
t=0||1
t+ 1tX
i=0∇Li(θt; (SE
i, AE
i))||2
, (2)
s.t.πθt= arg max
πJθt(π) +H(π), (3)
where (SE
t, AE
t)∼PπE
t(·,·)means that (SE
t, AE
t)is drawn from the state-action distribution
PπE
t(·,·), andPπ
t(·,·)is the state-action distribution induced by πat time tin the MDP.
Difficulties of solving problem (2)-(3). We want to design a fast algorithm to solve problem (2)-(3)
since we need to finish the update of θandπbefore the next state-action pair is observed and the
time between two consecutive state-action pairs can be short. However, designing and analyzing such
a fast algorithm is difficult due to the following challenges:
(i) First and foremost, current state-of-the-arts [ 26,27] on online non-convex optimization use follow-
the-leader-based algorithms which solve minθPt
i=0Li(θ; (SE
i, AE
i))to near stationarity at each
timet. This is time-consuming because they require multiple gradient descent updates of θ. One way
to alleviate this problem is to use online gradient descent (OGD) which only updates θby one gradient
descent step at each time t. However, since OGD does not solve the problem to near stationarity
at any time t, it is extremely difficult to quantify the overall stationarity after Titerations. While
OGD has been well studied in online convex optimization, it is rarely studied in online non-convex
optimization. The recent work [ 11] uses OGD to quantify the local regret, however, its analysis
can only hold when the input data is i.i.d. In contrast, the input data in our problem is not i.i.d. In
specific, the input data at time t(i.e.,(SE
t, AE
t)) is actually affected by the input data at last step (i.e.,
3(SE
t−1, AE
t−1)). This correlation between any two consecutive input data makes it difficult to analyze
the growth rate of the local regret.
(ii) Second, it is time-consuming if we fully solve the lower-level problem (3)to get πθbecause this
requires multiple policy updates to solve an RL problem. Therefore, we use a “single-loop" method
which only requires one-step policy update for a given rθ. However, since the policy is only updated
once, the updated policy can be far from πθand thus making the analysis difficult. Single-loop
methods are widely adopted to solve hierarchical problems, including bi-level optimization [ 28,29],
game theory [ 30,31], min-max problems [ 32,33], etc. Recently, single-loop methods are applied to
IRL [ 5], however, the paper [ 5] only solves an offline optimization problem and its analysis does not
hold for non i.i.d. input data and continuous state-action space. We include a section in Appendix
A.1 to discuss our distinctions from [5].
4 Algorithm and Theoretical Analysis
This section has three parts. The first part presents a novel online learning algorithm that solves
the problem (2)-(3)and tackles the aforementioned two difficulties. The second part proves that
Algorithm 1 achieves sub-linear local regret. If the reward function is linear, we prove that Algorithm
1 achieves sub-linear regret. The third part introduces a meta-learning method to get the meta-prior ¯θ.
4.1 The proposed algorithm
In practice, the expert will demonstrate a specific trajectory sE
0, aE
0, sE
1, aE
1,···. For distinction, we
use the capital letters (e.g., S) to represent random variables and the lower-case letters (e.g., s) to
represent specific values. To design a fast algorithm, we propose an online-gradient-descent-based
single-loop algorithm. In specific, at each time t, the algorithm updates both policy πand reward
parameter θonly once. The policy update is to solve the lower-level problem (3)and the reward
update is to solve the upper-level problem (2). In the following, we elaborate the procedure of policy
update and reward update.
Algorithm 1 Meta-regularized In-trajectory Inverse Reinforcement Learning (MERIT-IRL)
Input : Initialized policy π0, the streaming input data {(sE
t, aE
t)}t≥0
Output : Learned reward parameter θTand policy πT
1:Compute ¯θusing the meta-regularization in Section 4.3 and Appendix C, and set θ0=¯θ
2:fort= 0,1,···, T−1do
3: Compute the soft Q-function Qsoft
θt,πt(defined in Appendix B.1) under the current reward
function rθtand policy πt
4: Update πt+1(a|s)∝exp(Qsoft
θt,πt(s, a))for any (s, a)∈ S × A
5: Roll out policy πt+1twice: one starting from sE
0to get sE
0, a′
0, s′
1, a′
1,···, and the other
starting from (sE
t, aE
t)to get s′′
t+1, a′′
t+1,···
6: Compute gt=P∞
i=0γi∇θrθt(s′
i, a′
i)−P∞
i=0γi∇θrθt(s′′
i, a′′
i) +λ(1−γt+1)
1−γ(θt−¯θ)where
s′
0=sE
0and(s′′
i, a′′
i) = (sE
i, aE
i)for0≤i≤t
7: Update θt+1=θt−αtgt
8:end for
Policy update (lines 3-4 of Algorithm 1). At each time t, we only partially solve the lower-level
problem (3)via one-step soft policy iteration [ 5,34]. In specific, the soft policy iteration contains
two steps: policy evaluation and policy improvement. Policy evaluation aims to compute the soft
Q-function Qsoft
θt,πt(see the expression in Appendix B.1) under the current learned reward function
rθtand learned policy πt. Policy improvement aims to update policy according to πt+1(s, a)∝
exp(Qsoft
θt,πt(s, a))for any (s, a)∈ S × A . In practical implementations, πt+1can be obtained by
one-step policy update in soft Q-learning [34] or one-step actor update in soft actor-critic [35].
Reward update (lines 5-7 of Algorithm 1). At each time t, the algorithm observes (sE
t, aE
t)and
aims to leverage all the previously observed data to update the reward parameter. In specific, as
Lt(θ; (sE
t, aE
t)) =−γtlogπθ(aE
t|sE
t)+λγt
2||θ−¯θ||2is the meta-regularized negative log-likelihood
4of(sE
t, aE
t), the algorithm can formulatePt
i=0Li(θ; (sE
i, aE
i))at time tusing all the previously
collected data (i.e., {(sE
i, aE
i)}i=0,···,t). To update the reward parameter, the algorithm partially
minimizesPt
i=0Li(θ; (sE
i, aE
i))via one-step gradient descent.
Lemma 1. The gradient ofPt
i=0Li(θ; (sE
i, aE
i))can be calculated as follows:
∇tX
i=0Li(θ; (sE
i, aE
i)) =Eπθ
S,A∞X
i=0γi∇θrθ(Si, Ai)S0=sE
0
+λ(1−γt+1)
1−γ(θ−¯θ)
−tX
i=0γi∇θrθ(sE
i, aE
i)−Eπθ
S,A∞X
i=t+1γi∇θrθ(Si, Ai)St=sE
t, At=aE
t
. (4)
Note that the gradient (4)holds for continuous state-action space. Since the gradient (4)has expecta-
tion terms under the policy πθ, we can only approximate it. In specific, we roll out the policy πt+1
twice: one starting from sE
0to get a trajectory {(s′
i, a′
i)}i≥0where s′
0=sE
0, and the other one starting
from (sE
t, aE
t)to get a trajectory {(s′′
i, a′′
i)}i≥0where (s′′
i, a′′
i) = (sE
i, aE
i)for0≤i≤t. Then we
use the empirical estimate gt=P∞
i=0γi∇θrθt(s′
i, a′
i)−P∞
i=0γi∇θrθt(s′′
i, a′′
i)+λ(1−γt+1)
1−γ(θt−¯θ)
to approximate ∇Pt
i=0Li(θt; (sE
i, aE
i)). With the gradient approximation gt, we utilize stochastic
online gradient descent θt+1=θt−αtgtto update the reward parameter.
Discussion on our special design of the reward update . The right subfigure in Figure 1 visualizes
our reward update (modulo the meta-regularization term). The green trajectory (i.e., {(sE
t, aE
t)}t≥0) is
the expert trajectory, and the red trajectories (i.e., {(s′
t, a′
t)}t≥0and{(s′′
i, a′′
i)}i>t) are the trajectories
generated by the learned policy. Given the expert trajectory prefix (i.e., the incomplete trajectory
{(sE
i, aE
i)}t
i=0observed so far), our method completes the expert trajectory by rolling out the learned
policy starting from (sE
t, aE
t)and filling the trajectory suffix {(s′′
i, a′′
i)}i>t. The combined complete
trajectory includes the expert trajectory prefix {(sE
i, aE
i)}t
i=0and the learner-filled trajectory suffix
{(s′′
i, a′′
i)}i≥t. We update the reward function by comparing this combined trajectory to a complete
trajectory {(s′
t, a′
i)}t≥0generated by the learned policy starting from the expert’s initial state sE
0.
A more straightforward way for the reward update is to directly compare the trajectory prefixes
(visualized in the middle of Figure 1) at each time t. However, this naive method can be problematic.
We explain the issue of this naive method and the advantage of our method in the following context.
Figure 1: Standard IRL (left), the naive method for in-
trajectory learning (middle), and our method (right).Figure 1 visualizes the reward up-
date for standard IRL (left), the naive
method (i.e., directly run standard
IRL algorithms on the expert trajec-
tory prefix) (middle), and our method
(right). The standard IRL (left) up-
dates the reward function by com-
paring the complete expert trajec-
tory and the complete trajectory gen-
erated by the learned policy. This
case is ideal, however, it is infeasible
when the trajectory is ongoing and
we can only observe an incomplete
expert trajectory {(sE
i, aE
i)}t
i=0at
each time t. The naive method (mid-
dle) updates the reward function by simply comparing the expert trajectory prefix {(sE
i, aE
i)}t
i=0
and the incomplete trajectory {(s′
i, a′
i)}t
i=0with the same length generated by the learned policy
starting from the expert’s initial state sE
0. This kind of reward update is myopic as it does not consider
for the future. Since it runs standard IRL on the trajectory prefix observed so far, it will always
regard the current state-action pair as the terminal state-action pair and thus has no ability to consider
the state-action pairs in the future. In contrast, our special design gives the algorithm the ability
to consider for the future. Instead of only comparing incomplete trajectory prefixes, our method
compares complete trajectories just as the standard IRL. In specific, we compare the combined
complete trajectory {{(sE
i, aE
i)}t
i=0,{(s′′
i, a′′
i)}i>t}and the complete trajectory {(s′
i, a′
i)}i≥0solely
generated by the learned policy. The comparison between the learner prefix {s′
i, a′
i}t
i=0and expert
5prefix{sE
i, aE
i}t
i=0encourages the learned reward function to explain the expert’s demonstrated be-
haviors so far, and the comparison between the suffixes ( {s′
i, a′
i}i>tand{s′′
i, a′′
i}i>t) encourages that
we are learning a reward function that is useful for predicting the future. Note that as tincreases, the
expert trajectory prefix weights more and more in the combined complete trajectory, and eventually
we will recover the standard IRL reward update when tgoes to infinity. In Theorems 1 and 2, we
theoretically guarantee that the proposed reward update can achieve sub-linear (local) regret. This
shows the perfect consistency between the intuition and theory.
4.2 Theoretical analysis
To quantify the local regret of Algorithm 1, we have two challenges: (i) Since we only update πby
one step at each time t, we have πt+1instead of the optimal solution πθtof the lower-level problem
(3). The policy πt+1can be far away from πθtand thus the empirical gradient estimate gtcan be a
bad approximation of the gradient (4). (ii) Since we only update θonce at each time tinstead of
finding a near-stationary point θ′such that ||Pt
i=0Li(θ′; (sE
i, aE
i))|| ≤ϵas in [ 26,27], the gradient
norm||Pt
i=0∇Li(θt; (sE
i, aE
i))||is not stabilized under the threshold ϵat every time t. Therefore
the local regret (i.e.,PT−1
t=0||1
t+1Pt
i=0∇Li(θt; (sE
i, aE
i))||2) is hard to quantify and may not be
sub-linear in T. What’s worse, the input data is not i.i.d. but correlated, i.e., the input data (sE
t, aE
t)
at time tis affected by the input data (sE
t−1, aE
t−1)at last step. This correlation makes it even more
difficult to quantify the local regret.
To solve the first challenge, we adopt the idea of two-timescale stochastic approximation [ 28] where
the lower level updates in a faster timescale and the upper level updates in a slower timescale. The
policy update is faster because it converges linearly under a fixed reward function [ 36] while the
reward update is slower given that we choose αt∝(t+ 1)−1/2. Intuitively, since the policy update
is faster than the reward update, the reward parameter is “relatively fixed" compared to the policy. It
is expected that πt+1shall stay close to πθtand at last converges to πθtwhen tincreases.
To solve the second challenge, we divide our analysis into two steps: (i) We quantify the difference of
the gradient norms between the current correlated state-action distribution PπE
t(·,·)and a stationary
state-action distribution for any loss function Li,i≥0. Note that PπE
t+1(·,·)is affected by PπE
t(·,·).
(ii) We quantify the local regret under the stationary distribution. The benefit of doing so is that the
input data is i.i.d. under the stationary distribution, and thus we can cast the online gradient descent
method as a stochastic gradient descent method and quantify its local regret. Finally, we can quantify
the local regret under the current correlated distribution PπE
t(·,·)by combining (i) and (ii).
We start our analysis with the definitions of stationary state distribution and stationary state-action
distribution. For a given policy π, the corresponding stationary state distribution is µπ(s)≜(1−
γ)P∞
t=0γtPπ
t(s)and the stationary state-action distribution is µπ(s, a)≜(1−γ)P∞
t=0γtPπ
t(s, a).
Assumption 1. The parameterized reward function rθsatisfies |rθ1(s, a)−rθ2(s, a)| ≤¯Cr||θ1−θ2||
and||∇θrθ1(s, a)− ∇ θrθ2(s, a)|| ≤ ˜Cr||θ1−θ2||for any (θ1, θ2)and any (s, a)∈ S × A where
¯Crand˜Crare positive constants.
Assumption 2 (Ergodicity) .There exist constants CM>0andρ∈(0,1)such that for any
policy πand any t≥0, the following holds for the Markov chain induced by the policy πand
the state transition function P:supS0∼P0dTV(Pπ
t(·), µπ(·))≤CMρtwhere dTV(P1(·),P2(·))≜
1
2R
s∈S|P1(s)−P2(s)|dsis the total variation distance between the two state distributions P1and
P2,S0is the initial state, and Pπ
t(·)is the state distribution induced by the policy πat time t.
Assumptions 1-2 are common in RL [ 37,38,39,40]. Assumption 2 holds for any time-homogeneous
Markov chain with finite state space or any uniformly ergodic Markov chain with general state space.
Proposition 1. Suppose Assumptions 1-2 hold and αt∈(0,1−γ
λ), we have the following relation for
anyi≥0and any θt, t≥0:E(SE
i,AE
i)∼PπE
i(·,·)
||∇Li(θt; (SE
i, AE
i))||2
−E(SE
i,AE
i)∼µπE(·,·)
||∇Li(θt; (SE
i, AE
i))||2,
≤8CM¯C2
r 2−γ
1−γ2ρiγ2i,
where (SE
i, AE
i)∼PπE
i(·,·)means that (SE
i, AE
i)is drawn from the correlated distribution PπE
i(·,·)
and(SE
i, AE
i)∼µπE(·,·)means that (SE
i, AE
i)is drawn from the stationary distribution µπE(·,·).
6Proposition 1 quantifies the gap of gradient norms between the current correlated distribution PπE
i
and the stationary distribution µπE. We next quantify the local regret under the stationary distribution
µπEwith the following lemma:
Lemma 2. Suppose Assumptions 1-2 hold and choose αt=(1−γ)(t+1)−1/2
λ, it holds that:
E{(SE
t,AE
t)∼µπE(·,·)}t≥0T−1X
t=0||1
t+ 1tX
i=0∇Li(θt; (SE
i, AE
i))||2
≤D1(logT+ 1) + D2√
T+D3√
T(logT+ 1),
where D1,D2, and D3are positive constants whose expressions can be found in Appendix B.4.
Lemma 2 quantifies the local regret under the stationary distribution µπE. With Proposition 1 and
Lemma 2, we can quantify the local regret under the current correlated distribution.
Theorem 1. Suppose Assumptions 1-2 hold and choose αt=(1−γ)(t+1)−1/2
λ, we have that:
E{(SE
t,AE
t)∼PπE
t(·,·)}t≥0T−1X
t=0||1
t+ 1tX
i=0∇Li(θt; (SE
i, AE
i))||2
≤
D1+8CM¯C2
r(2−γ)2
(1−ργ2)(1−γ)2
(logT+ 1) + D2√
T+D3√
T(logT+ 1).
Theorem 1 is based on Proposition 1 and Lemma 2. It shows that Algorithm 1 achieves sub-linear
local regret. Moreover, if the reward function is linear, Algorithm 1 achieves sub-linear regret:
Theorem 2. Suppose the expert reward function rEand the parameterized reward rθare linear, and
Assumptions 1-2 hold. Choose αt=1−γ
λ(t+1)(1−γt+1)we have that:
E{(SE
t,AE
t)∼PπE
t(·,·)}t≥0T−1X
t=0Lt(θt; (SE
t, AE
t))
−min
θE{(SE
t,AE
t)∼PπE
t(·,·)}t≥0T−1X
t=0Lt(θ; (SE
t, AE
t))
≤D4+D5(logT+ 1),
where D4andD5are positive constants whose expressions are in Appendix B.6.
4.3 Meta-Regularization
Since there is only one training trajectory and this trajectory is not complete during the learning
process, we need to add a regularization term to avoid overfitting. Inspired by humans’ using relevant
experience to help do inference, we introduce the meta-regularizationλ
2||θ−¯θ||2where λis the
hyper-parameter and the meta-prior ¯θis learned from “relevant experience". In specific, we introduce
a set of relevant tasks {Tj}j∼PTwhere each task Tjis an IRL problem and PTis the implicit
task distribution. The tasks {Tj}j∼PTare relevant in the sense that they share the components
(S,A, γ, P 0, P)of the MDP with our in-trajectory learning problem. However, the expert’s reward
functions of different tasks are different and are drawn from an unknown reward function distribution.
For example, in the stock market case mentioned in the introduction, the experts of different tasks
invest in the same stock market but may have different preferences. As standard in meta-learning
[41,42,43], we assume that the expert’s reward function rEof our in-trajectory learning problem is
also drawn from the same unknown reward function distribution. Note that the reward functions of
the relevant tasks {Tj}j∼PTare different from rEeven if they are drawn from the same unknown
reward function distribution.
For each task Tj, there is a batch of trajectories and we divide this batch into two sets, i.e., Dtr
j
andDeval
j. The training set Dtr
jonly has one trajectory, just as our in-trajectory learning problem,
and the evaluation set Deval
jhas abundant trajectories. Define the loss function on a certain data
setD≜{ζv}m
v=1asL(θ,D)≜−Pm
v=1P∞
t=0γtlogπθ(av
t|sv
t). The goal of each task Tjis to
learn a task-specific adaptation ϕjusing the training set Dtr
j, such that ϕjcan minimize the test
7lossL(ϕj,Deval
j)on the evaluation set Deval
j. The goal of meta-regularization is to find a meta-prior
¯θ, from which such task-specific adaptations ϕjcan be adapted to all tasks {Tj}j∼PT. In specific,
meta-regularization [ 25] proposes a bi-level optimization problem (5). The lower-level problem uses
only one trajectory Dtr
jto find the task-specific adaptation ϕjsuch that the meta-regularized loss
function L(ϕ,Dtr
j) +λ
2(1−γ)||ϕ−¯θ||2is minimized. The upper-level problem is to find a meta-prior
¯θsuch that the corresponding task-specific adaptations {ϕj}j∼PTcan minimize the expected loss
function L(ϕj,Deval
j)over the evaluation sets of all tasks {Ti}j∼PT.
min
¯θEj∼PT
L(ϕj,Deval
j)
,s.t.ϕj= arg min
ϕL(ϕ,Dtr
j) +λ
2(1−γ)||ϕ−¯θ||2. (5)
The lower-level loss function in (5)is the offline version of our in-trajectory loss function (1)(i.e.,
L(ϕ,Dtr
j) +λ
2(1−γ)||ϕ−¯θ||2=P∞
t=0Lt(ϕ; (str
t, atr
t))) where (str
t, atr
t)∈ Dtr
j. Our in-trajectory
learning problem can also be regarded as to find a task-specific adaptation. Note that the in-trajectory
learning problem is online while the lower-level problem in (5)is offline because the in-trajectory
problem is ongoing where we keep observing new state-action pairs. In contrast, the lower-level
problem in (5)is based on “experience" that has already happened. Due to the space limit, we include
the algorithm and theoretical guarantees of solving the problem (5) in Appendix C.
5 Experiments
We present three experiments to show the effectiveness of MERIT-IRL. We use four baselines for
comparisons. (i) IT-IRL : this method is MERIT-IRL without meta-regularization. (ii) Naive MERIT-
IRL: this method has the meta-regularization term but uses the naive way (in the middle of Figure 1)
to update reward. (iii) Naive IT-IRL : this method uses the naive way to update the learned reward
and does not have the meta-regularization term. (iv) Hindsight : this method is meta-regularized
ML-IRL [ 5] which can access the complete expert trajectory and uses the standard IRL (visualized in
the left of Figure 1) with meta-regularization to update the learned reward. The experiment details
are in Appendix D.
5.1 MuJoCo experiment
In this subsection, we consider the target velocity problem for three MuJoCo robots: HalfCheetah,
Walker, and Hopper. The target velocity problem is widely used in meta-RL [ 44] and meta-IRL
[45]. In specific, the robots aim to maintain a target velocity in each task and the target velocity
of different tasks is different. To test the performance of MERIT-IRL, we use 10test tasks whose
target velocity is randomly between 1.5and2.0. In the test tasks, there is only one expert trajectory
and the state-action pairs of this trajectory are sequentially revealed (to MERIT-IRL, IT-IRL, Naive
MERIT-IRL, and Naive IT-IRL) in an online fashion. The baseline Hindsight uses the complete
expert trajectory to learn a reward function. The ground truth reward is designed as −|v−vtarget|(as
in [44]) where vis the current robot velocity and vtarget is the target velocity. To learn the meta-prior
¯θ, we use 50relevant tasks whose target velocity is randomly between 0and3.
Figures 2a-2c show the in-trajectory learning performance where the x-axis is the time step tof
the expert trajectory and the y-axis is the cumulative reward of the learned policy πtwhen only
the first tsteps of the expert trajectory are observed. The x-limit is 1,000because the trajectory
length in MuJoCo is 1,000. Note that the baseline “Hindsight" is not in-trajectory learning since
it learns from a complete expert trajectory. For comparison, we use two horizontal lines (close to
each other) to show the performance of Hindsight and the expert in the figures. Figure 2a shows that
MERIT-IRL achieves similar performance with the expert when only 40% of the complete expert
trajectory ( t= 400 ) is observed while IT-IRL can only achieve performance close to the expert after
observing more than 90% of the complete expert trajectory ( t= 900 ). This shows the effectiveness
of the meta-regularization. Naive MERIT-IRL and Naive IT-IRL fail to imitate the expert even if
the complete expert trajectory is observed ( t= 1,000). This shows the effectiveness of our special
design of the reward update. The discussions on Figures 2b and 2c are in Appendix D.2.
Table 1 shows the results after observing the complete expert trajectory. MERIT-IRL performs much
better than IT-IRL, Naive MERIT-IRL, and Naive IT-IRL. MERIT-IRL achieves similar performance
8Table 1: Experiment results. The mean and standard deviation are calculated from 10test tasks.
MERIT-IRL IT-IRL Naive MERIT-IRL Naive IT-IRL Hindsight Expert
HalfCheetah −214.63±53.96−386.78±152.65−548.22±40.51 −765.27±104.41 −208.74±37.23 −181.51±28.35
Walker −654.77±102.59−891.79±156.90−962.42±111.60−1349.25±158.88−648.17±157.92−634.17±120.57
Hopper −476.72±32.09 −669.88±53.63 −691.03±93.35 −1112.06±74.33 −455.70±74.93 −421.74±84.30
Stock market 386.70±62.95 256.81±68.61 192.49±75.34 72.33±16.73 390.30±77.37 403.15±61.94
(a) HalfCheetah
 (b) Walker
 (c) Hopper
 (d) Stock Market
Figure 2: In-trajectory learning performance.
with Hindsight and expert. Note that it is not expected that MERIT-IRL outperforms Hindsight since
Hindsight uses the complete expert trajectory to learn.
5.2 Stock market experiment
RL to train a stock trading agent has been widely studied in AI for finance [ 46,47,48]. In this
experiment, we use IRL to learn the reward function (i.e., investing preference) of the target investor
in a stock market scenario. In specific, we use the real-world data of 30constituent stocks in Dow
Jones Industrial Average from 2021-01-01 to 2022-01-01. We use a benchmark called “FinRL" [ 48]
to configure the real-world stock data into an MDP environment. The target investor (i.e., expert) has
an initial asset of $1,000and trades stocks on every stock market opening day. The stock market
opens 252 days between 2021-01-01 and 2022-01-01, and thus the trajectory length is 252. The
reward function of the target investor is defined as p1−p2where p1is the investor’s profit which is
the money earned from trading stocks subtracting the transaction cost, and p2models the investor’s
preference of whether willing to take risks. In specific, p2is positive if the investor buys stocks whose
turbulence indices are larger than a certain turbulence threshold, and zero otherwise. The value of p2
depends on the type and amount of the trading stocks. The turbulence thresholds of different investors
are different. The turbulence index measures the price fluctuation of a stock. If the turbulence index
is high, the corresponding stock has a high fluctuating price and thus is risky to buy [ 48]. Therefore,
an investor unwilling to take risks has a relatively low turbulence threshold. We include experiment
details in Appendix D.3. To test performance, we use 10test tasks whose turbulence thresholds are
randomly between 45and50. To learn ¯θ, we use 50relevant tasks whose turbulence thresholds are
randomly between 30and60.
Figure 2d shows that MERIT-IRL achieves similar cumulative reward with the expert at t= 140
which is less than 60% of the whole trajectory, while the three in-trajectory baselines fail to imitate
the expert before the ongoing trajectory terminates. The last row in Table 1 shows that MERIT-IRL
achieves similar performance with Hindsight and the expert. More discussions on the results are in
Appendix D.3.
5.3 Learning from a shooter’s ongoing trajectory
This part presents the experiment of learning from an ongoing shooter trajectory. Following [ 12],
we model the shooter’s movement as a navigation problem. We build a simulator in Gazebo (Figure
3a) where the shooter moves from the door (lower left corner) to the red target (upper right corner).
The learner observes the ongoing trajectory of the shooter and keeps updating the learned reward and
policy. In our case, the complete shooter trajectory has the length of 140. Figures 3b-3g show our
in-trajectory learning performance where the heat maps visualize the learned reward. We normalize
the learned reward to [0,1]. We can observe that as the ongoing trajectory is expanding, the learned
reward function becomes more and more precise to locate the goal area. When t= 40 , we cannot tell
9the goal area from the heat map (Figure 3b). However, as the time tgrows, we can almost locate the
goal area when t= 60 (Figure 3c) and precisely locate the goal area when t= 80 (Figure 3d).
(a) Gazebo environment
0.00.20.40.60.81.0 (b) t= 40
 (c)t= 60
 (d) t= 80
(e)t= 100
 (f)t= 120
 (g) t= 140
 (h) Success rate
Figure 3: Learning performance on the active shooting scenario.
Figure 3h shows the policy learning performance. Since there is no ground truth reward in this
problem, we use “success rate" to quantify the performance of the learned policy. The success rate is
the rate that the learned policy successfully reaches the goal. From 3h, we can see that MERIT-IRL
outperforms the other baselines and can achieve 100% success rate when t= 80 (i.e., only observing
57% of the complete trajectory). Note that we do not include Hindsight and Expert in Figure 3 since
they both achieve 100% success rate.
6 Conclusion
This paper proposes MERIT-IRL, the first in-trajectory inverse reinforcement learning theoretical
framework that learns a reward function while observing an initial portion of a trajectory and keeps
updating the learned reward function when extended portions (i.e., new state-action pairs) of the
trajectory are observed. Experiments show that MERIT-IRL can imitate the expert from the ongoing
expert trajectory before it terminates.
7 Acknowledgements
This work is partially supported by the National Science Foundation through grants ECCS 1846706
and ECCS 2140175. We would like to thank the reviewers for their insightful and constructive
suggestions.
10References
[1]P. Abbeel and A. Y . Ng, “Apprenticeship learning via inverse reinforcement learning,” in
International Conference on Machine Learning , pp. 1–8, 2004.
[2]N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, “Maximum margin planning,” in International
Conference on Machine Learning , pp. 729–736, 2006.
[3]B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, “Maximum entropy inverse reinforce-
ment learning,” in National Conference on Artificial intelligence , pp. 1433–1438, 2008.
[4]B. D. Ziebart, J. A. Bagnell, and A. K. Dey, “Modeling interaction via the principle of maximum
causal entropy,” in International Conference on Machine Learning , pp. 1255–1262, 2010.
[5]S. Zeng, C. Li, A. Garcia, and M. Hong, “Maximum-likelihood inverse reinforcement learning
with finite-time guarantees,” in Advances in Neural Information Processing Systems , 2022.
[6]S. Liu and M. Zhu, “Distributed inverse constrained reinforcement learning for multi-agent
systems,” Advances in Neural Information Processing Systems , vol. 35, pp. 33444–33456, 2022.
[7]D. Ramachandran and E. Amir, “Bayesian inverse reinforcement learning.,” in International
Joint Conference on Artificial Intelligence , pp. 2586–2591, 2007.
[8]A. J. Chan and M. van der Schaar, “Scalable bayesian inverse reinforcement learning,” in
International Conference on Learning Representations , 2021.
[9]N. Rhinehart and K. M. Kitani, “First-person activity forecasting with online inverse rein-
forcement learning,” in IEEE International Conference on Computer Vision , pp. 3696–3705,
2017.
[10] S. Arora, P. Doshi, and B. Banerjee, “Online inverse reinforcement learning under occlusion,”
inInternational Conference on Autonomous Agents and Multiagent Systems , pp. 1170–1178,
2019.
[11] S. Liu and M. Zhu, “Learning multi-agent behaviors from distributed and streaming demonstra-
tions,” Advances in Neural Information Processing Systems , vol. 36, 2024.
[12] A. Aghalari, N. Morshedlou, M. Marufuzzaman, and D. Carruth, “Inverse reinforcement
learning to assess safety of a workplace under an active shooter incident,” IISE Transactions ,
vol. 53, no. 12, pp. 1337–1350, 2021.
[13] Q. Sun, X. Gong, and Y .-W. Si, “Transaction-aware inverse reinforcement learning for trading
in stock markets,” Applied Intelligence , vol. 53, no. 23, pp. 28186–28206, 2023.
[14] J. Chang and W. Tu, “A stock-movement aware approach for discovering investors’ personalized
preferences in stock markets,” in International Conference on Tools with Artificial Intelligence ,
pp. 275–280, 2018.
[15] X. Hu, Y . Chen, L. Ren, and Z. Xu, “Investor preference analysis: An online optimization
approach with missing information,” Information Sciences , vol. 633, pp. 27–40, 2023.
[16] Y . Xu, W. Gao, and D. Hsu, “Receding horizon inverse reinforcement learning,” Advances in
Neural Information Processing Systems , vol. 35, pp. 27880–27892, 2022.
[17] G. Swamy, D. Wu, S. Choudhury, D. Bagnell, and S. Wu, “Inverse reinforcement learning
without reinforcement learning,” in International Conference on Machine Learning , pp. 33299–
33318, 2023.
[18] K. Yan, A. Schwing, and Y .-X. Wang, “A simple solution for offline imitation from observations
and examples with possibly incomplete trajectories,” Advances in Neural Information Processing
Systems , 2024.
[19] M. Sun and X. Ma, “Adversarial imitation learning from incomplete demonstrations,” in
International Joint Conference on Artificial Intelligence , pp. 3513–3519, 2019.
11[20] D. Xu, F. Zhu, Q. Liu, and P. Zhao, “Arail: Learning to rank from incomplete demonstrations,”
Information Sciences , vol. 565, pp. 422–437, 2021.
[21] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in Advances in Neural Infor-
mation Processing Systems , pp. 4572–4580, 2016.
[22] G. Qiao, G. Liu, P. Poupart, and Z. Xu, “Multi-modal inverse constrained reinforcement learning
from a mixture of demonstrations,” Advances in Neural Information Processing Systems , vol. 36,
2024.
[23] S. Liu and M. Zhu, “Meta inverse constrained reinforcement learning: Convergence guarantee
and generalization analysis,” in International Conference on Learning Representations , 2023.
[24] G. Qiao, G. Quan, R. Qu, and G. Liu, “Modelling competitive behaviors in autonomous driving
under generative world model,” in European Conference on Computer Vision , 2024.
[25] A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine, “Meta-learning with implicit gradients,”
inAdvances in Neural Information Processing Systems , pp. 113–124, 2019.
[26] E. Hazan, K. Singh, and C. Zhang, “Efficient regret minimization in non-convex games,” in
International Conference on Machine Learning , pp. 1433–1441, 2017.
[27] N. Hallak, P. Mertikopoulos, and V . Cevher, “Regret minimization in stochastic non-convex
learning via a proximal-gradient approach,” in International Conference on Machine Learning ,
pp. 4008–4017, 2021.
[28] M. Hong, H.-T. Wai, Z. Wang, and Z. Yang, “A two-timescale framework for bilevel optimiza-
tion: Complexity analysis and application to actor-critic,” arXiv preprint arXiv:2007.05170 ,
2020.
[29] T. Chen, Y . Sun, Q. Xiao, and W. Yin, “A single-timescale method for stochastic bilevel
optimization,” in International Conference on Artificial Intelligence and Statistics , pp. 2466–
2488, 2022.
[30] F. Schäfer and A. Anandkumar, “Competitive gradient descent,” Advances in Neural Information
Processing Systems , pp. 7625–7635, 2019.
[31] V . S. Varma, J. Veetaseveera, R. Postoyan, and I.-C. Mor ˘arescu, “Distributed gradient methods
to reach a nash equilibrium in potential games,” in IEEE Conference on Decision and Control ,
pp. 3098–3103, 2021.
[32] S. Boyd and L. Vandenberghe, Convex Optimization . Cambridge university press, 2004.
[33] J. Zhang, P. Xiao, R. Sun, and Z. Luo, “A single-loop smoothed gradient descent-ascent algo-
rithm for nonconvex-concave min-max problems,” Advances in Neural Information Processing
Systems , pp. 7377–7389, 2020.
[34] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement learning with deep energy-based
policies,” in International Conference on Machine Learning , pp. 1352–1361, 2017.
[35] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy maximum entropy
deep reinforcement learning with a stochastic actor,” in International Conference on Machine
Learning , pp. 1861–1870, 2018.
[36] S. Cen, C. Cheng, Y . Chen, Y . Wei, and Y . Chi, “Fast global convergence of natural policy
gradient methods with entropy regularization,” Operations Research , vol. 70, no. 4, pp. 2563–
2578, 2022.
[37] Z. Zheng, F. Gao, L. Xue, and J. Yang, “Federated Q-learning: Linear regret speedup with low
communication cost,” arXiv preprint arXiv:2312.15023 , 2023.
[38] H. Gong and M. Wang, “A duality approach for regret minimization in average-award ergodic
markov decision processes,” in Conference on Learning for Dynamics and Control , vol. 120,
pp. 862–883, 2020.
12[39] Z. Zheng, H. Zhang, and L. Xue, “Federated Q-learning with reference-advantage de-
composition: Almost optimal regret and logarithmic communication cost,” arXiv preprint
arXiv:2405.18795 , 2024.
[40] Z. Zheng, H. Zhang, and L. Xue, “Gap-dependent bounds for Q-learning using reference-
advantage decomposition,” arXiv preprint arXiv:2410.07574 , 2024.
[41] S. Xu and M. Zhu, “Efficient gradient approximation method for constrained bilevel optimiza-
tion,” in AAAI Conference on Artificial Intelligence , vol. 37, pp. 12509–12517, 2023.
[42] S. Xu and M. Zhu, “Meta value learning for fast policy-centric optimal motion planning,” in
Robotics science and systems , 2022.
[43] S. Xu and M. Zhu, “Online constrained meta-learning: provable guarantees for generalization,”
Advances in Neural Information Processing Systems , vol. 36, 2024.
[44] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast adaptation of deep
networks,” in International Conference on Machine Learning , pp. 1126–1135, 2017.
[45] S. K. Seyed Ghasemipour, S. S. Gu, and R. Zemel, “Smile: Scalable meta inverse reinforcement
learning through context-conditional policies,” in Advances in Neural Information Processing
Systems , pp. 7881–7891, 2019.
[46] Y . Deng, F. Bao, Y . Kong, Z. Ren, and Q. Dai, “Deep direct reinforcement learning for financial
signal representation and trading,” IEEE Transactions on Neural Networks and Learning
Systems , vol. 28, no. 3, pp. 653–664, 2016.
[47] Z. Zhang, S. Zohren, and S. Roberts, “Deep reinforcement learning for trading,” The Journal of
Financial Data Science , vol. 2, no. 2, pp. 25–40, 2020.
[48] X.-Y . Liu, H. Yang, J. Gao, and C. D. Wang, “Finrl: Deep reinforcement learning framework to
automate trading in quantitative finance,” in ACM International Conference on AI in Finance ,
pp. 1–9, 2021.
[49] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, “Human behavior modeling with
maximum entropy inverse optimal control.,” in AAAI Spring Symposium: Human Behavior
Modeling , vol. 92, 2009.
[50] M. Monfort, A. Liu, and B. D. Ziebart, “Intent prediction and trajectory forecasting via predictive
inverse linear-quadratic regulation,” in AAAI Conference on Artificial Intelligence , pp. 3672–
3678, 2015.
[51] S. Gaurav and B. Ziebart, “Discriminatively learning inverse optimal control models for pre-
dicting human intentions,” in International Conference on Autonomous Agents and MultiAgent
Systems , pp. 1368–1376, 2019.
[52] W. Krichene, M. Balandat, C. Tomlin, and A. Bayen, “The hedge algorithm on a continuum,” in
International Conference on Machine Learning , pp. 824–832, 2015.
[53] N. Agarwal, A. Gonen, and E. Hazan, “Learning in non-convex games with an optimization
oracle,” in Conference on Learning Theory , pp. 18–29, 2019.
[54] D. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon, “Iq-learn: Inverse soft-q learning for
imitation,” Advances in Neural Information Processing Systems , vol. 34, pp. 4028–4039, 2021.
[55] M.-F. Balcan, M. Khodak, and A. Talwalkar, “Provable guarantees for gradient-based meta-
learning,” in International Conference on Machine Learning , pp. 424–433, 2019.
[56] K. Xu, E. Ratner, A. Dragan, S. Levine, and C. Finn, “Learning a prior over intent via meta-
inverse reinforcement learning,” in International Conference on Machine Learning , pp. 6952–
6962, 2019.
[57] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning in neural networks:
A survey,” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 44, no. 9,
pp. 5149–5169, 2021.
13[58] X.-Y . Liu, Z. Xia, J. Rui, J. Gao, H. Yang, M. Zhu, C. Wang, Z. Wang, and J. Guo, “FinRL-
Meta: Market environments and benchmarks for data-driven financial reinforcement learning,”
Advances in Neural Information Processing Systems , vol. 35, pp. 1835–1849, 2022.
14This appendix consists of four parts: related works, proof, meta-regularization algorithm and conver-
gence guarantee, and Experiment details.
A Related works
Applying IRL to predict from ongoing trajectories . Papers [ 49,50,51] use standard IRL to
predict goals of incomplete (or ongoing) trajectories. In specific, they first learn the reward function
corresponding to each potential goal candidate from complete trajectories in the training phase and
then use Bayesian methods to pick the most likely goal candidate of incomplete trajectories in the
testing phase. However, these works are not in-trajectory learning since they do not learn a reward
function from an incomplete ongoing trajectory.
Online non-convex optimization . This paper casts the in-trajectory learning problem as an online
non-convex bi-level optimization problem where at each online iteration, a new state-action pair is
input. Current literature on online non-convex optimization has two major categories. The first one
is to use the regret originally defined in online convex optimization [ 52,53]. However, it assumes
to find a global optimal solution of a non-convex optimization problem at each online iteration.
Therefore, the second category studies “local regret" [ 26,27] and uses follow-the-leader-based
methods to minimize the local regret. However, the follow-the-leader-based methods need to solve a
non-convex optimization problem to obtain a near-stationary point at each online iteration, which
can be computationally expensive and time-consuming. If the streaming data arrives at a fast speed,
the computation at each online iteration may not be finished before the next data arrives. The
computational burden of each online iteration can be mitigated by online gradient descent (OGD)
methods where we only partially solve the non-convex optimization problem by one-step gradient
descent at each online iteration. While OGD is widely studied in online convex optimization, it
is rarely studied in online non-convex optimization. [ 11] uses OGD to quantify the local regret,
however, its analysis only holds when the input data is identically independent distributed (i.i.d.). In
contrast, the input data in our problem is not i.i.d. In specific, the input data at time t(i.e.,(SE
t, AE
t))
is affected by the input data at last step (i.e., (SE
t−1, AE
t−1)). This temporal correlation between any
two consecutive input data makes it difficult to analyze the growth rate of the local regret.
Regularization and meta-learning in IRL . Moreover, the data of in-trajectory learning is extremely
lacking since there is only one demonstrated trajectory and this trajectory is not complete during
the learning process. The lack of data can easily lead to overfitting and a common way to alleviate
this problem is to use regularizers [ 21,54]. Inspired by humans’ using relevant experience to
help the inference, we introduce a novel regularization method called meta-regularization [ 25,55].
Compared to the regularizers commonly used in IRL [ 21,54], the meta-regularizer provides human-
experience-like prior information which helps recover the reward function from few data. Similar to
the meta-initialization method [ 55,44] commonly used in IRL [ 56], meta-regularization provides
an initialization that the algorithm starts at. However, more importantly, meta-regularization also
provides a regularization term to avoid overfitting.
A.1 Distinction from Maximum-likelihood inverse reinforcement learning (ML-IRL) [5]
We discuss our distinctions from ML-IRL from the following three aspects: problem setting, algorithm
design, and theoretical analysis.
Distinction in problem setting . We study in-trajectory IRL and formulate an online optimization
problem, while ML-IRL studies standard IRL and formulates an offline optimization problem.
Distinctions in algorithm design . ML-IRL and our algorithm both update policy and reward in
a single loop. However, we propose a novel reward update mechanism specially designed for the
in-trajectory learning case. This special design requires to use the current learned policy to complete
the expert trajectory, which gives the algorithm the ability to consider for the future. This special
design of reward update is novel compared to ML-IRL.
Distinctions in theoretical analysis . The analysis in our paper is substantially different from that in
ML-IRL due to three facts: (1) The input data in our paper is not i.i.d., while the input data in ML-IRL
is i.i.d. (2) We solve an online optimization problem, while ML-IRL solves an offline optimization
problem. (3) Our analysis holds for continuous state-action space, while the analysis of ML-IRL is
15limited to finite state-action space. We now discuss the distinctions in theoretical analysis caused by
the three facts in detail.
In our case, the input data is not i.i.d. but temporally correlated, i.e., the input data (st, at)is affected
by the input data (st−1, at−1)at last time step. In contrast, the input data in ML-IRL is i.i.d. sampled
from a pre-collected data set. To solve this non i.i.d. issue of the input data, we propose a novel
theoretical technique that has three steps (detailed in Subsection 4.2). Step 1: We propose the
stationary distribution µπEand quantify the gradient norm difference between the real distribution
PπE
tand this stationary distribution µπEin Proposition 1. Step 2: We quantify the local regret over
the stationary distribution in Lemma 2. The benefit of doing this is that the input data can be regarded
as i.i.d. sampled from this stationary distribution. Step 3: We combine step 1 and step 2, and quantify
the (local) regret over the real distribution, where the data is not i.i.d., in Theorem 1 and Theorem
2. We can see that step 1 and step 3 are to solve the non i.i.d. issue of the input data, so that the
corresponding theorem statements (Proposition 1, Theorem 1, and Theorem 2) are novel compared to
ML-IRL because ML-IRL does not have this non i.i.d. issue. The only theorem statement relevant to
ML-IRL is Lemma 2 in step 2 where we both analyze the algorithm over a stationary distribution,
and the data is i.i.d. sampled from the stationary distribution.
However, Lemma 2 in step 2 still has significant distinctions from ML-IRL because Lemma 2
quantifies the local regret in the context of online optimization, while ML-IRL quantifies convergence
in the context of offline optimization. First, the objective function is dynamically changing in the
online setting because the learner observes a new state-action pair at each online iteration, while the
objective function is fixed in ML-IRL. Second, the local regret contains the term Lt(θt; (sE
t, aE
t)),
however, θtis computed before the learner knows (sE
t, aE
t). This makes it more difficult to quantify
the local regret because the learner does not know Ltwhen it computes θt. These two difficulties do
not appear in the offline optimization in ML-IRL. To solve these two issues, we need to additionally
construct a new time-invariant function ¯Lin Appendix B.4 and quantify the convergence of the new
function ¯L. Then, in order to quantify the local regret of {Lt}t≥0, we need to quantify the difference
between the real loss function {Lt}t≥0and the constructed loss function ¯L.
Moreover, our theoretical analysis holds for continuous state-action space while the theoretical
analysis in ML-IRL is limited to finite state-action space. The extension to continuous state-action
space brings new difficulties and requires significant novel analysis. In general, the difficulties stem
from two aspects: (1) The constants in ML-IRL, e.g., the smoothness constant of the loss function L
and the coefficient of convergence rate, include the term |S| × |A| . When the state-action space is
continuous, those constants are not finite because |S| × |A| is now infinite. To address this issue, we
propose new methods to bound those constants. For example, in order to show that the loss function
Lis smooth, rather than using ||∇L(θ1)− ∇L(θ2)||to find the smoothness constant as in ML-IRL,
we aim to show that ||∇2L(θ)||is upper bounded by a constant CLin Lemma A.2 and this constant
CLdoes not rely on |S| × | A|. Given that ||∇2L(θ)|| ≤CL, the loss function LisCL-smooth. (2)
Since the action space Ais finite in ML-IRL, their proved properties of the Q-function Qsoft(e.g.,
Lipschitz continuity, contraction property, monotonic improvement, and smoothness) can be easily
extended to the value function Vsoftby summing over different actions a∈ A. When the action
space becomes continuous, summing over infinitely many different actions does not preserve those
properties. Thus we have to propose new methods to prove those propoerties of the value function
Vsoft. In specific, we prove the Lipschitz continuity, contraction property, monotonic improvement,
and smoothness of the value function Vsoftin Claims 3-5 in Appendix B.4.
B Proof
This section provides the proof of all the proposition, lemmas, and theorems in the paper. To start
with, we first introduce the expression of soft Q-function and soft Bellman policy.
B.1 Notions
The soft Q-function and soft value function are:
Qsoft
θ,π(s, a)≜rθ(s, a) +γZ
s′∈SP(s′|s, a)Vsoft
θ,π(s′)ds′,
16Vsoft
θ,π(s)≜Eπ
S,A∞X
t=0γt 
rθ(St, At)−logπ(At|St)S0=sE
0
.
The soft Bellman policy is as follows:
πθ(a|s) =exp(Qsoft
θ(s, a))
exp(Vsoft
θ(s)),
Qsoft
θ(s, a) =rθ(s, a) +γZ
s′∈SP(s′|s, a)Vsoft
θ(s′)ds′,
Vsoft
θ(s) = logZ
a∈Aexp(Qsoft
θ(s, a))da
.
It has been proved [ 34] that the soft Bellman policy πθis the optimal solution of the lower-level
problem (4). We define Jθ(s)≜Eπθ
S,A[P∞
t=0γtrθ(St, At)|S0=s]as the expected cumulative
reward of policy πθstarting from state sandJθ(s, a)≜Eπθ
S,A[P∞
t=0γtrθ(St, At)|S0=s, A0=a].
Lemma 3. We have the gradient ∇θlogπθ(a|s) =Eπθ
S,A[P∞
t=0γt∇θrθ(St, At)|S0=s, A0=
a]−Eπθ
S,A[P∞
t=0γt∇θrθ(St, At)|S0=s].
Proof. Define Zθ(s, a)≜exp(Qsoft
θ(s, a))andZθ(s)≜exp(Vsoft
θ(s)), therefore Zθis smooth in θ
given that it is a composition of logarithmic, exponential, and linear functions of rθandrθis smooth
inθ(Assumption 1).
∇θlogZθ(s) =R
a∈A∇θZθ(s, a)da
Zθ(s),
=Z
a∈AZθ(s, a)
Zθ(s)∇θlogZθ(s, a)da,
=Z
a∈Aπθ(a|s)
∇θrθ(s, a) +γZ
s′∈SP(s′|s, a)∇θlogZθ(s′)ds′
da,
=Z
a∈Aπθ(a|s)
∇θrθ(s, a) +γZ
s′∈SP(s′|s, a)Z
a′∈A
∇θrθ(s′, a′)
+γZ
s′′∈SP(s′′|s′, a′)∇θlogZθ(s′′)ds′′
da′ds′
da.
Keep the expansion, we can get ∇θlogZθ(s) =Eπθ
S,A[P∞
t=0γt∇θrθ(St, At)|S0=s]and similarly
we can get ∇θlogZθ(s, a) =Eπθ
S,A[P∞
t=0γt∇θrθ(St, At)|S0=s, A0=a]. Thus we have the
gradient ∇θlogπθ(a|s) =∇θlogZθ(s, a)− ∇ θlogZθ(s).
B.2 Proof of Lemma 1
Recall thatPt
i=0Li(θ; (sE
i, aE
i)) =−Pt
i=0γilogπθ(aE
i|sE
i). When the dynamics Pis determin-
istic, we have that
∇tX
i=0Li(θ; (sE
i, aE
i)) =−tX
i=0γi∇θlogπθ(aE
i|sE
i),
=−tX
i=0γi
∇θQsoft
θ(sE
i, aE
i)− ∇ θVsoft
θ(sE
i)
,
=−tX
i=0γi
∇θrθ(sE
i, aE
i) +γ∇θVsoft
θ(sE
i+1)− ∇ θVsoft
θ(sE
i)
,
=−tX
i=0γi∇θrθ(sE
i, aE
i)−t+1X
i=1γi∇θVsoft
θ(sE
i) +tX
i=0γi∇θVsoft
θ(sE
i),
17=−tX
i=0γi∇θrθ(sE
i, aE
i)−γt+1∇θVsoft
θ(sE
t+1) +∇θVsoft
θ(sE
0),
(a)=−tX
i=0γi∇θrθ(sE
i, aE
i)−Eπθ
S,A∞X
i=t+1γi∇θrθ(Si, Ai)|Si=sE
t+1
+Eπθ
S,A∞X
i=0γi∇θrθ(Si, Ai)|S0=sE
0
,
=−tX
i=0γi∇θrθ(sE
i, aE
i)−Eπθ
S,A∞X
i=t+1γi∇θrθ(Si, Ai)|St=sE
t, At=aE
t
+Eπθ
S,A∞X
i=0γi∇θrθ(Si, Ai)|S0=sE
0
,
where equality (a)follows from the proof of Lemma 3.
When the dynamics Pis stochastic, we can prove that the above gradient is an unbiased estimate of
∇E{(SE
i,AE
i)∼PπE
i(·,·)(·,·)}i≥0Pt
i=0Li(θ; (SE
i, AE
i))
:
∇E{(SE
i,AE
i)∼PπE
i(·,·)(·,·)}i≥0tX
i=0Li(θ; (SE
i, AE
i))
,
=∇E{(SE
i,AE
i)∼PπE
i(·,·)(·,·)}i≥0
−tX
i=0γi∇θlogπθ(AE
i|SE
i)
,
=E{(SE
i,AE
i)∼PπE
i(·,·)(·,·)}i≥0
−tX
i=0γi[∇θrθ(SE
i, AE
i) +γESi+1∼P(·|SE
i,AE
i)[∇θVsoft
θ(Si+1)]
− ∇ θVsoft
θ(SE
i)]
,
=E{(SE
i,AE
i)∼PπE
i(·,·)(·,·)}i≥0
−tX
i=0γi[∇θrθ(SE
i, AE
i) +γ∇θVsoft
θ(SE
i+1)− ∇ θVsoft
θ(SE
i)]
,
=E{(SE
i,AE
i)∼PπE
i(·,·)(·,·)}i≥0
−tX
i=0γi∇θrθ(SE
i, AE
i)−γt+1∇θVsoft
θ(SE
t+1)
+Eπθ
S,A∞X
i=0γi∇θrθ(Si, Ai)|S0=SE
0
,
(b)=E{(SE
i,AE
i)∼PπE
i(·,·)(·,·)}i≥0
−tX
i=0γi∇θrθ(SE
i, AE
i)
−Eπθ
S,A[∞X
i=t+1γi∇θrθ(Si, Ai)|St=SE
t, At=AE
t] +Eπθ
S,A[∞X
i=0γi∇θrθ(Si, Ai)|S0=SE
0]
,
where equality (b)follows from the fact that
E{(SE
i,AE
i)∼PπE
i(·,·)(·,·)}i≥0
Eπθ
S,A[∞X
i=t+1γi∇θrθ(Si, Ai)|St=SE
t, At=AE
t]
,
=E{(SE
i,AE
i)∼PπE
i(·,·)(·,·)}i≥0
Eπθ
S,A[∞X
i=t+1γi∇θrθ(Si, Ai)|St+1=SE
t+1]
,
because PπE
t+1(·) =PπE
t(SE
t, AE
t)P(·|SE
t, AE
t)andSE
t+1∼P(·|SE
t, AE
t).
Since we quantify the local regret in expectation in Theorem 1, this unbiased estimate can be used
when the dynamics is stochastic.
18B.3 Proof of Proposition 1
From Assumption 2, we know that dTV(PπE
t(·), µπE(·)) =1
2R
s∈S|PπE
t(s)−µπE(s)|ds≤CMρt
where the initial state is s0. For any state-action pair (s, a)∈ S × A , we know that PπE
t(s, a) =
PπE
t(s)πE(a|s)andµπE(s, a) =µπE(s)πE(a|s). Therefore, we have that:
dTV(PπE(·,·), µπE(·,·)),
=1
2Z
s∈SZ
a∈A|PπE
t(s)πE(a|s)−µπE(s)πE(a|s)|dsda,
=1
2Z
s∈SZ
a∈A|PπE
t(s)−µπE(s)|πE(a|s)dsda,
=1
2Z
s∈S|PπE
t(s)−µπE(s)|ds,
≤CMρt.
Claim 1. The trajectory of θtis bounded, i.e., ||θt−¯θ|| ≤2¯Cr
λfor any t≥0.
Proof.
||θt+1−¯θ||=||θt−αtgt−¯θ||,
=θt−¯θ−αt∞X
i=0γi∇θrθt(s′
i, a′
i)−∞X
i=0γi∇θrθt(s′′
i, a′′
i) +λ(1−γt+1)
1−γ(θt−¯θ),
=(1−αtλ(1−γt+1)
1−γ)(θt−¯θ)−αt∞X
i=0γi∇θrθt(s′
i, a′
i)−∞X
i=0γi∇θrθt(s′′
i, a′′
i),
(a)
≤(1−αtλ(1−γt+1)
1−γ)||θt−¯θ||+αt∞X
i=0γi∇θrθt(s′
i, a′
i)−∞X
i=0γi∇θrθt(s′′
i, a′′
i),
(b)
≤(1−αtλ(1−γt+1)
1−γ)||θt−¯θ||+2αt¯Cr
1−γ,
≤(1−αtλ
1−γ)||θt−¯θ||+2αt¯Cr
1−γ,
where (a)follows triangle inequality and (b)uses the upper bound of ∇θrθin Assumption 1.
Therefore, we have the following relation:
||θt+1−¯θ|| −2¯Cr
λ≤(1−αtλ
1−γ)
||θt−¯θ|| −2¯Cr
λ
,
⇒ ||θt−¯θ|| ≤(1−αtλ
1−γ)t
||θ0−¯θ|| −2¯Cr
λ
+2¯Cr
λ,
(c)=2¯Cr
λ
1−(1−αtλ
1−γ)t(d)
≤2¯Cr
λ,
where (c)follows the fact that θ0=¯θand(d)follows the fact that αt≤1−γ
λ.
Recall that the loss function Li(θ; (sE
i, aE
i)) = −γilogπθ(aE
i|sE
i) +λγi
2||θ−¯θ||2and thus
∇Li(θ; (sE
i, aE
i)) = −γi[∇θQsoft
θ(sE
i, aE
i)− ∇ θVsoft
θ(sE
i)] + λγi(θ−¯θ). From Lemma 3,
we know that ∇θVsoft
θ(sE
i) = Eπθ
S,A[P∞
t=0γt∇θrθ(St, At)|S0=sE
i]and∇θQsoft
θ(sE
i, aE
i) =
Eπθ
S,A[P∞
t=0γt∇θrθ(St, At)|S0=sE
i, A0=aE
i]. Then, ||∇θVsoft
θ(sE
i)|| ≤¯Cr
1−γand
||∇θQsoft
θ(sE
i, aE
i)|| ≤¯Cr
1−γ.
Now we can see that
||∇Li(θt; ; (sE
i, aE
i))||=γi||∇θQsoft
θt(sE
i, aE
i)− ∇ θVsoft
θt(sE
i) +λ(θt−¯θ)||,
19≤γi||∇θQsoft
θt(sE
i, aE
i)− ∇ θVsoft
θt(sE
i) +λ(θt−¯θ)|| ≤γi2¯Cr
1−γ+ 2¯Cr
,
⇒ ||∇ Li(θt; (sE
i, aE
i))||2≤4¯C2
rγ2i2−γ
1−γ2
.
Therefore, we have that
E(SE
i,AE
i)∼PπE
i(·,·)
||∇Li(θt; (SE
i, AE
i))||2
−E(SE
i,AE
i)∼µπE(·,·)
||∇Li(θt; (SE
i, AE
i))||2,
=Z
s∈SZ
a∈APπE
i(s, a)||∇Li(θt; (s, a))||2dads
−Z
s∈SZ
a∈AµπE(s, a)||∇Li(θt; (s, a))||2dads,
≤Z
s∈SZ
a∈A|PπE
i(s, a)−µπE(s, a)| · ||∇ Li(θt; (s, a))||2dads,
≤2CMρi·4¯C2
rγ2i2−γ
1−γ2
= 8CM¯C2
r2−γ
1−γ2
ρiγ2i.
Lemma 4. Suppose Assumptions 1-2 hold, the we have the following for any (s, a)∈ S ×A and any
θ1, θ2, t:||∇Lt(θ1,(s, a))−∇Lt(θ2,(s, a))|| ≤CL||θ1−θ2||and|Qsoft
θ1,πθ1(s, a)−Qsoft
θ2,πθ2(s, a)| ≤
CQ||θ1−θ2||, where CL=2˜Cr
1−γ+4¯C3
r
(1−γ)4+λandCQ=¯Cr
1−γ.
Proof. Note that Qsoft
θ,πθ=Qsoft
θand∇θQsoft
θ(s, a) =Eπθ
S,A[P∞
t=0γt∇θrθ(St, At)|S0=s, A0=a]
(proof of Lemma 3). Therefore, we have that
||∇θQsoft
θ(s, a)|| ≤¯Cr
1−γ≜CQ
We know from Lemma 1 that ∇Lt(θ; (sE
t, aE
t)) =−γt[∇θQsoft
θ(sE
t, aE
t)−∇θVsoft
θ(sE
t)]+λγt(θ−¯θ).
To find the smoothness constant of Lt, we need to compute the Hessian of Lt. First, we have that
∇2
θθQsoft
θ(s, a) =∇θEπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=s, A0=a],
=∇2
θθrθ(s, a) +γZ
s′∈SP(s′|s, a)∇θEπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=s′]ds′,
=∇2
θθrθ(s, a)
+γZ
s′∈SP(s′|s, a)∇θZ
a′∈Aπθ(a′|s′)Eπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=s′, A0=a′]da′ds′,
=∇2
θθrθ(s, a) +γZ
s′∈SP(s′|s, a)Z
a′∈A
∇θπθ(a′|s′)·Eπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=s′,
A0=a′] +πθ(a′|s′)· ∇θEπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=s′, A0=a′]
da′ds′.
Keep the expansion, we can get
∇2
θθQsoft
θ(s, a) =Eπθ
S,A[∞X
t=0γt∇2
θθrθ(St, At)|S0=s0, A0=a0]
+Eπθ
S,A∞X
i=0γi∇θπθ(Ai|Si)·Eπθ
S′,A′[∞X
t=0γt∇θrθ(S′
t, A′
t)|S′
0=Si, A′
0=Ai]S0=s0, A0=a0
.
20Now we take a look at the second term in the above equality:
∇θπθ(a|s)·Eπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=s, A0=a],
=πθ(a|s)∇θlogπθ(a|s)·Eπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=s, A0=a],
=πθ(a|s)
∇θQsoft
θ(s, a)− ∇ θVsoft
θ(s)
·Eπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=s, A0=a],
⇒∇θπθ(a|s)·Eπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=s, A0=a],
≤(¯Cr
1−γ+¯Cr
1−γ)·¯Cr
1−γ=2¯C3
r
(1−γ)3.
Therefore, we have that
||∇2
θθQsoft
θ(s, a)|| ≤Eπθ
S,A[∞X
t=0γt∇2
θθrθ(St, At)|S0=s0, A0=a0]
+Eπθ
S′,A′∞X
i=0γi∇θπθ(A′
i|S′
i)·Eπθ
S,A[∞X
t=0γt∇θrθ(St, At)|S0=S′
i, A0=A′
i],
≤˜Cr
1−γ+∞X
t=0γt2¯C3
r
(1−γ)3=˜Cr
1−γ+2¯C3
r
(1−γ)4.
Similarly, we can get ||∇2
θθVsoft
θ(s)|| ≤˜Cr
1−γ+2¯C3
r
(1−γ)4. Therefore, we have that
||∇2Lt(θ; (sE
t, aE
t))|| ≤γt
||∇2
θθQsoft
θ(sE
t, aE
t)||+||∇2
θθVsoft
θ(sE
t)||+λ
,
≤γt2˜Cr
1−γ+4¯C3
r
(1−γ)4+λ
≤2˜Cr
1−γ+4¯C3
r
(1−γ)4+λ≜CL. (6)
B.4 Proof of Lemma 2
This proof is based on the proof in ML-IRL [ 5]. The differences are: (i) their proof only holds for
finite state-action space while we extend to continuous state-action space; (ii) their analysis is for
offline settings while we extend to online settings to quantify the local regret. We first introduce the
following claims which serve as building blocks in this subsection.
Claim 2. For any given policy πand state-action pair (s, a), it holds that |Qsoft
θ1,π(s, a)−
Qsoft
θ2,π(s, a)| ≤CQ||θ1−θ2||and|Vsoft
θ1,π(s)−Vsoft
θ2,π(s)| ≤CQ||θ1−θ2||.
Proof.
Qsoft
θ1,π(s, a)−Qsoft
θ2,π(s, a),
=Eπ
S,A∞X
t=0γt
rθ1(St, At)−logπ(At|St)S0=s, A0=a
−Eπ
S,A∞X
t=0γt
rθ2(St, At)−logπ(At|St)S0=s, A0=a
,
=Eπ
S,A∞X
t=0γt[rθ1(St, At)−rθ2(St, At)]S0=s, A0=a
,
21⇒ |Qθ1,π(s, a)−Qθ2,π(s, a)| ≤∞X
t=0γt¯Cr||θ1−θ2||=¯Cr
1−γ||θ1−θ2||=CQ||θ1−θ2||.
Similarly, we can get that
|Vsoft
θ1,π(s)−Vsoft
θ2,π(s)| ≤Eπ
S,A∞X
t=0γt[rθ1(St, At)−rθ2(St, At)]S0=s
,
≤∞X
t=0γt¯Cr||θ1−θ2||=¯Cr
1−γ||θ1−θ2||=CQ||θ1−θ2||.
Claim 3. The soft Bellman operator Tsoft
θ:
(Tsoft
θQ)(s, a)≜rθ(s, a) +γZ
s′∈SP(s′|s, a) logZ
a′∈Aexp(Q(s′, a′))da′
ds′,
(Tsoft
θV)(s)≜logZ
a∈Aexp
rθ(s, a) +γZ
s′∈SP(s′|s, a)V(s′)ds′
da
,
is a contraction map with constant γ.
Proof. It has been proved that Tsoft
θQis a contraction map with constant γ(Appendix A.2 in
[34]). Here we show that Tsoft
θVis a contraction map with constant γ. Define a norm of Vas
||V1−V2||= sups∈S|V1(s)−V2(s)|and suppose ||V1−V2||=ϵ. Then we have that
Tsoft
θV1(s) = logZ
a∈Aexp
rθ(s, a) +γZ
s′∈SP(s′|s, a)V1(s′)ds′
da
,
≤logZ
a∈Aexp
rθ(s, a) +γZ
s′∈SP(s′|s, a)[V2(s′) +ϵ]ds′
da
,
= logZ
a∈Aexp
rθ(s, a) +γZ
s′∈SP(s′|s, a)V2(s′)ds′+γϵ
da
,
= logZ
a∈Aexp(γϵ) exp
rθ(s, a) +γZ
s′∈SP(s′|s, a)V2(s′)ds′
da
,
=Tsoft
θV2(s) +γϵ.
Similarly, we can get Tsoft
θV1(s)≥ Tsoft
θV2(s)−γϵ. Therefore, ||Tsoft
θV1− Tsoft
θV2|| ≤ γϵ=
γ||V1−V2||.
Claim 4. It holds that Qsoft
θt,πt+1(s, a)≥ Tsoft
θt(Qsoft
θt,πt)(s, a)andVsoft
θt,πt+1(s)≥ Tsoft
θt(Vsoft
θt,πt)(s)for
any(s, a).
Proof.
Qsoft
θt,πt+1(s, a)(i)=rθt(s, a) +γZ
s′∈SP(s′|s, a)Ea′∼πt+1[Qsoft
θt,πt+1(s′, a′)−logπt+1(a′|s′)]ds′,
(ii)
≥rθt(s, a) +γZ
s′∈SP(s′|s, a)EA′∼πt+1(·|s′)[Qsoft
θt,πt(s′, A′)−logπt+1(A′|s′)]ds′,
=rθt(s, a) +γZ
s′∈SP(s′|s, a) logZ
a′∈Aexp(Qsoft
θt,πt(s′, a′))da′
ds′,
=Tsoft
θ(Qsoft
θt,πt)(s, a),
where (i)follow equations (2)-(3) in [ 35] and (ii)follows policy improvement theorem (Theorem 4
in [34]). Similarly, we can get that
Vsoft
θt,πt+1(s) =EA∼πt+1(·|s)[Qsoft
θt,πt+1(s, A)−logπt+1(A|s)],
22≥EA∼πt+1(·|s)[Qsoft
θt,πt(s, A)−logπt+1(A|s)],
= logZ
a∈Aexp(Qsoft
θt,πt(s, a))da
,
= logZ
a∈Aexp
rθt(s, a) +γZ
s′∈SP(s′|s, a)Vsoft
θt,πt(s′)ds′
da
,
=Tsoft
θt(Vsoft
θt,πt)(s).
Claim 5. The following holds for any (s, a)∈ S × A andθ1,θ2,t:
|Vsoft
θ1(s, a)−Vsoft
θ2(s, a)| ≤CQ||θ1−θ2||.
Proof. Note that ∇θVsoft
θ(s) =Eπθ
S,A[P∞
t=0γt∇θrθ(St, At)|S0=s](proof of Lemma 3), therefore
||∇θVsoft
θ(s)|| ≤¯Cr
1−γ=CQ.
We first show the convergence of πtin Algorithm 1. For any (s, a)∈ S × A , we know that
|logπt+1(a|s)−logπθt(a|s)| ≤ |Qsoft
θt,πt(s, a)−Qsoft
θt(s, a)|+|Vsoft
θt,πt(s)−Vsoft
θt(s)|.
Now we take a look at the term |Qsoft
θt,πt(s, a)−Qsoft
θt(s, a)|:
|Qsoft
θt,πt(s, a)−Qsoft
θt(s, a)|,
≤ |Qsoft
θt,πt(s, a)−Qsoft
θt−1,πt(s, a)|+|Qsoft
θt−1,πt(s, a)−Qsoft
θt−1(s, a)|+|Qsoft
θt−1(s, a)−Qsoft
θt(s, a)|,
(a)
≤CQ||θt−θt−1||+|Qsoft
θt−1,πt(s, a)−Qsoft
θt−1(s, a)|+CQ||θt−θt−1||,
= 2CQ||θt−θt−1||+|Qsoft
θt−1,πt(s, a)−Qsoft
θt−1(s, a)|,
(b)= 2CQ||θt−θt−1||+Qsoft
θt−1(s, a)−Qsoft
θt−1,πt(s, a),
(c)
≤2CQ||θt−θt−1||+Qsoft
θt−1(s, a)− Tsoft
θt−1(Qsoft
θt−1,πt−1)(s, a),
(d)= 2CQ||θt−θt−1||+Tsoft
θt−1(Qsoft
θt−1)(s, a)− Tsoft
θt−1(Qsoft
θt−1,πt−1)(s, a),
(e)
≤2CQ||θt−θt−1||+γ|Qsoft
θt−1(s, a)−Qsoft
θt−1,πt−1(s, a)|, (7)
where (a)follows Lemma 2 and Claim 2, (b)follows the fact that πθis the optimal solution, (c)
follows Claim 4, (d)follows the fact that Qsoft
θt−1is a fixed point of Tsoft
θt−1(Theorem 2 in [ 34]), and (e)
follows Claim 3.
Similarly we can bound the term |Vsoft
θt,πt(s)−Vsoft
θt(s)|:
|Vsoft
θt,πt(s)−Vsoft
θt(s)|,
≤ |Vsoft
θt,πt(s)−Vsoft
θt−1,πt(s)|+|Vsoft
θt−1,πt(s)−Vsoft
θt−1(s)|+|Vsoft
θt−1(s)−Vsoft
θt−1,πt−1(s)|,
(f)
≤CQ||θt−θt−1||+|Vsoft
θt−1,πt(s)−Vsoft
θt−1(s)|+CQ||θt−θt−1||,
(g)
≤2CQ||θt−θt−1||+Vsoft
θt−1(s)− Tsoft
θt−1(Vsoft
θt−1,πt)(s),
≤2CQ||θt−θt−1||+γ|Vsoft
θt−1(s)−Vsoft
θt−1,πt(s)|,
where (f)follows Claim 2 and claim 5 and (e)follows Claim 4.
Now we take a look at the term ||θt−θt−1||:
||θt−θt−1||=αt−1||gt−1||,
23≤αt−1∞X
t=0γt∇θrθt−1(s′
t, a′
t)+∞X
t=0γt∇θrθt−1(s′′
t, a′′
t)+λ(1−γt−1)
1−γθt−1−¯θ
,
(h)
≤αt−1¯Cr
1−γ+¯Cr
1−γ+λ(1−γt−1)
1−γ·2¯Cr
λ
,
≤4αt−1¯Cr
1−γ, (8)
where (h)follows claim 1. Recall that
|logπt+1(a|s)−logπθt(a|s)| ≤ |Qsoft
θt,πt(s, a)−Qsoft
θt(s, a)|+|Vsoft
θt,πt(s)−Vsoft
θt(s)|.
Summing from i= 0tot, we get
tX
i=1
|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)|+|Vsoft
θi,πi(s)−Vsoft
θi(s)|
,
≤tX
i=1
4CQ||θi−θi−1||+γ
|Qsoft
θi−1(s, a)−Qsoft
θi−1,πi−1(s, a)|+|Vsoft
θi−1,πi−1(s)−Vsoft
θi−1(s)|
,
⇒(1−γ)t−1X
i=0
|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)|+|Vsoft
θi,πi(s)−Vsoft
θi(s)|
,
(i)
≤16CQ¯Cr
1−γtX
i=1αi−1+
|Qsoft
θ0(s, a)−Qsoft
θ0,π0(s, a)|+|Vsoft
θ0,π0(s)−Vsoft
θ0(s)|
− |Qsoft
θt(s, a)−Qsoft
θt,πt(s, a)| − |Vsoft
θt,πt(s)−Vsoft
θt(s)|
,
⇒1
tt−1X
i=0
|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)|+|Vsoft
θi,πi(s)−Vsoft
θi(s)|
≤16CQ¯Cr
t(1−γ)2tX
i=1αi−1+1
t(1−γ)
|Qsoft
θ0(s, a)−Qsoft
θ0,π0(s, a)|+|Vsoft
θ0,π0(s)−Vsoft
θ0(s)|
− |Qsoft
θt(s, a)−Qsoft
θt,πt(s, a)| − |Vsoft
θt,πt(s)−Vsoft
θt(s)|
,
=¯D1√
t+¯D2
t,
where (i)follows (8),¯D1=16CQ¯Cr
(1−γ)2, and ¯D2=1
1−γ
|Qsoft
θ0(s, a)−Qsoft
θ0,π0(s, a)|+|Vsoft
θ0,π0(s)−
Vsoft
θ0(s)| − |Qsoft
θt(s, a)−Qsoft
θt,πt(s, a)| − |Vsoft
θt,πt(s)−Vsoft
θt(s)|
. Therefore, we can see that
1
tt−1X
i=0|logπi+1(a|s)−logπθi(a|s)| ≤¯D1√
t+¯D2
t. (9)
Define the loss function ¯L(θ)≜E(S,A)∼µπE(·,·)
−logπθ(A|S) +λ
2||θ−¯θ||2
, then we can see that
E(SE
i,AE
i)∼µπE(·,·)[Li(θ; (SE
i, AE
i))] = γi¯L(θ). Moreover, we have that
||∇Li(θt; (SE
i, AE
i))||(j)
≤γiλ||θt−¯θ||
+γi||Eπθt
S,A[∞X
k=0γk∇θrθt(Sk, Ak)|S0=SE
i]−Eπθt
S,A[∞X
k=0γk∇θrθt(Sk, Ak)|S0=SE
i, A0=AE
i]||,
(k)
≤2γi¯Cr+2γi¯Cr
1−γ, (10)
24where (j)follows Lemma 3 and (k)follows Claim 1.
Therefore, we have that
E(SE
i,AE
i)∼µπE(·,·)[||∇Li(θt; (SE
i, AE
i))
γi− ∇¯L(θt)||2](l′)
≤
2¯Cr+2¯Cr
1−γ2
,
⇒E(SE
i,AE
i)∼µπE(·,·)[||1
tt−1X
i=0Li(θt; (SE
i, AE
i))−γi¯L(θt)||2],
=1−γt
1−γ2
E(SE
i,AE
i)∼µπE(·,·)[||1
tt−1X
i=0Li(θt; (SE
i, AE
i))
γi−¯L(θt)||2],
≤1
t·4¯C2
r(2−γ)2
(1−γ)4, (11)
where (l′)follows the fact that a bounded variable X∈[−a, a]has bounded variance at most a2.
Now we take a look at the term gt−1−γt+1
1−γ∇¯L(θt):
E(S,A)∼µπE(·,·)
gt−1−γt+1
1−γ∇¯L(θt)
,
=E{(SE
i,AE
i)∼µπE(·,·)}i≥0
gt−tX
i=0∇Li(θt; (SE
i, AE
i))
+E{(SE
i,AE
i)∼µπE(·,·)}i≥0tX
i=0∇Li(θt; (SE
i, AE
i))−1−γt+1
1−γ∇¯L(θt)
,
=E{(SE
i,AE
i)∼µπE(·,·)}i≥0
gt−tX
i=0∇Li(θt; (SE
i, AE
i))
,
=E{(SE
i,AE
i)∼µπE(·,·)}i≥0∞X
i=0γi∇θrθt(s′
i, a′
i)−Eπθt
S,A[∞X
i=0γi∇θrθt(Si, Ai)|S0=SE
0]
+E{(SE
i,AE
i)∼µπE(·,·)}i≥0∞X
i=t+1γi∇θrθt(s′′
i, a′′
i)
−Eπθt
S,A[∞X
i=t+1γi∇θrθt(Si, Ai)|St=SE
t, At=AE
t]
,
=E{(SE
i,AE
i)∼µπE(·,·)}i≥0
Eπt+1
S,A[∞X
i=0γi∇θrθt(Si, Ai)|S0=SE
0]
−Eπθt
S,A[∞X
i=0γi∇θrθt(Si, Ai)|S0=SE
0] +Eπt+1
S,A[∞X
i=t+1γi∇θrθt(Si, Ai)|St=SE
t, At=AE
t]
−Eπθt
S,A[∞X
i=t+1γi∇θrθt(Si, Ai)|St=SE
t, At=AE
t]
.
From equation (64) in [5], we know that
Eπt+1
S,A[∞X
i=0γi∇θrθt(Si, Ai)|S0=s0]−Eπθt
S,A[∞X
i=0γi∇θrθt(Si, Ai)|S0=s0],
≤2¯Cr
1−γZ
s∈SZ
a∈A|Qsoft
θt(s, a)−Qsoft
θt,πt(s, a)|dads,
≤2¯CrCd
1−γsup
(s,a)∈S×A{|Qsoft
θt(s, a)−Qsoft
θt,πt(s, a)|},
25where Cdis the product of the area of Sand the area of A.
Therefore, we can get that
E(S,A)∼µπEgt−1−γt+1
1−γ∇¯L(θt)
≤4¯CrCd
1−γsup
(s,a)∈S×A{|Qsoft
θt(s, a)−Qsoft
θt,πt(s, a)|}.(12)
From (7) and (8), we know that
|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)| ≤γ|Qsoft
θi−1,πi−1(s, a)−Qsoft
θi−1(s, a)|+8αi−1CQ¯Cr
1−γ,
⇒αi|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)| ≤αi−1γ|Qsoft
θi−1,πi−1(s, a)−Qsoft
θi−1(s, a)|+8α2
i−1CQ¯Cr
1−γ,
⇒tX
i=1αi|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)| ≤t−1X
i=0αiγ|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)|+t−1X
i=08α2
iCQ¯Cr
1−γ,
⇒(1−γ)t−1X
i=0αi|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)|,
≤α0|Qsoft
θ0,π0(s, a)−Qsoft
θ0(s, a)| −αt|Qsoft
θt,πt(s, a)−Qsoft
θt(s, a)|+t−1X
i=08α2
iCQ¯Cr
1−γ, (13)
and similarly we can see that
tX
i=1|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)| ≤t−1X
i=0γ|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)|+8αiCQ¯Cr
1−γ,
⇒(1−γ)t−1X
i=1|Qsoft
θi,πi(s, a)−Qsoft
θi(s, a)|,
≤ |Qsoft
θ0,π0(s, a)−Qsoft
θ0(s, a)| − |Qsoft
θt,πt(s, a)−Qsoft
θt(s, a)|+t−1X
i=08αiCQ¯Cr
1−γ, (14)
Telescoping from i= 0tot−1, we get
t−1X
i=0αiE(S,A)∼µπEgi−1−γi+1
1−γ∇¯L(θi)
,
(l)
≤4¯CrCd
1−γt−1X
i=0αi|Qsoft
θi,πi(S, A)−Qsoft
θi(S, A)|,
(m)
≤4¯CrCd
(1−γ)2
α0|Qsoft
θ0,π0(S, A)−Qsoft
θ0(S, A)| −αt|Qsoft
θt,πt(S, A)−Qsoft
θt(S, A)|+t−1X
i=08α2
iCQ¯Cr
1−γ
,
(15)
where (l)follows (12) and (m)follows (13). Similarly, we can see that
t−1X
i=0E(S,A)∼µπEgi−1−γi+1
1−γ∇¯L(θi)
,
≤4¯CrCd
1−γt−1X
i=0|Qsoft
θi,πi(S, A)−Qsoft
θi(S, A)|,
≤4¯CrCd
(1−γ)2
|Qsoft
θ0,π0(S, A)−Qsoft
θ0(S, A)| − |Qsoft
θt,πt(S, A)−Qsoft
θt(S, A)|+t−1X
i=08αiCQ¯Cr
1−γ
,
(16)
26Now, we start to quantify the local regret:
¯L(θi+1)≥¯L(θi) + [∇¯L(θi)]⊤(θi+1−θi)−CL
2||θi+1−θi||2,
(n)
≥¯L(θi) +αt[∇¯L(θi)]⊤gi−8α2
i¯C2
rCL
(1−γ)2,
=¯L(θi) +αi1−γi+1
1−γ||∇¯L(θi)||2+αi[∇¯L(θi)]⊤(gi−1−γi+1
1−γ∇¯L(θi))−8α2
i¯C2
rCL
(1−γ)2,
≥¯L(θi) +αi1−γi+1
1−γ||∇¯L(θi)||2−αi||∇¯L(θi)|| · ||gi−1−γi+1
1−γ∇¯L(θi)|| −8α2
i¯C2
rCL
(1−γ)2,
(o)
≥¯L(θi) +αi1−γi+1
1−γ||∇¯L(θi)||2−αi2¯Cr(2−γ)
1−γ· ||gi−1−γi+1
1−γ∇¯L(θi)|| −8α2
i¯C2
rCL
(1−γ)2,
⇒E{(SE
i,AE
i)∼µπE(·,·)}i≥0t−1X
i=0αi1−γi+1
1−γ||∇¯L(θi)||2
,
≤¯L(θt)−¯L(θ0) +2¯Cr(2−γ)
1−γt−1X
i=0αiE{(SE
i,AE
i)∼µπE(·,·)}i≥0
||gi−1−γi+1
1−γ∇¯L(θi)||
+t−1X
i=08α2
i¯C2
rCL
(1−γ)2,
(p)
≤¯L(θt)−¯L(θ0) +8¯C2
rCd(2−γ)
(1−γ)3E{(SE
i,AE
i)∼µπE(·,·)}i≥0
α0|Qsoft
θ0,π0(SE
0, AE
0)−Qsoft
θ0(SE
0, AE
0)|
−αt|Qsoft
θt,πt(SE
t, AE
t)−Qsoft
θt(SE
t, AE
t)|
+32CQ¯C2
rCd
(1−γ)3+8¯C2
rCL
(1−γ)2t−1X
i=0α2
i,
where (n)follows (8), (o)follows (10), (p)follows (15). Therefore, we have that
E{(SE
i,AE
i)∼µπE(·,·)}i≥0T−1X
t=0αT−1||∇¯L(θt)||2
, (17)
≤E{(SE
i,AE
i)∼µπE(·,·)}i≥0T−1X
t=0αt1−γt
1−γ||∇¯L(θt)||2
,
≤¯L(θT)−¯L(θ0) +8¯C2
rCd(2−γ)
(1−γ)3E{(SE
i,AE
i)∼µπE(·,·)}i≥0
α0|Qsoft
θ0,π0(SE
0, AE
0)−Qsoft
θ0(SE
0, AE
0)|
−αT|Qsoft
θT,πT(SE
T, AE
T)−Qsoft
θT(SE
T, AE
T)|
+32CQ¯C2
rCd
(1−γ)3+8¯C2
rCL
(1−γ)2T−1X
i=0α2
i,
⇒E{(SE
i,AE
i)∼µπE(·,·)}i≥0T−1X
t=0||∇¯L(θt)||2
≤D2√
T+D3√
T(logT+ 1), (18)
where D2=¯L(θT)−¯L(θ0) +8¯C2
rCd(2−γ)
(1−γ)3E{(SE
i,AE
i)∼µπE(·,·)}i≥0
α0|Qsoft
θ0,π0(SE
0, AE
0)−
Qsoft
θ0(SE
0, AE
0)|−αT|Qsoft
θT,πT(SE
T, AE
T)−Qsoft
θT(SE
T, AE
T)|
andD3=2(1−γ)
λ
32CQ¯C2
rCd
(1−γ)3+8¯C2
rCL
(1−γ)2
.
Then we can see that
E{(SE
i,AE
i)∼µπE(·,·)}i≥0T−1X
t=0||1
t+ 1tX
i=0∇Li(θt; (SE
i, AE
i))||2
,
≤2E{(SE
i,AE
i)∼µπE(·,·)}i≥0T−1X
t=0||1
t+ 1tX
i=0∇Li(θt; (SE
i, AE
i))−γi∇¯L(θt)||2
27+ 2E{(SE
i,AE
i)∼µπE(·,·)}i≥0T−1X
t=0||1−γt
(t+ 1)(1 −γ)∇¯L(θt)||2
,
≤2E{(SE
i,AE
i)∼µπE(·,·)}i≥0T−1X
t=0||1
t+ 1tX
i=0∇Li(θt; (SE
i, AE
i))−γi∇¯L(θt)||2
+ 2E{(SE
i,AE
i)∼µπE(·,·)}i≥0T−1X
t=0||∇¯L(θt)||2
,
(q)
≤T−1X
t=08¯C2
r(2−γ)2
(t+ 1)(1 −γ)4+ 2E{(SE
i,AE
i)∼µπE(·,·)}i≥0T−1X
t=0||∇¯L(θt)||2
,
≤D1(logT+ 1) + D2√
T+D3√
T(logT+ 1),
where D1=8¯C2
r(2−γ)2
(1−γ)4. The (q)follows (11). Note that to achieve this local regret rate, we actually
need to take an extra expectation over the dynamics Pbecause we need to roll out πtto formulate gt.
Here, we omit the expectation over the dynamics.
B.5 Proof of Theorem 1
We know that
E{(SE
i,AE
i)∼µπE(·,·)}i≥0tX
i=0||∇Li(θt; (SE
i, AE
i))||2
,
≤E{(SE
i,AE
i)∼µπE(·,·)}i≥0tX
i=0||γi∇¯L(θt)||2
+E{(SE
i,AE
i)∼µπE(·,·)}i≥0tX
i=0||∇Li(θt; (SE
i, AE
i))−γi∇¯L(θt)||2
,
≤E{(SE
i,AE
i)∼µπE(·,·)}i≥0tX
i=0||∇¯L(θt)||2
+E{(SE
i,AE
i)∼µπE(·,·)}i≥0tX
i=0||∇Li(θt; (SE
i, AE
i))−γi∇¯L(θt)||2
,
(a)
≤O(√
t+ 1 +√
t+ 1 log( t+ 1)) +tX
i=0γ2i·4¯C2
r2−γ
1−γ2
,
⇒E{(SE
i,AE
i)∼µπE(·,·)}i≥01
t+ 1tX
i=0||∇Li(θt; (SE
i, AE
i))||2
≤O(1√t+ 1+log(t+ 1)√t+ 1+1
t+ 1),
(19)
where (a)follows (18) and (11).
Therefore,
T−1X
t=0E{(SE
i,AE
i)∼PπE
i(·,·)}i≥0
||1
t+ 1tX
i=0∇Li(θt; (SE
i, AE
i))||2
,
≤T−1X
t=0E{(SE
i,AE
i)∼PπE
i(·,·)}i≥01
t+ 1tX
i=0||∇Li(θt; (SE
i, AE
i))||2
,
≤T−1X
t=0E{(SE
i,AE
i)∼µπE(·,·)}i≥01
t+ 1tX
i=0||∇Li(θt; (SE
i, AE
i))||2
28+T−1X
t=01
t+ 1tX
i=0
E{(SE
i,AE
i)∼PπE
i(·,·)}i≥0[||∇Li(θt; (SE
i, AE
i))||2
−E{(SE
i,AE
i)∼µπE(·,·)}i≥0[||∇Li(θt; (SE
i, AE
i))||2]
,
(s)
≤D1logT+D2√
T+D3logT√
T+T−1X
t=01
t+ 1tX
i=08CM¯C2
r2−γ
1−γ2
ρiγ2i,
≤D1logT+D2√
T+D3logT√
T+T−1X
t=01
t+ 1·8CM¯C2
r2−γ
1−γ21
1−ργ2,
≤
D1+8CM¯C2
r(2−γ)2
(1−ργ2)(1−γ)2
logT+D2√
T+D3logT√
T,
where (s)follows (19) and Proposition 1. Note that to achieve this local regret rate, we actually need
to take an extra expectation over the dynamics Pbecause we need to roll out πtto formulate gt. Here,
we omit the expectation over the dynamics.
B.6 Proof of Theorem 2
Suppose the expert reward function rEand the parameterized reward function rθare both linear, i.e.,
rE=θ⊤
Eϕandrθ=θ⊤ϕ, where ϕ:S × A → Rn
+is an n-dimensional feature vector such that
||ϕ(s, a)|| ≤¯Crfor any (s, a)∈ S × A . The proof follows the similar idea with that of Theorem 1
where follow two-step process: step (i) quantifies the regret under the stationary distribution µπE(·,·)
and step (ii) quantifies the difference between the correlated distribution PπE
tand the stationary
distribution µπE. We start our proof with the following claim:
Claim 6. If the parameterized reward function rθis linear, the function ¯L(θ)isλ-strongly convex for
anyθ.
Proof. Recall that ¯L(θ) =E(¯S,¯A)∼µπE(·,·)[−logπθ(¯A|¯S)] +λ
2||θ−¯θ||2. Define ¯L(θ; (S, A))≜
−logπθ(¯A|¯S) +λ
2||θ−¯θ||2, from Lemma 3, we can see that
∇¯L(θ; (¯S,¯A)) =Eπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=¯S
−Eπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=¯S, A 0=¯A
+λ(θ−¯θ).
Therefore, we have that
∇2
θθ¯L(θ; (¯S,¯A)),
=∇θEπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=¯S
− ∇ θEπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=¯S, A 0=¯A
+λ.
Now, we take a look at
∇θEπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=¯S, A 0=¯A
=∇θ
ϕ(¯S,¯A)
+Z
s1∈SP(s1|¯S,¯A)Z
at+1∈Aπθ(a1|s1)Eπθ
S,A∞X
i=1γiϕ(Si, Ai)S1=s1, A1=a1
da1ds1
,
=Z
s1∈SP(s1|¯S,¯A)Z
a1∈A
∇θπθ(a1|s1)·Eπθ
S,A∞X
i=1γiϕ(Si, Ai)S1=s1, A1=a1
+πθ(a1|s1)· ∇θEπθ
S,A∞X
i=1γiϕ(Si, Ai)S1=s1, A1=a1
da1ds1.
Keep the expansion, we can see that
∇θEπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=¯S, A 0=¯A
,
29=Eπθ
S,A∞X
i=1∇θπθ(A′
i|S′
i)·Eπθ
S,A∞X
i=1γiϕ(Si, Ai)S1=S′
1, A1=A′
1S′
0=¯S, A′
0=¯A
,
∇θEπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=¯S
,
=Eπθ
S,A∞X
i=0∇θπθ(A′
i|S′
i)·Eπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=S′
0, A0=A′
0S′
0=¯S
,
Thus we have that
E(¯S,¯A)∼µπE(·,·)
∇θEπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=¯S
− ∇ θEπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=¯S, A 0=¯A
,
=E(¯S,¯A)∼µπE(·,·)Eπθ
S′,A′
∇θπθ(A′
0|S′
0)·Eπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=S′
0, A0=A′
0S′
0=¯S0
+E(¯S,¯A)∼µπE(·,·)
Eπθ
S′,A′∞X
i=1∇θπθ(A′
i|S′
i)·Eπθ
S,A∞X
i=1γiϕ(Si, Ai)S1=S′
1, A1=A′
1S′
0=¯S
−Eπθ
S,A∞X
i=1∇θπθ(A′
i|S′
i)·Eπθ
S,A∞X
i=1γiϕ(Si, Ai)S1=S′
1, A1=A′
1S0=¯S, A 0=¯A
,
=E(¯S,¯A)∼µπE(·,·)Eπθ
S′,A′
∇θπθ(A′
0|S′
0)·Eπθ
S,A∞X
i=0γiϕ(Si, Ai)S0=S′
0, A0=A′
0S′
0=¯S0
(a)=E(¯S,¯A)∼µπE(·,·)Eπθ
S′,A′
πθ(A′
0|SE
0)[X−EX]X|S0=¯S
,
=Cov(X)≻0, (20)
where (a)follows Lemma 3, X≜Eπθ
S,A[P∞
i=0γiϕ(Si, Ai)|S0=¯S, A 0=A]is a random vector,
EX≜Eπθ
S,A[P∞
i=0γiϕ(Si, Ai)|S0=¯S]is the expectation of Xover the action distribution, and
Cov(X)is the covariance matrix of the random vector Xwith itself, which is always positive definite
because the policy πθis always stochastic.
Therefore, we can see that
||∇2
θθ¯L(θ)||=||Cov(X) +λ||(b)
≥λ,
where (b)follows (20).
Step (i) . We first quantify the regret under the stationary distribution µπE(·,·). Suppose θ∗∈
arg min ¯L(θ) = arg min1−γT
1−γ¯L(θ) = arg min E(SE
t,AE
t)∼µπE[PT−1
t=0Lt(θ; (SE
t, AE
t))].
||θt+1−θ∗||2=||θt−αtgt−θ∗||2=||θt−θ∗||2+α2
t||gt||2−2αt⟨gt, θt−θ∗⟩,
⇒E[||θt+1−θ∗||2],
(c)
≤E[||θt−θ∗||2] +16α2
t¯C2
r
(1−γ)2−2αt⟨1−γt+1
1−γ∇¯L(θt), θt−θ∗⟩ −2αt⟨gt−1−γt+1
1−γ∇¯L(θt), θt−θ∗⟩,
(21)
where (c)follows (8)
Since ¯L(θ)isλ-strongly convex, we have that
¯L(θ∗)≥¯L(θt) +⟨∇¯L(θt), θ∗−θt⟩+λ
2||θt−θ∗||2,
⇒¯L(θt)−¯L(θ∗)≤ ⟨∇ ¯L(θt), θt−θ∗⟩ −λ
2||θt−θ∗||2,
30(d)
≤1−γ
2αt(1−γt+1)
||θt−θ∗||2− ||θt+1−θ∗||2
−1−γ
1−γt+1⟨gt−1−γt+1
1−γ∇¯L(θt), θt−θ∗⟩
+8αt¯C2
r
(1−γ)(1−γt+1)−λ
2||θt−θ∗||2,
⇒E(SE
t,AE
t)∼µπE(·,·)[¯L(θt)−¯L(θ∗)],
≤1−γ−λαt(1−γt+1)
2αt(1−γt+1)E(SE
t,AE
t)∼µπE(·,·)[||θt−θ∗||2]−1−γ
2αt(1−γt+1)E(SE
t,AE
t)∼µπE(·,·)[||θt+1−θ∗||2]
+8αt¯C2
r
(1−γ)(1−γt+1)+1−γ
1−γt+1E(SE
t,AE
t)∼µπE(·,·)
||gt−1−γt+1
1−γ∇¯L(θt)|| · ||θt−θ∗||
,
(e)
≤1−γ−λαt(1−γt+1)
2αt(1−γt+1)E(SE
t,AE
t)∼µπE(·,·)[||θt−θ∗||2]−1−γ
2αt(1−γt+1)E(SE
t,AE
t)∼µπE(·,·)[||θt+1−θ∗||2]
+8αt¯C2
r
(1−γ)(1−γt+1)+ˆC(1−γ)
1−γt+1E(SE
t,AE
t)∼µπE(·,·)
||gt−1−γt+1
1−γ∇¯L(θt)||
, (22)
where (d)follows (21), and (e)follows Claim 1 which shows that the trajectory of θtis bounded and
thus there is a positive constant ˆCsuch that ||θt−θ∗|| ≤ˆC.
Telescoping (22) from t= 0tot=T−1, we can see that
E(SE
t,AE
t)∼µπE(·,·)T−1X
t=0Lt(θt; (SE
t, AE
t))−T−1X
t=0Lt(θ∗; (SE
t, AE
t))
,
=E(SE
t,AE
t)∼µπE(·,·)T−1X
t=0γt¯L(θt)−T−1X
t=0γt¯L(θ∗)
,
≤E(SE
t,AE
t)∼µπE(·,·)T−1X
t=0¯L(θt)−T−1X
t=0¯L(θ∗)
,
(f)
≤1−γ−λα0
2α0E(SE
t,AE
t)∼µπE(·,·)[||θt−θ∗||2]−1−γ
2αT−1(1−γT)E(SE
t,AE
t)∼µπE(·,·)[||θT−θ∗||2]
+4¯CrCdˆC
(1−γ)2
|Qsoft
θ0,π0(S, A)−Qsoft
θ0(S, A)| − |Qsoft
θT,πT(S, A)−Qsoft
θT(S, A)|+t−1X
i=08αiCQ¯Cr
1−γ
,
≤¯D4+32CQCdˆC¯C2
r
λ(1−γ)3(logT+ 1), (23)
where (f)follows (16), and ¯D4=1−γ−λα0
2α0E(SE
t,AE
t)∼µπE(·,·)[||θt−θ∗||2]−
1−γ
2αT−1(1−γT)E(SE
t,AE
t)∼µπE(·,·)[||θT−θ∗||2] +4¯CrCdˆC
(1−γ)2[|Qsoft
θ0,π0(S, A)−Qsoft
θ0(S, A)| −
|Qsoft
θT,πT(S, A)−Qsoft
θT(S, A)|].
Step (ii) . We now quantify the difference between the stationary distribution µπE(·,·)and the
correlated distribution PπE
t(·,·). Note that ||θt||is bounded (proved in Claim 1), the soft Bellman
policy πθis continuous in θand(s, a), and we assume that the state-action space is bounded, there
is a positive constant Cπsuch that ||logπθt(a|s)|| ≤Cπfor any (s, a)∈ S × A . We start with the
following relation:
E(SE
t,AE
t)∼µπE(·,·)
Lt(θt; (SE
t, AE
t))
−E(SE
t,AE
t)∼PπE
t(·,·)
Lt(θt; (SE
t, AE
t)),
=γtZ
s∈SZ
a∈A|PπE
t(s, a)−µπE(s, a)| · ||logπθt(a|s)||dads,
≤CπCMγtρt,
⇒E(SE
t,AE
t)∼µπE(·,·)T−1X
t=0Lt(θt; (SE
t, AE
t))
−E(SE
t,AE
t)∼PπE
t(·,·)T−1X
t=0Lt(θt; (SE
t, AE
t)),
31≤CπCM
1−γρ. (24)
Therefore, we have that
E(SE
t,AE
t)∼PπE
tT−1X
t=0Lt(θt; (SE
t, AE
t))
−min
θE(SE
t,AE
t)∼PπE
tT−1X
t=0Lt(θ; (SE
t, AE
t))
,
≤E(SE
t,AE
t)∼PπE
tT−1X
t=0Lt(θt; (SE
t, AE
t))
−E(SE
t,AE
t)∼µπET−1X
t=0Lt(θt; (SE
t, AE
t))
+E(SE
t,AE
t)∼µπET−1X
t=0Lt(θt; (SE
t, AE
t))−T−1X
t=0Lt(θ∗; (SE
t, AE
t))
+E(SE
t,AE
t)∼µπET−1X
t=0Lt(θ∗; (SE
t, AE
t))
−min
θE(SE
t,AE
t)∼PπE
tT−1X
t=0Lt(θ; (SE
t, AE
t))
,
≤E(SE
t,AE
t)∼PπE
tT−1X
t=0Lt(θt; (SE
t, AE
t))
−E(SE
t,AE
t)∼µπET−1X
t=0Lt(θt; (SE
t, AE
t))
+E(SE
t,AE
t)∼µπET−1X
t=0Lt(θt; (SE
t, AE
t))−T−1X
t=0Lt(θ∗; (SE
t, AE
t))
+E(SE
t,AE
t)∼µπET−1X
t=0Lt(θ∗; (SE
t, AE
t))
−E(SE
t,AE
t)∼PπE
tT−1X
t=0Lt(θ∗; (SE
t, AE
t))
,
(g)
≤CπCM
1−γρ+¯D4+32CQCdˆC¯C2
r
λ(1−γ)3(logT+ 1) +CπCM
1−γρ,
=D4+D5(logT+ 1),
where (f)follows (23) and (24), D4=2CπCM
1−γρ+¯D4, and D5=32CQCdˆC¯C2
r
λ(1−γ)3.
C Meta-Regularization Algorithm and Convergence Guarantee
To solve problem (5), we use a double-loop method, i.e., we first solve the lower-level problem for K
iterations to get an approximate ϕj,Kofϕjand then use the approximate ϕj,Kto solve the upper-level
problem. We do not use a single-loop method to solve (5)because we need to get the task-specific
adaptation ϕjfor each task Tj. The single-loop method only partially solves the lower-level problem
by one-step gradient descent and thus the obtained parameter can be far away from ϕj.
Lemma 5. The gradient of the lower-level problem in (5)isEπϕ
S,AP∞
t=0γt∇ϕrϕ(St, At)S0=
str
0
−P∞
t=0γt∇ϕrϕ(str
t, atr
t) +λ
1−γ(ϕ−¯θ)where (str
t, atr
t)∈ Dtr
j.
We roll out the policy πϕfrom str
tto get a trajectory sϕ
0, aϕ
0,···where sϕ
0=str
tand approximate the
lower-level gradient by gϕ=P∞
t=0γt∇ϕrϕ(sϕ
t, aϕ
t)−P∞
t=0γt∇ϕrϕ(str
t, atr
t) +λ
1−γ(ϕ−¯θ). We
update ϕj,k+1=ϕj,k−βkgϕj,kforKtimes to get ϕj,Kwhere βkis the step size and ϕj,kis the
parameter at time k, and use ϕj,Kto calculate the approximate of the hyper-gradientd
d¯θL(ϕj,Deval
j).
Lemma 6. The hyper-gradient (i.e., gradient of the upper-level problem in (5)) isd
d¯θL(ϕj,Deval
j) =
I+1−γ
λ∇2
ϕϕL(ϕj,Dtr
j)−1∇ϕL(ϕj,Deval
j), where
∇ϕL(ϕj,Deval
j) =|Deval
j| ·Eπϕj
S,A∞X
t=0γt∇ϕrϕj(St, At)S0∼P0
−|Deval
j|X
v=1∞X
t=0γt∇ϕrϕj(sv
t, av
t),
∇2
ϕϕL(ϕj,Dtr
j) =Eπϕj
S,A∞X
t=0γt∇2
ϕϕrϕj(St, At)S0=str
0
−∞X
t=0γt∇2
ϕϕrϕj(str
t, atr
t) +e.
32The|Deval
j|is the number of trajectories in Deval
j,(sv
t, av
t)∈ Deval
j, and eis an extra term whose
expression can be found in Appendix C.2.
To approximate the hyper-gradient, we first roll out πϕj,Kto get two trajectories sϕj,K
0, aϕj,K
0,···and
¯sϕj,K
0,¯aϕj,K
0,···where sϕj,K
0 is drawn from P0and¯sϕj,K
0=str
0. We estimate ∇ϕL(ϕj,Deval
j)via
¯∇ϕL(ϕj,K,Deval
j) =|Deval
j| ·P∞
t=0γt∇ϕrϕj,K(sϕj,K
t, aϕj,K
t)−P|Deval
j|
v=1P∞
t=0γt∇ϕrϕj,K(sv
t, av
t)
and estimate the term ∇2
ϕϕL(ϕj,Dtr
j)via¯∇2
ϕϕL(ϕj,K,Dtr
j) =P∞
t=0γt∇2
ϕϕrϕj,K(¯sϕj,K
t,¯aϕj,K
t)−P∞
t=0γt∇2
ϕϕrϕj,K(str
t, atr
t). Therefore, we can approximate the hyper-gradient termd
d¯θL(ϕj,Deval
j)
viahj= [I+1−γ
λ¯∇2
ϕϕL(ϕj,K,Dtr
j)]−1¯∇ϕL(ϕj,K,Deval
j). We omit the extra term ein the approxi-
mate hjand its impact on the convergence can be bounded (proved in Appendix C.4).
To solve the upper-level problem in (5), at each iteration n, we sample a batch of Btasks, and
compute the task-specific adaptation ϕj,Kand the hyper-gradient hjfor each task. The update law to
solve the upper-level problem is: ¯θn+1=¯θn−τn
BPB
j=1hjwhere τnis the step size. Note that the
time index kis for the lower-level problem and nis for the upper-level problem.
Algorithm 2 Meta-regularization
Input : Initialized meta-prior ¯θ0and task-specific adaptation ϕj,0
Output : Learned prior ¯θN
1:forn= 0,1,···, N−1do
2: Sample a batch of Btasks{Tj}B
j=1∼PT
3: foreach task Tjdo
4: fork= 0,1,···, K−1do
5: Compute the soft Bellman policy πϕj,kvia soft Q-learning or soft actor-critic
6: Roll out the policy πϕj,kto get a trajectory sϕj,k
0, aϕj,k
0,···
7: Compute the gradient gϕj,k=P∞
t=0γt∇ϕrϕj,k(sϕj,k
t, aϕj,k
t)−P∞
t=0γt∇ϕrϕj,k(str
t, atr
t)
+λ
1−γ(ϕj,k−¯θn)
8: Update ϕj,k+1=ϕj,k−βkgϕj,k
9: end for
10: Compute the soft Bellman policy πϕj,Kvia soft Q-learning or soft actor-critic
11: Roll out the policy πϕj,Kto get two trajectories sϕj,K
0, aϕj,K
0,···and¯sϕj,K
0,¯aϕj,K
0,···
12: Compute the hyper-gradient hj= [I+1−γ
λ¯∇2
ϕϕL(ϕj,K,Dtr
j)]−1¯∇ϕL(ϕj,K,Deval
j)
13: end for
14: Update ¯θn+1=¯θn−τn
BPB
j=1hj
15:end for
Lemma 7 (Convergence of the lower-level problem) .Suppose Assumptions 1-2 hold and λ≥CL
2+η
where η∈(0,CL
2). Letβk=1−γ
η(k+1), then we have
E[||ϕj,K−ϕj||2]≤O(1
K).
Assumption 3. The parameterized reward function rθhas bounded third-order gradient, i.e.,
||∇3
θθθrθ(s, a)|| ≤ˆCrfor any (s, a)where ˆCris a positive constant.
Theorem 3 (Convergence of the upper-level problem) .Suppose Assumption 3 and the condition in
Lemma 7 hold. Let τn= (n+ 1)−1/2and define F(¯θ)asEj∼PT[L(ϕj,Deval
j)]under the meta-prior
¯θ. Then we have the following convergence:
1
NN−1X
n=0E[||∇F(¯θn)||2]≤O(1√
N+logN√
N+1
K) +C1,
and the expression of C1can be found in (27).
33C.1 Proof of Lemma 5
The proof is similar to that of Lemma 1. We first prove the case of deterministic dynamics and
the corresponding result can be an unbiased estimate in cases of stochastic dynamics (proved in
Subsection B.2).
∇ϕL(ϕ,Dtr
j) =−∞X
t=0γt∇ϕlogπϕ(atr
t|str
t),
=−∞X
t=0γt
∇ϕQsoft
ϕ(str
t, atr
t)− ∇ ϕVsoft
ϕ(str
t)
,
=−∞X
t=0γt
∇ϕrϕ(str
t, atr
t) +γ∇ϕVsoft
ϕ(str
t+1)− ∇ ϕVsoft
ϕ(str
t)
,
=∇ϕVsoft
ϕ(str
0)−∞X
t=0γt∇ϕrϕ(str
t, atr
t),
(a)=Eπϕ
S,A∞X
t=0γt∇ϕrϕ(St, At)S0=str
0
−∞X
t=0γt∇ϕrϕ(str
t, atr
t),
where (a)follows the proof of Lemma 3.
C.2 Proof of Lemma 6
Since the lower-level problem of (5) is unconstrained and ϕiis the optimal solution, we know that
∇ϕL(ϕj,Dtr
j) +λ
1−γ(ϕj−¯θ) = 0 (25)
We further take derivative of both sides in (25) with respect to ¯θand we get that

∇2
ϕϕL(ϕj,Dtr
j) +λ
1−γ
∇¯θϕj−λ
1−γ= 0⇒ ∇ ¯θϕj=
I+1−γ
λ∇2
ϕϕL(ϕj,Dtr
j)−1
.
Therefore, we have that
d
d¯θL(ϕj,Deval
j) = (∇¯θϕj)⊤∇ϕL(ϕj,Deval
j) =
I+1−γ
λ∇2
ϕϕL(ϕj,Dtr
j)−1
∇ϕL(ϕj,Deval
j).
Similar to Lemma 5, we can know that
∇ϕL(ϕj,Deval
j) =|Deval
j|X
v=1Eπϕj
S,A∞X
t=0γt∇ϕrϕj(St, At)S0=sv
0
−|Deval
j|X
v=1∞X
t=0γt∇ϕrϕj(sv
t, av
t),
(26)
where (sv
t, av
t)∈ζvandζv∈ Deval
j.
Since there is usually abundant data in Deval
jandS0∼P0, we can reformulate (26) as the following:
∇ϕL(ϕj,Deval
j)≈ |Deval
j| ·Eπϕj
S,A∞X
t=0γt∇ϕrϕj(St, At)S0∼P0
−|Deval
j|X
v=1∞X
t=0γt∇ϕrϕj(sv
t, av
t).
Claim 7. The second-order information ∇2
ϕϕQsoft
ϕ(s, a) = ∆( s, a) +Eπϕ
S,A[P∞
t=0γtCov(St)|S0=
s, A0=a]and∇2
ϕϕVsoft
ϕ(s) = ∆( s) +Eπϕ
S,A[P∞
t=0γtCov(St)|S0=s]where ∆(s, a) =
Eπϕ
S,A[P∞
t=0γt∇2
ϕϕrϕ(St, At)|S0=s, A0=a],∆(s) =Eπϕ
S,A[P∞
t=0γt∇2
ϕϕrϕ(St, At)|S0=s],
and Cov (s)≜R
a∈Aπϕ(a|s)[∆(s, a)−∆(s)]∆(s)dais the covariance matrix of ∆(s,·)at state s.
34The proof of Claim 7 follows the proof of Lemma 4
Now we take a look at the term ∇2
ϕϕL(ϕj,Dtr
j)and consider the case of deterministic dynamics:
∇2
ϕϕL(ϕj,Dtr
j) =−∞X
t=0γt∇2
ϕϕlogπϕj(atr
t|str
t),
=−∞X
t=0γt
∇2
ϕϕQsoft
ϕj(str
t, atr
t)− ∇2
ϕϕVsoft
ϕj(str
t)
,
=−∞X
t=0γt
∇2
ϕϕrϕj(str
t, atr
t) +γ∇2
ϕϕVsoft
ϕj(str
t+1)− ∇2
ϕϕVsoft
ϕj(str
t)
,
=∇2
ϕϕVsoft
ϕj(str
0)−∞X
t=0γt∇2
ϕϕrϕj(str
t, atr
t),
(a)=Eπϕj
S,A[∞X
t=0γt∇2
ϕϕrϕj(St, At)|S0=str
0]−∞X
t=0γt∇2
ϕϕrϕj(str
t, atr
t)
+Eπϕj
S,A[∞X
t=0γtCov(St)|S0=str
0],
where (a)follows Claim 7. As proved in Subsection B.2, we can still use this expression in the case
of stochastic dynamics. We define e≜Eπϕj
S,A[P∞
t=0γtCov(St)|S0=str
0]. However, eis intractable
to compute because it requires to compute COV(St)at every visited state. To approximate the
covariance matrix COV(St), we need to empirically roll out the policy πϕjfromStfor enough times
to get enough samples. We need to do these policy roll-outs at every state St. For example, suppose
we roll out the policy πϕjten times to get ten samples at each state St. Empirically, we need to do
these roll-outs 10×¯Twhere ¯Tis a very large integer that we regard as infinity because the trajectory
horizon is infinite. This is intractable because we need to roll out the policy for too many times.
Moreover, we cannot guarantee that we can approximate COV(St)well given that we only use ten
samples to approximate it.
C.3 Proof of Lemma 7
We know that ||∇2
ϕϕL(ϕ,Dtr
j)+λ
1−γ|| ≤ ||∇2
ϕϕL(ϕ,Dtr
j)||+λ
1−γ≤P∞
t=0
||∇2Lt(ϕ)||+λγt(a)
≤
1
1−γCLwhere (a)follows (6). Therefore, the lower-level objective function in (8) isCL
1−γ-smooth.
Moreover, since λ≥CL
2+η, then||∇2
ϕϕL(ϕ,Dtr
j)|| ≤CL−2η
2(1−γ). Therefore, the lower-level objective
function in (8) is2η
1−γ-strongly convex. Following the standard result for strongly-convex and smooth
stochastic optimization, we can reach the result in Lemma 7.
C.4 Proof of Theorem 3
In this proof, we first bound the hyper-gradient approximation error (i.e., ||d
d¯θL(ϕj,Deval
j)−hj||)
and then prove the convergence. Define ¯hj≜[I+1−γ
λ¯∇2
ϕϕL(ϕj,Dtr
j)]−1∇ϕL(ϕj,Deval
j)where
¯∇2
ϕϕL(ϕj,Dtr
j)≜Eπϕj
S,A[P∞
t=0γt∇2
ϕϕrϕj(St, At)|S0=str
0]−P∞
t=0γt∇2
ϕϕrϕj(str
t, atr
t). Therefore,
we have that
||¯hj−d
d¯θL(ϕj,Deval
j)||,
≤[I+1−γ
λ∇2
ϕϕL(ϕj,Dtr
j)]−1−[I+1−γ
λ¯∇2
ϕϕL(ϕj,Dtr
j)]−1· ||∇ ϕL(ϕj,Deval
j)||,
≤[I+1−γ
λ∇2
ϕϕL(ϕj,Dtr
j)]−1+[I+1−γ
λ¯∇2
ϕϕL(ϕj,Dtr
j)]−1
· ||∇ ϕL(ϕj,Deval
j)||,
(a)
≤2λ
2λ+ 2η−CL+λ
λ−2˜Cr
·2|Deval
j|¯Cr
1−γ≜C1>0, (27)
35where (a)follows the fact that ||I+1−γ
λ∇2
ϕϕL(ϕj,Dtr
j)|| ≥ 1− ||1−γ
λ∇2
ϕϕL(ϕj,Dtr
j)|| ≥ 1−
CL−2η
2λgiven that ||∇2
ϕϕL(ϕ,Dtr
j)|| ≤CL−2η
2(1−γ)(proved in the proof of Lemma 7). Therefore ||[I+
1−γ
λ∇2
ϕϕL(ϕj,Dtr
j)]−1|| ≤2λ
2λ+2η−CL. Similarly, we can bound ||[I+1−γ
λ¯∇2
ϕϕL(ϕj,Dtr
j)]−1|| ≤
λ
λ−2˜Crgiven that ||¯∇2
ϕϕL(ϕj,Dtr
j)|| ≤2˜Cr
1−γ. Note that λ >CL
2andCL>˜Cr(proved in (6)),
therefore C1is a positive constant.
Now, we bound the term ||hj−¯hj||. We define ∆ϕj=I+1−γ
λ¯∇2
ϕϕL(ϕj,Dtr
j)and∆ϕj=
I+1−γ
λ¯∇2
ϕϕL(ϕj,K,Dtr
j). Thus we have ||∆−1
ϕj|| ≤λ
λ−2˜Cr(follows (27)) and similarly ||∆−1
ϕj,K|| ≤
λ
λ−2˜Cr. Therefore,
E[||hj−¯hj||] =E∆−1
ϕj,K∇ϕL(ϕj,K,Deval
j)−∆−1
ϕj∇ϕL(ϕj,Deval
j)
,
≤E∆−1
ϕj,K∇ϕL(ϕj,K,Deval
j)−∆−1
ϕj,K∇ϕL(ϕj,Deval
j)
+∆−1
ϕj,K∇ϕL(ϕj,Deval
j)−∆−1
ϕj∇ϕL(ϕj,Deval
j)
,
≤E
||∆−1
ϕj,K|| · ||∇ ϕL(ϕj,K,Deval
j)− ∇ ϕL(ϕj,Deval
j)||+||∆−1
ϕj,K−∆−1
ϕj|| · ||∇ ϕL(ϕj,Deval
j)||
,
(b)
≤λ
λ−2˜Cr· |Deval
j| ·CL−2η
2(1−γ)||ϕj,K−ϕj||+E[||∆−1
ϕj,K−∆−1
ϕj||]· |Deval
j| ·2¯Cr
1−γ,
≤O(1
K) +E[||∆−1
ϕj,K|| · ||∆−1
ϕj|| · ||∆−1
ϕj,K−∆−1
ϕj||]· |Deval
j| ·2¯Cr
1−γ,
≤O(1
K) +¯CLE[||∆−1
ϕj,K|| · ||∆−1
ϕj||]· |Deval
j| ·2¯Cr
1−γ||ϕj,K−ϕj|| ≤O(1
K), (28)
where ||∇3
ϕϕϕL(ϕj,Dtr
j)|| ≤ ˜CL. The expression of ˜CLcan be derived following the proof
of Lemma 4 by notifying Assumption 3. The (b)holds because (i) ||∇2
ϕϕL(ϕ,Deval
j)||=
|Deval
j|
|Dtr
j|||∇2
ϕϕL(ϕ,Dtr
j)|| ≤ |Deval
j| ·Cl−2η
2(1−γ)and (ii) ||∇ϕL(ϕj,Deval
j)|| ≤ |Deval
j| ·2¯Cr
1−γ(see the ex-
pression of ∇ϕL(ϕj,Deval
j)in Lemma 5).
E[||hj−d
d¯θL(ϕj,Deval
j)||]≤E[||hj−¯hj||+||¯hj−d
d¯θL(ϕj,Deval
j)||](c)
≤C1+O(1
K),(29)
where (c)follows (27)-(28). Define F(¯θ)asEj∼PT[L(ϕj,Deval
j)]under the meta-prior ¯θ. Note that
||hj|| ≤ || ∆−1
ϕj,K|| · ||∇ ϕL(ϕj,K,Deval
j)|| ≤λ
λ−2˜Cr· |Deval
j| ·2¯Cr
1−γ. Therefore,
F(¯θn+1)≥F(¯θn) + (∇F(¯θn))⊤(¯θn+1−¯θn)−|Deval|(CL−2η)
4(1−γ)||¯θn+1−¯θn||2,
≥F(¯θn) +τn
BBX
j=1(∇F(¯θn))⊤hj−|Deval|(CL−2η)τ2
n
4(1−γ)||1
BBX
j=1hj||2,
≥F(¯θn) +τn
BBX
j=1(∇F(¯θn))⊤hj−|Deval|(CL−2η)τ2
n
4(1−γ)·λ
λ−2˜Cr· |Deval
j| ·2¯Cr
1−γ,
≥F(¯θn) +τn
BBX
j=1(∇F(¯θn))⊤hj−2λ¯Cr|Deval|2(CL−2η)τ2
n
4(1−γ)2(λ−2˜Cr),
⇒E[F(¯θn+1)]≥E[F(¯θn)] +τnE[||∇F(¯θn)||2] +τn
BBX
j=1E[hj−d
d¯θL(ϕj,Deval
j)]
+τn
BBX
j=1E[d
d¯θL(ϕj,Deval
j)− ∇F(¯θn)]−2λ¯Cr|Deval|2(CL−2η)τ2
n
4(1−γ)2(λ−2˜Cr),
36(d)
≥E[F(¯θn)] +τnE[||∇F(¯θn)||2] +τn(C1+O(1
K))−2λ¯Cr|Deval|2(CL−2η)τ2
n
4(1−γ)2(λ−2˜Cr),
⇒1
NN−1X
n=0τNE[||∇F(¯θn)||2]≤1
NN−1X
n=0τnE[||∇F(¯θn)||2],
≤1
N[F(¯θN)−F(¯θ0)] +1
NN−1X
n=0τn(C1+O(1
K)) +1
NN−1X
n=02λ¯Cr|Deval|2(CL−2η)τ2
n
4(1−γ)2(λ−2˜Cr),
⇒1
NN−1X
n=0E[||∇F(¯θn)||2]≤O(1√
N+logN√
N+1
K) +C1,
where |Deval|≜supi∼PT{|Deval
j|}and(d)follows (29).
D Experiment details
The code was running on a laptop whose processor is AMD Ryzen 7 4700U with Radeon Graphics,
2.00GHz, and the installed RAM is 20.0GB. The operating system is Ubuntu 18.04. We use a neural
network to parameterize the learned reward function. The neural network has two hidden layers
where each hidden layer has 64 neurons. The activation functions are respectively ReLU and Tanh.
D.1 Baselines
Here we provide the update rule for each baseline. Given a learned reward function rθ, the policy
updates of the four baselines are the same with that of MERIT-IRL, i.e., one-step policy iteration.
The difference is the reward update.
IT-IRL : IT-IRL is MERIT-IRL without the meta-regularization term. Therefore, the reward update
of IT-IRL is θt+1=θt−αtg′
twhere g′
t=P∞
i=0γi∇θrθt(s′
i, a′
i)−P∞
i=0γi∇θrθt(s′′
i, a′′
i). Recall
from Subsection 4.1 that {(s′
i, a′
i)}i≥0is generated by the learned policy πθtstarting from the initial
states′
0=sE
0, and{(s′′
i, a′′
i)}i≥0is generated by the learned policy πθtstarting from (s′′
t, a′′
t)where
(s′′
i, a′′
i) = (sE
i, aE
i)for0≤i≤t.
Naive MERIT-IRL : This method has the meta-regularization term, however, it uses the naive way
(depicted in the middle of Figure 1) to update the reward function. In specific, it only compares
the partial expert trajectory {sE
i, aE
i}t
i=0and partial learner trajectory {s′
i, a′
i}t
i=0. Therefore, the
reward update of Naive MERIT-IRL is θt+1=θt−αtg′′
twhere g′′
t=Pt
i=0γi∇θrθt(s′
i, a′
i)−Pt
i=0γi∇θrθt(sE
i, aE
i) +λ(1−γt+1)
1−γ(θ−¯θ).
Naive IT-IRL : This method does not have the meta-regularization term and uses the naive way to
update the reward function. Therefore, the reward update of Naive IT-IRL is θt+1=θt−αtg′′′
t
where g′′′
t=Pt
i=0γi∇θrθt(s′
i, a′
i)−Pt
i=0γi∇θrθt(sE
i, aE
i).
Hindsight : This method is a standard IRL method with the meta-regularization term where the
complete expert trajectory {sE
i, aE
i}i≥0and the complete learner trajectory {s′
i, a′
i}i≥0are compared
to update the reward function. Therefore, the reward update of Hindsight is θt+1=θt−αtg′′′′
twhere
g′′′′
t=P∞
i=0γi∇θrθt(s′
i, a′
i)−P∞
i=0γi∇θrθt(sE
i, aE
i) +λ(1−γt+1)
1−γ(θ−¯θ).
D.2 MuJoCo
D.2.1 Walker
Figure 2b shows that MERIT can achieve similar performance with the expert after t= 600 while
the other three in-trajectory learning baselines fail to imitate the expert before the ongoing trajectory
terminates. Note that the naive methods (i.e., Naive MERIT-IRL and Naive IT-IRL) have much
smaller improvement from t= 0compared to MERIT-IRL and IT-IRL. The reason is that the naive
reward update method is flawed. Intuitively, the reward update mechanism of these two baselines are
myopic as explained in Subsection 4.1. Theoretically, the gradients g′′
tof Naive MERIT-IRL and g′′′
t
37of Naive IT-IRL are biased estimate of (4)even if πtapproaches πθtsince (4)includes the trajectory
suffix ( i > t ) terms while g′′
tandg′′′
tonly include the trajectory prefix ( i≤t) terms.
MERIT-IRL performs much better than IT-IRL. The reason is that the meta-regularization term
restricts the learned reward parameter within a certain neighborhood of the meta-prior ¯θ(proved in
Appendix B.3). Given that ¯θis trained over a family of relevant tasks, it is expected that the actual
reward function parameter of our task shall be “close" to ¯θ[25,44,57], i.e., inside this neighborhood.
Therefore, MERIT-IRL can efficiently learn the expert’s reward function. On the contrary, IT-IRL
does not have the meta-prior ¯θas a guidance and thus has to search over the whole parameter space,
which is extremely difficult to learn the expert’s reward function when the data is lacking. Note that
MERIT-IRL and Naive MERIT-IRL have better initial performance than IT-IRL and Naive IT-IRL
since MERIT-IRL and Naive MERIT-IRL starts at the meta-prior ¯θwhile IT-IRL and Naive IT-IRL
initializes randomly.
D.2.2 Hopper
Figure 2c shows that MERIT can achieve similar performance with the expert after t= 500 while
the other three in-trajectory learning baselines fail to imitate the expert before the ongoing trajectory
terminates.
D.3 Stock Market
We use the real-world data of 30 constitute stocks in Dow Jones Industrial Average from 2021-01-01
to 2022-01-01. The 30 stocks are respectively: ‘AXP’, ‘AMGN’, ‘AAPL’, ‘BA’, ‘CAT’, ‘CSCO’,
‘CVX’, ‘GS’, ‘HD’, ‘HON’, ‘IBM’, ‘INTC’, ‘JNJ’, ‘KO’, ‘JPM’, ‘MCD’, ‘MMM’, ‘MRK’, ‘MSFT’,
‘NKE’, ‘PG’, ‘TRV’, ‘UNH’, ‘CRM’, ‘VZ’, ‘V’, ‘WBA’, ‘WMT’, ‘DIS’, ‘DOW’.
The state of the stock market MDP is the perception of the stock market, including the open/close
price of each stock, the current asset, and some technical indices [ 48]. The action has the same
dimension as the number of stocks where each dimension represents the amount of buying/selling the
corresponding stock. The detailed formulation of the MDP can be found in FinRL [48, 58].
The turbulence index is a technical index of stock market and is included as a dimension of the state
[48,58]. The function p2is defined as the amount of buying the stocks whose turbulence index is
larger than the turbulence threshold. Therefore, the more the target investor buys the stocks whose
turbulence index is larger than the turbulence threshold, the larger p2will be and thus the smaller
reward the target investor will receive.
Discussion on the experiment results . In Figure 2d, MERIT-IRL can achieve the similar cumulative
reward with the expert when only the first 60% of the trajectory is observed while IT-IRL can achieve
performance close to the expert after t= 220 . This shows that the meta-regularization can help
imitate the expert faster. In contrast, Naive MERIT-IRL and Naive IT-IRL barely improves because
the naive reward update method is flawed. Intuitively, the reward update mechanism of these two
baselines are myopic as explained in Subsection 4.1. Theoretically, the gradients g′′
tof Naive MERIT-
IRL and g′′′
tof Naive IT-IRL are biased estimate of (4)even if πtapproaches πθtsince (4)includes
the trajectory suffix ( i > t ) terms while g′′
tandg′′′
tonly include the trajectory prefix ( i≤t) terms.
The last row in Table 1 shows the final results of the algorithms. We can see that MERIT-IRL
achieves much better performance than the other in-trajectory learning baselines (i.e., IT-IRL, Naive
MERIT-IRL, and Naive IT-IRL). MERIT-IRL achieves comparable performance with Hindsight and
the expert. Note that it is not expected that MERIT-IRL outperforms Hindsight since Hindsight has
the complete expert trajectory to learn.
E Potential negative societal impact
Since MERIT-IRL can infer the reward function of the expert, potential negative societal impact
may occur when the learner is malicious. Take the stock market experiment as an example, private
information like preferences or habits of the investors may be leaked by using MERIT-IRL. To avoid
this situation, the investors needs to take additional strategies such as protecting its investment data
from unsecure resources.
38F Limitations
From the objective (2), we can see that the goal of MERIT-IRL is to align with the expert demon-
stration, i.e., finding a reward function such that its corresponding policy makes the expert trajectory
most likely. An ideal case is that we can also directly quantify the reward learning performance and
study the reward identifiability issue. Thus, a future work is to study the reward identifiability issue
in the context of in-trajectory IRL.
39NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract summarizes our contributions, and the introduction has a “con-
tribution statement" part which elaborates our contributions and mentions the consistency
with theoretical and experiment results. The assumptions and limitations are included in
“theoretical analysis" (Subsection 4.2) and Appendix F.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitation is mentioned in Appendix F. Under the assumptions, i.e.,
Assumptions 1 and 2, we have a justification of either the assumption is widely used in
literature or the assumption can be satisfied by real scenarios.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
40Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We clearly state our assumptions in Assumptions 1 and 2. The theoretical state-
ments are also clearly stated in Subsection 4.2 and the correct proof is included in Appendix
B. All the assumptions and theoretical statements are numbered and cross-referenced.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: This paper provides pseudocode and elaborates each step of the algorithm in
Subsection 4.1 and Appendix 4.3. We also include the experiment details in Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
41some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code is submitted in the supplementary materials and we include a
document in the code folder to describe how to run the code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Appendix D includes the experiment details, and we also submit the code.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: In the experiment results (i.e., Table 1, Table ??, Table ??), we include both
the mean and standard deviation. In Figure 2, we also plot the mean and standard deviation.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
42•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We include the type of operating system, CPU, and RAM used in the first
paragraph in Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: This paper follows NeurIPS Code of Ethnics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer:[Yes]
Justification: We discuss the potential negative societal impact in Appendix E.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
43•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper poses no such risks because we do not have data nor model to
release.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We use the SAC code from a Github repository, and we clearly state it at the
top of that code script.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
44•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The submitted code can be considered as an asset, and we provide a file along
with the code to document the code.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
45•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
46