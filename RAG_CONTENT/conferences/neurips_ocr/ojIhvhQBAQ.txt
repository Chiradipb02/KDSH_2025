Efficient Discrepancy Testing for Learning
with Distribution Shift
Gautam Chandrasekaran∗
UT AustinAdam R. Klivans†
UT AustinVasilis Kontonis‡
UT Austin
Konstantinos Stavropoulos§
UT AustinArsen Vasilyan¶
UC Berkeley
Abstract
A fundamental notion of distance between train and test distributions from the field
of domain adaptation is discrepancy distance. While in general hard to compute,
here we provide the first set of provably efficient algorithms for testing localized
discrepancy distance, where discrepancy is computed with respect to a fixed output
classifier. These results imply a broad set of new, efficient learning algorithms in
the recently introduced model of Testable Learning with Distribution Shift (TDS
learning) due to Klivans et al. (2023).
Our approach generalizes and improves all prior work on TDS learning: (1)
we obtain universal learners that succeed simultaneously for large classes of
test distributions, (2) achieve near-optimal error rates, and (3) give exponential
improvements for constant depth circuits. Our methods further extend to semi-
parametric settings and imply the first positive results for low-dimensional convex
sets. Additionally, we separate learning and testing phases and obtain algorithms
that run in fully polynomial time at test time.
1 Introduction
Distribution shift remains a central challenge in machine learning. While practitioners may exert
some level of control over a model’s training distribution, they have far less insight into future,
potentially adversarial, test distributions. Developing algorithms that can predict whether a trained
classifier will perform well on an unseen test set is therefore critical to the widescale deployment of
modern foundation models.
A heavily-studied framework for modeling distribution shift is domain adaptation, where a learner
has access to labeled examples from some training distribution, unlabeled examples from some
test distribution and is asked to output a hypothesis with low error on the test distribution. Over
∗gautamc@cs.utexas.edu . Supported by the NSF AI Institute for Foundations of Machine Learning
(IFML).
†klivans@cs.utexas.edu . Supported by NSF award AF-1909204 and the NSF AI Institute for Founda-
tions of Machine Learning (IFML).
‡vasilis@cs.utexas.edu . Supported by the NSF AI Institute for Foundations of Machine Learning
(IFML).
§kstavrop@cs.utexas.edu . Supported by the NSF AI Institute for Foundations of Machine Learning
(IFML) and by scholarships from Bodossaki Foundation and Leventis Foundation.
¶arsenvasilyan@gmail.com . Supported in part by NSF awards CCF-2006664, DMS-2022448, CCF-
1565235, CCF-1955217, CCF-2310818, Big George Fellowship and Fintech@CSAIL. Work done in part while
visiting UT Austin. Part of this work was conducted while the author was visiting the Simons Institute for the
Theory of Computing.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).the last twenty years, researchers in domain adaptation and related fields [ BDBCP06 ,BCK+07,
MMR09 ,BDBC+10,RMH+20,ZLWJ20 ,KM21b ,HKM23 ,KZZ24 ] have established bounds for
out-of-distribution generalization in terms of some type of distance between train and test distributions.
By far the most commonly studied notion is discrepancy distance:
discC(D,D′) = sup
f1,f2∈CP
x∼D[f1(x)̸=f2(x)]−P
x∼D′[f1(x)̸=f2(x)]
Estimating or even testing discrepancy distance, however, seems difficult, as its definition involves
an enumeration over all classifiers from some underlying function class (in Appendix F we give the
first hardness result for computing discrepancy distance in general). As such, obtaining provably
efficient algorithms for domain adaptation has seen little progress (none of the above works give
polynomial-time guarantees).
In search of efficient algorithms for learning with distribution shift with certifiable error guarantees,
recent work by [ KSV24b ] defined the Testable Learning with Distribution Shift (TDS learning)
framework. In this model (similar to domain adaptation), a learner receives labeled examples
from train distribution D,unlabeled examples from test distribution D′, and then runs a test. If
the (efficiently computable) test accepts, the learner outputs hthat is guaranteed to have low test
error with respect to D′. No guarantees are given if the test rejects, but it must accept (with high
probability) if the marginals of DandD′are equal. This framework has led to the first provably
efficient algorithms for learning with distribution shift for certain concept classes (for example,
halfspaces) [KSV24b, KSV24a].
It is straightforward to see that if algorithm Alearns concept class Cin the (ordinary) PAC/agnostic
model, and we have an efficient localized discrepancy tester for C, thenCis learnable in the TDS
framework: simply apply the discrepancy tester to the output of Aand accept if this quantity is small.
A dream scenario would be to augment all known PAC/agnostic learning algorithms with associated
localized discrepancy testers. This is nontrivial in part because we cannot make any assumptions on
the test distribution D′(our test has to always accept or reject correctly). Nevertheless, our main
contribution is a suite of new discrepancy testers for well-studied function class/training distribution
pairs that unifies and greatly expands all prior work on TDS learning.
1.1 Our Contributions
Optimal Error Guarantees via L1Sandwiching. The work of [ KSV24b ] followed a moment-
matching approach to show that the existence of L2sandwiching polynomial approximators implies
TDS learning up to a constant factor of the optimum error. Although their result implies TDS learning
for several fundamental concept classes, the L2sandwiching requirement seems restrictive for classes
such as constant-depth circuits or polynomial threshold functions. In Theorem 3.1, we provide TDS
learning results in terms of the much more well-understood notion of L1sandwiching, resolving one
of the main questions left open in [ KSV24b ]. As such, we obtain exponential improvements for TDS
learning constant depth circuits (AC0), and the first results for degree-2 polynomial threshold functions
(see Table 1). Our result also bridges a gap between TDS learning and testable agnostic learning
[RV23 ], since the latter has been known to be implied by L1sandwiching [ GKK23 ]. Additionally,
in the agnostic setting, the error guarantees we achieve are essentially optimal (as opposed to the
constant-factor approximation by [KSV24b]).
Universal TDS Learners. A natural and important goal in TDS learning is to design algorithms that
accept and make trustworthy predictions whenever the distribution shift is benign. In Theorems 4.2
and 5.1, we give the first TDS learners that are guaranteed to accept whenever the test marginal falls in
a wide class of distributions that are not necessarily close to the training distribution (in say statistical
distance) but, instead, share some mild structural properties. In the literature of testable agnostic
learning, testers with relaxed completeness criteria are called universal [ GKSV23 ]. Our universal
TDS learners accept all distributions that are sufficiently concentrated and anti-concentrated and work
for convex sets with low intrinsic dimension (Theorem 4.2) and halfspace intersections (Theorem 5.1).
Surprisingly, our algorithms can handle distributions that are heavy-tailed and multimodal, for which
efficient (ordinary) agnostic learning algorithms are not known to exist. Our algorithms exploit
localization guarantees from the training phase (e.g., subspace or boundary recovery) to relax the
requirements of the testing phase.
2Fully Polynomial-Time Testing. All of the TDS learners we provide consist of two decoupled
phases. In the training phase, the algorithm uses labeled training examples to output a candidate
hypothesis h. The testing phase receives the candidate hand uses unlabeled test examples to decide
whether to reject or accept and output h. Separation of the two phases is an important feature of our
approach, as it may be desirable for these tasks to be performed by distinct parties who have different
amounts of available (computing) resources. Efficient implementations of the testing phase are of
utmost importance, especially for potential users of large pre-trained models who need to certify
that the candidate model at hand is safe to deploy. In Theorem 5.1, we give the first TDS learner
for intersections of halfspaces that runs in fully polynomial test time , and additionally improves
the overall runtime of the previous state-of-the-art TDS learner for intersection of halfspaces by
[KSV24a ]. In fact, our TDS learner’s overall runtime is polynomial in the dimension d, while the
time complexity of the TDS learner given by [ KSV24a ] involved a factor of dO(log(1 /ϵ)), where ϵis
the error parameter.
1.2 Our Techniques
Our approach for designing TDS learners focuses on efficient algorithms for testing a new notion of
localized discrepancy distance:
Definition 1.1 (Localized Discrepancy) .LetDbe a distribution over X ⊆Rdand let H,C ⊆ {± 1}X
be hypothesis and concept classes respectively. Define neighborhood Nto be a function N:H → 2C.
Forbf∈ H, the(bf,N)-localized discrepancy from DtoD′is defined as:
discbf,N(D,D′) = sup
f∈N(bf)
P
x∼D′[bf(x)̸=f(x)]−P
x∼D[bf(x)̸=f(x)]
Testing localized discrepancy is clearly easier than testing the traditional (global) discrepancy distance,
since global discrepancy is defined with respect to a supremum over all pairs of concepts within some
given class, while localized discrepancy only depends on a small neighborhood of concepts around
some given reference classifier bf.
Assume for a moment that we have fixed a neighborhood function Nand have obtained a learner that
always outputs a classifier close to the ground truth function f∗(i.e.,f∗∈N(bf)). In this case, if we
can test localized discrepancy, then we obtain a TDS learner as follows: output bfif the corresponding
localized discrepancy is small and reject otherwise (recall bfis close to the ground truth for both
training and test distributions).
The algorithmic challenge is finding a definition of neighborhood that admits both an efficient learner
(for outputting a classifier close to the ground truth) and an efficient localized discrepancy tester.
Smaller neighborhoods make the learning problem more difficult while larger neighborhoods make
discrepancy testing more challenging.
Ultimately, the appropriate localized discrepancy relaxation of the testing phase depends on the
guarantees one can ensure during training, which, in turn, depends on the properties of the concept
classCand the training distribution. For our main applications below we briefly describe the choice
of neighborhood and the corresponding discrepancy tester. Note that we give a different discrepancy
tester for each of the following cases.
Classes with Low-Degree Sandwiching Approximators. We show that the existence of degree- ℓ
L1-sandwiching approximators for a class CoverX ⊆Rdturns out to be sufficient to design a
localized discrepancy tester that runs in time dO(ℓ)where the notion of neighborhood is widest
possible, i.e., N(bf) =C.6In this case, the requirement for the training algorithm is minimal, as the
ground truth f∗lies within C, which coincides with N(bf). The proposed tester is based on estimating
the chow parameters of the reference hypothesis bfunder the test marginal and checking whether they
closely match the chow parameters of bfunder the training marginal. For more details, see Section 3.
Convex Sets with Low Intrinsic Dimension. For convex sets with few relevant dimensions, there
are algorithms from standard PAC learning that guarantee approximate recovery of the relevant
subspace. This guarantee allows one to choose a much stronger notion of neighborhood while
6The discrepancy is still localized, since it is defined with respect to a reference hypothesis bf.
3still ensuring that f∗∈N(bf). The appropriate notion of neighborhood contains low-dimensional
concepts whose relevant subspace is geometrically close to the subspace of the reference hypothesis.
The corresponding tester exhaustively checks that the marginal D′is well-behaved on the relevant
subspace. For more details, see Section 4.
Intersections of Halfspaces. For intersections of halfspaces, we prove a structural result stating that
finding a hypothesis with low Gaussian disagreement with the ground truth f∗implies approximate
pointwise recovery of the boundary of f∗. It is therefore sufficient to check whether the marginal
of the test distribution assigns unreasonably large mass near the boundary of the training output
hypothesis bf, which can be done in fully polynomial time. Any proper algorithm for learning
halfspace intersections under Gaussian training marginals is then sufficient for our purposes. For
more details, see Section 5.
1.3 Related Work
Domain Adaptation. In the past two decades, there has been a long line of research on generalization
bounds for domain adaptation. The work of [ MMR09 ] introduced the notion of discrepancy distance,
following work by [ BDBCP06 ,BDBC+10], which used similar notions of distance between distri-
butions. Other important notions of distribution similarity include bounded density ratios [ SSK12 ]
and related notions [ KM21b ,KZZ24 ]. A type of localized discrepancy distance was defined by
[ZLWJ20 ] and used to provide improved sample complexity bounds for domain adaptation. None of
the above works give efficient (polynomial-time) algorithms. Here, we give a more general notion of
localization and use it to obtain efficient and universal algorithms for TDS learning.
TDS Learning and Related Models. The framework of TDS learning was defined by [ KSV24b ],
where it was shown that any class that admits degree- ℓL2-sandwiching approximators can be TDS
learned in time dO(ℓ)up to error O(λ), where λis the standard (and necessary) benchmark for the
error in domain adaptation when the training and test distributions are allowed to be arbitrary. Here,
we show that the relaxed notion of L1-sandwiching approximators suffices for TDS learning and we
improve the error guarantee to nearly-match the information-theoretically optimal λ(see Section 3).
For intersections of halfspaces under Gaussian training marginals, [ KSV24a ] gave TDS learners
with improved guarantees compared to those given by [ KSV24b ] through L2sandwiching. Our
TDS learners for halfspace intersections are superior to the ones from [KSV24a] in terms of overall
runtime, universality and test-time efficiency (see Section 5).
Another related framework for learning with distribution shift is PQ learning , which was defined by
[GKKM20 ]. In PQ learning, the learner may reject regions of the domain where it is not confident to
make predictions, but the total mass of these regions under the training distribution must be small.
In fact, PQ learning is known to imply TDS learning (see [ KSV24b ]). However, the only known
algorithms for PQ learning, which were given by [ GKKM20 ,KK21 ], require access to oracles for
learning primitives that are known to be hard even for simple classes (see [KK21]).
The framework of TDS learning is also related to testable agnostic learning, where the goal of the
tester is to certify a near-optimal error guarantee. Testable agnostic learning was defined by [ RV23 ]
and there are several subsequent works in this framework [ GKK23 ,GKSV24 ,GKSV23 ,DKK+23].
There are many important differences between TDS learning and testable agnostic learning, including
the fact that, in testable agnostic learning, there is no distribution shift and that in TDS learning, the
learner does not have access to labels from the distribution on which it is evaluated. In particular,
testable agnostic learning is only defined in the presence of noise in the labels, while TDS learning is
meaningful even when the labels are generated noise-free (i.e., realizable learning).
PAC Learning. In the standard framework of PAC learning, there is an abundance of algorithmic
ideas and techniques that aim to achieve efficient learning, under various assumptions (see e.g.,
[LW94 ,BK97 ,KOS04 ,KLT09 ,KOS08a ,Vem10b ,Vem10a ,GKM12 ,KKM13 ,DKS18a ,DTK22 ]).
In this work, we make use of polynomial regression [ KKMS08 ], dimension reduction techniques
[Vem10a ], as well as techniques for robustly learning geometric concepts [ DKS18b ], in order to
obtain efficient TDS learners. In fact, our approach of designing TDS learning algorithms through
localized discrepancy testing sheds a light on what kinds of guarantees from the training algorithms are
desirable for learning in the presence of distribution shift. For example, we show that if approximate
subspace recovery is guaranteed after training, then the discrepancy testing problem can be relaxed to
4an easier, localized version. Moreover, our results on TDS learning halfspace intersections emphasize
the importance of proper learners in the context of learning with distribution shift.
2 Preliminaries
We use standard big-O notation (and ˜Oto hide poly-logarithmic factors), Rdis the d-dimensional
euclidean space and Ndthe standard Gaussian over Rd,{±1}dis the d-dimensional hypercube and
Unif({±1}d)the uniform distribution over {±1}d,Nis the set of natural numbers N={1,2, . . .}
andx∈Rddenotes a vector with x= (x1, . . . ,xd)and inner products x·v. See also Appendix A.
Localized Discrepancy Testing. Testing localized discrepancy (Definition 1.1) is defined as follows.
Definition 2.1 (Testing Localized Discrepancy) .For a set Dof distributions and DoverXandϵ >0,
we say that Tis a(N, ϵ)-tester for localized discrepancy from Dwith respect to D, if,T, upon
receiving bf∈ H and a set XofmTi.i.d. examples from some distribution D′overXsatisfies:
(a) (Soundness.) With probability at least 3/4: IfTaccepts, then discbf,N(D,D′)≤ϵ .
(b) (Completeness.) If D′∈D, thenTaccepts with probability at least 3/4.
For a concept class C, a distribution DoverX,ϵ∈(0,1), we say that Chasϵ-L1sandwiching degree
ℓwith respect to Dif for any f∈ C, there exist polynomials pup, pdown overXwith degree at most ℓ
such that (1) pdown(x)≤f(x)≤pup(x)for all x∈ X and (2) Ex∼D[pup(x)−pdown(x)]≤ϵ.
Learning Setting. ForX ⊆Rd, the learner is given labeled samples from a training distribution
Dtrain
XY overX ×{± 1}withX-marginal Dtrain
X=Dand unlabeled examples from the marginal Dtest
X
of a test distribution Dtest
XYoverX × {± 1}. For a concept class C ⊆ {X → {± 1}}, in the realizable
setting , there is f∗∈ Cthat generates the labels for both Dtrain
XY andDtest
XY. In the agnostic setting ,
the standard goal in domain adaptation is to achieve an error guarantee that is competitive with the
information-theoretically optimal joint error λ= min f∈C(err(f;Dtrain
XY) + err( f;Dtest
XY)), achieved
by some f∗∈ C, where err(f;Dtrain
XY) =P(x,y)∼Dtrain
XY[y̸=f(x)](and similarly for err(f;Dtest
XY)).
Definition 2.2 (Universal TDS Learning) .LetCbe a concept class over X ⊆Rd,Da distribution
overXandDsome class of distributions over X. The algorithm Ais said to D-universally TDS
learnCwith respect to Dup to error ψand probability of failure δif, upon receiving mtrain labeled
samples from a training distribution Dtrain
XY withX-marginal Dandmtestunlabeled samples from
a test distribution Dtest
XY, w.p. at least 1−δ, algorithm Aeither rejects, or accepts and outputs a
hypothesis h:X → {± 1}such that:
(a) (Soundness.) If Aaccepts, then the output hsatisfies err(h;Dtest
XY)≤ψ.
(b) (Completeness.) If Dtest
X∈DthenAaccepts.
In the agnostic setting, parameter ψmay depend on λ=λ(C;Dtrain
XY,Dtest
XY), whereas in the realizable
setting, ψ=ϵ∈(0,1). IfD={D}, then we simply say that Aψ-TDS learns Cw.r.t.D.
Note that the success probability for TDS learning can be amplified through repetition [ KSV24b ] and
we will consider δ= 0.1unless specified otherwise.
3 Classes with Low Sandwiching Degree
Prior work on TDS learning by [ KSV24b ] showed that the existence of degree- ℓL2-sandwiching
approximators implies TDS learning in time dO(ℓ). A major question left open was whether the more
traditional notion of L1sandwiching (see Definition C.1) suffices for TDS learning. We answer this
question in the affirmative, and as a consequence we obtain exponential improvements in the runtime
of TDS learning for constant depth circuits (AC0) and the first TDS learning results for degree- 2
polynomial threshold functions (see Table 1). For more details, see Appendix C.
Theorem 3.1 (L1-sandwiching implies TDS learning) .Letϵ, δ∈(0,1)and let C ⊆ {X → {± 1}}
be a concept class such that the ϵ-approximate L1-sandwiching degree of Cunder Disℓ(ϵ)∈N.
5Then, there exists a TDS learning algorithm for Cwith respect to Dup to error λ+opttrain+O(ϵ)
and fails with probability at most δwith time and sample complexity poly( dℓ(ϵ),1
ϵ) log(1 /δ).
Note that prior work [ KSV24b ] had only obtained a bound of O(λ)in the above error guarantee. Our
techniques allow us to achieve the optimal dependence of simply λ.
Concept class Training Marginal Time Prior Work
1 Degree-2 PTFsNdor
Unif({±1}d)deO(1/ϵ9)None
2 Circuits of size s, depth t Unif({±1}d) dO(log(s/ϵ))O(t)d√s·O(log(s/ϵ))O(t)
only for formulas
Table 1: New results for TDS learning through L1sandwiching. For constant-depth formulas, we
achieve an exponential improvement compared to [ KSV24b ] (which used L2-sandwiching), and our
results work for circuits as well.
For Gaussian and uniform halfspaces, intersections and functions of halfspaces, as well as for
decision trees over the uniform distribution, the L2-sandwiching approach of [ KSV24b ] provided
TDS learning algorithms with similar runtime as the one obtained here, but their error guarantee was
O(λ) +ϵinstead of λ+opttrain+ϵ(where opttrain= min f∈Cerr(f;Dtrain
XY)), which is the best
known upper bound on the error, even information theoretically (see [BDBC+10, DLLP10]).
Localized discrepancy testing via Chow matching. The improvements we obtain here are based
on the idea of substituting the moment-matching tester of [ KSV24b ] with a more localized test,
depending on a candidate output hypothesis bfprovided by a training algorithm run on samples from
the training distribution. In particular, we estimate the Chow parameters [ OS08 ]Ex∼Dtest
X[bf(x)xα]
for all low-degree monomials xα=Qd
i=1xαi
iand reject if they do not match the corresponding
quantities Ex∼D[bf(x)xα]under the training marginal. We obtain the following result.
Proposition 3.2 (Informal, see Theorem C.3) .For any class Cwith low sandwiching degree under
D, the low-degree chow matching tester is a tester for localized discrepancy for the neighborhood
N(bf) =C, i.e., it certifies that Px∼Dtest
X[bf(x)̸=f(x)]≤Px∼D[bf(x)̸=f(x)] +ϵfor all f∈ C.
Proof Outline. The main observation for obtaining the localized discrepancy testing result
is that the disagreement between two functions is a linear function of their correlation, i.e.,
2Px∼Dtest
X[bf(x)̸=f(x)] = 1 −Ex∼Dtest
X[bf(x)f(x)], and, because f∈ C, it is sandwiched
by two polynomials pup, pdown, which implies Ex∼Dtest
X[bf(x)f(x)]≥Ex∼Dtest
X[bf(x)pup(x)]−
Ex∼Dtest
X[pup(x)−pdown(x)]. The latter quantity can be certified to be close to the corresponding
quantity under the training marginal Dby Chow (and moment) matching.
Although the notion of neighborhood we require here is quite generic, it is sufficient to provide
significant improvements over prior work. The discrepancy tester is localized in the sense that
it certifies properties of the tested marginal distribution that are related to a particular candidate
hypothesis bf, but actually considers the whole concept class Cto be inside the neighborhood of
bf. Since the concept f∗that achieves λ= min f∈C(err(f;Dtrain
XY) + err( f;Dtest
XY))lies within Cby
definition, the total test error of bfis directly related to the error achieved by the training algorithm,
whenever the Chow matching tester accepts.
4 Non-Parametric Low-Dimensional Classes
For non-parametric classes like convex sets over Rd, dimension-efficient TDS learning is impossible,
even from an information-theoretic perspective [ KSV24b ] and 2Ω(d)time is required even in the
realizable setting. However, the best known upper bound on the L1sandwiching degree for convex
sets is given indirectly by known results in approximation of convex sets by intersections of halfspaces
(see, e.g., [ DNS23 ] and references therein) and implies a TDS learning algorithm that runs in time
doubly exponential in d. Improving on the doubly exponential bound based on L1-sandwiching,
6we provide a realizable TDS learner with singly exponential (in poly( d)) runtime for convex sets
that are ϵ-balanced, meaning that the Gaussian mass of both the interior and the exterior of the
convex set is at least ϵ. For convex sets with only a few relevant dimensions, our results actually give
dimension-efficient TDS learners. For more details, see Appendix D.
Theorem 4.1 (TDS Learning of Convex Subspace Juntas) .Forϵ∈(0,1/2),d, k∈N, letCbe the
class of ϵ-balanced convex sets over Rdwithkrelevant dimensions. There is an O(ϵ)-TDS learner for
Cwith respect to Ndin the realizable setting, which, for the training phase, uses poly( d)2poly( k/ϵ)
samples and time and, for the testing phase, uses poly( d)(k/ϵ)O(k)samples and time.
We note that the balancing assumption is mild, since it can be tested by using examples from the
training distribution and has been used in prior work on realizable TDS learning of intersections of
halfspaces with respect to the Gaussian distribution [KSV24a].
Universal TDS Learners. Importantly, the TDS learner of Theorem 4.1 can be made universal with
respect to a wide class of distributions that enjoy some mild concentration and anti-concentration
properties. The cost is an exponential deterioration of the runtime of the training phase. In other
words, finding a hypothesis with better performance on the training distribution suffices to give error
guarantees for a wide range of test distributions, including, for example, multi-modal and heavy-tailed
distributions. We believe that this result is interesting even from an information-theoretic perspective.
In Table 2 in the appendix, we give a more precise trade-off between universality and training runtime.
LetDkbe the class of distributions DoverRdsuch that Ex∼D[(v·x)4]≤Cfor any v∈Sd−1and
for any subspace W⊆Rdof dimension at most k, the marginal density of DonWis upper bounded
byCk2, where Cis some positive universal constant. Then the following is true.
Theorem 4.2 (Universal TDS Learning of Convex Subspace Juntas) .There is a Dk-universal O(ϵ)-
TDS learner for k-dimensional ϵ-balanced convex sets over Rdwith respect to Ndin the realizable
setting, which, for the training phase, uses poly( d) exp(2O(k2/ϵ))samples and time and, for the
testing phase, uses poly( d)kO(k3/ϵ2)samples and time.
We remark that the testing time for the universal TDS learner of Theorem 4.2 is still singly exponential
inpoly( k), although the dependence on ϵis exponentially worse. Having lower testing runtime is a
desirable feature because the potential users of large machine learning models might have limited
resources compared to those available during training. We provide a more thorough discussion about
this feature in the following section.
Cylindrical grids tester for localized discrepancy. To obtain our TDS learning results of The-
orems 4.1 and 4.2, we once more make use of the localized discrepancy testing framework. In
particular, we identify low-dimensionality (Definition D.1) and boundary smoothness (Definition D.4)
of the underlying concept class as sufficient conditions for efficient testing of localized discrepancy
when the notion of localization is defined with respect to the subspace neighborhood (Theorem D.7).
The subspace neighborhood Ns(bf)contains low-dimensional concepts fwhose relevant subspace
is geometrically close to the relevant subspace for bf(see Definition D.2). For TDS learning, we
combine such testers with known learning algorithms for subspace recovery of low-dimensional
convex sets (see, e.g., [ Vem10a ,KSV24a ] and Theorem D.13) to ensure that the training phase will
output some hypothesis bfsuch that the ground truth f∗lies within Ns(bf).
In other words, we exploit the existence of training algorithms with stronger guarantees (i.e., ap-
proximate subspace recovery) than merely training error bounds, to relax the discrepancy testing
problem to a low-dimensional localized version, while still providing end-to-end results for TDS
learning. This relaxation not only improves the testing runtime, but also enables universality, since
the localized discrepancy between two distributions can be much smaller than the global discrepancy
between them (see also [ZLWJ20] and references therein).
The idea behind the localized discrepancy tester for the subspace neighborhood is to split the
disagreement between bfand an arbitrary concept f∈Ns(bf)under the test distribution in two parts:
(1) the disagreement between bfand a rotated version ˜foffwhere the input xis projected on the
relevant subspace of the given hypothesis bfinstead of the actual, unknown relevant subspace of f
and (2) the disagreement between ˜fandf. For part (2), we use the fact that the relevant subspace
offis geometrically close to the relevant subspace for bf(since f∈Ns(bf)). We conclude that f
7and˜fcan only disagree far from the origin and, hence, testing that the test marginal is appropriately
concentrated suffices to give the desired bound.
Low-dimensional disagreement between concepts with smooth boundaries. For part (1), we use
the fact that the k-dimensional relevant subspace Vforbfis known. We construct a grid on Vand run
tests to certify that the probability (under the test marginal) of falling inside each of the cells is not
unreasonably large. In order to bound the size of the grid, we also test that the probability of falling
far from the origin on the subspace Vis appropriately bounded. We then argue that the disagreement
region can be approximated reasonably well by discretizing with respect to an appropriately refined
grid. To ensure that the discretization of the near-boundary region does not introduce a significant
error blow-up, it is important that bfand˜fhave smooth boundaries (see Figure 2 in the appendix).
5 Fully Polynomial-Time Testers
Algorithms for TDS learning that are efficient in testing time, can be useful to check whether a
pre-trained model can be applied to a particular population, without the need for overly expensive
resources. Here, we focus on the class of balanced intersections of halfspaces (see Definition E.9)
and provide the first TDS learner for this class that runs in fully polynomial time during test time.
Moreover, the proposed tester is universal with respect to a wide class of distributions that satisfy
some concentration and anticoncentration properties.
LetD1be the class of distributions DoverRdsuch that for any v∈Sd−1we have Ex∼D[(v·x)4]≤C
and, also, that the one-dimensional density of the projection v·xwhere x∼ D is upper bounded by
C, where Cis some positive universal constant. Then the following is true (see also Theorem E.10).
Theorem 5.1 (Universal TDS Learning of Balanced Intersections) .Forϵ∈(0,1/2),d, k∈N, there
is aD1-universal O(ϵ)-TDS learner for the class of ϵ-balanced intersections of khalfspaces over Rd
w.r.t.Ndin the realizable setting, which, for the training phase, uses poly( d) exp( O(k5/ϵ))samples
and time and, for the testing phase, uses poly( d, k,1/ϵ)samples and time.
For comparison, the previous state-of-the-art TDS learning algorithm for halfspace intersec-
tions by [ KSV24a ] had overall runtime dO(log(k/ϵ))+ poly( d) exp( O(k6/ϵ8))and testing runtime
dO(log(k/ϵ))+poly( d)(k/ϵ)O(k2)(although training and testing were not explicitly separated). Hence,
the overall runtime of the algorithm of Theorem 5.1 is better than the previous state-of-the-art, but
also enjoys two additional properties: (1) the testing time is fully polynomial and (2) the tester is
universal with respect to a wide class (of multimodal and even heavy-tailed distributions).
We note that it is not by chance that these two properties are satisfied simultaneously: they both relate
to the fact that it suffices to solve a simple discrepancy testing problem. Since the tested property
is relaxed, more distributions should satisfy it and testing the property can be made efficient. For
comparison, as well as to provide a TDS learner with better overall runtime in some regimes, we may
trade-off universality and test-time efficiency to obtain the following result (see Theorem E.10).
Theorem 5.2 (TDS Learning of Balanced Intersections) .Forϵ∈(0,1/2),d, k∈N, there is an
O(ϵ)-TDS learner for the class of ϵ-balanced intersections of khalfspaces over Rdw.r.t.Ndin the
realizable setting, which, for the training phase, uses poly( d)(k/ϵ)O(k3)samples and time and, for
the testing phase, uses (dk)O(log(1 /ϵ))samples and time.
Remark 5.3.The algorithms of Theorems 5.1 and 5.2 can both tolerate some amount of noise,
i.e., provide an O(ϵ)error guarantee even when λ= min f∈C(err(f;Dtrain
XY) + err( f;Dtest
XY))is
non-zero (but sufficiently small). For Theorem 5.1, the amount of noise that can be tolerated is
λ= exp( −˜O(k/ϵ)), while for Theorem 5.2, the tolerated amount is λ= (k/ϵ)−O(k)(see Table 3).
The amount of noise tolerated by the non-universal tester is more, because the test is more expensive
and, therefore, does a better job in translating the guarantees of the training phase to guarantees for
the test error. For comparison, the Chow matching tester of Theorem 3.1 runs much more expensive
tests and can, therefore, tolerate much more noise, i.e., λ=O(ϵ).
Discrepancy testing through boundary proximity. We once more use the framework of localized
discrepancy testing, in order to obtain TDS learners with strong guarantees. In order to achieve fully
polynomial-time performance, we aim to use a tester that is as simple as possible. In particular, for a
given halfspace intersection bf, we test whether the probability that an example drawn from the test
8marginal falls close to the boundary of bf, i.e., close to at least one of the defining halfspaces (see
Lemma E.13 and Definition E.3). We also test concentration of the test distribution marginal.
Interestingly, we show that these two tests are sufficient for certifying low localized discrepancy
from the Gaussian distribution with respect to the notion of disagreement neighborhood Ne, i.e.,
f∈Ne(bf)if the Gaussian disagreement Px∼Nd[f(x)̸=bf(x)]between fandbfis small enough
(see Definition E.2). In particular, we show that if fis a balanced intersection and f∈Ne(bf),
thenfandbfcan only differ either (1) far from the origin or (2) close to the boundary of bf(see
Proposition E.4 and Lemma E.12). Importantly, this property is point-wise: for any x∈Rdsuch that
f(x)̸=bf(x),xwill either satisfy (1) or (2) and, hence, no distribution over Rdcan fool our tester.
In the heart of our proof is a geometric lemma which demonstrates that any balanced convex set is
locally balanced as well (Lemma E.12), meaning that for any point x∈Rd, there is a large number
of points near xwith the same label as x. Therefore (unless the norm of xis large), any hypothesis
bfwith low Gaussian disagreement from the ground truth f∗, must encode all of the local structure
(or boundary) of f∗that is not very far from the origin. To show this, we use a geometric argument
about convex sets (see Figure 1 for the case when the label of xis1. The other case is simpler and
follows by the existence of a separating hyperplane between a convex set and any point outside it).
Figure 1: If xlies within a balanced convex set K, then many points close to xlie within Kas well,
i.e., there is a cone R′withR′⊆B(x, ϱ)∩ K, where B(x, ϱ)is a ball around x. The ball centered
atxcexists due to the fact that Kis balanced: any balanced convex set contains some ball with
non-negligible radius. The convex hull of xand the ball at xclies within K. (See also Fig. 3)
Since we have a localized discrepancy tester with respect to the disagreement neighborhood, all we
need from the training phase is to output some intersection of halfspaces bfwith low training error (so
that the ground truth f∗lies within Ne(bf)). Hence, we may use any proper PAC learning algorithm
for intersections of halfspaces under the Gaussian distribution. We use the algorithm by [ DKS18b ]
(see also Theorem E.11).
Remark 5.4.We note that the three important properties we used to apply the method of boundary
proximity are that (1) the hypothesis bfreturned by the learning algorithm admits an efficient boundary
proximity tester and (2) the ground truth f∗is locally balanced and (3) that bfandf∗are both low-
dimensional. For more details, see Appendix E.
6 Limitations, Future Work and Broader Impacts
TDS learning beyond discrepancy testing. We show that all of the known results in TDS learning
can be achieved (and improved) by decoupling the training and testing phases. While separating
training and testing phases is appealing and well-motivated by real-world scenarios, it is an interesting
open question whether using the examples from the test marginal during training time could lead to
improved TDS learning algorithms.
Characterizations of discrepancy testing complexity. We provide several positive results for local-
ized discrepancy testing which imply new results in TDS learning. Moreover, on the lower bounds
side, in Appendix F, we show that global discrepancy testing is NP-hard even for simple classes
under no further assumptions. It is an interesting open question to explore tight characterizations for
dimension-efficient, universal or fully polynomial-time localized discrepancy testing.
Lifting the balancing assumption. For our universal TDS learners (and universal discrepancy
testers), we require that the underlying concept class only contains concepts that are not too biased
9towards one of the two possible labels under the training distribution (so that the training examples
include enough information for localization). This condition is mild and can be easily tested by using
training examples. However, better understanding of the importance of this condition for (universal)
TDS learning could potentially lead to (or rule out) improved and/or universal algorithms for broader
concept classes, e.g., polynomial threshold functions.
Relaxing assumptions on training marginal. Our main results in this work hold under the assump-
tion that the marginal of the training distribution is either the Gaussian distribution or the uniform
distribution over the hypercube. Such assumptions are standard in learning theory, as they serve as a
concrete theoretical testbed for simplifying the analysis and presentation of the proposed algorithms
and ideas. Relaxing those assumptions is an important and obvious goal for future work and parts of
our analysis hint towards such relaxations (see, e.g., Remarks D.10 and E.5).
Broader Impacts. We do not identify any direct potential negative societal impacts. In fact, although
our results are of theoretical nature, our algorithms might, in principle, help mitigate potentially unfair
outcomes of applying certain pre-trained models on populations that are misrepresented in training
data. Our discrepancy testers will either certify low prediction error on the deployment population
or signal that the model at hand might not be applicable to the deployment population and another
model should be considered.
References
[AGM03] Noga Alon, Oded Goldreich, and Yishay Mansour. Almost k-wise independence versus
k-wise independence. Information Processing Letters , 88(3):107–110, 2003.
[Bal93] Keith Ball. The reverse isoperimetric problem for gaussian measure. Discrete Comput.
Geom. , 10(4):411–420, dec 1993.
[Baz09] Louay MJ Bazzi. Polylogarithmic independence can fool dnf formulas. SIAM Journal
on Computing , 38(6):2220–2272, 2009.
[BCK+07]John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman.
Learning bounds for domain adaptation. Advances in neural information processing
systems , 20, 2007.
[BDBC+10]Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
Jennifer Wortman Vaughan. A theory of learning from different domains. Machine
learning , 79:151–175, 2010.
[BDBCP06] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of
representations for domain adaptation. Advances in neural information processing
systems , 19, 2006.
[BGS18] Arnab Bhattacharyya, Suprovat Ghoshal, and Rishi Saket. Hardness of learning noisy
halfspaces using polynomial thresholds. In Sébastien Bubeck, Vianney Perchet, and
Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory ,
volume 75 of Proceedings of Machine Learning Research , pages 876–917. PMLR,
06–09 Jul 2018.
[BK97] Avrim Blum and Ravindran Kannan. Learning an intersection of a constant number of
halfspaces over a uniform distribution. J. Comput. Syst. Sci. , 54(2):371–380, 1997.
[Bog98] Vladimir Igorevich Bogachev. Gaussian measures . Number 62. American Mathemati-
cal Soc., 1998.
[Bra10] Mark Braverman. Polylogarithmic independence fools AC0circuits. Journal of the
ACM (JACM) , 57(5):1–10, 2010.
[DKK+23]Ilias Diakonikolas, Daniel Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Effi-
cient testable learning of halfspaces with adversarial label noise. Advances in Neural
Information Processing Systems , 36, 2023.
10[DKN10] Ilias Diakonikolas, Daniel M Kane, and Jelani Nelson. Bounded independence fools
degree-2 threshold functions. In 2010 IEEE 51st Annual Symposium on Foundations of
Computer Science , pages 11–20. IEEE, 2010.
[DKS18a] Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Learning geometric concepts
with nasty noise. In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors,
Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2018, Los Angeles, CA, USA, June 25-29, 2018 , pages 1061–1073. ACM, 2018.
[DKS18b] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts
with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on
Theory of Computing , pages 1061–1073, 2018.
[DLLP10] Shai Ben David, Tyler Lu, Teresa Luu, and Dávid Pál. Impossibility theorems for
domain adaptation. In Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics , pages 129–136. JMLR Workshop and Conference
Proceedings, 2010.
[DNS23] Anindya De, Shivam Nadimpalli, and Rocco A Servedio. Gaussian approximation of
convex sets by intersections of halfspaces. arXiv preprint arXiv:2311.08575 , 2023.
[DTK22] Ilias Diakonikolas, Christos Tzamos, and Daniel M Kane. A strongly polynomial
algorithm for approximate forster transforms and its application to halfspace learning.
arXiv preprint arXiv:2212.03008 , 2022.
[GKK23] Aravind Gollakota, Adam R Klivans, and Pravesh K Kothari. A moment-matching
approach to testable learning and a new characterization of rademacher complexity.
Proceedings of the fifty-fifth annual ACM Symposium on Theory of Computing , 2023.
[GKKM20] Shafi Goldwasser, Adam Tauman Kalai, Yael Kalai, and Omar Montasser. Beyond
perturbations: Learning guarantees with arbitrary adversarial test examples. Advances
in Neural Information Processing Systems , 33:15859–15870, 2020.
[GKM12] Parikshit Gopalan, Adam R. Klivans, and Raghu Meka. Learning functions of halfs-
paces using prefix covers. In Shie Mannor, Nathan Srebro, and Robert C. Williamson,
editors, COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-
27, 2012, Edinburgh, Scotland , volume 23 of JMLR Proceedings , pages 15.1–15.10.
JMLR.org, 2012.
[GKSV23] Aravind Gollakota, Adam Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan.
Tester-learners for halfspaces: Universal algorithms. Advances in Neural Information
Processing Systems , 36, 2023.
[GKSV24] Aravind Gollakota, Adam R Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan.
An efficient tester-learner for halfspaces. The Twelfth International Conference on
Learning Representations , 2024.
[HKM23] Steve Hanneke, Samory Kpotufe, and Yasaman Mahdaviyeh. Limits of model selection
under transfer learning. In The Thirty Sixth Annual Conference on Learning Theory ,
pages 5781–5812. PMLR, 2023.
[HS19] Prahladh Harsha and Srikanth Srinivasan. On polynomial approximations to AC0.
Random Structures & Algorithms , 54(2):289–303, 2019.
[KK21] Adam Tauman Kalai and Varun Kanade. Efficient learning with arbitrary covariate
shift. In Algorithmic Learning Theory , pages 850–864. PMLR, 2021.
[KKM13] Daniel M. Kane, Adam R. Klivans, and Raghu Meka. Learning halfspaces under
log-concave densities: Polynomial approximations and moment matching. In Shai
Shalev-Shwartz and Ingo Steinwart, editors, COLT 2013 - The 26th Annual Conference
on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA , volume 30 of
JMLR Workshop and Conference Proceedings , pages 522–545. JMLR.org, 2013.
11[KKMS08] Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio.
Agnostically learning halfspaces. SIAM Journal on Computing , 37(6):1777–1805,
2008.
[KLT09] Adam Klivans, Philip Long, and Alex Tang. Baum’s algorithm learns intersections of
halfspaces with respect to log-concave distributions. pages 588–600, 01 2009.
[KM21a] Zander Kelley and Raghu Meka. Random restrictions and prgs for ptfs in gaussian
space. arXiv preprint arXiv:2103.14134 , 2021.
[KM21b] Samory Kpotufe and Guillaume Martinet. Marginal singularity and the benefits of
labels in covariate-shift. The Annals of Statistics , 49(6):3299–3323, 2021.
[KOS04] Adam R. Klivans, Ryan O’Donnell, and Rocco A. Servedio. Learning intersections
and thresholds of halfspaces. J. Comput. Syst. Sci. , 68(4):808–840, 2004.
[KOS08a] Adam R. Klivans, Ryan O’Donnell, and Rocco A. Servedio. Learning geometric
concepts via gaussian surface area. In 49th Annual IEEE Symposium on Foundations
of Computer Science, FOCS 2008, October 25-28, 2008, Philadelphia, PA, USA , pages
541–550. IEEE Computer Society, 2008.
[KOS08b] Adam R Klivans, Ryan O’Donnell, and Rocco A Servedio. Learning geometric concepts
via gaussian surface area. In 2008 49th Annual IEEE Symposium on Foundations of
Computer Science , pages 541–550. IEEE, 2008.
[KSV24a] Adam Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Learning intersections
of halfspaces with distribution shift: Improved algorithms and sq lower bounds. In
Shipra Agrawal and Aaron Roth, editors, Proceedings of Thirty Seventh Conference on
Learning Theory , volume 247 of Proceedings of Machine Learning Research , pages
2944–2978. PMLR, 30 Jun–03 Jul 2024.
[KSV24b] Adam Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Testable learning
with distribution shift. In Shipra Agrawal and Aaron Roth, editors, Proceedings of
Thirty Seventh Conference on Learning Theory , volume 247 of Proceedings of Machine
Learning Research , pages 2887–2943. PMLR, 30 Jun–03 Jul 2024.
[KZZ24] Alkis Kalavasis, Ilias Zadik, and Manolis Zampetakis. Transfer learning beyond
bounded density ratios. arXiv preprint arXiv:2403.11963 , 2024.
[LV07] László Lovász and Santosh Vempala. The geometry of logconcave functions and
sampling algorithms. Random Structures & Algorithms , 30(3):307–358, 2007.
[LW94] Philip M. Long and Manfred K. Warmuth. Composite geometric concepts and polyno-
mial predictability. Inf. Comput. , 113(2):230–252, 1994.
[MMR09] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation:
Learning bounds and algorithms. In Proceedings of The 22nd Annual Conference on
Learning Theory (COLT 2009) , Montréal, Canada, 2009.
[Nel73] Edward Nelson. The free markoff field. Journal of Functional Analysis , 12(2):211–227,
1973.
[O’D14] Ryan O’Donnell. Analysis of boolean functions . Cambridge University Press, 2014.
[OS08] Ryan O’Donnell and Rocco A Servedio. The chow parameters problem. In Proceedings
of the fortieth annual ACM symposium on Theory of computing , pages 517–526, 2008.
[RMH+20]Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Younès Bennani.
A survey on domain adaptation theory: learning bounds and theoretical guarantees.
arXiv preprint arXiv:2004.11829 , 2020.
[RV23] Ronitt Rubinfeld and Arsen Vasilyan. Testing distributional assumptions of learn-
ing algorithms. Proceedings of the fifty-fifth annual ACM Symposium on Theory of
Computing , 2023.
12[SSK12] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in
machine learning . Cambridge University Press, 2012.
[Tal17] Avishay Tal. Tight bounds on the Fourier spectrum of AC0. In32nd Computational
Complexity Conference (CCC 2017) . Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-
matik, 2017.
[Vem10a] Santosh S Vempala. Learning convex concepts from gaussian distributions with pca.
In2010 IEEE 51st Annual Symposium on Foundations of Computer Science , pages
124–130. IEEE, 2010.
[Vem10b] Santosh S Vempala. A random-sampling-based algorithm for learning intersections of
halfspaces. Journal of the ACM (JACM) , 57(6):1–14, 2010.
[Ver18] Roman Vershynin. High-dimensional probability: An introduction with applications in
data science , volume 47. Cambridge university press, 2018.
[ZLWJ20] Yuchen Zhang, Mingsheng Long, Jianmin Wang, and Michael I Jordan. On localized
discrepancy for domain adaptation. arXiv preprint arXiv:2008.06242 , 2020.
13A Extended Preliminaries
We use standard big-O notation (and ˜Oto hide poly-logarithmic factors), Rdis the d-dimensional
euclidean space and Ndthe standard Gaussian over Rd,{±1}dis the d-dimensional hypercube and
Unif({±1}d)the uniform distribution over {±1}d,Nis the set of natural numbers N={1,2, . . .}
andx∈Rddenotes a vector with x= (x1, . . . ,xd)and inner products x·v. For α∈Nd, we
denote with xαthe productQ
i∈[d]xαi
i,Mα=E[xα]and∥α∥1=P
i∈[d]αi. For a polynomial7
poverRdandα∈Nd, we denote with pαthe coefficient of pcorresponding to xα, i.e., we
havep(x) =P
α∈Ndpαxα. Ifpis a polynomial over {±1}d, then we express it in its multilinear
form, using only coefficients pαwith α∈ {0,1}d, i.e., p(x) =P
α∈{0,1}dpαxα. We define
the degree of pand denote deg(p)the maximum degree of a monomial whose coefficient in pis
non-zero. We use standard notations for norms ∥x∥1=P
i∈[d]|xi|,∥x∥2= (P
i∈[d]x2
i)1/2and
∥x∥∞= max i∈[d]|xi|. We denote with Sd−1thed−1dimensional sphere on Rdand, for x∈Rk
andr >0,Bk(x, r) ={y∈Rd:∥x−y∥2≤r}.
For any v1,v2∈Rd, we denote with v1·v2the inner product between v1andv2and we let ∡(v1,v2)
be the angle between the two vectors, i.e., the quantity θ∈[0, π]such that ∥v1∥2∥v2∥2cos(θ) =
v1·v2. Forv∈Rd, τ∈R, we call a function of the form x7→sign(v·x)an origin-centered (or
homogeneous) halfspace and a function of the form x7→sign(v·x+τ)a general halfspace over Rd.
We let X ⊆Rdbe either the d-dimensional hypercube {±1}dorRd. For a distribution DoverX,
we use ED(orEx∼D) to refer to the expectation over distribution Dand for a given set X, we use
EX(orEx∼X) to refer to the expectation over the uniform distribution on X(i.e.,Ex∼X[g(x)] =
1
|X|P
x∈Xg(x), counting possible duplicates separately). We let R+= (0,∞).
We define the notion of balance as follows.
Definition A.1 (Balanced Concepts) .Forβ∈(0,1), we say that a function f:Rd→ {± 1}is
(globally) β-balanced if for any x∈Rdwe have Pz∼N[f(z) =f(x)]> β.
B Additional Tools
B.1 Boundary Smoothness of Structured Concepts
In this section, we prove that low dimensional polynomial threshold functions and convex sets
have smooth boundary, i.e., a non-asymptotic anticoncentration bounds that scales linearly with the
distance from the boundary. We first prove that PTFs have smooth boundary.
Lemma B.1 (Smooth Boundary for PTFs) .Letpbe a polynomial of degree ℓoverRk. LetF:Rk→
{±1}be the function defined as F(x) = sign( p(x)). Then, Fhas a Cℓ3k-smooth boundary with
respect to Nkfor a large universal constant C.
Proof. LetCbe a large universal constant that we fix later. Let δ= 3Cℓ3γk. Define the set
S:={x| ∃i∈[ℓ],∥∇ip(x)∥2>(Cℓ3/δ)· ∥∇i−1p(x)∥2}. Observe that Px∼Nk[x∈∂γF]≤
Px∼Nk[x∈S] +Px∼Nk[x∈∂γf|x/∈S]. We bound these two terms separately. To bound the first
term, we use the following theorem from [KM21a].
Lemma B.2 (Lemma 1.6 from [ KM21a ]).LetCbe a large universal constant. For any polynomial
p:Rk→Rof degree ℓandx∼ N k, the following event occurs with probability at least 1−δ:
∥∇ip(x)∥2≤(Cℓ3/δ)∥∇i−1p(x)∥2,for all 1≤i≤ℓ.
Thus, we have that Px∼Nk[x∈S]≤δ. Now consider a point x/∈S. From a multivariate taylor
expansion, we have that p(x+z) =p(x) +P
α∈Nk,1≤|α|≤ℓ∂αp(x)
α!·zα. Thus, for z∈Rkwith
7In Appendices D and E, we use the notation pto denote natural numbers and use qfor polynomials instead.
14∥z∥2≤γ, we obtain that
|p(x)−p(x+z)| ≤X
1≤|α|≤ℓ∂αp(x)· ∥z∥|α|
∞≤X
i∈[ℓ]∥z∥i
2· ∥∇ip(x)∥1
≤X
i∈[ℓ]γiki∥∇ip(x)∥2≤X
i∈[l]γiki(Cℓ3/δ)i|p(x)| ≤ |p(x)|/2.
The first inequality follows from the multivariate Taylor expansion. The third inequality follows from
the fact that ∥z∥2≤γand the bound on the number of monomials of size ibyk2i. The penultimate
inequality follows from the definition of the set Sand the last inequality is true by our choice of δ.
Since|p(x)−p(x+z)| ≤ |p(x)|/2, we have that F(x) =F(x+z)for all z∈Rkwith∥z∥2≤γ.
Thus, we have that Px∼Nk[x∈∂γF|x/∈S] = 0 . Thus, we have that Px∼Nk[x∈∂γF]≤
3Cℓ3γk.
We now move on to proving that low dimensional convex sets. To prove this, we will crucially use
the notion of Gaussian surface area (an asymptotic anticoncentration bound) that we will now define.
Definition B.3 (Gaussian Surface Area) .Letfbe a boolean function. The Gaussian surface area
Γ(f)is defined as
Γ(f) = lim inf
δ→01
δP
z∼N(0,Ik)
z∈Aδ
f\Af
,
where Af= 1{x|f(x) = 1}, Aδ
f={u: min v∈Af∥u−v∥2≤δ}.
We prove that convex sets have smooth boundary in two steps. We first prove that the set of
points inside the set that are close to it’s boundary have small mass. To do this, we use a noise
sensitivity argument (Lemma B.5). Then, we prove that points outside it that are close to the boundary
(Lemma B.7). This will follow from an argument uses the definition of Gaussian Surface area and a
bound on this quantity for convex sets due to [ Bal93 ]. Together, these two lemmas imply that convex
sets have smooth boundary.
The following lemma will be useful in proving the smooth boundary of the interior of the set.
Lemma B.4. Letλ∈(0,1/2). Let Sbe a convex set on Rkand let f(x) = 1{x∈S}be the
indicator function of S. Then, we have that Px∼Nk[f(x)̸=f(x/√
1−λ)]≤klogk√
λ.
Proof. For any vector w∈Rkwith∥w∥2= 1, letfw:R+→Rbe the function defined as
fw(r) =f(r·w). Also, note that fwis the indicator function of a one dimensional convex set.
Observe that Px∼Nk[f(x)̸=f(x/√
1−λ)]≤sup∥w∥2=1Pr∼χ2(k)[fw(√r)̸=fw(√r/√
1−λ)]
from the fact that the kdimensional Gaussian conditioned on pointing in direction wis distributed as√rwwhere r∼χ2(k). Here, χ2(k)is the one dimensional Chi-squared distribution with mean k.
We have thus reduced the problem to one dimension. Consider a function g:R→Rsuch that g(x) =
1{x∈[p
a/(1−λ),p
b/(1−λ)]}where a, bare from R+∪{+∞}. All one dimensional indicators
of convex sets are of this form. We will now prove that Pr∼χ2(k)[g(√r)̸=g(√r/p
1−λ)]≤
kλlog(k/λ).
Observe that Pr∼χ2(k)[g(√r)̸=g(p
r/1−λ)]≤Pr∼χ2(k)[r∈[a, a/(1−λ)]∪[b, b/(1−λ)]]. It
suffices to bound Pr∼χ2(k)[r∈[a, a/(1−λ)]]fora∈R+as the claim then follows from a union
bound. We bound this by splitting into two cases.
Case 1: a≥2klog(k/λ).Since χ2(k)is the distribution of the sum of squares of kindepen-
dentN(0,1)Gaussian random variables, we have that Pr∼χ2(k)[r≥a]≤kPx∼N(0,1)[|x|2≥
a/k]≤ke−a/(2k). Thus, when a≥2klog(k/λ), we have that Pr∼χ2(k)
r∈[a, a/(1−λ)]
≤
Pr∼χ2(k)
r≥a
≤λ.
Case 2: a < 2klog(k/λ).Letψbe the density function for χ2(k). It is a standard fact from
probability that ψ(x) =xk/2−1
2k/2Γ(k/2)e−x/2. Fork= 1, it is a fact that ψ(x)≤1. Fork≥2, by taking
15a derivative, we can see that this density function is maximized at x=k−2. We obtain that
ψ(x) =(k−2)k/2−1
2k/2Γ(k/2)e−k/2+1≤((k−2)·e)k/2−1
2k/2(k/2)k/2−1e−k/2+1≤1
2
where the second inequality follows from the fact that Γ(t)≥ t
et−1for all t≥2andΓ(1) = 1 .
We have that
P
r∼χ2(k)
r∈[a, a/(1−λ)]
≤ ∥ψ∥∞·a(1/(1−λ)−1)≤4kλlog(k/λ)≤4klogk√
λ .
We get the first inequality from the upper bound on the density. The second follows from the fact
that1/(1−λ)≤1 + 2λwhen λ <1/2. The third inequality follows from the assumption on a. The
final inequality follows from the fact that xlog(1/x)≤√x.
We are now ready to prove the set of points inside the convex set that are close to it’s boundary have
small mass under the Gaussian.
Lemma B.5. LetSbe a convex set on Rk. Letϱ∈(0,1). Then, we have that Px∼Nk[x∈S∩∂ϱS]≤
Cklogkϱwhere Cis a large universal constant.
Proof. Define the function f:Rk→Rasf(x) = 1{x∈S}. We now use a restatement of
Corollary 12 from [KOS08b].
Lemma B.6. Letgbe a boolean function on Rk. For any λ∈(0,1), it holds that
P
x,y∼Nkh
g(x)̸=g√
1−λx+√
λyi
≤C√
λΓ(g)
for large universal constant C.
Letgbe the function g(x) =f(x/√
1−λ). Observe that gis also an indicator of a convex set.
From [ Bal93 ] we have that Γ(g)≤4k1/4. Thus, applying Lemma B.6 to g, we obtain that for any
λ∈(0,1)
P
x,y∼Nk"
f(x/√
1−λ)̸=f 
x+r
λ
1−λy!#
≤C√
λk1/4
where Cis a large constant. Combining the above expression with Lemma B.4, we obtain that for
anyλ∈(0,1/2),
P
x,y∼Nk"
f(x)̸=f 
x+r
λ
1−λy!#
≤C√
λk1/4+ 2klogk√
λ . (B.1)
Now, consider any point pinS∩∂ϱS. Since Sis convex, there exists a hyperplane h(x) = 1{w·x+
b≥0}forw∈Rkwith∥w∥2= 1andb∈Rsuch that h(y) = 1 for all y∈Sandw·p+b≤ϱ.
This hyperplane correponds to the tangential plane whose normal vector is the line joining pand the
point closest to it in ∂S. We have that for any γ >0,Pz∼Nk[w·γz≤ −ϱ]≥1
2−ϱ
2γas the Gaussian
density is upper bounded by 1pointwise. Thus, for any γ >0,Pz∼Nk[f(p+γz)̸=f(p)]≥1
2−ϱ
2γ.
Combining this with Equation (B.1), we obtain that
 
1
2−ϱ
2·r
1−λ
λ!
·P
x∼Nk[x∈S∩∂ϱS]≤C√
λk1/4+ 2klogk√
λ .
Setting λ= 4ϱ2and rearranging terms, we obtain that Px∼Nk[x∈S∩∂ϱS]≤C′klogkϱwhere C′
is a sufficiently large universal constant.
We now prove the smoothness result for points outside the set.
Lemma B.7. LetSbe a convex set on Rk. Let ϱ∈(0,1). Then, we have that Px∼Nk[x∈
Sc∩∂ϱS]≤Ck1/4ϱwhere Cis a sufficiently large universal constant.
16Proof. Fort >0, define the set StasSt={x∈Rk|infy∈S∥x−y∥2≤t}. We have that
P
x∼Nk[x∈Sc∩∂ϱ] =P
x∼Nk[x∈Sϱ\S]
=Zϱ
t=0Z
x∈∂StN(x; 0, Ik)dxdt≤Zϱ
t=0Ck1/4dt≤Ck1/4ϱ
where Cis a large universal constant. We obtained the penultimate inequality using the definition of
Gaussian surface area.
We now state our final result on the smooth boundary of convex sets.
Lemma B.8 (Smooth Boundary for Convex sets) .LetSbe a convex set. Let F:Rk→ {± 1}be the
function defined as F(x) = 1{x∈S}. Then, Fhas a Cklogk-smooth boundary with respect to
Nkfor a sufficiently large universal constant C.
Proof. The proof is immediate from Lemma B.5 and Lemma B.7.
B.2 Sandwiching Polynomials
In this section, we present known results from pseudorandomness literature on the existence of
sandwiching polynomials for various function classes with respect to Unif{±1}dandNd. Although
previously known, these results are mostly not stated in the manner in which we need them. In
particular, the coefficient bounds are not explicity stated in previous work. We state these results in
terms of existence of sandwiching polynomials with coefficient bounds for completeness.
We now introduce the important notion of (δ, ℓ)-independent distributions.
Definition B.9 ((δ, ℓ)-independent distribution) .LetD,D′be distributions on Rd. For δ >0and
ℓ∈N, we say that the distribution D′is(δ, ℓ)-independent with respect to DifEx∼D[xα]−
Ex∼D′[xα]≤δfor all α∈Nd.
We drop the "with respect to D" when the distribution is clear from context. Let D,D′be distributions
onX ⊆Rdandf:X → {± 1}. For ϵ > 0, we say that D′ϵ-fools fwith respect to DifEx∼D[f(x)]−Ex∼D′[f(x)]≤ϵ(again, we drop the "with respect to" when the target distribution
is clear from context). For a concept class C, we say that D′ϵ-foolsCwith respect to DifD′ϵ-fools
fwith respect to Dfor all functions f∈ C.
We will use the following result from [ GKK23 ] which is a generalization of a result from [ Baz09 ].
We will only need one direction of the result which we state below.
Lemma B.10. [Theorem 3.2 from [ GKK23 ]] LetDbe a distribution on X ⊆Rd. Letδ, ϵ > 0and
ℓ∈N. Let f:X →Rdbe a function that satisfies the following property: given any distribution
D′that is (δ, ℓ)-independent with respect to D, we have thatEx∼D[f(x)]−Ex∼D′[f(x)]≤ϵ.
Then, there exists degree ℓpolynomials pdown, pupsuch that pdown≤f≤pupandEx∼D[pup(x)−
pdown(x)] +δ(|pup|+|pdown|)≤ϵ.
B.2.1 Sandwiching Polynomials: Boolean
In this section, the target distrbution is Unif{±1}d. We will find the following lemma useful.
Lemma B.11. Letϵ > 0andℓ∈N. Let f:{±1}d→ {± 1}be a function such that all
(0, ℓ)-independent distributions ϵ-foolf. Then, there exists polynomials pup, pdown of degree ℓand
coefficients bounded by O(dℓ)such that pdown≤f≤pupandEx∼Unif{±1}d[pup(x)−pdown(x)]≤
O(ϵ).
Proof. We use the following theorem from [ AGM03 ] that states that for any (δ, ℓ)-distribution , there
exists a (0, ℓ)distribution that is ϵ-close to it in TV distance.
Lemma B.12 (Theorem 2.1 from [ AGM03 ]).Forδ >0andℓ∈N, letDbe a(δ, ℓ)-independent
distribution on {±1}d. Then, there exists a distribution D′that is (0, ℓ)-independent such that the TV
distance between DandD′is at most δdℓ.
17From the above claim, we have that any (ϵ/dℓ, ℓ)-independent distribution 2ϵ-fools f. Thus, from
Lemma B.10, there exists polynomials pup, pdown of degree ℓwith coefficients bounded by O(dℓ)
such that Ex∼Unif{±1}d[pup(x)−pdown(x)]≤2ϵ. This proves the claim.
Lemma B.13 (Sandwiching polynomials for degree 2PTFs) .LetCbe the class of degree 2PTFs. For
ϵ >0, theO(ϵ)-approximate L1sandwiching degree of Cunder Unif{±1}dis at most ℓ=˜O(1/ϵ9)
with coefficient bound O(dℓ).
Proof. From [ DKN10 ], we have that (0, ℓ)-independent distributions ϵ-foolsCwhen ℓ=˜O(1/ϵ9).
Now, we apply Lemma B.11 to finish the proof.
Lemma B.14 (Sandwiching polynomials for depth- tAC0).LetCbe the class of depth- tAC0circuits
of size son{±1}d. Forϵ >0, theO(ϵ)-approximate L1sandwiching degree of Cunder Unif{±1}d
is at most ℓ= (log s)O(t)log(1/ϵ)with coefficient bound O(dℓ).
Proof. From [ Bra10 ,Tal17 ,HS19 ], we have that (0, ℓ)-independent distributions ϵ-fools fwhen
ℓ= (log s)O(t)log(1/ϵ). Now, we apply Lemma B.11 to finish the proof.
B.2.2 Sandwiching Polynomials: Gaussian
Lemma B.15. Letϵ >0andℓ∈N. Let f:Rd→ {± 1}be a function such that all (0, ℓ)-
independent distributions ϵ-fool f. Then, there exists polynomials pup, pdown of degree ℓand
coefficients bounded by O(dℓ)such that pdown≤f≤pupandEx∼Nd[pup(x)−pdown(x)]≤O(ϵ).
Proof. From Lemma B.10, we have that there exists pup, pdown of degree ℓsuch that Ex∼Nd[pup(x)−
pdown(x)]≤2ϵandpdown≤f≤pup. The claim now follows from the following lemma(proof is
included in the end of this section) that states that any sandwiching polynomial with respect to Nd
must have bounded coefficients.
Lemma B.16. Letf:Rd→ {± 1}be a function, and let pupandpdown be degree- ℓpolyno-
mials satisfying the following (i) for every x∈Rdwe have pup(x)≥f(x)≥pdown(x). (ii)
Ex∈N(0,I)[pup(x)−pdown(x)]≤1. Then, the polynomials pupandpdown both have coefficients
bounded by 2·(10d)ℓin absolute value.
Lemma B.17 (Sandwiching polynomials for degree 2PTFs) .LetCbe the class of degree 2PTFs.
Forϵ >0, theO(ϵ)-approximate L1sandwiching degree of CunderNdis at most ℓ=˜O(1/ϵ8)with
coefficient bound O(dℓ).
Proof. From [ DKN10 ], we have that (0, ℓ)-independent distributions ϵ-foolsCwhen ℓ=˜O(1/ϵ8).
Now, we apply Lemma B.15 to finish the proof.
In the remainder of this section, we prove Lemma B.16. We will use the notion of Hermite poly-
nomials. Recall that for i= 0,1,2,·Hermite polynomials {Hi}are the unique collection of
polynomials over Rthat are orthogonal with respect to Gaussian distribution. In other words
Ex∈N(0,1)[Hi(x)Hj(x)] = 0 whenever i̸=j. In this work, we normalize the Hermite polynomials
to further satisfy Ex∈N(0,1)[Hi(x)Hi(x)] = 1 . It is a standard fact from theory of orthogonal polyno-
mials that H0(x) = 1 ,H1(x) =xand for i≥2Hermite polynomials satisfy the following recursive
identity:
Hi+1(x)·p
(i+ 1)! = xHi(x)·√
i!−i·Hi−1(x)·p
(i−1)!
Proposition B.18. Each coefficient of Hiis bounded by 2iin absolute value.
Proof. This follows immediately from the recursion relation.
Proposition B.19. All coefficients of multi-dimensional polynomial Hi1(x1)Hi2(x2)···Hid(xd)
are bounded by 2i1+i2+···+id.
18Proof. Each monomial of Hi1(x1)Hi2(x2)···Hid(xd)can be expressed asQ
jmj(xj)where each
mj(xj)is a monomial of Hij(xj). But we know that the coefficient of mjis bounded by 2ijin
absolute value. Thus, each coefficient of Hi1(x1)Hi2(x2)···Hid(xd)is at most 2i1+i2+···+id.
Proposition B.20. Letpbe a polynomial over Rdof degree ℓ. Suppose that psatisfies
E
x∈N(0,I)[(p(x))2]≤1,
then every monomial of phas a coefficient of at most (2d)ℓin absolute value.
Proof. For an element x∈Rdwe let (x1,···,xd)be its coordinates. We expand p(x)as a sum of
multidimensional Hermite polynomials8:
p(x) =X
i1,i2,···id≥0
i1+i2+···id≤ℓαi1,i2,···,idHi1(x1)Hi2(x2)···Hid(xd) (B.2)
Due to orthogonality of Hermite polynomials, we have:
X
i1,i2,···id≥0
i1+i2+···id≤ℓα2
i1,i2,···,id=E
x∈N(0,I)[(p(x))2]≤1
In particular, this implies that each coefficient αi1,i2,···,idis bounded by 1in absolute value. Combin-
ing this with Equation B.2, Proposition B.19 and the fact that there are at most dℓways to choose
i1, i2,···id≥0satisfyingP
jij≤ℓ, we see that each coefficient of pbounded by (2d)ℓin absolute
value.
Finally, we need the following standard fact.
Fact B.21 (Gaussian Hypercontractivity [ Bog98 ],[Nel73 ]).Ifp:Rd→Ris a polynomial of degree
at most ℓ, for every t≥2,
E
x∼N(0,Id)[|p(x)|t]1
t≤(t−1)ℓ/2q
E
x∼Nd[p2(x)].
The following is a standard corollary:
Proposition B.22. Ifp:Rd→Ris a polynomial of degree ℓ, then
r
E
x∈N(0,I)[(p(x))2]≤eℓE
x∈N(0,I)[|p(x)|]
Proof. The proof is standard, and is included here for completeness (a completely analogous proof
for the Boolean case can be found in Theorem 9.22 from [ O’D14 ]). Let λ >0be a parameter and let
θ=1
2λ
1+λ. Using Generalized Holder’s inequality and Gaussian Hypercontractivity, we have
r
E
x∈N(0,I)[(p(x))2]≤
E
x∈N(0,I)[|p(x)|]θ
E
x∈N(0,I)[(p(x))2+λ]1−θ
2+λ
≤
≤
E
x∈N(0,I)[|p(x)|]θ 
(1 +λ)ℓ/2r
E
x∈N(0,I)[(p(x))2]!1−θ
Overall, r
E
x∈N(0,I)[(p(x))2]!θ
≤(1 +λ)(1−θ)ℓ/2
E
x∈N(0,I)[|p(x)|]θ
8Note that the expansion below is always possible for a degree ℓpolynomials because polynomials of the
formHi1(x1)Hi2(x2)· · ·Hid(xd)are polynomials of degree at most ℓthat are linearly independent, because
they are orthonormal with respect to the standard d-dimensional Gaussian.
19Taking power 1/θof both sides and recalling that θ=1
2λ
1+λwe get:
r
E
x∈N(0,I)[(p(x))2]≤(1 +λ)(1−θ)
θℓ/2E
x∈N(0,I)[|p(x)|] = (1 + λ)(1
λ−1
2)ℓE
x∈N(0,I)[|p(x)|].
Finally, taking λ→0proves the proposition.
Finally, we are ready to prove Theorem B.16.
Proof of Theorem B.16. Without loss of generality9, we bound the coefficients of pup(x). We have
E
x∈N(0,I)[|pup(x)|]≤E
x∈N(0,I)[|f(x)|] + E
x∈N(0,I)[|pup(x)−f(x)|]≤
≤E
x∈N(0,I)[|f(x)|] + E
x∈N(0,I)[pup(x)−pdown(x)]≤2.
Note that in the last inequality the value of Ex∈N(0,I)[|f(x)|]is bounded by 1because fis{±1}-
valued, and Ex∈N(0,I)[pup(x)−pdown(x)]was bounded by 1by the premise of the theorem. Com-
bining the equation above with Proposition B.22, we get
r
E
x∈N(0,I)[(pup(x))2]≤2·eℓ.
Finally, together with Proposition B.20 implies that each coefficient ofpup
2·eℓis bounded by (2d)ℓin
absolute value. This allows us to conclude that each coefficient of pupis bounded by 2·(10d)ℓin
absolute value.
C Chow Matching Tester
We now focus on functions that have low-degree sandwiching polynomials approximators under the
training distribution.
Definition C.1 (L1-sandwiching polynomials) .Consider X ⊆Rdand a distribution DoverX. For
ϵ >0andf:X → {± 1}, we say that the polynomials pup, pdown :X →Rareϵ-approximate
L1-sandwiching polynomials for funderDif the following are true.
1.pdown(x)≤f(x)≤pup(x), for all x∈ X.
2.Ex∼D[pup(x)−pdown(x)]≤ϵ
We say that the ϵ-approximate L1-sandwiching degree of CunderDis at most ℓand with (coefficient)
bound Bif for any f∈ Cthere are ϵ-approximate L1-sandwiching polynomials pup, pdown forfsuch
thatdeg(pup),deg(pdown)≤ℓand each of the coefficients of pup, pdown are absolutely bounded by
B.
It turns out that given a function class Cwith low degree sandwiching approximators, we can test
localized discrepancy of a hypothesis bfwith respect to a very global notion of neighbourhood: the
entire concept class C! We state the definition here.
Definition C.2 (Global Neighborhood) .The global (H,C)neighborhood is defined as N(bf) =Cfor
allbf∈ H. We denote this by NC.
C.1 Discrepancy Testing Result
We now present our discrepancy tester for concept classes with bounded ϵ-approximate L1sandwich-
ing degree. The primary advantage of this tester is it’s global nature: given a hypothesis bf, it certifies
low localized discrepancy with respect to every function in the concept class.
9This is indeed without loss of generality, because the function −fis bounded from above by −pdown and
from below by −pup.
20Theorem C.3 (Chow Matching Tester) .LetDbe a distribution over a set X ⊆Rd. LetC ⊆ {X →
{±1}}be a concept class. Let ϵ >0, mconc∈N. LetH={±1}X. Assume that the following are
true.
1.(L1-sandwiching) Theϵ
3-approximate L1-sandwiching degree of Cw.r.t.Disℓwith bound
B.
2.(Chow-concentration) For any function bf∈ H, ifX∼ D⊗mwithm≥mconc, then with
probability at least 9/10, we have that for all α∈Ndwith∥α∥1≤ℓ,ED[bf(x)·xα]−
EX[bf(x)·xα]≤ϵ
Bd2ℓ.
Then, there exists a (NC, ϵ)-tester Tfor localized discrepancy from Dwith respect to {D} that uses
mconc+O(1
ϵ2)samples and runs in time poly 
mconc, dℓ,1
ϵ
.
Proof. For an input distribution D′and function bf∈ H , the tester runs Algorithm 1 with mconc
samples XfromD′and function bfas input. We now prove it’s correctness.
Soundness We first consider the case where Taccepts D′. Letf∗= arg maxf∈C 
Px∼D′bf(x)̸=
f(x)
−Px∼Dbf(x)̸=f(x)
. Since Px∼D′[bf(x)̸=f∗(x)] = (1 −ED′[f∗(x)·bf(x)])/2, it
is sufficient to prove a lower bound on the second term. From a Chernoff bound, we have that
ED′[f∗(x)·bf(x)]≥EX[f∗(x)·bf(x)]−ϵwith probability at least 3/4when|X| ≥C/ϵ2for
some universal constant C≥1. We now bound EX[f∗(x)·bf(x)]. Letpup, pdown beϵ-approximate
L1-sandwiching polynomials for f∗underD. We have that
E
X[f∗(x)·bf(x)] =E
X[(f∗(x)−pup(x))·bf(x)] +E
X[pup(x)·bf(x)]
≥E
X[pdown(x)−pup(x)] +E
X[pup(x)·bf(x)]≥E
D[pdown(x)−pup(x)] +E
D[pup(x)·bf(x)]−3ϵ
≥E
D[f∗(x)·bf(x)] +E
D[(pup(x)−f∗(x))·bf(x)]−4ϵ≥E
D[f∗(x)·bf(x)]−5ϵ .
The first inequality follows from the fact that pdown(x)≤f∗(x)≤pup(x). To obtain the second
inequality, we use the fact that the tester accepts if and only if |EX[xα]−ED[xα]|<∆and
|EX[bf(x)·xα]−ED[bf(x)·xα]|<∆for∆ =ϵ
Bd2ℓand all α∈Nsuch that ∥α∥1≤ℓ. Since the
coefficients of pup, pdown are bounded by Band each have at most d2ℓmonomials, we obtain the
second inequality. The last two inequalities use the fact that ED[pup(x)−pdown(x)]≤ϵ.
Thus, we obtain that ED′[f∗(x)·bf(x)]≥ED[f∗(x)·bf(x)]−6ϵwith probability at least 3/4. This
implies that Px∼D′[f∗(x)̸=bf(x)]≤Px∼D[f∗(x)̸=bf(x)] + 3 ϵ. From the definition of f∗, we
therefore have that discbf,NC(D,D′)≤3ϵwith probability at least 3/4when the tester accepts.
Completeness In this case, we have that D′=D. Clearly, from our assumption on Chow
concentration, we have that with probability at least 4/5,|EX[xα]−ED[xα]|<∆and|EX[bf(x)·
xα]−ED[bf(x)·xα]|<∆for∆ =ϵ
Bd2ℓand all α∈Nsuch that ∥α∥1≤ℓ. Thus, with probability
at least 4/5, the tester will accept.
Algorithm 1: Chow Matching Tester
Input: SetXfromD′, function bf:X → {± 1}, parameters ϵ >0,ℓ∈N, B > 0
Set∆ =ϵ
Bd2ℓ
For each α∈Ndwith∥α∥1≤ℓ, compute the quantity bMα=EX[bf(x)·xα].
Accept if|bMα−ED[bf(x)·xα]|<∆and|EX[xα]−ED[xα]|<∆for all αwith∥α∥1≤ℓ.
Reject otherwise.
21C.2 Applications to TDS Learning
In this section we prove that any concept class with L1sandwiching polynomials can be TDS learned.
This improves on the results of Klivans et al. 2023 which proved that L2sandwiching implies TDS
learning. In particular, our result implies a new TDS learning algorithm for the class of all constant
depth circuits( AC0) which was unknown in prior work. We also achieve tight dependence on the
parameter λas compared to prior work which was off by constant factors.
Algorithm 2: TDS learning through Chow matching
Input: SetsStrain fromDtrain
XY,XtestfromDtest
X, Training Algorithm
A, ϵ∈(0,1), ℓ∈N, B > 0
Letbfbe the output of Awhen run on input Strain
Run the Chow matching tester(Algorithm 1) with inputs Xtest,bf, ϵ, ℓ andBwith source
distribution Dtrain
X.
Accept and output bfif the Chow matching tester accepts.
Reject otherwise.
We now state our general theorem about the connection between L1sandwiching and TDS learning.
In contrast to prior work, we completely decouple the training and testing phase of the TDS learner.
Theorem C.4 (L1-sandwiching implies TDS learning) .LetDbe a distribution over a set X ⊆Rd.
LetC ⊆ {X → {± 1}}be a concept class. Let ϵ, δ∈(0,1). LetH={±1}X. Assume that the
following are true.
1.(L1-sandwiching) The ϵ-approximate L1sandwiching degree of CunderDis at most ℓwith
bound B.
2.(Chow-concentration) For any function bf∈ H, ifX∼ D⊗mwithm≥mconc, then with
probability at least 9/10, we have that for all α∈Ndwith∥α∥1≤ℓ,ED[bf(x)·xα]−
EX[bf(x)·xα]≤ϵ
Bd2ℓ.
3.(Agnostic Learning Algorithm) There exists an algorithm Athat takes mtrain samples
fromDtrain
XY, runs in time Ttrain, and outputs w.p. at least 1−δ
2a hypothesis bfsuch that
P(x,y)∼Dtrain
XY[y̸=bf(x)]≤errA.
Then, there exists an algorithm that takes mtrain labelled samples from the training
distribution, O 
(mconc+ 1/ϵ2) log(1 /δ)
unlabelled test samples, runs in time Ttrain +
poly 
mconc, dℓ,1
ϵ,log(1/δ)
and TDS learns Cwith respect to Dup to error λ+ err A+ϵand fails
with probability at most δ.
Proof. LetDtrain
XY be the training distribution with marginal Dtrain
X=Dand let Dtest
XYbe the test
distribution with marginal equal . Let Strain be a set of mtrain samples from Dtrain
XY and let Xtestbe
a set of mconc+ 1/ϵ2samples from Dtest
X. Run Algorithm 2 with inputs Strain, Xtest,A, ϵ, ℓ andB.
We now prove it’s correctness.
Soundness We first consider the case when the input distribution is accepted. This happens when
Dtest
Xis accepted by the Chow Matching tester from Algorithm 1. From Theorem C.3, we have
that with probability at least 3/4,discbf,NC(Dtrain
X,Dtest
X)≤ϵ. This probability can be boosted
to1−δ/2by repeating the Chow matching tester O 
log(1/δ)
times with independent samples
and accepting if and only if a majority of the tests accept. Let f∗= arg minf∈C{err(f;Dtrain
XY) +
err(f;Dtest
XY)}. That is, λ= err( f∗;Dtrain
XY) + err( f∗;Dtest
XY). From Definition C.2 and the fact that
discbf,NC(Dtrain
X,Dtest
X)≤ϵ, we have that
P
x∼Dtest
X[f∗(x)̸=bf(x)]−P
x∼Dtrain
X[f∗(x)̸=bf(x)]≤ϵ (C.1)
22We also have that err(bf;Dtrain
XY)≤errAwith probability at least 1−δ/2from the error guarantee of
A. We are now ready to bound err(bf;Dtest
XY). We have that
err(bf;Dtest
XY)≤err(f∗;Dtest
XY) +P
x∼Dtest
X[f∗(x)̸=bf(x)]
≤err(f∗;Dtest
XY) + P
x∼Dtrain
X[f∗(x)̸=bf(x)] +ϵ
≤err(f∗;Dtest
XY) + P
(x,y)∼Dtrain
XY[f∗(x)̸=y] + P
(x,y)∼Dtrain
XY[bf(x)̸=y]
≤err(f∗;Dtrain
XY) + err( f∗;Dtest
XY) + err A≤λ+ err A+ϵ .
The first and third inequalities follow from the triangle inequality. The second inequality follows
from Equation (C.1). The penultimate inequality follows from the error guarantee of A. The last
inequality follows from the definition of λ.
Completeness This follows immediately from the completeness guarantee of Theorem C.3. As
seen before, the success probability can be boosted to 1−δ/2. Thus, the tester accepts when
Dtest
X=Dtrain
X with probability at least 1−δ/2.
Remark C.5.The above theorem completely decouples training and testing. This is in contrast to the
Klivans et al. 2023 which don’t make this distinction. In particular, this forces their output hypothesis
to be polynomial threshold function. In our theorem, the hypothesis can be any function output by
the training algorithm Athat achieves low error. This is also in contrast with the other TDS learning
algorithms in this paper that require additional structure from the hypothesis output by the training
algorithm.
In fact, we can drop Assumption 3 from Theorem C.4 entirely, if we restrict our training algorithm.
In particular, we use the following theorem from [KKMS08].
Theorem C.6 (Theorem 5 from [ KKMS08 ]).LetDbe a distribution on X × {± 1}forX ⊆Rd
with marginal DX. Let ϵ, δ∈(0,1). LetCbe a class of functions such that for all f∈ C, there
exists polynomials pof degree ℓsuch that Ex∼Dx[|f(x)−p(x)|]≤ϵ. Then there exists an agnostic
learning algorithm Athat has run time and sample complexity at most poly( dℓ,1/ϵ,log(1/δ))that
outputs a hypothesis bfsuch that with probability at least 1−δ, we have that
P
(x,y)∼D[y̸=bf(x)]≤inf
f∈CP
(x,y)∼D[f(x)̸=y]
Armed with this, we give our end to end result that L1sandwiching implies TDS learning.
Theorem C.7 (L1-sandwiching implies TDS learning) .LetDbe a distribution over a set X ⊆Rd.
LetC ⊆ {X → {± 1}}be a concept class. Let ϵ, δ∈(0,1). LetH={±1}X. Assume that the
following are true.
1.(L1-sandwiching) The ϵ-approximate L1sandwiching degree of CunderDis at most ℓwith
bound B.
2.(Chow-concentration) For any function bf∈ H, ifX∼ D⊗mwithm≥mconc, then with
probability at least 9/10, we have that for all α∈Ndwith∥α∥1≤ℓ,ED[bf(x)·xα]−
EX[bf(x)·xα]≤ϵ
Bd2ℓ.
Then, there exists an algorithm that takes poly( dℓ,1/ϵ)labelled samples from the training distribution,
O 
(mconc+ 1/ϵ2)·log(1/δ)
unlabelled test samples, runs in time poly 
mconc, dℓ,1
ϵ,log(1/δ)
and TDS learns Cwith respect to Dup to error λ+opttrain+ϵand fails with probability at most δ.
Proof. Observe that L1sandwiching polynomials are also L1approximating polynomials. Thus, C
satisfies the requirements of Theorem C.6. Thus, we can run Algorithm 2 with Ainstantiated to be
the algorithm from Theorem C.6. The proof of correctness follows from Algorithm 2.
We now argue that when Dtrain
X∈ {Unif{±1}d,Nd}, then we have that Assumption 2 of Theo-
rem C.4 is always true with mconc≤poly( dℓB/ϵ).
23Lemma C.8. LetD ∈ { Unif{±1}d,Nd}. Letfbe a function taking values in {±1}. Letℓ∈N. Let
X∼ D⊗mconcformconc≥poly( dℓ/ϵ). Then, with probability atleast 9/10overS, we have that
for all α∈Ndwith∥α∥1≤ℓ,
E
D[f(x)·xα]−E
X[f(x)·xα]≤ϵ.
Proof. Forα∈Nd, letbZ=EX[f(x)·xα]be the empirical mean over the samples. Let Z=
ED[f(x)·xα]be the true mean. Clearly, EX[bZ] =Z. Thus, we have that PX[|bZ−Z| ≥ϵ]≤VarX[bZ]
ϵ2.
We have that VarX[bZ]≤1
mconcVar[f(x)·xα]. We have that Var[f(x)·xα]≤ED[x2α]from the
fact that ftakes values in {±1}. When D= Unif {±1}d,x2α= 1. When D=Nd, we have that
ED[x2α]≤poly( dℓ)(see Proposition 2.5.2 [ Ver18 ]). Thus, Thus, choosing mconc= poly( dℓ/ϵ), we
have that PX[|bZ−Z| ≥ϵ]≤ϵ
dΩ(ℓ). Taking a union bound over all α∈Ndcompletes the proof.
Applying Theorem C.7, Lemma C.8 and the bounds on the sandwiching degrees(Lemmas B.13,
B.14 and B.17) from Appendix B.2, we immediately get the following results on TDS learning as
corollaries.
Corollary C.9 (TDS learning for degree 2PTFs with respect to Unif{±1}dorNd).LetCbe the
class of degree- 2PTFs. Let ϵ >0andℓ=˜O(1/ϵ9). Then, there exists an algorithm that runs in time
dO(ℓ)and TDS learning Cwith respect to Unif{±1}dorNdwith error at most opttrain+λ+ϵ.
Corollary C.10 (TDS learning for depth- tAC0).LetCbe the class of depth- tAC0circuits of size s
on{±1}d. Letϵ >0andℓ= (log s)O(t)log(1/ϵ). Then, there exists an algorithm that runs in time
dO(ℓ)and TDS learning Cwith respect to Unif{±1}dwith error at most opttrain+λ+ϵ.
D Cylindrical Grids Tester
We focus on functions whose values only depend on the projection of the input on some low-
dimensional subspace, i.e., we focus on the class of subspace juntas, which is formally defined as
follows.
Definition D.1 (Subspace Junta) .We say that a function f:Rd→ {± 1}is ak-subspace junta if
there exists W∈Rk×dwith∥W∥2= 1 andWW⊤=Ikas well as a function F:Rk→ {± 1}
such that
f(x) =fW(x) =F(Wx)for any x∈Rd
Since such functions only depend on a low-dimensional subspace, one might hope to exploit this
property to obtain more efficient discrepancy testers. However, the relevant subspaces of different
subspace juntas can be completely different and the low dimensional structure of a class of subspace
juntas does not seem enough to provide significant improvements for global discrepancy testing.
Nevertheless, it turns out that testing the localized discrepancy with respect to a notion of subspace
neighborhood can be benefited by the low-dimensional structure. In particular, we define the notion
of subspace neighborhood as follows.
Definition D.2 (Subspace Neighborhood) .LetHbe the class of k-subspace juntas (see Definition D.1)
andCbe some concept class. We define the (γs, γe)-subspace neighborhood Ns:H → Pow(C)as
follows for any bf=bfV∈ H.
Ns(bfV) ={fW∈ C |∥ W−V∥2≤γsandP
x∼N[f(x)̸=bf(x)]≤γe}
To design efficient testers for localized discrepancy in terms of the subspace neighborhood, we also
use the notion of boundary of concepts and we require the boundaries to be smooth, meaning that the
measure of the region close to the boundaries scales proportionally to its thickness. Formally, we
provide the following definitions.
Definition D.3 (Boundary of Concept) .LetF:Rk→ {± 1}some concept. For ϱ≥0, we denote
∂ϱFtheϱ-boundary of F, i.e., the region {x∈Rk:∃z∈Rkwith∥z∥2≤ϱandF(x+z)̸=F(x)}.
24Definition D.4 (Smooth Boundary) .LetF:Rk→ {± 1}. For σ≥1, we say that Fhasσ-smooth
boundary with respect to Nkif for any ϱ≥0
P
x∼Nk[x∈∂ϱF] :=P
x∼Nk[∃z:∥z∥2≤ϱ, F(x+z)̸=F(x)]≤σϱ
As we will show shortly, the choice of the subspace neighborhood not only enables obtaining faster
localized discrepancy testers, but also testers that are guaranteed to accept much wider classes of
distributions. This is because the properties of the test marginal that need to be tested in order to
ensure low localized discrepancy are much simpler, compared to the properties required for global
discrepancy. Such properties are not only easy to test, but are also satisfied by more distributions. The
structural properties we will require for the completeness criteria of our algorithms are concentration
in every direction and anti-concentration of low-dimensional marginals. More formally, we consider
structured distributions to be as follows.
Definition D.5 (Structured Distributions) .Forµc:N→R+,µac:R+→R+,k, d∈Nwithk≤d,
we say that the distribution DoverRdis(µc, µac)-structured on k-dimensions (w.r.t. Nk), if the
following are true.
1. (Concentration) For any v∈Sd−1andp∈N, we have Ex∼D′[(v·x)2p]≤µc(p).
2.(Anti-concentration) For any subspace Uof dimension k, ifQis the density of the marginal
ofDonUwe haveQ(x)
Nk(x)≤µac(R)for any x∈Rkwith∥x∥2≤R.
Moreover, if k=d, we simply say that Dis(µc, µac)-structured.
Remark D.6.We note that the two conditions of Definition D.5 are not always independent. For
example, if µac(R) =O(1), then the distribution Qof condition 2 is subgaussian, which implies
a bound on µc(p)for all p∈N(i.e., implies some version of condition 1). However, the anti-
concentration condition does not always imply the concentration condition (e.g., if µac(R) =
Θ(eR2/2)) and both conditions are important.
For example, isotropic log-concave distributions are structured on k-dimensions with µc(p)≤
(O(p))2pandµac(R) = (O(k))kexp(R2
2).
D.1 Discrepancy Testing Result
We now provide our main localized discrepancy testing result for subspace juntas with smooth
boundaries, where we use some free parameters R, p that can be chosen according to how structured
the target accepted class of distribution is.
Theorem D.7 (Discrepancy Testing through Cylindrical Grids) .Letµc:N→R≥1,µac:R+→
R≥1,p∈N,R, σ,bσ≥1andγs, γe∈(0,1). Let also H(resp. C) be a class whose elements are
k-subspace juntas over Rdwithbσ-smooth (resp. σ-smooth) boundaries. Consider Dto be the class
of distributions over Rdthat are (µc, µac)-structured on k-dimensions and Ns:H → Pow(C)the
(γs, γe)-subspace neighborhood. For any ϵ∈(0,1), there is a (Ns, ψ+ϵ)-tester (Algorithm 3)
for localized discrepancy from Ndwith respect to Dwith sample complexity m=10µc(2)
(µc(1))2d4+
12R2p
kµc(p)+14k(√
2πexp(R2))k
µac(R√
k)ηkln(3R
η) +O(1
ϵ2)and time complexity O(md3+mdk(2⌈R
η⌉)k), where
η=γsRp
2bσ√
kp
µc(1)/µc(p)and the error parameter ψis
ψ=14kµc(p)
R2p+ 122kR2pµc(1) ln µac(R√
k)
µc(p)1
2µac(R√
k)σγs+ 2µac(R√
k)γe
For different target distribution classes we obtain different results, that reveal a trade-off between
universality and the size of the subspace neighborhood tested. To accept wider classes of distribu-
tions, we restrict to testing localized discrepancy with respect to narrower neighborhoods, which is
parameterized by γsandγein the following corollary. Eventually, for applications in TDS learning,
this will result into requiring the training algorithm to provide stronger error guarantees by using
more training examples and time.
25Algorithm 3: Cylindrical Grids Tester
Input: SetXof points in Rd, matrix V∈Rk×d, parameters p∈N, R≥1, η > 0
Compute the matrix M=Ex∼X[xx⊤]andreject if the largest eigenvalue is larger than 2µc(1).
Compute the quantity Px∼X[∥Vx∥∞> R]andreject if the value is larger than2kµc(p)
R2p.
LetI={−⌈R
η⌉, . . . ,−1,0, . . . ,⌈R
η⌉ −1}and consider the grid
Gη,R={[i1η,(i1+ 1)η]× ··· × [ikη,(ik+ 1)η] :i1, . . . , i k∈ I}
foreach grid cell G∈ Gη,Rdo
Compute the quantity Px∼X[Vx∈G]andreject if the value is larger than
2µac(R√
k)Px∼N[Vx∈G].
end
Otherwise, accept .
Corollary D.8. Letϵ∈(0,1), letH,C, σ,bσbe as in Theorem D.7 and let Ns:H → Pow(C)be the
(γs, γe)-subspace neighborhood (on kdimensions). For a class of distributions DoverRd, there is a
(Ns, ϵ)-tester for localized discrepancy from Ndwith respect to Din each of the following cases for
appropriately large universal constants C1, C2≥1.
1.D={Nd},σγs≤γe≤(ϵ
C1k)C2. The tester has time and sample complexity
poly( d)(k
ϵ)O(k)(σbσ)k.
2.Dis the class of C-subgaussian and isotropic log-concave measures over Rdfor some
C=O(1)andσγs≤γe≤(ϵ
C1)C2k. The tester has time and sample complexity
poly( d)(k
ϵ)O(k2)(σbσ)k.
3.Dis the class of isotropic log-concave measures over Rdand also σγs≤
γe≤ (1
C1)−C2k2log2(1/ϵ). The tester has time and sample complexity
poly( d)kO(k3log2(1/ϵ))(σbσ)k.
4.Dis the class of distributions over Rdthat are (µc, µac)-structured on k-dimensions, with
µc(2)≤Candµac(R)≤Ck2eR2/2for some C=O(1)andσγs≤γe≤(1
C1)−C2k2/ϵ.
The tester has time and sample complexity poly( d)kO(k3/ϵ2)(σbσ)k.
Proof. To apply Theorem D.7 in each case, it suffices to show bounds for µc(p)andµac(R√
k)for
each of the choices for D. We then pick p= log(1 /ϵ)in Cases 1,2 and 3 and p= 1in Case 4 and R
sufficiently small to achieve error guarantee ϵ. For Case 1, µc(p)≤(Cp)pandµac(R√
k)≤1. For
case 2, µc(p)≤(2Cp)pandµac(R√
k)≤(Ck)kekR2/2. Finally, for Case 3, µc(p)≤(Cp)2pand
µac(R√
k)≤(Ck)kekR2/2. These bounds follow from properties of log-concave and subgaussian
distributions (see, e.g., [LV07, Ver18]).
In order to prove Theorem D.7, we first provide a tester which can certify that the mass assigned
by the tested distribution to the region near the boundary of any function with smooth boundary is
bounded. Structured distributions (Definition D.5) indeed have this property and the proposed tester
can certify it universally over the class of such distributions.
This can be done by considering a cover the low-dimensional space by a grid of bounded size and
checking whether the probability of falling within each of the grid cells is appropriately bounded. To
account for grid cells that are far from the origin, it suffices to check that the tested distribution is
sufficiently concentrated. If these tests pass, then we have a certificate that the mass of the tested
distribution close to the boundary of any smooth function is appropriately bounded, because such
regions can be covered by the union of a relatively small number of grid cells (see Figure 2).
Lemma D.9 (Grids Tester) .Letµc:N→R+,µac:R+→R+,p∈N,R, σ≥1andϱ∈(0,1).
There is a tester Twhich, upon receiving a set Xof vectors in Rk, and in time |X| ·(O(R√
k
ϱ))k,
either accepts or rejects and satisfies the following.
26(a)(Soundness.) If Taccepts, then for any F:Rk→ {± 1}withσ-smooth boundary we have
P
x∼X[x∈∂ϱF]≤2kµc(p)
R2p+ 4σϱ µ ac(R√
k)
(b)(Completeness.) If Xconsists of at least12R2p
kµc(p)+14k(3√
2πkexp(R2))k
µac(R√
k)ϱk ln(9Rk
ϱ)i.i.d. exam-
ples from some (µc, µac)-structured distribution over Rk, thenTaccepts with probability
at least 99%.
Proof. Letη=ϱ
3√
kbe some parameter, I={−⌈R
η⌉, . . . ,−1,0, . . . ,⌈R
η⌉ −1}be a set of indices
andGη,R={[i1η,(i1+ 1)η]× ··· × [ikη,(ik+ 1)η] :i1, . . . , i k∈ I} the corresponding finite grid
with cell length η(each cell corresponds to a hypercube in Rk, the cartesian product of kintervals
each of length η). The tester does the following.
1.Computes the quantity Px∼X[∥x∥∞> R]and rejects if the computed value is larger than
2kµc(p)
R2p.
2.For each cell Gin the grid Gη,R, computes the quantity Px∼X[x∈G]and rejects if the
computed value is Px∼X[x∈G]>2µac(R√
k)Px∼Nk[x∈G].
3. Otherwise, the tester accepts.
Soundness. Suppose that the tester Thas accepted. This means that the quantities Px∼X[∥x∥∞>
R]andPx∼X[x∈G]are appropriately bounded (for any G∈ Gη,R). Let Fbe any function with
σ-smooth boundary with respect to Nk.
Consider ˜G ⊆ G η,Rto be the set of grid cells that have non-empty intersection with the set ∂ϱF
(see Definition D.3), i.e., ˜G:={G∈ Gη,R:G∩∂ϱF̸=∅}. Observe that if x∈∂ϱFthen
either∥x∥∞> R , orx∈Gfor some G∈˜G, because the grid covers the set {x:∥x∥∞≤R}.
Moreover, if x∈˜G, then there is a point y∈˜G ∩∂ϱFthat falls in the same cell as xand, therefore,
∥x−y∥2≤η√
k, because each cell has length η. This implies that x∈∂ϱ+η√
kF. We overall have
the following (see also Figure 2).
∂ϱF\ {x:∥x∥∞> R} ⊆[
G∈˜GG⊆∂˜ϱF ,where ˜ϱ:=ϱ+η√
k (D.1)
Figure 2: Discretization of smooth boundary
27Combining the first inclusion in expression (D.1) with the fact that the tester has accepted, the quantity
Px∼X[x∈∂ϱF]is bounded as follows.
P
x∼X[x∈∂ϱF]≤P
x∼X[∥x∥∞> R] +X
G∈˜GP
x∼X[x∈G]
≤2kµc(p)
R2p+ 2µac(R√
k)X
G∈˜GP
x∼Nk[x∈G]
For any G, G′∈˜GwithG̸=G′, the events that x∈Gand that x∈G′are mutually exclusive.
ThereforeP
G∈˜GPx∼Nk[x∈G] =Px∼Nk[x∈ ∪G∈˜GG]≤Px∼Nk[x∈∂˜ϱF], where the final
inequality follows from the second inclusion in expression (D.1) . Since Fhasσ-smooth boundary,
we have Px∼Nk[x∈∂˜ϱF]≤σ˜ϱ. Overall, we have
P
x∼X[x∈∂ϱF]≤2kµc(p)
R2p+ 2σ(ϱ+η√
k)µac(R√
k)
≤2kµc(p)
R2p+ 4σϱ µ ac(R√
k),as desired.
Completeness. Suppose, now, that the examples Xare drawn independently from a (µc, µac)-
structured distribution Q. We first show that, with probability at least 1−1
200, we have Px∼X[∥x∥∞>
R]≤2kµc(p)
R2p.
We first bound the quantity Px∼Q[∥x∥∞> R], by using Markov’s inequality as follows.
P
x∼Q[∥x∥∞> R]≤ksup
v∈Sk−1P
x∼Q[|v·x|> R]
≤ksupv∈Sk−1Ex∼Q[(v·x)2p]
R2p
≤kµc(p)
R2p,sinceQis structured.
By the multiplicative Chernoff bound10, we have that Px∼X[∥x∥∞> R]≤2Px∼Q[∥x∥∞> R]
with probability at least 1−exp(−|X|kµc(p)
2R2p)≥1−1
200, since |X| ≥12R2p
kµc(p).
We will show that for each G∈ Gη,R,Px∼X[x∈G]≤2µac(R√
k)Px∼Nk[x∈G], with probability
at least 1−exp(−|X|
2µac(R√
k)ηk/(√
2πeR2)k). The desired result then follows by a union bound
overGη,R(where |Gη,R| ≤(3R/η)k) and the fact that |X| ≥14k(√
2πexp(R2))k
µac(R√
k)ηkln(3R
η).
We first bound Px∼Q[x∈G]as follows by using the fact that Qis structured and ∥x∥2≤ ∥x∥∞√
k≤
R√
kfor all x∈G(because G∈ Gη,R).
P
x∼Q[x∈G] =Z
x∈GQ(x)dx≤µac(R√
k)Z
x∈GN(x)dx=µac(R√
k)P
x∼N[x∈G]
By the multiplicative Chernoff bound, we once more have that Px∼X[x∈G]≤2Px∼Q[x∈G]with
probability at least 1−exp(−|X|
2µac(R√
k)Px∼N[x∈G])and conclude the proof by observing
thatPx∼N[x∈G]≥(η√
2πexp(R2))k.
Remark D.10 .We note that Lemma D.9 is not specialized to the Gaussian distribution. The only
requirement is that the distribution of the completeness criterion is structured with respect to the same
distribution for which the functions Fof the soundness criterion have smooth boundary. In particular,
in Definition D.5, the anti-concentration condition 2 is defined with respect to the Gaussian, but it
could also be defined with respect to some other distribution. The concentration condition 1 is always
the same.
10We use the version of the Chernoff bound that uses an upper bound on the expectation rather than the exact
value, through a standard coupling argument.
28We are now ready to prove Theorem D.7. The idea is that if a function flies within the subspace
neighborhood of another function bf, then the disagreement region between the two functions is
bounded by the union of: (1) their disagreement after projecting on the relevant subspace for bf(since
the subspace is known, it can be tested exhaustively, similarly to Lemma D.9) and (2) the region far
from the origin (for which testing concentration suffices).
Proof of Theorem D.7. LetD′be the unknown distribution and Xa set of mi.i.d. samples from D′
and let η=γsRp
2bσ√
kq
µc(1)
µc(p). Let (bfV, X)be an instance of the localized discepancy testing problem
(see Definition 1.1). We run Algorithm 3 with input (X, V, p, R, η )and accept (or reject) accordingly.
Soundness. Suppose that the algorithm accepts. We will show that Px∼X[bf(x)̸=f(x)]≤ψfor
anyf∈Ns(bf). Since the event that bf(x)̸=f(x)is independent for each x∈X, we may apply the
Hoeffding bound to show that Px∼D′[bf(x)̸=f(x)]≤ψ+ϵwith probability at least 3/4whenever
|X| ≥3
ϵ2. To bound the empirical quantity, we have the following, for Rs=Rp(µc(1)/µc(p))1/2
andϱ=γsRs
bσ.
P
x∼X[F(Wx)̸=bF(Vx)]≤P
x∼X[F(Wx)̸=F(Vx)]
| {z }
P1+P
x∼X[F(Vx)̸=bF(Vx)]
| {z }
P2
For the term P1, we observe that F(Wx) =F((W−V)x+Vx)and therefore
P1≤P
x∼X[∥(W−V)x∥2≥γsRs] +P
x∼X[∃z∈Rk:∥z∥2≤γsRs, F(Vx+z)̸=F(Vx)]
=P
x∼X[∥(W−V)x∥2≥γsRs] +P
x∼X[Vx∈∂γsRsF]
By applying Chebyshev’s inequality for the first term in the above expression and Lemma D.9 for the
second term (note that we have chosen η≤γsRs
3√
kand Algorithm 3 runs the tester corresponding to
Lemma D.9), we obtain the following bound for P1(recall that ∥W−V∥2≤γsand∥(W−V)x∥2≤
∥W−V∥2∥projUx∥2, where Uis the span of the columns of the matrix W−V).
P1≤ksupv∈Sd−1Ex∼X[(v·x)2]
R2s+2kµc(p)
R2p+ 4σγsRsµac(R√
k)
≤2kµc(1)
R2s+2kµc(p)
R2p+ 4σγsRsµac(R√
k)
The last inequality follows from the spectral bound on the empirical covariance matrix M=
Ex∼X[xx⊤]implied by Algorithm 3 upon acceptance.
For the term P2, consider the set of grid cells ˜Gwith non-zero intersection with the disagreement
region, i.e., ˜G={G∈ G η,R:there is xwithVx∈GandF(Vx)̸=bF(Vx)}. Recall that
ϱ=η√
kand let ˜Ginbe the interior part of ˜G, i.e., ˜Gin={G∈˜G:for any xwithVx∈
Gwe have F(Vx)̸=bF(Vx)}}.
Letxbe such that ∥Vx∥∞≤R,F(Vx)̸=bF(Vx)andVx/∈∂ϱF∪∂ϱbF. It must be that Vxlies
within some grid cell in ˜Gin. To see this, note that Vxmust be in exactly one grid cell Gin˜G(by
definition of ˜G) and if this grid cell was in ˜G \˜Gin, this would imply that for some x′withVx′∈G
we would have either F(Vx)̸=F(Vx′)orbF(Vx)̸=bF(Vx′)(because F,bFdisagree on Vxbut
agree on Vx′). However, ∥Vx−Vx′∥2≤η√
k=ϱ, because they are in the same grid cell and we
conclude that Vx∈∂ϱF∪∂ϱbF, which is a contradiction. Overall, we have the following.
P2≤P
x∼X[∥Vx∥∞> R]
| {z }
P21+P
x∼X[Vx∈∂ϱF]
|{z }
P22+P
x∼X[Vx∈∂ϱbF]
|{z }
P23+X
G∈˜GinP
x∼X[Vx∈G]
| {z }
P24
29For the term P21, we use the bound implied by Algorithm 3, for the terms P22, P23we apply
Lemma D.9 and for the term P24, we use the fact that (upon acceptance) Px∼X[Vx∈G]≤
2µac(R√
k)Px∼N[Vx∈G]to obtain the following.
P24≤2µac(R√
k)X
G∈˜GinP
x∼N[Vx∈G]
≤2µac(R√
k)P
x∼N[F(Vx)̸=bF(Vx)]
We bound the quantity Px∼N[F(Vx)̸=bF(Vx)]as follows.
P
x∼N[F(Vx)̸=bF(Vx)]≤P
x∼N[F(Wx)̸=bF(Vx)] +P
x∼N[F(Wx)̸=F(Vx)]
≤γe+P
x∼N[∥(W−V)x∥2> γsR′] +P
x∼N[Vx∈∂γsR′F]
≤γe+ 4ke−R′2
2k+σγsR′
where the last inequality follows from Gaussian concentration and the fact that Fhasσ-smooth
boundary. By choosing R′= (2kln(R2pµac(R√
k)
µc(p)))1/2, we obtain that
P24≤2µac(R√
k)γe+4kµc(p)
R2p+ 2σγsµac(R√
k)
2klnR2pµac(R√
k)
µc(p)1/2
Overall, for the term P2we have the following bound.
P2≤10kµc(p)
R2p+ 10σγsRps
2kµc(1)
µc(p)µac(R√
k)(lnµac(R√
k))1/2+ 2γeµac(R√
k)
Combining the bounds for P1andP2, we obtain the desired result.
Completeness. Suppose, now, that D′∈D. It suffices to show that all the tests will accept
with probability at least 3/4. For the quantity Px∼X[∥Vx∥∞> R]as well as the quantities
Px∼X[Vx∈G], we apply the Chernoff Bound as described in the proof of completeness of the grid
tester (see the proof of Lemma D.9). For the quantity M=Ex∼X[xx⊤], we use the Chebyshev’s
inequality on each of the random variables Mij=Ex∼X[xixj], the fact that E[M2
ij]≤µc(2)and a
union bound over i, j∈[d].
D.2 Application to TDS Learning
Interestingly, in learning theory, there are algorithms that are guaranteed to recover the relevant
subspace for certain classes of subspace juntas that have some additional properties. This enables us
to use the discrepancy tester of Theorem D.7 to obtain end-to-end results for TDS learning, because
the training phase can guarantee that the ground truth lies within the subspace neighborhood of the
output hypothesis bf, for which we have efficient localized discrepancy testers. Here, we present
a TDS learning result for balanced convex subspace juntas in the realizable setting. The class of
balanced convex subspace juntas is defined as follows.
Definition D.11 (Balanced Convex Subspace Juntas) .A concept f:Rd→ {± 1}is aβ-balanced
convex k-subspace junta if it is β-balanced (see Definition A.1), convex and a k-subspace junta (see
Definition D.1).
We make use of known algorithms from PAC learning that are guaranteed to approximately recover
the effective ground-truth subspace in terms of geometric distance, which is important since the tester
of Theorem D.7 works with respect to the subspace neighborhood and obtain the following theorem,
which underlines a trade-off between training time and universality.
Theorem D.12 (TDS Learning of Convex Subspace Juntas) .Forβ∈(0,1/2),d, k∈N, letCbe the
class of β-balanced convex k-subspace juntas over Rd. For any ϵ∈(0,1), there is a (decoupled)
ϵ-TDS learner for Cwith respect to Ndin the realizable setting, which, for the learning phase, uses
poly( d)(1
β)poly( k/ϵ)samples and time and, for the testing phase, uses poly( d)(k/ϵ)O(k)samples
and time. Moreover, in the same setting, there is a D-universal ϵ-TDS learner for Cfor each of the
cases listed in Table 2.
30ClassDoverRdTraining Time and Samples Testing Time and Samples
11-subgaussian &
Isotropic Log-Concavepoly( d)(1
β)poly(1 /ϵk)poly( d)(k/ϵ)O(k2)
2 Isotropic Log-Concave poly( d)(1
β)2O(k2log2(1/ϵ))poly( d)kO(k3log2(1/ϵ))
3Fourth Moments Bound:
E[(v·x)4]≤C∥v∥4
2&
Dimension- kMarginals
Density Bound: Ck2poly( d)(1
β)2O(k2/ϵ)poly( d)kO(k3/ϵ2)
Table 2: Specifications for D-universal (ϵ, δ)-TDS learning of β-balanced convex k-subspace juntas.
The properties that define the class Din line 3, hold for some given universal constant C≥1, for all
members of D, for all v∈Rdand the density bound holds for any projection on some k-dimensional
subspace of any member of D.
In order to obtain a TDS learner for some class C, one might hope to learn a hypothesis bfduring the
training phase, such that the subspace neighborhood of bf(see Definition D.2) contains the ground
truth. Then, the test error can be bounded simply by running the localized discrepancy tester of
Theorem D.7, assuming that both bfand the class Chave smooth boundaries. In Appendix B.1,
we show that, indeed, convex subspace juntas have smooth boundaries. However, for the learning
guarantee, prior work in standard PAC learning implicitly provides the following weaker guarantee
regarding subspace retrieval for convex subspace juntas, which, as we show, is, nevertheless, still
sufficient for our purposes.
Theorem D.13 (Implicit in [ Vem10a ], see also [ KSV24a ]).For any γ∈(0,1),β∈(0,1/2), there is
an algorithm that, upon receiving a number of i.i.d. examples from Nd, labeled by some β-balanced
convex k-subspace junta f∗(x) =F∗(W∗x), runs in time poly( d)(1
β)poly( k/γ)and returns, w.p. at
least 0.99, some polynomial bq:Rk→ {± 1}of degree at most poly( k/γ)and some V∈Rk×d
withV V⊤=Iksuch that the following are true for the hypothesis bf(x) = sign( bq(Vx))and some
f(x) =F∗(Wx)withWW⊤=Ik.
(a)f∈Ns(bf), where Nsis the k-dimensional (γ, γ)-subspace neighborhood, i.e., ∥W−
V∥2≤γandPx∼Nd[f(x)̸=bf(x)]≤γ.
(b)For any x∈Rdwith∥W∗x∥2≤p
k/γ, we have f(x) =f∗(x).
We are now ready to prove Theorem D.12.
Proof of Theorem D.12. Our plan is to combine Theorem D.7 with Theorem D.13. We will use an
additional test, to account for the fact that Theorem D.13 does not provide exact subspace recovery,
but, rather, recovery of the effectively relevant subspace (see Item (b)).
Suppose that the training distribution Dtrain
XY has marginal Dtrain
X=Ndand that the labels (both in
training and in test distribution Dtest
XYas well) are generated by some β-balanced convex k-subspace
junta f∗:Rd→ {± 1}, where f∗(x) =F∗(W∗x)for some W∗∈Rk×dwithW∗W∗⊤=Ik.
Learning Phase. The learner runs the algorithm of Theorem D.13 for γchosen so that the error
parameter ϵ′(γ)of Theorem D.7 is at most ϵ′≤ϵ/3using labeled examples from Dtrain
XY and
computes bf(x) = sign( bq(Vx))with the corresponding specifications. For the particular choice of γ,
see Corollary D.8, where σ= poly( k)according to Lemma B.8.
Testing Phase. The tester first computes the maximum eigenvalue of the matrix Ex∼Xtest[xx⊤]
using samples Xtestdrawn from Dtest
Xand rejects if the quantity is larger than 2. Then, the tester
runs the localized discrepancy tester of Theorem D.7 and rejects or accepts accordingly.
31Testing Run-Time. To bound the testing run-time we use Corollary D.8, where σ= poly( k)
(because Cis the class of convex subspace juntas and due to Lemma B.8) and bσ= poly( k/γ),
because bfis a polynomial threshold function of degree poly( k/γ)and, therefore, has poly( k/γ)-
smooth boundary according to Lemma B.1.
Soundness. If the tester accepts and |Xtest| ≥poly(1 /ϵ), then we have Px∼Dtest
X[∥W∗x∥2>p
k/γ]≤Px∼Xtest[∥W∗x∥2>p
k/γ] +ϵ/6(by the Hoeffding bound) and Px∼Xtest[∥W∗x∥2>p
k/γ]≤2γ≤ϵ/6forγ≤ϵ/12. Hence, overall, by combining Theorem D.13 with the guarantees
from the fact that the testing phase has accepted, we have
err(bf;Dtest
XY) = P
x∼Dtest
XY[f∗(x)̸=bf(x)]
≤P
x∼Dtest
X[∥W∗x∥2>p
k/γ] +P
x∼Dtest
X[bf(x)̸=f(x)]
≤ϵ
3+P
x∼Nd[bf(x)̸=f(x)] +ϵ
3
≤2ϵ
3+γ≤ϵ ,
where we used the soundness property of the cylindrical grids tester (Theorem D.7 and Corollary D.8)
and the fact that fis a hypothesis with the properties specified in Theorem D.13 and, in particular,
lies within the subspace neighborhood of bf.
Completeness. Combine the completeness guarantee of Theorem D.7 and the fact that
Ex∼Xtest[xx⊤]has, with probability at least 0.99, bounded maximum eigenvalue whenever Dtest
X
lies within D(for any Din Table 2) and |Stest| ≥poly( d).
E Testing Boundary Proximity
We now focus on classes of low-dimensional concepts (see Definition D.1) that are locally structured.
In particular, we consider subspace juntas that are locally balanced, meaning that near any point xin
the domain, there are several points with the same label as x. This condition is important to ensure
that there are, for example, no zero measure regions over the (Gaussian) training distribution that
contain significant information about the ground truth. We will show that this condition actually
enables significant improvements for the testing runtime for TDS learning. More formally, we give
the following definition.
Definition E.1 (Locally Balanced Concepts) .ForR≥1andr, β∈(0,1), we say that a function
F:Rk→ {± 1}is(R, r)-locally β-balanced if for any ϱ≤randx∈Rkwith∥x∥2≤R, the
following is true.
P
z∼Nk[F(z) =F(x)|z∈Bk(x, ϱ)]> β
For a subspace junta f(x) =F(Wx), we say that fis(R, r)-locally β-balanced on the relevant
subspace if Fis(R, r)-locally β-balanced.
For locally balanced concepts, it is possible to obtain efficient localized discrepancy testers with
respect to the disagreement neighborhood, i.e., the neighborhood of concepts that have low disagree-
ment with the reference hypothesis bfunder the Gaussian distribution (or, in general, the reference
distribution at hand).
Definition E.2 (Disagreement Neighborhood) .LetHandCbe some concept classes. We define the
(Gaussian) γe-disagreement neighborhood Ne:H → Pow(C)as follows for any bf∈ H.
Ne(bf) ={f∈ C | P
x∼N[f(x)̸=bf(x)]≤γe}
We also define the boundary proximity tester, which directly tests whether the probability of falling
close to the boundary of some reference hypothesis bfis appropriately bounded. This testing problem
can be solved efficiently, for example, for the fundamental class of halfspace intersections.
32Definition E.3 (Boundary Proximity Tester) .Forbσ≥1,ϱ∈(0,1), letHbe some class of functions
fromRdto{±1}and let Dbe some class of distributions over Rd. The tester Tis called a (ϱ,bσ)-
boundary proximity tester for Hwith respect to Dif, upon receiving some bf∈ H and a set Xof
points in Rd, the tester either accepts or rejects and satisfies the following.
(a) (Soundness.) If Taccepts, then Px∼X[x∈∂ϱbf]≤bσϱ.
(b)(Completeness.) If Xconsists of (at least) mTi.i.d. examples from some distribution in D,
then the tester Taccepts with probability at least 99%.
Note that the complexity of boundary proximity testing depends on the simplicity of bfand, therefore,
considering applications in TDS learning, where bfis the output of the learning algorithm, highlights
the importance of proper learning algorithms that output some simple hypothesis with low error. Since
the hypothesis is simple, disagreement-localized discrepancy testing is tractable and since its error
is low, the ground truth is likely within the disagreement neighborhood and disagreement-localized
discrepancy testing suffices to guarantee low test error.
E.1 Discrepancy Testing Result
In order to obtain a localized discrepancy tester assuming access to a boundary proximity tester, we
first show a simple proposition connecting local balance condition with boundary proximity testing.
In particular, if two functions have low Gaussian disagreement, but one of them is locally balanced,
then all of the points of disagreement are either close to the boundary of the other function, or far
from the origin.
Proposition E.4 (Localization of Disagreement from Locally Balanced Concepts) .LetF,bF:
Rk→ {± 1}, where Fis(R, ϱ)-locally β-balanced and F,bFhave disagreement γ=
βinf∥x∥2≤RPz∼Nk[z∈Bk(x, ϱ)], i.e.,Pz∼Nk[F(z)̸=bF(z)]≤γ. Then, for any xwith∥x∥2≤R
andF(x)̸=bF(x), we have x∈∂ϱbF.
Proof of Proposition E.4. Suppose, for contradiction, that there exists some x∈Rkwith∥x∥2≤R
andF(x)̸=bF(x), for which x/∈∂ϱbF. Then, it must be that bF(z) =bF(x)for all z∈Bk(x, ϱ)
(otherwise, x∈∂ϱbF). We have that Pz∼Nk[F(z)̸=bF(z)]≥Pz∼Nk[z∈Bk(x, ϱ)andF(z)̸=
bF(z)]and also F(z)̸=bF(z)is equivalent to F(z)̸=bF(x)(because bF(z) =bF(x)), which, in
turn, is equivalent to F(z) =F(x)(because F(x)̸=bF(x)). Overall, Pz∼Nk[F(z)̸=bF(z)]≥
Pz∼Nk[z∈Bk(x, ϱ)andF(z) =F(x)]> γ, by assumption, and we reached contradiction.
Remark E.5.Note that Proposition E.4 is not specialized to the Gaussian disagreement between F
andbF, but would also work for any distribution Q, if the local balance (Definition E.1) was also
defined w.r.t. Q.
We combine the boundary proximity tester with a moment matching tester for concentration (to
bound the probability of falling far from the origin) to obtain a non-universal localized discrepancy
tester (Theorem E.6). If we instead use a spectral tester for concentration, we obtain a universal
localized discrepancy tester (Theorem E.7).
Theorem E.6 (Discrepancy Testing through Boundary Proximity) .Letp∈N,R,bσ≥1,r, β∈(0,1)
and0≤γe≤βrk
kk/2e−2R2. Let also HandCbe a classes whose elements are k-subspace juntas over
RdandNe:H → Pow(C)theγe-disagreement neighborhood. Assume that the elements of Care
(R, r)-locally β-balanced on the relevant subspaces and let Tbe a(ϱ,bσ)-boundary proximity tester
forHw.r.t.Nd, requiring mTsamples, with ϱ= (γe
β)1/k√
ke2R2/k. For any ϵ∈(0,1), there is
a(Ne, ψ+ϵ)-tester for localized discrepancy from Ndwith respect to Ndwith sample complexity
m=mT+O(dk)4p+1+O(1
ϵ2), that calls Tonce and uses additional time O(md2p+1), where the
error parameter ψis
ψ= 24kp
R2p
+bσ√
kγeexp(2 R2)
β1/k
33Proof of Theorem E.6. Letϱ= (γe/β)1/k√
kexp(2 R2/k),∆ =1
(2kd)2pand let (bf, X)be an
instance of the localized discrepancy problem (see Definition 1.1). The algorithm does the following.
1.For each α∈Ndwith∥α∥1≤2p, compute the quantities Mα=Ex∼X[xα] =
Ex∼X[Q
i∈[d]xαi
i]andreject if for some αas such, we have |Mα−Ex∼N[xα]|>∆.
2. Run the boundary proximity tester Twith inputs (ϱ,bf, X)andreject ifTrejects.
3. Otherwise, accept .
Soundness. Assume, first, that all of the tests have passed. We will show that for any f∈Ne(bf),
we have Px∼X[f(x)̸=bf(x)]≤ψ. Since the event that bf(x)̸=f(x)is independent for each
x∈X, we may apply the Hoeffding bound to show that Px∼D′[bf(x)̸=f(x)]≤ψ+ϵwith
probability at least 3/4whenever |X| ≥3
ϵ2. Since fandbfarek-subspace juntas, we have that
f(x) = F(Wx)andbf(x) =bF(Vx)forW, V ∈Rk×dso that WW⊤=V V⊤=Ik. Let
U∈R2k×dbe a matrix such that UU⊤=I2kand the span of the rows of Ucontains the span of the
rows of Wand of Vtaken together. This, together with the fact that WW⊤=Ik, imply that for any
x∈Rdwe have Wx=WU⊤Uxand, similarly, Vx=V U⊤Ux(the part of xthat falls within the
subspace spanned by the rows of Wdoes not change by applying the projection matrix U⊤Uand the
remaining part is irrelevant). Moreover, we have that ∥U∥2=∥U⊤∥2=∥W∥2=∥V∥⊤
2= 1. Let
F′(z) =F(WU⊤z)andbF′(z) =bF(V U⊤z).
We have that Px∼N[F′(Ux)̸=bF′(Ux)]≤γe, by assumption. By Proposition E.4, applied on
F′,bF′, and since γe≤βrk
kk/2e−2R2, we have that for any x∈Rdsuch that F′(Ux)̸=bF′(Ux)
(i.e.,F(Wx)̸=bF(Vx))) at least one of the following is true: (a) ∥Ux∥2≥Ror (b) Ux∈∂ϱbF′.
According to Proposition E.8, Ux∈∂ϱbF′implies that V U⊤Ux∈∂ϱbF, which, in turn, implies
thatVx∈∂ϱbF, since Vx=V U⊤Uxand therefore, by Proposition E.8 we also have that x∈∂ϱbf.
Therefore, overall, we have
P
x∼X[f(x)̸=bf(x)]≤P
x∼X[∥Ux∥2≥R] +P
x∼X[x∈∂ϱbf]
In order to bound the term Px∼X[∥Ux∥2≥R], we use the fact that the test of step 1 of the al-
gorithm has passed. In particular, by applying Markov’s inequality appropriately, we obtain that
Px∼X[∥Ux∥2≥R]≤1
R2pEx∼X[∥Ux∥2p
2]. Note that the expression ∥Ux∥2p
2corresponds to a
polynomial of degree at most 2pand corresponding to coefficient vector whose absolute ( ℓ1) norm
is bounded by (4kd2)p. In particular, we have that (for all x∈Rd)∥Ux∥2p
2=P
α∈Ndcαxα
(recall that xα=Q
i∈[d]xαi
i), whereP
α∈Nd|cα| ≤(4kd2)pandcα= 0 whenever ∥α∥1>
2p. Therefore, by linearity of expectation, we have Ex∼X[∥Ux∥2p
2] =P
αcαEx∼X[xα] =P
αcα(Ex∼N[xα] + ∆ α) =Ex∼N[∥Ux∥2p
2] +P
αcα∆α, where |∆α| ≤1
(2kd)2pfor any αwith
∥α∥1≤2p. Hence, overall, we have Ex∼X[∥Ux∥2p
2]≤Ex∼N[∥Ux∥2p
2] + 1≤2(4kp)p, which
implies that Px∼X[∥Ux∥2≥R]≤2(4kp)p
R2p.
For the term Px∼X[Vx∈∂ϱbF], we use the fact that the tester Thas accepted and hence we have
Px∼X[x∈∂ϱbf]≤bσϱ≤bσ(γeexp(2 R2)
βk−k/2)1/k. We have shown that Px∼X[f(x)̸=bf(x)]≤ψ, as
desired.
Completeness. Suppose now that Xconsists of i.i.d. examples from the Gaussian distribution Nd.
To ensure that with probability at least 9/10, the tests of step 1 pass, we pick |X| ≥(Cdk)
∆2, for some
sufficiently large C. This is because the Gaussian moments concentrate (e.g., due to Chebyshev’s
inequality) as well as a union bound. For step 2, it suffices that |X| ≥mT.
We now give our universal discrepancy tester though testing boundary proximity.
Theorem E.7 (Universal Discrepancy Testing through Boundary Proximity) .In the setting of
Theorem E.6, if the tester Tworks with respect to a class Dof distributions over Rdsuch that for
34some µc≥1we have supv∈Sd−1Ex∼D[(v·x)4]≤µcfor allD ∈D, then there is a (Ne, ψ+ϵ)-tester
for localized discrepancy from Ndwith respect to Dwith sample complexity m=mT+ 20d4+3
ϵ2,
that calls Tonce and uses additional time O(md2+d3), where the error parameter ψis
ψ=4kµc
R2+bσ√
kγeexp(2 R2)
β1/k
Proof of Theorem E.7. Letϱ= (γe/β)1/k√
kexp(2 R2/k)and let (bf, X)be an instance of the
localized discrepancy problem (see Definition 1.1). The algorithm is similar to the one used in
Theorem E.6, but for the first step, instead of matching low degree moments, we compute the
maximum eigenvalue of the second moment matrix.
1.Compute the maximum eigenvalue of the matrix M=Ex∼X[xx⊤]andreject if the
computed value is larger than 2µc.
2. Run the boundary proximity tester Twith inputs (ϱ,bf, X)andreject ifTrejects.
3. Otherwise, accept .
Soundness. For the proof of soundness, we use a similar argument to the one for Theorem E.6, but
we instead bound the term Ex∼X[∥Ux∥2p
2]forp= 1and as follows
E
x∼X[∥Ux∥2
2] =2kX
i=1E
x∼X[(ui·x)2]≤2ksup
v∈Sd−1E
x∈X[(v·x)2]≤4kµc,
where uidenotes the vector corresponding to the i-th row of U.
Completeness. The completeness for step 1 follows by an application of Chebyshev’s inequality to
the random variables corresponding to each of the entries of the matrix Mand a union bound, to show
that the Frobenius norm (and hence the operator norm) of the matrix M−Ex∼D[xx⊤]is sufficiently
small (where Dis some distribution in DandXconsists of independent draws from D).
In the proofs of Theorems E.6 and E.7 we have used the following usedul proposition.
Proposition E.8. Letf:Rd→ {± 1}be ak-subspace junta, i.e., f(x) =F(Wx), where F:Rk→
{±1}andW∈Rk×dwithWW⊤=Ik. Then, we have x∈∂ϱfif and only if Wx∈∂ϱF.
Proof. Note, first that since WW⊤=Ikandk≤d, we have that ∥W∥2= 1. Consider x∈∂ϱf.
Then, by Definition D.3, we have that there exists z∈Rdwith∥z∥ ≤ϱandf(x+z)̸=f(x).
Note that for the same xandzwe have F(Wx+Wz)̸=F(Wx). Since ∥W∥2= 1, we have that
∥Wz∥2≤ ∥z∥2≤ϱ. Let ˜z=Wz∈Rk. We have ∥˜z∥2≤ϱandF(Wx+˜z)̸=F(Wx), i.e.,
Wx∈∂ϱF.
For the other direction, suppose that Wx∈∂ϱF. Then, there is ˜z∈Rkwith∥˜z∥2≤ϱsuch that
F(Wx+˜z)̸=F(Wx). We have that ˜z=Ik˜z=WW⊤˜z. Letz=W⊤˜z. We have ˜z=Wzand
∥z∥2=∥W⊤˜z∥2≤ ∥W⊤∥2∥˜z∥2=∥W∥2∥˜z∥2≤ϱ. We have that f(x+z) =F(Wx+Wz) =
F(Wx+˜z)̸=F(Wx) =f(x). Hence, x∈∂ϱf.
E.2 Application to TDS Learning
We now focus on the class of balanced intersections of halfspaces, which is formally defined as
follows.
Definition E.9 (Balanced Halfspace Intersections) .A concept f:Rd→ {± 1}is called a β-balanced
intersection of khalfspaces if it is β-balanced (see Definition A.1) and there are w1,w2, . . . ,wk∈
Sd−1andτ1, τ2, . . . , τ k∈Rsuch that f(x) = 2Qk
i=11{wi·x≥τi} −1for all x∈Rd.
35We will now combine Theorems E.6 and E.7 with results from robust learning ([ DKS18b ]) to obtain
the following theorem regarding TDS learning balanced intersections of halfspaces with respect to
Gaussian training marginals. Our results indicate a trade-off between the training runtime and testing
runtime and are robust to some amount of noise (in terms of the parameter λ).
Theorem E.10 (TDS Learning of Balanced Halfspace Intersections) .Forβ∈(0,1/2),d, k∈N, let
Cbe the class of β-balanced intersections of khalfspaces Rd. For any ϵ∈(0,1)withϵ=O(β
k2),
there is a D-universal ψ-TDS learner for Cw.r.t.Ndin the agnostic setting for each of the cases
listed in Table 3.
ClassDoverRdTraining Time Testing Time Error Guarantee ψ
Gaussian Nd poly( d)(k
ϵβ)O(k3)(dk)O(log(1 /ϵ))(k
ϵβ)O(1)λ1
12k+ϵ
Fourth Moments Bound:
E[(v·x)4]≤C∥v∥4
2&
Dimension- 1Marginal
Densities Bounded by Cpoly( d)(k
β)k32O(k3
ϵ)poly( d, k,1/ϵ) (k
β)O(1)2O(1
ϵ)λ1
12k+ϵ
Table 3: Specifications for D-universal ψ-TDS learning of β-balanced k-halfspace intersections. The
properties that define the class Din line 2, hold for some given universal constant C≥1, for all
members of D, for all v∈Rdand the density bound holds for all one-dimensional projections of any
member of D.
For the learning phase of the algorithm of Theorem E.10, we use an algorithm from [ DKS18b ] in the
context of learning with nasty noise. Since the algorithm works under nasty noise, it will also work
in the agnostic setting. The following result follows from [DKS18b, Theorem 5.1].
Theorem E.11 (Reformulation of Theorem 5.1 in [ DKS18b ]).LetCbe some hypothesis class that
consists of intersections of khalfspaces. For any γ∈(0,1), there is an algorithm that, upon
receiving a number of i.i.d. examples from some labeled distribution Dtrain
XY whose marginal is Nd,
runs in time poly( d)(k
γ)O(k2)and returns, w.p. at least 0.99, some intersection of khalfspaces
bf:Rd→ {± 1}such that for any distribution Dtest
XYoverRd× {± 1}, iff∗∈ Cis the intersection
that achieves λ= min f∈C(err(f;Dtrain
XY) + err( f;Dtest
XY)), then we have f∗∈Ne(bf), where Neis
the(Ckλ1
12+γ)-disagreement neighborhood (see Definition E.2), where Cis some sufficiently large
universal constant.
Note that for the above reformulation of Theorem 5.1 in [ DKS18b ], we used the following reasoning.
Their algorithm returns bfwith the guarantee that err(bf;Dtrain
XY)≤O(kopt1
12
train)+γ, where opttrain=
minf∈Cerr(f;Dtrain
XY)≤err(f∗;Dtrain
XY)≤λ. Therefore Px∼Nd[bf(x)̸=f∗(x)]≤err(bf;Dtrain
XY) +
err(f∗;Dtrain
XY)≤Ckλ1
12+γ, which implies that f∗∈Ne(bf).
Our plan is to use the discrepancy testers of Theorems E.6 and E.7. To this end, we have to show
that (1) balanced halfspace intersections are locally balanced and (2) there is a boundary proximity
tester (see Definition E.3) for the class. It turns out that any convex set that is globally balanced (see
Definition A.1), is also locally balanced (see Definition E.1), as we show in the following lemma.
Lemma E.12 (Globally Balanced Convex Sets are Locally Balanced) .Forβ∈(0,1), letF:
Rk→ {± 1}be the indicator of a (globally) β-balanced convex set K ⊆Rk, letC≥1some
sufficiently large universal constant and let R≥1. Then, Fis(R,β
Cklogk)-locally β′-balanced for
β′=βkexp(−1
2R)
(Ck2Rln(1
β))k.
Proof of Lemma E.12. Letϱ≤β
Cklogk. We will first show that for any x∈Rkwith∥x∥2≤R
andF(x) =−1, we have Pz∼Nk[F(z) =−1|z∈Bk(x, ϱ)]≥1
2e−ϱR. We have that x̸∈ K
and, therefore, there is a separating hyperplane between xandK, due to the convexity of K. This
hyperplane does not pass through xand, hence, at least half of Bk(x, ϱ)is outside K. We obtain the
36following.
P
z∼Nk[F(z) =−1|z∈Bk(x, ϱ)] =Pz∼Nk[F(z) =−1andz∈Bk(x, ϱ)]
Pz∼Nk[z∈Bk(x, ϱ)]
≥1
2vol(Bk(x, ϱ))
vol(Bk(x, ϱ))·infz∈Bk(x,ϱ)Nk(z)
supz∈Bk(x,ϱ)Nk(z)
≥1
2·exp(−1
2(∥x∥2+ϱ)2)
exp(−1
2(∥x∥2−ϱ)2
≥1
2e−1
2ϱ∥x∥2≥1
2e−1
2ϱR
For the case where F(x) = 1 , we first prove the following claim, which states that when a convex set
is (globally) balanced, it must contain some Euclidean ball with non-negligible mass.
Claim. SinceKisβ-balanced and convex, there is xc∈Rksuch that Bk(xc, r)⊆ K , where
r=β
Cklogk,∥xc∥2≤Rc= (2kln(8k
β))1/2andC≥1is a sufficiently large universal constant.
Proof. SinceKis balanced, we have Px∼Nk[F(x) = 1] > β. We now use Lemma B.8 to obtain that
Px∼Nk[x∈∂rF]≤C
2rklogk. We have the following.
P
x∼Nk[F(z) = 1 ,∀z∈Bk(x, r)] =P
x∼Nk[F(x) = 1 andF(x+z) = 1 ,∀zwith∥z∥2≤r]
=P
x∼Nk[F(x) = 1] −P
x∼Nk[F(x) = 1 and∃z:∥z∥2≤randF(x+z)̸= 1]
≥P
x∼Nk[F(x) = 1] −P
x∼Nk[∃z:∥z∥2≤randF(x+z)̸=F(x)]
=P
x∼Nk[F(x) = 1] −P
x∼Nk[x∈∂rF]
> β−C
2rklogk=β
2
Moreover, since Px∼Nk[∥x∥2> R c]≤4ke−R2
c
2k=β/2, we overall have that
P
x∼Nk[F(z) = 1 ,∀z∈Bk(x, r)and∥x∥2≤Rc]>0
Since the probability of such an xis positive, by the probabilistic method, there is some xcas
desired.
We have shown that for some xcwith∥xc∥2≤Rc, we have Bk(xc, r)⊆ K. Let now x∈Rkwith
∥x∥2≤RandF(x) = 1 (x∈ K). Since Kis convex, if K′is the convex hull of {x} ∪Bk(xc, r),
we have K′⊆ K. We will show that K′∩Bk(x, ϱ)contains some cone R′with non-trivial mass (see
Figure 3).
Letybe any point on the surface of Bk(xc, r)such that the tangent hyperplane of Bk(xc, r)ony
passes from x. Then, if we let θto be the angle [yxx c, we have sinθ=∥y−xc∥/∥x−xc∥2=
r/∥x−xc∥2, because [xyx c=π/2, by definition of y. Note that the triangle defined by x,yandxc
lies within K′and hence within Kas well. Since this is true for any yas defined above, we have that
Kcontains a rotational cone Rwith vertex x, angle θand height h∈[∥x−xc∥2−r,∥x−xc∥]. Note
that the volume of K′∩Bk(x, ϱ)is decreasing in ∥x−xc∥2, as long as ϱ≤r. Therefore, we may
assume that ∥x−xc∥2=R+Rc(which implies that h≥1≥ϱ≥ϱcosθ). LetR′=R∩Bk(x, ϱ).
37Figure 3: If x∈ K, then there is a cone R′⊆Bk(x, ϱ)∩ K
By observing that R′contains a cone of angle θ, height ϱcosθ, where cosθ≥1/2andϱ≤R, we
overall have the following.
P
z∼Nk[F(z) = 1|z∈Bk(x, ϱ)] =Pz∼Nk[F(z) = 1 andz∈Bk(x, ϱ)]
Pz∼Nk[z∈Bk(x, ϱ)]
≥vol(R′)
vol(Bk(x, ϱ))·infz∈Bk(x,ϱ)Nk(z)
supz∈Bk(x,ϱ)Nk(z)
≥ϱcosθ(ϱsinθ)k−1(2π)(k−1)/2k−((k−1)/2+1)
ϱk(2π/k)k/2·exp(−ϱR/2)
≥(sinθ)k−1
2√
2πk·e−1
2ϱR≥β
Ck2Rln(1/β)k
e−R/2
Combining the two cases considered ( F(x) =−1andF(x) = 1 ), we obtain the desired result.
Finally, we show that there is a boundary proximity tester for the class of halfspace intersections.
Lemma E.13 (Boundary Proximity Tester for Halfspace Intersections) .LetDbe some class of
distributions over Rdsuch that for each distribution in D, any one-dimensional marginal has density
upper bounded by C >0. Then, for any ϱ∈(0,1), there is a (ϱ,3Ck)-boundary proximity tester for
the class of intersections of khalfspaces over Rdwith time and sample complexity poly( d, k,1/ϱ).
Proof. The tester receives some intersection of halfspaces f= 2Qk
i=11{wi·x−τi} −1andmT
samples Xfrom some unknown distribution over Rdand does the following.
1. If for some i∈[k]we have Px∼X[|wi·x−τi| ≤ϱ]>3Cϱ, then reject .
2. Otherwise, accept .
Soundness then follows from the fact that Px∼X[x∈∂ϱf]≤P
i∈[k]Px∼X[|wi·x−τi| ≤ϱ]and a
Hoeffding bound. Completeness follows from the fact that under any distribution DinD, we have
Px∼D[|wi·x−τi| ≤ϱ]≤2Cϱ, due to the density upper bound in the direction wiand a Chernoff
bound.
All of the ingredients of the proof of Theorem E.11 are now in place.
Proof of Theorem E.11. The theorem follows by combining either Theorem E.6 or Theorem E.7
with Theorem E.11, Lemma E.12 and Lemma E.13. Note that since the parameter λis unknown
to the algorithm, we will run the corresponding discrepancy tester (either of Theorem E.6 or of
Theorem E.7) for all possible values of the parameter ϱ(of the discrepancy tester) within an O(ϵ/k2)-
net of the interval [0,β
Cklogk], where we know that the tester has to accept with high probability
(we can amplify the success probability for each fixed value of ϱthrough repetition). We accept if
the (amplified) discrepancy tester accepts for all the values of ϱin the net. In total, we will need
poly( k,1/ϵ)repetitions.
38F NP-Hardness of Global Discrepancy Testing
In this section, we prove that there exist worst case pairs of distributions such that testing the
globalized discrepancy between them with respect to the class of halfspaces is hard. These results
also extend to the class of constant degree polynomial threshol functions. This motivates our study of
localized notions of discrepancy. We now define the notion of discrepancy (globalized).
Definition F.1 (Discrepancy) .LetD1, D2be two distributions on Rdand let Fbe a set of boolean
functions on Rd. We say that the discrepancy between D1andD2with respect to F, denoted by
discF(D1, D2)is,
discF(D1, D2) = sup
f1,f2∈FP
x∼D1[f1(x)̸=f2(x)]−P
x∼D2[f1(x)̸=f2(x)]
We prove our hardness result by reducing the following problem of learning constant degree PTFs
with noise to the problem of identifying if the discrepancy between two distributions is large/small.
Definition F.2. For constants ϵ >0, k∈N, letPTF−MA(k, ϵ)refers to the following promise
problem: Given a set of tuples {xi, yi}i∈[n]where xi∈Rdandyi∈ {± 1}for all i∈[n], distinguish
between the following two cases:
• There exists a halfspace hsuch that1
nPn
i=11{h(xi) =yi} ≥1−ϵ,
• For every degree kPTFg, we have that1
nPn
i=11{g(xi) =yi} ≤1
2+ϵ
This problem is known to be NP hard through a reduction from label cover.
Lemma F.3 ([BGS18]) .For any constant k∈N, ϵ > 0,PTF−MA(k, ϵ)is NP-hard.
Given a set S⊆Rd, letUSdenote the uniform distribution on that set. We define decision version of
the problem of discrepancy testing for which we prove our NP-hardness result.
Definition F.4. For constants ϵ >0and a class Fof boolean functions on Rd, letDISC (F, ϵ)be the
following promise problem: Given sets S, S′⊆Rd, distinguish between the two cases:
•discF(US, US′)≥1−ϵ
•discF(US, US′)≤ϵ
We are now ready to state and prove our result on the NP-hardness of DISC (F, ϵ)whenFis the class
of constant degree polynomial threshold functions.
Theorem F.5. Letk∈Nandϵ >0. LetFbe the class of PTFs of degree k. The problem DISC (F, ϵ)
is NP-hard.
Proof. We give a reduction from PTF−MA(2k, ϵ)toDISC (F,8ϵ). The input to PTF−MA(2k, ϵ)
is a set of tuples {xi, yi}i∈[n]where xi∈Rdandyi∈ {± 1}for all i∈[n]. Let S+={xi|yi=
+1, i∈[n]}andS−={xi|yi=−1, i∈[n]}. We assume that|S+|
n−1
2≤ϵand|S−|
n−1
2≤ϵ.
Otherwise, there exists a trivial halfspace(taking constant value) that achieves success probability
greater than1
2+ϵand this can easily be checked in polynomial time. We say that S+, S−are
ϵ-unbiased if the above property holds. We now complete the proof by proving the following two
claims and using Lemma F.3.
Claim (Completeness) .LetS+, S−beϵ-unbiased. If there exists a halfspace hsuch that
1
nPn
i=11{h(xi) =yi} ≥1−ϵ, then discF(US+, US−)≥1−8ϵ.
Proof. We have that|S+|
nPx∼US+[h(x) = 1] +|S−|
nPx∼US+[h(x) = 0] ≥1−ϵ. Thus, simplifying
some terms, we obtain that
1−ϵ≤|S−|
n+|S+|
n·P
x∼US+[h(x) = 1] −|S−|
n·P
x∼US−[h(x) = 1]
≤1
2+1
2·
P
x∼US+[h(x) = 1] −P
x∼US−[h(x) = 1]
+ 3ϵ
39where the last inequality follows from the fact that S+, S−areϵ-unbiased. Thus, we obtain that
(Px∼US+[h(x) = 1] −Px∼US−[h(x) = 1]) ≥1−8ϵ. Let gbe the the halfspace that always
outputs −1. Clearly, we have that discF(US+, US−)≥(Px∼US+[h(x)̸=g(x)]−Px∼US−[h(x)̸=
g(x)])≥1−8ϵ.
Claim (Soundness) .LetS+, S−beϵ-unbiased. If there exists no degree 2kPTF hsuch that
1
nPn
i=11{h(xi) =yi} ≥1
2+ϵ, then discF(US+, US−)≤8ϵ.
Proof. SaydiscF(US+, US−)≥8ϵ. Since Fis closed under complements, we obtain without loss
of generality that there exist two PTFs h1,h2of degree dsuch that Px∼US−[h1(x)̸=h2(x)]−
Px∼US+[h1(x)̸=h2(x)]≥1
2+ϵ. Consider the function g(x) =h1(x)·h2(x). We have that gis a
degree 2kPTF. Thus, we obtain that
1
nnX
i=11{g(x) =y}=|S−|
n·P
x∼US−[g(x) =−1] +|S+|
n·P
x∼US+[g(x) = 1]
=|S−|
n·P
x∼US−[h1(x)̸=h2(x)] +|S+|
n·(1−P
x∼US+[h1(x)̸=h2(x)])
≥1
2+1
2
P
x∼US−[h1(x)̸=h2(x)]−P
x∼US+[h1(x)̸=h2(x)
−3ϵ
≥1
2+ϵ
where the penultimate inequality follows from the fact that S+, S−areϵ-unbiased and the last
inequality follows from our lower bound on the discrepancy. Since there exists no PTF of degree 2k
that succeeds with probability1
2+ϵ, we have a contradiction.
This concludes the proof of Theorem F.5.
40NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We provide detailed proofs and/or references for all the claims made in the
abstract and introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We include a dedicated section in the end of the main paper.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
41Justification: We provide full proofs for all of our results in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: Our paper does not have any experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
42Answer: [NA]
Justification: Our paper does not have any experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: Our paper does not have any experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: Our paper does not have any experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
43•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: Our paper does not have any experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We read the code of ethics and strongly believe that our work conforms with
stated code.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We include a discussion on the broader impacts of our work at the end of the
main paper.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
44•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We do not release any data and models.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: Our paper does not use any existing code, data or models.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
45•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Our paper does not release any new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our paper does not involve crowd-sourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
46