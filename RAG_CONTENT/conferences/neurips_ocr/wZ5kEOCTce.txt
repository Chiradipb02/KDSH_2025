Rethinking Patch Dependence for Masked
Autoencoders
Anonymous Author(s)
Affiliation
Address
email
Abstract
In this work, we examine the impact of inter-patch dependencies in the decoder of 1
masked autoencoders (MAE) on representation learning. We decompose the decod- 2
ing mechanism for masked reconstruction into self-attention between mask tokens 3
and cross-attention between masked and visible tokens. Our findings reveal that 4
MAE reconstructs coherent images from visible patches not through interactions 5
between patches in the decoder but by learning a global representation within the 6
encoder. This discovery leads us to propose a simple visual pretraining framework: 7
cross-attention masked autoencoders (CrossMAE). This framework employs only 8
cross-attention in the decoder to independently read out reconstructions for a small 9
subset of masked patches from encoder outputs, yet it achieves comparable or 10
superior performance to traditional MAE across models ranging from ViT-S to 11
ViT-H. By its design, CrossMAE challenges the necessity of interaction between 12
mask tokens for effective masked pretraining. Code is available here. 13
1 Introduction 14
Masked image modeling [ 46,30,61,4] has emerged as a pivotal unsupervised learning technique 15
in computer vision. One such recent work following this paradigm is masked autoencoders (MAE): 16
given only a small, random subset of visible image patches, the model is tasked to reconstruct the 17
missing pixels. By operating mostly on this small subset of visible tokens, MAE can efficiently 18
pre-train high-capacity models on large-scale vision datasets, demonstrating impressive results on a 19
wide array of downstream tasks [33, 38, 49]. 20
The MAE framework employs self-attention across the entire model for self-supervised reconstruction 21
tasks. In this setup, both masked and visible tokens engage in self-attention, not just with each other 22
but also with themselves, aiming to generate a holistic and context-aware representation. However, 23
the masked tokens inherently lack information. Intuitively, facilitating information exchange among 24
adjacent masked tokens should enable the model to synthesize a more coherent image, thereby 25
accomplishing the task of masked reconstruction and improving representation learning. A question 26
arises, though: Is this truly the case? 27
We decompose the decoding process of each mask token into two parallel components: self-attention 28
with other mask tokens, as well as cross-attention to the encoded visible tokens. If MAE relies on 29
the self-attention with other mask tokens, its average should be on par with the cross-attention. Yet, 30
the quantitative comparison in Figure 1.(b) shows the magnitude of mask token-to-visible token 31
cross-attention (1.42) in the MAE decoder evaluated over the entire ImageNet validation set far 32
exceeds that of mask token-to-mask token self-attention (0.39). 33
This initial observation prompts two questions: 1)Is the self-attention mechanism among mask 34
tokens in the decoder necessary for effective representation learning? 2)If not, can each patch be 35
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Figure 1: Method Overview .(A)Masked autoencoder (MAE) starts by masking random patches of the input
image. (B)To reconstruct a mask token (marked by the blue star), MAE attends to both the masked tokens
(B.Left) and the visible tokens (B.Right). A quantitative comparison over the ImageNet validation set shows
that the masked tokens in MAE disproportionally attend to the visible tokens (1.42 vs 0.39), questioning the
necessity of attention within mask tokens. (C)We propose CrossMAE, the masked patches are reconstructed
from only the cross attention between the masked tokens and the visible tokens. Surprisingly, CrossMAE attains
the same or better performance than MAE on ImageNet classification and COCO instance segmentation.
Figure 2: Example reconstructions of ImageNet validation images. For each set of 5 images, from left to right,
are the original image, masked image with a mask ratio of 75%, MAE [ 30], CrossMAE (trained to reconstruct
25% of image tokens, or 1/3 of the mask tokens), and CrossMAE (trained to reconstruct all masked tokens).
Since CrossMAE does not reconstruct them, all model outputs have the visible patches overlaid. Intriguingly,
CrossMAE, when trained for partial reconstruction, can decode all mask tokens in one forward pass (shown
above), indicating that the encoder rather than the decoder effectively captures global image information in
its output tokens. Its comparable reconstruction quality to full-image-trained models suggests that full-image
reconstruction might not be essential for effective representation learning.
independently read out from the encoder output, allowing the reconstruction of only a small subset of 36
masked patches, which in turn, accelerates the pretraining without performance degradation? 37
In addressing these questions, we introduce CrossMAE, which diverges from MAE in three ways: 38
1.Cross-attention for decoding. Rather than passing a concatenation of mask and visible 39
tokens to a self-attention decoder, CrossMAE uses mask tokens as queries to read out the masked 40
reconstructions from the visible tokens in a cross-attention decoder . In this setting, mask tokens 41
incorporate information from the visible tokens but do not interact with other mask tokens, thereby 42
reducing the sequence length for the decoder and cutting down computational costs. 43
2.Independent partial reconstruction. With self-attention removed, the decoding of each mask 44
token, based on the encoded features from visible tokens, becomes conditionally independent. This 45
enables the decoding of only a fraction of masked tokens rather than the entire image. 46
3.Inter-block attention. Due to the separation of visible and mask tokens, we can use features 47
from different encoder blocks for each decoder block. Empirically, we find solely relying on the last 48
2encoder feature map for reconstruction, the design present in MAE, hurts feature learning. We propose 49
a lightweight inter-block attention mechanism that allows the CrossMAE decoder to leverage a mix 50
of low-level and high-level feature maps from the encoder, improving the learned representation. 51
The analysis performed on CrossMAE led to a novel way to understand MAE. Even though the 52
patches to be reconstructed are independently decoded, our findings demonstrate that coherent 53
reconstruction for each masked patch can be independently read out from the encoder output, without 54
any interactions among masked tokens in the decoder for consistency (Figure 2). Furthermore, the 55
downstream performance of the model remains robust even without these interactions (Figure 1.(c), 56
Tables 1 and 2). Both pieces of evidence confirm that the encoder’s output features already encapsulate 57
the necessary global context for image reconstruction, while the decoder simply performs a readout 58
from the encoder output to reconstruct the pixels at the location of each patch. 59
To sum up, our main contributions are the following: 60
1.We present a novel understanding of MAE. Our findings show that MAE reconstructs coherent 61
images from visible patches not through interactions between patches to be reconstructed in the 62
decoder but by learning a global representation within the encoder . This is evidenced by the model’s 63
ability to generate coherent images and maintain robust downstream performance without such 64
interactions, indicating the encoder effectively captures global image information. 65
2.We advocate replacing self-attention layers with a simple cross-attention readout function. 66
Given our discovery that the encoder in MAE already captures a comprehensive global representation, 67
we propose replacing self-attention layers in the decoder with a more efficient information readout 68
function. Specifically, we suggest utilizing cross-attention to aggregate the output tokens of the 69
encoder into each input token within the decoder layers independently , thereby eliminating the need 70
for token-to-token communication within the decoder. 71
3.CrossMAE achieves comparable or superior performance with reduced computational 72
costs in image classification and instance segmentation compared to MAE on vision transformer 73
models ranging from ViT-S to ViT-H . Code is available here. 74
2 Related Works 75
2.1 Self-Supervised Learning 76
In self-supervised representation learning, a model trains on a pretext task where the supervision 77
comes from the input data itself without labels. Contrastive learning methods learn representations 78
by contrasting positive and negative samples, such as SimCLR [ 11], CPC [ 44], MoCo [ 29,12,13], 79
CLD [ 59] and SwA V [ 7]. Additionally, in BYOL [ 26], iBOT [ 65], DINO [ 8], DINOv2 [ 45], and 80
MaskAlign [62] make a student model to imitate a teacher model without negative pairs. 81
Generative modeling, focusing on acquiring a generative model capable of capturing the underlying 82
data distribution, is an alternative method for self-supervised learning. V AE/GAN [ 35] merges the 83
strengths of variational autoencoders and generative adversarial networks to acquire disentangled 84
representations of data. PixelCNN, PixelV AE, and PixelTransformer [ 55,27,54] generate images 85
pixel by pixel, taking into account the context of previously generated pixels. Masked modeling, a 86
large subclass of generative modeling, is discussed in the following subsection. After the pre-training 87
stage, these generative models can be finetuned for many downstream applications. 88
2.2 Masked Modeling 89
Masked modeling learns representations by reconstructing a masked portion of the input. Pioneering 90
works in natural language processing (NLP) present various such pretraining objectives. BERT [ 19] 91
and its extensions [ 41,34] use a bidirectional transformer and present few-shot learning capabil- 92
ities from masked language modeling. GPT [ 47,48,5], uses autoregressive, causal masking and 93
demonstrates multi-task, few-shot, and in-context learning capabilities. 94
Early works in computer vision, such as Stacked Denoising Autoencoders [ 57] and Context En- 95
coder [ 46], investigated masked image modeling as a form of denoising or representation learning. 96
Recently, with the widespread use of transformer [ 20] as a backbone vision architecture, where 97
images are patchified and tokenized as sequences, researchers are interested in how to transfer the 98
success in language sequence modeling to scale vision transformers. BEiT [ 3], MAE [ 30], and Sim- 99
3Figure 3: MAE [ 30] concatenates allmask tokens with the visible patch features from a ViT encoder and passes
them to a decoder with self-attention blocks to reconstruct the original image. Patches that correspond to visible
tokens are then dropped, and an L2 loss is applied to the rest of the reconstruction as the pretraining objective.
CrossMAE instead uses cross-attention blocks in the decoder to reconstruct only a subset of the masked tokens.
MIM [ 61] are a few of the early works that explored BERT-style pretraining of vision transformers. 100
Compared to works in NLP, both MAE and SimMIM [ 30,61] find that a much higher mask ratio 101
compared to works in NLP is necessary to learn good visual representation. Many recent works 102
further extend masked pretraining to hierarchical architectures [ 61,40] and study data the role of data 103
augmentation [ 9,21]. Many subsequent works present similar successes of masked pretraining for 104
video [ 52,58,22,28], language-vision and multi-modal pretraining [ 1,39,23] and for learning both 105
good representations and reconstruction capabilities [60, 37]. 106
However, BERT-style pretraining requires heavy use of self-attention, which makes computational 107
complexity scale as a polynomial of sequence length. PixelTransformer [ 54] and DiffMAE [ 60] both 108
use cross-attention for masked image generation and representation learning. Siamese MAE [ 28] 109
uses an asymmetric masking pattern and decodes frames of a video condition on an earlier frame. In 110
these settings, allmasked patches are reconstructed. In this work, we investigate if learning good 111
features necessitates high reconstruction quality and if the entire image needs to be reconstructed to 112
facilitate representation learning. PCAE [ 36] progressively discards redundant mask tokens through 113
its network, leading to a few tokens for reconstruction. VideoMAEv2 [ 58] concatenates randomly 114
sampled masked tokens with visible tokens and uses self-attention to reconstruct the masked patches. 115
In comparison, we minimally modify MAE with a cross-attention-only decoder and masked tokens 116
are decoded in a conditional independent way. 117
2.3 Applications of Cross-Attention 118
In addition to the prevalent use of self-attention in computer vision, cross-attention has shown to be a 119
cost-effective way to perform pooling from a large set of visible tokens. Intuitively, cross-attention 120
can be seen as a parametric form of pooling, which learnably weighs different features. Touvron 121
et al. [53] replace mean pooling with cross-attention pooling and find improvement in ImageNet 122
classification performance. Jaegle et al. [32] uses cross-attention to efficiently process large volumes 123
of multi-modal data. Cross-attention is also widely used for object detection. Carion et al. [6]utilizes 124
query tokens as placeholders for potential objects in the scene. Cheng et al. [16,15]further extend 125
this concept by introducing additional query tokens to specifically tackle object segmentation in 126
addition to the query tokens for object detection. Distinct from thes prior works, we are interested the 127
role of cross-anttention for representation learning in a self-supervised manner. 128
3 CrossMAE 129
We start with an overview of vanilla masked autoencoders in Section 3.1. Next, in Section 3.2, we 130
introduce the use of cross-attention in place of self-attention in the decoder for testing the necessity 131
of interaction between mask tokens for representation learning. In Section 3.3, we discuss how 132
eliminating self-attention in the decoding process enables us to reconstruct only a subset of masked 133
tokens, leading to faster pretraining. Finally, Section 3.4 presents our inter-block attention mechanism, 134
which allows decoder blocks to leverage varied encoder features. 135
3.1 Preliminaries: Masked Autoencoders 136
Masked Autoencoders (MAE) [ 30] pretrain Vision Transformers (ViTs) [ 20]. Each image input is 137
first patchified, and then a random subset of the patches is selected as the visible patches. As depicted 138
in Figure 3, the visible patches, concatenated with a learnable class token [CLS] , are subsequently 139
4Figure 4: Overview of CrossMAE. (a) The vanilla version of CrossMAE uses the output of the last encoder
block as the keys and queries for cross-attention. The first decoder block takes the sum of mask tokens and their
corresponding positional embeddings as queries, and subsequent layers use the output of the previous decoder
block as queries to reconstruct the masked patches. (b)Unlike the decoder block in [ 56], the cross-attention
decoder block does not contain self-attention, decoupling the generation of different masked patches. (c)
CrossMAE’s decoder blocks can leverage low-level features for reconstruction via inter-block attention. It
weighs the intermediate feature maps, and the weighted sum of feature maps is used as the key and value for
each decoder block.
fed into the ViT encoder, which outputs a set of feature latents. The latent vectors, concatenated with 140
the sum of the positional embeddings of the masked patches and the learnable mask token, are passed 141
into the MAE decoder. The decoder blocks share the same architecture as the encoder blocks (i.e., 142
both are transformer blocks with self-attention layers). Note that the number of tokens fed into the 143
decoder is the same length as the original input, and the decoding process assumes that the decoded 144
tokens depend on both visible and masked tokens. Decoder outputs pass through a fully connected 145
layer per patch for image reconstruction. After the reconstruction is generated, the loss is applied 146
only to the masked positions, while the reconstructions for visible spatial locations are discarded. 147
Recall in Sec. 1 we measure the mean attention value across all attention maps over the ImageNet 148
validation set to study the properties of MAE. We grouped the attention values by cross-attention 149
and self-attention between visible and masked tokens. We observed that in the decoding process 150
of an MAE, mask tokens attend disproportionately to the class token and the visible tokens (see 151
Figure 1.(b)). This motivates us to make design decisions and conduct experiments specifically to 152
answer the following question: Can we simplify the decoding process by eliminating self-attention 153
among masked tokens without compromising the model’s ability to generate coherent images and 154
perform well on downstream tasks? 155
3.2 Reconstruction with Cross-Attention 156
To address this question, we substitute the self-attention mechanism in the decoder blocks with 157
cross-attention, using it as a readout function to decode the latent embedding from the encoder to raw 158
pixel values. Specifically, the decoder employs multi-head cross-attention where the queries are the 159
output from previous decoder blocks (or the sum of position embedding of the masked patches and 160
mask token for the first decoder block). The keys and values are from the encoded features. 161
In the most basic CrossMAE, the output from the final encoder block is used as the key and value 162
tokens for all layers of the decoder, as illustrated in Fig. 4(a). Further exploration in Sec.3.4 reveals 163
that utilizing a weighted mean of selected encoder feature maps can be beneficial. The residual 164
connections in each decoder block enable iterative refinement of decoded tokens as they progress 165
through decoder blocks. 166
Diverging from the original transformer architecture [ 56], our decoder omits the causal self-attention 167
layer before the introduction of multi-head cross-attention. This elimination, coupled with the fact 168
that layer normalization and residual connections are only applied along the feature axis but not 169
5the token axis, enables the independent decoding of tokens. This design choice is evaluated in the 170
ablation study section to determine its impact on performance. 171
Given the disparity in the dimensions of the encoder and decoder, MAE adapts the visible features to 172
the decoder’s latent space using an MLP. However, in CrossMAE, as encoder features are integrated 173
at various decoder blocks, we embed the projection within the multi-head cross-attention module. 174
Cross-attention layers serve as a readout function that decodes the global representation provided 175
in the encoder’s output tokens to the pixel values within each patch to be reconstructed. However, 176
CrossMAE does not restrict the architecture to a single cross-attention block. Instead, we stack 177
multiple cross-attention decoder blocks in a manner more akin to the traditional transformer [56]. 178
3.3 Partial Reconstruction 179
The fact that CrossMAE uses cross-attention rather than self-attention in the decoder blocks brings 180
an additional benefit over the original MAE architecture. Recall that mask tokens are decoded inde- 181
pendently and thus there is no exchange of information between them, to obtain the reconstructions 182
at a specific spatial location, CrossMAE only needs to pass the corresponding mask tokens to the 183
cross-attention decoder. This allows partial reconstruction in contrast to the original full-image 184
reconstruction in the MAE architecture which needs to pass all the masked tokens as the input of the 185
decoder blocks due to the existence of self-attention in the decoder blocks. 186
To address the second question in Sec. 3.1, rather than decoding the reconstruction for all masked 187
locations, we only compute the reconstruction on a random subset of the locations and apply the loss 188
to the decoded locations. Specifically, we name the ratio of predicted tokens to all image tokens as 189
prediction ratio (γ), and the mask ratio ( p). Then the prediction ratio is bounded between γ∈(0, p]. 190
Because we are sampling within the masked tokens uniformly at random and the reconstruction 191
loss is a mean square error on the reconstructed patches, the expected loss is the same as in MAE, 192
while the variance is ( p/γ) times larger than the variance in MAE. Empirically, we find that scaling 193
the learning rate of MAE ( β) to match the variance (i.e. setting the learning rate as γβ/p)) helps 194
with model performance. Since cross-attention has linear complexity with respect to the number of 195
masked tokens, this partial reconstruction paradigm decreases computation complexity. Empirically, 196
we find that the quality of the learned representations is not compromised by this approach. 197
3.4 Inter-block Attention 198
MAE combines the feature of the last encoder block with mask tokens as the input to the self-attention 199
decoder, which creates an information bottleneck by making early encoder features inaccessible 200
for the decoder. In contrast, CrossMAE’s cross-attention decoder decouples queries from keys and 201
values. This decoupling allows different cross-attention decoder blocks to take in feature maps from 202
different encoder blocks. This added degree of flexibility comes with a design choice for selecting 203
encoder features for each decoder block. One naive choice is to give the feature of the ith encoder 204
block to the last ith decoder ( e.g., feeding the feature of the first encoder to the last decoder), in a 205
U-Net-like fashion. However, this assumes the decoder’s depth matches the depth of the encoder, 206
which is not the case for MAE or CrossMAE. 207
Instead of manually matching each decoder block with an encoder feature map, we make the selection 208
learnable and propose inter-block attention for feature fusion for each decoder block (Figure 4(c)). 209
Analogous to the inter-patch cross-attention that takes a weighted sum of the visible token embeddings 210
across the patch dimensions to update the embeddings of masked tokens, inter-block attention takes 211
a weighted sum of the visible token embeddings across different input blocks at the same spatial 212
location to fuse the input features from multiple blocks into one feature map for each decoder block. 213
Concretely, each decoder block takes a weighted linear combination of encoder feature maps {fi}as 214
keys and values. Specifically, for each key/value token tkin decoder block kin a model with encoder 215
depth n, we initialize a weight wk∈ Rn∼ N(0,1/n). Then tkis defined as 216
tk=nX
j=1wk
jfj. (1)
217In addition to feature maps from different encoder blocks, we also include the inputs to the first 218
encoder block to allow the decoder to leverage more low-level information to reconstruct the original 219
6Method ViT-S ViT-B ViT-L ViT-H
Supervised [50] 79.0 82.3 82.6 83.1
DINO [8] - 82.8 - -
MoCo v3 [14] 81.4 83.2 84.1 -
BEiT [3] - 83.2 85.2 -
MultiMAE [2] - 83.3 - -
MixedAE [9] - 83.5 - -
CIM [21] 81.6 83.3 - -
MAE [30] 78.9 83.3 85.4 85.8
CrossMAE (25%) 79.2 83.5 85.4 86.3
CrossMAE (75%) 79.3 83.7 85.4 -
Table 1: ImageNet-1K classification accuracy .
CrossMAE performs on par or better than MAE.
All experiments are run with 800 epochs. The best
results are in bold while the second best results are
underlined .APboxAPmask
Method ViT-B ViT-L ViT-B ViT-L
Supervised [38] 47.6 49.6 42.4 43.8
MoCo v3 [14] 47.9 49.3 42.7 44.0
BEiT [3] 49.8 53.3 44.4 47.1
MixedAE [9] 50.3 - 43.5 -
MAE [38] 51.2 54.6 45.5 48.6
CrossMAE 52.1 54.9 46.3 48.8
Table 2: COCO instance segmentation. Compared to
previous masked visual pretraining works, CrossMAE per-
forms favorably on object detection and instance segmen-
tation tasks.
image. We can select a subset of the feature maps from the encoder layers instead of all feature maps. 220
This reduces the computation complexity of the system. We ablate this in Table 3d. 221
We show that using the weighted features rather than simply using the features from the last block 222
greatly improves the performance of CrossMAE. Intriguingly, in the process of learning to achieve 223
better reconstructions, early decoder blocks tend to prioritize information from later encoder blocks, 224
while later decoder blocks focus on earlier encoder block information, as demonstrated in Section 4.5. 225
4 Experiments 226
We perform self-supervised pretraining on ImageNet-1K, following MAE [ 30]’s hyperparameter 227
settings, only modifying the learning rate and decoder depth. The hyperparameters were initially 228
determined on ViT-Base and then directly applied to ViT-Small, ViT-Large, and ViT-Huge. Both 229
CrossMAE and MAE are trained for 800 epochs. We provide implementation details and more 230
experiments in the appendix. 231
4.1 ImageNet Classification 232
Setup. The model performance is evaluated with end-to-end fine-tuning, with top-1 accuracy used 233
for comparison. Same as in Figure. 2, we compare two versions of CrossMAE: one with a prediction 234
ratio of 25% (1/3 of the mask tokens) and another with 75% (all mask tokens). Both models are 235
trained with a mask ratio of 75% and a decoder depth of 12. 236
Results. As shown in Table 1, CrossMAE outperforms vanilla MAE using the same ViT-B encoder 237
in terms of fine-tuning accuracy. This shows that replacing the self-attention with cross-attention 238
does not degrade the downstream classification performance of the pre-trained model. Moreover, 239
CrossMAE outperforms other self-supervised and masked image modeling baselines, e.g., DINO [ 8], 240
MoCo v3 [14], BEiT [3], and MultiMAE [2]. 241
4.2 Object Detection and Instance Segmentation 242
Setup. We additionally evaluate models pretrained with CrossMAE for object detection and instance 243
segmentation, which require deeper spatial understanding than ImageNet classification. Specifically, 244
we follow ViTDet [ 38], a method that leverages a Vision Transformer backbone for object detection 245
and instance segmentation. We report box AP for object detection and mask AP for instance 246
segmentation, following MAE [ 30]. We compare against supervised pre-training, MoCo-v3 [ 14], 247
BEiT [4], and MAE [30]. 248
Results. As listed in Table 2, CrossMAE, with the default 75% prediction ratio, performs better 249
compared to these baselines, including vanilla MAE. This suggests that similar to MAE, CrossMAE 250
performance on ImageNet positively correlates with instance segmentation. Additionally, Cross- 251
MAE’s downstream performance scales similarly to MAE as the model capacity increases from ViT-B 252
to ViT-L. This observation also supports our hypothesis that partial reconstruction is suprisingly 253
sufficient for learning dense visual representation. 254
7Method Acc. ( %)
MAE 83.0
CrossMAE 83.3
CrossMAE + Self-Attn 83.3
(a) Attention type in decoder
blocks. Adding back self-attention
between mask tokens does not im-
prove performance.Mask Ratio Acc. ( %)
65% 83.5
75% 83.3
85% 83.3
(b) Mask ratio. CrossMAE has
consistent performance across high
mask ratios.Pred. Ratio Acc. ( %)
15% 83.1
25% 83.2
75% 83.3
(c) Prediction ratio. CrossMAE
performs well even when only a
fraction of mask tokens are recon-
structed.
# Feature
Maps FusedAcc.
(%)
1 82.9
3 83.3
6 83.5
12 83.3
(d) Inter-block attention. A com-
bination of six select encoder fea-
ture maps is best.Decoder
DepthAcc.
(%)
1 83.0
4 83.1
8 83.1
12 83.3
(e) Decoder depth. CrossMAE
performance scales with decoder
depth.Image
ResolutionAcc.
(%)
224 83.2
448 84.6
(f) Input resolution. CrossMAE
scales to longer input sequences.
Table 3: Ablations on CrossMAE . We report fine-tuning performance on ImageNet-1K classification with 400
epochs ( i.e., half of the full experiments) with ViT-B/16. MAE performance is reproduced using the official
MAE code. Underline indicates the default setting for CrossMAE. Bold indicates the best hyperparameter
among the tested ones. 1feature map fused (row 1, Table 3(d)) indicates using only the feature from the last
encoder block. We use 25% prediction ratio for both settings in Table 3(f) to accelerate training.
4.3 Ablations 255
Cross-Attention vs Self-Attention. As shown in Table 3a, CrossMAE, with its cross-attention- 256
only decoder, outperforms vanilla MAE in downstream tasks as noted in Section 4.1. Additionally, 257
combining cross-attention with self-attention does not enhance fine-tuning performance, indicating 258
that cross-attention alone is adequate for effective representation learning. 259
Mask Ratio and Prediction Ratio. In our experiments with different mask and prediction ratios ( i.e., 260
the ratio of mask tokens to all tokens and the ratio of reconstructed tokens to all tokens, respectively) 261
(see Table 3b and Table 3c), we found that our method’s performance is not significantly affected by 262
variations in the number of masked tokens. Notably, CrossMAE effectively learns representations 263
by reconstructing as few as 15% of tokens, compared to the 100% required by vanilla MAE, with 264
minimal impact on downstream fine-tuning performance, which shows that partial reconstruction is 265
sufficient for effective representation learning. 266
Inter-block Attention. Our ablation study, detailed in Table 3d, explored the impact of varying the 267
number of encoder feature maps in our inter-block attention mechanism. We found that using only 268
the last feature map slightly lowers performance compared to using all 12. However, even a partial 269
selection of feature maps improves CrossMAE’s performance, with the best results obtained using 6 270
feature maps. This indicates that CrossMAE does not require all features for optimal performance. 271
Decoder Depth. Table 3e shows that a 12-block decoder slightly improves performance compared 272
to shallower ones. Remarkably, CrossMAE achieves similar results to MAE with just one decoder 273
block, demonstrating its efficiency. Our experiments in Figure 7 that models with lower prediction 274
ratios benefit more from deeper decoders. 275
Input Resolution. We extend CrossMAE to longer token lengths by increasing the image resolution 276
with constant patch size. Escalating the resolution from 224 to 448 increases the token length from 277
197 to 785, challenging the scalability of current approaches. Thus, we opt for a CrossMAE variant 278
with a 25% prediction ratio. In Table 3f, we observe that the classification accuracy positively 279
correlates with the input resolution, indicating that CrossMAE can scale to long input sequences. 280
4.4 Training Throughput and Memory Utilization 281
Due to partial reconstruction and confining attention to between mask tokens and visible tokens, 282
CrossMAE improves pre-training efficiency over MAE. Results in Table 10 show that the FLOPs 283
8Figure 5: We visualize the output of each decoder block. (a-b) Different decoder blocks play different roles
in the reconstruction , with most details emerging at later decoder blocks, which confirms the motivation for
inter-block attention. (c) Visualizations of inter-block attention shows that different decoder blocks indeed
attend to feature from different encoder blocks , with later blocks focusing on earlier encoder features to
achieve reconstruction. The reconstructions are unnormalized w.r.t ground truth mean and std for each patch.
reduction does translate to an 1.54 ×training throughput and at least 50% reduction in GPU memory 284
utilization compared to MAE. 285
4.5 Visualizations 286
Visualizing Per-block Reconstruction. Rather than only visualizing the final reconstruction, we 287
have two key observations that allow us to visualize the work performed by each decoder block: 288
1)Transformer blocks have skip connections from their inputs to outputs. 2)The final decoder 289
block’s output goes through a linear reconstruction head to produce the reconstruction. As detailed in 290
Appendix D, we can factor out each block’s contribution in the final reconstruction with linearity. 291
This decomposition allows expressing the reconstruction as an image stack, where summing up all the 292
levels gives us the final reconstruction. As shown in Figure 5 (a,b), we observe that different decoder 293
blocks play different roles in reconstruction, with most details emerging at later decoder blocks. This 294
justifies the need for low-level features from early encoder blocks, motivating inter-block attention. 295
Visualizing Inter-block Attention Maps. As shown in the visualizations of the attention maps of 296
inter-block attention in 5(c), CrossMAE naturally leverages the inter-block attention to allow the later 297
decoder blocks to focus on earlier encoder features to achieve reconstruction and allow the earlier 298
decoder blocks to focus on later encoder features. This underscores the necessity for different decoder 299
blocks to attend to different encoder features, correlating with the performance improvements when 300
inter-block attention is used. 301
5 Discussion and Conclusion 302
In our study, we present a novel understanding of MAE, demonstrating that coherent image recon- 303
struction is achieved not through interactions between patches in the decoder but by learning a global 304
representation within the encoder. Based on this insight, we propose replacing self-attention layers 305
in the decoder with a simple readout function, specifically utilizing cross-attention to aggregate 306
encoder outputs into each input token within the decoder layers independently. This approach, tested 307
across models ranging from ViT-S to ViT-H, achieves comparable or better performance in image 308
classification and instance segmentation with reduced computational requirements, showcasing the 309
potential for more efficient and scalable visual pretraining methods. Our findings underscore the 310
efficacy of the encoder’s global representation learning, paving the way for streamlined decoder 311
architectures in future MAE implementations. CrossMAE’s efficiency and scalability demonstrate 312
potential for large-scale visual pretraining, particularly on underutilized in-the-wild video datasets. 313
However, our work has not yet explored scaling to models larger than ViT-H, the largest model 314
examined in MAE, leaving this for future research. 315
9References 316
[1]Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task 317
masked autoencoders. arXiv:2204.01678 , 2022. 318
[2]Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task 319
masked autoencoders. In European Conference on Computer Vision , pages 348–367. Springer, 2022. 320
[3]Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv 321
preprint arXiv:2106.08254 , 2021. 322
[4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR , 2022. 323
[5]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind 324
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. 325
2020. 326
[6]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey 327
Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision , 328
pages 213–229. Springer, 2020. 329
[7]Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu- 330
pervised learning of visual features by contrasting cluster assignments. Advances in neural information 331
processing systems , 33:9912–9924, 2020. 332
[8]Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand 333
Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF 334
international conference on computer vision , pages 9650–9660, 2021. 335
[9]Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, and Dit-Yan Yeung. Mixed autoencoder for 336
self-supervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer 337
Vision and Pattern Recognition (CVPR) , pages 22742–22751, 2023. 338
[10] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. 339
Generative pretraining from pixels. 2020. 340
[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for 341
contrastive learning of visual representations. In International conference on machine learning , pages 342
1597–1607. PMLR, 2020. 343
[12] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive 344
learning. arXiv preprint arXiv:2003.04297 , 2020. 345
[13] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision 346
transformers, 2021. 347
[14] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision 348
transformers. arXiv preprint arXiv:2104.02057 , 2021. 349
[15] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need 350
for semantic segmentation. 2021. 351
[16] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked- 352
attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference 353
on computer vision and pattern recognition , pages 1290–1299, 2022. 354
[17] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data 355
augmentation with a reduced search space. arxiv e-prints, page. arXiv preprint arXiv:1909.13719 , 4, 2019. 356
[18] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. 357
[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- 358
tional transformers for language understanding. 2019. 359
[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas 360
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 361
16x16 words: Transformers for image recognition at scale. In ICLR , 2020. 362
10[21] Yuxin Fang, Li Dong, Hangbo Bao, Xinggang Wang, and Furu Wei. Corrupted image modeling for 363
self-supervised visual pre-training. In The Eleventh International Conference on Learning Representations , 364
2023. 365
[22] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal 366
learners. In Advances in Neural Information Processing Systems , 2022. 367
[23] Xinyang Geng, Hao Liu, Lisa Lee, Dale Schuurams, Sergey Levine, and Pieter Abbeel. Multimodal 368
masked autoencoders learn transferable representations. arXiv preprint arXiv:2205.14204 , 2022. 369
[24] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew 370
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. 371
arXiv:1706.02677 , 2017. 372
[25] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew 373
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv 374
preprint arXiv:1706.02677 , 2017. 375
[26] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, 376
Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your 377
own latent-a new approach to self-supervised learning. Advances in neural information processing systems , 378
33:21271–21284, 2020. 379
[27] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and 380
Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013 , 381
2016. 382
[28] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese masked autoencoders. arXiv preprint 383
arXiv:2305.14344 , 2023. 384
[29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised 385
visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern 386
recognition , pages 9729–9738, 2020. 387
[30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders 388
are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 389
Recognition (CVPR) , pages 16000–16009, 2022. 390
[31] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic 391
depth. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 392
11–14, 2016, Proceedings, Part IV 14 , pages 646–661. Springer, 2016. 393
[32] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, 394
Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture 395
for structured inputs & outputs. arXiv preprint arXiv:2107.14795 , 2021. 396
[33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, 397
Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. 398
InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 4015–4026, 399
2023. 400
[34] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 401
Albert: A lite bert for self-supervised learning of language representations. In International Conference on 402
Learning Representations , 2020. 403
[35] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding 404
beyond pixels using a learned similarity metric. In International conference on machine learning , pages 405
1558–1566. PMLR, 2016. 406
[36] Jin Li, Yaoming Wang, XIAOPENG ZHANG, Yabo Chen, Dongsheng Jiang, Wenrui Dai, Chenglin Li, 407
Hongkai Xiong, and Qi Tian. Progressively compressed auto-encoder for self-supervised representation 408
learning. In The Eleventh International Conference on Learning Representations , 2023. 409
[37] Tianhong Li, Huiwen Chang, Shlok Kumar Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. 410
Mage: Masked generative encoder to unify representation learning and image synthesis. arXiv preprint 411
arXiv:2211.09117 , 2022. 412
[38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones 413
for object detection. In European Conference on Computer Vision , pages 280–296. Springer, 2022. 414
11[39] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image 415
pre-training via masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 416
Recognition , pages 23390–23400, 2023. 417
[40] Jihao Liu, Xin Huang, Jinliang Zheng, Yu Liu, and Hongsheng Li. Mixmae: Mixed and masked autoencoder 418
for efficient pretraining of hierarchical vision transformers. arXiv:2205.13137 , 2022. 419
[41] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, 420
Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv 421
preprint arXiv:1907.11692 , 2019. 422
[42] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2017. 423
[43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint 424
arXiv:1711.05101 , 2017. 425
[44] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive 426
coding. arXiv preprint arXiv:1807.03748 , 2018. 427
[45] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre 428
Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual 429
features without supervision. arXiv preprint arXiv:2304.07193 , 2023. 430
[46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: 431
Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern 432
recognition , pages 2536–2544, 2016. 433
[47] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding 434
by generative pre-training. 2018. 435
[48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language 436
models are unsupervised multitask learners. 2019. 437
[49] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world 438
robot learning with masked visual pre-training. In Conference on Robot Learning , pages 416–426. PMLR, 439
2023. 440
[50] Andreas Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas 441
Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. Transactions 442
on Machine Learning Research , 2022. 443
[51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the 444
inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and 445
pattern recognition , pages 2818–2826, 2016. 446
[52] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient 447
learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems , 448
2022. 449
[53] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Piotr Bojanowski, Armand Joulin, Gabriel Synnaeve, 450
and Hervé Jégou. Augmenting convolutional networks with attention-based aggregation, 2021. 451
[54] Shubham Tulsiani and Abhinav Gupta. Pixeltransformer: Sample conditioned signal generation. In 452
Proceedings of the 38th International Conference on Machine Learning , pages 10455–10464. PMLR, 453
2021. 454
[55] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional 455
image generation with pixelcnn decoders. Advances in neural information processing systems , 29, 2016. 456
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz 457
Kaiser, and Illia Polosukhin. Attention is all you need. 2017. 458
[57] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and Léon 459
Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local 460
denoising criterion. Journal of machine learning research , 11(12), 2010. 461
[58] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. 462
Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF 463
Conference on Computer Vision and Pattern Recognition , pages 14549–14560, 2023. 464
12[59] Xudong Wang, Ziwei Liu, and Stella X Yu. Unsupervised feature learning by cross-level instance-group 465
discrimination. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 466
pages 12586–12595, 2021. 467
[60] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang 468
Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoder. In ICCV , 2023. 469
[61] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. 470
Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference 471
on Computer Vision and Pattern Recognition (CVPR) , pages 9653–9663, 2022. 472
[62] Hongwei Xue, Peng Gao, Hongyang Li, Yu Qiao, Hao Sun, Houqiang Li, and Jiebo Luo. Stare at what 473
you see: Masked image modeling without reconstruction. In Proceedings of the IEEE/CVF Conference on 474
Computer Vision and Pattern Recognition , pages 22732–22741, 2023. 475
[63] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. 476
Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the 477
IEEE/CVF international conference on computer vision , pages 6023–6032, 2019. 478
[64] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk 479
minimization. In International Conference on Learning Representations , 2018. 480
[65] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image 481
bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832 , 2021. 482
13A Implementation details 483
A.1 Attention Calculation 484
To compare the attention values for mask tokens in vanilla MAE (Figure 1), we trained a ViT-B/16 485
MAE for 800 epochs using the default hyperparameters provided in [ 30]. For each image, we 486
randomly generate a 75% binary mask ( m) for all tokens, with mi= 1representing a token being 487
masked and mi= 0 otherwise. During the forward pass of the decoder, for each self-attention 488
operation, the attention map is stored. This means that for the default MAE, a total of 8 attention 489
maps, each with 16 attention heads are stored. Based on the mask pattern, we calculate the outer 490
product ( m·m⊤) for the self-attention among mask tokens, and m·(1−m⊤)for the cross-attention 491
from the mask token to the visible tokens. We then calculate the average across all feature maps 492
and attention heads for self-attention and cross-attention to get the image average values. Lastly, we 493
averaged across the entire ImageNet validation set to obtain the final values. 494
A.2 Inter-Block Attention 495
We tried a few implementations for inter-block attention (IBA) and found the following implementa- 496
tion to be the fastest and most memory-efficient. In this implementation, we combine inter-block 497
attention for all encoder layers as a single forward pass of a linear layer. For each decoder block, 498
we index into the output tensor to extract the corresponding feature map, and a layer norm will be 499
applied before the feature map is fed into the decoder block. Other alternatives we tried include 1) 500
performing separate inter-block attentions before each decoder block, and 2) 1x1 convolution on the 501
stacked encoder feature maps. 502
In MAE, there exists a layer norm after the last encoder feature map before feeding into the decoder. 503
In our implementation, we only add layer norm after inter-block attention. We find that adding 504
an additional layer norm before inter-block attention to each encoder feature map does not lead to 505
improvements in model performance but will significantly increase GPU memory usage. 506
The pseudo-code of inter-block attention is the following: 507
1class InterBlockAttention (): 508
2 def __init__ (self , num_feat_maps , decoder_depth ): 509
3 self . linear = Linear ( num_feat_maps , decoder_depth , bias = False ) 510
4 std_dev = 1. / sqrt ( num_feat_maps ) 511
5 init . normal_ ( self . linear . weight , mean =0. , std= std_dev ) 512
6513
7 def forward (self , feature_maps : list ): 514
8 """ 515
9 feature_maps : a list of length num_feat_maps , each with 516
dimension 517
10 Batch Size x Num. Tokens x Embedding Dim. 518
11 """ 519
12 stacked_feature_maps = stack ( feature_maps , dim = -1) 520
13 return self . linear ( stacked_feature_maps ) 521
Additionally, we further investigate the importance of using a cross-attention decoder, where each 522
decoder block can use different feature maps from the encoder for decoding. In this experiment, we 523
incorporated IBA into MAE, which uses only a self-attention decoder. Specifically, we concatenate 524
the interblock attention features with the masked tokens. We then feed the combined features into 525
MAE’s self-attention decoder. We pre-trained the model and finetuned it for Imagenet classification. 526
The results are presented in Table. 4, where all models are pre-trained for 400 epochs. We observe that 527
inter-block attention has negligible performance improvements for MAE, potentially because MAE 528
only takes in one feature map in its decoder. In contrast, inter-block attention allows cross-attention 529
layers in CrossMAE to attend to features from different encoder blocks, thanks to its decoupling of 530
queries with keys and values. 531
A.3 Ablation that Adds Self-Attention 532
In Section 4.3 (a), we propose adding self-attention back to CrossMAE as an ablation. In that 533
particular ablation study, we analyze the effect of self-attention between the masked tokens, which 534
14Method Acc. (%)
MAE 83.0
MAE + IBA 83.0
CrossMAE (25%) 83.2
CrossMAE (75%) 83.3
Table 4: For MAE, inter-block attention has very small differences in terms of finetuning performance, potentially
due to the fact that MAE’s decoder only takes in one set of features.
can be used to improve the consistency for reconstruction. Specifically, we modify the formulation in 535
the original transformer paper [ 56], where the mask/query tokens are first passed through a multi- 536
head self-attention and a residual connection before being used in the multiheaded cross-attention 537
with the features from the encoder. The primary difference with the vanilla transformer decoder 538
implementation [ 56] is we do not perform casual masking in the multi-head self-attention. Please 539
reference Figure 6 for a more visual presentation of the method. 540
Figure 6: Modification for self-attention ablation
A.4 Ablation on Inter-block Attention 541
In Table 3d, the following cases are considered. 1 feature map (row 1) does not use inter-block 542
attention. Each decoder block only takes the last feature map from the encoder as the keys and values. 543
For scenarios where more than one feature map is used, the output of the patch embedding (input to 544
the ViT) is also used. 545
In addition to the simple design of inter-block attention proposed above, we also experimented 546
with a variant of inter-block attention by further parameterizing the attention with linear projections. 547
Specifically, rather than directly performing weighted sum aggregation to form the features for each 548
cross-attention layer in the decoder, we added a linear projection for each encoder feature before the 549
feature aggregation. We denote this variant as CrossMAE+LP . As shown in the Table. 5 (with ViT-B 550
pre-trained for 400 epochs, consistent with the setting in Table. 3), adding a linear projection slightly 551
improves the performance. This indicates that it is possible to design variants of readout functions, 552
such as through improved inter-block attention, to improve the feature quality of CrossMAE. 553
Method Acc. (%)
CrossMAE 83.3
CrossMAE + LP 83.5
Table 5: Improving inter-block attention by adding linear projections to the input features. The performance
gain indicates that it is possible to design variants of readout functions to improve CrossMAE.
15A.5 Hyperparameters 554
Pre-training : The default setting is in Table 6, which is consistent with the official MAE [ 30] 555
implementation. As mentioned in Sec. 3.4, we scale the learning rate by the ratio between mask ratio 556
(p) and prediction ratio ( γ) to ensure the variance of the loss is consistent with [ 30]. Additionally, we 557
use the linear learning rate scaling rule [ 25]. This results in lr=γ∗base_lr ∗batchsize /(256∗p). 558
For Table 1, we use 12 decoder blocks, with mask ratio and prediction ratio both 75%, and interblock 559
attention takes in all encoder feature maps. For the 400 epochs experiments in Table 2, we scale the 560
warm-up epochs correspondingly. Other hyperparameters, such as decoder block width, are the same 561
as MAE. 562
Finetuning : We use the same hyperparameters as MAE finetuning. We use global average pooling 563
for finetuning. In MAE, the layer norm for the last encoder feature map is removed for finetuning, 564
which is consistent with our pretraining setup. Please refer to Table 7 for more detail.
Config Value
optimizer AdamW [43]
base learning rate 1.5e-4
learning rate schedule cosine decay [42]
batch size 4096
weight decay 0.05
optimizer momentum β1, β2= 0.9, 0.95 [10]
warm up epoch [24] 20, 40
total epochs 400, 800
augmentationRandomResizedCrop,
RandomHorizontalFlip
Table 6: Pretraining Hyperparameters
565
A.6 Compute Infrastructure 566
Each of the pretraining and finetuning experiments is run on 2 or 4 NVIDIA A100 80GB GPUs. The 567
batch size per GPU is scaled accordingly and we use gradient accumulation to avoid out-of-memory 568
errors. ViTDet [ 38] experiments use a single machine equipped with 8 NVIDIA A100 (80GB) GPUs. 569
We copy the datasets to the shared memory on the machines to accelerate dataloading. We use 570
FlashAttention-2 [18] to accelerate attention calculation. 571
Config Value
optimizer AdamW
base learning rate 1e-3
learning rate schedule cosine decay
batch size 1024
weight decay 0.05
optimizer momentum β1, β2= 0.9, 0.999
warm up epoch 5
total epochs 100 (B), 50 (L)
augmentation RandAug (9, 0.5) [17]
label smoothing [51] 0.1
mixup [64] 0.8
cutmix [63] 1.0
drop path [31] 0.1
Table 7: Finetuning Hyperparameters
16B Additional Experiments 572
B.1 Linear Probe 573
We provide linear probe comparisons (at 800 epochs) for ViT-Small and ViT-Base in Table. 8. For both 574
of these experiments, we run CrossMAE with a prediction ratio of 75% (reconstruction of all masked 575
patches). These results show that CrossMAE achieves slightly better linear probe performance than 576
vanilla MAE. 577
Method ViT-S ViT-B
MAE 49.7 65.1
CrossMAE 51.5 65.4
Table 8: Linear probe experiments of CrossMAE.
B.2 Masking Strategy 578
Method Acc. (%)
Grid Masking 83.2
Random Masking 83.3
Table 9: Ablation of masking strategies.
Similar to MAE [ 30], we here ablate the masking pattern. Instead of random masking, we perform 579
grid-wise sampling that “keeps one of every four patches” (see MAE Figure 6). The finetuning 580
performance is reported in Table. 9 for ViT-B (at 400 epochs), which shows that grid masking does 581
not lead to additional improvements in downstream performance. 582
C Runtime and GPU Memory Comparisons with MAE 583
MethodMemory
(MB/GPU)Runtime
(min/epoch)Acc.
(%)
MAE OOM ( >81920) 5.19∗83.3
CrossMAE 41177 3.38 83.5
Table 10: CrossMAE greatly improves the training
throughput and reduces the memory requirements ,
lowering the barrier for masked pretraining. Statistics
are measured on 2 NVIDIA A100 80GB GPUs. Please
refer to Appendix C for comparison details.∗: MAE’s
default batch size exceeds the capacity of 4 GPUs, re-
quiring gradient accumulation for runtime measurement.
Figure 7: We compare ViT-B which is pre-trained
for 800 epochs with different variants of Cross-
MAE v.s. MAE. For CrossMAE, we vary the pre-
diction ratio pand number of decoder blocks n,
and we denote each as ( p,n). While all exper-
iments are run with inter-block attention, Cross-
MAE has lower decoder FLOPS than MAE [ 30]
and performs on par or better.
All experiments in Table 10 are conducted on a server with 4 NVIDIA A100 (80GB) GPUs, with the 584
standard hyperparameters provided above for pretraining. NVLink is equipped across the GPUs. We 585
use the default setting for MAE and set the global batch size to 4096. For CrossMAE, we also use 586
the default setting with a prediction ratio 0.25, and this takes around 41GB memory per GPU without 587
gradient accumulation (i.e., local batch size is set to 1024 samples per GPU). However, the same 588
local batch size results in out-of-memory (OOM), which indicates that the total memory requirement 589
is larger than the available memory for each GPU (80GB). To run MAE on same hardware, we 590
thus employ gradient accumulation with a local batch size of 512 to maintain the global batch size. 591
The benchmark runs each method and measures the average per epoch runtime as well as the max 592
memory allocation for 10 training epochs. Our experiments in Figure 7 show that models with lower 593
prediction ratios benefit more from deeper decoders. Our model performs on par or better when 594
compared to MAE, with up to 3.7 ×lower decoder FLOPS. 595
17D Visualizing the Contributions per Decoder Block 596
We propose a more fine-grained visualization approach that allows us to precisely understand the 597
effect and contribution of each decoder block. 598
Two key observations enable per-block visualization: 1)Transformer blocks have residual connections 599
from their inputs to outputs. Let fibe the output and gi(·)the residual function of decoder i, so 600
fi=fi−1+gi(fi−1).2)The final decoder block’s output goes through a reconstruction head h, 601
which is linear, consisting of a layer-norm and a linear layer, to produce the reconstruction. With 602
Das the decoder depth, f0the initial input, and ythe final output, yis recursively defined as 603
y=h(fD−1+gD(fD−1)), which simplifies due to the linearity of h: 604
y=h(f0+g1(f0) +···+gD(fD−1))
= h(f0)|{z}
Pos Embed. + Mask Token+h(g1(f0))|{z}
Block 1+···+h(gD(fD−1))|{z}
Block D
This decomposition allows us to express the reconstruction as an image stack, where the sum of all 605
the levels gives us the final reconstruction. We present the visualization in Figure 5. 606
18NeurIPS Paper Checklist 607
1.Claims 608
Question: Do the main claims made in the abstract and introduction accurately reflect the 609
paper’s contributions and scope? 610
Answer: [Yes] 611
Justification: The claims in the abstract are justified in the method and the experiments 612
section. 613
Guidelines: 614
•The answer NA means that the abstract and introduction do not include the claims 615
made in the paper. 616
•The abstract and/or introduction should clearly state the claims made, including the 617
contributions made in the paper and important assumptions and limitations. A No or 618
NA answer to this question will not be perceived well by the reviewers. 619
•The claims made should match theoretical and experimental results, and reflect how 620
much the results can be expected to generalize to other settings. 621
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 622
are not attained by the paper. 623
2.Limitations 624
Question: Does the paper discuss the limitations of the work performed by the authors? 625
Answer: [Yes] 626
Justification: The limitations of the work have been discussed in the Discussion and Conclu- 627
sion section. 628
Guidelines: 629
•The answer NA means that the paper has no limitation while the answer No means that 630
the paper has limitations, but those are not discussed in the paper. 631
• The authors are encouraged to create a separate "Limitations" section in their paper. 632
•The paper should point out any strong assumptions and how robust the results are to 633
violations of these assumptions (e.g., independence assumptions, noiseless settings, 634
model well-specification, asymptotic approximations only holding locally). The authors 635
should reflect on how these assumptions might be violated in practice and what the 636
implications would be. 637
•The authors should reflect on the scope of the claims made, e.g., if the approach was 638
only tested on a few datasets or with a few runs. In general, empirical results often 639
depend on implicit assumptions, which should be articulated. 640
•The authors should reflect on the factors that influence the performance of the approach. 641
For example, a facial recognition algorithm may perform poorly when image resolution 642
is low or images are taken in low lighting. Or a speech-to-text system might not be 643
used reliably to provide closed captions for online lectures because it fails to handle 644
technical jargon. 645
•The authors should discuss the computational efficiency of the proposed algorithms 646
and how they scale with dataset size. 647
•If applicable, the authors should discuss possible limitations of their approach to 648
address problems of privacy and fairness. 649
•While the authors might fear that complete honesty about limitations might be used by 650
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 651
limitations that aren’t acknowledged in the paper. The authors should use their best 652
judgment and recognize that individual actions in favor of transparency play an impor- 653
tant role in developing norms that preserve the integrity of the community. Reviewers 654
will be specifically instructed to not penalize honesty concerning limitations. 655
3.Theory Assumptions and Proofs 656
Question: For each theoretical result, does the paper provide the full set of assumptions and 657
a complete (and correct) proof? 658
19Answer: [NA] 659
Justification: This work offers observations and hypotheses justified with empirical results. 660
Guidelines: 661
• The answer NA means that the paper does not include theoretical results. 662
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 663
referenced. 664
•All assumptions should be clearly stated or referenced in the statement of any theorems. 665
•The proofs can either appear in the main paper or the supplemental material, but if 666
they appear in the supplemental material, the authors are encouraged to provide a short 667
proof sketch to provide intuition. 668
•Inversely, any informal proof provided in the core of the paper should be complemented 669
by formal proofs provided in appendix or supplemental material. 670
• Theorems and Lemmas that the proof relies upon should be properly referenced. 671
4.Experimental Result Reproducibility 672
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 673
perimental results of the paper to the extent that it affects the main claims and/or conclusions 674
of the paper (regardless of whether the code and data are provided or not)? 675
Answer: [Yes] 676
Justification: Our code, which reproduces our results, is provided through an anonymous 677
link in the abstract. 678
Guidelines: 679
• The answer NA means that the paper does not include experiments. 680
•If the paper includes experiments, a No answer to this question will not be perceived 681
well by the reviewers: Making the paper reproducible is important, regardless of 682
whether the code and data are provided or not. 683
•If the contribution is a dataset and/or model, the authors should describe the steps taken 684
to make their results reproducible or verifiable. 685
•Depending on the contribution, reproducibility can be accomplished in various ways. 686
For example, if the contribution is a novel architecture, describing the architecture fully 687
might suffice, or if the contribution is a specific model and empirical evaluation, it may 688
be necessary to either make it possible for others to replicate the model with the same 689
dataset, or provide access to the model. In general. releasing code and data is often 690
one good way to accomplish this, but reproducibility can also be provided via detailed 691
instructions for how to replicate the results, access to a hosted model (e.g., in the case 692
of a large language model), releasing of a model checkpoint, or other means that are 693
appropriate to the research performed. 694
•While NeurIPS does not require releasing code, the conference does require all submis- 695
sions to provide some reasonable avenue for reproducibility, which may depend on the 696
nature of the contribution. For example 697
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 698
to reproduce that algorithm. 699
(b)If the contribution is primarily a new model architecture, the paper should describe 700
the architecture clearly and fully. 701
(c)If the contribution is a new model (e.g., a large language model), then there should 702
either be a way to access this model for reproducing the results or a way to reproduce 703
the model (e.g., with an open-source dataset or instructions for how to construct 704
the dataset). 705
(d)We recognize that reproducibility may be tricky in some cases, in which case 706
authors are welcome to describe the particular way they provide for reproducibility. 707
In the case of closed-source models, it may be that access to the model is limited in 708
some way (e.g., to registered users), but it should be possible for other researchers 709
to have some path to reproducing or verifying the results. 710
5.Open access to data and code 711
20Question: Does the paper provide open access to the data and code, with sufficient instruc- 712
tions to faithfully reproduce the main experimental results, as described in supplemental 713
material? 714
Answer: [Yes] 715
Justification: Our method is evaluated on open datasets that are publicly available. 716
Guidelines: 717
• The answer NA means that paper does not include experiments requiring code. 718
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 719
public/guides/CodeSubmissionPolicy ) for more details. 720
•While we encourage the release of code and data, we understand that this might not be 721
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 722
including code, unless this is central to the contribution (e.g., for a new open-source 723
benchmark). 724
•The instructions should contain the exact command and environment needed to run to 725
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 726
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 727
•The authors should provide instructions on data access and preparation, including how 728
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 729
•The authors should provide scripts to reproduce all experimental results for the new 730
proposed method and baselines. If only a subset of experiments are reproducible, they 731
should state which ones are omitted from the script and why. 732
•At submission time, to preserve anonymity, the authors should release anonymized 733
versions (if applicable). 734
•Providing as much information as possible in supplemental material (appended to the 735
paper) is recommended, but including URLs to data and code is permitted. 736
6.Experimental Setting/Details 737
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 738
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 739
results? 740
Answer: [Yes] 741
Justification: We follow the hyperparam selection from MAE. The hyperparams introduced 742
by our work, such as the mask ratio and the number of feature maps used, are ablated. 743
Guidelines: 744
• The answer NA means that the paper does not include experiments. 745
•The experimental setting should be presented in the core of the paper to a level of detail 746
that is necessary to appreciate the results and make sense of them. 747
•The full details can be provided either with the code, in appendix, or as supplemental 748
material. 749
7.Experiment Statistical Significance 750
Question: Does the paper report error bars suitably and correctly defined or other appropriate 751
information about the statistical significance of the experiments? 752
Answer: [No] 753
Justification: Error bars are not reported because they would be too computationally expen- 754
sive. 755
Guidelines: 756
• The answer NA means that the paper does not include experiments. 757
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 758
dence intervals, or statistical significance tests, at least for the experiments that support 759
the main claims of the paper. 760
•The factors of variability that the error bars are capturing should be clearly stated (for 761
example, train/test split, initialization, random drawing of some parameter, or overall 762
run with given experimental conditions). 763
21•The method for calculating the error bars should be explained (closed form formula, 764
call to a library function, bootstrap, etc.) 765
• The assumptions made should be given (e.g., Normally distributed errors). 766
•It should be clear whether the error bar is the standard deviation or the standard error 767
of the mean. 768
•It is OK to report 1-sigma error bars, but one should state it. The authors should 769
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 770
of Normality of errors is not verified. 771
•For asymmetric distributions, the authors should be careful not to show in tables or 772
figures symmetric error bars that would yield results that are out of range (e.g. negative 773
error rates). 774
•If error bars are reported in tables or plots, The authors should explain in the text how 775
they were calculated and reference the corresponding figures or tables in the text. 776
8.Experiments Compute Resources 777
Question: For each experiment, does the paper provide sufficient information on the com- 778
puter resources (type of compute workers, memory, time of execution) needed to reproduce 779
the experiments? 780
Answer: [Yes] 781
Justification: We described the compute requirements in Appendix A.6. We do not use 782
GPUs from a cloud provider. 783
Guidelines: 784
• The answer NA means that the paper does not include experiments. 785
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 786
or cloud provider, including relevant memory and storage. 787
•The paper should provide the amount of compute required for each of the individual 788
experimental runs as well as estimate the total compute. 789
•The paper should disclose whether the full research project required more compute 790
than the experiments reported in the paper (e.g., preliminary or failed experiments that 791
didn’t make it into the paper). 792
9.Code Of Ethics 793
Question: Does the research conducted in the paper conform, in every respect, with the 794
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 795
Answer: [Yes] 796
Justification: The research conforms to the NeurIPS Code of Ethics. 797
Guidelines: 798
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 799
•If the authors answer No, they should explain the special circumstances that require a 800
deviation from the Code of Ethics. 801
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 802
eration due to laws or regulations in their jurisdiction). 803
10.Broader Impacts 804
Question: Does the paper discuss both potential positive societal impacts and negative 805
societal impacts of the work performed? 806
Answer: [Yes] 807
Justification: This paper aims to advance the field of self-supervised learning. Like other self- 808
supervised learning methods, our work may have various societal implications. However, 809
we do not believe any specific consequences need to be highlighted in this context. 810
Guidelines: 811
• The answer NA means that there is no societal impact of the work performed. 812
•If the authors answer NA or No, they should explain why their work has no societal 813
impact or why the paper does not address societal impact. 814
22•Examples of negative societal impacts include potential malicious or unintended uses 815
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 816
(e.g., deployment of technologies that could make decisions that unfairly impact specific 817
groups), privacy considerations, and security considerations. 818
•The conference expects that many papers will be foundational research and not tied 819
to particular applications, let alone deployments. However, if there is a direct path to 820
any negative applications, the authors should point it out. For example, it is legitimate 821
to point out that an improvement in the quality of generative models could be used to 822
generate deepfakes for disinformation. On the other hand, it is not needed to point out 823
that a generic algorithm for optimizing neural networks could enable people to train 824
models that generate Deepfakes faster. 825
•The authors should consider possible harms that could arise when the technology is 826
being used as intended and functioning correctly, harms that could arise when the 827
technology is being used as intended but gives incorrect results, and harms following 828
from (intentional or unintentional) misuse of the technology. 829
•If there are negative societal impacts, the authors could also discuss possible mitigation 830
strategies (e.g., gated release of models, providing defenses in addition to attacks, 831
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 832
feedback over time, improving the efficiency and accessibility of ML). 833
11.Safeguards 834
Question: Does the paper describe safeguards that have been put in place for responsible 835
release of data or models that have a high risk for misuse (e.g., pretrained language models, 836
image generators, or scraped datasets)? 837
Answer: [NA] 838
Justification: The paper does not pose such risks. 839
Guidelines: 840
• The answer NA means that the paper poses no such risks. 841
•Released models that have a high risk for misuse or dual-use should be released with 842
necessary safeguards to allow for controlled use of the model, for example by requiring 843
that users adhere to usage guidelines or restrictions to access the model or implementing 844
safety filters. 845
•Datasets that have been scraped from the Internet could pose safety risks. The authors 846
should describe how they avoided releasing unsafe images. 847
•We recognize that providing effective safeguards is challenging, and many papers do 848
not require this, but we encourage authors to take this into account and make a best 849
faith effort. 850
12.Licenses for existing assets 851
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 852
the paper, properly credited and are the license and terms of use explicitly mentioned and 853
properly respected? 854
Answer: [Yes] 855
Justification: The code and datasets used in this work follow the original MAE work. 856
Guidelines: 857
• The answer NA means that the paper does not use existing assets. 858
• The authors should cite the original paper that produced the code package or dataset. 859
•The authors should state which version of the asset is used and, if possible, include a 860
URL. 861
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 862
•For scraped data from a particular source (e.g., website), the copyright and terms of 863
service of that source should be provided. 864
•If assets are released, the license, copyright information, and terms of use in the 865
package should be provided. For popular datasets, paperswithcode.com/datasets 866
has curated licenses for some datasets. Their licensing guide can help determine the 867
license of a dataset. 868
23•For existing datasets that are re-packaged, both the original license and the license of 869
the derived asset (if it has changed) should be provided. 870
•If this information is not available online, the authors are encouraged to reach out to 871
the asset’s creators. 872
13.New Assets 873
Question: Are new assets introduced in the paper well documented and is the documentation 874
provided alongside the assets? 875
Answer: [NA] 876
Justification: The paper does not release new assets. 877
Guidelines: 878
• The answer NA means that the paper does not release new assets. 879
•Researchers should communicate the details of the dataset/code/model as part of their 880
submissions via structured templates. This includes details about training, license, 881
limitations, etc. 882
•The paper should discuss whether and how consent was obtained from people whose 883
asset is used. 884
•At submission time, remember to anonymize your assets (if applicable). You can either 885
create an anonymized URL or include an anonymized zip file. 886
14.Crowdsourcing and Research with Human Subjects 887
Question: For crowdsourcing experiments and research with human subjects, does the paper 888
include the full text of instructions given to participants and screenshots, if applicable, as 889
well as details about compensation (if any)? 890
Answer: [NA] 891
Justification: The paper does not involve crowdsourcing or research with human subjects. 892
Guidelines: 893
•The answer NA means that the paper does not involve crowdsourcing nor research with 894
human subjects. 895
•Including this information in the supplemental material is fine, but if the main contribu- 896
tion of the paper involves human subjects, then as much detail as possible should be 897
included in the main paper. 898
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 899
or other labor should be paid at least the minimum wage in the country of the data 900
collector. 901
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 902
Subjects 903
Question: Does the paper describe potential risks incurred by study participants, whether 904
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 905
approvals (or an equivalent approval/review based on the requirements of your country or 906
institution) were obtained? 907
Answer: [NA] 908
Justification: The paper does not involve crowdsourcing or research with human subjects. 909
Guidelines: 910
•The answer NA means that the paper does not involve crowdsourcing nor research with 911
human subjects. 912
•Depending on the country in which research is conducted, IRB approval (or equivalent) 913
may be required for any human subjects research. If you obtained IRB approval, you 914
should clearly state this in the paper. 915
•We recognize that the procedures for this may vary significantly between institutions 916
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 917
guidelines for their institution. 918
•For initial submissions, do not include any information that would break anonymity (if 919
applicable), such as the institution conducting the review. 920
24