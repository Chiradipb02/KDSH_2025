decoupleQ: Towards 2-bit Post-Training Uniform
Quantization via decoupling Parameters into Integer
and Floating Points
Anonymous Author(s)
Affiliation
Address
email
Abstract
Quantization emerges as one of the most promising compression technologies for 1
deploying efficient large models in recent years. However, existing quantization 2
schemes suffer from significant accuracy degradation at very low bits, or require 3
some additional computational overhead when deployed, making it difficult to be 4
applied to large-scale applications in industry. In this paper, we propose decoupleQ, 5
achieving a substantial increase in model accuracy, especially at very low bits. 6
decoupleQ abandons the traditional heuristic quantization paradigm and decouples 7
the model parameters into integer and floating-point parts, then transforming the 8
quantization problem into a mathematical constrained optimization problem, which 9
is then solved alternatively by off-the-shelf solution methods. decoupleQ gets rid 10
of any tricks for dealing with outliers, sensitive channels, etc., and focuses only 11
on the basic optimization objective to achieve high model accuracy on extreme 12
low bit quantization. Quantization via decoupleQ is linear and uniform, making 13
it hardware-friendlier than non-uniform counterpart, and enabling the idea to be 14
migrated to high-bit quantization to enhance its robustness. 15
decoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantization of 16
large speech models in our company. The code (including the W2 CUDA kernels) 17
is attached and will be made public. 18
1 Introduction 19
Serving large models ( 1;2;37;38) in industry is budget-consuming because of the huge computa- 20
tional, IO and storage cost. Model compression ( 10;11;16) has therefore become a necessity to 21
alleviate this pain. Among which, Post-Training Quantization (PTQ) ( 9;26) has gained more and 22
more popularity among researchers and engineers because it does not require heavy GPU-hours 23
training with labeled datasets. 24
However, previous quantization schemes remain confined within the traditional heuristic quantization 25
paradigm, e.g., how to deal with outliers ( 33;35), how to deal with sensitive channels ( 6), how 26
to determine the clipping range ( 29), and so on. These methods have achieved some success, but 27
the quantization at extreme low bit often suffers from significant accuracy degradation, thus failing 28
to meet the launching requirements of industrial practice. There are also some other options to 29
mitigate the accuracy loss. QuIP ( 4) pushes the accuracy limits of 2-bit quantization and can achieve 30
performance close to fp16/bf16. However, compared to traditional quantization schemes, its inference 31
imposes an additional burden due to the need to multiply two random orthogonal matrices to de- 32
quant the weights. N2UQ ( 20) fit the real-value distribution with non-uniform grids then quantize 33
them into equidistant output levels. But it need to train to get the input thresholds. SpQR ( 7) 34
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.and SqueezeLLM ( 14) use mixed-precision quantization or non-uniform scheme to safeguard the 35
important channels, but they need customized hardware support. 36
In order to alleviate the above pains in industry, we proposed decoupleQ, which completely abandons 37
the traditional heuristic quantization paradigm and instead decouples the model parameters into 38
integer and floating point parts, then transforming the quantization problem into a mathematical 39
constrained optimization problem, which is then solved alternatively by off-the-shelf solution methods. 40
The integer part contains the main weights of the model, and the floating-point part contains scales 41
and zero points induced via quantization. decoulpeQ starts from an abstract objective function and 42
thus does not need any tricks to deal with the minutiae of traditional quantization paradigm, such as 43
outlier, salient weights ( 19), and so on. Quantization via decoupleQ is linear and uniform, making it 44
hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit 45
quantization to enhance its robustness. 46
decoupleQ contains two stages: 1. layer-wise minimization, defined in Eq. 1, is used to optimize 47
the integer part and the floating-point part; 2. block-wise minimization, defined in Eq. 2, is used to 48
further optimize the floating-point part while freezing the integer part1. 49
Layer-wise minimization is to minimize the ℓ2loss of the outputs between pre- and post-quantization 50
for a linear layer: 51
min
fW∥XfW−XW 0∥2
2 (1)
where X∈Rbatch ×dinis the input of this layer, W0∈Rdin×doutis the pre-trained full precision 52
weight, dinanddoutare the input and output dimensions respectively. The objective is to find a 53
matrixfWwith quantized-then-dequantized elements to minimize Eq. 1. 54
Some works ( 4;8;9;13;25) started from Eq. 1 and achieved some success, but they still haven’t 55
thought outside the box of traditional quantization. GPTQ series ( 8;9) fake-quantize the first element 56
ofW0and then update the the remaining elements so as to keep Eq. 1 minimized. This process is 57
then continued element by element until all elements are fake-quantized. However, on the one hand, 58
they do not give any indication of how scale and zero point should be calculated, and on the other 59
hand, the optimization problem formulated for updating the remaining elements is unconstrained 60
(explained in detail later). decoupleQ models Eq. 1 as a constrained optimization problem, as shown 61
in Eq. 6. It no longer needs to pay attention to some of the minutiae unique to quantization, such as 62
outliers, clipping threshold, etc., but abstracts the essence of the problem from a higher level. 63
In the second stage, block-wise minimization is used to further improve the model accuracy: 64
min∥^Block (X)−Block (X)∥2
2 (2)
where^Block (·)is a common transformer block ( 32) with quantized weights. In this stage, we freeze 65
the integer part of the weights, and train the scales, zero points and norm layers. 66
decoupleQ implements 2-bit uniform quantization and achieves state-of-the-art accuracy in Llama- 67
1/2 ( 30;31). Like traditional uniform quantization, decoupleQ does not incur additional inference 68
burden and only requires a linear transformation to convert the quantized weights into floating point 69
ones. 70
Our main highlights are summarized as follows: 71
•New insight: We abandoned the traditional quantization paradigm, and no longer need 72
to focus on some of the minutiae unique to quantization, but abstracts the essence of the 73
problem from a higher level and transforms it into a constrained optimization problem. 74
•Extreme low-bit: decoupleQ achieves 2-bit uniform quantization with performance match- 75
ing fp16/bf16 for industrial applications in the ASR model in our company, and we will also 76
release the W2A16 CUDA kernel as one of our core contribution. 77
•Extensibility: As a bonus, if labeled datasets are available, the idea of decoupleQ can be 78
easily extended to supervised fune-tuning (sft) to further improve model accuracy, or the 79
adaptation to the downstream sub-tasks. 80
1We define the term “layer" as a linear transformation, “block" as a common transformer block containing
the multi-head attention, feed forward, and some layer norm.
22 Related Works 81
Quantization can be roughly divided into Quantization Aware Training (QAT) ( 21;33) and Post- 82
Training Quantization (PTQ) ( 4;35). In this paper, we focus on weight-only quantization in PTQ, 83
and we will only summarize a few works that are closely related to our work. 84
PTQ is commonly used for LLM quantization because it does not require a lot of GPU hours of 85
training with labeled datasets. AdaRound ( 25) and BRECQ ( 18) start from the rounding operation 86
and explore whether to round up or down is better. SqQR ( 7) and OWQ ( 17) use mixed-precision 87
quantization strategy to protect sensitive parameters, while AWQ ( 19) opts for scaling up the weights 88
of sensitive channels to reduce the loss of quantization of sensitive channels. OmniQuant ( 29) use 89
gradient decent to optimize for the weight clipping threshold and the rescale factors. In decoupleQ, we 90
abandon patchwork solutions and transform the quantization into a principled traditional optimization 91
problem by decoupling the model parameters into integer and floating-point parts. 92
GPTQ ( 9) is an influential work, and it quantizes the current weights and then updates the remaining 93
weights to minimize the ℓ2loss of the output of the layer between pre- and post-quantization. As we 94
will see later, this update actually approximates much, and GPTQ does not optimize for the scale and 95
zero point reduced by quantization. 96
QALora ( 36) also decouples model parameters at a certain level and uses labeled datasets to fine-tune 97
the zero points. decoupleQ takes this idea a step further, optimizing the scales, zero points and norm 98
layers with supervised fine-tuning, while freezing the integer weights. 99
3 Methods 100
3.1 Preliminaries 101
For a linear layer with input dimension dinand output dimension dout, quantization maps the weights 102
with high-precision into discrete level, and the previous scheme can be described as follows: 103
cW=clip(⌊W0−z
s⌉, α, β ) (3) fW=cW∗s+z (4)104
where W0∈Rdin×doutis the pre-trained full precision weights, sandzare the scale and zero point 105
(what we call floating-point part above), ⌊·⌉is the round-to-nearest function, cW∈Rdin×doutis the 106
quantized integer-point matrix (what we call integer part above), fWis the de-quantized floating-point 107
matrix, αandβare the lower and upper bounds of the range of integer representations, respectively. 108
For example, in 2-bit weight only linear quantization scheme, the value of each entry of cWis 109
limited to one of {−2,−1,0,1}, and α=−2,β= 1in this case. To get the values of fW, previous 110
methods ( 8;9) show that layer-wise ℓ2loss between the outputs pre- and post-quantization is well 111
related to the model accuracy, i.e., to optimize the following objective function, 112
arg min fW∥XfW−XW 0∥2
2=tr{(fW−W0)TH(fW−W0)} (5)
where X∈Rbatch ×dinis the input of this linear layer, generated by a small set of calibration dataset, 113
andH=XTX. 114
In the very low-bit quantization regime, the model accuracy can be further improved via finer-grained 115
grouping. This would impose additional overhead on inference. For example, when groupsize = 64 , 116
it imposes an average overhead of 0.5 bit per element (FP16/BF16 for scale sand zero point z). The 117
extra overhead is acceptable compared to the model accuracy gain. 118
3.2 decoupleQ 119
When a model is quantized, only the integer part cWand the floating-point part (s, z)in Eq. 4 are 120
delivered to the downstream inference engine, and the inference process does not need to know how 121
cWand(s, z)are obtained at all. That is, if we can find the values of cWand(s, z)to minimize Eq. 5 122
by other methods, then we don’t need to use Eq. 3. So, we can decouple the model parameters into 123
integer part cWand floating point part (s, z), which are then optimized alternatively via off-the-shelf 124
3solution methods. decoupleQ views the process of solving for cWand(s, z)in Eq. 4 as a constrained 125
optimization problem independent of the previous quantization paradigm! We only need to regard 126
Eq. 4 as an ordinary affine transformation, in which the value of scan be 0 or even negative. 127
In per-channel quantization, each column of the weight matrix is optimized independently of each 128
other. For simplicity of notation, we only focus on one column in cWlater and re-define the notations. 129
Based on Eq. 5, the optimization problem of decoupleQ in the first stage, layer-wise minimization, 130
can then be formulated as: 131
min
w;s,zg(w;s, z)
s.t.∀i= 1,2, ..., d in
wi−β≤0
−wi+α≤0
wi∈Z(6)
where the objective function is: 132
g(w;s, z) =1
2(w∗s+z−b)TH(w∗s+z−b) (7)
w∈Rdinis one column of cW,b∈Rdinis the corresponding column of W0,s∈Rngis the 133
scale and z∈Rngis the zero point, ngis the number of groups when grouping-quantization. The 134
operations w.r.t (s, z), i.e.,∗sand+z, need to be broadcasted to each group. In this paradigm, 135
we have completely abandoned the traditional framework of quantization and instead transformed 136
quantization into a constrained optimization problem 6, which is then solved to achieve the purpose 137
of quantization. (s, z)in problem 6 have lost the traditional meaning of scale and zero point, and are 138
just two optimization variables. 139
Transforming the traditional quantization problem into problem 6 is the soul of decoupleQ! Problem 6 140
is a quadratic programming problem with an additional non-convex constraints wi∈Z. Quadratic 141
programming has been studied for many years and there are now many well-established solution ( 24; 142
34). We provide one solution in the next subsection, which may not be efficient or optimal. 143
The core idea of decoupleQ is to decouple the model weights into the integer part wand the 144
floating-point part (s, z), with the integer part occupying most of the model’s expressive power. The 145
extensibility of the idea of decoupleQ is that we can freeze the integer part of the entire model, and 146
use labeled data to train the (s, z)as well as other floating point parameters. The advantage of this is 147
that on the one hand, it can further improve the accuracy of the model, on the other hand, it can fit 148
specific downstream sub-tasks while maintaining the generalization ability of the model. 149
3.3 Optimization via Alternative Iteration 150
The problem 6 is not easy to solve because of the non-convex constraint wi∈Z. After obtaining a 151
good initialization (explained in detail later), we solve for wand(s, z)alternately and iteratively. In 152
each round of alternation, the objective function 7 w.r.t (s, z)is an unconstrained quadratic function, 153
thus(s, z)can be readily determined analytically : by differentiating the objective function and 154
equating the derivative to zero, followed by solving the resultant linear system of equations. While 155
forw, the problem become problem 8: 156
min
wg(w;s, z)
s.t.∀i= 1,2, ..., d in
wi−β≤0
−wi+α≤0
wi∈Z(8)min
wi;i>jg(w;s, z)
s.t.∀i=j+ 1, ..., d in
wi−β≤0
−wi+α≤0
wi∈Z(9) 157
For problem 8, one solution is to round-and-clip one element of wto be integer in [α, β]and then 158
update the remaining. And then this process is then performed sequentially for all elements. After the 159
j-th element has been rounded-and-clipped, the objective for the updating then becomes problem 9. 160
problem 9 is also intractable, and we can make two levels of approximation: 161
4min
wi;i>jg(w;s, z)
s.t.∀i=j+ 1, ..., d in
wi−β≤0
−wi+α≤0(10)min
wi;i>jg(w;s, z) (11) 162
In the first-level approximation 10, only the non-convex constraint wi∈Zis discarded, while in the 163
second-level approximation 11, both the non-convex constraint wi∈Zand the convex constraint 164
wi∈[α, β]are discarded. Intuitively, problem 11 is much simpler to solve than problem 10, but 165
solving problem 10 will lead to a better convergence of the primary objective( 6) than solving 166
problem 11. GPTQ ( 9) provides an efficient analytical solution for problem 11, which we will 167
directly utilize in our experiments. ( GPTQ updates the remaining elements by considering only the 168
second-level approximation 11 and ignoring the constrain wi∈[α, β]in the first ( 10), which is what 169
we mentioned in the introduction, that the update of GPTQ is unconstrained.) As for problem 10, 170
there are many mature solutions in the field of convex optimization, such as active-set method, 171
projected gradient descent (PGD), projected coordinate descent and so on ( 3). We choose PGD 172
because its parallelization is much better than the other two methods. In the experimental part, we 173
will compare the final accuracy of the model via between solving the first level (10) and the second 174
level 11 approximation on small models, while on large models (e.g. lager than 7 billion parameters), 175
we have to choose the second level 11 approximation because the intolerable runtime of solving the 176
first (10). The algorithm is shown in Alg. 1 and Alg. 2. 177
Algorithm 1: Alternative Iteration to
solve problem 6.
Input: predefined iteration number N.
Result: w∗, s∗, z∗
1Initialize t= 1, w0, s0, z0;
2while t≤Ndo
3 Freeze (st−1, zt−1), and optimize
g(w;st−1, zt−1)to obtain an
approximate solution wtvia
solving 8 via 2;
4 Freeze wt, and solve the
unconstraint quadratic equation
g(wt;s, z)to obtain an analytic
solution for (st, zt);
5 t=t+ 1
6end
7w∗=wN;s∗=sN;z∗=zNAlgorithm 2: Approximate solution of 8
Input: predefined iteration number K, M , and
the froozen (s, z).
Result: w∗
1ifApproximaton (10) is used then
2 Ignoring the constraint wi∈Zin Eq. 8, and
train Eq. 8 with Miterations via PGD;
3Initialize j= 1;
4forj= 1→dindo
5 round and clip the j-th element of w, then
keep the first jelements frozen, and
update the remainings via PGD to
optimize 10 with Kiterations or until
converged, or via the method in GPTQ to
optimize 11.
6end
7w∗=w178
3.4 Initialization of wand(s, z) 179
min
p1
2(w∗s+z−b)TH(w∗s+z−b)
s.t.
w=clip(⌊b−z
s⌉, α, β )
s=p∗(bmax−bbmin)
β−α
z=p∗bmin−s∗α(12)Since the values of ware discrete, a good ini- 180
tialization is very important in order to obtain a 181
more accurate solution to the original problem 6 182
with a faster convergence. Intuitively, the func- 183
tiong(w;s, z)contains the term w∗s, which 184
means that the scales of the initial values of w 185
andshave to be reasonably distributed. For ex- 186
ample, in the extreme case when the initial value 187
of(s, z)have a very large scale, the first itera- 188
tion will make most of the entries of wstrictly 189
0, which will make the iteration crash. We start 190
by initializing (s, z). We can use grid search to 191
solve the Eq. 12 for the initial value of (s, z). In 192
Eq. 12, pis a single number, may be different 193
for different columns of W0,bminandbmax are the minimum and maximum value of brespectively. 194
This step is the same as the previous post-training quantization ( 19) process. Once the grid search is 195
5finished, we no longer need to concern ourselves with the (s, z)inside the ⌊·⌉function. The point of 196
this step is simply to find an initial value for (s, z)for the optimization problem 6. 197
When solving problem 8 via the first-level approximation ( 10), before entering the for-loop in Alg. 2, 198
we ignore the constraint wi∈Zin problem 8 and optimize it via projected gradient decent with M 199
iterations. The purpose of this is to allow the first-level approximation to converge in a small number 200
of iterations, i.e., a small K. 201
3.5 Block-wise minimization 202
After solving problem 6, we obtain a solution for the layer-wise minimization stage and a reasonable 203
model accuracy. But minimizing the ℓ2loss at the layer level does not necessarily lead to the 204
minimizing the ℓ2loss at the block level. We found that the model accuracy can be further improved 205
via optimization 2. BRECQ ( 18) also shows that block-reconstruction results in a better model 206
accuracy than layer-reconstruction. In this stage, we freeze the integer part cWin the whole block and 207
fine-tuning (s, z)and the parameters in norm layer with Jepochs. 208
4 Experiments 209
In this section, we describe in detail the experimental results of our method in comparison with other 210
methods. Unless otherwise stated, all the experiments are conducted on a single A100-SXM-80GB, 211
and the default experimental setting is as follows: 212
ResNet: 10240 images in the training dateloader are used as calibration data, with the standard 213
augmentation in Pytorch official code ( 27), and the pretrained full precision checkpoints are from 214
Torchvision ( 22).N= 4, M= 50 (NandMis defined in refalg1 and refalg2). All the convolution 215
layers and fully-connected layers are quantized into W2 without groups. 216
Llama-1/2: 128 2048-token segments from C4 ( 28) are used as calibration data. We choose C4 217
as calibration dataset instead of WikiText2 ( 23) to be consistent with GPTQ. If the block-wise 218
minimization is used, we use Adam optimizer ( 15) to finetune the (s, z)and the parameters in norm 219
layer with J= 4epochs. The learning rate is 1e-5, weight decay is 1e-6. 220
4.1 Private Experiments 221
Figure 1: The latency (in 1 e-6 seconds) of the
four GEMMs in transformer block on L4 GPU,
(The three GEMMs for query, key and value are
concatenated into GEMM 1), with hidden _dim=
5120 ,batch _size = 4.We applied decoupleQ to our company’s two 222
Automatic Speech Recognition models(ASR) 223
(corresponding to task A and task B). Each of 224
the models contain an encoder and an LLM de- 225
coder. The input of the models is a speech se- 226
quence and some prompt, and the output is the 227
corresponding text. We quantize the LLM de- 228
coder to W2A16g64. The decoders of the two 229
models contain 40 transformer blocks with 13 230
billion parameters and 32 transformer blocks 231
with 7 billion parameters, respectively. Word Er- 232
ror Rate (WER) is used as metric to measure the 233
accuracy of the models (less is better). In this 234
experiments, we use about 8 millions of speech 235
tokens as calibration dataset, and train 3 epoch 236
in each block-wise minimization process. When 237
an input batch contains sequences of varying 238
lengths, we use a mask to make sure that the 239
padding part is not involved in the computation of Hand the loss of Eq. 2. In task B, once the whole 240
model is quantized, we also fine-tune all the (s, z)and layer norm in the LLM with labeled dataset, 241
while freezing all the integer part cW, with 8 A100-SXM-80GB GPUs. The accuracy is shown in 242
Tab. 1, and the CUDA kernel latency is shown in Fig. 1. The W2A16 CUDA kernel is attached and 243
will be merged into the NVIDIA repo as one of our core contribution. 244
6Table 1: The results of our two ASR models. The models are quantized into W2A16g64. runtime for
the quantization process is measured in hours. There are two sub-domains in task B, and we report
the WER of both.
Task A Task B
BF16 decoupleQ BF16 decoupleQ decoupleQ+sft
WER 6.68 6.70 (5.86, 11.43) (5.87, 11.56) (5.77, 11.43)
runtime - 25 - 32 32+5
Table 2: Comparison of decoupleQ with other methods. In decoupleQ, we only use the first stage,
layer-wise minimization. All the models are quantized into W2A16 without groups. In decoupleQ+sft,
we train the (s, z)and norm layers for one epoch, using the regular labeled dataset containing 1.2
million images.
methodres18-69.76% res50-76.13%
2bit 3bit 4bit 2bit 3bit 4bit
GPTQ - 67.88 69.37 - 74.87 75.71
OBQ 64.04 68.69 69.56 70.71 75.24 75.72
BRECQ 64.70 68.47 69.37 72.41 75.32 75.88
decoupleQ 64.15 68.65 69.58 71.34 75.24 76.00
decoupleQ+sft 65.45 68.94 69.71 72.65 75.61 75.97
4.2 Public Comparison 245
As a first comparison, we compare decoupleQ with other methods on ImageNet ( 5) with ResNet ( 12), 246
which are standard benchmarks and are efficient to implement. Most importantly, its Top-1 is a strong 247
indicator of model accuracy. Tab. 2 shows the results of decoupleQ and others. The results other than 248
decoupleQ are copied from GPTQ (9) and OBQ (8). 249
Tab. 3 shows the results on Llama. In this experiment, we have to choose the second level approxima- 250
tion(11) because the intolerable runtime of solving the first(10). For a fair comparison, the calibration 251
dataset contains 128 samples, although a larger calibration dataset will result in stronger results. 252
we can see that decoupleQ outperforms others almost in all settings, although we use a weaker 253
approximation(11) to save time. As for the hype-parameters, we choose {N= 4, J= 4}. 254
4.3 Ablation studies 255
4.3.1 the two approximations 256
The soul of decoupleQ is problem 6, but when solving problem 6, we have to take some approxima- 257
tions(10 or 11). Obviously, solving approximation 10 will be much more time consuming than solving 258
approximation 11. But if solving approximation 10 yields better results, the time cost may be worth 259
it. We first evaluate these two approximations from the perspective of model accuracy. In practice, 260
we don’t have to wait for approximation 10 to fully converge when we solve it via projected gradient 261
decent, and only need to iterate some steps to get a sub-optimal solution. In Alg. 2, the for-loop takes 262
up the majority of the runtime. So, we first study the influence of the number of iterations K(defined 263
in the for-loop) on the final accuracy of the model. Fig. 2 shows the Top-1 accuracy of ResNet-18 on 264
ImageNet w.r.t the number of iterations K. First of all, in the blue line, we use only the layer-wise 265
minimization of decooupleQ to quantize the model. After the quantization is finished, in the red line, 266
we use the labeled dataset with the common 1.2 millions images to fine-tune all the (s, z)and norm 267
layers for one epoch, with the integer part being frozen. In this step, we use SGD optimizer with 268
learning rate 1e-6, weight decaying rate 1e-4to train for only one epoch. Fig. 2 clearly indicates the 269
following conclusions: 1. As the number of iterations Kincreases, the model accuracy increases 270
almost monotonically; 2. When K > 4, model accuracy via the first approximation(10) is better than 271
via the second(11). This is to be expected, since the second approximation(11) drops the constraint 272
α≤wi≤β, leading to a looser approximation; 3. By the supervised fine-tuning (sft), the model 273
accuracy is further improved. The same experimental phenomenon also occurs on the ResNet-50 274
model, which we do not show here. 275
7Table 3: The results of PPL of wikitext-2 on Llama-1/2. We also report the runtime (measured in
hours) for the W2 quantization via decoupleQ in the gray background row. The results other than
decoupleQ are copied from OmniQuant ( 29). All the results of decoupleQ use the approximation 11.
Llama 1-7B 1-13B 1-30B 1-65B 2-7B 2-13B 2-70B
FP16 5.68 5.09 4.10 3.53 5.47 4.88 3.31
GPTQ 2.1e3 5.5e3 499.75 55.91 7.7e3 2.1e3 77.95
OmniQuant 15.47 13.21 8.71 7.58 37.37 17.21 7.81
decoupleQ 9.49 7.86 6.37 5.59 9.74 13.03 5.23W2A16
runtime 2.5 4.8 12.7 27.6 2.5 4.5 33.4
GPTQ 44.01 15.60 10.92 9.51 36.77 28.14 -
OmniQuant 8.90 7.34 6.59 5.65 9.62 7.56 6.11
decoupleQ 8.65 7.25 6.04 5.19 8.79 7.44 4.96W2A16g128
runtime 3.7 7.7 24.3 55.0 3.7 7.9 70.6
GPTQ 22.10 10.06 8.54 8.31 20.85 22.44 -
OmniQuant 8.90 7.34 6.59 5.65 9.62 7.56 6.11
decoupleQ 8.18 6.96 5.81 5.07 8.41 6.98 5.34W2A16g64
runtime 4.3 8.9 27.9 64.5 4.4 9.0 98.2
GPTQ 8.06 6.76 5.84 5.06 8.37 6.44 4.82
AWQ 11.88 7.45 10.07 5.21 24.00 10.45 -
OmniQuant 6.49 5.68 4.74 4.04 6.58 5.58 3.92W3A16
decoupleQ 6.38 5.60 4.67 6.05 6.22 5.72 3.84
GPTQ 6.13 5.40 4.48 3.83 5.83 5.13 3.58
AWQ 6.08 5.34 4.39 3.76 6.15 5.12 -
OmniQuant 5.86 5.21 4.25 3.71 5.74 5.02 3.47W4A16
decoupleQ 5.85 5.21 4.24 3.67 5.70 5.06 3.45
Figure 2: The Top-1 accuracy of ResNet-18
on ImageNet. Solid and dashed lines are for
approximation 10 and 11 respectively.
Figure 3: The PPL of Llama-7B on Wiki-
Text2. Solid and dashed lines are for approxi-
mation 10 and 11 respectively.
In the experiment shown in 3, we randomly select 512 2048-token segments from C4 ( 28). We chose 276
512 segments here instead of the common 128 in order to reduce the effect of overfitting and thus 277
compare the two approximations more objectively. In this experiment, we take N= 2, and quantize 278
Llama-7B into W2A16 without groups, and only the layer-wise minimization is used to exclude the 279
interference of other factors. The PPL decrease almost monotonically as the number of iterations K 280
increases. It shows that, when K > 1, solving approximation 10 yields better model accuracy than 281
approximation 11. 282
However, when block-wise minimization is introduced in addition to the experiment in 3, the situation 283
becomes a little more elusive. The results are shown in 4. The model’s best PPL is where K= 1, 284
and then fluctuates within a range as Kcontinues to increase. But all PPLs are inferior to when 285
the second-level approximation (11) is used. We also plot the loss, defined in 2, of the first block 286
between pre-and post quantization on the right vertical axis. As Kincreases, the loss decreases 287
strictly monotonically, and when K > 2, the loss falls below the case when the approximation 11 is 288
used. This suggests that the correlation between PPL and loss is perhaps weak, and we will investigate 289
this in the future. 290
8Figure 4: The PPL of Llama-7B on WikiText2
and the loss of the first block between pre-and
post-quantization. Solid and dashed lines are
for approximation 10 and 11 respectively.
Figure 5: The perplexity of Llama-7B on
WikiText2 and C4 dataset w.r.t the number of
segments as calibration datasets. The model
is quantized into W2A16g64.
4.3.2 the size of calibration dataset 291
The solution of problem 6 is dependent on Hand thus on the the calibration dataset, as does Eq. 2. 292
Fig. 5 shows the relationship between dataset size and PPL. In this experiment, Llama-7B is quantized 293
into W2A16g64. We use the second-level approximation (11) to save time, and { N= 4, J= 4}. For 294
runtime reference, when the number of segments is 128/2048, the experiment took 4.3/19.5 hours. 295
4.3.3 the necessity of block-wise minimization 296
Table 4: The perplexity of Llama on WikiText2 with
and without the block-wise minimization. All the mod-
els are quantized into W2A16.
Llama 1-7B 1-13B 1-30B 2-7B 2-13B
w/o 13.66 9.68 7.35 14.66 12.93
w 9.49 7.86 6.37 9.74 13.03Tab. 4 shows that block-wise minimiza- 297
tion(2) can further improve the model accu- 298
racy. In this experiment, we choose N= 4 299
and the approximation 11 for the layer-wise 300
minimization, and J= 4if block-wise min- 301
imization is used. 302
5 Conclusion and Discussion 303
deocupleQ decouples the model parameters into the integer part and a floating point part, and 304
then optimizes them alternately. This optimization process contains two stages. In the layer-wise 305
minimization, we transform the quantization problem into the purely mathematical constrained 306
optimization problem refdecoupleQ; while in the block-wise minimization, we freeze the integer part 307
and then finetune the floating point part. 308
The risks of decoupleQ include the following: 1. How much the minimization of the ℓ2loss of 309
the layer’s or block’s output correlates with the accuracy of the model; 2. decoupleQ is prone to 310
overfitting the calibration dataset; 3. The runtime of the quantization process is longer than others. 311
For the first risk, we find experimentally that the correlation between Top-1 and the loss is strong in 312
the Imagenet classification task; however, the correlation between PPL and the loss is slightly weaker 313
in LLM. This could be mainly because of an inherent bias between the loss and the accuracy of the 314
model, or because PPL is not a good indicator of the accuracy of LLM, or for other reasons. For 315
the second risk, when Hin Eq. 7 is an underdetermined matrix, the risk of overfitting rises sharply. 316
In this case, the possibility of Hbeing underdetermined can be reduced either by enhancing the 317
diagonal element values of Hor by increasing the amount of calibration data. In our practice, we 318
found that the accuracy of quantization models can rise monotonically with the increase of the size of 319
the calibration dataset, especially in W2 quantization, but the runtime of quantization rise as well. In 320
addition, due to time constraints, we do not provide a wealth of public comparisons. However, we 321
believe that the novelty of a method may outweigh the number of experiments. 322
The idea of decoupleQ is helpful for the adaptation of large model to downstream sub-task. We can 323
quantize a large foundation model via decoupleQ, then freeze the integer part of the model, and 324
finetune the floating-point part with labeled dataset from downstream sub-task. Tab. 1 and Tab. 2 325
show that the model accuracy can be further improved by end-to-end supervised learning. 326
9References 327
[1]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind 328
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. 329
Advances in neural information processing systems , 33:1877–1901, 2020. 330
[2]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, 331
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early 332
experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023. 333
[3]Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends ®in 334
Machine Learning , 8(3-4):231–357, 2015. 335
[4]Jerry Chee, Yaohui Cai, V olodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large 336
language models with guarantees. arXiv preprint arXiv:2307.13304 , 2023. 337
[5]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical 338
image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. 339
Ieee, 2009. 340
[6]Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication 341
for transformers at scale. arXiv preprint arXiv:2208.07339 , 2022. 342
[7]Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, 343
Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for 344
near-lossless llm weight compression. arXiv preprint arXiv:2306.03078 , 2023. 345
[8]Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training 346
quantization and pruning. Advances in Neural Information Processing Systems , 35:4475–4488, 2022. 347
[9]Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for 348
generative pre-trained transformers. In The Eleventh International Conference on Learning Representations , 349
2022. 350
[10] Yi Guo, Yiqian He, Xiaoyang Li, Haotong Qin, Van Tung Pham, Yang Zhang, and Shouda Liu. Rdimkd: 351
Generic distillation paradigm by dimensionality reduction. arXiv preprint arXiv:2312.08700 , 2023. 352
[11] Yi Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, and Ji Liu. Gdp: Stabilized neural 353
network pruning via gates with differentiable polarization. In Proceedings of the IEEE/CVF International 354
Conference on Computer Vision , pages 5239–5250, 2021. 355
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 356
InProceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. 357
[13] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training 358
quantization with small calibration sets. In International Conference on Machine Learning , pages 4466– 359
4475. PMLR, 2021. 360
[14] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, 361
and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629 , 2023. 362
[15] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint 363
arXiv:1412.6980 , 2014. 364
[16] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. 365
arXiv preprint arXiv:1806.08342 , 2018. 366
[17] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Lessons learned from 367
activation outliers for weight quantization in large language models. arXiv preprint arXiv:2306.02272 , 368
2023. 369
[18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi 370
Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint 371
arXiv:2102.05426 , 2021. 372
[19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware 373
weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978 , 2023. 374
[20] Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform-to-uniform 375
quantization: Towards accurate quantization via generalized straight-through estimation. In Proceedings of 376
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4942–4952, 2022. 377
[21] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, 378
Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large 379
language models. arXiv preprint arXiv:2305.17888 , 2023. 380
[22] Sébastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Proceedings 381
of the 18th ACM international conference on Multimedia , pages 1485–1488, 2010. 382
[23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. 383
arXiv preprint arXiv:1609.07843 , 2016. 384
[24] Katta G Murty and Feng-Tien Yu. Linear complementarity, linear and nonlinear programming , volume 3. 385
Heldermann Berlin, 1988. 386
[25] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or 387
down? adaptive rounding for post-training quantization. In International Conference on Machine Learning , 388
pages 7197–7206. PMLR, 2020. 389
[26] Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, and 390
Avi Mendelson. Loss aware post-training quantization. Machine Learning , 110(11-12):3245–3262, 2021. 391
[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, 392
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep 393
10learning library. Advances in neural information processing systems , 32, 2019. 394
[28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, 395
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. 396
Journal of machine learning research , 21(140):1–67, 2020. 397
[29] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng 398
Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language 399
models. arXiv preprint arXiv:2308.13137 , 2023. 400
[30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, 401
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation 402
language models. arXiv preprint arXiv:2302.13971 , 2023. 403
[31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay 404
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and 405
fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 406
[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz 407
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 408
30, 2017. 409
[33] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong 410
Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal 411
shifting and scaling. arXiv preprint arXiv:2304.09145 , 2023. 412
[34] Stephen J Wright. Numerical optimization . 2006. 413
[35] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: 414
Accurate and efficient post-training quantization for large language models. In International Conference 415
on Machine Learning , pages 38087–38099. PMLR, 2023. 416
[36] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng 417
Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. arXiv 418
preprint arXiv:2309.14717 , 2023. 419
[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher 420
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. 421
arXiv preprint arXiv:2205.01068 , 2022. 422
[38] Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, 423
Vera Axelrod, Gary Wang, et al. Google usm: Scaling automatic speech recognition beyond 100 languages. 424
arXiv preprint arXiv:2303.01037 , 2023. 425
11NeurIPS Paper Checklist 426
1.Claims 427
Question: Do the main claims made in the abstract and introduction accurately reflect the 428
paper’s contributions and scope? 429
Answer: [Yes] 430
Justification: Our claims and justification include: 431
a.Our results are higher than others in very low bit (2-bit) quantization.( This is 432
justified in Tab. 3.); 433
b.decoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantiza- 434
tion of large speech models in our company. (This is justified in Tab. 1, and the 435
W2 CUDA kernel used in our company are attached.); 436
c.decoupleQ gets rid of any tricks for dealing with outliers, sensitive channels, 437
etc. (This is justified in the Problem 6, we do not use any tricks, such as scaling 438
factor ( 19;29), mixed-precision quantization ( 6), etc., to deal with outliers and 439
sensitive channels.) 440
Guidelines: 441
•The answer NA means that the abstract and introduction do not include the claims 442
made in the paper. 443
•The abstract and/or introduction should clearly state the claims made, including the 444
contributions made in the paper and important assumptions and limitations. A No or 445
NA answer to this question will not be perceived well by the reviewers. 446
•The claims made should match theoretical and experimental results, and reflect how 447
much the results can be expected to generalize to other settings. 448
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 449
are not attained by the paper. 450
2.Limitations 451
Question: Does the paper discuss the limitations of the work performed by the authors? 452
Answer: [Yes] 453
Justification: The paper has discussed the three limitations of decoupleQ in the last section, 454
Conclusion and Discussion , and the risk overall that we did not provide as many public 455
comparison experiments as other work due to time constraints. 456
Guidelines: 457
•The answer NA means that the paper has no limitation while the answer No means that 458
the paper has limitations, but those are not discussed in the paper. 459
• The authors are encouraged to create a separate "Limitations" section in their paper. 460
•The paper should point out any strong assumptions and how robust the results are to 461
violations of these assumptions (e.g., independence assumptions, noiseless settings, 462
model well-specification, asymptotic approximations only holding locally). The authors 463
should reflect on how these assumptions might be violated in practice and what the 464
implications would be. 465
•The authors should reflect on the scope of the claims made, e.g., if the approach was 466
only tested on a few datasets or with a few runs. In general, empirical results often 467
depend on implicit assumptions, which should be articulated. 468
•The authors should reflect on the factors that influence the performance of the approach. 469
For example, a facial recognition algorithm may perform poorly when image resolution 470
is low or images are taken in low lighting. Or a speech-to-text system might not be 471
used reliably to provide closed captions for online lectures because it fails to handle 472
technical jargon. 473
•The authors should discuss the computational efficiency of the proposed algorithms 474
and how they scale with dataset size. 475
•If applicable, the authors should discuss possible limitations of their approach to 476
address problems of privacy and fairness. 477
12•While the authors might fear that complete honesty about limitations might be used by 478
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 479
limitations that aren’t acknowledged in the paper. The authors should use their best 480
judgment and recognize that individual actions in favor of transparency play an impor- 481
tant role in developing norms that preserve the integrity of the community. Reviewers 482
will be specifically instructed to not penalize honesty concerning limitations. 483
3.Theory Assumptions and Proofs 484
Question: For each theoretical result, does the paper provide the full set of assumptions and 485
a complete (and correct) proof? 486
Answer:[NA] 487
Justification: This paper does not include theoretical results. 488
Guidelines: 489
• The answer NA means that the paper does not include theoretical results. 490
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 491
referenced. 492
•All assumptions should be clearly stated or referenced in the statement of any theorems. 493
•The proofs can either appear in the main paper or the supplemental material, but if 494
they appear in the supplemental material, the authors are encouraged to provide a short 495
proof sketch to provide intuition. 496
•Inversely, any informal proof provided in the core of the paper should be complemented 497
by formal proofs provided in appendix or supplemental material. 498
• Theorems and Lemmas that the proof relies upon should be properly referenced. 499
4.Experimental Result Reproducibility 500
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 501
perimental results of the paper to the extent that it affects the main claims and/or conclusions 502
of the paper (regardless of whether the code and data are provided or not)? 503
Answer: [Yes] 504
Justification: At the beginning of the section Experiments , we provide details of the experi- 505
mental parameters; specifically for each experiment, we also provide the key experimental 506
parameters. 507
Guidelines: 508
• The answer NA means that the paper does not include experiments. 509
•If the paper includes experiments, a No answer to this question will not be perceived 510
well by the reviewers: Making the paper reproducible is important, regardless of 511
whether the code and data are provided or not. 512
•If the contribution is a dataset and/or model, the authors should describe the steps taken 513
to make their results reproducible or verifiable. 514
•Depending on the contribution, reproducibility can be accomplished in various ways. 515
For example, if the contribution is a novel architecture, describing the architecture fully 516
might suffice, or if the contribution is a specific model and empirical evaluation, it may 517
be necessary to either make it possible for others to replicate the model with the same 518
dataset, or provide access to the model. In general. releasing code and data is often 519
one good way to accomplish this, but reproducibility can also be provided via detailed 520
instructions for how to replicate the results, access to a hosted model (e.g., in the case 521
of a large language model), releasing of a model checkpoint, or other means that are 522
appropriate to the research performed. 523
•While NeurIPS does not require releasing code, the conference does require all submis- 524
sions to provide some reasonable avenue for reproducibility, which may depend on the 525
nature of the contribution. For example 526
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 527
to reproduce that algorithm. 528
(b)If the contribution is primarily a new model architecture, the paper should describe 529
the architecture clearly and fully. 530
13(c)If the contribution is a new model (e.g., a large language model), then there should 531
either be a way to access this model for reproducing the results or a way to reproduce 532
the model (e.g., with an open-source dataset or instructions for how to construct 533
the dataset). 534
(d)We recognize that reproducibility may be tricky in some cases, in which case 535
authors are welcome to describe the particular way they provide for reproducibility. 536
In the case of closed-source models, it may be that access to the model is limited in 537
some way (e.g., to registered users), but it should be possible for other researchers 538
to have some path to reproducing or verifying the results. 539
5.Open access to data and code 540
Question: Does the paper provide open access to the data and code, with sufficient instruc- 541
tions to faithfully reproduce the main experimental results, as described in supplemental 542
material? 543
Answer: [Yes] 544
Justification: The code (including W2 CUDA kernels) is attached in supplementary material, 545
and can reproduce the results in the public experiments. 546
Guidelines: 547
• The answer NA means that paper does not include experiments requiring code. 548
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 549
public/guides/CodeSubmissionPolicy ) for more details. 550
•While we encourage the release of code and data, we understand that this might not be 551
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 552
including code, unless this is central to the contribution (e.g., for a new open-source 553
benchmark). 554
•The instructions should contain the exact command and environment needed to run to 555
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 556
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 557
•The authors should provide instructions on data access and preparation, including how 558
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 559
•The authors should provide scripts to reproduce all experimental results for the new 560
proposed method and baselines. If only a subset of experiments are reproducible, they 561
should state which ones are omitted from the script and why. 562
•At submission time, to preserve anonymity, the authors should release anonymized 563
versions (if applicable). 564
•Providing as much information as possible in supplemental material (appended to the 565
paper) is recommended, but including URLs to data and code is permitted. 566
6.Experimental Setting/Details 567
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 568
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 569
results? 570
Answer: [Yes] 571
Justification: At the beginning of the section Experiments , we provide details of the experi- 572
mental parameters; specifically for each experiment, we also provide the key experimental 573
parameters. The code is attached in supplementary material and will be made public. 574
Guidelines: 575
• The answer NA means that the paper does not include experiments. 576
•The experimental setting should be presented in the core of the paper to a level of detail 577
that is necessary to appreciate the results and make sense of them. 578
•The full details can be provided either with the code, in appendix, or as supplemental 579
material. 580
7.Experiment Statistical Significance 581
Question: Does the paper report error bars suitably and correctly defined or other appropriate 582
information about the statistical significance of the experiments? 583
14Answer: [No] 584
Justification: The cost of the experiment is high. 585
Guidelines: 586
• The answer NA means that the paper does not include experiments. 587
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 588
dence intervals, or statistical significance tests, at least for the experiments that support 589
the main claims of the paper. 590
•The factors of variability that the error bars are capturing should be clearly stated (for 591
example, train/test split, initialization, random drawing of some parameter, or overall 592
run with given experimental conditions). 593
•The method for calculating the error bars should be explained (closed form formula, 594
call to a library function, bootstrap, etc.) 595
• The assumptions made should be given (e.g., Normally distributed errors). 596
•It should be clear whether the error bar is the standard deviation or the standard error 597
of the mean. 598
•It is OK to report 1-sigma error bars, but one should state it. The authors should 599
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 600
of Normality of errors is not verified. 601
•For asymmetric distributions, the authors should be careful not to show in tables or 602
figures symmetric error bars that would yield results that are out of range (e.g. negative 603
error rates). 604
•If error bars are reported in tables or plots, The authors should explain in the text how 605
they were calculated and reference the corresponding figures or tables in the text. 606
8.Experiments Compute Resources 607
Question: For each experiment, does the paper provide sufficient information on the com- 608
puter resources (type of compute workers, memory, time of execution) needed to reproduce 609
the experiments? 610
Answer: [Yes] 611
Justification: We have reported that most of the experiments are conducted in one single 612
A100-SXM-80GB, except for the sft process. And we also reported the time of execution. 613
Guidelines: 614
• The answer NA means that the paper does not include experiments. 615
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 616
or cloud provider, including relevant memory and storage. 617
•The paper should provide the amount of compute required for each of the individual 618
experimental runs as well as estimate the total compute. 619
•The paper should disclose whether the full research project required more compute 620
than the experiments reported in the paper (e.g., preliminary or failed experiments that 621
didn’t make it into the paper). 622
9.Code Of Ethics 623
Question: Does the research conducted in the paper conform, in every respect, with the 624
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 625
Answer: [Yes] 626
Justification: The research conducted in the paper conform, in every respect, with the 627
NeurIPS Code of Ethics 628
Guidelines: 629
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 630
•If the authors answer No, they should explain the special circumstances that require a 631
deviation from the Code of Ethics. 632
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 633
eration due to laws or regulations in their jurisdiction). 634
1510.Broader Impacts 635
Question: Does the paper discuss both potential positive societal impacts and negative 636
societal impacts of the work performed? 637
Answer: [NA] 638
Justification: This is a work for accelerating the inference of deep models, where the social 639
impact is determined by the function of the model, not by how the inference is accelerated. 640
Guidelines: 641
• The answer NA means that there is no societal impact of the work performed. 642
•If the authors answer NA or No, they should explain why their work has no societal 643
impact or why the paper does not address societal impact. 644
•Examples of negative societal impacts include potential malicious or unintended uses 645
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 646
(e.g., deployment of technologies that could make decisions that unfairly impact specific 647
groups), privacy considerations, and security considerations. 648
•The conference expects that many papers will be foundational research and not tied 649
to particular applications, let alone deployments. However, if there is a direct path to 650
any negative applications, the authors should point it out. For example, it is legitimate 651
to point out that an improvement in the quality of generative models could be used to 652
generate deepfakes for disinformation. On the other hand, it is not needed to point out 653
that a generic algorithm for optimizing neural networks could enable people to train 654
models that generate Deepfakes faster. 655
•The authors should consider possible harms that could arise when the technology is 656
being used as intended and functioning correctly, harms that could arise when the 657
technology is being used as intended but gives incorrect results, and harms following 658
from (intentional or unintentional) misuse of the technology. 659
•If there are negative societal impacts, the authors could also discuss possible mitigation 660
strategies (e.g., gated release of models, providing defenses in addition to attacks, 661
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 662
feedback over time, improving the efficiency and accessibility of ML). 663
11.Safeguards 664
Question: Does the paper describe safeguards that have been put in place for responsible 665
release of data or models that have a high risk for misuse (e.g., pretrained language models, 666
image generators, or scraped datasets)? 667
Answer: [NA] 668
Justification: This work does not release models or datasets. 669
Guidelines: 670
• The answer NA means that the paper poses no such risks. 671
•Released models that have a high risk for misuse or dual-use should be released with 672
necessary safeguards to allow for controlled use of the model, for example by requiring 673
that users adhere to usage guidelines or restrictions to access the model or implementing 674
safety filters. 675
•Datasets that have been scraped from the Internet could pose safety risks. The authors 676
should describe how they avoided releasing unsafe images. 677
•We recognize that providing effective safeguards is challenging, and many papers do 678
not require this, but we encourage authors to take this into account and make a best 679
faith effort. 680
12.Licenses for existing assets 681
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 682
the paper, properly credited and are the license and terms of use explicitly mentioned and 683
properly respected? 684
Answer: [Yes] 685
Justification: The paper has cited all related works, and included the relevant license. 686
16Guidelines: 687
• The answer NA means that the paper does not use existing assets. 688
• The authors should cite the original paper that produced the code package or dataset. 689
•The authors should state which version of the asset is used and, if possible, include a 690
URL. 691
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 692
•For scraped data from a particular source (e.g., website), the copyright and terms of 693
service of that source should be provided. 694
•If assets are released, the license, copyright information, and terms of use in the 695
package should be provided. For popular datasets, paperswithcode.com/datasets 696
has curated licenses for some datasets. Their licensing guide can help determine the 697
license of a dataset. 698
•For existing datasets that are re-packaged, both the original license and the license of 699
the derived asset (if it has changed) should be provided. 700
•If this information is not available online, the authors are encouraged to reach out to 701
the asset’s creators. 702
13.New Assets 703
Question: Are new assets introduced in the paper well documented and is the documentation 704
provided alongside the assets? 705
Answer: [Yes] 706
Justification: We provide the source code, and a readme and license file are alongside. 707
Guidelines: 708
• The answer NA means that the paper does not release new assets. 709
•Researchers should communicate the details of the dataset/code/model as part of their 710
submissions via structured templates. This includes details about training, license, 711
limitations, etc. 712
•The paper should discuss whether and how consent was obtained from people whose 713
asset is used. 714
•At submission time, remember to anonymize your assets (if applicable). You can either 715
create an anonymized URL or include an anonymized zip file. 716
14.Crowdsourcing and Research with Human Subjects 717
Question: For crowdsourcing experiments and research with human subjects, does the paper 718
include the full text of instructions given to participants and screenshots, if applicable, as 719
well as details about compensation (if any)? 720
Answer: [NA] 721
Justification: the paper does not involve crowdsourcing nor research with human subjects 722
Guidelines: 723
•The answer NA means that the paper does not involve crowdsourcing nor research with 724
human subjects. 725
•Including this information in the supplemental material is fine, but if the main contribu- 726
tion of the paper involves human subjects, then as much detail as possible should be 727
included in the main paper. 728
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 729
or other labor should be paid at least the minimum wage in the country of the data 730
collector. 731
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 732
Subjects 733
Question: Does the paper describe potential risks incurred by study participants, whether 734
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 735
approvals (or an equivalent approval/review based on the requirements of your country or 736
institution) were obtained? 737
Answer: [NA] 738
17Justification: the paper does not involve crowdsourcing nor research with human subjects 739
Guidelines: 740
•The answer NA means that the paper does not involve crowdsourcing nor research with 741
human subjects. 742
•Depending on the country in which research is conducted, IRB approval (or equivalent) 743
may be required for any human subjects research. If you obtained IRB approval, you 744
should clearly state this in the paper. 745
•We recognize that the procedures for this may vary significantly between institutions 746
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 747
guidelines for their institution. 748
•For initial submissions, do not include any information that would break anonymity (if 749
applicable), such as the institution conducting the review. 750
18