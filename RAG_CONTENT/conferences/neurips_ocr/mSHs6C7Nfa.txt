Improving the Training of Rectified Flows
Sangyun Lee
Carnegie Mellon University
sangyunl@andrew.cmu.eduZinan Lin
Microsoft Research
zinanlin@microsoft.comGiulia Fanti
Carnegie Mellon University
gfanti@andrew.cmu.edu
Abstract
Diffusion models have shown great promise for image and video generation, but
sampling from state-of-the-art models requires expensive numerical integration
of a generative ODE. One approach for tackling this problem is rectified flows,
which iteratively learn smooth ODE paths that are less susceptible to truncation
error. However, rectified flows still require a relatively large number of function
evaluations (NFEs). In this work, we propose improved techniques for training
rectified flows, allowing them to compete with knowledge distillation methods
even in the low NFE setting. Our main insight is that under realistic settings, a
single iteration of the Reflow algorithm for training rectified flows is sufficient
to learn nearly straight trajectories; hence, the current practice of using multiple
Reflow iterations is unnecessary. We thus propose techniques to improve one-round
training of rectified flows, including a U-shaped timestep distribution and LPIPS-
Huber premetric. With these techniques, we improve the FID of the previous
2-rectified flow by up to 75% in the 1 NFE setting on CIFAR-10. On ImageNet
64×64, our improved rectified flow outperforms the state-of-the-art distillation
methods such as consistency distillation and progressive distillation in both one-step
and two-step settings and rivals the performance of improved consistency training
(iCT) in FID. Code is available at https://github.com/sangyun884/rfpp .
1 Introduction
Diffusion models [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song and Ermon, 2019, Song et al.,
2020b] have shown great promise in image [Ramesh et al., 2022] and video [Ho et al., 2022]
generation. They generate data by simulating a stochastic denoising process where noise is gradually
transformed into data. To sample efficiently from diffusion models, the denoising process is typically
converted into a counterpart of Ordinary Differential Equations (ODEs) [Song et al., 2020b] called
probability flow ODEs (PF-ODEs).
Despite the success of diffusion models using PF-ODEs, drawing high-quality samples requires
numerical integration of the PF-ODE with small step sizes, which is computationally expensive.
Today, two prominent classes of approaches for tackling this issue are: (1) knowledge distillation
(e.g., consistency distillation [Song et al., 2023], progressive distillation [Salimans and Ho, 2022])
and (2) simulation-free flow models (e.g., rectified flows [Liu et al., 2022], flow matching [Lipman
et al., 2022]).
In knowledge distllation-based methods [Luhman and Luhman, 2021, Salimans and Ho, 2022, Song
et al., 2023, Zheng et al., 2022] a student model is trained to directly predict the solution of the
PF-ODE. These models are currently state-of-the-art in the low number of function evaluations
(NFEs) regime (e.g. 1-4).
Another promising direction is simulation-free flow models such as rectified flows [Liu et al., 2022,
Liu, 2022], a generative model that learns a transport map between two distributions defined via
neural ODEs. Diffusion models with PF-ODEs are a special case. Rectified flows can learn smooth
38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2405.20320v2  [cs.CV]  8 Oct 2024ODE trajectories that are less susceptible to truncation error, which allows for high-quality samples
with fewer NFEs than diffusion models. They have been shown to outperform diffusion models in
the moderate to high NFE regime [Lipman et al., 2022, Liu et al., 2022, Esser et al., 2024], but they
still require a relatively large number of NFEs compared to distillation methods.
Compared to knowledge distillation methods [Luhman and Luhman, 2021, Salimans and Ho, 2022,
Song et al., 2023, Zheng et al., 2022] rectified flows have several advantages. First, they can be
generalized to map two arbitrary distributions to one another, while distillation methods are limited
to a Gaussian noise distribution. Also, as a neural ODE, rectified flows naturally support inversion
from data to noise, which has many applications including image editing [Hertz et al., 2022, Kim
et al., 2022, Wallace et al., 2023, Couairon et al., 2022, Mokady et al., 2023, Su et al., 2022, Hong
et al., 2023] and watermarking [Wen et al., 2023]. Further, the likelihood of rectified flow models can
be evaluated using the instantaneous change of variable formula [Chen et al., 2018], whereas this
is not possible with knowledge distillation-based methods. In addition, rectified flows can flexibly
adjust the balance between the sample quality and computational cost by altering NFEs, whereas
distillation methods either do not support multi-step sampling or do not necessarily perform better
with more NFEs (e.g. > 4) [Kim et al., 2023].
Given the qualitative advantages of rectified flows, a natural question is, can rectified flows compete
with distillation-based methods such as consistency models [Song et al., 2023] in the low NFE
setting? Today, the state-of-the-art techniques for training rectified flows use the Reflow algorithm to
improve low NFE performance [Liu et al., 2022, 2023]. Reflow is a recursive training algorithm where
the rectified flow is trained on data-noise pairs generated by the generative ODE of the previous stage
model. In current implementations of Reflow, to obtain a reasonable one-step generative performance,
Reflow should be applied at least twice, followed by an optional distillation stage to further boost
performance [Liu et al., 2022, 2023]. Each training stage requires generating a large number of
data-noise pairs and training the model until convergence, which is computationally expensive and
leads to error accumulation across rounds. Even with these efforts, the generative performance of
rectified flow still lags behind the distillation methods such as consistency models [Song et al., 2023].
We show that rectified flows can indeed be competitive with the distillation methods in the low
NFE setting by applying Reflow with our proposed training techniques . Our techniques are based
on the observation that under realistic settings, the linear interpolation trajectories of the pre-trained
rectified flow rarely intersect with each other. This provides several insights: 1) applying Reflow once
is sufficient to obtain straight-line generative ODE in the optima, 2) the training loss of 2-rectified
flow has zero lower bound, and 3) other loss functions than the squared ℓ2distance can be used
during training. Based upon this finding, we propose several training techniques to improve Reflow,
including: (1) a U-shaped timestep distribution, (2) an LPIPS-huber premetric, which we find to be
critical for the few-step generative performance. After being initialized with pre-trained diffusion
models such as EDM [Karras et al., 2022], our method only requires one training stage without
additional Reflow or distillation stages, unlike previous works [Liu et al., 2022, 2023].
Our evaluation shows that on several datasets (CIFAR-10 [Krizhevsky et al., 2009], ImageNet
64×64 [Deng et al., 2009]), our improved rectified flow outperforms the state-of-the-art distilla-
tion methods such as consistency distillation (CD) [Song et al., 2023] and progressive distillation
(PD) [Salimans and Ho, 2022] in both one-step and two-step settings, and it rivals the performance
of the improved consistency training (iCT) [Song et al., 2023] in terms of the Frechet Inception
Distance [Heusel et al., 2017] (FID). Our training techniques reduce the FID of the previous
2-rectified flow [Liu et al., 2022] by about 75%(12.21→3.07) on CIFAR-10. Ablations on three
datasets show that the proposed techniques give a consistent and sizeable gain. We also showcase the
qualitative advantages of rectified flow such as few-step inversion, and its application to interpolation
and image-to-image translation.
2 Background
2.1 Rectified Flow
Rectified flow (see also flow matching [Lipman et al., 2022] and stochastic interpolant [Albergo and
Vanden-Eijnden, 2022]) is a generative model that smoothly transitions between two distributions
pxandpzby solving ODEs [Liu et al., 2022]. For x∼pxandz∼pz, we define the interpolation
between xandzasxt= (1−t)x+tzfort∈[0,1]. Liu et al. [2022] showed that for z0∼px, the
2<latexit sha1_base64="4dtqlamXd6JJv7oyTT2iRs3QELM=">AAACC3icbZDLSsNAFIZP6q3WW9Slm6FFcFUS8bYsiuCygr1AGsJkOmmHTi7MTMRSu3fjq7hxoYhbX8Cdb+OkDaKtPwx8/Occ5pzfTziTyrK+jMLC4tLySnG1tLa+sbllbu80ZZwKQhsk5rFo+1hSziLaUExx2k4ExaHPacsfXGT11i0VksXRjRom1A1xL2IBI1hpyzPLnRCrvu+jS2dKAbpD9+iHPeV6ZsWqWhOhebBzqECuumd+droxSUMaKcKxlI5tJcodYaEY4XRc6qSSJpgMcI86GiMcUumOJreM0b52uiiIhX6RQhP398QIh1IOQ193ZjvK2Vpm/ldzUhWcuSMWJamiEZl+FKQcqRhlwaAuE5QoPtSAiWB6V0T6WGCidHwlHYI9e/I8NA+r9kn1+PqoUjvP4yjCHpThAGw4hRpcQR0aQOABnuAFXo1H49l4M96nrQUjn9mFPzI+vgHS1ZpL</latexit>E[x|xt]
<latexit sha1_base64="Wv+gCXWngcAZVnakXcJZgjNLOMw=">AAAB+HicbVC7TsMwFL3hWcqjAUYWiwqJqUoQr7GChbFI9CG1IXJcp7XqOJHtIErUL2FhACFWPoWNv8FpM0DLkSwdnXOv7vEJEs6Udpxva2l5ZXVtvbRR3tza3qnYu3stFaeS0CaJeSw7AVaUM0GbmmlOO4mkOAo4bQej69xvP1CpWCzu9DihXoQHgoWMYG0k364k946f9SKsh0GIHie+XXVqzhRokbgFqUKBhm9/9foxSSMqNOFYqa7rJNrLsNSMcDop91JFE0xGeEC7hgocUeVl0+ATdGSUPgpjaZ7QaKr+3shwpNQ4CsxknlDNe7n4n9dNdXjpZUwkqaaCzA6FKUc6RnkLqM8kJZqPDcFEMpMVkSGWmGjTVdmU4M5/eZG0Tmruee3s9rRavyrqKMEBHMIxuHABdbiBBjSBQArP8Apv1pP1Yr1bH7PRJavY2Yc/sD5/AIgTkwY=</latexit>p0x<latexit sha1_base64="yMWzgac/sEpxIv9ZN/QCzxOlibM=">AAAB+HicbVC7TsMwFL3hWcqjAUYWiwqJqUoQr7GChbFI9CG1IXJcp7XqOJHtIErUL2FhACFWPoWNv8FpM0DLkSwdnXOv7vEJEs6Udpxva2l5ZXVtvbRR3tza3qnYu3stFaeS0CaJeSw7AVaUM0GbmmlOO4mkOAo4bQej69xvP1CpWCzu9DihXoQHgoWMYG0k364k966f9SKsh0GIHie+XXVqzhRokbgFqUKBhm9/9foxSSMqNOFYqa7rJNrLsNSMcDop91JFE0xGeEC7hgocUeVl0+ATdGSUPgpjaZ7QaKr+3shwpNQ4CsxknlDNe7n4n9dNdXjpZUwkqaaCzA6FKUc6RnkLqM8kJZqPDcFEMpMVkSGWmGjTVdmU4M5/eZG0Tmruee3s9rRavyrqKMEBHMIxuHABdbiBBjSBQArP8Apv1pP1Yr1bH7PRJavY2Yc/sD5/AImjkwc=</latexit>p1x
<latexit sha1_base64="oCC2OZz80yY68y1aRXsgPp7bkFM=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsK9gHtUDJppg3NZGKSKdSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7skJJGfauO63U1hZXVvfKG6WtrZ3dvfK+wdNHSeK0AaJeazaAdaUM0EbhhlO21JRHAWctoLRbea3xlRpFosHM5HUj/BAsJARbKzky17ajbAZBiF6mvbKFbfqzoCWiZeTCuSo98pf3X5MkogKQzjWuuO50vgpVoYRTqelbqKpxGSEB7RjqcAR1X46Cz1FJ1bpozBW9gmDZurvjRRHWk+iwE5mCfWil4n/eZ3EhNd+yoRMDBVkfihMODIxyhpAfaYoMXxiCSaK2ayIDLHCxNieSrYEb/HLy6R5VvUuqxf355XaTV5HEY7gGE7BgyuowR3UoQEEHuEZXuHNGTsvzrvzMR8tOPnOIfyB8/kD6jKSNQ==</latexit>pz<latexit sha1_base64="oCC2OZz80yY68y1aRXsgPp7bkFM=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsK9gHtUDJppg3NZGKSKdSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7skJJGfauO63U1hZXVvfKG6WtrZ3dvfK+wdNHSeK0AaJeazaAdaUM0EbhhlO21JRHAWctoLRbea3xlRpFosHM5HUj/BAsJARbKzky17ajbAZBiF6mvbKFbfqzoCWiZeTCuSo98pf3X5MkogKQzjWuuO50vgpVoYRTqelbqKpxGSEB7RjqcAR1X46Cz1FJ1bpozBW9gmDZurvjRRHWk+iwE5mCfWil4n/eZ3EhNd+yoRMDBVkfihMODIxyhpAfaYoMXxiCSaK2ayIDLHCxNieSrYEb/HLy6R5VvUuqxf355XaTV5HEY7gGE7BgyuowR3UoQEEHuEZXuHNGTsvzrvzMR8tOPnOIfyB8/kD6jKSNQ==</latexit>pz<latexit sha1_base64="oCC2OZz80yY68y1aRXsgPp7bkFM=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsK9gHtUDJppg3NZGKSKdSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7skJJGfauO63U1hZXVvfKG6WtrZ3dvfK+wdNHSeK0AaJeazaAdaUM0EbhhlO21JRHAWctoLRbea3xlRpFosHM5HUj/BAsJARbKzky17ajbAZBiF6mvbKFbfqzoCWiZeTCuSo98pf3X5MkogKQzjWuuO50vgpVoYRTqelbqKpxGSEB7RjqcAR1X46Cz1FJ1bpozBW9gmDZurvjRRHWk+iwE5mCfWil4n/eZ3EhNd+yoRMDBVkfihMODIxyhpAfaYoMXxiCSaK2ayIDLHCxNieSrYEb/HLy6R5VvUuqxf355XaTV5HEY7gGE7BgyuowR3UoQEEHuEZXuHNGTsvzrvzMR8tOPnOIfyB8/kD6jKSNQ==</latexit>pz<latexit sha1_base64="oCC2OZz80yY68y1aRXsgPp7bkFM=">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsK9gHtUDJppg3NZGKSKdSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7skJJGfauO63U1hZXVvfKG6WtrZ3dvfK+wdNHSeK0AaJeazaAdaUM0EbhhlO21JRHAWctoLRbea3xlRpFosHM5HUj/BAsJARbKzky17ajbAZBiF6mvbKFbfqzoCWiZeTCuSo98pf3X5MkogKQzjWuuO50vgpVoYRTqelbqKpxGSEB7RjqcAR1X46Cz1FJ1bpozBW9gmDZurvjRRHWk+iwE5mCfWil4n/eZ3EhNd+yoRMDBVkfihMODIxyhpAfaYoMXxiCSaK2ayIDLHCxNieSrYEb/HLy6R5VvUuqxf355XaTV5HEY7gGE7BgyuowR3UoQEEHuEZXuHNGTsvzrvzMR8tOPnOIfyB8/kD6jKSNQ==</latexit>pz
<latexit sha1_base64="yMWzgac/sEpxIv9ZN/QCzxOlibM=">AAAB+HicbVC7TsMwFL3hWcqjAUYWiwqJqUoQr7GChbFI9CG1IXJcp7XqOJHtIErUL2FhACFWPoWNv8FpM0DLkSwdnXOv7vEJEs6Udpxva2l5ZXVtvbRR3tza3qnYu3stFaeS0CaJeSw7AVaUM0GbmmlOO4mkOAo4bQej69xvP1CpWCzu9DihXoQHgoWMYG0k364k966f9SKsh0GIHie+XXVqzhRokbgFqUKBhm9/9foxSSMqNOFYqa7rJNrLsNSMcDop91JFE0xGeEC7hgocUeVl0+ATdGSUPgpjaZ7QaKr+3shwpNQ4CsxknlDNe7n4n9dNdXjpZUwkqaaCzA6FKUc6RnkLqM8kJZqPDcFEMpMVkSGWmGjTVdmU4M5/eZG0Tmruee3s9rRavyrqKMEBHMIxuHABdbiBBjSBQArP8Apv1pP1Yr1bH7PRJavY2Yc/sD5/AImjkwc=</latexit>p1x<latexit sha1_base64="hDzAQ++0NEruBFQYpdjpUiAEM+I=">AAAB+HicbVC7TsMwFL0pr1IeDTCyWFRITFVS8RorWBiLRB9SGyLHdVqrjhPZDqJE/RIWBhBi5VPY+BvcNgO0HMnS0Tn36h6fIOFMacf5tgorq2vrG8XN0tb2zm7Z3ttvqTiVhDZJzGPZCbCinAna1Exz2kkkxVHAaTsYXU/99gOVisXiTo8T6kV4IFjICNZG8u1y4me9COthEKLHyX3NtytO1ZkBLRM3JxXI0fDtr14/JmlEhSYcK9V1nUR7GZaaEU4npV6qaILJCA9o11CBI6q8bBZ8go6N0kdhLM0TGs3U3xsZjpQaR4GZnGZUi95U/M/rpjq89DImklRTQeaHwpQjHaNpC6jPJCWajw3BRDKTFZEhlpho01XJlOAufnmZtGpV97x6dntaqV/ldRThEI7gBFy4gDrcQAOaQCCFZ3iFN+vJerHerY/5aMHKdw7gD6zPH43tkwg=</latexit>p2x(a)Linear interpolation induced by 𝑝𝒙𝒛#(c)Linear interpolation induced by 𝑝𝒙𝒛$(b) Generative processof 1-rectified flow(d) Generative processof 2-rectified flow
<latexit sha1_base64="knBlG3mOsKgv7m48+64offhQGrQ=">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPia1l047KCfcB0KJk004ZmkiG5I5ahn+HGhSJu/Rp3/o2ZdhbaeiBwOOdecu4JE8ENuO63U1pZXVvfKG9WtrZ3dveq+wdto1JNWYsqoXQ3JIYJLlkLOAjWTTQjcShYJxzf5n7nkWnDlXyAScKCmAwljzglYCW/FxMYhRF+6kO/WnPr7gx4mXgFqaECzX71qzdQNI2ZBCqIMb7nJhBkRAOngk0rvdSwhNAxGTLfUkliZoJsFnmKT6wywJHS9knAM/X3RkZiYyZxaCfziGbRy8X/PD+F6DrIuExSYJLOP4pSgUHh/H484JpREBNLCNXcZsV0RDShYFuq2BK8xZOXSfus7l3WL+7Pa42boo4yOkLH6BR56Ao10B1qohaiSKFn9IreHHBenHfnYz5acoqdQ/QHzucPIPmRKw==</latexit>xt<latexit sha1_base64="knBlG3mOsKgv7m48+64offhQGrQ=">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPia1l047KCfcB0KJk004ZmkiG5I5ahn+HGhSJu/Rp3/o2ZdhbaeiBwOOdecu4JE8ENuO63U1pZXVvfKG9WtrZ3dveq+wdto1JNWYsqoXQ3JIYJLlkLOAjWTTQjcShYJxzf5n7nkWnDlXyAScKCmAwljzglYCW/FxMYhRF+6kO/WnPr7gx4mXgFqaECzX71qzdQNI2ZBCqIMb7nJhBkRAOngk0rvdSwhNAxGTLfUkliZoJsFnmKT6wywJHS9knAM/X3RkZiYyZxaCfziGbRy8X/PD+F6DrIuExSYJLOP4pSgUHh/H484JpREBNLCNXcZsV0RDShYFuq2BK8xZOXSfus7l3WL+7Pa42boo4yOkLH6BR56Ao10B1qohaiSKFn9IreHHBenHfnYz5acoqdQ/QHzucPIPmRKw==</latexit>xt
<latexit sha1_base64="4dtqlamXd6JJv7oyTT2iRs3QELM=">AAACC3icbZDLSsNAFIZP6q3WW9Slm6FFcFUS8bYsiuCygr1AGsJkOmmHTi7MTMRSu3fjq7hxoYhbX8Cdb+OkDaKtPwx8/Occ5pzfTziTyrK+jMLC4tLySnG1tLa+sbllbu80ZZwKQhsk5rFo+1hSziLaUExx2k4ExaHPacsfXGT11i0VksXRjRom1A1xL2IBI1hpyzPLnRCrvu+jS2dKAbpD9+iHPeV6ZsWqWhOhebBzqECuumd+droxSUMaKcKxlI5tJcodYaEY4XRc6qSSJpgMcI86GiMcUumOJreM0b52uiiIhX6RQhP398QIh1IOQ193ZjvK2Vpm/ldzUhWcuSMWJamiEZl+FKQcqRhlwaAuE5QoPtSAiWB6V0T6WGCidHwlHYI9e/I8NA+r9kn1+PqoUjvP4yjCHpThAGw4hRpcQR0aQOABnuAFXo1H49l4M96nrQUjn9mFPzI+vgHS1ZpL</latexit>E[x|xt]Figure 1: Rectified flow process (figure modified from Liu et al. [2022]). Rectified flow rewires
trajectories so there are no intersecting trajectories (a)→(b). Then, we take noise samples from pz
and their generated samples from p1
x, and linearly interpolate them (c). In Reflow, rectified flow is
applied again (c)→(d)to straighten flows. This procedure is repeated recursively.
following ODE yields the same marginal distribution as xtfor any t:
dzt
dt=vt(zt) :=1
t(zt−E[x|xt=zt]). (1)
Since x1=z, Eq. (1)transports pxtopz. We can also transport pztopxby drawing z1from pz
and solving the ODE backwards from t= 1tot= 0. During training, we estimate the conditional
expectation E[x|xt=zt]with a vector-valued neural network xθtrained on the squared ℓ2loss:
min
θEx,z∼pxzEt∼pt[ω(t)||x−xθ(xt, t)||2
2], (2)
where pxzis the joint distribution of xandz,xθis parameterized by θ, and ω(t)is a weighting
function. ptis chosen to be the uniform distribution on [0,1]in Liu et al. [2022, 2023]. In the
optimum of Eq. (2), xθbecomes the conditional expectation as it is a minimum mean squared error
(MMSE) estimator, which is then plugged into the ODE (1)to generate samples. Instead of predicting
the conditional expectation directly, Liu et al. [2022] choose to parameterize the velocity vtwith a
neural network vθand train it on
min
θEx,z∼pxzEt∼pt[||(z−x)−vθ(xt, t)||2
2], (3)
which is equivalent to Eq. (2) with ω(t) =1
t2. See Appendix. A.
In this paper, we consider the Gaussian marginal case, i.e., pz=N(0,I). In this case, if we define x
andzas independent random variables (i.e., pxz(x,z) =px(x)pz(z)) and use a specific nonlinear
interpolation instead of the linear interpolation for xt, Eq. (2)becomes the weighted denoising
objective of the diffusion model [Vincent, 2011], and Eq. (1)becomes the probability flow ODE
(PF-ODE) [Song et al., 2020b].
2.2 Reflow
Algorithm 1 Reflow Procedure
1:First iteration:
2: θ1= arg min θEx,z∼p0xzEt∼pt
ω(t)∥x−xθ(xt, t)∥2
2
▷Train 1-rectified flow
3:fork= 1toK−1do
4: Tk(z) =z+R0
11
t(zt−xθk(zt, t))dtwithz1=z
5: pk
xz(x,z) =pz(z)δ 
x−Tk(z)
▷Generate synthetic pairs for next coupling
6: θk+1= arg min θEx,z∼pkxzEt∼pt
ω(t)∥x−xθ(xt, t)∥2
2
▷Train (k+ 1) -rectified flow
7:end for
The independent coupling pxz(x,z) =px(x)pz(z)is known to lead to curved ODE trajectories,
which require a large number of function evaluations (NFE) to generate high-quality samples [Poola-
dian et al., 2023, Lee et al., 2023]. Reflow [Liu et al., 2022] is a recursive training algorithm to
find a better coupling that yields straighter ODE trajectories. Starting from the independent cou-
pling p0
xz(x,z) =px(x)pz(z), the Reflow algorithm generates pk+1
xz(x,z)from pk
xz(x,z)by first
3<latexit sha1_base64="d7jAMeI8kNki1G/TfsTOzuAoJvA=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvoqsyIr2XRjcsK9oHtUDJppg1NMkOSEcvQv3DjQhG3/o07/8ZMOwttPRA4nHMvOfcEMWfauO63U1haXlldK66XNja3tnfKu3tNHSWK0AaJeKTaAdaUM0kbhhlO27GiWASctoLRTea3HqnSLJL3ZhxTX+CBZCEj2FjpoSuwGQYhejrulStu1Z0CLRIvJxXIUe+Vv7r9iCSCSkM41rrjubHxU6wMI5xOSt1E0xiTER7QjqUSC6r9dJp4go6s0kdhpOyTBk3V3xspFlqPRWAns4R63svE/7xOYsIrP2UyTgyVZPZRmHBkIpSdj/pMUWL42BJMFLNZERlihYmxJZVsCd78yYukeVr1Lqrnd2eV2nVeRxEO4BBOwINLqMEt1KEBBCQ8wyu8Odp5cd6dj9lowcl39uEPnM8f83CQdQ==</latexit>x0<latexit sha1_base64="NvVOCCXexnG6YU2mzEdCeA9ipn0=">AAAB/3icbVDLSsNAFL2pr1pfUcGNm8EiuCqJ+FoW3bgRKtgHtCFMppN26OTBzEQssQt/xY0LRdz6G+78GydpFtp6YOBwzr3cM8eLOZPKsr6N0sLi0vJKebWytr6xuWVu77RklAhCmyTikeh4WFLOQtpUTHHaiQXFgcdp2xtdZX77ngrJovBOjWPqBHgQMp8RrLTkmnu9AKshwRzduGnOPR89TFyzatWsHGie2AWpQoGGa371+hFJAhoqwrGUXduKlZNioRjhdFLpJZLGmIzwgHY1DXFApZPm+SfoUCt95EdCv1ChXP29keJAynHg6cksoZz1MvE/r5so/8JJWRgnioZkeshPOFIRyspAfSYoUXysCSaC6ayIDLHAROnKKroEe/bL86R1XLPPaqe3J9X6ZVFHGfbhAI7AhnOowzU0oAkEHuEZXuHNeDJejHfjYzpaMoqdXfgD4/MHvqWV8Q==</latexit>Mx
<latexit sha1_base64="qvwG4eD9XD2xzi4UkVLq7KCK/b0=">AAAB8XicbVDLSgMxFL1TX7W+qi7dBIvoqsyIr2XRjcsK9oHtUDJppg3NJEOSEerQv3DjQhG3/o07/8ZMOwttPRA4nHMvOfcEMWfauO63U1haXlldK66XNja3tnfKu3tNLRNFaINILlU7wJpyJmjDMMNpO1YURwGnrWB0k/mtR6o0k+LejGPqR3ggWMgINlZ66EbYDIMQPR33yhW36k6BFomXkwrkqPfKX92+JElEhSEca93x3Nj4KVaGEU4npW6iaYzJCA9ox1KBI6r9dJp4go6s0kehVPYJg6bq740UR1qPo8BOZgn1vJeJ/3mdxIRXfspEnBgqyOyjMOHISJSdj/pMUWL42BJMFLNZERlihYmxJZVsCd78yYukeVr1Lqrnd2eV2nVeRxEO4BBOwINLqMEt1KEBBAQ8wyu8Odp5cd6dj9lowcl39uEPnM8f9nqQdw==</latexit>z0
<latexit sha1_base64="+iH849VdeBKAya2oX0Cc4ArZTnY=">AAAB83icbVDLSsNAFL2pr1pfVZduBovUVUnE17LoxmUF+4AmlMl00g6dTMLMRCyhv+HGhSJu/Rl3/o2TNAttPTBwOOde7pnjx5wpbdvfVmlldW19o7xZ2dre2d2r7h90VJRIQtsk4pHs+VhRzgRta6Y57cWS4tDntOtPbjO/+0ilYpF40NOYeiEeCRYwgrWRXDfEeuwH6VO9PhtUa3bDzoGWiVOQGhRoDapf7jAiSUiFJhwr1XfsWHsplpoRTmcVN1E0xmSCR7RvqMAhVV6aZ56hE6MMURBJ84RGufp7I8WhUtPQN5NZRrXoZeJ/Xj/RwbWXMhEnmgoyPxQkHOkIZQWgIZOUaD41BBPJTFZExlhiok1NFVOCs/jlZdI5aziXjYv781rzpqijDEdwDKfgwBU04Q5a0AYCMTzDK7xZifVivVsf89GSVewcwh9Ynz/IpZGI</latexit>x00
<latexit sha1_base64="MIOweftlI8tKh8WRm3H1CsBDIRQ=">AAACAXicdVDLSgMxFM34rPU16kZwEyxSNw4z06e7ohuXFewD2qFk0kwbmnmQZMQy1I2/4saFIm79C3f+jZm24gM9EDg5517uvceNGBXSNN+1hcWl5ZXVzFp2fWNza1vf2W2KMOaYNHDIQt52kSCMBqQhqWSkHXGCfJeRljs6T/3WNeGChsGVHEfE8dEgoB7FSCqpp+93fSSHrgdv8vAEfn3yPT1nGqfVsl2yoWmYZsUulFNiV4p2AVpKSZEDc9R7+lu3H+LYJ4HEDAnRscxIOgnikmJGJtluLEiE8AgNSEfRAPlEOMn0ggk8UkofeiFXL5Bwqn7vSJAvxNh3VWW6ovjtpeJfXieWXtVJaBDFkgR4NsiLGZQhTOOAfcoJlmysCMKcql0hHiKOsFShZVUIn5fC/0nTNqyyUbos5mpn8zgy4AAcgmNggQqogQtQBw2AwS24B4/gSbvTHrRn7WVWuqDNe/bAD2ivH2PFlZc=</latexit>x0 x00
Generative process(1-Rectified Flow)
<latexit sha1_base64="X30HuBYG2o/EZAWFDMWGeVRjMWo=">AAAB8HicdVDLSgMxFM3UV62vqks3wSK4GmamT3dFNy4r2Ie0Q8mkmTY0yQxJRihDv8KNC0Xc+jnu/BszbQUVPXDhcM693HtPEDOqtON8WLm19Y3Nrfx2YWd3b/+geHjUUVEiMWnjiEWyFyBFGBWkralmpBdLgnjASDeYXmV+955IRSNxq2cx8TkaCxpSjLSR7gYxkogxwobFkmNfNGpe1YOO7Th1r1zLiFeveGXoGiVDCazQGhbfB6MIJ5wIjRlSqu86sfZTJDXFjMwLg0SRGOEpGpO+oQJxovx0cfAcnhllBMNImhIaLtTvEyniSs14YDo50hP128vEv7x+osOGn1IRJ5oIvFwUJgzqCGbfwxGVBGs2MwRhSc2tEE9MBFibjAomhK9P4f+k49luza7eVErNy1UceXACTsE5cEEdNME1aIE2wICDB/AEni1pPVov1uuyNWetZo7BD1hvn1jckMs=</latexit>k<latexit sha1_base64="X30HuBYG2o/EZAWFDMWGeVRjMWo=">AAAB8HicdVDLSgMxFM3UV62vqks3wSK4GmamT3dFNy4r2Ie0Q8mkmTY0yQxJRihDv8KNC0Xc+jnu/BszbQUVPXDhcM693HtPEDOqtON8WLm19Y3Nrfx2YWd3b/+geHjUUVEiMWnjiEWyFyBFGBWkralmpBdLgnjASDeYXmV+955IRSNxq2cx8TkaCxpSjLSR7gYxkogxwobFkmNfNGpe1YOO7Th1r1zLiFeveGXoGiVDCazQGhbfB6MIJ5wIjRlSqu86sfZTJDXFjMwLg0SRGOEpGpO+oQJxovx0cfAcnhllBMNImhIaLtTvEyniSs14YDo50hP128vEv7x+osOGn1IRJ5oIvFwUJgzqCGbfwxGVBGs2MwRhSc2tEE9MBFibjAomhK9P4f+k49luza7eVErNy1UceXACTsE5cEEdNME1aIE2wICDB/AEni1pPVov1uuyNWetZo7BD1hvn1jckMs=</latexit>k
(a)(c)(d)(b)Generated from 𝐳+𝐱!−𝐱′′Generated from 𝐳
<latexit sha1_base64="a6a3dXOjEGMIkUett/ZTnIPV7AI=">AAAB9HicbVC7TsMwFL0pr1JeAUYWiwqVqUoQr7GChbFI9CG1UeW4TmvVcYLtVCpRv4OFAYRY+Rg2/ganzQAtR7J0dM69usfHjzlT2nG+rcLK6tr6RnGztLW9s7tn7x80VZRIQhsk4pFs+1hRzgRtaKY5bceS4tDntOWPbjO/NaZSsUg86ElMvRAPBAsYwdpIXjfEeugHKH2qVKY9u+xUnRnQMnFzUoYc9Z791e1HJAmp0IRjpTquE2svxVIzwum01E0UjTEZ4QHtGCpwSJWXzkJP0YlR+iiIpHlCo5n6eyPFoVKT0DeTWUi16GXif14n0cG1lzIRJ5oKMj8UJBzpCGUNoD6TlGg+MQQTyUxWRIZYYqJNTyVTgrv45WXSPKu6l9WL+/Ny7SavowhHcAyn4MIV1OAO6tAAAo/wDK/wZo2tF+vd+piPFqx85xD+wPr8ASSqkbQ=</latexit>z00
<latexit sha1_base64="vsl0Q12/jVR0BXLe+QCNUJpHjKg=">AAACLHicbVDLSgMxFM3UV62vUZdugkVakZYZkepGKHbjsoJ9QFtKJs20oZkHyR2xDv0gN/6KIC4s4tbvMH1ItXogcO4595J7jxMKrsCyRkZiaXlldS25ntrY3NreMXf3qiqIJGUVGohA1h2imOA+qwAHweqhZMRzBKs5/dLYr90xqXjg38IgZC2PdH3uckpAS22z1PQI9BwXP2Qy+BLPK3yCm64kNLZzMIxhmP227jM4h+dF5hi3zbSVtybAf4k9I2k0Q7ltvjQ7AY085gMVRKmGbYXQiokETgUbppqRYiGhfdJlDU194jHViifHDvGRVjrYDaR+PuCJ+nMiJp5SA8/RneMl1aI3Fv/zGhG4F62Y+2EEzKfTj9xIYAjwODnc4ZJREANNCJVc74ppj+iIQOeb0iHYiyf/JdXTvF3IF27O0sWrWRxJdIAOURbZ6BwV0TUqowqi6BE9ozc0Mp6MV+Pd+Ji2JozZzD76BePzC6popM4=</latexit>z00=z0+1 tt(x0 x00)Figure 2: An illustration of the intuition in Sec. 3. (a) If two linear interpolation trajectories intersect,
z′′−z′is parallel to x′−x′′. This generally maps z′′to an atypical (e.g., one with high autocorrelation
or a norm that is too large to be on a Gaussian annulus) realization of Gaussian noise, so the 1-rectified
flow cannot reliably map z′′tox′′onMx. (b) Generated samples from the pre-trained 1-rectified
flow starting from z∼ N(0,I)(right ), which is the standard setting, and z′′=z+ (x′−x′′), where
x′,x′′are sampled from 1-rectified flow trained on CIFAR-10 ( left). Qualitatively, we see that the
left samples have very low quality. (c) Empirically, we show the ℓ2norm of z′′=z+ (x′−x′′)
compared to z′, which is sampled from the standard Gaussian. z′′generally lands outside the annulus
of typical Gaussian noise. (d) z+ (x′−x′′)has high autocorrelation while the autocorrelation of
Gaussian noise is nearly zero in high-dimensional space.
generating synthetic (x,z)pairs from pk
xz, then training rectified flow on the generated synthetic
pairs (Figure 1 (b)−(d)). We call the vector field resulting from the k-th iteration of this procedure
k-rectified flow. Pseudocode for Reflow is provided in Algorithm. 1.
Convergence: Liu et al. [2022] show that Reflow trajectories are straight in the limit as K→ ∞ .
Hence, to achieve perfectly straight ODE paths that allow for accurate one-step generation, Reflow
may need to be applied many times until equilibrium, with each training stage requiring many
data-noise pairs, training the model until convergence, and a degradation in generated sample quality.
Prior work has empirically found that Reflow should be applied at least twice (i.e.3-rectified
flow) for reasonably good one-step generative performance [Liu et al., 2022, 2023]. This has been
a major downside for rectified flows compared to knowledge distillation methods, which typically
require only one distillation stage [Luhman and Luhman, 2021, Song et al., 2023, Zheng et al., 2022].
3 Applying Reflow Once is Sufficient
In this section, we argue that under practical settings, the trajectory curvature of the optimal 2-rectified
flow is actually close to zero. Hence, prior empirical results requiring more rounds of Reflow may
be the result of suboptimal training techniques, and we should focus on improving those training
techniques rather than stacking additional Reflow stages.
First, note that the curvature of the optimal 2-rectified flow is zero if and only if the linear interpolation
trajectories of 1-rectified flow-generated pairs do not intersect, or equivalently, E[x|xt= (1−t)x′+
tz′] =x′for all pairs (x′,z′)[Liu et al., 2022].
To begin with, consider the manifold Mxof the synthetic distribution p1(x) =R
p1(x,z)dz.
Consider two points x′andx′′from the manifold, and two noises z′andz′′that are mapped to x′
andx′′by 1-rectified flow. Here, we say two pairs (x′,z′)and(x′′,z′′)intersect if ∃t∈[0,1]s.t.
(1−t)x′+tz′= (1−t)x′′+tz′′. For example, in Figure 2(a), we observe that the two trajectories
intersect at an intermediate t.
For an intersection to exist at tit must hold that 1) 1-rectified flow maps z′′tox′′, and 2) z′′=
z′+1−t
t(x′−x′′)by basic geometry. However, note that for realistic data distributions and if
41-rectified flow is sufficiently well-trained, z′′=z′+1−t
t(x′−x′′)is not a common noise realization
(e.g., it is likely to have nonzero autocorrelation or a norm that is too large to be on a Gaussian
annulus), as shown visually in Figure 2(a). As 1-rectified flow is almost entirely trained on common
Gaussian noise inputs, it cannot generally map an atypical z′′toMx. Figure 2(c) shows qualitatively
that if we draw values of z′′by first drawing z′∼ N(0, I)and then adding (x′−x′′)for independent
draws of x′,x′′, thez′′vectors fall outside the annulus of typical standard Gaussian noise. Similarly,
Figure 2(d) shows that the constructed noise vectors z′′have higher autocorrelation than expected. As
a result, Figure 2(b) visually shows that the generated samples have little overlap with the expected
samples from typical draws of z′.
This suggests empirically that when training 2-rectified flow, intersections are rare (i.e. E[x|xt=
(1−t)x′+tz′]≈x′), which in turn implies that the optimal 2-rectified flow trajectories are nearly
straight. Hence, additional rounds of Reflow are unnecessary, while also degrading sample quality.
This intuition allows us to focus on better training techniques for 2-rectified flow rather than training
3- or 4-rectified flow. It also leads us to several improved techniques, discussed in Sec. 4.
Edge cases: Note that if ||x′−x′′||2is small, 1-rectified flow could map z′′to some point on Mx.
However, it does not alter the conclusion because the average of x′andx′′is close to x′anyway, so
E[x|xt= (1−t)x′+tz′]≈x′. Similarly, if tis close to 1,1−t
t(x′−x′′)≈0, so 1-rectified flow
can map z′′toMx. If the 1-rectified flow is L-Lipschitz, ||x′−x′′||2≤L||z′−z′′||2. Therefore,
the expectation E[x|xt]again will not deviate much from x′.
4 Improved Training Techniques for Reflow
The observation in Sec. 3 suggests that the optimal 2-rectified flow is nearly straight. Therefore, if
the one-step generative performance of the 2-rectified flow model is not as good as expected, it is
likely due to suboptimal training. In this section, we show that the few-step generative performance
of the 2-rectified flow can be significantly improved by applying several new training techniques.
4.1 Timestep distribution
As in diffusion models, rectified flows are trained on randomly sampled timesteps t, and the distribu-
tion from which tis sampled is an important design choice. Ideally, we want to focus the training
effort on timesteps that are more challenging rather than wasting computational resources on easy
tasks. One common approach is to focus on the tasks where the training loss is high [Shrivastava et al.,
2016]. However, the training error of rectified flows is not a reliable measure of difficulty because
different timesteps have different non-zero lower bounds. To understand this, let us decompose the
training error into two terms:
L(θ, t) :=1
t2E[||x−xθ(xt, t)||2
2] =1
t2E[||x−E[x|xt]||2
2]
| {z }
Lower bound+¯L(θ, t). (4)
Figure 3: Training loss of the vanilla 2-
rectified flow on CIFAR-10 measured on
5,000samples after 200,000iterations. The
shaded area represents the 1 standard devi-
ation of the loss. The dashed curve is our
U-shaped timestep distribution, scaled by a
constant factor for visualization.The first term does not depend on θand thus can-
not be reduced. The second term represents the ac-
tual minimizable training error, but its value cannot
be directly observed because the first term is usu-
ally unknown. Fortunately, because of the finding
in Sec. 3, we expect that the first term is nearly zero
when training 2-rectified flow, so we can use L(θ, t)
for designing the timestep distribution.
Figure 3 shows that the training loss of 2-rectified
flow is large at each end of the interval t∈[0,1]and
small in the middle. We thus propose to use a U-
shaped timestep distribution for pt. Specifically, we
define pt(u)∝exp(au) + exp( −au)onu∈[0,1].
We find that a= 4works well in practice (Table 1).
Compared to the uniform timestep distribution (con-
fig B), the U-shaped distribution (config C) improves
5Table 1: Effects of the improved training techniques. The baseline (config A) is the 2-rectified flow
with the uniform timestep distribution and the squared ℓ2metric [Liu et al., 2022]. Config B is
the improved baseline with EDM initialization (Sec. 4.3) and increased batch size ( 128→512on
CIFAR-10). FID (the lower the better) is computed using 50,000synthetic samples and the entire
training set. We train the models for 800,000iterations on CIFAR-10 and 1,000,000iterations on
AFHQ and FFHQ and report the best FID for each setting.
CIFAR-10 AFHQ 64×64 FFHQ 64×64
Base [Liu et al., 2022] ( A) 12.21 - - - - -
(A) + EDM init + larger batch size ( B) 7.14 3.61 12.39 4.16 8.84 4.79
(B) + Our pt(C) 5.17 3.37 9.03 3.61 6.81 4.66
(C) + Huber ( D) 5.24 3.34 8.20 3.55 7.06 4.79
(C) + LPIPS-Huber ( E) 3.42 2.95 4.13 3.15 5.21 4.26
(C) + LPIPS-Huber-1
t(F) 3.38 2.76 4.11 3.12 5.65 4.41
(F) + Incorporating real data ( G) 3.07 2.40 - - - -
NFE 1 2 1 2 1 2
the FID of 2-rectified flow from 7.14to5.17(a28% improvement) on CIFAR-10, 12.39to9.03
(27%) on AFHQ, and 8.84to6.81(23%) on FFHQ in the one-step setting.
For 1-rectified flow training, ptwas chosen to be the uniform distribution [Liu et al., 2022, 2023] or
logit-normal distribution [Esser et al., 2024] which puts more emphasis on the middle of the interval.
When training 1-rectified flow, a model learns to simply output the dataset average when t= 1and
the noise average (i.e., zero) when t= 0. The meaningful part of the training thus happens in the
middle of the interval. In contrast, from Eq. (3)we can see that 2-rectified flow learns to directly
predict the data from the noise at t= 1and the noise from the data at t= 0, which are nontrivial
tasks. Therefore, the U-shaped timestep distribution is more suitable for 2-rectified flow.
4.2 Loss function
Previously, the squared ℓ2distance was used as the training metric for rectified flow to obtain the
MMSE estimator E[x|xt]. However, as we have shown in Sec. 3 that E[x|xt= (1−t)x′+tz′]≈x′,
we can generalize Eq. (2) or equivalently Eq. (3) to any premetric m(i.e.m(a,b) = 0⇔a=b):
min
θEx,z∼pxzEt∼pt[m(z−x,vθ(xt, t))], (5)
Note that without the intuition in Sec. 3, only the squared ℓ2distance would have been a valid
premetric , as any other premetric makes the model deviate from the intended optimum (the posterior
expectation E[x|xt]). Although the choice of mdoes not affect the optimum, it does affect the
training dynamics and thus the obtained model. Other than the squared ℓ2distance, we consider the
following premetrics:
•Pseudo-Huber [Charbonnier et al., 1997, Song and Dhariwal, 2023]: mhub(z−x,vθ((xt, t)) =p
||z−x−vθ(xt, t)||2
2+c2−c, where c= 0.00054 dwithdbeing data dimensionality.
•LPIPS-Huber: mlp-hub(z−x,vθ(xt, t)) = (1 −t)mhub(z−x,vθ(xt, t)) + LPIPS (x,xt−t·
vθ(xt, t)), where LPIPS (·,·)is the learned perceptual image patch similarity [Zhang et al., 2018].
•LPIPS-Huber-1
t:mlp-hub-1
t(z−x,vθ(xt, t)) = (1 −t)mhub(z−x,vθ(xt, t)) +1
tLPIPS (x,xt−
t·vθ(xt, t)),
The Pseudo-Huber loss is less sensitive to the outliers than the squared ℓ2loss, which can potentially
reduce the gradient variance [Song and Dhariwal, 2023] and make training easier. In our initial
experiments, we found that the Pseudo-Huber loss tends to work better than the squared ℓ2loss with
a small batch size (e.g. 128 on CIFAR-10). When the batch size is sufficiently large, it performs on
par with the squared ℓ2loss on CIFAR-10 and FFHQ-64 and outperforms it on AFHQ-64, as shown
in Table 1. As it is less sensitive to the batch size, we choose to use the Pseudo-Huber loss in the
following experiments.
We also explore the LPIPS, which forces the model to focus on reducing the perceptual distance
between the generated data and the ground truth. Since LPIPS is not a premetric as two different
6Table 2: The converted time and scale for the variance preserving (VP) and variance exploding (VE)
diffusion models. Here, α(t) = exp( −1
2Rt
0(19.9s+ 0.1)ds)following Song et al. [2020b], and the
perturbation kernel of the VE diffusion is N(x, t2I)following Karras et al. [2022].
tVP tVE sVP sVE
1
9.95
−0.05 +q
0.0025−19.9·ln1−t√
(1−t)2+t2
t
1−tα(tVP)
1−t1
1−t
points could have zero LPIPS if they are perceptually similar, we use it in combination with the
Pseudo-Huber loss with the weighting 1−t, thereby relying more on LPIPS when tis close to 1
where the task is more challenging. Note that in mlp-hub, the gradient vanishes when tis close to
zero. To compsensate, we experiment with mlp-hub-1
twhere we multiply LPIPS by1
t. Compared
to config D, the LPIPS-Huber loss improves the FID of 2-rectified flow from 5.24to3.38(a35%
improvement) on CIFAR-10, 8.20to4.11(50%) on AFHQ, and 7.06to5.21(26%) on FFHQ in the
one-step setting, as seen in Table 1.
4.3 Initialization with pre-trained diffusion models
Training 1-rectified flow from scratch is computationally expensive. Recently, Pokle et al. [2023]
showed that pre-trained diffusion models can be used to approximate E[x|xt=zt]in Eq. (1)by
adjusting the signal-to-noise ratio. The following proposition is the special cases of Lemma 2 of
Pokle et al. [2023] restated with extended proof and a minor fix. We provide the constants and proof
in Appendix. C.1.
Proposition 1 LetpRE(x|xt, t)be the posterior distribution of the perturbation kernel N((1−
t)x, t2I). Also, let pVP(x|xt, t)andpVE(x|xt, t)be the posterior distributions of N(α(t)x,(1−
α(t))2I)andN(x, t2I), each. Then,
Z
pRE(x|xt=zt, t)xdx=Z
pVP(x|xt=sVPzt, tVP)xdx=Z
pVE(x|xt=sVPzt, tVE)xdx,
(6)
where sVPandsVEare the scaling factors and tVPandtVEare the converted times for the VP and VE
diffusion models, respectively.
We have explicitly computed the time and scale conversion factors for the VP and VE diffusion
models in Table 2. See Appendix C for derivation.
Proposition 1 allows us to initialize the Reflow with the pre-trained diffusion models such as
EDM [Karras et al., 2022] or DDPM [Ho et al., 2020] and use Table 2 to adjust the time and
scaling factors.
Starting from the vanilla 2-rectified flow setup [Liu et al., 2022] (config A), we initialize 1-rectified
flow with the pre-trained EDM (VE). We also increase the batch size from 128to512on CIFAR-10
compared to Liu et al. [2022]. Overall, these improve the FID of 2-rectified flow from 12.21to7.14
(a42% improvement) in the one-step setting on CIFAR-10 (config B).
4.4 Incorporating real data
Training 2-rectified flow does not require real data (i.e., it can be data-free), but we can use real data
if it is available. To see the effects of incorporating real data, we integrate the generative ODE of
1-rectified flow backward from t= 0tot= 1using an NFE of 128 to collect 50,000pairs of (real
data, synthetic noise) on CIFAR-10. For quick validation, we take the pre-trained 2-rectified flow
model (config F) and fine-tune it using the (real data, synthetic noise) pairs for 5,000iterations with
a learning rate of 1e-5. This improves the FID of 2-rectified flow from 3.38to3.07in the one-step
setting on CIFAR-10 (config G).
In this fine-tuning setting, we also explored using (synthetic data, real noise) pair with a probability
ofp, but we found that not incorporating (synthetic data, real noise) pairs at all (i.e., p= 0) performs
7Table 3: Unconditional generation on CIFAR-10.
METHOD NFE ( ↓) FID ( ↓) IS (↑)
Diffusion models
Score SDE [Song et al., 2020b] 2000 2.38 9.83
DDPM [Ho et al., 2020] 1000 3.17 9.46
LSGM [Vahdat et al., 2021] 147 2.10
EDM [Karras et al., 2022] 35 1.97
Distilled diffusion models
Knowledge Distillation [Luhman and Luhman, 2021] 1 9.36
DFNO (LPIPS) [Zheng et al., 2022] 1 3.78
TRACT [Berthelot et al., 2023] 1 3.78
2 3.32
PD [Salimans and Ho, 2022] 1 9.12
2 4.51
Score distillation
Diff-Instruct [Luo et al., 2024] 1 4.53 9.89
DMD [Yin et al., 2023] 1 3.77
GANs
BigGAN [Brock et al., 2018] 1 14.7 9.22
StyleGAN2 [Karras et al., 2020b] 1 8.32 9.21
StyleGAN2-ADA [Karras et al., 2020a] 1 2.92 9.83
Consistency models
CD (LPIPS) [Song et al., 2023] 1 3.55 9.48
2 2.93 9.75
CT (LPIPS) [Song et al., 2023] 1 8.70 8.49
2 5.83 8.85
iCT [Song and Dhariwal, 2023] 1 2.83 9.54
2 2.46 9.80
iCT-deep [Song and Dhariwal, 2023] 1 2.51 9.76
2 2.24 9.89
CTM [Kim et al., 2023] 1 5.19
CTM [Kim et al., 2023] + GAN 1 1.98
Rectified flows
1-rectified flow (+distill) [Liu et al., 2022] 1 6.18 9.08
2-rectified flow [Liu et al., 2022] 1 12.21 8.08
110 3.36 9.24
+distill [Liu et al., 2022] 1 4.85 9.01
3-rectified flow [Liu et al., 2022] 1 8.15 8.47
104 3.96 9.01
+Distill [Liu et al., 2022] 1 5.21 8.79
2-rectified flow++ (ours) 1 3.07
2 2.40Table 4: Class-conditional generation on Ima-
geNet 64×64.
METHOD NFE ( ↓) FID ( ↓) Prec. ( ↑) Rec. ( ↑)
Diffusion models
DDIM [Song et al., 2020a] 50 13.7 0.65 0.56
10 18.3 0.60 0.49
DPM solver [Lu et al., 2022] 10 7.93
20 3.42
DEIS [Zhang and Chen, 2022] 10 6.65
20 3.10
DDPM [Ho et al., 2020] 250 11.0 0.67 0.58
iDDPM [Nichol and Dhariwal, 2021] 250 2.92 0.74 0.62
ADM [Dhariwal and Nichol, 2021] 250 2.07 0.74 0.63
EDM [Karras et al., 2022] 79 2.30
Distilled diffusion models
DFNO (LPIPS) [Zheng et al., 2022] 1 7.83 0.61
TRACT [Berthelot et al., 2023] 1 7.43
2 4.97
BOOT [Gu et al., 2023] 1 16.3 0.68 0.36
PD [Salimans and Ho, 2022] 1 15.39 0.59 0.62
2 8.95 0.63 0.65
4 6.77 0.66 0.65
Score distillation
Diff-Instruct [Luo et al., 2024] 1 5.57
DMD [Yin et al., 2023] 1 2.62
GANs
BigGAN-deep [Brock et al., 2018] 1 4.06 0.79 0.48
Consistency models
CD (LPIPS) [Song et al., 2023] 1 6.20 0.68 0.63
2 4.70 0.69 0.64
3 4.32 0.70 0.64
CT (LPIPS) [Song et al., 2023] 1 13.0 0.71 0.47
2 11.1 0.69 0.56
iCT [Song and Dhariwal, 2023] 1 4.02 0.70 0.63
2 3.20 0.73 0.63
iCT-deep [Song and Dhariwal, 2023] 1 3.25 0.72 0.63
2 2.77 0.74 0.62
CTM + GAN [Kim et al., 2023] 1 1.92 0.57
2 1.73 0.57
Rectified flows
2-rectified flow++ (ours) 1 4.31
2 3.64
The red rows correspond to the top-5 baselines for the 1-NFE setting, and the blue rows correspond to the top 5 baselines for the 2-NFE setting.
The lowest FID scores for 1-NFE and 2-NFE are boldfaced .
the best. We expect that training from scratch will further improve the performance with different
values of p, and leave it to future work. A similar idea is also explored in Anonymous [2024].
5 Experiments
We call these combined improvements to Reflow training 2-rectified flow++ and evaluate it on four
datasets: CIFAR-10 Krizhevsky et al. [2009], AFHQ Choi et al. [2020], FFHQ Karras et al. [2019],
and ImageNet Deng et al. [2009]. We compare our improved Reflow to up to 20 recent baselines, in
the families of diffusion models, distilled diffusion models, score distillation, GANs, consistency
models, and rectified flows. The details of our experimental setup are included in Appendix E.
5.1 Unconditional and class-conditional image generation
In Tables 3 and 4, we compare 2-rectified flow++ with the state-of-the-art methods on CIFAR-10 and
ImageNet 64×64. We observe two main messages:
On both datasets, 2-rectified flow++ (ours) outperforms or is competitive with SOTA baselines
in the 1-2 NFE regime. On CIFAR-10 (Table 3), our 2-rectified flow achieves an FID of 3.07in one
step, surpassing existing distillation methods such as consistency distillation (CD) [Song et al., 2023],
progressive distillation (PD) [Salimans and Ho, 2022], diffusion model sampling with neural operator
(DSNO) [Zheng et al., 2022], and TRAnsitive Closure Time-distillation (TRACT) [Berthelot et al.,
2023]. On ImageNet 64×64(Table 4), our model surpasses the distillation methods such as CD,
PD, DFNO, TRACT, and BOOT in one-step generation. We also close the gap with iCT (4.01 vs
8(a) CIFAR-10
 (b) AFHQ 64×64
 (c) FFHQ 64×64
Figure 4: Effects of ODE Solver and new update rule.
4.31), the state-of-the-art consistency model, even with half the batch size. Note that on ImageNet,
our model does not use real data during training, while consistency training (CT) requires real data.
We believe we could further reduce the gap by using config G in Tab. 1. Uncurated samples of our
model are provided in Appendix. G.
2-rectified flow++ reduces the FID of 2-rectified flows by up to 75%. Compared to vanilla rectified
flows [Liu et al., 2022], our one-step FID on CIFAR-10 is lower than that of the previous 2-rectified
flow by 9.14(a reduction of 75%), and of the 3-rectified flow by 5.08(see also Table 1 for ablations
on other datasets). In addition, it outperforms the previous 2-rectified flow with 110 NFEs using only
one step and also surpasses 2-rectified flow +distillation, which requires an additional distillation
stage.
5.2 Reflow can be computationally more efficient than other distillation methods
Table 6: Comparison of the number of forward
passes. Reflow uses 395M forward passes for gen-
erating pairs and 1,433.6M for training.
Method Per iteration Total Rel. total cost
Reflow 1 1828 .6M ×1
CD 4 5734 .4M ×3.1
CT 2 2867 .2M ×1.5At first glance, Reflow seems computationally
expensive compared to CD and CT as it requires
generating synthetic pairs before training. How-
ever, CD requires 4 (1 for student, 1 for teacher,
and 2 for Heun’s solver) forward passes for
each training iteration, and CT requires 2 (1
for student and 1 for teacher) forward passes,
while Reflow requires only 1. For example,
in our ImageNet experiment setting, the total
number of forward passes for Reflow is 395M +
1433.6M =1828.6M (395M for generating pairs
and 1,433.6M for training), while the total numbers of forward passes for CD and CT would be
1,433.6·4 = 5 ,734.4M and 1,433.6·2 = 2 ,867.2M under the same setting. See Table 6 for the
comparison. Moreover, generating pairs is a one-time cost since we can reuse the pairs for multiple
training runs.
In terms of the storage cost, the synthetic images for ImageNet 64×64require 42 GB. For noise, we
only store the states of the random number generator, which is negligible.
While these results should be further validated for larger datasets, our results suggest that the fact that
Reflow requires generating synthetic pairs does not necessarily make it less computationally efficient
than other distillation methods.
5.3 Effects of samplers
Unlike distillation methods, rectified flow is a neural ODE, and its outputs approach the true solution
of the ODE as NFE increases (i.e., precision grows). Figure 4 shows that with the standard Euler
solver, FID decreases as NFE increases on all datasets. Moreover, Heun’s second-order solver further
improves the trade-off curve between FID and NFE. This suggests that there may be further room for
improvement by using more advanced samplers. We provide some preliminary ideas towards this
goal in Appendix D.
9(a) Reconstruction loss
 (b) Distribution of ∥z∥2
2
OursEDM
DataNoiseReconstructed (c) Inversion/reconstruction results
Figure 5: Inversion results on CIFAR-10. (a) Reconstruction error between real and reconstructed
data is measured by the mean squared error (MSE), where the x-axis represents NFEs used for
inversion and reconstruction (e.g. 2 means 2 for inversion and 2 for reconstruction). (b) Distribution
of||z||2
2of the inverted noises as a proxy for Gaussianity (NFE = 8). The green histogram represents
the distribution of true noise, which is Chi-squared with 3×32×32 = 3072 degrees of freedom. (c)
Inversion and reconstruction results using (8 + 8) NFEs. With only 8 NFEs, EDM fails to produce
realistic noise, and also the reconstructed samples are blurry.
(a) InterpolationLion→TigerTiger→Lion(b) Image translation
Figure 6: Applications of few-step inversion. (a) Interpolation between two real images. (b) Image-
to-image translation. The total NFEs used are 6 (4 for inversion and 2 for generation).
5.4 Inversion
Unlike distillation methods, rectified flows are neural ODEs, thus they allow for inversion from data
to noise by simply integrating the ODE in the backward direction. In diffusion models, inversion has
been used for various applications such as image editing [Hertz et al., 2022, Kim et al., 2022, Wallace
et al., 2023, Su et al., 2022, Hong et al., 2023] and watermarking [Wen et al., 2023], but it usually
requires many NFEs. Figure 5 (a) demonstrates that our 2-rectified flow++ achieves significantly
lower reconstruction error than EDM. Notably, the reconstruction error of 2-rectified flow++ with
only 2 NFEs is lower than that of EDM with 16 NFEs. In (b), we compare the quality of the inverted
noise, where we find that the noise vectors of 2-rectified flow are more Gaussian-like than those of
EDM, in the sense that their norm is closer to that of typical Gaussian noise. These are also shown
visually in (c). In Figure 6, we show two applications of inversion: interpolating between two real
images (a) and image-to-image translation (b). Notably, the total NFE used is only 6 (4 for inversion
and 2 for generation), which is significantly lower than what is typically required in diffusion models
(≥100) [Hong et al., 2023].
6 Conclusion
In this work, we propose several improved training techniques for rectified flows, including the U-
shaped timestep distribution and LPIPS-Huber loss. We show that by combining these improvements,
2-rectified flows++ outperforms the state-of-the-art distillation methods in the 1-2 NFE regime on
CIFAR-10 and ImageNet 64×64and closes the gap with iCT, the state-of-the-art consistency model.
2-rectified flows++ have limitations though—they still do not outperform the best consistency models
(iCT), and their training is slower (by about 15% per iteration on ImageNet) than previous rectified
flows because of the LPIPS loss. Despite these shortcomings, the training techniques we propose
can easily and significantly boost the performance of rectified flows in the low NFE setting, without
harming performance at the higher NFE setting.
10Acknowledgments
This work was made possible in part by the National Science Foundation under grant CCF-2338772,
CNS-2325477, as well as generous support from Google, the Sloan Foundation, Intel, and Bosch. This
work used Bridges-2 GPU at the Pittsburgh Supercomputing Center through allocation CIS240037
from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS)
program, which is supported by National Science Foundation grants 2138259, 2138286, 2138307,
2137603, and 2138296 Boerner et al. [2023].
References
Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants.
arXiv preprint arXiv:2209.15571 , 2022.
Anonymous. Balanced conic rectified flow. In Submitted to The Thirteenth International Conference
on Learning Representations , 2024. URL https://openreview.net/forum?id=ctSjIlYN74 .
under review.
David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel
Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure
time-distillation. arXiv preprint arXiv:2303.04248 , 2023.
Timothy J Boerner, Stephen Deems, Thomas R Furlani, Shelley L Knuth, and John Towns. Access:
Advancing innovation: Nsf’s advanced cyberinfrastructure coordination ecosystem: Services &
support. In Practice and Experience in Advanced Research Computing , pages 173–176. 2023.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096 , 2018.
Pierre Charbonnier, Laure Blanc-Féraud, Gilles Aubert, and Michel Barlaud. Deterministic edge-
preserving regularization in computed imaging. IEEE Transactions on image processing , 6(2):
298–311, 1997.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. Advances in neural information processing systems , 31, 2018.
Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for
multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pages 8188–8197, 2020.
Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based
semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427 , 2022.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,
pages 248–255. Ieee, 2009.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances
in Neural Information Processing Systems , 34, 2021.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam
Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for
high-resolution image synthesis. arXiv preprint arXiv:2403.03206 , 2024.
Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua M Susskind. Boot: Data-free distil-
lation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured
Probabilistic Inference {\&}Generative Modeling , 2023.
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-
to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 , 2022.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural
information processing systems , 30, 2017.
11Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems , 33:6840–6851, 2020.
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J
Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458 , 2022.
Seongmin Hong, Kyeonghyun Lee, Suh Yoon Jeon, Hyewon Bae, and Se Young Chun. On exact
inversion of dpm-solvers. arXiv preprint arXiv:2311.18387 , 2023.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pages 4401–4410, 2019.
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. Advances in Neural Information Processing
Systems , 33:12104–12114, 2020a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing
and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 8110–8119, 2020b.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-
based generative models. arXiv preprint arXiv:2206.00364 , 2022.
Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Ue-
saka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning
probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279 , 2023.
Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models
for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 2426–2435, 2022.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Toronto, ON, Canada , 2009.
Sangyun Lee, Beomsu Kim, and Jong Chul Ye. Minimizing trajectory curvature of ode-based
generative models. arXiv preprint arXiv:2301.12003 , 2023.
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching
for generative modeling. arXiv preprint arXiv:2210.02747 , 2022.
Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint
arXiv:2209.14577 , 2022.
Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and
transfer data with rectified flow. arXiv preprint arXiv:2209.03003 , 2022.
Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough
for high-quality diffusion-based text-to-image generation. arXiv preprint arXiv:2309.06380 , 2023.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A
fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint
arXiv:2206.00927 , 2022.
Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved
sampling speed. arXiv preprint arXiv:2101.02388 , 2021.
Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-
instruct: A universal approach for transferring knowledge from pre-trained diffusion models.
Advances in Neural Information Processing Systems , 36, 2024.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. arXiv preprint arXiv:1710.03740 , 2017.
12Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for
editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6038–6047, 2023.
Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
InInternational Conference on Machine Learning , pages 8162–8171. PMLR, 2021.
Ashwini Pokle, Matthew J Muckley, Ricky TQ Chen, and Brian Karrer. Training-free linear image
inversion via flows. arXiv preprint arXiv:2310.04432 , 2023.
Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman,
and Ricky Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv
preprint arXiv:2304.14772 , 2023.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.
Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv
preprint arXiv:2202.00512 , 2022.
Neta Shaul, Juan Perez, Ricky TQ Chen, Ali Thabet, Albert Pumarola, and Yaron Lipman. Bespoke
solvers for generative flow models. arXiv preprint arXiv:2310.19075 , 2023.
Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors
with online hard example mining. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 761–769, 2016.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learning ,
pages 2256–2265. PMLR, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502 , 2020a.
Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv
preprint arXiv:2310.14189 , 2023.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in Neural Information Processing Systems , 32, 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456 , 2020b.
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint
arXiv:2303.01469 , 2023.
Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for
image-to-image translation. arXiv preprint arXiv:2203.08382 , 2022.
Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space.
Advances in Neural Information Processing Systems , 34:11287–11302, 2021.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computa-
tion, 23(7):1661–1674, 2011.
Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transfor-
mations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 22532–22541, 2023.
Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers
for diffusion models by differentiating through sample quality. In International Conference on
Learning Representations , 2021.
13Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fin-
gerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030 ,
2023.
Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman,
and Taesung Park. One-step diffusion with distribution matching distillation. arXiv preprint
arXiv:2311.18828 , 2023.
Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator.
arXiv preprint arXiv:2204.13902 , 2022.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 586–595, 2018.
Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast
sampling of diffusion models via operator learning. arXiv preprint arXiv:2211.13449 , 2022.
14(a) Reconstruction loss
 (b) Distribution of ∥z∥2
2
Figure 7: (a) Autocorrelation plot on CIFAR-10 analogous to Figure 2(d). (b) t-SNE visualization of
the inception-v3 features of the samples in Figure 2(b). They show negligible overlap.
A Equivalence of v-parameterization and x-parameterization
Given xt= (1−t)x+tzandz−x= (xt−x)/t, the equivalence of the x-parameterization (Eq. (2))
and the v-parameterization (Eq. (3)) can be shown in the following way. The result is borrowed from
the Appendix of Lee et al. [2023], and we provide here for completeness.
Z1
0E[||(z−x)−vθ(xt, t)||2
2]dt=Z1
0E[||(xt−x)/t−vθ(xt, t)||2
2]dt (7)
=Z1
0E[||(xt−x)/t−(xt−xθ(xt, t))/t||2
2]dt (8)
=Z1
0E[1
t2||x−xθ(xt, t))||2
2]dt. (9)
This is equivalent to Eq. (2) with ω(t) = 1 /t2.
B Additional Details for Figure 2
In this section, we provide additional results details for Figure 2. Figure 7(a) shows the autocorrelation
histogram on CIFAR-10 while Figure 2(d) is on FFHQ-64. Figure 7(b) shows that the inception
features of the samples in Figure 2(b) rarely overlap with each other.
For the autocorrelation plots, we use 30,000 pairs of (x′,x′′)and randomly sample zfrom the
standard Gaussian distribution. For a d-dimensional vector u, we define the autocorrelation as:
Ru(l) =1
d−ld−lX
k=1ukuk+l, (10)
where ukis the k-th element of a vector uandl >0represents the lag.
C Initialization Results
C.1 Proof of Proposition 1
Consider two perturbation kernels pt(xt|x) =N(s(t)x, σ(t)2I)andp′
t(xt|x) =N(s′(t)x, σ′(t)2I):
pt(xt|x) =1
(2πσ(t)2)d/2exp(−1
2σ(t)2||xt−s(t)x||2
2) (11)
p′
t(xt|x) =1
(2πσ′(t)2)d/2exp(−1
2σ′(t)2||xt−s′(t)x||2
2). (12)
15Lett′(t)be such thats(t)
σ(t)=s′(t′)
σ′(t′). We will show that pt(x|xt) =p′
t′(x|s′(t′)
s(t)xt). We start by
showing:
p′
t′(s′(t′)
s(t)xt|x) =1
(2πσ′(t′)2)d/2exp(−1
2σ′(t′)2||s′(t′)
s(t)xt−s′(t′)x||2
2) (13)
=1
(2πσ′(t′)2)d/2exp(−1
2σ′(t′)2||s′(t′)
s(t)(xt−s(t)x)||2
2) (14)
=1
(2πσ′(t′)2)d/2exp(−1
2σ′(t′)2s′(t′)2
s(t)2||xt−s(t)x||2
2) (15)
=1
(2πσ′(t′)2)d/2exp(−1
2σ(t)2s(t)2
s(t)2||xt−s(t)x||2
2) (16)
=1
(2πσ′(t′)2)d/2exp(−1
2σ(t)2||xt−s(t)x||2
2) (17)
=1
(2πσ′(t′)2)d/2(2πσ(t)2)d/2
(2πσ(t)2)d/2exp(−1
2σ(t)2||xt−s(t)x||2
2) (18)
=(2πσ(t)2)d/2
(2πσ′(t′)2)d/2pt(xt|x) (19)
=σ(t)
σ′(t′)d
pt(xt|x). (20)
Here, Eq. (20) says that pt(xt|x)∝p′
t′(s′(t′)
s(t)xt|x)(but not equal), which is a minor fix from the
original proof [Pokle et al., 2023]. Then, we have
pt(x|xt) =1
pt(xt)px(x)pt(xt|x) (21)
p′
t′(x|s′(t′)
s(t)xt) =σ(t)
σ′(t′)d1
p′
t′(s′(t′)
s(t)xt)px(x)pt(xt|x) (22)
Since p′
t′(x|s′(t′)
s(t)xt)should be integrated to one,
σ(t)
σ′(t′)d1
p′
t′(s′(t′)
s(t)xt)=R
px(x)pt(xt|x)dxand
thus two densities are equal. As the posterior densities are the same, their expectations are also the
same.
In our case, s(t) = 1−t,σ(t) =t, and p′
t(xt|x)is the perturbation kernel of either the VP or VE
diffusion model. Now we have to find t′such that1−t
t=s′(t′)
σ′(t′).
C.2 Perturbation Kernel Instantiations
We next provide the values of the converted time and scale of the variance preserving (VP) and
variance exploding (VE) diffusion models.
VE diffusion model Karras et al. [2022] defines the perturbation kernel of the VE diffusion model
asp′
t(xt|x) =N(x, t2I). Then, t′satisfies1−t
t=s′(t′)
σ′(t′)=1
t′, sot′=t
1−t, ands′(t′)
s(t)=1
1−t, which
correspond to tVEandsVEin Table 2.
VP diffusion model Song et al. [2020b] defines the perturbation kernel of the VP diffusion model
asp′
t(xt|x) =N(α(t)x,(1−α(t)2)I), where α(t) := exp( −1
2Rt
0(19.9s+ 0.1)ds)defined on
t∈[0,1]. Then, t′satisfies1−t
t=s′(t′)
σ′(t′)=α(t′)√
1−α(t′)2. From here, we have
(1−t)2
t2=α(t′)2
1−α(t′)2(23)
α(t′) =s
(1−t)2
t2+ (1−t)2, (24)
16where we used the fact that α(t)>0. Since α(t)is a monotonically decreasing function for t≥0,
we can use its inverse α−1to find t′=α−1(q
(1−t)2
t2+(1−t)2).
y=α(t) = exp( −1
2Zt
0(19.9s+ 0.1)ds) = exp( −19.9
4t2−0.05t) (25)
lny=−19.9
4t2−0.05t (26)
19.9
4t2+ 0.05t+ lny= 0 (27)
Applying the quadratic formula, we have
t=−0.05±q
0.052−4·19.9
4lny
2·19.9
4=−0.05±√0.0025−19.9 lny
9.95. (28)
Since y=α(t)is monotonically decreasing, we can choose the positive root:
α−1(y) =−0.05 +√0.0025−19.9 lny
9.95. (29)
Now, we arrive at
t′=α−1(s
(1−t)2
t2+ (1−t)2) =−0.05 +r
0.0025−19.9 lnq
(1−t)2
t2+(1−t)2
9.95, (30)
which corresponds to tVPin Table 2. Also, we haves′(t′)
s(t)=α(t′)
1−t, which is sVPin Table 2.
D New Update Rule
In the standard Euler solver, the update rule is zt−∆t:=zt−v(zt, t)∆t. Alternatively, as xθ(zt, t)
of our model generates pretty good samples, we can instead use the linear interpolation between
xθ(zt, t)andz1to get the next step: zt−∆t:= (1−(t−∆t))xθ(zt, t) + (t−∆t)z1. Note that
when NFE <3, the two update rules are equivalent and do not affect our results in Section 5.1. Fig. 4
shows that when applied to existing solvers, the new update rule improves the sampling efficiency up
to4×, achieving the best FID with ≤5NFEs.
Algorithm 2 shows the pseudocode for generating samples using the new update rule. Unlike the
standard Euler update rule which only depends on the current state zt, our new update rule utilizes
the previous state (i.e., z1) to generate the next state zt−∆tand thus can be viewed as a form of
history-dependent samplers. Obviously, incorporating the initial state only would not be the best
choice. We believe that the result can be further improved, especially by using learning-based
solvers [Watson et al., 2021, Shaul et al., 2023]; and leave such exploration to future work.
E Experimental Details
Before training 2-rectified flow, we generate data-noise pairs following the sampling regime of
EDM [Karras et al., 2022]. For CIFAR-10, we generate 1M pairs using 35 NFEs. For AFHQ, FFHQ,
and ImageNet, we generate 5M pairs using 79 NFEs. We use Heun’s second-order solver for all cases.
In Table 3, we report the result of config G in Table 1. In ImageNet, we use the batch size of 2048
and train the models for 700,000 iterations using mixed-precision training [Micikevicius et al., 2017]
with the dynamic loss scaling. We use config E in Table 1 for ImageNet.
We provide training configurations in Table 7. For all datasets, we use Adam optimizer. We use the
exponential moving average (EMA) with 0.9999 decay rate for all datasets.
On ImageNet, the training takes roughly 9 days with 64 NVIDIA V100 GPUs. On CIFAR-10 and
FFHQ/AFHQ, it takes roughly 4 days with 16 and 8 V100 GPUs, respectively. For all cases, we use
17Algorithm 2 Generate
def generate(z1, label, model, time_schedule, N, solver, sampler, device):
"""
z1: initial noise
label: class label
model: v_theta
time_schedule: time schedule, e.g., [0.99999, 0.5, 0] for 2 steps
N: NFE
solver: ’euler’ or ’heun’
sampler: ’default’ or ’new’
"""
z = z1.clone()
cnt = 0
for i in range(len(time_schedule[:-1])):
t = torch.ones((z.shape[0]), device=device) * time_schedule[i]
t_next = torch.ones((z.shape[0]), device=device) * time_schedule[i+1]
dt = t_next[0] - t[0]
vt = model(z, t, label)
x0hat = z - vt * t.view(-1,1,1,1)
if solver == ’heun’ and cnt < N - 1: # Heun correction
if sampler == ’default’:
z_next = z.detach().clone() + vt * dt
elif sampler == ’new’:
z_next = (1 - t_next.view(-1,1,1,1)) * x0hat + t_next.view(-1,1,1,1) * z1
vt_next = model(z_next, t_next, label)
vt = (vt + vt_next) / 2
x0hat = z - vt * t.view(-1,1,1,1)
if sampler == ’default’:
z = z.detach().clone() + vt * dt
elif sampler == ’new’:
z = (1 - t_next.view(-1,1,1,1)) * x0hat + t_next.view(-1,1,1,1) * z1
cnt += 1
return z
Table 7: Training configurations for each dataset. We linearly ramp up learning rates for all datasets.
Datasets Batch size Dropout Learning rate Warm up iter.
CIFAR-10 512 0.13 2e-4 5000
FFHQ / AFHQ 256 0.25 2e-4 5000
ImageNet 2048 0.10 1e-4 2500
the NVIDIA DGX-2 cluster. To prevent zero-division error with EDM initialization, we sample t
from [0.00001 ,0.99999] in practice. For a two-step generation, we evaluate vθatt= 0.99999 and
t= 0.8. For other NFEs, we uniformly divide the interval [0.00001 ,0.99999] .
License The following are licenses for each dataset we use:
• CIFAR-10: Unknown
• FFHQ: CC BY-NC-SA 4.0
• AFHQ: CC BY-NC 4.0
• ImageNet: Custom (research, non-commercial)
F Broader Impacts
This paper proposes an advanced algorithm to generate realistic data at high speed, which could
have both positive and negative impacts. For example, it could be used for generating malicious or
misleading content. Therefore, such technology should be deployed and used responsibly and with
caution. We believe that our work is not expected to have any more potential negative impact than
other work in the field of generative modeling.
18G Uncurated Synthetic Samples
We provide uncurated synthetic samples from our 2-rectified flow++ on CIFAR-10, AFHQ, and
ImageNet in Figures 8, 9, 10, 11, 20, 21, 16, 17, 18, 19, 12, 13, 14, and 15. We use our new sampler
(Sec. 5.3) to generate these images.
19Figure 8: Synthetic samples from 2-rectified flow++ on CIFAR-10 with NFE = 1 (FID=3.38).
Figure 9: Synthetic samples from 2-rectified flow++ on CIFAR-10 with NFE = 2 (FID=2.76).
20Figure 10: Synthetic samples from 2-rectified flow++ on CIFAR-10 with NFE = 4 (FID=2.50).
Figure 11: Synthetic samples from 2-rectified flow++ on CIFAR-10 with NFE = 5 (FID=2.45).
21Figure 12: Synthetic samples from 2-rectified flow++ on ImageNet 64 ×64 with NFE = 1 (FID=4.31).
Figure 13: Synthetic samples from 2-rectified flow++ on ImageNet 64 ×64 with NFE = 2 (FID=3.64).
22Figure 14: Synthetic samples from 2-rectified flow++ on ImageNet 64 ×64 with NFE = 4 (FID=3.44).
Figure 15: Synthetic samples from 2-rectified flow++ on ImageNet 64 ×64 with NFE = 8 (FID=3.32).
23Figure 16: Synthetic samples from 2-rectified flow++ on AFHQ 64 ×64 with NFE = 1 (FID=4.11).
Figure 17: Synthetic samples from 2-rectified flow++ on AFHQ 64 ×64 with NFE = 2 (FID=3.12).
24Figure 18: Synthetic samples from 2-rectified flow++ on AFHQ 64 ×64 with NFE = 4 (FID=2.90).
Figure 19: Synthetic samples from 2-rectified flow++ on AFHQ 64 ×64 with NFE = 5 (FID=2.86).
25Figure 20: Synthetic samples from 2-rectified flow++ on FFHQ 64 ×64 with NFE = 1 (FID=5.21).
Figure 21: Synthetic samples from 2-rectified flow++ on FFHQ 64 ×64 with NFE = 2 (FID=4.26).
26NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction reflect the paper’s contributions and scope such
as improved empirical performance, which is backed up by our experiments in Sec. 5.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations are discussed in the conclusion, such as the increased training
time of our method.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Our only theoretical result is Proposition 1, for which we provide the proof in
Appendix C.1. The argument in Sec. 3 is intuitive, and we do not prove it theoretically.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide our experimental details in Sec. 5 and App. E. For the new update
algorithm, we provide full pseudocode in Sec. D, and we will release code publicly as soon
as we obtain approval to do so.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: The datasets we used are publicly available. We will release the code publicly
as soon as we obtain internal approval.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide these details in Sec. E.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The shaded area in Figure 3 indicates the standard deviation. We only compute
FID once due to cost constraints.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
27Answer: [Yes]
Justification: We provide details in Sec. E.
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The paper conforms with the NeurIPS Code of Ethics.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We provide an impact statement in Sec. F. Effectively, as with other work on
generative models, they can be used for both beneficial and harmful purposes. Our work,
being focused on the mechanics of these models, does not introduce new risks, nor does it
mitigate existing ones.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We believe that our work is not expected to have any more potential risks than
other work in this field. We have discussed some of these considerations in Sec. F.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: For the datasets we use, we cite the original papers and describe the license
information in Sec. E.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide necessary information to reproduce our new models in Sec. E.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing or research with human subjects.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
28