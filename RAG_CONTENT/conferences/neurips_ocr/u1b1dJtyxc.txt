What Are Large Language Models Mapping to in the
Brain? A Case Against Over-Reliance on Brain Scores
Anonymous Author(s)
Affiliation
Address
email
Abstract
Given the remarkable capabilities of large language models (LLMs), there has 1
been a growing interest in evaluating their similarity to the human brain. One 2
approach towards quantifying this similarity is by measuring how well a model 3
predicts neural signals, also called "brain score". Internal representations from 4
LLMs achieve state-of-the-art brain scores, leading to speculation that they share 5
computational principles with human language processing. This inference is only 6
valid if the subset of neural activity predicted by LLMs reflects core elements 7
of language processing. Here, we question this assumption by analyzing three 8
neural datasets used in an impactful study on LLM-to-brain mappings, with a 9
particular focus on an fMRI dataset where participants read short passages. We 10
first find that when using shuffled train-test splits, as done in previous studies 11
with these datasets, a trivial feature that encodes temporal autocorrelation not only 12
outperforms LLMs but also accounts for the majority of neural variance that LLMs 13
explain. We therefore caution against shuffled train-test splits, and use contiguous 14
test splits moving forward. Second, we explain the surprising result that untrained 15
LLMs have higher-than-expected brain scores by showing they do not account 16
for additional neural variance beyond two simple features: sentence length and 17
sentence position. This undermines evidence used to claim that the transformer 18
architecture biases computations to be more brain-like. Third, we find that brain 19
scores of trained LLMs on this dataset can largely be explained by sentence 20
position, sentence length, and static word vectors; a small, additional amount is 21
explained by sense-specific word embeddings and contextual representations of 22
sentence structure. We conclude that over-reliance on brain scores can lead to 23
over-interpretations of similarity between LLMs and brains, and emphasize the 24
importance of deconstructing what LLMs are mapping to in neural signals. 25
1 Introduction 26
Recent developments in large language models (LLMs) have led many to wonder whether LLMs 27
process language like humans do. Whereas LLMs acquire many abstract linguistic generalizations, it 28
remains unclear to what extent their internal machinery bears resemblance to the human brain [ 1]. A 29
number of studies have attempted to answer this question through the framework of neural encoding 30
[2–4]. Within this framework, an LLM’s internal representations of some linguistic stimuli are used 31
to predict brain activity during comprehension of the same stimuli. Results have been uniformly 32
positive, showing that LLM representations are highly effective at predicting neural signals [5, 6]. 33
In one impactful study, authors evaluated the brain scores of 43 models on three neural datasets [ 2]. 34
They found that GPT2-XL [ 7] achieved the highest brain score and, in one neural dataset, accounted 35
for 100% of the "explainable" neural variance (i.e., taking into account the noise inherent in the data) 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.[8]. This result was interpreted as evidence that the brain may be optimizing for the same objective 37
as GPT2, namely, next-word prediction. Surprisingly, the authors further found that untrained (i.e. 38
randomly initialized) LLMs predict neural activity well, leading to speculations that the transformer 39
architecture biases computations to be more brain-like. The finding that untrained LLMs predict 40
neural signals significantly above chance has been replicated in other studies [9, 4, 10]. 41
More generally, many studies have compared models to brain activity and concluded that high 42
prediction performance reveals correspondence between some interesting aspect of the model and 43
biological linguistic processing [ 4,11–14]. One issue with this approach is that it assumes that the 44
subset of neural activity predicted by a model reflects core processes of the human language system 45
[15]. However, this assumption is not necessarily true. For example, a recent paper found that, when 46
participants listen to stories, the fMRI signal includes an initial ramping, positional artifact [ 16]. 47
It is likely that LLMs which contain absolute positional embeddings would be able to predict this 48
ramping signal, whereas a simpler model such as a static word embedding (e.g. GloVe, [ 17]) would 49
not, leading to exaggerated differences between LLMs and GloVe due to reasons of little theoretical 50
interest. This issue relates to a more general trend in machine learning research: a complex algorithm 51
solves a task, but it is later discovered that the key innovation was a very simple component of the 52
algorithm [ 18]. Analogous to Weinberger [18], without attempting to rigorously deconstruct the 53
mapping between LLMs and brains, it is possible to draw erroneous conclusions about the brain’s 54
mechanisms for processing language. 55
We analyze the same three neural datasets used in [ 2]. These include the Pereira fMRI dataset, where 56
participants read short passages [ 8]; the Fedorenko electrocorticography (ECoG) dataset, where 57
participants read isolated sentences [ 19]; and the Blank fMRI dataset, where participants listened to 58
short stories [ 20]. As in Schrimpf et al. [2], we focus our analyses on the Pereira dataset. In order to 59
deconstruct the mapping between LLMs and the brain, we follow Reddy and Wehbe [21] and de Heer 60
et al. [22] by building a set of predictors that describe simple features of the linguistic input, and 61
gradually add features that increase in complexity. Our goal is to find the simplest set of features 62
which account for the greatest portion of the mapping between LLMs and brains. 63
2 Methods 64
2.1 Experimental data 65
For all three neural datasets, we used the same version as used by [ 2]. For additional details, refer to 66
A.1. 67
Pereira (fMRI): The Pereira dataset is composed of two experiments. Experiment 1 (EXP1) consists 68
of96passages each containing 4sentences, with n= 9participants. Experiment 2 (EXP2) consists of 69
72passages each consisting of 3or4sentences, with n= 6participants. Passages in each experiment 70
were evenly divided into 24semantic categories which were not related across experiments ( 4 71
passages per category in EXP1, and 3passages per category in EXP2). A single fMRI scan (TR) 72
was taken after visual presentation of each sentence. Unless otherwise noted, we focus our results 73
on voxels from within the "language network" in the main paper. EXP1 was a 384×92450 matrix 74
(number of sentences ×number of voxels) and EXP2 was a 243×60100 matrix. All analyses were 75
conducted separately for each experiment. 76
Fedorenko (ECoG): Participants ( n= 5) read 52sentences of length 8words. A total of 97 77
language-responsive electrodes were used across 5participants: 47,8,9,15,and18, for participants 78
1through 5, respectively. Neural activity was temporally averaged across the full presentation of each 79
word after extracting high gamma, and the entire dataset was a 416×97matrix. 80
Blank (fMRI): The dataset consisted of 5participants listening to 8stories from the publicly 81
available Natural Stories Corpus [ 23]. An fMRI scan was taken every 2seconds, resulting in a total 82
of1317 TRs across the 8stories. fMRI BOLD signals were averaged across voxels within each 83
functional region of interest (fROI). There were 60 fROIs across all 5 participants, resulting in a 84
1317×60matrix. 85
22.2 Language models 86
We focus our analyses on GPT2-XL [ 7], as it was shown to be the best-performing model on the 87
Pereira dataset [ 10,24,2]. GPT2 is an auto-regressive transformer model, meaning that it can 88
only attend to current and past inputs, trained on next token prediction. The XL variant has ∼1.5B 89
parameters and 48 layers. We replicate some of our key findings on Pereira with RoBERTa-Large[ 25] 90
(A.6). RoBERTa is a transformer model with bidirectional attention trained on masked token 91
prediction, meaning that it can attend to past and future tokens. The large variant contains 335M 92
parameters and 24layers. Both GPT2 and RoBERTa use learned absolute positional embeddings, 93
such that a unique vector corresponding to each token position is added to the input static embeddings. 94
2.3 LLM feature pooling 95
Pereira: Each sentence was fed into an LLM, with previous sentences from the same passage also fed 96
as input. Since each fMRI scan was taken at the end of the sentence, we converted LLM token-level 97
embeddings to sentence-level embeddings by summing across all tokens within a sentence (sum 98
pooling). We used the sum pooling method because it is consistent with other neural encoding studies 99
[26,27], and it performed better than taking the representation at the last token which was done in 100
[2] A.5. 101
Fedorenko: The current and previous tokens from within the same sentence were fed into the LLM 102
as context. We converted LLM token-level embeddings to word embeddings, since each word has a 103
neural response, by summing across tokens in multi-token words, and leaving single token words 104
unmodified. 105
Blank: For each story, we fed the current and all preceding tokens up to a maximum context size of 106
512 tokens. As in Schrimpf et al. [2], for each TR, we took the representation of the word that was 107
closest to being 4 seconds before the TR. For multi-token words, we took the representation of the 108
last token of that word. 109
2.4 Banded ridge regression 110
We used ridge regression (linear regression with an L2 penalty) to predict activations for each 111
voxel/electrode/fROI independently. We did not use "vanilla" ridge regression because it applies a 112
single L2 penalty for all weights, whereas our analyses use multiple sets of distinct features. In such 113
a case, a single penalty causes the regression will be biased against small feature spaces. Moreover, 114
different L2 penalties are likely optimal for each feature space. To remedy this, we employed banded 115
ridge regression which effectively allows a different L2 penalty to be applied to each feature space 116
[28] (for further details, refer to A.2). 117
2.5 Out of sample R2metric 118
We define the brain score of a model as the out-of-sample R2metric ( R2
oos) [29].R2
oosquantifies 119
how much better a set of features performs at predicting held-out data compared to a model which 120
simply predicts the mean of the training data (i.e. a regression with only an intercept term). To be 121
precise, given mean squared error (MSE) values from a model using features Mand MSE values 122
from an intercept only regression ( I), then: 123
R2
oos= 1−MSE M
MSE I. (1)
A positive (negative) value indicates that Mwas more (less) helpful than predicting the mean of 124
training data. We elected to use R2
oosover the standard R2because of this clear interpretation 125
and because it is a less biased estimate of test set performance [ 29]. We use R2
oosover Pearson’s 126
correlation coefficient ( r) because R2
ooscan be interpreted as the fraction of variance explained, 127
which lends more straightforwardly to estimating how much variance one feature space explains 128
over others. Whenever averaging across voxels, we set R2
oosvalues to be non-negative to prevent 129
differences in performance on noisy voxels/electrodes/fROIs from significantly impacting the results. 130
We refer to R2
oosasR2throughout the rest of the paper for brevity, and use the notation R2
Mto refer 131
to the performance of features M. 132
32.6 Selection of best layer 133
We evaluate the R2for each LLM layer, and select the layer that performs best across vox- 134
els/electrodes/fROIs. Due to the stochastic nature of untrained LLMs, we selected the best layer for 135
10random seeds and computed the average R2across seeds. When reporting the best layer, we refer 136
to layer 0as the input static layer, and layer 1as the first intermediate layer. 137
2.7 Train, validation, and test folds: 138
For each dataset, we construct contiguous train-test splits by ensuring neural data from the same 139
passage/sentence/story is not included in both train and test data. Due to low sample sizes, we 140
employed a nested cross-validation procedure for each dataset (A.3). When computing R2across 141
inner or outer folds, we pooled predictions across folds and computed a single R2as recommended by 142
Hawinkel et al. [29]. The optimal parameters for banded regression were selected based on validation 143
data. 144
We created shuffled train-test splits, as done in [ 2], of the same size as the contiguous train-test splits. 145
Unless explicitly noted, all results are performed using contiguous train-test splits. 146
2.8 Correcting for decreases in test-set performance due to addition of feature spaces 147
It is possible for a "full" encoding model to perform worse than a "sub-model" (which consists 148
of only a subset of the predictors) because we are evaluating performance on a held-out test set 149
[22]. To address this problem, in some analyses we select the best performing sub-model for each 150
voxel/electrode/fROI which includes a given feature of interest. For instance, to examine how much 151
feature space Cadds onto features spaces AandB, we select the best sub-model which includes C 152
and denote it as A+B+C*. More precisely, the R2ofA+B+C* is: 153
R2
A+B+C∗= max( R2
C, R2
A+C, R2
B+C, R2
A+B+C). (2)
2.9 Orthogonal Auto-correlated Sequences Model (OASM) 154
To model temporal auto-correlation in neural activity, we construct a feature matrix for each dataset 155
by (i) forming an n-dimensional identity matrix, where nis the total number of time points in the 156
dataset (per voxel / electrode / TR), and (ii) applying a Gaussian filter within "chunks" along the 157
diagonal that correspond to temporally contiguous time points (i.e., within each passage in Pereira, 158
each sentence in Fedorenko, and each story in Blank). This generates an auto-correlated sequence for 159
each passage/sentence/story that is orthogonal to that of each other passage/sentence/story (A.7). 160
3 Pereira dataset 161
3.1 Shuffled train-test splits are severely affected by temporal auto-correlation 162
Prior LLM encoding studies using this dataset [ 24,2,10,30,11] used shuffled train-test splits. Here, 163
we demonstrate that this approach compromises the evaluation of the neural predictivity of LLMs. 164
First, we replicated the pattern of neural predictivity across GPT2-XL’s layers reported in [ 2] and [ 24] 165
when using shuffled splits. Using this procedure, early and late layers perform best and intermediate 166
layers perform worst. Strikingly, when using the alternative approach of contiguous train-test splits, 167
the opposite pattern is observed: intermediate layers perform best. Across layers, neural predictivity 168
using the shuffled method is highly anti-correlated with neural predictivity using the contiguous 169
method ( r=−.929in EXP1, r=−.764in EXP2) (Fig. 1a). 170
Next, we hypothesized that much of what LLMs might be mapping to when using shuffled splits 171
could be accounted for by OASM, a model which only represents within passage auto-correlation 172
and between passage orthogonality. OASM out-performed GPT2-XL on both EXP1 and EXP2 173
(Fig. 1b, blue and red bars), revealing that a completely non-linguistic feature space can achieve 174
absurdly high brain scores in the context of shuffled splits. This strongly challenges the assumption 175
of multiple previous studies [ 2,11,10] that performance on this benchmark is an indication of a 176
model’s brain-likeness, . 177
4a b*Figure 1: Comparing different approaches for creating train-test splits in the Pereira dataset. Within
each panel, EXP1 results are on the left and EXP2 results are on the right (same formatting in Figure
2,3) (a)R2values across layers for GPT2-XL on shuffled train-test splits (gray) and contiguous
(unshuffled) splits (blue). (b)Each dot shows the mean R2value across voxels within a participant,
with bars indicating mean R2across participants.
Moreover, we find that the unique neural variance that GPT2-XL explains over OASM is very small 178
relative to what OASM explains alone. To calculate this, we combine OASM with GPT2-XL and 179
observe how much neural variance they explain together. To prevent OASM from ever weakening 180
the reported performance of GPT2-XL for any voxel, we correct the R2value for each voxel with 181
the OASM+GPT2-XL model to be at least as high as with GPT2-XL alone (denoted OASM+GPT2- 182
XL*) (2.8). Even with these corrections, we find that R2
OASM +GPT 2−XL* was 13.6% higher than 183
R2
OASM in EXP1, and 31.5% higher than R2
OASM in EXP2 (Fig. 1b) (% differences after averaging 184
R2across participants). To be clear, this means that any linguistically-driven neural variance that 185
GPT2-XL uniquely explains over OASM is far smaller ( 13.6%on EXP1 and 31.5%on EXP2) than 186
what is predicted solely by OASM, a model with no linguistic features that completely lacks the 187
ability to generalize to fully held out passages. Thus, it appears that the largest determinant of 188
model predictivity on this dataset when using shuffled train-test splits is whether a model contains 189
autocorrelated sequences within passages that are orthogonal between passages. 190
3.2 Untrained LLM neural predictivity is fully accounted for by sentence length and position 191
We next sought to deconstruct what explains the neural predictivity of untrained GPT2-XL (GPT2- 192
XLU) in the Pereira dataset. We hypothesized that R2
GPT 2−XLU could be explained by two simple 193
features: sentence length (SL) and sentence position within the passage (SP). Sentence length is 194
captured by GPT2-XLU because the GELU nonlinearity in the first layer’s MLP transforms normally 195
distributed inputs with zero mean into outputs with a non-zero mean. This introduces a non-zero 196
mean component to each token’s representation in the residual stream. When these representations 197
are sum-pooled, this non-zero mean component accumulates in a way that reflects the sentence length, 198
making the length decodable in the intermediate layers (see A.9 for a formal proof). Sentence position 199
is encoded within GPT2-XLU due to absolute positional embeddings which, although untrained, still 200
result in sentences at the same position having similar representations when tokens are sum-pooled. 201
We represent sentence position as a 4-dimensional one-hot vector, where each element corresponds 202
to a given position within a passage, and sentence length as the number of words in a passage. 203
To obtain representations from GPT2-XLU, we selected the best-performing layer for each of the 10 204
untrained seeds. For EXP1 the best performing layer was layer 0for6seeds, layer 1 for 3seeds (first 205
intermediate layer), and layer 19for one seed. For EXP2 the best layer was layer 1for 5 seeds, layer 206
2for 4 seeds, and layer 5for1seed. 207
We fit a regression using all subsets of the following feature spaces, SL, SP, GPT2-XLU, resulting in 208
7 models. For both experiments, R2
SP+SLwas descriptively higher than all other models, including 209
the best-performing model with GPT2-XLU (SP+SL+GPT2-XLU) (Fig. 2a). Sentence position was 210
particularly important in EXP1, and sentence length was particularly important in EXP2. This may 211
explain why the static layer often outperformed intermediate layer representations in EXP1 despite 212
encoding sentence length more poorly. Overall, these results suggest that, when averaging across 213
voxels within the language network in this dataset, GPT2-XLU does not improve neural encoding 214
performance over sentence length and position. 215
5102
101
100
102
100101
0.2
0L R
0.3
0
a
b c
dSP+SL*
SP+SL+GPT2-XLU*
SP+SL* SP+SL+GPT2-XLU*Figure 2: For all panels, EXP1 results are on the left and EXP2 results are on the right. (a)Brain
score ( R2) for different combinations of features. Each dot represents R2values averaged across
voxels in a single participant, with bars showing mean across participants. (b)2D histogram of
R2values for the best model without GPT2-XLU (SP+SL), and the best model with GPT2-XLU
(GPT2-XLU+SP+SL). The dotted lines show y=x,y= 0, and x= 0. Values below y= 0 or
left of x= 0were clipped when averaging, but are shown here to visualize the full distribution. (c)
Same as (a), but after voxel-wise correction; lines connect data-points from the same participant. (d)
Glass brain plots showing R2values of SP+SL (left) and GPT2-XLU+SP+SL (right) after voxel-wise
correction. Conventions are the same as Figure 1.
Although GPT2-XLU did not enhance encoding performance when averaging across voxels, there 216
may be a subset of voxels where GPT2-XLU does explain significant additional neural vari- 217
ance. To examine this possibility, we plotted a 2D histogram of voxel-wise R2
SP+SLvalues vs. 218
R2
SP+SL+GPT 2−XLU values in the language network (Fig. 2b). Values were clustered around the 219
identity line, and there was no cluster of voxels where R2
SP+SL+GPT 2−XLU appeared significantly 220
higher. Next, for each voxel, we performed a one-sided paired t-test between the squared error 221
values obtained over sentences (EXP1: N= 384 , EXP2: N= 243 ) between SP+SL+GPT-XLU 222
and SP+SL. Across all functional networks, only 1.26% (EXP1) and 1.42% (EXP2) of voxels were 223
significantly ( α= 0.05) better explained by the GPT2-XLU model before false discovery rate 224
(FDR) correction; these numbers dropped to 0.001% (EXP1) and 0.078% (EXP2) after performing 225
FDR correction within each participant and network [ 31]. None of the significant voxels after FDR 226
correction were inside the language network. Taken together, these results suggest GPT2-XLU does 227
not enhance neural prediction performance over sentence length and position even at the voxel level. 228
To control for voxels where the neural encoding performance of GPT2-XLU is weakened by the 229
addition of SP+SL, we compared SP+SL* and SP+SL+GPT2-XLU*. When averaging across voxels, 230
R2
SP+SL* still exceeded R2
GPT 2−XLU +SP+SL* (Fig. 2c). Furthermore, the values for R2
SP+SL* 231
andR2
GPT 2−XLU +SP+SL* across brain areas were highly similar in both experiments (Fig. 2d). 232
Only 1.00% (EXP1) and 1.18% (EXP2) of voxels were significantly better explained by the addition 233
of GPT2-XLU before FDR correction; 0% (EXP1) and 0.05% (EXP2) of voxels were better explained 234
6Table 1: Mean R2values (across participants) for each model. For models composed of multiple
features, the best sub-model is used which includes the last feature.
Features EXP1 EXP2
GPT2-XL 0.032 0 .036
SP+SL 0.013 0 .031
SP+SL+WORD 0.024 0 .039
SP+SL+WORD+SENSE 0.026 0 .040
SP+SL+WORD+SENSE+SYNT 0.027 0 .043
SP+SL+WORD+SENSE+SYNT+GPT2-XL 0.032 0 .045
after FDR correction (once again, no significant voxels were inside the language network ). Thus, our 235
results hold even when controlling for decreases in performance due to the addition of feature spaces. 236
3.3 Sentence length, sentence position, and static word embeddings account for the majority 237
of trained LLM encoding performance 238
We next turned to explaining the neural predictivity of the trained GPT2-XL. In addition to sentence 239
position and sentence length, we added static word embeddings (WORD). Together, these features 240
defined a baseline model which does not account for any form of linguistic processing of words 241
in context. We next included three more complex features which involved contextual processing. 242
First, we added sense-specific word embeddings from RoBERTa-Large using the LMMS package 243
[32]. Sense embeddings contain distinct representations for different senses of the same word (e.g., 244
mouse: computer device , and mouse: rodent ). LMMS generates sense embeddings by averaging over 245
contextual embeddings corresponding to the same sense of a word (see A.10 for further details). 246
Whereas sense embeddings help disambiguate many content words, they do not disambiguate 247
pronouns, i.e., do not encode the entities that they refer to. Therefore, our sense embeddings were 248
generated for a version of the Pereira text where pronouns were dereferenced (i.e., replaced by 249
the words that they referred to). To maintain consistency with these sense embeddings, our static 250
word embeddings were created (1) by taking a frequency-weighted average of sense embeddings 251
for the same word, where frequency values were obtained from WordNet [ 33]; and (2) based on the 252
dereferenced Pereira texts. Importantly, this means the impact of pronoun dereferencing and word 253
and sense embeddings are not decoupled in this study. Finally, we created an abstract representation 254
of the syntax of each sentence (SYNT), using an approach highly similar to that of Caucheteux 255
et al. [34]: we collected sentences that are syntactically equivalent but semantically dissimilar to the 256
original sentence, and averaged their representations from the best layer of GPT2-XL (A.11). We 257
selected the best layer based on averaged R2across language voxels on test data (EXP1: layer 21, 258
EXP2: layer 16). 259
We fit a regression to the fMRI data using all subsets of the feature spaces SL+SP, WORD, SENSE, 260
SYNT, GPT2-XL, resulting in 64 models. In this list, features are ranked from least to most complex. 261
For each feature, we took the model that exhibited the best performance in the language network 262
which included that feature but did not include features more complex than it. For instance, values 263
reported for R2
SL+SP+WORD +SENSE were taken from the best model which included SENSE, 264
excluding models which included SYNT and GPT2-XL. By doing so, we were able to examine 265
the impact of adding more complex features in explaining R2 GPT 2−XLwhile still accounting for 266
decreases in test performance due to adding redundant features. We note that since this procedure is 267
not performed at the voxel-level, we do not add a * to the R2notation. 268
Table 1 displays the performance of each model, including GPT2-XL on its own (Fig. 2a, 2b). The 269
baseline SP+SL+WORD model, which does not account for any form of contextual processing, 270
performs 75% as well as GPT2-XL in EXP1, and outperforms GPT2-XL in EXP2. When adding 271
contextual features, namely SENSE and SYNT, our model performs 84.4% as well as GPT2-XL and 272
the full model in EXP1, and better than GPT2-XL and 95.5% as well as the full model in EXP2, 273
indicating that SENSE and SYNT play a modest role in accounting for GPT2-XL brain scores beyond 274
simple features in this dataset. 275
Similar to previous sections, we perform voxel-wise correction by selecting the best sub-model with 276
GPT2-XL and the best sub-model without GPT2-XL for each voxel. We focus only on sentence 277
7102
101
100
102
101
100
00.4
 0.35
0L R
*
*
* *a
bc
dFigure 3: For all panels, EXP1 results are on the left and EXP2 results are on the right. (a)For each
model, we display the sub-model which includes the added feature. Dots represent participants and
bars are mean across participants. Grey dashed line is the performance of GPT2-XL alone. (b)2d
histogram comparing full model and full model with GPT2-XL. (c)Same as (a)but after voxel-wise
correction for SP+SL+WORD and SP+SL+WORD+GPT2-XL. (d)Glass brain plots showing R2
values of SP+SL+WORD (left) and SP+SL+WORD+GPT2-XLU (right) after voxel-wise correction.
position, sentence length, and static word embeddings because sense and syntax had modest con- 278
tributions beyond these features. R2
SP+SL+WORD * was 0.028in EXP1 and 0.048in EXP2, and 279
R2
SP+SL+WORD +GPT 2−XL* was 0.036in EXP1 and 0.056in EXP2 (mean across participants) 280
(Fig. 3c). This indicates that even after controlling for a reduction in GPT2-XL performance from 281
the addition of simple features, GPT2-XL only explains an additional 28.57% (EXP1) and 16.7% 282
(EXP2) neural variance over a model composed of features that are all non-contextual. 283
4 Fedorenko dataset 284
4.1 Shuffled train-test splits also impact ECoG datasets, but less than with fMRI 285
We first evaluated the impact of shuffled train-test splits on the Fedorenko dataset. Unlike in Pereira, 286
the across-layer performance is well correlated between shuffled and contiguous splits ( r= 0.622) 287
(Fig. 4a). The OASM model performs 93.1% as well as GPT2-XL when averaging R2values across 288
participants (Fig. 4b). R2
OASM +GPT 2−XL* was 45.3% better than OASM, meaning that the unique 289
contribution of GPT2-XL is less than half the total contribution of a simple, auto-correlated model. 290
Therefore, shuffled train-test splits also impact results on Fedorenko, albeit less than Pereira. This 291
may be due to lower autocorrelation of ECoG compared to fMRI. We use contiguous splits for the 292
remainder of the Fedorenko analyses. 293
4.2 Word position explains all of untrained, and most of trained, GPT2-XL brain score 294
As noted in [ 35], there was a strong positional signal in the ECoG dataset during comprehension of 295
sentences that is likely related to the construction of sentence meaning. We therefore hypothesized 296
8a b c d
* * *Figure 4: (a)Across-layer R2, averaged across electrodes in the Fedorenko dataset, for GPT2-XL
with and without shuffled splits. (b)Each dot is a participant, lines connect data-points from the same
participant. Bars display mean across participants. (c)and(d)Same guidelines as (b).
that a feature space that accounted for word position (WP) would do well relative to untrained and 297
trained GPT2-XL. We generated a simple feature space that encodes word position, such that words 298
in nearby positions were given similar representations (A.12). When performing a one-sided paired 299
t-test between the squared error predictions of WP+GPT2-XLU* and WP, three electrodes were 300
significantly better explained by the addition of GPT2-XLU before FDR correction, and none were 301
better explained after FDR correction within each participant. Moreover, WP performs 86.7% as well 302
as GPT2-XL, and 82.1% as well as WP+GPT2-XL*. Our results therefore suggest that the mapping 303
between GPT2-XL and neural activity on the Fedorenko dataset is largely driven by positional signals. 304
305
5 Blank dataset is predicted at near chance levels 306
Lastly, we address the Blank dataset. We find that OASM achieves an R2that is 103.6times 307
larger than that of GPT2-XL when using shuffled splits A.13, demonstrating that such splits are 308
massively contaminated by temporal autocorrelation. We next turn to using contiguous splits, and test 309
whether GPT2-XL performs better than an intercept only model by applying a one-sided paired t-test 310
between the squared error values obtained from GPT2-XL and the intercept only model ( N= 1317 311
TRs). GPT2-XL predicts 1fROI significantly better than an intercept only model, and 0fROIs are 312
significantly better after FDR correction. Our results therefore suggest that GPT2-XL performs at 313
near chance levels on the version of the Blank dataset used by [2, 10, 11]. 314
6 Limitations and Conclusions 315
Our study has three main limitations. First, our method of examining how much neural variance 316
an LLM predicts over simple features scales poorly when the number of features is large. Second, 317
although we attempted to correct for cases where adding features decreases test set performance and 318
employed banded regression, fitting regressions with large feature spaces on noisy neural data with 319
low sample sizes can lead to poor estimations of the neural variance explained. Finally, we did not 320
analyze datasets with large amounts of neural data per participant, for instance [ 36], in which the gap 321
between the neural predictivity of simple and complex features might be much larger. 322
In summary, we find that on the Pereira dataset, shuffled splits are heavily impacted by temporal 323
autocorrelation, untrained GPT2-XL brain score is explained by sentence length and position, and 324
trained GPT2-XL brain score is largely explained by non-contextual features. We find that the 325
majority of GPT2-XL brain score on the Fedorenko dataset is accounted for by word position, and 326
on the Blank dataset GPT2-XL predicts neural activity at near chance levels. These results suggest 327
that (i) brain scores on these datasets should be interpreted with caution; and (ii) more generally, 328
analyses using brain scores should be accompanied by a systematic deconstruction of neural encoding 329
performance, and an evaluation against simple and theoretically uninteresting features. Only after 330
such deconstruction can we be somewhat confident that the neural predictivity of LLMs reflects core 331
aspects of human linguistic processing. 332
9References 333
[1]Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and 334
Evelina Fedorenko. Dissociating language and thought in large language models. Trends Cogn. 335
Sci., March 2024. 336
[2]Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy 337
Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language: 338
Integrative modeling converges on predictive processing. Proc. Natl. Acad. Sci. U. S. A. , 118 339
(45), November 2021. 340
[3]Mariya Toneva, Tom M Mitchell, and Leila Wehbe. Combining computational controls with 341
natural text reveals aspects of meaning composition. Nat Comput Sci , 2(11):745–757, November 342
2022. 343
[4]Charlotte Caucheteux and Jean-Rémi King. Brains and algorithms partially converge in natural 344
language processing. Commun Biol , 5(1):134, February 2022. 345
[5]Shailee Jain and Alexander Huth. Incorporating context into language encoding models for 346
fMRI. Adv. Neural Inf. Process. Syst. , 31, 2018. 347
[6]Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing (in 348
machines) with natural language-processing (in the brain). Adv. Neural Inf. Process. Syst. , pages 349
14928–14938, May 2019. 350
[7]Alec Radford, Jeff Wu, R Child, D Luan, Dario Amodei, and I Sutskever. Language models are 351
unsupervised multitask learners. 2019. 352
[8]Francisco Pereira, Bin Lou, Brianna Pritchett, Samuel Ritter, Samuel J Gershman, Nancy 353
Kanwisher, Matthew Botvinick, and Evelina Fedorenko. Toward a universal decoder of linguistic 354
meaning from brain activation. Nat. Commun. , 9(1):963, March 2018. 355
[9]Alexandre Pasquiou, Yair Lakretz, John Hale, Bertrand Thirion, and Christophe Pallier. Neural 356
language models are not born equal to fit brain data, but training helps. July 2022. 357
[10] Eghbal A Hosseini, Martin Schrimpf, Yian Zhang, Samuel Bowman, Noga Zaslavsky, and 358
Evelina Fedorenko. Artificial neural network language models predict human brain responses 359
to language even after a developmentally realistic amount of training. Neurobiol Lang (Camb) , 360
5(1):43–63, April 2024. 361
[11] Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, and Antoine Bosselut. 362
Instruction-tuned LLMs with world knowledge are more aligned to the human brain, 2024. 363
URL https://openreview.net/forum?id=DZ6B5u4vfe . 364
[12] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Rémi King. Evidence of a predictive 365
coding hierarchy in the human brain listening to speech. Nat Hum Behav , 7(3):430–441, March 366
2023. 367
[13] Ariel Goldstein, Eric Ham, Mariano Schain, Samuel Nastase, Zaid Zada, Avigail Dabush, 368
Bobbi Aubrey, Harshvardhan Gazula, Amir Feder, Werner K Doyle, Sasha Devore, Patricia 369
Dugan, Daniel Friedman, Roi Reichart, Michael Brenner, Avinatan Hassidim, Orrin Devinsky, 370
Adeen Flinker, Omer Levy, and Uri Hasson. The temporal structure of language processing in 371
the human brain corresponds to the layered hierarchy of deep language models, 2024. URL 372
https://openreview.net/forum?id=95ObXevgHx . 373
[14] Refael Tikochinski, Ariel Goldstein, Yoav Meiri, Uri Hasson, and Roi Reichart. Incremental ac- 374
cumulation of linguistic context in artificial and biological neural networks. bioRxiv , 2024. doi: 375
10.1101/2024.01.15.575798. URL https://www.biorxiv.org/content/early/2024/ 376
01/17/2024.01.15.575798 . 377
[15] Jeffrey S Bowers, Gaurav Malhotra, Federico Adolfi, Marin Dujmovi ´c, Milton L Montero, 378
Valerio Biscione, Guillermo Puebla, John H Hummel, and Rachel F Heaton. On the importance 379
of severely testing deep learning models of cognition. Cogn. Syst. Res. , 82:101158, December 380
2023. 381
10[16] Richard Antonello, Aditya R Vaidya, and Alexander G Huth. Scaling laws for language 382
encoding models in fMRI. Adv. Neural Inf. Process. Syst. , abs/2305.11863, May 2023. 383
[17] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for 384
word representation. In Proceedings of the 2014 conference on empirical methods in natural 385
language processing (EMNLP) , pages 1532–1543, 2014. 386
[18] Kilian Weinberger. On the importance of deconstruction in machine learning research. 387
ML-Retrospectives @ NeurIPS 2020, 2020. URL https://slideslive.com/38938218/ 388
the-importance-of-deconstruction . 389
[19] Evelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specificity for high-level 390
linguistic processing in the human brain. Proc. Natl. Acad. Sci. U. S. A. , 108(39):16428–16433, 391
September 2011. 392
[20] Idan Blank, Nancy Kanwisher, and Evelina Fedorenko. A functional dissociation between 393
language and multiple-demand systems revealed in patterns of BOLD signal fluctuations. J. 394
Neurophysiol. , 112(5):1105–1118, September 2014. 395
[21] Aniketh Janardhan Reddy and Leila Wehbe. Can fMRI reveal the representation of syntactic 396
structure in the brain? Adv. Neural Inf. Process. Syst. , 34:9843–9856, December 2021. 397
[22] Wendy A de Heer, Alexander G Huth, Thomas L Griffiths, Jack L Gallant, and Frédéric E 398
Theunissen. The hierarchical cortical organization of human speech processing. J. Neurosci. , 399
37(27):6539–6557, July 2017. 400
[23] Richard Futrell, Edward Gibson, Harry J Tily, Idan Blank, Anastasia Vishnevetsky, Steven 401
Piantadosi, and Evelina Fedorenko. The natural stories corpus. In Nicoletta Calzolari, Khalid 402
Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente 403
Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, 404
and Takenobu Tokunaga, editors, Proceedings of the Eleventh International Conference on 405
Language Resources and Evaluation (LREC 2018) , Miyazaki, Japan, May 2018. European 406
Language Resources Association (ELRA). 407
[24] Carina Kauf, Greta Tuckute, Roger Levy, Jacob Andreas, and Evelina Fedorenko. Lexical- 408
Semantic content, not syntactic structure, is the main contributor to ANN-Brain similarity of 409
fMRI responses in the language network. Neurobiol Lang (Camb) , 5(1):7–42, April 2024. 410
[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, 411
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT 412
pretraining approach. July 2019. 413
[26] Alexander G Huth, Wendy A de Heer, Thomas L Griffiths, Frédéric E Theunissen, and Jack L 414
Gallant. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature , 532 415
(7600):453–458, April 2016. 416
[27] Shailee Jain, Vy A V o, Shivangi Mahto, Amanda LeBel, Javier Turek, and Alexander G Huth. 417
Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech. 418
Adv. Neural Inf. Process. Syst. , 33, October 2020. 419
[28] Tom Dupré la Tour, Michael Eickenberg, Anwar O Nunez-Elizalde, and Jack L Gallant. Feature- 420
space selection with banded ridge regression. Neuroimage , 264:119728, December 2022. 421
[29] Stijn Hawinkel, Willem Waegeman, and Steven Maere. Out-of-Sample r2: Estimation and 422
inference. Am. Stat. , pages 1–11. 423
[30] Subba Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika Marreddy, Manish Gupta, and Bapi 424
Surampudi. Neural language taskonomy: Which NLP tasks are the most predictive of fMRI 425
brain activity? In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, 426
editors, Proceedings of the 2022 Conference of the North American Chapter of the Association 427
for Computational Linguistics: Human Language Technologies , pages 3220–3237, Seattle, 428
United States, July 2022. Association for Computational Linguistics. 429
11[31] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: A practical and 430
powerful approach to multiple testing. J. R. Stat. Soc. Series B Stat. Methodol. , 57(1):289–300, 431
1995. 432
[32] Daniel Loureiro, Alípio Mário Jorge, and Jose Camacho-Collados. LMMS reloaded: 433
Transformer-based sense embeddings for disambiguation and beyond. Artif. Intell. , 305:103661, 434
April 2022. 435
[33] George A Miller. WordNet: a lexical database for english. Commun. ACM , 38(11):39–41, 436
November 1995. 437
[34] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Remi King. Disentangling syntax and 438
semantics in the brain with deep networks. March 2021. 439
[35] Evelina Fedorenko, Terri L Scott, Peter Brunner, William G Coon, Brianna Pritchett, Gerwin 440
Schalk, and Nancy Kanwisher. Neural correlate of the construction of sentence meaning. Proc. 441
Natl. Acad. Sci. U. S. A. , 113(41):E6256–E6262, October 2016. 442
[36] Amanda LeBel, Lauren Wagner, Shailee Jain, Aneesh Adhikari-Desai, Bhavin Gupta, Allyson 443
Morgenthal, Jerry Tang, Lixiang Xu, and Alexander G Huth. A natural language fMRI dataset 444
for voxelwise encoding models. Sci Data , 10(1):555, August 2023. 445
[37] Zachary Mineroff, Idan Asher Blank, Kyle Mahowald, and Evelina Fedorenko. A robust 446
dissociation among the language, multiple demand, and default mode networks: Evidence from 447
inter-region correlations in effect size. Neuropsychologia , 119:501–511, October 2018. 448
[38] Jonathan D Power, Alexander L Cohen, Steven M Nelson, Gagan S Wig, Kelly Anne Barnes, 449
Jessica A Church, Alecia C V ogel, Timothy O Laumann, Fran M Miezin, Bradley L Schlaggar, 450
and Steven E Petersen. Functional network organization of the human brain. Neuron , 72(4): 451
665–678, November 2011. 452
[39] Thomas Lumley, Paula Diehr, Scott Emerson, and Lu Chen. The importance of the normality 453
assumption in large public health data sets. Annu. Rev. Public Health , 23:151–169, 2002. 454
[40] Simon Musall, Matthew T Kaufman, Ashley L Juavinett, Steven Gluf, and Anne K Churchland. 455
Single-trial neural dynamics are dominated by richly varied movements. Nat. Neurosci. , 22(10): 456
1677–1686, October 2019. 457
[41] Matthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom 458
embeddings, convolutional neural networks and incremental parsing. To appear, 2017. 459
A Appendix 460
A.1 Experimental data 461
Pereira: For both experiments, each sentence was visually presented for 4s with 4s between 462
sentences and an additional 4s between passages. A single fMRI scan was taken in the interval 463
between each sentence. Because fMRI data is noisy, each experiment was repeated three times and 464
fMRI data was averaged across the repetitions. A single fMRI scanning session consisted of 8runs, 465
where each run contained 12passages in EXP1 and 9passages in EXP2. Participants performed 466
a total of 3scanning sessions. The division of passages into runs and the order of the runs was 467
randomized for each participant and scanning session. 468
Fedorenko: Participants read sentence on word at a time, and each word was visually displayed for 469
450 or 700 ms. For each electrode, high gamma signal was extracted using gaussian filter banks at 470
center frequencies ranging from 73−144Hz, the envelope of the high gamma signal was computed 471
through a hilbert-transform, and the envelope was z-scored within each electrode. For each participant, 472
language-selective electrodes were selected where the z-scored envelope of the gamma activity was 473
significantly higher during the sentences than a condition where participants read nonword lists. 474
Z-scored high gamma activity from these language-selective electrodes were used in subsuquent 475
analyses. 476
12Blank: Text was split into 2s segments corresponding to each TR, with words that were on the 477
boundary being assinged to the later TR. Due to the delay in the hemodynamic response function 478
(HRF), neural activity was predicted using stimuli from 2TRs ( 4s) previous. 479
Functional localization: For Pereira and Blank, the language network was defined by the following 480
procedure [ 19]. First, voxels were identified in each participant which showed stronger responses 481
to sentences compared to lists of non-words (sentences > non-word lists contrast). These voxels 482
were then constrained by data-driven language activation maps formed by applying the same contrast 483
to many other participants. Finally, the top 10% of the voxels were selected which showed the 484
greatest sentences > non-word lists difference. For Pereira, we perform some analyses using four 485
other networks: multiple demand (MD), default mode network (DMN), auditory, and visual network. 486
The multiple demand (MD) and default mode network (DMN) networks were defined using the same 487
procedure, except that the contrast involved a spatial working memory task, where a hard > easy 488
condition contrast was used for MD and a fixation > hard contrast was used for DMN [ 37]. Auditory 489
and visual networks were defined using resting state connectivity [38]. 490
A.2 Banded ridge regression 491
We used a random search method to optimize the banded regression hyperparameters [ 28]. Banded 492
regression has two hyperparameters, γ, which is a vector of shape number of feature spaces that 493
determines how much each feature space is scaled, and α, which is the L2 penalty applied across 494
feature spaces. Values for γare drawn from a Dirichlet distribution and hence sum to 1. Down-scaling 495
a certain feature space relative to others is functionally equivalent to assigning a separate L2 penalty 496
for each feature space. This is because when a feature space is down-scaled, the L2 magnitude of 497
the weights must increase for it to have a meaningful contribution to the predictions, which equates 498
to increasing the L2 penalty for that feature space. The optimal γandαcombination was found 499
for each voxel/electrode/fROI by performing a random search over γvalues, storing the αvalue 500
that performed best for that γon validation data, and then selecting the best performing γandα 501
combination. 502
Before starting the random search, we tried all combinations of γvalues that removed feature spaces 503
(i.e. down-scaled at least one feature space to 0) to ensure the regression had an opportunity to 504
remove features which hurt performance. In theory, this should obviate the need for the procedure 505
implemented in 2.8. This is because the banded regression procedure can remove feature spaces 506
based on validation data, meaning if a model performs worse than a sub-model the banded procedure 507
has the opportunity to set the γvalue corresponding to the additional feature spaces to 0. However, 508
because neural data is noisy and there is often little data per subject, performance on validation data is 509
not always indicative of performance on test-data. Therefore it is possible for the banded regression 510
procedure to include a feature space (since it helps on validation data), and for this feature space to 511
ultimately hurt test set performance, necessitating the correction procedure detailed in 2.8. 512
We ran banded ridge regression for a maximum of 1000 random search iterations with early stopping 513
if the mean R2did not improve by more than 10−4after 50iterations. We treated feature spaces 514
with many dimensions as one features because preliminary results showed this performed better. 515
Specifically, we always treated the following feature spaces as one feature space: static word 516
embeddings, sense-specific word embeddings, syntactic representations, and GPT2-XL and Roberta- 517
Large representations. All other features were treated as their own feature space. 518
We z-score all features across samples before training regressions, as is standard when using ridge 519
regression in neural encoding studies. 520
A.3 Additional details on train, validation, and test folds 521
Pereira: During each outer fold, a single passage from each of the 24 semantic categories from one 522
experiment was selected, and half of these passages were designated as the test set. This equated to 8 523
test folds for experiment 1 (4 passages per semantic category) and 6 test folds for experiment 2 (3 524
passages per semantic category). During each inner fold, we again selected one passage from each 525
semantic category, and half of these passages were designated as validation (leading to 7 inner folds 526
for experiment 1, and 5 inner folds for experiment 2). 527
13Fedorenko: For each outer fold, we selected 4sentences as the test fold, resulting in 13outer folds. 528
For each inner fold, we once again select 4sentences as the validation set, resulting in 12inner folds 529
per outer fold. 530
Blank: For each outer fold, we selected a single story as the test fold, resulting in 8outer folds. For 531
each inner fold, each of the remaining stories served in turn as the validation set, resulting in 7inner 532
folds. 533
A.4 Justification of statistical tests 534
We performed a t-test between squared error values from two models to determine if one model 535
performs better than another. While squared error values are not always normally distributed, our 536
sample sizes were large (the minimum sample size was 243) and so we still opted to use a t-test over 537
a non-parametric alternative [ 39]. One issue with a t-test is that relies on the assumption that samples 538
are not correlated, which is not true for time-series data. However, we note that correlated samples 539
leads one to underestimate the standard error of the mean and exaggerate differences between two 540
models. Since we only perform one-sided t-tests to examine whether adding GPT2-XL representations 541
improves performance, the net impact of this on our results is to overestimate how much GPT2-XL 542
contributes over simple features. 543
A.5 Across layer R2values in the Pereira dataset 544
Across layer performances in the Pereira dataset for GPT2-XLU and GPT2-XL when using the sum 545
pooling method (Fig. 5a,b) and the last token method (Fig. 5c,d). Performance in language network 546
is higher across the board than performance in DMN, MD, and visual networks. We do not show 547
auditory network results because participants read passages in Pereira and hence auditory brain scores 548
are near 0. Furthermore, performance is lower with the last token method in every case except in 549
EXP1 trained results where the last token method performs slightly better.
a
c db
Figure 5: a)Across layer performances in Pereira dataset for GPT2-XLU for each functional network
when using the sum-pooling method. EXP1 is on the left, and EXP2 is on the right. b)Same as abut
for GPT2-XL, also using the sum-pooling method. c)Same as abut when using the last token method.
Dotted grey line shows performance of best layer of GPT2-XLU in language network when sum
pooling. d)Same as bbut when using the last token method. Dotted grey line shows performance of
best layer of GPT2-XL in language network when sum pooling.
550
A.6 RoBERTa-Large shows similar results as GPT2-XL 551
To examine whether our results depending on the choice of LLM, we replicated all of our Pereira 552
trained analyses with RoBERTa-Large (ROB). The overall trend in results was the same as 553
with GPT2-XL (Fig. 6). Namely, SP+SL+WORD performed 76.8% as well as the full model 554
(SP+SL+WORD+SENSE+SYNT+ROB) and 80.0% as well as ROB alone in EXP1, and in EXP2 it 555
14performed 88.0% as well as the full model and better than ROB. Furthermore, SENSE and SYNT 556
bridge the gap to the full model by a small amount. In sum, our main conclusion that a large amount 557
of trained LLM brain score in the Pereira dataset is accounted for by non-contextual features also 558
applies to RoBERTa-Large. 559
* *a
b
d
102
101
100
c
102
101
100
L R0.35
0.0
0.4
0
Figure 6: All panels are the same as Figure 3, except GPT2-XL is replaced with RoBERTa-Large
(ROB).
A.7 Orthogonal autocorrelated sequences model (OASM) hyperparameters 560
The width of the Gaussian filter used for within-block smoothing was σ= 2.2in Pereira, σ= 1.8in 561
Fedorenko, and σ= 1.5in Blank. Gaussian widths were determined by sweeping σacross 50 evenly 562
spaced values between 0.1 and 5.0 and choosing the best-performing σfor each dataset. 563
A.8 Shuffled train test splits confound task-relevant and task-irrelevant neural activity 564
OASM is a model which clearly lacks any linguistic representations that would allow it generalize to 565
fully held-out passages. However, this is is not to say that OASM is not correlated with linguistic 566
features. For instance, sentences in a given passage are more semantically related with each other 567
than with sentences in other passages. Nonetheless, using shuffled train-test splits almost certainly 568
exaggerates the variance explained by a model which, on the basis of semantic similarity, arrives 569
at a similar representational structure as OASM. This is because task-irrelevant neural responses 570
make up a large fraction of neural activity [ 40], and shuffled train-test splits allow a model with 571
OASM-like representational structure to predict not just the task-relevant neural responses driven 572
by the participant reading the passage, but also any task-irrelevant neural activity that was present 573
throughout the reading of the passage. Hence, we strongly urge researchers to avoid shuffled train 574
test splits when evaluating the neural predictivity of language models, and we surmise that previous 575
studies using shuffled train-test splits to compare neural predictivity between models might have 576
come to erroneous conclusions. 577
15A.9 Linear decodability of sentence length 578
Here, we show that the MLP block adds a linearly decodable component with non-zero mean to the 579
residual stream in the GPT2 architecture. 580
Proof : 581
We denote the i’th input to the MLP block in the first layer of GPT2-XL as xi. The output of the 582
MLP block is defined as follows: 583
MLP (xi) =xi+Wd(GELU (Wu(LayerNorm (xi))))
We assume that the elements of xiare normally distributed. For a given xi, it then follows that the 584
distribution of elements in LayerNorm (xi)is normal with µ= 0andσ= 1(assuming the standard 585
LayerNorm initialization). 586
Because Wuis initialized from a zero-mean normal distribution, Wu(LayerNorm (xi))also has 587
zero-mean. 588
Note that GELU is a function for which E[Y]>0forYnormally distributed with mean 0. Hence, the 589
mean value across elements following the GELU is non-zero. Let us denote this mean value across 590
all elements of GELU (Wu(LayerNorm (x)))and across all tokens xasm. Then, for an MLP 591
with up-projected dimension du, we can take the dot product of GELU (Wu(LayerNorm (xi))) 592
and1
dum×ˆk, where ˆkis adu-dimensional vector of 1s. The resulting value will have mean 1. 593
However, we cannot decode this value directly from the MLP in practice; first, this vector is down-
projected back to the residual stream by Wd. Nonetheless, we can still closely approximate it,
assuming it is approximately orthogonal to xi, by using the pseudo-inverse of Wd. More specifically,
we can extract a scalar with mean 1 as follows:
r
du
dd×1
dum×ˆkW†
dMLP (xi)
where ddis the down-projected dimension. Because this extracted scalar value is distributed with 594
mean 1 across token representations xi, assuming independence of token representations within a 595
sentence, the sum of the extracted scalar value across the tokens of a sentence is distributed with 596
mean equaling the number of tokens in the sentence. 597
A.10 LMMS 598
LMMS generates a sense embedding for each word by averaging across contextual embeddings (in 599
our case from RoBERTa-Large) of that sense derived from a sense-annotated corpus. For words in 600
WordNet where labeled senses don’t exist, LMMS sets their sense embeddings equal to the average 601
of sense embeddings with the same sense (or same hypernym/lexname if that approach fails). Finally, 602
the sense embeddings are averaged together with the gloss embeddings for that sense of the word 603
generated using the same LLM. For additional details refer to Loureiro et al. [32]. 604
A.11 Contextual syntactic representations 605
Syntactic embeddings are derived by substituting content words (nouns, verbs, adjectives, and 606
adverbs) in the original sentences with words from the Generics KB corpus, matching their part-of- 607
speech and dependency tag via the SpaCy transformer-based tagger [ 41]. For each sentence in the 608
Pereira dataset, we generate 170 new sentences, ensuring the subtree token indices from each token 609
match those of the original sentence. The top 100 sentences, selected based on summed surprisal 610
with GPT2-XL, are retained. Each sentence’s syntactic embedding is then computed by summing 611
token representations within each sentence and then averaging across the 100 sentences. 612
A.12 Word position feature in Fedorenko dataset 613
The primary finding in the paper which first collected the Fedorenko dataset [ 35] was a ramping of 614
neural activity across the words of sentences, where each sentence was 8 words long. Hence, we 615
concatenate a linearly ramping 1-dimensional positional signal to an 8-dimensional 1-hot positonal 616
161 2 3 4 5 6 7 8
Word Position
0.00.6Figure 7: Word Position feature for a single sentence in the Fedorenko dataset.
signal. Because we expect positional signals to be more simlar between adjacent words than more 617
distant words, we apply a Gaussian filter ( σ= 1) to the 8-dimensional positional signal. The resulting 618
feature space, which we refer to as "word position" in the main text, is shown for a single sentence in 619
the above figure. 620
A.13 OASM and GPT2 Model Comparison on Blank Dataset 621
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000016R2
/uni00000032/uni00000024/uni00000036/uni00000030
/uni00000032/uni00000024/uni00000036/uni00000030/uni00000003/uni0000000e/uni00000003/uni0000002a/uni00000033/uni00000037/uni00000015/uni00000010/uni0000003b/uni0000002f
/uni0000002a/uni00000033/uni00000037/uni00000015/uni00000010/uni0000003b/uni0000002f
Figure 8: OASM far outperforms GPT2-XL on the Blank dataset, and GPT2-XL does not appear to
explain any variance beyond that explained by OASM.
We find that OASM achieves 103.6 times higher neural predictivity than GPT2-XL on the Blank 622
dataset when using shuffled train-test splits. There could be several reasons for this. First, it might 623
be that the method for pooling representations from GPT2-XL used here 2.3 and in [ 2,10,11] 624
did not yield useful enough representations for GPT2-XL to map effectively to the brain data. An 625
additional likely culprit is that, of the three datasets we study here, Blank has the greatest potential for 626
autocorrelation in temporally adjacent samples. This is because, while the Pereira dataset typically 627
has a TR every 8 seconds, the Blank dataset has a TR every 2 seconds. We note that our results here 628
are not completely surprising; given that [ 2,10] observed untrained GPT2 models perform far better 629
than trained models on this dataset, it did not seem likely that GPT2-XL would map onto neural 630
representations of linguistic features here. 631
17A.14 Computational Resources 632
All analyses were done between 2 machines: One with 2 RTX 3090 GPUs, and another with 1 633
RTX 4090 GPU. The most computationally demanding parts of our analyses were fitting the banded 634
ridge regressions used to generate Figure 3, collecting untrained model results across 10 seeds, and 635
generating syntactic representations, which each took around 3 hours to complete. 636
A.15 Dataset Licenses 637
The Blank dataset was originally released as part of the Natural Stories Corpus, which is provided 638
under the CC BY-NC-SA license [23]. The Pereira dataset is released under the Creative Commons 639
License [ 8]. The version of the Fedorenko dataset used here is provided under the MIT license. All 640
datasets used are the same versions as in [ 2] and can be downloaded using the neural-nlp repository: 641
https://github.com/mschrimpf/neural-nlp/tree/master . All datasets were collected with 642
IRB approval at their respective institutions. 643
18NeurIPS Paper Checklist 644
1.Claims 645
Question: Do the main claims made in the abstract and introduction accurately reflect the 646
paper’s contributions and scope? 647
Answer: [Yes] 648
Justification: We support each of the three claims made in the abstract regarding shuffled 649
train-test splits, untrained LLM brain scores, and trained LLM brain scores in the Results 650
section. These results support the claim that it is important to deconstruct the mapping 651
between LLMs and the brain. 652
Guidelines: 653
•The answer NA means that the abstract and introduction do not include the claims 654
made in the paper. 655
•The abstract and/or introduction should clearly state the claims made, including the 656
contributions made in the paper and important assumptions and limitations. A No or 657
NA answer to this question will not be perceived well by the reviewers. 658
•The claims made should match theoretical and experimental results, and reflect how 659
much the results can be expected to generalize to other settings. 660
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 661
are not attained by the paper. 662
2.Limitations 663
Question: [Yes] 664
Justification:We discuss the three main limitations in the paper in the section titled "Limita- 665
tions and Conclusions", and additionally include limitations throughout the Appendix (e.g. 666
Justification of statistical tests). 667
Guidelines: 668
•The answer NA means that the paper has no limitation while the answer No means that 669
the paper has limitations, but those are not discussed in the paper. 670
• The authors are encouraged to create a separate "Limitations" section in their paper. 671
•The paper should point out any strong assumptions and how robust the results are to 672
violations of these assumptions (e.g., independence assumptions, noiseless settings, 673
model well-specification, asymptotic approximations only holding locally). The authors 674
should reflect on how these assumptions might be violated in practice and what the 675
implications would be. 676
•The authors should reflect on the scope of the claims made, e.g., if the approach was 677
only tested on a few datasets or with a few runs. In general, empirical results often 678
depend on implicit assumptions, which should be articulated. 679
•The authors should reflect on the factors that influence the performance of the approach. 680
For example, a facial recognition algorithm may perform poorly when image resolution 681
is low or images are taken in low lighting. Or a speech-to-text system might not be 682
used reliably to provide closed captions for online lectures because it fails to handle 683
technical jargon. 684
•The authors should discuss the computational efficiency of the proposed algorithms 685
and how they scale with dataset size. 686
•If applicable, the authors should discuss possible limitations of their approach to 687
address problems of privacy and fairness. 688
•While the authors might fear that complete honesty about limitations might be used by 689
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 690
limitations that aren’t acknowledged in the paper. The authors should use their best 691
judgment and recognize that individual actions in favor of transparency play an impor- 692
tant role in developing norms that preserve the integrity of the community. Reviewers 693
will be specifically instructed to not penalize honesty concerning limitations. 694
3.Theory Assumptions and Proofs 695
19Question: For each theoretical result, does the paper provide the full set of assumptions and 696
a complete (and correct) proof? 697
Answer: [Yes] 698
Justification: Our only theoretical result is that the MLP layer introduces a non-zero mean 699
component in the residual stream. We provide both a rough sketch in the main paper as well 700
as a formal proof. 701
Guidelines: 702
• The answer NA means that the paper does not include theoretical results. 703
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 704
referenced. 705
•All assumptions should be clearly stated or referenced in the statement of any theorems. 706
•The proofs can either appear in the main paper or the supplemental material, but if 707
they appear in the supplemental material, the authors are encouraged to provide a short 708
proof sketch to provide intuition. 709
•Inversely, any informal proof provided in the core of the paper should be complemented 710
by formal proofs provided in appendix or supplemental material. 711
• Theorems and Lemmas that the proof relies upon should be properly referenced. 712
4.Experimental Result Reproducibility 713
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 714
perimental results of the paper to the extent that it affects the main claims and/or conclusions 715
of the paper (regardless of whether the code and data are provided or not)? 716
Answer: [Yes] 717
Justification: We include all details regarding the following: banded regression proce- 718
dure, construction of feature spaces, train, validation, and test splits, and selection of 719
voxels/electrodes/fROIs in neural data. These are all the elements needed to reproduce our 720
results, with the exception of slight variability due to stochasticity in untrained LLM seeds 721
and the randoms search process in banded regression. 722
Guidelines: 723
• The answer NA means that the paper does not include experiments. 724
•If the paper includes experiments, a No answer to this question will not be perceived 725
well by the reviewers: Making the paper reproducible is important, regardless of 726
whether the code and data are provided or not. 727
•If the contribution is a dataset and/or model, the authors should describe the steps taken 728
to make their results reproducible or verifiable. 729
•Depending on the contribution, reproducibility can be accomplished in various ways. 730
For example, if the contribution is a novel architecture, describing the architecture fully 731
might suffice, or if the contribution is a specific model and empirical evaluation, it may 732
be necessary to either make it possible for others to replicate the model with the same 733
dataset, or provide access to the model. In general. releasing code and data is often 734
one good way to accomplish this, but reproducibility can also be provided via detailed 735
instructions for how to replicate the results, access to a hosted model (e.g., in the case 736
of a large language model), releasing of a model checkpoint, or other means that are 737
appropriate to the research performed. 738
•While NeurIPS does not require releasing code, the conference does require all submis- 739
sions to provide some reasonable avenue for reproducibility, which may depend on the 740
nature of the contribution. For example 741
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 742
to reproduce that algorithm. 743
(b)If the contribution is primarily a new model architecture, the paper should describe 744
the architecture clearly and fully. 745
(c)If the contribution is a new model (e.g., a large language model), then there should 746
either be a way to access this model for reproducing the results or a way to reproduce 747
the model (e.g., with an open-source dataset or instructions for how to construct 748
the dataset). 749
20(d)We recognize that reproducibility may be tricky in some cases, in which case 750
authors are welcome to describe the particular way they provide for reproducibility. 751
In the case of closed-source models, it may be that access to the model is limited in 752
some way (e.g., to registered users), but it should be possible for other researchers 753
to have some path to reproducing or verifying the results. 754
5.Open access to data and code 755
Question: Does the paper provide open access to the data and code, with sufficient instruc- 756
tions to faithfully reproduce the main experimental results, as described in supplemental 757
material? 758
Answer: [Yes] 759
Justification: We will release all our code on Github, and all neural datasets are openly 760
available for use. We also provide anonymized code. 761
Guidelines: 762
• The answer NA means that paper does not include experiments requiring code. 763
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 764
public/guides/CodeSubmissionPolicy ) for more details. 765
•While we encourage the release of code and data, we understand that this might not be 766
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 767
including code, unless this is central to the contribution (e.g., for a new open-source 768
benchmark). 769
•The instructions should contain the exact command and environment needed to run to 770
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 771
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 772
•The authors should provide instructions on data access and preparation, including how 773
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 774
•The authors should provide scripts to reproduce all experimental results for the new 775
proposed method and baselines. If only a subset of experiments are reproducible, they 776
should state which ones are omitted from the script and why. 777
•At submission time, to preserve anonymity, the authors should release anonymized 778
versions (if applicable). 779
•Providing as much information as possible in supplemental material (appended to the 780
paper) is recommended, but including URLs to data and code is permitted. 781
6.Experimental Setting/Details 782
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 783
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 784
results? 785
Answer: [Yes] 786
Justification: We dedicate sections towards explaining the data splits in the main paper, and 787
the necessary details to run the banded ridge regression in the main paper and Appendix. 788
Guidelines: 789
• The answer NA means that the paper does not include experiments. 790
•The experimental setting should be presented in the core of the paper to a level of detail 791
that is necessary to appreciate the results and make sense of them. 792
•The full details can be provided either with the code, in appendix, or as supplemental 793
material. 794
7.Experiment Statistical Significance 795
Question: Does the paper report error bars suitably and correctly defined or other appropriate 796
information about the statistical significance of the experiments? 797
Answer: [Yes] 798
Justification: We perform a paired t-test and justify its use in the Appendix. For all plots 799
which show the average across participants we show individual dots for each participant, 800
and for this reason we do not include standard deviation values for the values in Table 1. 801
21Guidelines: 802
• The answer NA means that the paper does not include experiments. 803
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 804
dence intervals, or statistical significance tests, at least for the experiments that support 805
the main claims of the paper. 806
•The factors of variability that the error bars are capturing should be clearly stated (for 807
example, train/test split, initialization, random drawing of some parameter, or overall 808
run with given experimental conditions). 809
•The method for calculating the error bars should be explained (closed form formula, 810
call to a library function, bootstrap, etc.) 811
• The assumptions made should be given (e.g., Normally distributed errors). 812
•It should be clear whether the error bar is the standard deviation or the standard error 813
of the mean. 814
•It is OK to report 1-sigma error bars, but one should state it. The authors should 815
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 816
of Normality of errors is not verified. 817
•For asymmetric distributions, the authors should be careful not to show in tables or 818
figures symmetric error bars that would yield results that are out of range (e.g. negative 819
error rates). 820
•If error bars are reported in tables or plots, The authors should explain in the text how 821
they were calculated and reference the corresponding figures or tables in the text. 822
8.Experiments Compute Resources 823
Question: For each experiment, does the paper provide sufficient information on the com- 824
puter resources (type of compute workers, memory, time of execution) needed to reproduce 825
the experiments? 826
Answer: [Yes] 827
Justification: We provide a section in the appendix describing the GPUs and CPUs used for 828
our analyses, and we describe how long each experiment took to run. 829
Guidelines: 830
• The answer NA means that the paper does not include experiments. 831
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 832
or cloud provider, including relevant memory and storage. 833
•The paper should provide the amount of compute required for each of the individual 834
experimental runs as well as estimate the total compute. 835
•The paper should disclose whether the full research project required more compute 836
than the experiments reported in the paper (e.g., preliminary or failed experiments that 837
didn’t make it into the paper). 838
9.Code Of Ethics 839
Question: Does the research conducted in the paper conform, in every respect, with the 840
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 841
Answer: [Yes] 842
Justification: We did not conduct any direct interactions with human participants, none of 843
the data-related concerns apply for us, and we do not see any direct societal impacts from 844
our work. We make our methods clear to the best of our ability and provide anonymized 845
code. 846
Guidelines: 847
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 848
•If the authors answer No, they should explain the special circumstances that require a 849
deviation from the Code of Ethics. 850
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 851
eration due to laws or regulations in their jurisdiction). 852
10.Broader Impacts 853
22Question: Does the paper discuss both potential positive societal impacts and negative 854
societal impacts of the work performed? 855
Answer: [NA] 856
Justification: We do not develop any novel technology that can be used for good or bad, but 857
rather show that some high-profile previous results have been over-interpreted. While our 858
results are relevant for the cognitive neuroscience community, we do not see a direct path to 859
any larger societal impacts. 860
Guidelines: 861
• The answer NA means that there is no societal impact of the work performed. 862
•If the authors answer NA or No, they should explain why their work has no societal 863
impact or why the paper does not address societal impact. 864
•Examples of negative societal impacts include potential malicious or unintended uses 865
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 866
(e.g., deployment of technologies that could make decisions that unfairly impact specific 867
groups), privacy considerations, and security considerations. 868
•The conference expects that many papers will be foundational research and not tied 869
to particular applications, let alone deployments. However, if there is a direct path to 870
any negative applications, the authors should point it out. For example, it is legitimate 871
to point out that an improvement in the quality of generative models could be used to 872
generate deepfakes for disinformation. On the other hand, it is not needed to point out 873
that a generic algorithm for optimizing neural networks could enable people to train 874
models that generate Deepfakes faster. 875
•The authors should consider possible harms that could arise when the technology is 876
being used as intended and functioning correctly, harms that could arise when the 877
technology is being used as intended but gives incorrect results, and harms following 878
from (intentional or unintentional) misuse of the technology. 879
•If there are negative societal impacts, the authors could also discuss possible mitigation 880
strategies (e.g., gated release of models, providing defenses in addition to attacks, 881
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 882
feedback over time, improving the efficiency and accessibility of ML). 883
11.Safeguards 884
Question: Does the paper describe safeguards that have been put in place for responsible 885
release of data or models that have a high risk for misuse (e.g., pretrained language models, 886
image generators, or scraped datasets)? 887
Answer: [NA] 888
Justification: We release no new models or datasets, and do not see any potential for our 889
results being misused in unsafe ways. 890
Guidelines: 891
• The answer NA means that the paper poses no such risks. 892
•Released models that have a high risk for misuse or dual-use should be released with 893
necessary safeguards to allow for controlled use of the model, for example by requiring 894
that users adhere to usage guidelines or restrictions to access the model or implementing 895
safety filters. 896
•Datasets that have been scraped from the Internet could pose safety risks. The authors 897
should describe how they avoided releasing unsafe images. 898
•We recognize that providing effective safeguards is challenging, and many papers do 899
not require this, but we encourage authors to take this into account and make a best 900
faith effort. 901
12.Licenses for existing assets 902
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 903
the paper, properly credited and are the license and terms of use explicitly mentioned and 904
properly respected? 905
Answer: [Yes] 906
23Justification: We cite the papers in which all datasets used were first published. We provide 907
the licenses for the Blank and Pereira datasets in the supplement (we could not find a license 908
for the Fedorenko dataset). We also specify the version of the datasets used and provide a 909
link. 910
Guidelines: 911
• The answer NA means that the paper does not use existing assets. 912
• The authors should cite the original paper that produced the code package or dataset. 913
•The authors should state which version of the asset is used and, if possible, include a 914
URL. 915
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 916
•For scraped data from a particular source (e.g., website), the copyright and terms of 917
service of that source should be provided. 918
•If assets are released, the license, copyright information, and terms of use in the 919
package should be provided. For popular datasets, paperswithcode.com/datasets 920
has curated licenses for some datasets. Their licensing guide can help determine the 921
license of a dataset. 922
•For existing datasets that are re-packaged, both the original license and the license of 923
the derived asset (if it has changed) should be provided. 924
•If this information is not available online, the authors are encouraged to reach out to 925
the asset’s creators. 926
13.New Assets 927
Question: Are new assets introduced in the paper well documented and is the documentation 928
provided alongside the assets? 929
Answer: [NA] 930
Justification: We do not release any new assets with this paper. 931
Guidelines: 932
• The answer NA means that the paper does not release new assets. 933
•Researchers should communicate the details of the dataset/code/model as part of their 934
submissions via structured templates. This includes details about training, license, 935
limitations, etc. 936
•The paper should discuss whether and how consent was obtained from people whose 937
asset is used. 938
•At submission time, remember to anonymize your assets (if applicable). You can either 939
create an anonymized URL or include an anonymized zip file. 940
14.Crowdsourcing and Research with Human Subjects 941
Question: For crowdsourcing experiments and research with human subjects, does the paper 942
include the full text of instructions given to participants and screenshots, if applicable, as 943
well as details about compensation (if any)? 944
Answer: [Yes] 945
Justification: We use open source datasets where neural data is obtained from consenting 946
human adults. Information regarding research protocols is detailed in the references for 947
these datasets. 948
Guidelines: 949
•The answer NA means that the paper does not involve crowdsourcing nor research with 950
human subjects. 951
•Including this information in the supplemental material is fine, but if the main contribu- 952
tion of the paper involves human subjects, then as much detail as possible should be 953
included in the main paper. 954
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 955
or other labor should be paid at least the minimum wage in the country of the data 956
collector. 957
2415.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 958
Subjects 959
Question: Does the paper describe potential risks incurred by study participants, whether 960
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 961
approvals (or an equivalent approval/review based on the requirements of your country or 962
institution) were obtained? 963
Answer: [Yes] 964
Justification: All datasets used here were collected with IRB approval at their respective 965
institutions, and this is stated in the appendix. We do not collect any data of our own from 966
human subjects. 967
Guidelines: 968
•The answer NA means that the paper does not involve crowdsourcing nor research with 969
human subjects. 970
•Depending on the country in which research is conducted, IRB approval (or equivalent) 971
may be required for any human subjects research. If you obtained IRB approval, you 972
should clearly state this in the paper. 973
•We recognize that the procedures for this may vary significantly between institutions 974
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 975
guidelines for their institution. 976
•For initial submissions, do not include any information that would break anonymity (if 977
applicable), such as the institution conducting the review. 978
25