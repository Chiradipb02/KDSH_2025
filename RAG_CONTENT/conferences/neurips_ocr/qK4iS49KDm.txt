Neural network learns low-dimensional polynomials
with SGD near the information-theoretic limit
Jason D. Lee1,Kazusato Oko2,4,Taiji Suzuki3,4,Denny Wu5,6
1Princeton University,2University of California, Berkeley,3University of Tokyo
4RIKEN AIP,5New York University,6Flatiron Institute
jasonlee@princeton.edu ,oko@berkeley.edu ,
taiji@mist.i.u-tokyo.ac.jp ,dennywu@nyu.edu
Abstract
We study the problem of gradient descent learning of a single-index target function
f∗(x) =σ∗(⟨x,θ⟩)under isotropic Gaussian data in Rd, where the unknown link
function σ∗:R→Rhas information exponent p(defined as the lowest degree in
the Hermite expansion). Prior works showed that gradient-based training of neural
networks can learn this target with n≳dΘ(p)samples, and such complexity is pre-
dicted to be necessary by the correlational statistical query lower bound. Surpris-
ingly, we prove that a two-layer neural network optimized by an SGD-based algo-
rithm (on the squared loss) learns f∗with a complexity that is not governed by the
information exponent. Specifically, for arbitrary polynomial single-index mod-
els, we establish a sample and runtime complexity of n≃T= Θ( d·polylog d),
where Θ(·)hides a constant only depending on the degree of σ∗; this dimension
dependence matches the information theoretic limit up to polylogarithmic factors.
More generally, we show that n≳d(p∗−1)∨1samples are sufficient to achieve low
generalization error, where p∗≤pis the generative exponent of the link function.
Core to our analysis is the reuse of minibatch in the gradient computation, which
gives rise to higher-order information beyond correlational queries.
1 Introduction
Single-index models are a classical class of functions that capture low-dimensional structure in the
learning problem. To efficiently estimate such functions, the learning algorithm should extract the
relevant (one-dimensional) subspace from high-dimensional observations; hence this problem set-
ting has been extensively studied in deep learning theory [BL20, BES+22, BBSS22, MHPG+23,
MZD+23, WWF24], to examine the adaptivity to low-dimensional targets and benefit of represen-
tation learning in neural networks (NNs) optimized by gradient descent (GD). In this work we study
the learning of a single-index target function under isotropic Gaussian data:
yi=f∗(xi) +ςi, f∗(xi) =σ∗(⟨xi,θ⟩),xii.i.d.∼ N (0,Id), (1.1)
where ςiis i.i.d. label noise, θ∈Rdis the direction of index features, and we assume the link
function σ∗:R→Rhas information exponent p∈N+defined as the index of the first non-zero
coefficient in the Hermite expansion (see Definition 1).
Equation (1.1) requires the estimation of the one-dimensional link function σ∗and the relevant direc-
tionθ; it is known that learning is information theoretically possible with n≳dtraining examples
[DH24, DPVLB24]. Indeed, when σ∗is polynomial, such statistical complexity can be achieved
up to logarithmic factors by a tailored algorithm that exploit the structure of low-dimensional target
[CM20]. On the other hand, for gradient-based training of two-layer NNs, existing works estab-
lished a sample complexity of n≳dΘ(p)[BAGJ21, BBSS22, DNGL23], which presents a gap
38th Conference on Neural Information Processing Systems (NeurIPS 2024).102103
dimensionality d104105sample size n
nd2
w,2
0.020.040.060.08(a) Online SGD (weak recovery).
102103
dimensionality d103104sample size n
nd
Err0.20.40.60.81.01.21.4 (b) Same-batch GD (generalization error).
Figure 1: We train a ReLU NN (3.1) with N= 1024 neurons using SGD (squared loss) with step size η= 1/d
to learn a single-index target f∗(x) =He3(⟨x,θ⟩); heatmaps are values averaged over 10 runs. (a)online
SGD with batch size B= 8;(b)GD on the same batch of size nforT= 214steps. For online SGD we only
report weak recovery (i.e., averaged overlap between neuron wand target θ) since the test error does not drop.
between the information theoretic limit and what is computationally achievable by (S)GD. Such a
gap is also predicted by the correlational statistical query (CSQ) lower bound [DLS22, AAM23],
which roughly states that for a CSQ algorithm to learn (isotropic) Gaussian single-index models
using less than exponential compute, a sample size of n≳dp/2is necessary.
Although CSQ lower bounds are frequently cited to imply a fundamental barrier of learning via SGD
(with the squared/correlation loss), strictly speaking, the CSQ model does not include empirical
risk minimization with gradient descent, due to the non-adversarial noise and existence of non-
correlational terms in the gradient computation. Very recently, [DTA+24] exploited higher-order
terms in the gradient update arising from the reuse of the same training data, and showed that for
certain link functions with high information exponent ( p > 2), two-layer NNs may still achieve
weak recovery (i.e., nontrivial overlap with θ) after two GD steps with Θ(d)batch size. While this
presents evidence that GD-trained NNs can learn f∗beyond the sample complexity suggested by
the CSQ lower bound, the weak recovery statement in [DTA+24] may not translate to statistical
guarantees; moreover, the class of functions where SGD can achieve vanishing generalization error
is not fully characterized, as only a few specific examples of link functions are discussed.
Given the existence of (non-NN) algorithms that learn any single-index polynomials in n=˜O(d)
samples [CM20] regardless of the information exponent p, and more generally, non-CSQ algo-
rithms with a sample complexity surpassing the CSQ lower bound [DPVLB24], it is natural to ask if
gradient-based training of NNs can achieve similar statistical efficiency for this function class. Mo-
tivated by observations in [DTA+24] that SGD with reused data may break the “curse of information
exponent”, we aim to address the question:
Can NN optimized by SGD with reused batch learn single-index f∗beyond the CSQ lower bound?
And for polynomial σ∗, can learning succeed near the information-theoretic limit n≃d?
Empirically, the separation between one-pass (online) and multi-pass SGD is clearly observed in
Figure 1, where we trained the same two-layer ReLU neural network to learn a single-index poly-
nomial with information exponent p= 3. We see that SGD with reused data (Figure 1(b)) reaches
low test error using roughly n≃dsamples, whereas online SGD fails to achieve even weak recov-
ery with much larger sample size n= Ω( d2). Our main contribution is to establish this improved
statistical complexity for two-layer NNs trained by a variant of SGD with reused training data.
1.1 Our Contributions
We answer the above question in the affirmative by showing that SGD training (with the squared
loss) on a natural class of shallow NNs can achieve small generalization error using polynomial
compute and a sample complexity that is not governed by the information exponent, if we employ
a layer-wise optimization procedure (analogous to that in [BES+22, DLS22, AAM23]) and reuse
of the same minibatch. The core insight is that SGD can implement a full statistical query (SQ)
algorithm that goes beyond CSQ, despite the correlational structure of the squared loss. Our main
finding is summarized by the following theorem.
2Information
theoretic limitSGD + batch reuse [ This work ]
SQ algorithm [CM20]Smoothed SGD [DNGL23]
CSQ lower bound [DLS22]One-pass SGD
[BAGJ21]Kernel methods
[GMMM21]
d˜Θ(d) ˜Θ(dp/2) ˜Θ(dp−1) Θ(dq)
Figure 2: Complexity of learning single-index model where the link function σ∗is a degree- qpolynomial with
information exponent p. For the CSQ lower bound, we translate the tolerance to sample complexity using the
i.i.d. concentration heuristic τ≈n−1/2. We restrict ourselves to algorithms using polynomial compute; this
excludes the sphere-covering procedure in [DPVLB24] or exponential-width neural network in [Bac17, TS24].
Theorem (informal) .A shallow NN with N=˜Θd(1)neurons can learn arbitrary single-index
models up to small population loss: Ex[(fΘ(x)−f∗(x))2] =od,P(1), if we employ an SGD-based
algorithm (with reused training data) to minimize the squared loss objective, with a sample and
runtime complexity of n, T=˜Θd(d(p∗−1)∨1), where p∗is the generative exponent of the link σ∗.
Note that the generative exponent [DPVLB24] is defined as the minimum information exponent of
the link function σ∗after arbitrary L2transformation, and hence by definition p∗≤p(equality is
achieved by the identity transformation). We make the following remarks on our main result.
• We know that p∗≤2for arbitrary polynomial link functions. Therefore, the theorem suggests that
NN + SGD with reused batch can learn single-index polynomials with a sample complexity n=
˜Od(d)which is information theoretically optimal up to polylogarithmic factors, hence matching
the efficiency of SQ algorithms tailored for low-dimensional polynomial regression [CM20].
• For non-polynomial σ∗with high generative exponent p∗>2, our sample complexity n≳dp∗−1
can be interpreted as an SQ version of the online SGD result in [BAGJ21]. Since the information
exponent pcan be arbitrarily larger than the generative exponent p∗, our main theorem disproves
a conjecture in [AAM23] stating that n≍dp/2is the optimal sample complexity for empirical
risk minimization with SGD on the squared loss / correlation loss.
• A key observation in our analysis is that with suitable activation function, SGD with reused batch
can go beyond correlational queries and implement (a subclass of) SQ algorithms. This enables
polynomial transformations to the labels that reduce the information exponent, and therefore op-
timization can escape the high-entropy “equator” at initialization in polylogarithmic time.
Upon completion of this work, we became aware of the preprint [ADK+24] showing weak recovery
(for polynomial targets with p∗≤2) with similar sample complexity, also by exploiting the reuse of
training data. Our work was conducted independently and simultaneously.
2 Problem Setting and Prior Works
Notations. ∥·∥denotes the ℓ2norm for vectors and the ℓ2→ℓ2operator norm for matrices. Od(·)
andod(·)stand for the big-O and little-o notations, where the subscript highlights the asymptotic
variable dand suppresses dependence on p, q; we write ˜O(·)when (poly-)logarithmic factors are
ignored. Od,P(·)(resp. od,P(·)) represents big-O (resp. little-o) in probability as d→ ∞ .Ω(·),Θ(·)
are defined analogously. γis the standard Gaussian distribution in R. We denote the L2-norm of a
function fwith respect to the data distribution (which will be specified) as ∥f∥L2. For g:R→R,
we denote gias its i-th exponentiation, and g(i)is the i-th derivative. We say an event happens with
high probability when the failure probability is bounded by exp(−Clogd)for large constant C.
2.1 Complexity of Learning Single-index Models
We aim to learn a single-index model (1.1) where the link function σ∗:R→Rhas information
exponent pdefined as follows [DH18, BAGJ21].
Definition 1 (Information exponent) .Let{Hej}∞
j=0denote the normalized Hermite polynomials.
The information exponent of g∈L2(γ), denoted by IE(g) := p∈N+, is the index of the first
non-zero Hermite coefficient of g, that is, given g(z) =P∞
i=0αiHei(z),p:= min {i>0 :αi̸=0}.
By definition, when σ∗is a degree- qpolynomial, we always have p≤q. Note that f∗contains
Θ(d)parameters to be estimated, and hence information theoretically n≳dsamples are both suf-
3ficient and necessary for learning [MM18, BKM+19, DPVLB24]; however, the sample complexity
achieved by different (polynomial time) algorithms depends on structure of the link function.
•Kernel Methods. Rotationally invariant kernels cannot adapt to the low-dimensional structure of
single-index f∗and hence suffer from the curse of dimensionality [YS19, GMMM21, DWY21,
BES+22]. By a standard dimension argument [KMS20, HSSVG21, AAM22], we know that in
the isotropic data setting, kernel methods (including neural networks in the lazy regime [JGH18,
COB19]) require n≳dqsamples to learn degree- qpolynomials in Rd.
•Gradient-based Training of NNs. While NNs can easily approximate a single-index model
[Bac17], the sample complexity of gradient-based learning established in prior works typically
scales as n≳dΘ(p): in the well-specified setting, [BAGJ21] proved a sample complexity of n=
˜Θ(dp−1)for online SGD, which is later improved to ˜Θ(dp/2)by a smoothed objective [DNGL23];
as for the misspecified setting, [BBSS22, DKL+23] showed that n≳dpsamples suffice, and
in some cases a ˜Θ(dp−1)complexity is achievable [AAM23, OSSW24a]. Consequently, at the
information-theoretic limit ( n≍d), existing results can only cover the learning of low information
exponent targets [AAM22, BMZ23, BES+23]. This exponential dependence on palso appears
in the CSQ lower bounds [DLS22, AAM22], which is often considered to be indicative of the
performance of SGD learning with the squared loss (see Section 2.2).
Statistical Query Learners. If we do not restrict ourselves to correlational queries, the sample
complexity of learning (1.1) can be drastically improved. Specifically, for polynomial σ∗, [CM20]
gave an SQ algorithm that achieves low generalization error in n=˜O(d)samples, which is near the
information-theoretic limit; the key ingredient is to construct nonlinear transformations to the labels
that lowers the information exponent to 2; similar preprocessing also appeared in context of phase
retrieval [MM18, BKM+19]. Such transformations do not belong to CSQ, but can be utilized by a
full SQ learner to enhance the statistical efficiency. Recently, [DPVLB24] introduced the generative
exponent which governs the complexity of SQ algorithms.
Definition 2 (Generative exponent) .The generative exponent (GE) of g∈L2(γ)is defined as the
lowest information exponent (IE) after arbitrary L2transformation, that is,
p∗=: GE( g) = inf
T ∈L2(Py)IE(T ◦g).
The generative exponent is the smallest information exponent obtained by all possible label transfor-
mations. By definition we always have p∗≤p, and the gap between the two indices can be arbitrarily
large; for example, for the Hermite polynomials we have IE(Hek) =kwhereas GE(Hek)≤2.
[DPVLB24] established a sample complexity lower bound of n= Ω( dp∗/2∨1)for full SQ learners
with polynomial compute (assuming τ≈n−1/2), and obtained matching upper bound by a tensor
partial-trace algorithm. Our goal is to show that SGD training of two-layer neural network can
also achieve a sample and runtime complexity that scales with n≃dΘ(p∗), where the dimension
dependence is governed by the generative exponent p∗instead of the information exponent p.
2.2 Can Gradient Descent Go Beyond Correlational Queries?
Correlational statistical query. A statistical query (SQ) learner [Kea98, Rey20] accesses the tar-
getf∗through noisy queries ˜ϕwith error tolerance τ:|˜ϕ−Ex,y[ϕ(x, y)]| ≤τ. Lower bound on
the performance of SQ algorithm is a classical measure of computational hardness. In the context
of gradient-based optimization, an often-studied subclass of SQ is the correlational statistical query
(CSQ) [BF02] where the query is restricted to (noisy version of) Ex,y[ϕ(x)y]. To see the connection
between CSQ and SGD, consider the gradient of expected squared loss for one neuron fw(x):
∇wEx,y(fw(x)−y)2∝ −Ex,y[y· ∇wfw(x)|{z}
correlational query] +Ex[fw(x)· ∇wfw(x)| {z }
can be evaluated without y].
One can see that information of the target function is encoded in the correlation term in the gradi-
ent. To infer the statistical efficiency of GD in the empirical risk minimization setting, we replace
the population gradient with the empirical average ∇w(1
nPn
i=1(fw(xi)−yi)2), and heuristically
equate the CSQ tolerance τwith the scale of i.i.d. concentration error n−1/2.
4For the Gaussian single-index model class with information exponent p, [DLS22] proved a lower
bound stating that a CSQ learner either has access to queries with tolerance τ≲d−p/4, or exponen-
tially many queries are needed to learn f∗with small population loss. Using the heuristic τ≈n−1/2,
this suggests a sample complexity lower bound n≳dp/2for polynomial time CSQ algorithm. This
lower bound can be achieved by a landscape smoothing procedure [DNGL23] (in the well-specified
setting), and is conjectured to be optimal for empirical risk minimization with SGD [AAM23].
SGD with reused data. As previously discussed, the gap between SQ and CSQ algorithms primar-
ily stems from the existence of label transformations that decrease the information exponent. While
such transformation cannot be utilized by a CSQ learner, [DTA+24] argued that they may arise from
two consecutive gradient updates using the same minibatch. For illustrative purposes, consider one
neuron fw(x) =σ(⟨x,w⟩)updated by two GD steps using the same data point (x, y), starting from
zero initialization w0=0(we focus on the correlational term in the loss for simplicity):
w2=w1+η·yσ′(⟨x,w1⟩)x=ησ′(0)y·x|{z}
CSQ term+η yσ′(ησ′(0)∥x∥2·y)x| {z }
non-CSQ term. (2.1)
Under appropriate learning rate scaling η· ∥x∥2= Θ(1) , one can see that in the second gradient
step, the label yis transformed by the nonlinearity σ′, even though the loss function itself is not
modified. Based on this observation, [DTA+24] showed that if the non-CSQ term in (2.1) reduces
the information exponent to 1, then weak recovery (i.e., nontrivial overlap between the first-layer
parameters wand index features θ) can be achieved after two GD steps with n= Θ( d)samples.
2.3 Challenges in Establishing Statistical Guarantees
Importantly, the analysis in [DTA+24] does not lead to concrete learnability guarantees for the class
of single-index polynomials for the following reasons: (i)it is not clear if an appropriate nonlinear
transformation that lowers the information exponent can always be extracted from SGD with reused
data, and (ii)the weak recovery guarantee may not translate to a sample complexity for the trained
NN to achieve small generalization error. We elaborate these technical challenges below.
SGD decreases information exponent. To show weak recovery, [DTA+24, Definition 3.1] as-
sumed that the student activation σcan reduce the information exponent of the labels to 1; while a
few examples are given, the existence of such transformations in SGD is not guaranteed:
• The label transformation employed in prior SQ algorithms [CM20] is based on thresholding,
which reduces the information exponent to 2for any polynomial σ∗; however, isolating such
function from SGD updates on the squared loss is challenging. Instead, we make use of monomial
transformation which can be extracted from SGD via Taylor expansion.
• If the link function satisfies p∗≥2, its information exponent after arbitrary nonlinear transforma-
tion is at least 2; such functions are predicted not be not learnable by SGD in the n≍dregime
[DTA+24]. To handle this setting, we analyze the SGD update up to poly( d)time, at which a non-
trivial overlap can be established by a Gr ¨onwall-type argument similar to [BAGJ21]. For p∗= 2,
this recovers results on phase retrieval when σ∗(z) =z2which requires n= Ω(dlogd)samples.
From weak recovery to sample complexity. Note that weak recovery (i.e., |⟨w,θ⟩|> ε for
some small constant ε > 0) is generally insufficient to establish low generalization error of the
trained NN. Therefore, we need to show that starting from a nontrivial overlap, subsequent gradient
steps can achieve strong recovery of the index features (i.e., |⟨w,θ⟩|>1−ε), despite the link
misspecification. After the first-layer parameters align with the target function, we train the second-
layer parameters with SGD to learn the link function σ∗with the aid of random bias units [DLS22].
3 Learning Polynomial f∗in Linear Sample Complexity
We first consider the setting where σ∗is polynomial with degree qspecified as follows.
Assumption 1. The target function is given as f∗(x) =σ∗(⟨x,θ⟩), where the link function σ∗:
R→Radmits the Hermite decomposition σ∗(z) =Pq
i=pαiHei(z).
For single-index polynomials, we do not expect a computational-to-statistical gap under the SQ class
[CM20] — indeed, we will establish learning guarantees near the information theoretic limit n≍d.
5Algorithm 1: Gradient-based training of two-layer neural network
Input : Step sizes ηt; momentum parameters ξt; training time T1, T2;ℓ2regularization λ.
Initialize w0
j∼Sd−1(1),aj∼Unif{±ca}.
Phase I: normalized SGD on first-layer parameters
fort= 0 toT1do
iftis even then
x∼ N(0,Id), y=f∗(x) +ς; // Draw i.i.d. data (x, y)
wt
j←wt
j−ξt
j(wt
j−wt−2
j), (when t >0) ; // Interpolation step
wt
j←wt
j/∥wt
j∥; // Normalization
end
wt+1
j←wt
j−ηt˜∇w(fΘ(x)−y)2,(j= 1, . . . , N ); // SGD step
end
Initialize bj∼Unif([−Cb, Cb]).
Phase II: SGD on second-layer parameters
ˆa←argmina∈RN1
T2PT2
i=1(fΘ(xi)−yi)2+λ∥a∥2; // Ridge regression
Output: Prediction function x7→fˆΘ(x)with ˆΘ= (ˆaj,wT1
j, bj)N
j=1.
3.1 Training Algorithm
We train the following two-layer network with Nneurons using SGD to minimize the squared loss:
fΘ(x) =1
NNX
j=1ajσj(⟨x,wj⟩+bj), (3.1)
where Θ= (wj, aj, bj)N
j=1are trainable parameters, and σj:R→Ris the activation function
defined as the sum of Hermite polynomials up to degree Cσ:σj(z) :=PCσ
i=0βj,iHei(z), where Cσ
only depends on the degree of link function σ∗. Note that we allow each neuron to have a different
nonlinearity as indicated by the subscript in σj; this subscript is omitted when we focus on the
dynamics of one single neuron. Our SGD training procedure is described in Algorithm 1, and below
we outline the key ingredients of the algorithm.
• Algorithm 1 employs a layer-wise training strategy common in the recent feature learning theory
literature [DLS22, BES+22, BBSS22, AAM23, MHWSE23], where in the first stage, we optimize
the first-layer parameters {wj}N
j=1with normalized SGD to learn the low-dimensional latent rep-
resentation (index features θ), and in the second phase, we train the second-layer {aj}N
j=1to fit
the unknown link function σ∗.
• The most crucial part in Phase I of Algorithm 1 is the reuse of the same minibatch in the gradient
computation. Specifically, we sample a fresh batch of training examples in every two GD steps ;
this enables us to extract non-CSQ terms from two consecutive gradient updates outlined in (2.1).
• We introduce an interpolation step between the current and previous iterates with hyperparameter
ξto stabilize the training dynamics; this resembles a negative momentum often seen in optimiza-
tion algorithms [AZ18, ZLBH19]; the role of this interpolation is discussed in Section 4.2. We use
a projected gradient update ˜∇wL(w) = (Id−w2tw2t⊤)∇wL(w)for steps 2tand2t+ 1, where
∇wis the Euclidean gradient; similar use of projection also appeared in [DNGL23, AAM23].
3.2 Convergence and Sample Complexity
Weak Recovery Guarantee. We first consider the “search phase” of SGD, and show that after
running Phase I of Algorithm 1 for T= polylog( d)steps, a subset of parameters wachieve non-
trivial overlap with the target direction θ. We denote H(g;j)as the j-th Hermite coefficient of some
g∈L2(γ). Our main theorems handle polynomial activations satisfying the following condition.
Assumption 2. We require the activation function to be a polynomial σ(z) =PCσ
i=0βiHei(z)and
its degree Cσto be sufficiently large so that Cσ≥Cqholds ( Cqis defined in Proposition 6). For all
2≤ℓ≤Cσandk= 0,1, we assume that H 
σ(ℓ)(σ(1))ℓ−1;k
>0.
6As discussed in Appendix B.1, for a given σ∗, the above assumption only needs to be met for one pair
of(k, ℓ). Appendix B.1.3 states that H 
σ(ℓ)(σ(1))ℓ−1;k
̸= 0also suffices if we set the momentum
parameter ξdifferently. Now we verify this condition for a wide range of polynomial activations.
Lemma 3. Given ℓ≥2andk≥0. For Cσ≥2ℓ+k−1
ℓ, if we choose {βi}Cσ
i=0where βiis randomly
drawn from some non-empty interval [ai, bi], then H(σ(ℓ)(σ(1))ℓ−1;k)̸= 0with probability 1.
The next theorem states that n=˜Θ(d)samples are sufficient for SGD to achieve weak recovery.
Theorem 1. Under Assumptions 1 and 2, for suitable choices of hyperparameters ηt=˜Od(Nd−1)
and1−ξt=od(1), there exists constant C(q)such that after Phase I of Algorithm 1 is run for
2T1,1=C(q)·dpolylog( d)steps, with high probability, there exists a subset of neurons w2T1
j∈ W
with|W|=˜Θ(N)such that⟨w2T1
j,θ⟩> cfor some c≳1/polylog( d).
Recall that at random initialization we have ⟨w,θ⟩ ≈d−1/2with high probability. The theorem
hence implies that SGD “escapes from mediocrity” after seeing n=˜O(d)samples, analogous to
the information exponent p= 2 setting studied in [BAGJ21]. We remark that due to the small
second-layer initialization, the squared loss is dominated by the correlation loss, which allows us
to track the evolution of each neuron independently; similar use of vanishing initialization also
appeared in [BES+22, AAM23].
Strong recovery and sample complexity. After weak recovery is achieved, we continue Phase I
to amplify the alignment. Due to the nontrivial overlap between wandθ, the objective is no longer
dominated by the lowest degree in the Hermite expansion. Therefore, to establish strong recovery
(⟨w,θ⟩>1−ε), we place an additional assumption on the activation function.
Assumption 3. Given the Hermite expansions σ∗(z) =Pq
i=pαiHei(z),σj(z) =PCσ
i=0βj,iHei(z),
we assume the coefficients satisfy αiβj,i≥0forp≤i≤q.
This assumption is easily verified in the well-specified setting σ∗=σ[BAGJ21] since αi=βi, and
under link misspecification, it has been directly assumed in prior work [MHWSE23]. We follow
[OSSW24a] and show that by randomizing the Hermite coefficients of the activation function, a
subset of neurons satisfy the above assumption for any degree- qpolynomial link function σ∗.
Lemma 4. If we set σj(z) =PCσ
i=0βj,iHei(z), where for each neuron we sample βj,ii.i.d.∼
Unif({±ri})with appropriate constant ri, then Assumption 2 and 3 are satisfied in exp(−Θ(q))-
fraction of neurons.
Note that in our construction of activation functions for both assumptions, we do not exploit knowl-
edge of the link function σ∗other than its degree qwhich decides the constant Cσ; see Appendix B.1
for more discussion of Assumption 3 and Lemma 4. The next theorem shows that by running Phase
I for˜Θ(d)more steps, a subset of neurons achieves sufficiently large overlap with the index features.
Theorem 2. For student neurons satisfying Assumptions 2, 3 and parameter wjstarting from non-
trivial overlap c > 0specified in Theorem 1, if Phase I of Algorithm 1 continues for 2T1,2=
˜Θd(dε−2)steps with hyperparameters ηt=˜Od(Nd−1ε),ξt= 1, we achieve
w2(T1,1+T1,2)
j ,θ
>
1−εwith high probability.
The following proposition shows that after strong recovery, training the second-layer parameters in
Phase II is sufficient for the NN model (3.1) to achieve small generalization error.
Proposition 5. After Phase I terminates, for suitable λ >0, the output of Phase II satisfies
Ex[(fˆΘ(x)−f∗(x))2]≲ε2.
with probability 1 as d→ ∞ , if we set T2=C(q)N4polylog( d)ε−4,N=C(q)polylog( d)ε−1for
some constant C(q)depending on the target degree q.
Putting things together. Combining the above theorems, we conclude that in order for two-layer
NN (3.1) trained by Algorithm 1 to achieve εpopulation squared loss, it is sufficient to set
n=T1+T2≍C(q)·(dε−2∨ε−8)polylog( d), N≍C(q)·ε−1polylog( d),
where constant C(q)only depends on the target degree q(although exponentially). Hence we may
setε−1≍polylog dto conclude an almost-linear sample and computational complexity for learning
arbitrary single-index polynomials up to od(1)population error.
74 Proof Sketch
In this section we outline the high-level ideas and key steps in our derivation.
4.1 Monomial Transformation Reduces Information Exponent
To prove the main theorem, we first establish the existence of nonlinear label transformation that (i)
reduces the information exponent, and (ii)can be easily extracted from SGD updates. If we ignore
desideratum (ii), then for polynomial link functions, transformations that decrease the information
exponent to at most 2have been constructed in [CM20, Section 2.1]. However, prior results are
based on the thresholding function, and it is not clear if such function naturally arises from SGD with
batch reuse. The following proposition shows that the effect of thresholding can also be achieved by
a simple monomial transformation where the required degree can be uniformly upper bounded.
Proposition 6. Letg:R→Rbe any polynomial with degree up to pand∥g∥2
L2(γ)= 1, then
(i) There exists some i≤Cq∈N+such that IE(gi)≤2, where constant Cqonly depends on q.
(ii) Let godd:R→Rbe the odd part of gwith∥godd∥2
L2(γ)≥ρ >0. Then there exists some
i≤Cq,ρ∈N+such that IE(gi) = 1 , where constant Cq,ρonly depends on qandρ.
The proof can be found in Appendix A. We make the following remarks.
• The proposition implies that for any polynomial link function that is not even, there exists some
i∈N+only depending on the degree of σ∗such that raising the function to the i-th power
reduces the information exponent to 1(this implies the generative exponent p∗= 1). For even
σ∗, the information exponent after arbitrary transformation is at least 2(p∗= 2), which can also
be attained by monomial transformation. Furthermore, we provide a uniform upper-bound on the
required degree of transformation ivia a compactness argument.
• The advantage of working with monomial transformations is that they can be obtained from two
GD steps on the same training example, by Taylor expanding the activation σ′. In Section 4.2,
we build upon this observation to show that Phase I of Algorithm 1 achieves weak recovery using
n≳dpolylog( d)samples.
Intuition behind the analysis. Our proof is inspired by [CM20] which introduced a (non-
polynomial) label transformation that reduces the information exponent of any degree- qpolynomial
to at most 2. To prove the existence of monomial transformation for the same purpose, we first show
that for a fixed link function σ∗, there exists some isuch that the i-th power of the link function has
information exponent 2, which mirrors the transformation used in [CM20]. Then, we make use of
the compactness of the space of link functions to define a test function and obtain a uniform bound
oni. As for the polynomial transformation for non-even functions, we exploit the asymmetry of σ∗
to further reduce the information exponent to 1.
4.2 SGD with Batch Reuse Implements Polynomial Transformation
Now we present a more formal discussion of (2.1) to illustrate how polynomial transformation can
be utilized in batch reuse SGD. We let ηt≡η. When one neuron fw(x) =σ(⟨x,w⟩)is updated by
two GD steps using the same sample (x, y), starting from w0:=ω, the alignment with θbecomes
⟨θ,w2⟩=
θ,
w1+η·yσ′(⟨x,w1⟩)x
=⟨θ,ω⟩+
η
yσ′(⟨ω,x⟩)⟨θ,x⟩+PCσ−1
i=0(η∥x∥2)iyi+1(i!)−1(σ′(⟨ω,x⟩))iσ(i+1)(⟨ω,x⟩)⟨θ,x⟩| {z }
=:ψi
.(4.1)
We take η≤cηd−1with a small constant cηso that η∥x∥2≪1with high probability. Crucially,
the strength of each term in (4.1) can vary depending on properties of the unknown link function
σ∗. Hence a careful analysis is required to ensure that a suitable monomial transformation is always
singled out from the gradient. We establish the following lemma on the evolution of alignment.
Lemma 7. Under the assumptions per Theorem 1, the following holds for p∗= 1,2:
⟨θ,w2(t+1)⟩ ≥ ⟨θ,w2t⟩+cI
ηcξcσd−p∗
2∨1(κ2t)p∗−1+cηcξd−p∗
2∨1ν2t.
8See Lemma 16 for the formal version. For p∗= 1, taking expectation immediately yields that weak
recovery within (η(1−ξ)γ)−1=O(d)steps. For p∗= 2,⟨θ,w2t
j⟩=:κtcan be approximated
by a differential equationdκt
dt=η(1−ξ)γκt. Solving this yields κt=κ0exp(η(1−ξ)γt)≈
d−1
2exp(η(1−ξ)γt), and weak recovery is obtained within t≲(η(1−ξ)γ)−1·logd=O(dlogd)
steps, similar to the analysis in [BAGJ21].
Why interpolation is needed. In our setting, the signal strength may not dominate the error from
discarding the effect of normalization. In prior analyses for online SGD, given the gradient −g
and projection Pw=Id−ww⊤, the spherical gradient changes the alignment as ⟨θ,wt+1⟩=
θ,wt+ηPwg
∥wt+ηPwg∥
≥ ⟨θ,wt⟩+η⟨θ,g⟩ −1
2η2∥g∥2⟨θ,wt⟩+(h.o.t.), see [BAGJ21, DNGL23]. Here
η⟨θ,g⟩corresponds to the signal, and −1
2η2∥g∥2⟨θ,wt⟩comes from normalization. Thus, taking
ηsufficiently small, the normalization error shrinks faster than the signal. However, in our case the
signal shrinks at the rate of cI
η(recall that η=cηd−1), and hence taking a smaller step may not
improve the signal-to-noise ratio when the degree of transformation Iis large. The interpolation
step in Algorithm 1 reduces the effect of normalization without shrinking the signal too much. In
particular, by setting ξ= 1−˜η, we see that the signal is affected by a factor of ˜ηwhereas the
normalization error shrinks by ˜η2; this allows us to boost the signal-to-noise ratio by taking ˜ηsmall.
4.3 Analysis of Phase II and Statistical Guarantees
Once strong recovery is achieved for the first-layer parameters, we turn to Phase II and optimize
the second-layer with ℓ2regularization. Since the objective is strongly convex, gradient-based op-
timization can efficiently minimize the empirical loss. In Appendix B.6, the learnability guarantee
follows from standard analysis analogous to that in [AAM22, DLS22, BES+22], where we construct
a “certificate” second-layer a∗∈RNthat achieves small loss and small norm:
Ex
f∗(x)−1
NPN
j=1a∗
jσj 
⟨wT1
j,x⟩+bj2
≤ε∗,∥a∗∥≲r∗,
from which the population loss of the regularized empirical risk minimizer can be bounded via stan-
dard Rademacher complexity argument. To construct such a certificate, we make use of the random
bias units {bj}N
j=1to approximate the link function σ∗as done in [DLS22, BBSS22, OSSW24a].
5 Beyond Polynomial Link Functions
Thus far we have shown that for polynomial single-index target functions (which satisfy p∗≤2),
SGD with data reuse can implement a polynomial transformation to the labels that reduces the
information exponent to at most 2; consequently, the trained two-layer neural network can achieve
small generalization error with n=dpolylog( d)samples. However, as shown in [DPVLB24], there
exists (non-polynomial) σ∗with generative exponent p∗>2(i.e., label transformations cannot lower
the information exponent to 2) and thus not learnable by SQ algorithms in linear sample complexity.
Nevertheless, for a single-index model with generative exponent p∗, we know there exists an “opti-
mal” label transformation that reduces the information exponent to p∗. If SGD can make use of such
transformation, then from the arguments in [BAGJ21], it is natural to conjecture that a sample size of
n≃dp∗−1is sufficient. In this section we confirm this intuition by proving that SGD with data reuse
(Algorithm 1) indeed matches this complexity. The following lemma is an analogue of Proposition 6
stating that polynomial transformations are sufficient to lower the information exponent.
Lemma 8. Given σ∗with generative exponent p∗∈N+. Suppose we can take an orthonormal
polynomial basis {ϕk}kfor the space L2(Py)with inner product ⟨f, g⟩=Ey=σ∗(z)[f(y)g(y)].
Then there exists some degree of transformation I∈N∗such that IE(σI
∗) =p∗.
We outline the differences and additional technical challenges to handle the GE(σ∗)>2setting.
• For general L2link functions σ∗, we can no longer make use of the compactness argument (see
proof of Proposition 6) to upper bound the degree of monomial transformation. Hence in Lemma 8
we do not state a uniform upper bound on the required degree I, unlike the polynomial setting.
• Any link function with p∗>2cannot be polynomial, and hence we cannot achieve low gener-
alization error using a neural network with polynomial nonlinearity. We therefore need to use an
activation function with universal function approximation ability.
95.1 Sample Complexity for Weak Recovery
We first show that Algorithm 1 achieves weak recovery (i.e., nontrivial overlap with the ground
truthθ) with a complexity governed by the generative exponent of the link function p∗= GE( σ∗).
Similar to Section 3.2, we make use of randomized activation functions to ensure the desired label
transformation is encoded — we defer the conditions on the student activation to Appendix B.1.2.
Proposition 9. Suppose the link function σ∗has generative exponent p∗, and let I∈N+be
the smallest degree of monomial transformation that lowers the information exponent to p∗(i.e.,
IE(σI
∗) =p). We can find a student activation function σdepending only on p, p∗andI, such that
if we take η2t, η2t+1=cηNd−1,ξ2(t+1)= 1−cξd−(p∗−2)+/2for small cη, cξ=od(1), and set
T1,1≃c−1
ξ

d (ifp∗= 1)
d(logd) (ifp∗= 2)
dp∗−1(ifp∗≥3),
then if the initial alignment ⟨w0,θ⟩ ≥2c−1
ηd−1/2, there exists τ∗≤T1,1such that for all τ≥τ∗,
⟨w2τ,θ⟩ ≥˜Θ(1),with probability 1−od(1).
Proposition 9 is a generalization of Theorem 1 beyond polynomial σ∗(the proof of both results are
presented in Appendix B.3,B.4), and can be interpreted as an SQ counterpart to [BAGJ21]: we es-
tablish a sufficient sample size of n≃d(p∗−1)∨1for Algorithm 1 to exit the search phase, which is
parallel to the n≃d(p−1)∨1rate for one-pass SGD (note that our rates are slightly sharper due to
logarithmic factors removed, since c−1
ξcan grow arbitrarily slowly with d). For high generative ex-
ponent σ∗withp∗>2, we no longer match the information theoretically optimal sample complexity
n≍d, which is consistent with the computational-to-statistical gap observed in [DPVLB24].
5.2 Generalization Error Guarantee
After Phase I of Algorithm 1, we learn the unknown link function σ∗via training the second-layer.
To approximate non-polynomial functions, we introduce a ReLU component in the student nonlin-
earity σ(see Lemma 12 for discussions), and make use of the approximation result for the (uni-
variate) ReLU kernel in [BBSS22], which handles general σ∗whose second derivative has bounded
4th moment. Combining the above, we arrive at the following end-to-end guarantee for learning
single-index models with arbitrary generative exponent using SGD training of neural network.
Proposition 10 (Informal) .Suppose the link function σ∗has generative exponent p∗∈N∗and
satisfies σ∗, σ′′
∗∈L4(γ). For appropriately chosen activation function σ(see Appendix B.1.2), a
neural network (3.1) withN=˜Θ(1) neurons optimized by Algorithm 1 achieves small population
lossEx[(fˆΘ(x)−f∗(x))2] =od,P(1)with a sample complexity of n=˜Θ(d(p∗−1)∨1).
See Appendix B.6 for the full statement with εdependence. This proposition confirms that the
sample complexity for weak recovery (Proposition 9) is the bottleneck in single-index learning, as
the total sample size required for Algorithm 1 to achieve low test error also scales with d(p∗−1)∨1.
6 Conclusion and Future Directions
We showed that a two-layer neural network (3.1) trained by SGD with reused batch can learn single-
index model (with generative exponent p∗) using n≃d(p∗−1)∨1samples and compute; in particular,
when the link function σ∗is polynomial, we established a sample complexity of n=˜O(dε−2)to
achieve εpopulation loss, which is almost information theoretically optimal. Our analysis is based
on the observation that by reusing the same training data twice in the gradient computation, a non-
correlational term arises in the SGD update that transforms the labels (despite the loss function not
modified). We proved that monomial transformations that lower the information exponent of σ∗
can be extracted by Taylor-expanding the SGD update; then we showed via careful analysis of the
trajectory that strong recovery and low population loss is achieved under suitable activation function.
Interesting future directions include extension to multi-index models [BAGJ22, BBPV23,
CWPPS23], hierarchical target functions [AZL19, NDL23], and in-context learning [OSSW24b].
Also, the SGD algorithm that we employ requires a layer-wise training procedure and a specific
batch reuse schedule; one may therefore ask if standard multi-pass SGD training of all parameters
simultaneously [Gla23] (as reported in Figure 1) also achieves the same statistical efficiency.
10Acknowledgements
The authors thank Gerard Ben Arous, Joan Bruna, Alex Damian, Marco Mondelli, and Eshaan
Nichani for the discussions and feedback on the manuscript. JDL acknowledges support of the ARO
under MURI Award W911NF-11-1-0304, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262,
ONR Young Investigator Award, and NSF CAREER Award 2144994. KO was partially supported
by JST ACT-X (JPMJAX23C4). TS was partially supported by JSPS KAKENHI (24K02905) and
JST CREST (JPMJCR2015). This research is unrelated to DW’s work at xAI.
References
[AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-
staircase property: a necessary and nearly sufficient condition for sgd learning of
sparse functions on two-layer neural networks. In Conference on Learning Theory ,
pages 4782–4887. PMLR, 2022.
[AAM23] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. SGD learning on
neural networks: leap complexity and saddle-to-saddle dynamics. In The Thirty Sixth
Annual Conference on Learning Theory , pages 2552–2623. PMLR, 2023.
[ADK+24] Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan.
Repetita iuvant: Data repetition allows sgd to learn high-dimensional multi-index
functions. arXiv preprint arXiv:2405.15459 , 2024.
[AZ18] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient
methods. Journal of Machine Learning Research , 18(221):1–51, 2018.
[AZL19] Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond
kernels? Advances in Neural Information Processing Systems , 32, 2019.
[Bac17] Francis Bach. Breaking the curse of dimensionality with convex neural networks.
The Journal of Machine Learning Research , 18(1):629–681, 2017.
[BAGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradi-
ent descent on non-convex losses from high-dimensional inference. The Journal of
Machine Learning Research , 22(1):4788–4838, 2021.
[BAGJ22] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit
theorems for sgd: Effective dynamics and critical scaling. Advances in Neural Infor-
mation Processing Systems , 35:25349–25362, 2022.
[BBPV23] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning Gaussian multi-
index models with gradient flow. arXiv preprint arXiv:2310.19793 , 2023.
[BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-
index models with shallow neural networks. Advances in Neural Information Pro-
cessing Systems , 35:9768–9783, 2022.
[BES+22] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg
Yang. High-dimensional asymptotics of feature learning: How one gradient step
improves the representation. Advances in Neural Information Processing Systems ,
35:37932–37946, 2022.
[BES+23] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Learning
in the presence of low-dimensional structure: A spiked random matrix perspective.
InThirty-seventh Conference on Neural Information Processing Systems , 2023.
[BF02] Nader H Bshouty and Vitaly Feldman. On using extended statistical queries to avoid
membership queries. Journal of Machine Learning Research , 2(Feb):359–395, 2002.
[BKM+19] Jean Barbier, Florent Krzakala, Nicolas Macris, L ´eo Miolane, and Lenka Zdeborov ´a.
Optimal errors and phase transitions in high-dimensional generalized linear models.
Proceedings of the National Academy of Sciences , 116(12):5451–5460, 2019.
11[BL20] Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order ap-
proximation of wide neural networks. In International Conference on Learning Rep-
resentations , 2020.
[BMZ23] Rapha ¨el Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in
two-layers neural networks. arXiv preprint arXiv:2303.00055 , 2023.
[CCM11] Seok-Ho Chang, Pamela C Cosman, and Laurence B Milstein. Chernoff-type
bounds for the Gaussian error function. IEEE Transactions on Communications ,
59(11):2939–2944, 2011.
[CM20] Sitan Chen and Raghu Meka. Learning polynomials in few relevant dimensions. In
Conference on Learning Theory , pages 1161–1227. PMLR, 2020.
[COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable
programming. Advances in Neural Information Processing Systems , 32, 2019.
[CWPPS23] Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, and Inbar Seroussi.
Hitting the high-dimensional notes: An ode for sgd learning dynamics on glms and
multi-index models. arXiv preprint arXiv:2308.08977 , 2023.
[DH18] Rishabh Dudeja and Daniel Hsu. Learning single-index models in gaussian space. In
Conference On Learning Theory , pages 1887–1930. PMLR, 2018.
[DH24] Rishabh Dudeja and Daniel Hsu. Statistical-computational trade-offs in tensor pca
and related problems via communication complexity. The Annals of Statistics ,
52(1):131–156, 2024.
[DKL+23] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan.
Learning two-layer neural networks, one (giant) step at a time. arXiv preprint
arXiv:2305.18270 , 2023.
[DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn
representations with gradient descent. In Conference on Learning Theory , pages
5413–5452. PMLR, 2022.
[DNGL23] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D. Lee. Smoothing the landscape
boosts the signal for SGD: Optimal sample complexity for learning single index mod-
els. In Thirty-seventh Conference on Neural Information Processing Systems , 2023.
[DPVLB24] Alex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna. The com-
putational complexity of learning gaussian single-index models. arXiv preprint
arXiv:2403.05529 , 2024.
[DTA+24] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborov ´a, and
Florent Krzakala. The benefits of reusing batches for gradient descent in two-layer
networks: Breaking the curse of information and leap exponents. arXiv preprint
arXiv:2402.03220 , 2024.
[DWY21] Konstantin Donhauser, Mingqi Wu, and Fanny Yang. How rotational invariance of
common kernels prevents generalization in high dimensions. In International Con-
ference on Machine Learning , pages 2804–2814. PMLR, 2021.
[Gla23] Margalit Glasgow. Sgd finds then tunes features in two-layer neural networks with
near-optimal sample complexity: A case study in the xor problem. arXiv preprint
arXiv:2309.15111 , 2023.
[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Lin-
earized two-layers neural networks in high dimension. The Annals of Statistics ,
49(2):1029–1054, 2021.
[HSSVG21] Daniel Hsu, Clayton H Sanford, Rocco Servedio, and Emmanouil Vasileios Vlatakis-
Gkaragkounis. On the approximation power of two-layer networks of random relus.
InConference on Learning Theory , pages 2423–2461. PMLR, 2021.
12[JGH18] Arthur Jacot, Franck Gabriel, and Cl ´ement Hongler. Neural tangent kernel: Con-
vergence and generalization in neural networks. In Advances in neural information
processing systems , pages 8571–8580, 2018.
[Kea98] Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of
the ACM (JACM) , 45(6):983–1006, 1998.
[KMS20] Pritish Kamath, Omar Montasser, and Nathan Srebro. Approximate is good enough:
Probabilistic variants of dimensional and margin complexity. In Conference on
Learning Theory , pages 2236–2262. PMLR, 2020.
[MHPG+23] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Mu-
rat A Erdogdu. Neural networks efficiently learn low-dimensional representations
with SGD. In The Eleventh International Conference on Learning Representations ,
2023.
[MHWSE23] Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, and Murat A. Erdogdu.
Gradient-based feature learning under structured data. In Thirty-seventh Conference
on Neural Information Processing Systems (NeurIPS 2023) , 2023.
[MM18] Marco Mondelli and Andrea Montanari. Fundamental limits of weak recovery with
applications to phase retrieval. In Conference On Learning Theory , pages 1445–1450.
PMLR, 2018.
[MZD+23] Arvind Mahankali, Haochen Zhang, Kefan Dong, Margalit Glasgow, and Tengyu Ma.
Beyond ntk with vanilla gradient descent: A mean-field analysis of neural networks
with polynomial width, samples, and time. Advances in Neural Information Process-
ing Systems , 36, 2023.
[NDL23] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlin-
ear feature learning in three-layer neural networks. Advances in Neural Information
Processing Systems , 36, 2023.
[O’D14] Ryan O’Donnell. Analysis of Boolean Functions . Cambridge University Press, 2014.
[OSSW24a] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Learning sum of diverse
features: computational hardness and efficient gradient-based training for ridge com-
binations. In Conference on Learning Theory . PMLR, 2024.
[OSSW24b] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Pretrained trans-
former efficiently learns low-dimensional target functions in-context. arXiv preprint
arXiv:2411.02544 , 2024.
[Rey20] Lev Reyzin. Statistical queries and statistical algorithms: Foundations and applica-
tions. arXiv preprint arXiv:2004.00557 , 2020.
[Sch80] Jacob T Schwartz. Fast probabilistic algorithms for verification of polynomial iden-
tities. Journal of the ACM (JACM) , 27(4):701–717, 1980.
[TS24] Shokichi Takakura and Taiji Suzuki. Mean-field analysis on two-layer neural net-
works from a kernel perspective. arXiv preprint arXiv:2403.14917 , 2024.
[WWF24] Zhichao Wang, Denny Wu, and Zhou Fan. Nonlinear spiked covariance matrices
and signal propagation in deep neural networks. In Conference on Learning Theory .
PMLR, 2024.
[YS19] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for
understanding neural networks. Advances in Neural Information Processing Systems ,
32, 2019.
[ZLBH19] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead op-
timizer: k steps forward, 1 step back. Advances in neural information processing
systems , 32, 2019.
13Table of Contents
1 Introduction 1
1.1 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2 Problem Setting and Prior Works 3
2.1 Complexity of Learning Single-index Models . . . . . . . . . . . . . . . . . . . . 3
2.2 Can Gradient Descent Go Beyond Correlational Queries? . . . . . . . . . . . . . . 4
2.3 Challenges in Establishing Statistical Guarantees . . . . . . . . . . . . . . . . . . 5
3 Learning Polynomial f∗in Linear Sample Complexity 5
3.1 Training Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Convergence and Sample Complexity . . . . . . . . . . . . . . . . . . . . . . . . 6
4 Proof Sketch 8
4.1 Monomial Transformation Reduces Information Exponent . . . . . . . . . . . . . 8
4.2 SGD with Batch Reuse Implements Polynomial Transformation . . . . . . . . . . 8
4.3 Analysis of Phase II and Statistical Guarantees . . . . . . . . . . . . . . . . . . . 9
5 Beyond Polynomial Link Functions 9
5.1 Sample Complexity for Weak Recovery . . . . . . . . . . . . . . . . . . . . . . . 10
5.2 Generalization Error Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
6 Conclusion and Future Directions 10
A Polynomial Transformation 15
A.1 Proof for Even Functions (i). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.2 Proof for Non-even Functions (ii). . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Proof for Non-Polynomial Functions . . . . . . . . . . . . . . . . . . . . . . . . . 17
B SGD with Reused Batch 17
B.1 Assumptions on Link Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 Weak Recovery: Population Update . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.4 Weak Recovery: Stochastic Update . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.5 From Weak Recovery to Strong Recovery . . . . . . . . . . . . . . . . . . . . . . 29
B.6 Second Layer Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
14A Polynomial Transformation
Proof of Proposition 6. We use a thresholding and compactness argument inspired by [CM20].
A.1 Proof for Even Functions (i)
We divide the analysis into the following steps.
(i-1): Monomials reducing the information exponent. Define τ(f) = max −2≤t≤2|f(t)|. This
entails that if |f(t)| ≥τ(f), then we have |t|>2.
Consider the following expectation:
Et∼N(0,1)f(t)
2τ(f)i
(t2−1)
. (A.1)
We evaluate the case when iis even. (A.1) can be lower bounded as
(A.1) =Et∼N(0,1)
1[|f(t)| ≥2τ(f)]f(t)
2τ(f)i
(t2−1)
+Et∼N(0,1)
1[τ(f)≤ |f(t)|<2τ(f)]f(t)
2τ(f)i
(t2−1)
+Et∼N(0,1)
1[|f(t)|< τ(f)]f(t)
2τ(f)i
(t2−1)
≥Et∼N(0,1)
1[|f(t)| ≥2τ(f)]2τ(f)
2τ(f)i
(22−1)
+Et∼N(0,1)
1[τ(f)≤ |f(t)|<2τ(f)]f(t)
2τ(f)i
(22−1)
+Et∼N(0,1)
1[|f(t)|< τ(f)]τ(f)
2τ(f)i
(02−1)
≥3Pt∼N(0,1)[|f(t)| ≥2τ(f)]−2−i.
Note that P[|f(t)| ≥2τ(f)]is positive (since fis polynomial) and independent of i, while 2−i
decays to 0asiincreases. Therefore, for sufficiently large i∈N, (A.1) is positive and hence
IE(fi)≤2. The subsequent analysis aims to provide an upper bound on i.
(i-2): Construction of test function. We introduce the notation H(·;j)which takes any function
(inL1) and returns its j-th Hermite coefficient. We consider the following test function:
H(f) :=∞X
i=2H(fi; 2)
2i
2(2i−1)iq
22
. (A.2)
(i-3): Lower bound of test function via compactness. LetFqbe a set of polynomials with degree
up to qwith unit L2norm. Because H(f)is positive for any f∈ Fq,H(fi; 2)is continuous with
respect to f, andFqis a compact set, inff∈FqH(f)admits a minimum value H0which is positive.
(i-4): Conclusion via hypercontractivity. Because fis a polynomial with degree at most q, Gaus-
sian hypercontractivity [O’D14] yields that
2H(fi; 2)2≤Et∼N(0,1)
(f(t))2i
≤(2i−1)iq 
Et∼N(0,1)
f(t)2i= (2i−1)iq.
Therefore, for all polynomials in Fq, a partial sum of (A.2) is uniformly bounded by
∞X
i=jH(fi; 2)
2i
2(2i−1)iq
22≤∞X
i=j2−i−1= 2−j→0 (j→ ∞ ).
Combining this with the fact that H(f)≥H0>0, we know that there exists some Cq≤1 +
log2(H−1
0)such that
CqX
i=2H(fi; 2)
2i
2(2i−1)iq
22
>1
2H0>0,
for all polynomials in Fq. This means that there is at least one i≤Cqsuch that H(fi; 2)̸= 0.
15A.2 Proof for Non-even Functions (ii)
(ii-1): Monomials reducing the information exponent. We prove that some exponentiation of
g:=f2has non-zero first Hermite coefficient. Denote goddas the odd part of g, and similarly
geven. Letυ(g)∈R+be the value at which the followings hold:
(a)godd(t)>0for all t≥υ(g)andgodd(t)<0for all t≤ −υ(g).
(b)geven(t)>|godd(t)|for all t≥υ(g)andt≤ −υ(g).
(c) For for all t≥υ(g)andt≤ −υ(g),g(s) =g(t)(as an equation of s) only has two real-valued
solutions with opposing signs.
Such threshold υ(g)exists because the tail of g=f2is dominated by the highest degree which is
even. Then, we let τ(g) = max −υ(g)≤t≤υ(g)|g(t)|.
Consider the following expectation:
Et∼N(0,1)g(t)
2τ(g)i
t
. (A.3)
(A.3) is decomposed as
(A.3) =Et∼N(0,1)
1[|g(t)| ≥3τ(f)]g(t)
3τ(g)i
t
+Et∼N(0,1)
1[2τ(g)≤ |g(t)|<3τ(g)]g(t)
3τ(f)i
t
+Et∼N(0,1)
1[|g(t)|<2τ(g)]g(t)
3τ(g)i
t
. (A.4)
We first evaluate the first term. Because of (c), g(t) = 3 τ(f)has two real-valued solutions α <
0< β. Because of (a) and (b), g(β) =geven(β) +godd(β) = 3 τ(f)> geven(−β) +godd(−β) =
godd(−β). Because limt→−∞ godd(t) = +∞, andαis the only solution in t <0, we have α <−β.
Moreover, for all t > β , we have g(t) =geven(t) +godd(t)> geven(−t) +godd(−t) =godd(−t).
Combining the above, the first term of (A.4) is bounded as
Et∼N(0,1)
1[|g(t)| ≥3τ(f)]g(t)
3τ(g)i
t
=Et∼N(0,1)
1[β≤t≤ −α]g(t)
3τ(g)i
t
+Et∼N(0,1)
1[t≥ −α]g(t)
3τ(g)i
t
+Et∼N(0,1)
1[t≤α]g(t)
3τ(g)i
t
=Et∼N(0,1)
1[β≤t≤ −α]t
+Et∼N(0,1)
1[t≥ −α]g(t)
3τ(g)i
−g(−t)
3τ(g)i
t
> βPt∼N(0,1)
β≤t≤ −α
.
Following the exact same reasoning, we know that the second term of (A.4) is positive. Finally, the
third term which is bounded by
Et∼N(0,1)
1[|g(t)|<2τ(g)]g(t)
3τ(g)i
t
≥ −Et∼N(0,1)
1[|g(t)|<2τ(g)]|t|2
3i
.
Putting things together,
(A.4) > βPt∼N(0,1)
β≤t≤ −α
−Et∼N(0,1)
1[|g(t)|<2τ(g)]|t|2
3i
.
The first term is independent of iand positive, while the second term goes to zero as igrows.
Therefore, there exists some isuch that IE(gi; 1) = 1 .
16(ii-2): Construction of test function. This time we consider the following function:
H(f) :=∞X
i=2H(fi; 1)
2i
2(2i−1)iq
22
.
(ii-3): Lower bound of test function via compactness. LetFqbe a set of unit L2-norm polynomi-
als with degree up to qandEt∼N(0,1)[fodd(t)2]≥c. SinceH(f)is always positive for Fq,H(f)
is continuous with respect to f, andFqis a compact set, inff∈FqH(f)has the minimum value H0
that is positive. Note that H(f)might depends on c.
(ii-4): Conclusion via hypercontractivity. Using the same argument as in (i), we conclude that
there exists some Cq,csuch that
CqX
i=2H(fi; 1)
2i(2i−1)iq
22
>1
2H0>0.
BecauseH0depends on c,Cq,cdepends on cas well as q.
A.3 Proof for Non-Polynomial Functions
For non-polynomial link functions, we note that similar to [DPVLB24], the existence of polynomial
basis is needed to exclude extreme cases, and we cannot upper bound the required degree Ibecause
general link functions are not included in a compact space.
Proof of Lemma 8. The derivation is analogous to [DPVLB24, Lemma F.14]. Let z∼ N(0,1)
andy=σ∗(z). We define ζp∗(y) =E[1√p∗!Hep∗(z)|y]and its basis expansion ζp∗(y) =P∞
k=0vkϕk. Let Kbe a smallest integer such that vk̸= 0. Then, there exists an integer with
I≤Ksuch that IE(yI) =p∗. Indeed,
E[ϕK(y)Hep∗(z)] =Ey[ΦK(y)Ez|y[Hep∗(z)|y]]
=Ey
ΦK(y)KX
k=0vkϕk(y)
=vK̸= 0,
which means that at least one of y, y2,···, yKyields a non-zero p∗-th Hermite coefficient.
B SGD with Reused Batch
In this section we show that Algorithm 1 learns single-index models in ˜O(d1∨(p∗−1))samples with
high probability. The algorithm trains the first layer for T1SGD steps, where we sample a new data
point in every two steps. The first layer training is further divided into two phases: weak recovery
(w⊤θ≳1) and strong recovery ( ∥w−θ∥≲ε). Then, we learn the second layer parameters.
Specifically, Section B.2 shows that at initialization, a (nearly) constant fraction of neurons has
alignment w⊤θbeyond a certain threshold. We focus on such neurons in the first phase of train-
ing. Section B.3 lower bounds the expected update of alignment w⊤θof two gradient steps, and
Section B.4 establishes that the neurons achieve weak recovery within 2T1,1=˜O(d1∨(p∗−1))steps.
Section B.5 discusses how to convert weak recovery to strong recovery using 2T1,2=˜O(dε−2)
more steps. We let T1= 2T1,1+ 2T1,2. Finally, Section B.6 analyzes second layer training and
concludes the proof.
In the following proofs, we use several constants, which depends on dat most at most polylogarith-
mically. Specifically, asymptotic strength of the constants is ordered as follows.
1≃cσ≃C1≲

c−1
η≃C2≲poly( c−1
η)≲
c−1
1≃C3
c−1
2
δ−1

≲δ−1poly( c−1
η)≲c−1
ξ
poly( c−1
1)≲¯c−1
η
≲polylog( d) =C4.
17Here, cηandδshould satisfy limd→∞cη= lim d→∞δ= 0, but the convergence can be arbitrarily
slow, (e.g., as slow as 1/log log log ···logd). This requirement comes from the fact that we do
not know the exact value of H(σI
∗;p∗). To ensure that one signal term (from the Taylor series)
is isolated, taking η≍d−1with a sufficiently small constant is insufficient but η≍cηd−1with
arbitrarily slow cηsuffices. Also, to guarantee that the failure probability is od(1), we require δto be
od(1).cξcan also decay arbitrarily slowly, as long as it satisfies cξ≲δpoly( c−1
η).C4= polylog( d)
will be used to represent any polylogarithmic factor that comes from high probability bounds.
For the first-layer training, we can reduce the argument into training of one neuron using the corre-
lation loss as follows. At each step, the gradient update (Line 8 of Algorithm 1) is written as
wt+1
j←wt
j−ηt˜∇w 
(fΘ(x)−y)2
=wt
j−ηt˜∇w1
NNX
j=1ajσj(wt
j⊤x)2
+ 2ηt
j˜∇w
y1
NNX
j=1ajσj(wt
j⊤x)
=wt
j−2ηtc2
a
N1
NNX
j=1σj(wt
j⊤x) ˜∇wσj(wt
j⊤x)
+2ηtca
Ny ˜∇wσj(wt
j⊤x)
.(B.1)
While the second term scales with ηtc2
aN−1, the third term scales with ηtcaN−1. Thus, by setting
casufficiently small, we can ignore the interaction between neurons. We will show that the strength
of the signal in the direction of θis at least (κt
j)p∗−1≳d−p∗−1
2(up to a polylogarithmic factor, and
p∗= GE( σ∗)). On the other hand, we can easily see that θ⊤ 1
NPN
j=1σj(wt
j⊤x) ˜∇wσj(wt
j⊤x)
is bounded by ˜O(1)with high probability. Therefore, by simply letting ca=˜Θ(d−p∗−1
2), we can
ignore the effect of the second term in (B.1). Moreover, for simplicity, we will reparameterize2ηtca
N
asηtbelow. Consequently, we may analyze the following update
wt+1
j←wt
j+ηt˜∇w 
yσj(wt
j⊤x)
,
instead of Line 8 of Algorithm 1. Since there is no interaction between neurons now, we omit the
subscript jwhen the context is clear.
B.1 Assumptions on Link Function
The analysis consists of three different phases: weak recovery and strong recovery of the first-layer
weights, and approximation of the link function (ridge regression of the second-layer). Each phase
requires different assumptions on the activation functions, depending on the link function. Before
starting the analysis, we decompose Assumptions 2 and 3 and clarify which conditions are needed
in each phase. We prove that instead of using a specific activation function tailored to different link
functions, a randomized activation function satisfies all required assumptions with probability Ω(1) .
In the following, we write the student activation function as
σj(s) :=∞X
i=0βj,iHei(s)
with coefficients {βj,i}Cσ
i=0(sometimes the subscript j, which is the index of the neurons, is omitted).
B.1.1 For polynomial link functions
In the following, we summarize the precise conditions to be satisfied by the activation functions
(these conditions are weaker than Assumptions 2 and 3). For polynomial link functions, we focus on
polynomial activation functions (with bounded degree) for simplicity, but non-polynomial activation
functions would not change the proof significantly.
Letpandqbe the minimum and maximum degree of non-zero Hermite coefficients of σ∗. Note that
GE(σ∗) = 1 or2holds (see Proposition 6). Let I≤Cq(according to Proposition 6) be the smallest
integer such that IE(σI
∗) = GE( σ∗) =p∗andCσbe the degree of the activation function.
(I) If I= 1⇔IE(σ∗) = GE( σ∗) =p∗.
Weak recovery: αp∗βp∗>0(covered by Assumption 3).
18Strong recovery:Pq
j=p∗j!αjβjsj−1>0for all s >0(covered by Assumption 2).
Approximation (ridge regression): βi̸= 0for some i≥q(covered by Assumption 3).
(II) If 2≤I={mini|IE(σI
∗) = GE( σ∗) =p∗} ≤Cσ.
Weak recovery: H((σ∗)I;p∗)H(σ(I)(σ(1))I−1;p∗−1)>0(covered by Assumption 3).
Strong recovery:Pq
j=p∗+1j!αjβjsj−1>0for all s >0(covered by Assumption 2).
Approximation: βi̸= 0for some i≥q(covered by Assumption 3).
Note that it is difficult to construct a deterministic activation function that satisfies all of the as-
sumptions for any link function σ∗(the simplest counterexample is to consider −σ∗which flips
the Hermite coefficients). Instead, we show the existence of randomized construction of such an
activation function that satisfies all of the assumptions on the activation function simultaneously
with constant probability, which entails that a subset of neurons can achieve strong recovery. The
construction does not depend on properties of the link function itself except for its degree q.
Lemma 11. There exists a randomized activation function sampled from a discrete set such that the
above conditions hold with constant probability.
Proof. Letcbe a sufficiently small constant only used in this proof and Cσbe the minimum odd
integer with Cσ≥max{Cq+ 1, q+ 2,3}, where Cqwas introduced in Proposition 6. With proba-
bility1
2, we let β1∼Unif({±1}), and βj∼Unif({±c})for2≤j≤Cσ. With probability1
2, we
letβj∼Unif({±c})for1≤j≤Cσ−2andβCσ−1=βCσ∼Unif({±1}).
We first consider (I). When β1∼Unif({±1}), and βj∼Unif({±c})for2≤j≤Cσ, it is easy to
seesign(αj) = sign( βj)for all j= 1,···, qhold with probability at least 2−q, which is sufficient
to satisfy (I).
We then consider (II). First focus on the case when p∗= 1andIis even. When β1∼Unif({±1})
andβj∼Unif({±c})for2≤j≤Cσ, by taking csufficiently small, we have
H(σ(I)(σ(1))I−1; 0) = I!βI(β1)I−1
|{z}
≍c+O(c2). (B.2)
When Iis even, by adjusting the sign of β1,H(σ(I)(σ(1))I−1; 0)is non-zero and has the same
sign as H((σ∗)I; 1)with probability1
2. Note that the sign of β1is independent from whetherPq
j=2j!αjβjsj−1>0for all s > 0holds. This holds with probability at least 2−q+1. Thus
we verified (II) for p∗= 1and even I.
Forp∗= 1 and odd I, consider βj∼Unif({±c})for1≤j≤Cσ−2andβCσ−1=βCσ∼
Unif({±1}). Note thatPq
j=2j!αjβjsj−1>0for all s > 0(this is the condition for strong
recovery) and the condition for ridge regression also holds. Furthermore, the sign of H((HeCσ+
HeCσ−1)(I)((HeCσ+HeCσ−1)(1))I−1; 0)is±1with equiprobability, independent of β2, . . . , β q.
Therefore, by taking csufficiently small, we can obtain the desired sign of H(σ(I)(σ(1))I−1; 0).
Thus we proved (II) for p∗= 1and odd I.
Regarding (II) for p∗= 2and even I, when β1∼Unif({±1})andβj∼Unif({±c})for2≤j≤
Cσ, we have
H(σ(I)(σ(1))I−1; 1) = ( I+ 1)!βI+1(β1)I−1
| {z }
≍c+O(c2).
Thus, similar to (II) with p∗= 1and even I, we get (II) for p∗= 2and even I.
Finally, consider (II) for p∗= 2 and odd I. When βj∼Unif({±c})for1≤j≤Cσ−2and
βCσ−1=βCσ∼Unif({±1}), the sign of H((HeCσ+HeCσ−1)(I)((HeCσ+HeCσ−1)(1))I−1; 1)is
±1with equiprobability when Iis odd, and this term dominates the others in H(σ(I)(σ(1))I−1; 1).
Thus, (II) for p∗= 2and odd Iholds similarly to (II) for p∗= 1and odd I.
Now we have obtained the assertion for all cases.
B.1.2 For general link functions
Now we consider non-polynomial link functions with potentially large generative exponent p∗=
GE(σ∗)≥2. For weak and strong recovery to succeed, the conditions on the activation function are
essentially the same as those for polynomial link functions:
19(I) If I= 1⇔IE(σ∗) = GE( σ∗) =p∗.
Weak recovery: αp∗βp∗>0.
Strong recovery:P∞
j=p∗j!αjβjsj−1>0for all s >0,
(II) If 2≤I={mini|IE(σI
∗) = GE( σ∗) =p∗} ≤Cσ.
Weak recovery: H((σ∗)I;p∗)H(σ(I)(σ(1))I−1;p∗−1)>0,
Strong recovery:P∞
j=p∗+1j!αjβjsj−1>0for all s >0.
Due to the proof strategy (which uses Taylor expansion), we also require that all differentials and
sum of expectations appearing in the following proofs are well-defined and bounded.
To approximate a non-polynomial σ∗, we introduce the following condition on the activation func-
tion. We sample σjfrom a discrete set (with bounded cardinality). Let Jbe an index set such
that the coefficients of σj(j∈J)satisfy the conditions above. Because we are selecting σjfrom
a discrete set, |J| ≃Nholds. We introduce the following condition, which states that the target
single-index model can be well-approximated by a linear combination of student neurons.
Assumption 4. When bj∼Unif([−Cb, Cb])where (Cb= polylog( d))andx1, . . . ,xT2∼
N(0,Id), there exists a set of coefficients a1, . . . , a |J|such that
1
T2T2X
i=11
|J|X
j∈Jajσj(θ⊤
jxi+bj)−σ∗(θ⊤xi)2
≲ε2,
holds with coefficients of reasonable magnitudesP
j∈Ja2
j= Θ(|J|)with high probability (w.r.t.
the randomness of bjandxi). Moreover, Ex[σj(θ⊤
jx+bj)4]≤polylog( d)for all jwith high
probability (w.r.t. the randomness of bj).
The following lemma states that we can design a randomized activation function that satisfies all of
the above assumptions with probability Ω(1) , as long as the link function σsatisfies Assumption 4
forσ= ReLU . In other words, we are able to cover the class of link functions σthat can be
efficiently approximated by a two-layer ReLU network. Since the general link functions are not
included in a compact space, we do not have an upper bound of exponent to obtain IE(σI
∗) =
GE(σ∗)as we had Cqin the polynomial case. Consequently, our student activation is not entirely
agnostic to the link function σ∗, as we require knowledge of p(information exponent), p∗andI.
Lemma 12. Suppose the target link function σ∗satisfies Assumption 4 for σj= ReLU . There
exists a randomized activation sampled from a discrete set such that the above conditions hold with
constant probability.
Before we sketch the design of activation function, we present the following approximation result
from [BBSS22], which establishes that Assumption 4 with σj= ReLU is satisfied for broad class of
functions, according to Lemma 4.4 and 4.5 of [BBSS22]. Specifically, taking τ= 1/2andλ=N−1
yields that Ex[(1
|J|P
j∈Jajσj(θ⊤x+bj)−σ∗(θ⊤x))2]≤N−2
7. Although they sample bjfrom
Gaussian N(0,2), the result translates to uniform sampling of biases from [−Cb, Cb]by introducing
additional logarithmic factor.
Lemma 13 (Lemma 4.4, 4.5 of [BBSS22]) .Suppose that Ez∼N(0,1)[σ∗(z)4],Ez∼N(0,1)[σ′′
∗(z)4]<
∞. Then, Assumption 4 with σj= ReLU holds with ε=N−1
7andCb≃√logd.
Proof of Lemma 12. We show the existence of suitable σin two steps: first we construct a
randomized polynomial activation function that satisfies conditions (I)(II) with constant probability;
then we add a small ReLU perturbation so that the activation can approximate non-polynomial σ∗.
Recall p∈N+is the information exponent of σ∗. We first show that there exists a randomized
polynomial activation that satisfies the conditions for weak and strong recovery with probability
Ω(1) . Note that the issue of differentiability and bounded moment is avoided when we focus on the
polynomial activation functions. We specify the following two distributions. With probability1
2,
letβ1∼Unif({−1,1}),βj∼Unif({−c, c})forj= 1,···, p∗+I−1andβj= 0 otherwise,
where c >0is a sufficiently small constant. With probability1
2, letβ1= Unif( {−1,1}),β2=
Unif({−c, c}),βj= Unif( {−c2, c2})for all 2≤j≤(p∗+I)∨pfor a sufficiently small constant
c >0, and βj= 0otherwise.
20Regarding (I), consider the case when the coefficients are sampled from the first distribution, and
|βj| ≪1except for j=p∗. Then,P∞
j=p∗j!αjβjsj−1≈p∗!αp∗βp∗sp∗. Choosing the sign of βp∗,
we have that the assumption holds with probability Ω(1) .
Regarding (II) with even I, consider coefficients sampled from the first distribution, and Sign( βj) =
Sign( αj)forj≤(p∗+I−1)∨p. Then,P∞
j=p∗+1j!αjβjsj−1>0for all s >0. Also, similarly
to (B.2),
H(σ(I)(σ(1))I−1;p∗−1) = i!βI+p∗−1(β1)I−1
| {z }
≍c+O(c2).
By flipping the sign of β1, we can change the sign of H(σ(I)(σ(1))I−1;p∗−1). Thus, (II) for even
Iis satisfied by a randomized choice of β1.
For (II) with odd I, consider coefficients sampled from the second distribution, and Sign( βj) =
Sign( αj)forj≤(p∗+I)∨p. Then,P∞
j=p∗+1j!αjβjsj−1>0for all s >0.
H(σ(I)(σ(1))I−1;p∗−1) =1
(p∗−1)!E[σ(I)(σ(1))I−1Hep∗−1]
=1
(p∗−1)!E[(I−1)(βp∗+IHep∗+I)(I)(β2He2)(1)(β1)I−2Hep∗−1] +O(c4).
=2(I−1)βp∗+Iβ2(β1)I−2(p∗+I)!
(p∗−1)!| {z }
≍c3+O(c4).
By flipping the sign of β1, we can change the sign of H(σ(I)(σ(1))I−1;p∗−1). Thus, (II) for odd
Iis satisfied by a randomized choice of β1.
Therefore, we have constructed a randomized polynomial activation σthat satisfies all of the con-
ditions for weak and strong recovery. Now we provide a sketch of reasoning that when the link
function σ∗is well-approximated by ReLU as specified in Assumption 4, we can find some σthat
additionally satisfies Assumption 4 by introducing a small ReLU component. Specifically, we add
cR·ReLU to the activation function with probability1
2, with a sufficiently small cR=˜Ω(1) , e.g.,
cR= (logd)−Cfor some C > 0. When a two-layer ReLU network approximates σ∗that satisfies
Assumption 4, by using the neurons with added ReLU component, σ∗can be approximated up to
some polynomial residual with degree (p∗+I)∨p. And by using the remaining polynomial neu-
rons, we can approximate the additional polynomial terms in σ∗(see Lemma 22,23). Subtracting the
latter from the former, we obtain the desired approximation result. When cRis sufficiently small,
this additional term does not impact the conditions for weak and strong recovery and the moment
calculations; similarly, since cR≪1we may discard this non-smooth term before Taylor expansion
without affecting the analysis of optimization dynamics. We remark that to avoid such unnatural de-
sign of activation function, we can also train the first-layer parameters using a polynomial activation
specified above, and then perturb it before the second-layer training to enhance the approximation
ability — such strategy has also been employed in prior layer-wise training analysis [AAM22].
B.1.3 More Discussion on Assumption 2
Assumption 2 requires H(σ(I)(σ(1))I−1;p∗−1)is not zero and has the same sign as H(σI
∗;p∗).
We remark that if we allow a negative momentum parameter larger than 1, i.e., setting ξ2(t+1)=
1+cξd−(p∗−2)+
2, we can negate the opposite sign of H(σ(I)(σ(1))I−1;p∗−1)(see Lemma 16), and
the subsequent analysis still holds. Therefore, what we essentially need is H(σ(i)(σ(1))i−1;k)̸= 0.
Lemma 3 confirms that it is satisfied by almost all polynomials:
Proof of Lemma 3. We note that H(σ(i)(σ(1))i−1;k) =E[σ(i)(σ(1))i−1Hek]is a polynomial of
{βj}Cσ
j=0. This polynomial is not identically equal to zero. To confirm this, consider σ=xCσ+
xCσ−1. Because σ(i)(σ(1))i−1is expanded as a sum of xl(i(Cσ−3)≤l≤i(Cσ−2) + 1 with
positive coefficients and each xlis a sum of Hel,Hel−2···with positive coefficients, σ(i)(σ(1))i−1
has all positive Hermite coefficients for degree 0,1,···, i(Cσ−2) + 1 . Ifk≤i(Cσ−2) + 1 ,
21this choice of σyields H(σ(i)(σ(1))i−1;k)>0, which confirms that H(σ(i)(σ(1))i−1;k)as a
polynomial of {βj}Cσ
j=0is not identically equal to zero. Hence the assertion follows from so-called
Schwartz–Zippel Lemma [Sch80], or the fact that zeros of a non-zero polynomial form a measure-
zero set.
B.2 Initialization
We first consider the initial alignment. In the following sections, we focus on the neurons that
satisfy κ0
j=θ⊤w0
j≥2c−1
ηd−1
2at the initialization. The following lemma states that roughly a
constant portion of the neurons satisfy the initial alignment condition upon random initialization. In
particular, if we take cη= Ω((log log d)−1
2), the fraction of neurons that satisfy the initial alignment
condition is at least e−16c−2
η=˜Ω(1) . Let us write C2=c−1
ηfor simplicity in the following.
Lemma 14. At the time of initialization, κ0
j=θ⊤w0satisfies the following:
P[κ0
j≥2C2d−1
2] =P[κ0
j≤ −2C2d−1
2]≳e−16C2
2=˜Ω(1).
We make use of the following lemma.
Lemma 15 (Theorem 2 of [CCM11]) .For any β >1ands∈R, we have
p
2e(β−1)
2β√πe−βs2
2≤Z∞
s1√
2πe−t2
2dt
Proof of Lemma 14. Because κ0=v⊤wd=e⊤
1g
∥g∥, where g∼ N(0,Id),
P[κ0
j≥2C2d−1
2] =Pg∼N(0,Id)
e⊤
1g≥4C2∧ ∥g∥ ≤2d1
2
≥Pg∼N(0,Id)
e⊤
1g≥4C2
−Pg∼N(0,Id)
∥g∥ ≥2d1
2
≳p
2e(β−1)
2β√πe−8βC2
2−e−Ω(d),
where we used Lemma 15 for the final inequality. By letting β= 2, we have that P[κ0
j≥C2d−1
2]≳
e−16C2
2.Because of the symmetry, P[κ0
j≤2C2d−1
2] =P[κ0
j≥2C2d−1
2].
B.3 Weak Recovery: Population Update
We divide the first layer training into the first phase (weak recovery) and the second phase (strong
recovery). We first evaluate the expected update of two gradient steps with the same training exam-
ple.
Lemma 16. Letη2t, η2t+1=η=cηd−1,ξ2(t+1)=ξ= 1−cξd−(p∗−2)+
2. Suppose that the link
function satisfies IE(σI
∗) = GE( σ∗) =p∗(we choose the smallest such I) and activation functions
satisfy all of the assumptions in Section B.1 for weak recovery. Then, for w2twithc−1
ηd−1
2≤
θ⊤w2t≤cI
η, the alignment θ⊤w2(t+1)can be evaluated as,
θ⊤w2(t+1)≥θ⊤w2t+cI
ηcξcσd−p∗
2∨1(κ2t)p∗−1+cηcξd−p∗
2∨1ν2t.
Here cσ=p∗!αp∗βp∗(when IE(σ∗) = GE( σ∗)) orcσ=p∗!H(σI
∗;p∗)H(σ(I)(σ(1))I−1;p∗−1)
2(I−1)!(other-
wise), and ν2tis a mean-zero sub-exponential random variable.
Proof. The expected alignment θ⊤w2(t+1)after two gradient steps from w2t=ωusing the same
sample (x, y), step size η2t=η2t+1=η=cηd−1and momentum parameter ξ2(t+1)=ξ=
221−cξd−(p∗−2)+
2 is evaluated as follows. With a projection matrix Pω=I−ωω⊤, the first step
updates the weight as
w2t+1←w2t+η˜∇wyσ(w2t⊤x) =ω+ηyσ′(ω⊤x)Pωx, (B.3)
and the next gradient step with the same sample is computed as
˜∇wyσ(w2t+1⊤x) =yσ′(w2t+1⊤x)x
=yσ′ 
(ω+ηyσ′(ω⊤x)Pωx)⊤x
Pωx
=yσ′ 
ω⊤x+η∥x∥2
Pωσ′(ω⊤x)y
Pωx, (B.4)
here we used the notation ∥θ∥2
A=θ⊤Aθfor a vector θ∈Rdand a positive symmetric matric
A∈Rd×d. From (B.3) and (B.4), the parameter after the two steps is obtained as
w2(t+1)←w2t+1+η˜∇wyσ(w2t+1⊤x)
=ω+ηyσ′(ω⊤x)Pωx+ηyσ′ 
ω⊤x+η∥x∥2
Pωσ′(ω⊤x)y
Pωx.
=ω+ηg2t,
where
g2t=yσ′(ω⊤x)Pωx+yσ′ 
ω⊤x+η∥x∥2
Pωσ′(ω⊤x)y
Pωx.
Finally, the normalization step yields
w2(t+1)←w2(t+1)−ξ2(t+1)(w2(t+1)−w2t)
∥w2(t+1)−ξ2(t+1)(w2(t+1)−w2t)∥=ω+ηξg2t
∥ω+ηξg2t∥=ω+cηcξd−p∗
2∨1g2t
∥ω+cηcξd−p∗
2∨1g2t∥.
Therefore, by writing θ⊤w2t=κ2t, the update of the alignment is
κ2(t+1)=θ⊤w2(t+1)
=κ2t+cηcξd−p∗
2∨1θ⊤g2t
∥ω+cηcξd−p∗
2∨1g2t∥
≥κ2t+cηcξd−p∗
2∨1θ⊤g2t−1
2κ2tc2
ηc2
ξd−p∗∨2∥g2t∥2−1
2c3
ηc3
ξd−3p∗
2∨3|θ⊤g2t|∥g2t∥2.(B.5)
We can easily see that E[∥g2t∥2]≲dandE[|θ⊤g2t|∥g2t∥2]≲d, which implies that the expec-
tation of the last two terms of (B.5) is bounded by ≲κ2tc2
ηc2
ξd−(p∗−1)∨1∨c3
ηc3
ξd−(3p∗
2−1)∨2≤
c2
ηc2
ξd−(p∗−1)∨1κ2t.
Now we bound E[θ⊤g2t]by≳cI−1
ηκp∗−1. Let Cσbe the maximum degree of the activation
function with non-zero coefficients of Hermite expansion, which may be infinity when we consider
general link functions, and there appear some infinite sums. For these cases we simply assume the
sums converge – we discuss the validity of this condition in Section B.1.2. We omit the subscript 2t
in the following for simplicity. We divide the analysis into the two cases.
(I) If I= 1⇔IE(σ∗) = GE( σ∗) =p∗.For the first term of E[θ⊤g], we have
θ⊤E[yσ′(ω⊤x)Pωx] =θ⊤PωE∞X
j=p∗αjHej(θ⊤x) CσX
j=1jβjHej−1 
ω⊤x
x
=θ⊤Pω∞X
j=p∗
j!αjβj 
θ⊤ωj−1θ+ (j+ 2)!αjβj+2 
θ⊤ωjω
=CσX
j=p∗j!αjβj 
θ⊤ωj−1θ⊤Pωθ
=p∗!αp∗βp∗κp∗−1+O(κp∗). (B.6)
23For the second term of E[θ⊤g], the following decomposition can be made.
θ⊤E[yσ′ 
ω⊤x+η∥x∥2
Pωσ′(ω⊤x)y
Pωx]
=Cσ−1X
i=1(i!)−1θ⊤PωE
yσ(i+1) 
ω⊤x 
η∥x∥2
Pωyσ(1)(ω⊤x)ix
+θ⊤E[yσ′(ω⊤x)Pωx]
=Cσ−1X
i=1(i!)−1ηiθ⊤PωE
∥x∥2i
Pωyi+1σ(i+1) 
ω⊤x 
σ(1)(ω⊤x)ix
(B.7)
+p∗!αp∗βp∗κp∗−1+O(κp∗). (B.8)
We evaluate each term in the summation. We need to show that although ∥x∥2
Pωis a function of
θ⊤xandω⊤x, it is mostly independent from the two quantities. To verify this, let e=θ−(θ⊤ω)ω
∥θ−(θ⊤ω)ω∥
be the orthogonal component of θtoω. Then, we have that
∥x∥2
Pω=x⊤(I−ωω⊤−ee⊤)x+ (e⊤x)2
=x⊤(I−ωω⊤−ee⊤)x| {z }
∼χ2
d−2,independent from ω⊤xandθ⊤x+θ⊤x−(θ⊤ω)ω⊤x
∥θ−(θ⊤ω)ω∥2
.
With Pω,θ=Id−ωω⊤−ee⊤, (B.7) is expanded as
(B.7) =Cσ−1X
i=1iX
j=0i+1X
l=0 i
j i+1
l
ηi
i!∥θ−(θ⊤ω)ω∥2j
θ⊤PωE
(x⊤Pω,θx)i−jςi+1−l(σ∗(θ⊤x))l(θ⊤x−(θ⊤ω)ω⊤x)2jσ(i+1) 
ω⊤x 
σ(1)(ω⊤x)ix
=Cσ−1X
i=1iX
j=0i+1X
l=02jX
k=0 i
j i+1
l 2j
k
ηiκk(−1)kE[ςi+1−l]Ez∼χ2
d−2[zi−j]
i!∥θ−(θ⊤ω)ω∥2j
E
(σ∗(θ⊤x))l(θ⊤x)2j−k(ω⊤x)kσ(i+1) 
ω⊤x 
σ(1)(ω⊤x)i(θ⊤x−θ⊤ωω⊤x)
(B.9)
For a general differentiable function g(x), we have E[Het(x1)g(x)] =E[dt
dxt
1g(x)]. Ifg(x)is a
polynomial (with a bounded coefficients) of x1andu⊤xand its degree with respect to x1is at most
s(≤t),|E[Het(x1)g(x)]|≲|u1|t−s, because differentiation of g(x) = ¯g(x1,u⊤x)is taken with
respect to the first variable at most stimes. Each term of (B.9) is an expectation of (σ∗(θ⊤x))l,
multiplied by the polynomial of θ⊤xandω⊤x, where its degree with respect to θ⊤xis at most
2j−k. Thus each term of (B.9) is evaluated as (here we omit the constants)
ηiκk(−1)kEz∼χ2
d−2[zi−j]
i!∥θ−(θ⊤ω)ω∥2jE
(σ∗(θ⊤x))l
|{z}
IE≥p∗(θ⊤x)2j−k(ω⊤x)kσ(i+1) 
ω⊤x 
σ(1)(ω⊤x)i(θ⊤x−θ⊤ωω⊤x)| {z }
degree w.r.t. θ⊤xis at most 2j−k+ 1
≲ci
ηd−idi−jκkκ((p∗−2j+k−1)∨0)≲cηd−jκ((p∗−2j−1)∨0)≤cηκp∗−1(d/κ2)−j≤cηκp∗−1.
The lower bound follows in the same fashion. Therefore,
|(B.9)|≲cηκp∗−1.
Now,E[θ⊤g]can be evaluated as
E[θ⊤g] =(B.6) +(B.7) +(B.8) = 2p∗!αp∗βp∗κp∗−1+O(cηκp∗−1+κp∗).
24(II) If I={mini|IE(σi
∗) = GE( σ∗) =p∗} ≥2.Note that αj= 0 for all j≤p∗from the
assumption. Following (B.6), the first term of E[θ⊤g]is evaluated as
θ⊤E[yσ′(ω⊤x)Pωx] =CσX
j=pj!αjβj 
θ⊤ωj−1θ⊤Pωθ=O(κp∗). (B.10)
For the second term of E[θ⊤g], similarly to (B.9), the following decomposition can be made.
θ⊤E[yσ′ 
ω⊤x+η∥x∥2
Pωσ′(ω⊤x)y
Pωx]
=Cσ−1X
i=0iX
j=0i+1X
l=02jX
k=0 i
j i+1
l 2j
k
ηiκk(−1)kE[ςi+1−l]Ez∼χ2
d−2[zi−j]
i!∥θ−(θ⊤ω)ω∥2j
E
(σ∗(θ⊤x))l(θ⊤x)2j−k(ω⊤x)kσ(i+1) 
ω⊤x 
σ(1)(ω⊤x)i(θ⊤x−θ⊤ωω⊤x)
.(B.11)
Each term of (B.11) (omitting constants) is evaluated as
ηiκkEz∼χ2
d−2[zi−j]E
(σ∗(θ⊤x))l(θ⊤x)2j−k(ω⊤x)kσ(i+1) 
ω⊤x 
σ(1)(ω⊤x)i(θ⊤x−θ⊤ωω⊤x)
(B.12)
=ci
ηκkd−jE
(σ∗(θ⊤x))l
|{z}
IE≥

p∗(l≥I)
p∗+ 1 (l < I )(θ⊤x)2j−k(ω⊤x)kσ(i+1) 
ω⊤x 
σ(1)(ω⊤x)i(θ⊤x−θ⊤ωω⊤x)| {z }
degree w.r.t. θ⊤xis at most 2j−k+ 1
≲ci
ηκkd−jκ(IE(σl
∗)−2j+k−1)∨0(B.13)
When i≤I−2andη=cηd−1, we have l≤i+ 1< IandIE((σ∗(θ⊤x))l)≥p∗+ 1. Thus
(B.13)≲κp∗
When i≥I,IE((σ∗(θ⊤x))l)≥p∗and we get
(B.13)≲cI
ηκp∗−1.
Now the case of i=I−1. When i=I−1andj̸= 0, and using the assumption that κ≤cη,
(B.13)≲cI−1
ηκp∗−1(κ−2/d)≤cI
ηκp∗−1.
When i=I−1,j= 0, and k̸= 0,
(B.13)≲cI−1
ηκp∗.
When i=I−1,j= 0,k= 0, and l≤I−1,
(B.13)≲cI−1
ηκp∗.
Therefore, except for i=I−1,j= 0,k= 0, and l≤I−1, we can bound (B.13) by ≲
cI
ηκp∗−1+κp∗. The lower bound follows in the same way. Finally, consider the case of i=I−1,
j= 0,k= 0, and l=I.
(B.12) =ηI−1Ez∼χ2
d−2[zI−1]E
(σ∗(θ⊤x))Iσ(I+1) 
ω⊤x 
σ(1)(ω⊤x)I−1(θ⊤x−θ⊤ωω⊤x)
=ηI−1Ez∼χ2
d−2[zI−1]CσIX
m=p∗m!H(σI
∗;m)H(σ(I)(σ(1))I−1;m−1)(1−κ2)κm−1
25=cI−1
ηp∗!d−(I−1)Ez∼χ2
d−2[zI−1]H(σI
∗;p∗)H(σ(I)(σ(1))I−1;p∗−1)(1−κ2)κp∗−1+O(cI−1
ηκp∗).
Putting it all together (recovering the constants omitted in (B.12) again),
(B.11)
=cI−1
ηp∗!d−(I−1)Ez∼χ2
d−2[zI−1]
(I−1)!H(σI
∗;p∗)H(σ(I)(σ(1))I−1;p∗−1)κp∗−1+O(cI
ηκp∗−1+κp∗),
and
E[θ⊤g] =(B.10) +(B.11)
=cI−1
ηp∗!d−(I−1)Ez∼χ2
d−2[zI−1]
(I−1)!| {z }
=Θ(1)H(σI
∗;p∗)H(σ(I)(σ(1))I−1;p∗−1)κp∗−1+O(cI
ηκp∗−1+κp∗).
Combining (i) and (ii), we have
E[θ⊤g]≥2cI−1
ηcσκp∗−1+O(cI
ηκp∗−1+κp∗)
for a positive constant cσ= Θ(1) . Here cσ>0satisfies 2cσ= 2p∗!αp∗βp∗(for (i)) or
2cσ=p∗!H(σI
∗;p∗)H(σ(I)(σ(1))I−1;p∗−1)
(I−1)!(for (ii)). Going back to (B.5), by setting ν2t= (θ⊤g2t−
E[θ⊤g2t]), we have
κ2(t+1)≥κ2t+ 2cηcξd−p∗
2∨1E[θ⊤g2t] +cηcξd−p∗
2∨1(θ⊤g2t−E[θ⊤g2t]) +O(c2
ηc2
ξd−(p∗−1)∨1κ2t)
=κ2t+ 2cI
ηcξd−p∗
2∨1cσ(κ2t)p∗−1+cηcξd−p∗
2∨1ν2t
+O
c2
ηc2
ξd−(p∗−1)∨1(κ2t)2t+cI+1
ηcξd−p∗
2∨1(κ2t)p∗−1+cηcξd−p∗
2∨1(κ2t)p∗
.
When cξ≤cI
ηandc−1
ηd−1
2≤κ≤cI
η, terms in the big- Onotation is smaller than
cI
ηcξd−p∗
2∨1cσ(κ2t)p∗−1and we have
κ2(t+1)≥κ2t+cI
ηcξcσd−p∗
2∨1(κ2t)p∗−1+cηcξd−p∗
2∨1ν2t.
It is straightforward to check ν2thas sub-Weibull tail.
B.4 Weak Recovery: Stochastic Update
This subsection proves weak recovery using the results on population update from the previous
section. Specifically, from the previous section, we know that
θ⊤w2(t+1)≥θ⊤w2t+cI
ηcξcσd−p∗
2∨1(κ2t)p∗−1+cηcξd−p∗
2∨1ν2t,
with the mean-zero sub-Weibull random variable ν2tand a positive cσ= Θ(1) . For notational
simplicity we write cI
ηcσ=c1. The following lemma is a detailed version of Proposition 9.
Lemma 17. Takeη2t, η2t+1=η=cηd−1,ξ2(t+1)=ξ= 1−cξd−(p∗−2)+
2. Suppose that the link
function satisfies IE(σI
∗) = GE( σ∗) =p∗(we choose the smallest such I) and activation functions
satisfy all of the assumptions in Section B.1 for weak recovery. Let
T1,1=C3c−1
ξ

d (ifp∗= GE( σ∗) = 1)
d(logd) (else if p∗= GE( σ∗) = 2)
dp∗−1(elsep∗= GE( σ∗)≥3),
and take cξ≲δpoly( cη),c2≳poly( cη), and C3≃c−1
1. Ifκ0≥2c−1
ηd−1
2, there exists some
τ∗≤T1,1such that
κ2τ∗≥2c2,
with probability at least 1−δ, and κ2τ≥2c2for all τ∗≤τ≤T1,1, with high probability.
26We may take δ=od(1)with arbitrarily slow decay. The proof is adapted from [BAGJ21], but
our bound on T1,1is slightly sharper (by a logdfactor for p∗= 2 and by a (logd)2factor for
p∗≥3). For p∗= 2, this is because of a trick that we carefully “restart” the dynamics, whose
failure probability exponentially decays.
Proof. We divide the proof into the following cases.
(i) When p∗= 1.Note that {Pτ
s=0ν2s}τis Martingale with E[(ν2s)2]≲1. By Doob’s maximal
inequality and Markov’s inequality, with probability 1−δ, we have
max
0≤τ≤TτX
s=0ν2s2
≤δ−1E[(TX
s=0ν2s)2]≤δ−1TX
s=0E[(ν2s)2]≤C1δ−1(T+ 1) (B.14)
for any fixed T≥0, with a sufficiently large constant C1= Θ(1) . In the following we consider the
case when (B.14) holds for T=c−1
1c−1
ξd−1.
Ifc−1
ηd−1
2≤κ2t≤cI
ηfor all t= 0,1,···, τ, we have
κ2(τ+1)≥κ2τ+c1cξd−1+cηcξd−1ν2τ
≥2c−1
ηd−1
2+c1cξ(τ+ 1)d−1γ−cηcξd−1τX
s=0ν2s. (B.15)
Now, applying (B.14) to get
κ2(τ+1)≥(B.15) ≥2c−1
ηd−1
2+c1cξ(τ+ 1)d−1−cηc1
2
ξc−1
2
1C1
2
1δ−1
2d−1
2,
when τ≤c−1
1c−1
ξd−1. By letting cξ≤c−4
ηc1C−1
1δ, we have c−1
ηd−1
2≤cηc1
2
ξc−1
2
1C1
2
1δ−1
2d−1
2,
and
κ2(τ+1)≥c−1
ηd−1
2+c1cξ(τ+ 1)d−1,
which verifies c−1
ηd−1
2≤κ2tfort=τ+ 1. Thus, there exists some τ∗≤c−1
1c−1
ξdsuch that
κ2τ∗≥4c2,
forc1≤1
4cI
η, with probability 1−δ.
Now we prove that κ2t≥2c2holds for all τ∗≤t≤T1,1=C3c−1
ξd. Because ν2tare mean-zero
sub-Weibull random variables, we also have that |Pτ+τ′−1
s=τν2s| ≤C4√
τ′for all 0≤τ, τ′≤
T1,1with high probability. Also, because ηt≪d−1and|1−ξt| ≪ 1, we can easily see that
|κ2(τ+1)−κ2τ|=˜O(d−1)for all τ= 0,1,···, T1,1−1, with high probability. Thus, when there
exists τ≥τ∗such that κ2(τ−1)≥4c2andκ2τ<4c2, we have κ2τ≥3c2with high probability.
Moreover, following the above argument, we can inductively show that
κ2(τ+τ′)≥3c2+c1cξτ′d−1−cηcξd−1C4√
τ′
≥3c2+c1cξτ′d−1−(
c2 (τ′≤c−2
ηc−2
ξC−2
4c2
2d2)
c1cξτ′d−1(τ′≥c2
ηc−2
1C2
4).
≥2c2,
forτ′≤T1,1=C3c−1
ξdor until κ2(τ+τ′)≥4c2holds again. By repeating this argument (if there
are multiple such τ), we see that κ2t≥2c2holds for all τ∗≤t≤T1,1=C3c−1
ξdwith high
probability.
(ii) When p∗= 2.We define ι0= 0, ι1= log(1+c1cξd−1)(4), ι2= 2 log(1+c1cξd−1)(4), . . .. We
show that, for each i, ifκ2ιi≥2c−1
ηd−1
2, we have κ2(ιi+1)≥2κ2ιi, with probability at least
1−δ4−i, or there exists some t(ιi< t≤ιi+1)withκ2t> cI
η.
Assume that the above statement holds until some i−1≥0(we do not need to assume anything for
i= 0). Then, we have κ2ιi≥2iκ0≥2c−1
ηd−1
2. Similarly to (B.15), if c−1
ηd−1
2≤κ2t≤cI
ηfor all
t=ιi, ιi+ 1,···, τ, we have
κ2(τ+1)≥κ2ιi+c1cξd−1τX
s=ιiκ2s−cηcξd−1τX
s=ιiν2s.
27Applying (B.14) with δ=δ/4iandT=1
4c−2
ηc−2
ξC−1(δ/4i)(κ2ιi)2d2−1to get
κ2(τ+1)≥κ2ιi+c1cξd−1τX
s=ιiκ2s−cηcξd−1C1
2δ−1
2√
τ+ 1−ιi
≥κ2ιi+c1cξd−1τX
s=ιiκ2s−1
2κ2ιi
when τ≤ιi+1
4c−2
ηc−2
ξC−1(δ/4i)(κ2ιi)2d2−1, which verifies c−1
ηd−1
2≤1
2κ2ιi≤κ2tfor
t=τ+ 1.
This implies that, with probability 1−δ/4i, we have
κ2(τ+1)≥1
2κ2ιi+c1cξd−1τX
s=ιiκ2s
for all τ=1
4c−2
ηc−2
ξC−1(δ/4i)(κ2ιi)2d2−1, which is equivalent to
κ2τ≥(1 +c1cξd−1)τ−ιi1
2κ2ιi
for all τ=ιi, ιi+ 1,···, ιi+1
4c−2
ηc−2
ξC−1(δ/4i)(κ2ιi)2d2. By taking cξ≪
c1c−2
ηC−1(δ/4i)(κ2ιi)2d, we have1
4c−2
ηc−2
ξC−1(δ/4i)(κ2ιi)2d2≥log(1+c1cξd−1)(4), and we get
κ2ιi+1≥2κ2ιi
with probability 1−δ/4i(or there exists t≤ιi+1such that κ2t> cI
η).
Thus, by induction, for all i, we have that
κ2ιi≥2iκ0, (B.16)
or that there exists some t≤ιisuch that κ2tis larger than cI
η, with probability 1−δ.
The LHS of (B.16) becomes larger than cI
ηfor some i≤logd. Because ιi= Θ( ic−1
1c−1
ξd), within
O(c−1
1cξdlogd)steps, there exists at least one τ∗=O(c−1
1c−1
ξdlogd)such that κ2τ∗≥4c2for
c2≤1
4cI
η, with probability 1−δ.
Once such τ∗is obtained, following the last paragraph of (i), we can see that κ2t≥2c2holds until
t=T1,1with high probability.
(iii) When p∗≥3.We apply (B.14) with T=1
p∗−2c−1
1c−1
ξdp∗
2(κ0)−(p∗−2)to obtain that
cηcξd−p∗
2τX
s=0ν2s≤cηc1
2
ξc−1
2
1C1
2δ−1
2d−p∗
4(κ0)−p∗−2
2 (B.17)
for all τ= 0,1,···, T−1, with probability 1−δ.
We take cξ≪c−2
ηc1C−1δdp∗
4(κ0)p∗
2so that (B.17) is bounded by c−1
1d−1
2. Then,
κ2(τ+1)≥κ0+c1cξd−p∗
2τX
s=0(κ2s)p∗−1+cηcξd−p∗
2τX
s=0ν2s
≥c−1
ηd−1
2+c1cξd−p∗
2τX
s=0(κ2τ)p∗−1.
It is easy to see that κ2(τ+1)is lower bounded by aτ+1, where a0=c−1
ηd−1
2andaτ+1=aτ+
c1cξd−p∗
2(aτ)p∗−1. By applying Lemma 18, we have
κ2τ≥κ0
 
1−c1cξd−p∗
2(p∗−2)(κ0)(p∗−2)t 1
p∗−2.
28Thus, until τ≤ 
c1cξd−p∗
2(p∗−2)(κ0)(p∗−2)−1≤T+1≪dp∗−1, with probability at least 1−δ,
there exists some τ∗such that
κ2τ∗≥4c2≥cI
η
when c2≤1
4cI
η.
Once such τ∗is obtained, following the last paragraph of (i), we can see that κ2t≥2c2holds until
t=T1,1with high probability.
In the above proof we used the (discrete version of) Bihari–LaSalle inequality from [BAGJ22].
Lemma 18. Forp≥3andc >0, consider a positive sequence (at)t≥0such that
at+1=at+c(at)p−1.
Then, we have
at≥a0
 
1−c(p−2)(a0)(p−2)t1
p−2.
Proof. From definition, we have
c=at+1−at
(at)p−1≤Zat+1
t=at1
xp−1≤1
p−21
(at)p−2−1
(at+1)p−2
.
Taking the summation and re-arranging the terms yield
(at)−(p−2)≤(a0)−(p−2)−c(p−2)t,
∴at≥a0
 
1−c(p−2)(a0)(p−2)t1
p−2,
which gives the lower bound.
B.5 From Weak Recovery to Strong Recovery
In the previous subsection, we proved that after t= 2T1,1=˜Θ(d)steps, with probability ˜Ω(1)
over the randomness of initialization, we obtain nontrivial alignment κ2T1,1
j≥2c2. This subsection
discusses how to convert the weak recovery into the strong recovery.
Lemma 19. Suppose the neuron satisfies κ2T1,1≥2c2. Take η2t=η= ¯cηεd−1,η2t+1= 0,
ξ2(t+1)= 0 for all t≥T1,1, where ¯cη≲poly( c1). If the activation functions satisfy all of the
assumptions in Section B.1 for strong recovery, then we have
θ⊤w2(T1,1+τ∗)≥1−ε,
with high probability, where τ∗≤T1,2=C3dε−2. Moreover, θ⊤w2(T1,1+t)≥1−εfor all
τ∗≤t≤T1,2=C3dε−2, with high probability.
Proof. Consider the Hermite expansions of σ∗andσ. Letpbe the smallest degree that both σ∗and
σhave non-zero coefficients. First we compute the population gradient (of the correlation term) as
E˜∇wyσ(w2t⊤x)
=E
˜∇w∞X
j=pαjHej(θ⊤x)∞X
j=0βjHej(w2t⊤x)
=∞X
j=p
j!αjβj(θ⊤w2t)j−1θ+ (j+ 2)!αjβj+2(θ⊤w2t)jw2t
.(B.18)
Applying Pw2t, we have
E
Pw2t˜∇wyσ(w2t⊤x)
= (θ−(w2t⊤θ)w2t)∞X
j=pj!αjβj(θ⊤w2t)j−1. (B.19)
29Thus, the update of the alignment κ2t=θ⊤w2tis
κ2(t+1)≥κ2t+ηθ⊤g−1
2η2κ2t∥g∥2−1
2η3˜η3|θ⊤g|∥g∥2,
where
g=Pw2tyσ′(w2t⊤x)x.
From (B.18), the expectation of (B.19) is bounded by
E[κ2(t+1)]≥κ2t+η(1−(κ2t)2)∞X
j=pj!αjβj(θ⊤w2t)j−1−η2C4d(κ2t+η)
≥κ2t+η(1−(κ2t)2)∞X
j=pj!αjβj(κ2t)p−1−η2C4d(κ2t+η).
By letting η≤cp−1
1εd−1, when κ2t≤1−ε, we have
E[κ2(t+1)]≥κ2t+1
2ηε∞X
j=pj!αjβj(κ2t)p−1≥κ2t+ηεcp
1.
It is easy to see that the noise ν2thas sub-Weibull tail and we obtain that
κ2(t+1)≥κ2t+1
2ηε∞X
j=pj!αjβj(κ2t)p−1+ην2t≥κ2t+ηεcp
1+ην2t. (B.20)
Suppose that 2c2≤κ2(T1,1+τ)≤1−εfor all t= 0,1, . . . , τ −1. By taking the summation of
(B.20), we have
κ2(T1,1+τ)≥κ2T1,1+ηεtcp
1+ηT1,1+τ−1X
s=T1,1ν2t≥2c2+ηετcp
1−C4η√τ, (B.21)
with high probability. The third term is bounded by C4η√τ≤c2when τ≤c2
2C−2
4η−2=
c2
2C−2
4¯c−2
ηε−2d2and by1
2ηετcp
1when τ≥4ε−2c−2p
1C2
4. Because c2
2C−2
4¯c−2
ηε−2d2≥
4ε−2c−2p
1C2
4, we can bound (B.21) by
κ2(T1,1+τ)≥c2+1
2ηετcp
1, (B.22)
which verifies 2c2≤κ2(T1,1+τ).
Therefore, by induction, until κ2t≥1−ε, we have the lower bound (B.22), whose RHS exceeds
1−εwhen τ≥2η−1ε−1c−p
1≤C3dε−2. Thus, there exists τ∗≤T1,2=C3dε−2such that
κ2(T1,1+τ∗)≥1−ε, with high probability.
Now, what remains is to prove that κ2(T1,1+τ)≥1−3εholds for all τ∗≤t≤T1,2=C3dε−2.
Because ν2tare mean-zero sub-Weibull random variables, we have that |Pτ+τ′−1
s=τν2s| ≤C4√
τ′
for all 0≤τ, τ′≤T1,1with high probability. Also, because ηt≪εd−1, we can easily see that
|κ2(τ+1)−κ2τ|=˜O(εd−1)for all τ= 0,1,···, T1,1−1, with high probability. Thus, when there
exists τ≥τ∗such that κ2(T1,1+τ−1)≥1−εandκ2(T1,1+τ)<1−ε, we have κ2(T1,1+τ)≥1−2ε
with high probability. Moreover, following the above argument, we can inductively show that
κ2(T1,1+τ+τ′)≥1−2ε+ηετ′cp
1−C4η√
τ′
≥1−2ε+ηετ′cp
1−ε (τ′≤¯c−2
ηC−2
4d2)
ηετ′cp
1(τ′≥ε−2C2
4c−2p
1).
≥1−3ε,
forτ′≤T1,2or until κ2(T1,1+τ+τ′)≥1−εholds again. Note that the last inequality follows from
¯c−2
ηC−2
4d2≥ε−2C2
4c−2p
1. By repeating this argument (if there are multiple such τ), we can see that
κ2(T1,1+t)≥1−εholds for all τ∗≤t≤T1,2=C3dε−2with high probability.
Adjusting hidden constants to remove a factor of 3from 3εyields the desired result.
30B.6 Second Layer Training
From the previous analysis, we know that at least Ω(1) portion of the neurons will satisfy the weak
and strong recovery conditions (Appendix B.1), at least ˜Ω(1) portion of the neurons (independent
from the choice of σj) satisfy initial alignment conditions (Appendix B.2), and at least 1−o(1)
fraction of them achieves strong recovery. This subsection proves a generalization error bound after
second-layer training. Let fa(x) =fΘ(x)forΘ= (ˆwj, aj,ˆbj)N
j=1where a∈RNand(ˆwj,ˆbj)N
j=1
are the parameters trained in the first stage. Let a∗∈RNbe the “certificate” with ∥a∗∥2=˜O(N)
that is shown to exist in Lemma 22.
Polynomial Link Functions. The following lemma is a complete version of Proposition 5.
Lemma 20. There exists a 4q-th order polynomial Q(Rw, b, q′)ofRw= max j∥wj∥,b= (bj)N
j=1
such that, if we set λ= Θq
2
T2δ0N2Q(Rw, b, q′)
for some δ0>0, the ridge estimator ˆasatisfies
∥fˆa−f∗∥2
L2(Px)≲(N−2+ε2) +1
T2λδ0 
2N2Q(Rw, b, q′) +Ex[(f∗)4]
+3λ
2∥a∗∥2,(B.23)
with probability 1−δ0. Hence taking T2=˜Θ((N4Q2(Rw, b, q′) +E[f∗(x)4]2)ε−4)andN=
˜Θ(ε−1), we have
Ex[(fˆa(x)−f∗(x))2]≲ε2.
Proof. LetPT2be the empirical distribution of the second stage: PT2:=1
T2PT2
i=1δxi. Letψ(x) =
(σ(⟨x,bwj)⟩+bj))N
j=1so that fa(x) =⟨a, ψ(x)⟩.
Part (1). We first bound the term ∥fˆa−f∗∥L2(PT2). Since ˆL(fˆa) +λ∥ˆa∥2≤ˆL(fa∗) +λ∥a∗∥2,
we have
∥fˆa−f∗∥2
L2(PT2)+λ∥ˆa∥2(B.24)
≤ ∥fa∗−f∗∥2
L2(PT2)+2
T2T2X
i=1(fa∗(xi)−fˆa(xi))εi+λ∥a∗∥2.
Now, by the Cauchy-Schwarz inequality, we have
2
T2T2X
i=1(fa∗(xi)−fˆa(xi))εi= (a∗−ˆa)⊤2
T2T2X
i=1ψ(xi)εi
≤2∥a∗−ˆa∥sP
i,jεiεjψ(xi)⊤ψ(xj)
T2
2.
By applying Markov’s inequality to the right hand side, it can be further bounded by
∥a∗−ˆa∥s
Ex[∥ψ(x)∥2]
T2δ1≤λ
2∥ˆa∥2+λ
2∥a∗∥2+Ex[∥ψ(x)∥2]
T2δ1λ,
with probability 1−δ1. Thus, by combining with (B.24), we arrive at
∥fˆa−f∗∥2
L2(PT2)+λ
2∥ˆa∥2≤ ∥fa∗−f∗∥2
L2(PT2)+Ex[∥ψ(x)∥2]
T2δ1λ+3λ
2∥a∗∥2.
Here, by using the evaluation ∥fa∗−f∗∥L2(PT2)=˜O(N−1+ε)in Lemma 22, the right hand side
can be further bounded by
∥fˆa−f∗∥2
L2(PT2)+λ
2∥ˆa∥2≤˜O(N−2+ε2) +Ex[∥ψ(x)∥2]
T2δ1λ+3λ
2∥a∗∥2.
31Part (2). Next we lower bound ∥fˆa−f∗∥2
L2(PT2)by noticing that
∥fˆa−f∗∥2
L2(PT2)
=∥fˆa−f∗∥2
L2(PT2)− ∥fˆa−f∗∥2
L2(Px)+∥fˆa−f∗∥2
L2(Px)
=∥fˆa∥2
L2(PT2)− ∥fˆa∥2
L2(Px)−2 
1
T2T2X
i=1fˆa(xi)f∗(xi)−E[fˆa(xi)f∗(xi)]!
+∥f∗∥2
L2(PT2)− ∥f∗∥2
L2(Px)+∥fˆa−f∗∥2
L2(Px). (B.25)
The first two terms of Eq. (B.25) can be bounded by
∥fˆa∥2
L2(PT2)− ∥fˆa∥2
L2(Px)=ˆa⊤ PT2
i=1ψ(xi)ψ(xi)⊤
T2−Ex[ψ(x)ψ(x)⊤]!
ˆa
≤ ∥ˆa∥2sup
a:∥a∥≤1∥fa∥2
L2(PT2)− ∥fa∥2
L2(Px).
The standard Rademacher complexity bound yields that
E(xi)T2
i=1"
sup
a∈RN:∥a∥≤1∥fa∥2
L2(Px)− ∥fa∥2
L2(PT2)#
≤2E(xi,σt)T2
t=1"
sup
a∈RN:∥a∥≤11
T2T2X
t=1σtfa(xi)2#
≤2vuutE(xi)T2
i=1"
sup
a∈RN:∥a∥≤11
T2
2T2X
i=1(a⊤ψ(xi))4#
≤2vuutE(xi)T2
i=1"
1
T2
2T2X
i=1∥ψ(xi)∥4#
=2r
1
T2Ex[∥ψ(x)∥4],
where (σi)T2
i=1is the i.i.d. Rademacher sequence independent of (xi)T2
i=1. Hence, Markov’s inequal-
ity yields
∥fˆa∥2
L2(PT2)− ∥fˆa∥2
L2(Px)= 2∥ˆa∥2r
1
T2δ2Ex[∥ψ(x)∥4],
with probability 1−δ2.
The third term in Eq. (B.25) can be evaluated as
2 
1
T2T2X
i=1fˆa(xi)f∗(xi)−Ex[fˆa(x)f∗(x)]!
=ˆa⊤ 
1
T2T2X
i=1(ψ(xi)f∗(xi)−Ex[ψ(x)f∗(x)])!
≤ ∥ˆa∥vuut1
T2
2T2X
i=1T2X
j=1(ψ(xi)f∗(xi)−Ex[ψ(x)f∗(x)])⊤(ψ(xj)f∗(xj)−Ex[ψ(x)f∗(x)])
≤ ∥ˆa∥r
1
T2δ3Ex[∥ψ(x)f∗(x)−Ex[ψ(x)f∗(x)]∥2]
≤ ∥ˆa∥r
1
T2δ3Ex[∥ψ(x)∥4+∥f∗(x)∥4]
32≤λ
4∥ˆa∥2+1
λT2δ3Ex[∥ψ(x)∥4+∥f∗(x)∥4],
with probability 1−δ3where we used Markov’s inequality again in the second inequality.
Finally, the fourth and fifth term in Eq. (B.25) can be bounded as
∥f∗∥2
L2(PT2)− ∥f∗∥2
L2(Px)=r
∥f∗∥2
L2(PT2)− ∥f∗∥2
L2(Px)2
≤r
1
T2δ4Ex[(f∗(x)4− ∥f∗∥2
L2(Px))2]
≤r
1
T2δ4Ex[(f∗(x))4],
with probability 1−δ4where we used Markov’s inequality in the last inequality.
Combining these inequalities, we finally arrive at
∥fˆa−f∗∥2
L2(Px)+λ
4−r
2
T2δ2Ex[∥ψ(x)∥4]
∥ˆa∥2
≤˜O(N−2+ε2) +1
T2λEx[∥ψ(x)∥2]
δ1+Ex[∥ψ(x)∥2]
δ3+Ex[(f∗(x))4]
δ3
+3λ
2∥a∗∥2,
with probability 1−P4
j=1δj. Hence, by setting λ≥8q
2
T2δ2Ex[∥ψ(x)∥4], we have that
∥fˆa−f∗∥2
L2(Px)
≤˜O(N−2+ε2) +1
T2λEx[∥ψ(x)∥2]
δ1+Ex[∥ψ(x)∥4]
δ3+Ex[(f∗(x))4]
δ3
+3λ
2∥a∗∥2.
When the activation function σis a polynomial, then each ψj(x) = σ(⟨x,wj⟩+bj)is an
order q-polynomial of a Gaussian random variable ⟨x,wj⟩which has mean 0 and variance
E[⟨x,wj⟩2] =∥wj∥2=˜O(1). Then, if we let Rw:= max j∥wj∥=˜O(1), the term
max jmax{Ex[ψ(x)2
j],Ex[ψ(x)4
j]}can be bounded by a 4q-th order polynomial of Rwandb, which
can be denoted by Q(Rw, b,4q).
Part (3). By combining evaluations of (1) and (2) together, if we let λ= 8q
2
T2δ0Ex[∥ψ(x)∥4]
for some δ0>0, (by ignoring polylogarithmic factors) we obtain that
∥fˆa−f∗∥2
L2(Px)≲(N−2+ε2) +1
T2λδ0 
2N2Q(Rw, b, q′) +Ex[(f∗(x))4]
+3λ
2∥a∗∥2,
with probability 1−4δ0. Thus, since ∥a∗∥2=˜O(N), by setting T2=˜Θ((N4Q2(Rw, b, q′) +
E[f∗(x)4]2)ε−4), and N=˜Θ(ε−1), we obtain that (B.23) ≲ε2.
Higher Generative Exponent Functions. For general link functions, under Assumption 4 and
the bounded fourth moment of the link function, we have the following counterpart of Lemma 20,
which provides the formal statement of Proposition 10.
Lemma 21. Suppose that E[σ∗(θ⊤x)4]<∞and Assumption 4 hold. Then, by setting λ=
˜Θq
N2
T2δ0
for some δ0>0, the ridge estimator ˆasatisfies
∥fˆa−f∗∥2
L2(Px)≲ε2+1√T2δ0 
N2C4+Ex[(f∗)4]
+1√T2δ0∥a∗∥2,
with probability 1−δ0. By taking T2=˜Θ((N4+N2)ε−4), we have
Ex[(fˆa(x)−f∗(x))2]≲ε2.
33Furthermore, applying Lemma 12 and 13 yields that, when σ∗=P∞
j=0αjHejsatisfiesP∞
j=0j2j!α2
jandE[σ∗(θ⊤x)4]are bounded, with a properly designed randomized activation in
Lemma 12, by taking N=˜Θ(ε−7)andT2=˜Θ(ε−32), Algorithm 1 yields
Ex[(fˆa(x)−f∗(x))2]≲ε2,
with probability 1−od(1).
Proof. The proof is identical to that of Lemma 20, with the difference being that we replace the
bounded moment assumptions with E[σ∗(θ⊤x)4]<∞or Assumption 4.
Approximation Guarantee. Note that for non-polynomial link function with generative exponent
p∗≥3, the approximation error is already controlled in Assumption 4 based on [BBSS22, Lemma
4.4, 4.5] (using activation function with a ReLU component). If σ∗is a degree- qpolynomial, we
have the following approximation result using polynomial activation, which follows Lemmas 29 and
30 of [OSSW24a].
Lemma 22. Suppose that there exist at least N′=˜Θ(N)neurons that satisfy ∥w2T1
j−θ∥ ≤εand
σis a polynomial link function with degree at least q. Letbj∼Unif([−Cb, Cb])withCb=˜O(1),
and consider approximation of a ridge function h(θ⊤x)with its degree at most q. Then, there exists
a1, . . . , a Nsuch that
1
NNX
j=1ajσj 
w2T1
j⊤x+bj
−h(θ⊤x)=˜O(N−1+ε)
with high probability, where (x, y)is a random sample, and we omit dependence on the degree qin
the big- Onotation. Moreover, we havePN
j=1a2
j=˜O(N).
Lemma 22 can be established from the following result in [OSSW24a].
Lemma 23. Suppose that Cb≥q. For any polynomial h(s)with its degree at most q, there exists
¯v(b;h)with|¯v(b;h)|≲Cbsuch that for all s,
E[¯v(b;h)σ(δs+b)] =h(s).
34NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research, ad-
dressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and precede the (optional) supplemental material. The checklist does NOT
count towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
• [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evalu-
ation. While ”[Yes] ” is generally preferable to ”[No] ”, it is perfectly acceptable to answer ”[No]
” provided a proper justification is given (e.g., ”error bars are not reported because it would be too
computationally expensive” or ”we were unable to find the license for the dataset we used”). In
general, answering ”[No] ” or ”[NA] ” is not grounds for rejection. While the questions are phrased
in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your
best judgment and write a justification to elaborate. All supporting evidence can appear either in the
main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question,
in the justification please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist” ,
•Keep the checklist subsection headings, questions/answers and guidelines below.
•Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The paper’s main contribution is studying the computational complexity of
learning single-index models with polynomial link functions by training neural networks
with stochastic gradient descent, which is what we claim in the abstract and introduction.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
35Justification: We discussed the limitations of our results in Section 4.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ”Limitations” section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The au-
thors should reflect on how these assumptions might be violated in practice and what
the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All assumptions required in our work are stated in Assumptions 1,2, and 3.
All proofs are presented in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: This paper is theoretical and experiments are only used for the purpose of
illustration in the Introduction.
36Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: This paper is theoretical and experiments are only used for the purpose of
illustration in the Introduction.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
• While we encourage the release of code and data, we understand that this might not
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
37• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: This is a theory paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [NA]
Justification: This is a theory paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: This is a theory paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
38• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have read and followed the NeurIPS Code of Ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper carries out a theoretical study, and we believe our work does not
have specific societal impacts that require a discussion.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
cific groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
39Justification: This paper is theoretical and the experiment is only about two-layer neural
network and synthetic data, which we do not have such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This paper is theoretical and does not use any assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justification: This paper is theoretical and does not introduce new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
40Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: This paper is theoretical and does not include crowdsourcing experiments nor
research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper is theoretical and does not include crowdsourcing experiments nor
research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
41