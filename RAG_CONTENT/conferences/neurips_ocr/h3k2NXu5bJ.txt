Certified Machine Unlearning
via Noisy Stochastic Gradient Descent
Eli Chien
Department of Electrical and Computer Engineering
Georgia Institute of Technology
Georgia, U.S.A.
ichien6@gatech.edu
Haoyu Wang
Department of Electrical and Computer Engineering
Georgia Institute of Technology
Georgia, U.S.A.
haoyu.wang@gatech.eduZiang Chen
Department of Mathematics
Massachusetts Institute of Technology
Massachusetts, U.S.A.
ziang@mit.edu
Pan Li
Department of Electrical and Computer Engineering
Georgia Institute of Technology
Georgia, U.S.A.
panli@gatech.edu
Abstract
‚ÄúThe right to be forgotten‚Äù ensured by laws for user data privacy becomes increas-
ingly important. Machine unlearning aims to efficiently remove the effect of certain
data points on the trained model parameters so that it can be approximately the
same as if one retrains the model from scratch. We propose to leverage projected
noisy stochastic gradient descent for unlearning and establish its first approximate
unlearning guarantee under the convexity assumption. Our approach exhibits sev-
eral benefits, including provable complexity saving compared to retraining, and
supporting sequential and batch unlearning. Both of these benefits are closely re-
lated to our new results on the infinite Wasserstein distance tracking of the adjacent
(un)learning processes. Extensive experiments show that our approach achieves a
similar utility under the same privacy constraint while using 2%and10% of the gra-
dient computations compared with the state-of-the-art gradient-based approximate
unlearning methods for mini-batch and full-batch settings, respectively.
1 Introduction
Machine learning models usually learn from user data where data privacy has to be respected. Certain
laws, such as European Union‚Äôs General Data Protection Regulation (GDPR), are in place to ensure
‚Äúthe right to be forgotten‚Äù, which requires corporations to erase all information pertaining to a user
if they request to remove their data. It is insufficient to comply with such privacy regulation by
only removing user data from the dataset, as machine learning models can memorize training data
information and risk information leakage [1, 2]. A naive approach to adhere to this privacy regulation
is to retrain the model from scratch after every data removal request. Apparently, this approach
is prohibitively expensive in practice for frequent data removal requests and the goal of machine
unlearning is to perform efficient model updates so that the resulting model is (approximately)
38th Conference on Neural Information Processing Systems (NeurIPS 2024).the same as retraining statistically. Various machine unlearning strategies have been proposed,
including exact [3 ‚Äì6] and approximate approaches [7 ‚Äì11]. The later approaches allow a slight
misalignment between the unlearned model and the retraining one in distribution under a notion
similar to Differential Privacy (DP) [12].
The most popular approach for privatizing machine learning models with DP guarantee is arguably
noisy stochastic gradient methods including the celebrated DP-SGD [13]. Mini-batch training is one
of its critical components, which not only benefits privacy through the effect of privacy amplification
by subsampling [14] but also provides improved convergence of the underlying optimization process.
Several recent works [9, 11] based on full-batch (noisy) gradient methods may achieve certified
approximate unlearning. Unfortunately, their analysis is restricted to the full-batch setting and it
is non-trivial to extend these works to the mini-batch setting with tight approximate unlearning
guarantees. The main challenge is to incorporate the randomness in the mini-batch sampling into the
sensitivity-based analysis [9] or the Langevin-dynamics-based [11] analysis.
We aim to study mini-batch noisy gradient methods for certified approximate unlearning. The high-
level idea of our unlearning framework is illustrated in Figure 1. Given a training dataset Dand
a fixed mini-batch sequence B, the model first learns and then unlearns given unlearning requests,
both via the projected noisy stochastic gradient descent (PNSGD). For sufficient learning epochs, we
prove that the law of the PNSGD learning process converges to a unique stationary distribution ŒΩD|B
(Theorem 3.1). When an unlearning request arrives, we update Dto an adjacent dataset D‚Ä≤so that
the data point subject to such request is removed. The approximate unlearning problem can then be
viewed as moving from the current distribution ŒΩD|Bto the target distribution ŒΩD‚Ä≤|BuntilŒµ-close in
R√©nyi divergence for the desired privacy loss Œµ1.
Our key observation is that the results of Altschuler and Talwar [15, 16], which study the conver-
gence of PNSGD under the (strong) convexity assumption, can be leveraged after we formulate the
approximate unlearning as above. They show that the R√©nyi divergence of two PNSGD processes
with the same dataset but different initial distributions decays at a geometric rate, starting from the
infinite Wasserstein distance (W‚àû)of initial distributions (Figure 1, step 3). As a result, if the initial
W‚àûdistance can be properly characterized, we achieve the corresponding approximate unlearning
guarantee by further taking the randomness of the mini-batches Binto account (Figure 1, step 4).
Therefore, the key step to establish the PNSGD-based unlearning guarantee is to characterize the
initial W‚àûdistance of the unlearning process tightly.
The projection set diameter 2Rfor this W‚àûdistance adopted in [15] for DP analysis, unfortunately,
leads to a vacuous unlearning guarantee, which cannot illustrate the computational advantage over the
retraining from scratch. To alleviate this issue, we perform a careful W‚àûdistance tracking analysis
along the adjacent PNSGD learning processes (Lemma 3.3), which leads to a much better bound
W‚àû(ŒΩD|B, ŒΩD‚Ä≤|B)‚â§ZB‚âàO(Œ∑M/b )(Figure 1, step 2) for bounded gradient norm M, mini-batch
sizeband step size Œ∑. This ultimately leads to our unlearning guarantee Œµ=O(Z2
Bc2Kn/b)forK
unlearning epochs and some rate c <1. The computational benefit compared to the retraining from
scratch naturally emerges by comparing two W‚àûdistances, O(R)for the retraining from scratch and
O(Œ∑M/b )for our unlearning framework.
Our approach also naturally extends to multiple unlearning requests, including sequential and
batch unlearning settings (Theorem 3.11 and Corollary J.1), by extending the W‚àû-tracking analysis
(Lemma 3.4). Here, we may use the triangle inequality of W‚àûdistance, which leads to a tigher privacy
loss bound (growing linearly to the number of unlearning requests) than that in the Langevin-dynamics-
based analysis [11] via weak triangle inequality of R√©nyi divergence (growing exponentially).
Our results highlight the insights into privacy-utility-complexity trade-off regarding the mini-batch
sizebfor approximate unlearning. A smaller batch size bleads to a better privacy loss decaying rate
O(c2Kn/b). However, an extremely small bmay degrade the model utility and incur instability. It
also leads to a worse bound ZB‚âàO(Œ∑M/b ), which degrades the computational benefits compared to
retraining. We demonstrate such trade-off of our PNSGD unlearning results via experiments against
the state-of-the-art full-batch (b=n)gradient-based approximate unlearning solutions [9, 11]. Our
analysis provides a significantly better privacy-utility-complexity trade-off even when we restrict
ourselves to b=n, and further improves the results by adopting mini-batches. Under the same
1We refer privacy loss as two-sided R√©nyi divergence of two distributions, which we defined as R√©nyi
difference in Definition 2.1.
2ùúìùë•;‚Ñ¨‡Øù,ùíü=‡∑ëùë•‚àíùúÇ
ùëè‡∑çùëìùë•;ùíÖùíä
‡Øú‚àà‚Ñ¨‡≥ï +2ùúÇùúé‡¨∂ùëä 
ùíû‡≥É
PNSGD update ùúì: contractive noisy iteration
Step 1:per-epoch PNSGD update ùúôconverge to a unique stationary 
distribution ùúàùíü|‚Ñ¨.ùúô‚ãÖ;‚Ñ¨ ,ùíü=ùúì‚ãÖ;‚Ñ¨‡¨¥,ùíü‚àòùúì‚ãÖ;‚Ñ¨‡¨µ,ùíü‚àò‚ãØùúì‚ãÖ;‚Ñ¨‡Ø°/‡Øï‡¨ø‡¨µ,ùíü
Step 2: Initial distance: ùëä‡Æ∂ùúàùíü|‚Ñ¨,ùúàùíü·á≤|‚Ñ¨‚â§ùëç‚Ñ¨
Step 3: Privacy loss given ‚Ñ¨: ùëê< 1,
 ùëë‡∞àùúå‡ØÑ|‚Ñ¨,ùúàùíü·á≤|‚Ñ¨‚â§ùëÇùëç‚Ñ¨‡¨∂ùëê‡¨∂‡ØÑ= ùëë‡∞à‚Ñ¨
Step 4: Final privacy loss: 
ùëë‡∞àùúå‡ØÑ,ùúàùíü·á≤log(ùîº‚Ñ¨exp(ùëë‡∞à‚Ñ¨)) ‚â§ùúÄùúÄ  Unlearningùúàùíü|‚Ñ¨
ùúàùíü·á≤|‚Ñ¨ùëç‚Ñ¨
ùúå‡ØÑ|‚Ñ¨Learning process ‚Ñ≥ùë°= 0 ùë°= ‚àûùë•‡Øß‡¨æ‡¨µ=ùúôùë•‡Øß;‚Ñ¨ ,ùíü
ùë•‡Øß‡¨æ‡¨µ·á±=ùúôùë•‡Øß·á±;‚Ñ¨ ,ùíü·á±ùëç‚Ñ¨ùúàùíü|‚Ñ¨
ùúàùíü·á≤|‚Ñ¨
ùëç‚Ñ¨ùúàùíü|‚Ñ¨=ùúå‡¨¥|‚Ñ¨
ùúàùíü·á≤|‚Ñ¨ùë¶‡Øû‡¨æ‡¨µ·á±=ùúôùë¶‡Øû·á±;‚Ñ¨ ,ùíü·á±ùë¶‡Øû‡¨æ‡¨µ =ùúôùë¶‡Øû ;‚Ñ¨ ,ùíü·á±
ùúå‡ØÑ|‚Ñ¨
ùúàùíü·á≤|‚Ñ¨Unlearning process ùí∞
ùëò= 0 ùëò=ùêæ<‚âàùëë‡∞à‚Ñ¨Figure 1: The overview of PNSGD unlearning. (Left) Proof sketch for PNSGD unlearning guarantees.
(Right) PNSGD (un)learning processes on adjacent datasets. Given a mini-batch sequence B, the
learning process Minduces a regular polyhedron where each vertex corresponds to a stationary
distribution ŒΩD|Bfor each dataset D.ŒΩD|BandŒΩD|B‚Ä≤are adjacent if D,D‚Ä≤differ in one data point.
We provide an upper bound ZBfor the infinite Wasserstein distance W‚àû(ŒΩD|B, ŒΩD|B‚Ä≤), which is
crucial for non-vacuous unlearning guarantees. Results of [16] allow us to convert the initial W‚àû
bound to R√©nyi difference bound dB
Œ±, and apply joint convexity of KL divergence to obtain the final
privacy loss Œµ, which also take the randomness of Binto account.
privacy constraint, our approach achieves similar utility while merely requiring 10%,2%of gradient
computations compared to baselines for full and mini-batch settings respectively.
1.1 Related Works
Machine unlearning with privacy guarantees. The concept of approximate unlearning uses a
probabilistic definition of (œµ, Œ¥)-unlearning motivated by differential privacy [12], which is studied
by [7, 8, 10]. Notably, the unlearning approach of these works involved Hessian inverse computation
which can be computationally prohibitive in practice for high dimensional problems. Ullah et
al. [5] focus on exact unlearning via a sophisticated version of noisy SGD. Their analysis is based
on total variation stability which is not directly applicable to approximate unlearning settings and
different from our analysis focusing on R√©nyi divergence. Neel et al. [9] leverage full-batch PGD for
(un)learning and achieve approximate unlearning by publishing the final parameters with additive
noise. Chien et al. [11] utilize full-batch PNGD for approximate unlearning with the analysis of
Langevin dynamics. The adaptive unlearning requests setting is studied in [6, 17, 18], where the
unlearning request may depend on the previous (un)learning results. It is possible to show that our
framework is also capable of this adaptive setting since we do not keep any non-private internal states,
though we only focus on non-adaptive settings in this work. We left a rigorous discussion for this as
future work.
2 Preliminaries
We consider the empirical risk minimization (ERM) problem. Let D={di}n
i=1be a training dataset
withndata point ditaken from the universe X. Let fD(x) =1
nPn
i=1f(x;di)be the objective
function that we aim to minimize with learnable parameter x‚àà CR, where CR={x‚ààRd| ‚à•x‚à• ‚â§
R}is a closed ball of radius R. We denote Œ†CR:Rd7‚Üí C Rto be an orthogonal projection to CR.
The norm ‚à• ¬∑ ‚à• is standard Euclidean ‚Ñì2norm.P(C)is denoted as the set of all probability measures
over a closed convex set C. Standard definitions such as convexity are in Appendix D. We use x‚àºŒΩ
to denote that a random variable xfollows the probability distribution ŒΩ. We say two datasets Dand
D‚Ä≤are adjacent if they ‚Äúdiffer‚Äù in only one data point. More specifically, we can obtain D‚Ä≤fromDby
replacing one data point. We next introduce a useful idea which we term as R√©nyi difference.
Definition 2.1 (R√©nyi difference) .LetŒ± >1. For a pair of probability measures ŒΩ, ŒΩ‚Ä≤with the same
support, the Œ±R√©nyi difference dŒ±(ŒΩ, ŒΩ‚Ä≤)is defined as dŒ±(ŒΩ, ŒΩ‚Ä≤) = max ( DŒ±(ŒΩ||ŒΩ‚Ä≤), DŒ±(ŒΩ‚Ä≤||ŒΩ)),
where DŒ±(ŒΩ||ŒΩ‚Ä≤)is the Œ±R√©nyi divergence defined as DŒ±(ŒΩ||ŒΩ‚Ä≤) =1
Œ±‚àí1log
Ex‚àºŒΩ‚Ä≤(ŒΩ(x)
ŒΩ‚Ä≤(x))Œ±
.
3Algorithm 1 (Un)learning with PNSGD
1:Parameters: stepsize Œ∑, noise standard deviation œÉ, dataset size n, mini-batch size b.
2:Minibatch Generation: randomly partition indices [n]inton/b mini-batches of size b:
B0, . . . ,Bn/b‚àí1.
3:Learning process M(D):given a dataset D={di}n
i=1‚àà Xn, sample initial parameter x0
0
from a given initialization distribution ŒΩ0supported on CR. The output is the last iterate x0
T.
4:forepoch t= 0, . . . , T ‚àí1do
5: foriteration j= 0, . . . , n/b ‚àí1do
6: xj+1
t= Œ†CR
xj
t‚àíŒ∑g(xj
t,Bj) +p
2Œ∑œÉ2Wj
t
,where g(xj
t,Bj) =1
bP
i‚ààBj‚àáf(xj
t;di)
andWj
tiid‚àº N(0, Id).
7: end for
8: x0
t+1=xn/b
t
9:end for
10:Unlearning process U(M(D),D‚Ä≤):given an updated dataset D‚Ä≤={d‚Ä≤
i}n
i=1‚àà Xnand current
parameter y0
0, the output is the last iterate y0
K.
11:forepoch k= 0, . . . , K ‚àí1do
12: foriteration j= 0, . . . , n/b ‚àí1do
13: yj+1
k= Œ†CR
yj
k‚àíŒ∑g(yj
k,Bj) +p
2Œ∑œÉ2Wj
k
,where g(yj
k,Bj) =1
bP
i‚ààBj‚àáf(yj
k;d‚Ä≤
i)
andWj
kiid‚àº N(0, Id).
14: end for
15: y0
k+1=yn/b
k
16:end for
We are ready to introduce the formal definition of differential privacy and unlearning.
Definition 2.2 (R√©nyi Differential Privacy (RDP) [19]) .LetŒ± > 1. A randomized algorithm
M:Xn7‚ÜíRdsatisfies (Œ±, Œµ)-RDP if for any adjacent dataset pair D,D‚Ä≤‚àà Xn, the Œ±R√©nyi
difference dŒ±(ŒΩ, ŒΩ‚Ä≤)‚â§Œµ, where M(D)‚àºŒΩandM(D‚Ä≤)‚àºŒΩ‚Ä≤.
It is known to the literature that an (Œ±, Œµ)-RDP guarantee can be converted to the popular (œµ, Œ¥)-DP
guarantee [12] relatively tight [19]. As a result, we will focus on establishing results with respect to
Œ±R√©nyi difference (and equivalently Œ±R√©nyi difference). Next, we introduce our formal definition
of unlearning based on Œ±R√©nyi difference as well.
Definition 2.3 (R√©nyi Unlearning (RU)) .Consider a randomized learning algorithm M:Xn7‚ÜíRd
and a randomized unlearning algorithm U:Rd√ó Xn√ó Xn7‚Üí Rd. We say (M,U)achieves
(Œ±, Œµ)-RU if for any Œ± >1and any adjacent datasets D,D‚Ä≤, theŒ±R√©nyi difference dŒ±(œÅ, ŒΩ‚Ä≤)‚â§Œµ,
where U(M(D),D‚Ä≤)‚àºœÅandM(D‚Ä≤)‚àºŒΩ‚Ä≤.
Our Definition 2.3 can be converted to the standard (œµ, Œ¥)-unlearning definition defined in [7 ‚Äì9],
similar to RDP to DP conversion (see Appendix N). Since we work with the replacement definition of
dataset adjacency, to unlearn a data point diwe can simply replace it with any data point d‚Ä≤
i‚àà X for
the updated dataset D‚Ä≤in practice. One may also repeat the entire analysis with the objective function
being the summation of individual loss for the standard add/remove notion of dataset adjacency.
Finally, we will also leverage the infinite Wasserstein distance in our analysis.
Definition 2.4 (W‚àûdistance) .The‚àû-Wasserstein distance between distributions ¬µandŒΩon
a Banach space (Rd,‚à• ¬∑ ‚à•)is defined as W‚àû(¬µ, ŒΩ) = inf Œ≥‚ààŒì(¬µ,ŒΩ)ess sup(x,y)‚àºŒ≥‚à•x‚àíy‚à•,where
(x, y)‚àºŒ≥means that the essential supremum is taken relative to measure Œ≥overRd√óRdparametrized
by(x, y).Œì(¬µ, ŒΩ)is the collection of couplings of ¬µandŒΩ.
2.1 Converting Initial W‚àûDistance to Final R√©nyi Divergence Bound
An important component of our analysis is to leverage the result of [16], which is based on the
celebrated privacy amplification by iteration analysis originally proposed in [20] and also utilized
for DP guarantees of PNSGD in [15]. The goal of [16] is to analyze the mixing time of the PNSGD
process, which can be viewed as the contractive noisy iterations due to the contractiveness of the
gradient update under strong convexity assumption.
4Definition 2.5 (Contractive Noisy Iteration ( c-CNI)) .Given an initial distribution ¬µ0‚àà P(Rd),
a sequence of (random) c-contractive (equivalently, c-Lipschitz) functions œàk:Rd7‚ÜíRd, and a
sequence of noise distributions Œ∂k, we define the c-Contractive Noisy Iteration ( c-CNI) by the update
ruleXk+1=œàk+1(Xk) +Wk+1,where Wk+1‚àºŒ∂kindependently and X0‚àº¬µ0. We denote the
law of the final iterate XKby CNI c(¬µ0,{œàk},{Œ∂k}).
Lemma 2.6 (Metric-aware privacy amplification by iteration bound [20], simplified by [16] in
Proposition 2.10) .Suppose XK‚àºCNIc(¬µ0,{œàk},{Œ∂k})andX‚Ä≤
K‚àºCNIc(¬µ‚Ä≤
0,{œàk},{Œ∂k})where
the initial distribution satisfy W‚àû(¬µ0, ¬µ‚Ä≤
0)‚â§Z, the update function œàkarec-contractive, and the
noise distributions Œ∂k=N(0, œÉ2Id). Then we have
DŒ±(XK||X‚Ä≤
K)‚â§Œ±Z2
2œÉ2c2Kifc <1
1/K ifc= 1. (1)
Roughly speaking, Lemma 2.6 shows that if we have the W‚àûdistance of initial distributions of two
PNSGD processes on the same dataset, we have the corresponding R√©nyi difference bound after
Kiterations. The projection set diameter 2Ris a default upper bound for W‚àûdistance if we do
not care about the initial distributions as the case studied in [16] for mixing time analysis. In the
unlearning scenario, the initial distributions of the unlearning processes, denoted by ŒΩD|B, ŒΩD‚Ä≤|B, are
much more relevant. We can show a much tighter bound by analyzing W‚àû(ŒΩD|B, ŒΩD‚Ä≤|B)along the
adjacent PNSGD learning processes.
3 Certified Unlearning Guaranatee for PNSGD
We start with introducing the (un)learning process with PNSGD with a cyclic mini-batch strategy
(Algorithm 1). Note that this mini-batch strategy is not only commonly used for practical DP-SGD
implementations in privacy libraries [21], but also in theoretical analysis for DP guarantees [22].
For the learning mechanism M, we optimize the objective function with PNSGD on dataset D(line
3-8 in Algorithm 1). Œ∑, œÉ2>0are hyperparameters of step size and noise variance respectively.
The initialization ŒΩ0is an arbitrary distribution supported on CRif not specified. For the unlearning
mechanism U, we fine-tune the current parameter with PNSGD on the updated dataset D‚Ä≤subject
to the unlearning request (line 10-15 in Algorithm 1) with y0
0=x0
T=M(D). For the rest of the
paper, we denote ŒΩj
t, œÅj
kas the probability density of xj
t, yj
krespectively. Furthermore, we denote
B={Bj}n/b‚àí1
j=0 the minibatch sequence described in Algorithm 1, where bis the step size and we
assume nis divided by bthroughout the paper for simplicity2. We use ŒΩ¬∑|Bto denote the conditional
distribution of ŒΩ¬∑givenB.
3.1 Certified Unlearning Guarantees
Now we introduce the certified unlearning guarantees for PNSGD and the corresponding analysis
illustrated in Figure 1. We first prove that for any fixed mini-batch sequence B, the limiting distribution
ŒΩD|Bof the learning process exists, is unique, and stationery. The proof is deferred to Appendix E
and is based on applying the results in [23] to establish the ergodicity of the learning process x0
t.
Theorem 3.1. Suppose that the closed convex set C ‚äÇRdis bounded with Chaving a positive
Lebesgue measure and that ‚àáf(¬∑;di) :C ‚Üí Rdis continuous for all i‚àà[n]. The Markov
chain{xt:=x0
t}in Algorithm 1 for any fixed mini-batch sequence Badmits a unique invariant
probability measure ŒΩD|Bon the Borel œÉ-algebra of C. Furthermore, for any x‚àà C, the distribution
ofxtconditioned on x0=xconverges weakly to ŒΩD|Bast‚Üí ‚àû , where ŒΩD|Bis the conditional
distribution of ŒΩDgivenB.
Suppose the training epoch Tis large enough so that the model is well-trained for now, which means
M(D)‚àºŒΩD|B=œÅ0
0|Band our target ‚Äúretraining distribution‚Äù is ŒΩD‚Ä≤|B. Our goal is then to upper
bound the R√©nyi difference dŒ±(œÅ0
K|B, ŒΩD‚Ä≤|B)afterKunlearning epochs. In the case of insufficient
training, the privacy loss is dŒ±(œÅ0
K|B, ŒΩ0,‚Ä≤
T|B)where M(D‚Ä≤)‚àºŒΩ0,‚Ä≤
T|BforTtraining epochs. It can
be upper bounded in terms of dŒ±(œÅ0
K|B, ŒΩD‚Ä≤|B)anddŒ±(ŒΩ0,‚Ä≤
T|B, ŒΩD‚Ä≤|B)via weak triangle inequality of
2When nis not divided by b, we can simply drop the last n‚àí ‚åän/b‚åãbpoints.
5R√©nyi divergence, which is provided in Theorem 3.2 below. We later provide a better bound by
considering the randomness of B.
Theorem 3.2 (RU guarantee of PNSGD unlearning, fixed B).Assume ‚àÄd‚àà X,f(x;d)isL-smooth,
M-Lipchitz and m-strongly convex in x. Let the learning and unlearning processes follow Algorithm 1
withy0
0=x0
T=M(D). Given any fixed mini-batch sequence B, for any Œ± >1, letŒ∑‚â§1
L, the
output of the Kthunlearning iteration satisfies (Œ±, Œµ)-RU for any adjacent dataset D,D‚Ä≤, where
Œµ‚â§Œ±‚àí1/2
Œ±‚àí1(Œµ1(2Œ±) +Œµ2(2Œ±)), Œµ1(Œ±) =Œ±(2R)2
2Œ∑œÉ2c2Tn/b, Œµ2(Œ±) =Œ±Z2
B
2Œ∑œÉ2c2Kn/b,
ZB=W‚àû(œÅ0
K|B, ŒΩD‚Ä≤|B)‚â§2RcTn/b+ min1‚àícTn/b
1‚àícn/b2Œ∑M
b,2R
,
andc= 1‚àíŒ∑m.
As we explained earlier, we need non-trivial W‚àûbounds for adjacent PNSGD processes to obtain
better unlearning guarantees when applying Lemma 2.6. We provide such results below and the
proofs are deferred to Appendix L and M respectively.
Lemma 3.3 (W‚àûbetween adjacent PNSGD learning processes) .Consider the learning process in
Algorithm 1 on adjacent datasets DandD‚Ä≤and a fixed mini-batch sequence B. Assume ‚àÄd‚àà X,
f(x;d)isL-smooth, M-Lipschitz and m-strongly convex in x. Let the index of different data point
between D,D‚Ä≤belongs to mini-batch Bj0. Then for Œ∑‚â§1
Land let c= (1‚àíŒ∑m), we have
W‚àû(ŒΩ0
T|B, ŒΩ0,‚Ä≤
T|B)‚â§min1‚àícTn/b
1‚àícn/bcn/b‚àíj0‚àí12Œ∑M
b,2R
.
Lemma 3.4 (W‚àûbetween PNSGD learning process to its stationary distribution) .Following the
same setting as in Theorem 3.2 and denote the initial distribution of the unlearning process as ŒΩ0
0.
Then we have
W‚àû(ŒΩ0
T|B, ŒΩD|B)‚â§(1‚àíŒ∑m)Tn/bW‚àû(ŒΩ0
0, ŒΩD|B).
Remark 3.5.In [15], the authors used the default projection set diameter 2Ras the W‚àûdistance
upper bound for their DP results, see equations (3.5) and (5.2) therein. However, it yields a vacuous
bound as an unlearning guarantee compared to retraining from scratch. Note that our tighter bound
forW‚àûdistance is also useful for deriving later sequential unlearning guarantees compared to the
prior work based on the analysis of Langevein dynamics [11]. Interestingly, this improved result can
also be utilized for tightening the DP guarantee in [15] and make it more practically useful, as 2R
can be very large in practice, which may be of independent interest.
We are ready to provide the sketch of proof for Theorem 3.2.
Sketch of proof. First note that the PNSGD update leads to a (1‚àíŒ∑m)-CNI process when Œ∑‚â§1/L
for any mini-batch sequence. Recall that yj
kis the unlearning process at epoch kat iteration j,
starting from y0
0=x0
T=M(D). Consider the ‚Äúadjacent‚Äù process yj,‚Ä≤
kstarting from y0,‚Ä≤
0=
x0,‚Ä≤
T=M(D‚Ä≤)but still fine-tune on D‚Ä≤so that yj
k, yj,‚Ä≤
konly differ in their initialization. Now,
consider three distributions: ŒΩD|B, ŒΩ0
T|B, œÅ0
K|Bare the stationary distribution for the learning processes,
learning process at epoch Tand unlearning process at epoch Krespectively. Similarly, consider
the ‚Äúadjacent‚Äù processes that learn on D‚Ä≤and still unlearn on D‚Ä≤(see Figure 1 for the illustration).
Denote distributions ŒΩD‚Ä≤|B, ŒΩ0,‚Ä≤
T|B, œÅ0,‚Ä≤
K|Bfor these processes similarly. Note that our goal is to bound
dŒ±(œÅ0
K|B, ŒΩ0,‚Ä≤
T|B)for the RU guarantee. By weak triangle inequality [19], we can upper bound it
in terms of d2Œ±(œÅ0
K|B, ŒΩD‚Ä≤|B)andd2Œ±(ŒΩD‚Ä≤|B, ŒΩ0,‚Ä≤
T|B), which are the Œµ2andŒµ1terms in Theorem 3.2
respectively. For dŒ±(ŒΩD‚Ä≤|B, ŒΩ0,‚Ä≤
T|B), we leverage the naive 2Rbound for the W‚àûdistance between
ŒΩD‚Ä≤|B, ŒΩ0,‚Ä≤
0|Band applying Lemma 2.6 leads to the desired result. For dŒ±(œÅ0
K|B, ŒΩD‚Ä≤|B), by triangle
inequality of W‚àûand Lemma 3.3, 3.4 one can show that the W‚àûbetween œÅ0
0|B, ŒΩD‚Ä≤|Bis bounded by
ZBin Theorem 3.2. Further applying Lemma 2.6 again completes the proof.
Remark 3.6.Our proof only relies on the bounded gradient difference ‚à•‚àáf(x;d)‚àí‚àáf(x;d‚Ä≤)‚à• ‚â§2M
‚àÄx‚ààRdand‚àÄd,d‚Ä≤‚àà X hence M-Lipchitz assumption can be replaced. In practice, we can leverage
the gradient clipping along with a ‚Ñì2regularization for the convex objective function [22].
6The convergent case. In practice, one often requires the model to be ‚Äúwell-trained‚Äù, where a similar
assumption is made in the prior unlearning literature [7, 8]. Under this assumption, we can further
simplify Theorem 3.2 into the following corollary.
Corollary 3.7. Under the same setting as of Theorem 3.2. When we additionally assume Tis
sufficiently large so that y0
0=M(D)‚àºŒΩD|B. Then for any Œ± >1andŒ∑‚â§1
L, the output of the Kth
unlearning iteration satisfies (Œ±, Œµ)-RU with c= 1‚àíŒ∑m, where
Œµ‚â§Œ±Z2
B
2Œ∑œÉ2c2Kn/b, ZB= min1
1‚àícn/b2Œ∑M
b,2R
.
For simplicity, the rest of the discussion on our PNSGD unlearning will based on the well-trained
assumption. From Corollary 3.7 one can observe that a smaller bleads to a better decaying rate
(c2Kn/b) but also a potentially worse initial distance ZB=O(1/((1‚àícn/b)b)). In general, choosing
a smaller bstill leads to less epoch for achieving the desired privacy loss. In practice, choosing btoo
small (e.g., b= 1) can not only degrade the utility but also incur instability (i.e., large variance) of the
convergent distribution ŒΩD|B, asŒΩD|Bdepends on the design of mini-batches B. One should choose a
moderate bto balance between privacy and utility, which is the unique privacy-utility-complexity
trade-off with respect to brevealed by our analysis.
Computational benefit compared to retraining. In the view of Corollary 3.7 or Lemma 2.6, it is
not hard to see that a smaller initial W‚àûdistance leads to fewer PNSGD (un)learning epochs for
being Œµ-close to a target distribution ŒΩD‚Ä≤|Bin terms of R√©nyi difference dŒ±. For PNSGD unlearning,
we have provided a uniform upper bound ZB=O(Œ∑M/((1‚àícn/b)b))of such initial W‚àûdistance
in Lemma 3.3. On the other hand, even if both ŒΩ0, ŒΩDare both Gaussian with identical various and
mean difference norm of ‚Ñ¶(1) , we have W‚àû(ŒΩ0, ŒΩD‚Ä≤|B) = ‚Ñ¶(1) for retraining from scratch. Our
results show that a larger mini-batch size bleads to more significant complexity savings compared to
retraining. As we discussed above, one should choose a moderate size bto balance between privacy
and utility. In our experiment, we show that for commonly used mini-batch sizes (i.e., b‚â•32), our
PNSGD unlearning is still much more efficient in complexity compared to retraining.
Improved bound with randomized B.So far our results are based on a fixed (worst-case) mini-batch
sequence B. One can improve the privacy bound in Corollary 3.7 by taking the randomness of Binto
account under a non-adaptive unlearning setting. That is, the unlearning request is independent of the
mini-batch sequence B. See also our discussion in the related work. By taking the average of the
bound in Corollary 3.7 in conjunction with an application of joint convexity of KL divergence [22],
we can derive an improved guarantee beyond the worst-case of B.
Corollary 3.8. Under the same setting as of Theorem 3.2 but with random mini-batch sequences
described in Algorithm 1. Then for any Œ± >1, and Œ∑‚â§1
L, the output of the Kthunlearning iteration
satisfies (Œ±, Œµ)-RU with c= 1‚àíŒ∑m, where Œµ‚â§1
Œ±‚àí1log
EBexp
Œ±(Œ±‚àí1)Z2
B
2Œ∑œÉ2c2Kn/b
andZBis
the bound described in Lemma 3.3.
Different mini-batch sampling strategies. We remark that our analysis can be extended to other
mini-batch sampling strategies, such as sampling without replacement for each iteration. However,
this strategy leads to a worse ZBin our analysis of Lemma 3.3, which may seem counter-intuitive at
first glance. This is due to the nature of the essential supremum taken in W‚àû. Although sampling
without replacement leads to a smaller probability of sampling the index that gets modified due to
the unlearning request, it is still non-zero for each iteration. Thus the worst-case difference 2Œ∑M/b
between two adjacent learning processes in the mini-batch gradient update occurs at each iteration,
which degrades the factor 1/(1‚àícn/b)to1/(1‚àíc)in Lemma 3.3. As a result, we choose to adopt
the cyclic mini-batch strategy so that such a difference is guaranteed to occur only once per epoch
and thus a better bound on W‚àû.
Discussion on utility bound. One can leverage the utility analysis in section 5 of [24] to derive the
utility guarantee for the full batch setting b=n. We relegate the proof to Appendix K.
Proposition 3.9. Under the same setting as Corollary 3.7 with b=n,Œ∑‚â§m
2L2and assume the
minimizer of any fDis in the relative interior of CR‚äÜRd, for any given adjacent dataset pair D,D‚Ä≤
the output of the Kthunlearning iteration y0
Ksatisfies
E[fD‚Ä≤(y0
K)‚àíinf
x‚ààCRfD‚Ä≤(x)]‚â§McKmin(1
1‚àíc2Œ∑M
n,2R) +2LdœÉ2
m. (2)
7Note that a similar analysis applies to both the mini-batch (i.e., b‚â§n) and non-convergent case (i.e.,
Theorem 3.2) but the result is more complicated. We leave the rigorous analysis as future work. An
important remark is that the second term2LdœÉ2
mis the excess risk of ŒΩD‚Ä≤, which is controlled by the
noise scale œÉ. This presents the privacy-utility trade-off as demonstrated in the DP scenario in [24].
Without strong convexity. Since Lemma 2.6 also applies to the convex-only case (i.e., m= 0so
thatc= 1), repeating the same analysis leads to the following extension.
Corollary 3.10. Under the same setting as of Theorem 3.2 but without strong convexity (i.e., m= 0).
When we additionally assume Tis sufficiently large so that y0
0=M(D)‚àºŒΩD|B. Then for any Œ± >1,
andŒ∑‚â§1
L, the output of the Kthunlearning iteration satisfies (Œ±, Œµ)-RU, where
Œµ‚â§Œ±Z2
B
2Œ∑œÉ2b
Kn, ZB= min2Œ∑MT
b,2R
.
There are several remarks for Corollary 3.10. First, the privacy loss now only decays linearly instead
of exponentially as opposed to the strongly convex case. Second, ZBnow can grow linearly in
training epoch T. As a result, the computational benefit of our approach compared to retraining
may vanish for large Tsuch that2Œ∑MT
b‚â•2R. Nevertheless, the computational benefit against
retraining persists for moderate Tsuch that2Œ∑MT
b<2R. This condition can be met if the model
learns reasonably well with moderate Tand the projection diameter 2Ris not set to be extremely
small. For example, with 2R= 10, b= 128 , Œ∑= 1andM= 1, any training epoch T <640will
lead to2Œ∑MT
b<2R. Still, we conjecture a better analysis is needed beyond strong convexity.
3.2 Unlearning Multiple Data Points
So far we have focused on one unlearning request and unlearning one point. In practice, multiple
unlearning requests can arrive sequentially (sequential unlearning) and each unlearning request may
require unlearning multiple points (batch unlearning). Below we demonstrate that our PNSGD
unlearning naturally supports sequential and batch unlearning as well.
Sequential unlearning. As long as we can characterize the initial W‚àûdistance for any mini-batch
sequences, we have the corresponding (Œ±, Œµ)-RU guarantee due to Corollary 3.7. Thanks to our
geometric view of the unlearning problem (Figure 2) and W‚àûis indeed a metric, applying triangle
inequality naturally leads to an upper bound on the initial W‚àûdistance. By combining Lemma 3.3
and Lemma 3.4, we have the following sequential unlearning guarantee.
Theorem 3.11 (W‚àûbound for sequential unlearning) .Under the same assumptions as in Corol-
lary 3.7. Assume the unlearning requests arrive sequentially such that our dataset changes from
D=D0‚Üí D 1‚Üí. . .‚Üí D S, where Ds,Ds+1are adjacent. Let yj,(s)
k be the unlearned pa-
rameters for the sthunlearning request at kthunlearning epoch and jthiteration following Al-
gorithm (1)onDsandy0,(s+1)
0 =y0,(s)
Ks‚àº¬ØŒΩDs|B, where y0,(0)
0=x‚àûandKsis the unlearning
steps for the sthunlearning request. For any s‚àà[S], we have W‚àû(¬ØŒΩDs‚àí1|B, ŒΩDs|B)‚â§Z(s)
B,where
Z(s+1)
B = min( cKsn/bZ(s)
B+ZB,2R),Z(1)
B=ZB,ZBandcare described in Corollary 3.7.
ùúàÃÖùíü‡∞≠|‚Ñ¨ùëç‚Ñ¨(‡¨µ)=ùëç‚Ñ¨ùúàùíü|‚Ñ¨ (a)
ùëç‚Ñ¨(‡Øå)(b)
Unlearning process ùí∞ùëç‚Ñ¨
ùëç‚Ñ¨(‡¨∂)ùëç‚Ñ¨(‡Ø¶‡¨æ‡¨µ)
‚â§ùëê‡ØÑ‡Ø°
‡Øïùëç‚Ñ¨(‡Ø¶)+ùëç‚Ñ¨
(Triangle inequality)
ùëê‚â§ 1ùëç‚Ñ¨ 
ùëë‡∞à‚Ñ¨
ùëë‡∞à‚Ñ¨ùúàùíü‡∞≠|‚Ñ¨ 
ùúàùíü‡∞Æ|‚Ñ¨ ùúàÃÖùíü‡∞Æ|‚Ñ¨ùúàùíü|‚Ñ¨ 
ùúàùíü‡∞≠|‚Ñ¨ 
ùúàùíü‡≥Ñ|‚Ñ¨ 
Figure 2: Illustration of (a) sequential unlearning and (b)
batch unlearning. The key idea is to establish an upper
bound on the initial W‚àûdistance. (a) For sequential un-
learning, the initial W‚àûdistance bound Z(s)
Bfor each sth
unlearning request can be derived with triangle inequality.
(b) For batch unlearning, we analyze the case that two
learning processes can differ in S‚â•1points.By combining Corollary 3.7 and Theo-
rem 3.11, we can establish the least un-
learning iterations of each unlearning re-
quest{Ks}S
s=1to achieve (Œ±, Œµ)-RU si-
multaneously. Notably, our sequential un-
learning bound is much better than the
one in [11], especially when the num-
ber of unlearning requests is large. The
key difference is that [11] have to lever-
age weak triangle inequality for R√©nyi
divergence, which double the R√©nyi di-
vergence order Œ±for each sequential un-
learning request. In contrast, since our
analysis only requires tracking the initial
8/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=n
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=/uni00000014/uni00000015/uni0000001b
/uni0000002f/uni00000038/uni0000000f/uni00000003K=/uni00000014
/uni00000027/uni00000015/uni00000027/uni0000000f/uni00000003K=/uni00000014
/uni00000027/uni00000015/uni00000027/uni0000000f/uni00000003K=/uni00000018
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/epsilon1(a) Unlearning one point
/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=n
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=/uni00000014/uni00000015/uni0000001b
/uni0000002f/uni00000038/uni0000000f/uni00000003S=/uni00000014/uni00000013
/uni00000027/uni00000015/uni00000027
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000003/uni00000003/uni00000003/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000058/uni00000051/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003K
/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000058/uni00000051/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044 (b) Unlearning 100points
/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=/uni00000016/uni00000015
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=/uni00000014/uni00000015/uni0000001b
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=/uni00000018/uni00000014/uni00000015
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=n/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni00000003/uni00000035/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni0000000f/uni00000003b=/uni00000016/uni00000015
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni00000003/uni00000035/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni0000000f/uni00000003b=/uni00000014/uni00000015/uni0000001b
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni00000003/uni00000035/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni0000000f/uni00000003b=/uni00000018/uni00000014/uni00000015
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni00000003/uni00000035/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni0000000f/uni00000003b=n
/uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013
/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni00000055/uni00000047/uni00000003/uni00000047/uni00000048/uni00000059/uni0000004c/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003œÉ/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c (c) Noise v.s. accuracy
/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013
/uni0000001a/uni00000013/uni00000015/uni00000019 /uni00000018/uni00000018/uni00000015/uni00000018 /uni00000017/uni00000014/uni0000001a/uni00000014 /uni00000015/uni00000019/uni00000014/uni00000017 /uni00000014/uni0000001a/uni0000001a/uni00000014/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=/uni00000016/uni00000015/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051=/uni00000014/uni00000013/uni00000013/uni00000013/uni0000000c
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=/uni00000014/uni00000015/uni0000001b/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051=/uni00000015/uni00000013/uni00000013/uni00000013/uni0000000c
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=/uni00000018/uni00000014/uni00000015/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051=/uni00000018/uni00000013/uni00000013/uni00000013/uni0000000c
/uni00000033/uni00000031/uni00000036/uni0000002a/uni00000027/uni0000000f/uni00000003b=n/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051=/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni0000000c
/uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013
/uni0000001b/uni00000019/uni00000016/uni00000015 /uni00000019/uni0000001c/uni00000016/uni00000013 /uni00000018/uni00000016/uni00000015/uni0000001a /uni00000016/uni00000017/uni00000015/uni00000013 /uni00000015/uni00000016/uni00000014/uni00000013
/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000056/uni00000057/uni00000044/uni00000051/uni00000047/uni00000044/uni00000055/uni00000047/uni00000003/uni00000047/uni00000048/uni00000059/uni0000004c/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003œÉ/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000058/uni00000051/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003K (d) Noise v.s. complexity
Figure 3: Main experiments, where the top and bottom rows are for MNIST and CIFAR10 respectively.
(a) Compare to baseline for unlearning one point using limited Kunlearning epoch. For PNSGD,
we use only K= 1unlearning epoch. For D2D, we allow it to use K= 1,5unlearning epochs. (b)
Unlearning 100points sequentially versus baseline. For LU, since their unlearning complexity only
stays in a reasonable range when combined with batch unlearning of size Ssufficiently large, we
report such a result only. (c,d) Noise-accuracy-complexity trade-off of PNSGD for unlearning 100
points sequentially with various mini-batch sizes b, where all methods achieve (œµ,1/n)-unlearning
guarantee with œµ= 0.01. We also report the required accumulated epochs for retraining for each b.
W‚àûdistance, where the standard triangle inequality can be applied. As a result, our analysis can
better handle the sequential unlearning case. We also demonstrate in Section 4 that the benefit offered
by our results is significant in practice.
Batch unlearning. We can extend Lemma 3.3 to the case that adjacent dataset D,D‚Ä≤can differ in
S‚â•1points, which further leads to batch unlearning guarantee. We relegate the result in Appendix J.
4 Experiments
Benchmark datasets. We consider binary logistic regression with ‚Ñì2regularization. We conduct
experiments on MNIST [25] and CIFAR10 [26], which contain 11,982 and 10,000 training instances
respectively. We follow the setting of [7, 11] to distinguish digits 3and8for MNIST so that the
problem is a binary classification. For the CIFAR10 dataset, we distinguish labels 3(cat) and 8(ship)
and leverage the last layer of the public ResNet18 [27] embedding as the data features, which follows
the setting of [7] with public feature extractor.
Baseline methods. Our baseline methods include Delete-to-Descent (D2D) [9] and Langevin Un-
learning (LU) [11], which are the state-of-the-art full-batch gradient-based approximate unlearning
methods. Note that when our PNSGD unlearning chooses b=n(i.e., full batch), the learning
and unlearning iterations become PNGD which is identical to LU. Nevertheless, the corresponding
privacy bound is still different as we leverage the analysis different from those based on Langevin
dynamics in [11]. Hence, we still treat these two methods differently in our experiment. For D2D,
we leverage Theorem 9 and 28 in [9] for privacy accounting depending on whether we allow D2D
to have an internal non-private state. Note that allowing an internal non-private state provides a
weaker notion of privacy guarantee [9] and both PNSGD and LU by default do not require it. We
include those theorems for D2D and a detailed explanation of its possible non-privacy internal state
in Appendix O. For LU, we leverage their Theorem 3.2, and 3.3 for privacy accounting [11], which
are included in Appendix P.
All experimental details can be found in Appendix N, including how to convert (Œ±, Œµ)-RU to the
standard (œµ, Œ¥)-unlearning guarantee. Our code is publicly available3. We choose Œ¥= 1/nfor each
dataset and require all tested unlearning approaches to achieve (œµ, Œ¥)-unlearning with different œµ.
We report test accuracy for all experiments as the utility metric. We set the learning iteration T=
10,20,50,1000 to ensure PNSGD converges for mini-batch size b= 32,128,512, nrespectively.
All results are averaged over 100independent trials with standard deviation reported as shades in
figures.
3https://github.com/Graph-COM/SGD_unlearning
9Unlearning one data point with K= 1 epoch. We first consider the setting of unlearning one
data point using only one unlearning epoch (Figure 3a). For both LU and PNSGD, we use only
K= 1 unlearning epoch. Since D2D cannot achieve a privacy guarantee with only limited (i.e.,
less than 10) unlearning epoch without a non-private internal state, we allow D2D to have it and set
K= 1,5in this experiment. Even in this case, PNSGD still outperforms D2D in both utility and
unlearning complexity. Compared to LU, our mini-batch setting either outperforms or is on par with
it. Interestingly, we find that LU gives a better privacy bound compared to full-batch PNSGD ( b=n)
and thus achieves better utility under the same privacy constraint, see Appendix P for the detailed
comparisons. Nevertheless, due to the use of weak triangle inequality in LU analysis, we will see that
our PNSGD can outperform LU significantly for multiple unlearning requests.
Unlearning multiple data points. Let us consider the case of multiple ( 100) unlearning requests
(Figure 3b). We let all methods achieve the same (1,1/n)-unlearning guarantee for a fair comparison.
We do not allow D2D to have an internal non-private state anymore in this experiment for a fair
comparison. Since the privacy bound of LU only gives reasonable unlearning complexity with a
limited number of sequential unlearning updates [11], we allow it to unlearn S= 10 points at once.
We observe that PNSGD requires roughly 10% and2%of unlearning epochs compared to D2D
and LU for b=nandb= 128 respectively, where all methods exhibit similar utility ( 0.9and0.98
for MNIST and CIFAR10 respectively). It shows that PNSGD is much more efficient compared
to D2D and LU. Notably, while both PNSGD with b=nand LU (un)learn with PNGD iterations,
the resulting privacy bound based on our PABI-based analysis is superior to the one pertaining to
Langevin-dynamic-based analysis in [11]. See our discussion in Section 3.2 for the full details.
Privacy-utility-complexity trade-off. We now investigate the inherent utility-complexity trade-off
regarding noise standard deviation œÉand mini-batch size bfor PNSGD under the same privacy con-
straint, where we require all methods to achieve (0.01,1/n)-unlearning guarantee for 100sequential
unlearning requests (Figure 3c and 3d). We can see that smaller œÉleads to a better utility, yet more
unlearning epochs are needed for PNSGD to achieve œµ= 0.01. On the other hand, smaller mini-batch
sizebrequires fewer unlearning epochs Kas shown in Figure 3d, since more unlearning iterations
are performed per epoch. Nevertheless, we remark that choosing btoo small may lead to degradation
of model utility or instability. Decreasing the mini-batch size bfrom 32to1reduces the average
accuracy of training from scratch from 0.87to0.64and0.97to0.81on MNIST and CIFAR10
respectively for œÉ= 0.03. In practice, one should choose a moderate mini-batch size bto ensure both
good model utility and unlearning complexity. Finally, we also note that PNSGD achieves a similar
utility while much better complexity compared to retraining from scratch, where PNSGD requires at
most 1,5unlearning epochs per unlearning request for b= 32,512respectively.
5 Limitations and Conclusion
Limitation. Since our analysis is built on the works of [15, 16], we share the same limitation
that the (strong) convexity assumption is required. It is an open problem on how to extend such
analysis beyond convexity assumption as stated in [15, 16]. While we resolve this open problem for
establishing DP properties of PNSGD in our recent work [28], it is still unclear whether the same
success can be generalized to the unlearning problem. One interesting direction is to leverage the
Langevin dynamic analysis [29] instead as in [11], which can deal with non-convex problems in
theory yet we conjecture the resulting bounds can be loose, and more complicated.
Conclusion. We propose to leverage projected noisy stochastic gradient descent (PNSGD) for
machine unlearning problem. We provide its unlearning guarantees as well as many other algorithmic
benefits of PNSGD for unlearning under the convexity assumption. Our results are closely related
to our new results on infinite Wasserstein distance tracking of the adjacent (un)learning processes,
which is also leveraged in our concurrent work for studying DP-PageRank algorithms [30]. Extensive
experiments show that our approach achieves a similar utility under the same privacy constraint while
using 2%and10% of the gradient computations compared with the state-of-the-art gradient-based
approximate unlearning methods for mini-batch and full-batch settings, respectively.
10Acknowledgments and Disclosure of Funding
The authors thank Sinho Chewi, Wei-Ning Chen, and Ayush Sekhari for the helpful discussions. E.
Chien, H. Wang and P. Li are supported by NSF awards OAC-2117997 and JPMC faculty award.
References
[1]N. Carlini, C. Liu, √ö. Erlingsson, J. Kos, and D. Song, ‚ÄúThe secret sharer: Evaluating and
testing unintended memorization in neural networks,‚Äù in 28th USENIX Security Symposium
(USENIX Security 19) , pp. 267‚Äì284, 2019.
[2]C. Guo, F. Bordes, P. Vincent, and K. Chaudhuri, ‚ÄúDo ssl models have d \‚Äôej\a vu? a case of
unintended memorization in self-supervised learning,‚Äù arXiv preprint arXiv:2304.13850 , 2023.
[3]Y . Cao and J. Yang, ‚ÄúTowards making systems forget with machine unlearning,‚Äù in 2015 IEEE
symposium on security and privacy , pp. 463‚Äì480, IEEE, 2015.
[4] L. Bourtoule, V . Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie,
and N. Papernot, ‚ÄúMachine unlearning,‚Äù in 2021 IEEE Symposium on Security and Privacy
(SP), pp. 141‚Äì159, IEEE, 2021.
[5]E. Ullah, T. Mai, A. Rao, R. A. Rossi, and R. Arora, ‚ÄúMachine unlearning via algorithmic
stability,‚Äù in Conference on Learning Theory , pp. 4126‚Äì4142, PMLR, 2021.
[6]E. Ullah and R. Arora, ‚ÄúFrom adaptive query release to machine unlearning,‚Äù in International
Conference on Machine Learning , pp. 34642‚Äì34667, PMLR, 2023.
[7]C. Guo, T. Goldstein, A. Hannun, and L. Van Der Maaten, ‚ÄúCertified data removal from machine
learning models,‚Äù in International Conference on Machine Learning , pp. 3832‚Äì3842, PMLR,
2020.
[8]A. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh, ‚ÄúRemember what you want to forget:
Algorithms for machine unlearning,‚Äù Advances in Neural Information Processing Systems ,
vol. 34, pp. 18075‚Äì18086, 2021.
[9]S. Neel, A. Roth, and S. Sharifi-Malvajerdi, ‚ÄúDescent-to-delete: Gradient-based methods for
machine unlearning,‚Äù in Algorithmic Learning Theory , pp. 931‚Äì962, PMLR, 2021.
[10] E. Chien, C. Pan, and O. Milenkovic, ‚ÄúEfficient model updates for approximate unlearning of
graph-structured data,‚Äù in The Eleventh International Conference on Learning Representations ,
2022.
[11] E. Chien, H. Wang, Z. Chen, and P. Li, ‚ÄúLangevin unlearning: A new perspective of noisy
gradient descent for machine unlearning,‚Äù Advances in neural information processing systems ,
2024.
[12] C. Dwork, F. McSherry, K. Nissim, and A. Smith, ‚ÄúCalibrating noise to sensitivity in private
data analysis,‚Äù in Theory of Cryptography: Third Theory of Cryptography Conference, TCC
2006, New York, NY, USA, March 4-7, 2006. Proceedings 3 , pp. 265‚Äì284, Springer, 2006.
[13] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, ‚ÄúDeep
learning with differential privacy,‚Äù in Proceedings of the 2016 ACM SIGSAC conference on
computer and communications security , pp. 308‚Äì318, 2016.
[14] B. Balle, G. Barthe, and M. Gaboardi, ‚ÄúPrivacy amplification by subsampling: Tight analyses
via couplings and divergences,‚Äù Advances in neural information processing systems , vol. 31,
2018.
[15] J. Altschuler and K. Talwar, ‚ÄúPrivacy of noisy stochastic gradient descent: More iterations
without more privacy loss,‚Äù Advances in Neural Information Processing Systems , vol. 35,
pp. 3788‚Äì3800, 2022.
[16] J. M. Altschuler and K. Talwar, ‚ÄúResolving the mixing time of the langevin algorithm to its
stationary distribution for log-concave sampling,‚Äù arXiv preprint arXiv:2210.08448 , 2022.
[17] V . Gupta, C. Jung, S. Neel, A. Roth, S. Sharifi-Malvajerdi, and C. Waites, ‚ÄúAdaptive machine
unlearning,‚Äù Advances in Neural Information Processing Systems , vol. 34, pp. 16319‚Äì16330,
2021.
11[18] R. Chourasia and N. Shah, ‚ÄúForget unlearning: Towards true data-deletion in machine learning,‚Äù
inInternational Conference on Machine Learning , pp. 6028‚Äì6073, PMLR, 2023.
[19] I. Mironov, ‚ÄúR√©nyi differential privacy,‚Äù in 2017 IEEE 30th computer security foundations
symposium (CSF) , pp. 263‚Äì275, IEEE, 2017.
[20] V . Feldman, I. Mironov, K. Talwar, and A. Thakurta, ‚ÄúPrivacy amplification by iteration,‚Äù in
2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS) , pp. 521‚Äì532,
IEEE, 2018.
[21] A. Yousefpour, I. Shilov, A. Sablayrolles, D. Testuggine, K. Prasad, M. Malek, J. Nguyen,
S. Ghosh, A. Bharadwaj, J. Zhao, et al. , ‚ÄúOpacus: User-friendly differential privacy library in
pytorch,‚Äù arXiv preprint arXiv:2109.12298 , 2021.
[22] J. Ye and R. Shokri, ‚ÄúDifferentially private learning needs hidden state (or much faster conver-
gence),‚Äù Advances in Neural Information Processing Systems , vol. 35, pp. 703‚Äì715, 2022.
[23] S. P. Meyn and R. L. Tweedie, Markov chains and stochastic stability . Springer Science &
Business Media, 2012.
[24] R. Chourasia, J. Ye, and R. Shokri, ‚ÄúDifferential privacy dynamics of langevin diffusion and
noisy gradient descent,‚Äù Advances in Neural Information Processing Systems , vol. 34, pp. 14771‚Äì
14781, 2021.
[25] L. Deng, ‚ÄúThe mnist database of handwritten digit images for machine learning research,‚Äù IEEE
Signal Processing Magazine , vol. 29, no. 6, pp. 141‚Äì142, 2012.
[26] A. Krizhevsky et al. , ‚ÄúLearning multiple layers of features from tiny images,‚Äù 2009.
[27] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image recognition,‚Äù in
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770‚Äì778,
2016.
[28] E. Chien and P. Li, ‚ÄúConvergent privacy loss of noisy-sgd without convexity and smoothness,‚Äù
2024.
[29] Chewi, Sinho, ‚ÄúLog-Concave Sampling.‚Äù https://chewisinho.github.io/main.pdf ,
2023. Online; accessed September 29, 2023.
[30] R. Wei, E. Chien, and P. Li, ‚ÄúDifferentially private graph diffusion with applications in person-
alized pageranks,‚Äù Advances in neural information processing systems , 2024.
[31] A. Ganesh and K. Talwar, ‚ÄúFaster differentially private samplers via r√©nyi divergence analysis
of discretized langevin mcmc,‚Äù Advances in Neural Information Processing Systems , vol. 33,
pp. 7222‚Äì7233, 2020.
[32] T. Ryffel, F. Bach, and D. Pointcheval, ‚ÄúDifferential privacy guarantees for stochastic gradient
langevin dynamics,‚Äù arXiv preprint arXiv:2201.11980 , 2022.
[33] S. Vempala and A. Wibisono, ‚ÄúRapid convergence of the unadjusted langevin algorithm:
Isoperimetry suffices,‚Äù Advances in neural information processing systems , vol. 32, 2019.
[34] T. maintainers and contributors, ‚ÄúTorchvision: Pytorch‚Äôs computer vision library.‚Äù https:
//github.com/pytorch/vision , 2016.
[35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,
S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, ‚ÄúPytorch: An imperative style,
high-performance deep learning library,‚Äù in Advances in Neural Information Processing Systems
32, pp. 8024‚Äì8035, Curran Associates, Inc., 2019.
[36] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison,
L. Antiga, and A. Lerer, ‚ÄúAutomatic differentiation in pytorch,‚Äù 2017.
[37] C. R. Harris, K. J. Millman, S. J. Van Der Walt, R. Gommers, P. Virtanen, D. Cournapeau,
E. Wieser, J. Taylor, S. Berg, N. J. Smith, et al. , ‚ÄúArray programming with numpy,‚Äù Nature ,
vol. 585, no. 7825, pp. 357‚Äì362, 2020.
[38] P. Kairouz, B. McMahan, S. Song, O. Thakkar, A. Thakurta, and Z. Xu, ‚ÄúPractical and pri-
vate (deep) learning without sampling or shuffling,‚Äù in International Conference on Machine
Learning , pp. 5213‚Äì5225, PMLR, 2021.
12A Additional related works
Differential privacy of noisy gradient methods. DP-SGD [13] is arguably the most popular method
for ensuring a DP guarantee for machine learning models. Since it leverages the DP composition
theorem and thus the privacy loss will diverge for infinite training epochs. Recently, researchers
have found that if we only release the last step of the trained model, then we can do much better
than applying the composition theorem. A pioneer work [31] studied the DP properties of Langevin
Monte Carlo methods. Yet, they do not propose to use noisy GD for general machine learning
problems. A recent line of work [22, 32] shows that PNSGD training can not only provide DP
guarantees, but also the privacy loss is at most a finite value even if we train with an infinite number
of iterations. The main analysis therein is based on the analysis of Langevin Monte Carlo [29, 33]. In
the meanwhile, [15] also provided the DP guarantees for PNSGD training but with analysis based on
privacy amplification by iteration [20]. None of these works study how PNSGD can also be leveraged
for machine unlearning.
B Limitations
Since our analysis is built on the works of [15, 16], we share the same limitation that the (strong)
convexity assumption is required. It is an open problem on how to extend such analysis beyond
convexity assumption as stated in [15, 16]. While we resolve this open problem for the DP properties
of PNSGD in our recent work [28], it is still unclear whether the same success can be generalized to
the unlearning problem. One interesting direction is to leverage the Langevin dynamic analysis [29]
instead as in [11], which can deal with non-convex problems in theory yet we conjecture the resulting
bounds can be loose, and more complicated.
C Broader Impact
Our work study the theoretical unlearning guarantees of projected stochastic noisy gradient descent
algorithm for convex problems. We believe our work is a foundational research and does not have a
direct path to any negative applications.
D Standard definitions
Letf:Rd7‚ÜíRbe a mapping. We define smoothness, Lipschitzness, and strong convexity as
follows:
L-smooth: ‚àÄx, y‚ààRd,‚à•‚àáf(x)‚àí ‚àáf(y)‚à• ‚â§L‚à•x‚àíy‚à• (3)
m-strongly convex: ‚àÄx, y‚ààRd,‚ü®x‚àíy,‚àáf(x)‚àí ‚àáf(y)‚ü© ‚â•m‚à•x‚àíy‚à•2(4)
M-Lipschitzs: ‚àÄx, y‚ààRd,‚à•f(x)‚àíf(y)‚à• ‚â§M‚à•x‚àíy‚à•. (5)
Furthermore, we say fis convex means it is 0-strongly convex.
E Existence of limiting distribution
Theorem. Suppose that the closed convex set C ‚äÇRdis bounded with Chaving a positive Lebesgue
measure and that ‚àáf(¬∑;di) :C ‚ÜíRdis continuous for all i‚àà[n]. The Markov chain {xt:=x0
t}in
Algorithm 1 for any fixed mini-batch sequence B={Bj}n/b‚àí1
j=1 admits a unique invariant probability
measure ŒΩD|Bon the Borel œÉ-algebra of C. Furthermore, for any x‚àà C, the distribution of xt
conditioned on x0=xconverges weakly to ŒΩD|Bast‚Üí ‚àû .
The proof is almost identical to the proof of Theorem 3.1 in [11] and we include it for completeness.
We start by proving that the process {xt:=x0
t}admits a unique invariant measure (Proposition E.1)
and then show that the process converges to such measure which is in fact a probability measure
(Theorem E.2). Combining these two results completes the proof of Theorem 3.1.
Proposition E.1. Suppose that the closed convex set C ‚äÇRdis bounded with Leb(C)>0where Leb
denotes the Lebesgue measure and that ‚àáf(¬∑;di) :C ‚ÜíRdis continuous for all i‚àà[n]. Then the
13Markov chain {xt:=x0
t}defined in Algorithm 1 for any fixed mini-batch sequence B={Bj}n/b‚àí1
j=1
admits a unique invariant measure (up to constant multiples) on B(C)that is the Borel œÉ-algebra of
C.
Proof. This proposition is a direct application of results from [23]. According to Proposition 10.4.2
in [23], it suffices to verify that {xt}is recurrent and strongly aperiodic.
1.Recurrency. Thanks to the Gaussian noise Wt,{xt}isLeb-irreducible, i.e., it holds for any
x‚àà Cand any A‚àà B(C)with Leb (A)>0that
L(x, A) :=P(œÑA<+‚àû |x0=x)>0,
where œÑA= inf{t‚â•0 :xt‚ààA}is the stopping time. Therefore, there exists a Borel
probability measure œàsuch that that {xt}isœà-irreducible and œàis maximal in the sense
of Proposition 4.2.2 in [23]. Consider any A‚àà B(C)withœà(A)>0. Since {xt}is
œà-irreducible, one has L(x, A) =P(œÑA<+‚àû |x0=x)>0for all x‚àà C. This
implies that there exists T‚â•0,Œ¥ > 0, and B‚àà B(C)with Leb(B)>0, such that
P(xT‚ààA|x0=x)‚â•Œ¥,‚àÄx‚ààB. Therefore, one can conclude for any x‚àà Cthat
U(x, A) :=‚àûX
t=0P(xt‚ààA|x0=x)
‚â•‚àûX
t=1P(xt+T‚ààA|xt‚ààB, x 0=x)¬∑P(xt‚ààB|x0=x)
‚â•‚àûX
t=1Œ¥¬∑inf
y‚ààCP(xt‚ààB|xt‚àí1=y)
= +‚àû,
where we used the fact that infy‚ààCP(xt‚ààB|xt‚àí1=y) = inf y‚ààCP(x1‚ààB|x0=y)>
0that is implies by Leb(B)>0and the boundedness of CandS
i‚ààBn/b‚àáf(C;di). Let us
remark that we actually have compactS
i‚ààBn/b‚àáf(C;di)sinceCis compact and ‚àáf(¬∑;di)
is continuous. The arguments above verify that {xt}is recurrent (see Section 8.2.3 in [23]
for definition).
2.Strong aperiodicity. SinceCandS
i‚ààBn/b‚àáf(C;di)are bounded and the density of Wthas
a uniform positive lower bound on any bounded domain, there exists a non-zero multiple of
the Lebesgue measure, say ŒΩ1, satisfying that
P(x1‚ààA|x0=x)‚â•ŒΩ1(A),‚àÄx‚àà C, A‚àà B(C).
Then{xt}is strongly aperiodic by the equation above and ŒΩ1(C)>0(see Section 5.4.3
in [23] for definition).
The proof is hence completed.
Theorem E.2. Under the same assumptions as in Proposition E.1, the Markov chain {xt}admits a
unique invariant probability measure ŒΩD|BonB(C). Furthermore, for any x‚àà C, the distribution of
xt=x0
tgenerated by the learning process in Algorithm 1 conditioned on x0=xconverges weakly
toŒΩD|Bast‚Üí ‚àû .
Proof. It has been proved in Proposition E.1 that {xt}is strongly aperiodic and recurrent with an
invariant measure. Consider any A‚àà B(C)withœà(A)>0and use the same settings and notations
as in the proof of Proposition E.1. There exists T‚â•0,Œ¥ >0, andB‚àà B(C)with Leb(B)>0, such
thatP(xT‚ààA|x0=x)‚â•Œ¥,‚àÄx‚ààB. This implies that for any t‚â•0and any x‚àà C,
P(xt+T+1‚ààA|xt=x) =P(xT+1‚ààA|x0=x)
‚â•P(xT+1‚ààA|x1‚ààB, x 0=x)¬∑P(x1‚ààB|x0=x)‚â•œµ,
where
œµ=Œ¥¬∑inf
y‚ààCP(x1‚ààB|x0=y)>0,
14which then leads to
Q(x, A) :=P(xt‚ààA,infinitely often ) = +‚àû.
This verifies that the chain {xt}is Harris recurrent (see Section 9 in [23] for definition). It can be
further derived that for any x‚àà C,
E(œÑA|x0=x) =‚àûX
t=1P(œÑA‚â•t|x0=x)‚â§(T+ 1)‚àûX
k=0P(œÑA>(T+ 1)k|x0=x)
‚â§(T+ 1)‚àûX
k=1(1‚àíœµ)k<+‚àû.
The bound above is uniform for all x‚àà Cand this implies that Cis a regular set of {xt}(see Section
11 in [23] for definition). Finally, one can apply Theorem 13.0.1 in [23] to conclude that there exists
a unique invariant probability measure ŒΩDonB(C)and that the distribution of xtconverges weakly
toŒΩD|Bconditioned on x0=xfor any x‚àà C.
F Proof of Theorem 3.2
Theorem (RU guarantee of PNSGD unlearning, fixed B).Assume ‚àÄd‚àà X,f(x;d)isL-smooth, M-
Lipchitz and m-strongly convex in x. Let the learning and unlearning processes follow Algorithm 1
withy0
0=x0
T=M(D). Given any fixed mini-batch sequence B, for any Œ± >1, letŒ∑‚â§1
L, the
output of the Kthunlearning iteration satisfies (Œ±, Œµ)-RU with c= 1‚àíŒ∑m, where
Œµ‚â§Œ±‚àí1/2
Œ±‚àí1(Œµ1(2Œ±) +Œµ2(2Œ±)),
Œµ1(Œ±) =Œ±(2R)2
2Œ∑œÉ2c2Tn/b, Œµ2(Œ±) =Œ±Z2
B
2Œ∑œÉ2c2Kn/b,
ZB= 2RcTn/b+ min1‚àícTn/b
1‚àícn/b2Œ∑M
b,2R
.
We first introduce an additional definition and a lemma needed for the our analysis.
Definition F.1 (Shifted R√©nyi divergence) .Let¬µandŒΩbe distributions defined on a Banach space
(Rd,‚à• ¬∑ ‚à•). For parameters z‚â•0andŒ±‚â•1, thez-shifted R√©nyi divergence between ¬µandŒΩis
defined as
D(z)
Œ±(¬µ||ŒΩ) = inf
¬µ‚Ä≤:W‚àû(¬µ,¬µ‚Ä≤)‚â§zDŒ±(¬µ‚Ä≤||ŒΩ). (6)
Lemma F.2 (Data-processing inequality for R√©nyi divergence, Lemma 2.6 in [16]) .For any Œ±‚â•1,
any (random) map h:Rd7‚ÜíRdand any distribution ¬µ, ŒΩwith support on Rd,
DŒ±(h#¬µ||h#ŒΩ)‚â§DŒ±(¬µ||ŒΩ), (7)
where h#¬µdenotes the pushforward operation for a function hand distribution ¬µ.
Proposition F.3 (Weak Triangle Inequality of R√©nyi divergence, Corollary 4 in [19]) .For any Œ± >1,
p, q > 1satisfying 1/p+ 1/q= 1and distributions P, Q, R with the same support:
DŒ±(P||R)‚â§Œ±‚àí1
p
Œ±‚àí1DpŒ±(P||Q) +Dq(Œ±‚àí1/p)(Q||R).
Proof. Recall that from the sketch of proof of Theorem 3.2, we have defined the six distributions:
ŒΩD|B, ŒΩ0
T|B, œÅ0
K|Bare the stationary distribution of the learning process, distribution at epoch Tof the
learning process and distribution at epoch Kof the unlearning process. Note that we learn on dataset
Dand fine-tune on D‚Ä≤. On the other hand, the corresponding distributions of ‚Äúadjacent‚Äù processes
that learn on D‚Ä≤and still unlearn on D‚Ä≤are denoted as ŒΩD‚Ä≤|B, ŒΩ0,‚Ä≤
T|B, œÅ0,‚Ä≤
K|Bsimilarly. Note that ŒΩ0,‚Ä≤
T|Bis
the distribution of retraining from scratch on D‚Ä≤, and we aim to bound dŒ±(œÅ0
K|B, ŒΩ0,‚Ä≤
T|B)for all possible
D,D‚Ä≤pairs. By Proposition F.3, we know that for any Œ± >1, by choosing p=q= 2, we have
dŒ±(œÅ0
K|B, ŒΩ0,‚Ä≤
T|B)‚â§Œ±‚àí1/2
Œ±‚àí1
d2Œ±(ŒΩD‚Ä≤|B, ŒΩ0,‚Ä≤
T|B) +d2Œ±(ŒΩD‚Ä≤|B, œÅ0
K|B)
. (8)
15Recall the Definition 2.1 that dŒ±(P, Q) = max( DŒ±(P||Q), DŒ±(Q||P))for distributions P, Q . The
above inequality is correct since consider any distributions P, Q, R , by Proposition F.3 we have
DŒ±(P||R)‚â§Œ±‚àí1
2
Œ±‚àí1D2Œ±(P||Q) +D2Œ±‚àí1(Q||R) (9)
(a)
‚â§Œ±‚àí1
2
Œ±‚àí1d2Œ±(P, Q) +d2Œ±‚àí1(Q, R) (10)
(b)
‚â§Œ±‚àí1
2
Œ±‚àí1d2Œ±(P, Q) +d2Œ±(Q, R) (11)
(c)
‚â§Œ±‚àí1
2
Œ±‚àí1(d2Œ±(P, Q) +d2Œ±(Q, R)) (12)
(13)
where (a) is due to the definition of R√©nyi difference, (b) is due to the monotonicity of R√©nyi
divergence in Œ±, and (c) is due to the fact that for all Œ± >1,Œ±‚àí1
2
Œ±‚àí1‚â•1.
Similarly, we have
DŒ±(R||P)‚â§Œ±‚àí1
2
Œ±‚àí1(d2Œ±(P, Q) +d2Œ±(Q, R)). (14)
Combining these two bounds leads to the weak triangle inequality for R√©nyi difference.
Now we establish the upper bound of dŒ±(ŒΩD‚Ä≤|B, ŒΩ0,‚Ä≤
T|B), which we denoted as Œµ1(Œ±)in Theorem 3.2.
We first note that due to the projection operator Œ†CR, we trivially have W‚àû(ŒΩD‚Ä≤|B, ŒΩ0
0)‚â§2R. On
the other hand, note that ŒΩ0,‚Ä≤
T|Bis the distribution of the learning process at epoch Twith respect
to dataset D‚Ä≤, where ŒΩD‚Ä≤|Bis the corresponding stationary distribution. Let us ‚Äúunroll‚Äù iterations
so that v=tn
b+j. We apply Lemma 2.6 for these two processes, where the initial distribution
are¬µ0=ŒΩ0
0and¬µ‚Ä≤
0=ŒΩD‚Ä≤|B. The updates œàv(x) = Œ† C(x)‚àíŒ∑
bP
i‚ààBj‚àáf(Œ†C(x);d‚Ä≤
i)are with
respect to the dataset D‚Ä≤, andŒ∂v=N(0, Œ∑œÉ2Id). First, note that we have Œ∑œÉ2instead of œÉ2as in the
Lemma 2.6, hence we need to apply a change of variable. The only thing we are left to prove is that
œàvisc-contractive for c= 1‚àíŒ∑mand any t, j. This is because for any mini-batch Bjand any data
pointd‚Ä≤
i‚àà X, i‚àà Bjof size b, we have
‚à•œàv(x)‚àíœàv(x‚Ä≤)‚à•=‚à•Œ†C(x)‚àíŒ∑
bX
i‚ààBj‚àáf(Œ†C(x);d‚Ä≤
i)‚àíŒ†C(x‚Ä≤) +Œ∑
bX
i‚ààBj‚àáf(Œ†C(x‚Ä≤);d‚Ä≤
i)‚à•
(15)
‚â§1
bX
i‚ààBj‚à•(Œ†C(x)‚àíŒ∑‚àáf(Œ†C(x);d‚Ä≤
i))‚àí(Œ†C(x‚Ä≤)‚àíŒ∑‚àáf(Œ†C(x‚Ä≤);d‚Ä≤
i))‚à• (16)
(a)
‚â§(1‚àíŒ∑m)‚à•Œ†C(x)‚àíŒ†C(x‚Ä≤)‚à• (17)
‚â§(1‚àíŒ∑m)‚à•x‚àíx‚Ä≤‚à•, (18)
Finally, due to data-processing inequality for R√©nyi divergence (Lemma F.2), applying the final
projection step does not increase the corresponding R√©nyi divergence. As a result, by Lemma 2.6 we
have
DŒ±(ŒΩD‚Ä≤|B||ŒΩ0,‚Ä≤
T|B)‚â§Œ±W‚àû(ŒΩD‚Ä≤|B, ŒΩ0,‚Ä≤
T|B)2
2Œ∑œÉ2c2Tn/b‚â§Œ±(2R)2
2Œ∑œÉ2c2Tn/b, (19)
where the last inequality is due to our naive bound on W‚àû(ŒΩD‚Ä≤|B, ŒΩ0
0). One can repeat the same anal-
ysis for the direction DŒ±(ŒΩ0,‚Ä≤
T|B||ŒΩD‚Ä≤|B), which leads to the same bound by symmetry of Lemma 2.6.
Together we have shown that
dŒ±(ŒΩD‚Ä≤|B||ŒΩ0,‚Ä≤
T|B)‚â§Œ±(2R)2
2Œ∑œÉ2c2Tn/b=Œµ1(Œ±). (20)
16Now we focus on bounding the term dŒ±(ŒΩD‚Ä≤|B, œÅ0
K|B). We once again note that ŒΩD‚Ä≤|Bis the stationary
distribution of the process œÅ0
K|B, since we fine-tune with respect to the dataset D‚Ä≤for the unlearning
process. As a result, the same analysis above can be applied, where the only difference is the initial
W‚àûdistance between ŒΩD‚Ä≤|B, œÅ0
0|B.
dŒ±(ŒΩD‚Ä≤|B, œÅ0
K|B)‚â§Œ±W‚àû(ŒΩD‚Ä≤|B, œÅ0
0|B)2
2Œ∑œÉ2c2Kn/b. (21)
We are left with establish an upper bound of W‚àû(ŒΩD‚Ä≤|B, œÅ0
0|B). Note that since W‚àûis indeed a
metric, we can apply triangle inequality which leads to the following upper bound.
W‚àû(ŒΩD‚Ä≤|B, œÅ0
0|B)‚â§W‚àû(ŒΩD‚Ä≤|B, ŒΩD|B) +W‚àû(ŒΩD|B, œÅ0
0|B) (22)
From Lemma 3.3, we have that
W‚àû(ŒΩ0
T|B, ŒΩ0,‚Ä≤
T|B)‚â§min1‚àícTn/b
1‚àícn/bcn/b‚àíj0‚àí12Œ∑M
b,2R
‚â§min1
1‚àícn/b2Œ∑M
b,2R
,(23)
where the last inequality is simply due to the fact that c <1. Since the upper bound is independent of
T, by letting T‚Üí ‚àû , and the definition that ŒΩD|B, ŒΩD‚Ä≤|Bare the limiting distribution of ŒΩ0
T|B, ŒΩ0,‚Ä≤
T|B
respectively, we have
W‚àû(ŒΩD|B, ŒΩD‚Ä≤|B)‚â§min1
1‚àícn/b2Œ∑M
b,2R
. (24)
On the other hand, note that by definition we know that œÅ0
0|B=ŒΩ0
T|B. Thus by Lemma 3.4 we have
(recall that c= 1‚àíŒ∑m)
W‚àû(ŒΩD|B, œÅ0
0|B) =W‚àû(ŒΩ0
T|B, ŒΩD|B)‚â§cTn/bW‚àû(ŒΩ0
0, ŒΩD|B)‚â§2R√ócTn/b, (25)
where the last inequality is again due to the naive bound induced by the projection to CRfor
W‚àû(ŒΩ0
0, ŒΩD|B). Together we have that
W‚àû(ŒΩD‚Ä≤|B, œÅ0
0|B)‚â§2R√ócTn/b+ min1
1‚àícn/b2Œ∑M
b,2R
=ZB. (26)
Combining things we complete the proof.
G Proof of Theorem 3.11
Theorem. Under the same assumptions as in Corollary 3.7. Assume the unlearning requests arrive
sequentially such that our dataset changes from D=D0‚Üí D 1‚Üí. . .‚Üí D S, where Ds,Ds+1are
adjacent. Let yj,(s)
kbe the unlearned parameters for the sthunlearning request at kthunlearning epoch
andjthiteration following Algorithm (1)onDsandy0,(s+1)
0 =y0,(s)
Ks‚àº¬ØŒΩDs|B, where y0,(0)
0=x‚àû
andKsis the unlearning steps for the sthunlearning request. For any s‚àà[S], we have
W‚àû(¬ØŒΩDs‚àí1|B, ŒΩDs|B)‚â§Z(s)
B,
where Z(s+1)
B = min( cKsn/bZ(s)
B+ZB,2R),Z(1)
B=ZB,ZBis described in Corollary 3.7 and
c= 1‚àíŒ∑m.
The proof is a direct application of triangle inequality, Lemma 3.4 and 3.3. We will prove it by
induction. For the base case s= 1 it trivial, since ¬ØŒΩD0=ŒΩD0as we choose y0,(0)
0=x0
‚àû. Thus
by our definition that ZBis the upper bound of W‚àû(ŒΩD|B, ŒΩD‚Ä≤|B)for any adjacent dataset D,D‚Ä≤.
Apparently, we also have
W‚àû(¬ØŒΩD0|B, ŒΩD1|B) =W‚àû(ŒΩD0|B, ŒΩD1|B)‚â§ZB=Z(1)
B (27)
17For the induction step, suppose our hypothesis is true until sstep. Then for the s+ 1step we have
W‚àû(¬ØŒΩDs|B, ŒΩDs+1|B)(a)
‚â§W‚àû(¬ØŒΩDs|B, ŒΩDs|B) +W‚àû(ŒΩDs|B, ŒΩDs+1|B) (28)
(b)
‚â§W‚àû(¬ØŒΩDs|B, ŒΩDs|B) +ZB (29)
(c)
‚â§cKs‚àí1n/bW‚àû(¬ØŒΩDs‚àí1|B, ŒΩDs|B) +ZB (30)
(d)
‚â§cKs‚àí1n/bZ(s)
B+ZB (31)
where (a)is due to triangle inequality as W‚àûis a metric. (b)is due to Corollary 3.7, where ZBis
an upper bound of W‚àûdistance between any two adjacent stationary distributions. (c)is due to
Lemma 3.4 and (d)is due to our hypothesis. Finally, note that 2Ris a natural universal upper bound
due to our projection on CR, which has diameter 2R. Together we complete the proof.
H Proof of Corollary 3.7
Note that under the training convergent assumption, the target retraining distribution is directly ŒΩD‚Ä≤|B
so that we do not need the weak triangle inequality for R√©nyi difference. Similarly, we do not need
the triangle inequality for the term ZB. Directly using Œµ2(Œ±)withZB= min
1
1‚àícn/b2Œ∑M
b,2R
from Theorem 3.2 leads to the result.
I Improved bound with randomized B
Corollary. Under the same setting as of Theorem 3.2 but with random mini-batch sequences described
in Algorithm 1. Then for any Œ± >1, and Œ∑‚â§1
L, the output of the Kthunlearning iteration satisfies
(Œ±, Œµ)-RU with c= 1‚àíŒ∑m, where
Œµ‚â§1
Œ±‚àí1log
EBexpŒ±(Œ±‚àí1)Z2
B
2Œ∑œÉ2c2Kn/b
,
where ZBis the bound described in Lemma 3.3.
We first restate the lemma in [22], which is an application of the joint convexity of KL divergence.
Lemma I.1 (Lemma 4.1 in [22]) .LetŒΩ1,¬∑¬∑¬∑, ŒΩmandŒΩ‚Ä≤
1,¬∑¬∑¬∑, ŒΩ‚Ä≤
mbe distributions over Rd. For any
Œ±‚â•1and any coefficients p1,¬∑¬∑¬∑, pm‚â•0such thatPm
i=1pi= 1, the following inequality holds.
exp(( Œ±‚àí1)DŒ±(mX
i=1piŒΩi||mX
i=1piŒΩ‚Ä≤
i)) (32)
‚â§mX
i=1piexp(( Œ±‚àí1)DŒ±(ŒΩi||ŒΩ‚Ä≤
i)). (33)
The proof of Corollary 3.8 directly follows by repeating the proof of Lemma 3.7 but with the B
dependent ZBdescribed in Lemma 3.3. Then leverage Lemma I.1 leads to the result.
JW‚àûbound for batch unlearning
Corollary J.1 (W‚àûbound for batch unlearning) .Consider the learning process in Algorithm 1 on
adjacent dataset DandD‚Ä≤that differ in 1‚â§Spoints and a fixed mini-batch sequence B. Assume
‚àÄd‚àà X,f(x;d)isL-smooth, M-Lipschitz and m-strongly convex in x. Let the index of different
data point between D,D‚Ä≤belongs to mini-batches Bj0,Bj1, . . . ,BjG‚àí1, each of which contains
1‚â§Sjg‚â§bsuch thatPG‚àí1
g=0Sjg=S. Then for Œ∑‚â§1
Landc= (1‚àíŒ∑m), we have
W‚àû(ŒΩ0
T|B, ŒΩ0,‚Ä≤
T|B)‚â§min 
1‚àícT
1‚àícG‚àí1X
g=0cn/b‚àíjg‚àí12Œ∑MS jg
b,2R!
.
18Proof. The proof is a direct generalization of Lemma 3.3. Recall that for the per-iteration bound
within an epoch, there are two possible cases: 1) we encounter the mini-batch Bjthat contains indices
of replaced points 2) or not. This is equivalently to 1) j‚àà {jg}G‚àí1
g=0or 2)j /‚àà {jg}G‚àí1
g=0.
Let us assume that case 1) happens and j=jgforg‚àà {0, . . . , G ‚àí1}. Also, let us denote the set of
those indices as Sjgwith a slight abuse of notation. By the same analysis, we know that
‚à•œàj(xj
t)‚àíœà‚Ä≤
j(xj,‚Ä≤
t)‚à• (34)
‚â§1
bX
i‚ààBj\Sjg‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di)‚àí ‚àáf(xj,‚Ä≤
t;di))‚à• (35)
+1
bX
i‚ààSjg‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di)‚àí ‚àáf(xj,‚Ä≤
t;d‚Ä≤
i))‚à•. (36)
For the first term, note that the gradient mapping (with respect to the same data point) is contractive
with constant (1‚àíŒ∑m)forŒ∑‚â§1
L, thus we have
‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di)‚àí ‚àáf(xj,‚Ä≤
t;di))‚à• ‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•. (37)
For the second term, by triangle inequality and the M-Lipschitzness of fwe have
‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di)‚àí ‚àáf(xj,‚Ä≤
t;d‚Ä≤
i))‚à• (38)
=‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di)‚àí ‚àáf(xj,‚Ä≤
t;di) +‚àáf(xj,‚Ä≤
t;di)‚àí ‚àáf(xj,‚Ä≤
t;d‚Ä≤
i))‚à• (39)
‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•+Œ∑‚à•‚àáf(xj,‚Ä≤
t;di)‚à•+Œ∑‚à•‚àáf(xj,‚Ä≤
t;d‚Ä≤
i)‚à• (40)
‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•+ 2Œ∑M. (41)
Note that there are Sjgterms above. Combining things together, we have
‚à•œàj(xj
t)‚àíœà‚Ä≤
j(xj,‚Ä≤
t)‚à• ‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•+2Œ∑MS jg
b. (42)
Iterate this bound over all n/biterations within this epoch, we have
‚à•œàj(x0
t+1)‚àíœà‚Ä≤
j(x0,‚Ä≤
t+1)‚à• ‚â§(1‚àíŒ∑m)n/b‚à•œàj(x0
t)‚àíœà‚Ä≤
j(x0,‚Ä≤
t)‚à•+G‚àí1X
g=0(1‚àíŒ∑m)n/b‚àíjg‚àí12Œ∑MS jg
b.
(43)
The rest analysis is the same as those in Lemma 3.3, where we iterate over Tepochs, infimum over
all possible coupling, and then simplify the expression using geometric series. Thus we complete the
proof.
K Proof of Proposition 3.9
Proposition. Under the same setting as Corollary 3.7 with b=n,Œ∑‚â§m
2L2and assume the minimizer
of any fDis in the relative interior of CR‚äÜRd, for any given adjacent dataset pair D,D‚Ä≤the output
of the Kthunlearning iteration y0
Ksatisfies
E[fD‚Ä≤(y0
K)‚àíinf
x‚ààCRfD‚Ä≤(x)]‚â§McKmin(1
1‚àíc2Œ∑M
n,2R) +2LdœÉ2
m. (44)
The proof is based on the following key lemma from [24].
Lemma K.1. ForM-Lipschitz, m-strongly convex and L-smooth loss function fD(x)over the closed
bounded convex set CR‚äÜRdwith radius R, step size Œ∑‚â§m
2L2and initial parameter x0‚àºŒΩ0, the
excess empirical risk of the learning process of Algorithm 1 for Tepoch is bounded by
E[fD(x0
T)‚àífD(x‚ãÜ)]‚â§L
2exp(‚àímŒ∑T)Ex0‚àºŒΩ0[‚à•x0‚àíx‚ãÜ‚à•2] +2LdœÉ2
m, (45)
where x‚ãÜis the minimizer of fD(x)in the relative interior of CRanddis the dimension of parameter.
19Clearly, we can see that under the convergent assumption of Corollary 3.7, the excess risk of y0
0=x‚àû
is
E[fD(y0
0)‚àífD(x‚ãÜ)]‚â§2LdœÉ2
m. (46)
This is true since ‚à•x0‚àíx‚ãÜ‚à• ‚â§2Ris finite by the boundedness of CR.
Now we are ready to prove Proposition 3.9.
Proof. Start by observing that
E[fD‚Ä≤(y0
K)‚àífD‚Ä≤(y‚ãÜ)] =E[fD‚Ä≤(y0
K)‚àífD‚Ä≤(y0,‚Ä≤
K)] +E[fD‚Ä≤(y0,‚Ä≤
K)‚àífD‚Ä≤(y‚ãÜ)], (47)
where we recall that y0,‚Ä≤
Kis the ‚Äúadjacent process‚Äù of y0
Kthat running noisy gradient descent on D‚Ä≤
for both learning and unlearning processes. As a result, we have y0,‚Ä≤
K=x0,‚Ä≤
T+KforTlearning epoch.
By taking T‚Üí ‚àû under the convergent assumption and utilizing Lemma K.1, we have
E[fD‚Ä≤(y0,‚Ä≤
K)‚àífD‚Ä≤(y‚ãÜ)]‚â§2LdœÉ2
m. (48)
Hence we are left with analyzing the first term. We will bound it in terms of ‚à•y0
K‚àíy0,‚Ä≤
K‚à•. Note
that we choose a particular coupling between y0
K, y0,‚Ä≤
Kas in Lemma 3.3, where the Gaussian noise in
y0
K, y0,‚Ä≤
Kare identical almost surely. Also for simplicity, we drop 0in the superscript since we are
analyzing the full batch case. By M-Lipschitzness of fD‚Ä≤, we have
‚à•fD‚Ä≤(yK)‚àífD‚Ä≤(y‚Ä≤
K)‚à• ‚â§M‚à•y0
K‚àíy0,‚Ä≤
K‚à•. (49)
For‚à•y0
K‚àíy0,‚Ä≤
K‚à•, we have
‚à•yK‚àíy‚Ä≤
K‚à• (50)
=‚à•Œ†CR[yK‚àí1‚àíŒ∑‚àáfD‚Ä≤(yK‚àí1) +p
2Œ∑œÉ2WK‚àí1]‚àíŒ†CR[y‚Ä≤
K‚àí1‚àíŒ∑‚àáfD‚Ä≤(y‚Ä≤
K‚àí1) +p
2Œ∑œÉ2WK‚àí1]‚à•
(51)
‚â§ ‚à•yK‚àí1‚àíŒ∑‚àáfD‚Ä≤(yK‚àí1)‚àí(y‚Ä≤
K‚àí1‚àíŒ∑‚àáfD‚Ä≤(y‚Ä≤
K‚àí1))‚à• (52)
‚â§(1‚àíŒ∑m)‚à•yK‚àí1‚àíy‚Ä≤
K‚àí1‚à•, (53)
where the first inequality is due to the fact that Œ†CRis a contraction map and noise cancels out
due to our coupling of y0
K, y0,‚Ä≤
K. The second inequality is due to the fact that gradient update is
(1‚àímŒ∑)-contraction map when fD‚Ä≤isL-smooth, m-strongly convex and Œ∑‚â§1
L. By iterating this
bound, we have
‚à•fD‚Ä≤(yK)‚àífD‚Ä≤(y‚Ä≤
K)‚à• ‚â§M(1‚àímŒ∑)K‚à•y0‚àíy‚Ä≤
0‚à• (54)
‚â§McKmin(1
1‚àíc2Œ∑M
n,2R), (55)
where c= 1‚àímŒ∑and the last inequality is due to Lemma 3.3 for the case b=nand taking the limit
T‚Üí ‚àû . Combining all results together, we have
E[fD‚Ä≤(y0
K)‚àífD‚Ä≤(y‚ãÜ)] (56)
‚â§E[fD‚Ä≤(y0
K)‚àífD‚Ä≤(y0,‚Ä≤
K)] +2LdœÉ2
m(57)
‚â§E[‚à•fD‚Ä≤(y0
K)‚àífD‚Ä≤(y0,‚Ä≤
K)‚à•] +2LdœÉ2
m(58)
‚â§McKmin(1
1‚àíc2Œ∑M
n,2R) +2LdœÉ2
m. (59)
Together we complete the proof.
20L Proof of Lemma 3.3
Lemma (W‚àûbetween adjacent PNSGD learning processes) .Consider the learning process in
Algorithm 1 on adjacent dataset DandD‚Ä≤and a fixed mini-batch sequence B. Assume ‚àÄd‚àà X,
f(x;d)isL-smooth, M-Lipschitz and m-strongly convex in x. Let the index of different data point
between D,D‚Ä≤belongs to mini-batch Bj0. Then for Œ∑‚â§1
Land let c= (1‚àíŒ∑m), we have
W‚àû(ŒΩ0
T|B, ŒΩ0,‚Ä≤
T|B)‚â§min1‚àícTn/b
1‚àícn/bcn/b‚àíj0‚àí12Œ∑M
b,2R
.
Proof. We prove the bound of one iteration and iterate over that result under a specific coupling of the
adjacent PNSGD learning processes, which is by definition an upper bound for taking infimum over
all possible coupling. Given an mini-batch sequence B, assume the adjacent dataset D,D‚Ä≤differ at
index i0‚àà[n]andi0belongs to jth
0mini-batch (i.e., i0‚àà Bj0. For simplicity, we drop the condition
onBfor all quantities in the proof as long as it is clear that we always condition on a fixed B. Let us
denote œàj(x) =x‚àíŒ∑
bP
i‚ààBj‚àáf(x;d‚Ä≤
i)and similar for œà‚Ä≤
jonD‚Ä≤. Note that for some coupling of
ŒΩj
t, ŒΩj,‚Ä≤
tdenoted as Œ≥j
t,
esssup(xj+1
t,xj+1,‚Ä≤
t)‚àºŒ≥j+1
t‚à•xj+1
t‚àíxj+1,‚Ä≤
t‚à• (60)
=esssup((xj
t,Wj
t),(xj,‚Ä≤
t,Wj,‚Ä≤
t))‚àºŒ≥j
t‚à•œàj(xj
t)‚àíœà‚Ä≤
j(xj,‚Ä≤
t) +p
2Œ∑œÉ2Wj
t‚àíp
2Œ∑œÉ2Wj,‚Ä≤
t‚à• (61)
‚â§esssup((xj
t,Wj
t),(xj,‚Ä≤
t,Wj,‚Ä≤
t))‚àºŒ≥j
t‚à•œàj(xj
t)‚àíœà‚Ä≤
j(xj,‚Ä≤
t)‚à•+‚à•p
2Œ∑œÉ2Wj
t‚àíp
2Œ∑œÉ2Wj,‚Ä≤
t‚à•. (62)
Now, note that by definition the noise Wj
tis independent of xj
t, we can simply choose the coupling Œ≥
so that Wj
t=Wj,‚Ä≤
tfor all t, jandx0
0=x0,‚Ä≤
0due to the same initialization distribution. So the last
term is 0. For the first term, there are two possible cases: 1) j=j0and 2) jÃ∏=j0. For case 1), we
have
‚à•œàj(xj
t)‚àíœà‚Ä≤
j(xj,‚Ä≤
t)‚à• (63)
‚â§1
bX
i‚ààBj\{i0}‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di)‚àí ‚àáf(xj,‚Ä≤
t;di))‚à• (64)
+1
b‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di0)‚àí ‚àáf(xj,‚Ä≤
t;d‚Ä≤
i0))‚à•. (65)
For the first term, note that the gradient mapping (with respect to the same data point) is contractive
with constant (1‚àíŒ∑m)forŒ∑‚â§1
L, thus we have
‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di)‚àí ‚àáf(xj,‚Ä≤
t;di))‚à• ‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•. (66)
For the second term, by triangle inequality and the M-Lipschitzness of fwe have
‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di0)‚àí ‚àáf(xj,‚Ä≤
t;d‚Ä≤
i0))‚à• (67)
=‚à•xj
t‚àíxj,‚Ä≤
t+Œ∑(‚àáf(xj
t;di0)‚àí ‚àáf(xj,‚Ä≤
t;di0) +‚àáf(xj,‚Ä≤
t;di0)‚àí ‚àáf(xj,‚Ä≤
t;d‚Ä≤
i0))‚à• (68)
‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•+Œ∑‚à•‚àáf(xj,‚Ä≤
t;di0)‚à•+Œ∑‚à•‚àáf(xj,‚Ä≤
t;d‚Ä≤
i0)‚à• (69)
‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•+ 2Œ∑M. (70)
Combining things together, we have
‚à•œàj(xj
t)‚àíœà‚Ä≤
j(xj,‚Ä≤
t)‚à• ‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•+2Œ∑M
b. (71)
On the other hand, for case 2) we can simply use the contrativity of the gradient update for all bterms,
which leads to
‚à•œàj(xj
t)‚àíœà‚Ä≤
j(xj,‚Ä≤
t)‚à• ‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•. (72)
Combining these two cases, we have
esssupŒ≥‚à•xj+1
t‚àíxj+1,‚Ä≤
t‚à• ‚â§esssupŒ≥(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•+2Œ∑M
b1[j=j0] (73)
(74)
21where 1[j=j0]is the indicator function of the event j=j0. Now we iterate this bound within the
epoch t, which leads to an epoch-wise bound
esssupŒ≥‚à•x0
t+1‚àíx0,‚Ä≤
t+1‚à• ‚â§esssupŒ≥(1‚àíŒ∑m)n/b‚à•x0
t‚àíx0,‚Ä≤
t‚à•+ (1‚àíŒ∑m)n/b‚àíj0‚àí12Œ∑M
b(75)
(76)
We can further iterate this bound over all iterations T, which leads to
esssupŒ≥‚à•x0
T‚àíx0,‚Ä≤
T‚à• ‚â§esssupŒ≥(1‚àíŒ∑m)Tn/b‚à•x0
0‚àíx0,‚Ä≤
0‚à•+T‚àí1X
t=0(1‚àíŒ∑m)(t+1)n/b‚àíj0‚àí12Œ∑M
b
(77)
=T‚àí1X
t=0(1‚àíŒ∑m)(t+1)n/b‚àíj0‚àí12Œ∑M
b, (78)
(79)
where the last equality follows by our choice of coupling Œ≥such that x0
0=x0,‚Ä≤
0due to the same
initialization distribution. Now by taking the infimum overall possible coupling Œ≥and simply the
expression, we have
W‚àû(ŒΩ0
T|B, ŒΩ0,‚Ä≤
T|B)‚â§1‚àícTn/b
1‚àícn/bcn/b‚àíj0‚àí12Œ∑M
b. (80)
Finally, note that there is also a naive bound W‚àû(ŒΩ0
T|B, ŒΩ0,‚Ä≤
T|B)‚â§2Rdue to the projection Œ†CR.
Combine these two we complete the proof.
M Proof of Lemma 3.4
Lemma (W‚àûbetween PNSGD learning process to its stationary distribution) .Following the same
setting as in Theorem 3.2 and denote the initial distribution of the unlearning process as ŒΩ0
0. Then we
have
W‚àû(ŒΩ0
T|B, ŒΩD|B)‚â§(1‚àíŒ∑m)Tn/bW‚àû(ŒΩ0
0, ŒΩD|B).
Proof. We follow a similar analysis as in Lemma 3.3 but with two key differences: 1) our initial W‚àû
distance is not zero and 2) we are applying the same c-CNI. Let us slightly abuse the notation to
denote the process xj,‚Ä≤
t‚àºŒΩj,‚Ä≤
tbe the process of learning on Das well but starting with ŒΩ0,‚Ä≤
0=ŒΩD|B.
As before, we first establish one iteration bound with epoch t, then iterate over n/biterations, and
then iterate over Tto complete the proof. Consider the two CNI processes xj
t, xj,‚Ä≤
twith the same
update œàj(x) =x‚àíŒ∑
bP
i‚ààBj‚àáf(x;di), where x0
0‚àºŒΩ0
0andx0,‚Ä≤
0‚àºŒΩD|B. We will use the similar
coupling construction as in the proof of Lemma 3.3 (i.e., Wj
t=Wj,‚Ä≤
t). However, note that we will not
restrict the coupling between the two initial distributions ŒΩ0
0=ŒΩ0|BandŒΩ0,‚Ä≤
0=ŒΩD|B. Let us denote
the coupling for WasŒ≥Wand the coupling for the initial distributions ŒΩ0
0=ŒΩ0|BandŒΩ0,‚Ä≤
0=ŒΩD|Bas
Œ≥I. We denote the overall coupling as Œ≥= (Œ≥I, Œ≥W). For our specific choice of coupling Œ≥W, we
have
‚à•xj+1
t‚àíxj+1,‚Ä≤
t‚à•(a)
‚â§ ‚à•œàj(xj
t)‚àíœàj(xj,‚Ä≤
t) +p
2Œ∑œÉ2(Wj
t‚àíWj,‚Ä≤
t)‚à•(b)
‚â§ ‚à•œàj(xj
t)‚àíœàj(xj,‚Ä≤
t)‚à•+ 0(c)
‚â§(1‚àíŒ∑m)‚à•xj
t‚àíxj,‚Ä≤
t‚à•,
(81)
where (a) is due to the fact that projection Œ†CRis1-contractive, (b) is due to our coupling choice
on the noise distribution, and (c) is due to the fact that œàjis(1‚àíŒ∑m)contractive and the coupling
choice on the mini-batch.
Now we iterate the bound above within the epoch and then over Tepoch similar as before, which
leads to the per epoch bound as follows
esssupŒ≥‚à•x0
T‚àíx0,‚Ä≤
T‚à• ‚â§esssupŒ≥(1‚àíŒ∑m)Tn/b‚à•x0
0‚àíx0,‚Ä≤
0‚à•=esssupŒ≥I(1‚àíŒ∑m)Tn/b‚à•x0
0‚àíx0,‚Ä≤
0‚à•.
(82)
22Note that this holds for all coupling Œ≥Ifor initial distributions. Let us now choose a specific (tie
break arbitrary) Œ≥Iso that the W‚àû(ŒΩ0|B, ŒΩD|B) =esssupŒ≥I‚à•x0
0‚àíx0,‚Ä≤
0‚à•is attained. For this specific
coupling Œ≥I, we have
esssup(Œ≥I,Œ≥W)‚à•x0
T‚àíx0,‚Ä≤
T‚à• ‚â§(1‚àíŒ∑m)Tn/bW‚àû(ŒΩ0|B, ŒΩD|B). (83)
Finally, by the definition of infimum and observe that x0,‚Ä≤
T‚àºŒΩD|Bdue to the stationary property of
ŒΩD|B(which is independent of our choice of coupling), we have
W‚àû(ŒΩ0
T|B, ŒΩD|B)‚â§esssup(Œ≥I,Œ≥W)‚à•x0
T‚àíx0,‚Ä≤
T‚à• ‚â§(1‚àíŒ∑m)Tn/bW‚àû(ŒΩ0|B, ŒΩD|B). (84)
Hence we complete the proof.
N Experiment Details
N.1 (Œ±, Œµ)-RU to (œµ, Œ¥)-Unlearning Conversion
Let us first state the definition of (œµ, Œ¥)-unlearning from prior literature [7‚Äì9].
Definition N.1. Consider a randomized learning algorithm M:Xn7‚ÜíRdand a randomized
unlearning algorithm U:Rd√ó Xn√ó Xn7‚ÜíRd. We say (M,U)achieves (œµ, Œ¥)-unlearning if for
any adjacent datasets D,D‚Ä≤and any event E, we have
P(U(M(D),D,D‚Ä≤)‚äÜE)‚â§exp(œµ)P(M(D‚Ä≤)‚äÜE) +Œ¥, (85)
P(M(D‚Ä≤)‚äÜE)‚â§exp(œµ)P(U(M(D),D,D‚Ä≤)‚äÜE) +Œ¥. (86)
Following the same proof of RDP-DP conversion (Proposition 3 in [19]), we have the following
(Œ±, Œµ)-RU to (œµ, Œ¥)-unlearning conversion as well.
Proposition N.2. If(M,U)achieves (Œ±, Œµ)-RU, it satisfies (œµ, Œ¥)-unlearning as well, where
œµ=Œµ+log(1/Œ¥)
Œ±‚àí1. (87)
N.2 Datasets
MNIST [25] contains the grey-scale image of number 0to number 9, each with 28√ó28pixels. We
follow [9] to take the images with the label 3and8as the two classes for logistic regression. The
training data contains 11264 instances in total and the testing data contains 1984 samples. We spread
the image into an x‚ààRd, d= 724 feature as the input of logistic regression.
CIFAR-10 [26] contains the RGB-scale image of ten classes for image classification, each with
32√ó32pixels. We also select class #3 (cat) and class #8 (ship) as the two classes for logistic
regression. The training data contains 9728 instances and the testing data contains 2000 samples. We
apply data pre-processing on CIFAR-10 by extracting the compact feature encoding from the last layer
before pooling of an off-the-shelf pre-trained ResNet18 model [27] from Torch-vision library [34,35]
as the input of our logistic regression. The compact feature encoding is x‚ààRd, d= 512 .
All the inputs from the datasets are normalized with the ‚Ñì2norm of 1. Note that We drop some date
points compared with [11] to make the number of training data an integer multiple of the maximum
batch size in our experiment, which is 512.
N.3 Experiment Settings
Hardware and Frameworks All the experiments run with PyTorch=2.1.2 [36] and
numpy=1.24.3 [37]. The codes run on a server with a single NVIDIA RTX 6000 GPU with AMD
EPYC 7763 64-Core Processor.
Problem Formulation Given a binary classification task D={xi‚ààRd, yi‚àà {‚àí 1,+1}}n
i=1, our
goal is to obtain a set of parameters wthat optimizes the objective below:
L(w;D) =1
nnX
i=1l(w‚ä§xi, yi) +Œª
2||w||2
2, (88)
23where the objective consists of a standard logistic regression loss l(w‚ä§xi, yi) =‚àílogœÉ(yiw‚ä§xi),
where œÉ(t) =1
1+exp( ‚àít)is the sigmoid function; and a ‚Ñì2regularization term where Œªis a hyperpa-
rameter to control the regularization, and we set Œªas10‚àí6√ónacross all the experiments. By simple
algebra one can show that [7]
‚àál(w‚ä§xi, yi) = (œÉ(yiw‚ä§xi)‚àí1)yixi+Œªw, (89)
‚àá2l(w‚ä§xi, yi) =œÉ(yiw‚ä§xi)(1‚àíœÉ(yiw‚ä§xi))xixT
i+ŒªId. (90)
Due to œÉ(yiw‚ä§xi)‚àà[0,1], it is not hard to see that we have smoothness L= 1/4 +Œªand strong
convexity Œª. The constant meta-data of the loss function in equation (88) above for the two datasets
is shown in the table below:
Table 1: The constants for the loss function and other calculation on MNIST and CIFAR-10.
expression MNIST CIFAR10
smoothness constant L1
4+Œª1
4+Œª1
4+Œª
strongly convex constant m Œª 0.0112 0.0097
Lipschitz constant M gradient clip 1 1
RDP constant Œ¥ 1/n 8.8778e-5 0.0001
The per-sample gradient with clipping w.r.t. the weights wof the logistic regression loss function is
given as:
‚àáclipl(w‚ä§xi, yi) = Œ† CM 
(œÉ(yiw‚ä§xi)‚àí1)yixi
+Œªw, (91)
where Œ†CMdenotes the gradient clipping projection into the Euclidean ball with the radius of M, to
satisfy the Lipschitz constant bound. According to Proposition 5.2 of [22], the per-sampling clipping
operation still results in a L-smooth, m-strongly convex objective. The resulting stochastic gradient
update on the full dataset is as follows:
1
nnX
i=1‚àáclipl(wTxi, yi), (92)
Finally, we remark that in our specific case since we have normalized the features of all data points
(i.e.,‚à•x‚à•= 1), by the explicit gradient formula we know that ‚à•(œÉ(yiw‚ä§xi)‚àí1)yixi‚à• ‚â§1.
Learning from scratch set-up For the baselines and our PNSGD unlearning framework, we all
sample the initial weight wrandomly sampled from i.i.d Gaussian distribution N(¬µ0,2œÉ2
m), where ¬µ0
is a hyper-parameter denoting the initialization mean and we set as 0. For the PNSGD unlearning
method, the burn-in steps Tw.r.t. different batch sizes are listed in Table. 2. we follow [11] and set
T= 10,000for the baselines (D2D and Langevin unlearning) to converge.
Table 2: The Burn-in step Tset for different batch sizes for the PNSGD unlearning framework
batch size 32 128 512 full-batch
burn-in steps 10 20 50 1000
Unlearning request implementation. In our experiment, for an unlearning request of removing
data point i, we replace its feature with random features drawn from N(0, Id)and its label with a
random label drawn uniformly at random drawn from all possible classes. This is similar to the DP
replacement definition defined in [38], where they replace a point with a special nullpoint‚ä•.
General implementation of baselines
D2D [9] :
‚Ä¢Across all of our experiments involved with D2D, we follow the original paper to set the step size
as2/(L+m).
24‚Ä¢For the experiments in Fig. 3a, we calculate the noise to add after gradient descent with the non-
private bound as illustrated in Theorem. O.1 (Theorem 9 in [9]); For experiments with sequential
unlearning requests in Fig. 3b, we calculate the least step number and corresponding noise with the
bound in Theorem. O.2(Theorem 28 in [9]).
‚Ä¢The implementation of D2D follows the pseudo code shown in Algorithm 1,2 in [9] as follows:
Algorithm 2 D2D: learning from scratch
1:Input : dataset D
2:Initialize w 0
3:fort= 1,2, . . . , 10000 do
4: wt=wt‚àí1‚àí2
L+m√ó1
nPn
i=1(‚àáclipl(wT
t‚àí1xi, yi))
5:end for
6:Output :ÀÜw=wT
Algorithm 3 D2D: unlearning
1:Input : dataset Di‚àí1, update ui; model wi
2:Update dataset Di=Di‚àí1‚ó¶ui
3:Initialize w‚Ä≤
0=wi
4:fort= 1, . . . , I do
5: w‚Ä≤
t=w‚Ä≤
t‚àí1‚àí2
L+m√ó1
nPn
i=1‚àáclipl((w‚Ä≤
t‚àí1)Txi, yi))
6:end for
7:Calculate Œ≥=L‚àím
L+m
8:Draw Z‚àº N(0, œÉ2Id)
9:Output ÀÜwi=w‚Ä≤
Ti+Z
The settings and the calculation of I, œÉin Algorithm. 3 are discussed in the later part of this section
and could be found in Section. O.
Langevin unlearning [11]
We follow exactly the experiment details described in [11].
General Implementation of PNSGD Unlearning (ours)
‚Ä¢We set the step size Œ∑for the PNSGD unlearning framework across all the experiments as 1/L.
‚Ä¢The pseudo-code for PNSGD unlearning framework is shown in Algorithm. 1.
N.4 Implementation Details for Fig. 3a
In this experiment, we first train the methods on the original dataset Dfrom scratch to obtain the
initial weights w0. Then we randomly remove a single data point ( S= 1) from the dataset to get
the new dataset D‚Ä≤, and unlearn the methods from the initial weights ÀÜwand test the accuracy on the
testing set. We follow [11] and set the target ÀÜœµwith6different values as [0.05,0.1,0.5,1,2,5]. For
each target ÀÜœµ:
‚Ä¢For D2D, we set two different unlearning gradient descent step budgets as I= 1,5, and calculate the
corresponding noise to be added to the weight after gradient descent on Daccording to Theorem. O.1.
‚Ä¢For the Langevin unlearning framework [11], we set the unlearning fine-tune step budget as ÀÜK= 1
only, and calculate the smallest œÉthat could satisfy the fine-tune step budget and target ÀÜœµat the same
time. The calculation follows the binary search algorithm described in the original paper.
‚Ä¢For the stochastic gradient descent langevin unlearning framework, we also set the unlearning
fine-tune step budget as ÀÜK= 1, and calculate the smallest œÉthat could satisfy the fine-tune step
budget and target ÀÜœµat the same time. The calculation follows the binary search algorithm as follows:
25Algorithm 4 PNSGD Unlearning: binary search œÉthat satisfy ÀÜKand target ÀÜœµbudget
1:Input :target ÀÜœµ, unlearn step budget K, lower bound œÉlow, upper bound œÉhigh
2:while œÉhigh‚àíœÉlow>1e‚àí8do
3: œÉmid=(œÉlow+œÉhigh)/2
4: call Alg. 5 to find the least Kthat satisfies ÀÜœµwithœÉ=œÉmid
5: ifK‚â§ÀÜKthen
6: œÉhigh=œÉmid
7: else
8: œÉlow=œÉmid
9: end if
10:end while
11:return K
Algorithm 5 PNSGD Unlearning [non-convergent]: find the least Ksatisfying ÀÜœµ
1:Input :target ÀÜœµ,œÉ, burn-in T, projection radius R
2:Initialize K= 1, œµ > ÀÜœµ
3:while œµ >ÀÜœµdo
4: c= 1‚àíŒ∑m
5: Œµ1(Œ±) =Œ±(2R)2
2Œ∑œÉ2c2Tn/b
6: Z=1
1‚àícn/b2Œ∑M
b
7: Œµ2(Œ±) =Œ±Z2
2Œ∑œÉ2c2kn/b+ 2RcTn/b
8: Œµ(Œ±) =Œ±‚àí1/2
Œ±‚àí1(Œµ1(2Œ±) +Œµ2(2Œ±))
9: œµ=minŒ±>1[Œµ(Œ±) + log(1 /Œ¥)/(Œ±‚àí1)]
10: K=K+ 1
11:end while
12:Return K
We set the projection radius as R= 100 , and the œÉfound is listed in Table. 3.
Table 3: œÉof PNSGD unlearning.
0.05 0.1 0.5 1 2 5
CIFAR-10b=128 0.2165 0.1084 0.0220 0.0112 0.0058 0.0025
full batch 1.2592 0.6308 0.1282 0.0653 0.0338 0.0148
MNISTb=128 0.0790 0.0396 0.0080 0.0041 0.0021 0.0009
full batch 0.9438 0.4728 0.0960 0.0489 0.0253 0.0111
26N.5 Implementation Details for Fig. 3b
In this experiment, we fix the target ÀÜœµ= 1, we set the total number of data removal as 100. We show
the accumulated unlearning steps w.r.t. the number of data removed. We first train the methods from
scratch to get the initial weight w0, and sequentially remove data step by step until all the data points
are removed. We count the accumulated unlearning steps Kneeded in the process.
‚Ä¢For D2D, According to the original paper, only one data point could be removed at a time. We
calculate the least required steps and the noise to be added according to Theorem. O.2.
‚Ä¢For Langevin unlearning, we fix the œÉ= 0.03, and we let the model unlearn [5,10,20]per time.
The least required unlearning steps are obtained following [11].
‚Ä¢For Stochastic gradient descent langevin unlearning, we replace a single point per request and
unlearn for 100requests. We obtain the least required unlearning step Kfollowing Corollary 3.7.
The pseudo-code is shown in Algorithm. 6.
Algorithm 6 PNSGD Unlearning: find the least unlearn step Kin sequential settings
1:Input :target ÀÜœµ,œÉ, total removal S
2:Klist= []
3:fori in range( S)do
4: Initialize Klist[i‚àí1] = 1 ,œµ >ÀÜœµ
5: while œµ >ÀÜœµdo
6: œµ= min Œ±>1[Œµ(Œ±, œÉ, b, K list[i‚àí1]) +log(1/Œ¥)
Œ±‚àí1]
7: Klist[i‚àí1] =Klist[i‚àí1] + 1
8: end while
9:end for
10:Return Klist
Algorithm 7 Œµ(Œ±, œÉ, b, K )
1:Input :target Œ±,œÉ, removal batch size bper time, i-th removal in the sequence
2:c= 1‚àíŒ∑m
3:ReturnŒ±ZB(b)2
2Œ∑œÉ2c2Kn/b
Algorithm 8 ZB(b)
1:c= 1‚àíŒ∑m
2:return1
1‚àícn/b2Œ∑M
b
N.6 Implementation Details for Fig. 3c and 3d
In this study, we fix S= 100 to remove, set different œÉ= [0.05,0.1,0.2,0.5,1]and set batch size
asb= [32 ,128,512,full batch ]. We train the PNSGD unlearning framework from scratch to get the
initial weight, then remove some data, unlearn the model, and report the accuracy. We calculate the
least required unlearning steps Kby calling Algorithm. 6.
O Unlearning Guarantee of Delete-to-Descent [9]
The discussion is similar to those in [11], we include them for completeness.
Theorem O.1 (Theorem 9 in [9], with internal non-private state) .Assume for all d‚àà X,f(x;d)
ism-strongly convex, M-Lipschitz and L-smooth in x. Define Œ≥=L‚àím
L+mandŒ∑=2
L+m. Let the
learning iteration T‚â•I+ log(2Rmn
2M)/log(1/Œ≥)for PGD (Algorithm 1 in [9]) and the unlearning
algorithm (Algorithm 2 in [9], PGD fine-tuning on learned parameters before adding Gaussian noise)
27run with Iiterations. Assume œµ=O(log(1 /Œ¥)), let the standard deviation of the output perturbation
gaussian noise œÉto be
œÉ=4‚àö
2MŒ≥I
mn(1‚àíŒ≥I)(p
log(1/Œ¥) +œµ‚àíp
log(1/Œ¥)). (93)
Then it achieves (œµ, Œ¥)-unlearning for add/remove dataset adjacency.
Theorem O.2 (Theorem 28 in [9], without internal non-private state) .Assume for all d‚àà X,f(x;d)
ism-strongly convex, M-Lipschitz and L-smooth in x. Define Œ≥=L‚àím
L+mandŒ∑=2
L+m. Let the
learning iteration T‚â•I+ log(2Rmn
2M)/log(1/Œ≥)for PGD (Algorithm 1 in [9]) and the unlearning
algorithm (Algorithm 2 in [9], PGD fine-tuning on learned parameters after adding Gaussian noise)
run with I+ log(log(4 di/Œ¥))/log(1/Œ≥)iterations for the ithsequential unlearning request, where I
satisfies
I‚â•log‚àö
2d(1‚àíŒ≥)‚àí1‚àö
2 log(2 /Œ¥)+œµ‚àí‚àö
2 log(2 /Œ¥)
log(1/Œ≥). (94)
Assume œµ=O(log(1 /Œ¥)), let the standard deviation of the output perturbation gaussian noise œÉto
be
œÉ=8MŒ≥I
mn(1‚àíŒ≥I)(p
2 log(2 /Œ¥) + 3œµ‚àíp
2 log(2 /Œ¥) + 2œµ). (95)
Then it achieves (œµ, Œ¥)-unlearning for add/remove dataset adjacency.
Note that the privacy guarantee of D2D [9] is with respect to add/remove dataset adjacency and ours
is the replacement dataset adjacency. However, by a slight modification of the proof of Theorem O.1
and O.2, one can show that a similar (but slightly worse) bound of the theorem above also holds for
D2D [9]. For simplicity and fair comparison, we directly use the bound in Theorem O.1 and O.2 in
our experiment. Note that [38] also compares a special replacement DP with standard add/remove
DP, where a data point can only be replaced with a null element in their definition. In contrast,
our replacement data adjacency allows arbitrary replacement which intuitively provides a stronger
privacy notion.
The non-private internal state of D2D. There are two different versions of the D2D algorithm
depending on whether one allows the server (model holder) to save and leverage the model parameter
before adding Gaussian noise. The main difference between Theorem O.1 and O.2 is whether their
unlearning process starts with the ‚Äúclean‚Äù model parameter (Theorem O.1) or the noisy model
parameter (Theorem O.2). Clearly, allowing the server to keep and leverage the non-private internal
state provides a weaker notion of privacy [9]. In contrast, our PNSGD unlearning approach by default
only keeps the noisy parameter so that we do not save any non-private internal state. As a result, one
should compare the PNSGD unlearning to D2D with Theorem O.2 for a fair comparison.
P Unlearning Guarantees of Langevin Unlearning [11]
In this section, we restate the main results of Langevin unlearning [11] and provide a detailed
comparison and discussion for the case of strongly convex objective functions.
Theorem P.1 (RU guarantee of PNGD unlearning, strong convexity) .Assume for all D ‚àà Xn,fDis
L-smooth, M-Lipschitz, m-strongly convex and ŒΩDsatisfies CLSI-LSI. Let the learning process follow
the PNGD update and chooseœÉ2
m< C LSIandŒ∑‚â§min(2
m(1‚àíœÉ2
mC LSI),1
L). Given Mis(Œ±, Œµ0)-RDP
andy0=x‚àû=M(D), forŒ± > 1, the output of the KthPNGD unlearning iteration achieves
(Œ±, Œµ)-RU, where
Œµ‚â§exp
‚àí1
Œ±2œÉ2Œ∑K
CLSI
Œµ0. (96)
Theorem P.2 (RDP guarantee of PNGD learning, strong convexity) .Assume f(¬∑;d)beL-smooth,
M-Lipschitz and m-strongly convex for all d‚àà X. Also, assume that the initialization of PNGD
28satisfies2œÉ2
m-LSI. Then by choosing 0< Œ∑‚â§1
Lwith a constant, the PNGD learning process is
(Œ±, Œµ(S)
0)-RDP of group size S‚â•1atTthiteration with
Œµ(S)
0‚â§4Œ±S2M2
mœÉ2n2(1‚àíexp(‚àímŒ∑T)). (97)
Furthermore, the law of the PNGD learning process is2œÉ2
m-LSI for any time step.
Combining these two results, the (Œ±, Œµ)-RU guarantee for Langevin unlearning is
Œµ‚â§exp
‚àímŒ∑K
Œ±4Œ±S2M2
mœÉ2n2. (98)
On the other hand, combining our Theorem 3.2, 3.3 and using the worst case bound on ZBalong
with some simplification, we have
Œµ‚â§c2Kn/b Œ±
2Œ∑œÉ22Œ∑M
(1‚àíc)b2
= (1‚àíŒ∑m)2Kn/b Œ±
2Œ∑œÉ22M
mb2
= (1‚àíŒ∑m)2Kn/b 2Œ±M2
Œ∑m2œÉ2b2.
(99)
To make an easier comparison, we further simplify the above bound with b=nand1‚àíx <exp(‚àíx)
which holds for all x >0:
Œµ‚â§(1‚àíŒ∑m)2Kn/b 2Œ±M2
Œ∑m2œÉ2b2<exp(‚àí2KnŒ∑m
b)4Œ±M2
mœÉ2b2√ó1
2mŒ∑= exp( ‚àí2KŒ∑m )4Œ±M2
mœÉ2n2√ó1
2mŒ∑.
(100)
Now we compare the bound (98) obtained by Langevin dynamic in [11] and our bound (100) based on
PABI analysis. First notice that the ‚Äúinitial distance‚Äù in (100) has an additional factor 1/(2mŒ∑). When
we choose the largest possible step size Œ∑=1
L, then this factor isL
2mwhich is most likely greater
than1in a real-world scenario unless the objective function is very close to a quadratic function. As
a result, when Kis sufficiently small (i.e., K= 1), possible that the bound (98) results in a better
privacy guarantee when unlearning one data point. This is observed in our experiment, Figure 3a.
Nevertheless, note that the decaying rate of (100) isindependent of the R√©nyi divergence order Œ±,
which actually leads to a much better rate in practice. More specifically, for the case n= 104(which
is roughly our experiment setting). To achieve (1,1/n)-unlearning guarantee the corresponding
Œ±is roughly at scale Œ±‚âà10, since log(n)/(Œ±‚àí1)needs to be less than 1in the (Œ±, Œµ)-RU to
(œµ, Œ¥)-unlearning conversion (Proposition N.2). As a result, the decaying rate of (100) obtained by
PABI analysis is superior by a factor of 2Œ±, which implies a roughly 20x saving in complexity for K
large enough.
Comparing sequential unlearning. Notably, another important benefit of our bound obtained by
PABI analysis is that it is significantly better in the scenario of sequential unlearning. Since we only
need standard triangle inequality for the upper bound Z(s+1)
B = min( cKsn/bZ(s)
B+ZB,2R)of the
(s+ 1)thunlearning request, only the ‚Äúinitial distance‚Äù is affected and it grows at most linearly in s
(i.e., for the convex only case). In contrast, since Langevin dynamic analysis in [11] requires the use
of weak triangle inequality of R√©nyi divergence, the Œ±in their bound will roughly grow at scale 2s
which not only affects the initial distance but also the decaying rate. As pointed out in our experiment
and by the author [11] themselves, their result does not support many sequential unlearning requests.
This is yet another important benefit of our analysis based on PABI for unlearning.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: All claimed theoretical results are provided in Section 3 and empirical studies
are provided in 4.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitation in section B.
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
30Justification: All assumptions are stated for each theorem and proof for every theoretical
statement is provided in the Appendix E to M.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The code is provided in the supplementary material and all experimental details
are stated in Appendix N.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
31Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: All data used in the experiments are publicly available with open-access
licenses. The code is provided in the supplementary material and all experimental details
are stated in Appendix N.
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All experimental details are stated in Appendix N.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: All experimental results are reported with one standard deviation gathered
from 100 independent trials, which is stated in Section 4.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
32‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The compute resources used for our experiments are detailed in Appendix N.3.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have read the NeurIPS Code of Ethics and affirm that this work
conforms with it in every respect.
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We briefly discuss why our work does not have any negative societal impact in
Section C
Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
33‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All used datasets are provided with citations to the original authors and works,
where all of them have proper open access license.
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
34‚Ä¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
35‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
36