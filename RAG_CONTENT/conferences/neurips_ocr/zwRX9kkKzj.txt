Published in Transactions on Machine Learning Research (09/2022)
Multitask Online Mirror Descent
Nicolò Cesa-Bianchi nicolo.cesa-bianchi@unimi.it
Department of Computer Science, University of Milan, Italy
Pierre Laforgue pierre.laforgue@unimi.it
Department of Computer Science, University of Milan, Italy
Andrea Paudice andrea.paudice@unimi.it
Department of Computer Science, University of Milan, Italy
Istituto Italiano di Tecnologia, Genova, Italy
Massimiliano Pontil massimiliano.pontil@iit.it
Istituto Italiano di Tecnologia, Genova, Italy
University College London, United Kingdom
Reviewed on OpenReview: https: // openreview. net/ forum? id= zwRX9kkKzj
Abstract
We introduce and analyze MT-OMD, a multitask generalization of Online Mirror Descent
(OMD) which operates by sharing updates between tasks. We prove that the regret of
MT-OMD is of order/radicalbig
1 +σ2(N−1)√
T, whereσ2is the task variance according to the
geometry induced by the regularizer, Nis the number of tasks, and Tis the time horizon.
Whenever tasks are similar, that is σ2≤1, our method improves upon the√
NTbound
obtained by running independent OMDs on each task. We further provide a matching
lower bound, and show that our multitask extensions of Online Gradient Descent and
Exponentiated Gradient, two major instances of OMD, enjoy closed-form updates, making
them easy to use in practice. Finally, we present experiments which support our theoretical
ﬁndings.
1 Introduction
In multitask learning (Caruana, 1997), one faces a set of tasks to solve, and tries to leverage their similarities
to learn faster. Task similarity is often formalized in terms of Euclidean distances among the best performing
models for each task, see Evgeniou & Pontil (2004) for an example. However, in online convex optimization,
and Online Mirror Descent (OMD) in particular, it is well known that using diﬀerent geometries to measure
distances in the model space can bring substantial advantages — see, e.g., Hazan (2016); Orabona (2019).
For instance, when the model space is the probability simplex in Rd, running OMD with the KL divergence
(corresponding to an entropic regularizer) allows one to learn at a rate depending only logarithmically on d.
It is thus natural to investigate to what extent measuring task similarities using geometries that are possibly
non-Euclidean could improve the analysis of online multitask learning. From an application perspective,
typical online multitask scenarios include federated learning applications for mobile users (e.g., personalized
recommendation or health monitoring) or for smart homes (e.g., energy consumption prediction), mobile
sensor networks for environmental monitoring, or even networked weather forecasting. These scenarios ﬁt
well with online learning, as new data is being generated all the time, and require diﬀerent losses and decision
sets, motivating the design of a general framework.
In this work, we introduce MT-OMD, a multitask generalization of OMD which applies to any strongly convex
regularizer. We present a regret analysis establishing that MT-OMD outperforms OMD (run independently
on each task) whenever tasks are similar according to the geometry induced by the regularizer. Our work
1Published in Transactions on Machine Learning Research (09/2022)
builds on the multitask extension of the Perceptron algorithm developed in Cavallanti et al. (2010), where
prior knowledge about task similarities is expressed through a symmetric positive deﬁnite interaction matrix
A. Typically, A=I+L, whereLis the Laplacian of a task relatedness graph with adjacency matrix W. The
authors then show that the number of mistakes depends on/summationtext
i/bardblui/bardbl2
2+/summationtext
i,jWij/bardblui−uj/bardbl2
2, where each ui
denotes the best model for task i. This expression can be seen as a measure of task dispersion with respect
to matrixWand norm/bardbl·/bardbl 2. The Euclidean norm appears because the Perceptron is an instance of OMD
for the hinge loss with the Euclidean regularizer, so that distances in the model space are measured through
the corresponding Bregman divergence, which is the Euclidean squared norm.
For an arbitrary strongly convex regularizer ψ, the regret of OMD is controlled by a Bregman divergence
and a term inversely proportional to the curvature of the regularizer. The key challenge we face is how to
extend the OMD regularizer to the multitask setting so that the dispersion term captures task similarities. A
natural strategy would be to choose a regularizer whose Bregman divergence features/summationtext
i,jWijBψ(ui,uj).
Although this mimics the Euclidean dispersion term of the Perceptron, the associated regularizer has a small
curvature, compromising the divergence-curvature balance which, as we said, controls the regret. Observing
that the Perceptron’s dispersion term can be rewritten /bardblA1/2u/bardbl2
2, whereAis a block version (across tasks)
ofAanduis the concatenation of the reference vectors ui, our solution consists in using the regularizer
 /parenleftbig
A1/2·/parenrightbig
, where is the compound version of any base regularizer ψdeﬁned on the model space. While
exhibiting the right curvature, this regularizer has still the drawback that A1/2umight be outside the domain
of . To get around this diﬃculty, we introduce a notion of variance aligned with the geometry induced by
 , such that the corresponding Bregman divergence B /parenleftbig
A1/2u,A1/2v/parenrightbig
is always deﬁned for sets of tasks
with small variance. We then show that the Bregman divergence can be upper bounded in terms of the task
varianceσ2, and by tuning appropriately the matrix Awe obtain a regret bound for MT-OMD that scales
as/radicalbig
1 +σ2(N−1). In contrast, the regret of independent OMD scales as√
N, highlighting the advantage
brought by MT-OMD when tasks have a small variance. We stress that this improvement is independent of
the chosen regularizer, thereby oﬀering a substantial acceleration in a wide range of scenarios. To keep the
exposition simple, we ﬁrst work with a ﬁxed and known σ2. We then show an extension of MT-OMD that
does not require any prior knowledge on the task similarity. The rest of the paper is organized as follows.
In Section 2, we introduce the multitask online learning problem and describe MT-OMD, our multitask
extension to solve it. In Section 3, we derive a regret analysis for MT-OMD, which highlights its advantage
when tasks are similar. Section 4 is devoted to algorithmic implementations, and Section 5 to experiments.
Related work. Starting from the seminal work by Caruana (1997), multitask learning has been intensively
studied for more than two decades, see Zhang & Yang (2021) for a recent survey. Similarly to Cavallanti et al.
(2010), our work is inspired by the Laplacian multitask framework of Evgeniou et al. (2005). This framework
has been extended to kernel-based learning (Sheldon, 2008), kernel-based unsupervised learning (Gu et al.,
2011), contextual bandits (Cesa-Bianchi et al., 2013), spectral clustering (Yang et al., 2014), stratiﬁed model
learning (Tuck et al., 2021), and, more recently, federated learning (Dinh et al., 2021). See also Herbster
& Lever (2009) for diﬀerent applications of Laplacians in online learning. A multitask version of OMD has
been previously proposed by Kakade et al. (2012). Their approach, unlike ours, is cast in terms of matrix
learning, and uses group norms and Schatten p-norm regularizers. Their bounds scale with the diameter of
the model space according to these norms (as opposed to scaling with the task variance, as in our analysis).
Moreover, their learning bias is limited to the choice of the matrix norm regularizer and does not explicitly
include a notion of task similarity matrix. Abernethy et al. (2007); Dekel et al. (2007) investigate diﬀerent
multitask extensions of online learning, see also Alquier et al. (2017); Finn et al. (2019); Balcan et al. (2019);
Denevi et al. (2019) for related extensions to meta-learning. Some online multitask applications are studied in
Pillonetto et al. (2008); Li et al. (2014; 2019), but without providing any regret analyses. Saha et al. (2011);
Zhang et al. (2018) extend the results of Cavallanti et al. (2010) to dynamically updated interaction matrices.
However, no regret bounds are provided. Murugesan et al. (2016) look at distributed online classiﬁcation and
prove regret bounds, but they are not applicable in our asynchronous model. Other approaches for learning
task similarities include Zhang & Yeung (2010); Pentina & Lampert (2017); Shui et al. (2019). We ﬁnally
note the recent work by Boursier et al. (2022), which establishes multitask learning guarantees with trace
norm regularization when the number of samples per task is small, and that by Laforgue et al. (2022), which
learns jointly the tasks and their structure, but only with the Euclidean regularizer and under the assumption
that the task activations are stochastic.
2Published in Transactions on Machine Learning Research (09/2022)
Although our asynchronous multitask setting is identical to that of Cavallanti et al. (2010), we emphasize
that our work extends theirs much beyond the fact that we consider arbitrary convex losses instead of just
the hinge loss. Algorithmically, MT-OMD generalizes the Multitask Perceptron in much the same way OMD
generalizes the standard Perceptron. From a technical point of view, Theorem 1 in Cavallanti et al. (2010) is
a direct consequence of the Kernel Perceptron Theorem, and is therefore limited to Euclidean geometries.
Instead, our work provides a complete analysis of all regularizers of the form  (A1/2·). Although Cavallanti
et al. (2010) also contains a non-Euclidean p-norm extension of the Multitask Perceptron, we point out that
their extension is based on a regularizer of the form /bardblAu/bardbl2
p. This is diﬀerent from MT-OMD for p-norms,
which instead uses/summationtext
i/bardbl(A1/2u)(i)/bardbl2
p. As a consequence, their bound is worse than ours (see Appendix C for
technical details), does not feature any variance term, and does not specialize to the Euclidean case when
p= 2. Note that our analysis on the simplex is also completely novel as far as we know.
2 Multitask Online Learning
We now describe the multitask online learning problem, and introduce our approach to solve it. We use a
cooperative and asynchronous multiagent formalism: the online algorithm is run in a distributed fashion by
communicating agents, that however make predictions at diﬀerent time steps.
Problem formulation and reminders on OMD. We consider an online convex optimization setting
with a set of N∈Nagents, each learning a possibly diﬀerent task on a common convex decision set V⊂Rd.
At each time step t= 1,2,...some agent it≤Nmakes a prediction xt∈Vfor its task, incurs loss /lscriptt(xt),
and observes a subgradient of /lscripttatxt, where/lscripttis a convex loss function. We say that itis the active agent
at timet. Both the sequence i1,i2,...of active agents and the sequence /lscript1,/lscript2,...of convex losses are chosen
adversarially and hidden from the agents. Note that the algorithm we propose is deterministic, such that /lscriptt
might be indiﬀerently chosen before xtis predicted (oblivious adversary) or after. Our goal is to minimize
themultitask regret , which is deﬁned as the sum of the individual regrets
RT=N/summationdisplay
i=1/parenleftBigg/summationdisplay
t:it=i/lscriptt(xt)−inf
u∈V/summationdisplay
t:it=i/lscriptt(u)/parenrightBigg
=T/summationdisplay
t=1/lscriptt(xt)−N/summationdisplay
i=1inf
u∈V/summationdisplay
t:it=i/lscriptt(u). (1)
A natural idea to minimize Equation (1) is to run Nindependent OMDs, one for each agent. Recall that OMD
refers to a family of algorithms, typically used to minimize a regret of the form/summationtext
t/lscriptt(xt)−infu∈V/summationtext
t/lscriptt(u),
for any sequence of proper convex loss functions /lscriptt. An instance of OMD is parameterized by a λ-strongly
convex regularizer ψ:Rd→R, and has the update rule
xt+1= arg min
x∈V/angbracketleftηtgt,x/angbracketright+Bψ(x,xt), (2)
wheregt∈Rdis a subgradient of /lscripttat pointxt,Bψ(x,y) =ψ(x)−ψ(y)−/angbracketleft∇ψ(y),x−y/angbracketrightdenotes the Bregman
divergence associated to ψ, andηt>0is a tunable learning rate. Standard results allow to bound the regret
achieved by the sequence of iterates produced by OMD. For a ﬁxed ηand any initial point x1∈V, we have
(Orabona, 2019, Theorem 6.8) that for all u∈V
T/summationdisplay
t=1/lscriptt(xt)−/lscriptt(u)≤Bψ(u,x1)
η+η
2λT/summationdisplay
t=1/bardblgt/bardbl2
⋆, (3)
with/bardbl·/bardbl⋆the dual norm of the norm with respect to which ψis strongly convex (see Deﬁnition 4 in the
Appendix). The choice of the regularizer ψshapes the above bound through the quantities Bψ(u,x1)and
/bardblgt/bardbl⋆. Whenψ=1
2/bardbl·/bardbl2
2, we haveBψ(x,y) =1
2/bardblx−y/bardbl2
2,/bardbl·/bardbl⋆=/bardbl·/bardbl 2,λ= 1, and the algorithm is called
Online Gradient Descent (OGD). However, depending on the problem, a diﬀerent choice of the regularizer
might better captures the underlying geometry. A well-known example is Exponentiated Gradient (EG), an
instance of OMD in which Vis the probability simplex in Rd, such thatV= ∆ :={x∈Rd
+:/summationtext
jxj= 1}. EG
uses the negative entropy regularizer x/mapsto→/summationtext
jxjln(xj), and assuming that /bardblgt/bardbl∞≤Lg, one achieves bounds
of orderO(Lg√
Tlnd), while OGD yields bounds of order O(Lg√
dT). We emphasize that our cooperative
3Published in Transactions on Machine Learning Research (09/2022)
extension adapts to several types of regularizers, and can therefore exploit these improvements with respect
to the dependence on d, see Proposition 8. Let Cbe a generic constant such that C√
Tbounds the regret
incurred by the chosen OMD (e.g., C=Lg√
lnd, orC=Lg√
dabove). Then, by Jensen’s inequality the
multitask regret of Nindependent OMDs satisﬁes
RT≤N/summationdisplay
i=1C/radicalbig
Ti≤C√
NT, (4)
whereTi=/summationtextT
t=1I{it=i}denotes the number of times agent iwas active. Our goal is to show that
introducing communication between the agents may signiﬁcantly improve on Equation (4) with respect to
the dependence on N.
A multitask extension. We now describe our multitask OMD approach. To gain some insights on it, we
ﬁrst focus on OGD. For i≤Nandt≤T, letxi,t∈Rddenote the prediction maintained by agent iat time
stept. By completing the square in Equation (2) for ψ=ψEuc:=1
2/bardbl·/bardbl2
2, the independent OGDs updates
can be rewritten for all i≤Nandtsuch thatit=i:
xi,t+1= ΠV,/bardbl·/bardbl2/parenleftbig
xi,t−ηtgt/parenrightbig
, (5)
where ΠV,/bardbl·/bardbldenotes the projection operator onto the convex set Vaccording to the norm /bardbl·/bardbl, that is
ΠV,/bardbl·/bardbl(x) =arg miny∈V/bardblx−y/bardbl. Our analysis relies on compound representations , that we explain next.
We use bold notation to refer to compound vectors, such that for u1,...,uN∈Rd, the compound vector
isu= [u1,...,uN]∈RNd. Fori≤N, we useu(i)to refer to the ithblock ofu, such thatu(i)=uiin
the above example. So xtis the compound vector of the (xi,t)N
i=1, such that xt=x(it)
t, and the multitask
regret rewrites as RT(u) =/summationtextT
t=1/lscriptt/parenleftbig
x(it)
t/parenrightbig
−/lscriptt/parenleftbig
u(it)/parenrightbig
. For any set V⊂Rd, letV=V⊗N⊂RNddenote the
compound set such that u∈Vis equivalent to u(i)∈Vfor alli≤N. Equipped with this notation, the
independent OGD updates Equation (5) rewrite as
xt+1= ΠV,/bardbl·/bardbl2/parenleftbig
xt−ηtgt/parenrightbig
, (6)
withgt∈RNdsuch thatg(i)
t=gtfori=it, and 0Rdotherwise. In other words, only the active agent has
a non-zero gradient and therefore makes an update. Our goal is to incorporate communication into this
independent update. To that end, we consider the general idea of sharing updates by considering (sub)
gradients of the form A−1gt, whereA−1∈RNd×Ndis a shortcut notation for A−1⊗IdandA∈RN×Nis
any symmetric positive deﬁnite interaction matrix. Note that Ais a parameter of the algorithm playing the
role of a learning bias. While our central result ( Theorem 1) holds for any choice of A, our more specialized
bounds (see Propositions 5 to 8) apply to a parameterized family of matrices A. A simple computation shows
that (A−1gt)(i)=A−1
iitgt. Thus, every agent imakes an update proportional to A−1
iitat each time step t. In
other words, the active agent (the only one to suﬀer a loss) shares its update with the other agents. Results
in Section 3 are proved by designing a matrix A−1(or equivalently A) such that A−1
iitcaptures the similarity
between tasks iandit. Intuitively, the active agent itshould share its update (gradient) with another agent i
to the extent their respective tasks are similar. Overall, denoting by /bardblu/bardblM=√
u/latticetopMuthe Mahalanobis norm
ofu, the MT-OGD update writes
xt+1= ΠV,/bardbl·/bardblA/parenleftbig
xt−ηtA−1gt/parenrightbig
. (7)
In comparison to Equation (6), the need for changing the norm in the projection, although unclear at ﬁrst
sight, can be explained in multiple ways. First, it is key to the analysis, as we see in the proof of Theorem 1.
Second, it can be interpreted as another way of exchanging information between agents, see Remark 1. Finally,
note that update Equation (7) can be decomposed as
/tildewidext+1= arg min
x∈RNd/angbracketleftηtgt,x/angbracketright+1
2/bardblx−xt/bardbl2
A,
xt+1= arg min
x∈V1
2/bardblx−/tildewidext+1/bardbl2
A,(8)
4Published in Transactions on Machine Learning Research (09/2022)
showing that it is natural to keep the same norm in both updates. Most importantly, what Equation (8) tells
us, is that the MT-OGD update rule is actually an OMD update—see e.g., (Orabona, 2019, Section 6.4)—
with regularizer x/mapsto→1
2/bardblx/bardbl2
A= Euc/parenleftbig
A1/2x/parenrightbig
. This provides a natural path for extending our multitask
approach to any regularizer. Given a base regularizer ψ:Rd→R, thecompound regularizer  is given
by :x∈RNd/mapsto→/summationtextN
i=1ψ/parenleftbig
x(i)/parenrightbig
. When there exists a function φ:R→Rsuch thatψ(x) =/summationtext
jφ(xj),
the compound regularizer is the natural extension of ψtoRNd. Note, however, that the relationship can
be more complex, e.g., when ψ(x) =1
2/bardblx/bardbl2
p. Using regularizer  /parenleftbig
A1/2·/parenrightbig
, whose associate divergence is
B /parenleftbig
A1/2x,A1/2x/prime/parenrightbig
, the MT-OMD update thus reads
xt+1= arg min
x∈V/angbracketleftηtgt,x/angbracketright+B /parenleftbig
A1/2x,A1/2xt/parenrightbig
. (9)
Clearly, ifψ=ψEuc, we recover the MT-OGD update. Observe also that whenever A=IN, MT-OMD is
equivalent to Nindependent OMDs. We conclude this exposition with a remark shedding light on the way
MT-OMD introduces communication between agents.
Remark 1 (Communication mechanism in MT-OMD) .Denotingyt=A1/2xt, Equation (9)rewrites
xt+1=A−1/2arg min
y∈A1/2(V)/angbracketleftηtA−1/2gt,y/angbracketright+B (y,yt). (10)
The two occurrences of A−1/2reveal that agents communicate in two distinct ways: one through the shared
update (the innermost occurrence of A−1/2), and one through computing the ﬁnal prediction xt+1as a linear
combination of the solution to the optimization problem. Multiplying Equation (10)byA1/2, MT-OMD can
also be seen as standard OMD on the transformed iterate yt.
3 Regret Analysis
We now provide a regret analysis for MT-OMD. We start with a general theorem presenting two bounds, for
constant and time-varying learning rates. These results are then instantiated to diﬀerent types of regularizer
and variance in Propositions 2 to 8. The main diﬃculty is to characterize the strong convexity of  /parenleftbig
A1/2·/parenrightbig
,
see Lemmas 11 and 12 in the Appendix. Throughout the section, V⊂Rdis a convex set of comparators, and
(/lscriptt)T
t=1is a sequence of proper convex loss functions chosen by the adversary. Note that all technical proofs
can be found in Appendix A.
Theorem 1. Letψ:Rd→Rbeλ-strongly convex with respect to norm /bardbl·/bardblonV, letA∈RN×Nbe
symmetric positive deﬁnite, and set x1∈V. Then, MT-OMD with ηt:=ηproduces a sequence of iterates
(xt)T
t=1such that for all u∈V,RT(u)is bounded by
B /parenleftbig
A1/2u,A1/2x1/parenrightbig
η+ max
i≤NA−1
iiη
2λT/summationdisplay
t=1/bardblgt/bardbl2
⋆. (11)
Moreover, for any sequence of nonincreasing learning rates (ηt)T
t=1, MT-OMD produces a sequence of iterates
(xt)T
t=1such that for all u∈V,RT(u)is bounded by
max
t≤TB /parenleftbig
A1/2u,A1/2xt/parenrightbig
ηT+ max
i≤NA−1
ii1
2λT/summationdisplay
t=1ηt/bardblgt/bardbl2
⋆. (12)
3.1 Multitask Online Gradient Descent
Forψ=1
2/bardbl·/bardbl2
2,A=IN(independent updates), unit-norm reference vectors (u(i))N
i=1,Lg-Lipschitz losses,
andx1=0, bound Equation (11) becomes: ND2/2η+ηTL2
g/2. Choosing η=D√
N/Lg√
T, we recover the
DLg√
NTbound of Equation (4). Our goal is to design interaction matrices Athat make Equation (11)
smaller. In the absence of additional assumptions on the set of comparators, it is however impossible to
get a systematic improvement: the bound is a sum of two terms, and introducing interactions typically
reduces one term but increases the other. To get around this diﬃculty, we introduce a simple condition on
the task similarity, that allows us to control the increase of B /parenleftbig
A1/2u,A1/2x1/parenrightbig
for a carefully designed class
of interaction matrices.
5Published in Transactions on Machine Learning Research (09/2022)
Deﬁnition 1. Let/bardbl·/bardbl:Rd→Rbe any norm, and ¯u= (1/N)/summationtextN
i=1u(i), for anyu∈RNd. We deﬁne the
variance ofuw.r.t./bardbl·/bardblas
Var/bardbl·/bardbl(u) =1
N−1N/summationdisplay
i=1/vextenddouble/vextenddoubleu(i)−¯u/vextenddouble/vextenddouble2.
LetD= supu∈V/bardblu/bardbl, andσ>0. The comparators with variance smaller than σ2D2are denoted by
V/bardbl·/bardbl,σ=/braceleftbig
u∈V: Var/bardbl·/bardbl(u)≤σ2D2/bracerightbig
. (13)
For sets of comparators of the form Equation (13), we show that MT-OGD achieves signiﬁcant improvements
over its independent counterpart. The rationale behind this gain is fairly natural: the tasks associated with
comparators in Equation (13) are similar due to the variance constraint, so that communication indeed helps.
Note that condition Equation (13) does not enforce any restriction on the norms of the individual u(i), and is
much more complex than a simple rescaling of the feasible set by σ2. For instance, one could imagine task
vectors highly concentrated around some vector u0, whose norm is D: the individual norms are close to D,
but the task variance is small. This is precisely the construction used in the separation result (Proposition 4).
As MT-OMD leverages the additional information of the task variance (unavailable in the independent case),
it is expected that an improvement should be possible. The problems of how to use this extra information
and what improvement can be achieved through it are addressed in the rest of this section. To that end, we
ﬁrst assume σ2to be known. This assumption can be seen as a learning bias, analog to the knowledge of the
diameterDin standard OGD bounds. In Section 3.4, we then detail a Hedge-based extension of MT-OGD
that does not require the knowledge of σ2and only suﬀers an additional regret of order√TlogN.
The class of interaction matrices we consider is deﬁned as follows. Let L=IN− 11/latticetop/N. We consider
matrices of the form A(b) =IN+bL, whereb≥0quantiﬁes the magnitude of the communication. For
more intuition about this choice, see Section 3.4. We can now state a ﬁrst result highlighting the advantage
brought by MT-OGD.
Proposition 2. Letψ=1
2/bardbl·/bardbl2
2,D=supx∈V/bardblx/bardbl2, andσ≤1. Assume that/bardbl∂/lscriptt(x)/bardbl2≤Lgfor allt≤T
and anyx∈V. Setb=N,x1=0, andη=D/radicalbig
N(N+ 1)(1 + (N−1)σ2)/Lg√
2T. Then, MT-OGD
produces a sequence of iterates (xt)T
t=1such that for all u∈V/bardbl·/bardbl2,σ
RT(u)≤DLg/radicalbig
1 +σ2(N−1)√
2T. (14)
Proof sketch. Withψ=1
2/bardbl·/bardbl2
2, andx1=0, we have 2B /parenleftbig
A(b)1/2u,A(b)1/20/parenrightbig
=/bardblu/bardbl2
2+b(N−1)Var/bardbl·/bardbl2(u),
which is smaller than ND2(1 +bN−1
Nσ2). Then, it is easy to check that/bracketleftbig
A(b)−1/bracketrightbig
ii=b+N
(1+b)Nfor alli≤N.
Substituting these values into Equation (11), we obtain
RT(u)≤ND2(1 +bN−1
Nσ2)
2η+ηTL2
g
2b+N
(1 +b)N.
Finally, set η=ND
Lg/radicalbigg/parenleftbig
1+bN−1
Nσ2/parenrightbig
(1+b)
(b+N)Tandb=N.
Thus, MT-OGD enjoys a/radicalbig
1 +σ2(N−1)dependence, which is smaller than√
Nwhen tasks have a variance
smaller than 1. Whenσ= 0(all tasks are equal), MT-OGD scales as if there were only one task. When
σ≥1, the analysis suggests to choose b= 0, i.e.,A=IN, and one recovers the performance of independent
OGDs. Note that the additional√
2factor in Equation (14) can be removed for limit cases through a better
optimization in b: the bound obtained in the proof actually reads DL/radicalbig
F(σ)T, withF(0) = 1andF(1) =N.
However, the function Flacks of interpretability outside of the limit cases (for details see Appendix A.2)
motivating our choice to present the looser but more interpretable bound Equation (14). For a large N, we
have/radicalbig
1 +σ2(N−1)≈σ√
N. The improvement brought by MT-OGD is thus roughly proportional to the
square root of the task variance. From now on, we refer to this gain as the multitask acceleration . This
improvement achieved by MT-OGD is actually optimal up to constants, as revealed by the following lower
bound, which is only 1/4of Equation (14).
6Published in Transactions on Machine Learning Research (09/2022)
Proposition 3. Under the conditions of Proposition 2, the regret of any algorithm satisﬁes
sup
u∈V/bardbl·/bardbl2,σRT(u)≥1
4/parenleftBig
DLg/radicalbig
1 +σ2(N−1)√
2T/parenrightBig
.
Another way to gain intuition about Equation (14) is to compare it to the lower bound for OGD considering
independent tasks (IT-OGD). The following separation result shows that MT-OGD may strictly improve
over IT-OGD.
Proposition 4. Letd≥9,N= 2d, andσ≤1to be tuned later. Then, there exists u∈V/bardbl·/bardbl2,σsuch that
RIT−OGD
T (u)≥/radicalbig
(1−2σ2)N
4√
2T.
Proposition 2 then yields that for any σ2<N−16
18N−16
RIT−OGD
T (u)>RMT−OGD
T (u).
3.2 Extension to any Norm Regularizers
A natural question is: can the multitask acceleration be achieved with other regularizers? Indeed, the proof of
Proposition 2 crucially relies on the fact that the Bregman divergence can be exactly expressed in terms of
/bardblu/bardbl2
2andVar/bardbl·/bardbl2(u). In the following proposition, we show that such an improvement is also possible for all
regularizers of the form1
2/bardbl·/bardbl2, for arbitrary norms /bardbl·/bardbl, up to an additional multiplicative constant. A crucial
application is the use of the p-norm on the probability simplex, which is known to exhibit a logarithmic
dependence in dfor a well-chosen p.
Proposition 5. Let/bardbl·/bardbl:Rd→Rbe any norm, ψ=1
2/bardbl·/bardbl2,D=supx∈V/bardblx/bardbl, andσ≤1. Assume that
/bardbl∂/lscriptt(x)/bardbl⋆≤Lgfor allt≤T,x∈V. Setb=N,x1=0andη=D/radicalbig
N(N+ 1)(1 + (N−1)σ2)/Lg√
2T.
Then, MT-OMD produces a sequence of iterates (xt)T
t=1such that for all u∈V/bardbl·/bardbl,σ
RT(u)≤DLg/radicalbig
1 +σ2(N−1)√
8T.
In particular, for d≥3andV= ∆, choosing/bardbl·/bardbl=/bardbl·/bardblp, forp= 2lnd/(2lnd−1), and assuming that
/bardbl∂/lscriptt(x)/bardbl∞≤Lg, it holds for all u∈/bardbl·/bardblp,σ
RT(u)≤Lg/radicalbig
1 +σ2(N−1)√
16e Tlnd.
In comparison, under the same assumptions, bound Equation (14)would write as: Lg/radicalbig
1 +σ2(N−1)√
2Td.
Projecting onto Vσ.Propositions 2 and 5 reveal that whenever tasks are similar (i.e., whenever u∈Vσ),
then using the regularizer  /parenleftbig
A1/2·/parenrightbig
withA/negationslash=INaccelerates the convergence. However, this is not the only
way to leverage the small variance condition. For instance, one may also use this information to directly
project ontoVσ⊂V, by considering the update
xt+1= arg min
x∈Vσ/angbracketleftηtgt,x/angbracketright+B /parenleftbig
A1/2x,A1/2xt/parenrightbig
. (15)
Although not necessary in general (Propositions 2 and 5 show that communicating the gradients is suﬃcient
to get an improvement), this reﬁnement presents several advantages. First, it might be simpler to compute in
practice, see Section 4. Second, it allows for adaptive learning rates, that preserve the guarantees while being
independent from the horizon T(Proposition 6). Finally, it allows to derive L⋆bounds with the multitask
acceleration for smooth loss functions (Proposition 7). Results are stated for arbitrary norms, but bounds
sharper by a factor 2can be obtained for /bardbl·/bardbl 2.
7Published in Transactions on Machine Learning Research (09/2022)
Proposition 6. Let/bardbl·/bardbl:Rd→Rbe any norm, ψ=1
2/bardbl·/bardbl2,D=supx∈V/bardblx/bardbl, andσ≤1. Setb=N, and
ηt=D/radicalbig
N(N+ 1)(1 + (N−1)σ2)(/summationtextt
i=1/bardblgi/bardbl2
⋆)−1/2. Then, Equation (15)produces a sequence of iterates
(xt)T
t=1such that for all u∈V/bardbl·/bardbl,σ
RT(u)≤8D/radicalbig
1 +σ2(N−1)/parenleftbiggT/summationdisplay
t=1/bardblgt/bardbl2
⋆/parenrightbigg1/2
.
Proposition 7. Let/bardbl·/bardbl:Rd→Rbe any norm, ψ=1
2/bardbl·/bardbl2,D=supx∈V/bardblx/bardbl, andσ≤1. Assume that the
/lscripttareM-smooth, i.e.,/bardbl∇/lscriptt(x)−∇/lscriptt(y)/bardbl⋆≤M/bardblx−y/bardblfor allt≤T, and anyx,y∈V. Setbandηtas in
Proposition 6. Then, update Equation (15)produces a sequence of iterates (xt)T
t=1such that for all u∈V/bardbl·/bardbl,σ
RT(u)≤16D/radicalbig
1 +σ2(N−1)
2MD/radicalbig
1 +σ2(N−1) +/radicaltp/radicalvertex/radicalvertex/radicalbtMT/summationdisplay
t=1/lscriptt/parenleftbig
u(it)/parenrightbig
.
Remark 2 (Strongly convex and exp-concave losses) .Another popular assumption to derive improved regret
bounds is to consider strongly convex or exp-concave losses. Recall that a function fis said to be α-exp-concave
ifexp(−αf)is concave (for instance, the logistic loss of a linear predictor with norm bounded by Uand unit-
norm inputs is exp(−2U)/2-exp-concave). In this context, standard single task algorithms achieve improved
regret bounds of order lnT, see e.g., (Orabona, 2019, Corollary 7.24 and Section 7.10). We highlight that a
simple adaptation of the single task analysis is not enough to exhibit a multitask acceleration in these cases.
Indeed, the cornerstone of our analysis is to leverage the compound representation, in which the interactions
are more easily analyzed. On the other hand, the strong convexity or exp-concavity of the losses provide a
sharper control on the instantaneous regret that depends on the norm of the active predictor/comparator only,
but cannot be extended to the compound framework. Another way to look at the problem is to recall that FTRL
deals with strongly convex losses by choosing ψ≡0. This means /tildewide = (A1/2·)≡0, such that MT-FTRL is
equivalent to independent FTRL and no multitask improvement can be achieved.
3.3 Regularizers on the Simplex
As seen in Propositions 2 to 7, MT-OMD induces a multitask acceleration in a wide range of settings, involving
diﬀerent regularizers (Euclidean norm, p-norms) and various kind of loss functions (Lipschitz continuous,
smooth continuous gradients). This systematic gain suggests that multitask acceleration essentially derives
from our approach, and is completely orthogonal to the improvements achievable by choosing the regularizer
appropriately. Bounds combining both beneﬁts are actually derived in the second claim of Proposition 5.
However, all regularizers studied so far share a crucial feature: they are deﬁned on the entire space Rd.
As a consequence, the divergence B (A1/2u,A1/2x)is always well deﬁned, which might not be true in
general, for instance when the comparator set studied is the probability simplex ∆. A workaround consists
in assigning the value +∞to the Bregman divergence whenever either of the arguments is outside of the
compound simplex = ∆⊗N. The choice of the interaction matrix Athen becomes critical to prevent the
bound from exploding, and calls for a new deﬁnition of the variance. Indeed, note that for i≤Nwe have
(A(b)1/2u)(i)=√
1 +bu(i)+ (1−√
1 +b)¯u. If allu(i)are equal (say to u0∈∆), then all (A(b)1/2u)(i)are
also equal to u0andA(b)1/2u∈. However, if they are diﬀerent, by deﬁnition of ¯u, for allj≤d, there
existsi≤Nsuch thatu(i)
j≤¯uj. Then, for blarge enough,√
1 +bu(i)
j+ (1−√
1 +b)¯ujbecomes negative,
and(A(b)1/2u)(i)is out of the simplex. Luckily, the maximum acceptable value for bcan be easily deduced
from the following variance deﬁnition.
Deﬁnition 2. Letu∈Rd. For allj≤d, let
umax
j= max
i≤Nu(i)
j,andumin
j= min
i≤Nu(i)
j.
Then, with the convention 0/0 = 0we deﬁne
Var∆(u) = max
j≤d/parenleftBigg
umax
j−umin
j
umax
j/parenrightBigg2
,
8Published in Transactions on Machine Learning Research (09/2022)
and for any σ≤1
σ=/braceleftbig
u∈: Var ∆(u)≤σ2/bracerightbig
.
Equipped with this new variance deﬁnition, we can now analyze regularizers deﬁned on the simplex.
Proposition 8. Letψ: ∆→Rbeλ-strongly convex w.r.t. norm /bardbl·/bardbl, and such that there exist x∗∈∆
andC<+∞such that for all x∈∆,Bψ(x,x∗)≤C. Letσ≤1, and assume that /bardbl∂/lscriptt(x)/bardbl⋆≤Lgfor all
t≤Tandx∈∆. Setb= (1−σ2)/σ2,x1= [x∗,...,x∗], andη=N/radicalbig
2λ(1 +b)C/Lg/radicalbig
(b+N)T. Then,
MT-OMD produces a sequence of iterates (xt)T
t=1such that for all u∈σ
RT(u)≤Lg/radicalbig
1 +σ2(N−1)/radicalbig
2CT/λ.
For the negative entropy we have x∗= 1/dandC= lnd. With subgradients satisfying /bardbl∂/lscriptt(x)/bardbl∞≤Lgwe
obtain
RT(u)≤Lg/radicalbig
1 +σ2(N−1)√
2Tlnd.
Proposition 8 shows that the multitask acceleration is not an artifact of the Euclidean geometry, but rather a
general feature of MT-OMD, as long as the variance deﬁnition is aligned with the geometry of the problem.
3.4 Adaptivity to the Task Variance
Most of the results we presented so far require the knowledge of the task variance σ2. We now present an
Hedge-based extension of MT-OMD, denoted Hedge-MT-OMD, that does not require any prior information
onσ2. First, note that for σ2≥1, MT-OMD becomes equivalent to independent OMDs. A simple approach
consists then in using Hedge—see, e.g., (Orabona, 2019, Section 6.8)—over a set of experts, each running an
instance of MT-OMD with a diﬀerent value of σ2chosen on a uniform grid of the interval [0,1].1We can
show that Hedge-MT-OGD only suﬀers an additional regret of order√TlogNagainst MT-OGD run with
the exact knowledge of Var/bardbl·/bardbl2(u).
Theorem 9. LetD=supx∈V/bardblx/bardbl2, and assume that /bardbl∂/lscriptt(x)/bardbl2≤Lgfor allt≤T,x∈V. Then, for all
u∈Vthe regret of Hedge-MT-OGD is bounded by
DLg/parenleftbigg
2 +/radicalbig
logN+/radicalBig
min/braceleftbig
Var/bardbl·/bardbl2(u),1/bracerightbig
·N/parenrightbigg√
2T.
Variance deﬁnition and choice of A.Note that we have (N−1)Var/bardbl·/bardbl2(u) =1
N/summationtext
i,j/bardblu(i)−u(j)/bardbl2
2=
u/latticetopLu, whereL=IN− 11/latticetop/Nis the Laplacian of the weighted clique graph over {1,...,N}, with edges of
1/N. A natural extension then consists in considering variances of the form
VarW
/bardbl·/bardbl2(u) =N/summationdisplay
i,j=1Wij/bardblu(i)−u(j)/bardbl2
2=u/latticetopLWu
for any adjacency matrix Wand its Laplacian LW. For instance, if we expect tasks to be concentrated in
clusters, it is natural to consider Wij= 1ifu(i)andu(j)(are thought to) belong to the same cluster, and 0
otherwise. This local version is interesting, as it allows to satisfy the variance condition with a smaller σ,
which improves the MT-OMD regret bound. Note that the proof of Theorem 1 can be readily adapted to this
deﬁnition by considering the class of interaction matrices {A(b) =IN+bLW}. The bound however features
maxi≤N[A(b)−1]ii, which depends on Win a nontrivial way and requires a case by case analysis, preventing
from stating a general result for an arbitrary W. Considering even more general matrices A, i.e., that do not
write asIN+bL, suﬀers from the same problem (one then also needs to compute B (A1/2u,A1/2v)on a
case by case basis), and does not enjoy anymore the variance interpretation seen above. Furthermore, note
that Proposition 2 is obtained by minimizing Equation (11) with respect to A. For matrices of the form
A(b), this tradeoﬀ only depends on b, and is thus much easier to solve than for general matrices. Finally, we
stress that local variances can be similarly considered on the simplex. Instead of involving the global umax
j,
the variance formula then features for each task/node a local maximum (respectively minimum) over its
neighbours.
1Note that Hedge-MT-OGD computes the loss subgradient at arbitrary points (corresponding to the expert’s predictions).
9Published in Transactions on Machine Learning Research (09/2022)
3.5 Additional Remarks
We conclude this section with two additional results. The ﬁrst one draws an interesting connection between
our multitask framework and the notion of dynamic regret, while the second emphasizes on the importance
of the asynchronous nature of the activations.
Remark 3 (Connection to dynamic regret) .Note that our online multitask setting can be viewed as a
special case of dynamic regret, where each comparator utin the sequence of comparators u1,u2,...belongs
to an unknown set {u/prime
1,...,u/prime
N}of known cardinality N. Moreover, at the beginning of each time step t,
the learner is told the index it∈{1,...,N}of the comparator utagainst which the regret is measured at
timet. As a consequence, any algorithm for dynamic regret minimization can be used in our setting. In
the Euclidean case, the optimal dynamic regret bound is of order√D2+DVT√
T, whereDis the Euclidean
diameter of the decision space and VT=/summationtextT
t=1/bardblut+1−ut/bardbl2. In order to facilitate the comparison to our bound/radicalbig
D2+D2(N−1)σ2√
T, let assume that the sequence of adversarial activations is such that it/negationslash=it−1, and
that all pairs of distinct elements in {u/prime
1,...,u/prime
N}appear with the same frequency as consecutive comparators
ut,ut+1. LetPT=/summationtextT
t=1||ut+1−ut||2
2. We have
PT=T/summationdisplay
t=1||ut+1−ut||2
2=T
N(N−1)/summationdisplay
i/negationslash=j||u/prime
i−u/prime
j||2
2= 2T1
N(N−1)/summationdisplay
i/negationslash=j||u/prime
i−u/prime
j||2
2
2= 2Tσ2D2,
such that our upper bound is at most
/radicalbig
D2+D2(N−1)σ2√
T=/radicalbigg
D2+ (N−1)PT
2T√
T≤/radicalbigg
D2+N−1
2TDVT√
T,
which is better as soon as T≥N−1
2.
We ﬁnally derive a regret bound in the case where several agents are active at each time step.
Proposition 10. Consider the setting of Proposition 2, but assume now that at each time step ta subset of
agentsAt⊂{1,...,N}, of cardinality|At|=p, is chosen by the adversary and asked to make predictions.
Recall that in this case the regret of an independent approach is of order DLg√pNT. Then, MT-OMD run
withb=√pNandη=ND/radicalbig
1 +√pσ2(N−1)/Lg/radicalbig
p(1 +p)Tproduces a sequence of iterates (xt)T
t=1such
that for allu∈V/bardbl·/bardbl2,σ
RT(u)≤DLg/radicalbig
pT/radicalbig
1 +σ2(N−1)/radicalbig
p1/2+p3/2.
Note that for p= 1, we recover exactly Proposition 2.
Proposition 10 highlights that having asynchronous activations is critical: the more active agents at a
single time step, i.e., the bigger p, the bigger/radicalbig
1 +σ2(N−1)/radicalbig
p1/2+p3/2, i.e., the smaller the multitask
acceleration. In the extreme case where p=N, our multitask framework reduces to a standard online
convex optimization problem, with diameter√
NDand gradients with Euclidean norms bounded by√
NLg.
Standard lower bounds are then of order NDLg√
T, conﬁrming that no multitask acceleration is possible.
4 Algorithms
We now show that MT-OGD and MT-EG enjoy closed-form updates, making them easy to implement. Note
that the MT-OGD derivation is valid for any matrix Apositive deﬁnite, while MT-EG requires A−1/2to be
stochastic. This is veriﬁed by matrices of the form A=IN+LW(Lemma 13).
MT-OGD. LetV={u∈Rd:/bardblu/bardbl2≤D}, andσ≤1. Recall that V=V⊗N={u∈RNd:/bardblu/bardbl2,∞≤D},
andV/bardbl·/bardbl2,σ={u∈V: Var/bardbl·/bardbl2(u)≤σ2}. Solving the ﬁrst equation in Equation (8), we obtain that the
iteratext+1produced by MT-OGD is the solution to
min
x∈RNd/braceleftBig/vextenddouble/vextenddoublext−ηtA−1gt−x/vextenddouble/vextenddouble2
A:/bardblx/bardbl2,∞≤D/bracerightBig
.
10Published in Transactions on Machine Learning Research (09/2022)
However, computing this update is made diﬃcult by the discrepancy between the norms used in the objective
and the constraint. A simple work around consists in considering the minimization over the Mahalanobis ball
VA={u∈RNd:/bardblu/bardbl2
A≤(1 +bσ2)ND2}instead. It is easy to check that V/bardbl·/bardbl2,σ⊂VA, so that every result
derived in Section 3 for MT-OGD remains valid (only the fact that comparators and iterates are in VAis
actually used). With the substitution yt=A1/2xtthe MT-OGD update then rewrites (see Appendix B.1 for
technical details)
yt+1= Proj/parenleftBig
yt−ηtA−1/2gt,/radicalbig
(1 +bσ2)ND/parenrightBig
, (16)
where Proj(x,τ) =min/braceleftbig
1,τ
/bardblx/bardbl2/bracerightbig
x. Note that Equation (16) can be easily turned back into an update on xt
by making the inverse substitution. In practice however, xtis only computed to make the predictions. The
pseudo-code of the algorithm is given in Algorithm 1.
Algorithm 1 MT-OGD
input:Apositivedeﬁnitematrix A∈RN×N(interactionmatrix), σ2>0(taskvariance), D> 0(comparators
set diameter), ηt>0(learning-rate schedule)
initialization: A=A⊗Id∈RNd×Nd,x1=0,y1=A−1/2x1
1:ComputeA1/2andA−1/2
2:fortime stepst= 1,2,...do
3:Compute compound prediction xt=A1/2yt
4:Activate agent itand predict x(it)
t
5:Getgt∈∂/lscript(xt)and compute gt
6:Computeyt+1= min/braceleftbigg
1,√
(1+bσ2)ND
/bardblyt−ηtA−1/2gt/bardbl2/bracerightbigg
(yt−ηtA−1/2gt)
7:end for
MT-EG. Using Equation (8) with yt=A1/2xt, MT-EG reads
˜yt+1= arg min
y∈RNd/angbracketleftηA−1/2gt,y/angbracketright+B (y,yt),
yt+1= arg min
y∈A1/2()B (y,˜yt+1), (17)
where is the compound negative entropy regularizer such that  (x) =/summationtextN
i=1/summationtextd
j=1x(i)
jln/parenleftbig
x(i)
j/parenrightbig
. One can
show (see Appendix B.2 for details) that the update can be rewritten for all i≤Nandj≤d
˜y(i)
t+1,j=y(i)
t,jexp/parenleftBig
−ηA−1/2
iitgt,j−1/parenrightBig
,
y(i)
t+1,j=˜y(i)
t+1,j/summationtextd
k=1˜y(i)
t+1,k.
Combining both equations, we ﬁnally obtain
y(i)
t+1,j=y(i)
t,je−ηA−1/2
iitgt,j
/summationtextd
k=1y(i)
t,ke−ηA−1/2
iitgt,k. (18)
Update Equation (18) enjoys a natural interpretation. Each block y(i)is operating an individual standard EG
update, but with gradient A−1/2
iitgt. WhenA=IN, only the active block is updated. Otherwise, the update
of blockiis proportional to A−1/2
iit, that quantiﬁes the similarity between tasks iandit. The pseudo-code
of the algorithm is given in Algorithm 2. Although this work only focuses on OMD for clarity, note that
considering Follow-the-Regularized-Leader—see, e.g., (Orabona, 2019, Section 7)—with /tildewide = (A1/2·)would
yield similar bounds. This would allow, for instance, the use of time-varying learning rates with entropic
regularization.
11Published in Transactions on Machine Learning Research (09/2022)
Algorithm 2 MT-EG
input:A positive deﬁnite matrix A∈RN×N(interaction matrix), η>0(learning-rate)
initialization: A=A⊗Id∈RNd×Nd,x1= 1/d∈RNd,y1=A−1/2x1
1:ComputeA1/2andA−1/2
2:fortime stepst= 1,2,...do
3:Compute compound prediction xt=A1/2yt
4:Activate agent itand predict x(it)
t
5:Getgt∈∂/lscript(xt)and compute gt
6:Computey(i)
t+1,j=y(i)
t,je−ηA−1/2
iitgt,j
/summationtextd
k=1y(i)
t,ke−ηA−1/2
iitgt,k,∀i∈[N],∀j∈[d]
7:end for
5 Experiments
In this section, we empirically compare the performance of Hedge-MT-OGD/EG against two natural
alternatives: an independent-task approach (IT-OGD/EG) where the agents do not communicate, and a
single-task approach (ST-OGD/EG) where a single model is learned and shared by all agents. Note that
both IT and ST approaches are special cases of MT-OMD, obtained respectively with the choices b= 0(i.e.,
σ2≥1), orb= +∞(i.e.,σ2= 0). In Appendix D we report an additional experiment where we empirically
validate the dependence of the performance of MT-OGD on the task variance.
Online Gradient Descent. For this experiment, we use the Lenkdataset (Lenk et al., 1996; Argyriou
et al., 2007). It consists of 2880computer ratings in the range {1,2,..., 10}, made by 180individuals (the
tasks) on the basis of 14binary features. Each computer is rated on a discrete scale from 0to10, expressing
the likelihood of an individual buying that computer. We run Hedge-MT-OGD using the clique interaction
matrixA= (1 +N)IN− 11/latticetopand the square loss. For all algorithms, the value of ηis set according to
the optimal theoretical value, see Proposition 2. In Hedge-MT-OGD, the variance σ2is learned in a set of
5experts uniformly spaced over [0,1]. For simplicity, we use D= 1and compute the resulting Lipschitz
constant accordingly. Results are reported in Figure 1(a).
Exponentiated Gradient. For our second experiment, we consider EMNIST , a classiﬁcation dataset
consisting of 62classes (images of digits, small and capital letters). To speed up computation, we reduced the
number of features from 784down to 10through a standard dimensionality reduction method. We created
61binary classiﬁcation tasks by considering the 0digit class against each other class. To each task, we
assigned 10examples ( 5positive, 5negative) randomly chosen from the set of examples for that task. We
considered the linear logistic regression and ran Hedge-MT-EG with the parameterized clique interaction
matrixA(b) = (1 +b)IN−b 11/latticetop/N. The value of bis set according to the theoretical value (that depends on
σ2, see Proposition 8), while σ2is learned in a set of 5experts uniformly spaced over [0,1]. For all algorithms,
the value of ηis set according to the optimal theoretical values. Results are reported in Figure 1(b).
6 Conclusion
We introduced and analyzed MT-OMD, a multitask extension of OMD whose regret is shown to improve as
the task variance, expressed in terms of the geometry induced by the regularizer, decreases. We provided a
unifying analysis and a single algorithm that explains when is multitask acceleration possible based on the
current geometry, and how to achieve it. Natural and interesting directions for future research include: (1)
analyzing the multitask acceleration in combination with other properties, such as strongly convexlosses,
and (2) designing and analyzing an extension of MT-OMD that is adaptive to the best interaction matrix.
References
JacobAbernethy, PeterBartlett, andAlexanderRakhlin. Multitasklearningwithexpertadvice. In Proceedinds
of the 20th International Conference on Computational Learning Theory , pp. 484–498, 2007.
12Published in Transactions on Machine Learning Research (09/2022)
(a) Lenk (OGD, cumulative square loss)
 (b) EMNIST (EG, cumulative logistic loss)
Figure 1: Comparison between multitask (MT), independent-task (IT), and single-task (ST) OGD and EG
on theLenkandEMNIST datasets. We plot the cumulative losses against time. Lenkis known to work well
in multi-task settings, and indeed Hedge-MT-OGD performs signiﬁcantly better than both baselines. On
the other hand, EMNIST has a variance signiﬁcantly higher than Lenk. However, even in this unfavorable
scenario, Hedge-MT-EG is still outperforming the baselines, though by a small margin.
Gotz Alefeld and Norbert Schneider. On square roots of m-matrices. Linear Algebra and its Applications , 42:
119–132, 1982.
Pierre Alquier, The Tien Mai, and Massimiliano Pontil. Regret bounds for lifelong learning. In Proceedinds
of the 20th International Conference Artiﬁcial Intelligence and Statistics , pp. 261–269, 2017.
Andreas Argyriou, Charles A Micchelli, Massimiliano Pontil, and Yiming Ying. A spectral regularization
framework for multi-task structure learning. In Proceedings of the 20th Annual Conference on Neural
Information Processing Systems , pp. 25–32, 2007.
Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based
meta-learning. In Proceedinds of the 36th International Conference on Machine Learning , pp. 424–433,
2019.
Heinz H. Bauschke and Patrick L. Combettes. Convex analysis and monotone operator theory in Hilbert
spaces. Springer, 2011.
Etienne Boursier, Mikhail Konobeev, and Nicolas Flammarion. Trace norm regularization for multi-task
learning with scarce data. arXiv preprint arXiv:2202.06742 , 2022.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex Optimization . Cambridge University
Press, 2004.
Rich Caruana. Multitask learning. Machine Learning , 28(1):41–75, 1997.
Giovanni Cavallanti, Nicolò Cesa-Bianchi, and Claudio Gentile. Linear algorithms for online multitask
classiﬁcation. Journal of Machine Learning Research , 11:2901–2934, 2010.
Nicolò Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella. A gang of bandits. In Proceedings of the 26th
Annual Conference on Neural Information Processing Systems , pp. 737–745, 2013.
Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games . Cambridge University Press, 2006.
Ofer Dekel, Philip M Long, and Yoram Singer. Online learning of multiple tasks with a shared loss. Journal
of Machine Learning Research , 8(10):2233–2264, 2007.
13Published in Transactions on Machine Learning Research (09/2022)
Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil. Online-within-online meta-learning.
InProceedings of the 32th Annual Conference on Advances in Neural Information Processing Systems 32 ,
pp. 13089–13099, 2019.
Canh T Dinh, Tung T Vu, Nguyen H Tran, Minh N Dao, and Hongyu Zhang. Fedu: A uniﬁed framework for
federated multi-task learning with Laplacian regularization. arXiv preprint arXiv:2102.07148 , 2021.
Theodoros Evgeniou and Massimiliano Pontil. Regularized multi–task learning. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 109–117, 2004.
Theodoros Evgeniou, Charles A Micchelli, and Massimiliano Pontil. Learning multiple tasks with kernel
methods. Journal of Machine Learning Research , 6(4):615–637, 2005.
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In Proceedings
of the 36th International Conference on Machine Learning , pp. 1920–1930, 2019.
Claudio Gentile. The robustness of the p-norm algorithms. Machine Learning , 53(3):265–299, 2003.
Adam J Grove, Nick Littlestone, and Dale Schuurmans. General convergence results for linear discriminant
updates. In Proceedings 10th Annual Conference on Computational Learning Theory , pp. 171–183, 1997.
Quanquan Gu, Zhenhui Li, and Jiawei Han. Learning a kernel for multi-task clustering. In Proceedings of the
25th AAAI Conference on Artiﬁcial Intelligence , pp. 368–373, 2011.
Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization , 2(3-4):
157–325, 2016.
Mark Herbster and Guy Lever. Predicting the labelling of a graph via minimum p-seminorm interpolation.
InProceedings of the 22nd Conference on Learning Theory , 2009.
Sham M Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Regularization techniques for learning with
matrices. Journal of Machine Learning Research , 13(1):1865–1890, 2012.
Pierre Laforgue, Andrea Della Vecchia, Nicolò Cesa-Bianchi, and Lorenzo Rosasco. Adatask: Adaptive
multitask online learning. arXiv preprint arXiv:2205.15802 , 2022.
Peter J Lenk, Wayne S DeSarbo, Paul E Green, and Martin R Young. Hierarchical bayes conjoint analysis:
Recovery of partworth heterogeneity from reduced experimental designs. Marketing Science , 15(2):173–191,
1996.
Guangxia Li, Steven CH Hoi, Kuiyu Chang, Wenting Liu, and Ramesh Jain. Collaborative online multitask
learning. IEEE Transactions on Knowledge and Data Engineering , 26(8):1866–1876, 2014.
Rui Li, Fenglong Ma, Wenjun Jiang, and Jing Gao. Online federated multitask learning. In Proceedings of
the 7th IEEE International Conference on Big Data , pp. 215–220, 2019.
Keerthiram Murugesan, Hanxiao Liu, Jaime Carbonell, and Yiming Yang. Adaptive smoothed online multi-
task learning. In Proceedings of the 29th Annual Conference on Advances in Neural Information Processing
Systems, pp. 4296–4304, 2016.
Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213 , 2019.
Anastasia Pentina and Christoph H Lampert. Multi-task learning with labeled and unlabeled tasks. In
Proceedings of the 34th International Conference on Machine Learning , pp. 2807–2816, 2017.
Gianluigi Pillonetto, Francesco Dinuzzo, and Giuseppe De Nicolao. Bayesian online multitask learning of
gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence , 32(2):193–205, 2008.
Avishek Saha, Piyush Rai, Hal Daumé, Suresh Venkatasubramanian, et al. Online learning of multiple tasks
and their relationships. In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and
Statistics , pp. 643–651, 2011.
14Published in Transactions on Machine Learning Research (09/2022)
Shai Shalev-Shwartz. Online learning: Theory, algorithms, and applications. PhD thesis, The Hebrew
University of Jerusalem, 2007. , 2007.
Daniel Sheldon. Graphical multi-task learning. Technical report, Computing and Information Science
Technical Reports, Cornell University, 2008.
Changjian Shui, Mahdieh Abbasi, Louis-Émile Robitaille, Boyu Wang, and Christian Gagné. A principled
approach for learning task similarity in multitask learning. In Proceedings of the 28th International Joint
Conference on Artiﬁcial Intelligence , pp. 3446–3452, 2019.
Suvrit Sra. Fast projections onto mixed-norm balls with applications. Data Mining and Knowledge Discovery ,
25(2):358–377, 2012.
Jonathan Tuck, Shane Barratt, and Stephen Boyd. A distributed method for ﬁtting Laplacian regularized
stratiﬁed models. Journal of Machine Learning Research , 22(60):1–37, 2021.
Yang Yang, Zhigang Ma, Yi Yang, Feiping Nie, and Heng Tao Shen. Multitask spectral clustering by exploring
intertask correlation. IEEE Transactions on Cybernetics , 45(5):1083–1094, 2014.
Chi Zhang, Peilin Zhao, Shuji Hao, Yeng Chai Soh, Bu Sung Lee, Chunyan Miao, and Steven CH Hoi.
Distributed multi-task classiﬁcation: a decentralized online learning approach. Machine Learning , 107(4):
727–747, 2018.
Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and Data
Engineering , 2021.
Yu Zhang and Dit Yan Yeung. A convex formulation for learning task relationships in multi-task learning. In
Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence , pp. 733, 2010.
15Published in Transactions on Machine Learning Research (09/2022)
A Technical Proofs
In this Appendix are gathered all the technical proofs of the results stated in the article. First, we provide a
notation table recalling the most important notation used along the paper.
Notation Meaning
T∈N Time horizon
N∈N Number of tasks
d∈N Dimension of single reference vectors
ui∈Rd, fori≤N Reference vector (best model) for task i
u= [u1,...,uN]∈RNdCompound reference vector
u(i)∈RdBlockiof the compound vector u(=uihere)
¯u= (1/N)/summationtext
iu(i)Average reference vector
Var/bardbl·/bardbl(u) = (1/(N−1))/summationtext
i/bardblu(i)−¯u/bardbl2Variance of the reference vectors w.r.t. norm /bardbl·/bardbl
V⊂RdGeneric set of comparators for a single task
D= supu∈V/bardblu/bardbl Half diameter of Vw.r.t. norm/bardbl·/bardbl
V={u∈RNd:∀i≤N,u(i)∈V} Generic set of compound comparators
V/bardbl·/bardbl,σ={u∈V: Var/bardbl·/bardbl(u)≤σ2D2}Comparators in Vwith small variance
umin
j= mini≤Nu(i)
j Minimum (among tasks) of component j≤d
umax
j= maxi≤Nu(i)
j Maximum (among tasks) of component j≤d
Var∆(u) = maxj≤d(1−umin
j/umax
j)2Variance aligned with the probability simplex ∆
∆ ={u∈Rd
+:/summationtext
juj= 1} Probability simplex in Rd
={u∈RNd:∀i≤N,u(i)∈∆} Compound probability simplex
σ={u∈: Var ∆(u)≤σ2} Comparators in with small variance
1∈RNVector ﬁlled with 1’s
IN∈RN×NIdentity matrix of dimension N
A∈RN×NGeneric interaction matrix
A(b) = (1 +b)IN−(b/N) 11/latticetopParameterized interaction matrix
A=A⊗Id∈RNd×NdBlock version (Kronecker product with Id) ofA
A(b) =A(b)⊗Id Block version of A(b)
xi,t∈Rd, fori≤Nandt≤T Prediction maintained by agent iat timet
xt= [x1,t,...,xN,t]∈RNdCompound predictions maintained at time t
it∈N Active agent at time t(chosen by the adversary)
/lscriptt:Rd→R Loss function at time t(chosen by the adversary)
gt=∂/lscriptt(x(it)
t)∈RdSubgradient of /lscripttat pointx(it)
t
gt= [0,..., 0,gt,0,..., 0]∈RNdCompound subgradient (null outside block it)
ψ:V→R Generic base regularizer
Bψ:V×V→R Bregman divergence induced by ψ
 :u∈RNd/mapsto→/summationtext
iψ(u(i)) Compound regularizer
/tildewide = (A1/2·) Regularizer of interest
Table 1: Notation Table.
A.1 Proof of Theorem 1
The ﬁrst important building block of our proof consists in characterizing the strong convexity of /tildewide (Lemma 11).
To that end, we need to introduce the following deﬁnition and notation about norms.
16Published in Transactions on Machine Learning Research (09/2022)
Deﬁnition 3. LetN:Rd→R, andN/prime:RN→Rbe two norms. We deﬁne the mixed norm /bardbl·/bardblN,N/prime:RNd→
Ras follows2. For allx∈RNd,/bardblx/bardblN,N/primeis given by theN/primenorm of the RNvector composed of the N
norms of the blocks (x(i))N
i=1. Formally, it reads
∀x∈RNd,/bardblx/bardblN,N/prime=N/prime/parenleftBig
N/parenleftbig
x(1)/parenrightbig
,...,N/parenleftbig
x(N)/parenrightbig/parenrightBig
.
WhenN=/bardbl·/bardblpandN/prime=/bardbl·/bardblq, forp,q∈[1,+∞], one recovers the standard /lscriptp,qmixed norm. We use the
following shortcut notation
/bardbl·/bardblp,N/prime:=/bardbl·/bardbl/bardbl·/bardblp,N/primeand/bardbl·/bardblN,q:=/bardbl·/bardblN,/bardbl·/bardblq.
Lets∈N,/bardbl·/bardbl:Rs→Rbe any norm, and M∈Rs×sbe a symmetric positive deﬁnite matrix. We deﬁne the
Mahalanobis version of /bardbl·/bardbl, denoted/bardbl·/bardblM, as
∀x∈Rs,/bardblx/bardblM=/vextenddouble/vextenddoubleM1/2x/vextenddouble/vextenddouble.
Notice that for/bardbl·/bardbl=/bardbl·/bardbl 2, we recover the standard Mahalanobis norm such that /bardblx/bardbl2,M=√
x/latticetopMx. For
simplicity, and when it is clear from the context, we use the shortcut notation /bardbl·/bardblM=/bardbl·/bardbl 2,M.
We are now equipped to establish the strong convexity of /tildewide .
Lemma 11. Assume that the base regularizer ψ:Rd→Risλ-strongly convex with respect to some norm
/bardbl·/bardbl. Then, the associated compound regularizer /tildewide isλ-strongly convex with respect to the Mahalanobis mixed
norm/bardbl·/bardbl/bardbl·/bardbl,2,A.
Proof.First, notice that for every x∈RNdit holds
H/tildewide (x) =A1/2H /parenleftbig
A1/2x/parenrightbig
A1/2,
whereH (x)∈RNd×Nddenotes the Hessian matrix of regularizer  at pointx. Moreover, due to the
deﬁnition of the compound regularizer  :x∈RNd/mapsto→/summationtextN
i=1ψ/parenleftbig
x(i)/parenrightbig
, matrixH (x)is block diagonal and
given by
H (x) =
Hψ(x(1)) 0 ... 0
0Hψ(x(2))... 0
............
0... 0Hψ(x(N))
.
Assuming that ψisλ-strongly convex with respect to norm /bardbl·/bardbl, it thus holds for all x,v∈RNd
/angbracketleftBig
H/tildewide (x)v,v/angbracketrightBig
=/angbracketleftBig
H /parenleftbig
A1/2x/parenrightbig/parenleftbig
A1/2v/parenrightbig
,A1/2v/angbracketrightBig
=N/summationdisplay
i=1/angbracketleftBig
Hψ/parenleftBig/parenleftbig
A1/2x/parenrightbig(i)/parenrightBig/parenleftbig
A1/2v/parenrightbig(i),/parenleftbig
A1/2v/parenrightbig(i)/angbracketrightBig
≥λN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/parenleftbig
A1/2v/parenrightbig(i)/vextenddouble/vextenddouble/vextenddouble2
=λ/bardblv/bardbl2
/bardbl·/bardbl,2,A.
The second important intermediate result (Lemma 12) deals with dual norms, that we recall now.
2Note that a suﬃcient condition for /bardbl·/bardblN,N/primeto be a norm is that N/prime(z1, . . . , z N) =N/prime(|z1|, . . . ,|zN|). In Lemma 11 we use
N/prime=/bardbl·/bardbl 2, which satisﬁes this property.
17Published in Transactions on Machine Learning Research (09/2022)
Deﬁnition 4. Lets∈N, and/bardbl·/bardbl:Rs→Rbe any norm. The dual norm of/bardbl·/bardbl, denoted/bardbl·/bardbl⋆, is deﬁned as
∀x∈Rs,/bardblx/bardbl⋆= sup
y∈Rs:/bardbly/bardbl≤1/angbracketleftx,y/angbracketright.
Lemma 12. Letp,q∈[1,+∞], andp⋆,q⋆their conjugates such that 1/p+ 1/p⋆= 1/q+ 1/q⋆= 1. Then it
holds
(/bardbl·/bardblp)⋆=/bardbl·/bardblp⋆,and (/bardbl·/bardblp,q)⋆=/bardbl·/bardblp⋆,q⋆.
Lets∈N,/bardbl·/bardbl:Rs→Rbe any norm, and M∈Rs×sbe a symmetric positive deﬁnite matrix. Then it holds
(/bardbl·/bardblM)⋆=/bardbl·/bardbl⋆,M−1.
Assume furthermore that scan be decomposed into s=s1×s2, fors1,s2∈N. Then, for any norm
/bardbl·/bardbl:Rs1→Rit holds
/parenleftbig
/bardbl·/bardbl/bardbl·/bardbl,2/parenrightbig
⋆=/bardbl·/bardbl/bardbl·/bardbl⋆,2.
In particular, combining the last two results yields:/parenleftbig
/bardbl·/bardbl/bardbl·/bardbl,2,A/parenrightbig
⋆=/bardbl·/bardbl/bardbl·/bardbl⋆,2,A−1.
Proof.The ﬁrst result is standard in convex analysis, see e.g.Appendix A.1.6 in Boyd et al. (2004). The
second result is due to Sra (2012), see Lemma 3 therein. The last two results rely on the following equality
(seee.g.Example 3.27 in Boyd et al. (2004)). For any norm /bardbl·/bardbl, it holds
1
2/bardbl·/bardbl2
⋆=/parenleftbigg1
2/bardbl·/bardbl2/parenrightbigg⋆
, (19)
wheref⋆denotes the Fenchel-Legendre conjugate of f, deﬁned as f⋆(x) =supy/angbracketleftx,y/angbracketright−f(y)Bauschke &
Combettes (2011). Applying Equation (19) to /bardbl·/bardblM, we get for all x∈Rs
/parenleftbigg1
2(/bardbl·/bardblM)2
⋆/parenrightbigg
(x) =/parenleftbigg1
2/bardbl·/bardbl2
M/parenrightbigg⋆
(x)
= sup
y∈Rs/braceleftbigg
/angbracketleftx,y/angbracketright−1
2/bardbly/bardbl2
M/bracerightbigg
= sup
y∈Rs/braceleftbigg
/angbracketleftM−1/2x,M1/2y/angbracketright−1
2/vextenddouble/vextenddoubleM1/2y/vextenddouble/vextenddouble2/bracerightbigg
= sup
z∈Rs/braceleftbigg
/angbracketleftM−1/2x,z/angbracketright−1
2/bardblz/bardbl2/bracerightbigg
=/parenleftbigg1
2/bardbl·/bardbl2/parenrightbigg⋆/parenleftbig
M−1/2x/parenrightbig
=1
2/vextenddouble/vextenddoubleM−1/2x/vextenddouble/vextenddouble2
⋆=/parenleftbigg1
2/bardbl·/bardbl2
⋆,M−1/parenrightbigg
(x).
18Published in Transactions on Machine Learning Research (09/2022)
Applying Equation (19) to /bardbl·/bardbl/bardbl·/bardbl,2, we get for any x∈Rs1·s2
/parenleftbigg1
2/parenleftbig
/bardbl·/bardbl/bardbl·/bardbl,2/parenrightbig2
⋆/parenrightbigg
(x) =/parenleftbigg1
2/bardbl·/bardbl2
/bardbl·/bardbl,2/parenrightbigg⋆
(x)
= sup
y∈Rs1·s2/braceleftbigg
/angbracketleftx,y/angbracketright−1
2/bardbly/bardbl2
/bardbl·/bardbl,2/bracerightbigg
= sup
y∈Rs1·s2/braceleftBiggs2/summationdisplay
i=1/angbracketleftx(i),y(i)/angbracketright−1
2s2/summationdisplay
i=1/vextenddouble/vextenddoubley(i)/vextenddouble/vextenddouble2/bracerightBigg
=s2/summationdisplay
i=1sup
y(i)∈Rs1/braceleftbigg
/angbracketleftx(i),y(i)/angbracketright−1
2/vextenddouble/vextenddoubley(i)/vextenddouble/vextenddouble2/bracerightbigg
=s2/summationdisplay
i=1/parenleftbigg1
2/bardbl·/bardbl2/parenrightbigg⋆/parenleftbig
x(i)/parenrightbig
=1
2s2/summationdisplay
i=1/vextenddouble/vextenddoublex(i)/vextenddouble/vextenddouble2
⋆=/parenleftbigg1
2/bardbl·/bardbl2
/bardbl·/bardbl⋆,2/parenrightbigg
(x).
We are now ready to prove Theorem 1. The proof follows from standard arguments to analyze OMD, combined
with Lemmas 11 and 12.
Proof of Theorem 1. First, notice that the compound representation allows to write
RT=T/summationdisplay
t=1/lscriptt(xt)−N/summationdisplay
i=1inf
u∈V/summationdisplay
t:it=i/lscriptt(u) = inf
u∈VT/summationdisplay
t=1/lscriptt(xt)−/lscriptt/parenleftbig
u(it)/parenrightbig
.
Next, for any u∈V, the convexity and sub-diﬀerentiability of /lscripttimplies
T/summationdisplay
t=1/lscriptt(xt)−/lscriptt/parenleftbig
u(it)/parenrightbig
≤T/summationdisplay
t=1/angbracketleftBig
gt,xt−u(it)/angbracketrightBig
=T/summationdisplay
t=1/angbracketleftgt,xt−u/angbracketright.
Now, observe that update Equation (9) actually deﬁnes an OMD on iterate xt, for the sequence of gradients
(gt)T
t=1, and with regularizer /tildewide . Recall also that /tildewide isλ-strongly convex with respect to /bardbl·/bardbl/bardbl·/bardbl,2,A(Lemma 11),
whose dual norm is /bardbl·/bardbl/bardbl·/bardbl⋆,2,A−1(Lemma 12). Applying the standard OMD bound of Equation (3), we thus
obtain that for all u∈Vandη>0it holds
T/summationdisplay
t=1/angbracketleftgt,xt−u/angbracketright≤B/tildewide (u,x1)
η+η
2λT/summationdisplay
t=1/bardblgt/bardbl2
/bardbl·/bardbl⋆,2,A−1.
Then, we use that for all x,y∈RNdit holdsB/tildewide (x,y) =B /parenleftbig
A1/2x,A1/2y/parenrightbig
, and that
/bardblgt/bardbl2
/bardbl·/bardbl⋆,2,A−1=/vextenddouble/vextenddouble/vextenddoubleA−1/2gt/vextenddouble/vextenddouble/vextenddouble2
/bardbl·/bardbl⋆,2
=N/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddoubleA−1/2
iitgt/vextenddouble/vextenddouble/vextenddouble2
⋆
=N/summationdisplay
i=1/parenleftBig
A−1/2
iit/parenrightBig2
/bardblgt/bardbl2
⋆
=A−1
itit/bardblgt/bardbl2
⋆
≤max
i≤NA−1
ii/bardblgt/bardbl2
⋆.
19Published in Transactions on Machine Learning Research (09/2022)
Combining all arguments, we ﬁnally obtain
RT(u)≤B /parenleftbig
A1/2u,A1/2x1/parenrightbig
η+ max
i≤NA−1
iiη
2λT/summationdisplay
t=1/bardblgt/bardbl2
⋆.
The second claim of the theorem is obtained in a similar fashion. Standard OMD results (Orabona, 2019,
Theorem 6.8) give that
RT(u)≤max
t≤TB/tildewide (u,xt)
ηT+1
2λT/summationdisplay
t=1ηt/bardblgt/bardbl2
/bardbl·/bardbl⋆,2,A−1.
ReplacingB/tildewide (u,xt)and/bardblgt/bardbl2
/bardbl·/bardbl⋆,2,A−1withB /parenleftbig
A1/2u,A1/2xt/parenrightbig
andmax
i≤NA−1
ii/bardblgt/bardbl2
⋆respectively yields the
desired result.
A.2 Proof of Proposition 2
First, it is easy to check that we have
A(b) = (1 +b)IN−b11/latticetop
N
A(b)1/2=√
1 +b IN+ (1−√
1 +b)11/latticetop
N
A(b)−1=1
1 +bIN+b
(1 +b)11/latticetop
N.
This implies
max
i≤N[A(b)−1]ii=b+N
(1 +b)N, (20)
and
2B /parenleftBig
A(b)1/2u,A(b)1/20/parenrightBig
=N/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftBig
A(b)1/2u/parenrightBig(i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
=N/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble√
1 +bu(i)+ (1−√
1 +b)¯u/vextenddouble/vextenddouble/vextenddouble2
2
=N/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble√
1 +b(u(i)−¯u) +¯u/vextenddouble/vextenddouble/vextenddouble2
2
= (1 +b)N/summationdisplay
i=1/vextenddouble/vextenddoubleu(i)−¯u/vextenddouble/vextenddouble2
2+N/summationdisplay
i=1/bardbl¯u/bardbl2
2+ 2√
1 +bN/summationdisplay
i=1/angbracketleftbig
u(i)−¯u,¯u/angbracketrightbig
=N/summationdisplay
i=1/parenleftBig/vextenddouble/vextenddoubleu(i)−¯u/vextenddouble/vextenddouble2
2+/bardbl¯u/bardbl2
2/parenrightBig
+b(N−1)Var/bardbl·/bardbl2(u)
=N/summationdisplay
i=1/vextenddouble/vextenddoubleu(i)/vextenddouble/vextenddouble2
2+b(N−1)Var/bardbl·/bardbl2(u). (21)
Substituting Equation (20), Equation (21) into Equation (11), and using the deﬁnition of V/bardbl·/bardbl2,σ, we get that
RT(u)is smaller than
/bardblu/bardbl2
2+b(N−1)Var/bardbl·/bardbl2(u)
2η+ηTL2
g
2b+N
(1 +b)N≤ND2(1 +bN−1
Nσ2)
2η+ηTL2
g
2b+N
(1 +b)N.
20Published in Transactions on Machine Learning Research (09/2022)
Settingη=ND
Lg/radicalbigg/parenleftbig
1+bN−1
Nσ2/parenrightbig
(1+b)
(b+N)T, we get
RT≤DLg/radicalBigg/parenleftbig
1 +bN−1
Nσ2/parenrightbig
(b+N)
1 +bT.
We now optimize on b. Letα=N−1
Nσ2, the optimality condition writes
(1 +αN+ 2αb∗)(1 +b∗) = (1 +αb∗)(b∗+N)
1 + (α−1)N+ 2αb∗+αb∗2= 0
b∗=/radicalbigg
1−α
α(N−1)−1 =/radicalbigg
1 +1−σ2
σ2N−1.
And we have
(1 +αb∗)(b∗+N)
1 +b∗= 1 +αN+ 2αb∗
= 1 +σ2(N−1) + 2σ2N−1
N/parenleftBigg/radicalbigg
1 +1−σ2
σ2N−1/parenrightBigg
.
Hence, the bound becomes DLg/radicalbig
F(σ)T, with
F(σ) = 1 +σ2(N−1) + 2σ2N−1
N/parenleftBigg/radicalbigg
1 +1−σ2
σ2N−1/parenrightBigg
.
Note that
2αb∗≤2/radicalbig
α(1−α)(N−1)≤α(1−α)(N−1) + 1≤1 +αN,
so that we have F(σ)≤2(1 +σ2(N−1)), and the bound becomes DLg/radicalbig
1 +σ2(N−1)√
2T, a value which
can also be achieved directly by setting b=Nabove.
A.3 Proof of Proposition 3
Thislowerboundisprovedbyadaptingthestandardlowerboundfor(single-agent)OnlineLinearOptimization,
see, e.g., (Orabona, 2019, Chapter 5). We consider linear losses, i.e., we assume that for all t≤Tthere
existsgt∈Rdsuch that/lscriptt(x) =/angbracketleftgt,x/angbracketright. Our goal is to show that for any any sequence x1,...,xTwe can
construct a sequence of losses (or equivalently of gt) such that the regret is lower bounded. Recall that dis
the dimension of the set VandD > 0its radius, and let σ <1. Assume that Nis even, smaller than 2d,
and for alli∈{1,...,N/ 2}, let
w(2i−1)=−/radicalbig
(N−1)/NDσeiandw(2i)=/radicalbig
(N−1)/NDσei,
where (ei)i≤dis the canonical basis of Rd. It is easy to check that /bardblw(i)/bardbl2≤D, and that Var(w) =σ2D2,
such thatw∈V/bardbl·/bardbl2,σ. Assume that NdividesT, and that the agents are activated in a cyclic fashion, i.e.,
it= 1 +tmodN. We introduce the family of gradient vectors
gt=/epsilon1⌈t/N⌉Lgw(it)
/bardblw(it)/bardbl2, (22)
where/epsilon11,...,/epsilon1T/Nare valued in{−1,1}, with exact values to be determined later on. Indeed, we show that
for any sequence of predictions, there exists a choice of /epsilon11,...,/epsilon1T/Nsuch that the regret is lower bounded.
To that end, we use the fact that for any function F:{−1,1}T/N→R, and any probability distribution P
with support in{−1,1}, we have
sup
∈{−1,1}T/NF()≥E∼P⊗T/N[F()].
21Published in Transactions on Machine Learning Research (09/2022)
In particular, we choose Pto be the Rademacher distribution, such that P{/epsilon1=−1}=P{/epsilon1= 1}= 1/2,
and all expectations are now understood to be taken with respect to this distribution. Note that we have
/bardblgt/bardbl2≤LgandE[gt] = 0, for allt. Given any sequence x1,...,xT, we can use gradients Equation (22) and
obtain
sup
/epsilon11,...,/epsilon1T/NRT≥E/bracketleftBiggT/summationdisplay
t=1/angbracketleftbig
gt,x(it)/angbracketrightbig
−min
u∈V/bardbl·/bardbl2,σT/summationdisplay
t=1/angbracketleftbig
gt,u(it)/angbracketrightbig/bracketrightBigg
=E/bracketleftBigg
max
u∈V/bardbl·/bardbl2,σT/summationdisplay
t=1/epsilon1⌈t/N⌉Lg
/bardblw(it)/bardbl2/angbracketleftbig
w(it),u(it)/angbracketrightbig/bracketrightBigg
=Lg√
N
Dσ√
N−1E/bracketleftBigg
max
u∈V/bardbl·/bardbl2,σT/summationdisplay
t=1/epsilon1⌈t/N⌉/angbracketleftbig
w(it),u(it)/angbracketrightbig/bracketrightBigg
=Lg√
N
Dσ√
N−1E
max
u∈V/bardbl·/bardbl2,σT/N/summationdisplay
τ=1/epsilon1τ/angbracketleftw,u/angbracketright

≥Lg√
N
Dσ√
N−1E
max
u∈{−w,w}T/N/summationdisplay
τ=1/epsilon1τ/angbracketleftw,u/angbracketright

≥DLgσ/radicalbig
N(N−1)E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/N/summationdisplay
τ=1/epsilon1τ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(23)
≥DLg
2σ√
N−1√
2T, (24)
where in Equation (23) we used /bardblw/bardbl2
2= (N−1)D2σ2and in Equation (24) we used the Khintchine inequality,
see, e.g., (Cesa-Bianchi & Lugosi, 2006, Lemma 8.2). We now combine this lower bound with the one obtained
by choosing it= 1for alltand applying the standard single-agent lower boundDLg
2√
2T, see, e.g., (Orabona,
2019, Theorem 5.1). Combining these two bounds, we obtain that the regret is lower bounded by
DLg
2√
2Tmax/braceleftbig
1,σ√
N−1/bracerightbig
≥DLg
4√
2T/parenleftbig
1 +σ√
N−1/parenrightbig
≥1
4/parenleftBig
DLg/radicalbig
1 +σ2(N−1)√
2T/parenrightBig
,
which is only 1/4of the upper bound Equation (14). Hence, with knowledge of σ2, there is no algorithm with
better multitask acceleration than MT-OMD, up to constant factors.
A.4 Proof of Proposition 4
Consider the following setting. Let N= 2d,σ<1,u0∈Rdbe such that/bardblu0/bardbl2
2= 1−σ2, and set for all i≤d:
u(2i−1)=u0−/radicalbig
(N−1)/Nσei,andu(2i)=u0+/radicalbig
(N−1)/Nσei,
where (ei)i≤dis the canonical basis of Rd. It is easy to check that/vextenddouble/vextenddoubleu(i)/vextenddouble/vextenddouble2
2≥1−2σ2for alli≤N, and that
Var/bardbl·/bardbl2(u) =σ2. Then, a standard lower bound for OGD (Orabona, 2019, Theorem 5.1) applied to the N
individual independent OGDs of IT-OGD with linear losses, unit-norm loss gradients, and cyclic activations
such thatTi=T/N, gives that
RIT−OGD
T≥N/summationdisplay
i=1/vextenddouble/vextenddoubleu(i)/vextenddouble/vextenddouble
2√2Ti
4≥/radicalbig
2(1−2σ2)NT
4.
This lower bound is strictly greater than the MT-OGD upper bound Equation (14) as soon as√
(1−2σ2)N
4>/radicalbig
1 +σ2(N−1), or equivalently as soon as σ2≤N−16
18N−16.
22Published in Transactions on Machine Learning Research (09/2022)
A.5 Proof of Proposition 5
As discussed in the main body, the analysis for general norms is made more complex by the fact that
Equation (21) does not hold with equality anymore. Instead, a series of approximations leveraging the
properties of norms is required. Indeed, for any norm /bardbl·/bardbl, and regularizer ψ=1
2/bardbl·/bardbl2, it holds
2B /parenleftBig
A(b)1/2u,A(b)1/20/parenrightBig
=N/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftBig
A(b)1/2u/parenrightBig(i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=N/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble√
1 +bu(i)+ (1−√
1 +b)¯u/vextenddouble/vextenddouble/vextenddouble2
=N/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble¯u+√
1 +b(u(i)−¯u)/vextenddouble/vextenddouble/vextenddouble2
≤2/parenleftBigg
N/bardbl¯u/bardbl2+ (1 +b)N/summationdisplay
i=1/vextenddouble/vextenddoubleu(i)−¯u/vextenddouble/vextenddouble2/parenrightBigg
≤2/parenleftBig
2ND2+b(N−1)Var/bardbl·/bardbl(u)/parenrightBig
≤4ND2/parenleftbigg
1 +bN−1
Nσ2/parenrightbigg
. (25)
In comparison to Equation (21), the bound is thus multiplied by 4. The rest of the proof (i.e. the optimization
inηandb) is similar to that of Proposition 2, and we obtain that for all u∈V/bardbl·/bardbl,σit holds:
RT(u)≤DLg/radicalbig
1 +σ2(N−1)√
8T.
An interesting use case unlocked by the previous result is the use of the p-norm (for p∈[1,2]) on the the
probability simplex ∆ ={x∈Rd
+:/summationtext
jxj= 1}. Indeed, by a careful tuning of pone can derive bounds scaling
as√
lnd, instead of dfor OGD. Interestingly, this improvement is orthogonal to our multitask acceleration,
so that it is possible to beneﬁt from both. Recall that regularizer1
2/bardbl·/bardbl2
pis(p−1)strongly convex with
respect to/bardbl·/bardblp(Shalev-Shwartz, 2007, Lemma 17), and that the dual norm of /bardbl·/bardblpis/bardbl·/bardblq, withqsuch
that 1/p+ 1/q= 1. Consider V= ∆, and loss functions such that /bardblgt/bardbl∞≤L∞for allgt∈∂/lscriptt(x),x∈∆.
One can check that /bardblgt/bardbl2
q≤L2
∞d2/q. Then, substituting Equation (25) into Equation (11) and using the
previous remark, we get that for all u∈/bardbl·/bardblp,σit holds
RT(u)≤2ND2/parenleftbig
1 +bN−1
Nσ2/parenrightbig
η+b+N
(1 +b)Nη
2(p−1)TL2
∞d2/q
≤2DL∞d1/q
√p−1/radicalBigg/parenleftbig
1 +bN−1
Nσ2/parenrightbig
(b+N)
1 +bT,
with the choice η= 2ND/radicalbigg
(1+b)(1+bN−1
Nσ2)
b+N(p−1)
TL2∞d2/q. Hence, the bound obtained is the product of two terms,
one depending only on b, the other only on p. We can thus optimize independently. The term in bis the
same as in previous proofs, we can reuse the analysis made for Proposition 2. The term in dis the same as in
the original proof Grove et al. (1997); Gentile (2003), and the optimization is thus similar. We repeat it here
for completeness. One has d1/q/√p−1=√q−1d1/q≤√qd1/q. By diﬀerentiating with respect to q, the last
term is minimized for q= 2 lnd, with a value of√
2elnd. The ﬁnal bound obtained is
L∞/radicalbig
1 +σ2(N−1)√
16e Tlnd.
We conclude with a few remarks. First, in order to ensure that q≥2(we needp≤2), we may assume
thatd≥3. Second, note that the improvement on the dependence with respect to dcomes at the price
of a stronger variance condition, as we have Var/bardbl·/bardbl2(u)≤Var/bardbl·/bardblp(u)for allp≤2. If one is interested in a
condition independent from d(indeedpdepends on q, which depends on d), the variance with respect to
/bardbl·/bardbl 1can be used. Finally, note that we have used D= supx∈∆/bardblx/bardblp≤supx∈∆/bardblx/bardbl1= 1.
23Published in Transactions on Machine Learning Research (09/2022)
A.6 Proof of Proposition 6
This proposition builds upon the second claim of Theorem 1. We start by detailing the proof for the speciﬁc
choice/bardbl·/bardbl=/bardbl·/bardbl 2. Observe that for ψ=1
2/bardbl·/bardbl2
2and allu,x∈V/bardbl·/bardbl2,σwe have
B /parenleftbig
A1/2u,A1/2x/parenrightbig
=1
2/bardblu−x/bardbl2
A≤/bardblu/bardbl2
A+/bardblx/bardbl2
A≤2ND2/parenleftbigg
1 +bN−1
Nσ2/parenrightbigg
.
Substituting into Equation (12), we obtain
RT(u)≤2ND2/parenleftbig
1 +bN−1
Nσ2/parenrightbig
ηT+b+N
2(1 +b)NT/summationdisplay
t=1ηt/bardblgt/bardbl2
2
=b+N
(1 +b)N/parenleftBigg¯D2
2ηT+1
2T/summationdisplay
t=1ηt/bardblgt/bardbl2
2/parenrightBigg
,
with ¯D2=4N2D2(1+b)(1+bN−1
Nσ2)
b+N. Usingηt=√
2¯D
2/radicalBig/summationtextt
i=1/bardblgi/bardbl2
2=ND/radicalbigg
2(1+b)(1+bN−1
Nσ2)
(b+N)/summationtextt
i=1/bardblgi/bardbl2
2, (Orabona, 2019,
Lemma 4.13) gives
RT(u)≤b+N
(1 +b)N√
2¯D/radicaltp/radicalvertex/radicalvertex/radicalbtT/summationdisplay
t=1/bardblgt/bardbl2
2≤D/radicalBigg
(b+N)/parenleftbig
1 +bN−1
Nσ2/parenrightbig
1 +b/radicaltp/radicalvertex/radicalvertex/radicalbt8T/summationdisplay
t=1/bardblgt/bardbl2
2. (26)
Choosingb=Nconcludes the proof. In comparison to Proposition 2 (and assuming that /bardblgt/bardbl2≤Lg), the
bound is multiplied by√
8. One√
2multiplication is due to the choice of ηt, while the other√
4multiplication
comes from the upper bound on maxt≤TB /parenleftbig
A1/2u,A1/2xt/parenrightbig
, which is 4times bigger than the upper bound
ofB /parenleftbig
A1/2u,A1/2x1/parenrightbig
. The proof for any norm follows the same path. The only diﬀerence is on bounding
maxt≤TB /parenleftbig
A1/2u,A1/2xt/parenrightbig
, which is 4times bigger than the same quantity for the Euclidean case, see
Equation (25). Therefore, an additional√
4 = 2factor is added.
A.7 Proof of Proposition 7
Using Equation (26) with b=N, the smoothness of the losses gives
RT(u)≤4D/radicalbig
1 +σ2(N−1)/radicaltp/radicalvertex/radicalvertex/radicalbtMT/summationdisplay
t=1/lscriptt/parenleftbig
x(it)
t/parenrightbig
.
Using Lemma 4.20 in Orabona (2019), we obtain
RT(u)≤8D/radicalbig
1 +σ2(N−1)
2MD/radicalbig
1 +σ2(N−1) +/radicaltp/radicalvertex/radicalvertex/radicalbtMT/summationdisplay
t=1/lscriptt/parenleftbig
u(it)/parenrightbig
.
As for Proposition 6, the claim for general losses is obtained by multiplying the bound by 2.
A.8 Proof of Proposition 8
Letx1= [x∗,...,x∗], and observe that A(b)1/2x1=x1. Then, for all u∈such thatA(b)1/2u∈we
have
B /parenleftBig
A(b)1/2u,A(b)1/2x1/parenrightBig
=B /parenleftBig
A(b)1/2u,x1/parenrightBig
=N/summationdisplay
i=1Bψ/parenleftBig/parenleftbig
A(b)1/2u/parenrightbig(i),x∗/parenrightBig
≤NC. (27)
24Published in Transactions on Machine Learning Research (09/2022)
Plugging Equation (27) into Equation (11), we obtain for all u∈:
RT(u)≤NC
η+η(b+N)
2λ(1 +b)NTL2
g≤Lg/radicalbigg
2
λb+N
b+ 1CT, (28)
where we have set η=N
Lg/radicalBig
2λ(1+b)C
(b+N)T. The next natural question is how to choose b?As bound Equation (28)
is decreasing in b, one is encouraged to choose bas large as possible. However, recall that Equation (27)
requiresA(b)1/2uto be in . So the optimal choice is b∗=max{b≥0:A(b)1/2u∈}. This value
unfortunately depends on uand cannot be used uniformly over . However, the variance condition for σ
allows to choose a global b, as we show now. Let u∈σ. Recall that for all i≤Nwe have
/parenleftbig
A(b)1/2u/parenrightbig(i)=√
1 +bu(i)+ (1−√
1 +b)¯u.
We have to check that these vectors are in the simplex for all i≤N. There are two conditions that a vector
xshould verify to be in the simplex
(i) 1/latticetopx= 1,and (ii)xj≥0∀j≤d.
It is immediate to see that the ﬁrst condition is always satisﬁed. To analyze the second condition, we recall
the following notation from Deﬁnition 2
∀j≤d,umax
j= max
i≤Nu(i)
j,umin
j= min
i≤Nu(i)
j.
Now, letj≤d. A suﬃcient condition for (ii)to hold for every/parenleftbig
A(b)u/parenrightbig(i),i≤N, is
0≤√
1 +bumin
j+ (1−√
1 +b)umax
j,
Or, equivalently,
b≤/parenleftBigg
umax
j
umax
j−umin
j/parenrightBigg2
−1.
Since this condition must hold for every j≤d, the overall condition is
b≤min
j≤d/parenleftBigg
umax
j
umax
j−umin
j/parenrightBigg2
−1 =1
σ2−1 =1−σ2
σ2.
This condition is quite intuitive. If all best models are equal, umin
j=umax
jfor allj≤d, one can choose
b= +∞, and achieves a bound independent from N. On the contrary, if there exists j≤dsuch that
umax
j= 1andumin
j= 0, i.e., two diﬀerent corners are linked, then b= 0is the only possible choice, and a√
Ndependence is unavoidable. Finally, with b= (1−σ2)/σ2≥0, bound Equation (28) becomes
Lg/radicalbig
1 +σ2(N−1)/radicalbig
2CT/λ.
A.9 Proof of Theorem 9
Letε>0, and consider the covering Cε={ε,2ε,..., 1}, of cardinality 1/ε. Letu∈V, we want to derive an
upper bound of the regret achieved by the best expert in Cεagainstu. First, assume that Var/bardbl·/bardbl2(u)≤1,
and let ¯σ2=inf{z∈Cε: Var/bardbl·/bardbl2(u)≤z}. Note that by deﬁnition we have Var/bardbl·/bardbl2(u)≤¯σ2≤Var/bardbl·/bardbl2(u) +ε.
Now, let us bound the regret of MT-OGD run with σ2=¯σ2. Recall that for any ﬁxed learning rate η, the
regret of MT-OGD is bounded by
ND2/parenleftbig
1 + (N−1)Var/bardbl·/bardbl2(u)/parenrightbig
2η+ηTL2
g
N≤ND2/parenleftbig
1 + (N−1)¯σ2/parenrightbig
2η+ηTL2
g
N. (29)
25Published in Transactions on Machine Learning Research (09/2022)
MT-OGD run with σ2=¯σ2usesη=ND
Lg/radicalBig
1+(N−1)¯σ2
2T. Plugging this into Equation (29), we get that the
regret is upper bounded by
DLg/radicalbig
1 + ¯σ2(N−1)√
2T≤DLg/parenleftBig
1 +/radicalBig
Var/bardbl·/bardbl2(u)·N+√
εN/parenrightBig√
2T
=DLg/parenleftbigg
1 +/radicalBig
min/braceleftbig
Var/bardbl·/bardbl2(u),1/bracerightbig
·N+√
εN/parenrightbigg√
2T. (30)
On the other hand, if Var/bardbl·/bardbl2(u)≥1, we know that MT-OGD run with σ2= 1is equivalent to independent
OGDs and has a regret bounded by
DLg√
NT≤DLg/parenleftBig
1 +√
N+√
εN/parenrightBig√
2T
=DLg/parenleftbigg
1 +/radicalBig
min/braceleftbig
Var/bardbl·/bardbl2(u),1/bracerightbig
·N+√
εN/parenrightbigg√
2T. (31)
Combining Equation (30) and Equation (31), we know that in any case the best expert in Cεhas always a
regret against usmaller than
DLg/parenleftbigg
1 +/radicalBig
min/braceleftbig
Var/bardbl·/bardbl2(u),1/bracerightbig
·N+√
εN/parenrightbigg√
2T. (32)
Let us now compute the regret of Hedge-MT-OGD against the best expert in Cε. By the analysis of Hedge
with linear combination of the experts, we know that it is bounded by (Orabona, 2019):
√
2
2L∞/radicalbig
Tlog(1/ε), (33)
whereL∞is an upper bound of the inﬁnity norm of the gradients received by Hedge. The latter are equal
to the vectors of losses incurred by the diﬀerent experts at each time step. With linear(ized) losses, and
denoting byxexpert
tthe prediction of one expert at time step t, we have
/lscriptt/parenleftbig
xexpert
t/parenrightbig
=/angbracketleftBig
gt,xexpert
t/angbracketrightBig
=/angbracketleftbig
gt,xexpert,(it)
t/angbracketrightbig
≤/bardblgt/bardbl2·/vextenddouble/vextenddoublexexpert,(it)
t/vextenddouble/vextenddouble
2≤DLg.
Hence,L∞≤DLg. Now, for any u∈V, the regret of Hedge-MT-OGD against uis upper bounded by the
sum of: (1) the regret of Hedge with respect to the best expert in Cε, that is upper bounded by Equation (33),
and (2) the regret of the best expert in Cεagainstu, that is upper bounded by Equation (32). Summing the
two upper bounds and setting ε= 1/Nyields the desired result.
A.10 Proof of Proposition 10
Let(/lscriptt,i)i∈Atbe the losses associated to the active agents at times step t. For MT-OMD run with matrix
A=A(b) = (1 +b)IN−b
N11/latticetop, forb≥0, and constant learning rate η>0, we have
RT(u) =T/summationdisplay
t=1/summationdisplay
i∈At/lscriptt,i/parenleftbig
x(i)
t/parenrightbig
−/lscriptt,i/parenleftbig
u(i)/parenrightbig
≤u/latticetopAu
2η+η
2T/summationdisplay
t=1/bardblgt/bardbl2
A−1
=ND2/parenleftbig
1 +bN−1
Nσ2/parenrightbig
2η+η
2T/summationdisplay
t=1/bardblgt/bardbl2
A−1, (34)
26Published in Transactions on Machine Learning Research (09/2022)
wheregt∈RNdis the compound gradient vector with non-zero blocks at the indices present in At. Recall
thatA(b)−1=1
1+bIN+b
1+b11/latticetop
N. We have
/bardblgt/bardbl2
A−1=/summationdisplay
i,jA−1
ij/angbracketleftbig
g(i)
t,g(j)
t/angbracketrightbig
=/summationdisplay
i∈AtA−1
ii/vextenddouble/vextenddoubleg(i)
t/vextenddouble/vextenddouble2
2+/summationdisplay
i/negationslash=j∈AtA−1
ij/angbracketleftbig
g(i)
t,g(j)
t/angbracketrightbig
≤/parenleftbigg
pb+N
(1 +b)N+p(p−1)b
(1 +b)N/parenrightbigg
L2
g
=pL2
gpb+N
(1 +b)N
Substituting into equation 34 and setting b=√pN, we have
RT(u)≤ND2/parenleftbig
1 +√pσ2(N−1)/parenrightbig
2η+η
2TL2
gp(1 +p)
N
Settingη=ND
Lg√
T/radicalBig
1+√pσ2(N−1)
p(1+p), we ﬁnally get
Rt(u)≤DLg/radicalbig
pT/radicalbig
1 +σ2(N−1)/radicalbig
p1/2+p3/2.
B Derivation of the Algorithms
In this appendix we gather all the technical details about the algorithms.
B.1 Details on MT-OGD
With the change of feasible set, xt+1produced by MT-OGD is the solution to
min
x∈RNd/vextenddouble/vextenddoublext−ηtA−1gt−x/vextenddouble/vextenddouble2
A,
s.t./bardblx/bardbl2
A≤(1 +bσ2)ND2,
or again
min
x∈RNd/vextenddouble/vextenddoubleA1/2xt−ηtA−1/2gt−A1/2x/vextenddouble/vextenddouble2
2,
s.t./vextenddouble/vextenddoubleA1/2x/vextenddouble/vextenddouble2
2≤(1 +bσ2)ND2.
Introducing the notation yt=A1/2xt, the update on the yt’s writes
yt+1= Proj/parenleftBig
yt−ηtA−1/2gt,/radicalbig
(1 +bσ2)ND/parenrightBig
.
B.2 Details on MT-EG
Recall that for the negative entropy regularizer we have B (x,y) =N/summationdisplay
i=1d/summationdisplay
j=1x(i)
jlnx(i)
j
y(i)
j. Expliciting the
objective function of the ﬁrst update, we obtain
/angbracketleftηA−1/2gt,y/angbracketright+B (y,yt) =N/summationdisplay
i=1d/summationdisplay
j=1ηA−1/2
iitgt,jy(i)
j+y(i)
jlny(i)
j
y(i)
t,j.
27Published in Transactions on Machine Learning Research (09/2022)
For alli≤Nandj≤d, diﬀerentiating with respect to y(i)
jand setting the gradient to 0we get
˜y(i)
t+1,j=y(i)
t,jexp/parenleftbig
−ηA−1/2
iitgt,j−1/parenrightbig
. (35)
We focus now on the second updateEquation (17). The constraint y∈A1/2()rewritesA−1/2y∈, or
equivalently
∀i≤N, 1/latticetop/parenleftBig
A−1/2y/parenrightBig(i)
= 1,
∀i≤N, j≤d,/parenleftBig
A−1/2y/parenrightBig(i)
j≥0.
We introduce matrix Y∈RN×dsuch thatYkl=y(k)
l. Then it holds for all i≤N
1/latticetop/parenleftBig
A−1/2y/parenrightBig(i)
=d/summationdisplay
l=1/parenleftBig
A−1/2y/parenrightBig(i)
l=d/summationdisplay
l=1N/summationdisplay
k=1A−1/2
ikYkl=d/summationdisplay
l=1/bracketleftbig
A−1/2Y/bracketrightbig
il=/bracketleftbig
A−1/2Y 1/bracketrightbig
i.
Hence, using that A−1/2is stochastic, the ﬁrst constraint rewrites
A−1/2Y 1= 1⇐⇒Y 1= 1⇐⇒ ∀i≤N, 1/latticetopy(i)= 1. (36)
Similarly, the second constraint reads:
∀k≤N, j≤d,/bracketleftbig
A−1/2Y/bracketrightbig
kj≥0. (37)
The Lagrangian associated to Problem Equation (17) writes
L(y,Λ,) =N/summationdisplay
i=1d/summationdisplay
j=1y(i)
jlny(i)
j
˜y(i)
t+1,j−N/summationdisplay
k=1d/summationdisplay
j=1Λkj/bracketleftbig
A−1/2Y/bracketrightbig
kj+N/summationdisplay
i=1µi( 1/latticetopy(i)−1)
=N/summationdisplay
i=1d/summationdisplay
j=1YijlnYij
˜y(i)
t+1,j−N/summationdisplay
k=1d/summationdisplay
j=1N/summationdisplay
i=1ΛkjA−1/2
kiYij+N/summationdisplay
i=1d/summationdisplay
j=1µiYij− 1/latticetop,
with Λ∈RN×dand∈RNtheLagrangemultipliersassociatedtoconstraintsEquation(37)andEquation(36)
respectively. For all i≤N,j≤d, diﬀerentiating with respect to Yijand setting the gradient to 0yields
/bracketleftBig
Y(t+1)/bracketrightBig
ij:=y(i)
t+1,j=˜y(i)
t+1,je/bracketleftbig
A−1/2Λ/bracketrightbig
ij−µi−1. (38)
Furthermore, the complementary slackness writes for all i≤Nandj≤d
Λij/bracketleftbig
A−1/2Y(t+1)/bracketrightbig
ij= 0.
However, Equation (38) gives/bracketleftbig
Y(t+1)/bracketrightbig
ij>0, and matrix A−1/2is stochastic (see Lemma 13), so we have/bracketleftbig
A−1/2Y(t+1)/bracketrightbig
ij>0, and consequently Λij= 0. Then
y(i)
t+1,j=˜y(i)
t+1,je−µi−1.
Using 1/latticetopy(i)
t+1= 1, we gete−µi−1= 1//parenleftbig/summationtextd
j=1˜y(i)
t+1,j/parenrightbig
. Substituting Equation (35) we get for all i≤N
y(i)
t+1,j=˜y(i)
t+1,j/summationtextd
k=1˜y(i)
t+1,k
=y(i)
t,je−ηA−1/2
iitgt,j−1
/summationtextd
k=1y(i)
t,ke−ηA−1/2
iitgt,k−1
=y(i)
t,je−ηA−1/2
iitgt,j
/summationtextd
k=1y(i)
t,ke−ηA−1/2
iitgt,k.
28Published in Transactions on Machine Learning Research (09/2022)
Lemma 13. LetA=IN+L, whereLis the Laplacian matrix associated to any weighted undirected graph
on{1,...,N}. ThenA−1andA−1/2are (doubly) stochastic matrices.
Proof.It is immediate to see from the deﬁnition of AthatA−1andA−1/2are both symmetric and satisfy
A−11=A−1/21= 1. It remains to check that their entries are nonnegative. To that end, we use that
inverses ofM-matrices are entrywise nonnegative. Matrix Ais a non-singular M-matrix, as its oﬀ-diagonal
entries are nonpositive (i.e., Ais aZ-matrix) and its eigenvalues are positive. As a consequence, A1/2is also
aM-matrix (Alefeld & Schneider, 1982, Theorem 4), which concludes the proof.
C Technical Comparison to Cavallanti et al. (2010)
First, wewouldliketoremindthereaderthat, unlikeCavallantietal.(2010): ( i)wetackleanysubdiﬀerentiable
loss function, and not only the hinge loss for classiﬁcation, ( ii) we address any strongly convex regularizer,
and not only the squared Euclidean norm, ( iii) our proofs provide clear insights on the regularizer’s behaviour,
and are not just black box applications of the Kernel Perceptron Theorem, ( iv) we provide (matching) lower
bounds, (v) we develop what we believe is the correct generalization to p-norms, see technical details below,
(vi) we provide a unifying framework and a general analysis that allow to deal with non-Euclidean geometries
on the simplex, ( vii) we are adaptive to the task variance σ2.
About the p-norm extension, the two regularizers used in Cavallanti et al. (2010) and this paper are
fundamentally diﬀerent, as they write respectively
 (u) =/bardblAu/bardbl2
p,and (u) =N/summationdisplay
i=1/vextenddouble/vextenddouble(A1/2u)(i)/vextenddouble/vextenddouble2
p.
A ﬁrst advantage of our method is that we recover the Euclidean framework by setting p= 2, which is not
the case for the regularizer used in Cavallanti et al. (2010). This makes us believe that we propose the right
way to address p-norms, and that a general theory of multitask acceleration is worth developing to avoid
misinterpretations of this kind. A second advantage is that our bounds have a much better dependence with
respect to the task variance σ2and the number of tasks N. Recall that after several approximations, see
Appendix A.5, our bound for p-norms on the simplex features the variance term
/radicalbig
1 + (N−1)σ2≤1 +√
Nσ, (39)
whereσ2is an upper bound of the task variance according to /bardbl·/bardbl 1, such that
Var/bardbl·/bardbl1(u) =1
N−1N/summationdisplay
i=1/vextenddouble/vextenddoubleu(i)−¯u/vextenddouble/vextenddouble2
1≤σ2.
On the other hand, the bound of Theorem 6 in Cavallanti et al. (2010) features the variance term
/bardblAu/bardbl1
N=1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble(1 +N)u(i)−N¯u/vextenddouble/vextenddouble/vextenddouble
1
≤1
NN/summationdisplay
i=1/parenleftBig/vextenddouble/vextenddoubleu(i)/vextenddouble/vextenddouble
1+N/vextenddouble/vextenddoubleu(i)−¯u/vextenddouble/vextenddouble
1/parenrightBig
≤1 +N/summationdisplay
i=1/vextenddouble/vextenddoubleu(i)−¯u/vextenddouble/vextenddouble
1
≤1 +/radicaltp/radicalvertex/radicalvertex/radicalbtNN/summationdisplay
i=1/vextenddouble/vextenddoubleu(i)−¯u/vextenddouble/vextenddouble2
1
≤1 +Nσ. (40)
29Published in Transactions on Machine Learning Research (09/2022)
From Equations (39) and (40), it is clear that our bounds exhibit a much better dependence with respect to
σ2and N. We further highlight that the bound of Theorem 6 in Cavallanti et al. (2010) contains an extra
term which is the square of Equation (40), without being multiplied by any time-related quantity though.
D Additional Experiment
We report the results of a synthetic experiment where we plot the multitask regret of MT-OGD against
the standard deviation σof the reference vectors. Speciﬁcally, we consider a regression problem in R2with
respect to the square loss. We set D= 1and generate N= 4tasks deﬁning the reference vectors as in the
proof of Proposition 3 (see Appendix A.3), so that the task variance is set to the desired value. Each task
consists of a sequence of 10losses, so that T= 40. The losses for the i-th task are generated by ﬁrst sampling
an instance xfrom the unit sphere centered at 1, and then computing its label as y=/angbracketleftu(i),x/angbracketright+/epsilon1, where/epsilon1is
an independent Gaussian noise with variance 0.1and truncated in [-1, 1]. We plot the ﬁnal multitask regret
RTaveraged over 30runs. MT-OGD is tuned with the optimal parameter choices for bandηaccording
to the theory. Results are shown in Figure 2. As expected, the regret of MT-OGD increases linearly with
σ, as suggested by the upper bound Equation (14) and the lower bound in Proposition 3. For the sake of
comparison, we also benchmark IT-OGD and report its best average performance. The switch in performance
occurs slightly before σ= 1, which is also expected as, aside from the dependence in σ, the bounds for
MT-OGD scales with√
2T(instead of√
Tfor IT-OGD) and are thus slightly worse for σ= 1.
Figure 2: Multitask regret RTof MT-OGD and IT-OGD for T= 40against the standard deviation σ.
30