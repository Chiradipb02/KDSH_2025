Theoretical characterisation of the Gauss-Newton
conditioning in Neural Networks
Jim Zhao∗
University of Basel, Switzerland
jim.zhao@unibas.chSidak Pal Singh∗
ETH Zürich, Switzerland
sidak.singh@inf.ethz.ch
Aurelien Lucchi
University of Basel, Switzerland
aurelien.lucchi@unibas.com
Abstract
The Gauss-Newton (GN) matrix plays an important role in machine learning, most
evident in its use as a preconditioning matrix for a wide family of popular adaptive
methods to speed up optimization. Besides, it can also provide key insights into the
optimization landscape of neural networks. In the context of deep neural networks,
understanding the GN matrix involves studying the interaction between different
weight matrices as well as the dependencies introduced by the data, thus rendering
its analysis challenging. In this work, we take a first step towards theoretically
characterizing the conditioning of the GN matrix in neural networks. We establish
tight bounds on the condition number of the GN in deep linear networks of arbitrary
depth and width, which we also extend to two-layer ReLU networks. We expand
the analysis to further architectural components, such as residual connections and
convolutional layers. Finally, we empirically validate the bounds and uncover
valuable insights into the influence of the analyzed architectural components.
1 Introduction
The curvature is a key geometric property of the loss landscape, which is characterized by the Hessian
matrix or approximations such as the Gauss-Newton (GN) matrix, and strongly influences the conver-
gence of gradient-based optimization methods. In the realm of deep learning, where models often have
millions of parameters, understanding the geometry of the optimization landscape is essential to under-
standing the effectiveness of training algorithms. The Hessian matrix helps identify the directions in
which the loss function changes most rapidly, aiding in the selection of appropriate learning rates and
guiding optimization algorithms to navigate the complex, high-dimensional space of parameters. How-
ever, in practice, the Hessian is not easily accessible due to the high computational cost and memory re-
quirements. Instead, the GN matrix (or its diagonal form) is commonly employed in adaptive optimiza-
tion methods such as Adam [Kingma and Ba, 2014] with the goal to improve the conditioning of the
landscape. Although the Gauss-Newton matrix GOis only an approximation to the full Hessian ma-
trix, it does seem to capture the curvature of the loss very well given the success of many second-order
optimization methods based on approximations of the Gauss-Newton matrix, such as K-FAC [Martens
and Grosse, 2020], Shampoo [Gupta et al., 2018] or Sophia [Liu et al., 2023]. Particularly interesting
is the last method, in which the authors observe that their optimizer based on the Gauss-Newton
matrix performs even better than their optimizer based on the full Hessian matrix, implying that the
Gauss-Newton matrix is a good preconditioner and captures the curvature of the loss landscape well.
∗First two authors have equal contribution
38th Conference on Neural Information Processing Systems (NeurIPS 2024).0 20 40 60 80 100
Epochs0102030Training loss
0 20 40 60 80 100
Epochs104105106107Condition number κ
Pruning rate
0% 20% 40% 60% 80%Figure 1: Training loss
(left) and condition num-
berκof GN (right) for a
ResNet20 trained on a sub-
set of Cifar10 ( n= 1000 )
with different proportions of
pruned weights. Weights
were pruned layerwise by
magnitude at initialization.
The prevalence of adaptive optimizers and their success in training neural networks illustrates the
ability of the GN matrix to capture curvature information similar to that of the Hessian matrix.
Although the non-convexity of neural loss landscapes can be considered as a given, some landscapes
can be tougher to traverse than others. This can, for instance, arise due to, or be amplified by,
aspects such as: (i) disparate scales of the input data and intermediate features, (ii) initializing the
optimization process at a degenerate location in the landscape (imagine an entire set of neurons being
dead), (iii) architectural aspects such as width, depth, normalization layers, etc.
Given the close relation of the GN matrix to the network Jacobian (with respect to the parameters) and
hence signal propagation [Lou et al., 2022], its significance also reaches more broadly to problems
such as model pruning and compression. For instance, it has been extensively observed empirically
that training extremely sparse neural networks from scratch poses a significantly greater challenge
compared to pruning neural networks post-training [Evci et al., 2019]. In an illustrative example
shown in Fig. 1, we observe that training sparse networks from scratch using stochastic gradient
descent results in a slowdown of training, which is also reflected in an increase in the condition
number of the GN matrix (similar experiments on Vision Transformers and other architectures can
be found in Appendix I.1). This underscores the relevance of the conditioning of the GN matrix
for understanding the behaviour of gradient-based optimizers and emphasizes the significance of
maintaining a well-behaved loss landscape of the neural network throughout training.
In fact, many fundamental components of deep learning frameworks, such as skip-connections as
well as various normalization techniques [Ioffe and Szegedy, 2015, Salimans and Kingma, 2016, Ba
et al., 2016, Wu and He, 2018, Miyato et al., 2018], have been to some degree designed to mitigate
the challenges posed by ill-conditioning in neural network optimization. This line of work continues
to expand to this date, seeking out better normalization schemes, optimization maneuvers, and regu-
larization strategies [Kang et al., 2016, Wan et al., 2013] that allow for easier and faster optimization
while avoiding directions of pathological curvature. But despite the numerous approaches to redress
the poor conditioning of the landscape, understanding the precise factors of ill-conditioning within
the network structure, and their relative contributions, has remained largely underdeveloped.
Hence, our aim in this work is to carry out a detailed theoretical analysis of how the conditioning
of the Gauss-Newton matrix is shaped by constituting structural elements of the network — i.e.,
the hidden-layer width, the depth, and the presence of skip connections, to name a few. We will
shortly formally introduce the Gauss-Newton matrix and how it is connected to the Hessian of the
loss function. Concretely, we would like to provide tight bounds for the condition number of these
two terms as a function of the spectra of the various weight matrices and reveal the interplay of the
underlying architectural parameters on conditioning. Furthermore, we aim to investigate the impact of
both the dataset’s structure and the initialization procedure on the conditioning of the loss landscape.
Contributions. Taking inspiration from prior theoretical analyses of deep linear networks, we make
a first foray into this problem by rigorously investigating and characterizing the condition number
of the GN matrix for linear neural networks (the extension to the second term of the Gauss-Newton
decomposition is touched upon in the Appendix). Our analysis holds for arbitrary-sized networks
and unveils the intriguing interaction of the GN matrix with the conditioning of various sets of
individual layers and the data-covariance matrix. We also complement our analysis in the linear case
by studying the effect of non-linearities, via the Leaky-ReLU activation, albeit for two-layer networks.
Importantly, as a consequence of our analysis, we show the precise manner in which residual networks
2with their skip connections or batch normalization can help enable better conditioning of the loss
landscape. While our work builds on Singh et al. [2021], our main contribution is the introduction of
tight upper bounds for the condition number of the Gauss-Newton (GN) matrix for linear and residual
networks of arbitrary depth and width. To the best of our knowledge, this has not been addressed in
the literature before. Lastly, given that our bounds are agnostic to the specific values of the parameters
in the landscape, we show the phenomenology of conditioning during the training procedure and the
corresponding validity of our bounds.
2 Setup and background
Setting. Suppose we are given an i.i.d. dataset S={(x1,y1), . . . , (xn,yn)}, of size |S|=n,
drawn from an unknown distribution pX,Y, consisting of inputs x∈X⊆Rdand targets y∈Y⊆Rk.
Based on this dataset S, consider we use a neural network to learn the mapping from the inputs to
the targets, Fθθθ:X7→Y, parameterized by θθθ∈ΘΘΘ⊆Rp. To this end, we follow the framework of
Empirical Risk Minimization [Vapnik, 1991], and optimize a suitable loss function L : ΘΘΘ7→R. In
other words, we solve the following optimization problem,
θθθ⋆= argmin
θθθ∈ΘΘΘL(θθθ) =1
nnX
i=1ℓ(θθθ; (xi,yi)),
say with a first-order method such as (stochastic) gradient descent and the choices for ℓcould be the
mean-squared error (MSE), cross-entropy (CE), etc. For simplicity, we will stick to the MSE loss.
Gauss-Newton Matrix. We analyze the properties of the outer gradient product of the loss function,
GO, which we call the Gauss-Newton matrix, defined as
GO=1
nnX
i=1∇θθθFθθθ(xi)∇θθθFθθθ(xi)⊤, (1)
where, ∇θθθFθθθ(xi)∈Rp×kis the Jacobian of the function with respect to the parameters θθθ,pis the
number of parameters, kis the number of outputs or targets. This outer product of the gradient is
closely related to the Hessian of the loss function via the Gauss-Newton decomposition [Schraudolph,
2002, Sagun et al., 2017, Martens, 2020, Botev, 2020], hence the chosen name, which decomposes
the Hessian via the chain rule as a sum of the following two matrices:
HL=HO+HF=1
nnX
i=1∇θθθFθθθ(xi)
∇2
Fθθθℓi
∇θθθFθθθ(xi)⊤+1
nnX
i=1KX
c=1[∇Fθθθℓi]c∇2
θθθFc
θθθ(xi),
where ∇2
Fθθθℓi∈Rk×kis the Hessian of the loss with respect to the network function, at the i-th
sample. Note that if ℓiis the MSE loss, then HO=GO.
Remark R1 (Difference between HLandGO).When considering MSE loss, the difference between
the Gauss Newton matrix GOand the Hessian of the loss function HLdepends on both the residual
and the curvature of the network Fθθθ(x). Thus, close to convergence when the residual becomes small,
the contribution of HFwill also be negligible and GOis essentially equal to HL. Furthermore,
Lee et al. [2019] show that sufficiently wide neural networks of arbitrary depth behave like linear
models during training with gradient descent. This implies that the Gauss-Newton matrix is a close
approximation of the full Hessian in this regime throughout training.
Condition number and its role in classical optimization. Consider we are given a quadratic
problem, argminw∈Rp1
2w⊤Aw, where A≻0is a symmetric and positive definite matrix. The
optimal solution occurs for w∗= 0. When running gradient descent with constant step size
η > 0, the obtained iterates would be wk= (I−ηA)wk−1. This yields a convergence rate
of∥wk−w∗∥
∥wk−1−w∗∥≤max 
|1−ηλmax(A)|,|1−ηλmin(A)|
.The best convergence is obtained for
η= 2 ( λmax(A) +λmin(A))−1, resulting in ∥wk∥ ≤κ−1
κ+1∥wk−1∥, where κ(A) =λmax(A)
λmin(A)is
called the condition number of the matrix. This ratio which, intuitively, measures how disparate
are the largest and smallest curvature directions, is an indicator of the speed with which gradient
descent would converge to the solution. When κ→ ∞ , the progress can be painfully slow, and
κ→1indicates all the curvature directions are balanced, and thus the error along some direction
does not trail behind the others, hence ensuring fast progress.
3Effect of condition number at initialization on the convergence rate As the condition number
is a very local property, it is in general hard to connect the conditioning at network initialization to
a global convergence rate. However, we would like to argue below that an ill-conditioned network
initialization will still affect the rate of convergence for gradient descent (GD) in the initial phase of
training. Let us denote the Lipschitz constant by Land the smoothness constant by µ. Furthermore,
let the step size be such that ηk≤1
L. We present a modified analysis of GD for strongly convex
functions, where we use local constants µ(k)andL(k)instead of the global smoothness and Lipschitz
constant, respectively. Then by the definition of a single step of gradient descent and using the strong
convexity and smoothness assumption2we have:
||θk+1−θ∗||2≤(1−ηkµ)||θk−θ∗||2(2)
So by recursively applying (2) and replacing µby the local smoothness constants µ(k):
||θk−θ∗||2≤k−1Y
i=0(1−ηiµ(i))||θ0−θ∗||2(3)
One can clearly see the effect of µ(0)in the bound, which is even more dominant when µ(k)changes
slowly. Of course, the effect of µ(0)attenuates over time, and that’s why we are talking about a local
effect. However, one should keep in mind that overparametrization leads the parameter to stay closer
to initialization (at least in the NTK regime [Lee et al., 2019]).
3 Related work
Since the Gauss-Newton matrix is intimately related to the Hessian matrix, and the fact that towards
the end of training, the Hessian approaches the Gauss-Newton matrix [Singh et al., 2021], we carry
out a broader discussion of the related work, by including the significance of the Hessian at large.
The relevance of the Hessian matrix for neural networks. (i)Generalization-focused work:
There is a rich and growing body of work that points towards the significance of various Hessian-
based measures in governing different aspects of optimization and generalization. One popular
hypothesis [Hochreiter and Schmidhuber, 1997, Keskar et al., 2016, Dziugaite and Roy, 2017,
Chaudhari et al., 2019] is that flatter minima generalize better, where the Hessian trace or the spectral
norm is used to measure flatness. This hypothesis is not undisputed [Dinh et al., 2017], and the extent
to which this is explanatory of generalization has been put to question recently [Granziol, 2021,
Andriushchenko et al., 2023]. Nevertheless, yet another line of work has tried to develop regularization
techniques that further encourage reaching a flatter minimum, as shown most prominently in proposing
sharpness-aware minimization [Foret et al., 2021].
(ii) Understanding Architectural and Training aspects of Neural Networks: Some other work has
studied the challenges in large-batch training via the Hessian spectrum in Yao et al. [2018]. Also,
the use of large learning rates has been suggested to result in flatter minima via the initial catapult
phase [Lewkowycz et al., 2020]. The effect of residual connections and Batch normalization on Hes-
sian eigenspectrum were empirically studied in Ghorbani et al. [2019], Yao et al. [2020], which intro-
duced PyHessian, a framework to compute Hessian information in a scalable manner. More recently,
the so-called edge of stability [Cohen et al., 2021] phenomenon connects the optimization dynamics of
gradient descent with the maximum eigenvalue of the loss Hessian. Very recently, the phenomenon of
Deep neural collapse was studied in Beaglehole et al. [2024] via the average gradient outer-product.
(iii) Applications: There has also been a dominant line of work utilizing the Hessian for second-order
optimization, albeit via varying efficient approximations, most notably via K-FAC [Martens and
Grosse, 2020] but also others such as Yao et al. [2021], Liu et al. [2023], Lin et al. [2023]. Given its
versatile nature, the Hessian has also been used for model compression through pruning [LeCun et al.,
1989, Hassibi et al., 1993, Singh and Alistarh, 2020] as well as quantization [Dong et al., 2019, Frantar
et al., 2023], but also in understanding the sensitivity of predictions and the function learned by neural
networks via influence functions [Koh and Liang, 2020, Grosse et al., 2023], and countless more.
Theoretical and empirical studies of the Hessian. There have been prior theoretical studies that
aim to deliver an understanding of the Hessian spectrum in the asymptotic setting [Pennington
and Bahri, 2017, Jacot et al., 2019], but it remains unclear how to extract results for finite-width
2For details of the derivation, we refer the reader to Appendix H.1
4networks as used in practice. Besides, past work has analyzed the rank, empirically [Sagun et al.,
2016, 2017] as well as theoretically [Singh et al., 2021, 2023]. Further, the layer-wise Hessian of
a network can be roughly approximated by the Kronecker product of two smaller matrices, whose
top eigenspace has shown to contain certain similarities with that of the Hessian [Wu et al., 2020].
In a different line of work by Liao and Mahoney [2021], the limiting eigenvalue distribution of the
Hessian of generalized generalized linear models (G-GLMs) and the behaviour of potentially isolated
eigenvalue-eigenvector pairs is analyzed.
Hessian and landscape conditioning. The stock of empirical repertoire in deep learning has been en-
riched by successful adaptive optimization methods plus their variants [Kingma and Ba, 2014] as well
as various tricks of the trade, such as Batch Normalization [Ioffe and Szegedy, 2015], Layer Normal-
ization [Ba et al., 2016], orthogonal initialization [Saxe et al., 2013, Hu et al., 2020], and the kind, all of
which can arguably be said to aid the otherwise ill-conditioning of the landscape. There have also been
theoretical works establishing a link between the conditioning of the Hessian, at the optimum and the
double-descent like generalization behavior of deep networks [Belkin et al., 2019, Singh et al., 2022].
Gauss-Newton matrix and NTK. In the context of over-parametrized networks, GOis for instance
connected to the (empirical) Neural Tangent Kernel, which has been the focus of a major line of
research in the past few years [Jacot et al., 2018, Wang et al., 2022, Yang, 2020] as the NTK
presents an interesting limit of infinite-width networks. As a result the asymptotic spectrum and the
minimum eigenvalue of the NTK has been studied in [Nguyen et al., 2022, Liu and Hui, 2023], but
the implications for finite-width networks remain unclear.
Despite this and the other extensive work discussed above, a detailed theoretical study on the Gauss
Newton conditioning of neural networks has been absent. In particular, there has been little work
trying to understand the precise sources of ill-conditioning present within deep networks. Therefore,
here we try to dissect the nature of conditioning itself, via a first principle approach. We hope this
will spark further work that aims to precisely get to the source of ill-conditioning in neural networks
and, in a longer horizon, helps towards designing theoretically-guided initialization strategies or
normalization techniques that seek to also ensure better conditioning of the GN matrix.
4 Theoretical characterisation
The main part of this paper will focus on analyzing the conditioning of GOin Eq. (1)as prior
work [Ren and Goldfarb, 2019, Schraudolph, 2002] has demonstrated its heightened significance
in influencing the optimization process. We will further discuss an extension to HFin Appendix F.
Tying both bounds together yields a bound on the condition number of the overall loss Hessian in
some simple setups.
Pseudo-condition number. Since the GN matrix of deep networks is not necessarily full rank [Sagun
et al., 2017, Singh et al., 2021], we will analyze the pseudo-condition number defined as the ratio
of the maximum eigenvalue over the minimum non-zero eigenvalue. This choice is rationalized by
the fact that gradient-based methods will effectively not steer into the GN null space, and we are
interested in the conditioning of the space in which optimization actually proceeds. For brevity, we
will skip making this distinction between condition number and pseudo-condition number hereafter.
4.1 Spectrum of Gauss-Newton matrix
We will start with the case of linear activations. In this case a network with Llayers can be expressed
byFθθθ(x) =WLWL−1···W1x, with Wℓ∈Raℓ×aℓ−1forℓ= 1, . . . , L andaL=k, a0=d. To
facilitate the presentation of the empirical work, we will assume that the widths of all hidden layers
are the same, i.e. αℓ=mfor all ℓ= 1, . . . , L −1. Also, let us denote by Σ=1
nPn
i=1xixT
i∈
Rd×dthe empirical input covariance matrix. Furthermore, we introduce the shorthand-notation
Wk:ℓ=Wk···Wℓfork > ℓ andk < ℓ ,Wk:ℓ=Wk⊤···Wℓ⊤. Then, Singh et al. [2021] show
that the GN matrix can be decomposed as GO=U(Ik⊗Σ)U⊤, where Ikis the identity matrix of
dimension kandU∈Rp×kdis given by:
U= 
W2:L⊗Id. . .Wℓ+1:L⊗Wℓ−1:1. . .Ik⊗WL−1:1⊤.
By rewriting U(IK⊗Σ)U⊤=U(IK⊗Σ1/2)(IK⊗Σ1/2)U⊤, where Σ1/2is the unique positive
semi-definite square root of Σand noting that AB andBA have the same non-zero eigenvalues, we
5have that the non-zero eigenvalues of GOare the same as those of
bGO= (IK⊗Σ1/2)U⊤U(IK⊗Σ1/2), (4)
where U⊤U∈RKd×Kdis equal to
U⊤U=LX
ℓ=1WL:ℓ+1Wℓ+1:L⊗W1:ℓ−1Wℓ−1:1. (5)
Warm-up: the one-hidden layer case. In the case of one-hidden layer network, Fθθθ(x) =WVx ,
we have that bGO=WW⊤⊗Σ+Ik⊗Σ1/2V⊤VΣ1/2.To derive an upper bound of the
condition number, we will lower bound the smallest eigenvalue λmin(bGO)and upper bound the
largest eigenvalue λmax(bGO)separately. Using standard perturbation bounds for matrix eigenvalues
discussed in Appendix A, we obtain the following upper bound on the condition number:
Lemma 1. Letβw=σ2
min(W)/(σ2
min(W) +σ2
min(V)). Then the condition number of GN for the
one-hidden layer network with linear activations with m > max{d, k}is upper bounded by
κ(bGO)≤κ(Σ)·σ2
max(W) +σ2
max(V)
σ2
min(W) +σ2
min(V)=κ(Σ)·(βwκ(W)2+ (1−βw)κ(V)2).(6)
Proof sketch. Choosing A=WW⊤⊗ΣandB=Ik⊗Σ1/2V⊤VΣ1/2andi=j= 1 for the
Weyl’s inequality, i=j=nfor the dual Weyl’s inequality and using the fact that WW⊤⊗Σand
Ik⊗Σ1/2V⊤VΣ1/2are positive semidefinite yield the result.
It is important to point out that the convex combination in Eq. (6)is crucial and a more naive bound
where we take the maximum of both terms is instead too loose, see details in Appendix E. Later on,
we will observe that the general case with Llayers also exhibits a comparable structure, wherein the
significance of the convex combination becomes even more pronounced in deriving practical bounds.
Remark R2 (Role of the data covariance) .Besides the above dependence in terms of the convex
combination of the bounds, we also see how the conditioning of the input data affects the conditioning
of the GN spectra. This is observed in Figure 19, where the condition number of the GN matrix is
calculated on whitened and not whitened data. This observation might also shed light on why data
normalization remains a standard choice in deep learning, often complemented by the normalization
of intermediate layer activations.
4.2 The general L-layer case
Following our examination of the single-hidden layer case, we now broaden our analysis to include L-
layer linear networks. As before, we will first derive an expression of the GN matrix and subsequently
bound the largest and smallest eigenvalue separately to derive a bound on the condition number.
Obtaining the GN matrix involves combining (4) and (5), which yields
bGO= (IK⊗Σ1/2)U⊤U(IK⊗Σ1/2) =LX
l=1(WL:ℓ+1Wℓ+1:L)⊗(Σ1/2W1:ℓ−1Wℓ−1:1Σ1/2).
By repeatedly applying Weyl’s inequalities, we obtain an upper bound on the condition number.
Lemma 2. Assume that m > max{d, k}and that αℓ:=σ2
min(WL:ℓ+1)·σ2
min(W1:ℓ−1)>0∀ℓ=
1, . . . , L . Letγℓ:=αℓPL
i=1αi. Then the condition number of the GN matrix of a L-layer linear network
can be upper-bounded in the following way:
κ(bGO)≤κ(Σ)XL
ℓ=1γℓκ(WL:ℓ+1)2κ(W1:ℓ−1)2≤κ(Σ) max
1≤ℓ≤L
κ(WL:ℓ+1)2κ(W1:ℓ−1)2	
.
As mentioned earlier, we observe the same convex combination structure which, as we will soon see
experimentally, is crucial to obtain a bound that works in practice.
Empirical validation. The empirical results in Figure 2a show that the derived bound seems to
be tight and predictive of the trend of the condition number of GN at initialization. If the width of
the hidden layer is held constant, the condition number grows with a quadratic trend. However, the
condition number can be controlled if the width is scaled proportionally with the depth. This gives
another explanation of why in practice the width of the network layers is scaled proportionally with
the depth to enable faster network training.
62 4 6 8
DepthL0100200κ(/hatwideGO)
Width: 800
2 4 6 8
DepthL
Width: 1400
κ(/hatwideGO) upper bound(a)
2 4 6 8
DepthL5101520κ(/hatwideGO)
Width: 800·(L−1)
2 4 6 8
DepthL
Width: 800 + 2200 ·(L−1)
κ(/hatwideGO)
upper bound(b)
Figure 2: a)Condition number at initialization under Kaiming normal initialization of GN κ(ˆGO)
and first upper bound derived in Lemma 2 and Eq. (7)for whitened MNIST as a function of depth L
for different hidden layer widths mfor a Linear Network over 3 initializations. b)Scaling the width
of the hidden layer proportionally to the depth leads to slower growth of the condition number (left)
or improves the condition number with depth if the scaling factor is chosen sufficiently large (right).
Figure 4: Adding skip connections between each
layer for a general L-layer linear Neural Network.
2 4 6 8
DepthL1041091014κ(/hatwideGO)
Width: 800
2 4 6 8
DepthL
Width: 1400
κ(/hatwideGO) upper bound loose upper bound
Figure 3: Comparison of derived upper bounds
in Lemma 2 for the condition number at ini-
tialization for whitened MNIST over 20 runs.
Note the logarithmic scaling of the y-axis.Furthermore, Figure 3 empirically demonstrates the
importance of bounding the condition number as
a convex combination of the condition number of
the weight matrices, as simply taking the maxi-
mum makes the bound vacuous. This is due to
a ‘self-balancing’ behavior of the individual terms
in Lemma 2, which is further elaborated in Appendix
E. We show that the difference in magnitude be-
tween each condition number is largely driven by
their smallest eigenvalue. Thus at the same time,
they also have a smaller weight in the convex com-
bination, making the overall condition number grow
slower than the maximum bound would predict.
4.3 L-layer linear residual networks
Following the examination of linear networks, our focus will now shift to the analysis of linear
networks where we introduce a skip connection with weight βbetween each layer, as illustrated in
Figure 4. This results in a residual network of the form Fθ(x) = (W(L)+βI)···(W(1)+βI)x.
Given the architecture of the residual network, a substantial portion of the analysis conducted in
Section 4.2 for the L-layer linear neural network can be repurposed.
The key insight lies in the realization that when the skip connection precisely bypasses one layer, it
implies a modification of Eq. (5)toU⊤U=PL
ℓ=1WL:ℓ+1
βWℓ+1:L
β⊗W1:ℓ−1
βWℓ−1:1
β , where we
define Wk:l
β:= (Wk+βI)· · ·(Wl+βI)ifk > l andWk:l
β:= (Wk+βI)⊤· · ·(Wl+βI)⊤if
k < l .Idenotes the rectangular identity matrix of appropriate dimensions. Note, that we can apply
the upper bounds derived in Lemma 2 analogously to arrive at
κ(bGO)≤κ(Σ)LX
ℓ=1γβ
ℓκ2(WL:ℓ+1
β)κ2(W1:ℓ−1
β)≤κ(Σ) max
1≤ℓ≤Lκ2(WL:ℓ+1
β)κ2(W1:ℓ−1
β)(7)
where γβ
ℓ:=αβ
ℓPL
i=1αβ
iand analogously to the L-layer case αβ
ℓ:=σ2
min(WL:ℓ+1
β)·σ2
min(W1:ℓ−1
β)
forℓ= 1, . . . , L .
Let us now analyze how skip connections affect the condition number. Denoting the SVD of Wℓ=
UℓSℓVℓ⊤note that Wℓ+βI=Uℓ(Sℓ+βI)VℓT, and therefore σi(Wℓ+βI) =σi(Wℓ)+βfor all
singular values of Wℓ. Incorporating skip connections results in a spectral shift of each weight matrix
to the positive direction by a magnitude of β. Furthermore, we will assume that the left and right
7singular vectors of the layers in-between coincide, that is Vℓ=Uℓ−1. This assumption is fulfilled
if the initialization values are sufficiently close to zero [Saxe et al., 2013]. Then for each ℓwe have
κ2(WL:ℓ+1
β)κ2(W1:ℓ−1
β) =QL
i=ℓ+1(σmax(Wi) +β)2Qℓ−1
i=1(σmax(Wi) +β)2
QL
i=ℓ+1(σmin(Wi) +β)2Qℓ−1
i=1(σmin(Wi) +β)2
=Y
i̸=ℓσmax(Wi) +β
σmin(Wi) +β2
. (8)
Sinceσmax(Wi)+β
σmin(Wi)+β<σmax(Wi)
σmin(Wi)for all β >0, we can conclude that the second upper bound in Eq.
(7)for residual networks is smaller than the second upper bound for linear networks in Lemma 2.
We verify that this observation also holds for the actual condition number and the tight upper bound
in Eq. (7)as can be seen in Figure 5. The latter shows that adding skip connections improves
the conditioning of the network. Furthermore, Figure 20 in the appendix shows that the condition
number is improved more for larger βbecause this pushes the condition number towards one as
can be seen from Eq. (8). To conclude, the analysis of the condition number provides a rigorous
explanation for the popularity of residual networks compared to fully connected networks. This is in
line with existing empirical results in the literature [He et al., 2016, Li et al., 2018, Liu et al., 2019].
Remark: extension to convolutional layers. Note that the analysis on fully connected layers can
also be extended to convolutional layers in a straightforward way by making use of the fact that the
convolution operation can be reformulated in the form of a matrix-vector product using Toeplitz
matrices, as discussed for instance in Singh et al. [2023]. In this case, we can apply the same analysis
as for fully connected networks. See Appendix B for more details.
5 Extension to non-linear activations
After the analysis of networks with a linear activation in the previous section, we will extend our
results to non-linear activation functions σthat satisfy σ(z) =σ′(z)zfor one-hidden layer networks
Fθ(x) =Wσ(Vx). Specifically, this extension includes all piece-wise linear activation functions
such as ReLU or Leaky ReLU. For this purpose, we first rewrite the network output as a super-position
of unit-networks Fθi(x)over the number of hidden neurons mas
Fθ(x) =mX
i=1Fθi(x) =mX
i=1W•,iσ(Vi,•x), (9)
where W•,idenotes the i-th column of WandVi,•denotes the i-th row of V. Using the following
lemma, we can then derive an expression for the Gauss-Newton matrix.
Lemma 3. (Lemma 25 in Singh et al. [2021]) Let Fθi(x) =W•,iσ(Vi,•x)be a unit-network
corresponding to i-th neuron, with the non-linearity σsuch that σ(z) =σ′(z)z. LetX∈Rd×n
denote the data matrix. Further, let Λi∈Rn×nbe defined as (Λi)jj=σ′(Vi,•x)j,forj= 1, . . . , n
and zero elsewhere. Then the Jacobian matrix ∇θFθi(X)is given (in transposed form) by:
∇θFθi(X) = 
0 XΛi⊗W⊤
•iVi•XΛi⊗Ik0⊤.
The Jacobian matrix of the full network is simply the sum over all units ∇θFθ(X) =Pm
i=1∇θFθi(X). Now that we have an expression for the Jacobian matrix, we can compute
the GN matrix as GO=∇Fθ(X)∇Fθ(X)⊤. Note that the non-zero eigenvalue of GOand
bGO:=∇Fθ(X)⊤∇Fθ(X)∈Rkn×knare the same, for which we have
bGO=mX
i=1∇Fθi(X)⊤∇Fθi(X) =mX
i=1bGi
O
withbGi
O:=ΛiX⊤XΛi⊗W•iW⊤
•i+ΛiX⊤V⊤
i•Vi•XΛi⊗Ik∈Rkn×kn, since the mixed
terms∇Fθi(X)⊤∇Fθj(X)withi̸=jare zero due to the block structure of the Jacobian of the
unit-networks. Using the mixed product property of the Kronecker product we obtain
bGO=Xm
i=1ΛiX⊤XΛi⊗W•iW⊤
•i+Xm
i=1ΛiX⊤V⊤
i•Vi•XΛi⊗Ik. (10)
82 4 6 8
DepthL0100200κ(/hatwideGO)
Width: 800
2 4 6 8
DepthL
Width: 1400
network
ResNet
FCNNtype
κ(/hatwideGO)
upper bound
2 4 6 8
DepthL0100κ(/hatwideGO)
Width: 1500
2 4 6 8
DepthL
Width: 3400
network
ResNet
FCNNtype
κ(/hatwideGO)
upper boundFigure 5: Comparison of condition number of GN κ(ˆGO)between Linear Network and Residual
Network with β= 1for whitened MNIST (left) and whitened Cifar-10 (right) at initialization using
Kaiming normal initialization over three seeds. The upper bounds refer to the first upper bound
in Lemma 2 and Eq. (7), respectively.
To improve readability, we will write Γ:=Pm
i=1ΛiX⊤V⊤
i•Vi•XΛihereafter.
Leaky ReLU activation. Let’s now consider the case where the non-linear activation σis
LeakyReLU (x) = max {αx, x}for some constant α∈[0,1). Typically αis chosen to be close to
zero, e.g. α= 0.01. Again, the condition number κ(bGO)can be upper bounded by bounding the
extreme eigenvalues independently. By observing that all terms in Eq. (10) are symmetric, we can
again apply the Weyl’s and dual Weyl’s inequality. However to get a lower bound larger than zero for
λmin(bGO)a different approach is needed as W•,iW⊤
•,iis rank-deficient and the same steps to bound
the smallest eigenvalue would lead to a vacuous value of zero. Instead, we will use the following obser-
vation that we can bound the extreme eigenvalues of the sum of a Kronecker product of PSD matrices
Ai,Bifori= 1, . . . , m byλmin(Pm
i=1(Ai⊗Bi))≥min1≤j≤mλmin(Aj)·λmin(Pm
i=1Bi).
2000 4000 6000 8000
Widthm103104κ(/hatwideGO)
 κ(/hatwideGO)
upper bound
loose upper bound
Figure 6: Condition number of a
one-hidden layer Leaky ReLU network
with α= 0.01and upper bounds for
whitened MNIST over n= 500 data
points. The upper bound refers to Eq.
(11). Note that the y-axis is log scaled.By first using Weyl’s inequalities and subsequently ap-
plying the above bound with Ai=ΛiX⊤XΛiand
Bi=W•,iW⊤
•,iwe achieve the following bound for
the condition number for κ(bGO).
Lemma 4. Consider an one-hidden layer network with
a Leaky-ReLU activation with negative slope α∈[0,1].
Then the condition number of the GN matrix defined in
Equation (10) can be bounded as:
κ(bGO)≤σ2
max(X)σ2
max(W) +λmax(Γ)
α2σ2
min(X)σ2
min(W) +λmin(Γ)(11)
The proof can be found in Appendix H. We further
empirically validate the tightness of the upper bounds on a
subset of the MNIST dataset ( n= 500 ), where we chose
α= 0.01, which is the default value in Pytorch [Paszke
et al., 2019]. As can be seen in Figure 6, contrary to the
linear setting the condition number increases with width. The upper bound also becomes tighter
and more predictive with increasing width.
Comparison to linear network. By running the same setup for varying values of α∈[0,1]we
can interpolate between ReLU ( α= 0) and linear activation ( α= 1) to see how the introduction of
the non-linearity affects the conditioning. In Figure 23, which had to be deferred to the appendix
due to space constraints, we can see how reducing the value of αseems to consistently improve the
conditioning of the GN matrix for the one-hidden layer case.
96 Conditioning under batch normalization
80 100 120 140
Widthm4006008001000κ(/hatwideGO)
batchnorm
BN
no BN
Figure 7: Comparison of conditioning of the
GN matrix for a one-hidden layer linear net-
work with and without Batch normalization
on downsampled Cifar-10 data ( d= 64, n=
1000 ) at initialization over 5 runs.Based on the observation that conditioning of the
input data affects the conditioning of the GN spec-
trum, we also tested whether Batch normalization
(BN) [Ioffe and Szegedy, 2015], which is a very
commonly used normalization scheme, has a similar
effect on the condition number. For this, the condi-
tion number of the GN matrix was calculated for a
one-hidden layer linear network with and without a
Batch normalization layer. The experiment was run
on a downsampled and subsampled version of Cifar-
10, which was converted to grayscale with d= 64
andn= 1000 . The data was not whitened to see the
effect of BN more strongly. As can be seen in Fig-
ure 7, we indeed observe a clear improvement of the
condition number when Batch normalization layers
are added. Also, the trend of improved conditioning
with increasing width remains after adding BN.
7 Discussion and conclusion
Summary. In this work, we derived new analytical bounds for the condition number of neural
networks, showcasing the following key findings: a)The conditioning of the input data has a
nearly proportional impact on the conditioning of the GN spectra, underscoring the significance of
data normalization. Empirical evidence further demonstrates that Batch Normalization similarly
enhances the condition number. b)For linear networks, we showed that the condition number grows
quadratically with depth for fixed hidden width. Also, widening hidden layers improves conditioning,
and scaling the hidden width proportionally with depth can compensate for the growth. c)We showed
how adding residual connections improves the condition number, which also explains how they enable
the training of very deep networks. d)Preliminary experiments suggest that the ReLU activation
seems to improve the conditioning compared to linear networks in the one-hidden layer case.
Interesting use cases of our results. Through our analysis, we highlighted that the condition number
as a tool from classical optimization is also an attractive option to better understand challenges in
neural network training with gradient-based methods. Especially, knowing how different architectural
choices will affect the optimization landscape provides a more principled way to design the network
architecture for practitioners, for instance how to scale the width in relation to the depth of the
network. The paper also gives a justification of why pruned networks are more difficult to train as
they have worse conditioning. Although this is not our focus, it is possible that our analysis could
inspire better techniques for pruning neural networks.
Limitations and future work. We made a first step toward understanding the impact of different
architectural choices on the conditioning of the optimization landscape. However, there are still
many design choices, that have not been covered yet, such as an extension to non-linear networks
for arbitrary layers, other architectures, such as transformers, and analytic bounds for normalization
schemes, such as batch or layer normalization, which we will leave to future work. Another limitation
of our current work is that the derived upper bounds are agnostic to the training dynamics. Therefore,
they cannot distinguish the difference between random initializations and solutions of deep learning
models after convergence. Incorporating training dynamics into the upper bounds would allow us to
characterize solutions found by different architectures, which is left for future work. Another future
direction is to extend the analysis to the Generalized Gauss-Newton matrix, which is particularly
relevant for training with cross-entropy loss.
Acknowledgements
Aurelien Lucchi acknowledges the financial support of the Swiss National Foundation, SNF grant No
223031. Sidak Pal Singh would like to acknowledge the financial support of Max Planck ETH Center
for Learning Systems.
10References
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature, 2020.
Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimiza-
tion. In International Conference on Machine Learning , pages 1842–1850. PMLR, 2018.
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic
second-order optimizer for language model pre-training, 2023.
Yizhang Lou, Chris E Mingard, and Soufiane Hayou. Feature learning and signal propagation in deep
neural networks. In International Conference on Machine Learning , pages 14248–14282. PMLR,
2022.
Utku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difficulty of training sparse neural
networks. arXiv preprint arXiv:1906.10732 , 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning , pages 448–456.
pmlr, 2015.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. Advances in neural information processing systems , 29, 2016.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV) , pages 3–19, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957 , 2018.
Guoliang Kang, Jun Li, and Dacheng Tao. Shakeout: A new regularized deep neural network training
scheme. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 30, 2016.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International conference on machine learning , pages 1058–1066.
PMLR, 2013.
Sidak Pal Singh, Gregor Bachmann, and Thomas Hofmann. Analytic insights into structure and rank
of neural network hessian maps. Advances in Neural Information Processing Systems , 34, 2021.
Vladimir Vapnik. Principles of risk minimization for learning theory. Advances in neural information
processing systems , 4, 1991.
Nicol N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural Computation , 14:1723–1738, 2002.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the
hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454 , 2017.
James Martens. New insights and perspectives on the natural gradient method. Journal of Machine
Learning Research , 21(146):1–76, 2020.
Aleksandar Botev. The Gauss-Newton matrix for Deep Learning models and its applications . PhD
thesis, UCL (University College London), 2020.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems , 32, 2019.
11Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural computation , 9(1):1–42, 1997.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836 , 2016.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data, 2017.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. Journal of Statistical Mechanics: Theory and Experiment , 2019(12):124018,
2019.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for
deep nets. In International Conference on Machine Learning , pages 1019–1028. PMLR, 2017.
Diego Granziol. Flatness is a false friend, 2021. URL https://openreview.net/forum?id=
I6-3mg29P6y .
Maksym Andriushchenko, Francesco Croce, Maximilian Müller, Matthias Hein, and Nicolas Flam-
marion. A modern look at the relationship between sharpness and generalization. arXiv preprint
arXiv:2302.07011 , 2023.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization, 2021.
Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based analysis
of large batch training and robustness to adversaries. Advances in Neural Information Processing
Systems , 31, 2018.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218 ,
2020.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density, 2019.
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks
through the lens of the hessian. In 2020 IEEE international conference on big data (Big data) ,
pages 581–590. IEEE, 2020.
Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on
neural networks typically occurs at the edge of stability. arXiv preprint arXiv:2103.00065 , 2021.
Daniel Beaglehole, Peter Súkeník, Marco Mondelli, and Mikhail Belkin. Average gradient outer
product as a mechanism for deep neural collapse. arXiv preprint arXiv:2402.13728 , 2024.
Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael W. Mahoney.
Adahessian: An adaptive second order optimizer for machine learning, 2021.
Wu Lin, Felix Dangel, Runa Eschenhagen, Kirill Neklyudov, Agustinus Kristiadi, Richard E Turner,
and Alireza Makhzani. Structured inverse-free natural gradient: Memory-efficient & numerically-
stable kfac for large neural nets. arXiv preprint arXiv:2312.05705 , 2023.
Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information
processing systems , 2, 1989.
Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network
pruning. In IEEE international conference on neural networks , pages 293–299. IEEE, 1993.
Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient second-order approximation for neural
network compression. Advances in Neural Information Processing Systems , 33:18098–18109,
2020.
12Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Hawq: Hessian aware
quantization of neural networks with mixed-precision, 2019.
Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for
accurate post-training quantization and pruning, 2023.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions, 2020.
Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit
Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamil ˙e Lukoši ¯ut˙e, Karina Nguyen,
Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying large
language model generalization with influence functions, 2023.
Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random matrix
theory. In International conference on machine learning , pages 2798–2806. PMLR, 2017.
Arthur Jacot, Franck Gabriel, and Clément Hongler. The asymptotic spectrum of the hessian of dnn
throughout training. arXiv preprint arXiv:1910.02875 , 2019.
Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singularity
and beyond. arXiv preprint arXiv:1611.07476 , 2016.
Sidak Pal Singh, Thomas Hofmann, and Bernhard Schölkopf. The hessian perspective into the nature
of convolutional neural networks. arXiv preprint arXiv:2305.09088 , 2023.
Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, and Rong Ge. Dissecting hessian: Understanding
common structure of hessian in neural networks. arXiv preprint arXiv:2010.04261 , 2020.
Zhenyu Liao and Michael W Mahoney. Hessian eigenspectra of more realistic nonlinear models.
Advances in Neural Information Processing Systems , 34:20104–20117, 2021.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120 , 2013.
Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in
optimizing deep linear networks. In International Conference on Learning Representations , 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences ,
116(32):15849–15854, July 2019. ISSN 1091-6490. doi: 10.1073/pnas.1903070116.
Sidak Pal Singh, Aurelien Lucchi, Thomas Hofmann, and Bernhard Schölkopf. Phenomenology
of double descent in finite-width neural networks. In International Conference on Learning
Representations , 2022.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems , 31, 2018.
Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent
kernel perspective. Journal of Computational Physics , 449:110768, 2022.
Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint
arXiv:2006.14548 , 2020.
Quynh Nguyen, Marco Mondelli, and Guido Montufar. Tight bounds on the smallest eigenvalue of
the neural tangent kernel for deep relu networks, 2022.
Chaoyue Liu and Like Hui. Relu soothes the ntk condition number and accelerates optimization for
wide neural networks. arXiv preprint arXiv:2305.08813 , 2023.
Yi Ren and Donald Goldfarb. Efficient subsampled gauss-newton and natural gradient methods for
training neural networks. arXiv preprint arXiv:1906.02353 , 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
13Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. Advances in neural information processing systems , 31, 2018.
Tianyi Liu, Minshuo Chen, Mo Zhou, Simon S Du, Enlu Zhou, and Tuo Zhao. Towards understanding
the importance of shortcut connections in residual networks. Advances in neural information
processing systems , 32, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems , 32,
2019.
Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgle-
ichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen ,
71(4):441–479, 1912.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027 , 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision , pages 1026–1034, 2015.
Jack W Silverstein. The smallest eigenvalue of a large dimensional wishart matrix. The Annals of
Probability , pages 1364–1368, 1985.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
14A Standard pertubation bounds for matrix eigenvalues
A.1 Weyl’s inequality and dual Weyl’s inequality
In this section, we first briefly review the Weyl’s inequality and its dual form. We then demonstrate
how these inequalities can be used to bound the extreme eigenvalues and thus the condition number
of the sum of two Hermitian matrices A,Bby the extreme eigenvalues of AandB.
Lemma 5. Weyl [1912] Let A,B∈Rn×nbe two Hermitian matrices, and let λ1≥. . .≥λn
denote the ordered eigenvalues, then the Weyl’s inequality and dual Weyl’s inequality state that
λi+j−1(A+B)≤λi(A) +λj(B), i, j≥1, i+j−1≤n (Weyl)
λi+j−n(A+B)≥λi(A) +λj(B), i, j≥1, i+j−n≤n. (Dual Weyl)
In the following, we derive an upper and lower bound of the condition number of a symmetric matrix
A, for which we have
κ(A) =|λmax(A)|
|λmin(A)|.
Recall that given a Hermitian n×nmatrix A, we can diagonalize it by the spectral theorem. We
obtain the following sequence of real eigenvalues
λ1(A)≥λ2(A)≥. . .≥λn(A).
We will make use of the Weyl’s inequality and the dual Weyl’s inequality to bound the eigenvalues of
the sum of two Hermitian matrices A+B.
From Lemma 5, we obtain the following inequalities: From Weyl’s inequality we have
λ1(A+B)≤λ1(A) +λ1(B)
λn(A+B)≤λn−j(A) +λ1+j(B),for0≤j≤n−1.
From the dual Weyl’s inequality we have
λ1(A+B)≥λn−j(A) +λ1+j(B),for0≤j≤n−1
λn(A+B)≥λn(A) +λn(B).
Hence, we can find a lower and upper bound for the condition number of A+B
|max 0≤j≤n−1(λn−j(A) +λ1+j(B))|
|min0≤j≤n−1(λn−j(A) +λ1+j(B))|≤κ(A+B)≤|λmax(A) +λmax(B)|
|λmin(A) +λmin(B)|. (12)
A.2 Further bounds for extreme eigenvalues for positive semidefinite matrices
For square matrices A∈Rp×p,B∈Rq×q, denote by λ1, . . . , λ pthe eigenvalues of Aand by
µ1, . . . , µ qthe eigenvalues of Blisted by multiplicity. Then the eigenspectrum of A⊗B∈Rpq×pq
consists of the eigenvalues
λiµji= 1, . . . , p, j = 1, . . . , q.
Note that if A,Bare additionally positive semidefinite (PSD), we have the following equality for the
largest and smallest eigenvalues of A⊗B
λmax(A⊗B) =λmax(A)λmax(B) (13)
λmin(A⊗B) =λmin(A)λmin(B). (14)
Furthermore, it follows from the sub-multiplicativity of the matrix norm that for square, PSD matrices
A,Bof same dimensions it holds that
λmax(AB)≤λmax(A)·λmax(B)
λmin(AB)≥λmin(A)·λmin(B).
15B Extension of analysis to convolutional layers
The analysis on fully connected layers can be extended to convolutional layers in a straight forward
way, by using the fact that the convolution operation can be reformulated in the form of a matrix-
vector product using Toeplitz matrices, as discussed for instance in Singh et al. [2023]. This would
mean that we can apply the same analysis as for fully connected networks. For completeness we will
discuss how below how the convolution operation can be rewritten as a matrix-vector product. For
further details, the readers is referred to Singh et al. [2023].
Consider a convolution operation W∗xof an input x∈Rdwithmfilters of size k≤d, which are
organized in the matrix W∈Rm×k. For simplicity a stride of 1and zero padding is assumed. Let
zj:j+k−1∈Rkdenote the vector formed by considering the indices jtoj+k−1of the original
vector z∈Rd. In this case, the convolution operation can be written as
W∗x=
⟨W1•,x1:k⟩. . .⟨W1•,xd−k+1:d⟩
......
⟨Wm•,x1:k⟩. . .⟨Wm•,xd−k+1:d⟩
∈Rm×(d−k+1),
where Wi•denotes the i-th row of W.
Let us further introduce Toeplitz matrices,
TWi•	m
i=1, for each filter with TWi•:=toep(Wi•, d)∈
R(d−k+1)×dsuch that,
TWi•=
Wi1. . . Wik 0 . . . 0
0Wi1. . . Wik0...
... 0.........0
0 . . . 0Wi1. . .Wik
.
Finally, by stacking the Toeplitz matrices in a row-wise fashion, that is
TW:=
TW1•
. . .
TWm•
∈Rm(d−k+1)×d,
we see that the matrix multiplication of TWwith an input xgives the same result as vectorizing the
convolution operation row-wise, i.e.
vecr(W∗x) =TWx.
For a linear network with Lhidden layers, each of which is a convolutional kernel, the network
function can be formally represented as
Fθ(x) =W(L+1)∗ W(L)∗. . .∗ W(1)∗x, (15)
where the parameters of each hidden layer ℓare denoted by W(ℓ)∈Rmℓ×mℓ−1×kl.mℓis the number
of output channels, mℓ−1the number of input channels, and kℓthe kernel size. As we assumed xto
be one-dimensional, we have m0= 1.
Similar to a single convolutional operation, the output of a single convolutional layer can be expressed
via Toeplitz matrices. The Toeplitz matrix associated with the ℓ-th convolutional layer W(ℓ)can be
expressed by
T(ℓ):=
TW(ℓ)
(1,1)•. . . TW(ℓ)
(1,mℓ−1)•
... . . .
TW(ℓ)
(mℓ,1)•. . .TW(ℓ)
(mℓ,mℓ−1)•
∈Rmℓdℓ×mℓ−1dℓ−1
where W(ℓ)
(i,j)•∈Rkℓrefers to the (i, j)-th fibre of W(ℓ)andTW(ℓ)
(i,j)•:=toep(W(ℓ)
(i,j)•, dℓ−1)∈
Rdℓ×dℓ−1to itsassociated Toeplitz matrix. Now, the network function can be equivalently written as
Fθ(x) =T(L+1)T(L). . .T(1)x. (16)
16Figure 8: Experiments on the condition number of the Gauss Newton matrix at initialization for a
linear two-layer CNN at initialization on a random subsample of n=1000 of MNIST, whitened, with
varying kernel size and number of filters. We can see a trend where the number of filters increases
the condition number (in analogy to depth in MLPs) and the kernel size improves conditioning (in
analogy to width in MLPs).
We further provide experiments on linear CNNs in Figure 8 at initialization showcasing that we
can observe similar trends where the number of filters increases the condition number of the Gauss
Newton matrix (in analogy to depth in MLPs) and the kernel size improves conditioning (in analogy
to width in MLPs).
C Sensitivity of condition number on choice of smallest eigenvalue
This work considers the pseudo condition number of some matrix A∈Rn×n, which is the ratio
of the largest eigenvalue over the smallest non-zero eigenvalueλmax(A)
λnz,min(A). IfAis of full rank, the
pseudo condition number is equivalent to the condition number. However, if Ais rank deficient, it has
to be typically estimated numerically if the rank is not known analytically. Based on the numerical
estimation of the matrix rank, the smallest eigenvalue is chosen accordingly and the pseudo condition
number is calculated.
In this section we evaluate the sensitivity of choice of the smallest eigenvalue on the resulting pseudo
condition number. This is illustrated in the example of a 1-hidden layer network with ReLU activation
function and a hidden width of m= 20 at initialization and after 100 epochs of training. The details
of this experimental setup are further specified in Appendix I.2.
In Figure 9 we can see that depending on the eigenvalue distribution, the condition number can be
quite robust (Epoch=0), but also very sensitive (Epoch=100) to the numerical matrix rank estimation.
0 2000 4000 6000 8000 10000
i10−2410−1610−8100λiOrdered spectrum by magnitude
Epoch=0
Epoch=100
λnz,min
rk(ˆG)−10rk(ˆG)−5 rk(ˆG) rk(ˆG) + 5 rk(ˆG) + 10101010121014Condition number κSensitivity of κon matrix rank estimation rk( ˆG)
Epoch=0
Epoch=100
λnz,min
Figure 9: The spectrum of the GN matrix ordered by magnitude at initialization and after 100 epochs.
The star marks the smallest non-zero eigenvalue, which is determined by the computed matrix rank
(left). Sensitivity of condition number as a function of the matrix rank (right).
17D Evolution of the condition number during training
We conducted an experiment to track the condition number and evaluate how tight the upper bound is
throughout training. For this we trained a 3-layer linear network with a hidden width of m= 500
with three different seeds for 5000 epochs with SGD with a mini-batch size of 256 and a constant
learning rate of 0.2 on a subset of Cifar-10 ( n= 1000 ) [Krizhevsky et al., 2009], which has been
downsampled to 3×8×8images and whitened. The network was trained on a single NVIDIA
GeForce RTX 3090 GPU and took around 5 minutes per run. The condition number was computed
via the explicit formula in Lemma 2 on CPU and took around 4 minutes. The condition number
and the upper bound are shown in Figure 10 together with the training loss. We make the following
two observations: 1. The condition number takes values in the range of 6 to 12 throughout training,
which corresponds to a maximal change of around 50% of the condition number at initialization.
This indicates that the condition number at initialization can be indicative of how the conditioning
is along the optimization trajectory. 2. The upper bound remains tight throughout training, which
highlights that it can provide reliable information about the condition number of the Gauss-Newton
matrix throughout training.
0 2000 4000
epoch0.070.080.090.10Training loss
0 2000 4000
epoch81012Condition number κ
type
κ(/hatwideGO)
upper bound
Figure 10: Training loss (left) and corresponding evolution of the condition number throughout
training for three seeds. The shaded area in the figures corresponds to one standard deviation from
the mean.
18E Understanding the difference between the convex combination bound and
the maximum Bound
In Lemma 2 we have seen that the upper bound of the condition number can be expressed in terms
of a convex combination of the condition number of the weight matrices. In this section, we will
elucidate the ‘self-balancing’ behavior of the individual terms in Lemma 2. For completeness, let us
restate Lemma 2 here again:
Lemma 2. Assume that m > max{d, k}and that αℓ:=σ2
min(WL:ℓ+1)·σ2
min(W1:ℓ−1)>0∀ℓ=
1, . . . , L . Letγℓ:=αℓPL
i=1αi. Then the condition number of the GN matrix of a L-layer linear network
can be upper-bounded in the following way:
κ(bGO)≤κ(Σ)XL
ℓ=1γℓκ(WL:ℓ+1)2κ(W1:ℓ−1)2≤κ(Σ) max
1≤ℓ≤L
κ(WL:ℓ+1)2κ(W1:ℓ−1)2	
.
By looking at the individual values of κ2(WL:ℓ+1)andσ2
min(WL:ℓ+1)and similarly κ2(W1:ℓ−1)
andσ2
min(W1:ℓ−1)overℓ= 1, . . . , L for networks of increasing depth L, we can see the ‘self-
balancing’ behavior in Figure 11 for downsampled MNIST ( d= 196 ) over 10 runs and m= 300
neurons per hidden layer. Note that this behavior is also consistent for different widths m, input
dimension d, and output dimension k. By first looking at the first row, we observe that κ2(W1:ℓ−1)
(top left) dominates the product of each term in Lemma 2 and follows an exponential growth rule
inℓ(note the log scale of the y-axis). By looking at the second row, we see that at the same time,
the smallest singular value of κ2(W1:ℓ−1)(bottom left) is decreasing exponentially, leading to the
‘self-balancing’ behavior of the condition number. The same observation also holds for WL:ℓ+1, but
since the number of weight matrices decreases with ℓthis leads to a mirrored plot. The ‘self-balancing’
behavior can be seen below in Figure 12 for ease of reading, where the individual terms of Lemma 2
are plotted with and without the weighting factor γℓ.
0 2 4 6 8
/lscript101103105κ2(W1:/lscript−1)
 L
1.0
2.0
4.0
6.0
8.0
2 4 6 8
/lscript1002×1003×1004×1006×100κ2(WL:/lscript+1)
L
1.0
2.0
4.0
6.0
8.0
2 4 6 8
/lscript10−510−410−310−210−1100σ2
min(W1:/lscript−1)
L
1.0
2.0
4.0
6.0
8.0
2 4 6 8
/lscript100
4×10−16×10−1σ2
min(WL:/lscript+1)
L
1.0
2.0
4.0
6.0
8.0
Figure 11: Condition number and smallest eigenvalue of W1:ℓ−1(first column) and WL:ℓ+1(second
column) for downsampled MNIST ( d= 196 ) for three seeds. Shaded area corresponds to one
standard deviation. Note that the y-axis is log-scaled at the different limits for each subplot.
192 4 6 8
/lscript101103105κ2(WL:/lscript+1)κ2(W1:/lscript−1)
L
1.0
2.0
4.0
6.0
8.0
0 2 4 6 8
/lscript100101γ/lscriptκ2(WL:/lscript+1)κ2(W1:/lscript−1)
L
1.0
2.0
4.0
6.0
8.0Figure 12: Condition number of weight matrices with and without weighting factor γℓfor downsam-
pled MNIST ( d= 196 ) for three seeds, which highlights the ‘self-balancing’ effect of the weight
condition number. Note that the y-axis is log-scaled and both plots have different limits. Shaded area
corresponds to one standard deviation.
F Conditioning of functional Hessian for one-hidden layer network
Consider the case of a one-hidden layer linear network Fθθθ(x) =WVx , with x∈Rd,W∈Rk×m,
andV∈Rm×dand some corresponding label y∈Rk. Under the assumption of the MSE loss, the
functional Hessian is then given as:
HF= 0km×km Ω⊗Im
Ω⊤⊗Im0dm×dm,!
(17)
where Ω=E[δx,yx⊤]∈Rk×dis the residual-input covariance matrix, with δx,y:= F θθθ(x)−y
being the residual. Since we would like to estimate the spectrum of HF, let us suppose that it has an
eigenvector v=
a⊗b
c⊗d
. Assuming that λis the corresponding eigenvalue, we have the following
eigenproblem:
HF·v=Ω c⊗d
Ω⊤a⊗b
=λ
a⊗b
c⊗d
Let’s start with the guess that b=d, then we obtain the following equations:
λa=Ω c and λc=Ω⊤a.
Solving this gives:
Ω⊤Ω c=λ2candΩΩ⊤a=λ2a.
Hence, aandcare simply the eigenvectors of ΩΩ⊤andΩ⊤Ωrespectively. Both these matrices
have the same non-zero eigenvalues, and the resulting eigenvalues for the functional Hessian are
nothing but the square root of corresponding eigenvalues of the matrix Ω⊤Ω— with both positive
and negative signs. About b=d: We still have a degree of freedom left in choosing them. Any
m-orthogonal vectors with unit norm would satisfy the above equation and, for simplicity, we can
just pick the canonical basis vectors in Rm.
Teacher-Student setting Let us now further assume a Teacher-student setting, i.e. y=Zxfor some
Z∈Rk×d. In this case note that Ω=E[(WVx −y)x⊤] = (WV−Z)E[xx⊤] = (WV−Z)Σ.
Now recall the definition of the condition number κ(HF) =∥HF∥·∥H−1
F∥and using the observation
20from above we have that
∥HF∥=σmax(HF) =σmax((WV−Z)Σ)
≤σmax(WV−Z)·λmax(Σ)
∥H−1
F∥=σmin(HF) =σmin((WV−Z)Σ)
≥σmin(WV−Z)·λmin(Σ).
Therefore we have
κ(HF)≤κ(WV−Z)·κ(Σ). (18)
Similar to bounding the condition number of the GN matrix, we can also clearly see a dependency on
the conditioning of the input covariance.
G Concrete bounds at initialization for one-hidden layer linear network
G.1 Non-asymptotic bound
Based on the upper bound in Eq. (6)derived for the one-hidden layer linear network, we can look at
how this bound behaves concretely at initialization. If we assume the entries of WandVto be i.i.d.
Gaussian entries, then WW⊤andV⊤Vwill be Wishart matrices, for which asymptotically the
distribution of the smallest and largest eigenvalues are known. Also there are known non-asymptotic
bounds to bound the maximum and minimum singular value of a Gaussian matrix:
Lemma 6 (Corollary 5.35 in Vershynin [2010]) .LetAbe an N×nmatrix whose entries are
independent standard normal random variables N(0,1). Then for every t≥0, with probability at
least1−2 exp(−t2
2)one has
√
N−√n−t≤σmin(A)≤σmax(A)≤√
N+√n+t. (19)
Using the Lemma above and a union bound, we can also derive a non-asymptotic bound for the
condition number of the GN matrix under the assumption that we have a wide layer m > max( d, k):
Lemma 7. Assume that m≥max( d, k)and that the entries of V∈Rm×d,W∈Rk×mare
initialized Gaussian i.i.d. with Vij∼ N(0, σ2
v)andWij∼ N(0, σ2
w), then with probability at least
1−8 exp(−t2
2)the condition number of the GN matrix κ(bGO)is upper bounded by
κ(bGO) =λmax(bGO)
λmin(bGO)≤κ(Σ)(σ2
w(√m+√
k+t)2+σ2
v(√m+√
d+t)2)
(σ2w(√m−√
k−t)2+σ2v(√m−√
d−t)2).
Proof. Assume that V∈Rm×d,W∈Rk×mare initialized Gaussian i.i.d. with Vij∼ N(0, σ2
v)
andWij∼ N(0, σ2
w), then the bounds in Lemma 6 are scaled with σvandσwrespectively:
σv(√m−√
d−t)≤σmin(V)≤σmax(V)≤σv(√m+√
d+t)
σw(√m−√
k−t)≤σmin(W)≤σmax(W)≤σw(√m+√
k+t).
Further noting that
λmin(V⊤V) =σ2
min(V), λ max(V⊤V) =σ2
max(V)
λmin(WW⊤) =σ2
min(W)λmax(WW⊤) =σ2
max(W),
we get the following non-asymptotic upper respectively lower bounds for the extreme eigenvalues
with probability at least 1−2 exp(−t2
2)each:
λmin(V⊤V)≥σ2
v(√m−√
d−t)2, λ max(V⊤V)≤σ2
v(√m+√
d+t)2
λmin(WW⊤)≥σ2
w(√m−√
k−t)2, λ max(WW⊤)≤σ2
w(√m+√
k+t)2
Using a union bound, we get the upper bound for the condition number of the GN matrix.
21Based on the bound derived above, we can understand how the condition number behaves under
initialization schemes used in practice. Consider the setting of a wide hidden layer m≫max{d, k},
in which the input dimension is much larger than the output dimension, i.e. d≫k. This is for
instance the case for image classification, where the number of image pixels is typically much larger
than the number of image classes. A popular initialization scheme is the one proposed in He et al.
[2015], commonly known as the Kaiming/He initialization, which corresponds to σ2
w=1
m, σ2
v=1
d
in our case. Hence, under the assumption of m≫d, we will have σ2
v≫σ2
wand therefore
κ(bGO)≤κ(Σ)(σ2
w(√m+√
k+t)2+σ2
v(√m+√
d+t)2)
(σ2w(√m−√
k−t)2+σ2v(√m−√
d−t)2)(20)
≈κ(Σ)(√m+√
d+t)2
(√m−√
d−t)2≈κ(Σ). (21)
G.2 Asymptotic bound
To complement the analysis, we can also derive an asymptotic bound for the condition number when
d, k, m go to infinity together.
For this, we will use a result from Silverstein [1985], in which it is shown that the smallest eigenvalue
of the matrix Ms=1
sUsU⊤
s, where Us∈Rn×scomposed of i.i.d. N(0,1)random variables,
converges to (1−√y)2ass→ ∞ , whilen
s→y∈(0,1)ass→ ∞ . This corresponds to the case
in which the hidden layer is larger than the input and output dimensions, that is m≥max( d, k).
Similarly it can be shown that λmax→(1 +√y)2for all y >0fors→ ∞ .
Using this result we can bound the extreme eigenvalues of WW⊤andV⊤Vat initialization.
Assuming that the entries of WandVare entry-wise independent normally distributed Wij∼
N(0, σ2
w),Vij∼ N(0, σ2
v)(for instance σ2
w=1
m, σ2
v=1
dfor Kaiming-initialization) we get for
d, k, m → ∞ :
λmin(WW⊤) =σ2
w·m·λmin1
mUmU⊤
m
k,m→∞→ σ2
w·m· 
1−r
k
m!2
λmax(WW⊤) =σ2
w·m·λmin1
mUmU⊤
m
k,m→∞→ σ2
w·m· 
1 +r
k
m!2
λmin(VV⊤) =σ2
v·m·λmin1
mUmU⊤
m
d,m→∞→ σ2
v·m· 
1−r
d
m!2
λmax(VV⊤) =σ2
v·m·λmax1
mUmU⊤
m
d,m→∞→ σ2
v·m· 
1 +r
d
m!2
.
Thus, in the asymptotic case in which the hidden layer is wide m≥max{d, k}, we can derive the
following upper bound for the condition number:
κ(bGO)≤Cd,k,m→∞→ κ(Σ)·σ2
w·
1 +q
k
m2
+σ2
v·
1 +q
d
m2
σ2w·
1−q
k
m2
+σ2v·
1−q
d
m2(22)
Again, if m≫max{d, k}we also have in the asymptotic limit that κ(bGO)≈κ(Σ).
22H Proofs
Lemma 1. Letβw=σ2
min(W)/(σ2
min(W) +σ2
min(V)). Then the condition number of GN for the
one-hidden layer network with linear activations with m > max{d, k}is upper bounded by
κ(bGO)≤κ(Σ)·σ2
max(W) +σ2
max(V)
σ2
min(W) +σ2
min(V)=κ(Σ)·(βwκ(W)2+ (1−βw)κ(V)2).(6)
Proof. Note that we have by the Weyl’s in dual Weyl’s inequality that
λmax(bGO) =λmax(WW⊤⊗Σ+Ik⊗Σ1/2V⊤VΣ1/2)≤λmax(WW⊤⊗Σ) +λmax(Ik⊗Σ1/2V⊤VΣ1/2)
λmin(bGO) =λmin(WW⊤⊗Σ+Ik⊗Σ1/2V⊤VΣ1/2)≥λmin(WW⊤⊗Σ) +λmin(Ik⊗Σ1/2V⊤VΣ1/2).
Then note that WW⊤⊗ΣandIk⊗Σ1/2V⊤VΣ1/2are positive semidefinite and using the equality
for the extreme eigenvalues of Kronecker products in Eq. (13), we have the bound
λmax(WW⊤⊗Σ) +λmax(Ik⊗Σ1/2V⊤VΣ1/2) =λmax(WW⊤)λmax(Σ) +λmax(Σ1/2V⊤VΣ1/2)
λmin(WW⊤⊗Σ) +λmin(Ik⊗Σ1/2V⊤VΣ1/2) =λmin(WW⊤)λmin(Σ) +λmin(Σ1/2V⊤VΣ1/2).
Using the submultiplicativity of the matrix norm we get the upper bound
κ(bGO)≤κ(Σ)·λmax(WW⊤) +λmax(V⊤V)
λmin(WW⊤) +λmin(V⊤V).
Finally, by noting that we have λi(WW⊤) =σ2
i(W)andλi(V⊤V) =σ2
i(V)fori∈ {min,max}
because we have m > max{d, k}yields the result.
Lemma 4. Consider an one-hidden layer network with a Leaky-ReLU activation with negative slope
α∈[0,1]. Then the condition number of the GN matrix defined in Equation (10) can be bounded as:
κ(bGO)≤σ2
max(X)σ2
max(W) +λmax(Γ)
α2σ2
min(X)σ2
min(W) +λmin(Γ)(11)
Proof. As before we will bound the condition number by separately bounding the extreme eigenvalues
first.
First, recall the expression of the GN matrix as
bGO=Xm
i=1ΛiX⊤XΛi⊗W•iW⊤
•i+Xm
i=1ΛiX⊤V⊤
i•Vi•XΛi⊗Ik
and note that we can apply the Weyl’s and dual Weyl’s inequality because (ΛiX⊤XΛi)⊗(W•,iW⊤
•,i)
and(ΛiX⊤Vi,•V⊤
i,•XΛi)⊗Ikare symmetric, yielding the following bounds:
λmin(bGO)≥λmin mX
i=1(ΛiX⊤XΛi)⊗(W•,iW⊤
•,i)!
+λmin mX
i=1(ΛiX⊤V⊤
i,•Vi,•XΛi)⊗Ik!
λmax(bGO)≤λmax mX
i=1(ΛiX⊤XΛi)⊗(W•,iW⊤
•,i)!
+λmax mX
i=1(ΛiX⊤V⊤
i,•V⊤
i,•XΛi)⊗Ik!
.
Then we can bound the first term by using the fact that for the sum of a Kronecker product of
PSD matrices Ai,Bifori= 1, . . . , m we have λmin(Pm
i=1(Ai⊗Bi))≥min1≤j≤mλmin(Aj)·
λmin(Pm
i=1Bi)to get
λmin mX
i=1(ΛiX⊤XΛi)⊗(W•,iW⊤
•,i)!
≥λmin(ΛiX⊤XΛi)λmin mX
i=1W•,iW⊤
•,i!
≥α2λmin(X⊤X)λmin(WW⊤) (23)
=α2σ2
min(X)σ2
min(W) (24)
23by noting that Λionly contains 1 or −αon its diagonals in case of Leaky-ReLU activation andPm
i=1W•,iW⊤
•,i=WW⊤. Analogously, we have
λmax mX
i=1(ΛiX⊤XΛi)⊗(W•,iW⊤
•,i)!
≤σ2
max(X)σ2
max(W).
This yields the first upper bound.
To get the second upper bound, we need to further bound the second term. For this, note that
λmin mX
i=1(ΛiX⊤V⊤
i,•Vi,•XΛi)⊗Ik!
=λmin mX
i=1(ΛiX⊤V⊤
i,•Vi,•XΛi)!
≥α2λmin mX
i=1(X⊤V⊤
i,•Vi,•X)!
=α2λmin 
X⊤mX
i=1(V⊤
i,•Vi,•)X!
=α2λmin 
X⊤V⊤VX
. (25)
Analogously we have
λmax mX
i=1(ΛiX⊤V⊤
i,•Vi,•XΛi)⊗Ik!
≤λmax 
X⊤V⊤VX
, (26)
which gives the second upper bound.
H.1 Further details on the effect of condition number at initialization on convergence rate
We present here further details of the modified analysis of GD for strongly convex functions to
study the effect of the condition number at initialization on the convergence rate, where we use local
constants µ(k)andL(k)instead of the global smoothness and Lipschitz constant, respectively. Let L
denote the Lipschitz constant and let the smoothness constant be denoted by µ. Furthermore, let the
step size be such that ηk≤1
L. Then by the definition of gradient descent we get
||θk+1−θ∗||2=||θk−θ∗−ηk∇f(θk)||2
=||θk−θ∗||2−2ηk∇f(θk)⊤(θk−θ∗) +η2
k||∇f(θk)||2
Strong convexity
≤ (1−ηkµ)||θk−θ∗||2−2ηk(f(θk)−f(θ∗)) +η2
k||∇f(θk)||2
Smoothness
≤ (1−ηkµ)||θk−θ∗||2−2ηk(f(θk)−f(θ∗)) + 2 η2
kL(f(θk)−f(θ∗))
= (1−ηkµ)||θk−θ∗||2−2ηk(1−ηkL)(f(θk)−f(θ∗))
Since we assumed that ηk≤1
L, the last term is negative. Therefore:
||θk+1−θ∗||2≤(1−ηkµ)||θk−θ∗||2(27)
So by recursively applying (27) and replacing µby the local smoothness constants µ(k):
||θk−θ∗||2≤k−1Y
i=0(1−ηiµ(i))||θ0−θ∗||2. (28)
One can see the effect of µ(0)in the bound, which is even more dominant when µ(k)changes slowly.
Of course, the effect of µ(0)attenuates over time, and that’s why we are talking about a local effect.
24I Further experimental results
I.1 Further experiments on pruning networks at initialization
We repeated the experiments of pruning the weights at initialization for different architectures, includ-
ing a small Vision Transformer (ViT)3(Figure 13), a ResNet20 (Figure 14), a ResNet32 (Figure 15),
a VGG5Net4(Figure 16) and a Feed-forward network (Figure 17). The default initialization from
PyTorch [Paszke et al., 2019] was used. In all setups, the weights were pruned layer-wise by mag-
nitude and trained on a subset of Cifar-10 of n= 1000 images. The ViT was trained with AdamW
with a learning rate of 1e−2and weight decay 1e−2. The ResNet20, ResNet32 and VGGnet were
trained with SGD with momentum = 0.9 and weight decay of 10−4and a learning rate of 0.1with a
step decay to 0.01after 91 epochs for ResNet20 and ResNet32 and after 100 epochs for the VGGnet.
The Feed-forward network was trained with SGD with a constant learning rate of 0.01. All networks
were trained with a mini-batch size of 64, the ResNet20, ResNet32 and the Feed-forward network
were trained on a single NVIDIA GeForce RTX 3090 GPU and the ViT on a single GPU NVIDIA
GeForce RTX 4090. The training time was around 1 hour for each setup. The computation of the
condition number was run in parallel on GPU and took around 8 hours for each pruning rate of each
network.
I.2 Effect of width on conditioning and convergence speed
For a one-hidden layer feed-forward network with ReLU activation, we empirically examined the
effect of the hidden width on both the conditioning and the convergence speed when trained on a
subset of MNIST ( n= 1000) . The networks were trained with SGD with a fixed learning rate, which
was chosen via grid-search, on a single NVIDIA GeForce RTX 3090 GPU. The learning rate was 0.3
for the networks with width 15 and 20 and 0.5 for all the remaining networks. The networks were
initialized via the Xavier normal initialization in PyTorch [Paszke et al., 2019]. The training time
took less than one hour for all networks. The computation of the condition number was performed in
parallel for each width and took around 4 hours in the largest setting. Figure 18 shows the trend that
a larger width improves both the conditioning and the convergence speed. Additionally, the condition
number is also more stable throughout training for larger widths.
I.3 Additional experiments on the effect of residual connections
As we have seen previously in Figure 3, the upper bound as a linear combination in Lemma 2
is crucial to get a tight bound on the condition number. Additionally, as illustrated in Figure 19,
3The implementation of the ViT was based on Code taken from https://github.com/tintn/
vision-transformer-from-scratch/tree/main , which was published under the MIT license.
4The implementation of the ResNet20, ResNet32 and VGG5 was based on Code taken from https://
github.com/jerett/PyTorch-CIFAR10 . The author granted explicit permission to use the code.
0 50 100 150 200
Epochs10−1100Training loss
50 100 150 200
Epochs107108109101010111012Condition number κ
Pruning rate
0% 20% 40% 60% 80% 90%
Figure 13: Training loss (left) and condition number (right) for different amounts of pruning at
initialization for a Vision Transformer for three seeds. The shaded area in the figures corresponds to
one standard deviation from the mean.
250 20 40 60 80 100
Epochs0102030Training loss
0 20 40 60 80 100
Epochs104105106107Condition number κ
Pruning rate
0% 20% 40% 60% 80%Figure 14: Training loss (left) and condition number (right) for different amounts of pruning at
initialization for a ResNet20.
0 20 40 60 80 100
Epochs010203040Training loss
0 20 40 60 80 100
Epochs10610910121015Condition number κ
Pruning rate
0% 20% 40% 60% 80%
Figure 15: Training loss (left) and condition number (right) for different amounts of pruning at
initialization for a ResNet32.
0 50 100 150
Epochs010203040Training loss
0 50 100 150
Epochs1051071091011Condition number κ
Pruning rate
0% 20% 40% 60% 80%
Figure 16: Training loss (left) and condition number (right) for different amounts of pruning at
initialization for a VGG5.
260 50 100 150 200
Epochs10−1100Training loss
0 50 100 150 200
Epochs105
4×1046×104Condition number κ
Pruning rate
0% 20% 40% 60% 80%Figure 17: Training loss (left) and condition number (right) for different amounts of pruning at
initialization for a Feed-forward network.
0 100 200 300 400
Epochs10−310−210−1Training loss
0 100 200 300 400
Epochs10610810101012Condition number κ
Width
15 20 50 100 150 200
Figure 18: Training loss (left) and condition number (right) for different widths of a one-hidden layer
Feed-forward network trained on subset of MNIST ( n= 1000 ).
whitening the data is essential to improve the conditioning. Therefore, if not otherwise specified,
MNIST LeCun et al. [1998] and Cifar-10 Krizhevsky et al. [2009] will refer to whitened data and we
refer to Lemma 2 or its equivalent in Eq. (7)for residual networks as the “upper bound”. The shaded
area in the figures corresponds to one standard deviation from the mean. If not other specified via the
Kaiming normal initialization in PyTorch was used.
As one can see from (8), the condition number for each term in the sum of the first upper bound in (8)
improves when βincreases. This can be seen by the fact that the ratio will be dominated by βand will
go to 1 for β→ ∞ . This is also what we observe empirically in Figure 20 and Figure 22, where the
condition number is smaller for β= 1compared to the other two settings, where β=1
L<1√
L<1
for deeper networks with L >1.
In Figure 23 the condition number of the GN matrix for a one-hidden layer network with Leaky-
ReLU activation is plotted for different values of α, where α= 0corresponds to ReLU and α= 1
corresponds to a linear activation. We can see how ReLU activation improves the conditioning in the
one-hidden layer setting.
The condition number was computed on CPU via the explicit formulas given in Lemma 2 and Eq.
(7) and took approximately a few hours per experiment.
I.4 Further details on experiments compute resources
The total compute for all experiments conducted amounts to approximately less than 100 hours. The
main bottleneck is the computation of the condition number, which can be potentially further sped
up by making full use of parallel computations on all available GPUs. The full research project
required more compute than the experiments reported in the paper due to preliminary experiments.
272 4 6 8
DepthL102104106108κ(/hatwideGO)
whitened:κ(Σ) = 1.0, not whitened: κ(Σ) = 3.01e+ 07
normalization
whitened
not whitenedFigure 19: Comparison of the condition number for a Linear Network with hidden width m= 3100
with and without whitening Cifar-10 at initialization over three runs. Note, that the y-axis is displayed
in log scale.
2 4 6 8
DepthL0100200κ(/hatwideGO)
Width: 800
2 4 6 8
DepthL
Width: 1000
2 4 6 8
DepthL
Width: 1200
2 4 6 8
DepthL
Width: 1400MNIST
beta
1
1/√
L
1/Ltype
κ(/hatwideGO)
upper bound
Figure 20: Condition number of outer-product Hessian κ(bGO)and upper bound of a Residual
Network for different values of βas a function of depth for whitened MNIST at initialization over 20
runs.
Nevertheless, the total compute of the full research project amounts to a run time of similar order of
magnitude.
I.4.1 Computational complexity of computing condition number of Gauss-Newton matrix
In order to compute the condition number we need to compute the eigenspectrum of the GN matrix
GO, which has dimension p×p, where pis the number of parameters or of the matrix ˆGO, which
has dimensions kd×kd, where dandkare the input, respectively output dimension of the network.
The time complexity of calculating the eigenvalue decomposition has a cubic complexity. That is, in
order to compute the condition number, we have a computational complexity of O(min( p, kd)3).
282 4 6 8
DepthL0100κ(/hatwideGO)
Width: 1500
2 4 6 8
DepthL
Width: 1800
2 4 6 8
DepthL
Width: 3100
2 4 6 8
DepthL
Width: 3400Cifar-10
network
ResNet
FCNNtype
κ(/hatwideGO)
upper boundFigure 21: Comparison of condition number of outer-product Hessian κ(bGO)between Linear
Network and Residual Network for whitened Cifar-10 at initialization over three runs.
2 4 6 8
DepthL0100κ(/hatwideGO)
Width: 1500
2 4 6 8
DepthL
Width: 1800
2 4 6 8
DepthL
Width: 3100
2 4 6 8
DepthL
Width: 3400Cifar-10
beta
1
1/√
L
1/Ltype
κ(/hatwideGO)
upper bound
Figure 22: Condition number of outer-product Hessian κ(bGO)and upper bounds for Residual
Network with different values of βat initialization over three runs for Cifar-10.
1000 2000 3000 4000 5000
Widthm103
3×1024×1026×102κ(/hatwideGO)
 α
0.0
0.1
0.3
0.5
1.0
Figure 23: Condition number of a one-hidden layer Leaky-ReLU network for different values of
αfor whitened MNIST over n= 500 data points, where α= 0 corresponds to ReLU and α= 1
corresponds to a linear activation. Note that the y-axis is log-scaled.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We state our main contributions in the abstract and devote a separate paragraph
in the introduction on the paper’s contributions and scope.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In the final paragraph of the paper on "Limitations and future work" we discuss
settings, which the paper does not cover, such as the extension to non-linear activations for
arbitrary number of hidden layers, the extension to transformer architectures and analytic
bounds for normalization layers.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
30Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All assumptions are stated at the beginning of the paper and formal proofs of
all Lemmas and Theorems are provided in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: For the main experimental results, all information needed, such as the network
architecture, the initialization scheme, and the optimizer with its hyperparameters are
specified. Additionally, the dataset as well as the preprocessing steps were specified.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
31some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Code to run the experiments is provided in the supplementary material together
with instructions how to reproduce the results.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: For each experimental setting, the architectural setup, the initialization scheme
and the optimizer together with its hyperparameters were specified. Since our work only
evaluated the training setting, a data split was not necessary.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: For the experiments that support the main claims of the paper 1-sigma error
bars are shown as shaded area. Figure 2a, Figure 2b, and Figure 5 were calculated over 3
seeds and Figure 3 over 20 seeds.
Guidelines:
32• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: For each experiment the (approximate) compute and whether CPU or GPU
were used were specified. Additionally a brief subsection is provided in Appendix I.4 to
discuss the total estimated compute.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The paper conforms in every aspect with the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
33Justification: This paper is a theoretic contribution and as such we do not see any direct
societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper poses no such risks for misuse or dual-use as it is primarily
theoretical in nature.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The creators and original owners of the datasets used (MNIST and Cifar-10)
the creators and original owners of models which were modified and used (ViT, ResNet20,
ResNet32) were properly credited. In the case of the ResNet20, and ResNet32, the original
owner Jiang Wenjie provided explicit permission to use the code.
Guidelines:
34• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
35Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
36