Causal Effect Identification in a Sub-Population with
Latent Variables
Amir Mohammad Abouei1, Ehsan Mokhtarian1, Negar Kiyavash2, Matthias Grossglauser1
1School of Computer and Communication Sciences, EPFL
2College of Management of Technology, EPFL
{amir.abouei, ehsan.mokhtarian, negar.kiyavash, matthias.grossglauser}@epfl.ch
Abstract
The S-ID problem seeks to compute a causal effect in a specific sub-population
from the observational data pertaining to the same sub-population [ AMK24 ]. This
problem has been addressed when all the variables in the system are observable.
In this paper, we consider an extension of the S-ID problem that allows for the
presence of latent variables. To tackle the challenges induced by the presence of
latent variables in a sub-population, we first extend the classical relevant graphical
definitions, such as C-components and Hedges, initially defined for the so-called
ID problem [ Pea95 ,TP02 ], to their new counterparts. Subsequently, we propose a
sound algorithm for the S-ID problem with latent variables.
1 Introduction
Causal inference, i.e., understanding the effect of an intervention in a stochastic system, is a key
focus of research in statistics and machine learning [ Rub74 ,Pea00 ,Pea09 ,SGSH00 ]. Scientists,
policymakers, business leaders, and healthcare professionals must understand causal relationships to
move beyond correlations and make informed, evidence-based decisions. To perform causal inference
tasks, it is crucial to differentiate between two types of data: observational and interventional [ PM18 ].
Biased SamplerUnbiased Sampler
PopulationSamples from the population
Samples from a sub-populationùëÉ(ùëΩ)
Observe ùëΩùëÉ(ùëΩ	|ùëÜ=1)Observe ùëΩ
(a) Observational data.
Biased SamplerUnbiased SamplerPopulation after an intervention on ùëøSamples from the population
Samples from a sub-populationStudy ùíÄùëÉùëø(ùíÄ)
ùëÉùëø(ùíÄ|ùëÜ=1)Study ùíÄ (b) Interventional data.
Figure 1: A population consists of a sample space for the study of the causal effect of an intervention.
While the unbiased sampler draws samples uniformly at random, the biased sampler selects samples
based on certain criteria, forming a sub-population.
Observational Data. Figure 1 illustrates a population that pertains to the entire sample space for a
study of the causal effect of some intervention. A sampler draws samples from the population. The
sampler is unbiased if it draws samples at random such that each individual in the population has
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Table 1: Various causal effect identification problems. Xis the set of intervened variables, Yis the
set of outcome variables, and S= 1corresponds to a sub-population. ID, c-ID, and S-Recoverability
have been addressed in the presence of latent variables. S-ID problem has only been studied in
causally sufficient cases where all variables are observed.
Problem Given distribution Target distribution Presence of latent variables
ID P(V) PX(Y) ‚úì
c-ID P(V) PX(Y|Z) ‚úì
S-Recoverability P(V|S= 1) PX(Y) ‚úì
S-ID P(V|S= 1) PX(Y|S= 1) √ó
an equal chance of being selected. As a result, the obtained sample is representative of the entire
population. In contrast, a biased sampler selects samples based on certain criteria forming a sub-
population . For each extracted sample, we collect data from a set of observed features denoted by V.
As depicted in Figure 1a, when the sampler is unbiased, the observational data comes from the joint
distribution P(V). For a biased sampler, the observations can be modeled as drawn from a conditional
distribution P(V|S= 1) , where S= 1indicates that the sample belongs to a sub-population.
Interventional Data. Anintervention on a subset X‚äÜVassigns specific values to the variables in
the subset. If performing an intervention results in changes in other variables of interest, it suggests a
causal relationship, apart from mere correlation. Interventions are often represented with the do()
operator, highlighting the deliberate change of a variable [ Pea00 ,Pea09 ]. For the sake of simplicity
in notation, we use PX(¬∑)to denote the distribution of the variables after an intervention on X. Figure
1b depicts the population after an intervention on subset X‚äÜV, where we seek to understand how
changes in Xwould affect a set of outcome variables Y‚äÜV\X. To analyze this causal effect
across the entire population, we must compute the distribution PX(Y). On the other hand, if we are
merely interested in the results of the intervention on a specific sub-population, it suffices to compute
the conditional distribution PX(Y|S= 1) pertaining to the sub-population.
Causal Effect Identification. Performing interventions in populations can be challenging due to
high costs, ethical concerns, or sheer impracticability. Instead, researchers often use observational
methods, leveraging the environment‚Äôs causal graph , a graphical representation that depicts the causal
relationships between variables [ Pea09 ,SGSH00 ], and observational data to estimate interventional
distributions of interest. Various causal effect identification problems in the causal inference literature
are concerned with this issue.
Related Work. Table 1 lists four causal effect identification problems. The most renowned among
them is the ID problem, introduced by [ Pea95 ], which seeks to determine a causal effect for the entire
population using the observational distribution pertaining to the entire population. Specifically, it
aims to compute PX(Y)from P(V). The c-ID problem, introduced by [ SP06a ], extends the ID
problem to handle conditional causal effects, i.e., compute the conditional causal effect PX(Y|Z)
from the observational distribution P(V)pertaining to the entire population. [ BTP14 ] introduced
the S-Recoverability problem that focuses on inferring the causal effect of XonYfor the entire
population using data drawn solely from a specific sub-population. [ AMK24 ] introduced S-ID, which
asks whether a causal effect in a sub-population such as PX(Y|S= 1) can be uniquely computed
from the observational distribution pertaining to that sub-population, i.e., P(V|S= 1) . Another
direction of research considers learning a causal effect from multiple datasets [ LCB19 ,KMEK22 ,
CLB21 ,KEK23 ,THK21 ,LGS24 ]. In all aforementioned causal inference problems, the causal graph
is assumed to be known. Some recent work relax this assumption [ JZB19 ,JRZB22 ] or introduce
additional conditions on the causal graph with the goal of identifying a broader range of causal effects
[THK19 ,MJEK22 ,JAK23 ]. Settings where data samples are dependent introduce new challenges to
causal inference, which have been explored in another line of research [SS18, BMS20, ZMP23].
S-ID Is Not ID. It is worth emphasizing that the S-ID problem is not a special case of ID problem,
where the population is restricted to the target sub-population. The presence of selection bias S
introduces additional dependencies among variables, and ignoring Sin the graph invalidates the
application of the rules of do-calculus [ Pea00 ] (which are the main tools used to tackle the ID
problem) on input distribution, i.e., P(V|S= 1) . Consequently, there are many instances where a
causal effect is identifiable in the ID setting but not identifiable in the S-ID setting. In particular, when
all the variables in a causal system are observable, all causal effects are identifiable in the setting of
2the ID problem [ Pea00 ]. This is not the case in the S-ID setting, and some causal effects become
non-identifiable, as noted by [ AMK24 ]. Moreover, even when a causal effect is identifiable in both
the ID and S-ID settings, using the expression from the ID algorithm can lead to erroneous inference.
Example 1 illustrates this case.
Example 1. Consider an example pertaining to study of the effect of a cholesterol-lowering
medication on cardiovascular disease. Figure 2 depicts the causal graph of this example, where X
is the medication choice that directly affects Y, cardiovascular disease. Variable Zrepresents the
diet and exercise routine of a person. In this scenario, XandZare confounded by the person‚Äôs
socioeconomic status (e.g., income). It can be shown that the causal effect of XonY(in the entire
population, for instance, the people around the globe) is identifiable (ID) from P(X, Y, Z )and can
be computed as PX(Y) =P(Y|X).
X Z
Y S
Figure 2: ADMG GSin Example 1.However, we might instead be interested in a study that
focuses on the people of a specific region. In this case,
the target sub-population would correspond to individuals
who are biased toward particular diet and exercise rou-
tines and possibly have a higher genetic predisposition
for heart disease. Let Sbe an indicator node for this sub-
population, Zhas a directed edge toward S, and Sand
Yare confounded by the latent genetic predisposition of
the people of this group. We will show in Section 5 that
the causal effect in this sub-population, PX(Y|S= 1) , is
S-ID and equalsP
ZP(Y|X, Z, S = 1)P(Z|S= 1) . In
this example, the presence of Sintroduces a spurious correlation between XandYthrough the path
involving ZandS. Therefore, if we were to ignore the presence of Sand apply the ID algorithm to
the input P(X, Y, Z |S= 1) , it would result in incorrect inference: P(Y|X, S = 1) as opposed to
the correct value PX(Y|S= 1) . In Appendix D, we empirically compare the differences between ID
and S-ID settings. We present another example in Appendix A where a causal effect is ID but not
S-ID.
In this paper, we consider the S-ID problem in the presence of latent variables. Our main contributions
are as follows.
‚Ä¢We extend the classical relevant graphical definitions, such as c-components and Hedges, initially
defined for the ID problem so that they inherit the key properties of their predecessors but are
applicable to the S-ID setting in the presence of latent variables (Section 4).
‚Ä¢We present a sufficient graphical condition to determine whether a causal effect is S-ID (Theorem
5.1). Accordingly, we propose a sound algorithm for the S-ID problem (Algorithm 1).
‚Ä¢We show a reduction from the S-Recoverability problem to the S-ID problem (Theorem 6.1),
indicating that solving S-ID can also solve the S-Recoverability problem.
Organization. In Sections 2 and 3, we cover the preliminaries and review key definitions and results
for the ID problem. In Section 4, we formally define the S-ID problem in the presence of latent
variables and present the proper modifications of the classical graphical notions of interest for the
S-ID problem. We present our main results in Section 5. In Section 6, we introduce a reduction
from S-Recoverability to S-ID. The appendix includes proofs of our results, as well as a numerical
experiment.
2 Preliminaries
Throughout the paper, we use capital letters to represent random variables and bold letters to represent
sets of variables. Furthermore, to facilitate ease of reading, we have summarized the key notations in
Table 2.
Graph Definitions. Acyclic directed mixed graphs (ADMGs) consist of a mix of directed and
bidirected edges and have no directed cycles. Let G= (V,E1,E2)be an ADMG, where Vis a
set of variables, E1is a set of directed edges ( ‚Üí), andE2is a set of bidirected edges ( ‚Üî). The set
of parents of a variable X‚ààV, denoted by PaG(X), consists of the variables with a directed edge
toX. Similarly, the set of ancestors of X‚ààV, denoted by AncG(X), includes all variables on a
3Table 2: Table of notations.
Notation Description
V,U Sets of observed and unobserved variables
S Auxiliary vertex (variable) used to model a sub-population
GSAugmented ADMG over V‚à™ {S}
PaG(X) Parents of vertex Xin graph G
AncG(X),AncG(X) Ancestors of vertex X(including X); the union of ancestors for all X‚ààX
VAS,VNS V‚à©AncGS(S)and its complement, V\AncGS(S)
G[X] Subgraph of Ginduced by the vertices in X
GXZ Subgraph of Gafter removing incoming edges to Xand outgoing edges from Z
PS(V) Sub-population distribution, i.e., P(V|S= 1)
PX(Y) Causal effect of XonY, i.e., post-interventional distribution
PS
X(Y) Causal effect of XonYin the sub-population, i.e., PX(Y|S= 1)
Q[H] PV\H(H)
QS[H] PVNS\H(H|AncGS(S)\ {S},S= 1),‚àÄH‚äÜVNS
directed path to X, including Xitself. For a set X‚äÜV, we define AncG(X) =S
X‚ààXAncG(X).
In an ADMG GoverV, a subset X‚äÜVis called ancestral if AncG(X) =X.
A path is called bidirected if it only consists of bidirected edges. A non-endpoint vertex Xion a path
(X1, X2, . . . , X k)is called a collider if one of the following situations arises:
Xi‚àí1‚ÜíXi‚ÜêXi+1, X i‚àí1‚ÜîXi‚ÜêXi+1, X i‚àí1‚ÜíXi‚ÜîXi+1, X i‚àí1‚ÜîXi‚ÜîXi+1.
LetX,Y,Wbe three disjoint subsets of variables in an ADMG G. A path P= (X, Z 1, . . . , Z k, Y)
between X‚ààXandY‚ààYinGis called blocked byWif there exists 1‚â§i‚â§ksuch that Ziis a
collider on PandZi/‚ààAncG(W), orZiis not a collider on PandZi‚ààW.
Denoted by (X‚ä• ‚ä•Y|W)G, we say Wm-separates XandYif for any X‚ààXandY‚ààY,W
blocks all the paths in Gbetween XandY. Conversely, (XÃ∏‚ä• ‚ä•Y|W)Gif there exists at least one
path between a variable in Xand a variable in Ythat is not blocked by W.
ForX,Z‚äÜV,GXZdenotes the edge subgraph of Gobtained by removing the edges with an
arrowhead to a variable in X(including bidirected edges) and outgoing edges of Z(excluding
bidirected edges). Moreover, G[X]denotes the vertex subgraph of Gconsisting of Xand bidirected
and directed edges between them.
SCM. A structural causal model (SCM) is a tuple (V,U,F, P(U)), where Vis a set of endogenous
variables, Uis a set of exogenous variables independent from each other with the joint probability
distribution P(U), andF={fX}X‚ààVis a set of deterministic functions such that for each X‚ààV,
X=fX(PaX,UX),
where PaX‚äÜV\ {X}andUX‚äÜU. This SCM induces a causal graph GoverVsuch that
PaG(X) = PaXand there is a bidirected edge between two distinct variables X, Y‚ààVwhen
UX‚à©UYÃ∏=‚àÖ. Henceforth, we assume the underlying SCM induces a causal graph that is ADMG,
i.e., it contains no directed cycles.
An SCM M= (V,U,F, P(U))with causal graph Ginduces a unique joint distribution over the
variables Vthat can be factorized as
PM(V) =X
UY
X‚ààVPM(X|PaG(X))Y
U‚ààUPM(U).
This property is known as the Markov factorization [ Pea09 ]. Note thatP
Xdenotes marginalization,
i.e., summation (or integration for continuous variables) over all the realizations of the variables in
setX. We often drop the MinPM(¬∑)when it is clear from the context.
Modeling a Sub-Population. We model a sub-population using an auxiliary variable Sand a biased
sampler from a causal environment akin to [ BP12 ,AMK24 ]. Suppose Mis the underlying SCM
of an environment with the set of observed variables V. In this causal environment, an unbiased
sampler produces samples drawn from P(V). When the sampler is biased, it draws samples from
the conditional distribution PS(V):=P(V|S= 1) , where Sis an auxiliary variable defined as
4S:=fS(PaS,US),where fSis a binary function, PaS‚äÜV, andUSis the set of exogenous variables
corresponding to S. Note that UScan intersect with U, but the variables in U‚à™USare assumed to be
independent. In this model, S= 1indicates that the sample is drawn from the target sub-population.
Furthermore, we define the augmented SCM MS= 
V‚à™ {S},U‚à™US,F‚à™ {fS}, P(U‚à™US)
obtained by adding Sto the underlying SCM M. We denote by GS, the causal graph of MS, which
is an augmented ADMG over V‚à™ {S}. Note that in GS, variable Sdoes not have any children, but it
can have several parents and bidirected edges.
Intervention. Anintervention on a set X‚äÜVconverts Mto a new SCM where the equations
of the variables in Xare replaced by some constants. We denote by Q[V\X]:=PX(V\X)the
corresponding post-interventional distribution, i.e., the joint distribution of the variables in the new
SCM. The causal effect of XonYrefers to the post-interventional distribution PX(Y), where X
andYare disjoint subsets of V. Accordingly, the causal effect of XonYin a sub-population is
denoted by PX(Y|S= 1) .1
Problem Setup. Let(V,U,F, P(U))be an SCM with ADMG Grepresenting its causal graph.
Additionally, let Sbe an auxiliary variable representing a specific sub-population. In this paper,
given the augmented graph GSand two arbitrary, disjoint subsets XandY, we address the following
question: Can the causal effect PS
X(Y)be uniquely identified from the observational distribution
PS(V)? Please refer to Definition 4.1 for the formal definition of the S-ID problem.
3 ID, C-component, and Hedge
Our proposed approach to address the S-ID problem extends certain definitions and properties from
the classic ID problem [ Pea95 ]. For the sake of completeness and pedagogical reasons, in this section,
we review some definitions and the main results in the ID problem [TP02, HV06, SP06b].
Definition 3.1 (ID).Suppose Gis an ADMG over Vand let XandYbe disjoint subsets of V.
Causal effect PX(Y)is said to be identifiable (or ID for short) in Gif for any two SCMs M1and
M2with causal graph Gfor which PM1(V) =PM2(V)>0, then PM1
X(Y) =PM2
X(Y).
Next, we review C-components, a fundamental concept to address the ID problem.
Definition 3.2 (C-component) .Suppose Gis an ADMG over V. The C-components of Gare
the connected components in the graph obtained by considering only the bidirected edges of G.
Furthermore, Gis called a single C-component if it contains only one C-component.
There exist a few different definitions for Hedge , another central notion in the ID literature. Here, we
provide a somewhat simplified definition that not only suffices to present the main result of the ID
problem but also allows us to extend it in the next section to the S-ID setting.
Definition 3.3 (Hedge) .Suppose Gis an ADMG over V, and let Y‚äÜVsuch that G[Y]is a
single C-component. A subset H‚äÜVis called a Hedge for YinG, ifY‚ääH,G[H]is a single
C-component, and H=AncG[H](Y).
Example 2. Consider the ADMG GoverV={X1, X2, Y1, Y2}depicted in Figure 3a. In this
case,G,G[X1, X2], andG[Y1, Y2]are single C-components. The C-components of G[X1, X2, Y1]
are{X1, X2}and{Y1}. Furthermore, H={X1, Y1, Y2}is a Hedge for Y={Y1, Y2}sinceG[H]
andG[Y]are single C-components and AncG[H](Y) =H. Similarly, Vis a Hedge for Y, but there
exists no Hedge for either {Y1}or{Y2}.
The following theorem, restating the results in [ SP06b ] and [ HV06 ], outlines a necessary and
sufficient condition to determine the identifiability of a causal effect in an ADMG.
Theorem 3.4 (ID).LetGbe an ADMG over V, andXandYbe two disjoint subsets of V. Causal
effect PX(Y)is ID in Gif and only if Q[D]is ID in G, where D=AncG[V\X](Y). Furthermore, let
{Di}k
i=1be the C-components of G[D], then Q[D]is ID in Gif and only if there are no Hedge in G
for any of the C-components {Di}k
i=1.
Example 3. Following Example 2, Theorem 3.4 implies that PX1(Y1),PX2(Y2),P{X1,X2}(Y1), and
P{X1,X2}(Y2)are ID since no Hedge for either {Y1}or{Y2}exists. However, PX1(Y1, Y2)is not
1Note that in this notation, the order of operations is intervention first, then conditioning.
5X1 X2
Y1 Y2
(a) ADMG Gin Examples 2-3.X1
X2 Z2Z1
Y2 SY1
(b) Augmented ADMG GSin Examples 4-9.
Figure 3: ADMGs in Examples of Sections 3, 4, and 5.
ID because D={Y1, Y2, X2}and the C-components of Q[D]areD1={Y1, Y2}andD2={X2},
and{X1, Y1, Y2}(orV) is a Hedge for D1. Similarly, we can show that P{X1,X2}(Y1, Y2)is not ID.
4 S-ID, S-component, and S-Hedge
We begin by providing a formal definition of the S-ID problem in the presence of latent variables, i.e.,
when the causal graph is an ADMG. Then, we present modifications of the graphical notions from
the previous section so that they inherit the key properties of their predecessors and can be applied to
theS-ID setting.
To avoid repetition, henceforth, we denote by Vthe set of observed variables and by GSan augmented
ADMG over V‚à™ {S}. Furthermore, we denote by VASandVNSthe ancestors and non-ancestors of
SinV, i.e.,
VAS:=V‚à©AncGS(S),VNS:=V\AncGS(S).
Definition 4.1 (S-ID).LetXandYbe disjoint subsets of V. Conditional causal effect PX(Y|S= 1)
(orPS
X(Y)) is S-ID in GSif for any two augmented SCMs MS
1andMS
2with causal graph GSfor
which PMS
1(V|S= 1) = PMS
2(V|S= 1)>0, then PMS
1
X(Y|S= 1) = PMS
2
X(Y|S= 1) .
Next definition extends Q[¬∑]and introduces QS[¬∑].
Definition 4.2 (QS[¬∑]).ForH‚äÜVNS, we define QS[H]:=PVNS\H(H|AncGS(S)\ {S}, S= 1).
The next definition extends C-components (Definition 3.2) and introduces S-components.
Definition 4.3 (S-component) .For a subset H‚äÜVNS, letC1, . . . ,Ckdenote the C-components of
GS[H‚à™AncGS(S)]. We define the S-components of HinGSas the subsets Hi:=Ci‚à©Hwhich are
non-empty. Furthermore, His called a single S-component in GSif it contains only one S-component.
Note that QS[¬∑]and S-components are only defined for the subsets of VNS. Figure 4a visualizes
the structure of S-components of a subset H‚äÜVNS. In this figure, each blue subset (e.g., M1)
represents a c-component, which means all the nodes within them are connected via bidirected edges.
Therefore, according to Definition 4.3, all nodes inside S-components (e.g., H1) ofHare connected
via bidirected edges in GS[H‚à™VAS]. Figure 4b shows the structure of a single S-component, where
all the nodes of Hare connected via bidirected edges in GS[H‚à™VAS].
Example 4. Consider the ADMG GSin Figure 3b over V‚à™ {S}, where V=
{X1, X2, Y1, Y2, Z1, Z2}. Since AncGS(S) = {Z1, Z2, S}, we have VAS={Z1, Z2}and
VNS={X1, X2, Y1, Y2}. In this case, the S-components of VNSare{X1, X2}and{Y1, Y2}.
Moreover, the S-components of {X1, Y1, Y2}are{X1}and{Y1, Y2}.
We now provide two crucial properties for QS[¬∑].
Lemma 4.4. LetW,W‚Ä≤be two subsets of VNSsuch that W‚Ä≤‚äÇW. IfW‚Ä≤is an ancestral set in
GS[W], then
QS[W‚Ä≤] =X
W\W‚Ä≤QS[W]. (1)
6Anc(S) H
H1C1
C2M2
H2N2N1M1
M3(a)S-components of H
areH1,H2.
Anc(S) H
C1
M2N1
N2M1
M3(b)His a single
S-component.
Anc(S)
H
YN1
N2(c)His an S-Hedge for Y.
H
Y (d)His a Hedge for Y.
Figure 4: Visualization of the graph structures defined in Sections 3 and 4.
Lemma 4.5. Suppose H‚äÜVNSand letH1, . . . ,Hkdenote the S-components of HinGS. Then,
‚Ä¢QS[H]decomposes as
QS[H] =QS[H1]QS[H2]. . . QS[Hk]. (2)
‚Ä¢Letmbe the number of variables in H, and consider a topological ordering of the variables in
graph GS[H], denoted as Vh1<¬∑¬∑¬∑< Vhm. LetH(0)=‚àÖand for each 1‚â§i‚â§m,H(i)denote
the set of variables in Hordered before Vhi(including Vhi). For every 1‚â§j‚â§k,QS[Hj]can be
computed from QS[H]by
QS[Hj] =Y
{i|Vhi‚ààHj}QS[H(i)]
QS[H(i‚àí1)], (3)
where QS[H(i)]s can be computed by
QS[H(i)] =X
H\H(i)QS[H]. (4)
The aforementioned lemmas are extensions of similar lemmas for Q[¬∑][TP03] to QS[¬∑].
Example 5. Following Example 4, since {Y1}is ancestral in GS[Y1, Y2], Lemma 4.4 implies that
QS[Y1] =P
Y2QS[Y1, Y2]. Furthermore, since the S-components of VNSare{X1, X2}and{Y1, Y2},
Lemma 4.5 implies that QS[Y1, Y2] =QS[VNS]P
Y1,Y2QS[VNS]. Thus, QS[Y1]can be computed from QS[VNS].
Finally, we define S-Hedges, which extends Definition 3.3 for Hedges.
Definition 4.6 (S-Hedge) .Suppose Y‚äÜVNSis a single S-component in GS. A subset H‚äÜVNSis
called an S-Hedge for YinGS, ifY‚ääH,His a single S-component in GS, andH=AncGS[H](Y).
WhenH‚äÜVNSis a single c-component, it is also a single S-component. Therefore, if His a Hedge
forY, it will also be an S-Hedge for Y. Thus, Hedges can be seen as special cases of S-Hedges when
H‚äÜVNS. Figure 4d shows the structure of H, which is a single c-component and forms a Hedge
forY. Moreover, Figure 4c presents the structure of an S-hedgeHforY. Note that S-hedges are
more complex graph structures compared to Hedges. This complexity is required for us to be able to
determine whether a causal effect is S-ID.
Example 6. Following Examples 4 and 5, {X1, X2}is an S-Hedge for {X2}, because both {X1, X2}
and{X2}are single S-components and {X1, X2}=AncGS[X1,X2](X2). Similarly, {Y1, Y2}is an
S-Hedge for {Y2}.
5 Main Results
In this section, we provide a sufficient graphical condition for a causal effect to be S-ID in an ADMG.
This extends the condition presented in [ AMK24 ], which assumes that the causal graph is a DAG.
Accordingly, we propose a sound algorithm for the S-ID problem in the presence of latent variables.
7Recall that GSis an augmented ADMG over the set observed variables Vand auxiliary variable S,
and we defined VAS=V‚à©AncGS(S)andVNS=V\AncGS(S).
Theorem 5.1. For disjoint subsets XandYofV, letXAS:=X‚à©VAS,XNS:=X‚à©VNS, and
YNS:=Y‚à©VNS.
1. Conditional causal effect PS
X(Y)isS-ID in GSif and only if
(XAS‚ä• ‚ä•Y|XNS, S)GS
XASXNS, (5)
andPS
XNS(Y,XAS)isS-ID in GS.
2.Suppose D:=AncGS[VNS\XNS](YNS)and let {Di}k
i=1denote the S-components of DinGS.
Conditional causal effect PS
XNS(Y,XAS)isS-ID in GSif there are no S-Hedge in GSfor any
of{Di}k
i=1.
Remark 5.2. If either XNSorYNSis an empty set, then PS
X(Y)isS-ID inGSif and only if Equation
(5) holds.
In the absence of latent variables, i.e., when GSis a directed acyclic graph (DAG), there are no
S-Hedge in GSsince all the edges are directed. Therefore, Theorem 5.1 states that PS
XNS(Y,XAS)
is always S-ID, and PS
X(Y)isS-ID in GSif and only if Equation (5)holds. We note that this is
consistent with the condition presented in [ AMK24 , Theorem 2] for the S-ID problem in the absence
of latent variables.
Example 7. Consider again ADMG GSin Figure 3b, where we want to determine whether
PS
{X1,X2,Z1}(Y1, Y2)isS-ID in GS. In this case, XAS={Z1},XNS={X1, X2},YNS={Y1, Y2},
and(Z1‚ä• ‚ä• {Y1, Y2}|X1, X2, S)GS
Z1X1,X2.This shows that Equation (5)holds. Hence, we need
to determine whether PS
X1,X2(Y1, Y2, Z1)isS-ID in GS. In this case, D=AncGS[Y1,Y2](Y1, Y2) =
{Y1, Y2}, which is a single S-component. Since there exists no S-Hedge for {Y1, Y2}, Theorem 5.1
implies that PS
{X1,X2,Z1}(Y1, Y2)isS-ID in GS.
Algorithm for S-ID. So far, we have presented a graphical condition to determine whether a causal
effect is S-ID. In this section, we propose a recursive algorithm that returns an expression for PS
X(Y)
in terms of PS(V)when the condition of Theorem 5.1 holds, and otherwise, returns F AIL.
In the proof of Theorem 5.1 presented in the appendix, we show that when Equation (5) holds, then
PS
X(Y) =X
WPS(YAS,W|XAS)PS
XNS(YNS|VAS), (6)
whereW=VAS\(XAS‚à™YAS). Thus, it suffices to find an expression for PS
XNS(YNS|VAS)in terms
ofPS(V). LetD=AncGS[VNS\XNS](YNS)and{Di}k
i=1be the S-components of DinGS. From
Lemmas 4.4 and 4.5 we have
PS
XNS(YNS|VAS) =X
D\YNSY
iQS[Di]. (7)
Therefore, it suffices to find an expression for each Diin terms of PS(V). Note that Diis a single
S-component in GS. We can now propose Algorithm 1 for computing PS
X(Y)fromPS(V).
Function sID takes disjoint subsets XandYofValong with an augmented ADMG GSand
conditional distribution PS(V)as input. After defining the required notations in lines 3-5, it checks
Equation (5)in line 6. If this condition is met, it defines Dand its S-components {Di}k
i=1inGS. For
each1‚â§i‚â§k, it finds the corresponding S-component TiofVNSinGSthat contains Di. Note
thatTiis well-defined as Dicannot partially intersect with the S-components of VNSinGS. Next,
the algorithm seeks to compute QS[Di]from QS[Ti]by calling Function sID-Single . If Function
sID-Single succeeds in returning an expression for each i, then the algorithm uses Equations (6)and
(7) to return an expression for PS
X(Y)in terms of PS(Y). Otherwise, the algorithm returns F AIL.
Function sID-Single takes two single S-components CandTinGSsuch that C‚äÜTand aims to
drive an expression for QS[C]in terms of QS[T]. The procedure is recursive and uses Lemmas 4.4
and 4.5. In each recursion, the algorithm reduces Tto a smaller subset T‚Ä≤such that T‚Ä≤is still a
single S-component in GSandC‚äÜT‚Ä≤(lines 7-10). Eventually, the function either returns FAILor an
expression for QS[C].
8Algorithm 1 Computing PS
X(Y)fromPS(V)
1:Function sID (X,Y,GS, PS(V))
2:Output: Expression for PS
X(Y)in terms of
PSor F AIL
3:VAS‚ÜêV‚à©AncGS(S),VNS‚ÜêV\AncGS(S)
4:XAS‚ÜêX‚à©VAS,XNS‚ÜêX‚à©VNS
5:YAS‚ÜêY‚à©VAS,YNS‚ÜêY‚à©VNS
6:if(XASÃ∏‚ä• ‚ä•Y|XNS, S)GS
XASXNSthen
7: Return FAIL
8:D‚ÜêAncGS[VNS\XNS](YNS)
9:{D1, . . . ,Dk} ‚Üê S-components of DinGS
10:foriin[1 :k]do
11:Ti‚ÜêThe S-component of VNSthat con-
tainsDi
12: Compute QS[Ti]using Lemma 4.5
13: QS[Di]‚ÜêsID-Single (Di,Ti, QS[Ti])
14: ifQS[Di]= FAILthen
15: Return FAIL
16:W‚ÜêVAS\(XAS‚à™YAS)
17:ReturnP
WPS(YAS,W|XAS)P
D\YNSQ
iQS[Di]1:Function sID-Single (C,T,QS[T])
2:Input : Two single S-components CandTinGS
such that C‚äÜT
3:Output: Expression for QS[C]in terms of QS[T]
or F AIL
4:A‚ÜêAncGS[T](C)
5:ifA=C:ReturnP
T\CQS[T]
6:ifA=T:Return FAIL
7:ifC‚ääA‚ääT,then
8:T‚Ä≤‚ÜêThe S-component of AinGSthat contains
C
9: Compute QS[T‚Ä≤]fromQS[T]using Lemma 4.5
10: Return sID-Single (C,T‚Ä≤,QS[T‚Ä≤])
Example 8. Consider again ADMG GSdepicted in Figure 3b, where we want to apply Algorithm
1 for causal effect PS
X2(Y2). Herein, XNS={X2},YNS={Y2}, andXAS=YAS=‚àÖ. Function
S-IDpasses the condition in line 6 and defines D={X1, Y1, Y2}, leading to D1={X1}and
D2={Y1, Y2}. It then defines Ti‚Äôs in line 12 as T1={X1, X2}andT2={Y1, Y2}. In line 13, it
uses Lemma 4.5 to compute QS[T1] =P
Y1,Y2QS[VNS]andQS[T2] =QS[VNS]P
Y1,Y2QS[VNS](See Example
5). It then calls Function sID-Single , which returns QS[D1] =P
X2QS[T1]andQS[D2] =QS[T2].
Finally, in line 20, the function returns
PS
X2(Y2) =X
Z1,Z2PS(Z1, Z2)X
X1,Y1QS[X1]QS[Y1, Y2],
where QS[X1] =PS(X1|Z1, Z2)andQS[Y1, Y2] =PS(Y1, Y2|X1, X2, Z1, Z2).
Example 9. Following the previous example, suppose we want to apply Algorithm 1 for computing
causal effect PS
X1(Y1, Y2). In this case, the algorithm needs to compute QS[Y1, Y2]andQS[X2].
However, when the algorithm calls sID-Single (X2,{X1, X2}, QS[X1, X2]), Function sID-Single
returns F AIL. Accordingly, Function sIDreturns F AILforPS
X1(Y1, Y2).
Remark 5.3. Algorithm 1 is sound for the S-ID problem in the presence of latent variables. We con-
jecture that this algorithm is also complete , meaning that whenever it returns FAIL, the corresponding
causal effect is not S-ID.
6 Reduction from S-Recoverability to S-ID
Recall that the objective in S-Recoverability is to compute PX(Y)from PS(V)[BTP14 ], while
S-ID aims to compute PS
X(Y)from PS(V). [BT15 ] proposed RC, a sound algorithm for the S-
Recoverability problem. Subsequently, [ CTB19 ] proved that RC is complete. In this section, we
present a reduction from the S-Recoverability problem to the S-ID problem. This indicates that
solving S-ID can solve the S-Recoverability problem (but not the other way around).
Theorem 6.1. For disjoint subsets XandYofV,PX(Y)can be uniquely computed from PS(V)
in the augmented ADMG GSif and only if
(Y‚ä• ‚ä•S|X)GS
X, (8)
andPS
X(Y)isS-ID in GS.
9Algorithm 2
Reduction from S-Recoverability to S-ID
1:Input:X,Y,GS, PS
2:Output: Expression for PX(Y)in terms
ofPSor F AIL
3:if(YÃ∏‚ä• ‚ä•S|X)GS
Xthen
4: Return FAIL
5:else
6: Return sID (X,Y,GS, PS)X1
X2 Z2Z1
Y S
Figure 5: The augmented ADMG GSin Example 10.
Remark 6.2. Equation (8)is a very restrictive condition, and when it holds, Rule 1 of do-calculus
implies that PX(Y) =PS
X(Y).
Theorem 6.1 implies that when Equation (8)does not hold, then PX(Y)is not S-Recoverable.
However, this causal effect might be identifiable in the target sub-population, i.e., PS
X(Y)isS-ID.
As a consequence of Theorem 6.1, we propose Algorithm 2 for computing PX(Y)fromPS(V). The
algorithm takes as input two disjoint subsets XandYofValong with an augmented ADMG over
V‚à™ {S}and the conditional distribution PS(V). It first checks Equation (8)in line 3, and then calls
Algorithm 1 as a subroutine to compute PS
X(Y)fromPS(V)when it is S-ID in GS.
Example 10. Consider the augmented ADMG GSin Figure 5. In this graph, (YÃ∏‚ä• ‚ä•S|X1)GS
X1, thus,
Theorem 6.1 implies that PX1(Y)cannot be uniquely computed from PS(V). On the other hand,
PX2(Y)can be identified from PS(V)since (Y‚ä• ‚ä•S|X2)GS
X2and due to Theorem 5.1, PS
X2(Y)is
S-ID in GS. In this case, Algorithm 2 returns the following expression for PX2(Y)in terms of PS
PX2(Y) =X
Z1,Z2PS(Z1, Z2)X
X1PS(X1, X2, Y|Z1, Z2)
PS(X2|X1, Z1, Z2).
7 Conclusion
The S-ID problem, introduced by [ AMK24 ], asks whether, given the causal graph, a causal effect
in a sub-population can be identified from the observational distribution pertaining to the same
sub-population. [ AMK24 ] addressed this problem when all the variables in the causal graph are
observable. In this paper, we studied the S-ID problem in the presence of latent variables and provided
a sufficient graphical condition to determine whether a causal effect is S-ID. Consequently, we
proposed a sound algorithm for S-ID. While this paper proves the soundness of our proposed method,
we also conjecture that our approach is not only sound but also complete. Finally, by presenting an
appropriate reduction, we showed that solving S-ID can solve the S-Recoverability problem.
Acknowledgments
We thank the anonymous reviewers for their feedback. This research was in part supported by the
Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40_180545 and
Swiss SNF project 200021_204355 /1.
References
[AMK24] Amir Mohammad Abouei, Ehsan Mokhtarian, and Negar Kiyavash. s-id: Causal effect
identification in a sub-population. Proceedings of the AAAI Conference on Artificial
Intelligence , 38(18):20302‚Äì20310, Mar. 2024.
[BMS20] Rohit Bhattacharya, Daniel Malinsky, and Ilya Shpitser. Causal inference under interfer-
ence and network uncertainty. In Uncertainty in Artificial Intelligence , pages 1028‚Äì1038.
PMLR, 2020.
10[BP12] Elias Bareinboim and Judea Pearl. Controlling selection bias in causal inference. In
Neil D. Lawrence and Mark Girolami, editors, Proceedings of the Fifteenth International
Conference on Artificial Intelligence and Statistics , volume 22 of Proceedings of Machine
Learning Research , pages 100‚Äì108, La Palma, Canary Islands, 21‚Äì23 Apr 2012. PMLR.
[BT15] Elias Bareinboim and Jin Tian. Recovering causal effects from selection bias. Proceed-
ings of the AAAI Conference on Artificial Intelligence , 29(1), Mar. 2015.
[BTP14] Elias Bareinboim, Jin Tian, and Judea Pearl. Recovering from selection bias in causal
and statistical inference. Proceedings of the AAAI Conference on Artificial Intelligence ,
28(1), Jun. 2014.
[CLB21] Juan Correa, Sanghack Lee, and Elias Bareinboim. Nested counterfactual identification
from arbitrary surrogate experiments. Advances in Neural Information Processing
Systems , 34:6856‚Äì6867, 2021.
[CTB19] Juan D. Correa, Jin Tian, and Elias Bareinboim. Identification of causal effects in the
presence of selection bias. Proceedings of the AAAI Conference on Artificial Intelligence ,
33(01):2744‚Äì2751, Jul. 2019.
[HV06] Yimin Huang and Marco Valtorta. Identifiability in causal bayesian networks: A sound
and complete algorithm. In AAAI , pages 1149‚Äì1154, 2006.
[JAK23] Fateme Jamshidi, Sina Akbari, and Negar Kiyavash. Causal imitability under context-
specific independence relations. In A. Oh, T. Naumann, A. Globerson, K. Saenko,
M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems ,
volume 36, pages 26810‚Äì26830. Curran Associates, Inc., 2023.
[JRZB22] Amin Jaber, Adele Ribeiro, Jiji Zhang, and Elias Bareinboim. Causal identification under
markov equivalence: Calculus, algorithm, and completeness. In S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information
Processing Systems , volume 35, pages 3679‚Äì3690. Curran Associates, Inc., 2022.
[JZB19] Amin Jaber, Jiji Zhang, and Elias Bareinboim. Causal identification under Markov
equivalence: Completeness results. In Kamalika Chaudhuri and Ruslan Salakhutdi-
nov, editors, Proceedings of the 36th International Conference on Machine Learning ,
volume 97 of Proceedings of Machine Learning Research , pages 2981‚Äì2989. PMLR,
09‚Äì15 Jun 2019.
[KEK23] Yaroslav Kivva, Jalal Etesami, and Negar Kiyavash. On identifiability of conditional
causal effects. The 39th Conference on Uncertainty in Artificial Intelligence , 2023.
[KMEK22] Yaroslav Kivva, Ehsan Mokhtarian, Jalal Etesami, and Negar Kiyavash. Revisiting
the general identifiability problem. In Uncertainty in Artificial Intelligence , pages
1022‚Äì1030. PMLR, 2022.
[LCB19] Sanghack Lee, Juan David Correa, and Elias Bareinboim. General identifiability with
arbitrary surrogate experiments. In Conference on Uncertainty in Artificial Intelligence ,
2019.
[LGS24] Jaron J.R. Lee, AmirEmad Ghassami, and Ilya Shpitser. A general identification algo-
rithm for data fusion problems under systematic selection. In The 40th Conference on
Uncertainty in Artificial Intelligence , 2024.
[MJEK22] Ehsan Mokhtarian, Fateme Jamshidi, Jalal Etesami, and Negar Kiyavash. Causal effect
identification with context-specific independence relations of control variables. In
International Conference on Artificial Intelligence and Statistics , pages 11237‚Äì11246.
PMLR, 2022.
[Pea95] Judea Pearl. Causal diagrams for empirical research. Biometrika , 82(4):669‚Äì688, 1995.
[Pea00] Judea Pearl. Causality: Models, reasoning and inference. Cambridge, UK: Cambridge-
UniversityPress , 19(2):3, 2000.
11[Pea09] Judea Pearl. Causality . Cambridge university press, 2009.
[PM18] Judea Pearl and Dana Mackenzie. The book of why: the new science of cause and effect .
Basic books, 2018.
[Rub74] Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandom-
ized studies. Journal of educational Psychology , 66(5):688, 1974.
[SGSH00] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation,
prediction, and search . MIT press, 2000.
[SP06a] Ilya Shpitser and Judea Pearl. Identification of conditional interventional distributions.
Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence , 2006.
[SP06b] Ilya Shpitser and Judea Pearl. Identification of joint interventional distributions in
recursive semi-markovian causal models. In Proceedings of the National Conference
on Artificial Intelligence , volume 21, page 1219. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999, 2006.
[SS18] Eli Sherman and Ilya Shpitser. Identification and estimation of causal effects from
dependent data. Advances in neural information processing systems , 31, 2018.
[THK19] Santtu Tikka, Antti Hyttinen, and Juha Karvanen. Identifying causal effects via context-
specific independence relations. Advances in neural information processing systems , 32,
2019.
[THK21] Santtu Tikka, Antti Hyttinen, and Juha Karvanen. Causal effect identification from mul-
tiple incomplete data sources: A general search-based approach. Journal of Statistical
Software , 99(5), 2021.
[TP02] Jin Tian and Judea Pearl. A general identification condition for causal effects. In
Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI
2002) , pages 567‚Äì573, Menlo Park, CA, 2002. AAAI Press/The MIT Press.
[TP03] Jin Tian and Judea Pearl. On the identification of causal effects. Technical report,
Department of Computer Science, University of California, 2003.
[ZMP23] Chi Zhang, Karthika Mohan, and Judea Pearl. Causal inference with non-iid data under
model uncertainty. Proceedings of Machine Learning Research vol TBD , 1:14, 2023.
12Appendix
The structure of the appendix is as follows. Appendix A includes an additional example of S-ID
problem in the presence of latent variables. In Appendix B, we provide some preliminary lemmas
used throughout our proofs. The proofs for the main results, namely, Lemmas 4.4, 4.5, and Theorems
5.1, 6.1 are presented in Appendix C. In Appendix D, we will conduct an experiment to compare the
outputs of S-ID algorithm and the classic ID algorithm.
A Additional Example
U1
U2 S YX
Figure 6: ADMG GSof the example of Appendix A.
In this section, we provide an example where ignoring Sresults in an identifiable effect, whereas the
target causal effect is, in fact, not S-ID.
Consider the following two SCMs.
SCMM1:
U1‚àºBern (0.5)
U2‚àºBern (0.5)
Œµy‚àºBern (0.3)
X=U1
Y=X‚äïU2‚äïŒµy
S=U1‚äïU2
where‚äïdenotes the XOR operator, and Bern (p)denotes a Bernoulli random variable with parameter
p.
SCMM2:
U1‚àºBern (0.5)
U2‚àºBern (0.5)
Œµy‚àºBern (0.3)
X=U1
Y=Œµy
S= 1
According to the above equations, we have
PM1(X=x, Y=y|S= 1) = P(U1=x)P(Œµy=y) = 0.5√óP(Œµy=y)>0.
Similarly for M2we have
PM2(X=x, Y=y|S= 1) = PM2(X=x, Y=y)
=PM2(X=x)PM2(Y=y)
=P(U1=x)P(Œµy=y) = 0.5√óP(Œµy=y)>0.
Thus, PM1(X, Y|S= 1) = PM2(X, Y|S= 1)>0. Furthermore, we have
PM1
x=0(Y= 1|S= 1) = PM1
x=0(U2+Œµy= 1|S= 1) = PM1(U2+Œµy= 1) = 0 .5.
13Similarly, for SCM M2:
PM2
x=0(Y= 1|S= 1) = PM2(Œµy= 1|S= 1) = P(Œµy= 1) = 0 .3
This shows that PS
X(Y)is not S-ID in this causal graph, as PM1(X, Y|S= 1) = PM2(X, Y|S=
1)>0, butPM1
x=0(Y= 1|S= 1)Ã∏=PM2
x=0(Y= 1|S= 1) .
Note that ignoring the sub-population, the causal effect PX(Y)is clearly identifiable from P(X, Y),
but as we showed above, the causal effect of XonYis not identifiable in the sub-population from
the observational data of that sub-population.
B Technical Preliminaries
Pearl‚Äôs do-calculus rules [ Pea00 ]: LetX,Y,Z,Wbe four disjoint subsets of V. The following
three rules, commonly referred to as Pearl‚Äôs do-calculus rules [ Pea00 ], provide a tool for calculating
interventional distributions using the causal graph.
‚Ä¢Rule 1 : If(Y‚ä• ‚ä•Z|X,W)GX, then
PX(Y|Z,W) =PX(Y|W).
‚Ä¢Rule 2 : If(Y‚ä• ‚ä•Z|X,W)GXZ, then
PX,Z(Y|W) =PX(Y|Z,W).
‚Ä¢Rule 3 : If(Y‚ä• ‚ä•Z|X,W)GXZ(W), where Z(W):=Z\AncGX(W), then
PX,Z(Y|W) =PX(Y|W).
Lemma B.1 (TP03) .For two sets W‚Ä≤‚äÇW, ifW‚Ä≤is an ancestral set in G[W], then
Q[W‚Ä≤] =X
W\W‚Ä≤Q[W]. (9)
Lemma B.2 (TP03 ).LetC‚äÜV, and assume that C is partitioned into C-components C1, . . . ,Cm
in the subgraph G[C]. Then we have
‚Ä¢Q[C]decomposes as
Q[C] =mY
i=1Q[Ci] (10)
‚Ä¢Letkdenote the number of variables in H, and let us assume a topological order of variables
inCasVc1< Vc2<¬∑¬∑¬∑< VckinGC. LetCibe the set of variables in Cordered before
Vci(including Vci), for i= 1,2, . . . , k , where C0is an empty set. Then each Q[Cj],
j= 1,2, . . . , m , is computable from Q[C]and is given by
Q[Cj] =Y
{i|Vci‚ààCj}Q[C(i)]
Q[C(i‚àí1)], (11)
where each Q[Ci]is given by
Q[C(i)] =X
C\C(i)Q[C]. (12)
Corollary B.3. LetH1‚äîH2‚äî ¬∑¬∑¬∑ ‚äî Hmbe a partition of set C, where for each Cithere exists
1‚â§j‚â§msuch that Ci‚äÜHj. Then, we have
Q[C] =Y
jQ[Hj].
Lemma B.4. LetHbe a subset of VNS, we have the following equation
QS[H] =Q[H‚à™Anc(S)]
Q[Anc(S)]. (13)
14Proof. According to definition of QS, we have
QS[H] =PVNS\H(H|Anc(S)) =PVNS\H(H,Anc(S))
PVNS\H(Anc(S))=Q[H‚à™Anc(S)]
PVNS\H(Anc(S)).
Note that Anc(S)is an ancestral set in G; Hence, for any H, we have
PVNS\H(Anc(S)) =P(Anc(S)) =Q[Anc(S)].
This implies Equation (13).
Lemma B.5. LetXNSandYNSbe disjoint subsets of VNS=V\AncGS(S). We have
PXNS(YNS|Anc(S)) =X
D\YNSQS[D1]. . . QS[Dk],
whereD=AncG[VNS\XNS](YNS), andDi‚Äôs are S-components of D.
Proof. Using marginalization over VNS\(XNS‚à™YNS), we have
PXNS(YNS|Anc(S)) =X
VNS\(XNS‚à™YNS)PXNS(VNS\XNS|Anc(S)) =X
VNS\(XNS‚à™YNS)QS[VNS\XNS].
SinceDis an ancestral set in G[VNS\XNS], according to Lemma 4.4, we have
PXNS(YNS|Anc(S)) =X
VNS\(XNS‚à™YNS)QS[VNS\XNS] =X
D\YNSX
VNS\(XNS‚à™D)QS[VNS\XNS] =X
D\YNSQ[D].
Therefore, the first property of Lemma 4.5 implies that
PXNS(YNS|Anc(S)) =X
D\YNSQS[D1]. . . QS[Dk].
Lemma B.6. If Function sID-Single returns FAILfor the inputs CandT, thenTis an S-Hedge for
C.
Proof. When the algorithm returns Fail,
1.CandTare both single S-components since the inputs of the algorithm have to be S-
components.
2. IfA:=AncG[T](C), thenT=A.
According to the definition of s-Hedge 4.6, Tis an s-Hedge for C.
C Proofs of Main Results
Lemma 4.4. LetW,W‚Ä≤be two subsets of VNSsuch that W‚Ä≤‚äÇW. IfW‚Ä≤is an ancestral set in
G[W], then we have
QS[W‚Ä≤] =X
W\W‚Ä≤QS[W]. (14)
Proof. According to Lemma B.4, we have the following equations
QS[W] =Q[W‚à™Anc(S)]
Q[Anc(S)],
QS[W‚Ä≤] =Q[W‚Ä≤‚à™Anc(S)]
Q[Anc(S)].
15Therefore, by replacing the above equations in Equation (1), it is sufficient to show that
Q[W‚Ä≤‚à™Anc(S)]
Q[Anc(S)]=X
W\W‚Ä≤Q[W‚à™Anc(S)]
Q[Anc(S)].
Since P(Anc(S))>0, this is equivalent to
Q[W‚Ä≤‚à™Anc(S)] =X
W\W‚Ä≤Q[W‚à™Anc(S)].
Note that W‚Ä≤‚à™Anc(S)in an ancestral set in G[W‚à™Anc(S)], andW\W‚Ä≤= (W‚à™Anc(S))\
(W‚Ä≤‚à™Anc(S)). Hence, Lemma B.1 concludes the proof.
Suppose H‚äÜVNSand letH1, . . . ,Hkdenote the S-components of HinGS. Then,
‚Ä¢QS[H]decomposes as
QS[H] =QS[H1]QS[H2]. . . QS[Hk].
‚Ä¢Letmbe the number of variables in H, and consider a topological ordering of the variables
in graph GS[H], denoted as Vh1<¬∑¬∑¬∑< Vhm. LetH(0)=‚àÖand for each 1‚â§i‚â§m,H(i)
denote the set of variables in Hordered before Vhi(including Vhi). For every 1‚â§j‚â§k,
QS[Hj]can be computed from QS[H]by
QS[Hj] =Y
{i|Vhi‚ààHj}QS[H(i)]
QS[H(i‚àí1)], (15)
where QS[H(i)]s can be computed by
QS[H(i)] =X
H\H(i)QS[H]. (16)
Proof. First part: according to the definition of QS[.], we have
QS[H] =PVNS\H(H|Anc(S)) =PVNS\H(H,Anc(S))
PVNS\H(Anc(S))=Q[H‚à™Anc(S)]
Q[Anc(S)].
Note the last equality holds because Anc(S)is an ancestral set in G. Now, we have
Q[Anc(S)] =PV\Anc(S)(Anc(S)) =P(Anc(S)) =PVNS\H(Anc(S)).
Similarly, for each i‚àà[1 :k], we have
QS[Hi] =Q[Hi‚à™Anc(S)]
Q[Anc(S)]
Therefore, the above equations imply that
QS[H] =QS[H1]QS[H2]. . . QS[Hk]‚áê‚áíQ[H‚à™Anc(S)]
Q[Anc(S)]=Q[H1‚à™Anc(S)]
Q[Anc(S)]√ó¬∑¬∑¬∑√óQ[Hk‚à™Anc(S)]
Q[Anc(S)]
(17)
LetCi‚Äôs be the corresponding C-component of Hi(i.e.,Hi‚äÜCiandCiis a C-component of
Hi‚à™Anc(S)). According to the definition of Ciand Corollary B.3 we have
Q[Hi‚à™Anc(S)] =Q[Ci]Q[Anc(S)\Ci].
Moreover, since Ciis a C-component of Anc(S)‚à™Hi, thenCi\Hishould be union of some
C-components of Anc(S). Therefore, for each i, Corollary B.3 implies
Q[Anc(S)] =Q[Ci\Hi]Q[Anc(S)\(Ci\Hi)] =Q[Ci\Hi]Q[Anc(S)\Ci]
16Putting the above equations together, we have
QS[Hi] =Q[Hi‚à™Anc(S)]
Q[Anc(S)]=Q[Ci]Q[Anc(S)\Ci]
Q[Ci\Hi]Q[Anc(S)\Ci]=Q[Ci]
Q[Ci\Hi]. (18)
Hence,
kY
i=1Q[Hi‚à™Anc(S)]
Q[Anc(S)]=kY
i=1Q[Ci]
Q[Ci\Hi].
Consequently, we have
QS[H] =QS[H1]QS[H2]. . . QS[Hk]‚áê‚áí Q[H‚à™Anc(S)] =Q[Anc(S)]Q
iQ[Ci\Hi]kY
i=1Q[Ci].
Note that Ci\Hiare disjoint subsets of Anc(S), where each of them is the union of some C-
components of Anc(S). IfCk+1:=Anc(S)\S
iCi, then Corollary B.3 implies the following.
Q[Anc(S)] =kY
i=1Q[Ci\Hi]Q[Ck+1] =‚áíQ[Anc(S)]
Qk
i=1Q[Ci\Hi]=Q[Ck+1]
Finally, applying Corollary B.3 for H‚à™Anc(S), we have
Q[H‚à™Anc(S)] =Q[Ck+1]kY
i=1Q[Ci].
This concludes the first part.
Second part: the proof is similar to proof of Lemma B.2. Define C:=H‚à™Anc(S). LetC1, . . . ,Cm
beC-components of C(w.l.o.g. assume that Hi‚äÜCifori‚àà[1 :k]). Consider a topological order of
Vh1< Vh2<¬∑¬∑¬∑< VhnforH, andVs1<¬∑¬∑¬∑< Vsn‚Ä≤forAnc(S). Since any Vhiis not an ancestor
ofAnc(S),Vs1<¬∑¬∑¬∑< Vsn‚Ä≤< Vh1< Vh2<¬∑¬∑¬∑< Vhnis a topological order for H‚à™Anc(S).
According to Lemma B.2, we have
Q[Cj] =Y
{i|Vci‚ààCj}Q[C(i)]
Q[C(i‚àí1)],
where (c1, c2, . . . , c n+n‚Ä≤) = ( s1, . . . , s n‚Ä≤, h1, . . . , h n). For each i‚àà[1 : n], letHi:=
{Vh1, Vh2, . . . , V hi}andH0=‚àÖ. For each i >0, we have
Q[C(i+n‚Ä≤)]
Q[C(i+n‚Ä≤‚àí1)]=Q[Hi‚à™Anc(S)]
Q[H(i‚àí1)‚à™Anc(S)]=Q[Hi‚à™Anc(S)]
Q[Anc(S)]
Q[H(i‚àí1)‚à™Anc(S)]
Q[Anc(S)]=QS[Hi]
QS[H(i‚àí1)],
where the last equality holds according to Lemma B.4. It suffices to show that
QS[Hj] =Y
{i|Vhi‚ààHj}QS[H(i)]
QS[H(i‚àí1)].
Equation 18 implies that
QS[Hj] =Q[Cj]
Q[Cj\Hj].
Hence, according to this equation and Lemma B.2, we have
Q[Cj\Hj]QS[Hj] =Q[Cj] =Y
{i|Vci‚ààCj}Q[C(i)]
Q[C(i‚àí1)]
=Y
{i|Vci‚ààCj‚à©Anc(S)}Q[C(i)]
Q[C(i‚àí1)]Y
{i|Vhi‚ààHj}QS[H(i)]
QS[H(i‚àí1)]
=Y
{i|Vsi‚ààCj\Hj}Q[C(i)]
Q[C(i‚àí1)]Y
{i|Vhi‚ààHj}QS[H(i)]
QS[H(i‚àí1)]. (19)
17Based on the definition CjandHj,Cj\Hjis the union of some C-components of Anc(S). Denote
the c-components of Cj\HjbyA1, . . . , A t. Applying Lemma B.2 to C=Anc(S)with topological
order Vs1<¬∑¬∑¬∑< Vsn‚Ä≤, we have
Q[A1] =Y
{i|Vsi‚ààA1}Q[C(i)]
Q[C(i‚àí1)]
...
Q[At] =Y
{i|Vsi‚ààAt}Q[C(i)]
Q[C(i‚àí1)]
By multiplying all Q[Ai], we have
Q[Cj\Hj] =Q[A1‚à™A2¬∑¬∑¬∑ ‚à™ At] =tY
t‚Ä≤=1Q[At‚Ä≤] =Y
{i|Vsi‚ààCj\Hj}Q[C(i)]
Q[C(i‚àí1)].
By substituting this in Equation 19, we obtain
QS[Hj] =Y
{i|Vhi‚ààHj}QS[H(i)]
QS[H(i‚àí1)].
To complete the proof, it suffices to show that QS[H(i)]are computable from QS[H]. Since we have
used the topological order, Hiis an ancestral set in G[H]. Therefore, Lemma 4.4 implies that
QS[Hi] =X
H\HiQS[H].
This shows that QS[Hi]is uniquely computable from QS[H].
Theorem 5.1. For disjoint subsets XandYofV, let
XAS:=X‚à©VAS,XNS:=X‚à©VNS,andYNS:=Y‚à©VNS.
1. Conditional causal effect PS
X(Y)isS-ID in GSif and only if
(XAS‚ä• ‚ä•Y|XNS, S)GS
XASXNS, (20)
andPS
XNS(Y,XAS)isS-ID in GS.
2.Suppose D:=AncGS[VNS\XNS](YNS)and let {Di}k
i=1denote the S-components of DinGS.
Conditional causal effect PS
XNS(Y,XAS)isS-ID inGSif there are no S-Hedge in GSfor any
of{Di}k
i=1.
Proof. We first show that (XAS‚ä• ‚ä•Y|XNS, S)GS
XASXNSis a necessary condition. Suppose (XASÃ∏‚ä•
‚ä•Y|XNS, S)GS
XASXNS. Denote by G‚Ä≤the equivalent DAG of GS, obtained by adding the unobserved
variables. Theorem 2 in [ AMK24 ] shows that there are two SEMs M1andM2compatible with G‚Ä≤
such that
PM1(V,U|S= 1) = PM2(V,U|S= 1),and
PM1
X(Y|S= 1)Ã∏=PM2
X(Y|S= 1).
We use these SEMs to construct SCMs M‚Ä≤
1andM‚Ä≤
2compatible with GS, in which we have
PM‚Ä≤
1(V,U|S= 1) = PM‚Ä≤
2(V,U|S= 1) = ‚áíPM‚Ä≤
1(V|S= 1) = PM‚Ä≤
2(V|S= 1).
Note that all causal effects in both models are the same for GSandG. Hence, when (XASÃ∏‚ä•
‚ä•Y|XNS, S)GS
XASXNSholds, then PS
X(Y)is not S-ID. It shows that this condition is a necessary
condition.
18Now suppose that (XAS‚ä• ‚ä•Y|XNS, S)GXASXNSholds. According to Rule 2 of do-calculus, we have
PX(Y|S= 1) = PXNS(Y|XAS, S= 1)
The above equation shows that PX(Y|S= 1) is s-ID in GSif and only if PXNS(Y|XAS, S= 1) is
s-ID in GS. Moreover,
PXNS(Y|XAS, S= 1) =PXNS(Y,XAS|S= 1)
PXNS(XAS|S= 1).
SinceXNS‚à©VAS=‚àÖ, acccording to Rule 3 of do-calculus, we have PXNS(XAS|S= 1) =
P(XAS|S= 1) . Hence,
PXNS(Y|XAS, S= 1) =PXNS(Y,XAS|S= 1)
P(XAS|S= 1). (21)
This shows that S-Identifiability of PXNS(Y|XAS, S= 1) andPXNS(Y,XAS|S= 1) are equivalent
(note that P(XAS|S= 1)>0due to the positivity assumption in Definition 4.1).
LetW:=VAS\(XAS‚à™YAS), then
PXNS(Y|XAS, S= 1) =X
WPXNS(Y,W|XAS, S= 1)
=X
WPXNS(YAS,W|XAS, S= 1)PXNS(YNS|XAS,YAS,W, S= 1)
=X
WPXNS(YAS,W|XAS, S= 1)PXNS(YNS|VAS, S= 1).
The above equalities have been obtained by a marginalization over W, chain rule, and replacing the
definition of W, respectively. According to Rule 3 of do-calculus, since XNS‚à©VAS=‚àÖ, we have
PXNS(YAS,W|XAS, S= 1) = P(YAS,W|XAS, S= 1).
Moreover, according to Lemma B.5, we have
PXNS(YNS|VAS, S= 1) =X
D\YNSQS[D1]. . . QS[Dk].
Combining the aforementioned equations with Equation (21), we get
PXNS(Y,XAS|S= 1) =X
WP(XAS,YAS,W|S= 1)X
D\YNSQS[D1]. . . QS[Dk]. (22)
Now, note that according to Lemma B.6, if there is not any S-Hedge for Dis, then Function sID-Single
will compute QS[Di]s. Therefore, we can compute PXNS(Y,XAS|S= 1) using the above equation,
which concludes the proof. As a result, if Equation (5) holds, we have
PS
X(Y) =PS
XNS(Y|XAS) =X
WP(YAS,W|XAS, S= 1)X
D\YNSQS[D1]. . . QS[Dk]. (23)
Theorem 6.1. For disjoint subsets XandYofV,PX(Y)can be uniquely computed from PS(V)
in the augmented ADMG GSif and only if
(Y‚ä• ‚ä•S|X)GS
X,
andPS
X(Y)isS-ID in GS.
Proof. If(YÃ∏‚ä• ‚ä•S|X)GX, then [ BT15 , Theorem 2] implies that PX(Y)is not S-recoverable. If
(Y‚ä• ‚ä•S|X)GX, then according to Rule 1 of do-calculus, we have
PX(Y|S= 1) = PX(Y). (24)
Therefore, the identifiability of PX(Y)is equivalent to PX(Y|S= 1) , which concludes the proof.
19U1 X Z
U2 S Y
Figure 7: A DAG, where U1andU2are unobservable, and Srepresents the auxiliary variable for
modeling a sub-population.
D Numerical Experiment
We conduct a numerical experiment to demonstrate the significance of the s-ID problem and assess
the output of Algorithm 1. Note that the experiment is simple and can be run on a system with any
level of computational power.
Consider the following structural causal model (SCM) with the causal graph depicted in Figure 7.
U1‚àºBern (0.5)
U2‚àºBern (0.5)
X=U1‚äïŒµx, Œµx‚àºBern (0.2)
Y=X‚äïU2
Z=U1‚äïŒµz, Œµz‚àºBern (0.2)
Herein, ‚äïdenotes the XOR operation, all the variables {U1, U2, Œµx, Œµz, Œµs1, Œµs2, Œµs3}are indepen-
dent, and Bern (p)denotes a Bernoulli random variable with parameter p. Now, consider the
sub-population with the following mechanism:
S= (Z√óŒµs1)‚äï(U2√óŒµs2)‚äïŒµs3,(Œµs1, Œµs2, Œµs3)‚àº(Bern (0.6), Bern (0.9), Bern (0.1))
We now consider the problem of estimating the causal effect of XonYin this sub-population.
Particularly, our goal is to calculate PX=0(Y= 1|S= 1) .
D.1 Theoretical Analysis
To analyze and compare the S-ID and ID algorithms, we first determine the exact values of PX=0(Y=
1|S= 1) andPX=0(Y= 1) . According to the equation of Yin the SCM, we have
PX=x(Y=y) =PX=x(y=x‚äïU2) =P(U2=y‚äïx).
Since U2is a Bernoulli random variable with parameter 0.5, the above probability always equals 0.5.
ForPX(Y|S= 1) , we have
PX=x(Y=y|S= 1) = PX=x(Y=x‚äïU2|S= 1) = P(U2=x‚äïy|S= 1).
Therefore, for X= 0andY= 1, we need to compute P(U2= 1|S= 1) . By using the equation of
Sin the model,
P(U2= 1|S= 1) =P(U2= 1, S= 1)
P(U2= 1, S= 1) + P(U2= 0, S= 1)
=P(U2= 1)P(S= 1|U2= 1)
P(U2= 0)P(S= 1|U2= 0) + P(U2= 1)P(S= 1|U2= 1)
Note that P(U2= 0) = P(U2= 1) = 0 .5, hence, we have
P(U2= 1|S= 1) =P(S= 1|U2= 1)
P(S= 1|U2= 0) + P(S= 1|U2= 1)
Since Œµs1andŒµs3andZare independent variables, let WbeZ√óŒµs1‚äïŒµs3; then, we have
20W‚àºBern (0.1√ó(1‚àí0.6√ó0.5) + 0 .9√ó0.5√ó0.6) =Bern (0.34).
Note that WandU2√óŒµs2are independent, and S=W+U2√óŒµs2; thus,
P(S= 1|U2= 0) = P(W= 1) = 0 .34
P(S= 1|U2= 1) = P(W= 1)P(Œµs2= 0) + P(W= 0)P(Œµs2= 1)
= 0.34√ó0.1 + 0.66√ó0.9 = 0 .628
Hence,
PX=0(Y= 1|S= 1) =0.628
0.628 + 0 .34‚âà0.648 (25)
D.2 Empirical Analysis
The ID algorithm returns the following simple expression for PX(Y)
PX(Y) =P(Y|X). (26)
On the other hand, the s-ID algorithm returns
PX(Y|S= 1) =X
ZP(Y|X, Z, S = 1)P(Z|S= 1). (27)
Next, we generated 3000 samples from the population. We then computed Sfor each generated
sample and collected the samples where S= 1, resulting in 1469 available samples from our target
sub-population. Recall that our goal was to estimate PX=0(Y= 1|S= 1) . Consider the following
two approaches.
‚Ä¢If we consider the s-ID algorithm and the existence of the selection bias S, applying the
simple plug-in estimator to the formula in (27) results in
PX=0(Y= 1|S= 1)‚âàÀÜP(Y= 1|X= 0, Z= 0)ÀÜP(Z= 0)
+ÀÜP(Y= 1|X= 0, Z= 1)ÀÜP(Z= 1) = 0 .641 (28)
‚Ä¢Suppose we ignore the presence of sub-population and apply the ID algorithm. In this
scenario, we have to estimate PX(Y)using the empirical distribution of variables, i.e.,
ÀÜP(V). If we use the ID algorithm, we need to estimate the quantity mentioned in equation
(26). The empirical estimate for this case is
PX(Y)‚âàÀÜP(Y|X) = 0.725. (29)
A comparison between the estimation results of our proposed method in Equation 28 and the classical
ID problem in Equation (29) with the true underlying value in Equation (25) shows that our approach
accurately computes the target causal effect. On the other hand, ignoring the subtleties related to
sub-population can lead to erroneous estimation.
21NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: We provided detailed explanations for all claims, including the new graph
structures and their properties in Section 4. In Section 5, we presented our algorithm for
solving the s-ID problem.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We concretely discussed the problem setting in Sections 2 and 4. We also
analyzed our proposed algorithm and its properties in Section 5.
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
22Answer: [Yes]
Justification: We included all the assumptions in the main part of the paper. We also proved
all our results in Appendices B and C.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All steps of the conducted experiment are detailed in Appendix D. We utilize
synthetic data corresponding to a causal model defined in Appendix D. Hence, the data
generation process is straightforward. After that, we only need some estimation of certain
distributions from this data to reproduce the results.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
235.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: As discussed in the previous question, our numerical experiment consists of
a few simple steps. One can generate data according to the defined causal model in our
experiment and reproduce the results. Hence, we have not provided the code.
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We thoroughly elaborated on all the steps of our experiment in Appendix D.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Our experiment aims to compare the outcomes of our algorithm and the ID
algorithm, which provide estimands for causal effects. We focus on scenarios with an
infinite sample size, eliminating the need to report error rates. Further details can be found
in Appendix D.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
24‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: In Appendix D, we note that our numerical experiment does not need specific
computational power.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper adheres fully to the NeurIPS Code of
Ethics.
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [No]
25Justification: We think there is no societal impact as our work primarily focuses on theoretical
analysis of a problem in causal effect identification.
Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The focus of our paper is the theoretical understanding of a specific problem
in the area of causal inference. Therefore, this question is not applicable.
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The focus of our paper is the theoretical understanding of a specific problem
in the area of causal inference. Therefore, this question is not applicable.
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
26‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The focus of our paper is the theoretical understanding of a specific problem
in the area of causal inference. Therefore, this question is not applicable.
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The focus of our paper is the theoretical understanding of a specific problem
in areas of causal inference. Therefore, this question is not applicable.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
27Justification: The focus of our paper is the theoretical understanding of a specific problem
in the area of causal inference. Therefore, this question is not applicable.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
28