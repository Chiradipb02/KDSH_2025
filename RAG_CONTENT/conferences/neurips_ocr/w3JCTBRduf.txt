Optimization Can Learn Johnson Lindenstrauss
Embeddings
Nikos Tsikouras
UOA & Archimedes / Athena RC
n.tsikouras@athenarc.grConstantine Caramanis
UT Austin & Archimedes / Athena RC
constantine@utexas.edu
Christos Tzamos
UOA & Archimedes / Athena RC
christos@tzamos.com
Abstract
Embeddings play a pivotal role across various disciplines, offering compact rep-
resentations of complex data structures. Randomized methods like Johnson-
Lindenstrauss (JL) provide state-of-the-art and essentially unimprovable theoretical
guarantees for achieving such representations. These guarantees are worst-case
and in particular, neither the analysis, nor the algorithm , takes into account any
potential structural information of the data. The natural question is: must we ran-
domize? Could we instead use an optimization-based approach, working directly
with the data? A first answer is no: as we show, the distance-preserving objective
of JL has a non-convex landscape over the space of projection matrices, with many
bad stationary points. But this is not the final answer.
We present a novel method motivated by diffusion models, that circumvents this
fundamental challenge: rather than performing optimization directly over the space
of projection matrices, we use optimization over the larger space of random solution
samplers , gradually reducing the variance of the sampler. We show that by moving
through this larger space, our objective converges to a deterministic (zero variance)
solution, avoiding bad stationary points.
This method can also be seen as an optimization-based derandomization approach
and is an idea and method that we believe can be applied to many other problems.
1 Introduction
Embeddings are foundational across diverse disciplines, offering compact representations of complex
data structures. Algorithms across different domains leverage embeddings to capture nuanced
relationships between data points, improving efficiency and effectiveness in processing. Within this
field, there are distinct lines of work.
Embeddings have been used as dimensionality-reducing tools, while preserving important structures
in data. They have been a major research focus for years, and have been a key ingredient in several
algorithmic applications such as graph sparsification [Spielman and Srivastava, 2008], nearest-
neighbor search [Indyk and Motwani, 1998], hashing [Dietzfelbinger et al., 1997] and digital research
[Schmidt, 2018]. A celebrated result in this area has been the Johnson-Lindenstrauss (JL) lemma
Johnson [1984] which shows that a random linear mapping can reduce the dimension of the dataset
while approximately preserving the L2norm of all points, with high probability. JL embeddings
give bounds on the maximum distortion over all the points. Many variants have been developed and
studied, as we discuss below in Section 2. For this paper, the salient point is that the algorithms that
38th Conference on Neural Information Processing Systems (NeurIPS 2024).enjoy theoretical guarantees are for random projections drawn from a distribution over projection
matrices. To the best of our knowledge, even among derandomization of JL-type techniques (e.g.,
Clarkson and Woodruff [2009], and see Section 2 for further discussion), there are no guarantees for
optimization-based algorithms that directly attempt to minimize a distortion objective.
On the other hand, in the last decade, embeddings have risen to prominence across various tasks in
deep learning. In encoder-decoder architectures, embeddings act as intermediary representations to
solve a broad range of challenges in natural language processing and speech processing, [Bengio
and Heigold, 2014, Cho et al., 2014a,b, Qi et al., 2018, Rush et al., 2015, Xiong et al., 2016]. In the
domain of face recognition, embeddings encode facial features into concise vectors, enabling precise
identification and matching [Liu et al., 2015, 2017, Schroff et al., 2015]. Furthermore, contrastive
learning techniques utilize embeddings to emphasize disparities between similar and dissimilar
instances, thereby augmenting the discriminative capabilities of models [Gao et al., 2021, Khosla
et al., 2020, Radford et al., 2021].
In contrast to the JL-type results, in the above applications these embeddings are learned using
optimization as part of the (pre)training process [Caron et al., 2021, Oquab et al., 2023, Press and
Wolf, 2016, Vaswani et al., 2017]. Though their empirical success is remarkable, the non-convex
landscape of the optimization process makes obtaining theoretical guarantees a key challenge.
Another important related area is the extensive body of literature dedicated to employing optimization
techniques for Principal Component Analysis (PCA). PCA aims to find linear embeddings that capture
the optimal average distortion in the data, thereby reducing its dimensionality while preserving as
much information as possible. The last decade saw significant success of direct matrix optimization
in various PCA-like settings [Garber and Hazan, 2015, Shamir, 2016, Xu and Li, 2021, Yi et al.,
2016]. Though clearly there are similarities to the JL objective, the difference between guarantees on
the maximum perturbation (JL) vs the average perturbation (PCA) are significant, and one reason
why the techniques pioneered for PCA have yet to be applied successfully to JL.
Nevertheless, the empirical success of optimization in neural networks, and its theoretical success for
PCA-type objectives, motivate us to revisit the JL low-distortion embedding task (see Section 3 for
the exact definition of the JL guarantee ), and ask the question that to the best of our knowledge, has
yet to find an answer:
Can optimization-based approaches be used to obtain a matrix
that satisfies the Johnson-Lindenstrauss guarantee?
Answering this question is the main goal of this paper.
Our Contributions and a Conceptual Road Map.
The main contribution of this paper is in developing a framework that allows a direct (and determin-
istic) optimization approach to obtain the same JL guarantees as random projection. After all, it is
well-known that the JL guarantees given by random projection are not improvable [Larsen and Nelson,
2014, 2017, Alon and Klartag, 2017]. Moreover, established derandomization techniques based on
conditional expectation and other methods have long been available [Raghavan, 1988, Engebretsen
et al., 2002, Bhargava and Kosaraju, 2005].
The key conceptual steps on the way to our main result are as follows:
Step 1: We first show that the optimization landscape in the ambient space of projection matrices is
not favorable, and in particular, any attempt to directly minimize distortion over this space using first
or second-order methods, is destined to fail.
Theorem (Informal version of Theorem 1) .The maximum distortion objective considered as a
function in the space of matrices has many suboptimal local minima.
Step 2: Given the above negative result, a different approach is required. We draw inspiration from
diffusion models and solution samplers [Bello et al., 2016, Ho et al., 2020]. Rather than optimize in
the space of matrices, we optimize in the larger space of (mean, variance) parameters of Gaussian
distributions over embedding matrices. Thanks to the original JL theorem, we know an initial choice
of parameters that define a solution sampler whose expected distortion is small: zero mean and unit
variance. Akin to diffusion, we then seek to sequentially decrease the magnitude of the variance,
without increasing the expected distortion of the sampler. Note that the space of matrices is properly
2contained in this space of samplers, as we can identify a specific (deterministic) projection matrix
with a Gaussian distribution with that mean, and zero variance. The challenge is to find a path from
our initial JL sampler, to a deterministic sampler (a projection matrix) whose maximum distortion is
approximately as good as the expected guarantee of the original JL sampler.
We turn this into an optimization problem by creating an objective function in the space of samplers.
Our first result demonstrates that if we find a second-order stationary point for this objective function,
then we have solved our original problem. Specifically:
Theorem (Informal Version of Theorem 2) .For data x1, . . . , x n∈Rdand target dimension k
all second-order stationary points reachable from the origin for the objective function defined in
Equation 4 have zero variance and hence correspond to fixed matrices; moreover, these matrices
satisfy the JL guarantee.
Step 3 : The final step requires proving the tractability of finding a second-order stationary point,
i.e., of solving this optimization problem. We do so using a generic deterministic second-order opti-
mization algorithm (see Alg. 1). Thus our result shows that our second-order optimization algorithm
successively performs reverse-diffusion-like steps, decreasing the variance without deteriorating the
quality of the solution sampler, until it has finally arrived at a deterministic solution.
Theorem (Informal Version of Theorem 3) .For data x1, . . . , x n∈Rdand target dimension k
running Algorithm 1 using Equation 4 for poly(n, k, d )steps returns a matrix that satisfies the JL
guarantee.
Step 4 : Finally, we show through simulations that the qualitative and quantitative results of our theory
are borne out.
2 Related Work and Alternative Approaches
The JL lemma is a well-studied result studied in the literature, with several simplifications and
extensions of the original proof [Dasgupta and Gupta, 2003, Frankl and Maehara, 1988, Kane and
Nelson, 2010, Matoušek, 2008]. It has also been shown that the JL lemma is optimal in terms of the
target dimension. The authors in [Larsen and Nelson, 2017, Alon and Klartag, 2017] provide a tight
lower bound on the target dimension required by the JL lemma, given a specific distortion, for any
random linear mapping. In addition to these theoretical insights, there have been various approaches
aimed at efficiently constructing random matrices that satisfy the JL guarantee with high probability.
One approach samples each matrix entry independently from a Gaussian distribution [Indyk and
Motwani, 1998], while others utilize Rademacher and sparse Rademacher distributions [Arriaga and
Vempala, 2006, Achlioptas, 2001]. Moreover, generalized sampling methods have shown that any
distribution with zero mean, unit variance, and a subgaussian tail can be used [Matoušek, 2008]. The
Fast Johnson–Lindenstrauss Transform employs sparse matrices and randomised Walsh–Hadamard
transforms for efficiency [Ailon and Chazelle, 2009]. Additionally, the subsampled randomised
Hadamard transform, achieves efficient embeddings by combining subsampling with randomised
Hadamard transforms, maintaining a high probability of preserving the distances between points
[Ailon and Liberty, 2013].
Derandomizing Johnson-Lindenstrauss. Derandomization is a technique for developing determin-
istic algorithms or algorithms that require fewer random bits, and it has proven to be a powerful
theoretical tool [Kabanets, 2002]. There have been numerous efforts to derandomize the JL lemma,
with a significant focus on using pseudorandom generators (PRGs) capable of fooling statistical
tests [Nisan, 1990]. These constructions aim to achieve reduced seed lengths while satisfying the JL
lemma’s norm-preserving properties with ±εdistortion and probability of failure δ. For example,
theℓ2-streaming algorithm achieves a JL family with seed length O(logd)and with k=O(1/(ε2δ))
[Alon et al., 1996]. The authors in [Clarkson and Woodruff, 2009] leveraged the use of scaled
random Bernoulli matrices with Ω(log(1 /δ))-wise independent entries, resulting in a seed length of
O(log(1 /δ) logd). Additionally, PRGs that δ-fool degree-2 polynomial threshold functions generate
JL families with seed lengths of poly (1/δ) logd[Meka and Zuckerman, 2010].
Other approaches have utilized conditional probabilities and pessimistic estimators, introduced in
[Raghavan, 1988], to derive deterministic algorithms for JL embeddings [Engebretsen et al., 2002,
Bhargava and Kosaraju, 2005].
3These works differ from ours in several ways. The Nisan pseudorandom generator uses a few
random bits. Additionally, the authors in [Bhargava and Kosaraju, 2005] fully derandomize the
Rademacher construction by [Achlioptas, 2001] using a combinatorial algorithm that greedily selects
the best matrix entries. Even in such a coordinate-wise fashion, using a gradient-descent (continuous-
optimization) approach is challenging, as even then the optimization landscape is bimodal.
Overall, the key difference in philosophy, setting and ultimately results, comes from our focus on
optimization: our method is a study in the power of local (first and second-order) optimization
methods.
3 Preliminaries and Notation
In this section, we introduce essential definitions and notation for our work. We consider without loss
of generality unit norm data points x1, . . . , x n∈Rd, which we aim to project into kdimensions while
preserving their norms with distortion at most ε=O(p
logn/k). Specifically, we seek matrices that
satisfy the Johnson-Lindenstrauss guarantee :
Definition 1 (Johnson-Lindenstrauss guarantee) .The Johnson-Lindenstrauss guarantee (JL guar-
antee) states that for given dataset x1, . . . , x n∈Rdand target dimension k, the distortion for all
points does not exceed O(p
logn/k).
To achieve this we define a linear mapping f(x) =Ax, where A∈Rk×d. The JL Lemma guarantees
that there exists a random linear mapping that achieves this projection with high probability:
Lemma 1 (Distributional Johnson-Lindenstrauss Lemma) .Forε, δ∈(0,1)andk=
O(log(1 /δ)/ε2), there exists a probability distribution Dover linear functions f:Rd→Rk
such that for every x∈Rd:
Pr
f∼D 
∥f(x)∥2
2∈
(1−ε)∥x∥2
2,(1 +ε)∥x∥2
2
≥1−δ.
There has been significant research aimed at improving the construction of these random mappings.
In contrast to traditional algorithms, our approach proposes learning the linear mapping directly from
the data, leveraging the inherent structure to surpass worst-case performance.
Next, we give essential definitions for our optimization framework.
Definition 2. A function f:Rd→Ris defined to be L-smooth, if for all x, y∈Rdit satisfies:
∥∇f(x)− ∇f(y)∥2≤L∥x−y∥2.
A function is called K−Hessian Lipschitz if for all x, y∈Rd:
∥∇2f(x)− ∇2f(y)∥2≤K∥x−y∥2.
Below, we give the definition for approximate stationarity.
Definition 3. (Approximate stationarity). For a K−Hessian Lipschitz function f(·), we say that a
point x∗is aρ−second-order stationary point ( ρ-SOSP) if:
∥∇f(x∗)∥2≤ρand λmin(∇2f(x∗))≥ −p
Kρ.
Notation. For vectors u, vwe use ⟨u, v⟩to denote their inner product and ∥u∥2to denote the L2
norm. For matrix M∈Rk×d, we denote the element of the ithrow and jthcolumn by µi,jand we
use∥M∥Fto denote the Frobenius norm. For matrix M∈Rk×dandσ2∈R+we use N(M, σ2)
to denote an k×drandom Gaussian matrix where each element ai,j∼N(µi,j, σ2). We use ∇fand
∇2fto denote the gradient and Hessian operators, respectively.
44 The Main Results
This section contains the full statement of our main theorems, and proof outlines. We organize the
flow of this section according to our conceptual outline given in the introduction. In most cases, we
defer the full proofs to the appendix.
Our goal is to find a matrix that satisfies the Johnson-Lindenstrauss guarantee as given in Lemma 1.
Consider the natural objective function of maximum distortion:
h(A) = max
x1,...,x n∥Ax∥2
2−1. (1)
Step 1 : The first step tells us what will not work. In particular, direct optimization over the space of
matrices cannot work. Our first result shows that minimizing this maximum distortion objective via a
first or second-order method, is a doomed approach. In particular, we show that there exist instances
which are bad local minima.
Theorem 1. For all k >1, there exists a family of matrices Ak×k+1which are strict local minima
for the objective function of Equation 1 reachable from the origin. The achieved distortion is Ω(1)
over a set of O(k2)points, while there exist matrices yielding distortion O(p
logk/k)→0.
The proof of this is constructive. We construct a dataset and then show that a set of matrices reachable
from the origin have large constant distortion, yet these points are locally unimprovable. The full
proof can be found in Appendix A.1 □
Step 2 : The key idea towards our final result is to perform an optimization over an extended space:
the space of parameters of random Gaussian solution samplers. We first define an optimization
objective over this space, and then prove properties about the resulting landscape over the space of
samplers.
A Gaussian solution sampler is defined by its mean and variance. We only consider the case where all
entries have the same variance. Thus, our new parameter space consists of pairs (M, σ2), where M
is a projection matrix, here interpreted as the mean of a Gaussian distribution, and σ2is the variance
parameter. Given (M, σ2), the solution sampler defined is simply: A∼N(M, σ2).
We note that our new parameter space has just one additional parameter than the ambient setting.
Step 2A : We next must extend the maximum distortion objective above, to the space of random
solution samplers we have defined. Consider the objective f∗, defined as follows:
f∗(M, σ2) = Pr A∼N(M,σ2)[h(A)> ε]. (2)
Thus, f∗(M, σ2)is the probability that a matrix sampled according to the corresponding Gaussian
distribution will have maximum distortion at least ε. When we take ε=O(p
logn/k), our objective
function f∗gives us the probability that a Gaussian solution sampler fails to produce a matrix that
meets the JL guarantee. Hence, a good sampler is one that makes f∗small.
We note that in the proof of the JL lemma in [Indyk and Motwani, 1998], the authors show that a
matrix with Gaussian entries satisfies the JL guarantee with high probability. Thus, in particular, we
know that taking M=0, where 0is ak×dzero matrix, and σ2= 1, gives a low objective value
forf∗.
In the context of these definitions, therefore, our goal is to find a matrix ˆMsuch that f∗(ˆM,0)
has a low objective value. To do this, we now define a related objective value, which thanks to a
regularization term, will allow the optimization algorithm to push us towards lower variance solutions.
The technical challenge is then to show that we can control any deterioration of the JL guarantee of
these lower variance solutions.
We define our final objective function starting from f∗defined above. First, we simplify the objective
by applying a standard union bound and write a relaxed objective that sums for every point the
probability that the point will have a norm outside the required bounds after the linear transformation.
5f(M, σ2) =nX
j=1PrA∼N(M,σ2)1
k∥Axj∥2
2̸∈(1−ε,1 +ε)
. (3)
For an appropriately chosen value of ε=O(p
logn/k), we have that no constraint is violated with
probability greater than 1/(3n)andf(0,1)<1/3. The function fserves as an upper bound on the
probability of generating a matrix that does not have the JL guarantee, effectively acting as a proxy
for “bad” events. Next, we add an appropriate regularization term that penalizes high variance points.
Our overall objective is thus:
g(M, σ2) =f(M, σ2) +σ2/2. (4)
At our initialization point M=0andσ2= 1, the value of the regularization is 1/2, thus: g(0,1)<
1/3 + 1 /2<5/6. This is crucial, as it implies that following any decreasing path in gleads to points
with a likelihood of a bad event being less than 1. Consequently, this convergence toward a solution
sampler maintains a positive (and O(1)) probability of achieving a projection matrix that satisfies the
JL guarantee. The next step provides our algorithm. After that, we characterize its fixed points.
Step 2B : Algorithm 1 is a second-order descent algorithm consisting of two simple steps: At a given
point xt= (Mt, σt), if the gradient is sufficiently large, we take a gradient step. If the gradient is
small, we consider the Hessian; if the smallest eigenvalue is sufficiently negative, we take a step
in that direction of negative curvature; otherwise the algorithm terminates by reporting the mean
parameter Mt(see Lemma 4 for discussion on this final point). To prove correctness of the algorithm,
we must show that any ρ-SOSP will have sufficiently small variance. The proof of correctness is
given in Step 2C , and a bound on its running time in Step 3 . We can call this algorithm recursively,
to find the best distortion, using a simple routine given in Algorithm 2.
We note that in principle, many first-order methods can also be used, for example, Perturbed Gradient
Descent (PGD) which has been shown to converge to second-order stationary points fast [Jin et al.,
2017]. We use a deterministic algorithm in our analysis to enable a straightforward derandomization
of the JL lemma through the optimization of Equation 4.
Algorithm 1 Hessian Descent.
Require: ∇g,∇2g, ν=1
L, h=3√ρ
K, L, K, ρ, Minit=0, σ2
init= 1
1:t←0
2:xt←(Minit, σ2
init)
3:while truedo
4: if∥∇g(xt)∥> ρthen
5: xt+1←xt−ν· ∇g(xt)
6: else if ∥∇g(xt)∥2≤ρandλmin(∇2g(xt))<−√Kρthen
7: u1←the eigenvector corresponding to λmin(∇2g(xt))
8: xt+1←xt+hu1
9: else
10: return xt[0] = Mt
11: end if
12: t←t+ 1
13:end while
Step 2C : Since we initialize at a good solution sampler and we use a descent algorithm on our
objective, we know that we can never move to a bad sampler. But that is not enough for us. For we
recall that our goal is not to find a good randomized algorithm, but rather to find a good (deterministic)
JL matrix, via optimization. Thus we must show that we do not get trapped in any points that have
non-zero variance.
We accomplish this in several lemmas. First in Lemma 2 we show that stationary points must have
zero variance. We then refine this in Lemma 3 and show that being in a ρ-second-order stationary
point requires the variance to be very small. We need this in order to show we can escape from any
point with sufficiently large variance. Finally, in Theorem 2 we show that our second-order algorithm
6Algorithm 2 An algorithm to find optimal distortion.
Require: ∇g,∇2g, L, K, ρ, x initial, εgrid
1:minε← ∞
2:min value← ∞
3:foreachεinεgriddo
4: Mε←HESSIAN DESCENT (∇g,∇2g, ν, h, L, K, ρ )
5: value←max distortion of Mε
6: ifvalue <min valuethen
7: min value←value
8: minε←ε
9: end if
10:end for
11:return minε,min value
will not get stuck at any point with large variance, and that once we are at a solution sampler with
small enough variance, the mean parameter itself will enjoy (deterministically) the JL guarantee.
In the following lemma, we show that points with non-zero variance cannot be local minima.
Specifically, we show that for any given mean matrix and variance, there exists a nearby mean matrix
with reduced variance that improves the objective value.
Lemma 2. LetM∈Rk×d, and σ >0. Then for any γ∈[0, σ], there exists M′∈Rk×dsuch that:
•∥M−M′∥F≤2γr
kdlog
3√
kd
γ
,
•g(M′, σ2−γ2)≤g(M, σ2)−γ2/6.
Proof Sketch: The proof of the lemma essentially is a small derandomization step, where we show
that by taking a sufficiently small variance-reducing step, even if we deteriorate the JL guarantee (i.e.,
the function fincreases), the decrease in the regularizer outweighs this increase, thereby decreasing
the overall value of the objective, thus showing we could not have been at a local minimum.
More specifically, the proof proceeds as follows. We begin with a Gaussian matrix A∼N(M, σ2)
and use the additivity property to partition it: A=Aγ+A′, where Aγ∼N(0, γ2)andA′∼
N(M, σ2−γ2), representing small additive noise and the remainder of A, respectively.
The core idea is to derandomize Aγto achieve a decreased objective value. We extend the definition
of a bad event by constraining Aγto take values only within a specific range R. Within R, there must
exist a realization of Aγ, denoted as αγ, which at worst, only slightly increases the probability of
failure due to the truncation of the Gaussian distribution tails. By choosing this specific value αγ, we
effectively derandomize Aγ. As we show in the appendix, this can result in an only slightly increased
probability of a bad event.
We then define M′=αγ+Mand show that the regularization term ensures an overall decrease in
the objective function. The full proof can be found in Appendix A.2. □
In Lemma 2, we established that any point with non-zero variance cannot be a local minimum, as there
always exists a nearby point with lower objective value. The next lemma addresses whether a descent
direction can be found at each step. We prove that this is indeed the case, specifically demonstrating
that the ρ-second-order stationary points of the objective function in Equation 4 correspond to points
with approximately zero variance.
Lemma 3. Consider x1, . . . , x n∈Rd. Given target dimension kchoose ε=O(p
logn/k). The
ρ-second-order stationary points of the objective function in Eq. 4 implies σ2<poly(n, k, d )·ρO(1).
Proof Sketch: We establish the lemma by examining the behavior of the variance σ2at points
approaching ρ-second-order stationarity under the objective function defined in Equation 4. While σ2
is large we employ a series of incremental reductions, and we show we can continue in this manner
untilσ2is reduced at least to the claimed level.
7Using Taylor’s theorem and the Lipschitzness of ∇2g, we prove that at any point (M, σ2), either
the gradient will be large and thus progress will be made using first-order methods, or that the
minimum eigenvalue of the Hessian will be negative and thus we can follow that direction to make
progress. We then show that convergence to ρ-second-order stationary points gives us the desired
result. Controlling the effect of the Lipschitz constant is a main challenge. The full proof can be
found in Appendix A.3. □
Step 2D : Since our Algorithm 1 finds an approximate ρ-SOSP, we need an additional result that gives
us a stopping criterion once the variance is small enough, and simply use the mean with controlled
deterioration of the JL guarantee. That is, instead of sampling from A∼N(M, σ2), we can directly
useMinstead. This is why in line 10 of Algorithm 1, we simply return the parameter Mt.
Lemma 4. Given nunit vectors in Rdand a target dimension k, choose ε=O(p
logn/k)such that
distribution A∼N(M, σ2)satisfies the JL guarantee with distortion εwith probability 1/6. Then
using matrix Minstead of sampling from Aretains the JL guarantee with a threshold increased by
at most poly(σ,1/k).
Proof Sketch: By assumption, A∼N(M, σ2)satisfies 1/k∥Ax∥2
2∈(1−ε,1 +ε)with probability
at least 1/6. Next, we decompose AasA=M+ZwithZ∼N(0, σ2), and invoking the JL lemma,
we choose ε0such that 1/k∥Zx∥2
2∈[σ2(1−ε0), σ2(1 +ε0)]with probability at least 6/7. This
choice ensures that there exists a region in the space such that the JL guarantee holds simultaneously
for both AandZ. Our goal is to establish a bound on the distortion when using Minstead of
sampling from A. We show that ∥Mx∥2
2can be bounded with terms involving only ∥Ax∥2
2and
∥Zx∥2
2and derive an upper and lower bound on the distortion that must hold deterministically. The
full proof can be found in Appendix A.4. □
Putting the above results together, we have our first main result that shows the correctness of
Algorithm 1: it will not get trapped at any point with a large variance, and once it does finally arrive
at a point of small enough variance, the mean parameter satisfies the JL property.
Theorem 2. Given nunit vectors in Rdand a target dimension k, consider the corresponding function
gof Equation 4 for any ε≥Cp
logn/k where Cis a sufficiently large constant. Any ρ-SOSP , that
is a pair of parameters (M, σ2), ofgreachable from the origin satisfies σ2<poly(n, k, d )·ρO(1)
and goes to 0 as ρ→0. Moreover, when ρ <1/poly(n, k, d ),Msatisfies the JL guarantee having
distortion at most O(ε).
Proof. We choose the parameter Csuch that g(0,1)<5/6. Using Lemmas 2 and 3 we find that a ρ-
SOSP of gis a pair of parameters (M, σ2), with σ2<poly(n, k, d )·ρO(1)andg(M, σ2)< g(0,1).
This implies that drawing sample from A∼N(M, σ2)satisfies the JL guarantee with distortion at
most εwith probability 1/6.
Then, from Lemma 4, using the matrix Msatisfies the JL guarantee with distortion O(ϵ). □
Step 3 : The above result proves correctness of Algorithm 1, but is qualitative: we have not proved
how many steps are required. Below we give a quantitative result proving that we can minimize
our objective function efficiently and learn a deterministic JL embedding in polynomial time, while
incurring a minor increase in the distortion. Though as mentioned, we see the main contribution of
our work as centering on the optimization formulation, we note that this theorem constitutes a novel
approach to derandomizing the Gaussian JL transformation.
Theorem 3. Given nunit vectors in Rdand a target dimension k, consider any ε≥Cp
logn/k
where Cis a sufficiently large constant. Then, running Algorithm 1 to deterministically optimize the
objective function gof Equation 4 for poly(n, k, d )steps returns a matrix Mthat satisfies the JL
guarantee with distortion at most O(ε).
Proof. The first part of this theorem relies on two auxiliary results:
Lemma 5 (Sufficient Descent for Gradient Descent) .If∥∇g(xt)∥2> ρandLthe smoothness of g,
then for ν=1
Landxt+1=xt−ν· ∇g(xt), we have g(xt+1)≤g(xt)−νρ2
2.
Lemma 6 (Sufficient Descent for Negative Curvature Descent) .If∥∇g(xt)∥2≤ρand
λmin(∇2g(xt))<−√Kρ, and Kthe Hessian Lipschitz parameter, then for h=3√ρ
Kand
xt+1=xt+hu1, where u1corresponds to the eigenvector of the minimum eigenvalue, we have
g(xt+1)≤g(xt)−3ρ1.5
4√
K.
8The proofs of both lemmas are in Appendices A.5, A.6, respectively. Given these two results, now
standard optimization techniques show that Algorithm 1 finds a ρ-SOSP in O(1/ρ1.5)steps.
According to Theorem 2, choosing ρ <1/poly(n, k, d ), implies that a ρ-SOSP for gis a pair of
parameters (M, σ2)withMsatisfying the JL guarantee with distortion at most O(ε). Therefore,
running Algorithm 1 for poly(n, k, d )steps returns a matrix Mwhich satisfies the JL guarantee with
distortion at most O(ε). □
5 Simulations
We empirically validate our theoretical results, demonstrating that first and second-order information
suffices to learn high-quality embeddings. Our goal is to identify an optimal matrix Mthat produces
embeddings satisfying the JL guarantee (see Definition 1) while minimizing distortion. We show that
our method generates embeddings with significantly lower distortion compared to those obtained
using the Gaussian construction of the JL lemma.
We generate a unit norm dataset with n= 100 data points in d= 500 dimensions, and target dimen-
sionk= 30 dimensions. We aim to minimize the expected maximum distortion EA∼N(M,σ2)[h(A)]
(see Equation 1). We employ the first-order optimization algorithm Adam [Kingma and Ba, 2014]
over5000 iterations and compare our method against the average and minimum distortions over 1000
trials using the baseline matrix Z∼N(0,1)(left plot of Figure 1). To calculate the distortions of
our method, we sample from the updated mean matrix and variance at each iteration.
As predicted by our theoretical analysis, we demonstrate that while the Gaussian randomized con-
struction achieves satisfactory distortion levels, our method converges to a high-quality deterministic
solution that nearly eliminates distortion (both plots of Figure 1). At the conclusion of the procedure,
we calculate the distortion using the resultant mean matrix M. Interestingly, the results demonstrate
that∥Mx∥2
2≈1(i.e. almost 0distortion), compared to approximately 1and0.6for the average
and minimum distortions of the random construction, respectively. We plot the progression of the
distortions and variance over the iterations in the plots of Figure 1.
This evidence highlights the effectiveness of our methodology in practice, showcasing the advantage
of integrating data structure into dimensionality reduction for more accurate embeddings with
provable guarantees.
Further details can be found in Appendix B.
0 1000 2000 3000 4000 5000
Iterations0.00.20.40.60.81.01.21.4DistortionDistortion (Optimization)
Average Distortion (Random)
Minimum Distortion (Random)
0 1000 2000 3000 4000 5000
Iterations0.00.20.40.60.81.0Variance 2
Variance 2
Figure 1: Plot of the distortion obtained through optimization over 5000 iterations vs the average
distortion using a random Gaussian matrix (left plot), and the progression of variance over the same
number of iterations (right plot). To calculate the distortions’ progression with our method, we
sample from the updated mean matrix and variance at each iteration and compute the distortion. We
remark that the distortion plotted is a proxy for our objective in Equation 4. We observe that our
optimization-based approach converges to a deterministic solution sampler. By using the mean matrix
M, we achieve nearly optimal distortion, where |Mx| ≈ |x|.
96 Conclusion
In this work, we have demonstrated that optimization used directly in the sampler space can find
a deterministic JL-quality embedding. While our initial focus has been on the relatively simple
random construction of the Johnson-Lindenstrauss Lemma, there exist more complex randomized
constructions that may offer greater flexibility. We believe that the methods we have introduced could
find applicability far outside the JL setting.
Acknowledgements
This work has been partially supported by project MIS 5154714 of the National Recovery and
Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program.
Constantine Caramanis was partially supported by the NSF IFML Institute (NSF 2019844), and the
NSF AI-EDGE Institute (NSF 2112471).
References
D. Achlioptas. Database-friendly random projections. In Proceedings of the twentieth ACM SIGMOD-
SIGACT-SIGART symposium on Principles of database systems , pages 274–281, 2001.
N. Ailon and B. Chazelle. The fast johnson–lindenstrauss transform and approximate nearest
neighbors. SIAM Journal on computing , 39(1):302–322, 2009.
N. Ailon and E. Liberty. An almost optimal unrestricted fast johnson-lindenstrauss transform. ACM
Transactions on Algorithms (TALG) , 9(3):1–12, 2013.
N. Alon and B. Klartag. Optimal compression of approximate inner products and dimension reduction.
In2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS) , pages 639–
650. IEEE, 2017.
N. Alon, Y . Matias, and M. Szegedy. The space complexity of approximating the frequency moments.
InProceedings of the twenty-eighth annual ACM symposium on Theory of computing , pages 20–29,
1996.
R. I. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random
projection. Machine learning , 63:161–182, 2006.
I. Bello, H. Pham, Q. V . Le, M. Norouzi, and S. Bengio. Neural combinatorial optimization with
reinforcement learning. arXiv preprint arXiv:1611.09940 , 2016.
S. Bengio and G. Heigold. Word embeddings for speech recognition. In Interspeech , pages 1053–
1057, 2014.
A. Bhargava and S. R. Kosaraju. Derandomization of dimensionality reduction and sdp based
algorithms. In Workshop on Algorithms and Data Structures , pages 396–408. Springer, 2005.
M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging
properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 9650–9660, 2021.
K. Cho, B. Van Merriënboer, D. Bahdanau, and Y . Bengio. On the properties of neural machine
translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259 , 2014a.
K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y . Bengio.
Learning phrase representations using rnn encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 , 2014b.
K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model. In Proceedings
of the forty-first annual ACM symposium on Theory of computing , pages 205–214, 2009.
S. Dasgupta and A. Gupta. An elementary proof of a theorem of johnson and lindenstrauss. Random
Structures & Algorithms , 22(1):60–65, 2003.
10M. Dietzfelbinger, T. Hagerup, J. Katajainen, and M. Penttonen. A reliable randomized algorithm for
the closest-pair problem. Journal of Algorithms , 25(1):19–51, 1997.
L. Engebretsen, P. Indyk, and R. O’Donnell. Derandomized dimensionality reduction with applica-
tions. 2002.
P. Frankl and H. Maehara. The johnson-lindenstrauss lemma and the sphericity of some graphs.
Journal of Combinatorial Theory, Series B , 44(3):355–362, 1988.
T. Gao, X. Yao, and D. Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv
preprint arXiv:2104.08821 , 2021.
D. Garber and E. Hazan. Fast and simple pca via convex optimization. arXiv preprint
arXiv:1509.05647 , 2015.
J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural
information processing systems , 33:6840–6851, 2020.
P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimen-
sionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing , pages
604–613, 1998.
C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efficiently.
InInternational conference on machine learning , pages 1724–1732. PMLR, 2017.
W. B. Johnson. Extensions of lipshitz mapping into hilbert space. In Conference modern analysis
and probability, 1984 , pages 189–206, 1984.
V . Kabanets. Derandomization: A brief overview. Current Trends in Theoretical Computer Science:
The Challenge of the New Century , 1:165–187, 2002.
D. M. Kane and J. Nelson. A derandomized sparse johnson-lindenstrauss transform. arXiv preprint
arXiv:1006.3585 , 2010.
P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y . Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan.
Supervised contrastive learning. Advances in neural information processing systems , 33:18661–
18673, 2020.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014.
K. G. Larsen and J. Nelson. The johnson-lindenstrauss lemma is optimal for linear dimensionality
reduction. arXiv preprint arXiv:1411.2404 , 2014.
K. G. Larsen and J. Nelson. Optimality of the johnson-lindenstrauss lemma. In 2017 IEEE 58th
Annual Symposium on Foundations of Computer Science (FOCS) , pages 633–638. IEEE, 2017.
J. Liu, Y . Deng, T. Bai, Z. Wei, and C. Huang. Targeting ultimate accuracy: Face recognition via
deep embedding. arXiv preprint arXiv:1506.07310 , 2015.
W. Liu, Y . Wen, Z. Yu, M. Li, B. Raj, and L. Song. Sphereface: Deep hypersphere embedding for face
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 212–220, 2017.
J. Matoušek. On variants of the johnson–lindenstrauss lemma. Random Structures & Algorithms , 33
(2):142–156, 2008.
R. Meka and D. Zuckerman. Pseudorandom generators for polynomial threshold functions. In
Proceedings of the Forty-second ACM Symposium on Theory of Computing , pages 427–436, 2010.
Y . Nesterov. Introductory lectures on convex optimization: A basic course , volume 87. Springer
Science & Business Media, 2013.
N. Nisan. Pseudorandom generators for space-bounded computations. In Proceedings of the twenty-
second annual ACM symposium on Theory of computing , pages 204–212, 1990.
11M. Oquab, T. Darcet, T. Moutakanni, H. V o, M. Szafraniec, V . Khalidov, P. Fernandez, D. Haziza,
F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv
preprint arXiv:2304.07193 , 2023.
O. Press and L. Wolf. Using the output embedding to improve language models. arXiv preprint
arXiv:1608.05859 , 2016.
Y . Qi, D. S. Sachan, M. Felix, S. J. Padmanabhan, and G. Neubig. When and why are pre-trained
word embeddings useful for neural machine translation? arXiv preprint arXiv:1804.06323 , 2018.
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
J. Clark, et al. Learning transferable visual models from natural language supervision. In
International conference on machine learning , pages 8748–8763. PMLR, 2021.
P. Raghavan. Probabilistic construction of deterministic algorithms: approximating packing integer
programs. Journal of Computer and System Sciences , 37(2):130–143, 1988.
A. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence summariza-
tion. arXiv preprint arXiv:1509.00685 , 2015.
B. Schmidt. Stable random projection: Lightweight, general-purpose dimensionality reduction for
digitized libraries. Journal of Cultural Analytics , 3(1), 2018.
F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A unified embedding for face recognition and
clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 815–823, 2015.
O. Shamir. Convergence of stochastic gradient descent for pca. In International Conference on
Machine Learning , pages 257–265. PMLR, 2016.
D. A. Spielman and N. Srivastava. Graph sparsification by effective resistances. In Proceedings of
the fortieth annual ACM symposium on Theory of computing , pages 563–568, 2008.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. Advances in neural information processing systems , 30, 2017.
C. Xiong, V . Zhong, and R. Socher. Dynamic coattention networks for question answering. arXiv
preprint arXiv:1611.01604 , 2016.
Z. Xu and P. Li. A comprehensively tight analysis of gradient descent for pca. Advances in Neural
Information Processing Systems , 34:21935–21946, 2021.
X. Yi, D. Park, Y . Chen, and C. Caramanis. Fast algorithms for robust pca via gradient descent.
Advances in neural information processing systems , 29, 2016.
12Appendix
A Proofs of Section 4
A.1 Proof of Theorem 1
Theorem. For all k >1, there exists a family of matrices Ak×k+1which are strict local minima for
the objective function of Equation 1 reachable from the origin. The achieved distortion is Ω(1) over
a set of O(k2)points, while there exist matrices yielding distortion O(p
logk/k)→0.
Proof. We show that trying to minimize the maximum distortion of points directly can lead to getting
stuck in bad local minima. Specifically, there exists a set of points such that the family of matrices
F ⊆Rk×(k+1), which consist of unitary matrices scaled by 2in the first k×kblock and a zero
vector in the last column, is a local minimum, and any matrix A′with∥A′−A∥2
F<1/poly(k)for
some matrix A∈ F results in worse maximum distortion.
Consider any matrix A′and its closest matrix A∈ F.A′can be written as [A′
[k×k]|v]where A′
[k×k]
represents the first k×kblock of A′andvits last column. If we compute the polar decomposition of
A′
[k×k]=UP, where Uis a unitary matrix and Pis a positive semidefinite symmetric matrix, the
closest matrix AtoA′can be written as [2U|0]. By rotating the space with U−1, we assume, without
loss of generality, that U=Iand thus A′
[k×k]is a positive definite symmetric matrix.
Dataset Construction: Define vectors ei∈Rkfori= 1, . . . , k . The first kcomponents of our
vectors will be either eiorei+ejwithi̸=j. For each such vector we have 4 different versions for
the last coordinate, denoted by last (x), is given by:
last(x)∈ {+√
15∥x∥2,−√
15∥x∥2+√
7/3∥x∥2,−√
7/3∥x∥2}
Thus, the data points are constructed as ˜x= (x,last(x)). Essentially, last (x)is the completion of the
xdepending on whether we want tight large distortion or tight small distortion.
Distortion Analysis:
We analyze ∥A˜x∥2
2− ∥A′˜x∥2
2. Using the Taylor expansion:
∥A˜x∥2
2− ∥A′˜x∥2
2= 2(A˜x)T(A−A′)˜x+O(∥A′−A∥2
F)
Since∥A−A′∥2
F=∥P−2I∥2
F+∥v∥2
2, let∥A−A′∥2
F=γ2. We consider two cases based on the
magnitude of ∥v∥2
2.
Case 1: ∥v∥2
2≥γ2/2
In this case, there exists at least one isuch that v2
i≥γ2/(2k). We consider:
2(A˜x)T(A−A′)˜x=xT(P−2I)x+xTvlast(x)
Choosing x=ei, we encounter two sub-cases, depending on the sign of xT(P−2I)x. In both
sub-cases, the term 2(A˜x)T(A−A′)˜xdominates the higher-order term. This holds because:
|xT(P−2I)x+xTvlast(x)|>|xT(P−2I)x+√
7/3γ|> γ2=∥A−A′∥2
F.
Then, we have:
- IfxT(P−2I)x < 0, select the negative value of the tight large distortion case for last(x) =
−√
7/3∥ei∥to ensure ∥A˜x∥2
2<∥A′˜x∥2
2, resulting in worse distortion. That is, we expected norm
4/3, but instead using matrix A, we got norm 2, and any A′nearby will only increase gap.
- IfxT(P−2I)x≥0, select the positive value of the tight small distortion case for last(x) =√
15∥ei∥2to ensure ∥A˜x∥2>∥A′˜x∥2, again resulting in worse distortion. resulting in worse
distortion. In this case, we expected norm 4, but instead we got norm 2, and any A′nearby will only
increase gap.
13Case 2: ∥v∥2
2< γ2/2
This implies that ∥P−2I∥2
F≥γ2/2. Again, we encounter two sub-cases. Either an entry on the
diagonal of P−2Ihas mass ≥γ/2k2or an off-diagonal entry does. We shall prove that in each
case, the first-order term dominates.
-Diagonal Entry: If|(P−2I)i,i| ≥γ/2k2, choose x=ei. Then:
|xT(P−2I)x+xTvlast(x)|>|γ/2k2+xTvlast(x)|> γ2=∥A−A′∥2
F.
We note here that we always match the sign of last (x)to that of xT(P−2I)xto increase the error.
Now, we can follow the same logic as in Case 1 to show that there always exists a point with worse
distortion.
-Off-Diagonal Entry: If|(P−2I)i,j| ≥γ/2k2for some i̸=j, choose x=ei+ej. Since Pis
symmetric, we are adding the contribution of two elements, and even if the diagonal elements have
the opposite sign, by definition of this sub-case, their magnitude does not suffice to make the norm of
the first-order term negligible:
|xT(P−2I)x+xTvlast(x)|>|γ/2k2+xTvlast(x)|> γ2=∥A−A′∥2
F.
We can follow the same logic as in Case 1 to show that there always exists a point with worse
distortion.
Conclusion:
In summary, in all cases, the first-order term xT(P−2I)x+xTvlast(x)dominates over higher-order
terms O(∥A−A′∥2
F), and thus the sign is always determined by the first term. We have shown
that there always exists a point xcausing increased distortion, thus confirming that any perturbation
∥A′−A∥2
F<1/poly(k)results in worse maximum distortion. Thus, this family of matrices is a
local minimum with constant distortion. □
A.2 Proof of Lemma 2
Lemma. LetM∈Rk×d, and σ >0and consider a random matrix A∼N(M, σ2). Then for any
γ∈[0, σ], there exists M′∈Rk×dsuch that:
•∥M−M′∥F≤2γr
kdlog
3√
kd
γ
•g(M′, σ2−γ2)≤g(M, σ2)−γ2/6
Proof. Consider a random matrix A∼N(M, σ2). Letγ∈[0, σ), then using the additivity property
of Gaussian distributions, we decompose AintoAγ∼N(0, γ2)andA′∼N(M, σ2−γ2), such
thatA=Aγ+A′. We note that the case for γ2=σ2follows from the same analysis but taking
A′=Mdeterministically.
We extend the definition of a “bad event”, and consider a failure when ∥Ax∥2falls outside the range
R1:=p
k(1−ε),p
k(1 +ε)
for any x, or when any entry generated from Aγfalls beyond
the range R2:=
−2γr
log√
kd
γ/3
,2γr
log√
kd
γ/3
. This extension results in an increased
probability of a bad event by γ2/3. Denote by ai,jthejthelement of the ithrow of matrix Aγ, then:
14Pr
|ai,j| ≥2γvuutlog √
kd
γ/3!
,∀i= 1, . . . , k, ∀i= 1, . . . , d
=kX
i=1dX
j=1
|a1,i| ≥2γvuutlog √
kd
γ/3!

≤kX
i=1dX
j=12 exp
−4γ2log√
kd
γ/3
2γ2

=kX
i=1dX
j=12 exp
logγ2/9
kd
=kX
i=1dX
j=12γ2/9
kd
= 2γ2/9< γ2/3.
We note that the inequality holds due to standard Gaussian properties.
We prove that there exists a realization for Aγ∼N(0, γ2)within the range R2, which leads to an
improvement in the objective function. To see this, we express Equation 3 as follows:
f(M, σ2) =nX
j=1PrA∼N(M,σ2)(∥Axj∥2̸∈R1)
=nX
j=1PrAγ,A′[∥Aγxj+A′xj∥2̸∈R1]. (5)
Letδdenote the current value of f(M, σ2)and consider the following expression of the extended
definition of a bad event:
nX
j=1PrAγ,A′[∥Aγxj+A′xj∥2̸∈R1∨Aγ̸∈R2]
=nX
j=1PrAγ,A′[∥Aγxj+A′xj∥2̸∈R1] + Pr Aγ[Aγ̸∈R2]
=EAγ
nX
j=1PrA′[∥Aγxj+A′xj∥2̸∈R1|Aγ] +1[Aγ̸∈R2]
. (6)
We lower bound Equation 6 by the minimum of the function inside the expectation to show that there
must exist a realization of Aγthat lies in the range R2, and increases the original objective function
from δtoδ+γ2
3:
min
Aγ∈R2nX
j=1PrA′[∥Aγxj+A′xj∥2̸∈R1|Aγ]< E Aγ
nX
j=1PrA′[∥Aγxj+A′xj∥2̸∈R1|Aγ] +1[Aγ̸∈R2]

< δ+γ2/3.
Denote αγ:= arg minAγ∈R2Pn
j=1PrA′[∥Aγxj+A′xj∥2̸∈R1|Aγ], then A=αγ+A′∼
N(M′, σ2−γ2), where M′=αγ+M. Consequently, this means that:
f(M′, σ2−γ2) =nX
j=1PrA[∥Axj∥2̸∈R1]< δ+γ2/3.
15However, since we are working with the regularized objective in Equation 4, reducing σ2toσ2−γ2
means that we have an overall decrease of the objective.
g(M′, σ2−γ2)−g(M, σ2)<(δ+γ2/3−γ2/2)−δ < γ2/3−γ2/2<−γ2/6.
□
A.3 Proof of Lemma 3
Lemma. Consider x1, . . . , x n∈Rd. Given target dimension kchoose ε=O(p
logn/k). The ρ-
second-order stationary points of the objective function in Equation 4 imply σ2<poly(n, k, d )·ρO(1).
Proof. Assume that we have reached a point v= (M, σ2). Our analysis proceeds in two distinct
cases based on the magnitude of σ2. If the variance exceeds a specified threshold, we employ a series
of incremental reductions. This procedure continues until the variance reaches a sufficiently low
threshold, facilitating a transition to a scenario wherein the variance can be reduced to zero in a single
step.
Case 1 :σ2>2−5(Kd3/2log(kdK)3/2)−2. Choose γ2= 2−5(Kd3/2log(kdK)3/2)−2. According
to Lemma 2, there exists a neighboring point M′and we define v′= (M′, σ2−γ2)which reduces
the objective function. Then, the distance between v′andvis given by:
∥v′−v∥2=vuut4γ2dlog √
kd
γ/3!
+γ4.
Denote Kthe Hessian Lipschitz constant, then, from Lemma 2 and Taylor’s theorem on the Lips-
chitzness of ∇2g[Nesterov, 2013] we get:
g(v′)−g(v)− ∇g(v)·(v′−v)−1
2(v′−v)· ∇2g(v)·(v′−v)≤K
6∥v′−v∥3
2
∇g(v)·(v′−v) +1
2(v′−v)· ∇2g(v)·(v′−v)−K
6∥v′−v∥3
2≤g(v′)−g(v)
∇g(v)·(v′−v) +1
2(v′−v)· ∇2g(v)·(v′−v)−K
6∥v′−v∥3
2≤ −γ2
6.
This implies that either
∇g(v)·(v′−v)<−γ2
12=⇒ ∥∇ g(v)∥2>γ
12r
4dlog√
kd
γ/3
+γ2,
or
(v′−v)· ∇2g(v)·(v′−v)−K
6∥v′−v∥3
2<−γ2
12.
The last inequality implies that
λmin(∇2g(v))<−1
12
4dlog√
kd
γ/3
+γ2+K
6vuut4γ2dlog √
kd
γ/3!
+γ4
<−1
24
4dlog√
kd
γ/3
+γ2. (7)
16The last inequality follows from the choice of γ2we made at the beginning.
This guarantees that in each step either the gradient will be large and thus progress will be made
using first-order methods or that the minimum eigenvalue of the Hessian will be negative and thus
there exists a direction which we can follow by a second-order method.
Case 2 :σ2≤2−5(Kd3/2log(kdK)3/2)−2.
Assume that after several iterations, we have reached a point v= (M, σ2). Given that the inequality
in 7 holds for all γ2≤σ2, we select the maximum allowable reduction. This allows us to reduce the
variance to zero in a single step. Based on Lemma 2, there exists a neighboring point v′= (M′,0)
which reduces the objective function and by following the steps as in the previous case, we get:
∥∇g(v)∥2>σ
12r
4dlog√
kd
σ/3
+σ2.
or
λmin(∇2g(v))<−1
24
4dlog√
kd
σ/3
+σ2.
This means that, convergence to a ρ-second-order stationary point for gimplies:
σ
12r
4dlog√
kd
σ/3
+σ2< ρ,
1
24
4dlog√
kd
σ/3
+σ2<p
Kρ.
We will use the second inequality. First, we observe that:
1
24
12√
kd
σ+σ2≤1
24
4dlog√
kd
σ/3
+σ2
Therefore, we have:
1√Kρ<288√
kd
σ+ 24σ2
Because of our analysis we have that at all times during the optimization σ2<1(refer to the
regularized objective function in Equation 4. In addition to that, we assume that d≥2, k≥1, which
are both natural assumptions. This in turn implies that:
1√Kρ<576√
kd
σ
1
576d√Kρ<3√
kd
σ
1
576d√Kρkd<1
σ
σ2<5762Kρkd3.
Finally, substituting in for the Lipschitz constant K, we get:
σ2<poly(n, k, d )·ρO(1).
This implies that σ2can become arbitrarily small for an appropriate choice of ρ.
□
17A.4 Proof of Lemma 4
Lemma. Given nunit vectors in Rdand a target dimension k, choose ε=O(p
logn/k)such that
distribution A∼N(M, σ2)satisfies the JL guarantee with distortion εwith probability 1/6. Then
using matrix Minstead of sampling from Aretains the JL guarantee with a threshold increased by
at most poly(σ,1/k).
Proof. We start with the assumption that1
k∥Ax∥2
2∈(1−ε,1 +ε)with probability at least1
6.
Expressing AasA=M+Zwhere Z∼N(0, σ2), and from the JL lemma, we can select ε0such
that
1
k∥Zx∥2
2∈[σ2(1−ε0), σ2(1 +ε0)],
with probability at least6
7. This ensures there exists an overlap where both inequalities for AandZ
hold simultaneously.
Our goal is to determine how much worse the distortion becomes when using Minstead of sampling
from the distribution A.
Using the triangle inequality we have:
1
k∥Mx∥2=1
k∥Mx+Zx−Zx∥2≤1
k∥Mx+Zx∥2+1
k∥Zx∥2=1
k∥Ax∥2+1
k∥Zx∥2,
which by squaring both sides and using the JL guarantee for AandZ, we obtain:
1
k∥Mx∥2
2≤1
k∥Ax∥2
2+2
k2∥Ax∥2∥Zx∥2+1
k∥Zx∥2
2
≤1 +ε+2σ
k√
1 +ε√
1 +ε0+σ2(1 +ε0)
≤1 +ε+2√
2σ
k√
1 +ε+ 2σ2.
For the lower bound, using the Cauchy-Schwarz inequality and the JL guarantee for AandZ, we
have:
1
k∥Mx∥2
2≥1
2k∥Mx+Zx∥2
2−1
k∥Zx∥2
2
=1
2k∥Ax∥2
2−1
k∥Zx∥2
2
≥1/2(1−ε)−σ2(1 +ε0)
≥1/2(1−ε)−σ2,
Combining these results, we observe that replacing AwithMmaintains the JL guarantee with an
increased distortion threshold, bounded by at most poly (σ,1/k).
□
A.5 Proof of Lemma 5
Proof. We use Taylor’s theorem on the smoothness of gand get:
Lemma. If∥∇g(xt)∥2> ρ, then for ν=1
Landxt+1=xt−ν· ∇g(xt), we have g(xt+1)≤
g(xt)−νρ2
2.
18g(xt+1)≤g(xt) +⟨∇g(xt), xt+1−xt⟩+L
2∥xt+1−xt∥2
2
=g(xt)−ν∥∇g(xt)∥2
2+Lν2
2∥∇g(xt)∥2
2
=g(xt)−
1−1
2Lν
νρ2.
We can use ν= 1/Lto get the following:
g(xt+1)≤g(xt)−1
2νρ2.
□
A.6 Proof of Lemma 6
Lemma. If∥∇g(xt)∥2≤ρandλmin(∇2g(xt))<−√Kρ, then for h=3√ρ
Kandxt+1=xt+hu1,
where u1corresponds to the eigenvector of the minimum eigenvalue, we have g(xt+1)≤g(xt)−
3ρ1.5
4√
K.
Proof. We use Taylor’s theorem on the Lipschitzness of ∇2g[Nesterov, 2013] and get:
g(xt+1)≤g(xt) +⟨∇g(xt), xt+1−xt⟩+1
2⟨∇2g(xt)(xt+1−xt),(xt+1−xt)⟩+K
6∥xt+1−xt∥3
=g(xt) +h⟨∇g(xt), u1⟩+h2λ1
2+h3K
6
≤g(xt) +hρ−h2p
Kρ+h3K
6.
We can use h=3√ρ√
Kto get the following:
g(xt+1)≤g(xt)−3ρ√ρ
2√
K.
□
B Details of the Experimental Evaluation
We explore the behavior of a distortion optimization process using by minimizing the expected
maximum distortion in a given dataset through a series of optimization descent steps. We show that
while the Gaussian randomized construction can achieve good enough distortion, our method goes
beyond that and achieves almost zero distortion, by taking into account the structure of the data.
B.1 Proxy for the Objective Function.
To do this, we use the maximum distortion (Equation 1) with a proxy of our objective function
(Equation 4) and we aim to minimize:
f(M, σ2) =EA∼N(M,σ2)[h(A)] +σ2/2. (8)
To do this we use the gradient of Equation 8 with respect to the parameters of interest θ= (M, σ2):
∇(M,σ2)EA∼N(M,σ2)[h(A)] +σ2/2. (9)
19We can approximate the expectation by taking y1, . . . , y Nsamples drawn from N(M, σ2)and using
Monte Carlo sampling we get the approximate gradient:
∇(M,σ2)f(M, σ2)≈1
NNX
i=1h(yi).
B.2 Methodology of the Simulation.
We generate a unit norm synthetic dataset of n= 100 data points in d= 500 dimensions and our goal
is to project these into k= 30 dimensions while minimizing f. We run our optimization for 5000
iterations, using the Adam optimizer [Kingma and Ba, 2014] with a learning rate of 0.01and batch
sizeN= 20 . At every iteration, we calculate the maximum distortion and store it. We demonstrate
that, through this procedure, the model consistently reduces the distortion and the variance, thus
converging to a deterministic solution sampler.
We compare our method with the Gaussian random construction, that is we draw Z∼N(0,1)and
calculate ∥Zx∥2
2. To have a fair evaluation we draw 1000 such matrices and calculate the mean and
the minimum distortion obtained.
Our results show that we consistently learn a matrix with close to optimal distortion, that is ∥Mx∥2
2≈
1, while the randomized construction achieves an average value of ∥Zavgx∥2
2≈2and minimum value
∥Zminx∥2≈1.6.
C Proving Smoothness and Hessian Lipschitzness
Denote by µ1, . . . , µ k, the rows of matrix MandA1, . . . , A kthe rows of the Gaussian random
matrix A. Then we can write the objective function:
g(M, σ2) =nX
j=1Pr
⟨A1, xj⟩2+···+⟨Ak, xj⟩2̸∈[k(1−ε), k(1 +ε)]
+σ2
2
=nX
j=1Pr
χ2
1
δ1,j=⟨µ1, xj⟩2
σ2
+···+χ2
1
δk,j=⟨µk, xj⟩2
σ2
̸∈R1
+σ2
2
=nX
j=1Pr
χ2
k(δj)̸∈
k(1−ε)/σ2, k(1 +ε)
+σ2
2
=nX
j=1"Zk(1−ε)
σ2
0fk,δj(z)dz+Z∞
k(1+ε)
σ2fk,δj(z)dz#
+σ2
2
=nX
j=1
1 +Fk,δjk(1−ε)
σ2
−Fk,δjk(1 +ε)
σ2
+σ2
2, (10)
where fk,δJ,Fk,δjare the pdf and cdf of χ2
k, the non-central chi-squared distribution with kdegrees
of freedom and δj=⟨µ1, xj⟩2+···+⟨µk, xj⟩2
σ2as the non centrality parameter, respectively.
Note that, instead of considering the k×dmean variables directly, it is simpler to reframe the
problem. Specifically, we can view the function gin terms of the inner product variables v1, . . . , v k
andσ2, where vi=⟨µi, x⟩andµiis the i-th row of the matrix M.
Therefore, our problem reduces to proving the Lipschitz continuity of the Gradient and Hessian
continuity with respect to these new variables. This approach simplifies the calculations significantly.
20To establish that the Gradient and Hessian is Lipschitz continuous, we examine the case for j= 1.
The extension to j=ncan be handled through summation. Let τ=σ2. Then, we have δ=∥v∥2
τ=
v2
1+···+v2
k
τ. Our function from Equation 10 becomes
g(v1, . . . , v k, τ) = 1 + Fk,δk(1−ε)
τ
−Fk,δk(1 +ε)
τ
+τ/2,
where Fk,δ(x) =e−δ/2∞P
j=0(δ/2)j
j!Q(x;k+ 2j), with Q(x;k) =γ(k/2,x/2)
Γ(k/2)andγ(y, t)is the lower
incomplete gamma function.
Notice that taking the derivative gives an extra part with increased degrees of freedom∂Fδ,k(x)
∂δ=
−1/2Fδ,k(x) + 1/2Fδ,k+2(x).
C.1 Gradient Lipschitzness.
Denote by Dthe elements of ∇g, and note that in this section to simplify notation we use ∥ · ∥ to
represent ∥ · ∥2. Then the derivatives are:
Dvi(v1, . . . , v k, τ) =−vi
τFk,δk(1−ε)
τ
+vi
τFk+2,δk(1−ε)
τ
+vi
τFk,δk(1 +ε)
τ
−vi
τFk+2,δk(1 +ε)
τ
.
Dτ(v1, . . . , v k, τ) =∥v∥2
2τ2Fk,δk(1−ε)
τ
−∥v∥2
2τ2Fk+2,δk(1−ε)
τ
−k(1−ε)
τ2fk,δk(1−ε)
τ
−∥v∥2
2τ2Fk,δk(1 +ε)
τ
+∥v∥2
2τ2Fk+2,δk(1 +ε)
τ
+k(1 +ε)
τ2fk,δk(1 +ε)
τ
.
To prove Gradient Lipschitzness of gwe can bound the Frobenius norm of the Hessian ∇2g. Denote
byD2the elements of ∇2g. Below we calculate and bound all the derivatives:
D2
vi,vig(v1, . . . , v k, τ) =
−1
τ+v2
i
τ2
Fk,δk(1−ε)
τ
+
−2v2
i
τ2+1
τ
Fk+2,δk(1−ε)
τ
+v2
i
τ2Fk+4,δk(1−ε)
τ
+1
τ−v2
i
τ2
Fk,δk(1 +ε)
τ
+2v2
i
τ2−1
τ
Fk+2,δk(1 +ε)
τ
−v2
i
τ2Fk+4,δk(1 +ε)
τ
.
Here we used the triangle inequality and the fact that the cdf is bounded by 1. Notice how there is no
dependency on ∥v∥since the range we are integrating over with the cumulative distribution functions
is independent of ∥v∥. Consequently, if ∥v∥is large, the probability becomes exponentially small.
Thus, we can bound this by:
|D2
vi,vig(v1, . . . , v k, τ)| ≤4
τ+8
τ2≤O1
τ2
.
Next, we have:
D2
vi,vjg(v1, . . . , v k, τ) =vivj
τ2Fk,δk(1−ε)
τ
−2vivj
τ2Fk+2,δk(1−ε)
τ
+vivj
τ2Fk+4,δk(1−ε)
τ
−vivj
τ2Fk,δk(1 +ε)
τ
+2vivj
τ2Fk+2,δk(1 +ε)
τ
−vivj
τ2Fk+4,δk(1 +ε)
τ
.
21Thus, we can bound this by:
|D2
vi,vjg(v1, . . . , v k, τ)| ≤8vivj
τ2≤O1
τ2
.
Next, we have:
D2
vi,τg(v1, . . . , v k, τ) =vi
τ2−vi∥v∥2
τ3
Fk,δk(1−ε)
τ
+2vi∥v∥2
τ3−vi
τ2
Fk+2,δk(1−ε)
τ
−vi∥v∥2
τ3Fk+4,δk(1−ε)
τ
+vik(1−ε)
τ3fk,δk(1−ε)
τ
−vik(1−ε)
τ3fk+2,δk(1−ε)
τ
−vi
τ2+vi∥v∥2
τ3
Fk,δk(1 +ε)
τ
+
−2vi∥v∥2
τ3+2vi
τ2
Fk+2,δk(1 +ε)
τ
+vi∥v∥2
τ3Fk+4,δk(1 +ε)
τ
−vik(1 +ε)
τ3fk,δk(1 +ε)
τ
+vik(1 +ε)
τ2fk+2,δk(1 +ε)
τ
.
Thus, we can bound this by:
|D2
vi,τg(v1, . . . , v k, τ)| ≤4vi
τ2+8vi∥v∥2
τ3+2vik(1−ε)
τ3+2vik(1 +ε)
τ3≤Ok
τ3
.
Next, we have:
D2
τ,τg(v1, . . . , v k, τ) =
−∥v∥2
τ3+∥v∥4
4τ4
Fk,δk(1−ε)
τ
+
−∥v∥4
2τ4+∥v∥2
τ3
Fk+2,δk(1−ε)
τ
+∥v∥4
4τ4Fk+4,δk(1−ε)
τ
+
−∥v∥2k(1−ε)
2τ4+2k(1−ε)
τ3−(k(1−ε))2
2τ4
fk,δk(1−ε)
τ
+∥v∥2k(1−ε)
2τ4fk+2,δk(1−ε)
τ
+k(1−ε)
τ3e−δ/2∞X
j=0(δ/2)j
j!((l+ 2j)/2−1)fk+2jk(1−ε)
τ
+∥v∥2
τ3−∥v∥4
4τ4
Fk,δk(1 +ε)
τ
+∥v∥4
2τ4−∥v∥2
τ3
Fk+2,δk(1 +ε)
τ
−∥v∥4
4τ4Fk+4,δk(1 +ε)
τ
+∥v∥2k(1 +ε)
2τ4−2k(1 +ε)
τ3+(k(1 +ε))2
2τ4
fk,δk(1 +ε)
τ
−∥v∥2k(1 +ε)
2τ4fk+2,δk(1 +ε)
τ
−k(1 +ε)
τ3e−δ/2∞X
j=0(δ/2)j
j!((l+ 2j)/2−1)fk+2jk(1 +ε)
τ
+1
2.
Thus, we can bound this by:
22|D2
τ,τg(v1, . . . , v k, τ)| ≤4∥v∥2
τ3+2∥v∥4
τ4+∥v∥2k(1−ε)
τ4+3k(1−ε)
τ3+(k(1−ε))2
2τ4
+∥v∥2k(1 +ε)
τ4+3k(1 +ε)
τ3+(k(1 +ε))2
2τ4
≤Okε2
τ4
.
Overall, for D2
vi,vig, we have:
ξvi,vi= max
v1,...,v k,τ∥D2
vi,vig(v1, . . . , v k, τ)∥
≤min
τ(1/τ2).
Similarly, for D2
vi,vjg, we obtain ξvi,vj≤minτ(1/τ2)forD2
vi,τg, we obtain ξvi,τ≤minτ(k/τ3)
and for D2
τ,τ, we obtain ξτ,τ≤minτ(k2ε2/τ4).
Substituting Mandσ2back, and assuming that the minimum value for σ2we allow is σ2
0we have
that the Frobenius norm of the Hessian is bounded from:
∥∇2g∥2
F≤kX
i=1ξ2
vi,vi+ξ2
σ2,σ2+ 2X
i̸=jξ2
vi,vj+ 2kX
i=1ξ2
vi,σ2
≤kξ2
vi,vi+ξ2
σ2,σ2+ 2(k×d−2k−1)ξ2
vi,vj+ 2kξ2
vi,σ2.
Therefore we get:
∥∇2g∥F≤Ok
σ4
0
+Ok2ε2
σ8
0
+Odk
σ4
0
+Ok
σ6
0
≤poly(k, ε, d, 1/σ0).
Finally, since we are summing over ndata points the smoothness Lipschitz constant is
poly(n, k, ε, d, 1/σ0)
C.2 Hessian Lipschitzness.
To prove Hessian Lipschitzness of gwe will use the definition and the third-order derivatives. Denote
byD3the elements of ∇3g. Below we calculate and bound all the third-order derivatives:
D3
vi,vi,vig(v1, . . . , v k, τ) =3vi
τ2−v3
i
τ3
Fk,δk(1−ε)
τ
+
−6vi
τ2+3v3
i
τ3
Fk+2,δk(1−ε)
τ
+3vi
τ2−3v3
i
τ3
Fk+4,δk(1−ε)
τ
+v3
i
τ3Fk+6,δk(1−ε)
τ
+
−3vi
τ2+v3
i
τ3
Fk,δk(1 +ε)
τ
+6vi
τ2−3v3
i
τ3
Fk+2,δk(1 +ε)
τ
+
−3vi
τ2+3v3
i
τ3
Fk+4,δk(1 +ε)
τ
−v3
i
τ3Fk+6,δk(1 +ε)
τ
.
Thus, we can bound this:
|D3
vi,vi,vig(v1, . . . , v k, τ)| ≤24vi
τ2+16v3
i
τ3≤1
τ3
.
23Here we used the triangle inequality and the fact that the cdf is bounded by 1. Notice how there is no
dependency on ∥v∥since the range we are integrating over with the cumulative distribution functions
is independent of ∥v∥. Consequently, if ∥v∥is large, the probability becomes exponentially small.
Next, we have:
D3
vi,vj,vjg(v1, . . . , v k, τ) ="
vi
τ2−viv2
j
τ3#
Fk,δk(1−ε)
τ
+"
−2vi
τ2+3viv2
j
τ3#
Fk+2,δk(1−ε)
τ
+"
vi
τ2−3viv2
j
τ3#
Fk+4,δk(1−ε)
τ
+viv2
j
τ3Fk+6,δk(1−ε)
τ
+"
−vi
τ2+viv2
j
τ3#
Fk,δk(1 +ε)
τ
+"
2vi
τ2−3viv2
j
τ3#
Fk+2,δk(1 +ε)
τ
+"
−vi
τ2+3viv2
j
τ3#
Fk+4,δk(1 +ε)
τ
−viv2
j
τ3Fk+6,δk(1 +ε)
τ
.
Thus, we can bound this by:
|D3
vi,vj,vjg(v1, . . . , v k, σ)| ≤8vi
τ2+16viv2
j
τ3≤1
τ3
.
Next, we have:
D3
vi,vj,vkg(v1, . . . , v k, τ) =−vivjvk
τ3Fk,δk(1−ε)
τ
+3vivjvk
τ3Fk+2,δk(1−ε)
τ
−3vivjvk
τ3Fk+4,δk(1−ε)
τ
+vivjvk
τ3Fk+6,δk(1−ε)
τ
+vivjvk
τ3Fk,δk(1 +ε)
τ
−3vivjvk
τ3Fk+2,δk(1 +ε)
τ
+3vivjvk
τ3Fk+4,δk(1 +ε)
τ
−vivjvk
τ3Fk+6,δk(1 +ε)
τ
.
Thus, we can bound this by:
|D3
vi,vj,vkg(v1, . . . , v k, τ)| ≤16vivjvk
τ3≤1
τ3
.
Next, we have:
24D3
vi,τ,vig(v1, . . . , v k, τ) =1
τ2−∥v∥2
τ3−3v2
i
τ3+v2
i∥v∥2
τ4
Fk,δk(1−ε)
τ
+6v2
i
τ3−3v2
i∥v∥2
τ4+2∥v∥2
τ3−1
τ2
Fk+2,δk(1−ε)
τ
+3v2
i∥v∥2
τ4−3v2
i
τ3−∥v∥2
τ3
Fk+4,δk(1−ε)
τ
−v2
i||v∥2
τ4Fk+6,δk(1−ε)
τ
+k(1−ε)
τ3−v2
ik(1−ε)
τ4
fk,δk(1−ε)
τ
+
−k(1−ε)
τ3+2v2
ik(1−ε)
τ4
fk+2,δk(1−ε)
τ
−v2
ik(1−ε)
τ4
fk+4,δk(1−ε)
τ
+
−1
τ2+∥v∥2
τ3+3v2
i
τ3−v2
i∥v∥2
τ4
Fk,δk(1 +ε)
τ
+
−6v2
i
τ3+3v2
i∥v∥2
τ4−2∥v∥2
τ3+1
τ2
Fk+2,δk(1 +ε)
τ
+
−3v2
i∥v∥2
τ4+3v2
i
τ3+∥v∥2
τ3
Fk+4,δk(1 +ε)
τ
+v2
i||v∥2
τ4Fk+6,δk(1 +ε)
τ
+
−k(1 +ε)
τ3+v2
ik(1 +ε)
τ4
fk,δk(1 +ε)
τ
+k(1 +ε)
τ3−2v2
ik(1 +ε)
τ4
fk+2,δk(1 +ε)
τ
+v2
ik(1 +ε)
τ4
fk+4,δk(1 +ε)
τ
.
Thus, we can bound this by:
|D3
vi,τ,vig(v1, . . . , v k, τ)| ≤4
τ2+8∥v∥2
τ3+24v2
i
τ3+16v2
i∥v∥2
τ4+
+2k(1−ε)
τ3+4v2
ik(1−ε)
τ4
+2k(1 +ε)
τ3+4v2
ik(1 +ε)
τ4≤k
τ4
.
Next, we have:
25D3
vi,τ,vjg(v1, . . . , v k, τ) =
−3vivj
τ3+vivj∥v∥2
τ4
Fk,δk(1−ε)
τ
+6vivj
τ3−3vivj∥v∥2
τ4
Fk+2,δk(1−ε)
τ
+3vivj∥v∥2
τ4−3vivj
τ3
Fk+4,δk(1−ε)
τ
−vivj∥v∥2
τ4Fk+6,δk(1−ε)
τ
+2vivjk(1−ε)
τ4fk,δk(1−ε)
τ
−4vivjk(1−ε)
τ4fk+2,δk(1−ε)
τ
+2vivjk(1−ε)
τ4fk+4,δk(1−ε)
τ
+3vivj
τ3−vivj∥v∥2
τ4
Fk,δk(1 +ε)
τ
+
−6vivj
τ3+3vivj∥v∥2
τ4
Fk+2,δk(1 +ε)
τ
+
−3vivj∥v∥2
τ4+3vivj
τ3
Fk+4,δk(1 +ε)
τ
+vivj∥v∥2
τ4Fk+6,δk(1 +ε)
τ
−2vivjk(1 +ε)
τ4fk,δk(1 +ε)
τ
+4vivjk(1 +ε)
τ4fk+2,δk(1 +ε)
τ
−2vivjk(1 +ε)
τ4fk+4,δk(1 +ε)
τ
.
Thus, we can bound this by:
|D3
vi,τ,vjg(v1, . . . , v k, τ)| ≤24vivj
τ3+16vivj∥v∥2
τ4+16vivjk(1−ε)
τ4+16vivjk(1 +ε)
τ4≤k
τ4
.
Next, we have:
26D3
vi,τ,τg(v1, . . . , v k, τ) =
−2vi
τ3+2||v||2vi
τ4−vi||v||
4τ5
Fk,δk(1−ε)
τ
+
−6||v||2vi
τ4+3vi∥v∥4
4τ5+2vi
τ3
Fk+2,δk(1−ε)
τ
+
−vi||v∥4
2τ5+2vi∥v∥2
τ4−vi∥v∥4
4τ4
Fk+4,δk(1−ε)
τ
+vi∥v∥4
4τ4Fk+6,δk(1−ε)
τ
+
−3vik(1−ε)
τ4+vi∥v∥2k(1−ε)
2τ5+vi(k(1−ε))2
τ5
fk,δk(1−ε)
τ
+
−vi∥v∥2k(1−ε)
τ5+3vik(1−ε)
τ4−vi(k(1−ε))2
2τ5
fk+2,δk(1−ε)
σ2
+vi∥v∥2k(1−ε)
2τ5fk+4,δk(1−ε)
τ
−vik(1−ε)
τ4e−δ/2∞X
j=0(δ/2)j
j!((l+ 2j)/2−1)fk+2jk(1−ε)
τ
+vik(1−ε)
τ4e−δ/2∞X
j=0(δ/2)j
j!((l+ 2j+ 2)/2−1)fk+2j+2k(1−ε)
τ
+2vi
τ3−2||v||2vi
τ4+vi||v||
4τ5
Fk,δk(1 +ε)
τ
+6||v||2vi
τ4−3vi∥v∥4
4τ5−2vi
τ3
Fk+2,δk(1 +ε)
τ
+vi||v∥4
2τ5−2vi∥v∥2
τ4+vi∥v∥4
4τ4
Fk+4,δk(1 +ε)
τ
−vi∥v∥4
4τ4Fk+6,δk(1 +ε)
τ
+3vik(1 +ε)
τ4−vi∥v∥2k(1 +ε)
2τ5−vi(k(1 +ε))2
τ5
fk,δk(1 +ε)
τ
+vi∥v∥2k(1 +ε)
τ5−3vik(1 +ε)
τ4+vi(k(1 +ε))2
2τ5
fk+2,δk(1 +ε)
σ2
−vi∥v∥2k(1 +ε)
2τ5fk+4,δk(1 +ε)
τ
+vik(1 +ε)
τ4e−δ/2∞X
j=0(δ/2)j
j!((l+ 2j)/2−1)fk+2jk(1 +ε)
τ
−vik(1 +ε)
τ4e−δ/2∞X
j=0(δ/2)j
j!((l+ 2j+ 2)/2−1)fk+2j+2k(1 +ε)
τ
.
Thus, we can bound this by:
27|D3
vi,τ,τg(v1, . . . , v k, τ)| ≤8vi
τ3+20||v||2vi
τ4+vi||v||
2τ5+14vi||v||4
4τ4
+8vik(1−ε)
τ4+2vi∥v∥2k(1−ε)
τ5+3vi(k(1−ε))2
2τ5
+8vik(1 +ε)
τ4+2vi∥v∥2k(1 +ε)
τ5+3vi(k(1 +ε))2
2τ5
≤k2ε2
τ5
.
Next, we have:
D3
τ,τ,τg(v1, . . . , v k, τ) =3∥v∥2
τ4−3∥v∥4
2τ5+∥v∥6
8τ6
Fk,δk(1−ε)
τ
+4∥v∥4
τ5−3∥v∥6
8τ6−3∥v∥2
τ4
Fk+2,δk(1−ε)
τ
+3∥v∥6
8τ6−3∥v∥4
2τ5
Fk+4,δk(1−ε)
τ
−∥v∥6
8τ6Fk+6,δk(1−ε)
τ
+4∥v∥2k(1−ε)
τ5−∥v∥4k(1−ε)
2τ6−6k(1−ε)
τ4+2(k(1−ε))2
τ5−∥v∥2(k(1−ε))2
4τ6
fk,δk(1−ε)
τ
+∥v∥4k(1−ε)
τ6−4∥v∥2k(1−ε)
τ5+∥v∥2(k(1−ε))2
4τ6
fk+2,δk(1−ε)
τ
−∥v∥4k(1−ε)
4τ6fk+4,δk(1−ε)
τ
+∥v∥2k(1−ε)
2τ5−3k(1−ε)
τ4+(k(1−ε))2
2τ5
e−δ/2∞X
j=0(δ/2)j
j!(k+ 2j−2)fk+2jk(1−ε)
τ
−∥v∥2k(1−ε)
2τ5e−δ/2∞X
j=0(δ/2)j
j!(l/2 +j)fk+2+2 jk(1−ε)
τ
−k(1−ε)
τ4e−δ/2∞X
j=0(δ/2)j
j!((l+ 2j)/2−1)2fk+2jk(1−ε)
τ
28+
−3∥v∥2
τ4+3∥v∥4
2τ5−∥v∥6
8τ6
Fk,δk(1 +ε)
τ
+
−4∥v∥4
τ5+3∥v∥6
8τ6+3∥v∥2
τ4
Fk+2,δk(1 +ε)
τ
+
−3∥v∥6
8τ5+3∥v∥4
2τ5
Fk+4,δk(1 +ε)
τ
+∥v∥6
8τ6Fk+6,δk(1 +ε)
τ
+
−4∥v∥2k(1 +ε)
τ5+∥v∥4k(1 +ε)
2τ6+6k(1 +ε)
τ4−2(k(1 +ε))2
τ5+∥v∥2(k(1 +ε))2
4τ6
fk,δk(1 +ε)
τ
+
−∥v∥4k(1 +ε)
τ6+4∥v∥2k(1 +ε)
τ5−∥v∥2(k(1 +ε))2
4τ6
fk+2,δk(1 +ε)
τ
+∥v∥4k(1 +ε)
4τ6fk+4,δk(1 +ε)
τ
+
−∥v∥2k(1−ε)
2τ5+3k(1 +ε)
τ4−(k(1 +ε))2
2τ5
e−δ/2∞X
j=0(δ/2)j
j!(k+ 2j−2)fk+2jk(1 +ε)
τ
+∥v∥2k(1 +ε)
2τ5e−δ/2∞X
j=0(δ/2)j
j!(l/2 +j)fk+2+2 jk(1 +ε)
τ
+k(1 +ε)
τ4e−δ/2∞X
j=0(δ/2)j
j!((l+ 2j)/2−1)2fk+2jk(1 +ε)
τ
.
Thus, we can bound this by:
|D3
τ,τ,τg(v1, . . . , v k, τ)| ≤12∥v∥2
τ4+7∥v∥4
τ5+∥v∥6
τ5
+8∥v∥2k(1−ε)
τ5+7∥v∥4k(1−ε)
4τ6+10k(1−ε)
τ4+5(k(1−ε))2
2τ5+∥v∥2(k(1−ε))2
2τ6
+7∥v∥4k(1−ε)
4τ6
+8∥v∥2k(1 +ε)
τ5+7∥v∥4k(1 +ε)
4τ6+10k(1 +ε)
τ4+5(k(1 +ε))2
2τ5+∥v∥2(k(1 +ε))2
2τ6
+7∥v∥4k(1 +ε)
4τ6
≤k2ε2
τ6
.
Overall, for D2
vi,vig, we have:
ρvi,vi= max
v1,...,v k,τ∥∇D2
vi,vig(v1, . . . , v k, τ)∥
≤max
v1,...,v k,τ{|D3
vi,vi,vig(v1, . . . , v k, τ)|+|D3
vi,vi,vjg(v1, . . . , v k, τ)|+|D3
vi,vi,τg(v1, . . . , v k, τ)|}
≤min
τ(k/τ4).
Similarly, for D2
vi,vjg, we obtain ρvi,vj≤minτ(kε/τ4)forD2
vi,τg, we obtain ρvi,τ≤
minτ(k2ε2/τ5)and for D2
τ,τ, we obtain ρτ,τ≤minτ(k2ε2/τ6).
Substituting Mandσ2back, and assuming that the minimum value for σ2we allow is σ2
0we have:
29∥H(M1, σ2
1)−H(M2, σ2
2)∥2=kX
i=1
gvi,vi(M1, σ2
1)−gvi,vi(M2, σ2
2)2+
gσ2,σ2(M1, σ2
1)−gσ2,σ2(M2, σ2
2)2
+X
i̸=j2
gvi,vj(M1, σ2
1)−gvi,vj(M2, σ2
2)2+ 2kX
i=1
gvi,σ2(M1, σ2
1)−gvi,σ2(M2, σ2
2)
|2
≤ 
kρ2
v1,v1+ρ2
σ2,σ2+ 2(k×d−2k−1)ρ2
v1,v2+ 2kρ2
v1,σ2
(M1−M2)2+ (σ2
1−σ2
2)2
.
Therefore, we get:
∥H(M1, σ2
1)−H(M2, σ2
2)∥ ≤k2
σ8
0
+k2ε2
σ12
0
+dk2ε
σ8
0
+k3ε2
σ10
0
∥(M1, σ2
1)−(M2, σ2
2)∥
≡poly
k, ε, d,1
σ0
∥(M1, σ2
1)−(M2, σ2
2)∥.
Finally, since we are summing over ndata points, the Hessian Lipschitz constant, is
poly(n, k, ε, d, 1/σ0).
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract summarizes the findings presented in the paper, while the in-
troduction summarizes the motivations behind our work and reviews prior research in the
field.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [No]
Justification: The paper focuses primarily on presenting a theoretical result. As such, it does
not delve into empirical validations or practical implementations where limitations would
typically be more relevant. The nature of the result is more abstract and formal, and thus,
potential limitations about practical application, empirical robustness, or computational
scalability are not directly addressed within the scope of this work.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
313.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All lemmas and theorem statements provide the full set of assumptions. We
provide proof sketches in the main body and complete and correct proofs for all our results
in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We present a simulation in the main body to demonstrate our method, along
with a detailed explanation of the simulation process.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
32(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We have provided the code we ran for our simulations in the supplementary
material.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide all the relevant information about the simulation in the main body
and appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We provide all the relevant information about the simulation error in the main
body and appendix.
Guidelines:
33• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: Given that this is a theoretical result, the experiments did not require substantial
computational resources, and we conducted all experiments on local machines.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper fully adheres to the NeurIPS Code of
Ethics in every respect.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
34Justification: Since this is a theoretical result, we do not foresee any societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Since this is a theoretical result, we do not think it has any risk of misuse
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
35• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
36•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
37