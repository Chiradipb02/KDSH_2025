Nearly Lossless Adaptive Bit Switching
Anonymous Author(s)
Affiliation
Address
email
Abstract
Model quantization is widely applied for compressing and accelerating deep neural 1
networks (DNNs). However, conventional Quantization-Aware Training (QAT) 2
focuses on training DNNs with uniform bit-width. The bit-width settings vary 3
across different hardware and transmission demands, which induces considerable 4
training and storage costs. Hence, the scheme of one-shot joint training multiple 5
precisions is proposed to address this issue. Previous works either store a larger 6
FP32 model to switch between different precision models for higher accuracy or 7
store a smaller INT8 model but compromise accuracy due to using shared quanti- 8
zation parameters. In this paper, we introduce the Double Rounding quantization 9
method, which fully utilizes the quantized representation range to accomplish 10
nearly lossless bit-switching while reducing storage by using the highest integer 11
precision instead of full precision. Furthermore, we observe a competitive inter- 12
ference among different precisions during one-shot joint training, primarily due 13
to inconsistent gradients of quantization scales during backward propagation. To 14
tackle this problem, we propose an Adaptive Learning Rate Scaling ( ALRS ) tech- 15
nique that dynamically adapts learning rates for various precisions to optimize the 16
training process. Additionally, we extend our Double Rounding to one-shot mixed 17
precision training and develop a Hessian-Aware Stochastic Bit-switching ( HASB ) 18
strategy. Experimental results on the ImageNet-1K classification demonstrate that 19
our methods have enough advantages to state-of-the-art one-shot joint QAT in both 20
multi-precision and mixed-precision. Our codes are available at here. 21
1 Introduction 22
Recently, with the popularity of mobile and edge devices, more and more researchers have attracted 23
attention to model compression due to the limitation of computing resources and storage. Model 24
quantization [ 1;2] has gained significant prominence in the industry. Quantization maps floating-point 25
values to integer values, significantly reducing storage requirements and computational resources 26
without altering the network architecture. 27
Generally, for a given pre-trained model, the quantization bit-width configuration is predefined for a 28
specific application scenario. The quantized model then undergoes retraining, i.e., QAT, to mitigate 29
the accuracy decline. However, when the model is deployed across diverse scenarios with different 30
precisions, it often requires repetitive retraining processes for the same model. A lot of computing 31
resources and training costs are wasted. To address this challenge, involving the simultaneous 32
training of multi-precision [ 3;4] or one-shot mixed-precision [ 3;5] have been proposed. Among 33
these approaches, some involve sharing weight parameters between low-precision and high-precision 34
models, enabling dynamic bit-width switching during inference. 35
However, bit-switching from high precision (or bit-width) to low precision may introduce significant 36
accuracy degradation due to the Rounding operation in the quantization process. Additionally, there is 37
severe competition in the convergence process between higher and lower precisions in multi-precision 38
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.FP32
INT8
INT6
INT4INT2First 
Rounding
High bitSecondRounding
Low bitAdaptiveLearn Rate6bit
2bit4bit8bit
8bit
6bit
2bit4bitBit Mixing
SelectionAdaptive 
Learning RateDouble 
Round Bit 
SwitchMulti-Precision Mixed-Precision Full-Precision 2bit 4bit 
6bit 8bit 
layer
layer8bit
2bit100%
0%ProbabilityStochastic Bit-switchingSensitivity
(a) Saving only 8 -bit representation 
for various low-precisions(b) Stabilize learning process 
across various precisions (c) Sensitivity-aware bit selection 
for different layersFigure 1: Overview of our proposed lossless adaptive bit-switching strategy.
scheme. In mixed-precision scheme, previous methods often incur vast searching and retraining costs 39
due to decoupling the training and search stages. Due to the above challenges, bit-switching remains 40
a very challenging problem. Our motivation is designing a bit-switching quantization method that 41
doesn’t require storing a full-precision model and achieves nearly lossless switching from high-bits to 42
low-bits. Specifically, for different precisions, we propose unified representation, normalized learning 43
steps, and tuned probability distribution so that an efficient and stable learning process is achieved 44
across multiple and mixed precisions, as depicted in Figure 1. 45
To solve the bit-switching problem, prior methods either store the floating-point parameters [ 6;7;4;8] 46
to avoid accuracy degradation or abandon some integer values by replacing rounding with floor [3;9] 47
but leading to accuracy decline or training collapse at lower bit-widths. We propose Double Rounding , 48
which applies the rounding operation twice instead of once, as shown in Figure1 (a). This approach 49
ensures nearly lossless bit-switching and allows storing the highest bit-width model instead of the 50
full-precision model. Specifically, the lower precision weight is included in the higher precision 51
weight, reducing storage constraints. 52
Moreover, we empirically find severe competition between higher and lower precisions, particularly 53
in 2-bit precision, as also noted in [ 10;4]. There are two reasons for this phenomenon: The optimal 54
quantization interval itself is different for higher and lower precisions. Furthermore, shared weights 55
are used for different precisions during joint training, but the quantization interval gradients for 56
different precisions exhibit distinct magnitudes during training. Therefore, we introduce an Adaptive 57
Learning Rate Scaling (ALRS) method, designed to dynamically adjust the learning rates across 58
different precisions, which ensures consistent update steps of quantization scales corresponding to 59
different precisions, as shown in the Figure 1 (b). 60
Finally, we develop an efficient one-shot mixed-precision quantization approach based on Double 61
Rounding . Prior mixed-precision approaches first train a SuperNet with predefined bit-width lists, 62
then search for optimal candidate SubNets under restrictive conditions, and finally retrain or fine-tune 63
them, which incurs significant time and training costs. However, we use the Hessian Matrix Trace [ 11] 64
as a sensitivity metric for different layers to optimize the SuperNet and propose a Hessian-Aware 65
Stochastic Bit-switching (HASB) strategy, inspired by the Roulette algorithm [ 12]. This strategy 66
enables tuned probability distribution of switching bit-width across layers, assigning higher bits to 67
more sensitive layers and lower bits to less sensitive ones, as shown in Figure 1 (c). And, we add the 68
sensitivity to the search stage as a constraint factor. So, our approach can omit the last stage. 69
2In conclusion, our main contributions can be described as: 70
•Double Rounding quantization method for multi-precision is proposed, which stores a single 71
integer weight to enable adaptive precision switching with nearly lossless accuracy. 72
•Adaptive Learning Rate Scaling (ALRS) method for the multi-precision scheme is intro- 73
duced, which effectively narrows the training convergence gap between high-precision 74
and low-precision, enhancing the accuracy of low-precision models without compromising 75
high-precision model accuracy. 76
•Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision 77
SuperNet is applied, where the access probability of bit-width for each layer is determined 78
based on the layer’s sensitivity. 79
•Experimental results on the ImageNet1K dataset demonstrate that our proposed methods are 80
comparable to state-of-the-art methods across different mainstream CNN architectures. 81
2 Related Works 82
Multi-Precision. Multi-Precision entails a single shared model with multiple precisions by one-shot 83
joint Quantization-Aware Training (QAT). This approach can dynamically adapt uniform bit-switching 84
for the entire model according to computing resources and storage constraints. AdaBits [ 13] is the 85
first work to consider adaptive bit-switching but encounters convergence issues with 2-bit quantization 86
on ResNet50 [ 14]. Bit-Mixer [ 9] addresses this problem by using the LSQ [ 2] quantization method 87
but discards the lowest state quantized value, resulting in an accuracy decline. Multi-Precision 88
joint QAT can also be viewed as a multi-objective optimization problem. Any-precision [ 6] and 89
MultiQuant [ 4] combine knowledge distillation techniques to improve model accuracy. Among these 90
methods, MultiQuant’s proposed "Online Adaptive Label" training strategy is essentially a form of 91
self-distillation [ 15]. Similar to our method, AdaBits and Bit-Mixer can save an 8-bit model, while 92
other methods rely on 32-bit models for bit switching. Our Double Rounding method can store the 93
highest bit-width model (e.g., 8-bit) and achieve almost lossless bit-switching, ensuring a stable 94
optimization process. Importantly, this leads to a reduction in training time by approximately 10% [ 7] 95
compared to separate quantization training. 96
One-shot Mixed-Precision. Previous works mainly utilize costly approaches, such as reinforcement 97
learning [16; 17] and Neural Architecture Search (NAS) [18; 19; 20], or rely on partial prior knowl- 98
edge [ 21;22] for bit-width allocation, which may not achieve global optimality. In contrast, our 99
proposed one-shot mixed-precision method employs Hessian-Aware optimization to refine a SuperNet 100
via gradient updates, and then obtain the optimal conditional SubNets with less search cost without 101
retraining or fine-tuning. Additionally, Bit-Mixer [ 9] and MultiQuant [ 4] implement layer-adaptive 102
mixed-precision models, but Bit-Mixer uses a naive search method to attain a sub-optimal solution, 103
while MultiQuant requires 300 epochs of fine-tuning to achieve ideal performance. Unlike NAS 104
approaches [ 20], which focus on altering network architecture (e.g., depth, kernel size, or channels), 105
our method optimizes a once-for-all SuperNet using only quantization techniques without altering 106
the model architecture. 107
3 Methodology 108
3.1 Double Rounding 109
Conventional separate precision quantization using Quantization-Aware Training (QAT) [ 23] attain 110
a fixed bit-width quantized model under a pre-trained FP32 model. A pseudo-quantization node is 111
inserted into each layer of the model during training. This pseudo-quantization node comprises two 112
operations: the quantization operation quant (x), which maps floating-point (FP32) values to lower- 113
bit integer values, and the dequantization operation dequant (x), which restores the quantized integer 114
value to its original floating-point representation. It can simulate the quantization error incurred 115
when compressing float values into integer values. As quantization involves a non-differentiable 116
Rounding operation, Straight-Through Estimator (STE) [ 24] is commonly used to handle the non- 117
differentiability. 118
However, for multi-precision quantization, bit-switching can result in significant accuracy loss, 119
especially when transitioning from higher bit-widths to lower ones, e.g., from 8-bit to 2-bit. To 120
3/uni00000014/uni00000011/uni00000013
 /uni00000013/uni00000011/uni00000018
 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013
/uni0000005b/uni00000014/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni0000005cStorage:32bit/uni0000002f/uni00000036/uni00000034
2bit
3bit
4bit
/uni00000014/uni00000011/uni00000013
 /uni00000013/uni00000011/uni00000018
 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013
/uni0000005b/uni00000014/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni0000005cStorage:4bit/uni00000024/uni00000047/uni00000044/uni00000025/uni0000004c/uni00000057/uni00000056
2bit
3bit
4bit
/uni00000014/uni00000011/uni00000013
 /uni00000013/uni00000011/uni00000018
 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013
/uni0000005b/uni00000014/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni0000005cStorage:4bit/uni00000025/uni0000004c/uni00000057/uni00000010/uni00000050/uni0000004c/uni0000005b/uni00000048/uni00000055
2bit
3bit
4bit
/uni00000014/uni00000011/uni00000013
 /uni00000013/uni00000011/uni00000018
 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013
/uni0000005b/uni00000014/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni0000005cStorage:4bit/uni00000027/uni00000052/uni00000058/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000035/uni00000052/uni00000058/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a
2bit
3bit
4bitFigure 2: Comparison of four quantization schemes:(from left to right) used in LSQ [2],AdaBits [3],
Bit-Mixer [9] and Ours Double Rounding . In all cases y=dequant (quant (x)).
mitigate this loss, prior works have mainly employed two strategies: one involves bit-switching from 121
a floating-point model (32-bit) to a lower-bit model each time using multiple learnable quantization 122
parameters, and the other substitutes the Rounding operation with the Floor operation, but this 123
results in accuracy decline (especially in 2-bit). In contrast, we propose a nearly lossless bit- 124
switching quantization method called Double Rounding . This method overcomes these limitations by 125
employing a Rounding operation twice. It allows the model to be saved in the highest-bit ( e.g., 8-bit) 126
representation instead of full-precision, facilitating seamless switching to other bit-width models. A 127
detailed comparison of Double Rounding with other quantization methods is shown in Figure 2. 128
Unlike AdaBits, which relies on the Dorefa [ 1] quantization method where the quantization scale is 129
determined based on the given bit-width, the quantization scale of our Double Rounding is learned 130
online and is not fixed. It only requires a pair of shared quantization parameters, i.e.,scale and 131
zero-point . Quantization scales of different precisions adhere to a strict "Power of Two" relationship. 132
Suppose the highest-bit and the target low-bit are denoted as h-bit and l-bit respectively, and the 133
difference between them is ∆ =h−l. The specific formulation of Double Rounding is as follows: 134
fWh=clip(W−zh
sh
,−2h−1,2h−1−1) (1)
fWl=clip($
fWh
2∆'
,−2l−1,2l−1−1) (2)
cWl=fWl×sh×2∆+zh (3)
where the symbol ⌊.⌉denotes the Rounding function, and clip(x, low, upper )means xis limited 135
to the range between low andupper . Here, Wrepresents the FP32 model’s weights, sh∈R 136
andzh∈Zdenote the highest-bit ( e.g., 8-bit) quantization scale andzero-point respectively. fWh 137
represent the quantized weights of the highest-bit, while fWlandcWlrepresent the quantized weights 138
and dequantized weights of the low-bit respectively. 139
Hardware shift operations can efficiently execute the division and multiplication by 2∆. Note that in 140
ourDouble Rounding , the model can also be saved at full precision by using unshared quantization 141
parameters to run bit-switching and attain higher accuracy. Because we use symmetric quantization 142
scheme, the zhis0. Please refer to Section A.4 for the gradient formulation of Double Rounding . 143
Unlike fixed weights, activations change online during inference. So, the corresponding scale and 144
zero-point values for different precisions can be learned individually to increase overall accuracy. 145
Suppose Xdenotes the full precision activation, and fXbandcXbare the quantized activation and 146
dequantized activation respectively. The quantization process can be formulated as follows: 147
fXb=clip(X−zb
sb
,0,2b−1) (4)
cXb=fXb×sb+zb (5)
where sb∈Randzb∈Zrepresent the quantization scale andzero-point of different bit-widths 148
activation respectively. Note that zbis0for the ReLU activation function. 149
3.2 Adaptive Learning Rate Scaling for Multi-Precision 150
Although our proposed Double Rounding method represents a significant improvement over most 151
previous multi-precision works, the one-shot joint optimization of multiple precisions remains 152
constrained by severe competition between the highest and lowest precisions [ 10;4]. Different 153
precisions simultaneously impact each other during joint training, resulting in substantial differences 154
4in convergence rates between them, as shown in Figure 3 (c). We experimentally find that this 155
competitive relationship stems from the inconsistent magnitudes of the quantization scale’s gradients 156
between high-bit and low-bit quantization during joint training, as shown in Figure 3 (a) and (b). For 157
other models statistical results please refer to Section A.6 in the appendix. 158
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013/uni00000014/uni00000014/uni00000014/uni00000015/uni00000014/uni00000016/uni00000014/uni00000017/uni00000014/uni00000018/uni00000014/uni00000019/uni00000014/uni0000001a/uni00000014/uni0000001b
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013
/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000018/uni0000002a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000056/uni00000003/uni00000052/uni00000049/uni00000003/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013/uni00000014/uni00000014/uni00000014/uni00000015/uni00000014/uni00000016/uni00000014/uni00000017/uni00000014/uni00000018/uni00000014/uni00000019/uni00000014/uni0000001a/uni00000014/uni0000001b
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013
/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000018/uni0000002a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000056/uni00000003/uni00000052/uni00000049/uni00000003/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000003/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048
1 11 21 31 61 71 81 41 51
Epoch510152070
65
60
55
50
45
40
35
30
25Acc1(%)
8bit
6bit
4bit
2bitgap
111 21 31 41 51 61 71 81
Epoch510152025303540455055606570 Acc1(%)
8bit
6bit
4bit
2bit
(a) 2-bit (b) 4-bit (c) w/o ALRS (d) w. ALRS
Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale
gradients’ statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision
training processes of our Double Rounding without and with the ALRS strategy.
Motivated by these observations, we introduce a technique termed Adaptive Learning Rate Scaling 159
(ALRS), which dynamically adjusts learning rates for different precisions to optimize the training 160
process. This technique is inspired by the Layer-wise Adaptive Rate Scaling (LARS) [ 25] optimizer. 161
Specifically, suppose the current batch iteration’s learning rate is λ, we set learning rates λbof 162
different precisions as follows: 163
λb=ηb 
λ−LX
i=1min 
max_abs 
clip_grad (∇si
b,1.0)
,1.0
L!
, (6)
ηb=(
1×10−∆
2, if∆is even
5×10−(∆+1
2),if∆is odd(7)
where the Lis the number of layers, clip_grad (.)represents gradient clipping that prevents gradient 164
explosion, max_abs (.)denotes the maximum absolute value of all elements. The ∇si
bdenotes the 165
quantization scale’s gradients of layer iandηbdenotes scaling hyperparameter of different precisions, 166
e.g., 8-bit is 1, 6-bit is 0.1, and 4-bit is 0.01. Note that the ALRS strategy is only used for updating 167
quantization scales. It can adaptively update the learning rates of different precisions and ensure 168
that model can optimize quantization parameters at the same pace, ultimately achieving a minimal 169
convergence gap in higher bits and 2-bit, as shown in Figure 3 (d). 170
In multi-precision scheme, different precisions share the same model weights during joint training. 171
For conventional multi-precision, the shared weight computes nforward processes at each training 172
iteration, where nis the number of candidate bit-widths. The losses attained from different precisions 173
are then accumulated, and the gradients are computed. Finally, the shared parameters are updated. 174
For detailed implementation please refer to Algorithm A.1 in the appendix. However, we find that 175
if different precision losses separately compute gradients and directly update shared parameters at 176
each forward process, it attains better accuracy when combined with our ALRS training strategy. 177
Additionally, we use dual optimizers to update the weight parameters and quantization parameters 178
simultaneously. We also set the weight-decay of the quantization scales to 0to achieve stable 179
convergence. For detailed implementation please refer to Algorithm A.2 in the appendix. 180
3.3 One-Shot Mixed-Precision SuperNet 181
Unlike multi-precision, where all layers uniformly utilize the same bit-width, mixed-precision 182
SuperNet provides finer-grained adaptive by configuring the bit-width at different layers. Previous 183
methods typically decouple the training and search stages, which need a third stage for retraining 184
or fine-tuning the searched SubNets. These approaches generally incur substantial search costs in 185
selecting the optimal SubNets, often employing methods such as greedy algorithms [ 26;9] or genetic 186
algorithms [ 27;4]. Considering the fact that the sensitivity [ 28],i.e., importance, of each layer 187
is different, we propose a Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot 188
mixed-precision training. 189
Specifically, the Hessian Matrix Trace (HMT) is utilized to measure the sensitivity of each layer. We 190
first need to compute the pre-trained model’s HMT by around 1000 training images [ 11], as shown in 191
58bit 2bit 4bit 6bitProbability
25% 25% 25% 25%
8bit 2bit 4bit 6bitProbability
25% 25% 25% 25%
8bit 2bit 4bit 6bitProbability
25% 25% 25% 25%
8bit 2bit 4bit 6bitProbability
10%20%30%40%
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013/uni00000014/uni00000014/uni00000014/uni00000015/uni00000014/uni00000016/uni00000014/uni00000017/uni00000014/uni00000018/uni00000014/uni00000019/uni00000014/uni0000001a/uni00000014/uni0000001b
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002b/uni00000048/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051/uni00000003/uni00000057/uni00000055/uni00000044/uni00000046/uni00000048weights
/uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b
/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000010/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b/uni00000019/uni00000018/uni00000019/uni00000019/uni00000019/uni0000001a/uni00000019/uni0000001b/uni00000019/uni0000001c/uni0000001a/uni00000013/uni0000001a/uni00000014/uni00000024/uni00000046/uni00000046/uni00000014/uni0000000b/uni00000008/uni0000000c
w. HASB
w/o HASB(a) Unsensitive (b) Sensitive (c) Hessian trace (d) Mixed precision
Figure 4: The HASB stochastic process and Mixed-precision of ResNet18 for {2,4,6,8}-bit.
Figure 4 (c). Then, the HMT of different layers is utilized as the probability metric for bit-switching. 192
Higher bits are priority selected for sensitive layers, while all candidate bits are equally selected for 193
unsensitive layers. Our proposed Roulette algorithm is used for bit-switching processes of different 194
layers during training, as shown in the Algorithm 1. If a layer’s HMT exceeds the average HMT of 195
all layers, it is recognized as sensitive, and the probability distribution of Figure 4 (b) is used for bit 196
selection. Conversely, if the HMT is below the average, the probability distribution of Figure 4 (a) is 197
used for selection. Finally, the Integer Linear Programming (ILP) [ 29] algorithm is employed to find 198
the optimal SubNets. Considering each layer’s sensitivity during training and adding this sensitivity 199
to the ILP’s constraint factors ( e.g., model’s FLOPs, latency, and parameters), which depend on 200
the actual deployment requirements. We can efficiently attain a set of optimal SubNets during the 201
search stage without retraining, thereby significant reduce the overall costs. All the searched SubNets 202
collectively constitute the Pareto Frontier optimal solution, as shown in Figure 4 (d). For detailed 203
mixed-precision training and searching process ( i.e., ILP) please refer to the Algorithm A.3 and the 204
Algorithm 2 respectively.
Algorithm 1 Roulette algorithm for bit-switching
Require: Candidate bit-widths set b∈B, the HMT of
current layer: tl, average HMT: tm;
1: Sample r∼U(0,1]from a uniform distribution;
2:iftl< tmthen
3: Compute bit-switching probability of all candi-
datebiwithpi= 1/n;
4: Set s= 0, andi= 0;
5: while s < r do
6: i=i+ 1;
7: s=pi+s;
8: end while
9:else
10: Compute bit-switching probability of all candi-
datebiwithpi=bi/∥B∥1;
11: Set s= 0, andi= 0;
12: while s < r do
13: i=i+ 1;
14: s=pi+s;
15: end while
16:end if
17:return bi;
Note thatnandLrepresent the number of candidate bit-widths and
model layers respectively, and ∥ · ∥ 1isL1norm.Algorithm 2 Our searching process for SubNets
Input: Candidate bit-widths set b∈B, the HMT of
different layers of FP32 model: tl∈{T}L
l=1, the
constraint average bit-width: ω, each layer param-
eters: nl∈{N}L
l=1;
1: Initial searched SubNets’solutions: S=ϕ
2: Minimal objective : O=PL
l=1tl
nl·bl
3: Constraints: ω≡PL
l=1bl
L4:The first solve: s1=pulp.solve (O, ω)and
S.append (s1)
5:forciins1do
6: forbinB[:idenx (max(s1))]do
7: ifb̸=cithen
8: Add constraint: b≡ci
9: Solve: s=pulp.solve (O, ω, b )
10: ifsnot in Sthen
11: S.append (s)
12: end if
13: Pop last constraint: b≡ci
14: end if
15: end for
16:end for
17:return S
205
4 Experimental Results 206
Setup. In this paper, we mainly focus on ImageNet-1K [ 30] classification task using both classical 207
networks (ResNet18/50 [ 14]) and lightweight networks (MobileNetV2 [ 31]), which same as previous 208
works. Experiments cover joint quantization training for multi-precision and mixed precision. We 209
explore two candidate bit configurations, i.e., {8,6,4,2}-bit and {4,3,2}-bit, each number represents 210
the quantization level of the weight and activation layers. Like previous methods, we exclude batch 211
6normalization layers from quantization, and the first and last layers are kept at full precision. We 212
initialize the multi-precision models with a pre-trained FP32 model, and initialize the mixed-precision 213
models with a pre-trained multi-precision model. All models use the Adam optimizer [ 32] with a batch 214
size of 256for 90 epochs and use a cosine scheduler without warm-up phase. The initial learning 215
rate is 5e-4 and weight decay is 5e-5. Data augmentation uses the standard set of transformations 216
including random cropping, resizing to 224 ×224 pixels, and random flipping. Images are resized to 217
256×256 pixels and then center-cropped to 224 ×224 resolution during evaluation. 218
4.1 Multi-Precision 219
Results. For {8,6,4,2}-bit configuration, the Top-1 validation accuracy is shown in Table 1. The 220
network weights and the corresponding activations are quantized into w-bit and a-bit respectively. 221
Ourdouble-rounding combined with ALRS training strategy surpasses the previous state-of-the-art 222
(SOTA) methods. For example, in ResNet18, it exceeds Any-Precision [ 6] by 2.7%(or 2.83%) under 223
w8a8 setting without(or with) using KD technique [ 15], and outperforms MultiQuant [ 4] by 0.63%(or 224
0.73%) under w4a4 setting without(or with) using KD technique respectively. Additionally, when 225
the candidate bit-list includes 2-bit, the previous methods can’t converge on MobileNetV2 during 226
training. So, they use {8,6,4}-bit precision for MobileNetV2 experiments. For consistency, we 227
also test {8,6,4}-bit results, as shown in the "Ours {8,6,4}-bit " rows of Table 1. Our method achieves 228
0.25%/0.11%/0.56% higher accuracy than AdaBits [3] under the w8a8/w6a6/w4a4 settings. 229
Notably, our method exhibits the ability to converge but shows a big decline in accuracy on Mo- 230
bileNetV2. On the one hand, the compact model exhibits significant differences in the quantization 231
scale gradients of different channels due to involving DeepWise Convolution [ 33]. On the other hand, 232
when the bit-list includes 2-bit, it intensifies competition between different precisions during training. 233
To improve the accuracy of compact models, we suggest considering the per-layer or per-channel 234
learning rate scaling techniques in future work.
Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets.
’KD’ denotes knowledge distillation. The " −" represents the unqueried value.
Model Method KD Storage Epoch w8a8 w6a6 w4a4 w2a2 FP
ResNet18Hot-Swap[34] ✗ 32bit − 70.40 70.30 70.20 64.90 −
L1[35] ✗ 32bit − 69.92 66.39 0.22 − 70.07
KURE[36] ✗ 32bit 80 70.20 70.00 66.90 − 70.30
Ours ✗ 8bit 90 70.74 70.71 70.43 66.35 69.76
Any-Precision[6] ✓ 32bit 80 68.04 − 67.96 64.19 69.27
CoQuant[7] ✓ 8bit 100 67.90 67.60 66.60 57.10 69.90
MultiQuant[4] ✓ 32bit 90 70.28 70.14 69.80 66.56 69.76
Ours ✓ 8bit 90 70.87 70.79 70.53 66.84 69.76
ResNet50Any-Precision[6] ✗ 32bit 80 74.68 − 74.43 72.88 75.95
Hot-Swap[34] ✗ 32bit − 75.60 75.50 75.30 71.90 −
KURE[36] ✗ 32bit 80 − 76.20 74.30 − 76.30
Ours ✗ 8bit 90 76.51 76.28 75.74 72.31 76.13
Any-Precision[6] ✓ 32bit 80 74.91 − 74.75 73.24 75.95
MultiQuant[4] ✓ 32bit 90 76.94 76.85 76.46 73.76 76.13
Ours ✓ 8bit 90 76.98 76.86 76.52 73.78 76.13
MobileNetV2AdaBits[3] ✗ 8bit 150 72.30 72.30 70.30 − 71.80
KURE[36] ✗ 32bit 80 − 70.00 59.00 − 71.30
Ours {8,6,4}-bit ✗ 8bit 90 72.42 72.06 69.92 − 71.14
MultiQuant[4] ✓ 32bit 90 72.33 72.09 70.59 − 71.88
Ours {8,6,4}-bit ✓ 8bit 90 72.55 72.41 70.86 − 71.14
Ours {8,6,4,2}-bit ✗ 8bit 90 70.98 70.70 68.77 50.43 71.14
Ours {8,6,4,2}-bit ✓ 8bit 90 71.35 71.20 69.85 53.06 71.14
235
For {4,3,2}-bit configuration, Table 2 demonstrate that our double-rounding consistently surpasses 236
previous SOTA methods. For instance, in ResNet18, it exceeds Bit-Mixer [ 9] by 0.63%/0.7%/1.2%(or 237
0.37%/0.64%/1.02%) under w4a4/w3a3/w2a2 settings without(or with) using KD technique, and 238
outperforms ABN[ 10] by 0.87%/0.74%/1.12% under w4a4/w3a3/w2a2 settings with using KD 239
technique respectively. In ResNet50, Our method outperforms Bit-Mixer [ 9] by 0.86%/0.63%/0.1% 240
under w4a4/w3a3/w2a2 settings. 241
Notably, the overall results of Table 2 are worse than the {8,6,4,2}-bit configuration for joint training. 242
We analyze that this discrepancy arises from information loss in the shared lower precision model 243
7(i.e., 4-bit) used for bit-switching. In other words, compared with 4-bit, it is easier to directly optimize 244
8-bit quantization parameters to converge to the optimal value. So, we recommend including 8-bit for 245
multi-precision training. Furthermore, independently learning the quantization scales for different 246
precisions, including weights and activations, significantly improves accuracy compared to using 247
shared scales. However, it requires saving the model in 32-bit format, as shown in "Ours*" of Table 2. 248
Table 2: Top1 accuracy comparisons on multi-precision of {4,3,2}-bit on ImageNet-1K datasets.
Model Method KD Storage Epoch w4a4 w3a3 w2a2 FP
ResNet18Bit-Mixer[9] ✗ 4bit 160 69.10 68.50 65.10 69.60
Vertical-layer[37] ✗ 4bit 300 69.20 68.80 66.60 70.50
Ours ✗ 4bit 90 69.73 69.20 66.30 69.76
Q-DNNs[7] ✓ 32bit 45 66.94 66.28 62.91 68.60
ABN[10] ✓ 4bit 160 68.90 68.60 65.50 −
Bit-Mixer[9] ✓ 4bit 160 69.40 68.70 65.60 69.60
Ours ✓ 4bit 90 69.77 69.34 66.62 69.76
ResNet50Ours ✗ 4bit 90 75.81 75.24 71.62 76.13
AdaBits[3] ✗ 32bit 150 76.10 75.80 73.20 75.00
Ours* ✗ 32bit 90 76.42 75.82 73.28 76.13
Bit-Mixer[9] ✓ 4bit 160 75.20 74.90 72.70 −
Ours ✓ 4bit 90 76.06 75.53 72.80 76.13
4.2 Mixed-Precision 249
Results. We follow previous works to conduct mixed-precision experiments based on the {4,3,2}-bit 250
configuration. Our proposed one-shot mixed-precision joint quantization method with the HASB tech- 251
nique comparable to the previous SOTA methods, as presented in Table 3. For example, in ResNet18, 252
our method exceeds Bit-Mixer [ 9] by 0.83%/0.72%/0.77%/7.07% under w4a4/w3a3/w2a2/3MP 253
settings and outperforms EQ-Net[ 5] by 0.2% under 3MP setting. The results demonstrate the effec- 254
tiveness of one-shot mixed-precision joint training to consider sensitivity with Hessian Matrix Trace 255
when randomly allocating bit-widths for different layers. Additionally, Table 3 reveals that our results 256
do not achieve optimal performance across all settings. We hypothesize that extending the number of 257
training epochs or combining ILP with other efficient search methods, such as genetic algorithms, 258
may be necessary to achieve optimal results in mixed-precision optimization. 259
Table 3: Top1 accuracy comparisons on mixed-precision of {4,3,2}-bit on ImageNet-1K dataset.
"MP" denotes average bit-width for mixed-precision. The " −" represents the unqueried value.
Model Method KD Training Searching Fine-tune Epoch w4a4 w3a3 w2a2 3MP FP
ResNet18Ours ✗ HASB ILP w/o 90 69.80 68.63 64.88 68.85 69.76
Bit-Mixer[9] ✓Random Greedy w/o 160 69.20 68.60 64.40 62.90 69.60
ABN[10] ✓ DRL DRL w. 160 69.80 69.00 66.20 67.70 −
MultiQuant[4] ✓ LRH Genetic w. 90 − 67.50 − 69.20 69.76
EQ-Net[5] ✓ LRH Genetic w. 120 − 69.30 65.90 69.80 69.76
Ours ✓ KD KD w/o 90 70.03 69.32 65.17 69.92 69.76
ResNet50Ours ✗ HASB ILP w/o 90 75.01 74.31 71.47 75.06 76.13
Bit-Mixer[9] ✓Random Greedy w/o 160 75.20 74.80 72.10 73.20 −
EQ-Net[5] ✓ LRH Genetic w. 120 − 74.70 72.50 75.10 76.13
Ours ✓ HASB ILP w/o 90 75.63 74.36 72.32 75.24 76.13
4.3 Ablation Studies 260
ALRS vs. Conventional in Multi-Precision. To verify the effectiveness of our proposed ALRS train- 261
ing strategy, we conduct an ablation experiment without KD, as shown in Table 4, and observe overall 262
accuracy improvements, particularly for the 2bit. Like previous works, where MobileNetV2 can’t 263
achieve stable convergence with {4,3,2}-bit, we also opt for {8,6,4}-bit to keep consistent. However, 264
our method can achieve stable convergence with {8,6,4,2}-bit quantization. This demonstrates the 265
superiority of our proposed Double-Rounding and ALRS methods. 266
Multi-Precision vs. Separate-Precision in Time Cost. We statistic the results regarding the time cost 267
for multi-precision compared to separate-precision quantization, as shown in Table 5. Multi-precision 268
training costs stay approximate constant as the number of candidate bit-widths. 269
8Table 4: Ablation studies of multi-precision, ResNet20 on CIFAR-10 dataset and other models on
ImageNet-1K dataset. Note that MobileNetV2 uses {8,6,4}-bit instead of {4,3,2}-bit.
Model ALRS{8,6,4,2}-bit {4,3,2}-bitFP
w8a8 w6a6 w4a4 w2a2 w4a4 w3a3 w2a2
ResNet20w/o 92.17 92.20 92.17 89.67 91.19 90.98 88.62 92.30
w. 92.25 92.32 92.09 90.19 91.79 91.83 88.88 92.30
ResNet18w/o 70.05 69.80 69.32 65.83 69.38 68.74 65.62 69.76
w. 70.74 70.71 70.43 66.35 69.73 69.20 66.30 69.76
ResNet50w/o 76.18 76.08 75.64 70.28 75.48 74.85 70.64 76.13
w. 76.51 76.28 75.74 72.31 75.81 75.24 71.62 76.13
MobileNetV2w/o 70.55 70.65 68.08 45.00 72.06 71.87 69.40 71.14
w. 70.98 70.70 68.77 50.43 72.42 72.06 69.92 71.14
Table 5: Training costs for multi-precision and separate-precision are averaged over three runs.
Model Dataset Bit-widths #V100 Epochs BatchSize Avg. hours Save cost (%)
ResNet20 Cifar10Separate-bit 1 200 128 0.9 0.0
{4,3,2}-bit 1 200 128 0.7 28.6
{8,6,4,2}-bit 1 200 128 0.8 12.5
ResNet18 ImageNetSeparate-bit 4 90 256 19.0 0.0
{4,3,2}-bit 4 90 256 15.2 25.0
{8,6,4,2}-bit 4 90 256 16.3 16.6
ResNet50 ImageNetSeparate-bit 4 90 256 51.6 0.0
{4,3,2}-bit 4 90 256 40.7 26.8
{8,6,4,2}-bit 4 90 256 40.8 26.5
Pareto Frontier of Different Mixed-Precision Configurations. To verify the effectiveness of our 270
HASB strategy, we conduct ablation experiments on different bit-lists. Figure 5 shows the search 271
results of Mixed-precision SuperNet under {8,6,4,2}-bit, {4,3,2}-bit and {8,4}-bit configurations 272
respectively. Where each point represents a SubNet. These results are obtained directly from ILP 273
sampling without retraining or fine-tuning. As the figure shows, the highest red points are higher than 274
the blue points under the same bit width, indicating that this strategy is effective. 275
/uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b
/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000010/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b/uni00000019/uni00000018/uni00000019/uni00000019/uni00000019/uni0000001a/uni00000019/uni0000001b/uni00000019/uni0000001c/uni0000001a/uni00000013/uni0000001a/uni00000014/uni00000024/uni00000046/uni00000046/uni00000014/uni0000000b/uni00000008/uni0000000c
w. HASB
w/o HASB
/uni00000015/uni00000011/uni00000013/uni00000013 /uni00000015/uni00000011/uni00000015/uni00000018 /uni00000015/uni00000011/uni00000018/uni00000013 /uni00000015/uni00000011/uni0000001a/uni00000018 /uni00000016/uni00000011/uni00000013/uni00000013 /uni00000016/uni00000011/uni00000015/uni00000018 /uni00000016/uni00000011/uni00000018/uni00000013 /uni00000016/uni00000011/uni0000001a/uni00000018 /uni00000017/uni00000011/uni00000013/uni00000013
/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000010/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b/uni00000019/uni00000018/uni00000019/uni00000019/uni00000019/uni0000001a/uni00000019/uni0000001b/uni00000019/uni0000001c/uni0000001a/uni00000013/uni0000001a/uni00000014/uni00000024/uni00000046/uni00000046/uni00000014/uni0000000b/uni00000008/uni0000000c
w. HASB
w/o HASB
/uni00000017/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni00000018/uni00000011/uni00000018 /uni00000019/uni00000011/uni00000013 /uni00000019/uni00000011/uni00000018 /uni0000001a/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni0000001b/uni00000011/uni00000013
/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000010/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b/uni00000019/uni00000018/uni00000019/uni00000019/uni00000019/uni0000001a/uni00000019/uni0000001b/uni00000019/uni0000001c/uni0000001a/uni00000013/uni0000001a/uni00000014/uni00000024/uni00000046/uni00000046/uni00000014/uni0000000b/uni00000008/uni0000000c
w. HASB
w/o HASB
(a) {8,6,4,2}-bit (b) {4,3,2}-bit (c) {8,4}-bit
Figure 5: Comparison of HASB and Baseline approaches for Mixed-Precision on ResNet18.
5 Conclusion 276
This paper first introduces Double Rounding quantization method used to address the challenges 277
of multi-precision and mixed-precision joint training. It can store single integer-weight parameters 278
and attain nearly lossless bit-switching. Secondly, we propose an Adaptive Learning Rate Scaling 279
(ALRS) method for multi-precision joint training that narrows the training convergence gap between 280
high-precision and low-precision, enhancing model accuracy of multi-precision. Finally, our proposed 281
Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision SuperNet 282
and efficient searching method combined with Integer Linear Programming, achieving approximate 283
Pareto Frontier optimal solution. Our proposed methods aim to achieve a flexible and effective model 284
compression technique for adapting different storage and computation requirements. 285
9References 286
[1]S. Zhou, Y . Wu, Z. Ni, X. Zhou, H. Wen, and Y . Zou, “Dorefa-net: Training low bitwidth convolutional 287
neural networks with low bitwidth gradients,” arXiv preprint arXiv:1606.06160 , 2016. 288
[2]S. K. Esser, J. L. McKinstry, D. Bablani, R. Appuswamy, and D. S. Modha, “Learned step size quantization,” 289
arXiv preprint arXiv:1902.08153 , 2019. 290
[3]Q. Jin, L. Yang, and Z. Liao, “Adabits: Neural network quantization with adaptive bit-widths,” in Proceed- 291
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 2146–2156. 292
[4]K. Xu, Q. Feng, X. Zhang, and D. Wang, “Multiquant: Training once for multi-bit 293
quantization of neural networks,” in IJCAI , L. D. Raedt, Ed. International Joint Conferences 294
on Artificial Intelligence Organization, 7 2022, pp. 3629–3635, main Track. [Online]. Available: 295
https://doi.org/10.24963/ijcai.2022/504 296
[5]K. Xu, L. Han, Y . Tian, S. Yang, and X. Zhang, “Eq-net: Elastic quantization neural networks,” in 297
Proceedings of the IEEE/CVF International Conference on Computer Vision , 2023, pp. 1505–1514. 298
[6]H. Yu, H. Li, H. Shi, T. S. Huang, and G. Hua, “Any-precision deep neural networks,” in Proceedings of 299
the AAAI Conference on Artificial Intelligence , vol. 35, no. 12, 2021, pp. 10 763–10 771. 300
[7]K. Du, Y . Zhang, and H. Guan, “From quantized dnns to quantizable dnns,” CoRR , vol. abs/2004.05284, 301
2020. [Online]. Available: https://arxiv.org/abs/2004.05284 302
[8]X. Sun, R. Panda, C.-F. R. Chen, N. Wang, B. Pan, A. Oliva, R. Feris, and K. Saenko, “Improved techniques 303
for quantizing deep networks with adaptive bit-widths,” in Proceedings of the IEEE/CVF Winter Conference 304
on Applications of Computer Vision , 2024, pp. 957–967. 305
[9]A. Bulat and G. Tzimiropoulos, “Bit-mixer: Mixed-precision networks with runtime bit-width selection,” 306
inProceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 5188–5197. 307
[10] C. Tang, H. Zhai, K. Ouyang, Z. Wang, Y . Zhu, and W. Zhu, “Arbitrary bit-width network: 308
A joint layer-wise quantization and adaptive inference approach,” 2022. [Online]. Available: 309
https://arxiv.org/abs/2204.09992 310
[11] Z. Dong, Z. Yao, D. Arfeen, A. Gholami, M. W. Mahoney, and K. Keutzer, “Hawq-v2: Hessian aware 311
trace-weighted quantization of neural networks,” Advances in neural information processing systems , 312
vol. 33, pp. 18 518–18 529, 2020. 313
[12] Y . Dong, R. Ni, J. Li, Y . Chen, H. Su, and J. Zhu, “Stochastic quantization for learning accurate low-bit 314
deep neural networks,” International Journal of Computer Vision , vol. 127, pp. 1629–1642, 2019. 315
[13] Q. Jin, L. Yang, and Z. Liao, “Towards efficient training for neural network quantization,” arXiv preprint 316
arXiv:1912.10207 , 2019. 317
[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” CoRR , vol. 318
abs/1512.03385, 2015. [Online]. Available: http://arxiv.org/abs/1512.03385 319
[15] K. Kim, B. Ji, D. Yoon, and S. Hwang, “Self-knowledge distillation with progressive refinement of targets,” 320
inProceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 6567–6576. 321
[16] K. Wang, Z. Liu, Y . Lin, J. Lin, and S. Han, “Haq: Hardware-aware automated quantization with mixed 322
precision,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 323
2019, pp. 8612–8620. 324
[17] A. Elthakeb, P. Pilligundla, F. Mireshghallah, A. Yazdanbakhsh, S. Gao, and H. Esmaeilzadeh, “Releq: an 325
automatic reinforcement learning approach for deep quantization of neural networks,” in NeurIPS ML for 326
Systems workshop, 2018 , 2019. 327
[18] B. Wu, Y . Wang, P. Zhang, Y . Tian, P. Vajda, and K. Keutzer, “Mixed precision quantization of convnets 328
via differentiable neural architecture search,” arXiv preprint arXiv:1812.00090 , 2018. 329
[19] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y . Wei, and J. Sun, “Single path one-shot neural architecture 330
search with uniform sampling,” in Computer Vision–ECCV 2020: 16th European Conference, Glasgow, 331
UK, August 23–28, 2020, Proceedings, Part XVI 16 . Springer, 2020, pp. 544–560. 332
[20] M. Shen, F. Liang, R. Gong, Y . Li, C. Li, C. Lin, F. Yu, J. Yan, and W. Ouyang, “Once quantization-aware 333
training: High performance extremely low-bit architecture search,” in Proceedings of the IEEE/CVF 334
International Conference on Computer Vision (ICCV) , October 2021, pp. 5340–5349. 335
10[21] J. Liu, J. Cai, and B. Zhuang, “Sharpness-aware quantization for deep neural networks,” arXiv preprint 336
arXiv:2111.12273 , 2021. 337
[22] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y . Wang, M. Mahoney 338
et al. , “Hawq-v3: Dyadic neural network quantization,” in International Conference on Machine Learning . 339
PMLR, 2021, pp. 11 875–11 886. 340
[23] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. G. Howard, H. Adam, and D. Kalenichenko, 341
“Quantization and training of neural networks for efficient integer-arithmetic-only inference,” CoRR , vol. 342
abs/1712.05877, 2017. [Online]. Available: http://arxiv.org/abs/1712.05877 343
[24] Y . Bengio, N. Léonard, and A. Courville, “Estimating or propagating gradients through stochastic neurons 344
for conditional computation,” arXiv preprint arXiv:1308.3432 , 2013. 345
[25] Y . You, I. Gitman, and B. Ginsburg, “Large batch training of convolutional networks,” arXiv preprint 346
arXiv:1708.03888 , 2017. 347
[26] Z. Cai and N. Vasconcelos, “Rethinking differentiable search for mixed-precision neural networks,” in 348
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 2349– 349
2358. 350
[27] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y . Wei, and J. Sun, “Single path one-shot neural architecture 351
search with uniform sampling,” in European conference on computer vision . Springer, 2020, pp. 544–560. 352
[28] Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer, “Hawq: Hessian aware quantization of 353
neural networks with mixed-precision,” in Proceedings of the IEEE/CVF International Conference on 354
Computer Vision , 2019, pp. 293–302. 355
[29] Y . Ma, T. Jin, X. Zheng, Y . Wang, H. Li, Y . Wu, G. Jiang, W. Zhang, and R. Ji, “Ompq: Orthogonal mixed 356
precision quantization,” in Proceedings of the AAAI conference on artificial intelligence , vol. 37, no. 7, 357
2023, pp. 9029–9037. 358
[30] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image 359
database,” in 2009 IEEE conference on computer vision and pattern recognition . Ieee, 2009, pp. 248–255. 360
[31] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2: Inverted residuals and 361
linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition , 362
2018, pp. 4510–4520. 363
[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980 , 364
2014. 365
[33] T. Sheng, C. Feng, S. Zhuo, X. Zhang, L. Shen, and M. Aleksic, “A quantization-friendly separable 366
convolution for mobilenets,” in 2018 1st Workshop on Energy Efficient Machine Learning and Cognitive 367
Computing for Embedded Applications (EMC2) . IEEE, 2018, pp. 14–18. 368
[34] Q. Sun, X. Li, Y . Ren, Z. Huang, X. Liu, L. Jiao, and F. Liu, “One model for all quantization: A quantized 369
network supporting hot-swap bit-width adjustment,” arXiv preprint arXiv:2105.01353 , 2021. 370
[35] M. Alizadeh, A. Behboodi, M. van Baalen, C. Louizos, T. Blankevoort, and M. Welling, “Gradient l1 371
regularization for quantization robustness,” arXiv preprint arXiv:2002.07520 , 2020. 372
[36] B. Chmiel, R. Banner, G. Shomron, Y . Nahshan, A. Bronstein, U. Weiser et al. , “Robust quantization: One 373
model to rule them all,” Advances in neural information processing systems , vol. 33, pp. 5308–5317, 2020. 374
[37] H. Wu, R. He, H. Tan, X. Qi, and K. Huang, “Vertical layering of quantized neural networks for heteroge- 375
neous inference,” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 45, no. 12, pp. 376
15 964–15 978, 2023. 377
[38] Y . Bhalgat, J. Lee, M. Nagel, T. Blankevoort, and N. Kwak, “Lsq+: Improving low-bit quantization through 378
learnable offsets and better initialization,” in Proceedings of the IEEE/CVF Conference on Computer Vision 379
and Pattern Recognition Workshops , 2020, pp. 696–697. 380
[39] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang, “Slimmable neural networks,” arXiv preprint 381
arXiv:1812.08928 , 2018. 382
11A Appendix / supplemental material 383
A.1 Overview 384
In this supplementary material, we present more explanations and experimental results. 385
• First, we provide a detailed explanation of the different quantization types under QAT. 386
• We then present a comparison of multi-precision and separate-precision on the ImageNet-1k dataset. 387
• Furthermore, we provide the gradient formulation of Double Rounding. 388
• And, the algorithm implementation of both multi-precision and mixed-precision training approaches. 389
• Finally, we provide more gradient statistics of learnable quantization scales in different networks. 390
A.2 Different Quantization Types 391
In this section, we provide a detailed explanation of the different quantization types during 392
Quantization-Aware Training (QAT), as is shown in Figure 6.
(a)Separate- Precision:  Each bit -width 
requires training a new network with 
separate weights by repeating multi -retrain.(b)Multi- Precision: A shared network can be 
quantized to any bit -width at runtime without 
re-training or finetuning. All layers inside the 
network uniformly share the same bit -width.(c)Mixed- Precision: A SuperNet whose 
individual layers can be quantized to any 
bit-width at runtime, and its searched 
subnets without re- training or fine -tuning.8bit 2bit
𝐿𝐿1
𝐿𝐿2
𝐿𝐿n32bit 6bit 8bit 2bit
𝐿𝐿1
𝐿𝐿2
𝐿𝐿n32bit 6bit 8bit 2bit
𝐿𝐿1
𝐿𝐿2
𝐿𝐿n32bit 6bit
Separate Train Layer Input Bit Mixing Data Flow Bit Switching
Figure 6: Comparison between different quantization types during quantization-aware training.
393
A.3 Multi-Precision vs. Separate-Precision. 394
We provide the comparison of Multi-Precision and Separate-Precision on ImageNet-1K dataset. 395
Table 6 shows that our Multi-Precision joint training scheme has comparable accuracy of different 396
precisions compared to Separate-Precision with multiple re-train. This further proves the effectiveness 397
of our proposed One-shot Double Rounding Multi-Precision method.
Table 6: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets.
Model Method One-shot Storage Epoch w8a8 w6a6 w4a4 w2a2 FP
ResNet18LSQ[2] ✗ {8,6,4,2}-bit 90 71.10 − 71.10 67.60 70.50
LSQ+[38] ✗ {8,6,4,2}-bit 90 − − 70.80 66.80 70.10
Ours ✓ 8-bit 90 70.74 70.71 70.43 66.35 69.76
ResNet50LSQ[2] ✗ {8,6,4,2}-bit 90 76.80 − 76.70 73.70 76.90
Ours ✓ 8-bit 90 76.51 76.28 75.74 72.31 76.13
398
A.4 The Gradient Formulation of Double Rounding 399
A general formulation for uniform quantization process is as follows: 400
fW=clip(W
s
+z,−2b−1,2b−1−1) (8)
cW= (fW−z)×s (9)
1where the symbol ⌊.⌉denotes the Rounding function, clip(x, low, upper )expresses xbelow low 401
are set to lowand above upper are set to upper .bdenotes the quantization level (or bit-width), 402
s∈Randz∈Zrepresents the quantization scale (or interval) and zero-point associated with each b, 403
respectively. Wrepresents the FP32 model’s weights, fWsignifies the quantized integer weights, and 404
cWrepresents the dequantized floating-point weights. 405
The quantization scale of our Double Rounding is learned online and not fixed. And it only needs a 406
pair of shared quantization parameters, i.e.,scale andzero-point . Suppose the highest-bit and the 407
low-bit are denoted as h-bit and l-bit respectively, and the difference between them is ∆ = h−l. 408
The specific formulation is as follows: 409
fWh=clip(W−zh
sh
,−2h−1,2h−1−1) (10)
fWl=clip($fWh
2∆'
,−2l−1,2l−1−1) (11)
cWl=fWl×sh×2∆+zh (12)
where sh∈Randzh∈Zdenote the highest-bit quantization scale andzero-point respectively. fWh 410
andfWlrepresent the quantized weights of the highest-bit and low-bit respectively. Hardware shift 411
operations can efficiently execute the division and multiplication by 2∆. And the zhis0for the 412
weight quantization in this paper. The gradient formulation of Double Rounding for one-shot joint 413
training is represented as follows: 414
∂bY
∂sh≃(j
Y−zh
shm
−Y−zh
shif n <Y−zh
sh< p,
n or p otherwise.(13)
∂bY
∂zh≃(
0if n <Y−zh
sh< p,
1otherwise.(14)
where nandpdenote the lower and upper bounds of the integer range [Nmin, Nmax]for quantizing 415
the weights or activations respectively. Yrepresents the FP32 weights or activations, and bYrepresents 416
the dequantized weights or activations. Unlike weights, activation quantization scale andzero-point 417
of different precisions are learned independently. However, the gradient formulation is the same. 418
A.5 Algorithms 419
This section provides the algorithm implementations of multi-precision, one-shot mixed-precision 420
joint training, and the search stage of SubNets. 421
A.5.1 Multi-Precision Joint Training 422
The multi-precision model with different quantization precisions shares the same model weight( e.g., 423
the highest-bit) during joint training. In conventional multi-precision, the shared weight ( e.g., multi- 424
precision model) computes nforward processes at each training iteration, where nis the number of 425
candidate bit-widths. Then, all attained losses of different precisions perform an accumulation, and 426
update the parameters accordingly. For specific implementation details please refer to Algorithm A.1. 427
However, we find that if separate precision loss and parameter updates are performed directly after 428
calculating a precision at each forward process, it will lead to difficulty convergence during training 429
or suboptimal accuracy. In other words, the varying gradient magnitudes of quantization scales of 430
different precisions make it hard to attain stable convergence during joint training. To address this 431
issue, we introduce an adaptive approach ( e.g., Adaptive Learning Rate Scaling, ALRS) to alter the 432
learning rate for different precisions during training, aiming to achieve a consistent update pace. 433
This method allows us to directly update the shared parameters after calculating the loss after every 434
forward. We update both the weight parameters and quantization parameters simultaneously using 435
dual optimizers. We also set the weight-decay of the quantization scales to 0to achieve more stable 436
convergence. For specific implementation details, please refer to Algorithm A.2. 437
2Algorithm A.1 Conventional Multi-precision training approach
Require: Candidate bit-widths set b∈B;
1:Initialize: Pretrained model Mwith FP32 weights W, the quantization scales sincluding of weights sw
and activations sx, BatchNorm layers: {BN}n
b=1, optimizer: optim (W,s, wd), learning rate: λ,wd: weight
decay, CE: CrossEntropyLoss, Dtrain : training dataset;
2: For one epoch:
3: Sample mini-batch data (x,y)∈ {Dtrain}
4:forbinBdo
5: forward (M,x,y, b):
6: foreach quantization layer do
7: cWb=dequant (quant (W,sb
w))
8: bXb=dequant (quant (X,sb
x))
9: Ob=Conv (cWb,bXb)
10: end for
11: ob=FC(W, Ob)
12: Update BNblayer
13: Compute loss: Lb=CE(ob,y)
14: Compute gradients: Lb.backward ()
15:end for
16: Update weights and scales: optim.step (λ)
17: Clear gradient: optim.zero _grad();
Note thatnandLrepresent the number of candidate bit-widths and model layers respectively.
Algorithm A.2 Our Multi-precision training approach
Require: Candidate bit-widths set b∈B
1:Initialize: Pretrained model Mwith FP32 weights W, the quantization scales sincluding of weights swand
activations sx, BatchNorm layers: {BN}n
b=1, optimizers: optim 1(W, wd ),optim 2(s, wd= 0) , learning
rate:λ,wd: weight decay, CE: CrossEntropyLoss, Dtrain : training dataset;
2: For every epoch:
3: Sample mini-batch data (x,y)∈ {Dtrain}
4:forbinBdo
5: forward (M,x,y, b):
6: foreach quantization layer do
7: cWb=dequant (quant (W,sb
w))
8: bXb=dequant (quant (X,sb
x))
9: Ob=Conv (cWb,bXb)
10: end for
11: ob=FC(W, Ob)
12: Update BNblayer
13: Compute loss: Lb=CE(ob,y)
14: Compute gradients: Lb.backward ()
15: Compute learning rate: λb # please see formula (6) of the main paper
16: Update weights and quantization scales: optim 1.step(λ);optim 2.step(λb)
17: Clear gradient: optim 1.zero _grad();optim 2.zero _grad()
18:end for
Note thatnandLrepresent the number of candidate bit-widths and model layers respectively.
A.5.2 One-shot Joint Training for Mixed Precision SuperNet 438
Unlike multi-precision joint quantization, the bit-switching of mixed-precision training is more 439
complicated. In multi-precision training, the bit-widths calculated in each iteration are fixed, e.g., 440
{8,6,4,2}-bit. In mixed-precision training, the bit-widths of different layers are not fixed in each 441
iteration, e.g., {8,random-bit,2}-bit, where "random-bit" is any bits of e.g., {7,6,5,4,3,2}-bit, similar 442
to the sandwich strategy of [ 39]. Therefore, mixed precision training often requires more training 443
epochs to reach convergence compared to multi-precision training. Bit-mixer [ 9] conducts the same 444
probability of selecting bit-width for different layers. However, we take the sensitivity of each layer 445
into consideration which uses sensitivity ( e.g.Hessian Matrix Trace [ 11]) as a metric to identify the 446
selection probability of different layers. For more sensitive layers, preference is given to higher-bit 447
widths, and vice versa. We refer to this training strategy as a Hessian-Aware Stochastic Bit-switching 448
3(HASB) strategy for optimizing one-shot mixed-precision SuperNet. Specific implementation details 449
can be found in Algorithm A.3. In additionally, unlike multi-precision joint training, the BN layers 450
are replaced by TBN (Transitional Batch-Norm) [ 9], which compensates for the distribution shift 451
between adjacent layers that are quantized to different bit-widths. To achieve the best convergence 452
effect, we propose that the threshold of bit-switching ( i.e.,σ) also increases as the epoch increases. 453
Algorithm A.3 Our one-shot Mixed-precision SuperNet training approach
Require: Candidate bit-widths set b∈B, the HMT of different layers of FP32 model: tl∈{T}L
l=1, average
HMT: tm=PL
l=1tl
L;
1:Initialize: Pretrained model Mwith FP32 weights W, the quantization scales sincluding of weights swand
activations sx, BatchNorm layers: {BN}n2
b=1, the threshold of bit-switching: σ, optimizer: optim (W,s, wd),
learning rate: λ,wd: weight decay, CE: CrossEntropyLoss, Dtrain : training dataset;
2: For one epoch:
3: Attain the threshold of bit-switching: σ=σ×epoch +1
total _epochs
4: Sample mini-batch data (x,y)∈ {Dtrain}
5:forbinBdo
6: forward (M,x,y, b, T, t m):
7: foreach quantization layer do
8: Sample r∼U[0,1];
9: ifr < σ then
10: b=Roulette (B, tl, tm) # Please refer to Algorithm 1 of the main paper
11: end if
12: cWb=dequant (quant (W,sb
w))
13: bXb=dequant (quant (X,sb
x))
14: Ob=Conv (cWb,bXb)
15: end for
16: ob=FC(W, Ob)
17: Update BNblayer
18: Compute loss: Lb=CE(ob,y)
19: Compute gradients: Lb.backward ()
20: Update weights and scales: optim.step (λ)
21: Clear gradient: optim.zero _grad();
22:end for
Note thatnandLrepresent the number of candidate bit-widths and model layers respectively.
A.5.3 Efficient one-shot searching for Mixed Precision SuperNet 454
After training the mixed-precision SuperNet, the next step is to select the appropriate optimal SubNets 455
based on conditions, such as model parameters, latency, and FLOPs, for actual deployment and 456
inference. To achieve optimal allocations for candidate bit-width under given conditions, we employ 457
the Iterative Integer Linear Programming (ILP) approach. Since each ILP run can only provide 458
one solution, we obtain multiple solutions by altering the values of different average bit widths. 459
Specifically, given a trained SuperNet ( e.g., RestNet18), it takes less than two minutes to solve 460
candidate SubNets. It can be implemented through the Python PULP package. Finally, these searched 461
SubNets only need inference to attain final accuracy, which needs a few hours. This forms a Pareto 462
optimal frontier. From this frontier, we can select the appropriate subnet for deployment. Specific 463
implementation details of the searching process by ILP can be found in Algorithm 2. 464
A.6 The Gradient Statistics of Learnable Scale of Quantization 465
In this section, we analyze the changes in gradients of the learnable scale for different models during 466
the training process. Figure 7 and Figure 8 display the gradient statistical results for ResNet20 on 467
CIFAR-10. Similarly, Figure 9 and Figure 10 show the gradient statistical results for ResNet18 on 468
ImageNet-1K, and Figure 11 and Figure 12 present the gradient statistical results for ResNet50 on 469
ImageNet-1K. These figures reveal a similarity in the range of gradient changes between higher-bit 470
quantization and 2-bit quantization. Notably, they illustrate that the value range of 2-bit quantization 471
is noticeably an order of magnitude higher than the value ranges of higher-bit quantization. 472
41234567891011121314151617181920
Layer6
4
2
024The gradient of weight scale1e2
 8bit
1234567891011121314151617181920
Layer3
2
1
0123The gradient of weight scale1e2
 6bit
1234567891011121314151617181920
Layer3
2
1
0123The gradient of weight scale1e2
 4bit
1234567891011121314151617181920
Layer8
6
4
2
02468The gradient of weight scale1e2
 2bitFigure 7: The scale gradient statistics of weight of ResNet20 on CIFAR-10 dataset. Note that the
outliers are removed for exhibition.
123456789101112131415161718
Layer2.5
2.0
1.5
1.0
0.5
0.0The gradient of activation scale1e3
 8bit
123456789101112131415161718
Layer3
2
1
01The gradient of activation scale1e4
 6bit
123456789101112131415161718
Layer1.2
1.0
0.8
0.6
0.4
0.2
0.00.2The gradient of activation scale1e3
 4bit
123456789101112131415161718
Layer4
3
2
1
01The gradient of activation scale1e3
 2bit
Figure 8: The scale gradient statistics of activation of ResNet20 on CIFAR-10 dataset. Note that the
first and last layers are not quantized.
12345678910111213141516171819
Layer6
4
2
0246The gradient of weight scale1e 2
 8bit
12345678910111213141516171819
Layer4
3
2
1
0123The gradient of weight scale1e 2
 6bit
12345678910111213141516171819
Layer4
2
024The gradient of weight scale1e 2
 4bit
12345678910111213141516171819
Layer2
1
012The gradient of weight scale1e 1
 2bit
Figure 9: The scale gradient statistics of weight of ResNet18 on ImageNet dataset. Note that the
outliers are removed for exhibition.
12345678910111213141516
Layer1.00
0.75
0.50
0.25
0.000.250.500.75The gradient of activation scale1e3
 8bit
12345678910111213141516
Layer6
4
2
0246The gradient of activation scale1e4
 6bit
12345678910111213141516
Layer4
2
024The gradient of activation scale1e4
 4bit
12345678910111213141516
Layer4
2
024The gradient of activation scale1e4
 2bit
Figure 10: The scale gradient statistics of activation of ResNet18 on ImageNet dataset. Note that the
outliers are removed for exhibition.
512345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152
Layer7.5
5.0
2.5
0.02.55.07.5The gradient of weight scale8bit
12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152
Layer4
2
024The gradient of weight scale6bit
12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152
Layer6
4
2
0246The gradient of weight scale4bit
12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152
Layer3
2
1
0123The gradient of weight scale1e2 2bitFigure 11: The scale gradient statistics of weight of ResNet50 on ImageNet dataset. Note that the
outliers are removed for exhibition, and the first and last layers are not quantized.
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer2
1
012The gradient of activation scale1e5
 8bit
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer7.5
5.0
2.5
0.02.55.07.5The gradient of activation scale1e5
 6bit
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer1.5
1.0
0.5
0.00.51.01.5The gradient of activation scale1e3
 4bit
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer6
4
2
02468The gradient of activation scale1e2
 2bit
Figure 12: The scale gradient statistics of activation of ResNet50 on ImageNet dataset. Note that the
outliers are removed for exhibition.
6NeurIPS Paper Checklist 473
1.Claims 474
Question: Do the main claims made in the abstract and introduction accurately reflect the 475
paper’s contributions and scope? 476
Answer: [Yes] 477
Justification: [TODO] Please refer to the Abstract Section and Section 1, where related 478
material for the question can be found. 479
Guidelines: 480
•The answer NA means that the abstract and introduction do not include the claims 481
made in the paper. 482
•The abstract and/or introduction should clearly state the claims made, including the 483
contributions made in the paper and important assumptions and limitations. A No or 484
NA answer to this question will not be perceived well by the reviewers. 485
•The claims made should match theoretical and experimental results, and reflect how 486
much the results can be expected to generalize to other settings. 487
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 488
are not attained by the paper. 489
2.Limitations 490
Question: Does the paper discuss the limitations of the work performed by the authors? 491
Answer: [TODO] [Yes] 492
Justification: [TODO] Although our proposed methods have achieved comparable results in 493
multi-precision and mixed-precision, this paper has several limitations and improvements. 494
(1) Due to time and computing resource constraints, our methods are only tested on common 495
CNNs-based networks and aren’t tested on ViTs-based networks. (2) For multi-precision, 496
compact networks, e.g., MobileNet, still have a big drop in 2bit. We will try to use per-layer 497
or per-channel adaptive learning rate adjustment in the future. (3) For mixed precision, 498
relying only on one-shot ILP-based SubNets search may yield a suboptimal solution. We 499
further need to combine it with other efficient search methods, e.g., genetic algorithms, to 500
achieve global optimal. 501
Guidelines: 502
•The answer NA means that the paper has no limitation while the answer No means that 503
the paper has limitations, but those are not discussed in the paper. 504
• The authors are encouraged to create a separate "Limitations" section in their paper. 505
•The paper should point out any strong assumptions and how robust the results are to 506
violations of these assumptions (e.g., independence assumptions, noiseless settings, 507
model well-specification, asymptotic approximations only holding locally). The authors 508
should reflect on how these assumptions might be violated in practice and what the 509
implications would be. 510
•The authors should reflect on the scope of the claims made, e.g., if the approach was 511
only tested on a few datasets or with a few runs. In general, empirical results often 512
depend on implicit assumptions, which should be articulated. 513
•The authors should reflect on the factors that influence the performance of the approach. 514
For example, a facial recognition algorithm may perform poorly when image resolution 515
is low or images are taken in low lighting. Or a speech-to-text system might not be 516
used reliably to provide closed captions for online lectures because it fails to handle 517
technical jargon. 518
•The authors should discuss the computational efficiency of the proposed algorithms 519
and how they scale with dataset size. 520
•If applicable, the authors should discuss possible limitations of their approach to 521
address problems of privacy and fairness. 522
•While the authors might fear that complete honesty about limitations might be used by 523
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 524
limitations that aren’t acknowledged in the paper. The authors should use their best 525
1judgment and recognize that individual actions in favor of transparency play an impor- 526
tant role in developing norms that preserve the integrity of the community. Reviewers 527
will be specifically instructed to not penalize honesty concerning limitations. 528
3.Theory Assumptions and Proofs 529
Question: For each theoretical result, does the paper provide the full set of assumptions and 530
a complete (and correct) proof? 531
Answer: [Yes] 532
Justification: [TODO] We display index numbers wherever formulas and theoretical support 533
are needed. For example, please refer to Section 3. 534
Guidelines: 535
• The answer NA means that the paper does not include theoretical results. 536
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 537
referenced. 538
•All assumptions should be clearly stated or referenced in the statement of any theorems. 539
•The proofs can either appear in the main paper or the supplemental material, but if 540
they appear in the supplemental material, the authors are encouraged to provide a short 541
proof sketch to provide intuition. 542
•Inversely, any informal proof provided in the core of the paper should be complemented 543
by formal proofs provided in appendix or supplemental material. 544
• Theorems and Lemmas that the proof relies upon should be properly referenced. 545
4.Experimental Result Reproducibility 546
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 547
perimental results of the paper to the extent that it affects the main claims and/or conclusions 548
of the paper (regardless of whether the code and data are provided or not)? 549
Answer: [Yes] To ensure that our experimental results can be reproduced: (1) we describe 550
the experimental training settings and algorithm pseudocode in detail in Section 4 and 551
Section A.5, and (2) we also provide the code related to all experiments in this paper, 552
allowing the community to improve and conduct further research. 553
Justification: [TODO] 554
Guidelines: 555
• The answer NA means that the paper does not include experiments. 556
•If the paper includes experiments, a No answer to this question will not be perceived 557
well by the reviewers: Making the paper reproducible is important, regardless of 558
whether the code and data are provided or not. 559
•If the contribution is a dataset and/or model, the authors should describe the steps taken 560
to make their results reproducible or verifiable. 561
•Depending on the contribution, reproducibility can be accomplished in various ways. 562
For example, if the contribution is a novel architecture, describing the architecture fully 563
might suffice, or if the contribution is a specific model and empirical evaluation, it may 564
be necessary to either make it possible for others to replicate the model with the same 565
dataset, or provide access to the model. In general. releasing code and data is often 566
one good way to accomplish this, but reproducibility can also be provided via detailed 567
instructions for how to replicate the results, access to a hosted model (e.g., in the case 568
of a large language model), releasing of a model checkpoint, or other means that are 569
appropriate to the research performed. 570
•While NeurIPS does not require releasing code, the conference does require all submis- 571
sions to provide some reasonable avenue for reproducibility, which may depend on the 572
nature of the contribution. For example 573
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 574
to reproduce that algorithm. 575
(b)If the contribution is primarily a new model architecture, the paper should describe 576
the architecture clearly and fully. 577
2(c)If the contribution is a new model (e.g., a large language model), then there should 578
either be a way to access this model for reproducing the results or a way to reproduce 579
the model (e.g., with an open-source dataset or instructions for how to construct 580
the dataset). 581
(d)We recognize that reproducibility may be tricky in some cases, in which case 582
authors are welcome to describe the particular way they provide for reproducibility. 583
In the case of closed-source models, it may be that access to the model is limited in 584
some way (e.g., to registered users), but it should be possible for other researchers 585
to have some path to reproducing or verifying the results. 586
5.Open access to data and code 587
Question: Does the paper provide open access to the data and code, with sufficient instruc- 588
tions to faithfully reproduce the main experimental results, as described in supplemental 589
material? 590
Answer: [Yes] 591
Justification: [TODO] Our code are available at here and the data is open source dataset. 592
Guidelines: 593
• The answer NA means that paper does not include experiments requiring code. 594
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 595
public/guides/CodeSubmissionPolicy ) for more details. 596
•While we encourage the release of code and data, we understand that this might not be 597
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 598
including code, unless this is central to the contribution (e.g., for a new open-source 599
benchmark). 600
•The instructions should contain the exact command and environment needed to run to 601
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 602
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 603
•The authors should provide instructions on data access and preparation, including how 604
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 605
•The authors should provide scripts to reproduce all experimental results for the new 606
proposed method and baselines. If only a subset of experiments are reproducible, they 607
should state which ones are omitted from the script and why. 608
•At submission time, to preserve anonymity, the authors should release anonymized 609
versions (if applicable). 610
•Providing as much information as possible in supplemental material (appended to the 611
paper) is recommended, but including URLs to data and code is permitted. 612
6.Experimental Setting/Details 613
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 614
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 615
results? 616
Answer: [Yes] 617
Justification: [TODO] Our codes are available here and include all related training and test 618
details. 619
Guidelines: 620
• The answer NA means that the paper does not include experiments. 621
•The experimental setting should be presented in the core of the paper to a level of detail 622
that is necessary to appreciate the results and make sense of them. 623
•The full details can be provided either with the code, in appendix, or as supplemental 624
material. 625
7.Experiment Statistical Significance 626
Question: Does the paper report error bars suitably and correctly defined or other appropriate 627
information about the statistical significance of the experiments? 628
Answer: [Yes] 629
3Justification: [TODO] Please refer to the Section A.6 in the appendix. 630
Guidelines: 631
• The answer NA means that the paper does not include experiments. 632
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 633
dence intervals, or statistical significance tests, at least for the experiments that support 634
the main claims of the paper. 635
•The factors of variability that the error bars are capturing should be clearly stated (for 636
example, train/test split, initialization, random drawing of some parameter, or overall 637
run with given experimental conditions). 638
•The method for calculating the error bars should be explained (closed form formula, 639
call to a library function, bootstrap, etc.) 640
• The assumptions made should be given (e.g., Normally distributed errors). 641
•It should be clear whether the error bar is the standard deviation or the standard error 642
of the mean. 643
•It is OK to report 1-sigma error bars, but one should state it. The authors should 644
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 645
of Normality of errors is not verified. 646
•For asymmetric distributions, the authors should be careful not to show in tables or 647
figures symmetric error bars that would yield results that are out of range (e.g. negative 648
error rates). 649
•If error bars are reported in tables or plots, The authors should explain in the text how 650
they were calculated and reference the corresponding figures or tables in the text. 651
8.Experiments Compute Resources 652
Question: For each experiment, does the paper provide sufficient information on the com- 653
puter resources (type of compute workers, memory, time of execution) needed to reproduce 654
the experiments? 655
Answer: [Yes] 656
Justification: [TODO] Please refer to the Table 5. 657
Guidelines: 658
• The answer NA means that the paper does not include experiments. 659
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 660
or cloud provider, including relevant memory and storage. 661
•The paper should provide the amount of compute required for each of the individual 662
experimental runs as well as estimate the total compute. 663
•The paper should disclose whether the full research project required more compute 664
than the experiments reported in the paper (e.g., preliminary or failed experiments that 665
didn’t make it into the paper). 666
9.Code Of Ethics 667
Question: Does the research conducted in the paper conform, in every respect, with the 668
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 669
Answer: [Yes] 670
Justification: [TODO] We have read the NeurIPS Code of Ethics and conform to it. 671
Guidelines: 672
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 673
•If the authors answer No, they should explain the special circumstances that require a 674
deviation from the Code of Ethics. 675
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 676
eration due to laws or regulations in their jurisdiction). 677
10.Broader Impacts 678
Question: Does the paper discuss both potential positive societal impacts and negative 679
societal impacts of the work performed? 680
4Answer: [NA] 681
Justification: [TODO] Due to space limitations, this social impact aspect is not discussed 682
in the main paper. This paper doesn’t involve negative societal impacts including potential 683
malicious or unintended uses. Our proposed methods aim to achieve an efficient and 684
effective model compression technique to flexible adaptive different storage and computation 685
requirements, which are beneficial to social advancement. 686
Guidelines: 687
• The answer NA means that there is no societal impact of the work performed. 688
•If the authors answer NA or No, they should explain why their work has no societal 689
impact or why the paper does not address societal impact. 690
•Examples of negative societal impacts include potential malicious or unintended uses 691
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 692
(e.g., deployment of technologies that could make decisions that unfairly impact specific 693
groups), privacy considerations, and security considerations. 694
•The conference expects that many papers will be foundational research and not tied 695
to particular applications, let alone deployments. However, if there is a direct path to 696
any negative applications, the authors should point it out. For example, it is legitimate 697
to point out that an improvement in the quality of generative models could be used to 698
generate deepfakes for disinformation. On the other hand, it is not needed to point out 699
that a generic algorithm for optimizing neural networks could enable people to train 700
models that generate Deepfakes faster. 701
•The authors should consider possible harms that could arise when the technology is 702
being used as intended and functioning correctly, harms that could arise when the 703
technology is being used as intended but gives incorrect results, and harms following 704
from (intentional or unintentional) misuse of the technology. 705
•If there are negative societal impacts, the authors could also discuss possible mitigation 706
strategies (e.g., gated release of models, providing defenses in addition to attacks, 707
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 708
feedback over time, improving the efficiency and accessibility of ML). 709
11.Safeguards 710
Question: Does the paper describe safeguards that have been put in place for responsible 711
release of data or models that have a high risk for misuse (e.g., pretrained language models, 712
image generators, or scraped datasets)? 713
Answer: [NA] 714
Justification: [TODO] This paper doesn’t have any high risk for misuse. 715
Guidelines: 716
• The answer NA means that the paper poses no such risks. 717
•Released models that have a high risk for misuse or dual-use should be released with 718
necessary safeguards to allow for controlled use of the model, for example by requiring 719
that users adhere to usage guidelines or restrictions to access the model or implementing 720
safety filters. 721
•Datasets that have been scraped from the Internet could pose safety risks. The authors 722
should describe how they avoided releasing unsafe images. 723
•We recognize that providing effective safeguards is challenging, and many papers do 724
not require this, but we encourage authors to take this into account and make a best 725
faith effort. 726
12.Licenses for existing assets 727
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 728
the paper, properly credited and are the license and terms of use explicitly mentioned and 729
properly respected? 730
Answer: [Yes] 731
Justification: [TODO] We conform to the CC-BY 4.0 license. 732
Guidelines: 733
5• The answer NA means that the paper does not use existing assets. 734
• The authors should cite the original paper that produced the code package or dataset. 735
•The authors should state which version of the asset is used and, if possible, include a 736
URL. 737
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 738
•For scraped data from a particular source (e.g., website), the copyright and terms of 739
service of that source should be provided. 740
•If assets are released, the license, copyright information, and terms of use in the 741
package should be provided. For popular datasets, paperswithcode.com/datasets 742
has curated licenses for some datasets. Their licensing guide can help determine the 743
license of a dataset. 744
•For existing datasets that are re-packaged, both the original license and the license of 745
the derived asset (if it has changed) should be provided. 746
•If this information is not available online, the authors are encouraged to reach out to 747
the asset’s creators. 748
13.New Assets 749
Question: Are new assets introduced in the paper well documented and is the documentation 750
provided alongside the assets? 751
Answer: [NA] 752
Justification: [TODO] This paper does not release new assets 753
Guidelines: 754
• The answer NA means that the paper does not release new assets. 755
•Researchers should communicate the details of the dataset/code/model as part of their 756
submissions via structured templates. This includes details about training, license, 757
limitations, etc. 758
•The paper should discuss whether and how consent was obtained from people whose 759
asset is used. 760
•At submission time, remember to anonymize your assets (if applicable). You can either 761
create an anonymized URL or include an anonymized zip file. 762
14.Crowdsourcing and Research with Human Subjects 763
Question: For crowdsourcing experiments and research with human subjects, does the paper 764
include the full text of instructions given to participants and screenshots, if applicable, as 765
well as details about compensation (if any)? 766
Answer: [NA] 767
Justification: [TODO] This paper does not involve crowdsourcing nor research with human 768
subjects. 769
Guidelines: 770
•The answer NA means that the paper does not involve crowdsourcing nor research with 771
human subjects. 772
•Including this information in the supplemental material is fine, but if the main contribu- 773
tion of the paper involves human subjects, then as much detail as possible should be 774
included in the main paper. 775
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 776
or other labor should be paid at least the minimum wage in the country of the data 777
collector. 778
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 779
Subjects 780
Question: Does the paper describe potential risks incurred by study participants, whether 781
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 782
approvals (or an equivalent approval/review based on the requirements of your country or 783
institution) were obtained? 784
Answer: [NA] 785
6Justification: [TODO] This paper does not involve crowdsourcing nor research with human 786
subjects. 787
Guidelines: 788
•The answer NA means that the paper does not involve crowdsourcing nor research with 789
human subjects. 790
•Depending on the country in which research is conducted, IRB approval (or equivalent) 791
may be required for any human subjects research. If you obtained IRB approval, you 792
should clearly state this in the paper. 793
•We recognize that the procedures for this may vary significantly between institutions 794
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 795
guidelines for their institution. 796
•For initial submissions, do not include any information that would break anonymity (if 797
applicable), such as the institution conducting the review. 798
7