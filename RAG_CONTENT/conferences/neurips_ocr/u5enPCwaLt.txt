Towards Estimating Bounds on the Effect of Policies
under Unobserved Confounding
Alexis Bellot˚Silvia Chiappa
Google DeepMind
London, UK
Abstract
As many practical fields transition to provide personalized decisions, data is in-
creasingly relevant to support the evaluation of candidate plans and policies ( e.g.,
guidelines for the treatment of disease, government directives, etc.). In the machine
learning literature, significant efforts have been put into developing machinery to
predict the effectiveness of policies efficiently. The challenge is that, in practice,
the effectiveness of a candidate policy is not always identifiable, i.e., not uniquely
estimable from the combination of the available data and assumptions about the do-
main at hand ( e.g., encoded in a causal graph). In this paper, we develop graphical
characterizations and estimation tools to bound the effect of policies given a causal
graph and observational data collected in non-identifiable settings. Specifically,
our contributions are two-fold: (1) we derive analytical bounds for general proba-
bilistic and conditional policies that are tighter than existing results, (2) we develop
an estimation framework to estimate bounds from finite samples, applicable in
higher-dimensional spaces and continuously-valued data. We further show that the
resulting estimators have favourable statistical properties such as fast convergence
and robustness to model misspecification.
1 Introduction
Understanding how to act upon the world around us requires humans and artificial systems to
thoughtfully evaluate the effect of different plans, policies, and interventions one might consider.
Data and methods that facilitate this process are increasingly relevant for high-stakes decision-making,
notably in medical care with the rise of precision medicine [ 41], but also in education [ 25], law
enforcement [ 13], public policy [ 44], and economics [ 2]. The interventions that a practitioner
might consider, could consist of complex policies where a variable is set to follow a conditional or
stochastic relationship depending on other variables in the system. For example, policy-makers might
contemplate higher taxes on processed foods and stronger campaigns on its health risks targeted to
overweight individuals. A sensible question in this context could be “what is the effect of a policy
that reduces the consumption of processed foods by 50% in people with a body mass index above
30?”. Contrary to atomic (also called hard) interventions that force a particular value, this policy
suggests a softer intervention.
The identifiability and estimation of policies from data is a widely studied problem in the reinforce-
ment learning [ 37,39] and causal inference [ 2,12,15,31,32,40] literatures. Despite the generality
entailed by many common approaches in these fields, they often rely on an impractical condition: the
assumption that the effectiveness of the policy is uniquely computable from the observed data and
˚Correspondence to Alexis Bellot: abellot@google.com
38th Conference on Neural Information Processing Systems (NeurIPS 2024).assumptions about the data generating process. This can be violated in real-world settings subject to
unobserved confounding, and lead to the non-identifiability of the effectiveness of a given policy. This
holds irrespective of the number of samples collected so that in reality multiple answers for the effect
of a policy may be equally plausible. In their seminal work in the early 1990’s, Manski and Robins
[27,33] showed that, nevertheless, the effect of an atomic intervention may always be bounded
in a non-trivial interval ( i.e., probabilities strictly contained in r0,1s), irrespective of unobserved
confounders or the causal structure underlying the variables involved. Causal effects are therefore in
general said to be partially identifiable ,i.e., one can derive bounds that shrink the range of a priori
possible values for a given effect and potentially serve as a useful support for decision-making.
Starting from this insight, the problem of partial identification has been gaining growing attention
in the literature. To improve upon these bounds, one common restriction on the data generating
mechanism is to assume knowledge of a causal graph describing the phenomenon of interest. Several
results have derived progressively tighter bounds by exploiting the independencies in observational
and interventional distributions implied by the causal graph [ 4,5,14,46,47,48]. For instance, Balke
and Pearl [ 4] (and subsequent refinements, e.g., [14,48]) defined polynomial optimization programs
to derive bounds that are provably optimal. Tighter analytical bounds have also been derived in
selected settings, such as for the "instrumental variable" graph [ 47] and discrete systems with general
graphs or equivalence classes [ 5,46]. These results, however, are almost exclusively concerned with
atomic interventions and small systems of discretely-valued variables. Despite their generality, there
is still a gap towards making the partial-identification and estimation of the effectiveness of policies
practical. In particular, making scalable inferences from finite samples with continuous outcomes
and higher-dimensional covariates is not possible with existing tools.
This paper aims to provide a novel estimation framework to support decision-making in non-
identifiable settings, overcoming some of these challenges. We introduce several new results for
the partial identification and estimation of the effect of stochastic and conditional policies from a
combination of observational data and assumptions on the domain, encoded in a causal graph. Our
contributions may be summarized as follows. We introduce several graphical criteria to derive new
analytical bounds on the effect of policies with continuous outcomes and covariates, that improve
upon the non-parametric bounds of [ 27,33] and [ 47]. Given these analytical bounds, we then con-
struct estimators leveraging the double machine learning [ 10] toolkit for scalable inference, and
demonstrate that estimators exhibit favourable statistical properties such as robustness to noise and
fast convergence. These results are applicable to arbitrary stochastic or conditional policies that an
investigator might design, and high-dimensional covariate spaces. Recent work has made progress is
developing powerful estimators for identifiable causal effects [ 6,8,19,20,36,49]. However, these
estimators are not applicable in non-identifiable settings. To our knowledge, the proposed estimation
machinery is the first result for bounding the effect of policies given a causal diagram.
Preliminaries. We use capital letters to denote variables ( X), small letters for their values ( x),
bold letters for sets of variables ( X) and their values ( x), and use supp to denote their domains of
definition ( xPsuppX). 1xpXqis the indicator function that equals 1if the statement tX“xuis
true, and equal to 0otherwise.
The framework we use to underpin the estimation of the effect of policies rests on structural causal
models (SCMs) [ 31, Def. 7.1.1]. An SCM Mis a tuplexV,U,F, PpUqy, where Vis a set of
endogenous (observed) variables, Uis a set of exogenous latent variables, and F“tfVuVPVis
a set of functions such that fVdetermines values of Vtaking as argument variables PaVĎV
andUVĎU, i.e. VÐfVpPaV,UVq. Values of Uare drawn from an exogenous distribution
Ppuq. We assume the model to be recursive, i.e. that there are no cyclic dependencies among the
variables. Graphically, each SCM Mis associated with a causal diagram GoverV, where VÑW
ifVappears as an argument of fWinM, andVL9999K WifUVXUW‰H ,i.e.VandWshare
an unobserved confounder.We will use graph-theoretic family abbreviations to represent graphical
relations, e.g. an, de, etc. Moreover, for a causal diagram GoverV, theX-lower-manipulation
ofGdeletes all those edges that are out of variables in X, and otherwise keeps Gas it is. The
resulting graph is denoted as GX. TheX-upper-manipulation of Gdeletes all those edges that are
into variables in X, and otherwise keeps Gas it is. The resulting graph is denoted as GX. We use|ùdto denote d-separation in causal diagrams [31, Def. 1.2.3].
2For a sample set D:“ tvpiqui“1,...,n„P, we use EDrfpVqs:“ p1{nqřn
i“1fpvpiqq. We use
}f}P:“a
EPrtfpVqu2s.pf´f“oPprnqdenotes that a function pfis a consistent estimator of f
at a rate rn,pf´f“OPprnqdenotes that it is bounded in probability at rate rn.
2 Partial Identification of the Effect of Policies
A policy πover a subset XĂVis a sequence of decision rules or plans π:“ tπXuXPXfor
determining the assignment of variables X. In its most general form, every πX:suppXˆsuppCXÑ
r0,1sis a probability mapping from domain of CXĂCĂVto the domain of X.
X YC
(a)GX YC
(b)Gπ
Figure 1: Graphs for Example 1.Qualitatively different types of interventions may be modelled
with πX. Specifically, a deterministic intervention setting
X:“gpcXqbased on the values of CXcan be encoded as
πXpx|cXq:“ 1gpcXqpxqwhile a probabilistic intervention
may be written πXpx|cXq:“Pinduced by πXpX“x|cxq.
The intervened model Mπrepresents a different regime in
which the assignment tfXuXPXis replaced with the assign-
ment induced by π. For an outcome YPV, the interventional
distribution PMπpYq, equivalently PπpYq, is defined as the distribution over YinMπ. It will be
useful to adopt the notation ¯π:“ś
XPXπXpx|cXqto denote the product of policy assignments on
individual variables subject to intervention in π.
Example 1 (Illustration of policies) .In the context of a public health program, let Xbe a measure
of an individual’s weekly physical exercise routine, Can individual’s family history of metabolic
diseases, and Yhospital admissions related to heart disease. Consider the causal diagram Fig. 1a;
currently, an individuals exercise voluntarily, which depends on unobserved factors that could be
associated with risk of heart disease. The government is considering an incentive plan aimed at
increasing the frequency of exercise depending individual’s family history of metabolic diseases
(with some probability). The proposed intervention can be encoded as π“tπXuand sets the new
assignment ˜fXsuch that the variable Xfollows the pre-specified distribution. Graphically, this
policy is represented by Fig. 1b (with the new edge corresponding to the implementation of the policy
highlighted in blue) and may be evaluated with the following quantity,
EPπrYs“ÿ
x,cEPπrY|x, csπXpx|cqPπpcq. (1)
The inferential challenge is that this quantity is not uniquely computable from Ppx, y, cqandG,
requiring more complex and nuanced notions of policy evaluation.
Formally, we are interested in the evaluation of the effectiveness of a plan or policy π, acting on a
(potentially multivariate) discrete action Xbased on values of observed covariates CPRd, on an
outcome of interest YPr0,1s2.
Definition 1 (Average treatment effect) .The effectiveness of a policy πonYisEPπrYs.
From the investigator’s perspective, only the causal diagram Gof the environment Mis available.
No assumptions about the form or shape of PpUqandFare made, but for the structural knowledge
encoded in G. In general, there might exist multiple SCMs Mthat induce the same causal diagram
Gand entail PpVqbut result in different values of EPπrYs. The identification of a setof possible
solutions that contain the true value of EPπrYsleads to the notion of partial identification .
Definition 2 (Partial Identification) .The effectiveness of a policy πis said to be partially identifiable
fromGandPpvqif
ψℓpPqďEPMπrYsďψupPq,for any Msuch that GM“G, PMpvq“Ppvq, (2)
wherepψℓ, ψuqare functionals of Pthat are bounded away from 0and1, respectively.
The earliest bound on the effect of policies, the so-called natural bounds , were developed considering
discrete atomic interventions, i.e. of the form πX“ 1xpXq, XPt1, . . . , d Xu.
2An upper bound of 1 is assumed for simplicity; the following results could be generalized to more general
bounded random variables.
3X1 YX2 C
(a)X1 YX2
C
(b)X2 YX1
SZ
(c)A X1 Y X3X2
CW
(d)
Figure 2: Graphs used in Sec. 2 and 3.
Proposition 1 (Natural Bounds) .For any x,EPrY 1xpXqsďEPxrYsďEPrpY´1q 1xpXqs`1.
Remarkably, these inequalities hold irrespective of the causal graph of the system G; a result that
dates back to [ 27,33]. Similarly, we could adapt the underlying proof strategy to derive functionals
ofPthat bound the effect of more general probabilistic or conditional policies π, given as follows.
Proposition 2 (Natural Policy Bounds (NPB)) .For any π,EPrY¯πsďEPπrYsďEPrpY´1q¯πs`1.
These inequalities also hold irrespective of the causal graph of the system. The natural bounds from
[27,33] are a special case of the natural policy bounds (NPBs) by setting π:“tπXuXPXwhere
πX:“ 1xpXq, and therefore ¯π“ 1xpXq. We could show that the NPBs are tight in some cases.
For example, in Example 1, provably, no better bounds for EPπrYsthan those defined by Prop. 2
could be derived (a result given in Prop. 10 in Appendix B). For other systems that involve variables
that are “separated” in G, however, better bounds may be derived by exploiting the implications of the
causal graph and definition of the policy on the induced observational and interventional distributions.
Consider the following example as a first illustration of this idea.
Example 2 (Tighter bounds with causal diagram) .We are interested in evaluating the effect of
a policy π:“ tπX1px1|cq, 1x2pX2qufrom Ppx1, c, x 2, yqandGin Fig. 2a. In particular, this
problem involves a stochastic conditional intervention on X1given Cand a deterministic intervention
onX2; both variables having differing dependencies onto the rest of the system. We could show that,
EPπrYs“EPπX1rYsěEPrY πX1s“ψℓpPq. (3)
The inequality gives an expression for the bound in terms of Pthat exploits the fact that in-
tervening on X2does not influence Y, and is tighter that the natural policy lower bound as
ψℓpPq ěEPrY πX1sPpX2“x2q “EPrY πX1 1x2pX2qsp“ NPBq. For the upper bound we
could similarly establish that,
EPπrYsďEPrpY´1qπX1s`1“ψupPq. (4)
which is smaller than the natural policy bound as ψupPqďEPrpY´1qπX1 1x2pX2qs`1p“NPBq.
This example, although relatively straightforward, serves to illustrate the potential of causal diagrams
(and the constraints they imply on the effect of policies) for defining tighter bounds.
3 Graphical Criteria for Partial Identification
This section aims to consider more general separation statements between variables encoded in a
causal diagram and the decomposition they imply to provide a systematic algorithm to bound the
effectiveness of policies. We start by introducing the notion of partial adjustment sets (Def. 3) that is
applicable with multiple intervention variables.
Definition 3 (Partial adjustment set) .Letπ:“ tπX1, πX2ube a policy on tX1,X2uwith a
conditioning set C. A set WĎVzpX1ŤX2ŤCŤYqis said to be a partial adjustment set for πX2
inGifpY
|ùdX2|W,C,X1qGπX1X2.
Proposition 3. Letπ:“tπX1, πX2ube a policy mapping a set of covariates Cto a set of treatment
variablestX1,X2u. LetWbe a partial adjustment set for πX2inG. Then,
EPrY¯πγsďEPπrYsďEPrpY´1q¯πγs`1, (5)
where γ:“γpX,C,Wq“1{PpX2|C,Wq.
In words, partial adjustment sets are designed to exploit the unconfounded status of some intervention
variables with respect to the outcome, and could be leveraged to derive tighter bounds when available.
4Example 3 (Tighter bounds with partial adjustment sets) .Consider the problem of evaluating
the effect of a policy π:“ tπX1px1|cq, πX2px2|cqufrom Ppx1, c, x 2, w, yqandGin Fig. 2b.
Following Def. 3, we could establish that W“H is a valid partial adjustment set for πX2since we
can verify that pY
|ùdX2|C, X 2qGπX1X2. Prop. 3 then gives us a valid expression for bounding the
effect of the policy,
EPπrYsěEPrY¯π{PpX2|Cqs pě EPrY¯πs,the NPBq. (6)
For the upper bound we similarly find that,
EPπrYsďEPrpY´1q¯π{PpX2|Cqs`1pďEPrpY´1q¯πs`1,the NPBq.(7)
Next, we define a second useful notion, so-called partial conditional instrumental variables sets , that
can be exploited to evaluate bounds in z-specific distributions Pp 1zpZq{PpZqqinstead of in Pas
described in Prop. 4.
Definition 4 (Partial conditional instrumental set) .A setZĎVis said to be a partial instrumental
set conditional on Rfor a policy πinGifpY
|ùdZ|RqGπ.
For illustration, we give a simple criterion below to show how this subgroup structure could be
exploited to derive tighter bounds.
Proposition 4. LetZbe an unconditional partial instrumental set with respect to a policy π, i.e.
pY
|ùdZqGπ. Then,
max
zEPrY¯π 1zpZq{PpZqsďEPπrYsďmin
zEPrpY´1q¯π 1zpZq{PpZqs`1. (8)
Algorithm 1 Bounds for the effect of policies
Input: GraphG, policy π, outcome Y.
Output: Bounds for EPπrYs.
1:LetC“Ť
XPXCX,K“VzpXŤCŤYq.
/* 1. Omit redundant intervention variables. */
2:LetR“XŞAnpYqinGπ.
3:LetπR:“tπXpcXquXPR.
/* 2. Find partial adjustment sets W. */
4:Initialize W“H,S“R.
5:forRPRdo
6: LetS“SzR.
7: LetWR“AnpCŤYqŞKinGS,R.
8: ifpY
|ùdR|C,S,W,WRqinGS,Rthen
9: W“WŤWR.
10: end if
11:end for
/* 3. Find partial instrumental sets Z. */
12:LetT“tR:WRPWu,U“RzT.
13:Initialize Z“H .
14:forZPKdo
15: ifpY
|ùdZ|WzZ,Z,T,CqinGUthen
16: Z“ZŤZ.
17: end if
18:end for
/* 4. Return bounds. */
19:Letγ:“¯πR 1zpZq{PpT,Z|WzZ,Cq.
20:Letψℓ
z:“EPrY γs.
21:Letψu
z:“EPrpY´1qγs`1.
22:Return bounds: pmaxzψℓ
z,minzψu
zq.The following example shows that partial adjust-
ment sets and partial conditional instrumental
sets may be usefully combined to derive tighter
bounds than would be available had each propo-
sition (Props. 3 and 4) been applied in isolation.
Example 4 (Tighter bounds with partial ad-
justment and instrumental sets) .For this ex-
ample, consider the evaluation of an atomic
intervention π:“ t 1x1pX1q, 1x2pX2quin the
causal diagram Ggiven in Fig. 2c. Note that
¯π“ 1xpXq. Following Def. 3, tHu is a valid
partial adjustment set for πX1since we can ver-
ify thatpY
|ùdX2|X1qGπX1X2. Further, we
could verify that tZuis a partial instrumental set
conditional on tX2uwith respect to πX1since
pY
|ùdZ|X2qGπX1. These two separation state-
ments in (manipulated versions of) Gcould be
leveraged to derive a tighter bound than previ-
ously considered:
max
zEPrY¯π 1zpZq{PpX2, ZqsďEPπrYs
ďmin
zEPrpY´1q¯π 1zpZq{PpX2, Zqs`1.
To derive bounds in a more systematic fash-
ion, combining the notions developed so far,
we present Alg. 1 that recursively seeks to find
partial adjustment and partial instrumental sets
in an efficient and automatic manner.
Intuitively, Alg. 1 seeks to recursively simplify
the query. First by omitting the intervened vari-
ables that have no effect on the outcome; second
by finding the set of intervened variables for
which a partial adjustment set could be used to
tighten the bound, and third by finding the set of variables that act as valid partial instrumental sets to
evaluate bounds on the most favorable conditional distributions rather than on the joint distribution.
5Proposition 5. Alg. 1 is sound.
In words, Prop. 5 says that for a given causal diagram, policy π, and joint distribution Ppvq, the
average treatment effect EPπrYsis contained in the bounds produced by Alg. 1.
Proposition 6. Letkbe the number of variables and mbe the number of edges in G. The run time of
Alg. 1 is Opkpk2`mqq.
Example 5 (Steps of Alg. 1) .For this example we consider evaluating a policy π:“ tπX1p¨ |
Cq, πX2p¨|Cq, πX3p¨|Cqufrom observational data Ppx1, c, x 2, x3, a, w, yqcompatible with Gin
Fig. 2d by explicitly following Alg. 1. On line 1, we define C“tCu,K“tA, Wu. We start by
omitting potentially irrelevant intervention variables by evaluating R“tX1, X2uand noticing that
EPπrYs“EPπRrYs. To find partial adjustment sets in line 4, we evaluate the for loop iterating
overR. For R“X1, we find that S“X2,WX1“ tA, Wubut the if condition fails as Yis
notd-separated from X1giventX2, C, A, Wu. We turn onto R“X2, where S“X1,WX2“
tA, Wu. The if condition is triggered as pY
|ùdX2|X1, C, A, WqinGπX1,X2and therefore we
update W“WX2“ tA, Wu(giving a partial adjustment set). We continue with line 12, set
T“tX2u,U“tX1u, and iterate over K“tA, Wuin the for loop. For Z“A, we have that
pY
|ùdA|W, X 2qinGπX1, the if condition is triggered and therefore we update Z“ tAu. For
Z“W, we find thatpY­K KdW|A, X 2qinGπX1and therefore terminate the for loop. Finally
putting together the pieces, we evaluate the bounds to be,
max
zEPrY γsďEPπrYsďmin
zEPrpY´1qγs`1, γ :“¯πR 1apAq{PpX2, A|W, Cq(9)
X1 YX2W1W2
Figure 3: GRemark (Multiple bound expressions). Alg. 1 is designed to make use of
partial adjustment and conditional instrumental variable sets but it will return
a single bounding expression. In general multiple different expressions could
be derived for a given causal diagram and data distribution. For example,
givenGin Fig. 3 and a policy πontX1, X2u,EPr¯πY{PpX2|W1qsand
EPr¯πY{PpX2|W2qsgive valid, but numerically different lower bounds for
EPπrYs. The superiority of one bound over another might depend on the
associations in the underlying distribution Ppvq. In this case, we could construct two different SCMs
compatible with Gthat entail different orders in the numerical values for the two bounds, such that
neither is always optimal. In general, we conjecture that finding an optimal analytical bound (using
the proposed procedure) is undecidable from the graph alone.
The observation in this remark motivates to develop a procedure to automatically enumerate all
partial adjustment sets to facilitate the search for tighter bounds. The following proposition adapts
theListSep algorithm by [43] to enumerate all partial adjustment sets.
Proposition 7 (Enumerating partial adjustment sets) .Letπ:“tπX1, πX2ube a policy ontX1,X2u
with a conditioning set C. All partial adjustment sets may be enumerated in time Opkpk`mqq
where kare the number of variables and mbe the number of edges in G.
Remark (Related methods for policy evaluation). In parallel to the graphical approach for encoding
structural assumptions about the domain at hand, a number of works have adopted sensitivity
assumptions that quantify the degree of unobserved confounding through various data statistics,
such as odds ratios, propensity scores, etc. A rich literature on sensitivity assumptions exists,
including Tan’s sensitivity model [ 38] and Rosenbaum’s sensitivity model [ 34]. Under these models,
[17,21,28,45], among others, present methods that achieve validity and rate properties for the
resulting estimators. These approaches start with estimators that optimize the average or conditional
treatment effect bounds subject to constraints implied by the sensitivity model. Several of these
works leverage Neyman orthogonality techniques to obtain rate guarantees and doubly-robustness
properties. For instance, [ 17,21,28] study the estimation of bounds under Tan’s sensitivity model
which quantifies the degree of unobserved confounding through odds ratios, and propose estimators
with various validity, sharpness, and favourable convergence rate guarantees. In contrast, [ 45]
study the estimation of bounds under Rosenbaum’s sensitivity model also deriving estimators with
sharpness and fast convergence guarantees. Some works under these assumptions have also considered
policy evaluation (as opposed to the evaluation of atomic interventions) under various sensitivity
models in the context of Reinforcement Learning, e.g., [7,22,23]. We interpret this line of work as
complementary (applicable under different assumptions, i.e., sensitivity models rather than causal
diagrams) to the techniques proposed in this paper.
64 Estimation of the Effect of Policies
This section aims to develop an estimation framework for the effect of a policy EPπrYsgiven finite
samples from Pthat partially identifies its value. The key observation of this section is that multiple
characterizations for estimation could be derived for a given bound returned by lines 20-21 in Alg. 1.
In a first instance, parameterized by the probability ratio given in Alg. 1 defined by γ“pγ1, γ2q,
TPW,ℓ:“EPrγ2Ys p“ ψℓ
zq, γ 2:“¯πUγ1, γ 1:“¯πT 1zpZq{PpT,Z|WzZ,Cq.(10)
In a second instance, parameterized by a collection of regression parameters µ“pµ0, µ1,˜µ1, µ2q
where
µ2:“µ2pR,C,W,Zq“EPrY|R,C,W,Zs, (11)
˜µ1:“˜µ1pR,C,W,Zq“¯πUpU|Cqµ2pR,C,W,Zq, (12)
µ1:“µ1pT,C,W,Zq“EPr˜µ1pR,C,W,Zq|T,C,W,Zs, (13)
µ0:“µ0pC,W,Zq“ÿ
tµ1pt,C,W,Zq¯πTpt|Cq. (14)
TREG,ℓ:“EPrµ0pC,W,zqscould be shown to equal ψℓ
z.
Both formulations define equivalent but different estimation targets for the lower bound3and may
be combined leveraging the double machine learning (DML) toolkit for more efficient and robust
inferences [10].
The following procedure is the main contribution of this section. It defines a DML estimator for
bounding the effectiveness EPπrYsof a policy π.
Definition 5 (DML Estimator) .Given πandG, lettR,T,W,C,Z, Yube defined as in Alg. 1.
Consider a finite sample of data D„P, randomly split into Kfolds. The k’th partition of the sample
is denoted DpkqandDp´kq:“DzDpkq. For each k, learn approximate nuisances pˆγk,ˆµkqwith
Dp´kq. Then, define
´
max
zˆTDML,ℓ,min
zˆTDML,u¯
(15)
to be an estimate for the bounds on EPπrYswhere,
ˆTDML,ℓ:“1
KKÿ
k“1EDpkqrˆγ2,ktY´ˆµ2,kus`EDpkqrˆγ1,ktˆ˜µ1,k´ˆµ1,kus`EDpkqrˆµ0,ks
ˆTDML,u:“1`1
KKÿ
k“1EDpkqrˆγ2,ktpY´1q´ˆµ2,kus`EDpkqrˆγ1,ktˆ˜µ1,k´ˆµ1,kus`EDpkqrˆµ0,ks.
To analyse the error of the DML estimator, we consider the case that nuisances can be estimated
consistently. Thus requirement is relatively mild in practice as accurate probability estimation
employing off-the-shelf classification and regression methods is feasible in general. Its error with
respect to the true bounds is given by the following proposition.
Proposition 8 (Error rates) .Suppose the nuisance estimates pˆµ,ˆγqareL2-consistent and bounded.
Then, the error of the DML estimator ˆTDMLPtˆTDML,ℓ,ˆTDML,uuin Def. 5 is given as follows
ˆTDML´TDML“1
KKÿ
k“1Rk`OP´
}ˆγ2,k´γ2}}ˆµ2,k´µ2}¯
`OP´
}ˆγ1,k´γ1}}ˆµ1,k´ˆ˜µ1,k}¯
where Rkis a random variable that converges to zero at a rate OPp1{?nq.
In words, the DML estimator exhibits a robustness property since the error of ˆTDMLis bounded
in probability at n´1{2rate whenever the nuisances converge at a rate n´1{4. Note that the term
3For the upperbound, a similar construction could be derived by defining TPW,u:“EPrγ2pY´1qs`1,
andTREG,u:“EPrµ0pC,W,zqs`1withYreplaced by Y´1in the definition of µ2. Both estimators could
be shown to be unbiased, i.e. equal to ψℓ
udefined in Alg. 1.
7Figure 4: Experimental results on bounding the effectiveness of policies with the proposed estimators.
Different rows highlight evaluations on different data generating mechanisms: the first row tests
estimation with a partial instrumental set, the second row tests estimation with a partial adjustment
set, the third row tests estimation with a high-dimensional partial adjustment set ( WPR100), and
the fourth row tests estimation with a combination of partial adjustment and instrumental sets.
}ˆ˜µ1,k´ˆµ1,k}quantifies the error in approximating the conditional expectation in Eq. (13) for a given
ˆ˜µ1estimated at that stage, and that the estimation error of ˆγ2andˆγ2are equivalent since they are
deterministic transformations of each other. The following proposition is a corolloray that shows that
the DML estimator is unbiased under misspecification.
Proposition 9 (Bias under misspecification) .Suppose either ˆγ1“γ1orˆµ2“µ2and that either
ˆγ1“γ1orˆ˜µ1“ˆµ1. Then, ˆTDMLPtˆTDML,ℓ,ˆTDML,uuis an unbiased estimator of the corresponding
bound defined in Alg. 1.
5 Experiments
This section evaluates the quality of policy effect estimation from finite samples. Our goal is to
illustrate the computation of bounds and provide empirical evidence of the fast convergence and
robustness to misspecification of estimators.
Finite sample bounds are estimated with gradient boosting classification and regression models
(for conditional expectations) or by taking sample averages (for unconditional expectations). We
truncated estimates of probability mass functions in the interval r0.01,0.99sto ensure positivity.
We assess the quality of an estimator ˆTby computing the absolute average error (AAE) with
respect to (a proxy for) the true bounds (estimated in practice with larger sample sizes), i.e., AAE
=|ˆTu´minzψu
z|`|ˆTℓ´maxzψℓ
z|. Throughout, we report various statistics: 25th, 50th, 75th
percentile, etc., across evaluation runs with ten different random seeds. Further details of the
simulations are provided in Appendix C.
5.1 Synthetic Simulations
The synthetic simulations consider 4 data generating mechanisms constructed according to the graphs
in Fig. 4. They highlight the use of partial instrumental sets, partial adjustment sets, high-dimensional
partial adjustment sets, and combinations of partial instrumental and adjustment sets of varying
dimensionality. The task is to estimate bounds on the effectiveness of a policy π:“tπX1, πX2u
defined as follows.
πX1:“πX1pX1“1q“0.5, π X2:“πX2pX2“1|cq“1{p1´expt´cuq. (16)
8That is the policy assigns X1Pt0,1urandomly with probability 0.5, and assigns X2Pt0,1uas a
function of Cwith the probability of X2“1increasing with the value of C.
To estimate bounds on EPπrYs, we consider the proposed estimators, labelled: TPW, estimated with
the nuisances in Eq. (10), TREG, estimated with the nuisances in Eq. (11), and TDMLestimated with
the procedure in Def. 5. Our evaluations test performance across 4 different settings designed to
highlight various properties.
•Setting 1 : All nuisances estimated correctly. This setting aims to show that all estimators converge
to the bound of interest.
•Setting 2 : Nuisances ˆγare sampled from a uniform distribution to induce misspecification in the
estimation of γ.
•Setting 3 : Nuisances ˆµare sampled from a uniform distribution to induce misspecification in the
estimation of µ. Settings 2 and 3 aim to demonstrate the doubly-robustness property of the DML
estimator.
•Setting 4 : Noise ϵis introduced in the estimation of all nuisances pγ,µqto emphasize error due to
finite sample variation. Specifically, noise ϵ„Normalpn´α, n´αq, α“1{4, that induces a slower
rate of convergence as a function of sample size, inspired by [ 24,20]. This setting aims to show
that the fast convergence behavior of the DML estimator compared to competing estimators.
The results are given in Fig. 44. We observe that across all data generating mechanisms, estimators
improve with the size of the dataset and converge under no misspecification (Setting 1) to the
underlying bounds. It is interesting to note also the differing accuracy of estimators in the small
sample regime. TPW, based the estimation of a ratio of probabilities, can be unstable with low sample
sizes if the ratio denominator is estimated to be close to zero while TREG, based on a sequence
of regression tasks, tends to be better behaved. TDMLin contrast is constructed as a combination
of elements of TPWandTREG. In particular, note in Def. 5 the use of nuisances µandγ. As a
result, TDMLhas quite a different performance profile. Settings 2 and 3 in Fig. 4 show that the
DML estimator TDMLis robust to misspecification in either the nuisances µorγthat highlights
the robustness property. Further, when decaying noise is introduced in the estimation of nuisances
(Setting 4), the DML estimator outperforms in general with a faster convergence rate. We also observe
that performance remains close to optimal with high-dimensional variables W, demonstrating that
the DML estimator provides a practical toolkit for bounding in practice.
5.1.1 Width of Bounds According to Different Graphical Criteria
Figure 5: Width of bounds.This section evaluates the width of the bounds returned by
exploiting the different graphical criteria provided in Sec. 3.
The simulations are based on the data generating mechanism
described by causal diagram illustrated in the fourth row of
Fig. 4. We consider evaluating the policy in Eq. (16) and
compute the bounds obtained by applying Prop. 2 (most con-
servative), Prop. 3 (using the partial adjustment set Wonly),
Prop. 4 (using the partial instrumental set Zonly), and finally
Alg. 1 (that combines all propositions and is the proposed ap-
proach). Fig. 5 gives the results over 10 seeds of the data and
across multiple data sizes, highlighting the gain achieved by
exploiting the causal structure using the proposed approaches.
Remark (Actual width of bounds in practice). The majority of our empirical evaluations are spent on
evaluating the accuracy of different methods at estimating bounds, without addressing whether the
returned bounds are actually informative. In practice, the graph structure can play an important role
in tightening bounds but the actual width of the bounds in a particular problem are primarily driven
by the distribution of data. Here is an example to make this more concrete.
For a given policy π, Prop. 2 defines tight bounds (under some circumstances) on EPπrYs. The
width of this bound is given by 1´EPr¯πsthat is ultimately driven by Ppxq. This term may therefore
evaluate to anything between 0 and 1 depending on Ppxqandπ.
4For the first row, the policy evaluated is: πX:“πXpX“1|cq“1{p1´expt´cuq.
9TS
F
H YE
MA
(a)GObesity
 (b)
Figure 6: Health campaign evaluations.
For more complex causal structures, the bounds proposed in Props. 3 and 4 (and Alg. 1) reduce the
width of the interval above. Loosely written, from 1´EPr¯πsto1´EPr¯πsˆαfor some αthat is a
function of the joint distribution Pand the structure of the graph. But again, the actual width of the
interval is ultimately determined by the values of Pandπthat may be large or small depending on
the value probabilities involved.
5.2 Evaluating Health Campaigns
This section illustrates the evaluation of lifestyle recommendations for the mitigation of obesity in
individuals from Colombia, Peru and Mexico [ 30]. The data was collected from anonymous users
using a web platform, and includes reported obesity levels measured according to BMI pYq, age
pAq, smoking status pSq, frequency of consumption of high caloric food pHq, whether individuals
monitored their calorie intake pMq, family history being overweight pFq, exercise frequency pEq,
and time using technology devices pTq. Obesity is a multi-factored medical condition for which
several causes have been acknowledged in the literature; causal diagrams relating the variables above
have been curated in several related studies [ 1,9]. We considered these findings to construct the
causal diagram in Fig. 6a that we assumed for this example.
We aim to study the effect of a health campaign designed to lower the intake of high caloric food pHq
and increase the frequency of exercise pEqon obesity levels pYq. For instance, we could hypothesize
that the campaign leads to an increase in the observed proportion of individuals rarely consuming of
high caloric food ( H) from 0.12to0.5and that of individuals doing exercise ( E) regularly from 0.05
to0.5. These statements could be formulated as a stochastic policy πα:“tπα
H, πα
Euacting on H
andE, with new assignments,
πα
H:“πα
HpH“rarelyq“α, πα
E:“πα
EpE“regularlyq“α. (17)
We consider the evaluation of expected BMI levels EPπαrYsthat range from 12to50in the population,
with a mean of 29.3. First note that this or other policies acting on pH, Eqare not identifiable due to
the bi-directed edge tHL9999K Yu, but may nevertheless be bounded using Alg. 1. We find that
max
tEPrY γsďEPπαrYsďmin
tEPrpY´1qγs`1, γ :“¯ππα 1tpTq{PpE, T|A, S, Fq.
(18)
To illustrate the inference of. bounds with the DML estimator, we consider evaluating policies with
α“0.2,0.4,0.6,0.8. Fig. 6b gives the results. The end-points of the intervals denote estimated
lower and upper bounds. We see that policies that promote a healthier lifestyle (larger values of α)
are expected to reduce obesity levels on average but substantial uncertainty is still expected.
6 Conclusions
The evaluation of policies is arguably one of the critical ingredients enabling more personalized
decision-making systems. When the effect of policies is not identifiable, bounds can provide an
effective support for making informed decisions. In this paper we developed partial identification
and estimation tools for bounding the effect of a (stochastic or conditional) policy given data and
assumptions encoded in a causal graph. We introduced several graphical characterizations that induce
tighter bounds, and developed an estimation framework that exhibit robustness to noise and fast
convergence. The results of this paper were illustrated through synthetic simulations and a real-world
health campaign example for the reduction of obesity levels.
10Acknowledgements
We thank the anonymous reviewers for helpful comments.
References
[1]Steven Allender, Brynle Owen, Jill Kuhlberg, Janette Lowe, Phoebe Nagorcka-Smith, Jill
Whelan, and Colin Bell. A community based systems diagram of obesity causes. PloS one ,
10(7):e0129683, 2015.
[2]Susan Athey and Guido W Imbens. The state of applied econometrics: Causality and policy
evaluation. Journal of Economic perspectives , 31(2):3–32, 2017.
[3]Vahid Balazadeh Meresht, Vasilis Syrgkanis, and Rahul G Krishnan. Partial identification of
treatment effects with implicit generative models. Advances in Neural Information Processing
Systems , 35:22816–22829, 2022.
[4]Alexander Balke and Judea Pearl. Bounds on treatment effects from studies with imperfect
compliance. Journal of the American Statistical Association , 92(439):1171–1176, 1997.
[5]Alexis Bellot. Towards bounding causal effects under Markov equivalence. In The 40th
Conference on Uncertainty in Artificial Intelligence . PMLR, 2024.
[6]Alexis Bellot, Anish Dhir, and Giulia Prando. Generalization bounds and algorithms for
estimating conditional average treatment effect of dosage. arXiv preprint arXiv:2205.14692 ,
2022.
[7]Andrew Bennett and Nathan Kallus. Policy evaluation with latent confounders via optimal
balance. Advances in neural information processing systems , 32, 2019.
[8]Rohit Bhattacharya, Razieh Nabi, and Ilya Shpitser. Semiparametric inference for causal
effects in graphical models with hidden variables. The Journal of Machine Learning Research ,
23(1):13325–13400, 2022.
[9]Bryony Butland, Susan Jebb, Peter Kopelman, Klim McPherson, Sandy Thomas, Jane Mardell,
Vivienne Parry, et al. Tackling obesities: future choices-project report , volume 10. Citeseer,
2007.
[10] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,
Whitney Newey, and James Robins. Double/debiased machine learning for treatment and
structural parameters, 2018.
[11] David Maxwell Chickering and Judea Pearl. A clinician’s tool for analyzing non-compliance.
InProceedings of the National Conference on Artificial Intelligence , pages 1269–1276, 1996.
[12] Juan Correa and Elias Bareinboim. A calculus for stochastic interventions: Causal effect
identification and surrogate experiments. In Proceedings of the AAAI conference on artificial
intelligence , volume 34, pages 10093–10100, 2020.
[13] Elizabeth K Drake, Steve Aos, and Marna G Miller. Evidence-based public policy options to
reduce crime and criminal justice costs: Implications in washington state. Victims and offenders ,
4(2):170–196, 2009.
[14] Noam Finkelstein and Ilya Shpitser. Deriving bounds and inequality constraints using logical
relations among counterfactuals. In Conference on Uncertainty in Artificial Intelligence , pages
1348–1357. PMLR, 2020.
[15] Limor Gultchin, Virginia Aglietti, Alexis Bellot, and Silvia Chiappa. Functional causal Bayesian
optimization. In Uncertainty in Artificial Intelligence , pages 756–765. PMLR, 2023.
[16] Yaowei Hu, Yongkai Wu, Lu Zhang, and Xintao Wu. A generative adversarial framework
for bounding confounded causal effects. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 35, pages 12104–12112, 2021.
11[17] Andrew Jesson, Sören Mindermann, Yarin Gal, and Uri Shalit. Quantifying ignorance in
individual-level causal-effect estimates under hidden confounding. In International Conference
on Machine Learning , pages 4829–4838. PMLR, 2021.
[18] Shalmali Joshi, Junzhe Zhang, and Elias Bareinboim. Towards safe policy learning under
partial identifiability: A causal approach. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 38, pages 13004–13012, 2024.
[19] Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating identifiable causal effects on Markov
equivalence class through double machine learning. In International Conference on Machine
Learning , pages 5168–5179. PMLR, 2021.
[20] Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating joint treatment effects by combining
multiple experiments. In International Conference on Machine Learning , pages 15451–15527.
PMLR, 2023.
[21] Nathan Kallus, Xiaojie Mao, and Angela Zhou. Interval estimation of individual-level causal
effects under unobserved confounding. In The 22nd international conference on artificial
intelligence and statistics , pages 2281–2290. PMLR, 2019.
[22] Nathan Kallus and Angela Zhou. Confounding-robust policy improvement. Advances in neural
information processing systems , 31, 2018.
[23] Nathan Kallus and Angela Zhou. Confounding-robust policy evaluation in infinite-horizon
reinforcement learning. Advances in neural information processing systems , 33:22293–22304,
2020.
[24] Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal effects.
arXiv preprint arXiv:2004.14497 , 2020.
[25] Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offline policy
evaluation across representations with applications to educational games. In AAMAS , volume
1077, 2014.
[26] Henry B Mann and Abraham Wald. On stochastic limit and order relationships. The Annals of
Mathematical Statistics , 14(3):217–226, 1943.
[27] Charles F Manski. Nonparametric bounds on treatment effects. The American Economic Review ,
80(2):319–323, 1990.
[28] Miruna Oprescu, Jacob Dorn, Marah Ghoummaid, Andrew Jesson, Nathan Kallus, and Uri Shalit.
B-learner: Quasi-oracle bounds on heterogeneous causal effects under hidden confounding.
arXiv preprint arXiv:2304.10577 , 2023.
[29] Kirtan Padh, Jakob Zeitler, David Watson, Matt Kusner, Ricardo Silva, and Niki Kilbertus.
Stochastic causal programming for bounding treatment effects. arXiv preprint arXiv:2202.10806 ,
2022.
[30] Fabio Mendoza Palechor and Alexis de la Hoz Manotas. Dataset for estimation of obesity levels
based on eating habits and physical condition in individuals from colombia, peru and mexico.
Data in brief , 25:104344, 2019.
[31] Judea Pearl. Causality . Cambridge university press, 2009.
[32] Judea Pearl and James M Robins. Probabilistic evaluation of sequential plans from causal
models with hidden variables. In UAI, volume 95, pages 444–453. Citeseer, 1995.
[33] James M Robins. The analysis of randomized and non-randomized aids treatment trials using a
new approach to causal inference in longitudinal studies. Health service research methodology:
a focus on AIDS , pages 113–159, 1989.
[34] Paul R Rosenbaum, P Briskman Rosenbaum, and Briskman. Design of observational studies ,
volume 10. Springer, 2010.
12[35] Andrea Rotnitzky, James Robins, and Lucia Babino. On the multiply robust estimation of the
mean of the g-functional. arXiv preprint arXiv:1705.08582 , 2017.
[36] Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect:
generalization bounds and algorithms. In International conference on machine learning , pages
3076–3085. PMLR, 2017.
[37] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press,
2018.
[38] Zhiqiang Tan. A distributional approach for causal inference using propensity scores. Journal
of the American Statistical Association , 101(476):1619–1637, 2006.
[39] Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforce-
ment learning. In International Conference on Machine Learning , pages 2139–2148. PMLR,
2016.
[40] Jin Tian. Identifying dynamic sequential plans. arXiv preprint arXiv:1206.3292 , 2012.
[41] Anastasios A Tsiatis, Marie Davidian, Shannon T Holloway, and Eric B Laber. Dynamic
treatment regimes: Statistical methods for precision medicine . CRC press, 2019.
[42] Mark J van der Laan and Susan Gruber. Targeted minimum loss based estimation of causal
effects of multiple time point interventions. The international journal of biostatistics , 8(1),
2012.
[43] Benito van der Zander. Algorithmics of Identifying Causal Effects in Graphical Models . PhD
thesis, Universität zu Lübeck, 2020.
[44] Evert Vedung. Public policy and program evaluation . Routledge, 2017.
[45] Steve Yadlowsky, Hongseok Namkoong, Sanjay Basu, John Duchi, and Lu Tian. Bounds on the
conditional and average treatment effect with unobserved confounding factors. arXiv preprint
arXiv:1808.09521 , 2018.
[46] Junzhe Zhang. Designing optimal dynamic treatment regimes: A causal reinforcement learning
approach. In International Conference on Machine Learning , pages 11012–11022. PMLR,
2020.
[47] Junzhe Zhang and Elias Bareinboim. Bounding causal effects on continuous outcome. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pages 12207–12215,
2021.
[48] Junzhe Zhang, Jin Tian, and Elias Bareinboim. Partial counterfactual identification from
observational and experimental data. arXiv preprint arXiv:2110.05690 , 2021.
[49] Yao Zhang, Alexis Bellot, and Mihaela Schaar. Learning overlapping representations for
the estimation of individualized treatment effects. In International Conference on Artificial
Intelligence and Statistics , pages 1005–1014. PMLR, 2020.
13Appendix
Table of Contents
A Preliminaries, Related Work, and Impact Statement 15
A.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.3 Broader Impact Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B Proofs 18
B.1 Proofs of statements in Sec. 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 Proofs of statements in Sec. 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
C Details on experiments 30
C.1 Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C.2 Health Campaign Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D NeurIPS Paper Checklist 33
14A Preliminaries, Related Work, and Impact Statement
A.1 Preliminaries
For the derivation of results, we use the notion of counterfactuals defined as follows.
For an SCM M, arbitrary subsets of endogenous variables X,Y, the potential outcome of Yto
intervention dopxq, denoted by Yxpuq, is the solution for YwithU“uin the sub-model Mx. It
can be read as the counterfactual sentence “the value that Ywould have obtained in situation U“u,
hadXbeenx.”. Statistically, averaging uover the distribution Ppuqleads to the counterfactual
variables Yx. The distribution of the variable Yxis denoted PpYxq. With this formalism, we can
express all the quantities in the main body of this paper in the language of counterfactuals. For
instance, the distribution of Yin sub-model Mxcan alternatively be written PpYxq ”PxpYq.
Moreover, EPrfpYxq|zs, also written EPp¨|zqrfpYxqs, denotes the conditional expectation of fpYq
overPpyx|zq. Similarly, the distribution over Yin the sub-model Mπcan be written PMpYπq, or
PpYπq”PπpYqfor short. See [31, Chapter 7] for further context on counterfactuals.
Definition 6 (The Axioms of Counterfactuals, Chapter 7.3.1 [ 31]).For any three sets of endogenous
variables X,Y,Win a causal model and x,win the domains of XandW, the following holds:
•Composition: Wx“wimplies that Yx,w“Yx.
•Effectiveness: Xw,x“x.
•Reversibility: Yx,w“yandWx,y“wimply that Yx“y.
Theorem 1 (Soundness and Completeness of the Axioms Theorems 7.3.3, 7.3.6 [ 31]).The Axioms
of counterfactuals are sound and complete for all causal models.
The following rules to manipulate experimental distributions produced by policies extend the do-
calculus and will be used for the proof of several theoretical statements [12].
Theorem 2 (Inference Rules σ-calculus [ 12]).LetGbe a causal diagram compatible with an SCM
M, with endogenous variables V. For any disjoint subsets X,Y,ZĎV, two disjoint subsets
T,WĎVzpZŤYq(i.e., possibly including X), the following rules are valid for any intervention
strategies πX,πZ, and π1
Zsuch that GπXπZ,GπXπ1
Zhave no cycles:
•Rule 1 (Insertion/Deletion of observations):
PπXpy|w,tq“PπXpy|wqifpT
|ùdY|WqinGπX.
•Rule 2 (Change of regimes under observation):
PπX,πZpy|z,wq“PπX,π1
Zpy|z,wqifpY
|ùdZ|WqinGπX,πZ,ZandGπX,π1
Z,Z
•Rule 3 (Change of regimes without observation):
PπX,πZpy|wq“PπX,π1
Zpy|wqifpY
|ùdZ|WqinGπX,πZ,ZpWqandGπX,π1
Z,ZpWq
where ZpWqis the set of elements in Zthat are not ancestors of WinGπX.
A.2 Related Work
Active experimentation by physically manipulating reality is generally not feasible for many conse-
quential applications. This motivates the study of causal effect identification and estimation. The
former studies the problem of inferring whether a unique expression or bound of the desired causal
effect could be derived from the available data and assumptions. The latter studies the problem of
providing efficient estimators from finite samples to compute causal effects or bounds on causal
effects in practice. In the following, we review these two lines of work to better contextualize our
contribution.
Partial identification . The natural bounds were derived to demonstrate that useful inference about
causal effects could be drawn without making identifying assumptions beyond the observed data
[27]. Relatedly, an analysis of bounds of causal effects was also provided for studies with imperfect
15compliance under a set of instrumental variable assumptions that are not sufficient however to identify
the causal effect of interest [ 33]. These bounds may be written in closed form and have recently been
extended as a more general strategy for bounding causal effects given assumptions encoded in causal
diagrams in discrete systems [ 5,46]. Recent work has also considered bounds in closed form with
access to both observational and interventional distributions [ 18]. These techniques were developed
alongside a second line of research that employs a polynomial optimization program to compute
causal bounds given a causal diagram [ 4]. They proposed a family of canonical models parameterized
according to the causal diagram, reducing the bounding problem to a series of equivalent linear
programs. [ 11] further used Bayesian techniques to investigate the sharpness of these bounds with
regard to the observational sample size. Recently, [ 14,48] describe a polynomial programming
approach to solve the partial identification for general causal graphs. More recent proposals consider
parameterizations in the space of linear combinations of a set of fixed basis functions [ 29] and neural
networks [3, 16].
In parallel, a number of works have adopted sensitivity assumptions (as an alternative or in combi-
nation with a causal diagram) that quantify the degree of unobserved confounding through various
data statistics, such as odds ratios, propensity scores, etc. A rich literature on sensitivity assumptions
exists, including Tan’s sensitivity model [ 38] and Rosenbaum’s sensitivity model [ 34]. Under these
models, [ 17,21,28,45], among others, present methods that achieve validity and rate properties
for the resulting estimators. These approaches start with estimators that optimize the average or
conditional treatment effect bounds subject to constraints implied by the sensitivity model. Several
of these works leverage Neyman orthogonality techniques to obtain rate guarantees and doubly-
robustness properties. For instance, [ 17,21,28] study the estimation of bounds under Tan’s sensitivity
model which quantifies the degree of unobserved confounding through odds ratios, and propose
estimators with various validity, sharpness, and favourable convergence rate guarantees. In contrast,
[45] study the estimation of bounds under Rosenbaum’s sensitivity model also deriving estimators
with sharpness and fast convergence guarantees.
While some works have considered policy evaluation under various sensitivity models in the context
of Reinforcement Learning, e.g., [ 7,22,23], most works target estimation of bounds on the effect of
atomic interventions. We interpret this line of work as complementary (applicable under different
assumptions, i.e., sensitivity models rather than causal diagrams) to the techniques proposed in this
paper.
Causal effect estimation. With a causal diagram as input, causal effect estimation has traditionally
focused on a subset of identifiable scenarios, relying on assumptions such as backdoor criterion or
the availability of adjustment sets. Identification expressions in those cases are given by (sequential)
covariate adjustments that have lead to statistically appealing estimators from observational data
[32]. Notable examples are doubly robust estimators [ 10,35,42]. Recently these techniques have
been extended to settings identifiable from multiple experimental distributions [ 8,19,20]. To our
knowledge, no estimation framework specific to the estimation of analytical expressions derived for
bounds on the effect of policies or atomic interventions has been developed.
A.3 Broader Impact Statement
Our work investigates the conditions under which policies may be bounded from observational data.
Issues of policy evaluation are central to fields of application involved in decision-making as well
as AI and ML. We believe that a tool to bound the effect of stochastic and conditional policies in
systems with unobserved confounding is an important addition to the scientific toolbox. Reasoning
instead without acknowledging for the potential of unobserved confounding may lead researchers
to operate on a more heuristical basis. For instance, the hypothetical effect of public policies might
be misrepresented if informed by the evaluation of an idealized atomic intervention that is difficult
to implement in practice. And, once implemented, might have unintended consequences. With this
background, we believe that research on the partial identification and estimation of policies based
on stochastic or conditional interventions can help scientists and individuals make more informed
decisions.
In this work, we start from the assumption that a causal diagram that is consistent with the underlying
data generating system of interest is available. In general, this requires domain knowledge. While
some of the bounds provided do not require full knowledge of the causal graph, whenever a d-
separation is assumed its truth value should be justified by prior knowledge or experiment. It is
16important also to make the distinction between the task of partial identification, that is inferring an
expression for bounds on causal effects, and that of estimation, that is providing efficient estimators
from finite samples to compute bounds in practice. This set of results concerns both of these
tasks. The first objective of our procedure is to provide an expression for lower and upper bounds,
irrespective of the accuracy with which one is able to approximate PpVqfrom finite samples. The
second objective is introduce efficient estimators for bounds on the effect of policies using finite
samples from PpVq. In higher-dimensional systems, the computational complexity of estimating
the conditional distributions that define lower and upper bounds on causal effects is a substantial
challenge. Consequently, practitioners must exercise caution when deploying the proposed method
in small sample scenarios where estimators may be inaccurate. Moreover, we have stated our
convergence guarantees in the infinite sample limit, without quantifying the finite-sample estimation
uncertainty. Finite-sample properties could be explored similarly to [ 24] given a particular choice
of function class to extend our results with high-probability bounds. Finally, we emphasize that
simulations on real and synthetic data are provided for illustration purposes only. These results do
not recommend or advocate for the implementation of a particular policy, and should be considered
in practice in combination with other aspects of the decision-making process.
17B Proofs
This section presents proofs for theoretical statements in the main body of this paper.
B.1 Proofs of statements in Sec. 2
Prop. 1 restated. For any x,EPrY 1xpXqsďEPxrYsďEPrpY´1q 1xpXqs`1.
Proof. Consider the derivation of the lower bound.
EPrYxs“ÿ
x1EPrYx|x1sPpx1q
ěEPrYx|xsPpxq
p1q“EPrY|xsPpxq
“ÿ
yyPpy|xqPpxq
“ÿ
y,xy 1xpxqPpy,xq
“EPrY 1xpXqs
(1) follows by the composition axiom of counterfactuals.
Consider the derivation of the upper bound. It holds that,
EPrYxs“ÿ
x1EPrYx|x1sPpx1q
“EPrYx|xsPpxq`ÿ
x1‰xEPrYx|x1sPpx1q
p1q
ďEPrY|xsPpxq`ÿ
x1‰xPpx1q
“EPrY|xsPpxq`1´Ppxq
“EPrY 1xpXqs`1´EPr 1xpXqs
“EPrpY´1q 1xpXqs`1.
(1) follows by the boundedness of Y, here assumed bounded by 1 for simplifying the derivation.
Proposition 10. The natural bounds are tight in general.
Proof. We show this statement by providing a pair of SCMs that agree on the input observational
distribution but evaluate to the lower and upperbounds, respectively, specified by the natural bounds.
LetMpGqdenote the space of SCMs that induce the causal diagram Gin Fig. 1a. We introduce a pair
of SCMs compatible with the causal diagram Gthat evaluate to lower and upper bounds respectively.
LetXbe binary, YPr0,1s, and CPR, and consider M1,M2PMpGqdefined as follows,
M1:“$
’’&
’’%x:“fXpu1q
c:“fCpu2q
y:“"fYpx, c, u 1, u2qifx“fXpu1q,
0 otherwise .
and,
M2:“$
’’&
’’%x:“fXpu1q
c:“fCpu2q
y:“"fYpx, c, u 1, u2qifx“fXpu1q,
1 otherwise .
18For a PM1puq “ PM2puq, both SCMs agree on observational distributions PM1px, y, cq “
PM2px, y, cq. However the following derivations show that the interventional expectation EPM1rY|
dopx“1qsdiffers across models: for M1equal to the analytical lower bound, and for M2equal to
the analytical upper bound demonstrating that (in this case) the bound is tight. In particular,
EPM1rY|dopx“1qs
“EPM1rY|x“1, u:x“fXpuqsPpu:x“fXpuqq
`EPM1rY|x“1, u:x‰fXpuqsPpu:x‰fXpuqq
“EPM1rY|x“1sPpx“1q
“EPrY|x“1sPpx“1q
“EPrY 11pXqs
EPM2rY|dopx“1qs
“EPM2rY|x“1, u:x“fXpuqsPpu:x“fXpuqq
`EPM2rY|x“1, u:x‰fXpuqsPpu:x‰fXpuqq
“EPM2rY|x“1sPpx“1q`EPM2rY|x“1, u:x“fXpuqsPpu:x“fXpuqq
“EPM2rY|x“1sPpx“1q`1´Ppx“1q
“EPrpY´1q 11pXqs`1.
Prop. 2 restated .For any π,EPrY¯πsďEPπrYsďEPrpY´1q¯πs`1.
Proof. Consider the derivation of the lower bound. Let πdenote a policy from an arbitrary set of
covariates CtoX. By marginalizing over X,Cwe find that
EPπrYs“EPrYπs“ż
EPrYπ|x,csź
XPXπXpx|cXqPpcqdcdx.
CXdenotes the subset of Cthat is used to inform the intervention on X. Moreover,
EPrYπ|x,csp1q“EPrYx|cs
“ÿ
x1EPrYx|x1,csPpx1|cq
ěEPrYx|x,csPpx|cq
p2q“EPrY|x,csPpx|cq.
(1) holds by Rule 2 in Thm. 2 by swapping the policy πby the do intervention dopX“xq: given
that the policy acts on Xtaking as inputs Cit is always true that Y
|ùdX|CinGπXandGXX
in any graph G. The following equality follows by marginalizing over the domain of X, which is
assumed discrete. (2) follows by the composition axiom of counterfactuals. By combining these two
expressions we get
EPrYπsěżź
XPXπpx|cXqPpcqEPrY|x,csPpx|cqdcdx
“ż
¯πyPpy|x,cqPpx|cqPpcqdydcdx
“EPrY¯πs.
19Consider the derivation of the upper bound. It holds that,
EPrYπ|x,cs“EPrYx|cs
“ÿ
x1EPrYx|x1,csPpx1|cq
“EPrYx|x,csPpx|cq`ÿ
x1‰xEPrYx|x1,csPpx1|cq
p1q
ďEPrY|x,csPpx|cq`ÿ
x1‰xPpx1|cq
“EPrY|x,csPpx|cq`1´Ppx|cq.
(1) follows by the boundedness of Y, here assumed bounded by 1 for simplifying the derivation. As
above by combining this inequality with the decomposition of the policy effect we get
EPπrYsďżź
XPXπpx|cXqPpcqtEPrY|x,csPpx|cq`1´Ppx|cqudcdx
“EPrY¯πs`1´EPr¯πs
“EPrpY´1q¯πs`1.
Prop. 3 restated .Letπ:“ tπX1, πX2ube a policy mapping a set of covariates Cto a set of
treatment variables tX1,X2u. LetWbe a partial adjustment set for πX2inG. Then,
EPrY¯πγsďEPπrYsďEPrpY´1q¯πγs`1,
where γ:“γpX,C,Wq“1{PpX2|C,Wq.
Proof. Consider the derivation of the lower bound. By marginalizing over X,Cwe find that
EPπrYs“EPrYπs“ż
EPrYπ|x,csź
XPXπXpx|cXqPpcqdcdx.
CXdenotes the subset of Cthat is used to inform the intervention on X. Denote ¯π:“ś
XPXπXpx|
cXq. LetX1,X2be a partition of Xsuch that WĎVzpXŤCŤYqis partial adjustment set for
πX2. Then,
EPrYπ|x,cs“EPrYπX1,πX2|x,cs
“ż
EPrYx1,x2|c,wsPpw|cqdw
p1q“ż
EPrYx1|x2,w,csPpw|cqdw
“żÿ
x1
1␣
EPrYx1|x1
1,x2,w,csPpx1
1|x2,w,cq(
Ppw|cqdw
ěż
EPrYx1|x1,x2,w,csPpx1|x2,w,cqPpw|cqdw
“ż
EPrY|x1,x2,w,csPpx1|x2,w,cqPpw|cqdw.
(1) follows by the definition of X2as a partial adjustment set: it holds that pY
|ùdX2|W,C,X1q
inGX1X2which induces the equality EPrYx1,x2|c,ws“EPrYx1|x2,w,cs. The last equality fol-
lows by the composition axiom of counterfactuals. Combining this expression with the decomposition
of the policy effect we get,
EPrYπsěżź
XPXπpx|cXqPpcqEPrY|x1,x2,w,csPpx1|x2,w,cqPpw|cqdwdcdx
“ż
¯πyPpy|x1,x2,w,cqPpx1|x2,w,cqPpw|cqPpcqdydcdx
“EPrY¯π{PpX2|W,Cqs.
20For the upper bound, consider the following derivation,
EPrYπ|x,cs
“ż
EPrYx1|x2,w,csPpw|cqdw
“żÿ
x1
1tEPrYx1|x1
1,x2,w,csPpx1
1|x2,w,cquPpw|cqdw
“ż!
EPrYx1|x1,x2,w,csPpx1|x2,w,cqPpw|cq
`ÿ
x1
1‰x1EPrYx1|x1
1,x2,w,csPpx1
1|x2,w,cqPpw|cq)
dw
ďż!
EPrY|x1,x2,w,csPpx1|x2,w,cqPpw|cq`ÿ
x1
1‰x1Ppx1
1|x2,w,cqPpw|cq)
dw
“ż!
EPrY|x1,x2,w,csPpx1|x2,w,cqPpw|cq`p1´Ppx1|x2,w,cqqPpw|cq)
dw
“ż
EPrY|x1,x2,w,csPpx1|x2,w,cqPpw|cqdw`1´ż
Ppx1|x2,w,cqqPpw|cqdw.
The inequality follows from the boundedness of Yand the rest of the arguments are analogous to
the lower bound derivation. Combining this expression with the decomposition of the policy effect
implies that,
EPπrYsďEPrpY´1q¯π{PpX2|W,Cqs`1.
Prop. 4 restated .LetZbe an unconditional partial instrumental set with respect to a policy π, i.e.
pY
|ùdZqGπ. Then,
EPπrYsěmax
zEPrY¯π 1zpZq{PpZqs
EPπrYsďmin
zEPrpY´1q¯π 1zpZq{PpZqs`1.
Proof. Consider the derivation of the lower bound. Let πdenote a policy from an arbitrary set of
covariates CtoX. Given thatpY
|ùdZqGπ, by marginalizing over X,Cwe find that
EPπrYs“EPrYπs“EPrYπ|zs“ż
EPrYπ|x,c,zsź
XPXπXpx|cXqPpc|zqdcdx.
CXdenotes the subset of Cthat is used to inform the intervention on X. Moreover,
EPrYπ|x,c,zsp1q“EPrYx|c,zs
“ÿ
x1EPrYx|x1,c,zsPpx1|c,zq
ěEPrYx|x,c,zsPpx|c,zq
p2q“EPrY|x,c,zsPpx|c,zq.
(1) holds by Rule 2 in Thm. 2 by swapping the policy πby the do intervention dopX“xq: given
that the policy acts on Xtaking as inputs Cit is always true that Y
|ùdX|CinGπXandGXX
in any graph G. The following equality follows by marginalizing over the domain of X, which is
assumed discrete. (2) follows by the composition axiom of counterfactuals. By combining these two
21expressions we get
EPrYπsěżź
XPXπpx|cXqPpc|zqEPrY|x,c,zsPpx|c,zqdcdx
“ż
¯πyPpy|x,c,zqPpx|c,zqPpc|zqdydcdx
“EPrY¯π|zs
“EPrY¯π 1zpZq{PpZqs.
And since the l.h.s. does not depend on Zwe can further tighten the bound by writing,
EPrYπsěmax
zEPrY¯π 1zpZq{PpZqs.
Consider the derivation of the upper bound. It holds that,
EPrYπ|x,c,zs“EPrYx|c,zs
“ÿ
x1EPrYx|x1,c,zsPpx1|c,zq
“EPrYx|x,c,zsPpx|c,zq`ÿ
x1‰xEPrYx|x1,c,zsPpx1|c,zq
p1q
ďEPrY|x,c,zsPpx|c,zq`ÿ
x1‰xPpx1|c,zq
“EPrY|x,c,zsPpx|c,zq`1´Ppx|c,zq.
(1) follows by the boundedness of Y, here assumed bounded by 1 for simplifying the derivation. As
above by combining this inequality with the decomposition of the policy effect we get
EPπrYsďżź
XPXπpx|cXqPpc|zqtEPrY|x,c,zsPpx|c,zq`1´Ppx|c,zqudcdx
“EPrY¯π|zs`1´EPr¯π|zs
“EPrpY´1q¯π|zs`1
“EPrpY´1q¯π 1zpZq{PpZqs`1.
And since the l.h.s. does not depend on Zwe can further tighten the bound by writing,
EPrYπsěmin
zEPrpY´1q¯π 1zpZq{PpZqs`1.
Prop. 5 restated. Alg. 1 is sound.
Proof. For the soundness of Alg. 1, we will consider each operation in turn and show that recovered
adjustment sets and conditional instrumental sets lead to a valid bound.
1. Omit redundant intervention variables. For a policy π:“ tπXpcXquXPXdenote πR:“
tπXpcXquXPR,RĎX. Let R“XŞAnpYqinGπ. By Rule 3 of the σ-calculus we have
that,
EPπrYs“EPπRrYs,
since Y
|ùdXzRinGπ,XzRandGπR,XzRas there are no directed paths from XzRtoYby
definition of R. The first two lines therefore reduce the number of intervention variables. We proceed
to bound the equivalent EPπRrYs.
2. Find partial adjustment sets W.Line 4-11 consider finding partial adjustment sets by traversing
the set of treatment variables RPR. Recall that
EPπRrYs“EPrYπRs“ż
EPrYπR|r,csź
XPRπXpx|cXqPpcqdcdr.
22Denote ¯πR:“ś
XPRπXpx|cXq. In words lines 4-11 iteratively considers the existence of
separators sets between RPRandYto reduce the scope of the policy on Y, i.e. YπR. By [ 43,
Lemma 3.18.], if there exists a separator Wbetween two sets of variables XandYsuch that
SĎWĎKin a graph G, i.e.X
|ùdY|WinG, thenW“AnpXŤYŤSqŞKis also a valid
separator. The definition of WRin line 7 aims to leverage this result to check for the existence of
separators in our case.
Consider a RPR, letS“RzR,K“VzpXŤCŤYq. In particular, if there exist a set tC,RuĂ
WĂKsuch thatpY
|ùdR|WqinGπS,RthenWR“AnpRŤYŤCqŞK“AnpYŤCqŞK
inGπSsatisfiespY
|ùdR|WRqinGπS,R, i.e.WRis a separator if one exists. Note that WRdoes
not include any member of tC,R, Yu. Assume this separation holds and that the if statement in line
8 is triggered. Then,
EPrYπR|r,cs“EPrYr|cs
“EPrYs,r|cs
“ż
EPrYs,r|c,wRsPpwR|cqdwR
“ż
EPrYs|r,c,wRsPpwR|cqdwR.
The last equality holds by assumption pY
|ùdR|WRqinGS,R.
Now considering the term EPrYs|r,c,wRs, for the second pass through the for loop, we can see
that if the d-separation statement in the if condition is fulfilled we can further reduce the scope of the
intervention on S. In particular, for R1PRzR, assuming that the if statement is triggered for WR1
we have that,
EPrYs|r,c,wRs“ż
EPrYs|r,c,wR,wR1sPpwR1|wR,cqdwR1
“ż
EPrYszr1|r1, r,c,wR,wR1sPpwR1|wR,cqdwR1.
Upon reaching the end of the for loop we have for Wdefined in line 9, T“tR:WRPWu,U“
RzTthat,
EPrYπ|r,cs“ż
EPrYu|t,c,wsPpw|cqdw.
3. Find partial instrumental sets Z.Starting line 12, we look for partial conditional instrumental sets.
Consider first the case that a potential instruments ZPWis evaluated in the for loop. On line 15,
assume that the if statement is triggered, i.e. pY
|ùdZ|WzZ,TqinGUIt then holds that,
EPrYπ|r,cs:“ż
EPrYu|w,t,csPpw|cqdw
“ż
EPrYu|wzz, z,t,csż
zPpw|cqdw
“ż
EPrYu|wzz, z,t,csPpwzz|cqdwzz.
In turn, for the case that some Z1RWis evaluated in the for loop, if pY
|ùdZ1|W,TqinGU,
EPrYπ|r,cs:“ż
EPrYu|w,t,csPpw|cqdw
“ż
EPrYu|w,t, z1,csPpw|cqdw
For the Zobtained in line 15, we derive that,
EPrYπ|r,cs“ż
EPrYu|wzz,z,t,csPpwzz|cqdwzz.
234. Return bounds. We now proceed to use the expression above to lower and upper bound the effect
of interest. In particular, for the lower bound we have that,
EPrYπ|x,cs“ż
EPrYu|wzz,z,t,csPpwzz|cqdwzz
“żÿ
u1EPrYu|u1,wzz,z,t,csPpu1|wzz,z,t,cqPpwzz|cqdwzz
ěż
EPrYu|u,wzz,z,t,csPpu|wzz,z,t,cqPpwzz|cqdwzz
“ż
EPrY|u,wzz,z,t,csPpu|wzz,z,t,cqPpwzz|cqdwzz.
By combining the inequality above with the decomposition of the policy effect we get
EPπrYs
ěżź
XPRπpx|cXqPpcqEPrY|u,wzz,z,t,csPpu|wzz,z,t,cqPpwzz|cqdcdrdwzz
“ż
¯πRyPpy|r,wzz,z,cqPpu|wzz,z,t,cqPpwzz|cqPpcqdcdrdwzzdy
“ż
¯πRyPpy|r,wzz,z,cqPpu|wzz,z,t,cqPpt,z|wzz,cq
Ppt,z|wzz,cqPpwzz|cqPpcq 1zpzqdcdrdwzzdydz
“ż
¯πRyPpy,r,wzz,z,cq1zpzq
Ppt,z|wzz,cqdcdrdwzzdydz
“EPrY γs
where γ:“¯πR 1zpZq{PpT,Z|WzZ,Cq. Note that we have used the definition R“TŤU.
In turn, for the upper bound we have that,
EPrYπ|x,cs“ż
EPrYu|wzz,z,t,csPpwzz|cqdwzz
“żÿ
u1EPrYu|u1,wzz,z,t,csPpu1|wzz,z,t,cqPpwzz|cqdwzz
“ż
EPrYu|u,wzz,z,t,csPpu|wzz,z,t,cqPpwzz|cqdwzz
`żÿ
u1‰uEPrYu|u1,wzz,z,t,csPpu1|wzz,z,t,cqPpwzz|cqdwzz
ďż
EPrYu|u,wzz,z,t,csPpu|wzz,z,t,cqPpwzz|cqdwzz
`żÿ
u1‰uPpu1|wzz,z,t,cqPpwzz|cqdwzz
“ż
EPrY|u,wzz,z,t,csPpu|wzz,z,t,cqPpwzz|cqdwzz
`ż
p1´Ppu|wzz,z,t,cqqPpwzz|cqdwzz
“1`ż
EPrY´1|u,wzz,z,t,csPpu|wzz,z,t,cqPpwzz|cqdwzz
24By combining the inequality above with the decomposition of the policy effect we get
EPπrYs
ď1`żź
XPRπpx|cXqPpcqEPrY´1|u,wzz,z,t,csPpu|wzz,z,t,cqPpwzz|cqdcdrdwzz
“1`ż
¯πRpy´1qPpy|r,wzz,z,cqPpu|wzz,z,t,cqPpwzz|cqPpcqdcdrdwzzdy
“1`ż
¯πRpy´1qPpy,r,wzz,z,cq1zpzq
Ppt,z|wzz,cqdcdrdwzzdydz
“1`EPrpY´1qγs
where γ:“¯πR 1zpZq{PpT,Z|WzZ,Cq. Note that we have used the definition R“TŤU.
Prop. 6 restated. Letkbe the number of variables and mbe the number of edges in G. The run time
of Alg. 1 is Opkpk2`mqq.
Proof. Letkbe the number of variables and mbe the number of edges in G. Operations in Alg. 1,
such as computing ancestors (e.g. lines 2,7), could be done in Opk2qtime, e.g. with a Breadth-First
Search algorithm. Checking for d-separation is commonly done with the Bayes-Ball algorithm that
can be implemented with a reachability search method, e.g. [ 43, Sec. 3.2.1], and could be done in
Opk`mqtime by [ 43, Prop. 3.17]. The sets RandZhave size at most k, so the for loops in lines 5
and 14 are executed at most ktimes each. Combining these we get that Alg. 1 requires Opkpk2`mqq
time to return the bounds.
Prop. 7 restated. Letπ:“tπX1, πX2ube a policy on tX1,X2uwith a conditioning set C. All
partial adjustment sets may be enumerated in time Opkpk`mqqwhere kare the number of variables
andmbe the number of edges in G.
Proof. This proposition is adapts Proposition 3.20 in [ 43] to find partial adjustment sets using the
ListSep algorithm. ListSep performs backtracking to enumerate all separator sets Zbetween
XandYsuch that IĎZĎR, aborting branches that will not find a valid separator. In particular,
it calls the TestSep andFindSep algorithms recursively.
TheTestSep algorithm takes as input a graph, two sets of nodes to be separated, and a candidate
separator set. By setting the graph to be GπX1X2, the two sets to be separated to be YandX2, and the
separator set Z,TestSep will provably return whether W“ZzpC,X1qis a partial adjustment
set.
TheFindSep algorithm uses the observation that if there exists a separator Zbetween Xand
Ysuch that IĎZĎRthenZ:“AnpXYYYIqXRis a separator. For each possible
partitionpX1,X2qofX, we can therefore find a separator by testing (using TestSep ) whether
AnpXYYYCqXpVzpXYYqqinGπX1X2is a separator. If it is then we have found a partial
adjustment set.
To find all partial adjustment sets, we then proceed as follows. For each possible partition pX1,X2q
ofX, apply the ListSep algorithm with graph GπX1X2, sets to be separated YandX2, and
possible separator sets Zconstrained as C,X1ĎZĎVzpXYYq. Then, for every separator set
Zfound, output partial adjustment sets W“ZzpC,X1q.
This procedure finds all possible partial adjustment sets in time Opkpk`mqqwhere kare the number
of variables and mbe the number of edges in Gby Proposition 3.20 [43].
B.2 Proofs of statements in Sec. 4
We make use of two auxiliary lemmas from the literature.
Lemma 1 (Continuous Mapping Theorem, [ 26]).LettXnunPN,Xbe random elements defined on
a metric space S. Consider a continuous function g:SÑS1(where S1is another metric space).
Then,
XnÑpXñgpXnqÑpgpXq.
25Lemma 2 (Lemma 2, [ 24]).Letfη:“fpV;ηqdenote a finite and continuous functional and η
denote its nuisances. For nindependent samples of P,D:“tvpiqui“1,...,n„P, letˆT“EDrfˆηsand
T:“EPrfηsfor some η. LetED´Prfηs:“EDrfηs´EPrfηs. Then, the following decomposition
holds:
EDrfˆηs´EPrfηs“ED´Prfηsloooomoooon
“A`ED´Prfˆη´fηsloooooooomoooooooon
“B`EPrfˆη´fηs. (19)
Suppose further that samples used for estimating ηare independent and separate; and the nuisances
are consistent. Then, R“A`Bis a random variable that converges to zero at a rate OPp1{?nq.
Prop. 8 restated. Suppose the nuisance estimates pˆµ,ˆγqareL2-consistent and bounded. Then, the
error of the DML estimator ˆTDMLPtˆTDML,ℓ,ˆTDML,uuin Def. 5 is given as follows
ˆTDML´TDML“1
KKÿ
k“1Rk`OP´
}ˆγ2,k´γ2}}ˆµ2,k´µ2}¯
`OP´
}ˆγ1,k´γ1}}ˆµ1,k´ˆ˜µ1,k}¯
where Rkis a random variable that converges to zero at a rate OPp1{?nq.
Proof. In this proof we consider the estimation of the lower bund without loss of generality. Recall
the definition of nuisance functions and estimators:
γ2:“¯πUγ1, γ 1:“¯πT 1zpZq{PpT,Z|WzZ,Cq,
withTPW,ℓ:“EPrγ2Ys“ψℓ
z. And µ“pµ0, µ1,˜µ2, µ2qdefined by
µ2:“µ2pR,C,W,Zq“EPrY|R,C,W,Zs,
˜µ1:“˜µ2pR,C,W,Zq“¯πUpU|Cqµ2pR,C,W,Zq,
µ1:“µ1pT,C,W,Zq“EPr˜µ2pR,C,W,Zq|T,C,W,Zs,
µ0:“µ0pC,W,Zq“ÿ
tµ1pt,C,W,Zq¯πTpt|Cq.
We first verify that the population level value of the regression estimator coincides with the lower
bound ψℓ
z. This can be seen with the following derivation,
TREG,ℓ:“EPrµ0pWzZ,C,zqs
“ÿ
wzz,cµ0pw,c,zqPpwzz,cq
“ÿ
wzz,c˜ÿ
tµ1pt,w,c,zq¯πT¸
Ppwzz,cq
“ÿ
t,wzz,c˜ÿ
uµ2pt,u,c,wzz,zq¯πUPpu|t,z,wzz,cq¸
¯πT
Ppt,z|wzz,cqPpt,z,wzz,cq
“ÿ
t,u,wzz,c˜ÿ
yyPpy|t,u,z,wzz,cq¸
¯πR
Ppt,z|wzz,cqPpt,u,z,w,cq
“ÿ
y,t,u,z1,wzz1,cy¯πR 1zpz1q
Ppt,z1|wzz1,cqPpy,t,u,z1,wzz1,cq
“EPrY¯πR 1zpZq{PpT,Z|WzZ,Cqs
“ψℓ
z
The DML estimator is similarly unbiased as,
TDML,ℓ:“EPrγ2tY´µ2us`EPrγ1t˜µ1´µ1us`EPrµ0s
“EPrγ2tEPrY|R,WzZ,C,Zs´µ2us`EPrγ1t˜µ1´µ1us`EPrµ0s
“EPrγ2tµ2´µ2us`EPrγ1tEPr˜µ1|T,C,WzZ,Zs´µ1us`EPrµ0s
“EPrγ1tµ1´µ1us`EPrµ0s
“EPrµ0s
“ψℓ
z.
26Consider the estimated value of TDML,ℓfollowing the procedure in Def. 5,
ˆTDML,ℓ:“1
KKÿ
k“1ˆTDML,ℓ
k,ˆTDML,ℓ
k:“EDpkqrˆγ2tY´ˆµ2us`EDpkqrˆγ1tˆ˜µ1´ˆµ1us`EDpkqrˆµ0s,
We could then write,
ˆTDML,ℓ
k´TDML,ℓ“A`B`C
where,
A“EDpkq´P”
γ2tY´µ2u`γ1t˜µ1´µ1u`µ0ı
B“EDpkq´P”´
γ2tY´µ2us`γ1t˜µ1´µ1u`µ0¯
´´
ˆγ2tY´ˆµ2u`ˆγ1tˆ˜µ1´ˆµ1u`ˆµ0¯ı
C“EP”´
γtY´µ2us`γt˜µ1´µ1u`µ0¯
´´
ˆγ2tY´ˆµ2u`ˆγ1tˆ˜µ1´ˆµ1u`ˆµ0¯ı
By the construction in Def. 5, the samples used for estimating pγ1, γ2, µ0, µ1,˜µ1, µ2qand evaluating
the outer expectation are independent and separate. Under the assumption that nuisance parameters
pˆγ1,ˆγ2,ˆµ0,ˆµ1,ˆ˜µ1,ˆµ2qare consistent, R“A`Bconverges to zero at a rate OPp1{a
|Dpkq|qby
Lem. 2.
Before manipulating Cand deriving its large sample behaviour, consider the following intermediate
results:
EPrµ0s“ÿ
c,wµ0pc,w,zqPpc,wq
“ÿ
c,wÿ
t¯πTµ1pt,c,w,zqPpc,wq
“ÿ
c,w,z1,t¯πT 1zpz1qµ1pt,c,w,z1qPpc,wq
“ÿ
c,w,z1,t¯πT 1zpz1q
Ppt,z1|c,wqµ1pt,c,w,z1qPpt,c,w,z1q
“EPrγ1µ1s,
and similarly EPrˆµ0s“EPrγ1ˆµ1s. Note further that,
EPrγ2µ2s“ÿ
c,w,t,u,z1¯πT¯πU 1zpz1q
Ppt,z1|c,wqµ2pc,w,t,u,z1qPpc,w,t,u,z1q
“ÿ
c,w,t,u,z1γ1¯πUµ2pc,w,t,u,z1qPpc,w,t,u,z1q
“ÿ
c,w,t,u,z1γ1˜µ1pc,w,t,u,z1qPpc,w,t,z1q
“EPrγ1˜µ1s,
and similarly EPrγ2ˆµ2s“EPrγ1ˆ˜µ1s.
Now consider C“C1`C2`C3where,
C1:“EPrˆγ2tY´ˆµ2u´γ2tY´µ2us“EPrˆγ2tµ2´ˆµ2us
C2:“EPrˆγ1tˆ˜µ1´ˆµ1u´γ1t˜µ1´µ1us
C3:“EPrˆµ0´µ0s“EPrγ1ˆµ1´γ1µ1s
27where the last equality follows from the result above. Therefore
C“EP”
ˆγ2tµ2´ˆµ2u`ˆγ1tˆ˜µ1´ˆµ1u´γ1t˜µ1´µ1u`γ1pˆµ1´µ1qı
“EP”
tˆγ2´γ2utµ2´ˆµ2u`γ2tµ2´ˆµ2u`ˆγ1tˆ˜µ1´ˆµ1u´γ1t˜µ1´µ1u`γ1pˆµ1´µ1qı
“EP”
tˆγ2´γ2utµ2´ˆµ2uı
`EP”
γ2tµ2´ˆµ2u`ˆγ1tˆ˜µ1´ˆµ1u´γ1t˜µ1´ˆµ1uı
p1q“EP”
tˆγ2´γ2utµ2´ˆµ2uı
`EP”
γ1t˜µ1´ˆ˜µ1u`ˆγ1tˆ˜µ1´ˆµ1u´γ1t˜µ1´ˆµ1uı
“EP”
tˆγ2´γ2utµ2´ˆµ2uı
`EP”
´γ1tˆ˜µ1´ˆµ1u`ˆγ1tˆ˜µ1´ˆµ1uı
“OP´
}ˆγ2´γ2}}µ2´ˆµ2}¯
`OP´
}ˆγ1´γ1}}ˆ˜µ1´ˆµ1}¯
(1) holds by the equalities EPrγ2µ2s“EPrγ1˜µ1s,EPrγ2ˆµ2s“EPrγ1ˆ˜µ1s.
Finally, this implies that
ˆTDML,ℓ´TDML,ℓ“R`OP´
}ˆγ2´γ2}}µ2´ˆµ2}¯
`OP´
}ˆγ1´γ1}}ˆ˜µ1´ˆµ1}¯
(20)
where Ris a random variable that converges to zero at a rate OPp1{?nq.
For the upper bound, consider the following definition of nuisance functions and estimators:
γ2:“¯πUγ1, γ 1:“¯πT 1zpZq{PpT,Z|WzZ,Cq,
withTPW,ℓ:“EPrγ2pY´1qs`1“ψℓ
z. And µ“pµ0, µ1,˜µ2, µ2qdefined by
µ2:“µ2pR,C,W,Zq“EPrY´1|R,C,W,Zs,
˜µ1:“˜µ2pR,C,W,Zq“¯πUpU|Cqµ2pR,C,W,Zq,
µ1:“µ1pT,C,W,Zq“EPr˜µ2pR,C,W,Zq|T,C,W,Zs,
µ0:“µ0pC,W,Zq“ÿ
tµ1pt,C,W,Zq¯πTpt|Cq.
We first verify that the population level value of the regression estimator coincides with the lower
bound ψℓ
z. This can be seen with the following derivation,
TREG,u:“EPrµ0pWzZ,C,zqs`1
“1`ÿ
wzz,cµ0pw,c,zqPpwzz,cq
“1`ÿ
wzz,c˜ÿ
tµ1pt,w,c,zq¯πT¸
Ppwzz,cq
“1`ÿ
t,wzz,c˜ÿ
uµ2pt,u,c,wzz,zq¯πUPpu|t,z,wzz,cq¸
¯πT
Ppt,z|wzz,cqPpt,z,wzz,cq
“1`ÿ
t,u,wzz,c˜ÿ
ypy´1qPpy|t,u,z,wzz,cq¸
¯πR
Ppt,z|wzz,cqPpt,u,z,w,cq
“1`ÿ
y,t,u,z1,wzz1,cpy´1q¯πR 1zpz1q
Ppt,z1|wzz1,cqPpy,t,u,z1,wzz1,cq
“1`EPrpY´1q¯πR 1zpZq{PpT,Z|WzZ,Cqs
“ψu
z
The DML estimator is similarly unbiased as,
TDML,u:“EPrγ2tpY´1q´µ2us`EPrγ1t˜µ1´µ1us`EPrµ0s`1
“EPrγ2tEPrY´1|R,WzZ,C,Zs´µ2us`EPrγ1t˜µ1´µ1us`EPrµ0s`1
“EPrγ2tµ2´µ2us`EPrγ1tEPr˜µ1|T,C,WzZ,Zs´µ1us`EPrµ0s`1
“EPrγ1tµ1´µ1us`EPrµ0s`1
“EPrµ0s`1
“ψu
z.
28The arguments of the proof are now analogous to that of the lower bound. We can conclude therefore
that,
ˆTDML,u´TDML,u“R`OP´
}ˆγ2´γ2}}µ2´ˆµ2}¯
`OP´
}ˆγ1´γ1}}ˆ˜µ1´ˆµ1}¯
. (21)
Prop. 9 restated. Suppose either ˆγ1“γ1orˆµ2“µ2and that either ˆγ1“γ1orˆ˜µ1“ˆµ1. Then,
ˆTDMLPtˆTDML,ℓ,ˆTDML,uuis an unbiased estimator of the corresponding bound defined in Alg. 1.
Proof. Consider the estimated value of TDML,ℓfollowing the procedure in Def. 5,
ˆTDML,ℓ:“1
KKÿ
k“1ˆTDML,ℓ
k,ˆTDML,ℓ
k:“EDpkqrˆγ2tY´ˆµ2us`EDpkqrˆγ1tˆ˜µ1´ˆµ1us`EDpkqrˆµ0s,
Under the assumption that,
EPrˆTDML,ℓs“EPrˆγ2tY´ˆµ2us`EPrˆγ1tˆ˜µ1´ˆµ1us`EPrˆµ0s
The bias of the estimator is given by,
EPrˆTDML,ℓs´TDML,ℓ
“EP”´
γtY´µ2us`γt˜µ1´µ1u`µ0¯
´´
ˆγ2tY´ˆµ2u`ˆγ1tˆ˜µ1´ˆµ1u`ˆµ0¯ı
“EP”
tˆγ2´γ2utµ2´ˆµ2uı
`EP”
tˆγ1´γ1utˆ˜µ1´ˆµ1uı
The last equality follows from the derivation of (C) in the proof of Prop. 8.
If either ˆγ1“γ1orˆµ2“µ2and that either ˆγ1“γ1orˆ˜µ1“˜µ1then this expression equals 0 and
we have that,
EPrˆTDML,ℓs´TDML,ℓ“0.
An analogous result holds for the upper bound.
29C Details on experiments
This section provides details of the data generating mechanisms used for synthetic experiments, and
implementations.
As described in the experimental section, for estimating nuisances pγ,µqwe used Gradi-
ent Boosting models for classification and regression where appropriate. We implemented
the models using Python using the commands GradientBoostingClassifier() and
GradientBoostingRegressor() using default hyperparameters. We systematically bound
probability values in the interval r0.01,0.99sto avoid propagating potentially large approximating
errors.
C.1 Simulations
We make use of the following 4 data generating systems, summarized by the graphs in Fig. 4. In this
section, we use the notation 1tXuthat equals 1if the statement tXuis true, and equal to 0otherwise.
Example 1. The data generating mechanism for the first scenario is given by:
UZ, UXY, UC„Np0,1q,
Z:“fZpUZq,
C:“fCpUCq,
X:“fXpZ, C, U XYq,
Y:“fYpX, C, U XYq,
where,
fZpUZq:“´1¨ 1tUZď´0.5u`0¨ 1t´0.5ăUZă0.5u`1¨ 1tUZě0.5u,
fCpUCq:“UC,
fXpC, U XYq:“ 1tUXYą1`expt0.75C`0.5Z`0.5uu,
fYpX, C, U XYq:“p2X´1q´2¨p2UXY´1q
The outcome Yis then re-scaled to lie in the r0,1sinterval. The target for estimation can be derived
with Alg. 1 and equals, for the lower and upper bound respectively,
max
zPt´1,0,1uEPr¯π 1zpZqY{PpZ|Cqs, min
zPt´1,0,1uEPr¯π 1zpZqpY´1q{PpZ|Cqs`1.
Example 2. The data generating mechanism for the second scenario is given by:
UX1Y, UX2W, UX2, UX1, UC, UW„Np0,1q,
C:“fCpUCq,
W:“fWpUX2Wq,
X1:“fX1pC, U X1Y, UX1q,
X2:“fX2pUX2W, UX2q,
Y:“fYpX1, X2, C, W, U X1Yq,
where,
fCpUCq:“UC,
fWpUWq:“UX2W,
fX1pC, U X1, UX1Yq:“ 1tUX1Y¨p1`expt0.75C`0.5uqąUX1u,
fX2pUX2W, UX2q:“ 1tUX2W`UX2ą0u,
fYpX1, X2, C, W, U X1Yq:“p2X1´1q¨pC`1q`2 sinp2C¨p2X1´1qq
´2p2UX1Y´1qp1`0.5Cq`X2`W.
The outcome Yis then re-scaled to lie in the r0,1sinterval. The target for estimation can be derived
with Alg. 1 and equals, for the lower and upper bound respectively,
EPr¯πY{PpX2|W, Cqs,EPr¯πpY´1q{PpX2|W, Cqs`1.
30Example 3. The data generating mechanism for the third scenario is given by:
UX1Y, UX2W, UX2, UX1, UC„Np0,1q,
W:“tW1, . . . , W 100us.t.Wi„Np0,1q, i“1, . . . , 100,
C:“fCpUCq,
X1:“fX1pC, U X1Y, UX1q,
X2:“fX2pW, UX2q,
Y:“fYpX1, X2, C,W, UX1Yq,
where,
fCpUCq:“UC,
fX1pC, U X1Y, UX1q:“ 1tUX1Y¨p1`expt0.75C`0.5uqąUX1u,
fX2pW, UX2q:“ 1tW1`W2`UX2ą0u,
fYpX1, X2, C,W, UX1Yq:“p2X1´1q¨pC`1q`2 sinp2C¨p2X1´1qq
´2p2UX1Y´1qp1`0.5Cq`X2`ÿ
i“1,...,100Wi{100.
The outcome Yis then re-scaled to lie in the r0,1sinterval. The target for estimation can be derived
with Alg. 1 and equals, for the lower and upper bound respectively,
EPr¯πY{PpX2|W, Cqs,EPr¯πpY´1q{PpX2|W, Cqs`1.
Example 4. The data generating mechanism for the fourth scenario is given by:
UZ, UX1Y, UX2, UX1, UC, UZ„Np0,1q,
Z:“fZpUZq
W:“tW1, . . . , W 5us.t.Wi„Np0,1q, i“1, . . . , 5,
C:“fCpUCq,
X1:“fX1pC, Z, U X1Y, UX1, Zq,
X2:“fX2pW, UX2q,
Y:“fYpX1, X2, C, W, U X1Yq,
where,
fCpUCq:“UC,
fZpUZq:“ 1tUZą0u,
fX1pUX1Y, UX1, Z, U Cq:“ 1tUX1Y¨p1`expt0.75UC`0.5Z`0.5uqąUX1u,
fX2pW, UX2q:“ 1tW1`W2`UX2ą0u,
fYpX1, X2, C,W, UX1Yq:“p2X1´1q¨pC`1q`2 sinp2C¨p2X1´1qq
´2p2UX1Y´1qp1`0.5Cq`X2`W1
The outcome Yis then re-scaled to lie in the r0,1sinterval. The target for estimation can be derived
with Alg. 1 and equals, for the lower and upper bound respectively,
max
zPt0,1uEPr¯π 1zpZqY{PpZ, X 2|W, Cqs,min
zPt0,1uEPr¯π 1zpZqpY´1q{PpZ, X 2|W, Cqs`1.
C.2 Health Campaign Evaluation
The data was curated from anonymous from Colombia, Peru and Mexico, using a web platform [ 30].
The exact questions considered in the survey can be found in [ 30]. The authors made the data available
under a Creative Commons license5and is currently hosted by Kaggle as a c.s.v file, accessible through
the following link: kaggle.com/code/mpwolke/obesity-levels-life-style/ .
5creativecommons.org/licenses/by/4.0/
31TS
F
H YE
MA
(a)GTS
F
H YE
MA
(b)GπTS
F
H YE
MA
(c)GπHE
Figure 7: Obesity diagrams.
We considered a number of pre-processing steps. In particular, we created a BMI variable from weight
and height data that were a posteriori removed from the dataset, imputed (with median or mode
among observed instances) missing values, scaled continuously-valued covariates by subtracting
the mean and dividing by the standard deviation, and scaled the outcome BMI to lie in the interval
r0,1s. Several covariates were ignored, including data on the consumption of water, consumption
of vegetables, consumption of food between meals, consumption of alcohol, and number of main
meals. These may be interpreted as unobserved variables, potentially confounding other relationships
in the data as specified by the assumed causal diagram. The data from a total of 2111 individuals was
recorded in this study.
The bounds on the effect of a policy πα:“tπα
H, πα
Euacting on HandE, with new assignments
defined astPnewpH“rarelyq “αu, PnewpE“regularlyq “αuare approximated from
the expression in Prop. 4. Following this proposition we find that Age pAq, SmokingpSq, calories
consumption Monitoring pMq, and Family history with overweight pFq, that is W“tA, S, M, Fu,
form a partial adjustment set for πα
Egiven thatpY
|ùdE|W, HqGπHE. We can verify also that
Z“tTuis a partial instrumental set for πsincepY
|ùdTqGπ. For illustration, the corresponding
mutilated diagrams are shown in Fig. 7. Finally, we partition the data equally to obtain two sets of
samples for training first and second stage classifiers and regressors, respectively, obtaining a first
estimate of bounds, before then switching the role of the two data samples and averaging over the
resulting estimates.
32D NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The claims made match the theoretical and experimental results presented
in the paper. The impact statement in the Appendix reflects how much the results can be
expected to generalize to other settings.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We include several remarks in the paper to qualify our contribution, e.g. the
remark in Sec. 3, and include an Impact Statement in the Appendix to more thoroughly
describe the limitations of our analysis, assumptions, and applicability in real-world settings.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
33Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All theoretical statement are quoted in full in the paper. We have attempted to
provide an example to illustrate the significance of each theoretical statement and highlight
its implications. The formal proof of all statements is given in the Appendix.
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide the data generating mechanisms, details of the target of estimation
and information as to what python functions and libraries can be used to fit the proposed
estimators. We do not, however, disclose an open source implementation of the proposed
methods at this moment.
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
34some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes] and [No]
Justification: The data is publicly available and we have provided full details as to where to
access the data and how to run the synthetic data generation pipeline. The code will not be
open sourced at this moment but we believe to have provided sufficient details to reproduce
our results.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provided details where applicable. In our case, data splits, hyper-
parameters, optimizers, etc., are not significant for the implementation the method.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report our results as box plots that include several statistics, e.g. median,
25th, 75th, percentiles, etc., to summarize our results for different seeds of the data and
algorithm.
Guidelines:
35• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: Experiments were run on a single CPU in under a few minutes of wall time.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have read the guidelines and we do not think that our work presents any
notable concerns.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
36Justification: We include a broader impact statement in the Appendix.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: No risk of misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing asset
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
37• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
38•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
39