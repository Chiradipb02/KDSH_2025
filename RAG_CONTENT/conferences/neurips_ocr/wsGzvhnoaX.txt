Quantum Algorithms for Non-smooth Non-convex
Optimization
Chengchang Liu* 1Chaowen Guan* 2Jianhao He# 1John C.S. Lui1
1The Chinese University of Hong Kong2University of Cincinnati
7liuchengchang@gmail.com guance@ucmail.uc.edu
jianhaohe9@cuhk.edu.hk cslui@cse.cuhk.edu.hk
Abstract
This paper considers the problem of ﬁnding the (δ,ǫ)-Goldstein stationary point
of the Lipschitz continuous objective, which is a rich funct ion class to cover a
large number of important applications. We construct a nove l zeroth-order quan-
tum estimator for the gradient of the smoothed surrogate. Ba sed on such estima-
tor, we propose a novel quantum algorithm that achieves a que ry complexity of
˜O(d3/sl⊗sh.left2δ−1ǫ−3)on the stochastic function value oracle, where dis the dimension
of the problem. We also improve the query complexity to ˜O(d3/sl⊗sh.left2δ−1ǫ−7/sl⊗sh.left3)by
introducing a variance reduction variant. Our ﬁndings demo nstrate the clear ad-
vantages of using quantum techniques for non-convex non-sm ooth optimization,
as they outperform the optimal classical methods in depende nce onǫby a factor
ofǫ−2/sl⊗sh.left3.
1 Introduction
In this paper, we study the following problem
min
x∈Rd/brac⟩l⟩ft.alt1f(x)≜Eξ[F(x;ξ)]/brac⟩right.alt1, (1)
where the stochastic component F(x;ξ)isL-Lipschitz continuous but possibly non-convex and
non-smooth . This problem has received increasing attention recently b ecause it is general enough to
cover many important applications, including deep neural n etworks [21, 40], reinforcement learn-
ing [9, 49], and statistical learning [17, 39, 62].
Due to the absence of both smoothness and convexity in the obj ective function, neither the gradient
nor the sub-differentials are valid anymore to measure the c onvergence behavior. The Clarke subdif-
ferential is a natural extension for describing the ﬁrst-or der information of the Lipschitz continuous
function [10], however, it is intractable for ﬁnding the nea r-approximate stationary point in terms of
the Clarke subdifferential as suggested by the hard instanc es [31, 50, 63]. Zhang et al. [63] intro-
duce the notion of (δ,ǫ)-Goldstein stationary point (cf. Section 2.2), which weake ns the traditional
stationary point by considering the convex hull of the Clark e subdifferentials. Following this, we
focus on the problem of ﬁnding the (δ,ǫ)-Goldstein stationary points of the objective.
There are many optimization methods for ﬁnding the (δ,ǫ)-Goldstein stationary points via classical
stochasic oracles [6, 14, 28, 31, 35, 47, 52, 63]. Zhang et al. [63] proposed stochastic interpo-
lated normalized gradient descent method (SINGD) with the ﬁ rst non-asymptotic result, which has
the stochastic ﬁrst-order complexity of O(δ−1ǫ−4). Later, Tian et al. [52] developed the perturbed
*denotes equal contributions;#denotes the corresponding author.
38th Conference on Neural Information Processing Systems (NeurI PS 2024).Table 1: We summarize the complexities of classical and quan tum zeroth-order methods for ﬁnding
the(ǫ,δ)-Goldstein point of a non-smooth non-convex objective, where dis the dimension of the
problem.
Methods Oracle Query Complexity Reference
GFM classicalO/par⟩nl⟩ft.alt1d3/sl⊗sh.left2δ−1ǫ−4/par⟩nright.alt1 Lin et al. [35]
GFM+ classicalO/par⟩nl⟩ft.alt1d3/sl⊗sh.left2δ−1ǫ−3/par⟩nright.alt1 Chen et al. [6]
OptimalZO classicalO/par⟩nl⟩ft.alt1dδ−1ǫ−3/par⟩nright.alt1 Kornowski and Shamir [32]
QGFM quantum ˜O/par⟩nl⟩ft.alt1d3/sl⊗sh.left2δ−1ǫ−3/par⟩nright.alt1 Theorem 4.1
QGFM+ quantum ˜O/par⟩nl⟩ft.alt1d3/sl⊗sh.left2δ−1ǫ−7/sl⊗sh.left3/par⟩nright.alt1 Theorem 4.3
Table 2: We summarize the complexities of classical and quan tum ﬁrst-order methods for ﬁnding
theǫ-stationary point of a smooth non-convex objective, where dis the dimension of the problem.
Methods Oracle Query Complexity Reference
SPIDER/PAGE classicalO(ǫ−3) Fang et al. [18], Li et al. [34]
Q-SPIDER quantum ˜O(d1/sl⊗sh.left2ǫ−5/sl⊗sh.left2) Sidford and Zhang [48]
QGM+ quantum ˜O(d1/sl⊗sh.left2ǫ−7/sl⊗sh.left3) Theorem G.1
SINGD method which queries the gradient at the differentiab le point and established the same com-
plexity. Cutkosky et al. [13] improved the stochastic ﬁrst- order oracle complexities to O(δ−1ǫ−3)by
using the “online to non-convex conversation”, assuming f(⋅)is differentiable. This improvement
aligns with the theoretical lower bound [13].
Zeroth-order methods, which only query the function value o racle, are more practical for the Lips-
chitz continuous objective. This is because computing ﬁrst -order oracles can be extremely challeng-
ing [29, 52] or even inaccessible for numerous real-world ap plications [16, 27, 43]. Lin et al. [35]
proposed a gradient-free method to ﬁnd the (δ,ǫ)-Goldstein stationary point within O(d3/sl⊗sh.left2δ−1ǫ−4)
query complexity to the stochastic function value via a conn ection between the randomized smooth-
ing [41] and the Goldstein stationary point. This complexit y was further improved to O(d3/sl⊗sh.left2δ−1ǫ−3)
andO(dδ−1ǫ−3)by Chen et al. [6], Kornowski and Shamir [32] respectively. H owever, all these
methods using the classical oracles to ﬁnd the Goldstein sta tionary point face a bottleneck of δ−1ǫ−3
due to the lower bound reported by [13].
Recently, we have witnessed the power of quantum optimizati on methods by accessing the quantum
counterparts of classical oracles for non-convex optimization [7, 23, 37, 48, 61, 64], convex opti-
mization [4, 5, 48, 55, 64], and semi-deﬁnite programming [1 , 2, 53, 54]. However, most of these
results focus on deterministic methods and the case where th e objective function is smooth . Garg
et al. [19] and Zhang and Li [60] showed the negative results f ornon-smooth convex andsmooth non-
convex optimization that quantum algorithms have no improved rate s over classical ones when the
dimension is large. Sidford and Zhang [48] proposed stochas tic quantum methods which show the
advantage of using quantum stochastic ﬁrst-order oracles f orsmooth objectives when the dimension
is relatively small. To the best of our knowledge, there is no work showing the quantum speedups for
minimizing non-smooth non-convex objectives, which is the most general and fundamental funct ion
class. Based on this, it is a natural question to ask:
Can we go beyond the complexity of O(δ−1ǫ−3)to ﬁnd the(δ,ǫ)-Goldstein stationary point for
non-smooth non-convex stochastic optimization by involvi ng quantum oracles?
We give an afﬁrmative answer to the above question by proposi ng novel quantum zeroth-order meth-
ods and showing their explicit query complexities. We summa rize our contributions as follows.
• We construct efﬁcient quantum gradient estimators for the smoothed surrogate of the objectives
withO(1)-queries of the function value oracles, which allows us to co nstruct efﬁcient quantum
2zeroth-order methods. Moreover, we provide explicit const ructions of quantum superposition
over required distributions. We present these results in Se ction 3 and Appendix A.
• We propose the quantum gradient-free method (QGFM) and the fast quantum gradient-free
method (QGFM+) for non-smooth non-convex optimization. We achieve the query complexities
ofO(d3/sl⊗sh.left2δ−1ǫ−3)for QGFM andO(d3/sl⊗sh.left2δ−1ǫ−7/sl⊗sh.left3)for QGFM+ in ﬁnding the (δ,ǫ)-Goldstein
stationary point using quantum stochastic function value o racle. The query complexity of
QGFM+ surpasses the optimal result achieved by classical me thods by a factor of ǫ−2/sl⊗sh.left3. We
compare our methods with the classical zeroth-order method s in Table 1 and present the results
in Section 4.
• We generalize the algorithm framework of QGFM+ for smooth non-convex optimization (i.e. the
gradient of the objective function is Lipschitz continuous ). We propose the fast quantum gradient
method (QGM+), which takes the advantage of QGFM+ to choose t he variance level adaptively.
QGM+ enjoys an improved complexity of ˜O(d1/sl⊗sh.left2ǫ−7/sl⊗sh.left3)queries of the quantum stochastic gradi-
ent oracle, which outperforms the existing state-of-the-a rt method (Q-SPIDER [48]) by a factor
ofǫ−1/sl⊗sh.left6. We compare our method with the classical and quantum ﬁrst-o rder methods in Table 2.
A discussion of this is presented in Remark 4.5, and the forma l results are stated in Appendix G.
2 Preliminaries
We introduce preliminaries for quantum computing model and non-smooth non-convex optimization
in this section.
2.1 Preliminaries for Quantum Computing Model
Here we formally review the basics and some concepts from qua ntum computing with which we
work. For more details, see Nielsen and Chuang [42].
Quantum Basics. A quantum state can be seen as a vector x=(x1,x2,...,x m)⊺in the Hilbert
spaceHmsuch that∑i/divid⟩s.alt0xi/divid⟩s.alt02=1. We follow the Dirac bra/ket notation on quantum states, i.e .,
we denote the quantum state for xby/divid⟩s.alt0x⟩and denote x†by⟨x/divid⟩s.alt0, where † means the Hermitian
conjugation.
Given a state/divid⟩s.alt0ψ⟩=∑m
i=1ci/divid⟩s.alt0i⟩, we callci∈Cthe amplitude of the state /divid⟩s.alt0i⟩. Given two quantum
states/divid⟩s.alt0x⟩∈Hmand/divid⟩s.alt0y⟩∈Hm, we denote their inner product by ⟨x/divid⟩s.alt0y⟩≜∑ix†
iyi. Given/divid⟩s.alt0x⟩∈Hm
and/divid⟩s.alt0y⟩∈Hn, we denote their tensor product by /divid⟩s.alt0x⟩⊗/divid⟩s.alt0y⟩≜(x1y1,⋯,xmyn)⊺∈Hm×n. If we
measure state/divid⟩s.alt0ψ⟩=∑m
i=1ci/divid⟩s.alt0i⟩on a computational basis, we will obtain iwith probability/divid⟩s.alt0ci/divid⟩s.alt02and
the state will collapse into /divid⟩s.alt0i⟩after measurement for all i. A quantum algorithm works by applying
a sequence of unitary operators to a initial quantum state.
Quantum Query Complexity. Corresponding to the classical query model, quantum query c om-
plexity considers the number of queries to a black box of a par ticular function which needs to be
invoked to solve a problem. In many cases, the black box corre sponds to the process that has the
highest overhead, and therefore reducing the number of quer ies to it will effectively reduce the com-
putational complexity of the entire algorithm. For example , if a classical oracle Cffor a function
fis a black box that, when queried with a point x, outputs the function value Cf(x)=f(x), then
the corresponding quantum oracle Ufis a unitary transformation that maps a quantum state /divid⟩s.alt0x⟩/divid⟩s.alt0q⟩
to the state/divid⟩s.alt0x⟩/divid⟩s.alt0q+f(x)⟩. Moreover, given the superposition input ∑x,qαx,q/divid⟩s.alt0x⟩/divid⟩s.alt0q⟩, applying the
quantum oracle once will, by linearity, output the quantum s tate∑x,qαx,q/divid⟩s.alt0x⟩/divid⟩s.alt0q+f(x)⟩.
2.2 Preliminaries for Non-convex Non-smooth Optimization
We introduce the necessary background for non-convex non-s mooth optimization, with the follow-
ing mild assumption that the objective function is Lipschit z continuous.
Assumption 1. We assume the stochastic component F(⋅;ξ)of the objective f(⋅)satisﬁes that
/divid⟩s.alt0F(x;ξ)−F(y;ξ)/divid⟩s.alt0≤L/parall⟩l.alt1x−y/parall⟩l.alt1for every x,y∈Rd. In addition, we assume f∶Rd→Ris lower
bounded and denote f∗≜infx∈Rdf(x).
3The Rademencher theorem indicates that f(⋅)is differentiable almost everywhere under Assump-
tion 1, which allows us to deﬁne its Clarke subdifferential a s follows [10].
Deﬁnition 2.1 (Clarke sub-differential) .The Clarke sub-differential of a Lipschitz function at poin t
xis deﬁned by ∂f(x)≜conv{g∶g=limxk→x∇f(xk)}.
We then introduce the Goldstein subdifferential [22] and th e(δ,ǫ)-Goldstein stationary point [63].
Deﬁnition 2.2 (Goldstein sub-differential) .The Goldstein subdifferential of a Lipschitz function at
pointxis deﬁned by ∂δf(x)≜conv/brac⟩l⟩ft.alt1∪y∈Bδ(x)∂f(y)/brac⟩right.alt1.
Deﬁnition 2.3 ((δ,ǫ)-Goldstein stationary point) .We callxthe(δ,ǫ)-Goldstein stationary point of
a given Lipschitz function if it satisﬁes dist(0,∂δf(x))≤ǫ,where∂δf(x)is the Goldstein subdif-
ferential.
Next, we deﬁne the smoothed surrogate of f(⋅)as follows.
Deﬁnition 2.4 (δ-smoothed surrogate) .Theδ-smoothed surrogate of fis deﬁned by
fδ(x)≜Ew∼P[f(x+δw)], (2)
wherePis the uniform distribution on a unit ball.
Althoughf(⋅)is non-smooth, its smoothed surrogate fδ(⋅)enjoys some good properties as presented
in the following proposition [6, 15, 35, 59].
Proposition 2.1. Iff(⋅)satisﬁes Assumption 1, its smoothed surrogate fδ(⋅)satisﬁes that:
•/divid⟩s.alt0fδ(⋅)−f(⋅)/divid⟩s.alt0≤δL and/divid⟩s.alt0fδ(x)−fδ(y)/divid⟩s.alt0≤L/parall⟩l.alt1x−y/parall⟩l.alt1.
•∇fδ(⋅)isc√
dLδ−1-Lipschitz for some constant c>0, i.e./parall⟩l.alt1∇fδ(x)−∇fδ(y)/parall⟩l.alt1≤c√
dL/parall⟩l.alt1x−y/parall⟩l.alt1.
•∇fδ(⋅)∈∂δf(⋅), where∂δf(⋅)is the Goldstein subdifferential.
Remark 2.2.Proposition 2.1 implies that the task of ﬁnding the (δ,ǫ)-Goldstein stationary point of
f(⋅)is equivalent to ﬁnding the ǫ-stationary point of a smoothed function fδ(⋅), i.e. ﬁnding some
pointxsuch that/parall⟩l.alt1∇fδ(x)/parall⟩l.alt1≤ǫ.
3 Zeroth-order Based Stochastic Quantum Estimator
In this section, we present a novel quantum estimator for the gradient of the smoothed surrogate
fδ(⋅)by using the quantum stochastic function value oracle, whic h is essential for designing our
quantum algorithms for non-convex non-smooth optimizatio n.
3.1 Quantum Estimators via Quantum Stochastic Function Val ue Oracle
In this section, we construct quantum estimators for the gra dient of the smoothed surrogate by O(1)
-queries of the quantum stochastic function value oracle.
We start with the deﬁnition of the stochastic function value oracle. Classically, a stochastic function
value evaluation is deﬁned as F(x,ξ)for a function f∶Rd→Rwithξsuch that Eξ[F(x,ξ)]=
f(x).In this work, we assume access to a quantum stochastic function value oracle UFforf(⋅),
which is deﬁned as follows.
Deﬁnition 3.1 (Quantum stochastic function value oracle) .Forf∶Rd→R, the quantum stochastic
function value oracle, denoted by UF, works as: UF∶/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0b⟩/leftfootline→/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0b+F(x,ξ)⟩,
whereF(x,ξ)is sampled from a distribution pξ(⋅)such that Eξ[F(x;ξ)]=F(x).
It is common to construct the following stochastic gradient estimator for∇fδ(⋅)[6, 32, 35, 36, 41]:
gδ(x;w,ξ)≜d
2δ(F(x+δw;ξ)−F(x−δw;ξ))⋅w, (3)
wherew∈Rdis uniformly distributed on a unit sphere. The following pro position shows that
gδ(x;w,ξ)is a good estimator of ∇fδ(⋅).
4Proposition 3.1 ([6, Proposition 3 and 4]) .Under Assumption 1, i.e. the random variable ξsatisﬁes
that
/divid⟩s.alt0F(x;ξ)−F(y;ξ)/divid⟩s.alt0≤L/parall⟩l.alt1x−y/parall⟩l.alt1andEξ[F(x;ξ)]=f(x), (4)
hold for all x,y∈Rd, thengδ(x;w,ξ)deﬁned in eq. (3)satisﬁes that Ew,ξ[gδ(x;w,ξ)]=∇fδ(x),
Ew,ξ[/parall⟩l.alt1gδ(x;w,ξ)−∇fδ(x)/parall⟩l.alt12]≤cπdL2, andEw,ξ[/parall⟩l.alt1gδ(x;w,ξ)−gδ(y;w,ξ)/parall⟩l.alt12≤d2L2
δ2/parall⟩l.alt1x−y/parall⟩l.alt12,
wherec=16√
2π.
Next, to exploit the power of quantum algorithms, we general ize eq. (3) to its quantum counter-
part. Based on eq. (3) and Proposition 3.1, gδ(x;w,ξ)can be interpreted as a random variable. In
the quantum setting, accessing a random variable typically involves querying a quantum sampling
oracle , which returns a quantum superposition over the associated distribution.
Deﬁnition 3.2 (Quantum sampling oracle) .For a random variable Xwith sample space Ω, its
quantum sampling oracle OXis deﬁned as OX∶/divid⟩s.alt00⟩/leftfootline→∑x/radical.alt1
Pr[X=x]/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ψx⟩,where/divid⟩s.alt0ψx⟩is
an arbitrary quantum state for every x.
The content in the second quantum register can also be viewed as possible quantum garbage appear-
ing during the implementation of the oracle. Observe that if we directly measure the output of OX,
it will collapse to a classical sampling access to Xthat returns a random sample xwith respect to
probability Pr[X=x]. Note that the output of OXcan also be represented as integral to continuous
random variables, as used in [8, 48].
Hence, based on our observation that gδ(x;w,ξ)can be viewed as a random variable, our target
oracleOgδ–quantum stochastic gradient oracle–is essentially a quan tum sampling oracle. Given
this, we formally deﬁne the quantum δ-estimated stochastic gradient oracle as follows.
Deﬁnition 3.3 (Quantumδ-estimated stochastic gradient oracle) .Forfδ(⋅)∶Rd→R, its quantum
δ-estimated stochastic gradient oracle is deﬁned as
Ogδ∶/divid⟩s.alt0x⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩/leftfootline→/divid⟩s.alt0x⟩⊗/summation.disp
ξ,w/radical.alt1
Pr[w,ξ]/divid⟩s.alt0gδ(x;w,ξ)⟩⊗/divid⟩s.alt0ψw,ξ⟩,
where the random variable wis uniformly distributed in a unit sphere and ξsatisﬁes eq. (4).
Proposition 3.1 implies gδ(⋅)can serve as an estimator of ∇fδ, and can be calculated with access
to a quantum δ-estimated stochastic gradient oracle as deﬁned above. The following theorem shows
that such an oracle can be built with only O(1)access to the quantum stochastic function value
oracle.
Lemma 3.2. Given access to a quantum sampling oracle Oξ,wto the joint distribution on (ξ,w),
one can construct a quantum δ-estimated stochastic gradient oracle (as deﬁned in Deﬁnit ion 3.3)
with two queries to the quantum stochastic function value or acleUF.
Remark 3.3.In Lemma 3.2, we assume a black-box access to quantum samplin g oracleOξ,wfol-
lowing Sidford and Zhang [48]. We present the explicit const ruction of this oracle in Appendix A.
Similarly, we can also constructed the estimator of ∇fδ(x)−∇fδ(y)by the following oracle:
O∆gδ∶/divid⟩s.alt0x⟩⊗/divid⟩s.alt0y⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩/leftfootline→/divid⟩s.alt0x⟩⊗/divid⟩s.alt0y⟩⊗/summation.disp
ξ,w/radical.alt1
Pr[w,ξ]/divid⟩s.alt0gδ(x;w,ξ)−gδ(y;w,ξ)⟩⊗/divid⟩s.alt0ψw,ξ⟩,
with onlyO(1)-queries of stochastic quantum function value oracle.
Corollary 3.4. Under the same conditions as in Lemma 3.2, one can construct O∆gδwith four
queries to the quantum stochastic function value oracle UF.
3.2 Mini-batch Quantum Estimators via Quantum Mean Estimatio n
We constructed the quantum oracles OgδandO∆gδwithO(1)-queries of quantum function
value oracles in Section 3.1. These oracles produce outputs in the form of random variables.
Speciﬁcally, Ogδprovides an output with expectation ∇fδ(x)with the input x, andO∆gδprovides
an output with expectation ∇fδ(x)−∇fδ(y)forO∆gδwith the inputs xandy.
5Algorithm 1 Quantum Gradient-Free Method (QGFM)
1:fort=0,1...T
2: Construct gtas an unbiased quantum estimator of ∇fδ(xt)with variance at most ˆσ2
tusing
UFaccording to Theorem 3.5.
3:xt+1=xt−ηgt
4:end for
The variance of the outputs can be reduced by constructing th e mini-batch estimator. Inspired by
the recent advance on quantum mean estimation [11, 12, 48] wh ich improve the classical mini-batch
estimator for multi-dimensional random variables, we cons truct improved estimators for ∇fδ(x)
and∇fδ(x)−∇fδ(y). We formally present the results in the following theorem.
Theorem 3.5. Under Assumption 1, and given access to a quantum sampling or acleOξ,wto the
joint distribution on (ξ,w), it holds that:
1. there exists an algorithm that can construct an unbiased q uantum estimator ˆgof∇fδ(x)such
thatE/brack⟩tl⟩ft.alt1/parall⟩l.alt1ˆg−∇fδ(x)/parall⟩l.alt12/brack⟩tright.alt≤ˆσ2
1within˜O(dLˆσ−1
1)queries of UFin expectation.
2. there exists an algorithm that can construct an unbiased q uantum estimator ∆gof∇fδ(x)−
∇fδ(y)such that E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∆g−(∇fδ(x)−∇fδ(y))/parall⟩l.alt12/brack⟩tright.alt≤ˆσ2
2within˜O(d3/sl⊗sh.left2L/parall⟩l.alt1y−x/parall⟩l.alt1ˆσ−1
2δ−1)
queries of UFin expectation.
Remark 3.6.Compared to the classical mini-batch estimator for ∇fδ(x), which requires
O(dL2ˆσ−2
1)queries of CFto achieve the level of variance ˆσ2
1([6, Corollary 2.1]), our mini-batch
quantum estimator for ∇fδ(x)in Theorem 3.5 reduces a factor of Lˆσ−1
1without increasing the
dimension dependence.
4 Quantum Algorithms for Finding the Goldstein Stationary P oint
In this section, we develop novel quantum algorithms for ﬁnd ing the(δ,ǫ)-Goldstein stationary
point of a non-smooth non-convex objective f(⋅). Instead of ﬁnding the stationary point directly,
we consider ﬁnding the ǫ-stationary point of its smoothed surrogate fδ(⋅), which is equivalent to
the original problem according to Remark 2.2. The classical zeroth-order methods based on such
equivalence require to access the gradient estimator to ∇fδ(⋅)by stochastic function values [6, 32,
35, 36]. Different from the classical methods, we can take th e advantage of the quantum estimators,
which can be constructed by accessing quantum stochastic fu nction value oracles due to our novel
results in Section 3.
We ﬁrst propose an algorithm which uses the quantum gradient estimator to replace ∇fδ(x)to do
the gradient descent step at each iteration. We present the q uantum gradient-free method (QGFM) in
Algorithm 1. Given a desired variance level ˆσ2
t, line 2 of Algorithm 1 can be constructed explicitly
and efﬁciently by the quantum stochastic function value ora clesUFaccording to Theorem 3.5. The
following theorem gives the upper bound on the total UFthat Algorithm 1 require to access for
ﬁnding the(δ,ǫ)-Goldstein stationary point.
Theorem 4.1. Under Assumption 1, by setting the parameter in Algorithm 1 a sη=
δ/slash.l⟩ft(2d1/sl⊗sh.left2L)andˆσ2
t≡ǫ2/slash.l⟩ft2,then the total queries of stochastic quantum function value oracleUF
for ﬁnding the(δ,ǫ)-Goldstein stationary point of f(⋅)can be bounded by ˜O/par⟩nl⟩ft.alt2d3/sl⊗sh.left2/par⟩nl⟩ft.alt2L3
ǫ3+L2∆
δǫ3/par⟩nright.alt2/par⟩nright.alt2,
where∆=f(x0)−f∗.
Remark 4.2.QGFM(Algorithm 1) speedups the gradient-free method (GFM) [35] for ﬁnding(δ,ǫ)-
stationary point by a factor of Lǫ−1.
In particular, Algorithm 1 utilized a simple gradient desce nt step to achieve Ω(δ−1ǫ−3), which is
optimal for classical zeroth-order and ﬁrst-order methods in terms ofǫandδ. It is worth mentioning
that the classical methods that achieve this lower bound typ ically involve multiple loops [6] or rely
on additional online optimization algorithms [13, 32].
To further enhance the query complexity in Theorem 4.1, we pr opose the fast quantum gradient-
free method (QGFM+) by incorporating variance reduction te chniques, as outlined in Algorithm 2.
6Algorithm 2 Fast Quantum Gradient-Free Method (QGFM+)
1:Construct g0as an unbiased estimator of ∇fδ(x0)with variance at most ˆσ2
1,0.
2:fort=0,1...T
3:xt+1=xt−ηgt
4: Flip a coinθt∈{0,1}whereP(θt=1)=pt
5: Ifθt=1then
6: Construct gt+1as an unbiased quantum estimator of ∇fδ(xt+1)with variance at most
ˆσ2
1,t+1usingUFaccording to Theorem 3.5.
7: else
8: Construct ∆gt+1as an unbiased quantum estimator of ∇fδ(xt+1)−∇fδ(xt)with variance
at mostˆσ2
2,t+1usingUFaccording to Theorem 3.5.
9:gt+1=gt+∆gt+1.
10:end for
QGFM+ can be seen as a quantum-accelerated version of GFM+ [6 ]. Unlike GFM+, which required
double loops, QGFM+ simpliﬁes the implementation by using a single loop based on the PAGE
framework [34]. Moreover, we replace all classical estimat ors with quantum estimators in lines 6
and 8 of Algorithm 2. These quantum estimators can be constru cted efﬁciently using stochastic
quantum function value oracles with a desired variance leve l, as demonstrated in Theorem 3.5. We
present the total number of queries of UFfor QGFM+ in the following theorem. We present the
total queries of UFfor QGFM+ in the following theorem.
Theorem 4.3. Under Assumption 1, by setting the parameters in Algorithm 2 as follows
η=δ/slash.l⟩ft(2d1/sl⊗sh.left2L), pt≡ǫ2/sl⊗sh.left3/slash.l⟩ftL2/sl⊗sh.left3,ˆσ2
1,t≡ǫ2/slash.l⟩ft2,andˆσ2
2,t=ǫ2/sl⊗sh.left3L4/sl⊗sh.left3d/parall⟩l.alt1xt−xt−1/parall⟩l.alt12/slash.l⟩ftδ2,
then the total queries of stochastic quantum function value oracleUFfor ﬁnding the(δ,ǫ)-Goldstein
stationary point of f(⋅)can be bounded by ˜O/par⟩nl⟩ft.alt2d3/sl⊗sh.left2/par⟩nl⟩ft.alt2L7/slash.left3
ǫ7/slash.left3+L4/slash.left3∆
δǫ7/slash.left3/par⟩nright.alt2/par⟩nright.alt2,where∆=f(x0)−f∗.
Remark 4.4.QGFM+ (Algorithm 2) speedups the GFM+ [6] for ﬁnding (δ,ǫ)-stationary point by a
factor ofLǫ−2/sl⊗sh.left3.
We can see that QGFM+ achieves the query complexity of ˜O(d3/sl⊗sh.left2ǫ−7/sl⊗sh.left3δ−1), which cannot be
achieved by any of the classical methods. Furthermore, we ob serve the applicability of our frame-
work to smooth non-convex optimization.
Remark 4.5.QGFM+ is different from the quantum speedups algorithm (Q-S PIDER) for non-
convex smooth stochastic optimization [48]: QGFM+ adjusts the variance l evel of∆gtaccording
to the difference between the current iteration point and th e previous one, while Q-SPIDER ﬁxes
the variance levels. Using the adaptive variance level and t he QGFM + framework, we can further
accelerate the Q-SPIDER for smooth non-convex optimization. In Appendix G, we propose the fast
quantum gradient method (QGM +) with the query complexity of ˜O(√
dǫ−7/sl⊗sh.left3), which improves the
one of˜O(√
dǫ−5/sl⊗sh.left2)obtained in Sidford and Zhang [48].
5 Conclusion and Future Work
In this paper, we have presented quantum algorithms for ﬁndi ng the(δ,ǫ)-Goldstein stationary point
for a non-smooth non-convex objective. Our query complexit ies demonstrate a clearly quantum
speedup over the classical methods. In future work, it would be intriguing to explore the framework
without ideal distributions which is caused by the limitati on of classical or quantum resources. It
is also interesting to ﬁnd the quantum speedups for determin istic methods [14, 28, 51] or the NS-
NC objective with constraints [38]. We are also interested i n seeing if similar strategies can be
applied to quantum online optimization with zeroth-order f eedback [25, 26, 33, 56, 58]. The query
complexity of the proposed methods still have heavy depende ncy on the dimension; it is also possible
to reduce the dimension dependency based on other quantum te chniques and design efﬁcient ﬁrst-
order quantum methods.
7Acknowledgement
Chengchang Liu thanks Luo Luo and Zongqi Wan for a valuable di scussion. The work of John C.S.
Lui was supported in part by the RGC GRF:14207721.
References
[1] Fernando GSL Brandao and Krysta M Svore. Quantum speed-u ps for solving semideﬁnite pro-
grams. In 2017 IEEE 58th Annual Symposium on Foundations of Computer S cience (FOCS) ,
pages 415–426. IEEE, 2017.
[2] Fernando GSL Brand ˜ao, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta M Svor e, and
Xiaodi Wu. Quantum sdp solvers: Large speed-ups, optimalit y, and applications to quan-
tum learning. In 46th International Colloquium on Automata, Languages, and Programming
(ICALP 2019) . Schloss-Dagstuhl-Leibniz Zentrum f ¨ur Informatik, 2019.
[3] Sergey Bravyi, David Gosset, and Robert K ¨onig. Quantum advantage with shallow circuits.
Science , 362(6412):308–311, 2018.
[4] Shouvanik Chakrabarti, Andrew M Childs, Tongyang Li, an d Xiaodi Wu. Quantum algorithms
and lower bounds for convex optimization. Quantum , 4:221, 2020.
[5] Shouvanik Chakrabarti, Andrew M Childs, Shih-Han Hung, Tongyang Li, Chunhao Wang, and
Xiaodi Wu. Quantum algorithm for estimating volumes of conv ex bodies. ACM Transactions
on Quantum Computing , 4(3):1–60, 2023.
[6] Lesi Chen, Jing Xu, and Luo Luo. Faster gradient-free alg orithms for nonsmooth nonconvex
stochastic optimization. ICML , 2023.
[7] Andrew M Childs, Jiaqi Leng, Tongyang Li, Jin-Peng Liu, a nd Chenyi Zhang. Quantum
simulation of real-space dynamics. Quantum , 6:860, 2022.
[8] Andrew M Childs, Tongyang Li, Jin-Peng Liu, Chunhao Wang , and Ruizhe Zhang. Quan-
tum algorithms for sampling log-concave distributions and estimating normalizing constants.
Advances in Neural Information Processing Systems , 35:23205–23217, 2022.
[9] Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and Adrian Weller.
Structured evolution with compact architectures for scala ble policy optimization. In Interna-
tional Conference on Machine Learning , pages 970–978. PMLR, 2018.
[10] Frank H Clarke. Optimization and nonsmooth analysis . SIAM, 1990.
[11] Arjan Cornelissen and Yassine Hamoudi. A sublinear-ti me quantum algorithm for approximat-
ing partition functions. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discre te
Algorithms (SODA) , pages 1245–1264. SIAM, 2023.
[12] Arjan Cornelissen, Yassine Hamoudi, and Soﬁene Jerbi. Near-optimal quantum algorithms for
multivariate mean estimation. In Proceedings of the 54th Annual ACM SIGACT Symposium on
Theory of Computing , pages 33–43, 2022.
[13] Ashok Cutkosky, Harsh Mehta, and Francesco Orabona. Op timal stochastic non-smooth non-
convex optimization through online-to-non-convex conver sion. In International Conference
on Machine Learning , pages 6643–6670. PMLR, 2023.
[14] Damek Davis, Dmitriy Drusvyatskiy, Yin Tat Lee, Swati P admanabhan, and Guanghao Ye. A
gradient sampling method with complexity guarantees for li pschitz functions in high and low
dimensions. Advances in neural information processing systems , 35:6692–6703, 2022.
[15] John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochas-
tic optimization. SIAM Journal on Optimization , 22(2):674–701, 2012.
[16] Darrell Dufﬁe. Dynamic asset pricing theory . Princeton University Press, 2010.
8[17] Jianqing Fan and Runze Li. Variable selection via nonco ncave penalized likelihood and its
oracle properties. Journal of the American statistical Association , 96(456):1348–1360, 2001.
[18] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang . Spider: Near-optimal non-
convex optimization via stochastic path-integrated diffe rential estimator. Advances in neural
information processing systems , 31, 2018.
[19] Ankit Garg, Robin Kothari, Praneeth Netrapalli, and Su hail Sherif. No quantum speedup over
gradient descent for non-smooth convex optimization. arXiv preprint arXiv:2010.01801 , 2020.
[20] Craig Gidney. Asymptotically efﬁcient quantum karats uba multiplication. arXiv preprint
arXiv:1904.07356 , 2019.
[21] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In
Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics ,
pages 315–323. JMLR Workshop and Conference Proceedings, 2 011.
[22] AA Goldstein. Optimization of lipschitz continuous fu nctions. Mathematical Programming ,
13:14–22, 1977.
[23] Weiyuan Gong, Chenyi Zhang, and Tongyang Li. Robustnes s of quantum algorithms for non-
convex optimization. arXiv preprint arXiv:2212.02548 , 2022.
[24] Lov Grover and Terry Rudolph. Creating superpositions that correspond to efﬁciently inte-
grable probability distributions. arXiv preprint quant-ph/0208112 , 2002.
[25] Jianhao He, Feidiao Yang, Jialin Zhang, and Lvzhou Li. Q uantum algorithm for online convex
optimization. Quantum Science and Technology , 7(2):025022, 2022.
[26] Jianhao He, Chengchang Liu, Xutong Liu, Lvzhou Li, and J ohn CS Lui. Quantum algo-
rithm for online exp-concave optimization. In Forty-ﬁrst International Conference on Machine
Learning , 2024.
[27] L Jeff Hong, Barry L Nelson, and Jie Xu. Discrete optimiz ation via simulation. Handbook of
simulation optimization , pages 9–44, 2015.
[28] Michael Jordan, Guy Kornowski, Tianyi Lin, Ohad Shamir , and Manolis Zampetakis. De-
terministic nonsmooth nonconvex optimization. In The Thirty Sixth Annual Conference on
Learning Theory , pages 4570–4597. PMLR, 2023.
[29] Sham M Kakade and Jason D Lee. Provably correct automati c sub-differentiation for qualiﬁed
programs. Advances in neural information processing systems , 31, 2018.
[30] Iordanis Kerenidis and Anupam Prakash. Quantum recomm endation systems. In 8th Inno-
vations in Theoretical Computer Science Conference (ITCS 2 017). Schloss Dagstuhl-Leibniz-
Zentrum fuer Informatik, 2017.
[31] Guy Kornowski and Ohad Shamir. Oracle complexity in non smooth nonconvex optimization.
Advances in Neural Information Processing Systems , 34:324–334, 2021.
[32] Guy Kornowski and Ohad Shamir. An algorithm with optima l dimension-dependence for zero-
order nonsmooth nonconvex stochastic optimization. arXiv preprint arXiv:2307.04504 , 2023.
[33] Tongyang Li and Ruizhe Zhang. Quantum speedups of optim izing approximately convex func-
tions with applications to logarithmic regret stochastic c onvex bandits. Advances in Neural
Information Processing Systems , 35:3152–3164, 2022.
[34] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Rich t´arik. Page: A simple and optimal
probabilistic gradient estimator for nonconvex optimizat ion. In International conference on
machine learning , pages 6286–6295. PMLR, 2021.
[35] Tianyi Lin, Zeyu Zheng, and Michael Jordan. Gradient-f ree methods for deterministic and
stochastic nonsmooth nonconvex optimization. Advances in Neural Information Processing
Systems , 2022.
9[36] Zhenwei Lin, Jingfan Xia, Qi Deng, and Luo Luo. Decentra lized gradient-free methods for
stochastic non-smooth non-convex optimization. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , 2024.
[37] Yizhou Liu, Weijie J Su, and Tongyang Li. On quantum spee dups for nonconvex optimization
via quantum tunneling walks. Quantum , 7:1030, 2023.
[38] Zhuanghua Liu, Cheng Chen, Luo Luo, and Bryan Kian Hsian g Low. Zeroth-order meth-
ods for constrained nonconvex nonsmooth stochastic optimi zation. In Forty-ﬁrst International
Conference on Machine Learning , 2024.
[39] Rahul Mazumder, Jerome H Friedman, and Trevor Hastie. S parsenet: Coordinate descent with
nonconvex penalties. Journal of the American Statistical Association , 106(495):1125–1138,
2011.
[40] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann ma-
chines. In Proceedings of the 27th international conference on machin e learning (ICML-10) ,
pages 807–814, 2010.
[41] Yurii Nesterov and Vladimir Spokoiny. Random gradient -free minimization of convex func-
tions. Foundations of Computational Mathematics , 17(2):527–566, 2017.
[42] Michael A Nielsen and Isaac L Chuang. Quantum computation and quantum information .
Cambridge university press, 2010.
[43] Damien Power. Supply chain management integration and implementation: a literature review.
Supply chain management: an International journal , 10(4):252–263, 2005.
[44] John Preskill. Quantum computing in the nisq era and bey ond. Quantum , 2:79, 2018.
[45] Mehdi Ramezani, Morteza Nikaeen, Farnaz Farman, Seyed Mahmoud Ashraﬁ, and Alireza
Bahrampour. Quantum multiplication algorithm based on the convolution theorem. Physical
Review A , 108(5):052405, 2023.
[46] Lidia Ruiz-Perez and Juan Carlos Garcia-Escartin. Qua ntum arithmetic with the quantum
fourier transform. Quantum Information Processing , 16:1–14, 2017.
[47] Emre Sahinoglu and Shahin Shahrampour. An online optim ization perspective on ﬁrst-order
and zero-order decentralized nonsmooth nonconvex stochas tic optimization. In Forty-ﬁrst In-
ternational Conference on Machine Learning , 2024.
[48] Aaron Sidford and Chenyi Zhang. Quantum speedups for st ochastic optimization. In Thirty-
seventh Conference on Neural Information Processing Syste ms, 2023.
[49] Hyung Ju Suh, Max Simchowitz, Kaiqing Zhang, and Russ Te drake. Do differentiable simu-
lators give better policy gradients? In International Conference on Machine Learning , pages
20668–20696. PMLR, 2022.
[50] Lai Tian and Anthony Man-Cho So. On the hardness of compu ting near-approximate stationary
points of clarke regular nonsmooth nonconvex problems and c ertain dc programs. In ICML
Workshop on Beyond First-Order Methods in ML Systems , 2021.
[51] Lai Tian and Anthony Man-Cho So. No dimension-free dete rministic algorithm computes
approximate stationarities of lipschitzians. Mathematical Programming , pages 1–24, 2024.
[52] Lai Tian, Kaiwen Zhou, and Anthony Man-Cho So. On the ﬁni te-time complexity and prac-
tical computation of approximate stationarity concepts of lipschitz functions. In International
Conference on Machine Learning , pages 21360–21379. PMLR, 2022.
[53] Joran Van Apeldoorn and Andr ´as Gily ´en. Improvements in quantum sdp-solving with appli-
cations. arXiv preprint arXiv:1804.05058 , 2018.
[54] Joran Van Apeldoorn, Andr ´as Gily ´en, Sander Gribling, and Ronald de Wolf. Quantum sdp-
solvers: Better upper and lower bounds. In 2017 IEEE 58th Annual Symposium on Foundations
of Computer Science (FOCS) , pages 403–414. IEEE, 2017.
10[55] Joran van Apeldoorn, Andr ´as Gily ´en, Sander Gribling, and Ronald de Wolf. Convex optimiza-
tion using quantum oracles. Quantum , 4:220, 2020.
[56] Zongqi Wan, Zhijie Zhang, Tongyang Li, Jialin Zhang, an d Xiaoming Sun. Quantum multi-
armed bandits and stochastic linear bandits enjoy logarith mic regrets. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , 2023.
[57] Daochen Wang, Aarthi Sundaram, Robin Kothari, Ashish K apoor, and Martin Roetteler. Quan-
tum algorithms for reinforcement learning with a generativ e model. In International Confer-
ence on Machine Learning , pages 10916–10926. PMLR, 2021.
[58] Yulian Wu, Chaowen Guan, Vaneet Aggarwal, and Di Wang. Q uantum heavy-tailed bandits.
arXiv preprint arXiv:2301.09680 , 2023.
[59] Farzad Youseﬁan, Angelia Nedi ´c, and Uday V Shanbhag. On stochastic gradient and subgra-
dient methods with adaptive steplength sequences. Automatica , 48(1):56–67, 2012.
[60] Chenyi Zhang and Tongyang Li. Quantum lower bounds for ﬁ nding stationary points of non-
convex functions. In International Conference on Machine Learning , pages 41268–41299.
PMLR, 2023.
[61] Chenyi Zhang, Jiaqi Leng, and Tongyang Li. Quantum algo rithms for escaping from saddle
points. Quantum , 5:529, 2021.
[62] Cun-Hui Zhang. Nearly unbiased variable selection und er minimax concave penalty. Ann.
Statist. , 38(1):894–942, 2010.
[63] Jingzhao Zhang, Hongzhou Lin, Stefanie Jegelka, Suvri t Sra, and Ali Jadbabaie. Complexity
of ﬁnding stationary points of nonconvex nonsmooth functio ns. In International Conference
on Machine Learning , pages 11173–11182. PMLR, 2020.
[64] Yexin Zhang, Chenyi Zhang, Cong Fang, Liwei Wang, and To ngyang Li. Quantum algorithms
and lower bounds for ﬁnite-sum optimization. arXiv preprint arXiv:2406.03006 , 2024.
11A Explicit Construction of Quantum Sampling Oracles
In this section, we propose a novel quantum process to realiz e quantum sampling oracle Ow,ξ∶
/divid⟩s.alt00⟩/leftfootline→∑w,ξ/radical.alt1
Pr[w,ξ]/divid⟩s.alt0w,ξ⟩/divid⟩s.alt0ψw,ξ⟩with uniform distribution, where ξis uniformly distributed on
{0,⋯,N−1}andwis sampled uniformly on a discrete unit sphere.
The uniform distribution of ξin the quantum state can be constructed using Hadamard gates . The
construction of a uniform distribution on a discrete unit sp here is more tricky. Classically, such a
distribution can be constructed by sampling each coordinat e from a standard Gaussian distribution
and then normalizing the vector to have unit length by dividi ng by its norm. However, preparing
a superposition state with Gaussian amplitudes is not trivi al because the Gaussian distribution is
deﬁned in an inﬁnite interval. Constructing such a state wit h Grover’s method [24] will lead to some
issues in dealing with the domain and normalization of the me asurement probability. Instead, here,
starting with the simple uniform superposition state, we us e a central limit theorem to construct the
standard Gaussian distribution.
The overall quantum algorithm proceeds as follows:
Step1. Prepare the initial quantum state /divid⟩s.alt00⟩⊗m1⊗/divid⟩s.alt00⟩⊗(dm2)⊗/divid⟩s.alt00⟩⊗(dlogm2). Setk=0. Apply
H⊗m1⊗H⊗dm2⊗I, that is, apply Hadamard gates to the ﬁrst and second registe rs. Here,
m1,m2∈N+.
Step2. Deﬁneh∶{0,1}m2→R,h(j)=2√m2/par⟩nl⟩ft.alt2j1+j2+⋅⋅⋅+jm2√m2−0.5/par⟩nright.alt2. ApplyI⊗U⊗d
h, whereUh,
the unitary transform corresponding to h, maps the quantum state /divid⟩s.alt0j⟩/divid⟩s.alt00⟩to the quantum
state/divid⟩s.alt0j⟩/divid⟩s.alt00+h(j)⟩. Thek-thUhtakes thek-thm2qubits in the second register as input, and
the output is stored in the k-thlogm2qubits in the third register, for all k∈{0,...,d−1}.
Step3. Consider the third register as a d-dimension vector w′, withlogm2qubits to store each
coordinate w′
k. ApplyUnorm∶/divid⟩s.alt0w⟩/divid⟩s.alt00+/parall⟩l.alt1w/parall⟩l.alt1⟩, the result is stored in an additional ancillary
register. Then normalize w′to have unit length by dividing by /parall⟩l.alt1w/parall⟩l.alt1in each component.
Analysis and Correctness. In Step1, it starts with the quantum state /divid⟩s.alt00⟩⊗m1⊗/divid⟩s.alt00⟩⊗(dm2)⊗
/divid⟩s.alt00⟩⊗(dlogm2), where all the registers are initialized to 0. The ﬁrst register is prepared to create the
superposition of ξ, and the second and third registers are prepared for creatin g the superposition of
w. We apply Hadamard gates to the ﬁrst and the second registers , to obtain a uniform superposition
of computation basis, which gives
1√
2m1dm22m1−1
/summation.disp
i=0/divid⟩s.alt0i⟩⊗1
/summation.disp
j(0)
1,...,j(0)
m2=0/divid⟩s.alt2j(0)
1...j(0)
m2/uni27E9.alt2⊗⋅⋅⋅⊗1
/summation.disp
j(d−1)
1,...,j(d−1)
m2=0/divid⟩s.alt2j(d−1)
1...j(d−1)
m2/uni27E9.alt2⊗/divid⟩s.alt00⟩.
Letm1=⌈logN⌉, and we relabel the ﬁrst register to obtain
1√
2m1dm2/summation.disp
ξ/divid⟩s.alt0ξ⟩⊗1
/summation.disp
j(0)
1,...,j(0)
m2=0/divid⟩s.alt2j(0)
1...j(0)
m2/uni27E9.alt2⊗⋅⋅⋅⊗1
/summation.disp
j(d−1)
1,...,j(d−1)
m2=0/divid⟩s.alt2j(d−1)
1...j(d−1)
m2/uni27E9.alt2⊗/divid⟩s.alt00⟩.
After Step 2, as each Uhoperates in the same manner, we take one as an example,
1√
2m1dm2/summation.disp
ξ/divid⟩s.alt0ξ⟩⊗⋅⋅⋅1
/summation.disp
j1,...,jm2=0/divid⟩s.alt0j1j2...jm2⟩.../divid⟩s.alt42√m2/par⟩nl⟩ft.alt4j1+j2+⋅⋅⋅+jm2√m2−0.5/par⟩nright.alt4/uni27E9.alt4....
Once measured, j1,...,jm2are independent and identically distributed random variab les with mean
0.5and variance 0.25. By the central limit theorem, the average of {ji}m2
i=1approximates the Gaus-
sian distribution when m2is large. We obtain the standard Gaussian distribution afte r changing and
scaling the average of them. We denote w′
k≜2√m2/par⟩nl⟩ft.alt3j(k)
1+j(k)
2+⋅⋅⋅+j(k)
m2√m2−0.5/par⟩nright.alt3, then the measurement
results of∑j(k)/divid⟩s.alt0w′
k⟩follow the distribution of N(0,1).
After Step 3, the vector in the third register is mapped to the unit sphere and the measurement result
follows the uniform distribution on a discrete unit sphere. Rearrange the order of the registers,
12denote all the garbage qubits as /divid⟩s.alt0ψw,ξ⟩, we obtain
/summation.disp
w,ξ/radical.alt1
Pr[w,ξ]/divid⟩s.alt0w,ξ⟩/divid⟩s.alt0ψw,ξ⟩,
wherew=w′/slash.l⟩ft/parall⟩l.alt1w′/parall⟩l.alt1is uniformly distributed on a discrete unit sphere and ξis uniformly distributed
on{0,⋯,N−1}.
This realizes the discrete version of the quantum sample ora cleOw,ξwith uniform distribution.
Remark A.1.In the ideal scenario where we do not need to limit the number o f qubits, allowing
m1andm2to be sufﬁciently large, we can achieve ∫w∈Sd−1/radical.alt1
µ(w)dw/divid⟩s.alt0w⟩as needed in Proposi-
tion 3.1. Speciﬁcally, our process requires m1+m2×dHadamard gates,O(dm2)fundamental
arithmetic operations, and 1calls to the norm circuit. Here, m1=⌈logN⌉andm2is the number
of random variables that are used to approximate the Gaussia n distribution. Note that many gates
here can be performed in parallel, for example, all the Hgates can be performed simultaneously,
and the sum of m2qubits can be implemented in a circuit of O(logm2)depth. The total depth
complexity isO(logm2+log(dlogm2)), which indicates that the depth of the circuit will remain
small when m2increases. This ensures that our construction is feasible e ven in the context of the
NISQ quantum computer [3, 44], which only supports low-dept h circuits. In particular, this proce-
dure does not require querying UF, thus not increasing the query complexity of UF. Nevertheless,
it is still important to give such an explicit and efﬁcient co nstruction to ensure that the quantum state
preparation will not ruin the quantum advantage for the over all time complexity.
Remark A.2.Ifξis sampled from distribution other than uniform distributi on, there still exist quan-
tum techniques which can construct such quantum sample orac le. When detailed classical sampling
circuits are known, we can make it reversible by replacing ga tes in the classical circuits with re-
versible quantum gates such as the Toffoli gate [42], to obta in a quantum circuit [57]. When there
is only a black box access to the classical circuit, we discus s the construction by cases. For the
continuous case where the distribution is described by a pro bability density function, we can use the
Grover’s method [24], which requires an efﬁcient integrati ng circuit. For the discrete case, we can
extend the Grover’s method by using the QRAM data structure. The complexity of constructing a
QRAM data structure is linearly dependent on the size of the s ample space. Once it is constructed,
the complexity of generating the quantum sample oracle depe nds only logarithmly on the size of
sample space [30].
B The Proof of Lemma 3.2
Proof. First, we claim that a unitary operator Ug,δfor computing the stochastic gradient estimator
gδ(⋅;w,ξ)can be efﬁciently constructed. More precisely, we can const ruct
Ug,δ∶/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt0b⟩/leftfootline→/divid⟩s.alt0x⟩⊗/divid⟩s.alt0gδ(x;w,ξ)⟩⊗/divid⟩s.alt0ψw,ξ⟩ (5)
with2queries to UF. Now we assume the access to Ug,δand the description of its construction
will be deferred to the end of this proof. Next we show how this can lead to a quantum δ-estimated
stochastic gradient oracle Og,δas deﬁned in Deﬁnition 3.3. Given initial state /divid⟩s.alt0x⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩, we
can prepare the desired quantum state by ﬁrst applying the qu antum sampling oracle Ow,ξand then
Ug,δas follows:
Ug,δ⋅(I⊗Oξ,w⊗I)/divid⟩s.alt0x⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩=Ug,δ(/divid⟩s.alt0x⟩⊗/summation.disp
ξ,w/radical.alt1
p(ξ,w)/divid⟩s.alt0ξ,w⟩⊗/divid⟩s.alt00⟩)
=/summation.disp
ξ,w/radical.alt1
p(ξ,w)Ug,δ(/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ,w⟩⊗/divid⟩s.alt00⟩)
=/divid⟩s.alt0x⟩⊗/summation.disp
ξ,w/radical.alt1
p(ξ,w)/divid⟩s.alt0gδ(x;w,ξ)⟩⊗/divid⟩s.alt0ψw,ξ⟩.
Next we ﬁnish the proof by presenting how to implement Ug,δwith two queries to UF. Sinceδ
anddare ﬁxed and known beforehand, we can easily construct the fo llowing three operators via the
quantum unitary implementations of the corresponding clas sical arithmetic operations:
A+∶/divid⟩s.alt0x⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt00⟩/leftfootline→/divid⟩s.alt0x⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt0x+δw⟩,A−∶/divid⟩s.alt0x⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt00⟩/leftfootline→/divid⟩s.alt0x⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt0x−δw⟩,
13sub∶/divid⟩s.alt0a⟩⊗/divid⟩s.alt0b⟩/leftfootline→/divid⟩s.alt0a⟩⊗/divid⟩s.alt0a−b⟩, andFmul∶/divid⟩s.alt0c⟩/leftfootline→/divid⟩s.alt3δ
2dc/uni27E9.alt3.
LetF′(x;w,ξ)≜δ
2d(F(x+δw;ξ)−F(x−δw;ξ)). Then we construct a unitary Das follows:
D∶/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩
↦(a)/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt0x−δw⟩⊗/divid⟩s.alt00⟩
↦(b)/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt0F(x−δw;ξ)⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt0x−δw⟩⊗/divid⟩s.alt00⟩
↦(c)/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt0F(x−δw;ξ)⟩⊗/divid⟩s.alt0F(x+δw;ξ)⟩⊗/divid⟩s.alt0x−δw⟩⊗/divid⟩s.alt0x+δw⟩
↦(d)/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt0F′(x;w,ξ)⟩⊗/divid⟩s.alt0F(x+δw;ξ)⟩⊗/divid⟩s.alt0x−δw⟩⊗/divid⟩s.alt0x+δw⟩
=/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt0F′(x;w,ξ)⟩⊗/divid⟩s.alt1ψ′
w,ξ/uni27E9.alt1,(6)
where(a)follows by applying A−on the ﬁrst, third and sixth registers; (b)uses the quantum
stochastic function value oracle UFon the second, fourth and sixth registers; (c)usesA+andUF
in a way similar to steps (a)and(b);(d)appliessubon the fourth and ﬁfth registers, and then
appliesFmul on the fourth register. It is easy to see that this unitary Duses only 2queries to UF.
For any input state /divid⟩s.alt0x⟩⊗/divid⟩s.alt0w,ξ⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩⊗d, applyDto obtain
/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt0F′(x;w,ξ)⟩⊗/divid⟩s.alt0ψ′
w,ξ⟩⊗/divid⟩s.alt00⟩⊗d
=/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w1,⋯,wd⟩⊗/divid⟩s.alt0F′(x;w,ξ)⟩⊗/divid⟩s.alt0ψ′
w,ξ⟩⊗/divid⟩s.alt00⟩⊗d(7)
Next we will utilize quantum multiplication operator Umul∶/divid⟩s.alt0a⟩⊗/divid⟩s.alt0b⟩⊗/divid⟩s.alt0c⟩/leftrightline→/divid⟩s.alt0a⟩⊗/divid⟩s.alt0b⟩⊗/divid⟩s.alt0c⊕ab⟩.
This can be implemented by the quantization of classical mul tiplication algorithms, whose details
can be found in [20, 45, 46].
Applying Umulto each/divid⟩s.alt0wi,F′⟩⊗/divid⟩s.alt00⟩for alli∈[d]yields
/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w1,⋯,wd⟩⊗/divid⟩s.alt0F′(x+δw;ξ)⟩⊗/divid⟩s.alt0ψ′
w,ξ⟩⊗/divid⟩s.alt0F′(x+δw;ξ)w1,⋯,F′(x+δw;ξ)wd⟩
=/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w1,⋯,wd⟩⊗/divid⟩s.alt0F′(x+δw;ξ)⟩⊗/divid⟩s.alt0ψ′
w,ξ⟩⊗/divid⟩s.alt0F′(x+δw;ξ)w⟩
=/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w1,⋯,wd⟩⊗/divid⟩s.alt0F′(x+δw;ξ)⟩⊗/divid⟩s.alt0ψ′
w,ξ⟩⊗/divid⟩s.alt0gδ(x;w,ξ)⟩
=/divid⟩s.alt0x⟩⊗/divid⟩s.alt0ψw,ξ⟩⊗/divid⟩s.alt0gδ(x;w,ξ)⟩.
(8)
By swapping the last two quantum registers, we obtain /divid⟩s.alt0x⟩⊗/divid⟩s.alt0g(x;w,ξ)⟩⊗/divid⟩s.alt0ψw,ξ⟩.Hence,Ug,δcan
be implemented with two queries to UF.
C The Proof of Corollary 3.4
Proof. Analogous to eq. (5), we claim that the following unitary Vg,δcan be implemented with 4
queries to UF:
Vg,δ∶/divid⟩s.alt0x⟩⊗/divid⟩s.alt0y⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt0b⟩/leftfootline→/divid⟩s.alt0x⟩⊗/divid⟩s.alt0y⟩⊗/divid⟩s.alt0gδ(x;w,ξ)−gδ(y;w,ξ)⟩⊗/divid⟩s.alt0ψw,ξ⟩.
With access to Vg,δandOg,δ, we can construct O∆gδas
O∆gδ=Vg,δ⋅(I⊗I⊗Oξ,w⊗I).
Next, to implement Vg,δwith 4 queries to UF, we can ﬁrst follow the steps in eq. (6), eq. (7) and
eq. (8) to get a unitary that performs the mapping below
/divid⟩s.alt0x⟩⊗/divid⟩s.alt0y⟩⊗/divid⟩s.alt0ξ⟩⊗/divid⟩s.alt0w⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩⊗/divid⟩s.alt00⟩/leftfootline→/divid⟩s.alt0x⟩⊗/divid⟩s.alt0y⟩⊗/divid⟩s.alt0ψw,ξ⟩⊗/divid⟩s.alt0gδ(x;w,ξ)⟩⊗/divid⟩s.alt0gδ(y;w,ξ)⟩.
Then applying suband aSWAP gate to the output above yields
/divid⟩s.alt0x⟩⊗/divid⟩s.alt0y⟩⊗/summation.disp
ξ,w/radical.alt1
Pr[w,ξ]/divid⟩s.alt0gδ(x;w,ξ)−gδ(y;w,ξ)⟩⊗/divid⟩s.alt0ψw,ξ⟩.
14D The Proof of Theorem 3.5
Before we present the proof, we ﬁrst introduce the results fo r the quantum mean estimation by Sid-
ford and Zhang [48].
Theorem D.1 ([48, Theorem 4]) .For a random variable Xwith bounded variance such that
Var[X]≤ˆL2, there exists an algorithm that can output an unbiased estim atorˆµofµ=E[X]
satisfying E/brack⟩tl⟩ft.alt1/parall⟩l.alt1ˆµ−µ/parall⟩l.alt12/brack⟩tright.alt≤ˆσ2using an expected ˜O(ˆL√
dˆσ−1)queries of quantum sampling oracle
OXas deﬁned in Deﬁnition 3.2.
Proof. According to Proposition 3.1, the quantum δ-estimated stochastic gradient oracle given the
inputxsatisﬁes that
E[gδ]=∇fδ(x)and Var[gδ]≤16√
2πdL2.
Using Theorem D.1 with ˆL=√
dL, it requires only ˜O(dLˆσ−1
1)queries of Ogδto construct the
quantum estimator ˆgsuch that E/brack⟩tl⟩ft.alt1/parall⟩l.alt1ˆg−∇fδ(x)/parall⟩l.alt12/brack⟩tright.alt≤ˆσ2
1. According to Lemma 3.2, we can construct
eachOgδbyO(1)-queries of UF. Thus, it only requires ˜O(dLˆσ−1
1)queries of UFto construct the
mini-batch quantum estimator ˆg.
Similarly, since we can construct the quantum estimator ∆gδbyO(1)-queries of UFaccording to
Corollary 3.4, with the following properties
E[∆gδ]=∇fδ(x)−∇fδ(y)and Var[∆gδ]≤E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∆gδ/parall⟩l.alt12/brack⟩tright.alt≤d2L2δ−2/parall⟩l.alt1x−y/parall⟩l.alt12,
then, using Theorem D.1 with ˆL=dLδ−1/parall⟩l.alt1x−y/parall⟩l.alt1directly leads to second statement.
E The Proof of Theorem 4.1
Proof. According to the variance level we set, gtsatisﬁes that
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1gt−∇fδ(xt)/parall⟩l.alt12/brack⟩tright.alt≤ǫ2
2.
According to Proposition 2.1, fδ(⋅)is a nonconvex function, with (√
dLδ−1)-Lipschitz gradient,
which implies that
fδ(xt+1)≤fδ(xt)+⟨∇fδ(x),xt+1−xt⟩+√
dLδ−1
2/parall⟩l.alt1xt+1−xt/parall⟩l.alt12
=fδ(xt)−η⟨∇fδ(x),gt⟩+√
dLδ−1
2/parall⟩l.alt1xt+1−xt/parall⟩l.alt12
Taking expectation on both sides of the above inequality, we have
fδ(xt+1)≤fδ(xt)−η/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12+η2√
dLδ−1
2E/brack⟩tl⟩ft.alt1/parall⟩l.alt1gt/parall⟩l.alt12/brack⟩tright.alt
≤fδ(xt)−η/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12+η2√
dLδ−1/par⟩nl⟩ft.alt1/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12+E/brack⟩tl⟩ft.alt1/parall⟩l.alt1gt−∇δ(xt)/parall⟩l.alt12/brack⟩tright.alt/par⟩nright.alt1
≤fδ(xt)−/par⟩nl⟩ft.alt2η−√
dLδ−1η2/par⟩nright.alt2/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12+√
dLδ−1η2⋅ǫ2
2,
We letη=δ
2√
dL, then it holds that
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12/brack⟩tright.alt≤2√
dLδ−1(fδ(xt)−fδ(xt+1))+ǫ2
4.
Summing up the above inequality, we have
E/brack⟩tl⟩ft.alt4∑T
t=0/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12
T/brack⟩tright.alt4≤2√
dLδ−1(fδ(x0)−f∗
δ)
T+ǫ2
4≤2√
dLδ−1(f(x0)−f∗+2δL)
T+ǫ2
4.
15By setting
T=/uni2308.alt22ǫ−2(4√
dL2+2√
dLδ−1∆)/uni2309.alt2,
and choosing xoutrandomly from{x0,⋯,xT−1}, we have
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∇fδ(xout)/parall⟩l.alt12/brack⟩tright.alt≤1
TE/brack⟩tl⟩ft.alt4T
/summation.disp
i=1/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12/brack⟩tright.alt4≤ǫ2
4+ǫ2
2≤ǫ2.
Using Theorem 3.5, we require
b=˜O(dLǫ−1),
to achieve the desired variance level. Thus the total quantu m query of UFcan be bounded by
b⋅T=˜O/par⟩nl⟩ft.alt4d3/sl⊗sh.left2/par⟩nl⟩ft.alt4L∆
ǫ3δ+L2
ǫ3/par⟩nright.alt4/par⟩nright.alt4.
F The Proof of Theorem 4.3
Proof. We denoteLδ≜√
dL
δ. We also denote ˆgt+1as the unbiased estimator of ∇fδ(xt+1)we
have constructed in line 6 and ∆gt+1as the unbiased estimator of ∇fδ(xt+1)−∇fδ(xt)we have
constructed in line 8. We can see that gt+1is equivalent to
gt+1=/brac⟩l⟩ft.alt4ˆgt+1 with probability pt
gt+∆gt+1with probability 1−pt.
According to the variance level we set in Theorem 4.3, we have
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1ˆgt+1−∇fδ(xt+1)/parall⟩l.alt12/brack⟩tright.alt≤ˆσ2
1,t+1=ǫ2
2,
and
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∆gt+1−(∇fδ(xt+1)−∇fδ(xt))/parall⟩l.alt12/brack⟩tright.alt≤ˆσ2
2,t+1=ǫ2/sl⊗sh.left3/parall⟩l.alt1xt+1−xt/parall⟩l.alt12L4/sl⊗sh.left3d
δ2.
According to Proposition 2.1, ∇fδ(⋅)isLδ-Lipschitz continuous, which means
fδ(xt+1)≤fδ(xt)+⟨∇fδ(xt),xt+1−xt⟩+Lδ
2/parall⟩l.alt1xt+1−xt/parall⟩l.alt12
=fδ(xt)+⟨∇fδ(xt)−gt,xt+1−xt⟩+⟨gt,xt+1−xt⟩+Lδ
2/parall⟩l.alt1xt+1−xt/parall⟩l.alt12
≤fδ(xt)−η
2/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12−η
2/parall⟩l.alt1gt−∇fδ(xt)/parall⟩l.alt12−/par⟩nl⟩ft.alt31
2η−Lδ
2/par⟩nright.alt3/parall⟩l.alt1xt+1−xt/parall⟩l.alt12.(9)
On the other hand, we track the variance of gt+1by
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1gt+1−∇fδ(xt+1)/parall⟩l.alt12/brack⟩tright.alt
=ptE/brack⟩tl⟩ft.alt1/parall⟩l.alt1ˆgt+1−∇fδ(xt+1)/parall⟩l.alt12/brack⟩tright.alt
+(1−pt)E/brack⟩tl⟩ft.alt1/parall⟩l.alt1gt−∇fδ(xt)+(∆gt+1−(∇fδ(xt+1)−∇fδ(xt)))/parall⟩l.alt12/brack⟩tright.alt
=ptǫ2+(1−pt)/parall⟩l.alt1gt−∇fδ(xt)/parall⟩l.alt12+(1−pt)⋅L4/sl⊗sh.left3ǫ2/sl⊗sh.left3d
δ2/parall⟩l.alt1xt+1−xt/parall⟩l.alt12.(10)
16We havept≡pand can denote Φt≜fδ(xt)−f∗+η
2p/parall⟩l.alt1gt−∇fδ(xt)/parall⟩l.alt12. Combining eq. (9) and
eq. (10), we have
E[Φt+1]=E/brack⟩tl⟩ft.alt3fδ(xt+1)+η
2p/parall⟩l.alt1gt+1−∇f(xt+1)/parall⟩l.alt12/brack⟩tright.alt3
≤E/brack⟩tl⟩ft.alt3fδ(xt)−η
2/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12−η
2/parall⟩l.alt1gt−∇fδ(xt)/parall⟩l.alt12−/par⟩nl⟩ft.alt31
2η−Lδ
2/par⟩nright.alt3/parall⟩l.alt1xt+1−xt/parall⟩l.alt12/brack⟩tright.alt3
+η
2pE/brack⟩tl⟩ft.alt4pǫ2+(1−p)/parall⟩l.alt1gt−∇fδ(xt)/parall⟩l.alt12+(1−p)L4/sl⊗sh.left3dǫ2/sl⊗sh.left3
δ2/parall⟩l.alt1xt+1−xt/parall⟩l.alt12/brack⟩tright.alt4
≤E[Φt]−η
2/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12−/par⟩nl⟩ft.alt41
2η−Lδ
2−η(1−p)
p⋅/par⟩nl⟩ft.alt4L4/sl⊗sh.left3dǫ2/sl⊗sh.left3
δ2/par⟩nright.alt4/par⟩nright.alt4
/dcurlyl⟩ft/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlymid/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlyright
A/parall⟩l.alt1xt+1−xt/parall⟩l.alt12+ηǫ2
2.
(11)
We have chosen
η=1
2Lδandp=ǫ2/sl⊗sh.left3
L2/sl⊗sh.left3, (12)
such that
A≥√
dL
2δ−δ
2√
dL⋅L2/sl⊗sh.left3
ǫ2/sl⊗sh.left3⋅L4/sl⊗sh.left3dǫ2/sl⊗sh.left3
δ2=0.
Then, eq. (11) implies
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∇fδ(xt)/parall⟩l.alt12/brack⟩tright.alt≤2
ηE[Φt−Φt+1]+ǫ2. (13)
Since it holds that
2
ηE[Φ0−ΦT]≤2
ηE/brack⟩tl⟩ft.alt3fδ(x0)−f∗
δ+η
2p/parall⟩l.alt1ˆg0−∇f(x0)/parall⟩l.alt12/brack⟩tright.alt3
≤2
ηE/brack⟩tl⟩ft.alt3f(x0)−f∗+2δL+η
2p/parall⟩l.alt1ˆg0−∇f(x0)/parall⟩l.alt12/brack⟩tright.alt3
≤2
η(∆+2δL)+1
pǫ2,
summing up eq. (13) from t=0,⋯,T−1, we have
1
TT−1
/summation.disp
i=0E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∇fδ(xi)/parall⟩l.alt12/brack⟩tright.alt≤2
ηTE[Φ0−ΦT]+ǫ2
2.
By choosing
T=/uni2308.alt38Lδǫ−2(∆+2δL)+4
p/uni2309.alt3, (14)
we have
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∇fδ(xout)/parall⟩l.alt12/brack⟩tright.alt=1
TT−1
/summation.disp
i=0E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∇fδ(xi)/parall⟩l.alt12/brack⟩tright.alt≤2
ηTE[Φ0−ΦT]+ǫ2
2≤ǫ2
4+ǫ2
4+ǫ2
2=ǫ2.
Using Theorem 3.5, the expectation queries of UFto construct ˆgtis
b0=˜O/par⟩nl⟩ft.alt1dLˆσ−1
1,t/par⟩nright.alt1=˜O/par⟩nl⟩ft.alt1dLǫ−1/par⟩nright.alt1,
and the expectation queries of UFto construct ∆gtis
b1=˜O/par⟩nl⟩ft.alt1d3/sl⊗sh.left2L/parall⟩l.alt1xt−1−xt/parall⟩l.alt1ˆσ−1
2,tδ−1/par⟩nright.alt1=˜O/par⟩nl⟩ft.alt1dL1/sl⊗sh.left3ǫ−1/sl⊗sh.left3/par⟩nright.alt1.
Thus, the total quantum queries of UFfor ﬁnding the(δ,ǫ)-stationary point of f(⋅)can be bounded
by
˜O(T(b0p+b1(1−p)))=˜O/par⟩nl⟩ft.alt2√
dLǫ2(∆+2δL)⋅/par⟩nl⟩ft.alt1dLǫ−1L−2/sl⊗sh.left3ǫ2/sl⊗sh.left3+dL1/sl⊗sh.left3ǫ−1/sl⊗sh.left3/par⟩nright.alt1/par⟩nright.alt2
=˜O/par⟩nl⟩ft.alt4d3/sl⊗sh.left2/par⟩nl⟩ft.alt4L4/sl⊗sh.left3∆
ǫ7/sl⊗sh.left3δ+L7/sl⊗sh.left3
ǫ7/sl⊗sh.left3/par⟩nright.alt4/par⟩nright.alt4,
which ﬁnishes the proof.
17Algorithm 3 Fast Quantum Gradient Method (QGM+)
1:Construct g0as an unbiased estimator of ∇f(x0)with variance at most ˆσ2
1,0.
2:fort=0,1...T
3:xt+1=xt−ηgt
4: Flip a coinθt∈{0,1}whereP(θt=1)=pt
5: Ifθt=1then
6: Construct gt+1as an unbiased quantum estimator of ∇f(xt+1)with variance at most
ˆσ2
1,t+1.
7: else
8: Construct ∆gt+1as an unbiased quantum estimator of ∇f(xt+1)−∇f(xt)with variance
at mostˆσ2
2,t+1.
9:gt+1=gt+∆gt+1.
10:end for
G Improved Results for Quantum Stochastic Smooth Non-conve x
Optimization
Sidford and Zhang [48] introduced Q-SPIDER for smooth non-convex optimization, with the query
complexity of ˜O(d1/sl⊗sh.left2ǫ−5/sl⊗sh.left2)in the quantum stochastic gradient oracle. Using the same fr amework
as QGFM+, we propose the fast quantum gradient method (QGM+) , which further improves the
query complexity of Q-SPIDER.
We present QGM+ in Algorithm 3. The main difference between Q GFM+ and QGM+ is that QGM
constructs estimators for ∇f(x)and∇f(x)−∇f(y)instead of their smoothed surrogates in line
6 and line 8 by using the quantum stochastic gradient oracle d irectly [48, Deﬁnition 4]. We present
the setting for Q-SPIDER as follows as being self-contained .
Assumption 2 ([48, Setting of Theorem 7]) .We assume that we are able to access the quantum
stochastic oracle that outputs ∇F(⋅;ξ)which is a stochastic gradient of f(⋅)that satisﬁes
Eξ[∇F(x;ξ)]=∇f(x),Eξ[/parall⟩l.alt1∇F(x;ξ)−∇f(x)/parall⟩l.alt1]≤σ2,
and
Eξ/brack⟩tl⟩ft.alt1/parall⟩l.alt1∇F(x;ξ)−∇F(y;ξ)/parall⟩l.alt12/brack⟩tright.alt≤l2/parall⟩l.alt1x−y/parall⟩l.alt12.
We also present the deﬁnition of the ǫ-stationary point of a smooth function.
Deﬁnition G.1. We sayxis anǫ-stationary point of a smooth function f(⋅), if it satisﬁes/parall⟩l.alt1∇f(x)/parall⟩l.alt1≤
ǫ.
We present the query complexity of QGM+ in the following theo rem.
Theorem G.1. Under the same setting of [48, Theorem 7] for Q-SPIDER, QGM+ ( Algorithm 3)
ﬁnds theǫ-stationary point of f(⋅)using an expected ˜O(√
dǫ−7/sl⊗sh.left3)queries of quantum stochastic
gradient oracle by setting
η=1
2l, pt≡ǫ2/sl⊗sh.left3σ−2/sl⊗sh.left3,ˆσ2
1,t≡ǫ2
2,andˆσ2
2,t=l2ǫ2/sl⊗sh.left3/parall⟩l.alt1xt−xt−1/parall⟩l.alt1
σ2/sl⊗sh.left3.
Proof. According to the variance level we set in Theorem G.1 We have
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1ˆgt+1−∇f(xt+1)/parall⟩l.alt12/brack⟩tright.alt≤ˆσ2
1,t+1=ǫ2
2,
and
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∆gt+1−(∇f(xt+1)−∇f(xt))/parall⟩l.alt12/brack⟩tright.alt≤ˆσ2
2,t+1=l2ǫ2/sl⊗sh.left3
σ2/sl⊗sh.left3/parall⟩l.alt1x−y/parall⟩l.alt12
18f(xt+1)≤f(xt)+⟨∇f(xt),xt+1−xt⟩+l
2/parall⟩l.alt1xt+1−xt/parall⟩l.alt12
=f(xt)+⟨∇f(xt)−gt,xt+1−xt⟩+⟨gt,xt+1−xt⟩+l
2/parall⟩l.alt1xt+1−xt/parall⟩l.alt12
≤f(xt)−η
2/parall⟩l.alt1∇f(xt)/parall⟩l.alt12−η
2/parall⟩l.alt1gt−∇f(xt)/parall⟩l.alt12−/par⟩nl⟩ft.alt31
2η−l
2/par⟩nright.alt3/parall⟩l.alt1xt+1−xt/parall⟩l.alt12.(15)
The variance of gt+1can be traced by
E/brack⟩tl⟩ft.alt1/parall⟩l.alt1gt+1−∇f(xt+1)/parall⟩l.alt12/brack⟩tright.alt
=ptE/brack⟩tl⟩ft.alt1/parall⟩l.alt1ˆgt+1−∇f(xt+1)/parall⟩l.alt12/brack⟩tright.alt
+(1−pt)E/brack⟩tl⟩ft.alt1/parall⟩l.alt1gt−∇f(xt)+(∆gt+1−(∇f(xt+1)−∇f(xt)))/parall⟩l.alt12/brack⟩tright.alt
=ptǫ2+(1−pt)/parall⟩l.alt1gt−∇f(xt)/parall⟩l.alt12+(1−pt)l2ǫ2/sl⊗sh.left3
σ2/sl⊗sh.left3⋅/parall⟩l.alt1xt+1−xt/parall⟩l.alt12.(16)
We letpt≡pand denote Φt≜f(xt)−f∗+η
2p/parall⟩l.alt1gt−∇f(xt)/parall⟩l.alt12. Combining eq. (15) and eq. (16),
we have
E[Φt+1]=E/brack⟩tl⟩ft.alt3f(xt+1)+η
2p/parall⟩l.alt1gt+1−∇f(xt+1)/parall⟩l.alt12/brack⟩tright.alt3
≤E/brack⟩tl⟩ft.alt3f(xt)−η
2/parall⟩l.alt1∇f(xt)/parall⟩l.alt12−η
2/parall⟩l.alt1gt−∇f(xt)/parall⟩l.alt12−/par⟩nl⟩ft.alt31
2η−l
2/par⟩nright.alt3/parall⟩l.alt1xt+1−xt/parall⟩l.alt12/brack⟩tright.alt3
+η
2pE/brack⟩tl⟩ft.alt4pǫ2+(1−p)/parall⟩l.alt1gt−∇f(xt)/parall⟩l.alt12+(1−p)l2ǫ2/sl⊗sh.left3
σ2/sl⊗sh.left3/parall⟩l.alt1xt+1−xt/parall⟩l.alt12/brack⟩tright.alt4
≤E[Φt]−η
2/parall⟩l.alt1∇f(xt)/parall⟩l.alt12−/par⟩nl⟩ft.alt41
2η−l
2−η(1−p)
p⋅/par⟩nl⟩ft.alt4l2ǫ2/sl⊗sh.left3
σ2/sl⊗sh.left3/par⟩nright.alt4/par⟩nright.alt4
/dcurlyl⟩ft/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlymid/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlyright
B/parall⟩l.alt1xt+1−xt/parall⟩l.alt12+ηǫ2
2.(17)
Since we have chosen η=1
2landp=ǫ2/sl⊗sh.left3σ−2/sl⊗sh.left3, it holds that
B≥l
2/par⟩nl⟩ft.alt41−ǫ2/sl⊗sh.left3
pσ2/sl⊗sh.left3/par⟩nright.alt4≥0.
Thus we have:
1
TT−1
/summation.disp
i=0E/brack⟩tl⟩ft.alt1/parall⟩l.alt1∇f(xi)/parall⟩l.alt12/brack⟩tright.alt≤2
ηTE[Φ0−ΦT]+ǫ2
2
≤2
ηTE/brack⟩tl⟩ft.alt3f(x0)−f(xT)+η
p/parall⟩l.alt1g0−∇f(x0)/parall⟩l.alt12/brack⟩tright.alt3+ǫ2
2
≤ǫ2,
where the last inequality is by setting
T=/uni2308.alt18l∆ǫ−2+4σ2/sl⊗sh.left3ǫ−4/sl⊗sh.left3/uni2309.alt1.
In the following, we bound the total queries of quantum stoch astic gradient oracles. The expectation
oracles to construct ˆgtis
b0=˜O/par⟩nl⟩ft.alt2√
dσˆσ−1
1,t/par⟩nright.alt2=˜O/par⟩nl⟩ft.alt2σ√
dǫ−1/par⟩nright.alt2,
and the expectation queries to construct ∆gtis
b1=˜O/par⟩nl⟩ft.alt2√
dl/parall⟩l.alt1xt−xt−1/parall⟩l.alt1ˆσ−1
2,t/par⟩nright.alt2=˜O/par⟩nl⟩ft.alt2√
dσ1/sl⊗sh.left3ǫ−1/sl⊗sh.left3/par⟩nright.alt2.
Thus, the total stochastic quantum gradient oracles for ﬁnd ing theǫ-stationary point of f(⋅)can be
bounded by
T(b0p+(1−p)b1)=˜O/par⟩nl⟩ft.alt2√
d(l∆σ1/sl⊗sh.left3ǫ−7/sl⊗sh.left3+σǫ−5/sl⊗sh.left3)/par⟩nright.alt2.
Remark G.2.QGM+ (Algorithm 3) improves the quantum stochastic gradien t oracle of Q-SPIDER
([48, Algorithm 7]) by a factor of ǫ−1/sl⊗sh.left6.
19NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introdu ction accurately reﬂect the
paper’s contributions and scope?
Answer: [Yes]
Guidelines:
• The answer NA means that the abstract and introduction do no t include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions a nd limitations. A No or
NA answer to this question will not be perceived well by the re viewers.
• The claims made should match theoretical and experimental results, and reﬂect how
much the results can be expected to generalize to other setti ngs.
• It is ﬁne to include aspirational goals as motivation as lon g as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: We have discussed our limitations in Section 5.
Guidelines:
• The answer NA means that the paper has no limitation while th e answer No means
that the paper has limitations, but those are not discussed i n the paper.
• The authors are encouraged to create a separate ”Limitatio ns” section in their paper.
• The paper should point out any strong assumptions and how ro bust the results are to
violations of these assumptions (e.g., independence assum ptions, noiseless settings,
model well-speciﬁcation, asymptotic approximations only holding locally). The au-
thors should reﬂect on how these assumptions might be violat ed in practice and what
the implications would be.
• The authors should reﬂect on the scope of the claims made, e. g., if the approach was
only tested on a few datasets or with a few runs. In general, em pirical results often
depend on implicit assumptions, which should be articulate d.
• The authors should reﬂect on the factors that inﬂuence the p erformance of the ap-
proach. For example, a facial recognition algorithm may per form poorly when image
resolution is low or images are taken in low lighting. Or a spe ech-to-text system might
not be used reliably to provide closed captions for online le ctures because it fails to
handle technical jargon.
• The authors should discuss the computational efﬁciency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limita tions of their approach to ad-
dress problems of privacy and fairness.
• While the authors might fear that complete honesty about lim itations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The auth ors should use their best
judgment and recognize that individual actions in favor of t ransparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciﬁcally instructed to not penalize honesty conc erning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provi de the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Guidelines:
20• The answer NA means that the paper does not include theoreti cal results.
• All the theorems, formulas, and proofs in the paper should b e numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in t he statement of any theo-
rems.
• The proofs can either appear in the main paper or the supplem ental material, but if
they appear in the supplemental material, the authors are en couraged to provide a
short proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the pa per should be comple-
mented by formal proofs provided in appendix or supplementa l material.
• Theorems and Lemmas that the proof relies upon should be pro perly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affec ts the main claims and/or conclu-
sions of the paper (regardless of whether the code and data ar e provided or not)?
Answer: [NA]
Guidelines:
• The answer NA means that the paper does not include experime nts.
• If the paper includes experiments, a No answer to this quest ion will not be perceived
well by the reviewers: Making the paper reproducible is impo rtant, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors s hould describe the steps
taken to make their results reproducible or veriﬁable.
• Depending on the contribution, reproducibility can be acc omplished in various ways.
For example, if the contribution is a novel architecture, de scribing the architecture
fully might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation,
it may be necessary to either make it possible for others to re plicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibilit y can also be provided via
detailed instructions for how to replicate the results, acc ess to a hosted model (e.g., in
the case of a large language model), releasing of a model chec kpoint, or other means
that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conferen ce does require all sub-
missions to provide some reasonable avenue for reproducibi lity, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the pap er should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architectur e, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the resul ts or a way to re-
produce the model (e.g., with an open-source dataset or inst ructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some c ases, in which case au-
thors are welcome to describe the particular way they provid e for reproducibility.
In the case of closed-source models, it may be that access to t he model is limited in
some way (e.g., to registered users), but it should be possib le for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and c ode, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental result s, as described in supplemental
material?
Answer: [NA]
Guidelines:
21• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines
(https://nips.cc/public/guides/CodeSubmissionPolicy ) for more de-
tails.
• While we encourage the release of code and data, we understan d that this might not
be possible, so “No” is an acceptable answer. Papers cannot b e rejected simply for not
including code, unless this is central to the contribution ( e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and envi ronment needed to run
to reproduce the results. See the NeurIPS code and data submi ssion guidelines
(https://nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate dat a, and generated data, etc.
• The authors should provide scripts to reproduce all experi mental results for the new
proposed method and baselines. If only a subset of experimen ts are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors sho uld release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test de tails (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) n ecessary to understand the
results?
Answer: [NA]
Guidelines:
• The answer NA means that the paper does not include experime nts.
• The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make s ense of them.
• The full details can be provided either with the code, in app endix, or as supplemental
material.
7.Experiment Statistical Signiﬁcance
Question: Does the paper report error bars suitably and corr ectly deﬁned or other appropri-
ate information about the statistical signiﬁcance of the ex periments?
Answer: [NA]
Guidelines:
• The answer NA means that the paper does not include experime nts.
• The authors should answer ”Yes” if the results are accompan ied by error bars, conﬁ-
dence intervals, or statistical signiﬁcance tests, at leas t for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturin g should be clearly stated (for
example, train/test split, initialization, random drawin g of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explain ed (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distr ibuted errors).
• It should be clear whether the error bar is the standard devi ation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it . The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not veriﬁed.
• For asymmetric distributions, the authors should be caref ul not to show in tables or
ﬁgures symmetric error bars that would yield results that ar e out of range (e.g. negative
error rates).
22• If error bars are reported in tables or plots, The authors sh ould explain in the text how
they were calculated and reference the corresponding ﬁgure s or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufﬁc ient information on the com-
puter resources (type of compute workers, memory, time of ex ecution) needed to reproduce
the experiments?
Answer: [NA]
Guidelines:
• The answer NA means that the paper does not include experime nts.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research projec t required more compute
than the experiments reported in the paper (e.g., prelimina ry or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Guidelines:
• The answer NA means that the authors have not reviewed the Ne urIPS Code of Ethics.
• If the authors answer No, they should explain the special ci rcumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., i f there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive so cietal impacts and negative
societal impacts of the work performed?
Answer: [NA] This paper focus on the theory of solving nonlinear equation s.
Guidelines:
• The answer NA means that there is no societal impact of the wo rk performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential m alicious or unintended uses
(e.g., disinformation, generating fake proﬁles, surveill ance), fairness considerations
(e.g., deployment of technologies that could make decision s that unfairly impact spe-
ciﬁc groups), privacy considerations, and security consid erations.
• The conference expects that many papers will be foundation al research and not tied
to particular applications, let alone deployments. Howeve r, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, i t is not needed to point out
that a generic algorithm for optimizing neural networks cou ld enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could aris e when the technology is
being used as intended and functioning correctly, harms tha t could arise when the
technology is being used as intended but gives incorrect res ults, and harms following
from (intentional or unintentional) misuse of the technolo gy.
• If there are negative societal impacts, the authors could a lso discuss possible mitiga-
tion strategies (e.g., gated release of models, providing d efenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor ho w a system learns from
feedback over time, improving the efﬁciency and accessibil ity of ML).
2311.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g ., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Answer: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the mode l, for example by re-
quiring that users adhere to usage guidelines or restrictio ns to access the model or
implementing safety ﬁlters.
• Datasets that have been scraped from the Internet could pos e safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is chall enging, and many papers do
not require this, but we encourage authors to take this into a ccount and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g ., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA] We use open access datasets.
Guidelines:
• The answer NA means that the paper does not use existing asse ts.
• The authors should cite the original paper that produced th e code package or dataset.
• The authors should state which version of the asset is used a nd, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright informatio n, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide c an help determine the li-
cense of a dataset.
• For existing datasets that are re-packaged, both the origi nal license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors ar e encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well docume nted and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Guidelines:
• The answer NA means that the paper does not release new asset s.
• Researchers should communicate the details of the dataset /code/model as part of their
submissions via structured templates. This includes detai ls about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtain ed from people whose
asset is used.
• At submission time, remember to anonymize your assets (if a pplicable). You can
either create an anonymized URL or include an anonymized zip ﬁle.
2414.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participa nts and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Guidelines:
• The answer NA means that the paper does not involve crowdsou rcing nor research
with human subjects.
• Including this information in the supplemental material i s ﬁne, but if the main contri-
bution of the paper involves human subjects, then as much det ail as possible should
be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved i n data collection, cura-
tion, or other labor should be paid at least the minimum wage i n the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent f or Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Insti tutional Review Board (IRB)
approvals (or an equivalent approval/review based on the re quirements of your country or
institution) were obtained?
Answer: [NA]
Guidelines:
• The answer NA means that the paper does not involve crowdsou rcing nor research
with human subjects.
• Depending on the country in which research is conducted, IR B approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
• We recognize that the procedures for this may vary signiﬁca ntly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information th at would break anonymity
(if applicable), such as the institution conducting the rev iew.
25