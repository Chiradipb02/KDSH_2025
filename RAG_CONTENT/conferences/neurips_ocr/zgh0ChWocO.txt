Learning the Optimal Policy for Balancing
Short-Term and Long-Term Rewards
Qinwei Yang1, Xueqing Liu1, Yan Zeng1, Ruocheng Guo2, Yang Liu3, Peng Wu1∗
1Beijing Technology and Business University2ByteDance Research3UC Santa Cruz
Abstract
Learning the optimal policy to balance multiple short-term and long-term rewards
has extensive applications across various domains. Yet, there is a noticeable scarcity
of research addressing policy learning strategies in this context. In this paper, we
aim to learn the optimal policy capable of effectively balancing multiple short-term
and long-term rewards, especially in scenarios where the long-term outcomes are
often missing due to data collection challenges over extended periods. Towards this
goal, the conventional linear weighting method, which aggregates multiple rewards
into a single surrogate reward through weighted summation, can only achieve sub-
optimal policies when multiple rewards are related. Motivated by this, we propose a
novel decomposition-based policy learning (DPPL) method that converts the whole
problem into subproblems. The DPPL method is capable of obtaining optimal
policies even when multiple rewards are interrelated. Nevertheless, the DPPL
method requires a set of preference vectors specified in advance, posing challenges
in practical applications where selecting suitable preferences is non-trivial. To
mitigate this, we further theoretically transform the optimization problem in DPPL
into an ε-constraint problem, where εrepresents the minimum acceptable levels of
other rewards while maximizing one reward. This transformation provides intuitive
into the selection of preference vectors. Extensive experiments are conducted on
the proposed method and the results validate the effectiveness of the method.
1 Introduction
Learning an optimal policy for balancing multiple short-term and long-term rewards holds extensive
applications across various domains. For instance, content providers can optimize recommendations
to avoid short-term clickbait strategies, ensuring sustained user engagement and revenue growth [ 1].
IT companies can design web pages catering to immediate user preferences while enhancing long-
term engagement and satisfaction [ 2]. Economists explore the effects of early childhood interventions
on lifetime earnings, seeking optimal policies (e.g., class size) maximizing short-term test scores
and long-term earnings simultaneously [ 3]. Policymakers can improve job training program design,
considering both immediate income impacts and subsequent employment status improvements [ 4,5].
Medical practitioners can refine drug prescriptions, considering short-term alleviation and long-term
outcomes in chronic diseases like Alzheimer’s and AIDS [ 6]. Marketing professionals can optimize
incentive strategies to positively influence customer behavior in both short and long terms [7].
Despite the importance of balancing multiple short-term and long-term rewards, policy learning
methods in this area remain largely unexplored. Recent literature [ 8] employs a linear weighting
method to achieve this goal. It combines multiple rewards into a single surrogate reward by weighted
summation, which is optimized to learn the optimal policy. However, this strategy has several
limitations. First, it can only find optimal solutions in convex regions of objective space and cannot
obtain the optimal solutions in non-convex regions [ 9]. Second, it achieves the optimal solution only
∗Corresponding author: pengwu@btbu.edu.cn.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).when the rewards are independent of each other. When some of the rewards are interrelated, it can
only achieve sub-optimal solutions [ 10]. Consequently, although the linear weighting method is easy
to implement, the optimality of its solution cannot be guaranteed when balancing multiple objectives.
In this article, we propose a principled policy learning approach for balancing multiple long-term
and short-term rewards (objectives). Specifically, we first formulate it as a multiple-objective
problem (MOP) and aim to seek the Pareto optimal solutions (policies). A solution is Pareto
optimal if improving one objective necessitates worsening other objectives. Then, we propose a
novel decomposition-based policy learning (DPPL) method, which involves (1) introducing a set of
preference vectors, (2) dividing the whole optimization problem into several subproblems based on
the preference vectors, and (3) ultimately achieving different Pareto solutions for the objectives by
solving these subproblems. Compared with the linear weighting method, it can obtain Pareto optimal
solutions in non-convex regions and is applicable to cases where multiple objectives are interrelated.
While the proposed DPPL method can find Pareto optimal policies, it necessitates specifying a set of
preference vectors in advance. In practical applications, decision-makers may encounter the challenge
of determining which preference vector to choose. To mitigate this concern, we further theoretically
transform the optimization problem in DPPL into an ε-constraint problem. This transformation can
assist decision-makers in better understanding and selecting preference vectors.
The contributions of this paper are summarized as follows.
•We formulate the policy learning problem of balancing multiple long-term and short-term rewards
as a multi-objective optimization problem and propose a decomposition-based Pareto policy learning
(DPPL) method to obtain a set of Pareto optimal policies.
•We theoretically establish the connection between the DPPL method and the ϵ-constraint problem,
offering an intuitive interpretation of preference vectors and guiding their selection.
•We conduct extensive experiments to demonstrate the effectiveness of the proposed method.
2 Problem Formulation
Throughout, we employ bold letters for vectors, uppercase letters for random variables, and lowercase
letters for their realization values.
2.1 Notation
We introduce notations to delineate short-term and long-term causal effects. Let Adenote the binary
treatment indicator, where A= 1represents the treated group and A= 0represents the control group.
Xrepresents the features observed, S= (S1, ..., S I)∈RIandY= (Y1, ..., Y J)∈RJrepresent the
vector of short-term and long-term outcomes, respectively. Both short-term and long-term outcomes
are observed after the treatment A, and associations among them may exist.
Utilizing the potential outcome framework [ 11], we denote S(a) = (S1(a), ..., S I(a))andY(a) =
(Y1(a), ..., Y J(a))fora= 0,1as the potential short-term and long-term outcomes under treatment
A=a, respectively. We assume that larger short-term and long-term outcomes are preferable. The
observed short-term and long-term outcomes SandYcorrespond to the potential outcomes of the
actual treatment, that is, S=S(A)andY=Y(A).
In real-world applications, long-term outcomes often suffer from missing due to prolonged follow-up
periods and budget constraints. In contrast, collecting short-term outcomes is more manageable.
Therefore, we presume that all short-term outcomes Sare observable, while long-term outcomes Y
may be subject to missing. Let R= (R1, ..., R J)∈ {0,1}Jdenote the indicator for observing the
long-term outcome Y, where Rj= 1indicates that Yjis observed and Rj= 0indicates that Yjis
missing. The missingness of Ywould lead to identifiability and estimation problems [12–23].
2.2 Formulation
In this article, we aim to learn the Pareto optimal policy for balancing multiple correlated short-
term and long-term rewards, which has a wide range of application scenarios [ 1,6,8,24]. Let
π:X → { 0,1}be a policy that maps from the individual context X=xto the treatment space
2{0,1}. For a given policy π(θ) =π(X,θ)parameterized by θ, the policy values for the i-th
short-term outcome Siand the j-th long-term outcome Yjare defined as,
V(θ;si) =E[π(θ)Si(1) + (1 −π(θ))Si(0)], i= 1, ..., I
V(θ;yj) =E[π(θ)Yj(1) + (1 −π(θ))Yj(0)], j = 1, ..., J,
which are the i-th short-term reward and the j-the long-term reward induced by the policy π(θ).
Conventionally, we convert maximization problems to minimization problems. Let ¯V(θ;si) =
−V(θ;si),¯V(θ;yj) =−V(θ;yj). The trade-off among multiple correlated long-term and short-term
rewards can be formulated as a multi-objective optimization (MOP) problem given by
min
θ¯V(θ) = ( ¯V(θ;s1),···,¯V(θ;sI),¯V(θ;y1),···,¯V(θ;yJ))
≜(¯V1(θ),¯V2(θ),···,¯VM(θ))(1)
where M=I+Jand the symbol ≜means ‘denoted as’. Generally, there is no single solution that
can simultaneously optimize all objectives in problem (1)and thus we resort to the Pareto optimality.
This concept is employed to define the optimal solutions for the MOP problem.
Definition 1. (Pareto optimality)
(a) Pareto dominance. For two points θ1,θ2.θ1dominates θ2if and only if ¯Vm(θ1)≤
¯Vm(θ2),∀m∈ {1, ..., M }and¯Vm′(θ1)<¯Vm′(θ2),∃m′∈ {1, ..., M }
(b) Pareto optimality. θ∗is a Pareto optimal point if there is no other solution ˆθthat dominates θ∗.
Pareto optimality refers to a condition where improving one objective comes at the expense of
worsening other objectives. The collection of Pareto optimal solutions is called the Pareto set. Our
goal is to derive the set of Pareto optimal solutions (or Pareto optimal policies), each of them providing
a distinct optimal trade-off among all objectives.
2.3 Identification and Estimation of Short-term and Long-term Rewards
The long-term and short-term rewards are causal parameters that cannot be identified without imposing
causal assumptions [ 25–27]. Therefore, before seeking the Pareto optimal solutions for balancing
multiple long-term and short-term rewards, it is necessary to consider the identification and estimation
of long-term and short-term rewards. The proposed method is based on Assumptions 1 and 2 below.
Assumption 1 (Strong Ignorability) .
(a)(S(a),Y(a))⊥ ⊥A|Xfora= 0,1;
(b)0< e(x)≜P(A= 1|X=x)<1for all x.
Assumption 1(a) suggests that, given the feature X, treatment assignment Ais independent of
the potential outcomes S(a)andY(a). This implies that confounding bias between the treatment
Aand the short/long-term outcomes (S(a),Y(a))can be eliminated by conditioning on X[28].
Assumption 1(b) ensures that for the subpopulation of X=x, units with both A= 1andA= 0
exist. These assumptions are widely used in causal inference [11, 27, 29–35].
In addition to confounding bias, we also need to address the selection bias induced by the missingness
of long-term outcomes [8]. Thus, we further invoke the Assumption 2.
Assumption 2 (Missing Mechanism of Long-term Outcome) .Fora= 0,1andj= 1, ..., J ,
(a)Rj⊥ ⊥Yj(a)|X,S(a), A=a;
(b)0< rj(x, a,s)≜P(Rj= 1|X=x, A=a,S=s).
Assumption 2(a) can be reformulated as Rj⊥ ⊥Yj|(X,S, A), which means that Rjrelies only on
the observed variables (X, A,S). This assumption also ensures that P(Yj=y|X,S, A, R j= 1) =
P(Yj=y|X,S, A, R j= 0) . This implies that we can utilize the available data to draw conclusions
about the missing long-term outcome. Assumption 2(b) assumes that the long-term outcome for each
unit has a non-zero probability of being observed. Assumptions 1 and 2 ensures the identifiability of
V(θ;si)andV(θ;yj), as shown in Lemma 1.
3Lemma 1 (Identifiability of Short-term and Long-term Rewards) .Fori= 1, ..., I andj= 1, ..., J ,
(a) under Assumptions 1, the i-th short-term reward V(θ;si)is identifiable.
(b) under Assumptions 1-2, the j-th long-term reward V(θ;yj)is identifiable.
When we have access to only one short-term outcome and one long-term outcome, Lemma 1 reduces
to the identifiability result presented in [ 8]. In this article, our focus is on achieving the Pareto optimal
policy for multiple short-term and long-term rewards. Therefore, for the estimation of V(θ;si)and
V(θ;yj), we defer it to Appendix A.
3Pareto Policy Learning for Balancing Short-Term and Long-Term Rewards
In this section, we aim to learn Pareto optimal policies for the MOP problem (1). Section 3.1 gives
the motivation for this work and Section 3.2 introduces the proposed policy learning approach. In
Section 3.3, we theoretically establishe the connection between the linear weighting method, the
MOP problem for a given preference vector, and the ε-constraint problem. This connection offers an
intuitive interpretation and guides practitioners in selecting the preference vector.
3.1 Motivation
For seeking the optimal policy for balancing short-term and long-term rewards, previous work [ 8]
adopted the linear weighting method. Specifically, the authors formulate the goal as
min
θ¯V(θ) =MX
m=1ωm¯Vm(θ), (2)
where ωmis the pre-specified weight for the m-th objective. The objective function in the optimization
problem (2) is merely a linear combination of multiple objectives from the MOP problem (1). Due to
its intuitiveness and simplicity, the traditional linear weighting method is commonly used for solving
MOP or multi-task learning problems [36–38].
The linear weighting method simply combines multiple objectives into a single surrogate objective
through weighted summation. While simple, it has several limitations. First, the optimal solution is
found only in convex regions and not in non-convex regions [ 9]. Second, an optimal solution can only
be achieved if the objectives are independent of each other. That is, if some objectives are interrelated,
only a suboptimal solution can be obtained [ 10]. Thus, it does not guarantee the superiority of the
solution or its solution may deviate from the Pareto optimal solution.
To overcome the limitations of the linear weighting method in [ 8], we first introduce a decomposition-
based multi-objective optimization algorithm to achieve the Pareto optimal policy. However, this
algorithm relies on pre-specified preference vectors, which are used to express a decision maker’s
degree of preference for multiple conflicting objectives. In practice, the explanation and selection of
preference vectors is a challenging problem. To further tackle this issue, we establish a theoretical
relationship between preference vectors and the ε-constraint method [ 39]. This relationship provides
a clear interpretation on preference vectors, assisting in selecting more suitable ones.
3.2 Pareto Policy Learning for the MOP Problem
We introduce the decomposition-based Pareto policy learning (DPPL) method, which can generate
the Pareto set containing policies that are optimum from a trade-off perspective. The main idea of the
DPPL method is to first decompose the original MOP problem into several constrained subproblems
based on a predefined set of preference vectors, and then obtain a set of Pareto optimal policies by
solving these subproblems in parallel [40].
For obtaining the Pareto optimal policy for balancing Mshort-term and long-term objectives, first, we
are given a set of Kpreference vectors {u1,u2, ...,uK}inRM
+. Each element of a preference vector
specifies the importance of the corresponding short-term or long-term reward. For each preference
vector uk, the corresponding subproblem is given as
min
θ¯V(θ) = ( ¯V1(θ),¯V2(θ),···,¯VM(θ))
s.t.Gk′(θ) = (uk′−uk)T¯V(θ)≤0,∀k′= 1, ..., K,(3)
4where Gk′(θt)≤0means that objective space2of the subproblem is restricted in the subregion Ωk,
which is defined by Ωk={v∈RM
+|uT
k′v≤uT
kv,∀k′= 1, ..., K}. Geometrically speaking, Ωk
represents the set of vthat forms the smallest acute angle with uk, which means that the optimal
solution of the subproblem can be obtained by only searching the subregion. The preference vectors
divide the objective space into different subregions.
Solving the subproblem (3) involves the following two steps:
•Step (a) . Find a reasonable initial solution θ0. Specifically, we first randomly generate a solution
θrin the full decision space3, and then iteratively update it with the rule θrt+1=θrt+ηrdrt,
where ηris the step size. For a given θrt, the descent direction drtis updated by solving (4).
(drt, αrt) = arg min
d∈Rn,α∈Rα+1
2||d||2, s.t.∇Gk′(θrt)Td≤α, k′∈ I(θrt). (4)
where I(θrt) ={k′|Gk′(θrt)≥0, k′= 1, ..., K}is index set of all activated constraints, which
means ¯V(θrt)not in Ωk. The problem (4)aims to find the descent direction drtfor each iteration
tand then obtain the initial solution θ0such that ¯V(θ0)inΩk.
•Step (b) . Solving the subproblem (3). The descent direction dtfor the t-th iteration is obtained by
(dt, αt) = arg min
d∈Rn,α∈Rα+1
2||d||2
s.t.∇¯Vm(θt)Td≤α, m = 1, ..., M.
∇Gk′(θt)Td≤α, k′∈ Iϵ(θt),(5)
where Iϵ(θ) ={k′|Gk′(θ)≥ −ϵ}, and the threshold ϵis a slack variable used to deal with
the solutions near the constraint boundary. We further transform it into a dual problem which
will greatly reduce the dimension of decision space. Based on the KKT conditions, we have
dt=−(PM
m=1λm∇¯Vm(θt) +P
k′∈Iϵ(θ)βk′∇Gk′(θt)). Therefore, the dual problem is given as
max
λm,βk′−1
2||MX
m=1λm∇¯Vm(θt) +X
k′∈Iϵ(θ)βk′∇Gk′(θt)||2
s.t.MX
m=1λm+X
k′∈Iϵ(θ)βk′= 1, λm≥0, βk′≥0,∀m= 1, ..., M, ∀k′∈ Iϵ(θ).(6)
where λm≥0andβk′≥0are the Lagrange multipliers for the linear inequality constraints.
Step (a) is to find an initial solution θ0that is restricted in a subregion of the subproblem (3), and
once a feasible solution is found or a predetermined number of iterations is reached, the step stops.
For an given initial solution θ0, Step (b) is to find the optimal solution θ∗for the subproblem (3). We
summarize the proposed policy learning approach in Appendix B.
Lemma 2 ([41]) .Let(dt, αt)be the solution to the t-th iteration of problem (5).
(a) Ifθtis Pareto optimal restricted on Ωk, thendt= 0∈Rmandαt= 0.
(b) Ifθtis not Pareto optimal restricted on Ωk. then
αt≤ −(1/2)||dt||2<0,
∇¯Vm(θt)Tdt≤αt, m= 1, ..., M
∇Gk′(θt)Tdt≤αt, k′∈ Iϵ(θt).(7)
Lemma 2(a) implies that at the t-th iteration, no direction ( dt= 0) can simultaneously improve the
performance for all objectives, confirming that the solution θtsatisfies Pareto optimality. Lemma
2(b) suggests that if θtdoes not meet Pareto optimality, then the descent direction dt̸= 0 serves
as the descent direction for all objectives, such that the solution of the next iteration is closer to the
Pareto optimal solution. Thus, Lemma 2 demonstrates that we always attain Pareto optimal solutions
for each subproblem using the update rule θt+1=θt+ηrdt. By solving all subproblems, we can
acquire a diverse set of Pareto optimal solutions (or policies) confined to different subregions, even
when the multiple objectives are correlated.
2¯V(θ)is the objective vector, and the space spanned by the objective vectors is called the objective space Ω.
3The parameter vector θrepresents the decision variable and the space spanned it is called the decision space.
53.3 Deep Analysis of the Preference Vector
The DPPL method in Section 3.2 requires a set of pre-specified preference vectors, posing challenges
in practical applications where selecting suitable preference vectors is non-trivial. To mitigate this
problem, we provide a practical method for decision-makers to select appropriate preference vectors
by theoretically establishing the connection between the DPPL method and the ε-constraint problem.
We first give a brief introduction to the ε-constraint problem [10], which is defined as follows,
min
θ¯Vl(θ),s.t.¯Vm(θ)≤εmfor all m= 1, . . . , M, m ̸=l, (8)
where εmis pre-specified threshold. Compared to the MOP problem (1)and the linear weighting
objective (2), a notable advantage of the ε-constraint problem is its interpretation on the threshold
εm, which represents the maximum acceptable value (i.e., the acceptable worst-case scenario) for the
m-th objective. In contrast, the weights and the preference vectors in problems (1)and(2)are not
straightforward for relating the resulting values of objectives. Thus, if we can establish the connection
between ε= (ε1, ..., ε M)and the preference vector uk, then we can provide powerful guidance for
choosing appropriate preference vectors.
Theorem 1. For the preference vector uk= (uk1, ..., u kM)in problem (1), the weights ω=
(ω1, ...., ω M)in problem (2), and the thresholds εin problem (8), the following statements hold:
(a) the connection between εandωis given as
εm=−E[I(τl(X) +ωm
ωlτm(X)>0)·τm(X) +hm(X)],form= 1···M, and m ̸=l,(9)
where τm(X)is the conditional average causal effects for m-th short/long-term outcome,
τm(X) =(
E[Si(1)−Si(0)|X],ifωmis the weight of ¯V(θ, si),
E[Yj(1)−Yj(0)|X],ifωmis the weight of ¯V(θ, yj),
I(·)is the indicator function, and
hm(X) =(
E[Si(0)|X],ifωmis the weight of ¯V(θ, si),
E[Yj(0)|X,S, Rj= 1],ifωmis the weight of ¯V(θ, yj).
(b) the connection between ωandukis given as
ωm=λm+X
k′∈Iϵ(θ)βk′(uk′m−ukm),form= 1,···, M, (10)
where λmandβk′are defined in Eq. (6), andIϵ(θ) ={k′|Gk′(θ)≥ −ϵ}defined in Eq. (4).
Theorem 1 (see Appendix C for proofs) establishes a link between the preference vector ukandε
through ωin scenarios involving multiple long-term and short-term objectives. Specifically, Theorem
1(a) shows how to estimate the threshold εfor given weights ω, and Theorem 1(b) shows how to
assign weights ωvia preference vectors uk. This means that for the subproblem determined by
preference vectors uk, we can ascertain the maximum acceptable threshold εbased on Theorem 1,
thereby offering an intuitive interpretation of the preference vector uk.
There are several practical implications with Theorem 1. On one hand, it assists decision-makers
in better understanding and selecting preference vectors in practical applications. In practice, we
can initially pre-specify a set of preference vectors {u1,u2, ...,uK}inRM
+, then derive the weights
ωcorresponding to each preference vector ukthrough Eq. (10), and finally substitute the obtained
weight ωinto Eq. (9)to calculate the threshold ε. Leveraging the intuitive interpretability of the
threshold ε, decision-makers can select the appropriate preference vectors according to their specific
requirements. On the other hand, it also provides guidance for specifying εin the ε-constraint
problem (8). Inappropriate selection of εfor this problem may result in an empty feasible region,
yielding empty solutions. By utilizing a set of preference vectors, we can efficiently screen out some
reasonable choices of εand reduce the cumbersome trial-and-error process of testing different ε.
In conclusion, by establishing the connection between the DPPL method and the ε-constraint problem,
we can harness the advantages of both methods while mitigating their respective weaknesses.
64 Experiments
Datasets. Following the previous studies [ 8], we use two widely used datasets: IHDP and JOBS, for
evaluating the performance of the proposed method. The IHDP dataset explores the effectiveness of
high-quality home visiting in promoting children’s future cognitive development and covers a sample
of 747 units, including 139 treated and 608 controlled. In addition, the dataset has 25 characteristics
that provide a comprehensive picture of the children and their mothers. The second dataset, JOBS,
explores the effects of job training on income and employment status. It consists of 2,570 units
(237 treated, 2,333 controlled), with 17 covariates. Note that each unit in both datasets has only one
observed outcome from a single treatment, and neither dataset collects long-term outcomes.
Simulating Outcome. Consider the case of one long-term reward and one short-term reward.
Following the previous data-generation mechanisms [ 1,42], for the n-th unit ( n= 1, ..., N ), we
simulate the potential short-term outcomes S(0)andS(1)as follows:
Sn(0)∼Bern( σ(w0Xn+ϵ0,n)), S n(1)∼Bern( σ(w1Xn+ϵ1,n)),
where σ(·)is the sigmoid function, w0∼ N [−1,1](0,1)follows a truncated normal distribution,
w1∼Unif(−1,1)follows a uniform distribution, ϵ0,n∼ N(µ0, σ0)andϵ1,n∼ N(µ1, σ1). We set
µ0= 1, µ1= 3andσ0=σ1= 1for the IHDP dataset, and we set µ0= 0, µ1= 2andσ0=σ1= 1
for the JOBS dataset. For generating long-term potential outcomes Y(0)andY(1), we introduce
the time step t: we set the initial value at time step 0 as: Y0,n(0) = Sn(0),Y0,n(1) = Sn(1), then
generate Yt,n(0), Yt,n(1)according to the following equation and we eventually regard the outcome
at the last time step Tas the long-term outcome, Yn(0) = YT,n(0), Yn(1) = YT,n(1).
Yt,n(0)∼Bern( σ(β0Xn)+Ct−1X
t′=0Yt′,n(0))+ ϵ0,n, Yt,n(1)∼Bern( σ(β1Xn)+Ct−1X
t′=0Yt′,n(0))+ ϵ1,n,
where β0is randomly sampled from {0,1,2,3,4}with probabilities {0.5,0.2,0.15,0.1,0.05}, β1∼
4· N[0,4](0,1),andC= 1/Tis a scaling factor. For ϵ0,nandϵ1,n, we set µ0=µ1= 0, σ0= 1and
σ1= 3for the IHDP dataset and set µ0=µ1= 0, σ0= 1andσ1= 1for the JOBS dataset.
Assumption 2 shows that observing indicator Rdepends on the feature X, the treatment A, and short-
term outcome S. For a given missing rate r, we select the missing indexes for Yand derive the missing
indicator Raccording to the following criterion: calculate the mn= 1/DPD
d=1(Xnd+sn), n=
1,···, N,and choose the index of the row with the smallest rNvalues in {mn, n= 1,···, N}as
the missing indexes. Dis the feature dimension and Nis the sample size.
Experimental Details. In this paper, preference vectors are used to quantify an individual’s preference
for different objectives in the multi-objective optimization problem. For the case of two-objective,
we randomly generate 10 unit preference vectors (u1,u2,···,u10), where uk= (uk1, uk2),uk1=
cos(tk), uk2=sin(tk),tk∈(0,1), which implies that the L2-norm of the preference vectors is
1, ensuring the consistency and comparability of the preference measures. uk1anduk2are the
preferences for the short-term objective and the long-term objective, respectively. Each component of
the preference vector ukrepresents the strength or importance of the decision maker’s preference for
different objectives. Preference vectors are used as weights in the linear weighting method, whereas
our method uses them to divide the original problem (1) into several subproblems.
Evaluation Metrics. We measure the performance of our proposed method by three metrics: long and
short-term rewards, the variance of long and short-term rewards, and the change in welfare. Formally,
the short-term reward of the learned policy ˆπ(X,θ)isˆV(θ;s) =PN
n=1[ˆπ(Xn,θ)Sn(1) + (1 −
ˆπ(Xn,θ))Sn(0)], the long-term reward is ˆV(θ;y) =PN
n=1[ˆπ(Xn,θ)Yn(1)+(1 −ˆπ(Xn,θ))Yn(0)].
Similar as [ 42,43], the welfare changes are defined as ∆Ws=PN
n=1[(Sn(1)−Sn(0))·ˆπ(Xn,θ)]
for the short-term reward, ∆Wy=PN
n=1[(Yn(1)−Yn(0))·ˆπ(Xn,θ)]for the long-term reward,
∆W= 0.5∆Ws+ 0.5∆Wyfor the overall balanced-base reward. Among these metrics, ∆Wis the
most critical here, as it directly measures the balance reward achieved by the learned policy.
Policy learning with short-term and short-term reward. We choose MLP as the policy model
π(θ), and we average over 50 independent trials of policy learning with the short-term and long-term
reward in IHDP and JOBS. We fix the missing ratio r= 0.2and the time step T= 4. We measure
the uncertainty of the model by calculating the variance of the long and short-term reward over 50
experiments, and a smaller variance means a more stable model performance.
7Performance Comparison. From our previous analyses, the linear weighting method generally
achieves the sub-optimal policies. The proposed DPPL method can generate a set of Pareto optimal
policies. First, for the long-term reward, the short-term reward, and ∆W, it is not surprising to
observe that for most of the preference vectors, DPPL’s solutions have better performance. Second,
for the variance, our method performs more stable in 50 experiments. Because we will divide
the original problem into several subproblems according to preference vectors, and then solve the
subproblems in a relatively small subregion to obtain the Pareto optimal solution, whereas the linear
weighting method searches the entire objective space. The associated results are displayed in Table 1.
More experimental results with missing ratio r= 0.3are given in Appendix D.
Table 1: Comparison of our method (OURS) and linear weighting method (LW) on IHDP and
JOBS, with Short-Term Reward ( S-REWARDS ) and Long-Term Reward ( L-REWARDS ),∆W
and Variance (S- VAR and L- VAR) as evaluation metrics. The best result is bolded.
IHDP S-R EWARDS L-R EWARDS ∆W S-VAR L-VAR
PREFERENCE VECTOR OURS LW OURS LW OURS LW OURS LW OURS LW
1 (1.00, 0.00) 522.840 520.860 386.221 383.950 39.432 37.307 14.573 12.841 52.326 56.093
2 (0.98, 0.17) 521.820 524.660 382.774 387.102 37.199 40.782 13.275 11.079 54.181 59.895
3 (0.94, 0.34) 523.000 521.840 372.418 394.386 32.610 43.014 11.588 13.578 50.138 62.584
4 (0.86, 0.50) 521.060 519.680 382.419 379.174 36.641 34.328 11.512 13.299 52.165 48.457
5 (0.76, 0.64) 523.620 519.840 391.296 390.413 42.360 40.028 12.729 15.594 55.883 48.308
6 (0.64, 0.76) 521.460 517.420 387.015 390.206 39.139 38.714 14.217 15.479 46.342 56.219
7 (0.50, 0.87) 523.800 514.480 383.424 381.321 38.514 32.802 12.797 18.758 55.118 55.167
8 (0.34, 0.94) 521.360 516.800 373.307 400.510 32.235 43.556 11.701 18.618 50.002 60.847
9 (0.17, 0.98) 522.240 515.040 397.640 396.214 42.842 40.529 12.913 18.543 57.288 59.071
10 (0.00, 1.00) 523.600 516.780 387.933 390.387 40.668 38.485 11.531 21.079 60.714 54.246
JOBS S-R EWARDS L-R EWARDS ∆W S-VAR L-VAR
PREFERENCE VECTOR OURS LW OURS LW OURS LW OURS LW OURS LW
1 (1.00, 0.00) 1613.140 1612.340 1230.918 1223.416 159.381 155.230 54.724 56.502 84.846 88.008
2 (0.98, 0.17) 1618.400 1607.680 1219.517 1223.072 156.310 152.728 54.521 65.927 85.566 86.622
3 (0.94, 0.34) 1614.800 1598.460 1220.316 1223.813 154.910 148.488 61.466 74.649 94.643 98.588
4 (0.86, 0.50) 1612.880 1598.880 1217.305 1225.574 152.444 149.579 59.510 75.431 90.858 79.728
5 (0.77, 0.64) 1613.160 1602.320 1233.604 1227.886 160.734 152.455 59.042 77.481 85.553 85.854
6 (0.64, 0.76) 1612.380 1595.960 1218.100 1219.996 152.592 145.330 58.028 82.032 96.137 94.424
7 (0.50, 0.86) 1608.860 1596.280 1224.763 1230.471 154.163 150.727 58.706 86.083 89.392 92.135
8 (0.34, 0.94) 1613.600 1595.720 1232.958 1217.118 160.631 143.771 57.000 82.996 81.431 86.121
9 (0.17, 0.98) 1614.840 1596.320 1225.607 1224.383 157.575 147.703 58.278 84.221 99.329 82.437
10 (0.00, 1.00) 1610.380 1588.400 1228.679 1223.119 156.882 143.112 59.285 88.393 95.054 85.443
Sensitivity Analysis. We perform the sensitivity analysis of missing ratio rand time step Ton JOBS.
Our method achieves better performance in all missing rates r= [0.2,0.3,0.4,0.5]withT= 4, and
r= 0.2with time step T= [4,6,8,10]. Our method stably outperforms the linear weighting method
under varying randT, even in scenarios with a high missing ratio or a large time step. This further
illustrates the effectiveness of our method. The associated results are displayed in Figure 1.
(a) r = 0.2 on JOBS
 (b) r = 0.3 on JOBS
 (c) r = 0.4 on JOBS
 (d) r = 0.5 on JOBS
(e) T = 4 on JOBS
 (f) T = 6 on JOBS
 (g) T = 8 on JOBS
 (h) T = 10 on JOBS
Figure 1: Comparison of two methods with different missing ratios {0.2,0.3,0.4,0.5}on JOBS
Interpretation on preference vectors. By Theorem 1, for the set of pre-specified prefer-
ence vectors (u1,u2,···,u10), we transform the optimization subproblem corresponding to
each preference vector into the ε-constraint problem as minθ¯V(θ;y), s.t.¯V(θ;s)≤ε(<0)or
8maxθV(θ;y), s.t.V(θ;s)≥ −εand the threshold −εare shown in Table 2. This value of −εis the
minimum value of the short-term reward that the decision maker can accept while maximizing the
long-term reward. Our results show that as the second component of the preference vector increases,
the value of −εshows a decreasing trend. In essence, this signifies that a decision-maker who
emphasizes the long-term reward must necessarily loosen constraints on the short-term reward. In
practice, decision makers can determine the threshold based on their specific needs for the short-term
reward, and then select the most appropriate preference vector from the set of pre-specify preference
vectors with the help of the intuitive interpretability of the threshold according to Table 2. More
experimental results with different missing ratios {0.3,0.4,0.5}are provided in Appendix D.
Table 2: The εvalues correspond to each preference vector in IHDP and JOBS datasets, where T= 4
andr= 0.2, obtained according to Theorem 1.
Preference Vector -εon IHDP -εon JOBS Preference Vector -εon IHDP -εon JOBS
(1.00, 0.00) 0.820 0.878 (0.00, 1.00) 0.522 0.737
(0.98, 0.17) 0.827 0.875 (0.17, 0.98) 0.522 0.716
(0.94, 0.34) 0.826 0.868 (0.34, 0.94) 0.511 0.704
(0.86, 0.50) 0.833 0.868 (0.50, 0.86) 0.557 0.746
(0.77, 0.64) 0.741 0.865 (0.64, 0.76) 0.659 0.808
5 Related Work
Estimation of long-term causal effects. Assessing long-term causal effects is challenging due to
the delayed long-term outcomes, posing significant difficulties in both identification and estimation.
Recently, there has been increasing interest in using short-term surrogates to identify and estimate
long-term causal effects, such as [ 4,5,7,13,44,45]. In contrast to these previous works focusing on
long-term causal effects, this paper aims to balance multiple short-term and long-term causal effects.
Trustworthy policy learning. Trustworthy policy learning ensures that the learned policies or models
are reliable and dependable for practical applications. Traditional policy learning aims to identify
individuals who would maximize the utility function based on their features if treated [ 46]. Recently,
trustworthy policy learning has focused on ensuring that the learned policy adheres to principles such
as beneficence, non-maleficence, autonomy, justice, no-harm, and explicability [ 42,47–49]. Various
counterfactual-based metrics have been suggested to assess a policy’s trustworthiness [ 42,50–53]. In
this paper, we complement this series of work by developing a principled policy learning approach
that can effectively balance multiple rewards.
Multi-objective optimization (MOP) . MOP aims to find compromises or trade-offs among multiple
possibly contrasting objectives. It is widely used in the field of machine learning such as multi-task
learning [ 40,54], neural architecture search [ 55], and multi-objective reinforcement learning [ 56–
58]. We extend these works to a new setting by learning the optimal policy for balancing multiple
long-term and short-term rewards. Additionally, we provide a practical method for interpreting and
selecting preference vectors with theoretical guarantees.
6 Conclusion
In this paper, we focus on learning the optimal policy for balancing multiple long-term and short-
term rewards. We reveal the limitations of the previous linear weighting method, which usually
results in sub-optimal policies in practice. To address these limitations, we formulate the policy
learning problem as a multi-objective optimization problem and then propose the novel DPPL
method to learn optimal policies. The DPPL method obtains a set of Pareto optimal policies by
solving a series of subproblems based on pre-specified preference vectors, effectively balancing
multiple objectives. Furthermore, we theoretically establish the connection between the optimization
subproblems in the DPPL method and the ε-constraint problem. This connection aids decision-makers
in better understanding and selecting preference vectors. We conducted extensive experiments on
two benchmark datasets which validate the effectiveness of our proposed method. A limitation of
this work is that it focuses on discrete treatments in identification and estimation (Section 2.3). In
some application scenarios, continuous treatments (e.g., price) are of interest. Further investigation is
required to extend the proposed method to accommodate such cases.
9Acknowledgements
Qinwei Yang, Xueqing Liu, Yan Zeng, and Peng Wu were supported by the National Natural Science
Foundation of China (No. 12301370, 62473009), the funding from the Beijing Municipal Education
Commission for the Emerging Interdisciplinary Platform for Digital Business at Beijing Technology
and Business University, the Beijing Key Laboratory of Applied Statistics and Digital Regulation,
and the gift funding from ByteDance Research.
References
[1]Lu Cheng, Ruocheng Guo, and Huan Liu. Long-term effect estimation with surrogate repre-
sentation. In Proceedings of the 14th ACM International Conference on Web Search and Data
Mining , page 274–282, 2021.
[2]Henning Hohnhold, Deirdre O’Brien, and Diane Tang. Focusing on the long-term: It’s good
for users and business. In Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining , pages 1849–1858, 2015.
[3]Raj Chetty, John N. Friedman, Nathaniel Hilger, Emmanuel Saez, Diane Whitmore Schanzen-
bach, and Danny Yagan. How does your kindergarten classroom affect your earnings? evidence
from project star. The Quarterly Journal of Economics , 126:1593–1660, 2007.
[4]Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. The surrogate index:
Combining short-term proxies to estimate long-term treatment effects more rapidly and precisely.
Technical report, National Bureau of Economic Research, 2019.
[5]Susan Athey, Raj Chetty, and Guido Imbens. Combining experimental and observational data
to estimate treatment effects on long term outcomes. arXiv preprint arXiv:2006.09676 , 2020.
[6]Wenjie Hu, Xiao-Hua Zhou, and Peng Wu. Identification and estimation of treatment effects on
long-term outcomes in clinical trials with external observational data. Statistica Sinica , 2023.
[7]Jeremy Yang, Dean Eckles, Paramveer Dhillon, and Sinan Aral. Targeting for long-term
outcomes. Management Science , 2023.
[8]Peng Wu, Ziyu Shen, Feng Xie, Zhongyao Wang, Chunchen Liu, and Yan Zeng. Policy learning
for balancing short-term and long-term rewards. In ICML , 2024.
[9]Yair Censor. Pareto optimality in multiobjective problems. Applied Mathematics and Optimiza-
tion, 4(1):41–59, 1977.
[10] Jürgen Branke. Multiobjective optimization: Interactive and evolutionary approaches , volume
5252. Springer Science & Business Media, 2008.
[11] G. W. Imbens and D. B. Rubin. Causal Inference For Statistics Social and Biomedical Science .
Cambridge University Press, 2015.
[12] Susan Athey, Raj Chetty, Guido Imbens, and Hyunseung Kang. The surrogate index: Combining
short-term proxies to estimate long-term treatment effects more rapidly and precisely. Working
paper, National Bureau of Economic Research, 2019.
[13] Nathan Kallus and Xiaojie Mao. On the role of surrogates in the efficient estimation of treatment
effects with limited outcome data. arXiv preprint arXiv:2003.12408 , 2020.
[14] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. Propensity matters:
Measuring and enhancing balancing for recommendation. In ICML , 2023.
[15] Peng Wu, Shanshan Luo, and Zhi Geng. On the comparative analysis of average treatment
effects estimation via data combination. arXiv preprint arXiv:2311.00528 , 2024.
[16] Sihao Ding, Peng Wu, Fuli Feng, Xiangnan He, Yitong Wang, Yong Liao, and Yongdong Zhang.
Addressing unmeasured confounder for recommendation with sensitivity analysis. In KDD ,
2022.
10[17] Haoxuan Li, Quanyu Dai, Yuru Li, Yan Lyu, Zhenhua Dong, Xiao-Hua Zhou, and Peng Wu.
Multiple robust learning for recommendation. In AAAI , 2023.
[18] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, and Peng Wu. Balancing unobserved confound-
ing with a few unbiased ratings in debiased recommendations. In WWW , 2023.
[19] Quanyu Dai, Haoxuan Li, Peng Wu, Zhenhua Dong, Xiao-Hua Zhou, Rui Zhang, Xiuqiang
He, Rui Zhang, and Jie Sun. A generalized doubly robust learning framework for debiasing
post-click conversion rate prediction. In KDD , 2022.
[20] Peng Wu, Haoxuan Li, Yuhao Deng, Wenjie Hu, Quanyu Dai, Zhenhua Dong, Jie Sun, Rui
Zhang, and Xiao-Hua Zhou. On the opportunity of causal learning in recommendation systems:
Foundation, estimation, prediction and challenges. In IJCAI , 2022.
[21] Haoxuan Li, Yan Lyu, Chunyuan Zheng, and Peng Wu. TDR-CL: Targeted doubly robust
collaborative learning for debiased recommendations. In ICLR , 2023.
[22] Haoxuan Li, Chunyuan Zheng, and Peng Wu. Stabledr: Stabilized doubly robust learning for
recommendation on data missing not at random. In ICLR , 2023.
[23] Haoxuan Li, Chunyuan Zheng, Sihao Ding, Peng Wu, Zhi Geng, Fuli Feng, and Xiangnan
He. Be aware of the neighborhood effect: Modeling selection bias under interference for
recommendation. In ICLR , 2024.
[24] Guido Imbens, Nathan Kallus, Xiaojie Mao, and Yuhao Wang. Long-term causal inference
under persistent confounding via data combination. arXiv preprint arXiv:2202.07234 , 2022.
[25] Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. Causal Inference in Statistics: A Primer .
John Wiley & Sons, 2016.
[26] Judea Pearl and Dana Mackenzie. The Book of Why: The New Science of Cause and Effect .
Hachette Book Group, 2018.
[27] M.A. Hernán and J. M. Robins. Causal Inference: What If . Boca Raton: Chapman and
Hall/CRC, 2020.
[28] Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika , 70(1):41–55, 1983.
[29] Kosuke Imai and Marc Ratkovic. Covariate balancing propensity score. Journal of the Royal
Statistical Society (Series B) , 76(1):243–263, 2014.
[30] Elizabeth A. Stuart. Matching methods for causal inference: A review and a look forward.
Statistical Science , 25(1):1–21, 2010.
[31] Paul R. Rosenbaum. Design of Observational Studies . Springer Nature Switzerland AG, second
edition, 2020.
[32] Peng Wu, Xinyi Xu, Xingwei Tong, Qing Jiang, and Bo Lu. Semiparametric estimation for
average causal effects using propensity score-based spline. Journal of Statistical Planning and
Inference , 212:153–168, 2021.
[33] Peng Wu, Zhiqiang Tan, Wenjie Hu, and Xiao-Hua Zhou. Model-assisted inference for covariate-
specific treatment effects with high-dimensional data. Statistica Sinica , 34:459–479, 2024.
[34] Peng Wu, Shasha Han, Xingwei Tong, and Runze Li. Propensity score regression for causal
inference with treatment heterogeneity. Statistica Sinica , 34:747–769, 2024.
[35] Peng Wu, Peng Ding, Zhi Geng, and Yue Li. Quantifying individual risk for binary outcome:
Bounds and inference. arXiv:2402.10537 , 2024.
[36] Kalyanmoy Deb, Karthik Sindhya, and Jussi Hakanen. Multi-objective optimization. In
Decision sciences , pages 161–200. CRC Press, 2016.
11[37] Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge
and Data Engineering , 34(12):5586–5609, 2022.
[38] Haoxuan Li, Chunyuan Zheng, Yanghao Xiao, Hao Wang, Fuli Feng, Xiangnan He, Zhi Geng,
and Peng Wu. Removing hidden confounding in recommendation: A unified multi-task learning
approach. In NeurIPS , 2023.
[39] Amir Ismail-Yahaya and Achille Messac. Effective generation of the pareto frontier using the
normal constraint method. In 40th AIAA Aerospace Sciences Meeting & Exhibit , page 178,
2002.
[40] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task
learning. Advances in neural information processing systems , 32, 2019.
[41] Jörg Fliege and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization.
Mathematical methods of operations research , 51:479–494, 2000.
[42] Haoxuan Li, Chunyuan Zheng, Yixiao Cao, Zhi Geng, Yue Liu, and Peng Wu. Trustworthy
policy learning under the counterfactual no-harm criterion. In International Conference on
Machine Learning , pages 20575–20598. PMLR, 2023.
[43] T. Kitagawa and A. Tetenov. Who should be treated? empirical welfare maximization methods
for treatment choice. Econometrica , 86, 2018.
[44] Lu Cheng, Ruocheng Guo, and Huan Liu. Long-term effect estimation with surrogate repre-
sentation. In Proceedings of the 14th ACM International Conference on Web Search and Data
Mining , pages 274–282, 2021.
[45] Wenjie Hu, Xiaohua Zhou, and Peng Wu. Identification and estimation of treatment effects
on long-term outcomes in clinical trials with external observational data. arXiv preprint
arXiv:2208.10163 , 2022.
[46] Michael R Kosorok and Eric B Laber. Precision medicine. Annual review of statistics and its
application , 6:263–286, 2019.
[47] Scott Thiebes, Sebastian Lins, and Ali Sunyaev. Trustworthy artificial intelligence. Electronic
Markets , 31:447–464, 2021.
[48] Floridi Luciano. Establishing the rules for building trustworthy ai. Nature Machine Intelligence ,
1(6):261–262, 2019.
[49] Davinder Kaur, Suleyman Uslu, Kaley J Rittichier, and Arjan Durresi. Trustworthy artificial
intelligence: a review. ACM Computing Surveys (CSUR) , 55:1–38, 2022.
[50] Silvia Chiappa. Path-specific counterfactual fairness. In Proceedings of the AAAI conference on
artificial intelligence , volume 33, pages 7801–7808, 2019.
[51] Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong. PC-Fairness: A unified framework
for measuring causality-based fairness. Advances in Neural Information Processing Systems ,
32, 2019.
[52] Nathan Kallus. Treatment effect risk: Bounds and inference. Management Science , 69(8):4579–
4590, 2023.
[53] Nathan Kallus. What’s the harm? sharp bounds on the fraction negatively affected by treatment.
Advances in Neural Information Processing Systems , 35:15996–16009, 2022.
[54] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances
in neural information processing systems , 31, 2018.
[55] Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun. Dpp-net: Device-
aware progressive search for pareto-optimal neural architectures. In Proceedings of the European
conference on computer vision (ECCV) , pages 517–531, 2018.
12[56] Kristof Van Moffaert, Madalina M Drugan, and Ann Nowé. Scalarized multi-objective rein-
forcement learning: Novel design techniques. In 2013 IEEE symposium on adaptive dynamic
programming and reinforcement learning (ADPRL) , pages 191–199. IEEE, 2013.
[57] Nan Xu, Nitin Kamra, and Yan Liu. Treatment recommendation with preference-based rein-
forcement learning. In 2021 IEEE international conference on big knowledge (ICBK) , pages
1–8. IEEE, 2021.
[58] Jiangjiao Xu, Ke Li, and Mohammad Abusara. Preference based multi-objective reinforcement
learning for multi-microgrid system optimization problem in smart grid. Memetic Computing ,
14(2):225–235, 2022.
13A Estimation of Short-Term and Long-Term Rewards
For a given policy π(θ), the policy values for the short-term outcome Siand the long-term outcome
Yjare defined as
V(θ;si) =E[π(θ)Si(1) + (1 −π(θ))Si(0)], i= 1, ..., I
V(θ;yj) =E[π(θ)Yj(1) + (1 −π(θ))Yj(0)], j = 1, ..., J,
Under Assumptions 1-2, the short-term reward V(θ;si)and long-term reward V(θ;yj)are identified
as
V(θ;si) =E[π(θ)µi1(X) + (1 −π(θ))µi0(X)],
V(θ;yj) =E[π(θ˜mj1(X, S) + (1 −π(θ)) ˜mj0(X, S)].
where µia(X) =E[Si|X, A=a],˜mja(X,S) =E[Yj|X,S, A=a, Rj= 1] fora= 0,1. The
identifiability results are derived using a similar approach to that outlined in Section 5 of [ 8]. In
addition, for estimating the V(θ;si)andV(θ;yj), [8] proved the efficient bounds of V(θ;si)and
V(θ;yj), which we list them below for the sake of self-containedness.
Lemma A.1 (Efficiency Bounds of V(θ;si)andV(θ;yj), [8]).LetZ= (X, A,S,Y), under
Assumptions 1-2, we have that
(a) the efficient influence function of V(θ;si)isϕsi− V(θ;si), where
ϕsi=ϕsi(Z;e, µi0, µi1)
={π(θ)µi1(X) + (1 −π(θ))µi0(X)}
+π(θ)A(Si−µi1(X))
e(X)+(1−π(θ))(1−A)(Si−µi0(X))
1−e(X),
ande(X) =P(A= 1|X)is propensity score. The associated semiparametric efficiency bound of
V(θ;si)is Var( ϕsi).
(b) the efficient influence function of V(θ;yj)isϕyj− V(θ;yj), where
ϕyj=ϕyj(Z;e, rj, mj0, mj1,˜mj0,˜mj1)
={π(θ)mj1(X) + (1 −π(θ))mj0(X)}
+π(θ)ARj(Yj−˜mj1(X,S))
e(X)rj(1,X,S)+π(θ)A( ˜mj1(X,S)−mj1(X))
e(X)
+(1−π(θ))(1−A)Rj(Yj−˜mj0(X,S))
(1−e(X))rj(0,X,S)
+(1−π(θ))(1−A)( ˜mj0(X,S)−mj0(X))
1−e(X),
mja(X) =E[Yj|X, A=a, Rj= 1] is the regression function for Yj, and rj(A,X,S) =
P[Rj= 1|X,S, A]is selection score. The associated semiparametric efficiency bound of V(θ;yj)is
Var(ϕyj).
From Lemma A.1, for a given policy π(θ), it is natural to define the estimators of V(θ;si)and
V(θ;yj)as
ˆV(θ;si) =1
NNX
n=1ϕsi(Zn; ˆe,ˆµi0,ˆµi1),
ˆV(θ;yj) =1
NNX
n=1ϕyj(Zn; ˆe,ˆrj,ˆmj0,ˆmj1,ˆ˜mj0,ˆ˜mj1).
where Nis the sample size. All of them can be identified from the observed data.
And ˆe(x),ˆµia(x),ˆmja(x),ˆ˜mja(x,s),andˆrj(a,x,s)fora= 0 ,1are the estimators of
e(x), µia(x), mja(x),˜mja(x,s)andrj(a,x,s)respectively.
14B Algorithm Flowchart for DPPL
Algorithm 1 DPPL Algorithm
1:Input: A set of preference vectors {u1,u2, ...,uK}
(All subproblems can be solved in parallel)
2:fork= 1toKdo
3: randomly generate parameters θ(k)
r
4: find the initial parameters θ(k)
0fromθ(k)
rusing gradient-based method (step a)
5: fort= 1toTdo
6: obtain λ(k)
tm≥0, β(k)
tk′≥0,∀m= 1, ..., M, ∀k′∈Iϵ(θ)by solving subproblem (6)
7: calculate direction d(k)
t=−(PM
i=mλ(k)
tm∇¯Vm(θ(k)
t)+P
k′∈Iϵ(θ)β(k)
tk′∇Gk′(θ(k)
t))/d(k)
t=
−(λk
tm+P
k′∈Iϵ(θ)βk
tk′(uk′m−ukm))∇¯Vm(θk
t)(step b)
8: update the parameters θ(k)
t+1=θ(k)
t+ηd(k)
t
9: end for
10:end for
11:Output: The set of solutions for all subproblems with different trade-offs {θ(k)
T|k= 1, . . . , K }
C Proofs of Theorem 1
For the case of only have one long-term outcome Yand one short-term outcome S, considering the
ε-constraint optimization problem
min
θ¯V(θ;y), s.t., ¯V(θ;s)≤ε (A.1)
and the linear weighting optimization problem
min
θω1¯V(θ;y) +ω2¯V(θ;s) (A.2)
which can be reformulated as
min
θ¯V(θ;y) +λ¯V(θ;s).
where λ=ω2/ω1controls the balance between short-term and long-term rewards. Let
τs(X) =E[S(1)−S(0)|X]andτy(X) =E[Y(1)−Y(0)|X]. When λ= 0, it is equiva-
lent to finding an optimal policy for minimizing ¯V(θ;y)alone, π∗
y(θ) = arg min π¯V(θ;y) =
arg min π−E[π(θ)τy(X)] =I(τy(X)≥0). When λ=∞, it is equivalent to finding an optimal
policy for minimizing the ¯V(θ;s)alone, π∗
s(θ) = arg min π¯V(θ;s) = arg min π−E[π(θ)τs(X)] =
I(τs(X)≥0). We have the following theorem:
Theorem C.1. For the weights ωin problem (A.5) , and the thresholds εin problem (A.1) , the
following statements hold:
•When ε <−E[π∗
s(θ)S(1) + (1 −π∗
s(θ))S(0)], the solution of the constrained optimization
problem is empty.
•When ε≥ −E[π∗
s(θ)S(1)+(1 −π∗
s(θ))S(0)], the relationship between λandαis described
as follows:
–λ= 0, ifε≥ −E[π∗
y(θ)S(1) + (1 −π∗
y(θ))S(0)].
–λis the solution of the equation
−E[I(τy(X) +λτs(X)>0)·τs(X) +µ0(X)] =ε,
if−E[π∗
s(θ)S(1) + (1 −π∗
s(θ))S(0)]< ε≤ −E[π∗
y(θ)S(1) + (1 −π∗
y(θ))S(0)].
It is important to note that for a given λ, we could solve the value of εby solving the equation
−E[I(τy(X) +λτs(X)>0)·τs(X) +µ0(X)] =ε,
as the left side of the equation is a monotone function of λand the solution is unique, and all the
quantities such as τs(X), τy(X),andµ0(X)are identifiable.
15Proof. Initially, we recognize that εcannot be too small so that no policy can satisfy the constraint of
¯V(θ;s)≤ε. The optimal policy of minimizing only the ¯V(θ;s)isπ∗
s(θ) =I(τs(X)≥0). Thus,
ε≥ −E[π∗
s(θ)S(1) + (1 −π∗
s(θ))S(0)].
When ε≥ −E[π∗
s(θ)S(1) + (1 −π∗
s(θ))S(0)].First, the optimal policy of minimizing only ¯V(π;y)
is given as π∗
y(θ) =I(τy(X)≥0).Then, ε≤ −E[π∗
y(θ)S(1) + (1 −π∗
y(θ))S(0)].otherwise,
the constraint will be invalid and the constrained optimization problem becomes an unconstrained
optimization problem with λ= 0.
Second, when −E[π∗
s(θ)S(1) + (1 −π∗
s(θ))S(0)]≤ε≤ −E[π∗
y(θ)S(1) + (1 −π∗
y(θ))S(0)], we
show that the optimal policy π∗(θ)parameterized by θ∗, for the constrained optimization problem
min
θ¯V(θ;y), s.t., ¯V(θ;s)≤ε
is obtained only when ¯V(θ∗;s) =ε. Below, we prove it with the method of reduction to absurdity. If
−E[π∗
s(θ)S(1) + (1 −π∗
s(θ))S(0)]≤ε≤ −E[π∗
y(θ)S(1) + (1 −π∗
y(θ))S(0)], then there are some
units that satisfies {τs(X)<0, τy(X)>0}that not being assigned treatment by π∗(θ); otherwise,
the constraint ¯V(θ;s)≤εwill be violated. Thus, we could find another treatment policy ˜π∗that
assigns more treatment to the units with {τs(X)<0, τy(X)>0}, which yields a lower ¯V(θ;y)
but increases the ¯V(θ;s). That is, ˜π∗will lead to a ¯V(θ;s)closer to εbut has a lower ¯V(θ;y)than
π∗, thus, π∗is not the optimal policy, which contradicts its definition of π∗. Thus, the constrained
optimization problem becomes
min
θ¯V(θ;y), s.t., ¯V(θ;s) =ε.
By introducing the Lagrange multiplier β,π∗satisfies
π∗= arg min
π¯V(θ;y) +β¯V(θ;s) =I(τy(X) +βτs(X)>0),
where βis the solution of ¯V(θ∗;s) =ε, i.e.,
−E[I(τy(X) +βτs(X)>0)τs(X) +µ0(X)] =ε.
This completes the proof for Theorem C.1.
We can further extend Theorem C.1 to situations where there are multiple long-term rewards and
multiple short-term rewards. More generally, for the ε-constraint optimization problem
min
θ¯Vl(θ),s.t.¯Vm(θ)≤εmfor all m= 1, . . . , M, m ̸=l, (A.3)
and the linear weighting optimization problem
min
θ¯V(θ) =MX
i=mωm¯Vm(θ), (A.4)
where ωmis the pre-specified weight for the m-th reward. We have the following theorem:
Theorem 1. For the preference vector ukin problem (1), the weights ωin problem (2), and the
thresholds εin problem (8), the following statements hold:
(a) the connection between εandωis given as
−E[I(τl(X) +ωm
ωlτm(X)>0)·τm(X) +hm(X)] =εm,form= 1···M, and m ̸=l,
where τm(X)is the conditional average causal effects for m-th short/long-term outcome,
τm(X) =(
E[Si(1)−Si(0)|X],ifωmis the weight of ¯V(θ, si),
E[Yj(1)−Yj(0)|X],ifωmis the weight of ¯V(θ, yj),
and
hm(X) =(
E[Si(0)|X],ifωmis the weight of ¯V(θ, si),
E[Yj(0)|X,S, Rj= 1],ifωmis the weight of ¯V(θ, yj),
16andI(·)is the indicator function.
(b) the connection between ωandukis given as
ωm=λm+X
k′∈Iϵ(θ)βk′(uk′m−ukm),form= 1,···, M,
where λmandβk′are defined in Eq. (6),Iϵ(θ) ={k′|Gk′(θ)≥ −ϵ}
Proof. First, for the Theorem1(a), combining the TheoremC.1, more generally, for the ε-constraint
problem
min
θ¯Vl(θ),s.t.¯Vm(θ)≤εmfor all m= 1, . . . , M, m ̸=l, (A.5)
and the linear weighting optimization problem
min
θ¯V(θ) =MX
i=mωm¯Vm(θ), (A.6)
where ωmis the pre-specified weight for the m-th reward. By mathematical induction, we have:
−E[I(τl(X) +ωm/ωlτm(X)>0)τm(X) +hm0(X)] =εi, m= 1···M, and m ̸=l
where τm(X) =(
τsi=E[Si(1)−Si(0)|X],ifωmis the weight of ¯V(θ, si),
τyj=E[Yj(1)−Yj(0)|X],ifωmis the weight of ¯V(θ, yj),
hm0(X) =(
µi0(X) =E[Si|X, A= 0],ifωmis the weight of ¯V(θ, si),
˜mj0(X,S) =E[Yj|X,S, A=a, Rj= 1],ifωmis the weight of ¯V(θ, yj),
This completes the proof for Theorem 1(a)
Second, for the Theorem (b), motivated by [40], for constraint problem
(dt, αt) = arg min
d∈Rn,α∈Rα+1
2||d||2
s.t.∇¯Vm(θt)Td≤α, m = 1, ..., M.
∇Gk′(θt)Td≤α, k′∈ Iϵ(θt),(A.7)
we have
∇Gk′(θt) = (uk′−uk)T∇¯V(θt) =MX
m=1(uk′m−ukm)∇¯Vm(θt). (A.8)
Base on KKT conditions, we have
dt=−(MX
m=1λm∇¯Vm(θt) +X
k′∈Iϵ(θ)βk′∇Gk′(θt)),MX
m=1λm+X
k′∈Iϵ(θ)βk′= 1, (A.9)
where λm≤0andβk′≤0are the Lagrange multipliers. Then, the dual problem is given as
max
λm,βk′−1
2||MX
m=1λm∇¯Vm(θt) +X
k′∈Iϵ(θ)βk′∇Gk′(θt)||2
s.t.MX
m=1λm+X
k′∈Iϵ(θ)βk′= 1, λm≥0, βk′≥0,∀m= 1, ..., M, ∀k′∈ Iϵ(θ).(A.10)
Substituting Eq.(A.8) into Eq.A.9, we have
17dt=−(MX
m=1λm∇¯Vm(θt) +X
k′∈Iϵ(θ)βk′(MX
m=1(uk′m−ukm)∇¯Vm(θt)))
=−(λm+X
k′∈Iϵ(θ)βk′(uk′m−ukm))∇¯Vm(θt)(A.11)
For the problem(A.6), dtis the negative gradient direction. Thus, we have
¯V(θ) =MX
m=1ωm¯Vm(θ),where ωm=λm+X
k′∈Iϵ(θ)βk′(uk′m−ukm), (A.12)
where λmandβk′is obtained from Eq. (A.10) . This shows that the DPPL method can be transformed
into the linear weighting method. This completes the proof for Theorem 1(b)
D Additional Experimental Results
D.1 Sensitivity Analysis on Missing Ratio
In the following, we show more experimental result with missing ratio r= 0.3under IHDP and
JOBS datasets, in table D1.
In additional, We show the corresponding εvalue for each preference vector with different missing
ratio{0.3,0.4,0.5}under IHDP and JOBS datasets, in tables D2, D3 and D7.
Table D1: Comparison of our method (OURS) and linear weighting method (LW) with 10 preference
vectors on IHDP and JOBS, with Short-Term Reward ( S-REWARDS ) and Long-Term Reward
(L-REWARDS ),∆Wand Variance ( S-VAR andL-VAR) as evaluation metrics. The missing ratio
r= 0.3andT= 4. The best result is bolded.
IHDP S-R EWARDS L-R EWARDS ∆W S-VAR L-VAR
PREFERENCE VECTOR OURS LW OURS LW OURS LW OURS LW OURS LW
1 (1.00, 0.00) 523.060 520.760 389.485 385.990 41.174 38.277 12.673 13.621 49.054 58.344
2 (0.98, 0.17) 526.880 524.900 376.918 377.275 36.801 35.989 15.593 12.336 60.458 63.531
3 (0.94, 0.34) 522.300 522.440 386.931 393.534 39.517 42.889 14.998 12.181 51.183 59.204
4 (0.86, 0.50) 522.280 523.300 376.800 376.515 34.642 34.559 12.591 15.040 61.746 51.064
5 (0.76, 0.64) 523.480 518.440 380.358 398.327 36.820 43.285 12.959 15.250 59.013 47.264
6 (0.64, 0.76) 525.560 517.800 387.263 390.716 41.313 39.160 13.703 14.991 56.639 49.135
7 (0.50, 0.87) 523.420 517.440 389.624 385.813 41.424 36.528 12.594 18.091 59.317 59.628
8 (0.34, 0.94) 521.880 515.280 383.755 388.985 35.719 35.034 12.690 14.945 48.189 57.030
9 (0.17, 0.98) 520.300 515.800 386.994 399.012 38.549 42.308 13.622 19.584 56.273 58.640
10 (0.00, 1.00) 522.500 514.980 381.171 385.961 36.737 35.372 12.455 19.711 43.415 49.718
JOBS S-R EWARDS L-R EWARDS ∆W S-VAR L-VAR
PREFERENCE VECTOR OURS LW OURS LW OURS LW OURS LW OURS LW
1 (1.00, 0.00) 1615.540 1612.100 1221.629 1217.543 155.936 152.173 65.386 56.393 98.666 92.897
2 (0.98, 0.17) 1616.240 1600.280 1216.370 1217.547 153.657 146.265 58.903 75.467 87.611 92.085
3 (0.94, 0.34) 1616.380 1595.840 1229.393 1219.475 160.238 145.009 57.370 86.875 95.219 91.009
4 (0.86, 0.50) 1615.700 1592.200 1234.526 1201.847 162.465 134.375 56.556 88.052 89.535 94.647
5 (0.76, 0.64) 1608.600 1595.260 1214.387 1219.359 148.846 144.661 57.526 95.379 79.273 99.852
6 (0.64, 0.76) 1612.120 1591.480 1222.689 1221.671 154.756 143.927 55.446 97.238 94.283 97.522
7 (0.50, 0.87) 1614.240 1588.660 1225.527 1220.786 157.235 142.075 58.574 104.776 85.414 108.986
8 (0.34, 0.94) 1607.880 1585.280 1227.527 1223.203 155.055 141.593 55.923 105.193 85.365 101.940
9 (0.17, 0.98) 1610.600 1584.460 1221.183 1223.446 153.243 141.305 59.996 109.731 92.968 99.344
10 (0.00, 1.00) 1612.740 1590.880 1211.837 1224.826 149.640 145.205 60.330 106.403 92.767 106.106
18Table D2: The εvalues corresponding to each preference vector in the two datasets IHDP and JOBS,
where T= 4andr= 0.3, which are derived according to Theorem 1.
IHDP JOBS
PREFERENCE VECTOR -ε -ε
(1.00, 0.00) 0.827 0.865
(0.98, 0.17) 0.818 0.864
(0.94, 0.34) 0.825 0.859
(0.86, 0.50) 0.830 0.858
(0.77, 0.64) 0.800 0.858
(0.64, 0.76) 0.722 0.841
(0.50, 0.86) 0.592 0.738
(0.34, 0.94) 0.549 0.706
(0.17, 0.98) 0.539 0.726
(0.00, 1.00) 0.557 0.779
Table D3: The εvalues corresponding to each preference vector in the two datasets IHDP and JOBS,
where T= 4andr= 0.4, which are derived according to Theorem 1.
IHDP JOBS
PREFERENCE VECTOR -ε -ε
(1.00, 0.00) 0.822 0.877
(0.98, 0.17) 0.824 0.868
(0.94, 0.34) 0.823 0.852
(0.86, 0.50) 0.820 0.841
(0.77, 0.64) 0.813 0.806
(0.64, 0.76) 0.724 0.798
(0.50, 0.86) 0.524 0.703
(0.34, 0.94) 0.512 0.694
(0.17, 0.98) 0.523 0.667
(0.00, 1.00) 0.523 0.666
Table D4: The εvalues corresponding to the preference vectors in the two datasets IHDP and JOBS,
where T= 4andr= 0.5, which are derived according to Theorem 1.
IHDP JOBS
PREFERENCE VECTOR -ε -ε
(1.00, 0.00) 0.820 0.865
(0.98, 0.17) 0.826 0.863
(0.94, 0.34) 0.821 0.869
(0.86, 0.50) 0.816 0.853
(0.77, 0.64) 0.805 0.816
(0.64, 0.76) 0.679 0.781
(0.50, 0.86) 0.522 0.737
(0.34, 0.94) 0.489 0.722
(0.17, 0.98) 0.490 0.723
(0.00, 1.00) 0.541 0.684
19D.2 Sensitivity Analysis on Preference Vector
In the following, we show more experimental result with different numbers of preference vectors
K={4,8,12}under JOBS datasets, in table D5-D7.
Table D5: Comparison of our method (OURS) and linear weighting method (LW) with 4 pref-
erence vectors on JOBS, with Short-Term Reward ( S-REWARDS ) and Long-Term Reward ( L-
REWARDS ),∆Wand Variance ( S-VAR andL-VAR) as evaluation metrics. The missing ratio
r= 0.2andT= 4. The best result is bolded.
JOBS S-R EWARDS L-R EWARDS ∆W S-VAR L-VAR
PREFERENCE VECTOR OURS LW OURS LW OURS LW OURS LW OURS LW
1(1.00, 0.00) 1616.540 1613.940 1226.493 1232.147 158.869 160.396 60.171 57.758 94.783 92.298
2(0.87, 0.50) 1606.920 1599.620 1226.861 1222.933 154.242 148.628 60.608 77.760 78.282 92.699
3(0.50, 0.86) 1612.500 1601.260 1226.470 1213.741 156.837 144.852 58.438 82.862 87.381 94.363
4(0.00, 1.00) 1615.740 1596.360 1224.834 1223.110 157.639 147.087 58.856 86.287 86.425 87.150
Table D6: Comparison of our method (OURS) and linear weighting method (LW) with 8 pref-
erence vectors on JOBS, with Short-Term Reward ( S-REWARDS ) and Long-Term Reward ( L-
REWARDS ),∆Wand Variance ( S-VAR andL-VAR) as evaluation metrics. The missing ratio
r= 0.2andT= 4. The best result is bolded.
JOBS S-R EWARDS L-R EWARDS ∆W S-VAR L-VAR
PREFERENCE VECTOR OURS LW OURS LW OURS LW OURS LW OURS LW
1(1.00, 0.00) 1616.340 1615.940 1233.387 1227.531 162.215 159.088 55.283 57.953 93.105 95.263
2(0.97, 0.22) 1610.820 1605.220 1228.604 1223.384 157.064 151.654 63.065 66.878 85.019 87.506
3(0.90, 0.43) 1606.260 1599.940 1212.864 1226.675 146.914 150.659 60.809 70.123 95.023 97.185
4(0.78, 0.62) 1614.960 1604.000 1226.162 1222.282 157.913 150.493 62.883 78.589 94.868 89.057
5(0.62, 0.78) 1612.320 1594.220 1226.816 1225.956 156.920 147.440 59.959 77.331 81.906 89.562
6(0.43, 0.90) 1611.860 1593.840 1221.652 1215.291 154.108 141.917 60.059 82.969 99.027 90.661
7(0.22, 0.97) 1612.700 1596.060 1215.533 1224.358 151.468 147.561 56.015 89.441 86.439 92.825
8(0.00, 1.00) 1612.580 1592.260 1233.756 1227.061 160.520 147.012 58.113 83.188 88.058 98.805
Table D7: Comparison of our method (OURS) and linear weighting method (LW) with 12 pref-
erence vectors on JOBS, with Short-Term Reward ( S-REWARDS ) and Long-Term Reward ( L-
REWARDS ),∆Wand Variance ( S-VAR andL-VAR) as evaluation metrics. The missing ratio
r= 0.2andT= 4. The best result is bolded.
JOBS S-R EWARDS L-R EWARDS ∆W S-VAR L-VAR
PREFERENCE VECTOR OURS LW OURS LW OURS LW OURS LW OURS LW
1(1.00, 0.00) 1610.800 1614.600 1231.786 1232.158 158.645 160.731 56.774 60.101 89.746 87.940
2(0.98, 0.14) 1609.720 1610.740 1224.605 1222.904 154.515 154.174 59.048 60.887 88.027 92.283
3(0.95, 0.28) 1613.520 1606.320 1228.204 1226.660 158.214 153.842 59.249 65.024 84.400 79.787
4(0.91, 0.41) 1615.600 1598.940 1223.297 1231.718 156.800 152.681 58.106 70.854 98.610 88.081
5(0.84, 0.54) 1614.140 1604.860 1218.585 1220.647 153.714 150.105 61.420 65.414 89.712 95.134
6(0.75, 0.65) 1615.240 1598.960 1227.954 1225.251 158.949 149.457 54.882 76.213 86.061 89.256
7(0.65, 0.75) 1616.380 1596.160 1226.506 1218.845 158.795 144.854 61.503 77.284 91.857 95.620
8(0.54, 0.84) 1613.420 1598.460 1223.097 1229.293 155.610 151.228 58.566 83.100 92.342 97.554
9(0.41, 0.91) 1612.940 1594.320 1222.586 1224.040 155.115 146.532 57.262 84.387 87.325 93.589
10(0.28, 0.95) 1612.980 1596.880 1230.538 1218.671 159.111 145.127 61.465 81.949 92.381 95.530
11(0.14, 0.98) 1612.160 1591.760 1214.424 1224.345 150.644 145.404 58.148 86.692 80.732 90.234
12(0.00,1.00) 1613.040 1592.440 1228.213 1224.288 157.978 145.716 63.826 87.624 84.520 87.628
20NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: See the abstract and the third and fourth paragraphs in Section 1.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See the conclusion (especially the last sentence.)
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
21Justification: In Section 2.3, we provide a detailed discussion of the adopted assumptions.
Additionally, we present the complete proofs in Appendices A and C.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: In Section 4, we provide a detailed description for the experimental datasets.
In addition, we provide the datasets and codes in supplemental material to ensure easy
reproduction of all reported results.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
22Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide the supplemental material for datasets and codes in a zip file to
ensure easy reproduction of all reported results.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See Section 4 for the detailed description of simulating outcome and experi-
mental details. In addition, we provide the supplemental material for datasets and codes in a
zip file to ensure easy reproduction of all reported results.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report the variance for all experimental results in section 4, by replicating
each experiment 50 times.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
23•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: All experimental results can be easily reproduced on a personal computer.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: All experiments are conducted on publicly available datasets.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: In the first paragraph of Section 1 (Introduction), we outline various potential
applications of our work.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
24•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: All experiments are conducted on publicly available datasets.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: In Section 4, we provide references for the datasets and the simulation setups
of the data-generating process.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
25•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: In Section 4, we provide references for the datasets and the simulation setups of
the data-generating process. In addition, we provide the supplemental material for datasets
and codes in a zip file to ensure easy reproduction of all reported results.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: We don’t use a crowdsourcing service.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
26•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
27