Double-Bayesian Learning
Anonymous Author(s)
Affiliation
Address
email
Abstract
Contemporary machine learning methods will try to approach the Bayes error, as 1
it is the lowest possible error any model can achieve. This paper postulates that 2
any decision is composed of not one but two Bayesian decisions and that decision- 3
making is, therefore, a double-Bayesian process. The paper shows how this duality 4
implies intrinsic uncertainty in decisions and how it incorporates explainability. 5
The proposed approach understands that Bayesian learning is tantamount to finding 6
a base for a logarithmic function measuring uncertainty, with solutions being fixed 7
points. Furthermore, following this approach, the golden ratio describes possible 8
solutions satisfying Bayes’ theorem. The double-Bayesian framework suggests 9
using a learning rate and momentum weight with values similar to those used in 10
the literature to train neural networks with stochastic gradient descent. 11
1 Introduction 12
Despite the progress in machine learning, several problems stand out for which convincing solutions 13
have yet to be found. With massive training sets, enormously sized networks, and immense computing 14
power, training machine learning models has become a brute force approach, arguably more concerned 15
with memorization than generalization. However, quoting from a post by Y . LeCun (Nov. 23, 2023), 16
we know that 17
Animals and humans get very smart very quickly with vastly smaller amounts of training data than 18
current AI systems. Current large language models (LLMs) are trained on text data that would take 19
20,000 years for a human to read. And still, they haven’t learned that if A is the same as B, then B 20
is the same as A. Humans get a lot smarter than that with comparatively little training data. Even 21
corvids, parrots, dogs, and octopuses get smarter than that very, very quickly, with only 2 billion 22
neurons and a few trillion "parameters." 23
This raises the question of whether modern training techniques and principles are actually biologically 24
implemented in the human brain and, if not, what alternative methods could save resources. More 25
efficient methods would be better at generalizing with smaller amounts of training data, which almost 26
certainly would also improve the explainability and interpretability of neural networks. 27
This paper investigates what it takes for a classifier to be optimal. The starting point is Bayes’ theorem, 28
which is the foundation of the Bayes classifier. The Bayes classifier is considered optimal because 29
it minimizes the Bayes risk, meaning it has the smallest probability of misclassification among all 30
classifiers. However, applying the Bayes classifier directly is often impossible because of the difficulty 31
in computing the posterior probabilities. For this reason, most classifiers are trying to approximate 32
the Bayes classifier, like the naïve Bayes classifier, for instance. The information-theoretical analysis 33
presented in this paper splits the decision of a Bayes classifier into two decisions, each following 34
Bayes’ theorem, where one decision can serve as an explanation or verification of the other. Each of 35
the two decision processes faces intrinsic uncertainty, as its decision depends on the output of the 36
other process. The paper will investigate the theoretical ramifications of this approach. As a practical 37
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.result, it will discuss the consequences for two hyperparameters of stochastic gradient descent used 38
in the training process of a neural network: learning rate and momentum weight. 39
The structure of the paper is as follows: After this introduction, Section 2 motivates one of the main 40
ideas, namely that learning to make a decision involves solving two sub-problems and, thus, two 41
decisions. Section 3 discusses Bayes’ theorem, which is central to statistical decision-making and 42
is the starting point of the theoretical approach outlined in the following. Section 4 then introduces 43
the double-Bayesian model as the key concept of the paper. The next section, Section 5, shows 44
how to represent possible solutions of the double-Bayesian decision model. Section 6 discusses the 45
golden ratio, including its functional equations and how it defines a solution to the double-Bayesian 46
model. Then, Section 7 discusses the theoretical implications for training double-Bayesian networks 47
with stochastic gradient descent. Finally, Section 8 summarizes the key concepts, followed by a 48
conclusion. 49
2 Dual decisions 50
Suppose a sender transmits the image on the left-hand side of Figure 1 to a receiver. This image
Figure 1: An image of Rubin’s vase (left) and its inverted counterpart (right) - (Rubin, 1915)
51
depicts Rubin’s vase by the Danish psychologist Edgar Rubin (Rubin, 1915), which shows a vase 52
or two faces looking at each other, depending on the receiver’s perception. The receiver then faces 53
an unsolvable conundrum: 1) If the receiver thinks the image represents a vase, the receiver cannot 54
be certain that the vase is indeed the intended message the sender wanted to convey. Maybe the 55
sender wanted to send the faces. 2) If the receiver is expecting a picture of a vase (or faces) and 56
thus knows the intended message, there is no certainty that an image of a vase has been transmitted. 57
After all, the image could show faces. Therefore, two decisions are involved in making the final 58
interpretation of the image: 1) a decision about the perception of the image (vase or faces), and 2) 59
a decision about whether the perceived image coincides with the intended message, meaning the 60
image transmitted. Both decisions together are fraught with intrinsic uncertainty because deciding the 61
ultimate interpretation of Rubin’s vase, a vase or faces, is impossible. Therefore, neither the sender 62
nor the receiver can make both decisions without uncertainty. Instead, the knowledge is distributed. 63
The sender knows the intended message (a vase or faces) but not the receiver’s perceived image. On 64
the other hand, the receiver knows the perceived image (a vase or faces) but not the intended message. 65
Therefore, the sender and the receiver must collaborate to get the true interpretation across their 66
communication channel. 67
Let the sender and receiver perceive Rubin’s vase differently, with contrary opinions about the 68
foreground and background color (black or white), where the foreground represents the perceived 69
image, either a vase or faces. Furthermore, let the sender and the receiver both be able to send 70
an image of Rubin’s vase to each other so that both become senders and receivers alike and can 71
share their knowledge about the perceived image and intended message. The image that the sender 72
perceives is then the inverted image that the sender perceives. The goal is to collaborate so that the 73
perceived image (foreground) equals the intended message on both ends. 74
A sender can either send the image of Rubin’s vase on the left-hand side of Figure 1 or send the 75
image with colors inverted, as shown on the right-hand side of Figure 1, depending on the perceived 76
image or intended message, respectively. On the other end, the receiver has two options: 1) accept 77
the received image if it is identical to the image expected, or 2) tell the sender to invert the image if it 78
is different. After this feedback, the image on the receiver end will be the same as the image on the 79
2sender side. By making the images on both sides the same, the receiver has completed half of the 80
decision process without making a mistake and has thus behaved optimally. The receiver has ensured 81
that both sides see the same image. It is now up to the sender to make the final, second decision about 82
what image needs to be inverted to arrive at the final interpretation, either the image of the sender 83
or the image of the receiver. Thus, the first process tries to make the images identical, whereas the 84
second process tries to make the images different on both ends to reflect the different perceptions of 85
the sender and receiver. 86
Although described as a sequential process, the two dual decision processes leading to the final 87
interpretation are running in parallel. The sender is also a receiver, and the receiver is also a sender. 88
One of them conveys the correct foreground information (black or white), while the other conveys the 89
message. Note that neither the sender nor the receiver will ever see the true interpretation of the image. 90
The receiver in the example above will never know whether the received image needs to be inverted 91
after making the images identical because this would mean the receiver knows the true interpretation 92
of the image, which is not possible according to the uncertainty principle described above. A similar 93
statement can be made for the sender. The sender and the receiver can be considered dual and 94
complementary forces because of their different interpretations of foreground and background. They 95
make two binary decisions, deciding on the correct foreground color (black or white) and on the 96
message (a vase or faces). They decide whether Rubin’s vase should be interpreted as a white vase, a 97
black vase, white faces, or black faces. 98
3 Bayes theorem 99
Bayes’ theorem is a fundamental law in probability theory that describes the probability of an event 100
given prior knowledge. The theorem is of central importance in machine learning, where it guides the 101
training of machines for decision-making, such as in Bayesian inference or naïve Bayes classification. 102
For two events AandB, with prior probabilities P(A)andP(B), and P(B)̸= 0, Bayes’ theorem 103
states the following: 104
P(A|B) =P(A)·P(B|A)
P(B), (1)
where P(A|B)andP(B|A)are the conditional or posterior probabilities. Thus, P(A|B)is the 105
probability of event Aoccurring when Bis true, and analogously, P(B|A)is the probability of B 106
given that Ais true. 107
For a machine learning application, Awould be the class of an observed input pattern B. The 108
probability P(A)is then the prior probability of class A, andP(B)is the prior probability of seeing 109
pattern B. Consequently, P(A|B)is the posterior probability of class Awhen seeing pattern B, and 110
P(B|A)is the posterior probability of Bwithin A. According to Bayes’ theorem, three probabilities 111
are needed to compute the probability P(A|B)that class Ais observed when seeing pattern B:P(A), 112
P(B), and P(B|A). However, several obstacles prevent Bayes’ theorem from being applied in this 113
way. No particular method can help determine the prior probabilities, which are often unknown. 114
Furthermore, the posterior probability is often not readily available and is approximated by making 115
assumptions about the distribution of Bgiven A, for example, assuming a normal distribution. 116
To cope with these limitations, the next section describes decision-making as a dual process based on 117
Bayes’ theorem, with uncertainty intrinsically involved. 118
4 Double-Bayesian framework 119
The Bayes Theorem is typically stated as in Eq. 1. However, restating the theorem in the following 120
equivalent form highlights the two decision processes for the two subproblems involved, as motivated 121
in Section 2: 122
P(A|B)
P(B|A)=P(A)
P(B)(2)
The left-hand side of Eq. 2 features a fraction of the posterior probabilities, whereas the right-hand 123
side shows the prior probabilities. Following the motivation in Section 2, the posterior probabilities, 124
P(A|B)andP(B|A), can be understood as the probability that AorBis the intended message, 125
respectively. Then, the prior probabilities, P(A)andP(B), would express the probabilities that Aor 126
Bis in the foreground. 127
3With only one equation for four parameters, Eq. 2 is underdetermined. However, it is fair to assume 128
that1−(P(A|B) =P(B|A)and1−P(A) =P(B), which leaves one equation with one parameter 129
on each side. This is possible because either AorBcan be the message or foreground, not both of 130
them at the same time, following again the reasoning in Section 2. Therefore, the intrinsic uncertainty 131
in Bayes’ theorem can be described as follows: if the true foreground is known, then whether the 132
message needs to be swapped is unknown; on the other hand, if the message is known, then whether 133
the foreground needs to be swapped is unknown. The fractions on both sides of Eq. 2 are thus 134
"cognitively entangled." 135
The two remaining unknown parameters can be computed using two separate processes, each adding 136
a constraint to handle the uncertainty. To illustrate this, Eq. 3 restates Bayes’ theorem in yet another 137
way: 138
1 =P(A)
P(B)·P(B|A)
P(A|B)(3)
Assuming that P(B) =P(B|A), Eq. 3 simplifies to P(A|B) =P(A). This assumption of Bbeing 139
independent of Ais fair because, according to the motivation in Section 2, the decisions about the 140
message and the foreground are independent of each other. Under this assumption, only one unknown 141
remains, either P(A|B)orP(A), which follows directly from either P(A)orP(A|B), depending 142
on which is input and which is output. 143
A similar, symmetric statement can be made when using the reciprocals on both sides of Eq. 2, which 144
leads to the following equation: 145
1 =P(B)
P(A)·P(A|B)
P(B|A)(4)
Here, assuming that Ais independent of Bsimplifies Eq. 4 to P(B|A) =P(B). 146
Solving Eq. 2, Eq. 3, or Eq. 4 will be referred to as solving the outer Bayes equation. On the other 147
hand, making both multiplicands on the right-hand side of Eq. 3 or Eq. 4 identical will be referred to 148
as solving the inner Bayes equation, or simply solving the inner equation of Eq. 3 or Eq. 4. For Eq. 3, 149
the inner Bayes equation thus states as follows: 150
P(A)
P(B)=P(B|A)
P(A|B)(5)
Accordingly, the inner Bayes equation for Eq. 4 is obtained by using the reciprocals of the fractions 151
on both sides of Eq. 5: 152
P(B)
P(A)=P(A|B)
P(B|A)(6)
Consequently, the inner Bayes equations can derived by inverting a fraction on one side of Bayes’ 153
theorem, as stated in Eq. 2. The inner Bayes equations are thus "entangled" versions of Bayes’ 154
theorem. 155
The two independent decision processes motivated above are solving the inner and outer Bayes equa- 156
tions. To further formalize these processes, the following section will add a logarithmic expression 157
to Eq. 3 and Eq. 4. Adding a logarithm offers several advantages: 1) using information theory to 158
measure uncertainty; 2) using a reciprocal becomes equivalent to changing the sign of a logarithm; 159
and 3) solving the equation in Bayes’ theorem is reduced to finding a suitable base for a logarithm. 160
5 Fixpoint solutions 161
Using a logarithmic expression in Eq. 3 and Eq. 4 is possible when solutions become fixed points 162
of a logarithmic function. To illustrate this, let logb(x)be the logarithm for an input xand a base b. 163
By definition, the logarithm is the inverse function of taking the power. Therefore, the following 164
equation holds: 165
x= logb(bx) (7)
For the base bof a logarithm, any positive real number can be used so long as b̸= 1. A logarithm 166
computed for base bcan be converted into a logarithm for base b′as follows: 167
log′
b(x) = logb(x)/logb(b′) (8)
4Therefore, the simple term logis used for the logarithm in the following. 168
By applying the logarithm to probabilities, they become information. For the two dual processes 169
above, the information of one process will be its counterpart’s information with a different sign. To 170
achieve this, the following identity is required: 171
log(x) =x (9)
The following lemma states that this requirement can be met for general input values. 172
Lemma: For every x∈R+\ {1}, there exists a base λso that logλ(x) =x. 173
Proof: Letb∈R+\ {1}be an arbitrary basis for which logb(x) =y. Furthermore, let kbe 174
a multiplier so that ky=x. Then, logλ(x) =xforλ=b1/k. This follows from Eq.8, with 175
logλ(x) = logb(x)/logb(λ) = logb(x)/logb(b1/k) = logb(x)·k=x. 176
Note that the common logarithmic rules apply for a fixed λ. However, when requiring a λthat always 177
satisfies logλ(x) =x, computations become ambiguous, as seen here: −logλ(x) =−x̸= 1/x= 178
logλ(1/x). The base λshould be understood as a dynamic parameter that a learning system can 179
modify over time so that logλ(x)converges to the input x. 180
Using the logλexpression of the above Lemma, the Bayes’ equation in Eq. 3 can be written as 181
follows: 182
1 =P(A)
P(B)·logλ 
P(B|A)
P(A|B)!
(10)
Then, the following sequence of transformations can be derived from Eq. 10: 183
P(A|B) =P(A)
P(B)·logλ 
P(B|A)
1!
(11)
=1−P(B)
P(B)·logλ
P(B|A)
(12)
=
1−P(B)
·logλ
P(B)2
(13)
=P(B)·logλ
1−P(B)2
(14)
= 2·P(B)·logλp
1−P(B)2
(15)
= 2·sin(ϕ)·logλ
cos(ϕ)
, (16)
where the last expression holds for an angle ϕ∈
0 ;π
2
. The reasoning behind these transformations 184
is as follows: 185
The first step, Eq. 11, moves the posterior probability P(A|B)back to the left-hand side of the 186
equation. The result is Bayes’ theorem in its original form, as shown in Equation 1. 187
The next step, Eq. 12, replaces P(A)with1−P(B), removing one degree of freedom as motivated 188
above. 189
In the same way, Eq. 13 reformulates Eq. 12, assuming that P(B) =P(B|A)and that the two 190
multipliers on the right-hand side of the equation are equal to meet the inner Bayes equation. 191
Then, Eq. 14 rewrites the right-hand side of Eq. 13, transforming 1−P(B) =P(B)2into the 192
equivalent P(B) = 1−P(B)2, which must hold true to satisfy the inner Bayes equation. 193
Finally, Eq. 15 extracts a factor of two from the logλexpression to get a radical input expression for 194
the logarithm, following the standard rules for logarithms. The new input term to the logλexpression 195
in Eq. 15 allows visualizing all possible solutions to the outer and inner Bayes equations. 196
To illustrate this further, Eq. 16 rewrites Eq. 15 using trigonometric functions and the Pythagorean 197
relationship between sinandcos:sin2ϕ+cos2ϕ= 1, and thus sin ϕ =±p
1−cos2ϕand 198
cos ϕ =±p
1−sin2ϕ. Solutions to the outer and inner Bayes equations then correspond to an 199
angle ϕin Equation 16, depending on the base λ. Thus, solutions are points on the unit circle. 200
By changing the angle ϕin Equation 16, all the possible solutions to the outer and inner Bayes 201
5equations can be visualized. Following the reasoning above, the right-hand side of Eq. 16 represents 202
the inner Bayes equation. Accordingly, after bringing the factor 2on the other side of Eq. 16, 203
the inner Bayes equation is satisfied when sin(ϕ) = cos( ϕ), which is the case for ϕ=π/4, with 204
sin(π/4) = cos( π/4) = 1 /√
2. 205
For the dual process, the logλexpression can be used in combination with the other term of the inner 206
Bayes equation in Eq. 3, as shown here: 207
1 = logλ 
P(A)
P(B)!
·P(B|A)
P(A|B)(17)
Note that the logλexpression has moved to the left compared to the right-hand side of Eq. 10. From 208
this equation, the following sequence of transformations can be derived similar to the transformations 209
above. 210
P(B) = logλ 
P(A)
1!
·P(B|A)
P(A|B)(18)
= logλ
P(A)
·1−P(A|B)
P(A|B)(19)
= logλ
P(A|B)2
·
1−P(A|B)
(20)
= logλ
1−P(A|B)2
·P(A|B) (21)
= 2·logλp
1−P(A|B)2
·P(A|B) (22)
= 2·logλ 
sin(ϕ)
·cos(ϕ) (23)
During this sequence, assumptions similar to the ones in Eq. 12 and Eq. 13 are made. In Eq. 19, 211
P(B|A)was replaced by 1−P(A|B), and Eq. 20 assumes that P(A) =P(A|B). Again, all 212
transformations assume that both multiplicands on the right-hand side are equal to satisfy the inner 213
Bayes equation. 214
The intrinsic uncertainty for the dual processes can again be seen in Eq. 16 and Eq. 23, where it 215
manifests like this: if the base λis known, then the angle ϕis unknown; and vice versa, if ϕis known, 216
thenλis unknown. Each process contributes knowledge about λandϕ, which the other process does 217
not know. 218
The process knowledge about λandϕdoes not need to be "all-or-nothing." The uncertainty ranges 219
continuously between two extremes, and both dual processes can be somewhat knowledgeable about 220
both parameters. When sin(ϕ) = cos( ϕ), with ϕ=π/4, one process has no or full knowledge 221
about one parameter. With ϕapproaching 0orπ/2, where sin(ϕ)andcos(ϕ)become different, this 222
knowledge increases or decreases, respectively. 223
6 Golden ratio 224
The solution to the inner Bayes equation is connected to the golden ratio (Livio, 2002), which becomes 225
evident from the transformations of equations above and the assumptions made for both processes. 226
Based on their right-hand equations, both dual processes must meet the same requirement to satisfy 227
the inner Bayes equation, assuming that logλ(x)produces x. For Eq. 12, with P(B) =P(B|A), 228
and for the corresponding Eq. 19 of the dual process, with P(A) =P(A|B), this requirement can be 229
written as 230
p=1−p
p, (24)
where the variable pis a placeholder for one of the probabilities. Eq. 24 holds true if pis the golden 231
ratio, which is defined by the equivalent quadratic equation, 232
p2+p−1 = 0 , (25)
which has two irrational solutions p1andp2: 233
p1=√
5−1
2≈0.618, (26)
6and 234
p2=−√
5−1
2≈ −1.618 (27)
A key observation is that the complement of both solutions, 1−p, equals their square: 235
1−p=p2(28)
Alternatively, another quadratic equation that may be more frequently encountered in textbooks can 236
be used to arrive at the golden ratio. This equation is obtained by substituting −pforpin Eq. 25: 237
p2−p−1 = 0 (29)
The alternative equation also possesses two irrational solutions, namely the negations of p1andp2: 238
−p1≈ −0.618 and−p2≈1.618 (30)
For these solutions, the complement 1−pis the negative reciprocal: 239
1−p=−1
p(31)
Computing the complement of the golden ratio allows changing viewpoints and switching between 240
the solutions to the inner and outer Bayes equations. This will become important in the next section 241
for training neural networks. 242
The golden ratio is sometimes represented by the letter φin the literature. It is often defined as a 243
single value, usually φ≈1.618, and negative values are not considered (Livio, 2002; Huntley, 1970). 244
However, each of the four solutions to the aforementioned quadratic equations will be referred to as 245
the golden ratio in the context of this paper. 246
7 Theoretical implications 247
Supervised training methods first present a teaching input to a neural network and then try to make 248
the network’s output the same as the input by adjusting the network weights. This equalizing of 249
input and output can be related to equalizing multiplicands to satisfy the inner Bayes equation. For 250
example, in Eq. 18, the term P(B|A)/P(A|B)can be considered as input and the term P(A)in 251
the lambda expression as output. The task of the lambda expression is then to make both terms the 252
same to satisfy the inner Bayes equation. Moreover, the lambda expression logλ 
P(A)
becomes 253
the gradient of a linear function for the outer Bayes equation. These relationships help to determine 254
the optimal learning rate and momentum weight for training based on backpropagation and stochastic 255
gradient descent (SGD). 256
A training method based on backpropagation estimates the gradient of a loss function with respect to 257
each network weight, where the loss function measures the difference between input and network 258
output. Backpropagation methods try to minimize the loss by following the gradient and updating the 259
network weights accordingly (LeCun et al., 2012). They accomplish this for one network layer at 260
a time, iteratively propagating the gradient back from the output layer to the input layer. To move 261
along the gradient towards the minimum of the loss function, a delta is added to each weight, which 262
often has the following form, including a momentum term: 263
∆wij(t) =−η∂L
∂wij(t)+α·∆wij(t−1) (32)
In(32),Lis the loss function, and ∆wij(t)denotes the delta added to each weight wijbetween a 264
nodeiand a node jin the network at training iteration (or time) t. The term ∂L/∂w ij(t)is the partial 265
derivative of the loss function with respect to wij, at time t, which is multiplied with the learning 266
rateη. The sign of ∆wij(t)is negative, so the loss function approaches its minimum. In practice, a 267
momentum term describing the weight change at time t−1,∆wij(t−1), is commonly added. This 268
term is typically multiplied by a weighting factor α, as seen in (32). 269
The traditional understanding is that the momentum term improves stochastic gradient descent by 270
dampening oscillations. However, the dual process model offers another explanation for the per- 271
formance improvement brought about by the momentum term. As of yet, a conclusive theory for 272
the optimal values of the learning rate ηand the momentum weight αhas been lacking. Although 273
7second-order methods (Bengio, 2012; Sutskever et al., 2013; Spall, 2000) as well as adaptive meth- 274
ods (Jacobs, 1988; Kingma and Ba, 2014; Duchi et al., 2011; Tieleman and Hinton, 2012) have been 275
tried with various degrees of success, an ultimate answer has still to be found. Both parameters are 276
usually determined heuristically through empirical experiments or systematic search (Bergstra and 277
Bengio, 2012). Training results can be very sensitive to the value of the learning rate. For example, a 278
small learning rate may result in slow convergence, whereas a larger learning rate may result in the 279
search passing over the minimum loss. Negotiating this delicate trade-off in the regularization of the 280
training process can be time-consuming in practical applications. The literature seems to prefer initial 281
learning rates around 0.01or smaller for SGD, although reported values differ by several orders of 282
magnitude. For the momentum weight, higher initial values around 0.9are more common (Li et al., 283
2020; Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; He et al., 2016). 284
As shown in the following, the proposed dual process model allows deriving theoretical values for 285
both regularization parameters: learning rate ηand momentum weight α. In the weight adjustment 286
given by Eq. 32, each summand represents a gradient of one of the two dual processes. These are 287
the partial derivative ∂L/∂w ij(t)and the momentum term ∆wij(t−1). The momentum weight α 288
follows from the results above, where the lambda expression can be considered as the gradient of 289
the current iteration at time t. The other multiplicand of the inner Bayes equation corresponds to the 290
gradient of the other dual process at time t−1, assuming that both dual processes are interleaved, if 291
not in parallel. 292
The previous sections showed that the inner Bayes equation is met when both summands are equal to 293
sin(π/4) = cos( π/4) = 1 /√
2and when they are equal to the golden ratio. Therefore, the delta at 294
t−1,∆wij(t−1), needs to be multiplied by a constant to obtain the golden ratio. This constant is 295
the momentum weight α, which needs to satisfy α/√
2 =p1, and can thus be computed as follows. 296
α=√
2·p1≈0.874, (33)
where p1is the value of the golden ratio in Eq. 26. So, this logic provides the value of the first 297
regularization term, namely the momentum weight α, with α≈0.874. 298
The learning rate ηcan be derived from the momentum weight αby converting the latter to the 299
corresponding value for the dual process. The dual process does not aim to satisfy the inner Bayes 300
equation with ϕ=π/4. Instead, it aims to satisfy the outer Bayes equation, with ϕ= 0orϕ=π/2, 301
and thus sin(ϕ) = 0 andcos(ϕ) = 1 , orsin(ϕ) = 1 andcos(ϕ) = 0 . By moving in the opposite 302
direction of the gradient of its dual counterpart, the first process can minimize its loss in satisfying 303
the inner Bayes equation. Accordingly, taking the complement of the momentum weight αtwice 304
results in the learning rate ηfor the gradient change at time t. Taking the complement of αtwice can 305
be understood as looking at the same process from a dual point of view. Mathematically, this can be 306
achieved by squaring the simple complement, 1−α. Squaring the complement follows the functional 307
equation of the golden ratio described by Eq.28. Squaring also means bringing the multiplier 2back 308
in, which was extracted from the lambda expression in Eq. 15 and Eq. 22 to represent all solutions 309
graphically. Applying these steps to the momentum weight αthen results in the following equation 310
for the learning rate η: 311
η= (1−α)2≈0.016 (34)
So, this computation provides the value for the second regularization term, learning rate η, with η≈ 312
0.016. 313
8 Discussion 314
Starting from Bayes’ theorem, this paper develops a theoretical framework that describes any decision 315
of a machine classifier as the result of two processes. The first decision process determines the input 316
message; specifically, it decides whether the input is encoded according to its true value or needs to 317
be inverted. On the other hand, the second decision process decides whether the output should be 318
equal to the input or needs to be inverted. Although both decision processes run simultaneously, they 319
are independent processes, with each possessing knowledge not accessible to the other process. What 320
is uncertain for one process is certain for the other, and vice versa. The first process does not know 321
whether the input should be equal to the output, and conversely, the second process does not know 322
whether the input needs to be inverted. This means a binary decision always involves two bits, one 323
indicating the encoding of the input and the other defining the relationship between input and output. 324
8However, practically, only one of the two processes can be performed at a time, leaving one bit of 325
uncertainty for one of the processes. 326
Theoretically, the framework proposed here formulates this duality with two processes having 327
different perceptions of zero and one (black and white). The output of one process is the input to 328
the other process. While one process tries to make its output equal to its input, the other aims for 329
the opposite and tries to make its output as different as possible. The mathematical definitions of 330
these processes are defined by the outer and inner Bayes equation, the latter of which is an entangled 331
version of the original Bayes’ theorem. By introducing the logarithm, each process is given a control 332
parameter, namely the base of the logarithm, to achieve its goal. This parameter, which is essentially 333
a multiplier, allows each process to control the magnitude of the input/output. 334
The solution space of the proposed double-Bayesian decision framework can be visualized with the 335
trigonometric functions sinandcos. Furthermore, the golden ratio defines solutions to the inner 336
Bayes equation. Connecting these two observations leads to specific values for momentum weight 337
and learning rate for stochastic gradient descent, which tries to minimize the difference between 338
training input and output during training. 339
The supplemental material to this paper contains experiments for the MNIST dataset (LeCun et al., 340
accessed May 21, 2024), where the proposed double-Bayesian learning framework is practically 341
evaluated. The theoretical parameters found in this paper did, in fact, provide the best performance 342
for a network trained with stochastic gradient descent in a large grid search for learning rate and 343
momentum weight. 344
9 Conclusion 345
Three primary characteristics define the work presented in this paper: First, a double-Bayesian 346
approach that understands learning as a process involving two Bayesian decisions instead of a single 347
decision, like in contemporary approaches. Second, solving a Bayesian decision problem is equivalent 348
to finding a fixed point for a logarithmic function measuring uncertainty. Third, the golden ratio 349
defines solutions to a Bayesian decision problem. These three characteristics make the proposed 350
approach novel and unique. 351
The double-Bayesian framework leads to new theoretical results for training neural networks, particu- 352
larly specific hyperparameter values for backpropagation and gradient descent. These results are in 353
contrast with other gradient descent heuristics in the literature that either use dynamic hyperparame- 354
ters or second-order methods for adjusting parameters during training. It will be interesting to see how 355
this conceptual difference will be resolved in the future. The proposed framework offers new ways to 356
understand how neural networks make decisions and may thus contribute to the interpretability and 357
explainability of neural networks, an actively investigated research area. 358
The proposed framework may also help build bridges to other disciplines like neuroscience or 359
physics. For example, representing all possible solutions to a double-Bayesian decision by means 360
of trigonometric functions, as done in this paper, introduces waves. Incorporating brain waves into 361
machine learning, a feature that traditional machine learning approaches are arguably lacking, would 362
likely entail a better understanding of learning in general. This better understanding could mean 363
training methods for smaller networks that could achieve the same performance with less training 364
data, as motivated at the beginning of this paper. 365
Another example of a discipline that could be related to this work is quantum mechanics. One of the 366
fundamental concepts in quantum mechanics is Heisenberg’s uncertainty principle, which states that 367
certain pairs of physical properties, such as the position and momentum of an electron, cannot be 368
measured with absolute certainty. The more accurately one property is measured, the less is known 369
about the other property. The proposed double-Bayesian framework incorporates such an intrinsic 370
uncertainty and makes a connection to Bayesian decision theory, which could lead to new insights. 371
Although empirical evidence in the literature supports the theoretical hyperparameter values derived 372
in this paper, and the experiments in the supplemental material show that these values outperform 373
other value pairs, more practical experiments are needed to corroborate these values. To address this 374
limitation, future work will validate the practicality of the derived hyperparameter values in additional 375
experiments across different domains and compare their performance with the performance of other 376
values and other optimization strategies. 377
9References 378
Y . Bengio. Practical recommendations for gradient-based training of deep architectures. In Neural 379
networks: Tricks of the trade , pages 437–478. Springer, 2012. 380
J. Bergstra and Y . Bengio. Random search for hyper-parameter optimization. Journal of machine 381
learning research , 13(2), 2012. 382
L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel, V . Niculae, P. Prettenhofer, 383
A. Gramfort, J. Grobler, R. Layton, J. VanderPlas, A. Joly, B. Holt, and G. Varoquaux. API 384
design for machine learning software: experiences from the scikit-learn project. In ECML PKDD 385
Workshop: Languages for Data Mining and Machine Learning , pages 108–122, 2013. 386
J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic 387
optimization. Journal of machine learning research , 12(7), 2011. 388
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings 389
of the IEEE Conference on computer vision and pattern recognition , pages 770–778, 2016. 390
H. Huntley. The Divine Proportion . Dover Publications, 1970. 391
R. Jacobs. Increased rates of convergence through learning rate adaptation. Neural networks , 1(4): 392
295–307, 1988. 393
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 394
2014. 395
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural 396
networks. In Advances in neural information processing systems , pages 1097–1105, 2012. 397
Y . LeCun, L. Bottou, G. Orr, and K. Müller. Efficient backprop. In Neural networks: Tricks of the 398
trade , pages 9–48. Springer, 2012. 399
Y . LeCun, C. Cortes, and C. Burges. The MNIST Database , accessed May 21, 2024. URL http: 400
//yann.lecun.com/exdb/mnist/ . 401
H. Li, P. Chaudhari, H. Yang, M. Lam, A. Ravichandran, R. Bhotika, and S. Soatto. Rethinking the 402
hyperparameters for fine-tuning. arXiv preprint arXiv:2002.11770 , 2020. 403
M. Livio. The Golden Ratio . Random House, Inc., 2002. 404
F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- 405
hofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and 406
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 407
12:2825–2830, 2011. 408
E. Rubin. Rubin Vase . Wikimedia Commons (last accessed March 4, 2022, CC BY-SA 3.0), 1915. 409
URL https://commons.wikimedia.org/wiki/File:Facevase.png . 410
Scikit-learn developers (BSD License). Scikit-learn machine learning library , accessed May 21, 411
2024. URL https://scikit-learn.org/stable/modules/generated/sklearn.model_ 412
selection.StratifiedShuffleSplit.html . 413
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. 414
arXiv preprint arXiv:1409.1556 , 2014. 415
J. Spall. Adaptive stochastic approximation by the simultaneous perturbation method. IEEE transac- 416
tions on automatic control , 45(10):1839–1853, 2000. 417
I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum 418
in deep learning. In International Conference on Machine Learning , pages 1139–1147, 2013. 419
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its 420
recent magnitude. COURSERA: Neural networks for machine learning , 4(2):26–31, 2012. 421
10NeurIPS Paper Checklist 422
1.Claims 423
Question: Do the main claims made in the abstract and introduction accurately reflect the 424
paper’s contributions and scope? 425
Answer: [Yes] 426
Justification: This is a theoretical paper that tries to explain hyperparameter values that 427
have been successfully used in the literature. The paper investigates what it takes for a 428
classifier to be optimal, as stated in the introduction. Although the literature and the 429
practical experiments provided in the supplemental material support the theoretical 430
results, providing more practical experiments would be desirable. 431
Guidelines: 432
•The answer NA means that the abstract and introduction do not include the claims 433
made in the paper. 434
•The abstract and/or introduction should clearly state the claims made, including the 435
contributions made in the paper and important assumptions and limitations. A No or 436
NA answer to this question will not be perceived well by the reviewers. 437
•The claims made should match theoretical and experimental results, and reflect how 438
much the results can be expected to generalize to other settings. 439
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 440
are not attained by the paper. 441
2.Limitations 442
Question: Does the paper discuss the limitations of the work performed by the authors? 443
Answer: [Yes] 444
Justification: The limitations are discussed at the very end of the paper in the con- 445
clusion. A comparison with other hyperparameter optimization strategies would be 446
desirable to corroborate the theoretical results even more. Specifically, a systematic 447
comparison with second-order methods and other methods that dynamically adapt 448
hyperparameters during training should shed more light on the performance of this 449
approach. 450
Guidelines: 451
•The answer NA means that the paper has no limitation while the answer No means that 452
the paper has limitations, but those are not discussed in the paper. 453
• The authors are encouraged to create a separate "Limitations" section in their paper. 454
•The paper should point out any strong assumptions and how robust the results are to 455
violations of these assumptions (e.g., independence assumptions, noiseless settings, 456
model well-specification, asymptotic approximations only holding locally). The authors 457
should reflect on how these assumptions might be violated in practice and what the 458
implications would be. 459
•The authors should reflect on the scope of the claims made, e.g., if the approach was 460
only tested on a few datasets or with a few runs. In general, empirical results often 461
depend on implicit assumptions, which should be articulated. 462
•The authors should reflect on the factors that influence the performance of the approach. 463
For example, a facial recognition algorithm may perform poorly when image resolution 464
is low or images are taken in low lighting. Or a speech-to-text system might not be 465
used reliably to provide closed captions for online lectures because it fails to handle 466
technical jargon. 467
•The authors should discuss the computational efficiency of the proposed algorithms 468
and how they scale with dataset size. 469
•If applicable, the authors should discuss possible limitations of their approach to 470
address problems of privacy and fairness. 471
•While the authors might fear that complete honesty about limitations might be used by 472
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 473
limitations that aren’t acknowledged in the paper. The authors should use their best 474
11judgment and recognize that individual actions in favor of transparency play an impor- 475
tant role in developing norms that preserve the integrity of the community. Reviewers 476
will be specifically instructed to not penalize honesty concerning limitations. 477
3.Theory Assumptions and Proofs 478
Question: For each theoretical result, does the paper provide the full set of assumptions and 479
a complete (and correct) proof? 480
Answer: [Yes] 481
Justification: All assumptions are discussed in detail, and one proof has been included. 482
Guidelines: 483
• The answer NA means that the paper does not include theoretical results. 484
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 485
referenced. 486
•All assumptions should be clearly stated or referenced in the statement of any theorems. 487
•The proofs can either appear in the main paper or the supplemental material, but if 488
they appear in the supplemental material, the authors are encouraged to provide a short 489
proof sketch to provide intuition. 490
•Inversely, any informal proof provided in the core of the paper should be complemented 491
by formal proofs provided in appendix or supplemental material. 492
• Theorems and Lemmas that the proof relies upon should be properly referenced. 493
4.Experimental Result Reproducibility 494
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 495
perimental results of the paper to the extent that it affects the main claims and/or conclusions 496
of the paper (regardless of whether the code and data are provided or not)? 497
Answer: [Yes] 498
Justification: Experimental results are listed in the supplemental material, with infor- 499
mation to reproduce the results, including the code itself. 500
Guidelines: 501
• The answer NA means that the paper does not include experiments. 502
•If the paper includes experiments, a No answer to this question will not be perceived 503
well by the reviewers: Making the paper reproducible is important, regardless of 504
whether the code and data are provided or not. 505
•If the contribution is a dataset and/or model, the authors should describe the steps taken 506
to make their results reproducible or verifiable. 507
•Depending on the contribution, reproducibility can be accomplished in various ways. 508
For example, if the contribution is a novel architecture, describing the architecture fully 509
might suffice, or if the contribution is a specific model and empirical evaluation, it may 510
be necessary to either make it possible for others to replicate the model with the same 511
dataset, or provide access to the model. In general. releasing code and data is often 512
one good way to accomplish this, but reproducibility can also be provided via detailed 513
instructions for how to replicate the results, access to a hosted model (e.g., in the case 514
of a large language model), releasing of a model checkpoint, or other means that are 515
appropriate to the research performed. 516
•While NeurIPS does not require releasing code, the conference does require all submis- 517
sions to provide some reasonable avenue for reproducibility, which may depend on the 518
nature of the contribution. For example 519
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 520
to reproduce that algorithm. 521
(b)If the contribution is primarily a new model architecture, the paper should describe 522
the architecture clearly and fully. 523
(c)If the contribution is a new model (e.g., a large language model), then there should 524
either be a way to access this model for reproducing the results or a way to reproduce 525
the model (e.g., with an open-source dataset or instructions for how to construct 526
the dataset). 527
12(d)We recognize that reproducibility may be tricky in some cases, in which case 528
authors are welcome to describe the particular way they provide for reproducibility. 529
In the case of closed-source models, it may be that access to the model is limited in 530
some way (e.g., to registered users), but it should be possible for other researchers 531
to have some path to reproducing or verifying the results. 532
5.Open access to data and code 533
Question: Does the paper provide open access to the data and code, with sufficient instruc- 534
tions to faithfully reproduce the main experimental results, as described in supplemental 535
material? 536
Answer: [Yes] 537
Justification: Please see the supplemental material for the code and information about 538
reproducing the experimental results. The publicly available MNIST database has 539
been used for the experiments. 540
Guidelines: 541
• The answer NA means that paper does not include experiments requiring code. 542
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 543
public/guides/CodeSubmissionPolicy ) for more details. 544
•While we encourage the release of code and data, we understand that this might not be 545
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 546
including code, unless this is central to the contribution (e.g., for a new open-source 547
benchmark). 548
•The instructions should contain the exact command and environment needed to run to 549
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 550
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 551
•The authors should provide instructions on data access and preparation, including how 552
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 553
•The authors should provide scripts to reproduce all experimental results for the new 554
proposed method and baselines. If only a subset of experiments are reproducible, they 555
should state which ones are omitted from the script and why. 556
•At submission time, to preserve anonymity, the authors should release anonymized 557
versions (if applicable). 558
•Providing as much information as possible in supplemental material (appended to the 559
paper) is recommended, but including URLs to data and code is permitted. 560
6.Experimental Setting/Details 561
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 562
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 563
results? 564
Answer: [Yes] 565
Justification: Please see the information in the supplemental material. 566
Guidelines: 567
• The answer NA means that the paper does not include experiments. 568
•The experimental setting should be presented in the core of the paper to a level of detail 569
that is necessary to appreciate the results and make sense of them. 570
•The full details can be provided either with the code, in appendix, or as supplemental 571
material. 572
7.Experiment Statistical Significance 573
Question: Does the paper report error bars suitably and correctly defined or other appropriate 574
information about the statistical significance of the experiments? 575
Answer: [NA] 576
Justification: The paper provides theoretical results. For the experimental results in 577
the supplemental material, only the relative performance to other hyperparameter 578
combinations was investigated, significant or not, to see whether the proposed values 579
13define the optimum or are at least close to it. To compare the proposed method and 580
values with other optimization methods, future experiments may require significance 581
tests. 582
Guidelines: 583
• The answer NA means that the paper does not include experiments. 584
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 585
dence intervals, or statistical significance tests, at least for the experiments that support 586
the main claims of the paper. 587
•The factors of variability that the error bars are capturing should be clearly stated (for 588
example, train/test split, initialization, random drawing of some parameter, or overall 589
run with given experimental conditions). 590
•The method for calculating the error bars should be explained (closed form formula, 591
call to a library function, bootstrap, etc.) 592
• The assumptions made should be given (e.g., Normally distributed errors). 593
•It should be clear whether the error bar is the standard deviation or the standard error 594
of the mean. 595
•It is OK to report 1-sigma error bars, but one should state it. The authors should 596
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 597
of Normality of errors is not verified. 598
•For asymmetric distributions, the authors should be careful not to show in tables or 599
figures symmetric error bars that would yield results that are out of range (e.g. negative 600
error rates). 601
•If error bars are reported in tables or plots, The authors should explain in the text how 602
they were calculated and reference the corresponding figures or tables in the text. 603
8.Experiments Compute Resources 604
Question: For each experiment, does the paper provide sufficient information on the com- 605
puter resources (type of compute workers, memory, time of execution) needed to reproduce 606
the experiments? 607
Answer: [Yes] 608
Justification: Please see the supplemental material for more information. 609
Guidelines: 610
• The answer NA means that the paper does not include experiments. 611
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 612
or cloud provider, including relevant memory and storage. 613
•The paper should provide the amount of compute required for each of the individual 614
experimental runs as well as estimate the total compute. 615
•The paper should disclose whether the full research project required more compute 616
than the experiments reported in the paper (e.g., preliminary or failed experiments that 617
didn’t make it into the paper). 618
9.Code Of Ethics 619
Question: Does the research conducted in the paper conform, in every respect, with the 620
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 621
Answer: [Yes] 622
Justification: There is no violation of the NeurIPS Code of Ethics. 623
Guidelines: 624
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 625
•If the authors answer No, they should explain the special circumstances that require a 626
deviation from the Code of Ethics. 627
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 628
eration due to laws or regulations in their jurisdiction). 629
10.Broader Impacts 630
14Question: Does the paper discuss both potential positive societal impacts and negative 631
societal impacts of the work performed? 632
Answer: [NA] 633
Justification: The paper proposes a generic method to find hyperparameter values 634
for optimizing the performance of neural networks. Its societal impacts, therefore, 635
correlate with the risks of machine learning in general, which does not need to be 636
pointed out in particular according to the guidelines below. 637
Guidelines: 638
• The answer NA means that there is no societal impact of the work performed. 639
•If the authors answer NA or No, they should explain why their work has no societal 640
impact or why the paper does not address societal impact. 641
•Examples of negative societal impacts include potential malicious or unintended uses 642
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 643
(e.g., deployment of technologies that could make decisions that unfairly impact specific 644
groups), privacy considerations, and security considerations. 645
•The conference expects that many papers will be foundational research and not tied 646
to particular applications, let alone deployments. However, if there is a direct path to 647
any negative applications, the authors should point it out. For example, it is legitimate 648
to point out that an improvement in the quality of generative models could be used to 649
generate deepfakes for disinformation. On the other hand, it is not needed to point out 650
that a generic algorithm for optimizing neural networks could enable people to train 651
models that generate Deepfakes faster. 652
•The authors should consider possible harms that could arise when the technology is 653
being used as intended and functioning correctly, harms that could arise when the 654
technology is being used as intended but gives incorrect results, and harms following 655
from (intentional or unintentional) misuse of the technology. 656
•If there are negative societal impacts, the authors could also discuss possible mitigation 657
strategies (e.g., gated release of models, providing defenses in addition to attacks, 658
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 659
feedback over time, improving the efficiency and accessibility of ML). 660
11.Safeguards 661
Question: Does the paper describe safeguards that have been put in place for responsible 662
release of data or models that have a high risk for misuse (e.g., pretrained language models, 663
image generators, or scraped datasets)? 664
Answer: [NA] 665
Justification: There is no risk of misusing the proposed method beyond misusing 666
machine learning in general. 667
Guidelines: 668
• The answer NA means that the paper poses no such risks. 669
•Released models that have a high risk for misuse or dual-use should be released with 670
necessary safeguards to allow for controlled use of the model, for example by requiring 671
that users adhere to usage guidelines or restrictions to access the model or implementing 672
safety filters. 673
•Datasets that have been scraped from the Internet could pose safety risks. The authors 674
should describe how they avoided releasing unsafe images. 675
•We recognize that providing effective safeguards is challenging, and many papers do 676
not require this, but we encourage authors to take this into account and make a best 677
faith effort. 678
12.Licenses for existing assets 679
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 680
the paper, properly credited and are the license and terms of use explicitly mentioned and 681
properly respected? 682
Answer: [Yes] 683
15Justification: The main paper cites relevant references for the scientific content and the 684
supplemental material provides more details about the data and software sources. 685
Guidelines: 686
• The answer NA means that the paper does not use existing assets. 687
• The authors should cite the original paper that produced the code package or dataset. 688
•The authors should state which version of the asset is used and, if possible, include a 689
URL. 690
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 691
•For scraped data from a particular source (e.g., website), the copyright and terms of 692
service of that source should be provided. 693
•If assets are released, the license, copyright information, and terms of use in the 694
package should be provided. For popular datasets, paperswithcode.com/datasets 695
has curated licenses for some datasets. Their licensing guide can help determine the 696
license of a dataset. 697
•For existing datasets that are re-packaged, both the original license and the license of 698
the derived asset (if it has changed) should be provided. 699
•If this information is not available online, the authors are encouraged to reach out to 700
the asset’s creators. 701
13.New Assets 702
Question: Are new assets introduced in the paper well documented and is the documentation 703
provided alongside the assets? 704
Answer: [Yes] 705
Justification: The paper provides new assets in the form of knowledge about hyperpa- 706
rameter values to train neural networks with gradient descent and software to find 707
the best combination of momentum weight and learning rate with a grid search. Each 708
asset is documented in the paper and supplemental material, respectively. 709
Guidelines: 710
• The answer NA means that the paper does not release new assets. 711
•Researchers should communicate the details of the dataset/code/model as part of their 712
submissions via structured templates. This includes details about training, license, 713
limitations, etc. 714
•The paper should discuss whether and how consent was obtained from people whose 715
asset is used. 716
•At submission time, remember to anonymize your assets (if applicable). You can either 717
create an anonymized URL or include an anonymized zip file. 718
14.Crowdsourcing and Research with Human Subjects 719
Question: For crowdsourcing experiments and research with human subjects, does the paper 720
include the full text of instructions given to participants and screenshots, if applicable, as 721
well as details about compensation (if any)? 722
Answer: [NA] 723
Justification: The paper does not involve crowdsourcing nor research with human 724
subjects. 725
Guidelines: 726
•The answer NA means that the paper does not involve crowdsourcing nor research with 727
human subjects. 728
•Including this information in the supplemental material is fine, but if the main contribu- 729
tion of the paper involves human subjects, then as much detail as possible should be 730
included in the main paper. 731
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 732
or other labor should be paid at least the minimum wage in the country of the data 733
collector. 734
1615.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 735
Subjects 736
Question: Does the paper describe potential risks incurred by study participants, whether 737
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 738
approvals (or an equivalent approval/review based on the requirements of your country or 739
institution) were obtained? 740
Answer: [NA] 741
Justification: The paper does not involve crowdsourcing nor research with human 742
subjects. 743
Guidelines: 744
•The answer NA means that the paper does not involve crowdsourcing nor research with 745
human subjects. 746
•Depending on the country in which research is conducted, IRB approval (or equivalent) 747
may be required for any human subjects research. If you obtained IRB approval, you 748
should clearly state this in the paper. 749
•We recognize that the procedures for this may vary significantly between institutions 750
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 751
guidelines for their institution. 752
•For initial submissions, do not include any information that would break anonymity (if 753
applicable), such as the institution conducting the review. 754
A Appendix / supplemental material 755
Two grid searches for the publicly available MNIST dataset were performed to corroborate the 756
learning rate and momentum weight derived in the main paper (LeCun et al., accessed May 21, 2024). 757
The MNIST dataset contains gray-scale images of handwritten digits and is one of the prominent 758
datasets used to evaluate machine learning methods. It is split into a training and a test set, where the 759
latter serves as a standard of comparison. Figure 2 shows an example of the MNIST data.
Figure 2: A slightly enlarged example from the MNIST dataset showing a handwritten digit (4).
760
A.1 Experiments 761
The grid searches were performed on the full-size MNIST dataset and a smaller version of MNIST 762
containing only 50% of the training data. In the latter case, a stratified sampling method named 763
StratifiedShuffleSplit was used to create a stratified random subset of the training samples (Scikit- 764
learn developers , BSD License; Pedregosa et al., 2011; Buitinck et al., 2013). This ensured that the 765
class distribution in the training subset was the same as in the original full-size training set. The 766
degradation in dataset size allowed observing how each optimizer performed under varying amounts 767
of training data, assuming that providing less training data posed a harder problem. 768
A deep learning model was trained based on a convolutional neural network (CNN). The model 769
consisted of two convolutional layers, each followed by a ReLU activation function and a max 770
pooling operation. The first convolutional layer had a single-channel input (grayscale image) and 771
applied 16 filters, followed by a second convolutional layer that expanded the channel size to 32. 772
Both convolutional layers used a 3x3 kernel size, a stride of one, and a padding of one. After each 773
convolution, a ReLU activation function introduced non-linearity, and a max pooling operation with 774
17a 2x2 kernel and stride reduced the spatial dimensions by half. A dropout layer with a rate of 0.25 775
was applied after flattening the output to prevent overfitting. The network concluded with two fully 776
connected layers with a final output of 10 classes, where the maximum output value determined the 777
class of an input image. The number of parameters was around two hundred thousand for an MNIST 778
input image of size 28x28. A weight initialization was performed using the Kaiming uniform method. 779
No data augmentation techniques were applied; however, the input was normalized to the range [-1,1]. 780
The training used a batch size of 64 and was conducted over 30 epochs, employing cross entropy 781
as the loss function. The sizes of the training, validation, and test datasets were 54,000, 6,000, and 782
10,000, respectively. Finally, the model’s performance was assessed through 10-fold cross-validation. 783
A.2 Results 784
The results of both grid searches are shown in Figure 3 for the full-size training set and in Figure 3 785
for the smaller training set with 50% of the size. The following values were used as momentum
Figure 3: Grid search results for MNIST
786
weights for each grid search: 0, 0.2, 0.4, 0.6, 0.8, 0.825, 0.85, 0.874, 0.9, and 0.925. On the other 787
hand, the following values were used as learning rates: 0.0001, 0.001, 0.01, 0.016, 0.1, 0.2. These 788
values included the momentum weight derived in the paper ( α≈0.874) and the derived learning 789
rate (η≈0.016). Other values were chosen based on their use in the literature or to increase the 790
resolution around the derived theoretical values. All possible combinations of values span a 6x10 791
grid. The color of each square in the grids of Figure 3 and Figure 4 represent the performance of the 792
corresponding pair of momentum weight and learning rate, with lighter colors representing higher 793
performance. Green rectangles indicate the top ten performing pairs, whereas blue rectangles show 794
the best-performing pair. Note that more than one pair can share the best performance, as in Figure 3. 795
Figure 3 shows that no pair of momentum weight and learning rate provides better performance on 796
the full-size MNIST set than the pair derived in the paper, (0.016,0.874), although this pair has to 797
share its first place with other pairs. The classification accuracies for the reduced training set size 798
are slightly lower in the table of Figure 4, as one would expect for a problem with less training data. 799
18Nevertheless, the theoretical values derived in the paper for momentum weight and learning rate show
Figure 4: Grid search results for MNIST using only 50% of the training data
800
again the best performance. 801
A.3 Computational environment and runtime 802
The software was developed using Python 3.10, and the Convolutional Neural Network (CNN) model 803
was implemented in Pytorch 2.2.2. For each combination of learning rate and momentum weight (60 804
combinations in total), the training time was approximately three hours for 100% of the training set 805
size and about 1.5 hours for 50% of the training set. Consequently, the cumulative GPU time for all 806
experiments was approximately (3 + 1.5) ×60 hours, which is 270 hours. The average memory usage 807
was roughly 1 GB for each combination. For more information about the software requirements and 808
workflow, see the Readme file uploaded as supplemental material together with the code. 809
A.4 Computing cluster 810
Figure 5 shows an overview of the GPU computing cluster that was available for the experiments, 811
including the type of GPUs among which the processing was distributed. 812
19GPU nodes  Processor cores per node  Memory  Network  
36 32 x 2.8 GHz (AMD 
Epyc 7543p)  
hyperthreading enabled  
256 MB level 3 cache  
4 x NVIDIA A100 GPUs  
(80 GB VRAM, 6912 
cores, 432 Tensor cores)  
NVLINK  256 GB  200 Gb/s HDR Infiniband 
(1:1)  
56 36 x 2.3 GHz (Intel Gold 
6140)  
hyperthreading enabled  
25 MB secondary cache  
4 x NVIDIA V100 -
SXM2 GPUs  (32 GB 
VRAM, 5120 cores, 640 
Tensor cores)  
NVLINK  384 GB  200 Gb/s HDR Infiniband 
(1:1)  
8 28 x 2.4 GHz (Intel E5 -
2680v4)  
hyperthreading enabled  
35 MB secondary cache  
4 x NVIDIA V100 GPUs  
(16 GB VRAM, 5120 
cores, 640 Tensor cores)  128 GB   
56 Gb/s FDR Infiniband 
(1.11:1)  
48 28 x 2.4 GHz (Intel E5 -
2680v4)  
hyperthreading enabled  
35 MB secondary cache  
4 x NVIDIA P100 GPUs  
(16 GB VRAM, 3584 
cores)  128 GB  56 Gb/s FDR Infiniband 
(1.11:1)  
72 28 x 2.4 GHz (Intel E5 -
2680v4)  
hyperthreading enabled  
35 MB secondary cache  
2 x NVIDIA K80 GPUs  
with 2 x GK210 GPUs 
each (24 GB VRAM, 
4992 cores)  256 GB  56 Gb/s FDR Infiniband 
(1.11:1)  
 
 
 
 
 
 
 
 Figure 5: GPU computing cluster
20