UIR-LoRA: Achieving Universal Image Restoration
through Multiple Low-Rank Adaptation
Anonymous Author(s)
Affiliation
Address
email
Abstract
Existing unified methods typically treat multi-degradation image restoration as a 1
multi-task learning problem. Despite performing effectively compared to single 2
degradation restoration methods, they overlook the utilization of commonalities 3
and specificities within multi-task restoration, thereby impeding the model‚Äôs per- 4
formance. Inspired by the success of deep generative models and fine-tuning tech- 5
niques, we proposed a universal image restoration framework based on multiple 6
low-rank adapters (LoRA) from multi-domain transfer learning. Our framework 7
leverages the pre-trained generative model as the shared component for multi- 8
degradation restoration and transfers it to specific degradation image restoration 9
tasks using low-rank adaptation. Additionally, we introduce a LoRA composing 10
strategy based on the degradation similarity, which adaptively combines trained 11
LoRAs and enables our model to be applicable for mixed degradation restoration. 12
Extensive experiments on multiple and mixed degradations demonstrate that the 13
proposed universal image restoration method not only achieves higher fidelity and 14
perceptual image quality but also has better generalization ability than other unified 15
image restoration models. 16
1 Introduction 17
In the wild, a range of distortions commonly appear in captured images, including noise[ 56], blur[ 14, 18
47,6], low light[ 58,22,8], and various weather degradations[ 15,51,54,45]. As a fundamental task 19
in low-level vision, image restoration aims to eliminate these distortions and recover sharp details and 20
original scene information from corrupted images. With the assistance of deep learning, an abundance 21
of restoration approaches [ 56,3,54,2,16,14,53] have made significant progress in eliminating 22
single degradation from images. However, these approaches typically require additional training from 23
scratch on specific image pairs in multi-degraded scenarios, which leads to inconvenience in usage 24
and limited generalization ability. 25
For simplicity and practicality, some existing works [ 15,31,55]consider training a unified model 26
(also called all-in-one model) to handle multiple degradations as multi-task learning. These studies 27
primarily explore how to discern degradation from the image and integrate it into the restoration 28
network. Nevertheless, these methods share all parameters across different degradations, resulting in 29
gradient conflicts [40, 52] that hinder further improvement of unified models‚Äô performance. 30
Digging deeper, the underlying issue lies in that the similarities among different image restoration 31
tasks and the inherent specificity of each degradation are not well considered and utilized in the 32
training. This limitation drives us to seek solutions for multi-degradation restoration by leveraging 33
both commonalities and specificities. 34
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.noisyhazyblurrylow light‚Ä¶Pre-trained generative modelclean imagedomain
specificspecificspecificspecificùëùùë•
‚Ä¶
sharedtransfernoise, text, imageImageGenerationImage Restoration
Figure 1: Motivation of our work. A pre-trained generative model serves as the shared component
and minimal parameters are added to model the specificity of each degradation restoration task.
Inspired by the successes of deep generative models[ 37,36,35] and fine-tuning techniques[ 11,10,4], 35
we propose addressing the aforementioned issue from the perspective of multi-domain transfer 36
learning, as presented in Figure 1. The pre-trained generative model exhibits powerful capabilities, 37
implying rich prior knowledge of clear image distribution p(x), which is exactly what is needed 38
for image restoration. Since image prior p(x)is degradation-agnostic and applicable to all types 39
of degraded images, the pre-trained generative model is an excellent candidate for serving as the 40
shared component for multiple degradation restoration. To model the transition from the clean image 41
domain to different degraded image domains, minimal specific parameters are required to fine-tune 42
the pre-trained model for each degradation restoration task. This approach not only isolates conflicts 43
between each degradation task but also ensures efficiency and performance during training. 44
Following the idea of multi-domain transfer learning, we proposed a universal image restoration 45
framework based on multiple low-rank adaptations, named UIR-LoRA. In our framework, the pre- 46
trained SD-turbo [ 39] serves as the shared fundamental model for multiple degradation restoration 47
tasks due to its powerful one-step generation capability and extensive image priors. Subsequently, 48
we incorporate the low-rank adaptation (LoRA) technique [ 11] to fine-tune the base model for each 49
specific image restoration task. This involves augmenting low-dimensional parameter matrices on 50
selected layers within the base model, ensuring efficient fine-tuning while maintaining independence 51
between LoRAs for each specific degradation. Additionally, we propose a LoRA composition strategy 52
based on degradation similarity. We calculate the similarity between degradation features extracted 53
from degraded images and existing degradation types, utilizing it as weights for combining different 54
LoRA experts. This strategy enables our method to be applicable for restoring mixed degradation 55
images. Moreover, we conducted extensive experiments and compared our approach with several 56
existing unified image restoration methods. The experimental results demonstrate that our method 57
achieves superior performance in the restoration of various degradations and mixed degradations. Not 58
only does our approach outperform existing methods in terms of distortion and perceptual metrics, 59
but it also exhibits significant improvements in visual quality. 60
Our contributions can be summarized as follows: 61
‚Ä¢From the perspective of multi-domain transfer learning, we propose a novel universal image 62
restoration framework based on multiple low-rank adaptations. It leverages the pre-trained 63
generative model as the shared component for multi-degradation restoration and employs 64
distinct LoRAs for multiple degradations to efficiently transfer to specific degradation 65
restoration tasks. 66
‚Ä¢We introduce a LoRAs composition strategy based on the degradation similarity, which 67
adaptively combines trained LoRAs and enables our model to be applicable for mixed 68
degradation restoration. 69
‚Ä¢Through extensive experiments on multiple and mixed degradations, we demonstrate that the 70
proposed universal image restoration method not only achieves higher fidelity and perceptual 71
image quality but also has better generalization ability than other unified models. 72
22 Related Work 73
2.1 Image Restoration 74
Specific Degradation Restoration. According to degradation type, image restoration tasks are 75
categorized into different groups, including denoising, deblurring, inpainting, draining .etc. Most 76
existing image restoration methods [ 2,53,16,56,5,14] mainly address the issue with a single 77
degradation. Traditional approaches [ 27,28,7] have proposed image priors. While these priors can 78
be applied to different degraded images, their capability is limited. Due to the remarkable capability 79
of the deep neural network (DNN), numerous DNN-based methods [ 2,53,16] have been proposed 80
to tackle image restoration tasks. While DNN-based methods have made significant progress, they 81
struggle with multiple degradations and mixed degradations, since they typically require retraining 82
from scratch on data with the same degradation. 83
Universal degradation restoration. Increasing attention is currently focused on developing a 84
unified model to process multiple degradations. For example, AirNet[ 15] explores the degradation 85
representation in latent space for separating them in the restoration network. PromptIR[ 31] utilizes a 86
prompt block to extract the degradation-related features to improve the performance. Daclip-IR[ 20] 87
introduces the clip-based encoder to distinguish the type of degradation and extract the semantics 88
information from distorted images and embed them into a diffusion model to generate high-quality 89
images. Despite the advancements, these unified models still have limitations. They also require 90
retraining all parameters when unseen degradations arrive and have limited performance due to the 91
gradient conflict. 92
2.2 Low-Rank Adaptation 93
LoRA [ 11] is proposed to fine-tune large models by freezing the pre-trained weights and introducing 94
trainable low-rank matrices. This fine-tuning method leverages the property of "intrinsic dimension" 95
within neural networks, lowering the rank of additional matrices and making the re-training process 96
efficient. Concretely, given a weight matrices W‚ààRn√ómin pre-trained model Œ∏p, two trainable 97
matrices B‚ààRn√órandA‚ààRr√ómare inserted into the layer to represent the LoRA ‚àÜW=BA, 98
where ris the rank and satisfy r‚â™mim(n, m), the updated weights W‚Ä≤are calculated by 99
W‚Ä≤=W+ ‚àÜW. (1)
By applying LoRA in pre-trained models, numerous image generation methods [ 29,13], show 100
superior performance in the field of image style and semantics concept transferring. Additionally, 101
fine-tuning methods like ControlNet [ 57], T2i-adapter [ 24] are also commonly employed in large- 102
scale pre-trained generative models such as Stable Diffusion [37], SDXL [30], and Imagen [38]. 103
2.3 Mixture of Experts 104
Mixture of Experts (MoE) [ 41,49,48] is an effective approach to scale up neural network capacity to 105
improve performance. Specifically, MoE integrates multiple feed-forward networks into a transformer 106
block, where each feed-forward network is regarded as an expert. A gating function is introduced to 107
model the probability distribution across all experts in the MoE layer. The gating function is trainable 108
and determines the activation of specific experts within the MoE layer based on top-k values. Broadly 109
speaking, our framework aligns with the concept of MoE. However, unlike traditional MoE layers, we 110
employ the more efficient LoRA as experts in selected frozen layers and utilize a degradation-aware 111
router across all selected layers to uniformly activate experts, reducing learning complexity and 112
avoiding conflicts among different image restoration tasks on experts. 113
3 Methodology 114
3.1 Problem Definition 115
This paper seeks to develop a novel universal image restoration framework capable of handling 116
diverse forms of image degradation in the wild by fine-tuning the pre-trained generative model. 117
Consider a set of Timage restoration tasks D={Dk}T
k=1, where Dk={(xi, yi)}nk
i=1is the training 118
dataset containing nkimages pairs of the k-th image degradation task. Within the set of tasks D, 119
3Pre-trained WeightsFrozen Layer‚Ä¶
üî•Trainable LoRAs
hazyblurry‚Ä¶noisy‚àë
‚Ä¶Imageencoder‚àÜùëä!‚àÜùëä"‚àÜùëä#Degraded ImageRestored Image
weights‚Ä¶ùëì!ùë•ùëì"ùë•ùëì#ùë•ùëì$ùë•&%&#'ùë†%ùëì%ùë•ùëì$ùë•+
Degradationsimilarityùë†!=ùëëùêµ√óùëëùêµ‚Ä¶
ùë•!"ùë•#$%Text encoderDegradation-AwareRouterUniversalImageRestorer
hazy, blurry,‚Ä¶, noisy
‚Ä¶ùë†=ùëõùëúùëüùëö"ùë†!ùë°ùëúùëùùêæFigure 2: Overview of UIR-LoRA. UIR-LoRA consists of two components: a degradation-aware
router and a universal image restorer. The router calculates degradation similarity in the latent space
of CLIP, while the restorer utilizes the similarity provided by the router to combine LoRAs and frozen
base model and restore images with multiple or mixed degadations.
each task Dkonly has a specific type of image degradation, with no intersection between any two 120
tasks. Given a pre-trained generative model Œ∏pwith frozen parameters, our objective is to learn a 121
set of composite {Œ∏k}T
k=1to construct a unified model fŒ∏that performs well on multi-degradation 122
restoration and mixed degradation restoration by transferring learning, where Œ∏=Œ∏p+PT
k=1skŒ∏k 123
andskrepresents the composite weight for Œ∏k. The trainable {Œ∏k}T
k=1can be optimized through 124
minimizing the overall image reconstruction loss: 125
L=E(x,y)‚ààDl(fŒ∏(x), y). (2)
We will present how to design and optimize the trainable {Œ∏k}T
k=1and construct the composite 126
weights sin the next sections. 127
3.2 Overview of Universal Framework 128
Inspired by transferring learning, we introduce a novel universal image restoration framework based 129
on multiple low-rank adaptations, named UIR-LoRA. Referring to Figure 2, our framework consists 130
of two main components, namely degradation-aware router and universal image restorer, respectively. 131
The degradation-aware router first extracts the degradation feature from input degraded images and 132
then calculates the similarity probabilities swith existing degradations in the latent space of CLIP 133
model [ 35,20]. For the universal image restorer, it comprises a pre-trained generative model Œ∏pand 134
Ttrainable LoRAs {Œ∏k}T
k=1. This design is primarily motivated by two considerations: firstly, the 135
pre-trained generative model contains extensive image priors that are degradation-agnostic and can 136
be shared across all types of degraded images. Secondly, each LoRA can independently capture 137
specific characteristics of each degradation without gradient conflicts. In practice, the pre-trained 138
SD-turbo [ 39] is employed as the frozen base model in our framework and each LoRA Œ∏kserves 139
as an expert responsible for transferring the frozen base model to a specific degradation restoration 140
taskDk. By adjusting the value of Top-K parameter within the degradation-aware router, different 141
combinations of LoRAs in the universal image restorer can be activated, enabling the removal of a 142
specific degradation and mixed degradation in multi-degraded scenarios. 143
43.3 Degradation-Aware Router 144
The Degradation-Aware Router is designed to provide the restorer with weights for LoRA combination 145
based on degradation confidence. Following Daclip-ir [ 20], we utilized the pre-trained image encoder 146
in CLIP [ 35] to obtain the degradation vector d‚ààR1√ózfrom the input degraded image x, where zis 147
degradation length in latent space. Differing from Daclip-ir [20], we use the degradation vector and 148
existing degradations to calculate the similarity, instead of directly embedding the degradation vector 149
into the restoration network in Daclip-ir [ 20]. The existing degradations refer to the vocabulary bank 150
of diverse degradation types that we introduce in the router, such as "noisy", "blurry" and "shadowed". 151
This vocabulary bank is highly compact and flexible when adding new degradation types. Similarly, 152
by applying the text encoder of CLIP [ 35], the vocabulary bank can be encoded into the degradation 153
bank B‚ààRz√óTin the latent space. As presented in Figure 2, the original degradation similarity 154
so‚ààR1√óTis calculated by: 155
so=dB. (3)
Building upon the original similarity, we adopt a more flexible and controllable Top-K strategy 156
to modify so. Specifically, we select the Top-K largest values from the original similarity so, and 157
normalize them to reallocate the weights for LoRAs. The reallocation process can be formulated as : 158
s=so¬∑MKPso¬∑MK, (4)
where MKrepresents a binary mask with the same length as so, where it is 1 when the corresponding 159
value in sois among the Top-K, otherwise it is 0. With a smaller value of K, the restorer activates 160
fewer LoRAs, reducing its computational load. For instance, with K= 1, only the most similar 161
LoRA is activated and it yields effective results when sis accurate, but performance noticeably 162
declines with inaccurate s. Conversely, as Kincreases, the restorer exhibits higher tolerance to sand 163
the combination of LoRAs allows it to handle mixed degradation. 164
3.4 Universal Image Restorer 165
Our universal image restorer consists of a pre-trained generative model Œ∏pand a set of LoRAs 166
{Œ∏k}T
k=1. As illustrated in Figure 2, our universal image restorer takes the degraded image xand 167
similarity spredicted by the degradation-aware router as inputs. It then activates relevant LoRAs 168
based on sto recover the degraded image along with the frozen base model. Since one of our 169
objectives is to ensure that each LoRA serves as an expert in processing a specific degradation, the 170
number of LoRAs in the restorer aligns with the number of degradation types, T. In practice, we 171
select multiple layers from the base model, For a selected layer Wof the pre-trained base model, a 172
sequence of trainable matrices {‚àÜWk}T
k=1are added into this layer, and the parameters of all chosen 173
layers Lform a complete LoRA Œ∏k={‚àÜWj
k}j‚ààL. As previously explained, each LoRA is a unique 174
expert responsible for a specific degradation. Drawing inspiration from Mixture of Expert (MoE), we 175
aggregate the outputs of each expert rather than directly merging parameters in [ 11]. Therefore, given 176
the input feature xinof the current layer and the similarity s, the total output xoutof this modified 177
layer can be expressed as 178
xout=fo(xin) +KX
i=1sifi(xin), (5)
where fi(xin)denotes the result of i-th trainable matrice Wi, particularly fo(xin)is output of the 179
frozen base layer. From the equation 5, it can be observed that the introduced LoRAs interact with the 180
frozen base model at intermediate feature layers in our restorer. This interaction forces the restorer 181
to leverage the image priors of the pre-trained generative model and eliminate degradation with the 182
assistance of LoRAs. In contrast to employing stable diffusion [ 37] directly as a post-processing 183
technique, our restorer yields results closer to the true scene without introducing inaccurate structural 184
details. Since each Wis implemented using two low-rank matrices like the formula 1, the total 185
trainable parameters of our framework are much smaller than that of the pre-train generative model. 186
3.5 Training and Inference Procedure 187
During the training phase, for the efficient training of the universal image restorer, we ensure that 188
each batch is sampled from the same degradation type Dk, and activate the corresponding LoRA Œ∏k189
5Table 1: Comparison of the restoration results over ten different datasets. The best results are marked
in boldface.
ModelDistortion Perceptual Complexity
PSNR‚ÜëSSIM‚ÜëLPIPS ‚Üì FID‚Üì Param /M Runtime /s
SwinIR [16] 23.37 0.731 0.354 104.37 15.8 0.66
NAFNet [2] 26.34 0.847 0.159 55.68 67.9 0.54
Restormer [53] 26.43 0.850 0.157 54.03 26.1 0.14
AirNet [15] 25.62 0.844 0.182 64.86 7.6 1.50
PromptIR [31] 27.14 0.859 0.147 48.26 35.6 1.19
IR-SDE [21] 23.64 0.754 0.167 49.18 36.2 5.07
DiffBIR [17] 21.01 0.618 0.263 91.03 363.2 5.95
Daclip-IR [20] 27.01 0.794 0.127 34.89 295.2 4.09
UIR-LoRA (Ours) 28.08 0.864 0.104 30.58 95.2 0.44
for training. Since the dataset Dis organized by degradation type without overlap and each LoRA 190
is assigned to handle each type of degradation correspondingly, the overall optimization process in 191
equation 2 can be decomposed into independent optimization processes for each degradation. This 192
design and training process circumvent task conflicts among multiple degradations and makes it 193
possible to use suitable loss functions for the specific degradation. Due to the availability of accurate 194
sduring training and the use of pre-trained encoders from CLIP [ 35] and Daclip-ir [ 20] in our router, 195
the router was not utilized during training. 196
In the inference phase, the similarity sis unknown and needs to be estimated from the degraded 197
image. The estimated similarity sserves as a reference in our framework and can also be manually 198
specified by users. Subsequently, our universal image restorer composite LoRAs and recovers the 199
input image with the guidance of s. 200
4 Experiments 201
4.1 Experimental Setting 202
Datasets. We validate the effectiveness of our framework in multiple and mixed degradation scenarios. 203
In the case of multiple degradations, we follow Daclip-IR [ 20] and construct a dataset using 10 204
different single degradation datasets. Briefly, the composite dataset comprises a total of 52800 image 205
pairs for training and 2490 image pairs for testing. The degradation types included are commonly 206
encountered in image restoration, such as blur, noise, shadow, JPEG compression, and weather 207
degradations. For mixed degradations, we utilize two degradation datasets, REDS [ 25] and LOLBlur 208
[58]. In REDS, the images are distorted by JPEG compression and blur, and those images in LOLBlur 209
have blur and low light. For more details about datasets in our experiments, please refer to Appendix . 210
Metrics. The objective of the image restoration task is to output images with enhanced visual quality 211
while maintaining high fidelity to the original scene information. This differs from image generation 212
tasks, which prioritize visual quality. Therefore, to thoroughly evaluate the effectiveness of our 213
method, we utilize reference-based image quality assessment techniques from both distortion and 214
perceptual perspectives, including PSNR, SSIM, and LPIPS, as well as FID. 215
Comparison Methods. In the experiments, we primarily compare with several state-of-the-art 216
methods in image restoration, which fall into two categories: regression model and generative model. 217
Regression models include NAFNet [ 2], Restormer [ 53], as well as AirNet [ 15] and PromptIR [ 31] 218
proposed for multiple degradation restoration. DiffBIR [ 17], IR-SDE [ 21] and Daclip-IR [ 20] are 219
generative models built upon the diffusion model [9]. 220
4.2 Implementation Details 221
During the training, we adapt an AdamW optimizer to update the weights of trainable parameters in 222
our model. Before training LoRA for specific degradation, we add skip-connections in the V AE of 223
SD-turbo[ 39] like [ 29,44] and train them with multiple degraded images. We set the initialization 224
6JPEGGTRestormerPromptIRDaclip-IROurs
NoisyGTRestormerPromptIRDaclip-IROurs
RaindropGTRestormerPromptIRDaclip-IROurs
InpaintingGTRestormerPromptIRDaclip-IROurs
Figure 3: Qualitative comparison on multiple degraded images.
learning rate to 2e-4 and decrease it with CosineAnnealingLR . We trained every LoRA for 80K 225
iterations with batch size 8 and we keep the same hyper-parameters when training different LoRAs. 226
The default rank of LoRAs in V AE and Unet is 4 and 8, respectively. 227
4.3 Multiple Image Restoration 228
For fair comparisons, all methods are trained and tested on the multiple degradation dataset. The 229
results are presented in Table 1. We can find that our model, UIR-LoRA, considerably surpasses all 230
compared image restoration approaches across four metrics. This indicates that our approach can 231
balance generating clear structures and details while ensuring the restored images closely resemble the 232
original information of the scene. The visual comparison results depicted in Figure 7 also confirm this 233
assertion. Regression models such as NAFNet [ 2]and Restormer [ 53], lacking extensive image priors, 234
tend to produce blurred and over-smoothed images, leading to inferior visual outcomes. Conversely, 235
generative models Daclip-IR [ 20] excessively prioritize perceptual quality, yielding artifacts and 236
noise that diverge from the actual scene information. Our approach integrates the strengths of both 237
categories of methods, enabling strong performance in both distortion and perceptual aspects 238
4.4 Mixed Image Restoration 239
To evaluate the transferability of UIR-LoRA, we conduct some experiments on mixed degradation 240
datasets from REDS[ 25] and LOLBlur [ 58]. Each image in these two datasets contains more than one 241
type of degradation, like blur, jpeg compression, noise, and low light. We test the mixed degraded 242
images using models trained on multiple degradations and set Kto 2 in the router. As shown in 243
Table 2, our method achieves superior results in both distortion and perceptual quality, particularly 244
on the LOLBlur dataset. We also provide visual comparison results, as illustrated in Figure 4, our 245
approach effectively enhances the low-light image compared to SOTA methods, highlighting its 246
stronger transferability in the wild. More visual results can be found in Appendix . 247
7Table 2: Comparison of the restoration results on mixed degradation datasets. The best results are
marked in boldface.
ModelREDS LOLBlur
PSNR‚ÜëSSIM‚ÜëLPIPS ‚Üì FID‚Üì PSNR‚ÜëSSIM‚ÜëLPIPS ‚ÜìFID‚Üì
SwinIR 21.53 0.676 0.449 116.80 10.06 0.320 0.619 124.52
NAFNet 25.06 0.721 0.412 122.12 10.57 0.397 0.477 85.77
Restormer 23.15 0.713 0.413 118.61 12.77 0.479 0.478 87.23
PromptIR 24.98 0.712 0.424 128.11 9.09 0.275 0.560 91.68
DiffBIR 20.70 0.598 0.377 122.76 9.86 0.288 0.611 125.41
Daclip-IR 24.30 0.699 0.337 95.29 14.52 0.599 0.358 68.10
UIR-LoRA 25.11 0.718 0.315 89.79 18.16 0.690 0.318 61.55
Degradedimage
GTRestormerPromptIR
Daclip-IROurs
Figure 4: Qualitative comparison on multiple degraded images.
4.5 Ablation Study 248
Complexity Analysis. We compare model complexity with SOTA models. The comparison results 249
are shown in Table 1, where we report the number of trainable parameters and the runtime for a 250
256√ó256 image on an A100 GPU. The complexity of UIR-LoRA is comparable to regression models 251
like NAFNet [2] and significantly more efficient than generative models like Daclip-IR [20]. 252
Effectiveness of Degradation-Aware Router. The degradation-aware router plays a crucial role in 253
determining which LoRAs are activated in the inference. To comprehensively demonstrate the impact 254
of the router, we conduct experiments with different selection strategies. As illustrated in Table 3, 255
we have five strategies: "random" indicates activating a LoRA at random, "average" denotes using 256
average weights to activate all LoRAs, and "Top-1", "Top-2" and "All" correspond to setting Kin the 257
router to 1,2, and 10, respectively. From the comparison of these results, we can see that the random 258
and average strategies result in poorer performance while using the strategy based on degradation 259
Table 3: Impact of strategies in router
StrategyMultiple Degradation Mixed Degradation
PSNR‚ÜëSSIM‚ÜëLPIPS ‚Üì FID‚Üì PSNR‚ÜëSSIM‚ÜëLPIPS ‚ÜìFID‚Üì
Random 17.52 0.617 0.388 126.48 10.35 0.323 0.577 104.84
Average 17.62 0.617 0.370 129.06 9.28 0.277 0.549 106.05
Top-1 28.06 0.864 0.105 30.62 18.04 0.683 0.321 61.65
Top-2 28.05 0.864 0.105 30.60 18.16 0.690 0.318 61.55
All 28.05 0.864 0.105 30.61 18.16 0.691 0.318 61.58
8Figure 5: The impact of LoRA‚Äôs rank on deblurring and denoising tasks.
similarity achieves better outcomes. This suggests that the transferability between different types 260
of degradation is limited and that specific parameters are needed to address their particularities. 261
Furthermore, the selection of the K value also affects the model‚Äôs performance. When an image has 262
only one type of degradation, a smaller K value can result in comparable performance with lower 263
inference costs. However, for mixed degradations, a larger K value is required to handle the more 264
complex situation. 265
Impact of LoRA‚Äôs Rank. Within our framework, LoRA is utilized to facilitate the transfer from 266
the pre-trained generative model to the image restoration task. In order to investigate the impact of 267
LoRA‚Äôs rank on the performance of image restoration, we conduct experiments using deblurring 268
and denoising tasks chosen from ten distinct degradation categories. We set the initial rank to 2 and 269
incrementally increase the value by a factor of 2. The performance changes are depicted in Figure 5. 270
It is evident that as the rank grows, the restoration results improve in distortion and perceptual quality, 271
and at the same time, the number of trainable parameters also increases. Once the rank value exceeds 272
4, the performance improvement becomes progressively marginal. Therefore, we set the default rank 273
to 4 in our restorer to balance between performance and complexity. 274
Table 4: The accuracy of predicted degradation type
PSNR‚ÜëSSIM‚ÜëLPIPS ‚ÜìFID‚ÜìAccuracy ‚Üë
Original 26.66 0.839 0.159 18.72 91.6
Modified 26.87 0.842 0.155 18.42 99.2
Impact of Predicted Degradation. 275
The resizing operation on input images in CLIP models [ 20,35] may lead to inaccurate predictions 276
of degradation types, especially for blurry images. To reduce its negative impact on performance, we 277
introduce a simple way that uses the degradation vector of the image crop without resizing to correct 278
the potential error in the resized image. Table 4 is the comparison conducted on blurry images from 279
GoPro dataset. It can be observed that our model with modified operation has higher accuracy and 280
better performance for deblurring. 281
5 Conclusion 282
In this paper, we propose a universal image restoration framework based on multiple low-rank 283
adaptation, named UIR-LoRA, from the perspective of multi-domain transfer learning. UIR-LoRA 284
utilizes a pre-trained generative model as the frozen base model and transfers its abundant image 285
priors to different image restoration tasks using the LoRA technique. Moreover, we introduce a 286
LoRAs‚Äô composition strategy based on the degradation similarity that allows UIR-LoRA applicable 287
for multiple and mixed degradations in the wild. Extensive experiments on universal image restoration 288
tasks demonstrate the effectiveness and better generalization capability of our proposed UIR-LoRA. 289
6 Limitation and Discussion 290
Although our UIR-LoRA has achieved remarkable performance in image restoration tasks under both 291
multiple and mixed degradations, it still has limitations and problems for further exploration. For 292
instance, adding new trainable parameters into the network for unseen degradations is unavoidable in 293
image restoration tasks, although UIR-LoRA is already more efficient and flexible compared to other 294
approaches. 295
9References 296
[1]Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: 297
Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern 298
recognition workshops , pages 126‚Äì135, 2017. 299
[2]Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image 300
restoration. In European conference on computer vision , pages 17‚Äì33. Springer, 2022. 301
[3]Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Chengpeng Chen. Hinet: Half instance 302
normalization network for image restoration. In Proceedings of the IEEE/CVF Conference on 303
Computer Vision and Pattern Recognition , pages 182‚Äì192, 2021. 304
[4]Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. 305
Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural 306
Information Processing Systems , 35:16664‚Äì16678, 2022. 307
[5]Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, and John Paisley. Clearing the 308
skies: A deep network architecture for single-image rain removal. IEEE Transactions on Image 309
Processing , 26(6):2944‚Äì2956, 2017. 310
[6]Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian Reid, Chunhua Shen, Anton Van 311
Den Hengel, and Qinfeng Shi. From motion blur to motion flow: A deep learning solution 312
for removing heterogeneous motion blur. In Proceedings of the IEEE conference on computer 313
vision and pattern recognition , pages 2319‚Äì2328, 2017. 314
[7]Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm min- 315
imization with application to image denoising. In Proceedings of the IEEE conference on 316
computer vision and pattern recognition , pages 2862‚Äì2869, 2014. 317
[8]Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and 318
Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In 319
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 320
1780‚Äì1789, 2020. 321
[9]Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances 322
in neural information processing systems , 33:6840‚Äì6851, 2020. 323
[10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, 324
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning 325
for nlp. In International conference on machine learning , pages 2790‚Äì2799. PMLR, 2019. 326
[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, 327
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In 328
International Conference on Learning Representations , 2022. 329
[12] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for 330
improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 , 2017. 331
[13] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi- 332
concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference 333
on Computer Vision and Pattern Recognition , pages 1931‚Äì1941, 2023. 334
[14] Orest Kupyn, V olodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Ji Àár√≠ Matas. 335
Deblurgan: Blind motion deblurring using conditional adversarial networks. In Proceedings of 336
the IEEE conference on computer vision and pattern recognition , pages 8183‚Äì8192, 2018. 337
[15] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image 338
restoration for unknown corruption. In Proceedings of the IEEE/CVF Conference on Computer 339
Vision and Pattern Recognition , pages 17452‚Äì17462, 2022. 340
[16] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: 341
Image restoration using swin transformer. In Proceedings of the IEEE/CVF international 342
conference on computer vision , pages 1833‚Äì1844, 2021. 343
10[17] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, 344
and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior. arXiv 345
preprint arXiv:2308.15070 , 2023. 346
[18] Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, and Jenq-Neng Hwang. Desnownet: Context-aware 347
deep network for snow removal. IEEE Transactions on Image Processing , 27(6):3064‚Äì3073, 348
2018. 349
[19] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc 350
Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings 351
of the IEEE/CVF conference on computer vision and pattern recognition , pages 11461‚Äì11471, 352
2022. 353
[20] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj√∂lund, and Thomas B Sch√∂n. Controlling 354
vision-language models for universal image restoration. arXiv preprint arXiv:2310.01018 , 2023. 355
[21] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj√∂lund, and Thomas B Sch√∂n. Image 356
restoration with mean-reverting stochastic differential equations. International Conference on 357
Machine Learning , 2023. 358
[22] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and 359
robust low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer 360
vision and pattern recognition , pages 5637‚Äì5646, 2022. 361
[23] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human seg- 362
mented natural images and its application to evaluating segmentation algorithms and measuring 363
ecological statistics. In Proceedings of the IEEE/CVF International Conference on Computer 364
Vision , pages 416‚Äì423, 2001. 365
[24] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. 366
T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion 367
models. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 368
4296‚Äì4304, 2024. 369
[25] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, 370
and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset 371
and study. In Proceedings of the IEEE/CVF conference on computer vision and pattern 372
recognition workshops , pages 0‚Äì0, 2019. 373
[26] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural 374
network for dynamic scene deblurring. In Proceedings of the IEEE Conference on Computer 375
Vision and Pattern Recognition , pages 3883‚Äì3891, 2017. 376
[27] Jinshan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang. Deblurring text images via l0- 377
regularized intensity and gradient prior. In Proceedings of the IEEE Conference on Computer 378
Vision and Pattern Recognition , pages 2901‚Äì2908, 2014. 379
[28] Jinshan Pan, Deqing Sun, Hanspeter Pfister, and Ming-Hsuan Yang. Blind image deblurring 380
using dark channel prior. In Proceedings of the IEEE conference on computer vision and pattern 381
recognition , pages 1628‚Äì1636, 2016. 382
[29] Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, and Jun-Yan Zhu. One-step image 383
translation with text-to-image models. arXiv preprint arXiv:2403.12036 , 2024. 384
[30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe 385
Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image 386
synthesis. arXiv preprint arXiv:2307.01952 , 2023. 387
[31] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Khan. Promptir: Prompting 388
for all-in-one image restoration. In Thirty-seventh Conference on Neural Information Processing 389
Systems , 2023. 390
11[32] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative 391
adversarial network for raindrop removal from a single image. In Proceedings of the IEEE 392
conference on computer vision and pattern recognition , pages 2482‚Äì2491, 2018. 393
[33] Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu Jia. Ffa-net: Feature fusion 394
attention network for single image dehazing. In Proceedings of the AAAI conference on artificial 395
intelligence , pages 11908‚Äì11915, 2020. 396
[34] Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, and Rynson WH Lau. Deshad- 397
ownet: A multi-context embedding deep network for shadow removal. In Proceedings of the 398
IEEE conference on computer vision and pattern recognition , pages 4067‚Äì4075, 2017. 399
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, 400
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual 401
models from natural language supervision. In International conference on machine learning , 402
pages 8748‚Äì8763. PMLR, 2021. 403
[36] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark 404
Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on 405
machine learning , pages 8821‚Äì8831. Pmlr, 2021. 406
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High- 407
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF 408
conference on computer vision and pattern recognition , pages 10684‚Äì10695, 2022. 409
[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, 410
Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 411
Photorealistic text-to-image diffusion models with deep language understanding. Advances in 412
neural information processing systems , 35:36479‚Äì36494, 2022. 413
[39] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion 414
distillation. arXiv preprint arXiv:2311.17042 , 2023. 415
[40] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances 416
in neural information processing systems , 31, 2018. 417
[41] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, 418
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts 419
layer. arXiv preprint arXiv:1701.06538 , 2017. 420
[42] H Sheikh. Live image quality assessment database release 2. http://live.ece.utexas. 421
edu/research/quality , 2005. 422
[43] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, and Lei Zhang. Ntire 2017 423
challenge on single image super-resolution: Methods and results. In Proceedings of the IEEE 424
conference on computer vision and pattern recognition workshops , pages 114‚Äì125, 2017. 425
[44] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Ex- 426
ploiting diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015 , 427
2023. 428
[45] Yinglong Wang, Chao Ma, and Jianzhuang Liu. Smartassign: Learning a smart knowledge 429
assignment strategy for deraining and desnowing. In Proceedings of the IEEE/CVF Conference 430
on Computer Vision and Pattern Recognition , pages 3677‚Äì3686, 2023. 431
[46] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for 432
low-light enhancement. arXiv preprint arXiv:1808.04560 , 2018. 433
[47] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G Dimakis, 434
and Peyman Milanfar. Deblurring via stochastic refinement. In Proceedings of the IEEE/CVF 435
Conference on Computer Vision and Pattern Recognition , pages 16293‚Äì16303, 2022. 436
[48] Xun Wu, Shaohan Huang, and Furu Wei. Mole: Mixture of lora experts. In The Twelfth 437
International Conference on Learning Representations , 2023. 438
12[49] Yuan Xie, Shaohan Huang, Tianyu Chen, and Furu Wei. Moec: Mixture of expert clusters. In 439
Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pages 13807‚Äì13815, 440
2023. 441
[50] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep 442
joint rain detection and removal from a single image. In Proceedings of the IEEE conference on 443
computer vision and pattern recognition , pages 1357‚Äì1366, 2017. 444
[51] Wenhan Yang, Robby T Tan, Shiqi Wang, Yuming Fang, and Jiaying Liu. Single image deraining: 445
From model-based to data-driven and beyond. IEEE Transactions on pattern analysis and 446
machine intelligence , 43(11):4059‚Äì4077, 2020. 447
[52] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 448
Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems , 449
33:5824‚Äì5836, 2020. 450
[53] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and 451
Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In 452
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 453
5728‚Äì5739, 2022. 454
[54] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming- 455
Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In Proceedings of the 456
IEEE/CVF conference on computer vision and pattern recognition , pages 14821‚Äì14831, 2021. 457
[55] Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. All-in-one multi- 458
degradation image restoration network via hierarchical degradation representation. In Proceed- 459
ings of the 31st ACM International Conference on Multimedia , pages 2285‚Äì2293, 2023. 460
[56] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian 461
denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image 462
processing , 26(7):3142‚Äì3155, 2017. 463
[57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image 464
diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer 465
Vision , pages 3836‚Äì3847, 2023. 466
[58] Shangchen Zhou, Chongyi Li, and Chen Change Loy. Lednet: Joint low-light enhancement and 467
deblurring in the dark. In European conference on computer vision , pages 573‚Äì589. Springer, 468
2022. 469
13A Appendix 470
A.1 More Details about Datasets 471
For multiple degradations, we follow Daclip-IR [ 20] to construct the dataset, which includes a total 472
of ten distinct degradation types: blurry, hazy, JPEG-compression, low-light, noisy, raindrop, rainy, 473
shadowed, snowy, and inpainting. The data sources and data splits for each degradation type are 474
illustrated in Table 5. 475
Table 5: Details of the datasets with ten different image degradation types
DatasetTrain Test
Sources Num Sources Num
Blurry GoPro[26] 2 103 GoPro 1 111
Hazy RESIDE-6k[33] 6 000 RESIDE-6k 1 000
JPEG DIV2K[1] and Flickr2K[43] 3 550 LIVE1[42] 29
Low-light LOL[46] 485 LOL 15
Noisy DIV2K and Flickr2K 3 550 CBSD68[23] 68
Raindrop RainDrop[32] 861 RainDrop 58
Rainy Rain100H[50] 1 800 Rain100H 100
Shadowed SRD[34] 2 680 SRD 408
Snowy Snow100K-L[18] 1 872 Snow100K-L 601
Inpainting CelebaHQ[12] 29 900 CelebaHQ and RePaint[19] 100
For mixed degradations, we utilize images from REDS[ 25] and LOLBlur[ 58]to evaluate the trans- 476
ferability of models. We sample 60 images from REDS and 200 images from LOLBlur dataset for 477
testing. The degraded images from REDS dataset feature a variety of realistic scenes and objects, 478
which suffer from both motion blurs and compression. And the images from LOLBlur dataset cover 479
a range of real-world dynamic dark scenarios with mixed degradation of low light and blurs. 480
A.2 More Visual Results 481
InputGTRestormerPromptIRDaclip-IROurs
Figure 6: Qualitative comparison on mixed degraded images from LOLBlur dataset.
A.3 Details about Metrics on Multiple Dagradation 482
14Input
GTRestormerPromptIR
Daclip-IROurs
InputGTRestormerPromptIRDaclip-IROursFigure 7: Qualitative comparison on mixed degraded images from REDS dataset.
Table 6: Comparison of the restoration results over ten different datasets on PSNR
Blurry Hazy JPEG Low-light Noisy Raindrop Rainy Shadowed Snowy Inpainting Average
SwinIR 24.49 23.49 24.44 19.59 25.13 24.64 22.07 23.97 21.86 24.05 23.37
NAFNet 26.12 24.05 26.81 22.16 27.16 30.67 27.32 24.16 25.94 29.03 26.34
Restormer 26.34 23.75 26.90 22.17 27.25 30.85 27.91 23.33 25.98 29.88 26.43
AirNet 26.25 23.56 26.98 14.24 27.51 30.68 28.45 23.48 24.87 30.15 25.62
PromptIR 26.50 25.19 26.95 23.14 27.56 31.35 29.24 24.06 27.23 30.22 27.14
IR-SDE 24.13 17.44 24.21 16.07 24.82 28.49 26.64 22.18 24.70 27.56 23.64
DiffBIR 22.79 20.52 22.39 16.96 21.60 23.22 21.04 22.27 20.63 18.77 21.01
Daclip-IR 27.03 29.53 23.70 22.09 24.36 30.81 29.41 27.27 26.83 28.94 27.01
Ours 26.66 30.28 27.15 22.45 27.74 30.51 28.26 28.63 28.09 30.88 28.06
15Table 7: Comparison of the restoration results over ten different datasets on SSIM
Blurry Hazy JPEG Low-light Noisy Raindrop Rainy Shadowed Snowy Inpainting Average
SwinIR 0.758 0.848 0.734 0.735 0.690 0.758 0.623 0.757 0.665 0.743 0.731
NAFNet 0.804 0.926 0.780 0.809 0.768 0.924 0.848 0.839 0.869 0.901 0.847
Restormer 0.811 0.915 0.781 0.815 0.762 0.928 0.862 0.836 0.877 0.912 0.850
AirNet 0.805 0.916 0.783 0.781 0.769 0.926 0.867 0.832 0.846 0.911 0.844
PromptIR 0.815 0.933 0.784 0.829 0.774 0.931 0.876 0.842 0.887 0.918 0.859
IR-SDE 0.730 0.832 0.615 0.719 0.640 0.822 0.808 0.667 0.828 0.876 0.754
DiffBIR 0.695 0.761 0.607 0.665 0.395 0.682 0.573 0.568 0.566 0.678 0.618
Daclip-IR 0.810 0.931 0.532 0.796 0.579 0.882 0.854 0.811 0.854 0.894 0.794
Ours 0.839 0.962 0.782 0.826 0.789 0.908 0.857 0.862 0.893 0.916 0.864
Table 8: Comparison of the restoration results over ten different datasets on LPIPS
Blurry Hazy JPEG Low-light Noisy Raindrop Rainy Shadowed Snowy Inpainting Average
SwinIR 0.347 0.180 0.392 0.362 0.439 0.353 0.481 0.335 0.388 0.265 0.354
NAFNet 0.284 0.043 0.303 0.158 0.216 0.082 0.180 0.138 0.096 0.085 0.159
Restormer 0.282 0.054 0.300 0.156 0.215 0.083 0.170 0.145 0.095 0.072 0.157
AirNet 0.279 0.063 0.302 0.321 0.264 0.095 0.163 0.145 0.112 0.071 0.182
PromptIR 0.267 0.051 0.269 0.140 0.230 0.078 0.147 0.143 0.082 0.068 0.147
IR-SDE 0.198 0.168 0.246 0.185 0.232 0.113 0.142 0.223 0.107 0.065 0.167
DiffBIR 0.269 0.158 0.244 0.273 0.442 0.187 0.309 0.261 0.236 0.246 0.263
Daclip-IR 0.140 0.037 0.317 0.114 0.272 0.068 0.085 0.118 0.072 0.047 0.127
Ours 0.159 0.021 0.204 0.126 0.153 0.048 0.112 0.103 0.070 0.056 0.105
Table 9: Comparison of the restoration results over ten different datasets on FID
Blurry Hazy JPEG Low-light Noisy Raindrop Rainy Shadowed Snowy Inpainting Average
SwinIR 53.84 35.43 83.33 156.55 126.87 111.64 186.60 70.22 79.51 139.71 104.37
NAFNet 42.99 15.73 71.88 73.94 82.08 56.43 86.35 47.32 35.76 44.32 55.68
Restormer 39.08 15.34 72.68 78.22 87.14 50.97 78.16 48.33 33.45 36.96 54.03
AirNet 41.23 21.91 78.56 154.2 93.89 52.71 72.07 64.13 64.13 32.93 64.86
PromptIR 36.5 10.85 73.02 67.15 84.51 44.48 61.88 43.24 28.29 32.69 48.26
IR-SDE 29.79 23.16 61.85 66.42 79.38 50.22 63.07 50.71 34.63 32.61 49.18
DiffBIR 37.84 31.83 66.07 150.96 127.27 81.27 133.60 74.09 53.62 154.02 91.03
Daclip-IR 14.13 5.66 42.05 52.23 64.71 38.91 52.78 25.48 27.26 25.73 34.89
Ours 18.72 5.92 37.23 62.21 44.36 23.77 44.30 23.39 22.77 23.50 30.62
Table 10: Impact of rank in LoRAs
RankDeblurring Denoising
PSNR‚ÜëSSIM‚ÜëLPIPS ‚ÜìFID‚ÜìPSNR‚ÜëSSIM‚ÜëLPIPS ‚ÜìFID‚Üì
2 26.35 0.831 0.170 21.35 27.57 0.783 0.163 48.98
4 26.64 0.841 0.157 18.79 27.74 0.789 0.153 44.32
8 26.79 0.845 0.151 18.01 27.81 0.791 0.150 43.29
16 26.80 0.846 0.151 17.90 27.83 0.792 0.147 42.82
16NeurIPS Paper Checklist 483
1.Claims 484
Question: Do the main claims made in the abstract and introduction accurately reflect the 485
paper‚Äôs contributions and scope? 486
Answer: [Yes] 487
Justification: The main claims made in the abstract and introduction1 accurately reflect the 488
paper‚Äôs contributions and scope. 489
Guidelines: 490
‚Ä¢The answer NA means that the abstract and introduction do not include the claims 491
made in the paper. 492
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the 493
contributions made in the paper and important assumptions and limitations. A No or 494
NA answer to this question will not be perceived well by the reviewers. 495
‚Ä¢The claims made should match theoretical and experimental results, and reflect how 496
much the results can be expected to generalize to other settings. 497
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals 498
are not attained by the paper. 499
2.Limitations 500
Question: Does the paper discuss the limitations of the work performed by the authors? 501
Answer: [Yes] 502
Justification: The paper does discuss the limitations of the work in Sections 6. 503
Guidelines: 504
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that 505
the paper has limitations, but those are not discussed in the paper. 506
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper. 507
‚Ä¢The paper should point out any strong assumptions and how robust the results are to 508
violations of these assumptions (e.g., independence assumptions, noiseless settings, 509
model well-specification, asymptotic approximations only holding locally). The authors 510
should reflect on how these assumptions might be violated in practice and what the 511
implications would be. 512
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was 513
only tested on a few datasets or with a few runs. In general, empirical results often 514
depend on implicit assumptions, which should be articulated. 515
‚Ä¢The authors should reflect on the factors that influence the performance of the approach. 516
For example, a facial recognition algorithm may perform poorly when image resolution 517
is low or images are taken in low lighting. Or a speech-to-text system might not be 518
used reliably to provide closed captions for online lectures because it fails to handle 519
technical jargon. 520
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms 521
and how they scale with dataset size. 522
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to 523
address problems of privacy and fairness. 524
‚Ä¢While the authors might fear that complete honesty about limitations might be used by 525
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 526
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best 527
judgment and recognize that individual actions in favor of transparency play an impor- 528
tant role in developing norms that preserve the integrity of the community. Reviewers 529
will be specifically instructed to not penalize honesty concerning limitations. 530
3.Theory Assumptions and Proofs 531
Question: For each theoretical result, does the paper provide the full set of assumptions and 532
a complete (and correct) proof? 533
Answer: [Yes] 534
17Justification: The paper provides the full set of assumptions and a complete (and correct) 535
proof in Sections 3. 536
Guidelines: 537
‚Ä¢ The answer NA means that the paper does not include theoretical results. 538
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross- 539
referenced. 540
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems. 541
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if 542
they appear in the supplemental material, the authors are encouraged to provide a short 543
proof sketch to provide intuition. 544
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented 545
by formal proofs provided in appendix or supplemental material. 546
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced. 547
4.Experimental Result Reproducibility 548
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 549
perimental results of the paper to the extent that it affects the main claims and/or conclusions 550
of the paper (regardless of whether the code and data are provided or not)? 551
Answer: [Yes] 552
Justification: The paper provides a comprehensive description of the experimental setting in 553
Sections 4.1 and implementation details in Sections 4.2, which are crucial for reproducing 554
the main results. 555
Guidelines: 556
‚Ä¢ The answer NA means that the paper does not include experiments. 557
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived 558
well by the reviewers: Making the paper reproducible is important, regardless of 559
whether the code and data are provided or not. 560
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken 561
to make their results reproducible or verifiable. 562
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways. 563
For example, if the contribution is a novel architecture, describing the architecture fully 564
might suffice, or if the contribution is a specific model and empirical evaluation, it may 565
be necessary to either make it possible for others to replicate the model with the same 566
dataset, or provide access to the model. In general. releasing code and data is often 567
one good way to accomplish this, but reproducibility can also be provided via detailed 568
instructions for how to replicate the results, access to a hosted model (e.g., in the case 569
of a large language model), releasing of a model checkpoint, or other means that are 570
appropriate to the research performed. 571
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis- 572
sions to provide some reasonable avenue for reproducibility, which may depend on the 573
nature of the contribution. For example 574
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 575
to reproduce that algorithm. 576
(b)If the contribution is primarily a new model architecture, the paper should describe 577
the architecture clearly and fully. 578
(c)If the contribution is a new model (e.g., a large language model), then there should 579
either be a way to access this model for reproducing the results or a way to reproduce 580
the model (e.g., with an open-source dataset or instructions for how to construct 581
the dataset). 582
(d)We recognize that reproducibility may be tricky in some cases, in which case 583
authors are welcome to describe the particular way they provide for reproducibility. 584
In the case of closed-source models, it may be that access to the model is limited in 585
some way (e.g., to registered users), but it should be possible for other researchers 586
to have some path to reproducing or verifying the results. 587
5.Open access to data and code 588
18Question: Does the paper provide open access to the data and code, with sufficient instruc- 589
tions to faithfully reproduce the main experimental results, as described in supplemental 590
material? 591
Answer: [No] 592
Justification: Upon acceptance of the paper, we will release the code under an open-source 593
license, which will allow the community to access and verify the experimental results. 594
Guidelines: 595
‚Ä¢ The answer NA means that paper does not include experiments requiring code. 596
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 597
public/guides/CodeSubmissionPolicy ) for more details. 598
‚Ä¢While we encourage the release of code and data, we understand that this might not be 599
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not 600
including code, unless this is central to the contribution (e.g., for a new open-source 601
benchmark). 602
‚Ä¢The instructions should contain the exact command and environment needed to run to 603
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 604
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 605
‚Ä¢The authors should provide instructions on data access and preparation, including how 606
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 607
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new 608
proposed method and baselines. If only a subset of experiments are reproducible, they 609
should state which ones are omitted from the script and why. 610
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized 611
versions (if applicable). 612
‚Ä¢Providing as much information as possible in supplemental material (appended to the 613
paper) is recommended, but including URLs to data and code is permitted. 614
6.Experimental Setting/Details 615
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 616
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 617
results? 618
Answer: [Yes] 619
Justification: The paper provides detailed information on all training and test aspects, 620
including datasets, metrics, comparison methods, hyperparameters, the type of optimizer 621
used, and other relevant details necessary to understand the results in Sections 4.1 and 4.2. 622
Guidelines: 623
‚Ä¢ The answer NA means that the paper does not include experiments. 624
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail 625
that is necessary to appreciate the results and make sense of them. 626
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental 627
material. 628
7.Experiment Statistical Significance 629
Question: Does the paper report error bars suitably and correctly defined or other appropriate 630
information about the statistical significance of the experiments? 631
Answer: [Yes] 632
Justification: The paper reports error bars appropriately and includes correctly defined infor- 633
mation regarding the statistical significance of the experiments, ensuring the transparency 634
and reliability of the results. 635
Guidelines: 636
‚Ä¢ The answer NA means that the paper does not include experiments. 637
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi- 638
dence intervals, or statistical significance tests, at least for the experiments that support 639
the main claims of the paper. 640
19‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for 641
example, train/test split, initialization, random drawing of some parameter, or overall 642
run with given experimental conditions). 643
‚Ä¢The method for calculating the error bars should be explained (closed form formula, 644
call to a library function, bootstrap, etc.) 645
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors). 646
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error 647
of the mean. 648
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should 649
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 650
of Normality of errors is not verified. 651
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or 652
figures symmetric error bars that would yield results that are out of range (e.g. negative 653
error rates). 654
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how 655
they were calculated and reference the corresponding figures or tables in the text. 656
8.Experiments Compute Resources 657
Question: For each experiment, does the paper provide sufficient information on the com- 658
puter resources (type of compute workers, memory, time of execution) needed to reproduce 659
the experiments? 660
Answer: [Yes] 661
Justification: The paper provides detailed information regarding the computer resources 662
used in Sections 4.1, and time of execution in Table 1. 663
Guidelines: 664
‚Ä¢ The answer NA means that the paper does not include experiments. 665
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster, 666
or cloud provider, including relevant memory and storage. 667
‚Ä¢The paper should provide the amount of compute required for each of the individual 668
experimental runs as well as estimate the total compute. 669
‚Ä¢The paper should disclose whether the full research project required more compute 670
than the experiments reported in the paper (e.g., preliminary or failed experiments that 671
didn‚Äôt make it into the paper). 672
9.Code Of Ethics 673
Question: Does the research conducted in the paper conform, in every respect, with the 674
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 675
Answer: [Yes] 676
Justification: The research conducted in the paper is in full compliance with the NeurIPS 677
Code of Ethics. 678
Guidelines: 679
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 680
‚Ä¢If the authors answer No, they should explain the special circumstances that require a 681
deviation from the Code of Ethics. 682
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid- 683
eration due to laws or regulations in their jurisdiction). 684
10.Broader Impacts 685
Question: Does the paper discuss both potential positive societal impacts and negative 686
societal impacts of the work performed? 687
Answer: [Yes] 688
Justification: The paper provides both the potential benefits and the risks associated with the 689
research, ensuring a comprehensive assessment of its societal implications. 690
Guidelines: 691
20‚Ä¢ The answer NA means that there is no societal impact of the work performed. 692
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal 693
impact or why the paper does not address societal impact. 694
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses 695
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 696
(e.g., deployment of technologies that could make decisions that unfairly impact specific 697
groups), privacy considerations, and security considerations. 698
‚Ä¢The conference expects that many papers will be foundational research and not tied 699
to particular applications, let alone deployments. However, if there is a direct path to 700
any negative applications, the authors should point it out. For example, it is legitimate 701
to point out that an improvement in the quality of generative models could be used to 702
generate deepfakes for disinformation. On the other hand, it is not needed to point out 703
that a generic algorithm for optimizing neural networks could enable people to train 704
models that generate Deepfakes faster. 705
‚Ä¢The authors should consider possible harms that could arise when the technology is 706
being used as intended and functioning correctly, harms that could arise when the 707
technology is being used as intended but gives incorrect results, and harms following 708
from (intentional or unintentional) misuse of the technology. 709
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation 710
strategies (e.g., gated release of models, providing defenses in addition to attacks, 711
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 712
feedback over time, improving the efficiency and accessibility of ML). 713
11.Safeguards 714
Question: Does the paper describe safeguards that have been put in place for responsible 715
release of data or models that have a high risk for misuse (e.g., pretrained language models, 716
image generators, or scraped datasets)? 717
Answer: [NA] 718
Justification: The paper does not introduce assets that carry a high risk for misuse, therefore, 719
no specific safeguards for data or model release are required. 720
Guidelines: 721
‚Ä¢ The answer NA means that the paper poses no such risks. 722
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with 723
necessary safeguards to allow for controlled use of the model, for example by requiring 724
that users adhere to usage guidelines or restrictions to access the model or implementing 725
safety filters. 726
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors 727
should describe how they avoided releasing unsafe images. 728
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do 729
not require this, but we encourage authors to take this into account and make a best 730
faith effort. 731
12.Licenses for existing assets 732
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 733
the paper, properly credited and are the license and terms of use explicitly mentioned and 734
properly respected? 735
Answer: [Yes] 736
Justification: The paper meticulously cites all external assets in references, including code, 737
dataset, and models, acknowledging the contributions of their creators and respecting the 738
associated licenses and terms of use. 739
Guidelines: 740
‚Ä¢ The answer NA means that the paper does not use existing assets. 741
‚Ä¢ The authors should cite the original paper that produced the code package or dataset. 742
‚Ä¢The authors should state which version of the asset is used and, if possible, include a 743
URL. 744
21‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset. 745
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of 746
service of that source should be provided. 747
‚Ä¢If assets are released, the license, copyright information, and terms of use in the 748
package should be provided. For popular datasets, paperswithcode.com/datasets 749
has curated licenses for some datasets. Their licensing guide can help determine the 750
license of a dataset. 751
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of 752
the derived asset (if it has changed) should be provided. 753
‚Ä¢If this information is not available online, the authors are encouraged to reach out to 754
the asset‚Äôs creators. 755
13.New Assets 756
Question: Are new assets introduced in the paper well documented and is the documentation 757
provided alongside the assets? 758
Answer: [Yes] 759
Justification: The novel universal image restoration framework introduced in the paper is 760
well documented, and the documentation is provided alongside the model in Sections 3, 761
offering comprehensive details for replication and application. 762
Guidelines: 763
‚Ä¢ The answer NA means that the paper does not release new assets. 764
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their 765
submissions via structured templates. This includes details about training, license, 766
limitations, etc. 767
‚Ä¢The paper should discuss whether and how consent was obtained from people whose 768
asset is used. 769
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either 770
create an anonymized URL or include an anonymized zip file. 771
14.Crowdsourcing and Research with Human Subjects 772
Question: For crowdsourcing experiments and research with human subjects, does the paper 773
include the full text of instructions given to participants and screenshots, if applicable, as 774
well as details about compensation (if any)? 775
Answer: [NA] 776
Justification: The paper does not engage in crowdsourcing experiments or research with 777
human subjects therefore, it does not include participant instructions, screenshots, or details 778
about compensation. 779
Guidelines: 780
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 781
human subjects. 782
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu- 783
tion of the paper involves human subjects, then as much detail as possible should be 784
included in the main paper. 785
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 786
or other labor should be paid at least the minimum wage in the country of the data 787
collector. 788
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 789
Subjects 790
Question: Does the paper describe potential risks incurred by study participants, whether 791
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 792
approvals (or an equivalent approval/review based on the requirements of your country or 793
institution) were obtained? 794
Answer: [NA] 795
22Justification: The paper does not involve research with human subjects, so there are no 796
participant risks to disclose, and no Institutional Review Board (IRB) approvals or equivalent 797
reviews were required. 798
Guidelines: 799
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 800
human subjects. 801
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent) 802
may be required for any human subjects research. If you obtained IRB approval, you 803
should clearly state this in the paper. 804
‚Ä¢We recognize that the procedures for this may vary significantly between institutions 805
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 806
guidelines for their institution. 807
‚Ä¢For initial submissions, do not include any information that would break anonymity (if 808
applicable), such as the institution conducting the review. 809
23