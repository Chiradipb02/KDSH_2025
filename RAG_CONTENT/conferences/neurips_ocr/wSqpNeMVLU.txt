A Theoretical Perspective for Speculative Decoding
Algorithm
Ming Yin˚
Princeton University
my0049@princeton.eduMinshuo Chen
Northwestern University
minshuo.chen@northwestern.edu
Kaixuan Huang
Princeton University
kaixuanh@princeton.eduMengdi Wang
Princeton University
mengdiw@princeton.edu
Abstract
Transformer-based autoregressive sampling has been the major bottleneck for
slowing down large language model inferences. One effective way to accelerate
inference is Speculative Decoding , which employs a small model to sample a
sequence of draft tokens and a large model to validate. Given its empirical effec-
tiveness, the theoretical understanding of Speculative Decoding is falling behind.
This paper tackles this gap by conceptualizing the decoding problem via markov
chain abstraction and studying the key properties, output quality and inference
acceleration , from a theoretical perspective. Our analysis covers the theoretical
limits of speculative decoding, batch algorithms, and output quality-inference
acceleration tradeoffs. Our results reveal the fundamental connections between
different components of LLMs via total variation distances and show how they
jointly affect the efficiency of decoding algorithms.
1 Introduction
The recent surge of scaling Transformer models has led to the flourishing of AI, where success
has been witnessed in wide areas such as natural language [ 39,1], computer vision [ 12,17], video
generations [ 3,19], and robotics [ 8,31]. In the meantime, the decoding process also becomes more
and more time-consuming as the model size scales up. This is mainly due to the autoregressive nature
of Transformers, where each generated token also serves as the input for future generations. As a
result, decoding Ttokens would take Tforward passes of the full model.
A recent effort to tackle this challenge is speculative decoding (SD) [ 10,24], where the autoregressive
sampling is performed on a small draft model and the large language model verifies tokens generated
by draft model to decide whether it should be accepted/rejected. Once a token is rejected, the
generation process will start from the most recently accepted token, until a full response is completed.
Speculative decoding achieves 2-2.5 ˆLLM inference speedup empirically, while preserving the
quality of generation.
Subsequently, numerous studies [ 26,25,23,46] have expanded this methodology, enabling further
inference acceleration. Intuitively, for speculative decoding, when the generation distribution of small
model pand large model qare close to each other, decoding is faster (since less rejection occurs), and
when the distribution overlap between pandqis small, the opposite happens. However, a precise
understanding of inference accelerating given the small model pand large model qremains elusive.
This motivates us to ask the following question:
˚Correspondence to: my0049@princeton.edu ,mengdiw@princeton.edu .
38th Conference on Neural Information Processing Systems (NeurIPS 2024).What is the fundamental limit for inference acceleration via speculative decoding? In addition, what
is the best trade-off between inference acceleration and output quality for speculative decoding?
His first Final was in Rio, Brazil, 2014.Q: When and where was Messi’s first World Cup Final? 
His first Final was in Lusail, Qatar, 2024.Q: When and where was Messi’s first World Cup Final? 
Figure 1: Left: Standard Auto-Regressive Decoding (Algorithm 3) v.s. Right: Speculative Decoding
(Algorithm 1), where a large model is used to validate the responses of the small model.
In this paper, we answer these questions from the theoretical lens. Our contributions are summarized
as follows.
We formalize the decoding problem through the Markov Chain abstraction that establishes the
theoretical setup. We draw the connection runtime“# rejections and use it to measure efficiency.
We derive the exact formula, fully characterized by distribution pandq, for the expected rejections
ErNrejsfor Speculative Decoding (Theorem 1). This renders a theoretical reference for understanding
the acceleration rate T{ErNrejs.
Next, to understand whether Speculative Decoding can be further improved, we generalize it to a
class of rejection-based algorithms 2 where probability btand distribution Ptcan be customized.
We prove in Theorem 2 that any unbiased algorithm cannot have fewer rejections than Speculative
Decoding. This indicates its optimality among the class, and having fewer rejections needs to suffer
quality loss or requires extra information.
Furthermore, we consider a batch version of Speculative Decoding (Algorithm 4) that utilizes multiple
draft sequences. We show our batch algorithm is unbiased. We derive the expected rejections that is
fully expressed by pandqand exhibit the improvement over non-batch version in Theorem 3. We
provide examples and detailed discussion to explain how our theory characterize the improvement.
In section 4, we shift from unbiased algorithms and study the tradeoff between inference cost and
quality degradation . We formulate this into an optimization model (1). Theorem 5 established a
linear Pareto front that characterizes the tradeoff between inference cost and quality degradation
(Figure 4). A simple experiment in Section 4.2 is also consistent with our theoretical finding.
Last but not least, our technical results involve novel analysis, for instance, the design of V`,V´in
the lower bound proof C and the iterative computation for fin D.1. They are the first of its kind and
consist of our technical contributions. We provide a proof sketch section in Appendix A.
1.1 Related works
Speculative Decoding and its applications. Speculative execution, where performing speculative
work can expedite computation on parallel machines by allowing certain tasks to commence before
their necessity is confirmed, can date back to [ 9,15]. Recently, [ 10,24] formalize this idea with
rejection sampling based design for LLM Decoding and achieve multiple-time inference acceleration
compared to vanilla auto-regressive decoding. There are fruitful studies [ 41,26,25,37,23,46,32,
27,18,4,34,2,30,43,11,42,45,29,35,5,44,40,20] since then, and they improve speculative
decoding from different angles such as online updating [ 25], multiple candidates [ 45], retrieval
technique [18], Multimodality [16] or even decoding without draft models [14, 6].
Theoretical endeavor for Speculative Decoding. There are also works that study the theoretical
properties for speculative decoding (SD). In particular, [ 37] considers speculative decoding from the
optimal transport perspective and show it is optimal in the single token regime. It further extends to the
kmultiple draft token setting and formulates the optimal transport solution via linear programming
2with exponential in kcomputation time. An approximate sequential selection algorithm is also
proposed with linear time. [ 2] further proposes the improved plan, and [ 36] extends [ 37] to the
block-level optimal transport for SD. [ 34] investigates the synergy between draft length and batch size
for Speculative Decoding and formulate the optimal speculation length as the root of a polynomial
equation. [26] proposes the SpecInfer, a batch algorithm that uses small speculative models to form
the token tree, and proves its output matches the distribution of the large model. [ 45] considers batch
speculative decoding without replacement to avoid repeatedly sampling rejected tokens and proves it
keeps the decoding quality. Nevertheless, the findings for inference acceleration of these works are
mostly empirical, lacking theoretical guarantees.
Algorithm 1 Speculative Decoding [ 10,24]
1:Input : Set probability bt“mint1,qt
ptu
and the distribution Pt“rqt´pts`in
Algorithm 2.
2:Require :ptp¨q:“ptp¨|x1:n´1,˜xn:t´1q,
qtp¨q:“qtp¨|x1:n´1,˜xn:t´1q.@těn.
3:fort“n:Tdo
4: Sample r„Uniformr0,1s.
5: ifrďmin!
1,qtp˜xtq
ptp˜xtq)
then
6: Accept with xn“˜xt.nÐn`1.
7: else
8: Sample xn„rqt´pts`p¨q.
9: nÐn`1. Break.
10: // Recallrqt´pts`in Section 2.1
11: end if
12:end forAlgorithm 2 Framework for Rejection-based Decoding
1:Init: Horizon T, models qt,pt. Lookahead K“T.
Prompt x0.n“0.
2:Require : Probability bt, distribution Pt.
3:while năTdo
4: fort“n:Tdo
5: Sample drafts ˜xt„ptp¨|x1:n´1,˜xn:t´1q.
6: end for
7: Obtain the target logits in parallel for˜xn:Tas
qnp¨|x1:n´1q, . . . , q Tp¨|x1:n´1,˜xn:Tq.
8: fort“n:Tdo
9: Accept xn“˜xtwith prob. bt.nÐn`1.
10: Else REJECTION:
11: Sample xn„distribution Pt.nÐn`1.
Break.
12: end for
13:end while
2 Preliminaries
2.1 Background for decoding problems
In this section, we provide the mathematical formulation for decoding problems using Markov Chains,
and we explain auto-regressive models and speculative decoding algorithm based upon that.
A Markov Chain Model for Decoding. We denote x0as the prompt.2xnis the n-th token output
andx1:Tis the trajectory output by the algorithm with Tto be the fixed decoding horizon.3. Then
any decoding algorithm can be characterized by a Markov Chain: state at tis described by history
x0:t, the initial state is x0; the state transition Ptmaps state x0:tto state x0:t`1. In the context of
decoding problem, the transition matrix is defined as Ppx0:t`1|x0:tq:“ppxt`1|x1:tq. In particular,
we use pto denote the (conditional) distribution for the draft model that resembles small speculative
model, and qfor the target model that represents large language model. We use rq´ps`to denote
the normalized distribution for maxt0, qpxq´ppxqu,@xPVwithVbeing the token vocabulary. We
also denote ¯Ex„frgs:“ř
xfpxqgpxq.
Auto-Regressive Decoding. Consider sampling a trajectory x1:Tfrom an auto-regressive model,
where for the given x1:n´1, the next token xnfollows the conditional distribution q. This decoding
mechanism (Algorithm 3) is the prototype for Transformer-based LLMs ( e.g.GPT-4). As mentioned
in [29], the accuracy performance of Transformer-based LLMs has been shown to scale with model
size, with larger models demonstrating improved capabilities [ 22]. However, this improvement comes
at the cost of higher latency during inference and increased computational requirements.
Speculative Decoding. Different from large models, small models are usually much faster at the
inference stage. Consequently, we can use a small model to perform auto-regressive sampling
and assign large model as a verifier, where the goal of the large model is to check whether the
2We use the abstraction x0:“py1, y2, . . . , y npromptqwithyi’s being the tokens in the prompt.
3In general, Tis a random variable. However, we can append [EOS] tokens to keep the output length fixed.
3token sampled by the small model should be accepted/rejected. Concretely, this procedure can be
summarized into the following three steps:
•Draft sampling : given the verified tokens x1:n´1, the draft model obtains Ksequential
candidate tokens ˜xn:n`K´1via sampling from pp¨|x1:n´1,˜xn:n`iq,iPrK´1s;
•Conditional score computation : given ˜xn:n`K´1, computing the logits of the Ktokens
qp¨|x1:n´1,˜xn:n`iq,iPrK´1sin parallel ;
•Token validation : Accept the candidate token ˜xt(asxn) with some probability 0ďbtď1.
If accepted, continue to validate the next candidate ˜xt`1, otherwise reject and sample xn
from some designed distribution Pt.
The above process repeats until x1:Tare generated, and the whole algorithm is summarized as the
general rejection-based decoding 2. Specifically, Speculative Decoding [10,24] designs btp˜xtq:“
mint1,qtp˜xtq
ptp˜xtquand distribution Ptp¨q:“rqt´pts`p¨q(see Algorithm 1 and Figure 1).
2.2 Problem Setup
Speculative Decoding has two key merits. First, it maintains output quality , meaning the output
distribution by the algorithm is identical to the output distribution of the large model q, and we term it
distribution unbiasedness . Second, it has fast inference property since the auto-regressive sampling
is performed on the small model and the target model only verifies. With (possibly) multiple draft
tokens being accepted very round, Speculative Decoding can be much faster than direct decoding on
large models, and its inference is bottlenecked by the parallel score computation qp¨|x1:n´1,˜xn:n`iq,
iPrT´ns. To highlight this, we defined it as the oracle call and make an assumption based on that.
Definition 1. We define one trigger of obtaining logits for ˜xn:Tin Step 7 of 2 as one Oracle call .
Assumption 1. We assume that: (1) compared to the large model, the computational cost of the
draft/small model is negligible. (2) each oracle call has runtime Op1q.
Remark 1. We assume the above only for the theoretical cleanliness. With negligible draft model, the
lookhead K“Tin Algorithm 2. In other words, given x1:n´1, instead of sampling the next Kdraft
tokens ˜xn:n`K´1, we are allowed to sample until the end, i.e.˜xn:T. In practice, Assumption 1(1)
also holds true in many cases. One example of negligible-cost model c«0isn-gram models, and the
empirical evidence in [ 24,28] shows n-gram draft model speeds up inference pretty well. In addition,
for summarization tasks where long sequences are likely to repeat, any draft model that reuses tokens
from the context with a matching prefix, is also cost negligible. Assumption 1(2) is also a standard
abstraction, since parallelization consumes (roughly) equal computation as for a single logit. It will
consume more memory, but such aspect is beyond the scope of our theoretical study.
Metric for measuring efficiency. Desired algorithms should preserve the output quality and be
efficient for decoding. To formalize, our theory aims to study the following two quantities:
•Inference acceleration : the ratio between the inference time of decoding large model qand
the inference time of decoding the algorithm;
•Output quality : the distribution bias between the algorithm and the large model distribution
q. An algorithm maintains the output quality if it is distribution unbiased.
Rejections, and why consider it? A third metric for decoding is the number of draft tokens being
rejected. With more draft tokens being accepted, the fewer rejections would occur. Therefore,
algorithms with large number of rejections are slower than those with fewer rejections. This means
Rejections serves as an alternative metric for the inference speedups. Throughout the paper, we use
number of rejections for measuring inference acceleration .
To further motivate why choosing Rejections is appropriate, we draw its connection to inference run-
time. For Speculative Decoding, the runtime is dominated by the number of oracle calls (Definition 1).
After each rejection (Step 10 of 2 or Step 7 of 1), there is one oracle call, thus we obtain
inference time “# oracle calls“# rejections .
Notice the runtime for auto-regressive decoding (3)isT, therefore we have the relation: inference
acceleration“T{# rejections . Based on this setup, we present our main results in next sections.
43 Analysis on Efficiency and Optimality for Speculative Decoding
We start with the following theorem at the beginning of the section. It covers output quality, which is
measured by distribution bias, and expected number of rejections for speculative decoding. Its proof
is deferred to Appendix B.
Theorem 1. We have the following two results for Speculative Decoding.
(1) We define random variables RnPt0,1uthat indicates whether the n-th token is rejected (with 1
being rejected). Here rejection means Line 7 of Algorithm 1 is executed. Then, the total number of
rejections Nrej“řT
n“1Rn. For Speculative Decoding (here TVdenote the TV distance):
ErNrejs“Tÿ
n“1Ex1:n´1„qrTVppnp¨|x1:n´1q, qnp¨|x1:n´1qqs.
(2) The output distributions of Algorithm 1 and the target model qare identical, i.e.for any output
sequence x1:TPVT, the joint the distributions over x1:Tsatisfies: PSDpx1:Tq“qpx1:Tq.
The first part of Theorem 1, to our knowledge, is the first result that characterizes the expected
rejection for speculative decoding a sequence of length Tusing pandq. The second part of
Theorem 1 shows the distribution unbiasedness for SD, which has been presented in [ 10,24]. There
are three interesting indications:
•IfErTVppn, qnqp¨|x1:n´1qs“ 0for all n, all tokens are accepted and the accelerate rate
“T;
•IfErTVppn, qnqp¨|x1:n´1qs“1for all n, all tokens are rejected and the accelerate rate “1,
i.e.allTtokens are sampled from the large model q;
• In general, the accelerate rate for SD is T{řT
n“1ErTVppn, qnqp¨|x1:n´1qs.
Remark 2. [24] derive the expected number of token generated per run of Speculative De-
coding as 1{ErTVpp, qqsforK“ 8 .4Their result equals T{řT
nErTVppn, qnqp¨|x1:n´1qswhen
ErTVppn, qnqp¨|x1:n´1qsis identical for all n, and this is due to their assumption that the acceptance
rates βare i.i.d.. In contrast, our guarantee holds for the case that allows the sequential dependence
between different decoding steps.
Simulation. We also provide a simulation of Speculative Decoding and compare it with our Theorem 1
in the left panel of Figure 2(a) with horizon T“50,pn, qn, n“1, . . . , 50are nonstationary Markov
Chains. The green line is the empirical average rejections among 100ˆNruns of Algorithm 1
and the orange line the theoretical value computed via Theorem 1. From the simulation, after 5000
runs the empirical average rejections converge to our theoretical value 16.41. In this example, the
acceleration rate is 50{16.41“3.05. The specifications of the simulation is included in Appendix F.
3.1 Optimality of Speculative Decoding
Now we have analyzed the efficiency of speculated decoding and provided mathematical characteri-
zation of the number of expected rejections, which is depicted by the TVdistance and scales linearly
with the inference time. A natural next-step question to ask is: Is there any other rejection-based
algorithm 2 that can do better? We answer this question in the next theorem.
Theorem 2 (Instance-dependent Rejection Lower Bound ).For an instance P:“pp, qq, where
p, qstand for the distributions of the draft model and the target model respectively, defining
the family of algorithms as F:“ tA:Ais a specification of Algorithm 2 that satisfies PA
t“
qt@tpi.e., unbiasedqu.For an algorithm A, denote Nrejas the number of rejections. Then we have
the lower bound
inf
APFEA
PrNrejsěTÿ
n“1Ex1:n´1„qrTVppn, qnqp¨|x1:n´1qs.
4This is from their equation (1) that has 1{p1´αqwithα“Epminpp, qqqand1´Epminpp, qqq “
ErTVpp, qqs.
5Figure 2: The numeric instance in this figure chooses p, qto be nonstationary Markov Chains
with horizon T“50. Leftpaq: A simulation of Speculative Decoding. The green line is the
empirical average rejections among 100Nruns and the orange line the theoretical value computed
via Theorem 1. Middle pbq: Batch Speculative Decoding simulations with batch M“4,5. The
green/purple lines are the empirical average rejections among 100Nruns and the orange/pink lines
are the theoretical values computed via Theorem 3. Right pcq: The scaling law of expected rejections
for Batch SD as a function of M. It converges to a limit as MÑ8 .
Takeaways. Via Theorem 2, the answer to the key question is: No rejection improvement can be
made in Algorithm 2 by changing the acceptance probability btand the distribution Ptif we want
to keep the distribution unbiasedness. We point out that lower bound of Theorem 2 matches the
complexity upper bound of Speculative Decoding given by Theorem 1. This result confirms that
speculative decoding is optimal in the class of all rejection-based methods.
The practical implication is that there is no need to tweak acceptance probability, as it will not make
performance better. In the next Section 3.2, we will see that Speculative Decoding can be provably
improved, as long as one can decode and verify multiple sequence of tokens in parallel.
Connection to optimal transport. We mention [ 37] nicely consider maximizing the acceptance
probability from the optimal transport perspective. For a single token, the optimal transport cost
isř
xminpppxq, qpxqq, which corresponds to the optimal expected rejection TVpp, qq. However,
for the sequential Ttokens, their Section 5,6 does not provide a explicit formulation for optimal
acceptance/rejections. In this sense, their optimality result can be cast as a special case of ours.
However, we do emphasis there are differences in the settings, where [ 37] consider the optimal
transport and we study the class F.
3.2 Analysis for Batch Speculative Decoding
To further improve the provable efficiency, we consider batch algorithms that extend the speculative
decoding with multiple candidate draft sequences. Most of the existing works [ 32,34,26,45,21]
formulates batch sequences via a speculation tree structure. In particular, the representative work [ 26]
propose the merged token tree structure that combines sequences with the same prefix. A depth-first
search is designed for speculation to ensure the unbiasedness of the algorithm. In addition, [ 33]
devise the parallel decoding structure that speculate the token of each responses given its previous
tokens are accepted. Motivated by these works, we consider a batch version of speculative decoding
algorithm using a simpler parallel structure (Left of Figure 3). Our Algorithm 4 can be viewed as a
simplified approximation to those batch algorithms. There are several differences that distinguish
batch algorithm from the non-batch version. We highlighting them as follows.
Difference1: Oracle call. At the beginning of batch speculation, Mdraft sequences are generated in
parallel as well as the corresponding logits q. This corresponds to Step 4-9 of Alg 4 and is defined as
oneoracle call which we assume to have unit computation Op1q;5
Difference2: Speculation procedure. It follows the DFS principle: If the first token of a response
is accepted, only that response will be speculated until rejection. For instance in the Left panel of
Figure 3, if the token ‘deep’ is accepted, the algorithm will keep verifying token ‘learn’, ‘ing’ until
rejection and the rest of sequences won’t be examined; if ‘deep’ is not verified, then the algorithm
will keep examing ‘rein’. In this case, rejection happens only if ‘deep’, ‘rein’ and ‘atten’ are all
5We mention batch drafting in parallel will cause more memory and arithmetic operations, but not computa-
tion time.
6rejected. Once rejection happens, the process will restart. By this design, it still holds true that:
inference time “# oracle calls“# rejections .
Again, we measure the output quality and rejections for the batch algorithm. The following main
result (Theorem 3) provides the explicit characterization for rejections and batch improvement.
Detailed proofs and discussions can be found in D.
What are important components of AI?…-&'(MachineinglearndeeplearningreinforcementlearnattentionmechanismWhat are important components of AI?Machineinglearn
❌
❌
❌
❌-"
❌Large Model DecoderVerifyLarge Model DecoderVerify
✅
✅
❌
Figure 3: Left: Batch Speculative Decoding. Right: Batch Improvement vs. Batch size M. Upper:
Bernoulli distributions with q“Berp0.5q. Lower: p„UnifpVq,q„UnifpV1qwithr“V{V1.
Theorem 3 (Unbiasedness and efficiency of batch SD ).Recall the definition of RnandNrejin
Thm 1, and iteratively define: qm`1“rqm´ps`,@mPrMswithq1“qbeing the target distribution.
Then, for Batch Speculative Decoding 4, PBatchpx1:Tq“qpx1:Tq.@x1:TPVT. Moreover,
ErNrejs“Tÿ
n“1Ex1:n´1„qrTVrq, psp¨|x1:n´1qs´Tÿ
n“1¯Ex1:n´1„frTVpq, pqpx1:n´1q´rMź
m“1TVpqm, pqpx1:n´1qss
looooooooooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooooooooon
Batch Improvement
where fpx1:nq:“Ppx1:nXtn-th draft token rejected uq.fcan be iteratively computed via p, q.
Takeaways. The expected rejection of Batch Decoding composes of two parts: (i)řT
nEqrTVrq, pss
which is the rejection that matches the non-batch speculative decoding; (ii)the batch improvement
(BI) which comes from our design and is always non-negative. To better understand how the batch
improvement scale with M, we instantiate the single token BIpq, pq:“TVrq, ps´śM
m“1TVrqm, ps
with two simple examples.
Uniform Distribution. For the whole space V, letpbe a uniform distribution with support Vand
qbe a uniform distribution over a subset of Vwith size V1ăV. Let V{V1“r, in this case
BIpUnifpV1q,UnifpVqq“p 1´1
rq´p1´1
rqM, r“V{V1.Its pattern is visualized in the lower
right panel of Figure 3. The improvement converges to 1´1{rand is always positive as long as
batch size is at least 2. Also, when the vocabulary size Vscales up, i.e.rgoes up, the improvement is
going to zero. This is not surprising, since the probability of draft sample to fall within in the support
of target distribution is very small.
Bernoulli Distribution. Suppose uěvand let p„Berpuq,q„Berpvq. Then
BIpBerpvq,Berpuqq “ | u´v|¨p1´uM´1q.Its pattern is exhibited in the upper right panel of
Figure 3. Notice for both cases, the limiting batch improvement is TVpp, qq, which is only significant
when pdeviates from q. This provides the practical design heuristic: batch design can significantly
reduce the number of rejections when the draft distribution pand target distribution qare different
from each other and won’t make too much improvement when pandqare close.
Batch Simulation. The Middle panel of Figure 2(b) shows Batch Speculative Decoding simulations
with batch M“4,5. The green/purple lines are the empirical average rejections simulated via
Algorithm 4 and the orange/pink lines are the theoretical values computed via Theorem 3. The right
panel of Figure 2(c) plots the rejection vs. batch size. In particular, the the black dot with coordinate
p1,16.41qrepresents the Speculative Decoding 1. The numeric example shows when MÑ 8 ,
ErNrejsÛ0. Intuitively, this is partial due to, if the distribution mismatch between pandqis large,
7Rejection ProbabilityDistribution Distance TV[ℙ.,*]0TV[,,*]TV[,,*]Attainable RegionUnattainable Region
Figure 4: Left paq: The Pareto Front between Rejection Probability PAprejectqvs.Distribution bias
TVrPA, qs. For a given rejection probability, the black line denotes the optimal deviation Loss˚
TV.
Middlepbqand Rightpcq: A numeric example. In the plot, the over acceptance ϵ’s are set as positive
constants that define bpxq“mint1,qpxq`ϵ
ppxqu.
rejection is unavoidable no matter how many draft responses are sampled from p. We also formally
prove this in Proposition 1. This theoretical discovery indicates that having a very large draft batches
does not necessarily result in significant inference speedups compared to small batches.
4 Analysis on the Optimal Rejection-Distribution Bias Tradeoff
In earlier sections, we focus on analyzing SD or Batch SD which are distribution unbiased. To further
reduce rejections, the algorithm may have to compromise on output quality. Nevertheless, it is usually
acceptable for many real-world applications. For instance, for the family of Gemini models [ 38],
Google may deploy the small model Gemini Nano as the draft model pandGemini Ultra as the target
model qand tune the acceptance probability b(in Algorithm 2) higher such that Gemini Nano is
applied more often for the purpose of less expenditure. Therefore, an intriguing question to ask is:
For biased algorithms, what is the optimal tradeoff between rejection and distribution bias?
4.1 An Optimization Formulation and Pareto-optimal Characterization
We measure the distribution bias between PAandqviaTVdistance.6Concretely, for any algorithm
A:“pb,Pqin 2 with acceptance probability band rejection distribution P, we fix band aim to
optimize the following objective
Loss˚
TVpbq:“min
PTVrPA, qs, where A:“pb,Pq. (1)
We wish to solve the above since it characterizes the minimal distribution bias for any design b. We
present our solution in the next.
Theorem 4 (Optimal solution to optimization (1)) .We can show the objective (1)is equivalent to
Loss˚
TVpbq:“1
2min
Pÿ
xˇˇˇˇˇqpxq´bpxqppxq´Ppxqÿ
˜xr1´bp˜xqspp˜xqˇˇˇˇˇs.t.ÿ
xPpxq“1,Ppxqě0,@x.
Supposeř
xr1´bpxqsppxq ą 0. Define Apxq:“qpxq´bpxqppxqř
˜xr1´bp˜xqspp˜xq,and the sets A`“ txPV:
Apxqě0u,A´“txPV:Apxqă0u. Then the set of optimal distributions of objective (1)is
characterized as tP˚:P˚|A´p¨q“ 0; 0ďP˚|A`p¨qď Ap¨qu,and the optimal value is Loss˚
TVpbq“
1
2ř
x|qpxq´bpxqppxq|´1
2ř
xp1´bpxqqppxqě0.
Theorem 4 is a universal characterization that contains both biased and unbiased situations. 1. If
bis less than Speculative Decoding threshold, i.e.bďmint1, q{puthenAbecomes a probability
distribution and the optimal distribution P˚equals Awhich is also unbiased ( Loss˚
TVpbq“0); 2. If b
exceeds the Speculative Decoding threshold, then Ais no longer a distribution and there are multiple
optimal distributions P˚. In this case, the optimal distribution bias Loss˚
TVpbqą0for Algorithm 2.
6We mention TVdistance is only one metric for measuring distribution bias. In general, any distribution
distance can be considered. We leave a thorough study for all other distances as future work.
8Main Takeaways. Via Theorem 4, we derive the pareto front (the optimal tradeoff) between rejection
probability7Pprejectqvs. distribution distance TVrPA, qs(Left panel of Figure 4). The blue region
can be realized by some algorithm 2, and the red region cannot. p0,0qis the “perfect algorithm”
(no rejection, no bias) which does not exists, and, in particular, p0,TVrp, qsqstands for Speculative
Decoding. Surprisingly, the pareto front is a straight line connecting p0,TVrp, qsqandpTVrp, qs,0q,
which represents a linear relationship between the rejection probability and the optimal TVdeviation.
This is guaranteed by the following theorem.
Theorem 5 (Pareto front ).For any Algorithm Ain the class 2 that satisfies mint1,qpxq
ppxquďbpxqď
1,@xPV. Then
PAprejectq`Loss˚
TVpbq“TVrp, qs.
HerePAprejectq“1´ř
xbpxqppxqand Loss˚
TVpbq:“minPTVrPA, qs.
We plot the numerical example using two Markov Chains in the middle and right panel of Figure 4
that coincides with our theoretical finding. In the figure, the acceptance probability is set to be
bpxq “mint1,qpxq`ϵ
ppxqu. The orange line in pcqand the green boundary in pbqare computed via
Loss˚
TVpbqfrom Theorem 4. The complete proofs for Theorem 5 and Theorem 4 are deferred to
Appendix E. For the clearness of illustration, we focus on the pareto front between rejection vs. the
minimal TVdeviation for a single token.
4.2 Experiment
We provide an additional simple experiment to show the effectiveness of our Pareto-optimal solution
in Theorem 4. Consider objective (1). For the given acceptance probability bąmint1, q{pu, we
have two options for P: 1. Baseline: select Pto be suboptimal to (1), i.e.simply set P:“q, which
is the target distribution; 2. Set P:“P˚be the optimal distribution of Theorem 4. We call the
first method Decoding-UNO and the latter one Decoding-OPT . Instead of TV distance, we measure
the quality via WinRate in our experiment. Concretely, for each prompt, we let Decoding-UNO
andDecoding-OPT to generate responses independently, and use score models to compare whose
generation has higher quality. We specify draft model paspythia-70m and target model qas
pythia-2.8b from EleutherAI [ 7]. We apply the score model to be RM-Mistral-7B orGPT-4 .
We test 200 prompts from Alpaca-Farm-Eval Dataset [ 13] with 500 responses/comparisons per
prompt. Table 1 shows that Decoding-OPT achieves better performance than Decoding-UNO across
different choice of ϵ’s. Due to space constraint, the missing details are deffered to Appendix G.
MethodRM-Mistral-7B GPT-4
ϵ“0.1ϵ“0.2ϵ“0.4ϵ“0.8ϵ“0.1ϵ“0.2ϵ“0.4ϵ“0.8
Decoding-OPT 53% 53.5% 57.5% 52.5% 54.5% 53% 53.5% 55%
Decoding-UNO 47% 46.5% 42.5% 47.5% 45.5% 47% 46.5% 45%
Table 1: WinRate for Decoding-OPT vsDecoding-UNO with different over-acceptance threshold ϵ.
The acceptance probability bpxq“mint1,qpxq`ϵ
ppxqu.
5 Discussions
On the optimality for batch algorithms. Unlike their non-batch counterparts, our batch algorithm
studies do not come with optimality guarantees. This is largely due to the diverse and arbitrary nature
of batch algorithm designs, making it challenging to define a comprehensive class that encompasses a
wide range of batch algorithms. While [ 37] investigate optimal batch algorithms through an optimal
transport lens, their work does not extend to calculating optimal rejection rates or developing an
efficient algorithm to achieve this (they only propose an approximate solution). Consequently, the
pursuit of batch optimality remains an open field. Identifying the optimal batch algorithm could yield
valuable insights for enhancing practical applications in real-world scenarios.
7Here the rejection probability is computed as Pprejectq “ř
˜xPpreject,˜xq “ř
˜xPpreject|˜xqPp˜xq “ř
˜xp1´bp˜xqqpp˜xq.
9Extending Speculative Decoding to other studies. Speculative Decoding is a generic sampling
approach that extends beyond mere decoding tasks. It holds potential for wider applications such
as search engines and recommendation systems, where it can be employed to quickly generate and
refine search outcomes or content suggestions, enhancing the overall efficiency and user experience
of these systems. We leave these as future works.
Acknowledgments
The authors would like to thank anonymous reviewers for their valuable feedback. Mengdi Wang
acknowledges the support by NSF IIS-2107304, NSF CPS-2312093, and ONR 1006977.
References
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774 , 2023.
[2]Kwangjun Ahn, Ahmad Beirami, Ziteng Sun, and Ananda Theertha Suresh. Spectr++: Improved
transport plans for speculative decoding of large language models. In NeurIPS 2023 Workshop
Optimal Transport and Machine Learning , 2023.
[3]Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia
Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 6836–6846, 2021.
[4]Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exiting
framework for autoregressive language models with synchronized parallel decoding. arXiv
preprint arXiv:2310.05424 , 2023.
[5]Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, and
Babak Ehteshami Bejnordi. Think big, generate quick: Llm-to-slm for fast autoregressive
decoding. arXiv preprint arXiv:2402.16844 , 2024.
[6]Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and
Mahyar Najibi. Speculative streaming: Fast llm inference without auxiliary models. arXiv
preprint arXiv:2402.11131 , 2024.
[7]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,
Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward
Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In
International Conference on Machine Learning , pages 2397–2430. PMLR, 2023.
[8]Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,
Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics
transformer for real-world control at scale. arXiv preprint arXiv:2212.06817 , 2022.
[9]F Warren Burton. Speculative computation, parallelism, and functional programming. IEEE
Transactions on Computers , 100(12):1190–1193, 1985.
[10] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and
John Jumper. Accelerating large language model decoding with speculative sampling. arXiv
preprint arXiv:2302.01318 , 2023.
[11] Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin Chen-
Chuan Chang. Cascade speculative drafting for even faster llm inference. arXiv preprint
arXiv:2312.11462 , 2023.
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
10[13] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
methods that learn from human feedback. Advances in Neural Information Processing Systems ,
36, 2023.
[14] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm
inference using lookahead decoding. arXiv preprint arXiv:2402.02057 , 2024.
[15] Freddy Gabbay and Avi Mendelson. Speculative execution based on value prediction . Technion-
IIT, Department of Electrical Engineering, 1996.
[16] Mukul Gagrani, Raghavv Goel, Wonseok Jeon, Junyoung Park, Mingu Lee, and Christo-
pher Lott. On speculative decoding for multimodal large language models. arXiv preprint
arXiv:2404.08856 , 2024.
[17] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang,
An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. IEEE transactions on
pattern analysis and machine intelligence , 45(1):87–110, 2022.
[18] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based
speculative decoding. arXiv preprint arXiv:2311.08252 , 2023.
[19] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High
definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 , 2022.
[20] Kaixuan Huang, Xudong Guo, and Mengdi Wang. Specdec++: Boosting speculative decoding
via adaptive candidate lengths. arXiv preprint arXiv:2405.19715 , 2024.
[21] Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, and Christopher Lott.
Recursive speculative decoding: Accelerating llm inference via sampling without replacement.
arXiv preprint arXiv:2402.14160 , 2024.
[22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361 , 2020.
[23] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir
Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. In Thirty-seventh
Conference on Neural Information Processing Systems , 2023.
[24] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via
speculative decoding. In International Conference on Machine Learning , pages 19274–19286.
PMLR, 2023.
[25] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao
Zhang. Online speculative decoding. arXiv preprint arXiv:2310.07177 , 2023.
[26] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,
Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating
generative llm serving with speculative inference and token tree verification. arXiv preprint
arXiv:2305.09781 , 2023.
[27] Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning. An
emulator for fine-tuning large language models using small language models. arXiv preprint
arXiv:2310.12962 , 2023.
[28] Jie Ou, Yueming Chen, and Wenhong Tian. Lossless acceleration of large language model via
adaptive n-gram parallel decoding. arXiv preprint arXiv:2404.08698 , 2024.
[29] Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda,
Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, and Anoop Deoras. Bass: Batched attention-
optimized speculative sampling. arXiv preprint arXiv:2404.15778 , 2024.
11[30] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi,
Riccardo Marin, and Emanuele Rodol `a. Accelerating transformer inference for translation via
parallel decoding. arXiv preprint arXiv:2305.10427 , 2023.
[31] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for
robotic manipulation. In Conference on Robot Learning , pages 785–799. PMLR, 2023.
[32] Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding.
arXiv preprint arXiv:2308.04623 , 2023.
[33] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep
autoregressive models. Advances in Neural Information Processing Systems , 31, 2018.
[34] Qidong Su, Christina Giannoula, and Gennady Pekhimenko. The synergy of speculative
decoding and batching in serving large language models. arXiv preprint arXiv:2310.18813 ,
2023.
[35] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless
acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint
arXiv:2404.11912 , 2024.
[36] Ziteng Sun, Jae Hun Ro, Ahmad Beirami, and Ananda Theertha Suresh. Optimal block-level
draft verification for accelerating speculative decoding. arXiv preprint arXiv:2403.10444 , 2024.
[37] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix
Yu. Spectr: Fast speculative decoding via optimal transport. arXiv preprint arXiv:2310.15141 ,
2023.
[38] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly
capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.
[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-
oth´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
[40] Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie
Ma, Tianyu Feng, Xin You, Yongjun Bao, et al. Minions: Accelerating large language model
inference with adaptive and collective speculative decoding. arXiv preprint arXiv:2402.15678 ,
2024.
[41] Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative
decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of
the Association for Computational Linguistics: EMNLP 2023 , pages 3909–3925, 2023.
[42] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li,
and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive
survey of speculative decoding. arXiv preprint arXiv:2401.07851 , 2024.
[43] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe
Liu. Llmcad: Fast and scalable on-device large language model inference. arXiv preprint
arXiv:2309.04255 , 2023.
[44] Minghao Yan, Saurabh Agarwal, and Shivaram Venkataraman. Decoding speculative decoding.
arXiv preprint arXiv:2402.01528 , 2024.
[45] Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-candidate speculative decoding.
arXiv preprint arXiv:2401.06706 , 2024.
[46] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Ros-
tamizadeh, Sanjiv Kumar, Jean-Fran c ¸ois Kagy, and Rishabh Agarwal. Distillspec: Improving
speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461 , 2023.
12Appendix
A Proof Sketch
A.1 Proof sketch of Theorem 2.
For any algorithm APF, its design bncan be written as a function of any sequence x1:n´1,˜xnwith
0ďbnp˜xn, x1:n´1qď1.8Based on this, we can further define the new function ϵn:VˆVn´1ÞÑR
according to the following equation:
bnp˜xn, x1:n´1q“min"
1,qnp˜xn|x1:n´1q`ϵnp˜xn, x1:n´1q
pnp˜xn|x1:n´1q*
. (2)
Indeed, we can choose ϵn:“bn¨pn´qn, and the validity of the definition is guaranteed by the
Lemma 1. Then, the acceptance probability bnąmint1,qn
pnuimplies ϵną0.
Next, we show for any APF, it must satisfy bnďmint1,qn
pnufor all n. To this end, we design the
two token sets V`“tx:Dn s.t. ϵ npxqą0uandV´“tx:Dn s.t. ϵ npxqď0uand prove V`“H ,
V´“V.
Finally, by Lemma 2 in Appendix, any algorithm satisfies bnďmint1,qn
pnu,@nPrTsmust have
EA
PrNrejsěřT
n“1EqrTVppn, qnqp¨|x1:n´1qs. Since APFis arbitrary, this concludes the proof.
The full proof is included in C.
A.2 High Level Proof Sketch for the second part of Theorem 3
The derivation for the number of expected rejections using the batch speculative decoding is more
involved than the Algorithm 4 due to the parallel response structure. The key step is to compute
the intermediate quantity PAp˜xnacc,˜xn“xn|x1:n´1q. Let ˜xn´1„pp¨|x1:n´2q, then there are two
cases: ˜xn´1accepted or rejected. We have
PAp˜xnacc,˜xn“xn|x1:n´1q“PAp˜xnacc,˜xn“xn,˜xn´1acc|x1:n´1q`PAp˜xnacc,˜xn“xn,˜xn´1rej|x1:n´1q
“PAp˜xnacc,˜xn“xn|˜xn´1acc, x1:n´1qlooooooooooooooooooooooomooooooooooooooooooooooon
paPAp˜xn´1acc|x1:n´1q
`PAp˜xnacc,˜xn“xn|˜xn´1rej, x1:n´1qlooooooooooooooooooooooomooooooooooooooooooooooon
pbPAp˜xn´1rej|x1:n´1q
(3)
In the process of finding pb, we need to compute the the quantity fpx1:nq:“Ppx1:nX
tn-th draft token rejected uqand it can be recursively computed via (22) using p, q.
A.3 Proof sketch for Theorem 4
Due to space constraint, we only summarize the high-level proof ideas for Theorem 4. Sinceř
xApxq“1, the original objective (1) can be equivalently rewritten as
min
P1
2ÿ
x|Apxq´Ppxq|, s.t.ÿ
xPpxq“1,Ppxqě0,@xPV. (4)
We now find the solution of objective (4) in two steps.
Step1: Recall A`“txPV:Apxqě0uandA´“txPV:Apxqă0u, then any optimal P˚must
satisfy P˚pxq“0for all xPA´. This can be shown by contradiction via an alternative construction
P1to reason P˚is suboptimal to P1.
Step2: We characterize the optimal solutions of the objective. By Step1, we can show any optimal
solution P˚satisfiesř
x|Apxq´P˚pxq|ě∥A∥1´1and the equal sign can be achieved. Then we
can convert ∥A∥1´1back to Loss˚
TV. This also helps identify the optimal set of P˚.
8It needs to follow bnPr0,1ssince bnis a probability.
13Algorithm 3 Auto-Regressive Model Decoding
1:Init: Horizon T. Distribution q. Prompt x0.n“0.
2:while năTdo
3: Sample xn„qp¨|x1:n´1q.nÐn`1.
4:end while
B Proof of Theorem 1
Theorem 6 (Restatement of the first part of Theorem 1) .We define random variables RnPt0,1u
indicating whether the n-th token is rejected with 1being rejected (here rejection means Line 6 of
Algorithm 1 is executed). Then, the total number of rejections Nrej“řT
n“1Rn. For Speculative
Decoding,
ErNrejs“Tÿ
n“1Ex1:n´1„qrTVppnp¨|x1:n´1q, qnp¨|x1:n´1qqs.
Here TVdenote the TV distance between two distributions.
Proof. Given the verified the tokens x1:n´1, we first compute PpReject at n|x1:n´1q. Denote the
candidate draft token ˜x„pnp¨|x1:n´1q, then by law of total probability
PpReject at n|x1:n´1q“ÿ
˜xPpReject at n,˜x|x1:n´1q
“ÿ
˜xPpReject ˜x|˜x, x1:n´1qPp˜x|x1:n´1q
“ÿ
˜xPpReject ˜x|˜x, x1:n´1qpnp˜x|x1:n´1q
“ÿ
˜xp1´mint1,qnp˜x|x1:n´1q
pnp˜x|x1:n´1quqpnp˜x|x1:n´1q
“ÿ
˜xmaxt0, pn´qnup˜x|x1:n´1q“TVppn, qnqp¨|x1:n´1q,
where the third equal sign uses draft token is sampled from pnand the fourth equality is by design of
Speculative Decoding (Algorithm 1, Line 4).
Lastly, by law of total expectation and above
ErNrejs“Tÿ
t“1ErRts“Tÿ
t“1ErErRt|x1:t´1ss
“Tÿ
t“1ÿ
x1:n´1ErRt|x1:t´1sPpx1:n´1q
“Tÿ
t“1ÿ
x1:n´1ErRt|x1:t´1sqpx1:n´1q
“Tÿ
t“1ÿ
x1:n´1PpReject at n|x1:n´1qqpx1:n´1q
“Tÿ
n“1Ex1:n´1„qrTVppnp¨|x1:n´1q, qnp¨|x1:n´1qqs.
Here the fourth equal sign comes from Speculative Decoding keep the distribution q(Proposition ??),
the fifth equal sign comes from the event tRn“1u“t Reject at nu.
Theorem 7 (Restatement of the second part of Theorem 1) .The output distributions of Speculative
Decoding Algorithm 1 and the large model qare identical, i.e.for any output sequence x1:TPVT,
the joint the distributions over x1:Tsatisfies: PSpecDecodingpx1:Tq“qpx1:Tq.
14Remark 3. The proof of Theorem 7 is very similar to [ 10] except we have K“8 . In addition, [ 10]
proves the distribution match for a single token, we complement the proof to show the distribution
match holds for a sequence of tokens x1:T.
Proof. In particular, we use induction to show the stronger result that @tPrTs,@x1, x2, . . . , x tPV,
it holds PA
tpx1, x2, . . . , x tq“qtpx1, x2, . . . , x tq.
Step1: Since x0is the prompt, its distribution is independent to p, q. Then for t“1, applying
Theorem 1 of [ 10] (with K“ 8 ) directly with pandqto be conditional distributions p1p¨|x0q
andq1p¨|x0qgivesPA
1px1|x0q “q1px1|x0q, which further implies PA
1px1q “q1px1q(since the
distribution of x0is independent of p, q).
Step2: assume PA
tpx1, x2, . . . , x tq “ qtpx1, x2, . . . , x tq, we first show PApxt`1|x1:tq “
qpxt`1|x1:tq. Indeed, let ˜xt`1„pp¨|x1:tq, then by law of total probability
PApxt`1|x1:tq“PAp˜xt`1“xt`1|x1:tqPp˜xt`1acc|˜xt`1“xt`1, x1:tq
`PAp˜xt`1rej|x1:tqPApxt`1|˜xt`1rej, x1:tq(5)
By Algorithm 1,
PAp˜xt`1“xt`1|x1:tqPAp˜xt`1acc|˜xt`1“xt`1, x1:tq
“ppxt`1|x1:tqminˆ
1,qpxt`1|x1:tq
ppxt`1|x1:tq˙
“mintppxt`1|x1:tq, qpxt`1|x1:tqu.(6)
Next, the probability of rejection is:
Pp˜xt`1rej|x1:tq“1´Pp˜xt`1acc|x1:tq“1´ÿ
x1Pp˜xt`1“x1,˜xt`1acc|x1:tq
“1´ÿ
x1mintppx1|x1:tq, qpx1|x1:tqu“ÿ
x1maxt0, qpx1|x1:tq´ppx1|x1:tqu(7)
where the second equal sign comes from (6). Lastly, by the construction of the algorithm,
PApxt`1|˜xt`1rej, x1:tq“maxt0, qpxt`1|x1:tq´ppxt`1|x1:tquř
x1maxt0, qpx1|x1:tq´ppx1|x1:tqu. (8)
Combining (8) and (7) yields
PAp˜xt`1rej|x1:tqPApxt`1|˜xt`1rej, x1:tq“maxt0, qpxt`1|x1:tq´ppxt`1|x1:tqu.
Plugging (6) and the above equation into (5) to obtain
PApxt`1|x1:tq“mintppxt`1|x1:tq, qpxt`1|x1:tqu`maxt0, qpxt`1|x1:tq´ppxt`1|x1:tqu“qpxt`1|x1:tq.
Finally, applying the above we obtain
PA
t`1px1:t`1q“PA
tpx1:tq¨PApxt`1|x1:tq“PA
tpx1:tq¨qpxt`1|x1:tq“qt`1px1:t`1q,
where the last equal sign uses the induction hypothesis.
15C Lower Bound
Theorem 8 (Restatement of Theorem 2) .Define the arbitrary instance P:“pp, qq, and define the
family of algorithms as
F:“tA:Ais a specification of Algorithm 2 that satisfies PA
t“qt@tpi.e., distribution unbiased qu.
For an algorithm A, denote Nrejas the number of rejections. Then we have
inf
APFEA
PrNrejsěTÿ
n“1Ex1:n´1„qrTVppn, qnqp¨|x1:n´1qs:“CpPq.
Remark 4. Theorem 8 shows the rejections of Algorithm 1 is tight at the instance level (over the
family of algorithms F). Therefore, it attains the instance-optimality over sequential decoding
algorithms family F.
Proof. For any algorithm APF, itsbncan be written as a function of the sequence x1:n´1,˜xnwith
0ďbnp˜xn, x1:n´1qď1.9Based on this, we define the new function ϵn:VˆVn´1ÞÑRaccording
to the following equation:
bnp˜xn, x1:n´1q“min"
1,qnp˜xn|x1:n´1q`ϵnp˜xn, x1:n´1q
pnp˜xn|x1:n´1q*
. (9)
Indeed, we can choose ϵn:“bn¨pn´qn, and the validity of the definition is guaranteed by the
Lemma 1. Recall APFis a distribution unbiased algorithm w.r.t. q. Letx1, . . . , x nbe the validated
sequence of A, then we have
PA
npxn|x1:n´1q“PA
npx1:nq
PAnpx1:n´1q“qnpx1:nq
qnpx1:n´1q“qnpxn|x1:n´1q. (10)
On the other hand, let ˜xn„pnp¨|x1:n´1q, the decomposition holds
PA
npxn|x1:n´1q“PA
npxn,˜xnaccept|x1:n´1q`PA
npxn,˜xnreject|x1:n´1q
“PA
np˜xnaccept|˜xn“xn, x1:n´1qPA
np˜xn“xn|x1:n´1q
`PA
npxn|˜xnreject , x1:n´1qPA
np˜xnreject|x1:n´1q
“bnpxn, x1:n´1q¨pnpxn|x1:n´1q`PA
npxn|˜xnreject , x1:n´1q¨PA
np˜xnreject|x1:n´1q
“bnpxn, x1:n´1q¨pnpxn|x1:n´1q`PA
npxn|˜xnreject , x1:n´1q¨p1´ÿ
xbnpx, x1:n´1qpnpx|x1:n´1qq,
(11)
where the third equal sign uses PA
np˜xn“xn|x1:n´1q“pnpxn|x1:n´1qand the last equal sign is due
to
PA
np˜xnreject|x1:n´1q“1´PA
np˜xnaccept|x1:n´1q“1´ÿ
xPA
np˜xnaccept ,˜xn“x|x1:n´1q
“1´ÿ
xPA
np˜xnaccept|˜xn“x, x1:n´1qPA
np˜xn“x|x1:n´1q“1´ÿ
xbnpx, x1:n´1qpnpx|x1:n´1q.
Let’s do some further simplifications. First off,
bnpxn, x1:n´1q¨pnpxn|x1:n´1q“pnpxn|x1:n´1qmin"
1,qnpxn|x1:n´1q`ϵnpxn, x1:n´1q
pnpxn|x1:n´1q*
“mintpnpxn|x1:n´1q, qnpxn|x1:n´1q`ϵnpxn, x1:n´1qu,
(12)
and
1´ÿ
xbnpx, x1:n´1qpnpx|x1:n´1q“1´ÿ
xmintpnpx|x1:n´1q, qnpxn|x1:n´1q`ϵnpx, x1:n´1qu
“ÿ
xmaxt0, pnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx, x1:n´1qu
(13)
9For Speculative Decoding, its bnp˜xn, x1:n´1q“min!
1,qnp˜xn|x1:n´1q
pnp˜xn|x1:n´1q)
.
16Plug (12) and (13) into (11), and then plug (11) into (10) to obtain
qnpxn|x1:n´1q“mintpnpxn|x1:n´1q, qnpxn|x1:n´1q`ϵnpxn, x1:n´1qu
`PA
npxn|˜xnreject , x1:n´1q¨ÿ
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx, x1:n´1qq`.(14)
Now, we define the token set V`“tx:Dn, x1:n´1s.t. ϵ npx, x1:n´1qą0uand similarly the token
setV´“tx:Dn, x1:n´1s.t. ϵ npx, x1:n´1qď0u. Next, we show V`“H , andV´“V.
Case1. Ifpnpxn|x1:n´1qěqnpxn|x1:n´1q`ϵnpxn, x1:n´1q, then by (14) we have
qnpxn|x1:n´1q“qnpxn|x1:n´1q`ϵnpxn, x1:n´1q
`PA
npxn|˜xnreject ,˜xn“xn, x1:n´1q¨ÿ
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx, x1:n´1qq`
ěqnpxn|x1:n´1q`ϵnpxn, x1:n´1q,
and this implies 0ěϵnpxn, x1:n´1q, which means xnPV´;
Case2. Ifpnpxn|x1:n´1qăqnpxn|x1:n´1q`ϵnpxn, x1:n´1q, then by (14) we have
qnpxn|x1:n´1q“pnpxn|x1:n´1q
`PA
npxn|˜xnreject ,˜xn“xn, x1:n´1q¨ÿ
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx, x1:n´1qq`
ěpnpxn|x1:n´1q,
and this implies
ϵnpxn, x1:n´1q“bnpxn, x1:n´1qpnpxn|x1:n´1q´qnpxn|x1:n´1q
ďpnpxn|x1:n´1q´qnpxn|x1:n´1qď0,
which means xnPV´.
Therefore, combining the two cases we always have xPV´, which indicates V`“H . By (9), this
implies for all n,bnďmint1,qn
pnu. Finally, by Lemma 2, this implies EA
PrNrejsěCpPq. Since
APFis arbitrary, this concludes the proof.
Corollary 1. For any algorithm APF, it follows@nPrTs, xPVandx1:n´1,bnpx, x1:n´1qď
min!
1,qnpx|x1:n´1q
pnpx|x1:n´1q)
. In this case, the distribution Pnis defined as:
Pnpx|x1:n´1q“qnpx|x1:n´1q´mintpnpx|x1:n´1q, qnpx|x1:n´1q`ϵnpx, x1:n´1quř
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx|x1:n´1qq`.
Applying ϵn“bnpn´qn, this is equivalent to
Pnpx|x1:n´1q“qnpx|x1:n´1q´bnpx, x1:n´1qpnpx|x1:n´1qř
xp1´bnpx, x1:n´1qqpnpx|x1:n´1q.
Proof of Corollary 1. We reutilize (14) here and call it (15).
qnpxn|x1:n´1q“mintpnpxn|x1:n´1q, qnpxn|x1:n´1q`ϵnpxn, x1:n´1qu
`PA
npxn|˜xnreject , x1:n´1q¨ÿ
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx, x1:n´1qq`.(15)
By two cases discussion as in the proof of Theorem 8, we have @APF, it follows@nPrTs, xPV
andx1:n´1,bnpx, x1:n´1qďmin!
1,qnpx|x1:n´1q
pnpx|x1:n´1q)
. Then we can directly solve (15) to obtain
PA
npx|˜xnreject , x1:n´1q“qnpx|x1:n´1q´mintpnpx|x1:n´1q, qnpx|x1:n´1q`ϵnpx, x1:n´1quř
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx|x1:n´1qq`.
Lastly, we verify such a PA
npxn|˜xnreject , x1:n´1qis a valid distribution. First of all, since ϵn“
bn¨pn´qn, then bnpx, x1:n´1qďmin!
1,qnpx|x1:n´1q
pnpx|x1:n´1q)
implies ϵnpx, x1:n´1qď0, and this further
implies
qnpx|x1:n´1q´mintpnpx|x1:n´1q, qnpx|x1:n´1q`ϵnpx, x1:n´1qu
ěqnpx|x1:n´1q´mintpnpx|x1:n´1q, qnpx|x1:n´1quě 0
17which implies PA
npxn|˜xnreject , x1:n´1qě0. Second,
ÿ
xPA
npx|˜xnreject , x1:n´1q“ÿ
xqnpx|x1:n´1q´mintpnpx|x1:n´1q, qnpx|x1:n´1q`ϵnpx, x1:n´1quř
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx|x1:n´1qq`
“1´ř
xmintpnpx|x1:n´1q, qnpx|x1:n´1q`ϵnpx, x1:n´1quř
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx|x1:n´1qq`
“1`ř
xmaxt´pnpx|x1:n´1q,´qnpx|x1:n´1q´ϵnpx, x1:n´1quř
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx|x1:n´1qq`
“ř
xpnpx|x1:n´1q`ř
xmaxt´pnpx|x1:n´1q,´qnpx|x1:n´1q´ϵnpx, x1:n´1quř
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx|x1:n´1qq`
“ř
xmaxt0, pnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx, x1:n´1quř
xppnpx|x1:n´1q´qnpx|x1:n´1q´ϵnpx|x1:n´1qq`“1.
This concludes the proof.
Lemma 1. For any 0ďbnp˜xn, x1:n´1qď1, there exists ϵnp˜xn, x1:n´1qPRsuch that (9)holds
true.
Proof of Lemma 1. Indeed, set ϵn“bn¨pn´qn, then
min"
1,qnp˜xn|x1:n´1q`ϵnp˜xn, x1:n´1q
pnp˜xn|x1:n´1q*
“mint1, bnp˜xn, x1:n´1qu“bnp˜xn, x1:n´1q
where the first equal sign uses that ˜xnis sampled from pnp¨|x1:n´1qsopnp˜xn|x1:n´1qą0, and the
second equal sign uses 0ďbnď1.
Lemma 2. For any instance P“pp, qq, letF:“tA:Ais a realization of Algorithm 2 s.t. PA
t“
qt@tpi.e., unbiasedqu. Suppose there is an APFsuch that EA
PrNrejsăCpPq, then there exists a
bnin Line 8 of Template 2 such that Dx, x1:n´1
bnpx|x1:n´1qąmin"
1,qnpx|x1:n´1q
pnpx|x1:n´1q*
.
Proof of Lemma 2. Suppose for all n, x, x 1:n´1,bnpx|x1:n´1qďmin!
1,qnpx|x1:n´1q
pnpx|x1:n´1q)
. We define
random variables RnPt0,1uindicating whether the n-th token is rejected ( 1is rejected). Therefore,
the expected number of rejection is
EA«Tÿ
n“1Rnff
“Tÿ
n“1EArRns“Tÿ
n“1Ex1:n´1„PA“
EArRn|x1:n´1s‰
“Tÿ
n“1Ex1:n´1„q“
EArRn|x1:n´1s‰
.
Here the second to last equality comes from the tower property and the last equal signs is due to Ais
unbiased. Next, denote ˜xn„pnp¨|x1:n´1qto be the candidate token, we have
EArRn|x1:n´1s“PArRn“1|x1:n´1s“ÿ
˜xnPArRn“1,˜xn|x1:n´1s
“ÿ
˜xnPArRn“1|˜xn, x1:n´1sPAr˜xn|x1:n´1s“ÿ
˜xnPArRn“1|˜xn, x1:n´1spnr˜xn|x1:n´1s
“ÿ
˜xnp1´bnq¨pnr˜xn|x1:n´1sěÿ
˜xnp1´min"
1,qnp˜xn|x1:n´1q
pnp˜xn|x1:n´1q*
q¨pnr˜xn|x1:n´1s
“ÿ
˜xnrpnp˜xn|x1:n´1q´qnp˜xn|x1:n´1qs`“TVrppnp¨|x1:n´1q´qnp¨|x1:n´1qs
and this implies
EA
PrNrejs“EA«Tÿ
n“1rnff
ěTÿ
n“1Ex1:n´1„qrTVrppnp¨|x1:n´1q´qnp¨|x1:n´1qss“ CpPq
18contradicts EA
PrNrejsăCpPq! This concludes the proof.
19Algorithm 4 Batch Speculative Sampling
1:Init: Horizon T, Distributions qtandpt, with q“PLLM,p“PDraft. Lookahead K“8 .
2:while năTdo
3: Reset qt“PLLM
t@t.n0“n.
4: form“1 :M˛Sample Mdraft responses in parallel ˛do
5: fort“n:Tdo
6: Sample ˜xm
t„ptp¨|x1:n´1,˜xm
n:t´1q.
7: end for
8: end for
9: Obtain logits qnp¨|x1:n´1q, . . . , q Tp¨|x1:n´1,˜xm
n:Tq,@mPrMsin parallel for ˜xm
n:T.
10: ———˛Verification Begins ———
11: SetSample“False .
12: form“1, . . . , M do
13: fort“n:Tdo
14: Sample r„Uniformr0,1s.
15: ifrďmin!
1,qtp˜xm
t|x1:n´1,˜xm
n:t´1q
ptp˜xm
t|x1:n´1,˜xm
n:t´1q)
then
16: Accept with xn“˜xm
t.nÐn`1.Sample“True .
17: else
18: ift“n0then
19: Update qnÐrqnp¨|x1:n´1q´pnp¨|x1:n´1qs`. Break. //Here n0equals n.
20: else
21: ˛Rejection Sample xn„rqnp¨|x1:n´1q´pnp¨|x1:n´1qs`.nÐn`1. Break.
22: end if
23: end if
24: end for
25: ifSample“TRUE then
26: Break.
27: else
28:˛Rejection Sample xn„rqnp¨|x1:n´1q´pnp¨|x1:n´1qs`.nÐn`1. Break.
29: end if
30: end for
31:end while
20D Batch Speculative Decoding
We split the proofs for unbiasedness and rejections in two parts.
Theorem 9 (Unbiasedness of Theorem 3) .Denote the Algorithm 4 as AB. For any sequence x1:T
(where xiPV), we have
PAB
Tpx1, . . . , x Tq“PLLM
Tpx1, . . . , x Tq.
Proof. Step1: Letx1:n´1be the accepted tokens up to n´1. We first show PABnpxn|x1:n´1q“
PLLM
npxn|x1:n´1q@xnPV.
We partition the generation of xninto two cases: (i). accepted as the first token of the m-th responses
(m“1, . . . , M ), or rejected by all Mresponses and sampled by Line 28 of Algorithm 4; (ii).
accepted/rejected as the t-th token of the m-th responses ( m“1, . . . , M ) fortě2.
For the second case. Similar to the standard speculative decoding, let ˜xm
n„pnp¨|x1:n´1q, then
PABnpxn|x1:n´1q“PABnpxn,˜xm
nacc|x1:n´1q`PABnpxn,˜xm
nrej|x1:n´1q
“PABnp˜xm
n“xn|x1:n´1qPABnp˜xm
nacc|˜xm
n“xn, x1:n´1q
`PABnp˜xm
nrej|x1:n´1qPABnpxn|˜xm
nrej, x1:n´1q
“pnpxn|x1:n´1qmint1,qnpxn|x1:n´1q
pnpxn|x1:n´1qu
`ÿ
x1maxt0, qnpx1|x1:n´1q´pnpx1|x1:n´1qumaxt0, qnpxn|x1:n´1q´pnpxn|x1:n´1quř
x1maxt0, qnpx1|x1:n´1q´pnpx1|x1:n´1qu“qnpxn|x1:n´1q.
By the construction of Algorithm 4 (Line 3), when xnis accepted/rejected as the tpě2q-th draft token
of certain response, we have qnpxn|x1:n´1q“PLLM
npxn|x1:n´1q. This gives PABnpxn|x1:n´1q“
PLLM
npxn|x1:n´1q.
For the first case. This part of the proof largely follows Theorem 4.2 of [ 26]. In this case, the n-th
generated token xnhasM`1possibilities: accepted at the m-th response or rejected by all M
responses and sample at Line 28. Since the algorithm will iterate all Mresponses if not accepted, we
denote qm
nas the qnfor the m-th response. Then we have the recursion
qm`1
n“maxt0, qm
n´pnu
rm,
where q1
n“PLLM
nandrmis the rejection probability satisfies
rm“1´PABnp˜xm
nacc|x1:n´1q“1´ÿ
xPABnp˜xm
n“x|x1:n´1qPABnp˜xm
nacc|˜xm
n“x, x1:n´1q
“1´ÿ
xpnpx|x1:n´1qmint1,qm
npx|x1:n´1q
pnpx|x1:n´1qu“ÿ
xmaxt0, qm
npx|x1:n´1q´pnpx|x1:n´1qu.
Denote Em“tm-th response rejected uandE1:m“t1 :m-th responses all rejected u, then
PABnpxn|x1:n´1q“PABnpxn, Ec
1|x1:n´1q`PABnpxn, E1|x1:n´1q
“mintpnpxn|x1:n´1q, q1
n|pxn|x1:n´1qu`PABnpxn, E1|x1:n´1q
“mintpnpxn|x1:n´1q, q1
npxn|x1:n´1qu`r1¨PABnpxn|E1, x1:n´1q
Denote PABnpxn|x1:n´1q:“PABnpxn|E0, x1:n´1q, by similar calculation we have in general
PABnpxn|E0:m´1, x1:n´1q“mintpnpxn|x1:n´1q, qm
n|pxn|x1:n´1qu`rm¨PABnpxn|E0:m, x1:n´1q.
Next we prove PABnp¨|E0:m, x1:n´1q “ qm`1
np¨|x1:n´1q @mP t0,1, . . . , Muby back-
ward induction. First of all, PABnp¨|E0:M, x1:n´1q ““
qM
n`1p¨|x1:n´1q´pn`1p¨|x1:n´1q‰
`“
maxt0,qM
n´pnup¨|x1:n´1q
rM“qM`1
n . Suppose PABnp¨|E0:m`1, x1:n´1q“qm`2
np¨|x1:n´1q, then
PABnp¨|E0:m, x1:n´1q“PABnp¨, Ec
m`1|E0:m, x1:n´1q`PABnp¨, Em`1|E0:m, x1:n´1q
“mintpnp¨|x1:n´1q, qm`1
np¨|x1:n´1qu`PABnp¨, Em`1|E0:m, x1:n´1q
“mintpnp¨|x1:n´1q, qm`1
np¨|x1:n´1qu`rm`1¨PABnp¨|E0:m`1, x1:n´1q
“mintpnp¨|x1:n´1q, qm`1
np¨|x1:n´1qu`maxt0, qm`1
n´pnu”qm`1
n.
21In particular, we take m“0to obtain
PABnp¨|x1:n´1q“q1
np¨|x1:n´1q“PLLM
np¨|x1:n´1q.
Combining both cases we finish the proof of Step1.
Step2: For any n, first of all we have PABpx0q “PLLMpx0qsince x0is the prompt. Suppose
PAB
n´1px1:n´1q“PLLM
n´1px1:n´1q,@x1:n´1, then by Step1
PABnpx1:nq“PABnpxn|x1:n´1qPAB
n´1px1:n´1q“PABnpxn|x1:n´1qPLLM
n´1px1:n´1q“PLLM
npx1:nq
where the second equal sign is by induction and the third equal sign is by Step1. This finish the proof.
D.1 Expected Rejections for Batch Speculative Decoding (Proof for the second part of
Theorem 3)
In this section, we derive the number of expected rejections using the batch speculative decoding.
However, the analysis is more involved than the Algorithm 4 due to the parallel response structure,
since, given the verified token x1:n´1, the probability of n-th token being rejected does not possess a
unified expression and depends on the location of xn´1. We detail this below.
We recall the notion in Theorem 1 that random variables RnPt0,1uindicates whether the n-th token
is rejected ( 1is rejected). Therefore, the expected number of rejection is
E«Tÿ
n“1Rnff
“Tÿ
n“1ErRns“Tÿ
n“1ErErRn|x1:n´1ss, (16)
where the last equality comes from the tower property of expectation and we assume x0is a given
initial token and x1:0“tx0u. Then
ErRn|x1:n´1s“PApn-th token rej|x1:n´1q“1´PApn-th token acc|x1:n´1q.
In this scenario, we cannot obtain PApn-th token rej|x1:n´1q“TVppnp¨|x1:n´1q,PLLM
np¨|x1:n´1qq
since the conditional rejection probability implicitly encodes the location of pn´1q-th token: whether
˜xn´1„pp¨|x1:n´1qis rejected (at the root of the tree) or ˜xn´1„pp¨|x1:n´1qis accepted (at the
branch of the tree). To formalize this, given validated token x1:n´1, we denote qm
np¨|x1:n´1qto be
them-th rejection distribution, then by the construction of Algorithm 4 (Line 19),
qm`1
n“maxt0, qm
n´pnu
rm,@mPrMs.
Hererm“ř
x1maxt0, qm
npx1|x1:n´1q´pnpx1|x1:n´1quis normalizing factor and q1
n“PLLM. Let
˜xn„pp¨|x1:n´1q, then PApn-th token acc|x1:n´1q“PAp˜xnacc|x1:n´1q. Next, we compute the
quantity PAp˜xnacc|x1:n´1q.
We begin by first considering PAp˜xnacc,˜xn“xn|x1:n´1q. Let ˜xn´1„pp¨|x1:n´2q, then there are
two cases: ˜xn´1accepted or rejected. We have
PAp˜xnacc,˜xn“xn|x1:n´1q“PAp˜xnacc,˜xn“xn,˜xn´1acc|x1:n´1q`PAp˜xnacc,˜xn“xn,˜xn´1rej|x1:n´1q
“PAp˜xnacc,˜xn“xn|˜xn´1acc, x1:n´1qlooooooooooooooooooooooomooooooooooooooooooooooon
paPAp˜xn´1acc|x1:n´1q
`PAp˜xnacc,˜xn“xn|˜xn´1rej, x1:n´1qlooooooooooooooooooooooomooooooooooooooooooooooon
pbPAp˜xn´1rej|x1:n´1q
(17)
Forpa.Given that ˜xn´1is accepted, ˜xn“xncan only be accepted as the t-th token within the
certain response for tě2. This is because ˜xn´1is accepted and has to be at least the first token in
the response. In this case, qnp¨|x1:n´1q“PLLM
np¨|x1:n´1q“q1
np¨|x1:n´1q, then
pa“PAp˜xnacc|˜xn“xn,˜xn´1acc, x1:n´1qPAp˜xn“xn|˜xn´1acc, x1:n´1q
“mint1,qnpxn|x1:n´1q
pnpxn|x1:n´1qu¨pnpxn|x1:n´1q“mintpnpxn|x1:n´1q, qnpxn|x1:n´1qu
“mintpnpxn|x1:n´1q, q1
npxn|x1:n´1qu.(18)
22Forpb.Given that ˜xn´1is rejected, ˜xn“xncan only be accepted as the first token within the
certain response. This is because ˜xn´1is rejected will restart the parallel tree. Then
pb“PApxn|˜xn´1rej, x1:n´1q´PAp˜xnrej, xn|˜xn´1rej, x1:n´1q
“PLLM
npxn|x1:n´1q´PAp˜xnrej, xn|˜xn´1rej, x1:n´1q
“PLLM
npxn|x1:n´1q´˜Mź
m“1rm¸
qM`1
npxn|x1:n´1q,(19)
where the first equal sign comes from: since ˜xn´1rejimplies xnrepresents the first token of the
parallel tree, then it is identical to the proof of the first case of Step1 in Theorem 9. The second
equal sign is from: ˜xnis rejected means all Mresponses since xnis the first token of the tree. The
conditional rejection probability
Pp˜xm
nrej|˜x1:m´1
n rej, x1:n´1q“1´Pp˜xm
nacc|˜x1:m´1
n rej, x1:n´1q
“1´ÿ
xPABnp˜xm
n“x|˜x1:m´1
n rej, x1:n´1qPABnp˜xm
nacc|˜x1:m´1
n rej,˜xm
n“x, x1:n´1q
“1´ÿ
xpnpx|x1:n´1qmint1,qm
npx|x1:n´1q
pnpx|x1:n´1qu“ÿ
xmaxt0, qm
npx|x1:n´1q´pnpx|x1:n´1qu“rm,
so by chain rule, the total rejection probability isśM
m“1rm.
Plug (18), (19) into (17) to obtain
PAp˜xnacc, xn|x1:n´1q“mintpnpxn|x1:n´1q, q1
npxn|x1:n´1quPAp˜xn´1acc|x1:n´1q
`rq1
npxn|x1:n´1q´pMź
m“1rmq¨qM`1
npxn|x1:n´1qsPAp˜xn´1rej|x1:n´1q
(20)
which is equivalent to
q1
npxn|x1:n´1q´PAp˜xnrej, xn|x1:n´1q“mintpnpxn|x1:n´1q, q1
npxn|x1:n´1qur1´PAp˜xn´1rej|x1:n´1qs
`rq1
npxn|x1:n´1q´pMź
m“1rmq¨qM`1
npxn|x1:n´1qsPAp˜xn´1rej|x1:n´1q
ôPAp˜xnrej, xn|x1:n´1q“maxt0, q1
npxn|x1:n´1q´pn|pxn|x1:n´1qqu
´˜
maxt0, q1
npxn|x1:n´1q´pn|pxn|x1:n´1qqu´pMź
m“1rmqqM`1
npxn|x1:n´1q¸
PAp˜xn´1rej|x1:n´1q
ôPAp˜xnrej, xn|x1:n´1qPApx1:n´1q“maxt0, q1
npxn|x1:n´1q´pnpxn|x1:n´1qquPApx1:n´1q
´˜
maxt0, q1
npxn|x1:n´1q´pnpxn|x1:n´1qqu´pMź
m“1rmqqM`1
npxn|x1:n´1q¸
PAp˜xn´1rej, xn´1|x1:n´2qPApx1:n´2q
ôPAp˜xnrej, xn|x1:n´1qq1
npx1:n´1q“maxt0, q1
npxn|x1:n´1q´pnpxn|x1:n´1qquq1
npx1:n´1q
´˜
maxt0, q1
npxn|x1:n´1q´pnpxn|x1:n´1qqu´pMź
m“1rmqqM`1
npxn|x1:n´1q¸
PAp˜xn´1rej, xn´1|x1:n´2qq1
npx1:n´2q,
(21)
where the first line uses PApxn|x1:n´1q “q1
npxn|x1:n´1q, the second equivalence uses Bayes
rule, the third equivalence uses PApx1:nq“q1px1:nq“q1
npx1:nqagain. Now denote fpx1:nq:“
PAp˜xnrej, xn|x1:n´1qq1
npx1:n´1q, and sum over x1:nfor the above to obtain
ôEx1:n´1„q1rPAp˜xnrej|x1:n´1qs“Ex1:n´1„q1rTVpq1p¨|x1:n´1, pp¨|x1:n´1qqqs
´¯Ex1:n´1„f«
TVpq1, pqpx1:n´1q´rMź
m“1TVpqm, pqpx1:n´1qsff
,
23where by (21) pseudo-measure fsatisfies@x1:n
fpx1:nq“maxt0, q1
npxn|x1:n´1q´pnpxn|x1:n´1qquq1
npx1:n´1q
´˜
maxt0, q1
npxn|x1:n´1q´pnpxn|x1:n´1qqu´pMź
m“1rmqqM`1
npxn|x1:n´1q¸
fpx1:n´1q.(22)
Here we used rm“ř
xmaxt0, qm
npx|x1:n´1q´pnpx|x1:n´1qu“ TVpqm, pqpx1:n´1q.
Plug the above back to (16), we finally have
EABrTÿ
n“1Rns
“Tÿ
n“1Ex1:n´1„q1rTVpq1p¨|x1:n´1, pp¨|x1:n´1qqqs
´Tÿ
n“1¯Ex1:n´1„f«
TVpq1, pqpx1:n´1q´rMź
m“1TVpqm, pqpx1:n´1qsff
.
The formulation for fis iteratively obtained in (22).
D.2 Increasing batch size to inf doesn’t help
Proposition 1. LetfMbe the fin Theorem 3 with batch M, and let f8“limMÑ8fM. Then we
have:
•fMp¨qď q1p¨q,@MPN;f8p¨qď q1p¨q.
•f8px1:nq“hpxn|xănqrq1px1:n´1q´f8px1:n´1qs.
•limMÑ8ErNrejs“řT
n“1pEq1rTVrq1, pss´¯Ef8rTVpq1, pqsq.
•MÑ8 ,ErNrejsÛ0. This indicates increasing batch size to 8doesn’t help.
Proof. First item: Recall in the proof of Theorem 3, fis defined as
fpx1:nq:“PAp˜xnrej, xn|x1:n´1qq1
npx1:n´1q
“PAp˜xnrej, xn|x1:n´1qPApx1:n´1q
“PAp˜xnrej, x1:nqďPApx1:nq“q1px1:nq,
where the second equality is due to Batch algorithm is unbiased (Theorem 3).
Second item: by
fpx1:nq“hpxn|xănqq1
npx1:n´1q´rhpxn|xănq´pMź
m“1TVpqm, pqpxănqqqM`1pxn|xănqsfpx1:n´1q,
sinceśM
m“1TVpqm, pqpxănqÑ0asMÑ0(note TVpqm, pqpxănq“1iff there is no overlap
between qmandp), it implies f8px1:nq“hpxn|xănqrq1px1:n´1q´f8px1:n´1qs.
Third item: Similar to the second item, it holds true via taking MÑ8 . For proving ErN8
rejsą0,
suppose ErN8
rejs“0. Then it holds q1”f8. By the second item, this further implies q1”f8“0,
which is impossible.
Fourth item: We prove by contradiction. Suppose ErNrejsÛ0, then by the first item and the third
item this implies q1”f8. Plug this back to the second item, this further implies f8”0, so
q1”f8”0is a contradiction ( q1is a probability distribution)!
24E Proofs of Section 4
E.1 Proof of Theorem 4
For the sampling scheme where the acceptance probability bngoes beyond mint1,qnpx|x1:n´1q
pnpx|x1:n´1qu, there
is a quality degradation for the output sequence as the algorithm is leaning towards the draft model
(smaller model). In this case, the objective is to minimize the quality degradation via considering the
TVdistance
min
PnTVrPA
np¨|x1:n´1q, qnp¨|x1:n´1qs,@x1:n´1PVn´1.
Under the above, via equation (14) the objective is equivalent to the following (note that according to
Algorithm 2 PA
npxn|draft token rejected , x1:n´1q“Pnpxn|x1:n´1qis an algorithmic choice)
min
Pn1
2ÿ
xˇˇˇˇˇqnpxq´mintpnpxq, qnpxq`ϵnpxqu´Pnpxqÿ
˜xrpnp˜xq´qnp˜xq´ϵnp˜xqs`ˇˇˇˇˇ
s.t.ÿ
xPnpxq“1,Pnpxqě0,@xPV.(23)
where we removed x1:n´1for notation simplicity, and recall again ϵn:“bnpn´qn. Whenř
˜xrpnp˜xq´qnp˜xq´ϵnp˜xqs`“0, the objective degenerates to the constant (in Pn)TVpqn, pnq.
Therefore, for the rest of the section, we focus on the case whereř
˜xrpnp˜xq´qnp˜xq´ϵnp˜xqs`ą0.
We have the following Theorem that characterizes the solution of (23).
Theorem 10 (Restatement of Theorem 4) .Supposeř
xrpn´qn´ϵns`pxqą0. Define
Anpxq:“qnpxq´mintpnpxq, qnpxq`ϵnpxquř
˜xrpnp˜xq´qnp˜xq´ϵnp˜xqs`,
and define the positive token set A`“txPV:Anpxqě0uand the negative token set A´“txP
V:Anpxqă0u. Then the set of optimal distributions of objective (23) is characterized as
tP˚
n:P˚
npxq“0,@xPA´; 0ďP˚
npxqďAnpxq,@xPA`;ÿ
xP˚
npxq“1u,
and the optimal value is
Loss˚
TVpbq“1
2ÿ
x|qnpxq´mintpnpxq, qnpxq`ϵnpxqu|´1
2ÿ
xrpnpxq´qnpxq´ϵnpxqs`.
Remark 5. In the main context (Theorem 4) we define Anpxq:“qnpxq´bnpxqpnpxqř
˜xr1´bnp˜xqspnp˜xqandLoss˚
TVpbq“
1
2ř
x|qnpxq´bnpxqpnpxq|´1
2ř
xp1´bnpxqqpnpxq. This is equivalent to the above since ϵn:“
bnpn´qn.
Proof of Theorem 10. In this case, note mintpnpxq, qnpxq`ϵnpxqu`r pnpxq´qnpxq´ϵnpxqs`”
pnpxq, we have
ÿ
xqnpxq´mintpnpxq, qnpxq`ϵnpxqu“ÿ
˜xrpnp˜xq´qnp˜xq´ϵnp˜xqs`. (24)
By definition of Anthenř
xAnpxq“1, and the original objective (23) can be equivalently written
as
min
Pn1
2ÿ
x|Anpxq´Pnpxq|, s.t.ÿ
xPnpxq“1,Pnpxqě0,@xPPn. (25)
We now find the solution of objective (25) in two steps.
Step1: Let the positive token set A`“txPV:Anpxqě0uand the negative token set A´“txP
V:Anpxqă0u, then any optimal P˚
nmust satisfy P˚
npxq“0for all xPA´.
First, sinceř
xAnpxq“1, it implies A`‰H . Suppose for some optimal P˚
n, there exists ¯xPA´
such that P˚
np¯xqą0, then we show there exists ˇxPA`such that AnpˇxqąP˚
npˇxq. Suppose this is
25not the case, i.e. AnpxqďP˚
npxq@xPA`, then
1“ÿ
xAnpxq“ÿ
xPA´Anpxq`ÿ
xPA`AnpxqďAnp¯xq`ÿ
xPA`Anpxq
ăÿ
xPA`Anpxqďÿ
xPA`P˚
npxqďÿ
xPVP˚
npxq“1.
Contradiction! Hence, there exists ˇxPA`such that AnpˇxqąP˚
npˇxq.
Second, by AnpˇxqąP˚
npˇxq,´P˚
np¯xqă0, and triangular inequality, we have
|´P˚
np¯xq|`|Anpˇxq´P˚
npˇxq|ą| Anpˇxq´P˚
npˇxq´P˚
np¯xq|.
Note Anp¯xqă0, the above is equivalent to
|Anp¯xq´P˚
np¯xq|`|Anpˇxq´P˚
npˇxq|ą| Anp¯xq|`|Anpˇxq´P˚
npˇxq´P˚
np¯xq|. (26)
Now set
P1
npxq“$
&
%P˚
npxq, xRt¯x,ˇxu,
0, x“¯x,
P˚
npˇxq`P˚
np¯xq, x“ˇx,
then apply (26) we have
ÿ
x|Anpxq´P˚
npxq|“ÿ
xRt¯x,ˇxu|Anpxq´P˚
npxq|`|Anp¯xq´P˚
np¯xq|`|Anpˇxq´P˚
npˇxq|
“ÿ
xRt¯x,ˇxu|Anpxq´P1
npxq|`|Anp¯xq´P˚
np¯xq|`|Anpˇxq´P˚
npˇxq|
ąÿ
xRt¯x,ˇxu|Anpxq´P1
npxq|`|Anp¯xq|`|Anpˇxq´P˚
npˇxq´P˚
np¯xq|
“ÿ
xRt¯x,ˇxu|Anpxq´P1
npxq|`|Anp¯xq´P1
np¯xq|`|Anpˇxq´P1
npˇxq|“ÿ
x|Anpxq´P1
npxq|.
This contradicts P˚
nis the optimal solution! Therefore, for any optimal P˚
n, it holds P˚
npxq“0
@xPA´.
Step2: We characterize the optimal solutions of the objective. Indeed, by Step1, any optimal solution
P˚
nsatisfies
ÿ
x|Anpxq´P˚
npxq|“ÿ
xPA´|Anpxq´P˚
npxq|`ÿ
xPA`|Anpxq´P˚
npxq|
“ÿ
xPA´|Anpxq|`ÿ
xPA`|Anpxq´P˚
npxq|ěÿ
xPA´|Anpxq|`|ÿ
xPA`Anpxq´ÿ
xPA`P˚
npxq|
“ÿ
xPA´|Anpxq|`|ÿ
xPA`Anpxq´1|“ÿ
xPA´|Anpxq|`ÿ
xPA`Anpxq´1“∥An∥1´1.
The inequality becomes equality if and only if Anpxq´P˚
npxqě0. Finally, recall the definition of
Anwe receive the optimal value for the original objective is
1
2ÿ
x|qnpxq´mintpnpxq, qnpxq`ϵnpxqu|´1
2ÿ
xrpnpxq´qnpxq´ϵnpxqs`.
Replace ϵnbybngives the desired result.
Remark 6. The general optimization should follow
Loss˚
TVpb1:Tq:“min
PTVrPApx1:Tq, qpx1:Tqs, where A:“pb1:T,P1:Tq. (27)
Meanwhile, our current analysis only considers the single token setting. We mention that solving the
(27) is challenging as it corresponds to a high-dimensional discrete optimization with dimension T
and it might not have closed-form solutions in the general cases.
26E.2 Proof of Theorem 5
Proof. For an algorithm Awith the rejection probability bp¨q. Let ˜x„pp¨q, then the rejection
probability is computed as
Pprejectq“ÿ
˜xPpreject ,˜xq“ÿ
˜xPpreject|˜xqPp˜xq“ÿ
˜xp1´bp˜xqqpp˜xq.
Also, from Theorem 4
Loss˚
TVpbq“ÿ
x|qpxq´bpxqppxq|´ÿ
xp1´bpxqqppxq.
Next, we show
ÿ
x|qpxq´bpxqppxq|`ÿ
xp1´bpxqqppxq“ÿ
x|ppxq´qpxq|.
Indeed, since mint1,qpxq
ppxquďbpxqď1,@xPV, then bpxqppxqěmintppxq, qpxqu. Then we prove
the following stronger claim
|qpxq´bpxqppxq|`p 1´bpxqqppxq“|ppxq´qpxq|.
•Ifqpxq ěppxq, then 1“mint1,qpxq
ppxqu ďbpxq ď1implies bpxq “1, so the above is
equivalent to|qpxq´ppxq|“| ppxq´qpxq|is always true;
• Ifqpxqăppxq, then bpxqppxqěmintppxq, qpxqu“qpxq. In this case
|qpxq´bpxqppxq|`p1´bpxqqppxq“bpxqppxq´qpxq`p1´bpxqqppxq“ppxq´qpxq“|ppxq´qpxq|.
This concludes the proof.
27F Numerical Simulation Details
To validate the correctness of our theory, we provide the numeric simulations that are displayed in
Figure 4,2. We model the distribution p1:Tandq1:Tto be two non-stationary Markov Chains with 7
states/tokens. Instead of being ppxn|x1:n´1q, for Markov Chain, the one step transition is Markovian
withppxn|x1:n´1q“ppxn|xn´1q. We set the random seed to be 10. The prompt distribution p0is
set to be Uniform distribution. For Figure 2, the true value is computed via Theorem 1,3 respectively,
and solid line is computed by
Nrej:“1
NNÿ
i“1Ni
rej
and the shaded regions are error bars.
Below presents the simulation for horizon T“100. Left panel of Figure 5 is the standard speculative
decoding, the middle panel is batch speculative decoding with batch size M“2, and Right panel
shows the expected rejections with varying batch sizes Mcomputed from Theorem 1.
Figure 5: A simulation of (Batch) Speculative Decoding with horizon T“100.
28G Details for Experiment Section 4.2
Consider objective (1)
Loss˚
TVpbq:“min
PTVrPA, qs, where A:“pb,Pq. (28)
For any biased algorithm with the given acceptance probability bąmint1, q{pu, we can rewrite:
b“mint1,q`ϵ
pu,for some ϵą0.
We consider the following two decoding options for P:
•Baseline: select Pto be suboptimal to (28),i.e.simply set P:“q, which is the target
distribution;10We call this method Decoding-UNO .
•SetP:“P˚to be the optimal solution of (28), whose solution is presented in Theorem 4.
We call this method Decoding-OPT .
Measuring performance. Instead of TV distance, we measure the quality via WinRate in our experi-
ment. Concretely, for each prompt, we let Decoding-UNO andDecoding-OPT to generate responses
independently, and use score models to compare whose generation has higher quality. We specify
draft model paspythia-70m and target model qaspythia-2.8b from EleutherAI [ 7]. We apply
the score model to be RM-Mistral-7B orGPT-4 . We test 200 prompts from Alpaca-Farm-Eval
Dataset [ 13] with 500 responses/comparisons per prompt. For a given prompt, Decoding-OPT wins
if for more than 250 comparisons, score model prefer its response11over Decoding-UNO . Then the
WinRate for each method is computed as #wins{200.
Sanity Check for the experiment. To validate having smaller distance w.r.t. the large model
q(pythia-2.8b ) indicates higher performance, we preform the WinRate test for decoding via
pythia-70m only against decoding via pythia-2.8b only. Table 2 shows that there is a significant
performance gap between large model and small model, therefore validate the legitimacy of the
experiment in Table 1.
Method RM-Mistral-7B GPT-4
pythia-2.8b 64.5% 69%
pythia-70m 35.5% 31%
Table 2: WinRate for pythia-2.8b vspythia-2.8b .
Implementation detail for Table 1. Concretely, we leverage EleutherAI/pythia-2.8b and
EleutherAI/pythia-70m from HuggingFace. To perform speculative decoding, we spec-
ifyassistant model=EleutherAI/pythia-70m in the generation function for target model
EleutherAI/pythia-2.8b .
Notice that for the biased speculative decoding with bpxq“mint1,qpxq`ϵ
ppxqu,Apxqin Theorem 4 can
equivalently expressed as Apxq:“maxtqpxq´ppxq,´ϵuř
˜xmaxtqp˜xq´pp˜xq,´ϵu, and we can select P˚:“rAs`(recallr¨s`
in Section 2.1), which satisfies P˚|A´p¨q“ 0; 0ďP˚|A`p¨qď Ap¨q.
To implement Decoding-UNO , we modify the speculative sampling function in HuggingFace
transformers/generation/utils.py file as follows12(where variable eps isϵin Table 1). This
is conducted in a single A100 GPU.
10To be rigorous, we mention we didn’t choose P:“rq´ps`as the baseline since P:“rq´ps`might
fall in the optimal solution sets of P˚defined in Theorem 4.
11We mention for RM-Mistral-7B it output values, then the response with higer value wins. For GPT-4 , it
outputs preference.
12Note the HuggingFace code uses pas target model and qas the draft model, which is different from us.
29def _speculative_sampling(
candidate_input_ids,
......,
):
......
#####-----
## The following modification happens at Line 4727
## of the original file
mode_ = 1 // mode_=1 denotes Decoding-OPT, else denotes Decoding-UNO
eps_ = 0.1 // This is the epsilon in Table 1.
_eps_ = eps_ * torch.ones(p_i.shape,dtype=torch.float32,\
device=torch.device('cuda:0'))
probability_ratio = (p_i + _eps_) / q_i
#####-----
......
if last_assistant_token_is_eos and n_matches == candidate_length:
n_matches -= 1
valid_tokens = new_candidate_input_ids[:, : n_matches + 1]
else:
n_matches = min(n_matches, max_matches)
gamma = min(candidate_logits.shape[1], max_matches)
p_n_plus_1 = p[:, n_matches, :]
if n_matches < gamma:
q_n_plus_1 = q[:, n_matches, :]
## The following modification happens at Line 4760
## of the original file
if mode_ == 1:
## The following two lines compute A(x)
p_prime = torch.clamp((p_n_plus_1 - q_n_plus_1), min= -eps_)
p_prime.div_(p_prime.sum())
## The following two lines compute P* = [A]_+
p_prime = torch.clamp(p_prime, min= 0)
p_prime.div_(p_prime.sum())
else:
## Baseline Decoding-UNO
p_prime = q_n_plus_1
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We study the theoretical properties for Speculative decoding, which is identical
what is stated in abstract.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In the discussion section, we mentioned that we don’t have a lower bound the
batch SD algorithms.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ”Limitations” section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
31Answer: [Yes]
Justification: We provide the full proof in appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The code is written down in the appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
32Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code is in the appendix, the Alpaca data is also public.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We have detailed section in Appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We have error bars in Figure 2.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer ”Yes” if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
33• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: See Appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Yes, we followed that.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Our work is most theoretical.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
34•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper has no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: All the materials are open-sourced.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
35•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: NO new assets intorduced.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing involved.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
36