Meta Flow Matching:
Integrating Vector Fields on the Wasserstein Manifold
Anonymous Author(s)
Affiliation
Address
email
Abstract
Numerous biological and physical processes can be modeled as systems of interact- 1
ing samples evolving continuously over time, e.g. the dynamics of communicating 2
cells or physical particles. Flow-based models allow for learning these dynamics at 3
the population level — they model the evolution of the entire distribution of sam- 4
ples. However, current flow-based models are limited to a single initial population 5
and a set of predefined conditions which describe different dynamics. We argue that 6
multiple processes in natural sciences have to be represented as vector fields on the 7
Wasserstein manifold of probability densities. That is, the change of the population 8
at any moment in time depends on the population itself due to the interactions 9
between samples. In particular, this is crucial for personalized medicine where the 10
development of diseases and their treatments depend on the microenvironment of 11
cells specific to each patient. We propose Meta Flow Matching (MFM), a practical 12
approach to integrating along these vector fields on the Wasserstein manifold by 13
amortizing the flow model over the initial populations. Namely, we embed the 14
population of samples using a Graph Neural Network (GNN) and use these embed- 15
dings to train a Flow Matching model. This gives Meta Flow Matching the ability 16
to generalize over the initial distributions unlike previously proposed methods. 17
Finally, we demonstrate the ability of MFM to improve prediction of individual 18
treatment responses on a large scale multi-patient single-cell drug screen dataset. 19
1 Introduction 20
Understanding the dynamics of many-body problems is a central challenge across the natural sciences. 21
In the field of cell biology, a central focus is the understanding of the dynamic processes that cells 22
undergo in response to their environment, and in particular their response and interaction with other 23
cells. Cells communicate with one other in close proximity using cell signaling , exerting influence 24
over each other’s trajectories (Armingol et al., 2020; Goodenough and Paul, 2009). This signaling 25
presents an obstacle for modeling, but is essential for understanding and eventually controlling 26
cell dynamics during development (Gulati et al., 2020; Rizvi et al., 2017), in diseased states (Molè 27
et al., 2021; Binnewies et al., 2018; Zeng and Dai, 2019; Chung et al., 2017), and in response to 28
perturbations (Ji et al., 2021; Peidli et al., 2024). 29
The super-exponential decrease of sequencing costs and advances in microfluidics has enabled the 30
rapid advancement of single-cell sequencing and related technologies over the past decade. While 31
single-cell sequencing has been used to great effect to understand the heterogeneity in cell systems, 32
they are also destructive, making longitudinal measurements extremely difficult. Instead, most 33
approaches model cell dynamics at the population level (Hashimoto et al., 2016; Weinreb et al., 2018; 34
Schiebinger et al., 2019; Tong et al., 2020; Neklyudov et al., 2022; Bunne et al., 2023a). These 35
approaches involve the formalisms of optimal transport (Villani, 2009; Peyré and Cuturi, 2019) and 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.generative modeling (De Bortoli et al., 2021; Lipman et al., 2023) methods, which allow for learning 37
a map between empirical measures. While these methods are able to model the dynamics of the 38
population, they are fundamentally limited in that they model the evolution of cells as independent 39
particles evolving according to a shared dynamical system. Furthermore, these models can be trained 40
to match any given set of measures, but they are restricted to modeling of a single population and can 41
at best condition on a number of different dynamics that is available in the training data. 42
To address this we propose Meta Flow Matching (MFM) — the amortization of the Flow Matching 43
generative modeling framework (Lipman et al., 2023) over the input measures. In practice, our 44
method can be used to predict the time-evolution of distributions from a given dataset of the time- 45
evolved examples. Namely, we assume that the collected data undergoes a universal developmental 46
process, which depends only on the population itself as in the setting of the interacting particles or 47
communicating cells. Under this assumption, we learn the vector field model that takes samples from 48
the initial distribution as input and defines the push-forward map on the sample-space that maps the 49
initial distribution to the final distribution. 50
We showcase the utility of our approach on two applications. We first explore Meta Flow Matching on 51
a synthetic task of denoising letters. We show that MFM is able to generalize the denoising process 52
to letters in unseen orientations where a standard flow matching approach cannot. Next, we explore 53
how MFM can be applied to model single-cell perturbation data (Ji et al., 2021; Peidli et al., 2024). 54
We evaluate MFM on predicting the response of patient-derived cells to chemotherapy treatments 55
in a recently published large scale single-cell drug screening dataset where there are known to be 56
patient-specific responses (Ramos Zapatero et al., 2023). This dataset includes more than 25M cells 57
collected over ten patients under 2500 conditions. This is a challenging task due to the variance over 58
multiple patients, treatments applied and the local cell compositions, but it can be used to study the 59
tumor micro-environment (TME), thought to be essential in circumventing chemoresistance. We 60
demonstrate that Meta Flow Matching can successfully predict the development of cell populations 61
on replicated experiments, and, most importantly, it generalizes to previously unseen patients, thus, 62
capturing the patient-specific response to the treatment. 63
2 Background 64
2.1 Generative Modeling via Flow Matching 65
Flow Matching is an approach to generative modeling recently proposed independently in different 66
works: Rectified Flows (Liu et al., 2022), Flow Matching (Lipman et al., 2023), Stochastic Interpolants 67
(Albergo and Vanden-Eijnden, 2022). It assumes a continuous interpolation between densities p0(x0) 68
andp1(x1)in the sample space. That is, the sample from the intermediate density pt(xt)is produced 69
as follows 70
xt=ft(x0, x1),(x0, x1)∼π(x0, x1), (1)
whereZ
dx1π(x0, x1) =p0(x0),Z
dx0π(x0, x1) =p1(x1), (2)
where ftis the time-continuous interpolating function such that ft=0(x0, x1) = x0and 71
ft=1(x0, x1) =x1(e.g. linearly between x0andx1withft(x0, x1) = (1 −t)·x0+t·x1); 72
π(x0, x1)is the density of the joint distribution, which is usually taken as a distribution of inde- 73
pendent random variables π(x0, x1) =p0(x0)p1(x1), but can also be generalized to formulate the 74
optimal transport problems (Pooladian et al., 2023; Tong et al., 2024). The corresponding density can 75
be defined then as the following expectation 76
pt(x) =Z
dx0dx1π(x0, x1)δ(x−ft(x0, x1)). (3)
The essential part of Flow Matching is the continuity equation that describes the change of this 77
density through the vector field on the state space, which admits vector field v∗
t(x)as a solution 78
∂pt(x)
∂t=−⟨∇ x, pt(x)v∗
t(x)⟩, v∗
t(ξ) =1
pt(ξ)Eπ(x0,x1)
δ(ft(x0, x1)−ξ)∂ft(x0, x1)
∂t
.(4)
2Flow Matching
 Conditional Flow Matching
 Vector Field on P2(X)
Figure 1: Illustration of flow matching methods on the 2-Wasserstein manifold, P2(X), depicted as a two-
dimensional sphere. Flow Matching learns the tangent vectors to a single curve on the manifold. Conditional
generation corresponds to learning a finite set of curves on the manifold, e.g. classes c1andc2on the plot. Meta
Flow Matching learns to integrate a vector field on P2(X), i.e. for every starting density p0Meta Flow Matching
defines a push-forward measure that integrates along the underlying vector field.
Relying on this formula, one can derive the tractable objective for learning v∗
t(x), i.e. 79
LFM(ω) =Z1
0dtEpt(x)∥v∗
t(x)−vt(x;ω)∥2(5)
=Eπ(x0,x1)Z1
0dt∂
∂tft(x0, x1)−vt(ft(x0, x1);ω)2
+constant . (6)
Finally, the vector field vt(ξ, ω)≈v∗
t(ξ)defines the push-forward density that approximately matches 80
pt=1, i.e.T#p0≈pt=1, where Tis the flow corresponding to vector field vt(·, ω)with parameters ω. 81
2.2 Conditional Generative Modeling via Flow Matching 82
Conditional image generation is one of the most common applications of generative models nowadays; 83
it includes conditioning on the text prompts (Saharia et al., 2022b; Rombach et al., 2022) as well 84
as conditioning on other images (Saharia et al., 2022a). To learn the conditional generative process 85
with diffusion models, one merely has to pass the conditional variable (sampled jointly with the data 86
point) as an additional input to the parametric model of the vector field. The same applies for the 87
Flow Matching framework. 88
Conditional Generative Modeling via Flow Matching is independently introduced in several works 89
(Zheng et al., 2023; Dao et al., 2023; Isobe et al., 2024) and it operates as follows. Consider a family 90
of time-continuous densities pt(xt|c), which corresponds to the distribution of the following random 91
variable 92
xt=ft(x0, x1),(x0, x1)∼π(x0, x1|c). (7)
For every c, the density pt(xt|c)follows the continuity equation with the following vector field 93
v∗
t(ξ|c) =1
pt(ξ|c)Eπ(x0,x1)δ(ft(x0, x1)−ξ)∂ft(x0, x1)
∂t, (8)
which depends on c. Thus, the training objective of the conditional model becomes 94
LCGFM (ω) =Ep(c)Eπ(x0,x1|c)Z1
0dt∂
∂tft(x0, x1)−vt(ft(x0, x1)|c;ω)2
, (9)
where, compared to the original Flow Matching formulation, we first have to sample c, then produce 95
the samples from pt(xt|c)and pass cas input to the parametric model of the vector field. 96
3 Meta Flow Matching 97
In this paper, we propose the amortization of the Flow Matching framework over the marginal 98
distributions. Our model is based on the outstanding ability of the Flow Matching framework to 99
3learn the push-forward map for any joint distribution π(x0, x1)given empirically. For the given joint 100
π(x0, x1), we denote the solution of the Flow Matching optimization problem as follows 101
v∗
t(·, π) = argmin
vtLGFM(vt(·), π(x0, x1)). (10)
Analogously to the amortized optimization (Chen et al., 2022; Amos et al., 2023), we aim to learn the 102
model that outputs the solution of Eq. (10) based on the input data sampled from π, i.e. 103
vt(·, φ(π)) =v∗
t(·, π), (11)
where φ(π)is the embedding model of πand the joint density π(·|c)is generated using some 104
unknown measure of the conditional variables c∼p(c). 105
3.1 Modeling Process in Natural Sciences as Vector Fields on the Wasserstein Manifold 106
We argue that numerous biological and physical processes cannot be modeled via the vector field 107
propagating the population samples independently. Thus, we propose to model these processes as 108
families of conditional vector fields where we amortize the conditional variable by embedding the 109
population via a Graph Neural Network (GNN). 110
To provide the reader with the necessary intuition, we are going to use the geometric formalism 111
developed by Otto (2001). That is, time-dependent densities pt(xt)define absolutely-continuous 112
curves on the 2-Wasserstein space of distributions P2(X)(Ambrosio et al., 2008). The tangent space 113
of this manifold is defined by the gradient flows St={∇st|st:X →R}on the state space X. In 114
the Flow Matching context, we are going to refer to the tangent vectors as vector fields since one 115
can always project the vector field onto the tangent space by parameterizing it as a gradient flow 116
(Neklyudov et al., 2022). 117
Under the geometric formalism of the 2-Wasserstein manifold, Flow Matching can be considered 118
as learning the tangent vectors vt(·)along the density curve pt(xt)defined by the sampling process 119
in Eq. (2) (see the left panel in Fig. 1). Furthermore, the conditional generation processes pt(xt|c) 120
would be represented as a finite set of curves if cis discrete (e.g. class-conditional generation of 121
images) or as a family of curves if cis continuous (see the middle panel in Fig. 1). 122
Finally, one can define a vector field on the 2-Wasserstein manifold via the continuity equation with 123
the vector field vt(x, pt(x))on the state space Xthat depends on the current density pt(x)or its 124
derivatives. Below we give two examples of processes defined as vector fields on the 2-Wasserstein 125
manifold. 126
Example 1 (Mean-field limit of interacting particles) .In the limit of the infinite number of interacting 127
particles one can describe their state with the density function pt(x). Consider the interaction 128
according to the first order dynamics with the velocity k(x, y) :Rd×Rd→Rdof the particles at 129
point xthat interact with the particles at point y. Then the change of the density is described by the 130
following continuity equation 131
dx
dt=Ept(y)k(x, y),∂pt(x)
∂t=−
∇x, pt(x)Ept(y)k(x, y)
. (12)
Example 2 (Diffusion) .Even when the physical particles evolve independently in nature, the 132
deterministic vector field model might be dependent on the current density of the population. For 133
instance, for the diffusion process, the change of the density is described by the Fokker-Planck 134
equation, which results in the density-dependent vector field when written as a continuity equation, 135
i.e. 136
∂pt(x)
∂t=1
2∆xpt(x) =−
∇x, pt(x)
−1
2∇xlogpt(x)
=⇒dx
dt=−1
2∇xlogpt(x).(13)
Motivated by the examples above, we argue that using the information about the current or the initial 137
density is crucial for the modeling of time-evolution of densities in natural processes, to capture this 138
type of dependency one can model the change of the density as the following Cauchy problem 139
∂pt(x)
∂t=−⟨∇ x, pt(x)vt(x, pt)⟩, pt=0(x) =p0(x), (14)
where the state-space vector field vt(x, pt)depends on the density pt. 140
The dependency might vary across models, e.g. in Example 1 the vector field can be modeled as an 141
application of a kernel to the density function, while in Example 2 the vector field depends only on 142
the local value of the density and its derivative. 143
43.2 Integrating Vector Fields on the Wasserstein Manifold via Meta Flow Matching 144
Consider the dataset of joint populations D={(π(x0, x1|i))}i, where, to simplify the notation, 145
we associate every i-th population with its density π(·|i)and the conditioning variable here is the 146
index of this population in the dataset. We make the following assumptions regarding the ground 147
truth sampling process (i) we assume that the starting marginals p0(x0|i) =R
dx1π(x0, x1|i)are 148
sampled from some unknown distribution that can be parameterized with a large enough number of 149
parameters (ii) the endpoint marginals p1(x1|i) =R
dx0π(x0, x1|i)are obtained as push-forward 150
densities solving the Cauchy problem in Eq. (14), (iii) there exists unique solution to this Cauchy 151
problem. 152
One can learn a joint model of all the processes from the dataset Dusing the conditional version of 153
the Flow Matching algorithm (see Section 2.2) where the population index iplays the role of the 154
conditional variable. However, obviously, such a model will not generalize beyond the considered 155
dataDand unseen indices i. We illustrate this empirically in Section 5. 156
To be able to generalize to previously unseen populations, we propose learning the density-dependent 157
vector field motivated by Eq. (14). That is, we propose to use an embedding function φ:P2(X)→ 158
Rmto embed the starting marginal density p0, which we then input into the vector field model and 159
minimize the following objective over ω 160
LMFM(ω;φ) =Ei∼DEπ(x0,x1|i)Z1
0dt∂
∂tft(x0, x1)−vt(ft(x0, x1)|φ(p0);ω)2
. (15)
Note that the initial density p0is enough to predict the push-forward density p1since the Cauchy 161
problem for Eq. (14) has a unique solution. The embedding function φ(p0)can take different forms, 162
e.g. it can be the density value φ(p0) =p0(·), which is then used inside the vector field model to 163
evaluate at the current point (analogous to Example 2); a kernel density estimator (analogous to 164
Example 1); or a parametric model taking the samples from this density as an input. 165
Proposition 1. Meta Flow Matching recovers the Conditional Generation via Flow Matching 166
when the conditional dependence of the marginals p0(x0|c) =R
dx1π(x0, x1|c)andp1(x1|c) = 167R
dx0π(x0, x1|c)and the distribution p(c)are known, i.e. there exist φ:P2(X)→Rmsuch that 168
LMFM (ω) =LCGFM (ω). 169
Proof. Indeed, sampling from the dataset i∼ D becomes sampling of the conditional variable 170
c∼p(c)and the embedding function becomes φ(p0(·|c)) =c. 171
Furthermore, for the parametric family of the embedding models φ(pt, θ), we show that the parameters 172
θcan be estimated by minimizing the objective in Eq. (15) in the joint optimization with the vector 173
field parameters ω. We formalize this statement in the following theorem. 174
Theorem 1. Consider a dataset of populations D={(π(x0, x1|i))}igenerated from some unknown 175
conditional model π(x0, x1|c)p(c). Then the following objective 176
L(ω, θ) =Ep(c)Z1
0dtEpt(xt|c)∥v∗
t(xt|c)−vt(xt|φ(p0, θ), ω)∥2(16)
is equivalent to the Meta Flow Matching objective 177
LMFM(ω, θ) =Ei∼DEπ(x0,x1|i)Z1
0dt∂
∂tft(x0, x1)−vt(ft(x0, x1)|φ(p0, θ);ω)2
(17)
up to an additive constant. 178
Proof. We postpone the proof to Appendix A. 179
3.3 Learning Population Embeddings via Graph Neural Networks (GNNs) 180
In many applications, the populations D={(π(x0, x1|i))}N
i=1are given as empirical distributions, 181
i.e. they are represented as samples from some unknown density π 182
{(xj
0, xj
1)}Ni
j=1,(xj
0, xj
1)∼π(x0, x1|i), (18)
5where Niis the size of the i-th population. For instance, for the diffusion process considered in 183
Example 2, the samples from π(x0, x1|i)can be generated by generating some marginal p1(x1|i) 184
and then adding the Gaussian random variable to the samples xj
1. We use this model in our synthetic 185
experiments in Section 5.1. 186
Since the only available information about the populations is samples, we propose learning the 187
embedding of populations via a parametric model φ(p0, θ), i.e. 188
φ(p0, θ) =φ
{xj
0}Ni
j=1, θ
,(xj
0, xj
1)∼π(x0, x1|i). (19)
For this purpose, we employ GNNs, which recently have been successfully applied for simulation of 189
complicated many-body problems in physics (Sanchez-Gonzalez et al., 2020). To embed a population 190
{xj
0}Ni
j=1, we create a k-nearest neighbour graph Gibased on the metric in the state-space X, input it 191
into a GNN, which consists of several message-passing iterations (Gilmer et al., 2017) and the final 192
average-pooling across nodes to produce the embedding vector. Finally, we update the parameters of 193
the GNN jointly with the parameters of the vector field to minimize the loss function in Eq. (17). 194
4 Related Work 195
The meta-learning of probability measures was previously studied by Amos et al. (2022) where they 196
demonstrate that the prediction of the optimal transport paths can be efficiently amortized over the 197
input marginal measures. The main difference with our approach is that we are trying to learn the 198
push-forward map without embedding the second marginal. 199
Generative modeling for single cells. Single cell data has expanded to encompass multiple modalities 200
of data profiling cell state and activities (Frangieh et al., 2021; Bunne et al., 2023b). Single-cell 201
data presents multiple challenges in terms of noise, non-time resolved, and high dimension, and 202
generative models have been used to counter those problems. Autoencoder has been used to embed 203
and extrapolate data Out Of Distribution (OOD) with its latent state dimension (Lotfollahi et al., 2019; 204
Lopez et al., 2018; Hetzel et al., 2022). Orthogonal non-negative matrix factorization (oNMF) has 205
also been used for dimensionality reduction combined with mixture models for cell state prediction 206
(Chen et al., 2020). Other approaches have tried to use Flow Matching (FM) (Tong et al., 2023, 2024; 207
Neklyudov et al., 2023) or similar approaches such as the Monge gap (Uscidda and Cuturi, 2023) to 208
predict cell trajectories. Currently, the state of the art method uses the principle of Optimal Transport 209
(OT) to predict cell trajectories with Input Convex Neural Network (ICNN) (Makkuva et al., 2020; 210
Bunne et al., 2023b). What determines the significance of the method is its capability in generalizing 211
out of distribution to a new population of cells, which may be from different culture or individuals. 212
As of this time, our method is the only method that takes inter-cellular interactions into account. 213
Generative modeling for physical processes. The closest approach to ours is the prediction of the 214
many-body interactions in physics (Sanchez-Gonzalez et al., 2020) via GNNs. However, the problem 215
there is very different since these models use the information about the individual trajectories of 216
samples, which are not available for the single-cell prediction. Neklyudov et al. (2022) consider 217
learning the vector field for any continuous time-evolution of a probability measure, however, their 218
method is restricted to single curves and do not consider generalization to unseen data. Finally, the 219
weather/climate forecast models generating the next state conditioned on the previous one (Price 220
et al., 2023; Verma et al., 2024) are similar approaches to ours but operating on a much finer time 221
resolution. 222
5 Experiments 223
To show the effectiveness of MFM to generalize under previously unseen populations for the task 224
population prediction, we consider two experimental settings. (i) A synthetic experiment with well 225
defined coupled populations, and (ii) experiments on a publicly available single-cell dataset consisting 226
of populations from patient dependent treatment response trials. To quantify model performance, 227
we consider three distributional distances metrics: the 1-Wasserstein distance ( W1), 2-Wasserstein 228
(W2) distance, and the radial basis kernel maximum-mean-discrepancy (MMD) distance (Gretton 229
et al., 2012). We parameterize all vector field models vt(·|φ(p0);ω)using a Multi-Layer Perceptron 230
(MLP). For MFM, we additionally parameterize φ(pt;θ, k)using a Graph Convolutional Network 231
6source
 t=0.50
 t=1.00
 target
Train Test
FM
source
 t=0.50
 t=1.00
 target
 CGFM
source
 t=0.50
 t=1.00
 target
 MFM
Figure 2: Examples of model-generated samples for synthetic letters from the source distribution ( t= 0) to
predicted target distribution ( t= 1). See Fig. 4 in Appendix F for a larger set of examples.
Table 1: Results of the synthetic letters experiment for population prediction on seen train populations and
unseen test populations. We report the the 1-Wasserstein ( W1), 2-Wasserstein ( W2), and the maximum-mean-
discrepancy (MMD) distributional distances. We consider 4 settings for MFM with varying k.
Train Test
W1 W2 MMD ( ×10−3) W1 W2 MMD ( ×10−3)
FM 0.216±0.000 0 .280±0.000 2 .38±0.00 0 .237±0.000 0 .315±0.000 3.28±0.00
CGFM 0.093±0.000 0.112 ±0.000 0.34±0.00 0 .317±0.000 0 .397±0.000 6 .67±0.00
MFM ( k= 0)0.099±0.000 0 .128±0.000 0 .25±0.00 0 .221±0.000 0 .267±0.000 3 .77±0.00
MFM ( k= 1) 0.096±0.003 0.124±0.004 0.22±0.04 0.217±0.003 0 .261±0.003 3 .80±0.28
MFM ( k= 10 )0.096±0.003 0.124±0.003 0.23±0.04 0.213 ±0.008 0.256 ±0.008 3.68 ±0.45
MFM ( k= 50 )0.099±0.003 0 .127±0.003 0 .25±0.05 0 .226±0.005 0 .270±0.007 4 .38±0.30
(GCN) with a k-nearest neighbour graph edge pooling layer. We include details regarding model 232
hyperparameters, training/optimization, and implementation in Appendix B and Appendix B.2. The 233
results for all the models are averaged over three random seeds. 234
5.1 Synthetic Experiment 235
We curate a synthetic dataset of the joint distributions {(p0(x0,|i), p1(x1|i))}N
i=1by simulating a 236
diffusion process applied to a set of pre-defined target distributions p1(x1|i)fori= 1, . . . , N . To get 237
a paired population p0(x0|i)we simulate the forward diffusion process without drift x0∼ N(x1, σ). 238
After this setup, for reasonable values of σ, we assume that one can reverse the diffusion process and 239
learn the push-forward map from p0(x0|i)top1(x1|i)for every index i. For this task, given the i-th 240
population index we denote p0(x0|i)as the source population p1(x1|i)as the i-thtarget population. 241
To construct p1(x1|i), we discretize samples from a defined silhouette; e.g. an image of a character, 242
where iindexes the respective character. We use upper case letters as the silhouette and generate 243
the corresponding samples x1∼p1(x1|i)from the uniform distribution over the silhouette and run 244
the diffusion process for samples x1to acquire x0. We construct the training data using 10 random 245
orientations of 24 letters, while only using the upright orientation for the remaining letters “X” and 246
“Y”. We construct the test data by using 10 random orientations of “X” and “Y” (validation and test, 247
respectively) that differ from the upright orientations of the same letters in the training data. We 248
do this to simplify the generalization task – the model will see the shapes of “X” and “Y” during 249
training, but the same letters under different orientations remain unseen. 250
We train FM, CGFM and 4 variants of MFM of varying kfor the GCN population embedding model 251
φ(pt;θ, k). When k= 0,φ(pt;θ, k)becomes identical to the DeepSets model (Zaheer et al., 2017). 252
We compare MFM to Flow-Matching (FM) and Conditional Generation via Flow-Matching (CGFM). 253
FM does not have access to conditional information; hence will only learn an aggregated lens of the 254
distribution dynamics and will not be able to fit the training data, and consequently won’t generalize 255
to the test conditions. For the training data, CGFM vector field model takes in the distribution index 256
ias a one-hot input condition. On the test set, since none of these indices is present, we input the 257
normalized constant vector, which averages the learned embeddings of the indices. Because of this, 258
CGFM will fit the training data, however, will not be able to generalize to the unseen condition in 259
the test dataset. Note that the CGFM can be viewed as an idealized model for the train data since 260
7TrainTestReplica splitLearn cell-population-specific responseTreatment ConditioningPatient splitLearn patient-specific response
Figure 3: Organoid drug-screen dataset overview. Left: a given replica consists of a control distribution p0and
corresponding treatment response distribution p1for treatment condition ci.Right : train and test data splits for
replica (top) and patients (bottom) splits, restively. For each experiment there are 11 treatments, 10 patients and
3 culture conditions.
it gets perfect information regarding the population conditions. We use CGFM to assess if other 261
models are fitting the data. For MFM, we expect to both fit the training data and generalize to unseen 262
distributional conditions. 263
In Fig. 2, we observe that indeed FM fails to adequately learn to sample from p1(x1|i)in the training 264
set, and likewise fails to generalize, while CGFM is able to effectively sample from p1(x1|i)in 265
the training set, but fails to generalize. We report results for the synthetic experiment in Table 1. 266
As expected, CGFM fits the training data, however, fails to generalize beyond its set of training 267
conditions. In contrast, we see that MFM is able to both fit the training data (approaching the 268
performance of CGFM) while also generalizing to the unseen test distributions. FM fails to fit the 269
train data and fails to generalize under the test conditions. Interestingly, although MFM performs 270
better for certain values of kversus others, overall performance does not vary significantly for the 271
range considered. 272
5.2 Experiments on Organoid Drug-screen Data 273
Data. For experiments on biological data, we use the organoid drug-screen dataset from Ramos Zap- 274
atero et al. (2023). This dataset is a single-cell mass-cytometry dataset collected over 10 patients. 275
Somewhat unique to this dataset, unlike many prior perturbation-screen datasets which have a single 276
control population, this dataset has matched controls to each experimental condition. Populations from 277
each patient are treated with 11 different drug treatments of varying dose concentrations.1We use the 278
term replicate to define control-treatment population pairs, p0(x0|ci)andp1(x1|ci), respectively 279
(see Fig. 3-left). In each patient, cell population are categorized into 3 cell cultures : (i) cancer associ- 280
ated Fibroblasts, (ii) patient-derived organoid cancer cells (PDO), and (iii) patient-derived organoid 281
cancer cells co-cultured fibroblasts (PDOF). We report results averaged over Fibroblast/PDO/PDOF 282
cultures and results for the individual cultures (this is reported in Appendix F). 283
Pre-processing and data splits. We filter each cell population to contain at least 1000 cells and 284
consider 43 bio-markers. We consider two data splits for the organoid drug-screen dataset (see 285
Fig. 3-right). (1) Replicate split ; here we leave-out replicates evenly across all patients for testing. (2) 286
Patients split ; here we leave-out replicates fully in one patients – in this setting, we are testing the 287
ability of of model to generalize population prediction of treatment response for unseen patients. In 288
both settings, we normalize the data and embed it into a lower dimensional principle components 289
(PC) representation. We do this to reduce the dimensionality of the data and to extract the relevant 290
information from the 43 bio-markers (features) of the ambient space. We train and evaluate all models 291
in the PC space. For all organoid drug-screen dataset experiments we use PC=10. Further details 292
regarding data pre-processing and data splits are provided in Appendix B.2. 293
For the organoid drug-screen experiments, we consider an ICNN architecture in addition to the 294
Flow-matching models. The ICNN model is based on CellOT (Bunne et al., 2023a); a method for 295
learning cell specific response to treatments. The ICNN (and likewise CellOT) counterparts our FM 296
1We consider only the highest dosage and leave exploration of dose-dependent response to future work.
8Table 2: Experimental results on the organoid drug-screen dataset for population prediction of treatment response
across replicate populations averaged over co-culture conditions. Results are reported for models trained on data
embedded into 10 principle components. We report the the 1-Wasserstein ( W1), 2-Wasserstein ( W2), and the
maximum-mean-discrepancy (MMD) distributional distances. We consider two settings for MFM with varying
nearest-neighbours parameter. For extended results in Table 4.
Train Test
W1 W2MMD ( ×10−3) W1 W2MMD ( ×10−3)
FM 1.946±0.083 2 .178±0.092 6 .32±0.36 2 .087±0.035 2 .301±0.043 9 .29±0.77
ICNN 2.112±0.012 2 .317±0.011 190 .17±4.87 2 .200±0.011 2 .395±0.010 249 .33±4.67
CGFM 1.823±0.126 2.009 ±0.143 4.16 ±1.00 2.213±0.137 2 .416±0.154 13 .91±2.41
MFM ( k= 0)1.829±0.050 2 .012±0.058 4 .64±0.66 1 .959±0.050 2 .144±0.059 7 .35±1.20
MFM ( k= 10 )1.842±0.049 2 .020±0.057 4 .76±0.66 1.954±0.047 2.136 ±0.052 7.34 ±0.93
Table 3: Experimental results on the organoid drug-screen dataset for population prediction of treatment response
across patient populations. Results shown in this table are broken out in Table 5.
Train Test
W1 W2MMD ( ×10−3) W1 W2MMD ( ×10−3)
FM 1.995±0.138 2 .246±0.193 6 .87±2.65 2 .607±0.028 2 .947±0.050 21 .58±1.02
ICNN 2.163±0.067 2 .367±0.070 192 .67±4.22 2 .702±0.027 2 .996±0.033 452 .67±19.14
CGFM 1.773±0.072 1.954 ±0.092 3.03 ±0.69 2.675±0.019 2 .938±0.020 23 .75±0.61
MFM ( k= 0)1.863±0.056 2 .048±0.063 5 .01±0.53 2 .393±0.160 2 .685±0.122 16 .66±1.99
MFM ( k= 10 )1.881±0.071 2 .074±0.091 5 .25±0.78 2.326±0.072 2.610 ±0.073 14.30 ±2.27
model in that it does not take the population index ias a condition. Therefore, it will neither be able 297
to fit the training data, nor generalize. 298
Predicting treatment response across replicates. We show results for generalization across repli- 299
cates in Table 2. As expected, we observe that CGFM fits the training data, but does not generalize to 300
the test replicates. With this, we can observe that the FM and ICNN models fail to fit the train data, 301
relative to CGFM, and also fail to generalize. MFM ( k= 10 ) performs best on generalization to 302
unseen replicates. We include results reported for the separate cell cultures in Table 4 in Appendix F. 303
Predicting treatment response across patients. We show results for generalization across patients 304
in Table 3. Similar to the replicates data setting, we observe that CGFM fits the training data, but 305
does not generalize to the test replicates. Likewise, the FM and ICNN models fail to fit the train data, 306
relative to CGFM, and also fail to generalize. MFM ( k= 10 ) performs best on generalization to 307
unseen replicates. We include results reported for the separate cell cultures in Table 5 in Appendix F. 308
Through the biological and synthetic experiments, we have shown that MFM is able to generalize 309
to unseen distributions/populations. The implication of our results suggest that MFM can learn 310
population dynamics in unseen environments. In biological contexts, like the one we have shown 311
in this work, this result indicates that we can learn population dynamics, of treatment response or 312
any arbitrary perturbation, in new/unseen patients. This works towards a model where it is possible 313
to predict and design an individualized treatment regimen for each patient based on their individual 314
characteristics and tumor microenvironment. 315
6 Conclusion and Future Work 316
Our paper highlights the significance of modeling dynamics based on the entire distribution. While 317
flow-based models offer a promising avenue for learning dynamics at the population level, they were 318
previously restricted to a single initial population and predefined conditions. 319
In this paper, we introduce Meta Flow Matching (MFM) as a practical solution to address these 320
limitations. By integrating along vector fields of the Wasserstein manifold, MFM allows for a more 321
comprehensive model of dynamical systems with interacting particles. Crucially, MFM leverages 322
graph neural networks to embed the initial population, enabling the model to generalize over various 323
initial distributions. MFM opens up new possibilities for understanding complex phenomena that 324
emerge from interacting systems in biological and physical systems. 325
In practice, we demonstrate that MFM learns meaningful embeddings of single-cell populations along 326
with the developmental model of these populations. Moreover, our empirical study demonstrates the 327
possibility of modeling patient-specific response to treatments via the meta-learning. 328
9References 329
Albergo, M. S. and Vanden-Eijnden, E. (2022). Building normalizing flows with stochastic inter- 330
polants. arXiv preprint arXiv:2209.15571 . 331
Ambrosio, L., Gigli, N., and Savaré, G. (2008). Gradient flows: in metric spaces and in the space of 332
probability measures . Springer Science & Business Media. 333
Amos, B., Cohen, S., Luise, G., and Redko, I. (2022). Meta optimal transport. arXiv preprint 334
arXiv:2206.05262 . 335
Amos, B. et al. (2023). Tutorial on amortized optimization. Foundations and Trends ®in Machine 336
Learning , 16(5):592–732. 337
Armingol, E., Officer, A., Harismendy, O., and Lewis, N. E. (2020). Deciphering cell–cell interactions 338
and communication from gene expression. Nature Reviews Genetics , 22(2):71–88. 339
Benamou, J.-D. (2003). Numerical resolution of an “unbalanced” mass transport problem. ESAIM: 340
Mathematical Modelling and Numerical Analysis , 37(5):851–868. 341
Binnewies, M., Roberts, E. W., Kersten, K., Chan, V ., Fearon, D. F., Merad, M., Coussens, L. M., 342
Gabrilovich, D. I., Ostrand-Rosenberg, S., Hedrick, C. C., V onderheide, R. H., Pittet, M. J., Jain, 343
R. K., Zou, W., Howcroft, T. K., Woodhouse, E. C., Weinberg, R. A., and Krummel, M. F. (2018). 344
Understanding the tumor immune microenvironment (time) for effective therapy. Nature Medicine , 345
24(5):541–550. 346
Bunne, C., Stark, S. G., Gut, G., Del Castillo, J. S., Levesque, M., Lehmann, K.-V ., Pelkmans, L., 347
Krause, A., and Rätsch, G. (2023a). Learning single-cell perturbation responses using neural 348
optimal transport. Nature Methods , 20(11):1759–1768. 349
Bunne, C., Stark, S. G., Gut, G., del Castillo, J. S., Levesque, M., Lehmann, K.-V ., Pelkmans, L., 350
Krause, A., and Rätsch, G. (2023b). Learning single-cell perturbation responses using neural 351
optimal transport. Nature Methods , 20(11):1759–1768. 352
Chen, S., Rivaud, P., Park, J. H., Tsou, T., Charles, E., Haliburton, J. R., Pichiorri, F., and Thomson, 353
M. (2020). Dissecting heterogeneous cell populations across drug and disease conditions with 354
popalign. Proceedings of the National Academy of Sciences , 117(46):28784–28794. 355
Chen, T., Chen, X., Chen, W., Heaton, H., Liu, J., Wang, Z., and Yin, W. (2022). Learning to optimize: 356
A primer and a benchmark. Journal of Machine Learning Research , 23(189):1–59. 357
Chizat, L., Peyré, G., Schmitzer, B., and Vialard, F.-X. (2018). Unbalanced optimal transport: 358
Dynamic and kantorovich formulations. Journal of Functional Analysis , 274(11):3090–3123. 359
Chung, W., Eum, H. H., Lee, H.-O., Lee, K.-M., Lee, H.-B., Kim, K.-T., Ryu, H. S., Kim, S., Lee, J. E., 360
Park, Y . H., Kan, Z., Han, W., and Park, W.-Y . (2017). Single-cell rna-seq enables comprehensive 361
tumour and immune cell profiling in primary breast cancer. Nature Communications , 8(1). 362
Dao, Q., Phung, H., Nguyen, B., and Tran, A. (2023). Flow matching in latent space. arXiv preprint 363
arXiv:2307.08698 . 364
De Bortoli, V ., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion schrödinger bridge with 365
applications to score-based generative modeling. Advances in Neural Information Processing 366
Systems , 34:17695–17709. 367
Frangieh, C. J., Melms, J. C., Thakore, P. I., Geiger-Schuller, K. R., Ho, P., Luoma, A. M., Cleary, B., 368
Jerby-Arnon, L., Malu, S., Cuoco, M. S., Zhao, M., Ager, C. R., Rogava, M., Hovey, L., Rotem, 369
A., Bernatchez, C., Wucherpfennig, K. W., Johnson, B. E., Rozenblatt-Rosen, O., Schadendorf, 370
D., Regev, A., and Izar, B. (2021). Multimodal pooled perturb-cite-seq screens in patient models 371
define mechanisms of cancer immune evasion. Nature Genetics , 53(3):332–341. 372
Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). Neural message 373
passing for quantum chemistry. In International conference on machine learning , pages 1263–1272. 374
PMLR. 375
10Goodenough, D. A. and Paul, D. L. (2009). Gap junctions. Cold Spring Harb Perspect Biol , 376
1(1):a002576. 377
Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., and Smola, A. (2012). A kernel 378
two-sample test. The Journal of Machine Learning Research , 13(1):723–773. 379
Gulati, G. S., Sikandar, S. S., Wesche, D. J., Manjunath, A., Bharadwaj, A., Berger, M. J., Ilagan, F., 380
Kuo, A. H., Hsieh, R. W., Cai, S., Zabala, M., Scheeren, F. A., Lobo, N. A., Qian, D., Yu, F. B., 381
Dirbas, F. M., Clarke, M. F., and Newman, A. M. (2020). Single-cell transcriptional diversity is a 382
hallmark of developmental potential. Science , 367(6476):405–411. 383
Hashimoto, T. B., Gifford, D. K., and Jaakkola, T. S. (2016). Learning population-level diffusions 384
with generative recurrent networks. In Proceedings of the 33rd International Conference on 385
Machine Learning , pages 2417–2426. 386
Hetzel, L., Boehm, S., Kilbertus, N., Günnemann, S., Lotfollahi, M., and Theis, F. (2022). Predicting 387
cellular responses to novel drug perturbations at a single-cell resolution. In Koyejo, S., Mohamed, 388
S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information 389
Processing Systems , volume 35, pages 26711–26722. Curran Associates, Inc. 390
Huguet, G., Magruder, D. S., Tong, A., Fasina, O., Kuchroo, M., Wolf, G., and Krishnaswamy, S. 391
(2022). Manifold interpolating optimal-transport flows for trajectory inference. 392
Huguet, G., Tong, A., Zapatero, M. R., Wolf, G., and Krishnaswamy, S. (2023). Geodesic sinkhorn: 393
Optimal transport for high-dimensional datasets. In IEEE MLSP . 394
Isobe, N., Koyama, M., Hayashi, K., and Fukumizu, K. (2024). Extended flow matching: a method 395
of conditional generation with generalized continuity equation. arXiv preprint arXiv:2402.18839 . 396
Ji, Y ., Lotfollahi, M., Wolf, F. A., and Theis, F. J. (2021). Machine learning for perturbational 397
single-cell omics. Cell Systems , 12(6):522–537. 398
Koshizuka, T. and Sato, I. (2023). Neural lagrangian schr\"odinger bridge. In ICLR . 399
Lipman, Y ., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. (2023). Flow matching for 400
generative modeling. In The Eleventh International Conference on Learning Representations . 401
Liu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A., Nie, W., and Anandkumar, A. (2023). I2sb: 402
Image-to-image schrödinger bridge. In ICML . 403
Liu, X., Gong, C., and Liu, Q. (2022). Flow straight and fast: Learning to generate and transfer data 404
with rectified flow. arXiv preprint arXiv:2209.03003 . 405
Lopez, R., Regier, J., Cole, M. B., Jordan, M. I., and Yosef, N. (2018). Deep generative modeling for 406
single-cell transcriptomics. Nature Methods , 15(12):1053–1058. 407
Lotfollahi, M., Wolf, F. A., and Theis, F. J. (2019). scgen predicts single-cell perturbation responses. 408
Nature Methods , 16(8):715–721. 409
Makkuva, A. V ., Taghvaei, A., Oh, S., and Lee, J. D. (2020). Optimal transport mapping via input 410
convex neural networks. In ICML . 411
Molè, M. A., Coorens, T. H. H., Shahbazi, M. N., Weberling, A., Weatherbee, B. A. T., Gantner, 412
C. W., Sancho-Serra, C., Richardson, L., Drinkwater, A., Syed, N., Engley, S., Snell, P., Christie, 413
L., Elder, K., Campbell, A., Fishel, S., Behjati, S., Vento-Tormo, R., and Zernicka-Goetz, M. 414
(2021). A single cell characterisation of human embryogenesis identifies pluripotency transitions 415
and putative anterior hypoblast centre. Nature Communications , 12(1). 416
Neklyudov, K., Brekelmans, R., Tong, A., Atanackovic, L., Liu, Q., and Makhzani, A. (2023). A com- 417
putational framework for solving wasserstein lagrangian flows. arXiv preprint arXiv:2310.10649 . 418
Neklyudov, K., Severo, D., and Makhzani, A. (2022). Action matching: A variational method for 419
learning stochastic dynamics from samples. 420
Otto, F. (2001). The geometry of dissipative evolution equations: the porous medium equation. 421
11Peidli, S., Green, T. D., Shen, C., Gross, T., Min, J., Garda, S., Yuan, B., Schumacher, L. J., Taylor- 422
King, J. P., Marks, D. S., et al. (2024). scperturb: harmonized single-cell perturbation data. Nature 423
Methods , pages 1–10. 424
Peyré, G. and Cuturi, M. (2019). Computational Optimal Transport . arXiv:1803.00567. 425
Pooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C., Amos, B., Lipman, Y ., and Chen, R. T. 426
(2023). Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint 427
arXiv:2304.14772 . 428
Price, I., Sanchez-Gonzalez, A., Alet, F., Ewalds, T., El-Kadi, A., Stott, J., Mohamed, S., Battaglia, 429
P., Lam, R., and Willson, M. (2023). Gencast: Diffusion-based ensemble forecasting for medium- 430
range weather. arXiv preprint arXiv:2312.15796 . 431
Ramos Zapatero, M., Tong, A., Opzoomer, J. W., O’Sullivan, R., Cardoso Rodriguez, F., Sufi, J., 432
Vlckova, P., Nattress, C., Qin, X., Claus, J., Hochhauser, D., Krishnaswamy, S., and Tape, C. J. 433
(2023). Trellis tree-based analysis reveals stromal regulation of patient-derived organoid drug 434
responses. Cell, 186(25):5606–5619.e24. 435
Rizvi, A. H., Camara, P. G., Kandror, E. K., Roberts, T. J., Schieren, I., Maniatis, T., and Rabadan, R. 436
(2017). Single-cell topological rna-seq analysis reveals insights into cellular differentiation and 437
development. Nature Biotechnology , 35(6):551–560. 438
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image 439
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer 440
vision and pattern recognition , pages 10684–10695. 441
Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D., and Norouzi, M. (2022a). 442
Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings , 443
pages 1–10. 444
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, 445
R., Karagol Ayan, B., Salimans, T., et al. (2022b). Photorealistic text-to-image diffusion models 446
with deep language understanding. Advances in neural information processing systems , 35:36479– 447
36494. 448
Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., and Battaglia, P. (2020). Learning 449
to simulate complex physics with graph networks. In International conference on machine learning , 450
pages 8459–8468. PMLR. 451
Schiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V ., Solomon, A., Gould, J., Liu, 452
S., Lin, S., Berube, P., et al. (2019). Optimal-transport analysis of single-cell gene expression 453
identifies developmental trajectories in reprogramming. Cell, 176(4):928–943. 454
Somnath, V . R., Pariset, M., Hsieh, Y .-P., Martinez, M. R., Krause, A., and Bunne, C. (2023). Aligned 455
diffusion schr\"odinger bridges. In UAI. 456
Tong, A., FATRAS, K., Malkin, N., Huguet, G., Zhang, Y ., Rector-Brooks, J., Wolf, G., and Bengio, 457
Y . (2024). Improving and generalizing flow-based generative models with minibatch optimal 458
transport. Transactions on Machine Learning Research . Expert Certification. 459
Tong, A., Huang, J., Wolf, G., Van Dijk, D., and Krishnaswamy, S. (2020). Trajectorynet: A dynamic 460
optimal transport network for modeling cellular dynamics. In International conference on machine 461
learning , pages 9526–9536. PMLR. 462
Tong, A., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y ., Huguet, G., Wolf, G., and Bengio, 463
Y . (2023). Simulation-free schr \" odinger bridges via score and flow matching. arXiv preprint 464
arXiv:2307.03672 . 465
Uscidda, T. and Cuturi, M. (2023). The monge gap: A regularizer to learn all transport maps. In 466
Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J., editors, Proceedings 467
of the 40th International Conference on Machine Learning , volume 202 of Proceedings of Machine 468
Learning Research , pages 34709–34733. PMLR. 469
12Verma, Y ., Heinonen, M., and Garg, V . (2024). Climode: Climate and weather forecasting with 470
physics-informed neural odes. arXiv preprint arXiv:2404.10024 . 471
Villani, C. (2009). Optimal transport: old and new , volume 338. Springer. 472
Weinreb, C., Wolock, S., Tusi, B. K., Socolovsky, M., and Klein, A. M. (2018). Fundamental limits 473
on dynamic inference from single-cell snapshots. 115(10):E2467–E2476. 474
Yang, K. D. and Uhler, C. (2019). Scalable unbalanced optimal transport using generative adversarial 475
networks. In 7th International Conference on Learning Representations , page 20. 476
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017). 477
Deep sets. Advances in neural information processing systems , 30. 478
Zeng, T. and Dai, H. (2019). Single-cell rna sequencing-based computational analysis to describe 479
disease heterogeneity. Frontiers in Genetics , 10. 480
Zheng, Q., Le, M., Shaul, N., Lipman, Y ., Grover, A., and Chen, R. T. (2023). Guided flows for 481
generative modeling and decision making. arXiv preprint arXiv:2311.13443 . 482
13A Proof of Theorem 1 483
Theorem 1. Consider a dataset of populations D={(π(x0, x1|i))}igenerated from some unknown 484
conditional model π(x0, x1|c)p(c). Then the following objective 485
L(ω, θ) =Ep(c)Z1
0dtEpt(xt|c)∥v∗
t(xt|c)−vt(xt|φ(p0, θ), ω)∥2(16)
is equivalent to the Meta Flow Matching objective 486
LMFM(ω, θ) =Ei∼DEπ(x0,x1|i)Z1
0dt∂
∂tft(x0, x1)−vt(ft(x0, x1)|φ(p0, θ);ω)2
(17)
up to an additive constant. 487
Proof. The loss function 488
L(ω, θ) =Ep(c)Z1
0dtEpt(xt|c)∥v∗
t(xt|c)−vt(xt|φ(pt, θ);ω)∥2(20)
=−2Ep(c)Z
dtdx⟨pt(x|c)v∗
t(x|c), vt(x|φ(pt, θ);ω)⟩+ (21)
+Ep(c)Z1
0dtEpt(xt|c)∥vt(xt|φ(pt, θ), ω)∥2+ (22)
+Ep(c)Z1
0dtEpt(xt|c)∥v∗
t(xt|c)∥2. (23)
The last term does not depend on θ, the second term we can estimate, for the first term, we use the 489
formula for the (from Eq. (8)) 490
pt(ξ|c)v∗
t(ξ|c) =Eπ(x0,x1)δ(ft(x0, x1)−ξ)∂ft(x0, x1)
∂t. (24)
Thus, the loss is equivalent (up to a constant) to 491
L(ω, θ) =−2Ep(c)Eπ(x0,x1|c)Z
dt∂ft(x0, x1)
∂t, vt(ft(x0, x1)|φ(pt, θ);ω)
+ (25)
+Ep(c)Eπ(x0,x1|c)Z1
0dt∥vt(ft(x0, x1)|φ(pt, θ), ω)∥2± (26)
±Ep(c)Eπ(x0,x1|c)Z1
0dt∂ft(x0, x1)
∂t2
(27)
=Ec∼p(c)Eπ(x0,x1|c)Z1
0dt∂
∂tft(x0, x1)−vt(ft(x0, x1)|φ(pt, θ);ω)2
. (28)
Note that in the final expression we do not need access to the probabilistic model of p(c)if the joints 492
π(x0, x1|c)are already sampled in the data D. Thus, we have 493
L(ω, θ) =Ec∼p(c)Eπ(x0,x1|c)Z1
0dt∂
∂tft(x0, x1)−vt(ft(x0, x1)|φ(pt, θ);ω)2
(29)
=Ei∼DEπ(x0,x1|i)Z1
0dt∂
∂tft(x0, x1)−vt(ft(x0, x1)|φ(pt, θ);ω)2
(30)
=LMFM(ω, θ). (31)
494
B Experimental Details 495
B.1 Synthetic letters data 496
The synthetic letters dataset contains 242 train populations a 10 test populations. Each population 497
contains roughly between 750 and 2700 samples. In this dataset. 498
14B.2 Organoid drug-screen data 499
The organoid drug-screen dataset contains a total of 927 replicates (or coupled populations). In the 500
replicates split , we use 713 populations for training and 103 left-out populations for testing. In the 501
patients split , we use 861 populations for training and 33 left-out populations for testing. 502
B.3 Model architectures and hyperparameters 503
ICNN. The ICNN baseline was constructed with two networks ICNN network f(x)andg(x), with 504
non-negative leaky ReLU activation layers. f(x)is used to minimize the transport distance and g(x) 505
is used to transport from source to target. It has four hidden units with width of 64, and a latent 506
dimension of 50. Both networks uses Adam optimizer (lr= 1e−4,β1=0.5, β2=0.9). g(x)is trained 507
with an inner iteration of 10for every iteration f(x)is trained. 508
Vector Field Models. All vector field models vtare parameterized 4 linear layers with 512 hidden 509
units and SELU activation functions. The FM vector field model additionally takes a conditional 510
input for the one-hot treatment encoding. CGFM takes the conditional input for the one-hot treatment 511
conditions as well as a one-hot encoding for the population index condition i. The MFM vector field 512
model takes population embedding conditions, that is output from the GCN, as input, as well as the 513
treatment one-hot encoding. All vector field models use temporal embeddings for time and positional 514
embeddings for the input samples. We did not sweep the size of this embeddings space and found 515
that a temporal embedding and positional embeddings sizes of 128worked sufficiently well. 516
Graph Neural Network. We considered a GCN model that consists of a k-nearest neighbour graph 517
edge pooling layer and 3 graph convolution layers with 512 hidden units. The final GCN model 518
layer outputs an embedding representation e∈Rd. For the Synthetic experiment, we found that 519
d= 256 performed well, and d= 128 performed well for the biological experiments. We normalize 520
and project embeddings onto a hyper-sphere, and find that this normalization helps improve training. 521
Additionally, the GCN takes a one-hot cell-type encoding (encoding for Fibroblast cells or PDO 522
cells) for the control populations p0. This may be beneficial for PDOF populations where both 523
Fibroblast cells and PDO cells are present. However, it is important to note that labeling which cells 524
are Fibroblasts versus PDOs withing the PDOF cultures is difficult and noisy in itself, hence such a 525
cell-type condition may yield no additive information/performance gain. 526
Optimization. We use the Adam optimizer with a learning rate of 0.0001 for all Flow-matching 527
models (FM, CGFM, MFM). We also used the Adam optimizer with a learning rate of 0.0001 for 528
the GCN model. To train the MFM (FM+GCN) models, we alternate between updating the vector 529
field model parameters ωand the GCN model parameters θ. We alternate between updating the 530
respective model parameters every epoch. FM and CGFM model were trained for 2000 epochs, while 531
MFM models were trained for 4000 epochs. Due to the alternating optimization, the MFM vector 532
field model receives half as many updates compared to its counterparts (FM and CGFM). Therefore, 533
training for the double the epochs is necessary for fair comparison. 534
The hyperparameters stated in this section were selected from brief and small grid search sweeps. We 535
did not conduct any thorough hyperparameter optimization. 536
C Implementation Details 537
We implement all our experiments using PyTorch and PyTorch Geometric. We submitted our code as 538
supplementary material with our submission. 539
All experiments were conducted on a HPC cluster primarily on NVIDIA Tesla T4 16GB GPUs. Each 540
individual seed experiment run required only 1 GPU. Each experiment ran between 3-11 hours and 541
all experiments took approximately 500 GPU hours. 542
D Limitations 543
In this work we explored empirically the effect of conditioning the learned flow on the initial 544
distribution. We argue this is a more natural model for many biological systems. However, there 545
are many other aspects of modeling biological systems that we did not consider. In particular we 546
15did not consider extensions to the manifold setting (Huguet et al., 2022, 2023), unbalanced optimal 547
transport (Benamou, 2003; Yang and Uhler, 2019; Chizat et al., 2018), aligned (Somnath et al., 2023; 548
Liu et al., 2023), or stochastic settings (Bunne et al., 2023a; Koshizuka and Sato, 2023) in this work. 549
E Broader Impacts 550
This paper is primarily a theoretical and methodological contribution with little societal impact. MFM 551
can be used to better model dynamical systems of interacting particles and in particular cellular 552
systems. Better modeling of cellular systems can potentially be used for the development of malicious 553
biological agents. However, we do not see this as a significant risk at this time. 554
F Extended Results 555
Table 4: Experimental results on the organoid drug-screen dataset for population prediction of treatment response
across replicate populations. Results are reported for models trained on data embedded into 10 principle
components. We report the the 1-Wasserstein ( W1), 2-Wasserstein ( W2), and the maximum-mean-discrepancy
(MMD) distributional distances. We consider 2 settings for MFM with varying nearest-neighbours parameter.
Fibroblasts
Train Test
W1 W2 MMD ( ×10−3) W1 W2 MMD ( ×10−3)
FM 1.584±0.022 1 .730±0.015 3 .12±0.59 1 .612±0.014 1 .736±0.024 3 .62±0.15
ICNN 1.613±0.010 1 .703±0.010 52 .4±1.64 1 .655±0.008 1 .746±0.008 53 .0±5.00
CGFM 1.472±0.046 1.548 ±0.048 1.28 ±0.74 1.633±0.022 1 .724±0.023 4 .95±0.72
MFM ( k= 0)1.519±0.034 1 .599±0.036 2 .56±0.56 1.574±0.002 1.657 ±0.003 3.31 ±0.12
MFM ( k= 10 )1.547±0.027 1 .617±0.027 2 .84±0.56 1 .576±0.017 1 .658±0.019 3 .44±0.19
PDO
Train Test
W1 W2 MMD ( ×10−3) W1 W2 MMD ( ×10−3)
FM 2.002±0.027 2 .201±0.025 6 .40±0.10 2 .033±0.015 2 .210±0.016 6 .92±0.65
ICNN 2.29±0.005 2 .458±0.003 245 .8±9.18 2 .247±0.005 2 .415±0.004 153 ±1.00
CGFM 1.818±0.198 1 .931±0.229 3 .78±0.27 2 .255±0.216 2 .434±0.240 12 .16±3.87
MFM ( k= 0)1.817±0.043 1 .935±0.040 3.61±0.50 1.909±0.076 2 .057±0.098 5.14±0.92
MFM ( k= 10 )1.805±0.074 1.921 ±0.078 3.68±0.78 1.903±0.068 2.051 ±0.084 5.14 ±0.90
PDOF
Train Test
W1 W2 MMD ( ×10−3) W1 W2 MMD ( ×10−3)
FM 2.252±0.20 2 .603±0.236 9 .43±0.38 2 .616±0.076 2 .958±0.089 19 .34±1.51
ICNN 2.432±0.021 2 .791±0.020 272 .3±3.80 2 .699±0.021 3 .023±0.019 542 ±8.00
CGFM 2.179±0.133 2 .548±0.153 7.42±2.00 2.750±0.173 3 .089±0.200 22 .63±2.64
MFM ( k= 0) 2.150±0.073 2.502 ±0.099 7.75±0.93 2 .395±0.071 2 .717±0.076 13 .61±2.56
MFM ( k= 10 )2.174±0.046 2 .523±0.067 7 .75±0.65 2.382±0.055 2.699 ±0.054 13.45 ±1.69
16Table 5: Experimental results on the organoid drug-screen dataset for population prediction of treatment
response across patient populations. Results are reported for models trained on data embedded into 10 principle
components. We report the the 1-Wasserstein ( W1), 2-Wasserstein ( W2), and the maximum-mean-discrepancy
(MMD) distributional distances. We consider 2 settings for MFM with varying nearest-neighbours parameter.
Fibroblasts
Train Test
W1 W2 MMD ( ×10−3) W1 W2 MMD ( ×10−3)
FM 1.599±0.071 1 .761±0.137 2 .82±0.34 1 .667±0.003 1 .846±0.064 7 .85±0.15
ICNN 1.695±0.08 1 .796±0.09 48 .2±3.412 1 .6±0.009 1 .68±0.013 62 .2±1.32
CGFM 1.496±0.019 1.572 ±0.016 1.45 ±0.14 1.566±0.028 1 .652±0.026 6 .46±0.82
MFM ( k= 0)1.551±0.037 1 .632±0.042 2 .31±0.71 1 .453±0.200 1 .527±0.022 3 .66±0.67
MFM ( k= 10 )1.555±0.034 1 .635±0.039 2 .54±0.42 1.441±0.003 1.514 ±0.001 3.37 ±0.72
PDO
Train Test
W1 W2 MMD ( ×10−3) W1 W2 MMD ( ×10−3)
FM 1.996±0.196 2 .171±0.243 6 .79±3.40 2 .128±0.064 2 .312±0.075 7 .88±1.26
ICNN 2.315±0.060 2 .478±0.057 236 .8±0.006 2 .538±0.018 2 .731±0.027 232 .8±20.6
CGFM 1.662±0.026 1.760 ±0.023 1.74 ±0.16 2.460±0.018 2 .533±0.023 13 .6±0.25
MFM ( k= 0)1.837±0.058 1 .964±0.059 3 .74±0.29 2 .010±0.142 2 .168±0.182 6 .01±1.77
MFM ( k= 10 )1.838±0.035 1 .957±0.038 3 .75±0.41 1.971±0.082 2.114 ±0.101 5.42 ±1.11
PDOF
Train Test
W1 W2 MMD ( ×10−3) W1 W2 MMD ( ×10−3)
FM 2.390±0.148 2 .806±0.198 11 .0±2.21 4 .026±0.018 4 .683±0.011 49 .0±1.66
ICNN 2.479±0.06 2 .826±0.063 291 ±9.24 3 .968±0.0554 4 .579±0.060 1263 ±37.5
CGFM 2.160±0.170 2.530 ±0.237 7.90 ±1.79 4.000±0.010 4 .629±0.012 49 .2±0.76
MFM ( k= 0)2.202±0.072 2 .548±0.089 8 .98±0.59 3 .717±0.138 4 .360±0.162 40 .3±3.52
MFM ( k= 10 )2.251±0.143 2 .631±0.197 9 .45±1.52 3.565±0.132 4.201 ±0.119 36.1 ±4.97
17source
 t=0.50
 t=1.00
 target
Train
source
 t=0.50
 t=1.00
 target
source
 t=0.50
 t=1.00
 target
source
 t=0.50
 t=1.00
 target
 Test
FM
source
 t=0.50
 t=1.00
 target
 CGFM
source
 t=0.50
 t=1.00
 target
 MFM
Figure 4: Model-generated samples for synthetic letters from the source ( t= 0) to target ( t= 1) distributions.
18NeurIPS Paper Checklist 556
1.Claims 557
Question: Do the main claims made in the abstract and introduction accurately reflect the 558
paper’s contributions and scope? 559
Answer: [Yes] 560
Justification: Claims and contributions introduced in abstract and introduction are sup- 561
ported with theoretical result in Section 3 and empirical results through synthetic and real 562
experiments in Section 5. 563
Guidelines: 564
•The answer NA means that the abstract and introduction do not include the claims 565
made in the paper. 566
•The abstract and/or introduction should clearly state the claims made, including the 567
contributions made in the paper and important assumptions and limitations. A No or 568
NA answer to this question will not be perceived well by the reviewers. 569
•The claims made should match theoretical and experimental results, and reflect how 570
much the results can be expected to generalize to other settings. 571
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 572
are not attained by the paper. 573
2.Limitations 574
Question: Does the paper discuss the limitations of the work performed by the authors? 575
Answer: [Yes] 576
Justification: We discuss limitations in Appendix D. 577
Guidelines: 578
•The answer NA means that the paper has no limitation while the answer No means that 579
the paper has limitations, but those are not discussed in the paper. 580
• The authors are encouraged to create a separate "Limitations" section in their paper. 581
•The paper should point out any strong assumptions and how robust the results are to 582
violations of these assumptions (e.g., independence assumptions, noiseless settings, 583
model well-specification, asymptotic approximations only holding locally). The authors 584
should reflect on how these assumptions might be violated in practice and what the 585
implications would be. 586
•The authors should reflect on the scope of the claims made, e.g., if the approach was 587
only tested on a few datasets or with a few runs. In general, empirical results often 588
depend on implicit assumptions, which should be articulated. 589
•The authors should reflect on the factors that influence the performance of the approach. 590
For example, a facial recognition algorithm may perform poorly when image resolution 591
is low or images are taken in low lighting. Or a speech-to-text system might not be 592
used reliably to provide closed captions for online lectures because it fails to handle 593
technical jargon. 594
•The authors should discuss the computational efficiency of the proposed algorithms 595
and how they scale with dataset size. 596
•If applicable, the authors should discuss possible limitations of their approach to 597
address problems of privacy and fairness. 598
•While the authors might fear that complete honesty about limitations might be used by 599
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 600
limitations that aren’t acknowledged in the paper. The authors should use their best 601
judgment and recognize that individual actions in favor of transparency play an impor- 602
tant role in developing norms that preserve the integrity of the community. Reviewers 603
will be specifically instructed to not penalize honesty concerning limitations. 604
3.Theory Assumptions and Proofs 605
Question: For each theoretical result, does the paper provide the full set of assumptions and 606
a complete (and correct) proof? 607
19Answer: [Yes] 608
Justification: Theory is provided in Section 2 and Section 3. Proofs are provide in Ap- 609
pendix A 610
Guidelines: 611
• The answer NA means that the paper does not include theoretical results. 612
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 613
referenced. 614
•All assumptions should be clearly stated or referenced in the statement of any theorems. 615
•The proofs can either appear in the main paper or the supplemental material, but if 616
they appear in the supplemental material, the authors are encouraged to provide a short 617
proof sketch to provide intuition. 618
•Inversely, any informal proof provided in the core of the paper should be complemented 619
by formal proofs provided in appendix or supplemental material. 620
• Theorems and Lemmas that the proof relies upon should be properly referenced. 621
4.Experimental Result Reproducibility 622
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 623
perimental results of the paper to the extent that it affects the main claims and/or conclusions 624
of the paper (regardless of whether the code and data are provided or not)? 625
Answer: [Yes] 626
Justification: All details for reproducing results and experiments can be found through 627
the main text body and appendix. The details include: dataset resource Ramos Zapatero 628
et al. (2023), data processing, model architecture and optimization details, and performance 629
metrics. 630
Guidelines: 631
• The answer NA means that the paper does not include experiments. 632
•If the paper includes experiments, a No answer to this question will not be perceived 633
well by the reviewers: Making the paper reproducible is important, regardless of 634
whether the code and data are provided or not. 635
•If the contribution is a dataset and/or model, the authors should describe the steps taken 636
to make their results reproducible or verifiable. 637
•Depending on the contribution, reproducibility can be accomplished in various ways. 638
For example, if the contribution is a novel architecture, describing the architecture fully 639
might suffice, or if the contribution is a specific model and empirical evaluation, it may 640
be necessary to either make it possible for others to replicate the model with the same 641
dataset, or provide access to the model. In general. releasing code and data is often 642
one good way to accomplish this, but reproducibility can also be provided via detailed 643
instructions for how to replicate the results, access to a hosted model (e.g., in the case 644
of a large language model), releasing of a model checkpoint, or other means that are 645
appropriate to the research performed. 646
•While NeurIPS does not require releasing code, the conference does require all submis- 647
sions to provide some reasonable avenue for reproducibility, which may depend on the 648
nature of the contribution. For example 649
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 650
to reproduce that algorithm. 651
(b)If the contribution is primarily a new model architecture, the paper should describe 652
the architecture clearly and fully. 653
(c)If the contribution is a new model (e.g., a large language model), then there should 654
either be a way to access this model for reproducing the results or a way to reproduce 655
the model (e.g., with an open-source dataset or instructions for how to construct 656
the dataset). 657
(d)We recognize that reproducibility may be tricky in some cases, in which case 658
authors are welcome to describe the particular way they provide for reproducibility. 659
In the case of closed-source models, it may be that access to the model is limited in 660
some way (e.g., to registered users), but it should be possible for other researchers 661
to have some path to reproducing or verifying the results. 662
205.Open access to data and code 663
Question: Does the paper provide open access to the data and code, with sufficient instruc- 664
tions to faithfully reproduce the main experimental results, as described in supplemental 665
material? 666
Answer: [Yes] 667
Justification: The data used in the empirical study is either synthetic or publicly available. 668
The code reproducing all the experiments is attached to the paper. 669
Guidelines: 670
• The answer NA means that paper does not include experiments requiring code. 671
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 672
public/guides/CodeSubmissionPolicy ) for more details. 673
•While we encourage the release of code and data, we understand that this might not be 674
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 675
including code, unless this is central to the contribution (e.g., for a new open-source 676
benchmark). 677
•The instructions should contain the exact command and environment needed to run to 678
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 679
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 680
•The authors should provide instructions on data access and preparation, including how 681
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 682
•The authors should provide scripts to reproduce all experimental results for the new 683
proposed method and baselines. If only a subset of experiments are reproducible, they 684
should state which ones are omitted from the script and why. 685
•At submission time, to preserve anonymity, the authors should release anonymized 686
versions (if applicable). 687
•Providing as much information as possible in supplemental material (appended to the 688
paper) is recommended, but including URLs to data and code is permitted. 689
6.Experimental Setting/Details 690
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 691
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 692
results? 693
Answer: [Yes] 694
Justification: The paper discusses the experimental setup necessary to understand the results 695
in Section 5. Furthermore, the details of the empirical study are provided in Appendix B. 696
Guidelines: 697
• The answer NA means that the paper does not include experiments. 698
•The experimental setting should be presented in the core of the paper to a level of detail 699
that is necessary to appreciate the results and make sense of them. 700
•The full details can be provided either with the code, in appendix, or as supplemental 701
material. 702
7.Experiment Statistical Significance 703
Question: Does the paper report error bars suitably and correctly defined or other appropriate 704
information about the statistical significance of the experiments? 705
Answer: [Yes] 706
Justification: All the results presented in the paper are averaged over multiple independent 707
runs and the standard deviations are provided along the metrics. 708
Guidelines: 709
• The answer NA means that the paper does not include experiments. 710
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 711
dence intervals, or statistical significance tests, at least for the experiments that support 712
the main claims of the paper. 713
21•The factors of variability that the error bars are capturing should be clearly stated (for 714
example, train/test split, initialization, random drawing of some parameter, or overall 715
run with given experimental conditions). 716
•The method for calculating the error bars should be explained (closed form formula, 717
call to a library function, bootstrap, etc.) 718
• The assumptions made should be given (e.g., Normally distributed errors). 719
•It should be clear whether the error bar is the standard deviation or the standard error 720
of the mean. 721
•It is OK to report 1-sigma error bars, but one should state it. The authors should 722
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 723
of Normality of errors is not verified. 724
•For asymmetric distributions, the authors should be careful not to show in tables or 725
figures symmetric error bars that would yield results that are out of range (e.g. negative 726
error rates). 727
•If error bars are reported in tables or plots, The authors should explain in the text how 728
they were calculated and reference the corresponding figures or tables in the text. 729
8.Experiments Compute Resources 730
Question: For each experiment, does the paper provide sufficient information on the com- 731
puter resources (type of compute workers, memory, time of execution) needed to reproduce 732
the experiments? 733
Answer: [Yes] 734
Justification: The paper discuss the compute resources and reproducibility in Appendix C. 735
Guidelines: 736
• The answer NA means that the paper does not include experiments. 737
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 738
or cloud provider, including relevant memory and storage. 739
•The paper should provide the amount of compute required for each of the individual 740
experimental runs as well as estimate the total compute. 741
•The paper should disclose whether the full research project required more compute 742
than the experiments reported in the paper (e.g., preliminary or failed experiments that 743
didn’t make it into the paper). 744
9.Code Of Ethics 745
Question: Does the research conducted in the paper conform, in every respect, with the 746
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 747
Answer: [Yes] 748
Justification: The research does conform with the NeurIPS Code of Ethics. The study 749
presented involves only public or synthetic data, which is freely available online. The 750
considered models do not impose risks of misuse or dual-use. 751
Guidelines: 752
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 753
•If the authors answer No, they should explain the special circumstances that require a 754
deviation from the Code of Ethics. 755
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 756
eration due to laws or regulations in their jurisdiction). 757
10.Broader Impacts 758
Question: Does the paper discuss both potential positive societal impacts and negative 759
societal impacts of the work performed? 760
Answer: [Yes] 761
Justification: The paper discusses the broader impact in Appendix E. 762
Guidelines: 763
• The answer NA means that there is no societal impact of the work performed. 764
22•If the authors answer NA or No, they should explain why their work has no societal 765
impact or why the paper does not address societal impact. 766
•Examples of negative societal impacts include potential malicious or unintended uses 767
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 768
(e.g., deployment of technologies that could make decisions that unfairly impact specific 769
groups), privacy considerations, and security considerations. 770
•The conference expects that many papers will be foundational research and not tied 771
to particular applications, let alone deployments. However, if there is a direct path to 772
any negative applications, the authors should point it out. For example, it is legitimate 773
to point out that an improvement in the quality of generative models could be used to 774
generate deepfakes for disinformation. On the other hand, it is not needed to point out 775
that a generic algorithm for optimizing neural networks could enable people to train 776
models that generate Deepfakes faster. 777
•The authors should consider possible harms that could arise when the technology is 778
being used as intended and functioning correctly, harms that could arise when the 779
technology is being used as intended but gives incorrect results, and harms following 780
from (intentional or unintentional) misuse of the technology. 781
•If there are negative societal impacts, the authors could also discuss possible mitigation 782
strategies (e.g., gated release of models, providing defenses in addition to attacks, 783
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 784
feedback over time, improving the efficiency and accessibility of ML). 785
11.Safeguards 786
Question: Does the paper describe safeguards that have been put in place for responsible 787
release of data or models that have a high risk for misuse (e.g., pretrained language models, 788
image generators, or scraped datasets)? 789
Answer: [NA] . 790
Justification: The models considered in the paper do not carry the risks of misuse or dual-use. 791
Guidelines: 792
• The answer NA means that the paper poses no such risks. 793
•Released models that have a high risk for misuse or dual-use should be released with 794
necessary safeguards to allow for controlled use of the model, for example by requiring 795
that users adhere to usage guidelines or restrictions to access the model or implementing 796
safety filters. 797
•Datasets that have been scraped from the Internet could pose safety risks. The authors 798
should describe how they avoided releasing unsafe images. 799
•We recognize that providing effective safeguards is challenging, and many papers do 800
not require this, but we encourage authors to take this into account and make a best 801
faith effort. 802
12.Licenses for existing assets 803
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 804
the paper, properly credited and are the license and terms of use explicitly mentioned and 805
properly respected? 806
Answer: [Yes] . 807
Justification: We cite (Ramos Zapatero et al., 2023) that produced the dataset used in the 808
study. The dataset is available under the license CC BY 4.0. 809
Guidelines: 810
• The answer NA means that the paper does not use existing assets. 811
• The authors should cite the original paper that produced the code package or dataset. 812
•The authors should state which version of the asset is used and, if possible, include a 813
URL. 814
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 815
•For scraped data from a particular source (e.g., website), the copyright and terms of 816
service of that source should be provided. 817
23•If assets are released, the license, copyright information, and terms of use in the 818
package should be provided. For popular datasets, paperswithcode.com/datasets 819
has curated licenses for some datasets. Their licensing guide can help determine the 820
license of a dataset. 821
•For existing datasets that are re-packaged, both the original license and the license of 822
the derived asset (if it has changed) should be provided. 823
•If this information is not available online, the authors are encouraged to reach out to 824
the asset’s creators. 825
13.New Assets 826
Question: Are new assets introduced in the paper well documented and is the documentation 827
provided alongside the assets? 828
Answer: [NA] . 829
Justification: The paper does not release new assets. 830
Guidelines: 831
• The answer NA means that the paper does not release new assets. 832
•Researchers should communicate the details of the dataset/code/model as part of their 833
submissions via structured templates. This includes details about training, license, 834
limitations, etc. 835
•The paper should discuss whether and how consent was obtained from people whose 836
asset is used. 837
•At submission time, remember to anonymize your assets (if applicable). You can either 838
create an anonymized URL or include an anonymized zip file. 839
14.Crowdsourcing and Research with Human Subjects 840
Question: For crowdsourcing experiments and research with human subjects, does the paper 841
include the full text of instructions given to participants and screenshots, if applicable, as 842
well as details about compensation (if any)? 843
Answer: [NA] . 844
Justification: The empirical study presented in the paper is conducted on the synthetic or 845
publicly available data. 846
Guidelines: 847
•The answer NA means that the paper does not involve crowdsourcing nor research with 848
human subjects. 849
•Including this information in the supplemental material is fine, but if the main contribu- 850
tion of the paper involves human subjects, then as much detail as possible should be 851
included in the main paper. 852
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 853
or other labor should be paid at least the minimum wage in the country of the data 854
collector. 855
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 856
Subjects 857
Question: Does the paper describe potential risks incurred by study participants, whether 858
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 859
approvals (or an equivalent approval/review based on the requirements of your country or 860
institution) were obtained? 861
Answer: [NA] 862
Justification: The empirical study presented in the paper is conducted on the synthetic or 863
publicly available data. 864
Guidelines: 865
•The answer NA means that the paper does not involve crowdsourcing nor research with 866
human subjects. 867
24•Depending on the country in which research is conducted, IRB approval (or equivalent) 868
may be required for any human subjects research. If you obtained IRB approval, you 869
should clearly state this in the paper. 870
•We recognize that the procedures for this may vary significantly between institutions 871
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 872
guidelines for their institution. 873
•For initial submissions, do not include any information that would break anonymity (if 874
applicable), such as the institution conducting the review. 875
25