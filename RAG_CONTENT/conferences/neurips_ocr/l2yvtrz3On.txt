Improved Sample Complexity for Multiclass PAC
Learning
Steve Hanneke
Purdue University
steve.hanneke@gmail.comShay Moran
Technion and Google Research
smoran@technion.ac.il
Qian Zhang
Purdue University
zhan3761@purdue.edu
Abstract
We aim to understand the optimal PAC sample complexity in multiclass learning.
While finiteness of the Daniely-Shalev-Shwartz (DS) dimension has been shown
to characterize the PAC learnability of a concept class [Brukhim, Carmon, Dinur,
Moran, and Yehudayoff, 2022], there exist polylog factor gaps in the leading term of
the sample complexity. In this paper, we reduce the gap in terms of the dependence
on the error parameter to a single log factor and also propose two possible routes
towards completely resolving the optimal sample complexity, each based on a
key open question we formulate: one concerning list learning with bounded list
size, the other concerning a new type of shifting for multiclass concept classes.
We prove that a positive answer to either of the two questions would completely
resolve the optimal sample complexity up to log factors of the DS dimension.
1 Introduction
Multiclass learning refers to the problem of classifying an input feature from a set (feature space) X
to a label in a set (label space) Ywith|Y|ą2(Ycan be infinite) [Natarajan, 1989, Ben-David et al.,
1995, Daniely and Shalev-Shwartz, 2014, Brukhim et al., 2022]. When |Y|“2, the problem is known
as binary classification. Multiclass learning has wide applications to various tasks in machine learning
including image classification [Rawat and Wang, 2017], natural language processing [Kowsari et al.,
2019], tissue classification [Li et al., 2004], etc. For theoretical analysis of multiclass learning, a
probabilistic setting is typically assumed, where all the feature-label pairs in the training sequence are
assumed to be independent and identically distributed (iid) samples from some distribution Pover
XˆY. Then, the objective of the learner is to minimize the error rate of the output classifier under
the distribution P. A basic framework in the probabilistic setting is Probably Approximately Correct
(PAC) learning [Valiant, 1984]. Though the characterization of PAC learnability of a binary concept
class with the finiteness of its Vapnik-Chervonenkis (VC) dimension has been proved by Vapnik and
Chervonenkis [1968], the characterization of the multiclass PAC learnability remained open until
Brukhim et al. [2022] showed the equivalence between the PAC learnability of a concept class and
the finiteness of its Daniely-Shalev-Shwartz (DS) dimension ( dim, see Definition 1.4) [Daniely and
Shalev-Shwartz, 2014] instead of Natarajan dimension or graph dimension.
However, the problem of establishing the optimal sample complexity or error rate (see Section 1.1 for
formal definitions) for multiclass learning remains unsolved. For binary concept classes, Hanneke
[2016] showed that the sample complexity is in Θppd`logp1{δqq{εqwhere dis the VC dimension
of the concept class. Since DS dimension and VC dimension coincide for binary concept classes,
it is natural to ask if the sample complexity of a concept class HĎYXfor|Y| ą2is also in
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Θppd`logp1{δqq{εqwhere d“dimpHqis the DS dimension of H. In terms of the upper bound, it
asks if there exists a multiclass learner whose worst case error rate is in Oppd`logp1{δqq{nqwith
probability at least 1´δ, where ndenotes the size of the training sequence. However, on the one
hand, an explicit proof of the Ωppd`logp1{δqq{εqlower bound on the sample complexity is still
missing in the literature. On the other hand, the current best upper bound on worst case error rate to
our knowledge is O`pd3{2logpdq`dlogplogpnqqqlog2pnq`logp1{δq
n˘
[Brukhim et al., 2022], which differs
from the conjectured rate by the factor plogplogpnqq`?
dlogpdqqlog2pnq.
In this paper, we step forward towards improved sample complexity and error rate in multiclass
learning. As the concept class is fixed and the sample size increases during an online learning
process, we mainly focus on improving the error rate in terms of the sample size n. Specifi-
cally, for a concept class HĎYXwith dimpHq “d, we prove an Ωppd`logp1{δqq{εqlower
bound on its sample complexity and construct a multiclass learner whose worst case error rate is
Oppd3{2logpdqlogpnq`logp1{δqq{nqwith probability at least 1´δ, which implies that the sample
complexity of HisO`d3{2logpdqlogpd{εq`logp1{δq
ε˘
. Our results greatly narrow the gap between the
upper and lower bounds of the sample complexity and the error rate. The dependence of the upper
bound of the error rate on the sample size has also been improved from Oplogplogpnqqlog2pnq{nq
toOplogpnq{nq(treating das a constant). The multiclass learner we construct builds upon a list
learner which predicts a list of labels for the test point (see Section 2.1 for a detailed introduction
to list learning). Actually, we prove a reduction from multiclass learning to list learning and upper
bound the error rate of the constructed multiclass learner with some function of the list size and the
expected error rate of the list learner (the probability of excluding the true label in the predicted list).
Moreover, the upper bound indicates that a list learner with size independent of nand expected error
rate scaling linearly in 1{nin terms of the sample size nwould imply an Op1{nqerror rate (treating
das a constant). We leave the construction of such list learners an open question.
Furthermore, we also explore an alternative combinatorial approach towards improved sample
complexity in multiclass learning. For a concept class, we can define a hypergraph called the
one-inclusion graph [Haussler et al., 1994] on its projection to a finite sequence of features (see
Section 1.1 for definitions). Then, informally speaking, the “density” (defined through the average
degree of the one-inclusion graph) of a concept class can be used to upper bound the error rate of
multiclass learning [Daniely and Shalev-Shwartz, 2014, Aden-Ali et al., 2023]. Specifically, if we can
upper bound the density of any concept class Hby a multiple of its DS dimension, then the sample
complexity is in OppdimpHq`logp1{δqq{εq, which matches the lower bound we prove. Thus, a
proof of the above upper bound directly leads to a ΘppdimpHq`logp1{δqq{εqsample complexity for
multiclass learning. When dimpHq“1, we successfully prove the Θplogp1{δq{εqsample complexity
in Theorem 3.2. For general concept classes, we develop a technique named “pivot shifting” similar
to the shifting operator [Haussler, 1995] on concept classes. We show that if a pivot shifting does
not increase the DS dimension of a concept class, then its density is upper bounded by twice the DS
dimension. We leave the impact of pivot shifting on DS dimension as another open question.
Throughout the paper, we use Nto denote the set of positive integers. For any nPN, we definerns:“
t1, . . . , nu. For any sets X,Y, sequence x“px1, . . . , x nqPXn, and function fPYX, we define
the subsequence x´i:“px1, . . . , x i´1, xi`1, . . . , x nqforiPrnsandf|x:“pfpx1q, . . . , fpxnqq.
The projection of a set FĎYXtoxis defined as F|x:“tf|x:fPFuĎYn.
Outline In Section 1.1, we introduce the problem of multiclass learning and review some existing
results. In Section 1.2, we summarize the key points of our theoretical results. In Section 2, we
introduce list learning, present the reduction from multiclass learning to list learning, and improve
the sample complexity upper bound of multiclass learning via this reduction together with a boosting
technique for list learners. In Section 3, we prove the optimal sample complexity for classes of
DS dimension 1, introduce the intuition and the definition of “pivot shifting”, and demonstrate its
potential application to the proof of the optimal sample complexity of multiclass learning.
1.1 Multiclass learning
In this section, we formally introduce the problem of multiclass learning [Valiant, 1984]. For any
distribution PoverXˆY, the error rate of a classifier hPYXunder Pis defined as
erPphq:“Pptpx, yqPXˆY:y‰hpxquq.
2In this paper, we focus on realizable distributions: for a concept class HĎYX, a distribution Pover
XˆYisH-realizable if infhPHerPphq“0. LetREpHqdenote the set of H-realizable distributions.
Besides,ppxi, yiqqn
i“1PpXˆYqnisH-realizable ifDhPHsuch that yi“hpxiq,@iPrns.
Definition 1.1 (Multiclass learner) .Amulticlass learner (or a learner) Ais an algorithm which given
a sequence sPY8
n“0pXˆYqnand a concept class HĎYX, outputs a classifier Aps,HqPYX.
Then, we can define multiclass PAC learning as follows.
Definition 1.2 (Multiclass PAC learning) .For any concept class HĎYX, the (PAC) sample
complexity MA,H:p0,1q2ÑNof a multiclass learner Ais a mapping from pε, δqPp0,1q2to
the smallest positive integer such that for any měMA,Hpε, δqand any distribution PPREpHq,
PS„PnperPpApS,Hqqąεqďδ, and we define MA,Hpε, δq“8 if no such integer exists. We say
HisPAC learnable byAifMA,Hpε, δqă8 for allpε, δqPp0,1q2. The (PAC) sample complexity
ofHis defined as MHpε, δq:“infAMA,Hpε, δqfor anypε, δqPp0,1q2.
Sometimes it is easier to analyze the expected error rate
εA,H,P:NÑr0,1s, nÞÑES„PnrerPpApS,Hqqs“PpS,pX,Yqq„Pn`1pY‰ApS,HqpXqq
for a learner Aand distribution PoverXˆY, or transductive error rate
εA,H,trans:NÑr0,1s, nÞÑ sup
s“ppx1,hpx1qq,...,pxn,hpxnqqqPpXˆYqn:hPH1
nřn
i“11hpxiq‰Aps´i,Hqpxiq.
We further define εA,H:“supPPREpHqεA,H,P,εH:“infAεA,H, and εH,trans:“infAεA,H,trans.
By a leave-one-out argument [Brukhim et al., 2022, Fact 14], we observe that εA,HďεA,H,trans.
Aden-Ali et al. [2023, Theorem 2.1] upper bounded the high probability error rate using the trans-
ductive error rate, which leads to a guarantee on PAC sample complexity. Based on their result, we
prove the same upper bound up to a multiplicative constant on the high probability error rate using
the expected error rate in Theorem 2.6.
Next, we define pseudo-cubes and DS dimensions of concept classes. Here, we also present their
extensions to the setting of k-list learning for future reference in Section 2.1.
Definition 1.3 (Pseudo-cube and k-pseudo-cube) .For any d, kPN, a class HĎYdis called a
k-pseudo-cube of dimension dif it is non-empty, finite, and for every hPHandiPrds, there exist
at least k i-neighbors of hinH, where gis ani-neighbor of hifgpiq‰hpiqandgpjq“hpjqfor all
jPrdsztiu. Apseudo-cube of dimension dis a1-pseudo-cube of dimension d.
Definition 1.4 (DS dimension and k-DS dimension, Charikar and Pabbaraju 2023) .For any d, kPN,
we say xPXdisk-DS shattered byHĎYXifH|xcontains a d-dimensional k-pseudo-cube. The
k-DS dimension dimkpHqofHis the maximum size of a k-DS shattered sequence. We say xisDS
shattered byHif it is 1-DS shattered by H. The DS dimension dimpHqofHis defined as dim1pHq.
Now, we introduce some existing results in multiclass learning. Brukhim et al. [2022] proved that
a class HĎYXis PAC learnable if and only if d:“dimpHqă8 , and there exists a multiclass
learner Awhich for any PPREpHq,δPp0,1q,nPN, and S„Pn, satisfies that with probability
at least 1´δ,
erPpApS,Hqq“O´
pd3{2logpdq`dlogplogpnqqqlog2pnq`logp1{δq
n¯
, (1)
which is also the best upper bound before this paper to our knowledge. In terms of lower bound, it
follows from Charikar and Pabbaraju [2023, Theorem 6] that εHpnq“Ωpd{nq. Thus, the current
upper and lower bounds of the expected error rate does not match. Moreover, a potentially sharp
lower bound on the sample complexity MHis still missing.
The learner Ain Brukhim et al. [2022] relies on orienting the one-inclusion graphs defined below as
a building block.
Definition 1.5 (One-inclusion graph, Haussler et al. 1994) .Theone-inclusion graph (OIG) of
HĎYnfornPNis a hypergraph GpHq“pH, Eqwhere His the vertex-set and Edenotes the
edge-set defined as follows. For any iPrnsandf:rnsztiuÑY, we define the set ei,f:“thPH:
hpjq“fpjq,@jPrnsztiuu. Then, the edge-set is defined as
E:“tpei,f, iq:iPrns, f:rnsztiuÑY, ei,f‰Hu .
For anypei,f, iq PEandhPH, we say hP pei,f, iqifhPei,fand the size of the edge is
|pei,f, iq|:“|ei,f|.
3Typically, we consider the one-inclusion graph of the projection of a concept class HĎYXto a
sequence xPXnwithnPN, i.e.,GpH|xq. The “density” of Hdiscussed in Section 3 is defined via
the “maximal average degree” (defined below) of the hypergraph GpH|xq.
Definition 1.6 (Degree and average degree) .For any hypergraph G“pV, EqandvPV, we define
thedegree ofvinGto be degpv;Gq:“|tePE:vPe,|e|ě2u|. When the underlying graph is
clear in the context, we simply write degpvqin abbreviation. If |V|ă8 , we can define the average
degree andaverage out-degree ofGto be
avgdegpGq:“1
|V|ř
vPVdegpv;Gq“1
|V|ř
ePE:|e|ě2|e|andavgoutdegpGq:“1
|V|ř
ePEp|e|´1q.
For general V, we can define the maximal average degree ofGto be mdpGq:“
supUĎV:|U|ă8avgdegpGrUsq, where GrUs “ p U, ErUsqdenotes the induced hypergraph of G
onUĎVwithErUs:“teXU:ePE, eXU‰Hu .
Note that for finite graphs, the average out-degree does not depend on the choice of orientation on G.
Moreover, since |e|ě2for all ePEn, we have
avgdegpGq“1
|V|ř
ePE:|e|ě2|e|ď1
|V|ř
ePE2p|e|´1q“2avgoutdegpGq. (2)
Now, we can define the density of a concept class as follows.
Definition 1.7. Thedensity ofHĎYXis defined as µHpmq:“supxPXmmdpGpH|xqq,@mPN.
1.2 Main results
In this section, we summarize the key points of our theoretical results. The full versions of some
results are stated in Section 2 and 3. We first need the following definition to rule out trivial concept
classes for which one training point suffices to achieve zero error rate under any realizable distribution.
Definition 1.8 (Nondegenerate concept class, Hanneke et al. 2023) .A concept class HPYXis
called nondegenerate if there exist h1, h2PHandx0, x1PXsuch that h1px0q “h2px0qand
h1px1q‰h2px1q.His called degenerate if it is not nondegenerate.
Our main result on the multiclass PAC sample complexity is as follows.
Theorem 1.9 (Partial summary of Theorem 2.5 and 2.11) .For any nondegenerate concept class
HĎYXwith dimpHq“dand anypε, δqPp0,1q2, we have
Ωppd`logp1{δqq{εqďMHpε, δqďOppd3{2logpdqlogpd{εq`logp1{δqq{εq. (3)
Our upper bound follows from a reduction to list learning and an improved sample complexity for
list learning summarized below.
Theorem 1.10 (Informal summary of Theorem 2.7 and 2.10) .Assume that there exists a list learner
which given a concept class HwithdimpHq“dand training sequence of size nPNoutputs a menu
of size ppH, nqwith expected error rate upper bounded by βpH, nq{nfor some functions pandβ
nondecreasing in n. Then, there exists a multiclass learner whose error rate is
OppβpH, nq`dlogpppH, nqq`logp1{δqq{nqwith probability at least 1´δ.
Moreover, there exists a list learner satisfying ppH, nq “O`
pe?
dq?
dlogpnq˘
andβpH, nq “
O`
d3{2logpdqlogpnq˘
.
We refer readers to Section 2.1 for detailed definitions regarding list learning. Note that if ppH, nq
andβpH, nqof some list learner is independent of n, there exists a multiclass learner with error rate
linear in 1{n. We leave the establishment of such list learners as Open Question 1.
In addition to the above approach, we propose an alternative route toward obtaining the conjectured
Θppd`logp1{δqq{εqsample complexity, by directly bounding the average degrees of one-inclusion
graphs. In particular, we show in Theorem 3.2 that any Hwith dimpHq “1hasMHpε, δq “
Θplogp1{δq{εq, which was not previously known. Moreover, we approach the general case via a new
technique we call “pivot shifting”. Specifically, we obtain the following result, which relies on an
assumption on such pivot shifting. The verification of this assumption is left as Open Question 2.
Proposition 1.11 (Informal summary of the results in Section 3) .Assume that for any finite concept
class, there exists a pivot shifting such that the DS dimension of the concept class does not increase
after the pivot shifting, then we have MHpε, δq“ΘppdimpHq`logp1{δqq{εqfor any HĎYX.
42 Multiclass learning via list learning
In this section, we prove a reduction from multiclass learning to list learning in Section 2.2. We
improve the existing list learners using boosting in Section 2.3. Then, using a boosted list learner and
the reduction, we improve the multiclass learning sample complexity upper bound in Section 2.4. We
first present some definitions and results of list learning in Section 2.1.
2.1 List learning
In list learning, the menus defined below serve as classifiers in multiclass learning.
Definition 2.1 (k-menu, Brukhim et al. 2022) .Amenu of size kPNis a function µ:XÑtYĎ
Y:|Y|ďku. A1-menu can be viewed as a classifier in YX, and vice versa.
For any distribution PoverXˆY, the error rate of a k-menu µunder Pis defined as erPpµq:“
Pptpx, yqPXˆY:yRµpxquqwhich agrees with the definition of the error rate of classifiers when
the size of the menu is 1.
Definition 2.2 (k-list learner) .Alist learner Aof size kPNis an algorithm which given a sequence
sPY8
n“0pXˆYqnand a concept class HĎYX, outputs a k-menu Aps,Hq. A1-list learner can
be viewed as a multiclass learner, and vice versa.
Similar to multiclass learners, the expected error rate of a list learner Ais defined as
εA,H,P:NÑr0,1s, nÞÑES„PnrerPpApS,Hqqs“PpS,pX,Yqq„Pn`1pYRApS,HqpXqq
for any concept class HPYXand distribution PoverXˆY. Restricting to realizable distributions,
we can define
εA,H:“sup
PPREpHqεA,H,P and εk
H:“ inf
k-list learners AεA,H.
Next, we define list PAC learning.
Definition 2.3 (List PAC learning, Charikar and Pabbaraju 2023) .For any concept class HĎYX
andkPN, the (PAC) sample complexity MA,H:p0,1q2ÑNof ak-list learner Ais a mapping
frompε, δqPp0,1q2to the smallest positive integer such that for every měMA,Hpε, δqand every
distribution PPREpHq,PS„PnperPpApS,Hqqą εqďδ, and we define MA,Hpε, δq“8 if no
such integer exists. We say Hisk-list PAC learnable byAifMA,Hpε, δqă8 for allpε, δqPp0,1q2.
Thek-list (PAC) sample complexity ofHis defined as Mk
Hpε, δq:“infk-list learner AMA,Hpε, δqfor
anypε, δqPp0,1q2.
Note that His PAC learnable by a learner Aif and only if His1-list learnable by A, and the PAC
sample complexity of HisMH“M1
H. For list PAC learning, it was proved by Charikar and
Pabbaraju [2023] that a concept class Hisk-list learnabale if and only if dk:“dimkpHqă8 , and
there exists a k-list learner Akwhich for any PPREpHq,δPp0,1q,nPN, and S„Pn, satisfies
that with probability at least 1´δ,
erPpAkpS,Hqq“O´
k6dkp?dklogpdkq`logpklogpnqqqlog2pnq`logp1{δq
n¯
. (4)
For the expected error rate, the lower bound εk
Hpnq“Ωpdk{pknqqhas been proved in Charikar and
Pabbaraju [2023, Theorem 6]. However, a lower bound of the same order on the k-list PAC sample
complexity is still missing in the literature. To establish a lower bound, we also need to rule out trivial
classes in list learning. In analogy to Definition 1.8, we define k-nondegeneracy as follows.
Definition 2.4 (k-nondegenerate concept class) .A concept class HPYXis called k-nondegenerate
forkPNif there exist h1, . . . , h k`1PHandx0, x1PXsuch that|thjpx0q:jPrk`1su|“ 1and
|thjpx1q:jPrk`1su|“ k`1.His called k-degenerate if it is not k-nondegenerate.
We claim that only one training point is sufficient for the k-list learning of a k-degenerate concept
classH. Indeed, Hisk-list learnable if |H|ďk. Now, suppose that |H|ěk`1and is k-degenerate.
Upon observing any point px1, y1qPXˆYrealizable by H, if|thPH:hpx1q“y1u|ďk, then
xÞÑthpxq:hPH, hpx1q“y1uis ak-menu which always contains the correct label. If |thPH:
hpx1q“y1u|ěk`1, then, for any xPXztx1u, we must have |thpxq:hPH, hpx1q“y1u|ďk
5because otherwise Hisk-nondegenerate. Then, xÞÑthpxq:hPH, hpx1q“y1uis ak-list which
always contains the correct label.
Now, we are ready to present the following lower bound on the k-list PAC sample complexity.
Theorem 2.5. For any kPN,k-nondegenerate concept class HĎYXwith dimkpHq“dkPN,
εP´
0,1
8pk`1q¯
, and δP´
0,1
4pk`1q¯
, we have Mk
Hpε, δqěpdk´1qlogp2q`4 logp1{δq
16pk`1qε. In particular,
when k“1, for any εPp0,1{16qandδPp0,1{8q, we have
MHpε, δqěpdimpHq´1qlogp2q`4 logp1{δq
32ε. (5)
The proof of Theorem 2.5 is presented in Appendix A where we construct hard distributions based on
properties of k-pseudo-cubes.
2.2 Reduction from multiclass learning to list learning
We first introduce the theorem that provides a guarantee on PAC sample complexity based on expected
error rate, which will be used frequently in our analysis.
Theorem 2.6. Fix a concept class HĎYXand consider a learner Awhich satisfies εA,H,Ppnqď
Mn{nfor any nPNandPPREpHqwithMnnondecreasing in the sample size n. Then, there
exists a learner A1such that for any PPREpHq,δP p0,1q,ně4, and the training sequence
S„Pn, with probability at least 1´δ, we have
erPpA1pS,Hqqď4.82¨p8.34Mtn{2u`logp2{δqq{n.
The proof of Theorem 2.6 is provided in Appendix B. Now, we consider general list learners whose
sizes may depend on the sample size. The theorem below states our reduction to list learning.
Theorem 2.7. Assume that there exists a list learner Alistwhich for any HĎYX,DPREpHq,
nPN, and S„Dn, outputs a menu AlistpS,Hqof size ppH, nqsatisfying εAlist,H,DďβpH, nq{n
for some function β: 2YXˆNÑr0,8q. Without loss of generality, we assume that ppH, nqand
βpH, nqare nondecreasing in n. Then, there exist multiclass learners Ared(see Algorithm 1) and
A1
redwhich for any concept class Hof DS dimension d,DPREpHq,δPp0,1q, and ně4, satisfy
εAred,H,Dpnq“OppβpH, n1q`dlogppH, n1qq{nq
where n1:“n´2tn{3u, and for S„Dn, with probability at least 1´δ,
erDpA1
redpS,Hqq“OppβpH, n1q`dlogppH, n1q`logp1{δqq{nq. (6)
The proof of Theorem 2.7 is presented in Appendix C. Note that the order of the error rate upper
bound of the constructed multiclass learner is not smaller than that of the original list learner in the
above theorem. Thus, the list learner Akof size kPNdeveloped in Charikar and Pabbaraju [2023]
cannot lead to an improved error rate of multiclass learning using our current result. The construction
ofAredfromAlistis shown in Algorithm 1.
Algorithm 1: Multiclass learner Aredusing a list learner Alist
Input: List learner Alist, concept class HĎYX, training sequence
S“ppx1, y1q, . . . ,pxn, ynqqPpXˆYqnforně3, test feature xn`1PX.
Output: A label yPYfor the feature xn`1.
1n1Ðn´2tn{3u,n2Ðtn{3u;
2S1Ðppxi, yiqqiPrn1s,S2Ðppxi, yiqqn
i“n1`1,x1Ðpxn1`1, . . . , x n, xn`1q;
3pµÐAlistpS1,Hq,NÐř
px,yqPS2 1yRpµpxq;
4Hx1Ðth|x1:hPH,|tiPrn`1szrn1s:hpxiqRpµpxiqu|ď N`1u;
5SamplepI1, . . . , I n2q„Unifpr2n2sqn2;
6phÐAGpT,Hx1qwhere TÐppIj, yIj`n1qqjPrn2s;
7return the labelphp2n2`1q.
In step 6 of Algorithm 1, AGis a multiclass learner defined in Proposition H.5 in the appendix.
Moreover, we prove in Proposition H.5 that for any DPREpHq,nPN,δPp0,1q, and S„Dn,
with probability at least 1´δ, we have
erDpAGpS,Hqq“OppdimGpHq`logp1{δqq{nq
6where dimGpHqis the graph dimension [Natarajan and Tadepalli, 1988] of H(see Definition H.1 in
the appendix). The above bound for classes of finite graph dimensions is also novel in the literature.
We briefly comment on the analysis of Algorithm 1. We first apply the list learner to the first third of
the training samples to obtain the menu pµ. Then, we count the number of errors ( N) made by pµin
the last two thirds samples. Then, we consider Hx1which is a subset of H|x1such that the number
of errors on x1is bounded by N`1. We show in Lemma H.9 that dimGpHx1qis well controlled.
However, as we do not observe the label of the test point, we can only consider resampling from
elements in S2as the new training sequence fed to AGtogether with the concept class Hx1. Thus,
there still exist great challenges of upper bounding the error probability for the test point that will
never be sampled. We need to emphasize that the standard leave-one-out argument [Brukhim et al.,
2022, Fact 14] cannot be directly applied as the definition of Nthat determines Hx1only depends on
S2but not the test point pXn`1, Yn`1q. We tackle this challenge by proving that some permutation
of the error event together with the constraint on correctness of pµon the last two points in x1when
leaving the last element (i.e., the test point) out is a subset of the error event when leaving the previous
element (i.e., the point in S2) out. The details of the proof is presented in Appendix C.
2.3 Sampled boosting of list learners
We now build a list learner whose invocation to Theorem 2.7 yields the upper bound in Theorem
1.9. Brukhim et al. [2022, Lemma 39] proposed a list sample compression scheme of size r“
Opd3{2logpnqqfor concept classes of DS dimension dand sample size n. One can show that its error
rate is Opprlogpn{rq`logp1{δqq{nqusing standard techniques for sample compression schemes
[David et al., 2016], which however brings the extra log factor logpn{rq. Recently, da Cunha
et al. [2024] proposed stable randomized sample compression schemes for binary classification
whose generalization does not induce the extra log factor in nand used this framework to analyze a
subsampling-based boosting algorithm for weak learners. Motivated by its success, we extend their
boosting algorithm [da Cunha et al., 2024, Algorithm 1] for multilclass list learners in Algorithm
2. Before presenting the algorithm, we first need to define the majority vote of menus. For KPN
menus µ1, . . . , µ Keach of size p, we define their majority vote to be µ“Majpµ1, . . . , µ kqwith
µpxq“Majpµ1, . . . , µ kqpxq:“tyPY:|tkPrKs:yPµkpxqu|ą K{2u,@xPX.
Note that µhas size 2p´1. Forp“1, the above definition recovers the majority vote of classifiers.
Algorithm 2: Sampled boosting Aboost of a list learner Alist
Input: List learner Alist, concept class HĎYX, training sequence
S“tpx1, y1q, . . . ,pxn, ynquPpXˆYqn,γPp0,1{2q,νPp0, γ{18s,δPp0,1q.
Output: Menu µ.
1fori“1, . . . , n do
2D1ptpxi, yiquqÐ 1{n;
3αÐ1
2logpp1`γq{p1´γqq,mÐMAlist,Hp1{2´γ, νq,KÐr4 logpn{δq{γs;
4fork“1, . . . , K do
5 Draw msamples Sk„Dm
k;
6 µkÐAlistpSk,Hq;
7 fori“1, . . . , n do
8 Dk`1ptpxi, yiquqÐ Dkptpxi, yiquqexp`
´α`
21yiPµkpxiq´1˘˘
;
9Dk`1ÐDk`1{`řn
i“1Dkptpxi, yiquqexp`
´α`
21yiPµkpxiq´1˘˘˘
;
10return µÐMaj`
pµkqkPrKs˘
.
Here, γandνare fixed constants, enabling us to invoke weak list learners (of constant error and
confidence levels) to Algorithm 2. Next, we upper bound the error rate of the boosted list learner.
Theorem 2.8. Assume that Alistis a list learner with MAlist,Hp1{2´γ, νq ă 8 for some γP
p0,1{2qandνPp0, γ{18s. Then, for any DPREpHq,nPN, and δą0, sampling S„Dn, with
probability at least 1´δ, the menu µproduced by Aboost usingAlistin Algorithm 2 satisfies that
erDpµq“O´MAlist,Hp1{2´γ,νqlogpn{δq
γn¯
.
7The proof of Theorem 2.8 is a generalization of the proof of da Cunha et al. [2024, Theorem 1.1] and
is presented in Appendix D together with the proofs of other results in this section. Since multiclass
learners are list learners of size 1, we can also boost multiclass learners using Algorithm 2. For
instance, invoking the multiclass learner in Brukhim et al. [2022, Theorem 1] to Algorithm 2 and
applying Theorem 2.6, we achieve the following sample complexity in multiclass learning.
Corollary 2.9. There exists a multiclass learner AwithεA,Hpnq “ O`d3{2log2pdqlogpnq
n˘
and
MA,Hpε, δq“O`d3{2log2pdqlogpd{εq`logp1{δq
ε˘
for any nPN,pε, δqPp0,1q2, andHĎYXwith
dimpHq“d.
There is an extra logpdqfactor in the above upper bound compared to that in Theorem 1.9, which
explains the reason of routing through list learning with our reduction in Theorem 2.7: the list
sample compression scheme in Brukhim et al. [2022] saves a logpdqfactor compared to their
sample compression scheme. Therefore, we invoke their list sample compression scheme as Alistto
Algorithm 2 and build a list learner whose error rate depends on only one log factor in both nandd.
Theorem 2.10. There exists a list learner ALwhich for any HĎYXwithdimpHq“dand sample
sizenPNoutputs a menu of size O`
pe?
dq?
dlogpnq˘
withεAL,Hpnq“O`d3{2logpdqlogpnq
n˘
.
2.4 Improved upper bounds on sample complexity
Applying the list learner ALin Theorem 2.10 to our reduction, Theorem 2.7 immediately implies the
following result.
Theorem 2.11. There exists a multiclass learner Amulti such that for any HĎYXof DS dimension
d,DPREpHq,δPp0,1q,něd`1, and S„Dn, with probability at least 1´δ, we have
erDpAmultipS,Hqq“O´
d3{2logpdqlogpnq`logp1{δq
n¯
, (7)
which implies that
MAmulti,Hpε, δq“O´
d3{2logpdqlogpd{εq`logp1{δq
ε¯
,@ε, δPp0,1q. (8)
Furthermore, if there exists a list learner Agoodlist of size f1pdqand expected error rate
εAgoodlist ,Hpnq ďf2pdq{nfor some functions f1:NÑNandf2:NÑ r0,8q, then, there
exists a multiclass learner Alinsuch that
MAlin,Hpε, δq“Oppdlogpf1pdqq`f2pdq`logp1{δqq{εq,@ε, δPp0,1q. (9)
The proof of Theorem 2.11 follows directly from Theorem 2.7 and Theorem 2.10 and is provided
in Appendix E. Moreover, observing that dimkpHq ědimk1pHqforkăk1, our requirement on
Agoodlist does not violate the lower bound in Charikar and Pabbaraju [2023, Theorem 6].
Compared to the upper bound (1)by Brukhim et al. [2022], (7)improves the dependence of the
error rate on the sample size nfromO`
logplogpnqqlog2pnq{n˘
toOplogpnq{nq, which steps further
towards the goal of Op1{nqexpected error rate (treating the DS dimension as a constant). Combining
(5)and(8), we arrive at (3)where the gap has been improved to the factor?
dlogpdqlogpd{εq.
However, we are not aware of any existing list learner satisfying the requirements of Agoodlist in
Theorem 2.11. Thus, we leave the construction of Agoodlist as an open question.
Open Question 1. Does there exist a list learner such that given a concept class HĎYX, its
size is f1pdimpHqqand its expected error rate is εAlist,Hpnq“f2pdimpHqq{nfor some functions
f1:NÑNandf2:NÑr0,8q?
Ideally, we would expect a list learner with size OpdimpHqqand expected error rate OpdimpHq{nq
as it immediately implies an upper bound MHpε, δq“OppdimpHqlogpdimpHqq`logp1{δqq{εq
which matches the lower bound in (5) up to the factor logpdimpHqq.
3 Density, DS dimension, and pivot shifting
We now introduce an alternative route toward proving the optimal sample complexity of multiclass
PAC learning: bounding the density µH:NÑr0,8q(Definition 1.7) of concept classes H. The
8following proposition summarizes existing results that illustrate the role of density in multiclass
learning.
Proposition 3.1 (Daniely and Shalev-Shwartz 2014, Charikar and Pabbaraju 2023, Aden-Ali et al.
2023) .For any HĎYXandnPN, we have
µHpnq{p2enqďεHďεH,transďµHpnq{n. (10)
Assume that µHpnqďfpdimpHqqfor some function f:NÑr0,8qand all nPN. Then, there
exists a learner Abased on orienting the one-clusion graph (Definition 1.5) of the projected concept
class (see Aden-Ali et al. [2023, Appendix A] for the formal definition of the algorithm) with sample
complexity MA,Hpε, δq“O`fpdimpHqq`logp1{δq
ε˘
for all ε, δPp0,1q.1
In Proposition 3.1, the first inequality of (10) follows from Charikar and Pabbaraju [2023, Theorem
6], the last inequality of (10) follows from Daniely and Shalev-Shwartz [2014, Theorem 2], and
the last paragraph follows from Aden-Ali et al. [2023, Theorem 2.2]. Thus, for sharper multiclass
sample complexity, it suffices to bound the density of a concept class with some functions of its
DS dimension. Furthermore, by Definition 1.7 and (2), it suffices to bound the average out-degree
(Definition 1.6) of finite one-inclusion graphs. In fact, it has been conjectured that µHpnqďc¨dimpHq
for some constant cą0[Daniely and Shalev-Shwartz, 2014] and the question remained open since
then. A positive resolution of this conjecture would immediately imply that the MHpε, δq “
ΘppdimpHq`logp1{δqq{nqby Proposition 3.1 and Theorem 2.5. It is worth mentioning that for
HĎt0,1uX, Haussler et al. [1994] proved that µHď2dimpHq(for binary classes, the DS dimension
is the VC dimension), which also motivates the above conjecture. In this paper, we confirm the above
conjecture for concept classes of DS dimension 1.
Theorem 3.2. For any HĎYXwithdimpHq“1, we have µHpnqď2,@nPN. Thus, MHpε, δq“
Θplogp1{δq{εqfor any positive ε, δPOp1qand any Hwith dimpHq“1.
The above theorem follows from the following fact we prove for one-inclusion graphs of DS dimension
1 concept classes. The proofs of Theorem 3.2 and Proposition 3.3 are presented in Appendix F.
Proposition 3.3. For any nPNandVnĎYnwith|Vn|ă8 anddimpVnq“1, there exists no
cycle (see Definition 3.4) in the one-inclusion graph GpVnq(see Definition 1.5).
Definition 3.4 (Cycle in finite hypergraph) .Acycle of length mPNzt1uin a finite hyergraph
G“pV, Eqconsists of pairwise different vertices v0, . . . , vm´1PVand pairwise different edges
e0, . . . , em´1PEsuch that vj, vpj`1qmod mPejfor all 0ďjďm´1.
We prove Proposition 3.3 by contradiction and analyzing different cases of the cycle. However, it is
hard to extend such result to classes of higher DS dimensions. For general concept classes, motivated
by the proof for binary classes [Haussler et al., 1994, Lemma 2.4], we also consider proving by
induction on the size of the sequence the class projects to. Though the analysis for binary classes
does not apply to general concept classes, we discover that the analysis in the induction step proceeds
seamlessly for some special concept classes where a common label which we call a “pivot” exists for
each edge in the last dimension of size greater than 1 in its one-inclusion graph. Before summarizing
this result in Lemma 3.6 below, we first introduce the definition of a “pivot” formally.
Definition 3.5 (Pivot of finite concept class) .For any nPNzt1uandVnĎYn, we define
PpVnq:“Y yPYYy1PYztyu␣
py1, . . . , y n´1qPYn´1:py1, . . . , y n´1, yq,py1, . . . , y n´1, y1qPVn(
.
Then, aPYis said to be a pivot ofVnifpy1, . . . , y n´1, aqPVnfor anypy1, . . . , y n´1qPPpVnq.
We emphasize that when PpVnq“H , every aPYis a pivot of Vn.
Then, we can present Lemma 3.6 whose proof is provided in Appendix G.
Lemma 3.6. Assume that for some nPNzt1u, anydPN, anymPrn´1s, and any HĎYmwith
dimpHqďdand|H|ă8 , we have avgoutdegpGpHqqď d. Consider an arbitrary set VnĎYn
such that|Vn|ă8 anddimpVnqďd. IfVnhas a pivot, then we have avgoutdegpGpVnqqďd.
1In Aden-Ali et al. [2023, Section 2.4], the label space considered is finite. However, extending the
compactness argument in Brukhim et al. [2022, Appendix B], we can prove that there exists an orientation of the
hypergraph with its maximum out-degree upper bounded by the ceiling of the density even when the graph is
infinite, which implies that the above sample complexity of the learner Astill holds for infinite label spaces.
9Though it only works for special classes, Lemma 3.6 can serve as a building block in the induction
step for the proof of avgoutdegpGpHqq ď dimpHqfor finite HĎ Y nPNYn. Moreover, the base
casen“d`1has been verified in Brukhim et al. [2022, Lemma 13]. Consequently, it suffices
to extend the induction step for concept classes without a pivot. With Lemma 3.6, it is natural to
consider modifying the concept class to create a pivot for it while at the same time preserving the
DS dimension of the modified class nonincreasing. The technique used here is similar to shifting
[Haussler, 1995, Brukhim et al., 2022], though we do not shift the whole edge “downwards” but only
shift the last label in some vertex of the edge to a candidate pivot. The difference is necessary, as
it has already been shown that the DS dimension of a concept class can increase after the standard
shifting [Brukhim et al., 2022, Example 19]. Thus, we name the technique used here “pivot shifting”.
Definition 3.7 (Pivot shifting) .For any nPNzt1u,aPY, and VnĎYnwith|Vn|ă8 , we define
PapVnq:“Y yPY␣
py1, . . . , y n´1qPYn´1:py1, . . . , y n´1, yqPVn,py1, . . . , y n´1, aqRVn(
.
For any y“py1, . . . , y n´1qPPapVnqand the edgepen,y, nqinGpVnq, we define the set
Ly:“tyPY:py1, . . . , y n´1, yqPpen,y, nqu.
A mapping γ:PapVnqÑYis called a pivot shifting onVntoaifγpyqPLyfor all yPPapVnq.
LetΓa,Vndenote the set of all pivot shifting on Vntoa. For any γPΓa,Vn, we define
Vγ
n:“pVnztpy, γpyqq:yPPapVnquqYtp y, aq:yPPapVnqu;
i.e.,Vn,γis obtained by replacing the label γpyqinpy, γpyqqwithafor all yPPapVnq.
We prove that the average out-degree does not decrease after pivot shiftings in the following lemma.
Lemma 3.8. For any aPY,VĎY8
n“2Ynwith|V|ă8 , and γPΓa,V, we have
avgoutdegpGpVγqqě avgoutdegpGpVqq.
The proof is presented in Appendix G. A key observation for the proof is that by definition, only
edges of sizes greater than one contribute to the average out-degree. However, we are not able to
show that the DS dimension does not increase after some pivot shifting, which we leave as an open
question. Thus, whether pivot shifting is applicable to upper bounding density with DS dimension
remains open.
Open Question 2. For any dPNand any VĎY8
n“d`2Ynwith|V|ă8 anddimpVq“d, are
there some aPYandγPΓa,Vsuch that dimpVγqďd?
Nevertheless, we have taken a further and specific step toward the verification of the conjecture
thatµHď2dimpHq: a positive resolution of the above question would lead to the conclusion that
µHď2dimpHqby Lemma 3.6, Lemma 3.8, and Brukhim et al. [2022, Lemma 13].
Acknowledgments and Disclosure of Funding
Shay Moran is a Robert J. Shillman Fellow; he acknowledges support by ISF grant 1225/20, by BSF
grant 2018385, by Israel PBC-V ATAT, by the Technion Center for Machine Learning and Intelligent
Systems (MLIS), and by the the European Union (ERC, GENERALIZATION, 101039692). Views
and opinions expressed are however those of the author(s) only and do not necessarily reflect those
of the European Union or the European Research Council Executive Agency. Neither the European
Union nor the granting authority can be held responsible for them.
References
I. Aden-Ali, Y . Cherapanamjeri, A. Shetty, and N. Zhivotovskiy. Optimal pac bounds without
uniform convergence. In 2023 IEEE 64th Annual Symposium on Foundations of Computer
Science (FOCS) , pages 1203–1223, Los Alamitos, CA, USA, nov 2023. IEEE Computer Society.
doi: 10.1109/FOCS57990.2023.00071. URL https://doi.ieeecomputersociety.org/10.
1109/FOCS57990.2023.00071 .
S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. Long. Characterizations of learnability for classes
oft0, . . . , nu-valued functions. Journal of Computer and System Sciences , 50:74–86, 1995.
10S. Bendavid, N. Cesabianchi, D. Haussler, and P.M. Long. Characterizations of learnability for classes
of {0, ..., n}-valued functions. Journal of Computer and System Sciences , 50(1):74–86, 1995. ISSN
0022-0000. doi: https://doi.org/10.1006/jcss.1995.1008. URL https://www.sciencedirect.
com/science/article/pii/S0022000085710082 .
Nataly Brukhim, Daniel Carmon, Irit Dinur, Shay Moran, and Amir Yehudayoff. A characterization
of multiclass learnability. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer
Science (FOCS) , pages 943–955. IEEE, 2022.
Moses Charikar and Chirag Pabbaraju. A characterization of list learnability. In Proceedings of
the 55th Annual ACM Symposium on Theory of Computing , STOC 2023, page 1713–1726, New
York, NY , USA, 2023. Association for Computing Machinery. ISBN 9781450399135. doi:
10.1145/3564246.3585190. URL https://doi.org/10.1145/3564246.3585190 .
Arthur da Cunha, Kasper Green Larsen, and Martin Ritzert. Boosting, voting classifiers and random-
ized sample compression schemes. arXiv preprint arXiv:2402.02976 , 2024.
Amit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems. In Conference on
Learning Theory , pages 287–316. PMLR, 2014.
Ofir David, Shay Moran, and Amir Yehudayoff. Supervised learning through the lens of
compression. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, edi-
tors, Advances in Neural Information Processing Systems , volume 29. Curran Associates,
Inc., 2016. URL https://proceedings.neurips.cc/paper_files/paper/2016/file/
59f51fd6937412b7e56ded1ea2470c25-Paper.pdf .
S. Hanneke. The optimal sample complexity of PAC learning. Journal of Machine Learning Research ,
17(38):1–15, 2016.
Steve Hanneke, Shay Moran, and Qian Zhang. Universal rates for multiclass learning. In The Thirty
Sixth Annual Conference on Learning Theory , pages 5615–5681. PMLR, 2023.
D. Haussler. Sphere packing numbers for subsets of the Boolean n-cube with bounded Vapnik-
Chervonenkis dimension. Journal of Combinatorial Theory A , 69(2):217–232, 1995.
D. Haussler, N. Littlestone, and M. Warmuth. Predicting t0,1u-functions on randomly drawn points.
Information and Computation , 115(2):248–292, 1994.
Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura Barnes, and
Donald Brown. Text classification algorithms: A survey. Information , 10(4):150, 2019.
Tao Li, Chengliang Zhang, and Mitsunori Ogihara. A comparative study of feature selection and
multiclass classification methods for tissue classification based on gene expression. Bioinformatics ,
20(15):2429–2437, 2004.
B. K. Natarajan. On learning sets and functions. Machine Learning , 4:67–97, 1989.
Balas K Natarajan and Prasad Tadepalli. Two new frameworks for learning. In Machine Learning
Proceedings 1988 , pages 402–415. Elsevier, 1988.
Waseem Rawat and Zenghui Wang. Deep convolutional neural networks for image classification: A
comprehensive review. Neural Computation , 29(9):2352–2449, 2017. doi: 10.1162/neco_a_00990.
L. G. Valiant. A theory of the learnable. Communications of the ACM , 27(11):1134–1142, November
1984.
V . Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to
their probabilities. Proc. USSR Acad. Sci. , 181(4):781–783, 1968.
11A Lower bound
Before proving Theorem 2.5, we first introduce two lemmas regarding k-pseudo-cubes that will be
used in the proof.
Lemma A.1. For any positive integers k, d, any label class Y, any k-pseudo-cube HĎYdof
dimension d, any jPrds, and any label yPY, define Hj
y:“thPH:hpjq“yu. Then, we have
|Hj
y|ď|H|
k`1.
Proof. When d“1, the result follows trivially from the definition of k-pseudo-cubes. We prove the
result for dě2by contradiction. Suppose on the contrary that there exist some jPrdsandyPY
such that|Hj
y|ą|H|
k`1. The definition of pseudo-cubes implies that |H|ěk`1. Then, there exist
h, h1PHj
ywithh‰h1. Lettf1, . . . , f kuandtf1
1, . . . , f1
kudenote the set of j-neighbors of hand
h1inHrespectively. Since h‰h1andhpjq“y“h1pjq, there exists some j1Prdsztjusuch that
hpj1q‰h1pj1q. It follows that fipj1q“hpj1q‰h1pj1q“f1
lpj1qand thus fi‰f1
lfor any i, lPrks.
Then, we have
|thPH:hpjq‰yu|ěk|Hj
y|ąk|H|
k`1
and
|H|“|t hPH:hpjq‰yu|`| Hj
y|ą|H|,
which is a contradiction. Thus, we must have |Hj
y|ď|H|
k`1.
Lemma A.2. For any integer kě1,dě2,nPrd´1s, and 1ďj1ă¨¨¨ă jnďd, any label class
Y, any k-pseudo-cube HĎYdof dimension d, and any hypothesis gPH, define J:“pj1, . . . , j nq
andK“pk1, . . . , k d´nqsuch that 1ďk1ă¨¨¨ă kd´nďdandtj1, . . . , j n, k1, . . . , k d´nu“rds.
Then, Hg,J:“th|K:hPH, hpjiq“gpjiq,@iPrnsuis ak-pseudo-cube of dimension d´n.
Proof. For any fPHg,J, there exists some hPHsuch that f“h|K. Then, for any iPrd´ns, there
exists kdistinct h1, . . . , h kPHsuch that hmpkiq‰hpkiqandhmplq“hplqfor all lPrdsztkiuand
mPrks. Since kiRtj1, . . . , j nu, we have h|J“hm|J“g|Jand thus fm:“hm|KPHg,Jfor all
mPrks. Then, we have fpiq“hpkiq‰hmpkiq“fmpiqandfplq“hpklq“hmpklq“fmplqfor
anylPrd´nsztiuandmPrks, which implies that Hg,Jis ak-pseudo-cube of dimension d´n.
Proof of Theorem 2.5. Consider an arbitrary k-nondegenerate concept class HĎYXfor some kPN.
Letx0, x1PXandh1, . . . , h k`1PHwitness the k-nondegeneracy as specified in Definition 2.4.
For any εPp0,1{2pk`1qqandδPp0,1q, letI„Bernoullippk`1qεqandJ„Unifprk`1sq. Then,
for any jPrk`1s,pxI, hjpxIqqfollows the H-realizable distribution Pε,joverXˆYdefined by
Pε,jptpx0, hjpx0qquq“ 1´pk`1qεandPε,jptpx1, hjpx1qquq“p k`1qε. SamplepI1, . . . , I nq„
Bernoullipεqnindependent of pI, Jqand define S“ppxI1, hJpxI1q, . . . ,pxIn, hJpxInqqandS1“
ppx0, hJpx0q, . . . ,px0, hJpx0qq. Then, for any k-list learner A, we have
PphJpxIqRApS,HqpxIq|I1“0, . . . , I n“0q
ěPphJpx1qRApS1,Hqpx1q, I“1q
“PpI“1qPphJpx1qRApS1,Hqpx1qq
“pk`1qεErPphJpx1qRApS1,Hqpx1q|ApS1,Hqqs
ěε,
where the last inequality follows from the facts that hJpx0q“h1px0q,|th1px1q, . . . , h k`1px1qu|“
k`1,hJpx1q„Unifpth1px1q, . . . , h k`1px1quq. Since J„Unifprk`1sq, there exists jPrk`1s
such that
PphjpxIqRApSj,HqpxIq|I1“0, . . . , I n“0q
“PphJpxIqRApS,HqpxIq|J“j, I1“0, . . . , I n“0qěε,
12where Sj:“ ppxI1, hjpxI1qq, . . . ,pxIn, hjpxInqqq „ Pn
ε,jis independent of pxI, hjpxIqq „ Pε,j.
Since1
2pk`1qεď1´pk`1qε
pk`1qεă1
´logp1´pk`1qεqforεP p0,1{2pk`1qq, ifnďlogp1{δq
2pk`1qεă
logpδq
logp1´pk`1qεq, we have
PpI1“0, . . . , I n“0q“p1´pk`1qεqnąδ.
Then, with probability greater than δ, we have erPε,jpApSj,Hqqěε, which implies that
Mk
Hpε, δqělogp1{δq
2pk`1qε.
Next, consider an arbitrary concept class HĎYXsuch that dimkpHq “dkPNzt1ufor some
kPN. Then, there exist a sequence x:“px1, . . . , x dkqPXdkand a k-pseudo-cube HĎH|xof
dimension dk. Denote the elements in Hwith y1“py1
1, . . . , y1
dkq, . . . , y|H|“py|H|
1, . . . , y|H|
dkq. For
anyεPp0,1{8pk`1qq, consider the categorical distribution Qεoverrdksdefined by Qεpt1uq“
1´4pk`1qεandQεptiuq“4pk`1qε
dk´1foriPrdkszt1u. LetJ„Unifpr|H|sq. For any iPrdkszt1uand
I“pi1, . . . , i mqPpr dksztiuqmwithmPrdk´1sandi1ă¨¨¨ă im, define I1:“pi1
1, . . . , i1
dk´mq
such that i1
1ă¨¨¨ă i1
dk´mandti1, . . . , i m, i1
1, . . . , i1
dk´mu“rdks. Then, we have that conditional
onpi1, yJ
i1q, . . . ,pim, yJ
imq,yJ|I1follows the uniform distribution over the set HyJ,Iwhich is a
k-pseudo-cube by Lemma A.2. Consequently, we can apply Lemma A.1 to conclude that
|pHyJ,Iqi1
y|ď|HyJ,I|
k`1
for any yPYandi1Prdk´ms, which immediately implies that
PpyJ
iPtv1, . . . , v ku|pi1, yJ
i1q, . . . ,pim, yJ
imqqďk
k`1. (11)
for any distinct v1, . . . , v kPY.
LetpI, I1, . . . , I nq„Qn`1
εfornPNbe independent of J. Define S:“ppxI1, yJ
I1q, . . . ,pxIn, yJ
Inqq.
For any k-list leaner A, by (11), we have
PpyJ
IRApS,HqpxIq, I‰1q
ěPpyJ
IRApS,HqpxIq, I‰1, I‰I1, . . . , I‰Inq
ědkÿ
i“2PpyJ
IRApS,HqpxIq, I“i, I1‰i, . . . , I n‰iq
“dkÿ
i“2Er 1I1‰i,...,I n‰iPpyJ
iRApS,Hqpxiq, I“i|ApS,Hq,pI1, yJ
I1q, . . . ,pIn, yJ
Inqqs
ědkÿ
i“2PpI“iq
k`1PpI1‰i, . . . , I n‰iq
“4εˆ
1´4pk`1qε
dk´1˙n
.
Since J„Unifpr|H|sq, there exists j“jpA,HqPr|H|ssuch that
Ppyj
IRApSj,HqpxIq, I‰1q“PpyJ
IRApS,HqpxIq, I‰1|J“jqě4εˆ
1´4pk`1qε
dk´1˙n
,
where Sj:“ppxI1, yj
I1q, . . . ,pxIn, yj
Inqq. Note that if we define the distribution Pε,joverXˆYby
Pε,jptx1, yj
1uq“ 1´4pk`1qεand
Pε,jptxi, yj
iuq“4pk`1qε
dk´1,@iPrdkszt1u,
13then, we have pS,pxI, yj
Iqq„ Pn`1
ε,j. For any nďpdk´1qlogp2q
8pk`1qεălogp1{2q
logp1´4pk`1qε{pdk´1qq, we have´
1´4pk`1qε
dk´1¯n
ą1
2and
Ppyj
IRApSj,HqpxIq, I‰1qą2ε.
Now, we define the algorithm A1by
A1ps,Hqpx1q:“tyj
1uandA1ps,Hqpxq:“Aps,Hqpxq,@xPXztx1u
for any sPY8
m“0pXˆYqm. Then, we have
ErerPε,jpA1pSj,HqqsěPpyj
IRA1pSj,HqpxIq, I‰1q
“Ppyj
IRApSj,HqpxIq, I‰1q
ą2ε. (12)
On the other hand, the definition of A1yields that yj
1PA1pSj,px1qqand hence
erPε,jpA1pSj,HqqďPε,jpXztx1uq“ 4pk`1qε.
Suppose the following holds
PperPε,jpApSj,Hqqąεqď1
4pk`1q. (13)
Since erPε,jpA1pSj,HqqďerPε,jpApSj,Hqq, the above inequality implies that
PperPε,jpA1pSj,Hqqąεqď1
4pk`1q
and therefore
ErerPε,jpA1pSj,Hqqsďε`E”
erPε,jpA1pSj,Hqq 1erPε,jpA1pSj,Hqqąεı
ďε`4pk`1qεPperPε,jpA1pSj,Hqąεq
ď2ε,
which contradicts (12). Thus, we can conclude that (13) is false, i.e.,
PperPε,jpApSj,Hqqąεqą1
4pk`1q
fornďpdk´1qlogp2q
8pk`1qε. For δPp0,1{4pk`1qs, we have PperPε,jpApSj,Hqqąεqąδand thus
Mk
Hpε, δqěpdk´1qlogp2q
8pk`1qε.
In conclusion, for ε, δ“Op1{kq, we have
Mk
Hpε, δq“Ωˆdk`logp1{δq
kε˙
.
B Proof of Theorem 2.6
Proof. Consider an algorithm A:pY8
k“1pXˆYqkqˆ 2YXÑYXwhich for any hypothesis class
HĎYX,H-realizable distribution D, and nPN, satisfies that
PpT,pX,Yqq„Dn`1pApT,HqpXq‰YqďMn
n(14)
where Mnis nondecreasing in the sample size n.
14For an arbitrary sequence S“ppx1, y1q, . . . ,pxn, ynq,pxn`1, yn`1qqPpXˆYqn`1, define S´ito
be the subsequence of Sexcludingpxi, yiqfor any iPrn`1s. LetpTm,pX, Yqq„ UnifpSqm`1
where mPN,TmP pXˆYqmandpX, Yq PXˆY. Moreover, for any iP rn`1s, let
Ti
m„UnifpS´iqm. Then, for any algorithm A:Y8
k“1pXˆYqkˆ 2YXÑYXand concept class
HĎYX, we have
PpApTm,HqpXq‰Yq“1
n`1n`1ÿ
i“1PpApTm,Hqpxiq‰yiq
ě1
n`1n`1ÿ
i“1E“
1pxi,yiqRTm 1ApTm,Hqpxiq‰yi‰
“1
n`1n`1ÿ
i“1Pppxi, yiqRTmqPpApTm,Hqpxiq‰yi|pxi, yiqRTmq
ěn`1´m
n`11
n`1n`1ÿ
i“1PpApTm,Hqpxiq‰yi|pxi, yiqRTmq
“n`1´m
n`11
n`1n`1ÿ
i“1PpApTi
m,Hqpxiq‰yiq. (15)
Note that if Sis consistent with H, then UnifpSqisH-realizable.
Next, define the algorithm AMaj,m:pY8
k“1pXˆYqkqˆ 2YXÑYXby
AMaj,mpR,Hq:“MajorityppApr,HqqrPRmq,@RPpXˆYqn,@nPN.
By the definition above, for any permutation RπofRPpXˆYqn, we have
AMaj,mpRπ,Hq“AMaj,mpR,Hq. (16)
Moreover, for any iPrn`1s, the above definition yields that
1AMaj,mpS´i,Hqpxiq‰yiď 11
nm|tsPSm
´i:Aps,Hqpxiq‰yiu|ě1
2
ď2
nm|tsPSm
´i:Aps,Hqpxiq‰yiu|
“2PpApTi
m,Hqpxiq‰yiq.
Then, by (15), we have
1
n`1n`1ÿ
i“11AMaj,mpS´i,Hqpxiq‰yiď2pn`1q
n`1´mPpApTm.HqpXq‰Yq.
Choosing m“tpn`1q{2uand defining AMaj:“AMaj,tpn`1q{2u, for any H-realizable sequence S,
by (14) and the above results, we have
n`1ÿ
i“11AMajpS´i,Hqpxiq‰yiď2pn`1q2Mtpn`1q{2u
pn`1´tpn`1q{2uqtpn`1q{2uď8.34Mtpn`1q{2u (17)
for any ně4.(16) and(17) imply that the algorithm AMajsatisfies Assumptions 2.1 and 2.3 in
Aden-Ali et al. [2023].
Now, we can define the randomized algorithm ARanwhich given a sample SPpXˆYqnwithnPN
and a concept class H, outputs the classifier AMajpS,Hqifnď3and outputs a random classifier
following the uniform distribution over the sequence pAMajpSďt,Hqqtn{4uďtď4tn{4u´1ifně4, i.e.,
PpARanpS,Hq“AMajpSďt,Hqq“1
3tn{4u,@tPrtn{4u,4tn{4u´1s,
where Sďtdenotes the subsequence of Sconsisting of the first telements in S. Then, by Aden-Ali
et al. [2023, Theorem 2.1], for any ně4,H-realizable distribution D, and confidence parameter
15δPp0,1q, given a training sample S„Dn, we have
erDpARanpS,Hqq“1
3tn{4u4tn{4u´1ÿ
t“tn{4uerDpAMajpSďt,Hqq
ď4.82ˆ8.34Mtn{2u
n`1
nlogˆ2
δ˙˙
with probability at least 1´δ.
C Proof of Theorem 2.7
The following proof requires several lemmas from Appendix H.
Proof of Theorem 2.7. Assume that we have access to a list learning algorithm Alistwhich for any
concept class HĎYXof DS dimension d, anyH-realizable distribution D, any nPN, and
pS,pX, Yqq„Dn`1, outputs a menu AlistpS,Hqof size ppnqPNsatisfying
PpYRAlistpS,HqpXqqďβpH, nq
n
for some function β: 2YXˆNÑr0,8q.
Forn1, n2PN, letpS1, S2,pX, Yqq„Dn1`2n2`1where S1PpXˆYqn1,S2PpXˆYq2n2, and
pX, YqPXˆY. Letpµ“AlistpS1,Hq. According to the property of Alist, the size of pµisppn1q
and we have
PpYRpµpXqqďβpH, n1q
n1.
For notational convenience, define S1:“pS2,pX, Yqq,S2:“pS1, S2,pX, Yqq, and enumerate the
elements of S1as
ppX1, Y1q, . . . ,pX2n2`1, Y2n2`1qq
wherepX2n2`1, Y2n2`1qdenotespX, Yq. We now define
N:“ÿ
iPr2n2s1YiRpµpXiq
and
HS1:“HpX1,...,X 2n2`1q,pµ,N
“!
h|pX1,...,X 2n2`1q:hPH,|tiPr2n2`1s:hpXiqRpµpXiqu|ď N`1)
.
It follows from the property of pµand Lemma H.9 that
ErNs“2n2βpH, n1q
n1(18)
and, conditional on S1,
dimGpHS1qďp2 log2e`4qp5dlog2pppn1qq`2N`2q.
SamplepI, Iq„Unifpr2n2`1sqn2`1independent of S2where I“pI1, . . . , I n2qPr2n2`1sn2
andIPr2n2`1s. Then, we define the sequence TpS1,Iq:“ppI1, YI1q, . . . ,pIn2, YIn2qqand the
classifier
phS1,I:“AG`
TpS1,Iq,HS1˘
for any iPr2n2`1s, where AGis the algorithm specified in Proposition H.5. Since conditional on
S2, the distribution of each element in TpS1,IqisHS1-realizable, by Corollary H.6, there exists some
constant C1ą0such that
PpphS1,IpIq‰YI|S2qďC1dimGpHS1q
n2ďC1dlogpppn1qq`C2N`C3
n2. (19)
16for some constant C1, C2, C3ą0.
For I1“ pI1
1, . . . , I1
n2q „ Unifpr2n2sqn2independent of all other random variables, define
TipS1,I1q:“ pp ρipI1
1q, YρipI1
1qq, . . . ,pρipI1
n2q, YρipI1n2qqqwhere ρi:r2n2s Ñ r 2n2`1sztiu,
kÞÑk 1kăi`pk`1q 1kěifor all iPr2n2`1s. Consider the classifier
rhS1,I1,i:“AGpTipS1,I1q,HS1q
for each iPr2n2`1s. Since Yiis not used in the construction of rhS1,I1,i, we can also denote rhS1,I1,i
asrhS1
´i,Xi,I1,i, where S1
´i:“ ppX1, Y1q, . . . ,pXi´1, Yi´1q,pXi`1, Yi`1q, . . . ,pX2n2`1, Y2n2`1qq.
Thus, treating S:“ pS1, S2q „Dn1`2n2as the training sample, we can define the classifier
phSPYXby
phSpxq:“rhS2,x,I1,2n2`1p2n2`1q,
where we emphasize that the RHS depends on S1through the construction of pµ. Then, recalling that
pX, Yq„Dis independent of S, our task is to upper bound the expectated error rate of phS:
P´
phSpXq‰Y¯
.
We first relate phS1,ItorhS1,I1,iforiPr2n2`1s:
PpphS1,IpIq‰YI|S2qě1
2n2`12n2`1ÿ
i“1E”
1iRI 1phS1,Ipiq‰Yi|S2ı
“1
2n2`12n2`1ÿ
i“1PpiRIqE”
1phS1,Ipiq‰Yi|S2, iRIı
ě1
2p2n2`1q2n2`1ÿ
i“1E”
1rhS1,I1,ipiq‰Yi|S2ı
,
which implies that
PpphS1,IpIq‰YI|S1qě1
2p2n2`1q2n2`1ÿ
i“1E”
1rhS1,I1,ipiq‰Yi|S1ı
“1
2p2n2`1q2n2`1ÿ
i“1E”
E”
1rhS1,I1,ipiq‰Yi|S1,I1ıˇˇˇS1ı
.
Conditional on I1andS1, for any iPr2n2`1sand any sequence sPpXˆYq2n2`1, we letrhs,I1,i
denote the classifier when replacing S1with sinrhS1,I1,i, i.e.,rhs,I1,i“AGpTips,I1q,Hsq. For any
iPr2n2`1s, define the set
Bi:“!
s“ppx1, y1q, . . . ,px2n2`1, y2n2`1qqPpXˆYq2n2`1:rhs,I1,ipiq‰yi)
and the permutation
πi:pXˆYq2n2`1ÑpXˆYq2n2`1,pz1, . . . , z 2n2`1qÞÑp z1, . . . , z i´1, z2n2`1, zi, . . . , z 2n2q.
We also define the set
B:“!
s“ppx1, y1q, . . . ,px2n2`1, y2n2`1qqPpXˆYq2n2`1:
y2n2Ppµpx2n2q, y2n2`1Ppµpx2n2`1q,andrhs,I1,2n2`1p2n2`1q‰y2n2`1)
.
We would like to show that πipBq Ď Bifor all iP r2n2s. For any s“
ppx1, y1q, . . . ,px2n2`1, y2n2`1qqPB, we letpxi
j, yi
jqPXˆYdenote the j-th element of πipsqfor
eachjPr2n2`1s. By the definition of πiandTi, we have
T2n2`1ps,I1q“pp I1
1, yI1
1q, . . . ,pI1
n2, yI1n2qqand
Tipπipsq,I1q“pp ρipI1
1q, yI1
1q, . . . ,pρipI1
n2q, yI1n2qq.
17Define
ns:“ÿ
jPr2n2s1yjRpµpxjq.
Since y2n2Ppµpx2n2qandy2n2`1Ppµpx2n2`1q, we have
nπipsq:“ÿ
jPr2n2s1yi
jRpµpxi
jq“ÿ
jPr2n2`1szt2n2u1yjRpµpxjq“ns
and therefore,
Hπipsq“Hpxi
1,...,xi
2n2`1q,pµ,nπipsq
“Hpxi
1,...,xi
2n2`1q,pµ,n s
“!
h|px1,...,x i´1,x2n2`1,xi,...,x 2n2q:hPH,|tjPr2n2`1s:hpxjqRpµpxjqu|ď ns`1)
“tκiphq:hPHsu
where for any hPY2n2`1,κiphqPY2n2`1is defined by
κiphqpjq:“hpρ´1
ipjqq 1j‰i`hp2n2`1q 1j“i,@jPr2n2`1s,
i.e.,hpjq“κiphqpρipiqqforjPr2n2sandhp2n2`1q“κiphqpiq. Note that κiis a bijection from
HstoHπipsq. Thus, for any hPHπipsq, we have
hpρipI1
jqq“κ´1
iphqpI1
jq,@jPrn2s,andhpiq“κ´1
iphqp2n2`1q.
Similarly, for any hPHs, we have
hpI1
jq“κiphqpρipI1
jqq,@jPrn2s,andhp2n2`1q“κiphqpiq.
Given the above analysis, we have
rhs,I1,2n2`1p2n2`1q“AGpT2n2`1ps,I1q,Hsqp2n2`1q
“AGpTipπipsq,I1q,Hπipsqqpiq
“rhπipsq,I1,ipiq,
which immediately implies that
rhπipsq,I1,ipiq“rhs,I1,2n2`1p2n2`1q‰y2n2`1“yi
i
and thus πipsqPBi. Since iPr2n2sandsPBis arbitrary, we have πipBqĎBifor any iPr2n2s.
Then, conditional on I1andS1, we have
D2n2`1pBiqěD2n2`1pπipBqq“D2n2`1pBq
Since BĎB2n2`1holds trivially, we have
PpphS1,IpIq‰YI|S1qě1
2p2n2`1q2n2`1ÿ
i“1E”
E”
1rhS1,I1,ipiq‰Yi|S1,I1ıˇˇˇS1ı
ě1
2E”
1Y2n2PpµpX2n2q 1Y2n2`1PpµpX2n2`1q 1rhS1,I1,2n2`1p2n2`1q‰Y2n2`1ˇˇˇS1ı
.
Taking expectation on both sides and applying (18) and (19), we have
C1dlogppn1q`2C2βpH, n1qn2{n1`C3
n2
“C1dlogppn1q`C2ErNs`C3
n2
ěPpphS1,IpIq‰YIq
ě1
2E”
1Y2n2PpµpX2n2q 1Y2n2`1PpµpX2n2`1q 1rhS1,I1,2n2`1p2n2`1q‰Y2n2`1ı
18which leads to
P´
phSpXq‰Y¯
“P´
rhS1,I1,2n2`1p2n2`1q‰Y2n2`1¯
ďE”
1´ 1Y2n2PpµpX2n2q 1Y2n2`1PpµpX2n2`1qı
`E”
1Y2n2PpµpX2n2q 1Y2n2`1PpµpX2n2`1q 1rhS1,I1,2n2`1p2n2`1q‰Y2n2`1ı
ďPpY2n2RpµpX2n2qq`PpY2n2`1RpµpX2n2`1qq
`2pC1dlogppn1q`2C2βpH, n1qn2{n1`C3q
n2
ď2pβpH, n1qn2{n1`C1dlogppn1q`2C2βpH, n1qn2{n1`C3q
n2.
Now, for any nPNsuch that ně3, setting n1“n´2tn{3u,n2“tn{3u,pS,pX, Yqq„Dn`1
withSPpXˆYqnandpX, YqPXˆY, we have
P´
phSpXq‰Y¯
“OˆβpH, n1q`dlogppn1q
n˙
.
Then, when βpH, nqandppnqare nondecreasing in n, by Theorem 2.6, there exists a learner A1such
that for any ně4andδPp0,1q, with probability at least 1´δoverS„Dn, we have
erDpA1pS,Hq“OˆβpH, n1q`dlogppn1q`logp1{δq
n˙
.
D Proofs of the results in Section 2.3
In this section, we provide the proofs of Theorem 2.8, Corollary 2.9, and Theorem 2.10 in Section 2.3.
Proof of Theorem 2.8. Assume that Alistsatisfies that MAlist,Hp1{2´γ, νq ă 8 for some γP
p0,1{2qandνPp0, γ{18s. Define the random variable Jk:“1erDkpµkqą1{2´γfor any kPrKs.
Then, we have
ErJks“PpJk“1qďν.
Define the event E:“!řK
k“1Jkě2νK)
. By the multiplicative Chernoff bound, we have
PpEq“P˜Kÿ
k“1Jkě2νK¸
ďe´νK{3ďδ
asKě3 logp1{δq
ν. Define Zk:“řn
i“1Dkptpxi, yiquqexp`
´α`
21yiPµkpxiq´1˘˘
for all kPrKs.
SinceDK`1is a probability distribution over S, we have
1“nÿ
i“1DK`1ptpxi, yiquq
“nÿ
i“1Dkptpxi, yiquqexp`
´α`
21yiPµkpxiq´1˘˘
ZK
“1
nnÿ
i“1exp´
´αřK
k“1`
21yiPµkpxiq´1˘¯
śK
k“1Zk,
which implies that
nKź
k“1Zk“nÿ
i“1exp˜
´αKÿ
k“1`
21yiPµkpxiq´1˘¸
. (20)
19For any kPrKs, we have
Zk“nÿ
i“1Dkptpxi, yiquqexp`
´α`
21yiPµkpxiq´1˘˘
“ÿ
iPrns:yiPµkpxiqDkptpxi, yiquqe´α`ÿ
iPrns:yiRµkpxiqDkptpxi, yiquqeα
“p1´erDkpµkqqe´α`erDkpµkqeα“erDkpµkqpeα´e´αq`e´α.
IfJk“0, we have
Zkďγeα`p1´γqe´α“a
1´γ2.
IfJk“1, we have
Zkďeα“c
1`γ
1{2´γ{2ďa
1`4γ.
Then, under the event Ec, we can upper bound
Kź
k“1Zkďp1´γ2qp1{2´νqKp1`4γqνK
ďexp´
´γK´γ
2´pγ`4qν¯¯
ďexpˆ
´γK
4˙
ďδ
n.
where the second last inequality follows from νďγ
18ďγ
4pγ`4qand the last inequality follows from
Kě4 logpn{δq
γ. By (20) and the above inequality, we have that on Ec,
nÿ
i“1exp˜
´αKÿ
k“1`
21yiPµkpxiq´1˘¸
ďδ,
which implies that exp´
´αřK
k“1`
21yiPµkpxiq´1˘¯
ďδfor all iPrns. It follows that
expp´fpxi, yiqqďδ1{αK
forfpx, yq:“1
KřK
k“1`
21yPµkpxq´1˘
,@px, yqPXˆY. Thus, we have
fpxi, yiqělogp1{δq
αK“2 logp1{δq
log´
1{2`γ
1{2´γ¯
r4 logpn{δq{γsą0,@iPrns.
Since by definition,
fpx, yqą0ôyPµpxq,@px, yqPXˆY,
we can conclude that on Ec,
yiPµpxiq,@iPrns,
i.e., with probability at least 1´δ,yiPµpxiqfor all iP rns. Moreover, by da Cunha
et al. [2024, Lemma 3.3], the randomized compression scheme SÞÑ pS1, . . . , SKqis stable.
Thus, we can apply da Cunha et al. [2024, Theorem 1.2] with compression size sn“mK“
O´MAlist,Hp1{2´γ,νqlogpn{δq
γ¯
to conclude the proof.
For the proofs of Corollary 2.9 and Theorem 2.10, we will need the following Lemma.
Lemma D.1. Ifxą0satisfies xďalogpx{aq`bfor some a, bą0, then, we have xď2a`2b.
20Proof. Define fpxq:“x´alogpx{aq´bforxą0. Then, we have f1pxq“x´a
x, which implies
thatfdecreases with xforxPp0, aqand increases with xforxąa. Since 2a`2bąa, it suffices
to prove that fp2a`2bqě0. Indeed,
fp2a`2bq“p2´log 2qa`b´alogppa`bq{aqąappa`bq{a´logppa`bq{aqqě0.
Proof of Corollary 2.9. By Brukhim et al. [2022, Theorem 36] (choosing t“r?
ds), there exists an
nÑrsample compression scheme AMfor any HĎYXof DS dimension dimpHq“dă8 where
r“Oppd3{2`dlogppqqlogpnqqwithp“Oppe?
dq?
dlogpnqq. (21)
Then, by David et al. [2016, Theorem 3.1], there exists a universal constant Cą0such that for
anyDPREpHq,δPp0,1q,nPNlarge enough, and S„Dn, letting hS:“AMpS,Hqdenote the
output classifier of the above nÑrsample compression scheme AM, we have that
erDphSqďCprlogpn{rq`logp1{δqq
n
with probability at least 1´δ. Thus, for any εPp0,1q, if
něCr
εlogpn{rq`Clogp1{δq
ε“Cr
εlogˆn
Cr{ε˙
`Cr
εlogˆC
ε˙
`Clogp1{δq
ε,
we have erDphSqďεwith probability at least 1´δ. By Lemma D.1, it suffices to require
ně2Cr
εlogˆCe
ε˙
`2Clogp1{δq
ε“2C
εˆ
rlogˆCe
ε˙
`logˆ1
δ˙˙
.
Applying the upper bound of rin (21), it suffices to require
něC1ˆpd3{2`dlogppqqlogp1{εq
εlogpnq`logp1{δq
ε˙
for some universal constant C1ą0. By Lemma D.1 again, it suffices to require
ně2C1pd3{2`dlogppqqlogp1{εq
εlogˆeC1pd3{2`dlogppqqlogp1{εq
ε˙
`2C1logp1{δq
ε.
Since logplogp1{εq{εq“logp1{εq`log logp1{εqď2 logp1{εq, it suffices to require
něC2ˆpd3{2`dlogppqqlogp1{εq
εlogˆd3{2`dlogppq
ε˙
`logp1{δq
ε˙
for some universal constant C2ą0. Applying the upper bound of pin(21), it suffices to require that
něC3ˆpd3{2logpdq`dlog logpnqqlogp1{εq
εlogˆd3{2logpdq`dlog logpnq
ε˙
`logp1{δq
ε˙
for some universal constant C3ą0. For ε“1{6andδ“1{54, we require that
něC1pd3{2logpdq`dlog logpnqqlog´
d3{2logpdq`dlog logpnq¯
for some universal constant C1ą0. IfC1p?2C1`1qd3{2logpdqlog`
p?2C1`1qd3{2logpdq˘
ď
nďed?
2C1d, we have?2C1d3{2logpdqědlog logpnqand
C1pd3{2logpdq`dlog logpnqqlogpd3{2logpdq`dlog logpnqq
ďC1pa
2C1`1qd3{2logpdqlog´
pa
2C1`1qd3{2logpdq¯
ďn.
Ifnąed?
2C1d, we have logplogpnqq ą?2C1dlogpdq,logplogpnqq2ą2C1dlog2pdq ě
2C1pdlogp2dq`dq, and
C1pd3{2logpdq`dlog logpnqqlogpd3{2logpdq`dlog logpnqq
ď2C1dlogplogpnqqlogp2dlog logpnqq
“2C1dlogp2dqlogplogpnqq`2C1dlogplogpnqqlogplog logpnqq
ďlogplogpnqq3logplog logpnqqqď n.
21Therefore, we can conclude that if něC1p?2C1`1qd3{2logpdqlog`
p?2C1`1qd3{2logpdq˘
“
Θpd3{2log2pdqq, we have PperDphSqą1{6qď1{54, i.e.,
MAM,Hp1{2´1{3,1{54q“Opd3{2log2pdqq.
LetABdenote the multiclass learner output by Aboost in Algorithm 2 using AMas the weak list
learner of size 1. Then, by Theorem 2.8, we have
εAB,Hpnq“Oˆd3{2log2pdqlogpnq
n˙
,@nPN.
Next, by Theorem 2.6, there exists a multiclass learner Asuch that for any DPREpHq,δPp0,1q,
nPN, and S„Dn, with probability at least 1´δ, we have
erDpApS,Hqq“Oˆd3{2log2pdqlogpnq`logp1{δq
n˙
.
Setting δ“1{nand observing that erDPr0,1s, it follows that εA,Hpnq“O´
d3{2log2pdqlogpnq
n¯
.
Moreover, by Lemma D.1, for any εP p0,1q,něO´
d3{2log2pdqlogpd{εq`logp1{δq
ε¯
implies that
erDpApS,Hqqďεwith probability at least 1´δ. Thus, we have
MA,Hpε, δq“Oˆd3{2log2pdqlogpd{εq`logp1{δq
ε˙
.
Proof of Theorem 2.10. By Brukhim et al. [2022, Theorem 39] (choosing t“r?
ds), there exists an
nÑrlist sample compression scheme Alistof size p“Oppe?
dq?
dlogpnqqfor any HĎYXof
DS dimension dimpHq“dă8 where
r“Opd3{2logpnqq. (22)
Define the following loss function for menus µ,
ℓpµ,px, yqq:“ 1yRµpxq,@px, yqPXˆY.
Then, we can apply the proof of David et al. [2016, Theorem 3.1] with the loss function ℓto show
that there exists a universal constant Cą0such that for any nÑrlist sample compression
scheme ALSC forH, anyDPREpHq,δP p0,1q,nPNlarge enough, and S„Dn, letting
µS:“ALSCpS,Hqdenote the output menu, we have
erDpALSCpS,Hqq“EpX,Yq„DrℓpALSCpS,Hq,pX, Yqq|SsďCprlogpn{rq`logp1{δqq
n
with probability at least 1´δ. Thus, for any εPp0,1q, if
něCr
εlogpn{rq`Clogp1{δq
ε“Cr
εlogˆn
Cr{ε˙
`Cr
εlogˆC
ε˙
`Clogp1{δq
ε,
we have erDpALSCpS,Hqqďεwith probability at least 1´δ. By Lemma D.1, it suffices to require
ně2Cr
εlogˆCe
ε˙
`2Clogp1{δq
ε“2C
εˆ
rlogˆCe
ε˙
`logˆ1
δ˙˙
.
SinceAlistis annÑrlist sample compression scheme for Hwithrbounded in (22), The above
results imply that in order for PperDpAlistpS,Hqqąεqďδ, it suffices to require
něC1ˆd3{2logp1{εq
εlogpnq`logp1{δq
ε˙
for some universal constant C1ą0. By Lemma D.1 again, it suffices to require
ně2C1d3{2logp1{εq
εlogˆeC1d3{2logp1{εq
ε˙
`2C1logp1{δq
ε.
22Since logplogp1{εq{εq“logp1{εq`log logp1{εqď2 logp1{εq, it suffices to require
něC2ˆd3{2logpd{εqlogp1{εq`logp1{δq
ε˙
for some universal constant C2ą0, which implies that
MAlist,Hp1{2´1{3,1{54q“O´
d3{2logpdq¯
.
Now, we can define ALto be the list learner output by Aboost in Algorithm 2 using Alistas the weak
list learner of size Oppe?
dq?
dlogpnqqfor training sample size nPN. Then, by Theorem 2.8, we
have that for any training sample size nPN,
εAL,Hpnq“Oˆd3{2logpdqlogpnq
n˙
and the size of ALis also Oppe?
dq?
dlogpnqq.
E Proof of Theorem 2.11
Proof. By Theorem 2.10, there exists a list learner ALwithppH, nq“O`
pe?
dq?
dlogpnq˘
and
βpH, nq“O`
d3{2logpdqlogpnq˘
for any nPNand concept class HĎYXwith dimpHq“dPN
in the context of Theorem 2.7. Thus, by Theorem 2.7, there exists a multiclass learner Amulti“A1
red
such that for any DPREpHq, any δPp0,1q,S„Dn, and n1“n´2tn{3u, with probability at
least1´δ,
erDpAmultipS,Hqq“OˆβpH, n1q`dlogppH, n1q`logp1{δq
n˙
“O˜
d3{2logpdqlogpnq`d3{2logpe?
dlogpnqq`logp1{δq
n¸
“Oˆd3{2logpdqlogpnq`logp1{δq
n˙
.
For any εPp0,1q, by Lemma D.1, if
ně2d3{2logpdq
εˆ
1`5
2logpd{εq˙
`2 logp1{δq
ε
ě2d3{2logpdq
εˆ
1`3
2logpdq`log logpdq`logp1{εq˙
`2 logp1{δq
ε
ě2d3{2logpdq
εlogˆed3{2logpdq
ε˙
`2 logp1{δq
ε,
then, we have
něd3{2logpdq
εlogpnq`logp1{δq
ε
i.e.,
d3{2logpdqlogpnq`logp1{δq
nďε.
It follows that
MAmulti,Hpε, δq“Oˆd3{2logpdqlogpd{εq`logp1{δq
ε˙
.
(9)follows directly from (6)by plugging in ppH, nq“f1pdqandβpH, nq“f2pdqford“dimpHq
and any nPN.
23F Classes of DS dimension 1
In this section, we present the proof of Proposition 3.3 and Theorem 3.2.
Proof of Proposition 3.3. LetEndenote the edge set of the hypergraph GpVnq. Suppose on the
contrary that there exists a cycle consisting of pairwise different vertices y0, . . . , ym´1PVnand
pairwise different edges e0“ pei0,f0, i0q, . . . , em´1“ peim´1,fm´1, im´1q PEnfor some mP
t4, . . . ,|Vn|usuch that yj,ypj`1qmodmPpeij,fj, ijqfor all 0ďjďm´1. Since the edges are
pairwise different, by the definition of En, we have ij‰ipj´1qmodmfor all 0ďjďm´1.
Define a:“im´1Prns,b:“i0Prns,p0:“y0
a“y1
a,p´1:“ym´1
a‰p0, and q0:“y0
b“ym´1
b.
Then, we have a‰b.
For any kPN, we define
jk:“maxtjPN0:jďm´1, yj
a“pk´1u,
pk:“ypjk`1qmodm
a , and qk:“yjk
b“ypjk`1qmodm
bbecause ijk“a. Define
K:“mintkPN:pk“p´1u.
By definition, we have p0, . . . , p Kare pairwise different. There are two cases depending on the
values of q1, . . . , q Kas follows.
1. Suppose that there exists some kPrKssuch that qk‰q0. Define l0:“0and
lw:“mintkPrKs:kąlw´1, qk‰qlw´1u
for all wPNwith the convention that infH“`8 . Define
v:“maxtwPN:lw‰`8u .
We have vPrKsandqlw‰qlw´1for all wPrvs. Note that
p´1“pK“ym´1
a, q 0“ym´1
b,
pl0“p0“y0
a, q l0“q0“y0
b,
plw´1“yjlw´1`1
a , q lw´1“yjlw´1`1
b,
plw´1“yjlwa, q lw“yjlw
b,
plw“yjlw`1
a, q lw“yjlw`1
b
for all wPrvs, and
pK“p´1“yjK`1
a, q K“yjK`1
b.
Since it always holds that qK“qlv, there are two cases depending on the value of qlvand
q0as follows.
1.1Suppose that qlv‰q0. Then, Vn|pa,bqcontains the following pseudo-cube of dimension
2:
ppl1´1, q0q,
ppl1´1, ql1q,
ppl2´1, ql1q,
ppl2´1, ql2q,
...
pplv´1, qlv´1q,
pplv´1, qlvq,
pp´1, qlvq,
pp´1, q0q,
which contradicts the assumption that dimpVnq“1.
241.2Suppose that qlv“q0. Then, Vn|pa,bqcontains the following pseudo-cube of dimension
2:
ppl1´1, q0q,
ppl1´1, ql1q,
ppl2´1, ql1q,
ppl2´1, ql2q,
...
pplv´1, qlv´1q,
pplv´1, qlv“q0q,
which contradicts the assumption that dimpVnq“1.
Thus, Case 1 does not exist.
2.Suppose that qk“q0for all kPrKs. Since y0
a“p0“pj1a,y0
b“q0“q1“pj1
b, and
y0‰yj1, there must exist some cPrnszta, busuch that ym´1
c“y1
c“y0
c‰yj1c“yj1`1
c.
We define r0:“y0
candrk:“yjkc“yjk`1
c forkPrKs. There are two cases depending on
the value of rkforkPrKsas follows.
2.1Suppose that there exists some kPrKssuch that rk‰r1. Similar to Case 1, we define
l0:“1and
lw:“mintkPrKs:kąlw´1, rk‰rlw´1u
forwPN. Define v:“maxtwPN:lw‰`8u . We have vPrKsandrlw‰rlw´1
for all wPrvs. Note that
p´1“pK“ym´1
a, r 0“ym´1
c,
p0“y0
a, r 0“y0
c,
p0“yj1
a, r l0“r1“yj1
c,
plw´1“yjlw´1`1
a , r lw´1“yjlw´1`1
c ,
plw´1“yjlwa, r lw“yjlwc,
for all wPrvs, and
pK“p´1“yjK`1
a, r K“yjK`1
c.
Since it always holds that rK“rlv, there are two cases depending on the value of qlv
andq0as follows.
2.1.1 Suppose that rlv‰r0. Then, Vn|pa,cqcontains the following pseudo-cube of
dimension 2:
pp0, r0q,
pp0, rl0“r1q,
ppl1´1, rl0q,
ppl1´1, ql1q,
...
pplv´1, rlv´1q,
pplv´1, rlvq,
pp´1, rlvq,
pp´1, r0q,
which contradicts the assumption that dimpVnq“1.
252.1.2 Suppose that rlv“r0. Then, Vn|pa,cqcontains the following pseudo-cube of
dimension 2:
pp0, r0q,
pp0, rl0“r1q,
ppl1´1, rl0q,
ppl1´1, ql1q,
...
pplv´1, rlv´1q,
pplv´1, rlv“r0q,
which contradicts the assumption that dimpVnq“1.
2.2 Suppose that rk“r1for all kPrKs. Then, we have
p´1“ym´1
a, r 0“ym´1
c,
p0“y0
a, r 0“y0
c,
p0“yj1
a, r 1“yj1
c,
p´1“yjK`1
a, r 1“yjK`1
c,
which implies that Vn|pa,cqcontains a pseudo-cube of dimension 2.
Thus, Case 2 does not exist either.
In conclusion, there exists no cycle in the hypergraph GpVnq“pVn, Enq.
Proof of Theorem 3.2. It suffices to show that the for any nPNandVnĎYn, the average degree of
G“GpVnq“pVn, Enqwith|Vn|ă8 anddimpVnq“1is at most 2.
We prove by induction on |Vn|. When|Vn|“1, we have En“H andavgdegpGq“0ă2. When
|Vn| “2, we haveř
ePEn:|e|ě2|e| ď2andavgdegpGq ď1ă2. Suppose that avgdegpGq ď2
for any Vnof size|Vn|ďmwith some mPN. When|Vn|“m`1, since there is no cycle in G
according to Proposition 3.3, the set of vertices of degree 1 pV1
n:“tyPVn:degpyq“1uqis not
empty. Define V2
n:“VnzV1
nandE2
nto be the edge set such that pV2
n, E2
nqis the one-inclusion graph
onV2
n. Then, we have |V2
n|“|Vn|´|V1
n|ďmand
ÿ
ePEn:|e|ě2|e|ďÿ
ePE2n:|e|ě2|e|`2|V1
n|
because deleting a vertex of degree 1 decreases the total degree by at most 2. By the induction
hypothesis, we haveř
ePE2n|e|ď2|V2
n|. Thus,
ÿ
ePEn:|e|ě2|e|ď2p|V1
n|`|V2
n|q“2|Vn|
which implies that avgdegpGqď2. By induction, avgdegpGqď2for any Vnwith|Vn|ă8 and
dimpVnq“1.
G Pivot shifting
In this section, we present the proofs of Lemma 3.6 and Lemma 3.8.
Proof of Lemma 3.6. For notational convenience, we let V1
n´1denotePpVnqwhich is defined in
Definition 3.5. Define
Vn´1:“ď
yPY␣
py1, . . . , y n´1qPYn´1:py1, . . . , y n´1, yqPVn(
26and
V2
n´1:“Vn´1zV1
n´1.
LetEndenote the edge set in GpVnqandEn´1denote the edge set in GpVn´1q. For any yPY,
define
Vn,y:“␣
py1, . . . , y nqPVn:yn“y,py1, . . . , y n´1qPV1
n´1(
.
By the assumption on aPY, we have|Vn|“|V2
n´1|`ř
yPY|Vn,y|,|V1
n´1|“|Vn,a|, and|Vn|´
|Vn´1|“ř
yPYztau|Vn,y|. Defining
En
n:“tpei,f, iqPEn:i“nu,
we have
ÿ
ePEnnp|e|´1q“|Vn|´|Vn´1|.
For any yPY, we define
En,y:“tpei,f, iqPEn:fpnq“y, iPrn´1su
and let E1
n,ydenote the edge set in GpVn,yq; for any e“pei,f, iqPEn,y, we define
s1peq:“|tyPei,f:yPVn,yu|ands2peq:“|tyPei,f:yRVn,yu|.
Then, we have |e|“s1peq`s2peq,En“pY yPYEn,yqYEn
n, and
ÿ
ePEn´1p|e|´1q
ěÿ
ePEn,ap|e|´1q`ÿ
yPYztau˜ÿ
ePEn,y:s1peq“0ps2peq´1q`ÿ
ePEn,y:s1peqě1s2peq¸
.
Note that by the induction hypothesis, we have
ÿ
ePEn´1p|e|´1qďd|Vn´1|
and by definition, we also have
ÿ
ePE1n,yp|e|´1q“ÿ
ePEn,y:s1peqě2ps1peq´1q.
We claim that dimpVn,yqďd´1for all yPYztau. Suppose on the contrary that dimpVn,yqąd´1.
Since dimpVnq “d, we have dimpVn,yq “dand there exists a set i:“ ti1, . . . , i du Ď r nswith
i1ăi2ă ¨¨¨ ă idsuch that Vn,y|icontains a pseudo-cube Hyof dimension d. Since for any
py1, . . . , y nqPVn,y, we have yn“y, it must hold that idďn´1. Now, we can define
Hy,a:“tpyi1, . . . , y id, aqPYd`1:pyi1, . . . , y idqPHyuď
tpyi1, . . . , y id, yqPYd`1:pyi1, . . . , y idqPHyu.
By the assumption that |V1
n´1|“|Vn,a|, we have|Hy,a|“2|Hy|andHy,aĎVn|pi1,...,id,nq. For
anykP rdsand anypyi1, . . . , y id, yq,pyi1, . . . , y id, aq PHy,a, since there exists an k-neighbor
ofpyi1, . . . , y idqdenoted bypyi1, . . . , y1
ik, . . . , y idqinHy,pyi1, . . . , y1
ik, . . . , y id, y1qPHy,ais ak-
neighbor ofpyi1, . . . , y id, y1qinHy,afory1“y, a. Moreover,pyi1, . . . , y id, yqis apd`1q-neighbor
ofpyi1, . . . , y id, aqinHy,aand vise-versa. Thus, Hy,ais a pseudo-cube of dimension d`1, which
contradicts the assumption that dimpVnqďd. Therefore, we must have dimpVn,yqďd´1. Then,
by the induction hypothesis, we have for any yPYztau,
ÿ
ePE1n,yp|e|´1qďpd´1q|Vn,y|.
27Summarizing the results above, we haveÿ
ePEnp|e|´1q
“ÿ
ePEnnp|e|´1q`ÿ
yPYÿ
ePEn,yp|e|´1q
“ÿ
ePEnnp|e|´1q`ÿ
ePEn,ap|e|´1q`
ÿ
yPYztau˜ÿ
ePEn,y:s1peq“0ps2peq´1q`ÿ
ePEn,y:s1peq“1s2peq`ÿ
ePEn,y:s1peqě2ps2peq`ps1peq´1qq¸
“ÿ
ePEnnp|e|´1q`ÿ
ePEn,ap|e|´1q`ÿ
yPYztau˜ÿ
ePEn,y:s1peq“0ps2peq´1q`ÿ
ePEn,y:s1peqě1s2peq¸
`ÿ
yPYztauÿ
ePEn,y:s1peqě2ps1peq´1q
ď|Vn|´|Vn´1|`ÿ
ePEn´1p|e|´1q`ÿ
yPYztauÿ
ePE1n,yp|e|´1q
ď|Vn|´|Vn´1|`d|Vn´1|`ÿ
yPYztaupd´1q|Vn,y|
“|Vn|´|Vn´1|`d|Vn´1|`pd´1qp|Vn|´|Vn´1|q
“d|Vn|.
Proof of Lemma 3.8. Consider arbitrary nPNzt1uandVnĎYn. By the definition of PapVnq, we
have|Vγ
n|“|Vn|. Let Endenote the edge set of GpVnqandEn,γdenote the edge set of GpVγ
nq. It
suffices to prove thatř
ePEnp|e|´1qďř
ePEn,γp|e|´1q. Define
Ei
n:“tpek,f, kqPEn:k“iuandEi
n,γ:“tpek,f, kqPEn,γ:k“iu
for all iPrns. By the definition of Vγ
n, we have
ÿ
ePEnnp|e|´1q“ÿ
ePEnn,γp|e|´1q. (23)
For any iPrnsandf:rnsztiuÑY, we define
ei,f:“tpy1, . . . , y nqPVn:yk“fpkq@kPrnsztiuuand
eγ
i,f:“tpy1, . . . , y nqPVγ
n:yk“fpkq@kPrnsztiuu
to distinguish edges in EnandEn,γ. For any yPY,iPrn´1s, and f:rn´1sztiuÑY, define
fy:rnsztiuÑYsuch that fy|rn´1sztiu“fandfypnq“y. Then, we have
ÿ
yPYztau:pei,fy,iqPEnp|ei,fy|´1q´ÿ
yPYztau:peγ
i,fy,iqPEn,γp|eγ
i,fy|´1q
ď 1peγ
i,fa,iqPEn,γp|eγ
i,fa|´1q´ 1pei,fa,iqPEnp|ei,fa|´1q
which implies that
n´1ÿ
i“1ÿ
ePEinp|e|´1qďn´1ÿ
i“1ÿ
ePEin,γp|e|´1q.
and from (23),ÿ
ePEnp|e|´1qďÿ
ePEn,γp|e|´1q.
Thus, we can conclude that avgoutdegpGpVγ
nqqě avgoutdegpGpVnqqfor any γPΓa,Vn.
28H Lemmas regarding graph dimension
In this section, we provide the technical lemmas on learning with finite graph dimension and bounding
the graph dimension of certain classes. Those lemmas are used in the proof of Theorem 2.7.
H.1 Learning algorithm for classes with finite graph dimension
We first provide the definition of graph dimension.
Definition H.1 (Graph dimension) .ForHĎYXandnPN,x“px1, . . . , x nqPXnis said to be
G-shattered byHis there exists f:rnsÑYsuch that for any iĎrns, there exists gPHsatisfying
gpxiq“fpiqfor all iPiandgpxiq‰fpiqfor all iPrnszi. The graph dimension ofH, denoted as
dimGpHqis the maximum size of a G-shattered sequence.
Define Log :r0,8q Ñ r 1,8q, xÞÑlogpx_eqwhere x_e“maxtx, eu. For any HĎYX,
nPrdimGpHqs, and x“tx1, . . . , x nuĎXthat is G-shattered by H, there exists f:rnsÑYsuch
that for any iĎrns, there exists gPHsatisfying gpxiq“fpiqfor all iPiandgpxiq‰fpiqfor all
iPrnszi. Thus, we can define
Hpxq:“!
h:rnsÑt 0,1u, iÞÑ 1rhpxiq“fpiqˇˇrhPH)
.
For general nPN,xĎXwith|x|“n, and fPYn, we define
Hfpxq:“!
h:rnsÑt 0,1u, iÞÑ 1rhpxiq“fpiqˇˇrhPH)
and
τHpnq:“ sup
xĎX:|x|“nsup
fPYn|Hfpxq|.
Note that τHpnq“|Hpxq|“2nfor any nPrdimGpHqs. We have the following lemma.
Lemma H.2. For any HĎYXwith dimGpHq“d, we have τHpnqďřd
i“0`n
i˘
. In particular, if
něd, then τHpnqďpen{dqd.
Proof. We first prove by induction on nthat for any x“tx1, . . . , x nuĎXandfPYn,
|Hfpxq|ď|t x1Ďx:HG-shatters x1u|. (24)
Forn“1, it is obviously that |Hfpxq| ď 2and|tx1Ďx:HG-shatters x1u| ě 1. If|tx1Ďx:
HG-shatters x1u|“1, then xis not G-shattered by H, which implies that |Hfpxq|ď1. Thus, (24)
holds. Now, suppose that (24) holds for any kăn. Consider sx:“tx2, . . . , x nu,
Y0:“tpy2, . . . , y nqPt0,1un´1:p0, y2, . . . , y nqPHfpxqorp1, y2, . . . , y nqPHfpxqu,and
Y1:“tpy2, . . . , y nqPt0,1un´1:p0, y2, . . . , y nqPHfpxqandp1, y2, . . . , y nqPHfpxqu.
We have|Hfpxq|“| Y0|`|Y1|and|Y0|“|Hfpsxq|. Then, by the induction hypothesis, we have
|Y0|ď|t x1Ďsx:HG-shatters x1u|“|t x1Ďx:x1Rx1andHG-shatters x1u|.
For any yPY, define
Hy:“thPH:Dh1PHs.t.h|xandh1|xdiffers only at 1andyPthpx1q, h1px1quu.
Then,HyG-shatters x1Ďsximplies that HG-shatters x1Ytx1u. We also have Y1“Hfp1q
fpsxq. It
follows from the induction hypothesis that
|Y1|“|Hfp1q
fpsxq|ď|t x1Ďsx:Hfp1qG-shatters x1u|ď|t x1Ďx:x1Px1andHG-shatters x1u|.
In conclusion, we have
|Hfpxq|
“|Y0|`|Y1|
ď|tx1Ďx:x1Rx1andHG-shatters x1u|`|t x1Ďx:x1Px1andHG-shatters x1u|
“|tx1Ďx:HG-shatters x1u|,
29which is exactly (24). By (24), we have
τHpnqď|t x1Ďx:HG-shatters x1u|ďdÿ
i“0ˆn
i˙
.
For any (measurable) classifier h:XÑY, define
ERphq:“tpx, yqPXˆY:hpxq‰yu.
Then, for any probability measure DoverXˆY, we can define
erDphq:“DpERphqq“PpX,Yq„DphpXq‰Yq.
Definition H.3. SĎXˆYis said to be a an ε-net ( εPp0,1q) forHĎYXwith respect to a
distribution DoverXˆYif for any hPH,
erDphqěεùñERphqXS“tpx, yqPS:hpxq‰yu‰H .
For any integer nPN, setZ, and T“pz1, . . . , z nqPZn, we say zPTifz“zifor some iPrns
and use|T|to denote the length of the sequence T. For notational convenience, we use Hto also
denote an empty sequence (a sequence of length 0). For any subset EĎZ, we use TXE“EXT
to denote the subsequence of Tconsisting of all elements in E, i.e., for I:“tiPrns:ziPEu,
TXE“EXT“pziqiPI.
Then, we have |TXE|“|EXT|“|I|.
Proposition H.4. For any HĎYXwith dimGpHq “d, anyH-realizable distribution D, any
δPp0,1s, anynPN, and any ERM algorithm A, consider Sn„Dn. With probability at least 1´δ,
we have
erDpApSn,Hqqď2
n„
d_ˆ
dlog2ˆ2en
d˙˙
`log2ˆ2
δ˙ȷ
.
Proof. For any ně2andεPr2{n,1s, define
B:“tSPpXˆYqn:DhPHs.t.erDphqěεandERphqXS“Hu and
B1:“␣
pS, TqPpXˆYq2n:|S|“|T|“|n|,DhPHs.t.
erDphqěε,ERphqXS“H,and|ERphqXT|ąεn{2(
.
LetpS, Tq„D2nwithS, TPpXˆYqn. SincepS, TqPB1implies that SPB, we have
PppS, TqPB1q“Er 1pS,TqPB1 1SPBs“Er 1SPBPppS, TqPB1|Sqs.
OnSPB, there exists hPHsuch that erDphqěεandERphqXS“H . Then,|ERphqXT|ąεn{2
implies thatpS, TqPB1. It follows that
1SPBPppS, TqPB1|Sqě 1SPBPp|ERphqXT|ąεn{2|Sq.
Since Tis independent of S,his determined by S, andDpERphqq“ erDphqěεonSPB, we
know that on SPB,|ERphqXT|follows the Binomial distribution Bpn,erDphqqconditional on S,
and
1SPBEr|ERphqXT||Ss“erDphqn 1SPBěεn 1SPB.
Thus, by Lemma H.8, since nεě2, we have
1SPBPp|ERphqXT|ďεn{2|SqďPp|ERphqXT|ďerDphqn{2|Sq 1SPBă1
21SPB,
which implies that
PppS, TqPB1qą1
2PpSPBq.
30By the definition of B1, we have
PppS, TqPB1q“E„
sup
hPH1erDphqěε 1ERphqXS“H 1|ERphqXT|ąεn{2ȷ
“E„
sup
hPH1erDphqěε 1ERphqXS“H 1|ERphqXpS,Tq|ąεn{2ȷ
ďE„
sup
hPH1ERphqXS“H 1|ERphqXpS,Tq|ąεn{2ȷ
.
For any mPN,r“ppx1, y1q, . . . ,pxm, ymqqPpXˆYqm, and hPYX, we define
hr:rmsÑt 0,1u, iÞÑ 1hpxiq“yi
andHr:“thr:hPHu. Note that Hr“Hpy1,...,y mqppx1, . . . , x mqq, which implies that
|Hr|ďτHpmq. (25)
For any kPrmsand1ďi1ă¨¨¨ă ikďm, we use rpi1,...,ikqto denote a permutation of rwhere
pxj, yjqappears in the ij-th position for all jP rks, specifically, rti1,...,iku“ pxσpiq, yσpiqqiPrns
where σpijq:“jfor all jPrksandpσplqqlPrnszti1,...,iku:“pk`1, . . . , mq. Then, for any iĎr2ns
with|i|“n, we have
sup
hPH1ERphqXS“H 1|ERphqXpS,Tq|ąεn{2ďÿ
hPHpS,Tq1hpiq“1,@iPrns 1ř
iPr2nshpiqăp2´ε{2qn
“ÿ
hPHpS,Tqi1hpiq“1,@iPi 1ř
iPr2nshpiqăp2´ε{2qn.
SincepS, Tq„D2n, we also have
E»
–ÿ
hPHpS,Tqi1hpiq“1,@iPi 1ř
iPr2nshpiqăp2´ε{2qnfi
fl“E»
–ÿ
hPHpS,Tq1hpiq“1,@iPi 1ř
iPr2nshpiqăp2´ε{2qnfi
fl.
Thus,
E„
sup
hPH1ERphqXS“H 1|ERphqXpS,Tq|ąεn{2ȷ
ď1`2n
n˘ÿ
iĎr2ns:|i|“nE»
–ÿ
hPHpS,Tq1hpiq“1,@iPi 1ř
iPr2nshpiqăp2´ε{2qnfi
fl
“E»
–ÿ
hPHpS,Tq1ř
iPr2nshpiqăp2´ε{2qn1`2n
n˘ÿ
iĎr2ns:|i|“n1hpiq“1,@iPifi
fl
ďE»
–ÿ
hPHpS,Tq1ř
iPr2nshpiqăp2´ε{2qn`tp2´ε{2qnu
n˘
`2n
n˘fi
fl
ď2´εn{2E»
–ÿ
hPHpS,Tq1ř
iPr2nshpiqăp2´ε{2qnfi
fl (26)
ď2´εn{2E“
|HpS,Tq|‰
ď2´εn{2τHp2nq, (27)
where (26) follows from Lemma H.7 and (27) follows from (25). Finally, we have proved that
DnpBq“PpSPBqă2PppS, TqPB1qď2τHp2nq2´εn{2. (28)
SinceDisH-realizable and Ais an ERM algorithm, we must have ApSn,HqPHand
ERpApSn,HqqXSn“H
31almost surely. Moreover, by the definition of B, ifSnRB, then ERpApSn,HqqXSn“H implies
thaterDpApSn,Hqqăε. Thus, we have
PperDpApSn,HqqăεqěPpSnRBq“1´DnpBq.
Solving 2τHp2nq2´εn{2“δ, we get
ε“1^„2
nˆ
log2pτHp2nqq`log2ˆ2
δ˙˙ȷ
ď2
n„
d_ˆ
dlog2ˆ2en
d˙˙
`log2ˆ2
δ˙ȷ
where the last inequality follows from Lemma H.2. Note that 1^“2
n`
log2pτHp2nqq`log2`2
δ˘˘‰
ě
2
n, which implies that the above choice of εis legitimate. Applying (28), we can conclude that with
probability at least 1´δ,
erDpApSn,Hqqă2
n„
d_ˆ
dlog2ˆ2en
d˙˙
`log2ˆ2
δ˙ȷ
.
Proposition H.5. There exists a learning algorithm AGsuch that for any HĎYXwithdimGpHq“
d, anyH-realizable distribution D, any δPp0,1s, and any nPN, given Sn„Dn, it holds with
probability at least 1´δthat
erDpAGpSn,Hqq“Oˆ1
nˆ
d`Logˆ1
δ˙˙˙
. (29)
Proof. The algorithm AGis the algorithm SnÞÑMajoritypERM HpApSn;Hqqq defined in Hanneke
[2016], where ERM Hdenotes an ERM algorithm on the concept class H. Applying the error rate of
ERM algorithms proved in Proposition H.4 in the proof of Hanneke [2016, Theorem 2], we establish
(29).
The above proposition immediately implies the following corollary on the expected error rate of the
learning algorithm AG.
Corollary H.6. There exists a learning algorithm AGsuch that for any HĎYXwithdimGpHq“d,
anyH-realizable distribution D, and any nPNit holds that
ESn„DnrerDpAGpSn,Hqqs“EppSn,pX,Yqq„Dn`1rAGpSn,HqpXq‰Ys“Oˆd
n˙
. (30)
Proof. According to Proposition H.5, there exists some constant Cą0such that for any δPp0,1s,
it holds with probability at least 1´δthat
erDpAGpSn,HqqďC
nˆ
d`logˆ1
δ_e˙˙
,
which implies that for any těCpd`1q
n,
PperDpAGpSn,Hqqątqďe´nt
C`d.
Since erDpAGpSn,Hqqis nonnegative, we have
ErerDpAGpSn,Hqqs“ż8
0PperDpAGpSn,Hqqątqdt
ďCpd`1q
n`ż8
Cpd`1q
ne´nt
C`ddt
“Cpd`1`e´1q
n
“Oˆd
n˙
.
32Lemma H.7. For any nPNandmPNXrn,2ns, we have
`m
n˘
`2n
n˘ď2m´2n. (31)
Proof. Note that
`m
n˘
`2n
n˘“mpm´1q¨¨¨p m´n`1q
2np2n´1q¨¨¨p n`1q.
We prove by induction on m. When m“2n, we have
`m
n˘
`2n
n˘“1“2m´2n.
Suppose that (31) holds for some mPNXrn`1,2ns. Then, we have
`m´1
n˘
`2n
n˘“m´n
m`m
n˘
`2n
n˘ď1
2¨2m´2n“2m´1´2n.
Thus, by induction, (31) holds for any mPNXrn,2ns.
Lemma H.8. ForX„Bpn, pq, ifnpě2, then
PpXďnp{2qă1{2.
Proof. Ifnpą8, since ErXs“np, by the multiplicative Chernoff bound, we have
PpXďnp{2qďe´np{8ăe´1ď1{2.
For2ďnpď8, we have
PpXďnp{2q“tnp{2uÿ
i“0ˆn
i˙
pip1´pqn´i.
For6ďpnď8, we have 6{nďpď8{nandně6. Thus,
PpXďnp{2q“3ÿ
i“0ˆn
i˙
pip1´pqn´i.
Consider
f3px, pq:“logpxpx´1qpx´2qq`p x´3qlogp1´pq, xě6,1ěpě6{x.
Fixing pPp0,1s, we have xě6{pand for xě6{p,
B
Bxf3px, pq“1
x`1
x´1`1
x´2`logp1´pqď1
x`1
x´1`1
x´2´p
ďp
6`p
6´p`p
6´2p´pă0.
Thus, fixing pPp0,1s,f3p¨, pqis a decreasing function on r6{p,8q. Therefore, we have
f3px, pqďf3p6{p, pq,
which implies that
g3px, pqďg3p6{p, pq
for
g3px, pq:“xpx´1qpx´2q
6p3p1´pqx´3, pPp0,1s, xě6{p.
33Since logz´z`1ď0forzPp0,1sandg1pzq“logz´z`1forgpzq:“zlogpzq`1´z´1
2p1´zq2
defined on zPp0,1s, we have
0“gp1qďgpzqďlim
zÑ0`gpzq“1{2
forzPp0,1s. Thus, we have
logpzq`p1´zqp1´1´z
2q
zě0.
Plugging in z“1´6{tfortą6, we have
logp1´6{tq`6{tp1´3{tq
1´6{tě0.
Then, defining
fptq:“logptq`logpt´1q`logpt´2q´3 logptq`pt´3qlogp1´6{tq, tą6,
we have
f1ptq“1
t`1
t´1`1
t´3
t`logp1´6{tq`6{tp1´3{tq
1´6{tą0,
which implies that fptqincreases with tfortą6. Since
g3p6{p, pq“6{pp6{p´1qp6{p´2q
6p3p1´pq6{p´3
and
efptq“tpt´1qpt´2qt´3p1´6{tqt´3,
we know that g3p6{p, pqdecreases with pPp0,1s. Thus,
f3p6{p, pqďlim
pÑ0`g3p6{p, pq“lim
tÑ8tpt´1qpt´2q
6p6{tq3p1´6{tqt´3“36e´6.
Following the above steps, for
g2px, pq:“xpx´1q
2p2p1´pqx´2,
g1px, pq:“xpp1´pqx´1,and
g0px, pq:“p1´pqx,
where pPp0,1s, xě6{p,
it is easy to prove that for any i“0,1,2, and pPp0,1s,
gipx, pqďgip6{p, pqďlim
pÑ0`gip6{p, pq.
Specifically, we have
g2px, pqďlim
tÑ8tpt´1q
2p6{tq2p1´6{tqt´2“18e´6,
g1px, pqďlim
tÑ8tp6{tqp1´6{tqt´1“6e´6,and
g0px, pqďlim
tÑ8p1´6{tqt“e´6.
Then, we can conclude that
PpXďnp{2qď3ÿ
i“0sup
pPp0,1s,xě6{pgipx, pq“p36`18`6`1qe´6ă1
2.
34Next, we consider the regime that 4ďnpă6. Now, we have
PpXďnp{2q“2ÿ
i“0ˆn
i˙
pip1´pqn´iď2ÿ
i“0sup
pPp0,1s,xě4{ngipx, pq.
Following the procedures in the previous case, it is not hard to verify that for i“0,1,2,
sup
pPp0,1s,xě4{ngipx, pq“lim
pÑ0gip4{p, pq“lim
tÑ84i
i!p1´4{tqt´i“4i
i!e´4.
It implies that
PpXďnp{2q“2ÿ
i“0ˆn
i˙
pip1´pqn´iď2ÿ
i“04i
i!e´4ă1
2.
Finally, we consider the regime that 2ďnpă4. Now, we have
PpXďnp{2q“p1´pqn`npp1´pqn´1ď1ÿ
i“0sup
pPp0,1s,xě2{ngipx, pq.
Following the previous procedures, it is not hard to verify that for i“0,1,
sup
pPp0,1s,xě2{ngipx, pq“lim
pÑ0gip2{p, pq“lim
tÑ82i
i!p1´2{tqt´i“2i
i!e´2.
It implies that
PpXďnp{2qď1ÿ
i“02i
i!e´2ă1
2.
In conclusion, if npě2, then
PpXďnp{2qă1
2.
H.2 Bounding the graph dimension
For any HĎYXof DS dimension dimpHq“dă8 , sequence S“px1, . . . , x nqPXn, menu
µ:XÑtYĎY:|Y|ďpuof size pPNwithnPN, and d1Prns, define
HS,µ,d1:“␣
h|S:hPH,|tiPrns:hpxiqRµpxiqu|ď d1(
,
HS,µ, i:“th|S:hPH,tiPrns:hpxiqRµpxiquĎ iu,and
H1
S,µ, i:“!
h:rnsziÑY, iÞÑrhpiqˇˇrhPHS,µ, i)
for all iP 2rns
d1which denotes the collection of all subsets of rnsof size d1. We have the following
lemma.
Lemma H.9. dimGpHS,µ,d1qďp2 log2peq`4qp5dlog2ppq`2d1qfor any d1Prns.
Proof. For any iP 2rns
d1, by Bendavid et al. [1995], Daniely and Shalev-Shwartz [2014], we have
dimGpH1
S,µ, iqď5 log2ppqdimNpH1
S,µ, iqď5 log2ppqdimpHq“5dlog2ppq.
For any jĎ rnsthat is G-shattered by HS,µ, i, define j1:“jzi. We have that |j1| ě | j|´d1and
j1is G-shattered by H1
S,µ, i, which immediately implies that |j1|ďdimGpH1
S,µ, iqď5dlog2ppqand
|j|ď| j1|`d1ď5dlog2ppq`d1. It follows that
dimGpHS,µ, iqď5dlog2ppq`d1“:d1.
35For any mPNandj“tj1, . . . , j muĎrnsthat is G-shattered by HS,µ,d1, there exists f:rmsÑY
such that for any KĎ rms, there exists gPHS,µ,d1satisfying gpjkq “fpkqfor all kPKand
gpjkq‰fpkqfor all kPrmszK. It follows that |HS,µ,d1pjq|“2m. Ifmąd1, by Lemma H.2, we
have
2m“|HS,µ,d1pjq|ďÿ
iP 2j
d1|pHS,µ, iqfpjq|ďÿ
iP 2j
d1τHS,µ, ipmqďˆm
d1˙˜
2d1_ˆem
d1˙d1¸
,
which implies that
mďd1ˆ
1_log2ˆem
d1˙˙
`log2ˆm
d1˙
ďd1ˆ
1_log2ˆem
d1˙˙
`d1log2´em
d1¯
By Lemma H.11, we have
mďp2 log2peq`4qpd1`d1q“p2 log2peq`4qp5dlog2ppq`2d1q,
which implies that
dimGpHS,µ,d1qďp2 log2peq`4qp5dlog2ppq`2d1q.
Lemma H.10. Ifxą0satisfies xďalog2px{aq`bfor some a, bą0, then, we have xď2a`2b.
Proof. Define fpxq:“x´alog2px{aq´bforxą0. Then, we have f1pxq“1´a
xlogp2q, which
implies that fdecreases with xforxPp0, a{logp2qqand increases with xforxąa{logp2q. Since
2a`2bąa{logp2q, it suffices to prove that fp2a`2bqě0. Indeed,
fp2a`2bq“a`b´alog2ppa`bq{aq“appa`bq{a´log2ppa`bq{aqqě0.
Lemma H.11. Ifxą0satisfies xďalog2px{aq`blog2px{bq`cfor some a, b, cą0, then, we
havexď4a`4b`2c.
Proof. Since
xďalog2px{aq`blog2px{bq`c
“pa`bqlog2x
a`b`c´pa`bq„a
a`blog2a
a`b`b
a`blog2b
a`bȷ
ďpa`bqlog2x
a`b`c`a`b,
by Lemma H.10, we have
xď2pa`bq`2pa`b`cq“4a`4b`2c.
36NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In our abstract and introduction, we make the main claims that this paper
reduces the gap between the lower and upper bounds of the multiclass PAC sample com-
plexity and propose two possible routes towards completely resolving the optimal sample
complexity, which are the paper’s major contributions and scope.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: As is shown in Theorem 1.9, though we have reduced a log factor, there is still
anOp?
dlogpdqlogpd{εqqgap between our upper and lower bounds of the multiclass PAC
sample complexity. We leave the development of sharper sample complexity upper bound in
Open Question 1 and Open Question 2.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
373.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: For each of our theoretical results, we provide the full set of assumptions in its
statement as well as a complete and correct proof in the appendices.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper is fully theoretical and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
4.1If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
4.2If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
4.3If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
4.4We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
385.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The paper is fully theoretical and does not include experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The paper is fully theoretical and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper is fully theoretical and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
39•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The paper is fully theoretical and does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
40•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not include experiments and poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
41•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
42