Node-Level Topological Representation Learning on
Point Clouds
Anonymous Author(s)
Affiliation
Address
email
Abstract
Topological Data Analysis ( TDA) allows us to extract powerful topological, and 1
higher-order information on the global shape of a data set or point cloud. Tools 2
like Persistent Homology or the Euler Transform give a single complex description 3
of the global structure of the point cloud. However, common machine learning 4
applications like classification require point-level information and features to be 5
available. In this paper, we bridge this gap and propose a novel method to extract 6
node-level topological features from complex point clouds using discrete variants 7
of concepts from algebraic topology and differential geometry. We verify the 8
effectiveness of these topological point features ( TOPF ) on both synthetic and 9
real-world data and study their robustness under noise. 10
1
Input
2
3
4Compute persistent homologyPick significant generatorsWeighted harmonic representativesAverage over incident simplicesTopological Node Embeddings in Biology, Machine Learning, Computer Vision, etc.Feature vectors1234123412341234
Feature vectors
Output
Applications
Figure 1: Schematic of Computing Topological Point Features ( TOPF ).Input. A point cloud X
inn-dimensional space. Step 1. To extract global topological information, the persistent homology
is computed on an α/VR-filtration. The most significant topological features Facross all specified
dimensions are selected. Step 2. k-homology generators associated to all features fi,k∈ F are
computed. For every feature, a simplicial complex is built at a step of the filtration where fi,kis
alive. Step 3. The homology generators are projected to the harmonic space of the simplices. Step 4.
The vectors are normalised to obtain vectors ei
kindexed over the k-simplices. For every point xand
feature f∈ F, we compute the mean of the entries of ei
kcorresponding to simplices containing x.
The output is a |X| × |F| matrix which can be used for downstream MLtasks. Optional. We weigh
the simplicial complexes resulting in a topologically more faithful harmonic representative in Step 3 .
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.1 Introduction 11
In modern machine learning [ 39], objects are described by feature vectors within a high-dimensional 12
space. However, the coordinates of a single vector can often only be understood in relation to the 13
entire data set: if the value xis small, average, large, or even an outlier depends on the remaining 14
data. In a 1-dimensional (or low-dimensional) case this issue can be addressed simply by normalising 15
the data points according to the global mean and standard deviation or similar procedures. We can 16
interpret this as the most straight-forward way to construct local features informed by the global 17
structure of the data set. 18
In the case where not all data dimensions are equally relevant, or contain correlated and redundant 19
information, we can apply (sparse) PCA to project the data points to a lower dimensional space 20
using information about the global structure of the point cloud [ 51]. For even more complex data, 21
we may first have to learn the encoded structure itself: indeed, a typical assumption underpinning 22
many unsupervised learning methods is the so-called “manifold hypothesis” which posits that 23
real world data can be described well via submanifolds of n-dimensional space [ 36,21]. Using 24
eigenvectors of some Laplacian, we can then obtain a coordinate system intrinsic to the point cloud 25
(see e.g. [ 47,4,15]). Common to all these above examples is the goal is to construct locally 26
interpretable point-level features that encode globally meaningful positional information robust to 27
local perturbations of the data. However, none of these approaches is able to represent higher-order 28
topological information, making point clouds with these kind of structure inaccessible to point-level 29
machine learning algorithms. 30
Instead of focussing on the interpretation of individual points, topological data analysis ( TDA), [9], 31
follows a different approach. TDA extracts a global description of the shape of data, which is typically 32
considered in the form of a high-dimensional point cloud. This is done measuring topological features 33
like persistence homology, which counts the number of generalised “holes” in the point cloud on 34
multiple scales. Due to their flexibility and robustness these global topological features have been 35
shown to contain relevant information in a broad range of application scenarios: In medicine, TDA 36
has provided methods to analyse cancer progression [ 33]. In biology, persistent homology has been 37
used to analyse knotted protein structures [ 5], and the spectrum of the Hodge Laplacian has been 38
used for predicting protein behaviour [50]. 39
This success of topological data analysis is a testament to the fact that relevant information is encoded 40
in the global topological structure of point cloud data. Such higher-order topological information is 41
however invisible to standard tools of data analysis like PCA ork-means clustering, and can also not 42
be captured by graph models of the point cloud. We are now faced by a situation where (i)important 43
parts of the global structure of a complex point cloud can only be described by the language of 44
applied topology, however (ii)most standard methods to obtain positional point-level information are 45
not sensitive to the higher-order topology of the point cloud. 46
Contributions We introduce TOPF (Figure 1), a novel method to compute node-level topological 47
features relating individual points to global topological structures of point clouds. TOPF (i)outper- 48
forms other methods and embeddings for clustering downstream tasks on topologically structured data, 49
returns (ii)provably meaningful representations , and is (iii)robust to noise . Finally, we introduce the 50
topological clustering benchmark suite, the first benchmark for topological clustering. 51
Related Work The intersection of topological data analysis, topological signal processing and 52
geometry processing has many interesting related developments in the past few years. On the side 53
of homology and TDA, the authors in [ 16] and [ 41] use harmonic cohomology representatives to 54
reparametrise point clouds based on circular coordinates. This implicitly assumes that the underlying 55
structure of the point cloud is amenable to such a characterization. In [ 2,26], the authors develop and 56
use harmonic persistent homology for data analysis. However, among other differences their focus 57
is not on providing robust topological point features. [ 24] uses the harmonic space of the Hodge 58
Laplacians to cluster point clouds respecting topology, but is unstable against some form of noise, 59
has no possibility for features selection across scales and is computationally far more expensive than 60
TOPF . For a more in-depth review of related work, see Appendix A 61
Organisation of the paper In Section 2, we give an overview over the main ideas and concepts 62
behind of TOPF . In Section 3, we describe how to compute TOPF . In Section 4, we give a theoretical 63
2result guaranteeing the correctness of TOPF . Finally, we will apply TOPF on synthetic and real-world 64
data in Section 5. Furthermore, Appendix A contains a brief history of topology and a detailed 65
discussion of related work. Appendix B contains additional theoretical considerations, Appendix C 66
describes the novel topological clustering benchmark suite, Appendix D contains details on the 67
implementation and the choice of hyperparameters, Appendix E gives a detailed treatment of feature 68
selection, Appendix F discusses simplicial weights, and Appendix G discusses limitations in detail. 69
2 Main Ideas of TOPF 70
A main goal of algebraic topology is to capture the shape of spaces. Techniques from topology 71
describe globally meaningful structures that are indifferent to local perturbations and deformations. 72
This robustness of topological features to local perturbations is particularly useful for the analysis 73
of large-scale noisy datasets. To apply the ideas of algebraic topology in our TOPF pipeline, we 74
need to formalise and explain the notion of topological features . An important observation for 75
this is that high-dimensional point clouds and data may be seen as being sampled from topological 76
spaces — most of the time, even low-dimensional submanifolds of Rn[21]. 77
In this section we provide a broad overview over the most important concepts of topology and TDA 78
for our context, prioritising intuition over technical formalities. The interested reader is referred 79
to [7, 27, 49] for a complete technical account of topology and [38] for an overview over TDA. 80
Simplicial Complexes Spaces in topology are continuous , consist of infinitely many points, and 81
often live in abstract space . Our input data sets however consist of finitely many points embedded 82
inreal space Rn. In order to bridge this gap and open up topology to computational methods, we 83
need a notion of discretised topological spaces consisting of finitely many base points with finite 84
description length. A Simplicial Complex is the simplest discrete model that can still approximate 85
any topological space occuring in practice [43]: 86
Definition 2.1 (Simplicial complexes) .Asimplicial complex (SC)Sconsists of a set of vertices V 87
and a set of finite non-empty subsets (simplices, S) ofVclosed under taking non-empty subsets, such 88
that the union over all simplicesS
σ∈SσisV. In the following, we will often identify Swith its set 89
of simplicies Sand denote by Skthe set of simplices σ∈Swith|σ|=k+ 1, called k-simplices . We 90
say that Sisn-dimensional, where nis the largest ksuch that the set of k-simplices Skis non-empty. 91
Thek-skeleton ofSCcontains the simplices of dimension at most k. If the vertices Vlie in real space 92
Rn, we call the convex hull in Rnof a simplex σitsgeometric realisation |σ|. When doing this for 93
every simplex of S, we call this the geometric realisation of S,|S| ⊂Rn. 94
Concretely, we can construct an n-dimensional SCSinn+ 1steps: First, we start with a set of 95
vertices Vwhich we can identify with the 0-simplices S0. Second, we connect certain pairs of 96
vertices with edges, which constitute the set of 1-simplices. We can then choose to fill in some triples 97
of vertices which are fully connected by 1-simplices with triangles, i.e. 2-simplices. More generally, 98
in the kthstep, we can add a k-simplex for every set σkofk+ 1vertices such that every k-element 99
subset σk−1ofσkis already a (k−1)-simplex. 100
Vietoris–Rips and α-complexes We now need a way to construct a simplicial complex that 101
approximates the topological structure inherent in our data set X⊂Rn. Such a construction will 102
always depend on the scale of the structures we are interested in. When looking from a very large 103
distance, the point cloud will appear as a singular connected blob in the otherwise empty and infinite 104
real space, on the other hand when we continue to zoom in, the point cloud will at some point appear 105
as a collection of individual points separated by empty continuous space; all interesting information 106
can be found in-between these two extreme scales where some vertices are joined by simplices and 107
others are not. Instead of having to pick a single scale, the Vietoris–Rips ( VR) filtration and the 108
α-filtration take as input a point cloud and return a nested sequence of simplicial complexes indexed 109
by a scale parameter εapproximating the topology of the data across all possible scales. 110
Definition 2.2 (VRcomplex) .Given a finite point cloud Xin a metric space (M, d)and a non- 111
negative real number ε∈R≥0, the associated VRcomplex V Rε(X)is given by the vertex set Xand 112
the set of simplices S={σ⊂X|σ̸=∅,∀x, y∈σ:d(x, y)≤ε} 113
Intuitively, a VRcomplex with parameter εconsists of all simplices σwhere all vertices x∈σhave a 114
pair-wise distance of at most ε. Forr≤r′, we obtain the canonical inclusions ir,r′(X):V Rr(X),→ 115
3V Rr′(X). The set of VRcomplexes on Xfor all possible r∈R≥0together with the inclusions then 116
form the VRfiltration onX. For large point clouds, using the VRcomplex for computations becomes 117
expensive due to its large number of simplices. In contrast, the more sophisticated α-complex 118
approximates the topology of a point cloud using far fewer simplices and thus we will make use of it. 119
For a complete account and definition of α-complexes and our reason to use them, see Appendix B. 120
Boundary matrices So far, we have discussed a discretised version of topological spaces in the 121
form of SCs and a way to turn point clouds into a sequence of SCs indexed by a scale parameter. 122
However, we still need an algebraic representation of simplicial complexes that is capable of encoding 123
the structure of the SCand enables extraction of the topological features : The boundary matrices 124
Bkassociated to an SCSstore all structural information of SC. The rows of Bkare indexed by the 125
k-simplices of Sand the columns are indexed by the (k+ 1) -simplices. 126
Definition 2.3 (Boundary matrices) .LetSbe a simplicial complex and ⪯a total order on its vertices 127
V. Then, the i-th face map in dimension n fn
i:Sn→ S n−1is given by 128
fn
i:{v0, v1, . . . , v n} 7→ { v0, v1, . . . ,bvi, . . . , v n}
withv0⪯v1⪯ ··· ⪯ vnandbvidenoting the omission of vi. Now, the n-thboundary operator 129
Bn:R[Sn+1]→R[Sn]withR[Sn]being the real vector space over the basis Snis given by 130
Bn:σ7→n+1X
i=0(−1)ifn+1
i(σ).
When lexicographically ordering the simplex basis, we can view Bnas amatrix . We call R[Sn]the 131
space of n-chains. Now, B0is the vertex-edge incidence matrix of the associated graph consisting of 132
the0- and 1-simplices of SandB1is the edge-triangle incidence matrix of S 133
Figure 2: Sketch of Per-
sistent Homology, [ 23]Betti Numbers and Persistent Homology We now turn to the notion of 134
topological features and how to extract them. Homology is one of the main 135
algebraic invariants to capture the shape of topological spaces and SC. From 136
a technical point of view, the k-th homology module Hk(S)of an SCS 137
with boundary operators Bkis defined as Hk(S):= kerBk−1/ImBk. The 138
generator or representative of a homology class is an element of the kernel 139
kerBk−1. In dimension 1, these are given by formal sums of 1-simplices 140
forming closed loops in the SC. Importantly, the rank rkHk(S)is called 141
thek-thBetti number BkofS. In dimension 0,B0counts the number of 142
connected components, B1counts the number of loops around ‘holes’ of 143
the space, B2counts the number of 3-dimensional voids with 2-dimensional 144
boundary, and so on. 145
If we are now given a filtration of simplicial complexes instead of a single 146
SC, we can track how the homology modules evolve as the simplicial 147
complex grows. The mathematical formalisation, persistent homology , thus 148
turns a point cloud via a simplicial filtration into an algebraic object summarising the topological 149
feature of the point cloud. For better computational performance, the computations are usually done 150
in one of the small finite fields Z/pZ. Because we will later be interested in the sign of numbers 151
to distinguish different simplex orientations, we will use Z/3Z-coefficients, with Z/3Zbeing the 152
smallest field being able to distinguish 1and−1. 153
The Hodge Laplacian and the Harmonic Space In the previous part, we have introduced a 154
language to characterise the global shape of spaces and point clouds. However, we still need to find 155
a way to relate these global characterisations back to local properties of the point cloud. We will 156
do so by using ideas and concepts from differential geometry and topology: The simplicial Hodge 157
Laplacian is a discretisation of the Hodge–Laplace operator acting on differential forms of manifolds: 158
Definition 2.4 (Hodge Laplacian) .Given a simplicial complex Swith boundary operators Bk, we 159
define the n-th Hodge Laplacian Ln:R[Sn]→R[Sn]by setting 160
Ln:=B⊤
n−1Bn−1+BnB⊤
n.
The Hodge Laplacian gives rise to the Hodge decomposition theorem: 161
4Algorithm 1 Topological Point Features ( TOPF )
Input: Point cloud X∈Rn, maximum homology dimension d∈N, interpolation coeff. λ.
1.Compute persistent homology with generators in dimension k≤d.
2.Select set of significant features (bi, di, gi)with birth, death, and generator in F3coordinates.
3.Embed giinto real space and project into harmonic subspace of SCat step t=λbi+ (1−λ)di.
4.Normalise projections to ek
iand compute Fi
k(x):= avgx∈σ(ek
il(σ))for all points x∈X.
Output: Features of x∈X
2 4 6 8 10 12 14Birth2468101214 Death
Persistence diagram
1selected features
Figure 3: TOPF pipeline applied to NALCN channelosome, a membran protein [32]. Left: Steps
1&2a, when computing persistent 1-homology, three classes are more prominent than the rest. Centre:
Step 2b: The selected homology generators. Right: Step 3: The projections of the generators into
(weighted) harmonic are now each supported on one of the three rings.
Theorem 2.5 (Hodge Decomposition [ 34,46,44]).For an SCSwith boundary matrices (Bi)and 162
Hodge Laplacians (Li), we have in every dimension k 163
R[Sk] = Im B⊤
k−1|{z}
gradient space⊕kerLk|{z}
harmonic space⊕ImBk|{z}
curl space.
This, together with the fact that the k-th harmonic space is isomorphic to the k-th real-valued 164
homology group kerLk∼=Hk(R)means that we can associate a unique harmonic representative 165
to every homology class. The harmonic space encodes higher-order generalisations of smooth flow 166
around the holes of the simplicial complex. Intuitively, this means that for every abstract global 167
homology class of persistent homology from above we can now compute one unique harmonic 168
representative in kerLkthat assigns every simplex a value based on how much it contributes to the 169
homology class. Thus, the Hodge Laplacian is a gateway between the global topological features 170
and the local properties of our SC. It is easy to show that the kernel of the Hodge Laplacian is the 171
intersection of the kernel of the boundary and the coboundary map kerLk= kerBn−1∩kerB⊤
n. 172
Because we have finite SCs we can identify the spaces of chains and cochains. This leads to another 173
characterisation of the harmonic space: The space of chains that are simultaneously homology and 174
cohomology representatives. 175
3 How to Compute Topological Point Features 176
In this section, we will combine the ideas and insights of the previous section to give a complete 177
account of how to compute Topological point features ( TOPF ). A pseudo-code version can be found 178
in Algorithm 1 and an overview in Figure 1. We start with a finite point cloud X⊂Rn. 179
Step 1: Computing the persistent homology First, we need to determine the most significant 180
persistent homology classes which determine the shape of the point cloud. By doing this, we can 181
also extract the “interesting” scales of the data set. We will later use this to construct SCs to derive 182
local variants of the global homology features. Thus we first compute the persistent k-homology 183
modules Pkincluding a set of homology representatives RkofXusing an α-filtration for n≤3and 184
aVRfiltration for n >3. We use Z/3Zcoefficients to be sensitive to simplex orientations. In case we 185
have prior knowledge on the data set, we can choose a real number R∈R>0and only compute the 186
filtration and persistent homology connecting points up to a distance of at most R. In data sets like 187
protein atom coordinates, this might be useful as we have prior knowledge on what constitutes the 188
5“interesting” scale, reducing computational complexity. See Figure 3 leftfor a persistent homology 189
diagram. 190
Step 2: Selecting the relevant topological features We now need to select the relevant homology 191
classes which carry the most important global information . The persistent homology Pkmodule in 192
dimension kis given to us as a list of pairs of birth and death times (bk
i, dk
i). We can assume these 193
pairs are ordered in non-increasing order of the durations lk
i=dk
i−bk
i. This list is typically very 194
long and consists to a large part of noisy homological features which vanish right after they appear. 195
In contrast, we are interested in connected components, loops, cavities, etc. that persist over a long 196
time, indicating that they are important for the shape of the point cloud. Distinguishing between the 197
relevant and the irrelevant features is in general difficult and may depend on additional insights on 198
the domain of application. In order to provide a heuristic which does not depend on any a-priori 199
assumptions on the number of relevant features we pick the smallest quotient qk
i:=lk
i+1/lk
i>0 200
as the point of cut-off Nk:= arg miniqk
i. The only underlying assumption of this approach is that 201
the band of “relevant” features is separated from the “noisy” homological features by a drop in 202
persistence. If this assumption is violated, the only possible way to do meaningful feature selection 203
depends on application-specific domain knowledge. We found that our proposed heuristics work well 204
across a large scale of applications. See Figure 3 leftandcentre for an illustration and Appendix E 205
for more technical details and ways to improve and adapt the feature selection module of TOPF . We 206
call the chosen k-homology classes including k-homology generators in dimension fi
k. 207
Step 3: Projecting the features into harmonic space and normalising In this step, we need to 208
relate the global topology extracted in the previous step to the simplices which we will use to compute 209
thelocal topological point feature. Every selected feature fi
kof the previous step comes with a birth 210
timebi,kand a death time di,k. This means that the homology class fi
kis present in every SCof 211
the filtration between step ε=bi,kandε=di,kand we could choose any of the SCs for the next 212
step. Picking a small εwill lead to fewer simplices in the SCand thus to a very localised harmonic 213
representative. Picking a large εwill lead to many simplices in the SCand thus to a very smooth 214
and “blurry” harmonic representative with large support. Finding a middle ground between these 215
regimes returns optimal results. For the interpolation parameter γ∈(0,1), we will thus consider the 216
simplicial complex Sti,k(X)at step ti,k:=b1−γ
i,kdγ
i,kfork >0and at step ti,k:=γdi,kfork= 0 217
of the simplicial filtration. At this point, the homology class fi
kis still alive. We then consider the 218
real vector space R[Sti,k
k(X)]with formal basis consisting of the k-simplices of the SCSti,k. From 219
the persistent homology computation of the first step, we also obtain a generator of the feature fi
k, 220
consisting of a list Σi
kof simplices ˆσj∈ Sbi,k
kand coefficients cj∈Z/3Z. We need to turn this 221
formal sum of simplices with Z/3Z-coefficients into a vector in the real vector space R[Sti,k
k(X)]: 222
Letι:Z/3Zbe the map induced by the canonical inclusion of {−1,0,1},→R. We can now define 223
an indicator vector ei
k∈R[Sti,k
k(X)]associated to the feature fi
k. 224
ei
k(σ):=ι(cj)∃ˆσj∈Σi
k:σ= ˆσj
0 else.
While this homology representative lives in a real vector space, it is not unique, has a small support, 225
and can differ largely between close simplices. All of these problems can be solved by projecting 226
the homology representative to the harmonic subspace kerLkofR[Sti,k
k(X)]. Rather than directly 227
projecting ei
kto the harmonic subspace, we make use of the Hodge decomposition theorem (The- 228
orem 2.5) which allows us to compute the gradient and curl projections solving computationally 229
efficient least square problems: 230
ei
k,grad:=B⊤
k−1arg min
x∈R[Sk−1]ei
k− B⊤
k−1x2
2and ei
k,curl:=Bkarg min
x∈R[Sk+1]ei
k−ei
k,grad− Bkx2
2
and then setting ˆei
k:=ei
k−ei
k,grad−ei
k,curl. (Cf. Figure 3 right for a visualisation.) Because homology 231
representatives are gradient-free, we only need to consider the projection of ei
kinto the curl space. 232
Step 4: Processing and aggregation at a point level In the previous step, we have computed 233
a set of simplex-valued harmonic representatives of homology classes. However, these simplices 234
likely have no real-world meaning and the underlying simplicical complexes differ depending 235
on the birth and death times of the homology classes. Hence in this step, we will collect the 236
6features on the point-level after performing some necessary preprocessing. Given a simplex-valued 237
vector ˆei
kand a hyperparameter δ, we now construct ei
k:Sti,k
k(X)→[0,1]by setting ei
k:σ7→∈ 238
{|ˆei
k(σ)|/(δmaxσ′∈Sti,k
k(X)|ˆei
k(σ′)|),1}such that ˆei
kis normalised to [0,1], the values of [0, δ]are 239
mapped linearly to [0,1]and everything above is sent to 1. We found empirically that a thresholding 240
parameter of δ= 0.07works best across at the range of applications considered below. However, 241
TOPF is not sensitive to small changes to δbecause entries of ˆei
kare concentrated around 0. 242
For every feature fi
kin dimension kwith processed simplicial feature vector ei
kand simplicial 243
complex Sti,k, we define the point-level feature map Fk
i:X→Rmapping from the initial point 244
cloud XtoRby setting 245
Fk
i:v7→P
σk∈Sti,k
k:v∈σkei
k(σk)
max(1 ,|{σk∈ St
k:v∈σk}|).
For every point v, we can thus view the vector (Fk
i(v):fk
i∈ F)as a feature vector for v. We call 246
this collection of features Topological Point Features (TOPF ). (Cf. Figure 4 for an example). 247
Choosing Simplicial Weights By default, the simplicial complexes of α- and VRfiltrations are 248
unweighted. However, the weights determine the entries of the harmonic representatives, increasing 249
and decreasing the influence of certain simplices and parts of the simplicial complex. We can use this 250
observation to increase the robustness of TOPF against the influence of heterogeneous point cloud 251
structure, which is present in virtually all real-world data sets. For a complete technical account of 252
how and why we do this, see Appendix F. 253
4 Theoretical guarantees 254
In this section, we prove the relationship between TOPF and actual topological structure in datasets: 255
Theorem 4.1 (Topological Point Features of Spheres) .LetXconsist of at least (n+ 2) points 256
(denoted by S) sampled uniformly at random from a unit n-sphere in Rn+1and an arbitrary number 257
of points with distance of at least 2toS. When we now consider the α-filtration on this point 258
cloud, with probability 1we have that (i)there exists an n-th persistent homology class generated 259
by the 2-simplices on the convex hull hull of S,(ii)the associated unweighted harmonic homology 260
representative takes values in {0,±1}where the 2-simplices on the boundary of the convex hull are 261
assigned a value of ±1, and (iii)the support of the associated topological point feature ( TOPF )F∗
n262
is precisely S:supp(F∗
n) =S.(iv)The same holds true for point clouds sampled from multiple 263
ni-spheres if the above conditions are met on each individual sphere. 264
We will give a proof of this theorem in Appendix B. 265
Remark 4.2.In practice, datasets with topological structure consist in a majority of cases of points 266
sampled with noise from deformed n-spheres. The theorem thus guarantees that TOPF will recover 267
these structural information in an idealised setting. Experimental evidence suggests that this holds 268
under the addition of noise as well which is plausible as harmonic persistent homology is robust 269
against some noise [2]. 270
5 Experiments 271
In this section, we conduct experiments on real world and synthetic data, compare the clustering 272
results with clustering by TPCC , other classical clustering algorithms, and other point features, and 273
demonstrate the robustness of TOPF against noise. 274
Topological Point Cloud Clustering Benchmark We introduce the topological clustering bench- 275
mark suite (Appendix C) and report running times and the accuracies of clustering based on TOPF 276
and other methods and point embeddings, see Table 1. We see that TOPF outperforms all classical 277
clustering algorithms on all but one dataset by a wide margin. We also see that TOPF closely matches 278
the performance of the only other higher-order topological clustering algorithm, TPCC on two datasets 279
with clear topological features, whereas TOPF outperforms TPCC on datasets with more complex 280
structure. In addition, TOPF has a consistently lower running time with better scaling for the more 281
7Table 1: Quantitative performance comparison of clustering with TOPF and other fea-
tures/clustering algorithms. Four2Dand three 3Ddata sets of the topological clustering benchmark
suite (Appendix C, cf. Figure 6 for ground truth labels and Figure 7 for clustering results of TOPF ).
We ran each algorithm 20times and list the mean adjusted rand index ( ARI) with standard deviation
σand mean running time. We omit σfor algorithms with σ= 0on every dataset. TOPF consistently
outperforms or almost matches the other algorithms while having significantly better run time than
the second best performing algorithm TPCC . Spectral Clustering ( SC),DBSCAN , and Agglomerative
Clustering (AgC) are standard clustering algorithms, ToMATo is a topological clustering algorithm
[11], Geo clusters using 12-dimensional point geometric features extracted by pgeof and the normal
point coordinates, whereas node2vec [ 25] produces node embeddings on a k-nearest neighbour graph
built upon an affinity matrix. We highlight all ARIscores within ±0.05of the best ARIscore.
TOPF (ours) TPCC SC DBSCAN AgC ToMATo Geo node2vec
4spheres ARI 0.81 0.52±0.17 0.37 0.00 0.45 0.32 0.20 0.00 ±0.00
time (s) 14.5 23.3 0.2 0.0 0.0 0.0 0.2 48.4
Ellipses ARI 0.95 0.47±0.04 0.25 0.19 0.52 0.29 0.81 0.02 ±0.00
time (s) 12.7 14.4 0.1 0.0 0.0 0.0 0.1 11.2
Spheres+Grid ARI 0.70 0.39 ±0.04 0.90 0.92 0.89 0.82 0.41 0.01 ±0.00
time (s) 13.0 28.5 0.5 0.0 0.0 0.0 0.3 63.8
Halved Circle ARI 0.71 0.18±0.12 0.24 0.00 0.20 0.16 0.08 0.00 ±0.01
time (s) 12.2 14.3 0.1 0.0 0.0 0.0 0.1 18.2
2Spheres2Circles ARI 0.94 0.97 ±0.01 0.70 0.00 0.51 0.87 0.12 0.00 ±0.00
time (s) 38.9 1662.2 1.6 0.0 0.3 0.0 0.9 348.6
SphereinCircle ARI 0.97 0.98 ±0.0 0.34 0.00 0.29 0.06 0.69 0.13 ±0.03
time (s) 14.5 8.0 0.0 0.0 0.0 0.0 0.08 20.1
Spaceship ARI 0.92 0.56±0.03 0.28 0.26 0.47 0.30 0.87 0.07±0.00
time (s) 16.3 341.8 16.7 0.0 0.0 0.0 0.2 49.8
mean ARI 0.86 0.58 0.44 0.16 0.48 0.40 0.45 0.03
time (s) 17.5 298.9 0.4 0.0 0.0 0.0 0.3 80.0
Figure 4: TOPF on3Dreal-world and synthetic point clouds. For every point, we highlight the
largest corresponding topological feature, where colour stands for the different features and saturation
for the value of the feature. (a): Atoms of mutated Cys123 of E. coli [ 29]. We added auxiliary
points on the convex hull and considered 2-homology, to detect the protein pockets which are crucial
for protein-environment interactions (Cf. [ 40]).(b):Atoms of NALCN Channelosome [ 32] display
three distinct loops. (c):Points sampled in the state space of a Lorentz attractor. The two features
correspond to the two lobes of the attractor. (d):Point cloud spaceship of our newly introduced
topological clustering benchmark suite (See Appendix C).
80.0
0.2
0.4
0.6
0.8
1.0noise
0.0
0.2
0.4
0.6
0.8
1.0ARI
Method
TOPF untuned
TOPF tuned
KMeans
Spectral Clustering
TPCC
Spectral on SC
0
100
200
300
400
500
600
No. Outliers
0.0
0.2
0.4
0.6
0.8
1.0ARI
Figure 5: Performance of Clustering based on TOPF features in increasing noise/outlier levels
with 95% CI.Left: We add i.i.d. Gaussian noise to every point with standard deviation indicated by
thenoise parameter. We see that even when compared with TPCC on a data set specifically crafted
forTPCC ,TOPF requires significantly less information and delivers almost equal performance. When
tuned for datasets with a high noise level, the TOPF even outperform TPCC and drastically outperform
all classical clustering algorithms. Right: We add outliers with the same standard deviation as the
point cloud to the data set. We then measure the adjusted rand index obtained restricted on the
original points. We see that even when compared with TPCC on a data set specifically crafted for
TPCC ,TOPF requires significantly less information and delivers matching to superior performance,
significantly outperforming all other classical clustering algorithms.
complex datasets, while also not requiring prior knowledge on the best topological scale. As for the 282
other point embeddings, Node2Vec is not able to capture any meaningful topological information, 283
whereas the performance of clustering using geometric features depends on the data set. 284
Feature Generation In Figure 4, we show qualitatively that TOPF constructs meaningful topological 285
features on data sets from Biology and Physics, and synthetic data, corresponding to for example 286
rings and pockets in proteins or trajectories around different attractors in dynamical systems. (For 287
individual heatmaps see Figure 8) 288
Robustness against noise We have evaluated the robustness of TOPF against Gaussian noise on 289
the dataset introduced in [ 24] and compared the results against TPCC , Spectral Clustering, Graph 290
Spectral Clustering on the graph constructed by TPCC , and against k-means in Figure 5 Left. We have 291
also analysed the robustness of TOPF against the addition of outliers in Figure 5 Right . We see that 292
TOPF performs well in both cases, underlining our claim of robustness. 293
6 Discussion 294
Limitations TOPF can — by design — only produce meaningful output on point clouds with a 295
topological structure quantifiable by persistent homology. In practice it is thus desirable to combine 296
TOPF with some geometric or other point-level feature extractor. As TOPF relies on the computation of 297
persistent homology, its runtime increases on very large point clouds, especially in higher dimensions 298
where α-filtrations are computationally infeasible. However, subsampling, either randomly or using 299
landmarks, usually preserves relevant topological features while improving run time [ 41]. Finally, 300
selection of the relevant features is a very hard problem. While our proposed heuristics work well 301
across a variety of domains and application scenarios, only domain- and problem-specific knowledge 302
makes correct feature selection feasible. 303
Future Work The integration of higher-order TOPF features into MLpipelines that require point- 304
level features potentially leads to many new interesting insights across the domains of biology, drug 305
design, graph learning and computer vision. Furthermore, efficient computation of simplicial weights 306
leading to the provably most faithful topological point features is an exciting open problem. 307
Conclusion We introduced point-level features TOPF founded on algebraic topology relating global 308
structural features to local information. We gave theoretical guarantees for the correctness of their 309
construction and evaluated them quantitatively and qualitatively on synthetic and real-world data sets. 310
Finally, we introduced the novel topological clustering benchmark suite and showed that clustering 311
using TOPF outperforms other available clustering methods and features extractors. 312
9References 313
[1] Michael Atiyah. K-theory . CRC press, 1989. 314
[2]Saugata Basu and Nathanael Cox. Harmonic persistent homology. In 2021 IEEE 62nd Annual 315
Symposium on Foundations of Computer Science (FOCS) , pages 1112–1123. IEEE, 2022. 316
[3]Ulrich Bauer. Ripser: efficient computation of vietoris–rips persistence barcodes. Journal of 317
Applied and Computational Topology , 5(3):391–423, 2021. 318
[4]Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data 319
representation. Neural computation , 15(6):1373–1396, 2003. 320
[5]Katherine Benjamin, Lamisah Mukta, Gabriel Moryoussef, Christopher Uren, Heather A 321
Harrington, Ulrike Tillmann, and Agnese Barbensi. Homology of homologous knotted proteins. 322
Journal of the Royal Society Interface , 20(201):20220727, 2023. 323
[6]A. K. Bousfield. The localization of spaces with respect to homology. Topology , 14(2):133–150, 324
1975. 325
[7]G.E. Bredon, J.H. Ewing, F.W. Gehring, and P.R. Halmos. Topology and Geometry . Graduate 326
Texts in Mathematics. Springer, New York, 1993. 327
[8]Peter Bubenik et al. Statistical topological data analysis using persistence landscapes. J. Mach. 328
Learn. Res. , 16(1):77–102, 2015. 329
[9]Gunnar Carlsson and Mikael Vejdemo-Johansson. Topological Data Analysis with Applications . 330
Cambridge University Press, 2021. 331
[10] Charu Chaudhry, Arthur L Horwich, Axel T Brunger, and Paul D Adams. Exploring the struc- 332
tural dynamics of the e. coli chaperonin groel using translation-libration-screw crystallographic 333
refinement of intermediate states. Journal of molecular biology , 342(1):229–245, 2004. 334
[11] Frédéric Chazal, Leonidas J. Guibas, Steve Y . Oudot, and Primoz Skraba. Persistence-based 335
clustering in riemannian manifolds. J. ACM , 60(6), nov 2013. 336
[12] Frédéric Chazal and Bertrand Michel. An introduction to topological data analysis: fundamental 337
and practical aspects for data scientists. Frontiers in artificial intelligence , 4:108, 2021. 338
[13] Yu-Chia Chen and Marina Meil ˘a. The decomposition of the higher-order homology embedding 339
constructed from the k-laplacian. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and 340
J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, 341
pages 15695–15709. Curran Associates, Inc., 2021. 342
[14] Yu-Chia Chen, Marina Meil ˘a, and Ioannis G Kevrekidis. Helmholtzian eigenmap: Topological 343
feature discovery & edge flow learning from point cloud data. arXiv preprint arXiv:2103.07626 , 344
2021. 345
[15] Ronald R Coifman and Stéphane Lafon. Diffusion maps. Applied and computational harmonic 346
analysis , 21(1):5–30, 2006. 347
[16] Vin De Silva and Mikael Vejdemo-Johansson. Persistent cohomology and circular coordinates. 348
InProceedings of the twenty-fifth annual symposium on Computational geometry , pages 227– 349
236, 2009. 350
[17] Richard Dedekind. Was sind und was sollen die Zahlen? Verlag Friedrich Vieweg und Sohn, 351
Braunschweig, 1888. 352
[18] Boris Delaunay et al. Sur la sphere vide. Izv. Akad. Nauk SSSR, Otdelenie Matematicheskii i 353
Estestvennyka Nauk , 7(793-800):1–2, 1934. 354
[19] Stefania Ebli and Gard Spreemann. A notion of harmonic clustering in simplicial complexes. 355
In2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA) , 356
pages 1083–1090, 2019. 357
10[20] Samuel Eilenberg and Saunders MacLane. General theory of natural equivalences. Transactions 358
of the American Mathematical Society , 58:231–294, 1945. 359
[21] Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis. 360
Journal of the American Mathematical Society , 29(4):983–1049, Oct 2016. 361
[22] David Chin-Lung Fong and Michael Saunders. Lsmr: An iterative algorithm for sparse least- 362
squares problems. SIAM Journal on Scientific Computing , 33(5):2950–2971, 2011. 363
[23] Vincent P. Grande and Michael T Schaub. Non-isotropic persistent homology: Leveraging the 364
metric dependency of ph. In Learning on Graphs Conference , pages 17–1. PMLR, 2023. 365
[24] Vincent P. Grande and Michael T. Schaub. Topological point cloud clustering. In Proceedings 366
of the 40th International Coference on Machine Learning , ICML’23, 2023. 367
[25] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In 368
Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and 369
data mining , pages 855–864, 2016. 370
[26] Davide Gurnari, Aldo Guzmán-Sáenz, Filippo Utro, Aritra Bose, Saugata Basu, and Laxmi 371
Parida. Probing omics data via harmonic persistent homology. arXiv preprint arXiv:2311.06357 , 372
2023. 373
[27] Allen Hatcher. Algebraic Topology . Cambridge University Press, Cambridge, 2002. 374
[28] Felix Hausdorff. Grundzüge einer theorie der geordneten mengen. Mathematische Annalen , 375
65:435–505, 1908. 376
[29] Esther Hidber, Edward R Brownie, Koto Hayakawa, and Marie E Fraser. Participation of 377
cys123 αof escherichia coli succinyl-coa synthetase in catalysis. Acta Crystallographica 378
Section D: Biological Crystallography , 63(8):876–884, 2007. 379
[30] David Hilbert. Grundlagen der Geometrie . Wissenschaft und Hypothese. B. G. Teubner, 380
Leipzig, 1899. 381
[31] Sze-tsen Hu. Homotopy theory . Academic press, 1959. 382
[32] Marc Kschonsak, Han Chow Chua, Claudia Weidling, Nourdine Chakouri, Cameron L. Noland, 383
Katharina Schott, Timothy Chang, Christine Tam, Nidhi Patel, Christopher P. Arthur, Alexan- 384
der Leitner, Manu Ben-Johny, Claudio Ciferri, Stephan Alexander Pless, and Jian Payandeh. 385
Structural architecture of the human nalcn channelosome. Nature , 603(7899):180–186, Mar 386
2022. 387
[33] Peter Lawson, Andrew B Sholl, J Quincy Brown, Brittany Terese Fasy, and Carola Wenk. 388
Persistent homology for the quantitative evaluation of architectural features in prostate cancer 389
histology. Scientific reports , 9(1):1139, 2019. 390
[34] Lek-Heng Lim. Hodge laplacians on graphs. SIAM Review , 62(3):685–715, 2020. 391
[35] Jacob Lurie. Stable infinity categories. arXiv preprint math/0608228 , 2006. 392
[36] Yunqian Ma and Yun Fu. Manifold learning theory and applications , volume 434. CRC press 393
Boca Raton, 2012. 394
[37] Facundo Mémoli, Zhengchao Wan, and Yusu Wang. Persistent laplacians: Properties, algorithms 395
and implications. SIAM Journal on Mathematics of Data Science , 4(2):858–884, 2022. 396
[38] Elizabeth Munch. A user’s guide to topological data analysis. Journal of Learning Analytics , 397
4(2):47–61, 2017. 398
[39] Kevin P. Murphy. Probabilistic Machine Learning: An introduction . MIT Press, 2022. 399
[40] Haruhisa Oda, Mayuko Kida, Yoichi Nakata, and Hiroki Kurihara. Novel definition and quantita- 400
tive analysis of branch structure with topological data analysis. arXiv preprint arXiv:2402.07436 , 401
2024. 402
11[41] Jose A. Perea. Sparse circular coordinates via principal Z-bundles. In Nils A. Baas, Gunnar E. 403
Carlsson, Gereon Quick, Markus Szymik, and Marius Thaule, editors, Topological Data 404
Analysis , pages 435–458, Cham, 2020. Springer International Publishing. 405
[42] Henri Poincaré. Analysis situs. J. de l’Ecole Poly. , 1, 1895. 406
[43] Daniel G. Quillen. Homotopical Algebra , volume 43 of Lecture Notes in Mathematics . Springer, 407
Berlin, 1967. 408
[44] T Mitchell Roddenberry, Nicholas Glaze, and Santiago Segarra. Principled simplicial neural 409
networks for trajectory prediction. In International Conference on Machine Learning , pages 410
9020–9029. PMLR, 2021. 411
[45] Michael T Schaub, Austin R Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. Random 412
walks on simplicial complexes and the normalized hodge 1-laplacian. SIAM Review , 62(2):353– 413
391, 2020. 414
[46] Michael T. Schaub, Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra. 415
Signal processing on higher-order networks: Livin’ on the edge... and beyond. Signal Processing , 416
187:108149, 2021. 417
[47] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions 418
on pattern analysis and machine intelligence , 22(8):888–905, 2000. 419
[48] The GUDHI Project. GUDHI User and Reference Manual . GUDHI Editorial Board, 2015. 420
[49] Tammo tom Dieck. Algebraic topology , volume 8. European Mathematical Society, Zürich, 421
2008. 422
[50] JunJie Wee, Jiahui Chen, Kelin Xia, and Guo-Wei Wei. Integration of persistent laplacian and 423
pre-trained transformer for protein solubility changes upon mutation. Computers in Biology 424
and Medicine , page 107918, 2024. 425
[51] Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal 426
of computational and graphical statistics , 15(2):265–286, 2006. 427
[52] Matija ˇCufar. Ripserer.jl: flexible and efficient persistent homology computation in julia. 428
Journal of Open Source Software , 5(54):2614, 2020. 429
A Extended Background 430
A brief history of topology and machine learning Algebraic topology is a discipline of Mathe- 431
matics dating back roughly to the late 19thcentury [ 42]. Starting with Henri Poincaré and continuing 432
in the early 20thcentury, the mathematical community became interested in developing a framework 433
to capture the global shapes of manifolds and topological spaces in concise algebraic terms. This de- 434
velopment was partly made possible by the push towards a formalisation of mathematics and analysis, 435
in particular, which took place inside the mathematical community in the 1800’s and early 1900’s (e.g. 436
[17,30,28]). The axiomatisation of analysis in the early 20thcentury is an important result of this 437
process. These abstract ideas made it possible for Topologists to talk about the now common notions 438
of Euler characteristics, Betti number, simplicial homology of manifolds, topological spaces, and 439
simplicial and CWcomplexes. Over the course of the last 100 years, branching into many sub-areas 440
like low-dimensional topology, differential topology, K-theory or homotopy theory [ 1,31], algebraic 441
topology has resolved many of the important questions and provides a comprehensive tool-box for 442
the study of topological spaces. These achievements were tied to an abstraction and generalisation of 443
concepts: topological spaces turned into spectra, diffeomorphism to homotopy equvialences and later 444
weak equivalences, and Topologists turned to category theory [ 20], model categories [ 6] and recently 445
∞-categories [35] as the language of choice. 446
The 21stcentury saw the advent and rise of topological data analysis ( TDA, [8,12]). In short, 447
mathematicians realised that the same notions of shape and topology that their predecessors carefully 448
defined a century earlier were now characterising the difference between healthy and unhealthy 449
tissue, between normal and abnormal behaviour protein behaviour, or more general between different 450
categories in their complex data sets. 451
12Related Work The intersection of topological data analysis, topological signal processing and 452
geometry processing has many interesting related developments in the past few years. On the side 453
of homology and TDA, the authors in [ 16] and [ 41] use harmonic cohomology representatives to 454
reparametrise point clouds based on circular coordinates. This implicitly assumes that the underlying 455
structure of the point cloud is amenable to such a characterization. Although circular coordinates are 456
orthogonal to the core goal of TOPF , the approaches share many key ideas and insights. In [ 2,26], 457
the authors develop and use harmonic persistent homology and provide a way to pool features to 458
the point-level. However, their focus is not on providing robust topological point features and their 459
approach includes no tunable homology feature selection across dimensions, no support for weighted 460
simplicial complexes, and they only construct the simplicial complex at birth. In their paper on 461
topological mode analysis, [ 11] use persistent homology to cluster point clouds. However, they only 462
consider 0-dimensional homology to base the clustering on densities and there is no clear way to 463
generalise this to higher dimensions. 464
On the more geometric-centred side, [ 19] already provide a notion of harmonic clustering on simplices, 465
[13,14] analyse the notion of geometry and topology encoded in the Hodge Laplacian and its relation 466
to homology decompositions, [ 45] study the normalised and weighted Hodge Laplacian in the context 467
of random walks, and [ 24] use the harmonic space of the Hodge Laplacians to cluster point clouds 468
respecting topology. Finally, a persistent variant of the Hodge Laplacian is used to study filtrations of 469
simplicial complexes [37]. 470
In [24], the authors have introduced TPCC , the first method to cluster a point cloud based on the 471
higher-order topological features encoded in the data set. However, TPCC is(i)computationally 472
expensive due to extensive eigenvector computations, (ii)depending on high-dimensional subspace 473
clustering algorithms, which are prone to instabilities and errors, (iii)sensitive to the correct choice 474
of hyperparameters, (iv)requiring the topological true features and noise to occur in different steps 475
of the simplicial filtration, and it (v)solely focussed on clustering the points rather than extracting 476
relevant node-level features. This paper solves all the above by completely revamping the TPCC 477
pipeline, introducing several new ideas from applied algebraic topology and differential geometry. 478
The core insight is: When you have the time to compute persistent homology with generators on a 479
data set, you get the topological node features with similar computational effort. 480
B Theoretical Considerations 481
More details on VRandα-filtrations Vietoris–Rips complexes are easy to define, approximate 482
the topological properties of a point cloud across all scales and computationally easy to implement. 483
However for moderately large r, the associated VRcomplex contains a large number of simplices — 484
up to |X|
n
n-simplices for large enough r— leading to poor computational performance for any 485
downstream task on some large point clouds. One way to see this is the following: After adding the 486
first edge that connects two components or the final simplex that fills a hole in the simplicial complex 487
theVRcomplex keeps adding more and more simplices in the same area that keep the topology 488
unchanged. One way to mitigate this problem is to pre-compute a set of simplices that are able to 489
express the entire topology of the point cloud. For a point cloud X⊂Rn, theα-filtration consists of 490
the intersection of the simplicial complexes of the VRfiltration on Xwith the (higher-dimensional) 491
Delaunay triangulation of XinR. Due to algorithmic reasons, the filtration value of a simplex is 492
then the radius of the circumscribed sphere instead of the maximum pair-wise distance of vertices. 493
This reduces the number of required simplices across all dimensions to O(|X|⌈n/2⌉). However, the 494
Delaunay triangulation becomes computationally infeasible for larger n. 495
Definition B.1 (n-dimensional Delaunay triangulation) .Given a set of vertices V∈Rn, a Delaunay 496
triangulation DT(V)is a triangulation of Vsuch that for any n-simplex σn∈DT(V)the interior 497
of the circum-hypersphere of σncontains no point of DT(V). A triangulation of Vis aSCSwith 498
vertex set Vsuch that its geometric realisation covers the convex hull of Vhull(V) =|S|and we 499
have for any two simplices σ,σ′that the intersection of geometric realisations |σ| ∩ |σ′|is either 500
empty or the geometric realisation |ˆσ|of a common sub-simplex ˆσ⊂σ, σ′. 501
IfVis in general position, the Delaunay triangulation is unique and guaranteed to exist [18]. 502
Definition B.2 (α-complex of a point cloud) .Given a finite point cloud Xin real space Rn, the 503
α-complex αε(X)is the subset of the n-dimensional Delaunay triangulation DT(X)consisting of 504
allσ∈DT(X)with a radius rof its circumscribed sphere with r≤ε. 505
13Proof of the main theorem We will now give the proof of the theorem that guarantees that TOPF 506
works. First, let us recall Theorem 4.1: 507
Theorem 4.1 (Topological Point Features of Spheres) .LetXconsist of at least (n+ 2) points 508
(denoted by S) sampled uniformly at random from a unit n-sphere in Rn+1and an arbitrary number 509
of points with distance of at least 2toS. When we now consider the α-filtration on this point 510
cloud, with probability 1we have that (i)there exists an n-th persistent homology class generated 511
by the 2-simplices on the convex hull hull of S,(ii)the associated unweighted harmonic homology 512
representative takes values in {0,±1}where the 2-simplices on the boundary of the convex hull are 513
assigned a value of ±1, and (iii)the support of the associated topological point feature ( TOPF )F∗
n514
is precisely S:supp(F∗
n) =S.(iv)The same holds true for point clouds sampled from multiple 515
ni-spheres if the above conditions are met on each individual sphere. 516
Proof. Assume that we are in the scenario of the theorem. Now because the n-volume of (n−1)- 517
submanifolds is zero, we have that with probability 1the points of Sdon’t lie on a single (n−1) 518
sphere inside the n-sphere. Let us now look at the α-filtration of the simplices in S: Recall that the 519
filtration values of a k-simplex is given by the radius of the (k−1)-sphere determined by its vertices. 520
Because all of the (n+ 1)-simplices σn+1with vertices V⊂SinSlie on the same unit n-sphere Sn, 521
they all share the filtration value of α(σn+1) = 1 . By the same argument as above, with probability 1 522
there are no (n+ 1) points in Sthat lie on an unit(n−1)-sphere. Thus all of the n-simplices σnlie 523
on(n−1)-spheres Snwith a radius r <1smaller than 1and hence have a filtration value α(σn) 524
smaller than 1. Let 525
b:= max ( {α(σn) :σn⊂∂hull(S)})
be the maximum filtration value of an n-simplex on the boundary of the convex hull of S. Then, then 526
a linear combination gof the n-simplices of the boundary of the convex hull of Swith coefficients in 527
±1is a generator of a persistent homology class with life time (b,1)(this follows from the fact that 528
n-spheres and their triangulations are orientable). This proves claim (i). 529
Because of the assumption that all points not contained in Shave a distance of at least 2to the points 530
inS, all(n+1)-simplices σn+1with vertices both in Sand its complement in Xwill have a filtration 531
value α(σn+1)≥1of at least 1. Recall that all (n+ 1) -simplices σn+1⊂Swith vertices inside S 532
have a filtration value of α(σn+1) = 1 . Thus the adjoint of the n-th boundary operator B⊤
nis trivial 533
on the homology generator g. Thus, we have that for the n-th Hodge Laplacian 534
Lng=B⊤
n−1Bn−1g+BnB⊤
ng= 0 + 0 = 0
and hence gis a harmonic generator for the entire filtration range of (b,1), which proves claim (ii). 535
Claim (iii)and(iv)then follow from the construction of the TOPF values. □ 536
C Topological Clustering Benchmark Suite 537
We introduce seven point clouds for topological point cloud clustering in the topological clustering 538
benchmark suite ( TCBS ). The ground truth and the point clouds are depicted in Figure 6. The point 539
clouds represent a mix between 0-,1- and 2-dimensional topological structures in noiseless and noisy 540
settings in ambient 2-dimensional and 3-dimensional space. The results of clustering according to 541
TOPF can be found in Figure 7. 542
D Implementation 543
We will release an implementation of TOPF and the code and data required to reproduce 544
the experimental results of this paper under https://anonymous.4open.science/r/topf_ 545
submission-5C40/ . In particular, we will release the topological clustering benchmark suite. 546
All experiments were run on a Apple M1 Pro chipset with 10cores and 32GB memory. TOPF 547
and the experiments are implemented in Python and Julia. For persistent homology computations, 548
we used GUDHI [48] (©The GUDHI developers, MIT license) and Ripserer [ 52] (©mtsch, MIT 549
license), which is a modified Julia implementation of [ 3]. For the least square problems, we used 550
theLSMR implementation of SciPy [ 22]. We used the Node2Vec python implementation https: 551
//github.com/eliorc/node2vec (©Elior Cohen, MIT License ) based on the Node2Vec Paper 552
[25]. We used the pgeof Python package for computation of geometric features https://github. 553
14Figure 6: Data sets of the Topological Clustering Benchmark Suite ( TCBS ) with true la-
bels. Top: 2Ddata sets. From left to right: 4Spheres (656 points), Ellipses (158 points),
Spheres+Grid (866 points), Halved Circle (249 points). Bottom: 3Ddata sets. From left to
right: 2Spheres2Circles (4600 points), SphereinCircle (267 points), spaceship (650 points).
Figure 7: Data sets of the Topological Clustering Benchmark Suite ( TCBS ) with labels generated
byTOPF .Top: 2Ddata sets. From left to right: 4Spheres (0.81 ARI),Ellipses (0.95 ARI),
Spheres+Grid (0.70 ARI),Halved Circle (0.71 ARI).Bottom: 3Ddata sets. From left to right:
2Spheres2Circles (0.94 ARI),SphereinCircle (0.97 ARI),spaceship (0.92 ARI).
com/drprojects/point_geometric_features (©Damien Robert, Loic Landrieu, Romain Jan- 554
vier, MIT license). We use parts of the implementation of TPCC https://git.rwth-aachen. 555
de/netsci/publication-2023-topological-point-cloud-clustering (©Computational 556
Network Science Group, RWTH Aachen University, MIT license). 557
D.1 Hyperparameters 558
All the relevant hyperparameters are already mentioned in their respective sections. However, for 559
convenience we gather and briefly discuss them in this section. We note that TOPF is robust and 560
applicable in most scenarios when using the default parameters without tuning hyperparameters. The 561
hyperparameters should more be thought of as an additional way where detailed domain-knowledge 562
can enter the TOPF pipeline. 563
15Maximum Homology Dimension dThe maximum homology dimension determines the dimen- 564
sions of persistent homology the algorithm computes. 565
For the choice of the maximum homology degree dto be considered there are mainly three heuristics 566
which we will list in decreasing importance (Cf. [24]): 567
I.In applications, we usually know which kind of topological features we are interested in, which 568
will then determine d. This means that 1-dimensional homology and d= 1suffices when we 569
are looking at loops of protein chains. On the other hand, if we are working with voids and 570
cavities in 3d histological data, we need d= 2and thus compute 2-dimensional homology. 571
II.Algebraic topology tells us that there are no closed n-dimensional submanifolds of Rn. Hence 572
their top-homology will always vanish and all interesting homological activity will appear for 573
d < n . 574
III.In the vast majority of cases, the choice will be between d= 1ord= 2because empirically 575
there are virtually no higher-dimensional topological features in practice. 576
In our quantitative experiments, we have always chosen d=n−1. 577
Thresholding parameter δIn step 4 of the algorithm, we normalise and threshold the harmonic 578
representatives. After normalising, the entries of the vectors lie in the interval of [0,1]. The 579
thresholding parameter δnow essentially determines an interval of [0, δ]which we will linearly map 580
to[0,1], while mapping all entries above δto1as well. This is necessary as most of the entries in 581
the vector ei
kare very close to 0with a very small number of entries being close to 1. Without this 582
thresholding, TOPF would now be almost entirely determined by these few large values. Thus this 583
step limits the maximum possible influence of a single entry. However, because most of the entries of 584
ei
kare concentrated around 0, small changes in δwill not have a large effect and we chose δ= 0.07 585
in all our experiments. 586
Interpolation coefficient λThe interpolation coefficient λ∈[0,1)determines whether we build 587
our simplicial complexes close to the birth or the death of the relevant homological features at time 588
t=b1−λd. This then in turns controls how localised or smooth the harmonic representative will 589
be. In general, the noisier the ground data is the higher we should choose λ. However, TOPF is not 590
sensitive to small changes in λ. We have picked λ= 0.3for all the quantitative experiments, which 591
empirically represents a good choice for a broad range of applications. 592
Feature selection factor βIncreasing βleads to TOPF preferring to pick a larger number of relevant 593
topological features. Without specific domain-knowledge, β= 0represents a good choice. 594
Feature selection quotients max_total_quot ,min_rel_quot , and min_0_ratio These are 595
technical hyperparameters controlling the feature selection module of TOPF . For a technical account 596
of them, see Appendix E. In most of the cases without domain knowledge, they do not have an effect 597
on the performance of TOPF and should be kept at their default values. 598
Simplicial Complex Weights Although the simplicial weights are not technically a hyperparameter, 599
there are many potential ways to weigh the considers SCs that can highlight or suppress different 600
topological and geometric properties. In all our experiments, we use w∆weights discussed in 601
Appendix F. 602
E How to pick the most relevant topological features 603
Simplified heuristic The persistent homology Pkmodule in dimension kis given to us as a list of 604
pairs of birth and death times (bk
i, dk
i). We can assume these pairs are ordered in non-increasing order 605
of the durations lk
i=dk
i−bk
i. This list is typically very long and consists to a large part of noisy 606
homological features which vanish right after they appear. In contrast, we are interested in connected 607
components, loops, cavities, etc. that persist over a long time, indicating that they are important for 608
the shape of the point cloud. Distinguishing between the relevant and the irrelevant features is in 609
general difficult and may depend on additional insights on the domain of application. In order to 610
provide a heuristic which does not depend on any a-priori assumptions on the number of relevant 611
16Figure 8: TOPF heatmaps for three proteins. Top left NALCN channelosome [ 32]Top right:
Mutated Cys123 of E. coli [ 29], with convex hull added during computation, only 2-dimensional
homology features Bottom: GroEL of E. coli [10] (Selected features).
features we pick the smallest quotient qk
i:=lk
i+1/lk
i>0as the point of cut-off Nk:= arg miniqk
i. 612
The only underlying assumption of this approach is that the band of “relevant” features is separated 613
from the “noisy” homological features by a drop in persistence. 614
Advanced Heuristic However, certain applications have a single very prominent feature, followed 615
by a range of still relevant features with significantly smaller life times, that are then followed by 616
the noisy features after another drop-off. This then could potentially lead the heuristic to find the 617
wrong drop-off. We propose to mitigate this issue by introducing a hyperparameter β∈R>0. We 618
then define the i-th importance-drop-off quotient qk
iby 619
qk
i:=lk
i+1/lk
i(1 + β/i).
The basic idea is now to consider the most significant Nkhomology classes in dimension kwhen 620
setting Nkto be 621
Nk:= arg min
iqk
i.
Increasing βleads the heuristic to prefer selections with more features than with fewer features. 622
Empirically, we still found β= 0to work well in a broad range of application scenarios and used 623
it throughout all experiments. There are only a few cases where domain-specific knowledge could 624
suggest picking a larger β. 625
To catch edge cases with multiple steep drops or a continuous transition between real features and 626
noise, we introduce two more checks: We allow a minimal qk
iofmin_rel_quot = 0.1and a 627
maximal quotient qh
1/qk
iofmax_total _quot = 10 between any homology dimensions. Because 628
features in 0-dimensional homology are often more noisy than features in higher dimensions, we add 629
a minimum zero-dimensional homology ratio of min_0_ratio = 5, i.e. every chosen 0-dimensional 630
feature needs to be at least min_0_ratio more persistent then the minimum persistence of the 631
higher-dimensional features. Because these hyperparameters only deal with the edge cases of 632
feature selection, TOPF is not very sensitive to them. For all our experiments, we used the above 633
hyperparameters. We advise to change them only in cases where one has in-depth domain knowledge 634
about the nature of relevant topological features. 635
17Point cloud
Unweight. harm. rep.
Eff. Res.
Weight. harm. rep.
1/Num tris
VR complex
alpha complex
Weight. harm. rep.Figure 9: Effect of weighing a simplicial complex on harmonic representatives. Top: VRcomplex.
Bottom: α-complex Left: The base point cloud with different densities. 2ndLeft: Unweighted
harmonic homology representative of the large loop. 3rdRight: Effective resistance of the 1-simplices.
3rdRight: Harmonic homology representative of the complex weighted by effective resistance.
2ndRight: Inverse of number of incident triangles (Definition F.1). Right: Harmonic homology
representative of the complex weighted by number of incident triangles. Up to a small threshold,
the standard harmonic representative in the VRcomplex is almost exclusively supported in the
low-density regions of the simplicial complex. This leads to poor and unpredictable classification
performance in downstream tasks. In contrast, the harmonic homology representative of the weighted
VRcomplex has a more homogenous support along the loop, while still being able to discriminate the
edges not contributing to the loop. The α-complex suffers less from this phenomenon (at least in
dimension 2), and hence reweighing is not necessarily required.
F Simplicial Weights 636
In an ideal world, the harmonic eigenvectors in dimension kwould be vectors assigning ±1to all 637
k-simplices contributing to k-dimensional homological feature, a 0to all k-simplices not contributing 638
or orthogonal to the feature, and a value in (−1,1)for all simplices based on the alignment of the 639
simplex with the boundary of the void. However, this is not the case: In dimension 1, we can for 640
example imagine a total flow of 1circling around the hole. This flow is then split up between all 641
parallel edges which means twothings: IEdges where the loop has a larger diameter have smaller 642
harmonic values than edges in thin areas and IIinVRcomplexes, which are the most frequently 643
used simplicial complexes in TDA, edges in areas with a high point density have smaller harmonic 644
values than edges in low-density areas. Point IIis another advantage of α-complexes: The expected 645
number of simplices per point does not scale with the point density in the same way as it does in the 646
VRcomplex, because only the simplices of the Delaunay triangulation can appear in the complex. 647
We address this problem by weighing the k-simplices of the simplicial complex. The idea behind this 648
is to weigh the simplicial complex in such a way that it increases and decreases the harmonic values 649
of some simplices in an effort to make the harmonic eigenvectors more homogeneous. For weights 650
w∈RSk,W= diag( w), the symmetric weighted Hodge Laplacian [45] takes the form of 651
Lw
k=W1/2Bk−1B⊤
k−1W1/2+W−1/2BkB⊤
kW−1/2.
18Because we want the homology representative to lie in the weighted gradient space, we have to scale 652
its entries with the weight and set ei
k,w:=W−1/2ei
k. With this, we have that 653
B⊤
k−1W1/2ei
k,w=B⊤
k−1W1/2W−1/2ei
k=B⊤
k−1ei
k= 0
We propose two options to weigh the simplicial complex. The first option is to weigh a k-simplex by 654
the square of the number of k+ 1-simplices the simplex is contained in: 655
w∆(σk) = 1 /(|{σk+1∈ St
k+1:σk⊂σk+1}|+ 1)2
where the +1is to enforce good behaviour at simplices that are not contained in any higher-order 656
simplices. One of the advantages of the α-complex is that we don’t have large concentrations of 657
simplices in well-connected areas. The proposed weighting w∆is computationally straightforward, 658
as it can be obtained as the column sums of the absolute value of the boundary matrix |Bk|. The 659
weights also deal with the previously mentioned problem II: As the homology representative is scaled 660
inversely to the weight vector w, the simplices in high-density regions will be assigned a low weight 661
and thus their weighted homology representative will have a larger entry. By the projection to the 662
orthogonal complement of the curl space, this large entry is then diffused among the high-density 663
region of the SCwith many simplices, whereas the lower entries of the simplices in low-density 664
regions are only diffused among fewer adjacent simplices. 665
However, the first weight is not able to incorporate the number of parallel simplices into the weighting. 666
This is why we propose a second simplicial weight function based on generalised effective resistance. 667
Definition F.1 (Effective Hodge resistance weights) .For a simplicial complex Swith boundary 668
matrices (Bk), we define the effective Hodge resistance weights wRonk-simplices to be: 669
wR:= diag 
B+
k−1Bk−12
where diag(−)denotes the vector of diagonal entries and (−)+denotes taking the Moore–Penrose 670
inverse. 671
Intuitively for k= 1, we can assume that every edge has a resistance of 1and then the effective 672
resistance coincides with the notion from Physics. Thus simplices with many parallel simplices are 673
assigned a small effective resistance, whereas simplices with few parallel simplices are assigned an 674
effective resistance close to 1. However, computing the Moore–Penrose inverse is computationally 675
expensive and only feasible for small simplicial complexes. 676
In Figure 9, we show that the weights w∆are a good approximation of the effective resistance in 677
terms of the resulting harmonic representative. The standard form of TOPF used in all experiments 678
usesw∆-weights. 679
G Limitations 680
Topological features are not everywhere The proposed topological point features take relevant 681
persistent homology generators and turn these into point-level features. As such, applying TOPF 682
only produces meaningful results on point clouds that have a topological structure. On these point 683
clouds, TOPF can extract structural information unobtainable by non-topological methods. Although 684
TDA has been successful in a wide range of applications, a large number of data sets does not 685
possess a meaningful topological structure. Applying TOPF in these cases will produce no additional 686
information. Other data sets require pre-processing before containing topological features. In Figure 4 687
left, the2dtopological features characterising protein pockets of Cys123 only appear after artificially 688
adding points sampled on the convex hull of the point cloud (Cf [40]). 689
Computing persistent homology can be computationally expensive AsTOPF relies on the 690
computation of persistent homology including homology generators, its runtime increases on very 691
large point clouds. This is especially true when using VRinstead of α-filtrations, which become 692
computationally infeasible for higher-dimensional point clouds. Persistent homology computations 693
for dimensions above 2are only feasible for very small point clouds. Because virtually all discovered 694
relevant homological features in applications appear in dimension 0,1, or2, this does not present 695
a large problem. Despite these computational challenges, subsampling, either randomly or using 696
landmarks, usually preserves relevant topological features and thus extends the applicability of TDA 697
in general and TOPF even to very large point clouds. 698
19Automatic feature selection is difficult without domain knowledge While the proposed heuristics 699
works well across a variety of domains and application scenarios, only domain- and problem-specific 700
knowledge makes truthful feature selection feasible. 701
Experimental Evaluation There are no benchmark sets for topological point features in the 702
literature, which makes benchmarking TOPF not straightforward. On the level of clustering, we 703
introduced the topological clustering benchmark suite to make quantitative comparisons of TOPF 704
possible, and benchmarked TOPF on some of the point clouds of [ 24]. On both the level of point 705
features and real-world data sets, it is however hard to establish what a ground truth of topological 706
features would mean. Instead we chose to qualitatively report the results of TOPF on proteins and 707
real-world data, see Figure 4. 708
20NeurIPS Paper Checklist 709
1.Claims 710
Question: Do the main claims made in the abstract and introduction accurately reflect the 711
paper’s contributions and scope? 712
Answer: [Yes] 713
Justification: The claims about TOPF are supported by the theoretical background in Section 2 714
and Section 3, quantitatively and qualitatively validated and benchmarked in Section 5. 715
Furthermore, a theoretical guarantee can be found in Section 4. 716
Guidelines: 717
•The answer NA means that the abstract and introduction do not include the claims 718
made in the paper. 719
•The abstract and/or introduction should clearly state the claims made, including the 720
contributions made in the paper and important assumptions and limitations. A No or 721
NA answer to this question will not be perceived well by the reviewers. 722
•The claims made should match theoretical and experimental results, and reflect how 723
much the results can be expected to generalize to other settings. 724
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 725
are not attained by the paper. 726
2.Limitations 727
Question: Does the paper discuss the limitations of the work performed by the authors? 728
Answer: [Yes] 729
Justification: We believe that being open about limitations is crucial for the practice of 730
doing good Science. We briefly discuss the main limitations in ??, and talk in detail about 731
limitations in Appendix G. Finally, we are open about limitations when talking about the 732
theoretical background and the algorithm in Section 2 and Section 3 and the remark in 733
Section 4. 734
Guidelines: 735
•The answer NA means that the paper has no limitation while the answer No means that 736
the paper has limitations, but those are not discussed in the paper. 737
• The authors are encouraged to create a separate "Limitations" section in their paper. 738
•The paper should point out any strong assumptions and how robust the results are to 739
violations of these assumptions (e.g., independence assumptions, noiseless settings, 740
model well-specification, asymptotic approximations only holding locally). The authors 741
should reflect on how these assumptions might be violated in practice and what the 742
implications would be. 743
•The authors should reflect on the scope of the claims made, e.g., if the approach was 744
only tested on a few datasets or with a few runs. In general, empirical results often 745
depend on implicit assumptions, which should be articulated. 746
•The authors should reflect on the factors that influence the performance of the approach. 747
For example, a facial recognition algorithm may perform poorly when image resolution 748
is low or images are taken in low lighting. Or a speech-to-text system might not be 749
used reliably to provide closed captions for online lectures because it fails to handle 750
technical jargon. 751
•The authors should discuss the computational efficiency of the proposed algorithms 752
and how they scale with dataset size. 753
•If applicable, the authors should discuss possible limitations of their approach to 754
address problems of privacy and fairness. 755
•While the authors might fear that complete honesty about limitations might be used by 756
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 757
limitations that aren’t acknowledged in the paper. The authors should use their best 758
judgment and recognize that individual actions in favor of transparency play an impor- 759
tant role in developing norms that preserve the integrity of the community. Reviewers 760
will be specifically instructed to not penalize honesty concerning limitations. 761
213.Theory Assumptions and Proofs 762
Question: For each theoretical result, does the paper provide the full set of assumptions and 763
a complete (and correct) proof? 764
Answer: [Yes] 765
Justification: We provide a full set of assumptions for the theorem in Section 4 and a 766
complete proof in Appendix B. We give references for all cited propositions and theorems 767
exceeding basic common mathematical knowledge. 768
Guidelines: 769
• The answer NA means that the paper does not include theoretical results. 770
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 771
referenced. 772
•All assumptions should be clearly stated or referenced in the statement of any theorems. 773
•The proofs can either appear in the main paper or the supplemental material, but if 774
they appear in the supplemental material, the authors are encouraged to provide a short 775
proof sketch to provide intuition. 776
•Inversely, any informal proof provided in the core of the paper should be complemented 777
by formal proofs provided in appendix or supplemental material. 778
• Theorems and Lemmas that the proof relies upon should be properly referenced. 779
4.Experimental Result Reproducibility 780
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 781
perimental results of the paper to the extent that it affects the main claims and/or conclusions 782
of the paper (regardless of whether the code and data are provided or not)? 783
Answer: [Yes] 784
Justification: We list all the steps necessary to reproduce TOPF in Section 3, Appendix E, Ap- 785
pendix F and talk in detail about the hyperparameter choices in Appendix D.1. Furthermore, 786
we will both release the Topological Clustering Benchmark Suite and the code necessary to 787
reproduce all experiments in this paper. 788
Guidelines: 789
• The answer NA means that the paper does not include experiments. 790
•If the paper includes experiments, a No answer to this question will not be perceived 791
well by the reviewers: Making the paper reproducible is important, regardless of 792
whether the code and data are provided or not. 793
•If the contribution is a dataset and/or model, the authors should describe the steps taken 794
to make their results reproducible or verifiable. 795
•Depending on the contribution, reproducibility can be accomplished in various ways. 796
For example, if the contribution is a novel architecture, describing the architecture fully 797
might suffice, or if the contribution is a specific model and empirical evaluation, it may 798
be necessary to either make it possible for others to replicate the model with the same 799
dataset, or provide access to the model. In general. releasing code and data is often 800
one good way to accomplish this, but reproducibility can also be provided via detailed 801
instructions for how to replicate the results, access to a hosted model (e.g., in the case 802
of a large language model), releasing of a model checkpoint, or other means that are 803
appropriate to the research performed. 804
•While NeurIPS does not require releasing code, the conference does require all submis- 805
sions to provide some reasonable avenue for reproducibility, which may depend on the 806
nature of the contribution. For example 807
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 808
to reproduce that algorithm. 809
(b)If the contribution is primarily a new model architecture, the paper should describe 810
the architecture clearly and fully. 811
(c)If the contribution is a new model (e.g., a large language model), then there should 812
either be a way to access this model for reproducing the results or a way to reproduce 813
the model (e.g., with an open-source dataset or instructions for how to construct 814
the dataset). 815
22(d)We recognize that reproducibility may be tricky in some cases, in which case 816
authors are welcome to describe the particular way they provide for reproducibility. 817
In the case of closed-source models, it may be that access to the model is limited in 818
some way (e.g., to registered users), but it should be possible for other researchers 819
to have some path to reproducing or verifying the results. 820
5.Open access to data and code 821
Question: Does the paper provide open access to the data and code, with sufficient instruc- 822
tions to faithfully reproduce the main experimental results, as described in supplemental 823
material? 824
Answer: [Yes] 825
Justification: We will release the full code necessary to reproduce all experimental results of 826
this paper. Furthermore, we will release the topological clustering benchmark suite to the 827
public. 828
Guidelines: 829
• The answer NA means that paper does not include experiments requiring code. 830
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 831
public/guides/CodeSubmissionPolicy ) for more details. 832
•While we encourage the release of code and data, we understand that this might not be 833
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 834
including code, unless this is central to the contribution (e.g., for a new open-source 835
benchmark). 836
•The instructions should contain the exact command and environment needed to run to 837
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 838
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 839
•The authors should provide instructions on data access and preparation, including how 840
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 841
•The authors should provide scripts to reproduce all experimental results for the new 842
proposed method and baselines. If only a subset of experiments are reproducible, they 843
should state which ones are omitted from the script and why. 844
•At submission time, to preserve anonymity, the authors should release anonymized 845
versions (if applicable). 846
•Providing as much information as possible in supplemental material (appended to the 847
paper) is recommended, but including URLs to data and code is permitted. 848
6.Experimental Setting/Details 849
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 850
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 851
results? 852
Answer: [Yes] 853
Justification: We do not train neural networks in this paper. However, we will release 854
the topological clustering benchmark suite. We talk in detail about how to reproduce the 855
algorithm and the relevant choices of hyperparameters, and how we evaluate the experiments. 856
Guidelines: 857
• The answer NA means that the paper does not include experiments. 858
•The experimental setting should be presented in the core of the paper to a level of detail 859
that is necessary to appreciate the results and make sense of them. 860
•The full details can be provided either with the code, in appendix, or as supplemental 861
material. 862
7.Experiment Statistical Significance 863
Question: Does the paper report error bars suitably and correctly defined or other appropriate 864
information about the statistical significance of the experiments? 865
Answer: [Yes] 866
23Justification: We provide standard deviations where applicable in Table 1, unless the 867
standard deviation is 0, which we talk about in the caption of the table. In Figure 5 we give 868
a confidence interval for all the experiments. 869
Guidelines: 870
• The answer NA means that the paper does not include experiments. 871
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 872
dence intervals, or statistical significance tests, at least for the experiments that support 873
the main claims of the paper. 874
•The factors of variability that the error bars are capturing should be clearly stated (for 875
example, train/test split, initialization, random drawing of some parameter, or overall 876
run with given experimental conditions). 877
•The method for calculating the error bars should be explained (closed form formula, 878
call to a library function, bootstrap, etc.) 879
• The assumptions made should be given (e.g., Normally distributed errors). 880
•It should be clear whether the error bar is the standard deviation or the standard error 881
of the mean. 882
•It is OK to report 1-sigma error bars, but one should state it. The authors should 883
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 884
of Normality of errors is not verified. 885
•For asymmetric distributions, the authors should be careful not to show in tables or 886
figures symmetric error bars that would yield results that are out of range (e.g. negative 887
error rates). 888
•If error bars are reported in tables or plots, The authors should explain in the text how 889
they were calculated and reference the corresponding figures or tables in the text. 890
8.Experiments Compute Resources 891
Question: For each experiment, does the paper provide sufficient information on the com- 892
puter resources (type of compute workers, memory, time of execution) needed to reproduce 893
the experiments? 894
Answer: [Yes] 895
Justification: We list the hardware used in Appendix D and list the required running times in 896
our quantitative experiments, see Table 1. Because we did not train neural networks, the 897
results are easily reproducible on any PC in reasonable time. 898
Guidelines: 899
• The answer NA means that the paper does not include experiments. 900
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 901
or cloud provider, including relevant memory and storage. 902
•The paper should provide the amount of compute required for each of the individual 903
experimental runs as well as estimate the total compute. 904
•The paper should disclose whether the full research project required more compute 905
than the experiments reported in the paper (e.g., preliminary or failed experiments that 906
didn’t make it into the paper). 907
9.Code Of Ethics 908
Question: Does the research conducted in the paper conform, in every respect, with the 909
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 910
Answer: [Yes] 911
Justification: We have reviewed the NeurIPS Ethics guidelines to make sure our research 912
complies with them. (It does comply.) 913
Guidelines: 914
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 915
•If the authors answer No, they should explain the special circumstances that require a 916
deviation from the Code of Ethics. 917
24•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 918
eration due to laws or regulations in their jurisdiction). 919
10.Broader Impacts 920
Question: Does the paper discuss both potential positive societal impacts and negative 921
societal impacts of the work performed? 922
Answer: [NA] 923
Justification: As the paper is of foundational nature, we do not foresee any direct societal 924
impacts. 925
Guidelines: 926
• The answer NA means that there is no societal impact of the work performed. 927
•If the authors answer NA or No, they should explain why their work has no societal 928
impact or why the paper does not address societal impact. 929
•Examples of negative societal impacts include potential malicious or unintended uses 930
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 931
(e.g., deployment of technologies that could make decisions that unfairly impact specific 932
groups), privacy considerations, and security considerations. 933
•The conference expects that many papers will be foundational research and not tied 934
to particular applications, let alone deployments. However, if there is a direct path to 935
any negative applications, the authors should point it out. For example, it is legitimate 936
to point out that an improvement in the quality of generative models could be used to 937
generate deepfakes for disinformation. On the other hand, it is not needed to point out 938
that a generic algorithm for optimizing neural networks could enable people to train 939
models that generate Deepfakes faster. 940
•The authors should consider possible harms that could arise when the technology is 941
being used as intended and functioning correctly, harms that could arise when the 942
technology is being used as intended but gives incorrect results, and harms following 943
from (intentional or unintentional) misuse of the technology. 944
•If there are negative societal impacts, the authors could also discuss possible mitigation 945
strategies (e.g., gated release of models, providing defenses in addition to attacks, 946
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 947
feedback over time, improving the efficiency and accessibility of ML). 948
11.Safeguards 949
Question: Does the paper describe safeguards that have been put in place for responsible 950
release of data or models that have a high risk for misuse (e.g., pretrained language models, 951
image generators, or scraped datasets)? 952
Answer: [NA] 953
Justification: We do not foresee any such risks. 954
Guidelines: 955
• The answer NA means that the paper poses no such risks. 956
•Released models that have a high risk for misuse or dual-use should be released with 957
necessary safeguards to allow for controlled use of the model, for example by requiring 958
that users adhere to usage guidelines or restrictions to access the model or implementing 959
safety filters. 960
•Datasets that have been scraped from the Internet could pose safety risks. The authors 961
should describe how they avoided releasing unsafe images. 962
•We recognize that providing effective safeguards is challenging, and many papers do 963
not require this, but we encourage authors to take this into account and make a best 964
faith effort. 965
12.Licenses for existing assets 966
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 967
the paper, properly credited and are the license and terms of use explicitly mentioned and 968
properly respected? 969
Answer: [Yes] 970
25Justification: We credit the creators and owners of code used in the model, and state the 971
licenses. 972
Guidelines: 973
• The answer NA means that the paper does not use existing assets. 974
• The authors should cite the original paper that produced the code package or dataset. 975
•The authors should state which version of the asset is used and, if possible, include a 976
URL. 977
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 978
•For scraped data from a particular source (e.g., website), the copyright and terms of 979
service of that source should be provided. 980
•If assets are released, the license, copyright information, and terms of use in the 981
package should be provided. For popular datasets, paperswithcode.com/datasets 982
has curated licenses for some datasets. Their licensing guide can help determine the 983
license of a dataset. 984
•For existing datasets that are re-packaged, both the original license and the license of 985
the derived asset (if it has changed) should be provided. 986
•If this information is not available online, the authors are encouraged to reach out to 987
the asset’s creators. 988
13.New Assets 989
Question: Are new assets introduced in the paper well documented and is the documentation 990
provided alongside the assets? 991
Answer: [Yes] 992
Justification: The code and benchmark suite which we will release with the paper are 993
described and documented in the paper. 994
Guidelines: 995
• The answer NA means that the paper does not release new assets. 996
•Researchers should communicate the details of the dataset/code/model as part of their 997
submissions via structured templates. This includes details about training, license, 998
limitations, etc. 999
•The paper should discuss whether and how consent was obtained from people whose 1000
asset is used. 1001
•At submission time, remember to anonymize your assets (if applicable). You can either 1002
create an anonymized URL or include an anonymized zip file. 1003
14.Crowdsourcing and Research with Human Subjects 1004
Question: For crowdsourcing experiments and research with human subjects, does the paper 1005
include the full text of instructions given to participants and screenshots, if applicable, as 1006
well as details about compensation (if any)? 1007
Answer: [NA] 1008
Justification: This paper does not involve crowdsourcing nor research with human subjects. 1009
Guidelines: 1010
•The answer NA means that the paper does not involve crowdsourcing nor research with 1011
human subjects. 1012
•Including this information in the supplemental material is fine, but if the main contribu- 1013
tion of the paper involves human subjects, then as much detail as possible should be 1014
included in the main paper. 1015
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1016
or other labor should be paid at least the minimum wage in the country of the data 1017
collector. 1018
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1019
Subjects 1020
26Question: Does the paper describe potential risks incurred by study participants, whether 1021
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1022
approvals (or an equivalent approval/review based on the requirements of your country or 1023
institution) were obtained? 1024
Answer: [NA] 1025
Justification: This paper does not involve crowdsourcing nor research with human subjects. 1026
Guidelines: 1027
•The answer NA means that the paper does not involve crowdsourcing nor research with 1028
human subjects. 1029
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1030
may be required for any human subjects research. If you obtained IRB approval, you 1031
should clearly state this in the paper. 1032
•We recognize that the procedures for this may vary significantly between institutions 1033
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1034
guidelines for their institution. 1035
•For initial submissions, do not include any information that would break anonymity (if 1036
applicable), such as the institution conducting the review. 1037
27