Approximated Orthogonal Projection Unit: Stabilizing
Regression Network Training Using Natural Gradient
Shaoqi Wang†Chunjie Yang†Siwei Lou†
†Zhejiang University, China, {sq_w,cjyang999,swlou}@zju.edu.cn
Abstract
Neural networks (NN) are extensively studied in cutting-edge soft sensor models
due to their feature extraction and function approximation capabilities. Current
research into network-based methods primarily focuses on models’ offline accu-
racy. Notably, in industrial soft sensor context, online optimizing stability and
interpretability are prioritized, followed by accuracy. This requires a clearer un-
derstanding of network’s training process. To bridge this gap, we propose a novel
NN named the Approximated Orthogonal Projection Unit (AOPU) which has solid
mathematical basis and presents superior training stability. AOPU truncates the
gradient backpropagation at dual parameters, optimizes the trackable parameters
updates, and enhances the robustness of training. We further prove that AOPU
attains minimum variance estimation (MVE) in NN, wherein the truncated gradient
approximates the natural gradient (NG). Empirical results on two chemical process
datasets clearly show that AOPU outperforms other models in achieving stable
convergence, marking a significant advancement in soft sensor field.
1 Introduction
Deep learning methods have achieved recent success in many regression areas such as natural
language processing, protein structure prediction, and building energy consumption forecasting.
However, for these methods to be useful in the industrial soft sensor field, which demands higher
immediacy and stability, further research into model structure and the stability of the training process
is necessary [ 1,2,3]. The safety and economic impact of factory impose stringent requirements
on soft sensor models deployed online [ 4,5]. For example, each mini-batch update must not cause
significant performance fluctuations to ensure that downstream controllers and monitors do not
execute erroneous actions; soft sensor models must be deployed online to avoid fluctuations due
to changes in operating conditions and model switching [ 6]. Common network’s training tricks
are not suitable for soft sensor contexts. For example, it’s not feasible to use checkpoints for early
stopping but to always use the latest updated checkpoint; there wouldn’t be adaptive learning rate
changes but rather a constant learning rate maintained throughout. Experimental results demonstrate
that such differences lead to a substantial decline in performance. These constraints necessitate
the development of better-suited network architectures for regression task that ensure more stable
optimization during training [7, 6].
MVE is the best unbiased estimator under the Mean Squared Error (MSE) criterion, essentially
representing the performance ceiling for regression models [ 8]. Unfortunately, directly applying
MVE to NN is challenging due to the difficulty in obtaining the likelihood distribution p(y|x)of
inputs xand outputs y[9]. Traditional research on MVE has focused on techniques like Kalman
filtering [ 10,11], algorithms based on Ordinary Least Squares (OLS) [ 12,13], and other system
identification research [ 14,15,16] which operate under linear and convex conditions. These methods,
while effective within their scope, have limited expressive power [17].
38th Conference on Neural Information Processing Systems (NeurIPS 2024).acti
acti
acti
acti
actiaug
augTrackable UntrackableTrackable
(a) (b)Figure 1: Trackable parameters and Untrackable parameters. Solid green lines represent model
parameters, and orange curves represent non-parametric operations. (a) Conventional deep NN
framework. (b) Typical broad learning system framework through data enhancement.
Many studies have explored NN optimization from various perspectives such as adaptive learning
rates [ 18], momentum updates [ 19] and customized loss functions [ 20]. Compared to the first-order
optimization methods, second-order optimization algorithms based on Natural Gradient Descent
(NGD) [ 21,22] can achieve faster and more stable convergence [ 23]. This is because NGD considers
the manifold distribution of model parameters by computing the Fisher Information Matrix (FIM)
during the gradient update process. However, calculating FIM introduces significant computational
overhead, making NGD challenging to implement in most machine learning models [ 24]. Much
research is focused on adapting model structures for NGD [ 25,26,27] and reducing its computational
costs [28, 29, 30], yet applying NGD to NN optimization remains an unsolved issue [31, 32, 33].
Some studies have approached the regression task from the perspective of targeted modular design
[34,35,36], emphasizing the construction of local neuron-level rules to assist models in learning
conducive features , exemplified by SLSTM [ 37], SIAE [ 38], MIF-Autoformer [ 39], and CBMP
[40]. Some endeavors have yielded results with solid theoretical underpinnings [ 41,5,42], such as
S4 [43], V AE, VIOG [ 44]. These examples, integrate with other great work [ 27,45,42,46], jointly
demonstrate excellent integrations of NN with theoretical basis and also have a stronger expressivity
compared to the identification algorithms [ 14,15,47]. However, even though these networks possess
certain interpretability, it is still unclear whether these prior biases are beneficial for regression task
[48,49]. Furthermore, their mathematical foundations are not rooted in soft sensor tasks, which do
not guarantee stable performance in regression [50].
AOPU differs from conventional studies by focusing on training optimization and the overall input-
output relationships. Assuming there is a robust feature extraction module (augmentation block),
AOPU pays specific attention on better optimization and more stable convergence. AOPU innovatively
introduces trackable and dual parameters, enhancing an structure approximation of MVE. The dual
parameters are injective representations of trackable parameters, mainly aiding in truncating the
gradient backpropagation process. This truncation will be validated as an effective approximation to
NGD. The augmentation module boosts AOPU’s performance and also acts as an extension interface,
making AOPU more versatile. Rank Ratio (RR) is introduced as an interpretability index to provide
deep and comprehensive insights into the network dynamics. RR quantifies the ratio of linearly
independent samples within a batch, providing a measure of the heterogeneity and diversity of
information. By harnessing RR value, we can roughly foresee the performance in advance of the
training. A high RR suggests the model output more closely approximates the MVE, and optimization
is more in line with NGD, leading to superior performance. Conversely, a low RR implies that the
precision of computation is compromised, resulting in inferior performance.
2 AOPU:Methodology
2.1 Trackable vs. Untrackable
Parameters within NN that can be decoupled from input data xand computed through an inner
product are defined as trackable parameters. Conversely, parameters that cannot be decoupled are
classified as untrackable parameters.
f(x) =WTx=⟨W, x⟩ g(x) =M(x)Tx=⟨M(x), x⟩ (1)
2(a) gradient decline ignores the manifold (radical) (b) natural gradient recognizes the manifold (conservative)Figure 2: Comparison between NGD and GD. Direction matters more than step size (learning rate)
in stable convergence.
where Wrepresents a parameter matrix and M(·)denotes an operator dependent on x. According to
the definition, Wis identified as a trackable parameter, whereas M(x)is considered untrackable.
A significant proportion of parameters in NN are untrackable as depicted in Fig. 1. This predominance
is attributable to the networks’ reliance on stacking activation functions to bolster their nonlinear
modeling capabilities. Proposition 1 indicates that any parameter influenced by an activation function
transitions to being input-dependent, thus rendering it untrackable.
Proposition 1. There does not exist an transition operator Tindependent of xsuch that for a given
parameter matrix W, and∀x1, x2, the following equations hold,
acti(Wx 1) =T(W)x1,acti(Wx 2) = T(W)x2 (2)
Proof is in Appendix C.3.
2.2 Natural Gradient vs. Gradient
Fig. 2 vividly presents the major difference between NGD and GD using a simple GPR in experiment.
This GPR had only two parameters, the bias of the mean and the coefficient of the kernel matrix, both
constant values. We sampled 100 instances from this GPR and updated these two parameters 100
times using these samples. It was observed that NG require a higher learning rate, while conventional
gradients only need a smaller one. The major difference between NG and conventional gradients lies
in their directions. Conventional gradients ignore the parameter manifold and treat every parameter
equally. NG, by dividing the gradient by its second derivative, treat sensitive parameters cautiously
(low gradient) and non-sensitive parameters boldly (high gradient). This adjustment results in different
gradient directions and contributes to better convergence.
Nevertheless, the calculation of NG involves considering the inverse of the FIM, thereby introducing
computational complexity cubic to the number of parameters, making it infeasible for neural networks.
Existing research on NG focuses on conventional machine learning, e.g., considering more complex
distributions (such as the product of multiple exponential family) for computing NG. Research in
NN domain on NG mostly centers on second-order optimizers (such as AdamW), which are merely
first-order approximations of second-order NG.
2.3 Network’s structure
We wish to emphasize that within AOPU framework, the truncated gradient of the dual
parameters serves as an approximation of the NG of the trackable parameters, while AOPU’s
structured output approximates the MVE. The fidelity of these approximations is measured by
the RR, the closer RR is to 1, the more precise the approximation; conversely the closer RR is to
0, the more precision loss occurs. Furthermore, it can be demonstrated that the output of AOPU
fundamentally differs from traditional neural networks: instead of explicitly modeling a mapping
fromxtoy, it implicitly models a mapping from xto˜xy. To ensure that ycan be recovered from ˜xy,
it is imperative that RR equals 1. AOPU also guarantees the convergence of the dual parameters if
the input-output relationship can be characterized by specific system. The proof of above is intricate
and comprehensive, and one may refer to Appendix A, B and C for more detailed information. This
section focuses on the implementation of AOPU.
AOPU utilize data augmentation to replace stacked activation structures to enhance the nonlinear
modeling capabilitie. In such designs, the choice of the data augmentation module forms a crucial
3Trackable 
Parameter
Dual 
ParameterInjection
Objective
FunctionInference
Function
Back PropagationGradient Update
Input Data Output DataFigure 3: AOPU’s data flow schematic. The gradient is backpropagated but truncated at the dual
parameter, and this gradient is then used to update the trackable parameter.
model prior. For ease of implementation, AOPU adopts a random weight matrix approach for its
data augmentation module [ 51,52]. Specifically, suppose the original feature dimension of the input
data is d, and the defined model hidden dimension size is h. Let ˆGbe a fixed Gaussian initialized
random weight matrix, ˆG∈Rd,h, and for input data x∈Rd,bwhere bis the mini-batch size, the data
augmentation process is as follows,
˜x=concat [acti(ˆGTx), x] (3)
Subsequently, for the augmented ˜x, the output is processed using the trackable parameter ˜W∈
R(d+h),o, where orepresents the output dimension and, for simplicity, we assume o= 1indicating
research into univariate output. The output function is then,
g(ˆy|x) = ˜xT˜W (4)
The optimization of parameter ˜Win AOPU differs from other networks. We introduce a dual
parameter D(˜W)to describe this process,
D(˜W) = ˜x˜xT˜W (5)
and the loss is computed using the following objective function,
L=1
bbX
i=1h
yi−[(˜xT˜x)−1˜xTD(˜W)]ii2
(6)
AOPU innovatively introduces trackable and dual parameters. The dual parameters are injective
representations of trackable parameters, mainly aiding in truncating the gradient backpropagation
process. During the training process, gradient backpropagation is truncated at D(˜W), and the
gradient of the dual parameter updates the original trackable parameter as demonstrated in Fig. 3,
thus completing the training of the model.
It is important to note that the training process involves the inversion of a matrix ˜xT˜x, which is not
always invertible, thus introducing numerical computation issue. We employ the reciprocals of the
positive singular values to circumvent the solvability issues that arise when an inverse does not exist.
However, this approach introduces significant computational precision loss, which in turn prompts a
thorough analysis of RR.
We define the metric RR to represent the ratio of the rank of ˜xto its batch size. Clearly, RR is a value
between [0,1], and as RR approaches 1, the process of approximating the inverse of ˜xT˜xthrough
eigenvalue decomposition becomes more accurate (owing to the presence of more reciprocals of
eigenvalues). When RR equals 1, ˜xT˜xis an invertible matrix; conversely, the smaller the RR, the less
stable the model’s numerical computations and likely poorer performance.
AOPU utilizes parameters’ trackability nature to accelearte the NG computation. The key to AOPU’s
capability to rapidly approximate the NG is its ability to bypass the FIM computation and its inverse.
The key to the capability to skip the FIM lies in our utilization of Eq. 10, which separates the
model parameters from the variables and data. In this context, we can use the gradient matrix of
the expectation parameter with respect to natural parameter to replace the FIM (without actually
performing this calculation), thereby substituting the original complicated NG computation with
an equivalent conventional gradient computation, i.e., ∇λm=F(λ)→ ∇ mλ= (F(λ))−1, so
4that(F(λ))−1∇λL → ∇ mL. This allows us to use the automatic differentiation toolbox for rapid
calculations. Otherwise we must explicitly compute the inverse of the network’s FIM, which is very
time-consuming and memory-intensive. For instance, a network with 20kB of 32-bit parameters,
which equates to 5120 trainable parameters, requires inverting a 5120-dimensional matrix with each
training iteration. This requirement grows with model size and can easily lead to GPU memory
shortages. More critically, such large matrix inversions often lead to significant numerical precision
loss, severely impairing model performance.
3 Experiments and Analysis
In this section, we detail the experimental results of the AOPU model, analyze the impact of
hyperparameters on AOPU, its robustness regarding changes in hyperparameters, its advantages
over other comparative algorithms, and some inherent limitations of the model. Comprehensive
and detailed experiments and comparisons have been conducted on two publicly available chemical
process datasets, Debutanizer and Sulfur Recovery Unit (SRU). For more information of the dataset
please refer to Appendix D.
3.1 Baselines
We choose seven different NN models as baselines: Autoformer, Informer, DNN, SDAE, SV AE,
LSTM, and RVFLNN, covering four major domains including RNN-based networks, auto-encoder-
based networks, attention-based networks, and MLP-based networks. Notably, all baseline models
except RVFLNN and AOPU operate solely within the latent space, meaning that there are linear
transformations mapping the input data to the latent space and from the latent space to the output
space before and after the baseline models. This approach is designed to better control the model
size.
3.2 Experiment Implementation
Apart from AOPU, which is trained using the approximated minimum variance estimation loss
function as previously described, all other deep learning algorithms are trained using the Mean
Squared Error (MSE) loss. AOPU’s learning rate for gradient updates is set at 1.0, while for all
other deep learning algorithms, it is set at 0.005, with the Adam optimizer used for gradient updates.
The learning rates of all models remain static throughout the training process. The experimental
setup differs based on the requirements of various models regarding input dimensions. Models
such as Autoformer, Informer, and LSTM necessitate an input that includes an additional dimension
for ’sequence length’. This dimension is preserved as part of the input structure for these models.
Conversely, models like DNN, SDAE, SV AE, AOPU, and RVFLNN do not require this additional
dimension. For these models, the sequence length and input dimensions are combined and flattened
to serve as the feature dimensions in the input. AOPU’s latent space size is set at 2048. Autoformer,
Informer, SDAE, and SV AE utilize two layers each for their encoder and decoder layers; LSTM uses
two layers of LSTM layers; RVFLNN and AOPU share identical settings. All models except AOPU
and RVFLNN have their latent space sizes set at 16 to ensure the trainable parameters size across all
models are comparable.
3.3 Main Result
3.3.1 How certain we are about the inverse
According to the previous discussion, the existence of the inverse of ˜xT˜xis crucial as it does not
only impact the numerical stability of the model but also directly determines whether it is possible to
recover yfrom the approximated mapping relationship ˜x→˜xy. Clearly, the input feature dimensions
d+hand the batch size bsignificantly affect whether the inverse of ˜xT˜xexists. Specifically, the larger
the batch size, the more columns ˜xhas, and the less likely ˜xis to be column-full-rank; conversely,
the longer the sequence length and the larger the input feature dimensions, the more likely ˜xis to be
linearly independent and thus column-full-rank. From the following experimental results, it will be
clearly observed the impact of batch size and sequence length on RR.
5Figure 4: Histogram of the frequency distribution of RR on SRU dataset under varying batch sizes
and sequence length settings.
Figure 5: Curve of the mean of RR distribution on SRU dataset under varying batch sizes and
sequence length settings.
Fig. 4 shows the distribution of the RR for the AOPU model across various batch sizes and sequence
length combinations on the SRU dataset, where bsstands for batch size and seqfor sequence length.
The experimental results align with the previous analysis: increasing the batch size with a fixed
sequence length significantly decreases the RR distribution, whereas increasing the sequence length
with a fixed batch size significantly increases it.
Fig. 5 shows the mean values of RR distribution changing with sequence length under different batch
size settings, marked by red circles at every ten data points. Clearly, the experimental results shown
in the figure corroborate our analysis that with increasing batch size, the curve’s slope becomes flatter,
indicating the model’s decreasing sensitivity to changes in sequence length. Compared to Fig. 4, Fig.
5 provides additional insights into the mean values of the RR distribution relative to sequence length
and batch size, offering a more comprehensive insight for subsequent experimental interpretations.
Results of the RR study on the Debutanizer are listed in Appendix F.
3.3.2 Is the training stable
Stability is a crucial characteristic for the online deployment of deep learning models in actual produc-
tion processes. Specifically, the incremental updates to model parameters following the observation
of new mini-batch data should have a smooth impact on model performance. However, experimental
results indicate that most networks in the soft sensor field fail to achieve stable convergence. Fig. 6
provides a detailed display of how the MSE metrics for different networks change with training itera-
tions on the SRU validation dataset, with blue solid circles marked every 50 iterations. It is evident
that all models, except Autoformer, Informer, LSTM, and AOPU, exhibit significant performance
fluctuations as training iterations progress. The density of the blue solid circles can to some extent
represent the likelihood of corresponding performance fluctuations.
It can be observed that the SDAE and SV AE networks, despite experiencing significant fluctuations
in validation performance (indicated by large fluctuations in the curve), are mostly stable (as shown
by the blue circles concentrated below the curve). In contrast, the DNN and RVFLNN networks
6Figure 6: Curves of SRU validation loss changes with training iteration for different models with a
fixed batch size of 64 at different sequence length settings. The curves are shown in translucent blue,
with a solid blue circle labeled on the curve every 50 iterations.
have relatively unstable convergence (indicated by blue circles evenly distributed above and below
the curve). Although Autoformer and Informer have relatively stable convergence dynamics, their
performance is relatively poor. Specifically, Autoformer consistently converges to a bad output,
whereas Informer can effectively learn under identical settings but is sensitive to changes in seq,
which can lead to model performance collapse. The convergence process of LSTM is relatively stable,
partly explaining why it is a widely adopted baseline in the field of time series analysis; however,
LSTM is significantly prone to overfitting and its performance is not outstanding.
In contrast to all other network, AOPU exhibits exceptionally impressive performance. AOPU
demonstrates very stable and rapid convergence, with almost no fluctuations in performance as
training iterations progress. Furthermore, AOPU is less sensitive to changes in hyperparameters
and does not exhibit significant overfitting, making it a truly reliable and deployable NN model in
production processes. We also notice that with bigger batch size setting, the fluctuations will be
mitigated. For comparative training dynamics under other batch size settings, refer to Appendix F.
3.3.3 Quantitative analysis
To verify the reliability of the AOPU model’s performance and to quantify its comparison with other
methods, we implemented two different training strategies. Strategy one involved an early stopping
trick and used the best checkpoint to validate the model’s performance on the test dataset. Strategy
two involved training all models for 40 epochs and using the final checkpoint to test the model
performance. All following experiments has batch size set to 64. The outputs of strategy one are
presented at Table 1, while the results of strategy two are presented in Table 2. All NN configurations
were subjected to 20 independent repeat experiments, with the mean of the experiments represented
by uppercase numbers on the left of the table and the standard deviation by lowercase subscript
numbers on the right.
From Table 1, we can intuitively compare the optimal performance among all models. Overall, there
is not much difference in final performance among the various networks. Notably, almost all MAPE
metrics on the Debutanizer dataset exceed 100 due to a sample in the test dataset where the butane
content is nearly zero, which significantly distorts the MAPE calculation. While AOPU performs
comparably to other network models in terms of the R2metric, its stability is significantly superior,
as indicated by much lower standard deviations in the R2values compared to all other models.
Further, to more closely align with real industrial application scenarios, if we do not record the
optimal checkpoint but instead complete training for 40 epochs as shown in Table 2, the performance
of the models significantly declines. Despite this, AOPU continues to provide stable and reliable
performance. In Table 2, it is noted that using the R2metric, AOPU consistently performs best with
the sequence length set at 48, and the performance drop from the optimal results calculated using
7Table 1: The evaluation metrics for different models under different seq settings on Debutanizer and
SRU dataset with bs set to 64 and best training epoch recorded
Model Dataset & Metric
Seq NameDebutanizer SRU
MSE MAPE R2MSE MAPE R2
16Autoformer 0.0314 ±0.0017 177.2±24.56 0.1588 ±0.0481 0.00312 ±0.00037 0.3295 ±0.0310 0.2079 ±0.0957
Informer 0.0165 ±0.0023 130.9±19.26 0.5574 ±0.0641 0.00090 ±0.00011 0.1579 ±0.0117 0.7715 ±0.0294
DNN 0.0146 ±0.0016 121.9±30.82 0.6076 ±0.0440 0.00081 ±0.00017 0.1645 ±0.0249 0.7944 ±0.0441
SDAE 0.0134 ±0.0021 136.1±13.79 0.6406 ±0.0573 0.00080 ±0.00006 0.1453 ±0.0102 0.7967 ±0.0154
SV AE 0.0138 ±0.0013 128.8±30.74 0.6294 ±0.0352 0.00078 ±0.00004 0.1214 ±0.0091 0.7992 ±0.0102
LSTM 0.0259 ±0.0027 161.2±50.17 0.3058 ±0.0741 0.00088 ±0.00011 0.1445 ±0.0106 0.7750 ±0.0292
AOPU 0.0149 ±0.0009 200.5±9.277 0.5992 ±0.0243 0.00072 ±0.00007 0.1694 ±0.0114 0.8154 ±0.0190
RVFLNN 0.0114 ±0.0019 78.88±36.11 0.6936 ±0.0534 0.00074 ±0.00009 0.1584 ±0.0148 0.8128 ±0.0249
24Autoformer 0.0295 ±0.0020 153.1±39.66 0.2091 ±0.0550 0.00236 ±0.00030 0.2685 ±0.0262 0.4014 ±0.0774
Informer 0.0154 ±0.0042 139.3±19.59 0.5864 ±0.1145 0.00110 ±0.00022 0.1670 ±0.0232 0.7210 ±0.0572
DNN 0.0122 ±0.0027 124.7±43.75 0.6717 ±0.0733 0.00091 ±0.00014 0.1671 ±0.0192 0.7684 ±0.0361
SDAE 0.0120 ±0.0023 144.9±25.18 0.6786 ±0.0631 0.00076 ±0.00009 0.1489 ±0.0131 0.8070 ±0.0234
SV AE 0.0127 ±0.0023 143.0±33.10 0.6599 ±0.0642 0.00072 ±0.00004 0.1289 ±0.0143 0.8149 ±0.0122
LSTM 0.0262 ±0.0029 192.2±48.06 0.2961 ±0.0795 0.00091 ±0.00014 0.1420 ±0.0122 0.7672 ±0.0364
AOPU 0.0133 ±0.0006 178.3±11.16 0.6421 ±0.0181 0.00080 ±0.00004 0.1868 ±0.0063 0.7965 ±0.0110
RVFLNN 0.0109 ±0.0012 67.40±26.06 0.7070 ±0.0338 0.00079 ±0.00011 0.1638 ±0.0168 0.7986 ±0.0297
32Autoformer 0.0307 ±0.0018 199.3±31.82 0.1770 ±0.0495 0.00222 ±0.00022 0.2730 ±0.0291 0.4366 ±0.0573
Informer 0.0237 ±0.0049 119.4±18.42 0.3648 ±0.1325 0.00131 ±0.00020 0.1849 ±0.0164 0.6679 ±0.0522
DNN 0.0120 ±0.0027 93.96±45.53 0.6779 ±0.0735 0.00095 ±0.00014 0.1693 ±0.0149 0.7589 ±0.0374
SDAE 0.0111 ±0.0020 131.9±27.59 0.686±0.0539 0.00080 ±0.00005 0.1534 ±0.0124 0.7959 ±0.0145
SV AE 0.0133 ±0.0024 143.9±33.72 0.6419 ±0.0646 0.00078 ±0.00007 0.1300 ±0.0118 0.8010 ±0.0182
LSTM 0.0257 ±0.0029 180.1±48.29 0.3097 ±0.0785 0.00094 ±0.00009 0.1488 ±0.0084 0.7603 ±0.0234
AOPU 0.0119 ±0.0005 155.6±4.827 0.6791 ±0.0155 0.00076 ±0.00003 0.1847 ±0.0048 0.8063 ±0.0078
RVFLNN 0.0121 ±0.0016 57.82±34.15 0.6751 ±0.0445 0.00091 ±0.00012 0.1725 ±0.0129 0.7679 ±0.0308
40Autoformer 0.0307 ±0.0023 199.4±21.66 0.1771 ±0.0642 0.00245 ±0.00020 0.2879 ±0.0210 0.3790 ±0.0509
Informer 0.0237 ±0.0059 119.4±32.86 0.3648 ±0.1597 0.00168 ±0.00054 0.2205 ±0.0334 0.5723 ±0.1369
DNN 0.0120 ±0.0029 93.97±42.94 0.678±0.0792 0.00099 ±0.00017 0.1784 ±0.0191 0.7489 ±0.0445
SDAE 0.0111 ±0.0023 134.4±36.79 0.7009 ±0.0618 0.00081 ±0.00007 0.1546 ±0.0119 0.7938 ±0.0180
SV AE 0.0116 ±0.0025 127.2±31.86 0.6868 ±0.0687 0.00082 ±0.00012 0.1383 ±0.0162 0.7898 ±0.0310
LSTM 0.0261 ±0.0039 197.2±56.01 0.2994 ±0.1059 0.00093 ±0.00011 0.1479 ±0.0093 0.7638 ±0.0284
AOPU 0.0110 ±0.0004 143.4±6.977 0.7030 ±0.0121 0.00071 ±0.00001 0.1785 ±0.0034 0.8193 ±0.0048
RVFLNN 0.0148 ±0.0013 53.59±30.26 0.6013 ±0.0373 0.00093 ±0.00013 0.1761 ±0.0170 0.7644 ±0.0337
48Autoformer 0.0321 ±0.0022 189.4±24.87 0.1390 ±0.0614 0.00249 ±0.00015 0.2879 ±0.0220 0.3676 ±0.0380
Informer 0.0247 ±0.0069 122.6±36.11 0.3385 ±0.1865 0.00190 ±0.00043 0.2270 ±0.0386 0.5186 ±0.1092
DNN 0.0114 ±0.0027 110.0±42.75 0.6941 ±0.0732 0.00105 ±0.00013 0.1796 ±0.0168 0.7332 ±0.0344
SDAE 0.0105 ±0.0014 128.5±16.98 0.7182 ±0.0400 0.00083 ±0.00005 0.1564 ±0.0106 0.7890 ±0.0133
SV AE 0.0105 ±0.0017 116.5±34.32 0.7180 ±0.0473 0.00087 ±0.00010 0.1389 ±0.0098 0.7789 ±0.0256
LSTM 0.0268 ±0.0036 185.6±63.68 0.2813 ±0.0982 0.00091 ±0.00012 0.1471 ±0.0099 0.7691 ±0.0312
AOPU 0.0107 ±0.0003 144.9±7.492 0.7118 ±0.0087 0.00070 ±0.00001 0.1796 ±0.0019 0.8209 ±0.0037
RVFLNN 0.0157 ±0.0026 60.11±29.00 0.5792 ±0.0717 0.00100 ±0.00016 0.1833 ±0.0205 0.7454 ±0.0412
strategy one is minimal. For instance, on the Debutanizer dataset, AOPU’s final optimal performance
is 0.6054, which is a 14.9% decrease from the original 0.7118; on the SRU dataset, its best final
performance is 0.8069, only a 1.7% decrease from 0.8209.
Compared to the results on the Debutanizer and SRU datasets, other models show more significant
declines: Autoformer dropped from 0.2091 and 0.4366 to -0.2396 and 0.1536 respectively, a decline
of 123.9% and 64.8%; Informer decreased from 0.5864 and 0.7715 to 0.4047 and 0.6897 respectively,
declines of 30.9% and 10.6%; DNN dropped from 0.6941 and 0.7944 to 0.5109 and 0.5639 respec-
tively, declines of 26.3% and 29.0%; SDAE decreased from 0.7182 and 0.8070 to 0.4470 and 0.7209
respectively, declines of 37.7% and 10.7%; SV AE dropped from 0.7180 and 0.8149 to 0.4315 and
0.7397 respectively, declines of 39.9% and 9.2%; LSTM went from 0.3097 and 0.7750 to -0.3903
and 0.6280 respectively, declines of 226.0% and 19.0%; RVFLNN decreased from 0.7070 and 0.8128
to 0.2844 and 0.5652 respectively, declines of 59.8% and 30.5%.
In terms of robustness, AOPU shows a significant improvement. It is observed that AOPU’s model
performance steadily improves as sequence length increases, in contrast to the other comparison
networks, which exhibit large fluctuations. For example, RVFLNN’s R2on the SRU dataset drastically
drops from 0.5652 at seq 16 to -0.8785 at seq 32, indicating extreme robustness.
In terms of stability, AOPU not only demonstrates an outstanding advantage in terms of average
R2values but also in standard deviation. From Table 2, it can be seen that the standard deviation
for AOPU between the two training strategies changes only slightly, for instance, from 0.0087 to
8Table 2: The evaluation metrics for different models under different seq settings on Debutanizer and
SRU dataset with bs set to 64 and no best training epoch recorded
Model Dataset & Metric
Seq NameDebutanizer SRU
MSE MAPE R2MSE MAPE R2
16Autoformer 0.0942 ±0.0370 217.0±78.49 -1.5240 ±0.9912 0.00836 ±0.00426 0.5362 ±0.1755 -1.118±1.0810
Informer 0.0283 ±0.0070 140.8±30.66 0.2414 ±0.1885 0.00122 ±0.00036 0.1750 ±0.0226 0.6897 ±0.0918
DNN 0.0215 ±0.0028 161.8±46.14 0.4233 ±0.0760 0.00172 ±0.00073 0.2238 ±0.0410 0.5639 ±0.1850
SDAE 0.0217 ±0.0036 144.6±43.52 0.4184 ±0.0977 0.00113 ±0.00017 0.1559 ±0.0211 0.7114 ±0.0453
SV AE 0.0217 ±0.0061 161.6±53.46 0.4166 ±0.1644 0.00113 ±0.00044 0.1469 ±0.0329 0.7121 ±0.1131
LSTM 0.0521 ±0.0152 215.5±102.7 -0.3956 ±0.4094 0.00251 ±0.00112 0.2055 ±0.0325 0.3634 ±0.2844
AOPU 0.0215 ±0.0007 206.6±9.059 0.4239 ±0.0211 0.00098 ±0.00013 0.1963 ±0.0132 0.7518 ±0.0336
RVFLNN 0.0329 ±0.0391 107.1±40.42 0.1171 ±1.0470 0.00171 ±0.00112 0.2540 ±0.0867 0.5652 ±0.2853
24Autoformer 0.0969 ±0.0293 315.7±113.7 -1.5980 ±0.7864 0.00457 ±0.00145 0.3901 ±0.0764 -0.1590 ±0.3682
Informer 0.0222 ±0.0053 134.9±29.93 0.4047 ±0.1432 0.00146 ±0.00047 0.1878 ±0.0280 0.6280 ±0.1201
DNN 0.0204 ±0.0036 159.7±27.21 0.4518 ±0.0972 0.00281 ±0.00126 0.2760 ±0.0699 0.2879 ±0.3206
SDAE 0.0206 ±0.0069 152.2±44.43 0.4470 ±0.1873 0.00118 ±0.00023 0.1688 ±0.0219 0.6998 ±0.0590
SV AE 0.0217 ±0.0048 152.6±33.46 0.4165 ±0.1306 0.00106 ±0.00034 0.1582 ±0.0248 0.7286 ±0.0873
LSTM 0.0544 ±0.0146 226.9±129.5 -0.4574 ±0.3932 0.00313 ±0.00141 0.1878 ±0.0337 0.6280 ±0.3582
AOPU 0.0185 ±0.0005 193.5±5.519 0.5040 ±0.0136 0.00093 ±0.00009 0.2022 ±0.0099 0.7628 ±0.0232
RVFLNN 0.0338 ±0.0470 135.0±86.13 0.0945 ±1.2610 0.00507 ±0.00725 0.3854 ±0.2346 -0.2849 ±1.8360
32Autoformer 0.0583 ±0.0194 217.3±48.84 -0.5629 ±0.5216 0.00334 ±0.00101 0.3279 ±0.0407 0.1536 ±0.2573
Informer 0.0359 ±0.0134 135.4±43.65 0.0371 ±0.3612 0.00193 ±0.00064 0.2163 ±0.0331 0.5091 ±0.1637
DNN 0.0206 ±0.0049 173.2±44.64 0.4459 ±0.1316 0.00278 ±0.00146 0.2780 ±0.0858 0.2949 ±0.3695
SDAE 0.0211 ±0.0045 179.3±61.21 0.4342 ±0.1228 0.00110 ±0.00032 0.1812 ±0.0253 0.7209 ±0.0824
SV AE 0.0239 ±0.0083 151.3±46.68 0.3598 ±0.2234 0.00102 ±0.00049 0.1669 ±0.0361 0.7397 ±0.1256
LSTM 0.0519 ±0.0204 243.7±75.24 -0.3903 ±0.5471 0.00284 ±0.00132 0.2186 ±0.0513 0.2803 ±0.3348
AOPU 0.0171 ±0.0003 191.0±4.483 0.5396 ±0.0105 0.00085 ±0.00004 0.1945 ±0.0070 0.7833 ±0.0118
RVFLNN 0.0305 ±0.0340 86.84±46.24 0.1809 ±0.9113 0.00741 ±0.01854 0.4012 ±0.4214 -0.8785 ±4.6942
40Autoformer 0.0462 ±0.0136 213.2±62.30 -0.2396 ±0.3648 0.00341 ±0.00077 0.3528 ±0.0453 0.1343 ±0.1970
Informer 0.0194 ±0.0059 119.5±23.91 -0.1124 ±0.5220 0.00201 ±0.00049 0.2283 ±0.0285 0.4911 ±0.1259
DNN 0.0182 ±0.0033 166.4±36.94 0.5109 ±0.0892 0.00422 ±0.00360 0.3376 ±0.1262 -0.0703 ±0.9134
SDAE 0.0223 ±0.0053 158.9±37.77 0.4028 ±0.1430 0.00140 ±0.00096 0.1987 ±0.0462 0.6455 ±0.2437
SV AE 0.0214 ±0.0063 147.5±43.71 0.4259 ±0.1705 0.00118 ±0.00070 0.1699 ±0.0296 0.7001 ±0.1790
LSTM 0.0534 ±0.0211 219.6±80.80 -0.4307 ±0.5669 0.00391 ±0.00244 0.2460 ±0.0584 0.0090 ±0.6181
AOPU 0.0156 ±0.0003 179.0±4.713 0.5804 ±0.0105 0.00076 ±0.00004 0.1843 ±0.0052 0.8069 ±0.0101
RVFLNN 0.0267 ±0.0130 88.96±30.42 0.2844 ±0.3489 0.00338 ±0.00277 0.3133 ±0.1144 0.1442 ±0.7023
48Autoformer 0.0492 ±0.0122 226.2±46.25 -0.3187 ±0.3287 0.00338 ±0.00044 0.3655 ±0.0331 0.1440 ±0.1124
Informer 0.0440 ±0.0175 123.7±40.20 -0.1798 ±0.4709 0.00248 ±0.00072 0.2389 ±0.0309 0.3712 ±0.1826
DNN 0.0194 ±0.0043 167.9±37.16 0.4799 ±0.1166 0.00393 ±0.00219 0.3284 ±0.1002 0.0046 ±0.5544
SDAE 0.0212 ±0.0066 153.0±44.14 0.4316 ±0.1772 0.00120 ±0.00028 0.1831 ±0.0272 0.6953 ±0.0723
SV AE 0.0212 ±0.0059 161.0±39.33 0.4315 ±0.1595 0.00109 ±0.00038 0.1811 ±0.0378 0.7223 ±0.0984
LSTM 0.0682 ±0.0319 271.5±131.8 -0.8286 ±0.8559 0.00539 ±0.00357 0.2784 ±0.0844 -0.3652 ±0.9060
AOPU 0.0147 ±0.0003 177.4±4.092 0.6054 ±0.0094 0.00076 ±0.00003 0.1862 ±0.0063 0.8069 ±0.0092
RVFLNN 0.0479 ±0.0555 106.6±74.56 -0.2843 ±1.489 0.00898 ±0.00208 0.4238 ±0.3727 -1.2760 ±5.2820
0.0094 on the Debutanizer dataset, an increase of about 8.0%. In contrast, the standard deviation for
other networks often increases several-fold, such as Autoformer on the Debutanizer dataset, which
increases from an optimal 0.0481 to 0.3287, an increase of about 583.4%; similar trends are observed
with other models.
3.4 Ablation Study
In this section, we further investigate the effects of structural designs for augmentation through some
ablation studies, examining the impacts of the ReLU piecewise activation function, the Tanh smooth
activation function, and normalization on AOPU’s performance. It is important to note that if AOPU
is trained using direct gradient descent without dual parameter updates, it actually degenerates to an
RVFLNN model, and this part of the ablation study has been detailed in section 3.3.3.
From Table 3 we can draw two conclusions: The first is normalization significantly impairs AOPU’s
model performance. The second it ReLU piecewise non-linear activation function suits worse for
AOPU than the Tanh activation function. As previously analyzed in A where both the input data ˜x
andyshould to be zero mean, hence reducing the covariance operator Rto an inner product operator.
However, piecewise linear functions like ReLU and LeakyReLU are not zero-mean, which violates
such assumptions.
9Table 3: AOPU evaluation metrics on the Debutanizer and SRU datasets under various combinations
of activation functions and layer normalization settings.
Structure Dataset & Metric
Acti Norm Debutanizer SRU
Relu Tanh LaNorm MSE MAPE R2MSE MAPE R2
□✓□ □ 0.0162 ±0.00030 183.2±4.8862 0.5654 ±0.0082 0.00074 ±0.00002 0.1838 ±0.0031 0.8115 ±0.0060
□ □✓ □ 0.0103 ±0.00021 148.6±6.3927 0.7216 ±0.0059 0.00077 ±0.00003 0.1902 ±0.0049 0.8026 ±0.0084
□✓□ □✓ 0.0633 ±0.02113 117.2±57.219 -0.6961 ±0.5650 0.58831 ±0.17940 4.5829 ±0.6003 -147.98 ±45.422
□ □✓ □✓ 0.0189 ±0.00662 157.6±62.892 0.4930 ±0.1773 0.00205 ±0.00082 0.3149 ±0.0619 0.4803 ±0.2055
Table 4: Comprehensive experiments on activation function influence to AOPU
Structure Dataset & Metric
Class ActiDebutanizer SRU
MSE MAPE R2MSE MAPE R2
zero-meanHard Shrink 0.0103 ±0.0003 151.9±6.359 0.7236 ±0.0081 0.00076 ±0.00002 0.1882 ±0.0037 0.8069 ±0.0059
Tanh 0.0104 ±0.0004 152.0±6.425 0.7202 ±0.0114 0.00076 ±0.00004 0.1894 ±0.0052 0.8056 ±0.0106
Tanh Shrink 0.0103 ±0.0002 152.7±5.805 0.7221 ±0.0068 0.00077 ±0.00003 0.1891 ±0.0056 0.8043 ±0.0093
Soft Sign 0.0103 ±0.0003 152.8±6.309 0.7225 ±0.0091 0.00077 ±0.00002 0.1905 ±0.0033 0.8040 ±0.0068
Soft Shrink 0.0104 ±0.0002 152.6±4.548 0.7201 ±0.0060 0.00076 ±0.00003 0.1872 ±0.0050 0.8074 ±0.0081
non-zero-meanSigmoid 0.0157 ±0.0004 192.3±6.336 0.5777 ±0.0124 0.00080 ±0.00003 0.1925 ±0.0048 0.7955 ±0.0089
Relu6 0.0171 ±0.0004 187.9±5.502 0.5416 ±0.0109 0.00074 ±0.00003 0.1836 ±0.0051 0.8111 ±0.0076
RRelu 0.0165 ±0.0002 186.3±5.224 0.5582 ±0.0074 0.00076 ±0.00004 0.1858 ±0.0047 0.8072 ±0.0105
Hard Swish 0.0103 ±0.0002 149.2±5.500 0.7232 ±0.0078 0.00077 ±0.00003 0.1900 ±0.0036 0.8026 ±0.0076
Mish 0.0103 ±0.0003 151.4±6.260 0.7229 ±0.0098 0.00079 ±0.00003 0.1927 ±0.0054 0.7999 ±0.0089
To validate the analysis regarding the effects of zero-mean and non-zero-mean activation func-
tions on AOPU’s performance, an additional comparative experiment was conducted. This experi-
ment included 20 independent repetitions for activation functions classified into zero-mean, Hard
Shrink, Tanh, Tanh Shrink, Soft Sign, and Soft Shrink, and non-zero-mean, Sigmoid, Relu6, RRelu,
Hardswish, and Mish. The results are listed in Table 4
The experimental results largely confirmed the hypotheses outlined previously. In the zero-mean
group, whether on the Debutanizer or SRU dataset, fluctuations in MSE, MAPE, and R2metrics
were consistently controlled within 1% (with the maximum R2fluctuation being 0.48%, from 0.7236
to 0.7201). Conversely, in the non-zero-mean group, Sigmoid, Relu6, and RRelu all demonstrated
notable performance declines on the Debutanizer dataset. Notably, although Hard Swish and Mish are
classified as non-zero-mean activation functions, they did not negatively impact AOPU’s performance.
This could likely be attributed to the fact that Hard Swish and Mish are approximately zero-mean
near the zero index, unlike Sigmoid, Relu6, and RRelu, which are non-zero-mean across any arbitrary
small neighborhoods.
4 Conclusion and Limitation
This paper introduces a novel NN regression model, AOPU, which is grounded in solid mathematics
basis and validated through extensive experiments. The results demonstrate its superior performance,
robustness, and training stability. The development of AOPU lays the foundation for the practical im-
plementation of deep learning soft sensor techniques in industrial processes and provides guidance for
subsequent control, monitoring, and optimization management of these processes. The introduction
of RR also illuminates a promising and valuable direction for exploring the design of augmentation
models. Such prospective topics of value encompass how to reduce the sensitivity of AOPU to batch
size and sequence length, how to derive the NG optimization of the augmentation model, and how to
bolster the nonlinear modeling capability of the augmentation model.
We note that AOPU is not a "plug-and-play" model; it requires adjustments based on actual data
conditions. AOPU necessitates a clear understanding of the RR distribution of data intended for
application to guide the selection of batch size and sequence length hyperparameters. This requirement
stems from the inherent matrix inversion operations in AOPU. When the RR value is too low, noise
during the AOPU training process can greatly exceed the effective information, potentially leading to
model divergence as Appendix F discusses.
10References
[1]Xinyu, Z., G. Zhiqiang. Automatic deep extraction of robust dynamic features for industrial
big data modeling and soft sensor application. IEEE Transactions on Industrial Informatics ,
16(7):4456–4467, 2020.
[2]Xiaoxia, C., S. Xuhua, T. Chudong. Multi-time-scale tfe prediction for iron ore sintering process
with complex time delay. Control Engineering Practice , 89:84–93, 2019.
[3]Xiaofeng, Y ., Z. Jiao, H. Biao, et al. Hierarchical quality-relevant feature representation for soft
sensor modeling: A novel deep learning strategy. IEEE Transactions on Industrial Informatics ,
16(6):3721–3730, 2020.
[4]Xiaofeng, Y ., H. Biao, W. Yalin, et al. Deep learning-based feature representation and its
application for soft sensor modeling with variable-wise weighted sae. IEEE Transactions on
Industrial Informatics , 14(7):3235–3243, 2018.
[5]Zhiqiang, G. Process data analytics via probabilistic latent variable models: A tutorial re-
view. Industrial and Engineering Chemistry Research , 57(38):12646–12661, 2018. Doi:
10.1021/acs.iecr.8b02913.
[6]Zhang, Y ., Q. Wen, x. wang, et al. Onenet: Enhancing time series forecasting models under
concept drift by online ensembling. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt,
S. Levine, eds., Advances in Neural Information Processing Systems , vol. 36, pages 69949–
69980. Curran Associates, Inc., 2023.
[7]Siwei, L., Y . Chunjie, Z. Xujie, et al. Blast furnace ironmaking process monitoring with time-
constrained global and local nonlinear analytic stationary subspace analysis. IEEE Transactions
on Industrial Informatics , pages 1–14, 2023.
[8]Daniel, S., U. Elisabeth. On multilevel best linear unbiased estimators. SIAM/ASA Journal on
Uncertainty Quantification , 8(2):601–635, 2020.
[9]Peter, G., J. Ramesh, R. Mohammad. Adaptive experimental design with temporal interference:
A maximum likelihood approach. In Advances in Neural Information Processing Systems ,
vol. 33, pages 15054–15064. Curran Associates, Inc., 2020.
[10] Pinheiro, T. C. F., A. d. S. Silveira. Stochastic model predictive control using laguerre function
with minimum variance kalman filter estimation. International Journal of Dynamics and
Control , 11(3):1330–1350, 2023.
[11] Yuming, C., S.-A. Daniel, W. Rebecca. Autodifferentiable ensemble kalman filters. SIAM
Journal on Mathematics of Data Science , 4(2):801–833, 2022.
[12] Chong, Y ., Y . Chunjie, L. Junfang, et al. Forecasting of iron ore sintering quality index: A latent
variable method with deep inner structure. Computers in Industry , 141:103713, 2022.
[13] Yong, L., W. Haixu, W. Jianmin, et al. Non-stationary transformers: Exploring the stationarity
in time series forecasting. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh,
eds., Advances in Neural Information Processing Systems , vol. 35, pages 9881–9893. Curran
Associates, Inc., 2022.
[14] Elad, H., L. Holden, S. Karan, et al. Spectral filtering for general linear dynamical systems. In
Advances in Neural Information Processing Systems , vol. 31. Curran Associates, Inc., 2018.
[15] Elad, H., S. Karan, Z. Cyril. Learning linear dynamical systems via spectral filtering. In
Advances in Neural Information Processing Systems , vol. 30. 2017.
[16] Peter, V . O., D. M. Bart. N4sid: Subspace algorithms for the identification of combined
deterministic-stochastic systems. Automatica , 30(1):75–93, 1994. Special issue on statistical
signal processing and control.
[17] Gianluigi, P., A. Aleksandr, G. Daniel, et al. Deep networks for system identification: A survey.
arXiv preprint arXiv:2301.12832 , 2023.
[18] Ilya, L., H. Frank. Decoupled weight decay regularization. In International Conference on
Learning Representations . 2019.
[19] Kingma, D. P., J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2017.
11[20] James, H., R. Magnus, L. Neil. Fast variational inference in the conjugate exponential family.
InAdvances in Neural Information Processing Systems , vol. 25. Curran Associates, Inc., 2012.
[21] Amari, S. Natural gradient works efficiently in learning. Neural Computation , 10(2):251–276,
1998. Yv501 Times Cited:1681 Cited References Count:41.
[22] James, M. New insights and perspectives on the natural gradient method. Journal of Machine
Learning Research , 21, 2020. Np3tx Times Cited:69 Cited References Count:73.
[23] Matthew, D. H., M. B. David, W. Chong, et al. Stochastic variational inference. Journal of
Machine Learning Research , 14:1303–1347, 2013. 168md Times Cited:1175 Cited References
Count:103.
[24] Hensman, J., N. Fusi, N. D. Lawrence. Gaussian processes for big data. arXiv preprint
arXiv:1309.6835 , 2013.
[25] Matthias, B., v. d. W. Mark, R. Carl Edward. Understanding probabilistic sparse gaussian
process approximations. In Advances in Neural Information Processing Systems , vol. 29.
Curran Associates, Inc., 2016.
[26] Trong Nghia, H., H. Quang Minh, L. Bryan Kian Hsiang. A unifying framework of anytime
sparse gaussian process regression models with stochastic variational inference for big data. In
F. Bach, D. Blei, eds., Proceedings of the 32nd International Conference on Machine Learning ,
vol. 37 of Proceedings of Machine Learning Research , pages 569–578. PMLR, Lille, France,
2015.
[27] Dutordoir, V ., J. Hensman, M. van der Wilk, et al. Deep neural networks as point estimates for
deep gaussian processes. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P. Liang, J. W. Vaughan,
eds., Advances in Neural Information Processing Systems , vol. 34, pages 9443–9455. Curran
Associates, Inc., 2021.
[28] Wu, L., K. Mohammad Emtiyaz, S. Mark. Fast and simple natural-gradient variational inference
with mixture of exponential-family approximations. In K. Chaudhuri, R. Salakhutdinov, eds.,
Proceedings of the 36th International Conference on Machine Learning , vol. 97 of Proceedings
of Machine Learning Research , pages 3992–4002. PMLR, 2019.
[29] Wu, L., N. Frank, E. Khan Mohammad, et al. Tractable structured natural-gradient descent using
local parameterizations. In M. Meila, T. Zhang, eds., Proceedings of the 38th International
Conference on Machine Learning , vol. 139 of Proceedings of Machine Learning Research ,
pages 6680–6691. PMLR, 2021.
[30] Mohammad, K., L. Wu. Conjugate-computation variational inference : Converting variational
inference in non-conjugate models to inferences in conjugate models. In A. Singh, J. Zhu,
eds., Proceedings of the 20th International Conference on Artificial Intelligence and Statistics ,
vol. 54 of Proceedings of Machine Learning Research , pages 878–887. PMLR, 2017.
[31] James, M., G. Roger. Optimizing neural networks with kronecker-factored approximate curva-
ture. In F. Bach, D. Blei, eds., Proceedings of the 32nd International Conference on Machine
Learning , vol. 37 of Proceedings of Machine Learning Research , pages 2408–2417. PMLR,
Lille, France, 2015.
[32] Wu, L., D. Felix, E. Runa, et al. Can we remove the square-root in adaptive gradient methods?
a second-order perspective. arXiv preprint arXiv:2402.03496 , 2024.
[33] —. Structured inverse-free natural gradient: Memory-efficient and numerically-stable kfac for
large neural nets. arXiv preprint arXiv:2312.05705 , 2023.
[34] Qingqiang, S., G. Zhiqiang. A survey on deep learning for data-driven soft sensors. IEEE
Transactions on Industrial Informatics , 17(9):5853–5866, 2021.
[35] Luigi, F., G. Salvatore, R. Alessandro, et al. Soft Sensors for Monitoring and Control of
Industrial Processes , vol. 22. Springer, 2007. Springer.
[36] Zhiqiang, G., H. Biao, S. Zhihuan. Mixture semisupervised principal component regression
model and soft sensor application. AIChE Journal , 60(2):533–545, 2014.
[37] Yuan, X., L. Li, Y . Wang. Nonlinear dynamic soft sensor modeling with supervised long
short-term memory network. IEEE Transactions on Industrial Informatics , 16(5):3168–3176,
2020.
12[38] Yuan, X., Y . Wang, C. Yang, et al. Stacked isomorphic autoencoder based soft analyzer and its
application to sulfur recovery unit. Information Sciences , 534:72–84, 2020.
[39] Chong, Y ., Y . Chunjie, Z. Xinmin, et al. Multisource information fusion for autoformer: Soft
sensor modeling of feo content in iron ore sintering process. IEEE Transactions on Industrial
Informatics , 19(12):11584–11595, 2023.
[40] Feng, Y ., Y . Chunjie, Z. Xinmin, et al. A 3-d convolution-based burn-through point multistep
prediction model for sintering process. IEEE Transactions on Industrial Electronics , 71(4):4219–
4229, 2024.
[41] Zhichao, C., G. Zhiqiang. Knowledge automation through graph mining, convolution, and
explanation framework: A soft sensor practice. IEEE Transactions on Industrial Informatics ,
18(9):6068–6078, 2022.
[42] Haoyi, Z., Z. Shanghang, P. Jieqi, et al. Informer: Beyond efficient transformer for long
sequence time-series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence ,
35(12):11106–11115, 2021.
[43] Albert, G., G. Karan, R. Christopher. Efficiently modeling long sequences with structured state
spaces. arXiv preprint arXiv:2111.00396 , 2021.
[44] Zhichao, C., S. Zhihuan, G. Zhiqiang. Variational inference over graph: Knowledge representa-
tion for deep process data analytics. IEEE Transactions on Knowledge and Data Engineering ,
pages 1–16, 2023.
[45] Yi, T., B. Dara, M. Donald, et al. Synthesizer: Rethinking self-attention for transformer models.
In M. Meila, T. Zhang, eds., Proceedings of the 38th International Conference on Machine
Learning , vol. 139 of Proceedings of Machine Learning Research , pages 10183–10192. PMLR,
2021.
[46] James, W., B. Viacheslav, T. Alexander, et al. Efficiently sampling functions from gaussian
process posteriors. In H. D. III, A. Singh, eds., Proceedings of the 37th International Conference
on Machine Learning , vol. 119 of Proceedings of Machine Learning Research , pages 10292–
10302. PMLR, 2020.
[47] Zhiqiang, G., S. Zhihuan. Nonlinear soft sensor development based on relevance vector
machine. Industrial and Engineering Chemistry Research , 49(18):8685–8693, 2010. Doi:
10.1021/ie101146d.
[48] Lim, S. H. Understanding recurrent neural networks using nonequilibrium response theory.
Journal of Machine Learning Research , 22(47):1–48, 2021.
[49] Zachary C., L., B. John, E. Charles. A critical review of recurrent neural networks for sequence
learning. arXiv preprint arXiv:1506.00019 , 2015.
[50] Ailing, Z., C. Muxi, Z. Lei, et al. Are transformers effective for time series forecasting?
Proceedings of the AAAI Conference on Artificial Intelligence , 37(9):11121–11128, 2023.
[51] Chen, C. L. P., Z. Liu. Broad learning system: An effective and efficient incremental learning
system without the need for deep architecture. IEEE Transactions on Neural Networks and
Learning Systems , 29(1):10–24, 2018.
[52] Malik, A., R. Gao, M. Ganaie, et al. Random vector functional link network: Recent develop-
ments, applications, and future directions. Applied Soft Computing , 143:110377, 2023.
[53] Zhengyuan, Z., M. Panayotis, B. Nicholas, et al. Stochastic mirror descent in variationally
coherent optimization problems. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, R. Garnett, eds., Advances in Neural Information Processing Systems , vol. 30.
Curran Associates, Inc., 2017.
[54] —. On the convergence of mirror descent beyond stochastic convex programming. SIAM
Journal on Optimization , 30(1):687–716, 2020.
[55] Chaobing, S., Z. Zhengyuan, Z. Yichao, et al. Optimistic dual extrapolation for coherent
non-monotone variational inequalities. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,
H. Lin, eds., Advances in Neural Information Processing Systems , vol. 33, pages 14303–14314.
Curran Associates, Inc., 2020.
13[56] Bo, D., H. Niao, D. Hanjun, et al. Provable bayesian inference via particle mirror descent. In
A. Gretton, C. C. Robert, eds., Proceedings of the 19th International Conference on Artificial
Intelligence and Statistics , vol. 51 of Proceedings of Machine Learning Research , pages 985–
994. PMLR, Cadiz, Spain, 2016.
[57] Luigi, F., G. Salvatore, R. Alessandro, et al. Soft sensors for monitoring and control of industrial
processes . Springer, London, UK, 2007.
14A Network’s mechanism
In this section, we first demonstrate that AOPU implements a MVE through NN, explain the relation-
ship between data augmentation, minimum variance, and orthogonal projection. We then discuss the
physical significance and necessity of the dual parameter from NGD perspective.
A.1 From MVE perspective
We start by giving the definition of the MVE,
Definition 1. Given the independent variable xand the dependent variable y,f∗(y|x)is said to be
the MVE for yif the following hold,
E[(y−f∗(y|x))2]≤E[(y−f(y|x))2] (7)
where E[·]denotes the expectation operator, and f(y|x)represents any unbiased arbitrary estimation
function for ygiven x.
According to definition 1, it is straightforward that the MVE is the optimal unbiased estimator under
the Mean Squared Error loss metric, providing a performance boundary for all regression networks.
However, the solution to the MVE, detailed in Appendix C.1, represented asR
yp(y|x)dywhere
p(·)denotes the probability operator, is challenging to determine. Since the prior knowledge of the
likelihood distribution p(y|x)is not accessible, this integral is difficult to solve. Instead of delving
into modeling the likelihood, AOPU turns to referencing solvable linear MVE operators for network
design. Specifically, when the function form of f(y|x)is constrained to be linear with respect to x, it
can be set as f(y|x) =Wmvex+b, with the solution Wmve=RyxR−1
xxandb=E[y]−WmveE[x],
where Rabrepresents the covariance matrix E
(a−E[a])(b−E[b])T
. For clarity, the proof is listed
in Appendix C.2.
Given that variables xandyhave been normalized to have zero mean, i.e., E[x] = 0 andE[y] = 0 ,
it follows that b= 0, and the covariance operator Rdegenerates into an inner product operation.
Revisiting the loss function of AOPU, it is evident from Eq. 6 that AOPU essentially estimates
the parameters Wmveof the linear MVE. Here the (˜xT˜x)−1is aligned with the R−1
˜x˜x, and ˜xTD(˜W)
ought correspond to the estimation of the input-output covariance matrix. It is noteworthy that
the parameter Wmveis not the estimator’s output. Therefore, by approximating Wmvein the loss
function, AOPU implies an important assumption: unlike other regression algorithms that explicitly
model the mapping relationship from ˜xtoy, i.e., ˜x→y, AOPU implicitly models the relationship
˜x→˜xy. Given that ˜x∈Rd+h,bandy∈Rb,1, the key to deriving yfrom known ˜xand˜xylies in the
requirement that ˜xmust be column-full-rank. This requirement aligns with the numerical stability
needs during the computation process of AOPU, thereby establishing a self-consistent mathematical
framework for the unit.
The geometric interpretation of the linear minimum variance estimator as orthogonal projection
underpins the naming of the AOPU. AOPU differs from the orthogonal projection in threefold: (1)
Orthogonal projection is a non-parametric batch algorithm, whereas AOPU operates as a parametric,
gradient-based mini-batch optimization algorithm. (2) Orthogonal projection requires the covari-
ance matrix’s inverse to exist definitively, whereas AOPU can employ approximate inverses for its
computations. (3) Orthogonal projection strictly adheres to linear minimum variance estimation,
but AOPU introduces non-linearity through data augmentation, allowing it to serve as a versatile
minimum variance estimator. The data augmentation techniques illustrated in Fig. 1 are critical for
enhancing the expressive capabilities of AOPU. We initially improve model expressiveness through a
fixed, randomly initialized Gaussian matrix ˆG. However, this approach remains confined within the
linear transformation. Consequently, in subsequent experiments, the inputs are further augmented
using LeakyRelu (ˆGTx), pushing the model beyond linear transformations.
A.2 From NGD perspective
In this section, we introduce NGD, including the computation of FIM, how to reduce the computa-
tional complexity of NGD through EM, and ultimately demonstrate that the truncated gradient of
AOPU is an approximated NG.
15We begin with an introduction to the most basic optimization algorithm used in neural network
training, Gradient Descent (GD). Assuming the network parameters are represented as λ(λrepresents
˜Win AOPU), and ∇denotes the gradient, GD can be defined by the following equation:
GD: λt+1=λt−α∇λtL(λt) (8)
where α > 0represents the network’s learning rate. Many optimization algorithms assist the
network in escaping local optima and accelerating convergence by incorporating momentum gradients
and adaptive learning rates. However, fundamentally, these are first-order optimization methods
(considering only first-order derivatives) and typically exhibit suboptimal performance in practice.
NG, by calculating the FIM, exploits the information geometry of the model’s output distribution to
accelerate convergence more effectively. NGD can be defined by the following equation:
NGD: λt+1=λt−βF(λt)−1∇λtL(λt) (9)
where β, often set to 1, is a scale factor. F(λt)is the negative expectation of the second-order
derivative regarding the network parameter λt, expressed as F(λt) =−Ep(ˆy)[∇2
λtlogp(ˆy|λt)].
The essence of the FIM is to act as a preconditioning matrix that properly scales the gradient on
the manifold of the parameter space. The primary difference between NGD and GD is that FIM
accounts for dependencies among parameters, unlike the assumption of independent gradients for
each parameter as in Eq. 8 with α. NGD generally exhibits better performance compared to GD
when FIM is well-defined.
The computational cost of calculating the FIM and its inverse is substantial, i.e., cubic in terms of
time complexity and quadratic in terms of space complexity. However, for specific model structures,
the computation of NGD can be as straightforward as that of GD. We provide the definition of the
exponential family as follows,
p(z|λ) :=h(z) exp [⟨ϕ(z), λ⟩ −A(λ)] (10)
where p(z|λ)is defined as an exponential family function with the structure given on the right-hand-
side,h(z)is the base measure, ϕ(z)is the sufficient statistics, λis the natural parameter, and A(λ)is
the log-partition function. The expectation-parameter is defined as the expected value of the sufficient
statistics, expressed as m(λ) =Ep(z|λ)[ϕ(z)]. For exponential family distributions, we propose the
following proposition,
Proposition 2. The FIM of p(z|λ)with respect to λis equivalent to the gradient of the expectation
parameter m(λ)with respect to λ.
Proof. In Appendix C.4.
Therefore, the NGD can be simply expressed as follows:
NGD: λt+1=λt−β∇m(λt)L(λt) (11)
By utilizing the MSE loss function, we can establish a connection between AOPU and NGD. From
Eq. 6, it is known that the output of AOPU is the inner product of the model parameter ˜Wand
the augmented variable ˜x. Supposed the output is viewd as an Gaussian distribution characterized
by mean ˜xT˜Wand covariance I, where Iis an appropriately dimensioned identity matrix. The
following optimization objective can be proven to be equivalent to MSE:
ˆL(˜W) =EN(ˆy|˜xT˜W,I)[(y−ˆy)T(y−ˆy)]
=(y−ˆy)T(y−ˆy) +Tr(I)
≊MSE(y,ˆy)(12)
where≊denotes the equivalence up to constants.
Proposition 3. The NG of ˆL(˜W)with respect to ˜Wis approximately equivalent to the gradient of
ˆL(D(˜W))with respect to dual parameter D(˜W).
Proof. In Appendix C.5
Proposition 3 valids the effectiveness of using truncated gradients for parameter updates in AOPU.
Fact that ˆL(D(˜W))is actuallt the aforementioned loss function L, and it can only be an approximation
ofˆL(˜W)because the inverse of ˜xT˜xmay not be well-defined as discussed in sections 2.3 and A.1.
Therefore, RR not only serves as a measure of AOPU’s numerical stability and its approximation
degree to MVE but is also employed to quantify how closely truncated gradient approximates the
NG.
16B Convergence Analysis
In this section, we are going to analyze the convergence of AOPU referencing the conclusions from
[53,54]. We eventually demonstrate that under the condition ˜xis column-full-rank, AOPU converges
to the optimal solution almost surely. Firstly, thanks to the trackability of parameters, AOPU is
capable of being proven a coherent optimization problem under a strict assumption, which agrees
on previous analysis, is made about the distribution of the observed samples yduring the proof;
AOPU’s truncated gradient is then proven to structurally ensure consistency with the Stochastic
Mirror Descent (SMD), specifically that the dual parameters correspond directly to the mirror map;
the assumptions in [ 53] about regularity (assumption 3), differentiability (assumption 1), and bounded
second moments with Lipschitz continuity (assumption 2) are also proven to be met under the
condition ˜xis column-full-rank. Finally, by referencing theorem 3.4 from [ 53], we prove that AOPU
can enter arbitrarily small neighborhood of the optimal parameter solution ˜W∗.
Definition 2. The optimization problem min˜L(W)is said to be coherent if
D
∇E[˜L(W)], W−W∗E
≥0for all W∈ W, W∗∈ W∗(13)
with equality holds if and only if W∈ W∗, where Wis the feasible parameter space and W∗is
additionally constrainted by W∗= arg min ˜L.
Proposition 4. If the observed sample yis characterized by an underlying D(˜W)∗∈ D(˜W)∗,Dfor
short, AOPU’s training objective is coherent with respect to D.
Proof. In Appendix C.6
Referring the work of [ 53,55], Definition 2 introduces the concept coherent which involves the anal-
ysis of the first-order derivatives of the loss function. For conventional NN, due to the untrackability
of parameters, analyzing parameter gradients is exceedingly challenging. Although some studies
[56,32,31] have explored the parameters’ local characteristics, analyzing their global properties
remains difficult. Proposition 4 summarizes the properties of coherence of AOPU briefly.
Proposition 5. If the regularizer in Qis characterized by square Mahalanobis distance with covari-
ance matrix Σ = ˜x˜xTinstead of Euclidean distance Σ =I, AOPU’s training strategy is identical to
SMD.
Proof. In Appendix C.7
In SMD, each iteration involves calculating the stochastic gradient from the model’s current state,
updating within the dual space, and then mapping back to the parameter space. This process is
outlined in Algorithm 1, where XandYrepresent the parameters and dual parameters, respectively,
and subscripts denote the iteration number. Qrepresents the mirror map, which is defined as follows,
Q(Y) = arg max
X⟨X, Y⟩ −h(X) (14)
where hacts as a regularizer. Proposition 5 summarizes the connection between AOPU’s training and
SMD by carefully selecting regularizer.
Algorithm 1 Stochastic mirror descent
Require: Initial Y0
1:n←0
2:repeat
3: Xn=Q(Yn)
4: Yn+1=Yn−αn+1∇˜L(Xn)
5: n←n+ 1
6:until end
7:return solution candidate Xn
The assumption about regularity fundamentally guarantees the continuity of the Fenchel coupling at
the point where Yequals the subgradient of hwith respect to X, and can typically be considered
trivially satisfied. Regarding the properties of differentiability and bounded second moments and
17Lipschitz continuity, the objective function of AOPU is typically quadratic, thus both its first and
second derivatives exist and are linearly related to (˜xT˜x)−1. These assumptions are only satisfied
when ˜xis column-full-rank, i.e., when the inverse is well-defined.
Combining the conclusions from the proofs discussed above with theorem 3.4 from [ 53], it can be
determined that AOPU’s dual parameters D(˜W)always converge to the optimal values under the
constraints. Essentially, the primary difference between using ˜Wfor inference and D(˜W)for training
is term (˜xT˜x)−1. Therefore, when ˜xis column-full-rank, the convergence of the dual parameters
D(˜W)is equivalent to the convergence of the parameters ˜W. This conclusion is consistent with the
results from the earlier MVE analysis and highlights the important role of RR.
The proof of convergence complements the final piece of the puzzle for the deployment of AOPU
in advanced industrial applications. It provides solid theoretical support in the aspects of derivation
processes, optimization procedures, state monitoring, and convergence assurance. It can be confi-
dently stated that AOPU is ready to be applied in industrial soft sensing, having established robust
foundations for operational reliability and efficacy.
C Mathematic Proof
C.1 Solution to General Minimum Variance Estimator
In this subsection we are about to prove that given x, the solution to the general minimum variance
estimator of yisR
yp(y|x)dy, i.e., Ey|x[y]. Since it is the expectation of likelihood, this result is
intuitive to prove. Rewrite the covariance calculation in the following,
E
(y−f(y|x))(y−f(y|x))T
=E
(y+Ey|x[y]−Ey|x[y]−f(y|x))(y+Ey|x[y]−Ey|x[y]−f(y|x))T
=E
(y−Ey|x[y])(y−Ey|x[y])T
+E
(Ey|x[y]−f(y|x))(Ey|x[y]−f(y|x))T
+
E
(y−Ey|x[y])(Ey|x[y]−f(y|x))T
+E
(Ey|x[y]−f(y|x))(y−Ey|x[y])T(15)
Noting that f(y|x)is not a conditional probabilistic distribution representation, it denotes a
function that takes xas input, and the output of such function is regarded as an estima-
tor of y. In conclusion, f(y|x)is fundamentally independent of y, therefore, for the term
E
(Ey|x[y]−f(y|x))(y−Ey|x[y])T
we can rewrite it into,
E
(Ey|x[y]−f(y|x))(y−Ey|x[y])T
=Z Z
(Ey|x[y]−f(y|x))(y−Ey|x[y])Tp(x, y)dxdy
=Z
(Ey|x[y]−f(y|x))Z
(y−Ey|x[y])Tp(y|x)dy
p(x)dx
=Z
(Ey|x[y]−f(y|x))(Ey|x[y]−Ey|x[y])Tp(x)dx
=0(16)
The conclusion also applies to the term E
(y−Ey|x[y])(Ey|x[y]−f(y|x))T
. Consequently, the last
two terms in Eq. 16 consistently equal zero. Given that E
(Ey|x[y]−f(y|x))(Ey|x[y]−f(y|x))T
is semi-positive definite, it follows that E
(y−Ey|x[y])(y−Ey|x[y])T
establishes a lower bound
forE
(y−f(y|x))(y−f(y|x))T
. Equality holds if and only if f(y|x) =Ey|x[y]which completes
the proof.
C.2 Solution to Linear Minimum Variance Estimator
In this subsection, we are about to prove that the solution to the linear minimum variance estimator is
Wmve=RyxR−1
xxandb=E[y]−WmveE[x]. Initially, it is straightforward to see that the value of b
renders the estimator unbiased. By simply taking the expectation, we can complete the proof. Again
18we rewrite the covariance calculation in the following,
E
(y−f(y|x))(y−f(y|x))T
=E
(y−E[y] +WmveE[x]−Wmvex)(y−E[y] +WmveE[x]−Wmvex)T (17)
Incorporating the covariance matrices Ryy = E
(y−E[y])(y−E[y])T
,Rxx =
E
(x−E[x])(x−E[x])T
, and Rxy=E
(x−E[x])(y−E[y])T
. We can reformulate Eq.
17 as Ryy+WmveRxxWT
mve−RyxWT
mve−WmveRxy. Upon simplification, this equation
transforms to,
Ryy+WmveRxxWT
mve−RyxWT
mve−WmveRxy
=WmveRxxWT
mve−WmveRxxR−1
xxRxy−RyxR−1
xxRxxWT
mve+Ryy+RyxR−1
xxRxy−RyxR−1
xxRxy
=(Wmve−RyxR−1
xx)Rxx(Wmve−RyxR−1
xx)T+Ryy−RyxR−1
xxRxy
(18)
Noting that (Wmve−RyxR−1
xx)Rxx(Wmve−RyxR−1
xx)Tis again semi-positive definite, indicating
that the optimal Wmveis identical to R yxR−1
xx, which completes the proof.
C.3 Proof to Proposition 1
Suppose there exists an operator Tindependent of xsuch that for a given Wand any inputs x1and
x2, the Eq. 2 holds. From the linearity property of operators, it follows that,
acti(Wx 1) +acti(Wx 2) =T(W)(x1+x2)
acti(Wx 1) +acti(Wx 2) =acti(W(x1+x2))(19)
Due to the nonlinearity of the activation function, it is clear that Eq. 19 doesn’t hold, consequently
completing the proof.
C.4 Proof to Proposition 2
We are about to give concise and precise proof in this section, starting by proving the connection
between the expectation-parameter and the log-partition function.
Proposition 6. The expectation-parameter equals to the gradient of log-partition function with
respect to natural parameter.
Proof. Since EM represents a probability distribution, the log-partition function acts as a normalizing
factor, thus the following identity holds true,
A(λ) = logZ
h(z) exp (⟨ϕ(z), λ⟩)dz. (20)
Therefore, expectation-parameter could be derived from differentiating A(λ)with respect to λ.
∇λA(λ) =∇λlogZ
h(z) exp (⟨ϕ(z), λ⟩)dz
=∇λR
h(z) exp (⟨ϕ(z), λ⟩)dzR
h(z) exp (⟨ϕ(z), λ⟩)dz
=∇λ⟨ϕ(z), λ⟩ R
h(z) exp (⟨ϕ(z), λ⟩)dz
R
h(z) exp (⟨ϕ(z), λ⟩)dz
=Ep(z|λ)ϕ(z)(21)
Clearly, A(λ)is the only term that is second-order derivable in the score function logp(z|λ). The
FIM can then be intuitively derived from its definition.
F(λ) =−Ep(z|λ)[∇2
λlogp(z|λ)]
=−Ep(z|λ)[−∇2
λA(λ)]
=∇m(λ)(22)
19C.5 Proof to Proposition 3
We first reiterate that treating the output g(ˆy|x)as a Gaussian distribution is merely a prior assumption
and does not alter the structure or computation of AOPU. Representing this Gaussian distribution as
the minimal EM can be expressed as follows,
N(ˆy|˜xT˜W, I) =(2 π)−d+h
2|I|−1
2exp
−1
2D
ˆy−˜xT˜W,ˆy−˜xT˜WE
=(2π)−d+h
2|I|−1
2expD
˜xˆy,˜WE
−1
2
ˆyTˆy+˜WT˜x˜xT˜W (23)
Under this representation, the sufficient statistics and the natural parameter are respectively ˜xˆyand˜W.
From this, by the definition of the expectation parameter, we can calculate m(˜W) =EN(ˆy|˜xT˜W,I)[˜xˆy]
equals to ˜x˜xT˜Wwhich is exactly identical to D(˜W). According to proposition 2, the FIM with
respect to ˜Wis equivalent to the gradient of D(˜W)with respect to ˜W. Thus, by introducing D(˜W),
we can accelerate the NGD computation with respect to ˜Was shown below,
Note that ˆL(˜W)is equivalent to MSE. To compute the gradient of ˆLatD(˜W)using automatic
differentiation tools and avoid complex algebraic operations, we design ˆL(D(˜W))as,
ˆL(D(˜W)) =EN(ˆy|(˜xT˜x)−1˜xTD(˜W),I)[(y−ˆy)T(y−ˆy)] (24)
Clearly, ˆL(D(˜W))is identical to the Lintroduced in section 2.3.
C.6 Proof to Proposition 4
We now assume the observed sample yis fully characterized by dual parameter Daddition with a
zero-mean random variable ϵ. Such constraint implies that there exists an optimal parameter set D∗
which fully captures the mean trend of y, i.e., y= (˜xT˜x)−1˜xTD∗+ϵ. The coherence definition
could be rewritten as follows,D
∇E[ˆL(D)], D−D∗E
=EhD
∇ˆL(D), D−D∗Ei
=E
(˜xT˜x)−1˜xTD−y,(˜xT˜x)−1˜xT(D−D∗)
=E
(˜xT˜x)−1˜xTD−y,(˜xT˜x)−1˜xT(D−D∗)−y+y
=Eh
ˆL(D)i
+E
(˜xT˜x)−1˜xTD−y, y−(˜xT˜x)−1˜xTD∗
=Eh
ˆL(D)−ˆL(D∗)i
+E
(˜xT˜x)−1˜xT(D−D∗), ϵ(25)
In Eq. 25, the first equation arises due to the linear invariance of the gradient with respect to
expectation. The second equation is derived by expanding the objective function and calculating
its gradient, followed by reorganization. The third equation results from adding and subtracting the
same variable yon the right-hand side of the second equation. The fourth equation reconstructs
the objective function and cross-terms from the third equation. The fifth equation reconstructs the
objective function under optimal parameter settings from the fourth equation.
Note that the objective function under globally optimal parameter settings is necessarily less than or
equal to the objective function under any other parameter settings, thus term E[ˆL(D)−ˆL(D∗)]≥0.
Given the previous assumption that each instance within the optimal parameter set perfectly captures
the trend in y, the second term on the right-hand side of the fifth equation is equivalent to the
previously defined zero-mean random variable ϵ, and hence the second expected value is identically
zero. Thus, it is proven that AOPU’s optimization is coherent.
C.7 Proof to Proposition 5
Using distManddistErepresent Mahalanobis distance and Euclidean distance respectively we have
distE(x, y; Σ) =p
xTΣ−1yanddistE(x, y) =p
xTy. The major difference between them is that
20(a) (b)Sulfur
SulfurSulfurSteam
SWSATMMEA GAS
MAXISULFFigure 7: Schematic diagram of two industrial process. (a) SRU. (b) Debutanizer.
the former adjusts for the distribution of data across different dimensions. Euclidean distance is
essentially the Mahalanobis distance when the covariance matrix is I(i.e., when dimensions are
independent and identically distributed). Both Mahalanobis and Euclidean distances are strictly
convex functions with respect to the input, making them suitable for use as regularizer terms in
mirror maps. Referring to Algorithm 1, we have revised the training strategy for AOPU, presented
in Algorithm 2. It is evident that both share a consistent optimization structure, thus structurally
ensuring that AOPU’s optimization process aligns with SMD. The key to the proof lies in establishing
the relationship between the mirror map in SMD and the dual parameter in AOPU.
∇hD
˜Wn, DnE
−h(Dn)i
=∇D
˜Wn, DnE
−1
2DT
n(˜x˜xT)−1Dn
=˜Wn−(˜x˜xT)−1Dn(26)
Algorithm 2 SMD in AOPU
Require: Initial D(˜W)0
1:n←0
2:repeat
3: D(˜W)n=Q(˜Wn)
4: ˜Wn+1=˜Wn−η∇ˆL(D(˜W)n)
5: n←n+ 1
6:until end
7:return solution candidate ˜Wn
Clearly, the mirror map is the solution where the gradient with respect to Dnis zero inD
˜Wn, DnE
−
h(Dn). Eq. 26 details this gradient computation process, where the first equation is obtained by
incorporating the square Mahalanobis distance into the regularizer h, and the second equation is
derived by differentiating with respect to Dn. The solution Dn= ˜x˜xT˜Wprecisely matches the
definition of the dual parameter in AOPU, confirming the coherence of AOPU’s optimization strategy
with the principles of SMD.
D Dataset Description
D.1 Debutanizer
The Debutanizer column is part of a desulfuring and naphtha splitter plant. It is required to maximize
the C5 (stabilized gasoline) content in the Debutanizer overheads(LP gas splitter feed), and minimize
21Table 5: Variable Description
Debutanizer SRU
Process Variables Unit Description Process Variables Unit Description
U1◦C Top temperature U1 m3·h−1Gas flow MEA_GAS
U2 kg·cm−2Top pressure U2 m3·h−1Air flow AIR_MEA
U3 m3·h−1Reflux flow U3 m3·h−1Secondary air flow AIR_MEA_2
U4 m3·h−1Flow to next process U4 m3·h−1Gas flow in SWS zone
U5◦C 6thtemperature U5 m3·h−1Air flow in SWS zone
U6◦C Bottom temperature A
U7◦C Bottom temperature B
the C4 (butane) content in the Debutanizer bottoms (Naphtha splitter feed) [ 57]. However, the
butane content is not directly measured on the bottom flow, but on the overheads of the downstream
deisopentanizer column by the gas chromatograph resulting in a large measuring delay, which is the
reason soft sensor steps in.
The dataset comprises 2,394 records, each featuring 7 relevant sensor measurements. The flowchart
of the Debutanizer column, detailing the locations of these sensors and their respective descriptions,
is presented in Fig. 7 (b). The corresponding details can also be found in Table 5.
D.2 Sulfur Recovery Unit
The sulfur recovery unit (SRU) removes environmental pollutants from acid gas streams before
they are released into the atmosphere. The main chamber is fed with MEA gas, and combustion is
regulated, in air deficiency, by supplying an adequate airflow (AIR_MEA). The secondary combustion
chamber is mainly fed with SWS gas and a suitable air flow is provided (AIR_SWS). The combustion
of SWS gas occurs in a separate chamber with excess air, in order to prevent the formation of
ammonium salts in the equipment, thereby giving rise to the generation of nitrogen and nitrogen
oxides. Air flows are controlled by plant operators to guarantee a correct stoichiometric ratio in the tail
gas. Control is improved by a closed-loop algorithm which regulates a further airflow (AIR_MEA_2)
on the basis of analysis of the tail gas composition. On-line analyzers are used to measure the
concentration of both hydrogen sulfide and sulfur dioxide in the tail gas of each sulfur line. Hydrogen
sulfide and sulfur dioxide frequently cause damage to sensors, which often have to be removed for
maintenance. The design of soft sensors able to predict H2S and SO2 concentrations is therefore
required.
The dataset contains 10,080 records with 5 relevant sensor measurements. The flowchart for the SRU
is illustrated in Fig. 7 (a), with the input descriptions provided in Table 5
E Hyperparameter Scanning
The hyperparameter selection is guided by two principles: first, to ensure the model size of various
comparative methods remains comparable; second, to choose hyperparameters that optimize model
performance. Fig. 8 shows the hyperparameter scanning result, where we can see for SRU the
smaller setup is recommended, while for Debutanizer bigger model possibly leads to better but still
limited performance. However, the model size of the compared methods increases dramatically
with layers and hidden dims, which means that the efficiency of parameters drops. Therefore, we
chose hyperparameter settings that keep the model size comparable to that of AOPU, maintaining a
balance between performance and efficiency. We also study the influence of the learning rate on the
stable convergence. Fig. 9 shows that NGD outperforms the conventional gradient method on stable
convergence under various learning rate setups, further validating AOPU’s contributions.
F Supplementary Figure of Training Dynamics
In this section, we complete the comprehensive RR distribution experimental results and the stability
of the training process of AOPU on the SRU and Debutanizer datasets. We note that, with a fixed
sequence length and increasing batch size, the variance during training generally decreases for other
models, as illustrated in Fig. 10 and Fig. 11. However, for AOPU, increasing bs actually diminishes
22sru
debutanizerFigure 8: Hyper parameter scanning result on SRU and Debutanizer.
Figure 9: Training dynamics under various learning rate setups.
performance. As seen in Fig. 10, when bs is 128 and seq is 16, AOPU’s performance has declined
compared to bs of 64 and seq of 16 in Fig. 6, characterized by increased fluctuations and an upward
shift in the loss curve. Even more critical, Fig. 11 shows that when bs is 288 and seq is 16, AOPU
encounters convergence issues.
This performance degradation is due to the gradual shift of the RR distribution toward zero as bs
increases. In the AOPU structure design, RR serves not only as an indicator of numerical stability
during the matrix’s inversion in forward computation but also as a theoretical foundation for AOPU’s
recovery of yfrom ˜xy. Therefore, lower RR correlates with poorer AOPU performance. From Fig.
5, we observe that with bs at 128 and seq at 16, the mean RR is around 0.5, while at BS of 288 and
sequence length of 16, the mean RR distribution is near 0.25. Given that the RR distribution is highly
concentrated, the mean value represents the statistical properties of the entire distribution, indicating
that AOPU cannot converge when RR falls below 0.25.
Another interpretation of AOPU’s lack of convergence is that when RR is too low (e.g., assuming RR
is zero), AOPU’s pseudo-inverse (˜xT˜x)−1has been calculated as (˜xT˜x), effectively converting the
original normalization process (˜xT˜x)−1˜xT˜xinto a squared process of (˜xT˜x)˜xT˜x, significantly devi-
ating from the initial computational assumptions and causing model collapse. Similar observations
can be found in the Debutanizer experiment results in Fig. 12, Fig. 13, Fig. 14, Fig. 15, and Fig. 16.
23Figure 10: Curves of SRU validation loss changes with training iteration for different models with
fixed batch size of 128 at different sequence length settings.
Figure 11: Curves of SRU validation loss changes with training iteration for different models with
fixed batch size of 288 at different sequence length settings.
24Figure 12: Histogram of the frequency distribution of RR on Debutanizer dataset under varying batch
sizes and sequence length settings. In each subplot, the horizontal axis represents RR, while the
vertical axis indicates frequency, with the distribution normalized. Subplots within the same column
have the same sequence length, while subplots within the same row have the same batch size.
Figure 13: Curve of the mean of RR distribution on Debutanizer dataset under varying batch sizes
and sequence length settings. In each subplot, the horizontal axis represents sequence length, while
the vertical axis indicates the mean of RR distribution. The batch size increases from left to right and
from top to bottom.
25Figure 14: Curves of Debutanizer validation loss changes with training iteration for different models
with a fixed batch size of 64 at different sequence length settings.
Figure 15: Curves of Debutanizer validation loss changes with training iteration for different models
with a fixed batch size of 128 at different sequence length settings.
26Figure 16: Curves of Debutanizer validation loss changes with training iteration for different models
with a fixed batch size of 288 at different sequence length settings.
27NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The main idea of AOPU is an industrially usable stabilized neural network,
whose model structure, mathematical support and experimental results, ablation experiments
have been included.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We integrate limitation and conclusion to explore the limitations of AOPU
compared to conventional neural networks in terms of batch size and sequence length
selection constraint.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
28Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The proof have been given following each statement.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All experiment details have been disclosed in section 3.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
295.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The authors have developed their work within an iron-making facility where
both the code and data are treated as confidential. Despite these restrictions, we anticipate
that the AOPU can be readily replicated by reviewers and future researchers. This is due to
the model’s straightforward and uncomplicated design, NOcomplex modules or intricate
information exchanges between them. This paper focus on establishing a comprehensive
framework to support the AOPU’s effectiveness. Notably, replicating the model simply
involves applying gradient truncation at the appropriate stages (dual parameter stage),
underscoring its simplicity and efficiency.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All experiment details have been disclosed in section 3.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: All experimental results were obtained after 20 repetitions. The standard
deviation is also analyzed in section 3 to ensure that the results are significant
Guidelines:
30• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Computer information are disclosed in section 3
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: This paper conforms the Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: AOPU is an industrial-directed usable neural network, which we believe has
no potential societal impact.
31Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: AOPU is an industrial-directed usable neural network, which we believe has
no such risk.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: This paper have used the existing code repository: Autoformer, Informer; and
dataset: Debutanizer, SRU, and we have cite the original paper.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
32• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
33•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
34