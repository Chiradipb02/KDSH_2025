Adaptive Variance Reduction for Stochastic
Optimization under Weaker Assumptions
Wei Jiang1, Sifan Yang1,2, Yibo Wang1,2, Lijun Zhang1,2,∗
1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
2School of Artificial Intelligence, Nanjing University, Nanjing, China
{jiangw, yangsf, wangyb, zhanglj}@lamda.nju.edu.cn
Abstract
This paper explores adaptive variance reduction methods for stochastic optimization
based on the STORM technique. Existing adaptive extensions of STORM rely
on strong assumptions like bounded gradients and bounded function values, or
suffer an additional O(logT)term in the convergence rate. To address these
limitations, we introduce a novel adaptive STORM method that achieves an optimal
convergence rate of O(T−1/3)for non-convex functions with our newly designed
learning rate strategy. Compared with existing approaches, our method requires
weaker assumptions and attains the optimal convergence rate without the additional
O(logT)term. We also extend the proposed technique to stochastic compositional
optimization, obtaining the same optimal rate of O(T−1/3). Furthermore, we
investigate the non-convex finite-sum problem and develop another innovative
adaptive variance reduction method that achieves an optimal convergence rate of
O(n1/4T−1/2), where nrepresents the number of component functions. Numerical
experiments across various tasks validate the effectiveness of our method.
1 Introduction
This paper investigates the stochastic optimization problem
min
x∈Rdf(x), (1)
where f:Rd7→Ris a smooth non-convex function. We assume that only noisy estimations of its
gradient ∇f(x)can be accessed, denoted as ∇f(x;ξ), where ξrepresents the random sample drawn
from a stochastic oracle such that E[∇f(x;ξ)] =∇f(x).
Problem (1) has been comprehensively investigated in the literature [Duchi et al., 2011, Kingma
and Ba, 2015, Loshchilov and Hutter, 2017], and it is well-known that the classical stochastic
gradient descent (SGD) achieves a convergence rate of O(T−1/4), where Tdenotes the iteration
number [Ghadimi and Lan, 2013]. To further improve the convergence rate, variance reduction
methods have been developed, and attain an improved rate of O(T−1/3)under a slightly stronger
smoothness assumption [Fang et al., 2018, Wang et al., 2019]. However, these methods necessitate
the use of a huge batch size in each iteration, which is often impractical to use. To eliminate the
need for large batches, a momentum-based variance reduction method — STORM [Cutkosky and
Orabona, 2019] is introduced, which achieves a convergence rate of O(T−1/3logT).
Although aforementioned methods are equipped with convergence guarantees, their analyses rely on
delicate configurations of hyper-parameters, such as the learning rate and the momentum parameter.
To set them properly, the algorithm typically needs to know the value of the smoothness parameter L,
∗Lijun Zhang is the corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).the gradient upper bound G, and the variance upper bound σ, which are often unknown in practice.
Specifically, most algorithms require the learning rate ηtsmaller than O(1/L), and for the STORM
method, setting the momentum parameter to O(L2η2
t)is crucial for ensuring convergence [Cutkosky
and Orabona, 2019].
To overcome this limitation, many adaptive algorithms have been developed, aiming to obtain
convergence guarantees without prior knowledge of problem-dependent parameters such as L,G
andσ. Based on the STORM method, Levy et al. [2021] develop the STORM+ algorithm, which
attains the optimal O(T−1/3)convergence rate under the assumption of bounded function values
and gradients. To remove the need for the bounded function values assumption, Liu et al. [2022]
propose the META-STORM algorithm to attain an O(T−1/3logT)convergence rate, but it still
requires the bounded gradients assumption and includes an additional O(logT)term. In summary,
despite advancements in this field, existing adaptive STORM-based methods either depend on strong
assumptions or suffer an extra O(logT)term compared with the lower bound [Arjevani et al., 2023].
Hence, a fundamental question to be addressed is:
Is it possible to develop an adaptive STORM method that achieves the optimal convergence rate
for non-convex functions under weaker assumptions?
We give an affirmative answer to the above question by devising a novel optimal Adaptive STORM
method (Ada-STORM). The learning rate of our algorithm is set to be inversely proportional to a
specific power of the iteration number Tin the initial iterations, and then changes adaptively based
on the cumulative sum of past gradient estimations. In this way, we are able to adjust the learning rate
dynamically according to the property of stochastic gradients, and ensure a small learning rate in the
beginning. Leveraging this strategy, Ada-STORM achieves an optimal convergence rate of O(T−1/3)
for non-convex functions. Notably, our analysis does not require the function to have bounded values
and bounded gradients, which is a significant advancement over existing methods [Levy et al., 2021,
Liu et al., 2022]. Additionally, our convergence rate does not contain the extra O(logT)term, which
is often present in STORM-based methods [Cutkosky and Orabona, 2019, Liu et al., 2022]. To
highlight the versatility of our approach and its potential impact in the field of stochastic optimization,
we further extend our technique to develop optimal adaptive methods for compositional optimization.
Finally, we investigate adaptive variance reduction for the non-convex finite-sum problems. Inspired
by SAG algorithm [Roux et al., 2012], we incorporate an additional term in the STORM estimator,
which measures the difference of past gradients between the selected component function and the
overall objective. By changing the learning rate according to the sum of past gradient estimations,
we are able to obtain an optimal convergence rate of O(n−1/4T−1/2)for finite-sum problems in an
adaptive manner, where nis the number of component functions. Our result is better than the previous
convergence rate of O(n−1/4T−1/2log(nT))obtained by adaptive SPIDER method [Kavis et al.,
2022]. In summary, compared with existing methods, this paper enjoys the following advantages:
•For stochastic non-convex optimization, our method achieves the optimal convergence rate
ofO(T−1/3)under more relaxed assumptions. Specifically, it does not require the bounded
function values or the bounded gradients, and does not include the additional O(logT)term
in the convergence rate.
•Our learning rate design and the analysis exhibit broad applicability. We substantiate this
claim by obtaining an optimal rate of O(T−1/3)for stochastic compositional optimization,
using the technique proposed in this paper.
•For non-convex finite-sum optimization, we further improve our adaptive algorithm to attain
an optimal convergence rate of O(n1/4T−1/2), which outperforms the previous result by
eliminating the O(log(nT))factor.
A comparison between our method and other STORM-based algorithms is shown in Table 1. Numeri-
cal experiments on different tasks also validate the effectiveness of the proposed method.
2 Related work
This section briefly reviews related work on stochastic variance reduction methods and adaptive
stochastic algorithms.
2Table 1: Summary of results for STORM-based methods. Here, NC denotes non-convex, Comp.
indicates compositional optimization, FS represents finite-sum optimization, and BG/BF refers to
requiring bounded gradients or bounded function values assumptions. Adaptive means the method
does not require to know problem-dependent parameters, i.e., L,G, and σ.
Method Setting Convergence Rate Adaptive BG/BF
STORM [Cutkosky and Orabona, 2019] NC O
T−1/3logT
✗ ✓
Super-ADAM [Huang et al., 2021] NC O
T−1/3logT
✗ –
STORM+ [Levy et al., 2021] NC O
T−1/3
✓ ✓
META-STORM [Liu et al., 2022] NC O
T−1/3logT
✓ ✓
Theorem 1, 2 NC O
T−1/3
✓ –
Theorem 3 NC & Comp. O
T−1/3
✓ –
Theorem 4 NC & FS O
n1/4T−1/2
✓ –
2.1 Stochastic variance reduction methods
Variance reduction has been widely used in stochastic optimization to reduce the gradient estimation
error and thus improve the convergence rates. The idea of variance reduction can be traced back to
the SAG algorithm [Roux et al., 2012], which incorporates a memory of previous gradient values
to ensure variance reduction and achieves a linear convergence rate for strongly convex finite-sum
optimization. To avoid the storage of past gradients, SVRG [Zhang et al., 2013, Johnson and Zhang,
2013] proposes to calculate the full gradient periodically, obtaining the same convergence rate as the
SAG algorithm. Subsequent advancement has been made by the SARAH method [Nguyen et al.,
2017], which derives better convergence for smooth convex functions.
In the context of non-convex objectives, Fang et al. [2018] introduce the SPIDER estimator, which im-
proves the convergence rate from O(T−1/4)toO(T−1/3)in stochastic settings, and to O(n1/4T−1/2)
in finite-sum scenarios, with nrepresenting the number of components in the finite-sum. Following
this, the SpiderBoost algorithm [Wang et al., 2019] refines the SPIDER approach by employing a
larger constant step size and adapting it for composite optimization problems. However, a common
limitation among these methods is their reliance on large batch sizes for each iteration, posing practi-
cal challenges due to high computational demands. To mitigate this issue, Cutkosky and Orabona
[2019] introduce the STORM method, a momentum-based technique that achieves an O(T−1/3logT)
convergence rate without using large batches. Concurrently, Tran-Dinh et al. [2019] obtain the same
result using a similar algorithm but through a different analysis.
2.2 Adaptive stochastic algorithms
For stochastic optimization, it is well-known that the SGD algorithm can obtain a convergence rate
ofO(T−1/4)for non-convex objective functions with well-designed learning rates [Ghadimi and
Lan, 2013]. Instead of using pre-defined iteration-based learning rates, many stochastic methods
propose to adjust the learning rate based on past stochastic gradients. One of the foundational works
is the AdaGrad algorithm [Duchi et al., 2011], which proves to be effective for sparse data. Further
advancements include RMSprop [Tieleman and Hinton, 2012] and Adam [Kingma and Ba, 2015],
demonstrating broad effectiveness across a wide range of machine learning problems. Later, the
Super-Adam [Huang et al., 2021] algorithm further improves the Adam algorithm via the variance
reduction technique STORM [Cutkosky and Orabona, 2019] and obtains a convergence rate of
O(T−1/3logT). Nevertheless, to obtain the corresponding convergence rates, these methods still
require knowledge of certain problem-dependent parameters to set hyper-parameters accurately, hence
3Algorithm 1 STORM Algorithm
1:Input: time step T, initial point x1
2:fortime step t= 1toTdo
3: Set hyper-parameters βtandηt
4: Compute vtaccording to equation (2)
5: Update the decision variable: xt+1=xt−ηtvt
6:end for
7:Choose τuniformly at random from {1, . . . , T }
8:Return xτ
not adaptive.2To solve this problem, many research aims to develop fully adaptive SGD methods
that maintain the optimal convergence rate without knowing problem-specific parameters [Orabona,
2014, Chen et al., 2022, Carmon and Hinder, 2022, Ivgi et al., 2023, Yang et al., 2023].
Recently, adaptive adaptations of STORM have received considerable attention. A notable develop-
ment is the introduction of STORM+ [Levy et al., 2021], which presents a fully adaptive version of
STORM while attaining an optimal convergence rate. To circumvent the bounded function values
assumption in STORM+, the META-STORM [Liu et al., 2022] approach is developed, equipped with
a nearly optimal bound. However, META-STORM still requires the bounded gradients assumption,
and it includes an additional O(logT)in the convergence rate. Consequently, adaptive STORM with
the optimal convergence rate and under mild assumptions still needs further explorations.
3 Adaptive variance reduction for non-convex optimization
In this section, we develop an adaptive STORM method for non-convex functions. We first outline
the assumptions used, and then present our proposed method and analyze its convergence rate.
3.1 Assumptions
We introduce the following assumptions, which are standard and commonly adopted in the stochastic
optimization [Nguyen et al., 2017, Fang et al., 2018, Cutkosky and Orabona, 2019, Li et al., 2021].
Assumption 1 (Average smoothness)
Eh
∥∇f(x;ξ)− ∇f(y;ξ)∥2i
≤L2∥x−y∥2.
Assumption 2 (Bounded variance)
Eh
∥∇f(x;ξ)− ∇f(x)∥2i
≤σ2.
Assumption 3 f∗= inf xf(x)≥ −∞ andf(x1)−f∗≤∆ffor the initial solution x1.
Note that some additional assumptions are required in other STORM-based methods. Specifically,
STORM [Cutkosky and Orabona, 2019], STORM+ [Levy et al., 2021], and META-STORM [Liu
et al., 2022] assume the bounded gradients. Moreover, STORM+ makes an additional assumption of
the bounded function values.
3.2 The proposed method
In this subsection, we aim to develop an adaptive STORM method that achieves an optimal conver-
gence rate for non-convex functions under weaker assumptions. Our algorithm framework is the
same as the original STORM [Cutkosky and Orabona, 2019], and the only difference is the setup
of the momentum parameter βtand the learning rate ηt. First, we present the STORM algorithm in
Algorithm 1.
2In this paper, adaptive means the algorithm does not require problem-dependent parameters to set up
hyper-parameters such as the learning rate and the momentum parameter.
4The core idea of STORM lies in a carefully devised variance reduced estimator vt, which effectively
tracks the gradient ∇f(xt). For the first iteration ( t= 1), we set v1=PB0
i=11
B0∇f(x1;ξi
1), which
is estimated within a batch B0=T1/3. Note that we use large batch only in the first iteration, and
constant batch size in other iterations. In subsequent iterations ( t≥2), estimator vtis defined as:
vt= (1−βt)vt−1+βt∇f(xt;ξt) + (1 −βt) (∇f(xt;ξt)− ∇f(xt−1;ξt)), (2)
where the first two terms are similar to the momentum SGD, and the last term serves as the error
correction, which ensures the variance reduction effect. By choosing the values of βtandηtcarefully,
STORM ensures that the estimation error E[∥vt− ∇f(xt)∥2]would decrease gradually. In the
original STORM paper, these parameters are set up as:
ηt=k

w+Pt
i=1∥∇f(xt;ξt)∥21/3, β t=cη2
t,
where k=O(G2/3L−1),w=O(G2)andc=O(L2). The settings of these hyper-parameters are
crucial to the convergence analysis of STORM. However, it is worth noting that Lis the smoothness
parameter and Gis the gradient upper bound, which are often difficult to determine in practice.
To address this problem, our approach defines the hyper-parameters as follows:
ηt= min

1
T1/3,1
T(1−α)/3Pt
i=1∥vi∥2α

, β t=β=T−2/3, (3)
where 0< α < 1/3. Notably, our method does not rely on the parameters LandG, and also does
not need the bounded gradients or bounded function values assumptions that are common in other
methods. Although our formulation initially requires knowledge of the iteration number T, this
can be effectively circumvented using the doubling trick, which will be explained later. The above
learning rate ηtcan also be expressed in an alternative, more illustrative manner:
ηt=

1
T1/3 ifPt
i=1∥vi∥2≤T1/3;
1
T(1−α)/3(Pt
i=1∥vi∥2)αelse.
This formulation ensures that the learning rate starts sufficiently small in the initial stages and then
changes dynamically based on the gradient estimator vt. This design makes our learning rate setup
and convergence analysis distinctly different from previous methods. Next, we present the following
theoretical guarantee for our algorithm.
Theorem 1 Under Assumptions 1, 2 and 3, Algorithm 1 with hyper-parameters in equation (3)
guarantees that:
E[∥∇f(xτ)∥]≤ O
∆1
2(1−α)
f+σ1
1−α+L1
2α
T1/3
.
Remark: To ensure that E[∥∇f(xτ)∥]≤ϵ, the overall complexity is O(ϵ−3), which is known to
be optimal up to constant factors [Arjevani et al., 2023]. Compared with existing STORM-based
algorithms [Cutkosky and Orabona, 2019, Levy et al., 2021, Liu et al., 2022], our method does not
have the extra O(logT)term in the convergence rate, and our analysis does not require bounded
gradients or bounded function values assumptions. Also note that the selection of αdoes not affect the
order of T, and larger αleads to better dependence on parameter Land worse reliance on parameters
∆andσ. Considering we require that 0< α < 1/3, we can simply set α= 0.3in practice.
3.3 The doubling trick
While we have attained the optimal convergence rate using the proposed adaptive STORM method, it
requires knowing the total number of iterations Tin advance. Here, we show that we can avoid this
requirement by using the doubling trick, which divides the algorithm into several stages and increases
5the iteration number in each stage gradually. Specifically, we design a multi-stage algorithm over
k={1,2,···, K}stages. At the beginning of each new stage, we reset xt=x0. In each stage k,
the STORM algorithm is executed for 2k−1iterations, effectively doubling the iteration numbers
after each stage. In any step t, we first identify the current stage as 1 +⌊logt⌋and then calculate the
iteration number for this stage as It= 2⌊logt⌋. Then, we can set the hyper-parameters as:
ηt= min

1
I1/3
t,1
I(1−α)/3
tPt
i=It∥vi∥2α

, β t=I−2/3
t, I t= 2⌊logt⌋. (4)
This approach eliminates the need to predetermine the iteration number T. By using the doubling
trick, we can still obtain the same optimal convergence rate as stated in the following theorem.
Theorem 2 Under Assumptions 1, 2 and 3, Algorithm 1 with hyper-parameters in equation (4)
guarantees that:
E[∥∇f(xτ)∥]≤ O
∆1
2(1−α)
f+σ1
1−α+L1
2α
T1/3
.
4 Extension to stochastic compositional optimization
To demonstrate the broad applicability of our proposed technique, we extend it to stochastic composi-
tional optimization [Wang et al., 2017a,b, Yuan et al., 2019, Zhang and Xiao, 2019, 2021, Jiang et al.,
2023, 2024a], formulated as:
min
x∈RdF(x) =f(g(x)), (5)
where fandgare smooth functions. We assume that we can only access to unbiased estimations of
∇f(x),∇g(x)andg(x), denoted as ∇f(x;ξ),∇g(x;ζ)andg(x;ζ). Here ξandζsymbolize the
random sample drawn for a stochastic oracle such that E[∇f(x;ξ)] =∇f(x),E[g(x;ζ)] =g(x),
andE[∇g(x;ζ)] =∇g(x).
Existing variance reduction methods [Hu et al., 2019, Zhang and Xiao, 2019, Qi et al., 2021] are able
to obtain optimal O(T−1/3)convergence rates for problem (5), but they require the knowledge of
smoothness parameter and the gradient upper bound to set up hyper-parameters. In this section, we
aim to achieve the same optimal convergence rate without prior knowledge of problem-dependent
parameters. We develop our adaptive algorithm for this problem as follows. In each step t, the
algorithm maintains an inner function estimator utin the style of STORM, i.e.,
ut= (1−β)ut−1+g(xt;ζt)−(1−β)g(xt−1;ζt). (6)
Then, we construct a gradient estimator vtbased on utalso in the style of STORM:
vt= (1−β)vt−1+∇f(ut;ξt)∇g(xt;ζt)−(1−β)∇f(ut−1;ξt)∇g(xt−1;ζt). (7)
After that, we apply gradient descent using the gradient estimator vt. The whole algorithm is presented
in Algorithm 2, and hyper-parameters are set the same as in equation (3). For the first iteration, we
simply set u1=PB0
i=11
B0g(x1;ζi
1)andv1=PB0
i=11
B0∇f(u1;ξi
1)∇g(x1;ζ1), where B0=T1/3.
Next, we list common assumptions used in the literature of compositional optimization [Wang et al.,
2017a,b, Yuan et al., 2019, Zhang and Xiao, 2019, 2021].
Assumption 4 (Average smoothness and Lipschitz continuity)
Eh
∥∇f(x;ξ)− ∇f(y;ξ)∥2i
≤L∥x−y∥2;Eh
∥f(x;ξ)−f(y;ξ)∥2i
≤C∥x−y∥2;
Eh
∥∇g(x;ζ)− ∇g(y;ζ)∥2i
≤L∥x−y∥2;Eh
∥g(x;ζ)−g(y;ζ)∥2i
≤C∥x−y∥2.
Assumption 5 (Bounded variance)
Eh
∥g(x;ζ)−g(x)∥2i
≤σ2;Eh
∥∇g(x;ζ)− ∇g(x)∥2i
≤σ2;Eh
∥∇f(x;ξ)− ∇f(x)∥2i
≤σ2.
6Algorithm 2 Compositional STORM
1:Input: time step T, initial point x1
2:fortime step t= 1toTdo
3: Compute utaccording to equation (6)
4: Compute vtaccording to equation (7)
5: Update the decision variable: xt+1=xt−ηtvt
6:end for
7:Choose τuniformly at random from {1, . . . , T }
8:Return xτ
Assumption 6 F∗= inf xF(x)≥ −∞ andF(x1)−F∗≤∆Ffor the initial solution x1.
Remark: In Assumption 4, we further require standard Lipschitz continuity assumption, which is
essential and widely required in the literature for stochastic compositional optimization [Wang et al.,
2017b, Yuan et al., 2019, Jiang et al., 2022a,b]. This assumption is inherently introduced by the
compositional optimization itself rather than by our adaptive techniques.
With the above assumptions, our algorithm enjoys the following guarantee.
Theorem 3 Under Assumptions 4, 5 and 6, our Algorithm 2 ensures that:
E[∥∇F(xτ)∥]≤ O
T−1/3
.
Remark: This rate matches the state-of-the-art (SOTA) results in stochastic compositional opti-
mization [Hu et al., 2019, Zhang and Xiao, 2019, Qi et al., 2021], and our method achieve this in
an adaptive manner. Note that our convergence rate aligns with the lower bound for single-level
problems [Arjevani et al., 2023] and is thus unimprovable.
5 Adaptive variance reduction for finite-sum optimization
In this section, we further improve our adaptive variance reduction method to obtain an enhanced
convergence rate for non-convex finite-sum optimization, which is in the form of
min
x∈RdF(x) =1
nnX
i=1fi(x),
where each fi(·)is a smooth non-convex function. Existing adaptive method for this problem [Kavis
et al., 2022] achieves a convergence rate of O(n1/4T−1/2log(nT))based on the variance reduction
technique SPIDER [Fang et al., 2018], suffering from an extra O(log(nT))term compared with the
corresponding lower bound [Fang et al., 2018, Li et al., 2021].
To obtain the optimal convergence rate for finite-sum optimization, we incorporate techniques from
the SAG algorithm [Roux et al., 2012] into the STORM estimator. Specifically, in each step t, we
start by randomly sample itfrom the set {1,2,···, n}. Then, we construct a variance reduction
gradient estimator as
vt= (1−β)vt−1+∇fit(xt)−(1−β)∇fit(xt−1)−β 
git
t−1
nnX
i=1gi
t!
, (8)
where the first three terms align with the original STORM method, and the last term, inspired by the
SAG algorithm, deals with the finite-sum structure. Here, gttracks the gradient as
gi
t+1=
∇fit(xt)i=it
gi
t i̸=it. (9)
By such a design, we can ensure that the estimation error E[∥vt− ∇F(xt)∥2]reduces gradually.
The whole algorithm is stated in Algorithm 3. In this case, we set the hyper-parameters as:
ηt=1
n1−α
2Pt
i=1∥vi∥2α, β =1
n,
7Algorithm 3 STORM for Finite-sum Optimization (SAG-type)
1:Input: time step T, initial point x1
2:fortime step t= 1toTdo
3: Sample itrandomly from {1,2,···, n}
4: Compute estimator vtaccording to equation (8)
5: Update gt+1according to equation (9)
6: Update the decision variable: xt+1=xt−ηtvt
7:end for
8:Choose τuniformly at random from {1, . . . , T }
9:Return xτ
where 0< α < 1/3. The learning rate ηtis non-increasing and changes according to the gradient
estimations, and the momentum parameter βremains unchanged throughout the learning process.
Next, we show that our method enjoys the optimal convergence rate under the following assumptions,
which are standard and widely adopted in existing literature [Fang et al., 2018, Wang et al., 2019, Li
et al., 2021].
Assumption 7 (Smoothness) For each i∈ {1,2,···, m}, function fiisL-smooth such that
∥∇fi(x)− ∇fi(y)∥ ≤L∥x−y∥.
Assumption 8 F∗= inf xF(x)≥ −∞ andF(x1)−F∗≤∆Ffor the initial solution x1.
Theorem 4 Under Assumptions 7 and 8, our Algorithm 3 guarantees that:
E[∥∇F(xτ)∥]≤ On1/4
T1/2
∆1
2(1−α)
F +L1
2α
.
Remark: Our result matches the lower bound for non-convex finite-sum problems [Fang et al.,
2018, Li et al., 2021], and makes an improvement over the existing adaptive method, i.e.,
AdaSpider [Kavis et al., 2022]. Specifically, the convergence rate of the AdaSpider algorithm
isO 
n1/4T−1/2 
L2+ ∆ F
·log (1 + nTL)
, and our result is better than theirs when1
4< α <1
3.
We can avoid storing past gradients by following the SVRG method [Zhang et al., 2013, Johnson
and Zhang, 2013] to compute the full gradient periodically and incorporate it into STORM estimator.
Instead of storing the past gradients as in SAG algorithm, we can avoid this storage cost by incorpo-
rating elements from the SVRG method. Specifically, we compute a full batch gradient at the first
step and every Iiteration (we set I=n):
∇f(xτ) =1
nnX
i=1∇fi(xτ).
For other iterations, we randomly select an index itfrom the set {1,2,···, n}and compute:
vt= (1−β)vt−1+∇fit(xt)−(1−β)∇fit(xt−1)−β(∇fit(xτ)− ∇f(xτ)). (10)
Note that the first three terms match the original STORM estimator, and the last term, inspired from
SVRG, deals with the finite-sum structure. Compared with equation (8) in Algorithm 3, the difference
is that we use (∇fit(xτ)− ∇f(xτ))instead of 
git
t−1
nPn
i=1gi
t
in the last term. The detailed
procedure is outlined in Algorithm 4. This strategy maintains the same optimal rate, as stated below:
Theorem 5 Under Assumptions 7 and 8, our Algorithm 4 guarantees that:
E[∥∇F(xτ)∥]≤ On1/4
T1/2
∆1
2(1−α)
F +L1
2α
.
Remark: The obtained convergence rate is in the same order as the results in Theorem 4, and
Algorithm 4 does not require storing past gradients anymore.
8Algorithm 4 STORM for Finite-sum Optimization (SVRG-type)
1:Input: time step T, initial point x1
2:fortime step t= 1toTdo
3: iftmod I== 0 then
4: Sett=τand compute ∇f(xτ) =1
nPn
i=1∇fi(xτ)
5: end if
6: Sample itrandomly from {1,2,···, n}
7: Compute vtaccording to equation (10)
8: Update the decision variable: xt+1=xt−ηvt
9:end for
10:Select τuniformly at random from {1, . . . , T }
11:Return xτ
0 50 100 150 200
Epoch−6−4−202Training loss (log scale)
(a) Training loss
0 50 100 150 200
Epoch9092949698100Training accuracy (b) Training accuracy
0 50 100 150 200
Epoch−1.0−0.50.00.5Testing loss (log scale) (c) Testing loss
0 50 100 150 200
Epoch858789919395Testing accuracy (d) Testing accuracy
Adam AdaBelief SGD STORM STORM+ META-STORM Ada-STORM
Figure 1: Results for CIFAR-10 dataset.
6 Experiments
In this section, we evaluate the performance of the proposed Ada-STORM method via numerical ex-
periments on image classification tasks and language modeling tasks. In the experiments, we compare
our method with STORM [Cutkosky and Orabona, 2019], STORM+ [Levy et al., 2021] and META-
STORM [Liu et al., 2022], as well as SGD, Adam [Kingma and Ba, 2015] and AdaBelief [Zhuang
et al., 2020]. We use the default implementation of SGD and Adam from Pytorch [Paszke et al., 2019].
For STORM+, we follow its original implementation3, and build STORM, META-STORM and our
Ada-STORM based on it. When it comes to hyper-parameter tuning, we simply set α= 0.3for our
algorithm. For other methods, we either set the hyper-parameters as recommended in the original
papers or tune them by grid search. For example, we search the learning rate of SGD, Adam and
AdaBelief from the set {1e−5,1e−4,1e−3,1e−2,1e−1}and select the best one for each method.
All the experiments are conducted on eight NVIDIA Tesla V100 GPUs.
Image classification task First, we conduct numerical experiments on multi-class image classifica-
tion tasks to evaluate the performance of the proposed method. Specifically, we train ResNet18 and
ResNet34 models [He et al., 2016] on the CIFAR-10 and CIFAR-100 datasets [Krizhevsky, 2009]
respectively. For all optimizers, we set the batch size as 256 and train for 200 epochs. We plot the
loss value and the accuracy against the epochs on the CIFAR-10 and CIFAR-100 datasets in Figure 1
and Figure 2. It is observed that, for training loss and training accuracy, our Ada-STORM algorithm
achieves comparable performance with respect to other methods, and it outperforms the others in
terms of testing loss and thus obtains a better testing accuracy.
Language modeling task Then, we perform experiments on language modeling tasks. Concretely,
we train a 2-layer Transformer [Vaswani et al., 2017] over the WiKi-Text2 dataset [Merity, 2016].
We use 256dimensional word embeddings, 512hidden unites and 2 heads. The batch size is set as 20
and all methods are trained for 40 epochs with dropout rate 0.1. We also clip the gradients by norm
0.25in case of the exploding gradient. We report both the loss and perplexity versus the number of
epochs in Figure 3. From the results, we observe that our method converges more quickly than other
3https://github.com/LIONS-EPFL/storm-plus-code
90 50 100 150 200
Epoch−4−20Training loss (log scale)(a) Training loss
0 50 100 150 200
Epoch9092949698100Training accuracy (b) Training accuracy
0 50 100 150 200
Epoch0.250.500.751.001.25Testing loss (log scale) (c) Testing loss
0 50 100 150 200
Epoch6668707274Testing accuracy (d) Testing accuracy
Adam AdaBelief SGD STORM STORM+ META-STORM Ada-STORM
Figure 2: Results for CIFAR-100 dataset.
0 10 20 30 40
Epoch567Training loss
(a) Training loss
0 10 20 30 40
Epoch5001000Training perplexity (b) Training perplexity
0 10 20 30 40
Epoch5.05.56.0Testing loss (c) Testing loss
0 10 20 30 40
Epoch200300400500Testing perplexity (d) Testing perplexity
Adam AdaBelief SGD STORM STORM+ META-STORM Ada-STORM
Figure 3: Results for WikiText-2 dataset.
methods and obtains a slightly better perplexity compared with others, indicating the effectiveness of
the proposed method.
7 Conclusion
In this paper, we propose an adaptive STORM method to achieve the optimal convergence rate for non-
convex functions. Compared with existing methods, our algorithm requires weaker assumptions and
does not have the additional O(logT)term in the convergence rate. The proposed technique can also
be employed to develop optimal adaptive algorithms for compositional optimization. Furthermore,
we investigate an adaptive method for non-convex finite-sum optimization, obtaining an improved
convergence rate of O(n1/4T−1/2). Given that STORM algorithm has already been used in many
areas such as bi-level optimization [Yang et al., 2021], federated learning [Das et al., 2022], min-max
optimization [Xian et al., 2021], sign-based optimization [Jiang et al., 2024b], etc., the proposed
methods may also inspire the development of adaptive algorithms in these fields.
Acknowledgements
This work was partially supported by National Key R&D Program of China (2021ZD0112802), NSFC
(62122037), and the Postgraduate Research & Practice Innovation Program of Jiangsu Province (No.
KYCX24_0231).
References
Y . Arjevani, Y . Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. Woodworth. Lower bounds for
non-convex stochastic optimization. Mathematical Programming , 199(1):165–214, 2023.
Y . Carmon and O. Hinder. Making SGD parameter-free. In Proceedings of the 35rd Annual
Conference on Learning Theory , pages 2360–2389, 2022.
K. Chen, J. Langford, and F. Orabona. Better parameter-free stochastic optimization with ODE
updates for coin-betting. In Proceedings of the 36th AAAI Conference on Artificial Intelligence ,
pages 6239–6247, 2022.
10A. Cutkosky and F. Orabona. Momentum-based variance reduction in non-convex SGD. In Advances
in Neural Information Processing Systems 32 , pages 15210–15219, 2019.
R. Das, A. Acharya, A. Hashemi, S. Sanghavi, I. S. Dhillon, and U. Topcu. Faster non-convex
federated learning via global and local momentum. In Proceedings of the 38th Conference on
Uncertainty in Artificial Intelligence , pages 496–506, 2022.
J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research , 12(61):2121–2159, 2011.
C. Fang, C. J. Li, Z. Lin, and T. Zhang. SPIDER: Near-optimal non-convex optimization via stochastic
path-integrated differential estimator. In Advances in Neural Information Processing Systems 31 ,
pages 689–699, 2018.
S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic program-
ming. SIAM Journal on Optimization , 23(4):2341–2368, 2013.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016.
W. Hu, C. J. Li, X. Lian, J. Liu, and H. Yuan. Efficient smooth non-convex stochastic composi-
tional optimization via stochastic recursive gradient descent. In Advances in Neural Information
Processing Systems 32 , pages 6543–6553, 2019.
F. Huang, J. Li, and H. Huang. Super-adam: Faster and universal framework of adaptive gradients.
InAdvances in Neural Information Processing Systems 34 , pages 9074–9085, 2021.
M. Ivgi, O. Hinder, and Y . Carmon. Dog is SGD’s best friend: A parameter-free dynamic step
size schedule. In Proceedings of the 40th International Conference on Machine Learning , pages
14465–14499, 2023.
W. Jiang, G. Li, Y . Wang, L. Zhang, and T. Yang. Multi-block-single-probe variance reduced estimator
for coupled compositional optimization. In Advances in Neural Information Processing Systems
35, pages 32499–32511, 2022a.
W. Jiang, B. Wang, Y . Wang, L. Zhang, and T. Yang. Optimal algorithms for stochastic multi-level
compositional optimization. In Proceedings of the 39th International Conference on Machine
Learning , pages 10195–10216, 2022b.
W. Jiang, J. Qin, L. Wu, C. Chen, T. Yang, and L. Zhang. Learning unnormalized statistical models
via compositional optimization. In Proceedings of the 40th International Conference on Machine
Learning , pages 15105–15124, 2023.
W. Jiang, S. Yang, W. Yang, Y . Wang, Y . Wan, and L. Zhang. Projection-free variance reduction
methods for stochastic constrained multi-level compositional optimization. In Proceedings of the
41st International Conference on Machine Learning , pages 21962–21987, 2024a.
W. Jiang, S. Yang, W. Yang, and L. Zhang. Efficient sign-based optimization: Accelerating con-
vergence via variance reduction. In Advances in Neural Information Processing Systems 37 ,
2024b.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction.
InAdvances in Neural Information Processing Systems 26 , 2013.
A. Kavis, S. Skoulakis, K. Antonakopoulos, L. T. Dadi, and V . Cevher. Adaptive stochastic variance
reduction for non-convex finite-sum minimization. In Advances in Neural Information Processing
Systems 35 , pages 23524–23538, 2022.
D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In International Conference
on Learning Representations , 2015.
A. Krizhevsky. Learning multiple layers of features from tiny images. Masters Thesis, Deptartment
of Computer Science, University of Toronto , 2009.
11K. Y . Levy, A. Kavis, and V . Cevher. STORM+: Fully adaptive SGD with recursive momentum
for nonconvex optimization. In Advances in Neural Information Processing Systems 34 , pages
20571–20582, 2021.
Z. Li, H. Bao, X. Zhang, and P. Richtarik. Page: A simple and optimal probabilistic gradient estimator
for nonconvex optimization. In Proceedings of the 38th International Conference on Machine
Learning , pages 6286–6295, 2021.
Z. Liu, T. D. Nguyen, T. H. Nguyen, A. Ene, and H. L. Nguyen. Meta-storm: Generalized fully-
adaptive variance reduced SGD for unbounded functions. ArXiv e-prints , arXiv:2209.14853,
2022.
I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In International
Conference on Learning Representations , 2017.
H. B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization. In
Proceedings of the 23rd Annual Conference on Learning Theory , 2010.
S. Merity. The wikitext long term dependency language modeling dataset. Salesforce Metamind , 9,
2016.
L. M. Nguyen, J. Liu, K. Scheinberg, and M. Takac. SARAH: A novel method for machine learning
problems using stochastic recursive gradient. In Proceedings of the 34th International Conference
on Machine Learning , pages 2613–2621, 2017.
F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic
learning. In Advances in Neural Information Processing Systems 27 , pages 1116–1124, 2014.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems 32 , 2019.
Q. Qi, Z. Guo, Y . Xu, R. Jin, and T. Yang. An online method for a class of distributionally robust
optimization with non-convex objectives. In Advances in Neural Information Processing Systems
34, pages 10067–10080, 2021.
N. L. Roux, M. Schmidt, and F. R. Bach. A stochastic gradient method with an exponential
convergence rate for finite training sets. In Advances in Neural Information Processing Systems 25 ,
pages 2672–2680, 2012.
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 4 , pages 26–31, 2012.
Q. Tran-Dinh, N. H. Pham, D. T. Phan, and L. M. Nguyen. Hybrid stochastic gradient descent
algorithms for stochastic nonconvex optimization. ArXiv e-prints , arXiv:1905.05920, 2019.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polo-
sukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30 , pages
5998–6008, 2017.
M. Wang, E. X. Fang, and H. Liu. Stochastic compositional gradient descent: algorithms for
minimizing compositions of expected-value functions. Mathematical Programming , 161(1-2):
419–449, 2017a.
M. Wang, J. Liu, and E. X. Fang. Accelerating stochastic composition optimization. Journal of
Machine Learning Research , 18:105:1–105:23, 2017b.
Z. Wang, K. Ji, Y . Zhou, Y . Liang, and V . Tarokh. SpiderBoost and momentum: Faster variance
reduction algorithms. In Advances in Neural Information Processing Systems 32 , pages 2406–2416,
2019.
W. Xian, F. Huang, Y . Zhang, and H. Huang. A faster decentralized algorithm for nonconvex minimax
problems. In Advances in Neural Information Processing Systems 34 , pages 25865–25877, 2021.
12J. Yang, K. Ji, and Y . Liang. Provably faster algorithms for bilevel optimization. In Advances in
Neural Information Processing Systems 34 , pages 13670–13682, 2021.
J. Yang, X. Li, I. Fatkhullin, and N. He. Two sides of one coin: the limits of untuned SGD and the
power of adaptive methods. In Advances in Neural Information Processing Systems 36 , pages
1206–1216, 2023.
H. Yuan, X. Lian, C. J. Li, J. Liu, and W. Hu. Efficient smooth non-convex stochastic composi-
tional optimization via stochastic recursive gradient descent. In Advances in Neural Information
Processing Systems 32 , pages 14905–14916, 2019.
J. Zhang and L. Xiao. A stochastic composite gradient method with incremental variance reduction.
InAdvances in Neural Information Processing Systems 32 , pages 9075–9085, 2019.
J. Zhang and L. Xiao. Multilevel composite stochastic optimization via nested variance reduction.
SIAM Journal on Optimization , 31(2):1131–1157, 2021.
L. Zhang, M. Mahdavi, and R. Jin. Linear convergence with condition number independent access of
full gradients. In Advances in Neural Information Processing Systems 26 , pages 980–988, 2013.
J. Zhuang, T. Tang, Y . Ding, S. C. Tatikonda, N. Dvornek, X. Papademetris, and J. Duncan. Ad-
abelief optimizer: Adapting stepsizes by the belief in observed gradients. In Advances in Neural
Information Processing Systems 33 , pages 18795–18806, 2020.
13A Proof of Theorem 1
First, we introduce the following lemma, which is frequently used in our proof.
Lemma 1 Suppose ciis positive for i={1,2,···, n}, and let 0< α < 1. We can ensure that:
 nX
i=1ci!1−α
≤nX
i=1ciPi
j=1cjα≤1
1−α nX
i=1ci!1−α
.
Proof 1 The proof mainly follows McMahan and Streeter [2010] and a similar analysis also appears
in Levy et al. [2021]. First, we prove the right part by Induction.
(1) For n= 1, we can easily show that the hypothesis holds:
c1
cα
1=c1−α
1≤1
1−αc1−α
1.
(2) Next, assuming that the hypothesis holds for n=t−1, then we show that it also holds for n=t.
Define Z=Pt
i=1ciandX=ct. For n=t, we have
tX
i=1ciPi
j=1cjα=t−1X
i=1ciPi
j=1cjα+ctPt
j=1cjα
≤1
1−α t−1X
i=1ci!1−α
+ctPt
j=1cjα
=1
1−α(Z−X)1−α+X
Zα:=h(X).
Taking the derivative concerning x, we know that
dh(X)
dX=1
Zα−1
(Z−X)α,
which indicates that h(X)decreases as Xincreasing. Since 0≤X≤Z,
max
0≤X≤Zh(X) =h(0) =1
1−αZ1−α=1
1−α tX
i=1ci!1−α
,
which implies that the hypothesis is true for n=t.
Combining (1) and (2), we finish the proof for the right part. Then, we give the proof of the left part
as follows:
nX
i=1ciPi
j=1cjα≥nX
i=1ciPn
j=1cjα= nX
i=1ci!1−α
.
Thus, we finish the proof of this lemma.
Next, we can obtain the following guarantee for our algorithm.
Lemma 2 Our method enjoys the following guarantee:
TX
t=1Eh
ηt∥vt∥2i
≤ 
2∆f+σ2
+E"
2L2
βTX
t=1η3
t∥vt∥2#
| {z }
(A)+E"
LTX
t=1η2
t∥vt∥2#
| {z }
(B)+E"
2βσ2TX
t=1ηt#
|{z }
(C).
14Proof 2 According to the definition of estimator vt, we can deduce that:
Eξt+1h
∥∇f(xt+1)−vt+1∥2i
=Eξt+1h
∥(1−β)vt+∇f(xt+1;ξt+1)−(1−β)∇f(xt;ξt+1)− ∇f(xt+1)∥2i
=Eξt+1[∥(1−β)(vt− ∇f(xt)) + (∇f(xt)− ∇f(xt+1) +∇f(xt+1;ξt+1)− ∇f(xt;ξt+1))
+β(∇f(xt;ξt+1)− ∇f(xt))∥2
≤Eξt+1
(1−β)2||vt− ∇f(xt)||2
+Eξt+1[||∇f(xt)− ∇f(xt+1)
+∇f(xt+1;ξt+1)− ∇f(xt;ξt+1) +β(∇f(xt;ξt+1)− ∇f(xt))||2
≤Eξt+1
(1−β)2||vt− ∇f(xt)||2
+ 2Eξt+1
||∇f(xt)− ∇f(xt+1) +∇f(xt+1;ξt+1)− ∇f(xt;ξt+1)||2
+ 2Eξt+1
||β(∇f(xt;ξt+1)− ∇f(xt))||2
≤(1−β)2Eξt+1
∥vt− ∇f(xt)∥2
+ 2β2Eξt+1
∥∇f(xt;ξt+1)− ∇f(xt)∥2
+ 2Eξt+1
∥∇f(xt+1;ξt+1)− ∇f(xt;ξt+1)∥2
≤(1−β)∥vt− ∇f(xt)∥2+ 2β2σ2+ 2L2∥xt+1−xt∥2
= (1−β)∥vt− ∇f(xt)∥2+ 2β2σ2+ 2L2η2
t∥vt∥2.
(11)
Note that ηtis independent of random variable ξt+1. So we can guarantee that:
Eξt+1h
ηt∥∇f(xt+1)−vt+1∥2i
≤(1−β)ηt∥vt− ∇f(xt)∥2+ 2β2σ2ηt+ 2L2η3
t∥vt∥2.
After rearranging, we have:
ηt∥vt− ∇f(xt)∥2
≤ηt
β∥vt− ∇f(xt)∥2−Eξt+1ηt
β∥vt+1− ∇f(xt+1)∥2
+ 2βσ2ηt+2L2
βη3
t∥vt∥2.
Letting Htbe the history to time t, i.e.,Ht={ξ1,···, ξt}, we ensure that
EHth
ηt∥vt− ∇f(xt)∥2i
≤EHtηt
β∥vt− ∇f(xt)∥2
−EHt+1ηt
β∥vt+1− ∇f(xt+1)∥2
+ 2βσ2EHt[ηt] +2L2
βEHth
η3
t∥vt∥2i
.
By summing up and noting that ηtis non-increasing such that ηt+1≤ηt, we have:
TX
t=1EHth
ηt∥vt− ∇f(xt)∥2i
≤EH1η1
β∥v1− ∇f(x1)∥2
+ 2βσ2TX
t=1EHt[ηt] +2L2
βTX
t=1EHth
η3
t∥vt∥2i
.(12)
Since we use a large batch size in the first iteration, that is, B0=T1/3, we can now ensure that
EH1h
∥v1− ∇f(x1)∥2i
≤σ2
B0=σ2
T1/3. Due to the fact that η1≤T−1/3andβ=T−2/3, the first
term of the above inequality is less than σ2. So, we can finally have:
TX
t=1Eh
ηt∥vt− ∇f(xt)∥2i
≤σ2+ 2βσ2TX
t=1E[ηt] +2L2
βTX
t=1Eh
η3
t∥vt∥2i
. (13)
15Also, due to the smoothness of f(x), we know that:
f(xt+1)≤f(xt) +⟨∇f(xt),xt+1−xt⟩+L
2∥xt+1−xt∥2
=f(xt)−ηt⟨∇f(xt),vt⟩+η2
tL
2∥vt∥2
=f(xt)−ηt⟨∇f(xt),vt⟩+ηt
2∥∇f(xt)∥2+ηt
2∥vt∥2−ηt
2∥∇f(xt)∥2
−ηt
2∥vt∥2+η2
tL
2∥vt∥2
=f(xt) +ηt
2∥∇f(xt)−vt∥2−ηt
2∥∇f(xt)∥2−ηt
2∥vt∥2+η2
tL
2∥vt∥2.
By summing up and re-arranging, we have:
TX
t=1ηt∥vt∥2≤2f(x1)−2f(xT+1) +TX
t=1ηt∥∇f(xt)−vt∥2+TX
t=1η2
tL∥vt∥2. (14)
Then, by using equation (13) and the fact that f(x1)−f∗≤∆f, we have:
TX
t=1Eh
ηt∥vt∥2i
≤2∆f+σ2+ 2βσ2TX
t=1E[ηt] + 2L2TX
t=1Eη3
t
β∥vt∥2
+LTX
t=1Eh
η2
t∥vt∥2i
,
which finishes the proof of Lemma 2.
To effectively bound each term in the above lemma, we divide the algorithm into two stages. Suppose
that starting from iteration t=s, the conditionPt
i=1∥vi∥2≥T1/3begins to hold. We refer to
iterations t={1,2,···, s−1}as the first stage, and t={s,···, T}as the second stage.
Bounding LHS: In the first stage, we know that
s−1X
t=1ηt∥vt∥2=1
T1/3s−1X
t=1∥vt∥2.
For the second stage, our analysis leads to:
TX
t=sηt∥vt∥2=TX
t=s∥vt∥2
T1−α
3Pt
i=1∥vi∥2α
≥TX
t=s∥vt∥2
T1−α
3
T1
3+Pt
i=s∥vi∥2α
≥TX
t=s∥vt∥2
T1−α
3
Tα
3+Pt
i=s∥vi∥2α
≥TX
t=s∥vt∥2
T1−α
3·2 maxn
Tα
3,Pt
i=s∥vi∥2αo
≥1
2T1−α
3min

1
Tα
3TX
t=s∥vt∥2, TX
t=s∥vt∥2!1−α


=1
2min

1
T1/3TX
t=s∥vt∥2, 
1
T1/3TX
t=s∥vt∥2!1−α


| {z }
:=Γ,
16where the first inequality stems fromPs−1
t=1∥vt∥2≤T1/3, the second inequality results from
(x+y)α≤xα+yαfor positive x, yand0< α < 1/3, and the forth inequality applies Lemma 1.
Next, we bound (A),(B),(C)as follows.
Bounding (A):In the first stage, with ηt=T−1/3,β=T−2/3, andPs−1
t=1∥vt∥2≤T1/3, we can
derive:
2L2
βs−1X
t=1η3
t∥vt∥2=2L2
T1/3s−1X
t=1∥vt∥2≤2L2.
For the second stage, the analysis gives:
2L2
βTX
t=sη3
t∥vt∥2≤2L2TX
t=s∥vt∥2
T1−3α
3Pt
i=s∥vi∥23α≤2L2
1−3α 
1
T1/3TX
t=s∥vt∥2!1−3α
,
where the second inequality uses Lemma 1. Then we also have:
2L2
1−3α 
1
T1/3TX
t=s∥vt∥2!1−3α
=2L2
1−3α(8−24α)1−3α 
1
(8−24α)1
T1/3TX
t=s∥vt∥2!1−3α
≤3α2L2
1−3α(8−24α)1−3α1
3α
+1
8T1/3TX
t=s∥vt∥2,
where the inequality employs Young’s inequality, such that xy≤3αx1
3α+(1−3α)y1
1−3αfor positive
x, y. Very similarly, we have:
2L2
1−3α 
1
T1/3TX
t=s∥vt∥2!1−3α
=2L2
1−3α8−24α
1−α1−3α
1−α1−α
8−24α1−3α
1−α 
1
T1/3TX
t=s∥vt∥2!1−3α
≤2α
1−α 
2L2
1−3α8−24α
1−α1−3α
1−α!1−α
2α
+1
8 
1
T1/3TX
t=s∥vt∥2!1−α
,
where the second inequality employs Young’s inequality, such that xy≤2α
1−αx1−α
2α+1−3α
1−αy1−α
1−3α
for positive x, y. Combining all above, we know that
(A)≤2L2+ 3α2L2
1−3α(8−24α)1−3α1
3α
+2α
1−α 
2L2
1−3α8−24α
1−α1−3α
1−α!1−α
2α
+Γ
8.
Bounding (B):In the first stage, with the learning rate set at ηt=T−1/3andPs−1
t=1∥vt∥2≤T1/3,
we observe:
Ls−1X
t=1η2
t∥vt∥2=L
T2/3s−1X
t=1∥vt∥2≤L
T1/3≤L.
For the second stage, our analysis reveals:
LTX
t=sη2
t∥vt∥2≤LTX
t=s∥vt∥2
T2(1−α)
3Pt
i=s∥vi∥22α≤L
1−2α 
1
T1/3TX
t=s∥vt∥2!1−2α
,
17where the second inequality leverages Lemma 1, and we have:
L
1−2α 
1
T1/3TX
t=s∥vt∥2!1−2α
=L
1−2α(8−16α)1−2α 1
(8−16α)1−2α 
1
T1/3TX
t=s∥vt∥2!1−2α
≤2α(8−16α)1−2αL
1−2α1
2α
+1
8T1/3TX
t=s∥vt∥2,
where the inequality is due to Young’s inequality, such that xy≤2αx1
2α+ (1−2α)y1
1−2αfor
positive x, y. Very similarly, we have:
L
1−2α 
1
T1/3TX
t=s∥vt∥2!1−2α
=L
1−2α8−16α
1−α1−2α
1−α1−α
8−16α1−2α
1−α 
1
T1/3TX
t=s∥vt∥2!1−2α
≤α
1−α 
L
1−2α8−16α
1−α1−2α
1−α!1−α
α
+1
8 
1
T1/3TX
t=s∥vt∥2!1−α
,
where the second inequality employs Young’s inequality, such that xy≤α
1−αx1−α
α+1−2α
1−αy1−α
1−2α
for positive x, y. Combining all the above, we know that
(B)≤L+ 2α(8−16α)1−2αL
1−2α1
2α
+α
1−α 
L
1−2α8−16α
1−α1−2α
1−α!1−α
α
+Γ
8.
Bounding (C):Given that β=T−2/3andηt≤T−1/3, we can easily know that (C)≤2σ2.
So far, we bound all terms in Lemma 2, and we can deduce
LHS≥E"
1
T1/3s−1X
t=1∥vt∥2#
+EΓ
2
;RHS≤EΓ
4
+C0,
where
C0= 
2∆f+ 3σ2+L+ 2L2
+ 3α2L2
1−3α(8−24α)1−3α1
3α
+2α
1−α 
2L2
1−3α8−24α
1−α1−3α
1−α!1−α
2α
+ 2α(8−16α)1−2αL
1−2α1
2α
+α
1−α 
L
1−2α8−16α
1−α1−2α
1−α!1−α
α
.(15)
These suggest that
E"s−1X
t=1∥vt∥2#
≤C0T1/3;E[Γ]≤4C0.
With the definition of Γ, we know that
E
min

1
T1/3TX
t=s∥vt∥2, 
1
T1/3TX
t=s∥vt∥2!1−α


≤4C0,
18which indicates the following by applying Jensen’s inequality:
E"
1
TTX
t=s∥vt∥#
≤max{(4C0)1
2,(4C0)1
2(1−α)}T−1/3.
Also, because of Jensen’s inequality, we have:
E"
1
Ts−1X
t=1||vt||#
≤vuuutE
 
1
Ts−1X
t=1||vt||!2
=vuuutE
1
T2 s−1X
t=1||vt||!2

≤vuutE"
s
T2s−1X
t=1||vt||2#
≤vuutE"
1
Ts−1X
t=1||vt||2#
≤r
1
TC0T1/3=p
C0T−1/3.
Summing up, we have proven that
E"
1
TTX
t=1∥vt∥#
≤max{3(C0)1
2,(C0)1
2+ (4C0)1
2(1−α)}T−1/3. (16)
Finally, we finish our proof by introducing the following lemma.
Lemma 3 Suppose 0< β < 1, our method ensures that
TX
t=1Eh
∥vt− ∇f(xt)∥2i
≤3σ2T1/3+E"
2L2
βTX
t=1η2
t∥vt∥2#
.
Proof 3 First note that we have already proven the following in equation (11).
Eh
∥∇f(xt+1)−vt+1∥2i
≤(1−β)E
∥vt− ∇f(xt)∥2
+ 2β2σ2+ 2L2E
η2
t∥vt∥2
.
After rearranging the items, we can get the following:
Eh
∥vt− ∇f(xt)∥2i
≤1
β
Eh
∥vt− ∇f(xt)∥2i
−Eh
∥vt+1− ∇f(xt+1)∥2i
+ 2βσ2
+2L2
βEh
η2
t∥vt∥2i
.
By summing up, we have:
TX
t=1Eh
∥vt− ∇f(xt)∥2i
≤1
βEh
∥v1− ∇f(x1)∥2i
+ 2βσ2T+2L2
βTX
t=1Eh
η2
t∥vt∥2i
.(17)
Since we use a large batch size in the first iteration, that is, B0=T1/3, we can now ensure that
Eh
∥v1− ∇f(x1)∥2i
≤σ2
B0=σ2
T1/3. Note that β=1
T2/3, so first term equals to σ2T1/3and the
second term reduces to 2σ2T1/3. To this end, we ensure
TX
t=1Eh
∥vt− ∇f(xt)∥2i
≤3σ2T1/3+2L2
βTX
t=1Eh
η2
t∥vt∥2i
.
Thus we finish the proof for this lemma.
Here, we bound the term2L2
βPT
t=1η2
t∥vt∥2as follows. In the first stage, with ηt=1
T1/3, we have:
2L2
βs−1X
t=1η2
t∥vt∥2=2L2
βs−1X
t=11
T2/3∥vt∥2≤2L2T1/3.
19For the second stage, the analysis gives:
2L2
βTX
t=sη2
t∥vt∥2≤2L2T2/3TX
t=s∥vt∥2
T2−2α
3(Pt
i=s∥vi∥)2α
≤2L2T2α/3
1−2α((1−2α)/L)1−2α
((1−2α)/L)1−2α TX
t=s∥vt∥2!1−2α
≤2α2L2T2α/3((1−2α)/L)1−2α
1−2α1/2α
+LTX
t=s∥vt∥2
≤2α2L2((1−2α)/L)1−2α
1−2α1
2α
T1/3+LTX
t=1∥vt∥2,
where the second inequality uses Lemma 1, and the third one employs Young’s inequality, such that
xy≤2αx1
2α+ (1−2α)y1
1−2αfor positive x, y. We also know that
2L2
βTX
t=sη2
t∥vt∥2≤2L2T2/3TX
t=s∥vt∥2
T2−2α
3(Pt
i=s∥vi∥)2α
≤2L2T2α/3
1−2α1−2α
(1−α)LT−α
31−2α
1−α1−α
1−2αTα
3L1−2α
1−α TX
t=s∥vt∥2!1−2α
≤α
1−α 
2L2T2α/3
1−2α1−2α
(1−α)LT−α
31−2α
1−α!1−α
α
+Tα
3L TX
t=s∥vt∥2!1−α
≤α
1−α 
2L2
1−2α1−2α
(1−α)L1−2α
1−α!1−α
α
T1/3+Tα
3L TX
t=s∥vt∥2!1−α
,
where the second inequality uses Lemma 1, and the third one employs Young’s inequality, such that
xy≤α
1−αx1−α
α+1−2α
1−αy1−α
1−2αfor positive x, y.
As a result, we have:
TX
t=1Eh
∥vt− ∇f(xt)∥2i
≤
3σ2+ 4C0L+L1+2α
2α2(1−2α)1−2α
1−2α1
2α
+L1
α 
2
1−2α1−2α
1−α1−2α
1−α!1−α
α
T1/3.
By integrating these findings, we can finally have:
E"
1
TTX
t=1∥∇f(xt)∥#
≤1
TE"TX
t=1∥vt∥#
+1
T"TX
t=1∥∇f(xt)−vt∥#
≤C′
T1/3,
where
C′= max {3(C0)1
2,(C0)1
2+ (4C0)1
2(1−α)}
+vuuut3σ2+ 4C0L+L1+2α
2α2(1−2α)1−2α
1−2α1
2α
+L1
α 
2
1−2α1−2α
1−α1−2α
1−α!1−α
α
=O
∆1
2(1−α)
f+σ1
1−α+L1
2α
,
20withC0defined in equation (15). We find that larger αleads to better dependence on Land worse
reliance on parameters ∆andσ. Forα→1
3, we can obtain that
E"
1
TTX
t=1∥∇f(xt)∥#
≤ O 
∆3/4
f+σ3/2+L3/2
T1/3!
.
Since we require 0< α <1
3, in practice, we can use α= 0.3instead, which leads to a convergence
rate of O
∆5/7
f+σ10/7+L5/3
T1/3
.
B Proof of Theorem 2
Since 20+ 21+···+ 2K−1<2K, running the algorithm for Titerations guarantees at least
K=⌊log(T)⌋complete stages. In the theoretical analysis, we can simply use the output of the last
complete stage K=⌊log(T)⌋, which has been at least run for 2K−1≥T/4iterations. According to
the analysis of Theorem 1, we have already known that running the Algorithm 1 for T/4iterations
leads to the following guarantee:
E[∥∇f(xτ)∥]≤ O
∆1
2(1−α)
f+σ1
1−α+L1
2α
(T/4)1/3
=O
∆1
2(1−α)
f+σ1
1−α+L1
2α
T1/3
,
which is on the same order of the original convergence rate.
C Proof of Theorem 3
According to equation (14), we have already proven that
TX
t=1ηt∥vt∥2≤2F(x1)−2F(xT+1) +TX
t=1ηt∥∇F(xt)−vt∥2+TX
t=1η2
tL∥vt∥2.
Then we bound the termPT
t=1ηt∥∇F(xt)−vt∥2as follows:
∥∇F(xt)−vt∥2≤2∥∇f(g(xt))∇g(xt)− ∇f(ut)∇g(xt)∥2+ 2∥∇f(ut)∇g(xt)−vt∥2
≤2C2L2∥g(xt)−ut∥2+ 2∥vt− ∇f(ut)∇g(xt)∥2.
Define that Gt=∇f(ut)∇g(xt), then we have:
TX
t=1Eh
ηt∥∇F(xt)−vt∥2i
≤2C2L2TX
t=1Eh
ηt∥g(xt)−ut∥2i
+ 2TX
t=1Eh
ηt∥vt−Gt∥2i
.
For the termPT
t=1Eh
ηt∥vt−Gt∥2i
, following the very similar analysis of equation (11), we have
the following guarantee:
Eξt+1,ζt+1h
∥vt+1−Gt+1∥2i
≤(1−β)∥vt−Gt∥2
+ 2β2Eξt+1,ζt+1h
∥∇f(ut+1;ξt+1)∇g(xt+1;ζt+1)− ∇f(ut+1)∇g(xt+1)∥2i
+ 2Eξt+1,ζt+1h
∥∇f(ut+1;ξt+1)∇g(xt+1;ζt+1)− ∇f(ut;ξt+1)∇g(xt;ζt+1)∥2i
≤(1−β)∥vt−Gt∥2+ 4C2σ2β2+ 4C2L2Eh
η2
t∥vt∥2i
+ 4C2L2Eh
∥ut+1−ut∥2i
.
21That is to say:
TX
t=1Eh
ηt∥vt−Gt∥2i
≤Eη1
β∥v1−G1∥2
+ 4C2σ2βE"TX
t=1ηt#
+4C2L2
βTX
t=1Eh
η3
t∥vt∥2i
+4C2L2
βTX
t=1Eh
ηt∥ut+1−ut∥2i
≤(1 + 4 C2)σ2+4C2L2
βTX
t=1Eh
η3
t∥vt∥2i
+4C2L2
βTX
t=1Eh
ηt∥ut+1−ut∥2i
,
where the last inequality due to the fact that β=T−2/3,ηt≤T−1/3, and we use a large batch size
T1/3in the first iteration. Next, we further bound the termPT
t=1Eh
ηt−1∥ut−ut−1∥2i
.First, we
can ensure that:
Eζth
∥ut−ut−1∥2i
=Eζth
∥β(g(xt;ζt)−ut−1) + (1 −β)(g(xt;ζt)−g(xt−1;ζt))∥2i
=Eζth
∥β(g(xt−1)−ut−1) + (g(xt;ζt)−g(xt−1;ζt)) +β(g(xt−1;ζt)−g(xt−1))∥2i
≤3β2∥g(xt−1)−ut−1∥2+ 3C2η2
t−1∥vt−1∥2+ 3β2σ2.
So we know that
1
βTX
t=1Eh
ηt∥ut+1−ut∥2i
≤3βTX
t=1Eh
ηt∥g(xt)−ut∥2i
+ 3C2E"TX
t=1η3
t
β∥vt∥2#
+ 3βσ2E"TX
t=1ηt#
.
So far, we have
TX
t=1Eh
ηt∥∇F(xt)−vt∥2i
≤26C2L2TX
t=1Eh
ηt∥ut−g(xt)∥2i
+8C2L2+ 24C4L2
βTX
t=1Eh
η3
t∥vt∥2i
+ (2 + 8 C2+ 24C2L2)σ2.
Next, we can boundPT
t=1ηt∥ut−g(xt)∥2following equation (11), as:
∥ut−g(xt)∥2≤1
β
∥ut−g(xt)∥2−Eζt+1h
∥ut+1−g(xt+1)∥2i
+ 2βσ2+2C2η2
t
β∥vt∥2.
So we can have
E"TX
t=1ηt∥ut−g(xt)∥2#
≤Eη1
β∥u1−g(x1)∥2
+ 2βσ2E"TX
t=1ηt#
+2C2
βTX
t=1Eh
η3
t∥vt∥2i
≤3σ2+2C2
βTX
t=1Eh
η3
t∥vt∥2i
,
22where the last inequality due to the fact that β=T−2/3,ηt≤T−1/3, and we use a large batch size
T1/3in the first iteration. Combining all, we have:
TX
t=1Eh
ηt∥vt∥2i
≤2∆F+ (2 + 8 C2+ 102 C2L2)σ2
+ (8C2L2+ 76C4L2)E"TX
t=1η3
t
β∥vt∥2#
+LE"TX
t=1η2
t∥vt∥2#
.
Treating ∆F, C, L, σ as constant, the above inequality is very similar to Lemma 2. Thus following
the very similar analysis after Lemma 2, we can show that:
E"
1
TTX
t=1∥vt∥#
≤ O(T−1/3).
According to previous analysis, we also have that
TX
t=1Eh
∥∇f(xt)−vt∥2i
≤26C2L2TX
t=1Eh
∥ut−g(xt)∥2i
+8C2L2+ 24C4L2
βE"TX
t=1η2
t∥vt∥2#
+ (2 + 8 C2+ 24C2L2)σ2T1/3
≤(2 + 8 C2+ 102 C2L2)σ2T1/3+ 8C2L2+ 76C4L2E"TX
t=1η2
t
β∥vt∥2#
,
which is similar to Lemma 3, and leads toPT
t=1E[∥∇f(xt)−vt∥/T]≤ O(T−1/3)following the
same analysis. Combing all these together, we can deduce that
1
TTX
t=1∥∇F(xt)∥ ≤ O (T−1/3),
which finishes the proof of Theorem 3.
D Proof of Theorem 4
Due to the smoothness of F(x), we have proven the following in equation (14):
TX
t=1ηt∥vt∥2≤2F(x1)−2F(xT+1) +TX
t=1ηt∥∇F(xt)−vt∥2+TX
t=1η2
tL∥vt∥2.
For the LHS, we have the following guarantee:
TX
t=1ηt∥vt∥2=TX
t=1∥vt∥2
n1−α
2Pt
i=1∥vi∥2α≥ 
1√nTX
t=1∥vt∥2!1−α
.
Then, we bound the terms in the RHS. First, we have the following lemma.
Lemma 4 Define that zt=∇fit(xt)−git
t+1
nPn
i=1gi
t, we have:
E"TX
t=1ηt∥∇F(xt)−vt∥2#
≤2βE"TX
t=1ηt∥∇F(xt+1)−zt+1∥2#
+ 2L2E"TX
t=1η3
t
β∥vt∥2#
.
23Proof 4 According to the definition of zt, the estimator vtcan be expressed as
vt= (1−β)vt−1+βzt+ (1−β) (∇fit(xt)− ∇fit(xt−1)).
Note that Eit+1[zt+1] =∇F(xt+1), and we have:
Eit+1h
∥∇F(xt+1)−vt+1∥2i
=Eit+1h(1−β)vt+βzt+1+ (1−β) 
∇fit+1(xt+1)− ∇fit+1(xt)
− ∇F(xt+1)2i
=Eit+1[∥(1−β)(vt− ∇F(xt)) +β(zt+1− ∇F(xt+1))
+(1−β) 
∇fit+1(xt+1)− ∇fit+1(xt) +∇F(xt)− ∇F(xt+1)
∥2
≤ ∥(1−β)(vt− ∇F(xt))∥2+Eit+1[∥β(zt+1− ∇F(xt+1))
+(1−β) 
∇fit+1(xt+1)− ∇fit+1(xt) +∇F(xt)− ∇F(xt+1)2i
≤(1−β)2∥vt− ∇F(xt)∥2+ 2β2Eit+1h
∥zt+1− ∇F(xt+1)∥2i
+ 2(1−β)2Eit+1h∇fit+1(xt+1)− ∇fit+1(xt) +∇F(xt)− ∇F(xt+1)2i
≤(1−β)2∥vt− ∇F(xt)∥2+ 2β2Eit+1h
∥zt+1− ∇F(xt+1)∥2i
+ 2(1−β)2Eit+1h∇fit+1(xt+1)− ∇fit+1(xt)2i
≤(1−β)2∥vt− ∇F(xt)∥2+ 2β2Eit+1h
∥zt+1− ∇F(xt+1)∥2i
+ 2L2∥xt+1−xt∥2
≤(1−β)∥vt− ∇F(xt)∥2+ 2β2Eit+1h
∥zt+1− ∇F(xt+1)∥2i
+ 2L2η2
t∥vt∥2.(18)
Rearrange the items and multiply the both sides by ηt, we can get the following:
Eh
ηt∥vt− ∇F(xt)∥2i
≤Eηt
β∥vt− ∇F(xt)∥2
−Eηt
β∥vt+1− ∇F(xt+1)∥2
+ 2βEh
ηt∥zt+1− ∇F(xt+1)∥2i
+2L2
βEh
η3
t∥vt∥2i
.
Note that ηtis non-increasing. By summing up, we have:
TX
t=1Eh
ηt∥vt− ∇F(xt)∥2i
≤Eη1
β∥v1− ∇F(x1)∥2
+ 2βTX
t=1Eh
ηt∥zt+1− ∇F(xt+1)∥2i
+2L2
βTX
t=1Eh
η3
t∥vt∥2i
.
Since we use a full batch in the first iteration, we can finish the proof of this lemma.
Next, we bound two terms in the above lemma.
Lemma 5 We can ensure that
2βTX
t=1Eh
ηt∥∇F(xt+1)−zt+1∥2i
≤12L2TX
t=1Eη3
t
β∥vt∥2
.
24Proof 5
Eit+1h
ηt∥∇F(xt+1)−zt+1∥2i
=Eit+1
ηt∇F(xt+1)− ∇fit+1(xt+1) +git+1
t+1−1
nnX
i=1gi
t+12

=Eit+1
ηt∇fit+1(xt+1)−git+1
t+1− 
∇F(xt+1)−1
nnX
i=1gi
t+1!2

≤Eit+1
ηt∇fit+1(xt+1)−git+1
t+12
=1
nnX
i=1ηt∇fi(xt+1)−gi
t+12,(19)
where the last equation is due to the fact that it+1is randomly sample from {1,2,···, n}. Note that
we also have that
1
nnX
i=1ηt∇fi(xt+1)−gi
t+12=Eit+1
ηt∇fit+1(xt+1)−git+1
t+12
≤Eit+1
ηt(1 + 2 n)∇fit+1(xt+1)− ∇fit+1(xt)2+ηt(1 +1
2n)∇fit+1(xt)−git+1
t+12
≤Eit+1
(1 + 2 n)L2η3
t∥vt∥2+ηt(1 +1
2n)∇fit+1(xt)−git+1
t+12
≤Eit+1h
(1 + 2 n)L2η3
t∥vt∥2
+ηt(1 +1
2n)
(1−1
n)∇fit+1(xt)−git+1
t2
+1
n∇fit+1(xt)− ∇fit+1(xt)2
≤Eit+1
3nL2η3
t∥vt∥2+ηt(1−1
2n)∇fit+1(xt)−git+1
t2
≤3nL2η3
t∥vt∥2+
1−1
2n1
nnX
i=1ηt∇fi(xt)−gi
t2.
That is to say, we can ensure that
1
nnX
i=1ηt+1∇fi(xt+1)−gi
t+12≤nX
i=1ηt∇fi(xt+1)−gi
t+12
≤3nL2η3
t∥vt∥2+
1−1
2n1
nnX
i=1ηt∇fi(xt)−gi
t2.
By rearranging and summing up, we have
1
2nTX
t=11
nnX
i=1ηt∇fi(xt)−gi
t2≤3nL2TX
t=1η3
t∥vt∥2+1
nnX
i=1η1∇fi(x1)−gi
12.
Since we use a full batch nin the first iteration, the second term equals zero, and thus we obtain:
TX
t=11
nnX
i=1ηt∇fi(xt)−gi
t2≤6n2L2TX
t=1η3
t∥vt∥2=6L2
β2TX
t=1η3
t∥vt∥2,
which leads to the result of this lemma.
Lemma 6 We have the following guarantee:
2L2TX
t=1η3
t
β∥vt∥2≤2α
1−α 
2L2
1−3α24−72α
1−α1−3α
1−α!1−α
2α
+1
24 
1√nTX
t=1∥vt∥2!1−α
.
25Proof 6
2L2TX
t=1η3
t
β∥vt∥2
=2L2nTX
t=1η3
t∥vt∥2= 2L2TX
t=1∥vt∥2
n1−3α
2Pt
i=1∥vi∥23α
≤2L2
1−3α 
1√nTX
t=1∥vt∥2!1−3α
=2L2
1−3α24−72α
1−α1−3α
1−α1−α
24−72α1−3α
1−α 
1√nTX
t=1∥vt∥2!1−3α
≤2α
1−α 
2L2
1−3α24−72α
1−α1−3α
1−α!1−α
2α
+1
24 
1√nTX
t=1∥vt∥2!1−α
,
where the last inequality employs Young’s inequality, such that xy≤2α
1−αx1−α
2α+1−3α
1−αy1−α
1−3αfor
positive x, y.
Lemma 7 We can ensure the following guarantee:
E"TX
t=1η2
tL∥vt∥2#
≤α
1−α 
L
1−2α4−8α
1−α1−2α
1−α!1−α
α
+1
4E
 
1√nTX
t=1∥vt∥2!1−α

Proof 7
TX
t=1η2
tL∥vt∥2=LTX
t=1∥vt∥2
n1−αPt
i=1∥vi∥22α
≤L
1−2α1
n1−α TX
t=1∥vt∥2!1−2α
≤L
1−2α 
1√nTX
t=1∥vt∥2!1−2α
=L
1−2α4−8α
1−α1−2α
1−α1−α
4−8α1−2α
1−α 
1√nTX
t=1∥vt∥2!1−2α
≤α
1−α 
L
1−2α4−8α
1−α1−2α
1−α!1−α
α
+1
4 
1√nTX
t=1∥vt∥2!1−α
,
where the last inequality employs Young’s inequality, such that xy≤α
1−αx1−α
α+1−2α
1−αy1−α
1−2αfor
positive x, y. Combing all these, we have already proven that
E"TX
t=1ηt∥vt∥2#
≥E
 
1√nTX
t=1∥vt∥2!1−α
,
TX
t=1ηt∥vt∥2≤C3+3
4E
 
1√nTX
t=1∥vt∥2!1−α
,
26where
C3=2∆ F+14α
1−α 
2L2
1−3α24−72α
1−α1−3α
1−α!1−α
2α
+α
1−α 
L
1−2α4−8α
1−α1−2α
1−α!1−α
α
.
So we can know that:
E
 
1√nTX
t=1∥vt∥2!1−α
≤4C3.
which indicate that
E
 
1
TTX
t=1∥vt∥2!1−α
≤4C3√n
T1−α
.
as well as
E"
1
TTX
t=1∥vt∥#
≤(4C3)1
2(1−α)·n1/4
T1/2.
To finish the proof, we also have to show the following lemma.
Lemma 8
E"TX
t=1∥∇F(xt)−vt∥2#
≤α√n
1−α 
14L2
1−2α2−4α
(1−α)L1−2α
1−α!1−α
α
+nα
2L
2E
 TX
t=1∥vt∥2!1−α
.
Proof 8 According to the previous proof, we know that:
TX
t=1Eh
∥∇F(xt)−vt∥2i
≤2βTX
t=1Eh
∥∇F(xt+1)−zt+1∥2i
+ 2L2TX
t=1Eη2
t
β∥vt∥2
≤14nL2TX
t=1Eh
η2
t∥vt∥2i
.
Also, we can deduce that:
14nL2TX
t=1η2
t∥vt∥2=14L2nαTX
t=1∥vt∥2
Pt
i=1∥vi∥22α
≤14L2nα
1−2α TX
t=1∥vt∥2!1−2α
=14L2nα
1−2α2−4α
(1−α)nα
2L1−2α
(1−α)(1−α)nα
2L
2−4α1−2α
1−α TX
t=1∥vt∥2!1−2α
≤α
1−α 
14L2nα
1−2α2−4α
(1−α)nα
2L1−2α
1−α!1−α
α
+nα
2L
2 TX
t=1∥vt∥2!1−α
where the last inequality employs Young’s inequality, such that xy≤α
1−αx1−α
α+1−2α
1−αy1−α
1−2αfor
positive x, y.
27As a result, we can ensure that
1
TTX
t=1E[∥∇F(xt)∥]≤1
TE"TX
t=1∥vt∥#
+1
T"TX
t=1∥∇f(xt)−vt∥#
≤n1/4
T1/2
 
14L2
1−2α2−4α
(1−α)L1−2α
1−α!1−α
2α
+p
2C3L+ (4C3)1
2(1−α)

=O
∆1
2(1−α)
F +L1
2αn1/4
T1/2
.
E Proof of Theorem 5
The analysis is very similar to that of Theorem 4, and the difference only appears in Lemma 5. For
this new method, we can also prove the same lemma:
Lemma 9
2βTX
t=1Eh
ηt∥∇F(xt+1)−zt+1∥2i
≤12L2TX
t=1Eη3
t
β∥vt∥2
.
Proof 9 This time, we have zt+1=∇fit+1(xt+1)− ∇fit+1(xτ) +∇F(xτ). And we can know that:
Eit+1h
ηt∥∇F(xt+1)−zt+1∥2i
=Eit+1h
ηt∇F(xt+1)− ∇fit+1(xt+1) +∇fit+1(xτ)− ∇F(xτ)2i
≤Eit+1h
ηt∇fit+1(xt+1)−fit+1(xτ)2i
≤ηtL2∥xt+1−xτ∥2
≤ηtL2ItX
i=τ∥xi+1−xi∥2
≤ηtL2ItX
i=τη2
i∥vi∥2≤L2ItX
i=τη3
i∥vi∥2.
By summing up, we have
2βTX
t=1Eh
ηt∥∇F(xt+1)−zt+1∥2i
≤2βE"TX
t=1L2ItX
i=τη3
i∥vi∥2#
≤2βL2I2E"TX
t=1η3
t∥vt∥2#
≤2L2E"TX
t=1η3
t
β∥vt∥2#
.
The other analysis is exactly the same as that of Theorem 4.
28NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The claims presented in the abstract and introduction accurately represent the
contributions and scope of the paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The theoretical results demonstrated in the paper rely on specific assumptions,
which have been clearly stated in the main text.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
29Answer: [Yes]
Justification: The paper provides assumptions and proofs for each theoretical result.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The paper discloses the information necessary to reproduce the main experi-
mental results.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
30Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: Due to privacy concerns and ongoing research, we do not include the code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The paper describes the training and testing details.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The paper reports error bars.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
31• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We have provided the relevant information.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conforms with the NeurIPS Code of
Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is primarily a theoretical paper with no potential negative social impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
32•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The creators or original owners of assets used in the paper are properly credited
and the license and terms of use explicitly are properly respected.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
33•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
34