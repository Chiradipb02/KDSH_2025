Boosting Vision-Language Models with Transduction
Maxime Zanella∗
UCLouvain, UMonsBenoît Gérin∗
UCLouvainIsmail Ben Ayed
ÉTS Montréal
Code: https://github.com/MaxZanella/transduction-for-vlms
Abstract
Transduction is a powerful paradigm that leverages the structure of unlabeled
data to boost predictive accuracy. We present TransCLIP, a novel and compu-
tationally efficient transductive approach designed for Vision-Language Models
(VLMs). TransCLIP is applicable as a plug-and-play module on top of popular
inductive zero- and few-shot models, consistently improving their performances.
Our new objective function can be viewed as a regularized maximum-likelihood
estimation, constrained by a KL divergence penalty that integrates the text-encoder
knowledge and guides the transductive learning process. We further derive an
iterative Block Majorize-Minimize (BMM) procedure for optimizing our objec-
tive, with guaranteed convergence and decoupled sample-assignment updates,
yielding computationally efficient transduction for large-scale datasets. We report
comprehensive evaluations, comparisons, and ablation studies that demonstrate:
(i) Transduction can greatly enhance the generalization capabilities of inductive
pretrained zero- and few-shot VLMs; (ii) TransCLIP substantially outperforms
standard transductive few-shot learning methods relying solely on vision features,
notably due to the KL-based language constraint.
1 Introduction
/uni00000025/uni0000002e/uni0000002b/uni00000032 /uni00000025/uni0000002e/uni0000002b/uni00000032 /uni00000025/uni0000002e/uni0000002b/uni00000032 /uni00000025/uni0000002e/uni0000002b/uni00000032 /uni00000025/uni0000002e/uni0000002b/uni00000032 /uni00000027/uni00000038/uni00000023/uni0000000f/uni00000025/uni0000002e/uni0000002b/uni00000032/uni00000017/uni00000017/uni00000018/uni00000012/uni00000018/uni00000017/uni00000019/uni00000012/uni00000019/uni00000017/uni0000001a/uni00000012/uni0000001a/uni00000017 /uni00000036/uni00000051/uni00000052/uni0000000f/uni00000013/uni00000002/uni00000023/uni00000045/uni00000045/uni00000057/uni00000054/uni00000043/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000007/uni0000000b
/uni00000034/uni00000047/uni00000055/uni00000030/uni00000047/uni00000056/uni0000000f/uni00000017/uni00000012 /uni00000034/uni00000047/uni00000055/uni00000030/uni00000047/uni00000056/uni0000000f/uni00000013/uni00000012/uni00000013 /uni00000038/uni0000004b/uni00000036/uni0000000f/uni00000024/uni00000011/uni00000015/uni00000014 /uni00000038/uni0000004b/uni00000036/uni0000000f/uni00000024/uni00000011/uni00000013/uni00000018 /uni00000038/uni0000004b/uni00000036/uni0000000f/uni0000002e/uni00000011/uni00000013/uni00000016 /uni0000001a/uni00000024/uni0000000d/uni00000002/uni00000017/uni00000010/uni00000015/uni00000007/uni0000000d/uni00000002/uni00000018/uni00000010/uni00000018/uni00000007/uni0000000d/uni00000002/uni00000016/uni00000010/uni00000018/uni00000007/uni0000000d/uni00000002/uni00000017/uni00000010/uni00000012/uni00000007/uni0000000d/uni00000002/uni00000016/uni00000010/uni0000001b/uni00000007
/uni0000000d/uni00000002/uni00000017/uni00000010/uni00000014/uni00000007/uni0000000d/uni00000002/uni00000016/uni00000010/uni00000013/uni00000007/uni0000000d/uni00000002/uni00000016/uni00000010/uni00000019/uni00000007/uni0000000d/uni00000002/uni00000016/uni00000010/uni00000012/uni00000007/uni0000000d/uni00000002/uni00000015/uni00000010/uni00000016/uni00000007
/uni0000000d/uni00000002/uni00000014/uni00000010/uni0000001a/uni00000007/uni0000000d/uni00000002/uni00000016/uni00000010/uni00000015/uni00000007 /uni0000000d/uni00000002/uni00000015/uni00000010/uni00000013/uni00000007/uni0000000d/uni00000002/uni00000015/uni00000010/uni00000013/uni00000007/uni0000000d/uni00000002/uni00000015/uni00000010/uni00000015/uni00000007/uni0000000d/uni00000002/uni00000016/uni00000010/uni00000015/uni00000007
/uni0000003c/uni00000047/uni00000054/uni00000051/uni0000000f/uni00000055/uni0000004a/uni00000051/uni00000056
/uni00000014/uni0000000f/uni00000055/uni0000004a/uni00000051/uni00000056/uni00000002/uni00000032/uni00000054/uni00000051/uni0000004f/uni00000052/uni00000056/uni00000002/uni00000036/uni00000057/uni00000050/uni0000004b/uni00000050/uni00000049
/uni00000014/uni0000000f/uni00000055/uni0000004a/uni00000051/uni00000056/uni00000002/uni00000023/uni00000046/uni00000043/uni00000052/uni00000056/uni00000047/uni00000054
/uni00000036/uni00000054/uni00000043/uni00000050/uni00000055/uni00000025/uni0000002e/uni0000002b/uni00000032/uni00000002/uni0000004b/uni0000004f/uni00000052/uni00000054/uni00000051/uni00000058/uni00000047/uni0000004f/uni00000047/uni00000050/uni00000056
Figure 1: TransCLIP improves significantly the averaged top-1 accuracy on 11 datasets when used
on top of inductive zero-shot CLIP , 2-shot CoOp prompt tuning and 2-shot TaskRes adapter for
various encoder sizes.
∗Equal contributions and corresponding authors. {maxime.zanella,benoit.gerin}@uclouvain.be
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Combining vision and language modalities can greatly enhance expressiveness and reduce ambiguities
in the understanding and interpretation of our environment. This principle is central in the develop-
ment of Vision-Language Models (VLMs), such as CLIP [ 50], which learns visual representations
through natural-language supervision. In the pre-training phase, an input image xand associated text
description care encoded by separate vision and text encoders. This yields feature representations
f=θv(x)andt=θt(c), which can be aligned by contrastive learning. Such a joint embedding
space for the visual and textual modalities facilitates zero-shot recognition and yields powerful
adaptation capabilities for a large variety of tasks. The recent literature on adapting VLMs has
grown substantially, in both the zero-shot and few-shot learning settings [73, 72, 16, 70, 74, 25, 43].
However, so far, these techniques predominantly align with induction , i.e., inference for each test
sample is performed independently from the other samples within the target dataset.
In contrast, transduction performs joint inference on all the test samples of a task, leveraging the
statistics of the target unlabeled data [ 58,27,71]. In the context of standard vision-based classifiers,
this has enabled transductive methods to outperform inductive-inference approaches as evidenced by
benchmarks over large-scale datasets such as ImageNet [2].
Within the scope of deep learning, transduction has mainly been explored for few-shot learning to
address the inherent challenges of training under limited supervision. This recent and quite abundant
few-shot literature, e.g., [ 5,13,34,37,44,76,24,75], among others, has focused on adopting
standard vision-based pre-training models (such as ImageNet pre-training). However, as we will
show in our experiments (Table 4), the direct application of existing transductive few-shot methods to
VLMs yields poor performances, sometimes underperforming the inductive zero-shot predictions.
This might explain why the transductive paradigm has been overlooked in zero-shot and few-shot
learning for VLMs so far. The low performance of current transductive few-shot methods in the
context of VLMs could be explained by the fact that the underlying objective functions do not
account for the text knowledge. In this new multi-modal paradigm, additional supervision could
be leveraged from the textual descriptions of the classes (prompts) [ 50], e.g., ck=a photo of a
[kth class name ], along with their corresponding representation tk=θt(ck)derived from the
language encoder. We utilize the interleaved representation of text prompts and images with their
cosine2similarity f⊤tk, which yields text-based prediction ˆyk, thereby guiding our transductive
optimization procedure with text-encoder knowledge. Our method optimizes a new objective function
integrating a text-driven penalty. Optimization is carried out efficiently w.r.t the assignment variables
associated with the unlabeled samples, which are then used as final predictions.
Adapting VLMs has recently attracted wide attention in the literature, predominantly focusing on
inductive methods. Motivated by findings in NLP, which indicate that better prompt strategies could
enhance performance [ 53,26,22], substantial efforts were directed towards prompt tuning [ 35]
for VLMs, with CoOp [ 73] standing out as the pioneering work along this line. Following CoOp,
prompt tuning has become the favorite strategy for adapting VLMs in a variety of contexts, including
unsupervised [ 25,43,15,41,1] and few-shot [ 73,72,40,12,65,6,74,8,9,29,30,67] learning.
Meanwhile, there have been a few efforts towards computationally more efficient adapters [ 70,48,66].
Our transduction formulation aligns with this initiative. By operating solely on the output embeddings
(i.e., in a black-box setting), TransCLIP is computationally efficient and does not make assumptions
on the underlying encoder architectures. Still, our method is orthogonal to these design choices and
could be applied atop any of the above-mentioned inductive approaches.
Main contributions. (i) We introduce a transductive formulation that enhances the zero-shot
and few-shot generalization capabilities of VLMs by leveraging the structure of unlabeled data
(Figure 1). Our new objective function can be viewed as a regularized maximum-likelihood estimation,
constrained by a Kullback-Leibler (KL) divergence penalty integrating the text-encoder knowledge
and guiding the transductive learning process. We further derive an iterative Block Majorize-Minimize
(BMM) procedure for optimizing our objective, with guaranteed convergence and decoupled sample-
assignment updates, yielding computationally efficient transduction for large-scale datasets, such
as ImageNet. (ii)Our method can be used as a plug-and-play module on top of current inductive
zero-shot models and few-shot learning methods, consistently boosting their performance. Also, (iii)
our approach substantially outperforms recent transductive few-shot methods in the literature, notably
due to the KL-based language supervision as a critical success factor.
2In VLMs, such as CLIP [ 50], both visual and text embeddings are normalized (i.e., are withing the unit
hyper-sphere). Thus, the cosine similarity corresponds to the dot product.
22 Related Work
Transduction for vision-only classifiers. The use of unlabeled test data at inference time has
received attention lately in rapidly emerging subjects, such as few-shot learning and unsupervised test-
time adaptation. Examples include adjusting batch normalization layer statistics [ 46] and minimizing
the entropy of predictions [ 60], which can be supplemented by pseudo-labeling strategies [ 36]. In
the few-shot literature solely based on vision models, transduction leverages both the few labeled
samples and unlabeled test data, outperforming inductive methods [ 76,5,24,37,75]. One of
the first works introducing transduction in vision-based few-shot learning proposes propagating
the labels from the support (labeled) to the query (unlabeled) set with a meta-learned graph [ 39].
Building on this idea, another work proposes to iteratively augment the support set to improve label
propagation [ 34]. LaplacianShot [ 76] also exploits the inherent structure of the data through a graph-
Laplacian clustering, which discourages disparate class predictions for samples with close features,
while matching each query set point to the nearest support prototype. Alternative approaches propose
directly learning the class prototypes. For instance, Transductive Fine-Tuning (TF) [ 13] uses the
prediction entropy on the query samples as a regularization term, while TIM and its variants [ 5,59]
employ the mutual information between the query samples and their predictions. BD-CSPN [ 37]
refines the class prototypes by reducing the feature biases between the support set and the most
confident query samples. An additional group of methods performs clustering in the feature space,
for instance, by solving an optimal transport problem like PT-MAP [ 24], by projecting features into
sub-spaces to facilitate clustering [ 75], or by revisiting the standard K-means with an additional
partition-complexity regularizer to control the number of predicted classes [44].
Zero- and few-shot learning in VLMs. Thanks to their extensive pre-training, VLMs exhibit
stronger generalization capabilities than vision-only models but may also fail [ 50,69,57]. In response,
substantial recent efforts have been directed towards using their general knowledge and adapting
them on more specific tasks [ 63,73,70]. Arguably, the most popular strategy is prompt tuning [ 35],
which is explored both in the unsupervised [ 43,15,41,1] and few-shot [ 73,72,40,12,65,6,74,8,
9,29,30] settings. The pioneering work, CoOp [ 73], updates input text-prompt tokens by leveraging
the context provided by the few labeled samples (i.e., the support set). Building on this success,
various strategies have been developed to enhance this approach, especially through additional
regularization. For instance, ProGrad [ 74] guides the prompts towards the original hand-crafted ones
by gradient projection. Prompt tuning has also been explored in the zero-shot setting, e.g., using
the predictive confidence to generate pseudo-labels [ 25,41]. Despite its popularity, prompt tuning
remains tedious in terms of computations, due to the many back-propagations through the text encoder.
This challenge is compounded in the recent developments, which introduce visual tokens [ 29,30]
alongside the text tokens. In contrast, there has been limited efforts so far in developing black-box
methods [ 48,17,16,66,62], which only access the final embedding states. These methods often
rely on the so-called adapters [ 23], like Tip-Adapter(-F) [ 70], which adds a classifier at the output of
the vision encoder, in the form of a cache model involving the few-shot samples. Lately, a strong
baseline based on Gaussian discriminant analysis clustering [ 62] demonstrates VLMs’ adaptation
abilities with a Gaussian hypothesis on the embedding space.
Transductive inference in VLMs. Despite the growing interest in unsupervised, zero-shot and
few-shot learning for VLMs, the transductive-inference paradigm has not been explored so far in this
new multi-modal context, except for the very recent work in [ 45], which was deployed for small-size
tasks ( ≈102test samples). However, the method in [ 45] may not be computationally tractable
for large-scale query sets, due to expensive inner loops for estimating the Dirichlet distribution’s
parameters. We provide a computationally efficient solution, which can scale up to large target
datasets (such as ImageNet), while being easily amenable as a plug-and-play module on top of
state-of-the-art inductive methods. It is worth mentioning that test-time adaptation methods also
employ the transduction paradigm, but their settings are very different from those studied in this
work. For instance, SwapPrompt [ 41] has been designed to make batch predictions on-the-fly, and
has continual-learning mechanisms such as an exponential moving average prompt across batches.
TPT [ 43] work on a single sample with many data augmentations to train one prompt per image. Both
methods require access to model weights for training (i.e., do not operate in a black-box setting) and
an expensive training procedure. We also note that prompt tuning does not scale well with the model
size and is even impractical on very large models such as EV A-CLIP-8B [ 55]. We still report the
performances of this class of methods in the Appendix (Table 9).
33 TransCLIP: Transduction for Vision-Language Models
In this section, we describe our objective function for transductive inference in vision-language
models, and derive a block majorize-minimize (BMM) algorithm for minimizing it, with guaranteed
convergence and decoupled sample-assignment updates. When dealing with a zero-shot classification
problem based on a vision-language model, such as CLIP, and given a set of Kcandidate classes,
one creates textual descriptions, the so-called prompts [ 38], each corresponding to a class, e.g., ck
=a photo of a [kth class name ],k= 1, . . . , K . Lettk=θt(ck)denotes the corresponding
normalized (unit hyper-sphere) embedding representation, with θtrepresenting the language encoder.
Similarly, each test image xi,i= 1, . . . , N , is projected onto a normalized embedding space of
the same dimension, using visual encoder θv:fi=θv(xi). In the standard inductive zero-shot
inference, classification of a given image xiis done by evaluating the cosine similarity between these
two encoded modalities and predicting the class corresponding to the most similar text embedding:
ˆk= argmaxkf⊤
itk. Furthermore, one can compute pseudo-labels corresponding to these zero-
shot predictions by applying the softmax function with a temperature scaling3τ, which yields the
following probability-simplex vector for each sample:
ˆyi= (ˆyi,k)1≤k≤K∈∆K; ˆyi,k=exp(τf⊤
itk)P
jexp(τf⊤
itj)(1)
where ∆Kdenotes the probability simplex. Let D={i∈N: 1≤i≤N}=S ∪ Q denotes the
samples indices of the target dataset, with Qthe set of unlabeled query samples indices, i.e., those for
which we want to make a prediction, and Sthe set of labeled support samples indices in the few-shot
setting.
Note that, in the zero-shot setting, S=∅. We define a Gaussian Mixture Model-clustering (GMM)
term in our objective function by modeling the likelihood of these target data as a balanced mixture
of multivariate Gaussian distributions, each representing a class kand parameterized by mean vector
µkand a diagonal covariance matrix Σ:
pi,k=Pr(fi, k;µk,Σ)∝det(Σ)−1
2exp
−1
2(fi−µk)⊤Σ−1(fi−µk)
Notation pi,kis introduced here to simplify the equations in the sequel. Notice that, unlike standard
GMMs, we deploy a common diagonal covariance matrix Σacross all classes. Interestingly, in
our experiments, we found this simplifying choice improves the performance while reducing the
computational load as there are substantially fewer parameters to learn. This is particularly the case
when dealing with large numbers of classes as in large-scale target datasets such as ImageNet.
3.1 Proposed objective function
Our objective function depends on two types of variables: (i) Sample-to-class assignment variables
within the probability simplex: zi= (zi,k)1≤k≤K∈∆K,i∈ Q ; and (ii) GMM parameters
µ= (µk)1≤k≤KandΣ.
We propose to minimize the following objective, which integrates a GMM-clustering term, a Laplacian
regularizer and a Kullback-Leibler (KL) divergence penalty encoding the text-encoder knowledge
and guiding the transductive learning process:
LZERO-SHOT(z,µ,Σ) =−X
i∈Qz⊤
ilog(pi)
|{z }
GMM clustering−X
i∈DX
j∈Dwijz⊤
izj
| {z }
Laplacian reg.+X
i∈QKLλ(zi||ˆyi)
|{z }
Text knowledge(2)
where pi= (pi,k)1≤k≤K∈∆Kconcatenates the GMM probabilities, wijdenotes some measure
of affinity between visual embeddings fiandfj, and the sample-wise parameterized4KL terms are
given by:
KLλ(zi||ˆyi) =z⊤
ilogzi−λz⊤
ilogˆyi, i∈ Q;λ >0 (3)
In the following, we describe the effect of each term in our objective function in (2):
3Note that each CLIP version comes with a temperature scaling factor τ, which is optimized along with the
learnable parameters during pre-training.
4Notice that, for λ= 1, the expression in (3) corresponds to the KL divergence.
4•GMM-based clustering : This unsupervised-learning term is akin to the GMM-based
maximum-likelihood estimation objective in the standard EM algorithm [ 3]. By taking
the negative logarithm, its minimization corresponds to maximizing the likelihood of the
data. It can also be viewed as a probabilistic generalization of the K-means clustering
objective [ 28]. Indeed, assuming Σis the identity matrix reduces the first term in (2)to the
K-means objective.
•Laplacian regularization : The second term in (2)is the Laplacian regularizer, widely used
in the context of graph/spectral clustering [ 56] and semi-supervised learning [ 7]. This term
encourages nearby samples in the visual-embedding space (i.e., pairs of samples with high
affinity wi,j) to have similar zassignments. In our case, we propose to build a positive semi-
definite (PSD) affinity matrix based on the cosine similarities as wij=f⊤
ifj(Gram matrix).
As we see below, this PSD condition is important to obtain a convergent Majorize-Minimize
optimizer with decoupled (parallel) sample-wise updates for the z-assignments, yielding a
highly efficient transduction for large-scale target datasets (such as ImageNet).
•Text-guided KL divergence: This term is dedicated to vision-language models and, as we
will see in our experiments (ablation studies in Tables 4 and 6), has a substantial effect on
performance. It encourages the prediction not to deviate significantly from the zero-shot
predictions, thereby providing text supervision to the other two unsupervised-learning terms.
Furthermore, being convex over zi,i∈ Q , this term facilitates the optimization of the
objective w.r.t the assignment variables.
3.2 Extension to the few-shot setting
Our zero-shot formulation naturally extends to the few-shot setting. We integrate supervision from
the labeled-support samples, in the form of a cross-entropy, which corresponds to minimizing the
following overall loss:
LFEW-SHOT(z,µ,Σ) =−γ
|S|X
i∈Sz⊤
ilog(pi) +1
|Q|LZERO-SHOT(z,µ,Σ) (4)
Note that, in the first term, the ziare fixed, with zi=yi,i∈ S andyithe one-hot ground-truth label
associated with the corresponding shot.
3.3 Block Majorize-Minimize (BMM) optimization
As our objective depends on three types of variables ( z,µ,Σ), we proceed with a BMM procedure,
alternating three sub-step optimizers. Each sub-step optimizes over one block of variables while
the other two are fixed, ensuring the overall objective does not increase. Importantly, the obtained
z-updates (Eq. (5)) are decoupled, yielding computationally efficient transduction for large-scale
datasets. Also, our overall procedure is guaranteed to converge (Theorem 1).
Majorize-Minimize (MM) with respect to the z-block WhenµandΣare fixed, both the GMM-
and KL-based terms are convex w.r.t zi. However, the Laplacian term is concave5(for PSD matrix
W). Therefore, we proceed with inner iterations, each minimizing a linear and tight upper bound,
the so-called majorizing function in the MM-optimization literature [ 33,21,31], which guarantees
the overall objective does not increase. To obtain the tight linear bound, let us write the Laplacian
term conveniently in the following matrix form: z⊤Ψz, with Ψ=−W⊗I, where ⊗denotes the
Kronecker product and Iis the N×Nidentity matrix. Note that Ψis negative semi-definite for a
positive semi-definite W. Therefore, z⊤Ψzis a concave function with respect to z, and its first-order
approximation at current solution zl(lbeing the iteration index) gives the following tight6upper
bound on the Laplacian term:
z⊤Ψz≤(zl)⊤Ψzl+ (Ψzl)⊤(z−zl)
Replacing the quadratic Laplacian term by this linear bound yields a majorizing function on our
overall objective. Importantly, this majorizing function is a sum of decoupled objectives, each
5This makes the overall sub-problem non-convex and there is no closed-form solution.
6‘’Tight” means that the upper bound is equal to the original objective at the current solution zl.
5corresponding to one assignment variable zi, yielding a highly efficient optimizer for large-scale
target datasets. Indeed, using simplex constraints zi∈∆K, i∈ Q, and solving the Karush-Kuhn-
Tucker (KKT) conditions independently for each zi, we obtain the following decoupled update rules
for the z-block:
z(l+1)
i =ˆyλ
i⊙exp(log( pi) +P
j∈Dwijz(l)
j)
(ˆyλ
i⊙exp(log( pi) +P
j∈Dwijz(l)
j))⊤ 1K(5)
Closed-form updates of µandΣWhen both zandΣare fixed, our objective in (4)is convex. It
can be minimized by setting its gradient w.r.t each µkto zero, which yields the following closed-form
updates:
µk=γ
|S|P
i∈Szi,kfi+1
|Q|P
i∈Qzi,kfi
γ
|S|P
i∈Szi,k+1
|Q|P
i∈Qzi,k(6)
Similarly, when both zandµare fixed, the following closed-form updates minimize the overall
objective w.r.t Σ:
diag(Σ) =γ
|S|P
i∈SP
kzi,k(fi−µk)2+1
|Q|P
i∈QP
kzi,k(fi−µk)2
γ+ 1(7)
The complete procedure is summarized in Appendix B. Note that, after convergence, we use the
sample-to-class assignment variables zias predictions for each sample iof the query set Qusing the
argmax operation for conventional classification.
3.4 Convergence
Our optimizer can be viewed as an instance of the general Block Majorize-Minimize paradigm
for optimization [ 51], which optimizes a majorizing function for each block of variables. The
convergence of general BMM procedures is well studied in the optimization community [ 51]. Indeed,
under certain conditions (such as the strong convexity of the block-wise majorizing functions), we
can establish convergence of our procedure using the following result (more details in Appendix A):
Theorem 1 (Convergence of BMM [51]) Assume that, for each block, the majorizing function is
quasi-convex, and its first-order behavior is the same as the original objective locally. Furthermore,
assume that the sub-problem solved for each block has a unique solution. Then, every limit point of
the iterates generated by BMM is a coordinate-wise minimum of the overall objective.
4 Experiments
Datasets. Following the setting of previous works [ 73,43], we assess TransCLIP on ImageNet [ 11]
and ten datasets for fine-grained classification of scenes (SUN397 [ 64]), aircraft types (Aircraft [ 42]),
satellite imagery (EuroSAT [ 18]), automobiles (Cars [ 32]), food items (Food [ 4]), pet breeds
(Pets [ 49]), flowers (Flowers [ 47]), general objects (Caltech101 [ 14]), textures (DTD [ 10]) and
human actions (UCF101 [ 54]). We additionally measure performance on four variants of ImageNet
(Adversarial [ 20], ImageNetV2 [ 52], Rendition [ 19], Sketch [ 61]). Numerical results are reported in
terms of the top-1 accuracy with the ViT-B/16 encoder, averaged over three random seeds.
Benchmarks. We aim to show the breadth of potential applications of transduction in the context of
VLMs. Notably, employing supervised fine-tuning, followed by transduction with TransCLIP on the
unlabeled test samples, emerges as a powerful and efficient solution. This is particularly convenient
when the labeled samples (the support set) and/or computational power are not accessible at inference
(i.e., test) time7. To this end, we first study the applicability of our zero-shot formulation TransCLIP-
ZS (Eq. (2)) across three settings: (i) on top of inductive zero-shot learning and popular few-shot
learning methods; (ii) on top of 16-shot ImageNet pretraining for cross-dataset transferability, and
7This application is hardly discussed in the transductive literature. We make all zero-shot- and few-shot text
and image embeddings publicly available, to ease future works without resorting to heavy computations.
6Table 1: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods.
Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flower102 Caltech101DTDUCF101 Average0-shotCLIP-ViT-B/16 66.6 62.5 24.7 48.3 65.6 85.9 89.1 70.7 93.2 43.5 67.5 65.3
+ TransCLIP-ZS 70.3+3.768.9+6.326.9+2.265.1+16.869.4+3.887.1+1.292.6+3.576.7+5.992.7-0.549.5+6.074.4+6.970.3+5.11-shotCoOp (IJCV ’22) 65.7 66.9 20.7 56.4 67.6 84.3 90.2 78.2 92.5 50.1 71.2 67.6
+ TransCLIP-ZS 69.3+3.671.5+4.623.8+3.165.3+8.971.9+4.386.3+2.091.9+1.889.8+11.593.8+1.355.4+5.477.7+6.572.4+4.8
TIP-Adapter-F (ECCV ’22) 69.5 67.2 28.8 67.8 67.1 85.8 90.6 83.7 94.0 51.6 73.4 70.9
+ TransCLIP-ZS 72.0+2.571.8+4.630.7+1.976.9+9.171.0+3.986.9+1.193.1+2.492.8+9.193.5-0.557.7+6.180.0+6.775.1+4.3
PLOT (ICLR ’23) 66.9 67.0 28.9 72.8 68.5 84.9 91.9 81.8 94.0 52.8 74.7 71.3
+ TransCLIP-ZS 75.8+8.970.3+3.328.1-0.878.8+6.070.0+1.685.3+0.491.1-0.893.2+11.494.0-0.056.7+3.981.4+6.775.0+3.7
TaskRes (CVPR ’23) 69.6 68.1 31.2 65.6 69.1 84.5 90.2 81.6 93.6 53.4 71.8 70.8
+ TransCLIP-ZS 72.0+2.572.5+4.431.4+0.273.7+8.171.6+2.486.5+2.091.6+1.590.7+9.194.0+0.459.4+6.076.4+4.674.5+3.7
ProGrad (ICCV ’23) 67.0 67.0 28.7 57.0 68.2 84.9 91.4 80.8 93.5 52.8 73.3 69.5
+ TransCLIP-ZS 70.1+3.171.6+4.630.5+1.870.9+13.972.3+4.186.5+1.692.7+1.491.5+10.794.1+0.757.9+5.179.3+6.174.3+4.84-shotCoOp (IJCV ’22) 68.8 69.7 30.8 69.6 74.4 84.5 92.5 92.2 94.5 59.4 77.5 74.0
+ TransCLIP-ZS 71.4+2.673.3+3.533.1+2.377.2+7.577.7+3.286.5+1.993.6+1.195.3+3.195.1+0.663.0+3.681.8+4.377.1+3.1
TIP-Adapter-F (ECCV ’22) 70.7 70.8 35.7 76.8 74.1 86.5 91.9 92.1 94.8 59.8 78.1 75.6
+ TransCLIP-ZS 72.7+1.974.4+3.536.1+0.579.7+2.975.9+1.887.4+0.993.2+1.395.5+3.395.1+0.364.0+4.283.3+5.277.9+2.3
PLOT (ICLR ’23) 70.0 71.8 34.8 84.7 76.6 83.5 92.8 93.2 94.9 61.0 79.7 76.6
+ TransCLIP-ZS 77.2+7.273.5+1.733.9-0.981.8-2.975.8-0.885.6+2.292.5-0.395.8+2.694.8-0.163.6+2.683.3+3.678.0+1.4
TaskRes (CVPR ’23) 71.0 72.8 33.3 73.8 76.1 86.1 91.9 85.0 94.9 59.7 75.5 74.6
+ TransCLIP-ZS 73.0+2.075.3+2.534.4+1.178.1+4.477.2+1.187.3+1.293.0+1.192.4+7.495.1+0.264.3+4.679.2+3.777.2+2.7
ProGrad (ICCV ’23) 70.2 71.7 34.0 69.5 75.0 85.4 92.0 91.1 94.4 59.8 77.9 74.6
+ TransCLIP-ZS 72.3+2.175.0+3.335.5+1.674.9+5.377.9+2.987.0+1.593.7+1.795.3+4.295.1+0.864.8+5.183.2+5.477.7+3.116-shotCoOp (IJCV ’22) 71.9 74.9 43.3 85.0 82.8 84.2 91.9 96.8 95.8 69.7 83.1 79.9
+ TransCLIP-ZS 73.3+1.476.6+1.842.9-0.486.0+1.083.0+0.286.3+2.193.2+1.297.5+0.895.9+0.171.3+1.785.4+2.381.1+1.1
TIP-Adapter-F (ECCV ’22) 73.3 76.0 44.6 85.9 82.3 86.8 92.6 96.2 95.7 70.8 83.9 80.7
+ TransCLIP-ZS 74.2+0.976.8+0.844.9+0.385.2-0.782.7+0.487.4+0.693.5+0.996.9+0.795.7-0.169.2-1.585.6+1.781.1+0.4
PLOT (ICLR ’23) 72.5 76.0 46.8 92.1 84.6 85.6 92.5 97.1 96.0 71.1 84.8 81.7
+ TransCLIP-ZS 77.8+5.375.0-1.041.8-4.984.6-7.579.6-4.985.9+0.292.2-0.497.3+0.195.0-1.068.7-2.485.7+0.980.3-1.4
TaskRes (CVPR ’23) 73.0 76.0 44.8 80.7 83.5 86.9 92.5 97.3 95.9 70.9 83.4 80.5
+ TransCLIP-ZS 74.1+1.076.9+0.843.6-1.280.5-0.382.8-0.787.5+0.692.9+0.497.6+0.396.0+0.170.2-0.786.2+2.880.8+0.3
ProGrad (ICCV ’23) 72.1 75.1 42.8 83.6 82.9 85.8 92.9 96.6 95.9 68.9 82.6 79.9
+ TransCLIP-ZS 73.5+1.476.8+1.742.8-0.083.7+0.283.1+0.287.2+1.393.7+0.897.4+0.896.0+0.171.4+2.586.1+3.481.1+1.1
Table 2: Cross-Dataset transferability evaluation. Few-shot learning methods are trained on 16-shot
ImageNet and evaluate on the ten other fine-grained datasets. Average excludes ImageNet.
Source Target
Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flower102 Caltech101DTDUCF101 AverageCross-DatasetCoOp (IJCV ’22) 71.9 62.0 15.7 44.6 62.1 84.3 88.3 67.1 92.7 39.5 64.1 62.0
+ TransCLIP-ZS 73.3+1.467.4+5.417.1+1.454.5+9.966.8+4.886.3+2.089.4+1.174.2+7.293.4+0.742.1+2.669.9+5.766.1+4.1
CoCoOp (CVPR ’22) 71.1 67.0 22.7 44.6 64.9 86.2 90.7 71.6 93.9 45.2 68.8 65.6
+ TransCLIP-ZS 76.8+5.769.6+2.722.6-0.159.2+14.667.0+2.185.4-0.889.8-0.979.0+7.494.3+0.350.6+5.474.5+5.769.2+3.6
MaPLE (CVPR ’23) 70.5 67.3 24.4 45.8 65.7 86.4 90.4 72.0 93.7 46.3 68.7 66.1
+ TransCLIP-ZS 76.6+6.169.8+2.524.5+0.259.5+13.766.8+1.285.4-1.089.7-0.778.0+6.094.3+0.649.4+3.174.4+5.669.2+3.1
ProGrad (ICCV ’23) 72.1 63.9 21.6 38.9 64.0 85.9 90.2 67.8 92.9 43.2 65.9 63.4
+ TransCLIP-ZS 73.5+1.468.6+4.722.7+1.155.2+16.467.9+3.887.0+1.291.3+1.173.9+6.194.0+1.146.6+3.473.5+7.668.1+4.6
PromptSRC (ICCV ’23) 71.4 67.3 24.1 45.0 65.6 86.5 90.1 70.5 93.8 46.2 68.9 65.8
+ TransCLIP-ZS 76.9+5.569.9+2.624.9+0.859.4+14.467.6+2.085.3-1.289.4-0.776.7+6.294.2+0.451.1+5.076.0+7.069.4+3.7
(iii) on top of 16-shot ImageNet pretraining for domain generalization on the four ImageNet variants.
Secondly, we compare our few-shot extension TransCLIP-FS (Eq. (4)) to transductive few-shot
learning methods. As for TransCLIP-ZS, we operate in a black-box setting (i.e., using only the output
embeddings, without training the model parameters).
Implementation details. The main component of our transductive formulation is the text-guided
KL divergence penalty. We fix λ= 1 for all our zero-shot experiments (see ablation study in Table 6),
andλ= 0.5 in all the few-shot experiments to reduce the impact of the text-driven regularization.
Another component of our optimization problem is the Laplacian regularization, which enforces
consistent predictions for close instances. We truncate the affinity matrix to the 3 nearest-neighbors,
making it sparse. µis initialized with the top-8 most confident samples of each class for the zero-shot
setting. For the few-shot setting, we use the class-wise average over the shot embeddings.
4.1 Main results
Transduction improvements. Table 1 and 2 demonstrate the advantages of our transductive
approach in zero-shot, few-shot, and cross-dataset transferability. TransCLIP enhances the zero-shot
top-1 accuracy by over 5% and popular few-shot methods by 4% (1-shot) on average, without the
need for additional labels. Table 3 further highlights that TransCLIP can be applied on top of prompt
tuning and adapter fine-tuning solutions, enhancing performance for both in-domain and domain
7Table 3: Domain Generalization evaluation with improved manual prompting strategy (custom
templates are given in Table 24b), 16-shot prompt-tuning and 16-shot adapter.
Source Target
Method ImageNet Adversarial ImageNetV2 Rendition Sketch Average Average OOD0-shotCLIP-ViT-B/16 w/a photo of a 66.6 47.9 60.6 73.8 46.0 59.0 57.1
+ TransCLIP-ZS 70.3+3.749.5+1.762.3+1.775.0+1.349.7+3.761.4+2.459.2+2.1
CLIP-ViT-B/16 w/ custom templates 68.8 50.6 62.3 77.8 48.4 61.6 59.8
+ TransCLIP-ZS 71.5+2.752.1+1.463.4+1.178.1+0.251.1+2.763.2+1.661.1+1.3Domain G.CLIP-ViT-B/16 w/ prompt tuning (CoOp) 71.9 49.4 64.1 75.1 47.2 61.5 59.0
+ TransCLIP-ZS 73.3+1.450.8+1.464.6+0.575.8+0.750.3+3.163.0+1.560.4+1.4
CLIP-ViT-B/16 w/ adapter (TaskRes) 73.0 50.3 65.6 77.8 49.2 63.2 60.7
+ TransCLIP-ZS 74.1+1.151.9+1.665.4-0.278.4+0.651.6+2.464.3+1.161.8+1.1
Table 4: Transductive few-shot learning evaluation. w/o text denotes λ= 0in Eq. (3).
Shots Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flowers102 Caltech101DTDUCF101 Average
0 CLIP-ViT-B/16 66.6 62.5 24.7 48.3 65.6 85.9 89.1 70.7 93.2 43.5 67.5 65.3
1TF [13] 29.7 38.1 19.2 46.0 32.5 43.5 38.2 67.8 75.5 31.6 48.8 42.8
BD-CSPN [37] 35.4 45.7 22.0 45.7 42.0 54.2 52.9 82.9 83.5 34.7 58.0 50.6
LaplacianShot [76] 34.9 44.5 22.1 52.1 41.1 53.0 52.2 83.1 83.4 35.8 57.3 50.9
PT-MAP [24] 40.1 52.6 23.8 59.7 48.4 64.4 61.8 69.4 54.1 41.8 63.5 52.7
TIM [5] 37.5 48.3 22.8 48.2 44.8 65.7 53.9 86.4 75.1 35.8 62.7 52.8
TransCLIP-FS w/o text 30.2 43.4 23.7 56.6 41.0 50.9 54.3 83.5 77.7 36.9 54.5 50.2
TransCLIP-FS 69.8 70.6 29.9 72.5 70.9 87.9 93.8 84.8 93.1 53.3 78.4 73.2
4TF [13] 51.1 61.0 30.3 64.9 56.8 71.0 65.9 90.9 91.5 53.7 67.9 64.1
BD-CSPN [37] 53.8 62.5 30.5 64.8 58.5 75.3 72.0 92.5 92.0 52.1 70.9 65.9
LaplacianShot [76] 53.5 62.5 29.6 74.3 58.5 75.7 73.4 92.8 92.0 52.7 71.7 67.0
PT-MAP [24] 57.6 68.1 31.2 74.9 63.1 81.1 79.5 76.2 60.2 58.4 73.9 65.8
TIM [5] 57.4 67.0 32.8 79.3 65.8 83.5 82.3 93.4 88.5 58.1 76.5 71.3
TransCLIP-FS w/o text 53.9 63.8 34.2 79.4 63.5 76.7 76.7 93.3 92.8 57.0 74.8 69.6
TransCLIP-FS 70.3 71.9 34.0 79.4 74.0 86.4 91.6 93.6 94.0 61.1 79.1 75.9
16TF [13] 61.8 70.1 38.3 74.3 71.2 80.7 79.5 95.4 93.6 62.9 76.0 73.1
BD-CSPN [37] 61.7 69.4 37.7 73.4 70.7 80.2 81.2 94.8 93.3 61.3 76.0 72.7
LaplacianShot [76] 60.9 68.3 36.1 78.1 69.2 81.2 81.7 94.8 93.1 58.6 76.3 72.6
PT-MAP [24] 64.0 72.0 37.4 75.6 72.0 82.7 86.1 78.5 63.7 63.7 76.3 70.2
TIM [5] 67.8 73.6 40.6 83.6 79.5 84.9 88.7 95.4 92.4 67.5 82.1 77.8
TransCLIP-FS w/o text 65.9 72.6 41.9 81.1 77.0 83.2 86.1 95.2 94.6 65.3 80.0 76.6
TransCLIP-FS 71.8 74.7 38.6 83.0 79.8 86.9 92.4 94.4 94.0 65.1 82.1 78.4
generalization tasks. However, we observe in Table 1 that transductive gains sometimes decrease
with the number of shots, presumably because data structure information can be partially captured in
the shots. These results underline the value of considering the structure of the unlabeled test samples
during prediction, especially on top of zero- and low-shot models or when facing domain shifts, an
aspect not leveraged by the current zero- and few-shot VLM literature. More detailed results for five
different backbone architectures and comparisons with unsupervised non-transductive methods are
provided in Appendix C.1 for the zero-shot setting, in Appendix C.2 for TransCLIP on top of popular
few-shot methods, in Appendix C.3 for cross-dataset transferability and in Appendix C.4 for domain
generalization. With its hyper-parameters unchanged , TransCLIP exhibits strong generalization from
convolutional networks to transformer-based models, as also depicted in Figure 1.
Transductive few-shot learning. We compare TransCLIP-FS, TransCLIP-FS without text
regularization (i.e., λ= 0) and state-of-the-art transductive few-shot methods. It is important to
note that these few-shot methods were primarily developed for vision-centric tasks. Hence, they
rely on visual information, omitting the textual elements. This allows us to study the impact of
our text-based regularization term. Table 4 shows that incorporating language in the transductive
paradigm boosts the performance over vision-only methods. Especially for the 1- to 4-shot settings,
our language-driven KL penalty enhances the performance by a large margin on many tasks (e.g.,
ImageNet, SUN397, StanfordCars, DTD). As the number of shots increases, the text-driven penalty
becomes less useful, especially for the datasets capitalizing on the visual shots rather than the
text-encoder knowledge (e.g., EuroSat and Flowers). This points to promising future directions
involving more flexible text regularization (e.g., an adaptable λtaking into account the number of
shots and the quality of the text embeddings). Detailed results for five different encoder architectures
are provided in Appendix C.5, consistently showing similar conclusions.
8Table 5: Performance and runtime comparison between TransCLIP and prompt learning solutions
on average over ImageNet and the 10 fine-grained classification datasets. UPL∗is a transductive
adaptation of the original unsupervised procedure in [25], more details in Appendices C.1 and C.5.
(a) Zero-shot setting.
Performance Runtime
UPL∗69.8 >150 min
TransCLIP-ZS 70.3 14.4 sec(b) Few-shot setting (4-shot).
Performance Runtime
CoOp+UPL∗74.4 >12h
TransCLIP-FS 75.9 35.3 sec
Table 6: Analysis on the components and sensitivity to hyper-parameters of TransCLIP-ZS.
(a) Components of the procedure.
Update µ Update Σ Lapl. w ImageNet SUN397 Aircraft EuroSAT
✗ ✓ ✓ 69.7 67.5 25.5 63.9
✓ ✗ ✓ 68.7 66.0 25.1 51.6
✓ ✓ ✗ 69.9 68.8 27.0 64.5
✗ ✗ ✓ 68.6 65.9 25.2 61.8
✓ ✓ ✓ 70.3 68.9 26.9 65.1(b) Text regularization hyper-parameter λ.
λ ImageNet SUN397 Aircraft EuroSAT
0.1 56.3 58.6 26.0 65.5
0.5 69.8 69.3 26.6 65.6
1 70.3 68.9 26.9 65.1
2 69.5 67.6 26.2 64.1
5 68.2 65.2 25.2 51.2
(c) Number of nearest-neighbors.
# neighbors ImageNet SUN397 Aircraft EuroSAT
3 70.3 68.9 26.9 65.1
5 70.3 68.9 26.8 65.1
10 70.2 68.8 26.9 65.2(d) Impact of an isotropic Σ.
ImageNet SUN397 Aircraft EuroSAT
Σ(ours) 70.3 68.9 26.9 65.1
Σisotropic 69.4 68.0 26.4 64.1
∆ -0.9 -0.9 -0.5 -1.0
Comparison with prompt learning. Following current VLMs literature, adapting the input prompt
instead of GMM parameters could be seen as a more straightforward solution. For a fair comparison,
we adapt Unsupervised Prompt Learning (UPL) [ 25] for the transductive setting and reevaluate its
main hyper-parameter (see Appendix C.1). Table 5 shows clearly that TransCLIP outperforms UPL
while being two to three orders of magnitude faster. Additional details on runtime are provided in
Table 8 of the Appendix.
4.2 Ablation studies
Components of TransCLIP. We study the impact of the principal components involved in the
TransCLIP procedure over four diverse datasets. Table 6a shows that updating µandΣallows to
significantly boost TransCLIP’s performance. This indicates the importance of having a dynamic
parametric model instead of a fixed one. Table 6b demonstrates the critical role of text-driven penalty
for TransCLIP in the zero-shot setting. Additional results on the sensitivity of λin the few-shot setting
are depicted in Figure 2 of the Appendix. Alongside the prior findings from Table 4, it is evident that
incorporating text information is key to the success of TransCLIP and its wide applicability across the
zero- and few-shot learning scenarios. The number of nearest-neighbors considered in the Laplacian
term (Eq. (2)) does not make a significant difference in TransCLIP’s performance as suggested by
Table 6c. However, removing the Laplacian regularization (Table 6a) leads to inferior results on
some datasets such as ImageNet and EuroSAT. We choose to consider 3 nearest-neighbors to make
the affinity matrix Wsparse and reduce memory consumption. We also investigate the diagonal
covariance matrix design by restricting it to be isotropic (i.e., Σ=σ2IdwithIdthe identity matrix).
Table 6d shows that a non-isotropic Σperforms better without significantly increasing the amount of
trainable parameters.
Scaling to larger VLMs. We report TransCLIP-ZS performance on EV A-CLIP 8 billion parameter
version [ 55] (approximately 42 times larger than the CLIP-ViT-B/16). It is worth mentioning that
TransCLIP is easily applicable to multi-billion parameter models since it does not necessitate gradient
computation or model parameter training (i.e., it only requires the memory needed for single-sample
inference because the whole dataset processing can be performed one sample at a time). Table 7 shows
that transduction can also bring significant improvements to larger models (details in Appendix C.1).
9Table 7: Performance of TransCLIP-ZS for increasingly large VLMs. Relative ∆is the improvement
normalized by the zero-shot error: (A CC TRANS CLIP- ACC ZERO-SHOT)/(100- ACC ZERO-SHOT).
ImageNet Average (11 datasets)
#Params Zero-shot w/ TransCLIP-ZS relative ∆ Zero-shot w/ TransCLIP-ZS relative ∆
CLIP-ViT-B/16 177M 66.6 70.3+3.7+11% 65.3 70.3+5.0+14%
CLIP-ViT-L/14 427M 72.9 77.2+4.3+16% 72.5 77.4+4.9+18%
EV A-CLIP-8B 7.5B 82.5 84.6+2.1+12% 81.5 85.8+4.3+23%
5 Conclusion
In this work, we studied the transductive paradigm in the context of Vision-Language Models and
proposed the TransCLIP method. Our algorithm is highly efficient, as it operates solely in the output
embedding space (i.e., black-box setting), making it suitable for a wide range of models, including
very large ones. This also enables TransCLIP to be compatible with models that are accessible
only through APIs. We first showed how TransCLIP can bring transduction to the inductive zero-
shot setting, achieving consistent gains without additional supervision. Then, we proposed a new
setting that applies transduction on top of popular few-shot methods, offering a convenient strategy
to combine computationally intensive supervised fine-tuning with efficient test-time transduction.
Finally, we highlighted the limitations of current transductive few-shot methods and proposed a
simple extension of TransCLIP to incorporate labeled samples. In all our experiments, TransCLIP’s
text-guided KL divergence term appears as a key factor in its success. Future work may focus
on further enhancing this regularization term, for example, by making it more resilient (e.g., with
adaptive class-wise weighting) when text prompts are less reliable.
6 Acknowledgments
M. Zanella and B. Gérin are funded by the Walloon region under grant No. 2010235 (ARIAC by
DIGITALWALLONIA4.AI). The present research benefited from computational resources made
available on Lucia, infrastructure funded by the Walloon Region under grant No. 1910247.
10References
[1]Jameel Abdul Samadh, Mohammad Hanan Gani, Noor Hussein, Muhammad Uzair Khattak,
Muhammad Muzammal Naseer, Fahad Shahbaz Khan, and Salman H Khan. Align your prompts:
Test-time prompting with distribution alignment for zero-shot generalization. Advances in
Neural Information Processing Systems , 36, 2024.
[2]Omer Belhasin, Guy Bar-Shalom, and Ran El-Yaniv. Transboost: Improving the best imagenet
performance using deep transduction. Advances in Neural Information Processing Systems , 35:
28363–28373, 2022.
[3]Christopher M Bishop. Pattern recognition and machine learning. Springer google schola , 2:
5–43, 2006.
[4]Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative
components with random forests. In Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13 , pages 446–461. Springer,
2014.
[5]Malik Boudiaf, Imtiaz Ziko, Jérôme Rony, José Dolz, Pablo Piantanida, and Ismail Ben Ayed.
Information maximization for few-shot learning. Advances in Neural Information Processing
Systems , 33:2445–2457, 2020.
[6]Adrian Bulat and Georgios Tzimiropoulos. Lasp: Text-to-text optimization for language-aware
soft prompting of vision & language models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages 23232–23241, June 2023.
[7]Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning . The
MIT Press, 1st edition, 2010. ISBN 0262514125.
[8]Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang.
Plot: Prompt learning with optimal transport for vision-language models. In The Eleventh
International Conference on Learning Representations , 2022.
[9]Eulrang Cho, Jooyeon Kim, and Hyunwoo J Kim. Distribution-aware prompt tuning for vision-
language models. In Proceedings of the IEEE/CVF International Conference on Computer
Vision , pages 22004–22013, 2023.
[10] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.
Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 3606–3613, 2014.
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
Recognition , pages 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.
[12] Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor Guilherme Turrisi
da Costa, Cees GM Snoek, Georgios Tzimiropoulos, and Brais Martinez. Variational prompt
tuning improves generalization of vision-language models. arXiv preprint arXiv:2210.02390 ,
2022.
[13] Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline
for few-shot image classification. In International Conference on Learning Representations ,
2019.
[14] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few
training examples: An incremental bayesian approach tested on 101 object categories. In 2004
conference on computer vision and pattern recognition workshop , pages 178–178. IEEE, 2004.
[15] Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse data augmen-
tation with diffusions for effective test-time prompt tuning. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 2704–2714, 2023.
11[16] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li,
and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International
Journal of Computer Vision , pages 1–15, 2023.
[17] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, and Bin
Cui. Calip: Zero-shot enhancement of clip with parameter-free attention. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 37, pages 746–754, 2023.
[18] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel
dataset and deep learning benchmark for land use and land cover classification. IEEE Journal
of Selected Topics in Applied Earth Observations and Remote Sensing , 12(7):2217–2226, 2019.
[19] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,
Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness:
A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 8340–8349, 2021.
[20] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural
adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 15262–15271, 2021.
[21] Mingyi Hong, Xiangfeng Wang, Meisam Razaviyayn, and Zhi-Quan Luo. Iteration complexity
analysis of block coordinate descent methods. Mathematical Programming , 163:85–114, 2017.
[22] Z hong, D Friedman, and D Chen. Factual probing is [mask]: Learning vs. learning to recall. In
Conference of the North American Chapter of the Association for Computational Linguistics
(NAACL) , 2021.
[23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning
for nlp. In International Conference on Machine Learning , pages 2790–2799. PMLR, 2019.
[24] Yuqing Hu, Vincent Gripon, and Stéphane Pateux. Leveraging the feature distribution in
transfer-based few-shot learning. In International Conference on Artificial Neural Networks ,
pages 487–499, 2021.
[25] Tony Huang, Jack Chu, and Fangyun Wei. Unsupervised prompt learning for vision-language
models. arXiv preprint arXiv:2204.03649 , 2022.
[26] Z Jiang, F Xu, J Araki, and G Neubig. How can we know what language models know. In
Association for Computational Linguistics (ACL) , 2020.
[27] Thorsten Joachims. Transductive inference for text classification using support vector machines.
InProceedings of the Sixteenth International Conference on Machine Learning , pages 200–209,
1999.
[28] Michael Kearns, Yishay Mansour, and Andrew Y Ng. An information-theoretic analysis of
hard and soft assignment methods for clustering. Learning in graphical models , pages 495–520,
1998.
[29] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fa-
had Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 19113–19122, 2023.
[30] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan
Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation
without forgetting. In Proceedings of the IEEE/CVF International Conference on Computer
Vision , pages 15190–15200, 2023.
[31] Philipp Krähenbühl and Vladlen Koltun. Parameter learning and convergent inference for dense
random fields. In International conference on machine learning , pages 513–521. PMLR, 2013.
[32] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-
grained categorization. In Proceedings of the IEEE international conference on computer vision
workshops , pages 554–561, 2013.
12[33] Kenneth Lange, David R Hunter, and Ilsoon Yang. Optimization transfer using surrogate
objective functions. Journal of computational and graphical statistics , 9(1):1–20, 2000.
[34] Michalis Lazarou, Tania Stathaki, and Yannis Avrithis. Iterative label cleaning for transduc-
tive and semi-supervised few-shot learning. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 8751–8760, 2021.
[35] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient
prompt tuning. arXiv preprint arXiv:2104.08691 , 2021.
[36] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source
hypothesis transfer for unsupervised domain adaptation. In International conference on machine
learning , pages 6028–6039. PMLR, 2020.
[37] Jinlu Liu, Liang Song, and Yongqiang Qin. Prototype rectification for few-shot learning. In
Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part I 16 , pages 741–756. Springer, 2020.
[38] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing. ACM Computing Surveys , 55(9):1–35, 2023.
[39] Y Liu, J Lee, M Park, S Kim, E Yang, SJ Hwang, and Y Yang. Learning to propagate labels:
Transductive propagation network for few-shot learning. In 7th International Conference on
Learning Representations, ICLR 2019 , 2019.
[40] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5206–5215, 2022.
[41] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. Swapprompt: Test-time prompt
adaptation for vision-language models. In Thirty-seventh Conference on Neural Information
Processing Systems , 2023.
[42] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-
grained visual classification of aircraft. arXiv preprint arXiv:1306.5151 , 2013.
[43] Shu Manli, Nie Weili, Huang De-An, Yu Zhiding, Goldstein Tom, Anandkumar Anima, and
Xiao Chaowei. Test-time prompt tuning for zero-shot generalization in vision-language models.
InNeurIPS , 2022.
[44] Ségolène Martin, Malik Boudiaf, Emilie Chouzenoux, Jean-Christophe Pesquet, and Ismail
Ayed. Towards practical few-shot query sets: Transductive minimum description length infer-
ence. Advances in Neural Information Processing Systems , 35:34677–34688, 2022.
[45] Ségolène Martin, Yunshi Huang, Fereshteh Shakeri, Jean-Christophe Pesquet, and Ismail Ben
Ayed. Transductive zero-shot and few-shot clip. In CVPR 2024-IEEE Conference on Computer
Vision and Pattern Recognition , 2024.
[46] Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and
Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate
shift. arXiv preprint arXiv:2006.10963 , 2020.
[47] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large
number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image
processing , pages 722–729. IEEE, 2008.
[48] Yassine Ouali, Adrian Bulat, Brais Matinez, and Georgios Tzimiropoulos. Black box few-
shot adaptation for vision-language models. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 15534–15546, 2023.
[49] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012
IEEE conference on computer vision and pattern recognition , pages 3498–3505. IEEE, 2012.
13[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision, 2021.
[51] Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo. A unified convergence analysis
of block successive minimization methods for nonsmooth optimization. SIAM Journal on
Optimization , 23(2):1126–1153, 2013. doi: 10.1137/120891009. URL https://doi.org/10.
1137/120891009 .
[52] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet
classifiers generalize to imagenet? In International conference on machine learning , pages
5389–5400. PMLR, 2019.
[53] T Shin, Logan R. L. IV Razeghi, Y , E Wallace, and S Singh. Autoprompt: Eliciting knowledge
from language models with automatically generated prompts. In Empirical Methods in Natural
Language Processing (EMNLP) , 2020.
[54] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human
actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.
[55] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong
Wang. Eva-clip-18b: Scaling clip to 18 billion parameters, 2024.
[56] Meng Tang, Dmitrii Marin, Ismail Ben Ayed, and Yuri Boykov. Kernel cuts: Kernel and spectral
clustering meet regularization. International Journal of Computer Vision , 127:477–511, 2019.
[57] Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip HS Torr, Adel Bibi,
Samuel Albanie, and Matthias Bethge. No "zero-shot" without exponential data: Pretraining con-
cept frequency determines multimodal model performance. arXiv preprint arXiv:2404.04125 ,
2024.
[58] V .N. Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Networks ,
10(5):988–999, 1999. doi: 10.1109/72.788640.
[59] Olivier Veilleux, Malik Boudiaf, Pablo Piantanida, and Ismail Ben Ayed. Realistic evaluation
of transductive few-shot learning. Advances in Neural Information Processing Systems , 34:
9290–9302, 2021.
[60] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent:
Fully test-time adaptation by entropy minimization. In International Conference on Learning
Representations , 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c .
[61] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global repre-
sentations by penalizing local predictive power. Advances in Neural Information Processing
Systems , 32, 2019.
[62] Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, and Tieniu Tan. A hard-to-beat
baseline for training-free clip-based adaptation. In The Twelfth International Conference on
Learning Representations , 2023.
[63] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca
Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong,
et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 7959–7971, 2022.
[64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:
Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference
on computer vision and pattern recognition , pages 3485–3492. IEEE, 2010.
[65] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-
guided context optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6757–6767, 2023.
14[66] Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, and Xinchao Wang. Task residual for tuning vision-
language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10899–10909, 2023.
[67] Maxime Zanella and Ismail Ben Ayed. Low-rank few-shot adaptation of vision-language models.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops , pages 1593–1603, 2024.
[68] Maxime Zanella and Ismail Ben Ayed. On the test-time zero-shot generalization of vision-
language models: Do we really need prompt learning? In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 23783–23793, 2024.
[69] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision
tasks: A survey. arXiv preprint arXiv:2304.00685 , 2023.
[70] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and
Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In
European Conference on Computer Vision , pages 493–510. Springer, 2022.
[71] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Schölkopf.
Learning with local and global consistency. Advances in neural information processing systems ,
16, 2003.
[72] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learn-
ing for vision-language models. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2022.
[73] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for
vision-language models. International Journal of Computer Vision (IJCV) , 2022.
[74] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient
for prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer
Vision , pages 15659–15669, 2023.
[75] Hao Zhu and Piotr Koniusz. Ease: Unsupervised discriminant subspace learning for transductive
few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 9078–9088, June 2022.
[76] Imtiaz Ziko, Jose Dolz, Eric Granger, and Ismail Ben Ayed. Laplacian regularized few-shot
learning. In International conference on machine learning , pages 11660–11670. PMLR, 2020.
15A More details on convergence
As mentioned in the main paper, our derived block-wise optimization procedure in Eqs. (5),(6)
and(7)can be viewed as an instance of the the general Block Majorize-Minimize paradigm for
non-convex optimization, also referred to as the Block Successive Minimization (BSUM) method [ 51].
We update each block of variables, with the other blocks fixed, by minimizing a tight upper bound
(majorizing function), thereby guaranteeing the overall objective does not increase at each step. In
the steps with respect to µandΣ, we optimize directly the objective in closed-form, which could be
also viewed as a particular case of optimizing a tight upper bound. The convergence of the general
BSUM procedure is well studied in the optimization community [ 51]. Indeed, under the following
assumptions for each block of variables, one can establish convergence results for the application of
BSUM to non-convex problems [51]:
•A1: The majorizing function is a tight upper bound, i.e., equal to the objective at the current
solution.
•A2: The first-order behavior of the majorizing function is the same as the original objective
locally.
Indeed, when assumptions A1 and A2 are verified for each block, we have the result in Theorem 1 [ 51].
As for our case of alternating Eqs. (5),(6)and(7), it is straightforward to verify that Assumptions A1
and A2 are satisfied for each block of variables. Furthermore, the majorizing functions are convex and
thus quasi-convex. Also, the sub-problem solved for each block has a unique solution. In particular,
for the z-updates, the majorizing function is the sum of a linear and a strongly convex function (the
negative entropy). Therefore, it is strongly convex. As for the µ- andΣ-updates, the solutions are
obtained in closed form (hence unique).
Algorithm 1 TransCLIP
Require: A set of image embeddings (fi)1≤i≤N, a set of textual class embeddings (tk)1≤k≤K,τ
the temperature of the CLIP model.
1:wi,j←f⊤
ifj∀i, j ▷ Affinity measure, truncated with top-3 values
2:ˆyi←φ(τf⊤
it))∀i ▷ Initial predictions, φthe softmax function
3:µk←mean{fis.ty=k, i∈ S}8∀k ▷ Class centroids initialization
4:diag(Σ)←11
d▷Covariance matrix initialization, dis the emb. dim.
5:zi←ˆyi∀i ▷ Initial assignments
6:while (1),(2)and (3)not converged do ▷Block-wise updates loop
7: while (1)not converged do ▷z-update loop
8: zi,k←ˆyλ
i,kexp(log( pi,k)+P
j∈Dwijzj,k)P
k′ˆyλ
i,k′exp(log( pi,k′)+P
j∈Dwijzj,k′)∀i∀k ▷ (1)z-step
9: end while
10: µk←γ
|S|P
i∈Szi,kfi+1
|Q|P
i∈Qzi,kfi
γ
|S|P
i∈Szi,k+1
|Q|P
i∈Qzi,k∀k ▷ (2)µ-step
11: diag(Σ)←γ
|S|P
i∈SP
kzi,k(fi−µk)2+1
|Q|P
i∈QP
kzi,k(fi−µk)2
γ+1▷(3)Σ-step
12:end while
13:return argmaxk(z) ▷Prediction with assignment variables
B Further details on TransCLIP implementation
This section aims to provide an additional pseudo-algorithm to supplement Section 3 as well
as more details on TransCLIP hyper-parameters presented in Section 4. Our code is available
athttps://github.com/MaxZanella/transduction-for-vlms and a pseudo-code in Algo-
rithm 1 summarizes the main steps of the TransCLIP algorithm.
8For the zero-shot setting, we use the embedding of top-8 most confident initial predictions for each class as
explained in Section 4.
16Hardware. All our experiments were conducted on a single A100-40 GB. In terms of memory,
TransCLIP consumes 16.9 GB when inferring on ImageNet, and can therefore process large datasets
on a smaller 24 GB GPU.
Hyper-parameters. In practice, TransCLIP performs 10 iterations of z,µ,Σblock-wise updates.
For each z-update, we perform 5 iterations, as we found it sufficient for convergence. In the zero-shot
setting, we set λ= 1andγ= 0(as there are no support samples). In the few-shot setting, we set
λ= 0.5and search for the value of γin{0.002,0.01,0.02,0.2}. The number of validation shots is
set at min(4,#shots ), and we build a 1-nearest neighbor classifier with the query samples and their
final class assignment to predict the class of each validation sample.
Prompt templates. We employ the prompt templates detailed in Table 24a for all our experiments
in zero-shot setting unless otherwise explicitly specified. Only when specified, we utilize the custom
template ensembling for ImageNet as in [70] (see Table 24b).
C Additional results.
We provide detailed results for all the studied vision backbones of CLIP over the 11 datasets to
support the transferability of TransCLIP across both convolutional networks and transformer-based
models. We additionally report other methods that do not fit into the transductive setting.
C.1 Zero-shot
In Table 9. We report performances of 5 CLIP encoders as well as the 8 billion parameter EV A-
CLIP [ 55]. We compare TransCLIP-ZS to unsupervised methods namely TPT [ 43], MTA [ 68],
SwapPrompt [ 41], and UPL [ 25]. Note that TPT and MTA are two test-time augmentation methods
working on a single image at a time, thus they differ from our transductive setting, still we report
their performance for informational purposes.
UPL∗.As mentioned in Section 4, we slightly modify UPL to apply it to the test set in a transductive
manner (transductive UPL is denoted UPL∗). Indeed, UPL relies on the generation of N= 16 hard
pseudo-labels per class from a training set, after what a cross-entropy loss function on soft tokens is
minimized. Instead, UPL∗generates the pseudo-labels directly from the test set. For fairness, we
reevaluated the number of pseudo-labels to select and still found that 16 per class yields the best
results on average, as seen in Table 23.
C.2 TransCLIP-ZS on top of few-shot methods
In Tables 10, 11, 12, 13 and 14. We report the performance of TransCLIP-ZS on top of CoOp [ 73],
Tip-Adapter-F [ 70], PLOT [ 8], TaskRes [ 66] and ProGrad [ 74] for five encoders. The results are
consistent with the main findings of Section 4 and indicate their generalization for several encoder
architectures.
C.3 Cross-Dataset transferability
In Table 15. We report the performance of TransCLIP-ZS on top of CoOp [ 73], CoCoOp [ 72],
ProGrad [ 74], PromptSRC [ 30] and MaPLE[ 29]. We additionally report PromptAlign [ 1], which
is working on a single image at a time and thus differs from our transductive setting. Note that
PromptSRC and MaPLE introduce learnable vision tokens, and are therefore not compatible with
convolutional-based encoders. The results are similar to those of Section 4.
C.4 Domain Generalization
In Tables 16 and 17. We extend the results from Table 3 to five encoders. These results support those
of Section 4 and show that TransCLIP can improve both zero- and few-shot model generalization for
various encoders.
17C.5 Transductive few-shot learning
In Tables 18, 19, 20, 21 and 22, we implemented transductive methods from the traditional few-
shot literature that align the most with our work in terms of computational efficiency and wide
applicability: TIM [ 5], LaplacianShot [ 76], BD-CSPN [ 37], TF [ 13], and PT-MAP [ 24]. Additionally,
due to the lack of transductive methods in Vision-Language and to ensure more comprehensive
comparisons, we introduce a hybrid method named CoOp+UPL. This method combines prompt
learning with both labeled shots and selected pseudo-labels following the methodology of UPL [ 25].
More details on each method and their validation procedure are outlined below. Methods with tunable
hyper-parameters are fine-tuned using the validation split provided with each dataset. In line with
other work [ 48], validation is performed for each dataset and for every shot number, setting the
number of validation shots at min(4,#shots ). Hyper-parameters are then optimized through a grid
search to maximize accuracy on the validation set. Note that we only search for γacross 4 values for
TransCLIP. More details on the grid search for each method is given below. Detailed results for the
five architectures studied in this paper are available in Table 18, 19, 20, 21, 22. Now we describe the
implementation details for each reported transductive few-shot methods.
Transductive Fine-Tuning. We follow the original implementation of Transductive Fine-
Tuning [ 13]. The authors kept the hyper-parameters fixed for all datasets since the goal was to
propose a simple baseline, with a temperature set to 1and the number of training steps to 25. How-
ever, they pointed out possible improvements if the hyper-parameters were tuned for each dataset.
Therefore, we search for the optimal temperature value by validation in {0.25,0.5,1,2,4}and the
number of iterations in {10,15,20,25,30,35,40}.
BD-CSPN. We follow the original implementation of BD-CSPN [ 37]. Regarding the hyper-
parameters, this method generates Zpseudo-labels per class from the query set to augment the
support set and to build the Kprototype vectors. They also introduce a temperature scaling parameter
εfor the computation of the prototype vectors. The authors set Zto8and the temperature scaling ε
to10. We search for the value of Zin{0,1,2,3,4,5,6,7,8,9,10}andεin{2.5,5,10,20,40}by
validation.
LaplacianShot. We follow the original implementation of LaplacianShot [ 76]. They balanced
the Laplacian regularization term with a factor λand used k-nearest neighbors consistency.
We follow the proposed ranges to find the hyper-parameter values by validation, with λin
{0.1,0.3,0.5,0.7,0.8,1,1.2,1.5}and the number of neighbors to consider kin{3,5,10}.
PT-MAP. We follow the original implementation of PT-MAP [ 24]. In their work, the authors show
a small performance sensitivity to the learning rate αused to update the class prototypes through
iterative adaptation. Following their discussion, we search αin{0.2,0.4}.
TIM. We follow the original implementation of TIM [ 5]. The authors proposed two solvers to
find the solution to the minimization problem: gradient-descent TIM (TIM-GD) and alternating-
direction method (TIM-ADM). We decide to focus on the second approach since there are fewer
hyper-parameters to tune. They set the weighting factors of the cross-entropy, the marginal entropy,
and the conditional entropy terms to 0.1,1and0.1, respectively. They also introduced a temperature
parameter τin their classifier and set it to 15. We search for the values of the cross-entropy and the
conditional entropy factors in {0.05,0.1,0.4,0.7,1}and the temperature in {5,10,15,30,60}by
validation.
CoOp+UPL. We implement a natural extension of CoOp to include the pseudo-labels proposed by
UPL. As in UPL, N= 16 hard pseudo-labels per class are generated according to the prediction’s
confidence. Pseudo-labels from the query set P ⊆ Q and labeled shots from Sare unified into a
single learning set S ∪ P . To separate the contribution of the pseudo-labels from the labeled shots,
18we split the cross-entropy loss function into two terms:
LS∪P(V|{xi}|S∪P|
i=1) =β1
|S|X
j∈SLCOOP(V|xj) (8)
+(1−β)1
|P|X
j∈PLUPL(V|xj), β∈[0,1]
Where Vdenotes the vector of learnable context token embeddings. Despite increased computational
needs, we search for the value of βin{0.1,0.3,0.5,0.7,0.9}by validation for the sake of fairness.
The number of epochs, the learning rate and its schedule, the optimizer and the context tokens
initialization follow exactly the CoOp implementation.
D Limitations
As discussed in Section 4, the gain of TransCLIP-ZS on top of few-shot methods tends to decrease
when the number of shots is high (e.g., 16 shots) and future works may investigate this aspect.
Secondly, as TransCLIP’s performance relies greatly on its text-regularization term, TransCLIP is
subject to some biases. One notable bias pertains to the quality of text embeddings within each class.
Recent literature has highlighted that these embeddings exhibit a preference for more frequently
occurring concepts [ 57]. However, this issue may be mitigated through our proposed few-shot
extension (e.g., introducing labels for more challenging classes).
/uni00000012/uni00000010/uni00000012 /uni00000012/uni00000010/uni00000013 /uni00000012/uni00000010/uni00000014 /uni00000012/uni00000010/uni00000015 /uni00000012/uni00000010/uni00000016 /uni00000012/uni00000010/uni00000017 /uni00000012/uni00000010/uni00000018 /uni00000012/uni00000010/uni00000019 /uni00000012/uni00000010/uni0000001a /uni00000012/uni00000010/uni0000001b /uni00000013/uni00000010/uni00000012 /uni00000013/uni00000010/uni00000013 /uni00000013/uni00000010/uni00000014 /uni00000013/uni00000010/uni00000015 /uni00000013/uni00000010/uni00000016 /uni00000013/uni00000010/uni00000017 /uni00000013/uni00000010/uni00000018 /uni00000013/uni00000010/uni00000019 /uni00000013/uni00000010/uni0000001a /uni00000013/uni00000010/uni0000001b /uni00000014/uni00000010/uni00000012
/uni0000002e/uni00000043/uni0000004f/uni00000044/uni00000046/uni00000043/uni00000018/uni00000012/uni00000018/uni00000017/uni00000019/uni00000012/uni00000019/uni00000017/uni0000001a/uni00000012/uni00000036/uni00000051/uni00000052/uni0000000f/uni00000013/uni00000002/uni00000043/uni00000045/uni00000045/uni00000057/uni00000054/uni00000043/uni00000045/uni0000005b
/uni0000003c/uni00000047/uni00000054/uni00000051/uni0000000f/uni00000055/uni0000004a/uni00000051/uni00000056
/uni00000013/uni0000000f/uni00000055/uni0000004a/uni00000051/uni00000056
/uni00000016/uni0000000f/uni00000055/uni0000004a/uni00000051/uni00000056
/uni00000013/uni00000018/uni0000000f/uni00000055/uni0000004a/uni00000051/uni00000056
/uni00000036/uni00000054/uni00000043/uni00000050/uni00000055/uni00000025/uni0000002e/uni0000002b/uni00000032
Figure 2: Sensitivity analysis of λ. Lower values reduce penalty towards zero-shot prediction and are
more appropriate for higher number of shots. Top-1 accuracy averaged over 11 datasets is reported.
Table 8: Runtime and performance comparison between TransCLIP-ZS and zero-shot prompt learning.
UPL∗is a transductive adaptation of the original unsupervised procedure in [ 25]. "Prediction" refers
to similarity measurement for CLIP and UPL∗, and to the iterative procedure for TransCLIP-ZS.
Dataset #samples Training Images + Texts encoding Prediction Total Top-1 accuracy
CLIP / 58.7 sec ∼0 sec 58.7 sec 66.6
ImageNet 50,000 UPL∗151 min 58.7 sec ∼0 sec 152 min 69.6
TransCLIP-ZS / 58.7 sec 14.4 sec 73.1 sec 70.3
CLIP / 49.2 sec ∼0 sec 49.2 sec 62.5
SUN397 19,850 UPL∗39 min 49.2 sec ∼0 sec 40 min 67.4
TransCLIP-ZS / 49.2 sec 2.6 sec 51.8 sec 68.9
CLIP / 20.5 sec ∼0 sec 20.5 sec 65.6
StanfordCars 8,041 UPL∗20 min 20.5 sec ∼0 sec 20 min 71.1
TransCLIP-ZS / 20.5 sec 0.7 sec 21.2 sec 69.4
CLIP / 4.8 sec ∼0 sec 4.8 sec 70.7
Flowers 2,463 UPL∗9 min 4.8 sec ∼0 sec 9 min 73.5
TransCLIP-ZS / 4.8 sec 0.2 sec 5.0 sec 76.7
19Table 9: Adaptation of CLIP on 11 classification datasets with zero-shot methods.
Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flower102 Caltech101DTDUCF101 Average
CLIP-ResNet-50 58.0 58.8 17.0 36.2 55.7 77.4 85.8 66.0 85.7 42.9 61.9 58.7
+ TransCLIP-ZS 60.8+2.864.2+5.416.6-0.459.6+23.457.9+2.278.0+0.689.3+3.672.2+6.288.6+3.047.8+5.068.8+6.964.0+5.3
TPT w/a photo of a 60.7 61.5 17.6 28.3 58.5 74.9 84.5 62.7 87.0 40.8 60.8 57.9
UPL∗61.6 63.3 16.7 52.1 63.1 78.0 89.1 69.3 85.7 47.0 65.8 62.9
SwapPrompt 61.8 63.9 18.0 46.6 59.6 75.1 89.1 70.22 89.9 47.3 65.7 62.5
CLIP-ResNet-101 60.6 59.0 17.9 32.7 63.2 80.7 87.0 64.4 89.9 37.2 61.1 59.4
+ TransCLIP-ZS 64.8+4.265.1+6.019.2+1.359.3+26.668.6+5.481.9+1.289.8+2.772.6+8.293.0+3.142.9+5.768.9+7.866.0+6.6
UPL∗63.7 63.5 18.1 61.3 69.5 80.9 90.0 67.3 88.3 42.8 67.3 64.8
CLIP-ViT-B/32 61.9 62.1 19.1 45.2 60.2 80.4 87.4 66.5 91.5 42.7 63.6 61.9
+ TransCLIP-ZS 64.9+3.067.6+5.520.3+1.359.0+13.863.3+3.281.5+1.189.0+1.774.4+7.991.8+0.350.4+7.768.7+5.166.5+4.6
UPL∗64.6 66.4 19.1 59.3 64.8 81.0 89.8 69.7 89.8 48.3 67.8 65.5
CLIP-ViT-B/16 66.6 62.5 24.7 48.3 65.6 85.9 89.1 70.7 93.2 43.5 67.5 65.3
+ TransCLIP-ZS 70.3+3.768.9+6.326.9+2.265.1+16.869.4+3.887.1+1.292.6+3.576.7+5.992.7-0.549.5+6.074.4+6.970.3+5.1
TPT w/a photo of a 69.0 65.5 24.8 42.4 66.9 84.7 87.8 69.0 94.2 47.8 68.0 65.5
MTA w/a photo of a 69.3 65.0 25.3 38.7 68.1 85.0 88.2 68.3 94.1 45.6 68.1 65.1
UPL∗69.6 67.4 24.7 69.5 71.1 85.8 92.4 73.5 91.9 47.7 73.7 69.8
CLIP-ViT-L/14 72.9 67.7 32.6 60.3 76.9 90.9 93.5 79.5 95.2 53.5 74.9 72.5
+ TransCLIP-ZS 77.2+4.373.5+5.935.3+2.775.9+15.679.0+2.191.9+1.094.7+1.285.3+5.897.4+2.360.0+6.581.7+6.777.4+4.9
UPL∗76.6 72.2 35.1 61.7 82.6 90.9 95.2 83.7 94.9 57.2 80.1 75.5
EV A-CLIP-8B 82.5 76.4 57.9 62.5 94.8 93.5 96.3 86.8 98.0 63.6 84.4 81.5
+ TransCLIP-ZS 84.6+2.180.1+3.759.4+1.581.9+19.495.0+0.293.9+0.496.3+0.091.8+5.098.3+0.368.6+5.093.6+9.285.8+4.3
Table 10: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods for
ResNet-50 vision encoder.
Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flower102 Caltech101DTDUCF101 Average0-shotCLIP-ResNet-50 58.0 58.8 17.0 36.2 55.7 77.4 85.8 66.0 85.7 42.9 61.9 58.7
+ TransCLIP-ZS 60.8+2.864.2+5.416.6-0.459.6+23.457.9+2.278.0+0.689.3+3.672.2+6.288.6+3.047.8+5.068.8+6.964.0+5.31-shotCoOp 57.4 60.0 8.5 49.4 55.8 74.2 85.9 69.0 87.3 45.1 62.9 59.6
+ TransCLIP-ZS 60.2+2.865.3+5.39.3+0.857.1+7.758.8+3.077.0+2.786.9+1.181.6+12.688.7+1.453.2+8.269.1+6.164.3+4.7
TIP-Adapter-F 61.1 62.1 18.6 50.2 59.2 77.1 86.3 78.1 88.3 47.6 64.7 63.0
+ TransCLIP-ZS 62.3+1.166.5+4.319.2+0.666.4+16.260.3+1.177.8+0.789.2+2.989.1+11.088.9+0.652.8+5.271.0+6.367.6+4.6
TaskRes 61.4 62.0 20.9 59.8 59.4 74.8 84.4 75.4 88.5 49.6 64.5 63.7
+ TransCLIP-ZS 62.4+1.066.4+4.420.4-0.569.4+9.660.2+0.977.1+2.387.1+2.784.4+9.088.3-0.356.8+7.269.3+4.867.4+3.7
ProGrad 57.8 60.9 18.9 55.0 58.6 76.3 88.0 72.2 88.1 46.4 64.1 62.4
+ TransCLIP-ZS 60.5+2.766.0+5.118.4-0.669.4+14.460.5+1.977.7+1.387.9-0.083.8+11.688.7+0.651.8+5.471.8+7.767.0+4.64-shotCoOp 59.8 63.5 20.5 71.3 62.9 73.8 87.0 85.7 89.3 54.0 67.6 66.8
+ TransCLIP-ZS 61.7+1.968.1+4.621.8+1.374.4+3.164.0+1.176.9+3.188.9+1.991.4+5.890.5+1.359.6+5.773.8+6.370.1+3.3
TIP-Adapter-F 62.6 65.6 25.4 70.5 63.4 77.9 86.7 87.5 91.1 55.4 70.9 68.8
+ TransCLIP-ZS 63.0+0.468.5+2.924.6-0.770.9+0.463.2-0.278.1+0.289.0+2.492.4+4.990.3-0.859.7+4.376.7+5.870.6+1.8
TaskRes 62.8 66.7 23.1 70.3 66.3 76.8 86.7 79.3 90.6 57.4 67.9 68.0
+ TransCLIP-ZS 63.3+0.569.2+2.521.7-1.572.2+1.964.5-1.877.9+1.288.9+2.285.1+5.890.4-0.160.9+3.574.4+6.569.9+1.9
ProGrad 62.5 69.3 23.1 74.1 65.1 77.7 89.6 91.7 90.8 59.8 76.2 70.9
+ TransCLIP-ZS 62.5+1.269.3+3.323.1-0.474.1+3.665.1-0.377.7+1.889.6+1.291.7+7.490.8+1.059.8+5.376.2+7.470.9+2.916-shotCoOp 63.0 69.4 31.4 82.2 73.6 74.5 86.6 94.6 91.8 63.3 74.4 73.2
+ TransCLIP-ZS 63.5+0.571.1+1.729.9-1.581.3-0.870.5-3.177.5+2.988.6+2.094.9+0.391.4-0.565.7+2.479.4+5.074.0+0.8
TIP-Adapter-F 65.2 71.2 33.9 83.3 74.3 78.9 89.0 92.7 92.5 66.0 76.5 74.9
+ TransCLIP-ZS 64.6-0.571.0-0.233.3-0.780.5-2.872.9-1.578.4-0.589.4+0.594.1+1.491.0-1.665.5-0.679.9+3.474.6-0.3
TaskRes 64.4 70.8 29.1 75.5 69.8 78.6 89.3 94.7 90.5 64.7 79.4 73.3
+ TransCLIP-ZS 64.4-0.270.8+0.629.1-3.975.5-4.269.8-5.078.6+0.189.3+0.894.7+0.390.5-2.564.7-2.679.4+3.673.3-1.2
ProGrad 63.4 69.9 31.8 81.9 73.9 77.0 88.2 94.2 92.3 63.6 75.4 73.8
+ TransCLIP-ZS 63.5+0.271.2+1.329.0-2.879.8-2.270.2-3.778.4+1.489.2+1.094.5+0.391.0-1.365.7+2.079.9+4.573.9+0.1
20Table 11: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods for
ResNet-101 vision encoder.
Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flower102 Caltech101DTDUCF101 Average0-shotCLIP-ResNet-101 60.6 59.0 17.9 32.7 63.2 80.7 87.0 64.4 89.9 37.2 61.1 59.4
+ TransCLIP-ZS 64.8+4.265.1+6.019.2+1.359.3+26.668.6+5.481.9+1.289.8+2.772.6+8.293.0+3.142.9+5.768.9+7.866.0+6.61-shotCoOp 60.8 61.3 14.8 51.0 64.5 76.9 86.5 69.5 89.8 44.3 65.7 62.3
+ TransCLIP-ZS 64.3+3.566.3+4.916.3+1.658.2+7.270.2+5.779.8+2.989.1+2.680.6+11.192.5+2.849.9+5.572.7+6.967.3+5.0
TIP-Adapter-F 63.6 61.4 19.2 46.3 64.8 80.2 87.2 77.5 91.7 46.3 65.9 64.0
+ TransCLIP-ZS 66.4+2.867.1+5.721.0+1.866.1+19.870.6+5.781.8+1.690.3+3.188.4+10.992.8+1.151.7+5.473.1+7.369.9+5.9
TaskRes 63.6 62.6 22.5 52.9 66.4 78.2 86.3 74.8 91.2 49.2 67.3 65.0
+ TransCLIP-ZS 66.6+3.067.9+5.323.3+0.864.0+11.170.3+3.980.7+2.489.7+3.385.4+10.692.1+0.853.5+4.374.2+6.969.8+4.84-shotCoOp 63.0 65.9 26.8 67.4 70.3 77.8 87.4 85.5 92.3 55.5 72.3 69.5
+ TransCLIP-ZS 66.0+3.069.7+3.827.7+1.071.4+4.173.8+3.580.5+2.790.1+2.791.4+5.893.8+1.559.7+4.177.3+4.972.9+3.4
TIP-Adapter-F 65.0 65.3 27.4 68.6 70.9 81.2 88.4 90.1 93.0 58.3 74.1 71.1
+ TransCLIP-ZS 67.4+2.469.8+4.428.6+1.170.8+2.274.0+3.182.2+1.090.6+2.293.5+3.393.6+0.662.0+3.779.2+5.173.8+2.7
TaskRes 65.3 68.0 24.3 61.9 72.4 80.4 88.0 78.6 92.9 56.5 71.0 69.0
+ TransCLIP-ZS 67.7+2.471.3+3.325.4+1.168.5+6.574.9+2.581.8+1.490.9+2.886.9+8.393.8+0.961.6+5.278.5+7.572.8+3.816-shotCoOp 66.5 71.0 34.8 83.4 79.1 78.9 89.0 95.1 93.5 65.1 78.1 75.9
+ TransCLIP-ZS 68.5+2.073.0+2.034.9+0.183.0-0.479.8+0.881.1+2.390.9+1.995.8+0.793.6+0.168.2+3.181.2+3.177.3+1.4
TIP-Adapter-F 68.3 72.8 36.2 82.0 80.5 81.9 89.9 94.4 93.9 67.6 79.4 77.0
+ TransCLIP-ZS 69.2+0.973.5+0.736.6+0.480.0-2.081.3+0.882.4+0.591.7+1.895.3+0.994.2+0.368.0+0.381.9+2.677.7+0.7
TaskRes 67.6 72.1 35.5 74.9 80.6 81.9 89.5 94.9 94.6 68.1 79.5 76.3
+ TransCLIP-ZS 69.3+1.773.3+1.234.8-0.873.9-1.080.8+0.282.6+0.790.9+1.495.5+0.694.5-0.168.3+0.282.8+3.377.0+0.7
Table 12: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods for
ViT-B/32 vision encoder.
Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flower102 Caltech101DTDUCF101 Average0-shotCLIP-ViT-B/32 61.9 62.1 19.1 45.2 60.2 80.4 87.4 66.5 91.5 42.7 63.6 61.9
+ TransCLIP-ZS 64.9+3.067.6+5.520.3+1.359.0+13.863.3+3.281.5+1.189.0+1.774.4+7.991.8+0.350.4+7.768.7+5.166.5+4.61-shotCoOp 60.8 63.3 15.6 51.9 59.5 75.7 87.7 71.5 91.8 47.1 66.0 62.8
+ TransCLIP-ZS 63.9+3.168.3+5.017.7+2.064.9+13.063.4+3.978.8+3.089.2+1.584.3+12.892.2+0.453.1+5.971.9+5.968.0+5.1
TIP-Adapter-F 64.3 65.4 22.2 59.7 61.1 80.4 87.5 81.1 92.4 50.9 66.5 66.5
+ TransCLIP-ZS 66.5+2.169.9+4.523.3+1.171.8+12.164.8+3.781.4+1.089.5+1.989.6+8.592.3-0.255.9+5.072.5+5.970.7+4.1
TaskRes 64.6 65.3 23.8 60.8 62.4 79.0 84.6 77.7 91.2 52.7 67.5 66.3
+ TransCLIP-ZS 66.7+2.169.8+4.523.9+0.173.4+12.664.2+1.880.7+1.788.2+3.586.6+8.991.8+0.557.0+4.372.8+5.370.5+4.1
ProGrad 62.0 64.8 21.1 53.5 60.5 78.2 87.9 74.4 91.5 51.1 66.6 64.7
+ TransCLIP-ZS 64.9+2.969.2+4.422.2+1.163.3+9.863.6+3.180.2+2.089.6+1.786.4+12.092.1+0.655.8+4.772.0+5.469.0+4.34-shotCoOp 63.2 67.1 24.1 67.8 66.4 75.5 88.8 87.6 92.9 55.1 74.9 69.4
+ TransCLIP-ZS 65.7+2.670.7+3.725.3+1.377.2+9.469.4+3.078.8+3.390.5+1.792.0+4.493.4+0.559.3+4.279.3+4.472.9+3.5
TIP-Adapter-F 65.8 68.3 28.8 71.5 67.6 80.9 88.6 88.9 94.6 58.0 75.1 71.6
+ TransCLIP-ZS 67.5+1.772.0+3.728.5-0.376.8+5.368.5+0.981.7+0.890.2+1.692.5+3.593.8-0.862.1+4.178.5+3.573.8+2.2
TaskRes 66.1 70.1 25.3 68.8 69.5 80.4 87.3 81.8 93.9 57.9 71.7 70.2
+ TransCLIP-ZS 67.8+1.772.7+2.625.6+0.477.1+8.370.3+0.881.6+1.190.0+2.788.3+6.594.2+0.361.9+4.076.1+4.473.2+3.0
ProGrad 65.2 69.6 24.8 63.0 66.5 79.2 89.4 87.7 93.4 56.1 73.7 69.9
+ TransCLIP-ZS 67.1+1.972.7+3.025.6+0.874.0+11.069.5+2.980.9+1.791.1+1.792.7+5.092.8-0.561.2+5.178.0+4.373.2+3.416-shotCoOp 66.8 72.3 32.8 82.4 76.1 78.6 88.8 95.5 94.9 64.9 78.5 75.6
+ TransCLIP-ZS 68.4+1.674.2+1.932.7-0.184.0+1.677.1+1.080.7+2.290.2+1.495.5-0.095.4+0.567.3+2.481.2+2.777.0+1.4
TIP-Adapter-F 68.4 74.1 34.8 83.4 77.0 81.7 90.4 94.3 95.1 68.0 80.5 77.1
+ TransCLIP-ZS 69.0+0.574.8+0.735.0+0.284.1+0.777.3+0.382.0+0.391.0+0.695.3+1.095.1-0.167.4-0.682.5+2.077.6+0.5
TaskRes 68.2 73.5 37.0 76.9 78.1 81.4 89.4 95.5 95.6 68.1 80.3 76.7
+ TransCLIP-ZS 69.2+1.174.6+1.035.3-1.780.3+3.477.2-0.882.0+0.690.7+1.395.1-0.494.8-0.967.8-0.482.3+2.077.2+0.5
ProGrad 66.9 73.2 33.2 80.6 76.2 80.2 89.4 95.1 95.0 65.3 80.0 75.9
+ TransCLIP-ZS 68.4+1.574.8+1.533.2-0.082.8+2.277.1+0.981.6+1.490.4+1.095.3+0.394.3-0.867.8+2.582.7+2.877.1+1.2
21Table 13: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods for
ViT-B/16 vision encoder.
Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flower102 Caltech101DTDUCF101 Average0-shotCLIP-ViT-B/16 66.6 62.5 24.7 48.3 65.6 85.9 89.1 70.7 93.2 43.5 67.5 65.3
+ TransCLIP-ZS 70.3+3.768.9+6.326.9+2.265.1+16.869.4+3.887.1+1.292.6+3.576.7+5.992.7-0.549.5+6.074.4+6.970.3+5.11-shotCoOp 65.7 66.9 20.7 56.4 67.6 84.3 90.2 78.2 92.5 50.1 71.2 67.6
+ TransCLIP-ZS 69.3+3.671.5+4.623.8+3.165.3+8.971.9+4.386.3+2.091.9+1.889.8+11.593.8+1.355.4+5.477.7+6.572.4+4.8
TIP-Adapter-F 69.5 67.2 28.8 67.8 67.1 85.8 90.6 83.7 94.0 51.6 73.4 70.9
+ TransCLIP-ZS 72.0+2.571.8+4.630.7+1.976.9+9.171.0+3.986.9+1.193.1+2.492.8+9.193.5-0.557.7+6.180.0+6.775.1+4.3
PLOT 66.9 67.0 28.9 72.8 68.5 84.9 91.9 81.8 94.0 52.8 74.7 71.3
+ TransCLIP-ZS 75.8+8.970.3+3.328.1-0.878.8+6.070.0+1.685.3+0.491.1-0.893.2+11.494.0-0.056.7+3.981.4+6.775.0+3.7
TaskRes 69.6 68.1 31.2 65.6 69.1 84.5 90.2 81.6 93.6 53.4 71.8 70.8
+ TransCLIP-ZS 72.0+2.572.5+4.431.4+0.273.7+8.171.6+2.486.5+2.091.6+1.590.7+9.194.0+0.459.4+6.076.4+4.674.5+3.7
ProGrad 67.0 67.0 28.7 57.0 68.2 84.9 91.4 80.8 93.5 52.8 73.3 69.5
+ TransCLIP-ZS 70.1+3.171.6+4.630.5+1.870.9+13.972.3+4.186.5+1.692.7+1.491.5+10.794.1+0.757.9+5.179.3+6.174.3+4.84-shotCoOp 68.8 69.7 30.8 69.6 74.4 84.5 92.5 92.2 94.5 59.4 77.5 74.0
+ TransCLIP-ZS 71.4+2.673.3+3.533.1+2.377.2+7.577.7+3.286.5+1.993.6+1.195.3+3.195.1+0.663.0+3.681.8+4.377.1+3.1
TIP-Adapter-F 70.7 70.8 35.7 76.8 74.1 86.5 91.9 92.1 94.8 59.8 78.1 75.6
+ TransCLIP-ZS 72.7+1.974.4+3.536.1+0.579.7+2.975.9+1.887.4+0.993.2+1.395.5+3.395.1+0.364.0+4.283.3+5.277.9+2.3
PLOT 70.0 71.8 34.8 84.7 76.6 83.5 92.8 93.2 94.9 61.0 79.7 76.6
+ TransCLIP-ZS 77.2+7.273.5+1.733.9-0.981.8-2.975.8-0.885.6+2.292.5-0.395.8+2.694.8-0.163.6+2.683.3+3.678.0+1.4
TaskRes 71.0 72.8 33.3 73.8 76.1 86.1 91.9 85.0 94.9 59.7 75.5 74.6
+ TransCLIP-ZS 73.0+2.075.3+2.534.4+1.178.1+4.477.2+1.187.3+1.293.0+1.192.4+7.495.1+0.264.3+4.679.2+3.777.2+2.7
ProGrad 70.2 71.7 34.0 69.5 75.0 85.4 92.0 91.1 94.4 59.8 77.9 74.6
+ TransCLIP-ZS 72.3+2.175.0+3.335.5+1.674.9+5.377.9+2.987.0+1.593.7+1.795.3+4.295.1+0.864.8+5.183.2+5.477.7+3.116-shotCoOp 71.9 74.9 43.3 85.0 82.8 84.2 91.9 96.8 95.8 69.7 83.1 79.9
+ TransCLIP-ZS 73.3+1.476.6+1.842.9-0.486.0+1.083.0+0.286.3+2.193.2+1.297.5+0.895.9+0.171.3+1.785.4+2.381.1+1.1
TIP-Adapter-F 73.3 76.0 44.6 85.9 82.3 86.8 92.6 96.2 95.7 70.8 83.9 80.7
+ TransCLIP-ZS 74.2+0.976.8+0.844.9+0.385.2-0.782.7+0.487.4+0.693.5+0.996.9+0.795.7-0.169.2-1.585.6+1.781.1+0.4
PLOT 72.5 76.0 46.8 92.1 84.6 85.6 92.5 97.1 96.0 71.1 84.8 81.7
+ TransCLIP-ZS 77.8+5.375.0-1.041.8-4.984.6-7.579.6-4.985.9+0.292.2-0.497.3+0.195.0-1.068.7-2.485.7+0.980.3-1.4
TaskRes 73.0 76.0 44.8 80.7 83.5 86.9 92.5 97.3 95.9 70.9 83.4 80.5
+ TransCLIP-ZS 74.1+1.076.9+0.843.6-1.280.5-0.382.8-0.787.5+0.692.9+0.497.6+0.396.0+0.170.2-0.786.2+2.880.8+0.3
ProGrad 72.1 75.1 42.8 83.6 82.9 85.8 92.9 96.6 95.9 68.9 82.6 79.9
+ TransCLIP-ZS 73.5+1.476.8+1.742.8-0.083.7+0.283.1+0.287.2+1.393.7+0.897.4+0.896.0+0.171.4+2.586.1+3.481.1+1.1
Table 14: TransCLIP atop inductive vision-language zero-shot and popular few-shot methods for
ViT-L/14 vision encoder.
Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flower102 Caltech101DTDUCF101 Average0-shotCLIP-ViT-L/14 72.9 67.7 32.6 60.3 76.9 90.9 93.5 79.5 95.2 53.5 74.9 72.5
+ TransCLIP-ZS 77.2+4.373.5+5.935.3+2.775.9+15.679.0+2.191.9+1.094.7+1.285.3+5.897.4+2.360.0+6.581.7+6.777.4+4.91-shotCoOp 71.5 68.9 36.9 68.4 78.8 89.0 94.0 87.2 95.0 58.6 78.7 75.2
+ TransCLIP-ZS 75.9+4.574.3+5.438.0+1.080.4+11.981.5+2.891.0+2.195.3+1.495.0+7.896.3+1.364.1+5.583.5+4.879.6+4.4
TIP-Adapter-F 76.4 71.0 38.5 67.8 79.2 91.0 93.2 90.9 95.3 59.3 77.9 76.4
+ TransCLIP-ZS 78.8+2.475.5+4.540.9+2.475.5+7.780.5+1.391.9+0.994.1+0.997.4+6.596.9+1.664.9+5.683.8+5.980.0+3.6
TaskRes 76.2 71.4 39.6 71.8 79.9 89.8 93.5 87.4 95.0 60.1 77.7 76.6
+ TransCLIP-ZS 78.8+2.575.9+4.541.2+1.682.0+10.281.1+1.291.5+1.694.9+1.494.7+7.296.2+1.265.7+5.683.8+6.180.5+3.9
ProGrad 73.6 71.1 38.4 71.4 80.0 90.5 94.4 89.0 95.7 58.8 80.2 76.6
+ TransCLIP-ZS 76.9+3.375.8+4.741.1+2.878.7+7.381.1+1.191.7+1.295.6+1.297.5+8.596.4+0.765.9+7.084.0+3.880.4+3.84-shotCoOp 74.9 73.1 43.6 76.2 83.3 88.8 94.6 95.9 96.7 64.1 83.0 79.5
+ TransCLIP-ZS 77.9+3.076.9+3.844.0+0.581.6+5.584.0+0.791.2+2.495.8+1.297.3+1.497.4+0.767.7+3.685.9+3.081.8+2.3
TIP-Adapter-F 77.0 74.1 47.4 81.4 82.3 91.2 94.0 95.5 96.5 64.4 83.9 80.7
+ TransCLIP-ZS 79.0+2.077.2+3.147.6+0.283.0+1.682.9+0.691.9+0.794.8+0.898.5+3.097.5+1.169.0+4.687.7+3.782.6+1.9
TaskRes 77.1 74.9 42.5 77.3 83.6 90.6 94.4 90.1 96.6 65.1 80.0 79.3
+ TransCLIP-ZS 79.4+2.278.5+3.644.9+2.481.4+4.183.2-0.391.8+1.195.7+1.396.5+6.497.7+1.168.0+2.986.1+6.182.1+2.8
ProGrad 76.5 74.9 44.5 79.3 83.9 90.6 94.8 95.6 96.7 66.1 83.9 80.6
+ TransCLIP-ZS 78.8+2.378.2+3.246.8+2.382.6+3.384.0+0.191.8+1.295.8+1.197.9+2.397.4+0.670.3+4.287.7+3.882.8+2.216-shotCoOp 78.2 77.5 55.4 87.2 89.1 89.8 94.6 99.1 97.2 74.1 87.2 84.5
+ TransCLIP-ZS 79.5+1.379.8+2.354.6-0.790.5+3.488.0-1.191.5+1.795.4+0.899.4+0.498.1+0.975.3+1.289.0+1.885.6+1.1
TIP-Adapter-F 79.3 79.6 55.8 86.1 88.1 91.6 94.6 98.3 97.5 74.0 87.4 84.7
+ TransCLIP-ZS 80.1+0.980.0+0.456.0+0.288.8+2.787.4-0.791.9+0.495.7+1.199.1+0.997.9+0.473.9-0.188.8+1.485.4+0.7
TaskRes 78.1 76.7 55.0 83.7 87.6 91.5 94.6 97.7 97.2 74.2 86.2 83.9
+ TransCLIP-ZS 79.8+1.779.4+2.752.9-2.285.3+1.685.4-2.292.0+0.595.3+0.799.4+1.797.8+0.672.6-1.588.9+2.684.4+0.6
ProGrad 78.4 78.3 55.6 88.5 88.7 90.8 94.8 98.8 97.3 73.7 87.9 84.8
+ TransCLIP-ZS 79.6+1.280.1+1.854.2-1.490.7+2.287.3-1.491.9+1.195.8+1.099.4+0.697.8+0.575.1+1.390.0+2.185.6+0.8
22Table 15: Cross-Dataset transferability evaluation for five encoders. Few-shot learning methods are
trained on 16-shot ImageNet and evaluate on the ten other fine-grained datasets. Average excludes
ImageNet.
Source Target
Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flower102 Caltech101DTDUCF101 AverageResNet-50CoOp 63.0 56.5 13.8 22.7 53.1 73.6 84.2 56.7 85.7 34.5 56.9 53.8
+ TransCLIP-ZS 63.5+0.562.4+5.914.2+0.438.6+16.056.1+3.076.2+2.684.7+0.566.2+9.587.4+1.738.3+3.762.5+5.658.7+4.9
CoCoOp 63.2 61.5 16.5 27.1 55.9 78.1 88.2 65.5 88.6 39.6 61.1 58.2
+ TransCLIP-ZS 66.5+3.263.2+1.716.5-0.136.0+8.957.2+1.374.7-3.586.1-2.170.8+5.388.5-0.143.3+3.765.0+3.960.1+1.9
ProGrad 63.4 58.4 13.5 24.2 52.6 75.9 85.9 61.8 85.9 36.1 57.6 55.2
+ TransCLIP-ZS 63.5+0.263.3+4.914.1+0.637.2+13.055.7+3.077.2+1.387.5+1.670.0+8.288.5+2.642.1+6.062.5+4.959.8+4.6ResNet-101CoOp 66.5 58.4 14.2 25.3 59.5 79.1 86.0 60.4 88.3 34.2 56.4 56.2
+ TransCLIP-ZS 68.5+2.063.7+5.315.2+1.030.9+5.665.2+5.781.0+1.986.9+0.969.7+9.490.3+1.937.9+3.763.7+7.360.4+4.3
CoCoOp 65.2 62.9 17.8 25.8 62.8 81.4 87.2 64.0 91.3 39.8 61.1 59.4
+ TransCLIP-ZS 73.4+8.165.6+2.717.8-0.145.2+19.367.3+4.479.9-1.587.1-0.171.6+7.690.9-0.440.0+0.367.4+6.363.3+3.9ViT-B/32CoOp 66.8 60.6 14.2 31.8 56.9 78.8 85.6 58.9 90.3 35.9 61.8 57.5
+ TransCLIP-ZS 68.4+1.665.7+5.014.9+0.749.5+17.760.4+3.580.4+1.586.5+0.968.0+9.092.9+2.640.4+4.567.6+5.862.6+5.1
CoCoOp 66.0 64.6 17.8 40.5 59.6 80.8 88.2 65.4 92.1 42.7 64.9 61.7
+ TransCLIP-ZS 71.9+5.967.4+2.817.8+0.054.4+13.961.0+1.579.0-1.985.7-2.573.9+8.592.4+0.347.8+5.171.0+6.065.0+3.4
ProGrad 66.9 61.9 13.5 33.4 56.3 79.6 86.3 60.8 91.4 38.0 62.5 58.4
+ TransCLIP-ZS 68.4+1.566.5+4.514.2+0.751.7+18.459.8+3.580.8+1.386.9+0.670.9+10.192.5+1.042.5+4.567.8+5.363.4+5.0
MaPLE 65.7 65.0 18.1 41.0 60.6 80.8 88.4 65.5 91.6 42.3 63.6 61.7
+ TransCLIP-ZS 71.4+5.867.7+2.718.5+0.454.6+13.660.4-0.278.7-2.285.4-3.072.5+7.092.2+0.646.8+4.568.9+5.364.6+2.9
MaPLE w/PromptAlign / 66.1 18.8 39.7 63.5 82.1 88.4 66.1 92.1 42.5 65.6 62.5ViT-B/16CoOp 71.9 62.0 15.7 44.6 62.1 84.3 88.3 67.1 92.7 39.5 64.1 62.0
+ TransCLIP-ZS 73.3+1.467.4+5.417.1+1.454.5+9.966.8+4.886.3+2.089.4+1.174.2+7.293.4+0.742.1+2.669.9+5.766.1+4.1
CoCoOp 71.1 67.0 22.7 44.6 64.9 86.2 90.7 71.6 93.9 45.2 68.8 65.6
+ TransCLIP-ZS 76.8+5.769.6+2.722.6-0.159.2+14.667.0+2.185.4-0.889.8-0.979.0+7.494.3+0.350.6+5.474.5+5.769.2+3.6
ProGrad 72.1 63.9 21.6 38.9 64.0 85.9 90.2 67.8 92.9 43.2 65.9 63.4
+ TransCLIP-ZS 73.5+1.468.6+4.722.7+1.155.2+16.467.9+3.887.0+1.291.3+1.173.9+6.194.0+1.146.6+3.473.5+7.668.1+4.6
PromptSRC 71.4 67.3 24.1 45.0 65.6 86.5 90.1 70.5 93.8 46.2 68.9 65.8
+ TransCLIP-ZS 76.9+5.569.9+2.624.9+0.859.4+14.467.6+2.085.3-1.289.4-0.776.7+6.294.2+0.451.1+5.076.0+7.069.4+3.7
MaPLE 70.5 67.3 24.4 45.8 65.7 86.4 90.4 72.0 93.7 46.3 68.7 66.1
+ TransCLIP-ZS 76.6+6.169.8+2.524.5+0.259.5+13.766.8+1.285.4-1.089.7-0.778.0+6.094.3+0.649.4+3.174.4+5.669.2+3.1
Maple w/PromptAlign / 67.5 24.8 47.9 68.5 86.7 90.8 72.4 94.0 47.2 69.5 66.9ViT-L/14CoOp 78.2 64.9 21.6 51.4 75.5 89.3 91.0 68.9 93.6 43.6 68.8 66.9
+ TransCLIP-ZS 79.5+1.470.6+5.724.3+2.872.7+21.379.0+3.491.1+1.893.6+2.678.1+9.296.2+2.548.2+4.675.3+6.572.9+6.0
CoCoOp 77.8 70.8 31.0 47.4 77.9 91.4 94.1 76.2 97.1 50.7 74.1 71.1
+ TransCLIP-ZS 81.9+4.173.8+3.033.2+2.176.3+28.978.7+0.890.6-0.894.4+0.381.4+5.197.1+0.155.5+4.779.2+5.176.0+4.9
ProGrad 78.4 66.9 24.8 45.4 75.9 90.4 93.1 73.4 95.3 45.8 71.8 68.3
+ TransCLIP-ZS 79.6+1.272.4+5.526.8+2.067.2+21.778.7+2.891.6+1.295.6+2.579.4+6.096.6+1.351.9+6.078.4+6.673.8+5.6
MaPLE 77.2 71.6 30.2 55.7 77.3 91.3 93.1 76.7 96.2 53.8 74.9 72.1
+ TransCLIP-ZS 81.6+4.474.1+2.532.8+2.675.2+19.678.3+1.090.5-0.894.2+1.183.0+6.397.4+1.256.2+2.481.0+6.176.3+4.2
Table 16: Domain Generalization evaluation for five encoders with manual prompting strategies.
Source Target
Method ImageNet Adversarial ImageNetV2 Rendition Sketch Average Average OODResNet-50w/a photo of a 58.0 22.0 51.2 56.1 33.3 44.1 40.7
+ TransCLIP-ZS 60.8+2.821.5-0.451.4+0.152.8-3.335.1+1.844.3+0.240.2-0.5
w/ custom templates 60.3 23.8 53.4 60.5 35.5 46.7 43.3
+ TransCLIP-ZS 61.7+1.423.4-0.552.6-0.856.4-4.236.6+1.146.1-0.642.2-1.1ResNet-101w/a photo of a 60.6 28.2 54.3 64.2 38.0 49.1 46.2
+ TransCLIP-ZS 64.8+4.229.2+1.056.2+1.965.1+1.042.2+4.351.5+2.548.2+2.0
w/ custom templates 62.5 29.8 56.1 67.7 40.6 51.4 48.6
+ TransCLIP-ZS 65.6+3.030.6+0.857.0+0.968.2+0.544.0+3.453.1+1.749.9+1.4ViT-B/32w/a photo of a 61.9 29.9 54.7 66.8 40.8 50.8 48.1
+ TransCLIP-ZS 64.9+3.030.5+0.655.7+1.167.0+0.243.6+2.852.4+1.549.2+1.2
w/ custom templates 63.8 32.1 56.3 69.5 42.1 52.8 50.0
+ TransCLIP-ZS 66.2+2.532.4+0.356.6+0.269.2-0.344.3+2.153.7+1.050.6+0.6ViT-B/16w/a photo of a 66.6 47.9 60.6 73.8 46.0 59.0 57.1
+ TransCLIP-ZS 70.3+3.749.5+1.762.3+1.775.0+1.349.7+3.761.4+2.459.1+2.1
w/ custom templates 68.8 50.6 62.3 77.8 48.4 61.6 59.8
+ TransCLIP-ZS 71.5+2.752.1+1.463.4+1.178.1+0.251.1+2.763.2+1.661.2+1.4ViT-L/14w/a photo of a 72.9 68.4 67.2 85.3 57.4 70.2 69.6
+ TransCLIP-ZS 77.2+4.371.4+3.069.1+1.887.1+1.860.0+2.672.9+2.771.9+2.3
w/ custom templates 75.9 70.9 70.2 87.8 59.7 72.9 72.2
+ TransCLIP-ZS 78.6+2.773.6+2.770.8+0.589.0+1.161.9+2.274.8+1.873.8+1.6
23Table 17: Domain Generalization evaluation for five encoders. Few-shot learning methods are trained
on 16-shot ImageNet and evaluated on the 4 other variants.
Source Target
Method ImageNet Adversarial ImageNetV2 Rendition Sketch Average Average OODResNet-50CoOp 63.0 22.0 55.0 55.0 32.8 45.5 41.2
+ TransCLIP-ZS 63.5+0.521.0-1.053.6-1.452.3-2.734.8+2.045.0-0.540.4-0.8
TaskRes 64.6 22.9 56.4 60.8 35.9 48.1 44.0
+ TransCLIP-ZS 64.4-0.221.7-1.254.8-1.656.2-4.636.9+1.046.8-1.342.4-1.6ResNet-101CoOp 66.5 29.5 58.3 63.6 39.0 51.4 47.6
+ TransCLIP-ZS 68.5+2.029.9+0.558.6+0.264.8+1.242.3+3.352.8+1.448.9+1.3
TaskRes 67.6 30.0 59.6 68.4 41.8 53.5 49.9
+ TransCLIP-ZS 69.3+1.730.2+0.259.3-0.468.8+0.444.6+2.954.4+1.050.7+0.8ViT-B/32CoOp 66.8 31.2 58.5 65.2 40.1 52.3 48.7
+ TransCLIP-ZS 68.4+1.631.3+0.158.3-0.265.5+0.342.7+2.653.2+0.949.4+0.7
TaskRes 68.2 31.3 59.3 69.5 42.5 54.2 50.6
+ TransCLIP-ZS 69.2+1.131.3+0.159.1-0.269.3-0.344.9+2.454.8+0.651.2+0.5ViT-B/16CoOp 71.9 49.4 64.1 75.1 47.1 61.5 58.9
+ TransCLIP-ZS 73.3+1.450.8+1.364.6+0.475.7+0.750.3+3.262.9+1.460.4+1.4
TaskRes 73.0 50.3 65.6 77.8 49.2 63.2 60.7
+ TransCLIP-ZS 74.1+1.051.9+1.665.4-0.278.4+0.651.6+2.464.3+1.161.8+1.1ViT-L/14CoOp 78.2 69.4 70.8 85.4 57.5 72.3 70.8
+ TransCLIP-ZS 79.5+1.371.9+2.671.1+0.386.9+1.560.0+2.573.9+1.672.5+1.7
TaskRes 78.1 71.3 71.6 87.9 60.1 73.8 72.7
+ TransCLIP-ZS 79.8+1.774.2+3.071.8+0.288.9+1.162.0+1.975.4+1.674.2+1.5
Table 18: Detailed results of transductive methods in the few-shot setting for the 11 datasets with
ResNet-50 as visual backbone.
Shots Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flowers102 Caltech101DTDUCF101 Average
1TF 20.6 31.2 13.1 39.0 21.8 28.3 27.2 53.6 66.1 27.7 38.1 33.3
BD-CSPN 24.7 36.9 13.9 40.3 27.2 34.1 34.1 66.7 74.3 32.8 43.4 38.9
LaplacianShot 23.8 35.5 14.0 42.3 27.0 34.7 37.3 66.6 72.4 32.8 43.2 39.1
PT-MAP 29.4 42.9 15.7 48.0 33.8 44.8 56.5 61.4 46.9 38.6 52.2 42.7
TIM 26.1 40.0 13.4 42.5 27.3 41.4 35.0 69.1 62.3 31.7 46.9 39.6
CoOp + UPL 59.6 63.4 17.5 54.7 56.4 75.3 82.8 73.5 87.4 48.3 66.1 62.3
TransCLIP-FS 55.7 63.5 20.6 70.3 56.2 77.2 86.9 83.7 87.4 51.3 70.7 65.8
2TF 29.6 43.1 16.6 57.2 32.3 41.4 40.1 68.4 77.5 41.4 51.3 45.4
BD-CSPN 33.2 48.1 17.8 58.6 36.2 47.4 50.0 77.0 80.7 43.2 54.1 49.7
LaplacianShot 33.1 47.8 17.7 60.0 36.1 48.7 50.4 77.5 81.0 43.3 55.2 50.1
PT-MAP 39.3 54.6 19.3 61.4 43.5 60.1 67.0 68.9 51.5 50.4 61.9 52.5
TIM 35.5 52.2 18.2 60.2 38.1 57.2 51.7 79.7 76.1 44.2 59.6 52.1
CoOp + UPL 59.8 64.0 19.3 62.9 59.2 74.8 81.2 80.5 88.1 49.5 68.0 64.3
TransCLIP-FS 59.3 66.2 20.3 71.5 58.7 77.2 86.0 87.1 87.8 55.2 72.8 67.5
4TF 38.5 53.1 20.4 64.9 42.8 52.5 49.3 80.7 83.6 48.4 59.3 54.0
BD-CSPN 40.7 54.9 20.2 65.4 43.4 56.6 54.3 83.7 84.0 48.1 59.8 55.6
LaplacianShot 40.5 54.9 19.7 68.0 43.3 58.0 55.5 84.2 83.9 47.9 60.1 56.0
PT-MAP 46.8 61.4 22.8 69.5 50.7 66.6 70.0 71.0 54.6 56.3 68.0 58.0
TIM 43.3 59.1 22.9 71.0 49.6 64.0 58.8 87.6 79.1 53.2 65.8 59.5
CoOp + UPL 60.3 65.7 23.3 71.0 63.0 75.8 83.6 87.3 88.0 55.2 69.1 67.5
TransCLIP-FS 59.3 66.5 25.0 73.8 61.4 76.6 81.6 88.4 88.2 57.6 73.3 68.4
8TF 45.1 59.7 24.1 66.8 51.2 61.1 61.7 86.4 86.3 55.9 65.1 60.3
BD-CSPN 45.6 59.6 22.9 66.2 50.4 62.4 65.7 87.5 85.5 54.6 65.1 60.5
LaplacianShot 45.2 59.1 22.4 69.1 49.6 63.4 65.7 87.6 85.8 53.9 65.9 60.7
PT-MAP 50.6 64.2 23.4 66.7 55.9 69.6 76.9 72.9 54.8 60.4 70.6 60.5
TIM 49.9 63.4 25.0 69.5 59.7 70.0 71.8 89.9 82.9 59.1 70.8 64.7
CoOp + UPL 60.9 67.0 26.0 71.7 66.5 75.5 82.7 91.2 88.3 59.0 71.4 69.1
TransCLIP-FS 59.9 68.3 28.0 74.5 67.6 76.9 86.6 90.4 88.7 62.1 76.1 70.8
16TF 50.0 63.2 26.6 71.8 57.7 66.1 66.4 90.3 87.3 58.8 67.7 64.2
BD-CSPN 49.7 62.4 25.5 71.3 56.6 66.0 66.2 89.6 86.7 57.8 67.2 63.5
LaplacianShot 48.9 61.5 24.6 71.5 54.8 66.7 67.5 89.5 86.4 56.2 67.5 63.2
PT-MAP 54.1 66.1 25.6 68.1 61.1 70.6 79.0 75.2 57.0 62.4 71.0 62.7
TIM 55.5 66.8 30.8 81.6 68.0 72.4 75.0 88.9 85.7 63.1 74.4 69.3
CoOp + UPL 60.9 69.4 31.6 78.0 71.4 76.2 83.5 93.6 89.1 62.8 73.5 71.8
TransCLIP-FS 62.6 70.4 30.3 77.6 71.5 77.1 87.3 92.5 88.7 64.4 77.7 72.7
24Table 19: Detailed results of transductive methods in the few-shot setting for the 11 datasets with
ResNet-101 as visual backbone.
Shots Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flowers102 Caltech101DTDUCF101 Average
1TF 24.9 33.3 16.0 38.5 29.4 34.3 37.1 57.0 71.6 29.7 43.5 37.8
BD-CSPN 29.9 40.2 16.8 39.5 35.1 42.6 51.0 70.0 79.6 32.1 51.8 44.4
LaplacianShot 30.0 40.0 17.1 40.6 37.2 43.8 51.8 71.7 79.4 34.9 52.1 45.3
PT-MAP 34.3 46.2 18.1 49.3 44.0 53.0 69.5 65.0 51.6 39.1 58.9 48.1
TIM 31.5 44.2 16.6 42.9 39.0 54.9 51.8 77.6 66.5 36.1 56.2 47.0
CoOp + UPL 62.7 64.5 20.8 63.6 61.7 77.8 83.8 72.8 89.6 47.0 69.1 64.9
TransCLIP-FS 64.3 66.6 19.6 67.2 70.0 82.9 91.5 80.4 91.2 47.0 70.1 68.3
2TF 34.8 46.6 19.6 53.7 41.2 49.1 51.1 73.8 83.1 42.3 56.3 50.1
BD-CSPN 39.9 51.7 20.3 54.2 46.7 57.7 60.4 80.8 85.5 45.5 59.4 54.7
LaplacianShot 39.9 51.8 20.9 59.3 46.9 59.0 63.2 81.9 85.9 45.5 59.8 55.8
PT-MAP 44.3 57.4 21.8 62.0 52.9 65.7 76.6 71.0 56.2 52.5 65.8 56.9
TIM 42.4 55.6 19.9 63.5 50.2 69.2 67.3 85.5 81.5 49.0 62.6 58.8
CoOp + UPL 63.0 65.4 23.6 66.4 66.6 77.8 85.2 81.2 89.4 51.4 70.9 67.4
TransCLIP-FS 64.6 67.2 22.7 68.3 70.7 80.8 89.1 85.2 91.5 49.8 72.8 69.3
4TF 44.9 56.9 23.7 62.8 53.4 61.6 61.1 83.7 87.5 51.5 65.4 59.3
BD-CSPN 47.8 58.8 23.7 62.1 54.4 66.0 70.1 86.1 87.7 51.2 65.4 61.2
LaplacianShot 47.7 58.9 23.4 71.9 54.3 67.3 70.9 86.8 87.7 51.1 65.8 62.3
PT-MAP 51.7 63.8 25.5 68.0 60.3 71.6 79.9 74.6 56.4 57.4 71.0 61.8
TIM 51.2 63.2 25.1 73.6 61.4 75.8 76.8 87.0 87.8 55.3 71.7 66.3
CoOp + UPL 63.9 67.4 25.4 70.8 69.3 79.5 85.5 87.4 90.3 55.6 73.2 69.2
TransCLIP-FS 65.1 68.7 26.2 73.7 71.6 81.3 90.1 88.6 91.7 56.4 73.2 71.5
8TF 51.5 62.9 27.1 63.3 61.5 69.0 72.3 89.1 89.7 58.2 70.2 65.0
BD-CSPN 52.7 63.1 27.3 62.7 61.0 70.9 76.8 89.5 89.4 57.0 70.3 65.5
LaplacianShot 52.3 62.8 26.8 68.4 60.7 71.7 77.3 89.6 89.2 56.0 70.3 65.9
PT-MAP 55.5 66.5 28.1 67.0 64.6 73.7 84.6 76.6 59.4 61.1 72.2 64.5
TIM 56.6 67.3 28.1 74.3 70.0 77.0 85.3 91.5 88.6 60.5 71.7 70.1
CoOp + UPL 64.6 69.0 28.3 77.9 73.5 79.5 85.8 92.1 90.7 61.2 75.8 72.6
TransCLIP-FS 65.0 69.6 27.9 71.2 74.4 81.5 90.3 89.0 91.7 61.7 76.1 72.6
16TF 56.3 66.8 30.7 68.0 68.0 73.6 76.3 92.0 90.9 61.9 72.6 68.8
BD-CSPN 56.4 66.1 30.8 66.0 67.2 73.4 76.4 91.8 90.8 60.5 72.4 68.3
LaplacianShot 56.0 65.5 29.4 71.2 65.8 74.4 78.6 91.7 90.2 58.8 72.3 68.5
PT-MAP 58.6 68.3 30.9 69.5 69.2 75.3 85.3 78.2 61.5 62.9 73.4 66.6
TIM 61.4 70.6 34.6 79.2 75.8 78.8 84.4 91.8 88.9 67.2 76.4 73.6
CoOp + UPL 64.6 71.1 34.9 82.1 77.6 79.5 85.7 94.0 92.1 65.2 77.1 74.9
TransCLIP-FS 66.4 71.1 28.4 73.8 77.1 81.6 90.6 90.8 92.3 61.5 76.8 73.7
Table 20: Detailed results of transductive methods in the few-shot setting for the 11 datasets with
ViT-B/32 as visual backbone.
Shots Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flowers102 Caltech101DTDUCF101 Average
1TF 25.1 36.1 14.6 44.4 26.7 34.4 33.3 60.0 74.4 29.0 46.4 38.6
BD-CSPN 30.1 42.9 16.2 45.7 33.8 41.2 43.9 73.1 80.2 30.8 52.6 44.6
LaplacianShot 29.2 41.7 16.1 48.6 33.2 43.1 43.8 73.3 80.6 32.7 52.9 45.0
PT-MAP 33.1 48.8 17.0 54.8 38.6 49.8 50.9 62.4 52.5 37.9 57.0 45.7
TIM 31.5 47.6 16.6 55.2 36.4 51.4 48.4 76.8 71.5 35.6 57.6 48.1
CoOp + UPL 63.0 66.2 21.0 64.0 58.1 78.8 84.0 74.4 89.7 52.0 68.3 65.4
TransCLIP-FS 64.3 68.9 22.7 63.5 63.7 82.2 90.1 83.2 92.2 52.3 69.5 68.4
2TF 34.7 49.5 19.3 56.5 37.4 48.7 47.4 75.1 83.9 44.5 57.7 50.4
BD-CSPN 39.2 53.1 20.7 57.2 42.1 55.5 55.2 82.4 86.8 45.6 61.6 54.5
LaplacianShot 39.1 53.9 20.4 58.3 42.4 57.7 57.3 82.5 86.7 45.9 62.6 55.2
PT-MAP 42.6 60.1 22.3 63.7 46.0 63.9 64.0 69.5 55.6 50.4 66.8 55.0
TIM 41.1 59.0 21.1 68.9 44.1 66.2 60.1 86.5 81.5 48.6 68.1 58.7
CoOp + UPL 63.4 66.6 22.8 71.9 60.8 78.5 85.0 81.0 90.1 53.5 70.2 67.6
TransCLIP-FS 64.8 69.5 22.9 76.9 63.8 81.2 89.9 85.4 92.1 52.9 71.0 70.0
4TF 44.5 59.4 23.2 62.1 48.6 60.8 57.9 85.2 89.1 52.6 65.2 59.0
BD-CSPN 47.0 61.1 23.4 64.2 49.1 65.3 64.8 87.2 89.4 52.0 67.0 61.0
LaplacianShot 46.8 61.1 23.6 68.4 49.2 65.6 66.6 87.6 89.3 51.4 67.5 61.6
PT-MAP 50.1 65.5 24.1 68.9 52.3 70.3 69.0 73.3 57.3 56.1 70.1 59.7
TIM 50.4 65.0 24.7 70.0 56.1 73.0 74.4 90.5 88.7 55.9 71.8 65.5
CoOp + UPL 63.9 68.8 26.6 72.6 63.7 78.2 85.2 88.8 90.1 55.4 73.1 69.7
TransCLIP-FS 64.7 70.1 26.4 78.0 66.5 80.3 87.2 88.7 92.2 58.0 74.3 71.5
8TF 50.9 64.7 27.1 67.6 57.1 68.5 68.0 89.4 90.5 58.2 70.7 64.8
BD-CSPN 51.2 64.8 27.4 66.5 56.9 69.6 71.7 90.0 89.6 56.3 71.0 65.0
LaplacianShot 51.0 64.3 26.4 70.0 55.9 70.4 73.7 90.2 90.1 55.4 70.8 65.3
PT-MAP 53.7 68.3 27.4 70.9 58.5 72.8 75.4 75.2 59.7 59.4 71.5 63.0
TIM 56.2 69.0 28.4 75.8 65.1 76.1 79.6 92.3 87.4 63.3 75.4 69.9
CoOp + UPL 64.8 69.7 30.0 79.6 68.9 79.3 85.5 91.6 91.8 62.1 73.9 72.5
TransCLIP-FS 65.5 71.3 28.0 78.2 70.8 81.0 89.4 90.0 92.3 61.1 77.0 73.2
16TF 55.6 68.0 29.7 69.7 62.9 72.6 73.7 92.0 91.6 61.6 73.1 68.2
BD-CSPN 55.3 67.5 29.8 69.5 62.3 72.9 74.2 91.9 91.7 59.6 73.3 68.0
LaplacianShot 54.8 66.7 28.4 71.2 60.9 73.2 75.3 91.3 91.3 58.3 72.9 67.7
PT-MAP 56.9 69.9 29.2 71.3 63.1 74.1 78.7 77.1 60.7 61.9 72.9 65.1
TIM 60.5 71.8 33.0 79.4 72.2 78.1 85.0 92.8 88.4 66.6 78.1 73.3
CoOp + UPL 64.8 71.9 34.1 84.3 73.6 79.0 85.8 94.2 92.4 64.8 78.3 74.8
TransCLIP-FS 66.6 72.6 30.1 78.9 73.2 81.1 89.5 90.9 94.4 62.7 77.2 74.3
25Table 21: Detailed results of transductive methods in the few-shot setting for the 11 datasets with
ViT-B/16 as visual backbone.
Shots Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flowers102 Caltech101DTDUCF101 Average
1TF 29.7 38.1 19.2 46.0 32.5 43.5 38.2 67.8 75.5 31.6 48.8 42.8
BD-CSPN 35.4 45.7 22.0 45.7 42.0 54.2 52.9 82.9 83.5 34.7 58.0 50.6
LaplacianShot 34.9 44.5 22.1 52.1 41.1 53.0 52.2 83.1 83.4 35.8 57.3 50.9
PT-MAP 40.1 52.6 23.8 59.7 48.4 64.4 61.8 69.4 54.1 41.8 63.5 52.7
TIM 37.5 48.3 22.8 48.2 44.8 65.7 53.9 86.4 75.1 35.8 62.7 52.8
CoOp + UPL 68.8 68.5 27.2 70.0 68.9 83.6 90.6 81.7 92.7 51.3 73.1 70.6
TransCLIP-FS 69.8 70.6 29.9 72.5 70.9 87.9 93.8 84.8 93.1 53.3 78.4 73.2
2TF 40.5 51.6 25.3 63.1 45.1 58.8 54.8 83.2 87.0 47.3 59.4 56.0
BD-CSPN 46.1 56.1 26.7 64.7 50.7 67.5 64.6 89.6 89.6 48.9 64.0 60.8
LaplacianShot 45.8 55.9 27.1 68.2 51.1 68.2 66.0 89.7 89.6 48.9 65.1 61.4
PT-MAP 50.7 63.1 28.6 71.7 57.5 77.5 75.7 73.9 59.1 53.8 68.7 61.9
TIM 47.9 60.7 28.1 75.8 55.7 78.7 70.6 91.4 86.6 52.3 66.4 64.9
CoOp + UPL 69.2 69.2 30.1 73.4 71.0 83.8 88.4 87.9 93.3 53.9 75.8 72.4
TransCLIP-FS 70.3 70.9 30.0 77.1 71.7 87.0 91.7 90.6 93.5 55.1 78.5 74.2
4TF 51.1 61.0 30.3 64.9 56.8 71.0 65.9 90.9 91.5 53.7 67.9 64.1
BD-CSPN 53.8 62.5 30.5 64.8 58.5 75.3 72.0 92.5 92.0 52.1 70.9 65.9
LaplacianShot 53.5 62.5 29.6 74.3 58.5 75.7 73.4 92.8 92.0 52.7 71.7 67.0
PT-MAP 57.6 68.1 31.2 74.9 63.1 81.1 79.5 76.2 60.2 58.4 73.9 65.8
TIM 57.4 67.0 32.8 79.3 65.8 83.5 82.3 93.4 88.5 58.1 76.5 71.3
CoOp + UPL 69.7 71.4 32.6 74.0 74.6 83.8 91.3 92.1 93.2 58.9 76.9 74.4
TransCLIP-FS 70.3 71.9 34.0 79.4 74.0 86.4 91.6 93.6 94.0 61.1 79.1 75.9
8TF 57.2 66.8 34.7 68.5 65.4 77.4 74.3 93.8 92.4 60.3 73.8 69.5
BD-CSPN 57.9 66.5 34.1 68.3 64.6 78.0 77.2 93.2 92.4 59.0 74.2 69.6
LaplacianShot 57.6 65.9 33.4 73.2 64.7 79.3 79.3 93.3 92.3 56.5 74.6 70.0
PT-MAP 61.0 70.6 34.1 75.0 68.5 82.0 84.5 77.2 62.1 62.4 75.6 68.5
TIM 62.6 71.3 35.9 79.8 74.4 84.3 87.4 94.0 90.7 63.6 80.2 74.9
CoOp + UPL 70.5 72.8 38.6 79.1 78.3 84.5 90.4 94.4 93.3 60.6 79.6 76.6
TransCLIP-FS 70.5 73.2 36.4 79.7 76.9 86.7 91.9 93.9 94.2 65.7 81.5 77.3
16TF 61.8 70.1 38.3 74.3 71.2 80.7 79.5 95.4 93.6 62.9 76.0 73.1
BD-CSPN 61.7 69.4 37.7 73.4 70.7 80.2 81.2 94.8 93.3 61.3 76.0 72.7
LaplacianShot 60.9 68.3 36.1 78.1 69.2 81.2 81.7 94.8 93.1 58.6 76.3 72.6
PT-MAP 64.0 72.0 37.4 75.6 72.0 82.7 86.1 78.5 63.7 63.7 76.3 70.2
TIM 67.8 73.6 40.6 83.6 79.5 84.9 88.7 95.4 92.4 67.5 82.1 77.8
CoOp + UPL 71.6 75.1 43.2 83.0 82.3 85.0 90.4 95.8 94.3 68.7 80.4 79.1
TransCLIP-FS 71.8 74.7 38.6 83.0 79.8 86.9 92.4 94.4 94.0 65.1 82.1 78.4
Table 22: Detailed results of transductive methods in the few-shot setting for the 11 datasets with
ViT-L/14 as visual backbone.
Shots Method
ImageNet SUN397 Aircraft EuroSATStanfordCarsFood101Pets
Flowers102 Caltech101DTDUCF101 Average
1TF 36.6 41.2 26.3 49.8 45.2 53.9 45.8 81.8 79.7 35.8 58.3 50.4
BD-CSPN 45.3 50.5 28.9 53.3 57.5 67.3 66.7 93.4 88.4 39.6 67.2 59.8
LaplacianShot 43.5 48.4 30.9 56.6 56.1 69.3 65.8 93.3 87.9 40.1 66.2 59.8
PT-MAP 49.8 58.1 33.1 65.6 60.6 80.1 78.1 75.2 58.5 45.7 69.7 61.3
TIM 47.7 56.0 31.1 62.8 61.1 79.7 74.2 95.4 80.1 41.7 71.5 63.8
CoOp + UPL 76.0 72.6 35.8 72.7 79.2 89.5 93.2 86.8 94.9 60.3 81.1 76.6
TransCLIP-FS 75.9 74.5 37.9 77.4 78.8 92.2 95.4 95.9 95.6 61.3 83.3 78.9
2TF 50.1 56.6 33.5 71.7 58.3 71.6 65.7 93.0 90.5 49.8 69.4 64.6
BD-CSPN 57.0 61.2 35.6 72.6 65.1 79.9 77.2 95.7 92.8 52.3 74.7 69.5
LaplacianShot 56.5 61.3 35.9 76.8 65.4 80.3 77.4 96.2 93.3 52.4 74.8 70.0
PT-MAP 61.3 68.0 37.0 78.4 68.4 87.3 86.7 77.9 61.1 56.5 75.2 68.9
TIM 59.7 67.6 35.4 82.2 69.3 87.4 85.5 95.1 91.4 53.2 78.6 73.2
CoOp + UPL 76.1 73.4 39.9 72.3 81.4 90.3 92.5 94.0 94.7 62.0 82.2 78.1
TransCLIP-FS 76.8 75.1 40.0 82.1 79.9 91.8 95.0 96.6 95.9 62.6 83.2 79.9
4TF 61.6 66.5 40.6 71.4 69.6 81.9 79.0 96.4 94.4 58.5 77.5 72.5
BD-CSPN 64.3 67.8 40.6 71.4 72.2 84.7 82.8 96.7 95.2 56.9 79.6 73.8
LaplacianShot 63.8 67.6 40.0 78.9 72.0 85.4 85.7 97.3 95.2 56.7 79.6 74.7
PT-MAP 68.0 72.7 41.7 77.4 73.8 88.9 89.9 78.3 62.9 60.1 79.2 72.1
TIM 68.9 72.7 42.0 78.4 77.8 90.0 92.3 97.4 91.1 63.5 83.7 78.0
CoOp + UPL 76.5 75.1 44.1 79.3 83.1 90.1 92.6 95.2 95.3 65.8 83.9 80.1
TransCLIP-FS 76.9 76.2 45.9 81.5 81.2 91.4 94.3 98.2 96.1 66.8 84.9 81.2
8TF 67.4 72.0 45.6 76.1 76.5 86.2 85.1 97.2 95.1 65.1 81.5 77.1
BD-CSPN 68.0 71.5 44.8 76.1 76.5 86.8 86.8 97.3 94.9 63.8 81.3 77.1
LaplacianShot 67.3 70.4 43.6 78.2 75.9 87.3 88.3 97.0 94.9 61.2 80.8 76.8
PT-MAP 70.7 74.6 44.1 78.4 77.1 89.2 91.3 79.5 64.5 65.1 79.7 74.0
TIM 73.1 76.4 46.7 86.8 83.2 89.5 92.7 96.9 94.4 70.2 81.3 81.0
CoOp + UPL 76.9 75.8 49.6 81.7 85.5 90.1 93.2 95.9 95.3 65.6 84.0 81.2
TransCLIP-FS 77.2 77.3 50.0 82.6 84.1 91.6 94.5 98.5 97.0 70.7 86.0 82.7
16TF 71.1 74.9 50.1 78.6 81.5 88.1 88.6 98.5 96.1 67.3 83.0 79.8
BD-CSPN 71.1 74.4 49.4 78.1 81.2 88.0 89.8 98.4 95.8 66.5 82.5 79.6
LaplacianShot 69.8 72.7 47.0 81.7 80.2 88.0 90.1 98.0 95.7 63.3 82.8 79.0
PT-MAP 72.9 75.9 48.1 79.1 79.9 89.4 92.0 80.5 66.0 65.6 80.5 75.4
TIM 76.4 78.7 52.5 89.4 86.5 91.0 92.0 98.2 94.5 73.2 84.8 83.4
CoOp + UPL 76.9 77.2 54.1 85.9 87.8 90.6 93.2 97.1 95.6 72.8 86.2 83.4
TransCLIP-FS 77.8 78.7 53.0 84.4 86.3 91.6 94.8 98.8 97.3 71.2 86.5 83.7
26Table 23: UPL∗top-1 accuracy on ImageNet for 8, 16 and 32 top-confidence pseudo-labels drawn
from the test set.
Architecture N= 8 N= 16 N= 32
ResNet-50 60.60 61.60 59.66
ViT-B/16 68.92 69.62 68.87
Table 24: Prompt templates for each dataset.
(a) Prompt templates used in the experiments unless
otherwise specified.
Dataset Prompt template
ImageNet " a photo of a []. "
SUN397 " a photo of a []. "
Aircraft " a photo of a [], a type of aircraft. ",
EuroSAT " a centered satellite photo of []. ",
Cars " a photo of a []. ",
Food101 " a photo of [], a type of food. ",
Pets " a photo of [], a type of pet. ",
Flower102 " a photo of a [], a type of flower. ",
Caltech101 " a photo of a []. ",
DTD " [] texture. ",
UCF101 " a photo of a person doing []. ",(b) Custom prompt templates for ImageNet
dataset [50].
"itap of a []. "
"a bad photo of the []. "
"a origami []. "
"a photo of the large []. "
"a [] in a video game. "
"art of the []. "
"a photo of the small []. "
27NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Claim i) from the abstract is discussed and supported by results in Tables 1, 2
and 3, and claim (ii) is discussed and supported by results in 4. The contributions announced
in the introduction: (i) is presented in Section 3, (ii) is supported by Tables 1, 2 and 3,
and (iii) is supported by Table 4. More results are also available in the Appendix for five
encoders.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Limitations are presented in Appendix D.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
28Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The convergence of our algorithm is proved and assumptions are enumerated
in Appendix A.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The hyper-parameter values are stated in Section 4 and in Appendix B. A
complete pseudo-code is provided in Appendix B with further implementation details.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
295.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code is provided at the following link: https://github.com/
MaxZanella/transduction-for-vlms .
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: For the datasets used in our experiments, we follow the settings of previous
works, as stated in Section 4. The implementation of transductive methods is detailed in
Appendix C.5. If not specified, the exact hyper-parameters from the initial implementations
are used for other cited methods.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Every numerical accuracy is an average over three random seeds. Our experi-
ments cover 15 datasets and 6 encoder architectures.
Guidelines:
• The answer NA means that the paper does not include experiments.
30•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The hardware used for the experiments is presented in Appendix B. and
runtime is discussed in Section 4 (Table 5).
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Our work aligns with the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The paper is not tied to specific application.
31Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: There is no such risk.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All models and methods are credited.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
32•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new asset is released.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing nor research with human subject was involved.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing nor research with human subject was involved.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
33•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
34