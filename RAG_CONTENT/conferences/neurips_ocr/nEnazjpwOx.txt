Diffusion Models Meet Contextual Bandits with Large
Action Spaces
Anonymous Author(s)
Affiliation
Address
email
Abstract
Efficient exploration in contextual bandits is crucial due to their large action space, 1
where uninformed exploration can lead to computational and statistical inefficien- 2
cies. However, the rewards of actions are often correlated, which can be leveraged 3
for more efficient exploration. In this work, we use pre-trained diffusion model pri- 4
ors to capture these correlations and develop diffusion Thompson sampling ( dTS). 5
We establish both theoretical and algorithmic foundations for dTS. Specifically, 6
we derive efficient posterior approximations (required by dTS) under a diffusion 7
model prior, which are of independent interest beyond bandits and reinforcement 8
learning. We analyze dTSin linear instances and provide a Bayes regret bound 9
highlighting the benefits of using diffusion models as priors. Our experiments 10
validate our theory and demonstrate dTS’s favorable performance. 11
1 Introduction 12
Acontextual bandit is a popular and practical framework for online learning under uncertainty [Li 13
et al., 2010]. In each round, an agent observes a context , takes an action , and receives a reward based 14
on the context and action. The goal is to maximize the expected cumulative reward over nrounds, 15
striking a balance between exploiting actions with high estimated rewards from available data and 16
exploring other actions to improve current estimates. This trade-off is often addressed using either 17
upper confidence bound (UCB) [Auer et al., 2002] or Thompson sampling (TS) [Scott, 2010]. 18
The action space in contextual bandits is often large, resulting in less-than-optimal performance 19
with standard exploration strategies. Luckily, actions usually exhibit correlations, making efficient 20
exploration possible as one action may inform the agent about other actions. In particular, Thompson 21
sampling offers remarkable flexibility, allowing its integration with informative priors [Hong et al., 22
2022b] that capture these correlations. Inspired by the achievements of diffusion models [Sohl- 23
Dickstein et al., 2015, Ho et al., 2020], which effectively approximate complex distributions [Dhariwal 24
and Nichol, 2021, Rombach et al., 2022], this work captures action correlations by employing 25
diffusion models as priors in contextual Thompson sampling. 26
We illustrate the idea using video streaming. The objective is to optimize watch time for a user j 27
by selecting a video ifrom a catalog of Kvideos. Users jand videos iare associated with context 28
vectors xjand unknown video parameters θi, respectively. User j’s expected watch time for video i 29
is linear as x⊤
jθi. Then, a natural strategy is to independently learn video parameters θiusing LinTS 30
orLinUCB [Agrawal and Goyal, 2013a, Abbasi-Yadkori et al., 2011], but this proves statistically 31
inefficient for larger K. Fortunately, the reward when recommending a movie can provide informative 32
insights into other movies. To capture this, we leverage offline estimates of video parameters denoted 33
byˆθiand build a diffusion model on them. This diffusion model approximates the video parameter 34
distribution, capturing their dependencies. This model enriches contextual Thompson sampling as a 35
prior, effectively capturing complex video dependencies while ensuring computational efficiency. 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.We introduce a framework for contextual bandits with diffusion model priors, upon which we develop 37
diffusion Thompson sampling ( dTS) that is both computationally and statistically efficient. dTS 38
requires fast updates of the posterior andfast sampling from the posterior , both of which are achieved 39
through our novel efficient posterior approximations. These approximations become exact when 40
both the diffusion model and likelihood are linear. We establish a bound on dTS’s Bayes regret for 41
this specific case, highlighting the advantages of using diffusion models as priors. Our empirical 42
evaluations validate our theory and demonstrate dTS’s strong performance across various settings. 43
Diffusion models were applied in offline decision-making [Ajay et al., 2022, Janner et al., 2022, Wang 44
et al., 2022], but their use in online learning was only recently explored by Hsieh et al. [2023], who 45
focused on multi-armed bandits without theoretical guarantees . Our work extends Hsieh et al. [2023] 46
in two ways. First, we apply the concept to the broader contextual bandit, which is more practical and 47
realistic. Second, we demonstrate that with diffusion models parametrized by linear score functions 48
and linear rewards, we can derive exact closed-form posteriors without approximations. These exact 49
posteriors are valuable as they enable theoretical analysis (unlike Hsieh et al. [2023], who did not 50
provide theoretical guarantees) and motivate efficient approximations for non-linear score functions 51
in contextual bandits, addressing gaps in Hsieh et al. [2023]’s focus on multi-armed bandits. 52
A key contribution, beyond applying diffusion models in contextual bandits, is the efficient com- 53
putation andsampling of the posterior distribution of a d-dimensional parameter θ|Ht, with Ht 54
representing the data, when using a diffusion model prior on θ. This is relevant not only to bandits 55
and reinforcement learning but also to a broader range of applications [Chung et al., 2022]. To 56
motivate our approximations, we start with exact closed-form solutions for cases where both the 57
score functions of the diffusion model and the likelihood are linear. These solutions form the basis for 58
our approximations for non-linear score functions, demonstrating both strong empirical performance 59
and computational efficiency. Our approach avoids the computational burden of heavy approximate 60
sampling algorithms required for each latent parameter. For a detailed comparison with existing 61
studies, see Appendix A, where we discuss diffusion models in decision-making, structured bandits, 62
approximate posteriors, and more. 63
2 Setting 64
The agent interacts with a contextual bandit overnrounds. In round t∈[n], the agent observes a 65
context Xt∈ X, where X ⊆Rdis acontext space , it takes an action At∈[K], and then receives a 66
stochastic reward Yt∈Rthat depends on both the context Xtand the taken action At. Each action 67
i∈[K]is associated with an unknown action parameter θ∗,i∈Rd, so that the reward received in 68
round tisYt∼P(· |Xt;θ∗,At), where P(· |x;θ∗,i)is the reward distribution of action iin context 69
x. Throughout the paper, we assume that the reward distribution is parametrized as a generalized 70
linear model (GLM) [McCullagh and Nelder, 1989]. That is, for any x∈ X ,P(· |x;θ∗,i)is an 71
exponential-family distribution with mean g(x⊤θ∗,i), where gis the mean function. For example, we 72
recover linear bandits when P(· |x;θ∗,i) =N(·;x⊤θ∗,i, σ2)where σ >0is the observation noise. 73
Similarly, we recover logistic bandits [Filippi et al., 2010] if we let g(u) = (1 + exp( −u))−1and 74
P(· |x;θ∗,i) = Ber( g(x⊤θ∗,i)), where Ber(p)be the Bernoulli distribution with mean p. 75
We consider the Bayesian bandit setting [Russo and Van Roy, 2014, Hong et al., 2022b], where the 76
action parameters θ∗,iare assumed to be sampled from a known prior distribution. We proceed to 77
define this prior distribution using a diffusion model. The correlations between the action parameters 78
θ∗,iare captured through a diffusion model, where they share a set of Lconsecutive unknown latent 79
parameters ψ∗,ℓ∈Rdforℓ∈[L]. Precisely, the action parameter θ∗,idepends on the L-th latent 80
parameter ψ∗,Lasθ∗,i|ψ∗,1∼ N(f1(ψ∗,1),Σ1), where the score function f1:Rd→Rdisknown . 81
Also, the ℓ−1-th latent parameter ψ∗,ℓ−1depends on the ℓ-th latent parameter ψ∗,ℓasψ∗,ℓ−1|ψ∗,ℓ∼ 82
N(fℓ(ψ∗,ℓ),Σℓ), where the score function fℓ:Rd→Rdis known. Finally, the L-th latent parameter 83
ψ∗,Lis sampled as ψ∗,L∼ N(0,ΣL+1). We summarize this model in (1)and its graph in Fig. 1. 84
: taken action
in round 
Figure 1: Graphical model of (1).85
ψ∗,L∼ N(0,ΣL+1), (1)
ψ∗,ℓ−1|ψ∗,ℓ∼ N(fℓ(ψ∗,ℓ),Σℓ),∀ℓ∈[L]/{1},
θ∗,i|ψ∗,1∼ N(f1(ψ∗,1),Σ1), ∀i∈[K],
Yt|Xt, θ∗,At∼P(· |Xt;θ∗,At), ∀t∈[n].
2The model in (1)represents a Bayesian bandit, where the agent interacts with a bandit instance 86
defined by θ∗,iovernrounds (4-th line in (1)). These action parameters θ∗,iare drawn from the 87
generative process in the first 3 lines of (1). In practice, (1)can be built by pre-training a diffusion 88
model on offline estimates of the action parameters θ∗,i[Hsieh et al., 2023]. 89
A natural goal for the agent in this Bayesian framework is to minimize its Bayes regret [Russo and Van 90
Roy, 2014] that measures the expected performance across multiple bandit instances θ∗= (θ∗,i)i∈[K], 91
BR(n) =EhnX
t=1r(Xt, At,∗;θ∗)−r(Xt, At;θ∗)i
, (2)
where the expectation in (2) is taken over all random variables in (1). Here 92
r(x, i;θ∗) =EY∼P(·|x;θ∗,i)[Y]is the expected reward of action iin context xandAt,∗= 93
arg maxi∈[K]r(Xt, i;θ∗)is the optimal action in round t. The Bayes regret is known to capture the 94
benefits of using informative priors, and hence it is suitable for our problem. 95
3 Diffusion contextual Thompson sampling 96
We design Thompson sampling that samples the latent and action parameters hierarchically [Lindley 97
and Smith, 1972]. Precisely, let Ht= (Xk, Ak, Yk)k∈[t−1]be the history of all interactions up to 98
round tand let Ht,i= (Xk, Ak, Yk){k∈[t−1];Ak=i}be the history of interactions with action iup to 99
round t. To motivate our algorithm, we decompose the posterior P(θ∗,i=θ|Ht)recursively as 100
P(θ∗,i=θ|Ht) =Z
ψ1:LQt,L(ψL)LY
ℓ=2Qt,ℓ−1(ψℓ−1|ψℓ)Pt,i(θ|ψ1) dψ1:L,where (3)
Qt,L(ψL) =P(ψ∗,L=ψL|Ht)is the latent-posterior density of ψ∗,L|Ht. Moreover, for any 101
ℓ∈[2 :L],Qt,ℓ−1(ψℓ−1|ψℓ) =P(ψ∗,ℓ−1=ψℓ−1|Ht, ψ∗,ℓ=ψℓ)is the conditional latent- 102
posterior density of ψ∗,ℓ−1|Ht, ψ∗,ℓ=ψℓ. Finally, for any action i∈[K],Pt,i(θ|ψ1) = 103
P(θ∗,i=θ|Ht,i, ψ∗,1=ψ1)is the conditional action-posterior density of θ∗,i|Ht,i, ψ∗,1=ψ1. 104
The decomposition in (3)inspires hierarchical sampling. In round t, we initially sample the L-th 105
latent parameter as ψt,L∼Qt,L(·). Then, for ℓ∈[L]/{1}, we sample the ℓ−1-th latent parameter 106
given that ψ∗,ℓ=ψt,ℓ, asψt,ℓ−1∼Qt,ℓ−1(· |ψt,ℓ). Lastly, given that ψ∗,1=ψt,1, each action 107
parameter is sampled individually asθt,i∼Pt,i(θ|ψt,1). This is possible because action parameters 108
θ∗,iare conditionally independent given ψ∗,1. This leads to Algorithm 1, named diffusion Thompson 109
Sampling ( dTS).dTSrequires sampling from the K+Lposteriors Pt,iandQt,ℓ. Thus we start by 110
providing an efficient recursive scheme to express these posteriors using known quantities. We note 111
that these expressions do not necessarily lead to closed-form posteriors and approximation might be 112
needed. First, the conditional action-posterior Pt,i(· |ψ1)can be written as 113
Pt,i(θ|ψ1)∝Y
k∈St,iP(Yk|Xk;θ)N(θ;f1(ψ1),Σ1), (4)
where St,i={ℓ∈[t−1], Aℓ=i}are the rounds where the agent takes action iup to round t. 114
Moreover, let Lℓ(ψℓ) =P(Ht|ψ∗,ℓ=ψℓ)be the likelihood of observations up to round tgiven that 115
ψ∗,ℓ=ψℓ. Then, for any ℓ∈[L]/{1}, theℓ−1-th conditional latent-posterior Qt,ℓ−1(· |ψℓ)is 116
Qt,ℓ−1(ψℓ−1|ψℓ)∝ Lℓ−1(ψℓ−1)N(ψℓ−1, fℓ(ψℓ),Σℓ), (5)
andQt,L(ψL)∝ LL(ψL)N(ψL,0,ΣL+1). All the terms above are known, except the likelihoods 117
Lℓ(ψℓ)forℓ∈[L]. These are computed recursively as follows. First, the basis of the recursion is 118
L1(ψ1) =KY
i=1Z
θiY
k∈St,iP(Yk|Xk;θi)N(θi;f1(ψ1),Σ1) dθi. (6)
Then for ℓ∈[L]/{1}, the recursive step is Lℓ(ψℓ) =R
ψℓ−1Lℓ−1(ψℓ−1)N(ψℓ−1;fℓ(ψℓ),Σℓ) dψℓ−1. 119
All posterior expressions above use known quantities (fℓ,Σℓ, P(y|x;θ)). However, these expres- 120
sions typically need to be approximated, except when the score functions fℓare linear and the reward 121
distribution P(· |x;θ)is linear-Gaussian, where closed-form solutions can be obtained with careful 122
derivations. These approximations are not trivial, and prior studies often rely on computationally 123
intensive approximate sampling algorithms. In the following sections, we explain how we derive our 124
efficient approximations which are motivated by the closed-form solutions of linear instances. 125
3Algorithm 1 dTS:diffusion Thompson Sampling
Input: Prior: fℓ, ℓ∈[L],Σℓ, ℓ∈[L+ 1], and P.
fort= 1, . . . , n do
Sample ψt,L∼Qt,L(requires fast approximate posterior update and sampling)
forℓ=L, . . . , 2do
Sample ψt,ℓ−1∼Qt,ℓ−1(· |ψt,ℓ)(requires fast approximate posterior update and sampling)
fori= 1, . . . , K do
Sample θt,i∼Pt,i(· |ψt,1)(requires fast approximate posterior update and sampling)
Take action At= argmaxi∈[K]r(Xt, i;θt), where θt= (θt,i)i∈[K]
Receive reward Yt∼P(· |Xt;θ∗,At)and update posteriors Qt+1,ℓandPt+1,i.
3.1 Linear diffusion model 126
Assume the score functions fℓare linear such as fℓ(ψ∗,ℓ) = W ℓψ∗,ℓforℓ∈[L], where Wℓ∈Rd×d127
areknown mixing matrices . Then, (1)becomes a linear Gaussian system (LGS) [Bishop, 2006] in 128
this case. This model is important, both in theory and practice. For theory, it leads to closed-form 129
posteriors when the reward distribution is linear-Gaussian as P(· |x;θ∗,i) =N(·;x⊤θ∗,i, σ2). This 130
allows bounding the Bayes regret of dTS. For practice, the posterior expressions are used to motivate 131
efficient approximations for the general case in (1) as we show in Section 3.2. 132
The reward distribution is parameterized as a generalized linear model (GLM) [McCullagh and 133
Nelder, 1989], allowing for non-linear rewards. Thus, we need posterior approximation despite 134
linearity in score functions. Since this non-linearity arises solely from the reward distribution, we 135
approximate it by a Gaussian and propagate this approximation to the latent parameters. This results 136
in efficient posterior approximations that are exact when the reward function is Gaussian (a special 137
case of the GLM model). Specifically, the reward distribution P(· |x;θ)is an exponential family 138
distribution with a mean function denoted by g. Then, we approximate the corresponding likelihood 139
asP(Ht,i|θ∗,i=θ)≈ N 
θ;ˆBt,i,ˆG−1
t,i
, where ˆBt,iandˆGt,iare the maximum likelihood estimate 140
(MLE) and the Hessian of the negative log-likelihood, respectively, and they are defined as 141
ˆBt,i= arg maxθ∈RdlogP(Ht,i|θ∗,i=θ), ˆGt,i=P
k∈St,i˙g 
X⊤
kˆBt,i
XkX⊤
k.(7)
where St,i={ℓ∈[t−1] :Aℓ=i}represents the rounds where the agent takes action iup to 142
round t. This simple approximation makes all posteriors Gaussian. Specifically, the conditional 143
action-posterior is Gaussian and is given by Pt,i(· |ψ1) =N(·; ˆµt,i,ˆΣt,i), where ˆµt,iandˆΣt,iare 144
computed using ˆBt,iandˆGt,iin(7). Moreover, for ℓ∈[L−1], theℓ-th conditional latent-posterior is 145
also Gaussian, Qt,ℓ(· |ψℓ+1) =N(·; ¯µt,ℓ,¯Σt,ℓ), where ¯µt,ℓand¯Σt,ℓare computed recursively. The 146
recursion starts with ¯µt,1and¯Σt,1, which are calculated using ˆBt,iandˆGt,iin(7). Full expressions are 147
provided in Appendix B.1. The only approximation made is P(Ht,i|θ∗,i=θ)≈ N 
θ;ˆBt,i,ˆG−1
t,i
, 148
and we propagated it to latent posteriors. Thus, these posterior approximations become exact when 149
the reward distribution follows a linear-Gaussian model, P(· |x;θ∗,a) =N(·;x⊤θ∗,a, σ2). 150
3.2 Non-linear diffusion model 151
After deriving the posteriors for linear score functions, we return to the general model in (1). 152
Approximation is needed since both the score functions and rewards can be non-linear. To avoid 153
computational challenges, we use a simple and intuitive approximation, where all posteriors Pt,i 154
andQt,ℓare approximated by Gaussians that are computed recursively. First, the conditional action- 155
posterior is approximated by a Gaussian distribution as Pt,i(· |ψ1) =N(·; ˆµt,i,ˆΣt,i), where 156
ˆΣ−1
t,i= Σ−1
1+ˆGt,i ˆµt,i=ˆΣt,i 
Σ−1
1f1(ψ1) +ˆGt,iˆBt,i
. (8)
In the absence of samples, Gt,i= 0d×d. Thus, the approximate action posterior in (8)matches 157
precisely the term N(f1(ψ1),Σ1)in the diffusion prior (1). Moreover, as more data is accumulated, 158
Gt,iincreases, and the influence of the prior diminishes as ˆGt,iˆBt,iwill dominate the prior term 159
Σ−1
1f1(ψ1). Similarly, for ℓ∈[L]/{1}, theℓ−1-th conditional latent-posterior is approximated by 160
4a Gaussian distribution as Qt,ℓ−1(· |ψℓ) =N(¯µt,ℓ−1,¯Σt,ℓ−1), where 161
¯Σ−1
t,ℓ−1= Σ−1
ℓ+¯Gt,ℓ−1, ¯µt,ℓ−1=¯Σt,ℓ−1 
Σ−1
ℓfℓ(ψℓ) +¯Bt,ℓ−1
, (9)
and the L-th latent-posterior is Qt,L(·) =N(¯µt,L,¯Σt,L), 162
¯Σ−1
t,L= Σ−1
L+1+¯Gt,L, ¯µt,L=¯Σt,L¯Bt,L. (10)
Here, ¯Gt,ℓand¯Bt,ℓforℓ∈[L]are computed recursively. The basis of the recursion are 163
¯Gt,1=PK
i=1 
Σ−1
1−Σ−1
1ˆΣt,iΣ−1
1
, ¯Bt,1= Σ−1
1PK
i=1ˆΣt,iˆGt,iˆBt,i. (11)
Then, the recursive step for ℓ∈[L]/{1}is, 164
¯Gt,ℓ= Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ, ¯Bt,ℓ= Σ−1
ℓ¯Σt,ℓ−1¯Bt,ℓ−1. (12)
Similarly, in the absence of samples, Qt,ℓ−1in(9)precisely matches the term N(fℓ(ψ1),Σℓ)in the 165
diffusion prior (1). As more data is accumulated, the influence of this prior diminishes. Therefore, 166
this approximation retains a key attribute of exact posteriors: they match the prior when there is no 167
data, and the prior’s effect diminishes as data accumulates. 168
4 Analysis 169
We analyze dTSunder the linear diffusion model in Section 3.1 with linear rewards P(· |x;θ∗,a) = 170
N(·;x⊤θ∗,a, σ2). This assumption leads to a structure with Llayers of linear Gaussian relationships, 171
allowing for theory inspired by linear bandits [Agrawal and Goyal, 2013a, Abbasi-Yadkori et al., 172
2011]. However, proofs are not the same, and technical challenges remain (explained in Appendix D). 173
Although our result holds for milder assumptions, we make some simplifications for clarity and 174
interpretability. We assume that (A1) Contexts satisfy ∥Xt∥2
2= 1 for any t∈[n].(A2) Mixing 175
matrices and covariances satisfy λ1(W⊤
ℓWℓ) = 1 for any ℓ∈[L]andΣℓ=σ2
ℓIdfor any ℓ∈[L+ 1]. 176
Note that (A1) can be relaxed to any contexts Xtwith bounded norms ∥Xt∥2. Also, (A2) can be 177
relaxed to positive definite covariances Σℓand arbitrary mixing matrices Wℓ. In this section, we 178
write ˜Ofor the big-O notation up to polylogarithmic factors. We start by stating our bound for dTS. 179
Theorem 4.1. Letσ2
MAX= max ℓ∈[L+1]1 +σ2
ℓ
σ2. For any δ∈(0,1), the Bayes regret of dTSunder 180
Section 3.1 with linear rewards, (A1) and(A2) is bounded as 181
BR(n)≤r
2n 
RACT(n) +PL
ℓ=1RLAT
ℓ
log(1/δ)
+cnδ , withc >0is constant and , (13)
RACT(n) =c0dKlog 
1 +nσ2
1
d
, c0=σ2
1
log(1+σ2
1),RLAT
ℓ=cℓdlog 
1 +σ2
ℓ+1
σ2
ℓ
, cℓ=σ2
ℓ+1σ2ℓ
MAX
log(1+σ2
ℓ+1),
(13) holds for any δ∈(0,1). In particular, the term cnδis constant when δ= 1/n. Then, the 182
bound is ˜O(√n), and this dependence on the horizon naligns with prior Bayes regret bounds. The 183
bound comprises L+ 1main terms, RACT(n)andRLAT
ℓforℓ∈[L]. First, RACT(n)relates to action 184
parameters learning, conforming to a standard form [Lu and Van Roy, 2019]. Similarly, RLAT
ℓis 185
associated with learning the ℓ-th latent parameter. Roughly speaking, our bound captures that our 186
problem can be seen as L+ 1sequential linear bandit instances stacked upon each other. 187
Technical contributions. dTSuses hierarchical sampling. Thus the marginal posterior distribution of 188
θ∗,i|Htis not explicitly defined. The first contribution is deriving θ∗,i|Htusing the total covariance 189
decomposition combined with an induction proof, as our posteriors in Section 3.1 were derived 190
recursively. Unlike standard analyses where the posterior distribution of θ∗,i|Htis predetermined 191
due to the absence of latent parameters, our method necessitates this recursive total covariance 192
decomposition. Moreover, in standard proofs, we need to quantify the increase in posterior precision 193
for the action taken Atin each round t∈[n]. However, in dTS, our analysis extends beyond this. 194
We not only quantify the posterior information gain for the taken action but also for every latent 195
parameter, since they are also learned. To elaborate, we use the recursive formulas in Section 3.1 that 196
connect the posterior covariance of each latent parameter ψ∗,ℓwith the covariance of the posterior 197
action parameters θ∗,i. This allows us to propagate the information gain associated with the action 198
5taken in round Atto all latent parameters ψ∗,ℓ,forℓ∈[L]by induction. Finally, we carefully bound 199
the resulting terms so that the constants reflect the parameters of the linear diffusion model. More 200
technical details are provided in Appendix D. 201
To include more structure, we propose the sparsity assumption (A3) Wℓ= (¯Wℓ,0d,d−dℓ), where 202
¯Wℓ∈Rd×dℓfor any ℓ∈[L]. Note that (A3) is not an assumption when dℓ=dfor any ℓ∈[L]. 203
Notably, (A3) incorporates a plausible structural characteristic that a diffusion model could capture. 204
Proposition 4.2 (Sparsity) .Letσ2
MAX= max ℓ∈[L+1]1 +σ2
ℓ
σ2. For any δ∈(0,1), the Bayes regret of 205
dTSunder Section 3.1 with linear rewards, (A1) ,(A2) and(A3) is bounded as 206
BR(n)≤r
2n 
RACT(n) +PL
ℓ=1˜RLAT
ℓ
log(1/δ)
+cnδ , withc >0is constant, (14)
RACT(n) =c0dKlog 
1 +nσ2
1
d
, c0=σ2
1
log(1+σ2
1),˜RLAT
ℓ=cℓdℓlog 
1 +σ2
ℓ+1
σ2
ℓ
, cℓ=σ2
ℓ+1σ2ℓ
MAX
log(1+σ2
ℓ+1).
From Proposition 4.2, our bounds scales as BR(n) =˜Oq
n(dKσ2
1+PL
ℓ=1dℓσ2
ℓ+1σ2ℓ
MAX)
. The 207
Bayes regret bound has a clear interpretation: if the true environment parameters are drawn from 208
the prior, then the expected regret of an algorithm stays below that bound. Consequently, a less 209
informative prior (such as high variance) leads to a more challenging problem and thus a higher 210
bound. Then, smaller values of K,L,dordℓtranslate to fewer parameters to learn, leading to lower 211
regret. The regret also decreases when the initial variances σ2
ℓdecrease. These dependencies are 212
common in Bayesian analysis, and empirical results match them. The reader might question the 213
dependence of our bound on both LandK. We will address this next. 214
Why the bound increases with K?This arises due to our conditional learning of θ∗,igiven 215
ψ∗,1. Rather than assuming deterministic linearity, θ∗,i= W 1ψ∗,1, we account for stochasticity by 216
modeling θ∗,i∼ N(W1ψ∗,1, σ2
1Id). This makes dTSrobust to misspecification scenarios where θ∗,i 217
is not perfectly linear with respect to ψ∗,1, at the cost of additional learning of θ∗,i|ψ∗,1. If we were 218
to assume deterministic linearity ( σ1= 0), our regret bound would scale with Lonly. 219
Why the bound increases with L?This is because increasing the number of layers Ladds more 220
initial uncertainty due to the additional covariance introduced by the extra layers. However, this does 221
not imply that we should always use L= 1(the minimum possible L). While a higher Lcomplicates 222
online learning and increases regret bound, it also enables the capture of a more complex prior 223
distribution through offline pre-training of the diffusion model. Thus, a trade-off exists in practice. 224
A smaller Lresults in faster computation and easier learning for dTS, but the learned prior might 225
deviate from reality, potentially violating the "true prior assumption" used to derive the regret bound. 226
On the other hand, a larger Lallows for better modeling of complex action distributions, producing a 227
prior that more accurately reflects reality and strengthens the validity of the bound. 228
4.1 Discussion 229
Computational benefits. Action correlations prompt an intuitive approach: marginalize all latent 230
parameters and maintain a joint posterior of (θ∗,i)i∈[K]|Ht. Unfortunately, this is computationally 231
inefficient for large action spaces. To illustrate, suppose that all posteriors are multivariate Gaussians 232
(Section 3.1). Then maintaining the joint posterior (θ∗,i)i∈[K]|Htnecessitates converting and 233
storing its dK×dK-dimensional covariance matrix. Then the time and space complexities are 234
O(K3d3)andO(K2d2). In contrast, the time and space complexities of dTSareO  
L+K
d3
235
andO  
L+K
d2
. This is because dTSrequires converting and storing L+Kcovariance matrices, 236
each being d×d-dimensional. The improvement is huge when K≫L, which is common in 237
practice. Certainly, a more straightforward way to enhance computational efficiency is to discard 238
latent parameters and maintain Kindividual posteriors, each relating to an action parameter θ∗,i∈Rd239
(LinTS ). This improves time and space complexity to O 
Kd3
andO 
Kd2
, respectively. However, 240
LinTS maintains independent posteriors and fails to capture the correlations among actions; it only 241
models θ∗,i|Ht,irather than θ∗,i|Htas done by dTS. Consequently, LinTS incurs higher regret 242
due to the information loss caused by unused interactions of similar actions. Our regret bound and 243
empirical results reflect this aspect. 244
Statistical benefits. We do not provide a matching lower bound. The only Bayesian lower bound 245
that we know of is Ω(log2(n))for a much simpler K-armed bandit [Lai, 1987, Theorem 3]. All 246
6seminal works on Bayesian bandits do not match it and providing such lower bounds on Bayes regret 247
is still relatively unexplored (even in standard settings) compared to the frequentist one. Therefore, 248
we argue that our bound reflects the overall structure of the problem by comparing dTSto algorithms 249
that only partially use the structure or do not use it at all as follows. 250
The linear diffusion model in Section 3.1 can be transformed into a Bayesian linear model ( LinTS ) 251
by marginalizing out the latent parameters; in which case the prior on action parameters becomes 252
θ∗,i∼ N (0,Σ), with the θ∗,ibeing not necessarily independent, and Σis the marginal initial 253
covariance of action parameters and it writes Σ =σ2
1Id+PL
ℓ=1σ2
ℓ+1BℓB⊤
ℓwithBℓ=Qℓ
k=1Wk. 254
Then, it is tempting to directly apply LinTS to solve our problem. This approach will induce 255
higher regret because the additional uncertainty of the latent parameters is accounted for in Σ 256
despite integrating them. This causes the marginal action uncertainty Σto be much higher than the 257
conditional action uncertainty σ2
1Idin(3.1) , since we have Σ =σ2
1Id+PL
ℓ=1σ2
ℓ+1BℓB⊤
ℓ≽σ2
1Id. 258
This discrepancy leads to higher regret, especially when Kis large. This is due to LinTS needing to 259
learn Kindependent d-dimensional parameters, each with a considerably higher initial covariance Σ. 260
This is also reflected by our regret bound. To simply comparisons, suppose that σ≥max ℓ∈[L+1]σℓ 261
so that σ2
MAX≤2. Then the regret bounds of dTS(where we bound σ2ℓ
MAXby2ℓ) and LinTS read 262
dTS:˜O q
n(dKσ2
1+PL
ℓ=1dℓσ2
ℓ+12ℓ)
, LinTS :˜O q
ndK(σ2
1+PL
ℓ=1σ2
ℓ+1)
.
Then regret improvements are captured by the variances σℓand the sparsity dimensions dℓ, and we 263
proceed to illustrate this through the following scenarios. 264
(I) Decreasing variances. Assume that σℓ= 2ℓfor any ℓ∈[L+ 1]. Then, the regrets become 265
dTS:˜O q
n(dK+PL
ℓ=1dℓ4ℓ))
, LinTS :˜O p
ndK2L)
Now to see the order of gain, assume the problem is high-dimensional ( d≫1), and set L= log2(d) 266
anddℓ=⌊d
2ℓ⌋. Then the regret of dTSbecomes ˜O p
nd(K+L))
, and hence the multiplicative 267
factor 2LinLinTS is removed and replaced with a smaller additive factor L. 268
(II) Constant variances. Assume that σℓ= 1for any ℓ∈[L+ 1]. Then, the regrets become 269
dTS:˜O q
n(dK+PL
ℓ=1dℓ2ℓ))
, LinTS :˜O p
ndKL )
Similarly, let L= log2(d), and dℓ=⌊d
2ℓ⌋. Then dTS’s regret is ˜O p
nd(K+L)
. Thus the 270
multiplicative factor LinLinTS is removed and replaced with the additive factor L. By comparing 271
this to (I), the gain with decreasing variances is greater than with constant ones. In general, diffusion 272
models use decreasing variances [Ho et al., 2020] and hence we expect great gains in practice. 273
All observed improvements in this section could become even more pronounced when employing 274
non-linear diffusion models. In our current analysis, we used linear diffusion models, and yet we can 275
already discern substantial differences. Moreover, under non-linear diffusion (1), the latent parameters 276
cannot be analytically marginalized, making LinTS with exact marginalization inapplicable. Finally, 277
Appendix D.7 provide an additional comparison and connection to hierarchies with two levels. 278
Large action space aspect. dTS’s regret bound scales with Kσ2
1instead of KP
ℓσ2
ℓ, particularly 279
beneficial when σ1is small, as often seen in diffusion models. Our regret bound and experiments 280
show that dTS outperforms LinTS more distinctly when the action space becomes larger. Prior 281
studies [Foster et al., 2020, Xu and Zeevi, 2020, Zhu et al., 2022] proposed bandit algorithms that 282
do not scale with K. However, our setting differs significantly from theirs, explaining our inherent 283
dependency on Kwhen σ1>0. Precisely, they assume a reward function of r(x, i;θ∗) =ϕ(x, i)⊤θ∗, 284
with a shared θ∗∈Rdand a known mapping ϕ. In contrast, we consider r(x, i;θ∗) =x⊤θ∗,i, with 285
θ∗= (θ∗,i)i∈[K]∈RdK, requiring the learning of Kseparate d-dimensional action parameters. 286
In their setting, with the availability of ϕ, the regret of dTS would similarly be independent of 287
K. However, obtaining such a mapping ϕcan be challenging as it needs to encapsulate complex 288
context-action dependencies. Notably, our setting reflects a common practical scenario, such as in 289
recommendation systems where each product is often represented by its unique embedding. 290
5 Experiments 291
We evaluate dTSusing synthetic data, to validate our theory and test dTSin large action spaces. We 292
omit semi-synthetic data [Riquelme et al., 2018] as they often result in small action spaces. This 293
701000 2000 3000 4000 50000123456789Regret1e3Linear diffusion, linear reward 
    K=100, L=2, d=5
dTS-LL
HierTS
LinTS
LinUCB
01000 2000 3000 4000 50000.00.20.40.60.81.01.21.41.61.81e3Linear diffusion, nonlinear reward 
    K=100, L=2, d=5
dTS-LN
dTS-LL
GLM-TS
UCB-GLM
01000 2000 3000 4000 50000.00.20.40.60.81.01e4Nonlinear diffusion, linear reward 
    K=100, L=2, d=5
dTS-NL
LinTS
LinUCB
01000 2000 3000 4000 50000.00.20.40.60.81.01.21.41.61e3Nonlinear diffusion, nonlinear reward 
    K=100, L=2, d=5
dTS-NN
dTS-NL
GLM-TS
UCB-GLM
01000 2000 3000 4000 5000
Round t2[n]0.00.51.01.52.02.53.03.54.0Regret1e5    K=10000, L=4, d=20
dTS-LL
HierTS
LinTS
LinUCB
01000 2000 3000 4000 5000
Round t2[n]0.00.51.01.52.02.53.01e3    K=10000, L=4, d=20
dTS-LN
dTS-LL
GLM-TS
UCB-GLM
01000 2000 3000 4000 5000
Round t2[n]0.00.51.01.52.02.53.03.54.01e7    K=10000, L=4, d=20
dTS-NL
LinTS
LinUCB
01000 2000 3000 4000 5000
Round t2[n]0123456781e2    K=10000, L=4, d=20
dTS-NN
dTS-NL
GLM-TS
UCB-GLMFigure 2: Regret of dTSwith varying diffusion and reward models and varying parameters d,K,L.
choice is further justified by the fact that Hsieh et al. [2023] has already demonstrated the advantages 294
of diffusion models in multi-armed bandits using such data, without theoretical guarantees. 295
5.1 Settings and baselines 296
We run 50 random simulations and plot the average regret with its standard error. We consider both 297
linear and non-linear rewards. The distribution of linear rewards is P(· |x;θa) =N(x⊤θa, σ2)with 298
σ= 1. The non-linear rewards are binary and generated from P(· |x;θa) = Ber( g(x⊤θa))), where 299
gis the sigmoid function. The covariances are Σℓ=Id, and the context Xtis uniformly drawn from 300
[−1,1]d. We vary d∈ {5,20},L∈ {2,4}andK∈ {102,104}. We set the horizon n= 5000 . 301
Linear diffusion. We consider the linear diffusion model in (3.1) where score functions are linear as 302
fℓ(ψ) = W ℓψwhere Wℓare uniformly drawn from [−1,1]d×d. To introduce sparsity, we zero out 303
the last dℓcolumns of Wℓ, resulting in Wℓ= (¯Wℓ,0d,d−dℓ), where (d1, d2) = (5 ,2)when d= 5 304
andL= 2and(d1, d2, d3, d4) = (20 ,10,5,2)when d= 20 andL= 4. 305
Non-linear diffusion. We consider the general diffusion model in (1)with score functions fℓdefined 306
by two-layer neural networks with random weights in [−1,1],ReLU activation, and a hidden layer 307
dimension of h= 20 when d= 5andh= 60 when d= 20 . 308
Baselines. When rewards are linear, we use LinUCB [Abbasi-Yadkori et al., 2011], LinTS [Agrawal 309
and Goyal, 2013a], and HierTS [Hong et al., 2022b] that marginalizes out all latent parameters 310
except ψ∗,L. This corresponds to HierTS-1 in Appendix D.7. When rewards are non-linear, we 311
include UCB-GLM [Li et al., 2017], and GLM-TS [Chapelle and Li, 2012]. GLM-UCB [Filippi et al., 312
2010] induced high regret while HierTS was designed for linear rewards only and thus both are not 313
included. We name dTSfor each setting as dTS-dr , where the suffix dindicates the type of diffusion; 314
Lfor linear and Nfor non-linear. The suffix rindicates the type of rewards; Lfor linear and Nfor 315
non-linear. For instance, dTS-LL signifies dTSin linear diffusion (Section 3.1) with linear rewards. 316
5.2 Results and interpretations 317
Results are shown in Fig. 2 and we make the following observations: 318
1)dTShas better performance. dTSoutperforms the baselines. First, when both the diffusion and 319
rewards are linear, dTS-LL consistently outperforms all baselines that disregard the latent structure 320
(LinTS andLinUCB ) or incorporate it only partially ( HierTS ). Second, when the diffusion is linear 321
and rewards are non-linear, dTS-LN surpasses all baselines. Third, when the diffusion is non-linear 322
and rewards are linear, dTS-NL demonstrates significant performance gains compared to both LinTS 323
andLinUCB . With non-linear diffusion and rewards, dTS-NN surpasses both GLM-TS andUCB-GLM . 324
2) Latent diffusion structure may be more important than the reward distribution. When 325
rewards are non-linear (second and fourth columns in Fig. 2), we included variants of dTSthat use 326
the correct diffusion prior but the wrong reward distribution, employing linear-Gaussian instead of 327
logistic-Bernoulli ( dTS-LL in the second column and dTS-NL in the fourth column). In both cases, 328
despite the misspecification of the reward distribution, these variants outperform models that use the 329
correct reward distribution but neglect the latent diffusion structure, such as GLM-TS andUCB-GLM . 330
8This underscores the significance of accounting for the latent structure, which can sometimes be more 331
crucial than having an accurate reward distribution. Also, the performance gap between dTS-NL 332
(non-linear diffusion) and GLM-TS andUCB-GLM is even more pronounced compared to the gap 333
between dTS-LL (linear diffusion) and these baselines, possibly due to the increased complexity of 334
the latent structure, in the non-linear diffusion, overshadowing the impact of the reward model itself. 335
0 1000 2000 3000 4000 5000
Round t2[n]05001000150020002500RegretEffect of prior misspecification
LindTS (v=0.5)
LindTS (v=1)
LindTS (v=1.5)
LindTS
HierTS
Figure 3: Prior misspecification effect.3) Prior misspecification (Fig. 3). We consider a scenario 336
where the prior used by dTSdoes not match the true prior. 337
To simulate this, we use our setting with linear diffusion 338
and rewards above, but the true parameters WℓandΣℓare 339
replaced by misspecified parameters Wℓ+ϵ1andΣℓ+ϵ2. 340
Here, ϵ1andϵ2are sampled uniformly from [v, v+0.5]d×d, 341
withvcontrolling the level of misspecification. The higher 342
the value of v, the greater the misspecification. We vary 343
v∈ {0.5,1,1.5}and analyze its impact on dTS’s perfor- 344
mance. For comparison, we include the well-specified 345
dTS-LL and the most competitive baseline, HierTS . Re- 346
sults are shown in Fig. 3. As expected, dTS’s performance 347
decreases with increasing misspecification. However, even 348
with misspecification, dTSoutperforms the most competitive baseline, except when v= 1.5, where 349
their performances are comparable. Note that the entries of the true parameters WℓandΣℓare smaller 350
than 1, so values of v∈ {0.5,1,1.5}can lead to significant parameter misspecification. Yet, the 351
performance of dTSwith misspecified prior parameters remains favorable, suggesting that even an 352
imperfect pre-trained diffusion model can be beneficial when used as prior. 353
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Increasing values of either K;d or L0500100015002000250030003500RegretEffect of K;d;L on the regret of LindTS
Increasing K
Increasing d
Increasing L
Figure 4: dTS-LL ’s regret scaling.4) Regret scaling with K,dandLmatches our theory 354
(Fig. 4). We verify the impact of the number of actions 355
K, the context dimension d, and the diffusion depth L 356
on the regret of dTS. We maintain the same experimental 357
setup with linear diffusion and rewards, for which we have 358
derived a Bayes regret upper bound. In Fig. 4, we plot 359
the regret of dTS-LL across varying values of these pa- 360
rameters: K∈ {10,100,500,1000},d∈ {5,10,15,20}, 361
andL∈ {2,4,5,6}. As anticipated and aligned with our 362
theory, the empirical regret increases as the values of K,d, 363
orLgrow. This trend arises because larger values of K,d, 364
orLresult in problem instances that are more challenging 365
to learn, consequently leading to higher regret. 366
10 100 500 5000 50000
Number of actions K0.00.20.40.60.81.01.21.41.61.8Regret in round n1e4 Regret as a function of K
dTS-LL
LinTS
Figure 5: Regret of dTS-LL andLinTS
with varying K.5) Performance gap between dTS and LinTS widens 367
asKincreases (Fig. 5). To showcase dTS’s improved 368
scalability to larger action spaces, we examine its perfor- 369
mance across a range of Kvalues, from 10to50,000, 370
in our setting with linear diffusion and rewards. Fig. 5 371
reports the final cumulative regret for varying values of K 372
for both dTS-LL andLinTS , observing that the gap in the 373
performance becomes larger as Kincreases. 374
6 Conclusion 375
Grappling with large action spaces in contextual bandits is challenging. Recognizing this, we focused 376
on structured problems where action parameters are sampled from a diffusion model; upon which we 377
built diffusion Thompson sampling ( dTS). We developed both theoretical and algorithmic foundations 378
fordTSin numerous practical settings. We identified several directions for future work. Exploring 379
other approximations for non-linear diffusion models, both empirically and theoretically. From a 380
theoretical perspective, future research could explore the advantages of non-linear diffusion models 381
by deriving their Bayes regret bounds, akin to our analysis in Section 4. Empirically, investigating 382
our and other approximations in complex tasks would be interesting. Additionally, exploring the 383
extension of this work to offline (or off-policy) learning in contextual bandits [Swaminathan and 384
Joachims, 2015, Aouali et al., 2023a] represents a promising avenue for future research. 385
9References 386
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic 387
bandits. In Advances in Neural Information Processing Systems 24 , pages 2312–2320, 2011. 388
Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Proceedings of the 389
20th International Conference on Artificial Intelligence and Statistics , 2017. 390
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In 391
Proceedings of the 30th International Conference on Machine Learning , pages 127–135, 2013a. 392
Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In 393
Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics , 394
pages 99–107, 2013b. 395
Shipra Agrawal and Navin Goyal. Near-optimal regret bounds for thompson sampling. Journal of 396
the ACM (JACM) , 64(5):1–24, 2017. 397
Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is con- 398
ditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657 , 399
2022. 400
Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Exponential smoothing for 401
off-policy learning. In International Conference on Machine Learning , pages 984–1017. PMLR, 402
2023a. 403
Imad Aouali, Branislav Kveton, and Sumeet Katariya. Mixed-effect thompson sampling. In Interna- 404
tional Conference on Artificial Intelligence and Statistics , pages 2087–2115. PMLR, 2023b. 405
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit 406
problem. Machine Learning , 47:235–256, 2002. 407
Mohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Sequential transfer in 408
multi-armed bandit with finite set of models. In Advances in Neural Information Processing 409
Systems 26 , pages 2220–2228, 2013. 410
Hamsa Bastani, David Simchi-Levi, and Ruihao Zhu. Meta dynamic pricing: Transfer learning across 411
experiments. CoRR , abs/1902.10918, 2019. URL https://arxiv.org/abs/1902.10918 . 412
Soumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvari. No regrets for learning the 413
prior in bandits. In Advances in Neural Information Processing Systems 34 , 2021. 414
Christopher M Bishop. Pattern Recognition and Machine Learning , volume 4 of Information science 415
and statistics . Springer, 2006. 416
Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear 417
bandits. In Proceedings of the 37th International Conference on Machine Learning , 2020. 418
Leonardo Cella, Karim Lounici, and Massimiliano Pontil. Multi-task representation learning with 419
stochastic linear bandits. arXiv preprint arXiv:2202.10066 , 2022. 420
Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in 421
Neural Information Processing Systems 24 , pages 2249–2257, 2012. 422
Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion 423
posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687 , 2022. 424
Aniket Anand Deshmukh, Urun Dogan, and Clayton Scott. Multi-task learning for contextual bandits. 425
InAdvances in Neural Information Processing Systems 30 , pages 4848–4856, 2017. 426
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances 427
in neural information processing systems , 34:8780–8794, 2021. 428
Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The 429
generalized linear case. In Advances in Neural Information Processing Systems 23 , pages 586–594, 430
2010. 431
10Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in 432
contextual bandits. Advances in Neural Information Processing Systems , 33:11478–11489, 2020. 433
Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. In Proceedings of 434
the 31st International Conference on Machine Learning , pages 757–765, 2014. 435
Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online 436
problems. In Proceedings of the 31st International Conference on Machine Learning , pages 437
100–108, 2014. 438
Samarth Gupta, Shreyas Chaudhari, Subhojyoti Mukherjee, Gauri Joshi, and Osman Yagan. A 439
unified approach to translate classical bandit algorithms to the structured bandit setting. CoRR , 440
abs/1810.08164, 2018. URL https://arxiv.org/abs/1810.08164 . 441
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in 442
neural information processing systems , 33:6840–6851, 2020. 443
Joey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed, and Craig Boutilier. Latent 444
bandits revisited. In Advances in Neural Information Processing Systems 33 , 2020. 445
Joey Hong, Branislav Kveton, Sumeet Katariya, Manzil Zaheer, and Mohammad Ghavamzadeh. 446
Deep hierarchy in bandits. In International Conference on Machine Learning , pages 8833–8851. 447
PMLR, 2022a. 448
Joey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical Bayesian 449
bandits. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics , 450
2022b. 451
Yu-Guan Hsieh, Shiva Prasad Kasiviswanathan, Branislav Kveton, and Patrick Blöbaum. Thompson 452
sampling with diffusion generative prior. arXiv preprint arXiv:2301.05182 , 2023. 453
Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, and Liwei Wang. Near-optimal representation learning 454
for linear bandits and linear rl. In International Conference on Machine Learning , pages 4349–4358. 455
PMLR, 2021. 456
Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for 457
flexible behavior synthesis. arXiv preprint arXiv:2205.09991 , 2022. 458
Emilie Kaufmann, Nathaniel Korda, and Rémi Munos. Thompson sampling: An asymptotically 459
optimal finite-time analysis. In International conference on algorithmic learning theory , pages 460
199–213. Springer, 2012. 461
Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques . MIT 462
Press, Cambridge, MA, 2009. 463
Nathaniel Korda, Emilie Kaufmann, and Remi Munos. Thompson sampling for 1-dimensional 464
exponential family bandits. Advances in neural information processing systems , 26, 2013. 465
John K Kruschke. Bayesian data analysis. Wiley Interdisciplinary Reviews: Cognitive Science , 1(5): 466
658–676, 2010. 467
Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and 468
Craig Boutilier. Randomized exploration in generalized linear bandits. In International Conference 469
on Artificial Intelligence and Statistics , pages 2066–2076. PMLR, 2020. 470
Branislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-Wei Hsu, Martin Mladenov, Craig 471
Boutilier, and Csaba Szepesvari. Meta-Thompson sampling. In Proceedings of the 38th Interna- 472
tional Conference on Machine Learning , 2021. 473
Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of 474
Statistics , 15(3):1091–1114, 1987. 475
Tor Lattimore and Remi Munos. Bounded regret for finite-armed structured bandits. In Advances in 476
Neural Information Processing Systems 27 , pages 550–558, 2014. 477
11Lihong Li, Wei Chu, John Langford, and Robert Schapire. A contextual-bandit approach to personal- 478
ized news article recommendation. In Proceedings of the 19th International Conference on World 479
Wide Web , 2010. 480
Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual 481
bandits. In Proceedings of the 34th International Conference on Machine Learning , pages 2071– 482
2080, 2017. 483
Dennis Lindley and Adrian Smith. Bayes estimates for the linear model. Journal of the Royal 484
Statistical Society: Series B (Methodological) , 34(1):1–18, 1972. 485
Xiuyuan Lu and Benjamin Van Roy. Information-theoretic confidence bounds for reinforcement 486
learning. In Advances in Neural Information Processing Systems 32 , 2019. 487
Odalric-Ambrym Maillard and Shie Mannor. Latent bandits. In Proceedings of the 31st International 488
Conference on Machine Learning , pages 136–144, 2014. 489
P. McCullagh and J. A. Nelder. Generalized Linear Models . Chapman & Hall, 1989. 490
Amit Peleg, Naama Pearl, and Ron Meirr. Metalearning linear bandits by prior update. In Proceedings 491
of the 25th International Conference on Artificial Intelligence and Statistics , 2022. 492
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical 493
comparison of bayesian deep networks for thompson sampling. arXiv preprint arXiv:1802.09127 , 494
2018. 495
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High- 496
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer- 497
ence on computer vision and pattern recognition , pages 10684–10695, 2022. 498
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of 499
Operations Research , 39(4):1221–1243, 2014. 500
Steven Scott. A modern bayesian look at the multi-armed bandit. Applied Stochastic Models in 501
Business and Industry , 26:639 – 658, 2010. 502
Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro 503
Dudik, and Robert Schapire. Bayesian decision-making under misspecified priors with applications 504
to meta-learning. In Advances in Neural Information Processing Systems 34 , 2021. 505
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised 506
learning using nonequilibrium thermodynamics. In International conference on machine learning , 507
pages 2256–2265. PMLR, 2015. 508
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged 509
bandit feedback. In International Conference on Machine Learning , pages 814–823. PMLR, 2015. 510
Runzhe Wan, Lin Ge, and Rui Song. Metadata-based multi-task bandits with Bayesian hierarchical 511
models. In Advances in Neural Information Processing Systems 34 , 2021. 512
Runzhe Wan, Lin Ge, and Rui Song. Towards scalable and robust structured bandits: A meta-learning 513
framework. CoRR , abs/2202.13227, 2022. URL https://arxiv.org/abs/2202.13227 . 514
Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy 515
class for offline reinforcement learning. arXiv preprint arXiv:2208.06193 , 2022. 516
Neil Weiss. A Course in Probability . Addison-Wesley, 2005. 517
Yunbei Xu and Assaf Zeevi. Upper counterfactual confidence bounds: a new optimism principle for 518
contextual bandits. arXiv preprint arXiv:2007.07876 , 2020. 519
Jiaqi Yang, Wei Hu, Jason D Lee, and Simon S Du. Impact of representation learning in linear bandits. 520
arXiv preprint arXiv:2010.06531 , 2020. 521
12Tong Yu, Branislav Kveton, Zheng Wen, Ruiyi Zhang, and Ole Mengshoel. Graphical models meet 522
bandits: A variational Thompson sampling approach. In Proceedings of the 37th International 523
Conference on Machine Learning , 2020. 524
Yinglun Zhu, Dylan J Foster, John Langford, and Paul Mineiro. Contextual bandits with large action 525
spaces: Made practical. In International Conference on Machine Learning , pages 27428–27453. 526
PMLR, 2022. 527
13Supplementary materials 528
Notation. For any positive integer n, we define [n] ={1,2, ..., n}. Letv1, . . . , v n∈Rdbenvectors, 529
(vi)i∈[n]∈Rndis the nd-dimensional vector obtained by concatenating v1, . . . , v n. For any matrix 530
A∈Rd×d,λ1(A)andλd(A)denote the maximum and minimum eigenvalues of A, respectively. 531
Finally, we write ˜Ofor the big-O notation up to polylogarithmic factors. 532
A Extended related work 533
Thompson sampling (TS) operates within the Bayesian framework and it involves specifying a 534
prior/likelihood model. In each round, the agent samples unknown model parameters from the 535
current posterior distribution. The chosen action is the one that maximizes the resulting reward. TS 536
is naturally randomized, particularly simple to implement, and has highly competitive empirical 537
performance in both simulated and real-world problems [Russo and Van Roy, 2014, Chapelle and Li, 538
2012]. Regret guarantees for the TS heuristic remained open for decades even for simple models. 539
Recently, however, significant progress has been made. For standard multi-armed bandits, TS is 540
optimal in the Beta-Bernoulli model [Kaufmann et al., 2012, Agrawal and Goyal, 2013b], Gaussian- 541
Gaussian model [Agrawal and Goyal, 2013b], and in the exponential family using Jeffrey’s prior 542
[Korda et al., 2013]. For linear bandits, TS is nearly-optimal [Russo and Van Roy, 2014, Agrawal and 543
Goyal, 2017, Abeille and Lazaric, 2017]. In this work, we build TS upon complex diffusion priors 544
and analyze the resulting Bayes regret [Russo and Van Roy, 2014] in the linear contextual bandit 545
setting. 546
Decision-making with diffusion models gained attention recently, especially in offline learning 547
[Ajay et al., 2022, Janner et al., 2022, Wang et al., 2022]. However, their application in online 548
learning was only examined by Hsieh et al. [2023], which focused on meta-learning in multi-armed 549
bandits without theoretical guarantees. In this work, we expand the scope of Hsieh et al. [2023] to 550
encompass the broader contextual bandit framework. In particular, we provide theoretical analysis for 551
linear instances, effectively capturing the advantages of using diffusion models as priors in contextual 552
Thompson sampling. These linear cases are particularly captivating due to closed-form posteriors, 553
enabling both theoretical analysis and computational efficiency; an important practical consideration. 554
Hierarchical Bayesian bandits [Bastani et al., 2019, Kveton et al., 2021, Basu et al., 2021, Sim- 555
chowitz et al., 2021, Wan et al., 2021, Hong et al., 2022b, Peleg et al., 2022, Wan et al., 2022, Aouali 556
et al., 2023b] applied TS to simple graphical models, wherein action parameters are generally sampled 557
from a Gaussian distribution centered at a single latent parameter. These works mostly span meta- 558
and multi-task learning for multi-armed bandits, except in cases such as Aouali et al. [2023b], Hong 559
et al. [2022a] that consider the contextual bandit setting. Precisely, Aouali et al. [2023b] assume that 560
action parameters are sampled from a Gaussian distribution centered at a linear mixture of multiple 561
latent parameters. On the other hand, Hong et al. [2022a] applied TS to a graphical model represented 562
by a tree. Our work can be seen as an extension of all these works to much more complex graphical 563
models, for which both theoretical and algorithmic foundations are developed. Note that the settings 564
in most of these works can be recovered with specific choices of the diffusion depth Land functions 565
fℓ. This attests to the modeling power of dTS. 566
Approximate Thompson sampling is a major problem in the Bayesian inference literature. This is 567
because most posterior distributions are intractable, and thus practitioners must resort to sophisti- 568
cated computational techniques such as Markov chain Monte Carlo [Kruschke, 2010]. Prior works 569
[Riquelme et al., 2018, Chapelle and Li, 2012, Kveton et al., 2020] highlight the favorable empirical 570
performance of approximate Thompson sampling. Particularly, [Kveton et al., 2020] provide the- 571
oretical guarantees for Thompson sampling when using the Laplace approximation in generalized 572
linear bandits (GLB). In our context, we incorporate approximate sampling when the reward exhibits 573
non-linearity. While our approximation does not come with formal guarantees, it enjoys strong 574
practical performance. An in-depth analysis of this approximation is left as a direction for future 575
works. Similarly, approximating the posterior distribution when the diffusion model is non-linear as 576
well as analyzing it is an interesting direction of future works. 577
Bandits with underlying structure also align with our work, where we assume a structured relation- 578
ship among actions, captured by a diffusion model. In latent bandits [Maillard and Mannor, 2014, 579
Hong et al., 2020], a single latent variable indexes multiple candidate models. Within structured 580
14finite-armed bandits [Lattimore and Munos, 2014, Gupta et al., 2018], each action is linked to a known 581
mean function parameterized by a common latent parameter. This latent parameter is learned. TS 582
was also applied to complex structures [Yu et al., 2020, Gopalan et al., 2014]. However, simultaneous 583
computational and statistical efficiencies aren’t guaranteed. Meta- and multi-task learning with 584
upper confidence bound (UCB) approaches have a long history in bandits [Azar et al., 2013, Gentile 585
et al., 2014, Deshmukh et al., 2017, Cella et al., 2020]. These, however, often adopt a frequentist 586
perspective, analyze a stronger form of regret, and sometimes result in conservative algorithms. 587
In contrast, our approach is Bayesian, with analysis centered on Bayes regret. Remarkably, our 588
algorithm, dTS, performs well as analyzed without necessitating additional tuning. Finally, Low-rank 589
bandits [Hu et al., 2021, Cella et al., 2022, Yang et al., 2020] also relate to our linear diffusion model 590
when L= 1. Broadly, there exist two key distinctions between these prior works and the special 591
case of our model (linear diffusion model with L= 1). First, they assume θ∗,i= W 1ψ∗,1, whereas 592
we incorporate additional uncertainty in the covariance Σ1to account for possible misspecification 593
asθ∗,i=N(W1ψ∗,1,Σ1). Consequently, these algorithms might suffer linear regret due to model 594
misalignment. Second, we assume that the mixing matrix W1is available and pre-learned offline, 595
whereas they learn it online. While this is more general, it leads to computationally expensive 596
methods that are difficult to employ in a real-world online setting. 597
Large action spaces. Roughly speaking, the regret bound of dTS scales with Kσ2
1rather than 598
KP
ℓσ2
ℓ. This is particularly beneficial when σ1is small, a common scenario in diffusion models 599
with decreasing variances. A notable case is when σ1= 0, where the regret becomes independent of 600
K. Also, our analysis (Section 4.1) indicates that the gap in performance between dTSandLinTS 601
becomes more pronounced when the number of action increases, highlighting dTS’s suitability for 602
large action spaces. Note that some prior works [Foster et al., 2020, Xu and Zeevi, 2020, Zhu et al., 603
2022] proposed bandit algorithms that do not scale with K. However, our setting differs significantly 604
from theirs, explaining our inherent dependency on Kwhen σ1>0. Precisely, they assume a 605
reward function of r(x, i) =ϕ(x, i)⊤θ∗, with a shared θ∗∈Rdacross actions and a known mapping 606
ϕ. In contrast, we consider r(x, i) =x⊤θ∗,i, requiring the learning of Kseparate d-dimensional 607
action parameters. In their setting, with the availability of ϕ, the regret of dTSwould similarly be 608
independent of K. However, obtaining such a mapping ϕcan be challenging as it needs to encapsulate 609
complex context-action dependencies. Notably, our setting reflects a common practical scenario, 610
such as in recommendation systems where each product is often represented by its embedding. In 611
summary, the dependency on Kis more related to our setting than the method itself, and dTSwould 612
scale with donly in their setting. Note that dTSis both computationally and statistically efficient 613
(Section 4.1). This becomes particularly notable in large action spaces. Our empirical results in 614
Fig. 2, notably with K= 104, demonstrate that dTSsignificantly outperforms the baselines. More 615
importantly, the performance gap between dTSand these baselines is larger when the number of 616
actions ( K) increases, highlighting the improved scalability of dTSto large action spaces. 617
B Posterior derivations for linear diffusion models 618
Here, we assume the score functions fℓare linear such as fℓ(ψ∗,ℓ) = W ℓψ∗,ℓforℓ∈[L], where 619
Wℓ∈Rd×dareknown mixing matrices . Then, (1)becomes a linear Gaussian system (LGS) [Bishop, 620
2006] and can be summarized as follows 621
ψ∗,L∼ N(0,ΣL+1), (15)
ψ∗,ℓ−1|ψ∗,ℓ∼ N(Wℓψ∗,ℓ,Σℓ), ∀ℓ∈[L]/{1},
θ∗,i|ψ∗,1∼ N(W1ψ∗,1,Σ1), ∀i∈[K],
Yt|Xt, θ∗,At∼P(· |Xt;θ∗,At), ∀t∈[n].
In this section, we derive the K+Lposteriors Pt,iandQt,ℓ, for which we provide the full expressions 622
in Appendix B.1. In our proofs, p(x)∝f(x)means that the probability density psatisfies p(x) = 623
f(x)
Zfor any x∈Rd, where Zis a normalization constant. In particular, we extensively use that if 624
p(x)∝exp[−1
2x⊤Λx+x⊤m], where Λis positive definite. Then pis the multivariate Gaussian 625
density with covariance Σ = Λ−1and mean µ= Σm. These are standard notations and techniques 626
to manipulate Gaussian distributions [Koller and Friedman, 2009, Chapter 7]. 627
15B.1 Posterior expressions for linear diffusion models 628
Recall that we posit that the reward distribution is parameterized as a generalized linear model (GLM) 629
[McCullagh and Nelder, 1989], allowing for non-linear rewards. As a result, despite linearity in 630
score functions, the non-linearity in rewards makes it challenging to obtain closed-form posteriors. 631
However, since this non-linearity arises solely from the reward distribution, we approximate it using 632
a Gaussian distribution. This leads to efficient posterior approximations that are exact in cases where 633
the reward function is indeed Gaussian (a special case of the GLM model). Precisely, the reward 634
distribution P(· |x;θ)is an exponential-family distribution. Therefore, the log-likelihoods write 635
logP(Ht,i|θ∗,i=θ) =P
k∈St,iYkX⊤
kθ−A(X⊤
kθ) +C(Yk), where Cis a real function, and A 636
is a twice continuously differentiable function whose derivative is the mean function, ˙A=g. Now 637
we let ˆBt,iandˆGt,ibe the maximum likelihood estimate (MLE) and the Hessian of the negative 638
log-likelihood, respectively, defined as 639
ˆBt,i= arg max
θ∈RdlogP(Ht,i|θ∗,i=θ), ˆGt,i=X
k∈St,i˙g 
X⊤
kˆBt,i
XkX⊤
k. (16)
where St,i={ℓ∈[t−1] :Aℓ=i}are the rounds where the agent takes action iup to round t. 640
Then we approximation the respective likelihood as P(Ht,i|θ∗,i=θ)≈ N 
θ;ˆBt,i,ˆG−1
t,i
. This 641
approximation makes all posteriors Gaussian. First, the conditional action-posterior reads Pt,i(· | 642
ψ1) =N(·; ˆµt,i,ˆΣt,i), 643
ˆΣ−1
t,i= Σ−1
1+ˆGt,i ˆµt,i=ˆΣt,i 
Σ−1
1W1ψ1+ˆGt,iˆBt,i
. (17)
Forℓ∈[L]/{1}, theℓ−1-th conditional latent-posterior is Qt,ℓ−1(· |ψℓ) =N(¯µt,ℓ−1,¯Σt,ℓ−1), 644
¯Σ−1
t,ℓ−1= Σ−1
ℓ+¯Gt,ℓ−1, ¯µt,ℓ−1=¯Σt,ℓ−1 
Σ−1
ℓWℓψℓ+¯Bt,ℓ−1
, (18)
and the L-th latent-posterior is Qt,L(·) =N(¯µt,L,¯Σt,L), 645
¯Σ−1
t,L= Σ−1
L+1+¯Gt,L, ¯µt,L=¯Σt,L¯Bt,L. (19)
Finally, ¯Gt,ℓand¯Bt,ℓforℓ∈[L]are computed recursively. The basis of the recursion are 646
¯Gt,1= W⊤
1KX
i=1 
Σ−1
1−Σ−1
1ˆΣt,iΣ−1
1
W1, ¯Bt,1= W⊤
1Σ−1
1KX
i=1ˆΣt,iˆGt,iˆBt,i. (20)
Then, the recursive step for ℓ∈[L]/{1}is, 647
¯Gt,ℓ= W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
Wℓ, ¯Bt,ℓ= W⊤
ℓΣ−1
ℓ¯Σt,ℓ−1¯Bt,ℓ−1. (21)
This concludes the derivation of our posterior approximation. Note that these approximations are exact 648
when the reward distribution follows a linear-Gaussian model, P(· |x;θ∗,a) =N(·;x⊤θ∗,a, σ2). 649
B.2 Derivation of Action-Posteriors for Linear Diffusion Models 650
To simplify derivations, we consider the case where the reward distribution is indeed linear- 651
Gaussian as P(· |Xt;θ∗,At) =N 
X⊤
tθ∗,At, σ2
, but the same derivations can be applied when 652
the rewards are non-linear. In this case, the likelihood approximation in (16) becomes exact as 653
we have that P(Ht,i|θ∗,i=θ)∝ N 
θ;ˆBt,i,ˆG−1
t,i
, where ˆBt,iis the corresponding MLE and 654
ˆGt,i=σ−2P
k∈St,iXkX⊤
kin this case. Our derivations rely on the fact that the MLE ˆBt,iin this 655
linear-Gaussian case satisfies: ˆGt,iˆBt,i=vP
k∈St,iXkY⊤
k. 656
Proposition B.1. Consider the following model, which corresponds to the last two layers in Eq. (15) 657
θ∗,i|ψ∗,1∼ N (W1ψ∗,1,Σ1),
Yt|Xt, θ∗,At∼ N 
X⊤
tθ∗,At, σ2
, ∀t∈[n].
Then we have that for any t∈[n]andi∈[K],Pt,i(θ|ψ1) =P(θ∗,i=θ|ψ∗,1=ψ1, Ht,i) = 658
N(θ; ˆµt,i,ˆΣt,i), where 659
ˆΣ−1
t,i=ˆGt,i+ Σ−1
1, ˆµt,i=ˆΣt,i
ˆGt,iˆBt,i+ Σ−1
1W1ψ1
.
16Proof. Letv=σ−2,Λ1= Σ−1
1.Then the action-posterior decomposes as 660
Pt,i(θ|ψ1) =P(θ∗,i=θ|ψ∗,1=ψ1, Ht,i),
∝P(Ht,i|ψ∗,1=ψ1, θ∗,i=θ)P(θ∗,i=θ|ψ∗,1=ψ1),(Bayes rule )
=P(Ht,i|θ∗,i=θ)P(θ∗,i=θ|ψ∗,1=ψ1),(given θ∗,i,Ht,iis independent of ψ∗,1)
=Y
k∈St,iN(Yk;X⊤
kθ, σ2)N(θ; W1ψ1,Σ1),
= exph
−1
2
vX
k∈St,i(Y2
k−2YkX⊤
kθ+ (X⊤
kθ)2) +θ⊤Λ1θ−2θ⊤Λ1W1ψ1
+ 
W1ψ1⊤Λ1 
W1ψ1i
,
∝exph
−1
2
θ⊤(vX
k∈St,iXkX⊤
k+ Λ1)θ−2θ⊤
vX
k∈St,iXkYk+ Λ1W1ψ1i
,
∝ N 
θ; ˆµt,i,ˆΛ−1
t,i
,
with ˆΛt,i=vP
k∈St,iXkX⊤
k+ Λ 1,ˆΛt,iˆµt,i=vP
k∈St,iXkYk+ Λ 1W1ψ1. Using that, in this 661
linear-Gaussian case, ˆGt,i=vP
k∈St,iXkX⊤
kandˆGt,iˆBt,i=vP
k∈St,iXkYkconcludes the 662
proof. 663
The same proof applies when the reward distribution is not linear-Gaussian, with the approximation 664
P(Ht,i|θ∗,i=θ)≈ N 
θ;ˆBt,i,ˆG−1
t,i
. Using this approximation in the derivations above leads to 665
the same results. 666
B.3 Derivation of recursive latent-posteriors for linear diffusion models 667
Again, to simplify derivations, we consider the case where the reward distribution is indeed linear- 668
Gaussian as P(· |Xt;θ∗,At) =N 
X⊤
tθ∗,At, σ2
, but the same derivations can be applied when the 669
rewards are non-linear. 670
Proposition B.2. For any ℓ∈[L]/{1}, theℓ−1-th conditional latent-posterior reads Qt,ℓ−1(· | 671
ψℓ) =N(¯µt,ℓ−1,¯Σt,ℓ−1), with 672
¯Σ−1
t,ℓ−1= Σ−1
ℓ+¯Gt,ℓ−1, ¯µt,ℓ−1=¯Σt,ℓ−1 
Σ−1
ℓWℓψℓ+¯Bt,ℓ−1
, (22)
and the L-th latent-posterior reads Qt,L(·) =N(¯µt,L,¯Σt,L), with 673
¯Σ−1
t,L= Σ−1
L+1+¯Gt,L, ¯µt,L=¯Σt,L¯Bt,L. (23)
Proof. Letℓ∈[L]/{1}. Then, Bayes rule yields that 674
Qt,ℓ−1(ψℓ−1|ψℓ)∝P(Ht|ψ∗,ℓ−1=ψℓ−1)N(ψℓ−1,Wℓψℓ,Σℓ),
But from Lemma B.3, we know that 675
P(Ht|ψ∗,ℓ−1=ψℓ−1)∝exph
−1
2ψ⊤
ℓ−1¯Gt,ℓ−1ψℓ−1+ψ⊤
ℓ−1¯Bt,ℓ−1i
.
Therefore, 676
Qt,ℓ−1(ψℓ−1|ψℓ)∝exph
−1
2ψ⊤
ℓ−1¯Gt,ℓ−1ψℓ−1+ψ⊤
ℓ−1¯Bt,ℓ−1i
N(ψℓ−1,Wℓψℓ,Σℓ),
∝exph
−1
2ψ⊤
ℓ−1¯Gt,ℓ−1ψℓ−1+ψ⊤
ℓ−1¯Bt,ℓ−1
−1
2(ψℓ−1−Wℓψℓ)⊤Σ−1
ℓ(ψℓ−1−Wℓψℓ))i
,
(i)∝exph
−1
2ψ⊤
ℓ−1(¯Gt,ℓ−1+ Σ−1
ℓ)ψℓ−1+ψ⊤
ℓ−1(¯Bt,ℓ−1+ Σ−1
ℓWℓψℓ)i
,
(ii)∝ N (ψℓ−1; ¯µt,ℓ−1,¯Σt,ℓ−1),
17with¯Σ−1
t,ℓ−1= Σ−1
ℓ+¯Gt,ℓ−1and¯µt,ℓ−1=¯Σt,ℓ−1 
Σ−1
ℓWℓψℓ+¯Bt,ℓ−1
. In(i), we omit terms that 677
are constant in ψℓ−1. In(ii), we complete the square. This concludes the proof for ℓ∈[L]/{1}. For 678
Qt,L, we use Bayes rule to get 679
Qt,L(ψL)∝P(Ht|ψ∗,L=ψL)N(ψL,0,ΣL+1).
Then from Lemma B.3, we know that 680
P(Ht|ψ∗,L=ψL)∝exph
−1
2ψ⊤
L¯Gt,LψL+ψ⊤
L¯Bt,Li
,
We then use the same derivations above to compute the product exph
−1
2ψ⊤
L¯Gt,LψL+ψ⊤
L¯Bt,Li
× 681
N(ψL,0,ΣL+1), which concludes the proof. 682
Lemma B.3. The following holds for any t∈[n]andℓ∈[L], 683
P(Ht|ψ∗,ℓ=ψℓ)∝exph
−1
2ψ⊤
ℓ¯Gt,ℓψℓ+ψ⊤
ℓ¯Bt,ℓi
,
where ¯Gt,ℓand¯Bt,ℓare defined by recursion in Section 3.1. 684
Proof. We prove this result by induction. To reduce clutter, we let v=σ−2, and Λ1= Σ−1
1. We 685
start with the base case of the induction when ℓ= 1. 686
(I) Base case. Here we want to show that P(Ht|ψ∗,1=ψ1)∝exph
−1
2ψ⊤
1¯Gt,1ψ1+ψ⊤
1¯Bt,1i
, 687
where ¯Gt,1and¯Bt,1are given in Eq. (20). First, we have that 688
P(Ht|ψ∗,1=ψ1)(i)=Y
i∈[K]P(Ht,i|ψ∗,1=ψ1) =Y
i∈[K]Z
θP(Ht,i, θ∗,i=θ|ψ∗,1=ψ1) dθ ,
=Y
i∈[K]Z
θP(Ht,i|θ∗,i=θ)N(θ; W1ψ1,Σ1) dθ ,
=Y
i∈[K]Z
θY
k∈St,iN(Yk;X⊤
kθ, σ2)
N(θ; W1ψ1,Σ1) dθ
| {z }
hi(ψ1),
=Y
i∈[K]hi(ψ1), (24)
where (i)follows from the fact that θ∗,ifori∈[K]are conditionally independent given 689
ψ∗,1=ψ1and that given θ∗,i,Ht,iis independent of ψ∗,1. Now we compute hi(ψ1) = 690R
θQ
k∈St,iN(Yk;X⊤
kθ, σ2)
N(θ; W1ψ1,Σ1) dθas 691
hi(ψ1) =Z
θY
k∈St,iN(Yk;X⊤
kθ, σ2)
N(θ; W1ψ1,Σ1) dθ ,
∝Z
θexph
−1
2vX
k∈St,i(Yk−X⊤
kθ)2−1
2(θ−W1ψ1)⊤Λ1(θ−W1ψ1)i
dθ ,
=Z
θexph
−1
2
vX
k∈St,i(Y2
k−2Ykθ⊤Xk+ (θ⊤Xk)2) +θ⊤Λ1θ−2θ⊤Λ1W1ψ1
+ (W 1ψ1)⊤Λ1(W1ψ1)i
dθ ,
∝Z
θexph
−1
2
θ⊤
vX
k∈St,iXkX⊤
k+ Λ1
θ−2θ⊤
vX
k∈St,iYkXk
+ Λ1W1ψ1
+ (W 1ψ1)⊤Λ1(W1ψ1)i
dθ .
18But we know that ˆGt,i=vP
k∈St,iXkX⊤
k, and ˆGt,iˆBt,i=vP
k∈St,iYkXk(because we assumed 692
linear-Gaussian likelihood). To further simplify expressions, we also let 693
V= ˆGt,i+ Λ1−1, U =V−1, β =V ˆGt,iˆBt,i+ Λ1W1ψ1
.
We have that UV=V U=Id,and thus 694
hi(ψ1)∝Z
θexp
−1
2
θ⊤Uθ−2θ⊤UV
ˆGt,iˆBt,i+ Λ1W1ψ1
+ (W 1ψ1)⊤Λ1(W1ψ1)
dθ ,
=Z
θexp
−1
2 
θ⊤Uθ−2θ⊤Uβ+ (W 1ψ1)⊤Λ1(W1ψ1)
dθ ,
=Z
θexp
−1
2 
(θ−β)⊤U(θ−β)−β⊤Uβ+ (W 1ψ1)⊤Λ1(W1ψ1)
dθ ,
∝exp
−1
2 
−β⊤Uβ+ (W 1ψ1)⊤Λ1(W1ψ1)
,
= exp
−1
2
−
ˆGt,iˆBt,i+ Λ1W1ψ1⊤
V
ˆGt,iˆBt,i+ Λ1W1ψ1
+ (W 1ψ1)⊤Λ1(W1ψ1)
,
∝exp
−1
2
ψ⊤
1W⊤
1(Λ1−Λ1VΛ1) W1ψ1−2ψ⊤
1
W⊤
1Λ1VˆGt,iˆBt,i
,
= exp
−1
2ψ⊤
1Ωiψ1+ψ⊤
1mi
,
where 695
Ωi= W⊤
1(Λ1−Λ1VΛ1) W1= W⊤
1
Λ1−Λ1(ˆGt,i+ Λ1)−1Λ1
W1,
mi= W⊤
1Λ1VˆGt,iˆBt,i= W⊤
1Λ1(ˆGt,i+ Λ1)−1ˆGt,iˆBt,i. (25)
But notice that V= (ˆGt,i+ Λ1)−1=ˆΣt,iand thus 696
Ωi= W⊤
1 
Λ1−Λ1ˆΣt,iΛ1
W1, m i= W⊤
1Λ1ˆΣt,iˆGt,iˆBt,i. (26)
Finally, we plug this result in Eq. (24) to get 697
P(Ht|ψ∗,1=ψ1) =Y
i∈[K]hi(ψ1)∝Y
i∈[K]exp
−1
2ψ⊤
1Ωiψ1+ψ⊤
1mi
,
= exp
−1
2ψ⊤
1X
i∈[K]Ωiψ1+ψ⊤
1X
i∈[K]mi
,
= exp
−1
2ψ⊤
1¯Gt,1ψ1+ψ⊤
1¯Bt,1
,
where 698
¯Gt,1=KX
i=1Ωi=KX
i=1W⊤
1 
Λ1−Λ1ˆΣt,iΛ1
W1= W⊤
1KX
i=1 
Σ−1
1−Σ−1
1ˆΣt,iΣ−1
1
W1,
¯Bt,1=KX
i=1mi=KX
i=1ˆΣt,iˆGt,iˆBt,i= W⊤
1Σ−1
1KX
i=1ˆΣt,iˆGt,iˆBt,i.
This concludes the proof of the base case. 699
(II) Induction step. Letℓ∈[L]/{1}. Suppose that 700
P(Ht|ψ∗,ℓ−1=ψℓ−1)∝exp
−1
2ψ⊤
ℓ−1¯Gt,ℓ−1ψℓ−1+ψ⊤
ℓ−1¯Bt,ℓ−1
. (27)
19Then we want to show that 701
P(Ht|ψ∗,ℓ=ψℓ)∝exp
−1
2ψ⊤
ℓ¯Gt,ℓψℓ+ψ⊤
ℓ¯Bt,ℓ
,
where 702
¯Gt,ℓ= W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
Wℓ= W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ(Σ−1
ℓ+¯Gt,ℓ−1)−1Σ−1
ℓ
Wℓ,
¯Bt,ℓ= W⊤
ℓΣ−1
ℓ¯Σt,ℓ−1¯Bt,ℓ−1= W⊤
ℓΣ−1
ℓ(Σ−1
ℓ+¯Gt,ℓ−1)−1¯Bt,ℓ−1.
To achieve this, we start by expressing P(Ht|ψ∗,ℓ=ψℓ)in terms of P(Ht|ψ∗,ℓ−1=ψℓ−1)as 703
P(Ht|ψ∗,ℓ=ψℓ) =Z
ψℓ−1P(Ht, ψ∗,ℓ−1=ψℓ−1|ψ∗,ℓ=ψℓ) dψℓ−1,
=Z
ψℓ−1P(Ht|ψ∗,ℓ−1=ψℓ−1, ψ∗,ℓ=ψℓ)N(ψℓ−1; Wℓψℓ,Σℓ) dψℓ−1,
=Z
ψℓ−1P(Ht|ψ∗,ℓ−1=ψℓ−1)N(ψℓ−1; Wℓψℓ,Σℓ) dψℓ−1,
∝Z
ψℓ−1exph
−1
2ψ⊤
ℓ−1¯Gt,ℓ−1ψℓ−1+ψ⊤
ℓ−1¯Bt,ℓ−1i
N(ψℓ−1; Wℓψℓ,Σℓ) dψℓ−1,
∝Z
ψℓ−1exph
−1
2ψ⊤
ℓ−1¯Gt,ℓ−1ψℓ−1+ψ⊤
ℓ−1¯Bt,ℓ−1
+ (ψℓ−1−Wℓψℓ)⊤Λℓ(ψℓ−1−Wℓψℓ)i
dψℓ−1.
Now let S=¯Gt,ℓ−1+ ΛℓandV=¯Bt,ℓ−1+ ΛℓWℓψℓ. Then we have that, 704
P(Ht|ψ∗,ℓ=ψℓ)
∝Z
ψℓ−1exph
−1
2ψ⊤
ℓ−1¯Gt,ℓ−1ψℓ−1+ψ⊤
ℓ−1¯Bt,ℓ−1
+ (ψℓ−1−Wℓψℓ)⊤Λℓ(ψℓ−1−Wℓψℓ)i
dψℓ−1,
∝Z
ψℓ−1exph
−1
2
ψ⊤
ℓ−1Sψℓ−1−2ψ⊤
ℓ−1 ¯Bt,ℓ−1+ ΛℓWℓψℓ
+ψ⊤
ℓW⊤
ℓΛℓWℓψℓi
dψℓ−1,
=Z
ψℓ−1exph
−1
2
ψ⊤
ℓ−1S(ψℓ−1−2S−1V) +ψ⊤
ℓW⊤
ℓΛℓWℓψℓi
dψℓ−1,
=Z
ψℓ−1exph
−1
2
(ψℓ−1−S−1V)⊤S(ψℓ−1−S−1V)
+ψ⊤
ℓW⊤
ℓΛℓWℓψℓ−V⊤S−1Vi
dψℓ−1.
In the second step, we omit constants in ψℓandψℓ−1. Thus 705
P(Ht|ψ∗,ℓ=ψℓ)
∝Z
ψℓ−1exp
−1
2 
(ψℓ−1−S−1V)⊤S(ψℓ−1−S−1V) +ψ⊤
ℓW⊤
ℓΛℓWℓψℓ−V⊤S−1V
dψℓ−1,
∝exp
−1
2 
ψ⊤
ℓW⊤
ℓΛℓWℓψℓ−V⊤S−1V
.
20It follows that 706
P(Ht|ψ∗,ℓ=ψℓ)
∝exp
−1
2 
ψ⊤
ℓW⊤
ℓΛℓWℓψℓ−V⊤S−1V
,
= exp
−1
2
ψ⊤
ℓW⊤
ℓΛℓWℓψℓ− ¯Bt,ℓ−1+ ΛℓWℓψℓ⊤S−1 ¯Bt,ℓ−1+ ΛℓWℓψℓ
∝exp
−1
2 
ψ⊤
ℓ 
W⊤
ℓΛℓWℓ−W⊤
ℓΛℓS−1ΛℓWℓ
ψℓ−2ψ⊤
ℓW⊤
ℓΛℓS−1¯Bt,ℓ−1
,
= exp
−1
2ψ⊤
ℓ¯Gt,ℓψℓ+ψ⊤
ℓ¯Bt,ℓ
.
In the last step, we omit constants in ψℓand we set 707
¯Gt,ℓ= W⊤
ℓ 
Λℓ−ΛℓS−1Λℓ
Wℓ= W⊤
ℓ 
Λℓ−Λℓ(Λℓ+¯Gt,ℓ−1)−1Σ−1
ℓΛℓ
Wℓ,
¯Bt,ℓ= W⊤
ℓΛℓS−1¯Bt,ℓ−1= W⊤
ℓΛℓ(Λℓ+¯Gt,ℓ−1)−1¯Bt,ℓ−1.
This completes the proof. 708
Similarly, this same proof applies when the reward distribution is not linear-Gaussian, with the 709
approximation P(Ht,i|θ∗,i=θ)≈ N 
θ;ˆBt,i,ˆG−1
t,i
. Using this approximation in the derivations 710
above leads to the same results. 711
C Posterior derivations for non-linear diffusion models 712
After deriving the posteriors for linear score functions fℓ, we now get back to the general case in (1), 713
where the score functions are potentially non-linear. Approximation is needed since both the score 714
functions and rewards can be non-linear. To avoid any computational challenges, we use a simple 715
and intuitive approximation, where all posteriors Pt,iandQt,ℓare approximated by the Gaussian 716
distributions in Appendix B.1, with few changes. First, the terms Wℓψℓin(18) are replaced by fℓ(ψℓ). 717
This accounts for the fact that the prior mean is now fℓ(ψℓ)rather than Wℓψℓ, and this is the main 718
difference between the linear diffusion model in (15) and the general, potentially non-linear, diffusion 719
model in (1). Second, the matrix multiplications that involve the matrices Wℓin(20) and(21) are 720
simply removed. Despite being simple, this approximation is efficient and avoids the computational 721
burden of heavy approximate sampling algorithms required for each latent parameter. This is why 722
deriving the exact posterior for linear score functions was key beyond enabling theoretical analyses. 723
Moreover, this approximation retains some key attributes of exact posteriors. Specifically, in the 724
absence of data, it recovers precisely the prior in (1), and as more data is accumulated, the influence 725
of the prior diminishes. 726
D Regret proof and additional discussions 727
D.1 Sketch of the proof 728
We start with the following standard lemma upon which we build our analysis [Aouali et al., 2023b]. 729
Lemma D.1. Assume that P(θ∗,i=θ|Ht) =N(θ; ˇµt,i,ˇΣt,i)for any i∈[K], then for any δ∈ 730
(0,1), 731
BR(n)≤p
2nlog(1/δ)r
EhPn
t=1∥Xt∥2
ˇΣt,Ati
+cnδ , where c >0is a constant .(28)
Applying Lemma D.1 requires proving that the marginal action-posteriors P(θ∗,i=θ|Ht)in Eq. (3) 732
are Gaussian and computing their covariances, while we only know the conditional action-posteriors 733
Pt,iand latent-posteriors Qt,ℓ. This is achieved by leveraging the preservation properties of the 734
family of Gaussian distributions [Koller and Friedman, 2009] and the total covariance decomposition 735
[Weiss, 2005] which leads to the next lemma. 736
21Lemma D.2. Lett∈[n]andi∈[K], then the marginal covariance matrix ˇΣt,ireads 737
ˇΣt,i=ˆΣt,i+P
ℓ∈[L]Pi,ℓ¯Σt,ℓP⊤
i,ℓ,where Pi,ℓ=ˆΣt,iΣ−1
1W1Qℓ−1
k=1¯Σt,kΣ−1
k+1Wk+1.(29)
The marginal covariance matrix ˇΣt,iin Eq. (29) decomposes into L+ 1 terms. The first term 738
corresponds to the posterior uncertainty of θ∗,i|ψ∗,1. The remaining Lterms capture the posterior 739
uncertainties of ψ∗,Landψ∗,ℓ−1|ψ∗,ℓforℓ∈[L]/{1}. These are then used to quantify the posterior 740
information gain of latent parameters after one round as follows. 741
Lemma D.3 (Posterior information gain) .Lett∈[n]andℓ∈[L], then 742
¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ⪰σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
tPAt,ℓ, where σ2
MAX= max ℓ∈[L+1]1 +σ2
ℓ
σ2. (30)
Finally, Lemma D.2 is used to decompose ∥Xt∥2
ˇΣt,Atin Eq. (28) into L+ 1terms. Each term is 743
bounded thanks to Lemma D.3. This results in the Bayes regret bound in Theorem 4.1. 744
D.2 Technical contributions 745
Our main technical contributions are the following. 746
Lemma D.2. IndTS, sampling is done hierarchically, meaning the marginal posterior distribution of 747
θ∗,i|Htis not explicitly defined. Instead, we use the conditional posterior distribution of θ∗,i|Ht, ψ∗,1. 748
The first contribution was deriving θ∗,i|Htusing the total covariance decomposition combined with 749
an induction proof, as our posteriors in Section 3.1 were derived recursively. Unlike in Bayes 750
regret analysis for standard Thompson sampling, where the posterior distribution of θ∗,i|Htis 751
predetermined due to the absence of latent parameters, our method necessitates this recursive total 752
covariance decomposition, marking a first difference from the standard Bayesian proofs of Thompson 753
sampling. Note that HierTS , which is developed for multi-task linear bandits, also employs total 754
covariance decomposition, but it does so under the assumption of a single latent parameter; on which 755
action parameters are centered. Our extension significantly differs as it is tailored for contextual 756
bandits with multiple, successive levels of latent parameters, moving away from HierTS ’s assumption 757
of a 1-level structure. Roughly speaking, HierTS when applied to contextual would consider a single- 758
level hierarchy, where θ∗,i|ψ∗,1∼ N (ψ∗,1,Σ1)withL= 1. In contrast, our model proposes a 759
multi-level hierarchy, where the first level is θ∗,i|ψ∗,1∼ N(W1ψ∗,1,Σ1). This also introduces a new 760
aspect to our approach – the use of a linear function W1ψ∗,1, as opposed to HierTS ’s assumption 761
where action parameters are centered directly on the latent parameter. Thus, while HierTS also 762
uses the total covariance decomposition, our generalize it to multi-level hierarchies under Llinear 763
functions Wℓψ∗,ℓ, instead of a single-level hierarchy under a single identity function ψ∗,1. 764
Lemma D.3. In Bayes regret proofs for standard Thompson sampling, we often quantify the posterior 765
information gain. This is achieved by monitoring the increase in posterior precision for the action 766
taken Atin each round t∈[n]. However, in dTS, our analysis extends beyond this. We not only 767
quantify the posterior information gain for the taken action but also for every latent parameter, since 768
they are also learned. This lemma addresses this aspect. To elaborate, we use the recursive formulas 769
in Section 3.1 that connect the posterior covariance of each latent parameter ψ∗,ℓwith the covariance 770
of the posterior action parameters θ∗,i. This allows us to propagate the information gain associated 771
with the action taken in round Atto all latent parameters ψ∗,ℓ,forℓ∈[L]by induction. This is a 772
novel contribution, as it is not a feature of Bayes regret analyses in standard Thompson sampling. 773
Proposition 4.2. Building upon the insights of Theorem 4.1, we introduce the sparsity assumption 774
(A3) . Under this assumption, we demonstrate that the Bayes regret outlined in Theorem 4.1 can be 775
significantly refined. Specifically, the regret becomes contingent on dimensions dℓ≤d, as opposed 776
to relying on the entire dimension d. This sparsity assumption is both a novel and a key technical 777
contribution to our work. Its underlying principle is straightforward: the Bayes regret is influenced 778
by the quantity of parameters that require learning. With the sparsity assumption, this number is 779
reduced to less than dfor each latent parameter. To substantiate this claim, we revisit the proof of 780
Theorem 4.1 and modify a crucial equality. This adjustment results in a more precise representation by 781
partitioning the covariance matrix of each latent parameter ψ∗,ℓinto blocks. These blocks comprise 782
adℓ×dℓsegment corresponding to the learnable dℓparameters of ψ∗,ℓ, and another block of size 783
(d−dℓ)×(d−dℓ)that does not necessitate learning. This decomposition allows us to conclude that 784
the final regret is solely dependent on dℓ, marking a significant refinement from the original theorem. 785
22D.3 Proof of lemma D.2 786
In this proof, we heavily rely on the total covariance decomposition [Weiss, 2005]. Also, refer to 787
[Hong et al., 2022b, Section 5.2] for a brief introduction to this decomposition. Now, from Eq. (17), 788
we have that 789
cov [θ∗,i|Ht, ψ∗,1] =ˆΣt,i=
ˆGt,i+ Σ−1
1−1
,
E[θ∗,i|Ht, ψ∗,1] = ˆµt,i=ˆΣt,i
ˆGt,iˆBt,i+ Σ−1
1W1ψ∗,1
.
First, given Ht,cov [θ∗,i|Ht, ψ∗,1] =
ˆGt,i+ Σ−1
1−1
is constant. Thus 790
E[cov [θ∗,i|Ht, ψ∗,1]|Ht] = cov [ θ∗,i|Ht, ψ∗,1] =
ˆGt,i+ Σ−1
1−1
=ˆΣt,i.
In addition, given Ht,ˆΣt,i,ˆGt,iandˆBt,iare constant. Thus 791
cov [E[θ∗,i|Ht, ψ∗,1]|Ht] = covh
ˆΣt,i
ˆGt,iˆBt,i+ Σ−1
1W1ψ∗,1Hti
,
= covh
ˆΣt,iΣ−1
1W1ψ∗,1Hti
,
=ˆΣt,iΣ−1
1W1cov [ψ∗,1|Ht] W⊤
1Σ−1
1ˆΣt,i,
=ˆΣt,iΣ−1
1W1¯¯Σt,1W⊤
1Σ−1
1ˆΣt,i,
where¯¯Σt,1= cov [ ψ∗,1|Ht]is the marginal posterior covariance of ψ∗,1. Finally, the total covariance 792
decomposition [Weiss, 2005, Hong et al., 2022b] yields that 793
ˇΣt,i= cov [ θ∗,i|Ht] =E[cov [θ∗,i|Ht, ψ∗,1]|Ht] + cov [ E[θ∗,i|Ht, ψ∗,1]|Ht],
=ˆΣt,i+ˆΣt,iΣ−1
1W1¯¯Σt,1W⊤
1Σ−1
1ˆΣt,i, (31)
However,¯¯Σt,1= cov [ ψ∗,1|Ht]is different from ¯Σt,1= cov [ ψ∗,1|Ht, ψ∗,2]that we already derived 794
in Eq. (18). Thus we do not know the expression of¯¯Σt,1. But we can use the same total covariance 795
decomposition trick to find it. Precisely, let¯¯Σt,ℓ= cov [ ψ∗,ℓ|Ht]for any ℓ∈[L]. Then we have that 796
¯Σt,1= cov [ ψ∗,1|Ht, ψ∗,2] = 
Σ−1
2+¯Gt,1−1,
¯µt,1=E[ψ∗,1|Ht, ψ∗,2] =¯Σt,1
Σ−1
2W2ψ∗,2+¯Bt,1
.
First, given Ht,cov [ψ∗,1|Ht, ψ∗,2] = 
Σ−1
2+¯Gt,1−1is constant. Thus 797
E[cov [ψ∗,1|Ht, ψ∗,2]|Ht] = cov [ ψ∗,1|Ht, ψ∗,2] =¯Σt,1.
In addition, given Ht,¯Σt,1,˜Σt,1and¯Bt,1are constant. Thus 798
cov [E[ψ∗,1|Ht, ψ∗,2]|Ht] = covh
¯Σt,1
Σ−1
2W2ψ∗,2+¯Bt,1Hti
,
= cov¯Σt,1Σ−1
2W2ψ∗,2Ht
,
=¯Σt,1Σ−1
2W2cov [ψ∗,2|Ht] W⊤
2Σ−1
2¯Σt,1,
=¯Σt,1Σ−1
2W2¯¯Σt,2W⊤
2Σ−1
2¯Σt,1.
Finally, total covariance decomposition [Weiss, 2005, Hong et al., 2022b] leads to 799
¯¯Σt,1= cov [ ψ∗,1|Ht] =E[cov [ψ∗,1|Ht, ψ∗,2]|Ht] + cov [ E[ψ∗,1|Ht, ψ∗,2]|Ht],
=¯Σt,1+¯Σt,1Σ−1
2W2¯¯Σt,2W⊤
2Σ−1
2¯Σt,1.
Now using the techniques, this can be generalized using the same technique as above to 800
¯¯Σt,ℓ=¯Σt,ℓ+¯Σt,ℓΣ−1
ℓ+1Wℓ+1¯¯Σt,ℓ+1W⊤
ℓ+1Σ−1
ℓ+1¯Σt,ℓ, ∀ℓ∈[L−1].
23Then, by induction, we get that 801
¯¯Σt,1=X
ℓ∈[L]¯Pℓ¯Σt,ℓ¯P⊤
ℓ, ∀ℓ∈[L−1],
where we use that by definition¯¯Σt,L= cov [ ψ∗,L|Ht] = ¯Σt,Land set ¯P1=Idand¯Pℓ= 802Qℓ−1
k=1¯Σt,kΣ−1
k+1Wk+1for any ℓ∈[L]/{1}. Plugging this in Eq. (31) leads to 803
ˇΣt,i=ˆΣt,i+X
ℓ∈[L]ˆΣt,iΣ−1
1W1¯Pℓ¯Σt,ℓ¯P⊤
ℓW⊤
1Σ−1
1ˆΣt,i,
=ˆΣt,i+X
ℓ∈[L]ˆΣt,iΣ−1
1W1¯Pℓ¯Σt,ℓ(ˆΣt,iΣ−1
1W1)⊤,
=ˆΣt,i+X
ℓ∈[L]Pi,ℓ¯Σt,ℓP⊤
i,ℓ,
where Pi,ℓ=ˆΣt,iΣ−1
1W1¯Pℓ=ˆΣt,iΣ−1
1W1Qℓ−1
k=1¯Σt,kΣ−1
k+1Wk+1. 804
D.4 Proof of lemma D.3 805
We prove this result by induction. We start with the base case when ℓ= 1. 806
(I) Base case. Letu=σ−1ˆΣ1
2
t,AtXtFrom the expression of ¯Σt,1in Eq. (18), we have that 807
¯Σ−1
t+1,1−¯Σ−1
t,1= W⊤
1
Σ−1
1−Σ−1
1(ˆΣ−1
t,At+σ−2XtX⊤
t)−1Σ−1
1−(Σ−1
1−Σ−1
1ˆΣt,AtΣ−1
1)
W1,
= W⊤
1
Σ−1
1(ˆΣt,At−(ˆΣ−1
t,At+σ−2XtX⊤
t)−1)Σ−1
1
W1,
= W⊤
1
Σ−1
1ˆΣ1
2
t,At(Id−(Id+σ−2ˆΣ1
2
t,AtXtX⊤
tˆΣ1
2
t,At)−1)ˆΣ1
2
t,AtΣ−1
1
W1,
= W⊤
1
Σ−1
1ˆΣ1
2
t,At(Id−(Id+uu⊤)−1)ˆΣ1
2
t,AtΣ−1
1
W1,
(i)= W⊤
1
Σ−1
1ˆΣ1
2
t,Atuu⊤
1 +u⊤uˆΣ1
2
t,AtΣ−1
1
W1,
(ii)=σ−2W⊤
1Σ−1
1ˆΣt,AtXtX⊤
t
1 +u⊤uˆΣt,AtΣ−1
1W1. (32)
In(i)we use the Sherman-Morrison formula. Note that (ii)says that ¯Σ−1
t+1,1−¯Σ−1
t,1is one-rank 808
which we will also need in induction step. Now, we have that ∥Xt∥2= 1. Therefore, 809
1 +u⊤u= 1 + σ−2X⊤
tˆΣt,AtXt≤1 +σ−2λ1(Σ1)∥Xt∥2= 1 + σ−2σ2
1≤σ2
MAX,
where we use that by definition of σ2
MAXin Lemma D.3, we have that σ2
MAX≥1 +σ−2σ2
1. Therefore, 810
by taking the inverse, we get that1
1+u⊤u≥σ−2
MAX. Combining this with Eq. (32) leads to 811
¯Σ−1
t+1,1−¯Σ−1
t,1⪰σ−2σ−2
MAXW⊤
1Σ−1
1ˆΣt,AtXtX⊤
tˆΣt,AtΣ−1
1W1
Noticing that PAt,1=ˆΣt,AtΣ−1
1W1concludes the proof of the base case when ℓ= 1. 812
(II) Induction step. Letℓ∈[L]/{1}and suppose that ¯Σ−1
t+1,ℓ−1−¯Σ−1
t,ℓ−1is one-rank and that it 813
holds for ℓ−1that 814
¯Σ−1
t+1,ℓ−1−¯Σ−1
t,ℓ−1⪰σ−2σ−2(ℓ−1)
MAX P⊤
At,ℓ−1XtX⊤
tPAt,ℓ−1,where σ−2
MAX= max
ℓ∈[L]1 +σ−2σ2
ℓ.
Then, we want to show that ¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓis also one-rank and that it holds that 815
¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ⪰σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
tPAt,ℓ, where σ−2
MAX= max
ℓ∈[L]1 +σ−2σ2
ℓ.
24This is achieved as follows. First, we notice that by the induction hypothesis, we have that ˜Σ−1
t+1,ℓ−1− 816
¯Gt,ℓ−1=¯Σ−1
t+1,ℓ−1−¯Σ−1
t,ℓ−1is one-rank. In addition, the matrix is positive semi-definite. Thus we 817
can write it as ˜Σ−1
t+1,ℓ−1−¯Gt,ℓ−1=uu⊤where u∈Rd. Then, similarly to the base case, we have 818
¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ=˜Σ−1
t+1,ℓ−˜Σ−1
t,ℓ,
= W⊤
ℓ 
Σℓ+˜Σt+1,ℓ−1−1Wℓ−W⊤
ℓ 
Σℓ+˜Σt,ℓ−1−1Wℓ,
= W⊤
ℓh 
Σℓ+˜Σt+1,ℓ−1−1− 
Σℓ+˜Σt,ℓ−1−1i
Wℓ,
= W⊤
ℓΣ−1
ℓh 
Σ−1
ℓ+¯Gt,ℓ−1−1− 
Σ−1
ℓ+˜Σ−1
t+1,ℓ−1−1i
Σ−1
ℓWℓ,
= W⊤
ℓΣ−1
ℓh 
Σ−1
ℓ+¯Gt,ℓ−1−1− 
Σ−1
ℓ+¯Gt,ℓ−1+˜Σ−1
t+1,ℓ−1−¯Gt,ℓ−1−1i
Σ−1
ℓWℓ,
= W⊤
ℓΣ−1
ℓh 
Σ−1
ℓ+¯Gt,ℓ−1−1− 
Σ−1
ℓ+¯Gt,ℓ−1+uu⊤−1i
Σ−1
ℓWℓ,
= W⊤
ℓΣ−1
ℓh
¯Σt,ℓ−1− ¯Σ−1
t,ℓ−1+uu⊤−1i
Σ−1
ℓWℓ,
= W⊤
ℓΣ−1
ℓh
¯Σt,ℓ−1uu⊤
1 +u⊤¯Σt,ℓ−1u¯Σt,ℓ−1i
Σ−1
ℓWℓ,
= W⊤
ℓΣ−1
ℓ¯Σt,ℓ−1uu⊤
1 +u⊤¯Σt,ℓ−1u¯Σt,ℓ−1Σ−1
ℓWℓ
However, we it follows from the induction hypothesis that uu⊤=˜Σ−1
t+1,ℓ−1−¯Gt,ℓ−1=¯Σ−1
t+1,ℓ−1− 819
¯Σ−1
t,ℓ−1⪰σ−2σ−2(ℓ−1)
MAX P⊤
At,ℓ−1XtX⊤
tPAt,ℓ−1. Therefore, 820
¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ= W⊤
ℓΣ−1
ℓ¯Σt,ℓ−1uu⊤
1 +u⊤¯Σt,ℓ−1u¯Σt,ℓ−1Σ−1
ℓWℓ,
⪰W⊤
ℓΣ−1
ℓ¯Σt,ℓ−1σ−2σ−2(ℓ−1)
MAX P⊤
At,ℓ−1XtX⊤
tPAt,ℓ−1
1 +u⊤¯Σt,ℓ−1u¯Σt,ℓ−1Σ−1
ℓWℓ,
=σ−2σ−2(ℓ−1)
MAX
1 +u⊤¯Σt,ℓ−1uW⊤
ℓΣ−1
ℓ¯Σt,ℓ−1P⊤
At,ℓ−1XtX⊤
tPAt,ℓ−1¯Σt,ℓ−1Σ−1
ℓWℓ,
=σ−2σ−2(ℓ−1)
MAX
1 +u⊤¯Σt,ℓ−1uP⊤
At,ℓXtX⊤
tPAt,ℓ.
Finally, we use that 1 +u⊤¯Σt,ℓ−1u≤1 +∥u∥2λ1(¯Σt,ℓ−1)≤1 +σ−2σ2
ℓ. Here we use that 821
∥u∥2≤σ−2, which can also be proven by induction, and that λ1(¯Σt,ℓ−1)≤σ2
ℓ, which follows from 822
the expression of ¯Σt,ℓ−1in Section 3.1. Therefore, we have that 823
¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ⪰σ−2σ−2(ℓ−1)
MAX
1 +u⊤¯Σt,ℓ−1uP⊤
At,ℓXtX⊤
tPAt,ℓ,
⪰σ−2σ−2(ℓ−1)
MAX
1 +σ−2σ2
ℓP⊤
At,ℓXtX⊤
tPAt,ℓ,
⪰σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
tPAt,ℓ,
where the last inequality follows from the definition of σ2
MAX= max ℓ∈[L]1 +σ−2σ2
ℓ. This concludes 824
the proof. 825
D.5 Proof of theorem 4.1 826
We start with the following standard result which we borrow from [Hong et al., 2022a, Aouali et al., 827
2023b], 828
BR(n)≤p
2nlog(1/δ)vuutE"nX
t=1∥Xt∥2
ˇΣt,At#
+cnδ , where c >0is a constant .(33)
25Then we use Lemma D.2 and express the marginal covariance ˇΣt,Atas 829
ˇΣt,i=ˆΣt,i+X
ℓ∈[L]Pi,ℓ¯Σt,ℓP⊤
i,ℓ, where Pi,ℓ=ˆΣt,iΣ−1
1W1ℓ−1Y
k=1¯Σt,kΣ−1
k+1Wk+1. (34)
Therefore, we can decompose ∥Xt∥2
ˇΣt,Atas 830
∥Xt∥2
ˇΣt,At=σ2X⊤
tˇΣt,AtXt
σ2(i)=σ2 
σ−2X⊤
tˆΣt,AtXt+σ−2X
ℓ∈[L]X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt
,
(ii)
≤c0log(1 + σ−2X⊤
tˆΣt,AtXt) +X
ℓ∈[L]cℓlog(1 + σ−2X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt), (35)
where (i)follows from Eq. (34), and we use the following inequality in (ii) 831
x=x
log(1 + x)log(1 + x)≤
max
x∈[0,u]x
log(1 + x)
log(1 + x) =u
log(1 + u)log(1 + x),
which holds for any x∈[0, u], where constants c0andcℓare derived as 832
c0=σ2
1
log(1 +σ2
1
σ2), c ℓ=σ2
ℓ+1
log(1 +σ2
ℓ+1
σ2),with the convention that σL+1= 1.
The derivation of c0uses that 833
X⊤
tˆΣt,AtXt≤λ1(ˆΣt,At)∥Xt∥2≤λ−1
d(Σ−1
1+Gt,At)≤λ−1
d(Σ−1
1) =λ1(Σ1) =σ2
1.
The derivation of cℓfollows from 834
X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt≤λ1(PAt,ℓP⊤
At,ℓ)λ1(¯Σt,ℓ)∥Xt∥2≤σ2
ℓ+1.
Therefore, from Eq. (35) and Eq. (33), we get that 835
BR(n)≤p
2nlog(1/δ)
Eh
c0nX
t=1log(1 + σ−2X⊤
tˆΣt,AtXt)
+X
ℓ∈[L]cℓnX
t=1log(1 + σ−2X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt)i1
2+cnδ (36)
Now we focus on bounding the logarithmic terms in Eq. (36). 836
(I) First term in Eq. (36) We first rewrite this term as 837
log(1 + σ−2X⊤
tˆΣt,AtXt)(i)= log det( Id+σ−2ˆΣ1
2
t,AtXtX⊤
tˆΣ1
2
t,At),
= log det( ˆΣ−1
t,At+σ−2XtX⊤
t)−log det( ˆΣ−1
t,At) = log det( ˆΣ−1
t+1,At)−log det( ˆΣ−1
t,At),
where (i)follows from the Weinstein–Aronszajn identity. Then we sum over all rounds t∈[n], and 838
get a telescoping 839
nX
t=1log det( Id+σ−2ˆΣ1
2
t,AtXtX⊤
tˆΣ1
2
t,At) =nX
t=1log det( ˆΣ−1
t+1,At)−log det( ˆΣ−1
t,At),
=nX
t=1KX
i=1log det( ˆΣ−1
t+1,i)−log det( ˆΣ−1
t,i) =KX
i=1nX
t=1log det( ˆΣ−1
t+1,i)−log det( ˆΣ−1
t,i),
=KX
i=1log det( ˆΣ−1
n+1,i)−log det( ˆΣ−1
1,i)(i)=KX
i=1log det(Σ1
2
1ˆΣ−1
n+1,iΣ1
2
1),
26where (i)follows from the fact that ˆΣ1,i= Σ 1. Now we use the inequality of arithmetic and 840
geometric means and get 841
nX
t=1log det( Id+σ−2ˆΣ1
2
t,AtXtX⊤
tˆΣ1
2
t,At) =KX
i=1log det(Σ1
2
1ˆΣ−1
n+1,iΣ1
2
1),
≤KX
i=1dlog1
dTr(Σ1
2
1ˆΣ−1
n+1,iΣ1
2
1)
, (37)
≤KX
i=1dlog
1 +n
dσ2
1
σ2
=Kdlog
1 +n
dσ2
1
σ2
.
(II) Remaining terms in Eq. (36) Letℓ∈[L]. Then we have that 842
log(1 + σ−2X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt) =σ2ℓ
MAXσ−2ℓ
MAXlog(1 + σ−2X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt),
≤σ2ℓ
MAXlog(1 + σ−2σ−2ℓ
MAXX⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt),
(i)=σ2ℓ
MAXlog det( Id+σ−2σ−2ℓ
MAX¯Σ1
2
t,ℓP⊤
At,ℓXtX⊤
tPAt,ℓ¯Σ1
2
t,ℓ),
=σ2ℓ
MAX
log det( ¯Σ−1
t,ℓ+σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
tPAt,ℓ)−log det( ¯Σ−1
t,ℓ)
,
where we use the Weinstein–Aronszajn identity in (i). Now we know from Lemma D.3 that the 843
following inequality holds σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
tPAt,ℓ⪯¯Σ−1
t+1,ℓ−¯Σ−1
t,ℓ. As a result, we get that 844
¯Σ−1
t,ℓ+σ−2σ−2ℓ
MAXP⊤
At,ℓXtX⊤
tPAt,ℓ⪯¯Σ−1
t+1,ℓ. Thus, 845
log(1 + σ−2X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt)≤σ2ℓ
MAX
log det( ¯Σ−1
t+1,ℓ)−log det( ¯Σ−1
t,ℓ)
,
Then we sum over all rounds t∈[n], and get a telescoping 846
nX
t=1log(1 + σ−2X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt)≤σ2ℓ
MAXnX
t=1log det( ¯Σ−1
t+1,ℓ)−log det( ¯Σ−1
t,ℓ),
=σ2ℓ
MAX
log det( ¯Σ−1
n+1,ℓ)−log det( ¯Σ−1
1,ℓ)
,
(i)=σ2ℓ
MAX
log det( ¯Σ−1
n+1,ℓ)−log det(Σ−1
ℓ+1)
,
=σ2ℓ
MAX
log det(Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1)
,
where we use that ¯Σ1,ℓ= Σ ℓ+1in(i). Finally, we use the inequality of arithmetic and geometric 847
means and get that 848
nX
t=1log(1 + σ−2X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt)≤σ2ℓ
MAX
log det(Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1)
,
≤dσ2ℓ
MAXlog1
dTr(Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1)
,(38)
≤dσ2ℓ
MAXlog
1 +σ2
ℓ+1
σ2
ℓ
,
The last inequality follows from the expression of ¯Σ−1
n+1,ℓin Eq. (18) that leads to 849
Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1=Id+ Σ1
2
ℓ+1¯Gt,ℓΣ1
2
ℓ+1,
=Id+ Σ1
2
ℓ+1W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
WℓΣ1
2
ℓ+1, (39)
27since ¯Gt,ℓ= W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
Wℓ. This allows us to bound1
dTr(Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1)as 850
1
dTr(Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1) =1
dTr(Id+ Σ1
2
ℓ+1W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
WℓΣ1
2
ℓ+1),
=1
d(d+ Tr(Σ1
2
ℓ+1W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
WℓΣ1
2
ℓ+1),
≤1 +1
ddX
k=1λ1(Σ1
2
ℓ+1W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
WℓΣ1
2
ℓ+1,
≤1 +1
ddX
k=1λ1(Σℓ+1)λ1(W⊤
ℓWℓ)λ1 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
,
≤1 +1
ddX
k=1λ1(Σℓ+1)λ1(W⊤
ℓWℓ)λ1 
Σ−1
ℓ
,
≤1 +1
ddX
k=1σ2
ℓ+1
σ2
ℓ= 1 +σ2
ℓ+1
σ2
ℓ, (40)
where we use the assumption that λ1(W⊤
ℓWℓ) = 1 (A2) and that λ1(Σℓ+1) =σ2
ℓ+1andλ1(Σ−1
ℓ) = 851
1/σ2
ℓ. This is because Σℓ=σ2
ℓIdfor any ℓ∈[L+ 1]. Finally, plugging Eqs. (37) and (38) in Eq. (36) 852
concludes the proof. 853
D.6 Proof of proposition 4.2 854
We use exactly the same proof in Appendix D.5, with one change to account for the sparsity 855
assumption (A3) . The change corresponds to Eq. (38). First, recall that Eq. (38) writes 856
nX
t=1log(1 + σ−2X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt)≤σ2ℓ
MAX
log det(Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1)
,
where 857
Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1=Id+ Σ1
2
ℓ+1W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
WℓΣ1
2
ℓ+1,
=Id+σ2
ℓ+1W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
Wℓ, (41)
where the second equality follows from the assumption that Σℓ+1=σ2
ℓ+1Id. But notice that in 858
our assumption, (A3) , we assume that Wℓ= (¯Wℓ,0d,d−dℓ), where ¯Wℓ∈Rd×dℓfor any ℓ∈[L]. 859
Therefore, we have that for any d×dmatrix B∈Rdd×d, the following holds, W⊤
ℓBWℓ= 860¯W⊤
ℓB¯Wℓ0dℓ,d−dℓ
0d−dℓ,dℓ0d−dℓ,d−dℓ
. In particular, we have that 861
W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ
Wℓ=¯W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ¯Wℓ0dℓ,d−dℓ
0d−dℓ,dℓ 0d−dℓ,d−dℓ
.(42)
Therefore, plugging this in Eq. (41) yields that 862
Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1=
Idℓ+σ2
ℓ+1¯W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ¯Wℓ0dℓ,d−dℓ
0d−dℓ,dℓ Id−dℓ
. (43)
As a result, det(Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1) = det( Idℓ+σ2
ℓ+1¯W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ¯Wℓ). This allows 863
us to move the problem from a d-dimensional one to a dℓ-dimensional one. Then we use the inequality 864
28of arithmetic and geometric means and get that 865
nX
t=1log(1 + σ−2X⊤
tPAt,ℓ¯Σt,ℓP⊤
At,ℓXt)≤σ2ℓ
MAX
log det(Σ1
2
ℓ+1¯Σ−1
n+1,ℓΣ1
2
ℓ+1)
,
=σ2ℓ
MAXlog det( Idℓ+σ2
ℓ+1¯W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ¯Wℓ),
≤dℓσ2ℓ
MAXlog1
dℓTr(Idℓ+σ2
ℓ+1¯W⊤
ℓ 
Σ−1
ℓ−Σ−1
ℓ¯Σt,ℓ−1Σ−1
ℓ¯Wℓ)
,
≤dℓσ2ℓ
MAXlog
1 +σ2
ℓ+1
σ2
ℓ
. (44)
To get the last inequality, we use derivations similar to the ones we used in Eq. (40). Finally, the 866
desired result in obtained by replacing Eq. (38) by Eq. (44) in the previous proof in Appendix D.5. 867
D.7 Additional discussion: link to two-level hierarchies 868
The linear diffusion (15) can be marginalized into a 2-level hierarchy using two different strategies. 869
The first one yields, 870
ψ∗,L∼ N(0, σ2
L+1BLB⊤
L), (45)
θ∗,i|ψ∗,L∼ N(ψ∗,L,Ω1), ∀i∈[K],
withΩ1=σ2
1Id+PL−1
ℓ=1σ2
ℓ+1BℓB⊤
ℓandBℓ=Qℓ
k=1Wk. The second strategy yields, 871
ψ∗,1∼ N(0,Ω2), (46)
θ∗,i|ψ∗,1∼ N(ψ∗,1, σ2
1Id), ∀i∈[K],
where Ω2=PL
ℓ=1σ2
ℓ+1BℓB⊤
ℓ. Recently, HierTS [Hong et al., 2022b] was developed for such 872
two-level graphical models, and we call HierTS under (45) byHierTS-1 andHierTS under (46) 873
byHierTS-2 . Then, we start by highlighting the differences between these two variants of HierTS . 874
First, their regret bounds scale as 875
HierTS-1 :˜O q
nd(KPL
ℓ=1σ2
ℓ+Lσ2
L+1
,HierTS-2 :˜O q
nd(Kσ2
1+PL
ℓ=1σ2
ℓ+1)
.
When K≈L, the regret bounds of HierTS-1 andHierTS-2 are similar. However, when K > L , 876
HierTS-2 outperforms HierTS-1 . This is because HierTS-2 puts more uncertainty on a single 877
d-dimensional latent parameter ψ∗,1, rather than Kindividual d-dimensional action parameters 878
θ∗,i. More importantly, HierTS-1 implicitly assumes that action parameters θ∗,iare conditionally 879
independent given ψ∗,L, which is not true. Consequently, HierTS-2 outperforms HierTS-1 . Note 880
that, under the linear diffusion model (15),dTSandHierTS-2 have roughly similar regret bounds. 881
Specifically, their regret bounds dependency on Kis identical, where both methods involve mul- 882
tiplying Kbyσ2
1, and both enjoy improved performance compared to HierTS-1 . That said, note 883
that Theorem 4.1 and Proposition 4.2 provide an understanding of how dTS’s regret scales under 884
linear score functions fℓ, and do not say that using dTSis better than using HierTS when the score 885
functions fℓare linear since the latter can be obtained by a proper marginalization of latent parameters 886
(i.e., HierTS-2 instead of HierTS-1 ). While such a comparison is not the goal of this work, we still 887
provide it for completeness next. 888
When the mixing matrices Wℓare dense (i.e., assumption (A3) is not applicable), dTSandHierTS-2 889
have comparable regret bounds and computational efficiency. However, under the sparsity assumption 890
(A3) and with mixing matrices that allow for conditional independence of ψ∗,1coordinates given 891
ψ∗,2,dTSenjoys a computational advantage over HierTS-2 . This advantage explains why works 892
focusing on multi-level hierarchies typically benchmark their algorithms against two-level structures 893
akin to HierTS-1 , rather than the more competitive HierTS-2 . This is also consistent with prior 894
works in Bayesian bandits using multi-level hierarchies, such as Tree-based priors [Hong et al., 895
2022a], which compared their method to HierTS-1 . In line with this, we also compared dTSwith 896
HierTS-1 in our experiments. But this is only given for completeness as this is not the aim of 897
Theorem 4.1 and Proposition 4.2. More importantly, HierTS is inapplicable in the general case in (1) 898
with non-linear score functions since the latent parameters cannot be analytically marginalized. 899
29E Broader impact 900
This work contributes to the development and analysis of practical algorithms for online learning to 901
act under uncertainty. While our generic setting and algorithms have broad potential applications, 902
the specific downstream social impacts are inherently dependent on the chosen application domain. 903
Nevertheless, we acknowledge the crucial need to consider potential biases that may be present in 904
pre-trained diffusion models, given that our method relies on them. 905
F Limitations 906
Our work investigated contextual bandits, laying the groundwork for future exploration into reinforce- 907
ment learning. This exploration can be done from both practical (empirical) and theoretical angles. 908
While our method, which approximates rewards using a Gaussian distribution, worked well for linear 909
rewards and those following a generalized linear model, its effectiveness in real-world, complex 910
scenarios needs further testing. Another interesting direction for future research is pre-training the 911
diffusion model prior. Hsieh et al. [2023] proposed a method for this in multi-armed bandits, but its 912
application to contextual bandits remains unexplored. 913
G Amount of computation required 914
Our experiments were conducted on internal machines with 30 CPUs and thus they required a moder- 915
ate amount of computation. These experiments are also reproducible with minimal computational 916
resources. 917
30NeurIPS Paper Checklist 918
1.Claims 919
Question: Do the main claims made in the abstract and introduction accurately reflect the 920
paper’s contributions and scope? 921
Answer: [Yes] 922
Justification: All claims are supported by the theory in Section 4 (with proofs provided in 923
the appendix) and experiments in Section 5. 924
Guidelines: 925
•The answer NA means that the abstract and introduction do not include the claims 926
made in the paper. 927
•The abstract and/or introduction should clearly state the claims made, including the 928
contributions made in the paper and important assumptions and limitations. A No or 929
NA answer to this question will not be perceived well by the reviewers. 930
•The claims made should match theoretical and experimental results, and reflect how 931
much the results can be expected to generalize to other settings. 932
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 933
are not attained by the paper. 934
2.Limitations 935
Question: Does the paper discuss the limitations of the work performed by the authors? 936
Answer: [Yes] 937
Justification: Limitations were discussed in Section 6 and Appendix F. 938
Guidelines: 939
•The answer NA means that the paper has no limitation while the answer No means that 940
the paper has limitations, but those are not discussed in the paper. 941
• The authors are encouraged to create a separate "Limitations" section in their paper. 942
•The paper should point out any strong assumptions and how robust the results are to 943
violations of these assumptions (e.g., independence assumptions, noiseless settings, 944
model well-specification, asymptotic approximations only holding locally). The authors 945
should reflect on how these assumptions might be violated in practice and what the 946
implications would be. 947
•The authors should reflect on the scope of the claims made, e.g., if the approach was 948
only tested on a few datasets or with a few runs. In general, empirical results often 949
depend on implicit assumptions, which should be articulated. 950
•The authors should reflect on the factors that influence the performance of the approach. 951
For example, a facial recognition algorithm may perform poorly when image resolution 952
is low or images are taken in low lighting. Or a speech-to-text system might not be 953
used reliably to provide closed captions for online lectures because it fails to handle 954
technical jargon. 955
•The authors should discuss the computational efficiency of the proposed algorithms 956
and how they scale with dataset size. 957
•If applicable, the authors should discuss possible limitations of their approach to 958
address problems of privacy and fairness. 959
•While the authors might fear that complete honesty about limitations might be used by 960
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 961
limitations that aren’t acknowledged in the paper. The authors should use their best 962
judgment and recognize that individual actions in favor of transparency play an impor- 963
tant role in developing norms that preserve the integrity of the community. Reviewers 964
will be specifically instructed to not penalize honesty concerning limitations. 965
3.Theory Assumptions and Proofs 966
Question: For each theoretical result, does the paper provide the full set of assumptions and 967
a complete (and correct) proof? 968
Answer: [Yes] 969
31Justification: Assumptions are mentioned in the main text. Complete proofs are provided in 970
the appendix. 971
Guidelines: 972
• The answer NA means that the paper does not include theoretical results. 973
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 974
referenced. 975
•All assumptions should be clearly stated or referenced in the statement of any theorems. 976
•The proofs can either appear in the main paper or the supplemental material, but if 977
they appear in the supplemental material, the authors are encouraged to provide a short 978
proof sketch to provide intuition. 979
•Inversely, any informal proof provided in the core of the paper should be complemented 980
by formal proofs provided in appendix or supplemental material. 981
• Theorems and Lemmas that the proof relies upon should be properly referenced. 982
4.Experimental Result Reproducibility 983
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 984
perimental results of the paper to the extent that it affects the main claims and/or conclusions 985
of the paper (regardless of whether the code and data are provided or not)? 986
Answer: [Yes] 987
Justification: Information needed to reproduce the main experimental results of the paper is 988
described in Section 5. 989
Guidelines: 990
• The answer NA means that the paper does not include experiments. 991
•If the paper includes experiments, a No answer to this question will not be perceived 992
well by the reviewers: Making the paper reproducible is important, regardless of 993
whether the code and data are provided or not. 994
•If the contribution is a dataset and/or model, the authors should describe the steps taken 995
to make their results reproducible or verifiable. 996
•Depending on the contribution, reproducibility can be accomplished in various ways. 997
For example, if the contribution is a novel architecture, describing the architecture fully 998
might suffice, or if the contribution is a specific model and empirical evaluation, it may 999
be necessary to either make it possible for others to replicate the model with the same 1000
dataset, or provide access to the model. In general. releasing code and data is often 1001
one good way to accomplish this, but reproducibility can also be provided via detailed 1002
instructions for how to replicate the results, access to a hosted model (e.g., in the case 1003
of a large language model), releasing of a model checkpoint, or other means that are 1004
appropriate to the research performed. 1005
•While NeurIPS does not require releasing code, the conference does require all submis- 1006
sions to provide some reasonable avenue for reproducibility, which may depend on the 1007
nature of the contribution. For example 1008
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 1009
to reproduce that algorithm. 1010
(b)If the contribution is primarily a new model architecture, the paper should describe 1011
the architecture clearly and fully. 1012
(c)If the contribution is a new model (e.g., a large language model), then there should 1013
either be a way to access this model for reproducing the results or a way to reproduce 1014
the model (e.g., with an open-source dataset or instructions for how to construct 1015
the dataset). 1016
(d)We recognize that reproducibility may be tricky in some cases, in which case 1017
authors are welcome to describe the particular way they provide for reproducibility. 1018
In the case of closed-source models, it may be that access to the model is limited in 1019
some way (e.g., to registered users), but it should be possible for other researchers 1020
to have some path to reproducing or verifying the results. 1021
5.Open access to data and code 1022
32Question: Does the paper provide open access to the data and code, with sufficient instruc- 1023
tions to faithfully reproduce the main experimental results, as described in supplemental 1024
material? 1025
Answer: [Yes] 1026
Justification: The code for the main experiments is shared in the supplementary material. 1027
Guidelines: 1028
• The answer NA means that paper does not include experiments requiring code. 1029
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 1030
public/guides/CodeSubmissionPolicy ) for more details. 1031
•While we encourage the release of code and data, we understand that this might not be 1032
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 1033
including code, unless this is central to the contribution (e.g., for a new open-source 1034
benchmark). 1035
•The instructions should contain the exact command and environment needed to run to 1036
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 1037
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 1038
•The authors should provide instructions on data access and preparation, including how 1039
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 1040
•The authors should provide scripts to reproduce all experimental results for the new 1041
proposed method and baselines. If only a subset of experiments are reproducible, they 1042
should state which ones are omitted from the script and why. 1043
•At submission time, to preserve anonymity, the authors should release anonymized 1044
versions (if applicable). 1045
•Providing as much information as possible in supplemental material (appended to the 1046
paper) is recommended, but including URLs to data and code is permitted. 1047
6.Experimental Setting/Details 1048
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 1049
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 1050
results? 1051
Answer: [Yes] 1052
Justification: All experimental details are described in Section 5. 1053
Guidelines: 1054
• The answer NA means that the paper does not include experiments. 1055
•The experimental setting should be presented in the core of the paper to a level of detail 1056
that is necessary to appreciate the results and make sense of them. 1057
•The full details can be provided either with the code, in appendix, or as supplemental 1058
material. 1059
7.Experiment Statistical Significance 1060
Question: Does the paper report error bars suitably and correctly defined or other appropriate 1061
information about the statistical significance of the experiments? 1062
Answer: [Yes] 1063
Justification: Standard error bars are included in the figures. 1064
Guidelines: 1065
• The answer NA means that the paper does not include experiments. 1066
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 1067
dence intervals, or statistical significance tests, at least for the experiments that support 1068
the main claims of the paper. 1069
•The factors of variability that the error bars are capturing should be clearly stated (for 1070
example, train/test split, initialization, random drawing of some parameter, or overall 1071
run with given experimental conditions). 1072
•The method for calculating the error bars should be explained (closed form formula, 1073
call to a library function, bootstrap, etc.) 1074
33• The assumptions made should be given (e.g., Normally distributed errors). 1075
•It should be clear whether the error bar is the standard deviation or the standard error 1076
of the mean. 1077
•It is OK to report 1-sigma error bars, but one should state it. The authors should 1078
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 1079
of Normality of errors is not verified. 1080
•For asymmetric distributions, the authors should be careful not to show in tables or 1081
figures symmetric error bars that would yield results that are out of range (e.g. negative 1082
error rates). 1083
•If error bars are reported in tables or plots, The authors should explain in the text how 1084
they were calculated and reference the corresponding figures or tables in the text. 1085
8.Experiments Compute Resources 1086
Question: For each experiment, does the paper provide sufficient information on the com- 1087
puter resources (type of compute workers, memory, time of execution) needed to reproduce 1088
the experiments? 1089
Answer: [Yes] 1090
Justification: As mentioned in Appendix G, our experiments were conducted on internal 1091
machines with 30 CPUs and thus they required a moderate amount of computation. These 1092
experiments are also reproducible with minimal computational resources. 1093
Guidelines: 1094
• The answer NA means that the paper does not include experiments. 1095
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 1096
or cloud provider, including relevant memory and storage. 1097
•The paper should provide the amount of compute required for each of the individual 1098
experimental runs as well as estimate the total compute. 1099
•The paper should disclose whether the full research project required more compute 1100
than the experiments reported in the paper (e.g., preliminary or failed experiments that 1101
didn’t make it into the paper). 1102
9.Code Of Ethics 1103
Question: Does the research conducted in the paper conform, in every respect, with the 1104
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 1105
Answer: [Yes] 1106
Justification: This work contributes to the development and theoretical analysis of online 1107
learning to act under uncertainty and it adheres to the Neurips Code Of Ethics. 1108
Guidelines: 1109
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 1110
•If the authors answer No, they should explain the special circumstances that require a 1111
deviation from the Code of Ethics. 1112
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 1113
eration due to laws or regulations in their jurisdiction). 1114
10.Broader Impacts 1115
Question: Does the paper discuss both potential positive societal impacts and negative 1116
societal impacts of the work performed? 1117
Answer: [Yes] 1118
Justification: Broader Impacts are discussed in Appendix E. 1119
Guidelines: 1120
• The answer NA means that there is no societal impact of the work performed. 1121
•If the authors answer NA or No, they should explain why their work has no societal 1122
impact or why the paper does not address societal impact. 1123
34•Examples of negative societal impacts include potential malicious or unintended uses 1124
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 1125
(e.g., deployment of technologies that could make decisions that unfairly impact specific 1126
groups), privacy considerations, and security considerations. 1127
•The conference expects that many papers will be foundational research and not tied 1128
to particular applications, let alone deployments. However, if there is a direct path to 1129
any negative applications, the authors should point it out. For example, it is legitimate 1130
to point out that an improvement in the quality of generative models could be used to 1131
generate deepfakes for disinformation. On the other hand, it is not needed to point out 1132
that a generic algorithm for optimizing neural networks could enable people to train 1133
models that generate Deepfakes faster. 1134
•The authors should consider possible harms that could arise when the technology is 1135
being used as intended and functioning correctly, harms that could arise when the 1136
technology is being used as intended but gives incorrect results, and harms following 1137
from (intentional or unintentional) misuse of the technology. 1138
•If there are negative societal impacts, the authors could also discuss possible mitigation 1139
strategies (e.g., gated release of models, providing defenses in addition to attacks, 1140
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 1141
feedback over time, improving the efficiency and accessibility of ML). 1142
11.Safeguards 1143
Question: Does the paper describe safeguards that have been put in place for responsible 1144
release of data or models that have a high risk for misuse (e.g., pretrained language models, 1145
image generators, or scraped datasets)? 1146
Answer: [NA] 1147
Justification: Our paper is mainly theoretical and the used data is simulated. Thus, we 1148
believe that our work poses no such risks. 1149
Guidelines: 1150
• The answer NA means that the paper poses no such risks. 1151
•Released models that have a high risk for misuse or dual-use should be released with 1152
necessary safeguards to allow for controlled use of the model, for example by requiring 1153
that users adhere to usage guidelines or restrictions to access the model or implementing 1154
safety filters. 1155
•Datasets that have been scraped from the Internet could pose safety risks. The authors 1156
should describe how they avoided releasing unsafe images. 1157
•We recognize that providing effective safeguards is challenging, and many papers do 1158
not require this, but we encourage authors to take this into account and make a best 1159
faith effort. 1160
12.Licenses for existing assets 1161
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 1162
the paper, properly credited and are the license and terms of use explicitly mentioned and 1163
properly respected? 1164
Answer: [Yes] 1165
Justification: To the best of our knowledge, all relevant and used papers were cited. 1166
Guidelines: 1167
• The answer NA means that the paper does not use existing assets. 1168
• The authors should cite the original paper that produced the code package or dataset. 1169
•The authors should state which version of the asset is used and, if possible, include a 1170
URL. 1171
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1172
•For scraped data from a particular source (e.g., website), the copyright and terms of 1173
service of that source should be provided. 1174
35•If assets are released, the license, copyright information, and terms of use in the 1175
package should be provided. For popular datasets, paperswithcode.com/datasets 1176
has curated licenses for some datasets. Their licensing guide can help determine the 1177
license of a dataset. 1178
•For existing datasets that are re-packaged, both the original license and the license of 1179
the derived asset (if it has changed) should be provided. 1180
•If this information is not available online, the authors are encouraged to reach out to 1181
the asset’s creators. 1182
13.New Assets 1183
Question: Are new assets introduced in the paper well documented and is the documentation 1184
provided alongside the assets? 1185
Answer: [Yes] 1186
Justification: We include our code as supplementary material, with all details needed for 1187
reproducibility given in Section 5. 1188
Guidelines: 1189
• The answer NA means that the paper does not release new assets. 1190
•Researchers should communicate the details of the dataset/code/model as part of their 1191
submissions via structured templates. This includes details about training, license, 1192
limitations, etc. 1193
•The paper should discuss whether and how consent was obtained from people whose 1194
asset is used. 1195
•At submission time, remember to anonymize your assets (if applicable). You can either 1196
create an anonymized URL or include an anonymized zip file. 1197
14.Crowdsourcing and Research with Human Subjects 1198
Question: For crowdsourcing experiments and research with human subjects, does the paper 1199
include the full text of instructions given to participants and screenshots, if applicable, as 1200
well as details about compensation (if any)? 1201
Answer: [NA] 1202
Justification: This work does not involve crowdsourcing nor research with human subjects. 1203
Guidelines: 1204
•The answer NA means that the paper does not involve crowdsourcing nor research with 1205
human subjects. 1206
•Including this information in the supplemental material is fine, but if the main contribu- 1207
tion of the paper involves human subjects, then as much detail as possible should be 1208
included in the main paper. 1209
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1210
or other labor should be paid at least the minimum wage in the country of the data 1211
collector. 1212
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1213
Subjects 1214
Question: Does the paper describe potential risks incurred by study participants, whether 1215
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1216
approvals (or an equivalent approval/review based on the requirements of your country or 1217
institution) were obtained? 1218
Answer: [NA] 1219
Justification: This work does not involve crowdsourcing nor research with human subjects. 1220
Guidelines: 1221
•The answer NA means that the paper does not involve crowdsourcing nor research with 1222
human subjects. 1223
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1224
may be required for any human subjects research. If you obtained IRB approval, you 1225
should clearly state this in the paper. 1226
36•We recognize that the procedures for this may vary significantly between institutions 1227
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1228
guidelines for their institution. 1229
•For initial submissions, do not include any information that would break anonymity (if 1230
applicable), such as the institution conducting the review. 1231
37