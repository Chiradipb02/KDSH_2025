On the Robustness of Spectral Algorithms for
Semirandom Stochastic Block Models
Aditya Bhaskara∗Agastya Vibhuti Jha†Michael Kapralov‡Naren Sarayu Manoj§
Davide Mazzali¶Weronika Wrzos-Kaminska∥
Abstract
In a graph bisection problem, we are given a graph Gwith two equally-sized un-
labeled communities, and the goal is to recover the vertices in these communities.
A popular heuristic, known as spectral clustering, is to output an estimated com-
munity assignment based on the eigenvector corresponding to the second smallest
eigenvalue of the Laplacian of G. Spectral algorithms can be shown to provably
recover the cluster structure for graphs generated from certain probabilistic mod-
els, such as the Stochastic Block Model (SBM). However, spectral clustering is
known to be non-robust to model mis-speciﬁcation. Techniques based on semidef-
inite programming have been shown to be more robust, but they incur signiﬁcant
computational overheads.
In this work, we study the robustness of spectral algorithms against semirandom
adversaries. Informally, a semirandom adversary is allowed to “helpfully” change
the speciﬁcation of the model in a way that is consistent with the ground-truth so-
lution. Our semirandom adversaries in particular are allowed to add edges inside
clusters or increase the probability that an edge appears inside a cluster. Semiran-
dom adversaries are a useful tool to determine the extent to which an algorithm
has overﬁt to statistical assumptions on the input.
On the positive side, we identify classes of semirandom adversaries under which
spectral bisection using the unnormalized Laplacian is strongly consistent, i.e.,
it exactly recovers the planted partitioning. On the negative side, we show that
in these classes spectral bisection with the normalized Laplacian outputs a parti-
tioning that makes a classiﬁcation mistake on a constant fraction of the vertices.
Finally, we demonstrate numerical experiments that complement our theoretical
ﬁndings.
1 Introduction
Graph partitioning or clustering is a fundamental unsupervised learning primitive. In a graph par-
titioning problem, one seeks to identify clusters of vertices that are highly internally connected
and sparsely connected to the outside. This task is of particular signiﬁcance when the given graph
presents a latent community structure. In this setting, the goal is to recover the communities as
accurately as possible. Various statistical models that attempt to capture this situation have been pro-
posed and studied in the literature. Perhaps the most popular of these is the Symmetric Stochastic
Block Model (SSBM) [ HLL83 ].
∗University of Utah. Email: bhaskaraaditya@gmail.com .
†University of Chicago. Email: agastyavjha.28@gmail.com .
‡École Polytechnique Fédérale de Lausanne. Email: michael.kapralov@epfl.ch .
§Toyota Technological Institute at Chicago. Email: nsm@ttic.edu .
¶École Polytechnique Fédérale de Lausanne. Email: davide.mazzali@epfl.ch .
∥École Polytechnique Fédérale de Lausanne. Email: weronika.wrzos-kaminska@epfl.ch .
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Following the notation of previous works [ AFWZ20 ;DLS21 ], in this paper we describe an SSBM
with speciﬁcations n, P 1, P2, p, q, where nis an even positive integer, P1andP2are a partitioning of
the vertex set V={1, . . . , n}into subsets of equal size, and pandqare probabilities. Without loss
of generality, we may assume that the partitions P1andP2consist of vertices 1, . . . , n/ 2andn/2 +
1, . . . , n , respectively. Hence, with a mild abuse of notation, we write an SSBM with parameters
n, p, q only and write it as SSBM (n, p, q ). Now, let SSBM (n, p, q )be a distribution over random
undirected graphs G= (V, E)where each edge (v, w)∈P1×P1and(v, w)∈P2×P2(which
we refer to as “internal edges”) appears independently with probability p, and each edge (v, w)∈
P1×P2(which we refer to as “crossing edges”) appears independently with probability q. When
p≫q, there should be many more internal edges than crossing edges. Hence, we expect the
community structure to become more evident as ptends away from q.
In such scenarios, our general algorithmic goal is to efﬁciently identify P1andP2when given G
without any community labels. This task is hereafter referred to as the graph bisection problem .
In this work, we will be interested in exact recovery , also known as strong consistency , in which
we want an algorithm that, with probability at least 1−1/nover the randomness of the instance,
exactly returns the partition {P1, P2}for all nsufﬁciently large. Other approximate notions of
recovery (such as almost exact, partial, and weak recovery) are also well-studied but are beyond the
scope of this work.
Although the SSBM (n, p, q )distribution over graphs is a useful starting point for algorithm design
and has led to a deep theory about when recovery is possible and of what nature [ Abb18 ], it may not
be representative of all scenarios in which we should expect our algorithms to succeed. To remedy
this, researchers have proposed several different random graph models that may be more reﬂective
of properties satisﬁed by real-world networks. These include the geometric block model [ GNW24 ],
the Gaussian mixture block model [ LS24 ], and others.
In this paper, we take a different perspective to graph generation by considering various semiran-
dom models . At a high level, a semirandom model for a statistical problem interpolates between
an average-case input (for example produced by a model such as the SSBM) and a worst-case in-
put, in a way that still allows for a meaningful notion of ground-truth solution. In our context of
graph bisection, this can be achieved by an adversary adding internal edges or by the distribution of
internal edges itself being nonhomogeneous (i.e., every internal edge (v, w)appears independently
with probability pvw≥p, where the pvwmay be chosen adversarially for each internal edge). Re-
searchers have studied similar semirandom models for graph bisection [ beyondclusterin ;FK01 ;
MMV12 ;MPW16 ;Moi21 ] and other statistical problems such as classiﬁcation under Massart noise
[MN06 ], detecting a planted clique in a random graph [ FK01 ;CSV17 ;MMT20 ;BKS23 ], sparse
recovery [ KLLST23 ], and top- Kranking [ YCOM24 ].
These modeling modiﬁcations are not necessarily meant to capture a real-world data generation
process. Rather, they are a useful testbed with which we can determine whether commonly used
algorithms have overﬁt to statistical assumptions present in the model. In particular, observe that
these changes in model speciﬁcation are ostensibly helpful, in that increasing the number of internal
edges should only enhance the community structure. Perhaps surprisingly, it is known that a number
of natural algorithms that succeed in the SSBM setting no longer work under such helpful modiﬁ-
cations [ Moi21 ]. Therefore, it is natural to ask which algorithms for graph bisection are robust in
semirandom models.
At this point, the performance of approaches based on convex programming is well-understood in
various semirandom models [ FK01 ;MMV12 ;MPW16 ;Moi21 ;CdM24 ]. However, in practice, it is
impractical to run such an algorithm due to computational costs. Another class of algorithms, that
we call spectral algorithms , is more widely used in practice. Loosely speaking, a spectral algorithm
constructs a matrix Mthat is a function of the graph Gand outputs a clustering arising from the
embedding of the vertices determined by the eigenvectors of M. Popular choices of matrices include
the unnormalized Laplacian LGand the normalized Laplacian LG(we will formally deﬁne and
intuit these notions in the sequel) [ V on07 ]. This is because structural properties of both LGand
LGimply that the second smallest eigenvalue of each, denoted as λ2(LG)andλ2(LG), serves as
a continuous proxy for connectivity, and the corresponding eigenvector, u2(LG)andu2(LG), has
entries whose signs reveal a lot of information about the underlying community structure. This
motivates Algorithm 1. It can be run, for example, with Matrix (G):=LGorMatrix (G):=LG.
Following this discussion, we arrive at the question we study in this paper.
2Algorithm 1 SpectralBisection : given G= (V, E), outputs a bipartition of V
1:procedure SpectralBisection (G) ▷ G= (V, E)is the input graph
2:M←Matrix (G) ▷M∈RV×Vis a matrix with real eigenvalues
3: ((λi,ui))n
i=1←eigenvalue-eigenvector pairs of Mwithλ1≤···≤ λn ▷ n=|V|
4: S←{v∈V:u2[v]<0}
5: return{S, V\S}
Question 1. Under which semirandom models do the Laplacian-based spectral algorithms, using
the second eigenvector of LGorLG, exactly recover the ground-truth communities P1andP2?
Main contributions. Our results show a surprising difference in the robustness of spectral bi-
section when considering the normalized versus the unnormalized Laplacian. We summarize our
results below:
•Consider a nonhomogeneous symmetric stochastic block model with parameters q < p <
p, where every internal edge appears independently with probability puv∈[p,p]and every
crossing edge appears independently with probability q. We show that under an appropriate
spectral gap condition, the spectral algorithm with the unnormalized Laplacian exactly re-
covers the communities P1andP2. Moreover, this holds even if an adversary plants ≪np
internal edges per vertex prior to the edge sampling phase.
•Consider a stronger semirandom model where the subgraphs on the two communities P1
andP2are adversarially chosen and the crossing edges are sampled independently with
probability q. We show that if the graph is sufﬁciently dense and satisﬁes a spectral gap
condition, then the spectral algorithm with the unnormalized Laplacian exactly recovers
the communities P1andP2.
•We show that there is a family of instances from a nonhomogeneous symmetric stochastic
block model in which the spectral algorithm achieves exact recovery with the unnormalized
Laplacian, but incurs a constant error rate with the normalized Laplacian. This is surprising
because it contradicts conventional wisdom that normalized spectral clustering should be
favored over unnormalized spectral clustering [ V on07 ].
We also numerically complement our ﬁndings via experiments on various parameter settings.
Outline. The rest of this paper is organized as follows. In Section 2, we more formally deﬁne our
semirandom models, the Laplacians LandL, and formally state our results. In Section 3, we give
sketches of the proofs of our results. In Section 4, we show results from numerical trials suggested by
our theory. In Appendices A.1andA.5we prove important auxiliary lemmas we need for our results.
In Appendix A.6, we prove our robustness results for the unnormalized Laplacian. In Appendix A.8,
we prove our inconsistency result for the normalized Laplacian. In Appendix B, we give additional
numerical trials and discussion.
2 Models and main results
In this paper, we study unnormalized and normalized spectral clustering in several semirandom
SSBMs. These models permit a richer family of graphs than the SSBM alone.
Matrices related to graphs. Throughout this paper, all graphs are to be interpreted as being
undirected, and we assume that the vertices of an n-vertex graph coincide with the set {1, . . . , n}.
With this in mind, we begin with deﬁning various matrices associated with graphs, building up to the
unnormalized and normalized Laplacians, which are central to the family of algorithms we analyze
(Algorithm 1).
Deﬁnition 2.1 (Adjacency matrix) .LetG= (V, E)be a graph. The adjacency matrix AG∈RV×V
ofGis the matrix with entries deﬁned as AG[v, w] = /x31{(v, w)∈E}.
Deﬁnition 2.2 (Degree matrix) .LetG= (V, E)be a graph. The degree matrix DG∈RV×VofG
is the diagonal matrix with entries deﬁned as DG[v, v] =dG[v], where dG[v]is the degree of v.
3Deﬁnition 2.3 (Unnormalized Laplacian) .LetG= (V, E)be a graph. The unnormalized Laplacian
LG∈RV×VofGis the matrix deﬁned as LG:=DG−AG=/summationtext
(v,w)∈E(ev−ew)(ev−ew)⊤,
where eidenotes the i-th standard basis vector.
Deﬁnition 2.4 (Normalized Laplacians) .LetG= (V, E)be a graph. The symmetric normalized
LaplacianLG,sym∈RV×Vand the random walk Laplacian LG,rw∈RV×VofGare deﬁned as
LG,sym:=I−D−1/2
GAGD−1/2
G,LG,rw:=I−D−1
GAG.
For all notions above, when the graph Gis clear from context, we omit the subscript G. Furthermore,
when we discuss normalized Laplacians, we intend its symmetric version Lsymunless otherwise
stated. So, we omit this subscript as well and simply write L.
Next, we deﬁne the spectral bisection algorithms. We will discuss some intuition for why these
algorithms are reasonable heuristics in Section 3.
Deﬁnition 2.5 (Unnormalized and normalized spectral bisection) .LetG= (V, E)be a graph,
and let its unnormalized and normalized Laplacians be LandL, respectively. We refer to the
algorithm resulting from running Algorithm 1onGwithMatrix (G):=LGasunnormalized spectral
bisection . We refer to the algorithm resulting from running Algorithm 1onGwithMatrix (G) =LG
asnormalized spectral bisection .
Our goal is to understand when the above algorithms, applied to a graph with a latent community
structure, achieve exact recovery orstrong consistency , deﬁned as follows.
Deﬁnition 2.6. Let{P1, P2}be a partitioning of V={1, . . . , n}, and letD:=D({P1, P2})be a
distribution over n-vertex graphs G= (V, E). We say that an algorithm is strongly consistent or
achieves exact recovery onDif given a graph G∼D it outputs the correct partitioning {P1, P2}
with probability at least 1−1/nover the randomness of G.
2.1 Nonhomogeneous symmetric stochastic block model
Our ﬁrst model is a family of nonhomogeneous symmetric stochastic block models, deﬁned below.
Model 1 (Nonhomogeneous symmetric stochastic block model) .Letnbe an even positive integer,
V={1, . . . , n},{P1, P2}be a partitioning of Vinto two equally-sized subsets, and q < p≤p
be probabilities. Let Dbe any probability distribution over graphs G= (V, E)such that for every
(v, w)∈P1×P1and(v, w)∈P2×P2, the edge (v, w)appears in Eindependently with some
probability pvw∈[p,p], and for every (v, w)∈P1×P2, the edge (v, w)appears in Eindependently
with probability q. We call suchDa nonhomogeneous symmetric stochastic block model (which we
will abbreviate as NSSBM). We call the set of all such Dthe family of nonhomogeneous stochastic
block models with parameters p,p, q, written as NSSBM (n, p,p, q).
To visualize Model 1, consider the expected adjacency matrix of some NSSBM distribution. We
then have the relations/bracketleftbiggp·Jn/2q·Jn/2
q·Jn/2p·Jn/2/bracketrightbigg
≤/bracketleftbiggPP1 q·Jn/2
q·Jn/2PP2/bracketrightbigg
≤/bracketleftbiggp·Jn/2q·Jn/2
q·Jn/2p·Jn/2/bracketrightbigg
,
where the leftmost matrix denotes the expected adjacency matrix of SSBM (n, p, q ), the rightmost
matrix denotes the expected adjacency matrix of SSBM (n,p, q),Jkdenotes the k×kall-ones matrix,
andPP1andPP2denote the edge probability matrices for edges internal to P1andP2, respectively.
The above also shows that the rank of the expected adjacency matrix for SSBM (n, p, q )is2. How-
ever, the rank for the expected adjacency matrix for some NSSBM distribution may be as large
asΩ(n). Perhaps surprisingly, this will turn out to be unimportant for our entrywise eigenvector
perturbation analysis. In particular, the tools we use were originally designed for low-rank signal
matrices or spiked low-rank signal matrices [ AFWZ20 ;DLS21 ;BV24 ], but we will see that they
can be adapted to the signal matrices we consider.
The NSSBM family generalizes the symmetric stochastic block model described in the previous
section – this is attained by setting pvw=pfor all internal edges (v, w). However, it can also
encode biases for certain graph properties. For instance, a distribution from the NSSBM family may
encode the idea that certain subsets of P1are expected to be denser than P1as a whole.
With this deﬁnition in hand, we are ready to formally state our ﬁrst technical result in Theorem 1.
4Theorem 1. Letp,p, qbe probabilities such that q < p≤pand such that α:=p/(p−q)is an
arbitrary constant. Let D∈ NSSBM (n, p,p, q). Let n≥N(α)where the function N(α)only
depends on α. There exists a universal constant C > 0such that if
n(p−q)≥C/parenleftBig/radicalbig
nplogn+ log n/parenrightBig
, (gap condition)
then unnormalized spectral bisection is strongly consistent on D.
We prove Theorem 1in Appendix A.7.1 . In fact, we show a somewhat stronger statement – in
addition to the process described above, we also allow the adversary to, before sampling the graph,
set a small number of the pvwto1(at most np/log log nedges per vertex). We detail this further in
Appendix A.7.1 .
We now remark on the tightness of our gap condition in Theorem 1. A work of Abbe, Bandeira, and
Hall [ ABH16 ] identiﬁes an exact information-theoretic threshold above which exact recovery with
high probability is possible and below which no algorithm can be strongly consistent. In particu-
lar, the threshold states that for any pandqsatisfying√p−√q >/radicalbig
2 logn/n, exact recovery is
possible, and when pandqdo not satisfy this, exact recovery is information-theoretically impossi-
ble. Furthermore, Feige and Kilian [ FK01 ] prove that the information-theoretic threshold does not
change in a somewhat stronger semirandom model that includes the NSSBM family. Additionally,
Deng, Ling, and Strohmer [ DLS21 ] show that unnormalized spectral bisection is strongly consistent
all the way to this threshold in the special case where the graph is drawn from SSBM (n, p, q ). By
contrast, our gap condition holds in the same critical degree regime as in the information-theoretic
threshold (namely, p= Θ(log n/n)) but our constant is not optimal. We incur this constant loss be-
cause for the sake of presentation, we opt for a cleaner argument that can handle the nonhomogeneity
and generalizes more readily across degree regimes. To our knowledge, none of these features are
present in prior work analyzing spectral methods in an SSBM setting [ AFWZ20 ;DLS21 ].
2.2 Deterministic clusters model
Given Theorem 1, it is natural to ask what happens if we allow the adversary full control over
the structure of the graphs in P1andP2instead of simply allowing the adversary to perturb the
edge probabilities. In this section, we answer this question. We ﬁrst describe a more adversarial
semirandom model than the NSSBM family. We call this model the deterministic clusters model,
deﬁned as follows.
Model 2 (Deterministic clusters model) .Letnbe an even positive integer, V={1, . . . , n},
{P1, P2}be a partitioning of Vinto two equally-sized subsets, qbe a probability, and dinbe an
integer degree lower bound. Consider a graph G= (V, E)generated according to the following
process.
1.The adversary chooses arbitrarily graphs G[P1]andG[P2]with minimum degree din;
2.Nature samples every edge (v, w)∈P1×P2to be in Eindependently with probability q.
3.The adversary arbitrarily adds edges (v, w)∈P1×P1and(v, w)∈P2×P2toEafter
observing the edges sampled by nature.
We call a distribution Dof graphs generated according to the above process a deterministic clus-
ters model (DCM). We call the set of all such Dthe family of deterministic clusters models with
parameters dinandq, written as DCM (n, d in, q).
The DCM graph generation process is heavily motivated by the one studied by Makarychev,
Makarychev, and Vijayaraghavan [ MMV12 ]. This model is much more ﬂexible than the SSBM
and NSSBM settings in that the graphs the adversary draws on P1andP2are allowed to look very
far from random graphs. This means the DCM is a particularly good benchmark for algorithms to
ensure they are not implicitly using properties of random graphs that might not hold in the worst
case.
Within the DCM setting, we have Theorem 2.
Theorem 2. Letqbe a probability and dinbe an integer, and let D∈ DCM (n, d in, q). For G∼D ,
let/hatwideLdenote the expectation of Lafter step (2) but before step (3) in Model 2. There exists constants
5C1, C2, C3>0such that for all nsufﬁciently large, if
din≥C1·/parenleftBignq
2+√n/parenrightBig
and λ3(/hatwideL)−λ2(/hatwideL)≥√n+C2nq+C3/parenleftBig/radicalbig
nqlogn+ log n/parenrightBig
,
then unnormalized spectral bisection is strongly consistent on D.
We prove Theorem 2in Appendix A.7.2 . We remark that, as in Theorem 1, the constants that appear
in Theorem 2are somewhat arbitrary. They are chosen to make our proofs cleaner and can likely be
optimized.
As a basic application of Theorem 2, note that in the SSBM, if p=ω(1/√n)andq= 1/√n, then
fornsufﬁciently large, with high probability, the resulting graph satisﬁes the conditions needed to
apply Theorem 2. For a more interesting example, let P1andP2be two d-regular spectral expanders
withd=ω(√n)and let q≤1/√n. On top of both of these two graph classes, one can further
allow arbitrary edge insertions inside P1andP2while still being guaranteed exact recovery from
unnormalized spectral bisection.
2.3 Inconsistency of normalized spectral clustering
Notice that in Theorem 1and Theorem 2, we only address the strong consistency of the unnormal-
ized Laplacian in our nonhomogeneous and semirandom models. But what happens when we run
spectral bisection with the normalized Laplacian?
In Theorem 3, we prove that there is a subfamily of instances belonging to NSSBM (n, p,p, q)with
p= 6p, q=p/2on which unnormalized spectral bisection is strongly consistent (following from
Theorem 1) but normalized spectral clustering is inconsistent in a rather strong sense. Thus, one
cannot obtain results similar to Theorem 1and Theorem 2for normalized spectral bisection.
Theorem 3. For all nsufﬁciently large, there exists a nonhomogeneous stochastic block model such
that unnormalized spectral bisection is strongly consistent whereas normalized spectral bisection
(both symmetric and random-walk) incurs a misclassiﬁcation rate of at least 24% with probability
1−1/n.
We prove Theorem 3in Appendix A.8. Furthermore, we expect that it is straightforward to adapt
the example in Theorem 3to prove an analogous result for our DCM setting.
The result of Theorem 3may run counter to conventional wisdom, which suggests that normalized
spectral clustering should be favored over the unnormalized variant [ V on07 ]. Perhaps a more nu-
anced view in light of Theorem 1and Theorem 2is to acknowledge that the normalized Laplacian
and its eigenvectors enjoy stronger concentration guarantees [ SB15 ;DLS21 ], but the unnormalized
Laplacian’s second eigenvector is more robust to monotone adversarial changes.
2.4 Open problems
Perhaps the most natural follow-up question inspired by our results is to determine whether the
restriction that every internal edge probability pvw≤pcan be lifted entirely while still maintaining
strong consistency of the unnormalized Laplacian (Theorem 2). Another exciting direction for future
work is to lower the degree and/or spectral gap requirement present in our results in the DCM setting
(Theorem 2). Finally, we only study insertion-only monotone adversaries, as crossing edge deletions
change the second eigenvector of the expected Laplacian. It would be illuminating to understand the
robustness of Laplacian-based spectral algorithms against a monotone adversary that is also allowed
to delete crossing edges. We are optimistic that the answers to one or more of these questions
will further improve our understanding of the robustness of spectral clustering to “helpful” model
misspeciﬁcation.
3 Analysis sketch
First, let us give some intuition as to why one may expect that unnormalized spectral bisection is
robust against our monotone adversaries. Here and in the sequel, let u⋆
2= [ /x31n/2⊕− /x31n/2]/√n,
where /x31kdenotes the all- 1s vector in kdimensions and⊕denotes vector concatenation. Let Lbe
the unnormalized Laplacian of the graph we want to partition, L⋆:=E[L],E:=L−L⋆, and
λ⋆
i:=λi(L⋆)for1≤i≤n. For an edge (v, w), letevw:=ev−ew, so that evwis an edge
incidence vector corresponding to the edge (v, w). Let pvwbe the probability that the edge (v, w)
6appears in Gand observe that L⋆can be written as
L⋆=/summationdisplay
(v,w)∈Einternalpvw·evweT
vw+/summationdisplay
(v,w)∈Ecrossingq·evweT
vw,
where Einternal = (P1×P1)∪(P2×P2)andEcrossing =P1×P2. We can verify that u⋆
2is an
eigenvector of L⋆– indeed, we do so in Lemma A.14 . And, for now, assume that u⋆
2does correspond
to the second smallest eigenvalue of L⋆(in our NSSBM family, this is easily ensured by enforcing
p > q ). Moreover, for every internal edge (v, w)∈Einternal , we have⟨evw,u⋆
2⟩= 0. Hence, any
changes in internal edges do not change the fact that u⋆
2is an eigenvector of the perturbed matrix.
Thus, if the sampled Lis close enough to L⋆, then it is plausible that the second eigenvector of L,
denoted as u2, is pretty close to u⋆
2. In fact, the following conceptually stronger statement holds. If
the subgraph formed by selecting just the crossing edges of Gis regular, then u⋆
2is an eigenvector of
L. This follows from the fact that u⋆
2is an eigenvector of the unnormalized Laplacian of any regular
bipartite graph where both sides have size n/2and the previous observation that every internal edge
is orthogonal to u⋆
2.
To make this perturbation idea more formal, we recall the Davis-Kahan Theorem. Loosely, it states
that∥u2−u⋆
2∥2≲∥(L−L⋆)u⋆
2∥2/(λ⋆
3−λ⋆
2)(we give a more formal statement in Lemma A.15 ).
Expanding the entrywise absolute value |(L−L⋆)u⋆
2|reveals that its entries can be expressed as
2|dout[v]−E[dout[v]]|/√n, where dout[v]denotes the number of edges incident to vcrossing to
the opposite community as v. This is unaffected by any increase in the number of edges incident to v
that stay within the same community as v, denoted as din[v]. Hence, regardless of how many internal
edges we add before sampling or what substructures they encourage/create, if we have λ⋆
2≪λ⋆
3,
then we get∥u2−u⋆
2∥2≤o(1). This immediately implies that u2is a correct classiﬁer on all but
ano(1)fraction of the vertices.
Entrywise analysis of u2and NSSBM strong consistency. In order to achieve strong consis-
tency, we need that for all nsufﬁciently large, u2is a perfect classiﬁer. Unfortunately, the above
argument does not immediately give that. In particular, in the density and spectral gap regimes
we consider, the bound of o(1)yielded by the Davis-Kahan theorem is not sufﬁciently small to di-
rectly yield∥u⋆
2−u2∥2≪1/√n. Instead, we carry out an entrywise analysis of u2. A general
framework for doing so is given by Abbe, Fan, Wang, and Zhong [ AFWZ20 ] and is adapted to the
unnormalized and normalized Laplacians by Deng, Ling, and Strohmer [ DLS21 ].
At a high level, we adapt the analysis of Deng, Ling, and Strohmer [ DLS21 ] to our setting. We
consider the intermediate estimator vector (D−λ2I)−1Au⋆
2. This is a natural choice because
we can verify (D−λ2I)−1Au2=u2. We will see that it is enough to show that this interme-
diate estimator correctly classiﬁes all the vertices while satisfying |(D−λ2I)−1A(u⋆
2−u2)|≤
|(D−λ2I)−1Au⋆
2|(again, the absolute value is taken entrywise). With this in mind, taking some
entry indexed by v∈Vand multiplying both sides by d[v]−λ2(which we will show is positive
with high probability), we see that it is enough to show
|⟨av,u⋆
2−u2⟩|≤|⟨ av,u⋆
2⟩|=|din[v]−dout[v]|√n, (1)
where avdenotes the v-th row of A. The advantage of this rewrite is that the right hand side can be
uniformly bounded, so it is enough to control the left hand side.
To argue about the left hand side of ( 1), it may be tempting to use the fact that avis a Bernoulli
random vector and use Bernstein’s inequality to argue about the sum of rescalings of these Bernoulli
random variables. Unfortunately, we cannot do this since u2andavare dependent. To resolve this,
we use a leave-one-out trick [ AFWZ20 ;BV24 ]. We can think of this as leaving out the vertex v
corresponding to the entry we want to analyze and sampling the edges incident to the rest of the
vertices. The second eigenvector of the resulting L(v), denoted as u(v)
2, is a very good proxy for u2
and is independent from av. Hence, we may complete the proof of Theorem 1.
One of our main observations is that although this style of analysis was originally built for low-rank
signal matrices [ AFWZ20 ;BV24 ], it can be adapted to handle the nonhomogeneity inside P1and
P2. In particular, the nonhomogeneity we permit in the NSSBM family may make L⋆look very far
from a spiked low-rank signal matrix. Furthermore, our entrywise analysis of eigenvectors under
perturbations is one of the ﬁrst that we are aware of that moves beyond analyzing low-rank signal
matrices or spiked low-rank signal matrices.
7L1 L2 R
L1Kp· /x31n/4×n/4p· /x31n/4×n/4q· /x31n/2×n/2L2p· /x31n/4×n/4Kp· /x31n/4×n/4
R q· /x31n/2×n/2 p· /x31n/2×n/2
Table 1: A⋆for Theorem 3is deﬁned to have the above block structure.
Extension to deterministic clusters. To prove Theorem 2, we start again at ( 1). An alternate
way to upper bound the left hand side is to use the Cauchy-Schwarz inequality. A variant of the
Davis-Kahan theorem gives us control over ∥u2−u⋆
2∥2while∥av∥2=/radicalbig
d[v]. The advantage of
this is that we get a worst-case upper bound on the left hand side of ( 1) – it holds no matter what
edges orthogonal to u⋆
2are inserted before or after nature samples the crossing edges (which are
precisely the internal edges). Combining these and using the fact that the right hand side of ( 1) is
increasing in din[v](and increases faster than ∥av∥2=/radicalbig
d[v]) allows us to complete the proof of
Theorem 2.
Inconsistency of normalized spectral bisection. Finally, we describe the family of hard in-
stances we use to prove Theorem 3. To motivate this family of instances, recall that by the graph
version of Cheeger’s inequality, the second eigenvalue of Land the corresponding eigenvector can
be used to ﬁnd a sparse cut in G. Thus, if we create sparse cuts inside P1that are sparser than the cut
formed by separating P1andP2, then conceivably the normalized Laplacian’s second eigenvector
may return the new sparser cut.
To make this formal, consider the following graph structure. Let nbe a multiple of 4. Let L1
consist of indices 1, . . . , n/ 4,L2consist of indices n/4 + 1 , . . . , n/ 2, and Rconsist of indices
n/2 + 1 , . . . , n . Consider the block structure induced by the matrix A⋆=E[A]shown in Table 1.
Intuitively, as Kgets larger, the cut separating L1from V\L1becomes sparser. From Cheeger’s
inequality, this witnesses a small λ2(L)and therefore the corresponding u2(L)may return the cut
L1, V\L1. We formally prove that this is indeed what happens when Kis a sufﬁciently large
constant and then Theorem 3follows.
4 Numerical trials
We programmatically generate synthetic graphs that help illustrate our theoretical ﬁndings using the
libraries NetworkX 3.3 (BSD 3-Clause license), SciPy 1.13.0 (BSD 3-Clause License), and NumPy
1.26.4 (modiﬁed BSD license) [ HSS08 ;VGO+20;HMvdW+20]. We ran all our experiments on a
free Google Colab instance with the CPU runtime, and each experiment takes under one hour to run.
In this section we focus on a setting that allows relating Theorem 1and Theorem 3, and defer more
experiments that investigate both NSSBM and DCM graphs to Appendix B.
To put Theorem 1and Theorem 3in perspective, we consider graphs generated following the process
outlined in the proof of Theorem 3, which gives rise to the following benchmark distribution.
Benchmark distribution. Letnbe divisible by 4and let{P1, P−2}be a partitioning of V= [n]
into two equally-sized subsets. Let {L1, L2}be a bipartition of P1such that|L1|=|L2|=n/4and
callL=P1, R=P2for convenience as in the proof of Theorem 3. Then, for some p,p, q∈[0,1]
such that q≤p≤p, consider the distribution Dp,p,qover graphs G= (V, E)obtained by sampling
every edge (u, v)∈(L1×L1)∪(L2×L2)independently with probability p, every edge (u, v)∈
(L1×L2)∪(R×R)independently with probability p, and every edge (u, v)∈L×Rindependently
with probability q. One can see thatDp,p,qis in fact in the set NSSBM (n, p,p, q).
Setup. Let us ﬁx n= 2000 ,p= 24 log n/n,q= 8 log n/n. For varying values of pin the
range [p,1], we sample t= 10 independent draws GfromDp,p,q. For each of them, we run spectral
bisection (i.e. Algorithm 1) with matrices L,Lsym,Lrw,A. Then, we compute the agreement of
the bipartition hence obtained (with respect to the planted bisection), that is the fraction of correctly
classiﬁed vertices. We average the agreement across the tindependent draws. The results are shown
in the top left plot of Fig. 1. Another natural way to get a bipartition of Vfrom the eigenvector is
asweep cut . In a sweep cut, we sort the entries of u2and take the vertices corresponding to the
smallest n/2entries to be on one side of the bisection and put the remaining on the other side. The
average agreement obtained in this other fashion is shown in the bottom left plot of Fig. 1.
8Theoretical framing. As per Theorem 1, we expect unnormalized spectral bisection to achieve
exact recovery (i.e. agreement equal to 1) whenever p≤pmax, where
pmax=(n(p−q)−logn)2
nlogn(2)
is obtained by rearranging the precondition of Theorem 1, ignoring the constants and disregarding
the fact that αshould be O(1). On the contrary, the proof of Theorem 3shows that normalized
spectral bisection misclassiﬁes a constant fraction of vertices provided that p/q≥2(which our
choice of parameters satisﬁes) and p≥pthr, where
pthr= 3·p2/q . (3)
In Fig. 1, the solid vertical line corresponds to the value of pthron the x-axis, and the dashed vertical
line corresponds to the value of pmaxon the x-axis. In particular, observe that in our setting pthr<
pmax, so there is an interval of values for pwhere we expect Theorem 1and Theorem 3to apply
simultaneously.
Empirical evidence: consistency. One can see from the top left plot in Fig. 1that the agreement
of unnormalized spectral bisection is 100% for all values of p, even beyond pthrandpmax. On the
other hand, the agreement of the bipartition obtained from all other matrices (hence including nor-
malized spectral bisection) drops below 70% well before the threshold pthrpredicted by Theorem 3.
From the right plot in Fig. 1, we see that computing the bipartition by taking a sweep cut of n/2
vertices does not change the results – u2of the unnormalized Laplacian continues to achieve 100%
agreement, while for all other matrices the corresponding u2remains inconsistent.
Empirical evidence: embedding variance. From the setting of the experiment we just illustrated,
observe that as we increase p, we expect the subgraph G[L]to have increasing volume. As illustrated
in Fig. 1, this seems to correlate with a decrease in the “variance” of the second eigenvector u2of
the unnormalized Laplacian with respect to the ideal second eigenvector u⋆
2. More precisely, we
compute the average distance squared of the embedding of a vertex in u2from its ideal embedding
inu⋆
2, i.e. the quantity
min
s∈{± 1}1
n∥u2−s·u⋆
2∥2
2. (4)
This suggests that not only does the second eigenvector of the unnormalized Laplacian remain robust
to monotone adversaries, but it actually concentrates more strongly around the ideal embedding u⋆
2.
Empirical evidence: example embedding. Let us ﬁx the value p=pthr, for which we see in
Fig.3that all matrices except the unnormalized Laplacian fail to recover the planted bisection. We
generate a graph from Dp,p,q, and plot how the vertices are embedded in the real line by the second
eigenvector of all the matrices we consider. The result is shown in Fig. 1, where the three horizontal
dashed lines, from top to bottom, respectively correspond to the value of 1/√n,0,−1/√non the
y-axis.
4.1 Related work
Community detection. Community detection has garnered signiﬁcant attention in theoretical
computer science, statistics, and data science. For a general overview of recent progress and related
literature, see the survey by Abbe [ Abb18 ]. In what follows, we discuss the works we believe are
most related to what we study in this paper.
As mentioned in the introduction, perhaps the most fundamental and well-studied model is the sym-
metric stochastic block model (SSBM), due to [ HLL83 ]. The celebrated work of Abbe, Bandeira,
and Hall [ ABH16 ] gives sharp bounds on the threshold for exact recovery for the SSBM setting.
They complement their result by showing that SDP based methods can achieve the information the-
oretic lower bound for the planted bisection problem, even with a monotone adversary [ Moi21 ]. A
line of work [ AFWZ20 ;DLS21 ] demonstrates that natural spectral algorithms achieve exact recov-
ery for the SSBM all the way to the information-theoretic threshold.
Generalizations of the symmetric stochastic block model. Since the introduction of SBMs
[HLL83 ], numerous variants have been proposed that are designed to better reﬂect real-world graph
properties. For instance, real-life social networks are likely to contain triangles. To address this,
Sankararaman and Baccelli [ SB17 ] introduced a spatial stochastic block model, sometimes known
as the geometric stochastic block model (GSBM). Other variations were introduced in the works of
9Figure 1: Top left, bottom left : Agreement with the planted bisection of the bipartition ob-
tained from several matrices associated with an input graph generated from a distribution in
NSSBM (n, p,p, q)for ﬁxed values of n, p, q and varying values of p. In the top left plot, the biparti-
tion is the 0-cut of the second eigenvector, as in Algorithm 1. In the bottom left plot, the bipartition
is the sweep cut of the ﬁrst n/2vertices in the second eigenvector. The dashed vertical line corre-
sponds to pmax=pmax(n, p, q )(see ( 2)), and the solid vertical line corresponds to pthr=pthr(n, p, q )
(see ( 3)).Top middle, top right, bottom middle : Embedding of the vertices given by the second
eigenvector u2of several matrices associated with a graph sampled from Dp,p,qwithp=pthr. Hor-
izontal dashed lines, from top to bottom, correspond to 1/√n,0,−1/√nrespectively.
Bottom right : Variance of the embedding in the second eigenvector u2of the unnormalized Lapla-
cian with respect to the ideal eigenvector u⋆
2(see ( 4)), for input graphs generated from a distribution
inNSSBM (n, p,p, q)with ﬁxed values of n, p, q and varying values of p.
[GPMS18 ;GMPS19 ]. Subsequent work studies the performance of spectral algorithms on certain
Gaussian or Geometric Mixture block models [ ABRS20 ;ABD21 ;LS24 ;GNW24 ].
Studying community detection with a semirandom model approaches this modeling question differ-
ently. Rather than implicitly encouraging a particular structure within the clusters like the models
just mentioned, a semirandom adversary (including the ones we study in this paper) can more di-
rectly test the robustness of the algorithm to specially designed substructures.
Semirandom and monotone adversaries. As far as we are aware, Blum and Spencer [ BS95 ]
were the ﬁrst to introduce a semirandom model. Within this model, they studied graph coloring
problems. Feige and Kilian [ FK01 ] demonstrated that semideﬁnite programming methods can ac-
curately recover communities up to a certain threshold, even in the semi-random setting. Other
problems, such as detecting a planted clique [ Jer92 ;Ku95 ;BHKKMP19 ], have also been studied in
the semi-random model of [ FK01 ]. In the setting of planted clique, a natural spectral algorithm fails
against monotone adversaries [ MMT20 ;BKS23 ]. Monotone adversaries and semirandom models
have also been extensively studied for other statistical and algorithmic problems [ V A18 ;KLLST23 ;
GC23 ;BGLMSY24 ]. Finally, [ SL17 ] shows that a spectral heuristic due to Boppana [ Bop87 ] is
robust under a monotone adversary that is allowed to both insert internal edges and delete crossing
edges. However, as far as we are aware, this algorithm does not ﬁt in the framework of Algorithm 1.
We remark that the models we study in this paper are most closely related to models studied by
[MN06 ] and [ MMV12 ]. In particular, allowing increased internal edge probabilities is analogous to
Massart noise in classiﬁcation problems, and our model with adversarially chosen internal edges can
be seen as the same model as that studied in [ MMV12 ] (although without allowing crossing edge
deletions). Finally, note that Cohen-Addad, d’Orsi, and Mousavifar [ CdM24 ] give a near-linear time
algorithm for graph clustering in the model of [ MMV12 ], though they do not explicitly show their
algorithm is strongly consistent on instances that are information-theoretically exactly recoverable.
10Acknowledgments. AB was partially supported by the National Science Foundation under Grant
Nos. CCF-2008688 and CCF-2047288. NSM was supported by a National Science Foundation
Graduate Research Fellowship. We thank Avrim Blum and Yury Makarychev for helpful discussions.
We thank Nirmit Joshi for pointing us to the reference [ DLS21 ].
References
[Abb18] Emmanuel Abbe. Community detection and stochastic block models: recent de-
velopments. Journal of Machine Learning Research , 18(177):1–86, 2018. arXiv:
1703.10146 [math.PR] .URL:http://jmlr.org/papers/v18/16- 480.
html (cited on pages 2,9).
[ABH16] Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the
stochastic block model. IEEE Transactions on Information Theory , 62(1):471–487,
2016. DOI:10.1109/TIT.2015.2490670 . arXiv: 1405.3267 [cs.SI] (cited on
pages 5,9,36).
[ABRS20] Emmanuel Abbe, Enric Boix-Adserà, Peter Ralli, and Colin Sandon. Graph pow-
ering and spectral robustness. SIAM Journal on Mathematics of Data Science ,
2(1):132–157, 2020. DOI:10.1137/19M1257135 . arXiv: 1809.04818 [cs.DS] .
URL:https://doi.org/10.1137/19M1257135 (cited on page 10).
[AFWZ20] Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise
eigenvector analysis of random matrices with low expected rank. Annals of statis-
tics, 48(3):1452, 2020. arXiv: 1709.09565 [math.ST] (cited on pages 2,4,5,7,
9,23).
[ABD21] Konstantin Avrachenkov, Andrei Bobu, and Maximilien Dreveton. Higher-order
spectral clustering for geometric graphs. Journal of Fourier Analysis and Appli-
cations , 27(2):22, March 2021. DOI:10.1007/s00041- 021- 09825- 2 . arXiv:
2009 . 11353 [cs.LG] .URL:https : / / doi . org / 10 . 1007 / s00041 - 021 -
09825-2 (cited on page 10).
[BHKKMP19] Boaz Barak, Samuel Hopkins, Jonathan Kelner, Pravesh K Kothari, Ankur Moitra,
and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted
clique problem. SIAM Journal on Computing , 48(2):687–735, 2019. arXiv: 1604.
03084 [cs.CC] (cited on page 10).
[BV24] Abhinav Bhardwaj and Van Vu. Matrix perturbation: davis-kahan in the inﬁnity
norm . InProceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Al-
gorithms (SODA) . January 2024, pages 880–934. arXiv: 2304.00328 [math.PR]
(cited on pages 4,7).
[BGLMSY24] Avrim Blum, Meghal Gupta, Gene Li, Naren Sarayu Manoj, Aadirupa Saha, and
Yuanyuan Yang. Dueling optimization with a monotone adversary. In Proceedings
of Thirty Fifth Conference on Algorithmic Learning Theory (ALT) , February 2024.
arXiv: 2311.11185 [cs.DS] (cited on page 10).
[BS95] Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable
graphs. Journal of Algorithms , 19(2):204–234, 1995. ISSN : 0196-6774. DOI:
https : / / doi . org / 10 . 1006 / jagm . 1995 . 1034 .URL:https : / / www .
sciencedirect.com/science/article/pii/S0196677485710346 (cited
on page 10).
[Bop87] Ravi B. Boppana. Eigenvalues and graph bisection: an average-case analysis.
In28th Annual Symposium on Foundations of Computer Science (sfcs 1987) ,
pages 280–285, 1987. DOI:10.1109/SFCS.1987.22 (cited on page 10).
[BKS23] Rares-Darius Buhai, Pravesh K. Kothari, and David Steurer. Algorithms approach-
ing the threshold for semi-random planted clique. In Proceedings of the 55th
Annual ACM Symposium on Theory of Computing , STOC 2023, pages 1918–
1926, Orlando, FL, USA. Association for Computing Machinery, 2023. ISBN :
9781450399135. arXiv: 2212.05619 [cs.DS] (cited on pages 2,10).
[CSV17] Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted
data. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing , STOC 2017, pages 47–60, Montreal, Canada. Association for Comput-
ing Machinery, 2017. ISBN : 9781450345286. arXiv: 1611.02315 [cs.LG] (cited
on page 2).
11[CdM24] Vincent Cohen-Addad, Tommaso d’Orsi, and Aida Mousavifar. A near-linear time
approximation algorithm for beyond-worst-case graph clustering. In Forty-ﬁrst In-
ternational Conference on Machine Learning , 2024. arXiv: 2406.04857 [cs.DS] .
URL:https://openreview.net/forum?id=MSFxOMM0gK (cited on pages 2,
10).
[DLS21] Shaofeng Deng, Shuyang Ling, and Thomas Strohmer. Strong consistency, graph
laplacians, and the stochastic block model. Journal of Machine Learning Research ,
22(117):1–44, 2021. arXiv: 2004.09780 [stat.ML] (cited on pages 2,4–7,9,11,
27,35).
[FK01] Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. J. Comput.
Syst. Sci. , 63(4):639–671, December 2001. ISSN : 0022-0000. DOI:10.1006/jcss.
2001.1773 .URL:https://doi.org/10.1006/jcss.2001.1773 (cited on
pages 2,5,10).
[GMPS19] Sainyam Galhotra, Arya Mazumdar, Soumyabrata Pal, and Barna Saha. Connec-
tivity of Random Annulus Graphs and the Geometric Block Model. In Dimitris
Achlioptas and László A. Végh, editors, Approximation, Randomization, and Com-
binatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2019) ,
volume 145 of Leibniz International Proceedings in Informatics (LIPIcs) , 53:1–
53:23, Dagstuhl, Germany. Schloss Dagstuhl – Leibniz-Zentrum für Informatik,
2019. ISBN : 978-3-95977-125-2. arXiv: 1804.05013 [cs.DM] (cited on page 10).
[GPMS18] Sainyam Galhotra, Soumyabrata Pal, Arya Mazumdar, and Barna Saha. The geo-
metric block model and applications. In 2018 56th Annual Allerton Conference on
Communication, Control, and Computing (Allerton) , pages 1147–1150, 2018. DOI:
10.1109/ALLERTON.2018.8635938 (cited on page 10).
[GC23] Xing Gao and Yu Cheng. Robust matrix sensing in the semi-random model. In
Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL:
https://openreview.net/forum?id=nSr2epejn2 (cited on page 10).
[GNW24] Julia Gaudio, Xiaochun Niu, and Ermin Wei. Exact community recovery in the geo-
metric sbm . InProceedings of the 2024 Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA) . 2024, pages 2158–2184. DOI:10.1137/1.9781611977912.
78. arXiv: 2307.11196 [cs.SI] .URL:https://epubs.siam.org/doi/abs/
10.1137/1.9781611977912.78 (cited on pages 2,10).
[HSS08] Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dy-
namics, and function using NetworkX. Technical report, Los Alamos National
Lab.(LANL), Los Alamos, NM (United States), 2008 (cited on page 8).
[HMvdW+20] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers,
Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg,
Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van
Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe,
Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren
Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array pro-
gramming with NumPy. Nature , 585(7825):357–362, September 2020. DOI:10.
1038/s41586-020-2649-2 .URL:https://doi.org/10.1038/s41586-020-
2649-2 (cited on page 8).
[HLL83] Paul W. Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic
blockmodels: ﬁrst steps. Social Networks , 5(2):109–137, 1983. ISSN : 0378-8733.
DOI:https://doi.org/10.1016/0378- 8733(83)90021- 7 .URL:https:
//www.sciencedirect.com/science/article/pii/0378873383900217
(cited on pages 1,9).
[Jer92] Mark Jerrum. Large cliques elude the metropolis process. Random Struct. Algo-
rithms , 3(4):347–360, 1992. DOI:10 . 1002 / RSA . 3240030402 .URL:https :
//doi.org/10.1002/rsa.3240030402 (cited on page 10).
[KLLST23] Jonathan Kelner, Jerry Li, Allen X. Liu, Aaron Sidford, and Kevin Tian. Semi-
random sparse recovery in nearly-linear time. In Gergely Neu and Lorenzo
Rosasco, editors, Proceedings of Thirty Sixth Conference on Learning Theory , vol-
ume 195 of Proceedings of Machine Learning Research , pages 2352–2398. PMLR,
12July 2023. arXiv: 2203.04002 [cs.DS] .URL:https://proceedings.mlr.
press/v195/kelner23a.html (cited on pages 2,10).
[Ku95] Ludk Kuera. Expected complexity of graph partitioning problems. Discrete Ap-
plied Mathematics , 57(2):193–212, 1995. ISSN : 0166-218X. DOI:https : / /
doi . org / 10 . 1016 / 0166 - 218X(94 ) 00103 - K .URL:https : / / www .
sciencedirect.com/science/article/pii/0166218X9400103K . Combi-
natorial optimization 1992 (cited on page 10).
[LLV17] Can M Le, Elizaveta Levina, and Roman Vershynin. Concentration and regular-
ization of random graphs. Random Structures & Algorithms , 51(3):538–561, 2017.
arXiv: 1506.00669 [math.PR] (cited on page 17).
[LS24] Shuangping Li and Tselil Schramm. Spectral clustering in the gaussian mixture
block model, 2024. arXiv: 2305.00979 [stat.ML] (cited on pages 2,10).
[MMV12] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Ap-
proximation algorithms for semi-random partitioning problems. In Proceedings of
the Forty-Fourth Annual ACM Symposium on Theory of Computing , STOC ’12,
pages 367–384, New York, New York, USA. Association for Computing Machin-
ery, 2012. ISBN : 9781450312455. arXiv: 1205.2234 [cs.DS] (cited on pages 2,
5,10).
[MN06] Pascal Massart and Élodie Nédélec. Risk bounds for statistical learning. The
Annals of Statistics , 34(5), October 2006. ISSN : 0090-5364. DOI:10 . 1214 /
009053606000000786 .URL:http : / / dx . doi . org / 10 . 1214 /
009053606000000786 (cited on pages 2,10).
[MMT20] Theo McKenzie, Hermish Mehta, and Luca Trevisan. A new algorithm for the
robust semi-random independent set problem. In Proceedings of the Thirty-First
Annual ACM-SIAM Symposium on Discrete Algorithms , SODA ’20, pages 738–
746, Salt Lake City, Utah. Society for Industrial and Applied Mathematics, 2020.
arXiv: 1808.03633 [cs.DS] (cited on pages 2,10).
[Moi21] Ankur Moitra. Semirandom stochastic block models . InBeyond the Worst-Case
Analysis of Algorithms . Tim Roughgarden, editor. Cambridge University Press,
2021, pages 212–233. DOI:10.1017/9781108637435.014 (cited on pages 2,
9).
[MPW16] Ankur Moitra, William Perry, and Alexander S. Wein. How robust are re-
construction thresholds for community detection? In Proceedings of the Forty-
Eighth Annual ACM Symposium on Theory of Computing , STOC ’16, pages 828–
841, Cambridge, MA, USA. Association for Computing Machinery, 2016. ISBN :
9781450341325. DOI:10 . 1145 / 2897518 . 2897573 . arXiv: 1511 . 01473
[cs.DS] .URL:https://doi.org/10.1145/2897518.2897573 (cited on
page 2).
[SB17] Abishek Sankararaman and François Baccelli. Community detection on euclidean
random graphs. In 2017 55th Annual Allerton Conference on Communication, Con-
trol, and Computing (Allerton) , pages 510–517, 2017. DOI:10.1109/ALLERTON.
2017.8262780 (cited on page 9).
[SB15] Purnamrita Sarkar and Peter J. Bickel. Role of normalization in spectral clustering
for stochastic blockmodels. The Annals of Statistics , 43(3), June 2015. ISSN : 0090-
5364. DOI:10.1214/14-aos1285 . arXiv: 1310.1495 [stat.ML] .URL:http:
//dx.doi.org/10.1214/14-AOS1285 (cited on page 6).
[SL17] Martin R. Schuster and Maciej Liskiewicz. New Abilities and Limitations of Spec-
tral Graph Bisection. In Kirk Pruhs and Christian Sohler, editors, 25th Annual Eu-
ropean Symposium on Algorithms (ESA 2017) , volume 87 of Leibniz International
Proceedings in Informatics (LIPIcs) , 66:1–66:15, Dagstuhl, Germany. Schloss
Dagstuhl – Leibniz-Zentrum für Informatik, 2017. ISBN : 978-3-95977-049-1. DOI:
10 . 4230 / LIPIcs . ESA . 2017 . 66 .URL:https : / / drops . dagstuhl . de /
entities/document/10.4230/LIPIcs.ESA.2017.66 (cited on page 10).
[Ver18] Roman Vershynin. High-dimensional probability: An introduction with applica-
tions in data science , volume 47. Cambridge university press, 2018 (cited on
page 14).
13[V A18] Aravindan Vijayaraghavan and Pranjal Awasthi. Clustering semi-random mix-
tures of Gaussians. In Jennifer Dy and Andreas Krause, editors, Proceedings of
the 35th International Conference on Machine Learning , volume 80 of Proceed-
ings of Machine Learning Research , pages 5055–5064. PMLR, July 2018. arXiv:
1711 . 08841 [cs.DS] .URL:https : / / proceedings . mlr . press / v80 /
vijayaraghavan18a.html (cited on page 10).
[VGO+20] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy,
David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan
Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman,
Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson,
C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde,
Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris,
Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and
SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Comput-
ing in Python. Nature Methods , 17:261–272, 2020. DOI:10.1038/s41592-019-
0686-2 (cited on page 8).
[V on07] Ulrike V on Luxburg. A tutorial on spectral clustering. Statistics and computing ,
17:395–416, 2007. arXiv: 0711.0189 [cs.DS] (cited on pages 2,3,6).
[YCOM24] Yuepeng Yang, Antares Chen, Lorenzo Orecchia, and Cong Ma. Top- Kranking
with a monotone adversary. In Shipra Agrawal and Aaron Roth, editors, Proceed-
ings of Thirty Seventh Conference on Learning Theory , volume 247 of Proceed-
ings of Machine Learning Research , pages 5123–5162. PMLR, July 2024. arXiv:
2402.07445 [stat.ML] .URL:https://proceedings.mlr.press/v247/
yang24b.html (cited on page 2).
A Deferred proofs
In this section, we build the tools we need to prove Theorem 1, Theorem 2, andTheorem 3. Through-
out, it will be helpful to refer to the overview (Section 3) for a proof roadmap.
Notation in the proofs. In all proofs, we adopt the notation used in the technical overview
(Section 3). Additionally, for a vertex v∈V, letP(v)denote the community that vbelongs to.
A.1 Concentration inequalities
Our proof strategy for Theorem 1and Theorem 2is to appeal to Lemma A.23 , which guarantees
strong consistency provided that d[v]−λ2>0,din[v]>dout[v], and|⟨av,u⋆
2−u2⟩|≤ (din[v]−
dout[v])/√nfor all vertices v. Proving that the ﬁrst two conditions hold is relatively easy. In the
setting of Theorem 1, it essentially follows from concentration of the degrees, which is proved in
Appendix A.2. In the setting of Theorem 2, it follows from the assumptions of the Theorem. Proving
that the third condition holds is the main technical challenge.
For all three parts, our proofs rely on several auxiliary concentration results. We prove these in
Appendix A.3and Appendix A.4.
We extensively use the following variants of Bernstein’s Inequality, which can be derived from
[Ver18 , Theorem 2.8.4].
Lemma A.1. LetX=/summationtextm
i=1Xi, where Xi= 1 with probability piandXi= 0 with probability
1−piand all the Xiare independent. Let µ=E[X]. Then, for all t >0we have
Pr[|X−µ|≥t]≤2exp/parenleftbigg
−min/braceleftbiggt2
4/summationtextm
i=1pi(1−pi),3t
4/bracerightbigg/parenrightbigg
.
From this, we get the following very useful corollary.
Lemma A.2. LetX=/summationtextm
i=1Xi, where Xi= 1 with probability piandXi= 0 with probability
1−piand all the Xiare independent. Let µ=E[X]. Then, for all t >0, with probability at least
1−δwe have
|X−µ|≤/radicaltp/radicalvertex/radicalvertex/radicalbt4m/summationdisplay
i=1pi(1−pi) log ( 2/δ) + 4/3log ( 2/δ).
14A.2 Concentration of degrees
In this Section, we give concentration statements regarding the number of internal vertices incident
to each vertex and the number of crossing edges incident to each vertex. We then compare these
against λ2.
Lemma A.3. Suppose the crossing edges are sampled identically and independently with probability
q. Then, for some universal constant C > 0, with probability at least 1−δwe have that
∀v∈V,|dout[v]−E[dout[v]]|≤C/parenleftBig/radicalbig
nqlog ( n/δ) + log ( n/δ)/parenrightBig
.
Proof of Lemma A.3.Choose some v∈V. Consider the random variable dout[v]. Using
Lemma A.2, we have that there is a constant C > 0such that with probability at least 1−δ/n
one has
|dout[v]−E[dout[v]]|≤C/parenleftBig/radicalbig
4nq/2 log ( 2n/δ) + log ( 2n/δ)/parenrightBig
.
Taking a union bound over all nvertices completes the proof of Lemma A.3.
Note that Lemma A.3above applies in both the settings of Theorem 1and Theorem 2.
Lemma A.4. Suppose the internal edges are sampled independently with probabilities pvwsuch
thatp≤pvw≤p. Then, for some universal constant C > 0, with probability≥1−δwe have that
∀v∈V,|din[v]−E[din[v]]|≤C
/radicalBigg/summationdisplay
w∈P(v)\{v}pvw(1−pvw) log ( n/δ) + log ( n/δ)
.
Proof of Lemma A.4.As before, choose some v∈Vand consider the random variable din[v]. By
Lemma A.2, we have that there is a constant C > 0such that with probability at least 1−δ/none
has
|din[v]−E[din[v]]|≤C
/radicalBigg
4/summationdisplay
w∈P(v)\{v}pvw(1−pvw) log ( 2n/δ) + log ( 2n/δ)
.
Taking a union bound over all nvertices completes the proof of Lemma A.4.
Combining the above two lemmas, we obtain a lower-bound on din[v]−dout[v]. In particular, the
following lemma implies that in the setting of Theorem 1, we have din[v]>dout[v]. This will be
required for applying Lemma A.23 .
Lemma A.5. There exists a universal constant C > 0such that with probability ≥1−δ, in the
same settings as Lemma A.3and Lemma A.4and assuming the gap condition in Theorem 1, ifp≥q,
then for all v∈Vwe have
din[v]−dout[v]≥n(p−q)
2−C/parenleftBig/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig
.
Proof of Lemma A.5.Letv∈V. First, we call Lemma A.3with a failure probability of δ/(2n)to
conclude that
dout[v]≤nq
2+CA.3/parenleftbigg/radicalbiggnq
2log/parenleftbig
2n2/δ/parenrightbig
+ log/parenleftbig
2n2/δ/parenrightbig/parenrightbigg
.
15Next, we call Lemma A.4with a failure probability of δ/(2n)to conclude that
din[v]≥/summationdisplay
w∈P(v)\{v}pvw−CA.4
/radicalBigg/summationdisplay
w∈P(v)\{v}pvw(1−pvw) log/parenleftbig
2n2/δ/parenrightbig
+ log/parenleftbig
2n2/δ/parenrightbig

≥/summationdisplay
w∈P(v)\{v}pvw−CA.4
/radicalBigg/summationdisplay
w∈P(v)\{v}pvwlog/parenleftbig
2n2/δ/parenrightbig
+ log/parenleftbig
2n2/δ/parenrightbig

≥np
2−2CA.4/parenleftbigg/radicalbiggnp
2log/parenleftbig
n2/δ/parenrightbig
+ log/parenleftbig
2n2/δ/parenrightbig/parenrightbigg
.
where the last line uses the fact that x−c√xis increasing in xwhenever x≥c2/4andc >0. We
subtract and conclude the proof of Lemma A.5by a union bound.
The following lemma will be useful for lower-bounding d[v]−λ2in Theorem 1.
Lemma A.6. Suppose every crossing edge appears independently with probability q. Then, with
probability≥1−δ, for all v∈Vwe have
λ2≤2dout[v] +C/parenleftBig/radicalbig
nqlog ( n/δ) + log ( n/δ)/parenrightBig
.
Proof of Lemma A.6.Observe that with probability at least 1−δ,dout[w]−E[dout[v]]≤/radicalbig
2nqlog( 2n/δ) + 2 log(2 n/δ)for all w∈Vby Lemma A.2. Then, for every v∈Vwe have
2
n/summationdisplay
w∈P(v)dout[w]−dout[v] =
2
n/summationdisplay
w∈P(v)dout[w]−E[dout[v]]
+ (E[dout[v]]−dout[v])
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
n/summationdisplay
w∈P(v)dout[w]−E[dout[v]]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+|E[dout[v]]−dout[v]|
≤/radicalbig
2nqlog ( 2n/δ) +/radicalbig
2nqlog ( 2n/δ) + 4 log ( 2n/δ)
≤3/radicalbig
nqlog ( n/δ) + 10 log ( n/δ).
Next, by the min-max principle, we have
λ2≤/summationdisplay
(w,w′)∈E(u⋆
2[w]−u⋆
2[w′])2=4
n/summationdisplay
w∈P(v)dout[w].
Combining everything, we get
λ2≤2
2
n/summationdisplay
w∈P(v)dout[w]
≤2/parenleftBig
dout[v] + 3/radicalbig
nqlog ( n/δ) + 10 log ( n/δ)/parenrightBig
,
completing the proof of Lemma A.6.
We can now lower-bound d[v]−λ2. Note that the following lower bound implies that d[v]> λ 2, as
required by Lemma A.23 .
Lemma A.7. In the setting of Theorem 1, with probability ≥1−δ, for all v∈V, we have
d[v]−λ2> n(p−q)/4.
Proof of Lemma A.7.Recall that the gap condition in Theorem 1tells us that pandqare such that
for a universal constant C,
n(p−q)≥C/parenleftBig/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig
.
16We have for all nsufﬁciently large (speciﬁcally, n≥N(α, δ)for some Nthat is a function only of
the constant α, and we take δ≥1/nO(1)) that with probability at least 1−δ,
d[v]−λ2=din[v]−dout[v] + (2 dout[v]−λ2)
≥din[v]−dout[v]−CA.6/parenleftBig/radicalbig
nqlog ( n/δ) + log ( n/δ)/parenrightBig
≥n(p−q)
2−(CA.5+CA.6)/parenleftBig/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig
,
so insisting
n(p−q)
4≥(CA.5+CA.6)/parenleftBig/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig
+ 1
gives the condition required to complete the proof of Lemma A.7.
The following technical lemma will be useful for upper-bounding ∥u2∥∞in Lemma A.22 .
Lemma A.8. In the setting of Theorem 1, there exists a universal constant Csuch that with proba-
bility≥1−δ, for all v∈Vwe have
np+ log ( n/δ)
d[v]−λ2≤4α+C.
Proof of Lemma A.8.By Lemma A.7, we have with probability ≥1−δthat for all v∈V,
d[v]−λ2≥n(p−q)
4.
This gives
np+ log ( n/δ)
d[v]−λ2≤4(np+ log ( n/δ))
n(p−q)=4p
p−q+4 log ( n/δ)
n(p−q)≤4α+C.
This completes the proof of Lemma A.8.
A.3 Concentration of Laplacian and eigenvalue perturbations
For the matrix concentration lemmas, we need a result due to Le, Levina, and Vershynin [ LLV17 ].
We reproduce it below.
Lemma A.9 ([LLV17 , Theorem 2.1]) .Consider a random graph from the model G(n,{pij}). Let
d= max ijnpij. For any r≥1, the following holds with probability at least 1−n−rfor a universal
constant C. Consider any subset consisting of 10n/d vertices, and reduce the weights of the edges
incident to those vertices in an arbitrary way. Let d′be the maximal degree of the resulting graph.
Then, the adjacency matrix A′of the new weighted graph satisﬁes
∥A′−E[A]∥op≤Cr3/2/parenleftBig√
d+√
d′/parenrightBig
.
Moreover, the same holds for d′being the maximal ℓ2norm of the rows of A′.
Lemma A.10. LetLbe a Laplacian sampled from the nonhomogeneous Erd˝ os-Rényi model where
each edge (i, j)is present independently with probability pij. Then, there exists a universal constant
Csuch that for all nsufﬁciently large, with probability ≥1−δfor any δ≥n−10,
∥L−E[L]∥op≤C/parenleftBigg/radicalbigg
nmax
(i,j):pij̸=1pijlog ( n/δ) + log ( n/δ)/parenrightBigg
.
Proof of Lemma A.10.Without loss of generality, for all pijthat are 1, reset their probabilities to 0.
To see that this is valid, let L′be a Laplacian sampled from this modiﬁed distribution and notice that
L′−E[L′] =L−E[L].
17By Lemma A.9and Lemma A.2, we have with probability ≥1−δ/2that
∥A−E[A]∥op≤200CA.9/radicaltp/radicalvertex/radicalvertex/radicalbt2nmax
ijpij+CA.2/parenleftBigg/radicalbigg
nmax
ijpijlog( 8n/δ) + log( 8n/δ)/parenrightBigg
≤400CA.9CA.2/radicalbigg
nmax
ijpij+ log( 8n/δ)
≤400CA.9CA.2/parenleftBigg/radicalbigg
nmax
ijpijlog( 8n/δ) + log( 8n/δ)/parenrightBigg
and by Lemma A.3and Lemma A.4, we have with probability 1−δ/2that
∥D−E[D]∥op≤max
v∈V|dout[v]−E[dout[v]]|+ max
v∈V|din[v]−E[din[v]]|
≤2 max{CA.3, CA.4}/parenleftBigg/radicalbigg
nmax
ijpijlog ( 2n/δ) + log ( 2n/δ)/parenrightBigg
Now, observe that with probability ≥1−δ(following from a union bound),
∥L−E[L]∥op=∥D−E[D]−(A−E[A])∥op≤∥D−E[D]∥op+∥A−E[A]∥op
≤800CA.9CA.2max{CA.3, CA.4}/parenleftBigg/radicalbigg
nmax
ijpijlog ( 8n/δ) + log ( 8n/δ)/parenrightBigg
,
completing the proof of Lemma A.10 .
By applying the above lemma, we can show that there is a gap between λ3andλ⋆
2, which will allow
us to apply Davis-Kahan style bounds. More concretely, Lemma A.11 and Lemma A.12 , together
with Lemma A.16 , show that∥u2−u⋆
2∥2is small. This will be useful for proving that in the context
for Theorem 1, the condition|⟨av,u⋆
2−u2⟩|≤ (din[v]−dout[v])/√nin Lemma A.23 is satisﬁed.
Lemma A.11. In the setting of Theorem 1, there exists a universal constant Csuch that the following
holds.
Letpandqbe such that we have
n(p−q)≥C/parenleftBig/radicalbig
nplog ( n/δ) + log( n/δ)/parenrightBig
.
Then, for any δ≥n−10, with probability≥1−δ, we have λ3−λ⋆
2≥n(p−q)/4.
Proof of Lemma A.11.By Weyl’s inequality and Lemma A.10 , we have with probability ≥1−δ
that
λ3−λ⋆
2≥λ⋆
3−λ⋆
2−∥L−L⋆∥op≥n(p−q)
2−CA.10/parenleftBig/radicalbig
nplog ( n/δ) + log( n/δ)/parenrightBig
.
LetC≥4CA.10. Then,
n(p−q)
4≥CA.10/parenleftBig/radicalbig
nplog ( n/δ) + log( n/δ)/parenrightBig
.
Subtracting completes the proof of Lemma A.11 .
Next, we bound∥Eu⋆
2∥2, which we will need in order to apply our Davis-Kahan style bound in
Lemma A.16 . We remark that Lemma A.12 below holds both in the setting of Theorem 1and of
Theorem 2.
Lemma A.12. Suppose each crossing edge in our graph appears independently with probability q.
There exists a universal constant Csuch that for all nsufﬁciently large, with probability ≥1−δ,
we have
∥Eu⋆
2∥2≤C/parenleftbigglog ( 1/δ)
logn/parenrightbigg3/2/parenleftBig√nq+ (nqlog ( n/δ))1/4+/radicalbig
log ( n/δ)/parenrightBig
.
18Proof of Lemma A.12.Observe that|Eu⋆
2|= 2|dout−E[dout]|/√n. By Lemma A.3, for all v∈
V, with probability≥1−δ/2, we have dout[v]≤nq/2 +CA.3/parenleftBig/radicalbig
nq·log (2 n/δ) + log ( 2n/δ)/parenrightBig
.
So, if we let AoutandA⋆
outdenote the adjacency matrices consisting only of the crossing edges and
the expected value of that, respectively, then invoking Lemma A.9, with probability≥1−δ, we
have
∥Eu⋆
2∥2=2∥dout−E[dout]∥2√n=2∥(Aout−A⋆
out) /x31∥2√n≤2∥Aout−A⋆
out∥op
≤2CA.9/parenleftbigglog ( 2/δ)
logn/parenrightbigg3/2/parenleftbigg/radicalbiggnq
2+/radicalbig
CA.3/radicalBig
nq+/radicalbig
nqlog ( 2n/δ) + log ( 2n/δ)/parenrightbigg
,
completing the proof of Lemma A.12 .
Finally, we apply Lemma A.9in order to bound bound ∥av−a⋆
v∥2.
Lemma A.13. In the setting of Theorem 1, with probability≥1−δ, we have
∥av−a⋆
v∥2≤C/parenleftbigglog ( 1/δ)
logn/parenrightbigg3/2/parenleftBig/radicalbig
np+ (nplog ( n/δ))1/4+/radicalbig
log ( n/δ)/parenrightBig
.
Proof of Lemma A.13.We use a similar proof to that of Lemma A.12 . Indeed, invoke Lemma A.9
(observe that we can set pijfor the deterministic internal edges to 0as they do not affect A−E[A])
and notice that
∥av−a⋆
v∥2≤∥A−A⋆∥op≤CA.9/parenleftbigglog ( 2/δ)
logn/parenrightbigg3/2/parenleftBig/radicalbig
np+ (nplog ( 2n/δ))1/4+/radicalbig
log ( 2n/δ)/parenrightBig
,
where we used d′≤n(p+q)/2 + 2 max{CA.3, CA.4}/parenleftBig/radicalbig
nplog ( 2n/δ) + log ( 2n/δ)/parenrightBig
from com-
bining Lemma A.3and Lemma A.4. This completes the proof of Lemma A.13 .
A.4 Eigenvector perturbations
In this Appendix, we give our Euclidean norm eigenvector perturbation bounds.
First, we verify that u⋆
2is indeed the second eigenvector of L⋆.
Lemma A.14. In the setting of Theorem 1, we have L⋆u⋆
2=λ2(L⋆)u⋆
2=nqu⋆
2, where L⋆=E[L].
In the setting of Theorem 2, we have L⋆u⋆
2=λ2(L⋆)u⋆
2=nqu⋆
2, where L⋆denotes the Laplacian
matrix that agrees with Lon all internal edges and agrees with E[L]on all crossing edges.
Proof of Lemma A.14.In both cases, one can check that u⋆
2is an eigenvector of L⋆with eigenvalue
nq: for any v∈P2(i.e.u⋆
2[v] =−1/√nwithout loss of generality), one has
(L⋆u⋆
2)v=1√n
−(din[v] +nq/2)−/summationdisplay
w∈P1:{v,w}∈E(−1) +/summationdisplay
w∈P2(−q)
=−nq√n=nq·u⋆
2[v].
By virtue of the above observations, it sufﬁces to argue that nq < λ 3(L⋆)≤···≤ λn(L⋆).
In the setting of Theorem 1, we claim λ⋆
3≥n(p+q)
2> nq . This is because because pvw≥p, which
implies that if we consider L⋆
1to be the expected Laplacian for SSBM (n, p, q )andL⋆
2to be the
expected Laplacian for NSSBM (n, p,p, q), thenL⋆
2⪰L⋆
1..
In the setting of Theorem 2, we have λ3(/hatwideL)−λ2(/hatwideL)> nq , by the theorem assumption. Since L⋆
is obtained from /hatwideLby adding the adversarial edges, we have λi(L⋆)≥λi(/hatwideL)for all i. In particular,
we have λ3(L⋆)≥λ3(/hatwideL) =λ2(/hatwideL) + (λ3(/hatwideL)−λ2(/hatwideL))> nq , where the last inequality is using
the fact λ2(/hatwideL)≥0. Therefore, nqmust be the second eigenvalue of L⋆, completing the proof of
Lemma A.14 .
19Next, we prove a general Davis-Kahan style bound.
Lemma A.15. LetLand/hatwideLbe two weighted Laplacian matrices. Let u2and/hatwideru2be the second
eigenvectors of Land/hatwideL, respectively. Then,
∥u2−/hatwideru2∥2≤√
2·min

/vextenddouble/vextenddouble/vextenddouble(/hatwideL−L)u2/vextenddouble/vextenddouble/vextenddouble
2 /vextendsingle/vextendsingle/vextendsingleλ3(/hatwideL)−λ2(L)/vextendsingle/vextendsingle/vextendsingle,/vextenddouble/vextenddouble/vextenddouble(/hatwideL−L)/hatwideru2/vextenddouble/vextenddouble/vextenddouble
2 /vextendsingle/vextendsingle/vextendsingleλ3(L)−λ2(/hatwideL)/vextendsingle/vextendsingle/vextendsingle


Proof of Lemma A.15.One can get this sort of guarantee from variants of the Davis-Kahan theorem,
but it is more illuminating to write an eigenvalue decomposition and observe it from there. Without
loss of generality, assume that ⟨/hatwideru2,u2⟩≥0(indeed, otherwise we can always negate /hatwideru2if this is
not the case). Notice that
/vextenddouble/vextenddouble/vextenddouble(/hatwideL−L)u2/vextenddouble/vextenddouble/vextenddouble2
2=/vextenddouble/vextenddouble/vextenddouble/parenleftBig
/hatwideL−λ2(L)I/parenrightBig
u2/vextenddouble/vextenddouble/vextenddouble2
2
= (λ2(/hatwideL)−λ2(L))2⟨/hatwideru2,u2⟩2+n/summationdisplay
i=3/parenleftBig
λi(/hatwideL)−λ2(L)/parenrightBig2
⟨/hatwiderui,u2⟩2
≥n/summationdisplay
i=3/parenleftBig
λ3(/hatwideL)−λ2(L)/parenrightBig2
⟨/hatwiderui,u2⟩2=/parenleftBig
λ3(/hatwideL)−λ2(L)/parenrightBig2/parenleftBig
1−⟨/hatwideru2,u2⟩2/parenrightBig
,
which rearranges to
⟨/hatwideru2,u2⟩2≥1−
/vextenddouble/vextenddouble/vextenddouble(/hatwideL−L)u2/vextenddouble/vextenddouble/vextenddouble
2
λ3(/hatwideL)−λ2(L)
2
.
Now, if/vextenddouble/vextenddouble/vextenddouble(/hatwideL−L)u2/vextenddouble/vextenddouble/vextenddouble
2≥|λ3(/hatwideL)−λ2(L)|, then the condition ∥u2−/hatwideru2∥2≤√
2·/vextenddouble/vextenddouble(/hatwideL−L)u2/vextenddouble/vextenddouble
2/vextendsingle/vextendsingleλ3(/hatwideL)−λ2(L)/vextendsingle/vextendsingleis
trivially satisﬁed, since ∥u2−/hatwideru2∥2≤/radicalbig
2−2⟨/hatwideru2,u2⟩≤√
2. Otherwise, taking the square roots
of both sides, we obtain
⟨/hatwideru2,u2⟩≥/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt1−
/vextenddouble/vextenddouble/vextenddouble(/hatwideL−L)u2/vextenddouble/vextenddouble/vextenddouble
2
λ3(/hatwideL)−λ2(L)
2
,
which gives
∥/hatwideru2−u2∥2
2= 2−2⟨/hatwideru2,u2⟩≤2−2/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbt1−
/vextenddouble/vextenddouble/vextenddouble(/hatwideL−L)u2/vextenddouble/vextenddouble/vextenddouble
2
λ3(/hatwideL)−λ2(L)
2
≤2·
/vextenddouble/vextenddouble/vextenddouble(/hatwideL−L)u2/vextenddouble/vextenddouble/vextenddouble
2
λ3(/hatwideL)−λ2(L)
2
.
Taking the square root of both sides and repeating this argument by exchanging the roles of Land
/hatwideLyields the statement of Lemma A.15 .
This immediately implies the following upper-bound on ∥u2−u⋆
2∥2. We will use it repeatedly, both
in Theorem 1and Theorem 2.
Lemma A.16. We have
∥u2−u⋆
2∥2≤√
2·∥Eu⋆
2∥2
|λ3−λ⋆
2|.
Proof. Lemma A.16 immediately follows from Lemma A.15 by letting/hatwideL=L⋆.
20Combining with Lemma A.11 and Lemma A.12 , we can now upper-bound ∥u2−u⋆
2∥2in the setting
of Theorem 1.
Lemma A.17. In the setting of Theorem 1, there exists a universal constant Csuch that, for δ≥
3n−10, with probability≥1−δ, we have
∥u2−u⋆
2∥2≤C/radicalbig
log ( n/δ).
Proof of Lemma A.17.Using Lemma A.16 , Lemma A.11 and Lemma A.12 , we have
∥u2−u⋆
2∥2≤400√
2CA.12/parenleftBig√nq+ (nqlog ( 3n/δ))1/4+/radicalbig
log ( 3n/δ)/parenrightBig
n(p−q).
At this point, it is enough to show that there exists a universal constant Csuch that
Cn(p−q)≥400√
2CA.12/parenleftBig/radicalbig
nqlog ( n/δ) + (nq)1/4(log ( n/δ))3/4+ log ( n/δ)/parenrightBig
.
To see this, note that for any two nonnegative real numbers we have 2a1/4b1/4≤√
b+√a, which
implies 2a1/4b3/4≤b+√
ab. Leta=nqandb= log ( 3n/δ), and we get
400√
2CA.12/parenleftBig/radicalbig
nqlog ( 3n/δ) + (nq)1/4(log ( n/δ))3/4+ log ( 3n/δ)/parenrightBig
≤800√
2CA.12/parenleftBig/radicalbig
nqlog ( 3n/δ) + log ( 3n/δ)/parenrightBig
≤800√
2CA.12/parenleftBig/radicalbig
nplog ( 3n/δ) + log ( 3n/δ)/parenrightBig
≤Cn(p−q),
where the last inequality follows from the assumption we gave in Theorem 1. We therefore conclude
the proof of Lemma A.17 .
Next, we prove ℓ1norm concentration for the rows of Aand for the rows of Lin the setting of
Theorem 1. We will use this in Lemma A.19 , where we will bound/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u2/vextenddouble/vextenddouble/vextenddouble
2. Here u(v)
2
denotes the second eigenvector of the leave-one-out Laplacian L(v).
Lemma A.18. In the setting of Theorem 1, there exists a universal constant Csuch that with prob-
ability≥1−δ, for all v∈V, we have
∥av−a⋆
v∥1≤C/parenleftBig
np+/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig
∥lv−E[lv]∥1≤C/parenleftBig
np+/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig
.
Proof of Lemma A.18.It is easy to see that
∥lv−E[lv]∥1=|d[v]−E[d[v]]|+∥av−a⋆
v∥1.
Let us consider the second term above. By Lemma A.4and Lemma A.3, we have with probability
≥1−δ/2that for all v∈V
∥av−a⋆
v∥1≤∥av∥1+∥a⋆
v∥1
≤2/parenleftbiggnp
2+ max{CA.3, CA.4}/parenleftBig/radicalbig
nplog ( 4n/δ) + log ( 4n/δ)/parenrightBig/parenrightbigg
+np
= 2np+ 2 max{CA.3, CA.4}/parenleftBig/radicalbig
nplog ( 4n/δ) + log ( 4n/δ)/parenrightBig
.
Finally, by Lemma A.3and Lemma A.4, we have with probability 1−δ/2that for all v∈V,
|d[v]−E[d[v]]|≤max
v∈V|dout[v]−E[dout[v]]|+ max
v∈V|din[v]−E[din[v]]|
≤2 max{CA.3, CA.4}/parenleftBig/radicalbig
nplog ( 4n/δ) + log ( 4n/δ)/parenrightBig
Adding everything up means that with probability ≥1−δ, for all v∈V, we have
∥lv−E[lv]∥1≤2np+ 4 max{CA.4, CA.3}/parenleftBig/radicalbig
nplog ( 4n/δ) + log ( 4n/δ)/parenrightBig
,
which completes the proof of Lemma A.18 .
21Having established Lemma A.18 , we can now upper-bound/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u2/vextenddouble/vextenddouble/vextenddouble
2.
Lemma A.19. In the setting of Theorem 1, forδ≥2n−9with probability≥1−δ, for all v∈V,
we have
/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u2/vextenddouble/vextenddouble/vextenddouble
2≤∥u2∥∞·C/parenleftBig
p+/radicalbig
plog ( n/δ)/n+ log ( n/δ)/n/parenrightBig
p−q
Proof of Lemma A.19.Recall that the gap condition in Theorem 1means that pandqare such that
for a universal constant C,
n(p−q)≥C/parenleftBig/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig
.
To appeal to Lemma A.15 , we need to understand the entries of the matrix L−L(v). It is easy to
see that this matrix only has nonzero entries on the diagonal and in the vth row and column. There,
thevth row and column of L−L(v)are exactly equal to those of L−L⋆. Moreover, the w̸=vth
diagonal entry of L−L(v)is exactly /x31{(v, w)∈E}−pvw.
Hence, we have
/vextenddouble/vextenddouble/vextenddouble/parenleftBig
L−L(v)/parenrightBig
u2/vextenddouble/vextenddouble/vextenddouble
2
=/parenleftBiggn/summationdisplay
w=1/angbracketleftBig/parenleftBig
L−L(v)/parenrightBig
w,u2/angbracketrightBig2/parenrightBigg1/2
=
⟨(L−L⋆)v,u2⟩2+/summationdisplay
w̸=v((av[w]−pvw)u2[w]−(av[w]−pvw)u2[v])2
1/2
≤|⟨(L−L⋆)v,u2⟩|+
/summationdisplay
w̸=v((av[w]−pvw)u2[w]−(av[w]−pvw)u2[v])2
1/2
≤(∥lv−E[lv]∥1+ 2∥av−a⋆
v∥2)·∥u2∥∞
≤(∥lv−E[lv]∥1+ 2∥av−a⋆
v∥1)·∥u2∥∞
≤∥u2∥∞·3CA.18/parenleftbigg
np+/radicalBig
nplog/parenleftbig
2n2/δ/parenrightbig
+ log/parenleftbig
2n2/δ/parenrightbig/parenrightbigg
.
Now, let C≥8CA.10. Using Lemma A.10 to understand the concentration of sampling the graph
except edges incident to v, along with Weyl’s inequality, we have with probability ≥1−δthat for
allv∈Vand for all nsufﬁciently large,
/vextendsingle/vextendsingle/vextendsingleλ(v)
3−λ2/vextendsingle/vextendsingle/vextendsingle≥/parenleftBig
λ(v)
3−λ⋆
3/parenrightBig
−(λ2−λ⋆
2) + (λ⋆
3−λ⋆
2)
≥−2/parenleftbigg
CA.10/radicalBig
nplog/parenleftbig
2n2/δ/parenrightbig
+ log/parenleftbig
2n2/δ/parenrightbig/parenrightbigg
+n(p−q)
2≥n(p−q)
4.
Now, using Lemma A.15 , we get
/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u2/vextenddouble/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddouble/parenleftbig
L−L(v)/parenrightbig
u2/vextenddouble/vextenddouble
2
|λ(v)
3−λ2|≤∥u2∥∞·12CA.18/parenleftBig
np+/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig
n(p−q)
≤∥u2∥∞·12CA.18/parenleftBig
p+/radicalBig
plog/parenleftbig
2n2/δ/parenrightbig
/n+ log/parenleftbig
2n2/δ/parenrightbig
/n/parenrightBig
p−q,
completing the proof of Lemma A.19 .
22A.5 Leave-one-out and bootstrap
The main goal of this section is to establish an upper-bound on |⟨av−a⋆
v,u2−u⋆
2⟩|in the setting
of Theorem 1. To this end, we will need the following concentration inequality from [ AFWZ20 ].
Lemma A.20 (Lemma 7 from [ AFWZ20 ]).Letw∈RnandXi∼Ber(pi). Let p≥pifor all
i∈[n]. LetX∈Rnbe the vector formed by stacking the Xi. Then,
Pr
|⟨w, X−E[X]⟩|≥(2 +a)pn
max/parenleftBig
1,log/parenleftBig√n∥w∥∞
∥w∥2/parenrightBig/parenrightBig·∥w∥∞
≤2exp(−anp).
Lemma A.21. In the setting of Theorem 1, suppose avis such that av[w]∼Bernoulli (pvw)and let
p≥max w:pvw̸=1pvw. With probability≥1−δforδ≥1/n2, for all v∈V, we have
|⟨av−a⋆
v,u2−u⋆
2⟩|≤C(np+ log ( n/δ))/parenleftbigg∥u2∥∞
log log n+1√nlog log n/parenrightbigg
.
Proof of Lemma A.21.Ideally, one would treat u2−u⋆
2as ﬁxed and then apply Bernstein’s inequal-
ity to argue that the sum of centered Bernoulli random variables as written above concentrates well.
Unfortunately, since u2depends on av−a⋆
v, we cannot express this inner product as the sum of
independent random variables.
To resolve this, we use the leave-one-out method. Let u(v)
2be the second eigenvector of the leave-
one-out Laplacian L(v)ofA(v), where A(v)is chosen to agree with Aeverywhere except for the
vth row and vth column. The vth row and vth column of A(v)are replaced with those of A⋆. Now,
avdoes not depend on L(v)and therefore u(v)
2.
We therefore write
|⟨av−a⋆
v,u2−u⋆
2⟩|≤/vextendsingle/vextendsingle/vextendsingle/angbracketleftBig
av−a⋆
v,u2−u(v)
2/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/angbracketleftBig
av−a⋆
v,u(v)
2−u⋆
2/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle
≤∥av−a⋆
v∥2·/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u2/vextenddouble/vextenddouble/vextenddouble
2+/vextendsingle/vextendsingle/vextendsingle/angbracketleftBig
av−a⋆
v,u(v)
2−u⋆
2/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle
≤∥av−a⋆
v∥2·CA.19p
p−q∥u2∥∞+/vextendsingle/vextendsingle/vextendsingle/angbracketleftBig
av−a⋆
v,u(v)
2−u⋆
2/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle.
To bound the rightmost term of the RHS, we use Lemma 7 of [ AFWZ20 ], reproduced in
Lemma A.20 . In that, let w:=u(v)
2−u⋆
2. Leta=1
nplog ( 20n/δ)so that 2 exp(−2anp)≤δ/(10n).
Note that for the deterministic entries, we have av−a⋆
v= 1−1 = 0 , so in Lemma A.20 , we can
setXw∼Ber(0)for these entries. Now, by Lemma A.20 , with probability≥1−δ/n, we have
/vextendsingle/vextendsingle/vextendsingle/angbracketleftBig
u(v)
2−u⋆
2,av−a⋆
v/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle≤2np+ log/parenleftbig20n
δ/parenrightbig
max/parenleftBig
1,log/parenleftBig√n∥w∥∞
∥w∥2/parenrightBig/parenrightBig·∥w∥∞. (5)
Let us ﬁrst bound∥w∥∞=/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u⋆
2/vextenddouble/vextenddouble/vextenddouble
∞. We write
/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u⋆
2/vextenddouble/vextenddouble/vextenddouble
∞≤/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u2/vextenddouble/vextenddouble/vextenddouble
∞+∥u2−u⋆
2∥∞(6)
≤/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u2/vextenddouble/vextenddouble/vextenddouble
2+∥u2∥∞+∥u⋆
2∥∞(7)
≤2 max/braceleftbigg
CA.19(α, δ)∥u2∥∞,1√n/bracerightbigg
. (8)
In what follows, we omit the arguments αandδin mentions of CA.19. Next, using Lemma A.17 ,
the triangle inequality, and δ≥1/n3, we have
∥w∥2=/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u⋆
2/vextenddouble/vextenddouble/vextenddouble
2≤CA.19∥u2∥∞+4CA.17√logn.
23We now have two cases based on the value of√n·/vextenddouble/vextenddoubleu(v)
2−u⋆
2/vextenddouble/vextenddouble
∞/vextenddouble/vextenddoubleu(v)
2−u⋆
2/vextenddouble/vextenddouble
2.
Case 1 – wis not too “ﬂat.” Let us ﬁrst handle the case where
√n·/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u⋆
2/vextenddouble/vextenddouble/vextenddouble
∞ /vextenddouble/vextenddouble/vextenddoubleu(v)
2−u⋆
2/vextenddouble/vextenddouble/vextenddouble
2≥/radicalbig
logn.
We plug this into ( 5) and get
/vextendsingle/vextendsingle/vextendsingle/angbracketleftBig
u(v)
2−u⋆
2,av−a⋆
v/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle≤2np+ log/parenleftbig20n
δ/parenrightbig
max/parenleftBig
1,log/parenleftBig√n∥w∥∞
∥w∥2/parenrightBig/parenrightBig·∥w∥∞
≤4·np+ log ( 20n/δ)
log log n/parenleftbigg
CA.19∥u2∥∞+1√n/parenrightbigg
,
where the last inequality follows from ( 8).
Case 2 – wis “ﬂat.” We now assume
√n·/vextenddouble/vextenddouble/vextenddoubleu(v)
2−u⋆
2/vextenddouble/vextenddouble/vextenddouble
∞ /vextenddouble/vextenddouble/vextenddoubleu(v)
2−u⋆
2/vextenddouble/vextenddouble/vextenddouble
2≤/radicalbig
logn.
We can easily check that the function
x
max (1 ,logx)
is increasing, so its maximum will be attained at the largest value of xin the domain. Let x=√n∥w∥∞/∥w∥2and write
2np+ log/parenleftbig20n
δ/parenrightbig
max/parenleftBig
1,log/parenleftBig√n∥w∥∞
∥w∥2/parenrightBig/parenrightBig·∥w∥∞
=2np+ log/parenleftbig20n
δ/parenrightbig
max/parenleftBig
1,log/parenleftBig√n∥w∥∞
∥w∥2/parenrightBig/parenrightBig·√n∥w∥∞
∥w∥2·∥w∥2√n
≤2np+ log/parenleftbig20n
δ/parenrightbig
log log n·/radicalbigg
logn
n·∥w∥2
≤2np+ log/parenleftbig20n
δ/parenrightbig
log log n·/radicalbigg
logn
n·CA.19
∥u2∥∞+1/radicalBig
log/parenleftbig
20n2/δ/parenrightbig

=CA.19/parenleftBigg
2np+ log/parenleftbig20n
δ/parenrightbig
log log n·/radicalbigg
logn
n∥u2∥∞+2np+ log/parenleftbig20n
δ/parenrightbig
√n·log log n/parenrightBigg
.
All of this tells us that
/vextendsingle/vextendsingle/vextendsingle/angbracketleftBig
av−a⋆
v,u(v)
2−u⋆
2/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle≤4CA.19·(np+ log ( 20n/δ))/parenleftbigg∥u2∥∞
log log n+1√nlog log n/parenrightbigg
.
It remains to handle the term
∥av−a⋆
v∥2·∥u2∥∞.
Indeed, using Lemma A.13 , we have with probability ≥1−δthat
∥av−a⋆
v∥2·∥u2∥∞≤CA.13/parenleftbigglog ( 20n/δ)
logn/parenrightbigg3/2/radicalbig
np·∥u2∥∞.
24Combining everything tells us that
|⟨av−a⋆
v,u2−u⋆
2⟩|≤ 30CA.13/parenleftbigglog ( 20n/δ)
logn/parenrightbigg3/2/radicalbig
np·∥u2∥∞
+ 4CA.19·(np+ log ( 20n/δ))/parenleftbigg∥u2∥∞
log log n+1√nlog log n/parenrightbigg
≤C(np+ log ( 20n/δ))/parenleftbigg∥u2∥∞
log log n+1√nlog log n/parenrightbigg
.
Taking a union bound over all v∈Vconcludes the proof of Lemma A.21 .
Finally, we establish an upper-bound on ∥u2∥∞. This will be used repeatedly in the proof of Theo-
rem1.
Lemma A.22. In the same setting as Theorem 1, with probability≥1−δforδ≥10n2, we have
for some constant C(α, δ)that
∥u2∥∞≤C(α, δ)√n.
Proof of Lemma A.22.First, observe that
(D−A)u2=λ2u2,
which means that
(D−λ2I)−1Au2=u2.
By Lemma A.7, with probability≥1−δ, for all v∈Vwe have
d[v]−λ2≥n(p−q)
4.
Combining with Lemma A.6, we have
din[v]−dout[v]
din[v]−dout[v] + (2 dout[v]−λ2)= 1−2dout[v]−λ2
din[v]−dout[v] + (2 dout[v]−λ2)
≤1 +CA.6/parenleftBig/radicalbig
nqlog ( 10n/δ) + log ( 10n/δ)/parenrightBig
din[v]−dout[v] + (2 dout[v]−λ2)
≤1 +4CA.6/parenleftBig/radicalbig
nqlog ( 10n/δ) + log ( 10n/δ)/parenrightBig
n(p−q)≤C′,
for some constant C′>0, where the penultimate line follows from Lemma A.7and the last line
follows from the gap assumption in Theorem 1. Furthermore, by Lemma A.8and Lemma A.17 , we
have with probability ≥1−δthat for all v∈V,
|⟨a⋆
v,u⋆
2−u2⟩|
d[v]−λ2≤p√n
d[v]−λ2·CA.17/radicalbig
log ( 10n/δ)≤CA.8(α)·CA.17/radicalbig
nlog ( 10n/δ).
25Now, using Lemma A.8(and using Lemma A.7to ensure that d[v]−λ2>0for all v∈V), we have
∥u2∥∞=/vextenddouble/vextenddouble/vextenddouble(D−λ2I)−1Au2/vextenddouble/vextenddouble/vextenddouble
∞
=/vextenddouble/vextenddouble/vextenddouble(D−λ2I)−1Au2−(D−λ2I)−1Au⋆
2+ (D−λ2I)−1Au⋆
2/vextenddouble/vextenddouble/vextenddouble
∞
≤/vextenddouble/vextenddouble/vextenddouble(D−λ2I)−1Au⋆
2/vextenddouble/vextenddouble/vextenddouble
∞+/vextenddouble/vextenddouble/vextenddouble(D−λ2I)−1A(u⋆
2−u2)/vextenddouble/vextenddouble/vextenddouble
∞
= max
1≤v≤n|⟨av,u⋆
2⟩|
d[v]−λ2+ max
1≤v≤n|⟨av,u⋆
2−u2⟩|
d[v]−λ2
=1√n/parenleftbigg
max
1≤v≤n|din[v]−dout[v]|
d[v]−λ2/parenrightbigg
+ max
1≤v≤n|⟨av,u⋆
2−u2⟩|
d[v]−λ2
≤C√n+ max
1≤v≤n|⟨av−a⋆
v,u⋆
2−u2⟩|
d[v]−λ2+ max
1≤v≤n|⟨a⋆
v,u⋆
2−u2⟩|
d[v]−λ2
≤C√n+CA.21(np+ log ( 10n/δ))
d[v]−λ2·/parenleftbigg1√nlog log n+∥u2∥∞
log log n/parenrightbigg
+CA.8(α)·CA.17/radicalbig
nlog ( 10n/δ)
≤C√n+CA.21·CA.8(α)·/parenleftbigg1√nlog log n+∥u2∥∞
log log n/parenrightbigg
+CA.8(α)·CA.17/radicalbig
nlog ( 10n/δ).
Note that any nlarge enough
CA.21·CA.8(α)·∥u2∥∞
log log n≤∥u2∥∞
2.
Thus, rearranging and solving for ∥u2∥∞yields
∥u2∥∞≤2/parenleftBigg
C√n+CA.21·CA.8(α)·/parenleftbigg1√nlog log n/parenrightbigg
+CA.8(α)·CA.17/radicalbig
nlog ( 10n/δ)/parenrightBigg
,
completing the proof of Lemma A.22 .
A.6 Strong consistency of unnormalized spectral bisection
In this section, we prove our main positive results Theorem 1and Theorem 2. It will be helpful to
recall the proof sketches given in Section 3while reading this section.
At a high level, the proof plan is as follows.
1.We ﬁrst establish a sufﬁcient condition for a particular vertex to be classiﬁed cor-
rectly. We can think of this as simultaneously showing that the intermediate estima-
tor(D−λ2I)−1Au⋆
2is strongly consistent and that the corresponding “noise” term
(D−λ2I)−1A(u⋆
2−u2)is a lower-order term in comparison to this. For a more formal
way to see this, see Lemma A.23 .
2.For the proof of Theorem 1, the main technical challenge in showing that the noise term
above is small amounts to analyzing the random quantity |⟨av,u⋆
2−u2⟩|. This is where
we will have to use the leave-one-out method to decouple the dependence between avand
u2. The relevant lemmas for the leave-one-out analysis are Lemma A.21 and Lemma A.22 .
3.Finally, for the proof of Theorem 2, we again appeal to Lemma A.23 but use a different
approach to show that the noise term is small.
A.6.1 A sufﬁcient condition for exact recovery and proof
The main result of this subsection is Lemma A.23 , which gives a general condition under which a
particular vertex will be classiﬁed correctly. The proofs of Theorem 1and Theorem 2will follow
by invoking Lemma A.23 . We remark that the point of this lemma is mostly conceptual; the crux of
the analysis lies in establishing that these conditions are satisﬁed our models.
Lemma A.23. Letv∈Vbe some vertex. If d[w]−λ2>0for all w∈V,din[v]>dout[v],
and|⟨av,u⋆
2−u2⟩|≤ (din[v]−dout[v])/√n, then sign(u2[v]) = sign(u⋆
2[v]), i.e., u2correctly
classiﬁes vertex v.
26The goal of the rest of this section is to prove Lemma A.23 .
Our approach is to study the intermediate estimator
(D−λ2I)−1Au⋆
2.
At a high level, our goal is to show that this correctly classiﬁes all the vertices with high probability
and also is very close to u2inℓ∞norm with high probability. Deng, Ling, and Strohmer [ DLS21 ]
used this intermediate estimator to prove the strong consistency of unnormalized spectral bisection
forSBM (n, p, q )instances.
Next, we show that this estimator is consistent and prove Lemma A.23 .
Proof of Lemma A.23.Observe that
u2= (D−λ2I)−1Au⋆
2−(D−λ2I)−1A(u⋆
2−u2).
Without loss of generality, suppose v∈P1. In particular, this means that u⋆
2[v] = 1 /√n. Our goal
is to show that u2[v]>0. And, as per the above, this means that it is enough to show that
/parenleftBig
(D−λ2I)−1Au⋆
2/parenrightBig
[v]≥/parenleftBig
(D−λ2I)−1A(u⋆
2−u2)/parenrightBig
[v],
or equivalently, using the fact that d[v]−λ2>0,
⟨av,u⋆
2⟩≥⟨av,u⋆
2−u2⟩,
where avdenotes the v-th row of A. To see that the above holds, use the fact that we know that
din[v]−dout[v]>0, which gives
⟨av,u⋆
2⟩=din[v]−dout[v]√n≥|⟨av,u⋆
2−u2⟩|≥⟨ av,u⋆
2−u2⟩.
This is exactly what we needed, and we conclude the proof of Lemma A.23 .
A.7 Proofs of main results
At this point, we are ready to prove our main results.
A.7.1 Nonhomogeneous symmetric stochastic block model (Proof of Theorem 1)
We are ﬁnally ready to prove Theorem 1. For convenience, we reproduce its statement here.
Theorem 1. Letp,p, qbe probabilities such that q < p≤pand such that α:=p/(p−q)is an
arbitrary constant. Let D∈ NSSBM (n, p,p, q). Let n≥N(α)where the function N(α)only
depends on α. There exists a universal constant C > 0such that if
n(p−q)≥C/parenleftBig/radicalbig
nplogn+ log n/parenrightBig
, (gap condition)
then unnormalized spectral bisection is strongly consistent on D.
Proof of Theorem 1.As mentioned in Section 2, we actually prove a slightly stronger statement –
we will allow the adversary to set at most np/log log nof the pvwto1per vertex v(in other words,
the adversary can commit to at most np/log log nedges per vertex that are guaranteed to appear in
the ﬁnal graph).
Our plan is to apply Lemma A.23 . In order to do so, we start with showing that for all v, we have
din[v]>dout[v]. By Lemma A.5, with probability≥1−δ, we have for all v∈Vthat
din[v]−dout[v]≥n(p−q)
2−CA.5/parenleftBig/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig
>0.
Additionally, by Lemma A.7, we have for all vthatd[v]> λ 2.
The ﬁnal item we need is to show that for all v∈V, we have|⟨av,u⋆
2−u2⟩|≤|⟨ av,u⋆
2⟩|. Observe
that
|⟨av,u⋆
2−u2⟩|≤|⟨ a⋆
v,u⋆
2−u2⟩|+|⟨av−a⋆
v,u⋆
2−u2⟩|,
27where a⋆
vdenotes the v-th row of E[A]. We handle the terms one at a time. First, note that by
Lemma A.11 , with probability≥1−δ, we have
λ3−λ⋆
2≥n(p−q)
4.
Now, let E:=L−E[L], and let a⋆
v[rand]∈RVcorrespond to the vector that entrywise agrees
witha⋆
vwherever a⋆
vis not 1and is zero elsewhere. This corresponds to the edges incident to vthat
will be sampled randomly from the distribution over graphs. This means that for all n≥N(δ)and
choosing δ≥1/(10n), we have
|⟨a⋆
v[rand],u⋆
2−u2⟩|≤∥ a⋆
v[rand]∥2·√
2∥Eu⋆
2∥2
|λ3−λ⋆
2|( Lemma A.16 )
≤p√n·40√
2CA.12/parenleftbig√nq+ (nqlogn)1/4+√logn/parenrightbig
n(p−q)(Lemmas A.11 andA.12 )
≤1000CA.12np√nlog log n(gap in Theorem 1)
To handle the oblivious insertions, let ddet∈RVdenote the degree vector that counts the number of
deterministic edges inserted incident to v, for all v∈V. Under this notation, we have
|⟨a⋆
v−a⋆
v[rand],u⋆
2−u2⟩|≤ddet[v]·∥u2−u⋆
2∥∞≤np√nlog log n+np∥u2∥∞
log log n.
where the last inequality follows from using ∥u2−u⋆
2∥∞≤∥u2∥∞+∥u⋆
2∥∞. Combining yields
|⟨a⋆
v,u⋆
2−u2⟩|≤C′ np√nlog log n+np∥u2∥∞
log log n,
for some constant C′>0. Now, notice that for all nsufﬁciently large,
|⟨av−a⋆
v,u⋆
2−u2⟩|
≤CA.21(np+ log ( n/δ))/parenleftbigg∥u2∥∞
log log n+1√nlog log n/parenrightbigg
(Lemma A.21 )
≤CA.21(np+ log ( n/δ))
CA.22(α,δ)√n
log log n+1√nlog log n
 (Lemma A.22 )
≤C1(α, δ)·(np+ log ( n/δ))√nlog log n.
Adding yields for n≥N(α, δ),
|⟨av,u⋆
2−u2⟩|≤|⟨ a⋆
v,u⋆
2−u2⟩|+|⟨av−a⋆
v,u⋆
2−u2⟩|
≤C2(α, δ)·(np+ log ( n/δ))√nlog log n
≤1√n·/parenleftbiggn(p−q)
2−CA.5/parenleftBig/radicalbig
nplog ( n/δ) + log ( n/δ)/parenrightBig/parenrightbigg
(gap condition)
≤din[v]−dout[v]√n=|⟨av,u⋆
2⟩|,
which means we satisfy the conditions required by Lemma A.23 . Taking a union bound over all our
(constantly many) probabilistic statements, setting δ= Θ(1 /n), and rescaling completes the proof
of Theorem 1.
A.7.2 Deterministic clusters model
For convenience, we reproduce the statement of Theorem 2here.
28Theorem 2. Letqbe a probability and dinbe an integer, and let D∈ DCM (n, d in, q). For G∼D ,
let/hatwideLdenote the expectation of Lafter step (2) but before step (3) in Model 2. There exists constants
C1, C2, C3>0such that for all nsufﬁciently large, if
din≥C1·/parenleftBignq
2+√n/parenrightBig
and λ3(/hatwideL)−λ2(/hatwideL)≥√n+C2nq+C3/parenleftBig/radicalbig
nqlogn+ log n/parenrightBig
,
then unnormalized spectral bisection is strongly consistent on D.
Proof of Theorem 2.In this proof, let L⋆be the Laplacian matrix that agrees with Lon all internal
edges and agrees with E[L]on all crossing edges. Let L(cross)denote the Laplacian matrix corre-
sponding to the cross edges, so we can write L⋆=L−L(cross)+E/bracketleftbig
L(cross)/bracketrightbig
. Although L⋆̸=E[L]
due to the adaptive adversary, by Lemma A.14 , we still have L⋆u⋆
2=λ⋆
2u⋆
2=nqu⋆
2. Moreover,
(L−L⋆)u⋆
2is the vector whose entries are of the form 2(dout[v]−E[dout[v]])/√n. Thus, we will
be able to apply Lemma A.16 and Lemma A.12 later on. Finally, observe that λi(L⋆)≥λi(/hatwideL)for
alli≥3andλ2(/hatwideL) =λ2(L⋆) =nq. Thus, one can use the spectral gap λ3(/hatwideL)−λ2(/hatwideL)to reason
about λ⋆
3−λ⋆
2.
Letδ≥1/(10n). We will apply Lemma A.23 to get strong consistency. First, let us verify that
d[v]> λ 2for all v. Applying Lemma A.10 to the matrix L(cross)gives
∥L−L⋆∥op=/vextenddouble/vextenddouble/vextenddoubleL(cross)−E/bracketleftBig
L(cross)/bracketrightBig/vextenddouble/vextenddouble/vextenddouble
op≤CA.10/parenleftBig/radicalbig
nqlog ( n/δ) + log( n/δ)/parenrightBig
.
Thus, using Weyl’s inequality, for n > N (δ), we have
d[v]−λ2≥din[v]−λ⋆
2−∥L−L⋆∥op
≥C1nq
2+C1√n−nq−CA.10/parenleftBig/radicalbig
nqlog ( n/δ) + log( n/δ)/parenrightBig
>0.
Next, we verify that din[v]>dout[v]for all v. By Lemma A.3, with probability≥1−δ, for all
v∈V, we have
/vextendsingle/vextendsingle/vextendsingledout[v]−nq
2/vextendsingle/vextendsingle/vextendsingle≤CA.3/parenleftBig/radicalbig
nqlog ( n/δ) + log ( n/δ)/parenrightBig
.
So for n > N (δ), we obtain
din[v]−dout[v]≥C1nq
2+C1√n−nq
2−CA.3/parenleftBig/radicalbig
nqlog ( n/δ) + log ( n/δ)/parenrightBig
>0.
Here, in the last inequality we used the fact that/radicalbig
nqlog ( n/δ)≤max{nq,log(n/δ)}.
Finally, we need to show that for all v∈V,
|⟨av,u⋆
2−u2⟩|≤|⟨ av,u⋆
2⟩|=din[v]−dout[v]√n.
By Cauchy-Schwarz, we have
|⟨av,u⋆
2−u2⟩|≤∥ av∥2·∥u⋆
2−u2∥2=/radicalbig
din[v] +dout[v]·∥u⋆
2−u2∥2.
Thus, it is enough to show that for all v∈Vwe get
√n∥u⋆
2−u2∥2≤din[v]−dout[v]/radicalbig
din[v] +dout[v].
Observe that the RHS above is a decreasing function in dout[v]and an increasing function in din[v].
Now, by Lemma A.16 and Lemma A.12 , we have
√n∥u⋆
2−u2∥2≤√n∥Eu⋆
2∥2
|λ3−λ⋆
2|≤6CA.12√n/parenleftBig√nq+ (nqlog ( n/δ))1/4+/radicalbig
log ( n/δ)/parenrightBig
|λ3−λ⋆
2|.(9)
We now do casework on the value of q.
29Case 1: q≤log ( n/δ)/n.Carrying on from ( 9) and applying Lemma A.10 (we can set pijfor the
deterministic internal edges to 0as they do not affect L−E[L]) along with Weyl’s inequality, for
alln≥N(δ)we have
√n∥u⋆
2−u2∥2≤18CA.12/radicalbig
nlog ( n/δ)
|λ3−λ⋆
2|≤18CA.12/radicalbig
nlog ( n/δ)√n−3CA.10log ( n/δ)
≤C/radicalbig
log ( n/δ)≪din[v]−dout[v]/radicalbig
din[v] +dout[v],
as required. Here the last inequality follows using the fact that din[v]≥C1/parenleftbignq
2+√n/parenrightbig
and
dout[v]≤nq
2+ 2CA.3log(n/δ).
Case 2: log ( n/δ)/n≤q.Similar to the previous case, we get
√n∥u⋆
2−u2∥2≤18CA.12√n·√nq
|λ3−λ⋆
2|≤18CA.12√n·√nq√n+ (C2−2CA.10)nq(10)
≤18CA.12·max/braceleftbigg√nq,1
(C2−2CA.10)√q/bracerightbigg
. (11)
Additionally, we can use the conclusion of Lemma A.3to write with probability ≥1−δfor all
v∈Vandn≥N(δ)that
din[v]−dout[v]/radicalbig
din[v] +dout[v]≥(C1/2−2CA.3−1/2)nq+C1√n/radicalbig
(C1/2 + 2 CA.3+ 1/2)nq(12)
≥C1/2−2CA.3−1/2/radicalbig
C1/2 + 2 CA.3+ 1/2max/braceleftbigg√nq,/radicalbigg1
q/bracerightbigg
. (13)
From this, it is clear that one can choose constants C1andC2such that ( 11) is at most ( 13). Taking
a union bound over all our (constantly many) probabilistic statements, setting δ= Θ(1 /n), and
rescaling completes the proof of Theorem 2.
A.8 Inconsistency of normalized spectral bisection
In this section, we design a family of problem instances on which unnormalized spectral bisection
is strongly consistent whereas normalized spectral bisection is inconsistent. Speciﬁcally, our goal is
to prove Theorem 3.
Theorem 3. For all nsufﬁciently large, there exists a nonhomogeneous stochastic block model such
that unnormalized spectral bisection is strongly consistent whereas normalized spectral bisection
(both symmetric and random-walk) incurs a misclassiﬁcation rate of at least 24% with probability
1−1/n.
A.8.1 The nested block example
We ﬁrst state the family of instances on which we will prove our inconsistency results. Let nbe a
multiple of 4. Let L1consist of indices 1, . . . , n/ 4,L2consist of indices n/4 + 1 , . . . , n/ 2, and R
consist of indices n/2 + 1 , . . . , n .
As mentioned in Section 3, consider the following block structure determined by the A⋆written
below, where q < p andK≥3p/q.
L1 L2 R
L1Kp· /x31n/4×n/4p· /x31n/4×n/4q· /x31n/2×n/2L2p· /x31n/4×n/4Kp· /x31n/4×n/4
R q· /x31n/2×n/2 p· /x31n/2×n/2
Table 2: A⋆is deﬁned to have the above block structure.
We will draw our instances from the nonhomogeneous stochastic block model according to the
probabilities prescribed above. Note that within the two clusters L:=L1∪L2andR, each edge
30appears with probability at least p. Moreover, each edge in L×Rappears with probability exactly
q. However, there are also two subcommunities L1andL2that appear within L. Furthermore,
observe that unnormalized spectral bisection is consistent on this family of examples with probability
≥1−1/nby Theorem 1.
A.8.2 Technical lemmas
We next show some technical statements that we will need later in the proof of Theorem 3.
Lemma A.24. LetM∈Rk×k. Then,
∥M∥op≤max
i≤k|M[i][i]|+kmax
i̸=j|M[i][j]|.
Proof of Lemma A.24.For a matrix N∈Rk×k, it is easy to check that
∥N∥op≤∥N∥F≤kmax
i,j≤k|N[i][j]|.
Next, let diag(M)denote the matrix that agrees with Mon the diagonal and is 0elsewhere. Notice
that
∥M∥op≤∥diag(M)∥op+∥M−diag(M)∥op≤max
i≤k|M[i][i]|+kmax
i̸=j|M[i][j]|,
completing the proof of Lemma A.24 .
Lemma A.25. Letεxbe a constant where 0≤εx< x. Let εybe deﬁned similarly. The function
f(x, y)deﬁned as
f(x, y):=1√x−εx√y−εy−1√x√y
is decreasing in xandy.
Proof of Lemma A.25.It is enough to just check the inequality for x. We take the derivative of
f(x, y)with respect to xand get
1
2/parenleftbigg
−1
(x−εx)3/2(y−εy)1/2+1
x3/2y1/2/parenrightbigg
<0,
where the inequality follows from observing 0< x−εx≤xand similarly for y. This completes
the proof of Lemma A.25 .
A.8.3 Proof of Theorem 3
First, we construct L⋆.
Lemma A.26. LetL⋆:=I−(D⋆)−1/2A⋆(D⋆)−1/2. Then, I−L⋆has the following block
structure.
L1 L2 R
L1Kp
n
2·(p·K+1
2+q)· /x31n/4×n/4p
n
2·(p·K+1
2+q)· /x31n/4×n/4 q/radicalbig
n
2·(p·K+1
2+q)·n
2·(p+q)· /x31n/2×n/2
L2p
n
2·(p·K+1
2+q)· /x31n/4×n/4Kp
n
2·(p·K+1
2+q)· /x31n/4×n/4
Rq/radicalbig
n
2·(p·K+1
2+q)·n
2·(p+q)· /x31n/2×n/2p
n
2·(p+q)· /x31n/2×n/2
Proof of Lemma A.26.It is easy to see that for any v∈L, we have d⋆[v] =n
2·/parenleftbig
p·K+1
2+q/parenrightbig
and
for any v∈R, we have d⋆[v] =n
2·(p+q). Lemma A.26 follows by noting that every element of
I−L⋆is of the form a⋆
i[j]//radicalbig
d⋆[i]d⋆[j].
Next, we analyze the eigenvalues and eigenvectors of L⋆.
31Lemma A.27. Up to normalization and sign, the eigenvector-eigenvalue pairs of I−L⋆correspond-
ing to the nonzero eigenvalues of I−L⋆are
(λ⋆
1,u⋆
1) =/parenleftbig
1,/bracketleftbig
/x31n/4⊕ /x31n/4⊕y+· /x31n/4⊕y+· /x31n/4/bracketrightbig/parenrightbig
(λ⋆
2,u⋆
2) =/parenleftBigg
(K−1)p
2/parenleftbig
p·K+1
2+q/parenrightbig,/bracketleftbig
/x31n/4⊕− /x31n/4⊕0n/4⊕0n/4/bracketrightbig/parenrightBigg
(λ⋆
3,u⋆
3) =/parenleftbigg
−1 +p/parenleftbigg1
p+q+K+ 1
p(K+ 1) + 2 q/parenrightbigg
,/bracketleftbig
/x31n/4⊕ /x31n/4⊕y−· /x31n/4⊕y−· /x31n/4/bracketrightbig/parenrightbigg
where y+andy−are chosen according to the formulas
y+=/radicalBigg
2(p+q)
p(K+ 1) + 2 qy−=−/radicalBigg
p(K+ 1) + 2 q
2(p+q).
Moreover, we have λ⋆
1> λ⋆
2> λ⋆
3>0and
λ⋆
2−λ⋆
3≥1−p2(K+ 3) + 4 pq
p2(K+ 3) + 4 pq+ 2q2.
Proof of Lemma A.27.As we can see from Lemma A.26 ,I−L⋆is a matrix whose rank is at most
3, since it can be constructed by carefully repeating 3distinct column vectors. Thus, it can have at
most 3nonzero eigenvalues. In what follows, we consider the case where K > 1so that there are
exactly 3nonzero eigenvalues.
The next step is to conﬁrm that the stated eigenvalue-eigenvector pairs are in fact valid. We begin
withu⋆
1. Every entry in the ﬁrst n/2entries of (I−L⋆)u⋆
1can be expressed as
n
4·Kp
n
2·/parenleftbig
p·K+1
2+q/parenrightbig+n
4·p
n
2·/parenleftbig
p·K+1
2+q/parenrightbig+n
2·
q·/radicalBig
2(p+q)
p(K+1)+2 q/radicalBig
n
2·/parenleftbig
p·K+1
2+q/parenrightbig
·n
2·(p+q)

=(K+ 1)p
(K+ 1)p+ 2q+q·/radicalBig
2(p+q)
p(K+1)+2 q/radicalBig/parenleftbig
p·K+1
2+q/parenrightbig
(p+q)=(K+ 1)p
(K+ 1)p+ 2q+q·/radicalBig
2
p(K+1)+2 q/radicalBig/parenleftbig
p·K+1
2+q/parenrightbig
=(K+ 1)p
(K+ 1)p+ 2q+2q
(K+ 1)p+ 2q= 1,
and every entry in the second n/2entries of (I−L⋆)u⋆
1can be expressed as
n
2·q/radicalBig
n
2·/parenleftbig
p·K+1
2+q/parenrightbig
·n
2·(p+q)+n
2·p
n
2·(p+q)·/radicalBigg
2(p+q)
p(K+ 1) + 2 q
=q/radicalBig/parenleftbig
p·K+1
2+q/parenrightbig
(p+q)+p
(p+q)·/radicalBigg
2(p+q)
p(K+ 1) + 2 q
=q/radicalBig/parenleftbig
p·K+1
2+q/parenrightbig
(p+q)+p·/radicalBigg
1/parenleftbig
p·K+1
2+q/parenrightbig
(p+q)
=√p+q/radicalBig
p·K+1
2+q=/radicalBigg
2(p+q)
p(K+ 1) + 2 q=y+.
Foru⋆
2, we can use the block structure and easily verify
(I−L⋆)u⋆
2=n
4·(K−1)p
n
2·/parenleftbig
p·K+1
2+q/parenrightbig/bracketleftbig
/x31n/4⊕− /x31n/4⊕0n/4⊕0n/4/bracketrightbig
=λ⋆
2u⋆
2.
32We now address u⋆
3. The ﬁrst n/2entries of (I−L⋆)u⋆
3are
n
4·Kp
n
2·/parenleftbig
p·K+1
2+q/parenrightbig+n
4·p
n
2·/parenleftbig
p·K+1
2+q/parenrightbig+n
2·
q·−/radicalBig
p(K+1)+2 q
2(p+q)/radicalBig
n
2·/parenleftbig
p·K+1
2+q/parenrightbig
·n
2·(p+q)

=(K+ 1)p
(K+ 1)p+ 2q+
q·−/radicalBig
1
p+q√p+q
=(K+ 1)p
(K+ 1)p+ 2q−q
p+q=λ⋆
3,
and the second n/2entries of (I−L⋆)u⋆
3are
n
2·q/radicalBig
n
2·/parenleftbig
p·K+1
2+q/parenrightbig
·n
2·(p+q)+n
2·p
n
2·(p+q)·−/radicalBigg
p(K+ 1) + 2 q
2(p+q)
=q/radicalBig/parenleftbig
p·K+1
2+q/parenrightbig
(p+q)−p
(p+q)·/radicalBigg
p(K+ 1) + 2 q
2(p+q)
=−/radicalBigg
p(K+ 1) + 2 q
2(p+q)/parenleftbigg−2q
p(K+ 1) + 2 q+p
p+q/parenrightbigg
=y−·λ⋆
3.
Finally, it remains to check that 1> λ⋆
2> λ⋆
3>0. The fact that λ⋆
2<1easily follows from using
p+q >0. To prepare to bound λ⋆
2−λ⋆
3, we ﬁrst use p≥qto establish
p2−pq+ 2q2=p(p−q) + 2q2≥2q2.
This implies
pq(K−1) + 2 q2≥3p2−pq+ 2q2= 2p2+ (p2−pq+ 2q2)≥2p2+ 2q2,
which rearranges to
p2(K+ 1) + pq(K+ 3) + 2 q2≥p2(K+ 3) + 4 pq+ 2q2.
Next, we write
λ⋆
2−λ⋆
3=/parenleftBigg
(K−1)p
2/parenleftbig
p·K+1
2+q/parenrightbig/parenrightBigg
−/parenleftbigg
−1 +p/parenleftbigg1
p+q+K+ 1
p(K+ 1) + 2 q/parenrightbigg/parenrightbigg
= 1−p
p+q−2p
p(K+ 1) + 2 q= 1−/parenleftbiggp2(K+ 1) + 2 pq+ 2p2+ 2pq
(p+q)(p(K+ 1) + 2 q)/parenrightbigg
= 1−p2(K+ 3) + 4 pq
p2(K+ 1) + pq(K+ 3) + 2 q2≥1−p2(K+ 3) + 4 pq
p2(K+ 3) + 4 pq+ 2q2>0.
Finally, to show λ⋆
3>0, we write
λ⋆
3+ 1 =p
p+q+p(K+ 1)
p(K+ 1) + 2 q>2p
p+q>1,
which allows us to complete the proof of Lemma A.27 .
Next, we argue that studying L⋆, which is formed by taking into account the weighted self-loops,
gives us an understanding that is not too far from that of L⋆
nl, which is formed by setting pvv= 0for
allv∈V.
Lemma A.28. LetPbe the diagonal matrix where P[v, v] =pvv. LetL⋆
nlbe the normalized
Laplacian of the graph formed by A⋆−P. Then, we have
∥L⋆−L⋆
nl∥op≤6K
n−2.
33Proof of Lemma A.28.RecallL⋆:=D⋆−A⋆. LetD⋆
nlbe deﬁned analogously to L⋆
nl. Observe that
we have
L⋆= (D⋆)−1/2L⋆(D⋆)−1/2
L⋆
nl= (D⋆
nl)−1/2L⋆(D⋆
nl)−1/2.
From this, we see that writing down the v, wth entry of the difference gives
(L⋆
nl−L⋆) [v, w] =L⋆[v, w]/parenleftBigg
1/radicalbig
(d⋆[v]−pvv)(d⋆[w]−pww)−1/radicalbig
d⋆[v]d⋆[w]/parenrightBigg
.
This resolves to different forms based on whether v=w. When v=w, evaluating the formula
gives
(L⋆
nl−L⋆) [v, v] =d⋆[v]−pvv
d⋆[v]−pvv−d⋆[v]−pvv
d⋆[v]=pvv
d⋆[v].
When v̸=w, we apply Lemma A.25 and get
|(L⋆
nl−L⋆) [v, w]|=pvw/parenleftBigg
1/radicalbig
(d⋆[v]−pvv)(d⋆[w]−pww)−1/radicalbig
d⋆[v]d⋆[w]/parenrightBigg
≤Kp/parenleftbigg1
np/2−p−1
np/2/parenrightbigg
=4K
n2−2n.
Using this analysis and applying Lemma A.24 gives
∥L⋆
nl−L⋆∥op≤max
v∈Vpvv
d⋆[v]+nmax
v̸=wpvw/parenleftBigg
1/radicalbig
(d⋆[v]−pvv)(d⋆[w]−pww)−1/radicalbig
d⋆[v]d⋆[w]/parenrightBigg
≤2K
n+nmax
v̸=wpvw/parenleftBigg
1/radicalbig
(d⋆[v]−pvv)(d⋆[w]−pww)−1/radicalbig
d⋆[v]d⋆[w]/parenrightBigg
≤2K
n+4K
n−2≤6K
n−2,
completing the proof of Lemma A.28 .
This gives Lemma A.29 , which means we can use u⋆
2as a suitable proxy for sign(u2(L⋆
nl)).
Lemma A.29. There exists a constant C(α, K )depending on αandKsuch that we have
∥u2(L⋆
nl)−u⋆
2∥∞≤C(α, K )
n.
This implies that for all nsufﬁciently large, we have sign(u2(L⋆
nl)) = sign(u⋆
2).
Proof of Lemma A.29.By Lemma A.27 , Weyl’s inequality, and Lemma A.28 , we know that for all
nsufﬁciently large,
λ⋆
2−λ3(L⋆
nl) = (λ⋆
2−λ⋆
3) + (λ⋆
3−λ3(L⋆
nl))
≥/parenleftbigg
1−p2(K+ 3) + 4 pq
p2(K+ 3) + 4 pq+ 2q2/parenrightbigg
−CA.28
n≥C1(α, K ).
Combining this with Lemma A.28 again, the Davis-Kahan inequality tells us that
∥u2(L⋆
nl)−u⋆
2∥∞≤∥u2(L⋆
nl)−u⋆
2∥2≤C2(α, K )
n,
and then using the fact that ∥u⋆
2∥∞= 1/√n(arising from Lemma A.27 ) completes the proof of
Lemma A.29 .
We are now ready to prove the inconsistency of normalized spectral bisection on the nested block
examples.
34Proof of Theorem 3.LetGbe a graph drawn from the nested block example. We choose pandq
such that p≳logn/n andp/q=α≥2where αis some constant and such that pandqboth
satisfy the conditions of Theorem 1. Let K≥3α. Observe that the true communities are LandR.
We will show that bisection based on u2ofI−L (corresponding to the eigenvector associated with
the second smallest eigenvalue of L) will attain a large misclassiﬁcation rate. In particular, based
on our calculation in Lemma A.27 , we expect that u2will output a bisection that places L1andL2
into separate clusters. On the other hand, by Theorem 1, for all nlarge enough, the unnormalized
spectral bisection algorithm will be strongly consistent.
First, observe that it is enough to prove the inconsistency result just for the symmetric normalized
Laplacian. Indeed, observe that if u2is an eigenvector of I−L =D−1/2AD−1/2, then we have
λ2D−1/2u2=D−1AD−1/2u2=D−1A(D−1/2u2),
which shows that D−1/2u2must be the eigenvector of the random-walk normalized Laplacian I−
D−1Acorresponding to eigenvalue λ2. Since Dis a positive diagonal matrix, it does not change
the signs of u2and therefore the output of the normalized spectral bisection algorithm is the same.
Our general approach to prove the inconsistency is to use the Davis-Kahan Theorem, a bound on
∥L−L⋆
nl∥op, and a bound on the gap λ⋆
2−λ3. Letdminbe the minimum degree of the graph given
by adjacency matrix Aand let d⋆
minbe the minimum weighted degree of the graph given by the
adjacency matrix A⋆. First, using [ DLS21 , Theorem 3.1], we have with probability 1−n−rfor
some constant r≥1and constants C(r)andC(the latter of which does not depend on r), for all n
sufﬁciently large,
∥L−L⋆
nl∥op≤C(r)/parenleftbig
nmax (i,j)pij/parenrightbig5/2
min{dmin,d⋆
min}3
≤C(r) (n·Kp)5/2
min/braceleftBig
n(p+q)/3, n(p+q)/3−C/radicalbig
n(p+q) logn/bracerightBig3
≤C1(r, α)K5/2(np)5/2
(np)3=C1(r, α)K5/2
√np.
Next, we invoke Lemma A.28 to write
λ2(L⋆
nl)−λ3= (λ⋆
2−λ⋆
3) + (λ⋆
3−λ3) + (λ2(L⋆
nl)−λ⋆
2)
≥/parenleftbigg
1−p2(K+ 3) + 4 pq
p2(K+ 3) + 4 pq+ 2q2/parenrightbigg
−C2(r, α)K5/2
√np−CA.28
n≥Cg(α, K ),
where the last line denotes a positive constant depending on qandK(this constant will always be
positive for sufﬁciently large n, as we showed that λ⋆
2−λ⋆
3>0in Lemma A.27 ).
Putting everything together, we get by the Davis-Kahan theorem that some signing of u2satisﬁes
∥u2−u2(L⋆
nl)∥2≤∥L−L⋆
nl∥op
min{|λ2(L⋆
nl)−λ3|,1−λ2(L⋆
nl)}≤C3(r)K5/2
C′g(α, K )√np≤C4(r, α, K )√np.
Now, consider the subset of coordinates of u2belonging to L1. Suppose mof these coordinates do
not agree in sign with u⋆
2. To maximize m, each of these coordinates in u2should be 0, so using this
reasoning and applying Lemma A.29 means the total ℓ2error can be bounded (using Lemma A.28 )
as
m/parenleftBigg
1/radicalbig
n/2−CA.28
n/parenrightBigg2
≤∥u2−u2(L⋆
nl)∥2
2≤C4(r, α, K )2
np.
This means the number of coordinates mon which u2andu⋆
2disagree on is at most
n·C5(r, α, K )2
2np,
and therefore the misclassiﬁcation rate of u2with respect to the true labeling induced by LandR
must be at least
n
4−n·C5(r,α,K )2
2np
n=1
4−C5(r, α, K )2
2np.
Since p≳logn/n, this completes the proof of Theorem 3.
35Figure 2: Agreement with the planted bisection of the bipartition obtained from unnormalized spec-
tral bisection, for graphs generated from a distribution in NSSBM (n, p,p, q)for ﬁxed values of n,p
and varying values of p > q . The left plot uses p= 1/2, the right plot uses p= 1. The solid
red curves plot the function pthr(q)(see ( 14)), and the dashed red curves plot the function pinfo(q)
(see ( 15)).
B Additional experiments
In this section, we show more numerical trials that complement those discussed in Section 4.
B.1 Varying edge probabilities in an NSSBM
In Section 4, we investigated the behavior of an NSSBM model by ﬁxing the values of p, qand
varying the largest edge probability p. Here, we take an alternative approach, and instead ﬁx pand
vary the values of pandq.
Setup. Let us ﬁx n= 2000 ,p∈ {1/2,1}. For varying p, qin the range [1/n,9/20]such
thatp > q , we sample t= 3 independent draws Gfrom the same benchmark distribution Dp,p,q
used in Section 4. For each of them, we compute the agreement of the bipartition obtained by
unnormalized spectral bisection with respect to the planted bisection. For each (p, q), we plot the
average agreement across the tindependent draws. The results are shown in Fig. 2, where in the left
and right plot we ran the experiments with p= 1/2andp= 1respectively. The lower diagonal of
these plots, where p≤q, is artiﬁcially set to 0.
Theoretical framing. According to Theorem 1, ﬁxing the value of p∈{1/2,1}, we obtain that
unnormalized spectral bisection achieves exact recovery provided that for q∈[1/n,9/20]one has
p≥pthr(q)where
pthr(q) =√plogn√n+q (14)
is obtained by rearranging the precondition of Theorem 1, ignoring the constants, and disregarding
the fact that αshould be O(1). The solid red curve in Fig. 2plots pthr(q)as a function of q. For
comparison, the information-theoretic threshold for SSBM [ ABH16 ] demands that p≥pinfo(q)
where
pinfo(q) =/parenleftBigg
√
2/radicalbigg
logn
n+√q/parenrightBigg2
. (15)
The dashed red curve in Fig. 2plots pinfo(q)as a function of q.
Empirical evidence. From Fig. 2, one can see that our experiments reﬂect the behavior predicted
by Theorem 1quite closely, although empirically we achieve 100% agreement slightly above pthr(q)
(i.e. the solid red curve). However, this is likely due to the constant factors from Theorem 1that we
ignored, and also n= 2000 is plausibly too small to show asymptotic behaviors. Nevertheless, we
do achieve 100% agreement consistently as soon as we surpass the information-theoretic threshold
pinfo(q): in the regime of our experiment, it appears that the unnormalized Laplacian is robust all the
way to the optimal threshold for exact recovery in the SSBM.
36Figure 3: Agreement with the planted bisection of the bipartition obtained from several matrices
associated with an input graph generated from a distribution DG1,G2q∈DCM (n, d in, q)for ﬁxed
values of n, qand varying the size of the planted clique S. In the left plot, the bipartition is the 0-cut
of the second eigenvector, as in Algorithm 1. In the right plot, the bipartition is the sweep cut of the
ﬁrstn/2vertices in the second eigenvector.
B.2 Varying the size of a planted clique in a DCM
In some sense, the experiments from Section 4and Appendix B.1can be thought of as experiments
for the deterministic clusters model too. This is because each realization of the internal edges gives
rise to a different DCM distribution (see Section 2). We complement our previous discussion by
illustrating the behavior of certain families of DCM distributions that are conceptually different than
those considered in Section 4.
Benchmark distribution. Letnbe divisible by 4and let{P1, P−2}be a partitioning of V= [n]
into two equally-sized subsets. Fix p∈[0,1]. For some set S⊆P1such that S={1, . . . ,|S|}(for
simplicity), let G2= (P2, E2)∼ER(n/2, p)be a graph drawn from the Erd ˝os-Rényi distribution
with sampling rate p, and let G1= (P1, E1)∼ERPC (n/2, p, S )be also a graph drawn from the
Erd˝os-Rényi distribution with sampling rate pwhere we additionally plant a clique on the vertices
S. Fixing G1, G2, for q∈[0,1]we consider the distribution DG1,G2q over graphs G= (V, E)
where G[P1] =G1,G[P2] =G2, and every edge (u, v)∈P1×P2is sampled independently with
probability q. One can see thatDG1,G2q is in fact in the set DCM (n, d in, q)for some din.
Setup. Let us ﬁx n= 2000 ,p= 9/√n,q= 1/√n. For varying values of |S|in the range
[|P1|/10,|P1|], we sample G1= (P1, E1)∼ERPC (n/2, p, S )andG2= (P2, E2)∼ER(n/2, p),
and then draw t= 10 independent samples GfromDG1,G2q . For each sample G, we run spectral
bisection (i.e. Algorithm 1) with matrices L,Lsym,Lrw,A. Then, we compute the agreement of
the bipartition hence obtained with respect to the planted bisection, and average it out across the t
independent draws. The results are shown in the left plot of Fig. 3. Again, another natural way to get
a bipartition of Vfrom the eigenvector is a sweep cut, and the average agreements that this results
in are shown in the right plot of Fig. 3.
Theoretical framing. Ignoring the constants, Theorem 2guarantees that exact re-
covery is achieved by unnormalized spectral bisection as long as din≥nq+√nand
λ3(/hatwideL)−λ2(/hatwideL)≥√n+nq+√nqlogn+ log n, where/hatwideLis the expected Laplacian of DG1,G2q .
For each clique size that we consider, Fig. 4shows the minimum in-cluster degree of the graphs
G1, G2that we draw (in the left plot), and the spectral gap λ3(/hatwideL)−λ2(/hatwideL). The red hori-
zontal lines in the left and right plot respectively correspond to the value of nq+√nand√n+nq+√nqlogn+ log non the y-axis, indicating the lower bound on dinandλ3(/hatwideL)−λ2(/hatwideL)
demanded by Theorem 2.
Empirical evidence: consistency. From Fig. 4, one can see that all the distributions DG1,G2q
that we use roughly meet the requirement of Theorem 2. Indeed, in the left plot of Fig. 3one
sees that unnormalized spectral bisection consistently achieves exact recovery for all clique sizes.
On the contrary, the bipartition obtained by running spectral bisection with the adjacency matrix A
37Figure 4: The minimum in-cluster degree dinand the spectral gap λ3(/hatwideL)−λ2(/hatwideL)of distributions
DG1,G2q∈DCM (n, d in, q)with ﬁxed values of n, qand varying the size of the planted clique S. The
red horizontal line on the left corresponds to the value nq+√n, the red horizontal line on the right
corresponds to the value√n+nq+√nqlogn+ log n.
misclassiﬁes a fraction of the vertices for certain sizes of the planted clique. Nevertheless, the sweep
cut obtained from all the matrices recovers the planted bisection exactly.
Empirical evidence: example embedding. Let us ﬁx the value |S|= 800 for the size of the
planted clique, for which we see in Fig. 3that the adjacency matrix fails to recover the planted
bisection. We generate a graph from a distribution DG1,G2q with clique size|S|= 800 , and plot
how the vertices are embedded in the real line by the second eigenvector of all the matrices we
consider. The result is shown in Fig. 5, where the three horizontal dashed lines, from top to bottom,
respectively correspond to the value of 1/√n,0,−1/√non the y-axis. Graphically, one can see that
the embedding in the unnormalized Laplacian is indeed the one that moves the least away from the
values±1/√n, and in fact the vertices {1, . . . , 800}⊆P1where we plant the clique concentrate
even more around 1/√n. This is a phenomenon related to the one illustrated by Fig. 1. Finally, one
can see from the embedding that splitting vertices around 0does result in misclassifying a fraction
of the vertices for the adjacency matrix. However, taking a sweep cut that splits the vertices into two
equally sized parts recovers the planted bisection for all matrices. This reﬂects the results shown in
Fig.3.
38Figure 5: Embedding of the vertices given by the second eigenvector u2of several matrices as-
sociated with a graph sampled from a distribution DG1,G2q∈DCM (n, d in, q), with the size of the
planted clique set to |S|= 2/5·n. Horizontal dashed lines, from top to bottom, correspond to
1/√n,0,−1/√nrespectively.
39C NeurIPS paper checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?
Answer: [Yes]
Justiﬁcation: Please see Theorem 1, Theorem 2, and Theorem 3for formal theoretical
results. Please see Section 4and Appendix Bfor numerical results.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reﬂect how
much the results can be expected to generalize to other settings.
•It is ﬁne to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: Please see Theorem 1, Theorem 2, and Theorem 3for formal theoretical
results that include all assumptions and a corresponding discussion. Please see Section 4.1
for a set of open questions that we do not address in this work. Please see Section 4and
Appendix Bfor a numerical evaluation of our theoretical results, where we test our theory
beyond the statements of our theoretical results.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
•The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-speciﬁcation, asymptotic approximations only holding locally). The au-
thors should reﬂect on how these assumptions might be violated in practice and what
the implications would be.
•The authors should reﬂect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reﬂect on the factors that inﬂuence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
•The authors should discuss the computational efﬁciency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciﬁcally instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
40Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justiﬁcation: Please see the proofs of Theorem 1(Appendix A.7.1 ), Theorem 2(Ap-
pendix A.7.2 ), and Theorem 3(Appendix A.8), along with all lemmas referenced therein
in the appendices. Please also see Section 3for a proof sketch of all the main results.
Guidelines:
•The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
•Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justiﬁcation: Please see Section 4and Appendix Bfor details. We have also attached our
code.
Guidelines:
•The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or veriﬁable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
41(d)We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justiﬁcation: We have uploaded our experimental code with our submission.
Guidelines:
•The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not
be possible, so No is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justiﬁcation: Please see Section 4and Appendix B.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Signiﬁcance
Question: Does the paper report error bars suitably and correctly deﬁned or other appropri-
ate information about the statistical signiﬁcance of the experiments?
Answer: [No]
Justiﬁcation: It would be computationally expensive to do so, especially for large graph
sizes n. Decreasing nis not feasible because for small values of n, the asymptotic conver-
gence of the algorithms is not evident.
Guidelines:
42•The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, conﬁ-
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
•The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not veriﬁed.
•For asymmetric distributions, the authors should be careful not to show in tables or
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding ﬁgures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justiﬁcation: Please see the top of Section 4.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justiﬁcation: We attest that the research conducted through the course of this work adheres
to the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
43Justiﬁcation: This is a theoretical paper on the robustness of a common unsupervised learn-
ing algorithm.
Guidelines:
•The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
ciﬁc groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efﬁciency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justiﬁcation: This is a theoretical paper on the robustness of a common unsupervised learn-
ing algorithm.
Guidelines:
•The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety ﬁlters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justiﬁcation: Please see the top of Section 4.
Guidelines:
•The answer NA means that the paper does not use existing assets.
•The authors should cite the original paper that produced the code package or dataset.
44•The authors should state which version of the asset is used and, if possible, include a
URL.
•The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justiﬁcation: NA
Guidelines:
•The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip ﬁle.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justiﬁcation: NA
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
•Including this information in the supplemental material is ﬁne, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justiﬁcation: NA
Guidelines:
45•The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
•Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
•We recognize that the procedures for this may vary signiﬁcantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
46