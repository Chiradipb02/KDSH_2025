Parameter Symmetry and Noise Equilibrium
of Stochastic Gradient Descent
Liu Ziyin
Massachusetts Institute of Technology,
NTT Research
ziyinl@mit.eduMingze Wang
Peking University
mingzewang@stu.pku.edu.cn
Hongchao Li
The University of Tokyo
lhc@cat.phys.s.u-tokyo.ac.jpLei Wu
Peking University
leiwu@math.pku.edu.cn
Abstract
Symmetries are prevalent in deep learning and can significantly influence the
learning dynamics of neural networks. In this paper, we examine how expo-
nential symmetries – a broad subclass of continuous symmetries present in the
model architecture or loss function – interplay with stochastic gradient descent
(SGD). We first prove that gradient noise creates a systematic motion (a “Noether
flow”) of the parameters θalong the degenerate direction to a unique initialization-
independent fixed point θ∗. These points are referred to as the noise equilibria
because, at these points, noise contributions from different directions are balanced
and aligned. Then, we show that the balance and alignment of gradient noise can
serve as a novel alternative mechanism for explaining important phenomena such
as progressive sharpening/flattening and representation formation within neural
networks and have practical implications for understanding techniques like repre-
sentation normalization and warmup.
1 Introduction
Stochastic gradient descent (SGD) and its variants have become the cornerstone algorithms used in
deep learning. In the continuous-time limit, the algorithm can be written as [19, 13, 21, 32, 9]:
dθt=−∇L(θt)dt+√
2σ2Σ(θt)dWt, (1)
where Σ(θ)is the covariance matrix of gradient noise (Section 3) with the prefactor σ2=η/(2S)
modeling the impact of a finite learning rate ηand batch size S;Wtdenotes the Brownian motion.
When σ=0, Eq. (1) corresponds to gradient descent (GD)1. However, SGD and GD can exhibit
significantly different behaviors, often converging to solutions with significantly different levels of
performance [31, 39, 44, 22, 49]. Notably, even when σ2≪1, where we expect a close resemblance
between SGD and GD over finite time [19], their long-time behaviors still differ substantially [26].
These observations indicate that gradient noise can bias the dynamics significantly, and revealing its
underlying mechanism is thus crucial for understanding the disparities between SGD and GD.
Contribution. In this paper, we study the how of SGD noise biases training through the lens of
symmetry. Our key contributions are summarized as follows. We show that
1. when symmetry exists in the loss function, the dynamics of SGD can be precisely characterized
and is different from GD along the degenerate direction;
38th Conference on Neural Information Processing Systems (NeurIPS 2024).
1“Gradient descent” and “gradient flow” are used interchangeably as we work in the continuous-time limit.2. the treatment of common symmetries, including the rescaling and scaling symmetry in deep
learning, can be unified in a single theoretical framework that we call the exponential symmetry;
3. for any θ, every exponential symmetry implies the existence of a unique and attractive fixed point
along the degenerate direction for SGD;
4. symmetry and balancing of noise can serve as novel mechanisms for important deep learning
phenomena such as progressive sharpening/flattening and latent representation formation.
Figure 1: An example of a 2d loss
function with scale invariance: ℓ(θ)=
ℓ(λθ)for a scalar λandθ∈R2. Be-
cause of the symmetry, the gradient ∇ℓ
must be tangential to the circles whose
center is the origin. This implies that
the norm ∥θ∥does not change during
gradient flow training. However, when
the training is stochastic or discrete-
time, SGD must move outward. If the
model starts at θt, it must move to a
larger circle. As an illustrative exam-
ple, this loss function has a unique and
attractive fixed point: ∥θ∥=∞. SGD
will diverge after training under scale
invariance. Also, see Remark 4.4 for
a discussion of the difference between
discrete-time and continuous-time dy-
namics.See Figure 1 for an illustration of how symmetry leads to a
systematic flow of SGD. This work is organized as follows.
We discuss the most relevant works in Section 2. The main
theoretical results are presented in Section 4. We apply our
theory to understand specific problems and present numerical
results in Section 5. The last section concludes this work. All
the proofs are presented in the Appendix.
2 Related Works
The dynamics of SGD in the degenerate directions of the loss
landscape is a poorly understood problem. There are two
closely related prior works. Ref. [48] studies the dynamics of
SGD when there is a simple rescaling symmetry and applies
it to derive the stationary distribution of SGD for linear net-
works. Our result is more general because rescaling symmetry
is the simplest case of exponential symmetries2. Another re-
lated work is Ref. [20], which studies a different special case
of exponential symmetry, the scale invariance, and in the pres-
ence of weight decay. Their analysis assumes the existence
of the fixed point of the dynamics, which we proved to exist.
Also related is the study of conservation laws under gradient
flow [34, 18, 24, 43, 40], which we will discuss more closely in
Section 4. However, these works do not take the stochasticity
of training into account. Comparing with these results that as-
sume no stochasticity, our result could suggest that SGD con-
verges to initialization-independent solutions, whereas the GD
finds solutions are strongly initialization-dependent. In addi-
tion, Section D extends our main result to discrete-time SGD.
3 Preliminaries
Setup and Notations. Letℓ∶Ω×Z↦Rdenote the per-sample loss, with ΩandZdenoting
the parameter and sample space, respectively. Here, z∈Zincludes both the input and label and
accordingly. We use Ez=Eto denote the expectation over a given training set. Therefore, L(θ)=
Ez[ℓ(θ, z)]is the empirical risk function. The covariance of gradient noise is given by
Σ(θ)=Ez[∇ℓ(θ, z)∇ℓ(θ, z)⊺]−∇L(θ)∇L(θ)⊺.
Additionally, we use Σv(θ)∶=Ez[∇vℓ(θ, z)∇vℓ(θ, z)⊺]−∇vL(θ)∇vL(θ)⊺to denote the covari-
ance of gradient noise impacting on the subset of parameters v. Denote by (θt)t≥0the trajectory of
SGD or GD. For any h∶Ω↦R, we write ht=h(θt)and˙h(θt)=d
dth(θt)for brevity. When the
context is clear, we also use ℓ(θ)to denote ℓ(θ, z).
Symmetry. The per-sample loss ℓ(⋅,⋅)is said to possess the Q-symmetry if
ℓ(θ, z)=ℓ(Qρ(θ), z),∀ρ∈R, (2)
where(Qρ)ρ∈Ris a set of continuous transformation parameterized by ρ∈R. Without loss of
generality, we assume Q0=id. The most common symmetries exist within the model f, namely
fθis invariant under certain transformations of θ. However, our formalism is slightly more general
in the sense that it is also possible for the model to be variant while the per-sample loss remains
unchanged, which appears in self-supervised learning [50], for example.
2These are known as “continuous symmetries.” Prior works also studied SGD training under discrete sym-
metries [45, 3], which are different from continuous symmetries.
24 Continuous Symmetry and Noise Equilibria
Taking the derivative with respect to ρatρ=0in Eq. (2), we have
0=∇θℓ(θ, z)⋅J(θ), (3)
where J(θ)=dQρ(θ)
dρ∣ρ=0. Denote by Cbe the antiderivative of J, that is,∇C(θ)=J(θ). Then,
taking the expectation over zin (3) gives the following conservation law for GD solutions (θt)t≥0:
˙C(θt)=0. (4)
Essentially, this is a consequence of Noether’s theorem [25], and Cwill be called a “Noether charge”
in analogy to theoretical physics. The conservation law (4) implies that the GD trajectory is con-
strained on the manifold {θ∶C(θ)=C(θ0)}. We refer to Ref. [18] for a study of this type of
conservation law under the Bregman Lagrangian [16].
4.1 Noether Flow in Degenerate Directions
In this paper, we are interested in how C(θt)changes, if it changes at all, under SGD. By Ito’s
lemma, we have the following Noether flow (namely, the flow of the Noether charge):
˙C(θt)=σ2Tr[Σ(θt)∇2C(θt)], (5)
where∇2Cdenotes the Hessian matrix of C. The derivation is deferred to Appendix B. By def-
inition, Σ(θt)is always positive semidefinite (PSD). Thus, we immediately have: if ∇2Cis PSD
throughout training, C(θt)is a monotonically increasing function of time. Conversely, if ∇2
θCis
negative semidefinite (NPD), C(θt)is a monotonically decreasing function of time.
The existence of symmetry implies that (with suitable conditions of smoothness) any solution θ
resides within a connected, loss-invariant manifold, defined as Mθ∶={Qρ(θ)∶ρ∈R}. We
term directions within this manifold as “degenerate directions” since movement along them does
not change the loss value. Notably, the biased flow (5) suggests that SGD noise can drive SGD to
explore within this manifold along these degenerate directions since the value of C(θ)forθ∈Mθ
can vary.
4.2 Exponential symmetries
Now, let us focus on a family of symmetries that is common in deep learning. Since the correspond-
ing conserved quantities are quadratic functions of the model parameters, we will refer to this class
of symmetries as exponential symmetries .
Definition 4.1. (Qρ)ρis said to be a exponential symmetry if J(θ)∶=d
dρQρ(θ)∣ρ=0=Aθfor a
symmetric matrix A.
This implies when ρ≪1,Qρ=id+ρA+o(ρ). In the sequel, we also use the words “ A-symmetry”
and “Q-symmetry” interchangeably since all properties of Qρwe need can be derived from A. This
definition applies to the following symmetries that are common in deep learning:
•Rescaling symmetry :Qρ(a, b)=(a(ρ+1), b/(ρ+1)), which appears in linear and ReLU networks
[7, 48]. In this symmetry, A=diag(Ia,−Ib), where Iis the identity matrix with dimensions
matching that of aandb.
•Scaling symmetry :Qρθ=(ρ+1)θ, which exists whenever part of the model normalized using
techniques like batch normalization [14], layer normalization [2], or weight normalization [29].
In this case, A=I.
•Double rotation symmetry: This symmetry appears when parts of the model involve a matrix
factorization problem, where for an arbitrary invertible matrix B ℓ=ℓ(UW)=ℓ(UBB−1W).
Writing the exponential symmetry for this case is a little cumbersome. We need first to view U
andWas a single vector, and the exponential transformation is given by a block-wise diagonal
matrix diag(B, ..., B, B−1, ..., B−1). See Section 5.1 for more detail.
It is possible for only a subset of parameters to have a given symmetry. Mathematically, this cor-
responds to the case when Ais low-rank. It is also common for ℓto have multiple exponential
symmetries at once, often for different (but not necessarily disjoint) subsets of parameters. For
example, a ReLU network has a different rescaling symmetry for every hidden neuron.
3It is obvious that under this Qsymmetry, the Noether charge has a simple quadratic form:
C(θ)=θ⊺Aθ. (6)
Moreover, the interplay between this symmetry and weight decay can be explicitly characterized in
our framework. To this end, we need the following definition.
Definition 4.2. For any γ∈R, we say ℓγ(θ, x)∶=ℓ(θ, x)+γ∥θ∥2has the Qsymmetry as long as
ℓ(θ, x)has the Qsymmetry.
For the SGD dynamics that minimizes Lγ(θ)=Ex[ℓγ(θ, x)], it follows from (5) that
˙C(θt)=−4γC(θt)+σ2Tr[Σ(θt)A]=∶G(θt). (7)
Thus, a positive γalways causes ∣C(θt)∣to decay, and the influence of symmetry is determined by
the spectrum of A. Denote by A=∑jµjnjn⊺
jthe eigendecomposition of A. Then,
Tr[Σ(θt)A]=∑
i∶µi>0µin⊺
iΣ(θt)ni+∑
j∶µj<0µjn⊺
jΣ(θt)nj.
This gives a clear interpretation of the interplay between SGD noise and the exponential symmetry:
the noise along the positive directions of Acauses C(θt)to grow, while the noise along the negative
directions causes C(θt)to decay. In other words, the noise-induced dynamics of C(θt)is deter-
mined by the competition between the noise along the positive- and negative-eigenvalue directions
ofA.
Time Scales. The above analysis implies that the dynamics of SGD can be decomposed into two
parts: the dynamics that directly reduce loss, and the dynamics along the degenerate direction of the
loss, which is governed by Eq (5). These two dynamics have essentially independent time scales.
The first part is independent of the σ2in expectation, whereas the time scale of the dynamics in the
degenerate directions depends linearly on σ2.
The first time scale termis due to the dynamics of empirical risk minimization. The second time scale
tequiis the time scale for Eq. (5) to reach equilibrium, which is irrelevant to direct risk minimization.
When the parameters are properly tuned, termis of order 1, whereas tequiis proportional to σ2=
η/(2S). Therefore, when σ2is large, the parameters will stay close to the equilibrium point early in
the training, and one can expect that ˙C(θt)is approximately zero after tequi. In line with Ref. [20],
this can be called the fast-equilibrium phase of learning. Likewise, when σ2≪1, the approach to
equilibrium will be slower than the actual time scale of risk minimization, and the dynamics in the
degenerate direction only take off when the model has reached a local minimum. This can be called
the slow-equilibrium phase of learning.
4.3 Noise Equilibrium and Fixed Point Theorem
It is important and practically relevant to study the stationary points of dynamics in Eq. (7). For-
mally, the stationary point is reached when −γC(θ)+ηTr[Σ(θ)A]=0. Because we make essentially
no assumption about ℓ(θ)andΣ(θ), one might feel that it is impossible to guarantee the existence of
a fixed point. Remarkably, we prove below that a fixed point exists and is unique for every connected
degenerate manifold.
To start, consider the exponential maps generated by A:
eλAθ∶=lim
ρ→0(I+ρA+o(ρ))λ/ρθ,
which applies the symmetry transformation to θforλ/ρtimes. Then, it follows that if we apply Qρ
transformation to θinfinitely many times and for a perturbatively small ρ,
ℓ(θ)=ℓ(eλAθ). (8)
Thus, the exponential symmetry implies the symmetry with respect to an exponential map, a fun-
damental element of Lie groups [11]. Note that exponential-map symmetry is also an exponential
symmetry by definition. For the exponential map, the degenerate direction is clear: for any λ,θ
connects to eλAθwithout any loss function barrier. Therefore, the degenerate direction for any ex-
ponential symmetry is unbounded. Now, we prove the following fixed point theorem, which shows
that for every exponential symmetry and every θ, there is one and only one corresponding fixed point
in the degenerate direction.
4Theorem 4.3. Let the per-sample loss satisfy the A-exponential symmetry and θλ∶=exp[λA]θ.
Then, for any θand any γ≥0,3
(1)G(θλ)(Eq. (7)) and−C(θλ)are monotonically decreasing functions of λ;
(2) there exists a λ∗∈R∪{±∞}such that G(θλ∗)=0;
(3) in addition, if G(θλ)≠0,λ∗is unique and G(θλ)is strictly monotonic;
(4) in addition to (3), if Σ(θ)is differentiable, λ∗(θ)is a differentiable function of θ.
Remark 4.4.It is now worthwhile to differentiate gradient flow (GF), GD, SGD, and stochastic
gradient flow (SGF). Technically, one can prove that the same result holds for discrete-time GD and
SGD in expectation, and GF is the only of the four algorithms that do not obey this theorem (See
Section D), and so one could argue that the discrete step size is the essential cause of noise balance.
Mathematically, the SGF can be seen as a model of the leading order effect of having a finite step
size and thus also share this effect (remember that the Ito Lemma contains a second-order term in
dθ). That being said, there is a practical caveat: in practice, we find it much easier for models to
reach these fixed points with SGD than with GD, and so it is fair to say that this effect is the most
dominant when gradient noise is present.
Part (1), together with Part (2), implies that the unique stationary point is essentially attractive. This
is because ˙Cdecreases with λwhile Cincreases with it. Let C∗=C(θλ∗). Thus, C(θ)−C∗
always have the opposite sign of λ∗, whiled
dtC(θ)will have the same sign. Conceptually, this
means that Cwill always move to reduce its distance to C∗. Assuming that C∗is a constant in
time (or close to a constant, which is often the case at the end of training), Part (1) implies that
d
dt(C(θ)−C∗)∝−sgn(C(θ)−C∗), signaling a convergence to C(θ)=C∗. In other words, SGD
will move to restore the balance if it is perturbed away from λ∗=0. If the matrix ΣAis well-
behaved, one can indeed establish the convergence to the fixed point in the relative distance even if
C∗is mildly divergent due to diffusion. Because this part is strongly technical and our focus is on
the fixed points, we leave the formal statement and its discussion to Appendix B.3.
Theorem 4.5. (Informal) Let C∗follow a drifted Brownian motion and ΣAsatisfy two well-behaved
conditions. Then, either C−C∗→0inL2or(C−C∗)2/(C∗)2→0in probability.
Parts (2) and (3) show that a unique fixed point exists. We note that it is more common than not
for the conditions of uniqueness to hold because there is generally no reason for Tr[Σ(θ)A]or
Tr[θθ⊺A]to vanish simultaneously, except in some very restrictive subspaces. One major (perhaps
the only) reason for the first trace to vanish is when the model is located at an interpolation mini-
mum. However, interpolation minima are irrelevant for modern large-scale problems such as large
language models because the amount of available text for training far exceeds the size of the largest
models. Even when the interpolation minimum exists, the unique fixed point should still exist when
the training is not complete. See Figure 1. Part (4) means that the fixed points of the dynamics is
well-behaved. If the parameter θhas a small fluctuation around a given location, Cwill also have a
small fluctuation around the fixed point solution. This justifies approximating Cby a constant value
when θchanges slowly and with small fluctuation.
Fixed point as a Noise Equilibrium. Letθ∗be a fixed point of (7). It must satisfy
4γC(θ∗)=σ2Tr[Σ(θ∗)A]. (9)
Hence, a large weight decay leads to a small ∣C(θ∗)∣, whereas a large gradient noise leads to a large
∣C(θ∗)∣. When there is no weight decay, we get a different equilibrium condition: Tr[Σ(θ∗)A]=0,
which can be finite only when Acontains both positive and negative eigenvalues. This equilibrium
condition is equivalent to ∑i∶µi>0µin⊺
iΣ(θ∗)ni=−∑j∶µj<0µjn⊺
jΣ(θ∗)nj. Namely, the overall
gradient fluctuation in the two different subspaces specified by the symmetry Amust balance. We
will see that the main implication of this result is that the gradient noise between different layers of a
deep neural network should be balanced at the end of training. Conceptually, Theorem 4.3 suggests
the existence of a special type of fixed point for SGD, which the following definition formalizes.
Definition 4.6. θis anoise equilibrium for a nonconstant function C(θ)if˙C(θ)=0under SGD.
5 Applications
Now, we analyze the noise equilibria of a few important problems. These examples are prototypes
of what appears frequently in deep learning practice and substantiate our arguments with numerical
3A similar result can be proved for the discrete-time SGD. See Section D.
5Figure 2: Comparison between GD and SGD for matrix factorizations. Left: Example of a learning trajectory.
The convergence speed is almost exponential-like in experiments. Mid: evolution of 10individual elements of
∆ij∶=(U⊺ΓUU−WΓWW⊺)ij. As the theory shows, they all move close to zero and fluctuate with a small
variance. Right : Converged solutions of SGD agree with the prediction of Theorem 5.2, but are an order of
magnitude away from the solution found by GD, even if they start from the same init.
examples. In addition, an experiment with the scale invariance in normalized tanh networks is
presented in Appendix A.1.
5.1 Generalized Matrix Factorization
Exponential symmetry is also observed when the model involves a (generalized) matrix factoriza-
tion. This occurs in standard matrix completion problems [33] or within the self-attention of trans-
formers through the key and query matrices [35]. For a (generalized) matrix factorization problem,
we have the following symmetry in the objective:
ℓ(U, W, θ′)=ℓ(UA, A−1W, θ′) (10)
for any invertible matrix Aand symmetry-irrelevant parameters θ′. We consider matrices Athat are
close to identity: A=I+ρB+O(ρ2), and A−1=I−ρB+O(ρ2). Therefore, for an arbitrary
symmetric B, we have a conserved quantity for GD: CB(θ)=Tr[UBU⊺]−Tr[W⊺BW].This
conservation law can also be written in the matrix form, which is a well-known result for GD [8, 24]:
(WtW⊺
t−U⊺
tUt)=(W0W⊺
0−U⊺
0U0). (11)
For SGD, applying (5) gives the following proposition.
Proposition 5.1. Suppose the symmetry (10) holds. Let U=(˜u1,⋯,˜ud2)⊺∈Rd2×d,W=
(˜w1,⋯,˜wd0)∈Rd×d0, where ˜ui,˜wj∈Rd. LetCB(θ)=Tr[UBU⊺]−Tr[W⊺BW]for any symmet-
ric matrix B∈Rd×d. Then, for SGD, we have
˙CB(θt)=σ2⎛
⎝d2
∑
i=1Tr[Σ˜ui(θt)B]−d0
∑
j=1Tr[Σ˜wj(θt)B]⎞
⎠.
This dynamics is analytically solvable when U∈R1×dandW∈Rd×1. In this case, taking B=Ek,l+
El,kwhere Ei,jdenotes the matrix with entries of 1at(i, j)and zeros elsewhere. For this choice
ofB, we obtain that CB(θ)=WkWl−UkUl, and applying the results we have derived, it is easy to
show that for some random variable r:˙CB(θt)=−Var[r(θt)]CB(θt),which signals an exponential
decay. For common problems, Var[r(θt)]>0[48]. Since the choice of Bis arbitrary, we have that
WkWl→UkUlfor all kandl. The message is rather striking: SGD automatically converges to a
solution where all neurons output the same sign ( sgn(Ui)=sgn(Uj)) at an exponential rate.
5.2 Balance and Stability of Matrix Factorization
As a concrete example, let us consider a two-layer linear network (this can also be seen as a variant
of standard matrix factorizations):
ℓγ=∥UWx−y∥2+γ(∥U∥2
F+∥W∥2
F). (12)
where x∈Rdxis the input data, and y=y′+ϵ∈Rdyis a noisy version of the label. The ground
truth mapping is linear and realizable: y′=U∗W∗x. The second moments of the input and noise
are denoted as Σx=E[xx⊺]andΣϵ=E[ϵϵ⊺], respectively. Note that this problem is essentially
identical to a matrix factorization problem, which is not only a theoretical model of neural networks
but also an important algorithm frequently in use for recommender systems [41]. The following
theorem gives the fixed point of Noether flow.
6Figure 3: A two-layer linear network after training. Here, the problem setting is the same as Figure 8. The
theoretical prediction is computed from Theorem 5.2. Left: balance of the norm is only achieved when ϕx=1,
namely, when the data has an isotropic covariance. We also test SGD with a small weight decay ( 10−4), which
is sufficiently small that the solution we obtained for SGD without SGD still holds approximately. In contrast,
training with GD + WD always converges to a norm-balanced solution. Right : the sharpness of the converged
model trained with SGD. We see that for some data distributions, SGD converges to a sharper solution, whereas
it converges to flatter solutions for other data distributions. This flattening and sharpening effect are both due
to the noise-balance effect of SGD. Here, we find that the systematic error between experiment and theory is
due to the use of a finite learning rate and decreases as we decrease η.
Theorem 5.2. Letr=UWx−ybe the prediction residual. For all symmetric B,˙CB=0if
WΓWW⊺=U⊺ΓUU, (13)
where ΓW=E[∥r∥2xx⊺]+2γI,ΓU=E[∥x∥2rr⊺]+2γI.
See Figure 8 for the convergence of SGD to this solution under different learning rate, batch size
and width. The equilibrium condition takes a more suggestive form when the model is at the global
minimum, where U∗W∗x−y=ϵ. Assuming that ϵandxare independent and that there is no weight
decay, we have:
W¯ΣxW⊺=U⊺¯ΣϵU (14)
Here, the bar over the matrices indicates that they have been normalized by their traces: ¯Σ=
Σ/Tr[Σ]. The matrices ΓWandΓUsimplifies because at the global minimum, ri=ϵiand
soE[∥x∥2rr⊺]=Tr[Σx]ΣϵandE[∥r∥2xx⊺]=Tr[Σϵ]Σx. This condition should be compared
with the alignment condition for GD in Eq. (11), where the alignment is entirely determined by
the initialization and perfect alignment is achieved only if the initialization is perfectly aligned.
This condition simplifies further if both ¯Σxand¯Σϵare isotropic, where the equation simplifies to
WW⊺/dx=U⊺U/dy. Namely, the two layers will be perfectly aligned, and the overall balance
depends only on input and output dimensions. Figure 2-Left shows an experiment that shows that
the two-layer linear net is perfectly aligned after training. Here, every point corresponds to the
converged solution of an independent run with the same initialization and training procedures but
different values of Σϵ. In agreement with the theory, the two layers are aligned according to The-
orem 5.2 under SGD, but not under GD. In fact, GD finds solutions that are more than an order of
magnitude away from SGD.
Noise Driven Progressive Sharpening and Flattening. This result implies a previously unknown
mechanism of progressive sharpening and flattening, where, during training, the stability of the
algorithm steadily improves (during flattening) or deteriorates (during sharpening) [39, 15, 6]. To
see this, we first derive a metric of sharpness for this model.
Proposition 5.3. For the per-sample loss (12), let S(θ)∶=Tr[∇2L(θ)]. Then, S(θ)=
dy∥WΣ1/2
x∥2
F+∥U∥2
FTr[Σx].
The trace of the Hessian is a good metric of the local stability of the GD and SGD algorithm be-
cause the trace upper bounds the largest Hessian eigenvalue. Let us analyze the simplest case of an
autoencoding task, where the model is at the global minimum. Here, Σx∝Idx,Σϵ∝Idy. For
a random Gaussian initialization with variance σ2
Wandσ2
U, the trace at initialization is, in expec-
tation, Sinit=dydTr[Σx](σ2
W+σ2
U). At the end of the training, the model is close to the global
minimum and satisfies Proposition 5.3. Here, the rank of UandWmatters and is upper bounded by
min(d, dx), and at the global minimum, UandWare full-rank (equal to min(d, dx)), and all the
singular values are 1. Thus,
{Sinit=dxd(σ2
U+σ2
W)Tr[Σx],
Send=2 min(d, dx)Tr[Σx].(15)
7The change in the sharpness during training thus depends crucially on the initialization scheme.
For Xavier init, σ2
U=(dy+d)−1andσ2
W=(d+dx)−1, and so Sinit≈Send(butSinitis slightly
smaller). Thus, for the Xavier init., the sharpness of loss experiences a small sharpening during
training. For Kaiming init., σ2
U=1andσ2
W=d−1
x. Therefore, it always holds that Sinit≥Send, and
so the stability improves as the training proceeds. The only case when the Kaiming init. does not
experience progressive flattening is when d=dx=dy, which agrees with the common observation
that training is easier if the widths of the model are balanced [12]. See Figure 4 for an experiment.
In previous works, the progressive sharpening happens when the model is trained with GD [6]; our
theory suggests an alternative mechanism for it.
Figure 4: Dynamics of the stability condi-
tionSduring the training of a rank-1 ma-
trix factorization problem. The solid lines
show the training of SGD with Kaiming init.
When the learning rate ( η=0.008) is too
large, SGD diverges (orange line). However,
when one starts training at a small learning
rate (0.001) and increases ηto0.008 after
5000 iterations, the training remains stable.
This is because SGD training improves the
stability condition during training, which is
in agreement with the theory. In contrast, the
stability condition of GD and that of SGD
with a Xavier init increases only slightly.
Also, note that both Xavier and Kaiming
init. under SGD converges to the same sta-
bility condition because the equilibrium is
unique.A practical technique that the theory explains is using
warmup to stabilize training in the early stage. This tech-
nique was first proposed in Ref. [10] for training CNNs,
where it was observed that the training is divergent if we
start the training at a fixed large learning rate ηmax. How-
ever, this divergent behavior disappears if we perform
a warmup training, where the learning rate is increased
gradually from a minimal value to ηmax. Later, the same
technique is found to be crucially useful for training large
language models [27]. Our theory shows that the gradient
noise can drive Kaiming init. to a stabler status where a
larger learning can be applied.
Flat or Sharp? Prior works have often argued that
SGD prefers flatter solutions to sharper ones (e.g., see
Ref. [42]). The exact solution we found, however, implies
a subtle picture: for some datasets, SGD prefers sharper
solutions, while for others, SGD prefers flatter solutions.
Therefore, there is no causal relationship between SGD
training and the sharpness of the found solution. See Fig-
ure 3 for the dependence of the flatness on the data distri-
bution. A related question is whether SGD noise creates
a similar effect as weight decay training. The answer is
also negative: weight decay always prefers smaller norms
and, thus, norm-balanced solutions, which are not neces-
sarily noise-aligned solutions. Figure 3 shows that SGD
can also lead to unbalanced solutions, unlike weight de-
cay.
5.3 Noise-Aligned Solution of Deep Linear Networks
Here, we apply our result to derive the exact solution of an arbitrarily deep and wide deep linear
network, which has been under extensive study due to its connection in loss landscape to deep
neural networks [4, 5, 17, 23, 47, 38]. Deep linear networks have also been a major model for
understanding the implicit bias of GD [1]. The per-sample loss for a deep linear network can be
written as:
ℓ(θ)=∥WD...W 1x−y∥2, (16)
where Wiis an arbitrary dimensional matrix for all i. The global minimum is realizable: y=V x+ϵ,
for i.i.d. noises ϵ. Because there is a double rotation symmetry between every two neighboring
matrices, the Noether charge can be defined with respect to every such pair of matrices. Let Bibe a
symmetric matrix; we define the charges to be CBi=WT
iBiWi. The noise equilibrium solution is
given by the following theorem.
Theorem 5.4. LetWD...W 1=V. Let V′=√ΣϵV√Σxsuch that V′=LS′Ris its SVD and
d=rank(V′). Then, for all iand all Bi, a noise equilibrium for CBiat the global minimum is√
ΣϵWD=LΣDU⊺
D−1, Wi=UiΣiU⊺
i−1, W1√
Σx=U1Σ1R, (17)
fori=2,⋯, D−1.Uiare arbitrary matrices satisfying UT
iUi=Id×d, and Σiare diagonal matrices
such that
Σ1=ΣD=(d
TrS′)(D−2)/2D√
S′,Σi=(TrS′
d)1/D
Id×d. (18)
8Figure 5: Norms of weights of multilayer deep linear network during training on MNIST without weight decay.
We see that the intermediate layers converge to the same norm during training, whereas the input and output
layers are different because they are determined by the input and output noise. This effect is robust against
different initializations. This agrees with our analysis for deep linear nets ( Theorem 5.4). Left: initializing all
layers with the same norm. Right : initializing all layers at randomly different norms.
This solution has quite a few striking features. Surprisingly, the norms of all intermediate layers are
balanced:
Tr[Σ2
1]=Tr[Σ2
i]=(TrS′)2/Dd1−2/D. (19)
All intermediate layers are thus rescaled orthogonal matrices aligned with the neighboring matrices
and the only two matrices that process information are the first and the last layer. See Figure 5 for an
illustration of this effect. This explains an experimental result first observed in Ref. [30], where the
authors showed that the neural networks find similar solutions when the model is initialized with the
standard init., where there is no alignment at the start, and with the aligned init. Thus, the balance
and alignment between different layers in the neural networks can be attributed to the rescaling
symmetry between each pair of matrices.
5.4 Approximate Symmetry and Bias of SGD
Lastly, let us consider what happens if the loss function only has an approximate symmetry. As a
minimal model, let us consider the following loss function: ℓ=ℓ1(θ, x)+ζℓ2(θ). Here, ℓ1has the
A-symmetry, whereas ℓ2(θ)has no symmetry nor randomness and so ℓ2does not affect Σat all. ζ
determines the relative strength between the two terms. In totality, ℓno longer has the A-symmetry.
As before, let CA=θ⊺Aθ. Then, ˙CA(θ)=−ζ(∇ℓ2)⊺Aθ∗+σ2Tr[Σ(θ)A], whose fixed point is
ζ(∇ℓ2)⊺Aθ∗=σ2Tr[Σ(θ)A]. (20)
This equilibrium condition thus depends strongly on how large ζis. When ζis small, we see that
SGD still favors the fixed point given by Theorem 4.3, but with a first-order correction in ζ.
Conversely, if ζis large and σ2is small, we can expand around a local minimum of the loss function
θ∗, and so the fixed point becomes
ζ(θ−θ∗)⊺H(θ∗)Aθ∗=σ2Tr[Σ(θ∗)A]+O(σ2∥θ−θ∗∥+∥θ−θ∗∥2), (21)
where His the Hessian of ℓ2. Certainly, this implies that SGD will stay around a point that deviates
from the local minimum by an O(σ2)amount. This stationary point potentially has many solutions.
For example, one class of solution is when θ−θ∗is an eigenvector of Hwith eigenvalue h∗>0and
eigenvector n, we can denote s=(θ−θ∗)⊺nand obtain a direct solution of s:
s=σ2Tr[Σ(θ∗)A]
ζh∗n⊺Aθ∗. (22)
This deviation disappears in the limit σ2→0. Therefore, this implicit regularization effect is only
a consequence of SGD training and is not present under GD. With this condition, one can obtain a
clear expression of the deviation of the quantity Cfrom its local minimum value C∗∶=(θ∗)⊺Aθ∗.
We have that
C(θ)=C∗+2(θ−θ∗)⊺Aθ∗+O(∥θ−θ∗∥2)=C∗+2σ2
ζh∗Tr[ΣA]. (23)
Thus, our results in the previous section still apply. The quantity Cwill be systematically larger than
the local minimum values of Cif the approximate symmetry matrix Ais PD. It is systematically
smaller if Ais ND. When Acontains both positive and negative eigenvalues, the deviation of C
9Figure 6: The latent representations of a two-layer tanh net trained under SGD ( left) are similar across different
layers, in agreement with the theory. However, the learned representations are dissimilar under GD ( right ).
Here, we plot the matrices W¯ΣxW(first and third plots) and U¯ΣϵU(second and fourth plots). Note that the
quantity W¯ΣxWis equal to the covariance of the preactivation representation of the first layer. This means that
SGD and GD learn qualitatively different features after training. Also, see Appendix A.4 for other activations.
This mechanism also complements the recent result in Ref. [46], which proposes a physics-inspired theory
showing that gradient noise is a key factor in determining the latent representation of neural networks.
depends on the local gradient fluctuation balancing condition. When the smallest eigenvalue of His
close to zero (which is true for common neural networks), the dominant factor that biases Coccurs
in this space. Therefore, it is not bad to approximate the deviation as C(θ)≈C∗+2σ2Tr[ΣA]/hmin,
where hminis the smallest eigenvalue of the Hessian at the local minimum. In reality, ζis neither too
large nor too small, and one expects that the solution favored by SGD is an effective interpolation
between the true local minimum and the fixed point favored by the symmetries.
A set of experiments is shown in Figure 6, where we compare the latent representation of a two-
layer tanh net with the prediction of 5.2. This is a natural example because fully connected networks
are believed to be approximated by deep linear networks because they have the same connectivity
patterns. We thus compare the prediction of Theorem 5.2 with the experimental results of nonlinear
networks. Here, the task is a simple autoencoding task, where x∈R40andy=x+ϵ.xis sampled
from an isotropic Gaussian, and ϵis an independent non-isotropic (but diagonal) Gaussian noise
such that Var[ϵ1]=5andVar[ϵi]=1fori≠1. We train with SGD or GD for 104iterations.
The experimental results show that if trained with SGD, the learned representation agrees with the
prediction of Theorem 5.2 well, whereas under GD, the model learned a completely different rep-
resentation. This suggests that our result may be greatly useful for understanding the structures of
latent representations of trained neural networks because the quantity W¯ΣxWhas a clean interpre-
tation as the normalized covariance matrix of pre-activation hidden representation. Also, this result
is not a special feature of tanh networks. Appendix A.4 also shows that the same phenomenon can
be observed for swish [28], ReLU, and leaky-ReLU nets.
6 Conclusion
In this work, we have studied how continuous symmetries affect the learning dynamics and fixed
points of SGD. The result implies that SGD converges to initialization-independent solutions at
the end of training, in sharp contrast to GD, which converges to strongly initialization-dependent
solutions. We constructed the theoretical framework of exponential symmetries to study the special
tendency of SGD to stay close to a special fixed point along the constant directions of the loss
landscape. We proved that every exponential symmetry leads to a mapping of every parameter to a
unique and essentially attractive fixed point. This point also has a clean interpretation: it is the point
where the gradient noises of SGD in different subspaces balance andalign . Because of this property,
we termed these fixed points the “noise equilibria.” The advantage of our result is that it only relies
on the existence of symmetries and is independent of the particular definitions of model architecture
or data distribution. A limitation of our work is that we only focus on the problems that exponential
symmetries can describe. It would be important to extend the result to other types of symmetries in
the future. Another interesting future direction is to study these noise equilibria of more advanced
models, which may deepen both our understanding of deep learning and neuroscience.
Acknowledgement
Lei Wu is supported by the National Key R&D Program of China (No. 2022YFA1008200) and Na-
tional Natural Science Foundation of China (No. 2288101). Hongchao Li is supported by Forefront
Physics and Mathematics Program to Drive Transformation (FoPM), a World-leading Innovative
Graduate Study (WINGS) Program, the University of Tokyo. Mingze Wang is supported in part by
10the National Key Basic Research Program of China (No. 2015CB856000). We thank anonymous
reviewers for their valuable comments.
References
[1] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. Advances in Neural Information Processing Systems , 32, 2019.
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
[3] Feng Chen, Daniel Kunin, Atsushi Yamamura, and Surya Ganguli. Stochastic collapse:
How gradient noise attracts sgd dynamics towards simpler subnetworks. arXiv preprint
arXiv:2306.04251 , 2023.
[4] Anna Choromanska, Mikael Henaff, Michael Mathieu, G ´erard Ben Arous, and Yann LeCun.
The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics , pages 192–
204, 2015.
[5] Anna Choromanska, Yann LeCun, and G ´erard Ben Arous. Open problem: The landscape of
the loss surfaces of multilayer networks. In Conference on Learning Theory , pages 1756–1760.
PMLR, 2015.
[6] Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gra-
dient descent on neural networks typically occurs at the edge of stability. arXiv preprint
arXiv:2103.00065 , 2021.
[7] L. Dinh, R. Pascanu, S. Bengio, and Y . Bengio. Sharp Minima Can Generalize For Deep Nets.
ArXiv e-prints , March 2017.
[8] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homoge-
neous models: Layers are automatically balanced. Advances in neural information processing
systems , 31, 2018.
[9] Xavier Fontaine, Valentin De Bortoli, and Alain Durmus. Convergence rates and approxi-
mation results for sgd and its continuous-time counterpart. In Mikhail Belkin and Samory
Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learning Theory , volume 134
ofProceedings of Machine Learning Research , pages 1965–2058. PMLR, 15–19 Aug 2021.
[10] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training
imagenet in 1 hour. arXiv preprint arXiv:1706.02677 , 2017.
[11] Brian C Hall and Brian C Hall. Lie groups, Lie algebras, and representations . Springer, 2013.
[12] Boris Hanin and David Rolnick. How to start training: The effect of initialization and archi-
tecture. Advances in Neural Information Processing Systems , 31, 2018.
[13] Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of
nonconvex stochastic gradient descent. arXiv preprint arXiv:1705.07562 , 2017.
[14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 , 2015.
[15] Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor,
Kyunghyun Cho, and Krzysztof Geras. The break-even point on optimization trajectories of
deep neural networks. In International Conference on Learning Representations , 2019.
[16] Michael I Jordan. Dynamical, symplectic and stochastic perspectives on gradient-based op-
timization. In Proceedings of the International Congress of Mathematicians: Rio de Janeiro
2018 , pages 523–549. World Scientific, 2018.
[17] Kenji Kawaguchi. Deep learning without poor local minima. Advances in Neural Information
Processing Systems , 29:586–594, 2016.
11[18] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori
Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dy-
namics. arXiv preprint arXiv:2012.04728 , 2020.
[19] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and dynamics of
stochastic gradient algorithms i: Mathematical foundations. Journal of Machine Learning
Research , 20(40):1–47, 2019.
[20] Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with tra-
ditional optimization analyses: The intrinsic learning rate. Advances in Neural Information
Processing Systems , 33:14544–14555, 2020.
[21] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochas-
tic differential equations (sdes), 2021.
[22] Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate
stochastic gradient descent, 2021.
[23] Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint
arXiv:1702.08580 , 2017.
[24] Sibylle Marcotte, R ´emi Gribonval, and Gabriel Peyr ´e. Abide by the law and follow the flow:
Conservation laws for gradient flows. 2023.
[25] Emmy Noether. Invariante variationsprobleme. K¨oniglich Gesellschaft der Wissenschaften
G¨ottingen Nachrichten Mathematik-physik Klasse , 2:235–267, 1918.
[26] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for di-
agonal linear networks: a provable benefit of stochasticity. Advances in Neural Information
Processing Systems , 34:29218–29230, 2021.
[27] Martin Popel and Ond ˇrej Bojar. Training tips for the transformer model. arXiv preprint
arXiv:1804.00247 , 2018.
[28] Prajit Ramachandran, Barret Zoph, and Quoc V . Le. Searching for activation functions, 2017.
[29] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to ac-
celerate training of deep neural networks. Advances in neural information processing systems ,
29, 2016.
[30] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120 , 2013.
[31] N. Shirish Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On Large-
Batch Training for Deep Learning: Generalization Gap and Sharp Minima. ArXiv e-prints ,
September 2016.
[32] Justin Sirignano and Konstantinos Spiliopoulos. Stochastic gradient descent in continuous
time: A central limit theorem. Stochastic Systems , 10(2):124–151, 2020.
[33] Nathan Srebro, Jason Rennie, and Tommi Jaakkola. Maximum-margin matrix factorization.
Advances in neural information processing systems , 17, 2004.
[34] Hidenori Tanaka and Daniel Kunin. Noether’s learning dynamics: Role of symmetry breaking
in neural networks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wort-
man Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages
25646–25660. Curran Associates, Inc., 2021.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[36] Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun. Spherical motion dynamics: Learn-
ing dynamics of normalized neural network using sgd and weight decay. Advances in Neural
Information Processing Systems , 34:6380–6391, 2021.
12[37] Ming Chen Wang and George Eugene Uhlenbeck. On the theory of the brownian motion ii.
Reviews of modern physics , 17(2-3):323, 1945.
[38] Zihao Wang and Liu Ziyin. Posterior collapse of a linear latent variable model. Advances in
Neural Information Processing Systems , 35:37537–37548, 2022.
[39] Lei Wu, Chao Ma, and Weinan E. How sgd selects the global minima in over-parameterized
learning: A dynamical stability perspective. Advances in Neural Information Processing Sys-
tems, 31, 2018.
[40] Yizhou Xu and Liu Ziyin. When does feature learning happen? perspective from an analyti-
cally solvable model. arXiv preprint arXiv:2401.07085 , 2024.
[41] Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. Deep matrix
factorization models for recommender systems. In IJCAI , volume 17, pages 3203–3209. Mel-
bourne, Australia, 2017.
[42] Ning Yang, Chao Tang, and Yuhai Tu. Stochastic gradient descent introduces an effec-
tive landscape-dependent regularization favoring flat solutions. Physical Review Letters ,
130(23):237101, 2023.
[43] Bo Zhao, Nima Dehmamy, Robin Walters, and Rose Yu. Symmetry teleportation for accel-
erated optimization. Advances in Neural Information Processing Systems , 35:16679–16690,
2022.
[44] Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in
stochastic gradient descent: Its behavior of escaping from sharp minima and regularization
effects. arXiv preprint arXiv:1803.00195 , 2018.
[45] Liu Ziyin. Symmetry induces structure and constraint of learning. In Forty-first International
Conference on Machine Learning .
[46] Liu Ziyin, Isaac Chuang, Tomer Galanti, and Tomaso Poggio. Formation of representations in
neural networks. arXiv preprint arXiv:2410.03006 , 2024.
[47] Liu Ziyin, Botao Li, and Xiangming Meng. Exact solutions of a deep linear network. In
Advances in Neural Information Processing Systems , 2022.
[48] Liu Ziyin, Hongchao Li, and Masahito Ueda. Law of balance and stationary distribution of
stochastic gradient descent. arXiv preprint arXiv:2308.06671 , 2023.
[49] Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. Strength of minibatch noise in
SGD. In International Conference on Learning Representations , 2022.
[50] Liu Ziyin, Ekdeep Singh Lubana, Masahito Ueda, and Hidenori Tanaka. What shapes the loss
landscape of self supervised learning? In The Eleventh International Conference on Learning
Representations , 2023.
13Figure 7: When there is scaling symmetry, the norm of the parameters increases monotonically
under SGD but remains unchanged under GD. Left: evolution of the total model norm for two-layer
nonlinear networks where there is a rescaling symmetry. Mid: evolution of the second layer. Right :
evolution of the first layer. This shows that the evolution of each layer can be vastly different, but the
total norm of the parameters with the scaling symmetry is always monotonically increasing. Also,
note that for net B, each layer also has the rescaling symmetry, and so the norm of each layer for
net-Bis also increasing. In contrast, net- Adoes not have layer-wise symmetry, and the individual
norms can be either increasing or decreasing.
A Additional Experiments
A.1 Scale Invariance
The scale invariance appears when common normalization techniques such as batch normalization
[14] and layer normalization [2] are used. Let ℓ(θ, x)denote a per-sample loss such that for any
ρ∈R+:ℓ(θ, x)=ℓ(ρθ, x), where θ∈Rd. For this symmetry, A=I. Thus, by Eq. (7), we have
during SGD training that
d
dt∥θt∥2=−γ∥θt∥2+σ2Tr[Σ(θt)]. (24)
Thus, without weight decay, the parameter norm increases monotonically and even diverges, partic-
ularly for under-parameterized models where the gradient noise is typically non-degenerate.
Here, we numerically compare two networks trained on GD and SGD: Net-A: f(x)=
∑juj
∥w∥tanh(w⊺
jx/∥u∥F); and Net-B: f(x)=∑juij
∥u∥tanh(w⊺
jx/∥w∥F). Here, wanduare ma-
trices and wjdenotes the j-th row of wandujdenotes the j-th column of u. The two networks
are different functions of uandw. However, both networks have the global scale invariance: if we
scale both UandWby an arbitrary positive scalar ρ, the network output and loss function remain
unchanged for any sample x. We train these two networks on simple linear Gaussian data with GD
or SGD. Figure 7 shows the result. Clearly, for SGD, both networks have a monotonically increas-
ing norm, whereas the norm remains unchanged when the training proceeds with GD. What’s more,
Net-B has two additional layer-wise scale invariances where one can scale only u(or only w) byρ
while keeping the loss function unchanged. This means that both layers will have a monotonically
increasing norm, which is not the case for Net-A.
Recent works have studied the dynamics of SGD under the scale-invariant models when weight
decay is present [36, 20]. Our result shows that the model parameters will diverge without weight
decay, leading to potential numerical problems. Combining the two results, the importance of having
weight decay becomes clear: it prevents the divergence of models.
A.2 Experiment Detail for Alignment Dynamics of Matrix Factorization
Here, we give the details for the experiment in Figure 2. We train a two-layer linear net with
d0=d2=30andd=40. The input data is x∼N(0,1), andy=x+ϵ, where ϵis i.i.d. Gaussian with
unit variance. At the end of SGD training, every element of the matrix U⊺ΓUUis close to that of
WΓWW⊺, and they are therefore very well aligned. Such a phenomenon does not happen for GD.
A.3 Convergence of Matrix Factorization to the Noise Equilibrium
See Figure 8.
14Figure 8: The convergence of matrix factorization to the noise equilibria is robust against different hyperpa-
rameter settings. The task is an autoencoding task where y=x∈R100. The distribution of xis controlled
by a parameter ϕx:x1∶50∼N(0, ϕx),x51∶100∼N(0,2−ϕx). This directly controls the overall covariance
ofx. The output noise covariance is set to be identity. Unless it is the independent variable, η,Sanddare
set to be 0.1,100and2000 , respectively. Left: using different learning rates. Mid: different data dimension:
dx=dy=d.Right : different batch size S.
A.4 Alignment in Nonlinear Networks
Here, we complement the experiment in the main text with other types of activations. The experi-
mental setting is exactly the same except that we switch the activation to swish, ReLU, and Leaky-
ReLU. This shows that the prediction of Proposition 13 may have a surprisingly wide applicability.
See Figure 9.
15Figure 9: Activation patterns of nonlinear networks trained with SGD ( Upper to lower : ReLU,
leaky-ReLU, swish). Left:WΓWW⊺.Right :U⊺ΓUU. The similarity between the two matrices is
striking.
16B Proofs
B.1 Ito’s Lemma and Derivation of Eq. (5)
Let a vector Xtfollow the following stochastic process:
dXt=µtdt+GtdWt (25)
for a matrix Gt. Then, the dynamics of any function of Xtcan be written as (Ito’s Lemma)
df(Xt)=(∇⊺
Xfµt+1
2Tr[G⊺
t∇2f(Xt)Gt])dt+∇f(Xt)⊺GtdWt. (26)
Applying this result to quantity C(θ)under the SGD dynamics, we obtain that
dC=(∇⊺C∇L+σ2
2Tr[Σ(θ)∇2C])dt+∇⊺C√
σ2Σ(θ)dWt, (27)
where we have used µt=∇L,Gt=√
σ2Σ(θ). By Eq. (3), we have that
∇⊺C∇L=E[∇⊺C∇ℓ]=0, (28)
and
∇⊺CΣ=E[∇⊺C∇ℓ∇⊺ℓ]−E[∇⊺C∇ℓ]E[∇⊺ℓ]=0. (29)
Because Σ(θ)and√
Σ(θ)share eigenvectors, we have that
∇⊺C√
σ2Σ(θ)=0. (30)
Therefore, we have derived:
dC=σ2
2Tr[Σ(θ)∇2C]dt. (31)
B.2 Proof of Theorem 4.3
We first prove a lemma that links the gradient covariance at θto the gradient covariance at θλ.
Lemma B.1.
Tr[Σ(θλ)A]=Tr[e−2λAΣ(θ)A]. (32)
Proof. By the definition of the exponential symmetry, we have that for an arbitrary λ,
ℓ(θ)=ℓ(eλAθ). (33)
Taking the derivative of both sides, we obtain that
∇θℓ(θ)=eλA∇θλℓ(θλ), (34)
The standard result of Lie groups shows that eλAis full-rank and symmetric, and its inverse is e−λA.
Therefore, we have
e−λA∇θℓ(θ)=∇θλℓ(θλ). (35)
Now, we apply this relation to the trace of interest. By definition,
Σ(θλ)=E[∇θλℓ(θλ)∇⊺
θλℓ(θλ)] (36)
=e−λAΣ(θ)e−λA. (37)
Because eλAis a function of A, it commutes with A. Therefore,
Tr[Σ(θλ)A]=Tr[e−λAΣ(θ)e−λAA] (38)
=Tr[e−2λAΣ(θ)A]. (39)
Now, we are ready to prove the main theorem.
17Proof. First of all, it is easy to see that C(θλ)is a monotonically increasing function of λ. By
definition,
C(θλ)=θTeλAAeλAθ (40)
=θT(Z++Z−)θ, (41)
where we have decomposed the matrix eλAAeλA=Z++Z−into two symmetric matrices such that
Z+only contains nonnegative eigenvalues, and Z−only contains nonpositive eigevalues. Because
eλAcommute with A, they share the eigenvectors. Using elementary Lie algebra shows that the
eigenvalues of Z+area+eλa+and that of Z−area−eλa−, where a+≥0anda0≤0. This implies that
θTZ+θandθTZ−θare monotonically increasing functions of λ.
Now, by Lemma B.1, we have
Tr[Σ(θλ)A]=Tr[e−2λAΣ(θ)A]. (42)
Similarly, the regularization term is
γθ⊺
λAθλ=γTr[θθ⊺Ae2λA]. (43)
Now, by assumption, if G(θλ)≠0, we have either Tr[Σ(θ)A]≠0orTr[θθ⊺A]≠0.
IfTr[Σ(θ)A]=θ⊺Aθ=0, we have already proved item (2) of the theorem. Therefore, let us
consider the case when either (or both) Tr[Σ(θ)A]≠0orθ⊺Aθ≠0
Without loss of generality, we assume γ≥0, and the case of γ<0follows an analogous proof. In
such a case, we can write the trace in terms of the eigenvectors niofA:
−γθ⊺
λAθλ+ηTr[Σ(θλ)A]=η∑
µi>0e−2λ∣µi∣∣µi∣σ2
i+γ∑
µi<0e−2λ∣µi∣∣µi∣˜θ2
i
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
I1(λ)−⎛
⎝η∑
µi<0e2λ∣µi∣∣µi∣σ2
i+γ∑
µi>0e2λ∣µi∣∣µi∣˜θ2
i⎞
⎠
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
I2(λ)
=∶I(λ),
where µiis the i-th eigenvalue of A,˜θi=(n⊺
iθi)2,σ2
i=n⊺
iΣ(θ)ni≥0is the norm of the projection
ofΣin this direction.
By definition, I1is either a zero function or strictly monotonically increasing function with
I1(−∞)=+∞, I1(+∞)=0Likewise, I2is either a zero function or a strictly monotonically increas-
ing function with I2(−∞)=0, I2(+∞)=+∞. By the assumption Tr(Σ(θ)A)≠0orTr(θθ⊺A)≠0,
we have that at least one of I1andI2must be a strictly monotonic function.
• IfI1orI2is zero, we can take λto be either +∞or−∞to satisfy the condition.
• If both I1andI2are nonzero, then I=I1−I2is a strictly monotonically decreasing function
withI(−∞)=+∞andI(+∞)=−∞. Therefore, there must exist only a unique λ∗∈R
such that I(λ∗)=0.
For the proof of (4), we denote the multi-variable function J(θ;λ)∶=G(θλ). Given that Σ(θ)is
differentiable,∂J
∂θexists.
It is easy to see that∂J
∂λis continuous. Moreover, for any θandλ=λ∗(θ),
−∂J
2∂λ=η∑
µi>0e−2λ∣µi∣∣µi∣2σ2
i+γ∑
µi<0e−2λ∣µi∣∣µi∣2˜θ2
i+η∑
µi<0e2λ∣µi∣∣µi∣2σ2
i+γ∑
µi>0e2λ∣µi∣∣µi∣2˜θ2
i≠0.
Consequently, according to the Implicit Function Theorem, the function λ∗(θ)is differentiable.
Additionally,∂λ
∂θ=−∂J
∂θ
∂J
∂λ.
18B.3 Convergence
First of all, notice an important property, which follows from Theorem 4.3: λ∗=0if and only if
C−C∗=0.
Lemma B.2. For all θ(t),
˙C(θ)
λ∗(θ)≥{2σ2Tr[Σ(θ∗)A2
+]ifλ∗>0;
2σ2Tr[Σ(θ∗)A2
−]ifλ∗<0.(44)
Proof. As in the main text, let θ∗denote θλ∗,C=C(θ)andC∗=C(θ∗). Thus,
dC
dt=σ2Tr[Σ(θ)A] (45)
=σ2Tr[Σ(θ∗)e2λ∗AA], (46)
where the second equality follows from Lemma B.1. One can decompose Aas a sum of two sym-
metric matrices
A=QΣ+Q⊺
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
∶=A++QΣ−Q⊺
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
∶=A−, (47)
where Qis an orthogonal matrix, Σ+(Σ−) is diagonal and contains only non-negative (non-positive)
entries. Note that by the definition of λ∗, we have Tr[Σ(θ∗)A]=0and, thus,
Tr[Σ(θ∗)A+]=−Tr[Σ(θ∗)A−]. (48)
Thus,
Tr[Σ(θ)A]=Tr[Σ(θ∗)e2λ∗AA] (49)
=Tr[Σ(θ∗)(e2λ∗A−I)A] (50)
=Tr[Σ(θ∗)(e2λ∗A+−I)A+]+Tr[Σ(θ∗)(e2λ∗A−−I)A−]. (51)
Using the inequality I+A≤eA(namely, that eA−I−Ais PSD), we obtain a lower bound
Tr[Σ(θ)A]≥2λ∗Tr[Σ(θ∗)A2
+]−Tr[Σ(θ∗)(I−e2λ∗A−)A−] (52)
Ifλ∗>0,Tr[Σ(θ∗)(e2λ∗A−−I)A−]<0,
Tr[Σ(θ)A]≥2λ∗Tr[Σ(θ∗)A2
+]. (53)
Likewise, there is an upper bound, which simplifies to the following form if λ∗<0:
Tr[Σ(θ)A]≤2λ∗Tr[Σ(θ∗)A2
−]. (54)
This finishes the proof.
Lemma B.3. For any θ,
−C−C∗
λ∗≤{2(θ∗)⊺A2
+θ∗ifλ∗>0;
2(θ∗)⊺A2
−θ∗ifλ∗<0.(55)
Proof. The proof is conceptually similar to the previous one. By definition, we have
C−C∗=(θ∗)⊺e−2λ∗AAθ∗−(θ∗)⊺Aθ∗(56)
=(θ∗)⊺A(e−2λ∗A−I)θ∗(57)
=(θ∗)⊺A+(e−2λ∗A+−I)θ∗+(θ∗)⊺A−(e−2λ∗A−−I)θ∗. (58)
By the inequality I+A≤eA, we have an upper bound
C−C∗≥−2λ∗(θ∗)⊺A2
+θ∗+(θ∗)⊺A−(e−2λ∗A−−I)θ∗. (59)
Ifλ∗>0,(θ∗)⊺A−(e−2λ∗A−−I)θ∗≥0,
C−C∗≥−2λ∗(θ∗)⊺A2
+θ∗. (60)
Likewise, if λ∗<0, one can prove a lower bound:
C−C∗≤−2λ∗(θ∗)⊺A2
−θ∗. (61)
19Combining the above two lemmas, one can prove the following corollary.
Corollary B.4.
˙C
C−C∗≤⎧⎪⎪⎪⎨⎪⎪⎪⎩−σ2Tr[Σ(θ∗)A2
+]
(θ∗)⊺A2
+θ∗,ifC−C∗<0;
−σ2Tr[Σ(θ∗)A2
−]
(θ∗)⊺A2
−θ∗,ifC−C∗>0.(62)
Now, one can prove that as long as C∗is not moving too fast, Cconverges to C∗in mean square.
Lemma B.5. Let the dynamics of C∗be a drifted Brownian motion: dC∗=µdt+sdW , where
Wis a Brownian motion with variance s2. If there exists c0>0such thatTr[Σ(θ∗)A2
+]
(θ∗)⊺A2
+θ∗≥c0and
Tr[Σ(θ∗)A2
−]
(θ∗)⊺A2
−θ∗>c0,
E(C−C∗)2≤2µ2+s2
2σ4c2
0=O(1). (63)
Proof. By assumption,
˙C
C−C∗≤−σ2c0. (64)
Let us first focus on the case when C−C∗>0. By the definition of C∗and Ito’s lemma,
d(C−C∗)≤−σ2c0(C−C∗)dt−µdt−sdW. (65)
LetZ=eσ2c0t(C−C∗), we obtain that
dZ=eσ2c0td(C−C∗)+σ2c0eσ2c0t(C−C∗)dt (66)
≤−µeσ2c0tdt−seσ2c0tdW. (67)
Its solution is given by
Z≤−µeσ2c0t
σ2c0−s∫eσ2c0tdW. (68)
Alternatively, if C−C∗<0, we let Z=eσ2c0t(C∗−C), and obtain
Z≤µeσ2c0t
σ2c0+s∫eσ2c0tdW. (69)
Thus,
E[Z2]≤µ2e2σ2c0t
σ4c2
0+s2∫e2σ2c0tdt (70)
=µ2e2σ2c0t
σ4c2
0+s2e2σ2c0t
2σ2c0. (71)
where we have used Ito’s isometry in the first line. By construction,
E[(C−C∗)2]≤2µ2+s2
2σ4c2
0. (72)
The proof is complete.
Let→pdenote convergence in probability. One can now prove the following theorem, the conver-
gence of the relative distance to zero in probability.
Theorem B.6. Let the assumptions be the same as Lemma. (B.5) . Then, if s=µ=0,E[(C−
C∗)2]→0. Otherwise,
(C−C∗)2
(C∗)2→p0. (73)
20Proof. By Lemma B.5 and Markov’s inequality:
Pr(∣C(t)−C∗(t)∣>t1/4)→0. (74)
Now, consider the distribution of (C∗)2. Because C∗is a Gaussian variable with mean µtand
variance s2t, we have that
Pr(∣C∗∣>√
t)→1. (75)
Now,
Pr(∣C(t)−C∗(t)∣/∣C∗∣>t−1/4)≥Pr(∣C(t)−C∗(t)∣>t1/4&∣C∗∣<√
t) (76)
≥max(0,Pr(∣C(t)−C∗(t)∣>t1/4)+Pr(∣C∗∣<√
t)−1)(77)
→0, (78)
where we have used the Frechet inequality in the second line. This finishes the proof.
B.4 Proof of Proposition 5.1
Proof. Recall that U=(˜u1,⋯,˜ud2)⊺∈Rd2×d,W=(˜w1,⋯,˜wd0)∈Rd×d0, where ˜ui,˜wi∈Rd.
θ=vec(U, W)=(˜u⊺
1,⋯,˜u⊺
d2,˜w⊺
1,⋯,˜w⊺
d0)⊺∈R(d2+d0)d.
˙C=ηTr(Σ(θ)∇2
θθC),
For∇2
θθC, it holds that
∇2
˜ui,˜ujC={B, i=j;
0,otherwise .,∇2
˜wi,˜wjC={−B, i=j;
0, otherwise .∇2
˜ui,˜wjC=0.
Therefore,
Tr[Σ(θ)∇2
θθC]=n
∑
i=1Tr[∇θℓi∇θℓ⊺
i∇2
θθC]=n
∑
i=1(d2
∑
k=1Tr[∇˜ukℓi∇˜ukℓ⊺
iB]−d0
∑
l=1Tr[∇˜wlℓi∇˜wlℓ⊺
iB])
=d2
∑
k=1Tr[Σ(˜uk)B]−d0
∑
l=1Tr[Σ(˜wl)B].
The proof is complete.
B.5 Proofs of Proposition 5.3
Proof. The loss function is
ℓ=∥UWx−y∥2+γ(∥U∥2
F+∥W∥2
F).
Let us adopt the following notation: U=(˜u1,⋯,˜udy)⊺∈Rdy×d,W=(˜w1,⋯,˜wdx)∈Rd×dx, where
˜ui,˜wi∈Rd.θ=vec(U, W)=(˜u⊺
1,⋯,˜u⊺
dy,˜w⊺
1,⋯,˜w⊺
dx)⊺∈R(dx+dy)d.
Due to
∇˜uiℓ=Wx(˜u⊺
iWx−yi)+2γ˜ui,∀i∈[dy];
∇˜wjℓ=dy
∑
i=1˜uixj(˜u⊺
iWx−yi)+2γ˜wj,∀j∈[dx];
the diagonal blocks of the Hessian ∇2
θθℓhave the following form:
∇2
˜ui,˜uiℓ=Wxx⊺W⊺+2γI,∀i∈[dy];
∇2
˜wj,˜wjℓ=x2
jdy
∑
i=1˜ui˜u⊺
i+2γI,∀j∈[dx].
21The trace of the Hessian is a good metric of the local stability of the GD and SGD algorithm because
the trace upper bounds the largest Hessian eigenvalue. For this loss function, the trace of the Hessian
of the empirical risk is
S(U, W)∶=Tr[∇2
θθℓ−2γI]
=dy
∑
i=1Tr[Wxx⊺W⊺]+dx
∑
j=1Tr[x2
jdy
∑
i=1˜ui˜u⊺
i]
=dyTr[WΣxW⊺]+∥U∥2
FTr[Σx]=dy∥WΣ1/2
x∥2
F+∥U∥2
FTr[Σx],
where Σx=xx⊺.
C Proof of Theorem 5.2
Proof. First, we split UandWlikeU=(u1,⋯, ud)∈Rdy×dandW=(w⊺
1,⋯, w⊺
d)⊺∈Rd×dx. The
quantity under consideration is CB=Tr[UBU⊺]−Tr[W⊺BW]for an arbitrary symmetric matrix
B. What will be relevant to us is the type of Bthat is indexed by two indices kandlsuch that
⎧⎪⎪⎨⎪⎪⎩B(k,l)
ij=B(k,l)
ji=1ifi=kandj=lori=landj=k;
B(k,l)
ij=0 otherwise .(79)
Specifically, for k, l∈[d], we select B(k,l)
i,j=δi,kδj,l+δi,lδj,kinCB. With this choice, for an
arbitrary pair of kandl,
CB(k,l)=u⊺
kul−w⊺
kwl.
and
W⊺B(k,l)W=wkw⊺
l+wlw⊺
k, (80)
UB(k,l)U⊺=uku⊺
l+ulu⊺
k. (81)
Therefore,
Edy
∑
i=1Tr[Σ(˜ui)B(k)]=Edy
∑
i=1(˜u⊺
iWx−yi)2Tr[Wxx⊺W⊺B(k)] (82)
=E[∥r∥2Tr[Wxx⊺W⊺B(k,l)]] (83)
=Tr[E[∥r∥2xx⊺]W⊺B(k)W] (84)
=Tr[Σ′
W(wkw⊺
l+wlw⊺
k)] (85)
=2w⊺
kΣ′
wwl (86)
where we have defined ri=˜u⊺
iWx−yiandΣ′
W=E[∥r∥2xx⊺].
Likewise, we have that
Edx
∑
j=1Tr[Σ(˜wj)B(k)]=Edx
∑
j=1x2
jTr⎡⎢⎢⎢⎢⎣⎛
⎝dy
∑
i=1˜ui(˜u⊺
iWx−yi)⎞
⎠⎛
⎝dy
∑
i=1(˜u⊺
iWx−yi)˜u⊺
i⎞
⎠B(k)⎤⎥⎥⎥⎥⎦
=Tr[E[∥x∥2U⊺rr⊺UB(k,l)]]
=Tr[Σ′
UUB(k,l)U⊺]
=2u⊺
kΣ′
uul.
where we have defined Σ′
u=E[∥x∥2rr⊺]. Therefore, we have found that for arbitrary pair of kand
l
˙CB(k,l)=−2γ(u⊺
kul−w⊺
kwl)+2η(w⊺
kΣ′
wwl−u⊺
kΣ′
uul). (87)
The fixed point of this dynamics is:
w⊺
kΣwwl=u⊺
kΣuul. (88)
22where Σw=ηΣ′
w+γIandΣU=ηΣ′
u+γI. Because this holds for arbitrary kandl, the equation
can be written in a matrix form:
WΣwW⊺=U⊺ΣuU. (89)
LetV=UW . To show that a solution exists for an arbitrary V. LetW′=W√ΣwandU′=√ΣuU,
which implies that
U′W′=√
ΣuV√
Σw∶=V′, (90)
and
W′(W′)⊺=(U′)⊺U′. (91)
Namely, U′and(W′)⊺must have the same right singular vectors and singular values. This gives us
the following solution. Let V′=LSR be the singular value decomposition of V′, where LandR
are orthogonal matrices an Sis a positive diagonal matrix. Then, for an arbitrary orthogonal matrix
F, the following choice of U′andW′satisfies the two conditions:
{U′=L√
SF;
W′=F⊺√
SR.(92)
This finishes the proof.
D Discrete-Time GD and SGD
In fact, our results hold in a similar form for discrete-time GD andSGD. Let us focus on the expo-
nential symmetries with the symmetric matrix A.
The following equation holds with probability 1:
0=∇θℓ(θ, z)⋅Aθ. (93)
For discrete-time SGD, it is notationally simpler and without loss of generality to regard ℓ(θ)as the
minibatch-averaged loss, which is the notation we adopt here. This is because if a symmetry holds
for every per-sample loss, then it must also hold for every empirical average of these per-sample
losses.
The dynamics of SGD gives
∆θt=−η∇θℓ(θt, z). (94)
This means that
∆θt⋅J(θ)=0. (95)
Therefore, we have that
∆Ct=2∆θ⊺
tAθt+∆θ⊺
tA∆θt (96)
=∆θ⊺
tA∆θt. (97)
Therefore,
∆Ct=η2Tr[˜Σd(θ)A], (98)
where ˜Σd(θ)=∇θℓ(θ)∇⊺
θℓ(θ)is by definition PSD. Already, note the similarity between Eq. (98)
and its continuous-time version. The qualitative discussions carry over: if Ais PSD, Ctincreases
monotonically.
Now, while the first-order terms in ηalso vanish in the r.h.s, the problem is that the r.h.s. becomes
stochastic because Σd(θt)is different for every time step. However, one can still analyze the ex-
pected flow and show that the expected flow (over the sampling of minibatches) is zero at a unique
point in a way similar to the continuous-time limit of the problem. Therefore, we define
Gd(θt)=Ez[∆Ct], (99)
Σd(θt)=Ez[˜Σd]. (100)
We can now prove the following theorem. Note that this theorem applies for any batch size, and so
it applies to both SGD and GD.
Theorem D.1. (Discrete-time fixed point theorem of SGD.) Let the per-sample loss satisfy the A
exponential symmetry and θλ∶=exp[λA]θ. Then, for any θand any γ∈R,
23(1)Gd(θλ)is a monotonically decreasing function of λ;
(2) there exists a λ∗∈R∪{±∞}such that Gd(θλ)=0;
(3) in addition, if Tr[Σd(θ)A]≠0orTr[θθ⊺A]≠0,λ∗is unique and Gd(θλ)is strictly
monotonic;
(4) in addition to (3), if Σd(θ)is differentiable, λ∗(θ)is a differentiable function of θ.
Proof. Similarly, let us establish the relationship between ∇ℓ(θ)and∇ℓ(exp(λA)). By the defini-
tion of the exponential symmetry, we have that for an arbitrary λ,
ℓ(θ)=ℓ(eλAθ). (101)
Taking the derivative of both sides, we obtain that
∇θℓ(θ)=eλA∇θλℓ(θλ), (102)
The standard result of Lie groups shows that eλAis full-rank and symmetric, and its inverse is e−λA.
Therefore, we have
e−λA∇θℓ(θ)=∇θλℓ(θλ). (103)
Now, we apply this relation to the trace of interest. By definition,
Σd(θλ)=E[∇θλℓ(θλ)∇⊺
θλℓ(θλ)] (104)
=e−λAΣd(θ)e−λA. (105)
Because eλAis a function of A, it commutes with A. Therefore,
Tr[Σd(θλ)A]=Tr[e−λAΣd(θ)e−λAA] (106)
=Tr[e−2λAΣd(θ)A]. (107)
Similarly, the regularization term is
γθ⊺
λAθλ=γTr[θθ⊺Ae2λA] (108)
Now, if Tr[Σd(θ)A]=θ⊺Aθ=0, we have already proved item (2) of the theorem. Therefore, let us
consider the case when either (or both) Tr[Σd(θ)A]≠0orθ⊺Aθ≠0
Without loss of generality, we assume γ≥0, and the case of γ<0follows an analogous proof. In
such a case, we can write the trace in terms of the eigenvectors niofA:
−γθ⊺
λAθλ+ηTr[Σd(θλ)A]=η∑
µi>0e−2λ∣µi∣∣µi∣σ2
i+γ∑
µi<0e−2λ∣µi∣∣µi∣˜θ2
i
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
I1(λ)−⎛
⎝η∑
µi<0e2λ∣µi∣∣µi∣σ2
i+γ∑
µi>0e2λ∣µi∣∣µi∣˜θ2
i⎞
⎠
⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪
I2(λ)
=∶I(λ),
where µiis the i-th eigenvalue of A,˜θi=(n⊺
iθi)2,σ2
i=n⊺
iΣd(θ)ni≥0is the norm of the projection
ofΣdin this direction.
By definition, I1is either a zero function or strictly monotonically increasing function with
I1(−∞)=+∞, I1(+∞)=0Likewise, I2is either a zero function or a strictly monotonically
increasing function with I2(−∞)=0, I2(+∞)=+∞. By the assumption Tr(Σd(θ)A)≠0or
Tr(θθ⊺A)≠0, we have that at least one of I1andI2must be a strictly monotonic function.
• IfI1orI2is zero, we can take λto be either +∞or−∞to satisfy the condition.
• If both I1andI2are nonzero, then I=I1−I2is a strictly monotonically decreasing function
withI(−∞)=+∞andI(+∞)=−∞. Therefore, there must exist only a unique λ∗∈R
such that I(λ∗)=0.
The proof of (4) follows from the Implicit Function Theorem, as in the continuous-time case.
The final question is this: what does it mean for θto reach a point where Gd(θ)=0? An educated
guess can be made: the fluctuation in Cdoes not vanish, but the flow takes Ctowards this vanishing
flow point – something like a Brownian motion trapped in a local potential well [37]. However, it is
difficult to say more without specific knowledge of the systems.
24E Proof of Theorem 5.4
We first prove the following theorem, which applies to an arbitrary parameter that are not necessarily
local minima of of the loss.
Theorem E.1. Letr=WD⋯W1x−y,ξi+1∶=WD⋯Wi+2andhi∶=Wi−1⋯W1. For all layer i, the
equilibrium is achieved at
W⊺
i+1ξ⊺
i+1Ci
0ξi+1Wi+1=WihiCi
1h⊺
iW⊺
i, (109)
where Ci
0=E[∥hix∥2rr⊺], Ci
1∶=E[∥ξ⊺
i+1r∥2xx⊺]. Or equivalently,
ξ⊺
iCi
0ξi=hi+1Ci
1h⊺
i+1. (110)
Proof. By Proposition 5.1,
d
dtCi
B=σ2(ETr[∂ℓ
∂Wi+1B(∂ℓ
∂Wi+1)⊺
]−ETr[(∂ℓ
∂Wi)⊺
B∂ℓ
∂Wi]). (111)
The derivatives are
∂ℓ
∂Wi+1=ξ⊺
i+1r(Wihix)⊺, (112)
∂ℓ
∂Wi=ξ⊺
i+1W⊺
i+1r(hix)⊺. (113)
Therefore, the two terms on R.H.S of Eq. (111) are given by
ETr[∂ℓ
∂Wi+1B(∂ℓ
∂Wi+1)⊺
]=ETr[ξ⊺
i+1r(Wihix)⊺B(Wihix)r⊺ξi+1],
=E∥ξ⊺
i+1r∥2Tr[hixx⊺h⊺
iW⊺
iBWi] (114)
ETr[(∂ℓ
∂Wi)⊺
B∂ℓ
∂Wi]=Tr[W⊺
i+1ξ⊺
i+1r(hix)⊺B(hix)r⊺ξi+1Wi+1]
=E[∥hix∥2Tr[W⊺
i+1ξ⊺
i+1rr⊺ξi+1Wi+1B]]. (115)
Because the matrix Bis arbitrary, we can let Bi,j=δi,kδj,l+δi,lδj,k. Then, the two terms become
ETr[∂ℓ
∂Wi+1B(∂ℓ
∂Wi+1)⊺
]=2E[∥ξ⊺
i+1r∥2˜w⊺
i,khixx⊺h⊺
i˜wi,l], (116)
ETr[(∂ℓ
∂Wi)⊺
B∂ℓ
∂Wi]=2E[∥hix∥2˜w⊺
i+1,kξ⊺
i+1rr⊺ξi+1˜wi+1,l]. (117)
Here, we define the vectors Wi=(˜w⊺
i,1,⋯,˜w⊺
i,d)⊺andWi+1=(˜wi+1,1,⋯,˜wi+1,d). Because Eq.
(116) and (117) hold for arbitrary k, l, we have
ETr[∂ℓ
∂Wi+1B(∂ℓ
∂Wi+1)⊺
]=2WihiE[∥ξ⊺
i+1r∥2xx⊺]h⊺
iW⊺
i, (118)
ETr[(∂ℓ
∂Wi)⊺
B∂ℓ
∂Wi]=2W⊺
i+1ξ⊺
i+1E[∥hix∥2rr⊺]ξi+1Wi+1. (119)
For Eq. (111) to be 0, we must have
WihiE[∥ξ⊺
i+1r∥2xx⊺]h⊺
iW⊺
i=W⊺
i+1ξ⊺
i+1E[∥hix∥2rr⊺]ξi+1Wi+1, (120)
which is Eq. (109). The proof is complete.
We are now ready to prove Theorem 5.4.
25Proof. It suffices to specialize Theorem E.1 to the global minimum. At the global minimum, we
can define
r=W∗
D⋯W∗
1x−y=ϵ. (121)
Then, Eq. (109) can be written as
W⊺
i+1W⊺
i+2⋯W⊺
DΣϵWD⋯Wi+2
Tr[W⊺
i+2⋯W⊺
DΣϵWD⋯Wi+2]Wi+1=WiWi−1⋯W1ΣxW⊺
1⋯W⊺
i−1
Tr[Wi−1⋯W1ΣxW⊺
1⋯W⊺
i−1]W⊺
i. (122)
To solve Eq. (122), we substitute WDandW1withW′
1=W1√ΣxandW′
D=√ΣϵWD, which
transform Eq. (122) into
W⊺
i+1W⊺
i+2⋯W′⊺
DW′
D⋯Wi+2
Tr[W⊺
i+2⋯W′⊺
DW′
D⋯Wi+2]Wi+1=WiWi−1⋯W′
1W′⊺
1⋯W⊺
i−1
Tr[Wi−1⋯W′
1W′⊺
1⋯W⊺
i−1]W⊺
i. (123)
The global minimum condition can be written as
W′
DWD−1⋯W2W′
1=√
ΣϵV√
Σx∶=V′. (124)
Then, we can decompose the matrices W′
1,⋯, W′
Das
W′
D=LΣDU⊺
D−1, Wi=UiΣiU⊺
i−1(i≠1, D), W′
1=U1Σ1R, (125)
where ΣD,⋯,Σ1∈Rd×d,L∈Rdy×d,Ui∈Rdi×d,R∈Rd×dxwithd∶=rank(V′)and arbitrary di.
The matrices Uisatisfy U⊺
iUi=Id×d. By substituting the decomposition into Eq. (123), we have
Σi+1⋯ΣDΣD⋯Σi+1
Tr[Σi+2⋯ΣDΣD⋯Σi+2]=Σi⋯Σ1Σ1⋯Σi
Tr[Σi−1⋯Σ1Σ1⋯Σi−1]. (126)
Since these diagonal matrices commute with each other, we can see Σi=cId×d. Then we move on
to fix the parameter c. By taking i=1andi=D−1in Eq. (126), we obtain
Σ2
2⋯Σ2
D
Tr[Σ2
3. . .Σ2
D]=c2Σ2
D
Tr[Σ2
D]=Σ2
1
d, (127)
Σ2
D
d=Σ2
1⋯Σ2
D−1
Tr[Σ2
1. . .Σ2
D−1]=c2Σ2
1
Tr[Σ2
1], (128)
where drepresents the dimension of the learning space. By taking trace to both sides of Eqs. (127)
and (128), we can see Tr[Σ2
1]=Tr[Σ2
D]and hence, Σ1=ΣD. The parameter cis given by
c=√
Tr[Σ2
1]
d. (129)
With the SVD decomposition V′=LS′R, we have
Σ2
1cD−2=S′. (130)
Therefore, the solutions for candΣ1are
c=(TrS′
d)1/D
,Σ1=√
S′
c(D−2)/2=(d
TrS′)(D−2)/2D√
S′. (131)
The scaling of the diagonal matrices are shown as
Tr[Σ2
1]=d1−2/D(TrS′)2/D,Tr[Σ2
i]=(TrS′)2/Dd1−2/D=Tr[Σ2
1]. (132)
The proof is complete.
26NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We believe that the abstract and introduction reflect the contributions and
scope of the paper.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have discussed the limitations of our work in the conclusion.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ”Limitations” section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The au-
thors should reflect on how these assumptions might be violated in practice and what
the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
27Justification: We believe that the assumptions are clarified and complete proofs are pro-
vided for the theoretical parts.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We believe that all of the experimental results are reproducable in our work.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
28Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: The code or data of the experiments are simple and easy to reproduce follow-
ing the description in the main text.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
• While we encourage the release of code and data, we understand that this might not
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We have specified the training and test details of the experiments in the cap-
tions or the corresponding explanations in the main text for Figs. 2, 3, 4, 5 and 6.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [No]
Justification: Here the dynamics is deterministic and there is no need to consider the error
bars here.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
29• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: The experiments can be simply conducted on personal computers.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have confirmed that the research is conducted with the NeurIPS Code of
Ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Our work is a fundamental research on the learning dynamics of SGD and
hence it does not have direct positive or negative societal impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
30• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
cific groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [No]
Justification: We believe there is no risks for misuse for the data and models.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer:[NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
31• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/
datasets has curated licenses for some datasets. Their licensing guide can help
determine the license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [No]
Justification: Nothing introduced.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: We believe that neither the crowdsourcing nor the research with human sub-
jects is included in our work.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our work does not contain crowdsourcing or research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
32• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
33