Learning from higher-order correlations, efficiently:
hypothesis tests, random features, and neural networks
Eszter Székely†∗Lorenzo Bardone†Federica Gerace‡
Sebastian Goldt§
International School of Advanced Studies (SISSA)
Trieste, Italy
Abstract
Neural networks excel at discovering statistical patterns in high-dimensional data
sets. In practice, higher-order cumulants, which quantify the non-Gaussian cor-
relations between three or more variables, are particularly important for the per-
formance of neural networks. But how efficient are neural networks at extracting
features from higher-order cumulants? We study this question in the spiked cu-
mulant model, where the statistician needs to recover a privileged direction or
“spike” from the order- p≥4cumulants of d-dimensional inputs. We first discuss
the fundamental statistical and computational limits of recovering the spike by
analysing the number of samples nrequired to strongly distinguish between inputs
from the spiked cumulant model and isotropic Gaussian inputs. Existing literature
established the presence of a wide statistical-to-computational gap in this problem.
We deepen this line of work by finding an exact formula for the likelihood ratio
norm which proves that statistical distinguishability requires n≳dsamples, while
distinguishing the two distributions in polynomial time requires n≳d2samples
for a wide class of algorithms, i.e. those covered by the low-degree conjecture.
Numerical experiments show that neural networks do indeed learn to distinguish
the two distributions with quadratic sample complexity, while “lazy” methods like
random features are not better than random guessing in this regime. Our results
show that neural networks extract information from higher-order correlations in
the spiked cumulant model efficiently, and reveal a large gap in the amount of
data required by neural networks and random features to learn from higher-order
cumulants.
1 Introduction
Discovering statistical patterns in high-dimensional data sets is the key objective of machine learning.
In a classification task, the differences between inputs in different classes arise at different statistical
levels of the inputs: two different classes of images will usually have different means, different
covariances, and different higher-order cumulants (HOCs), which describe the non-Gaussian part of
the correlations between three or more pixels. While differences in the mean and covariance allow
for rudimentary classification, Refinetti et al. [1] recently highlighted the importance of HOCs for the
performance of neural networks: when they removed the HOCs per class of the CIFAR10 training
∗Current address: CCFE, Culham Science Centre, Abingdon, Oxon, OX14 3DB, UK
†These authors contributed equally.
‡Current address: Dipartimento di Matematica, Universita’ di Bologna, Bologna (BO), Italy
§Correspondence to: {eszekely, lbardone, fgerace, sgoldt}@sissa.it
38th Conference on Neural Information Processing Systems (NeurIPS 2024).set, the test accuracy of various deep neural networks dropped by up to 65 percentage points. The
importance of higher-order cumulants (HOCs) for classification in general and the performance of
neural networks in particular raises some fundamental questions: what are the fundamental limits
of learning from HOCs, i.e. how many samples n(“sample complexity”) are required to extract
information from the HOCs of a data set? How many samples are required when using a tractable
algorithm? And how do neural networks and other machine learning methods like random features
compare to those fundamental limits?
In this paper, we study these questions by analysing a series of binary classification tasks. In one class,
inputs x∈Rdare drawn from a normal distribution with zero mean and identity covariance. These
inputs are therefore isotropic: the distribution of the high-dimensional points projected along a unit
vector in any direction in Rdis a standard Gaussian distribution. Furthermore, all the higher-order
cumulants (of order p≥3) of the inputs are identically zero. Inputs in the second class are also
isotropic, except for one special direction u∈Rdin which their distribution is different. This
direction uis often called a “spike”, and it can be encoded in different cumulants: for example, we
could “spike” the covariance by drawing inputs from a Gaussian with mean zero and covariance
1+βuu⊤; the signal-to-noise ratio β >0would then control the variance of λ=⟨u, x⟩. Likewise,
we could spike a higher-order cumulant of the distribution and ask: what is the minimal number of
samples required for a neural network trained with SGD to distinguish between the two input classes?
This simple task serves as a proxy for more generic tasks: a neural network cannot be able to extract
information from a given cumulant if it cannot even recognise that it is different from an isotropic
one.
We can obtain the fundamental limits of detecting spikes at different levels of the cumulant hierarchy
by considering the hypothesis test between a null hypothesis (the isotropic multivariate normal
distribution with zero mean and identity covariance) and an alternative “spiked” distribution. We
can then compare the sample complexity of neural networks to the number of samples necessary to
distinguish the two distributions using unlimited computational power, or efficiently using algorithms
that run in polynomial time. A second natural comparison for neural networks are random features
or kernel methods. Since the discovery of the neural tangent kernel [2], and the practical success
of kernels derived from neural networks [3, 4], there has been intense interest in establishing the
advantage of neural networks with learnt feature maps over classical methods with fixed feature
maps, like kernel machines. The role of higher-order correlations for the relative advantage of these
methods has not been studied yet.
In the following, we first introduce some fundamental notions around hypothesis tests and in particular
the low-degree method [5–8], which will be a key tool in our analysis, using the classic spiked Wishart
model for sample covariance matrices [9, 10]. For spiked cumulants, the existing literature (see
section 1.1 for a complete discussion) establishes the presence of a wide statistical-to-computational
gap in this model. Our main contributions are then as follows:
•We deepen the understanding of the statistical-to-computational gap for learning from
higher-order correlations on the statistical side by showing that at unbounded computational
power , a number of samples linear in the input dimension is required to reach statistical
distinguishability . We prove this by explicitly computing the norm of the likelihood ratio ,
see theorem 2.
•On the algorithmic side, SQ bounds and previous low-degree analyses [11, 12], showed
that the sample complexity of learning from HOCs is instead quadratic for a wide class of
polynomial-time algorithms (section 3.2). Here, we provide a different, more direct proof of
such a bound.
•Using these fundamental limits on learning from HOCs as benchmarks, we show numerically
that neural networks learn the mixed cumulant model efficiently, while random features do
not, revealing a large separation between the two methods (sections 4.1 and 4.2).
•We finally show numerically that the distinguishability in a simple model for images [13]
is precipitated by a cross-over in the behaviour of the higher-order cumulants of the inputs
(section 4.3).
21.1 Further related work
Detecting spikes in high-dimensional data There is an enormous literature on statistical-to-
computational gaps in the detection and estimation of variously structured principal components of
high-dimensional data sets. This problem has been studied for the Wigner and Wishart ensembles
of random matrices [9, 14–24] and for spiked tensors [25–33]. For comprehensive reviews of the
topic, see refs. [34–37]. While the samples in these models are often non-Gaussian, depending on
the prior distribution over the spike u, the spike appears already at the level of the covariance of the
inputs. Here, we instead study a high-dimensional data model akin to the one used by Wang & Lu
[38] to study online independent component analysis (ICA). This data model can be interpreted as
having an additional whitening step to pre-process the inputs, which is a common pre-processing
step in ICA [39], hence inputs have identity covariance even when cumulants of order p≥3are
spiked. Wang & Lu [38] proved the existence of a scaling limit for online ICA, but did not consider
the sample complexity of recovering the spike/distinguishing the distributions, which is the focus of
this paper.
NGCA, Gaussian pancakes and low-degree polynomials Related models have been introduced
under the name of Non-Gaussian Component Analysis (NGCA) [40], and studied from the point
of view of statistical query (SQ) complexity in a sequence of papers [41–46]. In particular [11]
nicknames Gaussian pancakes a class of models that includes the spiked cumulant model presented
here. The SQ bounds found in [11] and refined in the subsequent works (see Diakonikolas et al. [47]
and references therein) could be used, together with SQ-LDLR equivalence [48], to provide estimates
on low-degree polynomials. Finally, [12] proves very general bounds on LDLR norm; our theorem 5
provides an alternative derivation of these bounds, in a setting that is closer to the experiments in
section 4.
Separation between neural networks and random features The discovery of the neural tangent
kernel by Jacot et al. [2] and the flurry of results on linearised neural networks [2, 3, 49–55] has
triggered a new wave of interest in the differences between what neural networks and kernel methods
can learn efficiently. While statistical separation results have been well-known for a long time [56],
recent work focused on understanding the differences between random features and neural networks
trained by gradient descent both with wide hidden layers [4, 57–61] or even with just a few hidden
neurons [62, 63]. At the heart of the data models in all of these theoretical models is a hidden,
low-dimensional structure in the task, either in input space (for mixture classification) or in the form
of single- or many-index target functions. The impact of higher-order input correlations in mixture
classification tasks on the separation between random features and neural networks has not been
directly studied yet.
Reproducibility We provide code for all of our experiments, including routines to generate the
various synthetic data sets we discuss, on GitHub https://github.com/eszter137/data-driven-separation.
2 The data models
Throughout the paper, we consider binary classification tasks where high-dimensional inputs xµ=
(xµ
i)∈Rdhave labels yµ=±1. The total number of training samples is 2n, i.e. we have nsamples
per class. For the class yµ=−1, inputs xµ=zµ, where zµ∼iidN(0, 1d)andNdenotes the
normal distribution. For the class yµ= 1instead, we consider “spiked” input models as follows.
The Gaussian case The simplest spiked model is the spiked Wishart model from random matrix
theory [9, 10], in which
xµ=r
β
dgµu+zµ, gµ∼iidN(0,1), (1)
where u= (ui)is ad-dimensional vector with norm ∥u∥=√
dwhose elements are drawn element-
wise i.i.d. according to some probability distribution Pu. In this model, inputs are Gaussian and
indistinguishable from white noise except in the direction of the “spike” u, where they have variance
1 +β, where β >0is the signal-to-noise ratio . By construction, all higher-order cumulants are zero.
3The spiked cumulant model To study the impact of HOCs, we draw inputs in the “spiked” class
from the data model used by Wang & Lu [38] to study online independent component analysis (ICA).
First, we sample inputs
˜xµ=r
β
dgµu+zµ, gµ∼i.i.d.pg, (2)
as in the Wishart model, but crucially sample the latent variables gµfrom some non-Gaussian
distribution pg(gµ), say, the Rademacher distribution; see assumption 1 for a precise statement. For
any non-Gaussian pg, the resulting inputs ˜xµhave a non-trivial fourth-order cumulant proportional
toκg
4u⊗4, where κg
4≡E(gµ)4−3E(gµ)2is the fourth cumulant of the distribution of gµ. We
fix the mean and variance of pgto be zero and one, respectively, so the covariance of inputs has a
covariance matrix Σ = 1d+β/duu⊤. To avoid trivial detection of the spike from the covariance, the
key ingredient of the spiked cumulant model is that we whiten the inputs in that class, so that the
inputs are finally given by
xµ=S˜xµ, S = 1−β
1 +β+√1 +βuu⊤
d, (3)
with the whitening matrix S(see appendix B.4.3). Hence inputs xµare isotropic Gaussians in all
directions except u, where they are a weighted sum of gµand⟨z, u⟩. The whitening therefore changes
the interpretation of β: rather than being a signal-to-noise ratio, as in the spiked Wishart model, here
βcontrols the quadratic interpolation between the distributions of gµandzin the direction of u(see
eq. (55) in the appendix). This leaves us with a data set where inputs in both classes have an average
covariance matrix that is the identity, which means that PCA or linear neural networks [64–67] cannot
detect any difference between the two classes.
3 How many samples do we need to learn?
Given a data set sampled from the spiked cumulant model, we can now ask: how many samples does
a statistician need to reliably detect whether inputs are Gaussian or not, i.e. whether HOCs are spiked
or not? This is equivalent to the hypothesis test between ni.i.d. samples of the isotropic normal
distribution in Rdas the null hypothesis Qn,d, and ni.i.d. samples of the spiked cumulant model
eq. (3) as the alternative hypothesis Pn,d. In section 3.1 we will first consider the problem from a
statistical point of view, assuming to have unbounded computational power and no restrictions on the
distinguishing algorithms. Then, in section 3.2 we will use the low-degree method to understand how
the picture changes when we restrict to algorithms whose running time is at most polynomial in the
space dimension d.
3.1 Statistical distinguishability: LR analysis
Recall the notion of strong asymptotic distinguishability : two sequences of probability measures are
strongly distinguishable if it is possible to design statistical tests that can classify correctly which
of the two distributions a sample was drawn from with probabilities of type I and II errors that
converge to 0 (see appendix B.2 for the precise definition). Using this definition of distinguishability,
we will ask what is the statistical sample complexity exponent , i.e. the minimum θsuch that in the
high-dimensional limit d→ ∞ , ifn≍dθ, thenPn,dandQn,dare strongly distinguishable (with no
constraints on the complexity of statistical tests).
A useful quantity to consider is the likelihood ratio (LR) of probability measures, which is defined as
L(x) :=dP
dQ(x). (4)
Computing the LR norm ||L||2:=EQ[L2]is an excellent tool to probe for distinguishability: if
(Pn,d)and(Qn,d)are strongly distinguishable, then ∥Ln,d∥ → ∞ (this is the well-known second
moment method for distinguishability , see proposition 6 in the appendix for the precise statement). In
the following we will apply this method, finding a formula for the LR norm and then study its limit
as a function of θ. Here and throughout, we will denote the data matrix by x= (xµ)⊤
1,...,n; in general
matrices of size n×dwill be denoted with underlined letters; see appendix B.1 for a complete
summary of our notation. We will use Hermite polynomials, denoted by (hk)k, see appendix B.3
40.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0
exponents0.00.20.40.60.81.0succes rate
d=5
d=10
d=15
d=18
d=19
d=20
d=22
d=25Figure 1: The performance of an exhaustive-search algorithm corroborates the presence of
a phase transition for θ= 1, as suggested by theorem 2. Success rate of an exponential-time
search algorithm over all the possible spikes in the d-hypercube as a function of the exponent θthat
quantifies as n=dθthe samples used in the log-likelihood test (8), in the g∼Radem (1/2)case.
for details. We assume that the spike uis drawn from a known prior P(u), and that the scalar latent
variables (gµ)µ=1,...,nare drawn i.i.d. from a distribution pgwith the following properties:
Assumption 1 (Assumption on latent variables gµ).We assume that the one-dimensional probability
distribution of the non-Gaussian random variable pg(g)is an even distribution, pg(g=dx) =
pg(−g=−dx), with mean 0 and variance 1, and that it satisfies the following requirement on the
growth of its Hermite coefficients:
E[hm(g)]≤Λmm! (5)
where Λ>0is a positive constant that does not depend on m. Finally, we assume that pghas tails
that cannot go to 0 slower than a standard Gaussian, E[exp 
g2/2
]<+∞.
A detailed discussion of these assumptions can be found in appendix B.4, as well as a proof that they
are satisfied by a wide class of distributions including all the compactly supported distributions with
mean 0 and variance 1 (some concrete examples are pg=Rademacher (1/2)orpg=Unif(−√
3,√
3)).
Under these assumption we can compute the following formula for the LR norm.
Theorem 2. Suppose that uhasi.i.d. Rademacher prior and that the non-Gaussian distribution pg
satisfies assumption 1. Then the norm of the total LR is given by
∥Ln,d∥2=dX
j=0d
j1
2df
β,2j
d−1n
, (6)
where fis defined as the following average over two independent replicas gu, gv∼gofg:
f(β, λ) := E
gu,gv"
1 +βp
(1 +β)2−β2λ2e−(1+β)((1+β)(g2
u+g2
v)−2β(gugv)λ)
2(1+β)2−2β2λ2 +g2
u+g2
v
2#
(7)
We prove theorem 2 in appendix B.5. The theorem has key consequences in two directions:
•on the one hand, eq. (6) implies that the LR norm is bounded for θ <1(see lemma 13
in appendix B.5.3), which confirms that below that threshold strong distinguishability is
impossible ;
•on the other hand, eq. (6) implies that whenever there exists ˜λsuch that f(β,˜λ)>1, we find
that∥Ln,d∥ ≥f(β,˜λ)n
2d.Thus, the LR norm diverges as soon as ngrows as n≍dθwith any
θ >1. In appendix B.5.3 we detail as an example the case in which g∼Rademacher( 1/2)
where the norm ∥Ln,d∥even diverges for θ= 1andd≍γnfor some γ >0.
So, besides the intrinsic value of providing an exact formula for the LR of the spiked cumulant
model, theorem 2 implies the presence of a phase transition at θ= 1 for the strong statistical
distinguishability .
5A complementary approach that substantiates the presence of the statistical phase transition at θ= 1
can be seen in fig. 1, where we perform a maximum log-likelihood test along u·xµforallthe possible
spikes uin the d-dimensional hypercube using the formula for the LR conditioned on the spike,
nX
µ=1logpx(xµ|u)
pz(xµ)
=nX
µ=1logE
gp
1 +βexp
−1 +β
2 
g−s
β
(1 +β)dxµ·u!2
+g2
2
,
(8)
(see eq. (61) in appendix B.5.2 for the derivation of this equation) and output the most likely u. Note
that due to the exponential complexity in dof the algorithm, it is unfeasible to reach large values for
this parameter. However, even at small dvalues, the success rate for retrieving the correct spike has a
very steep increase at around θ= 1, as predicted by our analysis of the LR norm in theorem 2.
3.2 Computational distinguishability: LDLR analysis
We now compare the statistical threshold of section 3.1 with the computational sample complexity ex-
ponent that quantifies the sample complexity of detecting non-Gaussianity with an efficient algorithm
that runs in a time that is polynomial in the input dimension. The algorithmic sample complexity can
be analysed rigorously for a wide class of algorithms using the low-degree method [5–8, 37]. The
low-degree method arose from the study of the sum-of-squares hierarchy [5] and rose to prominence
when Hopkins & Steurer [7] demonstrated that the method can capture the Kesten–Stigum threshold
for community detection in the stochastic block model [68–70]. In the case of hypothesis testing, the
key quantity is the low-degree likelihood ratio (LDLR) [8, 37].
Definition 3 (Low-degree likelihood ratio (LDLR)) .LetD≥0be an integer. The low-degree
likelihood ratio of degree Dis defined as
L≤D:=P≤DL (9)
whereP≤Dprojects Lonto the space of polynomials of degree up to D, parallel to the Hilbert space
structure defined by the scalar product ⟨f, g⟩L2(Ω,Q):=Ex∼Q[f(x)g(x)].
The idea of this method is that among degree- Dpolynomials, L≤Dcaptures optimally the difference
between PandQ, and this difference can be quantified by the normL≤D=L≤D
L2(Ωn,Qn).
Hence in analogy to the second moment method used in section 3.1, we can expect low-degree
polynomials to be able to distinguish PfromQonly whenL≤D(n)
n→
n∞, where D(n)is a
monotone sequence diverging with n. Indeed, the following (informal) conjecture from Hopkins [8]
states that this criterion is valid not only for polynomial tests, but for all polynomial-time algorithms:
Conjecture 4. For two sequences of measures QN,PNindexed by N, suppose that (i) QNis a
product measure; (ii) PNissufficiently symmetric with respect to permutations of its coordinates; and
(iii)PNisrobust with respect to perturbations with small amount of random noise. If ∥L≤D
N∥=O(1)
asN→ ∞ and for D≥(logN)1+ε, for some ε >0, then there is no polynomial-time algorithm
that strongly distinguishes the distributions QandP.
Even though this conjecture is still not proved in general, its empirical confirmation on many
benchmark problems has made it an important tool to probe questions of computational complexity,
see theorem 8 for a simple example.
We will now compute LDLR norm estimates for the spiked cumulant model, so that the application
of conjecture 4 will help to understand the computational sample complexity of this model.
Theorem 5 (LDLR for spiked cumulant model) .Suppose that (ui)i=1,...,d are drawn i.i.d. from the
symmetric Rademacher distribution and that the non-Gaussian distribution pgsatisfies assumption 1.
Let0< ε < 1and assume D(n)≍log1+ε(n). Take n, d→ ∞ , with the scaling n≍dθforθ >0.
The following bounds hold:
L≤D(n)
n,d2
≥ 
1
⌊D(n)/4⌋β2κg
4
(1 +β)22n
d2!⌊D(n)/4⌋
(10)
L≤D(n)
n,d2
≤1 +D(n)X
m=1Λ2β
1 +βm
m4mn
d2m/4
(11)
6Taken together, eq. (10) and eq. (11) imply the presence of a critical regime for θc= 2, and describe
the behaviour ofL≤D
n,dfor all θ̸=θc
lim
n,d→∞L≤D(n)
n,d=1 0 < θ < 2
+∞θ >2(12)
This theorem could be derived with different constants from lemma 26 and proposition 8 in Dudeja &
Hsu [12]. Here we also provide a different, more direct argument. We sketch the proof of theorem 5 in
appendix B.6.1 and give the complete proof in appendix B.6.2. We will discuss next the implications
of the results presented in section 3.1 and section 3.2
3.3 Statistical-to-computational gaps in the spiked cumulant model
Put together, our results for the statistical and computational sample complexities of detecting non-
Gaussianity in theorem 2 and theorem 5 suggest the existence of three different regimes in the spiked
cumulant model as we vary the exponent θ, with a statistical-to-computational gap: for 0≤θ <1,
the problem is statistically impossible in the sense that no algorithm is able to strongly distinguish
PandQwith so few samples, since the LR norm is bounded. For 1< θ < 2, the norm of the
likelihood ratio with g∼Rademacher (1/2)diverges (even at θ= 1 for some values of β), the
problem could be statistically solvable (as validated by the results of exhaustive-search algorithms in
fig. 1), but conjecture 4 suggests no polynomial-time algorithm is able to achieve distinguishability in
this regime; this is the so-called hard phase . Ifθ >2, the problem is solvable in polynomial time
by evaluating a polynomial function (fourth-order in each sample) and thresholding; this is the easy
phase .
The spiked cumulant model leads thus to intrinsically harder classification problems than the spiked
Wishart model, where the critical regime is at θ= 1. The proof of theorem 5 reveals that this increased
difficulty is a direct consequence of the whitening of the inputs in eq. (3). Without whitening, degree-
2 polynomials would also give contributions to the LDLR (75) which would yield linear sample
complexity. The difference in sample complexity of the spiked Wishart and spiked cumulant models
mirrors the gap between the sample complexity of the best-known algorithms for matrix factorisation,
which require linear sample complexity, and tensor PCA for rank-1 spiked tensors of order k[25,
28, 30], where sophisticated spectral algorithms can match the computational lower bound of dk/2
samples.
4 Learning from HOCs with neural networks and random features
The analysis of the (low-degree) likelihood ratio has given us a detailed picture of the statistical
and computational complexity of extracting information from the covariance or the higher-order
cumulants of data. We will now use these fundamental limits to benchmark the sample complexity of
two-layer neural networks (2LNN) trained with stochastic gradient descent on a binary discrimination
task, where inputs in one class are drawn from the normal distribution N(0, 1d), while inputs in the
other class are drawn from the spiked Wishart or the spiked cumulant model. In addition, we will also
benchmark random feature methods (RF) [71–73] as a finite-dimensional approximation of kernel
methods [71–73].
In a nutshell, the idea behind our experiments is to first validate both 2LNN and RF on the simpler
spiked Wishart task and then to apply both methods to the spiked cumulant model, where inputs
are generated in a way that mirrors the spiked Wishart: comparing eq. (1) with eqs. (2) and (3), we
see that the only differences are the whitening, and the latent distribution pg. In our experiments,
we choose the latent variables to be standard Gaussian for spiked Wishart, and Rademacher for the
spiked cumulant – hence the latent variables have matching first and second moments. However, we
will see that the spiked cumulant model exhibits a large gap in the sample complexity required for
neural networks or random features to learn the problem. We relegate details on the experimental
setups such as hyper-parameters to appendix A.
7ABCDABCDearly-stopping test error
max overlap
max overlap1.0
nclass/dnclass/d2nclass/d2nclass/d
nclass/dnclass/d2nclass/d2nclass/dmax overlap
max overlapm mearly-stopping test errorearly-stopping test error
early-stopping test errorReplica 
theoryFigure 2: Learning the spiked Wishart task with neural networks and random features. (A,B)
Test accuracy of random features (RF) and early-stopping test accuracy of two-layer ReLU neural
networks (NN) on the spiked Wishart task, eq. (1), with linear and quadratic sample complexity
(nclass≍d, d2, respectively, where dis the input dimension). Predictions for the performance of
random features obtained using replicas are shown in black. (C,D) Maximum normalised overlaps
of the networks’ first-layer weights with the spike u, eq. (1). Parameters :β= 5. Neural nets and
random features have m= 5dhidden neurons. Full experimental details in appendix A.
4.1 Spiked Wishart model
We trained two-layer ReLU neural networks ϕθ(x) =v⊤max 
0, wTx
withm= 5dhidden
neurons on the spiked Wishart classification task. We show the early-stopping test accuracy of the
networks in the linear and quadratic regimes with nclass≍d, d2samples per class in fig. 2A and
B, resp. Neural networks are able to solve this task, in the sense that their classification error per
sample is below 0.5, implying strong distinguishability of the whole data matrix. Indeed, some of the
hidden neurons converge to the spike uof the covariance matrix, as can be seen from the maximum
value of the normalised overlaps max kw⊤
ku/p
∥wk∥∥u∥in fig. 2C and D, where wkis the weight
vector of the kth hidden neuron. In the linear regime (C), there is an increasingly clear transition
from a random overlap for small data sets to a large overlap at large data sets as we increase the input
dimension d; in the quadratic regime (D), the neural network recovers the spike almost perfectly.
Therelatively large overlap between hidden neurons and spike at small sample complexities
(fig. 2C and D) is due to the fact that we plot the maximum overlaps over a relative large number
of hidden neurons m= 5d; hence even at initialisation, a few neurons will have large overlaps. We
verified that ensuring an initial overlap of only 1/√
dby explicit orthogonalisation did not change
our results on distinguishability, see fig. 5. A possible explanation is that the dynamics of the wide
network is dominated by the majority of neurons, which do not have a macroscopic overlap.
Meanwhile, we found that the performance of random features tends to random guessing at linear
sample complexity, where we let the input dimension d→ ∞ withm/d andnclass/dfixed, while
at quadratic sample complexity, random features learn the task, although they perform worse than
neural networks. The failure of RF in the linear regime makes sense in light of recent results that
suggest that random features in this scaling regime are limited to learning a linear approximation of
the target function [74–77], while the LDLR analysis appendix B.2.3 shows that the target function,
i.e. the low-degree likelihood ratio, is a quadratic polynomial. However, these results are, to the best
of our knowledge, restricted to the case of Gaussian isotropic inputs.
To ensure that the performance of random features does indeed tend to random guessing, we performed
areplica analysis following Loureiro et al. [78] for mixture classification tasks together with the
Gaussian equivalence theorem [79–83] (black lines in fig. 2A, details in appendix C). Replica theory
perfectly predicts the performance of RF we obtain in numerical experiments (red-ish dots) for various
values of dat linear sample complexity. We thus find a clear separation in the sample complexity
required by random features ( nclass≳d2) and neural networks ( nclass≳d) to learn the spiked
Wishart task. The replica analysis can be extended to the polynomial regime by a simple rescaling of
the free energy [84] on several data models, like the vanilla teacher–student setup [85], the Hidden
manifold model [79], and the vanilla Gaussian mixture classification (see fig. 8). However, we found
that for the spiked Wishart model, the Gaussian equivalence theorem which we need to deal with the
random feature distribution fails at quadratic sample complexity. This might be due to the fact that in
this case, the spike induces a dominant direction in the covariance of the random features, and this
8ABCDABCDearly-stopping test error
max overlap
max overlap1.0
nclass/dnclass/d2nclass/d2nclass/d
nclass/dnclass/d2nclass/d2nclass/dmax overlap
max overlapm mearly-stopping test errorearly-stopping test error
early-stopping test errorReplica 
theoryFigure 3: Learning the spiked cumulant task with neural networks and random features. (A,
B)Test accuracy of random features (RF) and early-stopping test accuracy of two-layer ReLU neural
networks (NN) on the spiked cumulant task eq. (3) with linear and quadratic sample complexity
(nclass≍d, d2, respectively, where dis the input dimension). (C, D) Maximum normalised overlaps
of the networks’ first-layer weights with the spike u,(3).Parameters :β= 10 . Neural nets and
random features have m= 5dhidden neurons, same optimisation as in fig. 2. Full experimental
details in appendix A.
direction is also key to solving the task, which can lead to a breakdown of Gaussian equivalence [83].
A similar breakdown of the Gaussian equivalence theorem at quadratic sample complexity has been
demonstrated recently in teacher–student setups by Cui et al. [86] and Camilli et al. [87].
4.2 Spiked cumulant
Having thus validated the performance of 2LNN and RF on the simpler spiked Wishart model, we turn
to the spiked cumulant model and to the question: can neural networks learn higher-order correlations,
efficiently? The LDLR analysis predicts that a polynomial-time algorithm requires at least a quadratic
number of samples to detect non-Gaussianity, and hence to solve the classification task, and we found
indeed that neural networks require at least quadratic sample complexity to solve the task, fig. 3A
and B. The high values of the maximum overlap between hidden neurons and cumulant spike in
the linear regime (compared to d−1/2) are again a consequence of choosing the maximum overlap
among m= 5dhidden neurons. Random features cannot solve this task even at quadratic sample
complexity, since they are limited to a quadratic approximation of the target function [75–77, 88], but
we know from the LDLR analysis that the target function is a fourth-order polynomial. We thus find
an even larger separation in the minimal number of samples required for random features and neural
networks to solve tasks that depend on directions which are encoded exclusively in the higher-order
cumulants of the inputs.
4.3 Phase transitions and neural network performance in a simple model for images
We finally show another example of a separation between the performance of random features and
neural networks in the feature-learning regime on a toy model for images that was introduced recently
by Ingrosso & Goldt [13], the non-linear Gaussian process (NLGP). The idea is to generate inputs
that are (i) translation-invariant and that (ii) have sharp edges, both of which are hallmarks of natural
images [89]. We first sample a vector z∈Rdfrom a normal distribution with zero mean and
covariance Cij=Ezizj= exp( −|i−j|/ξ)to ensure translation-invariance of the inputs, with
length scale ξ >0. We then introduce edges, i.e. sharp changes in luminosity, by passing zthrough a
saturating non-linearity like the error function, xi= erf( gzi)/Z(g), where Z(g)is a normalisation
factor that ensures that the pixel-wise variance Ex2
i= 1 for all values of the gain g > 0. The
classification task is to discriminate these “images” from Gaussian inputs with the same mean and
covariance, as illustrated in two dimensions in fig. 4A. This task is different from the spiked cumulant
model in that the cumulant of the NLGP is not low-rank, so there are many directions that carry a
signal about the non-Gaussianity of the inputs.
We trained wide two-layer neural networks on this task and interpolated between the feature-learning
and the “lazy” regimes using the α-renormalisation trick of Chizat et al. [90]. As we increase α,
the networks go from feature-learners ( α= 1) to an effective random feature model and require an
increasing amount of data to solve the task, fig. 4B. There appears to be a sharp transition from random
guessing to non-trivial performance as we increase the number of training samples for all values of α.
9same mean, same covariance
vs.Non-Gaussian
Gaussian
early-stopping test errorlazierABCFigure 4: A phase transition in the fourth-order cumulant precedes learning from the fourth
cumulant. (A) We train neural networks to discriminate inputs sampled from a simple non-Gaussian
model for images introduced by Ingrosso & Goldt [13] (top) from Gaussians with the same mean
and covariance (bottom). (B)Test error of two-layer neural networks interpolating between the
fully-trained ( α= 1) and lazy regimes (large α) – see section 4.3. (C)The localisation of the leading
CP-factor of the non-Gaussian inputs (dashed purple line) and the first-layer weights of the trained
networks, as measured by the inverse participation ratio (IPR), eq. (13). Large IPR denotes a more
localised vector w.Parameters :g= 3,ξ= 1, d= 20, m= 100 . Full details in appendix A.
This transition is preceded by a transition in the behaviour of the higher-order cumulant that was
reported by Ingrosso & Goldt [13]. They showed numerically that the CP-factors of the empirical
fourth-order cumulant T, defined as the vectors ˆu∈Rdthat give the best rank- rapproximation
ˆT=Pr
k=1γkˆu⊗4
kofT[91], localise in space if the data set from which the empirical cumulant
is calculated is large enough. Quantifying the localisation of a weight vector wusing the inverse
participation ratio
IPR(w) =Pd
i=1w4
iPd
i=1w2
i2, (13)
we confirm that the leading CP-factors of the fourth-order cumulant localise (purple dashed line in
fig. 4C). The localisation of the CP-factors occurs with slightly less samples than the best-performing
neural network requires to learn ( α= 1). The weights of the neural networks also localise at a sample
complexity that is slightly below the sample complexity for solving the task. The laziest network
(α= 100) , i.e. the one where the first-layer weights move least and which is hence closest to random
features, does not learn the task even with a training set containing n= 103dsamples when d= 20 ,
indicating again a large advantage of feature-learners over methods with fixed feature maps, such as
random features.
5 Concluding perspectives
Neural networks crucially rely on the higher-order correlations of their inputs to extract statistical
patterns that help them solve their tasks. Here, we have studied the difficulty of learning from
higher-order correlations in the spiked cumulant model, where the first non-trivial information in the
data set is carried by the input cumulants of order 4 and higher. Our LR analysis of the corresponding
hypothesis test confirmed that data sampled from the spiked cumulant model could be statistically
distinguishable (in the sense that it passes the second moment method for distinguishability ) from
isotropic Gaussian inputs at linear sample complexity, while the number of samples required to
strongly distinguish the two distributions in polynomial time scales as n≳d2for the class of
algorithms covered by the low-degree conjecture [5–8], suggesting the existence of a large statistical-
to-computational gap in this problem. Our experiments with neural networks show that they learn
from HOCs efficiently in the sense that they match the sample complexities predicted by the analysis
of the hypothesis test, which is in stark contrast to random features, which require a lot more data. In
the future, a key challenge will be extend this framework to null hypotheses that go beyond isotropic
Gaussian distributions. It will be intriguing to analyse the dynamics of neural networks on spiked
cumulant models or the non-linear Gaussian process to understand how neural networks extract
information from the higher-order cumulants of realistic data sets efficiently [92].
10Acknowledgements
We thank Zhou Fan, Yue Lu, Antoine Maillard, Alessandro Pacco, Subhabrata Sen, Gabriele Sicuro,
and Ludovic Stephan for stimulating discussions on various aspects of this work. SG acknowledges
co-funding from Next Generation EU, in the context of the National Recovery and Resilience Plan,
Investment PE1 – Project FAIR “Future Artificial Intelligence Research”, and from the European
Union - NextGenerationEU, in the framework of the PRIN Project SELFMADE (code 2022E3WYTY
– CUP G53D23000780001).
Contributions
ES performed the numerical experiments with neural networks and random features. LB performed
the (low-degree) likelihood analysis. FG performed the replica analysis of random features. SG
designed research and advised ES and LB. All authors contributed to writing the paper.
References
1.Refinetti, M., Ingrosso, A. & Goldt, S. Neural networks trained with SGD learn distributions of
increasing complexity inInternational Conference on Machine Learning (2023), 28843–28863
(cit. on p. 1).
2.Jacot, A., Gabriel, F. & Hongler, C. Neural Tangent Kernel: Convergence and Generalization
in Neural Networks inAdvances in Neural Information Processing Systems (eds Bengio, S.
et al. )31(Curran Associates, Inc., 2018) (cit. on pp. 2, 3).
3.Arora, S. et al. On exact computation with an infinitely wide neural net. Advances in neural
information processing systems 32(2019) (cit. on pp. 2, 3).
4.Geiger, M., Spigler, S., Jacot, A. & Wyart, M. Disentangling feature and lazy training in deep
neural networks. Journal of Statistical Mechanics: Theory and Experiment 2020, 113301
(2020) (cit. on pp. 2, 3).
5.Barak, B. et al. A nearly tight sum-of-squares lower bound for the planted clique problem.
SIAM Journal on Computing 48,687–735 (2 2019) (cit. on pp. 2, 6, 10).
6.Hopkins, S. B. et al. The power of sum-of-squares for detecting hidden structures inIEEE 58th
Annual Symposium on Foundations of Computer Science (FOCS) (2017), 720–731 (cit. on
pp. 2, 6, 10).
7.Hopkins, S. B. & Steurer, D. Bayesian estimation from few samples: com- munity detection
and related problems. arXiv:1710.00264 (2017) (cit. on pp. 2, 6, 10).
8.Hopkins, S. Statistical inference and the sum of squares method PhD thesis (Cornell University,
2018) (cit. on pp. 2, 6, 10).
9.Baik, J., Ben Arous, G. & Péché, S. Phase transition of the largest eigenvalue for nonnull
complex sample covariance matrices. The Annals of Probability 33,1643–1697 (2005) (cit. on
pp. 2, 3, 19).
10. Potters, M. & Bouchaud, J. -P.A First Course in Random Matrix Theory: for Physicists,
Engineers and Data Scientists (Cambridge University Press, 2020) (cit. on pp. 2, 3).
11. Diakonikolas, I., Kane, D. M. & Stewart, A. Statistical Query Lower Bounds for Robust
Estimation of High-Dimensional Gaussians and Gaussian Mixtures in2017 IEEE 58th Annual
Symposium on Foundations of Computer Science (FOCS) (2017), 73–84 (cit. on pp. 2, 3).
12. Dudeja, R. & Hsu, D. Statistical-computational trade-offs in tensor PCA and related problems
via communication complexity. The Annals of Statistics 52,131–156 (2024) (cit. on pp. 2, 3,
7).
13. Ingrosso, A. & Goldt, S. Data-driven emergence of convolutional structure in neural networks.
Proceedings of the National Academy of Sciences 119, e2201854119 (2022) (cit. on pp. 2, 9,
10).
14. Miolane, L. Phase transitions in spiked matrix estimation: information-theoretic analysis.
arXiv:1806.04343 (2018) (cit. on p. 3).
15. Lelarge, M. & Miolane, L. Fundamental limits of symmetric low-rank matrix estimation.
Probability Theory and Related Fields 173, 859–929 (2019) (cit. on p. 3).
1116. El Alaoui, A., Krzakala, F. & Jordan, M. Fundamental limits of detection in the spiked Wigner
model. The Annals of Statistics 48,863–885 (2020) (cit. on p. 3).
17. Paul, D. Asymptotics of sample eigenstructure for a large dimensional spiked covariance
model. Statistica Sinica, 1617–1642 (2007) (cit. on p. 3).
18. Berthet, Q. & Rigollet, P. Optimal detection of sparse principal components in high dimension.
The Annals of Statistics (2012) (cit. on p. 3).
19. Berthet, Q. & Rigollet, P. Complexity theoretic lower bounds for sparse principal component
detection inConference on Learning Theory (COLT) (2013) (cit. on p. 3).
20. Lesieur, T., Krzakala, F. & Zdeborová, L. Phase transitions in sparse PCA in2015 IEEE
International Symposium on Information Theory (ISIT) (2015), 1635–1639 (cit. on p. 3).
21. Lesieur, T., Krzakala, F. & Zdeborová, L. MMSE of probabilistic low-rank matrix estimation:
Universality with respect to the output channel in2015 53rd Annual Allerton Conference on
Communication, Control, and Computing (Allerton) (2015), 680–687 (cit. on p. 3).
22. Perry, A., Wein, A. S., Bandeira, A. S. & Moitra, A. Optimality and sub-optimality of PCA for
spiked random matrices and synchronization. arXiv:1609.05573 (2016) (cit. on pp. 3, 18).
23. Krzakala, F., Xu, J. & Zdeborová, L. Mutual information in rank-one matrix estimation in
2016 IEEE Information Theory Workshop (ITW) (2016), 71–75 (cit. on p. 3).
24. Dia, M., Macris, N., Krzakala, F., Lesieur, T., Zdeborová, L. et al. Mutual information for
symmetric rank-one matrix estimation: A proof of the replica formula. Advances in Neural
Information Processing Systems 29(2016) (cit. on p. 3).
25. Richard, E. & Montanari, A. A statistical model for tensor PCA. Advances in neural informa-
tion processing systems 27(2014) (cit. on pp. 3, 7).
26. Hopkins, S. B., Shi, J. & Steurer, D. Tensor principal component analysis via sum-of-square
proofs inConference on Learning Theory (2015), 956–1006 (cit. on p. 3).
27. Montanari, A., Reichman, D. & Zeitouni, O. On the limitation of spectral methods: From the
gaussian hidden clique problem to rank-one perturbations of gaussian tensors. Advances in
Neural Information Processing Systems 28(2015) (cit. on pp. 3, 18).
28. Perry, A., Wein, A. S. & Bandeira, A. S. Statistical limits of spiked tensor models.
arXiv:1612.07728 (2016) (cit. on pp. 3, 7).
29. Kim, C., Bandeira, A. S. & Goemans, M. X. Community detection in hypergraphs, spiked
tensor models, and sum-of-squares in2017 International Conference on Sampling Theory and
Applications (SampTA) (2017), 124–128 (cit. on p. 3).
30. Lesieur, T., Miolane, L., Lelarge, M., Krzakala, F. & Zdeborová, L. Statistical and computa-
tional phase transitions in spiked tensor estimation in2017 IEEE International Symposium on
Information Theory (ISIT) (2017), 511–515 (cit. on pp. 3, 7).
31. Arous, G. B., Gheissari, R. & Jagannath, A. Algorithmic thresholds for tensor PCA. The
Annals of Probability 48,2052–2087 (2020) (cit. on p. 3).
32. Jagannath, A., Lopatto, P. & Miolane, L. Statistical thresholds for tensor PCA. The Annals of
Applied Probability 30,1910–1933 (2020) (cit. on p. 3).
33. Niles-Weed, J. & Rigollet, P. Estimation of wasserstein distances in the spiked transport model.
Bernoulli 28,2663–2688 (2022) (cit. on p. 3).
34. Zdeborová, L. & Krzakala, F. Statistical physics of inference: Thresholds and algorithms.
Advances in Physics 65,453–552 (2016) (cit. on p. 3).
35. Lesieur, T., Krzakala, F. & Zdeborová, L. Constrained low-rank matrix estimation: Phase
transitions, approximate message passing and applications. Journal of Statistical Mechanics:
Theory and Experiment 2017, 073403 (2017) (cit. on p. 3).
36. Bandeira, A. S., Perry, A. & Wein, A. S. Notes on computational-to-statistical gaps: predictions
using statistical physics. Portugaliae mathematica 75,159–186 (2018) (cit. on p. 3).
37. Kunisky, D., Wein, A. S. & Bandeira, A. S. Notes on computational hardness of hypothesis
testing: Predictions using the low-degree likelihood ratio inISAAC Congress (International
Society for Analysis, its Applications and Computation) (2019), 1–50 (cit. on pp. 3, 6, 18, 19,
36).
38. Wang, C. & Lu, Y . M. The Scaling Limit of High-Dimensional Online Independent Component
Analysis inAdvances in neural information processing systems 31(2017) (cit. on pp. 3, 4).
39. Hyvärinen, A. & Oja, E. Independent component analysis: algorithms and applications. Neural
networks 13,411–430 (2000) (cit. on p. 3).
1240. Blanchard, G., Kawanabe, M., Sugiyama, M., Spokoiny, V . & Müller, K. -R. In Search of
Non-Gaussian Components of a High-Dimensional Distribution. Journal of Machine Learning
Research 7,247–282 (2006) (cit. on p. 3).
41. Bean, D. M. Non-Gaussian component analysis (University of California, Berkeley, 2014)
(cit. on p. 3).
42. Vempala, S. S. & Xiao, Y . Structure from Local Optima: Learning Subspace Juntas via Higher
Order PCA 2012. arXiv: 1108.3329 [cs.CC] (cit. on p. 3).
43. Tan, Y . S. & Vershynin, R. Polynomial Time and Sample Complexity for Non-Gaussian
Component Analysis: Spectral Methods inProceedings of the 31st Conference On Learning
Theory (eds Bubeck, S., Perchet, V . & Rigollet, P.) 75(PMLR, July 2018), 498–534 (cit. on
p. 3).
44. Goyal, N. & Shetty, A. Non-Gaussian component analysis using entropy methods. Proceedings
of the 51st Annual ACM SIGACT Symposium on Theory of Computing (2018) (cit. on p. 3).
45. Mao, C. & Wein, A. S. Optimal Spectral Recovery of a Planted Vector in a Subspace 2022.
arXiv: 2105.15081 [math.ST] (cit. on p. 3).
46. Damian, A., Pillaud-Vivien, L., Lee, J. D. & Bruna, J. Computational-Statistical Gaps in
Gaussian Single-Index Models 2024. arXiv: 2403.05529 [cs.LG] (cit. on p. 3).
47. Diakonikolas, I., Kane, D., Ren, L. & Sun, Y . SQ Lower Bounds for Non-Gaussian Compon-
ent Analysis with Weaker Assumptions inThirty-seventh Conference on Neural Information
Processing Systems (2023) (cit. on p. 3).
48. Brennan, M. S., Bresler, G., Hopkins, S., Li, J. & Schramm, T. Statistical Query Algorithms
and Low Degree Tests Are Almost Equivalent inProceedings of Thirty Fourth Conference on
Learning Theory (eds Belkin, M. & Kpotufe, S.) 134(PMLR, Aug. 2021), 774–774 (cit. on
p. 3).
49. Li, Y . & Yuan, Y . Convergence analysis of two-layer neural networks with relu activation.
Advances in neural information processing systems 30(2017) (cit. on p. 3).
50. Du, S., Zhai, X., Poczos, B. & Singh, A. Gradient Descent Provably Optimizes Over-
parameterized Neural Networks inInternational Conference on Learning Representations
(2019) (cit. on p. 3).
51. Li, Y . & Liang, Y . Learning Overparameterized Neural Networks via Stochastic Gradient
Descent on Structured Data inAdvances in Neural Information Processing Systems 31 (2018)
(cit. on p. 3).
52. Allen-Zhu, Z., Li, Y . & Song, Z. A convergence theory for deep learning via over-
parameterization inInternational Conference on Machine Learning (2019), 242–252 (cit. on
p. 3).
53. Bordelon, B., Canatar, A. & Pehlevan, C. Spectrum dependent learning curves in kernel
regression and wide neural networks inInternational Conference on Machine Learning (2020),
1024–1034 (cit. on p. 3).
54. Canatar, A., Bordelon, B. & Pehlevan, C. Spectral bias and task-model alignment explain
generalization in kernel regression and infinitely wide neural networks. Nature communications
12,2914 (2021) (cit. on p. 3).
55. Nguyen, Q., Mondelli, M. & Montufar, G. F. Tight bounds on the smallest eigenvalue of the
neural tangent kernel for deep relu networks inInternational Conference on Machine Learning
(2021), 8119–8129 (cit. on p. 3).
56. Bach, F. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research 18,629–681 (2017) (cit. on p. 3).
57. Ghorbani, B., Mei, S., Misiakiewicz, T. & Montanari, A. Limitations of Lazy Training of
Two-layers Neural Network inAdvances in Neural Information Processing Systems 32(2019),
9111–9121 (cit. on p. 3).
58. Ghorbani, B., Mei, S., Misiakiewicz, T. & Montanari, A. When do neural networks outperform
kernel methods? inAdvances in Neural Information Processing Systems 33(2020) (cit. on
p. 3).
59. Chizat, L. & Bach, F. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss inConference on Learning Theory (2020), 1305–1338 (cit. on
p. 3).
1360. Daniely, A. & Malach, E. Learning parities with neural networks. Advances in Neural Inform-
ation Processing Systems 33,20356–20365 (2020) (cit. on p. 3).
61. Paccolat, J., Petrini, L., Geiger, M., Tyloo, K. & Wyart, M. Geometric compression of invariant
manifolds in neural networks. Journal of Statistical Mechanics: Theory and Experiment 2021,
044001 (2021) (cit. on p. 3).
62. Yehudai, G. & Shamir, O. On the Power and Limitations of Random Features for Under-
standing Neural Networks inAdvances in Neural Information Processing Systems 32(2019),
6598–6608 (cit. on p. 3).
63. Refinetti, M., Goldt, S., Krzakala, F. & Zdeborová, L. Classifying high-dimensional gaussian
mixtures: Where kernel methods fail and neural networks succeed inInternational Conference
on Machine Learning (2021), 8936–8947 (cit. on p. 3).
64. Baldi, P. & Chauvin, Y . Temporal evolution of generalization during learning in linear networks.
Neural Computation 3,589–603 (1991) (cit. on p. 4).
65. Le Cun, Y ., Kanter, I. & Solla, S. A. Eigenvalues of covariance matrices: Application to
neural-network learning. Physical Review Letters 66,2396 (1991) (cit. on p. 4).
66. Saxe, A. M., McClelland, J. L. & Ganguli, S. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks inICLR (2014) (cit. on p. 4).
67. Advani, M. S., Saxe, A. M. & Sompolinsky, H. High-dimensional dynamics of generalization
error in neural networks. Neural Networks 132, 428–446 (2020) (cit. on p. 4).
68. Kesten, H. & Stigum, B. P. Additional limit theorems for indecomposable multidimensional
Galton-Watson processes. The Annals of Mathematical Statistics 37,1463–1481 (1966) (cit. on
p. 6).
69. Decelle, A., Krzakala, F., Moore, C. & Zdeborová, L. Inference and phase transitions in the
detection of modules in sparse networks. Physical Review Letters 107, 065701 (2011) (cit. on
p. 6).
70. Decelle, A., Krzakala, F., Moore, C. & Zdeborová, L. Asymptotic analysis of the stochastic
block model for modular networks and its algorithmic applications. Physical review E 84,
066106 (2011) (cit. on p. 6).
71. Balcan, M. -F., Blum, A. & Vempala, S. Kernels as features: On kernels, margins, and low-
dimensional mappings. Machine Learning 65,79–94 (2006) (cit. on p. 7).
72. Rahimi, A. & Recht, B. Random features for large-scale kernel machines inAdvances in
neural information processing systems (2008), 1177–1184 (cit. on pp. 7, 16).
73. Rahimi, A. & Recht, B. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning inAdvances in neural information processing systems (2009),
1313–1320 (cit. on pp. 7, 16).
74. Ghorbani, Mei, S., Misiakiewicz, T. & Montanari, A. "Linearized two-layers neural networks
in high dimension." Annals of Statistics 49,1029–1054 (2021) (cit. on p. 8).
75. Xiao, L., Hu, H., Misiakiewicz, T., Lu, Y . & Pennington, J. Precise Learning Curves and
Higher-Order Scalings for Dot-product Kernel Regression. Advances in Neural Information
Processing Systems 35,4558–4570 (2022) (cit. on pp. 8, 9).
76. Mei, S., Misiakiewicz, T. & Montanari, A. Generalization error of random feature and kernel
methods: Hypercontractivity and kernel matrix concentration. Applied and Computational
Harmonic Analysis 59,3–84 (2022) (cit. on pp. 8, 9).
77. Misiakiewicz, T. & Montanari, A. Six lectures on linearized neural networks. arXiv:2308.13431
(2023) (cit. on pp. 8, 9).
78. Loureiro, B. et al. Learning gaussian mixtures with generalized linear models: Precise asymp-
totics in high-dimensions. Advances in Neural Information Processing Systems 34,10144–
10157 (2021) (cit. on pp. 8, 36–38).
79. Goldt, S., Mézard, M., Krzakala, F. & Zdeborová, L. Modeling the influence of data structure
on learning in neural networks: The hidden manifold model. Physical Review X 10,041044
(2020) (cit. on pp. 8, 36).
80. Gerace, F., Loureiro, B., Krzakala, F., Mézard, M. & Zdeborová, L. Generalisation error in
learning with random features and the hidden manifold model inInternational Conference on
Machine Learning (2020), 3452–3462 (cit. on pp. 8, 36, 38).
81. Hu, H. & Lu, Y . M. Universality laws for high-dimensional learning with random features.
IEEE Transactions on Information Theory 69,1932–1964 (2022) (cit. on pp. 8, 36).
1482. Mei, S. & Montanari, A. The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathematics
75,667–766 (2022) (cit. on pp. 8, 36).
83. Goldt, S. et al. The gaussian equivalence of generative models for learning with shallow
neural networks inMathematical and Scientific Machine Learning (2022), 426–471 (cit. on
pp. 8, 9, 36).
84. Dietrich, R., Opper, M. & Sompolinsky, H. Statistical mechanics of support vector networks.
Physical review letters 82,2975 (1999) (cit. on pp. 8, 38).
85. Gardner, E. & Derrida, B. Three unfinished works on the optimal storage capacity of networks.
Journal of Physics A: Mathematical and General 22,1983 (1989) (cit. on p. 8).
86. Cui, H., Krzakala, F. & Zdeborova, L. Bayes-optimal learning of deep random networks of
extensive-width inInternational Conference on Machine Learning (2023), 6468–6521 (cit. on
p. 9).
87. Camilli, F., Tieplova, D. & Barbier, J. Fundamental limits of overparametrized shallow neural
networks for supervised learning. arXiv preprint arXiv:2307.05635 (2023) (cit. on p. 9).
88. Hu, H. & Lu, Y . M. Sharp asymptotics of kernel ridge regression beyond the linear regime.
arXiv:2205.06798 (2022) (cit. on p. 9).
89. Bell, A. & Sejnowski, T. J. Edges are the ’Independent Components’of Natural Scenes. in
Advances in Neural Information Processing Systems (eds Mozer, M., Jordan, M. & Petsche, T.)
9(MIT Press, 1996) (cit. on p. 9).
90. Chizat, L., Oyallon, E. & Bach, F. On Lazy Training in Differentiable Programming in
Advances in Neural Information Processing Systems (eds Wallach, H. et al. )32(Curran
Associates, Inc., 2019) (cit. on pp. 9, 17).
91. Kolda, T. G. & Bader, B. W. Tensor Decompositions and Applications. SIAM Review 51,
455–500 (2009) (cit. on p. 10).
92. Bardone, L. & Goldt, S. Sliding Down the Stairs: How Correlated Latent Variables Accelerate
Learning with Neural Networks inProceedings of the 41st International Conference on
Machine Learning (eds Salakhutdinov, R. et al. )235(PMLR, 2024), 3024–3045 (cit. on p. 10).
93. Pedregosa, F. et al. Scikit-learn: Machine Learning in Python. Journal of Machine Learning
Research 12,2825–2830 (2011) (cit. on p. 16).
94. Kossaifi, J., Panagakis, Y ., Anandkumar, A. & Pantic, M. TensorLy: Tensor Learning in Python.
Journal of Machine Learning Research (JMLR) 20(2019) (cit. on p. 17).
95. Banks, J., Moore, C., Neeman, J. & Netrapalli, P. Information-theoretic thresholds for com-
munity detection in sparse networks inConference on Learning Theory (2016), 383–416
(cit. on p. 18).
96. Bandeira, A. S., Kunisky, D. & Wein, A. S. Computational Hardness of Certifying Bounds on
Constrained PCA Problems in11th Innovations in Theoretical Computer Science Conference
(ITCS 2020) (ed Vidick, T.) 151(Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, 2020),
78:1–78:29 (cit. on pp. 19, 20, 22).
97. McCullagh, P. Tensor methods in statistics (Courier Dover Publications, 2018) (cit. on pp. 21,
23, 32).
98. Szeg ˝o, G. Orthogonal Polynomials (American mathematical society, 1939) (cit. on p. 21).
99. Abramowitz, M. & Stegun, I. A. Handbook of mathematical functions with formulas, graphs,
and mathematical tables xiv+1046 (National Bureau of Standards, 1964) (cit. on p. 21).
100. Lukacs, E. A Survey of the Theory of Characteristic Functions. Advances in Applied Probabil-
ity4,1–38 (1972) (cit. on p. 22).
15Appendix
A Experimental details
A.1 Figures 2 and 3
For the spiked Wishart and spiked cumulant tasks, we trained two-layer ReLU neural networks
ϕθ(x) =v⊤max 
0, wTx
. The number of hidden neurons m= 5d, where dis the input dimension.
We train the networks using SGD with a learning rate of 0.002 and a weight-decay of 0.002 for 50
epochs for the spiked Wishart task and for 200 epochs for the spiked cumulant task. The plots show
the early-stopping test errors. The results are plotted as averages over 10 random seeds, showing the
standard deviation by the errorbars.
The random features (RF) models [72, 73] have also a width of 5d. The ridge regression is performed
using scikit-learn [93] with a regularisation of 0.1.
For the spiked datasets, the spikes are from the Rademacher distribution, using a signal-to-noise ratio
of 5.0 for the spiked Wishart and 10.0 for the spiked cumulant datasets. For the overlaps between
spikes and features overlaps, we plot the highest overlap amongst the incoming weights of the hidden
neurons with the spike, calculated as the normalised dot product.
Starting from small initial overlap Since the neural networks have a large number of neurons
(m= 5d), some of them will have a relatively large overlap with the spike at initialisation, as can be
seen in fig. 3. To check that this relatively large overlap did not affect our results, we repeated the
same set of experiments while enforcing an overlap of all hidden neuron weights with the spike of
1/√
dby explicit orthogonalisation, as discussed in section 4.1. We show the results in fig. 5: while
the maximum overlaps do indeed decrease for small sample complexities, the qualitative behaviour is
unchanged.
Figures for the NeurIPS replies 2024ABCDmax overlap
max overlap1.0
nclass/dnclass/d2nclass/d2nclass/dABCD
nclass/dnclass/d2nclass/d2nclass/dmax overlap
max overlapearly-stop. test errorearly-stop. test errorearly-stop. test errorearly-stop. test error
Figure 5: Learning the spiked Wishart and spiked cumulant task, starting from small initial
overlaps. We repeat the neural network experiments on the spiked Wishart (top) and spiked cumulant
(bottom) task, see figs. 2 and 3, while enforcing that all hidden neurons have an overlap of exactly
1/√
dwith the spikes, by simple explicit orthogonalisation. While the maximum overlaps do indeed
decrease for small sample complexities, the qualitative behaviour is unchanged. All hyper-parameters
as in figs. 2 and 3, respectively.
16A.2 Figure 4
For the NLGP–GP task, we use the α-scaling trick of Chizat et al. [90] to interpolate between feature-
and lazy-learning. We define the network function as:
ϕNN(x;v, W, v 0, W0) =αNTK
K
mX
jvjσ dX
iwixi!
−mX
jv0,jσ dX
iw0,ixi!
 (14)
where v0, w0are kept fixed at their initial values. The mean-squared loss is also rescaled by ( 1/α2
NTK).
Changing αNTKfrom low to high allows to interpolate between the feature- and the lazy-learning
limits as the first-layer weights will move away less from their initial values.
For fig. 4, the network has a width of 100 and the optimisation is run by SGD for 200 epochs
with a weight-decay of 5·10−6and learning rate of 0.5. The one-dimensional data vectors have a
length of 20; the correlation length is 1 and the signal-to-noise ratio is set to 3. The error shown
is early-stopping test error. The localisation of the neural networks’ features and the data’s fourth
moments is shown by the IPR measure. (We used here a lower length for the data vectors so that
the calculations for fourth-order cumulants do not have high memory requirements.) For the neural
networks, the highest IPR is shown amongst the incoming features of the hidden neurons at the final
state of the training. For the data, the highest IPR is the highest amongst the CP-factors of the fourth
cumulants of the nlgp-class using a rank-1 PARAFAC decomposition from the tensorly package [94]
.
B Mathematical details on the hypothesis testing problems
In this section, we provide more technical details on Hermite polynomials, LR and LDLR, and present
the complete proofs of all theorems stated in the main.
B.1 Notation
We use the convention 0∈N.n∈Ndenotes the number of samples, dthe dimensionality of
each data point x. The letters k, m usually denote free natural parameters. Let [n] :={1, . . . , n },
µ∈[n]is an index that ranges on the samples and i∈[d]usually ranges on the data dimension.
Underlined letters x,P,Qare used for objects that are n×ddimensional. In proofs the letter C
denotes numerical constants whose precise value may change from line to line, and any dependency
is denoted as a subscript. m!denotes factorial and m!!denotes the double factorial (product of all the
numbers k≤mwith the same parity as m).
Multi-index notation We will need multi-index notation to deal with d-dimensional functions
and polynomials. Bold greek letters are used to denote multi-indices and their components, α=
(α1, . . . ,αd)∈Nd. The following conventions are adopted:
|α|:=dX
i=1αi,α! :=dY
i=1αi!, xα:=dY
i=1xαi
i
∂αf(x1, . . . , x d) =∂
∂xαf:=∂
∂xα1
1. . .∂
∂xαd
d
f(15)
Since we will have nsamples of d-dimensional variables, we will need to consider polynomials and
functions in ndvariables. To deal with all these variables we introduce multi-multi-indices (denoted
by underlined bold Greek letters α,β, . . .). They are n×dmatrices with entries in N(i.e. elements
ofNn×d)
α:= (αµ
i) =
α1
1. . . α1
d.........
αn
1. . . αn
d
 (16)
We denote by αµthe rows of α, that are d-dimensional multi-indices.
17All the notations (15) generalize to multi-multi-indices in the following way:
|α|:=nX
µ=1|αµ|=nX
µ=1dX
i=1αµ
i,α! :=Y
µαµ! =Y
µY
iαµ
i!,
xα:=nY
µ=1(xµ)αµ
=nY
µ=1dY
i=1(xµ
i)αµ
i
∂αf(x) :=∂
∂(x1)α1. . .∂
∂(xn)αn
f(x)(17)
B.2 More on LR and LDLR
B.2.1 Statistical and computational distinguishability
From a statistical point of view, distinguishing two sequences of probability measures P= (Pn)n∈N
andQ= (Qn)n∈Ndefined on a sequence of measure spaces (Ωn,Fn)means finding a sequence of
testsfn: Ωn→ {0,1}, which are measurable functions that indicate whether a given set of inputs
was sampled from PorQ. We will say that the two measures are statistically distinguishable in the
strong sense if there exists a statistical test ffor which
Qn(fn(x) = 0) −→
n→∞1andPn(fn(x) = 1) −→
n→∞1. (18)
The strong version of statistical distinguishability requires that the probability of success of the
statistical test must tend to 1 as n→ ∞ , whereas weak distinguishability just requires it to be
asymptotically greater than 1/2 +ε, for any ε >0. We will call the minimal number of samples
required to achieve strong statistical distinguishability the statistical sample complexity of the
problem. We can obtain a computationally bounded analogue of this definition by restricting the
complexity of the statistical test fnto functions that are computable in a time that is polynomial in
the input dimension d. The computational statistical complexity is then the minimal number of
samples required to achieve computational distinguishability .
B.2.2 Necessary conditions for distinguishability
A necessary condition for strong distinguishability is based on the likelihood ratio (LR) of probability
measures, which is defined as
Ln(x) :=dPn
dQn(x) (19)
The likelihood ratio provides a necessary condition for strong distinguishability of PandQvia the
so-called second moment method:
Proposition 6 (Second Moment Method for Distinguishability) .Suppose that Pnis absolutely
continuous with respect to Qn, and let Lnbe the corresponding LR. A necessary condition for strong
distinguishability of PfromQis
∥Ln∥2:=E
x∼Qn[Ln(x)2]−→
n→∞+∞. (20)
where ∥·∥is the norm with respect to the Hilbert space
L2(Ωn,Qn) =
f: Ωn→R|EQn[f2(x)]<∞	
. (21)
Proof. The proof is immediate: if (||Ln||)nwere bounded, by Cauchy-Schwartz Q(An)→0would
imply P(An)→0. But by the definition of strong asymptotic distinguishability there exists a
sequence of events (An)nsuch that P(An)→1andQ(An)→0; hence (||Ln||)nmust diverge.
The second moment method has been used to derive statistical thresholds for various high-dimensional
inference problems [22, 27, 37, 95]. Note that proposition 6 only states a necessary condition; while
it is possible to construct counterexamples, they are usually based on rather artificial constructions
(like the following example 1 from [37]), and the second moment method is considered a good proxy
for strong statistical distinguishability.
18Example 7. LetPandQbe two strongly distinguishable sequences of probability measures on
(Yn,Fn)n. Define P′
nonFnas a measure that with probability 1/2follows Pnand with probability
1/2follows Qn. Letting L′
nbe the LR of P′
nwith respect to Qn. We have that ||L′
n|| → ∞ , butP′
andQare not strongly distinguishable.
Proof. The density of P′
nis:
p′
n(y) =pn(y) +qn(y)
2(22)
hence
L′
n=1
2(1 +Ln) (23)
which clearly has the same asymptotic behaviour as Ln.
On the other hand, it cannot be that P′andQ′are strongly distinguishable since for any event A,
P′
n(A)≥1
2Qn(A).
B.2.3 LDLR analysis for spiked Wishart model
In this subsection we show the application of LDLR method to the spiked Wishart model. We obtain
the correct BBP threshold for the signal-to-noise ratio even when we restrict ourselves to the class of
polynomials of constant degree:
Theorem 8 (LDLR for spiked Wishart model) .Suppose the prior on ubelongs to the following
cases:
•(ui)i=1,...,d are i.i.d. and symmetric Rademacher random variables
•(ui)i=1,...,d are i.i.d. and ui∼ N(0,1)
LetD∈Nandd, n→ ∞ , with fixed ratio γ:=d/n, then
lim
d,n→∞∥L≤D∥=DX
k=0(2k−1)!!
(2k)!!β2k
γk, (24)
which, as Dincreases, stays bounded for β < β c:=√γ=p
d/nand diverges for β > β c.
The distinguishability threshold that we have recovered here is of course the famous BBP phase
transition in the spiked Wishart model [9]: if β < β c=p
d/n, the low-degree likelihood ratio stays
bounded and indeed a set of inputs drawn from 1 is statistically indistinguishable from a set of inputs
drawn from N(0, 1). Forβ > β cinstead, there is a phase transition for the largest eigenvalue of the
empirical covariance of the inputs in the second class, which can be used to differentiate the two
classes, and the LDLR diverges.
As mentioned in the main text, a thorough LDLR analysis of a more general model that encompasses
the spiked Wishart model was performed by Bandeira et al. [96]. Their theorem 3.2 is a more general
version of our theorem 8, that generalizes it in two directions: it works for a class of subgaussian priors
onuthat satisfy some concentration property, and also allows for negative SNR (the requirement
isβ >−1). For completeness, here we show that this result can be obtained as a straightforward
application of a Gaussian additive model.
Proof. We note that the problem belongs the class of Additive Gaussian Noise models which is
studied in depth by Kunisky et al. [37]. In those models the two hypotheses have to be expressed as
an additive perturbation of white noise:
•Pn:xn=yn+zn,
•Qn:xn=zn,
The spiked Wishart model that we consider belongs to this class. It can be seen by defining Rnd∋
yn=q
β
dg1u, . . . ,q
β
dgnu⊤
. So we can apply theorem 2.6 from Kunisky et al. [37], that
19computes the norm of the LDLR by using two independent replicas of the variable y. Denoting the
two replicas by ˆyand˜y, we get
||L≤D
n||2=E"DX
m=01
m!(ˆy·˜y)m#
=E"DX
m=0βm
m!dm nX
µ=1ˆgµ˜gµˆu·˜u!m#
=DX
m=0βm
m!dmE"
(ˆu·˜u)m nX
µ=1ˆgµ˜gµ!m#
=DX
m=0βm
m!dmE" nX
µ=1ˆgµ˜gµ!m#
E[(ˆu·˜u)m](25)
Note now thatPn
µ=1ˆgµ˜gµhas the same distribution as −Pn
µ=1ˆgµ˜gµ, so its distribution is even and
all the odd moments are 0. This means that we can reduce to the case m= 2k, so we need to study:
||L≤D
n||2=⌊D/2⌋X
k=0β2k
(2k)!d2kE
 nX
µ=1ˆgµ˜gµ!2k

| {z }
T1Eh
(ˆu·˜u)2ki
|{z}
T2(26)
Let us consider the term T1. Call Yµ:= ˆgµ˜gµ. The distribution of each of the Yµis not Gaussian,
but we have that E[Yµ] = 0 and Var (Yµ) = 1 . So by the central limit theorem Sn:=1√nPn
µ=1Yµ
converges to a standard normal in distribution, as n→ ∞ . Note that the cumulants of Sncan be
computed thanks to linearity and additivity of cumulants and are κ(Sn)
2k=n1−kκY
2k. So they all go to
0 except from the variance. Since the moments can be written as a function of the cumulants up to
that order, it follows that limnE[S2k
n]will be the 2k-th moment of the standard normal distribution,
which means that we have the following:
lim
n→+∞E
 
1√nnX
µ=1ˆgµ˜gµ!2k
= (2k−1)!! (27)
We turn now to T2 and do the same reasoning. Define vi:= ˆui˜ui, both in Rademacher and in
Gaussian prior case, we have that the (vi)i=1,...,d is an independent family of random variables that
have 0 mean and variance equal to 1. So we can again apply the central limit theorem to get that:
lim
d→+∞E"ˆu·˜u√
d2k#
= (2k−1)!! (28)
Taking the limit for n, d→+∞with the constraint γ=d/n, we have that:
lim
n,d→∞||L≤D
n||2= lim
n,d→∞⌊D/2⌋X
k=0β2knk
(2k)!dkE
 
1√nnX
µ=1ˆgµ˜gµ!2k
E"ˆu·˜u√
d2k#
=DX
k=0 
βk(2k−1)!!2
(2k)!γk=DX
k=0(2k−1)!!
(2k)!!β2k
γk(29)
which is what we wanted to prove.
As a final note we remark that the basic ideas of the arguments in [96] coincide with what exposed
above. However, the increased generality of the statement requires the use of the abstract theory of
Umbral calculus to generalize the notion of Hermite polynomials to negative SNR cases, as well as
more technical work to achieve the bounds on the LDLR projections.
20B.3 Hermite Polynomials
We recall here the definitions and key properties of the Hermite polynomials.
Definition 9. The Hermite polynomial of degree mis defined as
hm(x) := (−1)mex2
2dm
dxm
e−x2
2
(30)
Here is a list of the first 5 Hermite polynomials:
h0(x) = 1
h1(x) =x
h2(x) =x2−1
h3(x) =x3−3x
h4(x) =x4−6x2+ 3(31)
The Hermite polynomials enjoy the following properties that we will use in the subsequent proofs
(for details see section 5.4 in McCullagh [97], Szeg ˝o [98] and Abramowitz & Stegun [99]):
•they are orthogonal with respect to the L2product weighted with the density of the Normal
distribution:
1√
2πZ∞
−∞hn(x)hm(x)e−x2
2dx=n!δm,n; (32)
•hmis a monic polynomial of degree m, hence (hm)m∈{1,...,N}generates the space of
polynomials of degree ≤N;
•the previous two properties imply that the family of Hermite polynomials is an orthogonal
basis for the Hilbert space L2(R,Q)where Qis the normal distribution;
• they enjoy the following recurring relationship
hm+1(x) =xhm(x)−h′
m(x), (33)
which can also be expressed as a relationship between coefficients as follows. If hm(x) =Pm
k=0am,kxk, then
am+1,k=−am,1 k= 0,
am,k−1−(k+ 1)am,k+1k >0.(34)
They also satisfy identities of binomial type, like:
hm(x+y) =mX
k=0m
k
xm−khk(y) (35)
hm(γx) =⌊m/2⌋X
j=0γm−2j(γ2−1)jm
2j
(2j−1)!!hm−2j(x) (36)
Multivariate case In the multivariate m-dimensional case we can consider Hermite tensors
(Hα)α∈Nmdefined as:
Hα(x1, . . . , x m) =mY
i=1hαi(xi) (37)
all the properties of the one-dimensional Hermite polynomials generalize to this case, in particular
they form an orthogonal basis of L2(Rm,Q)where Qis a multivariate normal distribution. If ⟨·,·⟩is
the inner product of that Hilbert space, we have that:
⟨Hα, Hβ⟩=α!δα,β (38)
Of course all this is valid in the case m=nd, where we can also use multi-multi-indices to get the
following identity:
Hα(x1
1, . . . , xn
d) =nY
µ=1Hαµ(xµ) =nY
µ=1dY
i=1hαµ
i(xµ
i) (39)
We are now ready to see the proof of 75.
21Lemma 10. We consider a hypothesis testing problem in Rdwhere the null hypothesis Qis the
multivariate Gaussian distribution N(0, 1d×d), while the alternative hypothesis Pis absolutely
continuous with respect to it. Then:
L≤D=X
|α|≤D⟨L, Hα⟩Hα
α!=X
|α|≤DE
x∼P[Hα(x)]Hα
α!(40)
Which implies
||L≤D||2=X
|α|≤D⟨L, Hα⟩2
α!=X
|α|≤DE
x∼P[Hα(x)]2
α!(41)
Proof. First note that
⟨L, Hα⟩=E
x∼P[Hα(x)] (42)
due to the definition of likelihood ratio and a change of variable in the expectation. Then we can
just use the fact that (Hα)α∈Ndare an orthogonal basis for L2(Rd,Q), and if we consider the
Hermite polynomials up to degree D, they are also a basis of the space of polynomials in which we
want to project Lto get L≤D. Hence the formulas follow by just computing the projection using this
basis.
Note that we set the lemma in Rd, but of course it holds also in Rndjust switching to multi-multi-index
notation.
B.3.1 Hermite coefficients
Lemma 10 translates the problem of computing the norm of the LDLR to the problem of computing
the projections ⟨L, Hα⟩. Note that this quantity is equal to EP[Hα(x)], which we will call α-th
Hermite coefficient of the distribution P.
The following lemma from [96] provides a version of the integration by parts technique that is tailored
for Hermite polynomials.
Lemma 11. Letf:Rd→Rdbe a function that is continuously differentiable ktimes. Assume that
fand all of its partial derivatives up to order kare bounded by O(exp 
|y|λ
)for some λ∈(0,2),
then for any α∈Ndsuch that |α| ≤k
⟨f, Hα⟩=E
y∼N(0, 1)[Hα(y)f(y)] = E
y∼N(0, 1)[∂αf(y)] (43)
Proof. 43 can be proved by doing induction on kusing 33, see [96] for details.
B.3.2 Links with cumulant theory
The cumulants (κα)α∈Ndof a random variable x∈Rd, can be defined as the Taylor coefficients of
the expansion in ξof the cumulant generating function :
Kx(ξ) := log 
E
eξ·x
(44)
Order-one cumulant is the mean, order-two is the variance, and higher-order cumulants cumulants
encode more complex correlations among the variables. The Gaussian distribution is the only non
constant distribution to have a polynomial cumulant generating function (as proved in theorem 7.3.5
in [100]), if z∼ N(µ,Σ), then:
Kz(ξ) =µξ+1
2ξ⊤Σξ.
Hence cumulants with order higher than three can also be seen as a measure of how much a distribution
deviates from Gaussianity (i.e. how much it deviates from its best Gaussian approximation).
On this point the similarity with Hermite coefficients is evident. Indeed, up to order-five, on whitened
distributions, cumulants and Hermite coefficients coincide. But form sixth-order onward, they start
diverging. They are still linked by deterministic relationship, but its combinatorial complexity
increases swiftly and it is not easy to translate formulas involving Hermite coefficients into cumulants
22and vice versa. For this reason, our low-degree analysis of the likelihood ratio leading to 5 keeps
the formalism that naturally arises from the computations, which is based on Hermite coefficients.
A detailed discussion of the relation between Hermite polynomials and cumulants in the context of
asymptotic expansions of distributions like the Gram–Charlier and Edgeworth series can be found in
chapter 5 of McCullagh [97].
B.4 Details on the spiked cumulant model
Here we will expand on the mathematical details of the spiked cumulant model.
B.4.1 Prior distribution on the spike
For the prior distribution on u,P, its role is analogous to the spiked Wishart model, so all the
choices commonly used for that model can be considered applicable to this case. Namely symmetric
distributions so thatu√
d≈1asd→ ∞ . In the following we will make the computations assuming
uii.i.d. and with Rademacher prior:
ui∼Rademacher (1/2) (45)
It helps to have i.i.d. components and constant normu√
d≡1. However all the results should hold,
with more involved computations, also with the following priors:
•(ui)i=1,...,d are i.i.d. and ui∼ N(0,1)
•u∈Unif(∂B(0,√
d).
B.4.2 Distribution of the non-Gaussianity
As detailed in 1, we need the non-Gaussianity to satisfy specific conditions. Some of the requirements
are fundamental and cannot be avoided, whereas others could likely be removed or modified with
only technical repercussion that do not change the essence of the results.
The most vital assumptions are the first and the last: E[g]̸= 0 would introduce signal in the first
cumulant, changing completely the model. It is important to control the tails of the distribution with
E[exp 
g2/2
]<+∞, (46)
a fat-tailed gwould make the LR-LDLR technique pointless due to the fact that L /∈ L2(Rd,Q)and
||L||=∞for any n, d. For example it is not possible to use Laplace distribution for g.
On the opposite side, the least important assumptions are that Var (g) = 1 and eq. (5); removing the
first would just change the formula for whitening matrix S, while eq. (5) is a very weak requirement
and it is needed just to estimate easily the Hermite coefficients’ contribution and reach 11, it may be
even possible to remove it and try to derive a similar estimate from the assumption on the tails 46.
Finally, the requirement of symmetry of the distribution has been chosen arbitrarily thinking about
applications: the idea is that the magnitude of the fourth-order cumulant, the kurtosis, is sometimes
used as a test for non-Gaussianity, so it is interesting to isolate the contribution given by the kurtosis,
and higher-order, even degree, cumulants, by cancelling the contribution of odd-order cumulants.
However, it would be interesting to extend the model in the case of a centered distribution with
non-zero third-order cumulant. It is likely that the same techniques can be applied, but the final
thresholds on θmay differ.
The following lemma gives a criterion of admissibility that ensures Radem (1/2)and Unif (−√
3,√
3)
(together with many other compactly supported distributions) satisfy 1.
Lemma 12. Suppose pgis a probability distribution, compactly supported in [−Λ,Λ],Λ≥1, then
E
g∼pg[hm(g)]≤Λmm!
23Proof. Use the notation hm(x) =Pm
k=0am,kxkandSm:=Pm
k=0|am,k|. Then we have that
E
g∼pg[hm(g)] =E
g∼pg"mX
k=0am,kgk#
≤E
g∼pg"mX
k=0|am,k||g|k#
≤ΛmSm(47)
So we just need to prove that Sm≤m!, which can be done by induction using 34. Suppose it true for
m, we prove it for m+ 1. By 34 we have that:
|am+1,k| ≤|am,1| k= 0
|am,k−1|+ (k+ 1)|am,k+1|k >0(48)
Summing on both sides (and using am,k= 0when k > m ), we get:
Sm+1≤ |am,1|+m+1X
k=1|am,k−1|+ (k+ 1)|am,k+1|
Sm+1≤Sm+mX
j=0j|am,j|
Sm+1≤Sm+mmX
j=0|am,j|
Sm+1≤(m+ 1)Sm(49)
Hence by application of the inductive hypothesis we get Sm+1≤(m+1)! , completing the proof.
B.4.3 Computing the whitening matrix
In this paragraph all the expectations are made assuming ufixed and it will be best to work with its
normalized version ¯u=u/√
d. Note that E[x] = 0 and we want also that
1d×d=E[xx⊤] =E[Sp
βg¯u+zp
βg¯u⊤+z⊤
S⊤]
=S 
1+β¯u¯u⊤
S⊤(50)
Hence we need
S2=SST= 
1+β¯u¯u⊤−1= 1−β
1 +β¯u¯u⊤(51)
So we look for γsuch that:
( 1+γ¯u¯u⊤)2= 1−β
1 +β¯u¯u⊤(52)
By solving the second-order equation we get
γ±=−
1±1√1 +β
(53)
We choose the solution with −, so that Sis positive definite:
S= 1−
1−1√1 +β
¯u¯u⊤= 1−β
1 +β+√1 +βuu⊤
d. (54)
24Hence we can compute also the explicit expression for x:
x=z−
1−1√1 +β
¯u⊤z¯u+s
β
1 +βg¯u
x=z−¯u⊤z¯u|{z}
z⊥u+
r1
1 +β¯u⊤z+s
β
1 +β
|{z}
ηg
¯u
=z⊥u+p
1−η2¯u⊤z+ηg
¯u (55)
Soxis standard Gaussian in the directions orthogonal to u, whereas in udirection, it is a weighted
sum between gandz. Note that it is a quadratic interpolation: the sum of the square of the weights is
1.
B.5 Details of the LR analysis for spiked cumulant model
B.5.1 Proof sketch for theorem 2
Since the samples are independent, the total LR factorises as
L(y) =E
u"nY
µ=1l(yµ|u)#
, (56)
where the sample-wise likelihood ratio is
l(y|u) =px(y|u)
pz(y)=E
g∼pg
p
1 +βexp
−1 +β
2 
g−s
β
(1 +β)dy·u!2
+g2
2

.(57)
To compute the norm of the LR, we consider two independent replicas of the spike, uandv, to get
∥Ln,d∥2=E
y∼Q"
E
u"nY
µ=1l(yµ|u)#
E
v"nY
µ=1l(yµ|v)##
. (58)
The hard part now is to simplify the high-dimensional integrals in this expression, it can be done
because the integrand is almost completely symmetric, and the only asymmetries lie on the subspace
spanned by uandv.
∥Ln,d∥2=E
u,v
E
gu,gv
1 +βq
(1 +β)2−β2 u·v
d2e−(1+β)((1+β)(g2
u+g2
v)−2β(gugv)(u·v
d))
2(1+β)2−2β2(u·v
d)2 +g2
u+g2
v
2
n

Note that the integrand depends on uandvonly troughu·v
d=:λ. So, using that the prior on u,vis
i.i.d. Rademacher, the outer expectation can be transformed in a one dimensional expectation over λ,
leading to (6).
B.5.2 Proof of theorem 2
˜xµ=q
β
dgµu+zµ, to find the marginal density of xµwe integrate over the possible values of g
p˜x(y|u) =P(˜xµ∈dy|u) =E
g"
pz 
y−r
β
dgu!#
(59)
where pzis the density of a standard normal d-dimensional variable z∼ N (0, 1d). But we are
interested in the density of the whitened variable x,px(·|u), which can be seen as the push forward
of the density of ˜xwith respect to the linear transformation S. So
px(y|u) =p˜x(S−1y|u)|detS−1|. (60)
25It is easy to see from 3 that |detS−1|=√1 +β, so we can plug it into 59 and after expanding the
computations we get to
px(y|u) =pz(y)E
g
p
1 +βexp
−1 +β
2 
g−s
β
(1 +β)dy·u!2
+g2
2

. (61)
So we have found the likelihood ratio for a single sample, conditioned on the spike:
l(y|u) =px(y|u)
pz(y)=E
g
p
1 +βexp
−1 +β
2 
g−s
β
(1 +β)dy·u!2
+g2
2

. (62)
Note that conditioning on uthe samples are independent, so we have the following formula:
L(y) =E
u"nY
µ=1l(yµ|u)#
. (63)
So to compute the norm we consider two independent replicas of the spike, uandv, then we switch
the order of integration to get
||Ln,d||2=E
y∼Q"
E
u"nY
µ=1l(yµ|u)#
E
v"nY
µ=1l(yµ|v)##
=E
u,v"
E
y∼Q"nY
µ=1l(yµ|u)l(yµ|v)##
=E
u,v
E
y∼Q[l(y|u)l(y|v)]n
=E
u,v"
E
y∼Q 
E
gu
p
1 +βexp
−1 +β
2 
gu−s
β
(1 +β)dy·u!2
+g2
u
2


·E
gv
p
1 +βexp
−1 +β
2 
gv−s
β
(1 +β)dy·v!2
+g2
v
2

!n#
.
(64)
Switching the integral over yinside the new integrals over guandgvwe can isolate the following
integral over y:
I:=E
y∼Q
exp
−1 +β
2 
gu−s
β
(1 +β)dy·u!2
−1 +β
2 
gv−s
β
(1 +β)dy·v!2

.
(65)
It can be computed by noting the subspace orthogonal to {u, v}, we just have the integral of a standard
normal, and the remaining 2-dimensional integral can be computed explicitly. Since the Rademacher
prior implies that ||u||=||v||=√
d, the result depends only on their overlap λ(i.e.λ=u·v
d),
leading to:
I=1p
(1 +β)2−β2λ2exp
−1 +β
2(1 + β)2−2β2λ2 
(1 +β)(g2
u+g2
v)−2β(gugv)λ
(66)
If we plug this formula inside 64 and rearrange the terms we get an expression that can be written in
terms of the density of two bi-dimensional centered Gaussians
||Ln,d||2=E
λ"
E
gu,gv"
1 +βp
(1 +β)2−β2λ2e−(1+β)((1+β)(g2
u+g2
v)−2β(gugv)λ)
2(1+β)2−2β2λ2 +g2
u+g2
v
2#n#
=E
λ
E
gu,gvh
N((gu, gv); Σ)N((gu, gv); 12×2)−1in
.(67)
2605101520
1.01.52.02.5f(,1)
1.0
0.5
0.00.51.0
f(,)
=5
=10
=20
Figure 6: Graphs of f, defined in (7), when g∼Rademacher (1/2).
where
Σ−1=1 +β
(1 +β)2−β2λ2
1 +β−βλ
−βλ 1 +β
, λ =u·v
d. (68)
Now we turn to compute the expectation over λ. Note that since u, vare independent and their
components are Rademacher distributed, product of Rademacher is still Rademacher, hence u·v
is the sum of dindependent Rademacher random variables. Using moreover that the Rademacher
distribution is just a linear transformation of the Bernoulli, we can link the distribution of a the
overlap λto a linear transformation of a binomial distribution:d
2(λ+ 1)∼Binom (d,1/2).
We therefore define the auxiliary function
f(β, λ) := E
gu,gv"
1 +βp
(1 +β)2−β2λ2e−(1+β)((1+β)(g2
u+g2
v)−2β(gugv)λ)
2(1+β)2−2β2λ2 +g2
u+g2
v
2#
(69)
so that the LR norm can be rewritten as
||Ln,d||2=dX
j=0d
j1
2df
β,2j
d−1n
(70)
which is what we needed to prove.
B.5.3 Consequences of theorem 2
The precise value of fdepends on the choice of distribution for the non-Gaussianity g, however, it is
possible to prove the following
Lemma 13. Ifpgsatisfies assumptions 1 and θ <1, then there exists Csuch that:
||Ln,d|| ≤C ∀n, d
Proof. We first prove that, thanks to the sub-Gaussianity of g, we have that
f(β, λ)≤1 +Cβλ2,∀λ∈[−1,1] (71)
To do it, first note that fβ,0) = 1 , andf(β,·)is bounded on [−1,1]thanks to the sub-gaussianity of
g:
|f(β, λ)| ≤Egu,gvh
e1
2(g2
u+g2
v)i
sup
gu,gv 
1 +βp
(1 +β)2−β2λ2e−(1+β)((1+β)(g2
u+g2
v)−2β(gugv)λ)
2(1+β)2−2β2λ2!
≤C1 +βp
(1 +β)2−β2λ2
≤C1 
1 +C2λ2
So we just need to prove that, up to changes on C2, we can take C1= 1. To do that it si sufficient to
study the behaviour around λ= 0:
∂
∂λf(β,0) =Eβ
1 +βgugv
= 0
27So there is a neighborood of 0 such that f(β,0) = 1 + Cλ2+o(λ2). Hence we can take a suitable
constant Cβso that eq. (71) holds.
Now we can apply eq. (71) to (6), to get
||Ln,d||2≤dX
j=0d
j1
2d 
1 +Cβ2j
d−12!n
=dX
j=0d
j1
2dnX
k=0n
k
Ck
β2j
d−12k
=nX
k=0n
kCβ
d2k
E[Y2k
d]
Where Ydis a random variable distributed as the sum of dindependent Rademacher with parameter
1/2. So we can apply the central limit theorem onYd√
dto get thatE[Y2k
d]
d→(2k−1)!!hence we get:
||Ln,d||2≤CnX
k=0n
kCβ
dk
(2k−1)!!
≤C1nX
k=0C2n
dk
where C1andC2are constants independent of n, d. So now we can substitute that n≍dθ, and since
θ <1the series converges for all d, hence the LR norm is bounded.
We will analyze in more detail the case in which g∼Rademacher (1/2). It is a case that is particularly
interesting for the point of view of the applications because it amounts to comparing a standard
Gaussian with a Gaussian mixture with same mean and covariance. Moreover, with this choice
of non-Gaussianity, the technical work simplifies because the troublesome integral over gu, gvin
7 becomes just a simple sum over 4 possibilities. In this case fcan be computed exactly and it is
displayed in 6. The maximum of f(β,·)is attained at λ=±1andf(·,1)is monotonically increasing.
Assume that n≍dθ, then a sufficient condition for LR to diverge isf(β,1)dθ
2d→ ∞ , which holds as
soon as θ >1. Moreover, even at linear sample complexity, it is possible to find regimes that ensure
divergence of the LR norm, similar to BBP phase transition in spiked Wishart model. Assume that
samples and dimensions scale at the same rate as in spiked Wishart model: n≍d
γ, then a sufficient
condition for divergence is that
f(β,1)d/γ
2d→ ∞
which holds if and only if f(β,1)1/γ>2. Hence given β, you can always find
γβ:=log (f(β,1))
log 2(72)
And for γ > γ βthere is guarantee that ||Ln,d|| → ∞ . Vice-versa, we could also fix γand define βγ
as only value of βthat makes 72 true.
It is spontaneous to ask whether this condition for divergence of LR norm is also necessary, making
the SNR threshold βγthe analogous in this model of the threshold βc=√γin spiked Wishart model
(8). Although a rigorous proof of this is still missing, numerical evidence (7) suggests that for β≤βγ
the LR norm stays indeed bounded.
B.6 LDLR on spiked cumulant model
To discuss the proof of theorem 5, it is best to state it in two separate parts
Theorem 14 (LDLR for spiked cumulant model) .Suppose that (ui)i=1,...,d are drawn i.i.d. from the
symmetric Rademacher distribution. If the non-Gaussian distribution pgsatisfies assumption 1, then
the following lower and upper bounds hold:
28100101
d100101102103104||Ln,d||2=5
=10
=12
=15
100101102103104
d100101102103lower bound on LDLR norm=2
=2.3
=2.5
=3
Figure 7: On the left, LR norm when g∼Rademacher (1/2)in the regime n=γdwithγ= 1.
When β < β γ≈10.7the likelihood ratio remains bounded, whereas it goes to +∞forβ > β γ. On
the right, the lower bound on ||L≤D(n)
n,d||given by 73 goes to +∞forθ >2. Parameters for the plot:
g∼Radem (1/2),β= 10 ,D(n) = log3/2(n)
•LetD∈Nsuch that D/4≤n, then:
L≤D
n,d2
≥⌊D/4⌋X
m=0n
md+ 1
2mβ2κg
4√
4!d2(1 +β)22m
. (73)
where κg
4is the fourth-order cumulant of g.
•Conversely, for any D, n, d :
L≤D
n,d2
≤1 +DX
m=1Cm
dmsup
k≤m
E[hk(g)]2m/kn
⌊m/4⌋d
⌊m/2⌋
(74)
where Cm:=
β
1+βm ⌊m/4⌋⌊m/2⌋+m−1
m
.
Corollary 15 (Asymptotics of LDLR bounds) .Assume the hypotheses of theorem 5. Let 0< ε < 1
and assume D(n)≍log1+ε(n). Take n, d→ ∞ , with the scaling n≍dθforθ >0. Estimate (73)
implies that for n, dlarge enough, the following lower bound holds:
L≤D(n)
n,d2
≥ 
1
⌊D(n)/4⌋β2κg
4
(1 +β)22n
d2!⌊D(n)/4⌋
Conversely, (74) leads to:
L≤D(n)
n,d2
≤1 +D(n)X
m=1Λ2β
1 +βm
m4mn
d2m/4
Taken together, (10) and(11) imply the presence of a critical regime for θc= 2, and describe the
behaviour ofL≤D
n,dfor all θ̸=θc
lim
n,d→∞L≤D(n)
n,d=1 0 < θ < 2
+∞θ >2
In the following we will present the proofs of 14 and 15. We first give a sketch of the proof in B.6.1
before giving the detailed proof in B.6.2.
B.6.1 Proof sketch
The starting point of the argument is the observation that since the null hypothesis is white Gaussian
noise, the space L2(Rnd,⟨·,·⟩)has an explicit orthogonal basis in the set of multivariate Hermite
29polynomials (Hα). The multi-index α∈Nnddenotes the degree of the Hermite polynomial which
is computed for each entry of the data matrix; see B.3 for a detailed explanation. We can then expand
the LDLR norm in this basis to write:
∥L≤D
n,d∥2=X
|α|≤D⟨L, Hα⟩2
α!=X
|α|≤D1
α!E
x∼P[Hα(x)]2(75)
From now on the two bounds are treated separately.
Lower bound The idea is to bound the LDLR from below by computing the sum 75 only over
a restricted set of addends Imwhich we can compute explicitly. In particular, we consider the set
Imof all the polynomials with degree 4min the data matrix, which are at most of order 4 in each
individual sample xµ. Then we use that the expectation of such Hermite polynomials conditioned on
ucan be split into mintegrals of degree-4 Hermite polynomials in dvariables. In this way we can
use our knowledge of the fourth-order cumulant of xto compute those expectations (see 93), so we
find that
∥L≤D
n,d∥2≥⌊D/4⌋X
mX
|α|∈Im1
α!E
x∼P[Hα(x)]2≥⌊D/4⌋X
mX
α∈Imβ2κg
4√
4!d2(1 +β)22m
. (76)
Thanks to this manipulation, the bound does not depend on αexplicitly, so we can complete the
bound by estimating the cardinality of Im. Thanks to the fact that we have ni.i.d. copies of variable
xat disposal, the class of polynomials Imhas size that grows at least as n
m d+1
2m, allowing to
reach the lower bound in the statement.
Upper bound We can show that the expectation of Hermite polynomials for a single sample
xµover the distribution P(·|u)can be written as Ex∼P(·|u)[Hα(x)] = T|α|,g/d|α|/2uα, where
T|α|,g=
β
1+β|α|/2
E
h|α|(g)
, cf. lemma 16. Using the fact that inputs are sampled i.i.d. from
P(·|u)and substituting this result into 75, we find that
∥L≤D
n,d∥2=X
|α|≤DQn
µ=1T|αµ|,g2
α!d|α|E
u∼P(u)[uα]2. (77)
To obtain an upper bound, we will first show that many addends in this sum are equal to zero, then
estimate the remainder.
On the one hand, since we chose a Rademacher prior over the elements of the spike ui, the expectation
over the prior yields either 1 or 0 depending on whether the exponent of each uiis even or odd. On
the other hand, the whitening of the data means that T|α|,g= 0for0<|α|<4(as proved in lemma
16).
For each m∈N, we can denote Amthe set of multi-indices |α|=mthat give non-zero contributions
in 77, so that we can write:
∥L≤D
n,d∥2=DX
m=0X
α∈AmQn
µ=1T|αµ|,g2
α!dm(78)
Now we proved that we can bound the inner terms so that they depend on αonly through the norm
|α|=m(cf. 82 and 110):
∥L≤D
n,d∥2≤DX
m=0X
α∈Am1
dmβ
1 +βm
sup
k≤m
E[hk(g)]2m/k
=DX
m=01
dmβ
1 +βm
sup
k≤m
E[hk(g)]2m/k
#Am(79)
30Finally, the cardinality of Amis 1 in case m= 0(which leads to the addend 1 in 74) and if m > 0it
can be bounded by (for details see 111 in the appendix)⌊m/4⌋⌊m/2⌋+m−1
mn
⌊m/4⌋d
⌊m/2⌋
, (80)
leading to (74).
B.6.2 Detailed proof
First we prove a lemma that provides formulas and estimates for projections of the sample-wise
likelihood ratio l(·|u)on the Hermite polynomials.
Lemma 16. Letx=Sp
β/dgu +z
be a spiked cumulant random variable. Then for any
α∈Nd, with|α|=m, we have that:
⟨l(·|u), Hα⟩=E
x∼P(·|u)[Hα(x)] =Tm,g
dm/2uα(81)
where Tm,gis a coefficient defined as:
Tm,g=β
1 +βm/2
E[hm(g)] (82)
Proof. Recall that by (57)
l(y|u) =E
g
p
1 +βexp
−1
2 
p
1 +βg−r
β
dy·u!2
+g2
2

. (83)
Note that this expression, thanks to 46 that bounds the integral, is differentiable infinitely many times
in the yvariable. It can be quickly proven by induction on |α|=m(using the recursive definition of
Hermite polynomials 33) that:
∂αl(y|u) =β
dm/2
uα·
·E
g
p
1 +β hm 
p
1 +βg−r
β
dy·u!
exp
−1
2 
p
1 +βg−r
β
dy·u!2
+g2
2

(84)
Hence we can again use 46 together with the fact that
sup
ghm 
p
1 +βg−r
β
dy·u!
exp
−1
2 
p
1 +βg−r
β
dy·u!2
<∞
to deduce that the hypothesis of lemma 11 are met for any α∈Nd, leading to:
⟨l(·|u), Hα⟩=E
x∼P(·|u)[Hα(x)] =E
y∼Q[∂αl(y|u)] (85)
This, and 84 already prove 81. Now to compute the exact value of Tm,gwe need to take the
expectation with respect to y∼Q=N(0, 1d×d). Note that by the choice of Rademacher prior
onu, we know that ||u||=√
d. So, conditioning on u,y·u√
d∼ N(0,1). Hence switching the
expectations in 84, we get:
Tm,g=βm/2E
gZ∞
−∞dz√
2πp
1 +β hmp
1 +βg−p
βz
e−1
2(√1+βg−√βz)2+g2−z2
2
=βm/2E
gZ∞
−∞dz√
2πp
1 +β hmp
1 +βg−p
βz
exp
−1
2p
βg−p
1 +βz2
˜z=√1+βz= βm/2E
g"Z∞
−∞d˜z√
2πhm 
p
1 +βg−s
β
1 +β˜z!
exp
−1
2p
βg−˜z2#
(86)
31Now we use 35 on
x=β√1 +βg−s
β
1 +β˜z
y=p
1 +βg−β√1 +βg=g√1 +β(87)
applying also the translation change of variable ˆz= ˜z−√β gwe get:
E
y∼Q[∂αl(y|u)] =β
dm/2
uαE
g
mX
k=0m
k
hkg√1 +β 
−s
β
1 +β!m−k
E
ˆz∼N(0,1)[ˆzm−k]

(88)
Now recall the formula for the moments of the standard Gaussian (see for instance section 3.9 in
[97]):
E
ˆz∼N(0,1)[ˆzm−k] =(m−k−1)!! ifm−kis even
0 ifm−kis odd(89)
Plugging this formula inside and changing the summation index 2j:=m−kwe get
Tm,g=βm/2⌊m/2⌋X
j=0m
2jβ
1 +βj
(2j−1)!!E
hm−2jg√1 +β
. (90)
Note that 36 with x=g√1+βandγ=√1 +βgives the following rewriting of hm(g):
hm(g) =⌊m/2⌋X
j=0(p
1−β)m−2j(β)jm
2j
(2j−1)!!hm−2jg√1 +β
(91)
which is almost the same expression as in 90. This allows simplify everything, leading to 82.
Note that lemma 16 together with 1 on gimply:
m= 2ormodd=⇒Tm,g= 0, (92)
So, apart from m= 0which gives the trivial contribution +1, the first non zero contributions to the
LDLR norm is at degree m= 4:
E
x∼P(·|u)[Hα(x)] =β2
(1 +β)2κg
4uα
d2(93)
From now on we will consider separately the lower and the upper bounds.
Lower bound The idea of the proof is to start from
||L≤D
n,d||2=X
α∈Nnd
|α|≤D⟨L, Hα⟩2
α!(94)
and to estimate it from below by picking only few terms in the sum. So we restrict to particular sets
of multi-multi-indices for which we can exploit 93. Let m∈Nsuch that 4m≤D, define
Im=n
α∈ 
Ndn|α|= 4m,|αµ| ∈ {0,4} ∀1≤µ≤no
(95)
Imis non empty since m < n and for each α∈ Imwe can enumerate all the indices µ1, . . . , µ m
such that αµi̸= 0.
Now we go on to compute the term ⟨L, Hα⟩forα∈ Im:
⟨L, Hα⟩=Z
UE
x∼P(·|u)⊗n[Hα(x)]dP(u)
=Z
UE
x∼P(·|u)⊗n"mY
i=1Hαµi(xµi)#
dP(u)(96)
32Since the samples are independent conditionally on u, we can split the inner expectation along the m
contributing directions:
⟨L, Hα⟩=Z
UmY
i=1E
x∼P(·|u)[Hαµi(xµi)]dP(u) (97)
Now we need to compute the inner d-dimensional expectation. For that we use that |αµi|= 4, so,
recalling the notation η=q
β
1+β, we can apply 93 to get for each i
E
x∼P(·|u)[Hαµi(xµi)] =η4
d2κg
4uαµi. (98)
So the resulting integral can be written in the following way:
⟨L, Hα⟩=η4κg
4
d2mZ
UdY
j=1uγj
jdP(u) (99)
where for each j∈ {1, . . . , d },γj:=P
µαµ
j. So we also have thatP
jγj= 4m.
Now we take the expectation with respect to P(u), and use the fact that the components are i.i.d.
Rademacher so the result depends on the parity of the γjin the following way:
⟨L, Hα⟩=(
η4κg
4
d2m
if all(γj)j=1,...,n are even
0 if there is at least one γjwhich is an odd number(100)
Hence, if we restrict to the set:
˜Im=(
α∈ Im∀j∈ {1, . . . , d }nX
µ=1αµ
jis even)
(101)
We have that all the indices belonging to ˜Imgive the same contribution:
⟨L, Hα⟩2=η4
d2κg
42m
(102)
Also, note that inside Im,α!≤(4!)m, so get the following estimates
||L≤D
n,d||2=X
|α|≤D⟨L, Hα⟩2
α!
≥⌊D/4⌋X
m=0X
α∈˜Im⟨L, Hα⟩2
α!
≥⌊D/4⌋X
m=0X
α∈˜Imη4κg
4√
4!d22m
≥⌊D/4⌋X
m=0#˜Imη4κg
4√
4!d22m(103)
Now we just need to estimate the cardinality of ˜Im. A lower bound can be provided by considering
ˆIm=
α∈ Im∀µ∈[n]∀j∈[d]αµ
jis even	
(104)
Clearly ˆIm⊆˜Imand#ˆIm= n
m d+1
2mbecause first we can pick the n−mrows of αthat will
be0∈Nd, which can be done in n
m
. Then, for each non-zero row, we need to pick two columns
33(with repetitions) in which to place a 2 and leave all the other entries as 0, that can be done in d+1
2m
ways.
So plugging the lower bound in the previous estimate, we reach the inequality that we wanted to
prove:
||L≤D
n,d||2≥⌊D/4⌋X
m=0n
md+ 1
2mη4κg
4√
4!d22m
=⌊D/4⌋X
m=0n
md+ 1
2mβ2κg
4√
4!d2(1 +β)22m
(105)
Upper bound We start from (75) using the following rewriting:
||L≤D
n,d||2=X
|α|≤D⟨L, Hα⟩2
α!
=X
|α|≤DE
x∼P
Hα(x)2
α!
=X
|α|≤D1
α!E
u∼P(u)"nY
µ=1E
xµ∼P(·|u)[Hαµ(xµ)]#2(106)
Now use 16 and plug in the formulas for the inner expectations.
||L≤D
n,d||2=X
|α|≤D1
α!E
u∼P(u)"nY
µ=1T|αµ|,g
d|αµ|/2uαµ#2
=X
|α|≤DQn
µ=1T|αµ|,g2
α!d|α|E
u∼P(u)[uα]2(107)
Now we use our prior assumption that uii.i.d.∼Rad(1/2). Note that odd moments of uiare equal to 0
and even moments are equal to 1, so (denoting by χA(·)the indicator function of set A):
E
u∼P(u)[uα] =dY
i=1E
ui∼Rademacher (1/2)h
uPn
µ=1αµ
i
ii
=χ{Pn
µ=1αµ
iis even for all i}(α) (108)
Now the key point of the proof: this last formula, together with (92), implies that most of the addends
in the sum in (107) are zero. Set |α|=m, then the set of multi-indices that could give a non-zero
contribution is
Am=(
α∈Nn×d|α|=m,∀µ∈[n],αµ= 0or|αµ|>4,and∀i∈[d]nX
µ=1αµ
iis even)
(109)
Using this fact together with (82) we get:
||L≤D
n,d||2=DX
m=0X
α∈AmQn
µ=1T|αµ|,g2
α!d|α|
≤DX
m=0X
α∈Am1
dmnY
µ=1 β
1 +β|αµ|
E
h|αµ|(g)2!
≤DX
m=0X
α∈Am1
dmβ
1 +βm
sup
k≤m
E[hk(g)]2m/k
=DX
m=01
dmβ
1 +βm
sup
k≤m
E[hk(g)]2m/k
#Am(110)
34In the third step we used the following inequality
Y
µE
h|αµ|(g)2≤Y
µsup
k≤m
E[hk(g)]1/k2|αµ|
= sup
k≤mE[hk(g)]2m/k
That holds since α∈ AmhenceP
µ|αµ|=m.
It is hard to compute exactly the cardinality of Am, but we can estimate it by considering the inclusion
Am⊆˜Amdefined as
˜Am=n
α∈Nn×d|α|=m,there are at mostjm
4k
non-zero rows andjm
2k
non-zero columnso
(111)
Assume now m > 0(the case m= 0can be treated separately, leading to the addend +1 in (74)). To
compute the cardinality of ˜Amwe just have to multiply the different contributions
• There are n
⌊m/4⌋
possibilities for the non-zero rows
• There are d
⌊m/2⌋
possibilities for the non-zero columns
•once restricted to a ⌊m/4⌋ × ⌊ m/2⌋we have to place the units to get to norm m. It
is the counting problem of placing munits inside the ⌊m/4⌋⌊m/2⌋matrix entries. The
possibilities are⌊m/4⌋⌊m/2⌋+m−1
m
(112)
So we get the following estimate for the cardinality of Am
#Am≤n
⌊m/4⌋d
⌊m/2⌋⌊m/4⌋⌊m/2⌋+m−1
m
(113)
Plugging into (110) we reach the final formula (74), which completes the proof.
Proof of corollary 15 Now we turn to the proof of 15 on the asymptotic behavior of the bound
n
m
=nm
m!+O(nm−1) (114)
d+ 1
2m
≥d2m
2m(115)
So we have that:
||L≤D(n)
n,d||2≥⌊D/4⌋X
m=0nm
m!+O(nm−1)d2m
2m+O(1)β2κg
4√
4!d2(1 +β)22m
=
⌊D/4⌋X
m=01
m!β2κg
4√
2√
4!(1 + β)22mn
d2m
+O X
mnm−1
m!d2mβ2κg
4√
2√
4!(1 + β)22m!
(116)
Then we lower-bound the sum by considering just the last term.
||L≤D(n)
n,d||2≥1
⌊D/4⌋!β2κg
4√
2√
4!(1 + β)22⌊D/4⌋n
d2⌊D/4⌋
+o 
1
⌊D/4⌋!Cn
d2⌊D/4⌋!
(117)
So by using k!≤kkon⌊D/4⌋and picking n, dlarge enough so that numerical constants and the
o(. . .)become negligible for the estimate, we get (10).
Plugging in the scaling n≍dθandD(n)≍log1+ε(n)it is clear that what decides the behaviour of
the sequence is the termn
d2.
•If0< θ≤2,n
d2→0and so (10) does not provide information on the divergence of the
LDLR norm.
35•Ifθ >2it isn
d2→ ∞ faster that the logarithmic term at denominator, so the right hand side
in (10) diverges at ∞, proving the second regime in (12)
Now let us turn to proving (11). We are in the regime m≤D << min(n, d), hence we can use the
following estimates of binomial coefficients and factorial
n
⌊m/4⌋
≤nm/4
d
⌊m/2⌋
≤dm/2
⌊m/4⌋⌊m/2⌋+m−1
m
≤ 
m2/8m
m!≤mm(118)
on 74 to get
||L≤D
n,d||2≤1 +DX
m=1β
1 +βm
m2msup
k≤m
E[hk(g)]2m/kn
d2m/4
(119)
Now we use estimate the term depending on the Hermite coefficients of g, using the assumption 5:
sup
k≤m
E[hk(g)]2m/k
≤sup
k≤m
(Λkk!)2m/k
≤Λ2msup
k≤m 
k2m
≤Λ2mm2m(120)
Plugging into the estimate for the LDLR we get (11)
||L≤D
n,d||2≤1 +DX
m=1Λ2β
1 +βm
m4mn
d2m/4
(121)
By letting n≍dθ, forθ >0andD(n)≍log1+ε(n), we can see that the bound goes to 1 when
θ <2, proving the first regime in (12).
B.7 Limitations of our theoretical analysis
The main limitation of the theoretical portion of our work is that it relies on the assumption that
the null hypothesis is standard i.i.d. Gaussian noise. Since this assumption is central to the large
body of work analysing hypothesis tests [37], a very interesting future direction is to develop tools
for analysing the case of a different null hypothesis. On the technical side, while the assumption of
Rademacher prior on uis not essential and could be easily generalized to other isotropic distributions,
assumption 1 is a key requirement to carry out the proofs. We discuss the limitations of the random
feature analysis, and in particular the Gaussian equivalence theorem [79–83], at the end of section 4.1.
As is generally the case in high-dimensional statistics, our results do not constitute a complete
proof of the existence of a statistical-to-computational gap: our argument relies on the low-degree
conjecture 4, and the divergence of the norm of the likelihood ratio is only a necessary condition
forstrong distinguishability . In fact, there are no techniques at the moment that can prove that
average-case problems require super-polynomial time in the hard phase, even if we assume P̸=NP
[37]. So our work should be seen as providing rigorous evidence for a statistical-to-computational
gap in the spiked cumulant model.
C Details on the replica analysis
We analytically compute the generalisation error of a random feature model trained on the Gaussian
mixture classification task of B.2.3 by exploiting the Gaussian equivalence theorem for dealing with
structured data distributions [79–83]. In particular, we use equations derived by Loureiro et al. [78]
360.00.20.4test error
Theory
Hidden Manifold
T eacher-Student
Gaussian Mixture
0.00.20.4test error
0 1 2 3 4 5
n/d0.00.20.4test error
0 1 2 3 4 5
n/d2
Figure 8: Linear and quadratic sample regimes for different synthetic data models. (Right )
Generalization error of the hidden manifold model (top), the teacher-student setup (center) and a
mixture of two Gaussians as a function of the ratio of the number of samples and the input dimension.
(Left) Same except that the number of samples scales with the square of the input dimension. The solid
black line corresponds to the replica theory prediction while the coloured dots display the outcome of
the numerical simulations averaged over 10different seeds. In all the panels, d= 1000 andd= 20
for linear and quadratic sample regimes respectively, λ= 0.01for both the teacher–student setup
and the hidden manifold model while λ= 0.1for Gaussian mixtures. In the Gaussian mixture case,
µ±= (±1,0, ...,0)∈Rdwhile the covariance matrices are both isotropic and, in particular, both
equal to the identity matrix: Σ±=I.
that describe the generalisation error of random features on Gaussian mixture classification using the
replica method of statistical physics. The equations describe the high-dimensional limit where n, d,
and the size of the hidden layer p→ ∞ while their ratios stay finite, α=n/d andγ=d/p∼O(1).
In this regime, the generalisation error is a function of only scalar quantities, i.e.
ϵg= 1−ρ+Eξ
sign 
m⋆
++p
q⋆
+ξ+b⋆
−
+ρ−Eξ
sign 
m⋆
−+p
q⋆
−ξ+b⋆
, (122)
where ξ∼ N (0,1),ρ±defines the fraction of data points belonging to class y=±1while m⋆
±,q⋆
±
andb⋆are the so-called overlap parameters and bias term respectively. Their value for every size of
the training set can be determined by solving the following optimisation problem [78]:
fβ= extr
{qk,mk,Vk,ˆqk,ˆmk,ˆVk,b}k=+,−
X
k=+,−−1
2
ˆqkVk−qkˆVk
+mkˆmk+lim
p→∞1
pΨs+αγΨe
.
(123)
where the entropic potential Ψscan be expressed as a function of the means µ±∈Rpand covariances
Σ±∈Rp×pof the hidden-layer activations of the random features model, while the energetic potential
Ψedepends on the specific choice of the loss function ℓ(·).
Solving the optimization problem in 123, leads to a set of coupled saddle-point equations for
the overlap parameters and the bias term. This set of equations is nothing but a special case of
the equations already derived in [78], that is high-dimensional classification of a mixture of two
Gaussians with random features, except for the typo in the self-consistency equation of the bias term
37b= X
k=±ρk
Vk!−1X
k=±ρkEξ[ηk−mk], (124)
withηkbeing the extremizer
ηk=argmin
λ" 
λ−√qkξ−mk−b2
2V++ℓ(yk, λ)#
. (125)
andℓ(·)being any convex loss function. Analogously to the more general setting of [78], the
Gaussian Equivalence Theorem allows to express the entropic potential Ψsas a function of the means
µk∈Rpand covariances Σk∈Rp×pof the random features
Ψs=lim
p→∞1
p(ˆq+µ++ ˆq−µ−)t
λIp+ˆV+Σ++ˆV−Σ−−1
(ˆq+µ++ ˆq−µ−) +
+lim
p→∞1
pTr
(ˆq+Σ++ ˆq−Σ−)
λIp+ˆV+Σ++ˆV−Σ−−1
;(126)
while the energetic potential Ψedepends on the specific choice of the loss function ℓ(·)
Ψe=−1
2Eξ"X
k=±ρk 
ηk−√qkξ−mk−b2
2Vk+ℓ(yk, ηk)#
. (127)
As discussed in 4.1, the Gaussian Equivalence Theorem breaks in the quadratic sample regime if the
data are sampled from the spiked Wishart model. Interestingly, this is not the case for the Hidden
Manifold model. Indeed, as shown in fig. 8, we still get a perfect match between the Gaussian theory
and the numerical simulations even in the quadratic sample regime. The theoretical prediction can
be easily derived by rescaling the free-energy associated to the Hidden Manifold model as in [80]
by a factor 1/d2. This is the same trick already proposed in [84] for teacher–student settings and
displayed in the middle panel of fig. 8.
38NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer:[Yes]
Justification: The bullet points at the end of introduction detail thoroughly the main claims
of the paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitations of our results by clearly stating the assumptions
of our theorems, in particular in assumption 1. We discuss general limitations and how to
overcome them in the concluding perspectives.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an import-
ant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
39Answer:[Yes]
Justification: All the assumptions for the theorems are in the main text. In appendix B we
present all the proofs together with further discussions of the assumptions.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We will provide the code; we state parameters of the simulations below the
figures.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
40Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We will provide a GitHub repository in the introduction in the de-anonymised
version.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The essential details are provided in the caption of figures, while addtional
details can be found in appendix A.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We provide error bars and explain how they are computed in appendix A.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confid-
ence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
41•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: There are no particular computational requirements for the simulations. They
can all be carried out in few hours on standard computers.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer:[Yes]
Justification:
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [No]
Justification: This paper presents work whose goal is to advance the field of Machine
Learning. There are many potential societal consequences of our work, none which we feel
must be specifically highlighted here
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
42•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification:
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
43•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
44