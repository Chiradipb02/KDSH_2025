Generative Subspace Adversarial Active Learning for
Outlier Detection in Multiple Views of
High-dimensional Tabular Data
Anonymous Author(s)
Affiliation
Address
email
Abstract
Outlier detection in high-dimensional tabular data is an important task in data min- 1
ing, essential for many downstream tasks and applications. Existing unsupervised 2
outlier detection algorithms face one or more problems, including inlier assumption 3
(IA), curse of dimensionality (CD), and multiple views (MV). To address these 4
issues, we introduce Generative Subspace Adversarial Active Learning (GSAAL), 5
a novel approach that uses a Generative Adversarial Network with multiple ad- 6
versaries. These adversaries learn the marginal class probability functions over 7
different data subspaces, while a single generator in the full space models the entire 8
distribution of the inlier class. GSAAL is specifically designed to address the MV 9
limitation while also handling the IA and CD, making it the only method to address 10
all three. We provide a mathematical formulation of MV , theoretical guarantees 11
for the training, and scalability analysis for GSAAL. Our extensive experiments 12
demonstrate the effectiveness and scalability of GSAAL, highlighting its superior 13
performance compared to other popular OD methods, especially in MV scenarios. 14
1 Introduction 15
Outlier detection (OD), a fundamental and widely recognized issue in data mining, involves the 16
identification of anomalous or deviating data points within a dataset. Outliers are typically defined 17
as low-probability occurrences within a population [ 41,19]. In the absence of access to the true 18
probability distribution of the data points, OD algorithms rely on constructing a scoring function. 19
Points with higher scores are more likely to be outliers. Existing unsupervised OD algorithms have 20
one or more of the following problems, in high-dimensional tabular data scenarios. 21
•The inlier assumption (IA): OD algorithms often make assumptions about what constitutes 22
an inlier, which can be challenging to verify and validate [30]. 23
•The curse of dimensionality (CD): As the dimensionality of data increases, the challenge of 24
identifying outliers intensifies, decreasing the effectiveness of certain OD algorithms [2] 25
•Multiple Views (MV): Outliers are often only visible in certain "views" of the data and are 26
hidden in the full space of original features [31] 27
We now explain these problems one by one. 28
The inlier assumption poses a challenge to algorithms that assume a standard profile of the inlier 29
data. For example, angle-based algorithms like ABOD [ 24] assume that inliers have other inliers 30
at all angles. Similarly, neighbor-based algorithms like kNN [ 34] assume that inliers have other 31
neighboring points nearby. These assumptions influence the scoring as it measures the degree to 32
which a sample deviates from this assumed norm. Consequently, the performance of these algorithms 33
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Figure 1: Scatterplots of the dataset from example 1.
may degrade if these assumptions do not hold [ 30]. This means that a general OD method should not 34
make any inlier assumptions. 35
The curse of dimensionality [2] refers to the decrease in the relative proximity of data points as the 36
number of dimensions increases. Simply put, with high dimensionality, the distance between any pair 37
of points becomes similar, regardless of whether none, one, or both of the points in a pair are outliers. 38
This is particularly problematic for OD algorithms that rely on distances or on identifying neighbors 39
to detect outliers, such as density- (e.g., LOF [ 3]), neighbor- (e.g., kNN [ 34]), and cluster-based (e.g., 40
SVDD [1, Chapter 2]) OD algorithms. 41
Multiple Views refers to the phenomenon that certain complex correlations between features are only 42
observable in some feature subspaces [ 31]. As detailed in [ 1], this occurs when the dataset contains 43
additional irrelevant features, making some outliers only detectable in certain subspaces. In scenarios 44
where multiple subspaces contain different interesting structures, this problem is exacerbated. It then 45
becomes increasingly difficult to explain the variability of a data point based solely on its behavior in 46
a single subspace [ 23]. This problem can occur regardless of the dimensionality of the dataset if the 47
number of points is insufficient to capture a complex correlation structure. 48
The following example illustrates the three problems described above 49
Example 1 (Effect of MV , IA and CD) .Consider the random variables x1,x2andx3, where x1and 50
x2are highly correlated and x3is Gaussian noise. Figure 1 plots datasets with 20, 100 and 1000 51
realizations of (x1,x2,x3). It also contains the classification boundaries from both a locality-based 52
method (green) and a cluster-based method (red) in the subspace. The cluster-based detector fitted in 53
the full 3D space fails to detect the outlier shown in the figure (red cross). However, the outlier is 54
always detected in the 2D subspace, as we can see. Once we increase the number of samples over 55
n= 1000 , the cluster-based method detects the outlier in the full space (MV). On the contrary, the 56
locality-based method could not detect the outlier in any tested scenario (MV + IA). If we increase 57
the dimensionality by adding more features consisting of noise, no method can detect the outlier in 58
the full space (MV + IA + CD). 59
We are interested in tackling outlier detection whenever a population exhibits MV , like [ 31,23,25] 60
and as showcased in [ 1]. Particularly, the goal of this paper is to propose the first outlier detection 61
method that explicitly addresses IA, CD, and MV simultaneously. 62
As we will explain in the next section, we build on Generative Adversarial Active Learning 63
(GAAL) [ 44], a widely used approach for outlier detection [ 30,17,39]. It involves training a 64
Generative Adversarial Network (GAN) to mimic the distribution of outlier data, and it enhances 65
the discriminator’s performance through active learning [ 38], leveraging the GAN’s data generation 66
capability. GAAL methods avoid IA [ 30] and use the multi-layered structure of the GAN to overcome 67
the curse of dimensionality [33]. However, they often miss important subspaces, leading to MV . 68
Challenges. Training multiple GAN-based models in individual subspaces is not trivial. (1) The 69
joint training of generators and discriminators in GANs requires careful monitoring to determine 70
the optimal stopping point, a task that becomes daunting for large ensembles. (2) The generation of 71
difficult-to-detect points in a subspace remains hard [ 40]. (3) While several authors have proposed 72
2Table 1: Families of OD methods with the limitations they address.
Type IA CD MV
Classical ✗ ✗ ✗
Subspace ✗✓ ✓
Generative w/ uniform distribution ✓ ✗ ✗
Generative w/ param. distribution ✗✓ ✗
Generative w/ subspace behavior ✗✓ ✓
GAAL ✓ ✓ ✗
GSAAL (Our method) ✓ ✓ ✓
multi-adversarial architectures for GANs [ 11,5], none of them address adversaries tailored to 73
subspaces composed of feature subsets. Furthermore, these methods may not be suitable for GAAL 74
since they do not have convergence guarantees for detectors, as we will explain. 75
Contributions. (1) We propose GSAAL (Generative Subspace Adversarial Active Learning), a 76
novel GAAL method that uses multiple adversaries to learn the marginal inlier probability functions 77
in different data subspaces. Each adversary focuses on a single subspace. Simultaneously, we train 78
a single generator in the full space to approximate the entire distribution of the inlier class. All 79
networks are trained end-to-end, avoiding the ensembling problem. (2) To our knowledge, we give 80
the first mathematical formulation of the “multiple views” problem. We used it to show the ability of 81
GSAAL to mitigate the MV problem. (3) We formulate the novel optimization problem for GSAAL 82
and give convergence guarantees of each discriminator to the marginal distribution of its respective 83
subspace. We also analyze the worst-case complexity of the method. (4) In extensive experiments we 84
compare GSAAL with multiple competitors. GSAAL was the only method capable of consistently 85
detecting anomalous data under MV . Furthermore, on 22 popular benchmark datasets for the one-class 86
classification task, GSAAL demonstrated SOTA-level performance and was orders of magnitude 87
faster in inference than its best competitors. (5) Our code is publicly available.188
Paper outline: Section 2 reviews related work, Section 3 contains the theoretical results for our method, 89
Section 4 features our experimental results, and Section 5 concludes and addresses limitations. 90
2 Related Work 91
This section is a brief overview of popular unsupervised outlier detection methods for tabular data 92
related to our approach. We categorize them based on their ability to address the specific limitations 93
outlined above. Table 1 is a comparative summary. Further comments about OD in other data types 94
can be found in the appendix. 95
Classical Methods Conventional outlier detection approaches, such as distance-based strategies 96
like LOF and KNN, angle-based techniques like ABOD, and cluster-based methods like SVDD, 97
rely on specific assumptions on the behavior of inlier data. They use a scoring function to measure 98
deviations from this assumed norm. These methods face the inlier assumption limitation by definition. 99
For example, local methods that assume isolated outliers fail when several outlying samples fall 100
together. In addition, many classical methods, which rely on measuring distances, are susceptible to 101
thecurse of dimensionality . Both limitations impair the effectiveness of these methods [30]. 102
Subspace Methods Subspace-based methods [ 25] operate in lower-dimensional subspaces formed 103
by subsets of features. They effectively counteract the curse of dimensionality by focusing on 104
identifying so-called “subspace outliers” [ 22]. These outliers, which are prevalent in high-dimensional 105
datasets with many correlated features, are often elusive to conventional non-subspace methods [ 29, 106
31]. However, existing subspace methods inherently operate on specific assumptions on the nature of 107
anomalies in each subspace they explore, and thus face the inlier assumption limitation. 108
Generative Methods A common strategy to mitigate the IA and CD limitations is to reframe the 109
task as a classification task using self-supervision. A prevalent self-supervised technique, particularly 110
1https://anonymous.4open.science/r/GSAAL-8D6E
3for tabular data, is the generation of artificial outliers [ 13,30]. This method involves distinguishing 111
between actual training data and artificially generated data drawn from a predetermined “reference 112
distribution”. [ 21] showed that by approximating the class probability of being a real sample, one 113
approximates the probability function of being an inlier. One then uses this approximation as a 114
scoring function [ 30]. However, it is not easy to find the right reference distribution, and a poor 115
choice can affect OD by much [21]. 116
A first approach to this challenge proposed the use of naïve reference distributions by uniformly 117
generating data in the space. This approach showed promising results in low-dimensional spaces but 118
failed in high dimensions due to the curse of dimensionality [ 21]. Other approaches, such as assuming 119
parametric distributions for inlier data [ 1, Chapter 2] or directly generating in susbpaces [ 12], can 120
avoid CD when the parametric assumptions are met. Methods that generate in the subspaces can 121
model the subspace behavior, additionally tackling the MV limitation. However, these last two 122
approaches do not address the IA limitation, as they make specific assumptions about the behavior of 123
the inlier data. 124
Generative Adversarial Active Learning According to [ 21], the closer the reference distribution 125
is to the inlier distribution, the better the final approximation to the inlier probability function will 126
be. Hence, recent developments in generative methods have focused on learning the reference 127
distribution in conjunction with the classifier. A key approach is the use of Generative Adversarial 128
Networks (GANs), where the generator converges to the inlier distribution [ 15]. The most common 129
approaches for this are GAAL-based methods [ 30,17,39]. These methods differentiate themselves 130
from other GANs for OD by training the detectors using active learning after normal convergence of 131
the GAN [ 36,10]. The architecture of GAAL inherently addresses the curse of dimensionality, as 132
GANs can incorporate layers designed to manage high-dimensional data [ 33]. In practice, GAAL- 133
based methods outperformed all their competitors in their original work. However, they overlook the 134
behavior of the data in subspaces and therefore may be susceptible to MV . 135
Our method, GSAAL, incorporates several subspace-focused detectors into GAAL. These detectors 136
approximate the marginal inlier probability functions of their subspaces. Thus, GSAAL effectively 137
addresses MV while inheriting GAAL’s ability to overcome IA and CD limitations. 138
3 Our Method: GSAAL 139
We first formalize the notion of data exhibiting multiple views. We then use it to design our 140
outlier detection method, GSAAL, and give convergence guarantees. Finally, we derive the runtime 141
complexity of GSAAL. All the proofs and extra derivations can be found in the technical appendix. 142
3.1 Multiple Views 143
Several authors [ 1,31,23,25,29] have observed that at times the variability of the data can only be 144
explained from its behavior in some subspaces. Researchers variably call this problem “the subspace 145
problem” [ 1,25] or “multiple views of the data” [ 22,31]. Previous research has largely focused on 146
practical scenarios, leaving aside the need for a formal definition. In response, we propose a unifying 147
definition of “multiple views” that provides a foundation for developing methods to address this 148
challenge effectively. 149
The problem “multiple views” of data (MV) arises from two different effects. First, it involves the 150
ability to understand the behavior of a random vector xby examining lower-dimensional subsets of 151
its components (x1, . . . ,xd). Second, it stems from the challenge of insufficient data to obtain an 152
effective scoring function in the full space of x. As Example 1 shows, combining these two effects 153
obscures the behavior of the data in the full space. Hence, methods not considering subspaces when 154
building their scoring function may have issues detecting outliers under MV . The next definition 155
formalizes the first effect. 156
Definition 1 (myopic distribution) .Consider a random vector x: Ω−→RdandDiagd×d({0,1}), 157
the set of diagonal binary matrices without the identity. If there exists a random matrix u: Ω−→ 158
Diagd×d({0,1}), such that 159
px(x) =pux(ux)for almost all x, (1)
we say that the distribution of xismyopic to the views of u. Here, xanduxare realizations of x 160
andux, and pxandpuxare the pdfs of xandux. 161
4It is clear that, under MV , using puxto build a scoring function instead of pxmitigates the effects. 162
This comes as the subspaces selected by uare smaller in dimensionality. Hence it should take fewer 163
samples to approximate the pdf of ux. The difficulty is that it is not yet clear how to approximate 164
pux. The following proposition elaborates on a way to do so. It states that by averaging a collection 165
of marginal distributions of xin the subspaces given by realizations of u, one can approximate the 166
distribution of pux. 167
Proposition 1. Letxandube as before with pxmyopic to the views of u. Consider a set of 168
independent realizations of u:{ui}k
i=1. Then1
kP
ipuix(uix)is an unbiased statistic for pux(ux). 169
MV appears when there is a lack of data, and its distribution is myopic. To improve OD under MV , 170
one can exploit the distribution myopicity to model xin the subspaces, where less data is sufficient. 171
Proposition 1 gives us a way to do so, by approximating pux. In this way, under myopicity, this also 172
approximates px, avoiding MV . Our method, GSAAL, exploits these derivations, as we explain next. 173
3.2 GSAAL 174
GAAL methods tackle IA by being agnostic to outlier definition and mitigate CD through the use of 175
multilayer neural networks [30, 28, 33]. GAAL methods have two steps: 176
1.Training of the GAN . Train the GAN consisting of one generator Gand one detector Dusing 177
the usual min-max optimization problem as in [15]. 178
2.Training of the detector through active learning . After convergence, Gis fixed, and D 179
continues to train. This last step is an active learning procedure with [ 44]. Following [ 21], 180
D(x)now approximates the pdf of the training data px. 181
After Step 2, the detector converges to px. However, our goal is to approximate pxby exploiting 182
a supposed myopicity of the distribution. We extend GAAL methods to also address MV in what 183
follows. The following theorem adapts the objective function of the GAN to the subspace case and 184
gives guarantees that the detectors converge to the marginal pdfs used in Proposition 1: 185
Theorem 1. Consider xanduas in the previous definition, with xa realization of xand{ui}ia set 186
of realizations of u. Consider a generator G:z∈Z7−→ G (z)∈Rdand{Di},i= 1, . . . , k , a set 187
of detectors such as Di:uix∈Si⊂Rd7−→ D i(uix)∈[0,1].Zis an arbitrary noise space where 188
Grandomly samples from. Consider the following optimization problem 189
min
Gmax
Di,∀iX
iV(G,Di) =
min
Gmax
Di,∀iX
iEuixlogDi(uix) +Ezlog (1− Di(uiG(z))),(2)
where each addend V(G,Di)is the binary cross entropy in each subspace. Under these conditions, 190
the following holds: 191
i)Each detector in optimum is D∗
i(uix) =1
2,∀x. Thus, in optimum V(G,Di) =−log(4) ,∀i. 192
ii)Each individual Diconverges to D∗
i(uix) =puix(uix)after trained in Step 2 of a GAAL 193
method. 194
iii)D∗(x) =1
kPk
i=1D∗
i(uix)approximates pux(ux). Ifpxis myopic, D∗(x)also approxi- 195
mates px(x). 196
Using Theorem 1 we can extend the GAAL methods to the subspace case: 197
1.Training the GAN . Train a GAN with one generator Gand multiple detectors {Di}with 198
Equation (2) as the objective function. The training of each detector stops when the loss 199
reaches its value with the optimum in Statement (i). 200
2.Training of the kdetectors by active learning . Train each Dias in Step 2 of a regular GAAL 201
method using G. By Statement (ii)of the Theorem, each Diwill approximate puix. By 202
Statement (iii),D(x) =1
kPk
i=1Di(uix)will approximate pxunder the myopicity of the 203
data. 204
We call this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL). 205
The appendix contains the pseudo-code for GSAAL. 206
53.3 Complexity 207
In this section, we focus on studying the theoretical complexity of GSAAL. We study both its usability 208
for training and, more importantly, for inference. 209
Theorem 2. Consider our GSAAL method with generator Gand detectors {Di}k
i=1, each with four 210
fully connected hidden layers,√nnodes in the detectors and din the generator. Let Dbe the training 211
data for GSAAL, with ndata points and dfeatures. Then the following holds: 212
i)Time complexity of training is O(ED·n·(k·n+d2)).EDis an unknown complexity 213
variable depicting the unique epochs to convergence for the network in dataset D. 214
ii)Time complexity of single sample inference is in O(k·n), with kthe number of detectors 215
used. 216
The linear inference times make GSAAL particularly appealing in situations where the model can be 217
trained once for each dataset, like one-class classification. We build on this particular strength in the 218
following section. 219
4 Experiments 220
This section presents experiments with GSAAL. We will outline the experimental setting, and examine 221
the handling of “multiple views” in GSAAL and other OD methods. We then evaluate GSAAL’s 222
performance against various OD methods and investigate its scalability. The appendix includes a 223
study on the sensitivity to the number of detectors, IA experiments, an ablaition study and extra 224
competitors evaluated in the real world datasets. System specifications are included in the appendix. 225
4.1 Experimental Setting 226
This section has three parts: First, we describe the synthetic and real data for the outlier detection 227
experiments. Then, we describe the configuration of GSAAL. Finally, we present our competitors. 228
4.1.1 Datasets 229
Synthetic. We constructed synthetic datasets, each containing two correlated features, x1andx2, 230
along with 58 independent features xj,j= 3, . . . , 60consisting of Gaussian noise. This approach 231
simulates datasets that exhibit the MV property by adding irrelevant features into a pair of highly 232
correlated variables. We detail the methodology and all correlation patterns in the technical appendix. 233
Real. We selected 22 real-world tabular datasets for our experiments from [ 19]. The selection 234
criteria included datasets with less than 10,000 data points, more than 10 outliers, and more than 15 235
features, focusing on high-dimensional data while keeping the runtime (of competing OD methods) 236
tractable. Table 2a contains the summary of the datasets. For datasets with multiple versions, we chose 237
the first in alphanumeric order. Details about each dataset are available in the original source [19]. 238
4.1.2 Network Settings 239
Structure. Unless stated otherwise, GSAAL uses the following network architecture. It consists of 240
four fully connected layers with ReLu activation functions used in the generator and the detectors. 241
Each layer in k= 2√
ddetectors has√nnodes, where nanddare the number of data points 242
and features in the training set, respectively. This configuration ensures linear inference time. The 243
generator has dnodes in each layer, a standard in GAAL approaches, which ensures polynomial 244
training times. We assumed uto be distributed uniformly across all subspaces. Therefore, we 245
obtained each subspace for the detectors by drawing uniformly from the set of all subspaces. 246
Training. Like other GAAL methods [ 30,44], we train the generator Gtogether with all the 247
detectors Diuntil the loss of Gstabilizes. Then we train each detector Diuntil convergence with 248
Gfixed. To automate this process, we introduce an early stopping criterion: Training stops when a 249
detector’s loss approaches the theoretical optimum ( −log(4) ), see statement (ii)of Theorem 1. For 250
consistency across experiments, training parameters remain fixed unless otherwise noted. Specifically, 251
6Table 2: Real-world datasets and Competitors
(a) Real-world datasets converted to tabular if needed
Dataset Category Dataset Category
20news Text MNIST Image
Annthyroid Health MVTec Text
Arrhythmia Cardiology Optdigits Image
Cardiot.. Cardiology Satellite Astronomy
CIFAR10 Image Satimage-2 Astronomy
F-MNIST Image SpamBase Document
Fault Industrial Speech Linguistics
InternetAds Image SVHN Image
Ionosphere Weather Waveform Elect. Eng.
Landsat Astronomy WPBC Oncology
Letter Image Hepatitis Health(b) Competitors
Type Competitors
ClassicalkNN, LOF
ABOD, OCSVM w/ rbf
Subspace IForest, SOD
Gen., uniform dist. NA (see the text)
Gen., parametric dist. GMM
Gen., subspace behavior NA (see the text)
GAAL MO-GAAL
the learning rates of the detectors and the generator are 0.01 and 0.001, respectively. We use minibatch 252
gradient descent [14] optimization, with a batch size of 500. 253
4.1.3 Competitors 254
We selected popular and accessible methods from each category, as summarized in Table 2b, guided 255
by related work. We excluded generative methods with uniform distributions because they prove 256
ineffective for large datasets [ 21]. We could not include a generative method with subspace behavior 257
due to operational issues with the most relevant method in this class, [ 12], caused by its outdated 258
repository. We used the recommended parameters for all methods, as usual in OD [19]. 259
We used the pyod [43] library to access all competitors except MO-GAAL. We used MO-GAAL 260
from its original source and implemented our method GSAAL in keras [6]. 261
4.2 Effect of Multiple Views on Outlier Detection 262
To demonstrate the effectiveness of GSAAL under MV , we use synthetic datasets. Visualizing the 263
outlier scoring function in a 60-dimensional space is challenging, so we project it into the x1-x2 264
subspace. A method adept at handling MV should have a boundary that accurately reflects the x1and 265
x2dependency structure. We first generate a synthetic dataset Dsynthas described in section 4.1.1 266
and train the OD model. Using this model, we compute the scores for the points (x1, x2,0, . . . , 0) 267
and visualize the level curves on the x1-x2plane. 268
Figure 2 shows results for selected datasets and competitors, which are detailed in the Appendix. It 269
shows the level curves and decision boundaries (dashed lines) of the methods. Notably, our model 270
effectively detects correlations in the right subspace. To quantify this, we generated outliers in the 271
subspace of interest and extra inliers. We tested the one-class classification performance of each 272
method in 10 different MV datasets. On average, GSAAL managed to obtain 0.70 AUC, while the 273
second-best performer (IForest) did not surpass a random classifier —0.49 AUC. All results and 274
further details can be found in section B.2 in the appendix. 275
4.3 One-class Classification 276
This section evaluates GSAAL on a one-class classification task [ 37]. First, we study the effectiveness 277
of GSAAL on real data. Then, we investigate the scalability of GSAAL in practical scenarios. 278
4.3.1 Real-world Performance 279
We perform the outlier detection experiments on real datasets. Specifically, we take on the task of 280
one-class classification, where the goal is to detect outliers by training only on a collection of inliers 281
[19]. To evaluate the performance of OD methods, we use AUC as it is robust to test data imbalance, 282
a common issue in OD tasks . The procedure is as follows: 283
7Figure 2: GSAAL finds classification boundaries for datasets banana and star under MV .
Table 3: Results of the Conover-Iman test for pairwise comparisons of the rankings.
Method ABOD GSAAL GMM IForest KNN LOF MO GAAL OCSVM SOD
ABOD = ++ ++ ++ ++ ++
GSAAL = ++ ++ + ++ ++ ++
GMM – – – – = ++ – – – – ++ ++
IForest – – – – – – = – – ++ ++
KNN ++ ++ = ++ ++
LOF – ++ = ++ + ++
MO GAAL – – – – – – – – – – = ++
OCSVM – – – – – – – = ++
SOD – – – – – – – – – – – – – – – – =
1.Split the dataset Dinto a training set Dtraincontaining 80% of the inliers from D, and a test 284
setDtestcontaining the remaining inliers and all outliers. 285
2.Train an outlier detection model with Dtrainand evaluate its performance on Dtestwith ROC 286
AUC. 287
To save space, we moved the detailed AUC results to the appendix; showing that GSAAL obtained 288
the lowest median rank —see Figure 10 in the appendix. Although other subspace methods tend to 289
perform better with irrelevant attributes [ 29,25], they did not outperform classical OD methods on 290
average in our experiments. Notably, ABOD, the second-best method in our experiments, performed 291
poorly in the MV tests (Section 4.2). 292
For statistical comparisons, we use the Conover-Iman post hoc test for pairwise comparisons be- 293
tween multiple populations [ 7]. It is superior to the Nemenyi test due to its improved type I error 294
boundings [ 8]. Conover-Iman test requires a preliminary positive result from a multiple population 295
comparison test, for which we employ the Kruskal-Wallis test [26]. 296
Table 3 shows the test results. In each cell, ‘ +’ indicates that the method in the row has a significantly 297
lower median rank than the method in the column, while ‘ −’ indicates a significantly higher median 298
rank. One symbol indicates p-values ≤0.15and two symbols indicate p-values ≤0.05. A blank 299
indicates no significant difference. The table shows that GSAAL is superior to most of its competitors. 300
Our method does not significantly outperform the classical methods ABOD and kNN. However, these 301
methods struggle to detect structures in subspaces, showing their inadequacy in dealing with the MV 302
limitation, see Section 4.2. 303
Overall, the results support GSAAL’s superiority in outlier detection tasks involving multiple views. 304
Additionally, they establish our method as the leading GAAL option for One-class classification 305
4.3.2 Scalability 306
In section 3.3, we derived that the inference time of GSAAL scales linearly with the number of 307
training points if the number of detectors kis fixed, while it does not depend on the number of 308
features d. This is in contrast to other methods, in particular LOF, KNN, and ABOD, which have 309
quadratic runtimes in d[3, 24]. We now validate this experimentally. The procedure is as follows: 310
8(a)
 (b)
Figure 3: Plots of different performance metrics for scalability
1. Generate datasets DtrainandDtestconsisting of random points. |Dtest|= 106. 311
2. Train an OD method using Dtrainand record the inference time over Dtest. 312
Following the result of the sensitivity study in our appendix, we fixed k= 30 . Figure 3a plots the 313
inference time of a single data point as a function of the number of features when |Dtrain|= 500 . 314
Figure 3b plots the inference time as a function of the number of points in Dtrain, for a fixed number of 315
100 features. Both figures confirm our complexity derivations and show that GSAAL is particularly 316
well-suited for large datasets. 317
5 Limitations & Conclusions 318
5.1 Limitations and Future Work 319
In section 4 we randomly selected subspaces for training the detectors in GSAAL, i.e. we took 320
a uniform distribution of u. This was already sufficient to demonstrate the highly competitive 321
performance of our method. In practice, this assumption seemed to perform well for our experiments. 322
However, GSAAL can work with any subspace search strategy to obtain the distribution of u, for 323
example, the methods exploiting multiple views [ 23,22]. We have not included them in this paper 324
due to the lack of an official implementation. In the future, we plan to benchmark various subspace 325
search methods in GSAAL. 326
Next, GSAAL is limited to tabular data, since the “multiple views” problem has only been observed 327
for this data type. The mathematical formulation of MV in section 3 does not exclude unstructured 328
data. The difficulty lies in identifying good search strategies for ufor non-tabular data, which remains 329
an open question [ 18]. However, depending on the type of unstructured data, extending GSAAL to 330
work with it is not immediate. Therefore, building a method that exploits the theoretical derivations 331
of GSAAL for structured data is future work. 332
5.2 Conclusions 333
Unsupervised outlier detection (OD) methods rely on a scoring function to distinguish inliers from 334
outliers, since the true probability function that generated the dataset is usually unavailable in practice. 335
However, they face one or more of the following problems — Inlier Assumption (IA), Curse of 336
Dimensionality (CD), or Multiple Views (MV). In this article, we have proposed the first mathematical 337
formulation of MV , which allows for a better understanding of how to solve this occurrence. Using 338
this formulation, we developed GSAAL, which is the first OD approach that solves MV , CD, and IA. 339
In short, GSAAL is a generative adversarial network with a generator and multiple detectors fitted in 340
the subspaces to find outliers not visible in the full space. In our experiments on 27 different datasets, 341
we demonstrated the usefulness of GSAAL, in particular, its ability to deal with MV and its superior 342
performance on OD tasks with real datasets. In addition, we have shown that GSAAL can scale up to 343
deal with high-dimensional data, which is not the case for our most competent competitors. These 344
results confirm GSAAL’s ability to deal with data exhibiting MV and its usability in any practical 345
scenario involving large datasets. 346
References 347
[1] C. C. Aggarwal. Outlier Analysis . Springer International Publishing, Cham, 2017. 348
9[2]R. Bellman. Dynamic programming. Princeton, New Jersey: Princeton University Press. XXV, 349
342 p. (1957)., 1957. 350
[3]M. M. Breunig, H. Kriegel, R. T. Ng, and J. Sander. LOF: identifying density-based local 351
outliers. In SIGMOD Conference , pages 93–104. ACM, 2000. 352
[4]G. O. Campos, A. Zimek, J. Sander, R. J. G. B. Campello, B. Micenková, E. Schubert, I. Assent, 353
and M. E. Houle. On the evaluation of unsupervised outlier detection: measures, datasets, and 354
an empirical study. Data Mining and Knowledge Discovery , 30(4):891–927, Jul 2016. 355
[5]J. Choi and B. Han. Mcl-gan: Generative adversarial networks with multiple specialized 356
discriminators. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, 357
editors, Advances in Neural Information Processing Systems , volume 35, pages 29597–29609. 358
Curran Associates, Inc., 2022. 359
[6] F. Chollet et al. Keras. https://keras.io , 2015. 360
[7]W. Conover and R. Iman. Multiple-comparisons procedures. informal report. Technical report, 361
Los Alamos National Laboratory (LANL), Feb. 1979. 362
[8]W. J. W. J. Conover. Practical nonparametric statistics / W.J. Conover. Wiley series in 363
probability and statistics. Applied probability and statistics section. Wiley, New York ;, third 364
edition. edition, 1999. 365
[9]J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional 366
transformers for language understanding. In North American Chapter of the Association for 367
Computational Linguistics , 2019. 368
[10] J. Donahue, P. Krähenbühl, and T. Darrell. Adversarial feature learning. In International 369
Conference on Learning Representations , 2017. 370
[11] I. Durugkar, I. M. Gemp, and S. Mahadevan. Generative multi-adversarial networks. ArXiv , 371
abs/1611.01673, 2016. 372
[12] C. Désir, S. Bernard, C. Petitjean, and L. Heutte. One class random forests. Pattern Recognition , 373
46(12):3490–3506, 2013. 374
[13] R. El-Yaniv and M. Nisenson. Optimal single-class classification strategies. In B. Schölkopf, 375
J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems , volume 19. 376
MIT Press, 2006. 377
[14] I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning . MIT Press, 2016. http: 378
//www.deeplearningbook.org . 379
[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and 380
Y . Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, 381
and K. Weinberger, editors, Advances in Neural Information Processing Systems , volume 27. 382
Curran Associates, Inc., 2014. 383
[16] A. Goodge, B. Hooi, S.-K. Ng, and W. S. Ng. Lunar: Unifying local outlier detection methods 384
via graph neural networks. ArXiv , abs/2112.05355, 2021. 385
[17] J. Guo, Z. Pang, M. Bai, P. Xie, and Y . Chen. Dual generative adversarial active learning. 386
Applied Intelligence , 51(8):5953–5964, Aug 2021. 387
[18] N. Gupta, D. Eswaran, N. Shah, L. Akoglu, and C. Faloutsos. Lookout on time-evolving graphs: 388
Succinctly explaining anomalies from any detector, 2017. 389
[19] S. Han, X. Hu, H. Huang, M. Jiang, and Y . Zhao. Adbench: Anomaly detection benchmark. In 390
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in 391
Neural Information Processing Systems , volume 35, pages 32142–32159. Curran Associates, 392
Inc., 2022. 393
[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE 394
Conference on Computer Vision and Pattern Recognition (CVPR) , pages 770–778, 2015. 395
10[21] K. Hempstalk, E. Frank, and I. H. Witten. One-class classification by combining density and 396
class probability estimation. In W. Daelemans, B. Goethals, and K. Morik, editors, Machine 397
Learning and Knowledge Discovery in Databases , pages 505–519, Berlin, Heidelberg, 2008. 398
Springer Berlin Heidelberg. 399
[22] F. Keller, E. Muller, and K. Bohm. Hics: High contrast subspaces for density-based outlier 400
ranking. In 2012 IEEE 28th International Conference on Data Engineering , pages 1037–1048, 401
2012. 402
[23] F. Keller, E. Müller, A. Wixler, and K. Böhm. Flexible and adaptive subspace search for 403
outlier analysis. In Proceedings of the 22nd ACM International Conference on Information & 404
Knowledge Management , CIKM ’13, page 1381–1390, New York, NY , USA, 2013. Association 405
for Computing Machinery. 406
[24] H. Kriegel, M. Schubert, and A. Zimek. Angle-based outlier detection in high-dimensional data. 407
InKDD , pages 444–452. ACM, 2008. 408
[25] H.-P. Kriegel, P. Kröger, E. Schubert, and A. Zimek. Outlier detection in axis-parallel subspaces 409
of high dimensional data. In T. Theeramunkong, B. Kijsirikul, N. Cercone, and T.-B. Ho, editors, 410
Advances in Knowledge Discovery and Data Mining , pages 831–838, Berlin, Heidelberg, 2009. 411
Springer Berlin Heidelberg. 412
[26] W. H. Kruskal. A nonparametric test for the several sample problem. The Annals of Mathemati- 413
cal Statistics , 23(4):525–540, 1952. 414
[27] Y . LeCun, Y . Bengio, and G. Hinton. Deep learning. Nature , 521(7553):436–444, May 2015. 415
[28] C.-L. Li, W.-C. Chang, Y . Cheng, Y . Yang, and B. Poczos. Mmd gan: Towards deeper 416
understanding of moment matching network. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, 417
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing 418
Systems , volume 30. Curran Associates, Inc., 2017. 419
[29] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation forest. In 2008 Eighth IEEE International 420
Conference on Data Mining , pages 413–422, 2008. 421
[30] Y . Liu, Z. Li, C. Zhou, Y . Jiang, J. Sun, M. Wang, and X. He. Generative adversarial active 422
learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data 423
Engineering , 32(8):1517–1528, 2020. 424
[31] E. Müller, I. Assent, P. Iglesias, Y . Mülle, and K. Böhm. Outlier ranking via subspace analysis 425
in multiple views of the data. In 2012 IEEE 12th International Conference on Data Mining , 426
pages 529–538, 2012. 427
[32] B. Perozzi, L. Akoglu, P. Iglesias Sánchez, and E. Müller. Focused clustering and outlier 428
detection in large attributed graphs. In Proceedings of the 20th ACM SIGKDD International 429
Conference on Knowledge Discovery and Data Mining , KDD ’14, page 1346–1355, New York, 430
NY , USA, 2014. Association for Computing Machinery. 431
[33] T. Poggio, A. Banburski, and Q. Liao. Theoretical issues in deep networks. Proceedings of the 432
National Academy of Sciences , 117(48):30039–30045, 2020. 433
[34] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient algorithms for mining outliers from large 434
data sets. In Proceedings of the 2000 ACM SIGMOD International Conference on Management 435
of Data , SIGMOD ’00, page 427–438, New York, NY , USA, 2000. Association for Computing 436
Machinery. 437
[35] L. Ruff, R. Vandermeulen, N. Goernitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and 438
M. Kloft. Deep one-class classification. In J. Dy and A. Krause, editors, Proceedings of the 439
35th International Conference on Machine Learning , volume 80 of Proceedings of Machine 440
Learning Research , pages 4393–4402. PMLR, 10–15 Jul 2018. 441
[36] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised 442
anomaly detection with generative adversarial networks to guide marker discovery. In M. Ni- 443
ethammer, M. Styner, S. Aylward, H. Zhu, I. Oguz, P.-T. Yap, and D. Shen, editors, Information 444
Processing in Medical Imaging , pages 146–157, Cham, 2017. Springer International Publishing. 445
11[37] N. Seliya, A. Abdollah Zadeh, and T. M. Khoshgoftaar. A literature review on one-class 446
classification and its potential applications in big data. Journal of Big Data , 8(1):122, Sep 2021. 447
[38] B. Settles. Active learning literature survey. 2009. 448
[39] S. Sinha, S. Ebrahimi, and T. Darrell. Variational adversarial active learning. In Proceedings of 449
the IEEE/CVF International Conference on Computer Vision , pages 5972–5981, 2019. 450
[40] G. Steinbuss and K. Böhm. Hiding outliers in high-dimensional data spaces. International 451
Journal of Data Science and Analytics , 4(3):173–189, Nov 2017. 452
[41] H. Wang, M. J. Bah, and M. Hammad. Progress in outlier detection techniques: A survey. IEEE 453
Access , 7:107964–108000, 2019. 454
[42] H. Xu, G. Pang, Y . Wang, and Y . Wang. Deep isolation forest for anomaly detection. IEEE 455
Transactions on Knowledge and Data Engineering , 35(12):12591–12604, 2023. 456
[43] Y . Zhao, Z. Nasrullah, and Z. Li. Pyod: A python toolbox for scalable outlier detection. Journal 457
of Machine Learning Research , 20(96):1–7, 2019. 458
[44] J.-J. Zhu and J. Bento. Generative adversarial active learning. arXiv preprint arXiv:1702.07956 , 459
2017. 460
12A Theoretical Appendix 461
In this appendix, we will include all the proofs of the included theorems and propositions. Addition- 462
ally, we also extend all non-experimental sections with relevant information for the experimental 463
appendix. 464
A.1 Previous Remarks 465
Before starting to prove our main results, it is important to add a remark about our notation in this 466
article. Whenever we denote ux, we mean the operation resulting in the following vector: u(ω)x(ω). 467
Thus, uxis a random vector following its distribution pux. However, it is important to remark that 468
ux, and therefore, also uix, does not state the usual matrix-vector multiplication. What we mean by 469
uxis the operation U×Mx, where Ustands for the range-complete version of uand×Mthe usual 470
matrix multiplication. This means that whenever we write uxwe are considering the projection of x 471
into the subspace of the features selected in u. This means that uixis the random vector composed 472
of the features selected by ui, and therefore, puix(uix)denotes subsequent marginal pdf of x. We 473
do not state this in the main text as it functionally does not change anything of our derivations, and 474
simply works as a notation. The only important remarks stemming from this fact are the following: 475
1.px(uix) =px(πui(x)), where πuidenotes the projection of a point xinto the subspace of 476
ui. Therefore, we can write px(uix) =puix(uix). 477
2.The operator as stated before is not distributive. This is trivial, as given ua random matrix as 478
in definition 1, ( 1d−u)xis defined properly, as 1d−u∈Diag ({0,1}). However, x−ux 479
denotes the vector subtraction between two vectors with different dimensionality. 480
While not important to understand the following proofs and the derivations from the main text, 481
understanding this is crucial for anyone seeking to work with these definitions. 482
A.2 Proofs 483
We will reformulate all of the statements for completion before introducing each proof. 484
Proposition 2. Letxandube as before with pxmyopic to the views of u. Consider a set of 485
independent realizations of u:{ui}k
i=1, a realization of x,x, and a realization of ux,ux. Then 486
1
kP
ipuix(uix)is a statistic for pux(ux). 487
Proof. Consider xanduas in the statement. Recall the law of total probabilities: 488
pux(ux) =Eu 
pux|u=u′(ux|u′)
.
By taking the definition of uand the myopicity, it is trivial that: 489
pux|u=u′(ux|u′) =pu′x(u′x)
foru′such that pu(u′)̸= 0. 490
Then, by definition of marginal probability and expectation, we have that: 491
pux(ux) =NX
i=1pu(ui)puix(uix),
asuis discrete with finite set of occurrences of size N. Thus, we can approximate 492PN
i=1pu(ui)puix(uix))by1
kP
ipuixwithuiindependent samples of u. 493
Theorem 3. Consider xanduas in the previous definition, with xa realization of xand{ui}ia set 494
of realizations of u. Consider a generator G:z∈Z7−→ G (z)∈Rdand{Di},i= 1, . . . , k , a set 495
of detectors such as Di:uix∈Si⊂Rd7−→ D i(uix)∈[0,1].Zis an arbitrary noise space where 496
Grandomly samples from. Consider the following objective function 497
min
Gmax
Di,∀iX
iV(G,Di) =
min
Gmax
Di,∀iX
iEuixlogDi(uix) +Ezlog (1− Di(uiG(z)))(3)
Under these conditions, the following holds: 498
13i)Each detector’s loss in optimum is V(G,D∗
i) =1
2. 499
ii)Each individual Diconverges to D∗
i(uix) =puix(uix)after trained in Step 2 of a GAAL 500
method. 501
iii)D∗(x) =1
kPk
i=1D∗
i(uix)approximates pux(ux). Ifpxis myopic, D∗(x)also approxi- 502
mates px(x). 503
Proof. This proof will follow mainly the results in [ 15], adapted for our case. We will first derivative 504
two general results that we are going to use to immediately prove (i),(ii)and(iii). First, consider 505
the objective function 506
X
iV(G,Di) =X
iEuix∼puixlog(Di(uix))+
Ez∼pz(1−log(Di(uiG(z)))),
where zis the random vector used by Gto sample from the noise space Z. We will write Ex,Ezand 507
Euixinstead of Ex∼px,Ez∼pzandEuix∼puixas an abuse of notation. 508
The problem is, then, to optimize: 509
min
Gmax
Di,∀iX
iV(G,Di). (4)
Fixing Gand maximizing for all Di, each detector individually maximizes V(G,Di). Let us try to 510
obtain the optimal of each Diwith a fixed G. First, we write: 511
V(G,Di) =Z
uixpuix(uix) logDi(uix)duix+
Z
zpz(z) log(1 − Di(uiG(z)))dz.
AsGuseszto sample from its sample distribution pG(x), we can rewrite the second addent, like in 512
[15], as: 513
V(G,Di) =Z
uixpuix(uix) logDi(uix)duix+
Z
uixpG(uix) log(1 − Di(uix))duix.
Aggregating both integrals, we have a function of the type f(t) =alog(t) +blog(1−t), with 514
a, b∈R−{0}. We know that f(t)obtains its optimum in t=a
a+b. Asf(t)∈R+,V(G,Di)obtains 515
its optimum for a given Gin: 516
D∗
i(uix) =puix(uix)
puix(uix) +pG(uix). (5)
Let us now consider the following function 517
C(G) =X
imax
Di,∀iV(G,Di)
=X
iEuixlogpuix(uix)
puix(uix) +pG(uix)+
Euix∼pGlogpG(uix)
puix(uix) +pG(uix).(6)
This is known in Game Theory as the cost function of player “ G” in the null-sum game defined by 518
themin max optimization problem. [ 15] refers to it as the virtual training criterion of the GAN. The 519
adversarial game defined by (4) reaches an equilibrium (and thus, the min max problem an optimum) 520
whenever C(G)is minimized. We will study the value of Gin such equilibrium and use it, together 521
with (5), to prove the statements. 522
14Rewriting C(G)it is clear that: 523
C(G) =X
iKL
puix(uix)∥puix(uix) +pG(uix)
2
+KL
pG(uix)∥puix(uix) +pG(uix)
2
.
This expression corresponds to that of a sum of multiple binary cross entropies between a population 524
coming from puixand from pGprojected by ui. Therefore, as we know, we can rewrite: 525
C(G) =X
i2JSD (puix(uix)∥pG(uix)),
withJSD the Jensen-Shannon divergence. Since JSD (s∥r)∈[0,log(2)) , it is clear that C(G) 526
obtains its minimum only whenever 527
pG(uix) =puix(uix),∀∀x2; (7)
and for all i∈ {1, . . . , k }. 528
Knowing GandDiin the optimum for all i, we can prove the statements above: 529
(i) AspG(uix) =puix(uix)for almost all x, in the optimum of (4), it is immediate that: 530
Di(uix) =1
2,
i.e., the detectors cannot differentiate between the real training data and the synthetic data of the 531
generator. If one employs the numerically stable version of each V(G,Di)(equivalent to the 532
numerically stable version of the binary cross entropy [6]), it is trivial to see that 533
Vstable(G,Di) = log(2) .
(ii) After optimizing (4), training each Diindividually with Gfixed, is the equivalent of building a 534
two-class classifier distinguishing between the artificial class generated by pG(uix) =puix(uix)and 535
the real data coming from puix(uix). By [21], the resulting two-class classifier would be such as: 536
Di(uix) =puix(uix).
(iii) By proposition 2 and statement (ii),1
kP
iD∗
i(uix)is an estimator for pux(ux). By myopicity, 537
it is also of px(x). 538
Theorem 4. Giving our GSAAL method with generator Gand detectors {Di}k
i=1, each with four 539
fully connected hidden layers,√nnodes in the detectors and din the generator, we obtain that: 540
i)The training time complexity is bounded with O(ED·n·(k·n+d2)), for a dataset Dwith 541
ntraining samples and dfeatures. EDis an unknown complexity variable depicting the 542
unique epochs to convergence for the network in dataset D. 543
ii)The single sample inference time complexity is bounded with O(k·n), with kthe number of 544
detectors used. 545
Proof. An evaluation of a neural network is composed of two steps, the backpropagation, and the 546
fowardpass steps. While training the network requires both, inference requires only a fowardpass. 547
Therefore, we will first prove (ii)and will build upon it to prove (i). 548
2For almost all x
15(ii). GSAAL consists of a generator and kdetectors. Single point inference consists of a single 549
fowardpass of all the detectors. We will first prove the general complexity of a fowardpass of a 550
general fully connected 4 layer network and will use it to derive all the other complexities. Let us 551
consider three weight matrices Wji,WhjandWlheach between two layers, with j, i, h andlbeing 552
the number of nodes in each. Therefore, Wjidenotes a matrix with jrows and icolumns, and so 553
on. Now, let us consider xi1the datapoint after passing the input layer. Lastly, without any loss of 554
generality, consider fto be the activation function for all layers. This way, the forward pass of a 555
single detector can be written as: 556
cl1=f(Wlhf(Whjf(Wjixi1))).
We will study the complexity in the first layer and use it to derive the complexity of the others. 557
Aj1=Wjixi1is a simple matrix-vector multiplication that we know to be O(j·i)atmost. Then, as 558
fis an activation function, f(Aj1)is equivalent to writing fj1⊙Aj1, with⊙being the element-wise 559
multiplication. Thus, f(Wjixi1)is: 560
O(j·i+j) =O(j·(i+ 1)) = O(j·i).
Doing this for all layers, we obtain: 561
O(l·h+k·j+j·i). (8)
As all layers have√nnodes, 562
O(3n) =O(n).
As we have kdetectors, the complexity for a fowardpass of all detectors, and thus, for a single sample 563
inference of GSAAL is: 564
O(k·n).
(i). A backpropagation step has the same complexity as an inference step on all training samples. 565
As we have ntraining samples, this then becomes 566
O(k·n2)
for the detectors. As the training consists of multiple epochs, we will write 567
O(ED·k·n2),
withEDbeing the number of epochs needed for convergence for the training data set D. As the 568
training consists of both backpropagation and fowardpass steps on all training samples, the total 569
training time complexity for all detectors is: 570
O(ED·k·n2+k·n2) =O(ED·k·n2).
As we also need to consider the generator, we will use equation 8 to derive both steps on the generator. 571
As the generator is also a fully connected 4-layer network, with all layers having dnodes, the 572
complexity for a single fowardpass is: 573
O(d2).
As during training one generates nsamples during each fowardpass: 574
O(n·d2).
Now, on each backpropagation pass the network calculates the backpropagation error for each 575
generated sample, thus, 576
O(n·d2)
is also the time complexity for the backpropagation step of the generator. Considering all EDepochs 577
and both backpropagation and fowardpass steps of the generator and all the detectors, the time 578
complexity of GSAAL’s training is: 579
O(ED·k·n2+ED·n·d2) =O(ED·n·(k·n+d2))
580
16Figure 4: Difference in statistical distance between two populations.
A.3 Related Work (extension) 581
Deep Outlier Detection for other data types. Outlier detection is also very popular in different 582
data types, especially in unstructured data [ 42,16,36,35,32]. Due to the complexity of the data they 583
are used for, deep methods are the main approach employed for this task. The main difference with 584
the other deep methods introduced for tabular data, is that the deep architecture in the later targets 585
mainly CD. For unstructured data types, like images or natural language, is the complexity of the data 586
that drives the architecture. For example, to treat image data, multiple linear layers do not suffice, 587
complex layers like convolutional or residual layers are employed for this [27]. 588
Although popular, most deep methods have limited to no use at all in tabula data in their original 589
articles. However, some have appeared in the literature of tabular data as competitors [ 36,35]. We 590
identified the most common for our task in related articles and benchmarks, and included them as an 591
extension of our main experiments in sections B.2 and B.3. 592
A.4 Multiple Views (extension) 593
In this section we extend the derivations in section 3.1 by providing an example of a myopic 594
distribution: 595
Example 2 (Myopic distribution) .Consider a xlike in example 1. Here, it is clear that x1,x2⊥x3. 596
Consider, then, usuch that: 597
u:{1} −→ { diag(1,1,0)}.
To test whether pxis myopic, we employed a simple test utilizing a statistical distance ( MMD with 598
the identity kernel) between pxandpux. This way, if ˆMMD (px∥pux) = 0 , it would be clear that the 599
equality holds. As a control measure, we also calculated the same distance for a different population 600
x′, where x3=x2
1. We have plotted the results in image 4, where Population 1 refers to xand 601
Population 2 to x′. As we can see, we do obtain a positive result in the test of myopicity for xand a 602
negative one for x′. 603
A.5 GSAAL (extension) 604
We now extend the results from section 3.2 by providing the pseudocode for the training of our 605
method. It is important to consider that, while theorem 3 formulates the optimization problem 606
in terms of the neural networks Gand{Di}i, in practice this will not be the case. Instead, we 607
will consider the optimization in terms of their weights, ΘGandΘDi. Therefore, in practice, the 608
convergence into an equilibrium will be limited by the capacity of the networks themselves [ 14]. 609
We considered the optimization to follow minibatch-stochastic gradient descent [ 14]. To consider 610
any other minibatch-gradient method it will suffice to perform the necessary transformations to the 611
gradients. 612
The pseudocode is located in Algorithm 1. As it is the training for the method, it takes both 613
the parameters for the method and the training. In this case, epochs refers to the total number 614
of epochs we will train in total, while stop_epoch marks the epoch where we start step 2 of the 615
GAAL training. Lines 1-3 initialize both the detectors in their subspaces and the generator with 616
17Algorithm 1 GSAAL training
Require: Data set D, Number of Discriminators κ,u,epochs ,stop_epoch
1:Initialize Generator G{#dis the dimensionality of D}
2:{ui}κ
i=1←DRAW FROMu(κ)
3:Initialize Discriminators {Di}κ
i=1with unique subspaces {ui}κ
i=1
4:forepoch ∈ {1, ..., epochs }do
5: forbatch∈ {1, ..., batches }do
6: noise←Random noise z(1), ..., z(m)fromZ
7: data←Draw current batch x(1), ..., x(m)
8: forj∈ {1...k}do
9: Update Djby ascending the stochastic gradient: ∇ΘDj1
mPm
i=1log(Dj(ujx(i))) +
log(1− Dj(ujG(z(i))))
10: end for
11: ifepoch < stop _epoch then
12: Update Gby descending the stochastic gradient: ∇ΘG1
kPk
j=11
mPm
i=1log(1−
Dj(G(z(i))))
13: end if
14: end for
15:end for
Table 4: Different outliers generated for the experiments.
Outlier Type Assumption Description Outlier Description M
LocalAssumes that all inliers are
located close to other inliersAs a result, outliers are
far away from inliersLOF
AngleAssumes that all inliers
have other inliers in all angles from their positionAs a result, outliers are
not surrounded by other pointsABOD
ClusterAssumes that all inliers
form large clusters of dataAs a result, outliers are
gathered in small clustersFn,µ+εi
random weight matrices ΘDiandΘG. Lines 4-13 correspond to the normal GAN training loop 617
across multiple epochs, referred to as step 1 of a GAAL method, if epoch < stop _epoch . Here 618
we proceed with training each detector and the generator using their gradients. Lines 8-10 update 619
each detector by ascending its stochastic gradient, while line 11 updates the generator by descending 620
its stochastic gradient. After the normal GAN training, we start the active learning loop [ 30] once 621
epoch ≥stop_epoch . The only difference with the regular GAN training is that Gremains fixed, i.e., 622
we do not descend using its gradient. This allows us to additionally train the detectors and, in case of 623
equilibrium of step 1, converge to the desired marginal distributions as derived in theorem 3. 624
B Experimental Appendix 625
In this section, we will include a supplementary experiment testing the IA condition for completion, 626
the sensibility experiments, and an ablation study. Additionally, we extended both main experimental 627
studies featured in the main text. All of the code for the extra experiments, as well as for all 628
experiments in the main text, can be found in our remote repository3. Our experiments used a RTX 629
3090 GPU and an AMD EPYC 7443p CPU running Python in Ubuntu 22.04.3 LTS. Deep neural 630
network methods were trained on the GPU and inferred on the CPU; shallow methods used only the 631
CPU. 632
B.1 Effects of Inlier Assumptions on Outlier Detection 633
GAAL methodologies are capable of dealing with the inlier assumption by learning the correct inlier 634
distribution pxwithout any assumption [ 30]. While this should also extend to our methodology, we 635
will study experimentally whether this condition holds in practice. To do so, as one cannot identify 636
3https://anonymous.4open.science/r/GSAAL-8D6E
18(a)
 (b)
 (c)
Figure 5: 2D-example of the different types of anomalies we generate using the method summarized
in table 4.
Figure 6: AUCs of the different methods in the IA experiments. From left to right: Local (blue),
Angle (orange) and Cluster (green).
beforehand whether a method is going to fail due to IA, we will generate synthetic datasets. This will 637
allow us to generate outliers that we know to follow from a specific IA, ensuring that failure comes 638
from the anomalies themselves. We will include all of the code in the code repository. To generate 639
the synthetic datasets we follow: 640
1. Generate D, a population of 2000 inliers following some distribution FinR20. 641
2.Select an outlier detection method Mwith some assumption about the normality of the data 642
and fit it using D. We will call such Mas the reference model for the generation. 643
3.Generate 400outliers by sampling on R20uniformly and keeping only those points osuch 644
thatM(o) = 1 (i.e., they are detected as outliers). We will write ODto refer to such a 645
collection of points. 646
4. Repeat step 3 10times, to obtain OD
1, . . . , OD
10. 647
5.Sample out 20% of the points in D. The remainder 80% will be stored in Dtrain, and the 648
other 20% inDtest
1, . . . , Dtest
10together with each OD
i. 649
These steps were repeated 4times with different F, to create 4different training sets and 40different 650
testing sets, corresponding to a total of 40different datasets employed per model Mselected in step 651
2. As we used 3different reference models, we have a total of 120different datasets employed in 652
this experiment alone. In particular, the models used for this are collected in table 4. The table 653
contains the name of the outlier type, the description of the IA taken to generate them, and a brief 654
description of how the outliers should look. Column Mcontains the method employed to generate 655
each, these being LOF ,ABOD , and the same inlier distribution as D, but with multiple shifted 656
means µiand with a significantly lower amount of points n. A visualization of how these outliers 657
would look with 2features is located in figure 5. To study how different methods behave when 658
detecting these outliers, we have performed the same experiments as in section 4.3, but with these 659
synthetic datasets. Figure 6 gathers all the AUCs of a method in 3boxplots, one for each outlier type 660
in each training set. Additionally, we grouped all based on the IA and assigned a similar color for 661
all of them. We have done this for the classical OD methods LOF, ABOD, and kNN, besides our 662
method GSAAL. We cropped the image below 0.45in the yaxis as we are not interested in results 663
below a random classifier. As we can see, classical methods seem to correctly detect outliers for 664
19an outlier type that verifies its IA. However, whenever we introduce outliers behaving outside of 665
their IA, the performance hit is significant. Notoriously, it appears that none of them had trouble 666
detecting the Local andAngle outlier type. regardless of their IA. This can be easily explained by 667
those outliers types being similar, as we can see in figure 5. On the other hand, GSAAL manages to 668
have a significant detection rate regardless of the outlier type. 669
B.2 Effects of Multiple Views on Outlier Detection (extension) 670
In this section, we will include a brief description of the generation process for the datasets used in 671
section 4.2. We will also perform the same experiment as in section 4.2 for all methods showcased in 672
the main text and additional datasets. The datasets were generated by the following formulas: 673
•Banana. Given θ∈[0, π]we have x= sin( θ) +U(0,0.1)andy= sin( θ)3+U(0,0.1). 674
•Spiral. Given θ∈[0,4π]andr∈(0,1), we have x=rcos(θ) +U(0,0.1)andy= 675
rsin(θ). 676
•Star. Given θ∈[0,2π]andr∈ {r∈R|r= sin(5 θ);r≥0,1,0.4},we have x=rcos(θ)+ 677
U(0,0.1)andy=rsin(θ) +U(0,0.1). 678
•Circle. Given θ∈[0,2π], we have x= cos( θ) +U(0,0.1)andy= sin( θ) +U(0,0.1). 679
•L.Given x1=N(0,0.1), x2=U(0,5), y1=U(−5,0),andy2=N(0,0.1); we have 680
x=concat (x1, x2)andy=concat (y1, y2). 681
We considered N(0,0.1)to denote a random normal realization with µ= 0 andσ2= 0.1, and 682
U(a, b)to denote a uniform realization in the [a, b]interval. 683
Figure 7 contains all images from the MV experiment. We employed the default parameters for all 684
methods in this experiments. We did that as those were the employed parameters in our real world 685
experiments. Additonally, the choice of parameter did not impact the outcome of the experiment 686
much. Our remote repository includes extra images for every competitor with multiple parameters 687
for comparison. We do not have any new insight beyond the ones exposed in the main article. Note 688
that we have included all methods but SOD. The reason was that SOD failed to execute for datasets 689
Star, Spiral, and Circle. 690
Additionally, we added competitors from outside of our related work that will later be used in section 691
B.3. In particular, we employed LUNAR, DIF and DeepSVDD with default parameters. We included 692
extra images in our remote repository with multiple parameters for the deep competitors as well. The 693
method AnoGAN was not included due to it failing in datasets Star, Spiral and Circle. Their results 694
can be seen in Figure 8. As it also happened our main competitors, some of the extra competitors were 695
capable of detecting the data structure in very sparse occasions. However they remained incapable to 696
properly describe a boundary consistently. The only method that was sensible enough in all datasets 697
was GSAAL. 698
In order to quantify this, we tested the ability of all methods to perform one-class classification in 699
each dataset. As outliers, we used white noise in the x1−x2subspace. Additionally, we created two 700
extra datasets greatly different from the rest, Xandwave : 701
•X.Given x1=x2=U(−1,1)andy1=x1+U(0,0.1), y2=x2+U(0,0.1); we have 702
x=concat (x1, x2)andy=concat (y1, y2).. 703
•Wave. Given θ∈[0,4π], we have x=θandy= sin( x) +U(0,0.1). 704
We will also use them as outleirs, for a total of 15 different datasets. We also generated extra inliers 705
in each test set. We gathered the AUC results in Figure 9. As we can see, all other methods struggel 706
to come ahead of the random classifier, marked with a dashed line. The only method well above that 707
is GSAAL. 708
B.3 One-class Classification (extension) 709
As we noted in Section 4, we obtained our benchmark datasets from [ 19], a benchmark study for 710
One-class classification methods in tabular data. Some of the datasets featured in the study, and 711
also in our experiments, were obtained from embedding image or text data using a pre-trained NN 712
20(a) Banana
(b) Spiral
(c) Star
(d) Circle
(e) L
Figure 7: Projected classification boundaries for the datasets in section 4.2 and the extra datasets.
(ResNet [ 20] and BERT [ 9], respectively). We shunt the interested reader into [ 19] for additional 713
information. Additionally, we found discrepancies between the versions of the datasets in the study 714
of [4] and [ 19]. We utilized the version of those datasets featured in [ 4] for our experiments due 715
to popularity. This affected the datasets Arrhythmia, Annthyroid, Cardiotocography, InternetAds, 716
Ionosphere, SpamBase, Waveform, WPBC andHepatitis . Figure 10 summarizes the ranks from the 717
one-class experiments in section 4.3. Table 5 summarizes the AUC results from our experiments. As 718
mentioned in section A.3, we also included extra methods outside of our related work. Particularly, 719
we added deep versions tailored to image data of previously included methods —DeepSVDD [ 35] 720
and Deep Isolation Forest [ 42] (DIF)— and others that extend some types of outlier detectors into 721
image and text data —LUNAR [ 16], as an extension of Locality-based classical methods, and 722
AnoGAN [ 36], as an extension of Generative methods. For their parameters, we employed the 723
recommended ones for LUNAR and DIF, and trained the models the same way that the authors did 724
in their articles. As for DeepSVDD and AnoGAN, as they do not have any recommended way of 725
training nor hyperparameters, we performed a grid search for their training parameters and kept the 726
best result. We used all of their official implementations4. All deep methods (including MO-GAAL 727
4LUNAR and DIF have official implementations by their authors in pyod [43].
21(a) Banana
(b) Spiral
(c) Star
(d) Circle
(e) L
Figure 8: Projected classification boundaries of the competitors outside of our related work.
Figure 9: AUC results in the MV datasets.
22GSAALLOFIForest ABODSOD kNN SVDDMO-GAALGMM123456789Ranks
Figure 10: Boxplots of the ranks used for the Conover-Iman experiment in section 4.3.
(a)
 (b)
Figure 11: Performance of the detector with different values of k.
and GSAAL) were trained multiple times with the same train set and their results were averaged to 728
account for initialization. 729
Additionally, we gathered all extra deep methods and performed the same statistical analysis as in 730
section 4.3. We also included MO GAAL besides GSAAL for completion. SO GAAL, the single 731
generator version of MO GAAL was not included, even if popular in the related literature. The 732
reason is that authors in [ 30] showed that MO GAAL constantly outperforms SO GAAL in the outlier 733
detection task. Results are included in table 6, gathered after a positive Kruskal-Wallis test. As we can 734
see, GSAAL outperform almost all competitors except LUNAR (the most recent method). However, 735
LUNAR is incapable to detect change in the subspaces as GSAAL does, see section B.2. Therefore, 736
regardless of considering the tabular related work, or the more generalist deep methods, GSAAL 737
still can outperform most competitors in the field. Additionally, for those that GSAAL performs 738
similar to, we showed that we are more sensible to changes in subspaces. This fact makes GSAAL 739
the preferred option for One-class classification under MV . 740
B.4 Parameter Sensibility 741
We now explore the effect of the number of detectors in GSAAL, k, by repeating the previous 742
experiments with varying k. Figure 11a plots the median AUC for different kvalues, showing a 743
stabilization at larger k. Next, Figure 11b compares the results with a fixed k= 30 and the default 744
value k= 2√
dused in the previous experiments; there is no large difference in either the AUC or the 745
ranks. We also found that the results in Table 3 remain almost the same if one sets k= 30 . So we 746
recommend fixing k= 30 , which makes GSAAL very suitable for high-dimensional data. 747
B.5 Ablation study 748
Lastly, we also performed an ablation study for GSAAL. We identify two critical components in our 749
method, the subspace nature of our detectors, and the multiple detectors used. Table 7 contains a 750
summary of the included features in each considered configuration. We will compare the performance 751
of all the different configurations of GSAAL. 752
23Table 5: AUC of all the methods tested in section 4.3 and extra methods.
Dataset GSAAL LOF IForest ABOD SOD KNN SVDD MO-GAAL GMM DeepSVDD AnoGAN DIF LUNAR
annthyroid 0,7681 0,6753 0,7094 0,7008 0,5243 0,6291 0,4611 0,5047 0,6932 0,872 0,4038 0,6228 0,8120
Arrhythmia 0,7532 0,7277 0,7695 0,7422 0,6514 0,7334 0,7442 0,6901 0,7296 0,7485 0,6133 0,7904 0,7412
Cardiotocography 0,8727 0,8038 0,7772 0,7956 0,3524 0,7733 0,8351 0,7912 0,7413 0,874 0,3248 0,5561 0,8219
CIFAR10 0,7862 0,7333 0,6853 0,7622 0,6607 0,7493 0,7074 0,6256 0,7462 0,6158 0,3705 0,6542 0,7612
FashionMNIST 0,8001 0,8995 0,8298 0,9009 0,7136 0,9179 0,8130 0,7930 0,9072 0,6981 0,7137 0,8336 0,9093
fault 0,6726 0,6436 0,6518 0,8019 0,5670 0,7849 0,5651 0,6821 0,6856 0,4972 0,4074 0,7240 0,8047
InternetAds 0,7809 0,8565 0,4739 0,8600 0,3663 0,8090 0,7063 0,7603 0,9113 0,8411 0,5165 0,4330 0,8036
Ionosphere 0,9593 0,9591 0,9377 0,9483 0,8250 0,9825 0,8379 0,9727 0,9644 0,967 0,8406 0,9159 0,9234
landsat 0,5217 0,7598 0,5927 0,7627 0,4821 0,7726 0,4792 0,4432 0,4998 0,69 0,4835 0,5579 0,7743
letter 0,6625 0,8888 0,6493 FA 0,7182 0,9066 0,9334 0,4828 0,8435 0,676 0,5257 0,6709 0,9450
mnist 0,7638 0,9484 0,8647 0,9189 0,4858 0,9318 FA 0,6151 0,9210 0,7604 0,2502 0,8540 0,9352
optdigits 0,8935 0,9991 0,8625 0,9846 0,4260 0,9983 0,9999 0,8105 0,8221 0,9086 0,6203 0,4751 0,9988
satellite 0,8630 0,8456 0,7834 FA 0,4745 0,8753 0,8740 FA 0,7957 0,7798 0,3099 0,7661 0,8517
satimage-2 0,9836 0,9966 0,9910 0,9977 0,6745 0,9992 0,9826 0,6317 0,9967 0,9755 0,3968 0,9987 0,9993
SpamBase 0,8717 0,7132 0,8374 0,7730 0,3774 0,7036 0,6302 0,7377 0,8034 0,7807 0,4826 0,4579 0,8244
speech 0,6029 0,5075 0,5030 0,8741 0,4364 0,4853 0,4640 0,5138 0,5217 0,6076 0,4821 0,4553 0,5070
SVHN 0,6859 0,7192 0,5834 0,6989 0,5781 0,6788 0,6150 0,7055 0,6684 0,5894 0,4621 0,6076 0,6319
Waveform 0,8092 0,7530 0,6902 0,7115 0,5814 0,7623 0,5514 0,6049 0,5791 0,7214 0,7018 0,7223 0,7570
WPBC 0,6326 0,5695 0,5681 0,6156 0,5333 0,5830 0,5681 0,5972 0,5660 0,4907 0,4121 0,3355 0,4872
Hepatitis 0,6982 0,5030 0,6568 0,5207 0,2959 0,5680 0,4024 FA 0,7574 0,8284 0,3787 0,3905 0,7219
MVTec-AD 0,9806 0,9679 0,9755 0,9689 0,9662 0,9703 0,9645 0,6412 0,9776 0,7422 0,5179 0,9689 0,9727
20newsgroups 0,5535 0,7854 0,6675 FA 0,7109 0,7260 0,6329 0,5313 0,8103 0,6063 0,4833 0,6715 0,7425
24Table 6: Results of the Conover-Iman test for all the Deep methods.
Method AnoGAN DIF DeepSVDD GSAAL LUNAR MO GAAL
AnoGAN = – – – – – – – – – –
DIF ++ = – – – – –
DeepSVDD ++ + = – – ++
GSAAL ++ ++ + = ++
LUNAR ++ ++ + = ++
MO GAAL ++ – – – – – – =
Table 7: Summary of the included components in the ablation study.
Name Subspace Multiple Di
GSAAL ✗✗ ✗ ✗
GSAAL ✓✗✓ ✗
GSAAL ✗✓ ✗ ✓
GSAAL ✓ ✓
We will employ, once again, the Conover-Iman test to compare the performance of all configuration 753
in a statistically sound way. Table 8 contains the results of the ablation experiment. As expected, our 754
fully configured method significantly outperformed all of the others. This further confirms that the 755
performance increase over our competitors comes directly from tackling the MV problem. 756
Table 8: Results of the Connover-Iman test for the ablation study.
GSAAL ✗✗ GSAAL ✓✗GSAAL ✗✓ GSAAL
GSAAL ✗✗ = ++ – – – –
GSAAL ✓✗ – – = – – – –
GSAAL ✗✓ ++ ++ = – –
GSAAL ++ ++ ++ =
25NeurIPS Paper Checklist 757
1.Claims 758
Question: Do the main claims made in the abstract and introduction accurately reflect the 759
paper’s contributions and scope? 760
Answer: [Yes] 761
Justification: sections 3 for the theoretical claims, 4.2 for the MV claims, and 4.3 for the 762
real world performance claims. 763
Guidelines: 764
•The answer NA means that the abstract and introduction do not include the claims 765
made in the paper. 766
•The abstract and/or introduction should clearly state the claims made, including the 767
contributions made in the paper and important assumptions and limitations. A No or 768
NA answer to this question will not be perceived well by the reviewers. 769
•The claims made should match theoretical and experimental results, and reflect how 770
much the results can be expected to generalize to other settings. 771
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 772
are not attained by the paper. 773
2.Limitations 774
Question: Does the paper discuss the limitations of the work performed by the authors? 775
Answer: [Yes] 776
Justification: Section 5. 777
Guidelines: 778
•The answer NA means that the paper has no limitation while the answer No means that 779
the paper has limitations, but those are not discussed in the paper. 780
• The authors are encouraged to create a separate "Limitations" section in their paper. 781
•The paper should point out any strong assumptions and how robust the results are to 782
violations of these assumptions (e.g., independence assumptions, noiseless settings, 783
model well-specification, asymptotic approximations only holding locally). The authors 784
should reflect on how these assumptions might be violated in practice and what the 785
implications would be. 786
•The authors should reflect on the scope of the claims made, e.g., if the approach was 787
only tested on a few datasets or with a few runs. In general, empirical results often 788
depend on implicit assumptions, which should be articulated. 789
•The authors should reflect on the factors that influence the performance of the approach. 790
For example, a facial recognition algorithm may perform poorly when image resolution 791
is low or images are taken in low lighting. Or a speech-to-text system might not be 792
used reliably to provide closed captions for online lectures because it fails to handle 793
technical jargon. 794
•The authors should discuss the computational efficiency of the proposed algorithms 795
and how they scale with dataset size. 796
•If applicable, the authors should discuss possible limitations of their approach to 797
address problems of privacy and fairness. 798
•While the authors might fear that complete honesty about limitations might be used by 799
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 800
limitations that aren’t acknowledged in the paper. The authors should use their best 801
judgment and recognize that individual actions in favor of transparency play an impor- 802
tant role in developing norms that preserve the integrity of the community. Reviewers 803
will be specifically instructed to not penalize honesty concerning limitations. 804
3.Theory Assumptions and Proofs 805
Question: For each theoretical result, does the paper provide the full set of assumptions and 806
a complete (and correct) proof? 807
Answer: [Yes] 808
26Justification: Section A. 809
Guidelines: 810
• The answer NA means that the paper does not include theoretical results. 811
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 812
referenced. 813
•All assumptions should be clearly stated or referenced in the statement of any theorems. 814
•The proofs can either appear in the main paper or the supplemental material, but if 815
they appear in the supplemental material, the authors are encouraged to provide a short 816
proof sketch to provide intuition. 817
•Inversely, any informal proof provided in the core of the paper should be complemented 818
by formal proofs provided in appendix or supplemental material. 819
• Theorems and Lemmas that the proof relies upon should be properly referenced. 820
4.Experimental Result Reproducibility 821
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 822
perimental results of the paper to the extent that it affects the main claims and/or conclusions 823
of the paper (regardless of whether the code and data are provided or not)? 824
Answer: [Yes] 825
Justification: Section 4 includes all details about our experimental setup (competitors, 826
datasets, experiments & training). Section A in the appendix includes the pseudo-code as 827
well 828
Guidelines: 829
• The answer NA means that the paper does not include experiments. 830
•If the paper includes experiments, a No answer to this question will not be perceived 831
well by the reviewers: Making the paper reproducible is important, regardless of 832
whether the code and data are provided or not. 833
•If the contribution is a dataset and/or model, the authors should describe the steps taken 834
to make their results reproducible or verifiable. 835
•Depending on the contribution, reproducibility can be accomplished in various ways. 836
For example, if the contribution is a novel architecture, describing the architecture fully 837
might suffice, or if the contribution is a specific model and empirical evaluation, it may 838
be necessary to either make it possible for others to replicate the model with the same 839
dataset, or provide access to the model. In general. releasing code and data is often 840
one good way to accomplish this, but reproducibility can also be provided via detailed 841
instructions for how to replicate the results, access to a hosted model (e.g., in the case 842
of a large language model), releasing of a model checkpoint, or other means that are 843
appropriate to the research performed. 844
•While NeurIPS does not require releasing code, the conference does require all submis- 845
sions to provide some reasonable avenue for reproducibility, which may depend on the 846
nature of the contribution. For example 847
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 848
to reproduce that algorithm. 849
(b)If the contribution is primarily a new model architecture, the paper should describe 850
the architecture clearly and fully. 851
(c)If the contribution is a new model (e.g., a large language model), then there should 852
either be a way to access this model for reproducing the results or a way to reproduce 853
the model (e.g., with an open-source dataset or instructions for how to construct 854
the dataset). 855
(d)We recognize that reproducibility may be tricky in some cases, in which case 856
authors are welcome to describe the particular way they provide for reproducibility. 857
In the case of closed-source models, it may be that access to the model is limited in 858
some way (e.g., to registered users), but it should be possible for other researchers 859
to have some path to reproducing or verifying the results. 860
5.Open access to data and code 861
27Question: Does the paper provide open access to the data and code, with sufficient instruc- 862
tions to faithfully reproduce the main experimental results, as described in supplemental 863
material? 864
Answer: [Yes] 865
Justification: We include our GitHub (anonymized for the double-blind phase). 866
Guidelines: 867
• The answer NA means that paper does not include experiments requiring code. 868
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 869
public/guides/CodeSubmissionPolicy ) for more details. 870
•While we encourage the release of code and data, we understand that this might not be 871
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 872
including code, unless this is central to the contribution (e.g., for a new open-source 873
benchmark). 874
•The instructions should contain the exact command and environment needed to run to 875
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 876
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 877
•The authors should provide instructions on data access and preparation, including how 878
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 879
•The authors should provide scripts to reproduce all experimental results for the new 880
proposed method and baselines. If only a subset of experiments are reproducible, they 881
should state which ones are omitted from the script and why. 882
•At submission time, to preserve anonymity, the authors should release anonymized 883
versions (if applicable). 884
•Providing as much information as possible in supplemental material (appended to the 885
paper) is recommended, but including URLs to data and code is permitted. 886
6.Experimental Setting/Details 887
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 888
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 889
results? 890
Answer: [Yes] 891
Justification: We explain our processes for one-class classification in section 4.3. Hyper- 892
parameters, as well as optimizers, are included in section 4.1. Additionally, our remote 893
repository contains the full details. 894
Guidelines: 895
• The answer NA means that the paper does not include experiments. 896
•The experimental setting should be presented in the core of the paper to a level of detail 897
that is necessary to appreciate the results and make sense of them. 898
•The full details can be provided either with the code, in appendix, or as supplemental 899
material. 900
7.Experiment Statistical Significance 901
Question: Does the paper report error bars suitably and correctly defined or other appropriate 902
information about the statistical significance of the experiments? 903
Answer: [Yes] 904
Justification: We utilized a statistical test to study the significance of all of our performance 905
results —see tables 3, 6, 8. We also extensively used boxplots of all AUC results to visualize 906
our performance in different scenarios —see figures 6, 9, 10, 11.b. 907
Guidelines: 908
• The answer NA means that the paper does not include experiments. 909
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 910
dence intervals, or statistical significance tests, at least for the experiments that support 911
the main claims of the paper. 912
28•The factors of variability that the error bars are capturing should be clearly stated (for 913
example, train/test split, initialization, random drawing of some parameter, or overall 914
run with given experimental conditions). 915
•The method for calculating the error bars should be explained (closed form formula, 916
call to a library function, bootstrap, etc.) 917
• The assumptions made should be given (e.g., Normally distributed errors). 918
•It should be clear whether the error bar is the standard deviation or the standard error 919
of the mean. 920
•It is OK to report 1-sigma error bars, but one should state it. The authors should 921
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 922
of Normality of errors is not verified. 923
•For asymmetric distributions, the authors should be careful not to show in tables or 924
figures symmetric error bars that would yield results that are out of range (e.g. negative 925
error rates). 926
•If error bars are reported in tables or plots, The authors should explain in the text how 927
they were calculated and reference the corresponding figures or tables in the text. 928
8.Experiments Compute Resources 929
Question: For each experiment, does the paper provide sufficient information on the com- 930
puter resources (type of compute workers, memory, time of execution) needed to reproduce 931
the experiments? 932
Answer: [Yes] 933
Justification: See the beginning of section B 934
Guidelines: 935
• The answer NA means that the paper does not include experiments. 936
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 937
or cloud provider, including relevant memory and storage. 938
•The paper should provide the amount of compute required for each of the individual 939
experimental runs as well as estimate the total compute. 940
•The paper should disclose whether the full research project required more compute 941
than the experiments reported in the paper (e.g., preliminary or failed experiments that 942
didn’t make it into the paper). 943
9.Code Of Ethics 944
Question: Does the research conducted in the paper conform, in every respect, with the 945
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 946
Answer: [Yes] 947
Justification: We reviewed the NeurIPS Code of Ethics and found no violation. 948
Guidelines: 949
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 950
•If the authors answer No, they should explain the special circumstances that require a 951
deviation from the Code of Ethics. 952
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 953
eration due to laws or regulations in their jurisdiction). 954
10.Broader Impacts 955
Question: Does the paper discuss both potential positive societal impacts and negative 956
societal impacts of the work performed? 957
Answer: [Yes] 958
Justification: In sections, 1 & 5 we go through the importance of outlier detection in 959
many fields, particularly for our use-case. Our positive impact on society consists of the 960
improvement of the tasks where outlier detection is needed. 961
Guidelines: 962
• The answer NA means that there is no societal impact of the work performed. 963
29•If the authors answer NA or No, they should explain why their work has no societal 964
impact or why the paper does not address societal impact. 965
•Examples of negative societal impacts include potential malicious or unintended uses 966
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 967
(e.g., deployment of technologies that could make decisions that unfairly impact specific 968
groups), privacy considerations, and security considerations. 969
•The conference expects that many papers will be foundational research and not tied 970
to particular applications, let alone deployments. However, if there is a direct path to 971
any negative applications, the authors should point it out. For example, it is legitimate 972
to point out that an improvement in the quality of generative models could be used to 973
generate deepfakes for disinformation. On the other hand, it is not needed to point out 974
that a generic algorithm for optimizing neural networks could enable people to train 975
models that generate Deepfakes faster. 976
•The authors should consider possible harms that could arise when the technology is 977
being used as intended and functioning correctly, harms that could arise when the 978
technology is being used as intended but gives incorrect results, and harms following 979
from (intentional or unintentional) misuse of the technology. 980
•If there are negative societal impacts, the authors could also discuss possible mitigation 981
strategies (e.g., gated release of models, providing defenses in addition to attacks, 982
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 983
feedback over time, improving the efficiency and accessibility of ML). 984
11.Safeguards 985
Question: Does the paper describe safeguards that have been put in place for responsible 986
release of data or models that have a high risk for misuse (e.g., pretrained language models, 987
image generators, or scraped datasets)? 988
Answer: [NA] 989
Justification: We do not identify any risks. 990
Guidelines: 991
• The answer NA means that the paper poses no such risks. 992
•Released models that have a high risk for misuse or dual-use should be released with 993
necessary safeguards to allow for controlled use of the model, for example by requiring 994
that users adhere to usage guidelines or restrictions to access the model or implementing 995
safety filters. 996
•Datasets that have been scraped from the Internet could pose safety risks. The authors 997
should describe how they avoided releasing unsafe images. 998
•We recognize that providing effective safeguards is challenging, and many papers do 999
not require this, but we encourage authors to take this into account and make a best 1000
faith effort. 1001
12.Licenses for existing assets 1002
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 1003
the paper, properly credited and are the license and terms of use explicitly mentioned and 1004
properly respected? 1005
Answer: [Yes] 1006
Justification: We include URLs and citations for all dataset selections, packages, and 1007
methods. 1008
Guidelines: 1009
• The answer NA means that the paper does not use existing assets. 1010
• The authors should cite the original paper that produced the code package or dataset. 1011
•The authors should state which version of the asset is used and, if possible, include a 1012
URL. 1013
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1014
•For scraped data from a particular source (e.g., website), the copyright and terms of 1015
service of that source should be provided. 1016
30•If assets are released, the license, copyright information, and terms of use in the package 1017
should be provided. For popular datasets, paperswithcode.com/datasets has 1018
curated licenses for some datasets. Their licensing guide can help determine the license 1019
of a dataset. 1020
•For existing datasets that are re-packaged, both the original license and the license of 1021
the derived asset (if it has changed) should be provided. 1022
•If this information is not available online, the authors are encouraged to reach out to 1023
the asset’s creators. 1024
13.New Assets 1025
Question: Are new assets introduced in the paper well documented and is the documentation 1026
provided alongside the assets? 1027
Answer: [Yes] 1028
Justification: We include the documentation of our implementation in the repository. 1029
Guidelines: 1030
• The answer NA means that the paper does not release new assets. 1031
•Researchers should communicate the details of the dataset/code/model as part of their 1032
submissions via structured templates. This includes details about training, license, 1033
limitations, etc. 1034
•The paper should discuss whether and how consent was obtained from people whose 1035
asset is used. 1036
•At submission time, remember to anonymize your assets (if applicable). You can either 1037
create an anonymized URL or include an anonymized zip file. 1038
14.Crowdsourcing and Research with Human Subjects 1039
Question: For crowdsourcing experiments and research with human subjects, does the paper 1040
include the full text of instructions given to participants and screenshots, if applicable, as 1041
well as details about compensation (if any)? 1042
Answer: [NA] 1043
Justification: The paper does not involve crowdsourcing nor research with human subjects. 1044
Guidelines: 1045
•The answer NA means that the paper does not involve crowdsourcing nor research with 1046
human subjects. 1047
•Including this information in the supplemental material is fine, but if the main contribu- 1048
tion of the paper involves human subjects, then as much detail as possible should be 1049
included in the main paper. 1050
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1051
or other labor should be paid at least the minimum wage in the country of the data 1052
collector. 1053
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1054
Subjects 1055
Question: Does the paper describe potential risks incurred by study participants, whether 1056
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1057
approvals (or an equivalent approval/review based on the requirements of your country or 1058
institution) were obtained? 1059
Answer: [NA] 1060
Justification: The paper does not involve crowdsourcing nor research with human subjects. 1061
Guidelines: 1062
•The answer NA means that the paper does not involve crowdsourcing nor research with 1063
human subjects. 1064
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1065
may be required for any human subjects research. If you obtained IRB approval, you 1066
should clearly state this in the paper. 1067
31•We recognize that the procedures for this may vary significantly between institutions 1068
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1069
guidelines for their institution. 1070
•For initial submissions, do not include any information that would break anonymity (if 1071
applicable), such as the institution conducting the review. 1072
32