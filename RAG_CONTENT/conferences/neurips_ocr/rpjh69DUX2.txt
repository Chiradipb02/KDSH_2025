Meta-Reinforcement Learning with Universal Policy
Adaptation: Provable Near-Optimality under All-task
Optimum Comparator
Siyuan Xu & Minghui Zhu
School of Electrical Engineering and Computer Science
The Pennsylvania State University
University Park, PA 16801
{spx5032, muz16}@psu.edu
Abstract
Meta-reinforcement learning (Meta-RL) has attracted attention due to its capability
to enhance reinforcement learning (RL) algorithms, in terms of data efficiency and
generalizability. In this paper, we develop a bilevel optimization framework for
meta-RL (BO-MRL) to learn the meta-prior for task-specific policy adaptation,
which implements multiple-step policy optimization on one-time data collection.
Beyond existing meta-RL analyses, we provide upper bounds of the expected
optimality gap over the task distribution. This metric measures the distance of
the policy adaptation from the learned meta-prior to the task-specific optimum,
and quantifies the model’s generalizability to the task distribution. We empirically
validate the correctness of the derived upper bounds and demonstrate the superior
effectiveness of the proposed algorithm over benchmarks.
1 Introduction
Meta-learning [ 58,15,25] aims to extract the shared prior knowledge, known as meta-prior, from
the similarities and interdependencies of multiple existing learning tasks, in order to accelerate the
learning process, increase the efficiency of data usage, and improve the overall learning performance
in new tasks. Meta-learning has been extended to solve RL problems, known as meta-RL [ 15,5],
and shows its promise to overcome the challenges of traditional RL algorithms, including scarce
real-world data [3, 44, 65], limited computing resources, and slow learning speed [54, 63].
Meta-learning methods can be generally categorized into optimization-based, model-based (black box
methods), and metric-based methods [ 27,5]. The optimization-based meta-learning approach [ 25] is
compatible with any model trained by an optimization algorithm, such as gradient descent, and thus
is applicable to a vast range of learning problems, including RL problems. Specifically, it formulates
meta-learning as a bilevel optimization problem. At the lower-level optimization, the task-specific
model is adapted from a shared meta-parameter by an optimization algorithm. At the upper-level
optimization, the meta-parameter is to maximize the meta-objective, i.e., the performance of the
model adapted from the meta-parameter over training tasks. The existing methods, including MAML
and its variants [ 15,38,12], take a one-step gradient ascent as the lower-level policy optimization
algorithm, which limits its data inefficiency and leads to sub-optimality.
During the meta-test, MAML conducts one-time data collection, i.e., collecting data using one policy
(the meta-policy), and adapts the policy by one step of policy gradient to the new task. However,
the collected data is only used in one policy gradient step, which may not sufficiently leverage the
data and potentially fail to achieve a good performance. To mitigate the issue, a typical practice is to
implement the data collection and the policy gradient alternately multiple times [ 15]. However, the
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Table 1: Solved theoretical challenges of meta-RL
Convergence
of meta-objectiveOptimality of
meta-objectiveNear-optimality
under all-task optimum
[12, 57] ✓ × ×
[60] × ✓When assuming convergence ×
[42] × ×✓Under optimal
expert policy supervision
This paper ✓ Immediate result from [60] ✓
environment exploration is usually costly and time-consuming during the meta-test in applications of
meta-RL [ 44,6,36]. As a result, the low data efficiency limits the optimality of task-specific policies.
In contrast, in this paper, we collect data by meta-policy for one time and utilize multiple policy
optimization steps to improve the data efficiency.
The optimality analysis of MAML is studied in [ 12,60] with a metric of optimality on the meta-
objective , where the error of the meta-objective is defined by the expectation of the optimality gap
between the task-specific policy adapted from the learned meta-parameter and the policy adapted
from the best meta-parameter [ 60,14,26]. However, the best meta-parameter is shared for all tasks.
Even if the meta-objective error is close to zero, i.e., the learned meta-parameter is close to the best
one, the model adapted from the learned meta-parameter might be far from task-specific optimum for
some tasks. In contrast, we aim to design a meta-RL algorithm that can fit a stronger optimality metric,
called near-optimality under all-task optimum , where the comparator, i.e., the policy adapted from
the best meta-parameter, is replaced by the task-specific optimal policy for each task. This metric
offers a more strict comparator for the model adapted from the learned meta-parameter, i.e., when
the metric achieves zero, the policy adaptation produces the optimal policy for every task. A similar
metric is studied by [ 42]. It assumes that the task-specific optimal expert policy for each task is
accessible and serves the supervision for policy adaptation during meta-training, which alleviates the
analysis difficulty caused by the optimal policy comparator. However, the expert policy supervision
is not accessible in a standard meta-RL problem. The metric under all-task optimum is also studied
by [9, 10, 65] in the context of supervised meta-learning.
Main contribution. We develop a bilevel optimization framework for meta-RL, which implements
multiple-step policy optimization on one-time data collection during task-specific policy adaptation.
The overall contributions are summarized as follows. (i) We develop a universal policy optimization
algorithm, which performs multiple optimization steps to maximize a surrogate of the accumulated
reward function. The surrogate is developed only using one-time data collection. It includes various
widely used policy optimization algorithms, including the policy gradient, the natural policy gradient
(NPG) [ 30], and the proximal policy optimization (PPO) [ 52] as the special cases. Then, to learn
the mete-prior, we formulate the meta-RL problem as a bilevel optimization problem, where the
lower-level optimization is the universal policy optimization algorithm from the meta-policy and
the upper-level optimization is to maximize the meta-objective function, i.e., the total reward of the
models adapted from the meta-policy. (ii) We derive the implicit differentiation for both unconstrained
and constrained lower-level optimization problems to compute the hypergradient, i.e., the gradient of
the meta-objective, and propose the meta-training algorithm. In contrast to [ 60], we do not require
to know the closed-form solution of the lower-level optimization. (iii) We derive upper bounds that
quantify (a) the optimality gap between the adapted policy and the optimal task-specific policy for any
task, and (b) the expected optimality gap over the task distribution. Since the proposed framework
incorporates several existing meta-RL methods, such as MAML, as a special case, the analysis also
provides the theoretical motivation for them. (iv) We conduct experiments to validate the theoretical
bounds and verify the efficacy of the proposed algorithm on meta-RL benchmarks.
Table 1 compares the solved theoretical challenges of meta-RL between this paper and previous
works [ 12,57,60,42]. Specifically, paper [ 60] derives the optimality on the meta-objective under the
assumption of bounded hypergradient. Papers [ 12,57] consider the convergence of the meta-objective.
The near-optimality under all-task optimum is considered in [ 42]. However, it assumes the optimal
expert policies of the training tasks are available in meta-training, such that it can learn to approach
the expert policies, while the other methods do not require the expert policies and learn from the
explorations of the environments. In this paper, we show the convergence and optimality guarantee
on the meta-objective, and, more importantly, the optimality guarantee under the all-task optimum
comparator. It is noted that the optimality on the meta-objective is an immediate result from [60].
22 Related works.
Categorization of meta-RL. Meta-RL methods can be generally categorized into (i) optimization-
based meta-RL, (ii) black-box (also called context-based) meta-RL. Optimization-based meta-RL
approaches, such as MAML [ 15] and its variants [ 55,38], usually include a policy adaptation
algorithm and a meta-algorithm. During the meta-training, the meta-algorithm aims to learn a
meta-policy, such that the policy adaptation algorithm can achieve good performance starting from
the meta-policy. The learned meta-policy parameter is adapted to the new task using the policy
adaptation algorithm during the meta-test. Black-box meta-RL [ 11,59,49,47,68] aims to learn an
end-to-end neural network model. The model has fixed parameters for the policy adaptation during
the meta-test, and generates the task-specific policy using the trajectories of the new task takes. In
optimization-based meta-RL, the task-specific policy is adapted from a shared meta-policy over the
task distribution. The learned meta-knowledge is not specialized for each task, and its meta-test
performance on a task depends on a general policy optimization algorithm applied to new data from
that task. In contrast, the end-to-end model in black-box meta-RL typically includes specialized
knowledge for any task within the task distribution, and uses the new data merely as an indicator to
identify the task within the distribution. As a result, the optimality of optimization-based methods is
usually worse than black-box methods, especially when the task distribution is heterogeneous and
the data scale for adaptation is extremely small. On the other hand, the policy adaptation algorithms
in the meta-test of optimization-based methods can generally improve the policy starting from any
initial policy, not only the learned meta-policy. Therefore, it is robust to sub-optimal meta-policy
and can deal with tasks that are out of the training task distribution [ 16,62]. In contrast, due to the
specialization of the learned model, black-box methods cannot be generalized outside of the training
task distribution. In this paper, we focus on the category of optimization-based meta-RL and compare
the proposed algorithm with the existing optimization-based meta-RL approaches in terms of both
experimental results and theory.
Bilevel optimization in meta-RL. Bilevel optimization has been widely studied empirically [ 45,21,
17,18,53,29] and theoretically [ 20,22,29]. It has been applied to many machine learning problems,
including meta-learning [ 35,48], hyperparameter optimization [ 45,17,18], RL [ 24,34], and inverse
RL [39,40,41]. Since the overall objective function in bilevel optimization is generally non-convex,
theoretical analyses of bilevel optimization mainly focus on the algorithm convergence [20, 29, 64],
rarely on the optimality. This paper formulates meta-RL as a bilevel optimization problem. The key
theoretical contribution of this paper is to derive upper bounds on the near-optimality under all-task
optimum, i.e., the expected optimality of the solutions of the lower-level optimization compared with
that of the task-specific optimal policies. The near-optimality under all-task optimum is unique to
meta-learning and has not been studied in the literature on bilevel optimization.
3 Problem statement
MDP. A Markov decision process (MDP) M≜{S,A, γ, ρ, P, r }is defined by the bounded state
spaceS, the discrete or bounded continuous action space A, the discount factor γ, the initial state
distribution ρoverS, the transition probability P(s′|s, a) :S × A × S → [0,1], and the reward
function r:S × A × S → [0, rmax].
Policy and value function. A stochastic policy π:S →P(A)is a map from states to probability
distributions over actions, and π(a|s)denotes the probability of selecting action ain state s. For
a policy π, the value function is defined as Vπ(s)≜E[P∞
t=0γtr(st, at, st+1)|s0=s, π]. The
action-value function is defined as Qπ(s, a)≜E[P∞
t=0γtr(st, at, st+1)|s0=s, a0=a, π]. The
advantage function is defined as Aπ(s, a)≜Qπ(s, a)−Vπ(s). The accumulated reward func-
tion is J(π)≜Es∼ρ[Vπ(s)]. Define the discounted state visitation distribution of a policy πas
νπ(s)≜Es0∼ρ[(1−γ)P∞
t=0γtP(st=s|π)]. In this paper, we consider parametric policy πθ,
parameterized by θ. The optimal parameter θ∗can maximize the accumulated reward function, i.e.,
θ∗≜argmaxθJ(πθ). Ifθ∗is not unique, denote the set of the optimal solutions by Θ∗.
Meta-reinforcement learning. Meta-RL aims to solve multiple RL tasks. Consider a space of RL
tasks Γ, where each task τ∈Γis modeled by a MDP Mτ≜{S,A, γ, ρ τ, Pτ, rτ}. Correspondingly,
the notations Vπ
τ,Qπ
τ,Aπ
τ,νπ
τ,θ∗
τ,Θ∗
τandJτare defined for task τ. The RL tasks follow a probability
distribution P(Γ). Meta-RL aims to learn a meta-policy πϕparameterized by a meta parameter ϕ,
3such that it can adapt to an unseen task τnew∼P(Γ)with a few iterations and a small number of new
environment explorations. In specific, during the meta-training, several tasks can be i.i.d. sampled
fromP(Γ), i.e.,{τj}T
j=1∼P(Γ), and the tasks’ MDPs {Mτj}T
j=1can be explored. The meta-learner
applies a meta-algorithm to update the meta parameter ϕby using the data collected from the sampled
tasks. During the meta-test, a new task τnewis given, one time of a within-task algorithm Algwith
data collected from τnewis applied, the meta-parameter ϕis adapted to the task-specific parameter
θ′
τnewand the task-specific policy πθ′τnewis tested on the task τnew.
Optimality Metric. Consider a meta-RL algorithm that produces a meta-parameter ϕ, and the
take-specific parameter πθ′
τis adapted from the meta-parameter ϕon a task τ, denoted as πθ′
τ=
Alg(πϕ, τ). We define the task-expected optimality gap (TEOG) as the metric to evaluate the
algorithm, i.e., Eτ∼P(Γ)[Jτ(πθ∗τ)−Jτ(Alg(πϕ, τ))], where θ∗
τis the optimal parameter for task
τ. First, the TEOG considers the expected error over the task distribution P(Γ), reflecting the
generalizability of the produced meta-parameter. Second, the TEOG adopts the comparator of the
optimal task-specific policy πθ∗τfor any task τ(all-task optimum comparator), and evaluates the
optimality gap Jτ(πθ∗τ)−Jτ(Alg(πϕ, τ)). In contrast, [ 60,14,26] adopts the comparator of the policy
adapted from the optimal meta-parameter πϕ∗, and evaluates the optimality gap Jτ(Alg(πϕ∗, τ))−
Jτ(Alg(πϕ, τ)). The latter only considers the optimality on the meta-objective, i.e., how well the
trained meta-objective can approach the optimal meta-objective. However, even if the error of the
meta-objective is approaching zero, i.e., the learned meta-policy is close to the best candidate, the
performance of the model adapted from the optimal meta-policy might still be lacking. This is
because policy optimization usually requires thousands of value/policy iterations to converge; when
tasks are heterogeneous, even if it starts from the best meta-policy, one time of Algwith one time of
value estimate may not be sufficient. In contrast, if our metric is zero, the policy adapted from the
meta-parameter to any task is optimal for the task.
Policy distance and task variance. To find the solution for a new task within a few iterations of
policy optimization, it is crucial that the meta-policy πϕcan benefit from learning on correlated tasks.
Similar to [ 4,9,31], we measure the correlation of tasks in the task distribution P(Γ)by its variance,
defined by the minimal mean square of the distances among the optimal task-specific policies, i.e.,
Var(P(Γ))≜minθminθ∗τ∈Θ∗τEτ∼P(Γ)[D2
τ(πθ, πθ∗τ)]. Here, Dτ(πθ, πθ∗τ)is the distance metric
between πθandπθ∗τon the task τand is defined by Dτ(πθ, πθ′)≜q
Es∼νπθτ[d2(πθ(·|s), πθ′(·|s))],
where d(πθ(·|s), πθ′(·|s))is the distance of the policies πθandπθ′on the state s.
Note that the distance metrics Dτ(·,·)andd(·,·, s)can be custom-defined, leading to multiple
policy update algorithms, as shown in Section 4. Here, we introduce several examples of d(·,·, s)
andDτ(·,·), which are commonly used as the distance metrics in RL literature [ 51,30,37]. For
policies πθandπθ′, we apply (i) the KL-divergence of the action probability distribution, i.e.,
d2
1(πθ, πθ′, s)≜DKL(πθ(·|s)∥πθ′(·|s)), which is similar to the definition in [ 31]; (ii) The KL-
divergence with the other order, i.e., d2
2(πθ, πθ′, s)≜DKL(πθ′(·|s)∥πθ(·|s)); (iii) the Euclidean
distance of the parameters, i.e., d2
3(πθ, πθ′, s)≜∥θ−θ′∥2. Correspondingly, for i= 1,2, and 3,
we define Dτ,i(πθ, πθ′)≜q
Es∼νπθτ[d2
i(πθ, πθ′, s)]. Note that the distance metrics (i)(ii) are not
symmetric, i.e., Dτ(πθ′, πθ′′)̸=Dτ(πθ′′, πθ′), and (iii) is symmetric.
In the subsequent sections, we present algorithms based on the generalized distance definitions of
Dτ(·,·)andd(·,·, s). Moreover, we conduct analyses for the introduced distance metrics, from Dτ,1
toDτ,3, to provide comprehensive insights into their respective performances.
4 Meta-Reinforcement Learning Framework
In this section, we develop a meta-RL algorithm by bilevel optimization, where the lower-level
optimization is the within-task algorithm that adapts the parameter from the meta-parameter and
the upper-level optimization is the meta-algorithm that obtains the meta-parameter. The proposed
algorithm has two distinctions compared with existing algorithms. First, it uses one time of a
universal policy optimization algorithm as the lower-level within-task algorithm. Second, we derive
the hypergradient by the implicit differentiation, where the closed-form solution of the lower-level
optimization is not required.
4Within-task algorithm. Consider the policy optimization from the meta policy as the within-task
algorithm Alg. Specifically, given the meta-parameter ϕand a task τ, the task-specific policy
πθ′τ=Alg(πϕ, λ, τ)is defined by θ′
τ= argmaxθEs∼νπϕ
τ,a∼πθ(·|s)
Qπϕτ(s, a)
−λD2
τ(πϕ, πθ).
When the action space Ais discretized and the policy is tabular, i.e., the probabilities of actions are
independent between different states, the above problem can be solved by πθ′τ(·|s) =
Alg(πϕ, λ, τ)(·|s) = argmax
πθ(·|s)X
a∈Aπθ(a|s)Qπϕτ(s, a)−λd2(πϕ(·|s), πθ(·|s)), (1)
for all states s∈ S. When the policy is parameterized by an approximation function, in both
continuous and discrete action space A,πθ′τ=Alg(πϕ, λ, τ)is computed by θ′
τ=
argmaxθEs∼νπϕ
τ,a∼πϕ(·|s)πθ(a|s)
πϕ(a|s)Qπϕτ(s, a)
−λD2
τ(πϕ, πθ). (2)
In (1) and (2), λ > 0is a tuning hyperparameter and the distance metric Dτcan be arbitrarily
chosen. Considering the explorations for the task τare limited, Algonly needs to evaluate the Qπϕτ
by Monte-Carlo sampling on a single policy πϕ, where the data sampling complexity is exactly the
same as the one-step gradient descent in MAML [ 15]. Therefore, we denote Alg, i.e., collecting data
on the meta-policy and solving the optimal solution of (1) and (2) as the one-time policy adaptation.
More details about the data sample complexity and the computational complexity of (1) and (2) are
clarified in Appendix F. On the other hand, one gradient step is usually not sufficient to identify a
good policy. Therefore, Algis to solve the optimal solution of (1) or (2). As shown in Section 5.4,
the objective function of (1) or (2) is an approximation of the true objective function Jτ(π).
Note that the objective function in (1) and (2) can reduce to that of multiple widely used policy
optimization approaches: (i) PPO in [ 51,52] when Dτ=Dτ,2; (ii) a variant of the PPO [ 60,37],
when Dτ=Dτ,1; (iii) the proximally regularized policy update, i.e., the policy optimization
regularized by Euclidean distance of the policy parameter [ 51], when Dτ=Dτ,3. Moreover, (iv) if
we approximate the expectation in (2) by its first-order approximation and also select Dτ=Dτ,3, the
within-task algorithm (2) also can be reduced to one-step policy gradient, as shown in Appendix H;
(v) if we use the first-order approximation of the expectation in (2), the second-order approximation
of the term D2
τ(πϕ, πθ), and select Dτ=Dτ,2, the within-task algorithm (2) is reduced to the natural
policy gradient (NPG).
Meta-algorithm. The performance of the meta-parameter ϕis evaluated by the meta-objective
function, which is defined as the expected accumulated reward after the parameter is adapted by the
within-task algorithm, i.e., Eτ∼P(Γ)[Jτ(Alg(πϕ, λ, τ))]. In the meta-algorithm, we maximize the
meta-objective to obtain the optimal meta-parameter ϕ∗, i.e.,
ϕ∗= argmax
ϕEτ∼P(Γ)[Jτ(Alg(πϕ, λ, τ))]. (3)
As (1) and (2) provide multiple choices of the within-task algorithms when selecting different Dτ,
the meta-algorithm (3) provides the algorithms to learn the corresponding meta-priors. For example,
(3) takes on the role of the meta-PPO algorithm when Dτ=Dτ,1orDτ,2, i.e., (3) learns the
meta-initialization for PPO. It is a meta-NPG algorithm with the corresponding approximation and
Dτ. Moreover, when Alg(πϕ, λ, τ)in (2) reduces to the one-step policy gradient shown in (iv) of
the last paragraph, (3) represents a precise formulation of MAML in [ 15]. More details about the
formulation and its relations with MAML are shown in Appendix G and H.
Hypergradient computation. Simlar to [ 29,64], the meta-algorithm in (3) aims to solve a bilevel
optimization problem. In previous works [ 60], they apply the policy optimizations that have known
closed-form solutions as the lower-level within-task algorithms. As a result, the bilevel optimization
problem is reduced to a single-level problem. In contrast, in this paper, as we consider a universal pol-
icy optimization, its closed-form solution cannot be obtained. To address the challenge, we compute
∇ϕAlg(πϕ, λ, τ)and the hypergradient by deriving the implicit differentiation on Alg(πϕ, λ, τ). As
shown in Section 4, the optimization problem Alg(πϕ, λ, τ)is unconstrained in (2), but is constrained
in (1) due toP
a∈Aπ(a|s) = 1 . Therefore, we derive the implicit differentiation for both uncon-
strained and constrained optimization problems. The following proposition shows the hypergradient
computation for the tabular policy. Its proof is shown in Appendix J.1.
Proposition 1 (Hypergradient for the tabular policy ).For the tabular policy in the discrete
state-action space, consider any meta-parameter ϕand the within-task algorithm (1). Let
5πθ′τ=Alg(πϕ, λ, τ). If M(s)≜λ∇2
π(·|s)d2(πϕ(·|s), π(·|s))is non-singular for each s∈
S, we have ∇ϕJτ(πθ′τ) =1
1−γE
s∼νπθ′ττhP
a∈A∇ϕπθ′τ(a|s)Qπθ′ττ(s, a)i
, where ∇⊤
ϕπθ′τ(·|s) =

M(s)−1−M(s)−11 1⊤M(s)−1
1⊤M(s)−11
∇⊤
ϕQπϕτ(s,·)−λ∇⊤
ϕ∇π(·|s)d2(πϕ(·|s), π(·|s))
|π=πθ′τ.
The computation of ∇ϕQπϕτ(s,·)is shown in Appendix C. A sufficient condition of M(s)being
non-singular is that dis locally strongly-convex at π=πθ′τ, shown in Appendix J.1. Moreover,
when d=d1ord=d2(correspondingly, Dτ=Dτ,1orDτ=Dτ,2in (1)), the matrix M(s) =
λ∇2
π(·|s)d2(πϕ(·|s), π(·|s))is always non-singular for any ϕandM(s)is always diagonal, and thus
it is easy to compute M−1(s). The hypergradient computation ∇ϕJτ(πθ′τ)forDτ=Dτ,1andDτ,2
is shown in Appendix K.1 and L.1.
The following proposition shows the hypergradient computation for the policy with function approxi-
mation. Its proof is shown in Appendix J.2.
Proposition 2 (Hypergradient for the policy with function approximation ).When a policy
is represented by a function approximation, in both the discrete and continuous action spaces,
for any meta-parameter ϕand the within-task algorithm in (2). Let πθ′τ=Alg(πϕ, λ, τ).
If∇ϕJτ(πθ′τ)exists, ∇ϕJτ(πθ′τ) =1
1−γ∇ϕθ′
τE
s∼νπθ′ττ,a∼πθ′τ(·|s)h∇θ′τπθ′τ(a|s)
πθ′τ(a|s)Qπθ′ττ(s, a)i
, and
∇⊤
ϕθ′
τ=−Es∼νπϕ
τ,a∼πϕ(·|s)[∇2
θd2(πϕ(·|s), πθ(·|s))−∇2
θπθ(a|s)
λπϕ(a|s)Qπϕτ(s, a)]−1Es∼νπϕ
τ,a∼πϕ(·|s)
[∇⊤
ϕ∇θd2(πϕ(·|s), πθ(·|s))−∇θπθ(a|s)
λπϕ(a|s)∇⊤
ϕQπϕτ(s, a)]|θ=θ′τ.
A sufficient condition of ∇ϕJτ(πθ′τ)being existent is the objective function of (2)is locally strongly
concave at θ=θ′
τ, as proven in Appendix J.2. The computation of ∇ϕQπϕτ(s,·)is shown in Appendix
C. Note that we need to compute the inverse of the Hessian when computing the hypergradient in
Proposition 2. Similar to several widely used RL algorithms, such as TRPO [ 51] and CPO [ 1],
we apply the conjugate gradient algorithm [ 23] to compute the inverse of the Hessian, which has
demonstrated high efficiency across a wide range of applications of RL and meta-learning [ 51,29,15].
More clarifications about the computation efficiency of the Hessian inverse are shown in Appendix E.
Algorithm 1 Meta-Training for BO-MRL
Require: Regularization weight λ >0; Initial meta-parameter ϕ0; learning rate α
1:fort= 0,···, Tdo
2: Sample a task τ∼P(Γ)with the MDP Mτi.i.d.
3: Evaluate Qπϕtτ(·,·)for current meta-policy πϕtby Monte-Carlo sampling
4: Adapt the task-specific policy πθ′τfrom the meta-policy πϕtby solving πθ′τ=Alg(λ, ϕt, τ)defined in
(1) or (2).
5: Evaluate Qπθ′ττ(·,·)for adapted policy πθ′τMonte-Carlo sampling
6: Compute the hypergradient ∇ϕJτ(πθ′τ)in Proposition 1 or 2 by conjugate gradient method
7: Update meta-parameter ϕt+1=ϕt+α∇ϕJτ(πθ′τ)
8:end for
9: Return ϕT
With the hypergradient computations in Proposition 1 and Proposition 2, we apply the stochastic
gradient ascent (SGD) to solve the optimization problem in (3). The meta-training of the bilevel
optimization framework for meta-RL (BO-MRL) is formally stated in Algorithm 1. The state-action
value function in lines 3 and 5 can be estimated by many approaches, including Monte-Carlo sampling
used in MAML [ 15] and vine in [ 51]. We also propose a practical algorithm of Algorithm 1, as shown
in Algorithm 2 in Appendix D, which includes more implementation details of the algorithm and
several mechanisms to improve Algorithm 1.
5 Theoretical Results
In this section, we quantify the performance of Algorithm 1, where the softmax policies and several
distance metrics introduced in Section 3 are adopted. For convenience, we denote Alg(1)asAlgin
(1) and (2) when Dτ=Dτ,1, and denote Alg(2)andAlg(3)in an analogous way. In Section 5.1, we
6introduce the softmax policy and necessary assumptions. In the following three sections, we consider
two cases of Algorithm 1, including (i) Algorithm 1 with the within-task algorithm Alg(1)andAlg(2)
for the tabular softmax policy; and (ii) Algorithm 1 with the within-task algorithm Alg(3)for the
softmax policy with function approximation. For the algorithms in (i) and (ii), we study the existence
of hypergradient in Section 5.2, derive the convergence guarantees in Section 5.3, and derive the
near-optimality under the all-task optimum, i.e., derive the upper bounds of TEOG, in Section 5.4.
5.1 Softmax policy and assumptions
We apply the softmax policies, which are commonly applied in [ 66,37,60], and use the following
assumptions on the task τ.
Softmax policies. Consider the softmax policies ˆπθparameterized by θfor (i) the tabular policy and
(ii) the policy with function approximation. In particular, the tabular policy in a discrete state-action
space is defined by ˆπθ(·|s)∝exp(θ(s,·)), where θ∈R|S|×|A|is a tabular map. The policy with
function approximation is defined by ˆπθ(·|s)∝exp(fθ(s,·)), where fθis a function approximation
model S × A → Rwith the parameter θ∈Rn.
Assumption 1 (Upper bound of advantage function) .For any task τ∈Γand any softmax policy ˆπθ,
|Aˆπθτ(s, a)| ≤Amax for any a∈ A and any s∈ S.
Since the reward rτ≤rmax is bounded, it is easy to show that |Aˆπθτ(s, a)| ≤rmax
1−γand Assumption
1 always holds. But we still keep Assumption 1 here, since there usually exist Amax such that
Amax≪rmax
1−γ. We also have the following assumption and show its remark.
Assumption 2 (Sufficient state visit) .For any task τ∈Γ, there exists a constant ϵ >0, such that for
all bounded parameters ϕ,νˆπϕτ(s)≥ϵfor all s∈ S.
Remark 1. Here are two sufficient conditions for Assumption 2: (i) For any task τ∈Γ, the MDP
Mτis ergodic [43, 56]; or (ii) the initial state distribution ρτhasρτ(s)>0for any s∈ S.
The proof of Remark 1 is shown in Appendix O. Note that (i) of Remark 1 is a mild condition and is
assumed in recent studies on RL algorithm analysis [61, 46].
For the policy with function approximation, we require the following additional assumptions on the
approximate function fθ, which are standard or weaker than those in the analysis of meta-learning
and meta-RL problems [9, 12, 13, 14].
Assumption 3 (Property of the approximate function) .For any state-action pair (s, a)∈ S × A ,
(i) the approximate function fθ(s, a)are cubic differentiable. (ii) fθ(s, a)isL1-Lipschitz, i.e.,
∥fθ1(s, a)−fθ2(s, a)∥ ≤L1∥θ1−θ2∥for any θ1, θ2∈Rn. (iii)∇θfθ(s, a)isL2-Lipschitz, i.e.,
∥∇θfθ1(s, a)− ∇ θfθ2(s, a)∥ ≤L2∥θ1−θ2∥for any θ1, θ2∈Rn, (iv)∇2
θfθ(s, a)isL3-Lipschitz,
i.e.,∇2
θfθ1(s, a)− ∇2
θfθ2(s, a)≤L3∥θ1−θ2∥for any θ1, θ2∈Rn.
5.2 Existence of hypergradient.
An essential prerequisite for using Algorithm 1 is that the hypergradients in Propositions 1 and
2 exist. As shown in Section 4, for the tabular policy, when i= 1 or2, the hypergradient
∇ϕJτ(Alg(i)(ˆπϕ, λ, τ))exists for any ϕ. For the policy with function approximation, we derive the
following sufficient condition of the hypergradient being existent. Its proof is shown in Appendix M.
Proposition 3 (Existence of hypergradient for the policy with function approximation) .In both
discrete and continuous action space, consider the softmax policy with function approximation
shown in Section 5.1. Suppose that Assumptions 1 and 3 hold. If λ > (6L2
1+ 2L2)Amax,
∇ϕJτ(Alg(3)(ˆπϕ, λ, τ))always for any ϕ.
5.3 Convergence guarantee
We begin with the convergence guarantee of Algorithm 1 for the tabular policy. The following
notations are used in the theorem: Bi,Ci,Gi,Ki,Mi(i= 1and2), where Ki≜2(Bi+2C2
i)r2
max
(1−γ)4 ,
Mi≜(Bi+2C2
i)Girmax
(1−γ)4 fori= 1and2.B1≜16rmax
λ(1−γ)3+24
1−γ+12
λ,C1≜6
1−γ, and G1≜4Amax
(1−γ)2.
B2≜16rmax
λ(1−γ)3+18
(1−γ)2,C2≜4
1−γ, and G2≜2Amax
(1−γ)2.
7Theorem 1 (Convergence guarantee for tabular softmax policy ).Consider the tabular
softmax policy in the discrete action space. Suppose that Assumptions 1 and 2 hold.
Let{ϕt}T
t=1be the sequence of meta-parameters generated by Algorithm 1 with λ≥
2Amax and the step size α= min
rmaxBi
(1−γ)2+2γrmaxC2
i
(1−γ)3−1
,1
Gi√
T
.Then, the bound:
1
TPT
t=1E[∥∇ϕEτ∼P(Γ)[Jτ(Alg(i)(ˆπϕt, λ, τ))]∥2]≤Ki
T+Mi√
T.holds for i= 1or2.
The first expectation comes from the random sampling in line 2 of Algorithm 1. The proofs of
Theorem 1 are shown in Appendices K.2 and L.2.
The following theorem shows the convergence guarantee for the policy with function approxima-
tion. The notations are used in the theorem: B3,C3,G3,K3,M3, where K3≜2(B3+2C2
3)r2
max
(1−γ)4 ,
M3≜(B3+2C2
3)G3rmax
(1−γ)4 ,G3≜L1Amax(λ+2γ
1−γL2
1Amax)
(1−γ)(λ−(6L2
1+2L2)Amax),C3≜2L1(λ+2γ
1−γL2
1Amax)
(1−γ)(λ−(6L2
1+2L2)Amax), and
B3≜(160L3
1+56L1L2+4L3)(λ+2γ
1−γL2
1Amax)2
(1−γ)3(λ−(6L2
1+2L2)Amax)2 .
Theorem 2 (Convergence guarantee for softmax policy with function approximation ).In both dis-
crete and continuous action space, consider the softmax policy with function approximation. Suppose
that Assumptions 1, 2, and 3 hold. Let {ϕt}T
t=1be the sequence of meta-parameters generated by Algo-
rithm 1 with λ >(6L2
1+2L2)Amaxand the step size α= min
rmaxB3
(1−γ)2+2γrmaxC2
3
(1−γ)3−1
,1
G3√
T
.
Then, the bound1
TPT
t=1E
∥∇ϕEτ∼P(Γ)[Jτ(Alg(3)(ˆπϕt, λ, τ))]∥2
≤K3
T+M3√
T.holds.
The first expectation arises from the random sampling in line 2 of Algorithm 1. The proof of Theorem
2 is shown in Appendix M. Theorems 1 and 2 show that the convergence rate of Algorithm 1 is
O(1√
T)and the constants in the notation Oare only related to the discount factor γ, the reward bound
rmax, the bound of the advantage function Amax, and the Lipschitz constants of fθ.
5.4 Near-optimality under all-task optimum
Before the derivation of the optimality analysis, we first introduce two intermediate Lemmas.
Lemma 1. Suppose that Assumptions 1, 2 hold. For any task τ, any bounded parameters θandθ′,
andi= 1or2, we have Jτ(ˆπθ′)−Jτ(ˆπθ)≥Es∼νˆπτ,a∼ˆπ′(·|s)h
Aˆπθτ(s,a)
1−γi
−2γAmax
(1−γ)2ϵD2
τ,i(ˆπθ,ˆπθ′).
Lemma 2. Consider the softmax policy with function approximation shown in Section 5.1. Suppose
that Assumptions 1, 2, and 3 hold. For any task τ, and any softmax policies parameterized by bounded
θandθ′, we have Jτ(ˆπθ′)−Jτ(ˆπθ)≥Es∼νˆπθτ,a∼ˆπθ′(·|s)h
Aˆπθτ(s,a)
1−γi
−4γAmaxL2
1
(1−γ)2ϵD2
τ.3(ˆπθ,ˆπθ′).
The proofs of Lemmas 1 and 2 are shown in Appendix N.1. Given Lemma 1, when λ=2γAmax
(1−γ)ϵ, the
within-task algorithm Alg(1,2)(ˆπ, λ, τ )in (1) is actually designed to maximize the right-hand side of
the inequality, where ˆπ′is the decision variable. Similarly, Given Lemma 2, when λ=4γAmaxL2
1
(1−γ)ϵ,
Alg(3)(ˆπθ, λ, τ)in (2) maximizes the right-hand side of the inequality, where ˆπθ′is the decision
variable. In other words, for each i= 1,2,and3, the within-task algorithm Alg(i)is to maximize a
lower bound of Jτ(ˆπθ), denoted as ¯Jτ(ˆπθ). This idea, referred to as the minorization-maximization
(MM) [ 28], is widely used in [ 51,33]. The design of Alg(i)enables us to connect the accumulated
reward of the policy after the policy adaptation with that of the optimal policy ˆπθ∗τfor task τ, i.e.,
¯Jτ(Alg(i)(ˆπϕ, λ, τ))≥¯Jτ(ˆπθ∗τ), which is a key intermediate result for the optimality analysis.
The final preparatory step is that we borrow the analysis of the meta-training error from [ 60]. In
particular, its theoretical result is encapsulated in the following assumption.
Assumption 4. (Bounding error of meta-objective using gradient) Let F(i)(ϕ)≜
Eτ∼P(Γ)[Jτ(Alg(i)(ˆπϕ, λ, τ))]. For both the tabular policy and the policy with functional approxi-
mation, there exists a concave positive non-decreasing function hi: [0,+∞)→[0,+∞), such that
max ϕ′F(i)(ϕ′)−F(i)(ϕ)≤hi(∥∇ϕF(i)(ϕ)∥2).
8Assumption 4 assumes the optimality gap of ˆπϕon the meta-objective is upper bounded by an
increasing function of its gradient. A sufficient condition of Assumption 4 is provided by [ 60].
Combine the Assumption 4 and the convergence analysis in Theorems 1 and 2, we can bound the
error of the meta-objective, i.e., max ϕF(i)(ϕ)−F(i)(ϕt). This result is referred to as the optimality
of the meta-objective shown in Table 1. Finally, we derive the upper bounds of the TEOG for both
the tabular policy and the policy with function approximation.
Theorem 3 (Optimality guarantee for softmax tabular policy ).Consider the tabular soft-
max policy for the discrete state-action space. Suppose that Assumptions 1,2 and 4
hold. Let {ϕt}T
t=1be the sequence of meta-parameters generated by Algorithm 1 with
λ=2Amax
(1−γ)ϵand the step size αshown in Theorem 1. Then, the following holds for
i= 1 or2:1
TPT
t=1Et
Eτ∼P(Γ)[Jτ(ˆπθ∗τ)−Jτ(Alg(i)(ˆπϕt, λ, τ))]
≤hi
Ki
T+Mi√
T
+
2(1+γ)Amax
(1−γ)2ϵVari(P(Γ)), where ˆπθ∗τis the optimal softmax policy for task τand the constants Kiand
Miare shown in Theorem 1.
Theorem 4 (Optimality guarantee for softmax policy with function approximation ).In both
discrete and continuous action space, consider the softmax policy with function approximation.
Suppose that Assumptions 1,2, 3 and 4 hold. Let {ϕt}T
t=1be the sequence of meta-parameters
generated by Algorithm 1 with λ=(6L2
1+2L2)Amax
(1−γ)ϵand the step size αshown in Theorem 2.
The following holds:1
TPT
t=1Et
Eτ∼P(Γ)[Jτ(ˆπθ∗τ)−Jτ(Alg(3)(ˆπϕt, λ, τ))]
≤h3
K3
T+M3√
T
+
((6+4 γ)L2
1+2L2)Amax
(1−γ)2ϵVar3(P(Γ)), where ˆπθ∗τis the optimal softmax policy for task τand the constants
K3andM3are the same as Theorem 2.
The proofs of Theorems 3 and 4, as well as the selection of the hyperparameter λin these two
theorems, are shown in Appendix N.2. The theorems derive the upper bounds of the TEOGs
between the parameter adapted by one-time policy adaptation from the produced meta-parameter
ϕtand the task-specific optimal parameter θ∗
τ. It is shown that, with at most Titerations, we can
achieve the upper bounds in the order of O(hi(1√
T) +Var(P(Γ))) . In other words, there exists a
t≤TwithEτ∼P(Γ)[Jτ(Alg(ˆπϕt, λ, τ))]≥Eτ∼P(Γ)[Jτ(ˆπθ∗τ)]− O(hi(1√
T) +Var(P(Γ))) . As the
number of iterations Tincreases, or the variance of the task distribution Var(P(Γ)) reduces, the
optimality of the meat-parameter ϕtimproves. The second term Var(P(Γ)) in the upper bounds
of Theorems 3 and 4 corresponds the intuition of meta-learning, which is that, if the variance of
a task distribution is smaller, the meta-policy learned from the task distribution is more helpful
for new tasks in the task distribution, then the performance is better. Moreover, this term shows
that the learned meta-policy achieves a better performance than the meta-policy ϕcenterdefined by
arg min ϕEτ∼P(Γ)[D2
τ,i(πϕ, πθ∗τ)], which is the center of all the task-specific optimal policies πθ∗τ.
The order of our upper bounds are comparable to O(T−1
4+Var(P(Γ)) that is shown in [ 31]. On
the other hand, compared with [ 31], in this paper, the constants in the notation Oonly consist of γ,
rmax,Amax, and the Lipschitz constants of f, and do not rely on |A|and|S|. As a result, our upper
bounds are tighter when handling high-dimensional problems or continuous spaces.
Monotonic improvement of the within-task algorithm. Another benefit from Lemmas 1 and 2
and the idea of MM used by the within-task algorithm is that, the policy update by the within-task
algorithm monotonically improves, i.e., Jτ(Alg(i)(ˆπθ, λ, τ))≥Jτ(ˆπθ)fori= 1,2and3and any θ
and any task τ. Therefore, multiple times of Algalways perform better than one-time Alg.
6 Experiments
6.1 Verification of theoretical results
We conduct an experiment to verify the optimality bounds of Algorithm 1 shown in Theorems 3 and
4. We consider two scenarios of the Frozen Lake environment in Gym: two task distributions with
a high task variance and a low task variance. More details of the setting and the hyperparameter
selection are shown in Appendix A. We consider the within-task algorithm Alg(i)for all i= 1,2and
3, where the results of i= 2and3are shown in Appendix A.
90 1 2 3 4
Number of policy adaptation steps-0.50.00.51.01.5Expected Accumulated reward
High task variance ((1) applied)
BO-MRL (ours)
Random initialization
MAML
Optimal task-specific policies
0 1 2 3 4
Number of policy adaptation steps0.00.51.01.5Expected Accumulated reward
Low task variance ((1) applied)
BO-MRL (ours)
Random initialization
MAML
Optimal task-specific policies
No
 adaptationOne-time
 of (1)
One-step of
 policy gradient0.00.20.40.60.81.01.2Expected Optimality GapUpper bound for
 one-time (1)
High task variance ((1) applied)
No
 adaptationOne-time
 of (1)
One-step of
 policy gradient0.000.050.100.15Expected Optimality GapUpper bound for
 one-time (1)
Low task variance ((1) applied)
Figure 1: Results of the meta-test on Frozen Lake, where Alg(1)is applied. Left: Average accumulated reward
across all test tasks v.s. number of policy adaptation steps; Right : Comparing the expected optimality gap by the
BO-MRL and baselines with the upper bound of the accumulated reward of one-time Alg(1).
0 1 2 3
Number of policy adaptation steps-160-140-120-100-80-60-40Accumulated reward
Half-cheetah, goal velocity
BO-MRL with (1)
BO-MRL with (2)
BO-MRL with (3)
MAML-TRPO
ProMP
E-MAML
0 1 2 3
Number of policy adaptation steps0100200300400500600Accumulated reward
Half-cheetah, moving direction
BO-MRL with (1)
BO-MRL with (2)
BO-MRL with (3)
MAML-TRPO
ProMP
E-MAML
0 1 2 3
Number of policy adaptation steps406080100120Accumulated reward
Ant, goal velocity
BO-MRL with (1)
BO-MRL with (2)
BO-MRL with (3)
MAML-TRPO
0 1 2 3
Number of policy adaptation steps0100200300400500600700Accumulated reward
Ant, moving direction
BO-MRL with (1)
BO-MRL with (2)
BO-MRL with (3)
MAML-TRPO
ProMP
E-MAML
Figure 2: Average accumulated reward across all test tasks during the meta-test under the practical algorithm of
BO-MRL on the locomotion tasks.
We compare our algorithm with MAML [ 15] and the random initialization. Figure 1 shows that,
for Algorithm 1 with the within-task algorithm Alg(1), it outperforms the baseline methods. For all
scenarios, the expected optimality gap of the one-time policy adaptation is smaller than the upper
bounds shown in Theorems 3 and 4, which verify our theoretical analysis. Moreover, in Figure 1,
the expected optimality gap of the policy adaptation is better (smaller) but close to the upper bound,
while that of the other policy adaptation approach, the policy gradient, is worse (larger) than the
upper bound. It shows that the derived upper bound is tight.
6.2 High-dimensional Experiment
To evaluate the proposed practical algorithm, Algorithm 2 in Appendix D, we conduct experiments
on high-dimensional locomotion settings in the MuJoCo simulator, including Half-Cheetah with goal
directions and goal velocities, Ant with goal directions and goal velocities. We compare the proposed
algorithm with several optimization-based meta-RL algorithms, including MAML, E-MAML [ 55],
and ProMP [ 50]. For the fairness of the comparison, all the methods share the same data requirement
and task setting. More details of the task setting, the hyperparameter selection, and the supplemental
results are shown in Appendix B.
Figure 2 shows that the proposed algorithm with the within-task algorithms Alg(i)outperforms
the baseline methods in all four experimental settings. For example, we achieve about 25% of
performance improvement in Half-cheetah direction and Ant direction experiments. Moreover,
compared with the baseline methods, the proposed algorithm achieves more policy improvement
when more policy optimization steps are given. For example, our approach achieves about 10% of
performance improvement in the second policy optimization step, while those of baseline methods
are almost 0%.
7 Conclusion
This paper develops a bilevel optimization framework for meta-RL, which implements multiple-step
policy optimization on one-time data collection during task-specific policy adaptation. Beyond
existing meta-RL analyses, we provide upper bounds of the expected optimality gap over the task
distribution. Our experiments validate the bounds derived from our theoretical analysis and show the
superior effectiveness of the proposed framework.
10Acknowledgments and Disclosure of Funding
This work is partially supported by the National Science Foundation through grants ECCS 1846706
and ECCS 2140175. We would like to thank the reviewers for their constructive and insightful
suggestions.
References
[1]Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization.
InInternational conference on machine learning , pages 22–31. PMLR, 2017.
[2]Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. The Journal of Machine
Learning Research , 22(1):4431–4506, 2021.
[3]Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. Meta reinforcement learning
for sim-to-real domain adaptation. In 2020 IEEE International Conference on Robotics and
Automation , pages 2725–2731. IEEE, 2020.
[4]Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-
based meta-learning. In International Conference on Machine Learning , pages 424–433. PMLR,
2019.
[5]Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and
Shimon Whiteson. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028 ,
2023.
[6]Suneel Belkhale, Rachel Li, Gregory Kahn, Rowan McAllister, Roberto Calandra, and Sergey
Levine. Model-based meta-reinforcement learning for flight with suspended payloads. IEEE
Robotics and Automation Letters , 6(2):1471–1478, 2021.
[7]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540 , 2016.
[8]Imre Csiszár and János Körner. Information theory: coding theorems for discrete memoryless
systems . Cambridge University Press, 2011.
[9]Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn
stochastic gradient descent with biased regularization. In International Conference on Machine
Learning , pages 1566–1575. PMLR, 2019.
[10] Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil. Online-within-online
meta-learning. Advances in Neural Information Processing Systems , 32, 2019.
[11] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2:
Fast reinforcement learning via slow reinforcement learning. In International Conference on
Learning Representations , 2017.
[12] Alireza Fallah, Kristian Georgiev, Aryan Mokhtari, and Asuman Ozdaglar. On the conver-
gence theory of debiased model-agnostic meta-reinforcement learning. Advances in Neural
Information Processing Systems , 34:3096–3107, 2021.
[13] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-
based model-agnostic meta-learning algorithms. In International Conference on Artificial
Intelligence and Statistics , pages 1082–1092. PMLR, 2020.
[14] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Generalization of model-agnostic meta-
learning algorithms: Recurring and unseen tasks. Advances in Neural Information Processing
Systems , 34:5469–5480, 2021.
[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In International conference on machine learning , pages 1126–1135.
PMLR, 2017.
11[16] Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and
gradient descent can approximate any learning algorithm. In International Conference on
Learning Representations , 2018.
[17] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and
reverse gradient-based hyperparameter optimization. In International Conference on Machine
Learning , pages 1165–1173. PMLR, 2017.
[18] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.
Bilevel programming for hyperparameter optimization and meta-learning. In International
Conference on Machine Learning , pages 1568–1577. PMLR, 2018.
[19] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization , 23(4):2341–2368, 2013.
[20] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv
preprint arXiv:1802.02246 , 2018.
[21] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and
Edison Guo. On differentiating parameterized argmin and argmax problems with application to
bi-level optimization. arXiv preprint arXiv:1607.05447 , 2016.
[22] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration
complexity of hypergradient computation. In International Conference on Machine Learning ,
pages 3748–3758. PMLR, 2020.
[23] Magnus R Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving. Journal of
research of the National Bureau of Standards , 49(6):409, 1952.
[24] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic
algorithm framework for bilevel optimization: Complexity analysis and application to actor-
critic. SIAM Journal on Optimization , 33(1):147–180, 2023.
[25] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in
neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence ,
44(9):5149–5169, 2021.
[26] Yu Huang, Yingbin Liang, and Longbo Huang. Provable generalization of overparameterized
meta-learning trained with SGD. Advances in Neural Information Processing Systems , 35:16563–
16576, 2022.
[27] Mike Huisman, Jan N Van Rijn, and Aske Plaat. A survey of deep meta-learning. Artificial
Intelligence Review , 54(6):4483–4541, 2021.
[28] David R Hunter and Kenneth Lange. A tutorial on mm algorithms. The American Statistician ,
58(1):30–37, 2004.
[29] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and
enhanced design. In International Conference on Machine Learning , pages 4882–4892. PMLR,
2021.
[30] Sham M Kakade. A natural policy gradient. Advances in neural information processing systems ,
14, 2001.
[31] Vanshaj Khattar, Yuhao Ding, Javad Lavaei, and Ming Jin. A CMDP-within-online framework
for meta-safe reinforcement learning. In International Conference on Learning Representations ,
2023.
[32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[33] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 , 2013.
12[34] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems , pages 1008–1014, 2000.
[35] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning
with differentiable convex optimization. In 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 10649–10657, 2019.
[36] Thomas Lew, Apoorva Sharma, James Harrison, Andrew Bylard, and Marco Pavone. Safe
active dynamics learning and control: A sequential exploration–exploitation framework. IEEE
Transactions on Robotics , 2022.
[37] Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy
optimization attains globally optimal policy. Advances in neural information processing systems ,
32, 2019.
[38] Hao Liu, Richard Socher, and Caiming Xiong. Taming MAML: Efficient unbiased meta-
reinforcement learning. In International conference on machine learning , pages 4061–4071.
PMLR, 2019.
[39] Shicheng Liu and Minghui Zhu. Distributed inverse constrained reinforcement learning for
multi-agent systems. Advances in Neural Information Processing Systems , 35:33444–33456,
2022.
[40] Shicheng Liu and Minghui Zhu. Learning multi-agent behaviors from distributed and streaming
demonstrations. In Thirty-seventh Conference on Neural Information Processing Systems , 2023.
[41] Shicheng Liu and Minghui Zhu. Meta inverse constrained reinforcement learning: Convergence
guarantee and generalization analysis. In The Twelfth International Conference on Learning
Representations , 2023.
[42] Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea
Finn. Guided meta-policy search. Advances in Neural Information Processing Systems , 32,
2019.
[43] Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes.
InProceedings of the 29th International Coference on International Conference on Machine
Learning , pages 1451–1458, 2012.
[44] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine,
and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-
reinforcement learning. In International Conference on Learning Representations , 2018.
[45] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International
Conference on Machine Learning , pages 737–746. PMLR, 2016.
[46] Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. On finite-time convergence of
actor-critic algorithm. IEEE Journal on Selected Areas in Information Theory , 2(2):652–664,
2021.
[47] Roberta Raileanu, Max Goldstein, Arthur Szlam, and Rob Fergus. Fast adaptation to new
environments via policy-dynamics value functions. In Proceedings of International Conference
on Machine Learning , pages 7920–7931, 2020.
[48] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with
implicit gradients. Advances in neural information processing systems , 32, 2019.
[49] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient
off-policy meta-reinforcement learning via probabilistic context variables. In International
Conference on Machine Learning , pages 5331–5340. PMLR, 2019.
[50] Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal
meta-policy search. In International Conference on Learning Representations , 2019.
13[51] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
region policy optimization. In International conference on machine learning , pages 1889–1897.
PMLR, 2015.
[52] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
[53] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-
propagation for bilevel optimization. In The 22nd International Conference on Artificial
Intelligence and Statistics , pages 1723–1732. PMLR, 2019.
[54] Xingyou Song, Yuxiang Yang, Krzysztof Choromanski, Ken Caluwaerts, Wenbo Gao, Chelsea
Finn, and Jie Tan. Rapidly adaptable legged robots via evolutionary meta-learning. In 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems , pages 3769–3776. IEEE,
2020.
[55] Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and
Ilya Sutskever. Some considerations on learning to explore via meta-reinforcement learning.
arXiv preprint arXiv:1803.01118 , 2018.
[56] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press,
2018.
[57] Yunhao Tang. Biased gradient estimate with drastic variance reduction for meta reinforcement
learning. In Proceedings of the 39th International Conference on Machine Learning , volume
162 of Proceedings of Machine Learning Research , pages 21050–21075. PMLR, 17–23 Jul
2022.
[58] Sebastian Thrun and Lorien Pratt. Learning to learn . Springer Science & Business Media,
2012.
[59] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.
arXiv preprint arXiv:1611.05763 , 2016.
[60] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. On the global optimality of model-
agnostic meta-learning. In International conference on machine learning , pages 9837–9846.
PMLR, 2020.
[61] Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two
time-scale actor-critic methods. Advances in Neural Information Processing Systems , 33:17617–
17628, 2020.
[62] Zheng Xiong, Luisa M Zintgraf, Jacob Austin Beck, Risto Vuorio, and Shimon Whiteson. On
the practical consistency of meta-reinforcement learning algorithms. In Fifth Workshop on
Meta-Learning at the Conference on Neural Information Processing Systems , 2021.
[63] Siyuan Xu and Minghui Zhu. Meta value learning for fast policy-centric optimal motion
planning. Robotics Science and Systems , 2022.
[64] Siyuan Xu and Minghui Zhu. Efficient gradient approximation method for constrained bilevel
optimization. Proceedings of the AAAI Conference on Artificial Intelligence , 37(10):12509–
12517, 2023.
[65] Siyuan Xu and Minghui Zhu. Online constrained meta-learning: Provable guarantees for
generalization. In Thirty-seventh Conference on Neural Information Processing Systems , 2023.
[66] Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement
learning with convergence guarantee. In International Conference on Machine Learning , pages
11480–11491. PMLR, 2021.
[67] L Zintgraf, K Shiarlis, M Igl, S Schulze, Y Gal, K Hofmann, and S Whiteson. Varibad: a very
good method for bayes-adaptive deep rl via meta-learning. Proceedings of ICLR 2020 , 2020.
[68] Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep rl via meta-
learning. In International Conference on Learning Representations , 2019.
14Appendix for "Meta-Reinforcement Learning with Universal
Policy Adaptation: Provable Near-Optimality under All-task
Optimum Comparator"
Experimental Supplements
All experiments are executed on a computer with a 5.20 GHz Intel Core i12 CPU.
A Experimental Supplements of Verification of Theoretical Results.
Experimental settings. In Section 6, we use the Frozen Lake environment in Gym [ 7] and consider
a task distribution P(Γ)with high task variance and a task distribution P(Γ)with low task variance.
In each distribution, there are 20tasks. The tasks are characterized by the different settings of holes
in the lake, where the holes are generated by random sampling. In the task distribution with high
variance, the probability of the appearing hole in each grid is 0.3; in the task distribution with low
variance, its probability is 0.1. We set γ= 0.8, the reward is 1when reaching the goal, and the
reward is −1when reaching the holes. When deriving the upper bound in Theorems 3 and 4, we
approximately regard Tbe sufficiently large, and O(hi(1√
T))be close to 0. The Lipschitz of the
tabular policy is 1, i.e., L1= 1; the Lipschitz of the derivative and the second-order derivative of the
tabular policy are both 0, i.e., L2= 0andL3= 0.
Selection of hyper-parameters. We consider the tabular softmax policy and use Monte Carlo
sampling to evaluate the Q-value. For the task distribution with high task variance, we set λ= 0.5
forAlg(1),λ= 0.5forAlg(2), and λ= 0.04forAlg(3). For the task distribution with low task
variance, we set λ= 0.25forAlg(1),λ= 0.25forAlg(2), and λ= 0.02forAlg(3). There is a
clarification about the hyper-parameter selection and the verified bound shown in Appendix N.3.
Supplemental results. Figures 3 and 4 show the results of the proposed algorithm with Alg(2)and
Alg(3). It shows that, for all scenarios, the expected optimality gap of the policy adaptation Alg(2)
orAlg(3)is smaller than the upper bound shown in Theorems 3 and 4, which verify our theoretical
analysis.
0 1 2 3 4
Number of policy adaptation steps0.00.51.01.5Expected Accumulated reward
High task variance ((2) applied)
BO-MRL (ours)
Random initialization
MAML
Optimal task-specific policies
No
 adaptationOne-time
 of (2)
One-step of
 policy gradient0.000.250.500.751.001.25Expected Optimality GapUpper bound for
 one-time (2)
High task variance ((2) applied)
0 1 2 3 4
Number of policy adaptation steps0.00.51.01.5Expected Accumulated reward
Low task variance ((2) applied)
BO-MRL (ours)
Random initialization
MAML
Optimal task-specific policies
No
 adaptationOne-time
 of (2)
One-step of
 policy gradient0.000.050.100.15Expected Optimality GapUpper bound for
 one-time (2)
Low task variance ((2) applied)
Figure 3: Results of the meta-test of BO-MRL on Frozen Lake, where Alg(2)is applied. Left: Average
accumulated reward across all test tasks v.s. number of policy adaptation steps; Right : Comparing the expected
optimality gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time
Alg(2).
No
 adaptationOne-time
 of (3)
One-step of
 policy gradient0.00.51.01.5Expected Optimality GapUpper bound for
 one-time (3)
High task variance ((3) applied)
No
 adaptationOne-time
 of (3)
One-step of
 policy gradient0.00.20.40.6Expected Optimality GapUpper bound for
 one-time (3)
Low task variance ((3) applied)
Figure 4: Results of BO-MRL on Frozen Lake, where Alg(3)is applied. Comparing the expected optimality
gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time Alg(3).
15B Experimental Supplements of Locomotion.
Experimental settings. We consider locomotion tasks HalfCheetah with goal directions and goal
velocities, Ant with goal directions and goal velocities. We follow the problem setups of [ 67,15]. In
the goal velocity experiments, the moving reward is the negative absolute value between the agent’s
current velocity and a goal velocity, which is chosen uniformly at random between 0.0and2.0for the
cheetah and between 0.0and3.0for the ant. In the goal direction experiments, the moving reward is
the magnitude of the velocity in either the forward or backward direction, chosen at random for each
taskτinP. For the Half-cheetah, the total reward = moving reward - ctrl cost. For the ant, the total
reward = healthy reward + moving reward - ctrl cost - contact cost. The horizon is H= 200 , with 20
rollouts per policy adaption step for all problems except the ant direction task, which used 40rollouts
per step.
Selection of hyper-parameters. We apply the proposed practical algorithm of Algorithm 1, Algo-
rithm 2 in Appendix D. We consider the policy as a Gaussian distribution, where the neural network
produces the means and variances of the actions. The neural network policy has two hidden layers
of size 64, with tanh nonlinearities. We use Monte Carlo sampling to evaluate the Q-value. At the
lower-level task-specific policy adaptation, the optimization number by Adam is 50. The models
are trained for up to 500 meta-iterations. For the TRPO in meta-parameter optimization, we use the
KL-divergence constraint as δ= 1e−3.
For the experiment of Half-Cheetah with goal velocities, we set λ= 0.5forAlg(1),λ= 0.4for
Alg(2). For the experiment of Half-Cheetah with goal directions, we set λ= 0.5forAlg(1),λ= 0.5
forAlg(2). For the experiment of Ant with goal velocities, we set λ= 0.5forAlg(1),λ= 0.5for
Alg(2). For the experiment of Ant with goal directions, we set λ= 0.5forAlg(1),λ= 0.5for
Alg(2).
Comparison setting. We compare the proposed algorithm with several optimization-based meta-RL
algorithms, including MAML, E-MAML [ 55], and ProMP [ 50]. The experiment results of E-MAML,
ProMP, and MAML-TRPO come from [ 67,15]. We do not compare the proposed algorithm with
black-box meta-RL algorithms, as they are based on the task context and even can achieve good
performance without adaptation.
Supplemental results. Figure 5 shows that the proposed algorithm with both within-task algorithms
Alg(i)outperform the baseline methods in four experimental settings. The accumulated rewards
of proposed algorithms increase fast and stop at points with better performance than the baseline
methods.
0 60 120 180 240 300 360 420 480
Number of meta-training iterations-250-225-200-175-150-125-100-75Accumulated rewardHalf-cheetah, goal velocity
BO-MRL with (1)
BO-MRL with (2)
BO-MRL with (3)
MAML-TRPO
0 60 120 180 240 300 360 420 480
Number of meta-training iterations0100200300400500600Accumulated rewardHalf-cheetah, goal direction
BO-MRL with (1)
BO-MRL with (2)
BO-MRL with (3)
MAML-TRPO
0 60 120 180 240 300 360 420 480
Number of meta-training iterations-50-250255075100125Accumulated rewardAnt, goal velocity
BO-MRL with (1)
BO-MRL with (2)
BO-MRL with (3)
MAML-TRPO
0 50 100 150 200 250 300 350 400
Number of meta-training iterations100200300400500600Accumulated rewardAnt, goal direction
BO-MRL with (1)
BO-MRL with (2)
BO-MRL with (3)
MAML-TRPO
Figure 5: Accumulated rewards during the meta-training under the practical algorithm of BO-MRL on the
locomotion tasks.
Algorithm supplement
C Computation of ∇ϕQπϕ
τ(s, a)
In the computation of meta-objective shown in Propositions 1 and 2, we need to compute ∇ϕQπϕτ(s, a)
∇ϕQπϕτ(s, a) =γ
1−γ·E(s′,a′)∼σ(s,a)
τ,πϕ
∇ϕlnπϕ(a′|s′)Qπϕτ(s′, a′)
.
where the state-action visitation probability σ(s,a)
τ,πθinitialized at (s, a)∈ S × A is defined by
σ(s,a)
τ,πϕ(s′, a′) = (1 −γ)∞X
t=0γtP(st=s′, at=a′|πϕ, s0∼Pτ(·|s, a)).
16For the tabular softmax policy in discrete state-action space shown in Section 5.1,
∇ϕ(s′,·)Qˆπϕτ(s, a) =γ
1−γ·σ(s,a)
τ,ˆπϕ(s′)·ˆπϕ(·|s′)⊙Aˆπϕτ(s′,·), (4)
where ⊙is the element-wise product, ϕ(s′,·)is the vector which includes ϕ(s′, a′)for all a′∈ A
as the elements, and Aˆπϕτ(s,·)is the vector which includes Aˆπϕτ(s, a)for all a∈ A as the elements.
Equivalently,
∇ϕ(s′,a′)Qˆπϕτ(s, a) =γ
1−γ·σ(s,a)
τ,ˆπϕ(s′)ˆπϕ(a′|s′)Aˆπϕτ(s′, a′). (5)
For the softmax policy with the function approximation,
∇ϕQˆπϕτ(s, a) =γ
1−γ·E(s′,a′)∼σ(s,a)
τ,ˆπϕ∇ϕˆπϕ(a′|s′)
ˆπϕ(a′|s′)Qˆπϕτ(s′, a′)
=γ
1−γ·E(s′,a′)∼σ(s,a)
τ,ˆπϕh
∇ϕfϕ(s′, a′)Qˆπϕτ(s′, a′)i
=γ
1−γ·E(s′,a′)∼σ(s,a)
τ,ˆπϕh
∇ϕfϕ(s′, a′)Aˆπϕτ(s′, a′)i(6)
Proof. As shown in [60],
∇ϕQπϕτ(s, a) =∇ϕ 
(1−γ)·rτ(s, a) +γ·Es′∼Pτ(·|s,a)
Vπϕτ(s′)
=γ
1−γ·E(s′,a′)∼σ(s,a)
τ,πϕ
∇ϕlnπϕ(a′|s′)·Qπϕτ(s′, a′)
=γ
1−γ·E(s′,a′)∼σ(s,a)
τ,πϕ
∇ϕlnπϕ(a′|s′)·Aπϕτ(s′, a′)
.
=γ
1−γ·E(s′,a′)∼σ(s,a)
τ,πϕ∇ϕπϕ(a′|s′)
πϕ(a′|s′)·Aπϕτ(s′, a′)
.
By Lemma 4, from (12), we can obtain (4); from (14), we can obtain (6).
D Practical algorithm
In Sections 4 and 5, we develop a theoretically guaranteed algorithm with Assumptions 1, 2, and 3.
In this section, we develop a practical instantiation of Algorithm 1 and evaluate its performance in
high-dimensional experiments in Section 6.
Algorithm 2 states the practical algorithm of Algorithm 1. Compared with Algorithm 1, Algorithm 2
considers and overcomes the following limitations of Algorithm 1: (a) evaluating the exact expectation
in (1) and (2) is costly and the approximation error could influence the task-specific policy adaptation
if using sampling, especially in the meta-RL problem where the sampling data is limited; (b) the
optimization problems in (1) and (2) have no closed-form solution; (c) the computation of the gradients
of the meta-objectives shown in Propositions 1 and 2 is time-consuming; (d) the gradient-based
approach to optimize the meta-objective is not stable in RL problems.
In the beginning of Algorithm 2, we first sample a batch of tasks {τi}N
i=1∼P(Γ). On each task τi,
we sample the trajectories of the meta-policy πϕtasBτi, and evaluate the state-action value function
Qπϕtτi(·,·)for each τi. Next, since the number of the sampling state-action pairs in Bτiis limited, if
we directly use the sampling average to approximate the expectation in (2), the approximation error
will be very large when πϕ(a|s)is small. Therefore, we solve the following optimization problem as
the within-task algorithm instead of (2):
πθ′τ=Alg(λ, ϕt, τ) = arg min
θ1
|Bτ|X
(a,s)∈Bτhπθ(a|s)
πϕ(a|s)
Qπϕτ(s, a)−λD2
τ(πϕ, πθ),(7)
where h(x) =2
1+e−2(x−1). The function havoids the termπθ(a|s)
πϕ(a|s)is optimized to very large. We
use Adam [ 32] to solve the problem in (7). Next, the computation of the gradients of the meta-
objectives shown in Proposition 2 is time-consuming, since the computation complexity of the term
17Algorithm 2 Practical Algorithm of BO-MRL
Require: Regularization weight λ >0; initial meta-parameter ϕ0; learning rate α.
1:fort= 1,···, Tdo
2: Sample a batch of tasks {τi}N
i=1∼P(Γ)with the MDP Mτii.i.d.
3: On each task τi, sample the trajectories of the meta-policy πϕtasBτi.
4: Evaluate the state-action value function Qπϕtτi(·,·)for each τi.
5: For each task τi, compute the task-specific policy πθ′τiby solving Alg(λ, ϕt, τi)defined in (7)
by Adam.
6: Compute ∇ϕJτi(πθ′τi)in (8) by conjugate gradient method
7: Update meta-parameter by the TRPO with the gradient1
NP
i∇ϕJτi(πθ′τi)and the sampling
trajectories {Bτi}N
i=1.
8:end for
9:Return ϕT
−∇θπθ(a|s)
λπϕ(a|s)∇⊤
ϕQπϕτ(s, a)is very high. So, we omit the term, and compute ∇ϕJτ(πθ′τ)as
1
1−γ∇ϕθ′
τ·E
s∼νπθ′ττ
a∼πθ′τ(·|s)∇θ′τπθ′τ(a|s)
πθ′τ(a|s)Qπθ′ττ(s, a)
,(8)
where
∇⊤
ϕθ′
τ≈ − E
s∼νπϕ
τ
a∼πϕ(·|s)
∇2
θd2(πϕ(·|s), πθ(·|s))−∇2
θπθ(a|s)
λπϕ(a|s)Qπϕτ(s, a)−1
E
s∼νπϕ
τ
a∼πϕ(·|s)
∇⊤
ϕ∇θd2(πϕ(·|s), πθ(·|s))
|θ=θ′τ.
Finally, since the gradient-based approach is not stable in RL problems, we optimize meta-parameter
by the TRPO with the gradient1
NP
i∇ϕJτi(πθ′
τi)and the sampling trajectories {Bτi}N
i=1, similar
to [15].
E Discussion about computational complexity of hyper-gradient
In Algorithms 1 and 2, we compute the inverse of the Hessian matrix when computing the hyper-
gradient by Proposition 2 and (8). The computation of the inverse of the Hessian matrix is not
time-consuming and does not increase the processing time much. Here are the two reasons.
First, we apply the conjugate gradient algorithm to compute the inverse of the Hessian and its
computation complexity is not high. According to our experiment of Half-cheetah, the computation
time of the hyper-gradient with the inverse of Hessian for a three-layer neural network is about 0.3
second in each meta-parameter update, where we use only the CPU to compute the hyper-gradient.
This approach has demonstrated high efficiency across a wide range of applications, including several
widely used RL algorithms, such as TRPO [ 51] and CPO [ 1], which compute the inverse of the
Hessian in each policy update iteration. The detail is shown in Appendix C of [ 51]. They usually
compute thousands times of the Hessian inverse for a single RL task. In the simplest meta-RL method,
MAML [ 15], the authors use the TRPO to update the meta-parameter, as shown in Section 5.3 of
[15], the inverse of the Hessian is also computed. Therefore, the computational complexity of the
hyper-gradient in our proposed method is comparable to many existing RL and meta-RL approaches,
which are shown efficient.
Second, the biggest computational bottleneck in the meta-RL framework is not the hyper-gradient
computation. According to our experiment, the percentage of the computation time in the meta-
parameter update, including the computation time of the hyper-gradient computation, is less than
5%, where we use only the CPU to compute the hyper-gradient. The percentage of computation
time in the data collection and the Q value computation by Monte-Carlo sampling is more than 70%,
although the state-action data points are collected in the MDP simulator Gym and the data collection
is very fast. In real-world applications, the state-action data points are even harder to collect and
18data collection consumes a longer time. Therefore, the computational time of the hyper-gradient
computation has a relatively small impact on the mete-RL framework.
FData sampling complexity and computational complexity of one-time policy
adaptation
The one-time policy adaptation in our algorithm is defined as solving the optimal solution of the
optimization problem in (1)or(2)by multiple optimization iterations. The definition of the one-time
policy adaptation follows many widely used RL algorithms, such as TRPO [ 51] and CPO [ 1], which
evaluate the Q-values for the current policy and solve the optimal solution for an optimization
problem to obtain the next policy in each policy optimization iteration. For example, TRPO solves
the optimization problem in (14) of [51] in each iteration.
In the one-time policy adaptation, we only need to evaluate the Q-function for one policy πϕby
Monte-Carlo sampling, which requires the agent to explore the MDP using one policy πϕ, then solve
the optimization problem in (1)or(2)by multiple optimization iterations with the fixed Q-function.
The data sampling complexity is exactly the same as the one-step gradient descent in MAML, which
uses Monte-Carlo sampling to evaluate the Q-function and compute the policy gradient based on the
Q-function.
The multiple optimization steps in the one-time policy adaptation are different from the multi-step
policy gradient update in MAML. In our algorithm, the multiple optimization steps in a one-time
policy adaptation only need to evaluate the Q-function for one policy πϕ, which requires the agent to
explore the MDP using only πϕ. In MAML, the Q-function for a new policy needs to be evaluated
in each policy gradient update, and then multiple Q-functions are evaluated for multiple policies,
which requires the agent to explore the MDP using multiple policies. Instead, the one-time policy
adaptation in our algorithm corresponds to a one-step policy gradient update in MAML, as they use
the same number of data points.
Moreover, we would like to claim that the computation complexity for the one-time policy adaptation
in our algorithm and that of the one-step policy gradient update in MAML is comparable, although our
algorithm requires multiple optimization iterations. As mentioned in Appendix E, the computation
time in the data collection and the Q value computation takes more than 70% of total computation
time, which is much longer than other parts of the algorithm, including the multiple optimization
iterations in police adaptation ( 15% of total computation time). This happens although the state-action
data points are collected in the MDP simulator Gym and the data collection is very fast. In real-world
applications, the state-action data points are even harder to collect and the consuming time of data
collection is much longer. Therefore, the computational time of the multiple optimization iterations
has a relatively small impact on the mete-RL framework. Therefore, the computation time of our
algorithm and that of MAML is comparable.
From the statement in the above paragraphs, both the data sampling complexity and computational
complexity of the one-time policy adaptation in our algorithm and the one-step policy gradient update
in MAML are similar. Thus, we define solving the optimal solution of the optimization problem in
(1) or (2) as a single policy adaptation step.
G Algorithm details with the first-order approximation
As we mentioned in Section 4, we can approximate the first term Es∼νπϕ
τ,a∼πϕ(·|s)[πθ(a|s)
πϕ(a|s)Qπϕτ(s, a)]
in (2) by its first-order approximation as the within-task algorithm, similar to the implementations
in TRPO [ 51] and PPO [ 52]. In particular, the within-task algorithm is reduced to the following
formulation,
πθ′τ=Alg(πϕ, λ, τ)≜argmin
πθ−1
λG(ϕ)⊤θ+D2
τ(πϕ, πθ). (9)
Here, we use the first-order approximation to replace the first term of (2). In particular,
G(ϕ)⊤(θ−ϕ)is the first order approximation of Es∼νπϕ
τ,a∼πϕ(·|s)[πθ(a|s)
πϕ(a|s)Qπϕτ(s, a)], where
G(ϕ) =∇θEs∼νπϕ
τ,a∼πϕ(·|s)[∇θπθ(a|s)
πϕ(a|s)Qπϕτ(s, a)]|θ=ϕ=Es∼νπϕ
τ,a∼πϕ(·|s)[∇ϕπϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)].
Under the simplified within-task algorithm Alg, the hypergradient of the meta-objective function
19∇ϕJτ(πθ′τ)can be computed by
∇ϕJτ(πθ′τ) =1
1−γ∇ϕθ′
τ·E
s∼νπθ′ττ
a∼πθ′τ(·|s)∇θ′τπθ′τ(a|s)
πθ′τ(a|s)Qπθ′ττ(s, a)
,
where
∇⊤
ϕθ′
τ=∇2
θ′
τD2
τ(πϕ, πθ′τ)−1( E
s∼νπϕ
τa∼πϕ(·|s)[1
λ∇ϕπϕ(a|s)
πϕ(a|s)∇⊤
ϕQπϕτ(s, a)+
1
λ∇2
ϕπϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)]− ∇⊤
ϕ∇θ′τD2
τ(πϕ, πθ′τ)).(10)
The computation of ∇⊤
ϕθ′
τis derived in Section J.3.
H Connection between the proposed algorithm and MAML
As we claim in Section 4, when we approximate the first term Es∼νπϕ
τ,a∼πϕ(·|s)[πθ(a|s)
πϕ(a|s)Qπϕτ(s, a)]
in (2) by its first-order approximation and also select Dτ=Dτ,3, the within-task algorithm (2)
is reduced to the policy gradient ascent. In particular, the term Es∼νπϕ
τ,a∼πϕ(·|s)[πθ(a|s)
πϕ(a|s)Qπϕτ(s, a)]
is approximated by (θ−ϕ)⊤Es∼νπϕ
τ,a∼πϕ(·|s)[∇πϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)], then the within-task algorithm
Alg(πϕ, λ, τ)becomes to
θ′
τ=Alg(πϕ, λ, τ)≜argmax
θ−λ∥θ−ϕ∥2+θ⊤·E
s∼νπϕ
τ
a∼πϕ(·|s)∇ϕπϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)
.(11)
Solve the optimization problem, we have
θ′
τ=ϕ+1
λE
s∼νπϕ
τ
a∼πϕ(·|s)∇ϕπϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)
=ϕ+1−γ
λ∇ϕJτ(ϕ),
which is policy gradient ascent. Thus, when we select (11) as the within-task algorithm, the meta-
algorithm (3) is reduced to the algorithm that can learn the initialization parameter for the policy
gradient ascent.
As shown in [ 15], MAML also learns the initialization parameter ϕfor the policy gradient ascent.
However, MAML ignores that the sampled trajectories with policy πϕalso depend on ϕ. Specifically,
MAML first uses the sampled trajectories to approximate Qπϕτ(s, a)by (Monte Carlo sampling on
the REINFORCE algorithm), then computes the policy gradient and does one step of gradient ascent
for the task-specific adaptation. Next, it computes ∇ϕJτ(θ′
τ)to update the meta-parameter ϕ. When
it computes ∇ϕθ′
τ, it treats Qπϕτ(s, a)as a given data point that is independent with ϕ, and then
ignore the ∇ϕQπϕτ(s, a). In contrast, our reduced meta-algorithm takes it into account and provides a
precise formulation to learn the meta-initialization for the policy gradient algorithm.
Since the proposed meta-RL framework can include MAML as a special case, our analysis in Section
5 also provides the theoretical motivation for MAML.
Analysis and Proof
I Auxiliary Results
Lemma 3 (Policy gradient [ 56,2]).Letπθbe the parameterized policy with the parameter θ. It
holds that
∇θJτ(πθ) =1
1−γEs∼νπθτ,a∼πθ(·|s)[∇θlnπθ(a|s)Qπθτ(s, a)]
=1
1−γEs∼νπθτ,a∼πθ(·|s)[∇θlnπθ(a|s)Aπθτ(s, a)].
20Lemma 4 (Policy gradient of the softmax policy) .Consider the softmax policy ˆπθparameterized by
θ. For a discrete state-action space and the tabular policy, ˆπθ(a|s) =exp(θ(s,a))P
a′∈Aexp(θ(s,a′)),∀(s, a)∈
S × A . It holds that
∇θ(s,·)Jτ(ˆπθ) =1
1−γνˆπθτ(s)·ˆπθ(·|s)⊙Aˆπθτ(s,·), (12)
where⊙is the element-wise product, θ(s,·)is the vector which includes θ(s, a)for all a∈ A as the
elements, Aˆπθτ(s,·)is the vector which includes Aˆπθτ(s, a)for all a∈ A as the elements. Equivalently,
∇θ(s,a)Jτ(ˆπθ) =1
1−γνˆπθτ(s)ˆπθ(a|s)Aˆπθτ(s, a), (13)
For the softmax policy with function approximation, the policy πθis defined by πθ(a|s) =
exp(fθ(s,a))R
Aexp(fθ(s,a′))da′,∀(s, a)∈ S × A . It holds that
∇θJτ(ˆπθ) =1
1−γEs∼νˆπθτ,a∼ˆπθ(·|s)
∇θfθ(s, a)Aˆπθτ(s, a)
. (14)
Proof. For the discrete state-action space and the tabular policy, (12) is shown in Lemma C.1 of [ 2].
For the softmax policy with function approximation, from Lemma 3, we have
∇θJτ(ˆπθ) =1
1−γEs∼νˆπθτ,a∼ˆπθ(·|s)
∇θln ˆπθ(a|s)Aˆπθτ(s, a)
=1
1−γEs∼νˆπθτ,a∼ˆπθ(·|s)
∇θlnexp(fθ(s, a))R
Aexp(fθ(s, a′))da′
Aˆπθτ(s, a)
=1
1−γEs∼νˆπθτ,a∼ˆπθ(·|s)
∇θfθ(s, a)− ∇ θlnZ
Aexp(fθ(s, a′))da′
Aˆπθτ(s, a)
Here,∇θln R
Aexp(fθ(s, a′))da′
is independent with a, then∇θJτ(ˆπθ)
=1
1−γEs∼νˆπθτ,a∼ˆπθ(·|s)
∇θfθ(s, a)− ∇ θlnZ
Aexp(fθ(s, a′))da′
Aˆπθτ(s, a)
=1
1−γEs∼νˆπθτ,a∼ˆπθ(·|s)
∇θfθ(s, a)Aˆπθτ(s, a)
−
1
1−γEs∼νˆπθτ
∇θlnZ
Aexp(fθ(s, a′))da′
Ea∼ˆπθ(·|s)Aˆπθτ(s, a)
.
SinceEa∼ˆπθ(·|s)Aˆπθτ(s, a) =Ea∼ˆπθ(·|s)[Qˆπθτ(s, a)]−Vˆπθτ(s) = 0 . Then,
∇θJτ(ˆπθ) =1
1−γEs∼νˆπθτ,a∼ˆπθ(·|s)
∇θfθ(s, a)Aˆπθτ(s, a)
.
J Proofs of the computation of hypergradient
J.1 Proofs of Propositions 1
Proofs of Propositions 1. Consider the within-task algorithm in discrete space:
Alg(πϕ, λ, τ) = argmax
πEs∼νπϕ
τ"X
a∈Aπ(a|s)Qπϕτ(s, a)#
−λD2
τ(πϕ, π)
= argmax
πEs∼νπϕ
τ"X
a∈Aπ(a|s)Qπϕτ(s, a)−λd2(πϕ(·|s), π(·|s))#
.
Here, dcan be selected from d1tod3defined in Section 3, corresponding to the selection of Dτfrom
Dτ,1toDτ,3.
21The above optimization problem is formally defined by the following problem,
Alg(πϕ, λ, τ) = argmax
πEs∼νπϕ
τ"X
a∈Aπ(a|s)Qπϕτ(s, a)−λd2(πϕ(·|s), π(·|s), s)#
,
subject toX
a∈Aπ(a|s) = 1 ,for any s∈ S.(15)
With Assumption 2, the problem is equivalent to that, for any s∈ S,
Alg(πϕ, λ, τ)(·|s) = argmax
π(·|s)X
a∈Aπ(a|s)Qπϕτ(s, a)−λd2(πϕ(·|s), π(·|s)),
subject toX
a∈Aπ(a|s) = 1 .(16)
Consider a s∈ S, the Lagrangian of the above maximization problem is
−X
a∈Aπ(a|s)Qπϕτ(s, a) +λd2(πϕ(·|s), π(·|s)) +µ(X
a∈Aπ(a|s)−1),
where µis the Lagrangian multiplier. The optimality condition of π(·|s)is that,
−Qπϕτ(s,·) +λ∇π(·|s)d2(πϕ(·|s), π(·|s)) +µ[1,···,1]⊤= 0.
Here, Qπϕτ(s,·)denotes a vector include Qπϕτ(s, a)for each a∈ A, and π(·|s)denotes a vector
include π(a|s)for each a∈ A.
Then, we have
−Qπϕτ(s,·) +λ∇π(·|s)d2(πϕ(·|s), π(·|s))|π=Alg(πϕ,λ,τ)+µ[1,···,1]⊤= 0. (17)
Note that the optimization problem (15) depends on ϕ, andπ=Alg(πϕ, λ, τ)is a function of ϕ, we
have
−Qπϕτ(s,·) +λ∇π(·|s)d2(πϕ(·|s), π(·|s))|π=Alg(πϕ,λ,τ)+µ(ϕ)[1,···,1]⊤= 0,
i.e.,µis a function of ϕ.
Also, we have
µ(ϕ)(X
a∈AAlg(πϕ, λ, τ)(a|s)−1) = 0 . (18)
With (17) and (18), we can compute ∇ϕAlg(πϕ, λ, τ), where Alg(πϕ, λ, τ)is continuously dif-
ferentiable as shown in [ 64]. We do derivative of (17) and (18) with respect to ϕ, we have
[∇ϕAlg(πϕ, λ, τ),∇ϕµ(ϕ)]⊤=
−λ∇2
π(·|s)d2(πϕ(·|s), π(·|s))1
1⊤0−1
−∇⊤
ϕQπϕτ(s,·) +λ∇⊤
ϕ∇π(·|s)d2(πϕ(·|s), π(·|s))
0
where π=Alg(πϕ, λ, τ).
Solve the equation, we have
∇⊤
ϕAlg(πϕ, λ, τ)(·|s) =
M(s)−1−M(s)−11 1⊤M(s)−1
1⊤M(s)−11
 
∇⊤
ϕQπϕτ(s,·)−λ∇⊤
ϕ∇π(·|s)d2(πϕ(·|s), π(·|s))
,(19)
where M(s) =λ∇2
π(·|s)d2(πϕ(·|s), π(·|s)). It is easy to show that ∇2
π(·|s)d2(πϕ(·|s), π(·|s))is
non-singular for any ϕfor any selected d=d1,d=d2, ord=d3.
22From the policy gradient theorem in Lemma 3,
∇ϕJτ(πθ′τ) =1
1−γE
s∼νπθ′ττ,a∼πθ′τ(·|s)[∇ϕlnπθ′τ(a|s)Aπθ′ττ(s, a)]|πθ′τ=Alg(πϕ,λ,τ)
=1
1−γE
s∼νπθ′ττ,a∼πθ′τ(·|s)∇ϕπθ′τ(a|s)
πθ′τ(a|s)Aπθ′ττ(s, a)
|πθ′τ=Alg(πϕ,λ,τ),
=1
1−γE
s∼νπθ′ττ"X
a∈A∇ϕπθ′τ(a|s)Aπθ′ττ(s, a)#
|πθ′τ=Alg(πϕ,λ,τ).
where ∇ϕπθ′τ(·|s) =∇ϕAlg(πϕ, λ, τ)(·|s)is shown in (19).
J.2 Proofs of Propositions 2
Proofs of Propositions 2. First, we have
∇ϕJτ(πθ′τ) =∇ϕθ′
τ∇θ′τJτ(πθ′τ)
From the policy gradient theorem in Lemma 3,
∇ϕJτ(πθ′τ) =1
1−γ∇ϕθ′
τ⊤E
s∼νπθ′ττ,a∼πθ′τ(·|s)h
∇θ′τlnπθ′τ(a|s)Aπθ′ττ(s, a)i
|θ′τ=Alg(πϕ,λ,τ).
We have
∇ϕJτ(πθ′τ) =1
1−γ∇ϕθ′
τ⊤E
s∼νπθ′ττ,a∼πθ′τ(·|s)∇θ′τπθ′τ(a|s)
πθ′τ(a|s)Aπθ′ττ(s, a)
|θ′τ=Alg(πϕ,λ,τ).
Next, we compute ∇ϕθ′
τ, where
θ′
τ=Alg(πϕ, λ, τ)≜argmax
θEs∼νπϕ
τ,a∼πϕ(·|s)πθ(a|s)
πϕ(a|s)Qπϕτ(s, a)
−λD2
τ(πϕ, πθ).
The optimization problem is equivalent to
θ′
τ≜argmax
θEs∼νπϕ
τZ
Aπθ(a|s)Qπϕτ(s, a)da−λd2(πϕ(·|s), πθ(·|s))
= argmin
θEs∼νπϕ
τ
−Z
Aπθ(a|s)Qπϕτ(s, a)da+λd2(πϕ(·|s), πθ(·|s))
= argmin
θX
s∈Sνπϕτ(s)
−Z
Aπθ(a|s)Qπϕτ(s, a)da+λd2(πϕ(·|s), πθ(·|s))
.
Similar to the derivation from (15) to (16) with Assumption 2, we have that, when θ=Alg(πϕ, λ, τ),
∇θ
−Z
Aπθ(a|s)Qπϕτ(s, a)da+λd2(πϕ(·|s), πθ(·|s))
= 0.
Then, we have
X
s∈S∇ϕνπϕτ(s)∇θ
−Z
Aπθ(a|s)Qπϕτ(s, a)da+λd2(πϕ(·|s), πθ(·|s))
= 0.
By using implicit differentiation, if the matrix Es∼νπϕ
τ
−R
A∇2
θπθ(a|s)Qπϕτ(s, a)da+λ∇2
θd2(πϕ(·|s), πθ(·|s))
is invertible, i.e., Es∼νπϕ
τ
−R
Aπθ(a|s)Qπϕτ(s, a)da+λd2(πϕ(·|s), πθ(·|s))
is strongly convex at
θ=θ′
τ, we have
∇⊤
ϕθ′
τ=−
Es∼νπϕ
τ
−Z
A∇2
θπθ(a|s)Qπϕτ(s, a)da+λ∇2
θd2(πϕ(·|s), πθ(·|s))−1

−Es∼νπϕ
τZ
A∇θπθ(a|s)∇⊤
ϕQπϕτ(s, a)da
+Es∼νπϕ
τ
λ∇⊤
ϕ∇θd2(πϕ(·|s), πθ(·|s))
+
X
s∈S∇ϕνπϕτ(s)∇θ
−Z
Aπθ(a|s)Qπϕτ(s, a)da+λd2(πϕ(·|s), πθ(·|s))!
|θ=θ′τ,
23This is equivalent to
∇⊤
ϕθ′
τ=−
Es∼νπϕ
τ,a∼πϕ(·|s)
−∇2
θπθ(a|s)
πϕ(a|s)Qπϕτ(s, a) +λ∇2
θd2(πϕ(·|s), πθ(·|s))−1
Es∼νπϕ
τ,a∼πϕ(·|s)
−∇θπθ(a|s)
πϕ(a|s)∇⊤
ϕQπϕτ(s, a) +λ∇⊤
ϕ∇θd2(πϕ(·|s), πθ(·|s))
|θ=θ′τ.
J.3 Proofs of hypergradient of the algorithm in Section G
Deviation of (10). Asθ′
τ= argmin
θ−1
λθ⊤Es∼νπϕ
τ,a∼πϕ(·|s)[∇ϕπϕ(a|s)
πϕ(a|s)Aπϕτ(s, a)] +D2
τ(πϕ, πθ), by
the implicit differentiation theorem in bilevel optimization analysis,
∇⊤
ϕθ′
τ=−∇2
θ
−1
λθ⊤Es∼νπϕ
τ,a∼πϕ(·|s)[∇ϕπϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)] +D2
τ(πϕ, πθ)−1
∇ϕ∇θ
−1
λθ⊤Es∼νπϕ
τ,a∼πϕ(·|s)[∇ϕπϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)] +D2
τ(πϕ, πθ)
|θ=θ′τ
Also, we have
∇2
θ(1
λθ⊤Es∼νπϕ
τ,a∼πϕ(·|s)[∇ϕπϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)]) = 0 ,
and
∇ϕ∇θ(1
λθ⊤Es∼νπϕ
τ,a∼πϕ(·|s)[∇ϕπϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)])
= E
s∼νπϕ
τa∼πϕ(·|s)[1
λ∇ϕπϕ(a|s)
πϕ(a|s)∇⊤
ϕQπϕτ(s, a) +1
λ∇2
ϕπϕ(a|s)
πϕ(a|s)Qπϕτ(s, a)].
Then, we can get ∇⊤
ϕθ′
τ.
K Proofs of convergence when Dτ=Dτ,1
K.1 Gradients of ∇ϕJτ(πθ′τ)when Dτ=Dτ,1
From Proposition 1,
∇ϕJτ(πθ′τ) =1
1−γE
s∼νπθ′ττ"X
a∈A∇ϕπθ′τ(a|s)Aπθ′ττ(s, a)#
=1
1−γE
s∼νπθ′ττh
∇ϕπθ′τ(·|s)·Aπθ′ττ(s,·)i
,(20)
where
∇⊤
ϕπθ′τ(·|s) =
M(s)−1−M(s)−11 1⊤M(s)−1
1⊤M(s)−11 
∇⊤
ϕQπϕτ(s,·)−λ∇⊤
ϕ∇π(·|s)d2
1(πϕ, π, s)
|π=πθ′τ,
where
M(s) =λ∇2
π(·|s)d2
1(πϕ, π, s) =λ
πϕ(a1|s)
πθ′τ(a1|s)2
...
πϕ(an|s)
πθ′τ(an|s)2
.
Then,
M(s)−1=1
λ
πθ′τ(a1|s)2
πϕ(a1|s)
...
πθ′τ(an|s)2
πϕ(an|s)
, (21)
24and
M(s)−11 1⊤M(s)−1
1⊤M(s)−11=1
λP
a∈Aπθ′τ(a|s)2
πϕ(a|s)
πθ′τ(a1|s)2
πϕ(a1|s)
...
πθ′τ(an|s)2
πϕ(an|s)
hπθ′τ(a1|s)2
πϕ(a1|s)···πθ′τ(an|s)2
πϕ(an|s)i
.
Also,
∇⊤
ϕ∇π(·|s)d2
1(πϕ, π, s)|π=πθ′τ=∇⊤
ϕ
−πϕ(a1|s)
πθ′τ(a1|s)
...
−πϕ(an|s)
πθ′τ(an|s)
=
−∇⊤
ϕπϕ(a1|s)
πθ′τ(a1|s)
...
−∇⊤
ϕπϕ(an|s)
πθ′τ(an|s)
. (22)
Then, plugging these equations into (20), we have
∇⊤
ϕJτ(πθ′τ) =1
1−γE
s∼νπθ′ττh
Aπθ′ττ(s,·)⊤∇⊤
ϕπθ′τ(·|s)i
,
=1
1−γE
s∼νπθ′ττ
Aπθ′ττ(s,·)⊤
M(s)−1−M(s)−11 1⊤M(s)−1
1⊤M(s)−11
1
λ∇⊤
ϕQπϕτ(s, a1) +∇⊤
ϕπϕ(a1|s)
πθ′τ(a1|s)
...
1
λ∇⊤
ϕQπϕτ(s, an) +∇⊤
ϕπϕ(an|s)
πθ′τ(an|s)


=1
1−γE
s∼νπθ′ττhh
(Aπθ′ττ(s, a1)−cτ(s))πθ′τ(a1|s)2
πϕ(a1|s)··· (Aπθ′ττ(s, an)−cτ(s))πθ′τ(an|s)2
πϕ(an|s)i

1
λ∇⊤
ϕQπϕτ(s, a1) +∇⊤
ϕπϕ(a1|s)
πθ′τ(a1|s)
...
1
λ∇⊤
ϕQπϕτ(s, an) +∇⊤
ϕπϕ(an|s)
πθ′τ(an|s)

,
where
cτ(s) =P
a∈AAπθ′ττ(s, a)πθ′τ(a|s)2
πϕ(a|s)
P
a∈Aπθ′τ(a|s)2
πϕ(a|s). (23)
Then, we simplify the computation of ∇⊤
ϕJτ(πθ′τ), we have ∇⊤
ϕJτ(πθ′τ) =
1
1−γE
s∼νπθ′ττ"X
a∈A(Aπθ′ττ(s, a)−cτ(s))πθ′τ(a|s)2
πϕ(a|s)(1
λ∇⊤
ϕQπϕτ(s, a) +∇⊤
ϕπϕ(a|s)
πθ′τ(a|s))#
=1
1−γE
s∼νπθ′ττ"X
a∈Aπθ′
τ(a|s)(Aπθ′ττ(s, a)−cτ(s))(πθ′τ(a|s)
λπϕ(a|s)∇⊤
ϕQπϕτ(s, a) +∇⊤
ϕπϕ(a|s)
πϕ(a|s))#
.
When the tabular policy is the softmax policy, we have ˆπϕ(a|s) =exp(ϕ(s,a))P
a′∈Aexp(ϕ(s,a′)), then
∇⊤
ϕˆπϕ(a|s)
ˆπϕ(a|s)=∇⊤
ϕln ˆπϕ(a|s) =∇⊤
ϕϕ(s, a)− ∇⊤
ϕlnX
a′∈Aexp (ϕ(s, a′))
=1(s, a)−ˆπϕ(·|s).(24)
Here, 1(s′, a′)denote the column vector where the element is 1ifs=s′anda=a′, otherwise is 0,
for each pair (s, a)∈ S × A ;ˆπϕ(·|s′)is the column vector, where the element is ˆπϕ(a|s′)ifs=s′,
0ifs̸=s′, for each pair (s.a)∈ S × A .
25So, we have
∇⊤
ϕJτ(ˆπθ′τ) =1
1−γE
s∼νˆπθ′ττ"X
a∈Aˆπθ′τ(a|s)(Aˆπθ′ττ(s, a)−cτ(s))
(ˆπθ′τ(a|s)
λˆπϕ(a|s)∇⊤
ϕQˆπϕτ(s, a) +∇⊤
ϕˆπϕ(a|s)
ˆπϕ(a|s))#
=1
1−γE
s∼νˆπθ′ττ"X
a∈Aˆπθ′τ(a|s)(Aˆπθ′ττ(s, a)−cτ(s))
(ˆπθ′τ(a|s)
λˆπϕ(a|s)∇⊤
ϕQˆπϕτ(s, a) +1⊤(s, a)−ˆπϕ(·|s)⊤)
=1
1−γE
s∼νˆπθ′ττ,a∼ˆπθ′τh
(Aˆπθ′ττ(s, a)−cτ(s))
(ˆπθ′τ(a|s)
λˆπϕ(a|s)∇⊤
ϕQˆπϕτ(s, a) +1⊤(s, a)−ˆπϕ(·|s)⊤)
.(25)
K.2 Convergence guarantee when Dτ=Dτ,1
K.2.1 Auxiliary lemmas
Lemma 5. Suppose that Assumption 2 holds. Let πθ′τ=Alg(πϕ, λ, τ)where Dτ=Dτ,1, for any
s∈ S anda∈ A, we have
λ
λ+ max s,a|Aπϕτ(s, a)|≤πθ′τ(a|s)
πϕ(a|s)≤λ
λ−max s,a|Aπϕτ(s, a)|.
Proof. From (16), when Dτ=Dτ,1, we have πθ′τ=Alg(πϕ, λ, τ)and
πθ′τ(·|s) = argmax
(·|s)X
a∈Aπ(a|s)Qπϕτ(s, a)−λd2
1(πϕ, π, s),
subject toX
a∈Aπ(a|s) = 1 .
For any s∈ S, the Lagrangian of the above maximization problem is
−X
a∈Aπ(a|s)Qπϕτ(s, a) +λd2(πϕ(·|s), π(·|s)) +µ(s)(X
a∈Aπ(a|s)−1),
where µis the Lagrangian multiplier. The optimality condition of π(·|s)is that,
−Qπϕτ(s,·) +λ∇π(·|s)d2
1(πϕ, π, s) +µ(s)[1,···,1]⊤= 0.
Solve the equation,
−Qπϕτ(s, a)−λπϕ(a|s)
πθ′τ(a|s)+µ(s) = 0 .
Letµ1(s) =−Vπϕτ(s) +µ(s), we have
−Aπϕτ(s, a)−λπϕ(a|s)
πθ′τ(a|s)+µ1(s) = 0 . (26)
Then,
−πθ′τ(a|s)Aπϕτ(s, a)−λπϕ(a|s) +πθ′τ(a|s)µ1(s) = 0 .
We derive the summation of all a∈ A,
X
a∈A−πθ′τ(a|s)Aπϕτ(s, a)−λ+µ1(s) = 0 .
26We have
µ1(s) =X
a∈Aπθ′τ(a|s)Aπϕτ(s, a) +λ.
From (26), we have
πϕ(a|s)
πθ′τ(a|s)=µ1(s)−Aπϕτ(s, a)
λ
=λ+P
a′∈Aπθ′τ(a′|s)Aπϕτ(s, a′)−Aπϕτ(s, a)
λ
So, we have
πθ′τ(a|s)
πϕ(a|s)=λ
λ+P
a′∈Aπθ′τ(a′|s)Aπϕτ(s, a′)−Aπϕτ(s, a),
then
λ
λ+ max s,a|Aπϕτ(s, a)|≤πθ′τ(a|s)
πϕ(a|s)≤λ
λ−max s,a|Aπϕτ(s, a)|.
Lemma 6. Suppose that Assumption 2 holds. Let πθ′τ=Alg(πϕ, λ, τ)where Dτ=Dτ,1, we have
∥∇ϕJτ(πθ′τ)∥ ≤max s,a|Aπθ′ττ(s, a)|
1−γ(max s,a|Aπθ′ττ(s, a)|
λ−max s,a|Aπϕτ(s, a)|γ
1−γ+ 2).
Proof. As shown in (25),
∇⊤
ϕJτ(ˆπθ′τ) =1
1−γE
s∼νˆπθ′ττ"X
a∈Aˆπθ′τ(a|s)(Aˆπθ′ττ(s, a)−cτ(s))
(ˆπθ′τ(a|s)
λˆπϕ(a|s)∇⊤
ϕQˆπϕτ(s, a) +1⊤(s, a)−ˆπϕ(·|s)⊤)
=1
1−γX
s∈SX
a∈Aνˆπθ′ττ(s)ˆπθ′τ(a|s)(Aˆπθ′ττ(s, a)−cτ(s))
(ˆπθ′τ(a|s)
λˆπϕ(a|s)∇⊤
ϕQˆπϕτ(s, a) +1⊤(s, a)−ˆπϕ(·|s)⊤).
SinceP
s∈Sνˆπθ′ττ(s) = 1 andP
a∈Aˆπθ′τ(a|s) = 1 for all s∈ S, we have ∥∇ϕJτ(ˆπθ′τ)∥ ≤
1
1−γmax
a,s∥(Aˆπθ′ττ(s, a)−cτ(s))(ˆπθ′τ(a|s)
λˆπϕ(a|s)∇⊤
ϕQˆπϕτ(s, a) +1⊤(s, a)−ˆπϕ(·|s)⊤)∥.
From (23), for any s∈ S anda∈ A, we have
|Aˆπθ′ττ(s, a)−cτ(s)| ≤max s,a|Aˆπθ′ττ(s, a)|.
Also, for any s∈ S anda∈ A,
∥1(s, a)−ˆπϕ(·|s))∥ ≤1 +∥ˆπ(·|s)∥ ≤2.
From Lemma 5,
ˆπθ′τ(a|s)
λˆπϕ(a|s)≤1
λ−max s,a|Aˆπϕτ(s, a)|.
From the computation of ∇ϕQˆπϕτ(s, a)shown in (5) of Appendix C,
|∇ϕ(s′,a′)Qˆπϕτ(s, a)|=|γ
1−γ·σ(s,a)
τ,ˆπϕ(s′)ˆπϕ(a′|s′)Aˆπϕτ(s, a)|
≤γ
1−γσ(s,a)
τ,ˆπϕ(s′)ˆπϕ(a′|s′)|max
a,sAˆπϕτ(s, a)|.
27Also, sinceP
a∈A,s∈Sσ(s,a)
τ,ˆπϕ(s′)ˆπϕ(a′|s′) = 1 , we have
∥∇ϕQˆπϕτ(s, a)∥ ≤γ
1−γmax
a,s|Aˆπϕτ(s, a)|. (27)
Therefore, we have
∥∇ϕJτ(ˆπθ′τ)∥ ≤max s,a|Aˆπθ′ττ(s, a)|
1−γ(max s,a|Aˆπθ′ττ(s, a)|
λ−max s,a|Aˆπϕτ(s, a)|γ
1−γ+ 2).
Lemma 7. Suppose that Assumption 2 holds. Let ˆπθ′τ=Alg(ˆπϕ, λ, τ)where Dτ=Dτ,1, for any
s∈ S we have
X
a∈A∥∇ϕˆπθ′τ(a|s)∥ ≤1
λ−max s,a|Aˆπϕτ(s, a)|(γmax a,s|Aˆπϕτ(s, a)|
1−γ+ 2(λ+ max
a,s|Aˆπϕτ(s, a)|))
and
X
a∈A∥∇2
ϕˆπθ′τ(a|s)∥ ≤1
λ−max s,a|Aˆπϕτ(s, a)|(8rmax
(1−γ)3
+λ+ max s,a|Aˆπϕτ(s, a)|
λ−max s,a|Aˆπϕτ(s, a)|((2−γ) max a,s|Aˆπϕτ(s, a)|
1−γ+ 2λ+ 2)) .
Proof. From (19), for any s∈ S,
∇⊤
ϕˆπθ′τ(·|s) =
M(s)−1−M(s)−11 1⊤M(s)−1
1⊤M(s)−11
∇⊤
ϕQˆπϕτ(s,·)−λ∇⊤
ϕ∇ˆπ(·|s)d2
1(ˆπϕ,ˆπ, s)
|ˆπ=ˆπθ′τ.
From the computations of M(s)−1,∇⊤
ϕ∇ˆπ(·|s)d2
1(ˆπϕ,ˆπ, s)|ˆπ=ˆπθ′τ, and∇ϕQˆπϕτ(s,·)in (21) (27), we
have
M(s)−1−M(s)−11 1⊤M(s)−1
1⊤M(s)−11
j≤ˆπθ′τ(a|s)
λmax
a,sˆπθ′τ(a|s)
ˆπϕ(a|s)≤ˆπθ′τ(a|s)
λ−max s,a|Aˆπϕτ(s, a)|,
and
∥∇ϕQˆπϕτ(s, a)∥ ≤max
a∥∇ϕQˆπϕτ(s, a)∥ ≤γ
1−γmax
a,s|Aˆπϕτ(s, a)|.
From (22)(24)
∥λ∇⊤
ϕ∇ˆπ(a|s)d2
1(ˆπϕ,ˆπ, s)∥=∥λ(1(s, a)−ˆπϕ(·|s))ˆπϕ(a|s)
ˆπθ′τ(a|s)∥ ≤2(λ+ max
a,s|Aˆπϕτ(s, a)|).
The last inequality comes from Lemma 5. So, we have
∥∇ϕˆπθ′τ(a|s)∥ ≤ˆπθ′τ(a|s)
λ−max s,a|Aˆπϕτ(s, a)|(γmax a,s|Aˆπϕτ(s, a)|
1−γ+ 2(λ+ max
a,s|Aˆπϕτ(s, a)|)).
Therefore,
X
a∈A∥∇ϕˆπθ′τ(a|s)∥ ≤1
λ−max s,a|Aˆπϕτ(s, a)|(γmax a,s|Aˆπϕτ(s, a)|
1−γ+ 2(λ+ max
a,s|Aˆπϕτ(s, a)|)).
Also, we have
∥∇2
ϕˆπθ′τ(a|s)∥ ≤ˆπθ′τ(a|s)
λ−max s,a|Aˆπϕτ(s, a)|(∥∇2
ϕQˆπϕτ(s, a)∥+λ∥∇2
ϕ∇ˆπ(a|s)d2
1(ˆπϕ,ˆπ, s)∥).
From Lemma D.4 in [2], we have
∥∇2
ϕQˆπϕτ(s, a)∥ ≤8rmax
(1−γ)3.
28Moreover, we have
λ∥∇2
ϕ∇ˆπ(a|s)d2
1(ˆπϕ,ˆπ, s)∥
=λ∥∇ϕ((1(s, a)−ˆπϕ(·|s))ˆπϕ(a|s)
ˆπθ′τ(a|s))∥
≤λ+ max s,a|Aˆπϕτ(s, a)|
λ−max s,a|Aˆπϕτ(s, a)|(γmax a,s|Aˆπϕτ(s, a)|
1−γ+ 2(λ+ max
a,s|Aˆπϕτ(s, a)|) + 2)
=λ+ max s,a|Aˆπϕτ(s, a)|
λ−max s,a|Aˆπϕτ(s, a)|((2−γ) max a,s|Aˆπϕτ(s, a)|
1−γ+ 2λ+ 2)
So,
X
a∈A∥∇2
ϕˆπθ′τ(a|s)∥ ≤1
λ−max s,a|Aˆπϕτ(s, a)|(8rmax
(1−γ)3
+λ+ max s,a|Aˆπϕτ(s, a)|
λ−max s,a|Aˆπϕτ(s, a)|((2−γ) max a,s|Aˆπϕτ(s, a)|
1−γ+ 2λ+ 2)) .
Lemma 8. Suppose that Assumptions 1 and 2 hold. Let ˆπθ′τ=Alg(ˆπϕ, λ, τ)where Dτ=Dτ,1, we
have
∥∇2
ϕJτ(ˆπθ′τ)∥ ≤rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3, (28)
where C =1
λ−Amax(γAmax
1−γ+ 2λ+ 2Amax)and B =1
λ−Amax(8rmax
(1−γ)3+
λ+Amax
λ−Amax((2−γ)Amax
1−γ+ 2λ+ 2)) .
Proof. From Lemma 7, we have boundedP
a∈A∥∇ϕˆπθ′τ(a|s)∥andP
a∈A∥∇2
ϕˆπθ′τ(a|s)∥. Borrow
the result from Lemma D.2 in [2].
K.2.2 Convergence guarantee
Theorem 5. Consider the tabular softmax policy for the discrete state-action space shown in Section
5.1, and the within-task algorithm Algin (1). Suppose that Assumptions 1 and 2 hold. Let {ϕt}T
t=1
be the sequence generated by Algorithm 1 with Dτ=Dτ,1,λ > A max, and the step size selected as
α= min(rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3−1
,1
G√
T)
.
Then,
1
TTX
t=1Et
∥∇ϕEτ∼P(Γ)[Jτ(Alg(ˆπϕt, λ, τ))]∥2
≤2r2
maxB
(1−γ)3+4γr2
maxC2
(1−γ)41
T+2rmax
1−γ+rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3G√
T,
where
G=2Amax
1−γ(Amax
λ−Amaxγ
1−γ+ 2),
C=1
λ−Amax(γAmax
1−γ+ 2λ+ 2Amax),
and
B=1
λ−Amax(8rmax
(1−γ)3+λ+Amax
λ−Amax((2−γ)Amax
1−γ+ 2λ+ 2)) .
29Proof. As the smoothness constant of Jτ(ˆπθ′τ), i.e., Jτ(Alg(ˆπϕ, λ, τ)is obtained in (8), the smooth-
ness constant of Eτ∼P(Γ)[Jτ(Alg(ˆπϕ, λ, τ))]is the same, i.e.,
∥∇2
ϕEτ∼P(Γ)[Jτ(Alg(ˆπϕ, λ, τ))]∥ ≤Brmax
(1−γ)2+2γrmaxC2
(1−γ)3.
Moreover, from Lemma 6, we have
∥∇ϕJτ(Alg(ˆπϕ, λ, τ))∥ ≤Amax
1−γ(Amax
λ−Amaxγ
1−γ+ 2).
From the convergence theorem of SDG with smoothness and bounded gradient shown in [ 19], let the
step size
α= min(rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3−1
,1
G√
T)
,
we have
1
TTX
t=1Et
∥∇ϕEτ∼P(Γ)[Jτ(Alg(ˆπϕt, λ, τ))]∥2
≤2rmaxB
(1−γ)2+4γrmaxC2
(1−γ)3
Eτ∼P(Γ)[Jτ(Alg(ˆπϕT, λ, τ))−Jτ(Alg(ˆπϕ0, λ, τ))]1
T
+
2Eτ∼P(Γ)[Jτ(Alg(ˆπϕT, λ, τ))−Jτ(Alg(ˆπϕ0, λ, τ))] +rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3
SinceEτ∼P(Γ)[Jτ(Alg(ˆπϕT, λ, τ))−Jτ(Alg(ˆπϕ0, λ, τ))]≤rmax
1−γ, we have
1
TTX
t=1Et
∥∇ϕEτ∼P(Γ)[Jτ(Alg(ˆπϕt, λ, τ))]∥2
≤2r2
maxB
(1−γ)3+4γr2
maxC2
(1−γ)41
T+2rmax
1−γ+rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3G√
T,
where
G=2Amax
1−γ(Amax
λ−Amaxγ
1−γ+ 2),
C=1
λ−Amax(γAmax
1−γ+ 2λ+ 2Amax),
and
B=1
λ−Amax(8rmax
(1−γ)3+λ+Amax
λ−Amax((2−γ)Amax
1−γ+ 2λ+ 2)) .
Corollary 1. Suppose all assumptions and conditions in Theorem 5 hold, and we set λ≥2Amax,
then
1
TTX
t=1Et
∥∇ϕEτ∼P(Γ)[Jτ(Alg(ˆπϕt, λ, τ))]∥2
≤(B+ 2C2)rmax
(1−γ)4(2rmax
T+G√
T),
where B≜16rmax
λ(1−γ)3+24
1−γ+12
λ,C≜6
1−γ, and G≜4Amax
(1−γ)2.
Proof. Since λ≥2Amax, we have1
λ−Amax≤1
Amaxand1
λ−Amax≤2
λ. Then, simplify the
inequality in Theorem 5.
30L Proofs of convergence when Dτ=Dτ,2
L.1 Gradients of ∇ϕJτ(ˆπθ′τ)when Dτ=Dτ,2
From Proposition 1, we have
∇ϕJτ(ˆπθ′τ) =1
1−γE
s∼νˆπθ′ττh
∇ϕˆπθ′τ(·|s)·Aˆπθ′ττ(s,·)i
, (29)
where
∇⊤
ϕˆπθ′τ(·|s) =
M(s)−1−M(s)−11 1⊤M(s)−1
1⊤M(s)−11

∇⊤
ϕQˆπϕτ(s,·)−λ∇⊤
ϕ∇ˆπ(·|s)d2
2(ˆπϕ,ˆπ, s)
|ˆπ=ˆπθ′τ,(30)
where
M(s) =λ∇2
ˆπ(·|s)d2
2(ˆπϕ,ˆπ, s) =λ
1
ˆπθ′τ(a1|s)
...
1
ˆπθ′τ(an|s)
.
Then,
M(s)−1=1
λ
ˆπθ′τ(a1|s)
...
ˆπθ′τ(an|s)
. (31)
Also,
∇⊤
ϕ∇ˆπ(·|s)d2
2(ˆπϕ,ˆπ, s)|ˆπ=ˆπθ′τ=
−∇⊤
ϕˆπϕ(a1|s)
ˆπϕ(a1|s)
...
−∇⊤
ϕˆπϕ(an|s)
ˆπϕ(an|s)
. (32)
Specially, Aˆπθ′ττ(s,·)⊤M(s)−11 1⊤M(s)−1
1⊤M−11= 0, because we have
Aˆπθ′ττ(s,·)⊤M(s)−11=X
a∈Aˆπθ′τ(a|s)Aˆπθ′ττ(s, a) = 0 .
Then,
∇⊤
ϕJτ(ˆπθ′τ) =1
1−γE
s∼νˆπθ′ττ,a∼ˆπθ′τ
Aˆπθ′ττ(s, a)(1
λ∇⊤
ϕQˆπϕτ(s, a) +1⊤(s, a)−ˆπϕ(·|s)⊤)
.
SinceP
a∈Aˆπθ′τ(a|s)Aˆπθ′ττ(s, a) = 0 , thenP
a∈Aˆπθ′τ(a|s)Aˆπθ′ττ(s, a)ˆπϕ(·|s)⊤= 0. We have
∇⊤
ϕJτ(ˆπθ′τ) =1
1−γE
s∼νˆπθ′ττ,a∼ˆπθ′τ
Aˆπθ′ττ(s, a)(1
λ∇⊤
ϕQˆπϕτ(s, a) +1⊤(s, a))
. (33)
Here, 1(s′, a′)denote the column vector where the element is 1ifs=s′anda=a′, otherwise is 0,
for each pair (s, a)∈ S × A .
L.2 Convergence guarantee when Dτ=Dτ,2
L.2.1 Auxiliary lemmas
Lemma 9. Suppose that Assumption 2 holds. Let ˆπθ′τ=Alg(ˆπϕ, λ, τ)where Dτ=Dτ,2, we have
∥∇ϕJτ(ˆπθ′τ)∥ ≤max s,a|Aˆπθ′ττ(s, a)|
1−γ(max s,a|Aˆπθ′ττ(s, a)|
λγ
1−γ+ 1).
31Proof. As shown in (33)
∇⊤
ϕJτ(ˆπθ′τ) =1
1−γE
s∼νˆπθ′ττ,a∼ˆπθ′τ
Aˆπθ′ττ(s, a)(1
λ∇⊤
ϕQˆπϕτ(s, a) +1⊤(s, a))
.
As shown in proof of Lemma 6 in (27),
∥∇ϕQˆπϕτ(s, a)∥ ≤γ
1−γmax
a,s|Aˆπϕτ(s, a)|,
we have that
∥∇ϕJτ(ˆπθ′τ)∥ ≤max s,a|Aˆπθ′ττ(s, a)|
1−γ(max s,a|Aˆπθ′ττ(s, a)|
λγ
1−γ+ 1).
Lemma 10. Suppose that Assumption 2 holds. Let ˆπθ′τ=Alg(ˆπϕ, λ, τ)where Dτ=Dτ,2, for any
s∈ S,we have
X
a∈A∥∇ϕˆπθ′τ(a|s)∥ ≤2γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 4
and
X
a∈A∥∇2
ϕˆπθ′τ(a|s)∥ ≤(2γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 4)2+16rmax
λ(1−γ)3+ 2.
Proof. As shown in 30, we have ∇⊤
ϕˆπθ′τ(·|s) =

M(s)−1−M(s)−11 1⊤M(s)−1
1⊤M(s)−11
∇⊤
ϕQˆπϕτ(s,·)−λ∇⊤
ϕ∇ˆπ(·|s)d2
2(ˆπϕ,ˆπ, s)
|ˆπ=ˆπθ′τ,
where the computations of M(s)−1and∇⊤
ϕ∇ˆπ(·|s)d2
2(ˆπϕ,ˆπ, s)are shown in (31) (32) and (24), then
∇ϕˆπθ′τ(a|s) = ˆπθ′τ(a|s)(1
λ∇ϕQˆπϕτ(s, a) +1(s, a)−ˆπϕ(·|s))
−ˆπθ′τ(a|s)X
a′∈Aˆπθ′τ(a′|s)(1
λ∇ϕQˆπϕτ(s, a′) +1(s, a′)−ˆπϕ(·|s)).
Therefore,
∥∇ϕˆπθ′τ(a|s)∥ ≤ˆπθ′τ(a|s)(1
λ∇ϕQˆπϕτ(s, a) +1(s, a)−ˆπϕ(·|s))
+ˆπθ′τ(a|s)X
a′∈Aˆπθ′τ(a′|s)(1
λ∇ϕQˆπϕτ(s, a′) +1(s, a′)−ˆπϕ(·|s)).
Then,
X
a∈A∥∇ϕˆπθ′τ(a|s)∥ ≤X
a∈Aˆπθ′τ(a|s)(1
λ∇ϕQˆπϕτ(s, a) +1(s, a)−ˆπϕ(·|s))
+X
a∈Aˆπθ′τ(a|s)X
a′∈Aˆπθ′τ(a′|s)(1
λ∇ϕQˆπϕτ(s, a′) +1(s, a′)−ˆπϕ(·|s)).
From (27), we have
∥∇ϕQˆπϕτ(s, a)∥ ≤γ
1−γmax
a,s|Aˆπϕτ(s, a)|.
32Then,
X
a∈A∥∇ϕˆπθ′τ(a|s)∥ ≤X
a∈Aˆπθ′τ(a|s)1
λ∇ϕQˆπϕτ(s, a) +1(s, a)−ˆπϕ(·|s)
+X
a∈Aˆπθ′τ(a|s)X
a′∈Aˆπθ′τ(a′|s)(1
λ∇ϕQˆπϕτ(s, a′) +1(s, a′)−ˆπϕ(·|s))
≤X
a∈Aˆπθ′τ(a|s)(γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 2)
+X
a∈Aˆπθ′τ(a|s)X
a′∈Aˆπθ′τ(a′|s)(γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 2)
≤2γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 4,
And
∥∇ϕˆπθ′τ(a|s)∥ ≤ˆπθ′τ(a|s)(2γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 4) (34)
Moreover, since
∇ϕˆπθ′τ(a|s) = ˆπθ′τ(a|s)(1
λ∇ϕQˆπϕτ(s, a) +1(s, a)−ˆπϕ(·|s))
−ˆπθ′τ(a|s)X
a′∈Aˆπθ′τ(a′|s)(1
λ∇ϕQˆπϕτ(s, a′) +1(s, a′)−ˆπϕ(·|s)).
we have
∇2
ϕˆπθ′τ(a|s) =∇ϕˆπθ′τ(a|s)(1
λ∇ϕQˆπϕτ(s, a) +1(s, a)−ˆπϕ(·|s))
+ ˆπθ′τ(a|s)(1
λ∇2
ϕQˆπϕτ(s, a) +−∇ϕˆπϕ(·|s))
− ∇ ϕ 
ˆπθ′τ(a|s)X
a′∈Aˆπθ′τ(a′|s)(1
λ∇ϕQˆπϕτ(s, a′) +1(s, a′)−ˆπϕ(·|s))!
.
Then,
∥∇2
ϕˆπθ′τ(a|s)∥ ≤2∥∇ϕˆπθ′τ(a|s)∥∥1
λ∇ϕQˆπϕτ(s, a) +1(s, a)−ˆπϕ(·|s)∥
+ 2ˆπθ′τ(a|s)∥1
λ∇2
ϕQˆπϕτ(s, a)− ∇ ϕˆπϕ(·|s)∥.
From (34),
∥∇ϕˆπθ′τ(a|s)∥ ≤ˆπθ′τ(a|s)(2γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 4).
From (27)
∥1
λ∇ϕQˆπϕτ(s, a) +1(s, a)−ˆπϕ(·|s)∥ ≤γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 2
From Lemma D.4 in [2], we have
∥∇2
ϕQˆπϕτ(s, a)∥ ≤8rmax
(1−γ)3,
then
∥1
λ∇2
ϕQˆπϕτ(s, a)− ∇ ϕˆπϕ(·|s)∥ ≤8rmax
λ(1−γ)3+ 1.
Therefore,
∥∇2
ϕˆπθ′τ(a|s)∥ ≤ˆπθ′τ(a|s)(2γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 4)2+ 2ˆπθ′τ(a|s)(8rmax
λ(1−γ)3+ 1).
So,X
a∈A∥∇2
ϕˆπθ′τ(a|s)∥ ≤(2γ
λ(1−γ)max
a,s|Aˆπϕτ(s, a)|+ 4)2+16rmax
λ(1−γ)3+ 2.
33Lemma 11. Suppose that Assumptions 1 and 2 hold. Let ˆπθ′τ=Alg(ˆπϕ, λ, τ)where Dτ=Dτ,2,
we have
∥∇2
ϕJτ(ˆπθ′τ)∥ ≤rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3, (35)
where C=2γ
λ(1−γ)Amax+ 4andB= (2γ
λ(1−γ)Amax+ 4)2+16rmax
λ(1−γ)3+ 2.
Proof. Similar to the proof of Lemma 8 by using Lemma 10.
L.2.2 Convergence guarantee
Theorem 6. Consider the tabular softmax policy for the discrete state-action space shown in Section
5.1, and the within-task algorithm Algin (1). Suppose that Assumptions 1 and 2 hold. Let {ϕt}T
t=1
be the sequence generated by Algorithm 1 with Dτ=Dτ,2and the step size selected as
α= min(rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3−1
,1
G√
T)
.
Then,
1
TTX
t=1Et
∥∇ϕEτ∼P(Γ)[Jτ(Alg(ˆπϕt, λ, τ))]∥2
≤2r2
maxB
(1−γ)3+4γr2
maxC2
(1−γ)41
T+2rmax
1−γ+rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3G√
T,
where
G=2Amax
1−γ(Amax
λγ
1−γ+ 1),
C=2γ
λ(1−γ)Amax+ 4,
and
B= (2γAmax
λ(1−γ)+ 4)2+16rmax
λ(1−γ)3+ 2.
Proof. Similar to the proof of Theorem 5, by using the gradient bound in Lemma 9 and the smoothness
in Lemma 11.
Corollary 2. Suppose all assumptions and conditions in Theorem 6 hold, and we set λ≥2Amax,
then
1
TTX
t=1Et
∥∇ϕEτ∼P(Γ)[Jτ(Alg(ˆπϕt, λ, τ))]∥2
≤(B+ 2C2)rmax
(1−γ)4(2rmax
T+G√
T),
where B≜16rmax
λ(1−γ)3+18
(1−γ)2,C≜4
1−γ, and G≜2Amax
(1−γ)2).
Proof. Since λ≥2Amax, we have1
λ≤1
2Amax. Then, simplify the inequality in Theorem 5.
M Proofs of convergence when Dτ=Dτ,3
Lemma 12. Suppose that Assumptions 1, 2, and 3 hold. Let ˆπθ′τ=Alg(ˆπϕ, λ, τ)where Dτ=Dτ,3.
Ifλ >(6L2
1+ 2L2)Amax, then∇ϕJτ(Alg(3)(ˆπϕ, λ, τ))exists for any ϕ, and
∥∇ϕJτ(ˆπθ′τ)∥ ≤L1Amax(λ+2γ
1−γL2
1Amax)
(1−γ)(λ−(6L2
1+ 2L2)Amax).
34Proof. From Proposition 2, we have
∇ϕJτ(ˆπθ′τ) =1
1−γ∇ϕθ′
τ·E
s∼νˆπθ′ττ
a∼ˆπθ′τ(·|s)∇θ′τˆπθ′τ(a|s)
ˆπθ′τ(a|s)Aˆπθ′ττ(s, a)
,
where
∇⊤
ϕθ′
τ=−E
s∼νˆπϕ
τ
a∼ˆπϕ(·|s)
−∇2
θˆπθ(a|s)
ˆπϕ(a|s)Qˆπϕτ(s, a) +λ∇2
θd2(ˆπϕ(·|s),ˆπθ(·|s))−1
E
s∼νˆπϕ
τ
a∼ˆπϕ(·|s)
−∇θˆπθ(a|s)
ˆπϕ(a|s)∇⊤
ϕQˆπϕτ(s, a) +λ∇⊤
ϕ∇θd2(ˆπϕ(·|s),ˆπθ(·|s))
|θ=θ′τ.
When Dτ=Dτ,3, and the policy with function approximation is defined by ˆπθ(a|s)≜
exp(fθ(s,a))R
Aexp(fθ(s,a′))da′,∀(s, a)∈ S × A , from Lemma 4,
∇ϕJτ(ˆπθ′
τ) =1
1−γ∇ϕθ′
τ·E
s∼νˆπθ′ττ
a∼ˆπθ′τ(·|s)h
∇θ′τfˆπθ′τ(s, a)Aˆπθ′ττ(s, a)i
,
where ∇⊤
ϕθ′
τ=
E
s∼νˆπϕ
τ
a∼ˆπϕ(·|s)"
−∇2
θ′τˆπθ′τ(a|s)
ˆπϕ(a|s)Qˆπϕτ(s, a) +λI#−1
E
s∼νˆπϕ
τ
a∼ˆπϕ(·|s)∇θ′τˆπθ′τ(a|s)
ˆπϕ(a|s)∇⊤
ϕQˆπϕτ(s, a) +λI
=Es∼νˆπϕ
τ
−Z
A∇2
θ′τˆπθ′τ(a|s)Qˆπϕτ(s, a)da+λI−1
Es∼νˆπϕ
τZ
A∇θ′τˆπθ′τ(a|s)∇⊤
ϕQˆπϕτ(s, a)da+λI
=Es∼νˆπϕ
τ
−Z
A∇2
θ′τˆπθ′τ(a|s)Aˆπϕτ(s, a)da+λI−1
Es∼νˆπϕ
τZ
A∇θ′τˆπθ′τ(a|s)∇⊤
ϕQˆπϕτ(s, a)da+λI
First, we have
∥∇ϕJτ(ˆπθ′τ)∥=1
1−γ∥∇ϕθ′
τ∥∥E
s∼νˆπθ′ττ
a∼ˆπθ′τ(·|s)h
∇θfθ(s, a)Aˆπθ′ττ(s, a)i
∥,
and
∥E
s∼νˆπθ′ττ
a∼ˆπθ′τ(·|s)h
∇θfθ(s, a)Aˆπθ′ττ(s, a)i
∥ ≤ ∥ max
a,s∇θfθ(s, a)∥max
a,s|Aˆπθ′ττ(s, a)| ≤L1Amax.
For the term ∇ϕθ′
τ, consider ∇θ′τˆπθ′τ(a|s)and∇2
θ′τˆπθ′τ(a|s), we have
∇θˆπθ(a|s) = ˆπθ(a|s)∇θfθ(s, a)−ˆπθ(a|s)R
A∇θfθ(s, a′) exp ( fθ(s, a′))da′
R
Aexp (fθ(s, a′))da′. (36)
Then,
∥∇θˆπθ(a|s)∥ ≤ˆπθ(a|s)∥∇θfθ(s, a)∥+ ˆπθ(a|s)R
A∇θfθ(s, a′) exp ( fθ(s, a′))da′
R
Aexp (fθ(s, a′))da′
≤2ˆπθ(a|s)L1(37)
We also have ∇2
θˆπθ(a|s) =
∇θˆπθ(a|s)∇⊤
θfθ(s, a) + ˆπθ(a|s)∇2
θfθ(s, a)− ∇ˆπθ(a|s)R
A∇⊤
θfθ(s, a′) exp ( fθ(s, a′))da′
R
Aexp (fθ(s, a′))da′
−ˆπθ(a|s)R
A∇2
θfθ(s, a′) exp ( fθ(s, a′))da′+∇θfθ(s, a′)∇⊤
θfθ(s, a′) exp ( fθ(s, a′))da′
R
Aexp (fθ(s, a′))da′
+ ˆπθ(a|s)R
A∇θfθ(s, a′) exp ( fθ(s, a′))da′R
A∇⊤
θfθ(s, a′) exp ( fθ(s, a′))da′
(R
Aexp (fθ(s, a′))da′)2.
35Then,
∥∇2
θˆπθ(a|s)∥ ≤2ˆπθ(a|s)L2
1+ ˆπθ(a|s)L2+ 2ˆπθ(a|s)L2
1+ ˆπθ(a|s)L2+ 2ˆπθ(a|s)L2
1
= 6ˆπθ(a|s)L2
1+ 2ˆπθ(a|s)L2.(38)
So, Es∼νˆπϕ
τZ
A∇2
θ′τˆπθ′τ(a|s)Aˆπϕτ(s, a)da≤(6L2
1+ 2L2)Amax.
Since Es∼νˆπϕ
τhR
A∇2
θ′τˆπθ′τ(a|s)Aˆπϕτ(s, a)dai
is a diagonal matrix, the above shown its largest
absolute eigenvalue is smaller than (6L2
1+ 2L2)Amax. Then, the smallest eigenvalue of
Es∼νˆπϕ
τh
−R
A∇2
θ′τˆπθ′
τ(a|s)Aˆπϕτ(s, a)da+λIi
is larger than λ−(6L2
1+ 2L2)Amax. Therefore, if
λ >(6L2
1+ 2L2)Amax,
Es∼νˆπϕ
τ
−Z
A∇2
θ′τˆπθ′τ(a|s)Aˆπϕτ(s, a)da+λI−1≤1
λ−(6L2
1+ 2L2)Amax. (39)
Moreover, if λ >(6L2
1+2L2)Amax, the objective function in the optimization problem Alg(ˆπϕ, λ, τ)
is strongly concave. Then, from [64], the solution is unique and ∇ϕJτ(Alg(3)(ˆπϕ, λ, τ))exists.
From (6),
∇ϕQˆπϕτ(s, a) =γ
1−γ·E(s′,a′)∼σ(s,a)
τ,ˆπϕh
∇ϕfϕ(s′, a′)Aˆπϕτ(s′, a′)i
.
Then,
∥∇ϕQˆπϕτ(s, a)∥ ≤γ
1−γL1Amax.
Combine (37), we have
Es∼νˆπϕ
τZ
A∇θ′τˆπθ′τ(a|s)∇⊤
ϕQˆπϕτ(s, a)da+λI≤λ+2γ
1−γL2
1Amax.
So we have
∥∇ϕθ′
τ∥ ≤λ+2γ
1−γL2
1Amax
(1−γ)(λ−(6L2
1+ 2L2)Amax). (40)
Therefore, we have
∥∇ϕJτ(ˆπθ′τ)∥ ≤L1Amax(λ+2γ
1−γL2
1Amax)
(1−γ)(λ−(6L2
1+ 2L2)Amax).
Lemma 13. For a softmax policy parameterized by ϕ,
∥∇2
ϕJτ(ˆπϕ)∥ ≤(6L2
1+ 2L2)rmax
(1−γ)2+8γL2
1rmax
(1−γ)3
∥∇2
ϕQˆπϕτ(s, a)∥ ≤8γ2L2
1rmax
(1−γ)3+γ(6L2
1+ 2L2)rmax
(1−γ)2. (41)
Proof. From 37,Z
A∥∇ϕˆπϕ(a|s)∥da≤2L1.
From 38, Z
A∥∇ϕˆπϕ(a|s)∥da≤6L2
1+ 2L2.
Borrow the result from Lemma D.2 in [2],
∥∇2
ϕJτ(ˆπϕ)∥ ≤(6L2
1+ 2L2)rmax
(1−γ)2+8γL2
1rmax
(1−γ)3
36∥∇2
ϕQˆπϕτ(s, a)∥ ≤8γ2L2
1rmax
(1−γ)3+γ(6L2
1+ 2L2)rmax
(1−γ)2.
Lemma 14. Suppose that Assumption 2 holds. Let ˆπθ′τ=Alg(ˆπϕ, λ, τ)where Dτ=Dτ,3, for any
s∈ S,we haveZ
A∥∇ϕˆπθ′τ(a|s)∥da≤2L1(λ+2γ
1−γL2
1Amax)
(1−γ)(λ−(6L2
1+ 2L2)Amax)
andZ
A∥∇2
ϕˆπθ′τ(a|s)∥da≤(160L3
1+ 56L1L2+ 4L3)(λ+2γ
1−γL2
1Amax)2
(1−γ)3(λ−(6L2
1+ 2L2)Amax)2.
Proof. First consider ∇ϕˆπθ′τ(a|s), we have
∇ϕˆπθ′τ(a|s) = ˆπθ′τ(a|s)∇ϕθ′
τ∇θ′τfθ′τ(s, a)−ˆπθ′τ(a|s)∇ϕθ′
τR
A∇θ′τfθ′τ(s, a′) exp ( fθ′τ(s, a′))da′
R
Aexp (fθ′τ(s, a′))da′,
(42)
Then,
∥∇ϕˆπθ′τ(a|s)∥ ≤ˆπθ′τ(a|s)∥∇ϕθ′
τ∥∥∇ θ′τfθ′τ(s, a)∥+
ˆπθ′τ(a|s)∥∇ϕθ′
τ∥R
A∇θ′τfθ′τ(s, a′) exp ( fθ′τ(s, a′))da′
R
Aexp (fθ′τ(s, a′))da′
≤2ˆπθ′τ(a|s)(λ+2γ
1−γL2
1Amax)L1
(1−γ)(λ−(6L2
1+ 2L2)Amax).(43)
Then,Z
A∥∇ϕˆπθ′τ(a|s)∥da≤2L1(λ+2γ
1−γL2
1Amax)
(1−γ)(λ−(6L2
1+ 2L2)Amax).
Next, we consider ∇2
ϕˆπθ′τ(a|s). From (42), we have
∇2
ϕˆπθ′τ(a|s) =∇ϕθ′
τ∇θ′τfθ′τ(s, a)∇⊤
ϕˆπθ′τ(a|s) + ˆπθ′τ(a|s)∇2
ϕθ′
τ∇θ′τfθ′τ(s, a)
+ ˆπθ′τ(a|s)∇ϕθ′
τ∇2
θ′τfθ′τ(s, a)∇⊤
ϕθ′
τ− ∇ ϕθ′
τR
A∇θ′τfθ′τ(s, a′) exp ( fθ′τ(s, a′))da′
R
Aexp (fθ′τ(s, a′))da′∇⊤
ϕˆπθ′τ(a|s)
−ˆπθ′τ(a|s)∇2
ϕθ′
τR
A∇θ′τfθ′τ(s, a′) exp ( fθ′τ(s, a′))da′
R
Aexp (fθ′τ(s, a′))da′−ˆπθ′τ(a|s)∇ϕθ′
τ
R
A(∇2
θ′τfθ′τ(s, a′) exp ( fθ′τ(s, a′)) +∇θ′τfθ′τ(s, a′)∇⊤
θ′τfθ′τ(s, a′) exp ( fθ′τ(s, a′)))da′
R
Aexp (fθ′τ(s, a′))da′∇⊤
ϕθ′
τ
+ ˆπθ′τ(a|s)∇ϕθ′
τR
A∇θ′τfθ′τ(s, a′) exp ( fθ′τ(s, a′))da′
R
Aexp (fθ′τ(s, a′))da′2
∇⊤
ϕθ′
τ.
Therefore,
∥∇2
ϕˆπθ′τ(a|s)∥ ≤ ∥∇ ϕθ′
τ∥∥∇ θ′τfθ′τ(s, a)∥∥∇ ϕˆπθ′τ(a|s)∥+ ˆπθ′τ(a|s)∥∇2
ϕθ′
τ∥∥∇ θ′τfθ′τ(s, a)∥
+ ˆπθ′τ(a|s)∥∇ϕθ′
τ∥2∥∇2
θ′τfθ′τ(s, a)∥+∥∇ϕθ′
τ∥∥∇ θ′τfθ′τ(s, a)∥∥∇ ϕˆπθ′τ(a|s)∥
+ ˆπθ′τ(a|s)∥∇2
ϕθ′
τ∥∥∇ θ′τfθ′τ(s, a)∥+ ˆπθ′τ(a|s)∥∇ϕθ′
τ∥2(∥∇2
θ′τfθ′τ(s, a)∥+∥∇θ′τfθ′τ(s, a)∥2)
+ ˆπθ′τ(a|s)∥∇ϕθ′
τ∥2∥∇θ′τfθ′τ(s, a)∥2
≤2L1∥∇ϕθ′
τ∥∥∇ ϕˆπθ′τ(a|s)∥+ 2ˆπθ′τ(a|s)L1∥∇2
ϕθ′
τ∥+ 2ˆπθ′τ(a|s)∥∇ϕθ′
τ∥2(L2+L2
1).
From (40) and (43)
∥∇2
ϕˆπθ′τ(a|s)∥ ≤ˆπθ′τ(a|s) 
2L2+ 6L2
1(λ+2γ
1−γL2
1Amax)2
(1−γ)2(λ−(6L2
1+ 2L2)Amax)2+ 2L1∥∇2
ϕθ′
τ∥!
.
37Then,Z
A∥∇2
ϕˆπθ′τ(a|s)∥da≤2L2+ 6L2
1(λ+2γ
1−γL2
1Amax)2
(1−γ)2(λ−(6L2
1+ 2L2)Amax)2+ 2L1∥∇2
ϕθ′
τ∥.
Next, we consider ∇2
ϕθ′
τ. We have
∇2
ϕθ′
τ=Es∼νˆπϕ
τ
−Z
A∇2
θ′τˆπθ′
τ(a|s)Qˆπϕτ(s, a)da+λI−1
Es∼νˆπϕ
τZ
A
∇2
θ′τˆπθ′τ(a|s)∇⊤
ϕθ′
τ∇⊤
ϕQˆπϕτ(s, a) +∇θ′τˆπθ′τ(a|s)∇2
ϕQˆπϕτ(s, a)
da
−
MEs∼νˆπϕ
τ
−Z
A
∇3
θ′τˆπθ′τ(a|s)∇ϕθ′
τAˆπϕτ(s, a) +∇2
θ′τˆπθ′τ(a|s)∇⊤
ϕQˆπϕτ(s, a)
da
M−1N
where M = Es∼νˆπϕ
τh
−R
A∇2
θ′τˆπθ′τ(a|s)Aˆπϕτ(s, a)da+λIi
and N =
Es∼νˆπϕ
τhR
A∇θ′τˆπθ′τ(a|s)∇⊤
ϕQˆπϕτ(s, a)da+λIi
. Also, we have M−1N=∇ϕθ′
τ.
Similar to (37)(38), we can derive the upper bound of ∥∇3
ϕˆπϕ∥, then
∥∇3
θ′τˆπθ′τ(a|s)∥ ≤ˆπθ′τ(a|s)(40L3
1+ 16L1L2+ 2L3).
So, from (38)(39)(40)(41), we have
∥∇2
ϕθ′
τ∥ ≤2γL2
1Amax(6L2
1+ 2L2)(λ+2γ
1−γL2
1Amax)
(1−γ)2(λ−(6L2
1+ 2L2)Amax)2
+8γ2L2
1rmax
(1−γ)3+γ(6L2
1+ 2L2)rmax
(1−γ)21
λ−(6L2
1+ 2L2)Amax
+λ+2γ
1−γL2
1Amax
(1−γ)(λ−(6L2
1+ 2L2)Amax)2((40L3
1+ 16L1L2+ 2L3)(λ+2γ
1−γL2
1Amax)Amax
(1−γ)(λ−(6L2
1+ 2L2)Amax)
+2γ
1−γL1(6L2
1+ 2L2)Amax).
Simplify the inequality by γ <1and1−γ <0,
∥∇2
ϕθ′
τ∥ ≤(80L3
1+ 28L1L2+ 2L3)(λ+2γ
1−γL2
1Amax)2
(1−γ)3(λ−(6L2
1+ 2L2)Amax)2
Then,Z
A∥∇2
ϕˆπθ′τ(a|s)∥da≤(160L3
1+ 56L1L2+ 4L3)(λ+2γ
1−γL2
1Amax)2
(1−γ)3(λ−(6L2
1+ 2L2)Amax)2.
Lemma 15. Suppose that Assumptions 1, 2, and 3 hold. Let ˆπθ′τ=Alg(ˆπϕ, λ, τ)where Dτ=Dτ,3,
we have
∥∇2
ϕJτ(ˆπθ′τ)∥ ≤rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3, (44)
where C=2L1(λ+2γ
1−γL2
1Amax)
(1−γ)(λ−(6L2
1+2L2)Amax)andB=(160L3
1+56L1L2+4L3)(λ+2γ
1−γL2
1Amax)2
(1−γ)3(λ−(6L2
1+2L2)Amax)2 .
Proof. Similar to the proofs of Lemma 8 and Lemma 11 by using Lemma 14.
Theorem 7. In both discrete and continuous action space, consider the softmax policy with func-
tion approximation shown in Section 5.1, and the within-task algorithm Algis defined in (2)
withDτ=Dτ,3. Suppose that Assumptions 1, 2, and 3 hold. If λ > (6L2
1+ 2L2)Amax, then
∇ϕJτ(Alg(3)(ˆπϕ, λ, τ))exists for any ϕ.
38Let{ϕt}T
t=1be the sequence generated by Algorithm 1 with λ >(6L2
1+ 2L2)Amax and the step size
α= min(rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3−1
,1
G√
T)
.
Then, the following bound holds:
1
TTX
t=1Et
∥∇ϕEτ∼P(Γ)[Jτ(Alg(ˆπϕt, λ, τ))]∥2
≤2r2
maxB
(1−γ)3+4γr2
maxC2
(1−γ)41
T+2rmax
1−γ+rmaxB
(1−γ)2+2γrmaxC2
(1−γ)3G√
T,
where
G=L1Amax(λ+2γ
1−γL2
1Amax)
(1−γ)(λ−(6L2
1+ 2L2)Amax),
C=2L1(λ+2γ
1−γL2
1Amax)
(1−γ)(λ−(6L2
1+ 2L2)Amax),
and
B=(160L3
1+ 56L1L2+ 4L3)(λ+2γ
1−γL2
1Amax)2
(1−γ)3(λ−(6L2
1+ 2L2)Amax)2.
Proof. Similar to the proof of Theorem 5, by using the gradient bound in Lemma 12 and the
smoothness in Lemma 15.
N Optimality of one-time policy adaptation
N.1 Important Lemmas
Lemma 16. Suppose that Assumptions 1, 2 hold. For any task τ, and any policies πandπ′, the
following bound holds:
1
1−γE
s∼νπ
τ
a∼π′(·|s)[Aπ
τ(s, a)]−Cπ
τ(π′)≤Jτ(π′)−Jτ(π)≤1
1−γE
s∼νπ
τ
a∼π′(·|s)[Aπ
τ(s, a)] +Cπ
τ(π′)
where
Cπ
τ(π′) =4γAmax
(1−γ)2Dmax
TV(π||π′)Es∼νπτ[DTV(π(·|s)||π′(·|s))].
Here, we define DTV(π(·|s)||π′(·|s))≜1
2P
a∈A|π(a|s)−π′(a|s)|in a discrete action space
orDTV(π(·|s)||π′(·|s))≜1
2R
a∈A|π(a|s)−π′(a|s)|dain a continuous action space, and
Dmax
TV(π||π′)≜max s∈SDTV(π(·|s)||π′(·|s)).
Proof. LetPπ
τis a matrix where Pπ
τ(i, j) =Ea∼π(·|si)Pτ(sj|si, a)andPπ′
τis a matrix where
Pπ′
τ(i, j) =Ea∼π′(·|si)Pτ(sj|si, a). Let G= (1 + γPπ
τ+ (γPπ
τ)2+. . .) = (1 −γPπ
τ)−1, and
similarly ˜G= (1 + γPπ′
τ+ (γPπ′
τ)2+. . .) = (1 −γPπ′
τ)−1. Let ρbe a density vector on state
space and rτis a reward function vector on state space, thus r⊤
τρis a scalar meaning the expected
reward under density ρ. Note that Jτ(π) =r⊤
τGρτ, and Jτ(π′) =r⊤
τ˜Gρτ. Here, ρτis the initial
state distribution for task τ. Let∆ =Pπ′
τ−Pπ
τ.
Follow the proof in Appendix B in [51], we have
G−1−˜G−1= (1−γPπ)−(1−γP˜π) =γ∆.
Left multiply by ˜Gand right multiply by G,
˜G=γ˜G∆G+G. (45)
39Left multiply by Gand right multiply by ˜G,
˜G=γG∆˜G+G. (46)
Substituting the right-hand side in (45) into ˜Gin (46), then
˜G=G+γG∆G+γ2G∆˜G∆G.
So we have
Jτ(π′)−Jτ(π) =r⊤
τ(˜G−G)ρτ=γr⊤
τG∆Gρτ+γ2r⊤
τG∆˜G∆Gρτ. (47)
Note that r⊤
τG=vπ
τ⊤, where vis the value function on the state space. We also have Gρτ=1
1−γνπ
τ,
where νπ
τis the state visitation distribution vector. So,
Jτ(˜π)−Jτ(π) =r⊤
τ(˜G−G)ρτ=γ
1−γvπ
τ⊤∆νπ
τ+γ2
1−γvπ
τ⊤∆˜G∆νπ
τ.
Consider the first termγ
1−γvπ
τ⊤∆νπ
τ, similar to Equation (50) in [51], we have
γvπ
τ⊤∆νπ
τ=vπ
τ⊤(Pπ′
τ−Pπ
τ)νπ
τ
=X
sνπ
τ(s)X
s′X
a(π′(a|s)−π(a|s))Pτ(s′|s, a)γvπ
τ(s′)
=X
sνπ
τ(s)X
a(π′(a|s)−π(a|s))"
r(s) +X
s′Pτ(s′|s, a)γvπ
τ(s′)−v(s)#
=X
sνπ
τ(s)X
a(π′(a|s)−π(a|s))Aπ
τ(s, a)(48)
Since we haveP
aπ(a|s)Aπ
τ(s, a) = 0 , we have
γvπ
τ⊤∆νπ
τ=X
sνπ
τ(s)X
aπ′(a|s)Aπ
τ(s, a) = E
s∼νπ
τ
a∼π′(·|s)[Aπ
τ(s, a)].
Combine (47) and the above equation, we have the following for the second term:
γ2
1−γvπ
τ⊤∆˜G∆νπ
τ=Jτ(π′)−Jτ(π)−1
1−γE
s∼νπ
τ
a∼π′(·|s)[Aπ
τ(s, a)].
Then we need to show γ2
1−γvπ
τ⊤∆˜G∆νπ
τ≤Cπ
τ(π′).
First, by Hölder’s inequality,
γ2
1−γvπ
τ⊤∆˜G∆νπ
τ≤γ
1−γ∥γvπ
τ⊤∆∥∞∥˜G∆νπ
τ∥1.
Similar to (48), each element in the vector γvπ
τ⊤∆isP
a(π′(a|s)−π(a|s))Aπ
τ(s, a), then we have
∥γvπ
τ⊤∆∥∞≤X
a|π′(a|s)−π(a|s)|Aπ
τ(s, a)≤2AmaxDmax
TV(π||π′).
From the Lemma 3 of [1], we have
∥˜G∆νπ
τ∥1≤2
1−γEs∼νπτ[DTV(π(·|s)||π′(·|s))].
Therefore, we have
γ2
1−γvπ
τ⊤∆˜G∆νπ
τ≤Cπ
τ(π′) =4γAmax
(1−γ)2Dmax
TV(π||π′)Es∼νπτ[DTV(π(·|s)||π′(·|s))].
Then the bounds hold.
40Lemma 17. Suppose that Assumptions 1, 2 hold. For any task τ, any bounded parameters θandθ′,
andi= 1or2, the following bound holds for both i= 1and2:
Jτ(ˆπθ′)−Jτ(ˆπθ)≤1
1−γE
s∼νˆπθτ
a∼ˆπθ′(·|s)
Aˆπθτ(s, a)
+2γAmax
(1−γ)2ϵD2
τ,i(ˆπθ,ˆπθ′)
and
Jτ(ˆπθ′)−Jτ(ˆπθ)≥1
1−γE
s∼νˆπθτ
a∼ˆπθ′(·|s)
Aˆπθτ(s, a)
−2γAmax
(1−γ)2ϵD2
τ,i(ˆπθ,ˆπθ′).
Proof. The proof follows similar lines of Theorem 1 in [ 51] and Corollary 1 and 2 in [ 1]. For the
sake of self-containedness, we provide the complete proof.
We show the first inequality. The second inequality follows a similar way. From Lemma 16,
Jτ(ˆπθ′)−Jτ(ˆπθ)−1
1−γE
s∼νˆπθτ
a∼ˆπθ′(·|s)
Aˆπθτ(s, a)
≤4γAmax
(1−γ)2Dmax
TV(ˆπθ||ˆπθ′)Es∼νˆπθτ[DTV(ˆπθ(·|s)||ˆπθ′(·|s))].
From Assumption 2, νˆπθ(s)≥ϵfor any s∈ A. Also, DTV(ˆπθ(·|s)||ˆπθ′(·|s))≥0for any s∈ A.
Then, we have
ϵDmax
TV(ˆπθ||ˆπθ′)≤Es∼νˆπθτ[DTV(ˆπθ(·|s)||ˆπθ′(·|s))].
From Jensen’s inequality, we have
Es∼νˆπθτ[DTV(ˆπθ(·|s)||ˆπθ′(·|s))]2≤Es∼νˆπθτ
D2
TV(ˆπθ(·|s)||ˆπθ′(·|s))
.
From the above three inequalities, we have
Jτ(ˆπθ′)−Jτ(ˆπθ)−1
1−γE
s∼νˆπθτ
a∼ˆπθ′(·|s)
Aˆπθτ(s, a)
≤4γAmax
(1−γ)2ϵEs∼νˆπθτ
D2
TV(ˆπθ(·|s)||ˆπθ′(·|s))
.
(49)
From [8], we have
D2
TV(ˆπθ(·|s)||ˆπθ′(·|s))≤1
2DKL(ˆπθ(·|s)||ˆπθ′(·|s)),
and
D2
TV(ˆπθ(·|s)||ˆπθ′(·|s))≤1
2DKL(ˆπθ′(·|s)||ˆπθ(·|s)).
Therefore,
Jτ(ˆπθ′)−Jτ(ˆπθ)≤1
1−γE
s∼νˆπθτ
a∼ˆπθ′(·|s)
Aˆπθτ(s, a)
+2γAmax
(1−γ)2ϵD2
τ,1(ˆπθ,ˆπθ′),
and
Jτ(ˆπθ′)−Jτ(ˆπθ)≤1
1−γE
s∼νˆπθτ
a∼ˆπθ′(·|s)
Aˆπθτ(s, a)
+2γAmax
(1−γ)2ϵD2
τ,2(ˆπθ,ˆπθ′).
Lemma 18. Consider the softmax policy with function approximation shown in Section 5.1. Suppose
that Assumptions 1, 2, and 3 hold. For any task τ, and any softmax policies parameterized by bounded
θandθ′, the following bound holds:
Jτ(ˆπθ′)−Jτ(ˆπθ)≤1
1−γE
s∼νˆπθτ
a∼ˆπθ′(·|s)
Aˆπθτ(s, a)
+4γAmaxL2
1
(1−γ)2ϵ∥θ−θ′∥2
and
Jτ(ˆπθ′)−Jτ(ˆπθ)≥1
1−γE
s∼νˆπθτ
a∼ˆπθ′(·|s)
Aˆπθτ(s, a)
−4γAmaxL2
1
(1−γ)2ϵ∥θ−θ′∥2.
41Proof. From (36), for any θ∈Rn,
∇θˆπθ(a|s) = ˆπθ(a|s)∇θfθ(s, a)−ˆπθ(a|s)R
A∇θfθ(s, a′) exp ( fθ(s, a′))da′
R
Aexp (fθ(s, a′))da′.
Then,
∥∇θˆπθ(a|s)∥ ≤ˆπθ(a|s)∥∇θfθ(s, a)∥+ ˆπθ(a|s)R
A∇θfθ(s, a′) exp ( fθ(s, a′))da′
R
Aexp (fθ(s, a′))da′
≤2ˆπθ(a|s)L1
From the mean value theorem, we have
|ˆπθ(a|s)−ˆπθ′(a|s)| ≤2ˆπϕ(a)(a|s)L1∥θ−θ′∥,
where ϕ(a) =δ(a)θ+ (1−δ(a))θ′and0≤δ(a)≤1. So,
1
2X
a∈A|ˆπθ(a|s)−ˆπθ′(a|s)| ≤L1∥θ−θ′∥.
From (49), we have
Jτ(ˆπθ′)−Jτ(ˆπθ)−1
1−γE
s∼νˆπθτ
a∼ˆπθ′(·|s)
Aˆπθτ(s, a)
≤4γAmaxL2
1
(1−γ)2ϵ∥θ−θ′∥2.
We use the same way to show another inequality.
N.2 Proof of Theorems 3 and 4
Proof of Theorem 3. When the requirement of Theorem 1, λ≥2Amax, is satisfied, From Assump-
tion 4 and Theorem 1, for both i= 1and2,
1
TTX
t=1Et
max
ϕEτ∼P(Γ)[Jτ(Alg(i)(ˆπϕ, λ, τ))−Eτ∼P(Γ)[Jτ(Alg(i)(ˆπϕt, λ, τ))]]
≤1
TTX
t=1Eth
hi
∥∇ϕEτ∼P(Γ)[Jτ(Alg(i)(ˆπϕt, λ, τ))]∥2i
≤hi 
1
TTX
t=1Eth
∥∇ϕEτ∼P(Γ)[Jτ(Alg(i)(ˆπϕt, λ, τ))]∥2i!
≤hiKi
T+Mi√
T(50)
where the constants KiandMiare shown in Theorem 1. The last inequality sign comes from that hi
is a concave function and Jensen’s inequality.
Letˆπθ′τ(ϕ) =Alg(i)(ˆπϕ, λ, τ)for any meta-parameter ϕ. From the definition of the within-task
algorithm, we have
E
s∼νˆπϕ
τ
a∼ˆπθ′τ(ϕ)(·|s)h
Qˆπϕτ(s, a)i
−λD2
τ,i(ˆπϕ,ˆπθ′τ(ϕ))≥ E
s∼νˆπϕ
τ
a∼ˆπθ∗τ(·|s)h
Qˆπϕτ(s, a)i
−λD2
τ,i(ˆπϕ,ˆπθ∗τ).
This is equivalent to
E
s∼νˆπϕ
τ
a∼ˆπθ′τ(ϕ)(·|s)h
Aˆπϕτ(s, a)i
−λD2
τ,i(ˆπϕ,ˆπθ′
τ(ϕ))≥ E
s∼νˆπϕ
τ
a∼ˆπθ∗τ(·|s)h
Aˆπϕτ(s, a)i
−λD2
τ,i(ˆπϕ,ˆπθ∗
τ).
42when λ≥2γAmax
(1−γ)ϵ, from the second inequality in Lemma 17 and the above inequality,
Jτ(ˆπθ′τ(ϕ))−Jτ(ˆπϕ)≥1
1−γE
s∼νˆπϕ
τ
a∼ˆπθ′τ(ϕ)(·|s)h
Aˆπϕτ(s, a)i
−2γAmax
(1−γ)2ϵD2
τ,i(ˆπϕ,ˆπθ′τ(ϕ))
≥1
1−γE
s∼νˆπϕ
τ
a∼ˆπθ′τ(ϕ)(·|s)h
Aˆπϕτ(s, a)i
−λ
1−γD2
τ,i(ˆπϕ,ˆπθ′τ(ϕ))
≥1
1−γE
s∼νˆπϕ
τ
a∼ˆπθ∗τ(·|s)h
Aˆπϕτ(s, a)i
−λ
1−γD2
τ,i(ˆπϕ,ˆπθ∗τ).
From the second inequality in Lemma 17,
Jτ(ˆπθ∗τ)−Jτ(ˆπϕ)≤1
1−γE
s∼νˆπϕ
τ
a∼ˆπθ∗τ(·|s)h
Aˆπϕτ(s, a)i
+2γAmax
(1−γ)2ϵD2
τ,i(ˆπϕ,ˆπθ∗τ).
From the last two inequalities,
Jτ(ˆπθ′τ(ϕ))−Jτ(ˆπθ∗τ)≥ −(2γAmax
(1−γ)2ϵ+λ
1−γ)D2
τ,i(ˆπϕ,ˆπθ∗τ),
i.e.,
Jτ(ˆπθ∗τ)−Jτ(Alg(i)(ˆπϕ, λ, τ))≤(2γAmax
(1−γ)2ϵ+λ
1−γ)D2
τ,i(ˆπϕ,ˆπθ∗τ).
Then,
Eτ∼P(Γ)[Jτ(ˆπθ∗τ)−Jτ(Alg(i)(ˆπϕ, λ, τ))]≤(2γAmax
(1−γ)2ϵ+λ
1−γ)Eτ∼P(Γ)[D2
τ,i(ˆπϕ,ˆπθ∗τ)].
Letϕ∗= arg max ϕEτ∼P(Γ)[Jτ(Alg(i)(ˆπϕ, λ, τ))], we have
Eτ∼P(Γ)[Jτ(Alg(i)(ˆπϕ∗, λ, τ))]≥max
ϕEτ∼P(Γ)[Jτ(Alg(i)(ˆπϕ, λ, τ))].
Therefore,
Eτ∼P(Γ)[Jτ(ˆπθ∗τ)−Jτ(Alg(i)(ˆπϕ∗, λ, τ))]≤min
ϕEτ∼P(Γ)[Jτ(ˆπθ∗τ)−Jτ(Alg(i)(ˆπϕ, λ, τ))]
≤min
ϕ(2γAmax
(1−γ)2ϵ+λ
1−γ)Eτ∼P(Γ)[D2
τ,i(ˆπϕ,ˆπθ∗τ)]
Since
min
ϕEτ∼P(Γ)[D2
τ,i(ˆπϕ,ˆπθ∗τ)] =Vari(P(Γ)),
we have
Eτ∼P(Γ)[Jτ(ˆπθ∗τ)−Jτ(Alg(i)(ˆπϕ∗, λ, τ))]≤(2γAmax
(1−γ)2ϵ+λ
1−γ)Vari(P(Γ)).
Note that in the above analysis, we need λ≥2Amax and also λ≥2γAmax
(1−γ)ϵ. So, we select we select
λ=2Amax
(1−γ)ϵto satisfy the requirement. When λ=2Amax
(1−γ)ϵ, we have
Eτ∼P(Γ)[Jτ(ˆπθ∗τ)−Jτ(Alg(i)(ˆπϕ∗, λ, τ))]≤2(1 + γ)Amax
(1−γ)2ϵVari(P(Γ)). (51)
From (50) and (51) we have
1
TTX
t=1Eth
Eτ∼P(Γ)[Jτ(ˆπθ∗τ)−Jτ(Alg(i)(ˆπϕt, λ, τ))]i
≤hiKi
T+Mi√
T
+2(1 + γ)Amax
(1−γ)2ϵVari(P(Γ)).
43Proof of Theorem 4. Similar to the above proof of Theorem 3. The difference is using two inequalities
in Lemma 18 instead of those in Lemma 17 and using Theorem 2 for convergence instead of Theorem
1.
The requirement of Theorem 2 is λ >(6L2
1+ 2L2)Amax, and the requirement of Lemma 18 is
λ≥4γAmaxL2
1
(1−γ)ϵ. Therefore, we select λ=(6L2
1+2L2)Amax
(1−γ)ϵ. Then, the bound is
1
TTX
t=1Eth
Eτ∼P(Γ)[Jτ(ˆπθ∗τ)−Jτ(Alg(3)(ˆπϕt, λ, τ))]i
≤h3K3
T+M3√
T
+4γL2
1Amax
(1−γ)2ϵ+λ
1−γ
Var3(P(Γ)),
≤h3K3
T+M3√
T
+((6 + 4 γ)L2
1+ 2L2)Amax
(1−γ)2ϵVar3(P(Γ)),
N.3 Clarification of Amax
In all the proofs in Sections N.1 and N.1, we can replace as Amax toA′
max, where A′
max is defined by
the maximum advantage function value of policy ˆπϕ′, where ϕ′= arg min ϕEτ∼P(Γ)[D2
τ,i(ˆπϕ,ˆπθ∗τ)].
It is easy to see A′
max≤Amax. For simplification of the assumption statements, theorem statements,
and convenience of the proofs, we keep Amax in the proofs and Theorems 3 and 4. We actually
can make the bound in Theorems 3 and 4 tighter by replacing Amax toA′
max. In the verification of
the theoretical results of Section 6, we select λbased on A′
max and verify the tighter bounds by the
experiments.
O Proofs of Remarks
Proof of part (i) of Remark 1 . If the MDP Mτis ergodic, there exists a policy ˆπsuch that νˆπ
τ(s)≥
ϵ0. Asϕis bounded, the probability (or probability density) of each action of the softmax policy is
larger than 0and lower bounded by a ϵ1>0. Therefore, the action probability of the policy ˆπ(a|s)
can be upper bounded by ˆπϕ(a|s)/ϵ1for any a. Therefore, νˆπϕτ(s)≥ϵ0/ϵ1.
Proof of part (ii) of Remark 1 . If the initial state distribution ρτhasρτ(s)>0for any s∈ S. Since
Sis bounded, ρτ(s)≥ϵ2for any s∈ S. Then, νˆπϕτ(s)≥(1−γ)ϵ2.
P Limitations
In this paper, we provide several theorems, where the hyper-parameter selection, e.g., λ, is provided
by the theorems. The theoretical analysis usually chooses hyper-parameters, which are sometimes
conservative. In practice, we can tune them to improve the performance.
44NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction, including the main contribution statement and
related works, accurately reflect the paper’s contributions and scope.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We provide the limitations in the Appendix.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
45Justification: All proofs are provided in Appendix
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide all details of the information needed to reproduce the main
experimental results in the experiment section and in Appendix A and B.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
46Answer: [Yes]
Justification: We provide open access to the data and code with sufficient instructions in the
supplemental material.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide all training details in Appendix A and B.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We provide it in the section of the experiment.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
47•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide the information in the beginning of the Appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: It is followed.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper presents work whose goal is to advance the field of Machine
Learning. There is no potential societal consequence.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
48•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
49•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
50