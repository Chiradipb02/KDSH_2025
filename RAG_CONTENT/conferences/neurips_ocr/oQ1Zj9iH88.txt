Penalty-based Methods for Simple Bilevel
Optimization under Hölderian Error Bounds
Pengyu Chen∗
School of Data Science
Fudan University
pychen22@m.fudan.edu.cnXu Shi∗
School of Data Science
Fudan University
xshi22@m.fudan.edu.cnRujun Jiang†
School of Data Science
Fudan University
rjjiang@fudan.edu.cn
Jiulin Wang
School of Data Science
Fudan University
wangjiulin@fudan.edu.cn
Abstract
This paper investigates simple bilevel optimization problems where we minimize
an upper-level objective over the optimal solution set of a convex lower-level ob-
jective. Existing methods for such problems either only guarantee asymptotic
convergence, have slow sublinear rates, or require strong assumptions. To address
these challenges, we propose a penalization framework that delineates the rela-
tionship between approximate solutions of the original problem and its reformu-
lated counterparts. This framework accommodates varying assumptions regard-
ing smoothness and convexity, enabling the application of speciﬁc methods with
different complexity results. Speciﬁcally, when both upper- and lower-level objec-
tives are composite convex functions, under an α-Hölderian error bound condition
and certain mild assumptions, our algorithm attains an (ϵ,ϵβ)-optimal solution of
the original problem for any β > 0within O(√
1/ϵmax{α,β})
iterations. The
result can be improved further if the smooth part of the upper-level objective is
strongly convex. We also establish complexity results when the upper- and lower-
level objectives are general nonsmooth functions. Numerical experiments demon-
strate the effectiveness of our algorithms.
1 Introduction
Bilevel optimization involves embedding one optimization problem within another, creating a hi-
erarchical structure where the upper-level problem’s feasible set is inﬂuenced by the lower-level
problem. This framework frequently occurs in various real-world scenarios, such as meta-learning
[Bertinetto et al. ,2018 ,Rajeswaran et al. ,2019 ], hyper-parameter optimization [ Chen et al. ,2024 ,
Franceschi et al. ,2018 ,Shaban et al. ,2019 ], reinforcement learning [ Mingyi et al. ,2020 ] and ad-
versarial learning [ Bishop et al. ,2020 ,Wang et al. ,2021 ,2022 ]. In this paper, we concentrate on
a subset of bilevel optimization known as simple bilevel optimization (SBO), which has garnered
signiﬁcant interest in the machine learning community due to its relevance in dictionary learning
[Beck and Sabach ,2014 ,Jiang et al. ,2023 ], lexicographic optimization [ Kissel et al. ,2020 ,Gong
et al. ,2021 ], lifelong learning [ Malitsky ,2017 ,Jiang et al. ,2023 ]; see more details in Appendix A.
∗Equal contribution
†Corresponding author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).SBO aims to ﬁnd an optimal solution that minimizes the upper-level objective over the solution set
of the lower-level problem. In other words, we are interested in solving the following problem:
min
x∈RnF(x)s.t.x∈arg min
z∈RnG(z). (P)
HereF,G :Rn→R∪{∞} are proper, convex, and lower semi-continuous functions. We also
assume that the optimal solution set of the lower-level problem, denoted as Xopt, is nonempty. More-
over, sinceGis convex and lower semi-continuous, it holds that Xoptis closed and convex [ Bertsekas
et al. ,2003 , Proposition 1.2.2 and Page 49].
In this paper, we ﬁrst reformulate problem ( P) into the constrained form:
min
x∈RnF(x)s.t.G(x)−G∗≤0, (PVal)
whereG∗represents the optimal value of the unconstrained lower-level problem.
Based on this reformulation, we consider the following penalization of ( PVal),
min
x∈RnΦγ(x) =F(x) +γp(x), (Pγ)
wherep(x) :=G(x)−G∗is the so-called residual function and γ > 0is the penalized parameter.
Obviously, we have p(x)≥0, andp(x) = 0 if and only if x∈Xopt.
DenoteF∗andG∗as the optimal values of problem ( P) and the lower-level problem min x∈RnG(x),
respectively. We aim to ﬁnd an (ϵF,ϵG)-optimal solution ˜x∗of problem ( P), which satisﬁes
F(˜x∗)−F∗≤ϵF, G (˜x∗)−G∗≤ϵG. (1)
Moreover, a point ˜x∗
γis said to be an ϵ-optimal solution of problem ( Pγ) if
Φγ(˜x∗
γ)−Φ∗
γ≤ϵ,
where Φ∗
γis the optimal value of problem ( Pγ).
1.1 Related work
Various approaches have been developed to solve problem ( P) [Cabot ,2005 ,Solodov ,2007 ,Sabach
and Shtern ,2017 ,Dutta and Pandit ,2020 ,Gong et al. ,2021 ]. Among those, one category that is
the most related to penalization formulation ( Pγ) is the regularization method, which integrates the
upper- and lower-level objectives through Tikhonov regularization [ Tikhonov and Arsenin ,1977 ]
min
x∈Rnη(x) :=σF(x) +G(x), (PReg)
whereσis the so-called regularization parameter. When Fis strongly convex and its domain is com-
pact, Amini and Youseﬁan [2019 ] extended the IR-PG method from Solodov [2007 ], which achieved
a asymptotic convergence rate for the upper-level problem and a convergence rate of O(
1/K0.5−b)
for the lower-level problem, where b∈(0,0.5).Malitsky [2017 ] studied a version of Tseng’s ac-
celerated gradient method and showed a convergence rate of O(1/K)for the lower-level problem,
while the convergence rate for the upper-level objective is not explicitly provided. Kaushik and
Youseﬁan [2021 ] proposed an iteratively regularized gradient (a-IRG) method which obtains a com-
plexity of O(
1/K0.5−b)
andO(
1/Kb)
for the upper- and lower-level objective, respectively, where
b∈(0,0.5). Inspired by this research, and under a quasi-Lipschitz assumption for F,Merchav and
Sabach [2023 ] introduced a bi-subgradient (Bi-SG) method. This method demonstrates convergence
rates of O(1/Kb)andO(1/K1−b)for the lower- and upper-level objectives, respectively, where
b∈(0.5,1). In their framework, the convergence rate of the upper-level objective can be improved
to be linear when Fis strongly convex. Recently, under the weak-sharp minima assumption of the
lower-level problem, Samadi et al. [2023 ] proposed a regularized accelerated proximal method (R-
APM), showing a convergence rate of O(1/K2)for both upper- and lower-level objectives. When
the domain is compact and F,G are both smooth, Giang-Tran et al. [2023 ] proposed an iteratively
regularized conditional gradient (IR-CG) method, which ensures convergence rates of O(1/Kp)and
O(1/K1−p)for upper- and lower-level objectives, respectively, where p∈(0,1).
Despite the abundance of existing methodologies yielding non-asymptotic convergence outcomes,
their efﬁcacy is frequently contingent upon additional assumptions. Denote Lf1andLg1as the Lips-
chitz constants for the gradients of the smooth components in the upper- and lower-level objectives,
2respectively. Speciﬁcally, when Fis strongly convex and Gis smooth, Beck and Sabach [2014 ] pre-
sented the Minimal Norm Gradient (MNG) method and provided the asymptotic convergence to the
optimal solution set and a convergence rate of O(
L2
g1/ϵ2)
for the lower-level problem. When Fis
assumed to be smooth, Jiang et al. [2023 ] introduced a conditional gradient-based bilevel optimiza-
tion (CG-BiO) method, which invokes at most O(max{Lf1/ϵF,Lg1/ϵG})of linear optimization
oracles to achieve an (ϵF,ϵG)-optimal solution. Shen et al. [2023 ] combined an online framework
with the mirror descent algorithm and established a convergence rate of O(1/ϵ3)for both upper- and
lower-level objectives, assuming a compact domain and boundedness of the functions and gradients
at both levels. Furthermore, they showed that the convergence rate can be improved to O(1/ϵ2)
under additional structural assumptions. For a concise overview of overall methodologies, including
their assumptions and convergence outcomes, refer to Table 1in Appendix B.
For general bilevel optimization problems, there have been recent results on convergent guarantees
[Shen and Chen ,2023 ,Sow et al. ,2022 ,Chen et al. ,2023 ,Huang ,2023 ]. Among those, the one that
is the most related to ours is [ Shen and Chen ,2023 ]. It investigates the case when the upper-level
objective is nonconvex and gives convergence results under additional assumptions [ Shen and Chen ,
2023 , Theorem 3 and 4]. However, as the general bilevel optimization problem is nonconvex, the
algorithms in the literature often converge to weak stationary points, while our method for SBO
converges to global optimal solution.
1.2 Our approach
Our approach is straightforward. Firstly, we introduce a penalization framework delineating the
connection between approximate solutions of problems ( P) and ( Pγ). This framework enables the
attainment of an (ϵF,ϵG)-optimal solution by solving problem ( Pγ) approximately. Subsequently,
our focus shifts solely to resolving the unconstrained problem ( Pγ). Depending on varying assump-
tions regarding smoothness and convexity, we can employ different methods such as the accelerated
proximal gradient (APG) methods [ Beck and Teboulle ,2009 ,Nesterov ,2013 ,Lin and Xiao ,2014 ]
to solve problem ( Pγ). We summarize our main contributions as follows.
•We propose a framework that explicitly examines the relationship between an ϵ-optimal
solution of penalty formulation ( Pγ) and an (ϵF,ϵG)-optimal solution of problem ( P). We
also provide a lower bound for the metric F(x)−F∗.
•WhenFandGare both composite convex functions, we provide a penalty-based APG
algorithm that attains an (ϵ,ϵβ)-optimal solution of problem ( P) within O(√
1/ϵmax{α,β})
iterations. If the upper-level objective is strongly convex, the complexity can be improved
toO(√
1/ϵmax{α−1,β−1}log1
ϵ). We also apply our method for the scenario where both
the upper- and lower-level objectives are generalized nonsmooth convex functions.
•We present adaptive versions of PB-APG and PB-APG-sc with warm-start, which dynami-
cally adjust the penalty parameters, and solve the associated penalized problem with adap-
tive accuracy. The adaptive ones have similar complexity results as their primal counter-
parts but can achieve superior performance in some experiments.
Utilizing the penalization method to address the original SBO problem is a novel approach. While
Tikhonov regularization may seem similar to our framework, its principles differ. Implementing
Tikhonov regularization necessitates the "slow condition" ( limk→∞σk= 0,∑∞
k=0σk= +∞),
which requires iterative solutions for each iteration. In contrast, our method simply involves solving
a single optimization problem ( Pγ) for a given γ. Furthermore, we establish a relationship between
the approximate solutions of the original bilevel problem and those of the reformulated single-level
problem ( Pγ) for a speciﬁc γ. This is the ﬁrst theoretical result connecting the original bilevel
problem to the penalization problem, accompanied by an optimal non-asymptotic complexity result.
2 The penalization framework
We begin by outlining speciﬁc assumptions for FandG, as detailed below.
Assumption 2.1. The setS:=∪
x∈Xopt∂F(x)is bounded with a diameter lF:= maxξ∈S∥ξ∥.
Note that the type of subdifferential ∂Fused here is the most general form for a convex function,
as detailed in [ Bertsekas et al. ,2003 , Section 4.2]. When the upper-level objective Fis non-convex,
3we replace the assumption with the condition that the upper-level objective is Lipschitz continuous
(cf. Theorems 2.7and2.8).
Assumption 2.2 (Hölderian error bound) .The function p(x) :=G(x)−G∗satisﬁes the Hölderian
error bound with exponent α≥1andρ> 0. Namely,
dist(x,X opt)α≤ρp(x),∀x∈dom(G),
where dist (x,X opt) := inf y∈Xopt∥x−y∥.
We remark that Hölderian error bounds are satisﬁed by many practical problems and widely used in
optimization literature [ Pang ,1997 ,Bolte et al. ,2017 ,Zhou and So ,2017 ,Roulet and d’Aspremont ,
2020 ,Jiang and Li ,2022 ]. There are two notable special cases: (i) when α= 1, we often refer to
Xoptas a set of weak sharp minima of G[Burke and Ferris ,1993 ,Studniarski and Ward ,1999 ,Burke
and Deng ,2005 ,Samadi et al. ,2023 ]; (ii) whenα= 2, Assumption 2.2is known as the quadratic
growth condition [ Drusvyatskiy and Lewis ,2018a ]. Additional examples of functions exhibiting
Hölderian error bound, along with their corresponding parameters, are presented in Appendix C.
We are now ready to establish the connection between approximate solutions of problems ( P) and
(Pγ). The subsequent two lemmas build upon the work of Shen and Chen [2023 ] for (general) bilevel
optimization. Compared with their work, we generalize the exponent αfrom 2toα≥1, providing
a more general result. Furthermore, we also derive a lower bound for the penalized parameter for all
α≥1and present a theoretical framework for these scenarios.
Lemma 2.3. Suppose that Assumptions 2.1and2.2hold withα> 1. Then, for any ϵ>0, an optimal
solution of problem (P)is anϵ-optimal solution of problem (Pγ)whenγ≥ρlα
F(α−1)α−1α−αϵ1−α.
Lemma 2.3establishes the relationship between an optimal solution of problem ( P) and anϵ-optimal
solution of problem ( Pγ) whenα > 1. It also provides a lower bound for γ, which plays a pivotal
role in the complexity results. The proofs of this paper are deferred to Appendix E.
The lemma presented below yields a more favorable outcome when α= 1, which is referred to as
exact penalization. Notably, this speciﬁc result is not discussed in Shen and Chen [2023 ].
Lemma 2.4. Suppose that Assumptions 2.1and2.2hold withα= 1. Then an optimal solution of
problem (P)is also an optimal solution of problem (Pγ)ifγ≥ρlF, and vice versa if γ > ρlF. In
this case, we say that there is an exact penalization between problems (P)and(Pγ).
For simplicity, we deﬁne
γ∗={
ρlα
F(α−1)α−1α−αϵ1−αifα> 1
ρlF ifα= 1. (2)
Based on Lemmas 2.3and2.4, we give an overall relationship of approximate solutions between
problems ( Pγ) and ( P).
Theorem 2.5. Suppose that Assumptions 2.1and2.2hold. For any given ϵ>0andβ > 0, let
γ=γ∗+{
2lβ
Fϵ1−βifα> 1,
lβ
Fϵ1−βifα= 1,
withγ∗deﬁned in (2). If˜x∗
γis anϵ-optimal solution of problem (Pγ), then ˜x∗
γis an (ϵ,l−β
Fϵβ)-
optimal solution of problem (P).
Particularly, we are also able to establish a lower bound for F(˜x∗
γ)−F∗under the same conditions
outlined in Theorem 2.5.
Theorem 2.6. Suppose that the conditions in Theorem 2.5hold. Then, ˜x∗
γsatisﬁes the following
suboptimality lower bound,
F(˜x∗
γ)−F∗≥ −lF(ρl−β
Fϵβ)1
α.
By settingβ=α, we obtainF(˜x∗
γ)−F∗≥ −ρ1
αϵ. which along with Theorem 2.5gives
|F(˜x∗
γ)−F∗| ≤max{ϵ,ρ1
αϵ}.
We emphasize that the lower bound established in Theorem 2.6is an intrinsic property of problem
(P) under Assumptions 2.1and2.2. This property is independent of the algorithms we present.
42.1 The upper-level function is non-convex
Note that the upper-level objective Fis required to be convex in the above context (cf. Theorem
2.5). This raises a question: while Theorem 2.5establishes the relationship between approximate
solutions of problems ( P) and ( Pγ), the distinction between the global or local optimal solutions of
problem ( P) and ( Pγ) remains unclear when Fis non-convex.
We ﬁrst establish the relationship between global optimal solutions of problems ( P) and ( Pγ) when
Fis non-convex, which is similar to Theorem 2.5.
Theorem 2.7. Suppose that Assumption 2.2holds,Gis convex, and Fisl-Lipschitz continuous on
dom(F). For any given ϵ>0andβ > 0, let
γ=γ∗+{
2lβϵ1−βifα> 1,
lβϵ1−βifα= 1,(3)
whereγ∗is given by (2). If ˜x∗
γis anϵ-global optimal solution of problem (Pγ), then ˜x∗
γis an
(ϵ,l−βϵβ)-global optimal solution of problem (P).
Theorem 2.7provides the relationship between the global optimal solutions of problems ( Pγ) and
(P). However, the relationship between local optimal solutions of these problems is more intricate
than those of the global ones [ Shen and Chen ,2023 ]. Givenr > 0andz∈Rn, deﬁne B(z,r) :=
{x∈Rn:∥x−z∥ ≤r}. We present the following theorem, which demonstrates that local optimal
solutions of problem ( Pγ) can serve as approximate local optimal solutions of problem ( P).
Theorem 2.8. Suppose that Assumption 2.2holds andGis convex. Let x∗
γbe a local optimal
solution of problem (Pγ)onB(x∗
γ,r). AssumeFisl-Lipschitz continuous on B(x∗
γ,r). Then x∗
γis an
approximate local optimal solution of problem (P)that satisﬁes F(x∗
γ)−F∗
B≤0andG(x∗
γ)−G∗≤
ϵwhenα> 1andγ≥(ρlα
ϵα−1)1
α, whereF∗
Bis the optimal value of problem (P)onB(x∗
γ,r)∩Xopt.
Furthermore, x∗
γis a local optimal solution of problem (P)whenα= 1andγ >ρl .
Indeed, the relationship between approximate local optimal solutions of problems ( Pγ) and ( P) is
more intricate than the connection among global solutions presented in Theorem 2.5. These inter-
actions will be the focus of our future work. The proofs of Theorems 2.7and2.8are presented in
Appendixes E.5andE.6.
3 Main algorithms
In this section, we concentrate on addressing problem ( P), making various assumptions, and offering
distinct convergence outcomes.
3.1 Both objectives are convex composite functions
In this scenario, we address problem ( P) whereFandGare both composite functions, i.e., F=
f1+f2andG=g1+g2.
Assumption 3.1. FandGsatisfy the following assumptions.
(1) The gradient of f1(x), denoted as ∇f1, isLf1-Lipschitz continuous on dom(F);
(2) The gradient of g1(x), denoted as ∇g1, isLg1-Lipschitz continuous on dom(G);
(3)f2andg2are proper, convex, lower semicontinuous, and possibly non-smooth.
We remark that Assumption 3.1(1)(3) is more general than many existing papers in the literature.
Speciﬁcally, while previous works such as Beck and Sabach [2014 ],Amini and Youseﬁan [2019 ],
Jiang et al. [2023 ],Giang-Tran et al. [2023 ] require the upper-level objective to be smooth or strongly
convex, we simply assume that Fis a composite function composed of a smooth convex function
and a possibly non-smooth convex function. For the lower-level objective, previous works such
asBeck and Sabach [2014 ],Amini and Youseﬁan [2019 ],Jiang et al. [2023 ],Giang-Tran et al.
[2023 ] impose smoothness assumptions and, in some cases, convexity and compactness constraints
on the domain; while our approach does not require these additional constraints, allowing for more
ﬂexibility and generality as presented in Assumption 3.1(2)(3).
5We are now prepared to introduce two algorithms: the penalty-based accelerated proximal gradient
(PB-APG) algorithm and its adaptive counterpart, the aPB-APG to solve problem ( Pγ) and, subse-
quently, to obtain an (ϵF,ϵG)-optimal solution of problem ( P).
To simplify notations, we omit the constant term −γG∗, and rewrite problem ( Pγ) as follows,
min
x∈RnΦγ(x) :=ϕγ(x) +ψγ(x), (PΦ)
whereϕγ(x) =f1(x) +γg1(x)andψγ(x) =f2(x) +γg2(x)represent the smooth and nonsmooth
parts, respectively. Then, it follows that the gradient of ϕγ(x)isLγ-Lipschitz continuous with
Lγ=Lf1+γLg1.
To implement the APG methods, we need another assumption concerning ψγ(x).
Assumption 3.2. For anyγ > 0, the function ψγ(x)is prox-friendly, i.e., the proximal mapping
proxtψγ(y) := arg min
x∈Rn{ψγ(x) +1
2t∥x−y∥2},
is easy to compute for any t>0.
The function ψγ(x)represents the sum of two non-smooth functions, and proximal mapping for
such function sums is widely studied and used in the literature [ Yu,2013 ,Pustelnik and Condat ,
2017 ,Adly et al. ,2019 ,Boob et al. ,2023 ,Latafat et al. ,2023 ]. This assumption is also a more
general requirement compared to many existing algorithms [ Sabach and Shtern ,2017 ,Giang-Tran
et al. ,2023 ]. It is important to note that our assumption is more general than existing literature.
In the simple bilevel literature, when employing proximal mappings, researchers often consider the
scenario where only one level contains a nonsmooth term (see, e.g., [ Jiang et al. ,2023 ,Doron and
Shtern ,2023 ,Samadi et al. ,2023 ,Merchav and Sabach ,2023 ]). In this case, the proximal mapping
of the sumf2+γg2is then reduced to the proximal mapping of either f2org2, which is a more
easily satisﬁed condition.
3.1.1 Accelerated proximal gradient-based algorithm
We apply the APG algorithm [ Beck and Teboulle ,2009 ,Lin and Xiao ,2014 ,Nesterov ,2013 ]
to solve problem ( PΦ), as outlined in Algorithm 1. Moreover, if the Lipschitz constant Lγis
unknown or computationally infeasible, line search [ Beck and Teboulle ,2009 ] can be adopted
and will yield almost the same complexity bound. For brevity, we denote Algorithm 1asˆx=
PB-APG (ϕγ,ψγ,Lf1,Lg1,x0,ϵ), where ˆxrepresents an ϵ-optimal solution of ( PΦ).
Algorithm 1 Penalty-based APG (PB-APG)
1:Input:γ,Lγ=Lf1+γLg1,x−1=x0∈Rn,R> 0,t−1=t0= 1,k= 0,ϵ> 0and{tk}.
2:fork≥0do
3: yk=xk+tk(
t−1
k−1−1)
(xk−xk−1)
4: xk+1=proxL−1
γψγ(yk−L−1
γ∇ϕγ(yk))
5:end for
In Algorithm 1, we stop the loop of Line. 3 - 4 if the number of iterations satisﬁes that:
2(Lf+γLg)R2
(k+ 1)2≤ϵ,
whereRis a constant that satisﬁes ∥x0−x∗∥ ≤R.
Combining Theorem 2.5and [ Tseng ,2008 , Corollary 2], we establish the following complexity result for
problem ( P).
Theorem 3.3. Suppose that Assumptions 2.1,2.2,3.1and 3.2hold and the sequence {tk}in Algorithm 1
satisﬁes1−tk+1
t2
k+1≤1
t2
k. Letγbe given as in Theorem 2.5. Algorithm 1generates an (ϵ,l−β
Fϵβ)-optimal
solution of problem (P)after at most Kiterations, where
K=O
√
Lf1
ϵ+√
lmax{α,β}
FLg1
ϵmax{α,β}
.
6Note that Theorem 3.3encompasses all possible relationships between the magnitudes of ϵFandϵGin (1),
asα≥1andβ > 0are arbitrary. Specially, if α= 1 andβ≤α, the number of iterations is K=
O(√
(Lf1+lFLg1)/ϵ)
. This result matches the lower bound complexity for unconstrained smooth or convex
composite optimization [ Nemirovsky and Yudin ,1983 ,Woodworth and Srebro ,2016 ]. Additionally, if g1≡0,
the number of iterations for obtaining an (ϵ,ϵβ)-optimal solution of problems ( P) is independent of γ, which
can be improved to K=O(√
Lf1/ϵ).
Remark 3.4. It is noteworthy that Theorem 1 in a previous paper Samadi et al. [2023 ] provides the ﬁrst method
that needs O(√
(Lg1+lFLg1)/ϵ)iterations to achieve an (ϵ,ϵ)solution ifα= 1andFis smooth. Neverthe-
less, our methodology diverges in various respects. First, our approach is rooted in the penalization formulation
of problem ( PVal), while the approach proposed by Samadi et al. [2023 ] is based on the Tikhonov regularization
[Tikhonov and Arsenin ,1977 ]. Second, we provide a theoretical framework that clearly delineates the relation-
ship between approximate solutions of problems ( P) and ( Pγ) for all cases of α≥1andFis non-convex,
as indicated in Lemmas 2.3,2.4and Theorems 2.5,2.7,2.8. Therefore, we can ﬁrst shift our focus from ( P)
to (Pγ) based on the penalization framework and then use various methods to solve ( Pγ), not limited to using
the APG methods. Besides, the association between approximate solutions of problem ( P) and ( Pγ) differs
signiﬁcantly based on whether α > 1orα= 1. For the case of α > 1, the lower bound comprehensively
integrates the accuracy parameter ϵ, which results in a more sophisticated analysis of the convergence result,
while Samadi et al. [2023 ] did not consider the situation when α > 1. Third, our method applies to the case
thatFis composite, while Samadi et al. [2023 ] requiresFto be smooth. Finally, we also propose an adaptive
version of our algorithm (see Algorithm 2) that does not require an estimate of γ.
3.1.2 Adaptive version with warm-start mechanism
In practice, the penalty parameter γmight be difﬁcult to determine. This motivates us to propose Algorithm 2,
which adaptively updates γand invokes PB-APG with dynamic γand solution accuracies.
Algorithm 2 Adaptive PB-APG method (aPB-APG)
1:Input: x0∈Rn,γ0=γ1>0,Lf1,Lg1,ν > 1,η> 1,ϵ0>0.
2:fork≥0do
3:ϕk(x) =f1(x) +γkg1(x)
4:ψk(x) =f2(x) +γkg2(x)
5: Invoke xk=PB-APG (ϕk,ψk,Lf1,Lg1,xk−1,ϵk)
6:ϵk+1=ϵk/η
7:γk+1=νγk
8:end for
In Algorithm 2, we adaptively update the penalty parameter γk, and invoke the PB-APG to generate an ap-
proximate solution for ( Pγ) with accuracy ϵ=ϵk. Meanwhile, a warm-start mechanism is employed, meaning
that the initial point for each subproblem is the output of the preceding subproblem. The convergence result of
Algorithm 2is as follows.
Theorem 3.5. Suppose that Assumptions 2.1,2.2,3.1, and 3.2hold. Also assume that for every outcome of
inner loop in Algorithm 2,∥xk−x∗
k∥ ≤R. Letϵ0>0be given.
•Whenα > 1, setν > ηα−1, and deﬁne N:=⌈logη1−αν(ρLα
F(α−1)α−1α−αϵ1−α
0/γ0)⌉+and
γ∗
k:=ρLα
F(α−1)α−1α−αϵ1−α
0ηk(α−1).
•Whenα= 1, setν > 1, and deﬁneN:=⌈logν(ρlF/γ0)⌉+andγ∗
k:=ρLF.
Then, for any k≥N, Algorithm 2generates an (ϵ0
ηk,2ϵ0
ηk(γ0νk−γ∗
k))-optimal solution of problem (P)after at
mostKiterations, where Ksatisﬁes
K=O(√
Lf1ηk
ϵ0+√
Lg1γ0(ην)k
ϵ0)
.
Theorem 3.5shows that for any given initial accuracy ϵ0>0, Algorithm 2can produce an approximate solution
of problem ( P) with the desired accuracy.
Remark 3.6. From Theorem 3.5, one can obtain an (ϵ,ϵ
γ0νk−γ∗
k)-optimal solution of problem ( P) within
O(√
Lf1/ϵ+√
Lg1/ϵα)iterations when ϵ/η≤ϵ0/ηk≤ϵ, which is similar to the complexity results in
Theorem 3.3.
73.1.3 The upper-level objective is strongly convex
We investigate the convergence outcomes when the smooth part of the upper-level objective exhibits strong
convexity.
Assumption 3.7. f1(x)isµ-strongly convex on dom(F)withµ> 0.
Assumption 3.7is another widely adopted setting in the existing SBO literature [ Beck and Sabach ,2014 ,
Sabach and Shtern ,2017 ,Amini and Youseﬁan ,2019 ,Merchav and Sabach ,2023 ]. Here, we propose a
variant of PB-APG that can provide better complexity results than existing methods. Our main integration
is an APG-based algorithm, which has been studied in the existing literature [ Nesterov ,2013 ,Lin and Xiao ,
2014 ,Xu,2022 ]. In this paper, we adopt the algorithm proposed in Lin and Xiao [2014 ] and modify it with
a constant step-size for simplicity as in Algorithm 3. Similar to Algorithm 1, we denote Algorithm 3by
ˆx=PB-APG-sc (ϕγ,ψγ,µ,L f1,Lg1,y0,ϵ).
Algorithm 3 PB-APG method for Strong Convexity Case (PB-APG-sc)
1:Input:µ,γ,Lγ=Lf1+γLg1,x−1,y0∈Rn.
2:˜y=y0−L−1
γ∇ϕγ(x−1)
3:˜x=proxL−1
γψγ(˜y−L−1
γ∇ϕγ(˜y))
4:Initialization: Letx−1=x0=˜x,k= 0
5:fork≥0do
6: yk=xk+√
Lγ−√µ√
Lγ+√µ(xk−xk−1)
7: xk+1=proxL−1
γψγ(yk−L−1
γ∇ϕγ(yk))
8:end for
The convergence analysis of Algorithm 3is in the existing literature [ Nesterov ,2013 ,Lin and Xiao ,2014 ].
Combining [ Lin and Xiao ,2014 , Theorem 1] and Theorem 2.5, we have the following complexity result.
Theorem 3.8. Suppose that Assumptions 2.1,2.2,3.1,3.2, and 3.7hold. Algorithm 3can produce an
(ϵ,l−β
Fϵβ)-optimal solution of problem (P)after at most Kiterations, where Ksatisﬁes
K=O
√
Lf1
µlog1
ϵ+√
lmax{α,β}
FLg1
ϵmax{α−1,β−1}log1
ϵ
.
Theorem 3.8improves the complexity results of Theorem 3.3signiﬁcantly. Speciﬁcally, when 0<β≤α= 1,
the convergence rate can be improved to be linear, i.e., K=O(√
Lf1/µlog1
ϵ).
Additionally, we present an adaptive variant of PB-APG-sc, termed aPB-APG-sc, which adaptively executes
xk=PB-APG-sc (ϕk,ψk,µ,L f1,Lg1,xk−1,ϵk)and enjoys the similar complexity results of Algorithm 3, as
delineated in Algorithm 4within Appendix D.1.
3.2 Both objectives are non-smooth
In this section, we focus on the scenario where both the upper- and lower-level objectives are non-smooth,
namely,f1=g1≡0. Additionally, we assume that there is a point x∈Cin the lower level prob-
lem, whereCis either Rn(the unconstrained case) or a nonempty closed and convex set satisfying C⊆
int (dom(F)∩dom(G)).
It is worth noting that in the case where both FandGare non-smooth, the convergence result may not be as
favorable as those in the previous scenarios. This is primarily due to the limited availability of information and
unfavorable properties concerning FandG. In this case, we employ a subgradient method to solve problem
(Pγ), which has been extensively studied in the existing literature [ Shor,2012 ,Bubeck et al. ,2015 ,Beck ,2017 ,
Nesterov ,2018 ]. Speciﬁcally, we update
xk+1= ProjC(xk−ηkξk), (4)
whereξk∈∂Φγ(xk)is an subgradient of Φγ(xk), and ProjC(x)is the projection of xontoC.
Letx∗
γbe an optimal solution of problem ( Pγ) and suppose that there exists a constant Rsuch that ∥x0−x∗
γ∥ ≤
R. Motivated by Theorem 8.28 in Beck [2017 ], we establish the subsequent complexity result for problem ( P).
Theorem 3.9. Suppose that Assumption 3.1(3) holds,f2andg2arelf2- andlg2-Lipschitz continuous, re-
spectively. Set step-size ηk=R
lγ√k+1in(4). Then, the subgradient method produces an (ϵ,l−β
f2ϵβ)-optimal
8solution of problem (P)after at most Kiterations, where Ksatisﬁes
K=O(
l2
f2
ϵ2+lmax{2α,2β}
f2l2
g2
ϵmax{2α,2β})
.
For non-smooth SBO problems, our method has lower complexity compared to existing approaches. Speciﬁ-
cally, under a bounded domain assumption, Helou and Simões [2017 ] simply proposed an ϵ-subgradient method
with an asymptotic rate towards the optimal solution set. The a-IRG method in Kaushik and Youseﬁan [2021 ]
achieved convergence rates of O(1/ϵ1
0.5−b)andO(1/ϵ1
b)for the upper- and lower-level objectives, respec-
tively, where b∈(0,0.5). Settingb= 0.25yields the convergence rates of O(1/ϵ4)for both upper- and
lower-level objectives, which indicates that our complexity is more efﬁcient than theirs when α< 2andβ≤α.
Furthermore, the online framework proposed in Shen et al. [2023 ] performed a complexity of O(1/ϵ3)for both
upper- and lower-level objectives. Similarly, our approach prevails over theirs when α< 1.5andβ≤α.
Strongly convex upper-level objective. Based on Theorem 8.31 in Beck [2017 ], we next explore the
improved complexity result for problem ( P) whenf2is additionally strongly convex.
Theorem 3.10. Suppose that Assumption 3.1(3) holds,C⊆int (dom(F)∩dom(G)),f2islf2-Lipschitz
continuous and µf2-strongly convex3, andg2islg2-Lipschitz continuous. Choose step-size ηk=2
µf2(k+1)
in(4). Then, the subgradient method produces an (ϵ,l−β
f2ϵβ)-optimal solution of problem (P)after at most K
iterations, where Ksatisﬁes
K=O(
l2
f2
µf2ϵ+lmax{2α,2β}
f2l2
g2
µf2ϵmax{2α−1,2β−1})
.
To our knowledge, within the context of Theorem 3.10, current ﬁndings fail to exploit strong convexity to
enhance results. However, our approach capitalizes on distinct structural characteristics that yield superior
complexity outcomes relative to Theorem 3.9in cases where α< 2andβ≤α.
4 Numerical experiments
We apply our Algorithms 1,2,3and4to two simple bilevel optimization problems from the motivating ex-
amples in Appendix A. The performances of our methods are compared with several existing methods: MNG
[Beck and Sabach ,2014 ], BiG-SAM [ Sabach and Shtern ,2017 ], DBGD [ Gong et al. ,2021 ], a-IRG [ Kaushik
and Youseﬁan ,2021 ], CG-BiO [ Jiang et al. ,2023 ], Bi-SG [ Merchav and Sabach ,2023 ] and R-APM [ Samadi
et al. ,2023 ]. For practical efﬁciency, we use the Greedy FISTA algorithm proposed in Liang et al. [2022 ] as the
APG method in our approach. Detailed settings and additional experimental results are presented in Appendix
F.
0 0.1 0.2 0.3 0.4 0.5 0.610-1010-5100
0 0.1 0.2 0.3 0.4 0.5 0.605101520
Figure 1: Performances of methods in LRP.
0 10 20 30 4010-810-610-410-2100102
0 10 20 30 40020406080100 Figure 2: Performances of methods in LSRP.
4.1 Logistic regression problem (LRP)
The LRP reads
min
x∈Rn1
2∥x∥2s.t.x∈arg min
z∈Rn1
mm∑
i=1log(1 + exp( −aT
izbi)) +IC(z), (5)
whereIC(x)is the indicator function of the set C={x∈Rn:∥x∥1≤θ}withθ= 10 . Our goal is to
ﬁnd a solution to the lower-level problem with the smallest Euclidean norm. The upper-level objective only
3In this case, we must have Cbounded, as f2is both strongly convex and Lipschitz continuous.
9consists of the smooth part, which is 1-strongly convex and 1-smooth; meanwhile, the lower-level objective is a
composite function, where the smooth part is1
4mλmax(ATA)-smooth, and the nonsmooth part is prox-friendly
[Duchi et al. ,2008 ].
In this experiment, we compare our methods with MNG, BiG-SAM, DBGD, a-IRG, CG-BiO, and Bi-SG. We
plot the values of residuals of the lower-level objective G(xk)−G∗and the upper-level objective over time in
Figure 1.
As shown in Figure 1, the PB-APG, aPB-APG, PB-APG-sc, and aPB-APG-sc algorithms exhibit signiﬁcantly
faster convergence performance than the other methods for both lower- and upper-level objectives, although
R-APM attains similar outcomes, our PB-APG and PB-APG-sc ensure a more rapid decline than it, as shown
in the ﬁrst subﬁgure of Figure 1. This is because our methods achieve lower optimal gaps and desired func-
tion values of the lower- and upper-level objectives with less execution time. This observation conﬁrms the
improved complexity results stated in the theorems above. Although the high exactness of our methods for the
lower-level problem leads to larger upper-level objectives, Table 3in Appendix F.1shows that our methods are
much closer to the optimal value. This is reasonable because the other methods exhibit lower accuracy at the
lower-level problem, resulting in larger feasible sets compared to the lower-level optimal solution set Xopt. In
addition, Figure 1demonstrates that aPB-APG and aPB-APG-sc outperform PB-APG and PB-APG-sc in terms
of convergence rate. This improvement can be attributed to the adaptiveness incorporated in Algorithms 2and
4.
4.2 Least squares regression problem (LSRP)
The LSRP has the following form:
min
x∈Rnτ
2∥x∥2+∥x∥1s.t.x∈arg min
z∈Rn1
2m∥Az−b∥2, (6)
whereτ= 0.02regulates the trade-off between ℓ1andℓ2norms. We aim to ﬁnd a sparse solution for the lower-
level problem. The upper-level objective is formulated as a composite function, which consists of a τ-strongly
convex andτ-smooth component, along with a proximal-friendly non-smooth component [ Beck ,2017 ]. The
lower-level objective is a smooth function with a smoothness parameter of1
mλmax(ATA).
In this experiment, we compare the performances of our methods with a-IRG, BiG-SAM, and Bi-SG. We plot
the values of residuals of lower-level objective G(xk)−G∗and the upper-level objective over time in Figure
2.
Figure 2shows that the proposed PB-APG, aPB-APG, PB-APG-sc, and aPB-APG-sc converge faster than the
compared methods for both the lower- and upper-level objectives, as well. For the upper-level objective, our
methods achieve larger function values than other methods, except BiG-SAM ( δ= 0.01). This is because our
methods attain higher accuracy for the lower-level objective than other methods. We have similar observations
in Section 4.1. Furthermore, Figure 1also demonstrates that the adaptive mechanism produces staircase-shaped
curves for aPB-APG and aPB-APG-sc, which might prevent undesirable ﬂuctuations in PB-APG and PB-APG-
sc.
5 Conclusion
This paper proposes a penalization framework that effectively addresses the challenges inherent in simple
bilevel optimization problems. By delineating the relationship between approximate solutions of the orig-
inal problem and its penalized reformulation, we enable the application of speciﬁc methods under varying
assumptions for the original problem. Under the Hölderian error bound condition, our methods achieve su-
perior complexity results compared to the existing methods. The performance is further improved when the
smooth component of the upper-level objective is strongly convex. Additionally, we extend our framework to
scenarios involving general nonsmooth objectives. Numerical experiments also validate the effectiveness of
our algorithms.
Acknowledgements
This work is partly supported by the National Key R&D Program of China under grant 2023YFA1009300,
National Natural Science Foundation of China under grants 12171100 and the Major Program of NFSC
(72394360,72394364).
References
Samir Adly, Loïc Bourdin, and Fabien Caubet. On a decomposition formula for the proximal operator of the
sum of two convex functions. Journal of Convex Analysis , 26(2):699–718, 2019.
10Mostafa Amini and Farzad Youseﬁan. An iterative regularized incremental projected subgradient method for
a class of bilevel optimization problems. In 2019 American Control Conference (ACC) , pages 4069–4074.
IEEE, 2019.
Amir Beck. First-order methods in optimization . SIAM, 2017.
Amir Beck and Shoham Sabach. A ﬁrst order method for ﬁnding minimal norm-like solutions of convex
optimization problems. Mathematical Programming , 147(1-2):25–46, 2014.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences , 2(1):183–202, 2009.
Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differentiable
closed-form solvers. arXiv preprint arXiv:1805.08136 , 2018.
Dimitri Bertsekas, Angelia Nedic, and Asuman Ozdaglar. Convex analysis and optimization , volume 1. Athena
Scientiﬁc, 2003.
Nicholas Bishop, Long Tran-Thanh, and Enrico Gerding. Optimal learning from veriﬁed training data. Ad-
vances in Neural Information Processing Systems , 33:9520–9529, 2020.
Jérôme Bolte, Trong Phong Nguyen, Juan Peypouquet, and Bruce W Suter. From error bounds to the complexity
of ﬁrst-order descent methods for convex functions. Mathematical Programming , 165:471–507, 2017.
Digvijay Boob, Qi Deng, and Guanghui Lan. Stochastic ﬁrst-order methods for convex and nonconvex func-
tional constrained optimization. Mathematical Programming , 197(1):215–279, 2023.
Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends® in
Machine Learning , 8(3-4):231–357, 2015.
James V . Burke and Sien Deng. Weak sharp minima revisited, part ii: application to linear regularity and error
bounds. Mathematical Programming , 104(2-3):235–261, 2005.
James V Burke and Michael C Ferris. Weak sharp minima in mathematical programming. SIAM Journal on
Control and Optimization , 31(5):1340–1359, 1993.
Alexandre Cabot. Proximal point algorithm controlled by a slowly vanishing term: applications to hierarchical
minimization. SIAM Journal on Optimization , 15(2):555–572, 2005.
H. Chen, H. Xu, R. Jiang, et al. Lower-level duality based reformulation and majorization minimization algo-
rithm for hyperparameter optimization. In Proceedings of the International Conference on Artiﬁcial Intelli-
gence and Statistics , pages 784–792. PMLR, 2024.
L. Chen, J. Xu, and J. Zhang. Bilevel optimization without lower-level strong convexity from the hyper-
objective perspective. arXiv preprint arXiv:2301.00712 , 2023.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions.
SIAM Journal on Optimization , 29(1):207–239, 2019.
Stephan Dempe, Nguyen Dinh, Joydeep Dutta, and Tanushree Pandit. Simple bilevel programming and exten-
sions. Mathematical Programming , 188:227–253, 2021.
Lior Doron and Shimrit Shtern. Methodology and ﬁrst-order algorithms for solving nonsmooth and non-
strongly convex bilevel optimization problems. Mathematical Programming , 201:521–558, 2023.
Dmitriy Drusvyatskiy and Adrian S. Lewis. Error bounds, quadratic growth, and linear convergence of proximal
methods. Mathematics of operations research , 43(3):919–948, 2018a.
Dmitriy Drusvyatskiy and Adrian S Lewis. Error bounds, quadratic growth, and linear convergence of proximal
methods. Mathematics of Operations Research , 43(3):919–948, 2018b.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto the l 1-ball
for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning ,
pages 272–279, 2008.
Joydeep Dutta and Tanushree Pandit. Algorithms for simple bilevel programming. Bilevel Optimization: Ad-
vances and Next Challenges , pages 253–291, 2020.
11Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel program-
ming for hyperparameter optimization and meta-learning. In International conference on machine learning ,
pages 1568–1577. PMLR, 2018.
Michael P Friedlander and Paul Tseng. Exact regularization of convex programs. SIAM Journal on Optimiza-
tion, 18(4):1326–1350, 2008.
Khanh-Hung Giang-Tran, Nam Ho-Nguyen, and Dabeen Lee. Projection-free methods for solving convex
bilevel optimization problems. arXiv preprint arXiv:2311.09738 , 2023.
Chengyue Gong, Xingchao Liu, and Qiang Liu. Bi-objective trade-off with dynamic barrier gradient descent.
InInternational Conference on Neural Information Processing Systems , pages 29630–29642, 2021.
Elias S Helou and Lucas EA Simões. ϵ-subgradient algorithms for bilevel convex optimization. Inverse Prob-
lems, 33(5):055020, 2017.
F. Huang. On momentum-based gradient methods for bilevel optimization with nonconvex lower-level. arXiv
preprint arXiv:2303.03944 , 2023.
Ruichen Jiang, Nazanin Abolfazli, Aryan Mokhtari, and Erfan Yazdandoost Hamedani. A conditional gradient-
based method for simple bilevel optimization with convex lower-level problem. In International Conference
on Artiﬁcial Intelligence and Statistics , pages 10305–10323. PMLR, 2023.
Rujun Jiang and Xudong Li. Hölderian error bounds and kurdyka-łojasiewicz inequality for the trust region
subproblem. Mathematics of Operations Research , 47(4):3025–3050, 2022.
Harshal D. Kaushik and Farzad Youseﬁan. A method with convergence rates for optimization problems with
variational inequality constraints. SIAM Journal on Optimization , 31(3):2171–2198, 2021.
Matthias Kissel, Martin Gottwald, and Klaus Diepold. Neural network training with safe regularization in
the null space of batch activations. In Artiﬁcial Neural Networks and Machine Learning–ICANN 2020:
29th International Conference on Artiﬁcial Neural Networks, Bratislava, Slovakia, September 15–18, 2020,
Proceedings, Part II 29 , pages 217–228. Springer, 2020.
Puya Latafat, Andreas Themelis, Silvia Villa, and Panagiotis Patrinos. Adabim: An adaptive proximal gradient
method for structured convex bilevel optimization. arXiv preprint arXiv:2305.03559 , 2023.
Jingwei Liang, Tao Luo, and Carola-Bibiane Schonlieb. Improving fast iterative shrinkage-thresholding algo-
rithm: Faster, smarter, and greedier. SIAM Journal on Scientiﬁc Computing , 44(3):A1069–A1091, 2022.
Qihang Lin and Lin Xiao. An adaptive accelerated proximal gradient method and its homotopy continuation
for sparse optimization. In International Conference on Machine Learning , pages 73–81. PMLR, 2014.
Zhi-Quan Luo, Jong-Shi Pang, Daniel Ralph, and Shi-Quan Wu. Exact penalization and stationarity conditions
of mathematical programs with equilibrium constraints. Mathematical Programming , 75(1):19–76, 1996.
Yura Malitsky. Chambolle-Pock and Tsengs methods: relationship and extension to the bilevel optimization.
arXiv preprint arXiv:1706.02602 , 2017.
Roey Merchav and Shoham Sabach. Convex bi-level optimization problems with nonsmooth outer objective
function. SIAM Journal on Optimization , 33(4):3114–3142, 2023.
Hong Mingyi, Wai Hoi-To, Wang Zhaoran, and Zhuoran Yang. A two-timescale framework for bilevel opti-
mization: Complexity analysis and application to actor-critic. arXiv preprint arXiv:2007.05170 , 2020.
Arkadij Semenovi ˇc Nemirovsky and David Borisovich Yudin. Problem complexity and method efﬁciency in
optimization . Wiley, 1983.
Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical programming , 140(1):
125–161, 2013.
Yurii Nesterov. Lectures on convex optimization , volume 137. Springer, 2018.
Jong Shi Pang. Error bounds in mathematical programming. Mathematical Programming , 79(1-3):299–332,
1997.
Nelly Pustelnik and Laurent Condat. Proximity operator of a sum of functions; application to depth map
estimation. IEEE Signal Processing Letters , 24(12):1827–1831, 2017.
12Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients.
Advances in neural information processing systems , 32, 2019.
Vincent Roulet and Alexandre d’Aspremont. Sharpness, restart and acceleration. SIAM Journal on Optimiza-
tion, 30(1):262–289, 2020.
Shoham Sabach and Shimrit Shtern. A ﬁrst order method for solving convex bilevel optimization problems.
SIAM Journal on Optimization , 27(2):640–660, 2017.
Sepideh Samadi, Daniel Burbano, and Farzad Youseﬁan. Achieving optimal complexity guarantees for a class
of bilevel convex optimization problems. arXiv preprint arXiv:2310.12247 , 2023.
Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation for bilevel
optimization. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics , pages 1723–
1732. PMLR, 2019.
Han Shen and Tianyi Chen. On penalty-based bilevel gradient descent method. In Proceedings of the 40th
International Conference on Machine Learning , volume 202 of Proceedings of Machine Learning Research ,
pages 30992–31015. PMLR, 23–29 Jul 2023.
Lingqing Shen, Nam Ho-Nguyen, and Fatma Kılınç-Karzan. An online convex optimization-based framework
for convex bilevel optimization. Mathematical Programming , 198(2):1519–1582, 2023.
Naum Zuselevich Shor. Minimization methods for non-differentiable functions , volume 3. Springer Science &
Business Media, 2012.
Mikhail Solodov. An explicit descent method for bilevel convex optimization. Journal of Convex Analysis , 14
(2):227–237, 2007.
D. Sow, K. Ji, Z. Guan, and et al. A primal-dual approach to bilevel optimization with multiple inner minima.
arXiv preprint arXiv:2203.01123 , 2022.
Marcin Studniarski and Doug E Ward. Weak sharp minima: characterizations and sufﬁcient conditions. SIAM
Journal on Control and Optimization , 38(1):219–236, 1999.
Andre Nikolaevich Tikhonov and V . I. A. K. Arsenin. Solutions of ill-posed problems . Wiley, 1977.
Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. unpublished
manuscript , 2008.
Jiali Wang, He Chen, Rujun Jiang, Xudong Li, and Zihao Li. Fast algorithms for stackelberg prediction game
with least squares loss. In International Conference on Machine Learning , pages 10708–10716. PMLR,
2021.
Jiali Wang, Wen Huang, Rujun Jiang, Xudong Li, and Alex L Wang. Solving stackelberg prediction game with
least squares loss via spherically constrained least squares reformulation. In International Conference on
Machine Learning , pages 22665–22679. PMLR, 2022.
Blake E Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives. Advances
in neural information processing systems , 29, 2016.
Yangyang Xu. First-order methods for problems with o (1) functional constraints can have almost the same
convergence rate as for unconstrained problems. SIAM Journal on Optimization , 32(3):1759–1790, 2022.
Yao-Liang Yu. On decomposing the proximal map. Advances in neural information processing systems , 26,
2013.
Jinshan Zeng, Tim Tsz-Kit Lau, Shaobo Lin, and Yuan Yao. Global convergence of block coordinate descent
in deep learning. In International conference on machine learning , pages 7313–7323. PMLR, 2019.
Zirui Zhou and Anthony Man-Cho So. A uniﬁed approach to error bounds for structured convex optimization
problems. Mathematical Programming , 165:689–728, 2017.
13A Motivating examples
Many machine learning applications involve a primary objective G, which usually represents the training loss,
and a secondary objective F, which can be a regularization term or an auxiliary loss. A common approach for
such problems is to optimize Gfully and then use Fto select the optimal solutions from the ones obtained for G.
This is called lexicographic optimization [ Kissel et al. ,2020 ,Gong et al. ,2021 ]. Two classes of lexicographic
optimization problems are the regularized problem, also known as the ill-posed optimization problem [ Amini
and Youseﬁan ,2019 ,Jiang et al. ,2023 ], and the over-parameterized regression [ Jiang et al. ,2023 ], where the
upper-level objectives are the regularization terms or loss functions, and the lower-level objectives are the loss
functions and the constraint terms. We present some examples of these classes of problems as follows.
Example A.1 (Linear Inverse Problems) .Linear inverse problems aim to reconstruct a vector x∈Rnfrom
measurements b∈Rmthat satisfyb=Ax+ρε, whereA:Rn→Rmis a linear mapping, ε∈Rmis
unknown noise, and ρ> 0is its magnitude. Various optimization techniques can address these problems. We
focus on the bilevel formulation, widely adopted in the literature [ Beck and Sabach ,2014 ,Sabach and Shtern ,
2017 ,Dempe et al. ,2021 ,Latafat et al. ,2023 ,Merchav and Sabach ,2023 ].
The lower-level objective in the bilevel formulation is given by
G(x) =1
2m∥Ax−b∥2+IC(x), (7)
whereIC(x)is the indicator function of a set Cthat satiﬁesIC(x) = 0 ifx∈C, andIC(x) = + ∞if
x/∈C. The setCis a closed, convex set that can be chosen as C=Rn,C={x∈Rn:x≥0}, or
C={x∈Rn:∥x∥1≤θ}for someθ>0.
This problem may have multiple minimizer solutions. Hence, a reasonable option is to consider the minimal
norm solution problem, i.e., ﬁnd the optimal solution with the smallest Euclidean norm [ Beck and Sabach ,
2014 ,Sabach and Shtern ,2017 ,Latafat et al. ,2023 ]:
F(x) =1
2∥x∥2.
We need to solve the simple bilevel optimization problem:
min
x∈Rn1
2∥x∥2s.t.x∈arg min
z∈Rn1
2m∥Az−b∥2+IC(z).
Example A.2 (Sparse Solution of Linear Inverse Problems) .Consider the same setting as in Example A.1, but
with the additional goal of ﬁnding a sparse solution among all the minimizers of the linear inverse problem
(7). This can simplify the model and improve computational efﬁciency. To achieve sparsity, we can use any
function that encourages it. One such function is the well-known elastic net regularization [ Friedlander and
Tseng ,2008 ,Amini and Youseﬁan ,2019 ,Merchav and Sabach ,2023 ], which is deﬁned as
F(x) =∥x∥1+τ
2∥x∥2,
whereτ > 0regulates the trade-off between ℓ1andℓ2norms.
This example corresponds to our second experiment in Section 4.2.
Example A.3 (Logistic Regression Problem) .The logistic regression problem aims to map the feature vectors
aito the target labels bi. A standard machine learning technique for this problem is to minimize the logistic
loss function over the given dataset [ Amini and Youseﬁan ,2019 ,Gong et al. ,2021 ,Jiang et al. ,2023 ,Latafat
et al. ,2023 ,Merchav and Sabach ,2023 ]. We assume that the dataset consists of a feature matrix A∈Rm×n
and a label vector b∈Rm, withbi∈ {− 1,1}for eachi. The logistic loss function is deﬁned as
g1(x) =1
mm∑
i=1log(1 + exp( −aT
ixbi)). (8)
Over-ﬁtting is a common issue when the number of features is large compared to the number of instances
m. A possible approach is to regularize the logistic objective function with a speciﬁc function or a constraint
[Jiang et al. ,2023 ,Merchav and Sabach ,2023 ]. For instance, we can use g2(x) =IC(x), whereIC(x)is the
indicator of the set C={x∈Rn:∥x∥1≤θ}, as in Example A.1.
This problem may also have multiple optimal solutions. Hence, a natural extension is to consider the minimal
norm solution problem [ Gong et al. ,2021 ,Jiang et al. ,2023 ,Latafat et al. ,2023 ], as in Example A.1. This
requires solving the following problem:
min
x∈Rn1
2∥x∥2s.t.x∈arg min
z∈Rn1
mm∑
i=1log(1 + exp( −aT
izbi)) +IC(z).
14When choosing C={x∈Rn:∥x∥1≤θ}for someθ > 0, it corresponds to our ﬁrst experiment in Section
4.1.
Example A.4 (Over-parameterized Regression Problem) .The linear regression problem aims to ﬁnd a param-
eter vector x∈Rnthat minimizes the training loss ℓtr(x)over the training dataset Dtr. Without explicit
regularization, the over-parameterized regression problem has multiple minima. However, these minima may
have different generalization performance. Therefore, we introduce a secondary objective, such as the valida-
tion loss over a validation set Dval, to select one of the global minima of the training loss. This results in the
following bilevel problem:
min
x∈RnF(x) :=ℓval(x) s.t.x∈arg min
z∈RnG(z) :=ℓtr(z). (9)
For instance, we can consider the sparse linear regression problem, where the lower-level objective consists
of the training error and a regularization term, namely, G(x) =1
2∥Atrx−btr∥2+IC(x). Here,IC(x)
denotes the indicator of a convex set, as in Example A.2. The upper-level objective is the validation error, i.e.,
F(x) =1
2∥Avalx−bval∥2. The linear regression problem is over-parameterized when the number of features
nis larger than the number of data instances in the training set.
B Comparison between simple bilevel optimization methods
Table 1: Summary of simple bilevel optimization algorithms. The abbreviations “SC," “C,", “diff",
“comp", “WS" and “C3" represent “strongly convex," “convex,", “differentiable", “composite",
“weak sharpness" and “Convex objective with Convex Compact constraints," respectively. The ab-
breviationα-HEB refers to Hölderian error bound with exponent parameter α. We only include the
gradients Lipschitz constant in the complexity result when its relation to the complexity is clear;
otherwise, we omit it. Notation lFis the upper bound of subdifferentials of F,Lf1andLg1are the
Lipschitz constants of ∇f1and∇g1, respectively.
MethodsUpper-level Lower-level (ϵF,ϵG)-optimal Convergence
ObjectiveF ObjectiveG Solution Upper-level Lower-level
MNG [ Beck and Sabach ,2014 ] SC, diff C, smooth (/,ϵG) Asymptotic O(
L2
g1/ϵ2
G)
BiG-SAM [ Sabach and Shtern ,2017 ] SC, smooth C, comp (/,ϵG) Asymptotic O(Lg1/ϵG)
IR-IG [ Amini and Youseﬁan ,2019 ] SC C3, Finite sum (/,ϵG) Asymptotic O(
1/ϵ1
0.5−ε
G)
,ε∈(0,0.5)
IR-CG [ Giang-Tran et al. ,2023 ] C, smooth C3, smooth (ϵF,ϵG) O(
max{1/ϵ1
1−p
F,1/ϵ1
p
G})
p∈(0,1)
Tseng’s method [ Malitsky ,2017 ] C, comp C, comp (/,ϵG) Asymptotic O(1/ϵG)
ITALEX [ Doron and Shtern ,2023 ] C, comp C, comp (ϵ,ϵ2) O(
1/ϵ2)
a-IRG [ Kaushik and Youseﬁan ,2021 ] C, Lip C, Lip (ϵF,ϵG) O(
max{1/ϵ1
0.5−b
F,1/ϵ1
b
G})
,b∈(0,0.5)
CG-BiO [ Jiang et al. ,2023 ] C, smooth C3, smooth (ϵF,ϵG) O(max{Lf1/ϵF,Lg1/ϵG})
Bi-SG [ Merchav and Sabach ,2023 ]C, quasi-Lip/comp C, comp (ϵF,ϵG) O(
max{1/ϵ1
1−a
F,1/ϵ1
a
G})
,a∈(0.5,1)
µ-SC, comp C, comp (ϵF,ϵG) O(
max{(
log 1/ϵF
µ)1
1−a,1/ϵ1
a
G})
,a∈(0.5,1)
R-APM [ Samadi et al. ,2023 ] C, smooth C, comp, WS (ϵ,ϵ) O(√
1/ϵ)
Online Framework [ Shen et al. ,2023 ] C, Lip C3, Lip (ϵF,ϵG) O(
max{1/ϵ3
F,1/ϵ3
G})
Our methodC, comp C, comp,α-HEB (ϵ,l−β
Fϵβ) O(√
Lf1
ϵ+√
lmax{α,β}
FLg1
ϵmax{α,β})
,α≥1,β > 0
µ-SC, comp C, comp,α-HEB (ϵ,l−β
Fϵβ) O(√
Lf1
µlog1
ϵ+√
lmax{α,β}
FLg1
ϵmax{α−1,β−1}log1
ϵ)
,α≥1,β > 0
nonsmooth, Lip nonsmooth, Lip, α-HEB (ϵ,l−β
Fϵβ) O(
l2
f2
ϵ2+lmax{2α,2β}
f2l2
g2
ϵmax{2α,2β})
,α≥1,β > 0
C Examples of functions satisfying the Hölderian error bound
We present several examples of functions that satisfy the Hölderian error bound Assumption 2.2and their
corresponding exponent parameter αin Table 2. We also provide some clariﬁcations for Table 2below. The
abbreviations “ Q∈Sn” and “Q≻0” stand for “ Qis a symmetric matrix of order nand a positive deﬁnite
matrix, respectively. We refer the reader to Pang [1997 ],Bolte et al. [2017 ],Zhou and So [2017 ],Jiang and
Li[2022 ],Doron and Shtern [2023 ] and the references therein for more examples of functions that satisfy
Hölderian error bound Assumption 2.2. Furthermore, it is noteworthy that numerous applications in neural
networks, such as deep neural networks (DNNs), also comply with this assumption, as discussed in Bolte et al.
[2017 ],Zeng et al. [2019 ].
4According to Table 2 of Doron and Shtern [2023 ], the parameter αcan take values of either 1or2. Partic-
ularly, when α= 1, we haveρ= 1; whenα= 2, we haveρ= 2/τ.
15Table 2: Summary of some functions satisfying Hölderian error bound with corresponding expo-
nents.
G(x) Remarks Name α
maxi∈[m]{⟨ai,x⟩ −bi} ai∈Rn,i∈[m],b∈Rmpiece-wise maximum 1
∥x−x0∥Q=√
(x−x0)TQ(x−x0)Q∈Sn,Q≻0,x0∈RnQ-norm 1
∥x−x0∥px0∈Rn,p≥1 ℓp-norm 1
∥x∥1+τ
2∥x∥2τ > 0 Elastic net 1 or 24
∥Ax−b∥2A∈Rm×n,b∈RmLeast squares 2
1
m∑m
i=1log(1 + exp( −aT
ixbi)) ai∈Rn,i∈[m],b∈Rm,A∈Rm×nLogistic loss 2
η(x) +σ
2∥x∥2ηconvex,σ> 0 Strongly-convex 2
D Supplementary results
D.1 Adaptive version of PB-APG method with strong convexity assumption
Algorithm 4 Adaptive PB-APG-sc method (aPB-APG-sc)
1:Input: x−1=x0∈Rn,γ0=γ1>0,Lf1,Lg1,ν > 1,η> 1,ϵ0>0.
2:fork≥0do
3:ϕk(x) =f1(x) +γkg1(x)
4:ψk(x) =f2(x) +γkg2(x)
5: Invoke xk=PB-APG-sc (ϕk,ψk,µ,Lf1,Lg1,xk−1,ϵk)
6:ϵk+1=1
ηϵk
7:γk+1=νγk
8:end for
Similar to Algorithm 2, we have the following convergence results of Algorithm 4.
Theorem D.1. Suppose that Assumptions 2.1,2.2,3.1,3.2, and 3.7hold. Letϵ0>0be given.
•Whenα> 1, setν >ηα−1,N=⌈logη1−αν(ρLα
F(α−1)α−1α−αϵ1−α
0/γ0)⌉+andγ∗
k=ρLα
F(α−
1)α−1α−αϵ1−α
0ηk(α−1);
•Whenα= 1, setν > 1,N=⌈logν(ρlF/γ0)⌉+andγ∗
k=ρLF.
Then, for any k≥N, Algorithm 2generates an (ϵ0
ηk,2ϵ0
ηk(γ0νk−γ∗
k))-optimal solution of problem (P)after at
mostKiterations, where Ksatisﬁes
K=O
√
Lf1
µlogηk
ϵ0+√
νklmax{α,β}
FLg1
ϵmax{α−1,β−1}logηk
ϵ0
.
The proof is similar to the proof of Theorem 3.5in Appendix E.8. So we omit it here.
E Proofs of main results
In this section, we propose the proofs of our main convergence results in this paper.
E.1 Proof of Lemma 2.3
Proof. SinceXoptis closed and convex [ Beck and Sabach ,2014 ], the projection of any x∈RnontoXopt,
denoted as ¯x, exists and is unique. Furthermore, it holds that dist (x,X opt) =∥x−¯x∥.
Then, by Assumption 2.1, we have
F(x)−F(¯x)≥ −ξ⊤(x−¯x)≥ −∥ξ∥∥x−¯x∥ ≥ −lF∥x−¯x∥,∀ξ∈∂F(¯x). (10)
16Choosingγ∗=ρlα
F(α−1)α−1α−αϵ1−α, it follows that
F(x)−F(¯x) +γ∗p(x)(10)
≥ −lF∥x−¯x∥+γ∗p(x)
(a)
≥ −lF∥x−¯x∥+γ∗
ρ∥x−¯x∥α
≥min
z≥0−lFz+γ∗
ρzα
(b)=−ϵ,(11)
where (a)follows from the Hölderian error bound assumption of p(x), and (b)is from the fact that y=
−lFz+γ∗
ρzαattains its minimum at z∗=(
ρlF
αγ∗)1
α−1.
Since ¯x∈Xoptis feasible for problem ( P), we haveF(¯x)≥F∗. This along with ( 11) indicates
F(x) +γp(x)−F∗≥F(x) +γ∗p(x)−F(¯x)≥ −ϵ,∀x∈Rdandγ≥γ∗. (12)
Letx∗be an optimal solution of ( P) so thatF(x∗) =F∗. In addition, since x∗∈Xopt, we havep(x∗) = 0 .
Combine these results with ( 12), we have
F(x∗) +γp(x∗) =F∗(12)
≤F(x) +γp(x) +ϵ,∀x∈Rdandγ≥γ∗. (13)
This demonstrates that an optimal solution of ( P) is anϵ-optimal solution for ( Pγ).
E.2 Proof of Lemma 2.4
Proof. The proof is motivated by Theorem 1 in Luo et al. [1996 ]. Denote x∗,x∗
γas optimal solutions of
problem ( P) and ( Pγ), respectively.
For any x∈Rn, let¯xbe the projection of xontoXopt. Then ¯xis a feasible solution of ( P) andF(¯x)≥F(x∗)
holds. Then we have
F(x) +γp(x) =F(¯x) +F(x)−F(¯x) +γp(x)
≥F(x∗) +F(x)−F(¯x) +γp(x)
(a)
≥F(x∗)−lF∥x−¯x∥+γ
ρ∥x−¯x∥
=F(x∗) + (γ
ρ−lF)∥x−¯x∥
(b)
≥F(x∗) =F(x∗) +γp(x∗),(14)
where (a)follows from ( 10) and the Hölderian error bound assumption of p(x), and (b)follows from γ≥ρlF.
Therefore, we conclude that x∗is an optimal solution of ( Pγ).
For the converse part, let ¯x∗
γbe the projection of x∗
γontoXopt. Then ¯x∗
γis a feasible solution of ( P). Therefore,
it holds thatF(¯x∗
γ)≥F(x∗). Similarly, we have
F(x∗) =F(x∗) +γp(x∗)
≥F(x∗
γ) +γp(x∗
γ)
=F(x∗
γ)−F(x∗) +F(x∗) +γp(x∗
γ)
≥F(x∗) +F(x∗
γ)−F(¯x∗
γ) +γp(x∗
γ)
(c)
≥F(x∗)−lF∥x∗
γ−¯x∗
γ∥+γ
ρ∥x∗
γ−¯x∗
γ∥
≥F(x∗) + (γ
ρ−lF)∥x∗
γ−¯x∗
γ∥
≥F(x∗),(15)
where the inequality (c)follows from ( 10) and the Hölderian error bound assumption of p(x).
Therefore, all inequalities in ( 15) become equalities. We deduce that ∥x∗
γ−¯x∗
γ∥= 0ifγ >ρl F, implying that
x∗
γis inXopt, i.e.,p(x∗
γ) = 0 . Furthermore, as the ﬁrst inequality of ( 15) becomes an equality, we obtain
F(x∗) =F(x∗
γ) +γp(x∗
γ) =F(x∗
γ).
Therefore, x∗
γis also an optimal solution of ( P).
17E.3 Proof of Theorem 2.5
Proof. Denote x∗,x∗
γas optimal solutions of problem ( P) and ( Pγ), respectively.
•Case ofα> 1.Since ˜x∗
γis anϵ-optimal solution of ( Pγ), we have
F(˜x∗
γ) +γp(˜x∗
γ)≤F(x) +γp(x) +ϵ,∀x∈Rn. (16)
Note that the arguments in the proof of Lemma 2.3still hold. Substituting x=x∗into ( 16) and
utilizingp(x∗) = 0 , we have
F(˜x∗
γ) +γp(˜x∗
γ)≤F(x∗) +ϵ=F(x∗) +γ∗p(x∗) +ϵ≤F(˜x∗
γ) +γ∗p(˜x∗
γ) + 2ϵ,
where the last inequality follows from setting x=˜x∗
γin (13). Then, it holds that
p(˜x∗
γ)≤2ϵ
γ−γ∗=2ϵ
2lβ
Fϵ1−β=l−β
Fϵβ. (17)
By setting x=x∗in (16), we have
F(˜x∗
γ)−F(x∗)≤γ(p(x∗)−p(˜x∗
γ)) +ϵ.
Using the fact that p(x∗) = 0≤p(˜x∗
γ), we have
F(˜x∗
γ)−F(x∗)≤ϵ. (18)
Combing ( 18) with ( 17), we conclude that ˜x∗
γis an (ϵ,l−β
Fϵβ)-optimal solution of ( P).
•Case ofα= 1.Since ˜x∗
γis anϵ-optimal solution of ( Pγ), we have
F(˜x∗
γ) +γp(˜x∗
γ)≤F(x∗
γ) +γp(x∗
γ) +ϵ. (19)
On the one hand, as γ=γ∗+lβ
Fϵ1−β>γ∗, by Lemma 2.4,x∗
γis an optimal solution of ( P). On the
other hand, since γ≥γ∗, according to Lemma 2.4,x∗is also an optimal solution of ( Pγ). Therefore,
p(x∗) = 0 andp(x∗
γ) = 0 , it holds that
F(x∗)≤F(˜x∗
γ) +γp(˜x∗
γ)
(19)
≤F(x∗
γ) +γp(x∗
γ) +ϵ
=F(x∗
γ) +γ∗p(x∗
γ) +ϵ
=F(x∗) +γ∗p(x∗) +ϵ
≤F(˜x∗
γ) +γ∗p(˜x∗
γ) +ϵ,(20)
where the ﬁrst inequality follows from the fact that x∗is an optimal solution of ( Pγ), and the last
inequality follows from the optimality of x∗to (Pγ) whenγ≥γ∗.
The second inequality of ( 20) andp(˜x∗
γ)≥0imply that
F(˜x∗
γ)≤F(x∗
γ) +γp(x∗
γ) +ϵ=F(x∗) +γp(x∗) +ϵ≤F(x∗) +ϵ.
That is, it holds that
F(˜x∗
γ)≤F(x∗) +ϵ. (21)
In addition, from ( 20), we haveF(˜x∗
γ) +γp(˜x∗
γ)≤F(˜x∗
γ) +γ∗p(˜x∗
γ) +ϵ, which implies that
p(˜x∗
γ)≤ϵ
γ−γ∗=ϵ
lβ
Fϵ1−β=l−β
Fϵβ. (22)
This result along with ( 21) demonstrate that ˜x∗
γis an (ϵ,l−β
Fϵβ)-optimal solution of ( P).
E.4 Proof of Theorem 2.6
Proof. Letˆx∗
γbe the projection of ˜x∗
γonXopt, we have ∥˜x∗
γ−ˆx∗
γ∥= dist( ˜x∗
γ,X opt).
By Assumption 2.2, the following inequality holds,
∥˜x∗
γ−ˆx∗
γ∥α≤ρp(˜x∗
γ)(a)
≤ρl−β
Fϵβ=⇒ ∥ ˜x∗
γ−ˆx∗
γ∥ ≤(
ρl−β
Fϵβ)1
α, (23)
where (a)follows from ( 17) whenα> 1or from ( 22) whenα= 1.
By Assumption 2.1, we have
F(˜x∗
γ)−F∗≥F(˜x∗
γ)−F(ˆx∗
γ)(10)
≥ −lF∥˜x∗
γ−ˆx∗
γ∥ ≥ −lF(
ρl−β
Fϵβ)1
α,
where the ﬁrst inequality follows from F(ˆx∗
γ)≥F∗andˆx∗
γ∈Xopt.
18E.5 Proof of Theorem 2.7
Proof. For any x∈dom(F), let¯xbe the projection of xontoXopt, where the existence and uniqueness of ¯x
follows from that Xoptis closed and convex. Since Fisl-Lipschitz continuous, similar to ( 10), we have
F(x)−F(¯x)≥ −l∥x−¯x∥,∀ξ∈∂F(¯x). (24)
Therefore, all the requirements of ( 10) in equations ( 11), (14) and ( 15) can be replaced by ( 24). This implies
that Lemmas 2.3and2.4also hold for the global solutions of problems ( P) and ( Pγ) whenFis non-convex.
Then, the ﬁnal result follows a similar pattern to Theorem 2.5. Here we omit it.
E.6 Proof of Theorem 2.8
Proof. Let¯x∗
γbe the projection of x∗
γontoXoptandˆx∗
γ=cx∗
γ+ (1−c)¯x∗
γwithc= min {1,1−r
∥x∗γ−¯x∗γ∥},
which implies that ˆx∗
γ∈ B(x∗
γ,r). Then, we have
F(x∗
γ) +γp(x∗
γ)≤F(ˆx∗
γ) +γp(ˆx∗
γ)(i)
≤F(ˆx∗
γ) +γ(cp(x∗
γ) + (1 −c)p(¯x∗
γ)) =F(ˆx∗
γ) +γcp(x∗
γ),(25)
where inequality (i)follows from the convexity of p(x).
Inequality ( 25) demonstrates that
γ(1−c)p(x∗
γ)≤F(ˆx∗
γ)−F(x∗
γ)≤l∥ˆx∗
γ−x∗
γ∥=l(1−c)∥x∗
γ−¯x∗
γ∥ ≤l(1−c)(ρp(x∗
γ))1
α,
where the second inequality follows from the l-Lipschitz continuity of FonB(x∗
γ,r). Therefore, it holds that
γp(x∗
γ)≤l(ρp(x∗
γ))1
α. (26)
•Case ofα > 1.By (26), we havep(x∗
γ)≤(ρlα
γα)1
α−1, which demonstrates that p(x∗
γ)≤ϵif
γ≥(ρlα
ϵα−1)1
α.
Then, for any xγ∈ B(x∗
γ,r)that also satisﬁes p(xγ)≤p(x∗
γ)≤ϵ, we have
F(x∗
γ) +γp(x∗
γ)≤F(xγ) +γp(xγ), (27)
which implies that F(x∗
γ)−F(xγ)≤γ(p(xγ)−p(x∗
γ))≤0. The desired result follows.
•Case ofα= 1.By (26), we havep(x∗
γ) = 0 ifγ >ρl . Therefore, for any xγ∈ B(x∗
γ,r)∩Xopt,
by the deﬁnition of x∗
γ, it holds that
F(x∗
γ) +γp(x∗
γ)≤F(xγ) +γp(xγ),
which demonstrates that F(x∗
γ)≤F(xγ). The desired result follows.
E.7 Proof of Theorem 3.3
Proof. From [ Beck ,2017 , Theorem 10.34], the objective value after Kiterations can be bounded by
Φγ(xK)−Φ∗
γ≤2Lγ∥x0−x∗∥2
(K+ 1)2,
whereLγ=Lf1+γLg1.
Combining this with our stopping criterion, we ﬁnd that after Kiterations,
Φγ(xK)−Φ∗
γ≤ϵ.
This indicates that we obtain an ϵ-optimal solution to problem ( Pγ). The value of Ksatisﬁes:
K=√
2(Lf1+γLg1)
ϵR−1.
Speciﬁcally, we analyze the value of Kin various scenarios in the form of O(·).
•Case ofα > 1.In this case, γ=γ∗+ 2lβ
Fϵ1−βcomprises two components: γ∗and2lβ
Fϵ1−β.
Therefore, it is natural to discuss which of these two components plays the dominant role in the
complexity results. First, we write Kin the form:
K=√
2(Lf1+ (ρlα
F(α−1)α−1α−αϵ1−α+ 2lβ
Fϵ1−β)Lg1)
ϵR−1.
19Ifβ <α , the dominating term in γisγ∗=ρlα
F(α−1)α−1α−αϵ1−α. Then, the number of iterations
is
K=O(√
Lf1+lα
Fϵ1−αLg1
ϵ)
=O(√
Lf1
ϵ+√
lα
FLg1
ϵα)
.
Ifβ=α, we haveγ=(
ρ(α−1)α−1α−α+ 2)
lα
Fϵ1−α. Then, the number of iterations is
K=O(√
Lf1+lα
Fϵ1−αLg1
ϵ)
=O(√
Lf1
ϵ+√
lα
FLg1
ϵα)
.
Ifβ >α , the dominating term in γis2lβ
Fϵ1−β. Then, the number of iterations is
K=O
√
Lf1+ 2lβ
Fϵ1−βLg1
ϵ
=O
√
Lf1
ϵ+√
lβ
FLg1
ϵβ
.
•Case ofα= 1. In this case, γ=γ∗+lβ
Fϵ1−β, whereγ∗=ρlF. Similarly, we explore which of
these two elements plays a more signiﬁcant role.
Ifβ < 1, the dominating term in γisγ∗. Then, the number of iterations is
K=O(√
Lf1+ρlFLg1
ϵ)
=O(√
Lf1
ϵ+√
lFLg1
ϵ)
.
Ifβ= 1, we haveγ= (ρ+ 1)lFϵ1−α. Then the number of iterations is
K=O(√
Lf1+ (ρ+ 1)lFLg1
ϵ)
=O(√
Lf1
ϵ+√
lFLg1
ϵ)
.
Ifβ > 1, the dominating term in γislβ
Fϵ1−β. Then, the number of iterations is
K=O
√
Lf1+lβ
Fϵ1−βLg1
ϵ
=O
√
Lf1
ϵ+√
lβ
FLg1
ϵβ
.
Combining the above results, we conclude that
K=O
√
Lf1
ϵ+√
lmax{α,β}
FLg1
ϵmax{α,β}
.
E.8 Proof of Theorem 3.5
Proof. In this proof, we denote Φ∗
kas the optimal value of problem ( Pγ) whenγ=γk, andxkas the output of
PB-APG (Algorithm 1) in thek-th iteration.
•Case ofα> 1.Suppose that Nis the smallest nonnegative integer such that γN≥γ∗
N:=ρlα
F(α−
1)α−1α−αϵ1−α
N. In this case, we have
γN=γ0νN≥ρlα
F(α−1)α−1α−αϵ1−α
N =ρlα
F(α−1)α−1α−αϵ1−α
0(1/η)(1−α)N, (28)
which is equivalent to
γ0(
νη1−α)N≥ρlα
F(α−1)α−1α−αϵ1−α
0. (29)
From ( 29), after at most N:=⌈logη1−αν(
ρlα
F(α−1)α−1α−αϵ1−α
0
γ0)
⌉+iterations, ( 28) holds.
SincexN=PB-APG (ϕN,ψN,Lf1,Lg1,xN−1,ϵN), we have
ΦN(xN)−Φ∗
N≤ϵN, γ N≥γ∗
N,
20which shows that xNis anϵN-optimal solution of ( Pγ) withγ=γN. From the proof in Theorem
2.5(see inequalities ( 17) and ( 18) in Appendix E.3),xNis also an (ϵ0
ηN,2ϵ0
ηN(γ0νN−γ∗
N))-optimal
solution of problem ( P).
Furthermore, note that for any iteration k≥N, inequality ( 29) always holds, which means that the
following statement holds for any k≥N:
Φk(xk)−Φ∗
k≤ϵk, γ k≥γ∗
k. (30)
LetIkbe the number of iterations of PB-APG required to satisfy ( 30) at thek-th iteration of aPB-
APG. Then, for any k≥N, the total number of iterations is
K=I0+I1+···+Ik.
From [ Beck ,2017 , Theorem 10.34], the number of iterations in i-th inner loop satisﬁes:
Ii=√
2(Lf1+γiLg1)
ϵi∥xi−1−x∗
i∥ −1,
where x∗
iis the optimal solution in i-th inner loop. Then we have that
K=k∑
i=0√
2(Lf1+γiLg1)
ϵi∥xi−1−x∗
i∥ −k
≤k∑
i=0√
2(Lf1+γkLg1)
ϵiR−k
=ηk
2−1
η1
2−1√
2(Lf1+γ0νkLg1)
ϵ0−k.
For simplicity, we can also use O(·)to show the value of K.
K=O(√
Lf1+γ0Lg1
ϵ0)
+···+O(√
Lf1+γkLg1
ϵk)
≤ O(√
Lf1+γkLg1
ϵ0)
+···+O(√
Lf1+γkLg1
ϵk)
=O(√
Lf1+γkLg1
ϵk(
1 +√
1/η+√
1/η2+···+√
1/ηk))
=O(√
Lf1+γkLg1
ϵk)
=O(√
Lf1ηk
ϵ0+√
Lg1γ0(ην)k
ϵ0)
.
•Case ofα= 1.Suppose that after Nupdates, we have γN≥ρlF, i.e.,
γ0νN≥ρlF. (31)
This demonstrates that after for all k≥N:= logν(
ρlF
γ0)
, (31) always holds.
Similar to the case of α> 1, the total iteration number is:
K=O(√
Lf1+γ0Lg1
ϵ0)
+···+O(√
Lf1+γkLg1
ϵk)
=O(√
Lf1+γkLg1
ϵk)
=O(√
Lf1ηk
ϵ0+√
Lg1γ0(ην)k
ϵ0)
.
21E.9 Proof of Theorem 3.8
Before proving Theorem 3.8, we need the following lemma that is modiﬁed from Theorem 1 in Lin and Xiao
[2014 ], we state it in the subsequent lemma for completeness.
Lemma E.1. Suppose that Assumptions 2.1,3.1,3.2, and 3.7hold. Let x∗
γbe an optimal solution of problem
(Pγ)and suppose that there exists a constant Rsuch that max{∥y0−x∗
γ∥,∥˜x−x∗
γ∥} ≤R. Then, the sequence
{xk}generated by Algorithm 3satisfy
Φγ(xk)−Φγ(x∗
γ)≤(Lγ+µ
2R2)(
1−√µ
Lγ)k
. (32)
Proof. DenoteLγ=Lf1+γLg1. By Theorem 3.1 in Beck and Teboulle [2009 ], we have
Φγ(˜x)−Φγ(x∗
γ)≤Lγ
2∥y0−x∗
γ∥2. (33)
Utilize Theorem 1 in Lin and Xiao [2014 ], we have
Φγ(xk)−Φγ(x∗
γ)≤(
Φγ(˜x)−Φγ(x∗
γ) +µ
2∥˜x−x∗
γ∥2)(
1−√µ
Lγ)k
(33)
≤(Lγ
2∥y0−x∗
γ∥2+µ
2∥˜x−x∗
γ∥2)(
1−√µ
Lγ)k
≤(Lγ+µ
2R2)(
1−√µ
Lγ)k
.(34)
By Lemma E.1, we are now prepared to prove Theorem 3.8.
Proof. By Lemma E.1, the number of iterations required to achieve an ϵ-optimal solution for problem ( Pγ) is
K=O(√
Lγ
µlog(Lγ+µ
2ϵR2))
=O(√
Lγ
µlog1
ϵ)
.
•Case ofα> 1.In this case,γ=γ∗+ 2lβ
Fϵ1−β, whereγ∗=ρlα
F(α−1)α−1α−αϵ1−α.
Ifβ <α , the dominating term in γisγ∗. Then, the number of iterations is
K=O(√
Lf1+lα
Fϵ1−αLg1
µlog1
ϵ)
=O(√
Lf1
µlog1
ϵ+√
lα
FLg1
ϵα−1log1
ϵ)
.
Ifβ=α, we haveγ=(
ρ(α−1)α−1α−α+ 2)
lα
Fϵ1−α. Then, the number of iterations is
K=O(√
Lf1+lα
Fϵ1−αLg1
µlog1
ϵ)
=O(√
Lf1
µlog1
ϵ+√
lα
FLg1
ϵα−1log1
ϵ)
.
Ifβ >α , the dominating term in γis2lβ
Fϵ1−β. Then, the number of iterations is
K=O
√
Lf1+ 2lβ
Fϵ1−βLg1
µlog1
ϵ
=O
√
Lf1
µlog1
ϵ+√
lβ
FLg1
ϵβ−1log1
ϵ
.
•Case ofα= 1.Whenα= 1,γcan be written as γ=γ∗+lβ
Fϵ1−β, whereγ∗=ρlF.
Ifβ < 1, the dominating term in γisγ∗. Then, the number of iterations is
K=O(√
Lf1+ρlFLg1
µlog1
ϵ)
=O(√
Lf1
µlog1
ϵ+√
lFLg1
ϵα−1log1
ϵ)
.
Ifβ= 1, we haveγ= (ρ+ 1)lFϵ1−α. Then, the number of iterations is
K=O(√
Lf1+ρlFLg1
µlog1
ϵ)
=O(√
Lf1
µlog1
ϵ+√
lFLg1
ϵα−1log1
ϵ)
.
22Ifβ > 1, the dominating term in γislβ
Fϵ1−β. Then, we have
K=O
√
Lf1+lβ
Fϵ1−βLg1
µlog1
ϵ
=O
√
Lf1
µlog1
ϵ+√
lβ
FLg1
ϵβ−1log1
ϵ
.
Combining the above results, we conclude that
K=O
√
Lf1
µlog1
ϵ+√
lmax{α,β}
FLg1
ϵmax{α−1,β−1}log1
ϵ
.
E.10 Proof of Theorem 3.9
Proof. Denotelγ=lf2+γlg2. Deﬁne ΦK
γ,best = min
i=0,...,KΦγ(xi)andˆΦK,j
γ,best = min
i=j,...,KΦγ(xi)for all
0≤j≤K. We claim that the sequence generated by the subgradient method satisﬁes
ΦK
γ,best−Φ∗
γ≤lγ
4R2+ 2 log 2√
K+ 2. (35)
Speciﬁcally, from Lemma 8.24 in Beck [2017 ], for all 0≤j≤K, we have
ˆΦK,j
γ,best−Φ∗
γ≤1
2R2+∑K
k=jη2
k∥ξk∥2
∑K
k=jηk. (36)
Deﬁne ⌊·⌋and⌈·⌉as rounding up and rounding down, respectively. Let j=⌊K
2⌋in (36), by the deﬁnition of
step-sizeηk=R
lγ√k+1, we have
ˆΦK,j
γ,best−Φ∗
γ≤lγ
2R2+∑K
k=⌊K
2⌋1
k+1∑K
k=⌊K
2⌋1√k+1≤lγ
4R2+ 2 log 2√
K+ 2, (37)
where the second inequality follows from that∑K
k=⌊K
2⌋1
k+1≤∫K
⌈K
2⌉−11
s+1ds≤2 log 2 and
∑K
k=⌊K
2⌋1√k+1≥∫K+1
⌈K
2⌉1√s+1ds≥1
2√
K+ 2.
From the fact that ΦK
γ,best≤ˆΦK,j
γ,best , The desired result of ( 35) follows.
Then, inequality ( 35) demonstrates that the number of iterations to obtain an ϵ-optimal solution for problem
(Pγ) is
K=O(lf2+γlg2
ϵ)2
.
•Case ofα> 1.we haveγ=γ∗+ 2lβ
f2ϵ1−βandγ∗=ρlα
f2(α−1)α−1α−αϵ1−α.
Ifβ <α , the dominating term in γisγ∗. Then, the number of iterations is
K=O(
lf2+lα
f2ϵ1−αlg2
ϵ)2
=O(
l2
f2
ϵ2+l2α
f2l2
g2
ϵ2α)
.
Ifβ=α, we haveγ=(
ρ(α−1)α−1α−α+ 2)
lα
Fϵ1−α. Then, the number of iterations is
K=O(
lf2+lα
f2ϵ1−αlg2
ϵ)2
=O(
l2
f2
ϵ2+l2α
f2l2
g2
ϵ2α)
.
Ifβ >α , the dominating term in γis2lβ
Fϵ1−β. Then, the number of iterations is
K=O(
lf2+ 2lβ
f2ϵ1−βlg2
ϵ)2
=O(
l2
f2
ϵ2+l2β
f2l2
g2
ϵ2β)
.
23•Case ofα= 1.we haveγ=γ∗+lβ
f2ϵ1−βandγ∗=ρlf2.
Ifβ < 1, the dominating term in γisγ∗. Then, the number of iterations is
K=O(lf2+ρlf2lg2
ϵ)2
=O(
l2
f2
ϵ2+l2
f2l2
g2
ϵ2)
.
Ifβ= 1, we haveγ= (ρ+ 1)lFϵ1−α. Then, the number of iterations is
K=O(lf2+ρlf2lg2
ϵ)2
=O(
l2
f2
ϵ2+l2
f2l2
g2
ϵ2)
.
Ifβ > 1, the dominating term in γislβ
f2ϵ1−β. Then, the number of iterations is
K=O(
lf2+lβ
f2lg2ϵ1−β
ϵ)2
=O(
l2
f2
ϵ2+l2β
f2l2
g2
ϵ2β)
.
Combining the above results, we conclude that
K=O(
l2
f2
ϵ2+lmax{2α,2β}
f2l2
g2
ϵmax{2α,2β})
.
E.11 Proof of Theorem 3.10
Proof. Denotelγ=lf2+γlg2, deﬁne ΦK
γ,best = min
i=0,...,KΦγ(xi). From Theorem 8.31 in Beck [2017 ], the
sequence generated by the subgradient method satisﬁes
ΦK
γ,best−Φ∗
γ≤2l2
γ
µf2(K+ 1).
This demonstrates that the number of iterations to obtain an ϵ-optimal solution for problem ( Pγ) is
K=O((lf2+γlg2)2
µf2ϵ)
.
•Case ofα> 1.we haveγ=γ∗+ 2lβ
f2ϵ1−βandγ∗=ρlα
f2(α−1)α−1α−αϵ1−α.
Ifβ <α , the dominating term in γisγ∗. Then, the number of iterations is
K=O(
(lf2+lα
f2ϵ1−αlg2)2
µf2ϵ)
=O(
l2
f2
µf2ϵ+l2α
f2l2
g2
µf2ϵ2α−1)
.
Ifβ=α, we haveγ=(
ρ(α−1)α−1α−α+ 2)
lα
Fϵ1−α. Then, the number of iterations is
K=O(
(lf2+lα
f2ϵ1−αlg2)2
µf2ϵ)
=O(
l2
f2
µf2ϵ+l2α
f2l2
g2
µf2ϵ2α−1)
.
Ifβ >α , the dominating term in γis2lβ
Fϵ1−β. Then, the number of iterations is
K=O(
(lf2+ 2lβ
f2ϵ1−βlg2)2
µf2ϵ)
=O(
l2
f2
µf2ϵ+l2β
f2l2
g2
µf2ϵ2β−1)
.
•Case ofα> 1.we haveγ=γ∗+lβ
f2ϵ1−βandγ∗=ρlf2.
Ifβ < 1, the dominating term in γisγ∗. Then, the number of iterations is
K=O((lf2+ρlf2lg2)2
µf2ϵ)
=O(
l2
f2
µf2ϵ+l2
f2l2
g2
µf2ϵ)
.
24Ifβ= 1, we haveγ= (ρ+ 1)lFϵ1−α. Then, the number of iterations is
K=O((lf2+ρlf2lg2)2
µf2ϵ)
=O(
l2
f2
µf2ϵ+l2
f2l2
g2
µf2ϵ)
.
Ifβ > 1, the dominating term in γislβ
f2ϵ1−β. Then, the number of iterations is
K=O(
(lf2+lβ
f2lg2ϵ1−β)2
µf2ϵ)
=O(
l2
f2
µf2ϵ+l2β
f2l2
g2
µf2ϵ2β−1)
.
Combining the above results, we conclude that
K=O(
l2
f2
µf2ϵ+lmax{2α,2β}
f2l2
g2
µf2ϵmax{2α−1,2β−1})
.
F Implementation details
In this section, we provide supplementary experiment settings and results. Speciﬁcally, in Appendix F.1, we
present the detailed experimental settings, and in Appendix F.2, we provide the detailed experimental results.
Additionally, in Appendix F.3andF.4, we conduct experiments with different values of penalty parameter γ
and solution accuracy ϵ, respectively.
F.1 Experiment setting
All simulations are implemented using MATLAB R2023a on a PC running Windows 11 with an AMD (R)
Ryzen (TM) R7-7840H CPU (3.80GHz) and 16GB RAM.
F.1.1 Experiment setting of Section 4.1
We conduct the ﬁrst experiment using the a1a.t data from LIBSVM datasets5. This data consists of 30,956
instances, each with n= 123 features. For this experiment, a sample of 1,000instances is taken from the data,
denoted asA. The corresponding labels for these instances are denoted as b, where each label biis either −1
or1, corresponding to the i-th instance ai.
The Greedy FISTA algorithm [ Liang et al. ,2022 ] is used as a benchmark to compute G∗. To compute the
proximal mapping of f2(x) +γg2(x)in problem ( Pγ), i.e, projection onto a 1-norm ball, we utilize the method
proposed in Duchi et al. [2008 ], which performs exact projection in O(n)expected time, where n is the dimen-
sion of x.
For the PB-APG and PB-APG-sc algorithms, we set the value of γ= 105, and we terminate the algorithms
when∥xk+1−xk∥ ≤ 10−10. For the aPB-APG and aPB-APG-sc algorithms, we set γ0=1
25,ν= 20 ,
η= 10 , andϵ0= 10−6. The iterations of these two algorithms continue until ϵkreaches 10−10(meanwhile,
γ= 105).
We compare our methods with MNG, BiG-SAM, DBGD, a-IRG, CG-BiO, Bi-SG, and R-APM in this experi-
ment. Speciﬁcally, for R-APM [ Samadi et al. ,2023 ], the regularization parameter ηis set toη= 1/γ, reﬂecting
the equivalence of the penalty formulation ( Pγ) to ( PReg), withσ= 1/γ, as previously discussed.
We note that the termination criterion ∥xk+1−xk∥ ≤10−10used in our experiments is different from the one
proposed in our algorithms since the parameters required for the latter are not easily measurable. Nevertheless,
this termination criterion is also widely used in the literature, as it corresponds to a gradient mapping [ Beck ,
2017 ,Nesterov ,2018 ,Davis and Drusvyatskiy ,2019 ]. Furthermore, Theorem 3.5 of Drusvyatskiy and Lewis
[2018b ] implies that ∥xk+1−xk∥also measures the distance to the optimal solution set.
F.1.2 Experiment setting of Section 4.2
In the second experiment, we address the problem of least squares regression using the YearPredictionMSD
data from the UCI Machine Learning Repository6. This data consists of 515,345 songs with release years
ranging from 1992 to2011 . Each song has 90features, and the corresponding release year is used as the label.
5https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a.t
6https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD
25For this experiment, a sample of m= 1,000songs is taken from the data, and the feature matrix and release
years vector are denoted as Aandb, respectively.
Following Section 5.2 in Merchav and Sabach [2023 ], we apply the min-max scaling technique to normalize the
feature matrix A. Additionally, we add an intercept term and 90collinear features to Asuch that the resulting
matrixATAbecomes positive semi-deﬁnite, which implies that the feasible set Xoptis not a singleton.
We compare our methods with a-IRG, BiG-SAM, and Bi-SG in this experiment. Speciﬁcally, for BiG-SAM
[Sabach and Shtern ,2017 ], we consider the accuracy parameter δfor the Moreau envelope with two values,
namelyδ= 1andδ= 0.01.
To benchmark the performance, we utilize the MATLAB function lsqminnorm to computeG∗. Moreover, we
follow the parameter settings outlined in Section 4.1.
F.2 Detailed results of experiments
To approximate the optimal value F∗, we use the MATLAB function fmincon to solve a relaxed version of
the function-value-based reformulations in equation ( PVal). In this relaxed version, we replace the constraint in
(PVal) withG(x)−G∗≤ε, whereε= 10−10. This allows us to obtain an approximation of the optimal value
while allowing for a small deviation from the true optimal value G∗.
We gather the total number of iterations for our methods, as well as the lower- and upper-level objective values
and the optimal gaps for all the methods, in Table 3. Subsequently, we compare the optimal gaps of all methods,
which are deﬁned as G(x)−G∗andF(x)−F∗for the lower- and upper-level optimal gaps, respectively.
Table 3: Methods comparison: lower- and upper-level objectives and optimal gaps
Logistic Regression Problem ( 5)
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 1470 3.2794e-01 1.7630e-08 4.9382e+00 -3.3998e-03
aPB-APG 1010 3.2794e-01 1.7630e-08 4.9382e+00 -3.3998e-03
PB-APG-sc 2278 3.2794e-01 1.7630e-08 4.9382e+00 -3.3998e-03
aPB-APG-sc 1046 3.2794e-01 1.7630e-08 4.9382e+00 -3.3998e-03
MNG / 3.4540e-01 1.7459e-02 1.7469e+00 -3.1947e+00
BiG-SAM / 3.3878e-01 1.0840e-02 2.2873e+00 -2.6543e+00
DBGD / 5.2681e-01 1.9887e-01 8.8408e-02 -4.8532e+00
a-IRG / 3.3765e-01 9.7121e-03 2.5401e+00 -2.4016e+00
CG-BiO / 4.3040e-01 1.0246e-01 3.7684e-01 -4.5648e+00
Bi-SG / 3.2806e-01 1.1530e-04 4.6873e+00 -2.5432e-01
R-APM / 3.2794e-01 1.7645e-08 4.9382e+00 -3.4013e-03
Least Squares Regression Problem ( 6)
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 39314 7.3922e-03 6.0034e-07 4.7236e+00 -1.1888e-01
aPB-APG 40784 7.3922e-03 6.0030e-07 4.7236e+00 -1.1887e-01
PB-APG-sc 46446 7.3922e-03 6.0034e-07 4.7236e+00 -1.1888e-01
aPB-APG-sc 61777 7.3922e-03 6.0035e-07 4.7236e+00 -1.1888e-01
BiG-SAM (δ= 1) / 7.5189e-03 1.2733e-04 3.5081e+00 -1.3344e+00
BiG-SAM (δ= 0.01) / 7.3958e-03 4.2281e-06 5.8510e+01 5.3668e+01
a-IRG / 1.6224e-02 8.8328e-03 4.7745e-01 -4.3651e+00
Bi-SG / 8.5782e-03 1.1866e-03 1.3832e+00 -3.4593e+00
Table 3reveals that for the logistic regression problem ( 5), our PB-APG, aPB-APG, PB-APG-sc, and aPB-APG-
sc exhibit almost identical function values for both objectives, surpassing other methods in terms of optimal
gaps for the lower- and upper-level objectives (measured by the numerical value of the upper-level objective).
In the case of the least squares regression problem ( 6), aPB-APG achieves the smallest optimal gaps for both
objectives, followed by PB-APG and PB-APG-sc. These results demonstrate that our methods, despite yielding
larger upper-level function values, generate solutions that are signiﬁcantly closer to the optimal solution, as
depicted in Figure 1. Additionally, for the problem in ( 5), both aPB-APG and aPB-APG-sc require fewer
iterations than PB-APG and PB-APG-sc, respectively. This can be attributed to the warm-start mechanism
employed in aPB-APG and aPB-APG-sc. Moreover, for the problem in ( 6), both aPB-APG and aPB-APG-sc
require more iterations than PB-APG and PB-APG-sc, respectively. However, they exhibit staircase-shaped
curves, which avoid the unwanted oscillations in PB-APG and PB-APG-sc, we have a similar observation in
Figure 2.
F.3 Supplementary experiments for different penalty parameters
In this section, we investigate the impact of different values of penalty parameter γon the experimental results
of problems ( 5) and ( 6). We setγto be either 2×104or5×105for PB-APG and PB-APG-sc, and choose the
26corresponding γ0values as0.2
25or5
25for aPB-APG and aPB-APG-sc, respectively. The remaining settings are
the same as in Section 4.
We plot the values of the residuals of the lower-level objective G(xk)−G∗and the upper-level objective over
time in Figures 3and4. Additionally, we also collect the total number of iterations, the lower- and upper-level
objective values, and the optimal gaps of our methods in Table 4for problems ( 5) and ( 6) with different values
ofγ.
0 0.05 0.1 0.15 0.2 0.25 0.3 0.3510-810-610-410-2100102
0 0.05 0.1 0.15 0.2 0.25 0.3 0.3505101520
0 0.2 0.4 0.6 0.8 110-1010-5100
0 0.2 0.4 0.6 0.8 105101520
Figure 3: LRP ( 5) withγ= 2×104(left two subﬁgures) and γ= 5×105(right two subﬁgures).
0 5 10 15 20 25 3010-610-410-2100102
0 5 10 15 20 25 30020406080100
0 20 40 60 80 10010-1010-5100
0 20 40 60 80 100020406080100
Figure 4: LSRP ( 6) withγ= 2×104(left two subﬁgures) and γ= 5×105(right two subﬁgures).
As Figures 3and4show, our methods consistently outperform the other methods for both the lower- and upper-
level objectives, irrespective of the penalty parameter γ, since our methods achieve lower optimal gaps and
desired function values for the lower- and upper-level objectives, respectively. The only exception is problem
(6) withγ= 5×105, as the third subﬁgure of Figure 4shows, since we do not set the solution accuracy of
BiG-SAM (δ= 0.01), it attains a lower optimal gap than our PB-APG-sc and aPB-APG-sc for the lower-level
objective. However, BiG-SAM ( δ= 0.01) produces signiﬁcantly worse upper-level objective values, which
are much larger than the objective values of our methods.
Table 4: Lower- and upper-level objectives and optimal gaps with different penalty parameters for
problem ( 5).
γ= 2×104
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 883 3.2794e-01 4.3569e-07 4.9243e+00 -1.7362e-02
aPB-APG 967 3.2794e-01 4.3569e-07 4.9243e+00 -1.7362e-02
PB-APG-sc 1123 3.2794e-01 4.3569e-07 4.9243e+00 -1.7362e-02
aPB-APG-sc 879 3.2794e-01 4.3569e-07 4.9243e+00 -1.7362e-02
γ= 5×105
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 1623 3.2794e-01 7.0685e-10 4.9410e+00 -5.7820e-04
aPB-APG 976 3.2794e-01 7.0685e-10 4.9410e+00 -5.7820e-04
PB-APG-sc 4848 3.2794e-01 7.0684e-10 4.9410e+00 -5.7820e-04
aPB-APG-sc 1018 3.2794e-01 7.0687e-10 4.9410e+00 -5.7821e-04
Tables 3,4, and 5reveal that the number of iterations for our methods increases as penalty parameter γin-
creases. However, it is worth noting that the accuracy of the obtained solutions also increases, as indicated by
the decreasing optimal gaps of the lower- and upper-level objectives. This observation conﬁrms that the com-
plexity results and solution accuracies of our methods are indeed dependent on the choice of penalty parameters,
speciﬁcally,Lγ, as demonstrated in corresponding Theorem 3.3and other related theorems.
27Table 5: Lower- and upper-level objectives and optimal gaps with different penalty parameters for
problem ( 6).
γ= 2×104
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 17153 7.4052e-03 1.3619e-05 4.2843e+00 -5.5818e-01
aPB-APG 20877 7.4052e-03 1.3619e-05 4.2843e+00 -5.5818e-01
PB-APG-sc 27501 7.4052e-03 1.3619e-05 4.2843e+00 -5.5818e-01
aPB-APG-sc 40077 7.4052e-03 1.3619e-05 4.2843e+00 -5.5818e-01
γ= 5×105
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 85511 7.3916e-03 2.4094e-08 4.8198e+00 -2.2752e-02
aPB-APG 85502 7.3916e-03 2.4093e-08 4.8198e+00 -2.2752e-02
PB-APG-sc 173731 7.3916e-03 2.4071e-08 4.8198e+00 -2.2740e-02
aPB-APG-sc 166324 7.3916e-03 2.4091e-08 4.8198e+00 -2.2751e-02
F.4 Supplementary experiments for different solution accuracies
In this section, we investigate the impact of different solution accuracies on the experimental results of problems
(5) and ( 6). We setϵto be either 10−4or10−7and terminate the algorithms for PB-APG and PB-APG-sc when
∥xk+1−xk∥ ≤ϵ. For aPB-APG and aPB-APG-sc, we choose the corresponding ϵ0values as 1or10−3. The
remaining settings are the same as in Section 4.
We also plot the values of the residuals of the lower-level objective G(xk)−G∗and the upper-level objective
over time in Figures 5and6. Additionally, we also collect the total number of iterations, the lower- and upper-
level objective values, and the optimal gaps of our methods in Table 6for problems ( 5) and ( 6) with different
solution accuracies.
0 0.05 0.1 0.15 0.210-1010-5100
0 0.05 0.1 0.15 0.205101520
0 0.05 0.1 0.15 0.2 0.25 0.310-1010-5100
0 0.05 0.1 0.15 0.2 0.25 0.305101520
Figure 5: LRP ( 5) withϵ= 10−4(left two subﬁgures) and ϵ= 10−7(right two subﬁgures).
0 0.05 0.1 0.15 0.2 0.25 0.310-410-2100102
0 0.05 0.1 0.15 0.2 0.25 0.3020406080100
Figure 6: LSRP ( 6) withϵ= 10−4(left two subﬁgures) and ϵ= 10−7(right two subﬁgures).
From Figures 5and6, it is evident that in most cases, our methods outperform the other methods in terms
of both the lower- and upper-level objectives. However, there is an exception in the case of the upper-level
objective for problem ( 6) whenϵ= 10−4. As illustrated in the second subﬁgure in Figure 6, our methods
exhibit larger function values for the upper-level objective compared to the other methods (except BiG-SAM
(δ= 0.01)), despite still achieving smaller optimal gaps for the lower-level objective. This discrepancy actually
indicates that our methods have not yet achieved the desired accuracy when ϵ= 10−4, and it is important to note
that∥xk+1−xk∥ ≤ϵis not the termination criterion in our proposed algorithms, as explained in Appendix
F.1. Therefore, the larger optimality gaps for the upper-level objective in this case may be attributed to the
termination criterion.
28Table 6: Lower- and upper-level objectives and optimal gaps with different solution accuracies for
problem ( 5).
ϵ= 10−4
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 124 3.2794e-01 2.8671e-07 4.9483e+00 6.7024e-03
aPB-APG 148 3.2794e-01 2.3660e-07 4.9419e+00 2.9831e-04
PB-APG-sc 100 3.2794e-01 5.4674e-07 4.9287e+00 -1.2956e-02
aPB-APG-sc 149 3.2794e-01 7.9015e-07 4.9302e+00 -1.1404e-02
ϵ= 10−7
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 841 3.2794e-01 1.7631e-08 4.9382e+00 -3.3999e-03
aPB-APG 551 3.2794e-01 1.7707e-08 4.9382e+00 -3.4075e-03
PB-APG-sc 225 3.2794e-01 1.7493e-08 4.9383e+00 -3.3691e-03
aPB-APG-sc 614 3.2794e-01 1.7507e-08 4.9382e+00 -3.3874e-03
Table 7: Lower- and upper-level objectives and optimal gaps with different solution accuracies for
problem ( 6).
ϵ= 10−4
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 426 7.6950e-03 3.0342e-04 6.0249e+01 5.5407e+01
aPB-APG 432 7.8018e-03 4.1016e-04 4.8967e+01 4.4125e+01
PB-APG-sc 437 7.6456e-03 2.5400e-04 6.0196e+01 5.5354e+01
aPB-APG-sc 517 7.6143e-03 2.2274e-04 4.9292e+01 4.4449e+01
ϵ= 10−7
Method Total iterations Lower-level value Lower-level gap Upper-level value Upper-level gap
PB-APG 13707 7.3922e-03 5.9756e-07 4.7279e+00 -1.1460e-01
aPB-APG 7803 7.3923e-03 6.5025e-07 4.7300e+00 -1.1248e-01
PB-APG-sc 12724 7.3922e-03 5.7840e-07 4.7354e+00 -1.0714e-01
aPB-APG-sc 7429 7.3922e-03 6.3816e-07 4.7326e+00 -1.0992e-01
Tables 3,6, and 7demonstrate that the number of iterations for our methods also increases with the solution
accuracy, while the optimal gaps of the lower- and upper-level objectives decrease correspondingly. This ﬁnding
conﬁrms that the number of iterations and the optimal gaps are inﬂuenced by the solution accuracy, as illustrated
in the expressions for the number of iterations provided by Theorem 3.3and other related theorems.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope?
Answer: [Yes]
Justiﬁcation: Please refer to Abstract and Section 1.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims made in the
paper.
•The abstract and/or introduction should clearly state the claims made, including the contribu-
tions made in the paper and important assumptions and limitations. A No or NA answer to this
question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reﬂect how much the
results can be expected to generalize to other settings.
•It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals are not
attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: Please refer to the assumptions adopted in this paper (e.g. Assumptions 2.1,2.2, and
3.2), our study is based on these assumptions.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that the
paper has limitations, but those are not discussed in the paper.
•The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to violations of
these assumptions (e.g., independence assumptions, noiseless settings, model well-speciﬁcation,
asymptotic approximations only holding locally). The authors should reﬂect on how these as-
sumptions might be violated in practice and what the implications would be.
•The authors should reﬂect on the scope of the claims made, e.g., if the approach was only tested
on a few datasets or with a few runs. In general, empirical results often depend on implicit
assumptions, which should be articulated.
•The authors should reﬂect on the factors that inﬂuence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low or
images are taken in low lighting. Or a speech-to-text system might not be used reliably to
provide closed captions for online lectures because it fails to handle technical jargon.
•The authors should discuss the computational efﬁciency of the proposed algorithms and how
they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to address prob-
lems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by reviewers
as grounds for rejection, a worse outcome might be that reviewers discover limitations that
aren’t acknowledged in the paper. The authors should use their best judgment and recognize
that individual actions in favor of transparency play an important role in developing norms
that preserve the integrity of the community. Reviewers will be speciﬁcally instructed to not
penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and a com-
plete (and correct) proof?
Answer: [Yes]
Justiﬁcation: Please refer to the theorems and the proofs of them in this paper, please refer to Ap-
pendix E. For example, Theorem 3.3and its proof in Appendix E.7.
Guidelines:
•The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
30•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if they appear
in the supplemental material, the authors are encouraged to provide a short proof sketch to
provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented by
formal proofs provided in appendix or supplemental material.
•Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main experimen-
tal results of the paper to the extent that it affects the main claims and/or conclusions of the paper
(regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justiﬁcation: Please refer to Section 4and Appendix F.
Guidelines:
•The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived well by
the reviewers: Making the paper reproducible is important, regardless of whether the code and
data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken to make
their results reproducible or veriﬁable.
•Depending on the contribution, reproducibility can be accomplished in various ways. For ex-
ample, if the contribution is a novel architecture, describing the architecture fully might sufﬁce,
or if the contribution is a speciﬁc model and empirical evaluation, it may be necessary to either
make it possible for others to replicate the model with the same dataset, or provide access to the
model. In general. releasing code and data is often one good way to accomplish this, but repro-
ducibility can also be provided via detailed instructions for how to replicate the results, access
to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint,
or other means that are appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submissions
to provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how to
reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe the
architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should either
be a way to access this model for reproducing the results or a way to reproduce the model
(e.g., with an open-source dataset or instructions for how to construct the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of
closed-source models, it may be that access to the model is limited in some way (e.g.,
to registered users), but it should be possible for other researchers to have some path to
reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufﬁcient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [Yes]
Justiﬁcation: Please refer to Section 4and Appendix Fand the supplemental material.
Guidelines:
•The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be possible,
so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless
this is central to the contribution (e.g., for a new open-source benchmark).
•The instructions should contain the exact command and environment needed to run to repro-
duce the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
31•The authors should provide instructions on data access and preparation, including how to access
the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state which
ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized versions (if
applicable).
•Providing as much information as possible in supplemental material (appended to the paper) is
recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [Yes]
Justiﬁcation: Please refer to Section 4and Appendix F.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental material.
7.Experiment Statistical Signiﬁcance
Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate infor-
mation about the statistical signiﬁcance of the experiments?
Answer: [No]
Justiﬁcation: The error bars are not applicable in this paper.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, conﬁdence inter-
vals, or statistical signiﬁcance tests, at least for the experiments that support the main claims of
the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for example,
train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
•The method for calculating the error bars should be explained (closed form formula, call to a
library function, bootstrap, etc.)
•The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error of the
mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should preferably
report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of
errors is not veriﬁed.
•For asymmetric distributions, the authors should be careful not to show in tables or ﬁgures
symmetric error bars that would yield results that are out of range (e.g. negative error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how they were
calculated and reference the corresponding ﬁgures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufﬁcient information on the computer re-
sources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
Answer: [Yes]
Justiﬁcation: Please refer to Section 4and Appendix F.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual experimen-
tal runs as well as estimate the total compute.
32•The paper should disclose whether the full research project required more compute than the
experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it
into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS
Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justiﬁcation: Please refer to Section 4and Appendix F.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consideration
due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative societal im-
pacts of the work performed?
Answer: [NA]
Justiﬁcation: There is no societal impact of the work performed.
Guidelines:
•The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal impact or
why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses (e.g., dis-
information, generating fake proﬁles, surveillance), fairness considerations (e.g., deployment
of technologies that could make decisions that unfairly impact speciﬁc groups), privacy consid-
erations, and security considerations.
•The conference expects that many papers will be foundational research and not tied to partic-
ular applications, let alone deployments. However, if there is a direct path to any negative
applications, the authors should point it out. For example, it is legitimate to point out that
an improvement in the quality of generative models could be used to generate deepfakes for
disinformation. On the other hand, it is not needed to point out that a generic algorithm for
optimizing neural networks could enable people to train models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is being used
as intended and functioning correctly, harms that could arise when the technology is being used
as intended but gives incorrect results, and harms following from (intentional or unintentional)
misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation strate-
gies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for
monitoring misuse, mechanisms to monitor how a system learns from feedback over time, im-
proving the efﬁciency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators,
or scraped datasets)?
Answer: [NA]
Justiﬁcation: The paper poses no such risks.
Guidelines:
•The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere
to usage guidelines or restrictions to access the model or implementing safety ﬁlters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do not require
this, but we encourage authors to take this into account and make a best faith effort.
3312.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
properly credited and are the license and terms of use explicitly mentioned and properly respected?
Answer: [Yes]
Justiﬁcation: All the creators or original owners of assets are properly credited, and the license and
terms of use are explicitly mentioned and properly respected, please refer to Section 4and Appendix
F.
Guidelines:
•The answer NA means that the paper does not use existing assets.
•The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a URL.
•The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of service of
that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package should
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
some datasets. Their licensing guide can help determine the license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of the derived
asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to the asset’s
creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation provided
alongside the assets?
Answer: [Yes]
Justiﬁcation: Please refer to the supplemental materials and the ‘README.m’ ﬁle.
Guidelines:
•The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their submis-
sions via structured templates. This includes details about training, license, limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose asset is
used.
•At submission time, remember to anonymize your assets (if applicable). You can either create
an anonymized URL or include an anonymized zip ﬁle.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper include
the full text of instructions given to participants and screenshots, if applicable, as well as details about
compensation (if any)?
Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
•Including this information in the supplemental material is ﬁne, but if the main contribution of
the paper involves human subjects, then as much detail as possible should be included in the
main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
labor should be paid at least the minimum wage in the country of the data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
Question: Does the paper describe potential risks incurred by study participants, whether such risks
were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equiv-
alent approval/review based on the requirements of your country or institution) were obtained?
Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
34Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent) may
be required for any human subjects research. If you obtained IRB approval, you should clearly
state this in the paper.
•We recognize that the procedures for this may vary signiﬁcantly between institutions and loca-
tions, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
their institution.
•For initial submissions, do not include any information that would break anonymity (if applica-
ble), such as the institution conducting the review.
35