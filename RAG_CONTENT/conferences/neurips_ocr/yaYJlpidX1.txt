Metalearning to Continually Learn In Context
Anonymous Author(s)
Affiliation
Address
email
Abstract
General-purpose learning systems should improve themselves in open-ended fash- 1
ion in ever-changing environments. Conventional learning algorithms for neural 2
networks, however, suffer from catastrophic forgetting (CF)—previously acquired 3
skills are forgotten when a new task is learned. Instead of hand-crafting new 4
algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to 5
train self-referential neural networks to meta-learn their own in-context continual 6
(meta-)learning algorithms. ACL encodes continual learning desiderata—good 7
performance on both old and new tasks—into its meta-learning objectives. Our 8
experiments demonstrate that, in general, in-context learning algorithms also suffer 9
from CF but ACL effectively solves such “in-context catastrophic forgetting”. Our 10
ACL-learned algorithms outperform hand-crafted ones and popular meta-continual 11
learning methods on the Split-MNIST benchmark in the replay-free setting, and 12
enables continual learning of diverse tasks consisting of multiple few-shot and stan- 13
dard image classification datasets. Going beyond, we also highlight the limitations 14
of in-context continual learning, by investigating the possibilities to extend ACL to 15
the realm of state-of-the-art CL methods which leverage pre-trained models.116
1 Introduction 17
Enemies of memories are other memories [ 1]. Continually-learning artificial neural networks (NNs) 18
are memory systems in which their weights store memories of task-solving skills or programs, and 19
their learning algorithm is responsible for memory read/write operations. Conventional learning 20
algorithms—used to train NNs in the standard scenarios where all training data is available at once — 21
are known to be inadequate for continual learning (CL) of multiple tasks where data for each task 22
is available sequentially and exclusively , one at a time. They suffer from “catastrophic forgetting” 23
(CF; [ 2–5]); the NNs forget, or rather, the learning algorithm erases, previously acquired skills, in 24
exchange of learning to solve a new task. Naturally, a certain degree of forgetting is unavoidable 25
when the memory capacity is limited, and the amount of things to remember exceeds such an upper 26
bound. In general, however, capacity is not the fundamental cause of CF; typically, the same NNs, 27
suffering from CF when trained on two tasks sequentially, can perform well on both tasks when they 28
are jointly trained on the two tasks at once instead (see, e.g., [6]). 29
The real root of CF lies in the learning algorithm as a memory mechanism. A “good” CL algorithm 30
should preserve previously acquired knowledge while also leveraging previous learning experiences 31
to improve future learning, by maximally exploiting the limited memory space of model parameters. 32
All of this is the decision-making problem of learning algorithms . In fact, we can not blame the 33
conventional learning algorithms for causing CF, since they are not aware of such a problem. They 34
are designed to train NNs for a given task at hand; they treat each learning experience independently 35
(they are stationary up to certain momentum parameters in certain optimizers), and ignore any 36
1Here we’ll add a link to our public GitHub code repository.
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.potential influence of current learning on past or future learning experiences. Effectively, more 37
sophisticated algorithms previously proposed against CF [ 7,8], such as elastic weight consolidation 38
[9,10] or synaptic intelligence [ 11], often introduce manually-designed constraints as regularization 39
terms to explicitly penalize current learning for deteriorating knowledge acquired in past learning. 40
Here, instead of hand-crafting learning algorithms for continual learning, we train self-referential 41
neural networks [ 12,13] to meta-learn their own “in-context” continual learning algorithms. We 42
train them through gradient descent on learning objectives that reflect desiderata for continual learn- 43
ing algorithms—good performance on both old and new tasks, including forward and backward 44
transfer. In fact, by extending the standard settings of few-shot or meta-learning based on sequence- 45
processing NNs [ 14–18], the continual learning problem can also be formulated as a long-span 46
sequence processing task [ 19]. Corresponding CL sequences can be obtained by concatenating multi- 47
ple few-shot/meta-learning sub-sequences, where each sub-sequence consists of input/target examples 48
corresponding to the task to be learned in-context. As we’ll see in Sec. 3, this setting also allows us 49
to seamlessly express classic desiderata for CL as part of objective functions of the meta-learner. 50
Once formulated as such a sequence-learning task, we let gradient descent search for CL algorithms 51
achieving the desired CL behaviors in the program space of NN weights. In principle, all typical 52
challenges of CL—such as the stability-plasticity dilemma [ 20]—are automatically discovered and 53
handled by the gradient-based program search process. Once trained, CL is automated through 54
recursive self-modification dynamics of the trained NN, without requiring any human intervention 55
such as adding extra regularization or setting hyper-parameters for CL. Therefore, we call our 56
method, Automated Continual Learning (ACL). 57
Our experiments focus on supervised image classification, making use of standard few-shot learning 58
datasets for meta-training, namely, Mini-ImageNet [ 21,22], Omniglot [ 23], and FC100 [ 24], while 59
we also meta-test on other datasets including MNIST [ 25], FashionMNIST [ 26] and CIFAR-10 [ 27]. 60
Our core contribution is a set of focused experiments showing various facets of in-context CL: (1) 61
We first reveal the “in-context catastrophic forgetting” problem using two-task settings (Sec. 4.1) and 62
analyse its emergence (Sec. 4.2). We are not aware of any prior work discussing this problem. (2) 63
We show very promising results of our ACL-trained learning algorithm on the classic Split-MNIST 64
[6,28] benchmark, outperforming hand-crafted learning algorithms and prior meta-continual learning 65
methods [ 29–31]. (3) We experimentally illustrate the limitations of ACL on 5-datasets [ 32] and 66
Split-CIFAR100 by comparing to more recent prompt-based state-of-the-art CL methods [33, 34]. 67
2 Background 68
2.1 Continual Learning 69
The main focus of this work is on continual learning [ 35,36] insupervised learning settings even 70
though high-level principles we discuss here also transfer to reinforcement learning settings [ 37]. 71
In addition, we focus on the realm of CL methods that keep model sizes constant (unlike certain 72
CL methods that incrementally add more parameters as more tasks are presented; see, e.g., [ 38]), 73
and do not make use of any external replay memory (used in other CL methods; see, e.g., [39–43]). 74
Classic desiderata for a CL system (see, e.g., [ 44,45]) are typically summarized as good performance 75
on three metrics: classification accuracies on each dataset (their average), backward transfer (i.e., im- 76
pact of learning a new task on the model’s performance on previous tasks; e.g., catastrophic forgetting 77
is a negative backward transfer), and forward transfer (impact of learning a task for the model’s perfor- 78
mance on a future task). From a broader perspective of meta-learning systems, we may also measure 79
other effects such as learning acceleration (i.e., whether the system leverages previous learning ex- 80
periences to accelerate future learning); here our primary focus remains the classic CL metrics above. 81
2.2 Few-shot/meta-learning via Sequence Learning 82
In Sec. 3, we’ll formulate continual learning as a long-span sequence processing task. This is a direct 83
extension of the classic few-shot/meta learning formulated as a sequence learning problem. In fact, 84
since the seminal works [ 14–17] (see also [ 46]), many sequence processing neural networks (see, 85
e.g., [ 47–58] including Transformers [ 59,18]) have been trained as a meta-learner [ 13,12] that learn 86
by observing sequences of training examples (i.e., pairs of inputs and their labels) in-context. 87
2Figure 1: An illustration of meta-training in Automated Continual Learning (ACL) for a self-
referential/modifying weight matrix W0. Weights WAobtained by observing examples for Task A
(blue) are used to predict a test example for Task A. Weights WA,Bobtained by observing examples
for Task A then those for Task B ( yellow ) are used to predict a test example for Task A (backward
transfer) as well as a test example for Task B (forward transfer).
Here we briefly review such a formulation. Let d,N,K,Pbe positive integers. In sequential 88
N-way K-shot classification settings, a sequence processing NN with a parameter vector θ∈RP89
observes a pair ( xt,yt) where xt∈Rdis the input and yt∈ {1, ..., N}is its label at each step 90
t∈ {1, ..., N ·K}, corresponding to Kexamples for each one of Nclasses. After the presentation of 91
these N·Kexamples (often called the support set ), one extra input x∈Rd(often called the query ) 92
is fed to the model without its true label but with an “unknown label” token ∅(number of input labels 93
accepted by the model is thus N+1). The model is trained to predict its true label, i.e., the parameters 94
of the model θare optimized to maximize the probability p(y|(x1, y1), ...,(xN·K, yN·K),(x,∅);θ) 95
of the correct label y∈ {1, ..., N}of the input query x. Since class-to-label associations are 96
randomized and unique to each sequence ( (x1, y1), ...,(xN·K, yN·K),(x,∅)), each such a sequence 97
represents a new (few-shot or meta) learning example to train the model. To be more specific, this 98
is the synchronous label setting of Mishra et al. [18] where the learning phase (observing examples, 99
(x1, y1)etc.) is separated from the prediction phase (predicting label ygiven (x,∅)). We opt for 100
this variant in our experiments as we empirically find this (at least in our specific settings) more 101
stable than the delayed label setting [ 14] where the model has to make a prediction for every input, 102
and the label is fed to the model with a delay of one time step. 103
2.3 Self-Referential Weight Matrices 104
Our method (Sec. 3) can be applied to any sequence-processing NN architectures in principle. 105
Nevertheless, certain architectures naturally fit better to parameterize a self-improving continual 106
learner. Here we use the modern self-referential weight matrix (SRWM; [ 19,60]) to build a generic 107
self-modifying NN. An SRWM is a weight matrix that sequentially modifies itself as a response 108
to a stream of input observations [ 12,61]. The modern SRWM belongs to the family of linear 109
Transformers (LTs) a.k.a. Fast Weight Programmers (FWPs; [ 62–68]). Linear Transformers and 110
FWPs are an important class of the now popular Transformers [ 59]: unlike the standard ones whose 111
computational requirements grow quadratically and whose state size grows linearly with the context 112
length, LTs/FWPs’ complexity is linear and the state size is constant w.r.t. sequence length (like 113
in the standard RNNs). This is an important property for in-context CL, since, conceptually, we 114
want such a CL system to continue to learn for an arbitrarily long, lifelong time span. Moreover, 115
the duality between linear attention and FWPs [ 67]—and likewise, between linear attention and 116
gradient descent-trained linear layers [ 69,70]—have played a key role in certain theoretical analyses 117
of in-context learning capabilities of Transformers [71, 72]. 118
The dynamics of an SRWM [ 19] are described as follows. Let din,dout,tbe positive integers, and ⊗ 119
denote outer product. At each time step t, an SRWM Wt−1∈R(dout+2∗din+1)×dinobserves an input 120
3xt∈Rdin, and outputs yt∈Rdout, while also updating itself to Wtas: 121
[yt,kt,qt, βt] =Wt−1xt (1)
vt=Wt−1ϕ(qt);¯vt=Wt−1ϕ(kt) (2)
Wt=Wt−1+σ(βt)(vt−¯vt)⊗ϕ(kt) (3)
where vt,¯vt∈R(dout+2∗din+1)are value vectors, qt∈Rdinandkt∈Rdinare query and key vectors, 122
andσ(βt)∈Ris the learning rate. σandϕdenote sigmoid and softmax functions respectively. ϕ 123
is typically also applied to xtin Eq. 1; here we follow Irie et al. [19]’s few-shot image classification 124
setting, and use the variant without it. Eq. 3 corresponds to a rank-one update of the SRWM, from 125
Wt−1toWt, through the delta learning rule [73,67] where the self-generated patterns, vt,ϕ(kt), 126
andσ(βt), play the role of target ,input , and learning rate of the learning rule respectively. The 127
delta rule is crucial for the performance of LTs [67, 68, 74, 75]. 128
The initial weight matrix W0is the only trainable parameters of this layer, that encodes the initial 129
self-modification algorithm. We use the layer above as a direct replacement to the self-attention layer 130
in the Transformer architecture [59]; and use the multi-head version of the computation above [19]. 131
3 Method 132
Task Formulation. We formulate continual learning as a long-span sequence learning task. Let 133
D,N,K,Ldenote positive integers. Consider two N-way classification tasks AandBto be 134
learned sequentially (as we’ll see, this can be straightforwardly extended to more tasks). The 135
formulation here applies to both “meta-training” and “meta-test” phases (see Appendix A.1 for more 136
on this terminology). We denote the respective training datasets as AandB, and test sets as A′137
andB′. We assume that each datapoint in these datasets consists of one input feature x∈RDof 138
dimension D(generically denoted as vector x, but it is an image in all our experiments) and one label 139
y∈ {1, ..., N}. We consider two sequences of Ltraining examples 
(xA
1, yA
1), ...,(xA
L, yA
L)
and 140 
(xB
1, yB
1), ...,(xB
L, yB
L)
sampled from the respective training sets AandB. In practice, L=NK 141
where Kis the number of training examples for each class. By concatenating these two sequences, 142
we obtain one long sequence representing CL examples to be presented as an input sequence to 143
a (left-to-right) auto-regressive model. At the end of the sequence, the model is tasked to make 144
predictions on test examples sampled from both A′andB′; we assume a single test example for 145
each task (hence, without index): (xA′, yA′)and(xB′, yB′)respectively; which we simply denote as 146
(xA
test, yA
test)and(xB
test, yB
test)instead. 147
Our model is a self-referential NN that modifies its own weight matrices as a function of input 148
observations. To simplify the notation, we denote the state of our self-referential NN as a 149
single SRWM W∗(even though it may have many of them in practice) where we’ll replace ∗ 150
by various symbols representing the context/inputs it has observed. Given a training sequence 151  
(xA
1, yA
1), ...,(xA
L, yA
L),(xB
1, yB
1), ...,(xB
L, yB
L)
, our model auto-regressively consumes one input 152
at a time, from left to right, in the auto-regressive fashion. Let WAdenote the state of the SRWM that 153
has consumed the first part of the sequence, i.e., the examples from Task A,(xA
1, yA
1), ...,(xA
L, yA
L), 154
and let WA,Bdenote the state of our SRWM having observed the entire sequence. 155
ACL Meta-Training Objectives. The ACL meta-training objective function tasks the model to 156
correctly predict the test examples of all tasks learned so far at each task boundaries. That is, in the 157
case of two-task scenario described above (learning Task Athen Task B), we use the weight matrix 158
WAto predict the label yA
testfrom input (xA
test,∅), and we use the weight matrix WA,Bto predict the 159
label yB
testfrom input (xB
test,∅)as well as the label yA
testfrom input (xA
test,∅). By letting p(y|x;W∗) 160
denote the model’s output probability for label y∈ {1, .., N}given input xand model weights/state 161
W∗, the ACL objective can be expressed as: 162
minimize
θ− 
log(p(yA
test|xA
test;WA)) + log( p(yB
test|xB
test;WA,B)) + log( p(yA
test|xA
test;WA,B))
(4)
for an arbitrary input meta-training sequence 
(xA
1, yA
1), ...,(xA
L, yA
L),(xB
1, yB
1), ...,(xB
L, yB
L)
163
(which is extensible to mini-batches with multiple such sequences), where θdenotes the model 164
parameters (for the SRWM layer, it is the initial weights W0). Figure 1 illustrates the overall 165
meta-training process of ACL. 166
4Table 1: 5-way classification accuracies using 15 (meta-test training) examples for each class in the
context. Each row is a single model. Bold numbers highlight cases where in-context catastrophic
forgetting is avoided through ACL.
Meta-Test Tasks: Context/Train (top) & Test (bottom)
Meta-Training Tasks A A →B B B →A
Task A Task B ACL A B A B A B
Omniglot Mini-ImageNet No 97.6 ±0.2 52.8 ±0.7 22.9 ±0.7 52.1 ±0.8 97.8 ±0.3 20.4 ±0.6
Yes 98.3 ±0.2 54.4 ±0.898.2±0.2 54.8 ±0.9 98.0 ±0.354.6±1.0
FC100 Mini-ImageNet No 49.7 ±0.7 55.0 ±1.0 21.3 ±0.7 55.1 ±0.6 49.9 ±0.8 21.7 ±0.8
Yes 53.8 ±1.7 52.5 ±1.246.2±1.3 59.9 ±0.7 45.5 ±0.953.0±0.6
Table 2: Similar to Table 1 above but using MNIST and CIFAR-10 (unseen domains) for meta-testing.
Meta-Test Tasks: Context/Train (top) & Test (bottom)
Meta-Training Tasks MNIST MNIST →CIFAR-10 CIFAR-10 CIFAR-10 →MNIST
Task A Task B ACL MNIST CIFAR-10 MNIST CIFAR-10 MNIST CIFAR-10
Omniglot Mini-ImageNet No 71.1 ±4.0 49.4 ±2.4 43.7 ±2.3 51.5 ±1.4 68.9 ±4.1 24.9 ±3.2
Yes 75.4 ±3.0 50.8 ±1.381.5±2.7 51.6 ±1.3 77.9 ±2.351.8±2.0
FC100 Mini-ImageNet No 60.1 ±2.0 56.1 ±2.3 17.2 ±3.5 54.4 ±1.7 58.6 ±1.6 21.2 ±3.1
Yes 70.0 ±2.4 51.0 ±1.068.2±2.7 59.2 ±1.7 66.9 ±3.452.5±1.3
The ACL objective function above (Eq. 4) is simple but encapsulates desiderata for continual learning 167
(Sec. 2.1). The last term of Eq. 4 with p(yA
test|xA
test;WA,B)or schematically p(A′|A,B), optimizes 168
forbackward transfer : (1) remembering the first task Aafter learning B(combatting catastrophic 169
forgetting), and (2) leveraging learning of Bto improve performance on the past task A. The 170
second term of Eq. 4, p(yB
test|xB
test;WA,B)or schematically p(B′|A,B), optimizes forward transfer 171
leveraging the past learning experience of Ato improve predictions in the second task B, in addition 172
to simply learning to solve Task Bfrom the corresponding training examples. To complete, the first 173
term of Eq. 4 is the single-task meta-learning objective for Task A. 174
Overall Model Architecture. As we mention in Sec. 2, in our NN architecture, the core sequential 175
dynamics of CL are learned by the self-referential layers. However, as an image-processing NN, our 176
model makes use of a vision backend. We use the “Conv-4” architecture [ 21] (typically used in the 177
context of few-shot learning) in all our experiments, except in the last one where we use a pre-trained 178
vision Transformer [ 76]. Overall, the model takes an image as input, process it through a feedforward 179
vision NN, whose output is fed to the SRWM-layer block. Note that this is one of the limitations of 180
this work: more general ACL should also learn to modify the vision components.2181
Another crucial architectural choice that is specific to continual/multi-task image processing is 182
normalization layers (see also Bronskill et al. [78]). Typical NNs used in few-shot learning (e.g., 183
Vinyals et al. [21]) contain batch normalization (BN; [ 79]) layers. All our models use instance 184
normalization (IN; [ 80]) instead of BN because in our preliminary experiments, we expectably found 185
IN to generalize much better than BN layers in the CL setting. 186
4 Experiments 187
4.1 Two-Task Setting: Comprehensible Study 188
We first reveal the problem of “in-context catastrophic forgetting” and show how our ACL method 189
(Sec. 3) can overcome it. As a minimum setting for this, we focus on the two-task “domain- 190
2One “straightforward” architecture fitting the bill is an MLP-mixer architecture (Tolstikhin et al. [77]; built
of several linear layers), where all linear layers are replaced by the self-referential linear layers of Sec. 2.3.
While we implemented such a model, it turned out to be too slow for us to conduct corresponding experiments.
Our public code will include a “self-referential MLP-mixer” implementation, but for further experiments, we
leave the future work on such an architecture using more efficient CUDA kernels.
50 1000 2000 3000 4000 5000 6000
Training Step0.00.20.40.60.81.01.21.41.6LossTask A Position 1
Task A Position 2
Task B Position 1
Task B Position 2
Task A ACL bwd
Task B ACL bwd(a) Case: Two tasks are learned simultaneously.
0 1000 2000 3000 4000 5000
Training Step0.00.51.01.52.0LossTask A Position 1
Task A Position 2
Task B Position 1
Task B Position 2
Task A ACL bwd
Task B ACL bwd (b) Case: One task is learned first (here Task A).
Figure 2: ACL/No -case meta-training curves displaying 6 individual meta-training loss terms, when
the last term of the ACL objective (the backward tranfer loss; “ Task A ACL bwd ” and “ Task B ACL
bwd” in the legend) is notminimized ( ACL/No case in Tables 1 and 2). Here Task A is Omniglot and
Task B is Mini-ImageNet. We observe that, in both cases, without explicit minimization, backward
transfer capability ( purple andbrown curves) of the learned learning algorithm gradually degrades
as it learns to learn a new task (all other colors), causing in-context catastrophic forgetting. Note
thatblue/orange andgreen/red curve pairs almost overlap; indicating that when a task is learned, the
model can learn it whether it is in the first or second segment of the continual learning sequence.
incremental” CL setting (see Appendix A.1). We consider two meta-training task combinations: 191
Omniglot [ 23] and Mini-ImageNet [ 21,22] or FC100 [ 24] (which is based on CIFAR100 [ 27]) and 192
Mini-ImageNet. The order of appearance of two tasks within meta-training sequences is alternated 193
for every batch. Appendix A.2 provides further details. We compare systems trained with or without 194
the backward transfer term in the ACL loss (the last term in Eq. 4). 195
Unless otherwise indicated (e.g, later for classic Split-MNIST; Sec. 4.3), all tasks are configured 196
to be a 5-way classification task. This is one of the classic configurations for few-shot learning tasks, 197
and also allows us to evaluate the principle of ACL with reasonable computational costs (like any 198
sequence learning-based meta-learning methods, scaling this to many more classes is challenging; we 199
also discuss this in Sec. 5). For standard datasets such as MNIST, we split the dataset into sub-datasets 200
of disjoint classes [ 81]: for example for MNIST which is originally a 10-way classification task, we 201
split it into two 5-way tasks, one consisting of images of class ‘0’ to ‘4’ (‘MNIST-04’), and another 202
one made of class ‘5’ to ‘9’ images (‘MNIST-59’). When we refer to a dataset without specifying 203
the class range, we refer to the first sub-set. Unless stated otherwise, we concatenate 15 examples 204
from each class for each task in the context for both meta-training and meta-testing (resulting in 205
sequences of length 75 for each task). All images are resized to 32×32-size 3-channel images, and 206
normalized according to the original dataset statistics. We refer to Appendix A for further details. 207
Table 1 shows the results when the models are meta-tested on the test sets of the corresponding 208
few-shot learning datasets used for meta-training. We observe that for both pairs of meta-training 209
tasks, the models without the ACL loss catastrophically forget the first task after learning the second 210
one: the accuracy on the first task is at the chance level of about 20% for 5-way classification after 211
learning the second task in-context (see rows with “ACL No”). The ACL loss clearly addresses this 212
problem: the ACL-learned CL algorithms preserve the performance of the first task. This effect is 213
particularly pronounced in the Omniglot/Mini-ImageNet case (involving two very different domains). 214
Table 2 shows evaluations of the same models but using two standard datasets, 5-way MNIST and 215
CIFAR-10, for meta-testing. Again, ACL-trained models better preserve the memory of the first 216
task after learning the second one. In the Omniglot/Mini-ImageNet case, we even observe certain 217
positive backward tranfer effects: in particular, in the “MNIST-then-CIFAR10” continual learning 218
case, the performance on MNIST noticeably improves after learning CIFAR10 (possibly leveraging 219
‘more data’ provided in-context). 220
4.2 Analysis: Emergence of In-Context Catastrophic Forgetting 221
Now we closely look at the emergence of “in-context catastrophic forgetting” during meta-training 222
for the baseline models trained without the backward transfer term (the last/third term in Eq. 4) in 223
6Table 3: Classification accuracies (%) on the Split-MNIST domain-incremental (DIL) and class-
incremental learning (CIL) settings [ 6]. Both tasks are 5-task CL problems. For the CIL case, we
also report the 2-task case for which we can directly evaluate our out-of-the-box ACL meta-learner
of Sec. 4.1 (trained with a 5-way output and the 2-task ACL loss) which, however, is not applicable
(N.A.) to the 5-task CIL requiring a 10-way output. Mean/std over 10 training/meta-testing runs. No
method here requires replay memory . See Appendix A.7 & B for further details and discussions.
Domain Incremental Class Incremental
Method 5-task 2-task 5-task
Plain Stochastic Gradient Descent (SGD) 63.2 ±0.4 48.8 ±0.1 19.5 ±0.1
Adam 55.2 ±1.4 49.7 ±0.1 19.7 ±0.1
Adam + L2 66.0 ±3.7 51.8 ±1.9 22.5 ±1.1
Elastic Weight Consolidation (EWC) 58.9 ±2.6 49.7 ±0.1 19.8 ±0.1
Online EWC 57.3 ±1.4 49.7 ±0.1 19.8 ±0.1
Synaptic Intelligence (SI) 64.8 ±3.1 49.4 ±0.2 19.7 ±0.1
Memory Aware Synapses (MAS) 68.6 ±6.9 49.6 ±0.1 19.5 ±0.3
Learning w/o Forgetting (LwF) 71.0 ±1.3 - 24.2 ±0.3
Online-aware Meta Learning (OML) 69.9 ±2.8 46.6 ±7.2 24.9 ±4.1
+ optimized # meta-testing iterations 73.6 ±5.3 62.1 ±7.9 34.2 ±4.6
Generative Meta-Continual Learning (GeMCL) 63.8 ±3.8 91.2 ±2.8 79.0 ±2.1
ACL (Out-of-the-box, DIL, 2-task ACL model; Sec. 4.1) 72.2 ±0.9 71.5 ±5.9 N.A.
+ meta-finetuned with 5-task ACL loss, Omniglot 84.5±1.6 96.0±1.084.3±1.2
the ACL objective loss (corresponding to the ACL/No cases in Tables 1 and 2). We focus on the 224
Omniglot/Mini-ImageNet case, but similar trends can also be observed in the FC100/Mini-ImageNet 225
case. Figures 2a and 2b show two representative cases we typically observe. These figures show an 226
evolution of six individual meta-training loss terms (the lower the better), reported separately for 227
the cases where Task A (here Omniglot) or Task B (here Mini-ImageNet) appears at the first (1) or 228
second (2) position in the 2-task CL meta-training training sequences. 4 out of 6 curves correspond to 229
the learning progress, showing whether the model becomes capable of in-context learning the given 230
task (A or B) at the given position (1 or 2). The 2 remaining curves are the ACL backward tranfer 231
losses, also measured for Task A and B separately here. 232
Figure 2a shows the case where two tasks are learned about at the same time. We observe that when 233
the learning curves go down, the ACL losses go up, indicating that more the model learns, more it 234
tends to forget the task in-context learned previously. We also find this same trend when one task 235
is learned before the other one as is the case in Figure 2b. Here Task A alone is learned first; while 236
Task B is not learned, both learning and ACL curves go down for Task A (essentially, as the model 237
does not learn the second task, there is no force that encourages forgetting). After around 3000 steps, 238
the model also starts learning Task B. From this point, the ACL loss for Task A also starts to go 239
up, indicating again an opposing force effect between learning a new task and remembering a past 240
task. These observations clearly indicate that, without explicitly taking into account the backward 241
transfer loss as part of learning objectives, our gradient descent search tends to find solutions/CL 242
algorithms that prefer to erase previously learned knowledge (this is rather intuitive; it seems easier to 243
find such algorithms that ignore any influence of the current learning to past learning than those that 244
also preserve prior knowledge). In all cases, we find our ACL objective to be crucial for the learned 245
CL algorithms to be capable of remembering the old task while also learning the new one. 246
4.3 General Evaluation 247
Evaluation on Standard Split-MNIST. Here we evaluate ACL on the standard Split-MNIST task in 248
domain-incremental and class-incremental settings [ 6,28], and compare its performance to existing 249
CL and meta-CL algorithms (see Appendix A.7 for full references of these methods). Our comparison 250
focuses on methods that do not require replay memory. Table 3 shows the results. Since our 251
ACL-trained models are general-purpose learners, they can be directly evaluated (meta-tested) on 252
a new task, here Split-MNIST. The second-to-last row of Table 3, “ACL (Out-of-the-box model)”, 253
corresponds to our model from Sec. 4.1 meta-trained on Omniglot and Mini-ImageNet using the 254
2-task ACL objective. It performs very competitively with the best existing methods in the domain- 255
7incremental setting, while it largely outperforms them (all but another meta-CL method, GeMCL) in 256
the 2-task class-incremental setting. The same model can be further meta-finetuned using the 5-task 257
version of the ACL loss (here we only used Omniglot as the meta-training data). The resulting model 258
(the last row of Table 3) outperforms all other methods in all settings studied here. Note that on 259
the ‘in-domain’ Omniglot test set, ACL and GeMCL perform similarly (see Appendix B.2/Table 9). 260
We are not aware of any existing hand-crafted CL algorithms that can achieve ACL’s performance 261
without any replay memory. We refer to Appendix A.7/B for further discussions and ablation studies. 262
Evaluation on diverse task domains. Using the setting of Sec. 4.1, we also evaluate our ACL-trained 263
models for CL involving more tasks/domains; using meta-test sequences made of MNIST, CIFAR-10, 264
and Fashion MNIST. We also evaluate the impact of the number of tasks in the ACL objective: in 265
addition to the model meta-trained on Omniglot/Mini-ImageNet (Sec. 4.1), we also meta-train a model 266
(with the same architecture and hyper-parameters) using 3 tasks, Omniglot, Mini-ImageNet, and 267
FC100, using the 3-task ACL objective (see Appendix A.5); which is meta-trained not only on longer 268
CL sequences but also on more data. The full results of this experiment can be found in Appendix 269
B.4. We find that the two ACL-trained models are indeed capable of retaining the knowledge without 270
catastrophic forgetting for multiple tasks during meta-testing, while the performance on prior tasks 271
gradually degrades as the model learns new tasks, and performance on new tasks becomes moderate 272
(see also Sec. 5 on limitations). The 3-task version outperforms the 2-task one overall, encouragingly 273
indicating a potential for further improvements even with a fixed parameter count. 274
Going beyond: limitations and outlook. The experiments presented above effectively demonstrate 275
the possibility to encode a continual learning algorithm into self-referential weight matrices, that 276
outperforms handcrafted learning algorithms and existing metalearning approaches for CL. While 277
we consider this as an important result for metalearning and in-context learning in general, we note 278
that current state-of-the-art CL methods use neither regularization-based CL algorithms nor meta- 279
continual learning methods we mention above, but the so-called learning to prompt (L2P)-family 280
of methods [ 33,34] that leverage pre-trained models, namely a vision Transformer (ViT) pre-trained 281
on ImageNet [ 76]. A natural question we should ask is whether we could foresee ACL beyond the 282
scope considered so far, and evaluate it in such a setting. To study this, we take a pre-trained (frozen) 283
vision model, and add self-referential layers (to be meta-trained from scratch) on top of it to build a 284
continual learner. This allows us to highlight an important challenge of in-context CL in what follows. 285
We use two tasks from the L2P works above [ 33,34]: 5-datasets [ 32] and Split-CIFAR-100, in the 286
class-incremental setting, but we focus on a “ mini” versions thereof: we only use the two first classes 287
within each task (i.e., 2-way version) and for Split-CIFAR100, we only use the 5 first tasks; as we’ll 288
see, this setting is enough to illustrate an important limitation of in-context CL. Again following 289
L2P [ 33,34], we use ViT-B/16 [ 76] (available via PyTorch) as the pre-trained vision model, which 290
we keep frozen. We use the same configuration for the self-referential component from the Split- 291
MNIST experiment. We meta-train the resulting model using Mini-ImageNet and Omniglot with the 292
5-task ACL loss. Table 4 shows the results. Even in this simple “mini” version of the tasks, ACL’s 293
performance is far behind that of L2P methods. Notably, the frozen ImageNet-pre-trained features 294
with the meta-learner trained on Mini-ImageNet and Omniglot are not enough to perform well on the 295
5-th task of Split-CIFAR100, and SVHN and notMNIST of 5-datasets. This shows the necessity to 296
meta-train on more diverse tasks for in-context CL to be possibly successful in more general settings. 297
Table 4: Experiments with “ mini” Split-CIFAR100 and 5-datasets tasks. Meta-training is done using
Mini-ImageNet andOmniglot . All meta-evaluation images are therefore from unseen domains.
Numbers marked with * are reference numbers (evaluated in the more challenging, original version
of these tasks) which can not be directly compared to ours.
Split-CIFAR100 5-datasets
L2P [34] 83.9*±0.3 81.1*±0.9
DualPrompt [34] 86.5*±0.3 88.1*±0.4
ACL (Individual Task) Task 1 95.9 ±0.9 CIFAR10 91.3 ±1.2
Task 2 85.6 ±3.6 MNIST 98.9 ±0.3
Task 3 93.4 ±1.4 Fashion 93.5 ±2.0
Task 4 97.0 ±0.7 SVHN 66.1 ±9.4
Task 5 67.6 ±7.0 notMNIST 76.3 ±6.7
ACL 68.3 ±2.0 61.5 ±2.1
85 Discussion 298
Other Limitations. In addition to the limitations already mentioned above, here we discuss others. 299
First of all, as an in-context/learned learning algorithm, there are challenges in terms of both domain 300
and length generalization (we qualitatively observe these to some extent in Sec. 4; further discussion 301
and experimental results are presented in Appendix B.3 & B.5). Regarding the length generalization, 302
we note that unlike the standard “quadratic" Transformers, linear Transformers/FWPs-like SRWMs 303
can be trained by carrying over states across two consecutive batches for arbitrarily long sequences. 304
Such an approach has been successfully applied to language modeling with FWPs [ 67]. This 305
possibility, however, has not been investigated here, and is left for future work. Also, directly scaling 306
ACL for real-world tasks requiring many more classes does not seem straightforward: it would 307
require very long training sequences. That said, it may be possible that ACL could be achieved 308
without exactly following the process we propose; as we discuss below for the case of LLMs, certain 309
real-world data may naturally give rise to an ACL-like objective. This work is also limited to the 310
task of image classification, which can be solved by feedforward NNs. Future work may investigate 311
the possibility to extend ACL to continual learning of sequence learning tasks, such as continually 312
learning new languages. Finally, ACL learns CL algorithms that are specific to the pre-specified 313
model architecture; more general meta-learning algorithms may aim at achieving learning algorithms 314
that are applicable to any model, as is the case for many classic learning algorithms. 315
Related work. There are several recent works that are catagorized as ‘meta-continual learning’ or 316
‘continual meta-learning’ (see, e.g., [ 29,30,82–84,51]). For example, Javed and White [29], Beaulieu 317
et al. [30] use “model-agnostic meta-learning” (MAML; [ 85,86]) to meta-learn representations for 318
CL while still making use of classic learning algorithms for CL; this requires tuning of the learning 319
rate and number of iterations for optimal performance during CL at meta-test time (see, e.g., Appendix 320
A.7). In contrast, our approach learn learning algorithms in the spirit of Hochreiter et al. [14], Younger 321
et al. [15]; this may be categorized as ‘in-context continual learning.’ Several recent works (see, e.g., 322
[87,88]) mention the possibility of such in-context CL but existing works [ 19,89,90] that learn mul- 323
tiple tasks sequentially in-context do not focus on catastrophic forgetting which is one of the central 324
challenges of CL. Here we show that in-context learning also suffers from catastrophic forgetting in 325
general (Sec. 4.1-4.2) and propose ACL to address this problem. We also note that the use of SRWM is 326
relevant to ‘continual meta-learning’ since with a regular sequence processor with slow weights, there 327
remains the question of how to continually learn the slow weights (meta-parameters). In principle, re- 328
cursive self-modification as in SRWM is an answer to this question as it collapses such meta-levels into 329
single self-reference [12]. We also refer to [91–93] for other prior work on meta-continual learning. 330
Artificial v. Natural ACL in Large Language Models? Recently, “on-the-fly” few-shot/meta 331
learning capability of sequence processing NNs has attracted broader interests in the context of large 332
language models (LLMs; [94]). In fact, the task of language modeling itself has a form of sequence 333
processing with error feedback (essential for meta-learning [ 95]): the correct label to be predicted is 334
fed to the model with a delay of one time step in an auto-regressive manner. Trained on a large amount 335
of text covering a wide variety of credit assignment paths, LLMs exhibit certain sequential few-shot 336
learning capabilities in practice [ 96]. This was rebranded as in-context learning , and has been the 337
subject of numerous recent studies (e.g., [ 97–103,71,72]). Here we explicitly/artificially construct 338
ACL meta-training sequences and objectives, but in modern LLMs trained on a large amount of data 339
mixing a large diversity of dependencies using a large backpropagation span, it is conceivable that 340
some ACL-like objectives may naturally appear in the data. 341
6 Conclusion 342
Our Automated Continual Learning (ACL) trains sequence-processing self-referential neural networks 343
(SRNNs) to learn their own in-context continual (meta-)learning algorithms. ACL encodes classic 344
desiderata for continual learning (e.g., forward and backward transfer) into the objective function of 345
the meta-learner. ACL uses gradient descent to deal with classic challenges of CL, to automatically 346
discover CL algorithms with good behavior. Once trained, our SRNNs autonomously run their 347
own CL algorithms without requiring any human intervention. Our experiments reveal the original 348
problem of in-context catastrophic forgetting, and demonstrate the effectiveness of the proposed 349
approach to combat it. We demonstrate very promising results on the classic Split-MNIST benchmark 350
where existing hand-crafted algorithms fail, while also discussing its limitations in more general 351
scenarios. We believe this comprehensive study to be an important step for in-context CL research. 352
9References 353
[1] David Eagleman. Livewired: The inside story of the ever-changing brain . 2020. 354
[2]Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: 355
The sequential learning problem. In Psychology of learning and motivation , volume 24, pages 356
109–165. 1989. 357
[3]Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning 358
and forgetting functions. Psychological review , 97(2):285, 1990. 359
[4]Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive 360
sciences , 3(4):128–135, 1999. 361
[5] James L McClelland, Bruce L McNaughton, and Randall C O’Reilly. Why there are comple- 362
mentary learning systems in the hippocampus and neocortex: insights from the successes and 363
failures of connectionist models of learning and memory. Psychological review , 102(3):419, 364
1995. 365
[6]Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual 366
learning scenarios: A categorization and case for strong baselines. In NeurIPS Workshop on 367
Continual Learning , Montréal, Canada, December 2018. 368
[7]Chris A Kortge. Episodic memory in connectionist networks. In 12th Annual Conference. CSS 369
Pod, pages 764–771, 1990. 370
[8]Robert M French. Using semi-distributed representations to overcome catastrophic forgetting 371
in connectionist networks. In Proc. Cognitive science society conference , volume 1, pages 372
173–178, 1991. 373
[9]James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, 374
Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, 375
et al. Overcoming catastrophic forgetting in neural networks. Proc. National academy of 376
sciences , 114(13):3521–3526, 2017. 377
[10] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, 378
Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame- 379
work for continual learning. In Proc. Int. Conf. on Machine Learning (ICML) , pages 4535– 380
4544, Stockholm, Sweden, July 2018. 381
[11] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic 382
intelligence. In Proc. Int. Conf. on Machine Learning (ICML) , pages 3987–3995, Sydney, 383
Australia, August 2017. 384
[12] Jürgen Schmidhuber. Steps towards “self-referential” learning. Technical Report CU-CS-627- 385
92, Dept. of Comp. Sci., University of Colorado at Boulder, November 1992. 386
[13] Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how 387
to learn: the meta-meta-... hook . PhD thesis, Technische Universität München, 1987. 388
[14] Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient 389
descent. In Proc. Int. Conf. on Artificial Neural Networks (ICANN) , volume 2130, pages 390
87–94, Vienna, Austria, August 2001. 391
[15] A Steven Younger, Peter R Conwell, and Neil E Cotter. Fixed-weight on-line learning. IEEE 392
Transactions on Neural Networks , 10(2):272–283, 1999. 393
[16] Neil E Cotter and Peter R Conwell. Learning algorithms and fixed dynamics. In Proc. Int. 394
Joint Conf. on Neural Networks (IJCNN) , pages 799–801, Seattle, WA, USA, July 1991. 395
[17] Neil E Cotter and Peter R Conwell. Fixed-weight networks can learn. In Proc. Int. Joint Conf. 396
on Neural Networks (IJCNN) , pages 553–559, San Diego, CA, USA, June 1990. 397
[18] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive 398
meta-learner. In Int. Conf. on Learning Representations (ICLR) , Vancouver, Cananda, 2018. 399
10[19] Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. A modern self- 400
referential weight matrix that learns to modify itself. In Proc. Int. Conf. on Machine Learning 401
(ICML) , pages 9660–9677, Baltimore, MA, USA, July 2022. 402
[20] Stephen T Grossberg. Studies of mind and brain: Neural principles of learning, perception, 403
development, cognition, and motor control . Springer, 1982. 404
[21] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. 405
Matching networks for one shot learning. In Proc. Advances in Neural Information Processing 406
Systems (NIPS) , pages 3630–3638, Barcelona, Spain, December 2016. 407
[22] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Int. Conf. 408
on Learning Representations (ICLR) , Toulon, France, April 2017. 409
[23] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept 410
learning through probabilistic program induction. Science , 350(6266):1332–1338, 2015. 411
[24] Boris N. Oreshkin, Pau Rodríguez López, and Alexandre Lacoste. TADAM: task dependent 412
adaptive metric for improved few-shot learning. In Proc. Advances in Neural Information 413
Processing Systems (NeurIPS) , pages 719–729, Montréal, Canada, December 2018. 414
[25] Yann LeCun, Corinna Cortes, and Christopher JC Burges. The MNIST database of handwritten 415
digits. URL http://yann. lecun. com/exdb/mnist, 1998. 416
[26] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-MNIST: a novel image dataset for 417
benchmarking machine learning algorithms. Preprint arXiv:1708.07747 , 2017. 418
[27] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, 419
Computer Science Department, University of Toronto, 2009. 420
[28] Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. In NeurIPS 421
Workshop on Continual Learning , Montréal, Canada, December 2018. 422
[29] Khurram Javed and Martha White. Meta-learning representations for continual learning. 423
InProc. Advances in Neural Information Processing Systems (NeurIPS) , pages 1818–1828, 424
Vancouver, BC, Canada, December 2019. 425
[30] Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, Jeff Clune, 426
and Nick Cheney. Learning to continually learn. In Proc. European Conf. on Artificial 427
Intelligence (ECAI) , pages 992–1001, August 2020. 428
[31] Mohammadamin Banayeeanzade, Rasoul Mirzaiezadeh, Hosein Hasani, and Mahdieh So- 429
leymani. Generative vs. discriminative: Rethinking the meta-continual learning. In Proc. 430
Advances in Neural Information Processing Systems (NeurIPS) , pages 21592–21604, Virtual 431
only, December 2021. 432
[32] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach. 433
Adversarial continual learning. In Proc. European Conf. on Computer Vision (ECCV) , pages 434
386–402, Glasgow, UK, August 2020. 435
[33] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, 436
Vincent Perot, Jennifer G. Dy, and Tomas Pfister. Learning to prompt for continual learning. 437
InProc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 139–149, 438
New Orleans, LA, USA, June 2022. 439
[34] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi 440
Ren, Guolong Su, Vincent Perot, Jennifer G. Dy, and Tomas Pfister. Dualprompt: Complemen- 441
tary prompting for rehearsal-free continual learning. In Proc. European Conf. on Computer 442
Vision (ECCV) , pages 631–648, Tel Aviv, Israel, October 2022. 443
[35] Sebastian Thrun. Lifelong learning algorithms. In Learning to learn , pages 181–209. 1998. 444
[36] Rich Caruana. Multitask learning. Machine learning , 28:41–75, 1997. 445
11[37] Mark B. Ring. Continual Learning in Reinforcement Environments . PhD thesis, University of 446
Texas at Austin, Austin, TX, USA, 1994. 447
[38] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, 448
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. Preprint 449
arXiv:1606.04671 , 2016. 450
[39] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science , 451
7(2):123–146, 1995. 452
[40] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep 453
generative replay. In Proc. Advances in Neural Information Processing Systems (NIPS) , pages 454
2990–2999, Long Beach, CA, USA, December 2017. 455
[41] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Gregory Wayne. 456
Experience replay for continual learning. In Proc. Advances in Neural Information Processing 457
Systems (NeurIPS) , pages 348–358, Vancouver, Canada, December 2019. 458
[42] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and 459
Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing 460
interference. In Int. Conf. on Learning Representations (ICLR) , New Orleans, LA, USA, May 461
2019. 462
[43] Yaqian Zhang, Bernhard Pfahringer, Eibe Frank, Albert Bifet, Nick Jin Sean Lim, and Yunzhe 463
Jia. A simple but strong baseline for online continual learning: Repeated augmented rehearsal. 464
InProc. Advances in Neural Information Processing Systems (NeurIPS) , New Orleans, LA, 465
USA, December 2022. 466
[44] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. 467
InProc. Advances in Neural Information Processing Systems (NIPS) , pages 6467–6476, Long 468
Beach, CA, USA, December 2017. 469
[45] Tom Veniat, Ludovic Denoyer, and Marc’Aurelio Ranzato. Efficient continual learning with 470
modular networks and task-driven priors. In Int. Conf. on Learning Representations (ICLR) , 471
Virtual only, May 2021. 472
[46] Devang K Naik and Richard J Mammone. Meta-neural networks that learn by learning. In 473
Proc. International Joint Conference on Neural Networks (IJCNN) , volume 1, pages 437–442, 474
Baltimore, MD, USA, June 1992. 475
[47] Tom Bosc. Learning to learn neural networks. In NIPS Workshop on Reasoning, Attention, 476
Memory , Montreal, Canada, December 2015. 477
[48] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy P. Lillicrap. 478
Meta-learning with memory-augmented neural networks. In Proc. Int. Conf. on Machine 479
Learning (ICML) , pages 1842–1850, New York City, NY, USA, June 2016. 480
[49] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: 481
Fast reinforcement learning via slow reinforcement learning. Preprint arXiv:1611.02779 , 482
2016. 483
[50] Jane Wang, Zeb Kurth-Nelson, Hubert Soyer, Joel Z. Leibo, Dhruva Tirumala, Rémi Munos, 484
Charles Blundell, Dharshan Kumaran, and Matt M. Botvinick. Learning to reinforcement 485
learn. In Proc. Annual Meeting of the Cognitive Science Society (CogSci) , London, UK, July 486
2017. 487
[51] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In Proc. Int. Conf. on Machine 488
Learning (ICML) , pages 2554–2563, Sydney, Australia, August 2017. 489
[52] Tsendsuren Munkhdalai and Adam Trischler. Metalearning with Hebbian fast weights. Preprint 490
arXiv:1807.05076 , 2018. 491
[53] Thomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic 492
neural networks with backpropagation. In Proc. Int. Conf. on Machine Learning (ICML) , 493
pages 3559–3568, Stockholm, Sweden, July 2018. 494
12[54] Thomas Miconi, Aditya Rawal, Jeff Clune, and Kenneth O. Stanley. Backpropamine: training 495
self-modifying neural networks with differentiable neuromodulated plasticity. In Int. Conf. on 496
Learning Representations (ICLR) , New Orleans, LA, USA, May 2019. 497
[55] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned 498
neural memory. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , 499
pages 13310–13321, Vancouver, Canada, December 2019. 500
[56] Louis Kirsch and Jürgen Schmidhuber. Meta learning backpropagation and improving it. In 501
Proc. Advances in Neural Information Processing Systems (NeurIPS) , pages 14122–14134, 502
Virtual only, December 2021. 503
[57] Mark Sandler, Max Vladymyrov, Andrey Zhmoginov, Nolan Miller, Tom Madams, Andrew 504
Jackson, and Blaise Agüera y Arcas. Meta-learning bidirectional update rules. In Proc. Int. 505
Conf. on Machine Learning (ICML) , pages 9288–9300, Virtual only, July 2021. 506
[58] Mike Huisman, Thomas M Moerland, Aske Plaat, and Jan N van Rijn. Are LSTMs good 507
few-shot learners? Machine Learning , pages 1–28, 2023. 508
[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, 509
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Advances in Neural 510
Information Processing Systems (NIPS) , pages 5998–6008, Long Beach, CA, USA, December 511
2017. 512
[60] Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber. Practical computational power of linear 513
transformers and their recurrent and self-referential extensions. In Proc. Conf. on Empirical 514
Methods in Natural Language Processing (EMNLP) , Sentosa, Singapore, 2023. 515
[61] Jürgen Schmidhuber. A self-referential weight matrix. In Proc. Int. Conf. on Artificial Neural 516
Networks (ICANN) , pages 446–451, Amsterdam, Netherlands, September 1993. 517
[62] Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent 518
nets. Technical Report FKI-147-91, Institut für Informatik, Technische Universität München, 519
March 1991. 520
[63] Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic 521
recurrent networks. Neural Computation , 4(1):131–139, 1992. 522
[64] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers 523
are RNNs: Fast autoregressive transformers with linear attention. In Proc. Int. Conf. on 524
Machine Learning (ICML) , Virtual only, July 2020. 525
[65] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, 526
Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking 527
attention with performers. In Int. Conf. on Learning Representations (ICLR) , Virtual only, 528
2021. 529
[66] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng 530
Kong. Random feature attention. In Int. Conf. on Learning Representations (ICLR) , Virtual 531
only, 2021. 532
[67] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear Transformers are secretly fast 533
weight programmers. In Proc. Int. Conf. on Machine Learning (ICML) , Virtual only, July 534
2021. 535
[68] Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. Going beyond linear 536
transformers with recurrent fast weight programmers. In Proc. Advances in Neural Information 537
Processing Systems (NeurIPS) , Virtual only, December 2021. 538
[69] Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber. The dual form of neural networks 539
revisited: Connecting test time predictions to training patterns via spotlights of attention. In 540
Proc. Int. Conf. on Machine Learning (ICML) , Baltimore, MD, USA, July 2022. 541
13[70] Mark A. Aizerman, Emmanuil M. Braverman, and Lev I. Rozonoer. Theoretical foundations 542
of potential function method in pattern recognition. Automation and Remote Control , 25(6): 543
917–936, 1964. 544
[71] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander 545
Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by 546
gradient descent. In Proc. Int. Conf. on Machine Learning (ICML) , Honolulu, HI, USA, July 547
2023. 548
[72] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can 549
GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. 550
InProc. Findings Association for Computational Linguistics (ACL) , pages 4005–4019, Toronto, 551
Canada, July 2023. 552
[73] Bernard Widrow and Marcian E Hoff. Adaptive switching circuits. In Proc. IRE WESCON 553
Convention Record , pages 96–104, Los Angeles, CA, USA, August 1960. 554
[74] Kazuki Irie, Francesco Faccio, and Jürgen Schmidhuber. Neural differential equations for 555
learning to program neural nets through continuous learning rules. In Proc. Advances in Neural 556
Information Processing Systems (NeurIPS) , New Orleans, LA, USA, December 2022. 557
[75] Kazuki Irie and Jürgen Schmidhuber. Images as weight matrices: Sequential image generation 558
through synaptic learning rules. In Int. Conf. on Learning Representations (ICLR) , Kigali, 559
Rwanda, May 2023. 560
[76] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, 561
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, 562
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image 563
recognition at scale. In Int. Conf. on Learning Representations (ICLR) , Virtual only, May 564
2021. 565
[77] Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas 566
Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, 567
and Alexey Dosovitskiy. MLP-Mixer: An all-MLP architecture for vision. In Proc. Advances 568
in Neural Information Processing Systems (NeurIPS) , pages 24261–24272, Virtual only, 569
December 2021. 570
[78] John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard E. Turner. 571
TaskNorm: Rethinking batch normalization for meta-learning. In Proc. Int. Conf. on Machine 572
Learning (ICML) , pages 1153–1164, Virtual only, 2020. 573
[79] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training 574
by reducing internal covariate shift. In Proc. Int. Conf. on Machine Learning (ICML) , pages 575
448–456, Lille, France, July 2015. 576
[80] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing 577
ingredient for fast stylization. Preprint arXiv:1607.08022 , 2016. 578
[81] Rupesh Kumar Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino J. Gomez, and Jür- 579
gen Schmidhuber. Compete to compute. In Proc. Advances in Neural Information Processing 580
Systems (NIPS) , pages 2310–2318, Lake Tahoe, NV , USA, December 2013. 581
[82] Massimo Caccia, Pau Rodríguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas 582
Page-Caccia, Issam Hadj Laradji, Irina Rish, Alexandre Lacoste, David Vázquez, and Laurent 583
Charlin. Online fast adaptation and knowledge accumulation (OSAKA): a new approach to 584
continual learning. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , 585
Virtual only, December 2020. 586
[83] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh, and Razvan 587
Pascanu. Task agnostic continual learning via meta learning. Preprint arXiv:1906.05201 , 588
2019. 589
14[84] Pau Ching Yap, Hippolyt Ritter, and David Barber. Addressing catastrophic forgetting in 590
few-shot problems. In Proc. Int. Conf. on Machine Learning (ICML) , pages 11909–11919, 591
Virtual only, July 2021. 592
[85] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast 593
adaptation of deep networks. In Proc. Int. Conf. on Machine Learning (ICML) , pages 1126– 594
1135, Sydney, Australia, August 2017. 595
[86] Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations 596
and gradient descent can approximate any learning algorithm. In Int. Conf. on Learning 597
Representations (ICLR) , Vancouver, Canada, April 2018. 598
[87] Kazuki Irie and Jürgen Schmidhuber. Accelerating neural self-improvement via bootstrapping. 599
InICLR Workshop on Mathematical and Empirical Understanding of Foundation Models , 600
Kigali, Rwanda, May 2023. 601
[88] Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas 602
Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. 603
Uncovering mesa-optimization algorithms in Transformers. Preprint arXiv:2309.05858 , 2023. 604
[89] Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang, and Eric 605
Schulz. Meta-in-context learning in large language models. Preprint arXiv:2305.12907 , 2023. 606
[90] Soochan Lee, Jaehyeon Son, and Gunhee Kim. Recasting continual learning as sequence 607
modeling. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , New 608
Orleans, LA, USA, December 2023. 609
[91] Jürgen Schmidhuber. On learning how to learn learning strategies. Technical Report FKI-198- 610
94, Institut für Informatik, Technische Universität München, November 1994. 611
[92] Jürgen Schmidhuber. Beyond “genetic programming": Incremental self-improvement. In Proc. 612
Workshop on Genetic Programming at ML95 , pages 42–49, 1995. 613
[93] Jürgen Schmidhuber, Jieyu Zhao, and Marco Wiering. Shifting inductive bias with success- 614
story algorithm, adaptive Levin search, and incremental self-improvement. Machine Learning , 615
28(1):105–130, 1997. 616
[94] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Lan- 617
guage models are unsupervised multitask learners. [Online]. : https://blog.openai.com/better- 618
language-models/, 2019. 619
[95] Jürgen Schmidhuber. Making the world differentiable: On using fully recurrent self-supervised 620
neural networks for dynamic reinforcement learning and planning in non-stationary environ- 621
ments. Institut für Informatik, Technische Universität München. Technical Report FKI-126 , 622
90, 1990. 623
[96] Tom B Brown et al. Language models are few-shot learners. In Proc. Advances in Neural 624
Information Processing Systems (NeurIPS) , Virtual only, December 2020. 625
[97] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of 626
in-context learning as implicit bayesian inference. In Int. Conf. on Learning Representations 627
(ICLR) , Virtual only, April 2022. 628
[98] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and 629
Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning 630
work? In Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP) , pages 631
11048–11064, Abu Dhabi, UAE, December 2022. 632
[99] Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo 633
Lee, Sang-goo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input- 634
label demonstrations. In Proc. Conf. on Empirical Methods in Natural Language Processing 635
(EMNLP) , pages 2422–2437, Abu Dhabi, UAE, December 2022. 636
15[100] Stephanie CY Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K Singh, 637
Pierre Harvey Richemond, James McClelland, and Felix Hill. Data distributional properties 638
drive emergent in-context learning in transformers. In Proc. Advances in Neural Information 639
Processing Systems (NeurIPS) , New Orleans, LA, USA, November 2022. 640
[101] Stephanie CY Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K 641
Lampinen, and Felix Hill. Transformers generalize differently from information stored in 642
context vs in weights. In NeurIPS Workshop on Memory in Artificial and Real Intelligence 643
(MemARI) , New Orleans, LA, USA, November 2022. 644
[102] Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in- 645
context learning by meta-learning transformers. In NeurIPS Workshop on Memory in Artificial 646
and Real Intelligence (MemARI) , New Orleans, LA, USA, November 2022. 647
[103] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning 648
algorithm is in-context learning? investigations with linear models. In Int. Conf. on Learning 649
Representations (ICLR) , Kigali, Rwanda, May 2023. 650
[104] Gido M Van de Ven and Andreas S Tolias. Generative replay with feedback connections as a 651
general strategy for continual learning. Preprint arXiv:1809.10635 , 2018. 652
[105] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. 653
Reading digits in natural images with unsupervised feature learning. In NIPS workshop on 654
deep learning and unsupervised feature learning , Granada, Spain, December 2011. 655
[106] Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available: 656
http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html , 2011. 657
[107] Tristan Deleu, Tobias Würfl, Mandana Samiei, Joseph Paul Cohen, and Yoshua Bengio. 658
Torchmeta: A meta-learning library for PyTorch. Preprint arXiv:1909.06576 , 2019. 659
[108] Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical 660
analysis. Cognition , 28(1-2):3–71, 1988. 661
[109] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte- 662
laars. Memory aware synapses: Learning what (not) to forget. In Proc. European Conf. on 663
Computer Vision (ECCV) , pages 144–161, Munich, Germany, September 2018. 664
[110] Zhizhong Li and Derek Hoiem. Learning without forgetting. In Proc. European Conf. on 665
Computer Vision (ECCV) , pages 614–629, Amsterdam, Netherlands, October 2016. 666
[111] Adam Paszke et al. Pytorch: An imperative style, high-performance deep learning library. 667
InProc. Advances in Neural Information Processing Systems (NeurIPS) , pages 8026–8037, 668
Vancouver, Canada, December 2019. 669
[112] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The devil is in the detail: Simple tricks 670
improve systematic generalization of transformers. In Proc. Conf. on Empirical Methods in 671
Natural Language Processing (EMNLP) , Punta Cana, Dominican Republic, November 2021. 672
[113] Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. Improving baselines in 673
the wild. In Workshop on Distribution Shifts, NeurIPS , Virtual only, 2021. 674
[114] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E. Turner. 675
Fast and flexible multi-task classification using conditional neural adaptive processes. In Proc. 676
Advances in Neural Information Processing Systems (NeurIPS) , pages 7957–7968, Vancouver, 677
Canada, December 2019. 678
[115] Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross 679
Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. 680
Meta-dataset: A dataset of datasets for learning to learn from few examples. In Int. Conf. on 681
Learning Representations (ICLR) , Addis Ababa, Ethiopia, April 2020. 682
[116] Jürgen Schmidhuber. One big net for everything. Preprint arXiv:1802.08864 , 2018. 683
16[117] Alex Graves, Marc G. Bellemare, Jacob Menick, Rémi Munos, and Koray Kavukcuoglu. 684
Automated curriculum learning for neural networks. In Proc. Int. Conf. on Machine Learning 685
(ICML) , pages 1311–1320, Sydney, Australia, August 2017. 686
A Experimental Details 687
A.1 Continual and Meta-learning Terminologies 688
We review the following classic terminologies of continual learning and meta-learning used through- 689
out this paper. 690
Continual learning. “Domain-incremental learning (DIL)” and “class-incremental learning (CIL)” 691
are two classic settings in continual learning [ 104,28,6]. They differ as follows. Let MandN 692
denote positive integers. Consider continual learning of Mtasks where each task is an N-way 693
classification. In the DIL case, a model has an N-way output classification layer, i.e., the class ‘0’ of 694
the first task shares the same weights as the class ‘0’ of the second task, and so on. In the CIL case, a 695
model’s output dimension is N∗M; the class indices of different tasks are not shared, neither are the 696
corresponding weights in the output layer. In our experiments, all CIL models have the (N∗M)-way 697
output from the first task (instead of progressively increasing the output size). In this work, we skip 698
the third variant called “task-incremental learning” which assumes that we have access to the task 699
identity as an extra input, as it makes the CL problem almost trivial. CIL is typically reported to be 700
the hardest setting among them. 701
Meta-learning. We need to introduce “meta-training” and “meta-test” terminologie since each of 702
these phases involve “training/test” processes within itself. Each of them requires the corresponding 703
training and test examples. We refer to these as “meta-training training/test examples”, and “meta-test 704
training/test examples” following the terminology of Beaulieu et al. [30]. While these are rather 705
“heavy” terminologies, they are unambiguous and help avoid potential confusions. In both phases, 706
our sequence-processing neural net observes a sequence of (meta-training or meta-test) training 707
examples—each consisting of input features and a correct label—, and the resulting states of the 708
sequence processor (i.e., weights in the case of SRWM) are used to make predictions on (meta- 709
training or meta-test) test examples—input features presented to the model without its label. During 710
the meta-training phase, we modify the trainable parameters of the meta-learner through gradient 711
descent minimizing the meta-learning loss function (using backpropagation through time). During 712
meta-testing, no human-designed optimization for weight modification is used anymore; the SRWMs 713
modify their own weights following their own learning rules defined as their forward pass (Eqs. 1-3). 714
In connection with the now-popular in-context learning [ 96], we also refer to a (meta-training or 715
meta-test) training-example sequence as context . 716
A.2 Datasets 717
For classic image classification datasets such as MNIST [ 25], CIFAR10 [ 27], and FashionMNIST 718
(FMNIST; Xiao et al. [26]) we refer to the original references for details. 719
For Omniglot [ 23], we use Vinyals et al. [21]’s 1028/172/432-split for the train/validation/test set, as 720
well as their data augmentation methods using rotation of 90, 180, and 270 degrees. Original images 721
are grayscale hand-written characters from 50 different alphabets. There are 1632 different classes 722
with 20 examples for each class. 723
Mini-ImageNet contains color images from 100 classes with 600 examples for each class. We use the 724
standard train/valid/test class splits of 64/16/20 following [22]. 725
FC100 is based on CIFAR100 [ 27]. 100 color image classes (600 images per class, each of size 726
32×32) are split into train/valid/test classes of 60/20/20 [24]. 727
The “5-datasets” dataset [ 32] consists of 5 datasets: CIFAR10, MNIST, FashionMNST, SVNH [ 105], 728
and notMNIST [106]. 729
Split-CIFAR100 is also based on CIFAR100. The standard setting splits CIFAR100 into 10 10-way 730
classification tasks. 731
17Meta-train/test sequence construction procedure. We use torchmeta [107] which provides 732
common few-shot/meta learning settings for these datasets to sample and construct their meta- 733
train/test datasets. The construction of “meta-training training” sequences for an N-way classification, 734
using a dataset containing Cclasses works as follows; for each sequence, we sample Nrandom 735
but distinct classes out of C(N < C ). The resulting classes are re-labelled such that each class is 736
assigned to one out of Ndistinct random label index which is unique to the sequence. For each of 737
these Nclasses, we sample Kexamples. We randomly order these N∗Kexamples to obtain a 738
sequence. Each such a sequence “simulates” an unknown task the model has to learn. 739
A.3 Training Details & Hyper-Parameters 740
We use the same model and training hyper-parameters in all our experiments. All hyper-parameters 741
are summarized in Table 5. We use the Adam optimizer with the standard Transformer learning rate 742
warmup scheduling [ 59]. The vision backend is the classic 4-layer convolutional NN of Vinyals 743
et al. [21]. Most configurations follow those of Irie et al. [19]; except that we initialize the ‘query’ 744
sub-matrix in the self-referential weight matrix using a normal distribution with a mean value of 0 745
and standard deviation of 0.01/√dheadwhile other sub-matrices use an std of 1/√dhead(motivated 746
by the fact that a generated query vector is immediately multiplied with the same SRWM to produce 747
a value vector). For any further details, we’ll refer the readers to our public code we’ll release upon 748
acceptance. We conduct our experiments using a single V100-32GB, 2080-12GB or P100-16GB 749
GPUs, and the longest single training run takes about one day. 750
Table 5: Hyper-parameters.
Parameters Values
Number of SRWM layers 2
Total hidden size 256
Feedforward block multiplier 2
Number of heads 16
Batch size 16 or 32
A.4 Evaluation Procedure 751
For evaluation on few-shot learning datasets (i.e., Omniglot, Mini-Imagenet and FC100), we use 5 752
different sets consisting of 32 K random test episodes each, and report mean and standard deviation. 753
For evaluation on standard datasets, we use 5 different random support sets for in-context learning, 754
and evaluate on the entire test set. We report the corresponding mean and standard deviation across 755
these 5 evaluation runs. 756
For the Split-MNIST experiment, we do 10 meta-testing runs to compute the mean and standard 757
deviation as the baseline models are also trained for 10 runs in Hsu et al. [6](see other details in 758
Appendix A.7). 759
A.5 ACL Objectives with More Tasks 760
We can straightforwardly extend the 2-task version of ACL presented in Sec. 3 to more tasks. In 761
the 3-task case (we denote the three tasks as A,B, andC) used in Sec. 4.3, the objective function 762
contains six terms. Following three terms are added to Eq. 4: 763
−
log(p(yC
test|xC
test;WA,B,C)) + log( p(yB
test|xB
test;WA,B,C)) + log( p(yA
test|xA
test;WA,B,C))
This also naturally extends to the 5-task loss used in the Split-MNIST experiment (Table 3). As 764
one can observe, the number of terms rapidly/quadratically increases with the number of tasks. 765
Nevertheless, computing these loss terms isn’t immediately impractical because they essentially just 766
require forwarding the network for one step, for many independent inputs/images. This can be heavily 767
parallelized as a batch operation. While this can be a concern when scaling up more, a natural open 768
research question is whether we really need all these terms in the case we have many more tasks. 769
18Table 6: Impact of the choice of meta-validation datasets. Classification accuracies (%) on three
datasets: Split-CIFAR-10 ,Split-Fashion MNIST (Split-FMNIST), and Split-MNIST in the domain-
incremental setting (we omit “Split-” in the second column). “OOB” denotes “out-of-the-box”.
“mImageNet” here refers to mini-ImageNet.
Meta-Test on Split-
Meta-Finetune Datasets Meta-Validation Sets MNIST FMNIST CIFAR-10
None (OOB: 2-task ACL; Sec. 4.1) Omniglot + mImageNet 72.2 ±0.9 75.6 ±0.7 65.3 ±1.6
Omniglot MNIST 84.3±1.2 78.1 ±1.9 55.8 ±1.2
FMNIST 81.6 ±1.3 90.4±0.5 59.5 ±2.1
CIFAR10 75.2 ±2.3 78.2 ±0.9 63.4±1.4
Omniglot + mImageNet MNIST 76.6±1.4 85.3 ±1.1 66.2 ±1.1
FMNIST 73.2 ±2.3 89.9±0.6 66.6 ±0.7
CIFAR10 76.3 ±3.0 88.1 ±1.3 68.6±0.5
Ideally, we want these models to ‘systematically generalize’ to more tasks even when they are trained 770
with only a handful of them [ 108]. This is an interesting research question on generalization to be 771
studied in a future work. 772
A.6 Auxiliary 1-shot Learning Objective 773
In practice, instead of training the models only for “15-shot learning,” we also add an auxiliary loss 774
for 1-shot learning. This naturally encourages the models to learn in-context from the first examples. 775
A.7 Details of the Split-MNIST experiment 776
Here we provide details of the Split-MNIST experiments presented in Sec. 4 and Table 3. 777
Split-MNIST is obtained by transforming the classic 10-class single-task MNIST dataset into a 778
sequence of 5 tasks by partitioning the 10 classes into 5 groups/pairs of two classes each, in a fixed 779
order from 0 to 9 (i.e., grouping 0/1, 2/3, 4/5, 6/7, and 8/9). Regarding the difference between 780
domain/class-incremental settings, we refer to Appendix A.1. 781
The baseline methods presented in Table 3 include: standard SGD and Adam optimizers, Adam 782
with the L2 regularization, elastic weight consolidation [ 9] and its online variant [ 10], synaptic 783
intelligence [ 11], memory aware synapses [ 109], learning without forgetting (LwF; Li and Hoiem 784
[110] ). For these methods, we directly take the numbers reported in Hsu et al. [6]for the 5-task 785
domain/class-incremental settings. 786
For the 2-task class incremental setting, we use Hsu et al. [6]’s code to train the correspond models 787
(the number for LwF is currently missing as it is not implemented in their code base; we plan to add 788
the corresponding/missing entry in Table 3 for the final version of this paper). 789
Finally we also evaluate two meta-CL baselines: Online-aware Meta-Learning (OML; Javed and 790
White [29]) and Generative Meta-Continual Learning (GeMCL; Banayeeanzade et al. [31]). OML is 791
a MAML-based meta-learning approach. We note that as reported by Javed and White [29] in their 792
public code repository; after some critical bug fix, the performance of their OML matches that of 793
Beaulieu et al. [30] (which is a direct application of OML to another model architecture). Therefore, 794
we focus on OML as our main MAML-based baseline. We take the out-of-the-box model (meta- 795
trained for Omniglot, with a 1000-way output) made publicly available by Javed and White [29]. We 796
evaluate the corresponding model in two ways. In the first, ‘out-of-the-box’ case, we take the meta- 797
pre-trained model and only tune its meta-testing learning rate (which is done by Javed and White [29] 798
even for meta-testing in Omniglot). We find that this setting does not perform very well; in the other 799
case (‘optimized # meta-testing iterations’), we additionally tune the number of meta-test training 800
iterations. We’ve done a grid search of the meta-test learning rate in 3∗ {1e−2,1e−3,1e−4,1e−5} 801
and the number of meta-test training steps in {1,2,5,8,10}using a meta-validation set based on an 802
MNIST validation set (5 K held-out images from the training set); we found the learning rate of 3e−4803
and8steps to consistently perform the best in all our settings. We’ve also tried it ‘with’ and ‘without’ 804
19Table 7: Impact of the number of in-context examples. Classification accuracies (%) on Split-MNIST
in the 2-task and 5-task class-incremental learning (CIL) settings and the 5-task domain-incremental
learning (DIL) setting. For ACL models, we use the same number of examples for meta-validation as
for meta-training. According to Banayeeanzade et al. [31], GeMCL is meta-trained with the 5-shot
setting but meta-validated in the 15-shot setting.
Number of Examples DIL CIL 2-task CIL 5-task
Meta-Train/Valid Meta-Test GeMCL ACL GeMCL ACL GeMCL ACL
5 5 - 84.1 ±1.2 - 93.4 ±1.2 - 74.6 ±2.3
15 - 83.8 ±2.8 - 94.3 ±1.9 - 65.5 ±4.0
15 5 62.2 ±5.2 83.9 ±1.0 87.3 ±2.5 93.6 ±1.7 71.7 ±2.5 76.7 ±3.6
15 63.8±3.8 84.5±1.6 91.2±2.8 96.0±1.0 79.0±2.1 84.3±1.2
the standard mean/std normalization of the MNIST dataset; better performance was achieved without 805
such normalization (which is in fact consistent as they do not normalize the Omniglot dataset for 806
their meta-training/testing). Their performance on the 5-task class-incremental setting is somewhat 807
surprising/disappointing (since genenralization from Omniglot to MNIST is typically straightforward, 808
at least, in common non-continual few-shot learning settings; see, e.g., Munkhdalai and Yu [51]). At 809
the same time, to the best of our knowledge, OML-trained models have not been tested in such a 810
condition in prior work; from what we observe, the publicly available out-of-the-box model might 811
be overtuned for Omniglot/Mini-ImageNet or the frozen ‘representation network’ is not ideal for 812
genenralization. We note that the sensitivity of these MAML-based methods [ 29,30] w.r.t. meta-test 813
hyper-parameters has been also noted by Banayeeanzade et al. [31]; these are characteristics of 814
hand-crafted learning algorithms that we want to avoid with learned learning algorithms. 815
We use code and a pre-trained model (trained on Omniglot) made public by Banayeeanzade et al. 816
[31] for the GeMCL baseline (see also Table 7); like our method, GeMCL also do not require any 817
special tuning at test-time. 818
Our out-of-the-box ACL models (trained on Omniglot and Mini-ImageNet) do not require any 819
tuning at meta-test time. Nevertheless, we’ve checked the effect of the number of meta-test training 820
examples (5 vs. 15; 15 is the number used in meta-training); we found the consistent number, i.e., 15, 821
to work better than 5. For the version that is meta-finetuned using the 5-task ACL objective (using 822
only the Omniglot dataset), we use 5 or 15 examples for both meta-train and meta-test training (see an 823
ablation study in Table 7). To obtain a sequence of 5 tasks, we simply sample 5 tasks from Omniglot 824
(in principle, we should make sure that different tasks in the same sequence have no class overlap; 825
in practice, our current implementation simply randomly draws 5 independent tasks from Omniglot). 826
A.8 Details of the Split-CIFAR100 and 5-datasets experiment using ViT 827
As we described in Sec. 4, for the experiments on Split-CIFAR100 and 5-datasets, following 828
Wang et al. [33,34], we use ViT-B/16 pre-trained on ImageNet [ 76] which is available through 829
torchvision [111]. In this experiments, we resize all images to 3x224x224 and feed them to the 830
ViT. We remove the output layer of the ViT, and use its 768-dimensional feature from the penultimate 831
layer as the image encoding. The self-referential component which is added to this encoder has the 832
same architecture (2 layers, 16 heads) as the rest of the paper (see all hyper-parameters in Table 5) 833
All ViT parameters are frozen during meta-training. 834
B Extra Experimental Results 835
B.1 Ablation Studies on the Meta-validation Dataset 836
Here we conduct ablation studies on the choice of meta-validation sets to select model checkpoints. In 837
general, when dealing with out-of-domain generalization, the choice of validation procedures to select 838
final model checkpoints plays a crucial role in the evaluation of the corresponding method [ 112,113]. 839
The out-of-the-box models are chosen based on the average meta-validation performance on the 840
validation set corresponding to the few-shot learning datasets used in meta-training: Omniglot and 841
20Table 8: Meta-testing on sequences that are longer than those from meta-training. Classification
accuracies (%) on 5-task Split-FMNIST and 5-task Split-MNIST in the domain-incremental
settings. The model is the one finetuned with 5-task ACL loss using Omniglot as the meta-finetuning
set and FMNIST as the meta-validation set (i.e., the numbers in the top part of the table are taken
from Table 6). In the first column, “Split-FMNIST, Split-MNIST” indicates continual learning of 5
Split-FMNIST tasks followed by 5 tasks of Split-MNIST (and “Split-MNIST, Split-FMNIST” is the
opposite order). Performance is measured at the end of the entire sequence.
Meta-Test Test Tasks
Meta-Test Training Task Sequence # Tasks Split-FMNIST Split-MNIST
Split-FMNIST 5 90.4 ±0.5 -
Split-MNIST 5 - 81.6 ±1.3
Split-FMNIST, Split-MNIST 10 79.3 ±2.7 74.3 ±0.9
Split-MNIST, Split-FMNIST 10 78.1 ±3.1 78.5 ±1.7
Table 9: Classification acuracies (%) on 5-task 2-way Split-Omniglot. Mean/std is computed over 10
meta-test runs.
Method Domain Incremental Class Incremental
GeMCL 64.6 ±9.2 97.4 ±2.7
ACL 92.3 ±0.4 96.8 ±0.8
mini-ImageNet (or Omniglot, mini-ImageNet, and FC100 in the case of 3-task ACL), independently of 842
any potential meta-test datasets. In contrast, in the meta-finetuning process of Table 3, we selected our 843
model checkpoint by meta-validation on the MNIST validation dataset (we held out 5 K images from 844
the training set). Here we evaluate ACL models meta-finetuned for the “5-task domain-incremental 845
binary classification” on three Split-‘X’ tasks where ‘X’ is MNIST, FashionMNIST (FMNIST) or 846
CIFAR-10 for various choices of meta-validation sets (in each case we hold out 5 K images from 847
the corresponding training set). In addition, we also evaluate the effect of meta-finetuning datasets 848
(Omniglot only v. Omniglot and mini-ImageNet). Table 6 shows the results (we use 15 meta-training 849
and meta-testing examples except for the Omniglot-finedtuned/MNIST-validated model from Table 3 850
which happens to be configured with 5 examples; this will be fixed in the final version). Effectively, 851
meta-validation on the matching validation set is useful. Also, meta-finetuning only on Omniglot is 852
beneficial for the performance on MNIST when meta-validated on MNIST or FMNIST. However, 853
importantly, we emphasize that our ultimate goal is not to obtain a model that is specifically tuned for 854
certain datasets; we aim at building models that generally work well across a wide range of tasks 855
(ideally on any tasks); in fact, several existing works in the few-shot learning literature evaluate 856
their methods in such settings (see, e.g., Requeima et al. [114] , Bronskill et al. [78], Triantafillou 857
et al. [115] ). This also goes hand-in-hand with scaling up ACL (our current model is tiny; see 858
hyper-parameters in Table 5; the vision component is also a shallow ‘Conv-4’ net) and various other 859
considerations on self-improving continual learners (see, e.g., Schmidhuber [116] ), such as automated 860
curriculum learning [117]. 861
B.2 Performance on Split-Omniglot 862
Here we report the performance of the models used in the Split-MNIST experiment (Sec. 4.3) on 863
“in-domain” 5-task 2-way Split-Omniglot. Table 9 shows the result. Performance is very similar 864
between our ACL and the baseline GeMCL on this task in the class incremental setting, unlike on 865
Split-MNIST (Table 3) where we observe a larger performance gap between these same models. Here 866
we also include the “domain incremental” setting for the sake of completeness but note that GeMCL 867
is not originally trained for this setting. 868
21Table 10: 5-way classification accuracies using 15 examples for each class for each task in the context.
2-task models are meta-trained on Omniglot and Mini-ImageNet, while 3-task models are in addition
meta-trained on FC100. ‘A, B’ in ‘Context/Train’ column indicates that models sequentially observe
meta-test training examples of Task A then B; evaluation is only done at the end of the sequence. “no
ACL” is the baseline 2-task models trained without the ACL loss.
Meta-Testing Tasks Number of Meta-Training Tasks
Context/Train Test 2 (no ACL) 2 3
A: MNIST-04 A 71.1 ±4.0 75.4 ±3.0 89.7 ±1.6
B: CIFAR10-04 B 51.5 ±1.4 51.6 ±1.3 55.3 ±0.9
C: MNIST-59 C 65.9 ±2.4 63.0 ±3.3 76.1 ±2.0
D: FMNIST-04 D 52.8 ±3.4 54.8 ±1.3 59.2 ±4.0
Average 60.3 61.2 70.1
A, B A 43.7 ±2.3 81.5 ±2.7 88.0 ±2.2
B 49.4 ±2.4 50.8 ±1.3 52.9 ±1.2
Average 46.6 66.1 70.5
A, B, C A 26.5 ±3.2 64.5 ±6.0 82.2 ±1.7
B 32.3 ±1.7 50.8 ±1.2 50.3 ±2.0
C 56.5 ±8.1 33.7 ±2.2 44.3 ±3.0
Average 38.4 49.7 58.9
A, B, C, D A 24.6 ±2.7 64.3 ±4.8 78.9 ±2.3
B 20.6 ±2.3 47.5 ±1.0 49.2 ±1.3
C 38.5 ±4.4 32.7 ±1.9 45.4 ±3.9
D 36.1 ±2.5 31.2 ±4.9 30.1 ±5.8
Average 30.0 43.9 50.9
B.3 Effect of Number of In-Context Examples 869
Table 7 shows an ablation study on the number of examples used for meta-training and meta-testing 870
on the Split-MNIST task. We observe that for an ACL model trained only with 5 examples during 871
meta-training, more examples (15 examples) provided during meta-testing is not beneficial. In fact, 872
they even largely hurt in certain cases (see the last column); this is one form of “length generalization” 873
problem. When the number of meta-training examples is consistent with the one used during 874
meta-testing, the 15-example case consistently outperforms the 5-example one. 875
B.4 Effect of Number of Tasks in the ACL Loss 876
Table 10 provides the complete results discussed in Sec. 4.3 under “Evaluation on diverse task 877
domains”. 878
B.5 Further Discussion on Limitations 879
Here we provide further discussion and experimental results on the limitations of our approach as a 880
learned algorithm. 881
Domain generalization. As a data-driven learned algorithm, the domain generalization capability 882
is a typical limitation as it depends on the meta-trained data. Certain results we presented above 883
are representative of this limitation. In particular, in Table 6, the model meta-trained/finetuned on 884
Omniglot using Split-MNIST as meta-validation set do not perform well on Split-CIFAR10. While 885
meta-training and meta-validating on a larger/diverse set of datasets may be an immediate remedy 886
to obtain more robust ACL models, we note that since ACL is also a “continual meta-learning” 887
algorithm (Sec. 5), an ideal ACL model should also continually incorporate and learn from more data 888
during potentially lifelong meta-testing; we leave such an investigation for future work. 889
Length generalization. We already qualitatively observed the limited length generalization capabil- 890
ity in Table 10 (meta-trained with up to 3 tasks and meta-tested with up to 4 tasks). Here we provide 891
one more experiment evaluating ACL models meta-trained for 5 tasks on a concatenation of two 892
5-task Split-MNIST and Split-FMNIST tasks (resulting in 10 tasks). Table 8 shows the results. Again, 893
22while the model does not completely break, increasing the number of tasks to 10 rapidly degrades the 894
performance compared to the 5-task setting the model is meta-trained for. Similarly, its performance 895
on the Split-Omniglot domain incremental setting (Sec. B.2) degrades with increased numbers of 896
tasks: accuracies for 5, 10 and 20 tasks are 92.3%±0.4,82.0%±0.4and67.6%±1.1respectively. 897
As noted in Sec. 5, this is a general limitation of sequence processing neural networks, and there is a 898
potential remedy for this limitation (meta-training on more tasks and “context carry-over”) which we 899
leave for future work. 900
B.6 A Comment on Meta-Generalization 901
We also note that in general, “unseen” datasets do not necessarily imply that they are harder tasks than 902
“in-domain” test sets; when meta-trained on Omniglot and mini-ImageNet, meta-generalization on 903
“unseen” MNIST is easier (the accuracy is higher) than on the “in-domain” test set of mini-ImageNet 904
with heldout/unseen classes (compare Tables 1 and 2). 905
23NeurIPS Paper Checklist 906
1.Claims 907
Question: Do the main claims made in the abstract and introduction accurately reflect the 908
paper’s contributions and scope? 909
Answer: [Yes] 910
Justification: We accurately state contributions and scope of the work in the abstract and 911
introduction. 912
Guidelines: 913
•The answer NA means that the abstract and introduction do not include the claims 914
made in the paper. 915
•The abstract and/or introduction should clearly state the claims made, including the 916
contributions made in the paper and important assumptions and limitations. A No or 917
NA answer to this question will not be perceived well by the reviewers. 918
•The claims made should match theoretical and experimental results, and reflect how 919
much the results can be expected to generalize to other settings. 920
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 921
are not attained by the paper. 922
2.Limitations 923
Question: Does the paper discuss the limitations of the work performed by the authors? 924
Answer: [Yes] 925
Justification: We discuss limitations of our method in Sec. 4 and 5. 926
Guidelines: 927
•The answer NA means that the paper has no limitation while the answer No means that 928
the paper has limitations, but those are not discussed in the paper. 929
• The authors are encouraged to create a separate "Limitations" section in their paper. 930
•The paper should point out any strong assumptions and how robust the results are to 931
violations of these assumptions (e.g., independence assumptions, noiseless settings, 932
model well-specification, asymptotic approximations only holding locally). The authors 933
should reflect on how these assumptions might be violated in practice and what the 934
implications would be. 935
•The authors should reflect on the scope of the claims made, e.g., if the approach was 936
only tested on a few datasets or with a few runs. In general, empirical results often 937
depend on implicit assumptions, which should be articulated. 938
•The authors should reflect on the factors that influence the performance of the approach. 939
For example, a facial recognition algorithm may perform poorly when image resolution 940
is low or images are taken in low lighting. Or a speech-to-text system might not be 941
used reliably to provide closed captions for online lectures because it fails to handle 942
technical jargon. 943
•The authors should discuss the computational efficiency of the proposed algorithms 944
and how they scale with dataset size. 945
•If applicable, the authors should discuss possible limitations of their approach to 946
address problems of privacy and fairness. 947
•While the authors might fear that complete honesty about limitations might be used by 948
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 949
limitations that aren’t acknowledged in the paper. The authors should use their best 950
judgment and recognize that individual actions in favor of transparency play an impor- 951
tant role in developing norms that preserve the integrity of the community. Reviewers 952
will be specifically instructed to not penalize honesty concerning limitations. 953
3.Theory Assumptions and Proofs 954
Question: For each theoretical result, does the paper provide the full set of assumptions and 955
a complete (and correct) proof? 956
Answer: [NA] 957
24Justification: This is not a theoretical paper. 958
Guidelines: 959
• The answer NA means that the paper does not include theoretical results. 960
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 961
referenced. 962
•All assumptions should be clearly stated or referenced in the statement of any theorems. 963
•The proofs can either appear in the main paper or the supplemental material, but if 964
they appear in the supplemental material, the authors are encouraged to provide a short 965
proof sketch to provide intuition. 966
•Inversely, any informal proof provided in the core of the paper should be complemented 967
by formal proofs provided in appendix or supplemental material. 968
• Theorems and Lemmas that the proof relies upon should be properly referenced. 969
4.Experimental Result Reproducibility 970
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 971
perimental results of the paper to the extent that it affects the main claims and/or conclusions 972
of the paper (regardless of whether the code and data are provided or not)? 973
Answer: [Yes] 974
Justification: We provide experimental details in the main text and details in Appendix A. 975
We also provide our code in the supplemental material. 976
Guidelines: 977
• The answer NA means that the paper does not include experiments. 978
•If the paper includes experiments, a No answer to this question will not be perceived 979
well by the reviewers: Making the paper reproducible is important, regardless of 980
whether the code and data are provided or not. 981
•If the contribution is a dataset and/or model, the authors should describe the steps taken 982
to make their results reproducible or verifiable. 983
•Depending on the contribution, reproducibility can be accomplished in various ways. 984
For example, if the contribution is a novel architecture, describing the architecture fully 985
might suffice, or if the contribution is a specific model and empirical evaluation, it may 986
be necessary to either make it possible for others to replicate the model with the same 987
dataset, or provide access to the model. In general. releasing code and data is often 988
one good way to accomplish this, but reproducibility can also be provided via detailed 989
instructions for how to replicate the results, access to a hosted model (e.g., in the case 990
of a large language model), releasing of a model checkpoint, or other means that are 991
appropriate to the research performed. 992
•While NeurIPS does not require releasing code, the conference does require all submis- 993
sions to provide some reasonable avenue for reproducibility, which may depend on the 994
nature of the contribution. For example 995
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 996
to reproduce that algorithm. 997
(b)If the contribution is primarily a new model architecture, the paper should describe 998
the architecture clearly and fully. 999
(c)If the contribution is a new model (e.g., a large language model), then there should 1000
either be a way to access this model for reproducing the results or a way to reproduce 1001
the model (e.g., with an open-source dataset or instructions for how to construct 1002
the dataset). 1003
(d)We recognize that reproducibility may be tricky in some cases, in which case 1004
authors are welcome to describe the particular way they provide for reproducibility. 1005
In the case of closed-source models, it may be that access to the model is limited in 1006
some way (e.g., to registered users), but it should be possible for other researchers 1007
to have some path to reproducing or verifying the results. 1008
5.Open access to data and code 1009
Question: Does the paper provide open access to the data and code, with sufficient instruc- 1010
tions to faithfully reproduce the main experimental results, as described in supplemental 1011
material? 1012
25Answer: [Yes] 1013
Justification: We provide experimental details in the main text and details in Appendix A. 1014
We also provide our code in the supplemental material. The data we use are classic datasets 1015
which are publicly available. 1016
Guidelines: 1017
• The answer NA means that paper does not include experiments requiring code. 1018
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 1019
public/guides/CodeSubmissionPolicy ) for more details. 1020
•While we encourage the release of code and data, we understand that this might not be 1021
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 1022
including code, unless this is central to the contribution (e.g., for a new open-source 1023
benchmark). 1024
•The instructions should contain the exact command and environment needed to run to 1025
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 1026
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 1027
•The authors should provide instructions on data access and preparation, including how 1028
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 1029
•The authors should provide scripts to reproduce all experimental results for the new 1030
proposed method and baselines. If only a subset of experiments are reproducible, they 1031
should state which ones are omitted from the script and why. 1032
•At submission time, to preserve anonymity, the authors should release anonymized 1033
versions (if applicable). 1034
•Providing as much information as possible in supplemental material (appended to the 1035
paper) is recommended, but including URLs to data and code is permitted. 1036
6.Experimental Setting/Details 1037
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 1038
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 1039
results? 1040
Answer: [Yes] 1041
Justification: We provide experimental details in the main text and details in Appendix A. 1042
We also provide our code in the supplemental material. 1043
Guidelines: 1044
• The answer NA means that the paper does not include experiments. 1045
•The experimental setting should be presented in the core of the paper to a level of detail 1046
that is necessary to appreciate the results and make sense of them. 1047
•The full details can be provided either with the code, in appendix, or as supplemental 1048
material. 1049
7.Experiment Statistical Significance 1050
Question: Does the paper report error bars suitably and correctly defined or other appropriate 1051
information about the statistical significance of the experiments? 1052
Answer: [Yes] 1053
Justification: All our results are mean/std computed using 10 evaluation seeds. 1054
Guidelines: 1055
• The answer NA means that the paper does not include experiments. 1056
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 1057
dence intervals, or statistical significance tests, at least for the experiments that support 1058
the main claims of the paper. 1059
•The factors of variability that the error bars are capturing should be clearly stated (for 1060
example, train/test split, initialization, random drawing of some parameter, or overall 1061
run with given experimental conditions). 1062
•The method for calculating the error bars should be explained (closed form formula, 1063
call to a library function, bootstrap, etc.) 1064
26• The assumptions made should be given (e.g., Normally distributed errors). 1065
•It should be clear whether the error bar is the standard deviation or the standard error 1066
of the mean. 1067
•It is OK to report 1-sigma error bars, but one should state it. The authors should 1068
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 1069
of Normality of errors is not verified. 1070
•For asymmetric distributions, the authors should be careful not to show in tables or 1071
figures symmetric error bars that would yield results that are out of range (e.g. negative 1072
error rates). 1073
•If error bars are reported in tables or plots, The authors should explain in the text how 1074
they were calculated and reference the corresponding figures or tables in the text. 1075
8.Experiments Compute Resources 1076
Question: For each experiment, does the paper provide sufficient information on the com- 1077
puter resources (type of compute workers, memory, time of execution) needed to reproduce 1078
the experiments? 1079
Answer: [Yes] 1080
Justification: We provide compute resource related information in Appendix A. 1081
Guidelines: 1082
• The answer NA means that the paper does not include experiments. 1083
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 1084
or cloud provider, including relevant memory and storage. 1085
•The paper should provide the amount of compute required for each of the individual 1086
experimental runs as well as estimate the total compute. 1087
•The paper should disclose whether the full research project required more compute 1088
than the experiments reported in the paper (e.g., preliminary or failed experiments that 1089
didn’t make it into the paper). 1090
9.Code Of Ethics 1091
Question: Does the research conducted in the paper conform, in every respect, with the 1092
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 1093
Answer: [NA] 1094
Justification: We do not have anything to report. 1095
Guidelines: 1096
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 1097
•If the authors answer No, they should explain the special circumstances that require a 1098
deviation from the Code of Ethics. 1099
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 1100
eration due to laws or regulations in their jurisdiction). 1101
10.Broader Impacts 1102
Question: Does the paper discuss both potential positive societal impacts and negative 1103
societal impacts of the work performed? 1104
Answer: [NA] 1105
Justification: Our work does not have any such impacts. 1106
Guidelines: 1107
• The answer NA means that there is no societal impact of the work performed. 1108
•If the authors answer NA or No, they should explain why their work has no societal 1109
impact or why the paper does not address societal impact. 1110
•Examples of negative societal impacts include potential malicious or unintended uses 1111
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 1112
(e.g., deployment of technologies that could make decisions that unfairly impact specific 1113
groups), privacy considerations, and security considerations. 1114
27•The conference expects that many papers will be foundational research and not tied 1115
to particular applications, let alone deployments. However, if there is a direct path to 1116
any negative applications, the authors should point it out. For example, it is legitimate 1117
to point out that an improvement in the quality of generative models could be used to 1118
generate deepfakes for disinformation. On the other hand, it is not needed to point out 1119
that a generic algorithm for optimizing neural networks could enable people to train 1120
models that generate Deepfakes faster. 1121
•The authors should consider possible harms that could arise when the technology is 1122
being used as intended and functioning correctly, harms that could arise when the 1123
technology is being used as intended but gives incorrect results, and harms following 1124
from (intentional or unintentional) misuse of the technology. 1125
•If there are negative societal impacts, the authors could also discuss possible mitigation 1126
strategies (e.g., gated release of models, providing defenses in addition to attacks, 1127
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 1128
feedback over time, improving the efficiency and accessibility of ML). 1129
11.Safeguards 1130
Question: Does the paper describe safeguards that have been put in place for responsible 1131
release of data or models that have a high risk for misuse (e.g., pretrained language models, 1132
image generators, or scraped datasets)? 1133
Answer: [NA] 1134
Justification: Our work does not imply any such risks. 1135
Guidelines: 1136
• The answer NA means that the paper poses no such risks. 1137
•Released models that have a high risk for misuse or dual-use should be released with 1138
necessary safeguards to allow for controlled use of the model, for example by requiring 1139
that users adhere to usage guidelines or restrictions to access the model or implementing 1140
safety filters. 1141
•Datasets that have been scraped from the Internet could pose safety risks. The authors 1142
should describe how they avoided releasing unsafe images. 1143
•We recognize that providing effective safeguards is challenging, and many papers do 1144
not require this, but we encourage authors to take this into account and make a best 1145
faith effort. 1146
12.Licenses for existing assets 1147
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 1148
the paper, properly credited and are the license and terms of use explicitly mentioned and 1149
properly respected? 1150
Answer: [Yes] 1151
Justification: Our codebase includes certain publicly available code. The corresponding 1152
license files are included in the supplemental material. 1153
Guidelines: 1154
• The answer NA means that the paper does not use existing assets. 1155
• The authors should cite the original paper that produced the code package or dataset. 1156
•The authors should state which version of the asset is used and, if possible, include a 1157
URL. 1158
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1159
•For scraped data from a particular source (e.g., website), the copyright and terms of 1160
service of that source should be provided. 1161
•If assets are released, the license, copyright information, and terms of use in the 1162
package should be provided. For popular datasets, paperswithcode.com/datasets 1163
has curated licenses for some datasets. Their licensing guide can help determine the 1164
license of a dataset. 1165
•For existing datasets that are re-packaged, both the original license and the license of 1166
the derived asset (if it has changed) should be provided. 1167
28•If this information is not available online, the authors are encouraged to reach out to 1168
the asset’s creators. 1169
13.New Assets 1170
Question: Are new assets introduced in the paper well documented and is the documentation 1171
provided alongside the assets? 1172
Answer: [Yes] 1173
Justification: The documentations of our code are included in the readme file in the supple- 1174
mental material. 1175
Guidelines: 1176
• The answer NA means that the paper does not release new assets. 1177
•Researchers should communicate the details of the dataset/code/model as part of their 1178
submissions via structured templates. This includes details about training, license, 1179
limitations, etc. 1180
•The paper should discuss whether and how consent was obtained from people whose 1181
asset is used. 1182
•At submission time, remember to anonymize your assets (if applicable). You can either 1183
create an anonymized URL or include an anonymized zip file. 1184
14.Crowdsourcing and Research with Human Subjects 1185
Question: For crowdsourcing experiments and research with human subjects, does the paper 1186
include the full text of instructions given to participants and screenshots, if applicable, as 1187
well as details about compensation (if any)? 1188
Answer: [NA] 1189
Justification: We do not have such experiments. 1190
Guidelines: 1191
•The answer NA means that the paper does not involve crowdsourcing nor research with 1192
human subjects. 1193
•Including this information in the supplemental material is fine, but if the main contribu- 1194
tion of the paper involves human subjects, then as much detail as possible should be 1195
included in the main paper. 1196
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1197
or other labor should be paid at least the minimum wage in the country of the data 1198
collector. 1199
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1200
Subjects 1201
Question: Does the paper describe potential risks incurred by study participants, whether 1202
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1203
approvals (or an equivalent approval/review based on the requirements of your country or 1204
institution) were obtained? 1205
Answer: [NA] 1206
Justification: We do not have such experiments. 1207
Guidelines: 1208
•The answer NA means that the paper does not involve crowdsourcing nor research with 1209
human subjects. 1210
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1211
may be required for any human subjects research. If you obtained IRB approval, you 1212
should clearly state this in the paper. 1213
•We recognize that the procedures for this may vary significantly between institutions 1214
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1215
guidelines for their institution. 1216
•For initial submissions, do not include any information that would break anonymity (if 1217
applicable), such as the institution conducting the review. 1218
29