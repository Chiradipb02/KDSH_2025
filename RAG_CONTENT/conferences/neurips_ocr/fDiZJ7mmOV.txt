Non-Stationary Learning of Neural Networks
with Automatic Soft Parameter Reset
Alexandre Galashov∗
UCL Gatsby
Google DeepMind
agalashov@google.comMichalis K. Titsias
Google DeepMind
mtitsias@google.comAndrás György
Google DeepMind
agyorgy@google.com
Clare Lyle
Google DeepMind
clarelyle@google.comRazvan Pascanu
Google DeepMind
razp@google.comYee Whye Teh
Google DeepMind
University of Oxford
ywteh@google.com
Maneesh Sahani
UCL Gatsby
maneesh@gatsby.ucl.ac.uk
Abstract
Neural networks are traditionally trained under the assumption that data come
from a stationary distribution. However, settings which violate this assumption are
becoming more popular; examples include supervised learning under distributional
shifts, reinforcement learning, continual learning and non-stationary contextual
bandits. In this work we introduce a novel learning approach that automatically
models and adapts to non-stationarity, via an Ornstein-Uhlenbeck process with an
adaptive drift parameter. The adaptive drift tends to draw the parameters towards
the initialisation distribution, so the approach can be understood as a form of
softparameter reset. We show empirically that our approach performs well in
non-stationary supervised and off-policy reinforcement learning settings.
1 Introduction
Neural networks (NNs) are typically trained using algorithms like stochastic gradient descent (SGD),
assuming data comes from a stationary distribution. This assumption fails in scenarios such as
continual learning, reinforcement learning, non-stationary contextual bandits, and supervised learning
with distribution shifts [ 20,53]. A phenomenon occurring in non-stationary settings is the loss of
plasticity [12,2,13], manifesting either as a failure to generalize to new data despite reduced training
loss [ 4,2], or as an inability to reduce training error as the data distribution changes [ 13,37,1,42,34].
In [38], the authors argue for two factors that lead to the loss of plasticity: preactivation distribution
shift, leading to dead or dormant neurons [ 47], and parameter norm growth causing training instabili-
ties. To address these issues, strategies often involve hard resets based on heuristics like detecting
dormant units [ 47], assessing neuron utility [ 13,12], or simply after a fixed number of steps [ 43].
Though effective at increasing plasticity, hard resets can be inefficient as they can discard valuable
knowledge captured by the parameters.
We propose an algorithm that implements a mechanism of softparameter resets, in contrast to the
hard resets discussed earlier. A softreset partially moves NN parameters towards the initialization
∗Corresponding author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).while keeping them close to their previous values. It also increases learning rate of the learning
algorithm, allowing new NN parameters to adapt faster to the changing data. The amount by which
the parameters move towards the initialization and the amount of learning rate increase are controlled
by the drift parameters which are learned online. The exact implementation of softreset mechanism
is based on the use of a drift model in NN parameters update before observing new data. Similar
ideas which modify the starting point of SGD as well as increase the learning rate of SGD depending
on non-stationarity were explored (see [ 21,29]) in an online convex optimization setting. Specifically,
in [21], the authors assume that the optimal parameter of SGD changes according to some dynamical
model out of a finite family of models. They propose an algorithm to identify this model and a way to
leverage this model in a modified SGD algorithm. Compared to these works, we operate in a general
non-convex setting. Proposed drift model can be thought as a dynamical Bayesian prior over Neural
Network parameters, which is adapted online to new data. We make a specific choice of drift model
which implements softresets mechanism.
Our contributions can be summarized as follows. First, we propose an explicit model for the drift in
NN parameters and describe the procedure to estimate the parameters of this model online from the
stream of data. Second, we describe how the estimated drift model is incorporated in the learning
algorithm. Third, we empirically demonstrate the effectiveness of this approach in preventing the
loss of plasticity as well as in an off-policy reinforcement learning setting.
2 Non-stationary learning with Online SGD
In a non-stationary learning setting with changing data distributions pt(x, y), where x∈RL, y∈RK,
we define the loss function for parameters θ∈RDas
Lt(θ) =E(xt,yt)∼ptLt(θ, xt, yt) (1)
Our goal is to find a parameter sequence Θ = ( θ1, . . . , θ T)that minimizes the dynamic regret:
RT(Θ,Θ⋆) =1
TPT
t=1(Lt(θt)− Lt(θ⋆
t)), (2)
with a reference sequence Θ⋆= (θ⋆
1, . . . , θ⋆
T), satisfying θ⋆
t= arg min θLt(θ). A common approach
to the online learning problem is online stochastic gradient descent (SGD) [ 23]. Starting from initial
parameters θ0, the method updates these parameters sequentially for each batch of data {(xi
t, yi
t)}B
i=1
s.t.(xi
t, yi
t)∼pt(xt, yt). The update rule is:
θt+1=θt−αt∇θLt+1(θt),
where ∇θLt+1(θt) =1
BPB
i=1∇θLt+1(θt, xi
t, yi
t)andαtis learning rate. See also Appendix G for
the connection of SGD to proximal optimization.
Convex Setting. In the convex setting, online SGD with a fixed learning rate αcan handle non-
stationarity [ 56]. By selecting αappropriately – potentially using additional knowledge about the
reference sequence—we can optimize the dynamic regret in (2). In general, algorithms that adapt to
the observed level of non-stationarity can outperform standard online SGD. For example, in [ 29], the
authors propose to adjust the learning rate αt, while in [ 21] and in [ 29], the authors suggest modifying
the starting point of SGD from θtto an adjusted θ′
tproportional to the level of non-stationarity.
Non-Convex Setting. Non-stationary learning with NNs is more complex, since now there is a
changing set of local minima as the data distribution changes. Such changes can lead to a loss of
plasticity and other pathologies. Alternative optimization methods like Adam [ 30], do not fully
resolve this issue [ 13,37,1,42,34]. Parameter resets [ 13,48,12] partially mitigate the problem, but
could be too aggressive if the data distributions are similar.
3 Online non-stationary learning with learned parameter resets
Notation. We denote by N(θ;µ, σ2)a Gaussian distribution on θwith mean µand variance σ2.
We denote θithei-the component of the vector θ= (θ1, . . . , θD). Unless explicitly mentioned,
we assume distributions are defined per NN parameter and we omit the index i. We denote as
Lt+1(θ) =−logp(yt+1|xt+1, θ)the negative log likelihood on (yt+1, xt+1)for parameters θ.
We introduce Soft Resets , an approach that enhances learning algorithms on non-stationary data
distributions and prevents plasticity loss. The main idea is to assume that the data is generated in
2Graphical model
(c) Stationary case
(d) Non-stationary case without dynamical model(e) Non-stationary case with dynamical model
(a) i.i.d. assumption(b) non i.i.d. assumptionToy Bayesian Inference exampleFigure 1: Left: graphical model for data generating process in the (a) stationary case and (b) non-
stationary case with drift model p(θt+1|θt, γt).Right : (c) In a stationary online learning regime, the
Bayesian posterior (red dashed circles) in the long run will concentrate around θ∗(red dot). (d) In a
non-stationary regime where the optimal parameters suddenly change from current value θ∗
tto new
value θ∗
t+1(blue dot) online Bayesian estimation can be less data efficient and take time to recover
when the change-point occurs. (e) The use of p(θ|θt, γt)and the estimation of γtallows to increase
the uncertainty, by soft resetting the posterior to make it closer to the prior (green dashed circle), so
that the updated Bayesian posterior pt+1(θ)(blue dashed circle) can faster track θ∗
t+1.
non-i.i.d. fashion such that a change in the data distribution is modeled by the drift in the parameters
p(θt+1|θt, γt)at every time t+ 1before new data is observed. We assume a class of drift models
p(θt+1|θt, γt)which encourages the parameters to move closer to the initialization. The amount of
drift (and level of non-stationarity) is controlled by γtwhich are estimated online from the data.
As can be seen below, in the context of SGD, this approach adjusts the starting point θtof the update
to a point ˜θt(γt), which is closer to the initialization and increases the learning rate proportionally
to the drift. In the context of Bayesian inference, this approach shrinks the mean of the estimated
posterior towards the prior and increases the variance proportional to γt. This approach is inspired by
prior work in online convex optimization for non-stationary environments [e.g., 25, 21, 8, 18, 29].
3.1 Toy illustration of the advantage of drift models
Consider online Bayesian inference with 2-D observations yt=θ⋆+ϵt, where θ⋆∈R2are
unknown true parameters and ϵt∼ N(0;σ2I)is Gaussian noise with variance σ2. Starting from
a Gaussian prior p0(θ) =N(θ;µ0; Σ0), the posterior distribution pt+1(θ) =p(θ|y1, . . . , y t) =
N(θ;µt+1,Σt+1)is updated using Bayes’ rule
pt+1(θ)∼p(yt+1|θ)pt(θ). (3)
The posterior update (3)comes from the i.i.d. assumption on the data generation process (Figure 1a),
since pt+1(θ)∼p0(θ)Qt+1
s=1p(ys|θ). By the Central Limit Theorem (CLT), the posterior mean µt
converges to θ⋆and the covariance matrix Σtshrinks to zero (the radius of red circle in Figure 1c).
Suppose now that the trueparameters θ⋆
t(kept fixed before t) change to new parameters θ⋆
t+1at time
t+ 1. The i.i.d. assumption (Figure 1a) is violated and the update (3)becomes problematic because
the low uncertainty (small radius of red dashed circle in Figure 1d) in pt(θ)causes the posterior
pt+1(θ)(see blue circle) to adjust slowly towards θ⋆
t+1(blue dot) as illustrated in Figure 1d.
To address this issue, we assume that before observing new data, the parameters drift according
top(θt+1|θt, γt)where the amount of drift is controlled by γt. The corresponding conditional
independence structure is shown in Figure 1b. The posterior update then becomes:
pt+1(θ)∼p(yt+1|θ)R
p(θ|θ′
t, γt)pt(θ′
t)dθ′
t. (4)
For a suitable choice of drift model p(θt+1|θt, γt), this modification allows pt+1(θ)(blue circle) to
adjust more rapidly towards the new θ⋆
t+1(blue dot), see Figure 1e. This is because the new priorR
p(θ|θ′
t, γt)pt(θ′
t)dθ′
thas larger variance (green circle) than pt(θ)and its mean is closer to the center
of the circle. Ideally, the parameter γtshould capture the underlying non-stationarity in the data
distribution in order to control the impact of the priorR
p(θ|θ′
t, γt)pt(θ′
t)dθ′
t. For example, if at
some point the non-stationarity disappears, we want the drift model to exhibit no-drift to recover the
posterior update (3). This highlights the importance of the adaptive nature of the drift model.
33.2 Ornstein-Uhlenbeck parameter drift model
We motivate the specific choice of the drift model which is useful for maintaining plasticity. We
assume that our Neural Network has enough capacity to learn any stationary dataset in a fixed number
of iterations starting from a good initialization θ0∼p0(θ)[see, e.g., 24,16]. Informally, we call the
initialization θ0plastic and the region around θ0aplastic region .
Consider now a piecewise stationary datastream that switches between a distribution pa, with a set
of local minima Maof the negative likelihood L(θ), to a distribution pbat time t+ 1, with a set of
local minima Mb. IfMbis far from Ma, then hard reset might be beneficial, but if Mbis close to Ma,
resetting parameters is suboptimal. Furthermore, since θis high-dimensional, different dimensions
might need to be treated differently. We want a drift model that can capture all of these scenarios.
Drift model. The drift model p(θt+1|θt, γt)which exhibits the above properties is given by
p(θ|θt, γt) =N(θ;γtθt+ (1−γt)µ0; (1−γ2
t)σ2
0), (5)
which is separately defined for every parameter dimension θiwhere p0(θi
0)∼ N(θi
0;µi
0;
σi
02)is
the per-parameter prior distribution and γt= (γ1
t, . . . , γD
t). The model is a discretized Ornstein-
Uhlenbeck (OU) process [50] (see Appendix A for the derivation).
The parameter γt∈[0,1]is adrift parameter and controls the amount of non-stationarity in each
parameter. For γt= 1, there is no drift and for γt= 0, the drift model reverts the parameters back to
the prior. A value of γt∈(0,1)interpolates between these two extremities. A remarkable property
of(5)is that starting from the current parameter θt, if we simulate a long trajectory, as T→ ∞ , the
distribution of p(θT|θt)will converge to the prior p(θ0). This is only satisfied (for γt∈(0,1)) due
to the variance σ2
0(1−γ2
t). Replacing it by an arbitrary variance σ2would result in the variance
ofp(θT|θt)either going to 0or growing to ∞, harming learning. Thus, the model (5)encourages
parameters to move towards plastic region (initialization). In Appendix B, we discuss this further and
other potential choices for the drift model.
3.3 Online estimation of drift model
The drift model p(θt+1|θt, γt)quantifies prior belief about the change in parameters before seeing
new data. A suitable choice of an objective to select γtispredictive likelihood which quantifies the
probability of new data under our current parameters and drift model. From Bayesian perspective, it
means selecting the prior distribution which explains the future data the best.
We derive the drift estimation procedure in the context of approximate online variational infer-
ence [ 7] with Bayesian Neural Networks (BNN). Let Γt= (γ1, . . . , γ t)be the history of ob-
served parameters of the drift model and St={(x1, y1), . . . , (xt, yt)}be the history of ob-
served data. The objective of approximate online variational inference is to propagate an ap-
proximate posterior qt(θ|St,Γt−1)over parameters, such that it is constrained to some family
Qof probability distributions. In the context of BNNs, it is typical [ 5] to assume a family
Q={q(θ) :q(θ)∼QD
i=1N(θi;µi,
σi2);θ= (θ1, . . . , θD)}of Gaussian mean-field distributions
over parameters θ∈RD(separate Gaussian per parameter). For simplicity of notation, we omit the
index i. Letqt(θ)≜qt(θ|St,Γt−1)∈ Q be the Gaussian approximate posterior at time twith mean
µtand variance σ2
tfor every parameter. The new approximate posterior qt+1(θ)∈ Q is found by
qt+1(θ) = arg min qKL[q(θ)||p(yt+1|xt+1, θ)qt(θ|γt)], (6)
where the prior term is the approximate predictive look-ahead prior given by
qt(θ|γt) =R
qt(θt)p(θ|θt, γt)dθt=N(θ; ˜µt(γt),˜σ2
t(γt)) (7)
that has parameters ˜µt(γt) =γtµt+ (1−γt)µ0,˜σ2
t(γt) =γ2
tσ2
t+ (1−γ2
t)σ2
0, see Appendix I.1 for
derivation. The form of this prior qt(θ|γt)comes from the non i.i.d. assumption (see Figure 1b) and
the form of the drift model (5). For new batch of data (xt+1, yt+1)at time t+ 1, the approximate
predictive log-likelihood equals to
logqt(yt+1|xt+1, γt) = logR
p(yt+1|xt+1, θ)qt(θ|γt)dθ. (8)
The log-likelihood (8)allows us to quantify predictions on new data (xt+1, yt+1)given our current
distribution qt(θ)and the drift model from (5). We want to find such γ⋆
tthat
γ⋆
t≈arg max γtlogqt(yt+1|xt+1, γt) (9)
4Using γ⋆
tin(5)modifies the prior distribution (7)to fit the most recent observations the best by
putting more mass on the region where the new parameter could be found (see Figure 1,right).
Gradient-based optimization for γt.The approximate predictive prior in (7)is Gaussian which
allows us to use the so-called reparameterisation trick to optimize (8)via gradient descent. Starting
from an initial value of drift parameter γ0
tat time t, we perform Kupdates with learning rate ηγ
γt,k+1=γt,k+ηγ∇γlogR
p(yt+1|xt+1,˜µt(γt,k) +ϵ˜σt(γt,k))N(ϵ; 0, I)dϵ, (10)
The integral is evaluated by Monte-Carlo (MC) using Msamples ϵi∼ N(ϵ; 0, I),i= 1, . . . , M
R
p(yt+1|xt+1,˜µt(γt,k) +ϵ˜σt(γt,k))N(ϵ; 0, I)dϵ≈1
MPM
i=1p(yt+1|xt+1,˜µt(γt,k) +ϵi˜σt(γt,k))
(11)
Inductive bias in the drift model is captured by γ0
t, where γt,0= 1encourages stationarity, while
γt,0=γt−1,Kpromotes temporal smoothness. In practice, we found γt,0= 1was the most effective.
Structure in the drift model. The drift model can be defined to be shared across different subsets of
parameters which reduces the expressivity of the drift model but also provides regularization to (10).
We consider γtto be either defined for each parameter or for each layer . See Section 5 for details as
well as corresponding results in Appendix H.
Interpretation of γt.By linearising logp(yt+1|xt+1, θ)around µt, we can compute (8)in a closed
form and get the following loss for γt(see Appendix J for the proof) optimizing (9)
F(γt) = 0 .5(σ2
t(γt)⊙gt+1)⊤gt+1−(γt⊙µt+ (1−γt)⊙µ0)⊤gt+1,
where gt=∇Lt+1(µt)andLt+1(θ) =−logp(yt+1|xt+1, µt), and where ⊙denotes element-wise
product performed only over parameters for which γtis shared (see paragraph about structure in drift
model). The transpose operation is also defined on a subset of parameters for which γtis shared.
Adding the ℓ2penalty1
2λ||γt−γ0
t||2encoding the starting point γ0
t, gives us the closed form for γt
γt=(µt−µ0)Tgt+1+λγ0
t
((σ2
0−σ2
t)·gt+1)Tgt+1+λ, (12)
where we also clip parameters γtto[0,1]. The expression (12) gives us the geometric interpretation
forγt. The value of γtdepends on the angle between (µt−µ0)andgt+1When these vectors are
aligned, γtis high and is low otherwise. When these vectors are orthogonal or the gradient gt+1≈0,
the value of γtis heavily influenced by γ0
t. Moreover, when gt+1≈0, we can interpret it as being
close to a local minimum, i.e., stationary, which means that we want γt≈1, therefore adding the ℓ2
penalty is important. Also, when the norm of the gradients gt+1is high, the value of γtis encouraged
to decrease, introducing the drift. This means that using γtin the parameter update (see Section 3.5)
encourages the norm of the gradient to stay small. In practice, we found that update (12) was unstable
suggesting that linearization of the log-likelihood might not be a good approximation for learning γt.
3.4 Approximate Bayesian update of posterior qt(θ)with BNNs
The optimization problem (6)for the per-parameter Gaussian q(θ) =N(θ;µ, σ2)with Gaussian prior
qt(θ) =N(θ;µt, σ2
t), both defined for every parameter of NN, can be written (see Appendix I.1) to
minimize the following loss
˜Ft(µ, σ, γ t) =Eϵ∼N(0;I)[Lt+1(µ+ϵσ)] +PD
i=1λi
t
(µi−˜µi
t(γt))2+[σi]2
2[˜σi
t(γt)]2−1
2log
σi2
,(13)
where λi
t>0are per-parameter temperature coefficients. The use of small temperature λ > 0
parameter (shared for all NN parameters) was shown to improve empirical performance of Bayesian
Neural Networks [ 54]. Given that in (13), the variance ˜σ2
t(γt)can be small, in order to control the
strength of the regularization, we propose to use per parameter temperature λi
t=λ×
σi
t2, where
λ >0is a global constant. This leads to the following objective
ˆFt(µ, σ, γ t) =Eϵ∼N(0;I)[Lt+1(µ+ϵσ)]+λ
2P
iri
th
(µi−˜µi
t(γt))2+ [σi]2−
˜σi
t(γt)2log[σi]2i
,
(14)
where the quantity ri
t= [σi
t]2/[σi
t(γt)]2is a relative change in the posterior variance due to the drift.
The ratio ri
t= 1when γt= 1. Forγt<1since typically σ2
t< σ2
0, the ratio is ri
t<1. Thus, as long
as there is non-stationarity ( γt<1), the objective (14) favors the data term Eϵ∼N(0;I)[Lt+1(µ+ϵσ)]
5Algorithm 1 Soft-Reset algoritm
Input: Data-stream ST={(xt, yt)})T
t=1
Neural Network (NN) initializing distribution pinit(θ)and specific initialization θ0∼pinit(θ)
Learning rate αtfor parameters and ηγfor drift parameters
Number of gradient updates Kγon drift parameter γt
NN initial standard deviation (STD) scaling p≤1(see (23)) and ratio s=σt
pσ0.
forstept= 0,1,2, . . . , T do
For(xt+1, yt+1), predict ˆyt+1=f(xt+1|θt)
Compute performance metric based on (yt+1,ˆyt+1)
Initialize drift parameter γt,0= 1
forstepk= 0,1,2, . . . , K γdo
Sample θ′
0∼pinit(θ)
Stochastic update (21) on drift parameter using specific initialization (24)
γt,k+1=γt,k+ηγ∇γh
logp(yt+1|xt+1, γtθt+ (1−γt)θ0+θ′
0pp
1−γ2
t+γ2
ts2)i
γt=γt,k
end for
Get˜θt(γt,K)with (17) and ˜αt(γt,K)with (18)
Update parameters θt+1=˜θt(γt,K)−˜αt(γt,K)◦ ∇θLt+1(˜θt(γt,K))
end for
allowing the optimization to respond faster to changes in the data distribution. To find new parameters,
letµt+1,0= ˜µt(γt)andσt+1,0= ˜σt(γt), and perform updates Kon (14)
µt+1,k+1=µt+1,k−αµˆFt(µt+1,k, σt+1,k, γt), σt+1,k+1=σt+1,k−ασˆFt(µt+1,k, σt+1,k, γt),
(15)
where αµandασare learning rates for the mean and for the standard deviation correspondingly. All
derivations are provided in Appendix I.1. The full procedure is described in Algorithm 2.
3.5 Fast MAP update of posterior qt(θ)
As a faster alternative to propagating the posterior (6), we do MAP updates with the prior p0(θ) =
N(θ;µ0;σ2
0)and the approximate posterior qt(θ) =N(θ;θt;σ2
t=s2σ2
0), where s≤1is a
hyperparameter controlling the variance σ2
tofqt(θ). Since a fixed smay not capture the true
parameters variance, using a Bayesian method (see Section 3.4) is preferred but comes at a high
computational cost (see Appendix E for discussion). The MAP update is given by (see Appendix I.2
for derivations) finding a minimum of the following proximal objective
G(θ) =Lt+1(θ) +1
2PD
i=1|θi−˜θi
t(γt)|2
˜αi
t(γt)(16)
where the regularization target for the parameter dimension iis given by
˜θi
t(γt) =γi
tθi
t+ (1−γi
t)µi
0 (17)
and the per-parameter learning rate is given as (assuming that αtthe base SGD learning rate)
˜αi
t(γt) =αt
(γi
t)2+1−(γi
t)2
s2
. (18)
Linearising Lt+1(θ)around ˜θt(γt)and optimizing (16) for θleads to (see Appendix I.2)
θt+1=˜θt(γt)−˜αt(γt)◦ ∇θLt+1(˜θt(γt)), (19)
where ◦is element-wise multiplication. For γt= 1, we recover the ordinary SGD update, while
the values γt<1move the starting point of the modified SGD closer to the initialization as
well as increase the learning rate. Algorithm 1 describes the full procedure. In Appendix C we
describe additional practical choices made for the Soft Resets algorithm. Similarly to the Bayesian
approach (15), we can do multiple updates on (16). We describe this Soft Resets Proximal algorithm
in Appendix I.2 and full procedure is given in Algorithm 3.
64 Related Work
Plasticity loss in Neural Networks. Our model shares similarities with reset-based approaches such
as Shrink & Perturb (S&P) [ 2] and L2-Init [ 33]; however, whereas we learn drift parameters from
data, these methods do not, leaving them vulnerable to mismatch between assumed non-stationarity
and the actual realized non-stationarity in the data. Continual Backprop [ 13] or ReDO [ 47] apply
resets in a data-dependent fashion, e.g. either based on utility or whether units are dead. But they
use hard resets, and cannot amortize the cost of removing entire features. Interpretation (12) ofγt
connects to the notion of parameters utility from [ 14], but this quantity is used to prevent catastrophic
forgetting by decreasing learning rate for high γt. Our method increases the learning rate for low γt
to maximize adaptability, and is not designed to prevent catastrophic forgetting.
Non-stationarity. Non-stationarity arises naturally in a variety of contexts, the most obvious being
continual and reinforcement learning. The structure of non-stationarity may vary from problem to
problem. At one extreme, we have a piece-wise stationary setting, for example a change in the
location of a camera generating a stream of images, or a hard update to the learner’s target network in
value-based deep RL algorithms. This setting has been studied extensively due to its propensity to
induce catastrophic forgetting [e.g. 31,45,51,10] and plasticity loss [13,39,38,34]. At the other
extreme, we can consider more gradual changes, for example due to improvements in the policy of
an RL agent [ 40,46,42,13] or shifts in the data generating process [ 36,55,20,53]. Further, these
scenarios might be combined, for example in continual reinforcement learning [31,1,13] where the
reward function or transition dynamics could change over time.
Non-stationary online convex optimization. Non-stationary prediction has a long history in online
convex optimization, where several algorithms have been developed to adapt to changing data [see,
e.g., 25,8,22,17,21,18,29]. Our approach takes an inspiration from these works by employing a
drift model as, e.g., [ 25,21] and by changing learning rate as [ 29,52]. Further, our OU drift model
bears many similarities to the implicit drift model introduced in the update rule of [ 25] (see also
[8,17]), where the predictive distribution is mixed with a uniform distribution to ensure the prediction
could change quickly enough if the data changes significantly, where in our case p0plays the same
role as the uniform distribution.
Bayesian approaches to non-stationary learning. A standard approach is Variational Continual
Learning [ 41], which focuses on preventing catastrophic forgetting and is an online version of “Bayes
By Backprop” [ 5]. This method does not incorporate dynamical parameter drift components. In [ 35],
the authors applied variational inference (VI) on non-stationary data, using the OU-process and
Bayesian forgetting, but unlike in our approach, their drift parameter is not learned. Further, in [ 49],
the authors considered an OU parameter drift model similar to ours, with an adaptable drift scalar
γand analytic Kalman filter updates, but is applied over the final layer weights only, while the
remaining weights of the network were estimated by online SGD. In [ 28], the authors propose to deal
with non-stationarity by assuming that each parameter is a finite sum of random variables following
different OU process. They derive VI updates on the posterior of these variables. Compared to this
work, we learn drift parameters for every NN parameter rather than assuming a finite set of drift
parameters. A different line of research assumes that the drift model is known and use different
techniques to estimate the hidden state (the parameters) from the data: in [ 9], the authors use Extended
Kalman Filter to estimate state and in [ 3], they propagate the MAP estimate of the hidden state
distribution with Kgradient updates on a proximal objective similar to (43), whereas in Bayesian
Online Natural Gradient (BONG) [ 27], the authors use natural gradient for the variational parameters.
5 Experiments
Soft reset methods. There are multiple variations of our method. We call the method implemented
by Algorithm 1 with 1gradient update on the drift parameter Soft Reset , while other versions show
different parameter choices: Soft Reset (Kγ= 10 ) is a version with 10updates on the drift parameter,
while Soft Reset (Kγ= 10 ,Kθ= 10 ) is the method of Algorithm 3 in Appendix I.2 with 10updates
on drift parameter, followed by 10updates on NN parameters. Bayesian Soft Reset (Kγ= 10 ,
Kθ= 10 ) is a method implemented by Algorithm 2 with 10updates on drift parameter followed
by10updates on the mean µtand the variance σ2
t(uncertainty) for each NN parameter. Bayesian
method performed the best overall but required higher computational complexity (see Appendix E).
Unless specified, γtis shared for all the parameters in each layer (separately for weight and biases).
70 50 100 150 200
T ask id0.650.700.750.800.85Average T ask AccuracyPermuted MNIST
0 50 100 150 200
T ask id0.20.40.60.8Average T ask AccuracyRandom Label MNIST -- Data Efficient
Online SGD
L2 initHard Reset
Shrink and PerturbSoft Reset
Bayesian Soft Reset K=10, K=10,  per parameter
0 50 100 150 200
T ask id0.20.40.60.81.0Average T ask AccuracyRandom Label CIFAR-10 -- MemorizationFigure 2: Plasticity benchmarks. Left: performance on permuted MNIST .Center: performance on
random-label MNIST (data efficient). Right: performance on random-label CIFAR-10 (memoriza-
tion). The x-axis is the task id and the y-axis is the per-task training accuracy (25).
0 50 100 150 200
T ask id0.650.700.750.800.85Average T ask AccuracyPermuted MNIST
0 50 100 150 200
T ask id0.20.40.60.8Average T ask AccuracyRandom Label MNIST -- Data Efficient
Soft Reset
Soft Reset K=10
Soft Reset Proximal K=10, K=10
Bayesian Soft Reset K=10, K=10
Bayesian Soft Reset K=10, K=10,  per parameter
0 50 100 150 200
T ask id0.20.40.60.81.0Average T ask AccuracyRandom Label CIFAR-10 -- Memorization
Figure 3: Different variants of Soft Resets .Left: performance on permuted MNIST .Center:
performance on random-label MNIST (data efficient). Right: performance on random-label CIFAR-
10(memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (25).
Loss of plasticity. We analyze the performance of our method on plasticity benchmarks [34,39,38].
Here, we have a sequence of tasks, where each task consists of a fixed (for all tasks) subset of 10000
images images from either CIFAR-10 [ 32] or MNIST, where either pixels are permuted or the label
for each image is randomly chosen. Several papers [ 34,39,38] study a memorization random-label
setting where SGD can perfectly learn each task from scratch. To highlight the data-efficiency of our
approach, we study the data-efficient setting where SGD achieves only 50% accuracy on each task
when trained from scratch. Here, we expect that algorithms taking into account similarity in the data,
to perform better. To study the impact of the non-stationarity of the input data, we consider permuted
MNIST where pixels are randomly permuted within each task (the same task as considered by 34).
As baselines, we use Online SGD andHard Reset at task boundaries. We also consider L2 init [34],
which adds L2penalty ||θ−θ0||2to the fixed initialization θ0as well as Shrink&Perturb [2], which
multiplies each parameter by a scalar λ≤1and adds random Gaussian noise with fixed variance σ.
See Appendix D.1 for all details. As metrics, we use average per-task online accuracy (25), which is
At=1
NPN
i=1at
i,
where at
iare the online accuracies collected on the task tviaNtimesteps, corresponding to the
duration of the task. In Figure 5, we also use average accuracy over all Ttasks, i.e.
AT=1
TPT
t=1At
The results are provided in Figure 2. We observe that Soft Reset is always better than Hard Reset
and most baselines despite the lack of knowledge of task boundaries. The gap is larger in the data
efficient regime. Moreover, we see that L2 Init only performs well in the memorization regime, and
achieves comparable performance to Hard Reset in the data efficient one. The method L2 Init could
be viewed as an instantiation of our Soft Reset Proximal method optimizing (16) withγt= 0at every
step, which is sub-optimal when there is similarity in the data. Bayesian Soft Reset demonstrates
significantly better performance overall, see also discussion below.
In Figure 3, we compare different variants of Soft Reset . We observe that adding more compute for
estimating γt(thus, estimating non-stationarity, Kγ= 10 ) as well as doing more updates on NN
81 2 3 4 5
MLP Layer0.650.700.750.800.850.90Minimum 
(a) Minimum t encountered
MNIST CIFAR-10020000 40000 60000 80000 100000
t0.50.60.70.80.91.0(b) First layer t for Random Label MNIST
0100000 200000 300000 400000 500000 600000
t0.50.60.70.80.91.0(c) First layer t for Random Label CIFAR-10
Figure 4: Left: the minimum encountered γtfor each layer on random-label MNIST and CIFAR-10.
Center: the dynamics of γton the first 20 tasks on MNIST. Right: the same on CIFAR-10.
parameters (thus, more accurately adapting to non-staionarity, Kθ= 10 ) leads to better performance.
All variants of Soft Reset γtparameters are shared for each NN layer, except for the Bayesian method.
This variant is able to take advantage of a more complex per-parameter drift model, while other
variants performed considerably worse, see Appendix H.4. We hypothesize this is due to the NN
parameters uncertainty estimates σtwhich Bayesian method provide, while others do not, which
leads to a more accurate drift model estimation, since uncertainty is used in this update (10). But,
this approach comes at a higher computational cost, see Appendix E. In Appendix H, we provide
ablations of the structure of the drift model, as well as of the impact of learning the drift parameter.
Qualitative behavior of Soft Resets .ForSoft Reset , we track the values of γtfor the first MLP
layer when trained on random-label tasks studied above (only 20tasks), as well as the minimum
encountered value of γtfor each layer, which highlights the maximum amount of resets. Figure 4b,c
shows γtas a function of t, and suggests that γtaggressively decreases at task boundaries (red dashed
lines). The range of values of γtdepends on the task and on the layer, see Figure 4a. Overall, γt
changes more aggressively for long duration (memorization) random-label CIFAR-10 and less for
shorter (data-efficient) random-label MNIST. See Appendix H.2 for more detailed results.
To study the behavior of Soft Reset under input distribution non-stationarity, we consider a variant of
Permuted MNIST where each image is partitioned into patches of a given size. The non-stationarity
is controlled by permuting the patches (not pixels). Figure 5a shows the minimum encountered γt
for each layer for different patch sizes. As the patch size increases and the problem becomes more
stationary, the range of values for γtis less aggressive. See Appendix H.3 for more detailed results.
Impact of non-stationarity. We consider a variant of random-label MNIST where for each task, an
image has either a random or a true label. The label assignment is kept fixed throughout the task
and is changed at task boundaries. We consider cases of 20%,40% and60% of random labels and
we control the duration of each task (number of epochs). In total, the stream contains 200tasks.
In Figure 5b, we show performance of Online SGD ,Hard Reset and in Figure 5c, the one of Soft
Reset and of Bayesian Soft Reset . See Appendix D.2 for more details. The results suggest that for
the shortest duration of the tasks, the performance of all the methods is similar. As we increase the
duration of each of the task (moving along the x-axis), we see that both Soft Resets variants perform
better than SGD and the gap widens as the duration increases. This implies that Soft Resets is more
effective with infrequent data distribution changes. We also observe that Bayesian method performs
better in all the cases, highlighting the importance of estimating uncertainty for NN parameters.
5.1 Reinforcement learning
Reinforcement learning experiments. We conduct Reinforcement Learning (RL) experiments in the
highly off-policy regime, similarly to [ 43], since in this setting loss of plasticity was observed. We ran
SAC [19] agent with default parameters from Brax [ 15] on the Hopper -v5 and Humanoid -v4 GYM [ 6]
environments (from Brax [ 15]). To reproduce the setting from [ 43], we control the off-policyness of
the agent by setting the off-policy ratio Msuch that for every 128environment steps, we do 128M
gradient steps with batch size of 256on the replay buffer. As baselines we consider ordinary SAC,
hard-coded Hard Reset where we reset all the parameters K= 5times throughout training (every
200000 steps), while keeping the replay buffer fixed (similarly to [ 43]). We employ our Soft Reset
method as follows. After we have collected fresh data from the environment, we do one gradient
update on γt(shared for all the parameters within each layer) with batch size of 128on this new
chunk of data and the previously collected one, i.e., two chunks of data in total. Then we initialize
˜θt(γt)and we employ the update rule (43) where the regularization ˜θt(γt)is kept constant for all the
off-policy gradient updates on the replay buffer. See Appendix D.3 for more details.
9conv2_d_1 conv2_d_2 conv2_d_3 conv2_d_4linearlinear_10.8250.8500.8750.9000.9250.9500.975Minimum t encountered
(a) Permuted Patch MNIST
Permuted patch size
1 2 4 7 14
30 100 400
Number of epochs per task0.20.40.60.81.0Average accuracy over all tasks
(b) Online SGD and Hard Reset performance
Methods
Online SGD Soft Resets Bayesian Soft Resets (K=10,K=10,  per parameter)
 Hard ResetRandom labels %
20 40 6030 100 400
Number of epochs per task0.20.40.60.81.0Average accuracy over all tasks
(c) Soft Resets (Bayeisan / non-Bayesian) performance
Methods
Online SGD Soft Resets Bayesian Soft Resets (K=10,K=10,  per parameter)
 Hard ResetFigure 5: (a)the x-axis denotes the layer, the y-axis denotes the minimum encountered γtfor each
convolutional and fully-connected layer when trained on permuted Patches MNIST, color is the patch
size. The impact of non-stationarity on performance on random-label MNIST of Online SGD and
Hard Reset is shown in (b)while the one of Soft Resets is shown in (c). The x-axis denotes the
number of epochs each task lasts, while the marker and line styles denote the percentage of random
labels within each task, circle (solid) represents 20%, rectangle(dashed) 40%, while rhombus (dashed
and dot) 60%. The y-axis denotes the average performance (over 3seeds) on the stream of 200tasks.
0200040006000Humanoid. Replay Ratio -- 1.0 Humanoid. Replay Ratio -- 32.0 Humanoid. Replay Ratio -- 128.0
0.0 0.2 0.4 0.6 0.8 1.0
1e60200040006000Hopper. Replay Ratio -- 1.0
Soft Reset Baseline Hard Reset0.0 0.2 0.4 0.6 0.8 1.0
1e6Hopper. Replay Ratio -- 32.0
0.0 0.2 0.4 0.6 0.8 1.0
1e6Hopper. Replay Ratio -- 128.0
Figure 6: RL results. First row is humanoid, second is hopper. Each column corresponds to different
replay ratio. The x-axis is the number of total timesteps, the y-axis the average reward. The shaded
area denotes the standard deviation across 3random seeds and the solid line indicates the mean.
The results are given in Figure 6. As the off-policy ratio increases, Soft Reset becomes more efficient
than the baselines. This is consistent with our finding in Figure 5b,c, where we showed that the
performance of Soft Reset is better when the data distribution is not changing fast. Figure 8 in
Appendix D.3 shows the value of learned γt. It shows γtmostly change for the value function and
not for the policy indicating that the main source of non-stationarity comes from the value function.
6 Conclusion
Learning efficiently on non-stationary distributions is critical to a number of applications of deep
neural networks, most prominently in reinforcement learning. In this paper, we have proposed a new
method, Soft Resets , which improves the robustness of stochastic gradient descent to nonstationarities
in the data-generating distribution by modeling the drift in Neural Network (NN) parameters. The
proposed drift model implements soft reset mechanism where the amount of reset is controlled by the
drift parameter γt. We showed that we could learn this drift parameter from the data and therefore
we could learn when andhow far to reset each Neural Network parameter. We incorporate the drift
model in the learning algorithm which improves learning in scenarios with plasticity loss. The variant
of our method which models uncertainty in the parameters achieves the best performance on plasticity
benchmarks so far, highlighting the promise of the Bayesian approach. Furthermore, we found that
our approach is particularly effective either on data distributions with a lot of similarity or on slowly
changing distributions. Our findings open the door to a variety of exciting directions for future work,
such as investigating the connection to continual learning and deepening our theoretical analysis of
the proposed approach.
10References
[1]Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C. Machado. Loss of
plasticity in continual deep reinforcement learning, 2023.
[2] Jordan T. Ash and Ryan P. Adams. On warm-starting neural network training, 2020.
[3]Gianluca M. Bencomo, Jake C. Snell, and Thomas L. Griffiths. Implicit maximum a posteriori
filtering via adaptive optimization, 2023.
[4]Tudor Berariu, Wojciech Czarnecki, Soham De, Jorg Bornschein, Samuel Smith, Razvan
Pascanu, and Claudia Clopath. A study on the plasticity of neural networks. arXiv preprint
arXiv:2106.00042 , 2021.
[5]Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty
in neural network. In Francis Bach and David Blei, editors, Proceedings of the 32nd Inter-
national Conference on Machine Learning , volume 37 of Proceedings of Machine Learning
Research , pages 1613–1622, Lille, France, 07–09 Jul 2015. PMLR.
[6]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym, 2016.
[7]Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael I Jordan.
Streaming variational bayes. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q.
Weinberger, editors, Advances in Neural Information Processing Systems , volume 26. Curran
Associates, Inc., 2013.
[8]N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games . Cambridge University Press,
Cambridge, 2006.
[9]Peter G. Chang, Gerardo Durán-Martín, Alexander Y Shestopaloff, Matt Jones, and Kevin
Murphy. Low-rank extended kalman filtering for online learning of neural networks from
streaming data, 2023.
[10] Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artificial
Intelligence and Machine Learning , 12(3):1–207, 2018.
[11] Hugh Dance and Brooks Paige. Fast and scalable spike and slab variable selection in high-
dimensional gaussian processes, 2022.
[12] Shibhansh Dohare, J. Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, Ashique Ru-
pam Mahmood, and Richard S. Sutton. Loss of plasticity in deep continual learning. Nature ,
632:768 – 774, 2024.
[13] Shibhansh Dohare, Richard S. Sutton, and A. Rupam Mahmood. Continual backprop: Stochastic
gradient descent with persistent randomness, 2022.
[14] Mohamed Elsayed and A. Rupam Mahmood. Addressing loss of plasticity and catastrophic
forgetting in continual learning, 2024.
[15] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier
Bachem. Brax - a differentiable physics engine for large scale rigid body simulation, 2021.
http://github.com/google/brax .
[16] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward
neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics , volume 9 of Proceedings of
Machine Learning Research , pages 249–256, Chia Laguna Resort, Sardinia, Italy, 13–15 May
2010. PMLR.
[17] A. György, T. Linder, and G. Lugosi. Efficient tracking of large classes of experts. IEEE
Transactions on Information Theory , IT-58(11):6709–6725, Nov. 2012.
11[18] András György and Csaba Szepesvári. Shifting regret, mirror descent, and matrices. In
Proceedings of The 33rd International Conference on Machine Learning , pages 2943–2951,
2016.
[19] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor, 2018.
[20] Raia Hadsell, Dushyant Rao, Andrei A. Rusu, and Razvan Pascanu. Embracing change:
Continual learning in deep neural networks. Trends in Cognitive Sciences , 24(12):1028–1040,
2020.
[21] Eric Hall and Rebecca Willett. Dynamical models and tracking regret in online convex pro-
gramming. In International Conference on Machine Learning , pages 579–587. PMLR, 2013.
[22] E. Hazan and C. Seshadhri. Efficient learning algorithms for changing environments. In Proc.
26th Annual International Conference on Machine Learning , pages 393–400. ACM, 2009.
[23] Elad Hazan. Introduction to online convex optimization, 2023.
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification, 2015.
[25] M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning , 32(2):151–178,
1998.
[26] Hemant Ishwaran and J. Sunil Rao. Spike and slab variable selection: Frequentist and bayesian
strategies. The Annals of Statistics , 33(2), April 2005.
[27] Matt Jones, Peter Chang, and Kevin Murphy. Bayesian online natural gradient (bong), 2024.
[28] Matt Jones, Tyler R. Scott, and Michael Curtis Mozer. Human-like learning in temporally
structured environments. In AAAI Spring Symposia , 2024.
[29] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-
learning methods, 2019.
[30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
[31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,
Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,
Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catas-
trophic forgetting in neural networks. Proceedings of the National Academy of Sciences ,
114(13):3521–3526, March 2017.
[32] Alex Krizhevsky. Learning multiple layers of features from tiny images. Tech-
nical report, University of Toronto, 2009. https://www.cs.toronto.edu/~kriz/
learning-features-2009-TR.pdf .
[33] Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-
parameterization inhibits data-efficient deep reinforcement learning, 2021.
[34] Saurabh Kumar, Henrik Marklund, and Benjamin Van Roy. Maintaining plasticity in continual
learning via regenerative regularization, 2023.
[35] Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt, and Stephan Günnemann.
Continual learning with bayesian neural networks for non-stationary data. In International
Conference on Learning Representations , 2020.
[36] Zhiqiu Lin, Jia Shi, Deepak Pathak, and Deva Ramanan. The clear benchmark: Continual
learning on real-world imagery. In Thirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track , 2021.
[37] Clare Lyle, Mark Rowland, and Will Dabney. Understanding and preventing capacity loss in
reinforcement learning, 2022.
12[38] Clare Lyle, Zeyu Zheng, Khimya Khetarpal, Hado van Hasselt, Razvan Pascanu, James Martens,
and Will Dabney. Disentangling the causes of plasticity loss in neural networks, 2024.
[39] Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will
Dabney. Understanding plasticity in neural networks, 2023.
[40] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013.
[41] Cuong V . Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual
learning. In International Conference on Learning Representations , 2018.
[42] Evgenii Nikishin, Junhyuk Oh, Georg Ostrovski, Clare Lyle, Razvan Pascanu, Will Dabney,
and André Barreto. Deep reinforcement learning with plasticity injection, 2023.
[43] Evgenii Nikishin, Max Schwarzer, Pierluca D’Oro, Pierre-Luc Bacon, and Aaron Courville.
The primacy bias in deep reinforcement learning, 2022.
[44] Neal Parikh and Stephen Boyd. Proximal algorithms. Found. Trends Optim. , 1(3):127–239, jan
2014.
[45] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter.
Continual lifelong learning with neural networks: A review. Neural Networks , 113:54–71, 2019.
[46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms, 2017.
[47] Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci. The dormant neuron
phenomenon in deep reinforcement learning. In Andreas Krause, Emma Brunskill, Kyunghyun
Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the
40th International Conference on Machine Learning , volume 202 of Proceedings of Machine
Learning Research , pages 32145–32168. PMLR, 23–29 Jul 2023.
[48] Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci. The dormant neuron
phenomenon in deep reinforcement learning, 2023.
[49] Michalis K. Titsias, Alexandre Galashov, Amal Rannen-Triki, Razvan Pascanu, Yee Whye Teh,
and Jorg Bornschein. Kalman filter for online classification of non-stationary data, 2023.
[50] G. E. Uhlenbeck and L. S. Ornstein. On the theory of the brownian motion. Phys. Rev. ,
36:823–841, Sep 1930.
[51] Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint
arXiv:1904.07734 , 2019.
[52] Tim van Erven, Wouter M. Koolen, and Dirk van der Hoeven. Metagrad: Adaptation using
multiple learning rates in online learning. Journal of Machine Learning Research , 22(161):1–61,
2021.
[53] Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander
Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi,
Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias,
Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, and Gido M. van de Ven.
Continual learning: Applications and the road forward, 2024.
[54] Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub ´Swi ˛ atkowski, Linh Tran, Stephan
Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is
the bayes posterior in deep neural networks really?, 2020.
[55] Runtian Zhai, Stefan Schroedl, Aram Galstyan, Anoop Kumar, Greg Ver Steeg, and Pradeep
Natarajan. Online continual learning for progressive distribution shift (OCL-PDS): A practi-
tioner’s perspective, 2023.
[56] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the Twentieth International Conference on International Conference on Machine
Learning , ICML’03, page 928–935. AAAI Press, 2003.
13NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We outline main contributions of the paper in the introduction and abstract.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss limitations in the experimental section
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
14Justification:
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We disclose the experimental information in Experimental and Appendix
sections.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
15Answer: [No]
Justification: Unfortunately, due to IP constrains, we cannot release the code for the paper.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide experimental details in the appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We specify that we report results with 3random seeds with mean and standard
deviation.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
16•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide information about compute resources required in the appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Based on our understanding, our work conforms to the every aspect of NeurIPS
Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
17•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We cite the works which introduced the publicly available datasets
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
18•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
19A Ornstein-Uhlenbeck process
We make use that the Ornstein-Uhlenbeck process [ 50] defines a SDE that can be solved explicitly
and written as a time-continuous Gaussian Markov process with transition density
p(xt|xs) =N(xse−(t−s),(1−e−2(t−s))σ2
0I),
for any pair of times t > s . Based on this as a drift model for the parameters θt(soθtis the state xt)
we use the conditional density
p(θt+1|θt) =N(θtγt,(1−γ2
t)σ2
0I),
where γt=e−δtandδt≥0corresponds to the learnable discretization time step. In other words,
by learning γtonline we equivalently learn the amount of a continuous “time shift” δtbetween two
consecutive states in the OU process. This essentially models parameter drift since e.g. if γt= 1,
thenδt= 0and there is no “time shift” which means that the next state/parameter remains the same
as the previous one, i.e. θt+1=θt.
B Other choices of drift model
In this section, we discuss alternative choices of a drift model instead of (5).
Independent mean and variance of the drift. We consider the drift model where the mean and
the variance are not connected, i.e.,
p(θt+1|θt, γt, βt) =N(θt+1;γtθt+ (1−γt)µ0;β2
t), (20)
where γt∈[0,1]is the parameters controlling the mean of the distribution and βtis the learned
variance. When βis fixed, this would be similar to our experiment in Figure 16 where we assumed
known task boundaries and we do not estimate the drift parameters but assume it as a hyperparameter.
Figure 16, left corresponds to the case when βtis a fixed parameter independent from γtwhereas
Figure 16, right corresponds to the case when βt=p
1−γ2
tσ0, i.e., when we use the drift model (5).
We see from the results, using drift model (5)leads to a better performance. In case when βtare
learned, estimating the parameters of this model will likely overfit to the noise since there is a lot of
degrees of freedom.
Shrink & Perturb [ 2].When we do not use the mean of the initialization, we can use the following
drift model
p(θt+1|θt, λt, βt) =N(θt+1;λtθt;β2
t)
Similarly to the case of (20), estimating both parameters λtandβtfrom the data will likely overfit to
the noise.
Arbitrary linear model. We can use the arbitrary linear model of the form
p(θt+1|θt, At, Bt) =N(θt+1;Atθt;Bt),
but estimating the parameters AtandBthas too many degrees of freedom and will certainly overfit.
Gaussian Spike & Slab We consider a Gaussian [11] approximation to Spike & Slab [26] prior
p(θt+1|θt, γt) =γtp(θt+1|θt) + (1 −γt)p0(θt+1),
which is a mixture of two distributions - a Gaussian p(θt+1|θt) =N(θt+1;θt, σ2)centered around
the previous parameter θtand an initializing distribution p0(θt+1) =N(θt+1;µ0, σ2
0). This model,
however, implements the mechanism of Hard reset as opposed to the soft ones. Moreover, estimating
such a model and incorporating it into a learning update is more challenging since the mixture
of Gaussian is not conjugate with respect to a Gaussian which will make the KL term (34) to be
computed only approximately via Monte Carlo updates.
C Practical implementations of the drift model estimation
Stochastic approximation for drift parameters estimation In practice, we use M= 1, which
leads to the stochastic approximationR
p(yt+1|xt+1, µt(γk
t) +ϵσt(γk
t))N(ϵ; 0, I)dϵ≈p 
yt+1|xt+1, µt(γk
t) +ϵσt(γk
t)
(21)
20Using NN initializing distribution. In the drift model (5), we assume that the initial distribution
over parameters is given by p0(θ) =N(θ;µ0;σ2
0). In practice, we have access to the NN initializer
pinit(θ) =N(θ; 0;σ2
0)where µ0= 0 (for most of the NNs). This means that we can replace ϵ
from (10) by1
σ0θ′
0where θ′
0∼pinit(θ). This means that the term in (21) can be replaced by
p 
yt+1|xt+1, µt(γk
t) +ϵσt(γk
t)
=p
yt+1|xt+1, µt(γk
t) +θ′
0q
1−γ2
t+γ2
tσ2
t
σ2
0
, (22)
where we used the fact that σ2
t(γt) =γ2
tσ2
t+ (1−γ2
t)σ2
0. Note that in (22), we only need to know
the ratioσ2
σ2
0rather than both of these. We will see that in Section 3.5, only this ratio is used for the
underlying algorithm. Finally, in practice, we can tie p0(θ)to the specific initialization θ0∼pinit(θ).
It was observed empricially [ 34] that using a specific initialization in gradient updates led to better
performance than using samples from the initial distribution. This would imply that
p0(θ) =N(θ;θ0,˜σ2
0), (23)
with˜σ2
0=p2σ2
0. The parameter p≤1accounts for the fact that the distribution p0(θ)should have
lower than pinit(θ)variance since it uses the specific initializaiton from the prior. This modification
would imply the following modification on the drift model term (22)
p 
yt+1|xt+1, µt(γk
t) +ϵσt(γk
t)
=p
yt+1|xt+1, µt(γk
t) +θ′
0pq
1−γ2
t+γ2
tσ2
t
σ2
0
(24)
D Experimental details
D.1 Plasticity experiments
Tasks In this section we provide experimental details. As plasticity tasks, we use a randomly
selected subset of size 10000 from CIFAR-10 [ 32] and from MNIST. This subset is fixed for all
the tasks. Within each task, we randomly permute labels for every image; we call such problems
random-label classification problems. We study two regimes – data efficient , where we do 400epochs
on a task with a batch size of 128, and memorization , a regime where we do only 70epochs with a
batch size of 128. As the main backbone architecture, we use MLP with 4hidden layers each having
a hidden dimension of 256hidden units. We use ReLU activation function and do not use any batch
or layer normalization. For the incoming data, we apply random crop, for MNIST to produce images
of size 24×24and for CIFAR-10 to produce images of size 28×28. We normalize images to be
within [0,1]range by dividing by 255. On top of that, we consider permuted MNIST task with a
similar training ragime as in [ 34] – we consider a subset of 10000 images, with batch size 16and
each task is one epoch. As a backbone, we still use MLP with ReLU activation and 4hidden layers.
Moreover, we considered permuted Patch MNIST , where we permute patches, not individual pixels.
In this case, we used a simple 4 layer convolutional neural network with 2 fully connected layers at
the end.
Metrics We use online accuracy as first metric with results reported in Appendix H. Moreover we
useper-task Average Online Accuracy which is
At=1
NNX
i=1at
i, (25)
where at
iare the online accuracies collected on the task tviaNtimesteps.
Baselines First baseline is Online SGD which sequentially learns over the sequence of task, with a
fixed learning rate. Hard Reset is the Online SGD which resets all the parameters at task boundaries.
L2 init [34] adds a regularizer λ||θ−θ0||2term to each Online SGD update where the regularization
strength λis a hyperparameter. Shrink & Perturb applies the transformation λθt+σϵ, ϵ∼ N(ϵ; 0, I)
to each parameter before the gradient update. The hyperparameters are λandσ.
Soft Reset corresponds to one update (10) starting from 1using 1Monte Carlo estimate. We always
use1Monte Carlo estimate for updating γtas we found that it worked well in practice on these tasks.
The hyperparameters of the method – σ2
0initial variance of the prior, which we set to be equal to
21p21
Nwhere Nis the width of the hidden layer and pis a constant (hyperparameter). It always equals
top= 0.1. On top of that the second hyperparameter is s, such that σt=sσ0, which controls the
relative decrease of the constant posterior variance. This is the hyperparameter over which we sweep
over. Another hyperparameter is the learning rate for learning γt. For Soft Reset Proximal , we also
have a proximal coefficient regularization constant λ. Besides that, we also sweep over the learning
rate for the parameter. For the Bayesian Soft Reset , we just add an additional learning rate for the
variance ασand we do 1Monte Carlo sample for each ELBO update.
Hyper parameters selection and evaluation For all the experiments, we run a sweep over the
hyperparameters. We select the best hyperparameters based on the smallest cumulative error (sum
of all 1−at
ithroughout the training). We then report the mean and the standard deviation across 3
seeds in all the plots.
Hyperparameter ranges . Learning rate αwhich is used to update parameters, for all the methods,
is selected from {1e−4,5e−4,1e−3,5e−3,1e−2,5e−2,1e−1,5e−1,1.0}. The λinitparameter in
L2 Init , is selected from {10.0,1.0,0.0,1e−1,5e−1,1e−2,5e−2,1e−3,5e−3,1e−4,5e−4,1e−
5,5e−5,1e−6,5e−6,1e−7,5e−7,1e−8,5e−8,1e−9,5e−9,1e−10,}. For S&P, the shrink
parameter λis selected from {1.0,0.99999 ,0.9999,0.999,0.99,0.9,0.8,0.7,0.5,0.3,0.2,0.1}, and
the perturbation parameter σis from {1e−1,1e−2,1e−3,1e−4,1e−5,1e−6}. As noise
distribution, we use the Neural Network initial distribution. For Soft Resets , the learning rate
forγtis selected from {0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001}, the constant sis se-
lected from {1.0,0.95,0.9,0.8,0.7,0.6,0.5,0.3,0.1}, the proximal cost ˜λin(41) is selected from
{1.0,0.1,0.01}, the same is true for the proximal cost in the Bayesian method (38). On top of that
for the Bayesian method, we always use p(see Algorithm 2) equal to p= 0.05ands= 0.9, i.e. the
posterior is always slightly smaller than the prior. Finally for the Bayesian method we had to learn
the variance with learning rate from {0.01,0.1,1,10}range.
In practice, we found that there is one learning rate of 0.1, which was always the best in practice
for most of the methods and only proximal Soft Resets onmemorization CIFAR-10 required smaller
learning rate 0.01. This allowed us to significantly reduce the hyperparameter sweep.
D.2 Impact of non-stationarity experiments
In this experiment, we consider a subset of 10000 images from MNIST (fixed throughtout all
experiment) and a sequence of tasks. Each task is constructed by assigning either a true or a random
label to each image from MNIST, where the probability of assignment is controlled by the experiment.
The duration of each is controlled by the number of epochs with batch size of 128. As backbone we
use MLP with 4hidden layers and 256hidden units and ReLU activation. For all the methods, the
learning rate is 0.1. For Soft Resets , we use s= 0.9andp= 1andηγ= 0.01. Bayesian method uses
proximal cost λ= 0.01. Detailed results are given in Figure 7.
D.3 Reinforcement learning experiments
We conduct experiments in the RL environments. We take the canonical implementation of Soft-Actor
Critic(SAC) from Brax [ 15] repo in github, which uses 2layer MLPs for both policy and Q-function.
It employs ReLU activation functions for both. On top of that, it uses 2MLP networks to parameterize
Q-function (see Brax [ 15]) for more details. To employ Soft Reset , we do the following. After we
have collected a chunk of data ( 128) time-steps, we do one update (10) onγtstarting from 1at
every update of γt, where γtis shared for all the parameters within each layer of a Neural Network,
separately for weights and biases. On top of that, since we have policy and value function networks,
we have separate γtfor each of these. After the update on γt, we compute θt(γt)andαt(γt), see
Section 3.5. After that, we employ the proximal objective (41) with a fixed regularization target
θt(γt). Concretely, we use the update rule (43) where for each update the gradient is estimate on
the batch of data from the replay buffer. This is not exactly the same as what we did with plasticity
benchmarks since there the update was applied to the same batch of data, multiple times. Nevertheless,
we found this strategy effective and easy to implement on top of a SAC algorithm. In practice, we
swept over the parameter s(similar for both, policy and the value function) which controls the
relative learning rate increase in (18). Moreover, we swept over the proximal regularization constant
˜λfrom eqn. (41), which was different for the policy and for the value function. In practice, we found
220 25 50 75 100 125 150 175 2000.00.20.40.60.81.020% of random labels | 30 epochs
0 25 50 75 100 125 150 175 20020% of random labels | 100 epochs
0 25 50 75 100 125 150 175 20020% of random labels | 400 epochs
0 25 50 75 100 125 150 175 2000.00.20.40.60.81.040% of random labels | 30 epochs
0 25 50 75 100 125 150 175 20040% of random labels | 100 epochs
0 25 50 75 100 125 150 175 20040% of random labels | 400 epochs
0 25 50 75 100 125 150 175 200
T ask id0.00.20.40.60.81.0T ask accuracy60% of random labels | 30 epochs
0 25 50 75 100 125 150 175 200
T ask idT ask accuracy60% of random labels | 100 epochs
0 25 50 75 100 125 150 175 200
T ask idT ask accuracy60% of random labels | 400 epochs
Methods
Online SGD Soft Resets Bayesian Soft Resets (K=10,K=10,  per parameter)
 Hard ResetMethods
Online SGD Soft Resets Bayesian Soft Resets (K=10,K=10,  per parameter)
 Hard ResetFigure 7: Non-stationarity impact . The x-axis denotes task id, each column denotes the duration,
whereas a row denotes the amount of label noise. Each color denotes the method studied. The y-axis
denotes average over 3 seeds online accuracy.
0.0 0.2 0.4 0.6 0.8 1.0
1e67891e5+9.999e 1
Replay Ratio = 1.0,  for policy
0.0 0.2 0.4 0.6 0.8 1.0
1e60.20.40.60.81.0Replay Ratio = 32.0,  for policy
0.0 0.2 0.4 0.6 0.8 1.0
1e60.40.60.81.0Replay Ratio = 128.0,  for policy
Layer # 0 : Weights
Layer # 1 : Biases
Layer # 1 : Weights
Layer # 1 : Biases
Layer # 2 : Weights
0.0 0.2 0.4 0.6 0.8 1.0
1e60.960.970.980.991.00Replay Ratio = 1.0,  for Q1
0.0 0.2 0.4 0.6 0.8 1.0
1e60.70.80.91.0Replay Ratio = 32.0,  for Q1
0.0 0.2 0.4 0.6 0.8 1.0
1e60.40.60.81.0Replay Ratio = 128.0,  for Q1
Layer # 0 : Weights
Layer # 1 : Biases
Layer # 1 : Weights
Layer # 1 : Biases
Layer # 2 : Weights
0.0 0.2 0.4 0.6 0.8 1.0
1e60.960.970.980.991.00Replay Ratio = 1.0,  for Q2
0.0 0.2 0.4 0.6 0.8 1.0
1e60.70.80.91.0Replay Ratio = 32.0,  for Q2
0.0 0.2 0.4 0.6 0.8 1.0
1e60.40.60.81.0Replay Ratio = 128.0,  for Q2
Layer # 0 : Weights
Layer # 1 : Biases
Layer # 1 : Weights
Layer # 1 : Biases
Layer # 2 : Weights
Figure 8: Visualization of the γtdynamics for the run on Humanoid environment. Each column
corresponds to the replay ratio studied. First row denotes the γtfor the policy π. The second and the
third rows denote the γtfor the two Q-functions.
that using proximal constant of 0for the policy led to the best empirical results. The range for the
proximal constants ˜λwas{0.1,0.01,0.001}and for swas{0.8,0.9,0.95,0.97,1.0}. We used p= 1
for all the experiments. For each experiment, we used a 3hours of the A100GPU with 40Gb of
memory.
23E Computational complexity
We provide the study of computational cost for all the proposed methods. Notations:
•Pbe the number of parameters in the Neural Network
•Lis the number of layers
•O(S)is the cost of SGD backward pass.
•Mγ- number of Monte Carlo samples for the drift model
•Mθ- number of Monte Carlo samples for the parameter updates (Bayesian Method).
•Kγ- number of updates for the drift parameter
•Kθ- number of NN parameter updates.
Method Comp. cost Memory
SGD O(S) O(P)
Soft resets γper layer O(KγMγS+S) O(L+ (Mγ+ 1)P)
Soft resets γper param. O(KγMγS+S) O(P+ (Mγ+ 1)P)
Soft resets γper layer + proximal ( Kθiters) O(KγMγS+KθS) O(L+ (Mγ+ 1)P)
Soft resets γper param. + proximal ( Kθiters) O(KγMγS+KθS) O(P+ (Mγ+ 1)P)
Bayesian Soft Reset Proximal ( Kθiters) γper layer O(KγMγS+ 2MθKθS)P(L+ (Mγ+ 2)P)
Bayesian Soft Reset Proximal ( Kθiters) γper param. O(KγMγS+ 2MθKθS)P(P+ (Mγ+ 2)P)
Table 1: Comparison of methods, computational cost, and memory requirements
The general theoretical cost of all the proposed approaches is given in Table 1. In practice, for all
the experiments, we assume that Mγ= 1 andMθ= 1. Moreover, we used Kγ= 1 andKθ= 1
forSoft Reset ,Kγ= 10 andKθ= 1forSoft Reset with more computation. On top of that, for Soft
Reset proximal and all Bayesian methods, we used Kγ= 10 andKθ= 10 . Table 2, quantifying the
complexity of all the methods from Figure 2.
Method Comp. cost Memory
SGD O(S) O(P)
Soft resets γper layer O(2S) O(L+ 2P)
Soft resets γper param. O(2S) O(3P)
Soft resets γper layer + proximal ( Kθ= 10 iters) O(20S) O(L+ 2P)
Soft resets γper param. + proximal ( Kθiters) O(20S) O(3P)
Bayesian Soft Reset Proximal ( Kθiters) γper layer O(30S) P(L+ 3P)
Bayesian Soft Reset Proximal ( Kθiters) γper param. O(30S) P(4)
Table 2: Comparison of methods, computational cost, and memory requirements for methods in
Figure 2.
The complexity O(2S)of Soft Resets comes from one update on drift parameter and one updat
eon NN parameters. The memory complexity requires storing O(L)parameters gamma (one for
each layer), parameters θtwithO(P)and sampled parameters for drift model update which requires
O(P).
Note that as Figure 9 suggests, it is beneficial to spend more computational cost on optimizing gamma
and on doing multiple updates on parameters. However, even the cheapest version of our method Soft
Resets still leads to a good performance as indicated in Figure 2.
The complexity of soft resets in reinforcement learning setting requires only one gradient update on
γafter each new chunk of fresh data from the environment. In SAC, we do Ggradient updates on
parameters for every new chunk of data. Assuming that complexity of one gradient update in SAC is
O(S), soft reset only requires doing one additional gradient update to fit γparameter.
The computation complexity of Soft Reset in Reinforcement Learning is marginally higher than SAC
but leads to better empirical performance in a highly off-policy regime, see Appendix D.3.
24Soft Reset
Soft Reset K=10
Soft Reset Proximal K=10, K=10
Bayesian Soft Reset K=10, K=10
Bayesian Soft Reset K=10, K=10,  per parameter
0.810.820.830.840.850.860.87Permutted MNIST
Soft Reset
Soft Reset K=10
Soft Reset Proximal K=10, K=10
Bayesian Soft Reset K=10, K=10
Bayesian Soft Reset K=10, K=10,  per parameter
0.550.600.650.700.750.800.85Random Label MNIST -- Data Efficient
Soft Reset
Soft Reset K=10
Soft Reset Proximal K=10, K=10
Bayesian Soft Reset K=10, K=10
Bayesian Soft Reset K=10, K=10,  per parameter
0.800.850.900.95Random Label CIFAR-10 -- memorizationFigure 9: Compute-performance tradeoff. The x-axis indicates the method going from the cheapest
(left) to the most expensive (right). See Table 2 for complexity analysis. The y-axis is the average
performance on all the tasks across the stream.
Method Comp. cost Memory
SAC O(GS) O(P)
Soft resets γper layer O(S+GS)O(L+ 2P)
Table 3: Comparison of methods, computational cost, and memory requirements for methods in for
RL.
F Sensitivity analysis
We study the sensitivity of Soft Resets where γis defined per layer when trained on random-label
MNIST (data efficient). We fix the learning rate to α= 0.1. We study the sensitivity of learning rate
for the drift parameter, ηγ, as well as p– initial prior standard deviation rescaling, and s– posterior
standard deviation rescaling parameter.
On top of that, we conduct the sensitivity analysis of L2 Init [ 34] and Shrink&Perturb [ 2] methods.
The x-axis of each plot denotes one of the studied hyperparameters, whereas y-axis is the average
performance across all the tasks (see Experiments section for tasks definition). The standard deviation
is reported over 3 random seeds. A color indicates a second hyperparameter which is studied, if
available. In the title of each plot, we write hyperparameters which are fixed. The analysis is provided
in Figure 10 for Soft Resets and in Figure 11 for the baselines.
The most important parameter is the learning rate of the drift model ηγ. For each method, there
exists a good value of this parameter and performance is sensitive to it. This makes sense since this
parameter directly impacts how we learn the drift model.
The performance of Soft Resets is robust with respect to the posterior standard deviation scaling s
parameter as long as it is s≥0.5. Fors <0.5, the performance degrades. This parameter is defined
from σt=sσ0and affects relative increase in learning rate given by1
γ2+(1−γ2)/s2)which could be
ill-behaved for small s.
We also study the sensitivity of the baseline methods. We find that L2 Init [ 34] is very sensitive to the
parameter λ, which is a penalty term for λ||θ−θ0||2. In fact, Figure 11, left shows that there is only
one good value of this parameter which works. Shrink&Perturb [ 2] is very sensitive to the shrink
parameter λ. Similar to L2 Init, there is only one value which works, 0.9999 while values 0.999 and
values 0.99999 lead to bad performance. This method however, is not very sensitive to the perturb
parameter σprovided that σ≤0.001.
25Compared to the baselines, our method is more robust to the hyperparameters choice. Below, we
also add sensitivity analysis for other method variants. Figure 12 shows sensitivity of Soft Resets ,
Kγ= 10 , Figure 13 shows sensitivity of Soft Resets ,Kγ= 10 ,Kθ= 10 , Figure 14 shows sensitivity
ofBayesian Soft Resets ,Kγ= 10 ,Kθ= 10 withγtper layer, Figure 15 shows sensitivity of
Bayesian Soft Resets ,Kγ= 10 ,Kθ= 10 withγtper parameter.
0.2 0.4 0.6 0.8 1.0
Posterior standard deviation scaling `s`0.10.20.30.40.50.6Average accuracySensitivity Soft-Reset (MNIST). Prior standard deviation scaling p=0.05
 Learning rate
0.0001
0.0005
0.001
0.005
0.01
0.05
0.1
0.5
0.2 0.4 0.6 0.8 1.0
Posterior standard deviation scaling `s`0.10.20.30.40.50.6Average accuracySensitivity Soft-Reset (MNIST). Drift learning rate =0.1
Prior std scaling p
0.01
0.05
0.1
Figure 10: Soft Reset , sensitivity analysis of performance with respect to the hyperparameters on
data-efficient random-label MNIST. The x-axis denotes the studied hyperparameter, whereas the
y-axis denotes the average performance across the tasks. The standard deviation is computed over
3 random seeds. The color indicates additional studied hyperparameter. (Left) shows sensitivity
analysis where the x-axis is the posterior standard deviation scaling sand the color indicates the
drift model learning rate ηγ.(Right) shows sensitivity of Soft Reset where the x-axis is the posterior
standard deviation scaling sand the color indicates initial prior standard deviation scaling p.
109
107
105
103
101
101
L2 term regularization cost `s`0.00.20.40.60.8Average accuracySensitivity L2 Init (MNIST). Learning rate =0.1
106
105
104
103
102
101
Perturb parameter ``
0.100.150.200.250.300.350.400.45Average accuracySensitivity Shrink and Perturb (MNIST). Learning rate =0.1
Shrink parameter 
0.9
0.99
0.999
0.9999
0.99999
1.0
Figure 11: L2 Init and Shrink&Perturb sensitivity analysis of performance with respect to the
hyperparameters on data-efficient random-label MNIST. The x-axis denotes the studied hyperparame-
ter, whereas the y-axis denotes the average performance across the tasks. The standard deviation is
computed over 3 random seeds. The color optionally indicates additional studied hyperparameter.
(Left) shows sensitivity of L2 Init with respect to the L2penalty regularization cost λapplied to
||θ−θ0||2term. We do not use an additional hyperparameter, therefore there is only one color.
(Right) shows sensitivity of Shrink&Perturb method where the x-axis is the perturb parameter σ
while the color indicates the shrink parameter λ.
G Proximal SGD
Each step of online SGD can be seen in terms of a regularized minimization problem referred to as
the proximal form [44]:
ˆθt+1= arg min θLt+1(θ) +1
2αt||θ−θt||2. (26)
In general, we cannot solve (26) directly, so we consider a Taylor expansion of Lt+1around θt,
giving
θt+1= arg min θ∇θLt+1(θt)⊤(θ−θt) +1
2αt||θt−θ||2. (27)
Here we see the role of αt>0as both enforcing that the Taylor expansion around θtis accurate, and
regularising θt+1towards the old parameters θt(hence ensuring that the learning from past data is
not forgotten). Solving (27) naturally leads to the well known SGD update:
θt+1=θt−αt∇θLt+1(θt),
where αtcan now also be interpreted as the learning rate.
260.2 0.4 0.6 0.8 1.0
Posterior standard deviation scaling `s`0.10.20.30.40.50.6Average accuracySensitivity Soft-Reset, K=10 (MNIST). Prior standard deviation scaling p=0.05
 Learning rate
0.0001
0.0005
0.001
0.005
0.01
0.05
0.1
0.5
0.2 0.4 0.6 0.8 1.0
Posterior standard deviation scaling `s`0.10.20.30.40.50.6Average accuracySensitivity Soft-Reset K=10 (MNIST). Drift learning rate =0.01
Prior std scaling p
0.01
0.05
0.1Figure 12: Soft Reset ,Kγ= 10 , sensitivity analysis of performance with respect to the hyperparam-
eters on data-efficient random-label MNIST. The x-axis denotes the studied hyperparameter, whereas
the y-axis denotes the average performance across the tasks. The standard deviation is computed
over 3 random seeds. The color indicates additional studied hyperparameter. (Left) shows sensitivity
analysis where the x-axis is the posterior standard deviation scaling sand the color indicates the drift
model learning rate ηγ.(Right) shows sensitivity analysis where the x-axis is the posterior standard
deviation scaling sand the color indicates initial prior standard deviation scaling p.
0.2 0.4 0.6 0.8 1.0
Posterior standard deviation scaling `s`0.10.20.30.40.50.60.70.8Average accuracySensitivity Soft-Reset, K=10, K heta=10 (MNIST). Prior standard deviation scaling p=0.05
 Learning rate
0.0001
0.0005
0.001
0.005
0.01
0.2 0.4 0.6 0.8 1.0
Posterior standard deviation scaling `s`0.10.20.30.40.50.60.70.8Average accuracySensitivity Soft-Reset, K=10, K heta=10 (MNIST). Drift learning rate =0.0001
Prior std scaling p
0.01
0.05
0.1
0.5
1.0
Figure 13: Soft Reset ,Kγ= 10 , Kθ= 10 , sensitivity analysis of performance with respect
to the hyperparameters on data-efficient random-label MNIST. The x-axis denotes the studied
hyperparameter, whereas the y-axis denotes the average performance across the tasks. The standard
deviation is computed over 3 random seeds. The color indicates additional studied hyperparameter.
(Left) shows sensitivity analysis where the x-axis is the posterior standard deviation scaling sand the
color indicates the drift model learning rate ηγ.(Right) shows sensitivity analysis where the x-axis
is the posterior standard deviation scaling sand the color indicates initial prior standard deviation
scaling p.
103
102
101
100
Prior standard deviation scaling `p`0.10.20.30.40.50.60.70.8Average accuracySensitivity Bayesian Soft-Reset,  per layer (MNIST). KL cost =0.01
 Learning rate
0.0001
0.0005
0.001
0.005
0.01
0.1
103
102
101
100
KL cost, 
0.10.20.30.40.50.60.70.8Average accuracySensitivity Bayesian Soft-Reset,  per layer (MNIST). Initial prior rescaling p=0.05
 Learning rate
0.0001
0.0005
0.001
0.005
0.01
0.1
Figure 14: Bayesian Soft Reset ,Kγ= 10 , Kθ= 10 with γtper layer, sensitivity analysis of
performance with respect to the hyperparameters on data-efficient random-label MNIST. The x-axis
denotes the studied hyperparameter, whereas the y-axis denotes the average performance across the
tasks. The standard deviation is computed over 3 random seeds. The color indicates additional studied
hyperparameter. (Left) shows sensitivity analysis where the x-axis is the prior standard deviation
initial scaling pand the color indicates the drift model learning rate ηγ.(Right) shows sensitivity
analysis where the x-axis is the KL divergence coefficient λwhile the color indicates the learning
rateηγ.
27103
102
101
100
Prior standard deviation scaling `p`0.20.40.60.81.0Average accuracySensitivity Bayesian Soft-Reset,  per parameter (MNIST). KL cost =0.01
 Learning rate
0.0001
0.0005
0.001
0.005
0.01
103
102
101
100
KL cost, 
0.00.20.40.60.81.0Average accuracySensitivity Bayesian Soft-Reset,  per parameter (MNIST). Initial prior rescaling p=0.05
 Learning rate
0.0001
0.0005
0.001
0.005
0.01Figure 15: Bayesian Soft Reset ,Kγ= 10, Kθ= 10 withγtper parameter, sensitivity analysis of
performance with respect to the hyperparameters on data-efficient random-label MNIST. The x-axis
denotes the studied hyperparameter, whereas the y-axis denotes the average performance across the
tasks. The standard deviation is computed over 3 random seeds. The color indicates additional studied
hyperparameter. (Left) shows sensitivity analysis where the x-axis is the prior standard deviation
initial scaling pand the color indicates the drift model learning rate ηγ.(Right) shows sensitivity
analysis where the x-axis is the KL divergence coefficient λwhile the color indicates the learning
rateηγ.
0 50 100 150 200
T ask id0.00.20.40.60.8Average T ask AccuracyPerfect Soft-Reset with constant l.r.
0 50 100 150 200
T ask idAverage T ask AccuracyPerfect Soft-Reset with higher l.r. at switch
Online SGD
Hard Reset
Hard Reset (only last)
Soft Reset
Soft Reset (=0.2)
Soft Reset (=0.4)
Soft Reset (=0.6)
Soft Reset (=0.8)
Figure 16: Perfect soft-resets ondata-efficient random-label MNIST. Left,Soft Reset method does
not use higher learning rate when γ <1.Right ,Soft Reset increases the learning rate when γ <1,
see(18). The x-axis represents task id, whereas the y-axis is the average training accuracy on the
task.
H Qualitative behavior of soft resets and additional results on Plasticity
benchmarks
H.1 Perfect Soft Resets
To understand the impact of drift model (5), we study the data efficient random-label MNIST setting
where task boundaries are known. We run Online SGD ,Hard Reset which resets all parameters at task
boundaries, and Hard Reset (only last) which resets only the last layer. We use Soft Reset method (19)
where γt= 1all the time and becomes γt= ˆγt(with manually chosen ˆγt) at task boundaries. We
consider constant learning rate αt(γt)and increasing learning rate (18) at task boundary for Soft
Reset . On top of that, we run Soft Reset method unaware of task boundaries which learns γt. We
report Average training task accuracy metric in Figure 16. See Appendix D.1 for details. The results
suggest that with the appropriate choice of ˆγt,Soft Reset is much more efficient than Hard Reset and
the effect becomes stronger if the learning rate αt(γt)increases. We also see that Soft Reset could
learn an appropriate γtwithout the knowledge of task boundary.
H.2 Qualitative Behaviour on Soft Resets on random-label tasks.
We observe what values of γtwe get as we train Soft Reset method on random-label MNIST (data-
efficient) and CIFAR-10 (memorization). The results are given in Figure 17 for MNIST and in
Figure 18 for CIFAR-10. We report these for the first 20tasks.
280 200000 400000 600000
Step0.50.60.70.80.91.0CIFAR-10 for linear
0 200000 400000 600000
Step for linear_1
0 200000 400000 600000
Step for linear_2
0 200000 400000 600000
Step for linear_3
0 200000 400000 600000
Step for linear_4
Figure 17: Behaviour of γtfor different layers on random-label MNIST (data efficient) for the first
20 tasks.
0 200000 400000 600000
Step0.50.60.70.80.91.0CIFAR-10 for linear
0 200000 400000 600000
Step for linear_1
0 200000 400000 600000
Step for linear_2
0 200000 400000 600000
Step for linear_3
0 200000 400000 600000
Step for linear_4
Figure 18: Behaviour of γtfor different layers on random-label CIFAR-10 (memorization) for the
first 20 tasks.
H.3 Qualitative Behaviour on Soft Resets on permuted patches of MNIST.
We consider a version of permuted MNIST where instead of permuting all the pixels, we permute
patches of pixels with a patch size varying from 1to14. The patch size of 1corresponds to permututed
MNIST and therefore the most non-stationary case, while patch size of 14corresponds to least non-
stationary case. We use a convolutional Neural Network in this case. In Figure 19, we report the
behavior of γfor different convolutional and fully connected layers on first few tasks.
H.4 Bayesian method is better than non-Bayesian
As discussed in Section 5, we found that in practice Soft Reset andSoft Reset Proximal where γis
learned per-parameter, did not perform well on the plasticity benchmarks. However, the Bayesian
variant described in Section I.1, actually benefited from specifying γfor every parameter in Neural
Network. We report these additional results in Figure 20. We see that the non Bayesian variants
where γtis specified per parameter, do not perform well. The fact that the Bayesian method performs
better here suggests that it is important to have a good uncertainty estimate σ2
tfor the update (10)
onγt. When, however, we regularize γtto be shared across all parameters within each layer, this
introduces useful inductive bias which mitigates the lack of uncertainty estimation in the parameters.
This is because for non-Bayesian methods, we assume that the uncertainty is fixed, given by a
hyperparameter – assumption which would not always hold in practice.
H.5 Qualitative behavior of soft resets
In this section, we zoom-in in the data-efficient experiment on random-label MNIST. We use Soft
Reset Proximal ( γper layer) method with separate γfor layer (different for each weight and for each
bias) and run it for 20tasks on random-label MNIST. In Figure 21 we show the online accuracy as we
learn over this sequence of tasks. In Figure 22, we visualize the dynamics of parameters γfor each
layer. First of all, we see that γtseems to accurately capture the task boundaries. Second, we see that
the amount by which each γtchanges depends on the parameter type – weights versus biases, and
it depends on the layer. The architecture in this setting starts form linear and goes up to linear 4,
which represent the 4MLP hidden layers with a last layer linear 4.
0 10000 20000 30000 40000
Step0.800.850.900.951.00 for conv2_d_1
Permuted patch size
1 2 4 7 140 10000 20000 30000 40000
Step0.800.850.900.951.00 for conv2_d_2
0 10000 20000 30000 40000
Step0.800.850.900.951.00 for conv2_d_3
0 10000 20000 30000 40000
Step0.800.850.900.951.00 for conv2_d_4
0 10000 20000 30000 40000
Step0.800.850.900.951.00 for linear
0 10000 20000 30000 40000
Step0.800.850.900.951.00 for linear_1
Figure 19: Behaviour of γtfor different layers on permuted MNIST
290 50 100 150 200
T ask id0.30.40.50.60.70.8Average T ask AccuracyRandom Label MNIST -- Data Efficient
Hard Reset
Soft Reset
Soft Reset Proximal
Soft Reset Proximal ( per parameter)
Soft Reset ( per parameter)
Bayesian Soft Reset Proximal
Bayesian Soft Reset Proximal ( per parameter)
0 50 100 150 200
T ask id0.40.60.81.0Average T ask AccuracyRandom Label CIFAR-10 -- MemorizationFigure 20: Performance of γper-parameter methods
0 20000 40000 60000 80000 100000
Time-step0.00.20.40.60.81.0Accuracy
Figure 21: Visualization of accuracy when trained on data efficient random-label MNIST task. The
dashed red lines correspond to a task boundary.
0 20000 40000 60000 80000 1000000.00014
0.00012
0.00010
0.00008
0.00006
0.00004
0.00002
0.00000+1  Per Layer linear b
0 20000 40000 60000 80000 1000000.00006
0.00004
0.00002
0.000000.000020.000040.000060.000080.00010+9.999e 1
  Per Layer linear_1 b
0 20000 40000 60000 80000 1000000.99920.99940.99960.99981.0000 Per Layer linear_2 b
0 20000 40000 60000 80000 1000000.999820.999840.999860.999880.999900.999920.999940.99996 Per Layer linear_3 b
0 20000 40000 60000 80000 1000004.04.55.05.56.01e5+9.999e 1
 Per Layer linear_4 b
0 20000 40000 60000 80000 1000000.99700.99750.99800.99850.99900.99951.0000 Per Layer linear w
0 20000 40000 60000 80000 1000000.99700.99750.99800.99850.99900.99951.0000 Per Layer linear_1 w
0 20000 40000 60000 80000 1000000.99750.99800.99850.99900.99951.0000 Per Layer linear_2 w
0 20000 40000 60000 80000 1000000.99650.99700.99750.99800.99850.99900.99951.0000 Per Layer linear_3 w
0 20000 40000 60000 80000 1000000.99650.99700.99750.99800.99850.99900.99951.0000 Per Layer linear_4 w
Figure 22: Visualization of γand task boundaries on data-efficient Random-label MNIST.
H.6 Impact of specific initialization
In this section, we study the impact of using specific initialization θ0∼pinit(θ)inp0(θ)as discussed
in Appendix C. Using the specific initialization in Soft Resets leads to fixing the mean of the p0(θ)to
beθ0, see (23). This, in turn, leads to the predictive distribution (24). In case when we are not using
specific initialization θ0, the mean of p0(θ)is0and the predictive distribution is given by (22). To
understand the impact of this design decision, we conduct an experiment on random label MNIST
with Soft Reset , where we either use the specific initialization or not. For each of the variants, we
do a hyperparameters sweep. The results are given in Figure 23. We see that both variants perform
similarly.
I Learning parameters with estimated drift models
In this section, we provide a Bayesian Neural Network algorithm to learn the distributions of NN
parameters when there is a drift in the data distribution. Moreover, we provide a MAP-like inference
300 50 100 150 2000.30.40.50.6
Soft Reset -- no specific init in p0()
Soft Reset -- with specific init in p0()
Figure 23: Impact of specific initialization θ0as a mean of p0(θ)inSoft Resets . The x-axis represents
task id. The y-axis represents the average task accuracy with standard deviation computed over 3
random seeds. The task is random label MNIST – data efficient.
algorithm which does not require to learn the distributions over parameters, but simply propagates
the MAP estimate over these.
I.1 Bayesian Neural Networks algorithm
In this section, we describe an algorithm for parameters update based on Bayesian Neural Networks
(BNN). It is based on the online variational Bayes setting described below.
Let the family of distributions over parameters be
Q={q(θ) :q(θ)∼DY
i=1N(θi;µi, σ2
i);θ= (θ1, . . . , θD)}, (28)
which is the family of Gaussian mean-field distributions over parameters θ∈RD(separate Gaussian
per parameter). For simplicity of notation, we omit the index i. Let Γt= (γ1, . . . , γ t)be the
history of observed parameters of the drift model and St={(x1, y1), . . . , (xt, yt)}be the history
of observed data. We denote by qt(θ)≜qt(θ|St,Γt−1)∈ Q the Gaussian approximate posterior at
timetwith mean µtand variance σ2
tfor every parameter. The approximate predictive look-ahead
prior is given by
qt(θ|γt) =R
qt(θt)p(θ|θt, γt)dθt=N(θ;µt(γt), σ2
t(γt)), (29)
that has parameters µt(γt) =γtµt+ (1−γt)µ0, σ2
t(γt) =γ2
tσ2
t+ (1−γ2
t)σ2
0. To see this, we
will use the law of total expectation and the law total variance. For two random variables XandY
defined on the same space, law of total expectation says
E[Y] =E[E[Y|X]]
and the law of total variance says
V[Y] =E[V[Y|X]] +V[E[Y|X]]
In our case, from the drift model (5), we have the conditional distribution
θ|θt=γtθt+ (1−γt)µ0+q
(1−γ2
t)σ2
0ϵ, ϵ∼ N(0;I) (30)
From (30), we have
E[θ|θt] =γtθt+ (1−γt)µ0
V[θ|θt] = (1 −γ2
t)σ2
0
From here, we have that the mean is given by
E[θ] =E[E[θ|θt]] =γtµt+ (1−γt)µ0 (31)
and the variance is given by
V[θ] =E[V[θ|θt]] +V[E[θ|θt]]
V[θ] = (1 −γ2
t)σ2
0+γ2
tθ2
t (32)
31Now, we note that qt(θ|γt)is a Gaussian and its parameters are given by E[θ] =γtµt+ (1−γt)µ0
from (31) and by V[θ] = (1 −γ2
t)σ2
0+γ2
tθ2
tfrom (32). Then, for new data (xt+1, yt+1)at time
t+ 1, the approximate predictive log-likelihood equals to
logqt(yt+1|xt+1, γt) = logR
p(yt+1|xt+1, θ)qt(θ|γt)dθ.
We are looking for a new approximate posterior qt+1(θ)such that
qt+1(θ) = arg min
qKL[q(θ)||p(yt+1|xt+1, θ)qt(θ|γt)] (33)
The optimization problem (33) can be written as minimization of the following loss
Ft(θ, γt) =Eq[Lt+1(θ)] +KL[q(θ)||qt(θ|γt)], (34)
sinceLt+1(θ) =−logp(yt+1|xt+1, θ). Using the fact that we are looking for a member q∈ Q
from (28), we can write the objective (34) as
Ft(µ, σ, γ t) =Eϵ∼N(0;I)[Lt+1(µ+ϵσ)] +KL[q(θ)||qt(θ|γt)],
where we used the reparameterisation trick for the loss term. We now expand the regularization term
to get
Ft(µ, σ, γ t) =Eϵ∼N(0;I)[Lt+1(µ+ϵσ)] +X
i"
(µi−µt,i(γt))2+σ2
i
2σ2
t,i(γt)−1
2logσ2
i#
(35)
Since the posterior variance of NN parameters may become small, the optimization of (35) may
become numerically unstable due to division by σ2
t,i(γt). It was shown [ 54] that using small
temperature on the prior led to better empirical results when using Bayesian Neural Networks, a
phenomenon known as cold posterior. Here, we define a temperature per-parameter, i.e., λt,i>0for
every time-step t, such that the objective above becomes
˜Ft(µ, σ, γ t;{λt,i}i) =Eϵ∼N(0;I)[Lt+1(µ+ϵσ)] +X
iλt,i"
(µi−µt,i(γt))2+σ2
i
2σ2
t,i(γt)−1
2logσ2
i#
(36)
As said above, it is common to use the same temperature λt,i=λfor all the parameters. In this work,
we propose the specific choice of the temperature to be
λt,i=λσ2
t,i, (37)
where λ >0is some globally chosen temperature parameter. This leads to the following objective
ˆFt(µ, σ, γ t;{rt, i}i) =Eϵ∼N(0;I)[Lt+1(µ+ϵσ)]+1
2X
irt,i
(µi−µt,i(γt))2+σ2
i−σ2
t,i(γt) logσ2
i
,
(38)
where the quantity rt,iis defined as
rt,i=σ2
t,i
σ2
t(γt)=σ2
t,i
γ2
tσ2
t,i+(1−γ2
t)σ2
0,
which represents the relative change in the posterior variance due to the drift. In the exact stationary
case, when γt= 1, this ratio is rt,i= 1while for γt<1, since typically σ2
t< σ2
0, we have rt,i<1.
This means that in the non-stationary case, the strength of the regularization in (38) in favor of the
data term Eϵ∼N(0;I)[Lt+1(µ+ϵσ)], allowing the optimization to respond faster to the change in the
data distribution. In practice, this data term is approximated via Monte-Carlo, i.e.
Eϵ∼N(0;I)[Lt+1(µ+ϵσ)]∼1
MMX
i=1Lt+1(µ+ϵiσ) (39)
To find new parameters, µt+1andσt+1, we let µ0
t+1=µt(γt)andσ0
t+1=σt(γt)and perform
multiple Kupdates on (38)
µk+1
t+1=µk
t+1−αµˆFt(µk
t+1, σk
t+1, γt,{rt, i}i), σk+1
t+1=σk
t+1−ασˆFt(µk
t+1, σk
t+1, γt,{rt, i}i),
where αµandασare corresponding learning rates. The full algorithm of learning the drift parameters
γtas well as learning the Bayesian Neural Network parameters using the procedure above is given in
Algorithm 2.
32Algorithm 2 Bayesian Soft-Reset algorithm
Input: Data-stream ST={(xt, yt)})T
t=1
Neural Network initial variance for every parameter σ2
0coming from standard NN library
NN initializer pinit(θ)
Proximal cost λ≥0.
Initial prior variance rescaling p∈[0,1].
Initial posterior variance rescaling f∈[0,1].
Learning rate for the mean αµand for the standard deviation ασ
Number of gradient updates Kθto be applied on µandσ
Number of Monte-Carlo samples Mθfor estimating µandσin (39)
Number of gradient updates Kγon drift parameter γtin (10)
Number of Monte-Carlo samples Mγto estimate γtin (11)
Learning rate ηγfor drift parameter
Initial drift parameters γ0= 1for every iteration.
Initialization :
Initialize NN parameters θ0∼pinit(θ)
Initialize prior distribution p0(θ) =N(θ; 0;p2σ2
0)to be used for drift model (5).
Initialize posterior q0(θ)to beN(θ;θ0;σ2
init), where σ2
init=f2p2σ2
0.
forstept= 0,1,2, . . . , T do
Current posterior qt=N(θ;µt, σ2
t)
For(xt+1, yt+1), predict ˆyt+1=f(xt+1|µt)with current posterior mean parameters µt
Compute performance metric based on (yt+1,ˆyt+1)
Estimating the drift
Initialize drift parameter γ0
t=γ0.
Compute µt(γt) =γtµtandσ2(γt) =γ2
tσ2
t+ (1−γ2
t)σ2
0
fork= 0, . . . , K γ−1do
γk+1
t=γk
tηγ∇γlog1
MγPMγ
i=1p(yt+1|xt+1, µt(γk
t) +ϵiσt(γk
t))
end for
Updating variational posterior
Letµ0
t+1=γtµt,σ0
t+1=p
γ2
tσ2
t+ (1−γ2
t)σ2
0
Letrt,i=σ2
t,i
σ2
t(γt)=σ2
t,i
γ2
tσ2
t,i+(1−γ2
t)σ2
0to be used in
fork= 0, . . . , K θ−1do
µk+1
t+1=µk
t+1−αµˆFt(µk
t+1, σk
t+1, γt,{rt,i}i, λ)
σk+1
t+1=σk
t+1−ασˆFt(µk
t+1, σk
t+1, γt,{rt,i}i, λ)
end for
end for
I.2 Modified SGD with drift model
Instead of propagating the posterior (6), we do MAP updates on (4)with the prior p0(θ) =
N(θ;µ0;σ2
0)and the posterior qt(θ) =N(θ;θt;s2σ2
0), where s≤1is hyperparameter control-
ling the variance σ2
tof the posterior qt(θ). Since fixed smay not capture the true parameters variance,
using Bayesian method (see Appendix I.1) is preferred but comes at a high computational cost.
Instead of Bayesian update (33), we consider maximum a-posteriori (MAP) update
max
θlogp(yt+1|xt+1, θ) + log qt(θ|γt),
withqt(θ|γt)given by (29). Denoting Lt+1(θ) = log p(yt+1|xt+1, θ)and using the definition of
qt(θ|γt), we get the following problem
max
θ−Lt+1(θ)−X
iλt,i"
(µi−µt,i(γt))2
2σ2
t,i(γt)#
, (40)
where similarly to (36), we use a per-parameter temperature λt,i≥0. We choose temperature to be
equal to
λt,i=s2σ2
0,iλ,
33Algorithm 3 Proximal Soft-Reset algoritm
Input: Data-stream ST={(xt, yt)})T
t=1
Neural Network (NN) initializing distribution pinit(θ)and specific initialization θ0∼pinit(θ)
Learning rate αtfor parameters and ηγfor drift parameters
Number of gradient updates Kγon drift parameter γt
Number of gradient updates Kθon NN parameters
Proximal term cost λ≥0
NN initial standard deviation (STD) scaling p≤1(see (23)) and ratio s=σt
pσ0.
forstept= 0,1,2, . . . , T do
For(xt+1, yt+1), predict ˆyt+1=f(xt+1|θt)
Compute performance metric based on (yt+1,ˆyt+1)
Initialize drift parameter γ0
t= 1
forstepk= 0,1,2, . . . , K γdo
Sample θ′
0∼pinit(θ)
Stochastic update (21) on drift parameter using specific initialization (24)
γk+1
t=γk
t+ηγ∇γh
logp(yt+1|xt+1, γtθt+ (1−γt)θ0+θ′
0pp
1−γ2
t+γ2
ts2)i
γt=γk
tend for
Initialize θ0
t+1=θt(γK
t)with (42) and use αt(γK
t) =αt
(γi
t)2+1−(γi
t)2
s2
with (44)
forstepk= 0,1,2, . . . , K θdo
θk+1
t+1=θk
t+1−αt(γt)◦ ∇θGt+1(θk
t+1;λ)
end for
end for
where λis some constant. Such choice of temperature is motivated by the same logic as in (37) – it
is a constant multiplied by the posterior variance σ2
t,i=s2σ2
0,i. With such choice of temperature,
maximizing (40) is equivalent to minimizing
G(θ;λ) =Lt+1(θ) +λ
2PD
i=1|θi−θi
t(γt)|2
rt,i(γ)(41)
where the regularization target for the dimension iis
θi
t(γi
t) =γi
tθi
t+ (1−γi
t)µi
0 (42)
and the constant rt,i(γ)is given by
rt,i(γt) =
(γi
t)2+1−(γi
t)2
s2
We can perform Kgradient updates (41) with a learning rate αtstarting from θ0
t+1=θt(γt),
θk+1
t+1=θk
t+1−αt(γt)◦ ∇θGt+1(θk
t+1;λ), (43)
where the vector-valued learning rate αt(γt)is given by
αt(γt) =αtrt,i(γt) =αt
(γi
t)2+1−(γi
t)2
s2
, (44)
withαtthe base learning rate. Note that doing one update is equivalent to modified SGD method (19).
Doing multiple updates on (43) allows us to perform multiple computations on the same data. The
corresponding algorithm is given in Algorithm 3.
J Proof of linearisation
Interpretation of γt.By linearising logp(yt+1|xt+1, θ)around µt, we can simplify (8) to get
F(γt) = (γt⊙µt+ (1−γt)⊙µ0)Tgt+1−0.5(σ2
t(γt)⊙gt+1)Tgt+1−λPK
i=1(γt,i−γ0
t,i)2,
where ⊙denotes elementwise product, gt=−∇L t+1(µt)is the negative gradient of the loss (1)
evaluated at µtand we added the ℓ2-penalty1
2λ(γt,i−γ0
t,i)2to take into account the initialization.
34Proof . We assume that the following linearisation is correct
logp(yt+1|xt+1, θ)∼logp(yt+1|xt+1, µt) +gT
t+1(θ−µt),
where
gt+1=−∇θlogp(yt+1|xt+1, θ=µt) =∇θLt+1(µt)
Then, we have
p(yt+1|xt+1, θ)∼p(yt+1|xt+1, µt) expgT
t+1(θ−µt)
Let’s write the integral from (8)
logZ
p(yt+1|xt+1, θ) exp−1
2(θ−µt(γt))TΣ−1
t(γt)(θ−µt(γt))dθ1p
(2π)D|Σt(γt)|=
logZ
p(yt+1|xt+1, µt) expgT
t+1(θ−µt)exp−1
2(θ−µt(γt))TΣ−1
t(γt)(θ−µt(γt))dθ1p
(2π)D|Σt(γt)|=
logp(yt+1|xt+1, µt) + logZ
expgT
t+1(θ−µt)exp−1
2(θ−µt(γt))TΣ−1
t(γt)(θ−µt(γt))dθ1p
(2π)D|Σt(γt)|
Consider only the exp term inside the integral:
gT
t+1(θ−µt)−1
2(θ−µt(γt))TΣ−1
t(γt)(θ−µt(γt)) =
gT
t+1θ−gT
t+1µt−1
2θTΣ−1
t(γt)θ+θTΣ−1
t(γt)µt(γt)−1
2µt(γt)TΣ−1
t(γt)µt(γt) =
−1
2θTΣ−1
tθ+θT(Σ−1
t(γt)µt(γt) +gt+1)−gT
t+1µt−1
2µt(γt)TΣ−1
t(γt)µt(γt) =
−1
2 
θTΣ−1
tθ−2θT(Σ−1
t(γt)µt(γt) +gt+1)
−gT
t+1µt−1
2µt(γt)TΣ−1
t(γt)µt(γt)
Let’s focus on this term
−1
2 
θTΣ−1
tθ−2θT(Σ−1
t(γt)µt(γt) +gt+1)
=
−1
2 
θTΣ−1
tθ−2θTΣ−1
tb(γt)
=
−1
2 
θTΣ−1
tθ−2θTΣ−1
tb(γt) +b(γt)TΣ−1
tb(γt)−b(γt)TΣ−1
tb(γt)
=
−1
2 
θTΣ−1
tθ−2θTΣ−1
tb(γt) +b(γt)TΣ−1
tb(γt)
+1
2b(γt)TΣ−1
tb(γt) =
−1
2(θ−b(γt))TΣ−1
t(θ−b(γt)) +1
2b(γt)TΣ−1
tb(γt)
where
b(γt) = Σ t(γt)
Σ−1
t(γt)µt(γt) +gt+1
Therefore, the integral could be written as
logZ
expgT
t+1(θ−µt)exp−1
2(θ−µt(γt))TΣ−1
t(γt)(θ−µt(γt))dθ1p
(2π)D|Σt(γt)|=
1
2b(γt)TΣ−1
tb(γt) + logZ
exp−1
2(θ−b(γt))TΣ−1
t(θ−b(γt))dθ1p
(2π)D|Σt(γt)|−gT
t+1µt−1
2µt(γt)TΣ−1
t(γt)µt(γt) =
1
2b(γt)TΣ−1
t(γt)b(γt)−gT
t+1µt−1
2µt(γt)TΣ−1
t(γt)µt(γt)
Now, we only keep the terms depending on γt
1
2b(γt)TΣ−1
t(γt)b(γt)−1
2µt(γt)TΣ−1
t(γt)µt(γt) =
1
2
Σ−1
t(γt)µt(γt) +gt+1TΣt(γt)
Σ−1
t(γt)µt(γt) +gt+1
−1
2µt(γt)TΣ−1
t(γt)µt(γt) =
1
2µt(γt)TΣ−1
t(γt)µt(γt) +gT
t+1µt(γt) +gT
t+11
2Σt(γt)gt+1−1
2µt(γt)TΣ−1
t(γt)µt(γt) =
gT
t+1µt(γt) +1
2gT
t+1Σt(γt)gt+1
35Since Σt(γt) =diag(σ2
tγ2
t+ (1−γ2
t)σ2
0), we recover
gT
t+1(γt⊙µt+ (1−γt)⊙µ0) +1
2gT
t+1 
(σ2
tγ2
t+ (1−γ2
t)σ2
0)⊙gt+1
Now, we add an l2-penaltyλ
2||γt−γ0
t,i||2and we get
F(γt) =gT
t+1(γt⊙µt+ (1−γt)⊙µ0) +1
2gT
t+1 
(σ2
tγ2
t+ (1−γ2
t)σ2
0)⊙gt+1
−λ
2||γt−γ0
t,i||2
Let’s take the gradient wrt γt, we get
∇F(γt) =gT
t+1(µt−µ0) +gT
t+1 
(σ2
tγt−σ2
0γt)⊙gt+1
−λ(γt−γ0
t,i) =
gT
t+1(µt−µ0) +λγ0
t,i−γt(λ+gT
t+1 
(σ2
0−σ2
t)⊙gt+1
= 0
Then
γt=gT
t+1(µt−µ0) +λγ0
t,i
λ+gT
t+1((σ2
0−σ2
t)⊙gt+1)
Ifγtis defined per parameter, this becomes
γt=gt+1(µt−µ0) +λγ0
t,i
λ+g2
t+1(σ2
0−σ2
t)
KToy illustrative example for SGD underperformance in the non-stationary
regime
Illustrative example of SGD on a non-stationary stream. We consider a toy problem of tracking a
changing mean value. Let the observations in the stream Stfollow yt=µt+σϵ, where ϵ∼ N(0,1),
σ= 0.01. Every 50timesteps the mean µtswitches from −2to2. We fit a 3-layer MLP with layer
sizes (10,5,1)and ReLU activations, using SGD with two different choices for the learning rate:
α= 0.05andα= 0.15. Moreover, given that we know when a switch of the mean happens, we
reset (or not reset) all the parameters at every switch as we run SGD. Only during the reset, we use
different learning rate β= 0.05orβ= 0.15. Using higher learning rate during reset allows SGD to
learn faster from new data. We also ran SGD with α= 0.05andβ= 0.15, where the higher learning
rate is used during task switch but we do not reset the parameters. We found that it performed the
same as SGD with α= 0.05, which highlights the benefit of reset.
0 50 100 150 200 250 300
Number of timesteps2
1
012Predicted mean 
 True data
SGD (=0.05)
SGD (=0.05) + reset (=0.05)
SGD (=0.15)
SGD (=0.05) + reset (=0.15)
Figure 24: Non-stationary mean tracking with SGD.
We report the predicted mean ˆµtfor all SGD variants in Figure 24. We see that after the first switch
of the mean, the SGD without reset takes more time to learn the new mean compared to the version
with parameters reset. Increasing the learning rate speeds up the adaptation to new data, but it still
remains slower during the mean change from 2to−2compared to the version that resets parameters.
This example highlights that resets could be highly beneficial for improving the performance of SGD
which could be slowed down by the implicit regularization towards the previous parameters θtand
the impact of the regularization strength induced by the learning rate.
36L Using arbitrary drift models
L.1 Using arbitrary drift models
The approach described in section 3.5 provides a general strategy of incorporating arbitrary Gaussian
drift models p(θ|θt;ψt) =N(θ;f(θt;ψt);g2(θt;ψt))which induces proximal optimization problem
θt+1= arg min
θLt+1(θ) +1
2g(θt;ψt)||θ−f(θt;ψt)||2(45)
The choice of f(θt;ψt)andg(θt;ψt)affects the behavior of the estimate θt+1from (45) and ulti-
mately depends on the problem in hand. The objective function of the form (45) was studied in
context of online convex optimization in [ 21],[29], where the underlying algorithms estimated the
deterministic drift model online. These worked demonstrated improved regret bounds depending on
model estimation errors. This approach could also be used together with a Bayesian Neural Network
(BNN).
37