Convergence of log (1/ϵ)for Gradient-Based
Algorithms in Zero-Sum Games without the Condition
Number: A Smoothed Analysis
Ioannis Anagnostides
Carnegie Mellon University
ianagnos@cs.cmu.eduTuomas Sandholm
Carnegie Mellon University
Strategic Machine, Inc.
Strategy Robot, Inc.
Optimized Markets, Inc.
sandholm@cs.cmu.edu
Abstract
Gradient-based algorithms have shown great promise in solving large (two-player)
zero-sum games. However, their success has been mostly confined to the low-
precision regime since the number of iterations grows polynomially in 1/ϵ,
where ϵ > 0is the duality gap. While it has been well-documented that lin-
ear convergence—an iteration complexity scaling as log(1/ϵ)—can be attained
even with gradient-based algorithms, that comes at the cost of introducing a depen-
dency on certain condition number-like quantities which can be exponentially large
in the description of the game.
To address this shortcoming, we examine the iteration complexity of several
gradient-based algorithms in the celebrated framework of smoothed analysis , and
we show that they have polynomial smoothed complexity , in that their number of
iterations grows as a polynomial in the dimensions of the game, log(1/ϵ), and
1/σ, where σmeasures the magnitude of the smoothing perturbation. Our result
applies to optimistic gradient and extra-gradient descent/ascent, as well as a certain
iterative variant of Nesterov’s smoothing technique. From a technical standpoint,
the proof proceeds by characterizing and performing a smoothed analysis of a
certain error bound , the key ingredient driving linear convergence in zero-sum
games. En route, our characterization also makes a natural connection between
the convergence rate of such algorithms and perturbation-stability properties of the
equilibrium, which is of interest beyond the model of smoothed complexity.
1 Introduction
We consider the fundamental problem of computing an equilibrium strategy for a (two-player)
zero-sum game
min
x∈∆nmax
y∈∆m⟨x,Ay⟩, (1)
where ∆d+1:={x∈Rd+1
≥0:x⊤1d+1= 1}represents the d-dimensional probability simplex
andA∈Rn×mis the payoff matrix of the game. Tracing all the way back to V on Neumann’s
celebrated minimax theorem [von Neumann, 1928], zero-sum games played a pivotal role in the early
development of game theory [von Neumann and Morgenstern, 1947] and the crystallization of linear
programming duality [Dantzig, 1951]. Indeed, in light of the equivalence between zero-sum games
and linear programming [Adler, 2013, von Stengel, 2023, Brooks and Reny, 2023], many central
optimization problems can be cast as (1).
38th Conference on Neural Information Processing Systems (NeurIPS 2024).State of the art algorithms for solving zero-sum games can be coarsely classified based on the desired
accuracy of a feasible solution (x,y), measured in terms of the duality gap
Φ(x,y):= max
y′∈∆m⟨x,Ay′⟩ −min
x′∈∆n⟨x′,Ay⟩. (2)
In the so-called low-precision regime, where one is content with a crude solution (x⋆,y⋆)such
thatΦ(x⋆,y⋆) =:ϵ≫0, the best available algorithms typically revolve around the framework of
regret minimization , both in practice [Farina et al., 2021, Brown and Sandholm, 2019, Zinkevich
et al., 2007, Tang et al., 2023] and in theory [Carmon et al., 2020, 2019, 2024, Grigoriadis and
Khachiyan, 1995, Clarkson et al., 2012, Alacaoglu and Malitsky, 2022]—in conjunction with other
techniques to speed up the per-iteration complexity, such as variance reduction, data structure design,
and sparsification [Zhang and Sandholm, 2020, Farina and Sandholm, 2022]. Such algorithms have
been central to landmark results in practical computation of equilibrium strategies even in enormous
games [Brown and Sandholm, 2018, Bowling et al., 2015, Morav ˇcík et al., 2017, Perolat et al., 2022].
The high-precision regime, where ϵ≪1
poly(nm), has turned out to be more elusive, with current
LP-based techniques struggling to scale favorably in large instances. This deficiency can be in part
attributed to the relatively high per-iteration complexity of LP-based approaches, such as interior-
point methods or the ellipsoid algorithm, as well as their intense memory requirements. A promising
antidote is to instead rely on iterative gradient-based methods that have a minimal per-iteration
cost. Indeed, in a line of work pioneered by Tseng [1995], it is by known well-documented that
linear convergence —an iteration complexity scaling only as log(1/ϵ)—can been achieved even with
such methods [Tseng, 1995, Gilpin et al., 2012, Wei et al., 2021, Applegate et al., 2023, Fercoq,
2023]. There is, however, a major caveat to those results: the number of iterations no longer grows
polynomially with the dimensions of the game nandm, but instead depends on certain condition
number-like quantities that could be exponentially large in the description of the problem; it is thus
unclear how to interpret those results from a computational standpoint.
To address those shortcomings, in this paper we work in the celebrated framework of smoothed
analysis pioneered by Spielman and Teng [2004]. Namely, our goal is to characterize the iteration
complexity of certain gradient-based algorithms in zero-sum games when the payoff matrix Ais
subjected to small but random perturbations, as formally introduced below.
Definition 1.1 (Zero-sum games under Gaussian perturbations) .Let¯A∈[−1,1]n×m. We assume
that the payoff matrix is given by A:=¯A+G, where each entry of Gis an independent (univariate)
Gaussian random variable with zero mean and variance σ2≤1.
Randomness here is only injected into the payoff matrix and not the set of constraints (that is, the
probability simplex), which is the natural model; after applying the perturbation, the problem should
still be a zero-sum game in the form of (1). Under this model, we investigate the convergence of the
following gradient-based algorithms.1(Their formal description is given later in Appendix B.)
1.optimistic gradient descent/ascent ( OGDA )[Popov, 1980];
2.optimistic multiplicative weights update ( OMWU )[Syrgkanis et al., 2015, Chiang et al., 2012,
Rakhlin and Sridharan, 2013];
3.extra-gradient descent/ascent ( EGDA )[Korpelevich, 1976]; and
4.an iterative variant of Nesterov’s smoothing technique ( IterSmooth )[Gilpin et al., 2012,
Nesterov, 2005].
Smoothed complexity allows interpolating between worst-case analysis—when the variance of the
noise σ2is negligible—and average-case analysis—when the noise dominates over the underlying
input. An average-case analysis is often unreliable since—as Edelman [1993] convincingly argued—a
fully random matrix does not necessarily capture typical instances encountered in practice. Spielman
and Teng [2004] put forward the framework of smoothed analysis as an attempt to explain the
performance of algorithms in realistic scenarios; to understand how brittle worst-case instances
really are. They famously proved that the simplex algorithm, under a certain pivoting rule, enjoys
polynomial smoothed complexity , meaning that its running time is bounded by some polynomial in the
1The vanilla gradient descent/ascent algorithm does not even converge (in a last-iterate sense) in zero-sum
games ( e.g., [Mertikopoulos et al., 2018]), which is why our analysis revolves around certain variants thereof.
It is worth noting that regret minimization techniques provide guarantees concerning the average iterates, a
distinction blurred in our introduction.
2size of the input and 1/σ. Smoothed analysis is by now a well-accepted algorithmic framework with
a tremendous impact in the analysis of algorithms. We also argue that it is particularly well-motivated
from a game-theoretic perspective: there is often misspecification or noise when modeling a game,
so smoothed analysis offers a compelling way of bypassing pathological instances that are perhaps
artificial in the first place.
Nevertheless, we are not aware of any prior work operating in the smoothed complexity model
per Definition 1.1 in the context of zero-sum games. To clarify this point, it is important to stress
here that although zero-sum games can be immediately reduced to linear programs, that reduction
is less clear in the smoothed complexity model. In particular, one set of constraints in the induced
linear program takes the form Ay≤v1n=:b, where 1n∈Rnis the all-ones vector. According to
the usual model of smoothed complexity in the context of linear programs, randomness has to be
injected into both Aandb, but that clearly disturbs the validity of the equivalence. More broadly,
reductions in the smoothed complexity model are quite delicate [Bläser and Manthey, 2015]; as a
further example, even reductions involving solely linear transformations can break in the smoothed
complexity model since independence—a crucial assumption in this framework—is not guaranteed
to carry over. Relatedly, one interesting direction arising from the work of Spielman and Teng [2003]
is to perform smoothed analysis in linear programs which are guaranteed to be feasible and bounded,
no matter the perturbation; zero-sum games under Definition 1.1 constitute such a class. Besides the
point above, different algorithms designed for the same problem can have entirely different properties,
not least in terms of their smoothed complexity. The class of algorithms we consider in this paper is
quite distinct from the ones shown to have polynomial smoothed complexity in the context of linear
programs (described further in Appendix A). In many ways, gradient-based methods are simpler and
more natural, which partly justifies their tremendous practical use. As a result, understanding their
smoothed complexity is an important question.
1.1 Our results
Our main contribution is to show that, with the exception of OMWU , the other gradient-based algorithms
mentioned above (Items 1, 3 and 4) have polynomial smoothed complexity with high probability—that
is to say, with probability at least 1−1
poly(nm).
Theorem 1.2. With high probability over the randomness of A∈Rn×m(Definition 1.1), OGDA ,
EGDA andIterSmooth converge to an ϵ-equilibrium after poly(n, m, 1/σ)·log(1/ϵ)iterations.
The main takeaway of this result is that, modulo pathological instances, certain gradient-based
algorithms are reliable solvers in zero-sum games even in the high-precision regime. Similarly to
earlier endeavors in the context of linear programs [Spielman and Teng, 2004, Blum and Dunagan,
2002], a dependency of poly(1/σ)(as in Theorem 1.2) is what we should expect; the one exception
is the class of interior-point methods whose running time grows as log(1/σ), but those algorithms
are (weakly) polynomial even in the worst case. We further remark that the polynomial dependency
onnandmin Theorem 1.2 can almost certainly be improved, and we made no effort to optimize it.
Regarding OMWU , which is not covered by Theorem 1.2, we also obtain a significant improvement
in the iteration complexity compared to the worst-case analysis of Wei et al. [2021], but our bound
is still not polynomial. As we explain further in Appendix C.3, the main difficulty pertaining to
OMWU is that the analysis of Wei et al. [2021] gives (at best) an exponential bound no matter the
geometry of the problem . With that mind, our result is essentially the best one could hope for without
refining the worst-case analysis of OMWU , which is not within our scope here. We anticipate that our
characterization herein will prove useful in conjunction with future developments in the worst-case
complexity of OMWU , as well as in the analysis of other iterative methods.
The error bound The central ingredient that enables gradient-based algorithms to exhibit linear
convergence is a certain error bound , given below as Definition 1.3. For compactness in our notation,
we let X:= ∆nandY:= ∆m. We then let z:= (x,y),Z:=X × Y , andZ⋆:=X⋆× Y⋆, where
X⋆andY⋆represent the (convex) set of equilibria for Player xand Player y, respectively.
Definition 1.3 (Error bound) .LetΦ(z)denote the duality gap as introduced in (2). We say that the
zero-sum game (1) satisfies an error bound with modulus κ∈R>0if
Φ(z)≥κ∥z−ΠZ⋆(z)∥ ∀z∈ Z. (3)
3Above, ΠZ⋆(·)denotes the (Euclidean) projection operator; the set of games with a unique equilibrium
has measure one, so we can safely replace ΠZ⋆(z)by the unique equilibrium z⋆∈ Z⋆. It has been
known at least since the work of Tseng [1995] that affine variational inequalities indeed satisfy (3).
Nevertheless, it should come to no surprise that, even in 3×3games, κcan be arbitrarily small
(Proposition 3.1), which in turn means that, linear convergence notwithstanding, the number of
iterations prescribed by an analysis revolving around (3)can be arbitrarily large. In fact, with the
exception of OMWU , which is to be discussed further below, Definition 1.3 suffices to establish linear
convergence (essentially) based on existing results.2Our main result pertaining to Definition 1.3 is
that the modulus κis likely to be polynomial in the smoothed complexity model:
Theorem 1.4. With high probability over the randomness of A(Definition 1.1), the error bound
per Definition 1.3 is satisfied for any sufficiently small κ≥poly(σ,1/(nm)).
To establish this result, the first step is to lower bound κin terms of certain natural geometric features
of the problem (Theorem 3.6), which is discussed further in Section 3.1. Establishing Theorem 1.4
then reduces to analyzing each of those quantities under Definition 1.1. It turns out that bounding
those quantities also suffices for characterizing OMWU , whose existing analysis due to Wei et al. [2021]
involves some further ingredients besides the error bound of Definition 1.3.
Further implications Our characterization of the error bound given in Theorem 3.6 has some further
important implications. First, a well-known vexing issue regarding computing equilibria even in
zero-sum games is that a solution with small duality gap can still be relatively far from the equilibrium
in the geometric sense, a phenomenon further exacerbated in multi-player games [Etessami and
Yannakakis, 2007]. Therefore, results providing guarantees in terms of the duality gap are not
particularly informative when it comes to computing strategies close to the equilibrium in a geometric
sense. At the same time, there are ample reasons why the latter guarantee is more appealing [Etessami
and Yannakakis, 2007]. Theorem 1.4 implies that such concerns can be alleviated in the smoothed
complexity model:
Corollary 1.5. With high probability over the randomness of A(Definition 1.1), any point z∈ Z
withΦ(z)≤ϵsatisfies ∥z−z⋆∥ ≤ϵ·poly(n, m, 1/σ).
Beyond smoothed analysis, Theorem 3.6 applies to any non-degenerate game (Definition 3.2), and
can be thereby used to parameterize the rate of convergence of gradient-based algorithms based
on natural and interpretable game-theoretic quantities of the underlying game, which has eluded
prior work. In particular, we make a natural connection between the complexity of gradient-based
algorithms and perturbation stability properties of the equilibrium. In light of misspecifications which
are often present in game-theoretic modeling, focusing on games with perturbation-stable equilibria
is well-motivated and has already received ample of interest in prior work [Balcan and Braverman,
2017, Awasthi et al., 2010]; more broadly, perturbation stability is a common assumption in the
analysis of algorithms beyond the worst-case model [Makarychev and Makarychev, 2021]. There are
different natural ways of defining perturbation-stable games; here, we assume that any perturbation
with magnitude below δ >0, in that ∥A′−A∥2≤δ, maintains the support of the equilibrium and
the non-degeneracy of the game; we call such games δ-support-stable (Definition 4.1). In this context,
we show the following result.
Corollary 1.6. For any δ-support-stable zero-sum game, OGDA ,EGDA andIterSmooth converge to
anϵ-equilibrium after poly(n, m, 1/δ)·log(1/ϵ)iterations.
That is, games in which δis not too close to 0are more amenable to gradient-based algorithms,
which is a quite natural connection. Corollary 1.6 is shown by relating each of the quantities involved
in Theorem 3.6 to parameter δdefined above.
2 Notation
Before we proceed with our technical content, we first take the opportunity to streamline our notation;
further background on smoothed analysis and a description of the algorithms referred to earlier
(Items 1 to 4) is given later in Appendix B, as it is not important for the purpose of the main body.
2Definition 1.3 also readily establishes linear convergence for other compelling primal-dual algorithms, as
shown recently by Applegate et al. [2023]; in that paper, the error bound was referred to as “sharpness,” a
terminology employed in other papers as well ( e.g., [Zarifis et al., 2024]).
4We use boldface letters, such as x,y,b,c, to represent vectors in a Euclidean space. For a vector
x∈Rn, we access its ith coordinate via a subscript, namely xi. Superscripts (together with
parantheses) are typically reserved for the (discrete) time index. We denote by ∥x∥the Euclidean
norm, ∥x∥:=pPn
i=1x2
i, theℓ∞norm by ∥x∥∞:= max 1≤i≤n|xi|, and the ℓ1norm by ∥x∥1:=Pn
i=1|xi|. Forx,x′∈Rn, we let dist(x,x′):=∥x−x′∥.span(·)represents the linear space
spanned by a given set of vectors. For x∈Rnand a subset B⊆[n], we denote by xB∈RB
the subvector of xinduced by B. We let 1n∈Rnbe the all-ones vector of dimension n; we will
typically omit the subscript when it is clear from the context. For vectors x∈Rnandy∈Rm, we
write (x,y)∈Rn+mto denote their concatenation. Throughout this paper, we use xandyto denote
the strategy of Player xand Player y, respectively.
To represent matrices, we use boldface capital letter, such as A,Q. It will sometimes be convenient
to use A♭∈Rnmto represent a vectorization of A∈Rn×m. We overload notation by letting ∥A∥
be the spectral norm of A. For a matrix A∈Rn×mand subsets B⊆[n], N⊆[m], we denote
byAB,N∈RB×Nthe submatrix of Ainduced by BandN.Ai,:andA:,jrepresent the ith row
andjth column of A, respectively. The singular values of a matrix M∈Rd×dare denoted by
σ1(M)≥σ2(M)≥ ··· ≥ σd(M)≥0(not to be confused with our notation for the variance σ2).
To be more explicit, we may also use σmax(M):=σ1(M)andσmin(M):=σd(M).
3 Smoothed analysis of the error bound
In this section, we perform a smoothed analysis of the error bound—as introduced earlier in Defini-
tion 1.3—in (two-player) zero-sum games. It is first instructive to point out why smoothed analysis is
useful in the first place: the modulus κcan be arbitrarily close to 0even when n=m= 3(that is,
3×3games); this is detrimental as the iteration complexity of algorithms such as OGDA grows as a
polynomial in 1/κ.
Proposition 3.1. There exists a 3×3zero-sum game such that κper Definition 1.3 is arbitrarily
close to 0.
In proof, it is enough to consider the ill-conditioned diagonal matrix
A= γ0 0
0 2γ0
0 0 1!
, (4)
where 0< γ≪1. The (unique) equilibrium of (4)readsx⋆=y⋆=1
3+2γ(2,1,2γ)∈∆3. Now,
considering x= (1,0,0)andy= (0,0,1), for the duality gap we have Φ(x,y) =γ, while the
distance of (x,y)from the optimal solution (x⋆,y⋆)is at least3
3+2γ. In turn, by Definition 1.3, this
means that κ≤2γ. So, Proposition 3.1 follows by taking γ→0.3
Proposition 3.1 exposes one type of pathology that can decelerate gradient-based algorithms, which
is evidently related to the poor spectral properties of the payoff matrix. This intuition is quite helpful
when equilibria are fully supported—as is the case in (4)—but has to be significantly refined more
broadly, as we formalize in the sequel.
To sidestep such pathological examples, we thus turn to the smoothed analysis framework of Defini-
tion 1.1.
3.1 Overview
The most natural approach to analyze the error bound in the smoothed complexity model is to rely
on an existing (worst-case) analysis proving that a positive κexists, and then attempt to refine that
analysis. Yet, at least based on such prior results we are aware of, that turns out to be challenging.
As an example, let us consider the recent analysis of Wei et al. [2021]. As we explain in more detail
in Appendix C.3, Wei et al. [2021] relate the modulus κof the error bound to the (inverse of the) norm
of a solution to a certain feasible linear program; the existence of a legitimate κ >0then follows
readily from feasibility. Now, this reduction seems quite promising: Renegar [1994] has shown that the
3If we want to specify the game with a (finite) number of Lbits, Proposition 3.1 tells us that the modulus κ
can be exponentially small in L.
5norm of a solution to a linear program can be bounded in terms of its condition number —the distance
to infeasibility in our case, and Dunagan et al. [2011] later proved that the condition number of linear
programs is polynomial in the smoothed complexity model. Nevertheless, there are some difficulties
in materializing that argument. First, the induced linear program involves terms depending on both the
payoff matrix and the geometry of the constraints (the probability simplex in our case). Consequently,
the analysis of Dunagan et al. [2011] does not carry over since randomness is only injected into the
payoff matrix. The second and more important obstacle is that the induced linear program depends on
the optimal solution, which in turn depends on the randomness of the payoff matrix; this significantly
entangles the underlying distribution. As there are exponentially many possible configurations, we
cannot afford to argue about each one separately and then apply the union bound. This difficulty is
in fact known to be the crux in performing smoothed analysis [Spielman and Teng, 2004].4
To address those challenges, we provide a new characterization of the error bound in terms of
some natural quantities of the underlying game (Theorem 3.6), which in some sense capture the
difficulty of the problem. We are then able to use a technique due to Spielman and Teng [2004],
exposed in Section 3.3, to bound the probability that each of the involved quantities is close to 0
(Propositions 3.8 to 3.10), even though the underlying distribution is quite convoluted. The resulting
analysis follows the one given by Spielman and Teng [2003] in the context of termination of linear
programs, but still has to account for a number of structural differences.
In what follows, we structure our argument as follows. First, in Section 3.2, we relate the modulus κ
to some natural quantities capturing key geometric features of the problem. Section 3.3 then proceed
by analyzing those quantities in the smoothed analysis framework.
3.2 Characterization of the error bound
Our first goal is to characterize the error bound in terms of certain natural quantities, which will
then enable us to provide polynomial error bounds in the smoothed complexity model. Our only
assumption here is that the zero-sum game is non-degenerate , in the sense of Definition 3.2 below;
this can always be met with the addition of an arbitrarily small amount of noise (Lemma C.1). As
such, our characterization here has an interest beyond the smoothed analysis framework, casting
the error bound in terms of more interpretable game-theoretic quantities; for example, a concrete
implication is given in Section 4.
Let us denote by vthevalue of game (1), that is,
v= min
x∈Xmax
y∈Y⟨x,Ay⟩= max
y∈Ymin
x∈X⟨x,Ay⟩,
which is a consequence of the minimax theorem [von Neumann, 1928]. We are now ready to state the
formal definition of a non-degenerate game.
Definition 3.2 (Non-degenerate game) .A zero-sum game described with a payoff matrix Aand
value vis said to be non-degenerate if it admits a unique equilibrium (x⋆,y⋆)∈ Z, andx⋆and
y⋆make tight exactly nof the inequalities {xi≥0}i∈[n]∪ {⟨x,A:,j⟩ ≤v}j∈[m]andmof the
inequalities {yj≥0}j∈[m]∪ {⟨y,Ai,:⟩ ≥v}i∈[n], respectively.
In the sequel, we will make constant use of the fact that the set of degenerate games has measure zero
under the law induced by Definition 1.1 (Lemma C.1).
In this context, we let B(x⋆):={i∈[n] :x⋆
i>0}denote the support ofx⋆(corresponding to
Player x), and similarly N(y⋆):={j∈[m] :y⋆
j>0}for the support of Player y. The strict
complementarity theorem [Ye, 2011] tells us that Bindexes exactly the set of tight inequalities
{⟨y,Ai,:⟩ ≥v}i∈[n], and symmetrically, Nindexes exactly the set of tight inequalities {⟨x,A:,j⟩ ≤
v}j∈[m]. In particular, this implies that |B|=|N|with probability 1. It will also be convenient to
define B:= [n]\BandN:= [m]\N.
Now, at a high level, one can split solving a zero-sum game into two subproblems: i) identifying the
support of the equilibrium, and ii) solving the induced linear system to specify the exact probabilities
4This is not a concern in the unconstrained setting, where X=RnandY=Rm, in which a polynomial
smoothed complexity follows readily from existing results relating the convergence of OGDA orEGDA to the
condition number of the payoff matrix A(e.g., [Mokhtari et al., 2020, Li et al., 2023, Azizian et al., 2020]),
which in turn is well-known to be polynomial in the smoothed complexity model [Spielman and Teng, 2004].
6within the support. It will be helpful to have that viewpoint in mind in the upcoming analysis, and in
particular in the proof of Theorem 3.6. Roughly speaking, thinking of κas a measure of the problem’s
difficulty, we will relate κto i) the difficulty of identifying the support of the equilibrium, and ii) the
difficulty of solving the induced linear system. To be clear, those two subproblems are only helpful for
the purpose of the analysis, and they are certainty intertwined when using algorithms such as OGDA .
Staying on the latter task, we will make use of a certain transformation so as to eliminate one of
the redundant variables. Namely, for any bxB∈∆(B)andbyN∈∆(N), let us select a fixed pair
of coordinates (i, j)∈B×N(for example, the ones with the smallest index). Using the fact that
⟨bxB,1⟩= 1and⟨byN,1⟩= 1, we can eliminate bxiandbyjby writing
⟨bxB,AB,NbyN⟩=⟨ex,Qey⟩ − ⟨ex,c⟩ − ⟨ey,b⟩+d, (5)
whereex∈ReB
≥0,ey∈ReN
≥0(foreB:=B\ {i}andeN:=N\ {j}) coincide with bxBandbyNon
all coordinates in eBandeN, respectively, and A♭
B,N=T(Q♭,b,c, d)for a (non-singular) linear
transformation T∈R(BN)×(BN). (We spell out the exact definition of Tlater in Appendix C.1,
as it is not important for our purposes here; it follows by simply writing bxi= 1− ⟨ex,1⟩and
byj= 1− ⟨ey,1⟩.) The point of transformation (5)is that, by eliminating one of the redundant
variables, there is a convenient characterization of the equilibrium (Claim C.3); namely, Qy⋆=c
andQ⊤x⋆=b.
We are now ready to introduce the key quantities upon which our characterization relies on. It turns
out that those are analogous to the ones considered by Spielman and Teng [2003] in the context of
analyzing the termination of linear programs; this is not coincidental, as our analysis was especially
targeted to do so.
Definition 3.3. LetAbe the payoff matrix of a non-degenerate game, (x⋆,y⋆)∈ Z the unique
equilibrium, and B⊆[n], N⊆[m]the support of x⋆andy⋆respectively. We introduce the
following quantities.
1.αP(A):= min i∈B(x⋆
i)andαD(A):= min j∈N(y⋆
j);
2.βP(A):= minj∈N(v− ⟨x⋆
B,AB,j⟩)andβD(A):= mini∈B(⟨Ai,N,y⋆
N⟩ −v); and
3.γP(A):= min jdist(Q:,j,span(Q:,eN−j))andγD(A):= min idist(Qi,:,span(QeB−i,:)),
where we use the shorthand notation eB−i:=eB\{i}(eN−j:=eN\{j}), andQ=Q(A)
is defined in (5).
(Above, we adopt the convention that if a minimization problem is with respect to an empty set, the
minimum is to be evaluated as 1.)
Item 3 above will enable us to control the norm of solutions to any linear system induced by Q, as
we explain in the sequel. Our proof will actually rely on a slightly different matrix, which we call Q;
the lemma below relates the geometry of QtoQ, and reassures us that the condition number of Q
cannot be far from that of Qso long as 1−P
j∈eNy⋆
j≥αD(A)(by Item 1) is not too close to 0. (A
symmetric statement holds when focusing on Player y.)
Lemma 3.4. Letc=Qey⋆=P
j∈eNey⋆
jQ:,j, and suppose that Q∈ReB×eNis such that its jth
column is equal to Q:,j−c. Then,
min
j∈eNdist(Q:,j,span(Q:,eN−j))≤ 
1 +|eN|
1−P
j∈eNy⋆
j!
min
j∈eNdist(Q:,j,span(Q:,eN−j)).
Next, we recall a fairly standard bound relating the magnitude of a solution to a linear system
ex=Mpwith the smallest singular value of a full-rank matrix M.
Lemma 3.5. LetM∈Rd×dbe a full-rank matrix. For any ex∈Rdthere is p∈Rdwith
∥p∥ ≤1
σmin(M)∥ex∥such that
ex=Mp=dX
j=1pjM:,j.
7Moreover, to connect Lemma 3.5 with γP(A), we observe that the smallest singular value can also
be lower bounded in terms of the smallest distance of a column from the linear space spanned by the
rest of the columns—which now matches the expression of Item 3 we saw earlier. In particular, we
will make use of the so-called negative second moment identity [Tao et al., 2010] (Proposition C.4),
which implies that
σmin(Q)≥s
1P
j∈eNdist−2(Q:,j,span(Q:,eN−j))≥1q
|eN|min
j∈eNdist(Q:,j,span(Q:,eN−j)).(6)
Proposition C.4 also implies that γD(A)≥1√
|eB|γP(A), and so it will suffice to lower bound γP(A)
in the sequel. We are now ready to proceed with the main result of this subsection. Below, we use the
notation “ ≳” to suppress lower-order terms and absolute constants.
Theorem 3.6. LetAbe a non-degenerate payoff matrix, and suppose that (αP(A), αD(A)),
(βP(A), βD(A))and(γP(A), γD(A))are as in Definition 3.3. Then, the error bound (Defini-
tion 1.3) is satisfied for any sufficiently small modulus
κ≳1
∥A♭∥∞1
min(n, m)3min
(αD(A))2βD(A)γP(A),(αP(A))2βP(A)γD(A)	
.
It is enough to explain how to lower bound κ > 0such that max y′∈Y⟨x,Ay′⟩ −v≥κ∥x−
ΠX⋆(x)∥=κ∥x−x⋆∥for any x∈ X . In a nutshell, our argument is divided based on the
magnitude λ:=∥xB∥, which can be thought of as a measure of closeness from the support of
the equilibrium. When λ≪1, which means that xis still far from the support of the equilibrium,
max y′∈Y⟨x,Ay′⟩−vis governed by βD(A). In the contrary case, our basic strategy revolves around
showing that the error bound can be treated as in the unconstrained case, which would then relate the
modulus κto the smallest singular value of the underlying matrix (essentially by Lemma 3.5)—and
subsequently to γP(A)due to (6). Indeed, this turns out to be possible by working with matrix Q, as
defined earlier in Lemma 3.4. We defer the precise argument to Appendix C.1.
3.3 Smoothed analysis
Having established Theorem 3.6, our next step is to show that each of the quantities introduced in Def-
inition 3.3 is unlikely to be too close to 0in the smoothed complexity model, which would then im-
ply Theorem 1.4. The main difficulty lies in the fact that each configuration that may arise depends on
the support of the equilibrium, which in turn depends on the underlying randomization of A, thereby
significantly complicating the underlying distribution. Further, one cannot afford to argue about each
configuration separately and then apply the union bound as there are too many possible configurations.
To tackle this challenge, we follow the approach put forward by Spielman and Teng [2003].
In particular, given that all quantities of interest in Theorem 3.6 depend on the support of the
equilibrium, it is natural to proceed by partitioning the probability space over all possible supports,
and then bound the worst possible one—that is, the one maximizing the probability we want to
minimize. In doing so, the challenge is that one has to condition on the equilibrium having a given
support (formally justified by Proposition C.5). To argue about the induced probability density
function upon such a conditioning, it is convenient to perform a change of variables from Ato a new
set of variables that now contains the equilibrium (x⋆,y⋆)(Lemma C.6). The basic idea here is that
since the event we condition on concerns the equilibrium, it is helpful to have that equilibrium being
part of our set of variables. The induced probability density function is now quite complicated, but
can still be analyzed using the following lemma.
Lemma 3.7 (Spielman and Teng, 2003) .Letρbe the probability density function of a random
variable X. If there exist δ >0andc∈(0,1]such that
0≤t≤t′≤δ=⇒ρ(t′)
ρ(t)≥c, (7)
then
P[X≤ϵ|X≥0]≤ϵ
cδ.
8In words, random variables whose density is smooth—in the sense of (7)—are unlikely to be too
close to 0. Gaussian random variables certainly have that property (Lemma C.8), but it is not confined
to the Gaussian law; the analysis of Spielman and Teng [2003]—and subsequently our result—is not
tailored to the Gaussian case.
We are now ready to state our main results in the smoothed complexity model; the proofs are
deferred to Appendix C.2. We commence with βP(A), which is the easiest to analyze. In particular,
the following result is a consequence of an anti-concentration bound with respect to a conditional
Gaussian random variable (Lemma C.7).
Proposition 3.8. LetβP(A)be defined as in Item 2. For any ϵ≥0,
P
A
βP(A)≤ϵ
5∥A♭∥∞
≤ϵemin(n, m)2
σ2.
The analysis of γP(A)is more challenging, and makes crucial use of Lemma 3.7. As we alluded to
earlier, a key step is to change variables from AB,Nto(Q,b,c,·)—in accordance with (5)—and then
to(Q,x⋆,y⋆,·)based on Qey⋆=c,Q⊤ex⋆=b. It is important to note that Qno longer contains
independent random variables even though AB,N is (by Definition 1.1); this stems from the presence
of a redundant variable in x⋆
B(since ⟨x⋆
B,1⟩= 1). Nevertheless, we can still overcome this issue
using Lemma 3.7, leading to the following bound.
Proposition 3.9. LetγP(A)be defined as in Item 3. For any ϵ≥0,
P
A"
γP(A)≤ϵ
4 maxj∈eN∥Q:,j∥+ 20∥A♭∥∞+ 3#
≤ϵ4emin(n, m)3
σ2.
Similar reasoning, albeit with some further complications, provides a bound for αP(A), which is
given below.
Proposition 3.10. LetαP(A)be defined as in Item 1. For any ϵ≥0,
P
A
αP(A)≤ϵ
25(∥A♭∥∞+ 1)2
≤ϵ8e2mnmin(n, m)
σ2.
Armed with Propositions 3.8 to 3.10 and Theorem 3.6, we can establish Theorem 1.2 by suitably
leveraging existing results, as we formalize in Appendix C.3.
4 Parameterized results for perturbation-stable games
Another important implication of our characterization in Theorem 3.6 is that it enables connecting the
convergence rate of gradient-based algorithms to natural and interpretable game-theoretic quantities.
In particular, here we highlight a connection with perturbation-stable games, in the following formal
sense.
Definition 4.1 (Perturbation-stable games) .LetAbe the payoff matrix of a non-degenerate game.
We say that the game is δ-support-stable , with δ >0, if for any A′with∥A−A′∥ ≤δit holds that
A′is a non-degenerate game whose equilibrium has the same support as A.
Perhaps the simplest example of a support-stable game with a favorable parameter δ >0arises
whenAis the 2×2identity matrix. Indeed, as long as the perturbation parameter δremains below
a certain absolute constant, the perturbed game still admits a unique full-support equilibrium. To
see this, suppose for the sake of contradiction that the perturbed game has an equilibrium such that
Player xplays one of the two actions with probability 1. Player ywould then obtain a utility of at
least1−O(δ). But the value of the original game was 1/2, which in turn implies that the value of
the perturbed game is 1/2±Θ(δ); for a sufficiently small δthis leads to a contradiction. Similar
reasoning applies with respect to Player y. (The previous argument carries over more broadly to
diagonally dominant 2×2payoff matrices.)
As we have highlighted already, games with perturbation-stable equilibria—albeit under different
notions of stability—have already received attention in the literature [Balcan and Braverman, 2017,
Awasthi et al., 2010] ( cf.Cohen [1986]), and are part of a broader trend in the analysis of algorithms
beyond the worst case (for further background, we refer to the excellent book edited by Roughgarden
[2021]). Our goal here is to make the following natural connection.
9Theorem 4.2. Anyδ-support-stable game (per Definition 4.1) satisfies the error bound for any
sufficiently small modulus
κ≥poly1
n,1
m, δ
.
By virtue of our discussion in Appendix C.3, Theorem 4.2 immediately implies Corollary 1.6. Indeed,
we observe that all parameters involved in Theorem 3.6 can be lower bounded in terms of the stability
parameter of Definition 4.1, as we formalize in Appendix C.4.
5 Conclusions and future research
In conclusion, we performed the first smoothed analysis with respect to a number of well-studied
gradient-based algorithms in zero-sum games. In particular, we showed that OGDA ,EGDA and
IterSmooth all enjoy polynomial smoothed complexity, meaning that their iteration complexity
grows as a polynomial in the dimensions of the game, 1/σ, and log(1/ϵ); for OMWU , our analysis
reveals a significant improvement over the worst-case bound due to Wei et al. [2021], but it still
remains superpolynomial. We also made a connection between the rate of convergence of the above
algorithms and a natural perturbation-stability property of the equilibrium, which is interesting beyond
the model of smoothed complexity.
A number of interesting avenues for future research remain open. First, is it the case that OMWU has
polynomial smoothed complexity or is there an inherent separation with the other algorithms we stud-
ied? Answering this question in the positive would necessitate significantly improving the worst-case
analysis of OMWU due to Wei et al. [2021] ( cf.Cai et al. [2024] for a recent development concerning the
last-iterate convergence of OMWU ). Beyond OMWU , our results could also prove useful for establishing
polynomial bounds for other natural dynamics in the smoothed analysis framework. Moreover, our
characterization of the error bound in Theorem 3.6 assumes that the game is non-degenerate. This
is an innocuous assumption in the smoothed complexity model, as it holds with probability 1, but
nevertheless it would be interesting to generalize it to any game. Doing so could shed some light
into whether Theorem 4.2 holds with respect to other, perhaps more natural notions of perturbation
stability beyond Definition 4.1. It would also be interesting to investigate other models of smoothed
complexity that account for dependencies between the entries of the payoff matrix [Bhaskara et al.,
2024]. Moreover, our focus has been on zero-sum games under simplex constraints, but we suspect
that more general positive results should be attainable under polyhedral constraint sets; perhaps the
most notable such candidate is the class of extensive-form games [Romanovskii, 1962, von Stengel,
1996]. Even beyond (two-player) zero-sum games, Theorem 1.2 could apply to (multi-player) polyma-
trixzero-sum games [Cai et al., 2016]. It is less clear whether the model of smoothed complexity can
be informative when it comes to convergence to coarse correlated equilibria in multi-player games.
Acknowledgments
We are grateful to the anonymous reviewers at NeurIPS for their helpful feedback. The first author
is indebted to Ioannis Panageas for many insightful discussions. This material is based on work
supported by the Vannevar Bush Faculty Fellowship ONR N00014-23-1-2876, National Science
Foundation grants RI-2312342 and RI-1901403, ARO award W911NF2210266, and NIH award
A240108S001.
References
Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Kentaro Toyoshima, and Atsushi Iwasaki. Last-iterate
convergence with full and noisy feedback in two-player zero-sum games. In International Confer-
ence on Artificial Intelligence and Statistics (AISTATS) , 2023.
Ilan Adler. The equivalence of linear programs and zero-sum games. Int. J. Game Theory , 42(1):
165–177, 2013.
Ahmet Alacaoglu and Yura Malitsky. Stochastic variance reduction for variational inequality methods.
InConference on Learning Theory (COLT) , 2022.
10Kimon Antonakopoulos, Elena Veronica Belmega, and Panayotis Mertikopoulos. Adaptive extra-
gradient methods for min-max optimization and games. In International Conference on Learning
Representations (ICLR) , 2021.
David L. Applegate, Oliver Hinder, Haihao Lu, and Miles Lubin. Faster first-order primal-dual
methods for linear programming using restarts and sharpness. Mathematical Programming , 201
(1):133–184, 2023.
Pranjal Awasthi, Maria-Florina Balcan, Avrim Blum, Or Sheffet, and Santosh S. Vempala. On
nash-equilibria of approximation-stable games. In International Symposium on Algorithmic Game
Theory (SAGT) , 2010.
Waïss Azizian, Damien Scieur, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel.
Accelerating smooth games by manipulating spectral shapes. In International Conference on
Artificial Intelligence and Statistics (2020) , Proceedings of Machine Learning Research, 2020.
Maria-Florina Balcan and Mark Braverman. Nash equilibria in perturbation-stable games. Theory
Comput. , 13(1):1–31, 2017.
Aditya Bhaskara, Eric Evert, Vaidehi Srinivas, and Aravindan Vijayaraghavan. New tools for
smoothed analysis: Least singular value bounds for random matrices with dependent entries. In
Proceedings of the Annual Symposium on Theory of Computing (STOC) , 2024.
Markus Bläser and Bodo Manthey. Smoothed complexity theory. ACM Trans. Comput. Theory , 7(2):
6:1–6:21, 2015.
Avrim Blum and John Dunagan. Smoothed analysis of the perceptron algorithm for linear program-
ming. In Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) , 2002.
Shant Boodaghians, Joshua Brakensiek, Samuel B. Hopkins, and Aviad Rubinstein. Smoothed
complexity of 2-player nash equilibria. In Proceedings of the Annual Symposium on Foundations
of Computer Science (FOCS) , 2020.
Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold’em
poker is solved. Science , 347(6218):145–149, 2015.
Benjamin Brooks and Philip J. Reny. A canonical game—75 years in the making—showing the
equivalence of matrix games and linear programming. Economic Theory Bulletin , 2023.
Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats
top professionals. Science , 359(6374):418–424, 2018.
Noam Brown and Tuomas Sandholm. Solving imperfect-information games via discounted regret
minimization. In Conference on Artificial Intelligence (AAAI) , 2019.
Luciana S. Buriol, Marcus Ritt, Félix Carvalho Rodrigues, and Guido Schäfer. On the smoothed
price of anarchy of the traffic assignment problem. In Workshop on Algorithmic Approaches for
Transportation Modeling, Optimization, and Systems (ATMOS) , 2011.
Yang Cai, Ozan Candogan, Constantinos Daskalakis, and Christos H. Papadimitriou. Zero-sum
polymatrix games: A generalization of minmax. Mathematics of Operations Research , 41(2):
648–655, 2016.
Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Finite-time last-iterate convergence for learning
in multi-player games. In Proceedings of the Annual Conference on Neural Information Processing
Systems (NeurIPS) , 2022.
Yang Cai, Gabriele Farina, Julien Grand-Clément, Christian Kroer, Chung-Wei Lee, Haipeng Luo, and
Weiqiang Zheng. Fast last-iterate convergence of learning in games requires forgetful algorithms.
InProceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) ,
2024.
Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian. Variance reduction for matrix games. In
Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2019.
11Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian. Coordinate methods for matrix games. In
Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS) , 2020.
Yair Carmon, Arun Jambulapati, Yujia Jin, and Aaron Sidford. A whole new ball game: A primal
accelerated method for matrix games and minimizing the maximum of smooth functions. In
Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) , 2024.
Xi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player Nash
equilibria. Journal of the ACM , 2009.
Xi Chen, Chenghao Guo, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Mihalis Yannakakis, and
Xinzhi Zhang. Smoothed complexity of local max-cut and binary max-csp. In Proceedings of the
Annual Symposium on Theory of Computing (STOC) , 2020.
Xi Chen, Chenghao Guo, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Mihalis Yannakakis.
Smoothed complexity of SWAP in local graph partitioning. Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA) , 2024.
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and
Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning Theory
(COLT) , 2012.
Miranda Christ and Mihalis Yannakakis. The smoothed complexity of policy iteration for markov
decision processes. In Proceedings of the Annual Symposium on Theory of Computing (STOC) ,
2023.
Kenneth L. Clarkson, Elad Hazan, and David P. Woodruff. Sublinear optimization for machine
learning. Journal of the ACM , 59(5):23:1–23:49, 2012.
Joel E. Cohen. Perturbation theory of completely mixed matrix games. Linear Algebra and its
Applications , 79:153–162, 1986.
Johanne Cohen, Amélie Héliou, and Panayotis Mertikopoulos. Hedging under uncertainty: Regret
minimization meets exponentially fast convergence. In International Symposium on Algorithmic
Game Theory (SAGT) , 2017.
Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix
multiplication time. Journal of the ACM , 68(1):3:1–3:39, 2021.
Leonardo Cunha, Gauthier Gidel, Fabian Pedregosa, Damien Scieur, and Courtney Paquette. Only
tails matter: Average-case universality and robustness in the convex regime. In International
Conference on Machine Learning (ICML) , 2022.
George Dantzig. A proof of the equivalence of the programming problem and the game problem. In
Tjalling Koopmans, editor, Activity Analysis of Production and Allocation , pages 330–335. John
Wiley & Sons, 1951.
Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and
constrained min-max optimization. In Innovations in Theoretical Computer Science Conference
(ITCS) , 2019.
Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms
for zero-sum games. Games and Economic Behavior , 92:327–348, 2015.
Constantinos Daskalakis, Noah Golowich, Nika Haghtalab, and Abhishek Shetty. Smooth nash
equilibria: Algorithms and complexity. In Innovations in Theoretical Computer (ITCS) , 2024.
Asen L Dontchev and R Tyrrell Rockafellar. Implicit functions and solution mappings: A view from
variational analysis , volume 616. Springer, 2009.
John Dunagan, Daniel A. Spielman, and Shang-Hua Teng. Smoothed analysis of condition numbers
and complexity implications for linear programming. Mathematical Programming , 126(2):315–
350, 2011.
12Alan Edelman. Eigenvalue roulette and random test matrices. Linear Algebra for Large Scale and
Real-Time Applications , pages 365–368, 1993.
Kousha Etessami and Mihalis Yannakakis. On the complexity of Nash equilibria and other fixed
points (extended abstract). In Proceedings of the Annual Symposium on Foundations of Computer
Science (FOCS) , 2007.
Gabriele Farina and Tuomas Sandholm. Fast payoff matrix sparsification techniques for structured
extensive-form games. In Conference on Artificial Intelligence (AAAI) , 2022.
Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Faster game solving via predictive blackwell
approachability: Connecting regret matching and mirror descent. In Conference on Artificial
Intelligence (AAAI) , 2021.
Olivier Fercoq. Quadratic error bound of the smoothed gap and the restarted averaged primal-dual
hybrid gradient, 2023.
Nicola Gatti, Marco Rocco, and Tuomas Sandholm. Strong Nash equilibrium is in smoothed P. In
Conference on Artificial Intelligence (AAAI) , 2013. Late-breaking paper track.
Yiannis Giannakopoulos. A smoothed FPTAS for equilibria in congestion games. CoRR ,
abs/2306.10600, 2023.
Yiannis Giannakopoulos, Alexander Grosz, and Themistoklis Melissourgos. On the smoothed
complexity of combinatorial local search. CoRR , abs/2211.07547, 2022.
Angeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos. On
the rate of convergence of regularized learning in games: From bandits and uncertainty to optimism
and beyond. In Proceedings of the Annual Conference on Neural Information Processing Systems
(NeurIPS) , 2021.
Andrew Gilpin, Javier Peña, and Tuomas Sandholm. First-order algorithm with O(ln(1/ϵ))conver-
gence for ϵ-equilibrium in two-person zero-sum games. Mathematical Programming , 133(1–2):
279–298, 2012.
Noah Golowich, Sarath Pattathil, and Constantinos Daskalakis. Tight last-iterate convergence rates
for no-regret learning in multi-player games. In Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS) , 2020a.
Noah Golowich, Sarath Pattathil, Constantinos Daskalakis, and Asuman E. Ozdaglar. Last iterate is
slower than averaged iterate in smooth convex-concave saddle point problems. In Conference on
Learning Theory (COLT) , 2020b.
Eduard Gorbunov, Adrien Taylor, and Gauthier Gidel. Last-iterate convergence of optimistic gradient
method for monotone variational inequalities. In Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS) , 2022.
Michael D. Grigoriadis and Leonid G. Khachiyan. A sublinear-time randomized approximation
algorithm for matrix games. Operations Research Letters , 18(2):53–58, 1995.
Nika Haghtalab, Michael I. Jordan, and Eric Zhao. On-demand sampling: Learning optimally from
multiple distributions. In Proceedings of the Annual Conference on Neural Information Processing
Systems (NeurIPS) , 2022.
Nika Haghtalab, Michael I. Jordan, and Eric Zhao. A unifying perspective on multi-calibration:
Unleashing game dynamics for multi-objective learning. In Proceedings of the Annual Conference
on Neural Information Processing Systems (NeurIPS) , 2023.
Yu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, and Panayotis Mertikopoulos. On the convergence
of single-call stochastic extra-gradient methods. In Proceedings of the Annual Conference on
Neural Information Processing Systems (NeurIPS) , 2019.
Sophie Huiberts, Yin Tat Lee, and Xinzhi Zhang. Upper and lower bounds on the smoothed complexity
of the simplex method. In Proceedings of the Annual Symposium on Theory of Computing (STOC) ,
2023.
13Galina M Korpelevich. The extragradient method for finding saddle points and other problems.
Matecon , 12:747–756, 1976.
Chung-Wei Lee, Christian Kroer, and Haipeng Luo. Last-iterate convergence in extensive-form
games. In Proceedings of the Annual Conference on Neural Information Processing Systems
(NeurIPS) , 2021.
Chris Junchi Li, Huizhuo Yuan, Gauthier Gidel, Quanquan Gu, and Michael I. Jordan. Nesterov
meets optimism: Rate-optimal separable minimax optimization. In International Conference on
Machine Learning (ICML) , 2023.
Pouria Mahdavinia, Yuyang Deng, Haochuan Li, and Mehrdad Mahdavi. Tight analysis of extra-
gradient and optimistic gradient methods for nonconvex minimax problems. In Proceedings of the
Annual Conference on Neural Information Processing Systems (NeurIPS) , 2022.
Arnab Maiti, Kevin G. Jamieson, and Lillian J. Ratliff. Instance-dependent sample complexity bounds
for zero-sum matrix games. In International Conference on Artificial Intelligence and Statistics
(AISTATS) , 2023.
Konstantin Makarychev and Yury Makarychev. Perturbation Resilience , page 95–119. Cambridge
University Press, 2021.
Panayotis Mertikopoulos, Christos H. Papadimitriou, and Georgios Piliouras. Cycles in adversarial
regularized learning. In Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) , 2018.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar,
and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra
(gradient) mile. In International Conference on Learning Representations (ICLR) , 2019.
Aryan Mokhtari, Asuman E. Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In International
Conference on Artificial Intelligence and Statistics (AISTATS) , 2020.
Matej Morav ˇcík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial
intelligence in heads-up no-limit poker. Science , 356(6337):508–513, 2017.
Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming , 103,
2005.
Courtney Paquette, Bart van Merriënboer, Elliot Paquette, and Fabian Pedregosa. Halting time is
predictable for large models: A universality property and average-case analysis. Found. Comput.
Math. , 23(2):597–673, 2023.
Julien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer,
Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie,
Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair,
Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau,
Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi
Munos, David Silver, Satinder Singh, Demis Hassabis, and Karl Tuyls. Mastering the game of
stratego with model-free multiagent reinforcement learning. Science , 378(6623):990–996, 2022.
L.D. Popov. A modification to the Arrow-Hurwicz method for search of saddle-points. Mathematical
Notes of the Academy of Sciences of the USSR , 28(5):845–848, 1980.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Conference
on Learning Theory , pages 993–1019, 2013.
James Renegar. Incorporating condition measures into the complexity theory of linear programming.
SIAM Journal on Optimization , 5(3):506–524, 1995.
Jarnes Renegar. Some perturbation theory for linear programming. Mathematical Programming , 65:
73–91, 1994.
14Ralph Tyrell Rockafellar. Convex Analysis . Princeton university press, 2015.
I. Romanovskii. Reduction of a game with complete memory to a matrix game. Soviet Mathematics ,
3, 1962.
Tim Roughgarden. Beyond the Worst-Case Analysis of Algorithms . Cambridge University Press,
2021.
Aviad Rubinstein. Settling the complexity of computing approximate two-player nash equilibria.
In Irit Dinur, editor, Proceedings of the Annual Symposium on Foundations of Computer Science
(FOCS) , 2016.
Damien Scieur and Fabian Pedregosa. Universal average-case optimality of polyak momentum. In
International Conference on Machine Learning (ICML) , 2020.
Zhuoqing Song, Jason D. Lee, and Zhuoran Yang. Can we find nash equilibria at a linear rate in
markov games? In International Conference on Learning Representations (ICLR) , 2023.
Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of termination of linear programming
algorithms. Math. Program. , 97(1-2):375–404, 2003.
Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. Journal of the ACM , 51(3):385–463, 2004.
Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis: an attempt to explain the behavior of
algorithms in practice. Commun. ACM , 52(10):76–84, 2009.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of
regularized learning in games. In Advances in Neural Information Processing Systems , 2015.
Xiaohang Tang, Le Cong Dinh, Stephen Marcus McAleer, and Yaodong Yang. Regret-minimizing
double oracle for extensive-form games. In International Conference on Machine Learning (ICML) ,
Proceedings of Machine Learning Research, 2023.
Terence Tao. Topics in random matrix theory , volume 132. American Mathematical Society, 2023.
Terence Tao, Van Vu, and Manjunath Krishnapur. Random matrices: Universality of ESDs and the
circular law. The Annals of Probability , 38(5):2023 – 2065, 2010.
Paul Tseng. On linear convergence of iterative methods for the variational inequality problem.
Journal of Computational and Applied Mathematics , 60(1):237–252, 1995.
Eric van Damme. Stability and perfection of Nash equilibria , volume 339. Springer, 1991.
Jan van den Brand, Yin Tat Lee, Yang P. Liu, Thatchaphol Saranurak, Aaron Sidford, Zhao Song, and
Di Wang. Minimum cost flows, mdps, and ℓ1-regression in nearly linear time for dense instances.
InProceedings of the Annual Symposium on Theory of Computing (STOC) , 2021.
Daniil Vankov, Angelia Nedi ´c, and Lalitha Sankar. Last iterate convergence of popov method for
non-monotone stochastic variational inequalities, 2023.
John von Neumann. Zur Theorie der Gesellschaftsspiele. Mathematische Annalen , 100:295–320,
1928.
John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior . Princeton
University Press, 1947.
Bernhard von Stengel. Efficient computation of behavior strategies. Games and Economic Behavior ,
14(2):220–246, 1996.
Bernhard von Stengel. Zero-sum games and linear programming duality. Mathematics of Operations
Research , 2023.
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence
in constrained saddle-point optimization. In International Conference on Learning Representations
(ICLR) , 2021.
15Yinyu Ye. Interior point algorithms: theory and analysis . John Wiley & Sons, 2011.
Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, and Jelena Diakonikolas. Robustly learning single-
index models via alignment sharpness. In International Conference on Machine Learning (ICML) ,
2024.
Brian Hu Zhang and Tuomas Sandholm. Sparsified linear programming for zero-sum equilibrium
finding. In International Conference on Machine Learning (ICML) , 2020.
Martin Zinkevich, Michael Bowling, Michael Johanson, and Carmelo Piccione. Regret minimization
in games with incomplete information. In Proceedings of the Annual Conference on Neural
Information Processing Systems (NIPS) , 2007.
16A Further related work
Besides the pioneering work of Spielman and Teng [2004], which revolved around the simplex
algorithm, other prominent algorithms for solving linear programs have also been investigated
through the lens of smoothed complexity. Blum and Dunagan [2002] showed that perceptron, a
popular algorithm in machine learning, also enjoys a polynomial smoothed complexity (with high
probability) for solving linear programming feasibility problems, which can also capture general
linear programs via a binary search procedure. Further, Dunagan et al. [2011] performed a smoothed
analysis of interior-point methods by relying on an earlier characterization due to Renegar [1995].
Beyond linear programming and (two-player) zero-sum games, there has been a considerable interest
in understanding the smoothed complexity of Nash equilibria in general-sum games, but the outlook
that has emerged from this endeavor is rather bleak [Chen et al., 2009, Boodaghians et al., 2020,
Rubinstein, 2016]. On a more positive note, Daskalakis et al. [2024] recently considered a more
permissive solution concept they refer to as a smooth Nash equilibrium ; the basic idea of their
relaxation is that instead of considering best-response deviations, they restrict to deviations that do
not assign too much probability mass on any pure strategy, as controlled by a certain parameter. For a
certain regime of that parameter, they obtained positive results, bypassing the intractability of the
usual Nash equilibrium. Considering smooth Nash equilibria could also be fruitful in the context of
zero-sum games. In particular, we surmise that, if one is content with convergence to smooth Nash
equilibria, the error bound could exhibit more favorable properties. Smoothed analysis has also been
applied to more structured classes of games, such as congestion or potential games [Giannakopoulos,
2023, Giannakopoulos et al., 2022, Chen et al., 2020], as well as other important problems in game
theory [Gatti et al., 2013, Buriol et al., 2011]. Other notable developments in a broader context were
covered in an older survey by Spielman and Teng [2009]; for more recent developments, we point to,
for example, Christ and Yannakakis [2023], Chen et al. [2024], Huiberts et al. [2023], and the many
references therein.
Average-case analysis has also been a popular topic in the optimization literature [Cunha et al., 2022,
Paquette et al., 2023, Scieur and Pedregosa, 2020], and so it is worth relating our results to that line of
work. In particular, let us focus on the recent work of Cunha et al. [2022]. First, that paper targets a
certain class of convex quadratic problems, whereas we examine zero-sum games. They also operate
under a different perturbation model, deriving a parametrization based on the concentration of the
eigenvalues of a certain matrix. Further, without strong convexity, Cunha et al. [2022] establish a
complexity scaling with poly(1/ϵ), while here we target the log(1/ϵ)regime. We finally remark that
the techniques employed are also quite different. In particular, Cunha et al. [2022, Problem 2.1]
posit that the optimal solution does not depend on the underlying randomization. In contrast, as we
have already highlighted, the fact that the equilibrium is a function of the randomization constitutes
the main technical crux in our setting. At the same time, Cunha et al. [2022] encountered several
challenges not present in our setting, so overall those results are complementary.
Beyond smoothed complexity, understanding the last-iterate convergence of gradient-based methods
such as OGDA andEGDA has received tremendous interest in the literature; e.g., [Golowich et al., 2020a,
Cai et al., 2022, Gorbunov et al., 2022, Vankov et al., 2023, Golowich et al., 2020b, Mahdavinia et al.,
2022, Antonakopoulos et al., 2021, Mertikopoulos et al., 2019, Abe et al., 2023]. It is worth noting
that linear convergence has also been documented for the more challenging class of extensive-form
games [Lee et al., 2021], as well as Markov games [Song et al., 2023]. Nevertheless, there are lower
bounds precluding linear convergence beyond affine variational inequalities [Golowich et al., 2020a,
Wei et al., 2021]. We also refer to the works of Cohen et al. [2017] and Giannou et al. [2021] for
further characterizations of the convergence rate of no-regret dynamics in multi-player games.
Contrary to the above line of work, which focuses on last-iterate convergence, the most common
approach to solving zero-sum games revolves around regret minimization whereby optimality guar-
antees concern the average strategies. Learning in such settings has been a popular research topic
as it captures many central problems; two notable recent applications are learning from multiple
distributions [Haghtalab et al., 2022] and multi-calibration [Haghtalab et al., 2023]. Yet, there are
at least three limitations of the no-regret framework worth highlighting here. The first one, which
has been stressed extensively already, is that the number of iterations must grow at least as Ω(1/ϵ)
when one insists on taking (uniform) averages [Daskalakis et al., 2015]. The second and more
nuanced caveat is that the no-regret framework does not provide instance-based guarantees based
on natural game-theoretic parameters of the problem (see, for example, the discussion of Maiti
17et al. [2023]). Building on earlier work [Wei et al., 2021, Tseng, 1995], some of our results here
attempt to address this shortcoming by coming up with a more interpretable parameterization of the
iteration complexity of algorithms such as OGDA . The final limitation is that, convergence to the set of
equilibria notwithstanding, no-regret guarantees provide no information regarding properties of the
equilibrium reached. Although not an issue in non-degenerate zero-sum games, equilibrium selection
still remains a central problem. Earlier results [Wei et al., 2021, Tseng, 1995] provide an interesting
characterization for the last iterate of OGDA andEGDA by showing that the limit point is the projection
of the initial point to the set of equilibria.
Finally, it is worth pointing out the best available theoretical guarantees for solving zero-sum
games. Assuming that each entry of Ahas absolute value bounded by 1,(1)can be solved in
˜O(max{n, m}ω)[Cohen et al., 2021] or ˜O(nm+min{n, m}5/2)[van den Brand et al., 2021]. Here,
ωis the exponent of matrix multiplication and ˜Osuppresses polylogarithmic factors in nandm.
The complexity we obtain for algorithms such as OGDA is not competitive even though we work in
the more benign smoothed complexity model; we reiterate that we did not attempt to optimize the
polynomial factors in terms of nandm, and those can almost certainly be improved. On the other
hand, there are two main aspects in which algorithms such as OGDA are more appealing in terms of
their scalability: the per-iteration complexity and the memory requirements. An algorithm such as
OGDA requires a single matrix-vector product in each iteration, which can be implemented in linear
time for sparse matrices, and has a limited memory footprint. In contrast, implementing interior-point
methods in large games can be prohibitive.
B Preliminaries
In this section, we introduce some further background on smoothed complexity and define the
algorithms cited earlier (Items 1 to 4).
Further notation For a random variable X, we denote by E[X]its expectation and by V[X]its
variance, under the assumption that both are finite. For a sequence of random variables X1, . . . , X d
and scalars α1, . . . , α d∈R, linearity of expectation yields that E[α1X1+···+αdXd] =α1E[X1]+
···+αdE[Xd]. Assuming independence, it also holds that V[α1X1+···+αdXd] = (α1)2V[X1] +
···+ (αd)2V[Xd]. We will also use the fact that a linear combination of independent Gaussian
random variables is also Gaussian. More broadly, linear combinations can be understood through a
convolution in the space of probability density functions, which means that smoothness (in the sense
of Lemma C.7) is preserved in a certain regime.
B.1 Smoothed complexity
To fully specify Definition 1.1, we first recall that a (univariate) Gaussian random variable with zero
mean and variance σ2admits a probability density function of the form
µ:t7→1
σ√
2πexp
−t2
2σ2
.
The law of such a Gaussian random variable will be denoted by N(0, σ2). In the original work
of Spielman and Teng [2004], smoothed complexity was defined as the expected running time (or
some other cost function) of some algorithm over the perturbed input. More precisely, let Abe an
algorithm whose inputs can be expressed as vectors in Rd, and let TA(I)be the running time of
algorithm Aon input I ∈Rd. Then, the smoothed complexity ofAis
CA(d, σ):= max
I∈RdEg∼N(0d,σ2Id×d)[TA(I+∥I∥g)].
As pointed out by Spielman and Teng [2003], one does not need to limit smoothed analysis to measure
the expected running time, and high probability guarantees are also quite natural; see, for example,
the smoothed analysis of the perceptron algorithm due to Blum and Dunagan [2002]. Our main result
also provides a guarantee with high probability; it is not clear whether the expected running time can
also be bounded by poly(n, m, 1/σ), which is left for future work.
B.2 Algorithms
Next, we specify the algorithms we consider in this work.
18Optimistic gradient descent/ascent Originally proposed by Popov [1980], optimistic gradient
descent/ascent ( OGDA )—and variants thereof [Hsieh et al., 2019]—has been recently revived in the
online learning literature commencing from the pioneering works of Rakhlin and Sridharan [2013]
and Chiang et al. [2012]. If we denote for compactness F(z):= (Ay,−A⊤x),OGDA can be
expressed as follows for t∈N(={1,2, . . . ,}).
z(t):= ΠZ(bz(t)−ηF(z(t−1)),
bz(t+1):= ΠZ(bz(t)−ηF(z(t)).(OGDA )
Here, η >0is the learning rate ;ΠZ(·)denotes the (Euclidean) projection operator on set Z:=X×Y ;
andz(0)=bz(1)∈ Z is the initialization. That is, players simultaneously update their strategies
through optimistic gradient steps. Given that XandYare probability simplexes, each projection
can be computed exactly in nearly linear time. The key reference point for OGDA in affine variational
inequalities is the work of Wei et al. [2021] who established linear convergence using the notion of
metric subregularity (Definition C.9), which is strongly related to Definition 1.3; we discuss their
approach later in Appendix C.3.
Optimistic multiplicative weights update Deriving from the same class of online learning al-
gorithms as OGDA , optimistic multiplicative weights ( OMWU ) is the incarnation of optimistic mirror
descent with an entropic regularizer, namely
x(t)∝x(t−1)◦exp
−2ηAy(t−1)+ηAy(t−2)
,
y(t)∝y(t−1)◦exp
2ηA⊤x(t−1)−ηA⊤x(t−2) (OMWU )
fort∈N.5Above, ◦denotes the component-wise product; the exponential mapping exp(·)is also to
be applied component-wise; and z(−1):=z(0):= (1
n1n,1
m1m). Daskalakis and Panageas [2019]
first proved that OMWU exhibits asymptotic (last-iterate) convergence, and Wei et al. [2021] later
established linear convergence.
Remark B.1.It is important to note here that the exponential map of OMWU can produce iterates with
an arbitrarily large number of bits. Nevertheless, it is not hard to show that the analysis of Wei et al.
[2021] carries over when the iterates are truncated up to a certain length of the most significant bits,
and so we will not dwell further on this issue here.
Extra-gradient descent/ascent The extra-gradient method of Korpelevich [1976] is quite similar
toOGDA , namely
bz(t):= ΠZ(z(t)−ηF(z(t)),
z(t+1):= ΠZ(z(t)−ηF(bz(t))(EGDA )
fort∈N. Unlike OGDA , one caveat is that it requires two gradient evaluations per each iteration
t.EGDA is also less suited to use in an online environment: it requires more feedback than what is
provided in the online learning setting, and in fact, even legitimate variants of EGDA can still incur
substantial regret [Golowich et al., 2020a]. Tseng [1995] first established that EGDA exhibits linear
convergence for problems such as (1), discussed further in Appendix C.3.
Iterative smoothing This is a refinement of Nesterov’s classical smoothing technique [Nesterov,
2005] due to Gilpin et al. [2012]. Let us first recall the vanilla version of Nesterov, which we refer to
asSmoothing (A,z(0), ϵ):
1. Initialize η:=ϵ
DZandbz(0):=z(0), where DZis the ℓ2diameter of Z.
2. For t= 0,1, . . .
(a)u(t):=2
2+tbz(t)+t
t+2z(t).
(b)
z(t+1):= arg min
z∈Z
⟨∇Fη(u(t)),z−u(t)⟩+L2
2η∥z−u(t)∥2
,
5OMWU is oftentimes expressed via the (optimistic) mirror descent viewpoint, but the form we provide here is
easily seen to be equivalent.
19where Fη(z):= max bz∈Z{⟨F(z),z−bz⟩ −η
2∥z−bz∥2}andLis a suitable matrix
norm.
(c) If Φ(z(t+1))< ϵ,return .
(d)
bz(t+1):= arg min
bz∈Z(tX
τ=0τ+ 1
2⟨∇Fη(u(τ)),bz−u(τ)⟩+L2
2η∥bz−z(0)∥2)
.
In this context, IterSmooth (A,z(0), ρ, ϵ)is simple refinement of Smoothing , which nonetheless
attains linear convergence [Gilpin et al., 2012].
1. Let ϵ(0)=F(z(0)).
2. For t= 0,1, . . .
(a)ϵ(t+1):=ϵ(t)
ρ.
(b)z(t+1):=Smoothing (A,z(t), ϵ(t+1)).
(c) If Φ(z(t+1))< ϵ,return .
C Omitted proofs
We dedicate this section to the proofs omitted earlier from the main body.
C.1 Proofs from Section 3.2
We first point out that degenerates games have measure zero ( cf.Spielman and Teng [2003, Proposition
5.1]).
Lemma C.1. For a Gaussian distributed payoff matrix Aper Definition 1.1, the game is non-
degenerate (Definition 3.2) with probability 1(almost surely).
Indeed, the set of games with a non unique equilibrium has measure zero [van Damme, 1991, Theorem
3.5.1]. Regarding the characterization in terms of the number of tight inequalities of the corresponding
(primal and dual) linear programs, gathered in Definition 3.2, we note that if n+ 1of the inequalities
were tight at x⋆, that would induce a feasible linear system of nequalities (by eliminating v) inn−1
variables (by eliminating one of the redundant variables); such degeneracies have measure zero, and
there are only finitely many possible such degeneracies, leading to Lemma C.1. As a result, in the
smoothed complexity model, we can safely assume that the game is non-degenerate.
Now, as we alluded to earlier, establishing Definition 1.3 reduces to showing that for any points
x∈ X andy∈ Y,
max
y′∈Y⟨x,Ay′⟩ −v≥κ∥x−ΠX⋆(x)∥=κ∥x−x⋆∥, (8)
v−min
x′∈X⟨x′,Ay⟩ ≥κ∥y−ΠY⋆(y)∥=κ∥y−y⋆∥. (9)
(Definition 1.3 then indeed follows from the obvious fact ∥x−x⋆∥+∥y−y⋆∥ ≥ ∥ z−z⋆∥.)
Accordingly, our proof of Theorem 3.6 below will focus on lower bounding κso that (8)holds,
and (9) can then be treated similarly.
Before we proceed, let us make some observations regarding transformation (5)we saw earlier. First,
one can understand the transformation A♭
B,N=T(Q♭,b,c, d)through the equations
d=Ai,j;bj′=−Ai,j′+Ai,j;ci′=−Ai′,j+Ai,j;Qi′,j′=Ai′,j′−Ai,j′−Ai′,j+Ai,j(10)
for all (i′, j′)∈eB×eN. This can easily be derived from (5)by using the fact that bxB= (ex,1−1⊤ex)
andbyN= (ey,1−1⊤ey). From (10), we see that there is a permutation of the rows of Tthat is upper
triangular, with every entry being either 1or−1. This implies that |det(T)|= 1. With a slight
abuse of notation, we will write Ti,j(as opposed to T(i,j),:) to access the (i, j)row of T, so that
Ai,j=⟨Ti,j,(Q♭,b,c, d)⟩. From (10), we also see that Ti,jcontains at most 4non-zero entries. In
turn, this implies that ∥Ti,j∥ ≤2and∥Ti,j∥1≤4. We gather the above observations in the claim
below, which will be used in the sequel.
20Claim C.2. For the (linear) transformation T∈R(BN)×(BN)given in (10), it holds that |det(T)|=
1. Further, ∥Ti,j∥ ≤2and∥Ti,j∥1≤4for all (i, j)∈B×N.
The point of transformation (5)is that, as we claimed earlier, the spectral properties of matrix Q(as
opposed to AB,N, which is a natural candidate) suffice to capture the difficulty of addressing the
second subproblem identified in Section 3.2. In addition, there is a straightforward but convenient
characterization of the equilibrium (x⋆,y⋆)in terms of the transformed game in (5), as stated below.
Claim C.3. It holds that Qey⋆=candQ⊤ex⋆=b.
Proof. It is clear that the vector Qey⋆−cmust have the same value in every coordinate since ex⋆
is fully supported and a best response (by assumption). If that entry was positive, then ex⋆would
not be a best response since Player xcould profit from removing all the probability mass (which is
possible sinceP
i∈eBx⋆
i>0). If there was a negative entry, Player xwould profit from increasing its
probability mass (which is possible sinceP
i∈eBx⋆
i<1). Similar reasoning yields Q⊤ex⋆=b.
Having made the above observations, we next prove some lemmas claimed earlier in Section 3.2
which will be used for the proof of Theorem 3.6. First, we give the proof of Lemma 3.4.
Lemma 3.4. Letc=Qey⋆=P
j∈eNey⋆
jQ:,j, and suppose that Q∈ReB×eNis such that its jth
column is equal to Q:,j−c. Then,
min
j∈eNdist(Q:,j,span(Q:,eN−j))≤ 
1 +|eN|
1−P
j∈eNy⋆
j!
min
j∈eNdist(Q:,j,span(Q:,eN−j)).
Proof. LeteN∋j′∈arg minj∈eNdist(Q:,j,span(Q:,eN−j)). By definition, there is ρ∈ReN−j′and
r∈ReNwith∥r∥= 1such that
Q:,j:=−X
j∈eN−j′ey⋆
jQ:,j+ (1−y⋆
j′)Q:,j′=X
j∈eN−j′ρj(Q:,j−c) +ϵr,
where ϵ:= minj∈eNdist(Q:,j,span(Q:,eN−j)). Rearranging, we have
Q:,j′ϕj′z }| {
1−y⋆
j′+y⋆
j′X
j∈eN−j′ρj
+X
j∈eN−j′Q:,jϕjz }| {
−y⋆
j−ρj+y⋆
jX
j′′∈eN−j′ρj′′
=ϵr.(11)
Now, let us suppose that all coefficients above are such that |ϕj| ≤ϵ′:=1−P
j∈fNy⋆
j
1−P
j∈fNy⋆
j+|eN|for all
j∈eN. Then,P
j∈eNϕj=±|eN|ϵ′since|P
j∈eNϕj| ≤P
j∈eN|ϕj| ≤ϵ|eN|, where for convenience
we used the notationP
j∈eNϕj=±|eN|ϵ′⇐⇒ −| eN|ϵ′≤P
j∈eNϕj≤ |eN|ϵ′. Thus, by definition
ofϕj,
1−X
j∈eNy⋆
j

X
j∈eN−j′ρj
=
1−X
j∈eNy⋆
j
±ϵ′|eN|.
Since 0<1−P
j∈eNy⋆
j, we have

X
j∈eN−j′ρj
= 1±ϵ′|eN|
1−P
j∈eNy⋆
j.
Thus,
ϕj′= 1−y⋆
j′+y⋆
j′X
j∈eN−j′ρj= 1±ϵ′|eN|
1−P
j∈eNy⋆
j> ϵ′
21since ϵ′≤1−P
j∈fNy⋆
j
1−P
j∈fNy⋆
j+|eN|. The last displayed inequality contradicts our earlier assumption that
|ϕj′| ≤ϵ′. As a result, we conclude that at least one coefficient ϕjhas an absolute value at least ϵ′.
Dividing (11) by that coefficient, we get
min
j∈eNdist(Q:,j,span(Q:,eN−j))≤ϵ
ϵ′≤ 
1 +|eN|
1−P
j∈eNy⋆
j!
min
j∈eNdist(Q:,j,span(Q:,eN−j)).
This completes the proof.
We continue with the proof of Lemma 3.5.
Lemma 3.5. LetM∈Rd×dbe a full-rank matrix. For any ex∈Rdthere is p∈Rdwith
∥p∥ ≤1
σmin(M)∥ex∥such that
ex=Mp=dX
j=1pjM:,j.
Proof. LetM=UΣV⊤be a singular value decomposition (SVD) of Q, where UandVare
orthonormal. Then, given that Qis invertible (by assumption),
p=VΣ−1U⊤ex,
where Σ−1=diag(σ−1
min, . . . , σ−1
max). (Here, σmaxandσminare the maximum and minimum singular
values of M, respectively.) Thus, ∥p∥ ≤ ∥V∥∥Σ−1∥∥U⊤∥∥ex∥ ≤1
σmin(Q)∥ex∥, where we used the
fact that the spectral norm of any orthonormal matrix is 1and the spectral norm of any diagonal
matrix is its maximum entry in asbolute value.
We next state the negative second moment identity that connects the smallest singular values in
terms of a certain geometric property of the matrix (namely, Item 3) (see also [Tao, 2023] for further
background).
Proposition C.4 (Negative second moment identity [Tao et al., 2010]) .LetM∈Rd×dbe an
invertible matrix. Then,
dX
r=11
σ2r(M)=dX
r=11
dist2(Mr,:, H−r,:)=dX
r=11
dist2(M:,r, H:,−r), (12)
where H−r,::=span(M1,:, . . . ,Mr−1,:,Mr+1,:, . . . ,Md,:).
One can readily prove this identity by equivalently expressing the negative second moment
tr((M−1)⊤M−1)as eitherPd
r=1σ2
r(M−1) =Pd
r=1σ−2
r(M)orPd
r=1∥M−1
:,r∥2, leading to the
first identity in (12). The second one follows from the fact that the singular values of M⊤coincide
with the singular values of M.
We are now ready to prove Theorem 3.6, restated below.
Theorem 3.6. LetAbe a non-degenerate payoff matrix, and suppose that (αP(A), αD(A)),
(βP(A), βD(A))and(γP(A), γD(A))are as in Definition 3.3. Then, the error bound (Defini-
tion 1.3) is satisfied for any sufficiently small modulus
κ≳1
∥A♭∥∞1
min(n, m)3min
(αD(A))2βD(A)γP(A),(αP(A))2βP(A)γD(A)	
.
Proof. We lower bound κso that (8)holds; bound (9)will then be treated in a symmetric fashion,
and Definition 1.3 will follow.
Let us fix any point x∈ X. We can write xasλbxB+ (1−λ)bxBfor some λ∈[0,1]such that
bxB∈ X and all coordinates of bxBinBare zero, and bxB∈ X and all coordinates of bxBinBare
zero. For notational convenience, we define
P(A):=1
2|N|p
|B|σmin(Q)
1 +1
αD(A)−1
. (13)
We consider the following two cases.
22Case I: λP(A)∥bxB−x⋆
B∥ ≥4(1−λ)∥A♭∥∞. IfbxB=x⋆
B, it follows that x=x⋆(since λ= 1),
and the conclusion trivially follows. We can thus assume that bxB̸=x⋆
B. In this case, it follows that
eB̸=∅, and we proceed as follows.
max
y′∈Y⟨x,Ay′⟩ −v≥λmax
j∈N⟨bxB−x⋆
B,AB,j⟩+ (1−λ)
⟨xB,AB,j⟩ −v
(14)
≥λmax
j∈N⟨bxB−x⋆
B,AB,j⟩ −2(1−λ)∥A♭∥∞, (15)
where (14) follows from the definition x:=λbxB+ (1−λ)bxBand the fact that v=⟨x⋆
B,AB,j⟩for
allj∈N; and (15) uses definition of ∥A♭∥∞to lower bound the second term in (14). Continuing
from (15), we can use the transformation defined in (5) to get
max
j∈N⟨bxB−x⋆
B,AB,j⟩= max
j∈N⟨ex−ex⋆,Q:,j−c⟩, (16)
where, with an abuse of notation, the convention above is that Q:,j=0ifj̸=eN. For convenience,
let us define χj:=⟨ex−ex⋆,Q:,j−c⟩for all j∈N. Our goal is to lower bound max j∈Nχj. To
that end, we first observe that, by the fact that Qey⋆=c(Claim C.3),
0 =⟨ex−ex⋆,Qey⋆−c⟩=X
j∈eNey⋆
j⟨ex−ex⋆,Q:,j⟩ − ⟨ex−ex⋆,c⟩
=X
j∈eNey⋆
j⟨ex−ex⋆,Q:,j−c⟩+
1−X
j∈eNey⋆
j
⟨ex−ex⋆,−c⟩.
In other words,X
j∈Ny⋆
jχj= 0,
which in turn implies that
X
j∈Nmax(0 , χj)≥X
j∈Ny⋆
jmax(0 , χj) =−X
j∈Ny⋆
jmin(0 , χj)
≥ −αD(A)X
j∈Nmin(0 , χj), (17)
where we made use of the obvious identity t= max(0 , t) + min(0 , t)for all t∈R, as well as the
definition of αD(A)(Item 1). We let p∈ReNbe the (unique) solution to the linear system
ex−ex⋆=Qp=X
j∈eN(Q:,j−c)pj,
andpj= 0forj∈N\eN. By Lemma 3.5, we know that ∥p∥ ≤(σmin(Q))−1∥ex−ex⋆∥. Then, we
have
X
j∈Nχjpj=X
j∈eNχjpj=*
ex−ex⋆,X
j∈eN(Q:,j−c)pj+
=∥ex−ex⋆∥2. (18)
Moreover,X
j∈Nχjpj=X
j∈Npjmax(0 , χj) +X
j∈Npjmin(0 , χj)
≤X
j∈Nmax(0 ,pj) max(0 , χj) +X
j∈Nmin(0 ,pj) min(0 , χj) (19)
≤ ∥p∥∞X
j∈Nmax(0 , χj)− ∥p∥∞X
j∈Nmin(0 , χj) (20)
≤ ∥p∥∞
1 +1
αD(A)X
j∈Nmax(0 , χj) (21)
≤1
σmin(Q)
1 +1
αD(A)
|N|max
j∈Nχj∥ex−ex⋆∥, (22)
23where (19) follows from the fact that pjmax(0 , χj)≤max(0 ,pj) max(0 , χj)(by nonnegativity of
max(0 , χj)) andpjmin(0 , χj)≤min(0 ,pj) min(0 , χj)(by nonpositivity of min(0 , χj));(20) uses
thatmin(0 ,pj)≥ −|pj| ≥ −∥ p∥∞, which gives min(0 ,pj) min(0 , χj)≤ −∥p∥∞min(0 , χj);
(21) follows from (17); and (22) uses the bound ∥p∥2≤(σmin(Q))−1∥ex−ex⋆∥(Lemma 3.5).
Combining (18) and (22),
max
j∈N⟨bxB−x⋆
B,AB,j⟩ ≥1
|N|σmin(Q)
1 +1
αD(A)−1
∥ex−ex⋆∥ (23)
≥1
2|N|p
|B|σmin(Q)
1 +1
αD(A)−1
∥bxB−x⋆
B∥, (24)
where (23) uses the definition of χjand the assumption that ex̸=ex⋆(equivalently, x⋆
B̸=bxB),
and (24) follows from the bound
∥bxB−x⋆
B∥ ≤ ∥bxB−x⋆
B∥1≤X
i∈eB|xi−x⋆
i|+X
i∈eB(xi−x⋆
i)≤2∥ex−ex⋆∥1≤2p
|B|∥ex−ex⋆∥.
Returning to (15), we have
max
y′∈Y⟨x,Ay′⟩ −v≥λ1
2|N|p
|B|σmin(Q)
1 +1
αD(A)−1
∥bxB−x⋆
B∥ −2(1−λ)∥A♭∥∞
=λP(A)∥bxB−x⋆
B∥ −2(1−λ)∥A♭∥∞, (25)
where the equality above follows from the definition of P(A)in (13). Next, we bound
∥x−x⋆∥2=∥λbxB−x⋆
B∥2+ (1−λ)2∥bxB∥2
=∥λ(bxB−x⋆
B)−(1−λ)x⋆
B∥2+ (1−λ)2∥bxB∥2
≤2λ2∥bxB−x⋆
B∥2+ 2(1−λ)2∥x⋆
B∥2+ (1−λ)2∥bxB∥2(26)
≤2λ2∥bxB−x⋆
B∥2+ 3(1−λ)2, (27)
where (26) uses triangle inequality with respect to ∥ · ∥ along with the inequality (t1+t2)2≤
2t2
1+ 2t2
2, and (27) uses that ∥x⋆
B∥,∥bxB∥ ≤1. Since we are assuming that λP(A)∥bxB−x⋆
B∥ ≥
4(1−λ)∥A♭∥∞, (27) in turn implies that
∥x−x⋆∥2≤2λ2∥bxB−x⋆
B∥2+λ2P(A)
∥A♭∥∞2
∥bxB−x⋆
B∥2
=λ2 
2 +P(A)
∥A♭∥∞2!
∥bxB−x⋆
B∥2. (28)
Combining (25) and (28) with the assumption that λP(A)∥bxB−x⋆
B∥ ≥4(1−λ)∥A♭∥∞,
max
y′∈Y⟨x,Ay′⟩ −v≥λ
2P(A)∥bxB−x⋆
B∥
≥1
2P(A) 
2 +P(A)
∥A♭∥∞2!−2
∥x−x⋆∥ ≥κ(A)∥x−x⋆∥.
It is easy to see that P(A)/∥A♭∥∞is upper bounded by an absolute constant, and so we have
max
y′∈Y⟨x,Ay′⟩ −v≳P(A)∥x−x⋆∥=1
2|N|p
|B|σmin(Q)
1 +1
αD(A)−1
∥x−x⋆∥
≳1
|B|3(αD(A))2γP(A)∥x−x⋆∥.
Above, the last bound uses the fact that
σmin(Q)≥1q
|eB|min
j∈eNdist(Q:,j,span(Q:,eN−j))
≥1
|eB|3/2min
j∈eNdist(Q:,j,span(Q:,eN−j))αD(A) =1
|eB|3/2γP(A)αD(A),
where the first inequality uses (6), while the second one is a consequence of Lemma 3.4.
24Case II: λP(A)∥bxB−x⋆
B∥<4(1−λ)∥A♭∥∞. This case can only arise when B̸=∅(for
otherwise λ= 1). Then, we bound
max
y′∈Y⟨x,Ay′⟩ −v≥ ⟨x,Ay⋆⟩ −v
≥λ(⟨bxB−x⋆
B,AB,Ny⋆
N⟩) + (1 −λ)(⟨bxB,AB,Ny⋆
N−v⟩)
≥(1−λ)βD(A), (29)
by definition of βD(A)(Item 2) and the fact that ⟨bxB−x⋆
B,AB,Ny⋆
N⟩=v⟨bxB−x⋆
B,1⟩= 0.
Moreover, by (27) together with the assumption that λP(A)∥bxB−x⋆
B∥<4(1−λ)∥A♭∥∞,
∥x−x⋆∥2≤32∥A♭∥∞
P(A)2
(1−λ)2+ 3(1−λ)2= 
32∥A♭∥∞
P(A)2
+ 3!
(1−λ)2.
Combining with (29) yields
max
y′∈Y⟨x,Ay′⟩ −v≥ 
32∥A♭∥∞
P(A)2
+ 3!−2
βD(A)∥x−x⋆∥
≳P(A)
∥A♭∥∞βD(A)∥x−x⋆∥
≳1
∥A♭∥∞1
|B|3αD(A)2βD(A)γP(A)∥x−x⋆∥.
C.2 Proofs from Section 3.3
We continue with the proofs from Section 3.3. As we have noted already, given that all quantities
of interest in Definition 3.3 depend on the support of the equilibrium, it is natural to proceed by
partitioning the probability space over all possible such configurations. To do so, we will use the
following simple fact [Spielman and Teng, 2003, Proposition 8.1].
Proposition C.5 (Spielman and Teng, 2003) .LetXandYbe random variables distributed according
to an integrable density function. For any event E(X, Y),
P
X,Y[E(X, Y)]≤max
yP
X,Y[E(X, Y)|Y=y] =:max
YP
X,Y[E(X, Y)|Y].
In our application, we want to condition on the event that Bis the support of x⋆andNis the support
ofy⋆. For convenience, we let TypeB,N(A)denote the indicator random variable representing
whether BandNindeed index the positive coordinates of the equilibrium; that is, TypeB,N(A):=
1{B={i∈[n] :x⋆
i(A)>0} ∧N={j∈[m] :y⋆
j(A)>0}}. Unlike general linear programs,
which can be infeasible or unbounded, the linear program induced by a zero-sum game is guaranteed
to be primal and dual feasible, no matter the perturbation (under Definition 1.1). We will thus only
have to condition on events in which BandNare both nonempty. To be able to control the probability
density function upon conditioning on TypeB,N(A), it will be convenient to perform a certain change
of variables, which is described next.
Change of variables Let us denote by AB,Nthe entries of Aexcluding those in AB,N. We
first perform a change of variables from AB,N,AB,N toAB,N,Q,c,b, d, which uses the linear
transformation Tassociated with (5). With this new set of variables at hand, we can conveniently
express Qey⋆=candQ⊤ex⋆=b(Claim C.3). Accordingly, we next perform a change of variables
fromAB,N,Q,c,b, dtoAB,N,Q,x⋆,y⋆, v. When performing those change of variables one has
to account for the transformed probability density function. This can be understood as follows. The
probability of an event E(A)can be expressed asZ
AE(A)µA(A)dA.
The integral above can be cast in terms of a new set of variables Bby computing the corresponding
Jacobian, assuming that it is non-singular. We will make use of this fact in the sequel. The following
lemma gathers some of the above observations regarding the change of variables.
25Lemma C.6 (Change of variables) .LetE(A)be any event that depends on the randomness of A.
Then,
P
A[E(A)]≤max
B,NP
A[E(A)|TypeB,N(A)]
= max
B,NP
AB,N,Q,x⋆,y⋆,v[E(A)|AB,Ny⋆
N≥v1andA⊤
N,Bx⋆
B≤v1].
Indeed, the first inequality above is a consequence of Proposition C.5. The equality then follows from
noting that, when
c=Qey⋆,b=Q⊤ex⋆, v=d− ⟨ex⋆,Qey⋆⟩ ⇐⇒ AB,Ny⋆=v1,A⊤
N,Bx⋆=v1,
the event TypeB,N(A)can be equivalently expressed as AB,Ny⋆
N≥v1andA⊤
N,Bx⋆
B≤v1.
We first bound the probability that βP(A):= minj∈N(v− ⟨x⋆
B,AB,j⟩)is close to 0; the proof for
βD(A)is then symmetric. The key ingredient is the following anti-concentration lemma pertaining
to a conditional Gaussian distribution [Spielman and Teng, 2003, Lemma 8.3].
Lemma C.7 (Spielman and Teng, 2003) .Letgbe a Gaussian random variable of variance σ2and
mean of absolute value at most 1. For ϵ≥0,τ≥1andt≤τ,
P[g≤t+ϵ|g≥t]≤ϵτ
σ2eϵ(τ+3)
σ2.
Proposition 3.8. LetβP(A)be defined as in Item 2. For any ϵ≥0,
P
A
βP(A)≤ϵ
5∥A♭∥∞
≤ϵemin(n, m)2
σ2.
Proof. By Lemma C.6, it suffices to bound
max
B,NP
AB,N,Q,x⋆,y⋆,v[βP(A)≤ϵ′|AB,Ny⋆
N≥v1andA⊤
N,Bx⋆
B≤v1].
By Proposition C.5, it suffices to prove that for all B, N,AB,N,AB,N,Q,x⋆,y⋆, vsatisfying
AB,Ny⋆
N≥v1,
P
AB,N[∃j∈N:v−⟨x⋆
B,AB,j⟩ ≤ϵ′| ∀j∈N:v− ⟨x⋆
B,AB,j⟩ ≥0] (30)
≤X
j∈NP
AB,j[v− ⟨x⋆
B,AB,j⟩ ≤ϵ′| ∀j∈N:v− ⟨x⋆
B,AB,j⟩ ≥0] (31)
≤X
j∈NP
AB,j[v− ⟨x⋆
B,AB,j⟩ ≤ϵ′|v− ⟨x⋆
B,AB,j⟩ ≥0] (32)
=X
j∈NP
gj[gj≤ϵ′−v|gj≥ −v]. (33)
where in (30) the distribution of AB,Nafter conditioning on AB,N,AB,N,Q,x⋆,y⋆,vremains
the same, which is a consequence of independence per Definition 1.1; (31) is an application of the
union bound; (32) uses the fact that the events {v− ⟨x⋆
B,AB,j⟩ ≥0}j∈Nare pairwise indepen-
dent; and (33) defines gj:=−⟨x⋆
B,AB,j⟩, which is a Gaussian random variable with expectation
|E[gj]| ≤max i∈B|Ai,j|and variance V[gj] =P
i∈B(x⋆
i)2V[Ai,j] =σ2P
i∈B(x⋆
i)2(by in-
dependence). In particular, by Cauchy-Schwarz, V[gj]≥1
|B|σ2. Further, by Lemma C.7 (for
τ= max(1 ,|v|/|E[gj]|)), we have
P
gj[gj≤ϵ′−v|gj≥ −v]≤ϵ′max(|v|,|E[gj]|)
V[gj]eϵ′max(4|E[gj]|,3|E[gj]|+|v|)
V[gj]
≤ϵ′min(n, m) max( |v|,|E[gj]|)
σ2eϵ′min(n,m) max(4 |E[gj]|,3|E[gj]|+|v|)
σ2
26for any ϵ′≥0andj∈N, where we note that we applied Lemma C.7 for gj/|E[gj]|(since the
absolute value of the mean has to be at most 1), which has variance V[gj]/(E[gj])2. So, setting
ϵ:=ϵ′(|v|+ 4|E[gj]|),
P
gj
gj≤ϵ
|v|+ 4 max i∈B|Ai,j|−v|gj≥ −v
≤P
gj
gj≤ϵ
|v|+ 4|E[gj]|−v|gj≥ −v
≤ϵmin(n, m)
σ2eϵmin(n,m)
σ2. (34)
Now, when ϵmin(n,m)
σ2>1the proposition is vacuously true, while in the contrary case the claim
follows from (34) and (33).
Next, we proceed with the bound on γP(A). The key ingredient is the observation that a random
variable with a slowly changing density function cannot be too concentrated on any any interval
(Lemma 3.7 due to Spielman and Teng [2003, Lemma 8.2]; we restate it below for convenience). Gaus-
sian random variables have this property, as pointed out by Spielman and Teng [2003, Lemma 8.1].
Lemma C.8 (Spielman and Teng, 2003) .Letµbe the probability density function of a Gaussian
random variable in Rdof variance σ2centered at a point of norm at most 1. Ifdist(r,r′)≤ϵ≤1,
then
µ(r′)
µ(r)≥e−ϵ(∥r∥+2)
σ2.
Lemma 3.7 (Spielman and Teng, 2003) .Letρbe the probability density function of a random
variable X. If there exist δ >0andc∈(0,1]such that
0≤t≤t′≤δ=⇒ρ(t′)
ρ(t)≥c, (7)
then
P[X≤ϵ|X≥0]≤ϵ
cδ.
Proposition 3.9. LetγP(A)be defined as in Item 3. For any ϵ≥0,
P
A"
γP(A)≤ϵ
4 maxj∈eN∥Q:,j∥+ 20∥A♭∥∞+ 3#
≤ϵ4emin(n, m)3
σ2.
Proof. LetµA(A)be the probability density function of A, which, by independence (Definition 1.1),
can be expressed asQ
i∈[n],j∈[m]µAi,j, where µAi,jis a Gaussian random variable. We first perform
a change of variables from AB,N,AB,N toAB,N,Q,b,c, d, in accordance with (5); this can be
understood through the (non-singular; Claim C.2) linear transformation A♭
B,N=T(Q♭,b,c, d).
To express the density in the new variables, we first note that the Jacobian of the change of
variables is |det(T)|= 1 (Claim C.2), and so the density on Q,b,c, dcan be expressed as
µAB,N(T(Q♭,b,c, d))µAB,N(AB,N).
Next, we perform a change of variables from AB,N,Q,b,c, dtoAB,N,Q,ex⋆,ey⋆, vaccording to
the transformations Qey⋆=c;Q⊤ex⋆=b; andv=d−⟨ex⋆,Qey⋆⟩. It is easy to see that the Jacobian
of the change of variables is
det 
∂(AB,N,Q,b,c, d)
∂(AB,N,Q,ex⋆,ey⋆, v)!=det∂(b,c, d)
∂(ex⋆,ey⋆, v)= det( Q)2.
So, the density on AB,N,Q,ex⋆,ey⋆, vreads
µAB,N(T(Q♭,Q⊤ex⋆,Qey⋆, v+⟨ex⋆,Qey⋆⟩))µAB,N(AB,N) det(Q)2.
By Lemma C.6, it suffices to upper bound
max
B,NP
AB,N,Q,x⋆,y⋆,v[γP(A)≤ϵ|AB,Ny⋆
N≥v1andA⊤
N,Bx⋆
B≤v1].
27Further, by Proposition C.5, it is in turn enough to bound PQ[γP(A)≤ϵ]for all B,N(for the
non-trivial case where eB,eN̸=∅),AB,N,ex⋆,ey⋆,vsuch that AB,Ny⋆
N≥v1andA⊤
N,Bx⋆
B≤v1,
where the induced distribution on Qis
µAB,N(T(Q♭,Q⊤ex⋆,Qey⋆, v+⟨ex⋆,Qey⋆⟩)) det( Q)2.
We will prove that for any j∈eNandQ:,eN−j,
P
Q:,j"
dist(Q:,j,span(Q:,eN−j))≤ϵ
4∥Q:,j∥+ 4|v|+ 4∥Q♭
:,eN−j∥∞+ 3#
≤ϵ4emin(n, m)2
σ2,(35)
and then apply a union bound over j∈eN. Having fixed Q:,eN−j, we can express Q:,jasq∥+tq⊥,
where ReB∋q∥∈span(Q:,eN−j)andReB∋q⊥is the unit vector orthogonal to span(Q:,eN−j). Then,
|t|=dist(Q:,j,span(Q:,eN−j))and|det(Q)|=tC(Q:,eN−j), where C(Q:,eN−j)does not depend
onQ:,j(this can be obtained by expressing the determinant using the formula for parallelepipeds).
By symmetry, we can prove (35) by bounding the probability that tis at most ϵgiven that tis at least
0. We can thus focus on proving
max
q∥∈span(Q:,fN−j)P
t[t≤ϵ|t≥0]≤ϵ4emin(n, m)2(4∥q∥∥∞+ 4|v|+ 4∥Q♭
:,eN−j∥∞+ 3)
σ2,(36)
and then (35) follows from the fact that ∥Q:,j∥ ≥ ∥ q∥∥. Now, the induced distribution on tis
proportional to
ρ(t):=t2Y
(i,j)∈B×NµAi,j(⟨Ti,j,ri,j(t)⟩)
forri,j(t)defined as
(q∥+tq⊥,Q♭
:,eN−j,Q⊤
eN−j,:ex⋆,⟨ex⋆,q∥+tq⊥⟩,Q:,eN−jey⋆
eN−j+ey⋆
j(q∥+tq⊥),
v+⟨ex⋆,Q:,eN−jey⋆
eN−j⟩+ey⋆
j⟨ex⋆,q∥+tq⊥⟩).
We now want to apply Lemma 3.7. To that end, we have
|⟨Ti,j,ri,j(t)−ri,j(t′)⟩|2≤ ∥Ti,j∥2∥ri,j(t)−ri,j(t′)∥2
≤4(t−t′)2∥(q⊥,⟨ex⋆,q⊥⟩,ey⋆
jq⊥,ey⋆
j⟨ex⋆,q⊥⟩)∥2(37)
≤16(t−t′)2, (38)
where (37) follows from ∥Ti,j∥2≤2(Claim C.2), and (38) follows from the fact that
∥q⊥∥,∥ex⋆∥,∥ey⋆∥ ≤1. Moreover, again by Claim C.2,
|⟨Ti,j,ri,j(t)⟩| ≤ ∥Ti,j∥1∥ri,j(t)∥∞≤4(∥q∥∥∞+|v|+∥Q♭
:,eN−j∥∞+t).
Let0≤t≤t′≤δ≤1
4forδ=σ2
4|B||N|(4∥q∥∥+4|v|+4∥Q♭
:,fN−j∥∞+3). Lemma C.8 then implies that
µAi,j(⟨Ti,j,ri,j(t′)⟩)
µAi,j(⟨Ti,j,ri,j(t)⟩)≥e−1
|B||N|.
Thus,
ρ(t′)
ρ(t)≥t′
t2Y
(i,j)∈B×NµAi,j(⟨Ti,j,ri,j(t′)⟩)
µAi,j(⟨Ti,j,ri,j(t)⟩)≥e−1.
We conclude that (36) can be obtained from Lemma 3.7, and the theorem follows.
Finally, we bound the probability that αP(A)(Item 1) is close to 0;αD(A)can be bounded in a
similar fashion.
28Proposition 3.10. LetαP(A)be defined as in Item 1. For any ϵ≥0,
P
A
αP(A)≤ϵ
25(∥A♭∥∞+ 1)2
≤ϵ8e2mnmin(n, m)
σ2.
Proof. By Lemma C.6, it suffices to bound
max
B,NP
AB,N,Q,x⋆,y⋆,v[αP(A)≤ϵ|AB,Ny⋆
N≥v1andA⊤
N,Bx⋆
B≤v1],
where we recall that the induced probability density function on AB,N,Q,x⋆,y⋆,vreads
µAB,N(T(Q♭,Q⊤ex⋆,Qey⋆, v+⟨ex⋆,Qey⋆⟩))µAB,N(AB,N)µAB,N(AB,N)µAB,N(AB,N) det(Q)2.
We consider the non-trivial case where eB,eN̸=∅. We will perform a further change of variables.
Namely, let a=AN,ifori∈B\eB. We map AB,NtoAeB,N:=AeB,N−1a⊤,a, so that
A⊤
N,Bx⋆
B≤v1can be equivalently expressed as A⊤
N,eBex⋆≤v1−a. The induced density function
is now proportional to
µAB,N(T(Q♭,Q⊤ex⋆,Qey⋆, v+⟨ex⋆,Qey⋆⟩))µa(a)µAeB,N(AeB,N+1a⊤)ν(·),
where ν(·)does not depend on ex⋆anda. By Proposition C.5, it is enough to show that for any
B, N,AeB,N,AB,N,AB,N,Q,y⋆, vsatisfying AB,Ny⋆≥v1,
P
ex⋆,a"
αP≤ϵ
max((∥Q♭∥∞+ 1)2,(1 +∥A♭
eB,N∥∞)(5∥A♭
eB,N∥∞+|v|+ 4))|A⊤
N,eBex⋆≤v1−a#
≤ϵ8e2mnmin(n, m)
σ2,
where the induced distribution on ex⋆andais proportional to
µAB,N(T(Q♭,Q⊤ex⋆,Qey⋆, v+⟨ex⋆,Qey⋆⟩))µa(a)µAeB,N(AeB,N+1a⊤). (39)
We see that ex⋆is independent of aand{aj}j∈Nare pairwise independent. Thus, conditioning on
the event A⊤
N,eBex⋆≤v1−a, the induced distribution on ex⋆is proportional to
µAB,N(T(Q♭,Q⊤ex⋆,Qey⋆, v+⟨ex⋆,Qey⋆⟩))Y
j∈NPaj[⟨AeB,j,ex⋆⟩ ≤v−aj].
We can proceed by showing that for any fixed i∈eBandex⋆
eB−i,
P
ex⋆
i"
ex⋆
i≤ϵ
max((∥Q♭∥∞+ 1)2,(1 +∥A♭
eB,N∥∞)(5∥A♭
eB,N∥∞+|v|+ 4))|A⊤
N,eBex⋆≤v1−a#
≤ϵ8e2mmin(n, m)
σ2,
and then applying the union bound over all i∈eB. Having fixed ex⋆
eB−i, the induced density on ex⋆
i,
sayρ(t), is proportional to ρ1(t)·ρ2(t), where
ρ1(t):=µAB,N(T(Q♭,Q⊤
:,eB−iex⋆
eB−i+tQ⊤
:,i,Qey⋆, v+⟨ex⋆
eB−i,QeB−i,:ey⋆⟩+t⟨Qi,:,ey⋆⟩))
and
ρ2(t):=Y
j∈NPaj[⟨Aj,eB−i,ex⋆
eB−i⟩+Ai,jt≤v−aj].
We will first apply Lemma 3.7 to bound ρ1(t′)/ρ1(t)for0≤t≤t′≤δ≤1and a sufficiently small
δ. We define
ri,j(t):= (Q♭,Q⊤
:,eB−iex⋆
eB−i+tQ⊤
:,i,Qey⋆, v+⟨ex⋆
eB−i,QeB−i,:ey⋆⟩+t⟨Qi,:,ey⋆⟩),
29so that ρ1(t) =Q
(i,j)∈B×NµAi,j(⟨Ti,j,ri,j(t)⟩). Then, we have
|⟨Ti,j,ri,j(t)−ri,j(t′)⟩| ≤4|t−t′|∥Q♭∥∞,
where we used Claim C.2. Further,
|⟨Ti,j,ri,j(t)⟩| ≤(t+ 1)∥Q♭∥∞,
and so Lemma C.8 implies that for δ≤1
4∥Q♭∥∞,
µAi,j(⟨Ti,j,ri,j(t′)⟩)
µAi,j(⟨Ti,j,ri,j(t)⟩)≥e−8δ∥Q♭∥∞(∥Q♭∥∞+1)
σ2 .
As a result, for δ≤σ2
8|B||N|∥Q♭∥∞(∥Q♭∥∞+1),
ρ1(t′)
ρ1(t)=Y
(i,j)∈B×NµAi,j(⟨Ti,j,ri,j(t′)⟩)
µAi,j(⟨Ti,j,ri,j(t)⟩)≥e−1.
Next, we focus on lower bounding ρ2(t′)/ρ2(t). From (39), it is not hard to see that ajis a Gaussian
random variable with expectation |E[aj]| ≤1 +∥A♭
eB,N∥∞and variance V[aj]≥σ2
min(n,m). Also,
ρ2(t′)
ρ2(t)=Y
j∈NPaj[⟨AeB−i,j,ex⋆
eB−i⟩+Ai,jt′≤v−aj]
Paj[⟨AeB−i,j,ex⋆
eB−i⟩+Ai,jt≤v−aj]
≥Y
j∈NPaj[⟨AeB−i,j,ex⋆
eB−i⟩+Ai,jt′≤v−aj| ⟨AeB−i,j,ex⋆
eB−i⟩+Ai,jt≤v−aj].
By Lemma C.7 (for τ= (2∥A♭
eB,N∥∞+|v|+ 1)/(1 +∥A♭
eB,N∥∞)),
Paj[⟨AeB−i,j,ex⋆
eB−i⟩+Ai,jt′≤v−aj| ⟨AeB−i,j,ex⋆
eB−i⟩+Ai,jt≤v−aj]
≥1−δmin(n, m)∥A♭
eB,N∥∞(2∥A♭
eB,N∥∞+|v|+ 1)
σ2eδmin(n,m)∥A♭
eB,N∥∞(5∥A♭
eB,N∥∞+|v|+4)
σ2 .
Thus, for δ≤1
2emσ2
min(n,m)∥A♭
eB,N∥∞(5∥A♭
eB,N∥∞+|v|+4),
Paj[⟨AeB−i,j,ex⋆
eB−i⟩+Ai,jt′≤v−aj| ⟨AeB−i,j,ex⋆
eB−i⟩+Ai,jt≤v−aj]≥1−1
2m,
which in turn implies that
ρ2(t′)
ρ2(t)≥
1−1
2mN
≥e−1.
We conclude thatρ(t′)
ρ(t)≥e−2, and the proof follows from Lemma 3.7 by lower bounding the value
ofδ.
Armed with Propositions 3.8 to 3.10, Theorem 1.4 can be obtained from Theorem 3.6, in conjunction
with a union bound and the fact that ∥A♭∥∞≤poly(n, m)with high probability (by Gaussian
concentration).
C.3 Proof of Theorem 1.2
Having established Theorem 1.4, here we explain how existing results imply Theorem 1.2. We
first focus on OGDA . We also take the opportunity to explain in more detail how Wei et al. [2021]
established Definition 1.3, which was sketched earlier in Section 3.1. Our treatment of the rest of the
algorithms will be more brief.
30Metric subregularity A central ingredient in the approach of Wei et al. [2021] is what they refer to
as saddle-point metric subregularity , stated below as Definition C.9. For the sake of generality, we
give the definition for a general objective function f:X × Y ∋ (x,y)7→f(x,y), assumed to be
continuously differentiable; (1)corresponds to the bilinear case f(x,y) =⟨x,Ay⟩. We use again the
notation F(z):= (∇xf(x,y),−∇yf(x,y)), where Rn+m∋z:= (x,y). We also let L∈R>0be
a Lipschitz continuity parameter for Fwith respect to ∥ · ∥, so that ∥F(z)−F(z′)∥ ≤L∥z−z′∥;
in the context of (1), one can always take L:=∥A∥.
Definition C.9 (Metric subregularity for saddle-point problems [Wei et al., 2021]) .A saddle-point
problem satisfies metric subregularity if there exists a problem-dependent parameter κ′∈R>0such
that for any z∈ Z andz⋆:= ΠZ⋆(z),
sup
z′∈Z⟨F(z),z−z′⟩
∥z−z′∥≥κ′∥z−z⋆∥. (40)
The nomenclature of Definition C.9 can be justified by the fact that (40) is equivalent to a common type
of metric subregularity [Wei et al., 2021, Appendix F]; for more background, we refer to Dontchev
and Rockafellar [2009]. We further remark that Wei et al. [2021] introduced (40) in a more general
form by allowing an exponent β∈R≥0in the right-hand side, but that additional flexibility is not
relevant for our purposes.6
Now, there an obvious connection between Definition 1.3 and Definition C.9 in bilinear problems
with bounded domain; namely, we have
sup
z′∈Z⟨F(z),z−z′⟩
∥z−z′∥≥1
2Φ(z),
where we used the fact that ⟨F(z),z) = 0 and∥z−z′∥ ≤DZ= 2. So, Definition 1.3 with respect
to parameter κimplies Definition C.9 with parameter κ′:=κ/2.
Linear convergence of OGDA Under metric subregularity, in the sense of Definition C.9, Wei et al.
[2021] were able to establish that OGDA converges to the set Z⋆at a linear rate:
Theorem C.10 (Wei et al., 2021) .Consider a saddle-point problem (1)satisfying metric subregularity
with respect to some κ′∈R>0. For any η≤1
8L, the iterates (z(τ))1≤τ≤tofOGDA satisfy
dist(z(t),Z⋆)≤8
1 +16η2(κ′)2
81−t/2
dist(bz(1),Z⋆). (41)
As a result, Theorem C.10 implies that OGDA guarantees dist(z(t),Z⋆)≤ϵso long as
t≥2
log 8DZ
ϵ
log
1 +(κ′)2
324∥A∥2
. (42)
In conjunction with Theorem 3.6 and Propositions 3.8 to 3.10, this immediately implies that OGDA
has a polynomial smoothed complexity with high probability, as claimed earlier in Theorem 1.2.
Before we proceed, it is instructive to explain how Wei et al. [2021] treated the error bound in bilinear
problems where XandYare polyhedral sets. As we explained earlier, it is enough to show that for
anyx∈ X andy∈ Y,
max
y∈Yx⊤Ay−v≥κ∥x−ΠX⋆(x)∥,
v−min
x∈Xx⊤Ay≥κ∥y−ΠY⋆(y)∥.
We focus on the first inequality, which is with respect to Player x. We let X:={x∈Rn:c⊤
ix≤
bi∀i∈[ℓx]}, where ℓxdenotes the number of vertices of X. We also let oj:=Ayj, where yj
denotes the jth vertex of Y; for simplicity, we will denote by ky∈Nthe number of vertices of Y.
We consider a fixed x∈ X \ X⋆andx⋆= ΠX⋆(x).
6Wei et al. [2021] impose (40) only for points z∈ Z \ Z⋆, which is easily seen to be equivalent.
31It is easy to see that the set of optimal strategies for Player x,X⋆:={x∈ X: max y∈Y⟨x,Ay⟩ ≤v},
can be expressed as
X⋆:=
x∈Rn:c⊤
ix≤bi,o⊤
jx≤v∀(i, j)∈[ℓx]×[ky]	
.
Indeed, any point y∈ Y is a convex combination of the vertices of Y, and the converse direction is
also obvious. A feasibility constraint i∈[ℓx]is said to be tight ifc⊤
ix⋆=bi; similarly, an optimality
constraint j∈[ky]is tight if o⊤
jx⋆=v. We let Lx=Lx(x⋆)⊆[ℓx]be the set of tight feasibility
constraints and Ky=Ky(x⋆)⊆[ky]be the set of tight optimality constraints. We can assume
without any loss that Lx, Ky̸=∅. It is well-known ( e.g., [Rockafellar, 2015]) that the normal cone
ofX⋆atx⋆with respect to X⋆can be expressed as
Nx⋆:=

X
i∈Lxpici+X
j∈Kyqjoj∀(p,q)∈RLx
≥0×RKy
≥0

.
Wei et al. [2021] also define Mx⋆⊆Nx⋆as
Nx⋆∩
c⊤
ix≤0∀i∈Lx	
.
Now, the main parameter of interest that relates to Definition 1.3 in the analysis of Wei et al. [2021]
stems from the following quantity.
Definition C.11. We let C∈R>0be defined as the infimum over (0,∞)so that


X
i∈Lxpici+X
j∈Kyqjoj,0≤pi, qj≤C

⊇Mx⋆∩ B∞, (43)
where B∞⊆Rnis the set of points with ℓ∞norm upper bounded by 1.
By definition of Mx⋆, it is evident that there always exists a finite problem-dependent parameter
C∈R>0such that Definition C.11 is satisfied. It is then not hard to show that
max
y∈Yx⊤Ay−v≥1
C|Ky|∥x−ΠX(x⋆)∥.
Assuming that the number of vertices is polynomial in the dimensions,7this shows that Definition C.11
essentially captures the complexity of satisfying Definition 1.3. As we explained earlier in Section 3.1,
the constraint matrix of the linear program induced by Definition C.11 depends both on the payoff
matrix Aas well as the set of constraints. It is thus unclear how to use existing results in the model of
smoothed complexity [Dunagan et al., 2011] to bound C. The second and more important challenge
revolves around the fact that Definition C.11 depends solely on the tight constraints of the optimal
solution, which in turn depends on the randomness of A. Under our characterization, the latter
challenge was addressed earlier in Section 3.3.
Continuing for OMWU , we again rely on the analysis of Wei et al. [2021], which relates the rate of
convergence of OMWU to three quantities. The first one [Wei et al., 2021, Definition 3] is similar
to Definition C.9, but with the difference that the maximization is now constrained to be over points
whose support is a subset of the support of the equilibrium; namely,
κx:= min
x∈X\{ x⋆}max
y∈V⋆(Y)⟨x−x⋆,Ay⟩
∥x−x⋆∥1, (44)
where V⋆(Y):={y∈∆m:supp(y)⊆supp(y⋆)}. A symmetric definition is to be considered
with respect to Player y. To connect this to (8), we note that, when y∈ V⋆(Y),⟨x⋆,Ay⟩=v. We
are thus left to lower bound max y⟨x,Ay⟩ −vin terms of ∥x−x⋆∥1, but under the constraint that
y∈ V⋆(Y). An inspection of our proof of Theorem 3.6 (and in particular the proof of (8)) reveals that
its conclusion holds even when the maximization is subject to the above constraint, and so our analysis
7In fact, by virtue of Carathéodory’s theorem, one can refine Definition C.11 so that this holds even when
the number of vertices is exponential in the dimensions. Namely, a point v∈Mx⋆∩ B∞can be written as the
conical combination of at most nof the vectors describing the cone in (43), thereby maintaining feasibility. This
observation can be used to refine the (worst-case) analysis of Wei et al. [2021] to, for example, extensive-form
games wherein the number of vertices is typically exponential in the dimensions.
32immediately lower bounds (44) as well. The second quantity introduced by Wei et al. [2021, Definition
2] corresponds exactly to Item 2, which was bounded in Proposition 3.8. The third quantity [Wei et al.,
2021, Definition 4] is where the exponential overhead is introduced. Namely, the iteration complexity
ofOMWU in their analysis depends on exp 
min(αP(A), αD(A))−1
, where we recall the definition
in Item 1.8Unfortunately, for any game , it holds that αP(A)≤1/nandαD(A)≤1/m, and so
even if the geometry of the problem is favorable, the obtained bound is exponential. (The reason the
above quantity is crucial in their analysis is because it lower bounds the probability of playing any
action through the trajectory of OMWU .) Nevertheless, using Proposition 3.10, our analysis provides
instead a bound of exp( poly(n, m, 1/σ))with high probability, which is still a major improvement
over the worst-case bound of Wei et al. [2021], which can be doubly exponential in the number of
bitsLdescribing the game—one can easily make sure that αP(A)≈1/2L(Proposition 3.1).
Next, for EGDA , Tseng [1995] established linear convergence under the error bound
dist(z,z⋆)≤τ∥z−ΠZ(z−ηF(z))∥
for some τ >0and a suitable η >0[Tseng, 1995, Corollary 3.3]. It is easy to make the following
connection.
Lemma C.12. It holds that Φ(z)≤2
η∥z−ΠZ(z−ηF(z))∥.
Proof. Indeed, by the first-order optimality condition for the optimization problem associated with
z′:= ΠZ(z−ηF(z)) = arg min
z′∈Z
∥z′−(z−ηF(z))∥2:=h(z′)	
,
we get ⟨bz−z′,∇h(z′)⟩ ≥0for any bz∈ Z, or equivalently, minbz∈Z⟨bz−z′,z′−z+ηF(z)⟩ ≥0.
Observing that minbz∈Z⟨bz, F(z)⟩=−Φ(z)and bounding
⟨z−z′,bz−z′⟩ ≥ −∥ z−z′∥∥bz−z′∥ ≥ − DZ∥z−z′∥=−2∥z−ΠZ(z−ηF(z))∥
leads to the claim.
It can thus be shown that Definition 1.3 is again sufficient to dictate the rate of convergence of EGDA .
Finally, for IterSmooth , Gilpin et al. [2012] introduced a “condition measure” of the payoff
matrix A, which in fact corresponds precisely to Definition 1.3. Thus, Theorem 1.2 with respect
toIterSmooth follows readily from [Gilpin et al., 2012, Theorem 2].
C.4 Proof of Theorem 4.2
Finally, we conclude with the proof of Theorem 4.2, which is restated below.
Theorem 4.2. Anyδ-support-stable game (per Definition 4.1) satisfies the error bound for any
sufficiently small modulus
κ≥poly1
n,1
m, δ
.
Proof of Theorem 4.2. We treat each parameter separately.
•Let us start from βP(A)(Item 2). We let j′∈arg minj∈N(v− ⟨x⋆
B,AB,j⟩), where we
assume that N̸=∅. We consider a perturbed matrix A′such that
A′
i,j=Ai,j−βP(A)ifi∈B, j=j′,
Ai,j otherwise .
8More specifically, the proof of Wei et al. [2021, Theorem 3] upper bounds the Kullback-Leibler
divergence KL(z(t),z⋆)by a quantity that is at least as large as
1 +15η2C2
32−t
, where C2≤
exp 
min(αP(A), αD(A))−1. Thus, to guarantee KL(z(t),z⋆)≤ϵusing the analysis of Wei et al.
[2021] one needs at least log(1/ϵ)/log
1 +15η2C2
32
iterations. When C2≪1, this grows with 1/C2≥
exp (min( αP(A), αD(A))).
33Then, the game described by A′cannot be non-degenerate with the same support as A.
Indeed, in the contrary case it would follow that the (unique) equilibrium (x⋆
B,y⋆
N)remains
the same since A′
B,N=AB,N. But then, v−⟨x⋆
B,A′
B,j′⟩=v−⟨x⋆
B,AB,j′⟩−βP(A) = 0 ,
by definition of j′andβP(A), which is a contradiction. Further, ∥A−A′∥=βP(A). In
turn, this implies that δ≤βP(A). Similar reasoning yields that δ≤βD(A).
•Continuing for γP(A)(Item 3), we assume that eB,eN̸=∅. We let UΣV⊤be a
singular value decomposition (SVD) of Q. Then, a perturbation to Qof the form
Udiag(0,0, . . . , σ min(Q))V⊤leads to a singular matrix Q′, which cannot be the case
if the perturbed game is non-degenerate with the same support. This perturbation can be cast
in terms of A′
B,N through transformation Tin(5). This lower bounds σmin(Q)in terms of
δ, and Proposition C.4 can in turn lower bound γP(A)in terms of σmin(Q).
•Finally, we treat αP(A)(Item 1). The non-trivial case is again when eB,eN̸=∅. Let
i′∈arg min i∈B(x⋆
i). Ifi′∈eB, we define
ReB∋ex′
i=0 ifi=i′,
x⋆
iotherwise .
We know that Q⊤ex⋆=b. We then consider the perturbed vector b′:=Q⊤ex′. If the
perturbed game was non-degenerate with the same support, it would follow that (ex′,·)is
the unique equilibrium, which is a contradiction since exi′= 0. Further, the norm of the
perturbation ∥b−b′∥is upper bounded in terms of αP(A), which can be again expressed
in terms of AB,N through transformation (5). Similarly, if i′/∈eB, we define
ReB∋ex′
i=x⋆
i+αP(A)
|eB|,
and we consider the perturbed vector b′:=Q⊤ex′. If the perturbed game was non-degenerate
with the same support, it would follow that (ex′,·)is the unique equilibrium, which is a con-
tradiction sinceP
i∈eBex′
i=P
i∈eBx⋆
i+αD(A) = 1 . The norm of the perturbation is again
upper bounded in terms of αP(A). Overall, we have shown that δ≤αP(A)poly(n, m).
Similar reasoning applies with respect to αD(A). This completes the proof.
34NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: All claims made in the abstract and introduction are proven in Appendices C.1
to C.4.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: All limitations and assumptions are stated in Section 1.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
35Justification: The full set of assumptions and proofs are given in Appendices C.1 to C.4.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
36Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
37•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The contribution of the paper is theoretical, and conforms in every respect with
the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The contribution of the paper is theoretical, and we do not foresee any societal
impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
38•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
39•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
40