Prior-itizing Privacy: A Bayesian Approach to Setting
the Privacy Budget in Differential Privacy
Zeki Kazan
Department of Statistical Science
Duke University
Durham, NC 27708
zekican.kazan@duke.eduJerome P. Reiter
Department of Statistical Science
Duke University
Durham, NC 27708
jreiter@duke.edu
Abstract
When releasing outputs from confidential data, agencies need to balance the ana-
lytical usefulness of the released data with the obligation to protect data subjects’
confidentiality. For releases satisfying differential privacy, this balance is reflected
by the privacy budget, ε. We provide a framework for setting εbased on its relation-
ship with Bayesian posterior probabilities of disclosure. The agency responsible for
the data release decides how much posterior risk it is willing to accept at various
levels of prior risk, which implies a unique ε. Agencies can evaluate different risk
profiles to determine one that leads to an acceptable trade-off in risk and utility.
1 Introduction
Differential privacy (DP) [ 11] is a gold standard definition of what it means for data curators,
henceforth referred to as agencies, to protect individuals’ confidentiality when releasing sensitive data.
The confidentiality guarantee of DP is determined principally by a parameter typically referred to as
the privacy budget ε. Smaller values of εgenerally imply greater confidentiality protection at the cost
of injecting more noise into the released data. Thus, agencies must choose εto balance confidentiality
protection with analytical usefulness. This balancing act has resulted in a wide range of values of εin
practice. For example, early advice in the field recommends εof “0.01, 0.1, or in some cases, log(2)
orlog(3) ” [10], whereas recent large-scale implementations use ε= 8.6in OnTheMap [ 31],ε= 14
in Apple’s use of local DP for iOS 10.1.1 [ 44], and an equivalent of ε= 17.14in the 2020 decennial
census redistricting data release [1].
Some decision makers may find interpreting and selecting εdifficult [ 20,24,29,36,43]. For example,
a recent study of practitioners using DP Creator [ 43] found that users wished for more explanation
about how to select privacy parameters and better understanding of the effects of this choice. One
potential path for providing guidance is to convert the task of selecting εto setting bounds on the
allowable probabilities of adversaries learning sensitive information from the released data, as in the
classical disclosure limitation literature [ 8,14,41]. There is precedent for this approach: prior work in
the literature suggests that “privacy semantics in terms of Bayesian inference can shed more light on a
privacy definition than privacy semantics in terms of indistinguishable pairs of neighboring databases”
[28] and prevailing advice for setting δin(ε, δ)-DP originates from such Bayesian semantics [25].
We propose that agencies utilize relationships between DP and Bayesian semantics [ 2,11,25,26,27,
28,29,33] to select values of εthat accord with their desired confidentiality guarantees. The basic
idea is as follows. First, the agency constructs a function that summarizes the maximum posterior
probability of disclosure permitted for any prior probability of disclosure. For example, for a prior risk
of 0.001, the agency may be comfortable with a ten-fold (or more) increase in the ratio of posterior
risk to prior risk, whereas for a prior risk of 0.4, the agency may require the increase not exceed, say,
1.2. Second, for each prior risk value, the agency converts the posterior-to-prior ratio into the largest
38th Conference on Neural Information Processing Systems (NeurIPS 2024).εthat still ensures the ratio is satisfied. Third, the agency selects the smallest εamong these values,
using that value for the data release. Importantly, the agency does not use the confidential data in
these computations—they are theoretical and data free—so that they do not use up part of the overall
privacy budget. Our main contributions include:
•We propose a framework for selecting εunder certain conditions (see Section 3) that applies
to any DP mechanism, does not use additional privacy budget, and can account for disclosure
risk from both an individual’s inclusion and the sensitivity of values in the data.
•We enable agencies to tune the choice of εto achieve their desired posterior-to-prior risk
profile. This can avoid setting εunnecessarily small if, for example, the agency tolerates
larger posterior-to-prior ratios for certain prior risks. In turn, this can help agencies better
manage trade-offs in disclosure risk and data utility.
•We give theoretic justification for the framework and derive closed-form solutions for the ε
implied by several risk profiles. For more complex risk profiles, we also provide a general
form for εas a minimization problem.
To streamline the discussion, we focus on the release of discrete-valued statistics computed on
discrete-valued data. Extension to continuous-valued statistics and data can be accomplished by
replacing sums with integrals and PMFs with PDFs throughout the theorem statements and proofs.
2 Background and Motivation
We first describe some aspects of DP and Bayesian probabilities of disclosure relevant for our
approach. We summarize all notation and definitions in Tables 2 and 3 in Appendix A.1.
2.1 Differential Privacy
LetPrepresent a population of individuals. The agency has a subset of P, which we call Y,
comprising nindividuals measured on dvariables. For any individual i, letYibe the length- dvector
of values corresponding to individual i, and let Ii= 1when individual iis inYandIi= 0otherwise.
For all isuch that Ii= 1, letY−ibe the (n−1)×dmatrix of values for the n−1individuals in Y
excluding individual i.1The agency wishes to release some function of the data, T(Y). We assume
YandT(Y)each have discrete support but may be many-dimensional. The agency turns to DP and
will release T∗(Y), a noisy version of T(Y)under ε-DP.
Our work focuses on unbounded DP as defined in [ 9], which involves the removal or addition of one
individual’s information.2If a function T∗(Y)with discrete support satisfies unbounded ε-DP, then
for all i, allyin the support of Yi, ally−iin the support of Y−i, and all t∗in the support of T∗(Y),
e−ε≤P[T∗(Y) =t∗|Yi=y, Ii= 1,Y−i=y−i]
P[T∗(Y−i) =t∗|Ii= 0,Y−i=y−i]≤eε. (1)
In settings where the statistic of interest is a count, a commonly used algorithm to satisfy DP is the
geometric mechanism [18].
Definition 1 (Geometric Mechanism) .LetT(Y)∈Zbe a count statistic and suppose we wish
to release a noisy count T∗(Y)∈Zsatisfying ε-DP . The geometric mechanism produces a count
centered at T(Y)with noise from a two-sided geometric distribution with parameter e−ε. That is,
P[T∗(Y) =t∗|T(Y) =t] =1−e−ε
1 +e−εe−ε|t∗−t|, t∗∈Z. (2)
Under the geometric mechanism, the variance of T∗(Y)is2e−ε/(1−e−ε)2. The variance increases
asεdecreases, reflecting the potential loss in data usefulness when setting εto a small value.
Of course, data usefulness is only one side of the coin. Agencies also need to assess the implications
of the choice of εfor disclosure risks [ 7,42,43]. Prior works on setting εfocus on settings where (i)
1For all isuch that Ii= 0we letY−ibe an (n−1)×dmatrix of individuals not including individual i.
2This is in contrast to bounded DP [11], which involves the change of one individual’s information.
2the data have yet to be collected, and the goal is to simultaneously select εand determine how much
to compensate individuals for their loss in privacy [ 6,15,23,30], (ii) the population is already public
information, and the goal is to protect which subset of individuals is included in a release [ 29,35], or
(iii) data holders can utilize representative test data or the confidential data set itself [ 4,17,20,35].
We focus on the common setting where data already have been collected, the population they are
drawn from is not public information, and no data are available for tuning ε.
2.2 Bayesian Measures of Disclosure Risk
Consider an adversary who desires to learn about some particular individual iinYusing the release of
T∗(Y). We suppose that the adversary has a model, M, for making predictions about the components
ofYithat they do not already know.3For example, the adversary could know some demographic
information about individual ibut not some other sensitive variable. The adversary might predict
this sensitive variable from the demographic information using a model estimated with proprietary
information or data from sources like administrative records. We assume that the release mechanism
forT∗(Y)is known to the adversary, that the DP release mechanism does not depend on M, and
that, under M, the observations are independent but not necessarily identically distributed. These
conditions are formalized in Section 3. For our ultimate purpose, i.e., helping agencies set ε, the
exact form of the adversary’s Mis immaterial. In fact, as we shall discuss, we are not concerned
whether the adversary’s predictions from Mare highly accurate or completely awful.
On a technical note, we make the distinction that the agency views YandIias fixed quantities,
since it knows which rows are in the collected data and what values are associated to each row. The
adversary, however, views YandIias random variables, and thus probabilistic statements about
these quantities are well defined from the adversary’s perspective. Notationally, we signify that a
probabilistic statement is from the adversary’s perspective via the subscript M.
LetSbe the subset of the support of Yithat the agency considers a privacy violation. For example, if
d= 1andYis income data, then Smay be the set of possible incomes within 5,000or within 5%of
the true income for individual i. Alternatively, if d= 1andYis binary, then Sis a subset of {0,1}.
The selection of Smust not depend on P, as this might constitute a privacy violation.
The agency may be concerned about the risk that the adversary determines individual iis inYor the
risk that the adversary makes a disclosure for individual i; that is, Ii= 1andYi∈ S, respectively.
Assuming that the adversary’s model puts nonzero probability mass on these events, we can express
their relevant prior probabilities as follows.
PM[Ii= 1] = pi, P M[Yi∈ S | Ii= 1] = qi. (3)
For fixed piandqi, we can measure the risk of disclosure for individual iin a number of ways.
Drawing from [ 33], one measure is the relative disclosure risk, ri(pi, qi, t∗). Writing the noisy
statistic as T∗and suppressing the dependence on YorY−i, this is defined as follows.
Definition 2 (Relative Disclosure Risk) .For fixed data Y, individual i, adversary’s model M, and
released T∗=t∗, the relative disclosure risk is the posterior-to-prior risk ratio,
ri(pi, qi, t∗) =PM[Yi∈ S, Ii= 1|T∗=t∗]
PM[Yi∈ S, Ii= 1]. (4)
The relative risk can be decomposed into the posterior-to-prior ratio from inclusion ( Ii) and the
posterior-to-prior ratio from the values ( Yi). We have
ri(pi, qi, t∗) =PM[Yi∈ S | Ii= 1, T∗=t∗]
PM[Yi∈ S | Ii= 1]·PM[Ii= 1|T∗=t∗]
PM[Ii= 1]. (5)
The relative risk, however, does not tell the full story. As discussed in [ 22], the data holder also may
care about absolute disclosure risks, ai(pi, qi, t∗).
Definition 3 (Absolute Disclosure Risk) .For fixed data Y, individual i, adversary’s model M, and
released T∗=t∗, the absolute disclosure risk is the posterior probability,
ai(pi, qi, t∗) =PM[Yi∈ S, Ii= 1|T∗=t∗]. (6)
Since ri(pi, qi, t∗) =ai(pi, qi, t∗)/(piqi), we can convert between these risk measures.
3To be technically precise, we suppose the adversary assigns zero probability to predictions of Yiwith values
of components that they know to be incorrect.
32.3 Risk Profiles
The quantities from Section 2.2 can inform the choice of ε. For example, DP implies that
ri(pi, qi, t∗)≤e2ε(7)
for all (pi, qi, t∗).4The inequality in (7) implies a naive strategy for setting ε: select a desired bound
on the relative risk, r∗, and set ε= log( r∗)/2. Practically, however, this strategy suffers from two
drawbacks that could result in a smaller recommended εthan necessary. First, for any particular pi
andqi, the bound in (7) need not be tight. In fact, this bound is actually quite loose across a wide
range of values. Second, this strategy does not account for agencies willing to tolerate different
relative risks for different prior probabilities. For example, if piqi= 0.25, an agency may wish to
limit the adversary’s posterior to ai(pi, qi, t∗)≤2×0.25 = 0 .5, but for piqi= 10−6, the same
agency may find a limit of ai(pi, qi, t∗)≤2×10−6unnecessarily restrictive.
Rather than restricting to a constant bound, the agency can consider tolerable relative risks as a
function of a hypothetical adversary’s prior probabilities. We refer to this function as the agency’s
risk profile, denoted r∗(pi, qi). Thus, the agency establishes a r∗(pi, qi)so that, for all pi,qi, andt∗,
ri(pi, qi, t∗)≤r∗(pi, qi). (8)
As the following results show, the requirement in (8) translates to a maximum value of ε.
3 Theoretical Results
In this section, we describe our main theoretical results. To devote more space to examples and
applications, we describe supporting results in Appendix B.1 and provide all proofs in Appendix E.
We begin by formalizing the assumptions on the release mechanism for T∗and the adversary’s model,
M. We emphasize that P[T∗(Y) =t∗|Yi=y,Y−i=y−i, Ii= 1] andP[T∗(Y−i) =t∗|
Y−i=y−i, Ii= 0] are probabilities under the actual DP mechanism used for the release of T∗(Y)
when iis and is not included, respectively. We use these probabilities in two assumptions as follows.
Assumption 1. For all yin the support of Yi,y−iin the support of Y−i, and t∗in the support of T∗,
PM[T∗(Y) =t∗|Yi=y,Y−i=y−i, Ii= 1] = P[T∗(Y) =t∗|Yi=y,Y−i=y−i, Ii= 1]
PM[T∗(Y−i) =t∗|Y−i=y−i, Ii= 0] = P[T∗(Y−i) =t∗|Y−i=y−i, Ii= 0].
Assumption 1 implies that the mechanism for releasing T∗given Yis known to the adversary and
that the adversary uses the actual release mechanism. An adversary who uses something other than
the actual release mechanism to compute probabilities is unlikely in general to find more success than
the adversary who does. Hence, Assumption 1 assures that the agency’s computations are principled
and cover a type of “worst-case” analysis.
Assumption 2. For all yin the support of Yiandy−iin the support of Y−i,
PM[Y−i=y−i|Ii= 1, Yi=y] =PM[Y−i=y−i|Ii= 0]. (9)
Assumption 2 implies that the adversary’s beliefs about the distribution of Y−ido not change
whether individual iis included in the data or not, nor do they depend on individual i’s confidential
values. This assumption is similar to one explored in [ 28], who consider the case where “the
presence/absence/record-value of each individual is independent of the presence/absence/record-
values of other individuals,” although we do not require that (9) holds for all i. Since DP is designed
to capture the effect that a single atomic unit of data has on the output distribution, assuming this
type of independence structure ensures one atomic unit is a single data point.5We believe these two
assumptions are weaker than those in related work on choosing ε, which we discuss in Section 6.
Under Assumption 1 and Assumption 2, we can relate εto the distribution of T∗unconditional on
Y−ivia the following lemma. This result is similar, but not identical to, Theorem 6.1 in [28].
4This is proved in [ 19] under bounded DP in the case where |S|= 1. We show in Corollary 1 in the appendix
that, under our assumptions, it holds for larger sets and for unbounded DP (with εreplaced by 2ε).
5We thank a referee for highlighting this rationale for Assumption 2.
4Lemma 1. Under Assumption 1 and Assumption 2, if the release of T∗=t∗satisfies ε-DP , then for
any subset Sof the domain of Yi, we have
e−ε≤PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Ii= 0]≤eε, e−2ε≤PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Yi/∈ S, Ii= 1]≤e2ε.
(10)
We emphasize that Lemma 1 and all subsequent results require Assumption 2. Without it, gener-
alizable knowledge from the release can lead to arbitrarily large relative risk, as demonstrated by
examples in [25, 27].
For a given function r∗selected by the data holder, we can determine the εthat should be used for
the release. This is due to the following result relating the relative risk to ε.
Theorem 1. Under Assumption 1 and Assumption 2, if the release of T∗=t∗satisfies ε-DP , then
ri(pi, qi, t∗)≤1
qipi+e−2ε(1−qi)pi+e−ε(1−pi). (11)
One can solve for e−εin (11) to determine the recommended ε, which is given by Theorem 2.
Theorem 2. For any individual i, fix the prior probabilities, piandqi, and a desired bound on the
relative disclosure risk, r∗(pi, qi). Define εi(pi, qi)to be the function
εi(pi, qi) =

log
2pi(1−qi)r
(1−pi)2+4pi(1−qi)
1
r∗(pi,qi)−piqi
−(1−pi)
,if0< qi<1;
log
1−pi
1
r∗(pi,1)−pi
, ifqi= 1.(12)
Under the conditions of Theorem 1, any statistic T∗=t∗released under ε-DP with ε≤εi(pi, qi)
will satisfy ri(pi, qi, t∗)≤r∗(pi, qi).
By Theorem 2, to achieve ri(pi, qi, t∗)≤r∗(pi, qi)for all (pi, qi), the agency should set εvia the
following minimization problem:
ε= min
pi,qi∈(0,1]εi(pi, qi). (13)
Closed forms for the εresulting from a few specific risk profiles are included in the appendix.
4 Using Posterior-to-prior Risks for Setting ε
In this section, we illustrate how an agency can use the results of Section 3 to select ε. Notationally,
for a given quantity x, we use ˜xto indicate the agency has fixed xto a particular constant.
Given S, the agency must select a form for r∗(pi, qi). A default choice, equivalent to the naive
strategy using (7) discussed above, is to set the bound to a constant ˜r >1, i.e.,
r∗(pi, qi) = ˜r. (14)
As we prove in Theorem 3 in the appendix, the bound in (14) implies the agency should set ε=
log(˜r)/2. While a constant bound on relative risk is simple, agencies that tolerate different risk
profiles may be able to set εto larger values, as we now illustrate.
Consider an agency that seeks to bound the relative risks for high prior probabilities and bound the
absolute disclosure risk for low prior probabilities. For example, the agency may not want adversaries
whose prior probabilities are low to use t∗to increase those probabilities beyond 0.10. Simultaneously,
the agency may want to ensure adversaries with prior probabilities around, for example, 0.20 cannot
uset∗to triple their posterior probability. Such an agency can specify a risk profile that requires
either the relative risk be less than some ˜r >1or the absolute risk be less than some ˜a <1, as we
now illustrate via the following two examples.
The first example is a setting inspired by a case study in [13].
5Figure 1: Each column corresponds to a particular hypothetical agency. The first row presents the
agency’s risk profile and the second row presents the profile’s implied maximal allowable εat each
point on the curve. Agency 1’s risk profile is given by (15) with ˜a= 0.1,˜q= 1, and ˜r= 3, while
Agency 2’s risk profile is given by (16) with ˜a= 0.1,˜p= 0.05, and ˜r= 3. For Agency 2, r∗andεi
are very large for small qi; large values are truncated for readability.
Agency ˜r ˜a εRecommendation Noise Std. Dev. Prob. Exact
A 1.5 0 .25 0.51 2 .74 25%
B 3 0 .25 1.30 1 .02 57%
C 6 0 .25 2.04 0 .59 77%
Table 1: The εrecommended by our framework for three risk profiles of the form in (15). Also
included are the standard deviations of the noise distribution and the probabilities that the exact value
is released, i.e., the mass at zero, for geometric mechanisms that satisfy the corresponding ε-DP.
Example 1. A healthcare provider possesses a data set comprising demographic information about
individuals diagnosed with a particular form of cancer in a region of interest. They plan to release
the count of individuals diagnosed with this form of cancer in various demographic groups via the
geometric mechanism, but are concerned this release, if insufficient noise is added, could reveal
which individuals in the community have cancer. They wish to choose εappropriately.
In this example, the primary concern is with respect to inclusion in the data set. That is, for a given
individual i, the adversary’s prior probability piis the key quantity, whereas the qiis not as important
for any given S. The agency can fix some ˜q∈(0,1], for example ˜q= 1to focus on adversaries who
already know individual i’s demographic information. To simplify the analysis, we set the risk profile
to∞for adversaries with ˜q̸= 1, which indicates these adversaries are not considered. A reasonable
risk profile for this agency then might be of the form, for some ˜r >1and˜a <1,6
r∗(pi, qi) =(
maxn
˜a
pi˜q,˜ro
,ifqi= ˜q;
∞, ifqi̸= ˜q.=

˜a
pi˜q,ifqi= ˜qand0< pi<˜a
˜q˜r;
˜r, ifqi= ˜qand˜a
˜q˜r≤pi≤1;
∞, ifqi̸= ˜q.(15)
An example visualization of this risk function is presented in the first column of Figure 1. The upper
plot displays the risk profile as a function of piwhen qi= ˜q, and the lower plot displays the maximal
εfor which the relative risk bound holds for each pi. We can derive a closed form for εunder this
risk profile; see Table 4 in Appendix A.2 for a summary and Theorem 5 in Appendix B.1 for details.
Suppose the agency is generally willing to accept a maximum absolute disclosure risk of ˜a= 0.25for
adversaries with small prior probabilities. Table 1 presents the maximal εwhich satisfies the desired
risk profile for three such agencies. Agency A is risk averse, agency C is utility seeking, and agency
6The second equality in (15) assumes ˜q >˜a/˜r. If˜q≤˜a/˜r, then r∗(pi,˜q) = ˜a/(pi˜q)for all 0< p i≤1.
6B sits in between in terms of risk and utility. For adversaries with high prior probabilities, agencies
A, B, and C bound the relative disclosure risk at ˜r= 1.5,˜r= 3, and ˜r= 6, respectively. Figure 3 in
Appendix C.1 plots the forms of each agency’s risk profile. To provide intuition on the amount of
noise implied by these ε, in Table 1 we display the standard deviation of the noise distribution for
each statistic under the geometric mechanism and the probability that the geometric mechanism will
output the exact value of each statistic. The risk averse agency is recommended an εthat results in a
release with a high standard deviation and fairly low probability of releasing the exact value of the
statistic. The utility seeking agency is recommended an εthat results in a release with a fairly low
standard deviation and high probability of releasing the exact value of the statistic.
The recommendations from these risk profiles, which are tailored to the setting and agency preferences,
are higher than the recommendations from a corresponding simple risk profile of r∗(pi, qi) = ˜rfor
all prior probabilities. This simple profile yields ε≈0.20,ε≈0.55, and ε≈0.90for˜r= 1.5,
˜r= 3, and ˜r= 6, respectively, less than half the corresponding εvalues in Table 1.
We now consider a second example, inspired by Example 16 in [45], that alters Example 1.
Example 2. A survey is performed on a sample of individuals in the region of interest. 5% of the
region is surveyed, and respondents are asked whether they have this particular form of cancer
along with a series of demographic questions. The agency plans to release the counts of surveyed
individuals who have and do not have the cancer in various demographic groups via the geometric
mechanism, but is concerned this release, if insufficient noise is added, could reveal which individuals
in the community have cancer. The agency wishes to choose εappropriately.
In this example, the primary concern is with respect to the values in the data set. For ease of notation,
letYibe ad-vector of binary values, and let the first element, Y1i, be an indicator for whether
individual ihas this form of cancer. Set Sto be the subset of the support of Yifor which individual
ihas the cancer, i.e., S={y∈ {0,1}d:y1= 1}. For individual i, the adversary’s qiis the key
quantity, and their piis not of interest. The agency can fix some ˜p∈(0,1]; for example, if the
agency is most concerned about adversaries whose only prior knowledge is that individual iis in the
population, but not whether they were surveyed, they might set ˜p= 0.05. Alternatively, they might
set˜p= 1to imply an adversary that knows a priori that individual iis included in Y. A reasonable
risk profile for this agency might be of the form, for some ˜a <1and˜r >1,7
r∗(pi, qi) =(
maxn
˜a
˜pqi,˜ro
,ifpi= ˜p;
∞, ifpi̸= ˜p.=

˜a
˜pqi,ifpi= ˜pand0< qi<˜a
˜p˜r;
˜r, ifpi= ˜pand˜a
˜p˜r≤qi≤1;
∞,ifpi̸= ˜p.(16)
An example visualization of this risk function is presented in the second column of Figure 1. The
upper plot displays the risk profile as a function of qiwhen pi= ˜p, and the lower plot displays the
maximal εfor which the relative risk bound holds for each pi. The agency can select the smallest ε
on this curve for their release. We can derive a closed form the minimum; see Table 4 in Appendix
A.2 for a summary and Theorem 4 in Appendix B.1 for details. Notably, it can be shown that the ε
selected under these two profiles is bounded below by log(˜r)/2and may be much larger. We provide
further exploration and visualizations for this example in Appendix C.2.
As demonstrated by these examples, the gap between the baseline strategy of (14) and our recommen-
dation can be large. To further illustrate, suppose an agency has a simple, “point” risk profile, where
r∗(0.5,1) = r′forr′<2(and∞elsewhere). The baseline recommends ε= log ( r′)/2<0.35,
while the recommendation from Theorem 2 can be shown to have the form ε= log ( r′/(2−r′)),
which diverges as r′→2. Thus, the gap between the recommendations can be arbitrarily large. While
this likely does not represent a realistic disclosure risk profile of a real-world agency, it is instructive.
In general, the baseline’s recommendation is smaller because it is enforcing a relative risk of r′for
adversaries with small priors. If r′<2, then an adversary with prior probability piqi= 0.001is
restricted to a posterior probability at most 0.002, leading to a small εrecommendation. If adversaries
with small priors are ignored or allowed to have large relative risks, the recommendation from our
method will outperform the baseline, and possibly by a substantial amount.
A particular agency’s risk profile may not be characterized by one of the forms described above. For
example, an agency may be equally concerned about piandqi, rather than focusing on one; see
7The second equality in (16) assumes ˜p >˜a/˜r. If˜p≤˜a/˜r, then r∗(˜p, q i) = ˜a/(˜pqi)for all 0< q i≤1.
7Appendix C.3 for an example of this setting. Or, as argued in [ 32], in some settings, agencies might
be most concerned about the difference between the posterior and prior probabilities (rather than their
ratio).8In such settings, it is straightforward to write down the corresponding risk function, r∗(pi, qi),
and the optimal εcan be determined by numerically solving the minimization problem in (13).9
Regardless of the agency’s desiderata for a risk profile, we recommend that they keep the following
in mind when setting its functional form. First, for any region where r∗(pi, qi)>1/(piqi), the
risk profile generates a bound on the posterior probability that exceeds 1. Of course, the posterior
probabilities themselves cannot exceed 1; thus, in these regions, the risk profile effectively does not
bound the posterior risk. For example, an agency that sets r∗(pi,1) = 3 in the region where pi≥1/3
(as in the left column of Figure 1) implicitly is willing to accept an unbounded εfor prior probabilities
pi≥1/3. Second, when bounding the absolute disclosure risk below some ˜ain some region of
(pi, qi), the agency should require piqi<˜ain that region. When piqi= ˜a, the recommended ε= 0
since the data holder requires T∗to offer no information about Yi. This also suggests that an agency
bounding absolute disclosure risk in a region of (pi, qi)that sets ˜aclose to some value of piqiin the
region is willing to accept only small εvalues.
5 Managing the Trade-off in Privacy and Utility
Agencies can use the framework to manage trade-offs in disclosure risk and data utility. In particular,
the agency can evaluate potential impacts of the DP algorithm using data quality metrics under
different risk profiles, choosing an εthat leads to a satisfactory trade-off. We demonstrate this in the
below example, using data on infant mortality in Durham county, N.C. We note that these data may
not require the use of DP in reality, but they do allow us to illustrate the process with public data.
Example 3. Suppose that an agency in Durham county, N.C., wishes to release a differentially
private version of the number of infant deaths the county experienced in the year 2020. The agency
plans to use the geometric mechanism to release this value. Of particular interest is whether the
county infant death rate meets the U. S. Department of Health and Human Services’ Healthy People
2020 target of 6.0or fewer deaths per 1,000live births [ 21]. The agency wishes to minimize the
probability that they release a noisy count that changes whether or not the 6.0target is met; their
goal is to ensure this probability is below 10%. It is public information that there were 4,012live
births in Durham county in 2020 [38].
The primary concern in Example 3 is with respect to inclusion in the data. Thus, the agency focuses
on adversaries with qi= 1and varying pi. The agency is generally willing to incur large relative risks
ifpiis small and small relative risks if piis large. Furthermore, due to possible policy ramifications
of appearing not to meet the target rate, the agency is open to accepting greater privacy risks for
greater accuracy in the released count, within reason as determined by agency decision makers.
The agency’s privacy experts determine that they can characterize the agency’s risk profiles reasonably
well using the following general class of risk profiles.
r∗(pi, qi) =(
maxn
˜a
pi,˜ro
,ifqi= 1;
∞, ifqi̸= 1.(17)
To allow for consideration of the effects on accuracy of different specifications of (17), the agency
considers combinations of ˜r∈ {1.2,2,5}and˜a∈ {0.1,0.25,0.5}. They also examine ˜a= 0, which
corresponds to a constant risk profile r∗(pi, qi) = ˜r. The agency could consider other combinations
of parameters or risk profiles classes should these not lead to a satisfactory trade-off in risk and utility.
Figure 2 summarizes an analysis of the risk/utility trade-offs. For all ˜r, the recommended εis larger
when ˜a > 0than under the corresponding constant risk profile. Similar increases in εalso are
evident for the risk profiles of Section 4; see Appendix C. Naturally, increases in ˜rand˜adecrease the
RMSEs10of the noisy counts due to larger ε. The switching probabilities in the top panel do not use
8Requiring the difference between the posterior and prior probabilities to be bounded by some ˜b∈(0,1)is
equivalent to requiring the relative risk be bounded by (piqi+˜b)/(piqi). See Appendix B.2 for details.
9We provide Rcode at https://github.com/zekicankazan/choosing_dp_epsilon to determine the
recommended εfor a provided risk profile. This code was used to determine closed forms for εfor the additional
risk profiles described in Appendix B.2. The example in Appendix C.3 demonstrates the use of this code.
10Since T∗is unbiased for the true count, the RMSE is the standard deviation of the noise distribution.
8Figure 2: The top panel presents the probability that the differentially private algorithm switches
whether Durham county’s released rate is above or below the 6.0 target—e.g., the added noise makes
the released rate 6.5 but the actual rate is 5.5. Each color represents a different hypothetical absolute
difference between the true and target rate. The middle panel presents RMSEs of the noisy count
of infant deaths. The bottom panel presents the implied ε. Each bar corresponds to a different risk
profile of the form in (17).
the true count of infant deaths; rather, they consider hypothetical deviations of 0.25,0.5,1, and 2in
the true rate from the target rate of 6.0. In this way, the agency avoids additional privacy leakage.
The probability differs substantially for each of the four deviations, with the deviation of 0.25leading
to the largest probability of a switch. To ensure the probability is below 10% for the deviation of
0.25, the agency would need to use the most aggressive of the risk profiles, which allows absolute
disclosure risk of ˜a= 0.5or relative disclosure risk of ˜r= 5. This leads to ε≈2.20with RMSE
≈0.53. If they are willing to allow the probability to exceed 10% or to place priority on deviations
exceeding 0.25, the agency could select a less aggressive risk profile that yields a smaller ε.
In actuality, there were 6.2infant deaths per 1,000live births in Durham County in 2020 [ 37]. As the
truth is close to the 6.0target, the agency would need to use a large εto achieve their goal.
6 Relationship to Prior Work
The most similar work we are aware of is Lee and Clifton [ 29]’s “How Much is Enough? Choosing ε
for Differential Privacy,” which also uses a Bayesian approach to set ε. The authors focus on settings
where the population is public information and the adversary’s goal is to determine which subset of
individuals in the population was used for a differentially private release of a statistic. Their method
for selecting εis tailored to the setting where only disclosure of an individual’s inclusion in the data is
of concern and the values of the entire population can be used to inform the choice of ε. [29] focuses
on the setting where the Laplace mechanism is used and the population size is known; their proposal
was extended to any DP mechanism by [ 24,39] and settings where the population size is unknown by
[34]. In Appendix D.1, we compare the recommended εfrom the proposal in [ 29] and our framework
in several examples. We find that the approach of [ 29], for particular statistics and populations, can
recommend a larger εthan the one resulting from our approach. However, our framework may be
used in settings where the values of the entire population are not public and settings where the privacy
of the values in the release is of primary concern.
9Another series of related work involves using Bayesian semantics to communicate the meaning of
εto potential study participants. For an overview of recent work in this area, see [ 16], [36], and
citations therein. We highlight an example in [ 45] (corrected in [ 46]), which considers an individual
deciding whether or not to participate in a survey for which results will be released via DP with a
particular ε. The authors suggest that the individual consider a bound on a posterior-to-posterior ratio
similar to the bound in (11) to make an informed decision about whether to participate in the survey.
We describe the advice of [ 45] using our notation in Appendix D.2. While their and our bounds
have similar expressions, the goal in [ 45] differs from ours. They use the bound to characterize the
individual’s disclosure risks for a fixed ε, whereas we establish the agency’s disclosure risk profile in
order to set ε.
There are a number of other works that examine Bayesian semantics of ε-DP, which we now briefly
summarize. In their seminal work proposing DP, [ 11] showed that, under some conditions, bounded
DP is equivalent to a bound on the relative risk when all but one data point is fixed. [ 25] showed
that bounded DP implies a bound on the total variation distance between the posterior distribution
with all data points included and the posterior distribution with one data point removed. [ 28] showed
that both bounded and unbounded DP are equivalent to a bound on the posterior-to-prior odds ratio
exactly when the presence/absence/record-value of each individual is independent of that of other
individuals. Finally, [ 27] showed that both bounded and unbounded DP imply bounds on the ratio of
the posterior distribution with all data points included to the posterior with one observation replaced
with a draw from the posterior with their observation removed. These works do not discuss how these
semantic characterizations of DP can be used to select ε.
7 Commentary
In this article, we propose a framework for selecting εfor DP by establishing disclosure risk profiles.
Essentially, we provide a method for agencies to trade the problem of selecting εfor a release for
the problem of specifying their desired disclosure risk profile. This process involves focusing on
particular classes of adversaries the agency is most concerned about—represented by the set S—and
tuning εto ensure the risk from these adversaries is sufficiently low. We emphasize that, once applied,
DP will protect against all attacks with the guarantee of DP, not just the attack used to tune ε.
We primarily focus on the risk from one class of adversary attacks on one individual, assuming
all individuals are exchangeable. If agencies assign different risk profiles for different classes of
adversaries, they could repeat the analysis for each class For example, the agency might assign
different risk profiles to adversaries targeting different characteristics, e.g., they may consider whether
an individual has a disease more sensitive than their age. Similarly, if an agency does not treat
individuals in the data as exchangeable—for example, if an agency seeks to ensure lesser disclosure
risks for groups with some characteristics than for not-necessarily-disjoint groups without those
characteristics —the agency could repeat this analysis for each group. In both of these settings, the
agency has a decision problem on its hands. As with designing DP solutions in general, the agency
must prioritize some risks over others, e.g., using decision-theoretic criteria. This is an important
topic for future work and raises ethical questions regarding how assigning different risk profiles for
individuals with different characteristics could affect data equity.
One avenue for future work involves incorporating a version of this framework into differentially
private data analysis tools. Recently developed interfaces for DP data releases such as [ 24] and [ 35]
use the framework of [ 29] to guide users in setting ε. Our framework could be similarly incorporated
to provide guidance in settings that do not satisfy the assumption of [ 29] that the values of the
population are public. Another avenue for future work involves examining how an agency can set
the risk profile appropriate for a particular release. We envision agencies could do so analogously
to the elicitation of utility functions in decision theory, e.g., by considering a series of bets [ 40].
Other future extensions involve examining whether similar results follow under weaker assumptions.
This includes settings with multiple differentially private releases via existing results that relate the
relative risk to DP composition theorems (e.g., S5 of [ 26]). This also includes extension to variants
of DP—such as zero-concentrated DP [ 5] or approximate DP [ 12]—by deriving Bayesian semantics
analogous to Theorem 1 or leveraging existing results [ 25,27]. Additionally, it may be possible to
extend the results in this article from posterior-to-prior risks to the sorts of posterior-to-posterior risks
discussed in Section 6.
10Acknowledgments and Disclosure of Funding
This research was supported by NSF grant SES-2217456. We would like to thank the anonymous
reviewers for their helpful feedback as well as Salil Vadhan and Jörg Drechsler for insightful
discussion about a preliminary version of this work.
References
[1]John M Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Micah Heineck,
Christine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, et al. The
2020 census disclosure avoidance system topdown algorithm. arXiv preprint arXiv:2204.08986 ,
2022.
[2]John M Abowd and Lars Vilhuber. How protective are synthetic data? In International
Conference on Privacy in Statistical Databases , pages 239–246. Springer, 2008.
[3] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight
analyses via couplings and divergences. Advances in Neural Information Processing Systems ,
31, 2018.
[4]Mihai Budiu, Pratiksha Thaker, Parikshit Gopalan, Udi Wieder, and Matei Zaharia. Over-
look: Differentially private exploratory visualization for big data. Journal of Privacy and
Confidentiality , 12(1), 2022.
[5]Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions,
and lower bounds. In Theory of Cryptography Conference , pages 635–658. Springer, 2016.
[6]Pranav Dandekar, Nadia Fawaz, and Stratis Ioannidis. Privacy auctions for recommender
systems. ACM Transactions on Economics and Computation (TEAC) , 2(3):1–22, 2014.
[7]Jörg Drechsler. Differential privacy for government agencies—are we there yet? Journal of the
American Statistical Association , 118(541):761–773, 2023.
[8]George T Duncan and Diane Lambert. Disclosure-limited data dissemination. Journal of the
American Statistical Association , 81(393):10–18, 1986.
[9]Cynthia Dwork. Differential privacy. In International colloquium on automata, languages, and
programming , pages 1–12. Springer, 2006.
[10] Cynthia Dwork. A firm foundation for private data analysis. Communications of the ACM ,
54(1):86–95, 2011.
[11] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography Conference , pages 265–284.
Springer, 2006.
[12] Cynthia Dwork, Guy N Rothblum, and Salil Vadhan. Boosting and differential privacy. In 2010
IEEE 51st annual symposium on foundations of computer science , pages 51–60. IEEE, 2010.
[13] Amalie Dyda, Michael Purcell, Stephanie Curtis, Emma Field, Priyanka Pillai, Kieran Ricardo,
Haotian Weng, Jessica C Moore, Michael Hewett, Graham Williams, et al. Differential privacy
for public health data: An innovative tool to optimize information sharing while protecting data
confidentiality. Patterns , 2(12):100366, 2021.
[14] Stephen E Fienberg and Ashish P Sanil. A Bayesian approach to data disclosure: Optimal
intruder behavior for continuous data. Journal of Official Statistics , 13(1):75, 1997.
[15] Lisa K Fleischer and Yu-Han Lyu. Approximately optimal auctions for selling privacy when
costs are correlated with data. In Proceedings of the 13th ACM Conference on Electronic
Commerce , pages 568–585, 2012.
11[16] Daniel Franzen, Saskia Nuñez von V oigt, Peter Sörries, Florian Tschorsch, and Claudia Müller-
Birn. Am i private and if so, how many? communicating privacy guarantees of differential
privacy with risk communication formats. In Proceedings of the 2022 ACM SIGSAC Conference
on Computer and Communications Security , pages 1125–1139, 2022.
[17] Marco Gaboardi, James Honaker, Gary King, Jack Murtagh, Kobbi Nissim, Jonathan Ull-
man, and Salil Vadhan. Psi ( {\Psi}): a private data sharing interface. arXiv preprint
arXiv:1609.04340 , 2016.
[18] Arpita Ghosh, Tim Roughgarden, and Mukund Sundararajan. Universally utility-maximizing
privacy mechanisms. SIAM Journal on Computing , 41(6):1673–1693, 2012.
[19] Ruobin Gong and Xiao-Li Meng. Congenial differential privacy under mandated disclosure. In
Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference , pages 59–70,
2020.
[20] Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Yan Chen, Dan Zhang, and George
Bissias. Exploring privacy-accuracy tradeoffs using dpcomp. In Proceedings of the 2016
International Conference on Management of Data , pages 2101–2104, 2016.
[21] HHS. Healthy people 2020. Office of Disease Prevention and Health Promotion, U.S. Depart-
ment of Health and Human Services, 2020. Accessed on 22 April 2024.
[22] V Joseph Hotz, Christopher R Bollinger, Tatiana Komarova, Charles F Manski, Robert A
Moffitt, Denis Nekipelov, Aaron Sojourner, and Bruce D Spencer. Balancing data privacy and
usability in the federal statistical system. Proceedings of the National Academy of Sciences ,
119(31), 2022.
[23] Justin Hsu, Marco Gaboardi, Andreas Haeberlen, Sanjeev Khanna, Arjun Narayan, Benjamin C
Pierce, and Aaron Roth. Differential privacy: An economic method for choosing epsilon. In
2014 IEEE 27th Computer Security Foundations Symposium , pages 398–410. IEEE, 2014.
[24] Mark F St John, Grit Denker, Peeter Laud, Karsten Martiny, Alisa Pankova, and Dusko Pavlovic.
Decision support for sharing data using differential privacy. In 2021 IEEE Symposium on
Visualization for Cyber Security (VizSec) , pages 26–35. IEEE, 2021.
[25] Shiva P Kasiviswanathan and Adam Smith. On the’semantics’ of differential privacy: A
Bayesian formulation. Journal of Privacy and Confidentiality , 6(1), 2014.
[26] Zeki Kazan and Jerome Reiter. Assessing statistical disclosure risk for differentially private,
hierarchical count data, with application to the 2020 us decennial census. Statistica Sinica ,
34:to appear, 2024.
[27] Daniel Kifer, John M Abowd, Robert Ashmead, Ryan Cumings-Menon, Philip Leclerc, Ashwin
Machanavajjhala, William Sexton, and Pavel Zhuravlev. Bayesian and frequentist semantics
for common variations of differential privacy: Applications to the 2020 census. arXiv preprint
arXiv:2209.03310 , 2022.
[28] Daniel Kifer and Ashwin Machanavajjhala. Pufferfish: A framework for mathematical privacy
definitions. ACM Transactions on Database Systems (TODS) , 39(1):1–36, 2014.
[29] Jaewoo Lee and Chris Clifton. How much is enough? choosing εfor differential privacy. In
International Conference on Information Security , pages 325–340. Springer, 2011.
[30] Chao Li, Daniel Yang Li, Gerome Miklau, and Dan Suciu. A theory of pricing private data.
ACM Transactions on Database Systems (TODS) , 39(4):1–28, 2014.
[31] Ashwin Machanavajjhala, Daniel Kifer, John Abowd, Johannes Gehrke, and Lars Vilhuber.
Privacy: Theory meets practice on the map. In 2008 IEEE 24th International Conference on
Data Engineering , pages 277–286. IEEE, 2008.
[32] Charles F Manski. The lure of incredible certitude. Economics & Philosophy , 36(2):216–245,
2020.
12[33] David McClure and Jerome P Reiter. Differential privacy and statistical disclosure risk measures:
An investigation with binary synthetic data. Transactions on Data Privacy , 5(3):535–552, 2012.
[34] Luise Mehner, Saskia Nuñez von V oigt, and Florian Tschorsch. Towards explaining epsilon: A
worst-case study of differential privacy risks. In 2021 IEEE European Symposium on Security
and Privacy Workshops (EuroS&PW) , pages 328–331. IEEE, 2021.
[35] Priyanka Nanayakkara, Johes Bater, Xi He, Jessica Hullman, and Jennie Rogers. Visualiz-
ing privacy-utility trade-offs in differentially private data releases. Proceedings on Privacy
Enhancing Technologies , 2022.
[36] Priyanka Nanayakkara, Mary Anne Smart, Rachel Cummings, Gabriel Kaptchuk, and Elissa M
Redmiles. What are the chances? explaining the epsilon parameter in differential privacy. In
32nd USENIX Security Symposium (USENIX Security 23) , pages 1613–1630, 2023.
[37] NCDHHS. 2020 north carolina infant mortality report, table 1. NC State Center for Health
Statistics, 2020. Accessed on 22 April 2024.
[38] NCDHHS. Cy2020 north carolina resident births by county of residence and month. NC State
Center for Health Statistics, 2020. Accessed on 22 April 2024.
[39] Alisa Pankova and Peeter Laud. Interpreting epsilon of differential privacy in terms of advantage
in guessing or approximating sensitive attributes. In 2022 IEEE 35th Computer Security
Foundations Symposium (CSF) , pages 96–111. IEEE, 2022.
[40] Giovanni Parmigiani and Lurdes Inoue. Decision theory: Principles and approaches . John
Wiley & Sons, 2009.
[41] Jerome P Reiter. Estimating risks of identification disclosure in microdata. Journal of the
American Statistical Association , 100(472):1103–1112, 2005.
[42] Jerome P Reiter. Differential privacy and federal data releases. Annual Review of Statistics and
Its Application , 6:85–101, 2019.
[43] Jayshree Sarathy, Sophia Song, Audrey Haque, Tania Schlatter, and Salil Vadhan. Don’t look
at the data! how differential privacy reconfigures the practices of data science. arXiv preprint
arXiv:2302.11775 , 2023.
[44] Jun Tang, Aleksandra Korolova, Xiaolong Bai, Xueqiang Wang, and Xiaofeng Wang. Pri-
vacy loss in Apple’s implementation of differential privacy on macos 10.12. arXiv preprint
arXiv:1709.02753 , 2017.
[45] Alexandra Wood, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, James
Honaker, Kobbi Nissim, David R O’Brien, Thomas Steinke, and Salil Vadhan. Differen-
tial privacy: A primer for a non-technical audience. Vanderbilt Journal of Entertainment &
Technology Law , 21:209, 2018.
[46] Alexandra Wood, Micah Altman, Kobbi Nissim, and Salil Vadhan. Designing access with
differential privacy. In Handbook on Using Administrative Data for Research and Evidence-
based Policy . Abdul Latif Jameel Poverty Action Lab, Cambridge, MA, 2020.
13A Supplemental Tables
A.1 Notation Summary
Table 2 summarizes the notation of Section 2. Table 3 summarizes the definitions of key probabilistic
quantities.
Symbol Description
P Population of individuals the data is drawn from
Y n×dconfidential data set
Yi Length- dvector of values for individual i
Ii Indicator for whether individual iis included in Y
Y−i (n−1)×dmatrix of the values in Yexcluding those for individual i
S Subset of the support of Yiconstituting a privacy violation
r∗(pi, qi)Function describing the agency’s desired relative risk bound
M Adversary’s model for predicting Yi
T∗Noisy estimate of T(Y), the function being released
Table 2: Summary of notation.
Symbol Definition Description
pi PM[Ii= 1] Prior probability of inclusion
qi PM[Yi∈ S | Ii= 1] Prior probability values disclosed
ri(pi, qi, t∗)PM[Yi∈S,Ii=1|T∗=t∗]
PM[Yi∈S,Ii=1]Relative disclosure risk
ai(pi, qi, t∗)PM[Yi∈ S, Ii= 1|T∗=t∗]Absolute disclosure risk
Table 3: Summary of definitions.
14A.2 Closed Forms for ε
In this section, we provide a reference for all derived closed forms for ε. Corresponding formal
theorem statements are provided in Appendix B.1 and discussion of closed forms not referenced in
the main text is provided in Appendix B.2. Table 4 provides the closed forms.
Risk Profile Condition Recommended ε
(14); ˜r1
2log(˜r)
0<˜q≤˜a
˜r1
2log
˜a(1−˜q)
˜q(1−˜a)
(15) 0<˜q≤1
˜r+1and˜a
˜r<˜q <11
2log
1−˜q
1
˜r−˜q
maxn
˜a
pi˜q,˜ro
1
˜r+1<˜q <1and˜a
˜r<˜q <1log
2˜a(1−˜q)√
(˜r˜q−˜a)2+4˜a˜q(1−˜q)(1−˜a)−(˜r˜q−˜a)
˜q= 1 log
˜r−˜a
1−˜a
(16) 0<˜p≤˜a
˜rlog
˜a(1−˜p)
˜p(1−˜a)
maxn
˜a
˜pqi,˜ro
˜a
˜r<˜p≤1 log
2(˜p˜r−˜a)√
˜r2(1−˜p)2+4(˜p˜r−˜a)(1−˜a)−˜r(1−˜p)
(25) 0≤˜q0≤1
˜r+1log 
2˜p1(1−˜q0)q
(1−˜p1)2+4˜p1(1−˜q0)(1
˜r−˜p1˜q0)−(1−˜p1)!
˜r1
˜r+1<˜q0<1and˜p0>0 log 
2˜p0(1−˜q0)q
(1−˜p0)2+4˜p0(1−˜q0)(1
˜r−˜p0˜q0)−(1−˜p0)!
if˜p0≤pi≤˜p11
˜r+1<˜q0<1and˜p0= 0 log (˜r)
and˜q0≤qi≤˜q1˜q0= 1 log
1−˜p0
1
˜r−˜p0
(28);piqi+˜b
piqilog
1+˜b
1−˜b
Table 4: Closed forms for εfor various r∗(pi, qi).
15B Additional Results
B.1 Omitted Results
In this section, we present all results omitted from the paper. Proofs of these results are included in
Appendix E.
First, we state a corollary to Theorem 1, which generalizes Theorem 1.3 in [ 19] to sets where |S|>1
and to unbounded DP.
Corollary 1. Under the conditions of Theorem 1, for all pi, qi∈(0,1]and all t∗,
ri(pi, qi, t∗)≤e2ε. (18)
Next, we state a corollary to Theorem 2 which considers the special case where pi= 1. This
corresponds to a setting where individual i’s inclusion in the data is known a priori by the adversary,
for example data from a census or public social media platform.
Corollary 2. Under the conditions of Theorem 2, if pi= 1and0< qi<1, any statistic T∗=t∗
released under ε-DP with
ε≤1
2log 
1−qi
1
r∗(1,qi)−qi!
, (19)
will satisfy ri(1, qi, t∗)≤r∗(1, qi).
For particular forms of r∗, the optimization in (13) has a closed form solution. This is detailed by the
following theorems, which are preceded by two lemmas used in the proofs.
Lemma 2. Fixpi, qi∈(0,1)and let r∗(pi, qi) =˜a
piqi. Then the function
ε(pi, qi) = log
2pi(1−qi)r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
−(1−pi)
(20)
has partial derivatives such that
1.∂ε(pi,qi)
∂pi<0for all 0< pi<1and0< qi<1
2.∂ε(pi,qi)
∂qi<0for all 0< pi<1and0< qi<1.
Lemma 3. Fixpi, qi∈(0,1)and let r∗(pi, qi) = ˜r. Then the function
ε(pi, qi) = log
2pi(1−qi)r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
−(1−pi)
(21)
has partial derivatives such that
1.∂ε(pi,qi)
∂pi<0if0< qi<1
˜r+1and0< pi<1
2.∂ε(pi,qi)
∂pi= 0ifqi=1
˜r+1and0< pi<1
3.∂ε(pi,qi)
∂pi>0if1
˜r+1< qi<1and0< pi<1
4.∂ε(pi,qi)
∂qi>0for all 0< pi<1and0< qi<1.
Theorem 3. Under the conditions of Theorem 2, if r∗(pi, qi) = ˜r >1, the solution to the minimiza-
tion problem in (13) is
ε=1
2log (˜r). (22)
16Theorem 4. Under the conditions of Theorem 2, let ˜a <1,˜p≤1, and ˜r >1, and 0< qi<1. If the
function r∗is such that r∗(˜p, qi) = max {˜a/(˜pqi),˜r}andr∗(pi, qi) =∞ifpi̸= ˜p, then the solution
to the minimization problem in (13) is
ε=

log
˜a(1−˜p)
˜p(1−˜a)
, if0<˜p≤˜a
˜r;
log
2(˜p˜r−˜a)√
˜r2(1−˜p)2+4(˜p˜r−˜a)(1−˜a)−˜r(1−˜p)
,if˜a
˜r<˜p≤1.(23)
Theorem 5. Under the conditions of Theorem 2, let ˜a <1,˜q≤1, and ˜r >1, and 0< pi<1. If
the function r∗is such that r∗(pi,˜q) = max {˜a/(pi˜q),˜r}andr∗(pi, qi) =∞forqi̸= ˜q, then the
solution to the minimization problem in (13) is
ε=

1
2log
˜a(1−˜q)
˜q(1−˜a)
, if0<˜q≤˜a
˜r;
1
2log
1−˜q
1
˜r−˜q
, if0<˜q≤1
˜r+1and˜a
˜r<˜q <1;
log
2˜a(1−˜q)√
(˜r˜q−˜a)2+4˜a˜q(1−˜q)(1−˜a)−(˜r˜q−˜a)
,if1
˜r+1<˜q <1and˜a
˜r<˜q <1;
log
˜r−˜a
1−˜a
, if˜q= 1.(24)
B.2 Additional Closed Forms
In this section, we present closed form results for the εimplied by additional disclosure risk profiles
not discussed in the main text. Proofs of these results are not provided, although we note that proofs
would have a structure similar to the proofs of Theorems 3–5. It is straightforward to verify these
results empirically.
First, consider an agency that requires a constant relative risk bound to hold on a subset of the (pi, qi)
space and does not have any bounds outside that space. That is, rather than enforcing r∗(pi, qi) = ˜r
for all 0≤pi, qi≤1, the agency only enforces this condition for ˜p0≤pi≤˜p1and˜q0≤qi≤˜q1,
where 0≤˜p0≤˜p1≤1and0≤˜q0≤˜q1≤1(for˜p1,˜q1>0). Formally,
r∗(pi, qi) =˜r, if˜p0≤pi≤˜p1and˜q0≤qi≤˜q1;
∞,otherwise .(25)
The recommended εfor an agency with the risk profile in (25) is
ε=

log 
2˜p1(1−˜q0)q
(1−˜p1)2+4˜p1(1−˜q0)(1
˜r−˜p1˜q0)−(1−˜p1)!
,if0≤˜q0≤1
˜r+1;
log 
2˜p0(1−˜q0)q
(1−˜p0)2+4˜p0(1−˜q0)(1
˜r−˜p0˜q0)−(1−˜p0)!
,if1
˜r+1<˜q0<1and˜p0>0;
log (˜r), if1
˜r+1<˜q0<1and˜p0= 0;
log
1−˜p0
1
˜r−˜p0
, if˜q0= 1.(26)
Next, consider an agency that desires a constant bound on the difference between the posterior and
the prior. That is, for some ˜b∈(0,1), they would like to enforce
PM[Yi∈ S, Ii= 1|T∗=t∗]−PM[Yi∈ S, Ii= 1]≤˜b. (27)
This constraint corresponds to the following risk profile.
r∗(pi, qi) =piqi+˜b
piqi= 1 +˜b
piqi. (28)
The recommended εfor an agency with this risk profile is
ε= log 
1 +˜b
1−˜b!
. (29)
Notably, by a Taylor series approximation, −log(1−˜b)≈˜b+˜b2/2. Thus, for small ˜b, it follows
that the recommended εcan be approximated by
ε= log(1 + ˜b)−log(1−˜b)≈ 
˜b−˜b2
2!
+ 
˜b+˜b2
2!
= 2˜b. (30)
17Figure 3: The risk profiles for three agencies with risk profile given by (31). The lines in the lower
panels represent the risk profiles for qi= 1as a function of pi, and the colors represent the implied
εat each point on the curve. The lines in the upper panels represent the corresponding baseline
r∗(pi,1) = ˜ r. The left plots set ˜r= 1.5, the center plots set ˜r= 3, and the right plots set ˜r= 6.
C Additional Examples
This section provides additional examples of risk profiles.
C.1 Additional Figure for Example 1
Here we provide an additional figure for Example 1. The setting is restated below.
Example 1. A healthcare provider possesses a data set comprising demographic information about
individuals diagnosed with a particular form of cancer in a region of interest. They plan to release
the count of individuals diagnosed with this form of cancer in various demographic groups via the
geometric mechanism, but are concerned this release, if insufficient noise is added, could reveal
which individuals in the community have cancer. They wish to choose εappropriately.
As discussed in Section 4, a reasonable risk profile for this setting might focus on piand set qi= 1.
If we additionally suppose the agency is generally willing to accept a maximum absolute disclosure
risk of 0.25for adversaries with small prior probabilities. A risk profile for this agency might be of
the form, for some ˜r >1,
r∗(pi, qi) =(
maxn
0.25
pi,˜ro
,ifqi= 1;
∞, ifqi̸= 1.(31)
The three example risk functions considered in Section 4 are presented in Figure 3. Agency A is
risk averse, agency C is utility seeking, and agency B sits in between in terms of risk and utility. For
adversaries with high prior probabilities, agencies A, B, and C bound the relative disclosure risk at
˜r= 1.5,˜r= 3, and ˜r= 6, respectively.
C.2 Extended Example 2
Here we provide an extended analysis of Example 2. The setting is restated below.
18Figure 4: The risk profiles for three agencies with risk profile given by (32). The lines represent the
risk profiles for pi= 0.05as a function of qi, and the colors represent the implied εat each point on
the curve. The left plot sets ˜a= 0.025, the center sets ˜a= 0.15, and the right sets ˜a= 0.3.
Agency ˜a εRecommendation Noise Std. Dev. Prob. Exact
A 0.025 1.09 1 .24 50%
B 0.15 1.21 1 .10 54%
C 0.3 2.10 0 .56 78%
Table 5: For each of the three risk profiles in Figure 4, we present the εrecommended by our frame-
work. For a release satisfying ε-DP using the geometric mechanism, we present the corresponding
standard deviation of the noise distribution and the probability that the exact value is released (i.e.,
the noise distribution’s probability mass at zero).
Example 2. A survey is performed on a sample of individuals in the region of interest. 5% of the
region is surveyed, and respondents are asked whether they have this particular form of cancer
along with a series of demographic questions. The agency plans to release the counts of surveyed
individuals who have and do not have the cancer in various demographic groups via the geometric
mechanism, but is concerned this release, if insufficient noise is added, could reveal which individuals
in the community have cancer. The agency wishes to choose εappropriately.
As discussed in Section 4, a reasonable risk profile for this setting might set S={y∈ {0,1}d:y1=
1}and focus on qi, while fixing pi= 0.05. Additionally, suppose the agency is generally willing
to accept a maximum relative disclosure risk of 3for adversaries with large prior probabilities. A
reasonable risk profile for this agency might be of the form, for some ˜a <1,
r∗(pi, qi) =(
maxn
˜a
0.05qi,3o
,ifpi= 0.05;
∞, ifpi̸= 0.05.(32)
Three example risk functions of this form are presented in Figure 4. Once again, agency A is risk
averse, agency C is utility seeking, and agency B sits in between on risk and utility. Agencies A, B,
and C are willing to allow adversaries to achieve an absolute disclosure risk of ˜a= 0.025,˜a= 0.15,
and˜a= 0.3, respectively. Table 5 presents the εrecommendations for each agency along with the
standard deviation of the noise and probability of releasing the exact value of each statistic under the
geometric mechanism.
As in Table 1, the εrecommendations reflect trade offs between privacy and accuracy. They also are
much higher than the recommendations from a corresponding simple risk profile of r∗(pi, qi) = 3 for
all prior probabilities, which implies ε≈0.55. Even the most risk averse agency is recommended an
εthat is much larger than this baseline risk profile. This gain is primarily due to the assumptions that
the survey is a simple random sample from the population, and the adversary has no prior knowledge
about which individuals are surveyed. Essentially, the additional uncertainty from the sampling
mechanism allows for an εrecommendation with less noise injected. This is consistent with prior
work showing that DP mechanisms applied to random subsamples provide better privacy guarantees
[3]. The recommended εwill increase as the proportion of individuals from the population surveyed
19Figure 5: The top panel presents the r∗(pi, qi)from (33) as a function of piandqi. The bot-
tom panel presents the implied εi(pi, qi)as a function of piandqi. The red point represents
argmin(pi,qi)εi(pi, qi). For clarity of presentation, all r∗(pi, qi)>100are truncated to 100and all
εi(pi, qi)>4are truncated to 4.
(˜p) decreases. For example, for the risk function in (32) with ˜a= 0.025and˜r= 3, if˜p= 0.005, we
recommend ε≈1.63; if˜p= 0.0005 , we recommend ε≈3.94.
C.3 Two-dimensional Example
The examples in Appendices C.1 and C.2 focus on settings where only one of piandqiis important
and the other can be reasonably fixed to a constant. Now we consider the setting where both may be
simultaneously important. For example, in Example 2, the agency may wish to consider adversaries
with various pi, rather than only ˜p= 0.05. Perhaps the agency wishes to limit the absolute disclosure
to be below ˜a= 0.25when total prior probability of disclosure, piqi, is low and limit relative
disclosure to be below ˜r= 3when piqiis high. This corresponds to the following risk profile.
r∗(pi, qi) = max˜a
piqi,˜r
= max0.25
piqi,3
. (33)
A plot of this desired bound on pairs (pi, qi)on the space (0,1]×(0,1]is presented in the top panel
of Figure 5. Notably, if piis fixed at ˜p∈(0,1], a plot of r∗(˜p, qi)as a function of qihas a form
similar to the plots of Figure 4. Similarly, if qiis fixed at ˜q∈(0,1], a plot of r∗(pi,˜q)as a function
ofpihas a form similar to the plots of Figure 3.
20To determine the εrecommended by this profile, we numerically solve the minimization problem in
(13). A plot of εi(pi, qi)as a function of pairs (pi, qi)on the space (0,1]×(0,1]is presented in the
bottom panel of Figure 5. Our framework recommends ε≈0.65, which corresponds to pi= 1and
qi= 0.083 = ˜ a/˜r.
D Additional Discussion of Prior Work
In this section, we describe two previous works with similar aims to our framework using the notation
in Section 2.2, expanding on the discussion in Section 6.
D.1 “How Much is Enough? Choosing εfor Differential Privacy"
[29] focus on settings where the population, P, of size nis public information and the adversary’s
goal is to determine which subset of individuals in Pwas used for a differentially private release of
a statistic. We can characterize their setting with the notation of Section 2.2 as follows. We define
Yto be the subset of individuals’ values in Pused to compute the statistic of interest, T(Y), and
its released DP counterpart, T∗(Y). In their examples, the authors focus on the setting where only
one individual is removed from Pto create Y, and the adversary’s goal is to determine which iwas
removed.
We can apply our framework to this setting with a minor modification. For this comparison, we
assume the adversary’s qi=PM[Yi∈ S | Ii= 0] = 1 for any set S(although we note that this is a
weaker assumption than that of Lee and Clifton, since we do not assume Pis public). We redefine pi
and the risk measures to be in terms of Ii= 0, rather than Ii= 1.
pi=PM[Ii= 0] (34)
ri(pi,1, t∗) =PM[Ii= 0|T∗=t∗]
PM[Ii= 0](35)
ai(pi,1, t∗) =PM[Ii= 0|T∗=t∗]. (36)
[29] focus on the case of pi= 1/n, and seek to enforce the bound ai(1/n,1, t∗)≤˜afor some
constant ˜aand all t∗, which implies the relative risk bound
r∗(pi, qi) =n˜a, ifpi=1
n, qi= 1;
∞,otherwise .(37)
From an analogy to Theorem 2 with the redefined pi, it follows that under these conditions, our
method sets
ε= log1−1
n
1
n˜a−1
n
= log(n−1)˜a
1−˜a
. (38)
In the motivating example from their paper, the authors set n= 4and˜a= 1/3, giving r∗(1/n,1) =
4/3. This results in ε= log(3 /2)≈0.41from our method. When the release mechanism is the
addition of Laplace noise, [ 29]’s method arrives at a similar form, but with the recommendation
scaled by a factor of ∆T/∆v.
ε=∆T
∆vlog(n−1)˜a
1−˜a
, (39)
∆T= max {|T(Y1)−T(Y2)|:Y2⊂Y1⊂P,|Y1|=n−1,|Y2|=n−2} (40)
∆v= max {|T(Y1)−T(Y2)|:Y1⊂P,Y2⊂P,|Y1|=|Y2|=n−1}. (41)
The recommendation of [ 29]’s method thus depends on both the population and the particular release
function. This results in different εvalues for the same nand˜a, as low as 0.34and as high as 1.62in
the authors’ examples, depending on the statistic of interest and the values in the data.
D.2 “Differential Privacy: A Primer for a Non-Technical Audience"
The example in [ 45] considers an individual deciding whether or not to participate in a survey for
which results will be released via DP with a particular ε. Using our notation, let Zi=f(Yi, Ii)∈
21{0,1}be a quantity of interest to the adversary, who wishes to learn whether Zi= 1. They have
some prior qi=PM[Zi= 1]. Rather than considering the relative or absolute disclosure risk, the
individual is interested in comparing the adversary’s posterior probability if they participate in the
survey, a1i(qi, t∗) =PM[Zi= 1|Ii= 1, T∗=t∗], to the adversary’s posterior probability if they
do not participate, a0i(qi, t∗) =PM[Zi= 1|Ii= 0, T∗=t∗]. The authors of [ 45] state that for all
qiand all t∗,
a1i(qi, t∗)≤a0i(qi, t∗)
a0i(qi, t∗) +e−2ε(1−a0i(qi, t∗)). (42)
This expression is in the same spirit as the results from our framework with pi= 1. By Theorem 1,
we have
ri(1, qi, t∗)≤1
qi+e−2ε(1−qi)=⇒ ai(1, qi, t∗)≤qi
qi+e−2ε(1−qi). (43)
The authors of [ 45] suggest that the individual considering survey participation use (42) to bound a1i
for various values of a0i. The individual can examine these bounds to make an informed decision
about whether to participate in the survey.
22E Proofs
This section provides proofs of results from Section 3 and Appendix B.1.
Lemma 1. Under Assumption 1 and Assumption 2, if the release of T∗=t∗satisfies DP , then for
any subset Sof the domain of Yi, we have
e−ε≤PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Ii= 0]≤eε, e−2ε≤PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Yi/∈ S, Ii= 1]≤e2ε.
(10)
Proof. To begin, we demonstrate that for all yin the support of Yiandt∗in the support of T∗,
e−ε≤PM[T∗=t∗|Yi=y, Ii= 1]
PM[T∗=t∗|Ii= 0]≤eε. (44)
LetY−ibe the support of Y−iunderM. Then,
PM[T∗=t∗|Yi=y, Ii= 1]
=X
y−i∈Y−iP[T∗=t∗|Yi=y,Y−i=y−i, Ii= 1]PM[Y−i=y−i|Yi=y, Ii= 1] (45)
≤X
y−i∈Y−ieεP[T∗=t∗|Y−i=y−i, Ii= 0]PM[Y−i=y−i|Yi=y, Ii= 1] (46)
=eεX
y−i∈Y−iPM[T∗=t∗|Y−i=y−i, Ii= 0]P[Y−i=y−i|Ii= 0] (47)
=eεPM[T∗=t∗|Ii= 0]. (48)
The equality in (45) follows from the law of total probability and Assumption 1. The inequality in
(46) follows from DP via (1). The equality in (47) follows from Assumption 2. The equality in (48)
follows from the law of total probability and Assumption 1. This completes the proof of the right
inequality. The proof of the left inequality is identical with the other inequality in (1) applied in (46).
Now, we apply Bayes’ Theorem to PM[T∗=t∗|Yi∈ S, Ii= 1].
PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Ii= 0]=PM[Yi∈S|T∗=t∗,Ii=1]PM[T∗=t∗|Ii=1]
PM[Yi∈S|Ii=1]
PM[T∗=t∗|Ii= 0](49)
=PM[T∗=t∗|Ii= 1]PM[Yi∈ S | T∗=t∗, Ii= 1]
PM[Yi∈ S | Ii= 1]PM[T∗=t∗|Ii= 0](50)
We can then break the second term in the numerator into a summation and apply Bayes’ Theorem to
each term in the sum.
PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Ii= 0]
=PM[T∗=t∗|Ii= 1]P
y∈SPM[Yi=y|T∗=t∗, Ii= 1]
PM[Yi∈ S | Ii= 1]PM[T∗=t∗|Ii= 0](51)
=PM[T∗=t∗|Ii= 1]P
y∈SPM[T∗=t∗|Yi=y,Ii=1]PM[Yi=y|Ii=1]
PM[T∗=t∗|Ii=1]
PM[Yi∈ S | Ii= 1]PM[T∗=t∗|Ii= 0](52)
=P
y∈SPM[T∗=t∗|Yi=y,Ii=1]
PM[T∗=t∗|Ii=0]PM[Yi=y|Ii= 1]
PM[Yi∈ S | Ii= 1](53)
We apply (44) to achieve the left bound in the first expression of (10).
PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Ii= 0]≥P
y∈Se−εPM[Yi=y|Ii= 1]
PM[Yi∈ S | Ii= 1]=e−ε. (54)
We achieve the right bound in the first expression of (10) in a similar fashion.
PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Ii= 0]≤P
y∈SeεPM[Yi=y|Ii= 1]
PM[Yi∈ S | Ii= 1]=eε. (55)
23We now turn to the second expression in (10). First note that
PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Yi/∈ S, Ii= 1]
=PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Ii= 0]·PM[T∗=t∗|Ii= 0]
PM[T∗=t∗|Yi/∈ S, Ii= 1]. (56)
By applying the bounds in the first expression in (10) to SandSC,
e−ε≤PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Ii= 0]≤eε, (57)
e−ε≤PM[T∗=t∗|Yi/∈ S, Ii= 1]
PM[T∗=t∗|Ii= 0]≤eε. (58)
Thus, to achieve the left inequality in the second expression in (10),
PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Yi/∈ S, Ii= 1]≥e−ε·(eε)−1=e−2ε. (59)
To achieve the right inequality in the second expression in (10),
PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗|Yi/∈ S, Ii= 1]≤eε· 
e−ε−1=e2ε. (60)
24Theorem 1. Under Assumption 1 and Assumption 2, if the release of T∗=t∗satisfies DP , then
ri(pi, qi, t∗)≤1
qipi+e−2ε(1−qi)pi+e−ε(1−pi). (11)
Proof. We begin by applying Bayes’ Theorem to reverse the conditional in the relative risk.
ri(pi, qi, t∗) =PM[Yi∈ S, Ii= 1|T∗=t∗]
PM[Yi∈ S, Ii= 1](61)
=PM[T∗=t∗|Yi∈S,Ii=1]PM[Yi∈S,Ii=1]
PM[T∗=t∗]
PM[Yi∈ S, Ii= 1](62)
=PM[T∗=t∗|Yi∈ S, Ii= 1]
PM[T∗=t∗]. (63)
We may decompose the denominator via the law of total probability.
PM[T∗=t∗] =PM[T∗=t∗|Yi∈ S, Ii= 1]PM[Yi∈ S | Ii= 1]PM[Ii= 1]
+PM[T∗=t∗|Yi/∈ S, Ii= 1]PM[Yi/∈ S | Ii= 1]PM[Ii= 1]
+PM[T∗=t∗|Ii= 0]PM[Ii= 0] (64)
=PM[T∗=t∗|Yi∈ S, Ii= 1]qipi
+PM[T∗=t∗|Yi/∈ S, Ii= 1] (1 −qi)pi
+PM[T∗=t∗|Ii= 0] (1 −pi). (65)
Using this expansion in the expression for riand dividing through by the numerator yields
ri(pi, qi, t∗) =1
qipi+PM[T∗=t∗|Yi/∈S,Ii=1]
PM[T∗=t∗|Yi∈S,Ii=1](1−qi)pi+PM[T∗=t∗|Ii=0]
PM[T∗=t∗|Yi∈S,Ii=1](1−pi).(66)
Using Lemma 1, we then have
ri(pi, qi, t∗)≤1
qipi+e−2ε(1−qi)pi+e−ε(1−pi). (67)
25Theorem 2. For any individual i, fix the prior probabilities, piandqi, and a desired bound on the
relative disclosure risk, r∗(pi, qi). Define εi(pi, qi)to be the function.
εi(pi, qi) =

log
2pi(1−qi)r
(1−pi)2+4pi(1−qi)
1
r∗(pi,qi)−piqi
−(1−pi)
,if0< qi<1;
log
1−pi
1
r∗(pi,1)−pi
, ifqi= 1.(12)
Under the conditions of Theorem 1, any statistic T∗=t∗released under ε-DP with ε≤εi(pi, qi)
will satisfy ri(pi, qi, t∗)≤r∗(pi, qi).
Proof. We begin with the simpler case where qi= 1. By Theorem 1,
ri(pi,1, t∗)≤1
pi+e−ε(1−pi). (68)
Since ε≤log ((1 −pi)/(1/r∗(pi,1)−pi)), it follows that e−ε≥(1/r∗(pi,1)−pi)/(1−pi). Thus,
as desired,
ri(pi,1, t∗)≤1
pi+1
r∗(pi,1)−pi
1−pi(1−pi)=1
pi+
1
r∗(pi,1)−pi=r∗(pi,1). (69)
Now consider the case where 0< qi<1. By Theorem 1,
ri(pi, qi, t∗)≤1
qipi+e−2ε(1−qi)pi+e−ε(1−pi). (70)
Since
ε≤log
2pi(1−qi)r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
−(1−pi)
, (71)
it follows that
e−ε≥r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
−(1−pi)
2pi(1−qi). (72)
Taking the square gives
e−2ε≥4pi(1−qi)
1
r∗(pi,qi)−piqi
+ 2(1−pi)2
4p2
i(1−qi)2
−2(1−pi)r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
4p2
i(1−qi)2(73)
=1
r∗(pi,qi)−piqi
pi(1−qi)+(1−pi)2−(1−pi)r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
2p2
i(1−qi)2.
(74)
26Thus,
qipi+e−2ε(1−qi)pi+e−ε(1−pi)
≥qipi+1
r∗(pi,qi)−piqi
pi(1−qi)·(1−qi)pi
+(1−pi)2−(1−pi)r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
2p2
i(1−qi)2·(1−qi)pi
+r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
−(1−pi)
2pi(1−qi)·(1−pi) (75)
=qipi+1
r∗(pi, qi)−piqi
+ 
(1−pi)−s
(1−pi)2+ 4pi(1−qi)1
r∗(pi, qi)−piqi!
1−pi
2pi(1−qi)
− 
(1−pi)−s
(1−pi)2+ 4pi(1−qi)1
r∗(pi, qi)−piqi!
1−pi
2pi(1−qi)(76)
=1
r∗(pi, qi). (77)
It is then immediate that
ri(pi, qi, t∗)≤1
qipi+e−2ε(1−qi)pi+e−ε(1−pi)≤1
1
r∗(pi,qi)=r∗(pi, qi). (78)
27Corollary 1. Under the conditions of Theorem 1, for all pi, qi∈(0,1]and all t∗,
ri(pi, qi, t∗)≤e2ε. (79)
Proof. First note that since, 0≤e−ε≤1, it follows that e−2ε≤e−ε. Then, applying the result of
Theorem 1,
ri(pi, qi, t∗)≤1
qipi+e−2ε(1−qi)pi+e−ε(1−pi)≤1
qipi+e−2ε(1−qi)pi+e−2ε(1−pi)
(80)
Combining terms and using the fact that qipi≥0gives
ri(pi, qi, t∗)≤1
qipi+e−2ε(1−qipi)≤1
e−2ε+qipi(1−e−2ε)≤1
e−2ε+ 0=e2ε. (81)
Corollary 2. Under the conditions of Theorem 2, if pi= 1 and0< qi<1, then any statistic
T∗=t∗released under ε-DP with
ε≤1
2log 
1−qi
1
r∗(1,qi)−qi!
, (82)
will satisfy ri(1, qi, t∗)≤r∗(1, qi).
Proof. Plugging pi= 1into the expression from Theorem 2 yields
ε≤log
2(1−qi)r
0 + 4(1 −qi)
1
r∗(1,qi)−qi
−0
(83)
= log s
1−qi
1
r∗(1,qi)−qi!
(84)
=1
2log 
1−qi
1
r∗(1,qi)−qi!
. (85)
28Lemma 2. Fixpi, qi∈(0,1)and let r∗(pi, qi) = ˜a/(piqi). Then the function
ε(pi, qi) = log
2pi(1−qi)r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
−(1−pi)
(86)
has partial derivatives such that
1.∂ε(pi,qi)
∂pi<0for all 0< pi<1and0< qi<1
2.∂ε(pi,qi)
∂qi<0for all 0< pi<1and0< qi<1.
Proof. To begin, we re-express the function of interest in the form
ε(pi, qi) = log (2 pi(1−qi))−logr
(1−pi)2+ 4pi(1−qi)piqi
˜a−piqi
−(1−pi)
(87)
= log (2 pi(1−qi))−log s
(1−pi)2+ 4p2
iqi(1−qi)1
˜a−1
−(1−pi)!
.(88)
We first examine ∂ε(pi, qi)/∂pi. Taking the partial derivative with respect to qigives
∂ε(pi, qi)
∂pi=1
pi−−2(1−pi)+8piqi(1−qi)(1
˜a−1)
2q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)+ 1
q
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1
−(1−pi)(89)
=1
pi+(1−pi)−4piqi(1−qi)(1
˜a−1)−q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)q
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1
−(1−pi)(90)
=(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)+pi(1−pi)−4p2
iqi(1−qi)(1
˜a−1)−[(1−pi)−pi]q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)
pihq
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1
−(1−pi)i
(91)
=[(1−pi)+pi](1−pi)−q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)
pihq
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1
−(1−pi)i (92)
=−q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)−(1−pi)
q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)
pihq
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1
−(1−pi)i (93)
=−1
piq
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1. (94)
Certainly, the denominator of (94) is positive. Thus, ∂ε(pi, qi)/∂pi<0, as desired.
29We now examine ∂ε(pi, qi)/∂qi. Taking the partial derivative with respect to qigives
∂ε(pi, qi)
∂qi=−1
1−qi−4p2
i(1
˜a−1)∂
∂qi[qi(1−qi)]
2q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)q
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1
−(1−pi)(95)
=−q
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1
−(1−pi) +2p2
i(1
˜a−1)(1−qi)∂
∂qi[qi(1−qi)]
q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)
(1−qi)hq
(1−pi)2+ 42qi(1−qi) 1
˜a−1
−(1−pi)i (96)
=−(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)−(1−pi)q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)+2p2
i(1
˜a−1)(1−qi)∂
∂qi[qi(1−qi)]
q
(1−pi)2+4p2
iqi(1−qi)(1
˜a−1)
(1−qi)hq
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1
−(1−pi)i .
(97)
Consider the three parts of (97). Since ˜a <1, we have that in the denominator of the numerator,q
(1−pi)2+ 4p2
iqi(1−qi) 1
˜a−1
>0. The denominator is also positive, since (1−qi)>0and
"s
(1−pi)2+ 4p2
iqi(1−qi)1
˜a−1
−(1−pi)#
>hp
(1−pi)2−(1−pi)i
= 0.(98)
This leaves the numerator of the numerator of (97). Let us denote this expression as g(qi). If
g(qi)>0, then it follows that ∂ε(pi, qi)/∂qi<0. To show this, we begin by simplifying the
expression for g(qi):
g(qi) = (1 −pi)2+ 4p2
iqi(1−qi)1
˜a−1
−(1−pi)s
(1−pi)2+ 4p2
iqi(1−qi)1
˜a−1
+ 2p2
i1
˜a−1
(1−qi)∂
∂qi[qi(1−qi)] (99)
= (1−pi)2+ 2p2
i(1−qi)(2qi)1
˜a−1
−(1−pi)s
(1−pi)2+ 4p2
i(1−qi)qi1
˜a−1
+ 2p2
i(1−qi)1
˜a−1
(1−2qi) (100)
= (1−pi)2−(1−pi)s
(1−pi)2+ 4p2
i(1−qi)qi1
˜a−1
+ 2p2
i(1−qi)1
˜a−1
.
(101)
Taking the derivative of g, we find that
∂g(qi)
∂qi=−(1−pi)4p2
i(1−2qi) 1
˜a−1
2q
(1−pi)2+ 4p2
i(1−qi)qi 1
˜a−1−2p2
i1
˜a−1
(102)
=−2p2
i1
˜a−1
(1−pi)1−2qiq
(1−pi)2+ 4p2
i(1−qi)qi 1
˜a−1+ 1
. (103)
30For0< qi≤1/2,1−2qi≥0and so ∂g(qi)/∂qi<0. For 1/2< qi<1, since 2qi−1<1andp
(1−pi)2+ 4p2
i(1−qi)qi(1/˜a−1)>1−pi,
∂g(qi)
∂qi=−2p2
i1
˜a−1
1−(1−pi)2qi−1q
(1−pi)2+ 4p2
i(1−qi)qi 1
˜a−1
 (104)
<−2p2
i1
˜a−1
1−(1−pi)1
1−pi
(105)
<0. (106)
Thus, g(qi)strictly decreasing as a function of qion0< qi<1. Since
g(1) = (1 −pi)2−(1−pi)s
(1−pi)2+ 4p2
i(1−1)11
˜a−1
+2p2
i(1−1)1
˜a−1
= 0,(107)
it follows that g(qi)>0for0< qi<1and so ∂ε(pi, qi)/∂qi<0in this range.
31Lemma 3. Fixpi, qi∈(0,1)and let r∗(pi, qi) = ˜r <1/(piqi). Then the function
ε(pi, qi) = log
2pi(1−qi)r
(1−pi)2+ 4pi(1−qi)
1
r∗(pi,qi)−piqi
−(1−pi)
(108)
has partial derivatives such that
1.∂ε(pi,qi)
∂pi<0if0< qi<1
˜r+1and0< pi<1
2.∂ε(pi,qi)
∂pi= 0ifqi=1
˜r+1and0< pi<1
3.∂ε(pi,qi)
∂pi>0if1
˜r+1< qi<1and0< pi<1
qi˜r
4.∂ε(pi,qi)
∂qi>0if0< pi<1and0< qi<1
pi˜r.
Proof. To begin, we re-express the function of interest in the form
ε(pi, qi) = log (2 pi(1−qi))−log s
(1−pi)2+ 4pi(1−qi)1
˜r−piqi
−(1−pi)!
.(109)
We now examine ∂ε(pi, qi)/∂pi. Taking the partial derivative of with respect to piand simplifying
gives
∂ε(pi, qi)
∂pi=1
pi−∂
∂pi[(1−pi)2+4pi(1−qi)(1
˜r−piqi)]
2q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)+ 1
q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)(110)
=1
pi−−2(1−pi)+4(1−qi)(1
˜r−piqi)−4pi(1−qi)qi+2q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)
2q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)(111)
=(1−pi)2+4pi(1−qi)(1
˜r−piqi)−(1−pi)q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)
pihq
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)i
+pi(1−pi)−2pi(1−qi)(1
˜r−piqi)+2p2
i(1−qi)qi−piq
(1−pi)2+4pi(1−qi)(1
˜r−piqi)q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)
pihq
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)i (112)
=(1−pi)2+4pi(1−qi)(1
˜r−piqi)+pi(1−pi)−2pi(1−qi)(1
˜r−piqi)+2p2
i(1−qi)qi−q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)
pihq
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)i
(113)
=2pi(1−qi)1
˜r+ (1−pi)−q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
pihq
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)iq
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi.
(114)
We can rearrange the expression in (114) to the following form.
∂ε(pi, qi)
∂pi=2pi(1−qi)1
˜r q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)−(1−pi)−1
piq
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi. (115)
32The denominator of (115) is certainly always positive, so the sign of ∂ε(pi, qi)/∂piis determined by
the sign of the numerator. To determine the sign, we will use the following equality.
2pi(1−qi)1
˜r
=s
(1−pi)2+ 4pi(1−qi)1
˜r−piqi
+ 4p2
i(1−qi)˜r2−1
˜r2
qi−1
˜r+ 1
−(1−pi).
(116)
To demonstrate that (116) holds, note the following.

2pi(1−qi)1
˜r+(1−pi)2
= 4p2
i(1−qi)21
˜r2+ 4pi(1−pi)(1−qi)1
˜r+ (1−pi)2(117)
= 4p2
i(1−qi)21
˜r2+ 4pi(1−pi)(1−qi)1
˜r−4p2
i(1−qi)˜r2−1
˜r2
qi−1
˜r+ 1
+ 4p2
i(1−qi)˜r2−1
˜r2
qi−1
˜r+ 1
+ (1−pi)2(118)
= 4pi(1−qi)
pi(1−qi)1
˜r2+ (1−pi)1
˜r−pi˜r2−1
˜r2
qi−1
˜r+ 1
+ 4p2
i(1−qi)˜r2−1
˜r2
qi−1
˜r+ 1
+ (1−pi)2(119)
= 4pi(1−qi)1
˜r+pi1
˜r2−1
˜r+˜r−1
˜r2
−piqi1
˜r2+˜r2−1
˜r2
+ 4p2
i(1−qi)˜r2−1
˜r2
qi−1
˜r+ 1
+ (1−pi)2(120)
= 4pi(1−qi)1
˜r−piqi
+ 4p2
i(1−qi)˜r2−1
˜r2
qi−1
˜r+ 1
+ (1−pi)2.
(121)
Rearranging (121) gives (116).
Now note that, from (116), if qi= 1/(˜r+ 1) , then
2pi(1−qi)1
˜r=s
(1−pi)2+ 4pi(1−qi)1
˜r−piqi
−(1−pi). (122)
It follows that the ratio in the numerator of (115) equals 1and thus ∂ε(pi, qi)/∂pi= 0. If0< qi<
1
˜r+1, then the term 4p2
i(1−qi)(˜r2−1)/˜r2(qi−1/(˜r+ 1)) <0and so
2pi(1−qi)1
˜r<s
(1−pi)2+ 4pi(1−qi)1
˜r−piqi
−(1−pi). (123)
It follows that the ratio in the numerator of (115) is less than 1and thus ∂ε(pi, qi)/∂pi<0. If
1
˜r+1< qi<1, then the term 4p2
i(1−qi)(˜r2−1)/˜r2(qi−1/(˜r+ 1)) >0and so
2pi(1−qi)1
˜r>s
(1−pi)2+ 4pi(1−qi)1
˜r−piqi
−(1−pi). (124)
It follows that the ratio in the numerator of (115) is greater than 1and thus ∂ε(pi, qi)/∂pi>0.
33We now examine ∂ε(pi, qi)/∂qi. Taking the partial derivative of with respect to qigives
∂ε(pi, qi)
∂qi=−1
1−qi−4pi∂
∂qi[(1−qi)(1
˜r−piqi)]
2q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)(125)
=−q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi) +2pi(1−qi)∂
∂qi[(1−qi)(1
˜r−piqi)]q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)
(1−qi)hq
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)i (126)
=−(1−pi)2+4pi(1−qi)(1
˜r−piqi)−(1−pi)q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)+2pi(1−qi)∂
∂qi[(1−qi)(1
˜r−piqi)]q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)
(1−qi)hq
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)i .
(127)
Consider the three parts of (127). Since ˜r <1/(piqi), it follows that (1/˜r−piqi)>0. Thus, in
the denominator of the numerator,p
(1−pi)2+ 4pi(1−qi) (1/˜r−piqi)>0. The denominator is
also positive, since (1−qi)>0and
"s
(1−pi)2+ 4pi(1−qi)1
˜r−piqi
−(1−pi)#
>hp
(1−pi)2−(1−pi)i
= 0.(128)
This leaves the numerator of the numerator of (127). Let us denote this expression as g(qi). If
g(qi)<0, then it follows that ∂ε(pi, qi)/∂qi>0. To show this, we begin by simplifying the
expression for g(qi):
g(qi) = (1 −pi)2+ 4pi(1−qi)1
˜r−piqi
−(1−pi)s
(1−pi)2+ 4pi(1−qi)1
˜r−piqi
+ 2pi(1−qi)∂
∂qi
(1−qi)1
˜r−piqi
(129)
= (1−pi)2+ 2pi(1−qi)2
˜r−2piqi
−(1−pi)s
(1−pi)2+ 4pi(1−qi)1
˜r−piqi
+ 2pi(1−qi)
2piqi−pi−1
˜r
(130)
= (1−pi)2−(1−pi)s
(1−pi)2+ 4pi(1−qi)1
˜r−piqi
+ 2pi(1−qi)1
˜r−pi
.
(131)
Taking the derivative of g, we find that
∂g(qi)
∂qi=−(1−pi)4pi 
2piqi−pi−1
˜r
2q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi−2pi1
˜r−pi
(132)
= 2pi
(1−pi)2pi(1−qi) + 1
˜r−pi
q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi−1
˜r−pi
. (133)
34Note that since ˜r <1
piqi, it follows that 1
˜r−piqi
>0andpi(1−qi)> pi−1
r. Thus,
∂g(qi)
∂qi= 2pi
(1−pi)2pi(1−qi)− 
pi−1
˜r
q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi+
pi−1
˜r
 (134)
>2pi"
(1−pi) 
pi−1
˜r
p
(1−pi)2+
pi−1
˜r#
(135)
= 4pi
pi−1
˜r
(136)
>0. (137)
Thus, g(qi)is strictly increasing for ˜r <1/(piqi). Note that in terms of qi, this is equivalent to the
range 0< qi<1/(pi˜r). Since
g1
pi˜r
= (1−pi)2−(1−pi)s
(1−pi)2+ 4pi(1−1
pi˜r)1
˜r−pi1
pi˜r
+ 2pi
1−1
pi˜r1
˜r−pi
(138)
= 2pi
1−1
pi˜r1
˜r−pi
(139)
<0, (140)
it follows that g(qi)<0for0< qi<1/(pi˜r). Thus, as desired, ∂ε(pi, qi)/∂qi>0in this
range.
35Theorem 3. Under the conditions of Theorem 2, if r∗(pi, qi) = ˜r >1, the solution to the minimiza-
tion problem in (13) is
ε=1
2log (˜r). (141)
Proof. We define ε(pi, qi)as follows.
ε(pi, qi) =

log 
2pi(1−qi)q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)−(1−pi)!
,if0< qi<1;
log
1−pi
1
˜r−pi
, ifqi= 1.(142)
The solution to the minimization problem in (13) corresponds to the minimum of ε(pi, qi)over
(0,1]×(0,1]. Lemma 3 implies that ∂ε(pi, qi)/∂qi̸= 0for any 0< qi<1. This means that this
function must take its minimum value around its boundary, i.e., when pi= 1,qi= 1,pi→0, or
qi→0. We examine each of these in turn.
We begin with the boundary where pi= 1. By Corollary 2, for 0< qi<1,
ε(1, qi) =1
2log1−qi
1
˜r−qi
. (143)
We take the partial derivative of this function with respect to qi.
∂ε(1, qi)
∂qi=1
2
−1
1−qi+1
1
˜r−qi
. (144)
Since 1/˜r <1, it follows that ∂ε(1, qi)/∂qi>0for all 0< qi<1. Thus, the minimum occurs as
qi→0and is
lim
qi→0ε(1, qi) =1
2log(˜r). (145)
We next examine with the boundary where qi= 1. When 0< pi<1,
ε(pi,1) = log1−pi
1
˜r−pi
. (146)
We take the partial derivative of this function with respect to pi.
∂ε(pi,1)
∂pi=−1
1−pi+1
1
˜r−pi. (147)
Since 1/˜r <1, it follows that ∂ε(pi,1)/∂pi>0for all 0< pi<1. Thus, the minimum occurs as
pi→0and is
lim
pi→0ε(pi,1) = log(˜ r). (148)
We next examine the boundary where pi→0. Since the logarithm is a continuous function, for all
0< qi<1,
lim
pi→0ε(pi, qi) = lim
pi→0log
2pi(1−qi)q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)
 (149)
= log
lim
pi→02pi(1−qi)q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)
. (150)
Using L’Hôpital’s Rule,
lim
pi→0ε(pi, qi) = log
lim
pi→02(1−qi)
−2(1−pi)+4(1−qi)(1
˜r−piqi)−4piqi(1−qi)
2q
(1−pi)2+4pi(1−qi)(1
˜r−piqi)+ 1
(151)
= log
2(1−qi)
−1+2(1−qi)(1
˜r)√
1+ 1
= log(˜ r). (152)
36Finally, we examine the boundary where qi→0. For all 0< pi≤1
lim
qi→0ε(pi, qi) = lim
qi→0log
2pi(1−qi)q
(1−pi)2+ 4pi(1−qi) 1
˜r−piqi
−(1−pi)
 (153)
= log
2piq
(1−pi)2+ 4pi 1
˜r
−(1−pi)
. (154)
We take the partial derivative with respect to pi. From (115) in the proof of Lemma 3, we have that
∂
∂pi
lim
qi→0ε(pi, qi)
=∂
∂pi
log
2piq
(1−pi)2+ 4pi 1
˜r−pi
−(1−pi)

 (155)
=2pi1
˜r q
(1−pi)2+4pi(1
˜r)−(1−pi)−1
piq
(1−pi)2+ 4pi 1
˜r. (156)
By (116) with qi= 0 , the ratio in the numerator of (156) is less than 1and so
∂/∂p i(limqi→0ε(pi, qi))is negative for all 0< pi≤1. Thus, the minimum occurs at pi= 1
and is as in (145). The global minimum of ε(pi, qi)is thus 1/2 log(˜ r).
37Theorem 4. Under the conditions of Theorem 2, let ˜a <1,˜p≤1, and ˜r >1, and 0< qi≤1. If the
function r∗is such that r∗(˜p, qi) = max {˜a/(˜pqi),˜r}andr∗(pi, qi) =∞ifpi̸= ˜p, then the solution
to the minimization problem in (13) is
ε=

log
˜a(1−˜p)
˜p(1−˜a)
, if0<˜p≤˜a
˜r;
log
2(˜p˜r−˜a)√
˜r2(1−˜p)2+4(˜p˜r−˜a)(1−˜a)−˜r(1−˜p)
,if˜a
˜r<˜p≤1.(157)
Proof. We define ε(qi)as follows.
ε(qi) =

log
2˜p(1−qi)r
(1−˜p)2+4˜p(1−qi)
1
r∗(˜p,qi)−˜pqi
−(1−˜p)
,if0< qi<1;
log
1−˜p
1
r∗(˜p,qi)−˜p
, ifqi= 1.(158)
The solution to the minimization problem in (13) corresponds to the minimum of ε(qi)over(0,1].
We begin with the case where 0<˜p≤˜a/˜r. In this setting, we have that ˜r≤˜a/˜p≤˜a/(˜pqi), so
r∗(˜p, qi) = ˜a/(˜pqi)for all qi. By Lemma 2, ε(qi)is a decreasing function of qi, so the minimum
occurs at qi= 1and is
ε(1) = log 
1−˜p
1
r∗(˜p,1)−˜p!
= log 
1−˜p
˜p
˜a−˜p!
= log˜a(1−˜p)
˜p(1−˜a)
. (159)
We now consider the case where ˜a/˜r <˜p≤1. Note that ˜r≥˜a/(˜pqi)whenever qi≥˜a/(˜p˜r). Since
in this setting ˜a/(˜p˜r)<1, the risk bound can be written in piece-wise form
r∗(˜p, qi) =(
˜a
˜pqi,if0< qi<˜a
˜p˜r;
˜r, if˜a
˜p˜r≤qi<1.(160)
Note that if qi≥1/(˜r˜p), then ˜r≥1/(˜pqi). Thus, r∗(˜p, qi)≥˜r≥1/(˜pqi)and so
s
(1−˜p)2+ 4˜p(1−qi)1
r∗(˜p, qi)−˜pqi
−(1−˜p)
≤p
(1−˜p)2+ 4˜p(1−qi) (˜pqi−˜pqi)−(1−˜p) (161)
=p
(1−˜p)2−(1−˜p) (162)
= 0. (163)
Thus, ε(qi) =∞for any qi≥1/(˜r˜p). We may thus restrict our attention to qi<min{1,1/(˜r˜p)}.
Since r∗(˜p, qi)is a continuous function of qiandε(qi)is a continuous function of r∗(˜p, qi), it
follows that ε(qi)is a continuous function of qi. By Lemma 2, ε(qi)is a decreasing function of
qifor0< qi<˜a/(˜p˜r)and by Lemma 3, ε(qi)is an increasing function of qifor˜a/(˜p˜r)< qi<
min{1,1/(˜r˜p)}. Thus, the minimum must occur at qi= ˜a/(˜p˜r)and is
ε˜a
˜p˜r
= log
2˜p
1−˜a
˜p˜r
r
(1−˜p)2+ 4˜p
1−˜a
˜p˜r
1
˜r−˜p˜a
˜p˜r
−(1−˜p)
(164)
= log 
2 (˜p˜r−˜a)p
˜r2(1−˜p)2+ 4 (˜p˜r−˜a) (1−˜a)−˜r(1−˜p)!
. (165)
38Theorem 5. Under the conditions of Theorem 2, let ˜a <1,˜q≤1, and ˜r >1, and 0< pi<1. If
the function r∗is such that r∗(pi,˜q) = max {˜a/(pi˜q),˜r}andr∗(pi, qi) =∞forqi̸= ˜q, then the
solution to the minimization problem in (13) is
ε=

1
2log
˜a(1−˜q)
˜q(1−˜a)
, if0<˜q≤˜a
˜r;
1
2log
1−˜q
1
˜r−˜q
, if0<˜q≤1
˜r+1and˜a
˜r<˜q <1;
log
2˜a(1−˜q)√
(˜r˜q−˜a)2+4˜a˜q(1−˜q)(1−˜a)−(˜r˜q−˜a)
,if1
˜r+1<˜q <1and˜a
˜r<˜q <1;
log
˜r−˜a
1−˜a
, if˜q= 1.(166)
Proof. We define ε(pi)as follows.
ε(pi) =

log
2pi(1−˜q)r
(1−pi)2+4pi(1−˜q)
1
r∗(pi,˜q)−pi˜q
−(1−pi)
,if0<˜q <1;
log
1−pi
1
r∗(pi,˜q)−pi
, if˜q= 1.(167)
The solution to the minimization problem in (13) corresponds to the minimum of ε(pi)over(0,1].
We begin with the case where 0<˜q≤˜a/˜r. In this setting, we have that ˜r≤˜a/˜q≤˜a/(pi˜q)so
r∗(pi,˜q) = ˜a/(pi˜q)for all pi. By Lemma 2, ε(pi)is a decreasing function of pi, so the minimum
occurs at pi= 1and is, by Corollary 2,
ε(1) =1
2log 
1−˜q
˜q
˜a−˜q!
=1
2log˜a(1−˜q)
˜q(1−˜a)
. (168)
We now consider the case where ˜a/˜r <˜q≤1. Note that ˜r≥˜a/(pi˜q)whenever pi≥˜a/(˜q˜r). Since
in this setting ˜a/(˜q˜r)<1, the risk bound can be written in piece-wise form
r∗(˜p, qi) =(
˜a
pi˜q,if0< pi<˜a
˜q˜r;
˜r, if˜a
˜q˜r≤pi<1.(169)
Note that if pi≥1/(˜r˜q), then ˜r≥1/(pi˜q). Thus, r∗(pi,˜q)≥˜r≥1/(pi˜q)and so
s
(1−pi)2+ 4pi(1−˜q)1
r∗(pi,˜q)−pi˜q
−(1−pi)
≤p
(1−pi)2+ 4pi(1−˜q) (pi˜q−pi˜q)−(1−pi) (170)
=p
(1−pi)2−(1−pi) = 0 . (171)
Thus, ε(pi) =∞for any pi≥1/(˜r˜q). We may thus restrict our attention to pi<min{1,1/(˜r˜q)}.
Since r∗(pi,˜q)is a continuous function of piandε(pi)is a continuous function of r∗(pi,˜q), it
follows that ε(pi)is a continuous function of pi. By Lemma 2, ε(pi)is a decreasing function of pi
for0< pi<˜a/(˜q˜r).
When 0<˜q <1/(˜r+1), note that 1/(˜q˜r)>(˜r+1)/˜r >1and so min{1,1/(˜r˜q)}= 1. By Lemma
3,ε(pi)is a decreasing function of pifor˜a/(˜q˜r)< pi<1. Thus, the minimum must occur at pi= 1
and is, by Corollary 2,
ε(1) =1
2log1−˜q
1
˜r−˜q
. (172)
When ˜q= 1/(˜r+ 1) , again 1/(˜q˜r) = (˜r+ 1)/˜r >1. By Lemma 3, ε(pi)is flat for ˜a/(˜q˜r)< pi<
min{1,1/(˜r˜q)}= 1. Thus, the minimum is shared by all points in this range and is equal to (172).
39When 1/(˜r+ 1) <˜q <1, by Lemma 3, ε(pi)is an increasing function of pifor˜a/(˜q˜r)< pi<
min{1,1/(˜r˜q)}. Thus, the minimum must occur at pi= ˜a/(˜q˜r)and is
ε˜a
˜q˜r
= log
2˜a
˜q˜r(1−˜q)
r
(1−˜a
˜q˜r)2+ 4˜a
˜q˜r(1−˜q)
1
˜r−˜a
˜q˜r˜q
−(1−˜a
˜q˜r)
(173)
= log 
2˜a(1−˜q)p
(˜q˜r−˜a)2+ 4˜a˜q(1−˜q) (1−˜a)−(˜q˜r−˜a)!
. (174)
Finally, when ˜q= 1for˜a/(˜q˜r)< pi<min{1,1/(˜r˜q)},
ε(pi) = log1−pi
1
˜r−pi
. (175)
It was shown in (147) that ε(pi)is an increasing function of pifor˜a/˜r < p i<min{1,1/˜r}. The
minimum then must occur at pi= ˜a/˜rand is
ε˜a
˜r
= log 
1−˜a
˜r
1
˜r−˜a
˜r!
= log˜r−˜a
1−˜a
. (176)
40NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our contribution is a novel, general framework for selecting εin DP, which
is stated in the abstract and introduction. We emphasize throughout that the framework
requires some assumptions, which are provided at the beginning of Section 3.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The main limitation of our framework is the required assumptions, especially
Assumption 2. We make an effort to call out that these assumptions are required for our
framework to be used, while also noting that they are milder than assumptions in the existing
literature on selecting ε.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
41Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All theorems and lemmas are proved in Appendix E and all assumptions are
clearly stated in Section 3.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All equations required to reproduce the tables and figures are provided in
the text. To find the εimplied by each profile, the minimization problem is stated in (13),
closed forms are provided in Appendix A.2 where applicable, and code to solve the problem
numerically is provided.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
42(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Code to produce all figures and tables in the text is provided.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: We present illustrations of the approach with results derived from mathematical
properties of the framework rather than data-based experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: Our numerical results derive from calculations based on probabilistic properties
of the framework, so error bars or other measures of statistical significance are unnecessary.
Guidelines:
43• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: While we do not have any experiments, the code to produce all figures and
tables can be run on a personal computer in a few seconds.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have reviewed and ensured that our paper meets the NeurIPS Code of
Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [No]
44Justification: We demonstrate how our framework can help agencies better manage risk and
utility trade-offs in private data releases, which can help agencies release information that
better informs policy decisions. However, we do not discuss societal impacts beyond that
notion.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks as it uses no confidential data.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
45•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
46•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
47