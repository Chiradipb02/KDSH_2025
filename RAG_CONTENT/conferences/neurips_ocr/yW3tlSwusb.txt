Accelerating ERM for data-driven algorithm design
using output-sensitive techniques
Maria-Florina Balcan∗Christopher Seiler†Dravyansh Sharma‡
Abstract
Data-driven algorithm design is a promising, learning-based approach for be-
yond worst-case analysis of algorithms with tunable parameters. An important
open problem is the design of computationally efficient data-driven algorithms
for combinatorial algorithm families with multiple parameters. As one fixes the
problem instance and varies the parameters, the “dual” loss function typically has
a piecewise-decomposable structure, i.e. is well-behaved except at certain sharp
transition boundaries. Motivated by prior empirical work, we initiate the study of
techniques to develop efficient ERM learning algorithms for data-driven algorithm
design by enumerating the pieces of the sum dual loss functions for a collection
of problem instances. The running time of our approach scales with the actual
number of pieces that appear as opposed to worst case upper bounds on the number
of pieces. Our approach involves two novel ingredients – an output-sensitive algo-
rithm for enumerating polytopes induced by a set of hyperplanes using tools from
computational geometry, and an execution graph which compactly represents all
the states the algorithm could attain for all possible parameter values. We illustrate
our techniques by giving algorithms for pricing problems, linkage-based clustering
and dynamic-programming based sequence alignment.
1 Introduction
The data-driven algorithm design paradigm captures a widely occuring scenario of solving multiple
related problem instances and allows the design and analysis of algorithms that use machine learning
to learn how to solve the instances which come from the same domain [ GR16 ,Bal20 ]. Typically
there are large (often infinite) parameterized algorithm families to choose from, and data-driven
algorithm design approaches provide techniques to select algorithm parameters that provably perform
well for instances from the same domain. Data-driven algorithms have been proposed and analyzed
for a variety of combinatorial problems, including clustering, computational biology and mechanism
design [ BDL20 ,BDD+21,BPS20 ]. But most of the prior work has focused on sample efficiency of
learning good algorithms i.e. the number of problem instances needed to learn algorithm parameters
that perform well on a typical problem from the domain. A major open question for this line of
research is to design computationally efficient learning algorithms [GR20, BDS21].
The parameterized family may occur naturally in well-known algorithms used in practice, or one
could potentially design new families interpolating known heuristics. For the problem of aligning
pairs of genomic sequences, one typically obtains the best alignment using a dynamic program
with some costs or weights assigned to edits of different kinds, such as insertions, substitutions,
or reduplications [ Wat89 ]. These costs are the natural parameters for the alignment algorithm.
The quality of the alignment can strongly depend on the parameters, and the best parameters vary
depending on the application (e.g. aligning DNAs or RNAs), the pair of species being compared, or
∗Carnegie Mellon University, ninamf@cs.cmu.edu .
†Work done by Christopher Seiler while he was at CMU.
‡Corresponding author: dravy@ttic.edu . Work done by Dravyansh Sharma while he was at CMU.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Problem Dim. TS(prior work) TS(ours) TERM(minstances)
Linkage-based d= 2 O(n18logn)[BDL20] O(Rn3) mTS+˜O(mn2RΣ)
clustering any d O (n8d+2logn)[BDL20] ˜O(R2n3) mTS+˜O(mn2R2
Σ)
DP-based sequ- d= 2 O(R2+RT DP)[GBN94] O(RT DP) mTS+˜O(mT DPRΣ)
ence alignment any d sO(sd)TDP[BDD+21] ˜O(˜R2L+1TDP)mTS+˜O(mT DPR2
Σ)
Two-part tariff ℓ= 1 O(K3)[BPS20] ˜O(R+K)O(RΣ+mKlogmK)
pricing any L′KO(L′)[BPS20] ˜O(R2K) mTS+˜O(mKR2
Σ)
Table 1: Summary of running times of the proposed algorithms. TERMdenotes the running time for
computing the pieces in the sum dual class function in terms of TS, the time for enumerating the
pieces on a single problem instance (Theorem 2.3). RΣ(resp. R) denotes the number of pieces in
the sum dual class function for given mproblem instances (resp. dual class function for a single
problem instance). nis the size of the clustering instance. sis the length of sequences to be aligned,
Lis the maximum number of subproblems in the sequence alignment DP update, ˜Ris the maximum
number of pieces in the dual class function over all subproblems, TDPis the time to solve the DP on
a single instance. Kis the number of units of the item sold and ℓis the menu length. The ˜Onotation
suppresses logarithmic terms and multiplicative terms that only depend on dorL.
the purpose of the alignment. Similarly, item prices are natural parameters in automated mechanism
design [ BSV18 ,BPS20 ]. On the other hand, for linkage-based clustering, one usually chooses from
a set of different available heuristics, such as single or complete linkage. Using an interpolation
of these heuristics to design a parameterized family and tune the parameter, one can often obtain
significant improvements in the quality of clustering [BNVW17, BDL20].
A common property satisfied by a large number of interesting parameterized algorithm families is
that the loss4as a function of the real-valued parameters for any fixed problem instance—called the
“dual class function” [ BS21 ]—is a piecewise structured function, i.e. the parameter space can be
partitioned into “pieces” via sharp transition boundaries such that the loss function is well-behaved
(e.g. constant or linear) within each piece [ Bal20 ]. Prior work on data-driven algorithm design has
largely focused on the sample complexity of the empirical risk minimization (ERM) algorithm which
finds the loss-minimizing value of the parameter over a collection of problem instances drawn from
some fixed unknown distribution. The ERM on a collection of problem instances can be implemented
by enumerating the pieces of the sum dual class loss function. We will design algorithms for
computationally efficient enumeration of these pieces, when the transition boundaries are linear,
and our techniques can be used to learn good domain-specific values of these parameters given
access to multiple problem instances from the problem domain. More precisely, we use techniques
from computational geometry to obtain “output-sensitive” algorithms (formalized in Appendix C),
that scale with the output-size RΣ(i.e. number of pieces in the partition) of the piece enumeration
problem for the sum dual class function on a collection of problem instances. Often, on commonly
occurring problem instances, the number of pieces is much smaller than worst case bounds on it
[BDL20, GS96] and our results imply significant gains in running time whenever RΣis small.
Our contributions. We design novel approaches that use tools from computational geometry
and lead to output-sensitive algorithms for learning good parameters by implementing the ERM
(Empirical Risk Minimization) for several distinct data-driven design problems. The resulting learning
algorithms scale polynomially with the number of sum dual class function pieces RΣin the worst
case (See Table 1) and are efficient for small constant d.
1.We present a novel output-sensitive algorithm for enumerating the cells induced by a collection
of hyperplanes. Our approach applies to any problem where the loss is piecewise linear, with
convex polytopic piece boundaries. We achieve output-polynomial time by removing redundant
constraints in any polytope using Clarkson’s algorithm (from computational geometry) [ Cla94 ]
and performing an implicit search over the graph of neighboring polytopes (formally Definition 2).
Our results are useful in obtaining output-sensitive running times for several distinct data-driven
algorithm design problems. Theorem 2.3 bounds the running time of the implementation of the
4or utitity, basically a function that measures the performance of any algorithm in the family on any problem
instance.
2ERM for piecewise-structured duals with linear boundaries (Definition 1) that scales with RΣ
provided we can enumerate the pieces of the dual class function for a single problem instance.
It is therefore sufficient to enumerate the pieces of dual function for a single instance, for which
Theorem 2.2 is useful.
2.We show how to learn multidimensional parameterized families of hierarchical clustering algo-
rithms5. Our approach for enumerating the pieces of the dual class function on a single instance
extends the execution tree based approach introduced for the much simpler single-parameter family
in [BDL20 ] to multiple dimensions. The key idea is to track the convex polytopic subdivisions
of the parameter space corresponding to a single merge step in the linkage procedure via nodes
of the execution tree and apply our output-sensitive cell-enumeration algorithm at each node.
3.For dynamic programming (DP) based sequence alignment, our approach for enumerating pieces
of the dual class function extends the execution tree to an execution directed acyclic graph (DAG).
For a small fixed d, our algorithm is efficient for small constant L, the maximum number of
subproblems needed to compute any single DP update. Prior work [ GBN94 ] only provides an
algorithm for computing the full partition for d= 2, and the naive approach of comparing all
pairs of alignments takes exponential time.
Our main contribution is to show that when the number of pieces ( RΣin Table 1) is small we can
improve over computational efficiency of prior work. Our techniques involves a novel application
of Clarkson’s algorithm (used to remove redundant inequalities from system of linear equations) to
derive output-sensitive enumeration, and execution graphs which capture problem-specific structure.
Motivation from prior empirical work. For the clustering problem, we give theoretical bounds
on the output-size Rdepending on the specific parametric family considered (e.g. Lemmas I.1,
I.2). These bounds imply a strict improvement even for worst-case R, but can be much faster for
typical R. Indeed, prior empirical work indicates Ris much smaller than the worst case bounds in
practice [ BDL20 ], where even for d= 1there is a dramatic speed up of over 1015times by being
output-sensitive. For the sequence alignment problem, again the upper bounds on R depend on the
nature of cost functions involved. For example, Theorem 2.2 of [ GBN94 ] gives an upper bound on
Rfor the 2-parameter problem with substitutions and deletions. For the TPT pricing problem we
prove new theoretical bounds on R(Theorem G.3) that show worst-case improvements (cubic to
quadratic) in the running time over prior work. In practice, Ris usually much smaller than the worst
case bounds, making our results even stronger. In prior experimental work on computational biology
data (sequence alignment with d= 2, sequence length ⁄tildelow100), [ GS96 ] observe that of a possible more
than 2200alignments, only R= 120 appear as possible optimal sequence alignments (over R2) for a
pair of immunoglobin sequences (a speed-up of over 58 orders of magnitude).
Preliminaries. We follow the notation of [ Bal20 ]. For a given algorithmic problem (say clustering,
or sequence alignment), let Πdenote the set of problem instances of interest. We also fix a (potentially
infinite) family of algorithms A, parameterized by a set P ⊆Rd. Here dis the number of real
parameters, and will also be the ‘fixed’ parameter in the FPT (Fixed Parameter Tractability) sense
(Appendix C). Let Aρdenote the algorithm in the family Aparameterized by ρ∈ P. The performance
of any algorithm on any problem instance is given by a utility (or loss) function u: Π× P → [0, H],
i.e.u(x, ρ)measures the performance on problem instance x∈Πof algorithm Aρ∈ A. The utility
of a fixed algorithm Aρfrom the family is given by uρ: Π→[0, H], with uρ(x) =u(x, ρ). We
are interested in the structure of the dual class of functions ux:P → [0, H], with ux(ρ) =uρ(x),
which measure the performance of all algorithms of the family for a fixed problem instance x∈Π.
We will use I{·}to denote the 0-1 valued indicator function. For many parameterized algorithms, the
dual class functions are piecewise structured in the following sense [BDD+21].
Definition 1 (Piecewise structured with linear boundaries) .A function class H ⊆RPthat maps a
domain P ⊆RdtoRis(F, t)-piecewise decomposable for a class F ⊆RPof piece functions if the
following holds: for every h∈ H, there are tlinear threshold functions g1, . . . , g t:P → { 0,1}, i.e.
gi(x) =I{aT
ix+bi}and a piece function fb∈ F for each bit vector b∈ {0,1}tsuch that for all
y∈ P,h(y) =fby(y)where by= (g1(y), . . . , g t(y))∈ {0,1}t.
5One should carefully distinguish the parameterized clustering algorithm from the ERM-based learning
algorithm which learns a parameter from this family given problem instances. Our algorithmic approaches focus
on implementing the ERM efficiently.
3We will refer to connected subsets of the parameter space where the dual class function is a fixed
piece function fcas the (dual) pieces orregions , when the dual class function is clear from the context.
Past work [ BSV18 ] defines a similar definition for mechanism classes called (d, t)-delineable classes
which are a special case of Definition 1, where the piece function class Fconsists of linear functions,
and focus on sample complexity of learning, i.e. the number of problem instances from the problem
distribution that are sufficient learn near-optimal parameters with high confidence over the draw of
the sample. Our techniques apply for a larger class, where Fis a class of convex functions. Past
work [ BSV18 ,BDL20 ,BDD+21] provides sample complexity guarantees for problems that satisfy
Definition 1, on the other hand we will focus on developing fast algorithms. We develop techniques
that yield efficient algorithms for computing these pieces in the multiparameter setting, i.e. constant
d >1.The running times of our algorithms are polynomial in the output size (i.e, number of pieces).
Fori∈Z+, we will use [i]to denote the set of positive integers {1, . . . , i}.
2 Output-sensitive cell enumeration and data-driven algorithm design
Suppose His a collection of thyperplanes in Rd. We consider the problem of enumerating the d-faces
(henceforth cells) of the convex polyhedral regions induced by the hyperplanes. The enumerated cells
will be represented as sign-patterns6of facet-inducing hyperplanes. We will present an approach for
enumerating these cells in OFPT time (Output-sensitive Fixed Parameter Tractable) Definition 3),
which involves two key ingredients: (a) locality-sensitivity, and (b) output-sensitivity. By locality
sensitivity, we mean that our algorithm exploits problem-specific local structure in the neighborhood
of each cell to work with a smaller candidate set of hyperplanes which can potentially constitute
the cell facets. This is abstracted out as a sub-routine COMPUTE LOCALLY RELEVANT SEPARATORS
which we will instantiate and analyse for each individual problem. In this section we will focus more
on the output-sensitivity aspect.
To provide an output-sensitive guarantee for this enumeration problem, we compute only the
non-redundant hyperplanes which provide the boundary of each cell cin the partition induced by
the hyperplanes. We denote the closed polytope bounding cell cbyPc. A crucial ingredient for
ensuring good output-sensitive runtime of our algorithm is Clarkson’s algorithm for computing
non-redundant constraints in a system of linear inequalities [ Cla94 ]. A constraint is redundant in
a system if removing it does not change the set of solutions. The key idea is to maintain a set Iof
non-redundant constraints detected so far, and solve LPs that detect the redundancy of a remaining
constraint (not in I) when added to I. If the constraint is redundant relative to I, it must also be
redundant in the full system, otherwise we can add a (potentially different) non-redundant constraint
toI(see Appendix D for details). The following runtime guarantee is known for the algorithm.
Theorem 2.1 (Clarkson’s algorithm) .Given a list Lofkhalf-space constraints in ddimensions,
Clarkson’s algorithm outputs the set I⊆Lof non-redundant constraints in Lin time O(k·LP(d,|I|+
1)), where LP(v, c)is the time for solving an LP with vvariables and cconstraints.
Algorithm 1 uses AUGMENTED CLARKSON , which modifies Clarkson’s algorithm with some ad-
ditional bookkeeping (details in Appendix D) to facilitate a search for neighboring regions in our
algorithm, while retaining the same asymptotic runtime complexity. Effectively, our algorithm can be
seen as a breadth-first search over an implicit underlying graph (Definition 2), where the neighbors
(and some auxiliary useful information) are computed dynamically by A UGMENTED CLARKSON .
Definition 2 (Cell adjacency graph) .Define the cell adjacency graph for a set Hof hyperplanes in
Rd, written GH= (VH, EH), as:
•There is a vertex v∈VHfor each cell in the partition ˜CofRdinduced by the hyperplanes;
•Forv, v′∈VH, add the edge {v, v′}toEHif the corresponding polytopes intersect, i.e. Pv∩Pv′̸=∅.
This generalizes to a subdivision (Definition 6) of a polytope in Rd.
This allows us to state the following guarantee about the runtime of Algorithm 1. In the notation of
Table 1, we have R=|VH|and|EH|=O(R2).
Theorem 2.2. LetHbe a set of thyperplanes in Rd. Suppose that |EH|=Eand|VH|=Vin the
cell adjacency graph GH= (VH, EH)ofH; then if the domain Pis bounded by |P| ≤thyperplanes,
6For simplicity we will denote these by vectors of the form {0,1,−1}twhere non-zero co-ordinates corre-
spond to signs of facet-inducing hyperplanes. Hash tables would be a practical data structure for implementation.
4Algorithm 1: OUTPUT SENSITIVE PARTITION SEARCH
1:Input : Set H={ai·x=bi}i∈[t]ofthyperplanes in Rd, convex polytopic domain P ⊆Rd
given by a set of hyperplanes P, procedure
COMPUTE LOCALLY RELEVANT SEPARATORS (x, H) with x∈Rd.
Output : Partition cells ˜C={˜c(j)},c(j)∈ {0,1,−1}twith|˜c(j)
i|= 1iffai·x=biis a
bounding hyperplane for cell j, and sign(˜c(j)
i) =sign(ai·xj−bi)for interior point xj,
sign(·)∈ {± 1}.
2:x1←an arbitrary point in Rd(assumed general position w.r.t. H)
3:Cellc(1)with sign(c(1)
i) =sign(ai·x1−bi)
4:q←empty queue; q.enqueue ([c(1),x1])
5:C← {} ;˜C← {}
6:while q.non_empty ()do
7: [c,x]←q.dequeue ()
8: Continue to next iteration if c∈C
9: C←C∪ {c}
10: ˜H←COMPUTE LOCALLY RELEVANT SEPARATORS (x, H); /* subset of
hyperplanes in Hthat can be facets for cell containing x*/
11: H′← {(−sign(ci)ai·x,−sign(ci)bi)|ai·x=bi∈˜H} ∪P
12: (˜c,neighbors )←AUGMENTED CLARKSON (x, H′, c); /* Algorithm 2 */
13: ˜C←˜C∪ {˜c}
14: q.enqueue ([c′,x′])for each [c′,x′]∈neighbors
15:return ˜C
Algorithm 1 computes the set VHin time ˜O(dE+V T CLRS+tLRS·P
c∈VHLP(d,|Ic|+ 1)) , where
LP(r,s)denotes the time to solve an LP in rvariables and sconstraints, Icdenotes the number of
facets for cell c∈VH,TCLRS denotes the running time of COMPUTE LOCALLY RELEVANT SEPARA -
TORS andtLRSdenotes an upper bound on the number of locally relevant hyperplanes in Line 10.
We illustrate the significance of output-sensitivity and locality-sensitivity in our results with examples.
Example 1. The worst-case size of VHisO(td)and standard (output-insensitive) enumeration
algorithms for computing VH(e.g. [ EG86 ,Xu20 ]) take O(td)time even when the output size may be
much smaller. For example, if His a collection of parallel planes in R3, the running time of these
approaches is O(t3). Even a naive implementation of COMPUTE LOCALLY RELEVANT SEPARATORS
which always outputs the complete set Hgives a better runtime of O(t2). By employing a
straightforward algorithm which binary searches the closest hyperplanes to x(in a pre-processed
H) asCOMPUTE LOCALLY RELEVANT SEPARATORS we have tLRS=O(1)andTCLRS=˜O(1), and
Algorithm 1 attains a running time of ˜O(t). Analogously, if His a collection of thyperplanes in Rd
with1≤k < d distinct unit normal vectors, then output-sensitivity improves the runtime from O(td)
toO(tk+1), and locality-sensitivity can be used to further improve it to ˜O(tk).
ERM in the statistical learning setting. We now use Algorithm 1 to compute the sample mimimum
(aka ERM, Empirical Risk Minimization) for the (F, t)piecewise-structured dual losses with linear
boundaries (Definition 1) over a problem sample S∈Πm, provided piece functions in Fcan be
efficiently optimized over a polytope (typically the piece functions are constant or linear functions
in our examples). Formally, we define search-ERM for a given parameterized algorithm family A
with parameter space Pand the dual class utility function being (F, t)-piecewise decomposable
(Definition 1) as follows: given a set of mproblem instances S∈Πm, compute the pieces, i.e. a
partition of the parameter space into connected subsets such that the utility function is a fixed piece
function in Fover each subset for each of the mproblem instances. The following result gives a
recipe for efficiently solving the search-ERM problem provided we can efficiently compute the dual
function pieces in individual problem instances, and the the number of pieces in the sum dual class
function over the sample Sis not too large. The key idea is to apply Algorithm 1 for each problem
instance, and once again for the search-ERM problem. In the notation of Table 1, we have RΣ=|VS|,
5the number of vertices in the cell adjacency graph corresponding to the polytopic pieces in the sum
utility functionPm
i=1ui.|ES|=O(RΣ)when d= 2andO(R2
Σ)for general d.
Theorem 2.3. Let˜Cidenote the cells partitioning the polytopic parameter space P ⊂Rdcorre-
sponding to pieces of the dual class utility function uion a single problem instance xi∈Π, from
a collection s={x1, . . . , x m}ofmproblem instances. Let (VS, ES)be the cell adjacency graph
corresponding to the polytopic pieces in the sum utility functionPm
i=1ui. Then there is an algorithm
for computing VSgiven the cells ˜Ciin time ˜O((d+m)|ES|+mtLRS·P
c∈VSLP(d,|Ic|+1)) , where
Icdenotes the number of facets for cell c∈VS, andtLRSis the number of locally relevant hyperplanes
in a single instance.
An important consequence of the above result is an efficient output-sensitive algorithm for data-driven
algorithm design when the dual class utility function is (F, t)-piecewise decomposable. In the
following sections, we will instantiate the above results for various data-driven parameter selection
problems where the dual class functions are piecewise-structured with linear boundaries (Definition 1).
Prior work [ BSV18 ,BDL20 ,BDD+21] has shown polynomial bounds on the sample complexity of
learning near-optimal parameters via the ERM algorithm for these problems in the statistical learning
setting, i.e. the problem instances are drawn from a fixed unknown distribution. In other words, ERM
over polynomially large sample size mis sufficient for learning good parameters. In particular, we
will design and analyze running time for problem-specific algorithms for computing locally relevant
hyperplanes. Given Theorem 2.3, it will be sufficient to give an algorithm for computing the pieces
of the dual class function for a single problem instance.
Remark 1. Our results in this section directly imply efficient algorithms for pricing problems
in mechanism design which are known to be (F, t)-decomposable where Fis the class of linear
functions (summarized in Table 1, see Appendix F for details).
3 Linkage-based clustering
Clustering data into groups of similar points is a fundamental tool in data analysis and unsupervised
machine learning. A variety of clustering algorithms have been introduced and studied but it is not
clear which algorithms will work best on specific tasks. Also the quality of clustering is heavily
dependent on the distance metric used to compare data points. Interpolating multiple metrics and
clustering heuristics can result in significantly better clustering [BDL20].
Problem setup. LetXbe the data domain. A clustering instance from the domain consists of a point
setS={x1, . . . , x n} ⊆ X and an (unknown) target clustering C= (C1, . . . , C k), where the sets
C1, . . . , C kpartition Sintokclusters. Linkage-based clustering algorithms output a hierarchical
clustering of the input data, represented by a cluster tree. We measure the agreement of a cluster tree
Twith the target clustering Cin terms of the Hamming distance between Cand the closest pruning
ofTthat partitions it into kclusters (i.e., kdisjoint subtrees that contain all the leaves of T). More
formally, the loss ℓ(T,C) = min P1,...,P kminσ∈Sn1
|S|Pk
i=1|Ci\Pσi|, where the first minimum is
over all prunings P1, . . . , P kof the cluster tree Tintoksubtrees, and the second minimum is over all
permutations of the kcluster indices.
A merge function Ddefines the distance between a pair of clusters Ci, Cj⊆ X in terms of the
pairwise point distances given by a metric d. Cluster pairs with smallest values of the merge
function are merged first. For example, single linkage uses the merge function Dsgl(Ci, Cj;d) =
mina∈Ci,b∈Cjd(a, b)and complete linkage uses Dcmpl(Ci, Cj;d) = max a∈Ci,b∈Cjd(a, b). Instead
of using extreme points to measure the distance between pairs of clusters, one may also use more cen-
tral points, e.g. we define median linkage asDmed(Ci, Cj;d) =median ({d(a, b)|a∈Ci, b∈Cj}),
where median (·)is the usual statistical median of an ordered set S⊂R7. Appendix H provides
synthetic clustering instances where one of single, complete or median linkage leads to significantly
better clustering than the other two, illustrating the need to learn an interpolated procedure. Single,
7median (S)is the smallest element of Ssuch that Shas at most half its elements less than
median (S)and at most half its elements more than median (S). For comparison, the more well-
known average linkage is Davg(Ci, Cj;d) = mean({d(a, b)|a∈Ci, b∈Cj}). We may also use
geometric medians of clusters. For example, we can define mediod linkage asDgeomed(Ci, Cj;d) =
d(argmina∈CiP
a′∈Cid(a, a′),argminb∈CjP
b′∈Cjd(b, b′)).
6median and complete linkage are 2-point-based (Definition 4, Appendix I), i.e. the merge function
D(A, B;d)only depends on the distance d(a, b)for two points (a, b)∈A×B.
Parameterized algorithm families. Let∆ ={D1, . . . , D l}denote a finite family of merge func-
tions (measure distances between clusters) and δ={d1, . . . , d m}be a finite collection of distance
metrics (measure distances between points). We define a parameterized family of linkage-based
clustering algorithms that allows us to learn both the merge function and the distance metric. It
is given by the interpolated merge function D∆
α(A, B;δ) =P
Di∈∆,dj∈δαi,jDi(A, B;dj), where
α={αi,j|i∈[l], j∈[m], αi,j≥0}. In order to ensure linear boundary functions for the dual
class function, our interpolated merge function D∆
α(A, B;δ)takes all pairs of distance metrics and
linkage procedures. Due to invariance under constant multiplicative factors, we can setP
i,jαi,j= 1
and obtain a set of parameters which allows αto be parameterized by d=lm−1values8. Define
the parameter space P=▲d=n
ρ∈ 
R≥0d|P
iρi≤1o
; for any ρ∈ P we get α(ρ)∈Rd+1
asα(ρ)
i=ρifori∈[d],α(ρ)
d+1= 1−P
i∈[d]ρi. We focus on learning the optimal ρ∈ P for a single
instance (S,Y). With slight abuse of notation we will sometimes use D∆
ρ(A, B;δ)to denote the
interpolated merge function D∆
α(ρ)(A, B;δ). As a special case we have the family D∆
ρ(A, B;d0)
that interpolates merge functions (from set ∆) for different linkage procedures but the same distance
metric d0. Another interesting family only interpolates the distance metrics, i.e. use a distance metric
dρ(a, b) =P
dj∈δα(ρ)
jdj(a, b)and use a fixed linkage procedure. We denote this by D1
ρ(A, B;δ).
We will extend the execution tree approach introduced by [ BDL20 ] which computes the pieces
(intervals) of single-parameter linkage-based clustering. A formal treatment of the execution tree,
and how it is extended to the multi-parameter setting, is deferred to Appendix I.1. Informally, for
a single parameter, the execution tree is defined as the partition tree where each node represents an
interval where the first tmerges are identical, and edges correspond to the subset relationship between
intervals obtained by refinement from a single merge. The execution, i.e. the sequence of merges,
is unique along any path of this tree. The same properties, i.e. refinement of the partition with each
merge and correspondence to the algorithm’s execution, continue to hold in the multidimensional
case, but with convex polytopes instead of intervals (see Definition 5). Computing the children of
any node of the execution tree corresponds to computing the subdivision of a convex polytope into
polytopic cells where the next merge step is fixed. The children of any node of the execution tree
can be computed using Algorithm 1. We compute the cells by following the neighbors, keeping track
of the cluster pairs merged for the computed cells to avoid recomputation. For any single cell, we
find the bounding polytope along with cluster pairs corresponding to neighboring cells by computing
the tight constraints in a system of linear inequalities. Theorem 2.2 gives the runtime complexity
of the proposed algorithm for computing the children of any node of the execution tree. It only
remains to specify COMPUTE LOCALLY RELEVANT SEPARATORS . For a given x=ρwe find the
next merge candidate in time O(dn2)by computing the merge function D∆
ρ(A, B;δ)for all pairs of
candidate (unmerged) clusters A, B . If(A∗, B∗)minimizes the merge function, the locally relevant
hyperplanes are given by D∆
ρ(A∗, B∗;δ)≤D∆
ρ(A′, B′;δ)for(A′, B′)̸= (A∗, B∗)i.e.tLRS≤n2.
Using Theorem 2.2, we give the following bound for the overall runtime of the algorithm (soft-O
notation suppresses logarithmic terms and multiplicative constants in d, proof in Appendix I.2).
Theorem 3.1. LetSbe a clustering instance with |S|=n, and let Ri=|Pi|andR=Rn. Let
Ht={(Q1,Q2)∈ P2
t| Q1∩ Q2̸=∅}denote the total number of adjacencies between any two
pieces of PiandH=Hn. Then, the leaves of the execution tree on Scan be computed in time
˜O Pn
i=1(Hi+RiTM) (n−i+ 1)2
, where TMis the time to compute the merge function.
In the case of single, median, and complete linkage, we may assume TM=O(d)by carefully
maintaining a hashtable containing distances between every pair of clusters. Each merge requires
overhead at most O(n2
t),nt=n−tbeing the number of unmerged clusters at the node at depth t,
which is absorbed by the cost of solving the LP corresponding to the cell of the merge. We have the
following corollary which states that our algorithm is output-linear for d= 2.
Corollary 3.2. Ford= 2the leaves of the execution tree of any clustering instance Swith|S|=n
can be computed in time O(RTMn3).
Above results yield bounds on TS, the enumeration time for dual function of the pieces in a single
problem instance. Theorem 2.3 further implies bounds on the runtime of ERM (Table 1).
8In contrast, the parametric family in [ BDL20 ] hasl+m−2parameters but it does not satisfy Definition 1.
74 Dynamic Programming based sequence alignment
Sequence alignment is a fundamental combinatorial problem with applications to computational
biology. For example, to compare two DNA, RNA or amino acid sequences the standard
approach is to align two sequences to detect similar regions and compute the optimal alignment
[NW70 ,Wat89 ,CB00 ]. However, the optimal alignment depends on the relative costs or weights
used for specific substitutions, insertions/deletions, or gaps (consecutive deletions) in the sequences.
Given a set of weights, the optimal alignment computation is typically a simple dynamic program.
Our goal is to learn the weights, such that the alignment produced by the dynamic program has
application-specific desirable properties.
Problem setup. Given a pair of sequences s1, s2over some alphabet Σof lengths m=|s1|
andn=|s2|, and a ‘space’ character −/∈Σ, a space-extension tof a sequence soverΣis a
sequence over Σ∪ {−} such that removing all occurrences of −intgives s. A global alignment
(or simply alignment) of s1, s2is a pair of sequences t1, t2such that |t1|=|t2|,t1, t2are space-
extensions of s1, s2respectively, and for no 1≤i≤ |t1|we have t1[i] =t2[i] =−. Let s[i]
denote the i-th character of a sequence sands[:i]denote the first icharacters of sequence s.
For1≤i≤ |t1|, ift1[i] =t2[i]we call it a match . Ift1[i]̸=t2[i], and one of t1[i]ort2[i]
is the character −we call it a space , else it is a mismatch . A sequence of −characters (in t1
ort2) is called a gap. Matches, mismatches, gaps and spaces are commonly used features of an
alignment, i.e. functions that map sequences and their alignments (s1, s2, t1, t2)toZ≥0(for example,
the number of spaces). A common measure of cost of an alignment is some linear combination
of features. For example if there are dfeatures given by lk(·),k∈[d], the cost may be given
byc(s1, s2, t1, t2, ρ) =Pd
k=1ρklk(s1, s2, t1, t2)where ρ= (ρ1, . . . , ρ d)are the parameters that
govern the relative weight of the features [ KK06 ]. Let τ(s, s′, ρ) = argmint1,t2c(s, s′, t1, t2, ρ)and
C(s, s′, ρ) = min t1,t2c(s, s′, t1, t2, ρ)denote the optimal alignment and its cost respectively.
A general DP update rule. For a fixed ρ, suppose the sequence alignment problem can be
solved, i.e. we can find the alignment with the smallest cost, using a dynamic program Aρwith
linear parameter dependence (described below). Our main application will be to the family of
dynamic programs Aρwhich compute the optimal alignment τ(s1, s2, ρ)given any pair of sequences
(s1, s2)∈Σm×Σn= Π for any ρ∈Rd, but we will proceed to provide a more general abstraction.
See Section J.1 for example DPs using well-known features in computational biology, expressed using
the abstraction below. For any problem (s1, s2)∈Π, the dynamic program Aρ(ρ∈ P ⊆ Rd, the
set of parameters) solves a set π(s1, s2) ={Pi|i∈[k], Pk= (s1, s2)}ofksubproblems (typically,
π(s1, s2)⊆Πs1,s2={(s1[:i′], s2[:j′])|i′∈ {0, . . . , m }, j′∈ {0, . . . , n }} ⊆ Π) in some fixed
order P1, . . . , P k= (s1, s2). Crucially, the subproblems sequence P1, . . . , P kdo not depend on
ρ9. In particular, a problem Pjcan be efficiently solved given optimal alignments and their costs
(τi(ρ), Ci(ρ))for problems Pifor each i∈[j−1]. Some initial problems in the sequence P1, . . . , P k
of subproblems are base case subproblems where the optimal alignment and its cost can be directly
computed without referring to a previous subproblem. To solve a (non base case) subproblem Pj,
we consider Valternative cases q: Π→[V], i.e.Pjbelongs to exactly one of the Vcases (e.g. if
Pj= (s1[:i′], s2[:j′]), we could have two cases corresponding to s1[i′] =s2[j′]ands1[i′]̸=s2[j′]).
Typically, Vwill be a small constant. For any case v=q(Pj)∈[V]thatPjmay belong to, the
cost of the optimal alignment of Pjis given by a minimum over Lvterms of the form cv,l(ρ, Pj) =
ρ·wv,l+σv,l(ρ, Pj), where l∈[Lv],wv,l∈Rd,σv,l(ρ, Pj) =Ct(ρ)∈ {C1(ρ), . . . , C j−1(ρ)}is
the cost of some previously solved subproblem Pt= (s1[:i′
t], s2[:j′
t]) = ( s1[:i′
v,l,j], s2[:j′
v,l,j])(i.e.
tdepends on v, l, j but not on ρ), and cv,l(ρ, Pj)is the cost of alignment τv,l(ρ, Pj) =Tv,l(τt(ρ))
which extends the optimal alignment for subproblem Ptby aρ-independent transformation Tv,l(·).
That is, the DP update for computing the cost of the optimal alignment takes the form
DP(ρ, Pj) = min
l{ρ·wq(Pj),l+σq(Pj),l(ρ, Pj)}, (1)
and the optimal alignment is given by DP′(ρ, Pj) = τq(Pj),l∗(ρ, Pj),where l∗=
argminl{ρ·wq(Pj),l+σq(Pj),l(ρ, Pj)}. The DP specification is completed by including base cases
{C(s, s′, ρ) =ρ·ws,s′|(s, s′)∈ B(s1, s2)}(or{τ(s, s′, ρ) =τs,s′|(s, s′)∈ B(s1, s2)}for the
9For the sequence alignment DP in Appendix J.1.1, we have π(s1, s2) = Π s1,s2and we first solve the base
case subproblems (which have a fixed optimal alignment for all ρ) followed by problems (s1[:i′], s2[:j′])in a
non-decreasing order of i′+j′for any value of ρ.
8optimal alignment DP) corresponding to a set of base case subproblems B(s1, s2)⊆Πs1,s2. Let
L= max v∈[V]Lvdenote the maximum number of subproblems needed to compute a single DP
update in any of the cases. Lis often small, typically 2 or 3 (see examples in Section J.1). Our
main result is to provide an algorithm for computing the polytopic pieces of the dual class functions
efficiently for small constants dandL.
As indicated above, we consider the family of dynamic programs Aρwhich compute the optimal
alignment τ(s1, s2, ρ)given any pair of sequences (s1, s2)∈Σm×Σn= Π for any ρ∈Rd. For any
alignment (t1, t2), the algorithm has a fixed real-valued utility (different from the cost function above)
which captures the quality of the alignment, i.e. the utility function u((s1, s2), ρ)only depends on the
alignment τ(s1, s2, ρ). The dual class function is piecewise constant with convex polytopic pieces
(Lemma J.5 in Appendix J.3). For any fixed problem (s1, s2), the space of parameters ρcan be parti-
tioned into Rconvex polytopic regions where the optimal alignment is fixed. The optimal parameter
can then be found by simply comparing the costs of the alignments in each of these pieces. For the
rest of this section we consider the algorithmic problem of computing these Rpieces efficiently.
For the clustering algorithm family, as we have seen in Section 3, we get a refinement of the parameter
space with each new step (merge) performed by the algorithm. This does not hold for the sequence
alignment problem. Instead we obtain the following DAG, from which the desired pieces can be
obtained by looking at nodes with no out-edges (call these terminal nodes). Intuitively, the DAG
is built by iteratively adding nodes corresponding to subproblems P1, . . . , P kand adding edges
directed towards Pjfrom all subproblems that appear in the DP update for it. That is, for base case
subproblems, we have singleton nodes with no incoming edges. Using the recurrence relation (1), we
note that the optimal alignment for the pair of sequences (s1[:i], s2[:j])can be obtained from the
optimal alignments for subproblems {(s1[:iv′,l], s2[:jv′,l])}l∈[Lv′]where v′=q(s1[:i], s2[:j]). The
DAG for (s1[:i], s2[:j])is therefore simply obtained by using the DAGs Gv′,lfor the subproblems
and adding directed edges from the terminal nodes of Gv′,lto new nodes vp,i,jcorresponding to each
piece pof the partition P[i][j]ofPgiven by the set of pieces of u(s1[:i],s2[:j])(ρ). A more compact
representation of the execution graph would have only a single node vi,jfor each subproblem
(s1[:i], s2[:j])(the node stores the corresponding partition P[i][j]) and edges directed towards vi,j
from nodes of subproblems used to solve (s1[:i], s2[:j]). Note that the graph depends on the problem
instance (s1, s2)as the relevant DP cases v′=q(s1[:i], s2[:j])depend on the sequences s1, s2. A
naive way to encode the execution graph would be an exponentially large tree corresponding to the
recursion tree of the recurrence relation (1).
Execution DAG. Formally we define a compact execution graph Ge= (Ve, Ee)as follows. For the
base cases, we have nodes labeled by (s, s′)∈ B(s1, s2)storing the base case solutions (ws,s′, τs,s′)
over the unpartitioned parameter space P=Rd. For i, j > 0, we have a node vi,jlabeled by
(s1[:i], s2[:j])and the corresponding partition P[i][j]of the parameter space, with incoming edges
from nodes of the relevant subproblems {(s1[:iv′,l], s2[:jv′,l])}l∈[Lv′]where v′=q(s1[:i], s2[:j]).
This graph is a DAG since every directed edge is from some node vi,jto a node vi′,j′withi′+j′> i+j
in typical sequence alignment dynamic programs (Appendix J.1). Algorithm 5 (Appendix J.2)) gives a
procedure to compute the partition of the parameter space for any given problem instance (s1, s2)us-
ing the compact execution DAG. We give intuitive overviews of the three main routines in Algorithm 5.
•COMPUTE OVERLAY DPcomputes an overlay Piof the input polytopic subdivisions {Ps|s∈Si}
and uses Clarkson’s algorithm for intersecting polytopes with output-sensitive efficiency. We show
that the overlay can be computed by solving at most ˜RLlinear programs (Algorithm 6, Lemma J.1).
•COMPUTE SUBDIVISION DPapplies Algorithm 1, in each piece of the overlay we need to find the
polytopic subdivision induced by O(L2)hyperplanes (the set of hyperplanes depends on the piece).
This works because all relevant subproblems have the same solution within any piece of the overlay.
•Finally RESOLVE DEGENERACIES DPmerges pieces where the optimal alignment is identical
using a simple search over the resulting subdivision.
For our implementation of the subroutines, we have the following guarantee for Algorithm 5.
Theorem 4.1. LetRi,jdenote the number of pieces in P[i][j], and ˜R= max i≤m,j≤nRi,j. If the time
complexity for computing the optimal alignment is O(TDP), then Algorithm 5 can be used to compute
the pieces for the dual class function for any problem instance (s1, s2), in time O(d!L4d˜R2L+1TDP).
For the special case of d= 2, we show that (Theorem J.6, Appendix J.3) the pieces may be computed
inO(RT DP)time using the ray search technique of [Meg78].
95 Acknowledgments
We thank Dan DeBlasio for useful discussions on the computational biology literature. We also thank
Avrim Blum and Mikhail Khodak for helpful feedback. This material is based on work supported
by the National Science Foundation under grants CCF-1910321, IIS-1901403, and SES-1919453;
the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003;
a Simons Investigator Award; an AWS Machine Learning Research Award; an Amazon Research
Award; a Bloomberg Research Grant; a Microsoft Research Faculty Fellowship.
References
[AF96] David Avis and Komei Fukuda. Reverse search for enumeration. Discrete applied
mathematics , 65(1-3):21–46, 1996.
[Bal20] Maria-Florina Balcan. Data-Driven Algorithm Design. In Tim Roughgarden, editor,
Beyond Worst Case Analysis of Algorithms . Cambridge University Press, 2020.
[BB24] Maria-Florina Balcan and Hedyeh Beyhaghi. Learning revenue maximizing menus of
lotteries and two-part tariffs. Transactions on Machine Learning Research (TMLR) ,
2024.
[BBSZ23] Maria-Florina Balcan, Avrim Blum, Dravyansh Sharma, and Hongyang Zhang. An
analysis of robustness of non-lipschitz networks. Journal of Machine Learning Research
(JMLR) , 24(98):1–43, 2023.
[BDD+21]Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm,
and Ellen Vitercik. How much data is sufficient to learn high-performing algorithms?
Generalization guarantees for data-driven algorithm design. In Symposium on Theory of
Computing (STOC) , pages 919–932, 2021.
[BDL20] Maria-Florina Balcan, Travis Dick, and Manuel Lang. Learning to link. In International
Conference on Learning Representations (ICLR) , 2020.
[BDS20] Maria-Florina Balcan, Travis Dick, and Dravyansh Sharma. Learning piecewise Lip-
schitz functions in changing environments. In International Conference on Artificial
Intelligence and Statistics , pages 3567–3577. PMLR, 2020.
[BDS21] Avrim Blum, Chen Dan, and Saeed Seddighin. Learning complexity of simulated an-
nealing. In International Conference on Artificial Intelligence and Statistics (AISTATS) ,
pages 1540–1548. PMLR, 2021.
[BDSV18] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning
to branch. In International Conference on Machine Learning (ICML) , pages 344–353.
PMLR, 2018.
[BDV18] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven
algorithm design, online learning, and private optimization. In 2018 IEEE 59th Annual
Symposium on Foundations of Computer Science (FOCS) , pages 603–614. IEEE, 2018.
[BFM97] David Bremner, Komei Fukuda, and Ambros Marzetta. Primal-dual methods for vertex
and facet enumeration (preliminary version). In Symposium on Computational Geometry
(SoCG) , pages 49–56, 1997.
[BIW22] Peter Bartlett, Piotr Indyk, and Tal Wagner. Generalization bounds for data-driven
numerical linear algebra. In Conference on Learning Theory (COLT) , pages 2013–2040.
PMLR, 2022.
[BKST22] Maria-Florina Balcan, Mikhail Khodak, Dravyansh Sharma, and Ameet Talwalkar.
Provably tuning the ElasticNet across instances. Advances in Neural Information
Processing Systems , 35:27769–27782, 2022.
10[BNVW17] Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-
theoretic foundations of algorithm configuration for combinatorial partitioning problems.
InConference on Learning Theory (COLT) , pages 213–274. PMLR, 2017.
[BPS20] Maria-Florina Balcan, Siddharth Prasad, and Tuomas Sandholm. Efficient algorithms
for learning revenue-maximizing two-part tariffs. In International Joint Conferences on
Artificial Intelligence (IJCAI) , pages 332–338, 2020.
[BS21] Maria-Florina Balcan and Dravyansh Sharma. Data driven semi-supervised learning.
Advances in Neural Information Processing Systems , 34, 2021.
[BSV16] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. Sample complexity of
automated mechanism design. Advances in Neural Information Processing Systems , 29,
2016.
[BSV18] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. A general theory of
sample complexity for multi-item profit maximization. In Economics and Computation
(EC) , pages 173–174, 2018.
[Buc43] Robert Creighton Buck. Partition of space. The American Mathematical Monthly ,
50(9):541–544, 1943.
[CAK17] Vincent Cohen-Addad and Varun Kanade. Online optimization of smoothed piecewise
constant functions. In Artificial Intelligence and Statistics , pages 412–420. PMLR,
2017.
[CB00] Peter Clote and Rolf Backofen. Computational Molecular Biology: An Introduction .
John Wiley Chichester; New York, 2000.
[CE92] Bernard Chazelle and Herbert Edelsbrunner. An optimal algorithm for intersecting line
segments in the plane. Journal of the ACM (JACM) , 39(1):1–54, 1992.
[Cha96] Timothy M Chan. Optimal output-sensitive convex hull algorithms in two and three
dimensions. Discrete & Computational Geometry , 16(4):361–368, 1996.
[Cha18] Timothy M Chan. Improved deterministic algorithms for linear programming in low
dimensions. ACM Transactions on Algorithms (TALG) , 14(3):1–10, 2018.
[Cla94] Kenneth L Clarkson. More output-sensitive geometric algorithms. In Symposium on
Foundations of Computer Science (FOCS) , pages 695–702. IEEE, 1994.
[DF12] Rodney G Downey and Michael Ralph Fellows. Parameterized complexity . Springer
Science & Business Media, 2012.
[EG86] Herbert Edelsbrunner and Leonidas J Guibas. Topologically sweeping an arrangement.
InSymposium on Theory of Computing (STOC) , pages 389–403, 1986.
[Fer02] Henning Fernau. On parameterized enumeration. In International Computing and
Combinatorics Conference , pages 564–573. Springer, 2002.
[FGS+19]Henning Fernau, Petr Golovach, Marie-France Sagot, et al. Algorithmic enumera-
tion: Output-sensitive, input-sensitive, parameterized, approximative (Dagstuhl Seminar
18421). In Dagstuhl Reports , volume 8. Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-
matik, 2019.
[GBN94] Dan Gusfield, Krishnan Balasubramanian, and Dalit Naor. Parametric optimization of
sequence alignment. Algorithmica , 12(4):312–326, 1994.
[GR16] Rishi Gupta and Tim Roughgarden. A PAC approach to application-specific algorithm
selection. In Innovations in Theoretical Computer Science Conference (ITCS) , 2016.
[GR20] Rishi Gupta and Tim Roughgarden. Data-driven algorithm design. Communications of
the ACM , 63(6):87–94, 2020.
11[GS96] Dan Gusfield and Paul Stelling. Parametric and inverse-parametric sequence alignment
with XPARAL. Methods in Enzymology , 266:481–494, 1996.
[KK06] John Kececioglu and Eagu Kim. Simple and fast inverse alignment. In Annual Interna-
tional Conference on Research in Computational Molecular Biology , pages 441–455.
Springer, 2006.
[KKW10] John Kececioglu, Eagu Kim, and Travis Wheeler. Aligning protein sequences with
predicted secondary structure. Journal of Computational Biology , 17(3):561–580, 2010.
[KPT+21]Krishnan Kumaran, Dimitri J Papageorgiou, Martin Takac, Laurens Lueg, and Nicolas V
Sahinidis. Active metric learning for supervised classification. Computers & Chemical
Engineering , 144:107132, 2021.
[Lew41] W Arthur Lewis. The two-part tariff. Economica , 8(31):249–270, 1941.
[Meg78] Nimrod Megiddo. Combinatorial optimization with rational objective functions. In
Symposium on Theory of Computing (STOC) , pages 1–12, 1978.
[MR15] Jamie H Morgenstern and Tim Roughgarden. On the pseudo-dimension of nearly
optimal auctions. Advances in Neural Information Processing Systems , 28, 2015.
[Nak04] Shin-ichi Nakano. Efficient generation of triconnected plane triangulations. Computa-
tional Geometry , 27(2):109–122, 2004.
[NW70] Saul B Needleman and Christian D Wunsch. A general method applicable to the search
for similarities in the amino acid sequence of two proteins. Journal of Molecular
Biology , 48(3):443–453, 1970.
[Oi71] Walter Y Oi. A Disneyland dilemma: Two-part tariffs for a Mickey Mouse monopoly.
The Quarterly Journal of Economics , 85(1):77–96, 1971.
[RC18] Miroslav Rada and Michal Cerny. A new algorithm for enumeration of cells of hyper-
plane arrangements and a comparison with Avis and Fukuda’s reverse search. SIAM
Journal on Discrete Mathematics , 32(1):455–473, 2018.
[SC11] Shiliang Sun and Qiaona Chen. Hierarchical distance metric learning for large mar-
gin nearest neighbor classification. International Journal of Pattern Recognition and
Artificial Intelligence (IJPRAI) , 25:1073–1087, 11 2011.
[Sei91] Raimund Seidel. Small-dimensional linear programming and convex hulls made easy.
Discrete & Computational Geometry , 6:423–434, 1991.
[SJ23] Dravyansh Sharma and Maxwell Jones. Efficiently learning the graph for semi-
supervised learning. The Conference on Uncertainty in Artificial Intelligence (UAI) ,
2023.
[Sle99] Nora H Sleumer. Output-sensitive cell enumeration in hyperplane arrangements. Nordic
Journal of Computing , 6(2):137–147, 1999.
[Syr17] Vasilis Syrgkanis. A sample complexity measure with applications to learning optimal
auctions. Advances in Neural Information Processing Systems , 30, 2017.
[Wat89] Michael S Waterman. Mathematical methods for DNA sequences . Boca Raton, FL
(USA); CRC Press Inc., 1989.
[WS09] Kilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin
nearest neighbor classification. Journal of Machine Learning Research (JMLR) , 10(2),
2009.
[Xu20] Haifeng Xu. On the tractability of public persuasion with no externalities. In Symposium
on Discrete Algorithms (SODA) , pages 2708–2727. SIAM, 2020.
12A Additional insights and challenges
We first present an algorithm for enumerating the cells induced by a finite collection of hyperplanes in
Rdin output-sensitive time. Our approach is based on an output-sensitive algorithm for determining
the non-redundant constraints in a linear system due to Clarkson [ Cla94 ]. At any point, we compute
thelocally relevant hyperplanes that constitute the candidate hyperplanes which could bound the cell
(or piece) containing that point, using a problem-specific sub-routine, and apply Clarkson’s algorithm
over these to determine the facet-inducing hyperplanes for the cell as well as a collection of points
in neighboring cells. This allows us to compute all the induced cells by traversing an implicit cell
adjacency graph (Definition 2) in a breadth-first order. Our approach gives a recipe for computing
the pieces of piecewise structured dual class functions with linear boundaries, which can be used
to find the best parameters over a single problem instance. We further show how to compute the
pieces of the sum dual class function for a collection of problem instances in output-sensitive time,
by applying our approach to the collection of facet-inducing hyperplanes from each problem instance.
Our approach is useful for several mechanism design problems which are known to be piecewise
structured [ BSV18 ], and we instantiate our results for two-part tariff pricing and item pricing with
anonymous prices. For the single menu two-part tariff case ( d= 2), we use additional structure of
the dual class function to give a further improvement in the running time.
For linkage-based clustering, we extend the execution tree based approach of [ BDL20 ] for single-
parameter families to d >1. The key idea is that linkage-based clustering algorithms involve a
sequence of steps (called merges ), and the algorithmic decision at any step can only refine the partition
of the parameter space corresponding to a fixed sequence of steps so far. Thus, the pieces can be
viewed as leaves of a tree whose nodes at depth tcorrespond to a partition of the parameter space
such that the first tsteps of the algorithm are identical in each piece of the partition. While in the
single parameter family the pieces are intervals, we approach the significantly harder challenge of
efficiently computing convex polytopic subdivisions (formally Definition 6). We essentially compute
the execution tree top-down for any given problem instance, and use our hyperplane cell enumeration
to compute the children nodes for any node of the tree.
We further extend the execution tree approach to a problem where all the algorithmic states corre-
sponding to different parameter values cannot be concisely represented via a tree. For dynamic-
programming based sequence alignment, we define a compact representation of the execution graph
and use primitives from computational geometry, in particular for overlaying two convex subdivisions
in output-polynomial time. A key challenge in this problem is that the execution graph is no longer a
tree but a DAG, and we need to design an efficient representation for this DAG. We cannot directly
apply Clarkson’s algorithm since the number of possible alignments of two strings (and therefore
the number of candidate hyperplanes) is exponential in the size of the strings. Instead, we carefully
combine the polytopes of subproblems in accordance with the dynamic program update for solving
the sequence alignment (for a fixed value of parameters), and the number of candidate hyperplanes is
output-sensitive inside regions where all the relevant subproblems have fixed optimal alignments.
B Additional related work
Data-driven algorithm design. The framework for data-driven design was introduced by [ GR16 ] and
is surveyed in [ Bal20 ]. It allows the design of algorithms with powerful formal guarantees when used
repeatedly on inputs consisting of related problem instances, as opposed to designing and analyzing
for worst-case instances. Data-driven algorithms have been proposed and analyzed for a wide
variety of combinatorial problems, including clustering [ BDL20 ], computational biology [ BDD+21],
mechanism design [ BSV18 ], mixed integer linear programming [ BDSV18 ], semi-supervised learning
[BS21 ,SJ23 ], linear regression [ BKST22 ], low-rank approximation [ BIW22 ] and adversarially
robust learning [ BBSZ23 ]. Typically these are NP-hard problems with a variety of known heuristics.
Data-driven design provides techniques to select the heuristic (from a collection or parameterized
family) which is best suited for data coming from some domain. Tools for data-driven design
have also been studied in the online learning setting where problems arrive in an online sequence
[CAK17 ,BDV18 ,BDS20 ]. In this work, we focus on the statistical learning setting where the
“related” problems are drawn from a fixed but unknown distribution.
Linkage-based clustering. [BNVW17 ] introduce several single parameter families to interpolate
well-known linkage procedures for linkage-based clustering, and study the sample complexity of
13learning the parameter. [ BDL20 ] consider families with multiple parameters, and also consider the
interpolation of different distance metrics. However the proposed execution-tree based algorithm for
computing the constant-performance regions in the parameter space only applies to single-parameter
families, and therefore the results can be used to interpolate linkage procedures or distance metrics,
but not both. Our work removes this limitation and provides algorithms for general multi-parameter
families.
Other approaches to learning a distance metric to improve the quality of clustering algorithms have
been explored. For instance, [ WS09 ] and [ KPT+21] consider versions of global distance metric
learning, a technique to learn a linear transformation of the input space before applying a known
distance metric. [ SC11 ] also consider a hierarchical distance metric learning, which allows the learned
distance function to depend on already-merged clusters. For such distance learning paradigms, the
underlying objective function is often continuous, admitting gradient descent techniques. However,
in our setting, the objective function is neither convex nor continuous in ρ; instead, it is piecewise
constant for convex pieces. As a result, instead of relying on numerical techniques for finding the
optimum ρ, our multidimensional approach enumerates all the pieces for which the objective function
is constant. Furthermore, this technique determines the exact optimum ρrather than an approximation;
one consequence of doing so is that our analysis can be extended to apply generalization guarantees
from [BDL20] when learning the optimal ρover a family of instances.
Sequence alignment. For the problem of aligning pairs of genomic sequences, one typically obtains
the best alignment using a dynamic program with some costs or weights assigned to edits of different
kinds, such as insertions, substitutions, or reduplications [ Wat89 ]. Prior work has considered learning
these weights by examining the cost of alignment for the possible weight settings. [ BDD+21]
considers the problem for how many training examples (say pairs of DNA sequences with known
ancestral or evolutionary relationship) are needed to learn the best weights for good generalization on
unseen examples. More closely related to our work is the line of work which proposes algorithms
which compute the partition of the space of weights which result in different optimal alignments
[GBN94 ,GS96 ]. In this work, we propose a new algorithm which computes this partition more
efficiently.
Pricing problems. Data-driven algorithms have been proposed and studied for automated mech-
anism design [ MR15 ,BSV18 ,BDV18 ]. Prior work on designing multi-dimensional mechanisms
has focused mainly on the generalization guarantees, and include studying classes of structured
mechanisms for revenue maximization [ MR15 ,BSV16 ,Syr17 ,BSV18 ]. Computationally efficient
algorithms have been proposed for the two-part tariff pricing problem [ BPS20 ]. Our algorithms
have output-sensitive complexity and are more efficient than [ BPS20 ] even for worst-case outputs.
Also, our techniques are more general and apply to mechanism design besides two-part tariff pricing.
[BB24 ] consider discretization-based techniques which are not output-sensitive and known to not
work in problems beyond mechanism design, for example, data-driven clustering [BNVW17].
On cell enumeration of hyperplane arrangements. A finite set of thyperplanes in Rdinduces a
collection of faces in dimensions ≤d, commonly referred to as an arrangement . A well-known
inductive argument implies that the number of d-faces, or cells, in an arrangement of thyperplanes in
Rdis at most O(td). Several algorithms, ranging from topological sweep based [ EG86 ] to incremental
construction based [ Xu20 ], can enumerate all the cells (typically represented by collections of facet-
inducing hyperplanes) in optimal worst-case time O(td). The first output-sensitive algorithm proposed
for this problem was the reverse search based approach of [AF96], which runs in O(tdR·LP(d, t))
time, where Rdenotes the output-size (number of cells in the output) and LP(d, t)is the time for
solving a linear program in dvariables and tconstraints. This was further improved by a factor
oftvia a more refined reverse-search [ Sle99 ], and by additive terms via a different incremental
construction based approach by [ RC18 ]. We propose a novel approach for cell enumeration based on
Clarkson’s algorithm [ Cla94 ] for removing redundant constraints from linear systems of inequalities,
which asymptotically matches the worst case output-sensitive running times of these algorithms
while improving over their runtime in the presence of additional structure possessed by the typical
piecewise structured (Definition 1) loss functions encountered in data-driven algorithm design.
14C Output-sensitive Parameterized Complexity
Output-sensitive algorithms have a running time that depends on the size of the output for any input
problem instance. Output-sensitive analysis is frequently employed in computational geometry, for
example Chan’s algorithm [ Cha96 ] computes the convex hull of a set of 2-dimensional points in time
O(nlogR), where Ris the size of the (output) convex hull. Output-sensitive algorithms are useful if
the output size is variable, and ‘typical’ output instances are much smaller than worst case instances.
Parameterized complexity extends classical complexity theory by taking into account not only the
total input length n, but also other aspects of the problem encoded in a parameter k10. The motivation
is to confine the super-polynomial runtime needed for solving many natural problems strictly to the
parameter. Formally, a parameterized decision problem (or language) is a subset L⊆Σ∗×N, where
Σis a fixed alphabet, i.e. an input (x, k)to a parameterized problem consists of two parts, where the
second part kis the parameter. A parameterized problem Lisfixed-parameter tractable if there exists
an algorithm which on a given input (x, k)∈Σ∗×N, decides whether (x, k)∈Linf(k)·poly(|x|)
time, where f is an arbitrary computable function in k[DF12]. FPT is the class of all parameterized
problems which are fixed-parameter tractable. In contrast, the class XP (aka slicewise-polynomial )
consists of problems for which there is an algorithm with running time |x|f(k). It is known that FPT
⊊XP. We consider an extension of the above to search problems and incorporate output-sensitivity in
the following definition.
Definition 3 (Output-polynomial Fixed Parameter Tractable) .A parameterized search problem
P: Σ∗×N→˜Σ∗is said to be output-polynomial fixed-parameter tractable if there exists an
algorithm which on a given input (x, k)∈Σ∗×N, computes the output P(x, k)∈˜Σ∗in time
f(k)·poly(|x|, R), where R=|P(x, k)|is the output size and fis an arbitrary computable function
ink.
As discussed above, output-sensitvity and fixed-parameter tractability both offer more fine-grained
complexity analysis than traditional (input) polynomial time complexity. Both techniques have been
employed in efficient algorithmic enumeration [ Fer02 ,Nak04 ] and have gathered recent interest
[FGS+19]. In this work, we consider the optimization problem of selecting tunable parameters over
a continuous domain C ⊂Rdwith the fixed-parameter k=d, the number of tunable parameters. We
will design OFPT enumeration algorithms which, roughly speaking, output a finite “search space”
which can be used to easily find the best parameter for the problem instance.
D Augmented Clarkson’s algorithm
We describe here the details of the Augmented Clarkson’s algorithm, which modifies the algorithm of
Clarkson [ Cla94 ] with additional bookkeeping needed for tracking the partition cells in Algorithm 1.
The underlying problem solved by Clarkson’s algorithm may be stated as follows.
Problem Setup. Given a linear system of inequalities Ax≤b, an inequality Aix≤biis said to be
redundant in the system if the set of solutions is unchanged when the inequality is removed from
the system. Given a system (A∈Rm×d, b∈Rm), find an equivalent system with no redundant
inequalities.
Note that to test if a single inequality Aix≤biis redundant, it is sufficient to solve the following LP
indvariables and mconstraints.
maximize Aix
subject to Ajx≤bj,∀j∈[m]\ {i}
Aix≤bi+ 1(2)
Using this directly to solve the redundancy removal problem gives an algorithm with running time
m·LP(d ,m), where LP(d ,m)denotes the time to solve an LP in dvariables and mconstraints.
This can be improved using Clarkson’s algorithm if the number of non-redundant constraints sis
much less than the total number of constraints m(Theorem 2.1).
We assume that an interior point z∈Rdsatisfying Ax < b is given. At a high level, one maintains
the set of non-redundant constraints Idiscovered so far i.e. Aix≤biis not redundant for each
10N.B. The term “parameter” is overloaded. It is used to refer to the real-valued parameters in the algorithm
family, as well as the parameter to the optimization problem over the algorithm family.
15Algorithm 2: AUGMENTED CLARKSON (z, H= (A, b), c)
Input :A∈Rm×d, b∈Rm, z∈Rd, sign-pattern c∈ {0,1,−1}m.
Output : list of non-redundant hyperplanes I⊆[m], points in neighboring cells Z⊂Rd.
I← ∅, J←[m], Z← ∅
while J̸=∅do
Select k∈J
Detect if Akx≤bkis redundant in AI∪{k}x≤bI∪{k}by solving LP (2).
x∗←optimal solution of the above LP
ifredundant then
J←J\ {k}
j, z∗←RayShoot (A, b, z, x∗); /* Computes jsuch that Ajx=bjis a
facet-inducing hyperplane hit by ray from zalong x∗−z*/
J←J\ {j}
c′
j← −cj, c′
i←ci∀i∈[m]\ {j}
I←I∪ {j}, Z←Z∪ {[c′, z∗]}
return I, Z
i∈I. When testing a new index k, the algorithm solves an LP of the form 2 and either detects that
Akx≤bkis redundant, or finds index j∈[m]\Isuch that Ajx≤bjis non-redundant. The latter
involves the use of a procedure RayShoot (A, b, z, x )which finds the non-redundant hyperplane hit
by a ray originating from zin the direction x−z(x∈Rd) in the system A, b. The size of the LP
needed for this test is LP(d ,|I|+ 1) from which the complexity follows.
To implement the RayShoot procedure, we can simply find the intersections of the ray x∗−zwith
the hyperplanes Ajx≤bjand output the one closest to z(defining the cell facet in that direction).
We also output an interior point from the adjacent cell during this computation, which saves us time
relative to [ Sle99 ] where the interior point is computed for each cell (our Clarkson based approach
also circumvents their need for the Raindrop procedure [ BFM97 ]). Finally we state the running time
guarantee of Algorithm 2, which follows from the original result of Clarkson [Cla94].
Algorithm 3: RayShoot
Input :A∈Rm×d, b∈Rm, z∈Rd, x∈Rd.
ifAi·(x−z) = 0 for some ithen
z←z+ (ϵ, ϵ2, . . . , ϵd)for sufficiently small ϵ;/* Ensure general position. */
ti←bi−Ai·z
Ai·(x−z); /* intersection of z+t(x−z)with Aix=bi*/
j= argmini{ti|ti>0}, t′= max {mini̸=j{ti|ti>0},0}
return j, z+tj+t′
2(x−z)
Theorem D.1 (Augmented Clarkson’s algorithm) .Given a list Lofkhalf-space constraints in
ddimensions, Algorithm 2 outputs the set I⊆[L]of non-redundant constraints in L, as well as
auxiliary neighbor information, in time O(k·LP(d,|I|+ 1)) , where LP(v, c)is the time for solving
an LP with vvariables and cconstraints.
E Additional details and proofs from Section 2
Proof of Theorem 2.2. Algorithm 1 maintains a set of visited orexplored cells in C, and their bound-
ing hyperplanes (corresponding to cell facets) in ˜C. It also maintains a queue of cells, such that each
cell in the queue has been discovered in Line 12 as a neighbor of some visited cell in C, but is yet to
be explored itself. The algorithm detects if a cell has been visited before by using its sign pattern on
the hyperplanes in H. This can be done in O(dlogt)time, since |C|=O(td)using a well-known
combinatorial fact (e.g. [ Buc43 ]). For a new cell c, we run AUGMENTED CLARKSON to compute its
bounding hyperplanes Icas well as sign patterns for neighbors in time O(tLRS·LP(d,|Ic|+ 1)) by
Theorem 2.1. The computed neighbors are added to the queue in Line 14. A cell cgets added to the
queue at this step up to |Ic|times, but is not explored if already visited. Thus we run up to Viterations
16ofAUGMENTED CLARKSON and up to 1 +P
c∈VH|Ic|= 2E+ 1queue insertions/removals. Using
efficient union-find data-structures, the set union and membership queries (for Cand˜C) can be done
in˜O(1)time per iteration of the loop. So total time over the Vcell explorations and no more than
2E+ 1iterations of the while loop is ˜O(dE) +O(tLRS·P
c∈VHLP(d,|Ic|+ 1)) + O(V T CLRS).
Proof of Theorem 2.3. We will apply Algorithm 1 and compute the locally relevant hyperplanes by
simply taking a union of the facet-inducing hyperplanes at any point x, across the problem instances.
LetG(i)= (V(i), E(i))denote the cell adjacency graph for the cells in ˜Ci. We apply Algorithm 1
implicitly over H=∪iH(i)where H(i)is the collection of facet-inducing hyperplanes in ˜Ci. To
implement COMPUTE LOCALLY RELEVANT SEPARATORS (x, H)we simply search the computed
partition cell ˜Cifor the cell containing xin each problem instance xiin time O(P
i|E(i)|) =
O(m|ES|), obtain the set of corresponding facet-inducing hyperplanes H(i)
x, and output ˜Hx=
∪iH(i)
xin time O(mtLRS). The former step only needs to be computed once, as the partition cells
for subsequent points can be tracked in O(m)time. Theorem 2.2 now gives a running time of
˜O(d|ES|+m|ES|+mtLRS|VS|+mtLRS·P
c∈VSLP(d,|Ic|+ 1)) .
F Profit maximization in pricing problems
Prior work [ MR15 ,BSV18 ] on data-driven mechanism design has shown that the profit as a function
of the prices (parameters) is (F, t)-decomposable with Fthe set of linear functions on Rdfor a
large number of mechanism classes (named (d, t)-delineable mechanisms in [ BSV18 ]). We will
instantiate our approach for multi-item pricing problems which are (F, t)-decomposable and analyse
the running times. In contrast, recent work [ BB24 ] employs data-independent discretization for
computationally efficient data-driven algorithm design for mechanism design problems even in the
worst-case. This discretization based approach is not output-sensitive and is known to not work well
for other applications like data-driven clustering.
F.1 Two-part tariff pricing
In the two-part tariff problem [ Lew41 ,Oi71 ], the seller with multiple identical items charges a fixed
price, as well as a price per item purchased. For example, cab meters often charge a base cost for any
trip and an additional cost per mile traveled. Subscription or membership programs often require an
upfront joining fee plus a membership fee per renewal period, or per service usage. Often there is
a menu or tier of prices, i.e. a company may design multiple subscription levels (say basic, silver,
gold, platinum), each more expensive than the previous but providing a cheaper per-unit price. Given
access to market data (i.e. profits for different pricing schemes for typical buyers) we would like to
learn how to set the base and per-item prices to maximize the profit. We define these settings formally
as follows.
Two-part tariffs. The seller has Kidentical units of an item. Suppose the buyers have valuation
functions vi:{1, . . . , K } →R≥0where i∈ {1, . . . , m }denotes the buyer, and the value is assumed
to be zero if no items are bought. Buyer iwill purchase qquantities of the item that maximizes their
utility ui(q) =vi(q)−(p1+p2q), buying zero units if the utility is negative for each q >0. The
revenue, which we want to maximize as the seller, is zero if no item is bought, and p1+p2qifq >0
items are bought. The algorithmic parameter we want to select is the price ρ=⟨p1, p2⟩, and the
problem instances are specified by the valuations vi. We also consider a generalization of the above
scheme: instead of just a single two-part tariff (TPT), suppose the seller provides a menu of TPTs
(p1
1, p1
2), . . . , (pℓ
1, pℓ
2)of length ℓ. Buyer iselects a tariff (pj
1, pj
2)from the menu as well as the item
quantity qto maximize their utility uj
i(q) =vi(q)−(pj
1+pj
2q). This problem has 2ℓparameters,
ρ= (p1
1, p1
2, . . . , pℓ
1, pℓ
2), and the single two-part tariff setting corresponds to ℓ= 1.
The dual class functions in this case are known to be piecewise linear with linear boundaries ([ BSV18 ],
restated as Lemma G.2 in Appendix G). We will now implement Algorithm 1 for this problem by
specifying how to compute the locally relevant hyperplanes. For any price vector x=ρ, say the
buyers buy quantities (q1, . . . , q m)∈ {0, . . . , K }maccording to tariffs (j1, . . . , j m)∈[ℓ]m. For a
fixed price vector this can be done in time O(mKℓ )by computing argmaxq,juj
i(q)for each buyer
17at that price for each two-part tariff in the menu. Then for each buyer we have K(ℓ−1)potential
alternative quantities and tariffs given by hyperplanes uji
i(qi)≥uj′
i(q′), q′̸=qi, j′̸=ji, for a total
oftLRS=mK(ℓ−1)locally relevant hyperplanes. Thus TCLRS=O(mKℓ )for the above approach,
and Theorem 2.2 implies the following runtime bound.
Theorem F.1. There exists an implementation of COMPUTE LOCALLY RELEVANT SEPARATORS in
Algorithm 1, which given valuation function v(·)for a single problem instance, computes all the R
pieces of the dual class function uv(·)in time ˜O(R2(2ℓ)ℓ+2K), where the menu length is ℓ, and there
areKunits of the good.
Theorem F.1 together with Theorem 2.3 implies an implementation of the search-ERM problem over
mbuyers (with valuation functions vi(·)fori∈[m]) in time O(R2
Σ(2ℓ)ℓ+2mK), where RΣdenotes
the number of pieces in the total dual class function U⟨v1,...,v m⟩(·) =P
iuvi(·)(formally, Corollary
G.1 in Appendix G). In contrast, prior work for this problem has only obtained an XP runtime
of(mK)O(ℓ)[BPS20 ]. For the special case ℓ= 1, we also provide an algorithm (Algorithm 4 in
Appendix G.2) that uses additional structure of the polytopes and employs a computational geometry
algorithm due to [ CE92 ] to compute the pieces in optimal O(mKlog(mK) +RΣ)time, improving
over the previously best known runtime of O(m3K3)due to [ BPS20 ] even for worst-case RΣ. The
worst-case improvement follows from a bound of RΣ=O(m2K)on the number of pieces, which
we establish in Theorem G.3. We further show that our running time for ℓ= 1 is asymptotically
optimal under the algebraic decision-tree model of computation, by a linear time reduction from the
element uniqueness problem (Theorem G.6).
F.2 Item-Pricing with anonymous prices
We will consider a market with a single seller, interested in designing a mechanism to sell mdistinct
items to nbuyers. We represent a bundle of items by a quantity vector q∈Zm
≥0, such that the number
of units of the ith item in the bundle denoted by qis given by its ith component q[i]. In particular,
the bundle consisting of a single copy of item iis denoted by the standard basis vector ei, where
ei[j] =I{i=j}, where I{·}is the 0-1 valued indicator function. Each buyer j∈[n]has a valuation
function vj:Zm
≥0→R≥0over bundles of items. We denote an allocation as Q= (q1, . . . , q n)
where qjis the bundle of items that buyer jreceives under allocation Q. Under anonymous prices,
the seller sets a price piper item i. There is some fixed but arbitrary ordering on the buyers such
that the first buyer in the ordering arrives first and buys the bundle of items that maximizes their
utility, then the next buyer in the ordering arrives, and so on. For a given buyer jand bundle pair
q1, q2∈ {0,1}m, buyer jwill prefer bundle q1over bundle q2so long as uj(q1)> uj(q2)where
uj(q) =vj(q)−P
i:q[i]=1pi. Therefore, for a fixed set of buyer values and for each buyer j, their
preference ordering over the bundles is completely determined by the 2m
2
hyperplanes. The dual
class functions are known to be piecewise linear with linear boundaries [BSV18].
To implement Algorithm 1 for this problem we specify how to compute the locally relevant hy-
perplanes. For any price vector x= (p1, . . . , p m), say the buyers buy bundles (q1, . . . , q n). For a
fixed price vector this can be done in time O(n2m)by computing argmaxq⊆Ijuj(q)for each buyer
at that price vector, where Ijdenotes the remaining items after allocations to buyers 1, . . . , j −1
at that price. Then for each buyer we have at most 2m−1potential alternative bundles given by
hyperplanes uj(q)≥uj(q′), q′̸=qi, for a total of tLRS≤n(2m−1)locally relevant hyperplanes.
Thus TCLRS =O(n2m)for the above approach, and Theorem 2.2 implies the following runtime
bound.
Theorem F.2. There exists an implementation of COMPUTE LOCALLY RELEVANT SEPARATORS in
Algorithm 1, which given valuation functions vi(·)fori∈[n], computes all the Rpieces of the dual
class function in time ˜O(R2(2m)mn), where there are mitems and nbuyers.
Proof. In the terminology of Theorem 2.2, we have d=m,E≤R2,V=R,TCLRS=O(n2m),
tLRS=n(2m−1). By [ Cha18 ], we haveP
c∈VHLP(d,|Ic|+ 1)≤O(Edd)≤O(R2mm). Thus,
Theorem 2.2 implies a runtime bound on Algorithm 1 of ˜O(dE+V T CLRS+tLRS·P
c∈VHLP(d,|Ic|+
1)) = ˜O(R2(2m)mn).
18Our approach yields an efficient algorithm when the number of items mand the number of dual class
function pieces Rare small. Prior work on item-pricing with anonymous prices has only focussed on
sample complexity of the data-driven design problem [BSV18].
G Additional details and proofs from Section F
Proof of Theorem F .1. In the terminology of Theorem 2.2, we have d= 2ℓ,E≤R2,V=R,
TCLRS =O(Kℓ),tLRS≤Kℓ. By [ Cha18 ], we haveP
c∈VHLP(d,|Ic|+ 1)≤O(Edd/2+1)≤
O(R2(2ℓ)ℓ+1). Thus, Theorem 2.2 implies a runtime bound on Algorithm 1 of ˜O(dE+V T CLRS+
tLRS·P
c∈VHLP(d,|Ic|+ 1)) = ˜O(R2(2ℓ)ℓ+2K).
Corollary G.1. There exists an implementation of COMPUTE LOCALLY RELEVANT SEPARATORS
in Algorithm 1, which given valuation functions vi(·)fori∈[m], computes all the RΣpieces of
the total dual class function U⟨v1,...,v m⟩(·) =P
iuvi(·)in time ˜O(R2
Σ(2ℓ)ℓ+2mK), where the menu
length is ℓ, there are Kunits of the good and mis the number of buyers.
Proof. We first compute the pieces for each of the problem instances (single buyers) and then
the pieces in the sum dual class function using Theorem 2.3. By Theorem F.1, the former
takes time ˜O(mR2(2ℓ)ℓ+2K)and the latter can be implemented in time O((m+ 2ℓ)R2
Σ+
mKℓP
c∈VSLP(d,|Ic|+1)) = ˜O(R2
Σ(2ℓ)ℓ+2mK), which dominates the overall running time.
G.1 Piecewise structure of the dual class function
The following lemma restates the result from [ BSV18 ] in terms of Definition 1. Note that uρin the
following denotes the revenue function (or seller’s utility) and should not be confused with the buyer
utility function ui.
Lemma G.2. LetUbe the set of functions {uρ:v(·)7→pj∗
1+pj∗
2q∗|q∗, j∗= argmaxq,jv(q)−
ρj· ⟨1, q⟩, ρj=⟨pj
1, pj
2⟩}that map valuations v(·)toR. The dual class U∗is(F,(Kℓ)2)-piecewise
decomposable, where F={fc:U →R|c∈R2ℓ}consists of linear functions fc:uρ7→ρ·c.
We also bound the number of pieces Rin the worst-case for ℓ= 1. The following bound implies that
our algorithm is better than prior best algorithm which achieves an O(m3K3)runtime bound, even
for worst case outputs.
Theorem G.3. Let menu length ℓ= 1. The number of pieces RΣin the total dual class function
U⟨v1,...,v m⟩(·) =P
iuvi(·)is at most O(m2K).
Proof. By Lemma G.2, the dual class is (F, K2)-piecewise decomposable, where Fis the class
of linear functions. That is, the two-dimensional parameter space (p1, p2)can be partitioned into
polygons by at most K2straight lines such that any dual class function uviis a linear function inside
any polygon.
We first tighten the above result to show that the dual class is in fact (F,2K+ 2) -piecewise decom-
posable, that is the number of bounding lines for the pieces is O(K). This seems counterintuitive
since for any buyer i, we have Θ(K2)lines ui(q)≥ui(q′)forq < q′∈ {0, . . . , K }. Ifq >0,
ui(q) =ui(q′)are axis-parallel lines with interceptsvi(q′)−vi(q)
q′−q. Since for any pair q, q′the buyer
has a fixed (but opposite) preference on either side of the axis-parallel line, we have at most Kdistinct
horizontal ‘slabs’ corresponding to buyer’s preference of quantities, i.e. regions between lines p2=a
andp2=bfor some a, b > 0. Thus we have at most Knon-redundant lines. Together with another
Klinesui(0) = ui(q′)and the axes, we have 2K+ 2bounding lines in all as claimed.
We will next use this tighter result to bound the number of points of intersection of non-collinear
non-axial bounding lines, let’s call them crossing points , across all instances ⟨v1, . . . , v m⟩. Consider
a pair of instances given by buyer valuation functions vi, vj. We will establish that the number of
crossing points are at most 4Kfor the pair of instances. Let liandljbe bounding lines for the pieces
ofuvianduvjrespectively. If they are both axis-parallel, then they cannot result in a crossing point.
For pairs of bounding lines liandljsuch that liis axis-parallel and ljis not, we can have at most K
crossing points in all. This is because any fixed lican intersect at most one such ljsince buyer j’s
19preferred quantity qjis fixed along any horizontal line, unless qjchanges across liin which case the
crossing points for the consecutive lj’s coincide. Thus, there is at most one such crossing point for
each of at most Kaxis-parallel li’s. By symmetry, there are at most Kcrossing points between li, lj
where ljis axis parallel and liis not. Finally, if neither li, ljis axis parallel, we claim there can be
no more than 2Kcrossing points. Indeed, if we arrange these points in the order of increasing p2,
then the preferred quantity of at least one of the buyers iorjstrictly decreases between consecutive
crossing points. Thus, across all instances, there are at most 2m2Kcrossing points.
Finally, observe that the cell adjacency graph GUfor the pieces of the total dual class function
U⟨v1,...,v m⟩(·)is planar in this case. The vertices of this graph correspond to crossing points, or
intersections of bounding lines with the axes. The latter is clearly O(mK)since there are O(K)
bounding lines in any problem instance. Using the above bound on crossing points, the number of
vertices in GUisO(m2K). Since GUis a simple, connected, planar graph, the number of faces is no
more than twice the number of vertices and therefore the number of pieces RΣis also O(m2K).
G.2 Optimal algorithm for Single TPT pricing
Consider the setting with menu-length ℓ= 1. The key insight is to characterize the polytopic structure
of the pieces of the dual class function for a single buyer. We do this in Lemma G.4.
Lemma G.4. Consider a single buyer with valuation function v(·). The buyer buys zero units of the
item except for a set ϱv⊂R2, where ϱvis a convex polygon with at most K+ 2sides. Moreover,
ϱvcan be subdivided into K′≤Kpolygons ϱ(i)
v, each a triangle or a trapezoid with bases parallel
to the ρ1-axis, such that for each i∈[K′]the buyer buys the same quantity q(i)of the item for all
prices in ϱ(i)
v.
Proof. We proceed by an induction on K, the number of items. For K= 1, it is straightforward to
verify that ϱvis the triangle p1≥0, p2≥0, p1+p2≤v(1).
LetK > 1. If we consider the restriction of the valuation function v(·)toK−1items, we have a
convex polygon ϱ′
vsatisfying the induction hypothesis. To account for the K-th item we only need to
consider the region p1≥0, p2≥0, p2≤v(K)−v(q)
K−qfor0< q < K , and p1+p2K≤v(K). If this
region is empty, ϱv=ϱ′
v, and we are done. Otherwise, denoted by ϱ(K′)
v withq(K′)=K, the region
where the buyer would buy Kunits of the item is a trapezoid with bases parallel to the ρ1-axis. We
claim that ϱv=
ϱ′
v∩p2≤v(K)−v(q)
K−q
∪ϱ(K′)
v and it satisfies the properties in the lemma.
We have q(K′−1)= argminqv(K)−v(q)
K−qsuch that the buyer’s preference changes from q′=q(K′−1)
toq(K′)=Kunits across the line p2=v(K)−v(q′)
K−q′. To prove ϱvis convex, we use the inductive
hypothesis on ϱ′
vand observe that ρ1=v(K)−p2Kcoincides with ρ′
1=v(q′)−p2q′forp2=
v(K)−v(q′)
K−q′. Also the only side of ϱvthat is not present in ϱ′
vlies along the line p1+p2K=v(K),
thusϱvhas at most K+2sides. The subdivison property is also readily verified given the construction
ofϱvfrom ϱ′
vandϱ(K′)
v.
Based on this structure, we propose Algorithm 4 which runs in in O(mKlog(mK) +RΣ)time.
Theorem G.5. There is an algorithm (Algorithm 4) that, given valuation functions vi(·)fori∈[m],
computes all the Rpieces of the total dual class function U⟨v1,...,v m⟩(·)forKunits of the good from
themsamples in O(mKlog(mK) +RΣ)time.
Proof. Note that if 0< q < q′andvi(q)> vi(q′), then for any ρ1≥0, ρ2≥0we have that
ui(q) =vi(q)−(p1+p2q)> vi(q′)−(p1+p2q), or the buyer will never prefer quantity q′of the
item over the entire tariff domain. Thus, we will assume the valuations vi(q)are monotonic in q(we
can simply ignore valuations at the larger value for any violation). Algorithm 4 exploits the structure
in Lemma G.4 and computes the O(K)line segments bounding the dual class pieces for a single
buyer iinO(K)time. Across mbuyers, we have O(mK)line segments (computed in O(mK)
time). The topological plane-sweep based algorithm of [ CE92 ] now computes all the intersection
points in O(mK(logmK) +RΣ)time. Here we have used that the number of polytopic vertices is
O(RΣ)using standard result for planar graphs.
20Algorithm 4: ComputeFixedAllocationRegions
Input :vi(·), valuation functions
1. For i= 1. . . m do
2.Q← ∅ (stack), q′= 1,h=vi(1).
3. For q= 2. . . K do
3.1 h′←(vi(q)−vi(q′))/(q−q′)
3.2 if 0< h′< h
h←h′
Push (q, h′)ontoQ
q′←q
3.3 else if 0< h′
while h′≥hdo
Pop(q′, h)from Q
(q1, h1)←Top(Q)
h′←(vi(q)−vi(q1))/(q−q1)
h←h1
q′←q
Push (q, h′)ontoQ
4. For (q, h)∈Qdo
Obtain ϱ(j)
viforq(j)=qusing lines p2=handp1=v(q)−p2q.
5. Compute intersection of segments in ϱ(j)
vifori∈[m]using [CE92].
6. Use the intersection points to compute boundaries of all polygons (pieces) formed by the
intersections.
We further show that this bound is essentially optimal. A runtime lower bound of Ω(mK+RΣ)
follows simply due to the amount of time needed for reading the complete input and producing
the complete output. We prove a stronger lower bound which matches the above upper bound by
reduction to the element uniqueness problem (given a list of nnumbers, are there any duplicates?) for
which an Ω(nlogn)lower bound is known in the algebraic decision-tree model of computation.
Theorem G.6. Given a list of nnumbers L=⟨x1, . . . , x n⟩ ∈Nn, there is a linear time reduction to
am-buyer, K-item TPT pricing given by vi(·), i∈[m], with mK= Θ( n), such that the pieces of
the total dual class function U⟨v1,...,v m⟩(·)can be used to solve the element uniqueness problem for
LinO(n)time.
Proof. LetmK=nbe any factorization of ninto two factors. We construct a m-buyer, K+ 1item
single TPT pricing scheme as follows. Define yj=xj+ max kxk+ 1for each xjin the list L.
For every buyer i∈[m], we set vi(1) = max kxk+ 1andvi(q+ 1) =Pq
j=1yj+(i−1)Kfor each
q∈[K]. Buyer i’s pieces include the segments p2= (vi(q+ 1)−vi(q))/(q+ 1−1) = xq+(i−1)K
forq∈[K](Lemma G.4). Thus, across all buyers i∈[m], we have mK=nsegments along the
linesp2=xjforj∈[n]. We say a duplicate is present if there are fewer than mK segments parallel
to the p1-axis in the pieces of the total dual class function, otherwise we say ‘No’ (i.e. all elements
are distinct). This completes the linear-time reduction.
HComparing the quality of single, complete and median linkage procedures
on different data distributions
We will construct clutering instances where each of two-point based linkage procedures, i.e. single,
complete and median linkage, dominates the other two procedures. Let TS
sgl, TS
cmplandTS
meddenote
the cluster tree on clustering instance Susing Dsgl, DcmplandDmedas the merge function (defined in
Section 3) respectively for some distance metric dwhich will be evident from context. We have the
following theorem.
Theorem H.1. For any n≥10, fori∈ {1,2,3}, there exist clustering instances Siwith|Si|=n
and target clusterings Cisuch that the hamming loss of the optimal pruning of the cluster trees
constructed using single, complete and median linkage procedures (using the same distance metric d)
satisfy
21(i)ℓ(TS1
sgl,C1) =O(1
n),ℓ(TS1
cmpl,C1) = Ω(1) andℓ(TS1
med,C1) = Ω(1) ,
(ii)ℓ(TS2
cmpl,C2) =O(1
n),ℓ(TS2
sgl,C2) = Ω(1) andℓ(TS2
med,C2) = Ω(1) ,
(iii)ℓ(TS3
med,C3) =O(1
n),ℓ(TS3
cmpl,C3) = Ω(1) andℓ(TS3
sgl,C3) = Ω(1) .
Proof. In the following constructions we will have Si⊂R2and the distance metric dwill be the
Euclidean metric. Also we will have number of target clusters k= 2.
Construction of S1,C1. For S1, we will specify the points using their polar coordinates. We place
a single point xat the origin (0, ϕ)andn−1
8points each along the unit circle at locations y1=
(1,0), y2= (1,π
4−ϵ), y3= (1,π
2), y4= (1,3π
4−ϵ), y5= (1, π), y6= (1,5π
4−ϵ), y7= (1,3π
2)
andy8= (1,7π
4−ϵ), where ϵ= 0.001. Also set C1={{x}, S1\ {x}}.
In each linkage procedure, the first n−9merges will join coincident points at locations yj, j∈
[8], let˜yjdenote the corresponding sets of merged points. The next four merges will be zj:=
{˜yj,˜yj+1}forj∈ {1,3,5,7}for each procedure since d(yj, yj+1) =p
2−2 cos(π
4−ϵ)<
min{p
2−2 cos(π
4+ϵ),1}, again common across all procedures. Now single linkage will continue
to merge clusters on the unit circle sincep
2−2 cos(π
4+ϵ)<1, however both complete and
median linkage will join each of zj, j∈ {1,3,5,7}to the singleton cluster {x}since the median (and
therefore also the maximum distance between points in zj, zj′, j̸=j′is at least√
2>1. Therefore
a 2-pruning11ofTS1
sglyields C1i.eℓ(TS1
sgl,C1) = 0 , while a 2-pruning of TS1
cmplorTS1
medwould yield
{zj, S1\zj}for some j∈ {1,3,5,7}, corresponding to a hamming loss of1
4+ Ω(1
n) = Ω(1) .
Construction of S2,C2. ForS2, we will specify the points using their Cartesian coordinates. We place
single points x1, x2at(0,0)and(3.2,0.5)andn−2
2points each at y1= (1.1,1.8)andy2= (1.8,0.5).
We set C2={{(x, y)∈S2|y >1},{(x, y)∈S2|y≤1}}. The distances between pairs of points
may be ordered as
d(x2, y2) = 1 .4< d(y1, y2)≈1.5< d(x1, y2)≈1.9< d(x1, y1)≈2.1< d(x2, y1)≈2.5
< d(x1, x2)
All linkage procedures will merge the coincident points at y1andy2(respectively) for the first
n−4merges. Denote the corresponding clusters by ˜y1and˜y2respectively. The next merge
will be z2:={x2,˜y2}in all cases. Now single linkage will join z2with ˜y1. Further, since
n≥10,n−2
2≥4and therefore the median distance between z2and˜y1is also d(y1, y2). However,
since d(x1, y1)< d(x2, y1), the complete linkage procedure will merge {x1, z2}. Finally, the two
remaining clusters are merged in each of the two procedures. Clearly, 2-pruning of TS2
cmplyieldsC2or
ℓ(TS2
cmpl,C2) = 0 . However, ℓ(TS2
sgl,C2) =ℓ(TS2
med,C2) =1
2−O(1
n) = Ω(1) .
Construction of S3,C3. We specify the points in S3using their Cartesian coordinates. We placen−1
6
points each at x1= (0,0), x′
2= (0,1 + 2 ϵ),n−1
12points each at x′
1= (0, ϵ), x2= (0,1 +ϵ),n−1
4points each at y1= (1+0 .9ϵ, ϵ), y2= (1+ ϵ,1+1.9ϵ), and one point z1= (0,2)withϵ= 0.3. With
some abuse of notation we will use the coordinate variables defined above to also denote the collection
of points at their respective locations. We set C3={{(x, y)∈S2|x≤0},{(x, y)∈S2|x >0}}.
After merging the coincident points, all procedures will merge clusters ˜x1:={x1, x′
1}and˜x2:=
{x2, x′
2, z1}. Let’s now consider the single linkage merge function. We have Dsgl(˜x1,˜x2;d) = 1
and all other cluster pairs are further apart. The next merge is therefore ˜x:={˜x1,˜x2}. Also,
Dsgl(˜x, y1;d) = 1 + 0 .9ϵ <min{Dsgl(˜x, y2;d), Dsgl(y1, y2;d)}leading to the merge {˜x, y1}, and
finally y2is merged in. A 2-pruning therefore has loss ℓ(TS3
sgl,C3) = Ω(1) . On the other hand,
Dmed(˜x1,˜x2;d) = 1 + ϵ > D med(y1, y2;d) =p
(1 + 0 .9ϵ)2+ 0.01ϵ2andDmed(˜x1, y1;d) =p
(1 + 0 .9ϵ)2+ϵ2>{Dmed(y1, y2;d), Dmed(˜x1,˜x2;d)}. As a result, median linkage would first
11Ak0-pruning for a tree Tis a partition of the points contained in T’s root into k0clusters such that each
cluster is an internal node of T.
22Figure 1: Construction of clustering instances showing the need for interpolating linkage heuristics.
We give concrete instances and target clusterings where each of two-point based linkage procedures,
i.e. single, complete and median linkage, dominates the other two.
merge {y1, y2}, followed by {˜x1,˜x2}, and 2-pruning yields C3. Complete linkage also merges
{y1, y2}first. But Dcmpl(˜x1,˜x2;d) = 2 > D cmpl(˜x1,{y1, y2};d). Thus, ℓ(TS3
cmpl,C3) = Ω(1) .
I Additional details and proofs from Section 3
Definition 4 (2-point-based merge function [ BDL20 ]).A merge function Dis2-point-based if
for any pair of clusters A, B⊆ X and any metric d, there exists a set of points (a, b)∈A×B
such that D(A, B;d) =d(a, b). Furthermore, the selection of aandbonly depend on the relative
ordering of the distances between points in AandB. More formally, for any metrics dandd′
such that d(a, b)≤d(a′, b′)if and only if d′(a, b)≤d′(a′, b′), then D(A, B;d) =d(a, b)implies
D(A, B;d′) =d′(a, b).
For instance, single, median and complete linkage are 2-point-based, since the merge function
D(A, B;d)only depends on the distance d(a, b)for some a∈A, b∈B. We have the following
observation about our parameterized algorithm families D∆
ρ(A, B;δ)when ∆consists of 2-point-
based merge functions which essentially establishes piecewise structure with linear boundaries (in
the sense of Definition 1).
Lemma I.1. Suppose S∈Πis a clustering instance, ∆is a set of 2-point-based merge functions with
|∆|=l, and δis a set of distance metrics with |δ|=m. Consider the family of clustering algorithms
with the parameterized merge function D∆
ρ(A, B;δ). The corresponding dual class function uS(·)is
piecewise constant with O(|S|4lm)linear boundaries.
Proof. Let(aij, bij, a′
ij, b′
ij)1≤i≤l,1≤j≤m⊆Sbe sequences of lmpoints each; for each such a, let
ga:P →Rdenote the function
ga(ρ) =X
i∈[l],dj∈δαi,j(ρ)(dj(aij, bij)−dj(a′
ij, b′
ij))
and let G={ga|(aij, bij, a′
ij, b′
ij)1≤i≤l,1≤j≤m⊆S}be the collection of all such linear functions;
notice that |G|=O(|S|4lm). Fix ρ, ρ′∈ P withg(ρ)andg(ρ′)having the same sign patterns for
all such g. For each A, B, A′, B′⊆S,Di∈∆, and dj∈δ, we have Di(A, B;dj) =dj(a, b)and
Di(A′, B′;dj) =dj(a′, b′)for some a, b, a′, b′∈S(since Diis 2-point-based). Thus we can write
Dρ(A, B;δ) =P
i∈[m],dj∈δαi,j(ρ)dj(aij, bij)for some aij, bij∈S; similarly, Dρ(A′, B′;δ) =P
i∈[m],dj∈δαi,j(ρ)dj(a′
ij, b′
ij)for some a′
ij, b′
ij∈S. As a result, Dρ(A, B;δ)≤Dρ(A′, B′;δ)if
and only ifX
i∈[l],dj∈δαi,j(ρ) 
dj(aij, bij)−dj(a′
ij, b′
ij)
≤0
which is exactly when ga(ρ)≤0for some sequence a. Since ga(ρ)andga(ρ′)have the same sign
pattern, we have Dρ(A, B;δ)≤Dρ(A′, B′;δ)if and only if Dρ′(A, B;δ)≤Dρ′(A′, B′;δ). Soρ
23Figure 2: The first three levels of an example execution tree of a clustering instance on four points,
with a two-parameter algorithm ( P=▲2). Successive partitions P0,P1,P2are shown at merge
levels 0,1, and 2, respectively, and the nested shapes show cluster merges.
andρ′induce the same sequence of merges, meaning the algorithm’s output is constant on each piece
induced by g, as desired.
From Lemma I.1, we obtain a bound on the number of hyperplanes needed to divide Pinto output-
constant pieces. Let Hbe a set of hyperplanes which splits Pinto output-constant pieces; then,
a naive approach to finding a dual-minimizing ρ∈ P is to enumerate all pieces generated by H,
requiring O(|H|d)runtime. However, by constructing regions merge-by-merge and successively
refining the parameter space, we can obtain a better runtime bound which is output-sensitive in the
total number of pieces.
Proof of Lemma ??.This is a simple corollary of Lemma I.1 for m= 1. In this case, we have
l=d+ 1.
Lemma I.2. Consider the family of clustering algorithms with the parameterized merge function
D1
ρ(A, B;δ). Let TS
ρdenote the cluster tree computed using the parameterized merge function
D∆
ρ(A, B;d0)on sample S. LetUbe the set of functions {uρ:S7→ℓ(TS
ρ,C)|ρ∈Rd}that
map a clustering instance StoR. The dual class U∗is(F,|S|4)-piecewise decomposable, where
F={fc:U →R|c∈R}consists of constant functions fc:uρ7→c.
The key observation for the proof comes from [ BDL20 ] where it is observed that two parameterized
distance metrics dρ1, dρ2behave identically (yield the same cluster tree) on a given dataset Sif the
relative distance for all pairs of two points (a, b),(a′, b′)∈S2×S2,dρ(a, b)−dρ(a′, b′), has the
same sign for ρ1, ρ2. This corresponds to a partition of the parameter space with |S|4hyperplanes,
with all distance metrics behaving identically in each piece of the partition. More formally, we have
Proof of Lemma I.2. LetSbe any clustering instance. Fix points a, b, a′, b′∈S. Define the linear
function ga,b,a′,b′(ρ) =P
iρi(di(a, b)−di(a′, b′)). Ifdρ(·,·)denotes the interpolated distance
metric, we have that dρ(a, b)≤dρ(a′, b′)if and only if ga,b,a′,b′(ρ)≤0. Therefore we have a
setH={ga,b,a′,b′(ρ)≤0|a, b, a′, b′∈S}of|S|4hyperplanes such that in any piece of the
sign-pattern partition of the parameter space by the hyperplanes, the interpolated distance metric
behaves identically, i.e. for any ρ, ρ′in the same piece dρ(a, b)≤dρ(a′, b′)iffdρ′(a, b)≤dρ′(a′, b′).
The resulting clustering is therefore identical in these pieces. This means that for any connected
24component R of Rd\H, there exists a real value cRsuch that uρ(s1, s2) =cRfor all ρ∈Rd. By
definition of the dual, u∗
s1,s2(uρ) =uρ(s1, s2) =cR. For each hyperplane h∈H, letg(h)∈ G
denote the corresponding halfspace. Order these k=|S|4functions arbitrarily as g1, . . . , g k. For a
given connected component RofRd\H, letbR∈ {0,1}kbe the corresponding sign pattern. Define
the function f(bR)=fcRand for bnot corresponding to any R,f(b)=f0. Thus, for each ρ∈Rd,
u∗
s1,s2(uρ) =X
b∈{0,1}kI{gi(uρ) =bi∀i∈[k]}f(b)(uρ).
Corollary I.3. For any clustering instance S∈Π, the dual class function uS(·)for the family in
Lemma I.2 is piecewise constant with O 
|S|4d
pieces.
Lemma I.4. LetS∈Πbe a clustering instance, ∆be a set of merge functions, and δbe a set
of distance metrics. Then, the corresponding dual class function uS(·)is piecewise constant with
O(16|S|)linear boundaries of pieces.
Proof. For each subset of points A, B, A′, B′⊆S, letgA,B,A′,B′:P →Rdenote the function
gA,B,A′,B′(ρ) =Dρ(A, B;δ)−Dρ(A′, B′;δ)
and let Gbe the collection of all such functions for distinct subsets A, B, A′, B′. Observe that Gis
a class of linear functions with |G| ≤ 
2|S|4= 16|S|. Suppose that for ρ, ρ′∈ P,g(ρ)andg(ρ′)
have the same sign for all g∈G; then, the ordering over all cluster pairs A, B ofDρ(A, B;δ)is the
same as that of Dρ′(A, B;δ). At each stage of the algorithm, the cluster pair A, B⊆Sminimizing
Dρ(A, B;δ)is the same as that which minimizes Dρ′(A, B;δ), so the sequences of merges produced
byρandρ′are the same. Thus the algorithm’s output is constant on the region induced by gA,B,A′,B′,
meaning uS(·)is piecewise constant on the regions induced by G, which have linear boundaries.
I.1 Execution Tree
Formally we define an execution tree (Figure 2) as follows.
Definition 5 (Execution tree) .LetSbe a clustering instance with |S|=n, and∅ ̸=P ⊆ [0,1]d.
Theexecution tree onSwith respect to Pis a depth- nrooted tree T, whose nodes are defined
recursively as follows: r= ([],P)is the root, where []denotes the empty sequence; then, for any
node v= ([u1, u2, . . . , u t],Q)∈Twitht < n−1, the children of vare defined as
children (v) =(
[u1, u2, . . . , u t,(A, B)],QA,B
:A, B⊆Sis the (t+ 1)stmerge by Aρfor
exactly the ρ∈ QA,B⊆ P, with∅ ̸=QA,B⊆ Q)
.
For an execution tree Twithv∈Tand each iwith0≤i≤n, we let Pidenote the set of Qsuch
that there exists a depth- inodev∈Tand a sequence of merges Mwithv= (M,Q). Intuitively, the
execution tree represents all possible execution paths (i.e. sequences for the merges) for the algorithm
family when run on the instance Sas we vary the algorithm parameter ρ∈ P. Furthermore, each Pi
is a subdivision of the parameter space into pieces where each piece has the first imerges constant.
We establish the execution tree captures all possible sequences of merges by some algorithm Aρ
in the parameterized family via its nodes, and each node corresponds to a convex polytope if the
parameter space Pis a convex polytope (Lemmata I.5 and I.6).
Our cell enumeration algorithm for computing all the pieces of the dual class function now simply
computes the execution tree, using Algorithm 1 to compute the children nodes for any given node,
starting with the root.
Lemma I.5. LetSbe a clustering instance and Tbe its execution tree with respect to P. Then, if a
sequence of merges M= [u1, u2, . . . , u t]is attained by Aρfor some ρ∈ P, then there exists some
v∈Tat depth twithv= (M,Q)and with Q ⊆ P being the exact set of values of ρfor which
Aρmay attain M. Conversely, for every node v= (M,Q)∈T,Mis a valid sequence of merges
attainable by Aρfor some ρ∈ P.
25Proof. We proceed by induction on t. For t= 0, the only possible sequence of merges is the empty
sequence, which is obtained for all ρ∈ P. Furthermore, the only node in Tat depth 0is the root
([],P), and the set Pis exactly where an empty sequence of merges occurs.
Now, suppose the claim holds for some t≥0. We show both directions in the induction step.
For the forward direction, let Mt+1= [u1, u2, . . . , u t, ut+1], and suppose Mt+1is attained by Aρ
for some ρ∈ P. This means that Mt= [u1, u2, . . . , u t]is attained by Aρas well; by the induction
hypothesis, there exists some node vt= (Mt,Qt)∈Tat depth t, where ρ∈ QtandQtis exactly
the set of values for which Amay attain Mt. Now, ut+1is a possible next merge by Aρfor some
ρ∈ Qt; by definition of the execution tree, this means vthas some child vt+1= (Mt+1,Qt+1)in
Tsuch that Qt+1is the set of values where ut+1is the next merge in Qt. Moreover, Qt+1is exactly
the set of values ρ∈ P for which Aρcan attain the merge sequence Mt+1. In other words for any
ρ′∈ P \ Q t+1,Aρ′cannot attain the merge sequence Mt+1. Otherwise, either some ρ′∈ P \ Q t
attains Mt+1, meaning Aρ′attains Mt(contradicting the induction hypothesis), or Aρ′attains Mt+1
for some ρ′∈ Qt+1\ Qt, contradicting the definition of Qt+1.
For the backward direction, let vt+1= (Mt+1,Qt+1)∈Tat depth t+ 1. Since vt+1is not the
root, vt+1must be the child of some node vt, which has depth t. By the induction hypothesis,
vt= (Mt,Qt), where Mt= [u1, u2, . . . , u t]is attained by Aρfor some ρ∈ P. Thus by definition
of the execution tree, Mt+1has the form [u1, u2, . . . , u t,(A, B)], for some merging of cluster pairs
(A, B)which is realizable for ρ∈ Qt. Thus Mt+1is a valid sequence of merges attainable by Aρ
for some ρ∈ P.
Lemma I.6. LetSbe a clustering instance and Tbe its execution tree with respect to P. Suppose P
is a convex polytope; then, for each v= (M,Q)∈T,Qis a convex polytope.
Proof. We proceed by induction on the tree depth t. For t= 0, the only node is ([],P), andP
is a convex polytope. Now, consider a node v∈Tat depth t+ 1; by definition of the execution
tree,v= (Mv,Qv)is the child of some node u∈T, where the depth of uist. Inductively,
we know that w= (Mw,Qw), for some convex polytope Qw. We also know Mwhas the form
Mw= [u1, u2, . . . , u t], and thus Qvis defined to be the set of points ρ∈ Q wwhere the merge
sequence Mv= [u1, u2, . . . , u t,(A, B)]is attainable for some fixed A, B⊆S. Notice that the
definition of being attainable by the algorithm Aρis that Dρ(A, B;δ)is minimized over all choices
of next cluster pairs A′, B′to merge. That is, Qvis the set of points
Qv={ρ∈ Qw|Dρ(A, B;δ)≤Dρ(A′, B′;δ)for all available cluster pairs A′, B′afterMw}
Since Dρ(A, B;δ)is an affine function of ρ, the constraint Dρ(A, B;δ)≤Dρ(A′, B′;δ)is a half-
space. In other words, Qvis the intersection of a convex polytope Qwwith finitely many half space
constraints, meaning Qvis itself a convex polytope.
It follows from Lemma I.6 that Piforms a convex subdivision of P, where each Pi+1is a refinement
ofPi; Figure 2 (in the appendix) shows an example execution tree corresponding to a partition of a
2-dimensional parameter space. From Lemma I.5, the sequence of the first imerges stays constant on
each region P∈ Pi. Our algorithm computes a representation of the execution tree of an instance
Swith respect to P; to do so, it suffices to provide a procedure to list the children of a node in the
execution tree. Then, a simple breadth-first search from the root will enumerate all the leaves in the
execution tree.
Now, our goal is to subdivide Pinto regions in which the (j+ 1)stmerge is constant. Each region
corresponds to a cluster pair being merged at step j+ 1. Since we know these regions are always
convex polytopes (Lemma I.6), we can provide an efficient algorithm for enumerating these regions.
Our algorithm provides an output-sensitive guarantee by ignoring the cluster pairs which are never
merged. Supposing there are ntunmerged clusters, we start with some point x∈Pand determine
which piece Wit is in. Then, we search for more non-empty pieces contained in Pby listing the
“neighbors” of W. The neighbors of Ware pieces inside Pthat are adjacent to W; to this end, we
will more formally define a graph GPassociated with Pwhere each vertex is a piece and two vertices
have an edge when the pieces are adjacent in space. Then we show that we can enumerate neighbors
of a vertex efficiently and establish that GPis connected. It follows that listing the pieces is simply
a matter of running a graph search algorithm from one vertex of GP, thus only incurring a cost for
each non-empty piece rather than enumerating through all n4
tpairs of pieces.
26Proof of Corollary 3.2. The key observation is that on any iteration i, the number of adjacencies
Hi=O(Ri). This is because for any region P∈ Pi,Pis a polygon divided into convex subpolygons,
and the graph GPhas vertices which are faces and edges which cross between faces. Since the
subdivision of Pcan be embedded in the plane, so can the graph GP. Thus GPis planar, meaning
Hi=O(Ri). Plugging into Theorem 3.1, noting that (n−i+ 1)2≤n2,Hi≤H, andRi≤R, we
obtain the desired runtime bound of O Pn
i=1(R+RTM)n2
=O(RTMn3).
I.2 Auxiliary lemmas and proofs of runtime bounds for our algorithm
Lemma I.7. Fix an affine function f:R→Rdviaf(x) =xa+b, fora, b∈Rdanda̸= 0d. For a
subset S⊆R, iff(S)is convex and closed, then Sis also convex and closed.
Proof. First note that fis injective, since a̸= 0d. To show convexity of S, take arbitrary x, y∈S
andλ∈[0,1]; we show that λx+ (1−λ)y∈S. Consider f(λx+ (1−λ)y):
f(λx+ (1−λ)y) = (λx+ (1−λ)y)a+b
=λ(xa+b) + (1 −λ)(ya+b)
By definition, ya+b, xa +b∈f(S), so it follows that f(λx+ (1−λ)y)∈f(S)by convexity
off(S). So there exists some z∈Swithf(z) =f(λx+ (1−λ)y), but since fis injective,
λx+ (1−λ)y=z∈S. Thus Sis convex.
To show closedness of S, we show R\Sis open. Let N(x, r)denote the open ball of radius raround
x, in either one-dimensional or d-dimensional space. Let x∈R\S; we know f(x)/∈f(S)since f
is injective. Since Rd\f(S)is open, there exists some r >0withN(f(x), r)⊆Rd\f(S). Then,
takee=r
∥a∥2>0; for every y∈N(x,e), we have
∥f(x)−f(y)∥2=∥xa+b−ya−b∥2<|x−y|∥a∥2≤r
and so f(y)∈N(f(x), r)⊆Rd\f(S), meaning y /∈Ssince fis injective. Thus N(x,e)⊆R\S,
meaning Sis closed as desired.
This allows us to prove the following key lemma. We describe a proof sketch first. For arbitrary
(i, j),(i′, j′)∈V∗
P, we show that there exists a path from (i, j)to(i′, j′)inGP. We pick arbitrary
points w∈Qi,j, x∈Qi′,j′; we can do this because by definition, V∗
Ponly has elements correspond-
ing to non-empty cluster pairs. Then, we draw a straight line segment in Pfromwtox. When we do
so, we may pass through other sets on the way; each time we pass into a new region, we traverse an
edge in GP, so the sequence of regions we pass through on this line determines a GP-path from wto
x.
Lemma I.8. The vertices V∗
Pof the region adjacency graph GPform a connected component; all
other connected components of GPare isolated vertices.
Proof. It suffices to show that for arbitrary vertices (i1, j1),(i2, j2)∈VP, there exists a path from
(i1, j1)to(i2, j2)inGP. For ease of notation, define Q1=Qi1,j1andQ2=Qi2,j2.
Fix arbitrary points u∈Q1andw∈Q2. Ifu=wthen we’re done, since the edge from (i1, j1)to
(i2, j2)exists, so suppose u̸=w. Consider the line segment Ldefined as
L={λu+ (1−λ)w:λ∈[0,1]}
Since Q1, Q2⊆P, we have u, w∈P. Furthermore, by convexity of P, it follows that L⊆P.
Define the sets Ri,jas
Ri,j=Qi,j∩L
Since each Qi,jandLare convex and closed, so is each Ri,j. Furthermore, sinceS
i,jQi,j=P, we
must haveS
i,jRi,j=L. Finally, define the sets Si,jas
Si,j={t∈[0,1] :tu+ (1−t)w∈Ri,j} ⊆[0,1]
27Note that Si,jis convex and closed; the affine map f:Si,j→Ri,jgiven by f(x) =xu+(1−x)w=
x(u−w) +whasRi,jas an image. Furthermore, u−w̸= 0d; by Lemma I.7, the preimage Si,j
must be convex and closed. Furthermore,S
i,jSi,j= [0,1].
The only convex, closed subsets of [0,1]are closed intervals. We sort the intervals in increasing order
based on their lower endpoint, giving us intervals I1, I2, . . . , I ℓ. We also assume all intervals are
non-empty (we throw out empty intervals). Let σ(p)denote the corresponding cluster pair associated
with interval Ip; that is, if the interval Ipis formed from the set Si,j, then σ(p) = (i, j).
Define ai, bito be the lower and upper endpoints, respectively, of Ii. We want to show that for all
1≤i≤ℓ−1, the edge {σ(i), σ(i+ 1)} ∈EP; this would show that σ(1)is connected to σ(ℓ)in
theVP. But σ(1) = ( i1, j1)andσ(ℓ) = (i2, j2), so this suffices for our claim.
Now consider intervals Ii= [ai, bi]andIi+1= [ai+1, bi+1]. It must be the case that bi=ai+1;
otherwise, some smaller interval would fit in the range [bi, ai+1], and it would be placed before Ii+1
in the interval ordering.
Since bi∈Ii∩Ii+1, by definition, ubi+(1−bi)w∈Rσ(i)∩Rσ(i+1). In particular, ubi+(1−bi)w∈
Qσ(i)∩Qσ(i+1); by definition of EP, this means {σ(i), σ(i+ 1)} ∈EP, as desired.
Theorem 3.1. LetSbe a clustering instance with |S|=n, and let Ri=|Pi|andR=Rn. Let
Ht={(Q1,Q2)∈ P2
t| Q1∩ Q2̸=∅}denote the total number of adjacencies between any two
pieces of PiandH=Hn. Then, the leaves of the execution tree on Scan be computed in time
˜O Pn
i=1(Hi+RiTM) (n−i+ 1)2
, where TMis the time to compute the merge function.
Proof. LetTbe the execution tree with respect to S, and let Ttdenote the vertices of Tat depth t.
From Theorem 2.2, for each node v= (M,Q)∈Twith depth t, we can compute the children of v
in time O(n2
t·LP(d, Ev) +Vv·n2
tK), where Vvis the number of children of v, and
Ev=

(Q1,Q2)∈ P2
t+1| Q1∩ Q2̸=∅and
u1= (M1,Q1), u2= (M2,Q2)
for some children u1, u2ofv

.
Now, observe
X
v∈Tt+1Ev≤Ht+1
since Ht+1counts all adjacent pieces Q1,Q2inPt+1; each pair is counted at most once by some
Ev. Similarly, we haveP
v∈Tt+1Vv≤Rt+1, since Rt+1counts the total size of Pt+1. Note that
nt+1= (n−t), since tmerges have been executed by time t+1, soni=n−i+1. Seidel’s algorithm
is a randomized algorithm that may be used for efficiently solving linear programs in low dimensions,
the expected running time for solving an LP in dvariables and mconstraints is O(d!·Ev)(also
holds with high probability, e.g. Corollary 2.1 of [ Sei91 ]). There are also deterministic algorithms
with the same (in fact slightly better) worst-case runtime bounds [ Cha18 ]. Therefore, we can set
LP(d, Ev) =O(d!·Ev). So that the total cost of computing Piis
O nX
i=1X
v∈Tid!·Ev(n−i+ 1)2+VvK(n−i+ 1)2!
=O nX
i=1(d!·Hi+RiK) (n−i+ 1)2!
as desired.
J Further details and proofs from Section 4
We provide further details for Algorithm 5 in Appendix J.2, and other proofs from Section 4 are
located in Appendix J.3.
J.1 Example dynamic programs for sequence alignment
We exhibit how two well-known sequence alignment formulations can be solved using dynamic
programs which fit our model in Section 4. In Section J.1.1 we show a DP with two free parameters
(d= 2), and in Section J.1.2 we show another DP which has three free parameters ( d= 3).
28J.1.1 Mismatches and spaces
Suppose we only have two features, mismatches andspaces . The alignment that minimizes the cost
cmay be obtained using a dynamic program in O(mn)time. The dynamic program is given by
the following recurrence relation for the cost function which holds for any i, j > 0, and for any
ρ= (ρ1, ρ2),
C(s1[:i], s2[:j], ρ) =

C(s1[:i−1], s2[:j−1], ρ) ifs1[i] =s2[j],
min
ρ1+C(s1[:i−1], s2[:j−1], ρ),
ρ2+C(s1[:i−1], s2[:j], ρ),
ρ2+C(s1[:i], s2[:j−1], ρ)	ifs1[i]̸=s2[j].
The base cases are C(ϕ, ϕ, ρ ) = 0 , C(ϕ, s2[:j], ρ) =jρ2,=C(s1[:i], ϕ, ρ) =iρ2fori, j∈[m]×[n].
Here ϕdenotes the empty sequence. One can write down a similar recurrence for computing the
optimal alignment τ(s1, s2, ρ).
We can solve the non base-case subproblems (s1[:i], s2[:j])in any non-decreasing order of i+j. Note
that the number of cases V= 2, and the maximum number of subproblems needed to compute a single
DP update L= 3(L1= 1, L2= 3). For a non base-case problem (i.e. i, j > 0) the cases are given
byq(s1[:i], s2[:j]) = 1 ifs1[i] =s2[j], and q(s1[:i], s2[:j]) = 2 otherwise. The DP update in each
case is a minimum of terms of the form cv,l(ρ,(s1[:i], s2[:j])) = ρ·wv,l+σv,l(ρ,(s1[:i], s2[:j])).
For example if q(s1[:i], s2[:j]) = 2 , we have w2,1=⟨1,0⟩andσ2,1(ρ,(s1[:i], s2[:j]))equals
C(s1[:i−1], s2[:j−1], ρ), i.e. the solution of previously solved subproblem (s1[:i−1], s2[:j−1]),
the index of this subproblem depends on l, vand index of (s1[:i], s2[:j])but not on ρitself.
J.1.2 Mismatches, spaces and gaps
Suppose we have three features, mismatches ,spaces andgaps . Typically gaps (consecutive spaces)
are penalized in addition to spaces in this model, i.e. the cost of a sequence of three consecutive gaps
in an alignment (. . . a− − − b . . . , . . . a′p q r b′. . .)would be 3ρ2+ρ3where ρ2, ρ3are costs
forspaces andgaps respectively [ KKW10 ]. The alignment that minimizes the cost cmay again be
obtained using a dynamic program in O(mn)time. We will need a slight extension of our DP model
from Section 4 to capture this. We have three subproblems corresponding to any problem in Πs1,s2
(as opposed to exactly one subproblem, which was sufficient for the example in J.1.1). We have a set
of subproblems π(s1, s2)with|π(s1, s2)| ≤3|Πs1,s2|for which our model is applicable. For each
(s1[:i], s2[:j])we can compute the three costs (for any fixed ρ)
•Cs(s1[:i], s2[:j], ρ)is the cost of optimal alignment that ends with substitution of s1[i]with
s2[j].
•Ci(s1[:i], s2[:j], ρ)is the cost of optimal alignment that ends with insertion of s2[j].
•Cd(s1[:i], s2[:j], ρ)is the cost of optimal alignment that ends with deletion of s1[i].
The cost of the overall optimal alignment is simply C(s1[:i], s2[:j], ρ) = min {Cs(s1[:i], s2[:
j], ρ), Ci(s1[:i], s2[:j], ρ), Cd(s1[:i], s2[:j], ρ)}.
The dynamic program is given by the following recurrence relation for the cost function which holds
for any i, j > 0, and for any ρ= (ρ1, ρ2, ρ3),
Cs(s1[:i], s2[:j], ρ) = min

ρ1+Cs(s1[:i−1], s2[:j−1], ρ),
ρ1+Ci(s1[:i−1], s2[:j−1], ρ),
ρ1+Cd(s1[:i−1], s2[:j−1], ρ)
Ci(s1[:i], s2[:j], ρ) = min

ρ2+ρ3+Cs(s1[:i], s2[:j−1], ρ),
ρ2+Ci(s1[:i], s2[:j−1], ρ),
ρ2+ρ3+Cd(s1[:i], s2[:j−1], ρ)
Cd(s1[:i], s2[:j], ρ) = min

ρ2+ρ3+Cs(s1[:i−1], s2[:j], ρ),
ρ2+ρ3+Ci(s1[:i−1], s2[:j], ρ),
ρ2+Cd(s1[:i−1], s2[:j], ρ)
29Algorithm 5: COMPUTE COMPACT EXECUTION DAG
1:Input : Execution DAG Ge= (Ve, Ee), problem instance (s1, s2)
P0← P
2:v1, . . . , v n←topological ordering of vertices Ve
3:fori= 1tondo
4: LetSibe the set of nodes with incoming edges to vi
5: Forvs∈Si, letPsdenote the partition corresponding to vs
6:Pi←COMPUTE OVERLAY DP({Ps|s∈Si})
7: foreachp∈ Pido
8: p′←COMPUTE SUBDIVISION DP(p,(s1, s2))
9: Pi← P i\ {p} ∪p′
10: Pi←RESOLVE DEGENERACIES DP(Pi)
11:return Partition Pn
By having three subproblems for each (s1[:i], s2[:j])and ordering the non base-case problems again
in non-decreasing order of i+j, the DP updates again fit our model (1).
J.2 Details of the Execution-DAG based algorithm
We start with some well-known terminology from computational geometry.
Definition 6. A (convex) subdivision SofP⊆Rdis a finite set of disjoint d-dimensional (convex)
sets (called cells) whose union is P. The overlay Sof subdivisions S1, . . . , S nis defined as all
nonempty sets of the formT
i∈[n]siwithsi∈Si. With slight abuse of terminology, we will refer to
closures of cells also as cells.
TheCOMPUTE OVERLAY procedure takes a set of partitions, which are convex polytopic subdivisions
ofRd, and computes their overlay. We will represent a convex polytopic subdivision as a list of cells,
each represented as a list of bounding hyperplanes. Now to compute the overlay of subdivisions
P1, . . . , P L, with lists of cells C1, . . . ,CLrespectively, we define |C1|×···×|C L|sets of hyperplanes
Hj1,...,jL={S
l∈[L]H(c(l)
jl)}, where c(l)
jlis the jl-th cell of PlandH(c)denotes the hyperplanes
bounding cell c. We compute the cells of the overlay by applying Clarkson’s algorithm [ Cla94 ] to
eachHj1,...,jL. We have the following guarantee about the running time of Algorithm 6.
Algorithm 6: COMPUTE OVERLAY DP
Input : Convex polytopic subdivisions P1, . . . , P LofRd, represented as lists Cjof hyperplanes
for each cell in the subdivision
H(c(l)
jl)←hyperplanes bounding jl-th cell of Plforl∈[L], jl∈ Cl
foreachj1, . . . , j L∈ |C1|, . . . ,|CL|do
Hj1,...,jL← {S
l∈[L]H(c(l)
jl)}
H′
j1,...,jL←CLARKSON (Hj1,...,jL)
C ← non-empty lists of hyperplanes in H′
j1,...,jLforjl∈ Cl
return Partition represented by C
Lemma J.1. LetRi,jdenote the number of pieces in P[i][j], and ˜R= max i≤m,j≤nP[i][j]. There is
an implementation of the COMPUTE OVERLAY DProutine in Algorithm 5 which computes the overlay
ofLconvex polytopic subdivisions in time O(L˜RL+1·LP(d,˜RL+1)) , which is O(d!L˜R2L+1)using
algorithms for solving low-dimensional LPs [Cha18].
Proof. Consider Algorithm 6. We apply the Clarkson’s algorithm at most ˜RLtimes, once correspond-
ing to each L-tuple of cells from the Lsubdivisions. Each iteration corresponding to cell cin the
output overlay O(corresponding to C) has a set of at most L˜Rhyperplanes and yields at most Rc
non-redundant hyperplanes. By Theorem 2.1, each iteration takes time O(L˜R·LP(d, Rc+1)) , where
LP(d, Rc+ 1) is bounded by O(d!Rc)for the algorithm of [ Cha18 ]. Note thatP
cRccorresponds to
30Algorithm 7: COMPUTE SUBDIVISION DP
Input : Convex Polytope P, problem instance (s1, s2)
v←the DP caseq((s1, s2))for the problem instance
ρ0←an arbitrary point in P
(t0
1, t0
2)←optimal alignment of (s1, s2)for parameter ρ0, using subproblem
(s1[:iv,l0], s2[:jv,l0])for some l0∈[Lv]
mark← ∅,polys ←new hashtable, poly_queue ←new queue
poly_queue .enqueue (l0)
while poly_queue .non_empty ()do
l←poly_queue .dequeue ();
Continue to next iteration if l∈mark
mark←mark∪ {l}
L ← P
forall subproblems (s1[:iv,l1], s2[:jv,l1])forl1∈[Lv], l1̸=ldo
Add the half-space inequality bTρ≤ccorresponding to
cv,l(ρ,(s1, s2))≤cv,l1(ρ,(s1, s2))toL;/* Label the constraint (b, c)with
l1*/
I←CLARKSON (L)
poly_queue .enqueue (l′)
for each l′such that the constraint labeled by it is in I
polys [l]← {L [ℓ] :ℓ∈I}
return polys
the total number of edges in the cell adjacency graph of O, which is bounded by ˜R2L. Further note that
Rc≤˜RLfor each c∈ Cand|C| ≤ ˜RLto get a runtime bound of O(L˜RL+1·LP(d,˜RL+ 1)) .
We now consider an implementation for the COMPUTE SUBDIVISION DPsubroutine. The algorithm
computes the hyperplanes across which the subproblem used for computing the optimal alignment
changes in the recurrence relation (1) by adapting Algorithm 1. We restate the algorithm in the
context of sequence alignment as Algorithm 7.
Lemma J.2. LetRi,jdenote the number of pieces in P[i][j], and ˜R= max i≤m,j≤nRi,j. There
is an implementation of COMPUTE SUBDIVISION DProutine in Algorithm 5 with running time at
mostO((L2d+2+L2d˜RL)·LP(d, L2+˜RL))for each outer loop of Algorithm 5. If the algorithm
of [Cha18] is used to solve the LP , this is at most O(d!L2d+2˜R2L).
Proof. Consider Algorithm 7. For any piece pin the overlay, all the required subproblems have a
fixed optimal alignment, and we can find the subdivision of the piece by adapting Algorithm 1 (using
O(L2+˜RL)hyperplanes corresponding to subproblems and piece boundaries). The number of
pieces in the subdivision is at most L2dsince we have at most L2hyperplanes intersecting the piece,
so we need O(L2d+2+L2d˜RL)time to list all the pieces Cp. The time needed to run Clarkson’s
algorithm is upper bounded by O(P
c∈Cp(L2+˜RL)·LP(d, Rc+ 1)) = O(P
c∈Cp(L2+˜RL)·
LP(d, L2+˜RL)) =O((L2d+2+L2d˜RL)·LP(d, L2+˜RL)). Using [ Cha18 ] to solve the LP, this
is at most O(d!˜R2LL2d+4).
Lemma J.3. LetRi,jdenote the number of pieces in P[i][j], and ˜R= max i≤m,j≤nRi,j. There is
an implementation of RESOLVE DEGENERACIES DProutine in Algorithm 5 with running time at
mostO(˜R2LL4d)for each outer loop of Algorithm 5.
Proof. The RESOLVE DEGENERACIES DPis computed by a simple BFS over the cell adjacency
graph Gc= (Vc, Ec)(i.e. the graph with polytopic cells as nodes and edges between polytopes
sharing facets). We need to find (maximal) components of a subgraph of the cell adjacency graph
where each node in the same component has the same optimal alignment. This is achieved by a
simple BFS in O(|Vc|+|Ec|)time. Indeed, by labeling each polytope with the corresponding
optimal alignment, we can compute the components of the subgraph of Gcwith edges restricted
31to nodes joining the same optimal alignment. Note that the resulting polytopic subdivision after
the merge is still a convex subdivision using arguments in Lemma J.5, but applied to appropriate
sequence alignment subproblem. As noted in the proof of Lemma J.2, we have |Vc| ≤L2d˜RL
since the number of cells within each piece pis at most L2dand there are at most ˜RLpieces in the
overlay. Since |Ec| ≤ |Vc|2, we have an implementation of RESOLVE DEGENERACIES DPin time
O((L2d˜RL)2) =O(˜R2LL4d).
Finally we can put all the above together to give a proof of Theorem 4.1.
Proof of Theorem 4.1. The proof follows by combining Lemma J.1, Lemma J.2 and Lemma J.3. Note
that in the execution DAG, we have |Ve| ≤ |Ee|=O(TDP). Further, we invoke COMPUTE OVER-
LAYDPandRESOLVE DEGENERACIES DP|Ve|times across all iterations and COMPUTE SUBDIVI -
SION DP across the |Ve|outer loops.
J.3 Additional Proofs
The following results closely follow and extend the corresponding results from [ BDD+21]. Specifi-
cally, we generalize to the case of two sequences of unequal length, and provide sharper bounds on
the number of distinct alignments and boundary functions in the piecewise decomposition (even in
the special case of equal lengths). We first have a bound on the total number of distinct alignments.
Lemma J.4. For a fixed pair of sequences s1, s2∈Σm×Σn, with m≤n, there are at most
m(m+n)mdistinct alignments.
Proof. For any alignment (t1, t2), by definition, we have |t1|=|t2|and for all i∈[|t1|], if
t1[i] =−, then t2[i]̸=−and vice versa. This implies that t1has exactly n−mmore gaps
thant2. To prove the upper bound, we count the number of alignments (t1, t2)where t2has
exactly igaps for i∈[m]. There are n+i
i
choices for placing the gap in t2. Given a fixed
t2with igaps, there are n
n−m+i
choices for placing the gap in t1. Thus, there are at most n+i
i n
n−m+i
=(n+i)!
i!(m−i)!(n−m+i)!≤(m+n)mpossibilities since i≤m. Summing over all i, we
have at most m(m+n)malignments of s1, s2.
This implies that the dual class functions are piecewise-structured in the following sense.
Lemma J.5. LetUbe the set of functions {uρ: (s1, s2)7→u(s1, s2, ρ)|ρ∈Rd}that map sequence
pairs s1, s2∈Σm×ΣntoRby computing the optimal alignment cost C(s1, s2, ρ)for a set of
features (li(·))i∈[d]. The dual class U∗is(F,G, m2(m+n)2m)-piecewise decomposable, where
F={fc:U →R|c∈R}consists of constant functions fc:uρ7→candG={gw:U → { 0,1} |
w∈Rd}consists of halfspace indicator functions gw:uρ7→I{w·ρ <0}.
Proof. Fix a pair of sequences s1ands2. Let τbe the set of optimal alignments as we range over
all parameter vectors ρ∈Rd. By Lemma J.4, we have |τ| ≤m(m+n)m. For any alignment
(t1, t2)∈τ, the algorithm Aρwill return (t1, t2)if and only if
dX
i=1ρili(s1, s2, t1, t2)>dX
i=1ρili(s1, s2, t′
1, t′
2)
for all (t′
1, t′
2)∈τ\ {(t1, t2)}. Therefore, there is a set Hof at most |τ|
2
≤m2(m+n)2m
hyperplanes such that across all parameter vectors ρin a single connected component of Rd\H,
the output of the algorithm Aρon(s1, s2)is fixed. This means that for any connected component
R ofRd\H, there exists a real value cRsuch that uρ(s1, s2) =cRfor all ρ∈Rd. By definition
of the dual, u∗
s1,s2(uρ) =uρ(s1, s2) =cR. For each hyperplane h∈H, letg(h)∈ G denote the
corresponding halfspace. Order these k= |τ|
2
functions arbitrarily as g1, . . . , g k. For a given
connected component RofRd\H, letbR∈ {0,1}kbe the corresponding sign pattern. Define the
function f(bR)=fcRand for bnot corresponding to any R,f(b)=f0. Thus, for each ρ∈Rd,
u∗
s1,s2(uρ) =X
b∈{0,1}kI{gi(uρ) =bi∀i∈[k]}f(b)(uρ).
32For the special case of d= 2, we have an algorithm which runs in time O(RT DP), where Ris the
number of pieces in P[m][n]which improves on the prior result O(R2+RT DP)for two-parameter
sequence alignment problems. The algorithm employs the ray search technique of [ Meg78 ] (also
employed by [ GBN94 ] but for more general sequence alignment problems) and enjoys the following
runtime guarantee.
Theorem J.6. For the global sequence alignment problem with d= 2, for any problem instance
(s1, s2), there is an algorithm to compute the pieces for the dual class function in O(RT DP)time,
where TDPis the time complexity of computing the optimal alignment for a fixed parameter ρ∈R2,
andRis the number of pieces of u(s1,s2)(·).
Proof. We note that for any alignment (t1, t2), the boundary functions for the piece where (t1, t2)is
an optimal alignment are straight lines through the origin of the form
ρ1l1(s1, s2, t1,t2) +ρ2l2(s1, s2, t1, t2)> ρ1l1(s1, s2, t′
1, t′
2) +ρ2l2(s1, s2, t′
1, t′
2)
for some alignment (t′
1, t′
2)different from (t1, t2). The intersection of these halfplanes is either
the empty set or the region between two straight lines through the origin. The output subdivision
therefore only consists of the axes and straight lines through the origin in the positive orthant.
We will present an algorithm using the ray search technique of [ Meg78 ]. The algorithm computes
the optimal alignment (t1, t2),(t′
1, t′
2)at points ρ= (0,1)andρ= (1,0). If the alignments are
identical, we conclude that (t1, t2)is the optimal alignment everywhere. Otherwise, we find the
optimal alignment (t′′
1, t′′
2)for the intersection of line Ljoining ρ= (0,1)andρ= (1,0), with the
lineL′given by
ρ1l1(s1, s2, t1,t2) +ρ2l2(s1, s2, t1, t2)> ρ1l1(s1, s2, t′
1, t′
2) +ρ2l2(s1, s2, t′
1, t′
2)
If(t′′
1, t′′
2) = ( t1, t2)or(t′′
1, t′′
2) = ( t′
1, t′
2), we have exactly 2 optimal alignments and the piece
boundaries are given by L′and the axes. Otherwise we repeat the above process for alignment pairs
(t′′
1, t′′
2),(t′
1, t′
2)and(t′′
1, t′′
2),(t1, t2). Notice we need to compute at most R+ 1dynamic programs
to compute all the pieces, giving the desired time bound.
33NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
•[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist" ,
•Keep the checklist subsection headings, questions/answers and guidelines below.
•Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We clear state the claims and make sure they accurately reflect our contribu-
tions.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
34Justification: We clearly state that our results lead to faster algorithms when the output-size
is small, and worst-case improved runtimes are left for future work.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Complete proofs and details are in the Appendix and appropriately referenced
from the main body.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification:
Guidelines:
35• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
36•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
37•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We follow the Ethics Guidelines.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: We view our work as theoretical, with techniques leading to faster machine
learning algorithms.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification:
Guidelines:
38• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
39Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
40