Efficient Recurrent Off-Policy RL Requires a
Context-Encoder-Specific Learning Rate
Fan-Ming Luo1,2Zuolin Tu2Zefang Huang1Yang Yu1,2∗
1National Key Laboratory for Novel Software Technology, Nanjing University, China
School of Artificial Intelligence, Nanjing University, China
2Polixir.ai
luofm@lamda.nju.edu.cn, zuolin.tu@polixir.ai, zf.frank.huang@gmail.com,
yuy@nju.edu.cn
Abstract
Real-world decision-making tasks are usually partially observable Markov decision
processes (POMDPs), where the state is not fully observable. Recent progress
has demonstrated that recurrent reinforcement learning (RL), which consists of
a context encoder based on recurrent neural networks (RNNs) for unobservable
state prediction and a multilayer perceptron (MLP) policy for decision making,
can mitigate partial observability and serve as a robust baseline for POMDP
tasks. However, prior recurrent RL algorithms have faced issues with training
instability. In this paper, we find that this instability stems from the autoregressive
nature of RNNs, which causes even small changes in RNN parameters to produce
large output variations over long trajectories. Therefore, we propose Recurrent
Off-policy RL with Context- Encoder- Specific Learning Rate (RESeL) to tackle
this issue. Specifically, RESeL uses a lower learning rate for context encoder
than other MLP layers to ensure the stability of the former while maintaining
the training efficiency of the latter. We integrate this technique into existing off-
policy RL methods, resulting in the RESeL algorithm. We evaluated RESeL in
18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios,
as well as five MDP locomotion tasks. The experiments demonstrate significant
improvements in training stability with RESeL. Comparative results show that
RESeL achieves notable performance improvements over previous recurrent RL
baselines in POMDP tasks, and is competitive with or even surpasses state-of-
the-art methods in MDP tasks. Further ablation studies highlight the necessity
of applying a distinct learning rate for the context encoder. Code is available at
https://github.com/FanmingL/Recurrent-Offpolicy-RL .
1 Introduction
In many real-world reinforcement learning (RL) tasks [ 1], complete state observations are often
unavailable due to limitations such as sensor constraints, cost considerations, or task-specific require-
ments. For example, visibility can be obstructed by obstacles in autonomous driving [ 2,3] or robotic
manipulation [ 4], and measuring ground friction may be infeasible when controlling quadruped
robots on complex terrain [ 5]. These scenarios are common and typically conceptualized as Partially
Observable Markov Decision Processes (POMDPs). Traditional RL struggles with POMDPs due to
the lack of essential state information [6].
A mainstream class of POMDP RL algorithms infers unobservable states by leveraging historical
observation information, either explicitly or implicitly [ 6]. This often requires memory-augmented
∗Yang Yu is the corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).network architectures, such as recurrent neural networks (RNNs) [ 7,8] and Transformers [ 9]. Re-
placing standard networks in RL algorithms with these memory-based structures has proven effective
in solving POMDP problems [ 10,11,12,13]. Particularly, recurrent RL [ 11], which employs an
RNN-based context encoder to extract unobservable hidden states and an MLP policy to make
decisions based on both current observation and hidden states, demonstrates robust performance in
POMDP tasks. Compared to Transformer-based RL [ 13], recurrent RL offers lower inference time
complexity, making it highly applicable, especially in resource-constrained terminal controllers.
Despite these advantages, recurrent RL faces a significant challenge: while advanced RNN archi-
tectures [ 7,8,14] can effectively address gradient explosion issues, training in recurrent RL often
remains more unstable compared to MLP-based RL, particularly when handling long sequence
lengths. This instability can lead to poor policy performance and even training divergence [ 15].
Although existing methods usually avoid training with full-length trajectories by using shorter tra-
jectory segment [ 11], this practice introduces distribution shift due to the inconsistency between
the sequence lengths used during training and deployment. ESCP [ 16] addressed this by truncating
the history length during deployment to match the training sequence length, but this may limit
policy performance due to restricted memory length. Additionally, some methods introduce auxiliary
losses to aid context encoders in learning specific information. For instance, they train alongside a
transition model, forcing the outputs of context encoder to help minimize the prediction error of this
model [ 17,18,19]. However, these methods require the RNN to learn specific information, which
may limit the RNN’s potential and may only be applicable to a subset of tasks.
In this work, we found that, with the autoregressive property of RNNs, the output variations caused
by parameter changes are amplified as the sequence length increases. As RNN parameters change,
even slight variations in the RNN output and hidden state at the initial step can become magnified
in subsequent steps. This occurs because the altered hidden state is fed back into the RNN at each
step, causing cumulative output variations. Our theoretical analysis shows that these output variations
grow with the sequence length and eventually converge. The amplified output variations can lead to
instability in the RL process. For instance, in off-policy RL algorithms, the bootstrapped update target
of the Q-function may fluctuate significantly, resulting in unstable Q-function training. To avoid
training instability caused by excessive amplification of output variations, we propose Recurrent
Off-policy RL with Context- Encoder- Specific Learning Rate (RESeL). Specifically, we employ a
lower learning rate for the RNN context encoder while keeping the learning rate for the other MLP
layers unchanged. This strategy ensures efficient training for MLPs, which do not experience the
issue of amplified output variations.
Based on the SAC framework [ 20], and incorporating context-encoder-specific learning rate as well
as the ensemble-Q mechanism from REDQ [ 21] for training stabilization, we developed the practical
RESeL algorithm. We empirically evaluated RESeL across 18 POMDP tasks, consisting of classic
POMDP, meta-RL, credit assignment scenarios, as well as five MDP locomotion tasks. In our
experiments, we first observed the increasing RNN output variations over time and demonstrated that
the context-encoder-specific learning rate can mitigate the amplification issue, significantly enhancing
the stability and efficacy of RL training. Comparative results indicate that RESeL achieves notable
performance improvements over previous recurrent RL methods in POMDP tasks and is competitive
with, or even surpasses, state-of-the-art (SOTA) RL methods in MDP tasks. Further ablation studies
highlight the necessity of applying a distinct learning rate for the context encoder.
2 Background
<latexit sha1_base64="g1osP+/+6vWQkuZiNxcPiGQQeo8=">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gFaJJlOa2iaCZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8N0zjKlOe9Fpy5+YXFpeJyaWV1bX2jvLnVykQuGW8yEQvZCYOMx1HCmypSMe+kkgejMObtcHim4+07LrNIJJdqnPLuKBgkUT9igSKqIW7KFa/qmeXOAt+CCuyqi/ILrtGDAEOOETgSKMIxAmT0XMGHh5S4LibESUKRiXPco0TanLI4ZQTEDuk7oN2VZRPaa8/MqBmdEtMrSelijzSC8iRhfZpr4rlx1uxv3hPjqe82pn9ovUbEKtwS+5dumvlfna5FoY8TU0NENaWG0dUx65Kbruibu1+qUuSQEqdxj+KSMDPKaZ9do8lM7bq3gYm/mUzN6j2zuTne9S1pwP7Pcc6C1kHVP6r6jcNK7dSOuogd7GKf5nmMGi5QR9N4P+IJz865EzuZk3+mOgWr2ca35Tx8AFe0j3o=</latexit>o<latexit sha1_base64="YMf93LdZfclIx4KksAHp43DidMc=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RSbTaRuaJmEyEUrozh9wqx8m/oH+hXfGFNQiOiHJmXPPuTP3Xi8O/EQ5zmvBWlhcWl4prq6tb2xubZd2dhtJlEou6jwKItnyWCICPxR15atAtGIp2NgLRNMbXep4817IxI/CWzWJRXfMBqHf9zlTRDU7Q6YyNr0rlZ2KY5Y9D9wclJGvWlR6QQc9ROBIMYZACEU4AENCTxsuHMTEdZERJwn5Ji4wxRp5U1IJUjBiR/Qd0K6dsyHtdc7EuDmdEtAryWnjkDwR6SRhfZpt4qnJrNnfcmcmp77bhP5enmtMrMKQ2L98M+V/fboWhT7OTQ0+1RQbRlfH8yyp6Yq+uf2lKkUZYuI07lFcEubGOeuzbTyJqV33lpn4m1FqVu95rk3xrm9JA3Z/jnMeNI4r7mnFvTkpVy/yURexjwMc0TzPUMUVaqibKh/xhGfr2pLWxMo+pVYh9+zh27IePgA6QpI5</latexit>ˆa<latexit sha1_base64="9FhKdrH/ooIWOn/H7CX1C269GRY=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RZLptB2aJmEyEUrozh9wqx8m/oH+hXfGFNQiOiHJmXPPuTP3Xj8ORKIc57VgLSwuLa8UV9fWNza3tks7u40kSiXjdRYFkWz5XsIDEfK6EirgrVhyb+wHvOmPLnW8ec9lIqLwVk1i3h17g1D0BfMUUc3O0FOZnN6Vyk7FMcueB24OyshXLSq9oIMeIjCkGIMjhCIcwENCTxsuHMTEdZERJwkJE+eYYo28Kak4KTxiR/Qd0K6dsyHtdc7EuBmdEtAryWnjkDwR6SRhfZpt4qnJrNnfcmcmp77bhP5+nmtMrMKQ2L98M+V/fboWhT7OTQ2CaooNo6tjeZbUdEXf3P5SlaIMMXEa9yguCTPjnPXZNp7E1K5765n4m1FqVu9Zrk3xrm9JA3Z/jnMeNI4r7mnFvTkpVy/yURexjwMc0TzPUMUVaqibKh/xhGfr2pLWxMo+pVYh9+zh27IePgBis5JK</latexit>ˆrContext Encoderhidden stateMLP Policy<latexit sha1_base64="+3TeCIEN7voV8CxkmD0N3+Y6DaE=">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gFaJJlOa+g0CZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8NUxFlyvNeC87c/MLiUnG5tLK6tr5R3txqZUkuGW+yRCSyEwYZF1HMmypSgndSyYNRKHg7HJ7pePuOyyxK4ks1Tnl3FAziqB+xQBHVCG7KFa/qmeXOAt+CCuyqJ+UXXKOHBAw5RuCIoQgLBMjouYIPDylxXUyIk4QiE+e4R4m0OWVxygiIHdJ3QLsry8a0156ZUTM6RdArSelijzQJ5UnC+jTXxHPjrNnfvCfGU99tTP/Qeo2IVbgl9i/dNPO/Ol2LQh8npoaIakoNo6tj1iU3XdE3d79UpcghJU7jHsUlYWaU0z67RpOZ2nVvAxN/M5ma1Xtmc3O861vSgP2f45wFrYOqf1T1G4eV2qkddRE72MU+zfMYNVygjqbxfsQTnp1zRziZk3+mOgWr2ca35Tx8ADZ0j2w=</latexit>aRNNMLP
<latexit sha1_base64="g1osP+/+6vWQkuZiNxcPiGQQeo8=">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gFaJJlOa2iaCZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8N0zjKlOe9Fpy5+YXFpeJyaWV1bX2jvLnVykQuGW8yEQvZCYOMx1HCmypSMe+kkgejMObtcHim4+07LrNIJJdqnPLuKBgkUT9igSKqIW7KFa/qmeXOAt+CCuyqi/ILrtGDAEOOETgSKMIxAmT0XMGHh5S4LibESUKRiXPco0TanLI4ZQTEDuk7oN2VZRPaa8/MqBmdEtMrSelijzSC8iRhfZpr4rlx1uxv3hPjqe82pn9ovUbEKtwS+5dumvlfna5FoY8TU0NENaWG0dUx65Kbruibu1+qUuSQEqdxj+KSMDPKaZ9do8lM7bq3gYm/mUzN6j2zuTne9S1pwP7Pcc6C1kHVP6r6jcNK7dSOuogd7GKf5nmMGi5QR9N4P+IJz865EzuZk3+mOgWr2ca35Tx8AFe0j3o=</latexit>o<latexit sha1_base64="YMf93LdZfclIx4KksAHp43DidMc=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RSbTaRuaJmEyEUrozh9wqx8m/oH+hXfGFNQiOiHJmXPPuTP3Xi8O/EQ5zmvBWlhcWl4prq6tb2xubZd2dhtJlEou6jwKItnyWCICPxR15atAtGIp2NgLRNMbXep4817IxI/CWzWJRXfMBqHf9zlTRDU7Q6YyNr0rlZ2KY5Y9D9wclJGvWlR6QQc9ROBIMYZACEU4AENCTxsuHMTEdZERJwn5Ji4wxRp5U1IJUjBiR/Qd0K6dsyHtdc7EuDmdEtAryWnjkDwR6SRhfZpt4qnJrNnfcmcmp77bhP5enmtMrMKQ2L98M+V/fboWhT7OTQ0+1RQbRlfH8yyp6Yq+uf2lKkUZYuI07lFcEubGOeuzbTyJqV33lpn4m1FqVu95rk3xrm9JA3Z/jnMeNI4r7mnFvTkpVy/yURexjwMc0TzPUMUVaqibKh/xhGfr2pLWxMo+pVYh9+zh27IePgA6QpI5</latexit>ˆa<latexit sha1_base64="9FhKdrH/ooIWOn/H7CX1C269GRY=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RZLptB2aJmEyEUrozh9wqx8m/oH+hXfGFNQiOiHJmXPPuTP3Xj8ORKIc57VgLSwuLa8UV9fWNza3tks7u40kSiXjdRYFkWz5XsIDEfK6EirgrVhyb+wHvOmPLnW8ec9lIqLwVk1i3h17g1D0BfMUUc3O0FOZnN6Vyk7FMcueB24OyshXLSq9oIMeIjCkGIMjhCIcwENCTxsuHMTEdZERJwkJE+eYYo28Kak4KTxiR/Qd0K6dsyHtdc7EuBmdEtAryWnjkDwR6SRhfZpt4qnJrNnfcmcmp77bhP5+nmtMrMKQ2L98M+V/fboWhT7OTQ2CaooNo6tjeZbUdEXf3P5SlaIMMXEa9yguCTPjnPXZNp7E1K5765n4m1FqVu9Zrk3xrm9JA3Z/jnMeNI4r7mnFvTkpVy/yURexjwMc0TzPUMUVaqibKh/xhGfr2pLWxMo+pVYh9+zh27IePgBis5JK</latexit>ˆrContext Encoderhidden stateMLP Critic<latexit sha1_base64="miSMp+FM19vNvZc6CVjDgvxrXfs=">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gFaJEmndeg0CZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8NU8Ez5XmvBWdufmFxqbhcWlldW98ob261siSXEWtGiUhkJwwyJnjMmoorwTqpZMEoFKwdDs90vH3HZMaT+FKNU9YdBYOY93kUKKIajZtyxat6ZrmzwLegArvqSfkF1+ghQYQcIzDEUIQFAmT0XMGHh5S4LibESULcxBnuUSJtTlmMMgJih/Qd0O7KsjHttWdm1BGdIuiVpHSxR5qE8iRhfZpr4rlx1uxv3hPjqe82pn9ovUbEKtwS+5dumvlfna5FoY8TUwOnmlLD6Ooi65Kbruibu1+qUuSQEqdxj+KScGSU0z67RpOZ2nVvAxN/M5ma1fvI5uZ417ekAfs/xzkLWgdV/6jqNw4rtVM76iJ2sIt9mucxarhAHU3j/YgnPDvnjnAyJ/9MdQpWs41vy3n4ABB0j1w=</latexit>QRNNMLP
<latexit sha1_base64="+3TeCIEN7voV8CxkmD0N3+Y6DaE=">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gFaJJlOa+g0CZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8NUxFlyvNeC87c/MLiUnG5tLK6tr5R3txqZUkuGW+yRCSyEwYZF1HMmypSgndSyYNRKHg7HJ7pePuOyyxK4ks1Tnl3FAziqB+xQBHVCG7KFa/qmeXOAt+CCuyqJ+UXXKOHBAw5RuCIoQgLBMjouYIPDylxXUyIk4QiE+e4R4m0OWVxygiIHdJ3QLsry8a0156ZUTM6RdArSelijzQJ5UnC+jTXxHPjrNnfvCfGU99tTP/Qeo2IVbgl9i/dNPO/Ol2LQh8npoaIakoNo6tj1iU3XdE3d79UpcghJU7jHsUlYWaU0z67RpOZ2nVvAxN/M5ma1Xtmc3O861vSgP2f45wFrYOqf1T1G4eV2qkddRE72MU+zfMYNVygjqbxfsQTnp1zRziZk3+mOgWr2ca35Tx8ADZ0j2w=</latexit>a
Figure 1: A simple recurrent
policy architecture.Partially Observable Markov Decision Processes (POMDPs)
[22,6] enhance MDPs for scenarios with limited state visibil-
ity, addressing a number of real-world decision-making chal-
lenges with imperfect or uncertain data. A POMDP is defined
by⟨S,A,O, P, O, r, γ, ρ 0⟩, consisting of state space S, action
spaceA, observation space O, state transition function P, obser-
vation function O, reward function r, discount factor γ, and initial
state distribution ρ0. Unlike MDPs, agents in POMDPs receive
observations o∈ O instead of direct state information, requiring a belief state—a probability distribu-
tion over S—to inform decisions. The objective is to search for a policy πto maximize the expected
rewards, while factoring in observation and transition uncertainties.
2Recurrent RL [23,11] involves employing RNN-based policy or critic models within the framework
of RL. Figure 1 provides a commonly utilized recurrent policy architecture. This architecture has
two main components. First, a context encoder processes the current observation o, the last action ˆa,
the reward ˆr2, and the RNN hidden state. Then, an MLP policy uses the context embedding and oto
generate actions, facilitating the extraction of non-observable states into a context embedding. We
can train the recurrent policy by incorporating it into any existing RL algorithm. During training,
recurrent RL requires batch sampling and loss computation on a trajectory-by-trajectory basis.
3 Related Work
In this work, we focus on recurrent RL methods [ 24,25,23], which leverage RNN-based models
for RL tasks. These methods have demonstrated robust performance in POMDP [ 10,22] and meta-
RL tasks [ 25,26], having shown notable success in practical applications such as robotic motion
control [27, 28, 29] and MOBA games [30].
Recurrent RL utilizes RNNs such as GRUs [ 8], LSTMs [ 7], and more recently, SSMs [ 12] to construct
policy and critic models. Typically, in recurrent actor-critic RL, both policy and critic adopt RNN
architectures [ 23,11]. When full state information is available during training, an MLP critic can
also be used instead [ 29], receiving the full state as its input. In this work, we do not assume that
complete state information can be accessed, so policy and critic models are both RNN structures.
The most direct way to train RNN models in recurrent RL is combining an RNN structure with
existing RL algorithms, such as being integrated into on-policy methods like PPO [ 28,30,12,31]
and TRPO [ 25,32] as well as off-policy methods like TD3 [ 11] and SAC [ 23]. This work follows the
line of recurrent off-policy RL [ 11,23], as we believe that existing recurrent off-policy RL algorithms
have not yet achieved their maximum potential.
Despite the simplicity of direct integration, however, training RNNs often suffers from the training
instability issue [ 12,15], especially in long sequence length. Previous methods usually use truncated
trajectory to train RNNs [ 11]. However, deploying recurrent models with full-trajectory lengths can
result in a mismatch between training and deployment. ESCP [ 16] addressed this issue by using
the same sequence lengths in both training and deployment by truncating the historical length in
deployment scenarios, but this could impair policy performance due to the restricted memory length.
On the other hand, many studies also employed auxiliary losses in addition to the RL loss for RNN
training [ 19]. These losses aim to enable RNNs to stably extract some certain unobservable state either
implicitly or explicitly. For instance, they might train the RNNs to predict transition parameters [ 33],
distinguish between different tasks [ 16,34], or accurately predict state transitions [ 17,18,19]. These
methods require the RNN to learn specific information, which may limit the RNN’s potential and
may only be applicable to a subset of tasks, reducing the algorithmic generalizability.
RESeL directly trains recurrent models using off-policy RL due to its simplicity, flexibility, and
potential high sample efficiency. The most relevant study to RESeL is [ 11], which focused on
improving recurrent off-policy RL by analyzing the hyperparameters, network design, and algorithm
choices. More in depth, RESeL studies the instability nature of previous methods and enhances the
stability of recurrent off-policy RL through a specifically designed learning rate.
4 Method
In this section, we address the training stability challenges of recurrent RL. We will first elaborate
on our model architecture in Sec. 4.1 and introduce how we address the stability issue in Sec. 4.2.
Finally, we will summarize the training procedure of RESeL in Sec. 4.3.
4.1 Model Architectures
The architectures of RESeL policy and critic models are depicted in Fig. 2. Specifically, RESeL
policy initially employs MLPs as pre-encoders to map the current and last-step observations ( oand
ˆorespectively), actions ( ˆa), and rewards ( ˆr) into hidden spaces with equal dimensions, ensuring a
balanced representation of various information types. These pre-encoded inputs are then concatenated
2The inclusion of reward ( ˆr) in policy inputs is optional and depends on the RL task throughout the paper.
3<latexit sha1_base64="g1osP+/+6vWQkuZiNxcPiGQQeo8=">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gFaJJlOa2iaCZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8N0zjKlOe9Fpy5+YXFpeJyaWV1bX2jvLnVykQuGW8yEQvZCYOMx1HCmypSMe+kkgejMObtcHim4+07LrNIJJdqnPLuKBgkUT9igSKqIW7KFa/qmeXOAt+CCuyqi/ILrtGDAEOOETgSKMIxAmT0XMGHh5S4LibESUKRiXPco0TanLI4ZQTEDuk7oN2VZRPaa8/MqBmdEtMrSelijzSC8iRhfZpr4rlx1uxv3hPjqe82pn9ovUbEKtwS+5dumvlfna5FoY8TU0NENaWG0dUx65Kbruibu1+qUuSQEqdxj+KSMDPKaZ9do8lM7bq3gYm/mUzN6j2zuTne9S1pwP7Pcc6C1kHVP6r6jcNK7dSOuogd7GKf5nmMGi5QR9N4P+IJz865EzuZk3+mOgWr2ca35Tx8AFe0j3o=</latexit>o<latexit sha1_base64="YMf93LdZfclIx4KksAHp43DidMc=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RSbTaRuaJmEyEUrozh9wqx8m/oH+hXfGFNQiOiHJmXPPuTP3Xi8O/EQ5zmvBWlhcWl4prq6tb2xubZd2dhtJlEou6jwKItnyWCICPxR15atAtGIp2NgLRNMbXep4817IxI/CWzWJRXfMBqHf9zlTRDU7Q6YyNr0rlZ2KY5Y9D9wclJGvWlR6QQc9ROBIMYZACEU4AENCTxsuHMTEdZERJwn5Ji4wxRp5U1IJUjBiR/Qd0K6dsyHtdc7EuDmdEtAryWnjkDwR6SRhfZpt4qnJrNnfcmcmp77bhP5enmtMrMKQ2L98M+V/fboWhT7OTQ0+1RQbRlfH8yyp6Yq+uf2lKkUZYuI07lFcEubGOeuzbTyJqV33lpn4m1FqVu95rk3xrm9JA3Z/jnMeNI4r7mnFvTkpVy/yURexjwMc0TzPUMUVaqibKh/xhGfr2pLWxMo+pVYh9+zh27IePgA6QpI5</latexit>ˆa<latexit sha1_base64="9FhKdrH/ooIWOn/H7CX1C269GRY=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RZLptB2aJmEyEUrozh9wqx8m/oH+hXfGFNQiOiHJmXPPuTP3Xj8ORKIc57VgLSwuLa8UV9fWNza3tks7u40kSiXjdRYFkWz5XsIDEfK6EirgrVhyb+wHvOmPLnW8ec9lIqLwVk1i3h17g1D0BfMUUc3O0FOZnN6Vyk7FMcueB24OyshXLSq9oIMeIjCkGIMjhCIcwENCTxsuHMTEdZERJwkJE+eYYo28Kak4KTxiR/Qd0K6dsyHtdc7EuBmdEtAryWnjkDwR6SRhfZpt4qnJrNnfcmcmp77bhP5+nmtMrMKQ2L98M+V/fboWhT7OTQ2CaooNo6tjeZbUdEXf3P5SlaIMMXEa9yguCTPjnPXZNp7E1K5765n4m1FqVu9Zrk3xrm9JA3Z/jnMeNI4r7mnFvTkpVy/yURexjwMc0TzPUMUVaqibKh/xhGfr2pLWxMo+pVYh9+zh27IePgBis5JK</latexit>ˆrPre-Encoder
<latexit sha1_base64="4jrxdVIJ8SqG/gynVi9hfUso63o=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RZJ02g5NkzCZCCV05w+41Q8T/0D/wjvjFNQiOiHJmXPPuTP3Xj8JeSod57VgLSwuLa8UV9fWNza3tks7u400zkTA6kEcxqLleykLecTqksuQtRLBvLEfsqY/ulTx5j0TKY+jWzlJWHfsDSLe54EniWp2hp7M4+ldqexUHL3seeAaUIZZtbj0gg56iBEgwxgMESThEB5Setpw4SAhroucOEGI6zjDFGvkzUjFSOERO6LvgHZtw0a0VzlT7Q7olJBeQU4bh+SJSScIq9NsHc90ZsX+ljvXOdXdJvT3Ta4xsRJDYv/yzZT/9alaJPo41zVwqinRjKouMFky3RV1c/tLVZIyJMQp3KO4IBxo56zPtvakunbVW0/H37RSsWofGG2Gd3VLGrD7c5zzoHFccU8r7s1JuXphRl3EPg5wRPM8QxVXqKGuq3zEE56ta0tYEyv/lFoF49nDt2U9fABbkJJH</latexit>ˆoxxMLP Critics
: MLPx: concatenationContext Encoder (CE)Critic Architecture
<latexit sha1_base64="+3TeCIEN7voV8CxkmD0N3+Y6DaE=">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gFaJJlOa+g0CZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8NUxFlyvNeC87c/MLiUnG5tLK6tr5R3txqZUkuGW+yRCSyEwYZF1HMmypSgndSyYNRKHg7HJ7pePuOyyxK4ks1Tnl3FAziqB+xQBHVCG7KFa/qmeXOAt+CCuyqJ+UXXKOHBAw5RuCIoQgLBMjouYIPDylxXUyIk4QiE+e4R4m0OWVxygiIHdJ3QLsry8a0156ZUTM6RdArSelijzQJ5UnC+jTXxHPjrNnfvCfGU99tTP/Qeo2IVbgl9i/dNPO/Ol2LQh8npoaIakoNo6tj1iU3XdE3d79UpcghJU7jHsUlYWaU0z67RpOZ2nVvAxN/M5ma1Xtmc3O861vSgP2f45wFrYOqf1T1G4eV2qkddRE72MU+zfMYNVygjqbxfsQTnp1zRziZk3+mOgWr2ca35Tx8ADZ0j2w=</latexit>a<latexit sha1_base64="NDRwg/htFZ3GZJGVfDtM253sGVY=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRIR7bLoxo1QwT6gFkmm0zqYF5mJUKtLf8Ct/pf4B/oX3hlTUIvohCRnzj3nztx7/SQQUjnOa8GamZ2bXygulpaWV1bXyusbLRlnKeNNFgdx2vE9yQMR8aYSKuCdJOVe6Ae87V8f63j7hqdSxNG5GiW8F3rDSAwE8xRRnQslQi7t2mW54lQds+xp4Oaggnw14vILLtBHDIYMITgiKMIBPEh6unDhICGuhzFxKSFh4hz3KJE3IxUnhUfsNX2HtOvmbER7nVMaN6NTAnpTctrYIU9MupSwPs028cxk1uxvuccmp77biP5+niskVuGK2L98E+V/fboWhQFqpgZBNSWG0dWxPEtmuqJvbn+pSlGGhDiN+xRPCTPjnPTZNh5pate99Uz8zSg1q/cs12Z417ekAbs/xzkNWntV96Dqnu1X6kf5qIvYwjZ2aZ6HqOMEDTTNHB/xhGfr1JLWrXX3KbUKuWcT35b18AEgWZIn</latexit>⇥8<latexit sha1_base64="6QteD9i0DfmzVxIPW59HgQYEb8Y=">AAAC/XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl041LBaqGRkqRjHcyLyUQspfgn7tyJW3/ArW7FP9C/8M40FbWITkjm3nPPOZk710sCnkrLei0YY+MTk1PF6dLM7Nz8Qnlx6TiNM+Gzuh8HsWh4bsoCHrG65DJgjUQwN/QCduJd7Kn6ySUTKY+jI9lN2GnodiJ+xn1XEtQq7zge6/Co54WuFPyqXzpsWabj0GarzWnHMh3k2yWHRe1PYqtcsaqWXuZoYOdBBfk6iMsvcNBGDB8ZQjBEkBQHcJHS04QNCwlhp+gRJijius7QR4m0GbEYMVxCL+jboayZoxHlyjPVap/+EtArSGlijTQx8QTF6m+mrmfaWaG/efe0pzpbl3Yv9woJlTgn9C/dkPlfnepF4gw7ugdOPSUaUd35uUumb0Wd3PzSlSSHhDAVt6kuKPa1cnjPptakund1t66uv2mmQlXu59wM7+qUNGD75zhHg+ONqr1VtQ83K7XdfNRFrGAV6zTPbdSwjwPUyfsGj3jCs3Ft3Bp3xv2AahRyzTK+LePhA/MYpKw=</latexit>2664Q0Q1...Q73775Hidden StateRNN
: RNNRNN
<latexit sha1_base64="g1osP+/+6vWQkuZiNxcPiGQQeo8=">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gFaJJlOa2iaCZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8N0zjKlOe9Fpy5+YXFpeJyaWV1bX2jvLnVykQuGW8yEQvZCYOMx1HCmypSMe+kkgejMObtcHim4+07LrNIJJdqnPLuKBgkUT9igSKqIW7KFa/qmeXOAt+CCuyqi/ILrtGDAEOOETgSKMIxAmT0XMGHh5S4LibESUKRiXPco0TanLI4ZQTEDuk7oN2VZRPaa8/MqBmdEtMrSelijzSC8iRhfZpr4rlx1uxv3hPjqe82pn9ovUbEKtwS+5dumvlfna5FoY8TU0NENaWG0dUx65Kbruibu1+qUuSQEqdxj+KSMDPKaZ9do8lM7bq3gYm/mUzN6j2zuTne9S1pwP7Pcc6C1kHVP6r6jcNK7dSOuogd7GKf5nmMGi5QR9N4P+IJz865EzuZk3+mOgWr2ca35Tx8AFe0j3o=</latexit>o<latexit sha1_base64="YMf93LdZfclIx4KksAHp43DidMc=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RSbTaRuaJmEyEUrozh9wqx8m/oH+hXfGFNQiOiHJmXPPuTP3Xi8O/EQ5zmvBWlhcWl4prq6tb2xubZd2dhtJlEou6jwKItnyWCICPxR15atAtGIp2NgLRNMbXep4817IxI/CWzWJRXfMBqHf9zlTRDU7Q6YyNr0rlZ2KY5Y9D9wclJGvWlR6QQc9ROBIMYZACEU4AENCTxsuHMTEdZERJwn5Ji4wxRp5U1IJUjBiR/Qd0K6dsyHtdc7EuDmdEtAryWnjkDwR6SRhfZpt4qnJrNnfcmcmp77bhP5enmtMrMKQ2L98M+V/fboWhT7OTQ0+1RQbRlfH8yyp6Yq+uf2lKkUZYuI07lFcEubGOeuzbTyJqV33lpn4m1FqVu95rk3xrm9JA3Z/jnMeNI4r7mnFvTkpVy/yURexjwMc0TzPUMUVaqibKh/xhGfr2pLWxMo+pVYh9+zh27IePgA6QpI5</latexit>ˆa<latexit sha1_base64="9FhKdrH/ooIWOn/H7CX1C269GRY=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RZLptB2aJmEyEUrozh9wqx8m/oH+hXfGFNQiOiHJmXPPuTP3Xj8ORKIc57VgLSwuLa8UV9fWNza3tks7u40kSiXjdRYFkWz5XsIDEfK6EirgrVhyb+wHvOmPLnW8ec9lIqLwVk1i3h17g1D0BfMUUc3O0FOZnN6Vyk7FMcueB24OyshXLSq9oIMeIjCkGIMjhCIcwENCTxsuHMTEdZERJwkJE+eYYo28Kak4KTxiR/Qd0K6dsyHtdc7EuBmdEtAryWnjkDwR6SRhfZpt4qnJrNnfcmcmp77bhP5+nmtMrMKQ2L98M+V/fboWhT7OTQ2CaooNo6tjeZbUdEXf3P5SlaIMMXEa9yguCTPjnPXZNp7E1K5765n4m1FqVu9Zrk3xrm9JA3Z/jnMeNI4r7mnFvTkpVy/yURexjwMc0TzPUMUVaqibKh/xhGfr2pLWxMo+pVYh9+zh27IePgBis5JK</latexit>ˆrPre-Encoder
<latexit sha1_base64="4jrxdVIJ8SqG/gynVi9hfUso63o=">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoioy6IbFy4q2Ae0RZJ02g5NkzCZCCV05w+41Q8T/0D/wjvjFNQiOiHJmXPPuTP3Xj8JeSod57VgLSwuLa8UV9fWNza3tks7u400zkTA6kEcxqLleykLecTqksuQtRLBvLEfsqY/ulTx5j0TKY+jWzlJWHfsDSLe54EniWp2hp7M4+ldqexUHL3seeAaUIZZtbj0gg56iBEgwxgMESThEB5Setpw4SAhroucOEGI6zjDFGvkzUjFSOERO6LvgHZtw0a0VzlT7Q7olJBeQU4bh+SJSScIq9NsHc90ZsX+ljvXOdXdJvT3Ta4xsRJDYv/yzZT/9alaJPo41zVwqinRjKouMFky3RV1c/tLVZIyJMQp3KO4IBxo56zPtvakunbVW0/H37RSsWofGG2Gd3VLGrD7c5zzoHFccU8r7s1JuXphRl3EPg5wRPM8QxVXqKGuq3zEE56ta0tYEyv/lFoF49nDt2U9fABbkJJH</latexit>ˆoxContext Encoder (CE)xMLP Policy<latexit sha1_base64="+3TeCIEN7voV8CxkmD0N3+Y6DaE=">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gFaJJlOa+g0CZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8NUxFlyvNeC87c/MLiUnG5tLK6tr5R3txqZUkuGW+yRCSyEwYZF1HMmypSgndSyYNRKHg7HJ7pePuOyyxK4ks1Tnl3FAziqB+xQBHVCG7KFa/qmeXOAt+CCuyqJ+UXXKOHBAw5RuCIoQgLBMjouYIPDylxXUyIk4QiE+e4R4m0OWVxygiIHdJ3QLsry8a0156ZUTM6RdArSelijzQJ5UnC+jTXxHPjrNnfvCfGU99tTP/Qeo2IVbgl9i/dNPO/Ol2LQh8npoaIakoNo6tj1iU3XdE3d79UpcghJU7jHsUlYWaU0z67RpOZ2nVvAxN/M5ma1Xtmc3O861vSgP2f45wFrYOqf1T1G4eV2qkddRE72MU+zfMYNVygjqbxfsQTnp1zRziZk3+mOgWr2ca35Tx8ADZ0j2w=</latexit>a: MLP: RNNx: concatenationPolicy Architecture
Hidden StateRNNRNNFigure 2: Policy and critic architectures of RESeL.
into a vector and inputted into a context encoder. For the context encoder, RESeL utilizes RNN
such as GRU [ 8] and Mamba [ 14], to mix sequential information and extract non-observable state
information from the observable context. The MLP policy comprises a two-layer architecture, each
layer consisting of 256 neurons. The architecture of the pre-encoder and context encoder for the
RESeL critic mirrors that of the policy. To improve training stability, RESeL adopts the ensemble-Q
technique from REDQ [ 21], employing 8 MLP critic models, and each one of them comprises two
hidden layers with 256 neurons.
4.2 Stabilizing Training with a Context-Encoder-Specific Learning Rate
To locate the source of instability, we first formalize the RNN and its sequence generation process.
An RNN network fθ(xt, ht) :X × H → Y × H is defined as: yt, ht+1=fθ(xt, ht), where
θrepresents the network parameters, xt,yt, and htare the RNN input, output, and hidden state,
respectively. For simplicity, we denote yt=fθ
y(xt, ht)andht+1=fθ
h(xt, ht). Given an input
sequence {x0, x1, . . . , x T}of length T+ 1 and an initial hidden state h0, the output sequence
generated by fθis denoted as {y0, y1, . . . , y T}. Let θ′be the parameter neighboring θafter a one-
step gradient update. The output sequence produced by fθ′is denoted as {y′
0, y′
1, . . . , y′
T}. With
certain assumptions, we can derive Proposition 1, which bounds the difference between ytandy′
tat
any time step t.
Proposition 1. Assuming fθandfθ′both satisfy Lipschitz continuity, i.e., for all ˆθ∈ {θ, θ′},x∈ X,
h, h′∈ H, there exist constants Kh∈[0,1)andKy∈Rsuch that:
fˆθ
h(x, h)−fˆθ
h(x, h′)≤Kh∥h−h′∥,fˆθ
y(x, h)−fˆθ
y(x, h′)≤Ky∥h−h′∥,
and for all x∈ X,h∈ H, the output differences between the RNN parameterized by θandθ′are
bounded by a constant ϵ∈R:
maxfθ
h(x, h)−fθ′
h(x, h),fθ
y(x, h)−fθ′
y(x, h)
≤ϵ,
the RNN output difference at t-th step is bounded by
∥yt−y′
t∥ ≤ Ky1−Kt
h
1−Kh|{z}
amplification factorϵ+ϵ,∀t≥0. (1)
Refer to Appendix A for a detailed proof. Proposition 1 focuses on the case where Kh∈[0,1), as
most RNN models adhere to this condition. GRU [ 8] and LSTM [ 7] meet this requirement through
the use of sigmoid activation functions, while Mamba [ 14] achieves it by constraining the values in
the hidden state’s transition matrix.
InEq. (1) ,ϵrepresents the maximum variation of the model output when the model parameters
are modified and both the model input and hidden state remain constant, i.e., the first-step output
variation. However, from Eq. (1) , we observe that due to the variability in the hidden state, the final
variation in the model output is amplified. This amplification factor is minimal at t= 0, being 0.
Astincreases, this amplification factor gradually increases and ultimately converges to β:=Ky
1−Kh.
Additionally, it can be verified that as tincreases, the upper bound of the average variation in network
output1
tPt−1
i=0∥yi−y′
i∥also increases with time step tand eventually converges to βϵ+ϵ(proved
4in Appendix B), which is again amplified by β. This conclusion indicates that, compared to the case
of a sequence length of 1, in scenarios with longer sequence lengths, the average variations in the
RNN output induced by gradient descent are amplified.
This issue is different from the gradient explosion that commonly occurs in RNNs over long se-
quences [ 7]. Proposition 1 indicates that the variation in network output is amplified by a constant
factor, rather than undergoing exponential explosion. This type of instability is not very significant in
supervised learning but can lead to instability in RL processes. For instance, in off-policy RL algo-
rithms, the optimization target of the Q-function minimizes the difference with a bootstrapped target.
If the network output varies greatly, this target will fluctuate dramatically, making the Q-function
training unstable, which in turn can lead to overall training instability.
To ensure stable training, it is required to use a smaller learning rate to counterbalance the amplified
variations in network output caused by long sequence lengths, as a smaller learning rate can help
reduce ϵ. However, the output variations in MLPs are not amplified in the same way as in RNNs. If
MLPs are also set with a small learning rate, their learning efficiency could be significantly reduced,
as the learning rate may become too slow for MLPs, resulting in inefficient training and poor overall
performance. Consequently, we propose to use a context-encoder-specific learning rate for the context
encoder. In our implementation, we use a smaller learning rate LRCEfor the context encoder, which
contains the RNN architecture. For other layers, we use a normal learning rate LRother , e.g. 3×10−4.
Applying different learning rates to different modules is similar to Two-Timescale Network [ 35],
which has been shown to improve the training convergence.
4.3 Training Procedure of RESeL
Based on the SAC framework [ 20], combining context-encoder-specific learning rate and the
ensemble-Q mechanism from REDQ [ 21], we developed the practical RESeL algorithm. Specifically,
we initially configure context-encoder-specific optimizers for both the policy and value models.
After each interaction with the environment, RESeL samples a batch of data from the replay buffer,
containing several full-length trajectories. This batch is used to compute the critic loss according
to REDQ, and the critic network is optimized using its context-encoder-specific optimizer. Notably,
unlike REDQ, we did not adopt a update-to-data ratio greater than 1. Instead of that, we update the
network model once per interaction. The policy network is updated every two critic updates. During
the policy update, the policy loss from REDQ is used, and the policy network is optimized with
its context-encoder-specific optimizer. Here, we delay the policy update by one step to allow more
thoroughly training of the critic before updating the policy, which is inspired by TD3 [ 36] to improve
the training stability. The detailed algorithmic procedure is provided in Appendix C and Algorithm 1.
5 Experiments
To validate our argument and compare RESeL with other algorithms, we conducted a series of valida-
tion and comparison experiments across various POMDP environments. We primarily considered
three types of environments: classical partially observable environments, meta-RL environments,
and credit assignment environments. To assess the generalizablity of RESeL to MDP tasks, we also
test RESeL in five MuJoCo [ 37] locomotion tasks with full observation provided. We implement
the policy and critic models with a parallelizable RNN, i.e., Mamba [ 14] to accelerate the training
process. A detailed introduction of the network architecture can be found in Appendix D.3.2.
In all experiments, the RESeL algorithm was repeated six times using different random seeds, i.e.
1–6. All experiments were conducted on a workstation equipped with an Intel Xeon Gold 5218R
CPU, four NVIDIA RTX 4090 GPUs, and 250GB of RAM, running Ubuntu 20.04. For more detailed
experimental settings, please refer to Appendix D.
5.1 Training Stability
The updates of the RNN lead to large variations in the model output. In Sec. 4.2, we observe that
the variations in model output induced by a model update are amplified. In this part, we quantify the
amplification factor, assess its impact on RL training, and discover how a context-encoder-specific
learning rate addresses this issue. We use a POMDP task, specifically WalkerBLT-V-v0, to validate
our argument. We loaded a policy model trained by RESeL and updated it with various LRCEand
5Figure 3: Action variations as the rollout step increases after a single gradient update with different
values of LRCEandLRother . Action variation refers to the change in policy output after the gradient
update compared to its output before the update, using the same input sequences.
LRother for a one-step gradient update. The output differences for all rollout steps between the pre-
and post-update models are presented in Fig. 3. The right panel shows the variations in model output
as the rollout step increases, while the left panel zooms in on the rollout steps from 0 to 75.
In the zoom-in panel, comparing the brown-circle and yellow-triangle curves reveals that at the first
rollout step, the two curves are very close. This indicates that with the same learning rate, identical
input, and hidden states, the updates of RNN and MLP have almost the same impact on the model’s
output. However, as the rollout steps increase, the brown-circle curve gradually rises, increasing
by 400% at the 20-th step. This observation aligns with Proposition 1, demonstrating that changes
in the hidden state lead to increasing variations in the model’s output. With further increases in
rollout length, the right panel shows that the brown-circle curve converges at around 0.4 after 250
steps, indicating that the amplification of the output variations eventually converges. This also meets
Proposition 1. Ultimately, the increase in action variations is approximately tenfold.
This action variation magnitude is equivalent to an MLP policy network (the gray-dashed curve) with
a learning rate increased twentyfold. At this point, the learning rate reaches 0.006, which is typically
avoided in RL scenarios due to the risk of training instability. Conversely, reducing LRCEto10−5
(the purple-diamond curve) significantly suppresses the amplification of variations. The right panel
shows that the orange and purple curves remain at similar levels until the final time step.
Large output variations lead to instability in RL training. To investigate how the large variations
influence the RL process, we visualized the gradient norm and value function loss during training
for two POMDP tasks, as shown in Figs. 4 and 5. We fixed LRother = 3×10−4and compared
the training processes of LRCE= 10−5(red line) and 3×10−4(orange line). For the latter, we
applied gradient clipping; otherwise, training would diverge and stop early. The orange line in Fig. 4
shows the norm of the policy gradient before gradient rescaling, where significant oscillations appear
in the gradient in the later stages of training. The orange gradient norm is eventually scaled to
0.5, consistently smaller than the red line, which does not show oscillations. This suggests that the
late-stage instability of the orange line does not stem from large gradient magnitudes.
On the other hand, as seen in Fig. 5, the value loss for the orange line remains consistently high, with
extremely large values appearing in later stages. This is due to large variations in the RNN output,
which lead to an unstable bootstrapped update target for the value function. Consequently, the value
loss diverges, causing the abnormal gradient norm observed in Fig. 4. The results in Figs. 4 and 5
reveal the unique challenges of using RNN structures in RL, where traditional RNN stabilization
techniques may no longer be effective.
Using a context-encoder-specific learning rate improves training stability, while traditional
RNN stabilization techniques fall short. To further examine the impact of a lower learning rate on
the stability of RL training, we conducted experiments across several POMDP tasks. Building on the
setup in Fig. 4, we further evaluated a variant without the gradient clipping technique and another that
replaces gradient clipping with a truncation of recurrent backpropagation steps to 32 (green line). For
reference, we also included a variant where the CE output is entirely masked to zero (purple line).
Comparing the vanilla RNN-based RL (blue line) with the purple line shows that introducing an
RNN can partially address partial observability issues and improve policy performance. However, the
blue line still exhibits instability, even triggering early stopping due to outliers. Setting a specific
6learning rate for the CE (red line) significantly enhances training stability, indicating that reducing
the RNN learning rate indeed improves overall RL training stability. However, we also found that
traditional RNN stabilization techniques do not reliably improve RL training stability. For example,
truncating the number of recurrent backpropagation steps (green line) can still lead to early stopping,
while clipping the gradient norm (orange line) prevents early stopping but does not improve the
policy performance. This is because these two stabilization techniques can only suppress gradient
explosions in the RNN, keeping the RNN gradients within a normal range. However, the output of
the RNN can still vary significantly, which continues to cause instability in RL.
0.0 0.5 1.0 1.5
timestep 1e60.01.53.04.56.0policy grad. norm
HalfCheetahBLT-V-v0
LRCE=105, no_grad_norm_clip
LRCE=3×104, max_grad_norm=0.5
0.0 0.5 1.0 1.5
timestep 1e636912
WalkerBLT-V-v0
Figure 4: L2-norm of the policy gradient for
different RNN stabilization approaches.
0.0 0.5 1.0 1.5
timestep 1e6101102103104value loss
HalfCheetahBLT-V-v0
LRCE=105, no_grad_norm_clip
LRCE=3×104, max_grad_norm=0.5
0.0 0.5 1.0 1.5
timestep 1e6101102103
WalkerBLT-V-v0Figure 5: Value losses in log-scale for differ-
ent RNN stabilization approaches.
0.0 0.5 1.0 1.5
timestep 1e6060012001800return
AntBLT-V-v0
LRCE=105
LRCE=3×104
LRCE=3×104, grad_step_truncation
LRCE=3×104, grad_max_norm=0.5
NO_CE0.0 0.5 1.0 1.5
timestep 1e61000
0100020003000
HalfCheetahBLT-V-v0
0.0 0.5 1.0 1.5
timestep 1e6080016002400
HopperBLT-V-v0
0.0 0.5 1.0 1.5
timestep 1e60500100015002000
WalkerBLT-V-v0
Figure 6: Learning curves in four POMDP tasks with different learning rates and RNN stabilization
techniques, shaded with one standard error. We fixed LRother= 3×10−4. Some learning curves in
AntBLT-V and HalfCheetahBLT-V are incomplete as some runs encountered infinite or NaN outputs.
5.2 Performance Comparisons
In this part, we compare RESeL with previous methods in various tasks. The detailed introduction of
the baselines and tasks can be found in Appendix D.2 and Appendix D.1. More comparative results
can be found in Appendix E.3.
Classic POMDP Tasks. We first compare RESeL with previous baselines in four PyBullet locomotion
environments: AntBLT, HalfCheetahBLT, HopperBLT, and WalkerBLT. To create partially observable
tasks, we obscure part of the state information as done in previous studies [ 18,11,15], preserving
only position (-P tasks) or velocity (-V tasks) information from the original robot observations.
We benchmark RESeL against prior model-free recurrent RL (MF-RNN) [ 11], VRM [ 18], and
GPIDE [ 38]. GPIDE, which extracts historical features inspired by the principle of PID [ 39]
controller, is the state-of-the-art (SOTA) method for these tasks.
The comparative results are shown in Fig. 7. RESeL demonstrates significant improvements over
previous recurrent RL methods (MF-RNN, PPO-GRU, and A2C-GRU) in almost all tasks, except
for HalfCheetahBLT-P where its performance is close to that of MF-RNN. These results highlight
the advantage of RESeL in classic POMDP tasks over previous recurrent RL methods. Furthermore,
RESeL outperforms GPIDE in most tasks, establishing it as the new SOTA method. The learning
curves of RESeL are more stable than that of GPIDE, suggesting that fine-grained feature design could
also introduce training instability, while a stably trained RNN can even achieve superior performance.
Dynamics-Randomized Tasks. Instead of directly obscuring part of the immediate state, meta-RL
considers a different type of POMDP. In meta-RL scenarios [ 26], the agent learns across various
70.0 0.5 1.0 1.5
timestep 1e6080016002400return
AntBLT-P-v0
0.0 0.5 1.0 1.5
timestep 1e60500100015002000
AntBLT-V-v0
0.0 0.5 1.0 1.5
timestep 1e61500
015003000
HalfCheetahBLT-P-v0
0.0 0.5 1.0 1.5
timestep 1e61500
015003000
HalfCheetahBLT-V-v0
0.0 0.5 1.0 1.5
timestep 1e60800160024003200return
HopperBLT-P-v0
RESeL (ours)
PPO-GRUMF-RNN
SAC-TransformerSAC-MLP
TD3-MLPGPIDE-ESS
VRMA2C-GRU0.0 0.5 1.0 1.5
timestep 1e6080016002400
HopperBLT-V-v0
0.0 0.5 1.0 1.5
timestep 1e6080016002400
WalkerBLT-P-v0
0.0 0.5 1.0 1.5
timestep 1e6060012001800
WalkerBLT-V-v0Figure 7: Learning curves shaded with one standard error in classic POMDP tasks.
tasks with different dynamics or reward functions [ 16,19], where the parameters of the dynamics and
reward functions are not observable. We first compare RESeL with previous methods in the dynamics
randomization meta-RL tasks. Following previous meta-RL work [ 16], we randomized the gravity in
MuJoCo environments [ 37]. We created 60 dynamics functions with different gravities, using the
first 40 for training and the remaining for testing. The gravity is unobservable, requiring the agents to
infer it from historical experience. We compare RESeL to various meta-RL methods in these tasks,
including ProMP [ 40], ESCP [ 16], OSI [ 33], EPI [ 41], and PEARL [ 42]. Notably, ESCP, EPI, and
OSI use recurrent policies with different auxiliary losses for RNN training. ESCP and PEARL are
previous SOTA methods for these tasks. The comparative results are shown in Fig. 8.
We find that in Ant, Humanoid, and Walker2d, RESeL demonstrates significant improvement over
all other methods. In Hopper, RESeL performs on par with ESCP, while surpassing other methods.
Specifically, RESeL outperforms SAC-RNN by a large margin, further highlighting its advantages.
Moreover, ESCP, EPI, and OSI use RNNs to extract dynamics-related information, with ESCP having
inferred embeddings highly related to the true environmental gravity. The advantages of RESeL
over these methods suggest that the context encoder of RESeL may extract not only gravity (see
Appendix E.1) but also other factors that help the agent achieve higher returns.
0 1 2
timestep 1e60200040006000return
DM_Ant_gravity-v2
RESeL (ours) SAC-MLP SAC-GRU ESCP PEARL EPI OSI ProMP0 1 2
timestep 1e60250050007500
DM_HalfCheetah_gravity-v2
0 1 2
timestep 1e60100020003000
DM_Hopper_gravity-v2
0 1 2
timestep 1e60200040006000
DM_Humanoid_gravity-v2
0 1 2
timestep 1e601500300045006000
DM_Walker2d_gravity-v2
Figure 8: Learning curves shaded with one standard error in dynamics-randomized tasks.
Classic meta-RL Tasks. We further compare RESeL with baselines in other meta-RL tasks, particu-
larly the tasks possessing varying reward functions, sourced from [ 19,11]. In AntDir, CheetahDir,
and HalfCheetahVel, the robots’ target moving direction or target velocity are changeable and
unobservable. Agents must infer the desired moving direction or target velocity from their histori-
cal observations and rewards. Wind is a non-locomotion meta-RL task with altered dynamics. We
compare RESeL to previous meta-RL methods, including RL2[25], VariBad [ 19], and MF-RNN [ 11].
The learning curves of these methods are shown in Fig. 9. In the AntDir and HalfCheetahDir tasks,
we found that RESeL can converge within 5M steps. Our results show that RESeL demonstrates high
sample efficiency and superior asymptotic performance in AntDir, CheetahDir, and HalfCheetahVel.
In Wind, RESeL performs comparably to MF-RNN, as this task is less complex. These findings
suggest that RESeL effectively generalizes to POMDP tasks with hidden reward functions.
80 1 2
timestep 1e7600120018002400return
AntDir-v0
RESeL (ours) MF-RNN SAC-MLP VariBad-Onpolicy RL2VariBad-Offpolicy0 1 2
timestep 1e70150030004500
CheetahDir-v0
0 2 4
timestep 1e6600
400
200
0
HalfCheetahVel-v0
0 2 4 6
timestep 1e50204060
Wind-v0Figure 9: Learning curves shaded with one standard error in meta-RL tasks.
100 200 300 400 500
(Easy)  Credit assignment length  (Hard)
0.00.51.0success rate
Key-to-Door
RESeL (ours) MF-GPT MF-RNN
Figure 10: Success rate in the key-
to-door task with different credit as-
signment lengths.Credit Assignment Tasks. A notable application of recurrent
RL is solving credit assignment problem [ 11,15]. We also
compare RESeL with MF-GPT [ 15] and MF-RNN [ 11] in
a credit assignment task, namely Key-to-Door. In this task,
it is required to assign a reward obtained at the last-step to
an early action. We compared RESeL with algorithms MF-
GPT and MF-RNN. As did in [ 15], we test RESeL with credit
assignment length of [60,120,250,500]. the hardness of the
task grows as the length increases. In this task, the methods
are evaluated by success rate. The results are shown in Fig. 10.
The success rate of RESeL matches the previous SOTA MF-
GPT in tasks with [60,120] lengths, closed to 100% success
rate. For harder tasks with the lengths being [250, 500], RESeL reached higher success rates than
the others. The results indicate that RESeL outperforms Transformer-based methods in handling
challenging credit assignment tasks.
Classic MuJoCo Locomotion Tasks. Finally, we would like to discover how RESeL performs in
MDP tasks. We adopt five classic MuJoCo tasks and compare RESeL with previous RL methods,
e.g., SAC [ 20], TD3 [ 36], and TD7 [ 43]. As the current SOTA method for these environment,
TD7 introduces several enhancements based on TD3, e.g., a representation learning method and a
checkpoint trick, which are not existing in RESeL. The results, listed in Table 1, show that RESeL is
comparable to TD7, showing only a 4.8% average performance drop. This demonstrates that RESeL
is effective enough to nearly match the performance of the most advanced MDP algorithm. In Hopper
and Walker, RESeL even surpasses TD7 by a significant margin. By comparing RESeL with SAC,
we find RESeL is superior to SAC in all tasks and can improve SAC by 34.2%in average. These
results indicate that RESeL can also be effectively extended to MDP tasks with notable improvement.
Table 1: Average performance on the classic MuJoCo tasks at 5M time steps ±standard error.
TD3 SAC TQC TD3+OFE TD7 RESeL (ours)
HalfCheetah-v2 14337±1491 15526 ±697 17459 ±258 16596 ±164 18165 ±255 16750±432
Hopper-v2 3682±83 3167 ±485 3462 ±818 3423 ±584 4075 ±225 4408±5
Walker2d-v2 5078±343 5681 ±329 6137 ±1194 6379 ±332 7397 ±454 8004±150
Ant-v2 5589±758 4615 ±2022 6329 ±1510 8547 ±84 10133 ±966 8006±63
Humanoid-v2 5433±245 6555 ±279 8361 ±1364 8951 ±246 10281 ±588 10490 ±381
Average 6824 7109 8350 8779 10010 9532
5.3 Sensitivity and Ablation Studies
Sensitivity Studies. In this section, we analyze the impact of LRCEandLRother with experiments
on the WalkerBLT-V task. Initially, we fixed LRother = 3×10−4and varied LRCE. The final
returns for different LRCEvalues are shown in Fig. 11a. The figure demonstrates that the highest
model performance is achieved when LRCEis between 5×10−6to10−5. Both larger and smaller
LRCEvalues result in decreased performance. Notably, an overly small LRCEis preferable to an
overly large one, as a small learning rate can slow down learning efficiency, whereas a large learning
rate can destabilize training, leading to negative outcomes. Next, we fixed LRCE= 10−5and
varied LRother. The resulting final returns are presented in Fig. 11b. Here, the model achieves the
highest score with LRother= 3×10−4, while other values for LRother yield lower scores. Figs. 11a
and 11b together indicate that the optimal values for LRCEandLRother are not of the same order of
90.0E+00 5.0E-07 1.0E-06 5.0E-06 1.0E-05 5.0E-05 1.0E-04 3.0E-04 5.0E-04 1.0E-03
LRCE0500100015002000return
WalkerBLT-V-v0, LRother=3×104
(a) Fixing LRother , varying LRCE.
5.0E-06 1.0E-05 6.0E-05 3.0E-04 6.0E-04 1.2E-03
LRother0500100015002000returnWalkerBLT-V-v0, LRCE=×105
(b) Fixing LRCE, varying LRother .
3.0E-06 6.0E-06 1.5E-05 3.0E-05 6.0E-05 3.0E-04 6.0E-04
LRCE and LRother0500100015002000return
WalkerBLT-V-v0, LRCE=LRother(c)LRCE= LR other , varying both.
Figure 11: Sensitivity studies of varied learning rates in terms of the average final return.
0 2 4 6
timestep 1e51500
015003000return
HalfCheetahBLT-P-v0
RESeL-Mamba RESeL-GRU RESeL-Transformer MF-RNN (GRU) SAC-Transformer0 2 4 6
timestep 1e51000
010002000
HalfCheetahBLT-V-v0
0 2 4 6
timestep 1e50600120018002400
WalkerBLT-P-v0
0 2 4 6
timestep 1e50500100015002000
WalkerBLT-V-v0
Figure 12: Learning curves shaded with 1 standard error with different RNN architectures.
magnitude. Additionally, it can also be found from Fig. 11c that setting LRCEequally significantly
degrades policy performance. This emphasizes the importance of using different learning rates for
context encoder and other layers. We extended the experiments from Figs. 11a and 11c to additional
seven POMDP tasks, with results presented in Fig. 20 in Appendix E.6. The conclusions drawn from
Fig. 20 are largely consistent with those from Figs. 11a and 11c, indicating that the impact of the
context-encoder-specific learning rate on training is relatively generalizable.
Ablation Studies. We then explore whether the context-encoder-specific learning rate applies to other
RNN or Transformer architectures [ 9]. We integrated a GRU [ 8] or Transformer as the context encoder
while keeping all other settings constant. Due to the high computational cost of training GRUs and
Transformers on full-length trajectories, we tested the RESeL variant with these architectures on four
classic POMDP tasks for 0.75M time steps. Figure 12 displays the learning curves for various variants.
MF-RNN [ 11] and SAC-Transformer [ 38] serve as baselines based on GRUs and Transformers from
prior studies.
We observe that the context-encoder-specific learning rate also improves and stabilizes the per-
formance of the GRU and Transformer variants. Our findings show that both RESeL-GRU and
RESeL-Transformer perform comparably to RESeL-Mamba on three out of four tasks. These re-
sults suggest that the performance gains of RESeL are not solely attributed to an advanced RNN
architecture, and that different arhitectures may be suited to different tasks. Additional sensitivity
experiments and ablation studies on context length can be found in Appendices E.4 to E.6.
6 Conclusions and Limitations
In this paper, we proposed RESeL, an efficient recurrent off-policy RL algorithm. RESeL tackles the
training stability issue existing in previous recurrent RL methods. RESeL uses difference learning
rates for the RNN context encoder and other fully-connected layers to improve the training stability
of RNN and ensure the performance of MLP. Experiments in various POMDP benchmarks showed
that RESeL can achieve or surpass previous SOTA across a wider variety of tasks, including classic
POMDP tasks, meta-RL tasks, and credit assignment tasks.
Limitations. We also notice that there are several noticeable limitations concerning this work: (1) We
have shown that RESeL can even surpass SOTA methods in some MDP tasks, but it is unclear how
the RNN helps the policy improve the performance. (2) RESeL introduces one more hyper-parameter,
i.e.,LRCE. It is would be nice to explore the relationship between optimal LRCEandLRother or
automatically parameter-tuning strategy for LRCE.
10Acknowledgments and Disclosure of Funding
This work was supported by the Fundamental Research Program for Young Scholars (PhD Candidates)
of the National Science Foundation of China (623B2049) and the Jiangsu Science Foundation
(BK20243039). The authors thank the anonymous reviewers and Ms. Qianqian Cheng for their
constructive feedback on this paper.
References
[1]Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction . MIT Press,
1998.
[2]Henrik Bey, Maximilian Tratz, Moritz Sackmann, Alexander Lange, and Jörn Thielecke. Tutorial
on sampling-based pomdp-planning for automated driving. In Proceedings of the International
Conference on Vehicle Technology and Intelligent Transport Systems , Prague, Czech Republic,
2020.
[3]Zachary Sunberg and Mykel J. Kochenderfer. Improving automated driving through POMDP
planning with human internal states. IEEE Transactions on Intelligent Transportation Systems ,
23(11):20073–20083, 2022.
[4]Jing-Cheng Pang, Si-Hang Yang, Xiong-Hui Chen, Xinyu Yang, Yang Yu, Mas Ma, Ziqi Guo,
Howard Yang, and Bill Huang. Object-oriented option framework for robotics manipulation in
clutter. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and
Systems , Detroit, MI, 2023.
[5]Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, and Koushil Sreenath.
Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control. CoRR ,
abs/2401.16889, 2024.
[6]Mikko Lauri, David Hsu, and Joni Pajarinen. Partially observable markov decision processes in
robotics: A survey. IEEE Transactions on Robotics , 39(1):21–40, 2023.
[7]Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation ,
9(8):1735–1780, 1997.
[8]Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.
[9]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-
tion Processing Systems , Long Beach, CA, 2017.
[10] Siegmund Duell, Steffen Udluft, and V olkmar Sterzing. Solving partially observable reinforce-
ment learning problems with recurrent neural networks. In Neural Networks: Tricks of the
Trade - Second Edition , volume 7700, pages 709–733. Springer, 2012.
[11] Tianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. Recurrent model-free RL can be
a strong baseline for many pomdps. In Proceedings of the International Conference on Machine
Learning , Baltimore, MD, 2022.
[12] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob N. Foerster, Satinder Singh,
and Feryal M. P. Behbahani. Structured state space models for in-context reinforcement learning.
InAdvances in Neural Information Processing Systems 36 , New Orleans, LA, 2023.
[13] Jake Grigsby, Linxi Fan, and Yuke Zhu. AMAGO: scalable in-context reinforcement learning for
adaptive agents. In Proceedings of the International Conference on Learning Representations ,
Vienna, Austria, 2024.
[14] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
CoRR , abs/2312.00752, 2023.
11[15] Tianwei Ni, Michel Ma, Benjamin Eysenbach, and Pierre-Luc Bacon. When do Transformers
shine in RL? Decoupling memory from credit assignment. In Advances in Neural Information
Processing Systems 36 , New Orleans, LA, 2023.
[16] Fan-Ming Luo, Shengyi Jiang, Yang Yu, Zongzhang Zhang, and Yi-Feng Zhang. Adapt to
environment sudden changes by learning a context sensitive policy. In Proceedings of the AAAI
Conference on Artificial Intelligence , virtual, 2022.
[17] Wenxuan Zhou, Lerrel Pinto, and Abhinav Gupta. Environment probing interaction policies. In
Proceedings of the International Conference on Learning Representations , New Orleans, LA,
2019.
[18] Dongqi Han, Kenji Doya, and Jun Tani. Variational recurrent models for solving partially observ-
able control tasks. In Proceedings of the International Conference on Learning Representations ,
Addis Ababa, Ethiopia, 2020.
[19] Luisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja
Hofmann, and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep RL via
meta-learning. In Proceedings of the International Conference on Learning Representations ,
Addis Ababa, Ethiopia, 2020.
[20] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the
International Conference on Machine Learning , Stockholmsmässan, Sweden, 2018.
[21] Xinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross. Randomized ensembled double
Q-learning: Learning fast without a model. In Proceedings of the International Conference on
Learning Representations , virtual, 2021.
[22] Matthijs T. J. Spaan. Partially Observable Markov Decision Processes , pages 387–414. Springer
Berlin Heidelberg, Berlin, Heidelberg, 2012.
[23] Zhihan Yang and Hai Nguyen. Recurrent off-policy baselines for memory-based continuous
control. CoRR , abs/2110.12628, 2021.
[24] Daan Wierstra, Alexander Förster, Jan Peters, and Jürgen Schmidhuber. Solving deep memory
POMDPs with recurrent policy gradients. In Proceedings of the International Conference on
Artificial Neural Networks , Porto, Portugal, 2007.
[25] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2:
Fast reinforcement learning via slow reinforcement learning. CoRR , abs/1611.02779, 2016.
[26] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta-
tion of deep networks. In Proceedings of the International Conference on Machine Learning ,
Sydney, Australia, 2017.
[27] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen
Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science
Robotics , 4(26), 2019.
[28] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes
using massively parallel deep reinforcement learning. In Proceedings of the Conference on
Robot Learning , London, UK, 2021.
[29] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real
transfer of robotic control with dynamics randomization. In Proceedings of the International
Conference on Robotics and Automation, Brisbane, Australia , 2018.
[30] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik,
Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh,
Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P.
Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin
Dalibard, David Budden, Yury Sulsky, James Molloy, Tom Le Paine, Çaglar Gülçehre, Ziyu
12Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McK-
inney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis,
Chris Apps, and David Silver. Grandmaster level in starcraft II using multi-agent reinforcement
learning. Nature , 575(7782):350–354, 2019.
[31] Steven D. Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, and Amanda Prorok.
Popgym: Benchmarking partially observable reinforcement learning. In Proceedings of the
International Conference on Learning Representations , Kigali, Rwanda, 2023.
[32] John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust re-
gion policy optimization. In Proceedings of the International Conference on Machine Learning ,
Lille, France, 2015.
[33] Wenhao Yu, Jie Tan, C. Karen Liu, and Greg Turk. Preparing for the unknown: Learning
a universal policy with online system identification. In Robotics: Science and Systems XIII ,
Cambridge, MA, 2017.
[34] Haotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, and Wulong Liu.
Towards effective context for meta-reinforcement learning: an approach based on contrastive
learning. In Proceedings of the AAAI Conference on Artificial Intelligence , virtual, 2021.
[35] Wesley Chung, Somjit Nath, Ajin Joseph, and Martha White. Two-timescale networks for
nonlinear value function approximation. In Proceedings of the 7th International Conference on
Learning Representations , New Orleans, LA, 2019.
[36] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error
in actor-critic methods. In Proceedings of the International Conference on Machine Learning ,
Stockholmsmässan, Sweden, 2018.
[37] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based
control. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and
Systems , Vilamoura, Portugal, 2012.
[38] Ian Char and Jeff Schneider. PID-Inspired inductive biases for deep reinforcement learning in
partially observable control tasks. In Advances in Neural Information Processing Systems 36 ,
New Orleans, LA, 2023.
[39] Nicolas Minorsky. Directional stability of automatically steered bodies. Journal of the American
Society for Naval Engineers , 34(2):280–309, 1922.
[40] Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. ProMP:
Proximal meta-policy search. In Proceedings of the International Conference on Learning
Representations , New Orleans, LA, 2019.
[41] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real
transfer of robotic control with dynamics randomization. In Proceedings of the International
Conference on Robotics and Automation , Brisbane, Australia, 2018.
[42] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-
policy meta-reinforcement learning via probabilistic context variables. In Proceedings of the
International Conference on Machine Learning , Long Beach, CA, 2019.
[43] Scott Fujimoto, Wei-Di Chang, Edward J. Smith, Shixiang Gu, Doina Precup, and David Meger.
For SALE: state-action representation learning for deep reinforcement learning. In Advances in
Neural Information Processing Systems 36 , New Orleans, LA, 2023.
[44] Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments. In
Advances in Neural Information Processing Systems 33 , virtual, 2020.
[45] Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments for
reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelligence ,
44(10):6968–6980, 2022.
13[46] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C. Courville, and Yoshua
Bengio. A recurrent latent variable model for sequential data. In Advances in Neural Information
Processing Systems 28 , Montreal, Canada, 2015.
[47] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured
state spaces. In Proceedings of the International Conference on Learning Representations ,
virtual, 2022.
[48] Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space
layers for sequence modeling. In Proceedings of the International Conference on Learning
Representations , Kigali, Rwanda, 2023.
[49] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). In Proceedings of the International Conference on
Learning Representations , San Juan, Puerto Rico, 2016.
[50] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of
the International Conference on Learning Representations , New Orleans, LA, 2019.
[51] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty,
Kinal Mehta, and João G. M. Araújo. Cleanrl: High-quality single-file implementations of deep
reinforcement learning algorithms. Journal of Machine Learning Research , 23:274:1–274:18,
2022.
[52] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research , 9(86):2579–2605, 2008.
[53] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. CoRR , abs/1606.01540, 2016.
14A Proof Proposition 1
The following proof process is primarily based on the proof approach in [ 44,45]. To the best of our
knowledge, this is the first time the compounding error analysis method has been applied to study the
properties of RNNs.
Let an RNN network fθ(xt, ht) :X × H → Y × H be formalized as:
yt, ht+1=fθ(xt, ht),
where θis the network parameter, xtis the RNN input, ytis the RNN output, htandht+1are the
RNN hidden states. For simplicity, we denote
yt=fθ
y(xt, ht),
ht+1=fθ
h(xt, ht).
We consider another RNN parameter θ′, which is obtained by updating θby one gradient step. We
make the following assumptions:
1.A1 (Lipschitz continuity): fθandfθ′both satisfy the Lipschitz continuity. There exist
Kh∈[0,1)3andKy∈Rsatisfying:
fθ
h(x, h)−fθ
h(x, h′)≤Kh∥h−h′∥,∀x∈ X, h, h′∈ H, (A1.1)fθ′
h(x, h)−fθ′
h(x, h′)≤Kh∥h−h′∥,∀x∈ X, h, h′∈ H, (A1.2)
fθ
y(x, h)−fθ
y(x, h′)≤Ky∥h−h′∥,∀x∈ X, h, h′∈ H, (A1.3)fθ′
y(x, h)−fθ′
y(x, h′)≤Ky∥h−h′∥,∀x∈ X, h, h′∈ H. (A1.4)
2.A2 (one-step output difference constraint): The output differences of the RNN parameter-
ized by θandθ′are bounded by a constant ϵ∈R:
fθ
h(x, h)−fθ′
h(x, h)≤ϵ,∀x∈ X, h∈ H, (A2.1)
fθ
y(x, h)−fθ′
y(x, h)≤ϵ,∀x∈ X, h∈ H. (A2.2)
With the above assumptions, we now prove Proposition 1 in following.
Proof. ∀x∈ X, h, h′∈ H, we have
fθ′
h(x, h′)−fθ
h(x, h)
=fθ′
h(x, h′)−fθ′
h(x, h) +fθ′
h(x, h)−fθ
h(x, h)
≤fθ′
h(x, h′)−fθ′
h(x, h)+fθ′
h(x, h)−fθ
h(x, h)
Eq. (A1.2)
≤Kh∥h−h′∥+fθ′
h(x, h)−fθ
h(x, h)
Eq. (A2.1)
≤Kh∥h−h′∥+ϵ.(2)
Similarly, ∀x∈ X, h, h′∈ H, we also have
fθ′
y(x, h′)−fθ
y(x, h)≤Ky∥h−h′∥+ϵ. (3)
Given an input sequence {x0, x1, . . . , x T}with sequence length T+ 1and an initial hidden state h0,
let the output and hidden state sequences produced by fθbe{y0, y1, . . . , y T}and{h1, . . . , h T+1}.
3We only consider Kh∈[0,1)as most RNN models satisfy this condition. GRU/LSTM and SSMs achieve
this condition by adopting a sigmoid activation function and directly limits the value of the transition matrix of
the hidden state, respectively. The RNN with a Kh>1could diverge during long-sequence forwarding.
15Similarly, let the output and hidden state sequences produced by fθ′be{y′
0, y′
1, . . . , y′
T}and
{h′
1, . . . , h′
T+1}. We can measure the upper bound of the hidden state differences between fθ
andfθ′.
∥h1−h′
1∥=fθ
h(x0, h0)−fθ′
h(x0, h0)Eq. (A2.1)
≤ϵ. (4)
∥h2−h′
2∥=fθ
h(x1, h1)−fθ′
h(x1, h′
1)
Eq. (2)
≤Kh∥h1−h′
1∥+ϵ
Eq. (4)
≤Khϵ+ϵ.(5)
Similarly, we have
∥h3−h′
3∥=fθ
h(x2, h2)−fθ′
h(x2, h′
2)≤2X
i=0Ki
hϵ. (6)
∥ht−h′
t∥=fθ
h(xt−1, ht−1)−fθ′
h(xt−1, h′
t−1)≤t−1X
i=0Ki
hϵ. (7)
Then we can derive the output difference at the t-th step ( t >0):
∥yt−y′
t∥=fθ
y(xt, ht)−fθ′
y(xt, h′
t)
Eq. (3)
≤Ky∥ht−h′
t∥+ϵ
Eq. (7)
≤Kyt−1X
i=0Ki
hϵ+ϵ.(8)
As0≤Kh<1, Eq. (8) can be further simplified to
∥yt−y′
t∥ ≤Ky1−Kt
h
1−Khϵ+ϵ. (9)
Eq. (9) also holds for t= 0, i.e.,∥y0−y′
0∥ ≤ϵ. Thus, we finally have
∥yt−y′
t∥ ≤Ky1−Kt
h
1−Khϵ+ϵ,∀t≥0. (10)
B Average Output Differences Bound
Proposition 2. Following the setting in Proposition 1, the average output difference satisfies
1
t+ 1tX
i=0∥yt−y′
t∥ ≤Ky
1−Khϵ+ϵ−Ky1−Kt+1
h
(1 +t)(1−Kh)2ϵ,∀t≥0. (11)
Proof. From Eq. (10), we have
∥yt−y′
t∥ ≤Ky
1−Khϵ+ϵ−Kyϵ
1−KhKt
h.
As
1
t+ 1tX
i=0Ki
h
=1−Kt+1
h
(1−Kh)(t+ 1),
16Algorithm 1: Training Procedure of RESeL
Input: Environment E; Policy πϕ; Ensemble critic Qψ; Target ensemble critic Qψ′; Batch size
BS; Entropy coefficient α; Target entropy TE;
1Initialize context-encoder-specific policy optimizer and critic optimizer;
2Initialize an empty replay buffer B ← ∅ and trajectory τ← ∅;
3Initial context ˆa←0,ˆo←0,ˆr←0;
4Initial hidden state h←0;
5t←0;
6Obtain initial observation o0from E;
7while t < max_step do
8 at, h←πϕ(ot,ˆo,ˆa,ˆr, h);
9 Execute attoE;
10 Observe new observation ot+1, reward rt, and terminal signal dt;
11 Add(ot, at, rt, ot+1)toτ;
12 Update context data ˆa←at,ˆo←ot,ˆr←rt;
13 ifdtis True then
14 Insert τtoB;
15 τ← ∅;
16 Reset context ˆa←0,ˆo←0,ˆr←0;
17 Reset hidden state h←0;
18 Reset E, obtain new observation ot+1from E;
19 Sample data batch D={τi|i= 1,2, . . .}with data countP
i|τi| ≥BSfromB;
20 Obtain REDQ [21] critic loss with Qψ,Qψ′,α, and Dfollowing Eq. (19);
21 Update ψwith the critic optimizer via one-step gradient descent;
22 Softly assign ψtoψ′;
23 ift%2 = 0 then
24 Obtain REDQ [21] policy loss with πϕ,α, and Dfollowing Eq. (20);
25 Update ϕwith the policy optimizer via one-step gradient descent;
26 Automatically tune αto control the policy entropy to TE;
27 t←t+ 1;
we can get
1
t+ 1tX
i=0∥yi−y′
i∥
≤Ky
1−Khϵ+ϵ−Kyϵ
1−Kh1
t+ 1tX
i=0Kt
h
=Ky
1−Khϵ+ϵ−Ky1−Kt+1
h
(1−Kh)2(t+ 1)ϵ.
C Algorithmic Details
C.1 Detailed Procedure of RESeL
The training procedure of RESeL is summarized in Algorithm 1. In the following part, we elaborate
on the form of REDQ [ 21] losses in the setting of recurrent RL. Let the critic Qψ(o, a,ˆo,ˆa,ˆr, hQ) :
O × A × O × A × R→R8× H be a mapping function from the current observation oand action a,
last-step observation ˆo, action ˆa, reward ˆr, and hidden state hQto a 8-dim Q-value vector and the
next hidden state hQ. The target critic Qψ′(o, a,ˆo,ˆa,ˆr, hQ)shares the same architecture to Qψbut
parameterized by ψ′. The policy π(o,ˆo,ˆa,ˆr, hπ) :O × A × O × A × R→∆A× H is a mapping
function from the current observation o, last-step observation ˆo, action ˆa, reward ˆr, and hidden state
hπto an action distribution and the next hidden state hπ.
17Given a batch of data D={τi|i= 1,2, ...}, where τi={oi
0, ai
0, ri
0, oi
1, ai
1, ri
1, oi
2, . . . , ri
Li−1, oi
Li}
is the i-th trajectory in the batch, Liis the trajectory length. We then re-formalize τito tensorized data:
oi∈ OLi,ai∈ ALi,ˆoi∈ OLi,ˆai∈ ALi,ri∈RLi,ˆri∈RLi, where, for example, ot
i∈ O=oi
t
is the t-th row of oi.ˆo0
i=0,ˆa0
i=0,ˆr0
i= 0as the there is no last step at the first time step. The
policy receives the tensorized data and outputs action distributions. For simplicity, we formalize the
outputs of the policy as the action sampled from the action distribution and its logarithm sampling
probability:
˜ ai,log˜ pi, hLi
π←πϕ(oi,ˆoi,ˆai,ˆri,0), (12)
where ˜ ai∈ ALi,log˜ pi∈RLi, hLiπ∈ H are obtained autoregressively:
˜ a0
i,log˜ p0
i, h1
π←πϕ(o0
i,ˆo0
i,ˆa0
i,ˆr0
i,0),
˜ a1
i,log˜ p1
i, h2
π←πϕ(o1
i,ˆo1
i,ˆa1
i,ˆr1
i, h1
π),
. . . . . .
˜ at
i,log˜ pt
i, ht+1
π←πϕ(ot
i,ˆot
i,ˆat
i,ˆrt
i, ht
π).(13)
Similarly, the critic model receives the tensorized data and outputs 8-dim Q-values as follows:
˜Qi, hLi
Q←Qψ(oi,ai,ˆoi,ˆai,ˆri,0), (14)
where ˜Qi∈RLi×8, hLiπ∈ H are obtained autoregressively:
Q0
i, h1
Q←Qψ(o0
i,a0
i,ˆo0
i,ˆa0
i,ˆr0
i,0),
Q1
i, h2
Q←Qψ(o1
i,a1
i,ˆo1
i,ˆa1
i,ˆr1
i, h1
Q),
. . . . . .
Qt
i, ht+1
Q←Qψ(ot
i,at
i,ˆot
i,ˆat
i,ˆrt
i, ht
Q).(15)
Note that, in Mamba, the process demonstrated in Eq. (13) and Eq. (15) can be parallelized.
In order to compute the target value, we first extend the tensorized data
o+
i← {oi,oLi
i},
ˆo+
i← {ˆoi,oLi−1
i},
ˆa+
i← {ˆai,aLi−1
i},
ˆr+
i← {ˆri,rLi−1
i},
r+
i← {0,ri},(16)
where oLi
iis the next observation of the last step, oLi−1
i ,aLi−1
i , andrLi−1
i are the observation,
action, and reward at the last step, respectively.
The target value can be obtained:
˜a+
target,i,log˜ ptarget,i, hLi+1
π←πϕ(o+
i,ˆo+
i,ˆa+
i,ˆr+
i,0),
˜Q+
target,i, hLi+1
Q←r+
i+γ
Qψ′
o+
i,˜a+
target,i,ˆo+
i,ˆa+
i,ˆr+
i,0
−αlog˜ ptarget,i
.(17)
Then, we sample a set Mof 2 distinct indices from {0,1, ...,7}. Let ˜Q+,j
target,ibe the j-th column of
˜Q+
target,i. We choose the minimum value from {˜Q+,j
target,i|j∈ M} :
Q+
target,i= min
j∈M˜Q+,j
target,i, (18)
where Q+
target,i∈RLi+1. We then let yibe the last Lirows of Q+
target,i:
yi=Q+
target,i[1 :Li+ 1].
The REDQ critic loss can be written as
LQ=1
P|D|
i=1Li|D|X
i=1yi−˜Qi2
2, (19)
18where ˜Qiis obtained following Eq. (14) ,∥ · ∥ 2denotes L2-norm. As for the policy loss, we first
obtain ˜ aiandlog˜ piviaEq. (12) . We can get the critic value ˜Q⋆
icorresponding to the new action ˜ ai
following Eq. (14):
˜Q⋆
i, hLi
Q←Qψ(oi,˜ ai,ˆoi,ˆai,ˆri,0).
We can get the policy loss:
Lπ=−1
8P|D|
i=1Lisum
˜Q⋆
i−αlog˜ p
, (20)
where denominator of 8 results from our ensemble size being 8, requiring the averaging of the outputs
of the eight critics to compute the policy loss.
C.2 Sample Stacked Batch from Replay Buffer
replay buffer0123456trajectory index0102030time step<latexit sha1_base64="c8JrSF3JVf4KtFaS2eZkshifad4=">AAACzXicjVHLSsNAFD2Nr1pfVZdugkV0UUoioi6LbtxZwT6wFknSaQ3Ni5mJUKpu/QG3+lviH+hfeGdMQS2iE5KcOfeeM3PvdZPAF9KyXnPG1PTM7Fx+vrCwuLS8Ulxda4g45R6re3EQ85brCBb4EatLXwaslXDmhG7Amu7gWMWbN4wLP47O5TBhndDpR37P9xxJ1EVblJ0yL4vtzlWxZFUsvcxJYGeghGzV4uILLtFFDA8pQjBEkIQDOBD0tGHDQkJcByPiOCFfxxnuUCBtSlmMMhxiB/Tt066dsRHtlafQao9OCejlpDSxRZqY8jhhdZqp46l2Vuxv3iPtqe42pL+beYXESlwT+5dunPlfnapFoodDXYNPNSWaUdV5mUuqu6Jubn6pSpJDQpzCXYpzwp5Wjvtsao3QtaveOjr+pjMVq/ZelpviXd2SBmz/HOckaOxW7P2KfbZXqh5lo85jA5vYoXkeoIoT1FAn7wiPeMKzcWqkxq1x/5lq5DLNOr4t4+EDFY6SgQ==</latexit>[s, a, r, s0]
K-step zerosK: kernel size of conv1dhidden resetflagsamplebatchstacked batch<latexit sha1_base64="jKSDc9Cd/YTg3IlujKP4JuoBJus=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040aoYB9QS0mm0xqaJmFmItTapT/gVv9L/AP9C++MKahFdEKSM+eec2fuvX4SBlI5zmvOmptfWFzKLxdWVtfWN4qbW3UZp4LxGovDWDR9T/IwiHhNBSrkzURwb+iHvOEPznS8ccuFDOLoSo0S3h56/SjoBcxTRDWvlZd2xs6kUyw5Zccsexa4GSghW9W4+IJrdBGDIcUQHBEU4RAeJD0tuHCQENfGmDhBKDBxjgkK5E1JxUnhETugb592rYyNaK9zSuNmdEpIryCnjT3yxKQThPVptomnJrNmf8s9Njn13Ub097NcQ2IVboj9yzdV/tena1Ho4cTUEFBNiWF0dSzLkpqu6JvbX6pSlCEhTuMuxQVhZpzTPtvGI03tureeib8ZpWb1nmXaFO/6ljRg9+c4Z0H9oOweld3Lw1LlNBt1HjvYxT7N8xgVnKOKmpnjI57wbF1Y0rqz7j+lVi7zbOPbsh4+AO7/kn4=</latexit>⌧0<latexit sha1_base64="UP9zgnrXHOc7I5f5rqv+NY8dD8I=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040aoYB9QS0nSaR2aJiEzEWrt0h9wq/8l/oH+hXfGKahFdEKSM+eec2fuvX4SciEd5zVnzc0vLC7llwsrq2vrG8XNrbqIszRgtSAO47Tpe4KFPGI1yWXImknKvKEfsoY/OFPxxi1LBY+jKzlKWHvo9SPe44EniWpeSy/rjN1Jp1hyyo5e9ixwDSjBrGpcfME1uogRIMMQDBEk4RAeBD0tuHCQENfGmLiUENdxhgkK5M1IxUjhETugb592LcNGtFc5hXYHdEpIb0pOG3vkiUmXElan2Tqe6cyK/S33WOdUdxvR3ze5hsRK3BD7l2+q/K9P1SLRw4mugVNNiWZUdYHJkumuqJvbX6qSlCEhTuEuxVPCgXZO+2xrj9C1q956Ov6mlYpV+8BoM7yrW9KA3Z/jnAX1g7J7VHYvD0uVUzPqPHawi32a5zEqOEcVNT3HRzzh2bqwhHVn3X9KrZzxbOPbsh4+APFgkn8=</latexit>⌧1<latexit sha1_base64="cnOueuUG1wkkMn7DEpNlVz1q/p4=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVZIi6rLoxo1QwT6gLSVJp3VoXmQmQq1d+gNu9b/EP9C/8M6YglpEJyQ5c+45d+be68Y+F9KyXnPGwuLS8kp+tbC2vrG5VdzeaYgoTTxW9yI/SlquI5jPQ1aXXPqsFSfMCVyfNd3RuYo3b1kieBRey3HMuoEzDPmAe44kqtWRTtqbVKa9YskqW3qZ88DOQAnZqkXFF3TQRwQPKQIwhJCEfTgQ9LRhw0JMXBcT4hJCXMcZpiiQNyUVI4VD7Ii+Q9q1MzakvcoptNujU3x6E3KaOCBPRLqEsDrN1PFUZ1bsb7knOqe625j+bpYrIFbihti/fDPlf32qFokBTnUNnGqKNaOq87Isqe6Kurn5pSpJGWLiFO5TPCHsaeesz6b2CF276q2j429aqVi19zJtind1Sxqw/XOc86BRKdvHZfvqqFQ9y0adxx72cUjzPEEVF6ihruf4iCc8G5eGMO6M+0+pkcs8u/i2jIcP88GSgA==</latexit>⌧2<latexit sha1_base64="QE8q6QU5xMSH1CLl0XQ291NbRz4=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRIVdVl040aoYB/QljJJpzU0LyYTodYu/QG3+l/iH+hfeGdMQS2iE5KcOfecO3PvdWLfS6RlveaMufmFxaX8cmFldW19o7i5VU+iVLi85kZ+JJoOS7jvhbwmPenzZiw4CxyfN5zhuYo3brlIvCi8lqOYdwI2CL2+5zJJVLMtWdodH066xZJVtvQyZ4GdgRKyVY2KL2ijhwguUgTgCCEJ+2BI6GnBhoWYuA7GxAlCno5zTFAgb0oqTgpG7JC+A9q1MjakvcqZaLdLp/j0CnKa2CNPRDpBWJ1m6niqMyv2t9xjnVPdbUR/J8sVECtxQ+xfvqnyvz5Vi0Qfp7oGj2qKNaOqc7Msqe6Kurn5pSpJGWLiFO5RXBB2tXPaZ1N7El276i3T8TetVKzau5k2xbu6JQ3Y/jnOWVA/KNvHZfvqqFQ5y0adxw52sU/zPEEFF6iipuf4iCc8G5dGYtwZ959SI5d5tvFtGQ8f9iKSgQ==</latexit>⌧3<latexit sha1_base64="j8tdAwu9TVZZgEu1bGFyksPxTPQ=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRIp6rLoxo1QwT6gLSVJp3VoXiQTodYu/QG3+l/iH+hfeGecglpEJyQ5c+45d+be68Y+T4VlveaMhcWl5ZX8amFtfWNzq7i900ijLPFY3Yv8KGm5Tsp8HrK64MJnrThhTuD6rOmOzmW8ecuSlEfhtRjHrBs4w5APuOcIolod4WS9SWXaK5assqWWOQ9sDUrQqxYVX9BBHxE8ZAjAEEIQ9uEgpacNGxZi4rqYEJcQ4irOMEWBvBmpGCkcYkf0HdKurdmQ9jJnqtweneLTm5DTxAF5ItIlhOVppopnKrNkf8s9UTnl3cb0d3WugFiBG2L/8s2U//XJWgQGOFU1cKopVoysztNZMtUVeXPzS1WCMsTESdyneELYU85Zn03lSVXtsreOir8ppWTl3tPaDO/yljRg++c450HjqGwfl+2rSql6pkedxx72cUjzPEEVF6ihrub4iCc8G5dGatwZ959SI6c9u/i2jIcP+IOSgg==</latexit>⌧4<latexit sha1_base64="C9xHH3vDwmQaEOBOruXR+At1Bpo=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRLxtSy6cSNUsA9oS5mk0xqaF5OJUGuX/oBb/S/xD/QvvDOmoBbRCUnOnHvOnbn3OrHvJdKyXnPG3PzC4lJ+ubCyura+UdzcqidRKlxecyM/Ek2HJdz3Ql6TnvR5MxacBY7PG87wXMUbt1wkXhRey1HMOwEbhF7fc5kkqtmWLO2OjybdYskqW3qZs8DOQAnZqkbFF7TRQwQXKQJwhJCEfTAk9LRgw0JMXAdj4gQhT8c5JiiQNyUVJwUjdkjfAe1aGRvSXuVMtNulU3x6BTlN7JEnIp0grE4zdTzVmRX7W+6xzqnuNqK/k+UKiJW4IfYv31T5X5+qRaKPU12DRzXFmlHVuVmWVHdF3dz8UpWkDDFxCvcoLgi72jnts6k9ia5d9Zbp+JtWKlbt3Uyb4l3dkgZs/xznLKgflO3jsn11WKqcZaPOYwe72Kd5nqCCC1RR03N8xBOejUsjMe6M+0+pkcs82/i2jIcP+uSSgw==</latexit>⌧5<latexit sha1_base64="BjpoaTiHmjnXbid6/fSOjAiG0og=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRKR6rLoxo1QwT6gLSWZTmtoXiQTodYu/QG3+l/iH+hfeGecglpEJyQ5c+45d+be68a+lwrLes0ZC4tLyyv51cLa+sbmVnF7p5FGWcJ4nUV+lLRcJ+W+F/K68ITPW3HCncD1edMdnct485YnqReF12Ic827gDENv4DFHENXqCCfrTSrTXrFklS21zHlga1CCXrWo+IIO+ojAkCEARwhB2IeDlJ42bFiIietiQlxCyFNxjikK5M1IxUnhEDui75B2bc2GtJc5U+VmdIpPb0JOEwfkiUiXEJanmSqeqcyS/S33ROWUdxvT39W5AmIFboj9yzdT/tcnaxEY4FTV4FFNsWJkdUxnyVRX5M3NL1UJyhATJ3Gf4glhppyzPpvKk6raZW8dFX9TSsnKPdPaDO/yljRg++c450HjqGxXyvbVcal6pkedxx72cUjzPEEVF6ihrub4iCc8G5dGatwZ959SI6c9u/i2jIcP/UWShA==</latexit>⌧6
<latexit sha1_base64="jKSDc9Cd/YTg3IlujKP4JuoBJus=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040aoYB9QS0mm0xqaJmFmItTapT/gVv9L/AP9C++MKahFdEKSM+eec2fuvX4SBlI5zmvOmptfWFzKLxdWVtfWN4qbW3UZp4LxGovDWDR9T/IwiHhNBSrkzURwb+iHvOEPznS8ccuFDOLoSo0S3h56/SjoBcxTRDWvlZd2xs6kUyw5Zccsexa4GSghW9W4+IJrdBGDIcUQHBEU4RAeJD0tuHCQENfGmDhBKDBxjgkK5E1JxUnhETugb592rYyNaK9zSuNmdEpIryCnjT3yxKQThPVptomnJrNmf8s9Njn13Ub097NcQ2IVboj9yzdV/tena1Ho4cTUEFBNiWF0dSzLkpqu6JvbX6pSlCEhTuMuxQVhZpzTPtvGI03tureeib8ZpWb1nmXaFO/6ljRg9+c4Z0H9oOweld3Lw1LlNBt1HjvYxT7N8xgVnKOKmpnjI57wbF1Y0rqz7j+lVi7zbOPbsh4+AO7/kn4=</latexit>⌧0
<latexit sha1_base64="UP9zgnrXHOc7I5f5rqv+NY8dD8I=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040aoYB9QS0nSaR2aJiEzEWrt0h9wq/8l/oH+hXfGKahFdEKSM+eec2fuvX4SciEd5zVnzc0vLC7llwsrq2vrG8XNrbqIszRgtSAO47Tpe4KFPGI1yWXImknKvKEfsoY/OFPxxi1LBY+jKzlKWHvo9SPe44EniWpeSy/rjN1Jp1hyyo5e9ixwDSjBrGpcfME1uogRIMMQDBEk4RAeBD0tuHCQENfGmLiUENdxhgkK5M1IxUjhETugb592LcNGtFc5hXYHdEpIb0pOG3vkiUmXElan2Tqe6cyK/S33WOdUdxvR3ze5hsRK3BD7l2+q/K9P1SLRw4mugVNNiWZUdYHJkumuqJvbX6qSlCEhTuEuxVPCgXZO+2xrj9C1q956Ov6mlYpV+8BoM7yrW9KA3Z/jnAX1g7J7VHYvD0uVUzPqPHawi32a5zEqOEcVNT3HRzzh2bqwhHVn3X9KrZzxbOPbsh4+APFgkn8=</latexit>⌧1
<latexit sha1_base64="BjpoaTiHmjnXbid6/fSOjAiG0og=">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRKR6rLoxo1QwT6gLSWZTmtoXiQTodYu/QG3+l/iH+hfeGecglpEJyQ5c+45d+be68a+lwrLes0ZC4tLyyv51cLa+sbmVnF7p5FGWcJ4nUV+lLRcJ+W+F/K68ITPW3HCncD1edMdnct485YnqReF12Ic827gDENv4DFHENXqCCfrTSrTXrFklS21zHlga1CCXrWo+IIO+ojAkCEARwhB2IeDlJ42bFiIietiQlxCyFNxjikK5M1IxUnhEDui75B2bc2GtJc5U+VmdIpPb0JOEwfkiUiXEJanmSqeqcyS/S33ROWUdxvT39W5AmIFboj9yzdT/tcnaxEY4FTV4FFNsWJkdUxnyVRX5M3NL1UJyhATJ3Gf4glhppyzPpvKk6raZW8dFX9TSsnKPdPaDO/yljRg++c450HjqGxXyvbVcal6pkedxx72cUjzPEEVF6ihrub4iCc8G5dGatwZ959SI6c9u/i2jIcP/UWShA==</latexit>⌧6
Figure 13: Illustrating for sampling a stacked batch from replay buffer.
In RESeL, we collect several full-length trajectories from the replay buffer as a batch for policy
training. However, in many environments, trajectory lengths could vary a lot, making it impossible
to directly concatenate them into a single tensor for subsequent computations. Typically, we pad
trajectories to the same length with zeros, resulting in a tensor of shape [B, L, C ], where Bis the
batch size, Lis the sequence length, and Cis the tensor dimension. Additionally, we introduce a
mask tensor of shape [B, L, 1], where elements equal to 1 indicate valid data, and 0s indicate padding.
We then use a modified mean operation:
mask _mean (x, mask ) =X
(x×mask )/sum (mask ),
to prevent padding from biasing the mean. This approach allows for GPU parallelization of all
trajectories in the batch, though calculations for padded 0s are redundant. If trajectory lengths vary
greatly, the proportion of padding can become significant, severely reducing computational efficiency,
even resulting in out-of-GPU-memory. For instance, in a batch with one trajectory of length 1000
and ten trajectories of length 50, there would be 1500 valid data points and 9500 padded points,
substantially lowering efficiency.
To address this issue, we adopt the batch sampling method illustrated in Fig. 13, stacking shorter
trajectories along the time dimension. This requires the RNN to accept a hidden reset flag, which
we insert at the beginning of each trajectory to reset the hidden state. Additionally, we insert K
steps of zero data between trajectories to prevent convolution1d from mixing adjacent trajectory
data, as shown in Fig. 13. To implement this sampling strategy, we modify the original Mamba
implementation4, adding an input denoting hidden reset flag. When occurring the hidden reset flag
4https://github.com/state-spaces/mamba
19with value of 1, the hidden state will be reset to zero. The introduction of hidden reset flag will not
influence the parallelizability of Mamba but can avoid potential redundant computation.
D Experiment Details
D.1 Descriptions of the Environments
D.1.1 Classic POMDP Tasks
These classic POMDP tasks5are developed using PyBullet locomotion environments. The envi-
ronments include AntBLT, HopperBLT, WalkerBLT, and HalfCheetahBLT. Partial observability is
introduced by obscuring parts of the observation. For each environment, two types of tasks are created:
one where only joint velocity is observable and another where only joint position is observable. We
denote these as "-V" for joint velocity information and "-P" for joint position information. In such
settings, the agent must reconstruct the missing information, whether velocity or position, based on
historical observations.
D.1.2 Dynamics-Randomized Tasks
The Dynamics-Randomized Tasks6environments are based on the MuJoCo environment [ 37], fol-
lowing the training settings in [ 16]. In these tasks, the environment’s gravity is altered using an
open-source tool7. The gravity is sampled as following procedure: We first sample a value a∈R
uniformly from [−3,3]. Let the original gravity in MuJoCo be gorigin, the new gravity is obtained as:
gnew= 1.5agorigin. (21)
Before training, we sample 60 values of gnew. Of these, 40 are used for training and 20 for testing the
learned policies. In these tasks, the agent must infer the gravity based on historical interactions with
the environment and adjust its policy accordingly.
D.1.3 Classic meta-RL Tasks
These meta-RL tasks8have been utilized in various meta-RL algorithms [ 42,19]. Specifically, in
these tasks, the reward function is modified in the Ant and HalfCheetah MuJoCo environments.
In the ’Vel’ tasks, the agent is rewarded based on its proximity to a desired velocity. In the ’Dir’
tasks, the agent must move in a desired direction. The desired velocity and direction information are
reflected only in the reward function. In the Wind environment, an external, unobservable disturbance
factor (wind speed) influences state transitions. The agent must navigate to a goal under varying
wind conditions. In these tasks, the agent uses past interaction data, including rewards, to infer the
underlying reward functions or transition disturbance factors.
D.1.4 Key-to-Door
The Key-to-Door environment9consists of three phases. In the first phase, the agent spawns at a
random location on the map, and a key will appear at another location on the map, the agent can
touch the key. In the second phase, the agent will have the task of picking up apples. In the third
phase, if the agent has touched the key in the first phase, it can receive a reward; otherwise, it cannot.
The action space is four-dimensional, encompassing the actions of moving forward, backward, left
and right. The reward structure is dual-faceted: it includes rewards garnered from the collection of
apples during the second phase and the ultimate reward obtained upon door opening in the final phase.
The transition function is defined by the positional changes of the agent subsequent to the execution
of any of the four actions, with stage transitions dictated by an intrinsic timer, which facilitates a shift
to the subsequent stage upon the expiration of the countdown. The optimal policy should locate and
touch the key in the initial phase, which is served for the final reward acquisition upon door opening
5https://github.com/oist-cnru/Variational-Recurrent-Models
6https://github.com/FanmingL/ESCP
7https://github.com/dennisl88/rand_param_envs
8https://github.com/lmzintgraf/varibad
9https://github.com/twni2016/Memory-RL
20in the third phase, while maximizing apple collection during the second phase. The credit assignment
length in Fig. 10 denotes the time period staying at the second stage.
D.2 Baselines
In the section, we give brief introductions of the baseline algorithms we comparing in our work. We
first introduce the baselines in the classic POMDP tasks Fig. 7.
•Variational Recurrent Model (VRM) [ 18]. VRM utilizes a variational RNN [ 46] to learn a
transition model, and feeding the RNN hidden state into an MLP policy to obtain the action.
•GPIDE (GPIDE-ESS) [ 38]. Inspired by the PID controller’s success, which relies on
summing and derivative to accumulate information over time, GPIDE propose two history
encoding architectures: one using direct PID features and another extending these principles
for general control tasks by consisting of a number of “heads” for accumulating information
about the history.
•Recurrent Model-free RL (MF-RNN) [ 11]. MF-RNN proposes several design considerations
in recurrent off-policy RL methods. MF-RNN analyzed the best choice concerning the
context length, RL algorithm, whether sharing the RNN between critic and policy networks,
etc. We used the best results reported in [11] in Fig. 7.
The baseline results in Fig. 7 come from two sources: (1) The evaluation results provided in [ 11]10,
including MF-RNN, PPO-GRU, SAC-MLP, TD3-MLP, VRM, and A2C-GRU; (2) The results
provided in [38]11, consisting GPIDE-ESS and SAC-Transformer.
The baselines compared in Fig. 8 are elaborated as follows:
•Environment Probing Interaction Policy (EPI) [ 17]. This baseline aims to extracting features
from the transition. A context encoder is obtained to enable a transition model to predict the
future state through minimizing the following loss:
LDM
CE=Ezt
i,st,at,st+1T(st, at, zt
i;θT) +st−st+12
2, (22)
where T(st, at, zt
i;θT)is a transition model parameterized by θT,zt
iis the output of the
context encoder. In addition, the transition model is also optimized as described in Eq. (22) .
•Online system identification (OSI) [ 33]. OSI is structured by a context encoder which is
used to predict the parameters of the environment dynamics. The architecture of OSI is the
same as Fig. 1. OSI adopts a supervised learning loss function to train the context encoder:
LOSI
CE=Ezt
i∥zt
i−ci∥2
2, where ciis the dynamics parameter such as gravity ,zt
iis the
embedded context.
•Environment sensitive contextual policy learning (ESCP) [ 16]12. This approach is design to
improve both the the robustness and sensitivity context encoder, based on a contrastive loss.
In order to improve the policy robustness in environment where dynamics sudden changes
could occurs, ESCP truncate the memory length in both training and deployment phase.
•Proximal Meta-Policy Search (ProMP) [ 40]13. This method learns a pretrained policy model
in training phase and finetune it in deployment phase with few gradient steps. The reported
performance of ProMP is obtained by the policies after finetuning.
•Soft actor-critic with MLP network (SAC-MLP) [ 20]. An implementation of the soft
actor-critic algorithm with MLP policy and critic models.
•Probabilistic embeddings for actor-critic RL (PEARL) [ 42]. The context encoder of PEARL
is trained to make a precise prediction of the action-value function while maximize entropy.
The loss of the context encoder can be written as:
LPEARL
CE =Lcritic+DKL[p(z|τ)∥ N(0, I)]
10https://github.com/twni2016/pomdp-baselines
11https://github.com/ianchar/gpide
12https://github.com/FanmingL/ESCP
13https://github.com/jonasrothfuss/ProMP
21where Lcriticis the critic loss in SAC. DKL[p(z|τ)∥ N(0, I)]denotes the KL divergence
between distribution of the embedded context and a standard Gaussian distribution. In
p(z|τ),zdenotes the output of the context encoder based on a given trajectory. PEARL is
implemented with the officially provided codes14.
•SAC-GRU . This is an upgrade version of SAC with GRU models. It use standard SAC
algorithm to optimize an RNN policy.
The baselines compared in Fig. 9:
•RL2[25]. RL2is a meta-RL algorithm that trains a meta-learner to efficiently adapt to new
tasks by leveraging past experiences. It involves two nested loops: an inner loop where the
meta-learner interacts with the environment to learn task-specific policies, and an outer loop
where the meta-learner updates its parameters based on the performance across multiple
tasks. The inner loop is accomplished by an RNN.
•VariBad [19]. VariBAD (Variational Bayes for Adaptive Deep RL) is also a meta-RL method
that uses variational inference to enable fast adaptation to new tasks. It employs a Bayesian
approach to meta-learning, inferring task-specific posterior distributions over the parameters
of the policy and value function. Specifically, it first learns an RNN variational auto-encoder
to extract the sequential features of the dynamics or tasks. The policy then receives the
current state and the output of the context encoder to make adaptive decisions.
The results of the baselines in Fig. 9 are sourced from [11].
MF-GPT [ 15], i.e. the baseline in Fig. 10, denotes to train a Transformer network, i.e. GPT-2, to
solve the memory-based tasks. The baseline result of the baselines in Fig. 10 are from [15]15.
D.3 Implementation Details
D.3.1 State Space Models
In our implementation, we use Mamba to form the context encoder, as it supports processing
sequences in parallel. Therefore, we will first introduce state space models and Mamba before
discussing the network architecture.
State Space Model (SSM) is a sequential model [ 47,14,48] commonly used to model dynamic
systems. The discrete version of an SSM can be expressed as:
ht=Aht−1+Bxt, yt=Cht,
A= exp(∆ A),B= (∆A)−1(exp(∆ A)−I)·∆B,(23)
where ∆,A,B, andCare parameters; xt,yt, and htrepresent the input, output, and hidden state at
time step t, respectively. AandBare derived from ∆,A, andB.
Mamba [ 14] is an RNN variant that integrates SSM layers to capture temporal information. In Mamba,
the parameters ∆,B, andCare dependent on the input xt, forming an input-dependent selection
mechanism. Unlike traditional RNNs which repeatedly apply Eq. (23) in a for-loop to generate the
output sequence, Mamba employs a parallel associative scan, which enables the computation of the
output sequence in parallel. This method significantly enhances computational efficiency.
D.3.2 Network Architecture
Pre-encoder. The pre-encoders consist of one-layer MLPs with a consistent output dimension of
128.
Context Encoder. The context encoder is comprised of one or more Mamba blocks, each flanked by
two MLPs. Typically, the MLP before the Mamba block consists of a single layer with 256 neurons,
except for the Key-to-Door task, where a three-layer MLP with 256 neurons per layer is employed.
Except in Key-to-Door task, a single Mamba block is utilized, with a state dimension of 64. The
kernel size of the convolution1d layer is fixed at 8. Additionally, for these tasks, a position-wise
14https://github.com/katerakelly/oyster
15https://github.com/twni2016/Memory-RL
22feedforward network, used by Transformer [ 9], is appended to the end of the Mamba block to slightly
enhance stability during training. In Key-to-Door tasks, the convolution1d layer in Mamba is omitted,
as the focus shifts to long-term credit assignment, rendering the convolution1d unnecessary for
capturing short-term sequence features. Conversely, in Key-to-Door tasks with a credit assignment
length of 500, four Mamba blocks are employed to improve the long-term memory capacity.
The MLP after the Mamba block also comprises a single layer with an output dimension of 128.
MLP Policy/Critic. As detailed in Sec. 4.1, both the MLP policy and critic consist of MLPs with
two hidden layers, each comprising 256 neurons.
Activations. Linear activation functions are employed for the output of the pre-encoder, context
encoder, and the MLP policy/critic. For other layers, the ELU activation function [49] is used.
Optimizer. We use AdamW [50] for RESeL training.
D.3.3 Hyperparameters
The hyperparameters used for RESeL is listed in Table 2. We mainly tuned the learning rates, batch
size for each tasks. γand last reward as input are determined according to the characteristic of the
tasks. The batch size in Table 2 (1000 or 2000) is much larger than that used in previous MLP-based
SAC algorithms (128 or 256) [ 20]. This is because we train on at least one complete trajectory at a
time, and in most environments, the maximum trajectory length is 1000. Thus, batch sizes of 1000 or
2000 correspond to one or two trajectories of maximum length.
Table 2: Hyperparameters of RESeL.
Attribute Value Task
context encoder learning rate LRCE2×10−6classic MuJoCo and classic meta-RL tasks
10−5other tasks
other learning rate LRother for policy6×10−5classic MuJoCo and classic meta-RL tasks
3×10−4other tasks
other learning rate LRother for value2×10−4classic MuJoCo and classic meta-RL tasks
10−3other tasks
γ0.9999 Key-to-Door
0.99 other tasks
last reward as inputTrue classic meta-RL tasks
False other tasks
batch size2000 classic POMDP tasks
1000 other tasks
target entropy −1×Dimension of action all tasks
learning rate of α 10−4all tasks
soft-update factor for target value network 0.995 all tasks
number of the randomly sampled data 5000 all tasks
Hyperparameter Determination Some hyper-parameters (discount factor, reward as input) were
chosen based on the nature of the environments, while others (Context encoder LR, policy and value
function LR, and batch size) are determined via grid search:
•Context Encoder LR: We fixed the learning rate of the context encoder to be 1/30 of the
policy learning rate in all tasks. We aim to have the action variation caused by RNN updates
to be comparable to that caused by MLP updates, thus fixing the learning rate ratio between
the context encoder and MLP. After conducting parameter searches in several environments,
as shown in Fig. 20, we find the 1/30 ratio performed well, so we fixed this ratio across all
environments.
•Policy and Value Function LR: We mainly referred to the default learning rate settings in
CleanRL [ 51], using a policy learning rate of 3×10−4and an action-value function learning
rate of 10−3. We conducted a grid search between the aforementioned learning rates and
those reduced by a factor of 5(aiming to further stabilize training). We found that a smaller
23learning rate was more stable in tasks where the partial observability is not very significant
(e.g., MDPs like classic MuJoCo).
•Discount Factor ( γ):For general tasks, we used a common γvalue of 0.99. For environ-
ments requiring long-term memory (e.g., Key-to-Door), we used a larger γof 0.9999.
•Reward as Input: The Classic Meta-RL tasks require inferring the true reward function
based on real-time rewards. Thus, we also included real-time reward feedback as an input to
the policy and value in these tasks.
•Batch Size: We conducted a grid search for batch size, mainly exploring sizes that were 1x
or 2x the maximum trajectory length. We found that in classic POMDP tasks, a larger batch
size performed better. In other tasks, the advantage of a large batch size was not significant
and could even reduce training speed.
E More Experimental Results
E.1 Visualization
Figure 14: The t-SNE visualization of the outputs of the context encoder in Halfcheetah with different
gravity.
In order to explore what the context encoder has learned, we attempt to conduct in-depth analysis
on the embedded context data. We conducted experiments in the environment of Halfcheetah with
varying gravity. We uniformly sample gravity acceleration within the range of [2.90,33.10]m/s2,
resulting in 40 environments with different gravities. For each environment, we used a policy
learned by RESeL to collect one trajectory, with a length of 1000 steps. Ultimately, we obtained
data of size 40 ×1000, where each step’s output of the encoder, a 128-dimensional vector, was
saved. Subsequently, we performed t-SNE [ 52] processing on the 40 ×1000×128 data to map the
high-dimensional 128-dimensional data into 2-dimensional data which can assist in our visualization
analysis.
Finally, we presented the results in Fig. 14. In this figure, the colorbar represents the gravity
acceleration magnitude. The xandyaxes of the figure represent the two-dimensional data results
after t-SNE mapping. From the figure, we can distinctly observe the non-random distribution
characteristics of colors from left to right. The different data represented by the cool and warm
colors are not randomly intermingled but rather exhibit certain clustering characteristics. We can
find that the embeddings are strongly correlated with gravity acceleration, indicating that RESeL has
recovered the hidden factor of the POMDP environment from the historical interaction data.
E.2 Time Overhead Comparisons
We find the computational efficiency is also a common issue among previous recurrent RL methods, es-
pecially in the context of recurrent off-policy RL. This issue mainly comes from the non-parallelizable
nature of traditional RNNs. In our work, we tackle this problem by using the SSM structure, i.e.
Mamba to improve the computational efficiency. Mamba is able to process sequential data in parallel
based on the technique of parallel associative scan. In the following parts, we will analyze the time
cost of Mamba compared with that of GRU.
24MLP Mamba GRU
context encoder type0204060one-step update time (ms)Figure 15: One-step update time with different types of context encoder.
To evaluate the improvement in training speed of the policy, we conducted experiments utilizing
the HalfCheetah environment within Gym [ 53]. Maintaining consistency across all training con-
figurations, we substituted Mamba in the context encoder with MLP and GRU, ensuring identical
input/output dimensions for policy training. Each experiment contains 10,000 time steps, during
which we measure the average training time per step. Policy training employed full-length trajectories,
i.e. a fixed sequence length of 1000. The time costs associated with different network architectures
are depicted in Fig. 15.
The results indicate that Mamba possesses significantly lower time costs compared to GRU, amounting
to approximately 25% of the time taken by GRU. Mamba’s time cost surpasses that of MLP by
approximately 80%; nevertheless, it still presents a substantial speedup relative to GRU. These results
are based on utilizing a single layer/block.
Table 3: GPU utilization, memory, and time cost with various neural network types. Time denotes
the time cost for each update iteration, in HalfCheetah-v2. Normalized time is normalized with the
corresponding GRU time cost.
Network Type GPU Utilization GPU Memory (MB) Time (ms) Normalized Time
1 Layer/Block
FC 35% 1474 9.5 14.4%
Mamba 33% 1812 17 25.8%
GRU 58% 1500 66 100.0%
2 Layers/Blocks
FC 37% 1496 9.8 9.3%
Mamba 38% 1840 22 21.0%
GRU 75% 1558 105 100.0%
3 Layers/Blocks
FC 37% 1500 10.5 7.1%
Mamba 40% 1912 29 19.7%
GRU 77% 1592 147 100%
4 Layers/Blocks
FC 38% 1502 11 5.7%
Mamba 43% 1982 34 17.7%
GRU 78% 1612 192 100.0%
Time Overhead Comparisons with Different Number of Layers/Blocks. In Table 3, the GPU
utilization and time costs for different numbers of layers are detailed, complementing Fig. 15.
The results indicate that the normalized time for Mamba decreases as the layer count increases,
suggesting that Mamba becomes more computationally efficient with more blocks, compared with
GRU. Additionally, Mamba consistently uses less GPU resources than GRU, further demonstrating
its superior scalability and efficiency.
250 1000 2000 3000 4000 5000
sequence length0102030time (ms)
network forward
Mamba
GRU
0 1000 2000 3000 4000 5000
sequence length0102030time (ms)
network backwardFigure 16: Network forward/backward time overhead for GRU and Mamba layers.
Time Overhead Comparisons with Different Sequence Lengths. To evaluate how Mamba scales
with varying sequence lengths, we compared the forward and backward time costs of Mamba and GRU
using artificial data. The results, presented in Fig. 16, show the sequence length on the horizontal axis.
The left panel illustrates forward inference time, while the right panel depicts backward inference
time. The time cost for GRU increases linearly with sequence length in both forward and backward
scenarios. In contrast, Mamba’s time cost remains relatively constant, showing no significant growth
in either scenario. These results highlight Mamba’s computational efficiency and scalability with
respect to sequence length.
E.3 More Comparative Results
E.3.1 POMDP Tasks
Table 4: Average performance on the classic POMDP benchmark with gravity changes at 1.5M time
steps±one standard error over 6seeds.
RESeL (ours) PPO-GRU MF-RNN SAC-Transformer SAC-MLP TD3-MLP GPIDE-ESS VRM A2C-GRU
AntBLT-P-v0 2829±56 2103±80 352 ±88 894 ±36 1147 ±49 897 ±83 2597 ±76 323 ±37 916 ±60
AntBLT-V-v0 1971±60 690±158 1137 ±178 692 ±89 651 ±65 476 ±114 1017 ±80 291 ±23 264 ±60
HalfCheetahBLT-P-v0 2900±179 1460±143 2802 ±88 1400 ±655 970 ±47 906 ±19 2466 ±129 −1317±217 353 ±74
HalfCheetahBLT-V-v0 2678±176 1072±195 2073 ±69−449±723 513 ±77 177 ±115 1886 ±165 −1443±220−412±191
HopperBLT-P-v0 2769±85 1592±60 2234 ±102 1763 ±498 310 ±35 490 ±140 2373 ±568 557 ±85 467 ±78
HopperBLT-V-v0 2480±91 438 ±126 1003 ±426 240 ±192 243 ±4 223 ±28 2537±167 476±28 301 ±155
WalkerBLT-P-v0 2244±93 651±156 940 ±272 1150 ±320 483 ±86 505 ±32 1502 ±521 372 ±96 200 ±104
WalkerBLT-V-v0 1901±39 423±89 160 ±38 39 ±18 214 ±17 214 ±22 1701 ±160 216 ±71 26 ±5
Average 2471 1053 1462 716 566 486 2010 −190 264
In Table 4, the corresponding final returns in Fig. 7 are listed in detail. We can find that RESeL makes
notably enhancement and surpasses the previous SOTA (GPIDE) by 22.9%in terms of the average
performance.
E.3.2 Dynamics-Randomized Tasks
Table 5: Average performance on the MuJoCo benchmark with gravity changes at 2M time steps ±
one standard error over 6seeds.
RESeL (ours) SAC-MLP SAC-GRU ESCP PEARL EPI OSI ProMP
Ant-gravity-v2 5949±314 1840±358 758 ±98 3403 ±478 4065 ±293 854 ±2 878 ±62 1243 ±95
HalfCheetah-gravity-v2 8705±733 6225±1273 7022 ±451 7805 ±91 5453 ±19 6053 ±582 6874 ±147 885 ±104
Hopper-gravity-v2 2846±191 1237±192 1669 ±98 2683 ±369 2171 ±337 2193 ±68 1756 ±31 252 ±11
Humanoid-gravity-v2 6360±621 3491±569 3064 ±528 3857 ±8 3849 ±152 2673 ±196 3518 ±664 423 ±8
Walker2d-gravity-v2 5866±266 3242±3 2098 ±270 2983 ±73 4284 ±174 1901 ±242 1467 ±224 271 ±2
Average 5945 3207 2922 4146 3964 2735 2899 615
In Table 5, the corresponding final returns in Fig. 8 are listed in detail. We can find that RESeL makes
notably enhancement and surpasses the previous SOTA (ESCP) by 43.4%in terms of the average
performance.
26Table 6: Average performance on the classic MuJoCo benchmark at 300k, 1M, and 5M time steps,
over 6 trials ±standard errors.
Time step TD3 SAC TQC TD3+OFE TD7 RESeL (ours)
HalfCheetah-v2300k 7715 ±633 8052 ±515 7006 ±891 11294 ±247 15031±401 9456±232
1M 10574 ±897 10484 ±659 12349 ±878 13758 ±544 17434±155 12327±500
5M 14337 ±1491 15526 ±697 17459 ±258 16596 ±164 18165±255 16750±432
Hopper-v2300k 1289 ±768 2370 ±626 3251 ±461 1581 ±682 2948 ±464 3480±22
1M 3226 ±315 2785 ±634 3526±244 3121±506 3512±315 3508±522
5M 3682 ±83 3167 ±485 3462 ±818 3423 ±584 4075 ±225 4408±5
Walker2d-v2300k 1101 ±386 1989 ±500 2812 ±838 4018 ±570 5379±328 4373±144
1M 3946 ±292 4314 ±256 5321 ±322 5195 ±512 6097±570 5410±176
5M 5078 ±343 5681 ±329 6137 ±1194 6379 ±332 7397 ±454 8004±150
Ant-v2300k 1704 ±655 1478 ±354 1830 ±572 6348±441 6171 ±831 4181±438
1M 3942 ±1030 3681 ±506 3582 ±1093 7398 ±118 8509±422 6295±102
5M 5589 ±758 4615 ±2022 6329 ±1510 8547 ±84 10133±966 8006±63
Humanoid-v2300k 1344 ±365 1997 ±483 3117 ±910 3181 ±771 5332±714 4578±509
1M 5165 ±145 4909 ±364 6029 ±531 6032 ±334 7429±153 7280±168
5M 5433 ±245 6555 ±279 8361 ±1364 8951 ±246 10281±588 10490±381
E.3.3 Classic MDP tasks
Table 6 provides comprehensive training results for MuJoCo MDP environments, extending the
data from Table 1. Specifically, the table includes results recorded at 300k, 1M, and 5M time steps.
Baseline results are primarily sourced from [ 43]. In environments such as Hopper, Walker2d, and
Humanoid, RESeL achieves or surpasses the previous SOTA performance. Notably, RESeL may
underperform compared to previous baselines at the 300k time step. However, its asymptotic perfor-
mance eventually matches or exceeds the SOTA. This suggests that incorporating RNN structures
might slightly reduce sample efficiency initially, as the observation space expands to include full
historical data, requiring more optimization to find optimal policies. Nevertheless, RESeL ultimately
leverages the enlarged observation space to achieve superior asymptotic performance.
E.4 Ablation Studies on Context Length
0.0 0.5 1.0 1.5
timestep 1e60500100015002000return
AntBLT-V-v0
full trajectory trajectory segment0.0 0.5 1.0 1.5
timestep 1e61000
0100020003000
HalfCheetahBLT-V-v0
0.0 0.5 1.0 1.5
timestep 1e6080016002400
HopperBLT-V-v0
0.0 0.5 1.0 1.5
timestep 1e60500100015002000
WalkerBLT-V-v0
Figure 17: Ablation studies on the context length.
Since trajectory length (or called context length) in training data is often a critical parameter in
previous studies [ 11], our work has emphasized the importance of using complete trajectories.
Training with trajectory segments but deploying with full-trajectory length can lead to distribution
shift issues. To investigate this, we conducted experiments using full trajectory data versus trajectory
segments (64 steps). The results are illustrated in Fig. 17.
In general, models trained with full trajectory data perform better than those trained with trajec-
tory segments. However, in the AntBLT-V and HalfCheetah-V environments, the performance
gap is smaller. The key difference in these tasks is the longer trajectory lengths, especially in
HalfCheetahBLT-V , where the trajectory length is fixed at 1000 steps. The data in these tasks are
usually cyclic, while in other tasks, agents may drop and terminate trajectories before achieving stable
performance. We suspect that strong cyclic data can mitigate distribution shift issues, as trajectory
segments containing a complete cycle can effectively represent the properties of the full trajectory.
27E.5 More Comparisons of Policy Performance with different RNNs
0 2 4 6
timestep 1e51000
0100020003000stochastic policy return
HalfCheetahBLT-P-v0
RESeL-Mamba RESeL-GRU RESeL-Transformer0 2 4 6
timestep 1e51000
010002000
HalfCheetahBLT-V-v0
0 2 4 6
timestep 1e50600120018002400
WalkerBLT-P-v0
0 2 4 6
timestep 1e50500100015002000
WalkerBLT-V-v0
Figure 18: Learning curves in terms of the stochastic policy performance with different RNN
architectures.
5.0E-06 1.0E-05 6.0E-05 3.0E-04 6.0E-04 1.2E-03
LRother0500100015002000returnWalkerBLT-V-v0, LRCE=5×106
Figure 19: Sensitivity studies of the context-encoder-specific learning rates in terms of the average
final return.
The learning curves of the exploration policy with difference RNN architectures are depicted in
Fig. 18. Compared with Fig. 12, Fig. 18 includes evaluation with exploration noise added to the
actions. In Fig. 18, RESeL-GRU shows a more substantial improvement over RESeL-Mamba
than that in Fig. 12, suggesting that RESeL-GRU might perform better in tasks involving action
disturbances.
E.6 More Sensitivity Studies
Figure 19 serves as a supplementary panel to Fig. 11. In Fig. 19, we fixed LRCE= 5×10−6
and varied LRother from 5×10−6to1.2×10−3. The results suggest that the optimal LRother is
still0.0003 , which is significantly larger than LRCE. This finding further supports that the optimal
learning rates for LRCEandLRother should be different and not of the same magnitude.
The learning rate sensitivity analysis on the remaining seven POMDP tasks is presented in Fig. 20.
As shown, the conclusions in Sec. 5.3 still hold: when different learning rates are used for the
MLP and RNN components and the MLP learning rate is fixed, excessively low or high RNN
learning rates result in inefficient training. However, the optimal RNN learning rate generally falls
between 5.0×10−6and1.0×10−5. If the MLP and RNN share the same learning rate, their
performance rarely surpasses that of the setting with different learning rates, with the only exception
being the HalfCheetahBLT-P-v0 environment, where a single learning rate slightly outperforms the
different-learning-rate setting.
28AntBLT-P-v0 AntBLT-V-v00800160024003200return
HalfCheetahBLT-P-v0 HalfCheetahBLT-V-v00100020003000return
HopperBLT-P-v0 HopperBLT-V-v00800160024003200return
WalkerBLT-P-v0 WalkerBLT-V-v00600120018002400return
LRCE= 0.0, LRother= 3.0 × 104
LRCE= 5.0 × 107,LRother= 3.0 × 104
LRCE= 1.0 × 106,LRother= 3.0 × 104
LRCE= 5.0 × 106,LRother= 3.0 × 104
LRCE= 1.0 × 105,LRother= 3.0 × 104
LRCE= 5.0 × 105,LRother= 3.0 × 104
LRCE= 1.0 × 104,LRother= 3.0 × 104
LRCE= 3.0 × 104,LRother= 3.0 × 104
LRCE= 5.0 × 104,LRother= 3.0 × 104
LRCE= 1.0 × 103,LRother= 3.0 × 104
LRCE= 3.0 × 106,LRother= 3.0 × 106
LRCE= 6.0 × 106,LRother= 6.0 × 106
LRCE= 1.5 × 105,LRother= 1.5 × 105
LRCE= 3.0 × 105,LRother= 3.0 × 105
LRCE= 6.0 × 105,LRother= 6.0 × 105
LRCE= 6.0 × 104,LRother= 6.0 × 104
Figure 20: Sensitivity studies of the context-encoder-specific learning rates in terms of the average
final return in eight POMDP tasks. The variants with the highest final return are marked with ★.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification:
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See Sec. 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
30Justification: The proof of Proposition 1 can be found in Appendix A.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide a complete algorithm procedure in Appendix C, model architecture
in Sec. 4.1, and hyper-parameters in Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
31Answer: [Yes]
Justification: The codes and corresponding hyper-parameters can be found in https:
//github.com/FanmingL/Recurrent-Offpolicy-RL .
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide a the experiment details and hyper-parameters in Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Each result of RESeL is repeated with 6distinct random seeds. The standard
errors are reported.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
32•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We have analyzed and reported the information on the computer resources in
Sec. 5.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification:
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper presents work whose goal is to advance the field of recurrent
reinforcement learning. There are many potential societal consequences of our work, none
which we feel must be specifically highlighted here.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
33•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification:
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
34•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
35