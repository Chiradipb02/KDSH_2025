DH-Fusion: Depth-Aware Hybrid Feature Fusion for
Multimodal 3D Object Detection
Anonymous Author(s)
Affiliation
Address
email
Abstract
State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. 1
However, they neglect the factor of depth while designing the fusion strategy. In 2
this work, we for the first time point out that different modalities play different roles 3
as depth varies via statistical analysis and visualization. Based on this finding, we 4
propose a Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy that guides the 5
weights of point cloud and RGB image modalities by introducing depth encoding 6
at both global and local levels. Specifically, the Depth-Aware Global Feature 7
Fusion (DGF) module adaptively adjusts the weights of image Bird’s-Eye-View 8
(BEV) features in multi-modal global features via depth encoding. Furthermore, 9
to compensate for the information lost when transferring raw features to the BEV 10
space, we propose a Depth-Aware Local Feature Fusion (DLF) module, which 11
adaptively adjusts the weights of original voxel features and multi-view image 12
features in multi-modal local features via depth encoding. Extensive experiments 13
on the nuScenes dataset demonstrate that our DH-Fusion method surpasses previous 14
state-of-the-art methods w.r.t. NDS. Moreover, our DH-Fusion is more robust to 15
various kinds of corruptions, outperforming previous methods on nuScenes-C w.r.t. 16
both NDS and mAP. 17
1 Introduction 18
3D object detection has a wide range of applications in the fields of autonomous driving and robotics. 19
A large number of previous works have successfully focused on using a single modality, such as point 20
cloud or images, to design efficient 3D object detectors. However, the performance of these detectors 21
reaches a bottleneck due to the limitations of modality characteristics. For instance, the point cloud 22
modality can only provide rich geometric information while lacks detailed semantic information; 23
the image modality can only provide rich texture information while lacks three-dimensional spatial 24
information. To address the aforementioned issues, we are highly motivated to obtain comprehensive 25
information that represents objects by designing a LiDAR-camera 3D object detector. 26
In recent years, LiDAR-camera 3D object detection develops rapidly. Some works [ 1,4,28,33,67] 27
propose effective methods to integrate information from two modalities at the feature level. However, 28
they all overlook an important factor of depth in their fusion strategies. To understand how point 29
cloud and image information vary with depth, we first conduct statistical and visualization analysis 30
on the nuScenes-mini dataset [ 3], and find that: (1) The number of points representing objects at 31
near range is relatively large, which allows us to accurately determine the object’s location, size, and 32
category, even without the aid of images. As shown in Fig. 1a, there is an average of 163.7 points per 33
object within 0-10 meters, which is a substantial number. We also visualize a car at 6.8 meters in 34
Fig. 1b ①and find it encompasses a considerable number of points, well representing the shape. In 35
contrast, some background noise in the image may interfere with detection (Fig. 1b ②). (2) As the 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.5.9e4
10.4
1Number of Points Per Object
0-10      10 -20      20 -30     30 -40     40-50
Depth (m)20
103040100200
14.0e38.0e31.2e41.6e41.0e62.3e7
1.0e5
1.6e4
7.8e3
4.1e3163.9
2.9
0.40.3Point Pixel 1.0e7
Number of Pixels Per Object150(a) Statistical chart
Image Point Cloud6.8m
④(2)
③
①
 ②
42.1m
35.3m6.8m
42.1m
35.3m (b) Visualization
Figure 1: Statistical and visualization analysis on the nuScenes-mini dataset. (a) The average numbers
of points and pixels for each object at different depths. (b) Examples of near-range and long-range
objects in images and point cloud. Points within the bounding boxes are colored red for observation.
depth increases, the number of points representing objects decreases rapidly. As shown in Fig. 1a, 37
the number of points within 30-50 meters falls below one per object, meaning that many objects are 38
even not represented by any points, such as the object at 42.1 meters in Fig. 1b ③. In contrast, the 39
complete objects may still be observed on the image, as in Fig. 1b ④, where the image information 40
becomes more important. To address the above problems, we propose a feature fusion strategy that 41
adaptively adjusts the importance of the two modalities based on depth. 42
Specifically, we propose a novel method for multi-modal 3D object detection, namely Depth-Aware 43
Hybrid Feature Fusion (DH-Fusion). The innovation lies in adaptively adjusting the weights of 44
features by introducing depth encoding to hybrid feature fusion at both global and local levels. The 45
fusion strategy consists of two crucial components: Depth-Aware Global Feature Fusion (DGF) 46
module and Depth-Aware Local Feature Fusion (DLF) module. In DGF, we take point cloud Bird’s- 47
Eye-View (BEV) features and image BEV features as inputs, and dynamically adjust the weights of 48
image BEV features based on depth during fusion by utilizing a global-fusion transformer encoder 49
with a depth encoder. To compensate for the information lost when transforming raw features to 50
BEV space, we enhance the fused BEV features at a lower cost by utilizing the original instance 51
features. In DLF, we obtain 3D boxes by utilizing a Region Proposal Network (RPN). Then, the 52
3D boxes are projected into both LiDAR voxel features and multi-view image features to crop out 53
corresponding local instance features with more detailed information. Afterward, we take these as 54
inputs and dynamically adjust the weights of local multi-view image features and local LiDAR voxel 55
features based on depth through the use of a local-fusion transformer encoder with the depth encoder. 56
In the end, we update local features for each object on the global feature map to enhance the detailed 57
instance information of multi-modal global features for detection. 58
Our contributions are summarized as follows. 59
1. We for the first time point out that depth is an important factor to consider while fusing LiDAR 60
point cloud features and RGB image features for 3D object detection. From our statistical and 61
visualization analysis, we can see that image features play different roles as depth varies. 62
2. We propose a depth-aware hybrid feature fusion strategy that dynamically adjusts the weights of 63
features during feature fusion by introducing depth encoding at both global and local levels. The 64
above strategy can obtain high-quality features for detection, fully leveraging the advantages of 65
different modalities at various depths. 66
3. Our method is evaluated on the nuScenes [ 3] dataset and a more challenging nuScenes-C [ 13] 67
dataset, outperforming previous multi-modal methods and being robust to various kinds of data 68
corruptions. 69
2 Related Work 70
Since our method is based on conducting 3D object detection using data from multiple modalities, 71
including point cloud and images, we briefly review recent works in the following fields: LiDAR- 72
based 3D object detection, camera-based 3D object detection, and LiDAR-camera 3D object detection. 73
22.1 LiDAR-based 3D Object Detection 74
LiDAR-based 3D object detectors only take the point cloud as input. Based on their different data 75
representations, they can be divided into point-based [ 44–46,64,65], voxel-based [ 12,22,61,68,71], 76
and point-voxel-based [ 17,42,43] methods. The feature extraction networks of point-based methods 77
typically extract features directly from the point cloud through a point-based backbone [ 40], such as 78
PointRCNN [ 44]. The voxel-based methods first convert the point cloud into voxels and then extract 79
voxel features through a 3D sparse convolution network [ 14], such as V oxelNet [ 71]. Point-voxel- 80
based methods like PV-RCNN [ 42] combine the above two methods to extract and fuse point and 81
voxel features. The purpose of these approaches is to capture the geometric spatial information of the 82
point cloud. However, point cloud is sparse and incomplete, lacking detailed texture information, 83
which greatly limits the detection performance. 84
2.2 Camera-based 3D Object Detection 85
Camera-based 3D object detectors only take images as inputs. Depending on the form of inputs, 86
they can be divided into monocular [ 2,24,32,41,47,55], stereo [ 6,25,30,48,70], and multi-view 87
[19,27,56,62] 3D object detectors. Early works like FCOS3D [ 55] input a monocular image and 88
utilize 2D object detectors to directly predict 3D bounding boxes, but these approaches have limited 89
capability in capturing spatial information. Subsequently, stereo and multi-view 3D object detectors 90
are proposed to obtain more precise depth information by constructing spatial relationships among 91
multiple images, such as Stereo RCNN [ 25] and BEVDet [ 19]. These methods successfully achieve 92
purely visual 3D object detection, but they do not perform as well as LiDAR-based methods, because 93
the spatial depth information provided by images is not as direct and precise as that provided by point 94
cloud. 95
2.3 LiDAR-Camera 3D Object Detection 96
LiDAR-camera 3D object detectors take point cloud and images as inputs, and can be classified 97
into early-fusion-based [ 50,52,57,59,69], intermediate-fusion-based [ 1,4,28,33,67], and late- 98
fusion-based [ 37,38] 3D object detectors based on the location of multi-modal information fusion 99
[36]. 100
Early-fusion-based methods perform at the point level, where the typical approach involves enhancing 101
the raw point cloud with semantic information extracted from images. PointPainting [ 50] and Fu- 102
sionPainting [ 59] decorate the raw point cloud with semantic scores from 2D semantic segmentation. 103
Similarly, PointAugmenting [ 52] enhances the raw point cloud using features extracted from a 2D 104
semantic segmentation network. However, early-fusion-based methods are sensitive to alignment 105
errors between the two modalities. 106
Intermediate-fusion-based methods perform at the feature level. Transfusion [ 1] first proposes to 107
utilize the transformer for fine-grained fusion from LiDAR BEV features and multi-view image 108
features. FUTR3D [ 5] encode each modality using deformable attention [ 73] in its own coordinate 109
and concatenate them for fusion. BEVFusion [ 28,33] projects both point cloud and images to BEV 110
space for BEV feature fusion. SparseFusion [ 58] extracts instance-level features from both two 111
modalities separately, and fuse them to perform detection. Similarly, ObjectFusion [ 4] utilizes 3D 112
proposals from LiDAR modality to extract instance-level features for fusion. CMT [ 60] proposes 113
the simultaneous interaction between the object queries and multi-modal features in the transformer 114
encoder and decoder. IS-Fusion [ 67] proposes feature fusion at both the instance level and scene 115
level. The intermediate-fusion-based methods gradually become a mainstream approach due to the 116
diversity of fusion strategies. 117
Late-fusion-based methods perform at the bounding box level. Typically, CLOCs [ 37] obtains 2D and 118
3D bounding boxes by separately using 2D and 3D object detectors, and then combine them to achieve 119
more accurate 3D bounding boxes. However, the interaction between modalities in late-fusion-based 120
methods is very limited, which constrains model performance. 121
These multi-modal methods successfully outperform single-modal methods. However, their feature 122
fusion methods do not take depth into account. In contrast, our approach introduces depth information 123
to guide the hybrid feature fusion, boosting the performance of the detector. 124
3Multi -view ImagesLiDAR Point Cloud
3D Encoder
2D Encoder
…
…
…
Decoder
Detection
Head
Global Feature Fusion
 Local Feature Fusion
Depth EncoderInput Encoding Depth -Aware Global Feature Fusion Depth -Aware Local Feature Fusion Decoding
G
O
Select
SelectProject
Project
View 
TransferCompress
G
O
G
B
G
B
L
O
G
B
L
B
L
O
G
Bˆ
G
B
MSelect
Depth Encoder
Depth Encoder
M
Sine and 
Cosine
DeFigure 2: Overview of our method. It introduces depth encoding in both global and local feature
fusion to obtain depth-adaptive multi-modal representations for detection. is the multiplication
operation, and Mis the merge operation.
3 Methodology 125
In this section, we first give an overview of our proposed multi-modal 3D object detector, and then 126
provide a detailed introduction to our proposed feature fusion method. 127
3.1 Overview 128
We propose a multi-modal 3D object detection method via Depth-Aware Hybrid Feature Fusion 129
(DH-Fusion). As illustrated in Fig. 2, our approach consists of two important feature fusion modules: 130
Depth-Aware Global Feature Fusion (DGF) and Depth-Aware Local Feature Fusion (DLF). In the 131
following, we briefly describe the detection pipeline. 132
Inputs. First, we take the point cloud Pand multi-view images Ias inputs, where point cloud 133
consists of a set of points: P={P1, P2,···, PNl}, and each point has four dimensions: X-axis, 134
Y-axis, Z-axis, and intensity; the multi-view images comprise Ncimages: I={I1, I2,···, INc}, 135
each image captured by its corresponding camera. 136
Input Encoding. For the point cloud P, we use a 3D encoder to extract raw global voxel features 137
VG
O; for the multi-view images I, we use a 2D encoder to extract image features of all views IG
O. 138
Hybrid Feature Fusion. Then, for voxel features VG
O, we compress the height dimension to obtain 139
point cloud BEV features VG
B; for image features IG
O, we transform their perspective view to bird’s 140
eye view to obtain image BEV features IG
B. To fully leverage the features from two modalities, we 141
design a DGF module that aims to dynamically adjust the weights of image BEV features based 142
on depth values during feature fusion. Please refer to Sec. 3.2 for more details. To compensate 143
for the information lost when transforming raw features to BEV space, we propose a DLF module 144
that, based on depth, utilizes the raw features to enhance the detailed information of each object 145
instance in global multi-modal features. It consists of three processes: local feature selection, local 146
feature fusion, and merging local features into global features. First, we obtain the local multi-modal 147
BEV features FL
B, local voxel features VL
O, and local multi-view image features IL
O,by cropping the 148
corresponding global features based on the 3D boxes obtained from an RPN; then, it dynamically 149
and individually adjusts the weights of each local feature of VL
OandIL
Obased on depth values during 150
feature fusion; finally, we update local features for each object on the global feature map. Please 151
refer to Sec. 3.3 for more details. In this way, we obtain enhanced multi-modal global features for 152
detection. 153
Decoding. Based on the enhanced multi-modal global features ˆFG
Bthat contain rich semantic and 154
spatial information, we utilize a transformer decoder and a detection head to predict the object 155
categories and 3D bounding boxes. 156
4Cross Attention
Add & Norm
FFN
Add & Norm
Multiply & NormGlobal -Fusion Transformer
Q K V
G
B
G
B
Depth Encoder
G
B
Figure 3: Illustration of the DGF.
It consists of a global fusion trans-
former with the depth encoder.
Cross Attention
Add & Norm
Depth Encoder
Multiply & Norm
…Projection
Cross Attention
Add & Norm
Depth Encoder
Multiply & Norm
…
…Projection
Cat & Conv
MergeLocal Feature Selection Local -Fusion Transformer
Q KV Q KV
G
O
G
O
L
O
G
B
L
B
L
O
G
Bˆ
Figure 4: Illustration of the DLF. It consists of a local feature
selection module and a local fusion transformer with the
depth encoder.
3.2 Depth-Aware Global Feature Fusion 157
As shown in Fig. 3, the DGF module consists of a global-fusion transformer with a depth encoder. In 158
the following, we provide a detailed explanation of each component. 159
3.2.1 Depth Encoder 160
We introduce depth encoding (DE) in feature fusion to dynamically adjust the weights of image BEV 161
features during fusion. First, we build a depth matrix Mto store the depth value of each position 162
element pkrepresented as: 163
pk={(xk, yk) :dk}, k∈[1, n], (1)
where (xk, yk)are the positional coordinates, dkis the depth value, and nis the number of elements. 164
Then, we use Euclidean distance to calculate the distance between every element’s spatial location 165
(xk, yk)and the ego coordinate element’s location (xn
2, yn
2): 166
dk=E((xk, yk),(xn
2, yn
2)), k∈[1, n], (2)
where we denote E(·)as the Euclidean distance calculation. The depth matrix Mserves as a lookup 167
table to avoid redundant computation of depth values. Since the size of the BEV features is large and 168
the depth distribution is simple, to avoid introducing additional parameters, the depth encoding Deis 169
obtained by applying sine and cosine functions [49] to the depth matrix. 170
3.2.2 Global-Fusion Transformer 171
In the global-fusion transformer, we take the point cloud BEV features VG
B∈RW×H×Cand image 172
BEV features IG
B∈RW×H×Cas inputs, and integrate the depth encoding obtained above by multi- 173
plying it with the point cloud BEV features, forming the query QG
V=N(VG
B×Conv (De)), where 174
Conv (·)is a convolution operation to align with the channels of VG
B, and N(·)is a normalization 175
layer. The image BEV features are queried as the corresponding key KG
Iand value VG
I. We utilize 176
the multi-head cross attention to achieve the interacted feature ˆVG
Bbased on depth: 177
ˆVG
B=CA(QG
V, KG
I, VG
I), (3)
where CA(·)indicates the multi-head cross attention. Afterward, we aggregate the information from 178
both modalities to obtain the fused features FG
B: 179
FG
B=N(FFN (N(ˆVG
B+VG
B)) +N(ˆVG
B+VG
B)), (4)
5where N(·)is a normalization layer; FFN (·)specifies a feed-forward network containing two 180
convolution operations. In this way, we obtain fused features in which the image features play 181
different roles as the depth varies. 182
3.3 Depth-Aware Local Feature Fusion 183
As shown in Fig. 4, the DLF module consists of a local feature selection and a local-fusion transformer 184
with the depth encoder. In the following, we provide a detailed explanation of each component. 185
3.3.1 Local Feature Selection 186
To compensate for the information lost when transforming point cloud features and image features to 187
BEV space, we enhance the instance details of fused BEV features FG
Busing instance features from 188
raw voxel features VG
Oand multi-view image features IG
O. Specifically, we utilize an RPN to regress 189
t3D boxes based on the BEV features FG
B. We directly crop the global fused BEV features FG
B190
based on the regressed 3D boxes to obtain the local fused BEV features FL
B∈Rc×t. On the other 191
hand, we project the 3D boxes onto the raw voxel features and multi-view image features to obtain 192
their corresponding local features before global fusion, preserving richer information for each object 193
instance. Specifically, we utilize the voxel pooling operation [ 12], followed by a 3D convolution 194
operation and a linear layer, to extract local voxel features VL
O∈Rc×t; we transform the 3D boxes 195
from bird’s eye view to perspective view, and utilize the RoI Align operation [ 15], followed by a 196
linear layer, to extract instance image features IL
O∈Rc×t. By doing this, we obtain the hybrid 197
(before & after global fusion) local features, which will be sent to the subsequent fusion module. 198
3.3.2 Local-Fusion Transformer 199
In the local-fusion transformer, the weights of each local raw feature are dynamically adjusted based 200
on depth values during feature fusion, and we update local features for each object on the global 201
feature map. Specifically, we take the local multi-modal BEV features FL
B, local voxel features VL
O, 202
and local multi-view image features IL
Oas inputs, and integrate the depth encoding by multiplying 203
it with the local multi-modal BEV features, forming the query QL
F. The local multi-view image 204
features and local voxel features are respectively queried as the corresponding key KL
I,KL
Vand value 205
VL
I,VL
V. The two multi-head cross-attention modules are utilized to achieve the interacted features 206
ˆQL
F,ˆQL
F′. Note that the computation process of multi-head cross attention is similar to that described 207
in Sec. 3.2.2 and is omitted here. Afterward, we aggregate the above features: 208
ˆFL
B=Conv (Cat(ˆQL
F+FL
B,ˆQL
F′+FL
B′)), (5)
where Cat(·)is the concatenation operation; Conv (·)is used to align with the feature channels of 209
global fused BEV features FG
B. As a result, we obtain enhanced local features by dynamically calling 210
back rich information in raw modalities at various depths. Afterward, we update the global features 211
FG
Bby inserting the enhanced local features at corresponding locations. 212
4 Experiments 213
In this section, we will first introduce the dataset and evaluation metrics, followed by the implementa- 214
tion details. Then, we will compare our method with the state-of-the-art methods on nuScenes and 215
also present results on a more challenging dataset of nuScenes-C with data corruptions. Finally, we 216
will show the ablation studies and qualitative results. More experiments are provided in Appendix 217
A.2. 218
4.1 Experimental Setup 219
Datasets and evaluation metrics. We evaluate our proposed DH-Fusion on the nuScenes benchmark 220
[3] and a more challenging dataset of nuScenes-C [ 13] with data corruptions. nuScenes dataset 221
provides 700 scene sequences for training, 150 scene sequences for validation, and 150 scene 222
sequences for testing. Each sequence contains 40 frames of 32-beam LiDAR data, and each frame 223
6has six corresponding images covering a 360-degree field of view. It offers calibration matrices that 224
facilitate accurate projection of 3D points onto 2D pixels, and contains 10 object categories that are 225
commonly encountered within autonomous driving. nuScenes-C dataset provides 27 corruptions 226
with 5 severities on the nuScenes validation set, including corruptions at the weather, sensor, motion, 227
object, and alignment level. We use the nuScenes detection scores (NDS) and mean Average Precision 228
(mAP) to evaluate our detection results, where NDS is a comprehensive metric in nuScenes that 229
combines object translation, scale, orientation, velocity, and attribute errors. 230
Implementation details. We implement the proposed DH-Fusion with PyTorch [ 39] under the 231
open-source framework MMDetection3D [ 10]. Specifically, for the LiDAR branch, we use V oxelNet 232
[71] with FPN [ 61] as the 3D encoder. The voxel size is set to [0.075m, 0.075m, 0.1m], and the range 233
of point cloud is [-54m, 54m] along the X-axis, [-54m, 54m] along the Y-axis, and [-3m, 5m] along 234
the Z-axis. For the image branch, we use the ResNet18 [ 16], ResNet50 [ 16], and SwinTiny [ 34] with 235
FPN [ 29] as the 2D image encoder of DH-Fusion-light, -base, -large, respectively. Correspondingly, 236
the resolution of input images is resized to 256 ×704, 320 ×800, and 384 ×1056. Additionally, we 237
utilize BEVPoolV2 [ 18] to obtain image BEV features. Following [ 33], the feature size W×His set 238
to 180 ×180, the channel Cis set to 128, and the channel cis also set to 128. The multi-head cross 239
attention is implemented with 8 heads, and the FFN contains 2 MLP layers with a hidden dimension 240
of 128. Following [ 58], the number of regressed 3D boxes tis set to 200. More implementation 241
details are provided in Appendix A.1. 242
4.2 Comparison to the State of the Art 243
Aiming for a fair comparison, we categorize previous methods based on the types of 2D backbones 244
into ResNet50-based, SwinTiny-based, and others, and provide three versions of our proposed method, 245
named DH-Fusion-light, DH-Fusion-base, and DH-Fusion-large. The results are shown in Tab. 1. 246
(1) Compared with the ResNet50-based methods, our DH-Fusion-base outperforms the top method 247
FocalFormer3D [ 7] by up to 1 pp w.r.t. NDS under the same configuration. Specifically, we reach 248
74.0% w.r.t. NDS and 71.2% w.r.t. mAP on the validation set, and 74.7% w.r.t. NDS and 71.7% 249
w.r.t. mAP on the test set, while maintaining comparable inference speed of 8.7 FPS on a 3090 GPU. 250
(2) Compared with the SwinTiny-based methods and others, our DH-Fusion-large outperforms the 251
top method IS-Fusion [ 67] under the same configuration, and runs 2x faster than it. Specifically, we 252
reach 74.4% w.r.t. NDS on the validation set, and 75.4% w.r.t. NDS on the test set, while achieving a 253
faster inference speed of 5.7 FPS on a 3090 GPU, indicating that our proposed method is both more 254
effective and efficient. (3) Furthermore, our DH-Fusion-light surpasses the typical BEVFusion [ 33] 255
by up to 1 pp w.r.t. all metrics using a lighter 2D backbone, and achieves a real-time inference speed 256
of 13.8 FPS. Overall, our method achieves higher detection accuracy and faster inference speed. 257
4.3 Robustness to Corruptions 258
We further implement some experiments on the nuScenes-C [ 13] dataset to evaluate the model’s 259
robustness under various corruptions, including changes in weather, data loss or temporal-spatial 260
misalignment in multi-modal inputs, etc. The results for different kinds of corruptions are shown 261
in Tab. 2, and more detailed results for each fine-grained corruption are shown in Appendix A.2.3. 262
We find that our DH-Fusion-light still achieves an average performance of 68.67 %w.r.t. NDS and 263
63.07 %w.r.t. mAP under various corruptions, which only decreases by 4.63 pp w.r.t. NDS and 264
6.68 pp w.r.t. mAP, compared to its performance without corruptions. Performance drop is smaller 265
than that observed with previous methods including BEVFusion [ 28] across all kinds of corruptions, 266
indicating that our DH-Fusion-light possesses superior robustness. Furthermore, we observe that our 267
DH-Fusion-light is particularly robust against weather and object corruptions, where the performance 268
drop is less than 3pp. The more stable performance indicates that our method is more friendly to 269
practical applications, where data corruption may occur. 270
4.4 Ablation Studies 271
We conduct ablation studies to first demonstrate the effect of each component of DH-Fusion, then 272
to demonstrate the effect of depth encoding in DGF and DLF, and finally to assess the impact of 273
multiplying depth encoding. All method variants are implemented on the nuScenes validation dataset. 274
7Table 1: Comparisons with the state of the art on the nuScenes validation andtest sets. FPS is
measured on a 3090 GPU by default, and * denotes the inference speed on an A100 GPU referred
from the original paper. Note that all results are obtained without any model ensemble or test time
augmentation.
Methods Present at Image Size - 2D Backbone FPSValidation Test
NDS mAP NDS mAP
Image Backbone: ResNet50[16]
Trainsfusion [1] CVPR’22 320×800-ResNet50 6.5 71.3 67.5 71.7 68.9
DeepInteraction [66] NeurIPS’22 448×800-ResNet50 1.9 72.4 69.9 73.4 70.8
MSMDFusion [21] CVPR’23 448×800- ResNet50 2.1 72.1 69.7 74.0 71.5
FocalFormer3D [7] ICCV’23 320×800-ResNet50 9.2* 73.1 70.1 73.9 71.6
DH-Fusion-base (Ours) - 320×800-ResNet50 8.7 74.0 71.2 74.7 71.7
Image Backbone: SwinTiny[31]
BEVFusion [28] NeurIPS’22 448×800-SwinTiny 0.7* 71.0 67.9 71.8 69.2
BEVFusion [33] ICRA’23 256×704- SwinTiny 9.6 71.4 68.5 72.9 70.2
ObjectFusion [4] ICCV’23 256×704- SwinTiny - 72.3 69.8 73.3 71.0
SparseFusion [58] ICCV’23 256×704- SwinTiny 4.4 72.8 70.5 73.8 72.0
IS-Fusion [67] CVPR’24 384×1056-SwinTiny 3.2* 74.0 72.8 75.2 73.0
Image Backbone: Others
AutoAlignV2 [8] ECCV’22 640×1280-CSPNet [51] 4.8* 71.2 67.1 72.4 68.4
UVTR [26] NeurIPS’22 640×1280-ResNet101 [16] 1.8 70.2 65.4 71.1 67.1
FUTR3D [5] CVPR’23 900×1600-VOVNet [23] 3.3* 68.0 64.2 72.1 69.4
UniTR [54] ICCV’23 256×704-DSVT [53] 9.3* 73.3 70.5 74.5 70.9
CMT [60] ICCV’23 640×1600-VOVNet 6.0* 72.9 70.3 74.1 72.0
UniPAD [63] CVPR’24 900×1600-ConvNeXtS [34] - 73.2 69.9 73.9 71.0
DH-Fusion-large (Ours) - 384×1056-SwinTiny 5.7 74.4 72.3 75.4 72.8
DH-Fusion-light (Ours) - 256×704-ResNet18 13.8 73.3 69.8 74.2 70.9
Table 2: Robustness experiments on nuScenes-C. Numbers are NDS /mAP .
MethodsCorruptionAverageNone Weather Sensor Motion Object Alignment
FUTR3D [5] 68.05 / 64.17 62.75 / 55.51 63.66 / 56.83 53.16 / 44.43 65.45 / 61.04 62.83 / 57.60 62.82↓5.23/ 56.99↓7.18
TransFusion [1] 69.82 / 66.38 65.42 / 59.37 66.17 / 59.82 51.52 / 41.47 68.28 / 64.38 61.98 / 54.94 63.74↓6.08/ 58.73↓7.65
BEVFusion [33] 71.40 / 68.45 67.54 / 61.87 67.59 / 61.80 55.19 / 47.30 68.01 / 65.14 63.94 / 58.71 66.06↓5.34/ 61.03↓7.42
DH-Fusion-light (Ours) 73.30 /69.75 72.19 /67.48 69.16 /62.87 57.07 /47.52 71.01 /67.11 67.24 /62.38 68.67↓4.63/63.07↓6.68
Effect of DGF and DLF. To demonstrate the effect of DGF and DLF, we conduct experiments by 275
integrating the components one by one into the baseline, BEVFusion [ 33]. The results are shown 276
in Tab. 3. We find that our DGF improves the baseline performance by 1.0 pp w.r.t. NDS and 0.9 277
pp w.r.t. mAP. This demonstrates that dynamically adjusting the weights of the image BEV features 278
during fusion is effective for 3D object detection. Additionally, our DLF improves the baseline 279
performance by 1.3 pp w.r.t. NDS and 0.8 pp w.r.t. mAP, which indicates that dynamically adjusting 280
the weights of the local raw instance features based on depth during fusion effectively compensates 281
for the information loss caused by the transformation of global features into the BEV feature space. 282
The results of integrating both components show an improvement of 1.9 pp w.r.t. NDS and 1.3 pp 283
w.r.t. mAP, well verifying the benefits of dynamically fusing global and local hybrid features based 284
on depth. 285
Effect of depth encoding in DGF and DLF. To evaluate the effectiveness of our depth encoding, 286
we conduct experiments where the depth encoding is removed from the DGF and DLF modules, 287
respectively. The results are shown in Tab. 4. When removing the depth encoding from Baseline+DGF, 288
the performance drops by 0.6 pp w.r.t. NDS and 0.4 pp w.r.t. mAP. Similarly, when removing the 289
depth encoding from Baseline+DLF, the performance also decreases by 1.1 pp w.r.t. NDS and 0.9 pp 290
w.r.t. mAP. These results indicate that our depth encoding is effective. Furthermore, we observe that 291
removing the depth encoding from the DLF module results in a larger performance drop, suggesting 292
that depth encoding plays a more crucial role in local feature fusion. 293
Impact of different operations for depth encoding. We conduct experiments with different 294
operations of depth encoding, including concatenation, summation, and multiplication. The results 295
in Tab. 5, show that the multiplication operation consistently outperforms the summation and 296
concatenation operations w.r.t. both metrics. The superior performance of multiplication can be 297
attributed to its ability to more effectively modulate the feature maps based on depth information. 298
Unlike summation, which simply shifts the feature values, or concatenation, which increases the 299
dimensionality without direct interaction, multiplication allows for more interaction between the 300
8Table 3: Ablation studies of each
proposed module.
Baseline DGF DLF NDS mAP
! 71.4 68.5
! ! 72.4↑1.069.4↑0.9
! ! 72.7↑1.369.3↑0.8
! ! ! 73.3↑1.969.8↑1.3Table 4: Ablation studies of depth
encoding (DE) in DGF and DLF.
Methods NDS mAP
Baseline + DGF 72.4 69.4
w/o DE 71.8↓0.669.0↓0.4
Baseline + DLF 72.7 69.3
w/o DE 71.6↓1.168.4↓0.9Table 5: Ablation studies
of different operations for
depth encoding.
Methods NDS mAP
Summation 72.8 69.2
Concatenation 72.5 68.7
Multiplication 73.3 69.8
(a) Attention weights
 (b) Average map
Figure 5: Attention weights applied on
BEV image features in DGF vary with
depth.
Point Cloud BEV Feature
BEVFusion
DH-Fusion
（Ours）Left Front Image
Figure 6: Qualitative detection results and BEV fea-
tures of BEVFusion and ours. We show the ground
truth boxes in green, and the prediction boxes in blue.
depth encoding and features, leading to better feature representation and ultimately improving the 301
detection performance. 302
4.5 Qualitative Results 303
To better understand how depth encoding affects the feature fusion, in Fig. 5, we plot a curve to 304
observe how the attention weights applied on the image BEV features in our DGF module vary with 305
depth, and visualize the average attention map. It is evident that the weights of the image BEV 306
features stay low in near range, but go up significantly as depth increases when the depth is larger 307
than 40 meters. This trend supports our hypothesis that the image modality would become more 308
important as depth increases. In this way, our depth encoding allows the model to dynamically adjust 309
the weights of image BEV features based on depth. 310
We also compare the detection results of our DH-Fusion method with the baseline BEVFusion [ 33] 311
in Fig. 6, where we clearly find that our method better localizes those distant objects compared to 312
BEVFusion. These results demonstrate that our proposed multi-modal fusion strategy based on depth 313
is more effective for detection. Besides, we exhibit the corresponding BEV feature maps, where 314
our method shows a stronger feature response for the foreground objects, especially for distant ones. 315
That is why our feature fusion strategy can provide higher-quality detection results. More qualitative 316
results can be found in Appendix A.3. 317
5 Conclusion 318
In this paper, we for the first time point out that different modalities play different roles as depth varies 319
via statistical analysis and visualization. Based on this finding, we propose a feature fusion strategy 320
for multi-modal 3D object detection, namely Depth-Aware Hybrid Feature Fusion (DH-Fusion), that 321
dynamically adjusts the weights of features during feature fusion by introducing depth encoding at 322
both global and local levels. Extensive experiments on the nuScenes dataset demonstrate that our 323
DH-Fusion method surpasses previous state-of-the-art methods w.r.t. NDS. Moreover, our DH-Fusion 324
is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C 325
dataset w.r.t. both NDS and mAP. Our method uses an attention-based approach to interact with 326
the two modalities, making the detection results sensitive to modality loss. We plan to further 327
explore feature fusion methods that are robust to modality loss. Although our method improves 328
detection performance, emergency plans still need to be implemented in practical applications to 329
ensure personnel safety. 330
9References 331
[1]Bai, X., Hu, Z., Zhu, X., Huang, Q., Chen, Y ., Fu, H., Tai, C.L.: Transfusion: Robust lidar- 332
camera fusion for 3d object detection with transformers. In: CVPR (2022) 333
[2]Brazil, G., Liu, X.: M3d-rpn: Monocular 3d region proposal network for object detection. In: 334
ICCV (2019) 335
[3]Caesar, H., Bankiti, V ., Lang, A.H., V ora, S., Liong, V .E., Xu, Q., Krishnan, A., Pan, Y ., Baldan, 336
G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous driving. In: CVPR (2020) 337
[4]Cai, Q., Pan, Y ., Yao, T., Ngo, C.W., Mei, T.: Objectfusion: Multi-modal 3d object detection 338
with object-centric fusion. In: ICCV (2023) 339
[5]Chen, X., Zhang, T., Wang, Y ., Wang, Y ., Zhao, H.: Futr3d: A unified sensor fusion framework 340
for 3d detection. In: CVPR (2023) 341
[6]Chen, Y ., Liu, S., Shen, X., Jia, J.: Dsgn: Deep stereo geometry network for 3d object detection. 342
In: CVPR (2020) 343
[7]Chen, Y ., Yu, Z., Chen, Y ., Lan, S., Anandkumar, A., Jia, J., Alvarez, J.M.: Focalformer3d: 344
focusing on hard instance for 3d object detection. In: ICCV (2023) 345
[8]Chen, Z., Li, Z., Zhang, S., Fang, L., Jiang, Q., Zhao, F.: Deformable feature aggregation for 346
dynamic multi-modal 3d object detection. In: ECCV (2022) 347
[9]Chiu, H.k., Prioletti, A., Li, J., Bohg, J.: Probabilistic 3d multi-object tracking for autonomous 348
driving. arxiv 2020. arXiv preprint arXiv:2001.05673 (2020) 349
[10] Contributors, M.: MMDetection3D: OpenMMLab next-generation platform for general 3D 350
object detection. https://github.com/open-mmlab/mmdetection3d (2020) 351
[11] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical 352
image database. In: CVPR (2009) 353
[12] Deng, J., Shi, S., Li, P., Zhou, W., Zhang, Y ., Li, H.: V oxel r-cnn: Towards high performance 354
voxel-based 3d object detection. In: AAAI (2021) 355
[13] Dong, Y ., Kang, C., Zhang, J., Zhu, Z., Wang, Y ., Yang, X., Su, H., Wei, X., Zhu, J.: Bench- 356
marking robustness of 3d object detection to common corruptions. In: CVPR (2023) 357
[14] Graham, B., Engelcke, M., Van Der Maaten, L.: 3d semantic segmentation with submanifold 358
sparse convolutional networks. In: CVPR (2018) 359
[15] He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn. In: CVPR (2017) 360
[16] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR 361
(2016) 362
[17] Hu, J.S., Kuai, T., Waslander, S.L.: Point density-aware voxels for lidar 3d object detection. In: 363
CVPR (2022) 364
[18] Huang, J., Huang, G.: Bevpoolv2: A cutting-edge implementation of bevdet toward deployment. 365
arXiv:2211.17111 (2022) 366
[19] Huang, J., Huang, G., Zhu, Z., Ye, Y ., Du, D.: Bevdet: High-performance multi-camera 3d 367
object detection in bird-eye-view. arXiv:2112.11790 (2021) 368
[20] Huang, J., Ye, Y ., Liang, Z., Shan, Y ., Du, D.: Detecting as labeling: Rethinking lidar-camera 369
fusion in 3d object detection. arXiv arXiv:2311.07152 (2023) 370
[21] Jiao, Y ., Jie, Z., Chen, S., Chen, J., Ma, L., Jiang, Y .G.: Msmdfusion: Fusing lidar and camera 371
at multiple scales with multi-depth seeds for 3d object detection. In: CVPR (2023) 372
[22] Lang, A.H., V ora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars: Fast encoders 373
for object detection from point clouds. In: CVPR (2019) 374
10[23] Lee, Y ., Hwang, J.w., Lee, S., Bae, Y ., Park, J.: An energy and gpu-computation efficient 375
backbone network for real-time object detection. In: CVPR workshops (2019) 376
[24] Li, B., Ouyang, W., Sheng, L., Zeng, X., Wang, X.: Gs3d: An efficient 3d object detection 377
framework for autonomous driving. In: CVPR (2019) 378
[25] Li, P., Chen, X., Shen, S.: Stereo r-cnn based 3d object detection for autonomous driving. In: 379
CVPR (2019) 380
[26] Li, Y ., Chen, Y ., Qi, X., Li, Z., Sun, J., Jia, J.: Unifying voxel-based representation with 381
transformer for 3d object detection. In: NeurIPS (2022) 382
[27] Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Qiao, Y ., Dai, J.: Bevformer: Learning 383
bird’s-eye-view representation from multi-camera images via spatiotemporal transformers. In: 384
ECCV (2022) 385
[28] Liang, T., Xie, H., Yu, K., Xia, Z., Lin, Z., Wang, Y ., Tang, T., Wang, B., Tang, Z.: Bevfusion: 386
A simple and robust lidar-camera fusion framework. In: NeurIPS (2022) 387
[29] Lin, T.Y ., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks 388
for object detection. In: CVPR (2017) 389
[30] Liu, Y ., Wang, L., Liu, M.: Yolostereo3d: A step back to 2d for efficient stereo 3d detection. In: 390
ICRA. IEEE (2021) 391
[31] Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., Guo, B.: Swin transformer: 392
Hierarchical vision transformer using shifted windows. In: ICCV (2021) 393
[32] Liu, Z., Wu, Z., Tóth, R.: Smoke: Single-stage monocular 3d object detection via keypoint 394
estimation. In: CVPR (2020) 395
[33] Liu, Z., Tang, H., Amini, A., Yang, X., Mao, H., Rus, D.L., Han, S.: Bevfusion: Multi-task 396
multi-sensor fusion with unified bird’s-eye view representation. In: ICRA (2023) 397
[34] Liu, Z., Mao, H., Wu, C.Y ., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: 398
CVPR (2022) 399
[35] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint 400
arXiv:1711.05101 (2017) 401
[36] Mao, J., Shi, S., Wang, X., Li, H.: 3d object detection for autonomous driving: A comprehensive 402
survey. IJCV (2023) 403
[37] Pang, S., Morris, D., Radha, H.: Clocs: Camera-lidar object candidates fusion for 3d object 404
detection. In: IROS (2020) 405
[38] Pang, S., Morris, D., Radha, H.: Fast-clocs: Fast camera-lidar object candidates fusion for 3d 406
object detection. In: WACV (2022) 407
[39] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., 408
Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning 409
library. In: NeurIPS (2019) 410
[40] Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learning on point 411
sets in a metric space. In: NeurIPS (2017) 412
[41] Qin, Z., Wang, J., Lu, Y .: Monogrnet: A geometric reasoning network for monocular 3d object 413
localization. In: AAAI (2019) 414
[42] Shi, S., Guo, C., Jiang, L., Wang, Z., Shi, J., Wang, X., Li, H.: Pv-rcnn: Point-voxel feature set 415
abstraction for 3d object detection. In: CVPR (2020) 416
[43] Shi, S., Jiang, L., Deng, J., Wang, Z., Guo, C., Shi, J., Wang, X., Li, H.: Pv-rcnn++: Point-voxel 417
feature set abstraction with local vector representation for 3d object detection. IJCV (2022) 418
11[44] Shi, S., Wang, X., Li, H.: Pointrcnn: 3d object proposal generation and detection from point 419
cloud. In: CVPR (2019) 420
[45] Shi, S., Wang, Z., Shi, J., Wang, X., Li, H.: From points to parts: 3d object detection from point 421
cloud with part-aware and part-aggregation network. IEEE TPAMI (2020) 422
[46] Shi, W., Rajkumar, R.: Point-gnn: Graph neural network for 3d object detection in a point cloud. 423
In: CVPR (2020) 424
[47] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposi- 425
tion for monocular 3d object detection. In: ICCV (2021) 426
[48] Sun, J., Chen, L., Xie, Y ., Zhang, S., Jiang, Q., Zhou, X., Bao, H.: Disp r-cnn: Stereo 3d object 427
detection via shape prior guided instance disparity estimation. In: CVPR (2020) 428
[49] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., 429
Polosukhin, I.: Attention is all you need. In: NeurIPS (2017) 430
[50] V ora, S., Lang, A.H., Helou, B., Beijbom, O.: Pointpainting: Sequential fusion for 3d object 431
detection. In: CVPR (2020) 432
[51] Wang, C.Y ., Liao, H.Y .M., Wu, Y .H., Chen, P.Y ., Hsieh, J.W., Yeh, I.H.: Cspnet: A new 433
backbone that can enhance learning capability of cnn. In: CVPR workshops (2020) 434
[52] Wang, C., Ma, C., Zhu, M., Yang, X.: Pointaugmenting: Cross-modal augmentation for 3d 435
object detection. In: CVPR (2021) 436
[53] Wang, H., Shi, C., Shi, S., Lei, M., Wang, S., He, D., Schiele, B., Wang, L.: Dsvt: Dynamic 437
sparse voxel transformer with rotated sets. In: CVPR (2023) 438
[54] Wang, H., Tang, H., Shi, S., Li, A., Li, Z., Schiele, B., Wang, L.: Unitr: A unified and efficient 439
multi-modal transformer for bird’s-eye-view representation. In: ICCV (2023) 440
[55] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d 441
object detection. In: ICCV (2021) 442
[56] Wang, Y ., Guizilini, V .C., Zhang, T., Wang, Y ., Zhao, H., Solomon, J.: Detr3d: 3d object 443
detection from multi-view images via 3d-to-2d queries. In: Robot Learning (2022) 444
[57] Wu, H., Wen, C., Shi, S., Li, X., Wang, C.: Virtual sparse convolution for multimodal 3d object 445
detection. In: CVPR (2023) 446
[58] Xie, Y ., Xu, C., Rakotosaona, M.J., Rim, P., Tombari, F., Keutzer, K., Tomizuka, M., Zhan, W.: 447
Sparsefusion: Fusing multi-modal sparse representations for multi-sensor 3d object detection. 448
In: ICCV (2023) 449
[59] Xu, S., Zhou, D., Fang, J., Yin, J., Bin, Z., Zhang, L.: Fusionpainting: Multimodal fusion with 450
adaptive attention for 3d object detection. In: ITSC (2021) 451
[60] Yan, J., Liu, Y ., Sun, J., Jia, F., Li, S., Wang, T., Zhang, X.: Cross modal transformer via 452
coordinates encoding for 3d object dectection. In: ICCV (2023) 453
[61] Yan, Y ., Mao, Y ., Li, B.: Second: Sparsely embedded convolutional detection. Sensors (2018) 454
[62] Yang, C., Chen, Y ., Tian, H., Tao, C., Zhu, X., Zhang, Z., Huang, G., Li, H., Qiao, Y ., Lu, L., 455
et al.: Bevformer v2: Adapting modern image backbones to bird’s-eye-view recognition via 456
perspective supervision. In: CVPR (2023) 457
[63] Yang, H., Zhang, S., Huang, D., Wu, X., Zhu, H., He, T., Tang, S., Zhao, H., Qiu, Q., Lin, B., 458
He, X., Ouyang, W.: Unipad: A universal pre-training paradigm for autonomous driving. In: 459
CVPR (2024) 460
[64] Yang, Z., Sun, Y ., Liu, S., Shen, X., Jia, J.: Ipod: Intensive point-based object detector for point 461
cloud. arXiv:1812.05276 (2018) 462
12[65] Yang, Z., Sun, Y ., Liu, S., Shen, X., Jia, J.: Std: Sparse-to-dense 3d object detector for point 463
cloud. In: ICCV (2019) 464
[66] Yang, Z., Chen, J., Miao, Z., Li, W., Zhu, X., Zhang, L.: Deepinteraction: 3d object detection 465
via modality interaction. In: NeurIPS (2022) 466
[67] Yin, J., Shen, J., Chen, R., Li, W., Yang, R., Frossard, P., Wang, W.: Is-fusion: Instance-scene 467
collaborative fusion for multimodal 3d object detection. In: CVPR (2024) 468
[68] Yin, T., Zhou, X., Krahenbuhl, P.: Center-based 3d object detection and tracking. In: CVPR 469
(2021) 470
[69] Yin, T., Zhou, X., Krähenbühl, P.: Multimodal virtual point 3d detection. In: NeurIPS (2021) 471
[70] You, Y ., Wang, Y ., Chao, W.L., Garg, D., Pleiss, G., Hariharan, B., Campbell, M., Wein- 472
berger, K.Q.: Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving. 473
arXiv:1906.06310 (2019) 474
[71] Zhou, Y ., Tuzel, O.: V oxelnet: End-to-end learning for point cloud based 3d object detection. 475
In: CVPR (2018) 476
[72] Zhu, B., Jiang, Z., Zhou, X., Li, Z., Yu, G.: Class-balanced grouping and sampling for point 477
cloud 3d object detection. arXiv preprint arXiv:1908.09492 (2019) 478
[73] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers 479
for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020) 480
13NeurIPS Paper Checklist 481
1.Claims 482
Question: Do the main claims made in the abstract and introduction accurately reflect the 483
paper’s contributions and scope? 484
Answer: [Yes] 485
Justification: The main claims made in the abstract and introduction accurately reflect the 486
paper’s contributions and scope. The claims are clearly stated and are consistent with the 487
theoretical and experimental results presented in the paper. 488
Guidelines: 489
•The answer NA means that the abstract and introduction do not include the claims 490
made in the paper. 491
•The abstract and/or introduction should clearly state the claims made, including the 492
contributions made in the paper and important assumptions and limitations. A No or 493
NA answer to this question will not be perceived well by the reviewers. 494
•The claims made should match theoretical and experimental results, and reflect how 495
much the results can be expected to generalize to other settings. 496
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 497
are not attained by the paper. 498
2.Limitations 499
Question: Does the paper discuss the limitations of the work performed by the authors? 500
Answer: [Yes] 501
Justification: We discuss the limitations of our method, specifically that using an attention- 502
based approach to interact with the two modalities makes the detection results sensitive to 503
modality loss. 504
Guidelines: 505
•The answer NA means that the paper has no limitation while the answer No means that 506
the paper has limitations, but those are not discussed in the paper. 507
• The authors are encouraged to create a separate "Limitations" section in their paper. 508
•The paper should point out any strong assumptions and how robust the results are to 509
violations of these assumptions (e.g., independence assumptions, noiseless settings, 510
model well-specification, asymptotic approximations only holding locally). The authors 511
should reflect on how these assumptions might be violated in practice and what the 512
implications would be. 513
•The authors should reflect on the scope of the claims made, e.g., if the approach was 514
only tested on a few datasets or with a few runs. In general, empirical results often 515
depend on implicit assumptions, which should be articulated. 516
•The authors should reflect on the factors that influence the performance of the approach. 517
For example, a facial recognition algorithm may perform poorly when image resolution 518
is low or images are taken in low lighting. Or a speech-to-text system might not be 519
used reliably to provide closed captions for online lectures because it fails to handle 520
technical jargon. 521
•The authors should discuss the computational efficiency of the proposed algorithms 522
and how they scale with dataset size. 523
•If applicable, the authors should discuss possible limitations of their approach to 524
address problems of privacy and fairness. 525
•While the authors might fear that complete honesty about limitations might be used by 526
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 527
limitations that aren’t acknowledged in the paper. The authors should use their best 528
judgment and recognize that individual actions in favor of transparency play an impor- 529
tant role in developing norms that preserve the integrity of the community. Reviewers 530
will be specifically instructed to not penalize honesty concerning limitations. 531
3.Theory Assumptions and Proofs 532
14Question: For each theoretical result, does the paper provide the full set of assumptions and 533
a complete (and correct) proof? 534
Answer: [Yes] 535
Justification: We provide detailed theoretical statements and formulas along with their 536
descriptions in the paper. 537
Guidelines: 538
• The answer NA means that the paper does not include theoretical results. 539
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 540
referenced. 541
•All assumptions should be clearly stated or referenced in the statement of any theorems. 542
•The proofs can either appear in the main paper or the supplemental material, but if 543
they appear in the supplemental material, the authors are encouraged to provide a short 544
proof sketch to provide intuition. 545
•Inversely, any informal proof provided in the core of the paper should be complemented 546
by formal proofs provided in appendix or supplemental material. 547
• Theorems and Lemmas that the proof relies upon should be properly referenced. 548
4.Experimental Result Reproducibility 549
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 550
perimental results of the paper to the extent that it affects the main claims and/or conclusions 551
of the paper (regardless of whether the code and data are provided or not)? 552
Answer: [Yes] 553
Justification: We provide a detailed experimental setup in the paper, and the training and 554
testing details are provided in the supplementary material to ensure the reproducibility of 555
our results. 556
Guidelines: 557
• The answer NA means that the paper does not include experiments. 558
•If the paper includes experiments, a No answer to this question will not be perceived 559
well by the reviewers: Making the paper reproducible is important, regardless of 560
whether the code and data are provided or not. 561
•If the contribution is a dataset and/or model, the authors should describe the steps taken 562
to make their results reproducible or verifiable. 563
•Depending on the contribution, reproducibility can be accomplished in various ways. 564
For example, if the contribution is a novel architecture, describing the architecture fully 565
might suffice, or if the contribution is a specific model and empirical evaluation, it may 566
be necessary to either make it possible for others to replicate the model with the same 567
dataset, or provide access to the model. In general. releasing code and data is often 568
one good way to accomplish this, but reproducibility can also be provided via detailed 569
instructions for how to replicate the results, access to a hosted model (e.g., in the case 570
of a large language model), releasing of a model checkpoint, or other means that are 571
appropriate to the research performed. 572
•While NeurIPS does not require releasing code, the conference does require all submis- 573
sions to provide some reasonable avenue for reproducibility, which may depend on the 574
nature of the contribution. For example 575
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 576
to reproduce that algorithm. 577
(b)If the contribution is primarily a new model architecture, the paper should describe 578
the architecture clearly and fully. 579
(c)If the contribution is a new model (e.g., a large language model), then there should 580
either be a way to access this model for reproducing the results or a way to reproduce 581
the model (e.g., with an open-source dataset or instructions for how to construct 582
the dataset). 583
(d)We recognize that reproducibility may be tricky in some cases, in which case 584
authors are welcome to describe the particular way they provide for reproducibility. 585
In the case of closed-source models, it may be that access to the model is limited in 586
15some way (e.g., to registered users), but it should be possible for other researchers 587
to have some path to reproducing or verifying the results. 588
5.Open access to data and code 589
Question: Does the paper provide open access to the data and code, with sufficient instruc- 590
tions to faithfully reproduce the main experimental results, as described in supplemental 591
material? 592
Answer: [No] 593
Justification: We release the experimental details in the paper, and the code will be released 594
after the paper is accepted. 595
Guidelines: 596
• The answer NA means that paper does not include experiments requiring code. 597
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 598
public/guides/CodeSubmissionPolicy ) for more details. 599
•While we encourage the release of code and data, we understand that this might not be 600
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 601
including code, unless this is central to the contribution (e.g., for a new open-source 602
benchmark). 603
•The instructions should contain the exact command and environment needed to run to 604
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 605
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 606
•The authors should provide instructions on data access and preparation, including how 607
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 608
•The authors should provide scripts to reproduce all experimental results for the new 609
proposed method and baselines. If only a subset of experiments are reproducible, they 610
should state which ones are omitted from the script and why. 611
•At submission time, to preserve anonymity, the authors should release anonymized 612
versions (if applicable). 613
•Providing as much information as possible in supplemental material (appended to the 614
paper) is recommended, but including URLs to data and code is permitted. 615
6.Experimental Setting/Details 616
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 617
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 618
results? 619
Answer: [Yes] 620
Justification: We provide a detailed experimental setup in the paper, and the training and 621
testing details are provided in the supplementary material. 622
Guidelines: 623
• The answer NA means that the paper does not include experiments. 624
•The experimental setting should be presented in the core of the paper to a level of detail 625
that is necessary to appreciate the results and make sense of them. 626
•The full details can be provided either with the code, in appendix, or as supplemental 627
material. 628
7.Experiment Statistical Significance 629
Question: Does the paper report error bars suitably and correctly defined or other appropriate 630
information about the statistical significance of the experiments? 631
Answer: [Yes] 632
Justification: We provide data explanations and statistical methods for obtaining statistical 633
results in the paper. 634
Guidelines: 635
• The answer NA means that the paper does not include experiments. 636
16•The authors should answer "Yes" if the results are accompanied by error bars, confi- 637
dence intervals, or statistical significance tests, at least for the experiments that support 638
the main claims of the paper. 639
•The factors of variability that the error bars are capturing should be clearly stated (for 640
example, train/test split, initialization, random drawing of some parameter, or overall 641
run with given experimental conditions). 642
•The method for calculating the error bars should be explained (closed form formula, 643
call to a library function, bootstrap, etc.) 644
• The assumptions made should be given (e.g., Normally distributed errors). 645
•It should be clear whether the error bar is the standard deviation or the standard error 646
of the mean. 647
•It is OK to report 1-sigma error bars, but one should state it. The authors should 648
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 649
of Normality of errors is not verified. 650
•For asymmetric distributions, the authors should be careful not to show in tables or 651
figures symmetric error bars that would yield results that are out of range (e.g. negative 652
error rates). 653
•If error bars are reported in tables or plots, The authors should explain in the text how 654
they were calculated and reference the corresponding figures or tables in the text. 655
8.Experiments Compute Resources 656
Question: For each experiment, does the paper provide sufficient information on the com- 657
puter resources (type of compute workers, memory, time of execution) needed to reproduce 658
the experiments? 659
Answer: [Yes] 660
Justification: We provide hardware computer resources for training and testing. 661
Guidelines: 662
• The answer NA means that the paper does not include experiments. 663
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 664
or cloud provider, including relevant memory and storage. 665
•The paper should provide the amount of compute required for each of the individual 666
experimental runs as well as estimate the total compute. 667
•The paper should disclose whether the full research project required more compute 668
than the experiments reported in the paper (e.g., preliminary or failed experiments that 669
didn’t make it into the paper). 670
9.Code Of Ethics 671
Question: Does the research conducted in the paper conform, in every respect, with the 672
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 673
Answer: [Yes] 674
Justification: The research conducted in our paper complies with NeurIPS ethical standards 675
in all aspects. 676
Guidelines: 677
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 678
•If the authors answer No, they should explain the special circumstances that require a 679
deviation from the Code of Ethics. 680
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 681
eration due to laws or regulations in their jurisdiction). 682
10.Broader Impacts 683
Question: Does the paper discuss both potential positive societal impacts and negative 684
societal impacts of the work performed? 685
Answer: [Yes] 686
Justification: We discuss that although our method has good performance, practical applica- 687
tions need to ensure personnel safety. 688
17Guidelines: 689
• The answer NA means that there is no societal impact of the work performed. 690
•If the authors answer NA or No, they should explain why their work has no societal 691
impact or why the paper does not address societal impact. 692
•Examples of negative societal impacts include potential malicious or unintended uses 693
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 694
(e.g., deployment of technologies that could make decisions that unfairly impact specific 695
groups), privacy considerations, and security considerations. 696
•The conference expects that many papers will be foundational research and not tied 697
to particular applications, let alone deployments. However, if there is a direct path to 698
any negative applications, the authors should point it out. For example, it is legitimate 699
to point out that an improvement in the quality of generative models could be used to 700
generate deepfakes for disinformation. On the other hand, it is not needed to point out 701
that a generic algorithm for optimizing neural networks could enable people to train 702
models that generate Deepfakes faster. 703
•The authors should consider possible harms that could arise when the technology is 704
being used as intended and functioning correctly, harms that could arise when the 705
technology is being used as intended but gives incorrect results, and harms following 706
from (intentional or unintentional) misuse of the technology. 707
•If there are negative societal impacts, the authors could also discuss possible mitigation 708
strategies (e.g., gated release of models, providing defenses in addition to attacks, 709
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 710
feedback over time, improving the efficiency and accessibility of ML). 711
11.Safeguards 712
Question: Does the paper describe safeguards that have been put in place for responsible 713
release of data or models that have a high risk for misuse (e.g., pretrained language models, 714
image generators, or scraped datasets)? 715
Answer: [NA] 716
Justification: The model of the paper dos not address the issues mentioned in the guidelines. 717
Guidelines: 718
• The answer NA means that the paper poses no such risks. 719
•Released models that have a high risk for misuse or dual-use should be released with 720
necessary safeguards to allow for controlled use of the model, for example by requiring 721
that users adhere to usage guidelines or restrictions to access the model or implementing 722
safety filters. 723
•Datasets that have been scraped from the Internet could pose safety risks. The authors 724
should describe how they avoided releasing unsafe images. 725
•We recognize that providing effective safeguards is challenging, and many papers do 726
not require this, but we encourage authors to take this into account and make a best 727
faith effort. 728
12.Licenses for existing assets 729
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 730
the paper, properly credited and are the license and terms of use explicitly mentioned and 731
properly respected? 732
Answer: [Yes] 733
Justification: We have annotated the cited papers and datasets in our paper. 734
Guidelines: 735
• The answer NA means that the paper does not use existing assets. 736
• The authors should cite the original paper that produced the code package or dataset. 737
•The authors should state which version of the asset is used and, if possible, include a 738
URL. 739
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 740
18•For scraped data from a particular source (e.g., website), the copyright and terms of 741
service of that source should be provided. 742
•If assets are released, the license, copyright information, and terms of use in the 743
package should be provided. For popular datasets, paperswithcode.com/datasets 744
has curated licenses for some datasets. Their licensing guide can help determine the 745
license of a dataset. 746
•For existing datasets that are re-packaged, both the original license and the license of 747
the derived asset (if it has changed) should be provided. 748
•If this information is not available online, the authors are encouraged to reach out to 749
the asset’s creators. 750
13.New Assets 751
Question: Are new assets introduced in the paper well documented and is the documentation 752
provided alongside the assets? 753
Answer: [NA] 754
Justification: The paper does not release new assets 755
Guidelines: 756
• The answer NA means that the paper does not release new assets. 757
•Researchers should communicate the details of the dataset/code/model as part of their 758
submissions via structured templates. This includes details about training, license, 759
limitations, etc. 760
•The paper should discuss whether and how consent was obtained from people whose 761
asset is used. 762
•At submission time, remember to anonymize your assets (if applicable). You can either 763
create an anonymized URL or include an anonymized zip file. 764
14.Crowdsourcing and Research with Human Subjects 765
Question: For crowdsourcing experiments and research with human subjects, does the paper 766
include the full text of instructions given to participants and screenshots, if applicable, as 767
well as details about compensation (if any)? 768
Answer: [NA] 769
Justification: The paper does not involve crowdsourcing nor research with human subjects. 770
Guidelines: 771
•The answer NA means that the paper does not involve crowdsourcing nor research with 772
human subjects. 773
•Including this information in the supplemental material is fine, but if the main contribu- 774
tion of the paper involves human subjects, then as much detail as possible should be 775
included in the main paper. 776
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 777
or other labor should be paid at least the minimum wage in the country of the data 778
collector. 779
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 780
Subjects 781
Question: Does the paper describe potential risks incurred by study participants, whether 782
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 783
approvals (or an equivalent approval/review based on the requirements of your country or 784
institution) were obtained? 785
Answer: [NA] 786
Justification: The paper does not involve crowdsourcing nor research with human subjects. 787
Guidelines: 788
•The answer NA means that the paper does not involve crowdsourcing nor research with 789
human subjects. 790
19•Depending on the country in which research is conducted, IRB approval (or equivalent) 791
may be required for any human subjects research. If you obtained IRB approval, you 792
should clearly state this in the paper. 793
•We recognize that the procedures for this may vary significantly between institutions 794
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 795
guidelines for their institution. 796
•For initial submissions, do not include any information that would break anonymity (if 797
applicable), such as the institution conducting the review. 798
20A Appendix 799
A.1 Additional Implementation Details 800
During training, we adopt a one-stage strategy like DAL [ 20]. The whole pipeline is trained for a 801
total of 20 epochs with the AdamW optimizer [ 35] loading from the pre-trained weights from the 802
ImageNet [ 11] classification task only. Meanwhile, we use CBGS [ 72] to resample the training data, 803
and the one-cycle learning policy with a maximum learning rate of 2.0×10−4. The batch size is set 804
to 8 on 4 3090 RTX GPUs. We adopt random flipping along both X and Y-axis, the random scaling in 805
[0.95, 1.05], and random rotation in [- π/8,π/8] to augment the LiDAR data, and the random rotation 806
in [-5.4◦, 5.4◦] and random resizing in [-0.06, 0.44] to augment the images. During evaluation, we 807
test a single model without any data augmentation on a single 3090 RTX GPU. 808
A.2 Additional Experiments 809
A.2.1 3D Multi-Object Tracking Experiments 810
We evaluate our DH-Fusion on the nuScenes tracking benchmark for 3D multi-object tracking (MOT) 811
task. Following ObjectFusion [ 4], we adopt the same tracking-by-detection algorithm that uses 812
velocity-based closest point distance matching, which is more effective than 3D Kalman filter [ 9]. 813
For fair comparisons, we report the results of our DH-Fusion-light capable of real-time detection 814
on the nuScenes validation set, as shown in Tab. 6. We find that our DH-Fusion-light outperforms 815
BEVFusion [ 33] and ObjectFusion [ 4] by 2.0 pp and 0.6 pp w.r.t. AMOTA. These results demonstrate 816
that our DH-Fusion provides 3D detection boxes of higher quality, benefiting the downstream task of 817
3D MOT. 818
Table 6: Comparisons on nuScenes validation set for 3D multi-object tracking.
Methods AMOTA ↑AMOTP ↓IDS↓
TransFusion [1] 71.8 60.3 694
BEVFusion [33] 72.8 59.4 764
ObjectFusion [4] 74.2 54.3 611
DH-Fusion-light (Ours) 74.8 50.3 539
A.2.2 Evaluation at Different Depths 819
Since our fusion strategy is depth-aware, it is necessary to validate our method at different depths. 820
Following [ 4], we categorize annotation and prediction ego distances into three groups: Near (0- 821
20m), Middle (20-30m), and Far (>30m). As shown in Tab. 7, compared to ObjectFusion [ 4], our 822
DH-Fusion-light consistently improves performance across all depth ranges. Specifically, our method 823
achieves a 47.1 mAP in the long range (>30m), surpassing ObjectFusion by 5.5 pp w.r.t. mAP. These 824
results indicate that our method is more effective across different depths, especially in detecting 825
distant objects. 826
Table 7: Comparisons on nuScenes validation set at different depths. The numbers are mAP .
Methods Near Middle Far
TransFusion-L [1] 77.5 60.9 34.8
BEVFusion [33] 79.4 64.9 40.0
ObjectFusion [4] 79.7 65.4 41.6
DH-Fusion-light (Ours) 80.3 66.5 47.1
A.2.3 Detailed Results on the nuScenes-C 827
We further provide the detailed results of each fine-grained corruption on nuScenes-C in Tab. 8. The 828
results are highly consistent with the average values of each kind of data corruption. 829
A.3 More Visualization 830
As an extension of Fig. 6 in the manuscript, we provide additional examples of 3D object detection 831
results and BEV features from our baseline, BEVFusion [ 33], and our DH-Fusion. In various 832
samples, our method consistently achieves higher accuracy and recall in 3D detection results, with 833
21stronger feature responses for distant objects compared to BEVFusion. These results demonstrate the 834
effectiveness of the proposed method in dynamically adjusting the weights of features based on depth 835
during fusion at both global and local levels. 836
Table 8: Comparisons for each corruption level on the nuScenes-C. Corruptions exist in both
modalities by default. (L) means that only the point cloud modality has corruptions, and (C) means
that only the image modality has corruptions. Numbers are NDS /mAP .
Corruption FUTR3D TransFusion BEVFusion DH-Fusion
None 68.5 / 64.17 69.82 / 66.38 71.40 / 68.45 73.30 /69.75
WeatherSnow 61.52 / 52.73 68.29 / 63.30 68.33 / 62.84 71.47 /65.98
Rain 64.47 / 58.40 69.40 / 65.35 70.14 / 66.13 72.05 /67.32
Fog 61.20 / 53.19 62.62 / 53.67 62.73 / 54.10 72.13 /67.24
Sunlight 63.61 / 57.70 61.36 / 55.14 68.95 / 64.42 73.18 /69.44
SensorDensity 67.58 / 63.72 69.42 / 65.77 71.01 / 67.79 72.94 /69.15
Cutout 66.91 / 62.25 68.30 / 63.66 70.09 / 66.18 71.99 /67.45
Crosstalk 67.17 / 62.66 68.83 / 64.67 70.72 / 67.32 73.23 /69.55
FOV Lost 45.66 / 26.32 47.89 / 24.63 48.65 /27.17 43.41 / 20.78
Gaussian (L) 64.10 / 58.94 62.32 / 55.10 65.99 / 60.64 69.04 /63.51
Uniform (L) 67.28 / 63.21 68.68 / 64.72 70.18 / 66.81 72.54 /68.79
Impulse (L) 67.47 / 63.42 69.06 / 65.51 70.63 / 67.54 72.75 /68.91
Gussian (C) 62.92 / 54.96 68.94 / 64.52 69.35 / 64.44 71.55 /66.16
Uniform (C) 64.43 / 57.61 69.33 / 65.26 70.06 / 65.81 72.46 /67.99
Impulse (C) 63.07 / 55.16 68.89 / 64.37 69.25 / 64.30 71.66 /66.41
MotionCompensation 39.62 /31.87 25.69 / 9.01 36.76 / 27.57 32.51 / 15.99
Moving Obj. 56.41 / 45.43 60.03 / 51.01 59.42 / 51.63 68.12 /60.62
Motion Blur 63.44 / 55.99 68.85 / 64.39 69.38 / 64.74 70.58 /65.95
ObjectLocal Density 67.62 / 63.60 69.34 / 65.65 70.77 / 67.42 72.48 /68.87
Local Cutout 66.45 / 61.85 67.97 / 63.33 68.11 / 63.41 69.62 /64.17
Local Gaussian 66.85 / 62.94 67.96 / 63.76 68.32 / 64.34 71.32 /67.14
Local Uniform 67.92 / 64.09 69.67 / 66.20 70.68 / 67.58 71.34 / 66.03
Local Impulse 67.89 / 64.02 69.64 / 66.29 70.93 / 67.91 71.83 /68.15
Shear 61.15 / 55.42 66.43 / 62.32 62.95 / 60.72 68.41 /65.23
Scale 62.00 / 56.79 67.81 / 64.13 66.00 / 64.57 71.40 /68.90
Rotation 63.67 / 59.64 67.42 / 63.36 66.31 / 65.13 71.62 /68.35
AlignmentSpatial 67.75 / 63.77 69.72 / 66.22 71.35 / 68.39 71.95 /69.52
Temporal 57.91 / 51.43 54.23 / 43.65 56.62 / 49.02 62.53 /55.24
Average 62.82 / 56.99 64.71 / 58.73 66.06 / 61.03 68.67 /63.07
22Figure 7: More examples of 3D object detection results and BEV features from BEVFusion
and ours. We show the ground truth boxes in green, and the prediction boxes in blue. We
use red circles to highlight the comparisons of ours with BEVFusion.
23