Rule Based Learning with Dynamic (Graph) Neural
Networks
Anonymous Author(s)
Affiliation
Address
email
Abstract
A common problem of classical neural network architectures is that additional 1
information or expert knowledge cannot be naturally integrated into the learning 2
process. To overcome this limitation, we propose a two-step approach consisting 3
of (1) generating formal rules from knowledge and (2) using these rules to define 4
rule based layers – a new type of dynamic neural network layer. The focus of this 5
work is on the second step, i.e., rule based layers that are designed to dynamically 6
arrange learnable parameters in the weight matrices and bias vectors for each input 7
sample following a formal rule. Indeed, we prove that our approach generalizes 8
classical feed-forward layers such as fully connected and convolutional layers by 9
choosing appropriate rules. As a concrete application we present rule based graph 10
neural networks (RuleGNNs) that are by definition permutation equivariant and 11
able to handle graphs of arbitrary sizes. Our experiments show that RuleGNNs 12
are comparable to state-of-the-art graph classifiers using simple rules based on 13
the Weisfeiler-Leman labeling and pattern counting. Moreover, we introduce new 14
synthetic benchmark graph datasets to show how to integrate expert knowledge 15
into RuleGNNs making them more powerful than ordinary graph neural networks. 16
1 Introduction 17
Using expert knowledge to increase the efficiency, interpretability or predictive performance of 18
a neural network is an evolving research direction in machine learning [ 21,23]. Many ordinary 19
neural network architectures are not capable of using external and structural information such as 20
expert knowledge or meta-data, e.g., graph structures in a dynamic way. We would like to motivate 21
the importance of “expert knowledge” by considering the following example. Maybe one of the 22
best studied examples based on knowledge integration are convolutional neural networks [ 12]. 23
Convolutional neural networks for images use at least two extra pieces of “expert knowledge” that is: 24
neighbored pixels correlate , and the structure of images is homogeneous . The consequence of this 25
knowledge is the use of receptive fields and weight sharing. It is a common fact that the usage of 26
this information about images has highly improved the predictive performance over fully connected 27
neural networks. But what if expert knowledge suggests that rectangular convolutional kernels are 28
not suitable to solve the task? In this case the ordinary convolutional neural network architecture 29
is too static to adapt to the new information. Dynamic neural networks are not only applicable to 30
images but also to other data types such as video [ 25], text [ 10], or graphs [ 19]. The limitation 31
of such approaches is that expert knowledge is somehow implicit and not directly encoded in the 32
network structure, i.e., for each new information a new architecture has to be designed. Thus, our 33
goal is to extract the essence of dynamic neural networks by defining a new type of neural network 34
layer that is on the one side able to use expert knowledge in a dynamic way and on the other side 35
easily configurable. Our solution to this problem are rule based layers that are able to encode expert 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.(a) Learned weights and bias for the best model of
the DHFR dataset.
(b) Learned weights for the best model of the IMDB-
BINARY dataset.
Figure 1: Visualization of the learnable parameters of our RuleGNN on DHFR (a) and IMDB-
BINARY (b) for three different graphs. Positive weights are denoted by red arrows and negative
weights by blue arrows. The arrow thicknesss and color corresponds to the absolute value of the
weight. The bias is denoted by the size of the node. The second image of (a) resp. (b) shows the
weights the 10resp. 5largest positive and negative weights.
knowledge directly in the network structure. As far as we know, this is the first work that defines a 37
dynamic neural network layer in this generality. 38
Main Idea We simplify and unify the integration of expert knowledge and additional informa- 39
tion into neural networks by proposing a two-step approach and show how to encode given extra 40
information directly into the structure of a neural network in a dynamic way. In the first step the 41
extra information or expert knowledge is formalized using appropriate rules (e.g., certain pixels in 42
images are important ,only nodes in a graph of type A and B interact ,some patterns, e.g., cycles 43
or cliques, in a graph are important , etc.). In the second step the rules are used to manipulate the 44
structure of the neural network. More precisely, the rules determine the positions of the weights in 45
the weight matrix and the bias terms. We note that the focus of this work is on the second step as we 46
show how to use given rules to dynamically adapt the layers. In fact, we do not provide a general 47
instruction for deriving formal rules from given expert knowledge. In difference to ordinary network 48
layers we consider a set Wof learnable parameters instead of fixed weight matrices. The weight 49
matrices and bias terms are then constructed for each input sample independently using the learnable 50
parameters from W. Indeed, each learnable parameter in Wis associated with a specific relation 51
between an input and output feature of a layer. As an example consider Figure 1 where each input and 52
output feature corresponds to a specific node in the graph. The input samples are (a) molecule graphs 53
respectively (b) snippets of social networks and the task is to predict the graph class. Each colored 54
arrow in the figure corresponds to a learned parameter from W, i.e., a specific relation between two 55
atoms in the molecules or two nodes in the social network. Considering only the weights with the 56
largest absolute values, see the second image of (a) respectively (b), our approach has learned how to 57
propagate information from outer atoms to the rings respectively from the nodes to the “important” 58
nodes of the social network. This example shows several advantages of our approach: (1) rule based 59
layer type has a much more flexible structure than layers in classical architectures and allow to deal 60
with arbitrary input dimensions , (2) the layers are easily integrable into existing architectures, and 61
(3) the learned parameters, hence the model, is interpretable and can possibly be used to extract new 62
knowledge from the data or to improve the existing rules. 63
Main Contributions We define a new type of neural network layer called rule based layer. This 64
new layer can be integrated into arbitrary architectures making them dynamic, i.e., the structure 65
of the network changes based on the input data and predefined rules. We prove that rule based 66
layers generalize classical feed-forward layers such as fully connected and convolutional layers. 67
Additionally, we show that rule based layers can be applied to graph classification tasks, by introducing 68
RuleGNNs, a new type of graph neural networks. In this way we are able to extend the concept of 69
dynamic neural networks to graph neural networks together with all the advantages of dynamic neural 70
networks, e.g., that RuleGNNs are by definition permutation equivariant and able to handle graphs 71
2of arbitrary sizes. Considering various real-world graph datasets, we demonstrate that RuleGNNs 72
are competitive with state-of-the-art graph neural networks and other graph classification methods. 73
Using synthetic graph datasets we show that “expert knowledge” is easily integrable into our neural 74
network and also necessary for classification175
The rest of the paper is organized as follows: We introduce the concept of rule based layers in Section 2 76
and prove in Section 3 that rule based layers generalize fully connected and convolutional layers. 77
In Section 4 we present RuleGNNs and apply them in Section 5 to different benchmark datasets 78
and compare the results with state-of the art graph neural networks. Finally, we discuss limitations, 79
related work and conclude the paper in Section 6. 80
2 Rule Based Learning 81
Introducing the concept of rule based learning we first present some basic definitions followed by the 82
formal definition of rule based layers. 83
Preliminaries For some n∈Nwe denote by [n]the set {1, . . . , n }. A neural network is denoted 84
by a function f(−,Θ) :Rn−→Rmwith the learnable parameters Θ. We extend this notation 85
introducing an additional parameter R, that is the set of formal rules R={R1, . . . ,Rk}. The 86
exact definition of these rules is given in the next paragraph. Informally, a rule Ris a function 87
that determines the distribution of the weights in the weight matrix or the bias vector of a layer. A 88
ruleRis called dynamic if it is a function in the input samples x∈Rnotherwise it is called static . 89
An example of a static rule is the one used to define convolutional layers, see Proposition 2. An 90
example of a dynamic rule can be found in Section 4. In our setting, a neural network is a function 91
f(−,Θ,R) :R∗−→R∗that depends on a set of learnable parameters denoted by Θand some 92
rule set Rderived from expert knowledge or additional information. The notation ∗in the domain 93
and codomain of findicates that the input and output can be of arbitrary or variable dimension. As 94
usualfis a concatenation of sub-functions f1, . . . , flcalled the layers of the neural network. More 95
precisely, the i-th layer is a function fi(−,Θi,Ri) :R∗−→R∗where Θiis a subset of the learnable 96
parameters ΘandRiis an element of the ruleset R. We call a layer fistatic ifRiis a static rule and 97
dynamic ifRiis a dynamic rule. The input data is a triple (D,L,I), where D={x1. . . , x k}with 98
xi∈R∗is the set of examples drawn from some unknown distribution. The labels are denoted by 99
L= (y1. . . , y k)withyi∈R∗andIis some additional information known about the input data D. 100
This can be for example knowledge about the graph structure, node or edge labels, importance of 101
neighborhoods and many more. One main assumption of this paper is that Ican be used to derive a 102
set of static or dynamic rules R. Again we would like to mention that we concentrate on the analysis 103
of the effects of applying different rules Rand not on the very interesting but also wide field of 104
deriving the best rules RfromI, see some discussion in Section 6. Nonetheless, we always motivate 105
the choice of the rules derived by I. 106
Rule Based Layers We now give a formal definition of rule based layers. Given some dataset 107
(D,L,I)defined as before and the rule set Rderived from I, the task is to learn the weights Θof 108
the neural network fto predict the labels of unseen examples drawn from an unknown distribution. 109
Our contribution concentrates on single layers and is fully compatible with other layers such as 110
linear layers, convolutional layers Hence, in the following we restrict to the i-th layer fi(−,Θi,Ri) : 111
R∗−→R∗of a network f. For simplicity, we assume i= 1 and omit the indices, i.e., we write 112
f:=fi,Θ:= ΘiandR:=Ri. The forward propagation step of the rule based layer fwhich will be 113
a generalization of certain known layers as shown in Section 3 is as follows. Fix some input sample 114
x∈Dwithx∈Rn. Then f(−,Θ,R) :Rn−→Rmforn, m∈Nis given by 115
f(x,Θ,R) =σ(WRW(x)·x+bRb(x)). (1)
Here σdenotes an arbitrary activation function and WRW(x)∈Rm×nrsp.bRb(x)∈Rmis some 116
weight matrix rsp. weight vector depending on the input vector xand the rule R. The set Θ:= 117
{w1, . . . , w N, b1, . . . , b M}consists of all possible learnable parameters of the layer. The parameters 118
{w1, . . . , w N}are possible entries of the weight matrix while {b1, . . . , b M}are possible entries of 119
the bias vector. The key point here is that the rule Rdetermines the choices and the positions of 120
the weights from Θin the weight matrix WRW(x)and the bias vector bRb(x)depending on the input 121
1Our code, results and the datasplits used can be found here.
3sample x. More precisely, notall learnable parameters must be used in the weight matrix and the 122
bias vector for some input sample x. Note that for two samples x, y∈Dof different dimensionality, 123
e.g.,x∈Rnandy∈Rkwithn̸=kthe weight matrices WRW(x)andWRW(y)also have different 124
dimensions and the learnable parameters can be in totally different positions in the weight matrix. 125
This is where the rules Rand their associated rule functions, see (2) below, come into play. 126
Given the set of learnable parameters Θ:={w1, . . . , w N, b1, . . . , b M}, for each input x∈Rnthe 127
ruleRinduces the following two rule functions 128
RW(x) : [m]×[n]−→ { 0} ∪[N]and Rb(x) : [m]−→ { 0} ∪[M] (2)
where m∈Nis the output dimension of the layer that can also depend on x. In the following we 129
abbreviate RW(x)(i, j)byRW(x, i, j )andRb(x)(i)byRb(x, i). We note that for simplicity we 130
assume that the matrix and vector indices start at 1and not at 0. Using the associated rule functions (2) 131
we can construct the weight matrix resp. bias vector by defining the entry (i, j)∈Rm×nin the i-th 132
row and the j-th column of the weight matrix WR(x)∈Rm×nvia 133
WRW(x)(i, j) :=0 ifRW(x, i, j ) = 0
wRW(x,i,j)o.w.(3)
and the entry at position kin the bias vector bRb(x)∈Rmby 134
bRb(x)(k) :=0 ifRb(x, k) = 0
bRb(x,k)o.w.. (4)
Summarizing, the rule based layer defined in (1) is a standard feed-forward layer with the difference 135
that the weights in the weight matrix and the bias vector are determined by a predifined rule R. 136
In fact, weight matrix and bias vector depend on the input and can contain shared weights. More 137
precisely, the rule controls the connection between the i-th input and the j-th output feature in the 138
weight matrix. A rule Ris called static if it is independent of the input x∈D, i.e.,R(x)≡R(y) 139
for all inputs x, y∈R∈Dotherwise it is called dynamic . We call a rule based layer as defined in (1) 140
static if it is based on a static rule Randdynamic otherwise. We will show in Section 3 that rule 141
based layers generalize known concepts of neural network layers for specific rules R. In fact, we 142
show that fully connected layers and convolution layers are static rule based layers. Examples of 143
dynamic rule based layers are given later on in Section 4. The back-propagation of such a layer can 144
be done as usual enrolling the computation graph of the forward step and applying iteratively the 145
chain rule to all the computation steps. We will not go into the details of this computation as it is 146
similar to many other computations using backpropagation with shared weights. For the experiments 147
we use the automatic backpropagation tool of PyTorch [16] which fully meets our requirements. 148
Assumptions and Examples Rule based learning relies on the following two main assumptions: 149
A1)There is a connection between the additional information or expert knowledge Iand the used 150
ruleRandA2)The distribution of weights given by the rule Rin the weight matrix WR(x)improves 151
the predictive performance or increases the interpretability of the neural network. As stated before 152
we concentrate on the second assumption and consider different distribution of weights in the weight 153
matrix given by different rules. In fact, we assume without further consideration that it is possible to 154
derive a meaningful ruleset Rfrom the additional information or expert knowledge I. For example if 155
the dataset consists of images we can derive the “informal” rule that neighboured pixels are more 156
important than pixels far away and in case of chemical data there exists, e.g., the ortho-para rule for 157
benzene rings that makes assumptions about the influence of atoms for specific positions regarding 158
the ring. This rule was already learned by a neural network in [ 28]. It is another very interesting task 159
which is beyond the scope of this work how to formalize these “informal” rules or to learn the “best” 160
formal rules from the additional information I. 161
In the following sections we focus on the concept of rule based layers and therefore for simplicity 162
and space reasons only consider the rule function of weight matrices. The rule function associated 163
with the bias vector can be constructed similarly. For simplicity, we write Rinstead of RW. 164
43 Theoretical Aspects of Rule Based Layers 165
In this section we provide a theoretical analysis of rule based layers and show that they generalize fully 166
connected and convolutional layers. More precisely, we define two static rulesRFCandRCNN and 167
show that the rule based layer as defined in (1)based on RFCis a fully connected layer and the rule 168
based layer based on RCNN is a convolutional layer. All the proofs can be found in the Appendix A. 169
Proposition 1 Letf:Rn−→Rmwith 170
f(y,Θ,RFC) =σ(WRFC(x)·y)
be a rule based layer of a neural network as defined in (1)(without bias term) with learnable 171
parameters Θ ={w1, . . . , w n·m}andy=fi(x)is the result of the first i−1layers. Then for the 172
rule function RFC(x) : [m]×[n]→[m·n]defined for all inputs xas follows 173
RFC:=RFC(x)(i, j):= (i−1)·n+j,
the rule based layer fis equivalent to a fully connected layer with activation function σ. 174
Proposition 1 shows that rule based layers generalize fully connected layers of arbitrary size without 175
bias vector and can be easily adapted to include the bias vector. Hence, this shows that rule based 176
layers generalize arbitrary fully connected layers. Moreover, fully connected layers are static rule 177
based layers as the rule RFCis static because it does not depend on the particular input x. 178
Proposition 2 Letf:Rn·m−→R(n−N+1)·(m−N+1)with 179
f(y,Θ,RCNN) =σ(WRCNN(x)·y)
be a rule based layer of a neural network as defined in (1)(without bias term) and Wi= 180
{w1, . . . , w N2}be the set of learnable parameters. Then for the rule function RCNN : [(n− 181
N+ 1)·(m−N+ 1)]×[n·m]→[N2]defined by 182
RCNN:=RCNN(x)(i, j):=

τ(i, j)if0< γ(i, j)< N·nand
0< j(mod n)−j+γ(i, j)< N
0 o.w.
183
with τ(i, j) = γ(i, j)−((γ(i, j)−1)//n)·(n−N)
and γ(i, j) = j−((i−1)//(n−N+ 1))·n+ (i−1) (mod ( n−N+ 1))
the rule based layer fis equivalent to a convolution layer with quadratic kernel of size N(N < n , 184
N < m ) and a stride of one over a two-dimensional image of size n×m(without padding and bias 185
vector) with activation function σ. The notation a//b denotes the integer division of two integers a 186
andb. 187
Proposition 2 shows that rule based layers generalize 2D-image convolution without padding and 188
bias term. By adaption of the rule function it is possible to include the bias vector and padding. 189
Moreover, the result can be generalized to higher dimensions kernels, non-quadratic kernels and 190
arbitrary input and output channels. Hence, rule based layers also generalize arbitrary convolutional 191
layers. Convolutional layers are static rule based layers as the rule RCNN is static because it is 192
independent of the input. The following result is a direct implication from Propositions 1 and 2. 193
Theorem 1 Rule based layers generalize fully connected and convolutional feed-forward layers. 194
Moreover, both layers are static rule based layers. 195
We claim that also other types of feed-forward layers can be generalized by rule based layers using 196
appropriate rule functions. Because of space limitations we would rather present a specific application 197
of dynamic rule based layers on graphs. 198
4 Rule Based Learning on Graphs 199
One of the main advantages of rule based layers as introduced in this work is that they give rise to 200
a dynamic neural network architecture that is freely configurable using different rules. In fact, the 201
network is independent of the dimension and structure of the input samples. Hence, a natural applica- 202
tion of our approach is graph classification. We would like to emphasize that graph classification is 203
only one of many possible applications of rule based layers. Other possible applications are node 204
classification, regression tasks, graph embeddings or completely different data-structures. 205
5Graph Preliminaries By a graph we mean a pair G= (V, E)withVdenoting the set of nodes of 206
GandE⊆ {{i, j} |i, j∈V}the set of edges. We assume that the graph is undirected and does 207
not contain self-loops or parallel edges. In case that it is clear from the context we omit Gand only 208
useVandE. The distance between two nodes i, j∈Vin a graph, i.e., the length of the shortest 209
path between iandj, is denoted by d(i, j). A labeled graph is a graph G= (V, E)equipped with 210
a function l:V→ L that assigns to each node a label from the set L ⊆N. In this paper the input 211
samples corresponding to a graph (V, E)are always vectors of length equal to |V|. In particular, the 212
input vectors can be interpreted as signals over the graph and each dimension of the input vector 213
corresponds to the one-dimensional input signal of a graph node. 214
4.1 Graph Rules 215
The example on molecule graphs in Figure 2 and Appendix A.4 motivates the intuition behind 216
different graph specific rules that can be used to define a graph neural network based on rule layers. 217
The underlying general scheme to define a rule based layer on graphs is as follows: Let G= (V, E) 218
be a graph and l:V→ L a permutation equivariant labeling function of the nodes, i.e., for some 219
permutation πofVit holds l(π(V)) =π(l(V)). Assuming that input and output dimension of the 220
layer is equal to |V|the rule functions Ras defined in (2)map each pair of nodes (i, j)∈V×V 221
to an integer which is the index of the learnable parameter in the set of all learnable parameters. 222
The mapping is injective based on the labels l(i), l(j)and an additionally defined shared property 223
between the nodes iandj. Examples for such shared properties can be the distance between iand 224
j, the type of the edge connecting iandjor the information, that iandjare in one circle. As an 225
example RMolas defined in Appendix A.4 is induced by the permutation equivariant function lthat 226
maps each node to its atom label and the shared property between two nodes is the type of the edge 227
connecting the nodes or the absence of an edge. Besides RMolthe simple rule that is based on the 228
given node labels in this paper we focus on three different rule based layers for graphs. 229
Proposition 3 Letπbe some permutation of the nodes of G= (V, E)andxits corresponding input 230
vector. If Rpermutation equivariant, i.e., R(π(x))(i, j) =R(x)(π(i), π(j))then the rule based 231
layer is also equivariant under node permutations, i.e., f(π(x),Θ,RMol) =π(f(x,Θ,RMol)). 232
Weisfeiler-Leman Rule Recent research has shown that the Weisfeiler-Leman labeling is a powerful 233
tool for graph classification [ 18,14,2,22]. Thus, we propose to use Weisfeiler-Leman labels as 234
one option to define the rule based layer for graph classification. The Weisfeiler-Leman algorithm 235
assigns in the k-th iteration to each node of a graph a label based on the structure of its local k-hop 236
neighborhood, see [ 18]. Let l(v)be the result of the k-th iteration of the Weisfeiler-Leman algorithm 237
for some node v∈V. Then the Weisfeiler-Leman Rule RWLk,dassigns to each node pair (i, j)an 238
integer or zero based on the Weisfeiler-Leman labels l(i), l(j)and the distance between the nodes i 239
andj. The result is zero if the distance between iandjis not between 1andd. Note that we are not 240
restricted to look at consecutive distances from 1tod. It is also possible to look at certain distances 241
only if the expert knowledge suggests it. In fact, (i, j)and(k, l)are mapped to the same integer if 242
and only if l(i) =l(k), l(j) =l(l)and the distance between iandjis equal to the distance between 243
kandl. The layer defined by this rule is related to ordinary message passing but messages can pass 244
between nodes of arbitrary distances. For computational reasons in the experiments we restrict the 245
maximum number of different Weisfeiler-Leman labels considered by some bound L. We relabel 246
the most frequent l−1labels to 1,···, l−1and set all other labels to l. The corresponding layer is 247
denoted by fWLk,d,L. 248
Pattern Counting Rule Beyond labeling nodes via the Weisfeiler-Leman algorithm, it is a common 249
approach to use subgraph isomorphism counting to distinguish graphs [ 3]. This is in fact necessary 250
as the 1-Weisfeiler-Leman algorithm is not able to distinguish some types of graphs, for example 251
circular skip link graphs [ 4] and strongly regular graphs [ 2,3]. Thus, we propose the pattern counting 252
rule and show in Section 5 that RuleGNNs based on this rule are able to perform well on synthetic 253
benchmark datasets while message passing models based on the Weisfeiler-Leman algorithm fail. In 254
general, subgraph isomorphis counting is a hard problem [ 5], but for the real-world and synthetic 255
benchmark graph datasets that are usually considered, subgraphs of size k∈ {3,4,5,6}can be 256
enumerated in a preprocessing step in a reasonable time, see Table 5. Given a set of patterns, say 257
P, we compute all possible embeddings of these patterns in the graph dataset in a preprocessing 258
step. Then for each pattern P∈ P and each node i∈Vwe count how often the node iis part of an 259
6embedding of P. Using those counts we define a labeling function l:V→ L . Two nodes i, j∈V 260
are mapped to the same label if and only if their counts are equal for all patterns in P. Patterns 261
that are often used in practice are small cycles, cliques, stars, paths, etc. The Pattern Counting Rule 262
RPdassigns each node pair (i, j)an integer or zero based on the values of l(i), l(j)and the distance 263
between iandj. As for the Weisfeiler-Leman Rule we restrict the maximum number of different 264
labels to some number L. The corresponding layer is denoted by fPd,L. 265
Summary Rule The summary rule RN
Outcan be used as the output layer as its output is a fixed 266
dimensional vector of size N∈Nindependent of the size of the input data and the output is invariant 267
under node permutations. Again, let l:V→ L be a function that maps each node of a graph to some 268
integer. Then the summary rule RN
Outassigns each pair (n, i)withi∈Vandn∈[N]an integer 269
or zero based on nandl(i). In fact, for each element of Lthe rule defines ndifferent learnable 270
parameters. The corresponding layer is denoted by fRN
Out. 271
All the above rules define dynamic rule based neural network layers because the weight matrix and 272
bias terms defined by the rules depend on the input vectors xcorresponding to different graphs. Note 273
that the layers defined by the above rules are permutation equivariant as the node labeling function l 274
used to define the rule is equivariant under node permutations. Thus, using the layers corresponding 275
to the above defined rules we can build a graph classification architecture that by definition does not 276
depend on the order of the nodes in the input graphs. Moreover, a layer is able to pass information 277
between nodes of arbitrary distances in the graph. Thus, as shown in the experiments below, it is not 278
necessary to use deep networks to achieve good performance on the real-world benchmark datasets. 279
4.2 Rule Graph Neural Networks (RuleGNNs) 280
The layers derived from the above rules are the building blocks of the RuleGNNs. Each RuleGNN is 281
a concatenation of different rule based layers from Weisfeiler-Leman rules and pattern counting rules 282
followed by a summary rule using arbitrary activation functions. To define the learnable parameters 283
of the bias term we also use the summary rule. The input of the network is a signal x∈R|V|284
corresponding to a graph G= (V, E). We note that for simplicity we focus on one-dimensional 285
signals but also multidimensional signals, i.e., x∈R|V|×dare possible. The output of the network 286
is a vector of fixed size N∈Ndetermined by the summary rule where Nis usually the number 287
of classes of the graph classification task. The output can be also used as an intermediate vectorial 288
representation of the graph or for regression tasks. 289
5 Experiments 290
We evaluate the performance of RuleGNNs on different real-world and synthetic benchmark graph 291
dataset and compare the results to the state-of-the-art graph classification algorithms. For comparabil- 292
ity and reproducibility of the results, also with future algorithms, we make use of the experimental 293
setup from [ 7]. That means, for each graph dataset we perform a 10-fold cross validation, i.e., we use 294
fixed splits of the dataset into 10equally sized parts (the splits can be found in our repository), and 295
use9of them for training, parameter tuning and validation. We then use the model that performs 296
best on the validation set and report the performance on the previously unseen test set. We train the 297
best model 3times and average the results on each fold to decrease random effects. The standard 298
deviation reported in the tables is computed over the results on the 10folds. 299
Data and Algorithm Selection A problem of several heavily used graph benchmark datasets 300
like MUTAG or PTC [ 13] is that node and edge labels seems to be more important than the graph 301
structure itself, i.e., there is no significant improvement over simple baselines [ 17]. Moreover, in 302
case of MUTAG the performance of the model is highly dependent on the data split because of the 303
small number of samples. Thus, in this work for benchmarking we choose DHFR, Mutagenicity, 304
NCI1, NCI109, IMDB-BINARY and IMDB-MULTI from the TU Dortmund Benchmark Graphs 305
repository [ 13] because the structure of the graphs seems to play an important role, i.e., the simple 306
baselines presented in [ 17,7] are significantly worse than the state-of-the-art graph classification 307
algorithms. Additionally, we consider circular skip link graphs CSL [ 4] and constructed some new 308
synthetic benchmark graph datasets called LongRings, EvenOddRings and Snowflakes [ 15] to show 309
the advantages of RuleGNNs on more complex graph structures with given expert knowledge. For 310
7NCI1 NCI109 Mutagenicity DHFR IMDB-B IMDB-M
Baseline (NoG) [17] 69.2±1.9 68 .4±2.2 74 .8±1.8 71 .8±5.3 71.9±4.8 47 .7±4.0
WL-Kernel[18] 85.2±2.3 85 .0±1.7 83 .8±2.4 83.5±5.1 71.8±4.5 51 .9±5.6
DGCNN[27] 76.4±1.7 73 .0±2.4 77 .0±2.0 72 .6±3.1 69.2±3.0 45 .6±3.4
DGCNN (features) 73.6±1.0 72 .5±1.5 76 .3±1.2 76 .1±3.4 69.1±3.5 45 .8±2.9
GraphSage[8] 76.0±1.8 77 .1±1.8 79 .8±1.1 80 .7±4.5 68.8±4.5 47 .6±3.5
GraphSage (features) 79.4±2.2 78 .6±1.6 80 .1±1.3 82 .4±3.9 69.7±3.1 46 .6±4.8
GIN[26] 80.0±1.4 79 .7±2.0 81 .9±1.4 79 .1±4.4 71.2±3.9 48 .5±3.3
GIN (features) 77.3±1.8 77 .7±2.0 80 .6±1.3 81 .8±5.1 70.9±3.8 48 .3±2.7
GSN (paper) [3] 83.5±2.3 - - - 77.8±3.3 54 .3±3.3
CIN (paper) [1] 83.6±1.4 84 .0±1.6 - - 75.6±3.7 52 .7±3.1
SIN (paper)[2] 82.7±2.1 - - - 75.6±3.2 52 .4±2.9
PIN (paper) [22] 85.1±1.5 84 .0±1.5 - - 76.6±2.9 -
RuleGNN 82.8±2.0 83 .2±2.1 81 .5±1.3 84.3±3.2 75.4±3.3 52 .0±4.3
Table 1: Test set performance of several state-of-the-art graph classification algorithms averaged
over three different runs and 10folds. The ±values report the standard deviation over the 10folds.
The overall best results are colored red and the best ones obtained for the fair comparison from [ 7]
are in bold. The (features) variant of the algorithms uses the same information as the RuleGNN as
input features additionally to node labels. The (paper) results are taken from the respective papers
and might be obtained with different splits of the datasets.
more details on the datasets see Appendix A.5. For NCI1, IMDB-BINARY and IMDB-MULTI we 311
use the same splits as in [ 7] and for CSL we use the splits as in [ 6] and a 5-fold cross validation. We 312
evaluate the performance of the RuleGNNs on these datasets and compare the results to the baselines 313
from [ 7] and [ 17] and the Weisfeiler-Leman subtree kernel (WL-Kernel) [ 18] which is one of the best 314
performing graph classification algorithm besides the graph neural networks. For comparison with 315
state-of-the-art graph classification algorithms we follow [ 7] and compare to DGCNN [ 27], GIN [ 26] 316
and GraphSAGE [ 8]. Additionally, we compare to the results of some newer state-of-the-art graph 317
classification algorithms [ 3,1,2,22]. For the latter we use the results from the respective papers that 318
might be obtained with different splits of the datasets. 319
Experimental Settings and Resources All experiments were conducted on a AMD Ryzen 9 7950X 320
16-Core Processor with 128GB of RAM. For the competitors we use the implementations from [7]. 321
For the real-world datasets we were not aware of expert-knowledge, hence we tested different rules 322
and combinations of the layers defined in Section 4.1. More details on the tested hyperparameters 323
can be found in Appendix A.7. We always use tanh for activation and the Adam optimizer [ 11] with 324
a learning rate of 0.05(real-world datasets) resp. 0.1(synthetic datasets). For the real-world datasets 325
the learning rate was decreased by a factor of 0.5after each 10epochs. For the loss function we use 326
the cross entropy loss. All models are trained for 50(real-world) resp. 200(synthetic) epochs and the 327
batch size was set to 128. We stopped if the validation accuracy did not improve for 25epochs. 328
Real-World Datasets The results on the real-world datasets (Table 1) show that RuleGNNs are 329
able to outperform the state-of-the-art graph classification algorithms in the setting of [ 7] even if 330
we add all the additional label information that RuleGNNs use to the input features of the graph 331
neural networks (see the (features) results in Table 1). This shows that the structural encoding of 332
the additional label information is crucial for the performance of the graph neural networks and not 333
replacable by using more input features. Moreover, the results show that the Weisfeiler-Leman subtree 334
kernel is the best performing graph classification algorithm on NC1, NCI109 and Mutagenicity. For 335
IMDB-BINARY and IMDB-MULTI our approach performs worse than the state-of-the-art graph 336
classification algorithms that are not evaluated within the same experimental setup. 337
Synthetic Datasets The results on the synthetic benchmark graph dataset show that RuleGNNs 338
outperform the state-of-the-art graph classification algorithms if expert knowledge is available even 339
in the case that mesage passing is enough to solve the task. In fact, CLS and Snowflakes are not 340
solvable by the message passing model because they are not distinguishable by the 1-WL test. The 341
results on LongRings show that long range dependencies can be easily captured by RuleGNNs and 342
also dependencies between nodes of different distances as in case of the EvenOddRings dataset can 343
be encoded by appropriate rules. 344
8LongRings EvenOddRings EvenOddRingsCount CSL Snowflakes
Baseline (NoG) [17] 30.17±3.2 22 .25±3.0 47 .9±3.9 10.0±0.0 27 .3±5.3
WL-Kernel [18] 100.0±0.0 26 .83±4.2 47 .8±4.3 10.0±0.0 27 .9±4.1
DGCNN [27] 29.9±2.6 28 .4±2.5 59 .1±5.2 10.0±0.0 26 .0±3.3
GraphSAGE [8] 29.8±2.8 24 .9±2.7 51 .3±1.9 10.0±0.0 25 .0±1.8
GIN [26] 32.0±3.1 26 .8±2.5 51 .0±3.7 10.0±0.0 24 .5±2.2
RuleGNN 99.0±3.3 90 .2±7.2 100 .0±0.0 100.0±0.0 97 .9±3.2
Table 2: Test set performance of several state-of-the-art graph classification algorithms averaged
over three different runs and 10folds. The ±values report the standard deviation over the 10folds.
The best results and our algorithm are highlighted in bold.
Interpretability of the Rule Based Layers Each learnable parameter of RuleGNNs can be inter- 345
preted in terms of the importance of a connection between two nodes in a graph with respect to their 346
labels and their shared property (in our case the distance). In Figures 1 and6 we see how the network 347
has learned the importance of different connections between nodes for different distances and labels. 348
6 Related Work, Limitations and Concluding Remarks 349
Dynamic neural networks have been proven to be more efficient, have more representation power 350
and better interpretability than static neural networks [ 9]. Our approach can be seen as a sample 351
dependent dynamic neural network as for each input sample the network structure is adapted. In 352
contrast to other sample dependent dynamic neural networks [ 20,24], our approach changes the 353
layer structure based on a predefined rule instead of the whole architecture. The rule based layers 354
of RuleGNNs use the Weissfeiler-Leman labeling algorithm and subgraph isomorphism counting 355
which are both recently used concepts in graph classification algorithms [ 18,3,2,1]. The challenge 356
for graph neural networks is the heterogenicity of the input data and the lack of a fixed order of the 357
input data. [ 19] proposes a dynamic neural network for graph classification that uses node and edge 358
labels and is similar to our approach. In fact, they also show that their approach generalizes CNNs. 359
In contrast, they do not provide a general scheme to encode expert knowledge into the network. 360
Moreover, their approach is not able to encode long range dependencies in the graph using only 361
one layer. There exist graph neural networks that have learned the ortho-para rule for molecules 362
[28]. While the additional information used in these algorithms is mostly hard-coded, we are able to 363
integrate arbitrary rules. 364
Limitations Input Features: So far we have only considered 1-dimensional input signals and 365
node labels, i.e., our experimental results are restricted to graphs that have no multi-dimensional 366
node features. Additionally, we have not considered edge features in our rules. In principle, multi- 367
dimensional node features and edge labels can be handled by our approach with the cost of increased 368
complexity. Space: For each graph we need to precompute the pairwise distances and store the 369
positions of the weights in the weight-matrix. This is a disadvantage for large and dense graphs 370
as we need to store a large number of positions. For dense graphs the number of positions can be 371
quadratic in the number of nodes. Structure: To define a meaningful rule for a layer the input and 372
output features need to be logically connected. Fortunately, this is the case for graphs but this fact can 373
be a limitation for other structures. Combinatorics: If it is not possible to define a formal rule given 374
some informal expert knowledge the number of possible rules that have to be tested can be very large. 375
Thus, it is an interesting question if it is possible to automatically learn a rule that captures the expert 376
knowledge in the best way. Implementation: As stated in [ 9] there is a “gap between theoretical & 377
practical efficiency” regarding dynamic neural networks, i.e., common libraries such as PyTorch or 378
TensorFlow are not optimized for dynamic neural networks. 379
Concluding Remarks We have introduced a new type of neural network layer that dynamically 380
arranges the learnable parameters in the weight matrices and bias vectors according to a formal 381
rule. On the one hand our approach generalizes classical neural network components such as fully 382
connected layers and convolutional layers. On the other hand we are able to apply rule based layers 383
to the task of graph classification showing that expert knowledge can be integrated into the learning 384
process. Moreover, our approach gives rise to a more interpretable neural network architecture as 385
every learnable parameter is related to a specific connection between input and output features. 386
9References 387
[1]Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Li `o, Guido F. Mont ´ufar, 388
and Michael M. Bronstein. Weisfeiler and lehman go cellular: CW networks. In Marc’Aurelio 389
Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, 390
editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural 391
Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , pages 392
2625–2640, 2021. 393
[2]Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F. Mont ´ufar, Pietro Li ´o, 394
and Michael M. Bronstein. Weisfeiler and lehman go topological: Message passing simplicial 395
networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International 396
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of 397
Proceedings of Machine Learning Research , pages 1026–1037. PMLR, 2021. 398
[3]Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving 399
graph neural network expressivity via subgraph isomorphism counting. IEEE Trans. Pattern 400
Anal. Mach. Intell. , 45(1):657–668, 2023. 401
[4]Jin-yi Cai, Martin F ¨urer, and Neil Immerman. An optimal lower bound on the number of 402
variables for graph identification. Comb. , 12(4):389–410, 1992. 403
[5]Stephen A. Cook. The complexity of theorem-proving procedures. In Michael A. Harrison, 404
Ranan B. Banerji, and Jeffrey D. Ullman, editors, Proceedings of the 3rd Annual ACM Sym- 405
posium on Theory of Computing, May 3-5, 1971, Shaker Heights, Ohio, USA , pages 151–158. 406
ACM, 1971. 407
[6]Vijay Prakash Dwivedi, Chaitanya K. Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, 408
and Xavier Bresson. Benchmarking graph neural networks. J. Mach. Learn. Res. , 24:43:1–43:48, 409
2023. 410
[7]Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph 411
neural networks for graph classification. ArXiv , abs/1912.09893, 2019. 412
[8]William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on 413
large graphs. In Neural Information Processing Systems , 2017. 414
[9]Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic 415
neural networks: A survey. IEEE Trans. Pattern Anal. Mach. Intell. , 44(11):7436–7456, 2022. 416
[10] Yacine Jernite, Edouard Grave, Armand Joulin, and Tom ´as Mikolov. Variable computation in 417
recurrent neural networks. In 5th International Conference on Learning Representations, ICLR 418
2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 419
2017. 420
[11] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua 421
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, 422
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. 423
[12] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, 424
Wayne E. Hubbard, and Lawrence D. Jackel. Handwritten digit recognition with a back- 425
propagation network. In David S. Touretzky, editor, Advances in Neural Information Processing 426
Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989] , pages 396–404. 427
Morgan Kaufmann, 1989. 428
[13] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion 429
Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML 430
2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020) , 2020. 431
[14] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, 432
Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural 433
networks. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The 434
Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth 435
AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, 436
Hawaii, USA, January 27 - February 1, 2019 , pages 4602–4609. AAAI Press, 2019. 437
10[15] Harish G. Naik, Jan Polster, Raj Shekhar, Tam ´as Horv ´ath, and Gy ¨orgy Tur ´an. Iterative graph 438
neural network enhancement via frequent subgraph mining of explanations, 2024. 439
[16] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, 440
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas 441
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, 442
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, 443
High-Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, 444
F. d’Alch ´e Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing 445
Systems 32 , pages 8024–8035. Curran Associates, Inc., 2019. 446
[17] Till Hendrik Schulz and Pascal Welke. On the necessity of graph kernel baselines. 2019. 447
[18] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. 448
Borgwardt. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res. , 12:2539–2561, 2011. 449
[19] Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional 450
neural networks on graphs. In 2017 IEEE Conference on Computer Vision and Pattern Recogni- 451
tion, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 , pages 29–38. IEEE Computer Society, 452
2017. 453
[20] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. Branchynet: Fast inference via early 454
exiting from deep neural networks. In 23rd International Conference on Pattern Recognition, 455
ICPR 2016, Canc ´un, Mexico, December 4-8, 2016 , pages 2464–2469. IEEE, 2016. 456
[21] Geoffrey G. Towell and Jude W. Shavlik. Knowledge-based artificial neural networks. Artif. 457
Intell. , 70(1-2):119–165, 1994. 458
[22] Quang Truong and Peter Chin. Weisfeiler and lehman go paths: Learning topological features 459
via path complexes. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, 460
Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference 461
on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on 462
Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, 463
Canada , pages 15382–15391. AAAI Press, 2024. 464
[23] Laura von R ¨uden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, 465
Raoul Heese, Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, Michal 466
Walczak, Jochen Garcke, Christian Bauckhage, and Jannis Schuecker. Informed machine 467
learning - A taxonomy and survey of integrating prior knowledge into learning systems. IEEE 468
Trans. Knowl. Data Eng. , 35(1):614–633, 2023. 469
[24] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez. Skipnet: Learning 470
dynamic routing in convolutional networks. In Vittorio Ferrari, Martial Hebert, Cristian Smin- 471
chisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, 472
Munich, Germany, September 8-14, 2018, Proceedings, Part XIII , volume 11217 of Lecture 473
Notes in Computer Science , pages 420–436. Springer, 2018. 474
[25] Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, and Gao Huang. Adaptive 475
focus for efficient video recognition. In 2021 IEEE/CVF International Conference on Computer 476
Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 , pages 16229–16238. IEEE, 477
2021. 478
[26] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural 479
networks? In 7th International Conference on Learning Representations, ICLR 2019, New 480
Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. 481
[27] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning 482
architecture for graph classification. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, 483
Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 484
30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium 485
on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, 486
February 2-7, 2018 , pages 4438–4445. AAAI Press, 2018. 487
[28] Zhenpeng Zhou and Xiaocheng Li. Graph convolution: A high-order and adaptive approach. 488
arXiv: Learning , 2017. 489
11H
H
H
H
C
CH
H
H
H
C
CH
H
C
C
CH
H
C
C
C
Figure 2: Information propagation in a simple two layer RuleGNN based on the molecule graphs of
ethylene (left) and cyclopropenylidene (right) and the rules RMol(5)andROut(6). The input signal
is propagated from left to right. The graph nodes represent the neurons of the neural network. Edges
of the same color denote shared weights in a layer. For more details see Appendix A.4.
A Appendix / supplemental material 490
A.1 Proof of Proposition 1 491
To show the equivalence between the two layers it suffices to show that their weight matrices coincide. 492
In case of fully connected layers we have to show that the weight matrix WRFC(x)∈Rm×nis filled 493
withn·mdistinct weights. This can be easily checked by computing WRFC(x)using the definition 494
of the weight distribution based on the rule function in (3). 495
A.2 Proof of Proposition 2 496
Instead of the original two-dimensional image of size n×mwe consider a reshaped vector x∈Rn·m497
as our definition of rule based layers is restricted to simple vector matrix multiplication. The output 498
vector of dimension (n−N+1)·(m−N+1) can then again be reshaped into a two-dimensional image 499
of size (n−N+1)×(m−N+1). Unfortunately, the reshaping makes the rule function complicated 500
as the indices of the reshaped vector have to be mapped to the indices of the two-dimensional image. 501
First note that convolution with a N×Nkernel corresponds to matrix-vector multiplication of a 502
doubly block circulant matrix that is a special case of a block Toeplitz matrix. Hence, to show the 503
equivalence between the layers we have to compare the weight matrices and show that the entries in 504
WRCNN(x)∈R(n−N+1)·(m−N+1)×n·mexactly matches the entries in the block Toeplitz matrix of 505
the same dimension that corresponds to the convolution kernel. Comparing the definition of block 506
Toeplitz matrices with the above given rule shows that the rule exactly returns the entries of the block 507
Toeplitz matrix. Hence, the multiplication of xwithWRCNN(x)is equivalent to multiplication of x 508
with the block Toeplitz matrix that is equivalent to the convolution of xwith a kernel of size N×N. 509
A.3 Proof of Proposition 3 510
The proof of Proposition 3 follows directly from the definitions of the rule based layers, see (1), and 511
the rule functions, see (2). If the order of the nodes in the graph is permuted and rule function is 512
permutation equivariant, then the node labels are permuted accordingly. Hence, the positions of the 513
weights in the weight matrix and the bias term are permuted in the same way as the node labels. Thus, 514
the result of f, i.e., the multiplication of permuted weight matrix with the permuted input signal, is 515
the same as the permutation of the result of the multiplication of the original weight matrix with the 516
original input signal. 517
A.4 Example: RuleGNN for Molecule Graphs 518
Assume the task is to learn a property of a molecule based on its graph structure. In this example we 519
present a RuleGNN that is a concatenation of two very simple rule based layers. The advantage of 520
rule based layers and hence also RuleGNNs is that they encode the graph structure (in this example 521
the structure of two molecules) directly into the neural network. Moreover, the input samples can be 522
arbitrary molecule graphs and the output is a vector of fixed size kthat encodes the property of the 523
molecule or some intermediate vectorial representation. In this example we consider the molecule 524
graphs of ethylene and cyclopropenylidene given in Figure 3 together with their corresponding input 525
12C6 C5H1
H2 H3H4
C3H1
C5C4H2
Figure 3: Molecule graphs of ethylene (left) and cyclopropenylidene (right). The indices denote the
order of the nodes.
signals x∈R6andy∈R5. The atoms of the molecules (hydrogen Hand carbon C) correspond 526
to the nodes of a graph and the bond types ( single anddouble ) correspond to the edges. The atom 527
labels and the atom bond types can be seen as additional information Ithat is known about the input 528
samples. The graph nodes are indexed via integers in some arbitrary but fixed order and the atom 529
corresponding to a graph node are given by the labeling function l:V→ {H, C}. 530
The RuleGNN consists of two rule based layers f1(,Θ1,RMol)andf(,Θ2,ROut)with learnable 531
parameters Θ1={w1, . . . , w 6}andΘ2={w′
1, . . . , w′
2·k}and the following rule functions RMol 532
andROut. For some graph G= (V, E)and its corresponding input signal zwe define RMolas 533
follows: 534
RMol(z) : [|V|]×[|V|]−→ { 0} ∪[6]
(i, j) 7→

1ifi=jandl(i) =H
2ifi=jandl(i) =C
3if(i, j)is an edge (-), l(i) =H, l(j) =C
4if(i, j)is an edge (-), l(i) =C, l(j) =H
5if(i, j)is an edge (-), l(i) =l(j) =C
6if(i, j)is an edge (=), l(i) =l(j) =C
0o.w.(5)
For some graph G= (V, E)and its corresponding input signal zwe define ROutas follows: 535
ROut(z) : [|V|]×[k]−→ { 0} ∪[2·k]
(i, j) 7→

1·j l(i) =H
2·j l(i) =C
0 o.w.(6)
Note that RMolandROutare not restricted to the two molecules from above but can be applied 536
to arbitrary molecule graphs. Indeed, applying it to molecules with atom labels different from H 537
orCmakes the rules less powerful, i.e., it should be adapted to the type of molecules. Using the 538
definition (3)of weight distribution defined by the rule function we can construct the weight matrices 539
WRMol(x), WROut(x)for the ethylene graph and WRMol(y), WROut(y)for the cyclopropenylidene 540
graph as follows: 541
WRMol(x)=
w10 0 0 w30
0w10 0 w30
0 0 w10 0 w3
0 0 0 w10w3
w4w40 0 w2w5
0 0 w4w4w5w2
WROut(x)= w′
1 w′
1 w′
1 w′
1w′
2w′
2
..................
w′
2k−1w′
2k−1w′
2k−1w′
2k−1w′
2kw′
2k!
WRMol(y)=
w10w30 0
0w10w30
w40w2w6w5
0w3w6w2w5
0 0 w5w5w2
 WROut(y)= w′
1 w′
1w′
2w′
2w′
2
...............
w′
2k−1w′
2k−1w′
2kw′
2kw′
2k!
Combining the two rule based layers we obtain the RuleGNN and the forward propagation is given 542
byσ(WROut(x)·σ(WRMol(x)·x))for the ethylene graph and σ(WROut(y)·σ(WRMol(y)·y))for the 543
cyclopropenylidene graph. 544
13Dataset #Graphs #Nodes #Edges Diameter #Node Labels #Classes
max avg min max avg min max avg min
NCI1 4 110 111 29.9 3 119 32.3 2 45 11.5 0 37 2
NCI109 4 127 111 29.7 4 119 32.1 3 61 11.3 0 38 2
Mutagenicity 4 337 417 30.3 4 112 30.8 3 41 6.3 0 14 2
DHFR 756 71 42.4 20 73 44.5 21 22 14.6 8 9 2
IMDB-BINARY 1 000 136 19.8 12 1249 96.5 26 2 1.9 1 1 2
IMDB-MULTI 1 500 89 13.0 7 1467 65.9 12 2 1.5 1 1 3
Table 3: Details on the real-world datasets used in the experiments. The datasets are from the TU
Dortmund Graph Database [13].
Note that the forward propagation of the layer corresponding to the rule RMolis kind of a multiplica- 545
tion with a weighted adjacency matrix of the graph where the weights of the adjacency matrix are 546
given by the learnable parameters, see also Figure 2. In contrast to adjacency matrices the weight 547
matrix is not necessary symmetric. The computation graph induced by the weight matrix exactly 548
represent the graph structure while the edge weights are shared across the network using the rule, see 549
Figure 2. Note that the above defined rule is very flexible as also edge labels (e.g., atomic bonds) 550
can be taken into account by increasing the size of the weight set. Moreover, it is possible to include 551
bigger neighbourhoods, i.e., all nodes reachable by k-hops. Of course using other information of 552
the graph (e.g., substructures (such as circles or cliques), node degrees, connections not depicted by 553
edges) more complicated rules can be defined. 554
A.5 Dataset Details 555
In this section we provide additional details on the datasets used in the experiments. Table 3 shows 556
an overview of the real-world datasets and Table 4 provides an overview of the synthetic datasets. 557
We consider the following synthetic datasets. The CSL dataset is from []. We constructed the other 558
datasets to demonstrate the strength of our approach to encode expert knowledge into the neural 559
network. 560
LongRings LongRings consists of 1200 cycles of 100nodes each. Four nodes are labeled by 561
1,2,3,4and all other nodes are labeled by 0. The distance between each pair of the four nodes is 562
exactly 25or50. The label of the graph is 0if1and2have distance 50,1if1and3have distance 563
50and2if1and4have distance 50. There are 400graphs for each class. The difficulty of the 564
classification task is that information has to be propagated over a long distance. Regarding RuleGNNs 565
this is very easy because if the expert knows that distance 50is relevant we can define an appropriate 566
rule. 567
EvenOddRings EvenOddRings consists of 1200 cycles of 16nodes each. The nodes in each graph 568
are labeled from 0to15. The graph label is based on the labels of the nodes that have distance 8 569
respectively 4to the node with label 0. We denote them by xresp. y, z. We distinct four cases: xis 570
even and y+zis even, xis even and y+zis odd, xis odd and y+zis even, xis odd and y+zis 571
odd. There are 300graphs for each class, i.e., each of the four cases. The expert knowledge we use is 572
that the information has to be collected from nodes of distance 8and4. 573
EvenOddRingsCount EvenOddRingsCount consists of the same graphs as EvenOddRings but 574
the graph labels are different. For all nodes and their opposite node in the circle the sum of the 575
labels is computed. If there are more even sums than odd sums the graph is labeled by 0and by 1 576
otherwise. There are 600graphs for each class. The expert knowledge we use is the information that 577
only distance 8is relevant. 578
Snowflakes Snowflakes is a dataset consisting of graphs proposed by [ 15] that are not distinguish- 579
able by the 1-WL test, see Figure 4 for an example. The dataset consists of circles of length 3to12 580
and at each circle node a graph from M0, M1, M2orM3is attached, see Figure 5 and [ 15] for the 581
details. M0, M1, M2andM3are non-isomorphic graphs that are not distinguishable by the 1-WL 582
test. One label in the circle is labeled by 1and all other nodes are labeled by 0. The label of the graph 583
is determined by the graph M0, M1, M2orM3that is attached to the circle node with label 1. 584
14Figure 4: Example graphs from the Snowflakes dataset.
Figure 5: The graphs M0, M1, M2, M3from [15] that are not distinguishable by the 1-WL test.
Dataset #Graphs #Nodes #Edges Diameter #Node Labels #Classes
max avg min max avg min max avg min
LongRings 1 200 100 100.0 100 100 100.0 100 50 50.0 50 5 3
EvenOddRings 1 200 16 16.0 16 16 16.0 16 8 8.0 8 16 4
EvenOddRingsCount 1 200 16 16.0 16 16 16.0 16 8 8.0 8 16 2
CSL 150 41 41.0 41 82 82.0 82 10 6.0 4 1 10
Snowflakes 1 000 180 112.5 45 300 187.5 75 18 15.5 13 2 4
Table 4: Details of the synthetic datasets used in the experiments. The CSL dataset is from [4].
A.6 RuleGNNs: Runtimes 585
Table 5 shows more details of the RuleGNN model. In particular, we see that except for the DHFR 586
dataset we need less than 12epochs on average to reach the best result. This shows that our approach 587
is very efficient and converges quickly. At the first glance the average time per epoch seems to be very 588
high. This has two reasons. One is also mentioned in [ 9] that there is a gap between the theoretical 589
and practical runtime of dynamic neural networks because the implementation in PyTorch is not 590
optimized for dynamic neural networks. The other reason is that we parallelized the computation, i.e., 591
we are able to run all the three runs and 10folds in parallel on the same machine. Of course, this 592
produces some overhead. As stated above the preprocessing times are not relevant for the experiments 593
as they are only needed once. The third column shows the time needed to compute all the pairwise 594
distances between the nodes of the graph. The fourth column shows the time needed to compute the 595
node labels used for the best model. The most preprocessing time is needed for IMDB-BINARY and 596
IMDB-MULTI because the graphs are much denser than the other datasets. For the synthetic datasets 597
except for CSL we do not need any label preprocessing time as the original node labels are used. 598
A.7 RuleGNNs: Architectures and Hyperparameters 599
Table 6 provides an overview of the different architectures used in the experiments that achieved 600
the best results. One advantage of our approach is that messages can be passed over long distances. 601
Hence, except for the EvenOddRings dataset we used only one layer and the output layer. In case 602
of NCI1, NCI109, Mutagenicity it turns out that the best model uses the Weisfeiler-Leman rule 603
withk= 2 iterations. We restricted the number of maximum labels considered to 500which 604
results in 250000 learnable parameters for the weight matrix and 500for the bias vector. For the 605
output layer we used the bound of 50000 learnable parameters which was larger than the number of 606
different Weisfeiler-Leman labels in the second iteration. Interestingly, for NCI1 and NCI109 the 607
best validation accuracy was achieved if considering node pairs with maximum distance 10. In case 608
15Dataset Best Epoch Avg. Epoch (s) Preproc. Distances (s) Preproc. Labels (s) Num. Graphs
NCI1 7.3±5.3 377 .1±20.7 2.0 11.9 4 110
NCI109 5.4±2.9 386 .7±1.9 2.4 13.2 4 127
Mutagenicity 9.1±4.1 575 .8±66.4 2.2 15.2 4 337
DHFR 23.1±14.6 44 .4±9.0 0.7 3.1 756
IMDB-BINARY 11.3±4.6 24 .3±0.9 0.2 206.5 1 000
IMDB-MULTI 6.7±3.5 19 .6±1.3 0.2 195.0 1 500
LongRings 194.2±15.1 0 .7±0.2 6.6 - 1 200
EvenOddRings 176.1±15.2 1 .2±0.3 0.2 - 1 200
EvenOddRingsCount 199.0±0.0 0 .5±0.1 0.1 - 1 200
CSL 49.0±0.0 1 .6±0.0 0.1 11.8 150
Snowflakes 191.7±18.9 0 .5±0.1 7.1 - 1 000
Table 5: Runtimes and preprocessing times of the different datasets used in the experiments. All
values are averaged over the best runs. The first column shows the best epoch (highest validation
accuracy), the second column shows the average time per epoch, the third column shows the time
needed to compute all the pairwise distances between the nodes of the graph, the fourth column
shows the time needed to compute the node labels used for the best model and the last column shows
the number of graphs in the dataset.
of Mutagenicity the best model uses only node pairs with distance 3although we also considered 609
the hyperparameter d= 10 . We also tested different small patterns, e.g., simple cycles, but they 610
did not improve the results. For DHFR this was different as the best model uses the pattern (simple 611
cycles with length at most 10) for the output layer. We also tested the Weisfeiler-Leman rule in this 612
case but the validation accuracy was lower. For IMDB-BINARY and IMDB-MULTI the best model 613
uses the pattern (simple cycles with length at most 10, triangle, edge). Note that the embedding of 614
one edge as a pattern is equivalent to the degree of the node. We also tested the Weisfeiler-Leman 615
rule but the validation accuracy was lower. All in all we considered many different rules from type 616
Weisfeiler-Leman and patterns but of course we did not test all possible rules. A full list of tested 617
hyperparameters can be found here. As a next step it would be interesting to consider more rules, 618
rules that come from expert knowledge or also deeper architectures with more rule based layers 619
concatenated. Regarding the number of learnable parameters we would like to mention that the 620
number is relatively high but lots of parameters are not used in the weight matrix. Hence, it might be 621
possible to prune the set of learnable parameters by removing those that are not used or those that 622
have a small absolute value. 623
For the synthetic datasets we use “expert knowledge” to define the rules. Hence we did not tested 624
other rules than those in Table 6. For LongRings, EvenOddRings and EvenOddRingsCount we used 625
the original node labels for the rule based layers. Moreover, instead considering learnable parameters 626
for all node pairs of certain labels with distance smaller or equal to dwe considered only the node 627
pairs with distance d(denoted by “only: d”). In case of EvenOddRings we used two layers. The first 628
layer that considers only node pairs with distance 8collects all the necessary information of opposite 629
nodes. The second layer that considers only node pairs with distance 4collects the information of 630
the nodes that are 4hops away from the nodes with label 0, see also Figure 6. For CSL we used as 631
patterns all simple cycles with length at most 10. For the Snowflakes dataset we used the patterns 632
cycle of length 4and5and collect the information of the nodes that have pairwise distance 3. In this 633
way the RuleGNN is able to distinguish the graphs M0, M1, M2andM3that are not distinguishable 634
by the 1-WL test. In the output layer we used the Weisfeiler-Leman rule with k= 2iterations to 635
collect the relevant information from nodes with different Weisfeiler-Leman labels. 636
A.8 RuleGNNs: Interpretability 637
One advantage of our approach is that each weight can be interpreted, i.e., we can see the relevance 638
of two nodes i, jin a graph with labels l(i), l(j)and distance d(i, j). Figure 6 shows an example of 639
the learned parameters for some synthetic dataset. Figure 1 shows an example of the relevance of the 640
weights for a graph from the DHFR dataset using the weights of the best model. Considering Figure 6b 641
we can see that in the first layer the RuleGNN passes the messages between opposite nodes as given 642
by the rule. In the second layer it has learned to collect the information from the nodes that have 643
distance 4to the node with label 0(dark blue node) all other connections of distance 4have a smaller 644
weight. 645
16Dataset Rules Hyperparameter #Learnable Parameters per Layer
k d L
NCI1 wl 2 10 500 2 500 500
wl 2 - 50000 4 220
NCI109 wl 2 10 500 2 500 500
wl 2 - 50000 4 336
Mutagenicity wl 2 3 500 750 500
wl 2 - 50000 4 972
DHFR wl 2 6 500 1 382 880
pattern: (simple cycles≤10) - - - 112
IMDB-BINARY pattern: (triangle, edge) - 2 - 963 966
pattern: (induced cycles≤5) - - - 990
IMDB-MULTI pattern: (triangle, edge) - 2 - 551 775
pattern: (triangle, edge) 10 - - 1 578
LongRings labels - only: 25 - 30
labels - - - 18
EvenOddRings labels - only: 8 - 272
labels - only: 4 - 272
labels - - - 68
EvenOddRingsCount labels - only: 8 - 272
labels - - - 34
CSL pattern: (simple cycles≤10) - - - 8930
pattern: (simple cycles≤10) - - - 950
Snowflakes pattern: (cycle 4, cycle 5) - only: 3 - 90
wl 2 - - 20
Table 6: Overview over the hyperparameters of the best models.
(a) EvenOddCount
 (b) EvenOddRings
 (c) Snowflakes
Figure 6: Visualization of the learned weights and biases for the RuleGNN on the EvenOd-
dRingsCount (a), EvenOddRings (b) and Snowflakes (c) dataset. The first column shows the graphs
and the colors of the nodes represent the different node labels. The other columns show the learned
weights and biases of the RuleGNN for the respective rule based layer. The message passing weights
are visualized by arrows (thicker for higher absolute values) and the biases are visualized by the size
of the node (red for positive and blue for negative weights).
17NeurIPS Paper Checklist 646
1.Claims 647
Question: Do the main claims made in the abstract and introduction accurately reflect the 648
paper’s contributions and scope? 649
Answer: [Yes] 650
Justification: The theoretical and experimentally claims made in the abstract are consistent 651
with the results presented in the paper and reflect the contributions made. 652
Guidelines: 653
•The answer NA means that the abstract and introduction do not include the claims 654
made in the paper. 655
•The abstract and/or introduction should clearly state the claims made, including the 656
contributions made in the paper and important assumptions and limitations. A No or 657
NA answer to this question will not be perceived well by the reviewers. 658
•The claims made should match theoretical and experimental results, and reflect how 659
much the results can be expected to generalize to other settings. 660
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 661
are not attained by the paper. 662
2.Limitations 663
Question: Does the paper discuss the limitations of the work performed by the authors? 664
Answer: [Yes] 665
Justification: In the conclusion and also in the experiments section we discuss the limitations 666
of the approach. 667
Guidelines: 668
•The answer NA means that the paper has no limitation while the answer No means that 669
the paper has limitations, but those are not discussed in the paper. 670
• The authors are encouraged to create a separate ”Limitations” section in their paper. 671
•The paper should point out any strong assumptions and how robust the results are to 672
violations of these assumptions (e.g., independence assumptions, noiseless settings, 673
model well-specification, asymptotic approximations only holding locally). The authors 674
should reflect on how these assumptions might be violated in practice and what the 675
implications would be. 676
•The authors should reflect on the scope of the claims made, e.g., if the approach was 677
only tested on a few datasets or with a few runs. In general, empirical results often 678
depend on implicit assumptions, which should be articulated. 679
•The authors should reflect on the factors that influence the performance of the approach. 680
For example, a facial recognition algorithm may perform poorly when image resolution 681
is low or images are taken in low lighting. Or a speech-to-text system might not be 682
used reliably to provide closed captions for online lectures because it fails to handle 683
technical jargon. 684
•The authors should discuss the computational efficiency of the proposed algorithms 685
and how they scale with dataset size. 686
•If applicable, the authors should discuss possible limitations of their approach to 687
address problems of privacy and fairness. 688
•While the authors might fear that complete honesty about limitations might be used by 689
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 690
limitations that aren’t acknowledged in the paper. The authors should use their best 691
judgment and recognize that individual actions in favor of transparency play an impor- 692
tant role in developing norms that preserve the integrity of the community. Reviewers 693
will be specifically instructed to not penalize honesty concerning limitations. 694
3.Theory Assumptions and Proofs 695
Question: For each theoretical result, does the paper provide the full set of assumptions and 696
a complete (and correct) proof? 697
18Answer: [Yes] 698
Justification: For each of the theoretical results we provide a complete proof (in the appendix) 699
and a full set of assumptions. 700
Guidelines: 701
• The answer NA means that the paper does not include theoretical results. 702
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 703
referenced. 704
•All assumptions should be clearly stated or referenced in the statement of any theorems. 705
•The proofs can either appear in the main paper or the supplemental material, but if 706
they appear in the supplemental material, the authors are encouraged to provide a short 707
proof sketch to provide intuition. 708
•Inversely, any informal proof provided in the core of the paper should be complemented 709
by formal proofs provided in appendix or supplemental material. 710
• Theorems and Lemmas that the proof relies upon should be properly referenced. 711
4.Experimental Result Reproducibility 712
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 713
perimental results of the paper to the extent that it affects the main claims and/or conclusions 714
of the paper (regardless of whether the code and data are provided or not)? 715
Answer: [Yes] 716
Justification: We give all information that is needed to reproduce the experimental results 717
and also provide the code and data which is not online. 718
Guidelines: 719
• The answer NA means that the paper does not include experiments. 720
•If the paper includes experiments, a No answer to this question will not be perceived 721
well by the reviewers: Making the paper reproducible is important, regardless of 722
whether the code and data are provided or not. 723
•If the contribution is a dataset and/or model, the authors should describe the steps taken 724
to make their results reproducible or verifiable. 725
•Depending on the contribution, reproducibility can be accomplished in various ways. 726
For example, if the contribution is a novel architecture, describing the architecture fully 727
might suffice, or if the contribution is a specific model and empirical evaluation, it may 728
be necessary to either make it possible for others to replicate the model with the same 729
dataset, or provide access to the model. In general. releasing code and data is often 730
one good way to accomplish this, but reproducibility can also be provided via detailed 731
instructions for how to replicate the results, access to a hosted model (e.g., in the case 732
of a large language model), releasing of a model checkpoint, or other means that are 733
appropriate to the research performed. 734
•While NeurIPS does not require releasing code, the conference does require all submis- 735
sions to provide some reasonable avenue for reproducibility, which may depend on the 736
nature of the contribution. For example 737
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 738
to reproduce that algorithm. 739
(b)If the contribution is primarily a new model architecture, the paper should describe 740
the architecture clearly and fully. 741
(c)If the contribution is a new model (e.g., a large language model), then there should 742
either be a way to access this model for reproducing the results or a way to reproduce 743
the model (e.g., with an open-source dataset or instructions for how to construct 744
the dataset). 745
(d)We recognize that reproducibility may be tricky in some cases, in which case 746
authors are welcome to describe the particular way they provide for reproducibility. 747
In the case of closed-source models, it may be that access to the model is limited in 748
some way (e.g., to registered users), but it should be possible for other researchers 749
to have some path to reproducing or verifying the results. 750
5.Open access to data and code 751
19Question: Does the paper provide open access to the data and code, with sufficient instruc- 752
tions to faithfully reproduce the main experimental results, as described in supplemental 753
material? 754
Answer: [Yes] 755
Justification: We provide open access to the code, datasplits and synthetic datasets used in 756
the paper. 757
Guidelines: 758
• The answer NA means that paper does not include experiments requiring code. 759
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 760
public/guides/CodeSubmissionPolicy ) for more details. 761
•While we encourage the release of code and data, we understand that this might not be 762
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 763
including code, unless this is central to the contribution (e.g., for a new open-source 764
benchmark). 765
•The instructions should contain the exact command and environment needed to run to 766
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 767
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 768
•The authors should provide instructions on data access and preparation, including how 769
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 770
•The authors should provide scripts to reproduce all experimental results for the new 771
proposed method and baselines. If only a subset of experiments are reproducible, they 772
should state which ones are omitted from the script and why. 773
•At submission time, to preserve anonymity, the authors should release anonymized 774
versions (if applicable). 775
•Providing as much information as possible in supplemental material (appended to the 776
paper) is recommended, but including URLs to data and code is permitted. 777
6.Experimental Setting/Details 778
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 779
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 780
results? 781
Answer: [Yes] 782
Justification: We provide all data splits and hyperparameter choices for our algorithm in 783
the paper. Moreover, in the code we have understandable config files that contain all the 784
hyperparameters used in the experiments. 785
Guidelines: 786
• The answer NA means that the paper does not include experiments. 787
•The experimental setting should be presented in the core of the paper to a level of detail 788
that is necessary to appreciate the results and make sense of them. 789
•The full details can be provided either with the code, in appendix, or as supplemental 790
material. 791
7.Experiment Statistical Significance 792
Question: Does the paper report error bars suitably and correctly defined or other appropriate 793
information about the statistical significance of the experiments? 794
Answer: [Yes] 795
Justification: We use standard deviation to show the variability of the results. We do 796
not use statistical significance tests because our main claim is not that our method is 797
significantly better than state-of-the-art methods, but that it is an interesting new approach 798
that is applicable to a wide range of tasks. 799
Guidelines: 800
• The answer NA means that the paper does not include experiments. 801
•The authors should answer ”Yes” if the results are accompanied by error bars, confi- 802
dence intervals, or statistical significance tests, at least for the experiments that support 803
the main claims of the paper. 804
20•The factors of variability that the error bars are capturing should be clearly stated (for 805
example, train/test split, initialization, random drawing of some parameter, or overall 806
run with given experimental conditions). 807
•The method for calculating the error bars should be explained (closed form formula, 808
call to a library function, bootstrap, etc.) 809
• The assumptions made should be given (e.g., Normally distributed errors). 810
•It should be clear whether the error bar is the standard deviation or the standard error 811
of the mean. 812
•It is OK to report 1-sigma error bars, but one should state it. The authors should 813
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 814
of Normality of errors is not verified. 815
•For asymmetric distributions, the authors should be careful not to show in tables or 816
figures symmetric error bars that would yield results that are out of range (e.g. negative 817
error rates). 818
•If error bars are reported in tables or plots, The authors should explain in the text how 819
they were calculated and reference the corresponding figures or tables in the text. 820
8.Experiments Compute Resources 821
Question: For each experiment, does the paper provide sufficient information on the com- 822
puter resources (type of compute workers, memory, time of execution) needed to reproduce 823
the experiments? 824
Answer: [Yes] 825
Justification: We specify the computer resources and the time needed to run the experiments 826
in the paper. 827
Guidelines: 828
• The answer NA means that the paper does not include experiments. 829
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 830
or cloud provider, including relevant memory and storage. 831
•The paper should provide the amount of compute required for each of the individual 832
experimental runs as well as estimate the total compute. 833
•The paper should disclose whether the full research project required more compute 834
than the experiments reported in the paper (e.g., preliminary or failed experiments that 835
didn’t make it into the paper). 836
9.Code Of Ethics 837
Question: Does the research conducted in the paper conform, in every respect, with the 838
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 839
Answer: [Yes] 840
Justification: The research conducted in the paper conforms with the NeurIPS Code of 841
Ethics. 842
Guidelines: 843
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 844
•If the authors answer No, they should explain the special circumstances that require a 845
deviation from the Code of Ethics. 846
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 847
eration due to laws or regulations in their jurisdiction). 848
10.Broader Impacts 849
Question: Does the paper discuss both potential positive societal impacts and negative 850
societal impacts of the work performed? 851
Answer: [NA] 852
Justification: The paper does not address societal impact because we present a very basic 853
approach that is not directly applicable to any specific societal problem. 854
Guidelines: 855
21• The answer NA means that there is no societal impact of the work performed. 856
•If the authors answer NA or No, they should explain why their work has no societal 857
impact or why the paper does not address societal impact. 858
•Examples of negative societal impacts include potential malicious or unintended uses 859
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 860
(e.g., deployment of technologies that could make decisions that unfairly impact specific 861
groups), privacy considerations, and security considerations. 862
•The conference expects that many papers will be foundational research and not tied 863
to particular applications, let alone deployments. However, if there is a direct path to 864
any negative applications, the authors should point it out. For example, it is legitimate 865
to point out that an improvement in the quality of generative models could be used to 866
generate deepfakes for disinformation. On the other hand, it is not needed to point out 867
that a generic algorithm for optimizing neural networks could enable people to train 868
models that generate Deepfakes faster. 869
•The authors should consider possible harms that could arise when the technology is 870
being used as intended and functioning correctly, harms that could arise when the 871
technology is being used as intended but gives incorrect results, and harms following 872
from (intentional or unintentional) misuse of the technology. 873
•If there are negative societal impacts, the authors could also discuss possible mitigation 874
strategies (e.g., gated release of models, providing defenses in addition to attacks, 875
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 876
feedback over time, improving the efficiency and accessibility of ML). 877
11.Safeguards 878
Question: Does the paper describe safeguards that have been put in place for responsible 879
release of data or models that have a high risk for misuse (e.g., pretrained language models, 880
image generators, or scraped datasets)? 881
Answer: [NA] 882
Justification: We do not use scraped datasets or models that have a high risk for misuse. 883
Guidelines: 884
• The answer NA means that the paper poses no such risks. 885
•Released models that have a high risk for misuse or dual-use should be released with 886
necessary safeguards to allow for controlled use of the model, for example by requiring 887
that users adhere to usage guidelines or restrictions to access the model or implementing 888
safety filters. 889
•Datasets that have been scraped from the Internet could pose safety risks. The authors 890
should describe how they avoided releasing unsafe images. 891
•We recognize that providing effective safeguards is challenging, and many papers do 892
not require this, but we encourage authors to take this into account and make a best 893
faith effort. 894
12.Licenses for existing assets 895
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 896
the paper, properly credited and are the license and terms of use explicitly mentioned and 897
properly respected? 898
Answer: [Yes] 899
Justification: We mention all creators and original owners of code and data we use in the 900
paper. 901
Guidelines: 902
• The answer NA means that the paper does not use existing assets. 903
• The authors should cite the original paper that produced the code package or dataset. 904
•The authors should state which version of the asset is used and, if possible, include a 905
URL. 906
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 907
22•For scraped data from a particular source (e.g., website), the copyright and terms of 908
service of that source should be provided. 909
•If assets are released, the license, copyright information, and terms of use in the 910
package should be provided. For popular datasets, paperswithcode.com/datasets 911
has curated licenses for some datasets. Their licensing guide can help determine the 912
license of a dataset. 913
•For existing datasets that are re-packaged, both the original license and the license of 914
the derived asset (if it has changed) should be provided. 915
•If this information is not available online, the authors are encouraged to reach out to 916
the asset’s creators. 917
13.New Assets 918
Question: Are new assets introduced in the paper well documented and is the documentation 919
provided alongside the assets? 920
Answer: [NA] 921
Justification: The paper does not introduce new assets. 922
Guidelines: 923
• The answer NA means that the paper does not release new assets. 924
•Researchers should communicate the details of the dataset/code/model as part of their 925
submissions via structured templates. This includes details about training, license, 926
limitations, etc. 927
•The paper should discuss whether and how consent was obtained from people whose 928
asset is used. 929
•At submission time, remember to anonymize your assets (if applicable). You can either 930
create an anonymized URL or include an anonymized zip file. 931
14.Crowdsourcing and Research with Human Subjects 932
Question: For crowdsourcing experiments and research with human subjects, does the paper 933
include the full text of instructions given to participants and screenshots, if applicable, as 934
well as details about compensation (if any)? 935
Answer: [NA] 936
Justification: The paper does not involve crowdsourcing nor research with human subjects. 937
Guidelines: 938
•The answer NA means that the paper does not involve crowdsourcing nor research with 939
human subjects. 940
•Including this information in the supplemental material is fine, but if the main contribu- 941
tion of the paper involves human subjects, then as much detail as possible should be 942
included in the main paper. 943
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 944
or other labor should be paid at least the minimum wage in the country of the data 945
collector. 946
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 947
Subjects 948
Question: Does the paper describe potential risks incurred by study participants, whether 949
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 950
approvals (or an equivalent approval/review based on the requirements of your country or 951
institution) were obtained? 952
Answer: [NA] 953
Justification: The paper does not involve crowdsourcing nor research with human subjects. 954
Guidelines: 955
•The answer NA means that the paper does not involve crowdsourcing nor research with 956
human subjects. 957
23•Depending on the country in which research is conducted, IRB approval (or equivalent) 958
may be required for any human subjects research. If you obtained IRB approval, you 959
should clearly state this in the paper. 960
•We recognize that the procedures for this may vary significantly between institutions 961
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 962
guidelines for their institution. 963
•For initial submissions, do not include any information that would break anonymity (if 964
applicable), such as the institution conducting the review. 965
24