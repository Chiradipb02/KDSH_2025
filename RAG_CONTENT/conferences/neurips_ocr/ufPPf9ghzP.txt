A Neural Network Approach for Efficiently Answering
Most Probable Explanation Queries
in Probabilistic Models
Shivvrat Arya
Department of Computer Science
The University of Texas at Dallas
shivvrat.arya@utdallas.eduTahrima Rahman
Department of Computer Science
The University of Texas at Dallas
tahrima.rahman@utdallas.edu
Vibhav Gogate
Department of Computer Science
The University of Texas at Dallas
vibhav.gogate@utdallas.edu
Abstract
We propose a novel neural networks based approach to efficiently answer arbitrary
Most Probable Explanation (MPE) queries—a well-known NP-hard task—in large
probabilistic models such as Bayesian and Markov networks, probabilistic circuits,
and neural auto-regressive models. By arbitrary MPE queries, we mean that there
is no predefined partition of variables into evidence and non-evidence variables.
The key idea is to distill all MPE queries over a given probabilistic model into a
neural network and then use the latter for answering queries, eliminating the need
for time-consuming inference algorithms that operate directly on the probabilistic
model. We improve upon this idea by incorporating inference-time optimization
with self-supervised loss to iteratively improve the solutions and employ a teacher-
student framework that provides a better initial network, which in turn, helps reduce
the number of inference-time optimization steps. The teacher network utilizes
a self-supervised loss function optimized for getting the exact MPE solution,
while the student network learns from the teacher’s near-optimal outputs through
supervised loss. We demonstrate the efficacy and scalability of our approach on
various datasets and a broad class of probabilistic models, showcasing its practical
effectiveness.
1 Introduction
Probabilistic representations such as Probabilistic Circuits (PCs) [ 8], graphical models [ 26] such as
Bayesian Networks (BNs) and Markov Networks (MNs), and Neural Autoregressive Models (NAMs)
[50] are widely used to model large, multi-dimensional probability distributions. However, they face
a significant challenge: as the complexity of these distributions increases, solving practically relevant
NP-hard inference tasks such as finding the Most Probable Explanation (MPE) via exact inference
techniques [ 39,40] becomes increasingly difficult and time-consuming. In particular, although
various exact and approximate solvers exist for the MPE task in PCs, BNs and MNs, exact solvers
are often too slow for practical use, and approximate solvers tend to lack the necessary accuracy,
particularly in autoregressive models that currently rely on slow hill-climbing/beam search methods.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).In recent work, Arya et al. [ 4] proposed a method to overcome the limitations of existing approximate
methods by using neural networks (NNs) to solve the MPE task in PCs.1Their method draws
inspiration from the learning to optimize literature [ 12,15,29,42,55]. Given a PC and a predefined
partition of variables into query and evidence sets , the core idea is to train a NN that takes an
assignment to the evidence variables as input and outputs the most likely assignment to the query
variables w.r.t. the distribution defined by the PC. Arya et al. suggest using either supervised or
self-supervised learning techniques to train the NN; the former requires access to exact inference
schemes, while the latter does not and is therefore more practical.
In this paper, we address a more general and complex version of the MPE task than the one considered
by Arya et al. Specifically, we assume that there is no predefined partition of the variables into
evidence and query sets , which we refer to as the any-MPE task. The complexity of the any-MPE
task arises from the exponential increase in the number of input configurations, compounded by the
exponential number of possible divisions of variables into evidence and query sets. Furthermore, our
method applies to a broad class of probabilistic models, including BNs, MNs and NAMs, whereas
Arya et al.’s method is limited to PCs. In addition, Arya et al.’s method does not fully exploit the
capabilities of self-supervision, and the benefits of combining supervised and self-supervised loss
functions.
This paper presents a novel approach that uses a NN for solving the any-MPE task in a broad class of
probabilistic models (PMs) and achieves technical advancements in three key aspects:
1. Efficient MPE Inference via Encoding Scheme and Loss Function: We introduce a new
encoding scheme that tailors the NN architecture to the specific structure of the input PM. This
scheme not only delineates the input and output nodes for the NN but also establishes a methodology
for setting input values and extracting the MPE solution from the NN’s outputs. Furthermore, we
propose a tractable, and differentiable self-supervised loss function, enabling efficient training.
2. Inference Time Optimization with ITSELF: We introduce a novel inference technique called
Inference Time Self Supervised Training (ITSELF). This technique iteratively refines the MPE
solution during the inference process itself. It utilizes gradient descent ( back-propagation ) to
update the NN’s parameters using our proposed self-supervised loss, leading to continual (anytime)
improvement towards near-optimal solutions. ITSELF fully utilizes the power of our self-supervised
loss, as it does not require labeled data or an external MPE solver.
3. Two-Phase Pre-training with Teacher-Student Architecture: To address challenges associated
with self-supervised learning and ITSELF, we propose a two-phase pre-training strategy that leverages
a teacher-student architecture. Self-supervised learning can suffer from overfitting and requires careful
regularization. Additionally, ITSELF, especially with random initializations, might necessitate a
substantial number of gradient updates to converge on optimal solutions. Our approach addresses
these issues using the following methodology: (i) The teacher network first overfits the training data
using ITSELF and (ii) The student network is then trained using supervised loss functions (e.g.,
binary cross-entropy) by treating the teacher network’s output as pseudo-labels. This supervised
training phase improves and regularizes the parameter learning process of the student network. It also
provides a robust starting point for ITSELF, significantly reducing the required optimization steps
and leading to substantial performance gains.
Finally, we conduct a detailed experimental comparison of our method with existing approaches
on several types of PMs such as PCs, PGMs and NAMs. Our results demonstrate that our method
surpasses state-of-the-art approximate inference techniques in terms of both accuracy and speed.
2 Background and Motivation
Without loss of generality, we use binary variables which take values from the set {0,1}. We denote
a random variable by an uppercase letter (e.g., X), and a value assigned to it by the corresponding
lowercase letter (e.g., x). We denote a set of random variables by a bold uppercase letter (e.g., X) and
an assignment of values to all variables in the set by the corresponding bold lowercase letter (e.g., x).
1Arya et al. [ 4] developed a NN-based method for solving the marginal maximum-a-posteriori (MMAP) task
in PCs. In this paper, we focus on the MPE task, also sometimes referred to as the full MAP task, which is a
special case of MMAP. Our method can be easily extended for solving the MMAP problem in PCs and tractable
graphical models. For simplicity of exposition, we concentrate on the MPE task in this paper.
2Throughout the paper when we use the term probabilistic models (PMs), we are referring to a
broad class of probabilistic models in which computing the likelihood2of an assignment to all
variables in the model can be done in polynomial (preferably linear) time in the size of the model.
This class includes, among others, Bayesian and Markov networks collectively called Probabilistic
Graphical Models (PGMs) [ 26], smooth and decomposable Probabilistic Circuits (PCs) [ 8], and
Neural Autoregressive Models (NAMs) such as NADE [50] and MADE [17].
We are interested in solving the most probable explanation (MPE) task in PMs, namely the task of
finding the most likely assignment to all unobserved (non-evidence) variables given observations
(evidence). Formally, let Mdenote a probabilistic model defined over a set of variables Xthat
represents the distribution pM(x). We categorize the variables Xinto evidence E⊆Xand query
Q⊆Xgroups, ensuring that E∩Q=∅andE∪Q=X. Then, given an assignment eto the set of
evidence variables E, the MPE task can be formulated as:
MPE(Q,e) = argmax
qpM(q|e) = argmax
q{logpM(q,e)} (1)
It is known that the MPE task is NP-hard in general and even hard to approximate [ 9,11,36,41,44].
Motivation: The goal of this paper is to develop a method that trains a NN for a given PM and, at
test time, serves as an approximate MPE solver for any-MPE query posed over the PM. By any-MPE,
we mean that the NN can take an assignment to an arbitrary subset of variables (evidence) as input
and output the most likely assignments to the remaining (query) variables. Recently, Arya et al. [ 4]
proposed a NN-based solution for solving the MPE task in PCs under the constraint that the partition
of the variables into evidence and query sets is known before training the NN . This constraint is highly
restrictive because, for generative models, it is unlikely that such a partition of variables is known
in advance. In such cases, one would typically train a discriminative model rather than a generative
one. Unlike Arya et al.’s method, our approach yields an any-MPE solver. Additionally, Arya et al.’s
approach has several limitations in that it does not fully exploit the benefits of self-supervision during
inference time and requires the use of relatively large NNs to achieve good performance in practice.
Our proposed approach, described next, addresses these limitations.
3 A Self-Supervised Neural Approximator for any-MPE
In this section, we develop a neural network (NN) based approach for solving the any-MPE task.
Specifically, given a PM, we develop an input encoding (see Section 3.1) that determines the number
of input nodes of the NN and sets their values for the given MPE query. Additionally, we develop an
output encoding scheme that specifies the number of NN output nodes required for the given PM
and enables the recovery of the MPE solution from the outputs. For training the NN, we introduce
a tractable and differentiable self-supervised loss function (see Section 3.2), whose global minima
aligns with the MPE solutions to efficiently learn the parameters of the NN given unlabeled data .
3.1 An Encoding For any-MPE Instances
Since NNs require fixed-sized inputs and outputs, we introduce input and output encodings that
generate fixed-length input and output vectors for each PM from a given MPE problem instance
MPE(Q,e). To encode the input, for each variable Xi∈X, we associate two input nodes in the NN,
denoted by ˆXiand¯Xi. Thus for a PM having n(namely, |X|=n) variables, the corresponding NN
has2ninput nodes. Given a query MPE(Q,e), we set the values of the input nodes as follows: (1) If
Xi∈EandXi= 0is ine, then we set ˆXi= 0and¯Xi= 1; (2) If Xi∈EandXi= 1is ine, then
we set ˆXi= 1and¯Xi= 0; and (3) If Xi∈Qthen we set ˆXi= 0and¯Xi= 0. (The assignment
ˆXi= 1and¯Xi= 1is not used.) It is easy to see that the input encoding described above yields an
injective mapping between the set of all possible MPE queries over the given PM and the set {0,1}2n.
This means that each unique MPE query (Q,e)will yield a unique 0-1 input vector of size 2n.
The output of the neural network comprises of nnodes with sigmoid activation, where each output
node is associated with a variable Xi∈X. We ignore the outputs corresponding to the evidence
variables and define a loss function over the outputs corresponding to the query variables in the set
Q. The MPE solution can be reconstructed from the output nodes of the NN by thresholding the
2or a value proportional to it such as the unnormalized probability in Markov networks.
3output nodes corresponding to the query variables appropriately (e.g., if the value of the output node
is greater than 0.5, then the query variable is assigned the value 1; otherwise it is assigned to 0).
3.2 A Self-Supervised Loss Function for any-MPE
Since the output nodes of our proposed NN use sigmoid activation, each output is continuous and lies
in the range [0,1]. Given an MPE query MPE(Q,e), letqc∈[0,1]|Q|denote the (continuous) Most
Probable Explanation (MPE) assignment predicted by the NN. In MPE inference, given e, we want
to find an assignment qsuch that logpM(q,e)is maximized, namely, −logpM(q,e)is minimized.
Thus, a natural loss function that we can use is −logpM(q,e). Unfortunately, the NN outputs a
continuous vector qcand as a result pM(qc,e)is not defined.
Next, we describe how to solve the above problem by leveraging the following property of the class
of PMs that we consider in this paper—specifically BNs, MNs, PCs and NAMs. In these PMs, the
function ℓ(q,e) =−logpM(e,q), which is a function from {0,1}n→Ris either a multi-linear
polynomial or a neural network, and can be computed in linear time in the size of the PM. To facilitate
the use of continuous outputs, we define a loss function ℓc(qc,e) : [0,1]n→Rsuch that ℓccoincides
withℓon{0,1}n. For PGMs and PCs, ℓis a multi-linear function and ℓcis obtained by substituting
each occurrence of a discrete variable qi∈qwith the corresponding continuous variable qc
i∈qc
where qc
i∈[0,1]. In NAMs, ℓis a NN and we can perform a similar substitution—we substitute each
binary input qiin the NN with a continuous variable qc
i∈[0,1]. This substitution transforms the
discrete NN into a continuous function while preserving its functional form.
An important property of ℓcis that it can be evaluated and differentiated in polynomial time. Moreover,
when ℓis defined by either a neural network (in NAMs) or a multilinear function (in BNs, MNs and
PCs), the minimum value of ℓcover the domain [0,1]nis less than or equal to the minimum value of
the original function ℓover the discrete domain {0,1}n. Formally,
Proposition 1. Letl(q,e) :{0,1}n→Rbe either a neural network or a multilinear function, and
letlc(qc,e) : [0,1]n→Rbe its continuous extension obtained by substituting each binary input qi
with a continuous variable qc
i∈[0,1]. Then,
min
qc∈[0,1]nℓc(qc,e)≤min
q∈{0,1}nℓ(q,e)
Following Arya et al. [ 4], we propose to improve the quality of the loss function by tightening the
lower bound given in proposition 1 with an entropy-based penalty ( ℓE), governed by α >0.
ℓE(qc, α) =−α|Q|X
j=1
qc
jlog(qc
j) + (1 −qc
j) log(1 −qc
j)
(2)
This penalty encourages discrete solutions by preferring qc
jvalues close to 0 or 1, where αmodulates
the trade-off. Setting αto 0 yields the continuous approximation; conversely, an αvalue of ∞results
exclusively in discrete outcomes. From proposition 1 and by using the theory of Lagrange multipliers,
we can show that for any α >0, the use of the entropy penalty yields a tighter lower bound:
Proposition 2.
min
qc∈[0,1]nℓc(qc,e)≤min
qc∈[0,1]nℓc(qc,e) +ℓE(qc, α)≤min
q∈{0,1}nℓ(q,e)
How to use the Loss Function: Given a PM defined over nvariables, we can use the self-supervised
loss function ℓc(qc,e) +ℓE(qc, α)(treating αas a hyper-parameter) to train any neural network
(NN) architecture that has 2ninput nodes and noutput nodes. This trained NN can then be used to
answer any arbitrary MPE query posed over the PM. The training data for the neural network consists
of assignments (evidence e) to a subset of the variables. Each training example can be generated
using the following three-step process. We first sample a full assignment xto all variables in the PM
using techniques like Gibbs sampling or perfect sampling for tractable distributions such as PCs and
BNs. Second, we choose an integer kuniformly at random from the range {1, . . . , n }and designate
krandomly selected variables as evidence variables E, and the remaining n−kas query variables Q.
Finally, we project the full assignment xonE. The primary advantage of using the self-supervised
loss function is that it eliminates the need for access to a dedicated MPE solver to provide supervision
during training; gradient-based training of the neural network provides the necessary supervision.
4Test 
Dataset 
/
Example
NN 
(Random 
or 
pre-
trained)
No
Converged
Gradient 
Computation
and 
Update 
NN
Yes
Loss 
Computation
eFigure 1: ITSELF Training Procedure
Generate 
Mini-Batch
ITSELF
Training 
and 
Forward 
Pass
e
Train 
Student 
Network
Copy
e Figure 2: One Training Epoch for GUIDE
3.3 Inference-Time Neural Optimization using Self-Supervised Loss
At a high level, assuming that the NN is over-parameterized, if we use the self-supervised loss and
repeatedly run (stochastic) gradient updates over the NN for a given dataset, theoretical results [ 2,13]
as well as prior experimental work [ 46,56] suggest that the parameters of the NN will converge
to a point near the global minimum of the self-supervised loss function. This means that through
gradient updates, the network will find a near-optimal MPE assignment for each training example.
This strategy of performing gradient updates over the NN can also be used during inference (test)
time to iteratively improve the MPE solution , thereby maximizing the benefits of self-supervision.
Specifically, at test time, given a test dataset (or example), we initialize the NN either randomly
or using a pre-trained model and then run gradient-based updates over the NN iteratively until
convergence. The gradient is computed w.r.t. the self-supervised loss function ℓc(qc,e) +ℓE(qc, α).
We call the resulting algorithm ITSELF ( Inference Time Optimization using SELF -Supervised Loss),
as detailed in Figure 1. The performance of ITSELF typically improves with each iteration until the
loss converges.
Our proposed method, ITSELF, is closely related to test-time training approaches which are widely
used to solve problems in deep learning [ 1,10,19,30–32,38,49,51,57]. Our method differs from
these previous approaches in that the global minima of our proposed self-supervised loss correspond
to the MPE solutions, provided that the penalty αis sufficiently large.
4 Supervised Knowledge Transfer from ITSELF
A drawback of our self-supervised loss function is that, unlike supervised loss functions such as binary
cross entropy, it is a non-convex function of the NN outputs3. As a result, it has a significantly larger
number of local minima compared to the supervised loss function, but also a potentially exponential
number of global minima, because an MPE problem can have multiple optimal solutions [ 35], all of
which have the same loss function value. Thus, optimizing and regularizing using the self-supervised
loss is difficult compared to a supervised loss, especially when the number of training examples is
large.
Moreover, our experiments show that large datasets necessitate large, over-parameterized neural
networks (NNs) to achieve near-optimal MPE solutions for all examples. However, when the training
data is limited and the NN is sufficiently over-parameterized, our preliminary findings, along with
theoretical and empirical results from prior studies [ 3,6,23,27,28], suggest that the NN is more
likely to approach the global optima. Specifically, with a reasonably sized NN and a small dataset,
the algorithm ITSELF tends to yield near-optimal MPE solutions. A further challenge with ITSELF
is that even for small datasets, achieving convergence from a random initialization requires numerous
iterations of gradient descent, rendering the training process inefficient and slow.
3Note that we are referring to convexity with respect to the outputs, not the parameters of the NN.
5Algorithm 1 GUided I terative D ual LE arning with Self-supervised Teacher ( GUIDE )
1:Input: Training data D, teacher Tand student Shaving the same structure
2:Output: Trained student network S
3: ▷Database DB stores the best MPE assignment and loss value for each example in D
4:Initialize: Randomly initialize T,S, and DB
5:foreach epoch do
6:Sample a mini-batch D′fromD
7:Update the parameters of Tusing the algorithm ITSELF (self-supervised loss) with Dataset D′
8:foreach example eiinD′do
9: Make a forward-pass over Tto get an MPE assignment qiforei
10: Update the entry in DB foreiwithqiif it has a lower loss value than the current entry
11: end for
12: Update the parameters of Susing the mini-batch D′and labels from DB and a supervised loss
13:T ← S ▷Initialize TwithSfor the next epoch
14:end for
4.1 Teacher-Student Strategy
To address these challenges (using small datasets with ITSELF; designing better initialization for
it; and using non-convex loss functions for training), we propose a two-network teacher-student
strategy [ 7,16,20–22,24,37,47,52–54], where we have two networks with the same structure that
are trained via mini-batch gradient updates. The teacher network is overfitted to the mini-batch using
our self-supervised loss via the ITSELF algorithm, and the student network is subsequently trained
with a supervised loss function such as binary cross entropy. By overfitting the teacher network via
ITSELF on the mini-batch, we ensure that it finds near-optimal MPE assignments for all (unlabeled)
examples in the mini-batch and eventually over the whole training dataset.
The student network then learns from the teacher’s outputs, using them as soft labels in a supervised
learning framework. This transfer of knowledge mitigates the optimization difficulties associated
with the non-convex self-supervised loss, allowing the student network to achieve faster convergence
and better generalization with a more manageable model size. Additionally, this strategy reduces
the need for severe over-parameterization and extensive training iterations for the teacher network
because it is operating on a smaller dataset. It also helps achieve better initialization for ITSELF.
4.2 Training Procedure
Our proposed training procedure, which we call GUIDE , is detailed in Algorithm 1. The algorithm
trains a two-network system comprising a teacher network ( T) and a student network ( S) with the
same structure. The goal is to train the student network using a combination of self-supervised and
supervised learning strategies. The algorithm takes as input the training data D, along with the teacher
and student networks, TandS, respectively and outputs a trained network S. A database ( DB) is
utilized to store the best MPE assignment and corresponding loss value for each example in D. The
parameters of TandS, and the entries in DB, are randomly initialized at the start.
In each epoch, a mini-batch D′is sampled from the training data D. The parameters of the teacher
network Tare then updated using the ITSELF algorithm (which uses a self-supervised loss), applied
to the mini-batch D′(the mini-batch helps address large data issues associated with ITSELF). For
each example eiinD′, we perform a forward-pass over Tto obtain an MPE assignment qi. The
database DB is subsequently updated with qiif it has a lower loss value than the current entry for ei.
Following this, the parameters of the student network Sare updated using the mini-batch D′, the
labels from DB, and a supervised loss function ( ℓsup) such as Binary Cross Entropy or L2loss.
Finally, the parameters of the teacher network Tare reinitialized with the updated parameters of the
student network Sto prepare for the next epoch (addressing the initialization issue associated with
ITSELF). Figure 2 illustrates a single training epoch of GUIDE.
Thus, at a high level, Algorithm 1 leverages the strengths of both self-supervised and supervised
learning to improve training efficiency and reduce the model complexity, yielding a student network
S. Moreover, at test time, the student network can serve as an initialization for ITSELF.
65 Experiments
This section evaluates the ITSELF method (see section 3.3), the GUIDE teacher-student training
method (see section 4) and the method that uses only self-supervised training, which we call SSMP
(see section 3.2). We benchmark these against various baselines, including neural network-based and
traditional polynomial-time algorithms that directly operate on the probabilistic model. We begin
by detailing our experimental framework, including competing methods, evaluation metrics, neural
network architectures, and datasets.
5.1 Datasets and Graphical Models
We used twenty binary datasets extensively used in tractable probabilistic models literature [ 5,18,
34,50]—referred to as TPM datasets—for evaluating PCs and NAMs. For the purpose of evaluating
PGMs, we utilized high treewidth models from previous UAI inference competitions [14].
To train Sum Product Networks (SPNs), our choice of PCs, we employed the DeeProb-kit library
[33], with SPN sizes ranging from 46to9666 nodes. For NAMs, we trained Masked Autoencoder
for Distribution Estimation (MADE) models using PyTorch, following the approach in Germain
et al. [17]. For Markov Networks (MNs), a specific type of PGM, we applied Gibbs sampling to
generate 8,000, 1,000, and 1,000 samples for the training, testing, and validation sets, respectively.
The query ratio ( qr), defined as the fraction of variables in the query set, was varied across the set
{0.1,0.3,0.5,0.7,0.8,0.9}for each probabilistic model (PM).
5.2 Baseline Methods and Evaluation Criteria
PCs - We used three polynomial-time baseline methods from the probabilistic circuits and probabilis-
tic graphical models literature as benchmarks [41, 45].
•MAX Approximation (MAX) [ 45] transforms sum nodes into max nodes. During the upward pass,
max nodes output the highest weighted value from their children. The downward pass, starting
from the root, selects the child with the highest value at each max node and includes all children of
product nodes.
•Maximum Likelihood Approximation (ML) [ 41] computes the marginal distribution pM(Qi|e)for
each variable Qi∈Q, setting Qito its most likely value.
•Sequential Approximation (Seq) [ 41] iteratively assigns query variables according to an order o.
At each step j, it selects the j-th query variable Qjinoand assigns to it a value qjsuch that
pM(qj|e,y)is maximized, where yis an assignment of values to all query variables from 1to
j−1.
We further evaluated the impact of initializing stochastic hill climbing searches using solutions from
all baseline approaches and our proposed methods for MPE inference, conducting 60-second searches
for each MPE problem in our experiments, as detailed in Park and Darwiche [41].
NAM s - As a baseline, we used the stochastic hill-climbing search (HC) algorithm. Following a
procedure similar to that used for PCs, we conducted a 60-second hill-climbing search for each test
example, with query variables initialized randomly and setting evidence variables according to the
values in the given example.
PGMs - We employed the distributed AND/OR Branch and Bound (AOBB) method [ 39] as a baseline,
using the implementation outlined in Otten [40]. Since AOBB is an anytime algorithm, we set a
60-second time limit for inference per test example.
Neural Baselines - Arya et al. [4]introduced Self-Supervised learning based MMAP solver for PCs
(SSMP), training a neural network to handle queries on a fixed variable partition within PCs. We
extend this approach to address the any-MPE task in PMs (see Section 3.2), using a single network to
answer any-MPE queries as an additional neural baseline.
Evaluation Criteria - We evaluated competing approaches based on log-likelihood (LL) scores,
calculated as lnpM(e,q), and inference times for given evidence eand query output q. Higher
log-likelihood scores indicate better performance, while shorter inference times are preferable.
5.3 Neural Network-Based Approaches
7MAX ML Seq SSMPGUIDE SSMP
ITSELFGUIDE
ITSELFMAX
ML
Seq
SSMP
GUIDE
SSMP
ITSELF
GUIDE
ITSELF0 79 94 81 71 39 12
41 0 43 55 49 15 5
26 64 0 44 43 16 7
39 65 76 0 24 4 0
49 71 77 71 0 8 0
81 105 104 104 99 0 7
108 115 113 107 107 94 0(a) PCs: Initial Solutions
MAX ML Seq SSMPGUIDE SSMP
ITSELFGUIDE
ITSELFMAX
ML
Seq
SSMP
GUIDE
SSMP
ITSELF
GUIDE
ITSELF0 65 81 66 62 46 21
35 0 49 41 27 22 4
21 45 0 29 27 15 4
36 55 71 0 23 17 1
39 71 73 49 0 24 0
53 73 82 70 61 0 12
79 88 94 91 89 72 0
(b) PCs: Hill-Climbing
HC SSMPGUIDE SSMP
ITSELFGUIDE
ITSELFHC
SSMP
GUIDE
SSMP
ITSELF
GUIDE
ITSELF0 46 38 36 12
34 0 7 13 4
42 66 0 34 16
44 61 40 0 6
68 72 60 70 0
(c) NAMs
AOBB SSMPGUIDE SSMP
ITSELFGUIDE
ITSELFAOBB
SSMP
GUIDE
SSMP
ITSELF
GUIDE
ITSELF0 7 6 6 5
9 0 3 4 0
9 12 0 15 3
10 12 0 0 3
11 16 12 13 0
(d) PGMs
Figure 3: MPE method compari-
son across PMs. Blue shows row
superiority, red shows column su-
periority; darker shades indicate
larger values.We implemented two neural network training protocols for each
PM and query ratio: SSMP and GUIDE . Each model was
trained for 20 epochs following the training procedure outlined
by Arya et al. [4]for SSMP. Both protocols employed two dis-
tinct inference strategies, thus forming four neural-based variants.
In the first strategy, we performed a single forward pass through
the network to estimate the values of query variable, as specified
by Arya et al. [4]. The second strategy utilized our novel test-
time optimization-based ITSELF approach for inference. The
ITSELF optimization terminates after 100 iterations or upon
loss convergence for both PCs and PGMs. For NAMs, we in-
crease the limit to 1,000 iterations while keeping the convergence
criterion.
We standardized network architectures for PMs across all experi-
ments. For PCs, we used fully connected Neural Networks (NN)
with three hidden layers (128, 256, 512 nodes). For NAMs and
PGMs, a single hidden layer of 512 nodes was employed. All
hidden layers featured ReLU activation, while the output layers
used sigmoid functions with dropout for regularization [ 48]. We
optimized all models using Adam [ 25] and implemented them
in PyTorch [43] on an NVIDIA A40 GPU.
Results for PCs : We compare methods—including three
polynomial-time baselines, neural network-based SSMP, and our
ITSELF and GUIDE methods—on 20 TPM datasets as shown
in the contingency table in figure 3a (detailed results in the sup-
plementary materials). We generated 120 test datasets for the
MPE task using 20 PCs across 6 query ratios ( qr). Each cell
(i, j)in the table represents how often (out of 120) the method
in row ioutperformed the method in column jbased on average
log-likelihood scores. Any difference between 120 and the com-
bined frequencies of cells (i, j)and(j, i)indicates cases where
the compared methods achieved similar scores. We present sim-
ilar contingency tables for Hill Climbing Search over PCs (Fig.
3b), NAMs (Fig. 3c), and PGMs (Fig. 3d) to benchmark the
proposed methods against the baselines.
The contingency table for PC (Fig. 3a) shows that methods incor-
porating ITSELF consistently outperform both polynomial-time
and traditional neural baselines, as indicated by the dark blue
cells in the corresponding rows. Notably, GUIDE + ITSELF
is superior to all the other methods in almost two-thirds of the
120 cases, while SSMP + ITSELF is better than both SSMP
andGUIDE . In contrast, the polynomial-time baseline MAX
is better than both SSMP and GUIDE (as used in Arya et al.
[4]), highlighting ITSELF’s significant role in boosting model
performance for the complex any-MPE task.
We compare MAX and GUIDE + ITSELF using a heatmap in
Figure 4a. The y-axis presents datasets by variable count and the
x-axis represents query ratio. Each cell displays the percentage
difference in mean LL scores between the methods, calculated as
%Diff. = 100×(llnn−llmax)/|llmax|. The heatmap shows that
GUIDE + ITSELF achieves performance comparable to MAX
for small query sets. As the problem complexity increases with
an increase in query set size, our method consistently outper-
forms MAX across all datasets, except for NLTCS and Tretail,
as highlighted by the green cells. In the 12 cases where GUIDE
+ ITSELF underperforms, the performance gap remains minimal,
as indicated by the limited number of red cells in the heatmap.
80.10.30.50.70.80.9
Query RatioNLTCS
MSNBC
KDDCup2k
Plants
Netflix
Jester
Audio
Accidents
mshrms
Connect 4
Retail
RCV-1
DNA
Book
WebKB
Reuters-52
20 NGrp
mvrvw
BBC
AdDataset Name
<-10-8-6-3-11368>10
Percentage Difference (%)(a) PC: GUIDE + ITSELF vs. MAX
0.50.70.80.9
Query RatioNLTCS
MSNBC
KDDCup2k
Plants
Netflix
Jester
Audio
Accidents
mshrms
Connect 4
Retail
RCV-1
DNA
Book
WebKB
Reuters-52
20 NGrp
mvrvw
BBC
AdDataset Name
<-50-39-28-17-66172839>50
Percentage Difference (%)
(b) NAM: GUIDE + ITSELF vs.
HC
Figure 4: Heatmaps showing LL
% Differences. Top: PC; Bottom:
NAM. Green cells: our method
is better. Darker shades indicate
larger values.Figure 3b further analyzes the performance of our proposed meth-
ods against various baselines as initialization strategies for Hill
Climbing Search. This comparison evaluates the effectiveness of
ITSELF and GUIDE in enhancing anytime methods compared to
conventional heuristic initialization approaches. Notably, meth-
ods incorporating ITSELF provide superior initialization for local
search-based algorithms.
Results for NAMs : The contingency table in Figure 3c presents
our evaluation of several methods for NAMs, including HC and
two neural network approaches, SSMP and GUIDE , each tested
with two inference schemes. We evaluated these methods on 20
TPM datasets, creating 80 test sets for the MPE task using 20
MADEs across four query ratios ( qr).
TheGUIDE + ITSELF approach demonstrates superior perfor-
mance compared to both baseline methods and other neural in-
ference schemes, aligning with observations from PC. While
HC outperforms SSMP, both GUIDE and the combination of
SSMP-based training with ITSELF-based inference surpass HC,
highlighting their advantages over the baseline.
The heatmaps in Figure 4b further highlight the superior perfor-
mance of GUIDE + ITSELF for NAMs, particularly in larger
datasets where it outperforms the HC baseline by over 50% in
most cases, as indicated by the dark green cells. The combi-
nation of GUIDE -based learning with ITSELF-based inference
consistently outperforms the baseline across most datasets, with
exceptions only in the Mushrooms, Connect 4, and Retail. Over-
all, the GUIDE + ITSELF approach significantly enhances the
quality of the MPE solutions in NAM models.
Results for PGMs : The contingency table in 3d compares the
performance of AOBB and four neural-network-based methods
on PGMs across four high-treewidth networks. For this evalua-
tion, we generated 16 test datasets for the MPE task using four
PGMs across four query ratios ( qr).
Consistent with results from previous PMs, methods using IT-
SELF for inference consistently outperform the baseline methods
AOBB and SSMP across most scenarios. Both GUIDE and
SSMP outperform AOBB in at least 50 percent of the tests. The
supplementary material presents comparisons against exact so-
lutions, conducted on less complex probabilistic models where
ground truth computation remains tractable.
Does a teacher-student-based network outperform a single
network trained with the self-supervised loss? ( GUIDE vs.
SSMP):
This analysis aims to evaluate the performance of GUIDE against
traditional neural network training methods used in SSMP across
different PMs and inference schemes. Using traditional inference
scheme (i.e., one forward pass through the network), GUIDE
consistently outperforms SSMP, demonstrating its superiority
in 60% of scenarios for PCs, more than 80% for NAM models,
and 75% for PGM models. When employing ITSELF-based
inference, GUIDE maintains this advantage, achieving higher
quality solutions in more than 75%, 85%, and 80% of cases for
PCs, NAMs, and PGMs, respectively. Therefore, models trained
usingGUIDE are consistently superior to those trained with
SSMP for the any-MPE task.
9Does inference time optimization improve performance? (One-Pass vs. Multi-Pass):
In this analysis, we compare the performance of the single-pass inference method to that of the
proposed multi-pass inference method (ITSELF). ITSELF combined with SSMP training outperforms
the other methods in over 85% cases for PC, and more than 75% for NAM and PGM models. When
used on models trained with GUIDE , ITSELF demonstrates even better results, achieving superior
performance in nearly 90% of PC cases and 75% for both NAMs and PGMs. Overall, GUIDE with
ITSELF inference emerges as the most effective method across all experiments. Empirical evidence
consistently demonstrates ITSELF’s superiority over single-pass inference across PMs.
The inference time analysis, detailed in the supplementary material, compares computational ef-
ficiency across methods using the natural logarithm of execution time in microseconds. Neural
network-based approaches with traditional inference demonstrate the fastest performance across all
PMs, as they only require a single forward pass to compute query variable values. For MADE, models
trained with GUIDE and ITSELF are the next most efficient. In PGMs, GUIDE + ITSELF ranks
third, followed by SSMP + ITSELF. For PCs, MAX is marginally faster than both GUIDE + ITSELF
and SSMP + ITSELF, while ML and Seq have the longest computational times. In general, models
trained with GUIDE achieve shorter inference times than those trained with the self-supervised loss
(SSMP), as they require fewer ITSELF iterations due to more effective initial training.
Summary: Our experiments demonstrate that GUIDE + ITSELF outperforms both polynomial-time
and neural-based baselines across various PMs, as evidenced by higher log-likelihood scores. Notably,
ITSELF demonstrates significant advantages over traditional single-pass inference in addressing the
complex any-MPE query task within probabilistic models, emphasizing the importance of Inference
Time Optimization. Furthermore, the superior performance of models trained with GUIDE compared
to SSMP highlights the effectiveness of the dual network approach, which improves initial model
quality and establishes an optimal starting point for ITSELF.
6 Conclusion and Future Work
We introduced novel methods for answering Most Probable Explanation (MPE) queries in probabilistic
models. Our approach employs self-supervised loss functions to represent MPE objectives, enabling
tractable loss and gradient computations during neural network training. We also proposed a new
inference time optimization technique, ITSELF, which iteratively improves the solution to the
MPE problem via gradient updates. Additionally, we introduced a dual-network-based strategy that
combines supervised and unsupervised training which we call GUIDE to provide better initialization
for ITSELF and addressing various challenges associated with self-supervised training. Our method
was tested on various benchmarks, including probabilistic circuits, neural autoregressive models, and
probabilistic graphical models, using 20 binary datasets and high tree-width networks. It outperformed
polytime baselines and other neural methods, substantially in some cases. Additionally, it improved
the effectiveness of stochastic hill climbing (local) search strategies.
Future work includes solving complex queries in probabilistic models with constraints; training
neural networks with losses from multiple probabilistic models to embed their inference mechanisms;
boosting performance by developing advanced encoding strategies for similar tasks; implementing
sophisticated neural architectures tailored to probabilistic models; etc.
Acknowledgements
This work was supported in part by the DARPA Perceptually-Enabled Task Guidance (PTG) Program
under contract number HR00112220005, the DARPA Assured Neuro Symbolic Learning and Rea-
soning (ANSR) Program under contract number HR001122S0039, the National Science Foundation
grant IIS-1652835, and the AFOSR award FA9550-23-1-0239.
References
[1]Ferran Alet, Maria Bauza, Kenji Kawaguchi, Nurullah Giray Kuru, Tomás Lozano-Pérez, and
Leslie Kaelbling. Tailoring: Encoding inductive biases by optimizing unsupervised objectives
at prediction time. In Advances in Neural Information Processing Systems , volume 34, pages
29206–29217. Curran Associates, Inc., 2021.
10[2]Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings
of the 36th International Conference on Machine Learning , volume 97 of Proceedings of
Machine Learning Research , pages 242–252. PMLR, 09–15 Jun 2019.
[3]Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. In Pro-
ceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research ,
pages 322–332. PMLR, 2019.
[4]Shivvrat Arya, Tahrima Rahman, and Vibhav Gogate. Neural Network Approximators for
Marginal MAP in Probabilistic Circuits. Proceedings of the AAAI Conference on Artificial
Intelligence , 38(10):10918–10926, March 2024. ISSN 2374-3468. doi: 10.1609/aaai.v38i10.
28966.
[5]Jessa Bekker, Jesse Davis, Arthur Choi, Adnan Darwiche, and Guy Van den Broeck. Tractable
learning for complex probability queries. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett, editors, Advances in Neural Information Processing Systems , volume 28. Curran
Associates, Inc., 2015.
[6]Lénaïc Chizat and Francis R. Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in Neural Information Processing
Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS
2018, December 3-8, 2018, Montréal, Canada , pages 3040–3050, 2018.
[7]Jang Hyun Cho and Bharath Hariharan. On the Efficacy of Knowledge Distillation. 2019
IEEE/CVF International Conference on Computer Vision (ICCV) , pages 4793–4801, October
2019. doi: 10.1109/ICCV .2019.00489.
[8]YooJung Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: A unifying
framework for tractable probabilistic models. Technical report, University of California, Los
Angeles, 2020.
[9]Gregory F. Cooper. The computational complexity of probabilistic inference using bayesian
belief networks. Artificial Intelligence , 42(2-3):393–405, March 1990. ISSN 00043702. doi:
10.1016/0004-3702(90)90060-D.
[10] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close
the natural distribution shift performance gap in deep learning based compressed sensing.
InProceedings of the 39th International Conference on Machine Learning , volume 162 of
Proceedings of Machine Learning Research , pages 4754–4776. PMLR, 17–23 Jul 2022.
[11] Cassio P de Campos. New Complexity Results for MAP in Bayesian Networks. IJCAI
International Joint Conference on Artificial Intelligence , pages 2100–2106, 01 2011. doi:
10.5591/978-1-57735-516-8/IJCAI11-351.
[12] Priya L. Donti, David Rolnick, and J. Zico Kolter. DC3: A learning method for optimization
with hard constraints. In International Conference on Learning Representations , October 2020.
[13] S. Du, Xiyu Zhai, B. Póczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. International Conference on Learning Representations , 2018.
[14] G. Elidan and A. Globerson. The 2010 UAI Approximate Inference Challenge . 2010.
[15] Ferdinando Fioretto, Terrence W. K. Mak, and Pascal Van Hentenryck. Predicting AC Optimal
Power Flows: Combining Deep Learning and Lagrangian Dual Methods. Proceedings of the
AAAI Conference on Artificial Intelligence , 34(01):630–637, April 2020. ISSN 2374-3468. doi:
10.1609/aaai.v34i01.5403.
[16] Tommaso Furlanello, Zachary Chase Lipton, M. Tschannen, L. Itti, and Anima Anandkumar.
Born Again Neural Networks. In International Conference on Machine Learning , May 2018.
11[17] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In Francis Bach and David Blei, editors, Proceedings of the 32nd
International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning
Research , pages 881–889, Lille, France, 07–09 Jul 2015. PMLR.
[18] Jan Van Haaren and Jesse Davis. Markov Network Structure Learning: A Randomized Feature
Generation Approach. Proceedings of the AAAI Conference on Artificial Intelligence , 26(1):
1148–1154, 2012. ISSN 2374-3468. doi: 10.1609/aaai.v26i1.8315.
[19] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models.
InThe Twelfth International Conference on Learning Representations , 2024.
[20] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A
Comprehensive Overhaul of Feature Distillation. 2019 IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 1921–1930, October 2019. doi: 10.1109/ICCV .2019.00201.
[21] Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi. Knowledge Transfer via
Distillation of Activation Boundaries Formed by Hidden Neurons. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 33, pages 3779–3787, July 2019. doi:
10.1609/aaai.v33i01.33013779.
[22] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural
network. CoRR , abs/1503.02531, 2015.
[23] Arthur Jacot, Clément Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montréal, Canada , pages 8580–8589, 2018.
[24] Jangho Kim, Seonguk Park, and Nojun Kwak. Paraphrasing Complex Network: Network
Compression via Factor Transfer. ArXiv , February 2018.
[25] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd
International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings , 2015.
[26] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques .
MIT press, 2009.
[27] Jaehoon Lee, S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Narain Sohl-Dickstein. Finite versus infinite neural networks: an empirical study.
Neural Information Processing Systems , 2020.
[28] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha
Sohl-Dickstein, and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear
Models Under Gradient Descent. Journal of Statistical Mechanics: Theory and Experiment ,
2020(12):124002, December 2020. ISSN 1742-5468. doi: 10.1088/1742-5468/abc62b.
[29] Ke Li and Jitendra Malik. Learning to optimize. International Conference on Learning
Representations , 2016.
[30] Yushu Li, Xun Xu, Yongyi Su, and Kui Jia. On the Robustness of Open-World Test-Time
Training: Self-Training with Dynamic Prototype Expansion. In 2023 IEEE/CVF International
Conference on Computer Vision (ICCV) , pages 11802–11812, Paris, France, October 2023.
IEEE. ISBN 9798350307184. doi: 10.1109/ICCV51070.2023.01087.
[31] Huan Liu, Zijun Wu, Liangyan Li, Sadaf Salehkalaibar, Jun Chen, and Keyan Wang. Towards
Multi-domain Single Image Dehazing via Test-time Training. In 2022 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages 5821–5830, New Orleans, LA,
USA, June 2022. IEEE. ISBN 978-1-66546-946-3. doi: 10.1109/CVPR52688.2022.00574.
[32] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and
Alexandre Alahi. TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive? In
Advances in Neural Information Processing Systems , volume 34, pages 21808–21820. Curran
Associates, Inc., 2021.
12[33] Lorenzo Loconte and Gennaro Gala. DeeProb-kit: a python library for deep probabilistic
modelling, 2022. URL https://github.com/deeprob-org/deeprob-kit .
[34] Daniel Lowd and Jesse Davis. Learning Markov Network Structure with Decision Trees. In
2010 IEEE International Conference on Data Mining , pages 334–343. IEEE, 2010. ISBN
978-1-4244-9131-5. doi: 10.1109/ICDM.2010.128.
[35] Radu Marinescu and Rina Dechter. Counting the optimal solutions in graphical models. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
URL https://proceedings.neurips.cc/paper_files/paper/2019/file/
fc2e6a440b94f64831840137698021e1-Paper.pdf .
[36] Denis Deratani Mauá and Cassio P. de Campos. Approximation complexity of maximum A
posteriori inference in sum-product networks. CoRR , abs/1703.06045, 2017.
[37] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and
Hassan Ghasemzadeh. Improved Knowledge Distillation via Teacher Assistant. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 34, pages 5191–5198, April 2020.
doi: 10.1609/aaai.v34i04.5963.
[38] David Osowiechi, G. A. V . Hakim, Mehrdad Noori, Milad Cheraghalikhani, Ismail Ben Ayed,
and Christian Desrosiers. Tttflow: Unsupervised test-time training with normalizing flow. IEEE
Workshop/Winter Conference on Applications of Computer Vision , 2022. doi: 10.48550/arXiv.
2210.11389.
[39] L. Otten and R. Dechter. A case study in complexity estimation: Towards parallel branch-and-
bound over graphical models. Conference on Uncertainty in Artificial Intelligence , 2012.
[40] Lars Otten. DAOOPT: Sequential and distributed AND/OR branch and bound for MPE problems.
2012. URL https://github.com/lotten/daoopt .
[41] James D. Park and Adnan Darwiche. Complexity results and approximation strategies for map
explanations. J. Artif. Int. Res. , 21(1):101–133, feb 2004. ISSN 1076-9757.
[42] Seonho Park and Pascal Van Hentenryck. Self-Supervised Primal-Dual Learning for Constrained
Optimization. Proceedings of the AAAI Conference on Artificial Intelligence , 37(4):4052–4060,
June 2023. ISSN 2374-3468. doi: 10.1609/aaai.v37i4.25520.
[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. Advances in neural information processing
systems , 32, 2019.
[44] Robert Peharz. Foundations of Sum-Product Networks for Probabilistic Modeling . PhD thesis,
PhD thesis, Medical University of Graz, 2015.
[45] Hoifung Poon and Pedro Domingos. Sum-Product Networks: A New Deep Architecture. In
Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence , pages
337–346. AUAI Press, 2011.
[46] Tiancheng Qin, S. Rasoul Etesami, and Cesar A Uribe. Faster convergence of local SGD
for over-parameterized models. Transactions on Machine Learning Research , 2024. ISSN
2835-8856.
[47] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, C. Gatta, and
Yoshua Bengio. FitNets: Hints for Thin Deep Nets. CoRR , December 2014.
[48] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine
Learning Research , 15(56):1929–1958, 2014. ISSN 1533-7928.
13[49] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-Time
Training with Self-Supervision for Generalization under Distribution Shifts. In Proceedings of
the 37th International Conference on Machine Learning , pages 9229–9248. PMLR, November
2020.
[50] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. Journal of Machine Learning Research , 17(205):1–37,
2016.
[51] Dequan Wang, Evan Shelhamer, Shaoteng Liu, B. Olshausen, and Trevor Darrell. Tent:
Fully test-time adaptation by entropy minimization. International Conference on Learning
Representations , 2021.
[52] Chenglin Yang, Lingxi Xie, Chi Su, and Alan L. Yuille. Snapshot Distillation: Teacher-Student
Optimization in One Generation. 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 2854–2863, June 2019. doi: 10.1109/CVPR.2019.00297.
[53] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A Gift from Knowledge Distillation:
Fast Optimization, Network Minimization and Transfer Learning. 2017 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , pages 7130–7138, July 2017. doi:
10.1109/CVPR.2017.754.
[54] Sergey Zagoruyko and N. Komodakis. Paying More Attention to Attention: Improving the
Performance of Convolutional Neural Networks via Attention Transfer. ArXiv , November 2016.
[55] Ahmed S. Zamzam and Kyri Baker. Learning Optimal Solutions for Extremely Fast AC
Optimal Power Flow. In 2020 IEEE International Conference on Communications, Control,
and Computing Technologies for Smart Grids (SmartGridComm) , pages 1–6, November 2020.
doi: 10.1109/SmartGridComm47815.2020.9303008.
[56] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 , 2016.
[57] Wentao Zhu, Yufang Huang, Daguang Xu, Zhen Qian, Wei Fan, and Xiaohui Xie. Test-time
training for deformable multi-scale image registration. IEEE International Conference on
Robotics and Automation , 2021.
14A Experimental Setup
A.1 Datasets and Models
Table 1 summarizes the datasets and the probabilistic circuits trained on them. We use the same
datasets for both PCs [ 8] and NAMs [ 17,50]. The selection includes both smaller datasets, such as
NLTCS and MSNBC, and larger datasets with over 1000 nodes.
For Markov networks, we utilize high treewidth grid networks, specifically grid40x40.f2.wrap,
grid40x40.f5.wrap, grid40x40.f10.wrap, and grid40x40.f15.wrap. Each model contains 4800 vari-
ables and 1600 factors.
Table 1: Summary of datasets used with their respective numbers of variables and nodes in proba-
bilistic circuits.
Dataset Number of Variables Number of Nodes in PC
NLTCS 16 125
MSNBC 17 46
KDDCup2k 64 274
Plants 69 3737
Audio 100 348
Jester 100 274
Netflix 100 400
Accidents 111 1178
Mushrooms 112 902
Connect 4 126 2128
Retail 135 359
RCV-1 150 519
DNA 180 1855
Book 500 1628
WebKB 839 3154
Reuters-52 889 7348
20 NewsGroup 910 2467
Movie reviews 1001 2567
BBC 1058 3399
Ad 1556 9666
A.2 Hyperparameters Details
Our experimental framework was designed to ensure consistency and efficiency across all conducted
experiments. For NAM’s, we used MADE, training the model with two hidden layers of 512 and
1024 units, respectively, using the hyperparameters from Germain et al. [17].
For neural network-based solvers, the mini-batch size was set to 512 samples, and a learning rate
decay strategy, reducing the rate by 0.9 upon loss plateauing, was implemented to improve training
efficiency. Optimal hyperparameters were identified via extensive 5-fold cross-validation.
In discrete loss scenarios, the hyperparameter αplayed a pivotal role. We systematically explored the
optimal αvalue across the range 0.001,0.01,0.1,1,10,100,1000 for neural-based models, including
ITSELF and GUIDE . Notably, higher αvalues better constrain outputs to binary, thereby facilitating
near-optimal results.
BExtending the Current Approach to Other Data Types and Inference Tasks
The current approach can be extended to support both multi-valued discrete and continuous variables,
broadening its utility in diverse scenarios.
For multi-valued discrete variables, the method can be adapted by implementing a multi-class, multi-
output classification head. Each query variable is represented by a softmax output node, which
provides soft evidence by generating probabilistic distributions across multiple discrete values.
15To incorporate continuous variables, we introduce a linear activation function in the output layer. The
loss function, specifically the multi-linear representation of the PM, is modified to accommodate
continuous neural network outputs. For example, in Probabilistic Circuits that use Gaussian distribu-
tions, continuous values can be directly integrated into the loss function, facilitating gradient-based
backpropagation.
These extensions primarily involve adjusting the network’s output layer and refining the self-
supervised loss function represented by the PM. Notably, other elements of our approach, including
the ITSELF and GUIDE procedures, remain unchanged.
Our approach further extends to additional inference tasks over probabilistic models, including
marginal MAP and constrained most probable explanation (CMPE) tasks. However, the scalability of
this approach depends on the computational efficiency of evaluating the loss function for each infer-
ence task. When this evaluation becomes computationally infeasible, the proposed method—training
a neural network to answer queries over probabilistic models—may itself become infeasible. For
example, performing marginal MAP inference over NAMs and PGMs requires repeated evaluations
of the loss function associated with the marginal MAP task and its gradient during training. This
iterative process, essential for updating the neural network’s parameters, can become prohibitively
resource-intensive due to the high computational demands of evaluating the marginal MAP loss over
these probabilistic models.
C A Comparative Analysis of Performance of ITSELF for Different
Pre-Training Methods
This section evaluates the performance of models initialized through various techniques—random
initialization, SSMP, and GUIDE . Each plot represents the loss for a distinct test example, with the
x-axis denoting the number of ITSELF iterations and the y-axis showing the Negative Log Likelihood
(NLL) scores. Lower NLL values signify better solutions. Through this empirical assessment, we
compare the impact of different pre-training methods on model performance.
Figures 5 to 28 present the plots for NAMs. The plots for PCs are shown in Figures 29 to 67. Figures
68 to 78 illustrate the plots for PGMs. We selected the following datasets for PCs and NAMs: DNA,
RCV-1, Reuters-52, Netflix, WebKB, Audio, Moviereview, and Jester. For PGMs, we used all the
datasets presented in the main paper. Each plot consists of two sections. The left section presents the
Negative Log-Likelihood Loss for 1000 iterations for all methods. The right section contains two
sub-plots: the top sub-plot displays the zoomed-in losses for the first 200 iterations, while the bottom
sub-plot shows the zoomed-in losses for the last 200 iterations.
We randomly initialize the parameters for the random model and perform 1000 iterations of ITSELF
for inference. For the two pre-trained models (SSMP and GUIDE ), we update the top Nlayers,
where Nis the number of layers corresponding to that loss curve, and fix the remaining bottom layers.
We extract features by passing the input through these fixed layers and then train the parameters of
the top Nlayers. We again perform 1000 iterations of ITSELF for inference. For NAMs and PGMs,
we use neural networks with up to one hidden layer, while for PCs, we employ models with up to
three hidden layers.
From the plots for the three Probabilistic Modelss (PMs), we observe that models pre-trained using
the proposed GUIDE training scheme generally have a better starting point for ITSELF, indicated
by a lower loss, compared to all other models. Across a wide array of datasets, PGMs, and query
percentages, the GUIDE method consistently converges to a lower or equivalent loss compared to
other models. Remarkably, it sometimes achieves a loss value that is less than half of the nearest
competing model. Furthermore, the losses for GUIDE are typically more stable than those of other
initialization. In some scenarios, all models achieve a similar final loss, although models initialized
with SSMP and those randomly initialized may experience oscillations in their loss values.
Models pre-trained using the traditional self-supervised loss (SSMP) typically have better or similar
starting points than randomly initialized models. However, models pre-trained using the SSMP
method might converge to a worse loss than their GUIDE pre-trained counterparts.
In most cases, convergence is rapid, even with a reduced learning rate of 10−4compared to the
experiments shown in the main paper. Most methods converge within 200 to 300 iterations, although
some may still oscillate during the later iterations of ITSELF.
160 200 400 600 800 1000
Iterations708090100110120130140Negative Log Likelihood (NLL) ScoreDataset: DNA, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 20080100120140
900 1000747678Figure 5: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the DNA
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations80100120140160Negative Log Likelihood (NLL) ScoreDataset: DNA, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 20080100120140160
900 100072737475
Figure 6: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the DNA
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations6080100120140160180Negative Log Likelihood (NLL) ScoreDataset: DNA, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200100150
900 100060.062.565.067.570.0
Figure 7: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the DNA
Dataset at a Query Ratio of 0.9.
170 200 400 600 800 1000
Iterations20406080100Negative Log Likelihood (NLL) ScoreDataset: RCV-1, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200255075100
900 1000304050Figure 8: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the RCV-1
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations406080100Negative Log Likelihood (NLL) ScoreDataset: RCV-1, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200406080100120
900 100030405060
Figure 9: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the RCV-1
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations20406080100120Negative Log Likelihood (NLL) ScoreDataset: RCV-1, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200255075100125
900 1000204060
Figure 10: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the RCV-1
Dataset at a Query Ratio of 0.9.
180 200 400 600 800 1000
Iterations100200300400500600700Negative Log Likelihood (NLL) ScoreDataset: Reuters-52, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200200400600
900 1000100150200250Figure 11: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Reuters-52
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations02004006008001000Negative Log Likelihood (NLL) ScoreDataset: Reuters-52, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 2002505007501000
900 1000100200
Figure 12: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Reuters-52
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations020040060080010001200Negative Log Likelihood (NLL) ScoreDataset: Reuters-52, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 2005001000
900 100050100150200250
Figure 13: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Reuters-52
Dataset at a Query Ratio of 0.9.
190 200 400 600 800 1000
Iterations5055606570Negative Log Likelihood (NLL) ScoreDataset: Netﬂix, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200506070
900 1000505152Figure 14: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Netflix
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations40506070Negative Log Likelihood (NLL) ScoreDataset: Netﬂix, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 2004050607080
900 100036384042
Figure 15: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Netflix
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations4050607080Negative Log Likelihood (NLL) ScoreDataset: Netﬂix, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 2004050607080
900 1000363840
Figure 16: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Netflix
Dataset at a Query Ratio of 0.9.
200 200 400 600 800 1000
Iterations200300400500600Negative Log Likelihood (NLL) ScoreDataset: WebKB, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200200300400500600
900 1000175200225250Figure 17: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the WebKB
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations100200300400500600700Negative Log Likelihood (NLL) ScoreDataset: WebKB, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200200400600
900 1000100150200
Figure 18: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the WebKB
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations100200300400500600700Negative Log Likelihood (NLL) ScoreDataset: WebKB, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200200400600800
900 1000100200300
Figure 19: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the WebKB
Dataset at a Query Ratio of 0.9.
210 200 400 600 800 1000
Iterations203040506070Negative Log Likelihood (NLL) ScoreDataset: Audio, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200204060
900 100018192021Figure 20: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Audio
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations20304050607080Negative Log Likelihood (NLL) ScoreDataset: Audio, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 20020406080
900 1000152025
Figure 21: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Audio
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations304050607080Negative Log Likelihood (NLL) ScoreDataset: Audio, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200406080
900 10002530354045
Figure 22: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Audio
Dataset at a Query Ratio of 0.9.
220 200 400 600 800 1000
Iterations300400500600700Negative Log Likelihood (NLL) ScoreDataset: Movie reviews, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200400600
900 1000250260270280Figure 23: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Movie
reviews Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations200300400500600700800Negative Log Likelihood (NLL) ScoreDataset: Movie reviews, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200200400600800
900 1000180200220240260
Figure 24: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Movie
reviews Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations100200300400500600700800900Negative Log Likelihood (NLL) ScoreDataset: Movie reviews, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 2002004006008001000
900 1000140160180
Figure 25: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Movie
reviews Dataset at a Query Ratio of 0.9.
230 200 400 600 800 1000
Iterations5560657075Negative Log Likelihood (NLL) ScoreDataset: Jester, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 2005560657075
900 100053.053.554.0Figure 26: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Jester
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations4045505560657075Negative Log Likelihood (NLL) ScoreDataset: Jester, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200506070
900 1000424446
Figure 27: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Jester
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations304050607080Negative Log Likelihood (NLL) ScoreDataset: Jester, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200406080
900 100030354045
Figure 28: Analysis of ITSELF Loss Across Various Pre-Trained Models for NAMs on the Jester
Dataset at a Query Ratio of 0.9.
240 200 400 600 800 1000
Iterations8687888990919293Negative Log Likelihood (NLL) ScoreDataset: DNA, Query Percentage: 0.1
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200889092
900 100086.386.486.5Figure 29: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the DNA Dataset
at a Query Ratio of 0.1.
0 200 400 600 800 1000
Iterations7580859095100105Negative Log Likelihood (NLL) ScoreDataset: DNA, Query Percentage: 0.3
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 2008090100
900 100075.2575.5075.7576.0076.25
Figure 30: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the DNA Dataset
at a Query Ratio of 0.3.
250 200 400 600 800 1000
Iterations8090100110120130Negative Log Likelihood (NLL) ScoreDataset: DNA, Query Percentage: 0.5
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20080100120
900 1000788082Figure 31: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the DNA Dataset
at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations708090100110120130140Negative Log Likelihood (NLL) ScoreDataset: DNA, Query Percentage: 0.7
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20080100120140
900 1000717273
Figure 32: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the DNA Dataset
at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations8090100110120130140Negative Log Likelihood (NLL) ScoreDataset: DNA, Query Percentage: 0.9
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20080100120140
900 1000747576
Figure 33: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the DNA Dataset
at a Query Ratio of 0.9.
260 200 400 600 800 1000
Iterations6668707274Negative Log Likelihood (NLL) ScoreDataset: RCV-1, Query Percentage: 0.1
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20067.570.072.575.0
900 100065.9065.9566.00Figure 34: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the RCV-1
Dataset at a Query Ratio of 0.1.
0 200 400 600 800 1000
Iterations3040506070Negative Log Likelihood (NLL) ScoreDataset: RCV-1, Query Percentage: 0.3
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20040506070
900 100031.55831.55931.56031.561
Figure 35: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the RCV-1
Dataset at a Query Ratio of 0.3.
0 200 400 600 800 1000
Iterations2030405060708090Negative Log Likelihood (NLL) ScoreDataset: RCV-1, Query Percentage: 0.5
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200406080100
900 100023.200023.202523.205023.207523.2100
Figure 36: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the RCV-1
Dataset at a Query Ratio of 0.5.
270 200 400 600 800 1000
Iterations5060708090100Negative Log Likelihood (NLL) ScoreDataset: RCV-1, Query Percentage: 0.7
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 2006080100
900 1000464748Figure 37: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the RCV-1
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations406080100120Negative Log Likelihood (NLL) ScoreDataset: RCV-1, Query Percentage: 0.9
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200406080100120
900 100030.030.531.031.5
Figure 38: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the RCV-1
Dataset at a Query Ratio of 0.9.
0 200 400 600 800 1000
Iterations20406080100120140160Negative Log Likelihood (NLL) ScoreDataset: Reuters-52, Query Percentage: 0.1
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20050100150
900 100027.027.5
Figure 39: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Reuters-52
Dataset at a Query Ratio of 0.1.
280 200 400 600 800 1000
Iterations100150200250300350400Negative Log Likelihood (NLL) ScoreDataset: Reuters-52, Query Percentage: 0.3
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200100200300400
900 1000758085Figure 40: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Reuters-52
Dataset at a Query Ratio of 0.3.
0 200 400 600 800 1000
Iterations100200300400500600Negative Log Likelihood (NLL) ScoreDataset: Reuters-52, Query Percentage: 0.5
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200200400600
900 100090100
Figure 41: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Reuters-52
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations100200300400500600700Negative Log Likelihood (NLL) ScoreDataset: Reuters-52, Query Percentage: 0.7
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200200400600800
900 1000607080
Figure 42: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Reuters-52
Dataset at a Query Ratio of 0.7.
290 200 400 600 800 1000
Iterations200400600800Negative Log Likelihood (NLL) ScoreDataset: Reuters-52, Query Percentage: 0.9
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200200400600800
900 10006080Figure 43: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Reuters-52
Dataset at a Query Ratio of 0.9.
0 200 400 600 800 1000
Iterations53.554.054.555.055.556.056.557.057.5Negative Log Likelihood (NLL) ScoreDataset: Netﬂix, Query Percentage: 0.1
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20054555657
900 100053.6253.6453.66
Figure 44: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Netflix
Dataset at a Query Ratio of 0.1.
0 200 400 600 800 1000
Iterations505254565860Negative Log Likelihood (NLL) ScoreDataset: Netﬂix, Query Percentage: 0.3
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200505560
900 100049.249.349.4
Figure 45: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Netflix
Dataset at a Query Ratio of 0.3.
300 200 400 600 800 1000
Iterations55.057.560.062.565.067.570.072.5Negative Log Likelihood (NLL) ScoreDataset: Netﬂix, Query Percentage: 0.5
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20055606570
900 100054.054.555.0Figure 46: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Netflix
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations4550556065Negative Log Likelihood (NLL) ScoreDataset: Netﬂix, Query Percentage: 0.7
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 2004550556065
900 100044.2544.5044.7545.00
Figure 47: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Netflix
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations455055606570Negative Log Likelihood (NLL) ScoreDataset: Netﬂix, Query Percentage: 0.9
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200506070
900 1000434445
Figure 48: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Netflix
Dataset at a Query Ratio of 0.9.
310 200 400 600 800 1000
Iterations230240250260270280290Negative Log Likelihood (NLL) ScoreDataset: WebKB, Query Percentage: 0.1
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200240260280
900 1000228230232Figure 49: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the WebKB
Dataset at a Query Ratio of 0.1.
0 200 400 600 800 1000
Iterations200250300350400Negative Log Likelihood (NLL) ScoreDataset: WebKB, Query Percentage: 0.3
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200200300400
900 1000180190
Figure 50: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the WebKB
Dataset at a Query Ratio of 0.3.
0 200 400 600 800 1000
Iterations150200250300350400450500550Negative Log Likelihood (NLL) ScoreDataset: WebKB, Query Percentage: 0.5
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200200300400500
900 1000160162164166168
Figure 51: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the WebKB
Dataset at a Query Ratio of 0.5.
320 200 400 600 800 1000
Iterations100200300400500600Negative Log Likelihood (NLL) ScoreDataset: WebKB, Query Percentage: 0.7
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200200400600
900 1000120130140Figure 52: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the WebKB
Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations100200300400500600700Negative Log Likelihood (NLL) ScoreDataset: WebKB, Query Percentage: 0.9
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200200400600800
900 10004244464850
Figure 53: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the WebKB
Dataset at a Query Ratio of 0.9.
0 200 400 600 800 1000
Iterations58.559.059.560.060.561.061.562.062.5Negative Log Likelihood (NLL) ScoreDataset: Audio, Query Percentage: 0.1
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20059606162
900 100058.3658.3858.4058.4258.44
Figure 54: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Audio
Dataset at a Query Ratio of 0.1.
330 200 400 600 800 1000
Iterations47.550.052.555.057.560.062.565.0Negative Log Likelihood (NLL) ScoreDataset: Audio, Query Percentage: 0.3
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20050556065
900 100048505254Figure 55: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Audio
Dataset at a Query Ratio of 0.3.
0 200 400 600 800 1000
Iterations5055606570Negative Log Likelihood (NLL) ScoreDataset: Audio, Query Percentage: 0.5
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200506070
900 100048505254
Figure 56: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Audio
Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations354045505560657075Negative Log Likelihood (NLL) ScoreDataset: Audio, Query Percentage: 0.7
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20040506070
900 1000354045
Figure 57: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Audio
Dataset at a Query Ratio of 0.7.
340 200 400 600 800 1000
Iterations3040506070Negative Log Likelihood (NLL) ScoreDataset: Audio, Query Percentage: 0.9
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 2004060
900 1000304050Figure 58: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Audio
Dataset at a Query Ratio of 0.9.
0 200 400 600 800 1000
Iterations330340350360370380390400Negative Log Likelihood (NLL) ScoreDataset: Movie reviews, Query Percentage: 0.1
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200340360380400
900 1000333.00333.25333.50333.75334.00
Figure 59: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Movie
reviews Dataset at a Query Ratio of 0.1.
0 200 400 600 800 1000
Iterations350400450500550Negative Log Likelihood (NLL) ScoreDataset: Movie reviews, Query Percentage: 0.3
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200350400450500550
900 1000352354356
Figure 60: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Movie
reviews Dataset at a Query Ratio of 0.3.
350 200 400 600 800 1000
Iterations250300350400450500550600650Negative Log Likelihood (NLL) ScoreDataset: Movie reviews, Query Percentage: 0.5
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200300400500600
900 1000270275Figure 61: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Movie
reviews Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations200300400500600700Negative Log Likelihood (NLL) ScoreDataset: Movie reviews, Query Percentage: 0.7
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200200400600800
900 1000212.5215.0217.5220.0
Figure 62: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Movie
reviews Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations200300400500600700Negative Log Likelihood (NLL) ScoreDataset: Movie reviews, Query Percentage: 0.9
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200200400600800
900 1000135.0135.5136.0136.5
Figure 63: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Movie
reviews Dataset at a Query Ratio of 0.9.
360 200 400 600 800 1000
Iterations75.075.576.076.577.0Negative Log Likelihood (NLL) ScoreDataset: Jester, Query Percentage: 0.1
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200757677
900 100074.7074.7174.7274.73Figure 64: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Jester
Dataset at a Query Ratio of 0.1.
0 200 400 600 800 1000
Iterations545658606264Negative Log Likelihood (NLL) ScoreDataset: Jester, Query Percentage: 0.3
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200556065
900 10005455
Figure 65: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Jester
Dataset at a Query Ratio of 0.3.
0 200 400 600 800 1000
Iterations52.555.057.560.062.565.067.570.0Negative Log Likelihood (NLL) ScoreDataset: Jester, Query Percentage: 0.5
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 20055606570
900 100051525354
Figure 66: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Jester
Dataset at a Query Ratio of 0.5.
370 200 400 600 800 1000
Iterations5055606570Negative Log Likelihood (NLL) ScoreDataset: Jester, Query Percentage: 0.7
Random, LR
Random, NN - 1 layers
Random, NN - 2 layers
Random, NN - 3 layersSSMP, LR
SSMP, NN - 1 layers
SSMP, NN - 2 layers
SSMP, NN - 3 layersGUIDE , LR
GUIDE , NN - 1 layers
GUIDE , NN - 2 layers
GUIDE , NN - 3 layers
0 100 200506070
900 100048505254Figure 67: Analysis of ITSELF Loss Across Various Pre-Trained Models for PCs on the Jester
Dataset at a Query Ratio of 0.7.
380 200 400 600 800 1000
Iterations−12000−10000−8000−6000−4000−2000Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f10.wrap, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200−10000−7500−5000−2500
900 1000−12000−11500−11000−10500Figure 68: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f10.wrap Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations−12000−10000−8000−6000−4000−20000Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f10.wrap, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200−10000−50000
900 1000−11500−11000−10500−10000
Figure 69: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f10.wrap Dataset at a Query Ratio of 0.9.
390 200 400 600 800 1000
Iterations−18000−16000−14000−12000−10000−8000−6000Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f15.wrap, Query Percentage: 0.5
SSMP, LR
SSMP, NN - 1 layersGUIDE , LRGUIDE , NN - 1 layers
0 100 200−15000−10000−5000
900 1000−17000−16000−15000−14000Figure 70: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f15.wrap Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations−18000−16000−14000−12000−10000−8000−6000−4000−2000Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f15.wrap, Query Percentage: 0.7
SSMP, LR
SSMP, NN - 1 layersGUIDE , LRGUIDE , NN - 1 layers
0 100 200−15000−10000−5000
900 1000−17000−16000−15000
Figure 71: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f15.wrap Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations−17500−15000−12500−10000−7500−5000−25000Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f15.wrap, Query Percentage: 0.9
SSMP, LR
SSMP, NN - 1 layersGUIDE , LRGUIDE , NN - 1 layers
0 100 200−15000−10000−50000
900 1000−16500−16000−15500−15000
Figure 72: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f15.wrap Dataset at a Query Ratio of 0.9.
400 200 400 600 800 1000
Iterations−2500−2250−2000−1750−1500−1250−1000−750Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f2.wrap, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200−2500−2000−1500−1000−500
900 1000−2550−2500−2450−2400−2350Figure 73: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f2.wrap Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations−2500−2000−1500−1000−500Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f2.wrap, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200−2000−10000
900 1000−2500−2400−2300−2200
Figure 74: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f2.wrap Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations−2500−2000−1500−1000−5000Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f2.wrap, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200−2000−1500−1000−5000
900 1000−2500−2400−2300−2200
Figure 75: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f2.wrap Dataset at a Query Ratio of 0.9.
410 200 400 600 800 1000
Iterations−6000−5000−4000−3000−2000Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f5.wrap, Query Percentage: 0.5
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200−6000−4000−2000
900 1000−6250−6000−5750−5500−5250Figure 76: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f5.wrap Dataset at a Query Ratio of 0.5.
0 200 400 600 800 1000
Iterations−6000−5000−4000−3000−2000−1000Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f5.wrap, Query Percentage: 0.7
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200−6000−4000−20000
900 1000−6250−6000−5750−5500−5250
Figure 77: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f5.wrap Dataset at a Query Ratio of 0.7.
0 200 400 600 800 1000
Iterations−6000−5000−4000−3000−2000−10000Negative Log Likelihood (NLL) ScoreDataset: grid40x40.f5.wrap, Query Percentage: 0.9
Random, LR
Random, NN - 1 layersSSMP, LR
SSMP, NN - 1 layersGUIDE , LR
GUIDE , NN - 1 layers
0 100 200−4000−20000
900 1000−5800−5600−5400−5200
Figure 78: Analysis of ITSELF Loss Across Various Pre-Trained Models for PGMs on the
grid40x40.f5.wrap Dataset at a Query Ratio of 0.9.
42D Inference Time Comparison
HC SSMPGUIDE SSMP
ITSELFGUIDE
ITSELF
ApproachNLTCS
MSNBC
Plants
Audio
Jester
Netﬂix
Accidents
Mushrooms
Ad
Retail
DNA
Book
WebKB
BBCDataset17.91 5.72 5.60 11.28 9.95
17.91 5.74 5.65 11.22 10.26
17.91 5.69 5.61 11.10 10.03
17.91 5.55 5.48 11.10 10.35
17.91 5.67 5.60 11.15 9.86
17.91 5.29 5.23 10.79 9.88
17.91 5.08 4.97 10.92 9.99
17.91 5.02 4.97 10.81 9.86
17.91 5.64 5.25 10.99 9.90
17.91 4.97 4.89 10.70 10.12
17.91 5.29 5.17 10.84 9.68
17.91 4.48 4.28 10.95 9.92
17.91 4.53 4.21 10.79 9.68
17.91 5.04 4.60 10.94 9.74Execution Time Comparison
6810121416
Log Execution Time
Figure 79: Heatmap depicting the inference time for MADE on a logarithmic microsecond scale,
where a lighter color denotes shorter (more favorable) durations.
MAX ML Seq SSMPGUIDE SSMP
ITSELFGUIDE
ITSELF
ApproachNLTCS
MSNBC
Plants
Audio
Jester
Netﬂix
Accidents
Mushrooms
Ad
Retail
DNA
Book
WebKB
BBCDataset12.80 12.13 13.90 5.76 5.52 13.35 12.21
12.26 11.23 12.94 5.75 5.50 12.65 11.56
15.03 16.88 17.77 6.18 5.36 16.62 15.30
12.70 15.05 17.61 6.01 5.37 14.13 12.96
12.81 14.84 17.58 6.02 5.38 13.95 12.89
12.80 14.96 17.62 6.06 5.40 14.33 12.84
13.84 16.03 17.76 6.12 5.40 15.26 14.09
13.71 16.01 17.76 6.11 5.36 15.05 14.04
16.04 17.92 17.92 6.39 5.40 17.68 16.53
12.66 15.36 17.70 6.00 5.34 14.31 13.30
14.24 16.94 17.85 6.13 5.39 15.90 14.62
14.11 17.70 17.91 6.21 5.36 15.75 14.68
14.74 17.85 17.91 6.27 5.40 16.45 15.19
14.86 17.91 17.92 6.24 5.39 16.50 15.13Execution Time Comparison
6810121416
Log Execution Time
Figure 80: Heatmap depicting the inference time for PC on a logarithmic microsecond scale, where a
lighter color denotes shorter (more favorable) durations.
We present the inference times for all baselines and proposed methods in Figures 79 to 81. Figure 79
details the inference times for MADE, while Figures 80 and 81 respectively illustrate the times for
PCs and PGMs. This comparison facilitates a direct evaluation of the computational efficiency across
different models.
43AOBB SSMPGUIDE SSMP
ITSELFGUIDE
ITSELF
Approachgrid40x40.f15 grid40x40.f10 grid40x40.f5 grid40x40.f2Dataset17.91 5.59 5.24 12.28 11.28
17.91 5.57 5.06 12.54 10.89
17.91 5.25 5.06 12.11 10.56
17.91 5.20 4.98 12.22 11.27Execution Time Comparison
6810121416
Log Execution TimeFigure 81: Heatmap depicting the inference time for PGM on a logarithmic microsecond scale, where
a lighter color denotes shorter (more favorable) durations.
Each cell displays the natural logarithm of the time, measured in microseconds, for each method and
dataset. Lighter colors indicate lower values. Notably, inferences using SSMP and GUIDE require
the shortest time, as these methods necessitate only a single forward pass through the neural network
to obtain the values for the query variables.
For MADE, the subsequent fastest method employs a model trained with GUIDE and conducts
inference using ITSELF, outperforming the approach that uses SSMP for training. This advantage
stems from the reduced number of ITSELF iterations required by GUIDE , benefiting from a more
effectively trained model. In PGMs, a similar pattern emerges with GUIDE + ITSELF as the next
fastest method, followed by SSMP + ITSELF. For PCs, MAX ranks as the next fastest, closely
followed by the GUIDE + ITSELF and SSMP + ITSELF methods. Finally, the ML and Seq methods
display the highest inference times.
Thus, if you require a highly efficient method capable of performing inference in a fraction of a
millisecond, GUIDE is the optimal choice. It outperforms the baseline for both MADE and PGMs.
However, if higher log-likelihood scores are necessary, GUIDE + ITSELF would be suitable, as it
generally surpasses the baselines in speed and performance across various scenarios.
E Gap Analysis For PGM
Table 2 presents the log-likelihood score gap between the neural network methods (SSMP, GUIDE ,
SSMP + ITSELF, GUIDE + ITSELF) and exact solutions. These exact solutions are obtained using
AOBB, which provides near-optimal results for smaller datasets. For each approach M, the gap is
calculated as the relative difference between the score of the near-optimal solution (determined by
AOBB) and the score achieved by M. This approach is feasible due to the use of small datasets,
allowing identification of exact solutions.
The final column highlights the neural-based approach achieving the best performance for each
dataset and query ratio combination. Notably, GUIDE and ITSELF consistently surpass other neural
baselines across almost all dataset-query pairs. This analysis provides a comprehensive assessment
of the proposed methods relative to exact solutions on small datasets, enabling a direct comparison of
their effectiveness.
44Table 2: Gap Between AOBB And Other Methods.
Method Query Ratio SSMP GUIDE SSMP + ITSELF GUIDE + ITSELF Best Method
Grids-17 0.900 0.082 0.060 0.069 0.075 GUIDE
Grids-17 0.800 0.051 0.038 0.040 0.035 GUIDE + ITSELF
Grids-17 0.700 0.042 0.030 0.034 0.016 GUIDE + ITSELF
Grids-17 0.500 0.026 0.024 0.024 0.007 GUIDE + ITSELF
Grids-18 0.900 0.081 0.062 0.071 0.102 GUIDE
Grids-18 0.700 0.033 0.027 0.024 0.015 GUIDE + ITSELF
Grids-18 0.500 0.020 0.018 0.018 0.006 GUIDE + ITSELF
Grids-18 0.800 0.054 0.035 0.045 0.037 GUIDE
Segmentation-14 0.500 0.032 0.032 0.032 0.004 GUIDE + ITSELF
Segmentation-14 0.900 0.045 0.014 0.014 0.005 GUIDE + ITSELF
Segmentation-14 0.800 0.051 0.024 0.024 0.006 GUIDE + ITSELF
Segmentation-14 0.700 0.029 0.029 0.029 0.005 GUIDE + ITSELF
Segmentation-15 0.800 0.046 0.002 0.002 0.002 GUIDE + ITSELF
Segmentation-15 0.500 0.003 0.003 0.003 0.000 GUIDE + ITSELF
Segmentation-15 0.900 0.675 0.255 0.433 0.305 GUIDE
Segmentation-15 0.700 0.003 0.003 0.003 0.002 GUIDE + ITSELF
F Log Likelihood Scores Comparison
This section compares log-likelihood scores across baselines, SSMP, SSMP + ITSELF, GUIDE
andGUIDE + ITSELF for all datasets and PMs. The log likelihood plots for NAMs are depicted
in Figures 82 to 101, while those for PCs are illustrated in Figures 102 to 121. Each bar represents
the mean log likelihood score of the corresponding method, with tick marks indicating the mean
±standard deviation. Higher values in these scores signify better performance by the method,
considering they represent log likelihood scores.
F.1 Scores for NAM
Figures 82 to 101 present the log likelihood scores for NAMs, illustrating the performance of ITSELF
inference and GUIDE training relative to other baselines. The heatmaps and contingency tables
discussed in the main paper corroborate the superior performance of GUIDE + ITSELF. These
visual representations allow for a comprehensive understanding of the performance of our methods
and baseline approaches, including HC and SSMP, across various datasets and query ratios.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−8.419−7.195−5.971−4.747−3.523Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.857−6.633−5.408−4.183−2.959Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.482−6.350−5.217−4.084−2.952Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.330−6.143−4.955−3.767−2.580Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 82: Log-Likelihood Scores on NLTCS for NAM. Higher Scores Indicate Better Performance.
F.2 Scores for PCs
Analyzing Figures 102 to 121, which focuses on PCs, reveals similar patterns. The neural-based
methods significantly outperform the MAX baseline. Among these, GUIDE + ITSELF surpasses
all other polynomial-time baselines and neural methods in over 80 percent of the experiments. This
demonstrates that ITSELF substantially enhances the chances of approaching optimal solutions by
performing test-time optimization. When comparing traditional inference with ITSELF, ITSELF
consistently proves superior. Moreover, GUIDE outperforms the other neural-based training methods
(SSMP).
45HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−8.205−7.080−5.955−4.830−3.705Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.279−6.288−5.297−4.306−3.315Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.071−6.080−5.090−4.099−3.108Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−5.432−4.824−4.216−3.607−2.999Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 83: Log-Likelihood Scores on for NAM. Higher Scores Indicate Better Performance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−1.952−1.492−1.032−0.572−0.112Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−3.989−2.977−1.964−0.9510.061Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−6.185−4.589−2.993−1.3960.200Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−8.053−5.916−3.779−1.6410.496Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 84: Log-Likelihood Scores on KDDCup2k for NAM. Higher Scores Indicate Better Perfor-
mance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−22.86−18.72−14.58−10.44−6.29Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−20.94−17.25−13.57−9.89−6.20Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−16.44−13.72−11.00−8.27−5.55Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−24.19−19.11−14.03−8.95−3.87Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 85: Log-Likelihood Scores on Plants for NAM. Higher Scores Indicate Better Performance.
46HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−37.47−33.76−30.05−26.33−22.62Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−35.16−30.96−26.77−22.57−18.38Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−70.85−55.69−40.52−25.36−10.19Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−88.63−67.43−46.24−25.04−3.84Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 86: Log-Likelihood Scores on Audio for NAM. Higher Scores Indicate Better Performance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−59.14−53.09−47.05−41.00−34.96Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−63.29−54.75−46.20−37.66−29.12Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−69.98−58.40−46.82−35.24−23.66Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−78.59−62.51−46.44−30.37−14.29Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 87: Log-Likelihood Scores on Jester for NAM. Higher Scores Indicate Better Performance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−56.71−52.49−48.27−44.05−39.83Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−53.77−49.31−44.84−40.38−35.92Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−53.74−48.32−42.91−37.49−32.07Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−58.34−49.95−41.57−33.19−24.80Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 88: Log-Likelihood Scores on Netflix for NAM. Higher Scores Indicate Better Performance.
47HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−56.46−47.41−38.35−29.30−20.25Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−60.81−51.25−41.69−32.13−22.57Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−54.83−46.66−38.49−30.32−22.15Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−58.09−49.64−41.20−32.75−24.31Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 89: Log-Likelihood Scores on Accidents for NAM. Higher Scores Indicate Better Perfor-
mance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−35.05−29.66−24.26−18.87−13.48Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−49.9−40.3−30.7−21.1−11.5Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−53.45−42.92−32.39−21.86−11.33Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−61.54−48.88−36.23−23.57−10.92Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 90: Log-Likelihood Scores on Mushrooms for NAM. Higher Scores Indicate Better Perfor-
mance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−93.75−73.02−52.29−31.56−10.84Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−80.11−63.90−47.70−31.49−15.29Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−83.53−67.19−50.85−34.51−18.17Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−101.40−80.10−58.81−37.51−16.22Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 91: Log-Likelihood Scores on Connect 4 for NAM. Higher Scores Indicate Better Perfor-
mance.
48HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−45.10−40.67−36.25−31.82−27.39Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−33.92−30.79−27.67−24.55−21.42Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−41.52−35.22−28.91−22.61−16.30Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−48.61−38.91−29.22−19.53−9.84Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 92: Log-Likelihood Scores on RCV-1 for NAM. Higher Scores Indicate Better Performance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−9.961−8.844−7.727−6.610−5.493Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−8.090−7.103−6.116−5.128−4.141Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.239−6.396−5.553−4.710−3.867Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−5.642−4.944−4.246−3.548−2.850Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 93: Log-Likelihood Scores on Retail for NAM. Higher Scores Indicate Better Performance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−98.45−92.07−85.70−79.32−72.95Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−94.21−87.31−80.41−73.51−66.61Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−94.30−86.55−78.79−71.04−63.28Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−99.69−89.16−78.63−68.10−57.57Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 94: Log-Likelihood Scores on DNA for NAM. Higher Scores Indicate Better Performance.
49HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−305.03−282.93−260.84−238.74−216.64Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−412.0−346.5−281.0−215.5−150.0Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−480.9−388.3−295.8−203.2−110.6Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−539.1−420.8−302.5−184.1−65.8Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 95: Log-Likelihood Scores on Movie reviews for NAM. Higher Scores Indicate Better
Performance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−94.32−72.30−50.28−28.26−6.24Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−202.9−150.2−97.5−44.78.0Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−244.4−180.3−116.2−52.112.0Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−272.1−200.1−128.2−56.215.8Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 96: Log-Likelihood Scores on Book for NAM. Higher Scores Indicate Better Performance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−170.11−145.46−120.82−96.18−71.54Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−308.0−240.4−172.7−105.1−37.5Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−365.5−279.1−192.7−106.3−19.9Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−441.7−331.6−221.5−111.3−1.2Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 97: Log-Likelihood Scores on WebKB for NAM. Higher Scores Indicate Better Performance.
50HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−212.0−167.5−123.1−78.7−34.2Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−367.9−278.2−188.6−99.0−9.3Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−427.2−319.8−212.3−104.92.5Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−464.1−345.4−226.6−107.810.9Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 98: Log-Likelihood Scores on Reuters-52 for NAM. Higher Scores Indicate Better Perfor-
mance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−488.6−373.1−257.7−142.2−26.8Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−600.9−450.7−300.5−150.3−0.1Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−642.3−478.0−313.7−149.514.8Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−511.5−381.6−251.8−121.97.9Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 99: Log-Likelihood Scores on 20 NewsGroup for NAM. Higher Scores Indicate Better
Performance.
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−3014−2205−1396−587222Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−6137−4478−2818−1159500Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−8340−6102−3864−1625613Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−1397.6−1218.6−1039.6−860.6−681.6Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 100: Log-Likelihood Scores on Ad for NAM. Higher Scores Indicate Better Performance.
51HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−243.55−218.74−193.94−169.13−144.33Query Ratio: 0.5
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−436.9−352.1−267.3−182.5−97.7Query Ratio: 0.7
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−504.3−399.4−294.6−189.7−84.9Query Ratio: 0.8
HC
SSMPGUIDESSMP
ITSELFGUIDEITSELF−574.7−441.2−307.8−174.4−40.9Query Ratio: 0.9Mean Log-Likelihood ScoreHC SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 101: Log-Likelihood Scores on BBC for NAM. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.182−6.582−5.983−5.383−4.784Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−6.830−6.246−5.663−5.079−4.495Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−6.617−5.942−5.267−4.592−3.918Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−6.487−5.705−4.923−4.140−3.358Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−6.232−5.517−4.802−4.087−3.372Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−5.121−4.576−4.032−3.487−2.943Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 102: Log-Likelihood Scores on NLTCS for PCs. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.134−6.710−6.285−5.861−5.436Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−6.273−5.899−5.524−5.150−4.776Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−5.514−5.159−4.804−4.449−4.095Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−4.660−4.388−4.116−3.844−3.573Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−4.1843−3.9694−3.7545−3.5396−3.3247Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−3.7986−3.6268−3.4549−3.2831−3.1113Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 103: Log-Likelihood Scores on for PCs. Higher Scores Indicate Better Performance.
52MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−2.614−2.026−1.438−0.850−0.262Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−2.209−1.764−1.320−0.876−0.432Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−1.865−1.449−1.033−0.617−0.201Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−1.612−1.277−0.942−0.606−0.271Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−0.4434553−0.4433311−0.4432069−0.4430828−0.4429586Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−0.9359−0.7724−0.6090−0.4455−0.2820Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 104: Log-Likelihood Scores on KDDCup2k for PCs. Higher Scores Indicate Better Perfor-
mance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−16.905−15.154−13.404−11.653−9.903Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−16.176−14.372−12.569−10.765−8.961Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−16.235−14.025−11.814−9.604−7.394Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−19.44−16.07−12.70−9.33−5.95Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−14.552−12.113−9.673−7.234−4.795Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−9.808−8.401−6.995−5.589−4.183Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 105: Log-Likelihood Scores on Plants for PCs. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−48.80−45.06−41.33−37.60−33.87Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−44.78−41.12−37.45−33.78−30.11Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−38.77−35.53−32.29−29.04−25.80Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−35.86−32.18−28.50−24.82−21.13Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−32.89−28.97−25.04−21.11−17.19Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−29.93−25.97−22.02−18.07−14.12Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 106: Log-Likelihood Scores on Audio for PCs. Higher Scores Indicate Better Performance.
53MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−61.793−59.448−57.102−54.756−52.410Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−58.230−55.837−53.444−51.051−48.658Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−55.76−52.46−49.15−45.85−42.55Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−47.777−45.285−42.792−40.299−37.807Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−46.36−43.25−40.13−37.01−33.89Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−43.47−40.02−36.56−33.11−29.65Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 107: Log-Likelihood Scores on Jester for PCs. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−60.766−59.834−58.901−57.968−57.035Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−59.628−58.066−56.505−54.943−53.382Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−57.736−55.640−53.543−51.447−49.351Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−53.759−51.686−49.612−47.539−45.465Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−50.756−48.915−47.074−45.233−43.392Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−50.588−48.211−45.835−43.459−41.083Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 108: Log-Likelihood Scores on Netflix for PCs. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−35.064−33.165−31.266−29.368−27.469Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−43.84−38.92−34.00−29.08−24.15Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−68.59−56.24−43.89−31.54−19.18Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−96.80−75.84−54.88−33.92−12.96Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−108.85−84.14−59.44−34.73−10.03Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−124.9−95.4−65.9−36.3−6.8Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 109: Log-Likelihood Scores on Accidents for PCs. Higher Scores Indicate Better Perfor-
mance.
54MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−19.548−17.516−15.484−13.452−11.420Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−24.19−20.80−17.41−14.01−10.62Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−40.09−32.22−24.36−16.50−8.63Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−44.64−35.34−26.04−16.74−7.44Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−44.37−35.11−25.84−16.58−7.31Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−36.87−29.42−21.97−14.52−7.07Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 110: Log-Likelihood Scores on Mushrooms for PCs. Higher Scores Indicate Better Perfor-
mance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−20.712−19.217−17.722−16.227−14.732Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−48.55−39.32−30.10−20.87−11.64Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−114.0−86.6−59.1−31.6−4.2Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−200.4−148.9−97.5−46.05.5Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−224.3−166.0−107.7−49.48.9Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−246.7−181.8−117.0−52.112.8Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 111: Log-Likelihood Scores on Connect 4 for PCs. Higher Scores Indicate Better Perfor-
mance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−61.07−56.92−52.77−48.62−44.46Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−53.33−49.56−45.79−42.02−38.25Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−44.40−41.11−37.83−34.54−31.25Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−35.10−32.28−29.45−26.63−23.80Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−30.357−27.876−25.395−22.915−20.434Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−24.871−22.876−20.881−18.886−16.892Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 112: Log-Likelihood Scores on RCV-1 for PCs. Higher Scores Indicate Better Performance.
55MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−14.01−12.60−11.19−9.78−8.37Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−12.213−10.948−9.683−8.418−7.153Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−9.822−8.792−7.762−6.731−5.701Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.891−6.995−6.099−5.203−4.306Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−7.255−6.445−5.635−4.824−4.014Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−5.402−4.849−4.296−3.743−3.189Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 113: Log-Likelihood Scores on Retail for PCs. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−86.507−84.967−83.426−81.886−80.345Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−99.57−93.64−87.71−81.79−75.86Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−116.95−105.63−94.30−82.97−71.64Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−133.58−116.89−100.20−83.51−66.82Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−134.32−116.60−98.88−81.17−63.45Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−144.86−123.69−102.52−81.35−60.19Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 114: Log-Likelihood Scores on DNA for PCs. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−382.72−361.85−340.99−320.12−299.26Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−390.8−356.8−322.8−288.8−254.8Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−395.5−348.7−301.9−255.1−208.3Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−400.0−339.9−279.8−219.7−159.6Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−402.4−335.7−269.1−202.4−135.7Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−404.6−331.0−257.5−183.9−110.4Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 115: Log-Likelihood Scores on Movie reviews for PCs. Higher Scores Indicate Better
Performance.
56MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−41.91−35.94−29.97−23.99−18.02Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−44.04−36.37−28.69−21.02−13.34Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−44.73−36.26−27.80−19.33−10.87Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−45.01−35.34−25.68−16.01−6.35Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−45.31−35.24−25.18−15.11−5.05Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−45.67−35.08−24.50−13.91−3.33Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 116: Log-Likelihood Scores on Book for PCs. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−188.27−169.18−150.09−131.00−111.91Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−191.53−167.16−142.80−118.43−94.07Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−193.9−163.2−132.5−101.8−71.0Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−196.2−159.3−122.4−85.5−48.5Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−196.7−157.4−118.1−78.7−39.4Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−197.9−155.6−113.4−71.1−28.8Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 117: Log-Likelihood Scores on WebKB for PCs. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−134.56−117.25−99.94−82.62−65.31Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−135.98−115.55−95.13−74.70−54.28Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−137.40−113.13−88.87−64.60−40.34Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−138.8−110.9−83.1−55.3−27.5Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−139.4−109.3−79.2−49.2−19.1Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−140.4−107.5−74.6−41.7−8.8Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 118: Log-Likelihood Scores on Reuters-52 for PCs. Higher Scores Indicate Better Perfor-
mance.
57MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−153.16−138.52−123.89−109.25−94.62Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−155.73−136.46−117.19−97.92−78.64Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−157.00−133.48−109.95−86.42−62.89Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−158.5−130.1−101.7−73.4−45.0Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−159.4−128.4−97.5−66.5−35.5Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−160.2−127.4−94.6−61.8−29.1Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELFFigure 119: Log-Likelihood Scores on 20 NewsGroup for PCs. Higher Scores Indicate Better
Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−43.06−36.09−29.12−22.16−15.19Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−44.02−36.44−28.85−21.26−13.68Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−44.56−36.31−28.05−19.79−11.53Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−42.21−33.94−25.66−17.38−9.10Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−42.39−33.41−24.43−15.45−6.47Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−42.48−33.03−23.59−14.14−4.69Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 120: Log-Likelihood Scores on Ad for PCs. Higher Scores Indicate Better Performance.
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−293.05−270.78−248.50−226.23−203.95Query Ratio: 0.1
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−297.3−265.7−234.1−202.5−170.9Query Ratio: 0.3
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−300.8−259.9−219.1−178.2−137.3Query Ratio: 0.5
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−304.4−253.5−202.6−151.7−100.9Query Ratio: 0.7
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−305.9−249.7−193.6−137.4−81.2Query Ratio: 0.8
MAXML Seq
SSMPGUIDESSMP
ITSELFGUIDEITSELF−307.9−246.2−184.5−122.8−61.0Query Ratio: 0.9Mean Log-Likelihood ScoreMAX ML Seq SSMPGUIDESSMP
ITSELFGUIDE
ITSELF
Figure 121: Log-Likelihood Scores on BBC for PCs. Higher Scores Indicate Better Performance.
58NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: [NA]
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: [NA]
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
59Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
60Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
61•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: [NA]
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Our paper improves the speed, accuracy and scalability of MPE inference
algorithms. Since algorithms already exist for solving MPE tasks, we do not perceive any
negative or positive societal impacts beyond what currently exists.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
62•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
63•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
64