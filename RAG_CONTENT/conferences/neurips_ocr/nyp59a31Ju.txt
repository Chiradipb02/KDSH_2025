Is Value Learning Really
the Main Bottleneck in Offline RL?
Seohong Park1Kevin Frans1Sergey Levine1Aviral Kumar2
1University of California, Berkeley2Carnegie Mellon University
seohong@berkeley.edu
Abstract
While imitation learning requires access to high-quality data, offline reinforcement
learning (RL) should, in principle, perform similarly or better with substantially
lower data quality by using a value function. However, current results indicate that
offline RL often performs worse than imitation learning, and it is often unclear
what holds back the performance of offline RL. Motivated by this observation, we
aim to understand the bottlenecks in current offline RL algorithms. While poor
performance of offline RL is typically attributed to an imperfect value function,
we ask: is the main bottleneck of offline RL indeed in learning the value function,
or something else? To answer this question, we perform a systematic empirical
study of (1) value learning, (2) policy extraction, and (3) policy generalization in
offline RL problems, analyzing how these components affect performance. We
make two surprising observations. First , we find that the choice of a policy
extraction algorithm significantly affects the performance and scalability of offline
RL, often more so than the value learning objective. For instance, we show that
common value-weighted behavioral cloning objectives ( e.g., AWR) do not fully
leverage the learned value function, and switching to behavior-constrained policy
gradient objectives ( e.g., DDPG+BC) often leads to substantial improvements in
performance and scalability. Second , we find that a big barrier to improving offline
RL performance is often imperfect policy generalization on test-time states out of
the support of the training data, rather than policy learning on in-distribution states.
We then show that the use of suboptimal but high-coverage data or test-time policy
training techniques can address this generalization issue in practice. Specifically,
we propose two simple test-time policy improvement methods and show that these
methods lead to better performance.
1 Introduction
Data-driven approaches that convert offline datasets of past experience into policies are a predominant
approach for solving control problems in several domains [ 9,49,51]. Primarily, there are two
paradigms for learning policies from offline data: imitation learning and offline reinforcement learning
(RL). While imitation requires access to high-quality demonstration data, offline RL loosens this
requirement and can learn effective policies even from suboptimal data, which makes offline RL
preferable to imitation learning in theory. However, recent results show that tuning imitation learning
by collecting more expert data often outperforms offline RL even when provided with sufficient data
in practice [36, 48], and it is often unclear what holds back the performance of offline RL.
The primary difference between offline RL and imitation learning is the use of a value function , which
is absent in imitation learning. The value function drives the learning progress of offline RL methods,
enabling them to learn from suboptimal data. Value functions are typically trained via temporal-
difference (TD) learning, which presents convergence [ 40,55] and representational [ 27,29,56]
pathologies. This has led to the conventional wisdom that the gap between offline RL and imitation is
a direct consequence of poor value learning [ 26,33,36]. Following up on this conventional wisdom,
recent research in the community has been devoted towards improving the value function quality
of offline RL algorithms [ 1,11,14,19,25,26]. While improving value functions will definitely
help improve performance, we question whether this is indeed the best way to maximally improve
38th Conference on Neural Information Processing Systems (NeurIPS 2024).the performance of offline RL, or if there is still headroom to get offline RL to perform better even
with current value learning techniques. More concretely, given an offline RL problem, we ask: is
the bottleneck in learning the value function, the policy, or something else? What is the best way to
improve performance given the bottleneck?
We answer these questions via an extensive empirical study. There are three potential factors that
could bottleneck an offline RL algorithm: ( B1) imperfect value function estimation, ( B2) imperfect
policy extraction guided by the learned value function, and ( B3) imperfect policy generalization to
states that it will visit during evaluation. While all of these contribute in some way to the performance
of offline RL, we wish to identify how each of these factors interact in a given scenario and develop
ways to improve them. To understand the effect of these factors, we use data size, quality, and
coverage as levers for systematically controlling their impacts, and study the “data-scaling” properties,
i.e., how data quality, coverage, and quantity affect these three aspects of the offline RL algorithm, for
three value learning methods and three policy extraction methods on diverse types of environments.
These data-scaling properties reveal how the performance of offline RL is bottlenecked in each
scenario, hinting at the most effective way to improve the performance.
Through our analysis, we make two surprising observations, which naturally provide actionable
advice for both domain-specific practitioners and future algorithm development in offline RL. First,
we find that the choice of a policy extraction algorithm often has a larger impact on performance
than value learning algorithms , despite the policy being subordinate to the value function in theory.
This contrasts with the common practice where policy extraction often tends to be an afterthought
in the design of value-based offline RL algorithms. Among policy extraction algorithms, we find
that behavior-regularized policy gradient ( e.g., DDPG+BC [ 14]) almost always leads to much
better performance and favorable data scaling than other widely used methods like value-weighted
regression ( e.g., AWR [ 46,47,58]). We then analyze why constrained policy gradient leads to better
performance than weighted behavioral cloning via extensive qualitative and quantitative analyses.
Second, we find that the performance of offline RL is often heavily bottlenecked by how well the
policy generalizes totest-time states, rather than its performance on training states. Namely, our
analysis suggests that existing offline algorithms are often already great at learning an optimal policy
from suboptimal data on in-distribution states, to the degree that it is saturated, and the performance is
often simply bottlenecked by the policy accuracy on novel states that the agent encounters at test time.
This provides a new perspective on generalization in offline RL, which differs from the previous focus
on pessimism and behavioral regularization. Based on this observation, we provide two practical
solutions to improve the generalization bottleneck: the use of high-coverage datasets and test-time
policy extraction techniques. In particular, we propose new on-the-fly policy improvement techniques
that further distill the information in the value function into the policy on test-time states during
evaluation rollouts , and show that these methods lead to better performance.
Our main contribution is an analysis of the bottlenecks in offline RL as evaluated via data-scaling
properties of various algorithmic choices. Contrary to the conventional belief that value learning is
the bottleneck of offline RL algorithms, we find that the performance is often limited by the choice
of a policy extraction objective and the degree to which the policy generalizes at test time. This
suggests that, with an appropriate policy extraction procedure ( e.g., gradient-based policy extraction)
and an appropriate recipe for handling generalization ( e.g., test-time training with the value function),
collecting more high-coverage data to train a value function is a universally better recipe for improving
offline RL performance, whenever the practitioner has access to collecting some new data for learning.
These results also imply that more research should be pursued in developing policy learning and
generalization recipes to translate value learning advances into performant policies.
2 Related work
Offline reinforcement learning [ 31,33] aims to learn a policy solely from previously collected
data. The central challenge in offline RL is to deal with the distributional shift in the state-action
distributions of the dataset and the learned policy. This shift could lead to catastrophic value
overestimation if not adequately handled [ 33]. To prevent such a failure mode, prior works in offline
RL have proposed diverse techniques to estimate more suitable value functions solely from offline data
via conservatism [ 8,26], out-of-distribution penalization [ 14,53,59], in-sample maximization [ 17,
25,61], uncertainty minimization [ 1,19,60], convex duality [ 32,41,50], or contrastive learning [ 11].
Then, these methods train policies to maximize the learned value function with behavior-regularized
policy gradient ( e.g., DDPG+BC) [ 14,34], weighted behavioral cloning ( e.g., AWR) [ 46,47], or
sampling-based action selection ( e.g., SfBC) [ 7,15,21]. Depending on the algorithm, these value
2learning and policy extraction stages can either be interleaved [ 14,26,42] or decoupled [ 5,11,17,25].
Despite the presence of a substantial number of offline RL algorithms, relatively few works have
aimed to analyze and understand the practical challenges in offline RL. Instead of proposing a new
algorithm, we mainly aim to understand the current bottlenecks in offline RL via a comprehensive
analysis of existing techniques so that we can inform future methodological development.
Several prior works have analyzed individual components of offline RL or imitation learning algo-
rithms: value bootstrapping [ 14,15], representation learning [ 27,29,62], data quality [ 4], differences
between RL and behavioral cloning (BC) [ 28], and empirical performance [ 10,23,35,36,54]. Our
analysis is distinct from these lines of work: we analyze challenges appearing due to the interaction
between these individual components of value function learning, policy extraction, and generaliza-
tion, which allows us to understand the bottlenecks in offline RL from a holistic perspective. This can
inform how a practitioner could extract the most by improving one or more of these components, de-
pending upon their problem. Perhaps the closest study to ours is Fu et al. [13], which study whether
representations, value accuracy, or policy accuracy can explain the performance of offline RL. While
this study makes insightful recommendations about which algorithms to use and reveals the potential
relationships between some metrics and performance, the conclusions are only drawn from D4RL
locomotion tasks [ 12], which are known to be relatively simple and saturated [ 48,53], and the data-
scaling properties of algorithms are not considered. In addition, this prior study does not identify
policy generalization , which we find to be one of the most substantial yet overlooked bottlenecks in
offline RL. In contrast, we conduct a large-scale analysis on diverse environments ( e.g., pixel-based,
goal-conditioned, and manipulation tasks) and analyze the bottlenecks in offline RL with the aim of
providing actionable takeaways that can enhance the performance and scalability of offline RL.
3 Main hypothesis
Our primary goal is to understand when and how the performance of offline RL can be bottlenecked
in practice. As discussed earlier, there exist three potential factors that could bottleneck an offline RL
algorithm: ( B1) imperfect value function estimation from data, ( B2) imperfect policy extraction from
the learned value function, and ( B3) imperfect generalization on the test-time states that the policy
visits in evaluation rollouts. We note that the bottleneck of an offline RL algorithm under a certain
dataset can always be attributed to one or some of these factors, since the policy will attain optimal
performance if both value learning and policy extraction are perfect, and perfect generalization to
test-time states is possible.
Our main hypothesis in this work is that, somewhat contrary to the prior belief that the accuracy of
the value function is the primary factor limiting performance of offline RL methods, policy learning
is often the main bottleneck of offline RL . In other words, while value function accuracy is certainly
important, how the policy is extracted from the value function ( B2) and how well the agent generalizes
on states that it visits at the deployment time ( B3) are often the main factors that significantly affect
both the performance and scalability of offline RL. To verify this hypothesis, we conduct two main
analyses in this paper. In Section 4, we compare the effects of value learning and policy extraction on
performance under various types of environments, datasets, and algorithms ( B1andB2). In Section 5,
we analyze the degree to which the policy generalizes on test-time states affects performance ( B3).
4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)
We first perform controlled experiments to identify whether imperfect value functions ( B1) or
imperfect policy extraction ( B2) contribute more to holding back the performance of offline RL in
practice. To systematically compare value learning and policy extraction, we run different algorithms
while varying the the amounts of data for value function training and policy extraction, and draw
data-scaling matrices to visualize the aggregated results. Increasing the amount of data provides
a convenient lever to control the effect of each component, enabling us to draw conclusions about
whether the value or the policy serves as a bigger bottleneck in different regimes when different
amounts of training data are available (or can be collected by a practitioner for a given problem), and
to understand the differences between various algorithms.
To clearly dissect value learning from policy learning, we focus on offline RL methods with decoupled
value and policy training phases ( e.g., One-step RL [ 5], IQL [ 25], CRL [ 11], etc.), where policy
learning does not affect value learning. In other words, we focus on methods that first train a value
function without involving policies, and then extract a policy from the learned value function with a
separate objective. While this might sound a bit restrictive, we surprisingly find that policy learning
3is often the main bottleneck even in these decoupled methods , which attempt to solve a simple, single-
step optimization problem for extracting a policy given a static and stationary value function.
4.1 Analysis setup
We now introduce the value learning objectives, policy extraction objectives, and environments that
we study in our analysis (see Appendix B for preliminaries).
Value learning objectives. We consider three decoupled value learning objectives that fit value
functions without involving policy learning: (1) implicit Q-learning (IQL) [25],(2) SARSA [5], and
(3) contrastive RL (CRL) [11]. IQL fits an optimal Q function ( Q∗) by approximating the Bellman
optimality operator with an expectile loss. SARSA fits a behavioral Q function ( Qβ) using the Bellman
evaluation operator. In goal-conditioned tasks, we employ CRL instead of SARSA, which similarly
fits a behavioral Q function, but with a different contrastive learning-based objective that leads to better
performance. We refer to Appendix D.1 for detailed descriptions of these value learning methods.
Policy extraction objectives. Prior works in offline RL typically use one of the following objectives
to extract a policy from the value function. All of them are built upon the same principle: maximizing
values while being close to the behavioral policy, to avoid the exploitation of erroneous critic values.
•(1) Weighted behavioral cloning ( e.g., A WR). Weighted behavioral cloning is one of the most
widely used offline policy extraction objectives for its simplicity [ 25,42,44,46,47,58]. Among
weighted behavioral cloning methods, we consider advantage-weighted regression (AWR [ 46,47])
in this work, which maximizes the following objective:
max
πJAWR(π) =Es,a∼D[eα(Q(s,a)−V(s))logπ(a|s)], (1)
where αis an (inverse) temperature hyperparameter. Intuitively, AWR assigns larger weights to
higher-advantage transitions when cloning behaviors, which makes the policy selectively copy
only good actions from the dataset.
•(2) Behavior-constrained policy gradient ( e.g., DDPG+BC). Another popular policy extraction
objective is behavior-constrained policy gradient, which directly maximizes Q values while not
deviating far away from the behavioral policy [ 1,14,19,26,59]. In this work, we consider the ob-
jective that combines deep deterministic policy gradient and behavioral cloning (DDPG+BC [ 14]):
max
πJDDPG+BC (π) =Es,a∼D[Q(s, µπ(s)) +αlogπ(a|s)], (2)
where µπ(s) =Ea∼π(·|s)[a]andαis a hyperparameter that controls the strength of the BC
regularizer.
•(3) Sampling-based action selection ( e.g., SfBC). Instead of learning an explicit policy, some
previous methods implicitly define a policy as the action with the highest value among action
samples from the behavioral policy [ 7,15,18,21]. In this work, we consider the following
objective that selects the arg max action from behavioral candidates (SfBC [7]):
π(s) = arg max
a∈{a1,...,aN}[Q(s, a)], (3)
where a1, . . . , a Nare sampled from the learned BC policy πβ(· |s)[7, 21].
Environments and datasets. To understand how different value learning and policy extraction ob-
jectives affect performance and data scalability, we consider eight environments (Figure 10) across
state- and pixel-based, robotic locomotion and manipulation, and goal-conditioned and single-task
settings with varying levels of data suboptimality: (1)gc-antmaze-large ,(2)antmaze-large ,(3)
d4rl-hopper ,(4)d4rl-walker2d ,(5)exorl-walker ,(6)exorl-cheetah ,(7)kitchen , and
(8)gc-roboverse . We highlight some features of these tasks: exorl-{walker, cheetah} are
tasks with highly suboptimal, diverse datasets collected by exploratory policies, gc-antmaze-large
andgc-roboverse are goal-conditioned (‘ gc-’) tasks, and gc-roboverse is apixel-based robotic
manipulation task with a 48×48×3-dimensional observation space. For some tasks ( e.g.,
gc-antmaze-large andkitchen ), we additionally collect data to enhance dataset sizes to depict
scaling properties clearly. We refer to Appendix D.2 for the complete task descriptions.
4.2 Results: Policy extraction mechanisms substantially affect data-scaling trends
Figure 1 shows the data-scaling matrices of three policy extraction algorithms (AWR, DDPG+BC,
and SfBC) and three value learning algorithms (IQL and {SARSA or CRL}) on eight environments,
aggregated from a total of 15,488runs ( 8seeds for each cell, numbers after “ ±” denote standard
40.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10MPolicy Data22262727565958605357656358606161
±7±7±5±5±10±8±8±5±6±10±9±13±12±11±10±8IQL +AWR
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
48585557536160615153606456626259
±13±4±7±9±7±10±7±8±6±13±8±4±18±12±7±11IQL +DDPG
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
30364645526061606266696667697473
±7±9±9±10±6±6±5±6±8±6±3±5±9±7±5±5IQL +SfBC
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10MPolicy Data1111363936414543516252566074
±1±1±1±1±8±4±6±8±8±13±11±8±14±19±16±7CRL +AWR
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
381941505156736575778872818789
±2±5±9±21±7±12±8±13±8±5±7±5±5±5±4±3CRL +DDPG
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
12617404959626370757368748280
±1±2±6±8±9±9±8±15±9±9±5±15±18±3±6±6CRL +SfBCgc-antmaze-large
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 0 0 23 7 189 27 38
±0±1±2±3±8±8±4±14±7IQL +AWR
0.1M 0.3M 1M
Value Data0.1M0.3M1M
2 9 284 12 490 7 43
±4±10±5±6±17±7±0±4±19IQL +DDPG
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 16 254 23 503 48 45
±1±17±8±4±21±18±2±14±13IQL +SfBC
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0SARSA +AWR
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0SARSA +DDPG
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0SARSA +SfBCantmaze-large
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 49 52 5252 56 5552 53 55
±4±3±4±4±4±2±4±1±4IQL +AWR
0.1M 0.3M 1M
Value Data0.1M0.3M1M
55 50 5054 51 5255 52 52
±11±10±3±11±3±3±8±3±4IQL +DDPG
0.1M 0.3M 1M
Value Data0.1M0.3M1M
41 43 4443 42 4243 41 44
±5±4±3±4±4±4±4±2±5IQL +SfBC
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 54 53 5555 59 5755 57 60
±4±2±3±3±3±2±2±5±3SARSA +AWR
0.1M 0.3M 1M
Value Data0.1M0.3M1M
57 60 6061 59 6860 64 64
±10±9±10±7±6±8±12±8±12SARSA +DDPG
0.1M 0.3M 1M
Value Data0.1M0.3M1M
46 55 5447 52 5345 51 52
±2±4±5±6±8±6±5±3±6SARSA +SfBCd4rl-hopper
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 66 69 7470 74 7672 76 77
±4±2±1±3±3±2±2±2±2IQL +AWR
0.1M 0.3M 1M
Value Data0.1M0.3M1M
76 80 8280 81 8680 81 86
±3±2±2±2±2±2±4±2±3IQL +DDPG
0.1M 0.3M 1M
Value Data0.1M0.3M1M
59 67 7759 68 7859 70 80
±6±3±4±4±5±4±3±5±3IQL +SfBC
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 71 79 8274 79 8376 81 83
±3±3±1±2±2±1±2±1±1SARSA +AWR
0.1M 0.3M 1M
Value Data0.1M0.3M1M
81 84 8582 84 8682 85 85
±2±1±0±1±1±1±1±1±1SARSA +DDPG
0.1M 0.3M 1M
Value Data0.1M0.3M1M
74 84 8575 83 8574 84 86
±3±1±0±2±1±0±3±1±0SARSA +SfBCd4rl-walker2d
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10MPolicy Data767881868385891018991971249699114191
±1±0±1±1±1±1±1±2±2±1±1±1±1±1±2±8IQL +AWR
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
7696127180861432203529414525039897131241425
±1±8±6±67±1±2±12±16±1±3±8±25±1±12±11±13IQL +DDPG
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
75768296858911525288951383398997151368
±1±1±1±2±2±1±2±3±1±1±2±6±1±1±4±10IQL +SfBC
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10MPolicy Data75767982838489958991981149698113146
±1±1±1±1±1±1±1±1±1±1±1±1±2±1±2±4SARSA +AWR
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
7597188251861632522969414525134798125270351
±1±45±13±13±2±10±13±13±2±9±15±11±1±13±22±11SARSA +DDPG
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
7577861058590122191889714424289100156256
±1±1±1±1±1±1±2±4±1±1±1±4±1±1±2±6SARSA +SfBCexorl-walker
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10MPolicy Data35425864364774904257841194082109153
±2±6±3±2±3±6±4±4±4±14±4±4±3±16±4±6IQL +AWR
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
214375142246510320322841312351967139245
±7±8±9±7±13±16±5±8±8±9±9±10±5±24±9±18IQL +DDPG
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
3235436437447013340491031903655109198
±2±2±1±5±3±7±2±5±2±9±3±6±2±13±4±8IQL +SfBC
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10MPolicy Data384351575854697772748610891103119153
±3±3±2±1±2±3±3±3±3±2±3±6±7±7±5±2SARSA +AWR
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
755391157901021231809412715222498127159250
±11±5±13±7±8±8±4±7±5±9±6±12±7±6±6±11SARSA +DDPG
0.1M 0.3M 1M 10M
Value Data0.1M0.3M1M10M
3839446251587411872801091877682114216
±2±3±2±4±3±4±4±4±3±3±4±4±3±3±7±6SARSA +SfBCexorl-cheetah
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 44 50 7080 91 9397 98 97
±3±4±3±6±3±3±2±1±2IQL +AWR
0.1M 0.3M 1M
Value Data0.1M0.3M1M
67 72 8080 95 9888 98 99
±7±8±3±7±3±1±11±2±1IQL +DDPG
0.1M 0.3M 1M
Value Data0.1M0.3M1M
55 63 7768 82 9266 80 90
±4±4±3±4±2±2±2±3±4IQL +SfBC
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 40 48 6774 93 9596 97 98
±4±3±3±6±3±2±2±2±1SARSA +AWR
0.1M 0.3M 1M
Value Data0.1M0.3M1M
59 71 8074 93 9780 99 98
±5±6±4±6±4±2±8±2±2SARSA +DDPG
0.1M 0.3M 1M
Value Data0.1M0.3M1M
55 60 7469 79 9364 77 90
±3±6±5±2±3±1±2±5±2SARSA +SfBCkitchen
0.1M 1M
Value Data0.1M1MPolicy Data17 1730 27
±3 ±3±3 ±4IQL +AWR
0.1M 1M
Value Data0.1M1M
15 1723 23
±7 ±7±2 ±2IQL +DDPG
0.1M 1M
Value Data0.1M1M
10 1017 18
±4 ±4±3 ±4IQL +SfBC
0.1M 1M
Value Data0.1M1MPolicy Data8 620 18
±4 ±5±4 ±5CRL +AWR
0.1M 1M
Value Data0.1M1M
9 723 24
±3 ±3±5 ±5CRL +DDPG
0.1M 1M
Value Data0.1M1M
10 820 22
±3 ±2±5 ±3CRL +SfBCgc-roboverseFigure 1: Data-scaling matrices of three policy extraction methods (A WR, DDPG+BC, and SfBC) and
three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a
bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color
gradients (
 ,
,
) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.
Table 1: DDPG+BC is often the best policy extraction method. We aggregate the performances over the
entire data-scaling matrix and then over 8random seeds in each setting. Scores at or above 95% of the best
score are highlighted in bold. The table shows that DDPG+BC is better than or as good as AWR in 15out of 16
settings . We note that policy extraction hyperparameters are individually tuned for each setting (Figure 11).
Task (Value Algorithm) A WR DDPG+BC SfBC
gc-antmaze-large (IQL) 51±2 58±258±1
gc-antmaze-large (CRL) 37±2 58±251±2
antmaze-large (IQL) 12±2 17±424±3
antmaze-large (SARSA) 0±0 0±0 0±0
kitchen (IQL) 80±1 86±175±1
kitchen (SARSA) 79±1 83±173±1
exorl-walker (IQL) 99±1 191±6140±1
exorl-walker (SARSA) 94±0 193±5125±1Task (Value Algorithm) A WR DDPG+BC SfBC
exorl-cheetah (IQL) 71±1 101±277±2
exorl-cheetah (SARSA) 78±1 131±389±1
d4rl-hopper (IQL) 53±1 52±343±1
d4rl-hopper (SARSA) 56±1 61±350±2
d4rl-walker2d (IQL) 73±1 81±168±1
d4rl-walker2d (SARSA) 79±0 84±081±1
gc-roboverse (IQL) 23±2 20±214±2
gc-roboverse (CRL) 13±1 16±215±1
deviations). In each matrix, we individually tune the hyperparameter for policy extraction ( αorN)
for each entry. These matrices show how performance varies with different amounts of data for the
value and the policy. In our analysis, we specifically focus on the color gradients of these matrices,
which reveal the main limiting factor behind the performance of offline RL in each setting. Note that
the color gradients are mostly either vertical, horizontal, or diagonal. Vertical (
 ) color gradients
indicate that the performance is most strongly affected by the amount of policy data, horizontal (
 )
gradients indicate it is mostly affected by value data, and diagonal (
 ) gradients indicate both.
Side-by-side comparisons of the data-scaling matrices from different policy extraction methods in
Figure 1 suggest that, perhaps surprisingly, different policy extraction algorithms often lead to
significantly different performance and data-scaling behaviors, even though they extract policies
from the same value function (recall that the use of decoupled algorithms allows us to train a single
value function, but use it for policy extraction in different ways). For example, on exorl-walker and
exorl-cheetah , AWR performs remarkably poorly compared to DDPG+BC or SfBC on both value
learning algorithms. Such a performance gap between policy extraction algorithms exists even when
the value function is far from perfect, as can be seen in the low-data regimes in gc-antmaze-large
andkitchen . In general, we find that the choice of a policy extraction procedure affects performance
5BCRL0.1M0.3M1M10MValue Data0.1M0.3M1M10MPolicy Data1101383532332840382958604845
±1±1±1±1±6±4±6±7±6±10±8±5±12±11±15±12AWR(Æ=0.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M8748454442403443383839283637
±6±3±3±2±4±5±7±9±9±9±9±10±10±9±10±12AWR(Æ=1.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M22262525565954565357616354566161
±7±7±7±8±10±8±6±12±6±10±7±13±13±8±10±8AWR(Æ=3.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M19212727525558605256656143506155
±5±7±5±5±4±5±8±5±7±9±9±7±11±9±12±8AWR(Æ= 10.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10MPolicy Data19181719525148614545434656625858
±10±6±7±9±12±12±11±5±11±9±13±10±18±17±19±20DDPG(Æ=3.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M48585557536160615153566454626259
±13±4±7±9±7±10±7±8±6±13±7±4±5±12±7±11DDPG(Æ=1.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M20365149125153461649604921545352
±12±10±7±8±5±11±5±14±10±13±8±5±13±12±12±9DDPG(Æ=0.3)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M0107309146010138061212
±0±7±8±2±0±6±12±6±0±8±8±7±0±4±6±4DDPG(Æ=0.1)gc-antmaze-large(IQL)
0.1M0.3M1M10MValue Data0.1M0.3M1M10MPolicy Data73737373818182818687878789898889
±1±1±1±1±1±1±1±1±1±1±1±1±2±1±1±1AWR(Æ=0.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M7373737482818384899090969599108111
±1±1±0±1±1±1±1±1±2±1±1±2±2±1±3±2AWR(Æ=1.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M767881868285891018890971249698114191
±1±0±1±1±2±1±1±2±1±1±1±1±1±1±2±8AWR(Æ= 10.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M76767984838488968991971209499112171
±1±1±1±1±1±1±1±1±1±1±1±2±2±1±3±5AWR(Æ= 100.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10MPolicy Data7677828286921021049410413515397113206267
±1±1±1±2±1±1±1±1±1±2±2±3±1±2±5±5DDPG(Æ=1.0)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M5966809573991352427711521237163117241397
±3±4±3±3±6±3±4±12±7±6±14±12±17±11±11±18DDPG(Æ=0.1)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M5275105143731181642806912722339854117234425
±14±7±4±8±7±3±9±14±12±5±14±25±15±11±19±13DDPG(Æ=0.01)
0.1M0.3M1M10MValue Data0.1M0.3M1M10M6996127180821432203528314525039274131226412
±8±8±6±67±9±2±12±16±10±3±8±25±11±12±17±21DDPG(Æ=0.0)exorl-walker(IQL)
BCRLFigure 2: Data-scaling matrices of A WR and DDPG+BC with different BC strengths ( α).In
gc-antmaze-large , AWR is always policy-bounded (
 ), but DDPG+BC has both policy-bounded (
 ) and
value-bounded (
 ) modes, depending on the value of α. Notably, an in-between value of α= 1.0in DDPG+BC
leads to the best of both worlds (see the bottom left corner of gc-antmaze-large with 0.1M datasets)!
often more than the choice of a value learning objective except antmaze-large , where the value
function must be learned from sparse-reward, suboptimal datasets with long-horizon trajectories.
Among policy extraction algorithms, we find that DDPG+BC almost always achieves the best
performance and scaling behaviors across the board , followed by SfBC, and the performance of
AWR falls significantly behind the other two extraction algorithms in many cases (Table 1). Notably,
the data-scaling matrices of AWR always have vertical (
 ) or diagonal (
 ) color gradients, implying
that it does not fully utilize the value function (see Section 4.3 for clearer evidence). In other words, a
non-careful choice of the policy extraction algorithm ( e.g., weighted behavioral cloning) hinders the
use of learned value functions, imposing an unnecessary bottleneck on the performance of offline RL.
4.3 Deep dive 1: How different are the scaling properties of A WR and DDPG+BC?
To gain further insights into the difference between value-weighted behavioral cloning ( e.g., AWR)
and behavior-regularized policy gradient ( e.g., DDPG+BC), we draw data-scaling matrices with
different values of α(in Equations (1) and (2)), a hyperparameter that interpolates between RL and BC.
Note that α= 0corresponds to BC in AWR and α=∞corresponds to BC in DDPG+BC. We recall
that the previous results (Figure 1) use the best temperature for each matrix entry ( i.e., aggregated by
the maximum over temperatures), but here we show the full results with individual hyperparameters.
Figure 2 highlights the results on gc-antmaze-large andexorl-walker (see Appendix E for the
full results). The results on gc-antmaze-large show a clear difference in scaling matrices between
AWR and DDPG+BC. That is, AWR is always policy-bounded regardless of the BC strength α(i.e.,
vertical (
 ) color gradients), whereas DDPG+BC has two “modes”: it is policy-bounded (
 ) when α
is large, and value-bounded (
 ) and when αis small. Intriguingly, an in-between value of α= 1.0
in DDPG+BC enables having the best of both worlds, significantly boosting performances across
the entire matrix (note that it achieves very strong performance even with a 0.1M-sized dataset)!
This difference in scaling behaviors suggests that the use of the learned value function in weighted
behavioral cloning is limited. This becomes more evident in exorl-walker (Figure 2), where AWR
fails to achieve strong performance even with a very high temperature value ( α= 100 ).
4.4 Deep dive 2: Why is DDPG+BC better than A WR?
We have so far seen several empirical results that suggest behavior-regularized policy gradient ( e.g.,
DDPG+BC) should be preferred to weighted behavioral cloning ( e.g., AWR) in any case. What
makes DDPG+BC so much better than AWR? There are three potential reasons.
−1 0 1
Dimension 1−101Dimension 2AWR
−1 0 1
Dimension 1−101DDPG+BC
Figure 3: A WR vs. DDPG actions.First, AWR only has a mode-covering weighted behavioral
cloning term, while DDPG+BC has both mode-seeking first-order
value maximization and mode-covering behavioral cloning terms.
As a result, actions learned by AWR always lie within the con-
vex hull of dataset actions, whereas DDPG+BC can “hillclimb”
the learned value function, even allowing extrapolation to some
degree while not deviating too far away from the mode. This not
only enables a better use of the value function but produces a
wider range of actions. To illustrate this, we plot test-time action
sampled from policies learned by AWR and DDPG+BC on exorl-walker . Figure 3 shows that
AWR actions are relatively centered around the origin, while DDPG+BC actions are more spread out,
which can sometimes help achieve an even higher degree of optimality.
60500K1M010Policy LossAWR(Æ=0.0)0500K1M050AWR(Æ=3.0)
0500K1MGradient Steps050Policy LossDDPG(Æ=3.0)0500K1MGradient Steps02040DDPG(Æ=0.3)°0.04°0.020.000.020.04x°0.04°0.020.000.020.04y
TrainingValidationFigure 4: A WR overfits.Second, value-weighted behavioral cloning uses a much smaller
number of effective samples than behavior-regularized policy
gradient methods, especially when the temperature ( α) is large.
This is because a small number of high-advantage transitions
can potentially dominate learning signals for AWR ( e.g., a single
transition with a weight of e10can dominate other transitions with
smaller weights like e2). As a result, AWR effectively uses only
a fraction of datapoints for policy learning, being susceptible to
overfitting. On the other hand, DDPG+BC is based on first-order
maximization of the value function without any weighting, and
thus is free from such an issue. Figure 4 illustrates this, where
we compare the training and validation policy losses of AWR
and DDPG+BC on gc-antmaze-large with the smallest 0.1M
dataset ( 8seeds). The results show that AWR with a large temperature ( α= 3.0) causes severe
overfitting. Indeed, Figure 1 shows DDPG+BC often achieves significantly better performance than
AWR in low-data regimes.
Third, AWR has a theoretical pathology in the regime with limited samples: since the coefficient
multiplying logπ(a|s)in the AWR objective (Equation (1)) is always positive, AWR can increase
the likelihood of alldataset actions, regardless of how optimal they are. If the training dataset covers
all possible actions, then the condition for normalization of the probability density function of π(a|s)
would alleviate this issue, but this coverage assumption is rarely achieved in practice. Under limited
data coverage, and especially when the policy network is highly expressive and dataset states are
unique ( e.g., continuous control problems), AWR can in theory memorize all state-action pairs in the
dataset, potentially reverting to unweighted behavioral cloning.
Takeaway: Current policy extraction can inhibit effective use of the value function.
Donotuse value-weighted behavior cloning ( e.g., AWR); use behavior-constrained policy
gradient ( e.g., DDPG+BC), regardless of the value learning objective. This enables better
scaling of performance with more data and better use of the value function.
5 Empirical analysis 2: Policy generalization (B3)
We now turn our focus to the third hypothesis, that the degree to which the agent generalizes to
states that it visits at the evaluation time has a significant impact on performance. This is a unique
bottleneck to the offline RL problem setting, where the agent encounters new, potentially out-of-
distribution states at test time.
5.1 Analysis setup
To understand this bottleneck concretely, we first define three key metrics quantifying a notion of
accuracy of a given policy in terms of distances against the optimal policy. Specifically, we use the
following mean squared error (MSE) metrics to quantify policy accuracy:
DtrainSpπDTrainingDvalValidationEvaluation
Figure 5: Three distributions for the MSE metrics.
(Training MSE ) =Es∼Dtrain[(π(s)−π∗(s))2], (4)
(Validation MSE ) =Es∼Dval[(π(s)−π∗(s))2], (5)
(Evaluation MSE ) =Es∼pπ(·)[(π(s)−π∗(s))2], (6)
whereDtrain andDvalrespectively denote the training and validation datasets, π∗denotes an optimal
policy, which we assume access to for evaluation and visualization purposes only. Validation MSE
70 1M 2M050100Returnantmaze-medium
0 1M 2M050antmaze-large
0 1M 2M050kitchen-mixed
0 1M 2M050adroit-pen
0 1M 2M0510adroit-hammer
0 1M 2M05adroit-door
0 1M 2M0.00.10.2Eval. MSE
0 1M 2M0.00.10.2
0 1M 2M0.00.10.20.3
0 1M 2M1.21.31.4
0 1M 2M0.150.20
0 1M 2M0.200.250.300.35
0 1M 2M0.00.10.2Val. MSE
0 1M 2M0.00.10.2
0 1M 2M0.00.10.20.3
0 1M 2M1.21.31.4
0 1M 2M0.150.20
0 1M 2M0.200.250.300.35
0 1M 2M
Gradient Steps0.00.10.2Train. MSE
0 1M 2M
Gradient Steps0.00.10.2
0 1M 2M
Gradient Steps0.00.10.20.3
0 1M 2M
Gradient Steps1.21.31.4
0 1M 2M
Gradient Steps0.150.20
0 1M 2M
Gradient Steps0.200.250.300.35Figure 6: How do offline RL policies improve with additional interaction data? In many environments,
offline-to-online RL only improves evaluation MSEs, while validation MSEs and training MSEs often remain
completely flat (see Section 5 for the definitions of these metrics). This suggests that current offline RL algorithms
may already be great at learning an effective policy on in-distribution states, and the performance of offline RL
is often mainly determined by how well the policy generalizes on its own state distribution at test time.
measures the policy accuracy on states sampled from the same dataset distribution as the training
distribution ( i.e., in-distribution MSE, Figure 5), while evaluation MSE measures the policy accuracy
on states the agent visits at test time, which can potentially be very different from the dataset
distribution ( i.e., out-of-distribution MSE, Figure 5). We note that, while these metrics might not
always be perfectly indicative of the performance of a policy (see Appendix A), they serve as
convenient proxies to estimate policy accuracy in many continuous-control domains in practice.
One way to measure the degree to which test-time generalization affects performance is to evaluate
how much room there is for various policy MSE metrics to improve when further training on
additional policy rollouts is allowed. The distribution of states induced by rolling out the policy is an
ideal distribution to improve performance, as the policy receives direct feedback on its own actions at
the states it would visit. Hence, by tracking the extent to which various MSEs improve and how their
predictive power towards performance evolves over online interaction, we will be able to understand
which is a bigger bottleneck: in-distribution generalization ( i.e., improvements towards validation
MSE under the offline dataset distribution) or out-of-distribution generalization ( i.e., improvements
in evaluation MSE under the on-policy state distribution). To this end, we measure these three types
of MSEs over the course of online interaction, when learning from a policy trained on offline data
only ( i.e., the offline-to-online RL setting). Specifically, we train offline-to-online IQL agents on six
D4RL [ 12] tasks ( antmaze-{medium, large} ,kitchen , and adroit-{pen, hammer, door} ),
and measure the MSEs with pre-trained expert policies that approximate π∗(see Appendix D.4).
5.2 Results: Test-time generalization is often the main bottleneck in offline RL
Figure 6 shows the results ( 8seeds with 95% confidence intervals), where we denote online training
steps in red. The results show that, perhaps surprisingly, in many environments continued training
with online interaction only improves evaluation MSEs, while training and validation MSEs often
remain completely flat during online training. Also, we can see that the evaluation MSE is the most
predictive of the performance of offline RL among the three metrics. In other words, the results show
that, despite the fact that on-policy data provides for an oracle distribution to improve policy accuracy,
performance improvement is often only reflected in the evaluation MSEs computed under the policy’s
own state distribution.
What does this tell us? This indicates that, current offline RL methods may already be sufficiently
great at learning the best possible policy within the distribution of states covered by the offline dataset ,
andthe agent’s performance is often mainly determined by how well it generalizes under its
own state distribution at test time , as suggested by the fact that evaluation MSE is most predictive
of performance. This finding somewhat contradicts prior beliefs: while algorithmic techniques in
offline RL largely attempt to improve policy optimality on in-distribution states (by addressing the
issue with out-of-distribution actions ), our results suggest that modern offline RL algorithms may
8Low coverageHigh optimality0.1M1M10MValue Data0.1M1M10MPolicy Data193129535758576257
±7±12±12±8±8±6±9±8±4AWR(ædata=0.0)
0.1M1M10MValue Data0.1M1M10M192220657269676869
±8±8±6±5±7±6±15±8±7AWR(ædata=0.4)
0.1M1M10MValue Data0.1M1M10M344550667081838582
±4±8±7±15±7±5±16±7±7AWR(ædata=0.7)
0.1M1M10MValue Data0.1M1M10M376416055938994
±2±4±3±15±17±20±6±5±3AWR(ædata=1.0)
0.1M1M10MValue Data0.1M1M10MPolicy Data546458456060525965
±10±9±11±5±6±6±16±14±9DDPG(ædata=0.0)
0.1M1M10MValue Data0.1M1M10M435656746866737664
±10±5±9±9±7±7±11±12±22DDPG(ædata=0.4)
0.1M1M10MValue Data0.1M1M10M465655757683839492
±10±10±7±8±10±7±12±3±3DDPG(ædata=0.7)
0.1M1M10MValue Data0.1M1M10M385862799286909897
±20±8±10±12±7±6±10±1±1DDPG(ædata=1.0)gc-antmaze-large(IQL)
High coverageLow optimality
0.1M0.3M1MValue Data0.1M0.3M1MPolicy Data115115112112114113114114115
±5±3±4±4±3±4±5±4±4AWR(ædata=0.0)
0.1M0.3M1MValue Data0.1M0.3M1M116120118121120122120121123
±5±5±2±4±6±4±5±4±3AWR(ædata=0.5)
0.1M0.3M1MValue Data0.1M0.3M1M119129128120132133123132134
±8±4±4±7±5±2±7±4±2AWR(ædata=1.0)
0.1M0.3M1MValue Data0.1M0.3M1M121126129130135135129136138
±4±5±5±4±2±3±3±3±3AWR(ædata=1.5)
0.1M0.3M1MValue Data0.1M0.3M1MPolicy Data119121118114120116117124118
±6±4±6±6±5±6±6±3±5DDPG(ædata=0.0)
0.1M0.3M1MValue Data0.1M0.3M1M124128121123130124125128124
±6±4±6±10±3±4±6±3±4DDPG(ædata=0.5)
0.1M0.3M1MValue Data0.1M0.3M1M115127135123135134119132132
±12±5±6±6±5±4±18±8±5DDPG(ædata=1.0)
0.1M0.3M1MValue Data0.1M0.3M1M123133140122140137126143143
±11±5±3±9±4±4±10±3±3DDPG(ædata=1.5)adroit-pen(IQL)
Low coverageHigh optimalityHigh coverageLow optimalityFigure 7: Should we use high-coverage or high-optimality datasets? The data-scaling matrices above show
thathigh-coverage datasets can be much more effective than high-optimality datasets. This is because high-
coverage datasets can improve test-time policy accuracy , one of the main bottlenecks of offline RL.
already saturate on this axis. Further performance differences may simply be due to the effects of a
given offline RL objective on novel states , which very few methods explicitly control!
That said, controlling test-time generalization might also appear impossible: while offline RL methods
could hillclimb on validation accuracy via a combination of techniques that address statistical errors
such as regularization ( e.g., Dropout [ 52], LayerNorm [ 3], etc.), improving test-time policy accuracy
requires generalization to a potentially very different distribution (Figure 5), which is theoretically
impossible to guarantee without additional coverage or structural assumptions, as the test-time state
distribution can be arbitrarily adversarial in the worst case. However, we claim that if we actively
utilize the information available at test time or have the freedom to design offline datasets, it is
possible to improve test-time policy accuracy in practice, and we discuss such solutions below (see
Appendix C for further discussions).
5.3 Solution 1: Improve offline data coverage
If we have the freedom to control the data collection process, perhaps the most straightforward way
to improve test-time policy accuracy is to use a dataset that has as high coverage as possible so that
test-time states can be covered by the dataset distribution. However, at the same time, high-coverage
datasets often involve exploratory actions, which may compromise the quality (optimality) of the
dataset. This makes us wonder in practice: which is more important, high coverage or high optimality?
To answer this question, we revert back to our analysis tool of data-scaling matrices from Section 4 and
empirically compare the data-scaling matrices on datasets collected by expert policies with different
levels of action noises ( σdata). Figure 7 shows the results of IQL agents on gc-antmaze-large
andadroit-pen (8seeds each). The results suggest that the performance of offline RL generally
improves as the dataset has better state coverage, despite the increase in suboptimality. This is aligned
with our findings in Figure 6, which indicate that the main challenge of offline RL is often notlearning
an effective policy from suboptimal data, but rather learning a policy that generalizes well to test-time
states. In addition, we note that it is crucial to use a value gradient-based policy extraction method
(DDPG+BC; see Section 4) in this case as well, where we train a policy from high-coverage data.
For instance, in low-data regimes in gc-antmaze-large in Figure 7, AWR fails to fully leverage
the value function, whereas DDPG+BC still allows the algorithm to improve performance with better
value functions. Based on our findings, we suggest practitioners prioritize high coverage (particularly
around the states that the optimal policy will likely visit) over high optimally when collecting datasets.
5.4 Solution 2: Test-time policy improvement
If we do not wish to modify offline data collection, another way to improve test-time policy accuracy is
toon-the-fly train or steer the policy guided by the learned value function on test-time states . Especially
given that imperfect policy extraction from the value function is often a significant bottleneck in
offline RL (Section 4), we propose two simple techniques to further distill the information in the
value function into the policy on test-time states.
(1) On-the-fly policy extraction (OPEX). Our first idea is to simply adjust policy actions in the
direction of the value gradient at evaluation time. Specifically, after sampling an action from the
policy a∼π(· |s)at test time, we further adjust the action based on the frozen learned Qfunction
during evaluation rollouts with the following formula:
a←a+β· ∇aQ(s, a), (7)
9IQL SfBC OPEX TTT255075Returnd4rl-hopper
IQL SfBC OPEX TTT6080d4rl-walker2d
IQL SfBC OPEX TTT200400exorl-walker
IQL SfBC OPEX TTT100200exorl-cheetah
IQL SfBC OPEX TTT4060Returnkitchen-partial
IQL SfBC OPEX TTT405060kitchen-mixed
IQL SfBC OPEX TTT6080antmaze-medium
IQL SfBC OPEX TTT204060antmaze-largeFigure 8: Test-time policy improvement strategies (OPEX and TTT). Our two on-the-fly policy improvement
techniques (OPEX and TTT) lead to substantial performance improvements on diverse tasks, by mitigating the
test-time policy generalization bottleneck.
where βis a hyperparameter that corresponds to the test-time “learning rate”. Intuitively, Equation (7)
adjusts the action in the direction that maximally increases the learned Q function. We call this
technique on-the-fly policy extraction (OPEX) . Note that OPEX requires only a single line of
additional code at evaluation and does not change the training procedure at all.
(2) Test-time training (TTT). We also propose another variant that further updates the parameters of
the policy by continuously extracting the policy from the fixed value function on test-time states, as
more rollouts are performed. Specifically, we update the policy πwith the following objective:
max
πJTTT(π) =Es,a∼D ∪ pπ(·)[Q(s, µπ(s))−β·DKL(πoff∥π)], (8)
where πoffdenotes the fixed, learned offline RL policy, D ∪pπ(·)denotes the mixture of the dataset
and evaluation state distributions, and βdenotes a hyperparameter that controls the strength of the
regularizer. Intuitively, Equation (8) is a “parameter-updating” version of OPEX, where we further
update the parameters of the policy πto maximize the learned value function, while not deviating too
far away from the learned offline RL policy. We call this scheme test-time training (TTT) . Note that
TTT only trains πbased on test-time interaction data, while Qandπoffremain fixed.
Figure 8 compares the performances of vanilla IQL, SfBC (Equation (3), another test-time policy
extraction method that does not involve gradients), and our two gradient-based test-time policy
improvement strategies on eight tasks ( 8seeds each, error bars denote 95% confidence intervals).
The results show that OPEX and TTT improve performance over vanilla IQL and SfBC in many
tasks, often by significant margins, by mitigating the test-time policy generalization bottleneck.
Takeaway: Improving test-time policy accuracy significantly boosts performance.
Test-time policy generalization is one of the most significant bottlenecks in offline RL. Use
high-coverage datasets. Improve policy accuracy on test-time states with on-the-fly policy
improvement techniques.
6 Conclusion: What does our analysis tell us?
In this work, we empirically demonstrated that, contrary to the prior belief that improving the quality
of the value function is the primary bottleneck of offline RL, current offline RL methods are often
heavily limited by how faithfully the policy is extracted from the value function and how well this
policy generalizes on test-time states. For practitioners , our analysis suggests a clear empirical recipe
for effective offline RL: train a value function on as diverse data as possible, and allow the policy to
maximally utilize the value function, with the best policy extraction objective ( e.g., DDPG+BC) and/or
potential test-time policy improvement strategies. For future algorithms research , our analysis
emphasizes two important open questions in offline RL: (1) What is the best way to extract a policy
from the learned value function? (2) How can we train a policy in a way that it generalizes well on test-
time states? The second question is particularly notable, because it suggests a diametrically opposed
viewpoint to the prevailing theme of pessimism in offline RL, where only a few works have explicitly
aimed to address this generalization aspect of offline RL [ 37,38,63]. We believe finding effective
answers to these questions would lead to significant performance gains in offline RL, substantially
enhancing its applicability and scalability, and would encourage the community to incorporate a
holistic picture of offline RL alongside the current prominent research on value function learning.
10Acknowledgments
We thank Benjamin Eysenbach and Dibya Ghosh for insightful discussions about data-scaling matrices
and state representations, respectively, and Oleh Rybkin, Fahim Tajwar, Mitsuhiko Nakamoto, Yingjie
Miao, Sandra Faust, and Dale Schuurmans for helpful feedback on earlier drafts of this work. This
work was partly supported by the Korea Foundation for Advanced Studies (KFAS), National Science
Foundation Graduate Research Fellowship Program under Grant No. DGE 2146752, and ONR
N00014-21-1-2838. This research used the Savio computational cluster resource provided by the
Berkeley Research Computing program at UC Berkeley.
References
[1]Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement
learning with diversified q-ensemble. In Neural Information Processing Systems (NeurIPS) , 2021.
[2]Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In
Neural Information Processing Systems (NeurIPS) , 2017.
[3]Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. ArXiv , abs/1607.06450, 2016.
[4]Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Data quality in imitation learning. In Neural Information
Processing Systems (NeurIPS) , 2023.
[5]David Brandfonbrener, William F. Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-
policy evaluation. In Neural Information Processing Systems (NeurIPS) , 2021.
[6]Yuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations (ICLR) , 2019.
[7]Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. Offline reinforcement learning via high-
fidelity generative behavior modeling. In International Conference on Learning Representations (ICLR) ,
2023.
[8]Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for
offline reinforcement learning. In International Conference on Machine Learning (ICML) , 2022.
[9]Open X-Embodiment Collaboration, Abby O’Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta,
Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert
Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew
Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin,
Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick,
Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le,
Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia,
Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak
Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns,
Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S.
Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi
Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki
Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang,
Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg,
Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo,
Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey
Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han,
Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken
Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang,
Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap
Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-
Fei, Liam Tan, Linxi "Jim" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert,
Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian
Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama,
Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko
Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer,
Osbert Bastani, Pannag R Sanketi, Patrick "Tree" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David
Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael
Rafailov, Ran Tian, Ria Doshi, Roberto Mart’in-Mart’in, Rohan Baijal, Rosario Scalise, Rose Hendrix,
11Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel
Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham
Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon
Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep
Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta,
Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev,
Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan,
Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu,
Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng
Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui,
Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li,
Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, and Zipeng Lin.
Open x-embodiment: Robotic learning datasets and rt-x models. In IEEE International Conference on
Robotics and Automation (ICRA) , 2024.
[10] Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offline
rl via supervised learning? In International Conference on Learning Representations (ICLR) , 2022.
[11] Benjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, and Sergey Levine. Contrastive learning as
goal-conditioned reinforcement learning. In Neural Information Processing Systems (NeurIPS) , 2022.
[12] Justin Fu, Aviral Kumar, Ofir Nachum, G. Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven
reinforcement learning. ArXiv , abs/2004.07219, 2020.
[13] Yuwei Fu, Di Wu, and Benoît Boulet. A closer look at offline rl agents. In Neural Information Processing
Systems (NeurIPS) , 2022.
[14] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. In Neural
Information Processing Systems (NeurIPS) , 2021.
[15] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without explo-
ration. In International Conference on Machine Learning (ICML) , 2019.
[16] Scott Fujimoto, David Meger, Doina Precup, Ofir Nachum, and Shixiang Shane Gu. Why should i trust
you, bellman? the bellman error is a poor replacement for value error. In International Conference on
Machine Learning (ICML) , 2022.
[17] Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxent rl without
entropy. In International Conference on Learning Representations (ICLR) , 2023.
[18] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max
q-learning operator for simple yet effective offline and online rl. In International Conference on Machine
Learning (ICML) , 2021.
[19] Seyed Kamyar Seyed Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimating
uncertainties for offline rl through ensembles, and why their independence matters. In Neural Information
Processing Systems (NeurIPS) , 2022.
[20] Dibya Ghosh. dibyaghosh/jaxrl_m, 2023. URL https://github.com/dibyaghosh/jaxrl_m .
[21] Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. Idql:
Implicit q-learning as an actor-critic method with diffusion policies. ArXiv , abs/2304.10573, 2023.
[22] Leslie Pack Kaelbling. Learning to achieve goals. In International Joint Conference on Artificial
Intelligence (IJCAI) , 1993.
[23] Bingyi Kang, Xiao Ma, Yi-Ren Wang, Yang Yue, and Shuicheng Yan. Improving and benchmarking offline
reinforcement learning algorithms. ArXiv , abs/2306.00972, 2023.
[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR) , 2015.
[25] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning.
InInternational Conference on Learning Representations (ICLR) , 2022.
[26] Aviral Kumar, Aurick Zhou, G. Tucker, and Sergey Levine. Conservative q-learning for offline reinforce-
ment learning. In Neural Information Processing Systems (NeurIPS) , 2020.
12[27] Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization inhibits
data-efficient deep reinforcement learning. In International Conference on Learning Representations
(ICLR) , 2021.
[28] Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. Should i run offline reinforcement learning
or behavioral cloning? In International Conference on Learning Representations (ICLR) , 2021.
[29] Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron C. Courville, G. Tucker, and Sergey Levine. Dr3:
Value-based deep reinforcement learning requires explicit regularization. In International Conference on
Learning Representations (ICLR) , 2022.
[30] Cassidy Laidlaw, Stuart J. Russell, and Anca D. Dragan. Bridging rl theory and practice with the effective
horizon. In Neural Information Processing Systems (NeurIPS) , 2023.
[31] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement
learning: State-of-the-art , pages 45–73. Springer, 2012.
[32] Jongmin Lee, Wonseok Jeon, Byung-Jun Lee, Joëlle Pineau, and Kee-Eung Kim. Optidice: Offline policy
optimization via stationary distribution correction estimation. In International Conference on Machine
Learning (ICML) , 2021.
[33] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,
and perspectives on open problems. ArXiv , abs/2005.01643, 2020.
[34] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez, Yuval
Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
International Conference on Learning Representations (ICLR) , 2016.
[35] Cong Lu, Philip J. Ball, Tim G. J. Rudner, Jack Parker-Holder, Michael A. Osborne, and Yee Whye Teh.
Challenges and opportunities in offline reinforcement learning from visual observations. Transactions on
Machine Learning Research (TMLR) , 2023.
[36] Ajay Mandlekar, Danfei Xu, J. Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,
Silvio Savarese, Yuke Zhu, and Roberto Mart’in-Mart’in. What matters in learning from offline human
demonstrations for robot manipulation. In Conference on Robot Learning (CoRL) , 2021.
[37] Bogdan Mazoure, Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Improving zero-shot generalization
in offline reinforcement learning using generalized similarity functions. In Neural Information Processing
Systems (NeurIPS) , 2022.
[38] Ishita Mediratta, Qingfei You, Minqi Jiang, and Roberta Raileanu. The generalization gap in offline
reinforcement learning. In International Conference on Learning Representations (ICLR) , 2024.
[39] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv , abs/1312.5602, 2013.
[40] Rémi Munos. Error bounds for approximate policy iteration. In International Conference on Machine
Learning (ICML) , 2003.
[41] Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy
gradient from arbitrary experience. ArXiv , abs/1912.02074, 2019.
[42] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. ArXiv , abs/2006.09359, 2020.
[43] Whitney Newey and James L. Powell. Asymmetric least squares estimation and testing. Econometrica , 55:
819–847, 1987.
[44] Seohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine. Hiql: Offline goal-conditioned rl
with latent states as actions. In Neural Information Processing Systems (NeurIPS) , 2023.
[45] Seohong Park, Tobias Kreiman, and Sergey Levine. Foundation policies with hilbert representations. In
International Conference on Machine Learning (ICML) , 2024.
[46] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple
and scalable off-policy reinforcement learning. ArXiv , abs/1910.00177, 2019.
[47] Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space
control. In International Conference on Machine Learning (ICML) , 2007.
13[48] Rafael Rafailov, Kyle Beltran Hatch, Anikait Singh, Aviral Kumar, Laura Smith, Ilya Kostrikov, Philippe
Hansen-Estruch, Victor Kolev, Philip J Ball, Jiajun Wu, et al. D5rl: Diverse datasets for data-driven deep
reinforcement learning. In Reinforcement Learning Conference (RLC) , 2024.
[49] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel
Barth-maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent.
Transactions on Machine Learning Research (TMLR) , 2022.
[50] Harshit S. Sikchi, Qinqing Zheng, Amy Zhang, and Scott Niekum. Dual rl: Unification and new methods
for reinforcement and imitation learning. In International Conference on Learning Representations (ICLR) ,
2024.
[51] Jost Tobias Springenberg, Abbas Abdolmaleki, Jingwei Zhang, Oliver Groth, Michael Bloesch, Thomas
Lampe, Philemon Brakel, Sarah Bechtle, Steven Kapturowski, Roland Hafner, Nicolas Manfred Otto
Heess, and Martin A. Riedmiller. Offline actor-critic reinforcement learning scales to large models. In
International Conference on Machine Learning (ICML) , 2024.
[52] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research (JMLR) ,
15(1):1929–1958, 2014.
[53] Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov. Revisiting the minimalist
approach to offline reinforcement learning. In Neural Information Processing Systems (NeurIPS) , 2023.
[54] Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov. Corl:
Research-oriented deep offline reinforcement learning library. In Neural Information Processing Systems
(NeurIPS) , 2023.
[55] Ruosong Wang, Dean Phillips Foster, and Sham M. Kakade. What are the statistical limits of offline rl with
linear function approximation? In International Conference on Learning Representations (ICLR) , 2021.
[56] Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham M. Kakade. Instabilities of offline rl with
pre-trained neural representation. In International Conference on Machine Learning (ICML) , 2021.
[57] Tongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal goal-reaching reinforcement
learning via quasimetric learning. In International Conference on Machine Learning (ICML) , 2023.
[58] Ziyun Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott E. Reed, Bobak Shahriari,
Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Manfred Otto Heess, and Nando de Freitas. Critic
regularized regression. In Neural Information Processing Systems (NeurIPS) , 2020.
[59] Yifan Wu, G. Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. ArXiv ,
abs/1911.11361, 2019.
[60] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov,
and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In International
Conference on Machine Learning (ICML) , 2021.
[61] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Chan, and Xianyuan Zhan.
Offline rl with no ood actions: In-sample learning via implicit value regularization. In International
Conference on Learning Representations (ICLR) , 2023.
[62] Mengjiao Yang and Ofir Nachum. Representation matters: Offline pretraining for sequential decision
making. In International Conference on Machine Learning (ICML) , 2021.
[63] Rui Yang, Yong Lin, Xiaoteng Ma, Haotian Hu, Chongjie Zhang, and T. Zhang. What is essential for unseen
goal generalization of offline goal-conditioned rl? In International Conference on Machine Learning
(ICML) , 2023.
[64] Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, P. Abbeel, Alessandro Lazaric, and Lerrel
Pinto. Don’t change the algorithm, change the data: Exploratory data for offline reinforcement learning.
ArXiv , abs/2201.13425, 2022.
[65] Chongyi Zheng, Benjamin Eysenbach, Homer Walke, Patrick Yin, Kuan Fang, Ruslan Salakhutdinov, and
Sergey Levine. Stabilizing contrastive rl: Techniques for offline goal reaching. ArXiv , abs/2306.03346,
2023.
14Appendices
A Limitations
One limitation of our analysis is that the MSE metrics in Equations (4) to (6) are in some sense
“proxies” to measure the accuracy of the policy (somewhat similarly to how Bellman errors do not
always accurately reflect value errors in the context of value learning [ 16]). For instance, if there
exist multiple optimal actions that are potentially very different from one another, or the expert
policy used in practice is not sufficiently optimal, the MSE metrics might not be highly indicative of
the performance or accuracy of the policy. Nonetheless, we empirically find that there is a strong
correlation between the evaluation MSE metric and performance, and we believe our analysis could
further be refined with potentially more sophisticated metrics ( e.g., by considering E[Q∗(s, a)]instead
ofE[(π(s)−π∗(s))2]), which we leave for future work.
Another limitation of our analysis in Section 4 is we only consider policy extraction in continuous-
action environments. In discrete-action environments, our takeaway might not directly apply in its
current form because (1) DDPG+BC is not straightforwardly defined with discrete actions and (2)
it is possible to directly use the Q function to implicitly define a policy (without having a separate
policy network). We leave investigating the effect of policy extraction in discrete-action environments
for future work.
B Preliminaries
We consider a Markov decision process (MDP) defined by M= (S,A, r, µ, p ).Sdenotes the state
space, Adenotes the action space, r:S × A → Rdenotes the reward function, µ∈∆(S)denotes
the initial state distribution, and p:S × A → ∆(S)denotes the transition dynamics, where ∆(X)
denotes the set of probability distributions over a set X. We consider the offline RL problem, whose
goal is to find a policy π:S → ∆(A)(orπ:S → A if deterministic) that maximizes the dis-
count return J(π) =Eτ∼pπ(τ)[PT
t=0γtr(st, at)], where pπ(τ) =pπ(s0, a0, s1, a1, . . . , s T, aT) =
µ(s0)π(a0|s0)p(s1|s0, a0)···π(aT|sT)andγis a discount factor, solely from a static dataset
D={τi}i∈{1,2,...,N}without online interactions. In some experiments, we consider offline goal-
conditioned RL [ 2,11,22,44,57] as well, where the policy and reward function are also condi-
tioned on a goal state g, which is sampled from a goal distribution pg∈∆S. For goal-conditioned
RL, we assume a sparse goal-conditioned reward function, r(s, g) =1(s=g), which does not re-
quire any prior knowledge about the state space. We also assume that the episode ends upon goal-
reaching [44, 45, 57].
C Policy generalization: Rethinking the role of state representations
0500K1M050Return0500K1M0.51.0Evaluation MSE
0500K1MGradient Steps0.51.0Validation MSE0500K1MGradient Steps0.51.0Training MSE°0.04°0.020.000.020.04x°0.04°0.020.000.020.04y
º(a|s, g)º(a|¡(s),g)
Figure 9: A good state represen-
tation naturally enables test-time
generalization, leading to substan-
tially better performance.In this section, we introduce another way to improve test-time
policy accuracy from the perspective of state representations .
Specifically, we claim that we can improve test-time policy ac-
curacy by using a “good” representation that naturally enables
out-of-distribution generalization. Since this might sound a bit
cryptic, we first show results to illustrate this point.
Figure 9 shows the performances of goal-conditioned BC1on
gc-antmaze-large with two different homeomorphic represen-
tations: one with the original state representation s, and one with
a different representation ϕ(s)with a continuous, invertible ϕ
(specifically, ϕtransforms x-ycoordinates with invertible tanh
kernels; see Appendix D.6). Hence, these two representations
contain the exactly same amount of information and are even
topologically homeomorphic (under the standard Euclidean topol-
ogy). However, they result in very different performances, and
1Here, we use BC (not RL) to focus solely on state representations, obviating potential confounding factors
regarding the value function.
15the MSE plots in Figure 9 indicate that this difference is due to
nothing other than the better test-time, evaluation MSE (observe that their training and validation
MSEs are nearly identical)!
This result sheds light on an important perspective of state representations: a good state representa-
tion should be able to enable test-time generalization naturally . While designing such a good state
representation might require some knowledge or inductive biases about the task, our results suggest
that using such a representation is nonetheless very important in practice, since it affects the perfor-
mance of offline RL significantly by improving test-time policy generalization capability.
D Experimental details
We provide the full experimental details in this section.
D.1 Value learning objectives
One-step RL (SARSA) . SARSA [ 5] is one of the simplest offline value learning algorithms. Instead
of fitting a Bellman optimal value function Q∗, SARSA aims to fit a behavioral value function Qβ
with TD-learning, without querying out-of-distribution actions. Concretely, SARSA minimizes the
following loss:
min
QLSARSA (Q) =E(s,a,s′,a′)∼D[(r(s, a) +γ¯Q(s′, a′)−Q(s, a))2], (9)
where s′anda′denote the next state and action, respectively, and ¯Qdenotes the target Qnetwork [ 39].
Despite its apparent simplicity, extracting a policy by maximizing the value function learned by
SARSA is known to be a surprisingly strong baseline [5, 30].
Implicit Q-learning (IQL) . Implicit Q-learning (IQL) [ 25] aims to fit a Bellman optimal value
function Q∗by approximating the maximum operator with an in-sample expectile regression. IQL
minimizes the following losses:
min
QLQ
IQL(Q) =E(s,a,s′)∼D[(r(s, a) +γV(s′)−Q(s, a))2], (10)
min
VLV
IQL(V) =E(s,a)∼D[ℓ2
τ(¯Q(s, a)−V(s))], (11)
where ℓ2
τ(x) =|τ−1(x <0)|x2is the expectile loss [ 43] with an expectile parameter τ. Intuitively,
when τ >0.5, the expectile loss in Equation (11) penalizes positive errors more than negative errors,
which makes Vcloser to the maximum value of ¯Q. This way, IQL approximates V∗andQ∗only with
in-distribution dataset actions, without referring to the erroneous values at out-of-distribution actions.
Contrastive RL (CRL) . Contrastive RL (CRL) [ 11] is a value learning algorithm for offline goal-
conditioned RL based on contrastive learning. CRL maximizes the following objective:
max
fJCRL(f) =Es,a∼D,g∼p+
D(·|s,a),g−∼p+
D(·)[logσ(f(s, a, g )) + log(1 −σ(f(s, a, g−)))],(12)
where σdenotes the sigmoid function and p+
D(· |s, a)denotes the geometric future state distribution
of the dataset D. Eysenbach et al. [11] show that the optimal solution of Equation (12) is given as
f∗(s, a, g ) = log( p+
D(g|s, a)/p+
D(g)), which gives us the behavioral goal-conditioned Q function
asQβ(s, a, g ) =p+
D(g|s, a) =p+
D(g)ef∗(s,a,g ), where p+
D(g)is a policy-independent constant.
D.2 Environments and datasets
We describe the environments and datasets we employ in our analysis.
D.2.1 Data-scaling analysis
For the data-scaling analysis in Section 4, we employ the following environments and datasets
(Figure 10).
•antmaze-large andgc-antmaze-large are based on the antmaze-large-diverse-v2 envi-
ronment from the D4RL suite [ 12], where the agent must be able to manipulate a quadrupedal
robot to reach a given target goal ( antmaze-large ) or to reach any goal from any other state
16gc-antmaze-large
antmaze-larged4rl-hopperd4rl-walker2dkitchengc-roboverse
exorl-walkerexorl-cheetahFigure 10: Environments.
(gc-antmaze-large ) in a given maze. For the dataset for gc-antmaze-large in our data-
scaling analysis, we collect 10M transitions using a noisy expert policy that navigates through
the maze. We use the same policy and noise level ( σdata= 0.2) as the one used to collect
antmaze-large-diverse-v2 in D4RL.
•d4rl-hopper andd4rl-walker2d are the hopper-medium-v2 andwalker2d-medium-v2
tasks from the D4RL locomotion suite. We use the original 1M-sized datasets collected by par-
tially trained policies [12].
•exorl-walker andexorl-cheetah are the walker-run andcheetah-run tasks from the
ExORL benchmark [ 64]. We use the original 10M-sized datasets collected by RND agents [ 6].
Since the datasets are collected by purely unsupervised exploratory policies, they feature high
suboptimality and high state-action diversity.
•kitchen is based on the kitchen-mixed-v0 task from the D4RL suite, where the goal is to
complete four manipulation tasks ( e.g., opening the microwave, moving the kettle) with a robot
arm. Since the original dataset size is relatively small, for our data-scaling analysis, we collect
a large 1M-sized dataset with a noisy, biased expert policy, where we add noises sampled from
a zero-mean Gaussian distribution with a standard deviation of 0.2in addition to a randomly
initialized policy’s actions to the expert policy’s actions.
•gc-roboverse is a pixel-based goal-conditioned robotic task, where the goal is to manipulate a
robot arm to rearrange objects to match a target image. The agent must be able to perform object
manipulation purely from 48×48×3images. We use the 1M-sized dataset used by Park et al.
[44], Zheng et al. [65].
D.2.2 Policy generalization analysis
For the policy generalization analysis in Section 5, we use the antmaze-medium-diverse-v2 ,
antmaze-large-diverse-v2 ,kitchen-partial-v0 ,kitchen-mixed-v0 ,pen-cloned-v1 ,
hammer-cloned-v1 ,door-cloned-v1 ,hopper-medium-v2 , and walker2d-medium-v2 envi-
ronments and datasets from the D4RL suite [ 12] as well as the walker-run andcheetah-run from
the ExORL suite [64].
D.3 Data-scaling matrices
We train agents for 1M steps ( 500K steps for gc-roboverse ) with each pair of value learning and
policy extraction algorithms. We evaluate the performance of the agent every 100K steps with 50
rollouts, and report the performance averaged over the last 3evaluations and over 8seeds. In Figures 1
and 7, we individually tune the policy extraction hyperparameter ( αfor AWR and DDPG+BC, and N
for SfBC) for each cell, and report the performance with the best hyperparameter. To save computation,
we extract multiple policies with different hyperparameters from the same value function (note that this
is possible because we use decoupled offline RL algorithms). To generate smaller-sized datasets from
the original full dataset, we randomly shuffle trajectories in the original dataset using a fixed random
seed, and take the first Ktrajectories such that smaller datasets are fully contained in larger datasets.
D.4 MSE metrics
We randomly split the trajectories in a dataset into a training set ( 95%) and a validation set ( 5%)
in our experiments. For the expert policies π∗in the MSE metrics defined in Equations (4) to (6),
we use either the original expert policies from the D4RL suite ( adroit-{pen, hammer, door}
andgc-antmaze-large ) or policies pre-trained with offline-to-online RL until their performance
saturates ( antmaze-{medium, large} andkitchen-mixed ). To train “global” expert policies for
17antmaze-{medium, large} , we reset the agent to arbitrary locations in the entire maze. This initial
state distribution is only used to train an expert policy; we use the original initial state distribution for
the other experiments.
D.5 Test-time policy improvement methods
In Figure 8, for IQL, SfBC, and OPEX, we train IQL agents (with original AWR) for 500K (kitchen )
or1M (others) gradient steps. For TTT, we further train the policy up to 2M gradient steps with
a learning rate of 0.00003 . Inantmaze , we consider both deterministic evaluation and stochastic
evaluation with a fixed standard deviation of 0.4(which roughly matches the learned standard
deviation of the BC policy), and report the best performance of them for each method.
D.6 State representation experiments
We describe the state representation ϕused in Appendix C. An antmaze state consists of a 2-Dx-y
coordinates and 27-D proprioceptive information. We transform xandyindividually with 32 tanh
kernels, i.e.,
˜xi= tanhx−xi
δx
(13)
˜yi= tanhy−yi
δx
, (14)
where i∈ {1,2, . . . , 32},δx=x2−x1,δy=y2−y1, and x1, . . . , x 32andy1, . . . , y 32are de-
fined as numpy.linspace(-2, 38, 32) andnumpy.linspace(-2, 26, 32) , respectively. De-
noting the 27-D proprioceptive state as sproprio ,ϕ(s)is defined as follows: ϕ([x, y;sproprio ]) =
[˜x1, . . . , ˜x32,˜y1, . . . , ˜y32;sproprio ], where ‘ ;’ denotes concatenation. Intuitively, ϕis similar to the
discretization of the x-ydimensions with 32bins, but with a continuous, invertible tanh transforma-
tion instead of binary discretization.
D.7 Implementation details
Our implementation is based on jaxrl_minimal [20] and the official implementation of HIQL [ 44]
(for offline goal-conditioned RL). We use an internal cluster consisting of A5000 GPUs to run our
experiments. Each experiment in our work takes no more than 18hours.
D.7.1 Data-scaling analysis
Default hyperparameters. We mostly follow the original hyperparameters for IQL [ 25], goal-
conditioned IQL [ 44], and CRL [ 11]. Tables 2 and 3 list the common and environment-specific
hyperparameters, respectively. For SARSA, we use the same implementation as IQL, but with the
standard ℓ2loss instead of an expectile loss. For pixel-based environments ( i.e.,gc-roboverse ),
we use the same architecture and image augmentation as Park et al. [44]. In goal-conditioned
environments as well as antmaze tasks, we subtract 1from rewards, following previous works [ 25,
44].
Policy extraction methods. We use Gaussian distributions (without tanh squashing) to model action
distributions. We use a fixed standard deviation of 1for AWR and DDPG+BC and a learnable
standard deviation for SfBC. For DDPG+BC, we clip actions to be within the range of [−1,1]in
the deterministic policy gradient term in Equation (2). We empirically find that this is better than
tanh squashing [ 14] across the board, and is important to achieving strong performance in some
environments. We list the policy extraction hyperparameters we consider in our experiments in curly
brackets in Table 3.
D.7.2 Policy generalization analysis
Hyperparameters. Table 4 lists the hyperparameters that we use in our offline-to-online RL and
test-time policy improvement experiments. In these experiments, we use Gaussian distributions with
learnable standard deviations for action distributions.
18Table 2: Common hyperparameters for data-scaling matrices.
Hyperparameter Value
Learning rate 0.0003
Optimizer Adam [24]
Target smoothing coefficient 0.005
Discount factor γ 0.99
Table 3: Environment-specific hyperparameters for data-scaling matrices.
Environment gc-antmaze-large antmaze-large d4rl-hopper d4rl-walker
# gradient steps 106106106106
Minibatch size 1024 256 256 256
MLP dimensions (512,512,512) (256 ,256) (256 ,256) (256 ,256)
IQL expectile 0.9 0 .9 0 .7 0 .7
LayerNorm [3] True False True True
AWR α(IQL) {0,1,3,10} { 0,3,10,30} { 0,1,3,10} { 0,1,3,10}
AWR α(SARSA/CRL) {0,10,30,100} { 0,3,10,30} { 0,1,3,10} { 0,1,3,10}
DDPG+BC α(IQL) {0.1,0.3,1,3} { 0.1,0.3,1,3} { 1,3,10,30} { 1,3,10,30}
DDPG+BC α(SARSA/CRL) {0.1,0.3,1,3} { 0.1,0.3,1,3} { 1,3,10,30} { 1,3,10,30}
SfBC N(IQL) {1,16,64} { 1,16,64} { 1,16,64} { 1,16,64}
SfBC N(SARSA/CRL) {1,16,64} { 1,16,64} { 1,16,64} { 1,16,64}
Environment exorl-walker exorl-cheetah kitchen gc-roboverse
# gradient steps 1061061065×105
Minibatch size 1024 1024 1024 256
MLP dimensions (512,512,512) (512 ,512,512) (512 ,512,512) (512 ,512,512)
IQL expectile 0.9 0 .9 0 .7 0 .7
LayerNorm [3] True True False True
AWR α(IQL) {0,1,10,100} { 0,1,10,100} { 0,1,3,10} { 0,0.1,1,10}
AWR α(SARSA/CRL) {0,1,10,100} { 0,1,10,100} { 0,1,3,10} { 0,1,10,100}
DDPG+BC α(IQL) {0,0.01,0.1,1} { 0,0.01,0.1,1} { 10,30,100,300} { 3,10,30,100}
DDPG+BC α(SARSA/CRL) {0,0.01,0.1,1} { 0,0.01,0.1,1} { 10,30,100,300} { 3,10,30,100}
SfBC N(IQL) {1,16,64} { 1,16,64} { 1,16,64} { 1,16,64}
SfBC N(SARSA/CRL) {1,16,64} { 1,16,64} { 1,16,64} { 1,16,64}
E Additional results
We provide the full data-scaling matrices with different policy extraction hyperparameters ( αfor
AWR and DDPG+BC, and Nfor SfBC) in Figure 11.
19Table 4: Hyperparameters for policy generalization analysis.
Hyperparameter Value
Learning rate 0.0003
Optimizer Adam [24]
# offline gradient steps 106(antmaze ),5×105(kitchen ,adroit )
# total gradient steps 2×106
# gradient steps per environment step 1
Minibatch size 1024 (kitchen ),256(antmaze ,adroit )
MLP dimensions (512,512,512) (kitchen ),(256,256) (antmaze ,adroit )
Target smoothing coefficient 0.005
Discount factor γ 0.99
LayerNorm [3] True ( kitchen ), False ( antmaze ,adroit )
IQL expectile 0.9(antmaze ),0.7(kitchen ,adroit )
Policy extraction method AWR
AWR α 10(antmaze ),0.5(kitchen ),3(adroit )
SfBC N 16
OPEX β0.3(antmaze ),0.0003 (kitchen ),0.03(d4rl-hopper ),
0.1(d4rl-walker2d ),1(exorl-{walker, cheetah} )
TTTβ0.3(antmaze ),5(kitchen ),0.5(d4rl-hopper ),
0.3(d4rl-walker2d ),0.01(exorl-{walker, cheetah} )
200.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 1101383532332840382958604845
±1±1±1±1±6±4±6±7±6±10±8±5±12±11±15±12AWR (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
8748454442403443383839283637
±6±3±3±2±4±5±7±9±9±9±9±10±10±9±10±12AWR (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
22262525565954565357616354566161
±7±7±7±8±10±8±6±12±6±10±7±13±13±8±10±8AWR (α= 3.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
19212727525558605256656143506155
±5±7±5±5±4±5±8±5±7±9±9±7±11±9±12±8AWR (α= 10.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 19181719525148614545434656625858
±10±6±7±9±12±12±11±5±11±9±13±10±18±17±19±20DDPG (α= 3.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
48585557536160615153566454626259
±13±4±7±9±7±10±7±8±6±13±7±4±5±12±7±11DDPG (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
20365149125153461649604921545352
±12±10±7±8±5±11±5±14±10±13±8±5±13±12±12±9DDPG (α= 0.3)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
010 730914 601013 8061212
±0±7±8±2±0±6±12±6±0±8±8±7±0±4±6±4DDPG (α= 0.1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 1100332931284542495267677068
±1±1±0±0±4±11±7±7±7±11±9±6±9±12±8±20SfBC (N= 1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
19303436526061606266696664697473
±5±10±8±11±6±6±5±6±8±6±3±5±6±7±5±5SfBC (N= 16 )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
30364645455860555261605954686668
±7±9±9±10±10±6±8±3±5±5±6±6±8±5±9±5SfBC (N= 64 )gc-antmaze-large (IQL )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 1111363936333028303052565856
±1±1±1±1±8±4±6±8±13±10±9±8±14±19±15±23AWR (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
0000223330414543516237446074
±0±0±1±0±5±11±7±8±8±13±11±8±17±18±16±7AWR (α= 10.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
0000273336303642465645465871
±0±0±0±0±8±6±10±11±9±5±8±7±15±18±12±6AWR (α= 30.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
0000223432374340485748464869
±0±0±0±0±7±7±8±8±10±7±12±8±7±11±15±10AWR (α= 100.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 3423384744393446494760636973
±2±2±1±2±7±5±5±8±16±12±11±9±10±14±12±4DDPG (α= 3.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
281116505156586575677672818084
±1±5±4±8±7±12±8±10±8±5±10±5±5±5±6±4DDPG (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
021941333848736462778860698789
±1±2±9±21±13±18±15±13±12±15±7±5±12±9±4±3DDPG (α= 0.3)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
001932633723367012273
±0±0±2±11±2±4±9±15±13±3±18±13±0±1±16±11DDPG (α= 0.1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 0010312931305148524660676576
±1±0±1±0±8±6±8±6±10±8±9±9±21±10±12±11SfBC (N= 1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
11510404959626370757367748280
±1±1±5±7±9±9±8±15±9±9±5±15±15±3±6±6SfBC (N= 16 )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
12617374857546066706068748162
±1±2±6±8±9±11±11±19±9±7±10±18±18±8±8±11SfBC (N= 64 )gc-antmaze-large (CRL )
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0AWR (α= 0.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 23 7 169 27 26
±0±1±2±3±8±5±4±14±12AWR (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 12 7 164 26 38
±0±0±1±2±10±7±3±12±7AWR (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 12 5 182 13 38
±0±0±1±1±6±8±2±10±8AWR (α= 30.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 2 0 04 10 170 7 10
±4±0±0±6±11±5±0±4±5DDPG (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 9 280 12 490 2 43
±0±10±5±0±17±7±0±6±19DDPG (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 20 0 1
±0±0±1±0±0±2±0±0±1DDPG (α= 0.3)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0DDPG (α= 0.1)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0SfBC (N= 1)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 11 253 23 501 45 45
±0±12±8±3±21±18±1±10±13SfBC (N= 16 )
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 16 104 20 203 48 15
±1±17±5±4±20±11±2±14±10SfBC (N= 64 )antmaze-large (IQL )
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0AWR (α= 0.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0AWR (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0AWR (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0AWR (α= 30.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0DDPG (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0DDPG (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0DDPG (α= 0.3)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0DDPG (α= 0.1)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0SfBC (N= 1)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0SfBC (N= 16 )
0.1M 0.3M 1M
Value Data0.1M0.3M1M
0 0 00 0 00 0 0
±0±0±0±0±0±0±0±0±0SfBC (N= 64 )antmaze-large (SARSA )
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 42 45 4848 49 4850 50 48
±7±4±4±3±4±3±3±2±4AWR (α= 0.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
47 51 5052 51 5252 52 52
±5±3±4±4±3±3±4±4±3AWR (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
49 52 5251 56 5550 53 55
±4±3±4±3±4±2±3±1±4AWR (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
48 50 4849 51 5151 50 48
±5±4±4±2±4±4±2±4±5AWR (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 50 48 4853 51 5251 52 52
±2±2±4±3±3±3±5±3±4DDPG (α= 30.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
50 49 5051 50 5153 52 51
±3±4±3±4±4±7±3±4±5DDPG (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
55 50 4854 43 4455 49 44
±11±10±8±11±5±5±8±10±4DDPG (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
30 41 3536 43 4436 42 44
±10±10±11±9±12±6±13±9±5DDPG (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 31 31 3132 31 3231 32 32
±1±2±2±2±1±1±1±1±1SfBC (N= 1)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
41 42 4343 41 4243 41 43
±5±3±4±4±3±4±4±2±4SfBC (N= 16 )
0.1M 0.3M 1M
Value Data0.1M0.3M1M
40 43 4442 42 4242 41 44
±5±4±3±3±4±4±5±2±5SfBC (N= 64 )d4rl-hopper (IQL )
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 49 48 4650 49 5049 50 50
±2±3±6±3±3±2±3±2±2AWR (α= 0.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
50 53 5352 58 5455 57 57
±4±3±5±5±4±5±2±2±4AWR (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
54 53 5555 59 5755 57 60
±4±2±3±3±3±2±4±5±3AWR (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
50 52 5151 54 5452 54 53
±3±5±3±4±3±3±2±3±4AWR (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 52 49 5155 54 5456 55 56
±3±2±3±2±3±3±2±2±3DDPG (α= 30.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
54 54 5457 59 5957 62 63
±2±4±6±4±6±3±3±4±3DDPG (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
57 60 6061 57 6860 64 62
±10±9±10±7±6±8±12±8±5DDPG (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
28 44 5142 41 6053 57 64
±8±22±10±9±24±9±6±5±12DDPG (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 31 31 3132 32 3131 30 30
±1±2±1±2±1±1±2±1±1SfBC (N= 1)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
46 53 5346 51 5145 50 52
±2±4±4±6±5±4±5±5±7SfBC (N= 16 )
0.1M 0.3M 1M
Value Data0.1M0.3M1M
46 55 5447 52 5344 51 52
±3±4±5±6±8±6±4±3±6SfBC (N= 64 )d4rl-hopper (SARSA )
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 65 62 6367 69 6768 66 68
±4±3±5±3±3±5±4±5±4AWR (α= 0.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
66 69 7470 74 7672 76 77
±4±2±1±3±3±2±2±2±2AWR (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
57 63 7063 66 7367 69 76
±4±8±5±6±4±6±5±4±3AWR (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
52 54 6160 60 6363 65 69
±3±7±10±3±5±3±5±6±5AWR (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 75 74 7577 78 7779 77 79
±2±3±4±3±3±2±2±2±1DDPG (α= 30.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
76 80 8280 81 8280 81 83
±3±2±2±2±2±3±4±2±1DDPG (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
37 75 8051 80 8647 78 86
±8±8±4±11±6±2±10±7±3DDPG (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
7 17 407 38 5510 42 45
±4±10±12±4±14±11±4±20±18DDPG (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 57 56 5859 60 5958 59 58
±4±4±2±4±2±1±3±2±4SfBC (N= 1)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
59 67 7758 68 7859 70 80
±6±3±4±4±5±4±3±5±3SfBC (N= 16 )
0.1M 0.3M 1M
Value Data0.1M0.3M1M
55 64 7757 68 7856 68 79
±7±3±6±6±4±6±3±5±4SfBC (N= 64 )d4rl-walker2d (IQL )
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 63 64 6468 69 6869 66 67
±7±3±3±3±2±3±5±6±3AWR (α= 0.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
71 75 7874 78 7976 78 80
±3±3±2±2±1±2±2±2±4AWR (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
69 76 8272 79 8275 80 82
±3±2±1±4±2±1±4±1±1AWR (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
68 79 8173 79 8373 81 83
±4±3±3±2±3±1±4±1±1AWR (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 77 80 8079 80 8279 81 81
±3±2±2±1±1±1±3±2±1DDPG (α= 30.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
81 82 8382 83 8482 84 85
±2±2±1±1±1±1±1±1±1DDPG (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
69 84 8573 84 8567 85 85
±10±1±0±13±1±0±8±1±0DDPG (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
21 73 8029 83 8628 84 85
±9±10±18±15±3±1±15±4±1DDPG (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 55 54 5959 59 5858 58 59
±4±4±4±2±3±3±3±2±2SfBC (N= 1)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
74 83 8575 83 8574 83 85
±3±2±0±2±1±0±3±1±0SfBC (N= 16 )
0.1M 0.3M 1M
Value Data0.1M0.3M1M
73 84 8572 83 8573 84 86
±2±1±0±3±1±0±2±1±0SfBC (N= 64 )d4rl-walker2d (SARSA )210.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 73737373818182818687878789898889
±1±1±1±1±1±1±1±1±1±1±1±1±2±1±1±1AWR (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
7373737482818384899090969599108111
±1±1±0±1±1±1±1±1±2±1±1±2±2±1±3±2AWR (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
767881868285891018890971249698114191
±1±0±1±1±2±1±1±2±1±1±1±1±1±1±2±8AWR (α= 10.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
76767984838488968991971209499112171
±1±1±1±1±1±1±1±1±1±1±1±2±2±1±3±5AWR (α= 100.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 7677828286921021049410413515397113206267
±1±1±1±2±1±1±1±1±1±2±2±3±1±2±5±5DDPG (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
5966809573991352427711521237163117241397
±3±4±3±3±6±3±4±12±7±6±14±12±17±11±11±18DDPG (α= 0.1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
5275105143731181642806912722339854117234425
±14±7±4±8±7±3±9±14±12±5±14±25±15±11±19±13DDPG (α= 0.01)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
6996127180821432203528314525039274131226412
±8±8±6±67±9±2±12±16±10±3±8±25±11±12±17±21DDPG (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 73727373797879798080808079787979
±1±1±1±1±1±1±1±1±1±1±1±1±0±1±2±1SfBC (N= 1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
75758089848710719487921253058895138338
±1±1±1±2±2±1±1±3±1±1±2±4±1±1±2±7SfBC (N= 16 )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
75768296858911525288951383398997151368
±1±1±1±2±2±1±2±3±1±1±2±6±1±1±4±10SfBC (N= 64 )exorl-walker (IQL )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 73737373818281818787878789898989
±1±1±1±1±1±1±1±1±1±2±1±1±1±1±2±1AWR (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
74737373838283848990889194979999
±1±1±1±1±1±1±1±1±1±1±1±1±1±1±2±1AWR (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
75767982828489958891981149698113146
±1±1±1±1±1±1±1±1±1±1±1±1±2±1±2±4AWR (α= 10.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
75757882828387948990961099497106128
±1±1±1±1±0±1±1±1±2±1±1±2±1±2±3±3AWR (α= 100.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 75788080869194949410110810698108125123
±1±1±1±1±2±1±1±0±2±1±2±1±1±2±4±4DDPG (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
6477108118791111531608113019221576125218249
±3±3±2±2±5±3±5±4±7±6±4±5±12±13±9±6DDPG (α= 0.1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
7091146208781332102967614524634768119270351
±3±5±13±10±7±6±14±13±8±9±13±11±13±22±22±11DDPG (α= 0.01)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
7297188251821632522918012125134177113236330
±21±45±13±13±9±10±13±108±11±40±15±8±10±22±18±16DDPG (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 73737374797979788079807979797979
±1±1±1±1±1±1±1±1±1±1±1±1±1±1±1±1SfBC (N= 1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
75758397838911216287941302158896140236
±1±1±1±1±1±1±1±2±1±1±1±5±1±1±1±4SfBC (N= 16 )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
7577861058590122191889714424289100156256
±1±1±1±1±1±1±2±4±1±1±1±4±1±1±2±6SfBC (N= 64 )exorl-walker (SARSA )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 31313333363637344239404240413940
±1±2±3±2±3±3±2±3±4±2±3±4±3±4±2±5AWR (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
3241485036475759375577873982102122
±1±5±1±3±3±7±3±3±2±14±4±5±4±16±4±3AWR (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
35425864364774903755841193981109153
±2±6±3±2±3±6±4±4±3±15±3±4±4±16±9±6AWR (α= 10.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
32415459364772823857841103881109147
±1±6±2±5±3±5±2±3±2±14±4±3±4±15±4±4AWR (α= 100.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 64164671255849618781151281763137161
±2±6±3±4±7±18±4±4±7±12±8±4±4±25±10±4DDPG (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
7396710114579915119841162051766139245
±1±6±13±6±7±20±5±5±9±9±6±14±5±25±9±18DDPG (α= 0.1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
7417313917619818220801222131967138233
±3±4±12±9±10±20±7±5±11±7±6±15±5±24±8±13DDPG (α= 0.01)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
214375142246510320322711312351264129240
±7±8±9±7±13±16±5±8±8±6±9±10±8±23±14±15DDPG (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 32323233373737364039424236353534
±2±2±1±3±3±1±1±1±2±2±2±3±2±2±1±1SfBC (N= 1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
313540543642611083846961693251106185
±2±2±2±3±2±5±3±4±3±6±1±4±3±11±3±4SfBC (N= 16 )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
3235436436447013336491031903155109198
±2±2±1±5±2±7±2±5±3±9±3±6±4±13±4±8SfBC (N= 64 )exorl-cheetah (IQL )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 34343333363836374340424140424041
±3±3±2±2±3±1±2±2±4±2±3±3±3±4±3±4AWR (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
3843474958545658727470779095105110
±3±3±2±3±2±3±3±3±3±3±2±2±6±2±5±3AWR (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
374351575450697766748610891103119153
±2±2±2±1±2±4±3±3±2±2±3±6±7±7±5±2AWR (α= 10.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
3639495247456071637079998897114143
±1±3±4±6±4±5±5±2±4±5±3±3±4±4±5±2AWR (α= 100.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 575351617071728185939710794107112139
±3±5±4±3±2±4±4±3±4±6±5±4±3±6±6±5DDPG (α= 1.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
48317410280871091369311513018598122138228
±13±7±8±4±11±8±7±5±7±8±9±8±7±8±6±9DDPG (α= 0.1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
54399113676951231729411914021893124152250
±9±5±13±10±8±11±4±10±5±8±6±6±8±6±7±11DDPG (α= 0.01)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
755088157901021231808512715222490127159244
±11±18±9±7±8±8±8±7±7±9±6±12±4±6±6±11DDPG (α= 0.0)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10MPolicy Data 33343332373936374341424235363434
±1±2±1±1±2±3±3±1±2±2±2±2±2±2±2±2SfBC (N= 1)
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
36384151475261976571981647276105195
±1±3±2±2±3±3±3±4±3±3±5±5±1±1±5±4SfBC (N= 16 )
0.1M 0.3M 1M10M
Value Data0.1M0.3M1M10M
3839446251587411872801091877682114216
±2±3±2±4±3±4±4±4±3±3±4±4±3±3±7±6SfBC (N= 64 )exorl-cheetah (SARSA )
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 37 34 3446 42 4038 38 38
±7±8±6±9±8±7±6±6±4AWR (α= 0.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
39 49 7062 72 8296 97 95
±3±3±3±2±2±8±2±2±3AWR (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
40 42 5376 84 8796 98 97
±5±2±3±4±5±4±3±1±2AWR (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
44 50 5880 91 9397 98 96
±3±4±3±6±3±3±2±2±2AWR (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 63 65 7177 93 9488 96 96
±5±4±4±12±4±3±11±3±3DDPG (α= 300.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
67 72 8080 95 9869 98 99
±7±8±3±7±3±1±9±2±1DDPG (α= 100.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
26 51 799 53 905 57 95
±11±9±9±9±10±5±8±13±5DDPG (α= 30.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
3 16 522 16 581 9 64
±2±5±6±2±12±7±2±7±18DDPG (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 25 30 2528 27 2724 25 24
±5±5±4±3±2±4±3±2±2SfBC (N= 1)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
55 63 7768 82 9266 80 90
±4±5±4±4±2±2±2±3±4SfBC (N= 16 )
0.1M 0.3M 1M
Value Data0.1M0.3M1M
55 63 7765 76 8763 72 83
±3±4±3±5±3±2±2±3±7SfBC (N= 64 )kitchen (IQL )
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 34 36 3246 38 4040 36 40
±4±4±7±6±8±5±8±6±6AWR (α= 0.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
30 43 6758 68 8393 94 95
±7±5±3±3±5±5±2±3±2AWR (α= 1.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
37 41 4971 82 9095 97 98
±4±3±4±7±6±3±3±4±1AWR (α= 3.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
40 48 5374 93 9596 97 97
±4±3±4±6±3±2±2±2±1AWR (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 59 63 6474 87 7480 88 66
±5±5±5±6±6±9±8±6±13DDPG (α= 300.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
58 71 8068 93 9764 99 98
±5±6±4±7±4±2±15±2±2DDPG (α= 100.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
26 45 7710 57 937 49 97
±13±7±6±9±17±2±9±13±2DDPG (α= 30.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
3 10 552 8 562 5 69
±1±5±8±2±6±12±2±3±9DDPG (α= 10.0)
0.1M 0.3M 1M
Value Data0.1M0.3M1MPolicy Data 27 25 2629 25 2627 23 25
±4±4±4±4±3±3±2±3±2SfBC (N= 1)
0.1M 0.3M 1M
Value Data0.1M0.3M1M
53 60 7469 79 9364 77 90
±3±6±5±2±3±1±2±5±2SfBC (N= 16 )
0.1M 0.3M 1M
Value Data0.1M0.3M1M
55 60 7467 73 8959 67 82
±3±6±5±4±3±2±3±7±2SfBC (N= 64 )kitchen (SARSA )
0.1M 1M
Value Data0.1M1MPolicy Data9 817 19
±6 ±4±5 ±6AWR (α= 0.0)
0.1M 1M
Value Data0.1M1M
8 722 21
±3 ±3±4 ±3AWR (α= 0.1)
0.1M 1M
Value Data0.1M1M
13 1225 23
±3 ±3±2 ±3AWR (α= 1.0)
0.1M 1M
Value Data0.1M1M
17 1730 27
±3 ±3±3 ±4AWR (α= 10.0)
0.1M 1M
Value Data0.1M1MPolicy Data12 1222 23
±2 ±3±2 ±2DDPG (α= 100.0)
0.1M 1M
Value Data0.1M1M
14 1323 23
±2 ±3±2 ±2DDPG (α= 30.0)
0.1M 1M
Value Data0.1M1M
15 1620 22
±3 ±5±3 ±4DDPG (α= 10.0)
0.1M 1M
Value Data0.1M1M
15 1710 12
±7 ±7±4 ±7DDPG (α= 3.0)
0.1M 1M
Value Data0.1M1MPolicy Data7 717 18
±2 ±4±3 ±4SfBC (N= 1)
0.1M 1M
Value Data0.1M1M
9 1015 17
±3 ±4±2 ±4SfBC (N= 16 )
0.1M 1M
Value Data0.1M1M
10 1014 15
±4 ±4±2 ±5SfBC (N= 64 )gc-roboverse (IQL )
0.1M 1M
Value Data0.1M1MPolicy Data8 620 18
±6 ±5±4 ±5AWR (α= 0.0)
0.1M 1M
Value Data0.1M1M
8 619 14
±4 ±4±4 ±5AWR (α= 1.0)
0.1M 1M
Value Data0.1M1M
8 618 15
±4 ±2±4 ±6AWR (α= 10.0)
0.1M 1M
Value Data0.1M1M
7 617 14
±4 ±2±4 ±6AWR (α= 100.0)
0.1M 1M
Value Data0.1M1MPolicy Data7 721 22
±2 ±2±3 ±3DDPG (α= 100.0)
0.1M 1M
Value Data0.1M1M
7 621 23
±2 ±2±3 ±3DDPG (α= 30.0)
0.1M 1M
Value Data0.1M1M
7 620 24
±2 ±2±2 ±5DDPG (α= 10.0)
0.1M 1M
Value Data0.1M1M
9 723 23
±3 ±3±5 ±5DDPG (α= 3.0)
0.1M 1M
Value Data0.1M1MPolicy Data10 820 22
±3 ±2±5 ±3SfBC (N= 1)
0.1M 1M
Value Data0.1M1M
9 817 14
±3 ±2±6 ±2SfBC (N= 16 )
0.1M 1M
Value Data0.1M1M
9 715 11
±3 ±3±7 ±2SfBC (N= 64 )gc-roboverse (CRL )Figure 11: Full data-scaling matrices of A WR, DDPG+BC, and SfBC with different hyperparameters.
22NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Yes, we support the claims made in the abstract and introduction with empirical
results.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See Appendix A.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
23Justification: This is an empirical analysis paper.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide the full experimental details as well as the code.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
24Answer: [Yes]
Justification: See the supplementary materials.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperpa-
rameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [Yes]
Justification: See Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report 95% bootstrap confidence intervals or standard deviations for all of
the plots in the paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
25•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: See Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Yes, we follow the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is purely algorithmic research.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
26generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This is purely algorithmic research.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We properly acknowledge the code and datasets we use in the paper.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
27Answer: [NA]
Justification: We do not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing or human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing or human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
28