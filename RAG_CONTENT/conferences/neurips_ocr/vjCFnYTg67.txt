Bileve: SecuringText Provenancein Large Language
Models Against Spoofing with Bi-level Signature
Tong Zhou
Northeastern University, Boston
zhou.tong1@northeastern.eduXuandong Zhao
UC Berkeley
xuandongzhao@berkeley.edu
Xiaolin Xu
Northeastern University, Boston
x.xu@northeastern.eduShaolei Ren
UC Riverside
shaolei@ucr.edu
Abstract
Text watermarks for large language models (LLMs) have been commonly used
to identify the origins of machine-generated content, which is promising for as-
sessing liability when combating deepfake or harmful content. While existing
watermarking techniques typically prioritize robustness against removal attacks,
unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly
alter the meanings of LLM-generated responses or even forge harmful content,
potentially leading to the wrongful attribution of blame to the LLM developer. To
overcomethis,weintroduceabi-levelsignaturescheme, Bileve,whichembeds
fine-grained signature bits for integrity checks (mitigating spoofing attacks) as
wellasacoarse-grainedsignaltotracetextsourceswhenthesignatureisinvalid
(enhancingdetectability)viaanovelrank-basedsamplingstrategy. Comparedto
conventionalwatermarkdetectorsthatonlyoutputbinaryresults, Bilevecandiffer-
entiate5scenariosduringdetection,reliablytracingtextprovenanceandregulating
LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the
effectiveness of Bilevein defeatingspoofing attacks with enhanceddetectability.
Code is available at https://github .com/Tongzhou0101/Bileve-official .
1 Introduction
Watermarkshave beenenvisionedasa promisingmethodtodifferentiate contentgeneratedby large
language models (LLMs) from human [ 5,11,23,13,1,4]. It involves injecting statistical signals
intothetokensamplingprocessutilizingasecretwatermarkkey. Subsequently,theonewhoknows
the key can verify the content’s origin by assessing the presence of the predefined signal through
astatisticaltest. Currentwatermarkingschemesprimarilyfocusonuser-sideconcerns,strivingto
achieve robustness against watermark removal attacks (i.e., perturb the generated text to remove the
watermark), thereby combatting academic dishonesty and other deceptive practices.
However, a critical vulnerability remains unaddressed in watermark design: spoofing attacks directed
towardsmodelowners . Intheseattacks,maliciousactorsattempttofalselyattributecontentgenerated
byhumansorothermodelstothetargetedmodel,withtheaimofevadingaccountability1ordamaging
themodel’sreputation. Afewrecentworkshaveidentifiedtwokindsofspoofingattackstargetingthe
LLM watermark by exploiting either its symmetric characteristic [ 6,15] or learnability [ 8,10], as
detailed in Sec.2.3. Furthermore, we propose a new spoofing attack, named semantic manipulation,
1Tesla lawyers claimed that Elon Musk’s past statements about self-driving safety could be deepfakes, which
is found suspicious by the court [7].
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Table 1: Comparison of different watermarking methods based on four desired properties.
Methods Robust Unforgeable Tamper-evident Transparent
Kirchenbauer et al. [11] ✓ ✗ ✗ ✗
Zhaoet al. [23] ✓ ✗ ✗ ✗
Kuditipudi et al. [13] ✓ ✗ ✗ ✗
Liuet al. [15] ✓ ✗ ✗ ✗
Fairozeet al. [6] ✗ ✓ ✓ ✓
which enables attackers to alter the sentiment of generated content with minimal token modifications,
asdescribedinSec.3.2. Itassumesthemostconstrainedcapabilitiesofattackers,wheretheyonlyhave
accesstothevictimmodel’sdetector. DuetotherobustnessofLLMwatermarksagainstperturbations,
this attack can manipulate the originally helpful content into something harmful or offensive without
compromising the detectability of the watermark, thus successfully achieving spoofing attacks.
Giventheseriousconsequencesofspoofingattacks,itishighlyindemandtoanswerthequestion:
How to avoid an LLM being wrongly blamed? Tosolvethisproblem,weaimtodesignawatermark
for LLM, which focuses more on the model owners’ side instead of only watermarking on the users’
side. Toreliablyidentifytheprovenanceofmachine-generatedcontentwhilebeingabletodefend
against the above spoofing attacks, a signature should have the following properties:
•Robust:The signature remains capable of tracing the source of machine-generated text even when
subjected to certain perturbations, ensuring it is not overly fragile or easily rendered ineffective.
•Unforgeable: The signature is inherently resistant to being learned given the components utilized
in its detection.
•Tamper-evident: Itshouldbeabletochecktheintegrityofthegeneratedcontent,showingreliable
tampering evidence to safeguard the interests of model owners.
•Transparent: Itisdetectablewithoutneedingaccesstogenerationsecretsorrelyingonablack-box
API, allowing independent, reliable verification.
Despite the critical importance, achieving all desired properties in a single LLM watermark remains
challenging, as even state-of-the-art (SOTA) designs cannot meet them all (see Tab. 1). Indeed,
designingsuchawatermarkingschemeinvolvesafundamentaltrade-offbetweendefendingagainst
removalattacksandspoofingattacks. Specifically,beingrobusttoremovalattacksrequiresthatthe
watermark’sdetectabilityremainsunaffectedbycertainperturbations, whileanti-spoofingdemands
sensitivitytoperturbationstoverifytextintegrity,distinguishingharmfulcontentfromgenuinemodel
output and tampered content.
To overcome the above challenges, we propose Bileve, a novel sampling strategy by embedding
abi-levelsignatureintogeneratedtokens. Atthecoarse-grainedlevel,weutilizestatisticalsignals
acrosstheentiretexttodetectthepresenceofthewatermark,ensuringrobustnessagainstperturbations.
Concurrently, at the fine-grained level, we integrate content-dependent signature bits into each token
toupholdcontentintegrity,whichleveragesadigitalsignatureschemetoensureunforgeability,asthe
secretkeyrequiredforwatermarkembeddingwillbesecurelyheldbymodelowners. Thisscheme
enables transparent detection by allowing verification with a public key instead of embedding secrets,
so independent parties can authenticate without proprietary details or a black-box API, ensuring
reliable detection. And the tampering evidence will show when these two level detectionresults are
not consistent.
Our contributions are threefold: 1) We uncover an advanced spoofing attack thatexploits the robust-
ness of SOTA watermarking schemes; 2) We introduce Bileve, the first watermarking scheme to
simultaneously ensure robustness and unforgeability by embedding a bi-level signature through a
novel rank-based sampling strategy; 3) Bileveis capable of distinguishing five distinct scenarios
duringthedetectionphase,effectivelydefeatingspoofingattacksandservingasapromisingtoolto
regulate LLM safety mechanism.
2 Background and Related Works
2.1 Language Model Basics
Letdenote a language model with a vocabulary containing𝐾∶=||tokens. To generate
the next token 𝑤𝑡,will take prior tokens 𝑤1∶𝑡−1as the input and output a vector of logits 𝑙(𝑡),
2Table 2: Three categories of spoofing attacks (ranked by attackers’ capabilities from high to low).
Methods Exploited Vulnerabilities Attackers’ Capabilities
[6, 15] Symmetry Know the secret key for embedding watermarks
[8, 10] Learnability Get access to the victim model and query it multiple times
Our attack Robustness Only get access to the victim models’ detector
which is transformed into a probability distribution (𝑡)= (𝑝(𝑡)
1,...,𝑝(𝑡)
𝐾)via the softmax function.
Then the sampling strategy is applied to determine how the model selects 𝑤𝑡based on (𝑡). One
common sampling strategy is multinomial sampling, where randomly selects the next token from
according to the probabilities 𝑝(𝑡)
𝑘assigned to each token. This process is repeated iteratively to
generate a sequence of tokens.
2.2 LLM Watermarks
Watermarks for model-generated texts are used to identify the provenance of the text, ensuring
accountability in cases where generated content needs to be traced back to a specific LLM. The
existing watermark schemes rely on the specialized decoding algorithm to embed statistical signals
into generated contents, then enabling watermark detection via computing p-value [ 1,11,23,4,13].
For instance, for generating the next token, one approach dynamically partitions the vocabulary into
greenandredlistsbasedonitspreviousfewtokensandawatermarkkey[ 11],thenincreasingthelogits
of green tokens to enhance their chance of being selected. During detection, the watermark detection
key is used to count the number of green tokens in the text, with the calculated z-statistic indicating
the existence of the watermark. Moreover, Zhao et al. [23] simplify the scheme proposed in [11] by
fixing the green-red list for each token, demonstrating that their watermark is twice as robust to edit
as[11]. Furthermore,unlikemodifyinglogits,adistortion-freewatermarkisproposedtopreserve
theoriginaltextdistribution[ 13]. Itleveragesrobustsequencealignmenttoalignwatermarkedtextto
a watermark key sequence in the sampling phase, e.g., using exponential minimum sampling.
However, these watermarking schemes only enable detection by individuals possessing the key,
which doesn’t facilitate transparent regulation. On one hand, making the key public is susceptible
to attacks [ 15]. On the other hand, maintaining detection privately (e.g., via APIs) compromises
reliability, as it functions as a black box, allowing the model owner to manipulate detection results.
2.3 Spoofing Attacks
Spoofing attacks can fall into three categories based on the capability of attackers, and each of them
exploits different vulnerabilities in SOTA watermarks, as summarized in Tab. 2. First, due to the
watermarkembeddinganddetectionprocesssharingthesamesecretkey(i.e.,symmetricschemes),
the semi-honest detector knowing the secret key can embed the watermark to any content. Such a
vulnerability has been discussed in [ 6,15], where they design asymmetric watermarking schemes so
that detection does not rely on the key used for embedding. Specifically, [ 6] proposes embedding
watermarksusingdigitalsignaturealgorithms,ensuringthatonlymodelownerspossessthesecret
key for watermark embeddingwhile providing detectors withaccess to thepublic key for detection.
However, it is easily broken once the message tokens are perturbed. Besides, it suggests using
error-correcting encoding to improve robustness, which unfortunately would increase the risk of
spoofing attacks and should not be adopted. Moreover, [ 15] employs two distinct neural networks for
watermark embedding and detection, leveraging an asymmetric scheme for public detection.
However, [ 15] is based on [ 11] and thus can be learned as studied in [ 8], which is the second kind of
spoofingattacks. Inparticular,[ 8]demonstratedthat,byqueryingthevictimmodelandcollecting
its watermarked samples, attackers can fine-tune an adversary model utilizing a sampling-based
watermark distillation technique to learn the watermark (detailed in Appendix A). The fine-tuned
adversary model can respond to any malicious requests, with the response containing the watermark
ofthevictimmodel. Recently,anindependentworkalsoproposedattacksbyexploitingwatermark
robustness [ 18]. Beyond techniques such as randomly inserting toxic tokens or modifying tokens to
alter sentence accuracy, our attack leverages a reward model to guide targeted semantic manipulation.
More importantly, while they suggest compromising watermark robustness to mitigate spoofing
attacks, our work demonstrates how to achieve effective mitigation without sacrificing robustness.
33 Potential Attack: Semantic Manipulation
3.1 Threat Model
Attackers’ Objective. Given text generated by the victim LLM, attackers seek to alter the semantic
meaning of the text with minimal changes, transitioning it from something helpful or neutral to
harmfuloroffensive. Owingtotherobustnessoftheexistingwatermarks,thewatermarkdetectorcan
still identify the presence of the watermark in the modified content. Consequently, the altered content
erroneously attributes its origin to the victim model, potentially damaging the model’s reputation.
Attackers’ Capability. Contrary to existing spoofing attacks, our approach assumes the strictest
attacker capabilities, where theadversaryonlygains accessto thewatermark detector, asoutlined
inTab.2. Theseattackerslackknowledgeofthesecret keyandarenotrequiredtoquery thevictim
LLM multiple times to acquire watermarked samples for training other adversary models. They may
utilize public language models to enhance the efficiency of their attacks.
3.2 Attack Method
Weidentifythedilemmaofbeingrobustandunforgeable. Inparticular,beingrobustindicatesthatthe
watermarkshould bepreserved afterperturbation, thuswe canexploit thischaracteristic todesigna
novel spoofing attack. Specifically, attackers can query the victim model with harmless prompts, and
thenusebasicwordreplacementtechniquestochangeitssemanticmeaningtobetoxicorharmful.
Due to the robustness properties of LLM watermarks, the detectability will not be compromised
if the portion of word replacement is low. Consequently, a detector cannot discern whether the
contentoriginatedfromthevictimmodelorwasmanipulatedbymaliciousactors. Thishighlightsthe
limitation of current watermarks for auditing LLMs.
Byexploitingtheaboveobservation,weproposeanovelspoofingattack. Let 𝑤origdenotetheoriginal
responseofvictimmodels,and 𝑤attrepresentitsmanipulatedversion. Thegoalistogenerate 𝑤attthat
maximizes the change in sentiment while minimizing the Levenshtein distance between the original
and manipulated responses. The problem can be formulated as follows:
max
𝑤attΔ𝑅=𝑅(𝑤orig)−𝑅(𝑤att),s.t.LD(𝑤orig,𝑤att)≤𝜖𝑇 (1)
Here,Δ𝑅represents the sentiment change, defined as the difference between the reward scores
(denoted by𝑅(⋅)) of the original and manipulated responses obtained by the reward model.2A lower
scoreoftherewardmodelindicateslessalignmentwithhumanfeedback,suchasatoxicresponse. The
Levenshtein distance,denoted by LD(𝑡1,𝑡2), measurestheminimum numberof wordedits required
to transform text 𝑡1into text𝑡2.𝑇is the length of 𝑤origand𝜖is the word edit budget. A trade-off
existsinchoosing 𝜖,whereinalargervalueaffordsgreaterflexibilityinmanipulatingthesemantic
meaning of 𝑤orig, while a smaller value better preserves the detectability of the watermarks. To
strikeabalance,weoptforalarger 𝜖tomaximizesemanticalterationandintroduceatuningfactor
𝛼∈(0,1)to adjust𝜖in case thedetectability is broken. Furthermore, insteadof manually replacing
the words in 𝑤orig, attackers can simply leverage a powerful and accessible LLM (denoted as 𝑄)
toexecutesuchattacksefficiently. Toenhancethegenerationqualitywhilemeetingtheconstraint,
attackerscanapplyin-contextlearningbyprovidingafewtaskdemonstrations. Wesummarizethe
algorithm with more details of task demonstrations in Appendix B.
4 Proposed Defense
With the above attack, in conjunction with other existing spoofing attacks, we can recognize the
vulnerabilityofcurrentwatermarkschemes. Thisunderscorestheimportanceofdesigningsecure
schemestodefendagainstspoofingattacksandachieveallpropertieslistedinTab.1,ensuringreliable
identification of text provenance.
2For example, we use the following reward model in our experiment: https://huggingface .co/
OpenAssistant/reward-model-deberta-v3-large-v2
4Signature embedding (illustrated with 𝐵!)Regulating language models is Message(first m tokens) 
18fb7a48…𝐵!= 0ℎ𝑤"==𝐵!crucialCandidate tokens ranked by WRAHash houtputs
Generate digestSecret keyMessage digestSignature bits BFirst  𝑤" matches 𝐵! Output token with 𝐵! 
⋯Embed the rest of B
𝑤#: important𝑤$: crucial 𝑤%: urgent⋮ℎ(𝑤#) = 1ℎ(𝑤$) = 0ℎ(𝑤%) = 0⋮
Message digest(a)
0100010011001001001010101000100…Regulatinglanguage models is crucial to ensure they are…Extracted signature bits Public key
18fb7a48…Extract signature bits with	ℎDecrypted resultExtract messages18fb7a48…
=
0100010011001001001010101000100…Integrity check
Run statistical test(b)
Figure 1: Overview of Bileve. (a) Embedding: The first𝑚tokens from form the message,
which is signed using a secret key. Candidate tokens are selected via a rank-based strategy employing
a Weighted Rank Addition (WRA) score, with a coarse-grained signal embedded. It then embeds
the fine-grainedsignature bychoosingthe firstcandidate matching thedesignated signaturebit. (b)
Detection: We first extract the message-signature pair to conduct an integrity check using the public
key. A statistical test is performed if necessary.
4.1 Single-level Signature
Toenablesecureandreliabletextattribution,wefirstexaminethevulnerabilitiesexploitedbyattackers
in conducting spoofing attacks, including symmetry, learnability, and robustness, as summarized
in Tab. 2. In particular, previous methods embed the statistical signal into the generated texts
[11,23,13,1], so as to identify the existence of a watermark during detection. Such statistical signal
isconsistentforeverytext,thusenablingtheadversarymodeltolearnthewatermarkrulecanforgeit.
Therefore,todefendagainstspoofingattacks,weseektheoppositecharacteristics,i.e., asymmetric ,
unlearnable , andperturbation-sensitive , in the solution.
Weenvisionthedigitalsignatureasapromisingsolutionasin[ 6],wheretheschemeisdefinedas
SLS=(𝖪𝖾𝗒𝖦𝖾𝗇,𝖲𝗂𝗀𝗇,𝖤𝗆𝖻𝖾𝖽,𝖵𝖾𝗋𝗂𝖿𝗒), where:
•𝖪𝖾𝗒𝖦𝖾𝗇 →(𝗌𝗄,𝗉𝗄)outputs a public key pair (𝗌𝗄,𝗉𝗄), where 𝗉𝗄is public while 𝗌𝗄is held by the
owner of model .
•𝖲𝗂𝗀𝗇𝗌𝗄(𝐦)→𝝈uses 𝗌𝗄to sign the digest of the message 𝐦:=𝑤1∶𝑚via the hash function 𝐻(e.g.,
MD5) and obtain the signature 𝝈=𝗌𝗄×𝐻(𝐦).
•𝖤𝗆𝖻𝖾𝖽(𝝈)→𝑤embeds signature into subsequent tokens following 𝑤𝑚, and outputs 𝑤incorporat-
ing the message-signature pair ( 𝐦,𝝈).
•𝖵𝖾𝗋𝗂𝖿𝗒𝗉𝗄(𝑤)→{𝚝𝚛𝚞𝚎,𝚏𝚊𝚕𝚜𝚎}extracts 𝝈=𝗌𝗄×𝐻(𝐦)from𝑤andverifiesitusingthepublickey
𝗉𝗄. If verification succeeds, it outputs 𝚝𝚛𝚞𝚎; otherwise, it outputs 𝚏𝚊𝚕𝚜𝚎.
Unlike the digital signature methods, which typically attach signatures as metadata [ 20,19],SLS
assigns the first few tokens as the message and uses the following tokens to carry the signatures.
Specifically, the key idea of 𝖤𝗆𝖻𝖾𝖽is to embed signature bits into tokens, ensuring that the block
hashes to the corresponding signature bit (e.g., employing rejection sampling until the hash ℎresult
matches the next signature bit). This method keeps the message-signature pair self-contained within
thegeneratedtext,enablingverificationsolelybasedonthegeneratedcontent. Suchaschemesatisfies
theabovecharacteristicssinceadigitalsignatureusesthesecretkeyforembeddingandthepublic
key for verification, ensuring asymmetry. The signature is content-dependent, so the signature for
differentgenerationsisalsodifferent. Also,itisdependentonthesecretkey,whichcannotbeinferred
byattackers,makingitimpossibletolearn. Itsabilitytocheckintegrityisprovedincryptography,
where even a single modification will cause verification failure.
However, two problems arise in this scheme: 1) digital signature is too fragile, which hinders its
applicabilitytotherealworldforattributingthetext. Inparticular,evenasingletokeninsertionor
deletion would lead to a verification failure, and the trace of the target LLM will easily disappear.
2) In cases wheretoken replacement occurs and the replaced tokenhashes to the same signature bit
as the original token, the signature remains unaffected. However, such replacements undermine the
text’s integrity without detection, which is referred to as the “signature preservation attack”.
5Algorithm 1 Rank-based Sampling Strategy in Bileve
Require: Language model , secret key 𝗌𝗄, message length 𝑚, random key sequence Ξ
1:Apply cyclic shift to Ξ
2:for𝑡=1,⋯,𝑚do
3:Apply to prior tokens and sample 𝑤𝑡withΞ𝑡involved (Eq. 2)
4:end for
5:Apply a hash function on 𝑤1∶𝑚to get the digest of message
6:Use𝗌𝗄to sign the digest to obtain the signature and convert it into a bit string 𝐵
7:for𝑡=𝑚+1,⋯,𝑚+𝑏+1do
8:Apply to prior tokens to get a score vector 𝑊𝑅𝐴(𝑡)over
9:{𝑤𝑡,1,⋯,𝑤𝑡,𝐾}←Sorted tokens based on their logits in descending order
10:for𝑘=1to𝐾do
11:ifℎ(𝑤𝑡,𝑘)=𝐵𝑡−𝑚then
12:𝑤𝑡←𝑤𝑡,𝑘;break
13:end for
14:end for
4.2 Bi-level Signature ( Bileve)
Weintroduce Bileve,abi-levelsignatureschemethatimprovesuponthe SLSintermsofdetectability
and security. At the fine-grained level, Bileveembeds the message-signature pair to verify content
integrity,whilethecoarse-grainedlevelincorporatesarobustsignaltoboostdetectability. Follow-
ing[1,13],wedesignthesignalasarandomwatermarkkeysequence 𝜉∼Unif([0,1]𝐾). Wepropose
a ranking-based sampling strategy to embed 𝜉into generated tokens, where the objective is to let
therandomnessaffectthesamplingoutcomebuttheselectedtokenisalsoexpectedtohavealarge
probability of preserving the generation quality.
Generation. We propose a weighted rank addition ( 𝑊𝑅𝐴) score for each token in to rank the
candidate tokens instead of ranking them based on probability like conventional methods [ 14]. In
particular,givenaprobabilityvector 𝑝of𝑤𝑡andapre-definedrandomsequence 𝜉(bothofdimension
𝐾),𝑊𝑅𝐴is calculated by (we omit 𝑡for simplicity):
𝑊𝑅𝐴𝑘=R(𝑃𝑘)+𝛾⋅R(𝜉𝑘), 𝑘∈[1,𝐾] (2)
whereR(𝑝𝑘)andR(𝜉𝑘)arethe rankscores for 𝑘-thtoken basedon 𝑝and𝜉, respectively,determined
bytheirorderwhenvaluesaresortedinascendingorder(e.g.,if 𝑝𝑘isthesmallestonein 𝑝,thenR(𝑝𝑘)
is 0). Besides, by adjusting the hyperparameter 𝛾(where𝛾 <1), we enhance the impact of higher
probabilitieswhilestillallowingforrandomnesstoaffecttheoutcome. Duringgeneration,werank
tokencandidatesbyfavoringlarger 𝑊𝑅𝐴(1inFig.1). Whensampling tokenscarryingsignature
bits,weincorporateanadditionalsignaturebitmatchingstepbyselectingthefirstcandidatetoken
that, through the hash function ℎ, maps to the predetermined signature bit ( 2in Fig. 1).
Besides, we enhance the diversity of generation by using the shift-generate algorithm [ 13] (detained
inAppendixC).Thisinvolvespre-generating 𝑛𝜉sequencesanditerativelydecodingtokensusing
sequencesΞ= (𝜉𝑑,𝜉𝑑+1, ...,𝜉𝑛,𝜉0, ...,𝜉𝑑−1), where𝑑∈ [0,𝑛)shifts with each new response
generation. Suchashiftingstrategyensuresthat cangeneratediversetokenseveniftheirprefix
tokensarethesame,anditerativedecodingensuresthatgeneratedtokens 𝑤alignwellwith Ξ. The
rank-based sampling strategy with shift-generate is summarized in Alg. 1. Thus, although a signature
preservationattackmaymaintainalignmentwiththesignature,itislesslikelytosimultaneouslyalign
well withΞsequences, thereby effectively mitigating such attacks.
Statistical Test. Following [13], we define the alignment cost as
𝑑(𝑤,Ξ)∶=1
𝑇𝑇∑
𝑡=1log(1−Ξ𝑡,𝑤𝑡) (3)
If the text𝑤generated by ,Ξ𝑡,𝑤𝑡will be large due to Eq. 2, then 𝑑will be smaller compared to
human-generated text or text from other models. Thus, we test 𝑤𝑡with random Ξ′for𝑁times, and
got p-value as1
𝑁+1(1+∑𝑁
𝑖=11{𝑑(𝑤,Ξ′)≤𝑑(𝑤,Ξ)}) for the null hypothesis that 𝑤is not generated
by. Hence, a small p-value (e.g., <0.01 when𝑁=100) indicates 𝑤is high likely from . For
6checking the signature preservation attack, we run a local alignment , i.e., splitting 𝑤into several
segments, if the p-value for a certain segment is larger than the rest, then it indicates the token
replacement happens in that segment with their associated signature bits unchanged. When signature
validationfails,weruna globalalignment test,withEq.2enhancedbyLevenshteindistancetobe
robust against insertion and deletion, as detailed in Appendix D.
Detection. Withdetectorsgettingaccessto 𝗉𝗄,ℎ,andΞ,theywillapplytwoprimarymethodsduring
detection: extracting the message-signature pair for integrity verification using the public key 𝗉𝗄,
andconductingstatisticaltests. Theverificationprocessisdescribedasfollows: Step1: Checkthe
signatureatafine-grainedlevel( 1inFig.1). Ifthesignatureisvalidandmodelownersraiseno
doubts,verificationiscompleted,andthetextattributionisassignedtothetargetLLM( Case1).Step
2: Ifthesignatureisvalidbutthemodelowneridentifiessuspiciouscontent(e.g.,potentiallyoffensive
material not in line with their model’s safety mechanisms), they can conduct a local alignment test (
2inFig.1). Abnormalresultssuggestsignaturereplacement( Case2),whilenormalresultssuggest
that there is a high chance that the safety mechanisms of target LLM require improvement ( Case 3).
Step 3: If the signature is invalid, examine the coarse-grained signal through a global alignment test
(2inFig.1). Asmallp-valueservesastamperingevidencethatthecontentoriginatesfromthe
targeted LLM but has been altered ( Case 4). Otherwise, it suggests the text originates from a source
other than the targeted LLM ( Case 5). Overall, Bilevecan differentiate 5 cases with the bi-level
signature, reliably tracing the text provenance with mitigating spoofing attacks.
5 Experiments
Inthissection,weevaluateourapproachfrommultipleperspectives,includingdetectability,generation
quality, and security. Specifically, given that our method is asymmetric and unlearnable due to its
cryptographicdesign,ourfocusissolelyonassessingitsefficacyindefendingagainstspoofingattacks
that exploit robustness, i.e., semantic manipulation. Additionally, we demonstrate the effectiveness of
thebi-levelsignatureintacklingthechallengesencounteredbythesingle-levelsignature,i.e.,fragility
and signature preservation attacks.
5.1 Experimental Setup
Datasets and Models. We conduct experiments using two publicly available LLMs: OPT-1.3B [ 22]
and LLaMA-7B [ 21]. Our evaluation employs two datasets: 1) OpenGen [ 12] for text completion
task,consistingof3Ktwo-sentencesamplesfromWikiText-103[ 17],withthefirstsentenceasthe
promptandthesecondasthehumancompletion;2)LFQA[ 12]forlong-formquestionanswering
task, consisting of 3K question-answer pairs, where we use questions as prompt and answers as
human-written answers in experiments.
Evaluation. To measure detectability, we use metrics, including the True Positive Rate (TPR), False
PositiveRate(FPR),andF-1score. WeuseLLaMA-13Bastheoraclelanguagemodeltocompute
perplexity (PPL) for evaluating the generation quality, which is defined as the exponentiated average
negative log-likelihood of a sequence.
Schemes. To assess the effectiveness of Bileve, we conduct a comparative analysis with two state-
of-the-art schemes. The first scheme, Unigram [ 23], stands out for its robustness against removal
attacks. The second scheme, as proposed in [ 6], employs cryptographic techniques to defeat spoofing
attacks, denoted as the SLSin this work.
Settings. For Unigram, we set watermark strength to 2.0 and a green list ratio to 0.5, where the
thresholdofz-scorefordetectionis6.0andsetFPRas0.01duringdetection. Thenucleussampling[ 9]
isemployedtointroducerandomnessforUnigramand SLS. Also,for SLS,wegenerate300tokens
with the first 44 tokens as the message and the rest 256 tokens as the signature bit (the signature
length for Bileveis 256-bit). This also applies to Bileve, except we use rank-based sampling with
the𝛾set to0.001. Weset 𝑛for shift-generateto 300and 𝑁=100 fordetection. Allexperiments are
conducted on NVIDIA A100 GPUs.
7Table 3: The detectability of different schemes with OPT-1.3B.
Setting MethodOpenGen LFQA
TPR ↑FPR ↓F1↑TPR ↑FPR ↓F1↑
No EditingUnigram 1.000 0.010 0.995 1.000 0.010 0.995
SLS 1.000 0.000 1.000 1.000 0.000 1.000
Bileve 1.000 0.0001.0001.000 0.0001.000
10% EditingUnigram 0.992 0.010 0.991 0.997 0.010 0.994
SLS 0.000 0.000 / 0.000 0.000 /
Bileve 0.998 0.0000.9990.999 0.0000.999
2.5
 2.0
 1.5
Alignment Cost0510152025CountsComparison of Human and Generated Scores
Human
GeneratedFigure 2: The alignment cost
of human vs LLM.
5.2 Detectability
AsdemonstratedinTab.3,weevaluatethedetectabilityofeachschemeundertwoscenarios: noedits
tothegeneratedtext,andeditinginvolving10%ofthetokens(throughrandomdeletion,addition,and
replacement). Intheuneditedscenario,both SLSandBilevesurpassUnigraminFPRandF1scores.
This superiority is due to the use of digital signatures in SLSandBileve, which ensure integrity
bymakingthesignature 𝝈content-dependenton 𝐦andsignedby 𝗌𝗄. Thissetuppreventstextsnot
produced by the target LLM from passing verification with 𝗉𝗄.
Furthermore, Bileveexcels when 10% of tokens are edited, maintaining a high F1 score (0.999) and
achieving an FPR of 0. This contrasts sharply with SLS, whose F1 score becomes inapplicable due to
bothTPRandFPRdroppingto0,illustratingthefragilityofthe SLSscheme. Incontrast, Bileve
canleveragecoarse-grainedlevelsignaltotestglobalalignmentwith Ξ. Theresultingp-value <0.01
indicatesthesourceofperturbedtextisfromthetargetLLM.Thefailureofverificationcausedby
disruptedmessage-signaturepairalongwiththesmallp-valueserveasthetamperingevidencefor
texts from target LLM. Furthermore, alignment cost analysis in Bileve(Fig. 2) shows machine-
generated texts aligning with the key sequence Ξincur lower costs than human-written texts, aiding
in provenance tracing and distinguishing Case 5.
5.3 Generation Quality
Human             Unigram              SLS                BileveHuman             Unigram              SLS                Bileve
Figure3: Theperplexityofapplyingdifferent
schemes to OPT-1.3BWe measure the perplexity of texts generated by var-
ious schemes, with results of OPT-1.3B shown in
Fig.3andLLaMAshowninAppendixE.Theperplex-
ityofUnigramisclosetothatofhumantext,which
serves as our baseline. In contrast, the perplexities of
SLSandBilevearerelativelyhigher. Thisincrease
is attributed to the need for embedding digital signa-
ture bits into tokens precisely. Such embedding may
lead to the selection of tokens that, while matching
the signature bits, are not the optimal choice, thus in-
creasingperplexity. Notably, Bileveusesrank-based
samplingwithshift-generateinsteadof SLS’snucleus
sampling, achieving a 23.08% perplexity reduction
on OpenGen using OPT-1.3B, as tokens with higher
𝑊𝑅𝐴scores better preserve textual coherence.
WhileourmethodexhibitshigherperplexitythanUn-
igram, human evaluation reveals no noticeable degradation in generation quality, with examples
available in Appendix F. This discrepancy may result from Unigram’s lower perplexity due to repeti-
tivetextgeneration,asrecentstudiesindicatethatmodelperplexityoftenfavorsrepetition[ 6]. To
further assessquality, we conductzero-shot evaluationsusing GPT-4Turbo, following theapproach
in [6], wherehigher scoresrepresent better quality. Onthe question-answeringtask withOPT-1.3B,
Bileveand Unigram achieve scores of 16 ±6.52 and 16±9.62, respectively.
8Figure 4: The p-value and alignment cost of each segment.
Table 4: The results of semantic manipulation attacks, with w/o indicting benign generations and w/
indicating semantic manipulation attacks by perturbing tokens (shown in red).
Schemes Texts for Detection Metric
Unigram (w/o)1. We are many times better people 2. We are few times more intelligent
than cats 3. We are easier to deal with [continue...]z-score: 10.9
Unigram (w/)1. Menare manytimesbetter people2. Menare fewtimesmore intelli-
gent than women 3. Women are harder to deal with [continue...]z-score: 10.5
SLS(w/o) [message tokens] disagree with your opinion because you... 𝝈: 10101011...
SLS(w/) [messagetokens]disagreewithyourstupidopinionbecauseyou... 𝝈: 10101101...
Bileve(w/o)[message tokens] You are mature, especially mentally mature;
You are well manner, versing normal silence [continue...]𝝈: 1000100...
Bileve(w/)[message tokens] Youare awful, especially mentallysick; We are
well manner, versing normal silence [continue...]𝝈: 1010101...
p-value: 0.01
5.4 Security
AgainstSignaturePreservationAttack Thesignaturepreservationattackoccursonlywhenattackers
replace tokens in a way that satisfies Line 11 in Alg. 1. This is challenging, as attackers have to find
tokens also maintain contextual coherence at the same time. We demonstrate that, although rare,
whenattackersmeettheseconditions, Bilevecandetectsuchattacksthroughlocalalignmenttesting.
Wesplit𝑤into5segmentsandperformthesignaturepreservationattackonthethirdoneasacase
study. The local alignment test returns a p-value for each segment, as shown in Fig. 4. Using the
p-values of the rest segments as baselines, segment 3 has an abnormally high p-value, indicating
misalignmentwithkeysequence Ξ. Moreover,wefurtherexplorethealignmentcostandshowthe
best10alignmentscoresamong300shifts,withthelowestcostasthedecidingfactor. Theresults
inFig.4showthebestalignmentcostofsegment3aftertheattackhasincreasedfrom-2.5to-1.3,
further uncovering the misalignment caused by signature preservation attacks.
AgainstSemanticManipulation Here,weconductasemanticmanipulationattackonUnigram, SLS,
andBileve. Notethatevenasinglesuccessfulattackcouldjeopardizethemodel’sreputation. As
showninTab.4,duetotherobustnessofUnigram,afterreplacingafewtokenstochangethetone
fromneutraltooffensive,thez-scoreremainshighenough(>6)toshowthewatermark,demonstrating
its inefficacy in differentiating such spoof attacks. In the case of SLS, semantic manipulation perturbs
embedded signature bits, leading to verification failure. If the message tokens are perturbed, the
verification fails as well due to the unmatched results (see Fig. 1 (b)). In contrast, although signature
bits in Bileveare also perturbed by attacks, the small p-value of the statistical test indicates the text
is generated by the target LLM (with tampering since digital signature verification fails).
6 Discussion
Limitation and Potential Optimization While the effectiveness and security are well demonstrated,
theefficiencyandgenerationqualityof Bilevecanbeimprovedbyadoptingthefollowingstrategies.
Firstly, it is unnecessary to apply Bilevefor prompts characterized by low entropy, such as those
involving manual token replacements. Secondly, we can embed multiple message-signature pairs
in longer outputsor use digital signature schemeswith shorter signature lengths forshorter outputs.
Thirdly, similar to [ 16], we can adopt an adaptive embedding strategy, i.e., signature embedding into
tokens can be skipped when their entropy levels are low, thus maintaining the natural flow of the
9text. Lastly, we can embed a single signature bit across a block of tokens, rather than into individual
tokens, which is promising to improve text perplexity by reducing disruptions in token coherence [ 6].
Societal Impact Reliably tracing text provenance is crucial for trust and accountability in LLM
usage. Unlike previous mechanisms that only yield binary results—whether text originates from
target LLMs— Bilevecan distinguish five scenarios, enhancing the defense against spoofing attacks
andimprovingLLMregulation. Bileveeffectivelydifferentiatesbetweenjailbreaking(bypassing
safety mechanisms to generate harmful content [ 24,3]) and spoofing (altering benign outputs to
create harmful content [ 2]), which can damage an AI’s reputation. By embedding bi-level signatures,
Bilevenotonlypreservescontentintegritybutalsodetectstampering,clearlyidentifyinggenuine
securitybreaches fromfraudulentimitations. Thus, Bileveadvancesthe societalgoalsof ensuring
safe, transparent, and accountable LLM regulation.
7 Conclusion
Inthiswork,weproposeabi-levelsignaturescheme,named Bileve,whichintegratesrobuststatistical
signals with fine-grained signature bits, ensuring that the watermark remains detectable through
perturbations while simultaneously verifying content integrity. The explicit tampering evidence
generated by our watermark helps safeguard model owners’ interests and enhances the accountability
mechanisms necessary for ethical LLM utilization. As demonstrated in experiments, Bilevenot
only maintains generation quality but also supports robust, tamper-evident signatures that can discern
betweengenuineandmanipulatedcontent. Overall,ourapproachrepresentsasignificantstepforward
in regulating LLMs, promoting safer deployments, and ensuring that these powerful technologies are
used responsibly and transparently.
8 Acknowledgement
This work is supported in part by the U.S. National Science Foundation under Grants OAC-2319962,
CNS-2239672, CNS-2153690, CNS-2326597, CNS-2247892, and CNS-2326598.
References
[1] Scott Aaronson. Simons institute talk on watermarking of large language models, 2023.
[2]Federico Bianchi and James Zou. Large language models are vulnerable to bait-and-switch
attacks for generating harmful content. arXivpreprintarXiv:2402.13926, 2024.
[3]XuanChen,YuzhouNie,WenboGuo,andXiangyuZhang. Whenllmmeetsdrl: Advancing
jailbreaking efficiency via drl-guided search. arXivpreprintarXiv:2406.08705, 2024.
[4]MirandaChrist,SamGunn,andOrZamir. Undetectablewatermarksforlanguagemodels. In
TheThirtySeventhAnnualConference onLearning Theory, pages 1125–1139. PMLR, 2024.
[5]Google DeepMind, 2024. https://deepmind .google/discover/blog/watermarking-
ai-generated-text-and-video-with-synthid/ .
[6]Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, and
Mingyuan Wang. Publicly detectable watermarking for language models. arXivpreprint
arXiv:2310.18491, 2023.
[7]James Farrell, 2023. https://siliconangle .com/2023/04/27/tesla-defense-
lawyers-say-elon-musks-past-statements-autopilot-safety-deepfakes/ .
[8]Chenchen Gu, Xiang Lisa Li, Percy Liang, and Tatsunori Hashimoto. On the learnability
of watermarks for language models. In TheTwelfthInternational Conference onLearning
Representations, 2024.
[9]AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecuriouscaseofneural
text degeneration. In International Conference onLearning Representations, 2019.
10[10]Nikola Jovanović, Robin Staab, and Martin Vechev. Watermark stealing in large language
models. In Forty-first International Conference onMachineLearning, 2024.
[11]JohnKirchenbauer,JonasGeiping,YuxinWen,JonathanKatz,IanMiers,andTomGoldstein. A
watermarkforlargelanguagemodels. In Proceedings ofthe40thInternational Conference on
MachineLearning, volume 202 of Proceedings ofMachineLearning Research, pages 17061–
17084. PMLR, 23–29 Jul 2023.
[12]KalpeshKrishna, YixiaoSong, MarzenaKarpinska, JohnFrederick Wieting,and MohitIyyer.
Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. In
Thirty-seventh Conference onNeuralInformation Processing Systems, 2023.
[13]Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-
free watermarks for language models. Transactions onMachineLearning Research, 2024.
[14]Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Pre-trained language
models for text generation: A survey. ACMComputing Surveys, 56(9):1–39, 2024.
[15]AiweiLiu,LeyiPan,XumingHu,ShuangLi,LijieWen,IrwinKing,andPhilipS.Yu. Aprivate
watermark for large language models. In TheTwelfthInternational Conference onLearning
Representations, 2024.
[16]Yepeng Liu and Yuheng Bu. Adaptive text watermark for large language models. In Forty-first
International Conference onMachineLearning, 2024.
[17]Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. In International Conference onLearning Representations, 2016.
[18]QiPang,ShengyuanHu,WentingZheng,andVirginiaSmith. Nofreelunchinllmwatermarking:
Trade-offs in watermarking design choices. arXivpreprintarXiv:2402.16187, 2024.
[19]Md Sagar Hossen, Tasfia Tabassum, Md Ashiqul Islam, Rafat Karim, Laila Sultana Rumi,
andAyshaAkterKobita. Digitalsignatureauthenticationusingasymmetrickeycryptography
with different byte number. In Evolutionary Computing andMobileSustainable Networks:
Proceedings ofICECMSN 2020, pages 845–851. Springer, 2021.
[20] SR Subramanya and Byung K Yi. Digital signatures. IEEEPotentials, 25(2):5–8, 2006.
[21]HugoTouvron, ThibautLavril, GautierIzacard, XavierMartinet, Marie-AnneLachaux,Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXivpreprintarXiv:2302.13971, 2023.
[22]SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,Christo-
pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer
language models. arXivpreprintarXiv:2205.01068, 2022.
[23]XuandongZhao,PrabhanjanVijendraAnanth,LeiLi,andYu-XiangWang. Provablerobust
watermarking for AI-generated text. In TheTwelfthInternational Conference onLearning
Representations, 2024.
[24]Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and
William Yang Wang. Weak-to-strong jailbreaking on large language models. arXivpreprint
arXiv:2401.17256, 2024.
11A Fine-tune an Adversary Model
A recent work has identified a spoofing attack against LLM watermarks [ 8]. In particular, the process
involvesqueryingthevictimmodelforwatermarkedsamples,followedbyfine-tuninganadversary
model𝐴parameterizedby 𝜃onthesesamplesusingasampling-basedwatermarkdistillationtechnique,
as described in Alg. 2, where the fine-tuning can be achieved by minimizing the loss function:
𝐴(𝜃)=−1
|WS|∑
𝑤∈WSlen(𝑤)∑
𝑡=2log𝑝𝜃(𝑤𝑡∣𝑤1∶𝑡−1)(4)
Oncefine-tuned,theadversarymodeliscapableofrespondingtomaliciousrequests. Theresponse
𝑤spoof, characterized by a low watermark detection p-value, may be erroneously attributed to the
victim model.
Algorithm 2 Watermark Forgery
Require: Victim model 𝑉, adversary model 𝐴
1:Watermarked samples WS←Query𝑉
2:Filter out refusals from WS
3:Fine-tune𝐴onWSfollowing Eq. 4 to enable 𝐴to mimic the sampling outcomes of 𝑉
4:𝑤spoof←Query𝐴with malicious prompts
5:Return𝑤spoof
B More Details for Semantic Manipulation
The attack algorithm is outlined in Alg. 3, where the detector 𝐷outputs 1 to indicate the presence of
a watermark.
Algorithm 3 Semantic Manipulation
Require: language model 𝑄, victim LLM text 𝑤orig, edit budget 𝜖, tuning factor 𝛼, detector𝐷
1:whileTruedo
2:𝑤att←Provide𝑤origand𝜖to𝑄to maximize Δ𝑅according to Eq. 1
3:if𝐷(𝑤att)==1then
4:Return𝑤att
5:else
6:𝜖←𝛼×𝜖
7:end if
8:end while
Warning: Thefollowingpart inthissectioncontainsexamplesofoffensivelanguage duetoattacks.
For in-context learning, we provide task demonstrations following the below strategies:
Identity Substitution Inspired by [ 2], we can first query the target LLM with benign prompts, which
willoutputaneutralresponse,thensubstitutetheidentitytootherstocreateoffensivecontent. We
use ChatGPT as the target LLM as examples shown in Fig. 5. By substituting the identity, e.g.,
replacing “men/humans” with “Americans” and “dogs” with “Japanese”, the language can become
highly offensive.
Antonym Replacement Fig. 6 provides examples of how to apply antonym replacement to generate
offensive content. To avoid affect detectability, we can control edit distance as suggested in Alg. 3.
OffensiveWordsInsertion Anotherstrategyisaddingcursingwordsbetweensentences,wheretoken
insertion will not compromise the detectability of SOTA watermarking due to their robustness.
Alg.3imposesrestrictionsoneditdistance,preservingthedetectabilityofwatermarks. Attackers
mayexploitthisbyfalselyattributingmodifiedcontenttothevictimLLM,damagingitsreputation
andsuggestingsecurityvulnerabilities. Incontrast,ourwatermarkingmethodincorporatesdigital
12Figure 5: The generation from ChatGPT. Identity substitution can be applied to achieve semantic
manipulation.
Figure 6: Example from ChatGPT applied antonym replacement to achieve semantic manipulation.
13signatures, ensuring the integrity of generated content. When attackers use Algorithm 3 to spoof
jailbreaking, our watermark provides evidence of tampering, effectively thwarting such attempts.
Meanwhile, genuine jailbreaking incidents will originate from the victim LLM with its intact digital
signature watermark. Therefore, our approach enables efficient determination of real jailbreaking
instances, aiding LLM regulation effectively.
C Shift-generate Algorithm
To better embed the randomness, we sample the best tokens based on certain rules, e.g., exponential
minimal sampling [ 13], instead of sampling based on probability distribution. However, such a
strategyreducingsamplingrandomnessalsoaffectsgenerationdiversity. Theshift-generatealgorithm
is introduced in [ 13] to solve this problem. We refer readers to Algorithm 4 in [ 13] for more details.
We also include it in Fig 7, where 𝜏functions the same as 𝑑in our work.
Figure 7: Shift-generate Algorithm from [13]
Inparticular,thismethodrandomlyshiftsthewatermarkkeysequencebeforepassingittothegenerate
function. Thisshiftdoesnotimpacttheteststatisticusedindetection,asthedetectorsearchesover
all subsequences of the watermark key sequence to calculate the minimal alignment cost. There are 𝑛
possible shifts, each potentially creating a distinct text.
D Robust Alignment Cost Measurement
Toenhancethedetectabilityof Bileve,wemodifythealignmentcostinEq.3toincludeeditdistance
(more details are referred to Definition 5 in [13]) :
𝑑𝛾(𝑤,Ξ)=min⎧
⎪
⎨
⎪⎩𝑑𝛾(𝑤2∶,Ξ2∶)+𝑑0(𝑤1,Ξ1)
𝑑𝛾(𝑤,Ξ2∶)+𝛾
𝑑𝛾(𝑤2∶,Ξ)+𝛾,(5)
where𝑑0isdefinedbyEq.3. Bythenatureofeditdistance,nowwecanbetterpreservethedetectability
even if the insertion and deletion happened to the watermarked text.
E Perplexity on LLaMA-7B
We provide the results on LLaMA-7B in Fig. 8, where we can observe similar comparisons with
theresultsonOPT-1.3B.Specifically,theperplexityofUnigramisclosetohumanbecauseituses
a soft red list described in [ 11], which can better preserve contextual fluency. Moreover, Bileve
outperforms SLSbecauseweuserank-basedsampling,whichfavorsthetokenwithhigherprobability,
while SLSuses nucleus sampling, which still has the chance to select the token with low probability.
Also, its precise signature bits matching also compromise perplexity. This can be mitigated if we
embed a bit into a longer block of tokens, as discussed in [6].
F Generation Examples
We provide generation examples of Unigram and Bileveon LFQA tasks using LLaMa-7B in Tab. 5,
which show that the higher PPL does not indicate impractically bad quality.
14Human             Unigram              SLS                BileveHuman             Unigram              SLS                BileveFigure 8: The perplexity of applying different schemes to LLaMA-7B.
Table 5: Comparison of Unigram and BileveResponses.
Prompt Unigram Bileve
Q:What doesa Mayoreven
do?Uhhhhhhhhhhhhhhhhhhhh... Most of the problems be-
ing experienced by our City
arearesultofbadplanning,
decisions, and practices of
the City Council. Unfortu-
nately, the City Council re-
ceivesthemajorityofadula-
tionforwhat’sgoingoninthe
City. The mayor is the City
Manager...
Q: Mandatory arbitration I am sorry for this but I am
out of answers. I will ask
othersforasolution. Thank
you. Please ask more ques-
tions later on. If you wait 2
hours I will be back...Mandatory arbitration is a
means for eliminating af-
fected commerce and elim-
inating employees’ rights
to sue as private citizens.
Unions do not like the term...
Q: when does a case need ju-
rors?A court can order a jury as
a court order. If a party asks
for a jury. If a party appeals
acourt. Ifapartyfilesalaw-
suit(whichmustbedonebe-
fore a court can be held)...Tuesday at all times and
Thursdays at 9:00 am. If
you are qualified, you may
be called for a case or cases
may be filled from qualified
jurors already on the list...
WealsoprovideexamplesinTab.6toshowthatUnigramwouldgeneraterepetitivegeneration,which
may lead to its lower perplexity, although the perceived quality of Unigram’s outputs does not differ
significantly from Bileveas demonstrated in Tab. 5.
15Table 6: Comparison of Unigram and Bileve Generations using OPT-1.3B on text completion tasks
Prompt Unigram Bileve
The lava dome was created
by volcanic eruptions in the
early Miocene. A total of
five large ash flows were pro-
ducedalongwithalargerhy-
olite dome structure. The
caldera formed when the
domecollapsedabout16mil-
lion years ago.A mill eruption approxi-
mately 3 million years ago
producedmanyfine-grained
lavas, plus fly ash. Over
1,600 more fine-grained
lavas were produced around
3.6 Ma. A 6.8 Ma eruption
produced a voluminous
eruption with numerous
fine-grained lavas. The 6.8
Ma eruption also produced
numerous small diorites.
The most intense eruption
from the 6.8 Ma eruption
produced more...During the late Triassic pe-
riod, Steens mountain be-
gan to rise eastward from
the Cocoon Valley, and in
theirplace,eruptedaseries
of glacial-related geologic
structures. IntheearlyJuras-
sic period, lava flow-induced
tectonic activity in the up-
per section caused Steen and
its outflow area on this por-
tionwesttotheCanadianNa-
tional Mountains...
NeurIPS Paper Checklist
1.Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper’s contributions and scope?
Answer:[Yes]
Justification: contributions are summarized in introduction
Guidelines:
•TheanswerNAmeansthattheabstractandintroductiondonotincludetheclaimsmade
in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor
NA answer to this question will not be perceived well by the reviewers.
•Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow
much the results can be expected to generalize to other settings.
•Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: in discussion section
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
•The authors are encouraged to create a separate "Limitations" section in their paper.
•Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto
violations of these assumptions (e.g., independence assumptions, noiseless settings,
modelwell-specification,asymptoticapproximationsonlyholdinglocally). Theauthors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
16•Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.
Forexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•Ifapplicable,theauthorsshoulddiscusspossiblelimitationsoftheirapproachtoaddress
problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
Justification: does not include theoretical results
Guidelines:
•The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
by formal proofs provided in appendix or supplemental material.
•Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-
perimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: include in experiment setup
Guidelines:
•The answer NA means that the paper does not include experiments.
•Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceivedwell
bythereviewers: Makingthepaperreproducibleisimportant,regardlessofwhether
the code and data are provided or not.
•Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken
to make their results reproducible or verifiable.
•Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.
Forexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
ofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare
appropriate to the research performed.
17•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
eitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)Werecognizethatreproducibilitymaybetrickyinsomecases,inwhichcaseauthors
are welcome to describe the particular way they provide for reproducibility. In the
case of closed-source models, it may be that access to the model is limited in some
way(e.g.,toregisteredusers),butitshouldbepossibleforotherresearcherstohave
some path to reproducing or verifying the results.
5.Open access to data and code
Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstructions
tofaithfullyreproducethemainexperimentalresults,asdescribedinsupplementalmaterial?
Answer: [Yes]
Justification: provide code
Guidelines:
•The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips .cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
includingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much informationas possible in supplemental material (appended tothe
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: in experiment setup
Guidelines:
•The answer NA means that the paper does not include experiments.
•Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
that is necessary to appreciate the results and make sense of them.
•Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental
material.
187.Experiment Statistical Significance
Question: Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: defined in experiments
Guidelines:
•The answer NA means that the paper does not include experiments.
•Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confidence
intervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupportthe
main claims of the paper.
•Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for
example, train/testsplit,initialization, randomdrawingofsomeparameter,oroverall
run with given experimental conditions).
•Themethodforcalculating the errorbars shouldbe explained(closed form formula,
call to a library function, bootstrap, etc.)
•The assumptions made should be given (e.g., Normally distributed errors).
•Itshouldbe clearwhethertheerror baristhestandard deviationorthestandard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecomputer
resources(typeofcomputeworkers,memory,timeofexecution)neededtoreproducethe
experiments?
Answer: [Yes]
Justification: in experiment setting
Guidelines:
•The answer NA means that the paper does not include experiments.
•ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,
or cloud provider, including relevant memory and storage.
•Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips .cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: [TODO]
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
19•Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsider-
ation due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: in discussion section
Guidelines:
•The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
any negative applications, the authors should point it out. For example, it is legitimate
to pointout thatan improvement in the quality ofgenerativemodels couldbe used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing
from (intentional or unintentional) misuse of the technology.
•Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: do not release data or models and add warning
Guidelines:
•The answer NA means that the paper poses no such risks.
•Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessary safeguards to allow for controlled use of the model, for example by requiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properly respected?
Answer: [Yes]
20Justification: add references
Guidelines:
•The answer NA means that the paper does not use existing assets.
•The authors should cite the original paper that produced the code package or dataset.
•Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
•The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode .com/datasets
hascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe
license of a dataset.
•Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof
the derived asset (if it has changed) should be provided.
•Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto
the asset’s creators.
13.New Assets
Question: Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
provided alongside the assets?
Answer: [NA]
Justification: does not release new assets
Guidelines:
•The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: Forcrowdsourcingexperimentsandresearchwithhumansubjects, doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
well as details about compensation (if any)?
Answer: [NA]
Justification: does not involve crowdsourcing nor research with human subjects
Guidelines:
•TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
human subjects.
•Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-
tionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe
included in the main paper.
•AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution) were obtained?
21Answer: [NA]
Justification: does not involve crowdsourcing nor research with human subjects
Guidelines:
•TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
human subjects.
•Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
should clearly state this in the paper.
•Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
andlocations, andwe expectauthorsto adheretotheNeurIPSCode ofEthicsand the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
22