Stepwise Weighted Spike Coding for Deep Spiking
Neural Networks
Anonymous Author(s)
Affiliation
Address
email
Abstract
Spiking Neural Networks (SNNs) seek to mimic the spiking behavior of biological 1
neurons and are expected to play a key role in the advancement of neural computing 2
and artificial intelligence. The efficiency of SNNs is often determined by the 3
neural coding schemes. Existing coding schemes either cause huge delays and 4
energy consumption or necessitate intricate neuron models and training techniques. 5
To address these issues, we propose a novel Stepwise Weighted Spike (SWS) 6
coding scheme to enhance the encoding of information in spikes. This approach 7
compresses the spikes by weighting the significance of the spike in each step of 8
neural computation, achieving high performance and low energy consumption. A 9
Ternary Self-Amplifying (TSA) neuron model with a silent period is proposed for 10
supporting SWS-based computing, aimed at minimizing the residual error resulting 11
from stepwise weighting in neural computation. Our experimental results show 12
that the SWS coding scheme outperforms the existing neural coding schemes in 13
very deep SNNs, and significantly reduces operations and latency. 14
1 Introduction 15
Spiking Neural Networks (SNNs) are known as the third generation of neural network models 16
inspired by the biological structures and functions in the brain [ 32]. Unlike traditional Artificial 17
Neural Networks (ANNs) that use continuous activation functions, SNNs incorporate discrete spiking 18
events, enabling them to capture temporal dynamics and process information in a manner that closely 19
mimics the brain’s functioning [ 31]. This event-driven paradigm aligns with the brain’s energy- 20
efficient computation and has the potential for more efficient and lower-power computing systems. 21
[33]. 22
Various coding schemes have been proposed to describe neural activities, including rate coding and 23
temporal coding [ 9]. Rate coding counts the number of spikes fired within a broad time window 24
[23,3,18,6], which effectively mitigates the impact of short-term interference on the signal. It was 25
widely accepted in the early days and typically outperformed temporal coding [ 11,34,4,29,20]. 26
However, the rate coding scheme disregards the information in the temporal domain of the input 27
spike sequence and requires many pulses to represent the input signal value, making it an inefficient 28
coding method that negates the low-power benefits of SNN. Due to the functional similarity to the 29
biological neural network, spiking neural networks can embrace the sparsity found in biology and 30
are highly compatible with temporal coding [ 31,33,27,28,21,15]. Temporal coding relies on 31
the specific timing or patterns of input spikes, allowing for greater information capacity in a single 32
pulse. However, it requires a large number of time steps to provide fine-grained timing, which 33
increases inference latency. Its sensitivity to variations in spike timing also makes it more vulnerable 34
to temporal jitter or delays [ 25,24]. Additionally, decoding temporal-coded information usually 35
requires more complex neuron models [30, 36] and training methodologies [17, 26]. 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.In the study of the temporal information dynamics of spikes, Kim et al. [ 16] discovered a phenomenon 37
of temporal information concentration in SNNs. It is found that after training, information becomes 38
highly concentrated in the first few timesteps. Based on this observation, we hypothesize that, from 39
the perspective of the postsynaptic neuron, the first arriving spikes contain more information and 40
require stronger responses. Consequently, we propose a mechanism whereby the neuron augments 41
its own membrane potential with a specific coefficient prior to processing the subsequent input. 42
This enhancement serves to increase the importance of preceding pulses on neurons, which is why 43
the spikes are designated as Stepwise Weighted Spikes (SWS). Nevertheless, the amplification of 44
the membrane potential makes it difficult for neurons to reduce its value through traditional "soft 45
reset" (i.e. subtracted by an amount equal to the firing threshold), which can result in residual errors 46
after neuron firing. To address this issue, we make the membrane potential reduced by a magnitude 47
exceeding the threshold after firing. As a result, the membrane potential has both positive and negative 48
residual values, which will generate both positive and negative spikes. This neuron is designated as a 49
Ternary Self-Amplifying (TSA) neuron. To further reduce the error caused by the weighting process, 50
a silent period is incorporated into the TSA neuron, allowing it to receive more input information 51
before firing. We perform the classification tasks with SWS-based SNN on MNIST, CIFAR10, and 52
ImageNet. The results show that the SWS coding scheme can achieve better performance with much 53
fewer coding and computing steps. Even in very deep SNN, SWS coding scheme still performs well 54
and achieves similar accuracy to the ANN with the same structure. Our major contributions to this 55
paper can be summarized as follows: 56
•We propose the SWS coding scheme, which enables easy implementation of SNNs with 57
low energy consumption and high accuracy. The stepwise weighting process enhances 58
the information-carrying capacity of the preceding pulses, greatly reducing the number 59
of coding spikes. Negative pulses are introduced in SWS coding to ensure an accurate 60
information transmission. 61
•A novel TSA neuron model is proposed. TSA neuron progressively weights the input by 62
augmenting its residual membrane potential before receiving the subsequent spike. The 63
introduction of negative residual membrane potential and negative thresholds enhances the 64
accuracy of the model’s output. 65
•A silent period is added to TSA neuron to markedly improve accuracy at minimal latency 66
cost. By adjusting the silent period step and coding step, SWS-based SNNs can exhibit 67
performance advantages in different aspects, improving the flexibility of applications. 68
2 Related work 69
SNNs use spike sequences to convey information, making the encoding of real data into pulses a 70
crucial step. Currently, the mainstream schemes of neural coding are rate coding and temporal coding 71
[9,33,32]. Rate coding represents different activities with the number of spikes emitted within 72
a specific time window. Due to its simplicity, rate coding is commonly used in deep learning of 73
SNNs. However, it distributes information uniformly across a large number of spikes, resulting in an 74
inefficient transmission process that increases network latency and energy consumption. Numerous 75
researchers have proposed solutions to optimize inference latency in rate coding. Han et al.[ 11] 76
proposed a "soft reset" spiking neuron model that retains a residual membrane potential after firing 77
to better mimic the ReLU functionality. They demonstrated near lossless ANN-SNN conversion by 78
using 2-8 times fewer inference time steps. Still, a delay of thousands of steps is required in large 79
datasets or deep networks. In [ 14], Hu et al. reduced the encode time steps by converting a quantized 80
low-precision ANN to a rate-coded SNN. They also proposed a layer-wise fine-tuning mechanism 81
to minimize the inference latency. However, their neuron model and the subsequent fine-tuning 82
algorithm are relatively complex. Furthermore, in deeper neural networks such as ResNet56, a 1.5% 83
drop in accuracy can be observed. The above rate encoding solutions are limited because they do not 84
consider the significance of each spike. 85
In [15], Kim et al. proposed phase coding, which assigns different weights to spikes based on their 86
time phase. However, the transmission amount of information is bounded by the global phase, which 87
causes inefficiency in hidden layers, resulting in a latency of up to three thousand steps for a 32-layer 88
network. Burst coding [ 21] attempts to overcome this issue by introducing burst spikes, which 89
utilize Inter-Spike Interval (ISI). Burst spikes are capable of conveying more information quickly and 90
accurately by inducing Post-Synaptic Potential (PSP) dramatically. Nevertheless, it is still deficient 91
2Table 1: Common symbols and their meanings in this paper.
Symbol Meaning
Sl
i(t) The spike train fired by the ithneuron in the lthlayer
ul
i(t) The membrane potential of the ithneuron in the lthlayer
zl
i(t) The integrated inputs to the ithneuron in the lthlayer
Vl
th The firing threshold of the neurons in the lthlayer
θlThe amplitude of the spikes fired by the neurons in the lthlayer
in terms of latency and efficiency. Rueckauer and Liu [ 27] proposed an efficient temporal encoding 92
scheme where the analog activation values of the ANN neurons are represented by the inverse Time- 93
To-First-Spike (TTFS) in the SNN neurons. Their new spiking network model generates 7-10 times 94
fewer pulses by utilizing temporal information carried by a single spike. However, as pointed out 95
in [10], TTFS coding scheme incurs expensive memory access and computational overhead, which 96
diminishes the benefit of reduced pulse count. Furthermore, TTFS necessitates a large number of time 97
steps to differentiate between various time points, which also increases network latency. Han and 98
Roy [ 10] proposed the Temporal-Switch-Coding (TSC) scheme, in which each input image pixel is 99
represented by two spikes, and its intensity is proportional to the timing between the two pulses. Their 100
results showed a reduction in energy expenditure. However, TSC coding requires a large number of 101
time steps to provide distinguishable time intervals, rendering it an ineffective approach to addressing 102
the issue of the long latency. 103
Overall, rate coding employs a large number of pulses to encode information, which results in a 104
considerable energy overhead and inference delays. On the other hand, temporal coding allows for 105
greater information capacity in a single spike, but this does not reduce the computing latency as a 106
precise time point or period can be identified only with a sufficient number of time steps. Therefore, 107
new neural coding schemes should be developed. 108
3 Stepwise weighted spike coding scheme 109
3.1 Stepwise weighting 110
The spike train Sl
i(t)of the ithneuron in the lthlayer can be expressed as follows: 111
Sl
i(t) =X
tl,(f)
i∈Fl
iθlδ(t−tl,(f)
i) (1)
where δ(t)is the Dirac delta function, θlis the spike amplitude of the lthlayer, which is usually set 112
to the same value as the firing threshold. fis the index of the spike in the sequence, and Fl
idenotes a 113
set of spike times which satisfies the firing condition: 114
tl,(f)
i:ul
i(tl,(f)
i)≥Vl
th (2)
where ul
i(t)denotes the membrane potential and Vl
thdenotes the firing threshold of the neurons in 115
thelthlayer. 116
Our basic idea is to amplify the membrane potential before the receipt of the subsequent input, which 117
amplifies and prolongs the impact of the preceding input spikes on membrane potential, emulating the 118
phenomenon of information concentration identified in [ 16]. For clarity, the meanings of important 119
symbols are provided in table 1. The action of a neuron in SWS-SNN can be described as follows: 120
ul
j(t) =βul
j(t−1) +zl
j(t)−Sl
j(t) (3)
where βis the amplification factor which should be greater than one, zl
j(t)denotes the PSP (i.e. 121
integrated inputs): 122
zl
j(t) =X
iωl
ijSl−1
i(t) +bl
j (4)
where ωijis the synaptic weight and bl
jis the bias. Begin with the initial value ul
j(0) = 0 and 123
iteratively apply eq. (3) for each subsequent value until ul
j(n)and substitute eq. (1) and eq. (4) into it, 124
3Figure 1: (a) Illustration of the stepwise weighting process. The meanings of the symbol zl
j(t), ul
j(t)
andSl
j(t)can be found in table 1. The blue dotted line represents the membrane potential prior to the
spike firing, and the black exponential function-like dotted line is employed to illustrate the trend of
membrane potential amplification. (b) A Vl
thequal to θlresults in residual errors, leaving a lot of
information unencoded. (c) Vl
this set to1
2θl, which increases the possibility to fire spikes early to
better limit the residual. (d) Use negative spikes to correct the excessively emitted information.
eq. (3) can be written as: 125
ul
j(n) =βnul
j(0) +nX
τ=1βn−τzl
j(τ) =X
tl−1,(f)
iX
inX
τ=1βn−τωl
ijθl−1δ(τ−tl−1,(f)
i ) +βn−τbl
j(5)
Note that Sl
j(t)is set to zero for simplicity. From eq. (5), it can be seen that the stepwise augment of 126
the membrane potential results in the spike input at time tl−1,(f)
i encoding the value θl−1βn−tl−1,(f)
i . 127
This process is thus referred to as stepwise weighting, and βn−tl−1,(f)
i serves as the weight. The 128
earlier the input pulse, the greater its ability to carry information. This solves the problem of excessive 129
encoding steps in previous schemes, allowing faster information transmission. 130
3.2 Residual error 131
Stepwise weighting effectively assigns more weight to earlier arriving pulses, but it also makes spike 132
generation more tricky. To ensure that input information is efficiently encoded and transmitted to 133
the next layer, the residual membrane potential should be minimized after neural computation is 134
completed. The stepwise weighting, however, amplifies the residual potential from the previous 135
time step. If zl
j(t)remains high in subsequent steps, reducing the membrane potential becomes 136
challenging, as shown in fig. 1(b). This vicious cycle ultimately leads to a persistently high membrane 137
potential, indicating that a substantial amount of information remains unencoded. 138
We refer to this phenomenon as residual error. One contributing factor is that the threshold is set 139
too high, resulting in a pulse being emitted only when the membrane potential exceeds the value θl. 140
While this prevents excessive information transmission, it results in missed opportunities to bring 141
down ul
j(t)by firing a spike. 142
To address this issue, we propose setting the firing threshold Vl
thto1
2θl. This adjustment facilitates 143
pulse generation and reduces the residual membrane potential. After the neuron firing, the membrane 144
potential is subtracted by θl, which leads to the emergence of a negative residual that will be stepwise 145
4Figure 2: (a) Uncertainty in the input distribution leads to residual errors. (b) The silent period allows
more information to be known when firing pulses. Tsis set to 1 here. Vl
this amplified by βTs, and the
original threshold is represented by a gray solid line. The orange dashed line represents the amount of
membrane potential reduction after firing. (c) The silent period also avoids some unnecessary spikes
and increases sparsity. Without the silent period, since ul
j(1)exceeds the original threshold, a pulse
will be generated at t= 1, which will later be corrected by another negative spike. (d) The impact of
the silent period on network latency. The output spike sequences corresponding to different inputs
are drawn in blocks of different colors. The pulses drawn in the spike sequence are for illustrative
purposes only.
weighted over time. The coefficient 1/2is selected as it is capable of controlling both positive and 146
negative residuals within a narrow and balanced range. A negative threshold −Vl
this introduced into 147
the neuron model, which initiates a negative spike when the membrane potential falls below this 148
threshold. This mechanism allows the excessively emitted information to be corrected by the negative 149
spike, as shown in fig. 1(d). Given the above characteristics, we designate this neuron model as a 150
TSA neuron. 151
3.3 Silent period 152
Another contributing factor to residual error is the imbalanced distribution of zl
j(t). A burst input of 153
zl
j(t)at time point τresults in a sharp rise in membrane potential, making it difficult for subsequent 154
spikes to reduce it, as shown in fig. 2(a). 155
This can be addressed by incorporating a silent period Tsinto the TSA neuron model. The neurons 156
only integrates input and performs stepwise weighting, but are not allowed to fire in the first Ts 157
steps. This enables the acquisition of more known information before spike generation, resulting 158
in increased accuracy, as illustrated in fig. 2(b). Since the preceding input information has been 159
amplified by βTsafter the silent period, Vl
thalso needs to be adjusted accordingly, which is set to 160
βTs
2θl. Similarly, after firing, the membrane potential should be subtracted by θlβTs. Note that the 161
fired spike amplitude remains unchanged, that is, θl. 162
The impact of the silent period on network latency is shown in fig. 2(d). The output results for 163
different input sequences are distinguished by blocks of different colors. It can be observed that 164
as network depth increases, the silent period accumulates, leading to a higher output latency. The 165
inference latency of SWS-SNN can be calculated as follows: 166
Tinf=Tc+Ts·LTSA (6)
5where Tinfis the inference delay, Tcis the coding time steps, Tsis the length of the silent period and 167
LTSA is the number of TSA neuron layers. The neuron model in other coding schemes yields a zero 168
Ts, leading to an output delay equal to the coding time step, which is consistent with the definition in 169
the previous scheme. From fig. 2(d), it can be seen that different input sequences are processed in a 170
pipeline-like manner, and the value of Tc+Tsdetermines the throughput rate of SWS-SNN. 171
3.4 Input encoding 172
According to eq. (5), the value that can be losslessly encoded under the SWS coding scheme can be 173
expressed as follows: 174
Aj=TcX
τ=1aτ
j·θ0βTc−τ(7)
where Ajdenotes the encoded value. aτ
j∈ {− 1,0,1}indicates the type of the output spike at time τ: 175
1for a positive pulse, −1for a negative pulse and 0for no pulse. Tcdenoted the time steps used for 176
encoding. The weight βTc−τresults from the stepwise weighting process described in section 3.1. θ0177
denotes the spike amplitude of the input encoding layer, which can be assigned an appropriate value 178
based on the range to be encoded. 179
According to eq. (7), given a fixed Tcandθ0, the distribution of Ajis determined by β. Setting βto 180
2is reasonable, as it ensures Ajis evenly distributed within the codable range. Compared to rate 181
coding, which necessitates 2Tccoding steps to encode the same range with same precision, SWS 182
coding significantly enhances coding efficiency. Note that with the introduction of negative pulses, 183
setting βto3can also achieve a uniform distribution of Ajand offers even more values for accurate 184
encoding compared to β= 2.1When βis less than 2, the distribution of Ajbecomes denser at 185
smaller values, which may be suitable for encoding data that follows a similar distribution. 186
For static image classification tasks, the pixel value pjcan be encoded by applying a constant input 187
z0
j(t)to the TSA neuron. Considering the stepwise weighting process, we can write: 188
pj=TcX
τ=1z0
jβTc−τ(8)
wherez0
jdenotes the amplitude of the constant input z0
j(t). Solve forz0
jand we have: 189
z0
j(t) =TcX
σ=1pjPTc
τ=1βTc−τ·δ(t−σ) (9)
Given that z0
j(t)is a constant at each step, Tscan be set to 0for the encoding layer. However, the 190
neuron must await Tstime steps after the completion of an encoding. This allows neurons in the 191
subsequent layer to complete the previous neural computing before receiving the next encoded input. 192
4 Experiments 193
In this section, we convert quantized ANNs to SWS-based SNNs2and conduct experiments on 194
MNIST, CIFAR10, and ImageNet. Firstly, an overview of SWS-SNN’s performance across various 195
datasets is provided. Subsequently, the network’s inference latency and energy consumption is 196
compared with other spike coding schemes. Finally, an ablation study is conducted to investigate the 197
impact of lowered thresholds and silent periods on reducing residuals and enhancing accuracy. 198
ANNs used for conversion are all quantized to 8bits.βis set to 2in the experiments to ensure that 199
codable values are evenly distributed. Compared to β= 3, a smaller amplification factor reduces the 200
impact of residual errors, resulting in more accurate output. 201
1Setting βto2introduces some coding redundancy. E.g., a1
j= 1, a2
j=−1anda1
j= 0, a2
j= 1encodes the
same amount of information.
2Details of the conversion process can be found in appendix A.1 and appendix A.2
6Table 2: Performance on CIFAR10 and ImageNet.
Category Methods ArchitectureTime
StepTsSNN
Acc∆Acc†CIFAR10Directly
LearningSTBP-tdBN[35] ResNet-19 6 - 93.16% -
TET[5] ResNet-19 6 - 94.50% -
ANN-SNNTTRBR[20] ResNet-18 64 - 95.04% −0.13%
DSR[19] PreAct-ResNet-18 20 - 95.24% -
Calibration[18] VGG-16 256 - 95.79% +0.05%
OPI[1] VGG-16 256 - 94.49% −0.08%
Opt Conversion[4] ResNet-20 128 - 93.56% +1.25%
ANN-SNN SWS (ours)ResNet-18 8 1 95.67% +0.22%
VGG-16 8 2 95.86% −0.04%ImageNetDirectly
LearningTET[5] SEW-ResNet-34 4 - 68.00% -
STBP-tdBN[35] SEW-ResNet-34 4 - 67.04% -
SEW Resnet[8] SEW-ResNet-152 4 - 69.26% -
ANN-SNNHybrid training[26] ResNet-34 250 - 61.48% −8.72%
Spiking ResNet[13] ResNet-50 350 - 72.75% −2.70%
QCFS[2] VGG-16 64 - 72.85% −1.44%
Fast-SNN[14] VGG-16 7 - 72.95% −0.41%
COS[12] ResNet-34 8 - 74.17% −0.05%
RMP-SNN[11] ResNet-34 4096 - 69.89% −0.75%
TTRBR[20] ResNet-50 512 - 75.04% −0.98%
ANN-SNN SWS (ours)VGG-16 8 2 75.27% −0.11%
ResNet-34 8 2 76.10% −0.08%
Inception-v3 8 2 76.70% −0.70%
ResNet-50 8 2 80.34% −0.35%
ResNeXt101_32x8d 8 1 81.32% −1.17%
ResNeXt101_32x8d 8 2 82.06% −0.42%
†∆Acc=Acc SNN−Acc ANN
4.1 Overall performance 202
For simple classification tasks such as CIFAR10, our proposed SWS coding scheme has a faster 203
inference speed than other ANN-SNN models while achieving similar classification accuracy, or has 204
higher classification accuracy than direct learning at similar inference speeds. For example, ResNet18 205
with SWS improves throughput seven times over [ 20] while simultaneously improving accuracy. 206
Although the network in [ 5] has a slightly higher throughput, its accuracy is 1.17% lower than our 207
scheme. To fully test the potential of our proposed coding scheme, we conducted experiments on 208
ImageNet using networks with various structures. The experimental results demonstrate that SWS 209
coding has distinct advantages on extremely deep SNNs. Our SWS-based ResNet50 and ResNeXt101 210
achieved over 80% accuracy on ImageNet with only eight coding steps. The model in [ 12] achieves 211
an almost lossless conversion with eight time steps. However, their method has to adjust the resting 212
potential of neurons layer by layer, and the calibration effect for deeper networks is unclear. In [ 14], 213
the original ANN needs to be quantized to 3 bits, resulting in a larger conversion loss. Directly trained 214
SNNs typically achieve higher throughput, but their accuracy still requires improvement. In addition, 215
the SWS coding scheme is easy to implement. No further fine-tuning is required after the conversion. 216
4.2 Accuracy vs. latency 217
The comparison of latency results between SWS-SNN and other ANN-converted SNNs[ 1,11,10, 218
4,18,2,7] is illustrated in fig. 3. The latency of the network is calculated with eq. (6). In the 219
counterpart models, the variation of delay is mainly caused by the changes in Tc. In contrast, 220
Tsdetermines latency in deep SWS-SNNs. Therefore, SWS-SNN has an upper limit on latency: 221
Tmax
inf=Tc(1 +LTSA), which causes our curve to terminate earlier in fig. 3. 222
To ensure a fair comparison, we represent the ANN accuracy of each counterpart with dotted lines of 223
the same color. The experimental results indicate that SWS-SNN can achieve optimal performance 224
with minimal latency. Specifically, SWS-based VGG-16 can converge to the ANN performance 225
7242526272829
Latency888990919293949596 CIFAR10 acc (%)
/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000039/uni0000002a/uni0000002a/uni00000010/uni00000014/uni00000019/uni00000003/uni00000052/uni00000051/uni00000003/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013
TWS (ours)
RMP
Calibration
OPI
Opt
RNL
242526272829210
Latency5658606264666870727476 ImageNet acc (%)
/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000039/uni0000002a/uni0000002a/uni00000010/uni00000014/uni00000019/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057
TWS (ours)
RMP
Calibration
OPI
QCFS
TSC
242526272829210
Latency545658606264666870727476 ImageNet acc (%)
/uni0000000b/uni00000046/uni0000000c/uni00000003/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000016/uni00000017/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057
TWS (ours)
RMP
Calibration
QCFS
TSCFigure 3: Latency versus accuracy. The ANN accuracy of each compared SNN is marked by dotted
lines of the same colour. (a) VGG-16 on CIFAR10. (b) VGG-16 on ImageNet. (c) ResNet34 on
ImageNet.
/uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000015 /uni00000014/uni00000011/uni00000017 /uni00000014/uni00000011/uni00000019 /uni00000014/uni00000011/uni0000001b
OPF (M)/uni0000001c/uni0000001c/uni00000011/uni00000014/uni00000013/uni0000001c/uni0000001c/uni00000011/uni00000014/uni00000018/uni0000001c/uni0000001c/uni00000011/uni00000015/uni00000013/uni0000001c/uni0000001c/uni00000011/uni00000015/uni00000018/uni0000001c/uni0000001c/uni00000011/uni00000016/uni00000013/uni0000001c/uni0000001c/uni00000011/uni00000016/uni00000018/uni0000001c/uni0000001c/uni00000011/uni00000017/uni00000013/uni0000001c/uni0000001c/uni00000011/uni00000017/uni00000018 MNIST acc (%)
/uni0000000b/uni00000044/uni0000000c
T s=0
T s=1
T s=2
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000018 /uni00000015/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000016/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000018
OPF (M)/uni0000001c/uni0000001a/uni00000011/uni0000001b/uni0000001c/uni0000001b/uni00000011/uni00000013/uni0000001c/uni0000001b/uni00000011/uni00000015/uni0000001c/uni0000001b/uni00000011/uni00000017/uni0000001c/uni0000001b/uni00000011/uni00000019/uni0000001c/uni0000001b/uni00000011/uni0000001b/uni0000001c/uni0000001c/uni00000011/uni00000013/uni0000001c/uni0000001c/uni00000011/uni00000015/uni0000001c/uni0000001c/uni00000011/uni00000017 MNIST acc (%)/uni0000000b/uni00000045/uni0000000c
SWS (T c=8)
SWS (T c=6)
SWS (T c=5)
SWS (T c=4)
Rate (44 steps)
Rate (18 steps)TTFS (base)
TTFS (dyn thresh)
TTFS (clamped)
Pattern (4bits)
Pattern (8bits) TC=4Tc=5
Tc=6Tc=7Tc=8
Figure 4: (a) Accuracy versus OPF with different combinations of TcandTs. (b) Comparison of
accuracy and energy consumption of SWS-SNN with other SNNs.
in the shortest time on CIFAR10 and reduce the inference latency on ImageNet by more than one 226
order. Even though the silent period accumulates when the network gets deeper, the results in fig. 3(c) 227
demonstrate that our scheme still achieves the fastest inference speed with the highest accuracy in a 228
34-layer network. Note that Tsis set to the same value for each TSA layer for simplicity, resulting in 229
discontinuous Tinfvalues. This causes a sharp drop in accuracy at smaller delays. 230
4.3 Operation counting 231
To compare the energy consumption of SWS-SNN with SNNs under other encoding schemes, we 232
adopt the method as in [29, 27, 28] to count operations: 233
OPF = (Tc+Ts)NTSA+LTSAX
l=1Tsl+TcX
τ=Tsl+1fl
outnl(τ) (10)
where OPF (Operations Per Frame) denotes the number of operations for the classification of one 234
frame, TcandTsdenotes the coding steps and the length of the silent period, respectively. LTSA 235
denotes the number of TSA layers, fl
outdenotes the fan-out of neurons in layer l,nl(t)denotes the 236
number of spikes fired in layer lat time τandNTSA denotes the number of TSA neurons. The 237
first term on the right-hand side of the equation arises from the TSA’s requirement to amplify the 238
membrane potential. Note that due to the accumulation of Tsover the network depth, the time period 239
for counting nl(t)varies with l. 240
Experiments were conducted on MNIST using LeNet-5. We varied the silent periods and adjusted 241
the coding steps to study their effects on OPF. The results are presented in fig. 4(a). As indicated in 242
eq. (10), reducing Tclowers energy overhead. This presents a trade-off between energy consumption 243
and inference accuracy, as fewer coding steps also reduce the number of values that can be accurately 244
encoded. A larger Tsrequires TSA neurons to perform more operations to amplify the membrane 245
potential. On the other hand, it reduces the number of unnecessary pulse emissions. Overall, silent 246
period has a negligible impact on OPF. 247
80.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Residual (normalized)020406080100 Prob. density/uni0000000b/uni00000044/uni0000000c
Vl
th=l,Ts=0
Vl
th=l,Ts=1
Vl
th=l,Ts=2
Vl
th=l
2,Ts=0
Vl
th=l
2,Ts=1
Vl
th=l
2,Ts=2
0.20 0.25 0.30200250300350400
020406080100 CIFAR10 acc (%)
9.9184.21 94.89
35.4195.67 95.68/uni0000000b/uni00000045/uni0000000c
Vl
th=l,Ts=0
Vl
th=l,Ts=1
Vl
th=l,Ts=2
Vl
th=l
2,Ts=0
Vl
th=l
2,Ts=1
Vl
th=l
2,Ts=2
Figure 5: (a) The probability density of the residuals with/without a lowered Vl
thand a silent period.
(b) Inference accuracy of SWS-ResNet18 on CIFAR10 with/without a lowered Vl
thand a silent
period.
In fig. 4(b), the energy consumption of SWS-based SNN is compared with that of other SNNs. The 248
experimental results demonstrate that our coding scheme can achieve a favorable balance between 249
accuracy and energy consumption. The SWS coding scheme is superior to rate coding and temporal 250
pattern coding in that it requires fewer operations and achieves higher accuracy. In TTFS encoding, 251
each neuron fires at most one spike at a time, theoretically demanding the least OPF. With Tc= 4, 252
SWS-SNN can achieve significantly higher accuracy with minimal increase in OPF. Note that if 253
the ANN is quantized to a lower number of bits (e.g., 4 bits), the error caused by the reduced Tc 254
can actually be compensated by the quantization algorithm, which can potentially result in a higher 255
performance. 256
4.4 Ablation study 257
In section 3.2 and section 3.3, we proposed reducing the firing threshold and introducing a silent 258
period to mitigate residual error. To assess the impact of these two adjustments, we conducted 259
experiments on CIFAR10 using ResNet18. After the neural computation, the residuals (absolute 260
values) of the TSA neurons were analyzed. We first scaled the residuals by 1/βTsto counteract the 261
effect of membrane potential amplification caused by the silent period, and then normalized them in 262
units of θl. The probability density of the residuals is shown in fig. 5(a). 263
The results demonstrate that lowering Vl
thshifts the residual distribution from around 0.5θlto 264
approximately 0.25θl, corresponding to the quantization errors (i.e. rounding errors) under their 265
respective thresholds. The addition of silent periods further concentrates the distribution and reduces 266
large deviations. As can be seen from the green curve in fig. 5(a), setting Tsto2andVl
thtoθl/2 267
makes the residuals almost all distributed around the quantization error. Compared to the red curve 268
(without a lowered Vl
thor a silent period), the residuals are greatly reduced, which fully proves the 269
effectiveness of lowering the threshold and adding a silent period. The inference results on CIFAR10 270
is shown in fig. 5(b). When setting Vl
thtoθlandTsto zero, the network’s output is almost random. 271
Lowering the threshold and adding a silent period improve the accuracy to 35.41% and84.21%, 272
respectively. Ultimately, the combination of both adjustments enabled SWS-ResNet18 to achieve an 273
accuracy of 95.68% on CIFAR10. 274
5 Conclusion 275
In this work, we have proposed a novel SWS spike coding scheme. The stepwise weighting process 276
enhances the information-carrying capacity of the preceding pulses, greatly reducing the number of 277
time steps for encoding. Combined with a silent period, our proposed TSA neuron model solves the 278
problem of residual errors and achieves fast and accurate information transmission. Our experimental 279
results have demonstrated that SWS coding is highly effective in extremely deep SNNs and achieves 280
state-of-the-art accuracy. The SWS coding scheme is also highly flexible and can adapt to various 281
needs. 282
9References 283
[1]Bu, T., Ding, J., Yu, Z., Huang, T.: Optimized potential initialization for low-latency spiking 284
neural networks (2022) 285
[2]Bu, T., Fang, W., Ding, J., Dai, P., Yu, Z., Huang, T.: Optimal ann-snn conversion for high- 286
accuracy and ultra-low-latency spiking neural networks (2023) 287
[3]Cao, Y ., Chen, Y ., Khosla, D.: Spiking deep convolutional neural networks for 288
energy-efficient object recognition. International Journal of Computer Vision 113(1), 54– 289
66 (May 2015). https://doi.org/10.1007/s11263-014-0788-3, https://doi.org/10.1007/ 290
s11263-014-0788-3 291
[4]Deng, S., Gu, S.: Optimal conversion of conventional artificial neural networks to spiking neural 292
networks (2021) 293
[5]Deng, S., Li, Y ., Zhang, S., Gu, S.: Temporal efficient training of spiking neural network via 294
gradient re-weighting (2022) 295
[6]Diehl, P.U., Neil, D., Binas, J., Cook, M., Liu, S.C., Pfeiffer, M.: Fast-classifying, 296
high-accuracy spiking deep networks through weight and threshold balancing. In: 297
2015 International Joint Conference on Neural Networks (IJCNN). pp. 1–8 (2015). 298
https://doi.org/10.1109/IJCNN.2015.7280696 299
[7]Ding, J., Yu, Z., Tian, Y ., Huang, T.: Optimal ann-snn conversion for fast and accurate inference 300
in deep spiking neural networks (2021) 301
[8]Fang, W., Yu, Z., Chen, Y ., Huang, T., Masquelier, T., Tian, Y .: Deep residual learning in 302
spiking neural networks (2022) 303
[9]Guo, W., Fouda, M.E., Eltawil, A.M., Salama, K.N.: Neural coding in spiking neural net- 304
works: A comparative study for robust neuromorphic systems. Frontiers in Neuroscience 305
15(2021). https://doi.org/10.3389/fnins.2021.638474, https://www.frontiersin.org/ 306
journals/neuroscience/articles/10.3389/fnins.2021.638474 307
[10] Han, B., Roy, K.: Deep spiking neural network: Energy efficiency through time based coding. 308
In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision – ECCV 2020. pp. 309
388–404. Springer International Publishing, Cham (2020) 310
[11] Han, B., Srinivasan, G., Roy, K.: Rmp-snn: Residual membrane potential neuron for enabling 311
deeper high-accuracy and low-latency spiking neural network (2020) 312
[12] Hao, Z., Ding, J., Bu, T., Huang, T., Yu, Z.: Bridging the gap between anns and snns by 313
calibrating offset spikes (2023) 314
[13] Hu, Y ., Tang, H., Pan, G.: Spiking deep residual networks. IEEE Transac- 315
tions on Neural Networks and Learning Systems 34(8), 5200–5205 (2023). 316
https://doi.org/10.1109/TNNLS.2021.3119238 317
[14] Hu, Y ., Zheng, Q., Jiang, X., Pan, G.: Fast-snn: Fast spiking neural network by converting 318
quantized ann. IEEE Transactions on Pattern Analysis and Machine Intelligence 45(12), 14546– 319
14562 (2023). https://doi.org/10.1109/TPAMI.2023.3275769 320
[15] Kim, J., Kim, H., Huh, S., Lee, J., Choi, K.: Deep neural networks with weighted spikes. Neuro- 321
computing 311, 373–386 (2018). https://doi.org/https://doi.org/10.1016/j.neucom.2018.05.087, 322
https://www.sciencedirect.com/science/article/pii/S0925231218306726 323
[16] Kim, Y ., Li, Y ., Park, H., Venkatesha, Y ., Hambitzer, A., Panda, P.: Exploring temporal 324
information dynamics in spiking neural networks (2022) 325
[17] Lee, C., Sarwar, S.S., Panda, P., Srinivasan, G., Roy, K.: Enabling spike-based back- 326
propagation for training deep neural network architectures. Frontiers in Neuroscience 327
14(Feb 2020). https://doi.org/10.3389/fnins.2020.00119, http://dx.doi.org/10.3389/ 328
fnins.2020.00119 329
10[18] Li, Y ., Deng, S., Dong, X., Gong, R., Gu, S.: A free lunch from ann: Towards efficient, accurate 330
spiking neural networks calibration (2021) 331
[19] Meng, Q., Xiao, M., Yan, S., Wang, Y ., Lin, Z., Luo, Z.Q.: Training high-performance low- 332
latency spiking neural networks by differentiation on spike representation (2023) 333
[20] Meng, Q., Yan, S., Xiao, M., Wang, Y ., Lin, Z., Luo, Z.Q.: Training much deeper spiking 334
neural networks with a small number of time-steps. Neural Networks 153, 254–268 (2022). 335
https://doi.org/https://doi.org/10.1016/j.neunet.2022.06.001, https://www.sciencedirect. 336
com/science/article/pii/S0893608022002064 337
[21] Park, S., Kim, S., Choe, H., Yoon, S.: Fast and efficient information transmission with burst 338
spikes in deep spiking neural networks (2019) 339
[22] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., 340
Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, 341
A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, 342
high-performance deep learning library (2019) 343
[23] Pérez-Carrasco, J.A., Zhao, B., Serrano, C., Acha, B., Serrano-Gotarredona, T., Chen, S., 344
Linares-Barranco, B.: Mapping from frame-driven to frame-free event-driven vision sys- 345
tems by low-rate rate coding and coincidence processing–application to feedforward convnets. 346
IEEE Transactions on Pattern Analysis and Machine Intelligence 35(11), 2706–2719 (2013). 347
https://doi.org/10.1109/TPAMI.2013.71 348
[24] Querlioz, D., Bichler, O., Dollfus, P., Gamrat, C.: Immunity to device variations in a spiking 349
neural network with memristive nanodevices. IEEE Transactions on Nanotechnology 12(3), 350
288–295 (2013). https://doi.org/10.1109/TNANO.2013.2250995 351
[25] Querlioz, D., Bichler, O., Gamrat, C.: Simulation of a memristor-based spiking neural network 352
immune to device variations. In: The 2011 International Joint Conference on Neural Networks. 353
pp. 1775–1781 (2011). https://doi.org/10.1109/IJCNN.2011.6033439 354
[26] Rathi, N., Srinivasan, G., Panda, P., Roy, K.: Enabling deep spiking neural networks with hybrid 355
conversion and spike timing dependent backpropagation (2020) 356
[27] Rueckauer, B., Liu, S.C.: Conversion of analog to spiking neural networks using sparse temporal 357
coding. In: 2018 IEEE International Symposium on Circuits and Systems (ISCAS). pp. 1–5 358
(2018). https://doi.org/10.1109/ISCAS.2018.8351295 359
[28] Rueckauer, B., Liu, S.C.: Temporal pattern coding in deep spiking neural networks. 360
In: 2021 International Joint Conference on Neural Networks (IJCNN). pp. 1–8 (2021). 361
https://doi.org/10.1109/IJCNN52387.2021.9533837 362
[29] Rueckauer, B., Lungu, I.A., Hu, Y ., Pfeiffer, M., Liu, S.C.: Conversion of continuous-valued 363
deep networks to efficient event-driven networks for image classification. Frontiers in Neuro- 364
science 11(2017). https://doi.org/10.3389/fnins.2017.00682 365
[30] Stöckl, C., Maass, W.: Optimized spiking neurons can classify images with high accu- 366
racy through temporal coding with two spikes. Nature Machine Intelligence 3(3), 230– 367
238 (Mar 2021). https://doi.org/10.1038/s42256-021-00311-4, https://doi.org/10.1038/ 368
s42256-021-00311-4 369
[31] Taherkhani, A., Belatreche, A., Li, Y ., Cosma, G., Maguire, L.P., McGinnity, T.: A re- 370
view of learning in biologically plausible spiking neural networks. Neural Networks 122, 371
253–272 (2020). https://doi.org/https://doi.org/10.1016/j.neunet.2019.09.036, https://www. 372
sciencedirect.com/science/article/pii/S0893608019303181 373
[32] Wang, X., Lin, X., Dang, X.: Supervised learning in spiking neural networks: 374
A review of algorithms and evaluations. Neural Networks 125, 258–280 (2020). 375
https://doi.org/https://doi.org/10.1016/j.neunet.2020.02.011, https://www.sciencedirect. 376
com/science/article/pii/S0893608020300563 377
11[33] Yamazaki, K., V o-Ho, V .K., Bulsara, D., Le, N.: Spiking neural networks and their applications: 378
A review. Brain Sci 12(7) (Jun 2022) 379
[34] Yan, Z., Zhou, J., Wong, W.: Near lossless transfer learning for spiking neural networks. In: 380
AAAI Conference on Artificial Intelligence (2021), https://api.semanticscholar.org/ 381
CorpusID:235349069 382
[35] Zheng, H., Wu, Y ., Deng, L., Hu, Y ., Li, G.: Going deeper with directly-trained larger spiking 383
neural networks (2020) 384
[36] Zhou, S., LI, X., Chen, Y ., Chandrasekaran, S.T., Sanyal, A.: Temporal-coded deep spiking 385
neural network with easy training and robust performance (2021) 386
A Appendix 387
A.1 Convert quantized ANNs to SWS-SNNs 388
A pretrained ANN was first obtained from torchvision, which is part of the PyTorch[ 22] project, and 389
then quantized into nbits following the Quantization-Aware Training (QAT) Workflow provided by 390
PyTorch ( 8bits in the actual experiment, with nbits used here for generality). The quantized ANN 391
can be characterized by the parameters listed in table 3, and the basic idea of the conversion process is 392
illustrated in fig. 6(a). The activations of the quantized ANN can be mapped to an integer Qbetween 393
[0,2n−1]using a scaling factor Cand a zero point Z. With the same weight and bias between Ql
i394
andQl
o, the TSA layer can generate Sl, which encodes Ql
o, provided that Sl−1encodes Ql
iand no 395
residual error occurs. In the actual SNN, the pulse amplitude θlis normalized to 1. Therefore, the 396
bias need to be further scaled to derive the final weight Wland bias blfor the SWS-SNN. 397
Table 3: The notations and meanings of parameters in the quantized network.
Notation Meaning
ˆXl
i The quantized input of the lthlayer
ˆXl
o The quantized output of the lthlayer
Cl
i The scaling factor of the quantized input of the lthlayer
Zl
i The zero point of the quantized input of the lthlayer
Cl
o The scaling factor of the quantized output of the lthlayer
Zl
o The zero point of the quantized output of the lthlayer
ˆWlThe quantized weight of layer l
Cl
w The scaling factor of the quantized weight of layer l
Zl
w The zero point of the quantized weight of layer l
ˆblThe bias of layer l
The derivation is as follows. After QAT, we have: 398
ˆWlˆXl
i+ˆbl=ˆXl
o, (11)
Ql
i=ˆXl
i
Cl
i+Zl
i, (12)
Ql
o=ˆXl
o
Clo+Zl
o, (13)
where Ql
i,Ql
orepresent the integers to which the quantized input and output are mapped, respectively. 399
Substitute eq. (12) and eq. (13) into eq. (11), and we can write: 400
ˆWl(Ql
i−Zl
i)Cl
i+ˆbl= (Ql
o−Zl
o)Cl
o, (14)
which gives: 401
Ql
o=ˆWlCl
i
CloQl
i+ˆbl
Clo+Zl
o−ˆWlZl
iCl
i
Clo
=˜WlQl
i+˜bl,(15)
12Figure 6: (a) Convert quantized ANNs to SWS-SNNs. Ql
iandQl
orepresent the integers to which ˆXl
i
andˆXl
oare mapped, respectively. ˜Wland˜bldenotes the weight and bias to get Ql
ofromQl
i.Wland
bldenotes the weight and bias in SWS-SNN. The process of transferring weights and biases from
the quantized ANN to SWS-SNN is indicated by white arrows. The core of the conversion is that
the distribution of the integer Ql
ois known and can be easily encoded by Sl. (b) Process the input
pixels to encode by pulses with an amplitude of 1.¯Pdenotes the original pixel value, Pdenotes the
mapped value and ˜Pdenotes the value after scaled by 1/θ0.
where 402
˜Wl=ˆWlCl
i
Clo, (16)
˜bl=ˆbl
Clo+Zl
o−ˆWlZl
iCl
i
Clo. (17)
As seen in eq. (15), with the weight and bias set to ˜Wland˜blrespectively, the layer outputs Ql
owhen 403
receiving Ql
i. The pulse amplitude θlcan be set to any value as long as the codable range calculated 404
by eq. (7) covers [0,2n−1]. Then we have: 405
Wl=˜Wlθl−1
θl=ˆWlCl
i
Cloθl−1
θl(18)
Considering the membrane potential amplification, blcan be calculated as follows: 406
bl=1PTc
τ=1βTc−τ˜bl=1PTc
τ=1βTc−τ(ˆbl
Clo+Zl
o−ˆWlZl
iCl
i
Clo) (19)
Once the Tc,βandθl(θl−1is given by the previous layer) have been determined, all values on the 407
right side of eq. (18) and eq. (19) are known. Consequently, Wlandblin the SWS-SNN can be 408
readily calculated from the weight and bias of the quantized ANN. 409
After configuring the weights and biases as described above, the input pixel must be encoded into a 410
pulse sequence with an amplitude of 1 as well. This process is illustrated in fig. 6(b). First, map the 411
pixel value to [0,2n−1]using C0
iandZ0
iobtained from QAT. Assuming this range can be encoded 412
by SWSs with an amplitude of θ0, scaling the pixel value by 1/θ0allows the use of a sequence with 413
θ0= 1for encoding. Finally, encode the scaled pixels following section 3.4, and the required input 414
spike sequence is obtained. 415
A.2 Details for QAT 416
QAT is the quantization method that typically results in the highest accuracy. We basically follows 417
the workflow provided by PyTorch. The default QAT quantization configuration is chosen to specify 418
the kind of fake-quantization inserted after weights and activations. We choose Stochastic Gradient 419
Descent (SGD) optimizer in QAT, with the value of momentum set to 0.9and the learning rate set to 420
1×10−4since the weights only need to be fine-tuned. QAT is done for 12 epochs and 20 batches in 421
each epoch. We freeze the batch norm mean and variance estimates after three epochs and freeze the 422
quantizer parameters (scaling factor and zero point) after another two epochs. 423
13NeurIPS Paper Checklist 424
1.Claims 425
Question: Do the main claims made in the abstract and introduction accurately reflect the 426
paper’s contributions and scope? 427
Answer: [Yes] 428
Justification: Stepwise weighting enhances the encoding of information in spikes, as is 429
proved in eq. (5) in section 3.1. Our proposed SWS coding scheme achieves high perfor- 430
mance and low energy consumption, which is supported by our experimental results in 431
section 4. The TSA neuron model effectively minimizes the residual error, which can be 432
proved from the ablation study in section 4.4. 433
Guidelines: 434
•The answer NA means that the abstract and introduction do not include the claims 435
made in the paper. 436
•The abstract and/or introduction should clearly state the claims made, including the 437
contributions made in the paper and important assumptions and limitations. A No or 438
NA answer to this question will not be perceived well by the reviewers. 439
•The claims made should match theoretical and experimental results, and reflect how 440
much the results can be expected to generalize to other settings. 441
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 442
are not attained by the paper. 443
2.Limitations 444
Question: Does the paper discuss the limitations of the work performed by the authors? 445
Answer: [Yes] 446
Justification: The inclusion of silent periods can lead to increased latency, as noted in 447
section 3.3, which is a limitation we’ve found so far. However, our experimental results 448
demonstrate that our delay performance still surpasses that of other SNNs. 449
Guidelines: 450
•The answer NA means that the paper has no limitation while the answer No means that 451
the paper has limitations, but those are not discussed in the paper. 452
• The authors are encouraged to create a separate "Limitations" section in their paper. 453
•The paper should point out any strong assumptions and how robust the results are to 454
violations of these assumptions (e.g., independence assumptions, noiseless settings, 455
model well-specification, asymptotic approximations only holding locally). The authors 456
should reflect on how these assumptions might be violated in practice and what the 457
implications would be. 458
•The authors should reflect on the scope of the claims made, e.g., if the approach was 459
only tested on a few datasets or with a few runs. In general, empirical results often 460
depend on implicit assumptions, which should be articulated. 461
•The authors should reflect on the factors that influence the performance of the approach. 462
For example, a facial recognition algorithm may perform poorly when image resolution 463
is low or images are taken in low lighting. Or a speech-to-text system might not be 464
used reliably to provide closed captions for online lectures because it fails to handle 465
technical jargon. 466
•The authors should discuss the computational efficiency of the proposed algorithms 467
and how they scale with dataset size. 468
•If applicable, the authors should discuss possible limitations of their approach to 469
address problems of privacy and fairness. 470
•While the authors might fear that complete honesty about limitations might be used by 471
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 472
limitations that aren’t acknowledged in the paper. The authors should use their best 473
judgment and recognize that individual actions in favor of transparency play an impor- 474
tant role in developing norms that preserve the integrity of the community. Reviewers 475
will be specifically instructed to not penalize honesty concerning limitations. 476
143.Theory Assumptions and Proofs 477
Question: For each theoretical result, does the paper provide the full set of assumptions and 478
a complete (and correct) proof? 479
Answer: [Yes] 480
Justification: The membrane potential amplification enhances the information-carrying 481
capacity of the preceding pulses and is proved in eq. (5). 482
Guidelines: 483
• The answer NA means that the paper does not include theoretical results. 484
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 485
referenced. 486
•All assumptions should be clearly stated or referenced in the statement of any theorems. 487
•The proofs can either appear in the main paper or the supplemental material, but if 488
they appear in the supplemental material, the authors are encouraged to provide a short 489
proof sketch to provide intuition. 490
•Inversely, any informal proof provided in the core of the paper should be complemented 491
by formal proofs provided in appendix or supplemental material. 492
• Theorems and Lemmas that the proof relies upon should be properly referenced. 493
4.Experimental Result Reproducibility 494
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 495
perimental results of the paper to the extent that it affects the main claims and/or conclusions 496
of the paper (regardless of whether the code and data are provided or not)? 497
Answer: [Yes] 498
Justification: We set specific random number seeds when conducting experiments to ensure 499
that all the results of section 4 are reproducible. 500
Guidelines: 501
• The answer NA means that the paper does not include experiments. 502
•If the paper includes experiments, a No answer to this question will not be perceived 503
well by the reviewers: Making the paper reproducible is important, regardless of 504
whether the code and data are provided or not. 505
•If the contribution is a dataset and/or model, the authors should describe the steps taken 506
to make their results reproducible or verifiable. 507
•Depending on the contribution, reproducibility can be accomplished in various ways. 508
For example, if the contribution is a novel architecture, describing the architecture fully 509
might suffice, or if the contribution is a specific model and empirical evaluation, it may 510
be necessary to either make it possible for others to replicate the model with the same 511
dataset, or provide access to the model. In general. releasing code and data is often 512
one good way to accomplish this, but reproducibility can also be provided via detailed 513
instructions for how to replicate the results, access to a hosted model (e.g., in the case 514
of a large language model), releasing of a model checkpoint, or other means that are 515
appropriate to the research performed. 516
•While NeurIPS does not require releasing code, the conference does require all submis- 517
sions to provide some reasonable avenue for reproducibility, which may depend on the 518
nature of the contribution. For example 519
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 520
to reproduce that algorithm. 521
(b)If the contribution is primarily a new model architecture, the paper should describe 522
the architecture clearly and fully. 523
(c)If the contribution is a new model (e.g., a large language model), then there should 524
either be a way to access this model for reproducing the results or a way to reproduce 525
the model (e.g., with an open-source dataset or instructions for how to construct 526
the dataset). 527
(d)We recognize that reproducibility may be tricky in some cases, in which case 528
authors are welcome to describe the particular way they provide for reproducibility. 529
15In the case of closed-source models, it may be that access to the model is limited in 530
some way (e.g., to registered users), but it should be possible for other researchers 531
to have some path to reproducing or verifying the results. 532
5.Open access to data and code 533
Question: Does the paper provide open access to the data and code, with sufficient instruc- 534
tions to faithfully reproduce the main experimental results, as described in supplemental 535
material? 536
Answer: [No] 537
Justification: Code will be released when the paper is accepted. 538
Guidelines: 539
• The answer NA means that paper does not include experiments requiring code. 540
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 541
public/guides/CodeSubmissionPolicy ) for more details. 542
•While we encourage the release of code and data, we understand that this might not be 543
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 544
including code, unless this is central to the contribution (e.g., for a new open-source 545
benchmark). 546
•The instructions should contain the exact command and environment needed to run to 547
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 548
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 549
•The authors should provide instructions on data access and preparation, including how 550
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 551
•The authors should provide scripts to reproduce all experimental results for the new 552
proposed method and baselines. If only a subset of experiments are reproducible, they 553
should state which ones are omitted from the script and why. 554
•At submission time, to preserve anonymity, the authors should release anonymized 555
versions (if applicable). 556
•Providing as much information as possible in supplemental material (appended to the 557
paper) is recommended, but including URLs to data and code is permitted. 558
6.Experimental Setting/Details 559
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 560
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 561
results? 562
Answer: [Yes] 563
Justification: The details for acquiring different delays and the OPF calculation method 564
are provided in section 4.2 and section 4.3, respectively. The parameters used during QAT 565
training is outlined in appendix A.2. 566
Guidelines: 567
• The answer NA means that the paper does not include experiments. 568
•The experimental setting should be presented in the core of the paper to a level of detail 569
that is necessary to appreciate the results and make sense of them. 570
•The full details can be provided either with the code, in appendix, or as supplemental 571
material. 572
7.Experiment Statistical Significance 573
Question: Does the paper report error bars suitably and correctly defined or other appropriate 574
information about the statistical significance of the experiments? 575
Answer: [No] 576
Justification: We believe it is not necessary to include error bars in the results because each 577
experimental result itself is already the average of a large number of tests (E.g., the test 578
accuracy for an epoch is averaged over Num _of_batches ×Batch _size input images, 579
and is therefore very close to each other in every test epoch). 580
Guidelines: 581
16• The answer NA means that the paper does not include experiments. 582
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 583
dence intervals, or statistical significance tests, at least for the experiments that support 584
the main claims of the paper. 585
•The factors of variability that the error bars are capturing should be clearly stated (for 586
example, train/test split, initialization, random drawing of some parameter, or overall 587
run with given experimental conditions). 588
•The method for calculating the error bars should be explained (closed form formula, 589
call to a library function, bootstrap, etc.) 590
• The assumptions made should be given (e.g., Normally distributed errors). 591
•It should be clear whether the error bar is the standard deviation or the standard error 592
of the mean. 593
•It is OK to report 1-sigma error bars, but one should state it. The authors should 594
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 595
of Normality of errors is not verified. 596
•For asymmetric distributions, the authors should be careful not to show in tables or 597
figures symmetric error bars that would yield results that are out of range (e.g. negative 598
error rates). 599
•If error bars are reported in tables or plots, The authors should explain in the text how 600
they were calculated and reference the corresponding figures or tables in the text. 601
8.Experiments Compute Resources 602
Question: For each experiment, does the paper provide sufficient information on the com- 603
puter resources (type of compute workers, memory, time of execution) needed to reproduce 604
the experiments? 605
Answer: [No] 606
Justification: We found it difficult to quantify the computing resources used in every 607
experiments. 608
Guidelines: 609
• The answer NA means that the paper does not include experiments. 610
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 611
or cloud provider, including relevant memory and storage. 612
•The paper should provide the amount of compute required for each of the individual 613
experimental runs as well as estimate the total compute. 614
•The paper should disclose whether the full research project required more compute 615
than the experiments reported in the paper (e.g., preliminary or failed experiments that 616
didn’t make it into the paper). 617
9.Code Of Ethics 618
Question: Does the research conducted in the paper conform, in every respect, with the 619
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 620
Answer: [Yes] 621
Justification: We have read the NeurIPS Code of Ethics and the research conducted in this 622
paper conforms with it. 623
Guidelines: 624
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 625
•If the authors answer No, they should explain the special circumstances that require a 626
deviation from the Code of Ethics. 627
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 628
eration due to laws or regulations in their jurisdiction). 629
10.Broader Impacts 630
Question: Does the paper discuss both potential positive societal impacts and negative 631
societal impacts of the work performed? 632
Answer: [NA] 633
17Justification: There is no societal impact of the work performed. 634
Guidelines: 635
• The answer NA means that there is no societal impact of the work performed. 636
•If the authors answer NA or No, they should explain why their work has no societal 637
impact or why the paper does not address societal impact. 638
•Examples of negative societal impacts include potential malicious or unintended uses 639
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 640
(e.g., deployment of technologies that could make decisions that unfairly impact specific 641
groups), privacy considerations, and security considerations. 642
•The conference expects that many papers will be foundational research and not tied 643
to particular applications, let alone deployments. However, if there is a direct path to 644
any negative applications, the authors should point it out. For example, it is legitimate 645
to point out that an improvement in the quality of generative models could be used to 646
generate deepfakes for disinformation. On the other hand, it is not needed to point out 647
that a generic algorithm for optimizing neural networks could enable people to train 648
models that generate Deepfakes faster. 649
•The authors should consider possible harms that could arise when the technology is 650
being used as intended and functioning correctly, harms that could arise when the 651
technology is being used as intended but gives incorrect results, and harms following 652
from (intentional or unintentional) misuse of the technology. 653
•If there are negative societal impacts, the authors could also discuss possible mitigation 654
strategies (e.g., gated release of models, providing defenses in addition to attacks, 655
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 656
feedback over time, improving the efficiency and accessibility of ML). 657
11.Safeguards 658
Question: Does the paper describe safeguards that have been put in place for responsible 659
release of data or models that have a high risk for misuse (e.g., pretrained language models, 660
image generators, or scraped datasets)? 661
Answer: [NA] 662
Justification: The paper poses no such risks. 663
Guidelines: 664
• The answer NA means that the paper poses no such risks. 665
•Released models that have a high risk for misuse or dual-use should be released with 666
necessary safeguards to allow for controlled use of the model, for example by requiring 667
that users adhere to usage guidelines or restrictions to access the model or implementing 668
safety filters. 669
•Datasets that have been scraped from the Internet could pose safety risks. The authors 670
should describe how they avoided releasing unsafe images. 671
•We recognize that providing effective safeguards is challenging, and many papers do 672
not require this, but we encourage authors to take this into account and make a best 673
faith effort. 674
12.Licenses for existing assets 675
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 676
the paper, properly credited and are the license and terms of use explicitly mentioned and 677
properly respected? 678
Answer: [Yes] 679
Justification: The pretrained ANN model and the QAT workflow is provided by PyTorch 680
and we cited the original paper in appendix A.1 as [22]. 681
Guidelines: 682
• The answer NA means that the paper does not use existing assets. 683
• The authors should cite the original paper that produced the code package or dataset. 684
•The authors should state which version of the asset is used and, if possible, include a 685
URL. 686
18• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 687
•For scraped data from a particular source (e.g., website), the copyright and terms of 688
service of that source should be provided. 689
•If assets are released, the license, copyright information, and terms of use in the 690
package should be provided. For popular datasets, paperswithcode.com/datasets 691
has curated licenses for some datasets. Their licensing guide can help determine the 692
license of a dataset. 693
•For existing datasets that are re-packaged, both the original license and the license of 694
the derived asset (if it has changed) should be provided. 695
•If this information is not available online, the authors are encouraged to reach out to 696
the asset’s creators. 697
13.New Assets 698
Question: Are new assets introduced in the paper well documented and is the documentation 699
provided alongside the assets? 700
Answer: [NA] 701
Justification: The paper does not release new assets. 702
Guidelines: 703
• The answer NA means that the paper does not release new assets. 704
•Researchers should communicate the details of the dataset/code/model as part of their 705
submissions via structured templates. This includes details about training, license, 706
limitations, etc. 707
•The paper should discuss whether and how consent was obtained from people whose 708
asset is used. 709
•At submission time, remember to anonymize your assets (if applicable). You can either 710
create an anonymized URL or include an anonymized zip file. 711
14.Crowdsourcing and Research with Human Subjects 712
Question: For crowdsourcing experiments and research with human subjects, does the paper 713
include the full text of instructions given to participants and screenshots, if applicable, as 714
well as details about compensation (if any)? 715
Answer: [NA] 716
Justification: The paper does not involve crowdsourcing nor research with human subjects. 717
Guidelines: 718
•The answer NA means that the paper does not involve crowdsourcing nor research with 719
human subjects. 720
•Including this information in the supplemental material is fine, but if the main contribu- 721
tion of the paper involves human subjects, then as much detail as possible should be 722
included in the main paper. 723
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 724
or other labor should be paid at least the minimum wage in the country of the data 725
collector. 726
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 727
Subjects 728
Question: Does the paper describe potential risks incurred by study participants, whether 729
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 730
approvals (or an equivalent approval/review based on the requirements of your country or 731
institution) were obtained? 732
Answer: [NA] 733
Justification: The paper does not involve crowdsourcing nor research with human subjects. 734
Guidelines: 735
•The answer NA means that the paper does not involve crowdsourcing nor research with 736
human subjects. 737
19•Depending on the country in which research is conducted, IRB approval (or equivalent) 738
may be required for any human subjects research. If you obtained IRB approval, you 739
should clearly state this in the paper. 740
•We recognize that the procedures for this may vary significantly between institutions 741
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 742
guidelines for their institution. 743
•For initial submissions, do not include any information that would break anonymity (if 744
applicable), such as the institution conducting the review. 745
20