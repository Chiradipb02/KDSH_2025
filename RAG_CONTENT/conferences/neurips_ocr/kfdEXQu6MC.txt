A generalized neural tangent kernel
for surrogate gradient learning
Luke Eilers∗
Department of Physiology, University of Bern, Switzerland
Institute for Applied Mathematics, University of Bonn, Germany
luke.eilers@unibe.ch
Raoul-Martin Memmesheimer
Institute of Genetics, University of Bonn, Germany
rm.memmesheimer@uni-bonn.de
Sven Goedeke
Bernstein Center Freiburg, University of Freiburg, Germany
Institute of Genetics, University of Bonn, Germany
sven.goedeke@bcf.uni-freiburg.de
Abstract
State-of-the-art neural network training methods depend on the gradient of the
network function. Therefore, they cannot be applied to networks whose activation
functions do not have useful derivatives, such as binary and discrete-time spiking
neural networks. To overcome this problem, the activation function’s derivative is
commonly substituted with a surrogate derivative, giving rise to surrogate gradient
learning (SGL). This method works well in practice but lacks theoretical foundation.
The neural tangent kernel (NTK) has proven successful in the analysis of gradient
descent. Here, we provide a generalization of the NTK, which we call the surrogate
gradient NTK, that enables the analysis of SGL. First, we study a naive extension
of the NTK to activation functions with jumps, demonstrating that gradient descent
for such activation functions is also ill-posed in the infinite-width limit. To address
this problem, we generalize the NTK to gradient descent with surrogate derivatives,
i.e., SGL. We carefully define this generalization and expand the existing key
theorems on the NTK with mathematical rigor. Further, we illustrate our findings
with numerical experiments. Finally, we numerically compare SGL in networks
with sign activation function and finite width to kernel regression with the surrogate
gradient NTK; the results confirm that the surrogate gradient NTK provides a good
characterization of SGL.
1 Introduction
Artificial neural networks (ANNs) originate from the biologically inspired perceptron [Rosenblatt,
1958]. While the perceptron has a binary output that is faithful to the all-or-none behavior of spiking
neurons in the nervous system, most activation functions considered nowadays for ANNs are smooth
(like the logistic function) or at least semi-differentiable (like the ReLU function). Differentiable
network functions enable the learning of network weights with methods that leverage the gradient
of the network function with respect to the network weights like backpropagation [Rumelhart et al.,
∗Corresponding author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).1986]. These methods are very successful [LeCun et al., 2015], but cannot be used without well-
defined gradients.
This is a problem when considering more biologically plausible ANNs, which are typically used
in computational neuroscience to understand how the networks of spiking neurons in our nervous
system work. These include spiking neural networks (SNNs). Motivated by the energy efficiency of
our brain, SNNs and similar networks, such as binary neural networks (BNNs), are considered in
the context of neuromorphic computing [Merolla et al., 2014]. Both discrete-time SNNs and BNNs
have in common that their activation functions do not have useful derivatives, which renders standard
gradient-descent training impossible [Neftci et al., 2019, Taherkhani et al., 2020, Tavanaei et al.,
2019, Roy et al., 2019, Pfeiffer and Pfeil, 2018]. For the scope of this paper, these activation functions
can be thought of as step-like functions like the sign function, in which cases the derivative vanishes
almost everywhere and is thus no longer informative about the shape of the activation function.
Surrogate gradient learning resolves this issue by providing the missing information about the
activation function in the form of a surrogate derivative [Hinton, 2012, Bengio et al., 2013, Zenke and
Ganguli, 2018]. As a result, the gradient-based methods for classical ANNs can be leveraged with
great success [Zenke and V ogels, 2021]. However, a theoretical basis underpinning the intuition is
missing and it is often unclear which surrogate derivative should be chosen for a particular network.
For a review focusing on surrogate gradient learning methods, which we are most interested in, see
Neftci et al. [2019]. For a comprehensive review of other learning methods for SNNs, we refer to
Taherkhani et al. [2020], Tavanaei et al. [2019], and Eshraghian et al. [2021].
The neural tangent kernel (NTK) introduced by Jacot et al. [2018] allows to formulate gradient
descent as a kernel method. Just as ANNs with randomly initialized weights converge under certain
conditions to Gaussian processes (GPs) in the infinite-width limit [Matthews et al., 2018, Lee et al.,
2018], the NTK converges at initialization and during training to a deterministic kernel in the same
limit. Moreover, the NTK then describes how the network function changes under gradient descent in
the infinite-width regime. This led to both a better theoretical understanding of gradient descent and
practical applications to neural network training; see Section 1.2 for more details.
1.1 Contribution
We study the NTK for networks with sign function as activation function. As the NTK theory is not
directly applicable due to an ill-defined derivative, we consider the NTK for a sequence of activation
functions that approximates the sign function and then derive a principled way of generalizing the
NTK to gain more theoretical insight into surrogate gradient learning. Our contributions are as
follows:
•We provide a clear definition of the infinite-width limit in Section 2.2, capturing the different
notions used in the literature due to the different choices of rates at which the layer widths increase.
This definition is used consistently in all mathematical statements and the respective proofs.
•In Section 2.3, we demonstrate that the direct extension of the NTK for the sign activation function
using an approximating sequence is not well-defined due to the divergence of the kernel’s diagonal.
This illustrates, from the NTK perspective, that gradient descent is ill-defined for activation
functions with jumps and how this problem will be mitigated by surrogate gradient learning.
Moreover, we connect this divergence to results by Radhakrishnan et al. [2023] in Theorem 2.3 and
show that the direct extension of the NTK can be seen as a well-defined kernel for classification.
•We define a generalized version of the NTK in Section 2.4 using so-called quasi-Jacobian matrices
and prove the convergence to a deterministic, in general asymmetric, kernel in the infinite width
limit at initialization in Theorem 2.4. Using the generalized NTK, we formulate surrogate gradient
learning in terms of the generalized NTK for networks with differentiable activation functions.
This novel NTK is named the SG-NTK and we prove its convergence to a deterministic kernel
during training in Theorem 2.5.
•For both the diverging direct extension of the NTK and the SG-NTK with sign activation function
and arbitrary surrogate derivative, we derive exact analytical expressions in sections D.1, D.2 and
E.2. In particular, we identify the terms that emerge from SGL and that prevent divergence.
•In Section 3, we illustrate our findings, in particular Theorem 2.4 and Theorem 2.5, using simu-
lations. Numerical experiments show that the distribution of networks trained with SGL shows
agreement with the distribution given by the SG-NTK.
2Mathematically precise versions of all statements can be found in the appendix, which is self-
contained to ensure rigor and a consistent notation throughout all theorems and proofs.
1.2 Related work
The neural tangent kernel. The convergence of randomly initialized ANNs in the infinite-width
limit to Gaussian processes under appropriate scaling of the weights has first been described by
Neal [1996] for a single hidden layer and has been extended to multiple hidden layers and other
network architectures like CNNs [Matthews et al., 2018, Garriga-Alonso et al., 2018, Yang, 2019b,
Lee et al., 2018]. The NTK was introduced by Jacot et al. [2018] with first results on its convergence
at network initialization and during training. Theoretical results on the implication of the convergence
of NTKs on the behaviour of trained wide ANNs were given by Lee et al. [2019], Arora et al. [2019],
Allen-Zhu et al. [2019]. In Section C.2, we review central theorems on the NTK to enable a clear
comparison to our theoretical results.
The NTK has been generalized to all kinds of ANN standard architectures Yang [2020] such as CNNs
Arora et al. [2019] and attention layers Hron et al. [2020]. In particular, it has been generalized to
RNNs Alemohammad et al. [2020], which are particularly interesting in light of SNNs due to their
shared temporal dimension. Bordelon and Pehlevan [2022] derive a generalization of the NTK called
effective NTK for different learning rules using an approach similar to ours. Note that all of these
extensions require well-defined gradients.
The ability of overparameterized neural networks to converge during training and to generalize can be
explained using the NTK [Allen-Zhu et al., 2019, Bordelon et al., 2020, Bietti and Mairal, 2019, Du
et al., 2019]. The NTK has also been used in more applied areas such as neural architecture search
[Chen et al., 2021] and dataset distillation [Nguyen et al., 2021].
Surrogate gradient learning. In the context of computational neuroscience, the idea of replacing
the derivative of the output of a spiking neuron with a surrogate derivative was introduced by Bohte
[2011]. To deal with the temporal component in SNNs or more generally RNNs, the resulting gradient
is usually combined with backpropagation through time (BPTT). Prominent examples of surrogate
gradient approaches include SuperSpike by Zenke and Ganguli [2018] and a number of works with
different surrogate derivatives [Wu et al., 2018, 2019, Shrestha and Orchard, 2018, Bellec et al., 2018,
Esser et al., 2016, Wo´ zniak et al., 2020]. In the general ANN literature, the method is better known as
straight-through estimation (STE) and was introduced by Hinton [2012] and by Bengio et al. [2013]
in more detail. It was successfully applied by Hubara et al. [2016] and Cai et al. [2017].
Surrogate gradient learning or STE is only heuristically motivated and it is hence desirable to derive
a theoretical basis. The influence of different surrogate derivatives on the method has been analyzed
through systematic numerical simulations Zenke and V ogels [2021], revealing that the particular
shape has a minor impact compared to the scale. In a more theoretical work, Yin et al. [2019] analyzed
the convergence of STE for a Heaviside activation function with three different surrogate derivatives
and found that the descent directions of the respective surrogate gradients reduce the population loss
when the clipped ReLU function is used as surrogate derivative. Gygax and Zenke [2024] examine
how SGL is connected to smoothed probabilistic models [Bengio et al., 2013, Neftci et al., 2019,
Jang et al., 2019] and to stochastic automatic differentiation [Arya et al., 2022]. In particular, they
consider SGL for differentiable activation functions, as we do in our derivation of the SG-NTK.
2 Theoretical results
2.1 Notation and NTK parametrization
We consider multilayer perceptrons with so-called neural tangent kernel parametrization. For a
network with depth L, layer width nlfor0≤l≤L, activation function σ, weight matrices
W(l)∈Rnl×nl−1, and biases b(l)∈Rnl, the preactivations with NTK parametrization are given by
h(1)(x) =σw√n0W(1)x+σbb(1),
h(l+1)(x) =σw√nlW(l+1)σ
h(l)(x)
+σbb(l+1)forl= 1, . . . , L −1,
3where σw>0,σb≥0, and we initialize W(l)
ij, b(l)
iiid∼ N(0,1)for all i, j. The NTK parametrization
differs from the standard parametrization by a rescaling factor of 1/√nlin layer l+ 1. The network
function is then given by f(·) =h(L)(·):Rn0→RnLand notably the last layer is a preactivation
layer. We denote the total number of weights by P. More details can be found in Section C.1.
Notation (see Remark C.1 and C.2) By default, we will interpret any vector as a column vector,
i.e., we identify RnwithRn×1. This is the case even when writing x= (x1, . . . , x n)∈Rnfor
handier notation. Row vectors will be indicated within calculations using the transpose operator,
x⊺. For a function f:Rn1→Rn2andX= (x1, . . . , x d)∈Rd·n1, we define the vector f(X):=
(f(x1),···, f(xd))∈Rd·n2.Forn2= 1, we denote the gradient of fby∇f(x)∈Rn1for all
x∈Rn1. Ifn2>1, we denote the Jacobian matrix of fbyJf:Rn1→Rn2×n1. A subscript of the
form Jθf(x)denotes Jacobian matrices with respect to a subset of variables. We write f(n)∝∼g(n)
to denote that f(n)andg(n)are asymptotically proportional, i.e., f(n)∼Kg(n)for some constant
K̸= 0.
2.2 The infinite-width limit
We will consider neural networks in the limit of infinitely many hidden layer neurons, i.e., nl→ ∞
for all 1≤l≤L−1. We will see later, when paraphrasing existing results from the literature,
that different ways of taking the number of hidden neurons to infinity can be found. To formalize
these notions, we use the definition of a width function from Matthews et al. [2018] with slight
modifications:
Definition 2.1 (Width function) .For every layer l= 0, . . . , L and any m∈N, the number of neurons
in that layer is given by rl(m), and we call rl:N→Nthe width function of layer l. We say that a
width function rlis strictly increasing if rl(m)< rl(m+ 1) for all m≥1. We set
RL:=
(rl)L−1
l=1|rlis a strictly increasing width function for all 0< l < L	
,
the set of collections of strictly increasing width functions for network depth L.
Every element of RLprovides a way to take the widths of the hidden layers to infinity by setting
nl=rl(m)for any 1≤l < L and considering m→ ∞ . The notions of infinite-width limits used in
the literature will now correspond to classes R⊆ R Lfor which the respective limiting statements
hold. This is captured in the following definition.
Definition 2.2 (Types of infinite-width limits) .Consider a statement Sof the form “Let an ANN
have depth Land network layer widths defined by n0,nL, and nl:=rl(m)for1≤l < L and some
(rl)L−1
l=1∈ R L. Then, for fixed n0and any nL, the statement Pholds as m→ ∞ .” We also write the
statement SasS(r).
(i)We say that such a statement Sholds strongly, if S(r)holds for any r∈ R L. This can be
interpreted as requiring that the statement holds as min1≤l<L(nl)→ ∞ . We will also write
“Pholds as n1, . . . , n L−1→ ∞ strongly”.
(ii)We say that such a statement Sholds for (nl)1≤l≤L−1∝∼n, ifSholds for all r∈ R Lwith
rl(m)∝∼mfor all 1≤l < L . This means that S(r)holds for all r∈ R Lsuch that
rp(m)/rq(m)→αp,q∈(0,∞)asm→ ∞ . We will also write “ Pholds as (nl)1≤l<L∝∼n”.
(iii) We say that such a statement Sholds weakly, if Sholds for at least one r∈ R L. We will
also write “ Pholds as n1, . . . , n L−1→ ∞ weakly”. This type of infinite-width limit is tightly
connected to the sequential infinite-width limit.
Remark 2.1 (Connection between weak and sequential infinite-width limits) .In the sequential
infinite-width limit, meaning that n1→ ∞ , . . . , n L−1→ ∞ sequentially, the layer widths are not
strictly finite. This is opposed to applications, where layer widths may be large but finite. Hence,
the infinite-width limits using width functions as explained above are more meaningful in practice.
For a sequential limit limn1→∞limn2→∞f(n1, n2) =f∗, we can find functions n1(m)andn2(m)
such that limm→∞f(n1(m), n2(m)) =f∗. However, the rate at which n1(m), n2(m)diverge as
m→ ∞ cannot generally be controlled. Since the weak infinite-width limit allows for arbitrary rates,
any sequential infinite-width limit can hence be turned into a weak infinite-width limit as defined in
Definition 2.2 (iii).
4We use Definition 2.2 to paraphrase the convergence of ANNs to GPs in the infinite-width limit:
Theorem 2.1 (Theorem 4 from Matthews et al. [2018]) .Any network function fof depth Ldefined
as in Section 2.1 with continuous activation function σthat satisfies the linear envelope property,
i.e., there exist c, m≥0with|σ(u)| ≤c+m|u|for all u∈R, converges in distribution as
n1, . . . , n L−1→ ∞ strongly to a multidimensional Gaussian process (Xj)nL
j=1for any fixed count-
able input set (xi)∞
i=1. It holds Xjiid∼ N(0,Σ(L))where the covariance function Σ(L)is recursively
given by
Σ(1)(x, x′) =σ2
w
n0⟨x, x′⟩+σ2
b,Σ(L)(x, x′) =σ2
wEg∼N(0,Σ(L−1))[σ(g(x))σ(g(x′))] + σ2
b.(1)
We also write Σ(L)= Σ σ. The theorem states that the distribution of the network function, which
is given by its randomly initialized weights, approaches the distribution of independent GPs as the
hidden layer widths increase. Hence, a large-width network will approximately be a realization of
the respective GPs. Note that this result can be generalized to non-continuous activation functions
without well-defined derivatives that fulfil the linear envelope property, like σ(z) = sign(z). We
provide a rigorous proof of this kind of generalization for the case n1, . . . , n L−1→ ∞ weakly in
Theorem E.3. Σσremains well-defined in this case since the expectation in Equation (1)does. When
approximating the sign function with scaled error function, i.e., σ(z) = erf m(z):= erf( m·z), it
holds that limm→∞Σerfm= Σ signdue to the dominated convergence theorem.
2.3 Direct extension of the neural tangent kernel in the infinite-width limit
We consider a dataset D= (X,Y)withX= (x1, . . . , x d)∈Rd·n0andY= (y1, . . . , y d)∈Rd·nL.
To solve the regression problem, i.e., to find weights θ∈RPsuch that f(xi;θ) = yifor all
i= 1, . . . , d , we apply gradient descent in continuous time, also know as gradient flow, with learning
rateηand loss function L:Rd·nL×Rd·nL→R. This means that, using the chain rule, the learning
rule can then be written as
d
dtθt=−η∇θL(f(X;θt);Y) =−η Jθf(X;θt)⊺∇f(X;θt)L(f(X;θt);Y). (2)
To derive the NTK, we observe that the network function then evolves according to
d
dtf(x;θt) =Jθf(x;θt)d
dtθt(2)=−η Jθf(x;θt)Jθf(X;θt)⊺∇f(X;θt)L(f(X;θt);Y) (3)
=:−ηˆΘt(x,X)∇f(X;θt)L(f(X;θt);Y),
where we implicitly defined the empirical neural tangent kernel as ˆΘ(x, x′):=Jθf(x;θ)Jθf(x′;θ)⊺,
which depends on the current weights θ=θt. This means that the learning dynamics of the network
functions during training are given by a kernel whose entries are the scalar products between the
gradients of the network’s output neuron activity, ˆΘi j(x, x′) =⟨∇θfi(x;θ),∇θfj(x′;θ)⟩. The key
result on the NTK is that the empirical NTK converges in the infinite-width limit to a constant kernel,
the analytic NTK, at initialization, θ=θ0, and during training, θ=θt:
Theorem 2.2 (Theorem 1 from Jacot et al. [2018] for general σw>0).For any network function
of depth Ldefined as in Section 2.1 with Lipschitz continuous activation function σ,ˆΘ =:ˆΘ(L)
converges in probability to a constant kernel Θ(L)⊗InLasn1, . . . , n L−1→ ∞ weakly. This means
that for all x, x′∈Rn0and1≤i, j≤nL, it holds ˆΘ(L)
i j(x, x′)− →δijΘ(L)(x, x′)in probability,
where δijdenotes the Kronecker delta. We call Θ(L)the analytic neural tangent kernel of the network,
which is recursively given by
Θ(1)(x, x′) = Σ(1)(x, x′),Θ(L)(x, x′) = Σ(L)(x, x′) + Θ(L−1)(x, x′)·˙Σ(L)(x, x′),
where Σ(l)are defined as in Theorem 2.1 and we define
˙Σ(L)(x, x′) =σ2
wEg∼N(0,Σ(L−1))[ ˙σ(g(x)) ˙σ(g(y))]. (4)
We also write Θ(L)= Θ σ. Ifθtare the weights during gradient flow learning at time t≥0as before,
Theorem 2.2 shows that ˆΘ(L)
t→Θ(L)⊗InLfort= 0 in the infinite-width limit. This reveals
5that the gradients of the output neurons, ∇θfi(x;θ), are mutually orthogonal. Under additional
assumptions this convergence also holds for the whole training duration, t >0, see Theorem 2
of Jacot et al. [2018] for the case n1, . . . , n L−1→ ∞ weakly, and Chapter G of Lee et al. [2019]
for(nl)1≤l<L∝∼n. Hence, the kernel that describes the learning dynamics stays constant in the
infinite-width limit. This implies that the distribution of the network function during training also
converges to GPs [Lee et al., 2019, Theorem 2.2]. Then, any output neuron after training has mean
m(x) = Θ(L)(x,X)Θ(L)(X,X)−1Yunder the assumption of a mean squared error (MSE) loss, see
Section C.2.1. The expression for the mean is equivalent to kernel regression with the NTK.
The above results show that gradient flow for networks with randomly initialized weights is character-
ized by the analytic NTK Θ(L)in the infinite-width limit. Since we are interested in gradient flow for
networks with activation functions inadequate for gradient descent training, we want to know whether
the analytic NTK can be extended to such activation functions. We see that the derivative of the
activation function does not have to be defined point-wise for Equation (4). In particular, activation
functions with distributional derivatives, e.g.,d
dzsign(z) = 2 δ(z)can be taken into consideration,
where δdenotes the Dirac delta distribution. If we again approximate the sign function with scaled
error functions erfm,m∈N, it holds
Eg∼N(0,Σerfm)h
˙erfm(g(x))˙erfm(g(y))im→∞− − − − → Eg∼N(0,Σsign)[2δ(g(x))·2δ(g(y))],(5)
in case the right-hand side exists. A rigorous analysis of this limit can be found in Section D.1.
Heuristically, a simple observation suffices: if x=y, we have a one-dimensional integral over two
delta distributions, which yields infinity. On the other hand, if x̸=yandΣsignis non-degenerate,
a two-dimensional integral over two delta distributions yields a finite value. We derive analytic
expressions for limm→∞Θerfm=:Θsignin Lemma D.3. We call this kernel singular, because
Θsign(x, x) =∞andΘsign(x, y)∈Rifx̸=y. By the same reasoning, this divergence occurs for
any activation function with jumps. Note that for the mean given by kernel regression, this implies
m(xi) =yiandm(x) = 0 ifxis not a training point. Intuitively, this is because the network is
initialized with zero mean and different input points are uncorrelated under the singular kernel, so
only the training points are learned. A limit kernel with this property also arises if the activation
function is fixed but the depth of the network goes to infinity as was shown by Radhakrishnan et al.
[2023]. We adopt the ideas of their proof to show that the sign of mis still useful for classification:
Theorem 2.3 (Inspired by Lemma 5 of Radhakrishnan et al. [2023]; see Theorem D.4) .Letσ2
b>0
or let all xi∈Rn0be pairwise non-parallel. Let L≥2andxi∈ Sn0−1
R for all i= 1, . . . , d , where
Sn0−1
R⊆Rn0is the sphere of radius R. Assuming that Θ(L)
∞(x,X)Y ̸= 0for almost all x∈ Sn0−1
R ,
it holds
lim
m→∞sign
Θ(L)
m(x,X)Θ(L)
m(X,X)−1Y
=sign
Θ(L)
∞(x,X)Y
a.e. on Sn0−1
R.
The estimator Θ(L)
∞(x,X)Y=Pn
i=1Θ(L)
∞(x, xi)yihas the form of a so-called Nadaraya-Watson
estimator and is well-defined for singular kernels such as Θ(L). If we assume a classification problem
in the sense that Y ⊆ {− 1,1}n, it holds sign 
Θ(L)
∞(xi,X)Y
= Θ(L)
∞(xi,X)Yat training point xi.
2.4 Generalization of the neural tangent kernel and application to surrogate gradient learning
The above singularity of the limit kernel can be avoided by considering surrogate gradient learning
instead of gradient descent. First, we introduce a generalization of the NTK that later leads to the
surrogate gradient NTK.
By definition and originally due to the chain rule, the empirical NTK consists of two Jacobian
matrices of the network function with respect to the weights. The Jacobian matrix can be thought of
as a recursive formula, Jθf(x;θ) =G(x, θ;σ,˙σ), which is given by the input xand the architecture
of the network, including the activation function σand its derivative ˙σ. This formula can be modified
to define a quasi-Jacobian matrix, Jσ1,˜σ1(x;θ):=G(x, θ;σ1,˜σ1), where ˜σ1does not have to be
the derivative of σ1. Analogous to the definition of the empirical NTK we define the empirical
generalized NTK to be ˆI(x, x′):=Jσ1,˜σ1(x;θ)Jσ2,˜σ2(x′;θ)⊺, where σ1,˜σ1, σ2,˜σ2are specified in
the following theorem.
6Theorem 2.4 (Generalized version of Theorem 1 by Jacot et al. [2018]; see Theorem E.4) .For
activation functions σ1,σ2and so-called surrogate derivatives ˜σ1,˜σ2such that σ1, σ2,˜σ1, and ˜σ2
satisfy the linear envelope property and are continuous except for finitely many jump points, denote
the empirical generalized neural tangent kernel
ˆI(L)(x, x′) =J(L),σ1,˜σ1(x;θ)J(L),σ2,˜σ2(x′;θ)⊺forx, x′∈Rn0,
as before. Then, for any x, x′∈Rn0and1≤i, j≤nL, it holds ˆI(L)
ij(x, x′)P− →δijI(L)(x, x′),as
n1, . . . , n L−1→ ∞ weakly. We call I(L)the analytic generalized neural tangent kernel, which is
recursively given by
I(1)(x, x′) = Σ(1)
1,2(x, x′), I(L)(x, x′) = Σ(L)
1,2(x, x′) +I(L−1)(x, x′)·˜Σ(L)
1,2(x, x′)forL≥2,with
Σ(L)
1,2(x, x′) =σ2
wE[σ1(Z1)σ2(Z2)] +σ2
bforL≥2andΣ1,2(x, x′) =σ2
w
n0⟨x, x′⟩+σ2
b,where
˜Σ(L)
1,2(x, x′) =σ2
wE[˜σ1(Z1) ˜σ2(Z2)]and(Z1, Z2)∼ N
0,
Σ(L−1)
1(x,x) Σ(L−1)
1,2(x,x′)
Σ(L−1)
1,2(x,x′) Σ(L−1)
2(x′,x′)
.
Σ1andΣ2denote the covariance functions of the Gaussian processes that arise from network functions
f1, f2with activation functions σ1, σ2, respectively, in the infinite-width limit. The covariance
between these two Gaussian processes is denoted by Σ1,2. This covariance function is asymmetric in
the sense that Cov[f1(x1), f2(x2)]̸= Cov[ f1(x2), f2(x1)]in general.
We show in Theorem 2.4 that the generalized empirical NTK tends to a generalized analytic NTK at
initialization in an infinite-width limit. Now, the SGL rule can be written using the generalized NTK,
similar to Equation (3):
d
dtf(x;θt) =Jθf(x;θt)d
dtθt=−η Jθf(x;θt)Jσ,˜σ(x;θt)⊺∇f(X;θt)L(f(X;θt);Y) (6)
=:−ηˆIt(x,X)∇f(X;θt)L(f(X;θt);Y) (7)
Here, we chose σ1=σ2=σ,˜σ1= ˙σand˜σ2= ˜σin the previous definition of the generalized
NTK. This we call the surrogate gradient NTK (SG-NTK) with activation function σand surrogate
derivative ˜σ. Compared to the classical NTK, one of the true Jacobian matrices is replaced by the
quasi-Jacobian matrix, since the learning rule is SGL instead of gradient flow. Note that we assume
that the derivative of the activation function, ˙σ, exists. Theorem 2.4 shows convergence at time t= 0.
We extend this convergence to t >0in the following theorem:
Theorem 2.5 (Based on Theorem G.2 from Lee et al. [2019]; see Theorem E.5) .Letσ,˙σ,˜σas
before, all Lipschitz continuous, and ˙σ,˜σbounded. Let ftbe a network function with depth L
initialized as in Section 2.1 and trained with MSE loss and SGL with surrogate derivative ˜σ. Assume
that the generalized NTK converges in probability to the analytic generalized NTK of Theorem
2.4,ˆI(L)→I(L)⊗InL,as(nl)L−1
l=1∝∼n. Furthermore, assume that the smallest and largest
eigenvalue of the symmetrization of I(L)(X,X),S(L):= 
I(L)(X,X) +I(L)(X,X)⊺
/2,are given
by0< λmin≤λmax<∞and that the learning rate is given by η >0. Then, for any δ >0there
existR >0, N∈NandK > 1such that for every n≥N, the following holds with probability at
least1−δover random initialization:
sup
t∈[0,∞)ˆI(L)
t(X,X)−I(L)(X,X)
F≤6K3R
λminn−1
2,where ∥·∥Fdenotes the Frobenius norm.
We also write I(L)=Iσ,˜σor simply Iσ. The explicit analytic expression of the SG-NTK is derived
in Section E.2 for activation function σ= erf m,m∈Nand surrogate derivative ˜σ= erf as well
as general surrogate derivatives. Ierfm,˜σconverges as m→ ∞ , because compared to Equation (5)
we obtain E[˙erfm(g(x)) ˜σ(g(y))]→E[2δ(g(x)) ˜σ(g(y))]and the delta distribution yields a finite
value. In Section E.2, we show this rigorously and derive the analytic expressions. Hence, we define
Isign,˜σ:= lim m→∞Ierfm,˜σ.
For any m∈Nwe can consider the SGL dynamics given by Equation (7)in the infinite-width limit
to obtaind
dtf(x;θt) =−η Ierfm(x,X)∇f(X;θt)L(f(X;θt);Y).
7With MSE error, this equation is solved by a GP with mean m(x) =Ierfm(x,X)Ierfm(X,X)−1Y
fort→ ∞ and it is natural to assume that the networks trained with SGL converge in distribution to
this GP in the infinite-width limit, analogous to the standard NTK case [Lee et al., 2019, Theorem
2.2]. However, the empirical counterpart to Isigndoes not exist as we cannot formulate an empirical
SG-NTK for the sign activation function due to the missing Jacobian matrix, compare Equation
(6). We suggest that even in this case the network trained with SGL will converge in distribution to
the GP given by Isign, since SGL with activation function erfmwill approach SGL with activation
function sign asm→ ∞ and the GP given by Ierfmwill approach the GP given by Isignasm→ ∞ .
Numerical experiments in Section 3 indicate that this is indeed the case. We conclude that, remarkably,
the SG-NTK can be defined for network functions without well-defined gradients and is informative
about their learning dynamics under SGL.
3 Numerical experiments
We numerically illustrate the divergence of the analytic NTK, Θerfm, shown in Section 2.3 and the
convergence of the SG-NTK in the infinite-width limit, ˆI(L)→I(L), at initialization and during
training shown in Section 2.4. Simultaneously, we visualize the convergence of the analytic SG-NTK,
Ierfm→Isign. We consider a regression problem on the unit sphere S1={x∈R2:∥x∥= 1}with
|X|= 15 training points, which is shown in Figure B.1, and train 10 fully connected feedforward
networks with two hidden layers, and activation function erfmfort= 10000 time steps and with
MSE loss. The NTK only depends on the dot product [Radhakrishnan et al., 2023] and thus the angle
between its two arguments, ∆α=∢(x, x′). Hence, we plot the NTKs as functions of this angle,
where ∆α= 0corresponds to x=x′.
In Figure 1, the empirical and analytic NTKs for the networks described above and trained with
gradient descent are plotted for m∈ {2,5,20}and hidden layer widths n∈ {10,100,500,1000}.
In addition, the analytic NTK for m→ ∞ is plotted. Note that the steep slope of erfmform= 20
results in erfmbeing very close to the sign function. For any m, we observe that the empirical NTKs
converge to the analytic NTK both at initialization and after training as NTK theory states. Figure
B.3 illustrates this further by displaying the mean squared errors between the empirical NTKs and
the respective analytic NTK. The convergence slows down for larger m. Further, the plots confirm
that the analytic NTKs diverge as m→ ∞ if and only if ∆α= 0. To show this more clearly, we
scaled the y-axis with the inverse hyperbolic sine (asinh), which is approximately linear for small
absolute values and logarithmic for large absolute values.
102
101
100
100101102m = 2n = 10 n = 100 n = 500 n = 1000
102
101
100
100101102m = 5
0
m 
0
102
101
100
100101102m = 20
0
 0
 0
Analytic NTK
Emp. NTK at t=0
Emp. NTK at t=1e4
Figure 1: We plot empirical and analytic NTKs of 10 networks for different hidden layer widths
nand activation functions erfm. The kernels are plotted at initialization and after gradient descent
training with t= 1e4 time steps, learning rate η= 0.1, and MSE error. The y-axis is asinh-scaled.
For Figure 2, we use the same setup as before, but train the networks using SGL with surrogate
derivative ˜σ=˙erf, and compare the empirical and analytic SG-NTKs instead of NTKs. We observe
that the empirical SG-NTKs converge to the analytic SG-NTK as n→ ∞ both at initialization and
after training in accordance with Theorem 2.4 and Theorem 2.5. Figure B.4 illustrates this further by
displaying the mean squared errors between empirical SG-NTKs and respective analytic SG-NTK.
8Moreover, we observe that the analytic SG-NTKs indeed converge to a finite limit as m→ ∞ , as
shown in Section 2.4.
5
05m = 2n = 10 n = 100 n = 500 n = 1000
5
05m = 5
0
m 
0
5
05m = 20
0
 0
 0
Analytic SG-NTK
Emp. SG-NTK at t=0
Emp. SG-NTK at t=1e4
Figure 2: We plot empirical and analytic SG-NTKs of ten networks for different hidden layer widths
nand activation functions erfm. The kernels are plotted at initialization and after surrogate gradient
learning with t= 1e4 time steps, learning rate η= 0.1, MSE error, and surrogate derivative ˜σ=˙erf.
Finally, we consider SGL for networks with the same architecture and training objective as before,
but with sign activation function, which can be seen as the case m→ ∞ of the setups above. We
examine whether the distribution of network functions trained with SGL agrees with the distribution
of the GP given by the limit kernel Isign. Specifically, we compare 500 networks trained with SGL for
t= 30000 time steps, which represent the distribution of the network function after training, to the
mean and confidence band of the GP. The mean of the GP is given by kernel regression using the
SG-NTK, m(x) =Isign(x,X)Isign(X,X)−1Y, and the confidence band is given by m(x)±2σGP(x),
where σGP(x)is the standard deviation at x. We observe in Figure 3a that the mean of the trained
networks is close to the GP’s mean for network width n= 500 and that most networks lie within the
confidence band. The mean of the networks differs from the kernel regression using the kernel Σsign.
Figure 3b shows that this agreement between SGL and the SG-NTK already exists for a network
width of n= 100 , demonstrating that the SG-NTK predicts the SGL dynamics of networks with
moderate width. Note that the variance in the networks’ output and the confidence band can be
reduced (see Arora et al. [2019] and Section A).
0
2
1
0123
(a)
0
0.5
0.00.51.01.52.0
n=20
n=100
n=500 (b)
Figure 3: Comparison of SGL learning in networks with different hidden layer widths with SG-NTK
predictions. (a)500 networks (blue) with sign activation function and hidden layer widths n= 500
trained with SGL using the surrogate derivative ˜σ=˙erffort= 3e4 time steps plotted together with
their mean (cyan), the SG-NTK-GP’s mean (black) and confidence band (grey), and the Σsignkernel
regression (dashed). Training points are indicated with crosses. (b)The mean of ensembles of 500
networks is plotted as in (a) for different hidden layer widths.
94 Conclusion
Gradient descent training is not applicable to networks with sign activation function. In the present
study, we have first shown that this is even true for the infinite-width limit in the sense that the NTK
diverges to a singular kernel. We found that this singular kernel still has interesting properties and
allows for classification, but it is unusable for regression.
We then studied SGL, which is applicable to networks with sign activation function. We defined
a generalized version of the NTK that can be applied to SGL and derived a novel SG-NTK. We
proved that the convergence of the NTK in the infinite-width limit extends to the SG-NTK, both
at initialization and during training. Strikingly, we were able to derive an SG-NTK for the sign
activation function, Isign, by approximating the sign function with error functions. We suggest that
this SG-NTK predicts the learning dynamics of SGL, and support this claim with heuristic arguments
and numerical simulations.
A limitation of our work is that due to the considered NTK framework, our results are naturally
only applicable to sufficiently wide networks with random initialization. Further, we only prove the
convergence of the SG-NTK during training for activation functions with well-defined derivatives.
More rigorous analysis should be carried out on how the connection between SGL and the SG-NTK
carries over to activation functions with jumps, as shown by our simulations.
Our derivation of the SG-NTK opens a novel path towards addressing the many unanswered questions
regarding the training of binary networks, in particular regarding the class of functions that SGL
learns for wide networks and how that class differs for different activation functions and surrogate
derivatives.
Acknowledgments and Disclosure of Funding
We thank Andreas Eberle for helpful discussions. We thank the German Federal Ministry of Education
and Research (BMBF) for support via the Bernstein Network (Bernstein Award 2014, 01GQ1710).
References
Sina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk. The recurrent neural
tangent kernel. arXiv preprint arXiv:2006.10246 , 2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning , pages 242–252. PMLR,
2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. Advances in Neural Information Processing
Systems , 32, 2019.
Gaurav Arya, Moritz Schauer, Frank Schäfer, and Christopher Rackauckas. Automatic differentiation
of programs with discrete randomness. Advances in Neural Information Processing Systems , 35:
10435–10447, 2022.
Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass. Long
short-term memory and learning-to-learn in networks of spiking neurons. Advances in Neural
Information Processing Systems , 31, 2018.
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. Advances in Neural
Information Processing Systems , 32, 2019.
Patrick Billingsley. Convergence of Probability Measures . John Wiley & Sons, 2nd edition, 1999.
Sander M Bohte. Error-backpropagation in networks of fractionally predictive spiking neurons. In
International Conference on Artificial Neural Networks , pages 60–68. Springer, 2011.
10Blake Bordelon and Cengiz Pehlevan. The influence of learning rule on representation dynamics in
wide neural networks. In The Eleventh International Conference on Learning Representations ,
2022.
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning ,
pages 1024–1034. PMLR, 2020.
Daniele Bracale, Stefano Favaro, Sandra Fortini, and Stefano Peluchetti. Large-width functional
asymptotics for deep Gaussian neural networks. arXiv preprint arXiv:2102.10307 , 2021.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax .
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by
half-wave Gaussian quantization. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 5918–5926, 2017.
Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four
gpu hours: A theoretically inspired perspective. arXiv preprint arXiv:2102.11535 , 2021.
Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. Advances in Neural Informa-
tion Processing Systems , 22, 2009.
Giuseppe Da Prato. An Introduction to Infinite-Dimensional Analysis . Springer Science & Business
Media, 2006.
Luc Devroye, Laszlo Györfi, and Adam Krzy ˙zak. The Hilbert kernel regression estimate. Journal of
Multivariate Analysis , 65(2):209–227, 1998.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International conference on machine learning , pages 1675–
1685. PMLR, 2019.
Jason K Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mo-
hammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks using
lessons from deep learning. arXiv preprint arXiv:2109.12894 , 2021.
Steven K. Esser, Paul A. Merolla, John V . Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy,
Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch,
Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, and Dharmendra S.
Modha. Convolutional networks for fast, energy-efficient neuromorphic computing. Proceedings
of the National Academy of Sciences , 27:201604850, 2016.
Adrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional
networks as shallow Gaussian processes. arXiv preprint arXiv:1808.05587 , 2018.
Gene H. Golub and Charles F. Van Loan. Matrix Computations . John Hopkins University Press,
London, 3rd edition, 1996.
Julia Gygax and Friedemann Zenke. Elucidating the theoretical underpinnings of surrogate gradient
learning in spiking neural networks. arXiv preprint arXiv:2404.14964 , 2024.
Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, and Amin Karbasi. Fast neural
kernel embeddings for general activations. In Advances in Neural Information Processing Systems ,
2022. URL https://github.com/google/neural-tangents .
Herbert Heyer. Structural Aspects in the Theory of Probability , volume 8. World Scientific, 2009.
Geoffrey E Hinton. Coursera video lectures: Neural networks for machine learning. https:
//www.cs.toronto.edu/~hinton/coursera_lectures.html , 2012. Online; accessed 25
July 2023.
11Jørgen Hoffmann-Jørgensen and Gilles Pisier. The law of large numbers and the central limit theorem
in Banach spaces. The Annals of Probability , pages 587–599, 1976.
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and
ntk for deep attention networks. In International Conference on Machine Learning , 2020. URL
https://github.com/google/neural-tangents .
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks. Advances in Neural Information Processing Systems , 29, 2016.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in Neural Information Processing Systems , 31, 2018.
Hyeryung Jang, Osvaldo Simeone, Brian Gardner, and Andre Gruning. An introduction to probabilis-
tic spiking neural networks: Probabilistic models, learning rules, and applications. IEEE Signal
Processing Magazine , 36(6):64–77, 2019.
Olav Kallenberg. Foundations of Modern Probability , volume 3. Springer, 2021.
Annika Lang and Christoph Schwab. Isotropic Gaussian random fields on the sphere: regularity, fast
simulation and stochastic partial differential equations. The Annals of Applied Probability , 25(6):
3047–3094, 2015. ISSN 10505164.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature , 521(7553):436–444,
2015.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes ,
volume 23. Springer Science & Business Media, 1991.
Jaehoon Lee, Jascha Sohl-Dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as Gaussian processes. In International Conference on
Learning Representations , 2018. URL https://openreview.net/forum?id=B1EA-M-0Z .
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in Neural Information Processing Systems , 32, 2019.
Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when
and why the tangent kernel is constant. Advances in Neural Information Processing Systems , 33:
15954–15964, 2020.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271 , 2018.
Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp
Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spiking-
neuron integrated circuit with a scalable communication network and interface. Science , 345
(6197):668–673, 2014.
Radford M. Neal. Bayesian Learning for Neural Networks . Springer New York, NY , 1st edition,
1996.
Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking
neural networks: Bringing the power of gradient-based optimization to spiking neural networks.
IEEE Signal Processing Magazine , 36(6):51–63, 2019.
Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely
wide convolutional networks. Advances in Neural Information Processing Systems , 34:5186–5198,
2021.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python.
InInternational Conference on Learning Representations , 2020. URL https://github.com/
google/neural-tangents .
12Roman Novak, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. Fast finite width neural tangent
kernel. In International Conference on Machine Learning , 2022. URL https://github.com/
google/neural-tangents .
Michael Pfeiffer and Thomas Pfeil. Deep learning with spiking neurons: Opportunities and challenges.
Frontiers in neuroscience , 12:774, 2018. ISSN 1662-4548. doi: 10.3389/fnins.2018.00774.
Constantine Pozrikidis. An Introduction to Grids, Graphs, and Networks . Oxford University Press,
2014.
Yu V Prokhorov. Convergence of random processes and limit theorems in probability theory. Theory
of Probability & Its Applications , 1(2):157–214, 1956.
Yuming Qin. Integral and Discrete Inequalities and Their Applications. Volume I: Linear Inequalities .
Springer, 2016.
Adityanarayanan Radhakrishnan, Mikhail Belkin, and Caroline Uhler. Wide and deep neural networks
achieve consistency for classification. Proceedings of the National Academy of Sciences , 120(14):
e2208779120, 2023.
Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in
the brain. Psychological Review , 65(6):386, 1958.
Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda. Towards spike-based machine intelligence
with neuromorphic computing. Nature , 575(7784):607–617, Nov 2019. ISSN 1476-4687. doi:
10.1038/s41586-019-1677-2. URL https://doi.org/10.1038/s41586-019-1677-2 .
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by
back-propagating errors. Nature , 323(6088):533–536, 1986.
Sumit B Shrestha and Garrick Orchard. Slayer: Spike layer error reassignment in time. Advances in
Neural Information Processing Systems , 31, 2018.
Jascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, and Jaehoon Lee. On the infinite width
limit of neural networks with a standard parameterization, 2020. URL https://github.com/
google/neural-tangents .
Aboozar Taherkhani, Ammar Belatreche, Yuhua Li, Georgina Cosma, Liam P Maguire, and T Martin
McGinnity. A review of learning in biologically plausible spiking neural networks. Neural
Networks , 122:253–272, 2020. doi: 10.1016/j.neunet.2019.09.036.
Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timothée Masquelier, and
Anthony Maida. Deep learning in spiking neural networks. Neural Networks , 111:47–63, 2019.
Aad W. van der Vaart. Asymptotic Statistics . Cambridge Series in Statistical and Probabilistic
Mathematics. Cambridge University Press, 1998. doi: 10.1017/CBO9780511802256.
Christopher Williams. Computing with infinite networks. Advances in Neural Information Processing
Systems , 9, 1996.
David Williams. Probability with Martingales . Cambridge University Press, 1991.
Stanisław Wo´ zniak, Angeliki Pantazi, Thomas Bohnstingl, and Evangelos Eleftheriou. Deep learning
incorporating biologically inspired neural dynamics and in-memory computing. Nature Machine
Intelligence , 2(6):325–336, 2020.
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for
training high-performance spiking neural networks. Frontiers in Neuroscience , 12:331, 2018.
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi. Direct training for spiking neural
networks: Faster, larger, better. Proceedings of the AAAI Conference on Artificial Intelligence , 33
(01):1311–1318, 2019.
13Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760 ,
2019a.
Greg Yang. Wide feedforward or recurrent neural networks of any architecture are Gaussian processes.
Advances in Neural Information Processing Systems , 32, 2019b.
Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint
arXiv:2006.14548 , 2020.
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Under-
standing straight-through estimator in training activation quantized neural nets. arXiv preprint
arXiv:1903.05662 , 2019.
BA Zalesski ˘ı, VV Sazonov, and VV Ul’yanov. A precise estimate of the rate of convergence in the
central limit theorem in Hilbert space. Sbornik: Mathematics , 68(2):453–482, 1991.
Friedemann Zenke and Surya Ganguli. Superspike: Supervised learning in multilayer spiking neural
networks. Neural Computation , 30(6):1514–1541, 2018.
Friedemann Zenke and Tim P V ogels. The remarkable robustness of surrogate gradient learning for
instilling complex function in spiking neural networks. Neural Computation , 33(4):899–925, 2021.
14A Additional remarks on the numerical experiments
Weight initialization and implementation. All networks are initialized with σw= 1, σb= 0.1
For the implementation of the NTK and SG-NTK we use the JAX package [Bradbury et al., 2018]
and Neural Tangents package [Novak et al., 2020, 2022, Han et al., 2022, Sohl-Dickstein et al., 2020,
Hron et al., 2020] with modifications. Computations were done using an Intel Core i7-1355U CPU
and 16 GB RAM. The simulations for Figure 1 and Figure 2 took two hours each. The simulations
for Figure 3 took 12 hours.
Variance reduction trick. The variance in the outputs of the networks trained with gradient
descent or SGL and the confidence band given the NTK or SG-NTK respectively can be reduced by
multiplying the weights of the last layer with a constant κ <1at initialization [Arora et al., 2019].
This is explained in detail at the end of Section C.2.1. We can see from Figure B.2 that the agreement
between the distribution of the trained networks and the distribution given by the SG-NTK still holds;
however, network width and training time have to be increased.
B Additional figures
0
1.0
0.5
0.00.51.01.52.02.5
Figure B.1: Target function and training points for the numerical experiments, f(x, y) = 4 xy2−
0.8x3+ 1.2y2−0.8x2y.
0
2
1
0123
Figure B.2: Network functions of 100 networks with sign activation function and hidden layer widths
n= 500 trained with SGL using the surrogate derivative ˜σ=˙erffort= 2e4 time steps plotted
together with the SG-NTK-GP’s mean and confidence band. The parameters of the last layer have
been multiplied with κ= 0.2at initialization.
15101102103102
100102104
m = 2
101102103
m = 5
101102103
m = 20
hidden layer widths nMSE at t=0
MSE at t=1e4Figure B.3: Mean squared errors between empirical NTKs and analytic NTKs in Figure 1 for all 10
networks (thin lines) and averaged over the 10 networks (thick lines).
101102103103
102
101
100101
m = 2
101102103
m = 5
101102103
m = 20
hidden layer widths nMSE at t=0
MSE at t=1e4
Figure B.4: Mean squared errors between empirical SG-NTKs and analytic SG-NTKs in Figure 2 for
all 10 networks (thin lines) and averaged over the 10 networks (thick lines).
C Neural tangent kernel theory
C.1 Introduction of the neural tangent kernel
We begin by defining an artificial neural network with a parameterization suitable for considering
the limit of infinitely many hidden neurons. This parameterization is called neural tangent kernel
parameterization and differs from standard parameterization of multilayer perceptrons by a rescaling
factor of 1/√nlin layer l+ 1, where nlis the width of layer l. We follow the slightly more general
definition in [Lee et al., 2019, Equation (1)]. A discussion of this kind of parameterization can be
found in [Jacot et al., 2018, Remark 1].
Definition C.1 (Artificial neural network with NTK parameterization) .LetLbe the depth of the
network, nkfork= 0, . . . , L the widths of the layers, and σ:R→Ran activation function. We draw
network weight matrices W(l)∈Rnl×nl−1and biases b(l)∈Rnlforl= 1, . . . , L from a probability
distribution such that W(l)
ij, b(l)
iiid∼ N(0,1). For the parameters, we denote by θ(l)= (W(l), b(l))
the parameters of layer land by θ(1:l)= (θ(1), θ(2), . . . , θ(l))the parameters of layers 1up to and
including l. Given some σw>0andσb≥0we then define for all x∈Rn0
h(1)
x;θ(1)
=σw√n0W(1)x+σbb(1),
h(l+1)
x;θ(1:l+1)
=σw√nlW(l+1)σ
h(l)
x;θ(1:l)
+σbb(l+1)forl= 1, . . . , L −1.
16Therefore, h(l)(·;θ(1:l))is a map from Rn0toRnland we use the short-hand notation h(l)(x;θ) =
h(l)(x;θ(1:l)). Finally, we define our network function
f(·;θ) =h(L)(·;θ):Rn0→RnL.
With this definition, the total number of parameters, P=|θ|, is
P=LX
l=1θ(l)=LX
l=1nl(nl−1+ 1).
Given such a network function with NTK parameterization, we consider a dataset D= (X,Y)
withX= (x1, . . . , x d)∈Rd·n0andY= (y1, . . . , y d)∈Rd·nL. First, we want to solve the
regression problem, i.e., find parameters θ′such that f(xi;θ′) =yifor all i= 1, . . . , d . Later,
we will also consider the classification problem, i.e., we assume yi∈ {− 1,1}and want to solve
sign(f(xi;θ′)) = yifor all i= 1, . . . , d . Tackling both cases from the regression perspective, we
define the so-called loss functional and loss function. The following two definitions are similarly
formulated by Jacot et al. [2018].
Definition C.2 (Loss functional and loss function) .LetF={g|g:Rn0→RnL}andL:F →Ra
so-called loss functional. In addition, let Lbe convex, i.e., for all λ∈[0,1]andg1, g2∈ F it holds
L(λg1+ (1−λ)g2)≤λL(g1) + (1 −λ)L(g2).
We will assume that Lcan be written as
L(f) =1
ddX
i=1ℓ(f(xi;θ);yi) =:L(f(X);Y),
so that the so-called loss function L(·;Y):Rd·nL→Ris differentiable.
Remark C.1 (Function evaluation at sets and vector notation) .We want to detail the notation used in
Definition C.2. For a function f:Rn0→RnLandX= (x1, . . . , x d)∈Rd·n0we define the vector
f(X):= (f(x1),···, f(xd))∈Rd·nL.
By default, we will interpret any vector as a column vector, i.e., we identify RnwithRn×1. This is the
case even when writing x= (x1, . . . , x n)∈Rnfor handier notation. Row vectors will be indicated
within calculations using the transpose operator, x⊺.
Let us first consider regular gradient descent learning in continuous time, also known as gradient flow.
For this, we assume that our network function is differentiable with respect to its parameters.
Remark C.2 (Gradient and Jacobian matrix notation) .Letf:Rn→Rm. For m= 1, we denote
the gradient by ∇f(x)∈Rnfor all x∈Rn. Ifm > 1we denote the Jacobian matrix of fby
Jf:Rn→Rm×n. Therefore, Jf(x)is am×nmatrix for all x∈Rn. We do not always want
to consider the gradient or Jacobian matrix with respect to all variables. We indicate this with
subscripts as follows. Let f:Rn×RP→Rmandgx(θ) =f(x;θ)for fixed x∈Rn. Then, we write
∇θf(x;θ):=∇gx(θ)andJθf(x;θ):=Jgx(θ). In particular, for a map f:RP→R, the gradient
with respect to the j-th variable, 1≤j≤P, is a scalar and denoted by ∂jf(θ):=∇θjf(θ). This is
called the partial derivative of fwith respect to the j-th variable.
With this notation, we consider the gradient flow method with learning rate η >0and recall the
derivation of the neural tangent kernel. We move the weights in the opposite direction of the gradient
of the loss function with respect to the parameters of the network evaluated at the training points:
d
dtθt=−η∇θL(f(X;θt);Y)) =−η Jθf(X;θt)⊺∇f(X;θt)L(f(X;θt);Y) (S8)
=−η1
ddX
i=1Jθf(xi;θt)⊺∇f(xi;θt)ℓ(f(xi;θt);yi),
17using the chain rule for the second equality and with A⊺denoting the transpose of a matrix A. Again
using the chain rule, this then implies for any x∈Rn0
d
dtf(x;θt) =Jθf(x;θt)d
dtθt(S8)=−η Jθf(x;θt)Jθf(X;θt)⊺∇f(X;θt)L(f(X;θt);Y) (S9)
=−η1
ddX
i=1Jθf(x;θt)Jθf(xi;θt)⊺∇f(xi;θt)ℓ(f(xi;θt);yi). (S10)
We therefore define the neural tangent kernel as follows:
Definition C.3 (Neural tangent kernel) .Let f be a network function of depth Las in Definition C.1
with parameters θ, not necessarily drawn randomly. Then, we define the neural tangent kernel (NTK)
as:
ˆΘ(L):Rn0×Rn0→RnL×nL
(x, y)7→Jθf(x;θ)Jθf(y;θ)⊺.
Therefore, it holds for all x, y∈Rn0and1≤i, j≤nL
ˆΘ(L)
i j(x, y) =PX
p=1∂θpfi(x;θ)∂θpfj(y;θ).
Notice that the NTK depends on the parameters of the networks. It is therefore initialized randomly
and varies over the course of the training. With notation ft(x) =f(x;θt)andˆΘ(L)
t=ˆΘ(L)for
parameters θtat training time twe can now rewrite Equations (S9) and (S10) as follows:
d
dtft(x) =−ηˆΘ(L)
t(x,X)∇ft(X)L(ft(X);Y) (S11)
=−η1
ddX
i=1ˆΘ(L)
t(x, xi)∇ft(xi)ℓ(ft(xi);yi).
We are hence able to express the change of the network function during training in a kernel fash-
ion. Later, we will consider this change of the network function in the infinite-width limit, i.e.,
n1, . . . , n L−1→ ∞ .
Before doing so, we will generalize the NTK definition in order to apply the NTK to surrogate gradient
learning later. In particular, we will break the symmetry of the above definition and generalize the
Jacobian matrices to quasi-Jacobian matrices by replacing the derivatives of the activation function
by surrogate derivatives. Let us write the recursive formula of the Jacobian matrix of the network
function given by the chain rule as Jθf(x;θ) =G(σ; ˙σ;x;θ), where ˙σis the derivative of the
activation function σ. Then, surrogate gradient learning replaces Jθf(x;θ)withG(σ; ˜σ;x;θ)for the
surrogate derivative ˜σof the activation function σ. We call this the quasi-Jacobian matrix:
Definition C.4 (Quasi-Jacobian matrices for neural networks) .LetLbe the depth of the network,
nkfork= 0, . . . , L the width of the layers, σ:R→Rthe activation function, and ˜σ:R→R
the so-called surrogate derivative of the activation function. Let fbe the network function, h(l),
l= 1, . . . , L −1, the intermediate layers as in Definition C.1 and θthe network parameters. We then
define the quasi-Jacobian matrix J(L)offat point xrecursively as follows:
J(1)
x;θ(1)
∈Rn1×|θ(1)|with J(1)
k θp
x;θ(1)
=(
δkiσw√n0xjifθp=W(1)
ij
δkiσb ifθp=b(1)
i(S12)
J(l)
x;θ(1:l)
∈Rnl×|θ(1:l)|for2≤l≤Lwith
J(l)
k θp
x;θ(1:l)
=

δkiσw√nl−1σ
h(l−1)
j(x;θ)
ifθp=W(l)
ij
δkiσb ifθp=b(l)
i
σw√nl−1Pnl−1
j=1W(l)
ij˜σ
h(l−1)
j(x;θ)
J(l−1)
j θp 
x;θ(l−1)
ifθp∈θ(l−1).
(S13)
18Remark C.3 (Notations for the quasi-Jacobian) .With the above definition of the quasi-Jacobian
matrix of the network function fwith activation function σand surrogate derivative ˜σwe write
J(L),σ,˜σ(x;θ):=J(L)
x;θ(L)
.
It then holds
J(L),σ,˙σ(x;θ) =Jθf(x;θ).
For a data set Xinstead of a single point x, we concatenate the matrices row-wise as before, namely
J(L)(X;θ)∈Rd·nL×|θ|.
Definition C.5 (The generalized neural tangent kernel) .Letσ1, σ2be activation functions and ˜σ1,˜σ2
the surrogate derivatives respectively. Given a network depth Land parameters θwe define the
generalized neural tangent kernel as:
ˆI(L):Rn0×Rn0→RnL×nL(S14)
(x, y)7→J(L),σ1,˜σ1(x;θ)J(L),σ2,˜σ2(y;θ)⊺.
Remark C.4. The generalized neural tangent kernel agrees with the neural tangent kernel in the
case where σ=σ1=σ2and˙σ= ˜σ1= ˜σ2.
C.2 Notation for the infinite-width limit and review of key theorems for the NTK
In this section we will formulate all important theorems on the NTK that we will need for our later
analysis using the introduced notation. Furthermore, we will discuss and remark their proofs, in
particular in view of the generalizations that will be proved in Section E.1.
Convergence of networks to Gaussian processes in the infinite-width limit. We will consider
neural networks in the limit of infinitely many hidden layer neurons. The fact that such networks
converge to Gaussian processes was first mentioned by Neal [1996]. We follow and present the
formulations of Matthews et al. [2018] for the general mathematical statement. First, we formalize
the limit of infinitely many hidden neurons.
Definition C.6 (Width function, as in Definition 3 of Matthews et al. [2018] with modifications) .For
every layer l= 0, . . . , L and any m∈N, the number of neurons at that layer is given by rl(m), and
we call rl:N→Nthe width function of layer l. We say that a width function rlis strictly increasing
ifrl(m)< rl(m+ 1) for all m≥1. We set
RL:=
(rl)L−1
l=1|rlis a strictly increasing width function for all 1≤l < L	
,
the set of collections of strictly increasing width functions for network depth L.
Every element of RLprovides a way to take the widths of the hidden layers to infinity by setting
n0andnLto some constant, setting nl=rl(m)for any 1≤l < L and considering m→ ∞ . To
formally define for which ways of taking the widths of hidden layers to infinity a statement holds,
we can now state the set R⊆ R Lsuch that the statement holds for widths given by any r∈Ras
m→ ∞ . Clearly, the claim “The statement holds for all r∈R1.” is stronger than “The statement
holds for all r∈R2.” ifR2⊆R1. On the basis of these considerations, we define three types of
infinite-width limits using the previous definition in order to structure the different types of limits in
this thesis as well as in the literature.
Definition C.7 (Types of infinite-width limits) .Consider a statement Sof the form “Let an ANN
have depth Land network layer widths defined by n0, nLandnl:=rl(m)for1≤l < L and some
(rl)L−1
l=1∈ R L. Then, for fixed n0and any nL, the statement Pholds as m→ ∞ .” We also write the
statement SasS(r).
(i)We say that such a statement Sholds strongly, if S(r)holds for any r∈ R L. This can be
interpreted as requiring that the statement holds as min1≤l<L(nl)→ ∞ . We will also write
“Pholds as n1, . . . , n L−1→ ∞ strongly”.
(ii)We say that such a statement Sholds for (nl)1≤l≤L−1∝∼n, ifSholds for all r∈ R L
withrl(m)∝∼mfor all 1≤l < L . In other words S(r)holds for all r∈ R Lsuch that
rp(m)/rq(m)→αp,q∈(0,∞)asm→ ∞ . We will also write “ Pholds as (nl)1≤l<L∝∼n”.
19(iii) We say that such a statement Sholds weakly, if Sholds for at least one r∈ R L. This can
be read as requiring that the statement holds as n1→ ∞ , . . . , n L−1→ ∞ sequentially. We
will also write “ Pholds as n1, . . . , n L−1→ ∞ weakly”.
Theorem C.1 (Theorem 4 from Matthews et al. [2018]) .Any network function fof depth Ldefined
as in Definition C.1 with continuous activation function σthat satisfies the linear envelope property,
i.e., there exist c, m≥0with
|σ(u)| ≤c+m|u| ∀u∈R,
converges in distribution as n1, . . . , n L−1→ ∞ strongly to a multidimensional Gaussian process
(Xj)nL
j=1for any fixed countable input set (xi)∞
i=1. It holds Xjiid∼ N(0,Σ(L))where the covariance
function Σ(L)is recursively given by
Σ(1)(x, x′) =σ2
w
n0⟨x, x′⟩+σ2
b,
Σ(L)(x, x′) =σ2
wEg∼N(0,Σ(L−1))[σ(g(x))σ(g(x′))] + σ2
b.
Remark C.5. First, a proof of the above theorem can be found in the paper of Matthews et al. [2018].
While it takes a lot of effort to show that the statement holds strongly in the sense of Definition C.7,
the weak version of the statement can be proved via induction. This has been done by Jacot et al.
[2018] and we will later adapt their proof to show a generalized version, Theorem E.3.
Second, in the context of analyzing the network behavior, we are interested in the finite-dimensional
distributions first of all, since neural networks are trained and tested on a finite number of data points.
From the convergence of the marginal distributions, we can infer the convergence to an stochastic
process via the Kolmogorov extension theorem. However, this assumes the product σ-algebra, which
is why Theorem C.1 assumes a fixed countable input set. Matthews et al. [2018] have discussed
these formal restrictions in more detail (Chapter 2.2). If one does not want to be restricted to a
countable index set, one could, for example, consider the condition by Prokhorov [1956, Theorem
2.1]. A similar approach was taken by Bracale et al. [2021], which applied the Kolmogorov-Chentsov
criterion [Kallenberg, 2021, Theorem 4.23].
Finally, note that the theorem assumes continuity of the activation function. In the proof of Matthews
et al. [2018] this is only used in order to apply the continuous mapping theorem. However, it is
sufficient for the limiting process to attain possible points of discontinuity with probability zero for the
continuous mapping theorem to be applicable. The theorem is thus also valid for activation functions
that are continuous except at finitely many jump points, such as step-like activation functions.
Convergence of the NTK at initialization in the infinite-width limit. Jacot et al. [2018] showed
that the previously defined empirical NTK converges to a deterministic limit, which we will call the
analytic NTK.
Theorem C.2 (Theorem 1 from Jacot et al. [2018], slightly generalized) .For any network func-
tion of depth Ldefined as in Definition C.1 with Lipschitz continuous activation function σ, the
empirical neural tangent kernel ˆΘ(L)converges in probability to a constant kernel Θ(L)⊗InLas
n1, . . . , n L−1→ ∞ weakly. For all x, x′∈Rn0and1≤i, j≤nL, it holds
ˆΘ(L)
i j(x, x′)P
− − →δijΘ(L)(x, x′),
which we also write as
ˆΘ(L)P
− − →Θ(L)⊗InL.
We call Θ(L)the analytic neural tangent kernel of the network, which is recursively given by
Θ(1)(x, x′) = Σ(1)(x, x′)
Θ(L)(x, x′) = Σ(L)(x, x′) + Θ(L−1)(x, x′)·˙Σ(L)(x, x′),
where Σ(l)are defined as in Theorem C.1 and we define
˙Σ(L)(x, x′) =σ2
wEg∼N(0,Σ(L−1))[ ˙σ(g(x)) ˙σ(g(x′))].
Compared to Theorem 1 of Jacot et al. [2018], the statement is slightly generalized in the sense that it
allows for arbitrary σw>0. The arguments in the proof work the same way.
20Remark C.6 (Versions of Theorem C.2 in the literature) .A proof of this theorem for (nl)1≤l<L∝∼n
is given by Yang [2019a] and his proof is also referenced by Lee et al. [2019]. However, the proof is
given in terms of so-called tensor programs and therefore harder to follow. For the ReLU activation
function, a proof for n1, . . . , n L−1→ ∞ strongly is provided by Arora et al. [2019, Theorem 3.1].
We will later prove a version of this theorem for the generalized NTK.
Convergence of the NTK during training in the infinite-width limit. Not only does the NTK
converge to a constant kernel in the infinite-width limit, even the kernel during training, ˆΘ(L)
t,
converges to this constant kernel. This was also discovered by Jacot et al. [2018].
Theorem C.3 (Theorem 2 by Jacot et al. [2018]) .Assume any network function of depth Ldefined as
in Definition C.1 with Lipschitz continuous activation function σ, twice differentiable with bounded
second derivative, and trained with gradient flow as in Equation S10. Let T >0such that
ZT
0∥∇ℓ(ft(·);f∗(·))∥pempdt=ZT
0√
d∇ft(X)L(ft(X);f∗(X))
2dt∈ Op(1), (*)
where X∈ Op(1)denotes that Xis stochastically bounded. Then, as n1, . . . , n L−1→ ∞ weakly,
the empirical NTK ˆΘ(L)
tconverges in probability to the analytic NTK Θ(L)⊗InLin probability
uniformly for t∈[0, T]. We therefore write
ˆΘ(L)
tP
− − →Θ(L)⊗InL.
Remark C.7 (Versions of Theorem C.3 in the literature) .The proof of Jacot et al. [2018] relies
heavily on a function space perspective. Since this formulation tends to lack mathematical rigor, we
will rely on the proof of the theorem for the case (nl)1≤l<L∝∼ngiven by Lee et al. [2019, Chapter
G]. In particular, the first inequality of (S51) in Theorem G.2 of Lee et al. [2019] implies the condition
(*). Furthermore, a different approach to proving the above statement for the case (nl)1≤l<L∝∼n
using the Hessian matrix of the network function was taken by Liu et al. [2020, Proposition 2.3,
Theorem 3.2]. A partial proof of a version of this theorem for the generalized NTK will be given later.
Only an auxiliary lemma remains to be proved.
C.2.1 Gradient flow in the infinite-width limit
Given the results of the previous section, we can formulate an infinite width version of Equation
(S10) by replacing the empirical with the analytic NTK. This allows us to analyze the learning
dynamics of networks in the infinite-width limit, which yields connections to kernel methods and
reproducing kernel Hilbert spaces. We then discuss how far the resulting functions and solutions in
the infinite-width limit deviate from the finite width networks. This is essential to evaluate to what
extend the results in the infinite-width limit can inform us about the behavior of gradient flow in the
finite-width networks. First, we state the infinite-width version of Equation (S11) using Theorem C.3,
d
dtft(x) =−η
Θ(L)⊗InL
(x,X)∇ft(X)L(ft(X);Y)
=−η1
ddX
i=1Θ(L)(x, xi)·InL∇ℓ(ft(xi);yi)
=−ηdX
i=1Θ(L)(x, xi)1
d∇ℓ(ft(xi);yi) (S15)
=−ηΘ(L)(x,X)1
d[∇ℓ(ft(x1);y1), . . . ,∇ℓ(ft(xd);yd)]⊺
=:−ηΘ(L)(x,X)∇ft(X)L(ft(X);Y), (S16)
where in the last line we interpret ∇ft(X)L(ft(X);Y)∈Rd×nLas a matrix of size d×nLwith
entries
∇Lft(X)(ft(X);Y)
i j= 1/d·∂jℓ(ft(xi);yi). Recall that ∂jℓ(ft(xi);yi)is the partial
derivative of ℓ(·;yi)with respect to its j-th entry, i.e., with respect to ft,j(xi). Note that the last
line is a row vector, which we can identify as a column vector. The fact that the NTK is now
time-independent and non-random has two interesting implications:
21•Equation (S16) is now an differential equation that can be solved explicitly or numerically
for certain loss functions.
•According to Equation (S15) , the time derivative of ft(x)can now be expressed element-
wise as a linear combination of functions of the type Θ(L)(·,˜x):Rn0→R. For an arbitrary
symmetric and positive definite kernel k(·,·), the completion of the linear span of functions
of this type is called the reproducing kernel Hilbert space (RKHS) of k. Assuming that the
solution of Equation (S15) is an element of the RKHS of Θ(L), one can ask what the space
looks like.
The ODE of Equation (S16) has already been considered by Jacot et al. [2018, Chapter 5] and Lee
et al. [2019, Chapter 2.2], and we will follow the observations made there. To do this, we will assume
the mean squared error (MSE) loss,
L(˜Y;Y) =1
2∥˜Y − Y∥2
2,
implying ∇ft(X)L(ft(X);Y) =ft(X)− Y, where ft(X)andYare again interpreted as matrices of
dimension d×nL. This gives us the following ODE
d
dtft(x) =−ηΘ(L)(x,X) (ft(X)− Y).
Now, for simplicity, we denote Θ(x, y):= Θ(L)(x, y)andΘ:= Θ(X,X). Furthermore, we consider
an arbitrary set of test points XT. The solution of the ODE is then given by
ft(XT) =µt(XT) +γt(XT)for
µt(XT) = Θ( XT,X)Θ−1 
Id−e−ηΘt
Yand
γt(XT) =f0(XT)−Θ(XT,X)Θ−1 
Id−e−ηΘt
f0(X).
Recall that by Theorem C.1, the components f0,jare independent and identically distributed Gaussian
processes with mean zero and covariance function Σ:= Σ(L). Hence, γthas mean zero and the mean
offtis given by µt. By looking at the components of ft,
ft,j(XT) =f0,j(XT)−Θ(XT,X)Θ−1 
Id−e−ηΘt
(f0,j(XT)− Yj),
we can conclude that they are independent and identically distributed as well. One can show that the
components are indeed Gaussian processes again with mean µt. Using γtwe can also compute the
covariance matrix for our arbitrary set of test points XT,
Γt(XT,XT):=E[γt,j(XT)γt,j(XT)⊺] =E[f0,j(XT)f0,j(XT)⊺]
−E
f0,j(XT)f0,j(X)⊺ 
Id−e−ηΘt
Θ−1Θ(X,XT)
−E
Θ(XT,X)Θ−1 
Id−e−ηΘt
f0,j(X)f0,j(XT)⊺
+E
Θ(XT,X)Θ−1 
Id−e−ηΘt
f0,j(X)f0,j(X)⊺ 
Id−e−ηΘt
Θ−1Θ(X,XT)
= Σ(XT,XT)−Σ(XT,X) 
Id−e−ηΘt
Θ−1Θ(X,XT)
−Θ(XT,X)Θ−1 
Id−e−ηΘt
Σ(X,XT)
+ Θ(XT,X)Θ−1 
Id−e−ηΘt
Σ(X,X) 
Id−e−ηΘt
Θ−1Θ(X,XT).
Assuming that Θis positive definite immediately leads to pointwise convergence of the mean and
covariance functions. This implies that the gradient flow solution for networks in the infinite-width
limit converges to a Gaussian process as t→ ∞ with mean function µ∞and covariance function Γ∞
given below. This follows from the weak convergence of the finite-dimensional marginal distributions
by Lévy’s convergence theorem [Williams, 1991, Section 18.1]. Again, the discussions of Remark
C.5 applies. We have
µ∞(XT) = Θ( XT,X)Θ−1Yand (S17)
Γ∞(XT,XT) = Σ( XT,XT)−Σ(XT,X)Θ−1Θ(X,XT)
−Θ(XT,X)Θ−1Σ(X,XT) + Θ( XT,X)Θ−1Σ(X,X)Θ−1Θ(X,XT). (S18)
Lee et al. [2019] state that a network trained with gradient flow will indeed converge in distribution
to this Gaussian process as the width goes to infinity:
22Theorem C.4 (Theorem 2.2 from Lee et al. [2019]) .Let the learning rate η < η critical for
ηcritical := 2(λmin(Θ(L)(X,X)) +λmax(Θ(L)(X,X)))
with a network function ftas in Theorem C.3 with hidden layer widths n1=···=nL−1=nand
restricted to x∈Rn0with∥x∥2≤1. Ifλmin(Θ(L)(X,X))>0, then the components of ftconverge
in distribution to independent, identically distributed Gaussian processes N(µt,Γt)asn→ ∞ for
allt∈[0,∞)∪ {∞} .
Hence, the result of training a finite-width network with gradient flow for an infinite amount of time
will be arbitrarily close in distribution to a Gaussian process with mean function µ∞and covariance
function Γ∞, if the width is sufficiently large. Note that by Equations (S17) and (S18) the variance at
the training points Xis zero and the mean at the training points is exactly Y.
Since we will focus on the mean, we will first sketch a trick introduced in Chapter 3 of Arora et al.
[2019] to make the variance term arbitrarily small. If f0had mean variance, this would consequently
also be the case for all ftand for the solution f∞:= lim t→∞ft. This can be achieved by multiplying
f0by a small constant κ >0and considering the network function g0=κf0instead. It then holds
ˆΘg(x, y) =Jθg0(x;θ)Jθg0(y;θ)⊺=Jθκf0(x;θ)Jθκf0(y;θ)⊺=κ2ˆΘf(x, y),
and thus we have Θg=κ2Θf. In the infinite-width limit, the derivative of gtis then given by
d
dtgt(x) =−ηΘg(x,X) (gt(X)− Y)
=−η κ2Θf(x,X) (κft(X)− Y),
which implies as before
gt(x) =g0(x)−Θg(x,X)Θg(X,X)−1
Id−e−ηΘg(X,X)t
(g0(X)− Y)
= Θ f(x,X)Θf(X,X)−1
Id−e−ηκ2Θf(X,X)t
Y
+κ
f0(x)−Θf(x,X)Θf(X,X)−1
Id−e−ηκ2Θf(X,X)t
f0(X)
.
Note that the term in the second last line corresponds to the non-random mean of ftrained with
learning rate ηκ2, and that the term in the last line is random, but can be made arbitrarily small using
κ. We can think of this as a trade-off between learning rate and variance. This justifies why we can
focus on the mean in the next section.
To sum up, we are interested in network functions in the infinite-width limit that are trained over time
according to
d
dtft(x) =−ηΘ(x,X) (ft(X)− Y) =dX
i=1Θ(x, xi) (−η(ft(xi)−yi)), (S19)
where we change from a row vector to a column vector in the last equation. The mean of such network
functions after infinite training time is given by
fNTK(x):= Θ( x,X)Θ(X,X)−1Y=µ∞(x). (S20)
D The NTK for sign activation function
The first observation to make in our attempt to apply the neural tangent kernel to networks with the
sign function as activation function is that the sign function has a zero derivative almost everywhere.
Thus, the derivative of the network function with respect to the network weights is zero for all weights
that are not part of the last layer. The case where the weights θ(1:L−1)are frozen after initialization
and only θ(L)is trained has already been discussed by Lee et al. [2019, Chapter 2.3.1 and Chapter D].
For a network in the infinite-width limit, this approach is equivalent to applying Gaussian process
regression , i.e., knowing that f∼ N 
0,Σ(L)
for infinite width, one considers f|f(X) =Y. This
can be seen by realizing that Θ(L)= Σ(L)if˙σ= 0almost everywhere and applying Theorem C.4.
23While this is an interesting observation, and the strategy of optimizing only the last layer can also
be transferred to finite width networks, we would prefer to train the whole network and not identify
the derivative of the sign function with zero, since this discards all information about the jump
discontinuities in our networks. An obvious alternative would be to use the distributional derivative
of the sign function, which is given by 2δ0, where δ0denotes the delta distribution. We will see that
˙Σ(L)still exists when the distributional derivative is substituted into its formula. Alternatively, we
can obtain the same expression by approximating the sign function with scaled error functions,
erfm(z) = erf( m·z) =2√πZm·z
0e−t2dt,
and considering the limit m→ ∞ .
D.1 The NTK for error activation function
Due to the previous considerations, we begin by deriving the analytic NTK for the error function.
Following the notation of Lee et al. [2019], we need to find analytic expressions for the terms
Tm(Σ):=E(X,Y)∼N(0,Σ)[erfm(X) erf m(Y)]and
˙Tm(Σ):=E(X,Y)∼N(0,Σ)[˙erfm(X)˙erfm(Y)].
Note that by a change of variables we can alternatively consider the terms
T(m2·Σ):=E(X,Y)∼N(0,m2·Σ)[erf(X) erf(Y)] =Tm(Σ) and
˙T(m2·Σ):=E(X,Y)∼N(0,m2·Σ)[˙erf(X)˙erf(Y)] =1
m2˙Tm(Σ).
ForΣ′= (x·x x·y
x·y y·y),T(Σ′)and˙T(Σ′)are given in Chapter C of the supplementary material of Lee
et al. [2019]. However, we cannot assume that Σ′always has this form. While ˙Tcan be easily
calculated, Tis harder to deal with and a reference to Williams [1996, Chapter 3.1] is used. There,
the main idea of the proof, how to evaluate a more general expression, is given without further details.
We will derive analytic expressions for both terms explicitly.
We start by evaluating ˙T. Note that
d
dzerf(z) =2√πe−z2andd
dzerfm(z) =2m√πe−m2z2.
Lemma D.1. Given U∼ N(0,Σ)with invertible covariance matrix Σ∈Rd×dandx, y∈Rd, it
holds
E[˙erf(U⊺x)˙erf(U⊺y)] =4
π 
(1 + 2 x⊺Σx)(1 + 2 y⊺Σy)−(2x⊺Σy)2−1/2. (S21)
In particular, given (X, Y)∼ N(0,Σ)with invertible covariance matrix Σ∈R2×2or with X=Y
and singular covariance matrix Σ∈R2×2, it holds
˙T(Σ) = E[˙erf(X)˙erf(Y)] =4
π|I2+ 2·Σ|−1/2, (S22)
where |A|denotes the determinant of a matrix A.
Proof. It holds for U∼ N(0,Σ)with covariance matrix Σ∈Rd×dandx, y∈Rd:
E[˙erf(U⊺x)˙erf(U⊺y)] =Z
Rd1
(2π)d/2|Σ|1/22√πe−(u⊺x)22√πe−(u⊺y)2
e−1
2u⊺Σ−1udu
(⋆)=4
πZ
Rd1
(2π)d/2exp
−1
2v⊺(Id+ 2Σ1/2xx⊺Σ1/2+ 2Σ1/2yy⊺Σ1/2)v
dv
=4
π
Id+ 2Σ1/2xx⊺Σ1/2+ 2Σ1/2yy⊺Σ1/2−11/2
=4
πId+ 2Σ1/2xx⊺Σ1/2+ 2Σ1/2yy⊺Σ1/2−1/2
,
24using a change of variable Σ1/2u=vfor Equation (⋆)and using basic properties of the determinant.
We can evaluate the determinant in the last line by applying the Sylvester’s determinant theorem
[Pozrikidis, 2014, (B.1.16)], i.e., |In+AB⊺|=|Im+B⊺A|for any matrices A, B∈Rn×m. We
then define A=B= (√
2Σ1/2x,√
2Σ1/2y)∈Rd×2, which yields
Id+ 2Σ1/2xx⊺Σ1/2+ 2Σ1/2yy⊺Σ1/2=|Id+AB⊺|
=|I2+B⊺A|=
1 + 2 x⊺Σx 2x⊺Σy
2x⊺Σy 1 + 2 y⊺Σy.
This directly implies Equation (S21). Furthermore, Equation (S22) follows with Σ∈R2×2and
x= (1
0), y= (0
1)or with x=y= (1
0)and an arbitrary invertible covariance matrix Σ′such that
Σ′
11= Σ 11.
Corollary D.1. Given (X, Y)∼ N(0,Σ)with invertible covariance matrix Σ, it holds
˙Tm(Σ) = E[˙erfm(X)˙erfm(Y)] =2
πΣ + I2/(2m2)−1/2m→∞− − − − →2
π|Σ|−1/2. (S23)
If(X, Y)∼ N(0,Σ)withX=Yand singular covariance matrix Σ∈R2×2, it holds
˙Tm(Σ)m→∞− − − − → ∞ .
Proof.
˙Tm(Σ) = m2˙T(m2·Σ)Lemma D.1=4m2
π|I2+ 2m2Σ|−1/2=2m2
π 
4m4|I2/(2m2) + Σ|−1/2
=2
πΣ + I2/(2m2)−1/2m→∞− − − − →2
π|Σ|−1/2ifΣis invertible,
∞ ifΣis singular.
Remark D.1. As mentioned at the beginning of this chapter, we can get the same result by considering
the distributional derivative of the sign function, 2δ0. It holds for (X, Y)∼ N(0,Σ)with invertible
covariance matrix Σas before
E[δ0(X)δ0(Y)] =Z
R21
2π|Σ|1/22δ0(z1) 2δ0(z2)e−1
2z⊺Σ−1zdz=2
π|Σ|−1/2.
In the case of X=Y, the integral is no longer well-defined.
Next, we consider Tby solving a more general problem, which was formulated in slightly less
general form by Williams [1996, Chapter 3.1].
Lemma D.2. Given U∼ N(0,Σ)with invertible covariance matrix Σ∈Rd×dandx, y∈Rd, it
holds
V:=E[erf(U⊺x) erf(U⊺y)] =2
πarcsin2x⊺Σy√
1 + 2 x⊺Σx√1 + 2 y⊺Σy
.
In particular, given (X, Y)∼ N(0,Σ)with invertible covariance matrix Σ = Σ1Σ3
Σ3Σ2
∈R2×2or
withX=Yand singular covariance matrix Σ = Σ1Σ3
Σ3Σ2
∈R2×2,Σ1= Σ 2= Σ 3, it holds
E[erf(X) erf(Y)] =2
πarcsin2 Σ3√1 + 2 Σ 1√1 + 2 Σ 2
.
Proof. We follow the proof idea given by Williams [1996, Chapter 3.1], that is, we define V(λ),
differentiate the expectation, and integrate by parts. We can then see thatd
dλV(λ) = (1 −γ2)−1/2dγ
dλ,
which gives the desired arcsin . So, we define
V(λ) =E[erf(λ·U⊺x) erf(U⊺y)] =Z
Rd1
(2π)d/2|Σ|1/2erf(λ·u⊺x) erf(u⊺y)e−1
2u⊺Σ−1udu,
25and then differentiate with respect to λon both sides
d
dλV(λ) =Z
Rd1
(2π)d/2|Σ|1/22u⊺x√πe−λ2(u⊺x)2erf(u⊺y)e−1
2u⊺Σ−1udu
=x⊺Z
Rd1
(2π)d/2|Σ|1/22u√πe−λ2·u⊺xx⊺uerf(u⊺y)e−1
2u⊺Σ−1udu
(⋆)=2x⊺Σ1/2
√πZ
Rd|Σ|1/2v
(2π)d/2|Σ|1/2erf(v⊺Σ1/2y) exp
−1
2v⊺
Id+ 2λ2Σ1/2xx⊺Σ1/2
v
dv
=−2x⊺Σ1/2
√π(2π)d/2(Id+ 2λ2Σ1/2xx⊺Σ1/2)−1
×Z
Rderf(v⊺Σ1/2y)·(Id+ 2λ2Σ1/2xx⊺Σ1/2)v·exp
−1
2v⊺
Id+ 2λ2Σ1/2xx⊺Σ1/2
v
dv
=2x⊺Σ1/2
√π(2π)d/2(Id+ 2λ2Σ1/2xx⊺Σ1/2)−1
×Z
Rdexp
−1
2v⊺
Id+ 2λ2Σ1/2xx⊺Σ1/2
v
· ∇verf(v⊺Σ1/2y)dv, (S24)
using a change of variables u= Σ1/2vin Equation (⋆)and using partial integration and Gauss’
divergence theorem in the last equation. In addition, we used that
∇ve−1
2v⊺(Id+2λ2Σ1/2xx⊺Σ1/2)v=−(Id+ 2λ2Σ1/2xx⊺Σ1/2)v e−1
2v⊺(Id+2λ2Σ1/2xx⊺Σ1/2)v
and the partial integration rule for scalar functions. To be precise, for differentiable scalar functions f
andgit holds
∇(f·g) =f· ∇g+g· ∇f,
which then implies the partial integration rule. Furthermore, the left-hand side vanishes in our case
due to Gauss’ divergence theorem:
Z
Rd∇v
erf(v⊺Σ1/2y)·e−1
2v⊺(Id+2λ2Σ1/2xx⊺Σ1/2)v
dv
=lim
R→∞Z
Sd−1(R)erf(v⊺Σ1/2y)·e−1
2v⊺(Id+2λ2Σ1/2xx⊺Σ1/2)vdv
≤lim
R→∞Z
Sd−1(R)e−1
2v⊺(Id+2λ2Σ1/2xx⊺Σ1/2)vdv
≤lim
R→∞Sd−1(R)·e−1
2R2λmin(Id+2λ2Σ1/2xx⊺Σ1/2)= 0,
where Sd−1(R)is the surface area of the sphere in Rdwith radius R. Continuing with our previous
calculations, we see that
(S24) =2x⊺Σ1/2
√π(2π)d/2(Id+ 2λ2Σ1/2xx⊺Σ1/2)−1
×Z
Rde−1
2v⊺(Id+2λ2Σ1/2xx⊺Σ1/2)v·Σ1/2y2√πe−(v⊺Σ1/2y)2dv
=4
πx⊺Σ1/2(Id+ 2λ2Σ1/2xx⊺Σ1/2)−1Σ1/2y
(2π)d/2Z
Rde−1
2v⊺(Id+2λ2Σ1/2xx⊺Σ1/2+2Σ1/2yy⊺Σ1/2)vdv.
(S25)
We evaluate the expression outside of the integral in the last line by applying the Sherman-Morrison-
Woodbury formula. For A∈Rd×dandw1, w2∈Rdit holds [Golub and Van Loan, 1996, (2.4.1)]
(C+w1w⊺
2)−1=C−1−C−1w1w⊺
2C−1
1 +w⊺
2C−1w1.
26ForC= Idandw=w1=w2=√
2λΣ1/2xthis yields
(Id+ 2λ2Σ1/2xx⊺Σ1/2)−1= (I d+ww⊺)−1= Id−ww⊺
1 +w⊺w= Id−2λ2Σ1/2xx⊺Σ1/2
1 + 2 λ2x⊺Σx.
With this we see that
x⊺Σ1/2(Id+ 2λ2Σ1/2xx⊺Σ1/2)−1Σ1/2y=x⊺Σ1/2
Id−2λ2Σ1/2xx⊺Σ1/2
1 + 2 λ2x⊺Σx
Σ1/2y
=x⊺Σy−2λ2(x⊺Σx)(x⊺Σy)
1 + 2 λ2x⊺Σx=x⊺Σy
1−2λ2x⊺Σx
1 + 2 λ2x⊺Σx
=x⊺Σy
1 + 2 λ2x⊺Σx
Inserting this into Equation (S25), we obtain
(S25) =2
π2x⊺Σy
1 + 2 λ2x⊺ΣxZ
Rd1
(2π)d/2e−1
2v⊺(Id+2λ2Σ1/2xx⊺Σ1/2+2Σ1/2yy⊺Σ1/2)vdv
=2
π2x⊺Σy
1 + 2 λ2x⊺Σx
Id+ 2λ2Σ1/2xx⊺Σ1/2+ 2Σ1/2yy⊺Σ1/2−11/2
=2
π2x⊺Σy
1 + 2 λ2x⊺ΣxId+ 2λ2Σ1/2xx⊺Σ1/2+ 2Σ1/2yy⊺Σ1/2−1/2
.
As in the proof of Lemma D.1, we evaluate this using Sylvester’s determinant theorem. With the
same notation as before, we can define A=B= (√
2λΣ1/2x,√
2Σ1/2y)∈Rd×2. This yields
Id+ 2λ2Σ1/2xx⊺Σ1/2+ 2Σ1/2yy⊺Σ1/2=|Id+AB⊺|=|I2+B⊺A|
=
1 + 2 λ2x⊺Σx2λx⊺Σy
2λx⊺Σy 1 + 2 y⊺Σy= (1 + 2 λ2x⊺Σx)(1 + 2 y⊺Σy)−4λ2(x⊺Σy)2.
If we insert this this again, we have so far shown
d
dλV(λ) =2
π2x⊺Σy
1 + 2 λ2x⊺Σx 
(1 + 2 λ2x⊺Σx)(1 + 2 y⊺Σy)−4λ2(x⊺Σy)2−1/2. (S26)
We now define
γ(λ):=2λx⊺Σyp
(1 + 2 λ2x⊺Σx)(1 + 2 y⊺Σy),
and claim that
2
π 
1−γ(λ)2−1/2d
dλγ(λ) =d
dλV(λ). (S27)
We can find a solution to the claimed equation by finding a function ˜Vthat satisfies
d
dγ(λ)˜V(γ(λ)) =2
π 
1−γ(λ)2−1/2,
and by setting V(λ):=˜V(γ(λ)). This follows from the chain rule. ˜Vis thus simply given by
˜V(γ(λ)) =2
πarcsin( γ(λ)). In particular, this yields
V=V(1) = ˜V(γ(1)) =2
πarcsin 
2x⊺Σyp
(1 + 2 x⊺Σx)(1 + 2 y⊺Σy)!
.
It is now left to show Equation (S27) using Equation (S26). First, see that
 
1−γ(λ)2−1/2=
1−(2λx⊺Σy)2
(1 + 2 λx⊺Σx)(1 + 2 y⊺Σy)−1/2
=(1 + 2 λx⊺Σx)(1 + 2 y⊺Σy)
(1 + 2 λx⊺Σx)(1 + 2 y⊺Σy)−4λ2(x⊺Σy)21/2
.
27Second, it holds
d
dλh
λ(1 + 2 λ2x⊺Σx)−1/2i
=d
dλh
(λ−2+ 2x⊺Σx)−1/2i
=λ−3(λ−2+ 2x⊺Σx)−3/2
= (1 + 2 λ2x⊺Σx)−3/2=1
 √
1 + 2 λ2x⊺Σx3.
With the results of both calculations, we get
2
π 
1−γ(λ)2−1/2d
dλγ(λ) =2
π 
1−γ(λ)2−1/22x⊺Σy√1 + 2 y⊺Σyd
dλh
λ(1 + 2 λ2x⊺Σx)−1/2i
=2
π√
1 + 2 λx⊺Σx√1 + 2 y⊺Σyp
(1 + 2 λx⊺Σx)(1 + 2 y⊺Σy)−4λ2(x⊺Σy)22x⊺Σy√1 + 2 y⊺Σy1
 √
1 + 2 λ2x⊺Σx3
=2
π1p
(1 + 2 λx⊺Σx)(1 + 2 y⊺Σy)−4λ2(x⊺Σy)22x⊺Σy
1 + 2 λ2x⊺Σx(S26)=d
dλV(λ),
which yields Equation (S27) and concludes the proof. The special case Σ∈R2×2with invertible
covariance matrix or X=Yand singular covariance matrix follows as in the proof of Lemma
D.1.
Corollary D.2. If(X, Y)∼ N(0,Σ)with invertible covariance matrix Σ = Σ1Σ3
Σ3Σ2
∈R2×2or
withX=Yand singular covariance matrix Σ = Σ1Σ3
Σ3Σ2
∈R2×2,Σ1= Σ 2= Σ 3, it holds
Tm(Σ) =2
πarcsin
Σ3q
1
2m2+ Σ 1q
1
2m2+ Σ 2
m→∞− − − − →2
πarcsinΣ3√Σ1√Σ2
. (S28)
Proof. It holds
Tm(Σ) = T(m2·Σ)Lemma D.2=2
πarcsin2m2Σ3√1 + 2 m2Σ1√1 + 2 m2Σ2
=2
πarcsin
Σ3q
1
2m2+ Σ 1q
1
2m2+ Σ 2
m→∞− − − − →2
πarcsinΣ3√Σ1√Σ2
.
We can now use Corollary D.1 and Corollary D.2 to evaluate Σ(L),˙Σ(L)andΘ(L)for activation
function erfm, which we denote by Σ(L)
m,˙Σ(L)
m, and Θ(L)
mrespectively. We then are interested in the
limitm→ ∞ . First recall that
Σ(1)(x, x′) =σ2
w
n0⟨x, x′⟩+σ2
band
Σ(L)(x, x′) =σ2
wEg∼N(0,Σ(L−1))[σ(g(x))σ(g(y))] + σ2
b,
by Theorem C.2. Therefore, Σ(1)
mis independent of m, and it holds for any x, y∈Rn0
Σ(1)
m(x, y) =σ2
w
n0⟨x, y⟩+σ2
b=:Σ(1)
∞(x, y).
28Assuming that the limit limm→∞Σ(L)
m(x, y) =:Σ(L)
∞(x, y)exists and that Σ(L)
∞(x, x)̸= 0 ,
Σ(L)
∞(y, y)̸= 0, we can define
Σ(L+1)
m (x, y) =σ2
wE(X,Y)∼N(0,Σ(L)
m;x,y)[erfm(X) erf m(Y)] +σ2
b=σ2
wTm
Σ(L)
m;x,y
+σ2
b
Cor. D.2=2σ2
w
πarcsin
Σ(L)
m(x, y)q
1
2m2+ Σ(L)
m(x, x)q
1
2m2+ Σ(L)
m(y, y)
+σ2
b
m→∞− − − − →2σ2
w
πarcsin
Σ(L)
∞(x, y)q
Σ(L)
∞(x, x)q
Σ(L)
∞(y, y)
+σ2
b
=:Σ(L+1)
∞ (x, y). (S29)
Hence, it follows via induction and from the continuity of the arcsin function that
limm→∞Σ(L)
m(x, y) =:Σ(L)
∞(x, y)is well defined. We discuss the resulting kernel Σ(L)
∞in Re-
mark D.3. For ˙Σ(L+1)
m ,L≥1, it holds
˙Σ(L+1)
m (x, y) =σ2
wE(X,Y)∼N(0,Σ(L)
m;x,y)[˙erfm(X)˙erfm(Y)] =σ2
w˙Tm
Σ(L)
m;x,y
Cor. D.1=2σ2
w
πΣ(L)
m;x,y+1
2m2I2−1/2
=2σ2
w
π
Σ(L)
m(x, x) +1
2m2
Σ(L)
m(y, y) +1
2m2
−Σ(L)
m(x, y)2−1/2
.
(S30)
As we will see later, the limit
lim
m→∞˙Σ(L)
m(x, y) =:˙Σ(L)
∞(x, y) (S31)
exists for x̸=yapart from a few exceptions. In the case x=y, we obtain
˙Σ(L+1)
m (x, x) =2σ2
w
π 
Σ(L)
m(x, x) +1
2m22
−Σ(L)
m(x, x)2!−1/2
=2σ2
w
π1
m2Σ(L)
m(x, x) +1
4m4−1/2
=2σ2
w
πm
Σ(L)
m(x, x) +1
4m2−1/2
∼2σ2
w
πmΣ(L)
∞(x, x)−1/2m→∞− − − − → ∞ , (S32)
where ∼denotes asymptotic equality. Therefore, for x̸=y, the limit
lim
m→∞Θ(L)
m(x, y) =:Θ(L)
∞(x, y)
exists. However, due to Equation (S32), the NTK diverges for x=yasm→ ∞ . We will call a
kernel with this property a singular kernel. We also say that a kernel with this property is singular
along the diagonal .
Remark D.2 (Distributional neural tangent kernel) .An alternative conceivable approach to obtain
the same kernel Θ(L)
∞would have been to consider the distributional Jacobian matrix of the network
function with step-like activation function. Whether or not a distributional NTK can be formulated is
a question for further research. Here, we only want to point out that the corresponding formulas for
the recursive definition of the analytical distributional NTK would then naturally read as follows,
Θ(1)
∞(x, x′) = Σ(1)
∞(x, x′)
Θ(L)
∞(x, x′) = Σ(L)
∞(x, x′) + Θ(L−1)
∞ (x, x′)·˙Σ(L)
∞(x, x′)forL≥2,
where
Σ(L)
∞(x, x′) =σ2
wEg∼N(0,Σ(L−1)
∞ )[sign(g(x))sign(g(y))] + σ2
bforL≥2, (S33)
˙Σ(L)
∞(x, x′) =σ2
wEg∼N(0,Σ(L−1)
∞ )[2δ0(g(x)) 2δ0(g(y))] forL≥2. (S34)
Note that Equation (S34) can be derived from Remark D.1, and that Equation (S33) is also easy to
show.
29We now want to explore the implications of our findings in this section for fNTK, which we introduced
at the end of Section C.2. Equation (S20) yields
lim
m→∞fm
NTK(x) = lim
m→∞Θm(x,X)Θm(X,X)−1Y=:f∞
NTK(x).
Note that Θm(X,X):= Θ(L)
m(X,X)is invertible for sufficiently large m, as the matrix will be
dominated by the diagonal for large m. Using this and the fact Θm(x,X)m→∞− − − − → Θ∞(x,X)if
x̸=xifor all i= 1, . . . , d , we obtain for such x:
fm
NTK(x) = Θ m(x,X)Θm(X,X)−1Ym→∞− − − − → 0
However, if x=xi∈ X and denoting the i-th basic vector by ei∈Rdwe get:
fm
NTK(xi) = Θ m(xi,X)Θm(X,X)−1Y=e⊺
iY=yi
Therefore, fm
NTKconverges pointwise to a function that is zero almost everywhere, but interpolates
exactly at the data points. The singular kernel Θ(L)
∞, which we can see as the analytic NTK for the
sign function, is for that reason not suitable for regression. We will deal with this problem in the
following section.
D.2 From singular kernel to Nadaraya-Watson estimator
Radhakrishnan et al. [2023] considered the limit of infinite depth, limL→∞Θ(L), and a singular
kernel emerged similar to the previous section. It was shown that the resulting estimator using this
singular kernel behaves like a Nadaraya-Watson estimator when classification tasks are considered
instead of regression tasks. We will adapt the ideas of Radhakrishnan et al. [2023] to show similar
results. To consider a classification task, we assume that Y ∈ {− 1,1}d. The network is then trained
with data points (X,Y)as before, but we apply the sign function at the end to obtain the final
classifier. As Radhakrishnan et al. [2023], we assume that we are operating in the infinite-width limit
after infinite training. So, we are interested in
lim
m→∞sign 
Θm(x,X)Θm(X,X)−1Y
. (S35)
We will see that the resulting classifier is equal to
sign(Θ∞(x,X)Y) =sign dX
i=1Θ(x, xi)yi!
.
The form of this classifier is similar to a Nadaraya-Watson estimator. To be precise, for a singular
kernel kand data points (X,Y), the Nadaraya-Watson estimator is defined as
c(x):=Pd
i=1k(x, xi)yiPd
i=1k(x, xi)for all x̸=xi,1≤i≤d.
Since k(x, xi)→ ∞ asx→xi, it holds that c(x)→yiasx→xi. Thus, the continuous extension
ofctoRn0interpolates the data points.
We now begin to further evaluate Σ(L)
∞and˙Σ(L)
∞to analyze Equation (S35):
Σ(1)
∞(x, y) =σ2
w
n0⟨x, y⟩+σ2
b, (S36)
Σ(L)
∞(x, x)(S29)=2σ2
w
πarcsin(1) + σ2
b=σ2
w+σ2
bfor all L≥2, (S37)
Σ(2)
∞(x, y)(S29)=2σ2
w
πarcsin
σ2
w
n0⟨x, y⟩+σ2
bq
σ2w
n0∥x∥2+σ2
bq
σ2w
n0∥y∥2+σ2
b
+σ2
b,and (S38)
Σ(L+1)
∞ (x, y)(S29)=2σ2
w
πarcsin
Σ(L)
∞(x, y)q
Σ(L)
∞(x, x)q
Σ(L)
∞(y, y)
+σ2
b
(S37)=2σ2
w
πarcsin 
Σ(L)
∞(x, y)
σ2w+σ2
b!
+σ2
bfor all L≥2. (S39)
30Regarding the assumptions made for Equation (S29) , note that that Σ(L)
∞(x, x)̸= 0 ifL≥2and
Σ(1)
∞(x, x)̸= 0ifx̸= 0orσ2
b>0.
After Equation (S31), we mentioned exceptions to the existence of ˙Σ(L)
∞(x, y)forx̸=ythat were
postponed at that time. For ˙Σ(2)
mwithx̸=ywe can see that
˙Σ(2)
m(x, y)
(S30)=2σ2
w
π σ2
w
n0∥x∥2+σ2
b+1
2m2σ2
w
n0∥y∥2+σ2
b+1
2m2
−σ2
w
n0⟨x, y⟩+σ2
b2!−1/2
=2σ2
w
πσ4
w
n2
0 
∥x∥2∥y∥2− ⟨x, y⟩2
+σ2
wσ2
b
n0 
∥x∥2+∥y∥2−2⟨x, y⟩
+1
2m2Am−1/2
m→∞− − − − →2σ2
w
πσ4
w
n2
0 
∥x∥2∥y∥2− ⟨x, y⟩2
+σ2
wσ2
b
n0∥x−y∥2−1/2
=:˙Σ(2)
∞(x, y), (S40)
assuming that either σ2
b>0or that x, yare not parallel, i.e., |⟨x, y⟩| ̸=∥x∥∥y∥. The term1
2m2Amis
given by
1
2m2Am=1
2m2σ2
w
n0∥x∥2+σ2
b
+σ2
w
n0∥y∥2+σ2
b
+1
2m2
m→∞− − − − → 0.
ForL≥3the expression simplifies:
˙Σ(L)
m(x, y)
(S30)=2σ2
w
π
Σ(L−1)
m (x, x) +1
2m2
Σ(L−1)
m (y, y) +1
2m2
−Σ(L−1)
m (x, y)2−1/2
=2σ2
w
π
Σ(L−1)
m (x, x)·Σ(L−1)
m (y, y)−Σ(L−1)
m (x, y)2
+1
2m2Bm−1/2
m→∞− − − − →2σ2
w
π
Σ(L−1)
∞ (x, x)·Σ(L−1)
∞ (y, y)−Σ(L−1)
∞ (x, y)2
(S37)=2σ2
w
π
(σ2
w+σ2
b)2−Σ(L−1)
∞ (x, y)2−1/2
=:˙Σ(L)
∞(x, y), (S41)
with the term1
2m2Bmgiven by
1
2m2Bm=1
2m2
Σ(L−1)
m (x, x) + Σ(L−1)
m (y, y) +1
2m2
m→∞− − − − → 0.
From Equation (S39) and Equation (S41) we see that
d
d
Σ(L−1)
∞ (x, y)Σ(L)
∞(x, y) =˙Σ(L)
∞(x, y).
Although we did not use dual activation functions to denote the NTK as [Jacot et al., 2018, Section
A.4], we still find that the property (ˆσ)′=d(σ′)applies, i.e., the derivative of the dual activation
function is the dual of the derivative of the activation function. Here ˆσdenotes the dual activation
function of σ.
In the case x=ywe get
˙Σ(2)
m(x, x)(S32)∼2σ2
w
πmσ2
w
n0∥x∥2+σ2
b−1/2
,and using (S37) (S42)
˙Σ(L)
m(x, x)(S32)∼2σ2
w
πm 
σ2
w+σ2
b−1/2forL≥3. (S43)
We summarize the above calculations in the following lemma:
31Lemma D.3. Form, L∈NletΣ(L)
mand˙Σ(L)
mbe as in Theorem C.2 for activation function erfm. It
then holds for any x̸=ywithx, y̸= 0:
Σ(1)
∞(x, y) = lim
m→∞Σ(1)
m(x, y)(S36)=σ2
w
n0⟨x, y⟩+σ2
b,
Σ(2)
∞(x, y) = lim
m→∞Σ(2)
m(x, y)(S38)=2σ2
w
πarcsin
σ2
w
n0⟨x, y⟩+σ2
bq
σ2w
n0∥x∥2+σ2
bq
σ2w
n0∥y∥2+σ2
b
+σ2
b,
Σ(L)
∞(x, y) = lim
m→∞Σ(L)
m(x, y)(S39)=2σ2
w
πarcsin 
Σ(L−1)
∞ (x, y)
σ2w+σ2
b!
+σ2
bfor all L≥3,
Σ(L)
∞(x, x) = lim
m→∞Σ(L)
m(x, x)(S37)=σ2
w+σ2
bfor all L≥2,
and, assuming that x, yare not parallel or that σ2
b>0,
˙Σ(2)
∞(x, y) = lim
m→∞˙Σ(2)
m(x, y)(S40)=2σ2
w
πσ4
w
n2
0 
∥x∥2∥y∥2− ⟨x, y⟩2
+σ2
wσ2
b
n0∥x−y∥2−1
2
=2σ2
w
π
Σ(1)
∞(x, x) Σ(1)
∞(y, y))−Σ(1)
∞(x, y)2−1
2,
˙Σ(L)
∞(x, y) = lim
m→∞˙Σ(L)
m(x, y)(S41)=2σ2
w
π
(σ2
w+σ2
b)2−Σ(L−1)
∞ (x, y)2−1
2for all L≥3,
˙Σ(2)
m(x, x)(S42)∼2σ2
w
πmσ2
w
n0∥x∥2+σ2
b−1
2
,
˙Σ(L)
m(x, x)(S43)∼2σ2
w
πm 
σ2
w+σ2
b−1
2forL≥3.
Remark D.3 (Addendum to Remark C.5) .In Remark C.5 we discussed the topology of the space
of the Gaussian processes to which ANNs with continuous activation functions converge in the
infinite-width limit. The product σ-algebra restricts us to a countable input set, so it is not possible to
check for properties such as continuity or even differentiability. While Theorem C.4 is stated only
for continuous activation functions with linear envelope property, we will see in Theorem E.3 that
the convergence also holds in the (weak) infinite-width limit even for step-like activation functions.
For the sign function as activation function, the covariance function of this Gaussian process is then
given by Σ(L)
∞. Since we know explicitly what this covariance function looks like, we can examine
the sample-continuity of the process. Note that to get the full picture, one would still have to show
functional convergence of the network to this process, as Bracale et al. [2021] have done.
Σ(L)
∞is isotropic when restricted to a sphere, as will be discussed in the next section. In the case of
isotropic covariance functions, k(x, y) =k(∥x−y∥), the simplest way to show sample-continuity
using the Kolmogorov-Chentsov criterion is to show Lipschitz continuity of kat zero. This can be
seen from the proof of Lemma 4.3 by Lang and Schwab [2015]. In our case, the covariance function
is basically given by a composition of arcsin functions. Lipschitz continuity of kat zero is therefore
equivalent to Lipschitz continuity of the arcsin function at 1, which does not hold. In conclusion, it is
not possible to show sample-continuity of the Gaussian process given by Σ(L)
∞in the established way
using the Kolmogorov-Chentsov criterion.
Clearly, the kernel can be rewritten in terms of the arccos function. Cho and Saul [2009] analyzed
arc-cosine kernels in the context of deep learning in detail.
Corollary D.3. Form, L∈NletΘ(L)
mas in Theorem C.2 for activation function erfm. Ifx̸=yand
either x, ynot parallel or σ2
b>0, then the limit
Θ(L)
∞(x, y) = lim
m→∞Θ(L)
m(x, y)
exists. Furthermore, it holds, asymptotically as m→ ∞ ,
Θ(L)
m(x, x)∼2σ2
w
πσ2
w
n0∥x∥2+σ2
b1
22σ2
w
π 
σ2
w+σ2
b−1
2L−2
mL−1forL≥2.
32Proof. The first statement directly follows from Lemma D.3 and the definition of Θ(L)
m. The recursive
definition can be resolved to the following formula:
Θ(L)
m(x, y) =LX
k=1Σ(k)
m(x, y)·L−1Y
l=k˙Σ(l+1)
m(x, y).
With
K(z):=2σ2
w
πσ2
w
n0z+σ2
b−1
2
,
we get from Lemma D.3 that ˙Σ(2)
m(x, x)∼K 
∥x∥2
·mand˙Σ(L)
m(x, x)∼K(n0)·mforL≥3. This
impliesQL−1
l=1˙Σ(l+1)
m(x, x)∼K(∥x∥2)K(n0)L−2·mL−1andQL−1
l=k˙Σ(l+1)
m(x, x)∼K(n0)L−k·
mL−kfor any k≥2. For L≥2, we get that
Θ(L)
m(x, x)∼Σ(1)
∞(x, y)K 
∥x∥2
K(n0)L−2·mL−1+LX
k=2Σk
∞(x, x)·K(n0)L−k·mL−k
∼σ2
w
n0∥x∥2+σ2
b2σ2
w
πσ2
w
n0∥x∥2+σ2
b−1
2
K(n0)L−2mL−1
=2σ2
w
πσ2
w
n0∥x∥2+σ2
b1
22σ2
w
π 
σ2
w+σ2
b−1
2L−1
mL−2.
Theorem D.4 (Inspired by Lemma 5 of Radhakrishnan et al. [2023]) .Letσ2
b>0or let all xi∈Rn0
be pairwise non-parallel. Let L≥2andxi∈ Sn0−1
R for all i= 1, . . . , d , where Sn0−1
R⊆Rn0is
the sphere of radius R. Then, with
c(L)(x):= lim
m→∞sign
Θ(L)
m(x,X)Θ(L)
m(X,X)−1Y
,
and assuming that Θ(L)
∞(x,X)Y ̸= 0for almost all x∈ Sn0−1
R , it holds
c(L)(x) =sign
Θ(L)
∞(x,X)Y
a.e. on Sn0−1
R.
Proof. First note that almost all x∈ Sn0−1
R are not parallel to any xi∈ X. We denote Θ(L)
m(X,X) =:
Θm,m∈N∪ {∞} , for convenience. Let am>0be a positive constant that we will choose later. It
then holds for almost all x∈ Sn0−1
R
sign
Θ(L)
m(x,X)Θ−1
mY
=sign
amΘ(L)
m(x,X)Θ−1
mY
=sign h
amΘ(L)
m(x,X)Θ−1
mY −Θ(L)
m(x,X)Yi
| {z }
=:Am+h
Θ(L)
m(x,X)Y −Θ(L)
∞(x,X)Yi
| {z }
=:Bm
+ Θ(L)
∞(x,X)Y!
.
We now show that AmandBmgo to zero as m→ ∞ for a suitable choice of am. First, note that
|Bm| ≤Θ(L)
m(x,X)−Θ(L)
∞(x,X)
2∥Y∥2m→∞− − − − → 0,
since∥Y∥ 2<∞and by Corollary D.3 it holds that Θ(L)
m(x, xi)→Θ(L)
∞(x, xi)for all xi∈ X as
m→ ∞ . Second, we have
|Am| ≤amΘ(L)
m(x,X)Θ−1
m−Θ(L)
m(x,X)
2∥Y∥2≤Θ(L)
m(x,X)
2amΘ−1
m−Id
2∥Y∥2.
(S44)
33Again by Corollary D.3 it holds that ∥Θ(L)
m(x,X)∥2→ ∥Θ(L)
∞(x,X)∥2asm→ ∞ . Furthermore,
for all 1≤i≤dwe have Θ(L)
m(xi, xi)∼C(R)·mL−1for some constant C(R)which depends
onR=∥xi∥. Since it holds Θ(L)
m(xj, xi)→Θ(L)
∞(xj, xi)asm→ ∞ for any i̸=j, we choose
am=C(R)·mL−1and conclude
a−1
mΘ(L)
m(xi, xj)m→∞− − − − →1ifi=j
0ifi̸=j.
This implies a−1
mΘm→Idasm→ ∞ . In particular, {Id} ∪ {a−1
mΘm|m∈N}is a compact set
with respect to ∥·∥2. Since D7→D−1is a continuous function, {Id} ∪ { amΘ−1
m|m∈N}is
bounded. We getamΘ−1
m−Id
2≤amΘ−1
2Id−a−1
mΘm
2m→∞− − − − → 0,
and thus
|Am|(S44)
≤Θ(L)
m(x,X)
2amΘ−1
m−Id
2∥Y∥2m→∞− − − − → 0.
Recall that Θ(L)
∞(x,X)Y ̸= 0, which now concludes the proof:
lim
m→∞sign
Θ(L)
m(x,X)Θ−1
mY
= lim
m→∞sign
Am+Bm+ Θ(L)
∞(x,X)Y
=sign
Θ(L)
∞(x,X)Y
.
Remark D.4. One can generalize the above theorem by dropping the restriction to Sn0−1
R . By Lemma
D.3 it holds Θ(L)
m(xi, xi)∼q
σ2w
n0∥xi∥2+σ2
b·C·mL−1. For some constant Cindependent of L
and∥xi∥. If we now define am:=C·mL−1, we get
a−1
mΘ(L)
m(X,X)m→∞− − − − → diag(q
σ2w∥xi∥2/n0+σ2
b
1≤i≤d)
=:D,
where diag{v}of a vector vis a square matrix with diagonal v. As in the proof above, and using the
continuity of the inverse map D7→D−1, this implies
lim
m→∞sign
Θ(L)
m(x,X)Θ−1
mY
=sign
Θ(L)
∞(x,X)D−1Y
=sign
˜Θ(L)
∞(x,X)Y
,
if we define
˜Θ(L)
∞(x, y):= 
σ2
w∥x∥2/n0+σ2
b−1/2 
σ2
w∥y∥2/n0+σ2
b−1/2Θ(L)
∞(x, y).
In conclusion, dropping the restriction leads to a similar form for our classifier, but with a modified
kernel.
Theorem D.4 shows that the classifier
lim
m→∞sign
Θ(L)
m(x,X)Θ−1
mY
= lim
m→∞sign(fm
NTK(x))
has an easily interpretable form that is close to the Nadaraya-Watson estimator with singular kernel
Θ(L)
∞. This is despite the fact that the pointwise limit of fm
NTK is trivial, i.e., regression is no longer
possible
D.3 Checking for Bayes optimality
In addition to the ideas used in the above section, Radhakrishnan et al. [2023] proved that the
resulting estimator is Bayes optimal under certain conditions. This property is also referred to as
consistency. Let the probability distribution Pdataon our data space be given by a random variable
(X, Y)∈Rn0× {− 1,1}and let
c(x) = arg max
˜y∈{−1,1}P(Y= ˜y|X=x)
be the Bayes classifier with respect to this distribution. Denote Xd= (x1, . . . , x d)andYd=
(y1, . . . , y d)for(xi, yi)drawn independently from Pdata. We then define Bayes optimality as
follows:
34Definition D.1 (Bayes optimality) .Letcd(·) =cd(·;Xd,Y)be classifiers for d∈Nand estimators
ofc(·). We then say that (cd)d∈Nis Bayes optimal, if it is a consistent estimator of c, i.e., for all
ε >0andX-almost all x
lim
d→∞P
|cd(x)−c(x)|> ε
= 0.
First, we summarize the results of Radhakrishnan et al. [2023] and consider Θ(L). If the singular
limiting kernel for L→ ∞ behaves like a singular kernel of the form k(x, y) =∥x−y∥−α, then the
classifier for L→ ∞ will be of the form
sign Pd
i=1yi/∥x−xi∥α
Pd
i=11/∥x−xi∥α!
,
assumingPd
i=11/∥x−xi∥α>0. This classifier satisfies Bayes optimality for α=n0by Devroye
et al. [1998]. Radhakrishnan et al. [2023] generalized the results of Devroye et al. [1998], expressed
αin terms of the activation function and its derivative, and chose them in such a way as to achieve
α=n0. Going back to our setup, we want to see if we can write
lim
m→∞Θ(L)
m(x, y) =R(∥x−y∥)
∥x−y∥α,
for some constant αand for some function R:R+→Rbounded away from zero as ∥x−y∥→0.
We start by proving that Θ(L)
m(x, y) =G(∥x−y∥)for some function G. Recall that we have to
restrict ourselves to a sphere by Theorem D.4. For simplicity, we restrict ourselves to the unit sphere
Sn0−1. Then, it holds ⟨x, x⟩= 1for all x∈ Sn0−1, which gives us
∥x−y∥2=⟨x−y, x−y⟩=⟨x, x⟩ −2⟨x, y⟩+⟨y, y⟩= 2(1 − ⟨x, y⟩).
In the following, we will substitute z=⟨x, y⟩. Taking ∥x−y∥→0is then equivalent to taking
z→1. We can conclude from Lemma D.3 that we can indeed write Σ(L)
∞(x, y) = Σ(L)
∞(z),
˙Σ(L)
∞(x, y) =˙Σ(L)
∞(z), and hence Θ(L)
∞(x, y) = Θ(L)
∞(z)for all L≥1. Note that Σ(L)
∞(1) = σ2
w+σ2
b
forL≥2andΣ(L)
∞(1) = σ2
w/n0+σ2
b. Next, we consider ˙Σ(2)
∞(z)forz→1using Lemma D.3
˙Σ(2)
∞(z) =2σ2
w
πσ4
w
n2
0(1−z2) +σ2
wσ2
b
n0·2(1−z)−1
2
=2σ2
w
πσ4
w
n2
0(1−z)(1 + z) +2σ2
wσ2
b
n0(1−z)−1
2
z→1∼2σ2
w
π2σ4
w
n2
0(1−z) +2σ2
wσ2
b
n0(1−z)−1
2
=2σ2
w
π2σ2
w
n0σ2
w
n0+σ2
b−1
2
(1−z)−1
2
=n0
π2σ2
w
n01
2σ2
w
n0+σ2
b−1
2
(1−z)−1
2. (S45)
ForL≥3, we can observe that
˙Σ(L)
∞(z) =2σ2
w
π 
σ2
w+σ2
b2−Σ(L−1)
∞ (z)2−1
2
=2σ2
w
π
σ2
w+σ2
b−Σ(L−1)
∞ (z)
σ2
w+σ2
b+ Σ(L−1)∞(z)−1
2
z→1∼2σ2
w
π 
2 
σ2
w+σ2
b−1
2 
σ2
w+σ2
b
−Σ(L−1)
∞ (z)−1
2. (S46)
We will now prove a lemma that allows us to analyze the behavior of ˙Σ(L)
∞(z)asz→1.
Lemma D.5. It holds:
lim
z→1(1−z)1/2
1−2
πarcsin ( z)=π
2√
2.
35Proof. Since the numerator and the denominator both go to zero as z→1, we apply l’Hôpital’s rule
and differentiate both. This yields
d
dzh
(1−z)1
2i
=−1
2(1−z)−1
2andd
dz
1−2
πarcsin ( z)
=−2
π 
1−z2−1
2.
Thus,
lim
z→1(1−z)1/2
1−2
πarcsin ( z)= lim
z→11
2√1−z√1 +z
2
π√1−z=π
4√
2 =π
2√
2,
concluding the proof.
ForL= 3it now holds
˙Σ(3)
∞(z)(S46)∼2σ2
w
π 
2 
σ2
w+σ2
b−1
2 
σ2
w+σ2
b
−Σ(2)
∞(z)−1
2
=2σ2
w
π 
2 
σ2
w+σ2
b−1
2σ−1
w
1−2
πarcsinσ2
wz/n0+σ2
b
σ2w/n0+σ2
b−1
2
(⋆)∼2σ2
w
π 
2 
σ2
w+σ2
b−1
2σ−1
wrπ
2√
2
1−σ2
wz/n0+σ2
b
σ2w/n0+σ2
b−1
4
=2σ2
w
π 
2 
σ2
w+σ2
b−1
2σ−1
wrπ
2√
2σ2
w/n0·(1−z)
σ2w/n0+σ2
b−1
4
=r
2σ2w
π
2√
2 
σ2
w+σ2
b−1
2σ2
w/n0
σ2w/n0+σ2
b−1
4
(1−z)−1
4,
where we used Lemma D.5 at (⋆). For L≥4it holds
˙Σ(L)
∞(z)(S46)∼2σ2
w
π 
2 
σ2
w+σ2
b−1
2 
σ2
w+σ2
b
−Σ(L−1)
∞ (z)−1
2
L−1≥3=2σ2
w
π 
2 
σ2
w+σ2
b−1
2σ−1
w 
1−2
πarcsin 
Σ(L−2)
∞ (z)
σ2w+σ2
b!!−1
2
(⋆)∼2σ2
w
π 
2 
σ2
w+σ2
b−1
2σ−1
wrπ
2√
2 
1−Σ(L−2)
∞ (z)
σ2w+σ2
b!−1
4
=2σ2
w
π 
2 
σ2
w+σ2
b−1
2σ−1
wrπ
2√
2 
σ2
w+σ2
b1
4
σ2
w+σ2
b−Σ(L−2)
∞ (z)−1
4
(S46)∼r
2σ2w
π 
2 
σ2
w+σ2
b−1
4σ−1
wrπ
2√
2 
σ2
w+σ2
b1
4q
˙Σ(L−1)
∞ (z)
=q
˙Σ(L−1)
∞ (z)/2,
again using Lemma D.5 at (⋆). This implies for arbitrary L≥2
˙Σ(L)
∞(z)∼K(L)·(1−z)−1/2L−1,
for some constant K(L)depending on L. Recall the formula for the NTK, that was used in the proof
of Corollary D.3
Θ(L)
∞(z) =LX
k=1Σ(k)
∞(z)·L−1Y
l=k˙Σ(l+1)
∞(z)∼LX
k=1Σ(k)
∞(z)·L−1Y
l=kK(l+ 1)·(1−z)−1/2l
∼Σ(1)
∞(1)L−1Y
l=1K(l+ 1)·(1−z)−1/2l=K(L)·(1−z)−(1−1/2L−1)
=K′(L)· ∥x−y∥−(2−1/2L−2)=:K′(L)· ∥x−y∥−α(L),
36for some constants K(L), K′(L). It is therefore to possible find a function Rthat is bounded away
from zero near ∥x−y∥ →0, such that
Θ(L)
∞(x, y) =K′(L)·R(∥x−y∥)
∥x−y∥α(L).
However, we have that α(1) = 1 andα(L)↑2asL→ ∞ . So it is only possible to choose Lsuch
thatα(L) =n0, ifn0= 1. But this is a trivial case, since we have restricted ourselves to the unit
sphere. In conclusion, we cannot prove that c(L)is a Bayes optimal classifier for any choice of
L. Chapter 4 of Devroye et al. [1998] suggests that, in fact, the estimator will not be universally
consistent.
E The NTK for surrogate gradient learning
In this chapter we explore surrogate gradient learning , introduced in Section 1, by connecting it to
the NTK. Recall that the standard gradient flow dynamics of the parameters are given by
d
dtθt=−η∇θL(f(X;θt);Y) =−η Jθf(X;θt)⊺∇f(X;θt)L(f(X;θt);Y). (S8)
As mentioned several times before, the Jacobian matrix Jθwill vanish for all parameters except those
in the last layer if we consider the sign activation function due to its zero derivative. The idea of
surrogate gradient learning is to circumvent the zero derivative of the sign function by replacing the
derivative with a surrogate derivative [Neftci et al., 2019]. We can replace the derivative in two main
ways:
•The activation function in the full network can be replaced by a differentiable surrogate
activation function. In particular, we obtain a non-vanishing surrogate derivative of the
activation function and thus non-vanishing network gradients. Let gdenote the network
with surrogate activation function. Therefore, we can train the weights according to
d
dtθt=−η Jθg(X;θt)⊺∇g(X;θt)L(g(X;θt);Y),
or consider the loss with respect to fand train according to
d
dtθt=−η Jθg(X;θt)⊺∇f(X;θt)L(f(X;θt);Y). (S47)
•Instead of replacing the activation function, we can only replace the derivative of the
activation function ˙σwith a surrogate derivative ˜σin Equation (S8). LetJσ,˜σbe the quasi-
Jacobian matrix as in Definition C.4 with activation function σand surrogate derivative ˜σ.
Then the training is given by
d
dtθt =−η Jσ,˜σ(X;θt)⊺∇f(X;θt)L(f(X;θt);Y)
=⇒d
dtf(x;θt) =−η Jθf(x;θt)Jσ,˜σ(X;θt)⊺∇f(X;θt)L(f(X;θt);Y) (S48)
=−η Jσ,˙σ(x;θt)Jσ,˜σ(X;θt)⊺∇f(X;θt)L(f(X;θt);Y)
=−ηˆI(L)(x,X;θt)∇f(X;θt)L(f(X;θt);Y), (S49)
with ˆI(L)as in Definition C.5 for σ1=σ,˜σ1= ˙σ,σ2=σand˜σ2= ˜σ. For Equations
(S48) and(S49) we assume that ˙σexists and is non-vanishing, but we deliberately train with
a surrogate gradient. For example, we can again consider erfmas the activation function.
Its derivative explodes at zero and vanishes everywhere else as m→ ∞ . The hope is that
limn1,...nL−1→∞ˆI(L)
m=I(L)
mexists and limm→∞I(L)
mis not a singular kernel as before.
In this chapter we will deal with the second approach, because Jσ,˜σ(x;θt) =:G(σ; ˜σ;x;θt)is closer
toJθf(x;θt) =G(σ; ˙σ;x;θt)thanJθg(x;θt) =G(η; ˜σ;x;θt)as a formula if Jθf(x;θt)exists and
is non-vanishing. Here, ηdenotes the surrogate activation function with derivative ˜σ. To do this,
we provide asymmetric generalizations of Theorem C.1, Theorem C.2, and Theorem C.3. These
generalizations would, in principle, even allow us to compare the two approaches.
37E.1 Asymmetric generalization of the neural tangent kernel
For this section we adopt the so-called linear envelope property from [Matthews et al., 2018, Definition
1] to ensure that all expectations exist in the following theorems:
∃m, c≥0∀u∈R:|σ(u)| ≤c+m|u| (S50)
First, we consider networks under the weak infinite-width limit, and are thus interested in taking the
number of hidden neurons to infinity sequentially. This is done inductively while using the central
limit theorem and the weak law of large numbers. In order to do this rigorously, we state and prove
two lemmata.
The first lemma is stated in terms of Gaussian measures on Hilbert spaces. An introduction to
Gaussian measures on Hilbert spaces can be found in Chapter 1 of Da Prato [2006]. A rigorous
derivation and definition of convergence in distribution on arbitrary metric spaces is given by Heyer
[2009, Chapter 1.2]. This includes a definition of weak convergence in terms of continuous and
bounded functions (Remark 1.2.5 (b)), a version of the Portemanteau theorem (Theorem 1.2.7), and
the fact that convergence in probability implies convergence in distribution (Application 1.2.15).
Lemma E.1. LetHm
iandZi,i, m∈N, be random variables with values in a separable Hilbert
space H. Furthermore, let Zibe independent and identically distributed with finite mean and
covariance operator V. If(Hm
1, . . . , Hm
k)D− →(Z1, . . . , Z k)asm→ ∞ for all k∈N, then there
exists k:N→Nsuch that k(m)→ ∞ monotonically as m→ ∞ and
1p
k(m)k(m)X
i=1Hm
iD− − − − →
m→∞Z, (S51)
for anH-valued Gaussian random variable Zwith mean and covariance operator like Z1. Similarly,
there exists k′:N→Nsuch that k′(m)→ ∞ monotonically as m→ ∞ and
1
k′(m)k′(m)X
i=1Hm
iD− − − − →
m→∞E[Z]. (S52)
Proof. SinceHis separable and complete, convergence in distribution and convergence with respect
to the Prokhorov metric d(also known as the Lévy-Prokhorov metric) are equivalent [Billingsley,
1999, Theorem 6.8]. By the central limit theorem for separable Hilbert spaces [Zalesski ˘ı et al., 1991],
this implies
lim
k→∞d 
1√
kkX
i=1Zi, Z!
= 0. (S53)
In addition, the assumption together with the continuous mapping theorem gives that
lim
m→∞d 
1√
kkX
i=1Hm
i,1√
kkX
i=1Zi!
= 0 for all k∈N.
In particular, for any k∈N, there exists some mk∈Nsuch that
d 
1√
kkX
i=1Hm
i,1√
kkX
i=1Zi!
≤1
kfor all m≥mk. (S54)
We now want to choose k(m)as large as possible for any m, but small enough to ensure Inequality
(S54), i.e., m≥mk(m). So we define
k(m):= sup{k|m≥mk}.
First note that {k|m≥mk} ̸=∅, ifm≥m1. The map k:N→Nis therefore well-defined, as we
consider m→ ∞ . Similarly, we can find a m≥mkfor any given k. This yields
lim
m→∞k(m) = lim
m→∞sup{k|m≥mk}=∞.
38By definition of k(m), it holds m≥mk(m)for all m∈Nand thus
d
1p
k(m)k(m)X
i=1Hm
i,1p
k(m)k(m)X
i=1Zi
≤1
k(m)for all m∈N. (S55)
Together with Equation (S53) this yields the claim, (S51):
d
1p
k(m)k(m)X
i=1Hm
i, Z

≤d
1p
k(m)k(m)X
i=1Hm
i,1p
k(m)k(m)X
i=1Zi
+d
1p
k(m)k(m)X
i=1Zi, Z

(S55)
≤1
k(m)+d
1p
k(m)k(m)X
i=1Zi, Z
m→∞− − − − → 0.
For the second claim, (S52), one can follow the same procedure but use the law of large numbers
for Banach spaces instead of the central limit theorem. Suitable results are given by Ledoux and
Talagrand [1991, Corollary 7.10] and Hoffmann-Jørgensen and Pisier [1976, Theorem 2.1]. Note that
even the strong law of large numbers holds, but the weak law is sufficient.
In the second lemma, some properties about convergence in distribution and convergence in probabil-
ity are stated.
Lemma E.2 (Theorem 2.7 from van der Vaart [1998], modified and (iv) added) .LetXn, XandYn
be random vectors. Then
(i)XnP− →Ximplies XnD− →X;
(ii)XnP− →cfor a constant cif and only if XnD− →c;
(iii) if XnD− →XandYnP− →cfor a constant c, then (Xn, Yn)D− →(X, c);
(iv) ifXnD− →Xand W is a random vector independent of (Xn)n∈N, then (Xn, W)D− →(X, W ).
Proof. (i) – (iii). The proofs are given by van der Vaart [1998].
(iv). Letfbe a bounded and continuous function. Then,
lim
n→∞E[f(Xn, W)] = lim
n→∞Z
E[f(Xn, W)|W] (x)dP(w)
(⋆)= lim
n→∞Z
E[f(Xn, W(w))]dP(w) =Z
lim
n→∞E[f(Xn, W(w))]dP(w)
=Z
E[f(X, W (w))]dP(w) =Z
E[f(X, W )|W] (w)dP(w) =E[f(X, W )],
where we used the independence of Wand(Xn)n∈Nin Equation (⋆)and the boundedness of ffor
the interchange of limit and integration. This proves convergence in distribution.
Remark E.1. The above theorem can be generalized to metric spaces. One can easily check that the
proofs in [van der Vaart, 1998, Theorem 2.7] also work for metric spaces using the Portemanteau
theorem provided by Heyer [2009, Theorem 1.2.7]. However, it is necessary to derive some more
equivalent characterizations of convergence in distribution, which are given and used by van der
Vaart [1998] but are missing in the work of Heyer [2009].
Theorem E.3 (Generalized version of Proposition 1 by Jacot et al. [2018]) .For activation functions σ1
andσ2with property (S50), which are continuous except for finitely many jump points, let f1(·;θ)and
f2(·;θ)be network functions with hidden layers h(l)
1(·;θ),h(l)
2(·;θ), for1≤l≤L, respectively
39as in Definition C.1 and with shared weights θ. Then (f1(·;θ), f2(·;θ))converges in distribution
to a multidimensional Gaussian process (X(L)
j, Y(L)
j)j=1,...,n Las(nl)1≤l≤L−1→ ∞ weakly for
any fixed countable input set (zi)∞
i=1. The Gaussian process is defined by X(L)
jiid∼ N
0,Σ(L)
1
,
Y(L)
jiid∼ N
0,Σ(L)
2
, where we have for x, x′∈Rn0
Σ(1)
1(x, x′) = Σ(1)
2(x, x′) =σ2
w
n0⟨x, x′⟩+σ2
b (S56)
Σ(L)
k(x, x′) =σ2
wEg∼N 
0,Σ(L−1)
k[σk(g(x))σk(g(x′))] + σ2
bforL≥2, k∈ {1,2}.(S57)
Furthermore, X(L)
iandY(L)
jare independent if i̸=jand
Eh
X(L)
i(x)Y(L)
i(x′)i
=(
σ2
w
n0⟨x, x′⟩+σ2
b= Σ(1)
1(x, x′) =:Σ(1)
1,2(x, x′)forL= 1,
σ2
wE[σ1(Z1)σ2(Z2)] +σ2
b=:Σ(L)
1,2(x, x′) forL≥2,(S58)
where (Z1, Z2)∼ N
0,
Σ(L−1)
1(x,x) Σ(L−1)
1,2(x,x′)
Σ(L−1)
1,2(x,x′) Σ(L−1)
2(x′,x′)
.
Proof. We write h(l)(x) = h(l)(x;θ). We prove the theorem by induction, as in the proof of
Proposition 1 of Jacot et al. [2018], but expand on the technical details.
L=1.By definition, we have
h(1)
1(x) =h(1)
2(x) =σw√n0W(1)x+σbb(1).
This implies that h(1)
k1,i(x;θ)andh(1)
k2,j(x′;θ), thei-th and j-th component of h(1)
k1(x;θ)andh(1)
k2(x′;θ)
respectively, are independent for any k1, k2∈ {1,2},x, x′∈Rn0andi̸=j. To prove that
(h(1)
1(·;θ), h(1)
2(·;θ))is a Gaussian process, it is thus sufficient to show that vectors of the
form (h(1)
1,i(x1)... h(1)
1,i(xn)h(1)
2,i(x1)... h(1)
2,i(xn))⊺have a multivariate Gaussian distribution for any
x1, . . . , x n∈(zi)∞
i=1. It holds
h(1)
1,i=σw√n0W(1)
i·x+σbb(1)
i= (σwx⊺/n0σb) 
W(1)
i·⊺
b(1)
i!
,
and therefore

h(1)
1,i(x1). . . h(1)
1,i(xn)h(1)
2,i(x1). . . h(1)
2,i(xn)⊺
=
σwX⊺/n0σb 1d
σwX⊺/n0σb 1d 
W(1)
i·⊺
b(1)
i!
,
with 1da column vector of ones with length d. Now since
 
W(1)
i·⊺
b(1)
i!
∼ N(0,In0+1),
it holds
(h(1)
1,i(x1)... h(1)
1,i(xn)h(1)
2,i(x1)... h(1)
2,i(xn))⊺∼ N
0,
σwX⊺/n0σb 1d
σwX⊺/n0σb 1d
σwX⊺/n0σb 1d
σwX⊺/n0σb 1d⊺
.
Therefore, the vector has a multivariate Gaussian distribution with the covariances required for
Equation (S56) and the first case of Equations (S58).
L→L+1.We assume that the convergence holds for depth L. This means that there exists some
r∈ R Lsuch that, for given constant width n0, any width nL, and widths nl=rl(m),1≤l < L , it
holds 
h(L)
1(·), h(L)
2(·)D− − − − →
m→∞
X(L)
j, X(L)
j
j=1,...,n L.
To be precise, there is no such rin the case L= 1→L+ 1. However, this only makes the proof
simpler and one can still follow the same steps as for L≥2.
40By the continuous mapping theorem [Billingsley, 1999, Theorem 2.7] it holds
σ1
h(L)
1(·)
, σ2
h(L)
2(·)D− − − − →
m→∞
σ1
X(L)
j
, σ2
Y(L)
j
j=1,...,n L. (S59)
The theorem is applicable despite the finitely many jump points, since (X(L), Y(L))assumes the
values of the jump points with zero probability.
We now need to find an increasing width function rL:N→Nsuch that, if we additionally set
nL=rL(m), it holds for any fixed nL+1
h(L+1)
1 (·), h(L+1)
2 (·)D− − − − →
m→∞
X(L)
j, X(L)
j
j=1,...,n L+1.
Note that by Remark C.5 we consider the product σ-algebra. Therefore, to show convergence in
distribution for the whole process, it is sufficient to show convergence in distribution for the marginal
distributions. By definition, we have for k∈ {1,2}that
h(L+1)
k(x) =σw√nLW(L+1)σk 
h(L)
k(x)
+σbb(L+1)=σw√nLnLX
i=1σk 
h(L)
k,i(x)
W(L+1)
·i +σbb(L).
The marginal vector for points x1, . . . , x nas before can thus be written as

h(L+1)
1 (x1)
...
h(L+1)
1 (xn)
h(L+1)
2 (x1)
...
h(L+1)
2 (xn)
=σw√nLnLX
i=1
σ1 
h(L)
1,i(x1)
InL+1
...
σ1 
h(L)
1,i(xn)
InL+1
σ2 
h(L)
2,i(x1)
InL+1
...
σ2 
h(L)
2,i(xn)
InL+1
W(L+1)
·i +σb
b(L+1)
...
b(L+1)
b(L+1)
...
b(L+1)
.
With the same arguments as before and using the continuous mapping theorem in combination with
Lemma E.2 (iv) and the independence of W(L+1), it holds


σ1 
h(L)
1,i(x1)
InL+1
...
σ1 
h(L)
1,i(xn)
InL+1
σ2 
h(L)
2,i(x1)
InL+1
...
σ2 
h(L)
2,i(xn)
InL+1
W(L+1)
·i
nL
i=1D− − − − →
m→∞

σ1 
X(L)
i(x1)
InL+1
...
σ1 
X(L)
i(xn)
InL+1
σ2 
Y(L)
i(x1)
InL+1
...
σ2 
Y(L)
i(xn)
InL+1
W(L+1)
·i
nL
i=1.
Since (Xi, Yi)and(Xj, Yj)are independent for i̸=j, the conditions of Lemma E.1 are satisfied.
Now, there exists k:N→Nsuch that k(m)→ ∞ monotonically as m→ ∞ and, when setting
nL:=rL(m):=k(m), it holds
h(L+1)
1 (x1)
...
h(L+1)
1 (xn)
h(L+1)
2 (x1)
...
h(L+1)
2 (xn)
D− − − − →
m→∞σw
R(L+1)
x1
...
R(L+1)
xn
S(L+1)
x1
...
S(L+1)
xn
+σb
b(L+1)
...
b(L+1)
b(L+1)
...
b(L+1)
(⋆)=
X(L+1)(x1)
...
X(L+1)(xn)
Y(L+1)(x1)
...
Y(L+1)(xn)
,
for a Gaussian random variable 
R(L+1)
x1. . . , R(L+1)
xn, S(L+1)
x1, . . . , S(L+1)
xn
∈R2·nL+1·dwith

R(L+1)
x1
...
R(L+1)
xn
S(L+1)
x1
...
S(L+1)
xn
∼
σ1 
X(L)
1(x1)
InL+1
...
σ1 
X(L)
1(xn)
InL+1
σ2 
Y(L)
1(x1)
InL+1
...
σ2 
Y(L)
1(xn)
InL+1
W(L+1)
·1.
41Before considering the covariances, we want to comment on rL. First, note that this sequence may
not initially be strictly increasing, but this can be circumvented by considering a strictly increasing
subsequence. Second, rLcould theoretically depend on X. However, this can be resolved by not
evaluating the pair (h(L)
1, h(L)
2)at certain data points, but doing the same calculation as above for
(h(L)
1, h(L)
2). The starting point for this is Equation (S59) . To apply Lemma E.1, note additionally
that(h(L)
1, h(L)
2)∈N∞
i=1R2·nL, which is a separable Hilbert space because we are considering
a countable input set. Above, we worked with the marginal distribution because this makes the
following calculation of the covariances easier. Finally, the choice of rLshould be independent of
nL+1. This follows from the independence of W(L+1)
ji andW(L+1)
j′i forj̸=j′.
To verify Equation (⋆), we check the covariances of the random vector. They are given by
Covh
R(L+1)
xi, R(L+1)
xji
=Eh
σ1
X(L)
1(xi)
W(L+1)
·1
W(L+1)
·1⊺
σ1
X(L)
1(xj)i
=Eh
σ1
X(L)
1(xi)
Eh
W(L+1)
·1
W(L+1)
·1⊺X(L)
1(xi), X(L)
1(xj)i
σ1
X(L)
1(xj)i
=Eh
σ1(X(L)
1(xi)) InL+1σ1
X(L)
1(xj)i
=Eh
σ1
X(L)
1(xi)
σ1
X(L)
1(xj)i
InL+1.
Similarly, we get that
Covh
S(L+1)
xi, S(L+1)
xji
=Eh
σ2
Y(L)
1(xi)
σ2
Y(L)
1(xj)i
InL+1,
and together this implies using the independence of biases b(L+1)and weight matrices W(L+1)
Covh
σwR(L+1)
xi,k+σbb(L+1)
k, σwR(L+1)
xj,l+σbb(L+1)
li
=δkl
σ2
wEh
σ1
X(L)
1(xi)
σ1
X(L)
1(xj)i
+σ2
b
= Covh
X(L+1)
k(xi), X(L+1)
l(xj)i
,
Covh
σwS(L+1)
xi,k+σbb(L+1)
k, σwS(L+1)
xj,l+σbb(L+1)
li
=δkl
σ2
wEh
σ2
Y(L)
1(xi)
σ2
Y(L)
1(xj)i
+σ2
b
= Covh
Y(L+1)
k(xi), Y(L+1)
l(xj)i
.
We therefore proved Equation (S57). For the second case of (S58), we see that
Covh
R(L+1)
xi, S(L+1)
xji
=Eh
σ1
X(L)
1(xi)
W(L+1)
·1
W(L+1)
·1⊺
σ2
Y(L)
1(xj)i
=Eh
σ1
X(L)
1(xi)
Eh
W(L+1)
·1
W(L+1)
·1⊺X(L)
1(xi), Y(L)
1(xj)i
σ2
Y(L)
1(xj)i
=Eh
σ1
X(L)
1(xi)
σ2
Y(L)
1(xj)i
InL+1,
with, by induction hypothesis,

X(L)
1(xi), Y(L)
1(xj)
∼ N 
0, 
Σ(L)
1(xi, xi) Σ(L)
1,2(xi, xj)
Σ(L)
1,2(xi, xj) Σ(L)
2(xj, xj)!!
.
This finished the proof, since it now holds
Covh
σwR(L+1)
xi,k+σbb(L+1)
k, σwS(L+1)
xj,l+σbb(L+1)
li
=δkl
σ2
wEh
σ1
X(L)
1(xi)
σ2
Y(L)
1(xj)i
+σ2
b
= Covh
X(L+1)
k(xi), Y(L+1)
l(xj)i
,
Remark E.2. In the preceding proof, we checked the marginal distributions of arbitrary size in order
to give a complete proof of the convergence to a Gaussian process. However, the covariances can be
derived by considering only a pair of data points (x1, x2), which drastically simplifies the notation.
Also, we only need the distributions of pairs for the next theorems.
42Theorem E.4 (Generalized version of Theorem 1 by Jacot et al. [2018]) .For activation functions σ1,
σ2and so-called surrogate derivatives ˜σ1,˜σ2such that σ1, σ2,˜σ1, and ˜σ2are continuous except for
finitely many jump points with property (S50), let f1(·;θ)andf2(·;θ)be network functions with
hidden layers h(l)
1(·;θ),h(l)
2(·;θ),1≤l≤L, respectively as in Definition C.1 with shared weights
θ. Denote the empirical generalized neural tangent kernel
ˆI(L)(x, x′) =J(L),σ1,˜σ1(x;θ)J(L),σ2,˜σ2(x′;θ)⊺forx, x′∈Rn0,
as in Definition C.5. Then, for any x, x′∈Rn0and1≤i, j≤nL, it holds
ˆI(L)
ij(x, x′)P− →δijI(L)(x, x′),
asn1, . . . , n L−1→ ∞ weakly. We call I(L)the analytic generalized neural tangent kernel, which is
recursively given by
I(1)(x, x′) = Σ(1)
1,2(x, x′) (S60)
I(L)(x, x′) = Σ(L)
1,2(x, x′) +I(L−1)(x, x′)·˜Σ(L)
1,2(x, x′)forL≥2, (S61)
withΣ(L)
1,2as in Theorem E.3 and
˜Σ(L)
1,2(x, x′) =σ2
wE[˜σ1(Z1) ˜σ2(Z2)]forL≥2,
where (Z1, Z2)∼ N
0,
Σ(L−1)
1(x,x) Σ(L−1)
1,2(x,x′)
Σ(L−1)
1,2(x,x′) Σ(L−1)
2(x′,x′)
.
Proof. We prove the theorem by induction over L, as in the proof of Theorem 1 by Jacot et al. [2018].
We denote J(L),k(z) =J(L),σk,˜σk(z;θ)fork∈ {1,2}, z∈Rn0.
L=1.For1≤i, j≤n1it holds
ˆI(1)
ij(x, x′) =J(1),1
i·(x)J(1),2
j·(x′)⊺=X
θ′∈θ(1)J(1),1
i θ′(x)J(1),2
j θ′(x′)
(S12)=X
1≤k≤n1
1≤l≤n0δkiσw√n0xlδkjσw√n0x′
l+X
1≤k≤n1δkiσbδkjσb
=σ2
w
n0δijX
1≤k≤n0xlx′
l+σ2
bδij=δijσ2
w
n0⟨x, x′⟩+σ2
b
=δijI(1)(x, x′).
This proves Equation (S60).
L→L+1.Now we assume that the statement is true for Land need to prove it for L+ 1. Instead
of considering an explicit r∈ R Las in the proof of Theorem E.3 and taking m→ ∞ , we will just
write “ n1, . . . , n L−1→ ∞ weakly”.
In the induction step, we would like to use Theorem E.3. However, it is not obvious that this is
possible. In the setting of Definition C.7, let S1andS2be two statements that hold weakly. In our
setting, these are the induction hypothesis and Theorem E.3 for depth L. Then there exist s, t∈ R L
such that S1(s)andS2(t)are true. It is not clear that there exists some r∈ R Lsuch that S1(r)and
S2(r)are true. It would be natural to define rbyrl(m):= max {sl(m), tl(m)}for all 1≤l < L
andm∈N, but it is still unclear that this implies S1(r)andS2(r). Instead, one can consider the
combined statement “ S1andS2” as a new statement S. In our case, for any depth L, we would need
to find a r∈ R Lsuch that the statements in Theorem E.3 and Theorem E.4 are both true, which can
be done. Since we used the first part of Lemma E.1 to prove Theorem E.3 and will use the second part
of Lemma E.1 for this proof, we would have to define rL(m):= min {k(m), k′(m)}. For simplicity,
we will assume that the r∈ R Lwe get by the induction hypothesis also gives us the convergence
statement of Theorem E.3.
43Using the definition of the generalized NTK and the quasi-Jacobian matrices, we obtain for 1≤
i, j≤nL+1
I(L+1)
ij (x, x′) =J(L+1),1
i· (x)J(L+1),2
j· (x′)⊺=X
θ′∈θ(1:L+1)J(1),1
i θ′(x)J(1),2
j θ′(x′)
=X
1≤k≤nL+1
1≤l≤nLδkiσw√nLσ1
h(L)
1,l(x)
δkjσw√nLσ2
h(L)
2,l(x′)
+X
1≤k≤nL+1δkiσbδkjσb
+X
θ′∈θ(1:L)σ2
w
nL"nLX
m=1W(L+1)
i,m ˜σ1
h(L)
1,m(x)
J(L),1
m θ′(x)#"nLX
r=1W(L+1)
i,r ˜σ2
h(L)
2,r(x′)
J(L),2
r θ′(x′)#
=δij 
σ2
w
nLnLX
l=1σ1
h(L)
1,l(x)
σ2
h(L)
2,l(x′)
+σ2
b!
(S62)
+σ2
w
nLnLX
m,r=1W(L+1)
i,m W(L+1)
j,r ˜σ1
h(L)
1,m(x)
˜σ2
h(L)
2,r(x′)
·X
θ′∈θ(1:L)J(L),1
m θ′(x)J(L),2
r θ′(x′)(S63)
We want to apply the second part of Lemma E.1. We will consider the terms (S62) and (S63)
separately. First note that

σ1
h(L)
1,l(x)
, σ2
h(L)
2,l(x′)D− →
σ1
X(L)
l(x)
, σ2
Y(L)
l(x′)
, (S64)
asn1, . . . , n L−1→ ∞ weakly by Theorem E.3 and the continuous mapping theorem with

X(L)
l(x), Y(L)
l(x′)iid∼ N 
0, 
Σ(L)
1,1(x, x) Σ(L)
1,2(x, x′)
Σ(L)
1,2(x, x′) Σ(L)
2,2(x′, x′)!!
.
Again, we used that the values of the jump points are assumed with zero probability. Thus, again by
the continuous mapping theorem and by the second part of Lemma E.1, it holds
σ2
w
nLnLX
l=1σ1
h(L)
1,l(x)
σ2
h(L)
2,l(x′)
+σ2
bD− − − − − − − − →n1,...,n L→∞
weaklyEh
σ1
X(L)
1(x)
σ2
Y(L)
1(x′)i
+σ2
b.
Here, as in the proof of Theorem E.3, nL→ ∞ is given by nL:=rL(m):=k′(m), which is in
turn is given by Lemma E.1. Note also that the limit is a constant, which implies convergence in
probability according to Lemma E.2 (ii). In conclusion, we obtain
δij 
σ2
w
nLnLX
l=1σ1
h(L)
1,l(x)
σ2
h(L)
2,l(x′)
+σ2
b!
P− − − − − − − − →n1,...,n L→∞
weaklyδijΣ(L+1)
1,2(x, x′). (S65)
For term (S63), we can apply the induction hypothesis to obtain
X
θ′∈θ(1:L)J(L),1
θ′,m(x)J(L),2
θ′,r(y) =ˆI(L)
m,r(x, y)P− − − − − − − − − − →n1,...,n L−1→∞
weaklyδmrI(L)(x, y). (S66)
Also, we can again use the convergence given by (S64), but with σ1, σ2replaced by ˜σ1,˜σ2respectively.
This gives using Lemma E.2 (iv)

˜σ1
h(L)
1(·)
,˜σ1
h(L)
2(·)
, W(L+1)D− − − − − − − →n1,...,n L−1
weakly
˜σ1
X(L)
,˜σ1
Y(L)
, W(L+1)
.
Together with (S66) and Lemma E.2 (iii) this implies

˜σ1
h(L)
1(·)
,˜σ1
h(L)
2(·)
, W(L+1),ˆI(L)
mr(x, x′)
D− − − − − − − →n1,...,n L−1
weakly
˜σ1
X(L)
,˜σ1
Y(L)
, W(L+1), δmrI(L)(x, x′)
.
44For the summands in (S63) we therefore have by the continuous mapping theorem
nLX
r=1W(L+1)
i,m W(L+1)
j,r ˜σ1
h(L)
1,m(x)
˜σ2
h(L)
2,r(x′)
·X
θ′∈θ(1:L)J(L),1
θ′,m(x)J(L),2
θ′,r(x′)
D− − − − − − − →n1,...,n L−1
weaklynLX
r=1W(L+1)
i,m W(L+1)
j,r ˜σ1
X(L)
m(x)
˜σ2
Y(L)
r(x′)
·δmrI(L)(x, x′)
=W(L+1)
i,m W(L+1)
j,m ˜σ1
X(L)
m(x)
˜σ2
Y(L)
m(x′)
·I(L)(x, x′)
These terms are independent for different 1≤m≤nL. Therefore, the second part of Lemma E.1
can be applied as before. This yields
σ2
w
nLnLX
r=1W(L+1)
i,m W(L+1)
j,r ˜σ1
h(L)
1,m(x)
˜σ2
h(L)
2,r(x′)
·X
θ′∈θ(1:L)J(L),1
θ′,m(x)J(L),2
θ′,r(x′)
D− − − − − − − − →n1,...,n L→∞
weaklyI(L)(x, x′)·σ2
wEh
W(L+1)
i,1W(L+1)
j,1˜σ1
XL)
1(x)
˜σ2
Y(L)
1(x′)i
=I(L)(x, x′)·δijσ2
wEh
˜σ1
XL)
1(x)
˜σ2
Y(L)
1(x′)i
=δijI(L)(x, x′)·˜Σ(L)
1,2(x, x′).
Together with (S65) this yields Equation (S61) and concludes the proof.
The two theorems above are complemented by the convergence of the generalized NTK in the infinite-
width limit during training, which we will prove below. The theorem is a generalization of Theorem
G.2 from Lee et al. [2019]. There, the convergence of the NTK at initialization as (nl)L−1
l=1∝∼nis
assumed. More precisely, they refer to a result of Yang [2019a]. In consequence, we have to assume
the same for the next theorem. The proof of Yang [2019a] could also be generalizable to our case.
Alternatively, one could also try to generalize the statement of Jacot et al. [2018] on stability during
training in the weak infinite-width limit.
Theorem E.5 (Based on Theorem G.2 from Lee et al. [2019] ) .Letσbe a Lipschitz continuous
and differentiable activation function. Let the derivative of the activation function ˙σand a so-
called surrogate derivative ˜σbe Lipschitz continuous and bounded. Let ft(·;θ)be the corresponding
network function with depth Linitialized as in Definition C.1 and trained with MSE loss and surrogate
gradient learning, i.e., according to Equation (S49) with surrogate derivative ˜σ. The hidden layers
are denoted by h(l)
l(·;θ), for1≤l < L , as in Definition C.1. Assume that the generalized NTK
converges in probability to the analytic generalized NTK of Theorem E.4 as (nl)L−1
l=1∝∼n,

J(L),σ,˙σ
J(L),σ,˜σ⊺
=ˆI(L)P− − − − →
n→∞I(L)⊗InL.
Furthermore, assume that the smallest and largest eigenvalue of the symmetrization of I(L)(X,X),
S(L):=1
2
I(L)(X,X) +I(L)(X,X)⊺
,
are given by 0< λmin≤λmax<∞and that the learning rate is given by η >0. Then, for any
δ >0there exist R > 0, N∈NandK > 1such that for every n≥N, the following holds with
probability at least 1−δover random initialization
sup
t∈[0,∞)ˆI(L)(X,X)−I(L)(X,X)
F≤6K3R
λminn−1
2,
where ∥·∥Fdenotes the Frobenius norm.
Proof. We follow the proofs in Chapter G of Lee et al. [2019]. The analogous statement is given by
Theorem G.2 of Lee et al. [2019]. Since we are considering the infinite-width limit (nl)L−1
l=1∝∼n, we
will assume that the width of the hidden layers are given by n∈N. The more general case can easily
be proved using the same steps. Recall that
d
dtf(x;θt) =−ηˆI(L)(x,X;θt)∇f(X;θt)L(f(X;θt);Y). (S49)
45The loss function is the MSE loss by assumption. This yields
∇f(X;θt)L(f(X;θt);Y) =∇f(X;θt)1
2∥f(X;θt)− Y∥2
2=f(X;θt)− Y=:g(θt).
The change of weights over time is hence given by
d
dtθt(S48)=−η J(L),σ,˜σ(X;θt)⊺g(θt)
By Lemma E.6 below, for any δ1>0there exists a K > 0such that for any C > 0it holds with
probability at least 1−δ1J(L),σ,˜σ(X;θt)⊺
F≤K for all θt∈B(θ0, C).
Together, this implies
d
dt∥θt−θ0∥2(⋆)
≤d
dtθt
2=ηJ(L),σ,˜σ(X;θt)⊺g(θt)
2
≤ηJ(L),σ,˜σ(X;θt)⊺
F∥g(θt)∥2≤η K∥g(θt)∥2, (S67)
for all θt∈B(θ0, C)with probability at least 1−δ1. Inequality (⋆)is a consequence of the
Cauchy-Schwarz inequality.
To estimate the term ∥g(θt)∥2in Inequality (S67), we use Grönwall’s inequality in the differential
form, e.g., Lemma 1.1.1 from Qin [2016]). First, note that as a consequence of the proof of Lemma
E.6, for all δ2>0there exist R0>0andn0∈Nsuch that with probability at least 1−δ2it holds
for all n≥n0
∥g(θ0)∥2≤R0. (S68)
We now set C:= 3KR0/λmin. Next, observe that
d
dtg(θt) =d
dtf(X;θt)(S49)=−η J(L),σ,˙σ(X;θt)J(L),σ,˜σ(X;θt)⊺g(θt)
=⇒d
dt∥g(θt)∥2
2=d
dt⟨g(θt), g(θt)⟩= 2
g(θt),d
dtg(θt)
=−2ηD
g(θt), J(L),σ,˙σ(X;θt)J(L),σ,˜σ(X;θt)⊺g(θt)E
=−2ηD
g(θt),ˆI(L)
t(X,X)g(θt)E
=−2ηD
g(θt), S(L)
tg(θt)E
(⋆⋆)
≤ − 2η
g(θt),1
3λminIg(θt)
=−2
3ηλmin∥g(θt)∥2
2, (S69)
withS(L)
t:=1
2
ˆI(L)
t(X,X) +ˆI(L)
t(X,X)⊺
. To show (⋆⋆), we will prove that for any 0̸=z∈
RnL·n
z,
S(L)
t−1
3λminI
z
≥0. (S70)
First, note that by assumption for all z̸= 0
D
z,
S(L)−λminI
zE
≥0.
Since the generalized NTK converges at initialization in probability, we can assume that with
probability at least 1−δ3for any δ3>0that
ˆI(L)
0(X,X)−I(L)(X,X)
2≤1
3λmin,
for any n≥n1∈N. For the symmetrizations, this yields
S(L)
0−S(L)
2≤1
2ˆI(L)
0(X,X)−I(L)(X,X)
2+1
2ˆI(L)
0(X,X)⊺−I(L)(X,X)⊺
2
=ˆI(L)
0(X,X)−I(L)(X,X)
2≤1
3λmin. (S71)
46Furthermore, for θt∈B(θ0, C)it holds that
ˆI(L)
0(X,X)−ˆI(L)
t(X,X)
2
=J(L),σ,˙σ(X;θ0)J(L),σ,˜σ(X;θ0)⊺−J(L),σ,˙σ(X;θt)J(L),σ,˜σ(X;θt)⊺
2
≤J(L),σ,˙σ(X;θ0)−J(L),σ,˙σ(X;θt)
2J(L),σ,˜σ(X;θ0)
2
+J(L),σ,˙σ(X;θt)
2J(L),σ,˜σ(X;θ0)−J(L),σ,˜σ(X;θt)
2
≤2(K′)2
√n∥θt−θ0∥2≤6(K′)2KR0√n, (S72)
with probability at least 1−δ1using Lemma E.6 as before. With the same calculations as for
Inequality (S71), this implies for the symmetrizations that
S(L)
0−S(L)
t
2≤ˆI(L)
0(X,X)−ˆI(L)
t(X,X)
2≤6(K′)2KR0√n≤1
3λmin, (S73)
for all n≥N:= max
m0, m1,
λmin
18(K′)2KR02
. Now Inequalities (S70) , (S71) and (S73) give
usD
z, S(L)
tzE
=D
z,
S(L)+S(L)
0−S(L)+S(L)
t−S(L)
0
zE
=D
z, S(L)zE
+D
z,
S(L)
0−S(L)
zE
+D
z,
S(L)
t−S(L)
0
zE
(S70)
≥λmin∥z∥2−D
z,
S(L)
0−S(L)
zE−D
z,
S(L)
t−S(L)
0
zE
(S71)+(S73)
≥ λmin∥z∥2−1
3λmin∥z∥2−1
3λmin∥z∥2=1
3
z,1
3λminIz
,
and thus imply Inequality (S70) . To summarize, it holds with probability at least 1−δ2−δ3for any
n≥Nandθt∈B(θ0, C)that
∥g(θ0)∥2
2(S68)
≤R2
0andd
dt∥g(θt)∥2
2(S69)
≤ −2
3ηλmin∥g(θt)∥2
2.
Grönwall’s inequality now implies
∥g(θt)∥2
2≤e−2
3ηλmint∥g(θ0)∥2
2≤e−2
3ηλmintR2
0.
We can now return to Inequality (S67) to obtain with probability at least 1−δ1−δ2−δ3
d
dt∥θt−θ0∥2(S67)
≤ηK∥g(θt)∥2≤ηKR 0e−1
3ηλmint.
Integrating the inequality on both sides yields for all θt∈B(θ0, C)
∥θt−θ0∥2≤3KR0
λmin
1−e−1
3ηλmint
<3KR0
λmin=C. (S74)
Lett1:= inf{t:∥θt−θ0∥2< C}. Now, if t1<∞, it holds
C= lim
t↑t1∥θt−θ0∥2(S74)
< C.
This is a contradiction, so we conclude that t1=∞. In particular, Inequality (S74) holds for all
t >0. We can now repeat the calculations for Inequality (S72) with the Frobenius norm to finally
obtainˆI(L)
0(X,X)−ˆI(L)
t(X,X)
F≤2K2
√n∥θt−θ0∥2≤6K3R0
λminn−1
2.
Therefore, if we choose δ1=δ2=δ3=1
3δ, the desired inequality holds for any n≥Nwith
probability at least 1−δ.
47Lemma E.6 (Based on Lemma 1 and Lemma 2 from Lee et al. [2019]) .In the setting of Theorem E.5,
for any δ >0there exists a K > 0such that for any C >0, with probability at least 1−δit holds
J(L),σ,ˆσ(X;θ)−J(L),σ,ˆσ(X;˜θ)
F≤K√nθ−˜θ
2and
J(L),σ,ˆσ(X;θ)
F≤K,
for all θ,˜θ∈B(θ0, C)andˆσ∈ {˙σ,˜σ}. Due to the equivalence of matrix norms, the same
inequalities hold for the norm ∥·∥2and some constant K′>0.
Proof. Forˆσ= ˙σand a different but equivalent parameterization of the network parameters [Lee et al.,
2019, Chapter F], the proof can be found in Section G.2 of Lee et al. [2019]. The surrogate derivative
˜σis bounded and Lipschitz continuous. Also, J(L),σ,˜σ(X;θ)andJ(L),σ,˙σ(X;θ) =Jθf(X;θ)share
the same recursive formula. Therefore, the proof, which builds on the so-called Gaussian conditioning
technique [Yang, 2019a, Section E.1], should also hold for the surrogate derivative.
Remark E.3 (The analytic generalized NTK for surrogate gradient learning) .Since in Theorem E.5
we consider only one activation function σinstead of two different ones σ1, σ2as in Theorem E.3
and E.4, it holds
Σ(L)
1,2= Σ(L)
1= Σ(L)
2= Σ(L).
The analytic generalized NTK in the setting of Theorem E.4 is thus given by
I(1)= Σ(1)and I(L+1)= Σ(L+1)+I(L)·˜Σ(L+1)
1,2.
We therefore still obtain an asymmetric kernel due to the contribution of ˜Σ(L)
1,2.
Remark E.4 (Positive definiteness of the generalized NTK) .In Theorem E.5 we require that the
matrix I(L)(X,X)be positive definite. Equivalently, its symmetrization,
S(L)=1
2
I(L)(X,X) +I(L)(X,X)⊺
,
should be positive definite. For applications this can be checked numerically. More generally, it
would be interesting to know whether the symmetric kernel given by
S(L)(x, x′) =1
2
I(L)(x, x′) +I(L)(x′, x)
is positive definite. One could try to approach this question inductively. However, this leads to
problems in the induction step. We want to present a different ansatz, which reduces the question to a
question about ˜Σ(L)
1,2. We use the closed form of the analytic NTK and the symmetry of Σ(l)for all
l∈Nto see that
S(L)(x, x′) =1
2
I(L)(x, x′) +I(L)(x′, x)
=1
2  LX
k=1Σ(k)(x, x′)·L−1Y
l=k˜Σ(l+1)
1,2(x, x′)!
+ LX
k=1Σ(k)(x′, x)·L−1Y
l=k˜Σ(l+1)
1,2(x′, x)!!
=LX
k=1Σ(k)(x, x′)·1
2  L−1Y
l=k˜Σ(l+1)
1,2(x, x′)!
+ L−1Y
l=k˜Σ(l+1)
1,2(x′, x)!!
.
This defines a symmetric positive definite kernel if Σ(L)is positive definite (see Jacot et al. [2018,
Section A.4] for comparison) and the symmetrized kernels,
1
2  L−1Y
l=k˜Σ(l+1)
1,2(x, x′)!
+ L−1Y
l=k˜Σ(l+1)
1,2(x′, x)!!
,
are positive semi-definite for all k= 1, . . . , L −1.
48E.2 The analytic NTK for surrogate gradient learning with sign activation function
We would like to proceed as in Section C.2.1 and replace the empirical generalized NTK in Equation
(S49) , the equation defining surrogate gradient learning, with the analytic generalized NTK obtained
by Theorem E.5. Since the activation functions considered in the above section are Lipschitz
continuous with bounded and Lipschitz continuous derivative, we will again approximate the sign
function and its distributional derivative with the error function as in Section D. The results will not
depend on the approximation of the weak derivative of the sign function. This approach can only
lead to a useful result if the resulting kernel is not singular. We will check this in the following.
We choose activation function erfm(z) = erf( m·z),m∈N, with surrogate derivative ˜σ(z). We
will also consider the special case ˜σ(z) = ˙erf(z). As discussed in Remark E.3 and with the final
results of Section D.1, this immediately yields
Σ(1)
∞(x, y):= lim
m→∞Σ(1)
m(x, y)(S36)=σ2
w
n0⟨x, y⟩+σ2
band
Σ(L+1)
∞ (x, y):= lim
m→∞Σ(L+1)
m (x, y)(S29)=2σ2
w
πarcsin
Σ(L)
∞(x, y)q
Σ(L)
∞(x, x)q
Σ(L)
∞(y, y)
+σ2
b.
(S75)
Here, we have to assume that σ2
b>0orx, y̸= 0to ensure that Σ(1)
∞(x, x),Σ(1)
∞(y, y)̸= 0. This has
already been discussed after Equation (S39).
E.2.1 The derivative of the error function as surrogate derivative
Next, we want to calculate ˜Σ(L)
1,2;∞(x, y):= lim m→∞˜Σ(L)
1,2;m(x, y). We will first consider the case
˜σ=˙erf, for which we can use the already established tools. In particular, we will discuss the
differences to the results of Section D.
It holds
˜Σ(L)
1,2;m(x, y) =σ2
wE[˙erfm(Zm
1)˙erf(Zm
2)]forL≥2,
where
(Zm
1, Zm
2)∼ N
0,
Σ(L−1)
1;m(x,x) Σ(L−1)
1,2;m(x,y)
Σ(L−1)
1,2;m(x,y) Σ(L−1)
2;m(y,y)
=N
0,
Σ(L−1)
m (x,x) Σ(L−1)
m (x,y)
Σ(L−1)
m (x,y) Σ(L−1)
m (y,y)
=N
0,Σ(L−1)
m;x,y
,
again using Remark E.3 to simplify the covariance matrix. The notation Σ(L−1)
1,2;m, which comes from
Theorem E.3 in combination with the scaling variable mof the activation function, should not be
confused with Σ(L−1)
m;x,y, which is a shorthand notation for the Gram matrix Σ(L−1)
m ({x, y},{x, y}).
49We denote e1= (1
0),e2= (0
1)andU=
Zm
1
Zm
2
. This yields
˜Σ(L)
1,2;m(x, y) =σ2
wE[˙erfm(Zm
1)˙erf(Zm
2)] =σ2
wEh
m˙erf ((m·e1)⊺U)˙erf (e⊺
2U)i
(⋆)=4σ2
w
πm
1 +m2·2e⊺
1Σ(L−1)
m;x,y)e1
1 + 2 e⊺
2Σ(L−1)
m;x,y)e2
−
2m·e⊺
1Σ(L−1)
m;x,ye22−1
2
=2σ2
w
π1
4m2
1 + 2 m2Σ(L−1)
m (x, x)
1 + 2Σ(L−1)
m (y, y)
−4m2Σ(L−1)
m (x, y)2−1
2
=2σ2
w
π1
2m2+ Σ(L−1)
m (x, x)1
2+ Σ(L−1)
m (y, y)
−Σ(L−1)
m (x, y)2−1
2
m→∞− − − − →2σ2
w
π
Σ(L−1)
∞ (x, x)1
2+ Σ(L−1)
∞ (y, y)
−Σ(L−1)
∞ (x, y)2−1
2
=2σ2
w
πΣ(L−1)
∞;x,y+1
2Σ(L−1)
∞ (x, x)−1
2
=

2σ2
w
πΣ(L−1)
∞;x,y+σ2
w∥x∥2
2n0+σ2
b
2−1
2forL= 2,
2σ2
w
πΣ(L−1)
∞;x,y+σ2
w+σ2
b
2−1
2forL≥3.
=:˜Σ(L)
1,2;∞(x, y), (S76)
using Lemma D.1 in Equation (⋆). For the penultimate equality we used Equation (S36) and Equation
(S37). Compared to Equation (S41),
˙Σ(L)
m(x, y)m→∞− − − − →2σ2
w
π
Σ(L−1)
∞ (x, x)·Σ(L−1)
∞ (y, y)−Σ(L−1)
∞ (x, y)2
=2σ2
w
πΣ(L−1)
∞;x,y−1
2,
(S41)
which in fact holds for both L= 2andL≥3, an additional term appeared in Equation (S76) . It
always holds σ2
w+σ2
b>0and it holds σ2
w∥x∥2/n0+σ2
b>0ifσ2
b>0orx̸= 0. As discussed
earlier, we always assume that x̸= 0is satisfied. It follows that this asymmetric NTK is not singular,
since
˜Σ(L)
1,2;∞(x, x) =2σ2
w
πΣ(L−1)
∞;x,x+1
2Σ(L−1)
∞ (x, x)−1
2
=2σ2
w
π
0 +1
2Σ(L−1)
∞ (x, x)−1
2
=√
22σ2
w
π
Σ(L−1)
∞ (x, x)−1
2∈R. (S77)
Note that this is reminiscent of the constant factor depending on xin the asymptotics of ˙Σ(L)
m(x, x)
asm→ ∞ , given by (S42) and (S43),
˙Σ(L)
m(x, x)∼2σ2
w
πm
Σ(L−1)
∞ (x, x)−1
2.
According to Remark E.3 it holds
I(L)
m(x, y) = Σ(L)
m(x, y) +I(L−1)
m (x, y)·˜Σ1,2;m(x, y).
Since Σ(L)
m(x, y)and˜Σ(L)
1,2;mare continuous functions of the entries of the matrix Σ(L−1)
m;x,y, it follows
by induction using Equations (S75) and(S76) that the limit of the analytic NTK is well defined for
m→ ∞ . We can write
I(L)
∞(x, y) = lim
m→∞I(L)
m(x, y).
Thus, by approximating the sign function with error functions, we found that the analytic NTK for
surrogate gradient learning with sign function and error function as surrogate derivative is well-
defined and non-singular as a kernel on Rn0×Rn0. Furthermore, comparing this NTK with the
50NTK we derived in Section D, the term Σ(L)
∞does not change. This is a direct consequence of
not replacing the activation function with a surrogate activation function. In this sense, we inherit
more properties with this approach than by replacing not only the derivative but the entire activation
function including its derivative with a surrogate. Comparing Equation (S41) with Equation (S76),
we see that ˜Σ(L)
1,2;∞(x, y)can be obtained from ˙Σ(L)
∞(x, y)by adding a regularizing term depending
onx,Σ(L−1)
∞ (x, x)/2.
E.2.2 General surrogate derivative
Now we turn to the general case with a general surrogate derivative ˜σ. Similar to before, for
m∈N∪ {∞} we denote
(Zm
1, Zm
2)∼ N
0,
Σ(L−1)
1;m(x,x) Σ(L−1)
1,2;m(x,y)
Σ(L−1)
1,2;m(x,y) Σ(L−1)
2;m(y,y)
=N
0,
Σ(L−1)
m (x,x) Σ(L−1)
m (x,y)
Σ(L−1)
m (x,y) Σ(L−1)
m (y,y)
=N
0,Σ(L−1)
m;x,y
.
With u= (z1z2)and for invertible Σ = Σ(L−1)
∞;x,y, it holds for L≥2
˜Σ(L)
1,2;∞(x, y) = lim
m→∞˜Σ(L)
1,2;m(x, y) = lim
m→∞σ2
wEh
˙erfm(Zm
1) ˜σ(Zm
2)i
(⋆)=σ2
wE[2δ0(Z∞
1) ˜σ(Z∞
2)] = 2 σ2
wZ
R21
2π|Σ|−1
2δ0(z1)·˜σ(z2)·e−1
2u⊺Σ−1udu
=σ2
wr
2
πΣ−1
2
1,1Z
R1√
2πs
Σ1,1
|Σ|·˜σ(z2)·e−1
2Σ1,1
|Σ|z2
2dz2
=σ2
wr
2
π
Σ(L−1)
∞ (x, x)−1
2EY∼N 
0,Σ(L−1)
∞;x,y/Σ(L−1)
∞ (x,x)[˜σ(Y)], (S78)
where Equation (⋆)seems natural, but requires further reasoning. We will prove the above rigorously
in Lemma E.7. For now, we assume that the equality holds. Since ˜σis bounded and continuous, this
yields
lim
x→y˜Σ(L)
1,2;∞(x, y)
= limΣ(L−1)
∞;x,y→0σ2
wr
2
π
Σ(L−1)
∞ (x, x)−1
2EY∼N
0,Σ(L−1)
∞;x,y/Σ(L−1)
∞ (x,x)[˜σ(Y)]
=σ2
wr
2
π
Σ(L−1)
∞ (x, x)−1
2˜σ(0).
This agrees with the fact that
˜Σ(L)
1,2;∞(x, x) = lim
m→∞σ2
wEh
˙erfm(Zm
1) ˜σ(Zm
1)i
=σ2
wE[2δ0(Z∞
1) ˜σ(Z∞
1)] (S79)
=2σ2
w1√
2π
Σ(L−1)
∞ (x, x)−1
2˜σ(0) = σ2
wr
2
π
Σ(L−1)
∞ (x, x)−1
2˜σ(0),
where we again assumed an equality very similar to Equation (⋆). It is easy to check that Equations
(S76) and (S77) can be recovered by inserting ˜σ=˙erfinto the derived formulas.
We now prove the missing part of Equation (S78).
Lemma E.7. Let˜σbe a bounded and continuous function and ((Zm
1, Zm
2))m∈Na sequence of
random variables, (Zm
1, Zm
2)∼ N(0,Σm). If the covariance matrices are invertible and converge
to an invertible matrix, Σm→Σ∞∈R2×2asm→ ∞ , then it holds
lim
m→∞Eh
˙erfm(Zm
1) ˜σ(Zm
2)i
=r
2
π 
Σ∞
1,1−1
2EY∼N 
0,Σ∞/Σ∞
1,1[˜σ(Y)]. (S80)
51IfZm
1=Zm
2for all m∈Nso that the covariance matrices are given by Σm=
Σm
1Σm
1
Σm
1Σm
1
, and if
Σm
1→Σ∞
1̸= 0asm→ ∞ , then it holds
lim
m→∞Eh
˙erfm(Zm
1) ˜σ(Zm
2)i
=r
2
π(Σ∞
1)−1
2˜σ(0). (S81)
Proof. We begin with the case of invertible covariance matrices. Again denoting u= (z1z2), it holds
by assumption
Eh
˙erfm(Zm
1) ˜σ(Zm
2)i
=Z
R21
2πΣm−1
2·˜σ(z2)·2m√πe−m2z2
1·e−1
2u⊺(Σm)−1udu
=Z
R21
2πΣm−1
2·˜σ(z2)·2√π·e−1
2z1/m
z2⊺(Σm)−1z1/m
z2
+2u⊺e1e⊺
1u
du
=Z
R21
2πΣm−1
2·˜σ(z2)·2√π·e−1
2u⊺Bmudu, (S82)
for
Bm:=1
|Σm|Σm
2,2/m2−Σm
1,2/m
−Σm
1,2/m Σm
1,1
+
2 0
0 0
m→∞− − − − →
2 0
0 Σ∞
1,1/|Σ∞|
=:B∞.(S83)
The determinant of Bmis given by
|Bm|=Σm
2,2
m2|Σm|+ 2Σm
1,1
|Σm|− 
−Σm
1,22
m2|Σm|2=1
m2|Σm|+ 2Σm
1,1
|Σm|.
This now yields
(S82) =2√π|Σm|−1
2(Bm)−11
2Z
R21
2π(Bm)−1−1
2·˜σ(z2)·e−1
2u⊺Bmudu
=2√π|Σm|−1
2|Bm|−1
2E(Ym
1,Ym
2)∼N 
0,(Bm)−1[˜σ(Ym
2)].
The continuity of matrix inversion implies 
Bm−1→ 
B∞−1asm→ ∞ . The characteristic
functions of finite-dimensional Gaussian random variables are fully defined by their means and
covariance matrices. Thus, the convergence of the covariance matrices implies convergence of the
characteristic functions, which in turn implies convergence in distribution. Since ˜σis continuous and
bounded and the determinant is continuous, we obtain
2√π|Σm|−1
2|Bm|−1
2E(Ym
1,Ym
2)∼N 
0,(Bm)−1[˜σ(Ym
2)]
m→∞− − − − →2√π|Σ∞|−1
2|B∞|−1
2E(Y∞
1,Y∞
2)∼N 
0,(B∞)−1[˜σ(Y∞
2)]
(S83)=2√π|Σ∞|−1
22Σ∞
1,1
|Σ∞|−1
2
EY∼N 
0,|Σ∞|/Σ∞
1,1[˜σ(Y)]
=r
2
π 
Σ∞
1,1−1
2EY∼N 
0,|Σ∞|/Σ∞
1,1[˜σ(Y)].
52This proves Equation (S80). In the case Zm
1=Zm
2, we have
Eh
˙erfm(Zm
1) ˜σ(Zm
2)i
=EY∼N(0,Σm
1)[˙erfm(Y) ˜σ(Y)]
=Z
R1√
2π1p
Σm
12√πm·e−m2y2·˜σ(y)·e−1
2y2
Σm
1dy
=Z
R1√
2π1p
Σm
12√πm·˜σ(y)·e−1
2y2(1/Σm
1+2m2)dy
=2√π1p
Σm
1mp
1/Σm
1+ 2m2Z
R1√
2π(1/Σm
1+ 2m2)1
2·˜σ(y)·e−1
2y2(1/Σm
1+2m2)dy
=2√π1p
1/m2+ 2Σm
1EY∼N(0,(1/Σm
1+2m2)−1)[˜σ(Y)]
m→∞− − − − →2√π1p
2Σ∞
1˜σ(0) =r
2
π(Σ∞
1)−1
2˜σ(0),
again using the boundedness and continuity of ˜σin the last line. This proves Equation (S81).
With Σm= Σ(L−1)
m;x,ythe lemma yields
˜Σ(L)
1,2;∞(x, y) = lim
m→∞˜Σ(L)
1,2;m(x, y) = lim
m→∞σ2
wEh
˙erfm(Zm
1) ˜σ(Zm
2)i
=σ2
wr
2
π
Σ(L−1)
∞ (x, x)−1
2EY∼N 
0,Σ(L−1)
∞;x,y/Σ(L−1)
∞ (x,x)[˜σ(Y)].
In conclusion, we found that the analytic NTK for surrogate gradient learning with sign function is
well-defined and non-singular for any bounded and Lipschitz continuous surrogate derivative. As in
the special case of the error function, the term ˜Σ(L)
1,2;∞(x, y)can be expressed in terms of Σ(L−1)
∞ (x, x)
and the determinantΣ(L−1)
∞;x,y.
53NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: As stated in the abstract, we analyze the NTK for activation functions with
jumps and generalize the NTK to surrogate gradient learning. We focus on the sign activation
function in detail, which is also stated in the abstract. The derived theorems, however, are
valid for a general class of activation functions.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of the theorems are discussed throughout the paper and a
general discussion of limitations can be found in Section 4.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
54Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Yes, all definitions, theoretical results, and proofs are given in great detail in
the appendix and have only been slightly shortened for the paper itself. Only for Lemma E.6,
which is used to prove Theorem 2.5, the proof is not given in full detail. Most proofs rely on
similar results for the classical NTK and these results are properly discussed and referenced.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All information needed to reproduce the numerical experiments is given in
Section 3 and Section A.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
55(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Yes, the code is provided in the supplementary material with instructions to
reproduce the experiments and figures.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Since only plain gradient descent and gradient descent with surrogate gradients
(surrogate gradient learning) is used in the paper, no further additional specifications apart
from the ones made in Section 3 and Section A are necessary.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Yes, confidence bands are used for Figure 3 to demonstrate the agreement
between two distributions. Figure 1 and Figure 2 are complemented by Figure B.3 and
Figure B.4 respectively, showing the mean squared errors.
56Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The compute resources used for the numerical simulation are described in
Section A.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We reviewed the NeurIPS Code of Ethics and the paper conforms with it.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
57Answer: [NA]
Justification: The paper is solely theoretical and does not have conceivable societal impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not contain models. The data is only created to illustrate and
examine the theoretical results and has no risk for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The used packages have been properly referenced in Section 3.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
58•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification:
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:
59•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
60