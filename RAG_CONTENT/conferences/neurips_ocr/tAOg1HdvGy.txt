Interpolating Item and User Fairness in Multi-Sided
Recommendations
Qinyi Chen1Jason Cheuk Nam Liang1Negin Golrezaei1Djallel Bouneffouf2
1Massachusetts Institute of Technology2IBM Research
{qinyic,jcnliang,golrezae}@mit.edu djallel.bouneffouf@ibm.com
Abstract
Today’s online platforms heavily lean on algorithmic recommendations for bolster-
ing user engagement and driving revenue. However, these recommendations can
impact multiple stakeholders simultaneously—the platform, items (sellers), and
users (customers)—each with their unique objectives, making it difficult to find
the right middle ground that accommodates all stakeholders. To address this, we
introduce a novel fair recommendation framework, Problem (FAIR ), that flexibly
balances multi-stakeholder interests via a constrained optimization formulation.
We next explore Problem (FAIR )in a dynamic online setting where data uncertainty
further adds complexity, and propose a low-regret algorithm FORM that concurrently
performs real-time learning and fair recommendations, two tasks that are often at
odds. Via both theoretical analysis and a numerical case study on real-world data,
we demonstrate the efficacy of our framework and method in maintaining platform
revenue while ensuring desired levels of fairness for both items and users.
1 Introduction
Online recommendation systems have become essential components of various digital platforms and
marketplaces, playing a critical role in user experience and revenue generation. These platforms
usually operate as multi-sided entities, recommending items (services, products, contents) to users
(consumers). Examples include e-commerce (Amazon, eBay, Etsy), service platforms (Airbnb,
Google Local Services), job portals (LinkedIn, Indeed), streaming services (Netflix, Spotify), and so-
cial media (Facebook, TikTok). On the platform, there are typically two main groups of stakeholders:
items (products, services, contents) and users (those engaging with items), with the platform itself
acting as an independent stakeholder due to its unique objectives.
However, as these platforms play a more vital role in societal and economic realms, fairness concerns
have prompted increasing regulatory action. Notably, the European Union proposed the Digital
Markets Act [ 54], which emphasizes the need for contestable and fair markets in the digital sector,
designating recommendation systems as a key area of focus; the U.S. Algorithmic Accountability Act
[47] requires companies to assess the impacts of their automated decision systems to ensure they are
not biased or discriminatory. It has also become increasingly evident, to the platforms, that upholding
fairness across their stakeholder groups not only strengthens relationships with these stakeholders,
but also enhances long-term platform sustainability [67].
Fairness concerns within digital platforms significantly impact both users and items, manifesting
in various forms of disparities. For users, issues like racial discrimination in Airbnb’s host-guest
matching [ 25] and gender disparity in career ads [ 43] highlight the critical need for enforcing user
fairness. Similarly, items on the platforms also experience unfair allocation of opportunities induced
by algorithmic decisions, such as e-commerce platforms favoring their own private-label products
[19] and social media prioritizing influencers’ contents over others [ 63], indicating a disparity that
affects items (sellers, content creators) alike. These underscore the necessity to address biases and
create a more inclusive environment for all stakeholders involved.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Despite the many efforts by companies to mitigate fairness issues, most of the existing measures
solely focus on one stakeholder group, sometimes even at the expense of another. For example,
Airbnb has implemented strategies to mitigate racial discrimination among its users [ 4], yet their
listings of comparable relevance might still not receive similar levels of visibility. Etsy implements
measures to promote market diversity and supporting new entrants, which contributes to equality of
opportunities for its items (sellers) [ 27,13]; however, such a strategy may inadvertently disadvantage
users by potentially overwhelming them with choices and hindering their ability to quickly find their
preferred products. Similar to industrial practices, the majority of ongoing research on fairness in
recommender systems (e.g., [ 13,55,71]) also focus solely on either users or items, but seldom both.
Addressing multi-sided fairness is inherently complex due to the competing interests and objectives
of different stakeholders [ 39,12]. If algorithms solely prioritize the platform’s profits, this can lead to
inequitable exposure for those more niche items and users who prefer them. In addition, what users
perceive as fair may be viewed as unfair by items, and vice versa [16].
In face of these challenges, our work aims to answer the following two questions: (1) What constitutes
a fair recommendation within a multi-sided platform? and (2) How would a platform implement a
fair recommendation in a practical online setting? Our contributions are summarized as follows:
1.A novel fair recommendation framework, Problem ( FAIR ).Our fair recommendation problem,
framed as a constrained optimization problem, adopts a novel multi-sided perspective that first
achieves within-group fairness among items/users, and then enforces cross-group fairness that
addresses the trade-offs across groups. Notably, Problem (FAIR )enables the platform to (i) flexibly
define fair solutions for items/users rather than relying on a single predefined notion (see Section
2.2); (ii) adjust trade-offs between its own business goals and fairness for stakeholders (see, e.g.,
our case study in Section 4); (iii) flexibly accommodate additional operational considerations.
2.AFairOnline Recommendation algorithm for Multi-sided platforms ( FORM ). In Section 3,
we study an online setting where the platform must ensure fairness for a sequence of arriving
users amidst data uncertainty. We present FORM , a low-regret algorithm tailored for fair online
recommendation, whose efficacy is further validated via a real-world case study (Section 4). The
design of FORM contributes both methodologically and technically. (i) Methodologically, contrary
to prior works (e.g., [ 55,71]) that treats learning and enforcing fairness as two separate tasks,
we recognize that these two tasks, when conducted simultaneously, are often at odds. FORM
well balances learning and fairness via properly relaxing fairness constraints and introducing
randomized exploration. (ii) Technically, FORM overcomes a non-trivial challenge of managing
uncertain fairness constraints when solving a constrained optimization problem in an online
setting with bandit feedback. While existing works on online constrained optimization all would
require certain access to constraint feedback (see discussion in Section 1.1), our setup disallows
even verifying the satisfaction of item/user fairness constraints, demanding novel design in FORM .
1.1 Related Works
Our work primarily contributes to the emerging area of algorithmic fairness in recommender systems.
Additionally, from a technical aspect, our algorithm contributes to the field of constrained optimization
with bandit feedback. We highlight the literature most relevant to our work in both areas.
Algorithmic fairness in recommender systems. Algorithmic fairness is an emerging topic [ 8,2,
42] explored in various contexts such as supervised learning [ 14,24], resource allocation [ 9,37],
opportunity allocation [ 36], scheduling [ 50], online matching [ 46], bandits [ 6], facility location [ 34],
refugee assignment [ 29], assortment planning [ 17], online advertising [ 21], search [ 5], and online
combinatorial optimization [ 32]. Our research is primarily aligned with works investigating fairness
in recommender systems; see [67, 20] for comprehensive surveys.
The prior works on fairness in recommender systems can be categorized into three streams based on
their subjects: item fairness ,user fairness andjoint fairness . Item fairness focuses on whether the
decision treats items fairly, including fair ranking and display of search results [ 73,11,48,10,60,
66,17,72], similar prediction errors for items’ ratings [ 57], and long-term fair exposure [ 31]. User
fairness examines whether the recommendation is fair to different users, encompassing aspects like
similar recommendation quality [26, 45, 68] and comparable explainability [30] across users.
The third stream, also the stream that our work fits in, focuses on joint fairness that concerns whether
both items and users are treated fairly. However, the number of works in this stream remain scarce, as
2suggested in [ 1]. Existing works mainly focuses on achieving item and user fairness based on certain
pre-specified fairness notions: [ 13] seeks to balance item and user neighborhoods; [ 55] proposes a
method that guarantees maxmin fair exposure for items and envy-free fairness for users; [ 71,52]
promote fairness with respect to item exposures and user normalized discounted cumulative gains.
Additionally, [ 70] proposes and compares a family of joint multi-sided fairness metrics to tackle
systematic biases in content exposure, and [ 69] introduces a multi-objective optimization framework
that jointly optimizes accuracy and fairness for consumers and producers.
Our work distinguishes from the above works on multi-sided fairness in several aspects. (i) We not
only impose fairness for multi-stakeholders (items/users), but also take the platform’s revenue, which
is often neglected, into consideration. (ii) Our framework is not confined to a single, pre-specified
fairness/outcome notion. (iii) Most importantly, unlike works that solely focused on multi-sided
fairness, we recognize the challenge of jointly handling fairness and learning, and propose an
algorithm with theoretical guarantees (see Section 3).
Constrained optimization with bandit feedback. Our work formulates the fair recommendation
problem in an online setting as a constrained optimization with bandit feedback (see Section 3.1).
Our proposed algorithm (Algorithm 1) makes a notable contribution to the literature of constrained
optimization with bandit feedback by addressing the challenge of having uncertain constraints .
Previous research has explored constrained optimization with bandit feedback in various contexts.
Notably, works on online learning with knapsack [38,15,61] involves maximizing total reward
while adhering to a resource consumption constraint, using feedback on rewards and resource usage.
Other relevant studies include online bidding [ 28,22], online allocation [ 7], and safe sequential
decision-making [ 62], which develop algorithms to monitor and maintain “constraint balances” or
address adversarial objectives and constraints. Our setting crucially differs from prior works by not
assuming availability of constraint feedback or even the ability to monitor constraint satisfaction,
making our problem more challenging (see, also, Section 3.2 for more discussions).
2 Preliminaries
Throughout this paper, boldface symbols denote vectors or matrices, while regular symbols represent
scalars. For matrix A∈Rn×m,Ai,jis the element at the i-th row and j-th column; Ai,:andA:,jare
thei-th row and j-th column vectors, respectively. We set [n] :={1,2, . . . , n },∆nas the probability
distribution space over [n], and∆m
n= ∆ n×···× ∆n(m times). For any v∈R,(v)+:= max( v,0).
2.1 Platform’s Recommendation Problem
Consider a platform performing recommendations for Trounds, each indexed by t∈[T], where one
user visits the platform at each round. There are Nitems on the platform, each indexed by i∈[N],
and the users are classified into Mtypes, each indexed by j∈[M]. At each round t, a type- Jtuser
arrives and the platform observes the user’s type.1Here, we consider a stochastic setting where the
probability of having a type- juser arrival is P[Jt=j] =pj, where p= (pj)j∈[M]∈∆Mdenotes
thearrival probabilities2. The platform needs to select an item to display to the user, denoted by
It. If a type- juser is presented with item i, he/she would choose/purchase the item with probability
yi,j∈(0,1), where y= (yi,j)i∈[N]
j∈[M]denotes the purchase probabilities . The platform then observes
the user’s purchase decision zt∈ {0,1}. If item igets chosen/purchased, it generates revenue ri>0,
where r= (ri)i∈[N]denotes the revenues .
Letθ= (p,y,r)∈Θ, where Θ = ∆ M×(0,1)N×M×RN, define an instance of the platform’s
recommendation problem. Given the problem instance θ, the platform needs to determine its
recommendation probabilities x∈∆M
N, where xi,jis the probability of offering item iupon
observing a type- juser’s arrival.3In Section 2, we first fix the problem instance θand omit variables’
1Real-world recommendation systems can often categorize users based on available features, even when
sensitive attributes (e.g., gender, race) are restricted. These systems use data such as device types [ 64], zip codes
[44] and purchase histories, ensuring that users with similar attributes tend to have aligned preferences.
2We later extend our model/method to handling periodic arrivals; see our discussion in Sections 3.6 and E.1.
3In our setting, a platform can make personalized recommendations based on type of the arriving user.
However, even if personalization is limited due to legal considerations (e.g., Digital Market Act [ 54]) or user
privacy settings, our framework, method and theoretical results all remain valid.
3dependency on θwhenever the context allows. Later, we will transition to an online setting where the
problem instance θbecomes unknown (see Section 3) reestablish the dependency in our discussion.
Here, we focus on a single-item recommendation setting, where the platform highlights one item to
each arriving user. This approach applies to various real-world scenarios, such as Spotify’s “Song of
the Day”, Amazon’s “Best Seller”, Medium’s daily “Must-Read” article, etc. Later in Section 3.6 and
our case study on Amazon review data in Section 4, we will show that the core concepts of our model
and approach can naturally extend to recommending an assortment of items.
2.2 Single-Sided Fair Solutions: Within-Group Fairness for Items and Users
To properly address multi-sided fairness, we begin by first considering within-group fairness . That
is, we seek to answer: what constitutes a fair solution within our item/user group ? To address this
question, it is important to understand the different outcomes that items and users respectively care
about, and the fairness notions they would like to consider.
Single-Sided Item-Fair Solutions. LetOI
i(x)be the expected outcome received by item iat
a round with recommendation probabilities x. We let OI
i(x)take the following general form:
OI
i(x) =L⊤
i,:xi,:, where L= (Li,j)i∈[N]
j∈[M]andLi,jcan be any proxy for the expected outcome
received by item ifrom type- juser if it gets offered. One can consider any of the following metrics or
their weighted combinations as the item’s outcome: (1) visibility :Li,j=pjandOI
i(x) =P
jpjxi,j;
(2)marketshare :Li,j=pjyi,jandOI
i(x) =P
jpjyi,jxi,j; (3) expected revenue :Li,j=ripjyi,j
andOI
i(x) =P
jripjyi,jxi,j.
For items, a common way to achieve fairness is via the optimization of a social welfare function
(SWF), denoted as W, which merges fairness and efficiency metrics into a singular objective (see
[18]). The maximization of SWF can incorporate a broad spectrum of fairness notions commonly
adopted in practice, including maxmin fairness [ 58], Kalai-Smorodinsky (K-S) bargaining solution
[41], Hooker-Williams fairness [ 37], Nash bargaining solution [ 53], demographic parity, etc. (See
Section A.2 for an expanded discussion of these fairness notions and their social welfare functions.)
To preserve the generality of our framework, we let the platform freely determine the outcome
function and fairness notion for items, by broadly defining the item-fair solution as follows.
Definition 2.1 (Item-Fair Solution) Given items’ outcome matrix L(θ), and a SWF W: ∆M
N→R,
an item-fair solution is given by: fI∈arg maxx∈∆M
NW(OI(x)).
One popular fairness notion that can be captured by Definition 2.1 is maxmin fairness, which ensures
that the most disadvantaged item is allocated a fair portion of the outcome. A few prior works [ 55,72]
on fair recommendation solely focus on achieving maxmin fairness for the visibilities (exposure)
received by the items. Our model, however, can flexibly accommodate any outcome functions as
listed above and any fairness notions supported via SWF maximization (see Section A.2).
Single-Sided User-Fair Solutions. LetOU
j(x)be the expected outcome received by a type- j
user at a round with recommendation probabilities x. We again let OU
j(x)take a general form:
OU
j(x) =U⊤
:,jx:,j, where U= (Ui,j)i∈[N]
j∈[M]andUi,jis the expected outcome received by a type- j
user if offered item i. Here, the specific form of the user’s outcome matrix U(θ)is determined
by user’s utility model, which can vary depending on contexts. See Section A.1 for some example
forms of U(θ)based on discrete choice models (multinomial logit (MNL), probit) used in demand
modeling [ 65] and valuation-based models used in online auction design [ 51]. For generality, we do
not impose restrictions on the form of U(θ), but merely assume knowledge of it.
For users, achieving fairness is more straightforward. Since the platform can personalize recom-
mendations based on user types, given the users’ outcome matrix U(θ), it is best for type- jusers to
consistently receive the item that offers the highest utility. We thus define the user-fair solution as:
Definition 2.2 (User-Fair Solution) Given users’ outcome matrix U(θ), the user-fair solution fU∈
∆M
Nis given by fU
i,j= 1ifi= arg max iUi,jandfU
i,j= 0otherwise, for all i∈[N], j∈[M].
42.3 Drawbacks of a Single-Sided Solution
While a single-sided (fair) solution for one stakeholder group can be straightforward to identify, a
recommendation policy that solely benefits a single stakeholder group (platform, items, users) could
result in extensive costs for the rest of the stakeholders, as suggested by the following proposition.
Proposition 2.1 Letϵ1= min y/maxy, ϵ2= min U/maxU, and ϵ= max {ϵ1, ϵ2,1/N}. There
exists a problem instance such that: (1) The platform’s revenue-maximizing solution results in zero
outcomes for some items and all users attaining only ϵof their maximum attainable outcome. (2) The
item-fair solution leads to the platform receiving at most 2ϵof its maximum attainable revenue and
any user achieving at most 2ϵof their maximum attainable outcome. (3) The user-fair solution results
in zero outcomes for some items and the platform receiving ϵof its maximum attainable revenue.
Proposition 2.1 shows that single-sided solutions that can lead to extremely unfair outcomes for some
stakeholders under certain scenarios. This motivates us to investigate the concept of cross-group
fairness in the following section, where we aim to balance the needs of all stakeholders.
2.4 Multi-Sided Solutions: From Within-Group Fairness to Cross-Group Fairness
In this section, we proceed to investigate the concept of cross-group fairness by proposing a fair
recommendation framework, Problem (FAIR ). Specifically, we seek to answer the following: what
constitutes a fair recommendation policy across all stakeholder groups on a multi-sided platform?
The platform, our main stakeholder, primarily wishes to optimize its expected revenue, denoted by
REV(x) =P
i,jripjyi,jxi,j.Whenever a type- juser arrives, a revenue-maximizing platform with
full knowledge of the instance would show the most profitable item, i.e., i⋆
j= arg max i∈[N]riyi,j,
with probability one. We let OPT-REV= max xREV(x)be platform’s maximum attainable expected
revenue in the absence of fairness. Such a deterministic revenue-maximizing approach, as shown in
Section 2.3, is evidently undesirable for the less profitable items and any user who prefer them.
To create a fair ecosystem, the platform wishes to impose certain levels of fairness for all items/users.
In an offline setting with full knowledge of the instance θ, the platform can first compute item-fair and
user-fair solutions fI,fUand solve the following fair recommendation problem, Problem (FAIR ), to
prioritize its revenue maximization goal, while achieving cross-group fairness via fairness constraints:
FAIR -REV= max
x∈∆M
NREV(x)s.t.OI
i(x)≥δI·OI
i(fI)∀i∈[N]
OU
j(x)≥δU·OU
j(fU)∀j∈[M],(FAIR )
where δI, δU∈[0,1]are the fairness parameters for items and users respectively. Here, the item/user
fairness constraints ensure that the outcome received by any item/user is at least of a certain proportion
of the outcome that they would otherwise receive under their within-group fair solution. One can think
ofδIandδUas tunable handles that the platform can adjust to regulate the tradeoff between fairness
and its own revenue (see how a platform can tune these parameters in practice in Sections 4 and
C). If a platform wishes to incorporate other operational considerations such as diversity constraints
(xmin≤xi,j≤xmax) or fairness for more stakeholder groups beyond items/users (e.g., DoorDash
drivers, Airbnb hosts), additional constraints can be incorporated in a similar fashion.
We let x⋆denote the optimal solution to Problem (FAIR ), which serves as the benchmark that we
compare our algorithm against when we next work with the online setting (see Section 3).
Remark 2.1 (Constrained optimization versus fairness regularizers) An alternative method of
integrating fairness is to use fairness regularizers (i.e., Lagrangian multipliers) and optimize a
weighted sum of platform’s revenue and items’/users’ outcomes. Nonetheless, our constrained opti-
mization formulation offers several clear benefits: (i) while it is possible to translate our constrained
optimization formulation into a weighted sum with fairness regularizers via dualization, the reverse
process is not straightforward; (ii) our fairness parameters δI, δUare directly interpretable; (iii) our
framework accommodates additional operational considerations, with results remaining valid.
Remark 2.2 (Selecting fairness parameters via “price of fairness”) To choose the right fairness
parameters δI, δU, it is important to understand both (i) the extent of fairness needed for items/users,
5and (ii) the cost of implementing fairness constraints to balance its revenue and stakeholder interests,
often known as the “price of fairness” (PoF). In Section C, we investigate the concept of PoF under
our framework (Problem (FAIR )), and show a piecewise linear dependency of the PoF on both
(i) the amount of misalignment in the platform’s and its stakeholders’ objectives, (ii) the fairness
parameters δI, δU(see Theorem C.1). We further provide guidelines on how a platform can effectively
tune its fairness parameters based on its desired fairness level and acceptable PoF via online A/B
experimentation (see Section C for an extended discussion).
3 Fair Online Recommendation Algorithm for Multi-Sided Platforms
In practice, user data are often missing or inaccurate, so solving Problem (FAIR )with flawed data
could inadvertently result in unfair outcomes. Our online setting (Section 3.1) allows the platform to
improve its estimates of user data via a data collection process over time. However, this simultaneous
process of learning to be fair and understanding user preferences also introduces new challenges. In
this section, we address the more intricate online setting and introduce an algorithm for this scenario.
3.1 Online Setting and Goals
In an online setting, the platform no longer has prior knowledge of user preference yand arrival rates
p. At each round t, some non-anticipating algorithm Agenerates the recommendation probabilities
xt∈∆M
N, only based on past recommendation {xt′}t′<tand purchase decisions {zt′}t′<t. After
observing the type of the arriving user Jt, the platform offers item iwith probability xt,i,Jt. Following
this recommendation, the platform only observes the user’s binary purchase decision zt∈ {0,1}for
the offered item. We will measure the performance of our algorithm using the following metrics:
Definition 3.1 (Revenue and fairness regrets) Consider Problem (FAIR )under a problem instance
θ∈Θ, item outcome function L(θ) : Θ→RN, user outcome function U(θ) : Θ→RM, item social
welfare function W:RN→R, and fairness parameters δI, δU∈[0,1]. LetAbe a non-anticipating
algorithm that generates recommendation probabilities xtat each round t∈[T]. We define the
revenue regret , denoted by R(T), and the fairness regret , denoted by RF(T), ofArespectively as
R(T) =1
TTX
t=1 
REV(x⋆,θ)−REV(xt,θ)
andRF(T) = max {RI
F(T),RU
F(T)},
whereRI
F(T) = max i1
TPT
t=1(δI·OI
i(fI(θ),θ)−OI
i(xt,θ))+andRU
F(T) = max j1
TPT
t=1(δU·
OU
j(fU(θ),θ)−OU
j(xt,θ))+are respectively the maximum time-averaged violation of item/user-fair
constraints. Here, REV(x,θ) =P
i,jripjyi,jxi,jis platform’s expected revenue under instance θ;
OI
i(x,θ) =Li,:(θ)⊤xi,:andOU
j(x,θ) =U:,j(θ)⊤x:,jare the item/user outcomes under θ;fI(θ)
andfU(θ)are the item/user-fair solutions w.r.t. L(θ),U(θ)and SWF W, per Definition 2.1 and 2.2.
Observe that in Definition 3.1, we reintroduce the dependency of our variables on the instance θ. This
is because in the online setting, we typically work with an estimated instance, which in turn impacts
our estimates for the platform’s revenue, item/user outcomes as well as item/user-fair solutions.
3.2 Challenges of the Online Setting
Before proceeding, we highlight the two main challenges unique to the online setting.
(1) Data uncertainty and partial feedback can interfere with evaluating fairness. Due to lack
of knowledge of the instance θand limited feedback (as we only observe the purchase decision
for the offered items), it is difficult to assess the quality of our recommendations xtat each round.
In particular, verifying whether our fairness constraints are satisfied and/or measuring the amount
of constraint violations require evaluating the item/user-fair solutions fI(θ),fU(θ)and item/user
outcomes OI
i(fI(θ),θ), OI
i(xt,θ)andOU
j(fU(θ),θ), OU
j(xt,θ), all of which heavily depend on θ.
This sets our work apart from prior works on fairness in recommender systems, which assume full
knowledge of the problem instance (e.g., [ 55,71]), and from works on constrained optimization with
bandit feedback (e.g., [ 38,15,61]), which rely on the availability of constraint feedback. As we
6discussed in Section 1.1, the latter works need to access the amount of constraint violation or verify
constraint satisfaction after each decision to update their policies. For example, in online learning
with knapsack, the constraint is a resource budget, so constraint violations can be directly evaluated.
Our work contributes to the literature on constrained optimization with bandit feedback by directly
handling uncertain constraints and providing a sublinear regret bound (see Theorem 3.1).
(2) Fairness can interfere with the quality of learning. To ensure fairness for all stakeholder groups,
some items (e.g., low-revenue or low-utility items) would necessarily receive lower recommendation
probabilities than the others. Nonetheless, if we barely offer these items to the users, the lack of
exploration could also lead to poor estimation for their purchase probability.
3.3 Algorithm Description
We now present our algorithm, called FORM (FairOnline Recommendation algorithm for Multi-sided
platforms), which handles the aforementioned challenges by adopting a relaxation-then-exploration
technique, allowing it to achieve both low revenue regret and fairness regret. The design of FORM is
outlined in Algorithm 1, consisting of the following.
Algorithm 1 Fair Online Recommendation Algorithm for Multi-Sided Platforms ( FORM )
Input: (i)Nitems with revenues r∈RN, item outcome L(θ) : Θ→RN×M, item SWF W: ∆M
N→R; (ii)
Mtypes of users with user outcome U(θ) : Θ→RN×M; (iii) fairness parameters δI, δU∈[0,1].
1.Initialization. Setˆy1,i,j= 1/2andˆpj= 1/Mfori∈[N], j∈[M]. Let the magnitude of exploration be
ϵt= min
N−1, N−2
3t−1
3	
, and define the magnitude of relaxation as
ηt=Mlog(T) max{Γy,t,Γp,t}, (1)
where Γy,t= 2 log( T)/q
ϵt·max{1, t/log(T)−p
tlog(T)/2}andΓp,t= 5p
log(3 T)/t.
2. For t= 1, . . . , T
(a)Solve a relaxed version of Problem (FAIR )under data uncertainty. Given estimated instance
ˆθt= (ˆpt,ˆyt,r)and relaxation ηt, letˆxtbe the optimal solution to Problem ( FAIR -RELAX (ˆθt, ηt)).
(b)Recommend with randomized exploration.
• Let the recommendation probability be xt,i,j= (1−Nϵt)ˆxt,i,j+ϵtfor all i∈[N], j∈[M].
• Observe the type of the arriving user Jtand offer item Itbased on probabilities It∼xt,:,Jt.
•Observe purchase decision zt∈ {0,1}and update the number of user arrivals: nJt,t=nJt,t−1+ 1.
(c)Update estimates for purchase and arrival probabilities. Let
ˆyt+1,i,j=1
nj,tnj,tX
k=1I{Iτj,k=i, zτj,k= 1}/xτj,k,i,j,ˆpt+1,j=1
tnj,t,ˆθt+1= (ˆpt+1,ˆyt+1,r),(2)
where τj,k∈[T]denote the round at which the kth type- juser arrives.
Solve a relaxed version of Problem (FAIR )under the estimated instance. Recall, from our
first challenge, that we cannot directly verify if the fairness constraints have been satisfied due
to having data uncertainty and partial feedback. If the platform solves Problem (FAIR )using the
estimated instance, the flaw in estimation could easily lead to failure of maintaining fairness for some
stakeholders. In face of this, FORM solves a relaxed version of Problem ( FAIR ), defined as follows:
max
x∈∆M
NREV(x,ˆθt)s.t.OI
i(x,ˆθt)≥δI·OI
i(fI(ˆθt),ˆθt)−ηt∀i∈[N]
OU
j(x,ˆθt)≥δU·OU
j(fU(ˆθt),ˆθt)−ηt∀j∈[M].(FAIR -RELAX (ˆθt, ηt))
where ˆθtis the estimated instance at round tandηt>0is a parameter that regulates the magnitude
of relaxation on our fairness constraints (Eq. (1)). Here, Problem (FAIR -RELAX (ˆθt, ηt))differs from
Problem (FAIR )in that (i) it uses the estimated instance ˆθtrather than the ground-truth instance θ, and
(ii) it relaxes all fairness constraints by the amount of ηt. At a high level, the relaxation here ensures
that the solution fair to all stakeholders (i.e., x⋆) would be captured even under data uncertainty.
The magnitude of relaxation ηtdepends on Γy,tandΓp,t, which are respectively confidence bounds
associated with estimated purchase/arrival probabilities (see Definition D.1). As our estimates become
more accurate, both confidence bounds shrink, hence decreasing the magnitude of fairness relaxation.
7Recommend with randomized exploration. In order to handle the second challenge of some items
being inadequately explored, we incorporate randomized exploration by sampling from a distribution
that perturbs the estimated solution to Problem (FAIR ),ˆxt, by a carefully tailored amount ϵt. This
allows ongoing exploration of all items, with the magnitude of exploration ϵtalso decreasing over
time as our parameter estimates improve, shifting from exploration towards greater exploitation.
Unbiased estimators for our problem instance. In Algorithm 1, for simplicity, we estimate purchase
probabilities yusing an inverse probability weighted estimator and estimate arrival probabilities p
with the sample mean (See Eq. (2)). For sufficiently large t, our estimates ˆytandˆptwill be accurate
with high probability (see Lemma D.2). In practice, the platform, potentially with access to historical
data, can freely use any learning mechanism that yield unbiased estimators for user preferences and
arrival rates. As long as the estimates get sufficiently accurate over time with high probability, FORM
would ensure low revenue/fairness regrets, all while keeping the rest of its design unchanged.
3.4 Theoretical Analysis
We theoretically analyze the performance of FORM , under a mild local Lipschitzness assumption.
Assumption 3.1 Given instance θ∈Θ, there exists constants B, ζ > 0such that for any ˜θ∈Θ
where ∥˜θ−θ∥∞≤ζ,max{∥U(θ)−U(˜θ)∥∞,∥fI(θ)−fI(˜θ)∥∞} ≤B∥θ−˜θ∥∞.
Assumption 3.1 is well justified in practice. In terms of users’ outcome matrix, the assumption readily
holds for all prevalent users’ choice models and valuation-based models (see examples in Section
A.1) as long as θis bounded away from the boundaries of Θ. For item-fair solutions fI(θ), a wide
range of standard outcome functions (visibility, revenue, etc.) and fairness notions (maxmin, K-S,
etc.) readily induce locally Lipschitz item-fair solutions; see Section A.3 for some examples.
Theorem 3.1 is the main result of this section, which states that FORM achieves both sublinear revenue
regret and fairness regret, as desired. The proof of Theorem 3.1 is deferred to Section D.
Theorem 3.1 (Performance of FORM )Given any problem instance θ∈Θand assume that Assump-
tion 3.1 holds, for Tsufficiently large, we have that
•the revenue regret of FORM is at most E[R(T)]≤ O(MN1
3T−1
3);
•the fairness regret of FORM is at most E[RF(T)]≤ O(MN1
3T−1
3).
3.5 Computational complexity and scalability
In terms of the computational complexity of FORM , the dominant runtime cost in each iteration of
FORM arises from solving Problem (FAIR -RELAX (ˆθt, ηt)). Note that for a wide variety of commonly
used item-fairness notions (e.g., maxmin, K-S, demographic parity; see Table 1) and item outcome
functions (e.g., visibility, revenue), solving Problem (FAIR -RELAX (ˆθt, ηt))involves solving two
linear programs with MN variables, which is solvable in polynomial runtime O∗((MN)2+1/18)
[40]. The remaining operations in each iteration of FORM takes O(MN)time. Consequently, each
iteration of FORM has a worst-case complexity of O∗((MN)2+1/18). In practice, however, much
better performance can often be achieved by advanced LP solvers such as Gurobi and CPLEX.
For real-world deployments, FORM can be further adapted with scalability in consideration. First,
in practice, there is no need to solve Problem (FAIR -RELAX (ˆθt, ηt))at every user arrival. Instead,
platforms can resolve the problem after a given number of user arrivals or periodically at fixed
time intervals, while updating user data in real-time. This allows majority of the iterations to run
inO(MN)time and removes the computational overhead.4Second, we do not always encounter
a large-scale optimization problem when applying our framework. Real-world recommendation
systems often narrow down items through lightweight pre-filtering stages based on criteria like
keywords or price range (e.g., [ 49]), allowing us to enforce fairness within smaller, context-specific
subsets. Our fairness framework is also particularly impactful at this final stage, where items with
similar attributes compete for visibility and a revenue-maximizing strategy could lead to extremely
unfair outcomes.
4See our additional experiments on MovieLens data in Section F.3, where we resolve the constrained
optimization problem at most once every 100 user arrivals, while our algorithm remains effective.
83.6 Extensions to Additional Setups
Our framework, Problem (FAIR ), and the proposed algorithm can be extended to accommodate other
variations of our setup. Below, we briefly introduce these extensions; see Section E for more details.
Periodic arrivals. While our current model focuses on stochastic arrivals with fixed user arrival
probabilities p, real-world recommendation systems often observe non-stationary user arrivals with
hourly, daily or weekly periodicity. In Section E.1, we show that by additionally integrating a
sliding window mechanism, FORM can seamlessly accommodate periodic arrivals, and attain the same
O(MN1/3T−1/3)guarantees for both revenue and fairness regrets.
Recommending an assortment. As remarked in Section 2, while our model adopts a single-item
recommendation setting, the high-level ideas behind our framework/method naturally extend to
recommending an assortment of size at most K. To extend our framework, Problem (FAIR ), we will
let{qj(S) :S∈[N],|S| ≤K, j∈[M]}be the decision variables, where qj(S)is the likelihood of
proposing assortment Sto a type- juser. We can then apply the same relaxation-then-exploration
techniques to solve the fair recommendation problem in a dynamic online setting. See Section E.2
for details of our extension. In the case study that follows (Section 4), we also validate the efficacy of
our framework/method in a real-world assortment recommendation problem.
4 Case Studies on Amazon Review Data
In our case study on Amazon review data, we act as an e-commerce platform displaying featured
products to incoming users, aiming to maximize revenue while ensuring fairness for items and users.
Our experiments numerically validate the efficacy of our framework/method. All algorithms were
implemented in Python 3.7 and run on a MacBook with a 1.4 GHz Quad-Core Intel Core i5 processor.
Data and setup. We use an Amazon review dataset [ 71] from the “Clothing, Shoes and Jewelry”
category. Product reviews provide relevance scores between each item and user, serving as a proxy for
purchase likelihood. Users are classified into M= 5types using matrix factorization and k-means
clustering on user feature vectors. The arrival probability pjis set to the proportion of type- jusers.
We select N= 30 items with the highest variance in relevance scores across user types, indicating a
discrepancy between item and user interests and making this a challenging instance. Item revenues ri
are uniformly drawn from [0.5,1.5]. The purchase probability and users’ utilities are defined based
on the multinomial logit (MNL) model [ 65]. For a type- juser presented with assortment S, the
probability of purchasing item i∈Sisyi,j=evi,j
1+P
i′∈Sevi′,j, where vi,jis the relevance score. The
user’s perceived utility is log(1 +P
i′∈Sevi′,j). Each instance simulates T= 2000 user arrivals.
Upon arrival, each type- juser is shown an assortment Sof up to K= 3items.
The platform’s primary goal is to maximize its revenue while ensuring maxmin fairness for items
w.r.t. item revenue, and fairness for users w.r.t. utilities from the MNL model. We apply the extension
ofFORM for recommending assortments, using relaxation-then-exploration techniques to produce
fair recommendations while learning user data (see Section E.2). To establish generality of our
framework/method, we have also performed additional experiments under alternative outcomes and
fairness notions (see Section F.2) and an alternative movie recommendation setting using MovieLens
data (see Section F.3). In all cases, our experiments yielded consistent results.
Baselines. We consider six baselines for comparisons. Since all baselines assume full knowledge
of the instance and lack a learning phase, we let them use our unbiased estimator to update their
estimated instance as they observe purchase decisions, and recommend based on these estimates. (i)
greedy : offers Kitems with the highest expected revenue, prioritizing platform’s goal; (ii) max-utility :
offers Kitems with the highest user utilities, a user-centric approach; (iii) min-revenue : offers K
items generating the least revenue so far, promoting maxmin fairness for items w.r.t. revenue; (iv)
random : offers Kitems uniformly at random, promoting maxmin fairness for items w.r.t. visibility;
(v) FairRec [ 55]: an algorithm that addresses two-sided fairness in a static setting. While it’s not
designed for online settings, we adapt it for online arrivals by duplicating users and using the single-
shot recommendation solution. It ensures maxmin fairness for item visibility and envy-free fairness
for users; (vi) TFROM [ 71]: addresses two-sided fairness in an online setting, focusing on uniform
item visibility and similar user normalized discounted cumulative gain. However, neither FairRec nor
TFROM considers platform’s revenue; see Section F.1 for more details on these two baselines.
90 500 1000 1500 2000
t75%80%85%90%95%100%1
TT
t=1REV(xt)/FAIR-REV
FAIR-REV(0.2,0.2) FORM(0.2,0.2)(a) Convergence of revenue.
0.0 0.1 0.2 0.3 0.4 0.5I
6065707580859095100Revenue (%)
OPT-REV
FAIR-REV(., 0.2)
FORM(., 0.2)
FAIR-REV(., 0.8)FORM(., 0.8)
random
greedy
max-utilitymin-revenue
FairRec
TFROM (b) Normalized revenue.
0 5 10 15 20 25 30
items0.00.20.40.60.81.0normalized item outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (c) Item outcomes.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
users0.950.960.970.980.99normalized user outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (d) User outcomes.
Figure 1: Experiment results for Amazon review data. FAIR -REV(δI, δU)is the platform’s revenue from solving
Problem (FAIR )in hindsight with fairness parameters δI, δUandFORM(δI, δU)isFORM when adopting fairness
parameters δI, δU. In Figures 1c and 1d, item (user) outcomes are shown in ascending order. All results are
averaged over 10simulations, with the line indicating the mean and shaded region showing mean ±std/√
10.
Platform’s revenue. We first assess FORM ’s efficacy in achieving a low-regret solution. Figure
1a shows that the time-averaged revenue of FORM , i.e.,1
TPT
t=1REV(xt), converges rapidly to the
optimal revenue for Problem (FAIR ), complementing Theorem 3.1. Figure 1b compares the time-
averaged revenue (normalized by OPT-REV) ofFORM with other baselines. As expected, greedy
achieves revenue close to OPT-REV, while all other baselines result in significant revenue loss.
FairRec and TFROM, in particular, reduce platform revenue by about 38% as their sole focus is on
ensuring two-sided fairness. In contrast, FORM offers tunable parameters to balance platform and
stakeholder interests. As shown in Figure 1b, adjusting δIandδUallows platforms to control revenue
loss (e.g., choosing δI, δU= 0.2keeps loss within 10%). See, also, Section C for how a platform can
tune fairness parameters to control its “price of fairness” in practice.
Item and user fairness. Figure 1c shows average outcomes for each item i, normalized by the
outcome under item-fair solution, max{1
TOI
i(xt)/OI
i(fI),1}. As expected, since our item-fair
solution fIadopts maxmin fairness w.r.t. item revenues, min-revenue achieves the highest level of
item fairness, though at a high cost to the platform. Methods such as random , FairRec, TFROM also
achieve high item fairness but with some discrepancies in maximum and minimum item outcomes.
greedy andmax-utility show extremely skewed allocations, with some items receiving minimal or no
revenue. In comparison, our algorithm FORM strikes a good balance, ensuring all items nearly attain
or surpass the specified fairness levels, whether the level is high ( δI= 0.8) or moderate ( δI= 0.2).
Figure 1d shows the average outcomes for each user type, normalized by their outcome under the
user-fair solution, max{1
TOU
j(xT)/OU
j(fU),1}. In the Amazon review data, user interests align well
with the platform’s objectives (as validated in Section C), leading to most baselines performing fairly
well and achieving high levels of fairness for the users. FORM again ensures good user outcomes, all
while maintaining high platform revenue and desired item fairness levels.
5 Conclusion and Future Directions
Our work introduced a novel fair recommendation framework that maintains platform revenue while
addressing multi-stakeholder fairness, as well as a low-regret algorithm that effectively produces
fair recommendations amidst data uncertainty. It is worth noting that the high-level ideas behind
our versatile framework has the potential to be applied in settings beyond recommender systems,
such as dynamic pricing and online advertising, ensuring fairness across different stakeholders and
promoting stable market conditions in these applications.
There are several future directions worth investigating. (i) Our current framework calibrates rec-
ommendation policies within a shorter time period when user preferences and item attributes are
relatively fixed. Future research can explore long-term effects of our method by developing adaptive
fairness notions that account for evolving user and item attributes and quantifying the long-term
multi-stakeholder fairness. (ii) It would be interesting to pursue real-world deployments of our frame-
work/algorithm and evaluate their impact using an expanded set of metrics, such as user satisfaction,
retention rates, and recommendation diversity.
10Acknowledgments and Disclosure of Funding
N.G. and Q.C. were partially supported by funding from the Office of Naval Research (ONR) (Award
Number: N00014-23-1-2584) and the MIT-IBM Watson AI Lab.
References
[1]H. Abdollahpouri and R. Burke. Multi-stakeholder recommendation and its connection to
multi-sided fairness. arXiv preprint arXiv:1907.13158 , 2019.
[2]R. Abebe, S. Barocas, J. Kleinberg, K. Levy, M. Raghavan, and D. G. Robinson. Roles for
computing in social change. In Proceedings of the 2020 conference on fairness, accountability,
and transparency , pages 252–260, 2020.
[3]S. Agrawal, V . Avadhanula, V . Goyal, and A. Zeevi. Mnl-bandit: A dynamic learning approach
to assortment selection. Operations Research , 67(5):1453–1485, 2019.
[4]Airbnb. A six-year update on airbnb’s work to fight discrimination. https://news.airbnb.
com/sixyearadupdate/ , 2022. Accessed: 2023-10-01.
[5]M. R. Aminian, V . Manshadi, and R. Niazadeh. Fair markovian search. Available at SSRN
4347447 , 2023.
[6]J. Baek and V . F. Farias. Fair exploration via axiomatic bargaining. arXiv preprint
arXiv:2106.02553 , 2021.
[7]S. R. Balseiro, H. Lu, and V . Mirrokni. The best of many worlds: Dual mirror descent for online
allocation problems. Operations Research , 71(1):101–119, 2023.
[8]S. Barocas and A. D. Selbst. Big data’s disparate impact. California law review , pages 671–732,
2016.
[9]D. Bertsimas, V . F. Farias, and N. Trichakis. The price of fairness. Operations research ,
59(1):17–31, 2011.
[10] A. Beutel, J. Chen, T. Doshi, H. Qian, L. Wei, Y . Wu, L. Heldt, Z. Zhao, L. Hong, E. H. Chi,
et al. Fairness in recommendation ranking through pairwise comparisons. In Proceedings of the
25th ACM SIGKDD international conference on knowledge discovery & data mining , pages
2212–2220, 2019.
[11] A. J. Biega, K. P. Gummadi, and G. Weikum. Equity of attention: Amortizing individual
fairness in rankings. In The 41st international acm sigir conference on research & development
in information retrieval , pages 405–414, 2018.
[12] R. Burke. Multisided fairness for recommendation. arXiv e-prints , pages arXiv–1707, 2017.
[13] R. Burke, N. Sonboli, and A. Ordonez-Gauger. Balanced neighborhoods for multi-sided fairness
in recommendation. In Conference on fairness, accountability and transparency , pages 202–214.
PMLR, 2018.
[14] T. Calders, F. Kamiran, and M. Pechenizkiy. Building classifiers with independency constraints.
In2009 IEEE International Conference on Data Mining Workshops , pages 13–18. IEEE, 2009.
[15] M. Castiglioni, A. Celli, and C. Kroer. Online learning with knapsacks: the best of both worlds.
InInternational Conference on Machine Learning , pages 2767–2783. PMLR, 2022.
[16] H. A. Chaudhari, S. Lin, and O. Linda. A general framework for fairness in multistakeholder
recommendations. arXiv preprint arXiv:2009.02423 , 2020.
[17] Q. Chen, N. Golrezaei, and F. Susan. Fair assortment planning. arXiv preprint arXiv:2208.07341 ,
2022.
[18] V . Chen and J. Hooker. Welfare-based fairness through optimization. Technical report, Working
Paper, Carnegie-Mellon University, 2021.
11[19] A. Dash, A. Chakraborty, S. Ghosh, A. Mukherjee, and K. P. Gummadi. When the umpire is
also a player: Bias in private label product recommendations on e-commerce marketplaces.
InProceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency ,
pages 873–884, 2021.
[20] Y . Deldjoo, D. Jannach, A. Bellogin, A. Difonzo, and D. Zanzonelli. Fairness in recommender
systems: research landscape and future directions. User Modeling and User-Adapted Interaction ,
pages 1–50, 2023.
[21] Y . Deng, N. Golrezaei, P. Jaillet, J. C. N. Liang, and V . Mirrokni. Individual welfare guarantees
in the autobidding world with machine-learned advice. arXiv preprint arXiv:2209.04748 , 2022.
[22] Y . Deng, N. Golrezaei, P. Jaillet, J. C. N. Liang, and V . Mirrokni. Multi-channel autobidding
with budget and roi constraints. arXiv preprint arXiv:2302.01523 , 2023.
[23] L. Devroye. The equivalence of weak, strong and complete convergence in l1 for kernel density
estimates. The Annals of Statistics , pages 896–904, 1983.
[24] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In
Proceedings of the 3rd innovations in theoretical computer science conference , pages 214–226,
2012.
[25] B. Edelman, M. Luca, and D. Svirsky. Racial discrimination in the sharing economy: Evidence
from a field experiment. American economic journal: applied economics , 9(2):1–22, 2017.
[26] M. D. Ekstrand, M. Tian, I. M. Azpiazu, J. D. Ekstrand, O. Anuyah, D. McNeill, and M. S.
Pera. All the cool kids, how do they fit in?: Popularity and demographic biases in recommender
evaluation and effectiveness. In Conference on fairness, accountability and transparency , pages
172–186. PMLR, 2018.
[27] Etsy. Search engine optimization (seo) for shop and list-
ing pages. https://help.etsy.com/hc/en-us/articles/
115015663987-Search-Engine-Optimization-SEO-for-Shop-and-Listing-Pages?
segment=selling . Accessed: 2024-02-28.
[28] Z. Feng, S. Padmanabhan, and D. Wang. Online bidding algorithms for return-on-spend
constrained advertisers. arXiv preprint arXiv:2208.13713 , 2022.
[29] D. Freund, T. Lykouris, E. Paulson, B. Sturt, and W. Weng. Group fairness in dynamic refugee
assignment. arXiv preprint arXiv:2301.10642 , 2023.
[30] Z. Fu, Y . Xian, R. Gao, J. Zhao, Q. Huang, Y . Ge, S. Xu, S. Geng, C. Shah, Y . Zhang, et al.
Fairness-aware explainable recommendation over knowledge graphs. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information Retrieval ,
pages 69–78, 2020.
[31] Y . Ge, S. Liu, R. Gao, Y . Xian, Y . Li, X. Zhao, C. Pei, F. Sun, J. Ge, W. Ou, et al. Towards long-
term fairness in recommendation. In Proceedings of the 14th ACM international conference on
web search and data mining , pages 445–453, 2021.
[32] N. Golrezaei, R. Niazadeh, K. K. Patel, and F. Susan. Online combinatorial optimization with
group fairness constraints. Available at SSRN 4824251 , 2024.
[33] O. Güler, A. J. Hoffman, and U. G. Rothblum. Approximations to solutions to systems of linear
inequalities. SIAM Journal on Matrix Analysis and Applications , 16(2):688–696, 1995.
[34] S. Gupta, J. Moondra, and M. Singh. Socially fair and hierarchical facility location problems.
arXiv preprint arXiv:2211.14873 , 2022.
[35] F. M. Harper and J. A. Konstan. The movielens datasets: History and context. ACM Trans.
Interact. Intell. Syst. , 5(4), dec 2015.
[36] H. Heidari and J. Kleinberg. Allocating opportunities in a dynamic model of intergenerational
mobility. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and
Transparency , pages 15–25, 2021.
12[37] J. N. Hooker and H. P. Williams. Combining equity and utilitarianism in a mathematical
programming model. Management Science , 58(9):1682–1693, 2012.
[38] N. Immorlica, K. Sankararaman, R. Schapire, and A. Slivkins. Adversarial bandits with
knapsacks. Journal of the ACM , 69(6):1–47, 2022.
[39] D. Jannach and G. Adomavicius. Price and profit awareness in recommender systems. arXiv
preprint arXiv:1707.08029 , 2017.
[40] S. Jiang, Z. Song, O. Weinstein, and H. Zhang. Faster dynamic matrix inverse for faster lps.
arXiv preprint arXiv:2004.07470 , 2020.
[41] E. Kalai and M. Smorodinsky. Other solutions to nash’s bargaining problem. Econometrica:
Journal of the Econometric Society , pages 513–518, 1975.
[42] M. Kasy and R. Abebe. Fairness, equality, and power in algorithmic decision-making. In
Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency , pages
576–586, 2021.
[43] A. Lambrecht and C. Tucker. Algorithmic bias? an empirical study of apparent gender-based
discrimination in the display of stem career ads. Management science , 65(7):2966–2981, 2019.
[44] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel. Lars: A location-aware recommender
system. In 2012 IEEE 28th international conference on data engineering , pages 450–461. IEEE,
2012.
[45] Y . Li, H. Chen, Z. Fu, Y . Ge, and Y . Zhang. User-oriented fairness in recommendation. In
Proceedings of the Web Conference 2021 , pages 624–632, 2021.
[46] W. Ma, P. Xu, and Y . Xu. Fairness maximization among offline agents in online-matching
markets. arXiv preprint arXiv:2109.08934 , 2021.
[47] M. MacCarthy. An examination of the algorithmic accountability act of 2019. Available at
SSRN 3615731 , 2019.
[48] R. Mehrotra, J. McInerney, H. Bouchard, M. Lalmas, and F. Diaz. Towards a fair marketplace:
Counterfactual evaluation of the trade-off between relevance, fairness & satisfaction in recom-
mendation systems. In Proceedings of the 27th acm international conference on information
and knowledge management , pages 2243–2251, 2018.
[49] Meta. Powered by ai: Instagram’s explore recommender system. https://ai.meta.com/
blog/powered-by-ai-instagrams-explore-recommender-system/ . Accessed: 2024-
10-01.
[50] J. Mulvany and R. S. Randhawa. Fair scheduling of heterogeneous customer populations.
Available at SSRN 3803016 , 2021.
[51] R. B. Myerson. Optimal auction design. Mathematics of operations research , 6(1):58–73, 1981.
[52] M. Naghiaei, H. A. Rahmani, and Y . Deldjoo. Cpfair: Personalized consumer and producer
fairness re-ranking for recommender systems. In Proceedings of the 45th International ACM
SIGIR Conference on Research and Development in Information Retrieval , pages 770–779,
2022.
[53] J. F. Nash Jr. The bargaining problem. Econometrica: Journal of the econometric society , pages
155–162, 1950.
[54] Official Journal of the European Union. Regulation (eu) 2022/1925 of the european parliament
and of the council of 14 september 2022 on contestable and fair markets in the digital sector
and amending directives (eu) 2019/1937 and (eu) 2020/1828 (digital markets act). https://
eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32022R1925 , 2023. Ac-
cessed: 2023-04-19.
13[55] G. K. Patro, A. Biswas, N. Ganguly, K. P. Gummadi, and A. Chakraborty. Fairrec: Two-sided
fairness for personalized recommendations in two-sided platforms. In Proceedings of the web
conference 2020 , pages 1194–1204, 2020.
[56] J. Pena, J. C. Vera, and L. F. Zuluaga. New characterizations of hoffman constants for systems
of linear constraints. Mathematical Programming , 187:79–109, 2021.
[57] B. Rastegarpanah, K. P. Gummadi, and M. Crovella. Fighting fire with fire: Using antidote data
to improve polarization and fairness of recommender systems. In Proceedings of the twelfth
ACM international conference on web search and data mining , pages 231–239, 2019.
[58] J. Rawls. A theory of justice: Revised edition . Harvard university press, 2020.
[59] Y . Seldin, C. Szepesvári, P. Auer, and Y . Abbasi-Yadkori. Evaluation and analysis of the
performance of the exp3 algorithm in stochastic environments. In European Workshop on
Reinforcement Learning , pages 103–116. PMLR, 2013.
[60] A. Singh and T. Joachims. Policy learning for fairness in ranking. Advances in neural
information processing systems , 32, 2019.
[61] A. Slivkins, K. A. Sankararaman, and D. J. Foster. Contextual bandits with packing and
covering constraints: A modular lagrangian approach via regression. In The Thirty Sixth Annual
Conference on Learning Theory , pages 4633–4656. PMLR, 2023.
[62] W. Sun, D. Dey, and A. Kapoor. Safety-aware algorithms for adversarial contextual bandit. In
International Conference on Machine Learning , pages 3280–3288. PMLR, 2017.
[63] The New York Times. Food businesses lose faith in instagram after algorithm changes. https:
//www.nytimes.com/2022/03/22/dining/instagram-algorithm-reels.html , 2022.
Accessed: 2022-03-22.
[64] The Wall Street Journal. On orbitz, mac users steered to pricier hotels. https://www.wsj.
com/articles/SB10001424052702304458604577488822667325882 . Accessed: 2024-08-
01.
[65] K. E. Train. Discrete choice methods with simulation . Cambridge university press, 2009.
[66] L. Wang, Y . Bai, W. Sun, and T. Joachims. Fairness of exposure in stochastic bandits. In
International Conference on Machine Learning , pages 10686–10696. PMLR, 2021.
[67] Y . Wang, W. Ma, M. Zhang, Y . Liu, and S. Ma. A survey on the fairness of recommender
systems. ACM Transactions on Information Systems , 41(3):1–43, 2023.
[68] C. Wu, F. Wu, X. Wang, Y . Huang, and X. Xie. Fairness-aware news recommendation with de-
composed adversarial learning. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 35, pages 4462–4469, 2021.
[69] H. Wu, C. Ma, B. Mitra, F. Diaz, and X. Liu. A multi-objective optimization framework for
multi-stakeholder fairness-aware recommendation. ACM Transactions on Information Systems ,
41(2):1–29, 2022.
[70] H. Wu, B. Mitra, C. Ma, F. Diaz, and X. Liu. Joint multisided exposure fairness for recom-
mendation. In Proceedings of the 45th International ACM SIGIR Conference on research and
development in information retrieval , pages 703–714, 2022.
[71] Y . Wu, J. Cao, G. Xu, and Y . Tan. Tfrom: A two-sided fairness-aware recommendation model for
both customers and providers. In Proceedings of the 44th International ACM SIGIR Conference
on Research and Development in Information Retrieval , pages 1013–1022, 2021.
[72] C. Xu, S. Chen, J. Xu, W. Shen, X. Zhang, G. Wang, and Z. Dong. P-mmf: Provider max-min
fairness re-ranking in recommender system. In Proceedings of the ACM Web Conference 2023 ,
pages 3701–3711, 2023.
[73] M. Zehlike, F. Bonchi, C. Castillo, S. Hajian, M. Megahed, and R. Baeza-Yates. Fa* ir: A fair
top-k ranking algorithm. In Proceedings of the 2017 ACM on Conference on Information and
Knowledge Management , pages 1569–1578, 2017.
14Appendices for
Interpolating Item and User Fairness for Multi-Sided
Recommendations
A Example Choices of Outcomes, Fairness Notions and Fair Solutions
A.1 Example Utility Functions
Below are some common example utility functions U(θ)based on common discrete choice models
(multinomial logit (MNL), probit) used in demand modeling [ 65] and valuation-based models used in
online auction design [ 51]. For all of these examples, Ui,jis strictly increasing w.r.t. the purchase
probability yi,j.
•Multinomial logit (MNL) model. Let the utility of item ifor a type juser be vi,j+ϵi,j, where
vi,j≥0andϵi,jis the random part drawn i.i.d. from the standard Gumbel distribution. Let ϵ0,j
be the utility of no-purchase option, again drawn i.i.d. from the standard Gumbel distribution. The
expected utility is Ui,j=E[max{vi,j+ϵi,j, ϵ0,j}] = log(1 + exp( vi,j)) +γ ,where γis the
Euler-Mascheroni constant. The purchase probability is given by yi,j=P[vi,j+ϵi,j> ϵ0,j] =
exp(vi,j)
1+exp( vi,j). Hence, the expected utility can be viewed as Ui,j= log(1
1−yi,j) +γ.
•Probit model. Let the utility of item ifor a type juser again take the form of vi,j+ϵi,j,
where the random part ϵi,jand the utility of the no-purchase option ϵ0,jare drawn i.i.d. from a
normal distribution N(0, σ). Here, the expected utility Ui,j=E[max{vi,j+ϵi,j, ϵ0,j}] =
vi,jΦ
vi,j√
2σ
+√
2σϕ
vi,j√
2σ
,where Φ(.)andϕ(.)are the CDF and PDF of a standard
normal distribution. The purchase probability is yi,j= Φ(vi,j√
2σ). We thus have Ui,j=√
2σΦ−1(yi,j)yi,j+√
2σϕ(Φ−1(yi,j)).
•Valuation-based model. Letvi,j∼Fi,jbe the value of item ifor a type juser, where Fi,jis
the distribution of vi,j. Then, yi,j=P[(vi,j−ri)≥0]andUi,j=E[(vi,j−ri)+], where ri
is the price (revenue) of item i. If the valuations follow exponential distribution vi,j∼Exp(λ)
for some λ >0, we have purchase probability yi,j= exp( −λri)and expected utility Ui,j=
1
λexp(−λri) =yi,j/λ.
A.2 Example Fairness Notions
Given a group of Sstakeholders, each indexed by s∈[S], and any function that maps the platform’s
decision xto the stakeholders’ outcome O(x)∈RS
+, we can solve the following optimization prob-
lem to ensure that each of the Sstakeholders are offered a fair outcome: f= arg max xW(O(x)).
where the social welfare function Wdetermines which fairness notion we adopt. In Table 1, we list
some example fairness notions commonly adopted in practice, as well as their corresponding social
welfare functions W. Among them, we have
•Maxmin fairness [58]: It maximizes the minimum outcome across all stakeholders, ensuring that
the stakeholder with the least favorable outcome is as well-off as possible.
•Kalai-Smorodinsky (K-S) fairness [41]: It seeks an equitable outcome where each stakeholder
receives a proportional share of their maximum possible outcome. This is represented by a
proportional allocation up to a common factor β.
•Hooker-Williams fairness [ 37]: It aims to balance the efficiency/equity tradeoff using parameter ∆,
by prioritizing stakeholders whose outcomes are within ∆of the minimum outcome.
•Nash bargaining solution : It maximizes the product of stakeholders’ outcomes, ensuring an
equitable distribution that reflects their relative negotiating power and achieves proportional fairness.
•Demographic parity : It ensures that outcomes are equally distributed across different demographic
groups, by minimizing the disparity in outcomes between a subset of stakeholders S′and the rest.
See [18] for a comprehensive overview for all of the above fairness notions.
15Table 1: Fairness notions and their social welfare functions (SWF).
Fairness notion Social welfare function (SWF): W(O(x))
Maxmin fairness [58] minsOs(x)
Kalai-Smorodinsky (K-S)
fairness [41]P
sOs(x)· 1{Os(x) =βmax xOs(x)for some βfor all s}
Hooker-Williams fairness [37]P
smax{Os(x)−∆,minsOs(x)}, for some ∆≥0
Nash bargaining solution [53]P
slogOs(x)
demographic parity 1−1
|S′|P
s∈S′Os(x)−1
S−|S′|P
s∈[S]\S′Os(x), for some S′⊂[S]
Choosing the right fairness notion is non-trivial and very much depends on the context. Some
important considerations include:
•Stakeholder needs and outcomes. Understanding the desired outcomes for items and users is
crucial. Our framework, Problem (FAIR ), is designed to handle various outcome functions (e.g.,
revenue, marketshare, visibility), which can differ across platforms. For example, a video streaming
platform might aim to ensure fair visibility for both independent content creators and popular
studios, while an e-commerce platform might focus more on fair marketshare/revenue for small
sellers versus large brands.
•Implication of fairness notion. Each fairness notion has a different implication, and platforms
need to evaluate which best suits their goals and stakeholder needs. For example, maxmin fairness
maximizes the outcome received by the most disadvantaged stakeholder, which can be ideal for
video streaming platforms like Netflix or YouTube that wish to ensure independent and lesser-
known content creators receive fair visibility alongside popular creators. K-S fairness ensures that
each individual receives a fair share of his/her maximum attainable outcome. This can be suitable
for platforms like LinkedIn or Indeed that wish to ensure fair opportunities for their job seekers
relative to their qualifications and experience. Platforms may also need to experiment with different
fairness notions to understand how their choices impact their "price of fairness" (see discussion in
Section C).
•Regulatory requirements. As we discussed in Section 1, one important motivation for imposing
fairness in online recommendations is the increasing regulatory action. Therefore, the choice of
fairness notions can also depend on legislative or regulatory requirements. For example, since the
Digital Markets Act [ 54] calls for a fair and open digital marketplace, maxmin fairness can be
potentially suitable as it ensures even the most disadvantaged item receive a fair level of exposure.
A.3 Example Item-Fair Solutions with Local Lipschitzness
In this section, we discuss a number of item-fair solutions that adopt different outcome functions and
fairness notions, and show that they satisfy Assumption 3.1 in Section 3.4. In the following, we will
assume that the item-fair solution satisfies the following condition:
Condition A.1 Under problem instance θ∈Θ, the item outcome Land the social welfare function
Ware chosen such that
(i)Li,jis Lipschitz in θfor all i∈[N], j∈[M].
(ii) The item-fair solution fIis unique.
Note that statement (i) in Condition A.1 is satisfied by all of the common definitions of item outcome,
such as (1) visibility: Li,j=pj; (2) marketshare: Li,j=pjyi,j; (3) expected revenue: Li,j=
ripjyi,j. On the other hand, the uniqueness condition can be achieved via adding regularization or
lexicographic optimization to the social welfare function. In light of Condition A.1, we enumerate
several item-fair solutions adopting prevalent fairness notions (see Section A.2 for definitions) such
that local Lipschitzness is satisfied, as outlined in Assumption 3.1.
Example 1: Item-Fair Solution with Maxmin Fairness. Suppose that an item-fair solution fI
adopts maxmin fairness, fIwould be the solution to the following linear program (LP):
max
x∈∆M
N,z∈Rzs.t.L⊤
i,:xi,:≥z∀i∈[N].
16Letθ= (p,y,r),˜θ= (˜p,˜y,˜r)∈Θbe two problem instances such that ∥θ−˜θ∥∞≤ζfor some
ζ >0. We consider the two item-fair solutions enforcing maxmin fairness under the two problem
instances. For simplicity of notation, we let fI=fI(θ)and˜fI=fI(˜θ);L=L(θ)and˜L=L(˜θ).
The item-fair solutions are obtained via the following:
(fI, z⋆) = arg max
x∈∆M
N,z∈Rzs.t.L⊤
i,:xi,:≥z∀i∈[N]. (3)
and
(˜fI,˜z⋆) = arg max
x∈∆M
N,z∈Rzs.t. ˜L⊤
i,:xi,:≥z∀i∈[N]. (4)
We note that
∥fI−˜fI∥∞≤ ∥(fI, z⋆)−(˜fI,˜z⋆)∥∞. (5)
Consider the region D={(x, z) :L⊤
i,:xi,:≥zfor all i∈[N], z≥z⋆}. Under Condition A.1, we
know that (fI, z⋆)is only feasible point in D. We can then invoke Lemma G.1 (with x= (fI, z⋆)
andx′= (˜fI,˜z⋆)and the feasibility region D) to bound the distance between (fI, z⋆)and(˜fI,˜z⋆)
using a Hoffman constant H > 0and a constraint violation term:
∥(fI,˜z⋆)−(˜fI,˜z⋆)∥∞≤H·max{max
i∥(˜z⋆−L⊤
i,:˜fI
i,:)+∥∞,(z⋆−˜z⋆)+}. (6)
where the Hoffman constant can be characterized by invoking Lemma G.2 for the matrix defining
region D.
Now, let us bound the two terms (i.e. max i∥(˜z⋆−L⊤
i,:˜fI
i,:)+∥∞and(z⋆−˜z⋆)+), on the right-hand
side of (6) respectively. Let us first denote E=∥L−˜L∥∞.
To bound the first term, given that for all i∈[N],˜fIsatisfies ˜L⊤
i,:˜fI
i,:≥˜z⋆,we must have
˜z⋆−L⊤
i,:˜fI
i,:≤(˜z⋆−˜L⊤
i,:˜fI
i,:) + ( ˜L⊤
i,:˜fI
i,:−L⊤
i,:˜fI
i,:)≤ME . (7)
To bound the second term, let us additionally consider the following auxiliary problem:
(ˆfI,ˆz⋆) = arg max
x∈∆M
N,z∈Rzs.t.L⊤
i,:xi,:≥z+ME ∀i∈[N]. (8)
Note that if L⊤
i,:xi,:≥z+ME, we must also have ˜L⊤
i,:xi,:≥L⊤
i,:xi,:−ME≥z. Hence, the
feasibility region of Problem (8)is included in the feasibility region of Problem (4), which implies
that
ˆz⋆≤˜z⋆. (9)
On the other hand, note that Problem (8) is equivalent to
max
x∈∆M
N,z′∈Rz′−ME s.t.L⊤
i,:xi,:≥z′∀i∈[N]. (10)
if we perform change of variable z′=z+ME . This implies that
z⋆= ˆz⋆+ME (11)
Equations (9) and (11) together imply that
z⋆−˜z⋆≤ME . (12)
Finally, combining Equations (5), (6), (7) and (12), we get
∥fI−˜fI∥∞≤ ∥(fI,˜z⋆)−(˜fI,˜z⋆)∥∞≤H·max{∥(˜z⋆−L⊤
i,:˜fI
i,:)+∥∞,(z⋆−˜z⋆)+}
≤H·ME=H·M∥L−˜L∥∞.
By statement (i) in Condition A.1, we thus establish the local Lipschitzness of item-fair solution fI
that enforces maxmin fairness.
Example 2: Item-Fair Solution with Hooker-Williams Fairness. If the platform adopts an item-fair
solution fIw.r.t. Hooker-Williams fairness with some given ∆>0(see definition in Section A.2),
17fIwould be the solution to a mixed linear integer program, with the following LP relaxation (it is
shown that the LP relaxation describes the convex hull of the feasibility set; see [37]):
max
x∈∆M
N,
z,vi,w∈R+,δi∈[0,1]zs.t. (N−1)∆ +NX
i=1vi≥z
L⊤
i,:xi,:−∆≤vi≤L⊤
i,:xi,:−∆δi∀i
w≤vi≤w+ (Γ−∆)δi ∀i(13)
Here, ∆>0is a constant that regulates the equity/efficiency tradeoff, and Γcan be any constant
such that Γ≥¯Lwhere ¯L=∥L∥∞. We would fix Γ = 2 M¯L+ ∆ in the following.
Letθ,˜θ∈Θbe two problem instances such that ∥θ−˜θ∥∞≤ζfor some ζ >0. For simplicity of
notation, we let L=L(θ)and˜L=L(˜θ). Here, using the same techniques as in maxmin fairness,
we let (fI, z⋆,v⋆, w⋆,δ⋆)denote the solution to Problem (13) under the problem instance θ, and
(˜fI,˜z⋆,˜v,˜w,˜δ)denote the solution to Problem (13) under the problem instance ˜θ. We can bound
the difference in the two item-fair solutions similarly by
∥fI−˜fI∥∞≤ ∥(fI, z⋆,v, w,δ)−(˜fI,˜z⋆,˜v,˜w,˜δ)∥∞
≤H·max{max
i∥(˜vi−(L⊤
i,:˜xi,:−∆˜δi))+∥∞,max
i∥((L⊤
i,:˜xi,:−∆)−˜vi)+∥∞,(z⋆−˜z⋆)+},
(14)
where the second inequality follows from uniqueness of fIin Condition A.1 and applying Lemma
G.1 to the feasibility region of Problem (13) with the additional constraint z≥z⋆. The argument
here is similar to what we did for item-fair solution with maxmin fairness. Here, His the Hoffman
constant associated with Problem (13) under instance θ, which can be characterized using Lemma
G.2.
It suffices to bound the three terms on the right hand side of (14). The first two terms can be bounded
using similar techniques as done in the case of maxmin fairness. Using E=∥L−˜L∥∞and the fact
that˜L⊤
i,:˜xi,:−∆≤˜vi≤˜L⊤
i,:˜xi,:−∆˜δifor all i∈[N], we have
˜vi−(˜L⊤
i,:˜xi,:−∆˜δi)≤ME and (˜L⊤
i,:˜xi,:−∆)−˜vi≤ME
for all i∈[N]. To bound the third term in (14), we again adopt a similar technique as above and
consider an auxiliary problem
max
x∈∆N,
z,vi,w∈R+,δi∈[0,1]zs.t. (N−1)∆ +NX
i=1vi≥z
L⊤
i,:xi,:−∆ +ME≤vi≤L⊤
i,:xi,:−∆δi−ME ∀i
w≤vi≤w+ (Γ−∆)δi ∀i(15)
Let(ˆfI,ˆz⋆,ˆv,ˆw,ˆδ)denote the solution to Problem (13). Consider the feasibility region of Problem
(13) under the problem instance ˜θ, which contains the feasibility region of Problem (15). This then
gives
ˆz⋆≤˜z⋆. (16)
We note that if we solve Problem (13) under problem instance, we claim that we must have δ⋆
i≤1/2.
This is because if δ⋆
i>1/2, we would have w+ (Γ−∆)δ⋆
i≥2M¯Lδ⋆
i>L⊤
i,:fI
i,:−∆δi. That is,
the upper bound in the third constraint is not tight, so the upper bound in the second constraint should
be tight. However, setting δ⋆
i= 1/2would yield a better objective.
Having this in mind, we choose ζ >0sufficiently small such that ME <1
4∆(this is doable since
ℓi(θ)is Lipschitz in θ. Then, Problem (15) is feasible. We note that (fI,ˆz,ˆv,ˆw,δ⋆)is a feasible
solution to Problem (15), where ˆvi=v⋆
i−ME andˆw=w−ME, ˆz=z⋆−NME . This is
because under the optimal solution (fI, z⋆,v⋆, w⋆,δ⋆)to Problem (13), both upper bounds for vi
should be tight (otherwise, there exists a δthat yields a better objective). We thus have
ˆz⋆≥ˆz=z⋆−NME . (17)
18Equations (16) and (17) together give
z⋆−˜z⋆≤NME ,
which bounds the third term in the right-hand side of (14).
Having established the bounds in the right-hand side of (14), we have shown that
∥fI−˜fI∥∞≤H·NME =H·NM∥L−˜L∥∞
By statement (i) in Condition A.1, we thus establish the local Lipschitzness of item-fair solution fI
that enforces Hooker-Williams fairness.
Example 3: Item-Fair Solution with Kalai-Smorodinsky (K-S) Fairness. To obtain item-fair
solution fIunder K-S fairness, we solve the following problem given problem instance θ:
max
x∈∆M
N,β∈[0,1]βs.t.L⊤
i,:xi,:
L⋆
i≥β∀i∈[N]. (18)
where L⋆
i=P
jLi,jdenotes the maximum outcome that item ican receive. Note that this is
achievable if the platform always show item iregardless of the type of the arriving user.
By similar arguments as in our discussion for maxmin fairness, we would get
∥fU−˜fU∥∞≤HE
miniL⋆
i=H
miniL⋆
i· ∥L−˜L∥∞.
where His the Hoffman constant that can be characterized by invoking Lemma G.2. By statement (i)
in Condition A.1, we thus have the local Lipschitzness of item-fair solution fIthat enforces K-S
fairness.
B Proof of Proposition 2.1
As we discussed in Section 2.3, if a platform adopts a recommendation that solely benefits a single
stakeholder group, this could result in extensive costs for the rest of the stakeholders, including the
platform itself. The following Example B.1 establishes Proposition 2.1, where we consider a problem
instance with one highly popular item and another notably profitable item. Under such a problem
instance, the perceptions of a “fair solution” can vary widely among different stakeholders.
Example B.1 (A single-sided solution can be extremely unfair to the other sides.) Consider a
problem instance with Nitems and Mtypes of users. For i∈[M], j∈[N], let the probability of
purchases y1,j= 1 andyi,j=ϵ1where ϵ1≪1for all i̸= 1; probability of arrival pj= 1/M;
revenue r2= 1/ϵ2
1andri= 1, for all i̸= 2. For simplicity, let the utility of the users Ui,j=yi,j.
Letϵ= max {ϵ1,1/N}.Under such a instance, we have the following.
Platform’s revenue-maximizing solution. A platform that seeks to maximize its revenue would always
recommend item 2to any arriving user, which yields expected revenue of 1/ϵ1≫1. However, such a
solution is extremely unfair for any items i̸= 2that receive zero outcome. This is also unfair to all
users, as they would receive utility 1from item 1but only receives utility ϵ1from item 2.
An item-fair solution. Suppose items consider visibility as their outcome and adopts maxmin fairness,
offering all items equal visibility xi,j= 1/Nis an item-fair solution. However, this can be unfair for
all users who prefers item 1, whose utility under the item-fair solution is 1/N+ (N−1)/Nϵ 1<2ϵ.
This is also unfair to the platform that prefers item 2, as its current revenue becomes 1/N·1/ϵ1+
1/N+ (N−2)/N·ϵ1<1/N·1/ϵ1+ 1 = 1 /N·1/ϵ1+ϵ1·1/ϵ1≤2ϵ·1/ϵ1.
A user-fair solution. A user-fair solution would always display item 1to all types of users. Nonethe-
less, such a solution is extremely unfair to the rest of the items that receives zero outcome. It is also
unfair to the platform, as always displaying item 1results in expected revenue of 1, while if it displays
item2it would gain 1/ϵ1.
C Price of Fairness
In this section, we formally characterize the conflicting interests among different stakeholders using a
concept called the price of fairness , and show that our formulation of Problem (FAIR )provides the
platform with flexible handles to find the right middleground.
19The concept of price of fairness was first introduced by [9], which we formally define as follows.
Definition C.1 (Price of Fairness) Given a problem instance θ∈Θ, an item-fair solution fIand
a user-fair solution fU. We let OPT-REV= max xREV(x)be the maximum achievable expected
revenue in the absence of fairness, and let FAIR -REV be the solution to Problem (FAIR ). The price of
fairness (PoF) is defined as
POF=OPT-REV−FAIR -REV
OPT-REV.
The price of fairness, which depends on the problem instance θand(δI, δU), quantifies the trade-off
between item/user fairness and platform interests, where a lower value is favored by the platform. In
Theorem C.1, we formally upper bound the price of fairness introduced by Problem (FAIR )when we
interpolate item and user fairness with parameters δI, δU.
Theorem C.1 Given a problem instance θ∈Θ, item-fair solution fIand user-fair solution fU. Let
xOPT= arg max xREV(x)be the revenue-maximizing solution in the absence of fairness. If we solve
Problem (FAIR )with parameters δIandδUand assuming that the problem is feasible, the price of
fairness is at most
POF≤H·maxn
max
i∈[N](δI·OI
i(fI)−OI
i(xOPT))+,max
j∈[M](δU·OU
j(fU)−OU
j(xOPT))+o
,
where constant His the Hoffman constant associated with Problem (FAIR )under instance θ(see
definition in Lemma G.2).
There are two important takeaways from Theorem C.1: (1) The price of fairness arises from the
misalignment of objectives, captured by the difference in items’/users’ outcomes under the single-
sided item/user-fair solution and the platform’s optimal revenue solution, xOPT(i.e.,(δI·OI
i(fI)−
OI
i(xOPT))+and(δU·OU
j(fU)−OU
j(xOPT))+. A high price of fairness can result from high divergence
of platform’s goals from item/user interests (as illustrated by Proposition 2.1 and Example B.1). (2)
Theorem C.1 also underscores the value of the parameters δIandδUin achieving a balance among
stakeholder interests. Both takeaways are further supported empirically, as we next investigate the
price of fairness associated with our case study on Amazon review data in Section C.1.
C.0.1 Proof for Theorem C.1
Recall from Section 2.4 that i⋆
j= arg max i∈[N]riyi,jis the item with the maximum expected revenue,
if an arriving user is of type j. (Here, we assumed that i⋆
jis unique without loss of generality.) The
platform’s revenue-maximizing solution is xOPT
i,j= 1ifi=i⋆
jandxOPT
i,j= 0otherwise.
Now, suppose that the platform solves Problem (FAIR )with some fairness parameters δI, δU∈[0,1]
and assuming Problem ( FAIR ) is feasible. Let
F={x∈∆M
N:OI
i(x)≥δI·OI
i(fI);OU
j(x)≥δU·OU
j(fU)∀i∈[N], j∈[M]}
denote the feasibility region of Problem ( FAIR ). We define
x′= arg min
x∈F∥x−xOPT∥∞. (19)
That is, x′is a feasible solution to Problem (FAIR )that is closest to xOPTw.r.t.∥·∥∞. Then, by
Lemma G.1, we have the following bound
∥x′−xOPT∥∞≤H·maxn
max
i∈[N](δI·L⊤
i,:fI
i,:−L⊤
i,:xOPT
i,:)+,max
j∈[M](δU·U⊤
:,jfU
:,j−U⊤
:,jxOPT
:,j)+,0o
,
(20)
where the first term His the Hoffman constant associated with the feasibility region F, and the
second term encapsulates how much xOPTviolates the item/user-fairness constraints.
Now, note that we also have
∥x′−xOPT∥∞(a)= max
j∈[M]max{1−x′
i⋆
j,j,max
i̸=i⋆
jx′
i,j}(b)= max
j∈[M]1−x′
i⋆
j,j (21)
20where (a) follows from the form of xOPT, and (b) follows from 1−x′
i⋆
j,j=P
i̸=i⋆
j,jx′
i,j≥x′
i,jfor
anyi̸=i⋆. This then allows us to bound the PoF as follows
POF=OPT-REV−FAIR -REV
OPT-REV≤1−REV(x′)
OPT-REV≤1−P
j∈[M]Ri⋆
jx′
i⋆
j,jP
j∈[M]Ri⋆
j≤max
j∈[M]1−x′
i⋆
j,j=∥x′−xOPT∥∞,
where the first inequality follows from x′being a feasible solution to Problem (FAIR ), the second
inequality follows from REV(x′)≥P
j∈[M]Ri⋆
jx′
i⋆
j,j, and OPT-REV=P
j∈[M]Ri⋆
j, and the final
equality follows from (21).
Finally, combining Eq. (20) and Eq. (22) finishes the proof. ■
C.1 Price of Fairness for Our Case Study on Amazon Review Data
In this section, we investigate the price of fairness from our case study on the Amazon review data
(Section 4) and shed light on how our fair recommendation framework, Problem (FAIR ), can help the
platform achieve the right middleground among multiple stakeholders.
In our case study, we act as an e-commerce site (such as Amazon) recommending a collection of at
most K= 3products to incoming users. There are a total of N= 30 items to be shown to M= 5
types of users, where the relevance score between items and users as well as users’ arrival rates are
both obtained from an Amazon review dataset [ 71]; see Section 4 for a complete description of the
dataset. Assuming that the platform has full knowledge of the problem instance, we solve Problem
(FAIR -ASSORT )(here, we consider the assortment extension of Problem (FAIR ); see Section E.2 for
details) under different values of fairness parameters δI, δUto investigate how much loss the platform
needs to endure in order to achieve various levels of fairness for its items/users.
The left-hand side of Figure 2 shows the price of fairness ( POF) endured by the platform given
different (δI, δU). Note that in our recommendation problem based on Amazon review data, item
fairness is much more difficult to achieve than user fairness, since (i) the number of items is much
larger, making the fair outcome of many items differentiate a lot from the outcome attained under
platform’s revenue-maximizing solution; and (ii) the objective of users (MNL utility) aligns fairly
well with the platform’s objective (revenue). As a result, the item-fair constraints are the binding
constraints in majority of the cases. This can be seen in the left-hand side plot, where we see that
POFincreases more significantly when we increase δI. This concurs with the first takeaway from
Theorem C.1, which suggests that the misalignment of objectives directly impacts the P OF.
In the right-hand side of Figure 2, we plot the evolution of PoF when we fix δU= 0(upper-right plot)
andδI= 0(lower-right plot) respectively. Given that the item fairness constraints are primarily the
binding constraints, as discussed above, we observe a linear increase in PoF for the platform as the
fairness parameter δIfor items increases. This observation again corroborates the findings presented
in Theorem C.1. For the users, in the case of Amazon review data, the misalignment of objectives
(δU·OU
j(fU)−OU
j(xOPT))+only becomes noteworthy as δUgets close to 1. This indicates that under
the Amazon review data, the platform can potentially obtain a high level of user-fairness at very little
cost, which also matches our findings in our case study in Section 4.
0.00.10.20.30.40.50.60.70.80.91.0
U
0.00.10.20.30.40.50.60.70.80.91.0I
-0%10%19%29%39%
0.0 0.2 0.4 0.6 0.8 1.0
I
0%25%PoF (%)
0.0 0.2 0.4 0.6 0.8 1.0
U
0%25%PoF (%)
Figure 2: Price of Fairness ( POF) in our case study on the Amazon review data. Left: PoF when
solving Problem (FAIR -ASSORT )under different fairness parameters (δI, δU). The grid is colored
black if the problem is infeasible. Upper-right: PoF when δU= 0. Lower-right: PoF when δI= 0.
21Insights from the case study. Our empirical analysis provides practical guidelines for selecting
appropriate fairness parameters δI, δUin real-world settings. Specifically, platforms should focus
on (i) identifying which fairness constraints are binding and (ii) evaluating the degree of objective
misalignment, typically captured by the slope between the PoF and the fairness parameters.
One potential approach is to first identify the binding constraints by assessing which stakeholders
experience the most unfair outcomes under the current recommendation policy. Then, using the
piecewise linear relationship established in Theorem C.1, the platform can estimate the tradeoff
between the binding constraints and PoF. Based on the desired fairness levels and acceptable PoF, the
platform can now narrow down the range of fairness parameters to experiment with. Once a small
subset of promising fairness parameters is identified, a platform can conduct online A/B tests by
splitting its traffic to experiment with these parameters in parallel, thus selecting the optimal fairness
parameters efficiently without extensive trial-and-error.
D Proof for Theorem 3.1
D.1 Definitions and Lemmas
We first state a few definitions and lemmas that are useful for the proof of Theorem 3.1.
Good event and cutoff time. We define the good event at round t, denoted by Et, as the event under
which our estimates for yandpare accurate w.r.t. t.
Definition D.1 (Good event) At round t, a good event Etis
Et={∥ˆpt−p∥1≤Γp,t} ∩ {∥ ˆyt−y∥∞≤Γy,t}, (22)
where
Γy,t= 2 log( T)/p
m(t)ϵtandm(t) = maxn
1, t/log(T)−p
tlog(T)/2o
Γp,t= 5p
log(3 T)/t .(23)
Observe that Γp,tstrictly decreases to 0 as t→ ∞ . This is also the case for Γy,t. To see that, note that
m(t), used to define Γy,t, strictly increases in tand goes to ∞ast→ ∞ . Hence, we can additionally
define the cutoff time T, which is time after which the good event becomes “sufficiently good”:
T= minn
t∈[T] : Γy,t<1− ∥y∥∞,max{Γy,t,Γp,t} ≤ζ,andm(t)>1}=O(1).(24)
where ζis defined in Assumption 3.1. Here we also used yi,j∈(0,1)for all i∈[N], j∈[M],
which is assumed in Section 2.1, so we must have ∥y∥∞<1. Here, note that Γy,t=O(t−1
3),
Γp,t=O(t−1
2)soT=O(max{ζ ,∥y∥∞}3)and isO(1)w.r.t. T.
The following lemma states that under the good event, for t >0that is sufficiently large, the
optimal solution x⋆to Problem (FAIR )is feasible to the relaxed problem under the estimated instance,
Problem (FAIR -RELAX (ˆθt, ηt)), while our solution ˆxtis also feasible to the relaxed problem under
the ground-truth instance, Problem ( FAIR -RELAX (θ, ηt)). The proof is presented in Section D.3.
Lemma D.1 (Feasibility under the good event) Given problem instance θ∈Θ. Suppose that
Assumption 3.1 holds, where Bis the Lipschitz constant. Let constant ¯Ube the maximum utility in the
small neighborhood around θ; that is, ∥U(˜θ)∥∞≤¯Ufor all ˜θ∈Θsuch that ∥θ−˜θ∥ ≤ζ. Under
the good event Et, when t >T(defined in (24)) and log(T)>(2 +∥r∥∞)B, we have the following:
(i)∥ˆyt∥∞<1, soxt,i,j= (1−Nϵt)ˆxt,i,j+ϵtaccording to FORM .
(ii)x∗is feasible to Problem (FAIR -RELAX (ˆθt, ηt)).
(iii)ˆxtis feasible to Problem ( FAIR -RELAX (θ,2ηt))
where ˆθtandηtare defined in Eq. (1)and Eq. (2).
The second Lemma (Lemma D.2) shows that the good event happens with high probability. Its proof
is deferred to Section D.4.
Lemma D.2 (Concentration bounds for estimates) Assume log(T)>1/minj∈[M]pj. Then for
anyt >Tdefined in Eq. (24), we have P(Ec
t+1)≤(MN + 2)/T .
22D.2 Complete Statement and Proof of Theorem 3.1
Theorem D.3 (Complete statement of Theorem 3.1) Given problem instance θ∈Θand assume
that Assumption 3.1 holds. Then, for log(T)>maxn
(2 +∥r∥∞)B,1
minj∈[M]pjo
, we have
•the revenue regret of FORM is at most E[R(T)]≤ O(MN1
3T−1
3)
•the fairness regret of FORM is at most E[RF(T)]≤ O(MN1
3T−1
3).
Proof of Theorem D.3. We bound the revenue regret and the fairness regret respectively.
(1) Bounding revenue regret.
For a given t >T, assume that the good event Etdefined in Definition D.1 holds. Let R=
(Ri,j)i∈[N]
j∈[M], where Ri,j=ripjyi,jdenotes the expected revenue that the platform receives from
offering item ito user of type- j. That is, REV(x,θ) =P
i∈[N]
j∈[M]Ri,jxi,j. Similarly, for simplicity of
notation, let ˆRt= (ˆRt,i,j)i∈[N]
j∈[M], where ˆRt,i,j=riˆpt,jˆyt,i,j.
Let us first rewrite the revenue regret at round tas the following:
REV(x⋆,θ)−REV(xt,θ) =X
i∈[N]
j∈[M]Ri,jx⋆
i,j−X
i∈[N]
j∈[M]Ri,jxt,i,j
=X
i∈[N]
j∈[M]ˆRt,i,j(x⋆
i,j−xt,i,j) +X
i∈[N]
j∈[M]x⋆
i,j(Ri,j−ˆRt,i,j)−X
i∈[N]
j∈[M]xt,i,j(Ri,j−ˆRt,i,j)
=X
i∈[N]
j∈[M]ˆRt,i,j(x⋆
i,j−xt,i,j) +X
i∈[N]
j∈[M](x⋆
i,j−xt,i,j)(Ri,j−ˆRt,i,j)
We then have
REV(x⋆,θ)−REV(xt,θ)
(a)= (1−Nϵt)X
i∈[N]
j∈[M]ˆRt,i,j(x⋆
i,j−ˆxt,i,j) +NϵtX
i∈[N]
j∈[M]ˆRt,i,jx⋆
i,j−ϵtX
i∈[N]
j∈[M]ˆRt,i,j+X
i∈[N]
j∈[M](x⋆
i,j−xt,i,j)(Ri,j−ˆRt,i,j)
(b)
≤NϵtX
i∈[N]
j∈[M]ˆRt,i,jx⋆
i,j−ϵtX
i∈[N]
j∈[M]ˆRt,i,j+X
i∈[N]
j∈[M](x⋆
i,j−xt,i,j)(Ri,j−ˆRt,i,j)
≤NMϵ t∥ˆRt∥∞+ 2M· ∥R−ˆRt∥∞
≤NMϵ t∥R−ˆRt∥∞+NMϵ t∥R∥∞+ 2M· ∥R−ˆRt∥∞
(c)
≤3M∥R−ˆRt∥∞+NMϵ t∥R∥∞
(25)
where (a) follows from statement (i) in Lemma D.1, i.e., xt,i,j= (1−Nϵt)ˆxt,i,j+ϵt; (b) follows
from the fact that ϵt≤1
Nso1−Nϵt≥0, and by statement (ii) Lemma D.1, we have that x⋆and
ˆxtare both feasible to Problem (FAIR -RELAX (ˆθt, ηt)), and hence REV(x⋆,ˆθt)≤REV(ˆxt,ˆθt); (c)
follows from the fact that ϵt≤1
N.
Note that since
ˆRt,i,j−Ri,j=riˆyt,i,jˆpt,j−riyi,jpj
=ri(ˆyt,i,jˆpt,j−yi,jˆpt,j+yi,jˆpt,j−yi,jpj)
=ri
(ˆyt,i,j−yi,j)ˆpj+yi,j(ˆpt,j−pj)
≤ri
∥ˆyt−y∥∞+∥y∥∞· ∥ˆpt−p∥1
≤ri
Γy,t+ Γp,t
.(26)
We thus have
REV(x⋆,θ)−REV(xt,θ)≤3M¯r(Γy,t+ Γp,t) +NMϵ t∥R∥∞. (27)
23Hence, for any t >T, letFt−1be the sigma-algbra generated from all randomness up to round t−1,
and
E[REV(x⋆,θ)−REV(xt,θ)]
=Eh
Eh
REV(x⋆,θ)−REV(xt,θ)Ft−1ii
≤Eh
Eh
(REV(x⋆,θ)−REV(xt,θ))I{Et}Ft−1ii
+E[REV(x⋆,θ)I{Ec
t}]
(a)
≤3M¯r
Γy,t+ Γp,t
+NMϵ t∥R∥∞+∥R∥∞P(Ec
t)
(b)
≤3M¯r
Γy,t+ Γp,t
+NMϵ t∥R∥∞+(MN + 2)∥R∥∞
T(28)
where (a) follows from (27); (b) follows from the high probability bound for event Etin Lemma D.2,
given that t >Tholds.
Finally, by (28), we have
1
TX
t∈[T]Eh
REV(x⋆,θ)−REV(xt,θ)i
≤ OT
T+M
TX
t>T(Γp,t+ Γy,t) +NM
TX
t>Tϵt
=O(MN1
3T−1
3)
(29)
where in the final equality we recall ϵt= min
N−1, N−2
3t−1
3	
andΓy,t= 2 log( T)/p
m(t)ϵt<
2 log( T)q
(t/log(T)−√
tlog(T)/2)ϵt=O(N1
3t−1
3)since m(t)>1for all t >Tdefined in Eq. (24).
(2) Bounding constraint violation.
Item-Fair Constraints. For any item i∈[N], and any t >T, under the good event Etdefined in
Definition D.1, we have
δI·Li,:(θ)⊤fI
i,:(θ)−Li,:(θ)⊤xt,i,:(a)=δI·Li,:(θ)⊤fI
i,:(θ)−Li,:(θ)⊤
(1−Nϵt)ˆxt,i,:+ϵteM
≤δI·Li,:(θ)⊤fI
i,:(θ)−Li,:(θ)⊤ˆxt,i,:+NM∥Li,:(θ)∥∞ϵt
(b)
≤2Mlog(T) max{Γp,t,Γy,t}+NM∥Li,:(θ)∥∞ϵt
(30)
where (a) follows from Lemma D.1 part (i), which states that under the good event Etfort >T, we
havext,i,:= (1−Nϵt)ˆxt,i,:+ϵteMaccording to Algorithm 1; (b) follows from Lemma D.1 part (iii),
which states ˆxtis feasible to Problem ( FAIR -RELAX (θ, ηt)), where ηt= log( T) max{Γp,t,Γy,t}
according to Eq. (1).
Hence, we have
1
TTX
t=1Eh
(δI·Li,:(θ)⊤fI
i,:(θ)−Li,:(θ)⊤xt,i,:)+i
(a)
≤T
Tmax{1,¯r}+1
TX
t>TEh
(δI·Li,:(θ)⊤fI
i,:(θ)−Li,:(θ)⊤xt,i,:)+i
(b)
≤T
Tmax{1,¯r}+1
TX
t>TEh
δI·Li,:(θ)⊤fI
i,:(θ)−Li,:(θ)⊤xt,i,:+
I{Et}i
+P(Ec
t) max{1,¯r}
(c)
≤T
Tmax{1,¯r}+1
TX
t>T[2Mlog(T) max{Γp,t,Γy,t}+NM∥L(θ)∥∞ϵt] +P(Ec
t) max{1,¯r}
(d)
≤T
Tmax{1,¯r}+1
TX
t>T[2Mlog(T) max{Γp,t,Γy,t}+NM∥L(θ)∥∞ϵt] +(NM + 2) max {1,¯r}
T
=O(MN1
3T−1
3)
(31)
24where (a) and (b) follows from δI≤1andLi,:(θ)⊤fI
i,:(θ)≤max{1,¯r}depending on the item’s
outcome; (c) follows from Eq. (30); (d) follows from Lemma D.2 for sufficiently large T. Therefore,
the time-averaged item-fairness constraint violation for any item iis at most O(MN1
3T−1
3).
We can perform the same analysis for user-fairness constraints and obtain the same upper bound. The
proof is thus omitted.
D.3 Proof of Lemma D.1
We prove the three statements in Lemma D.1 respectively as follows.
Proof for (i). Consider the following under the good event Et, defined in Eq. (D.1):
∥ˆyt∥∞− ∥y∥∞≤ ∥ˆyt−y∥∞≤Γy,t(a)
<1− ∥y∥∞, (32)
where (a) follows from t >Tand the definition of Tin(24). This gives ∥ˆyt∥∞<1, and hence
given the design of FORM ,xt,i,j= (1−Nϵt)ˆxt,i,j+ϵt.
Proof for (ii). Here, we would like to show that x∗is feasible for Problem (FAIR -RELAX (ˆθt, ηt)),
where ˆθtandηtare defined in Eq. (1) and Eq. (2).
Item-Fair Constraints. Let¯L=∥L∥∞. For any i∈[N]andx∈∆M
N, under Assumption 3.1, we
haveLi,:(ˆθt)⊤xi,:−Li,:(θ)⊤xi,:≤ ∥Li,:(ˆθt)−Li,:(θ)∥∞∥xi,:∥1≤MB· ∥ˆθt−θ∥∞=MB·max{Γp,t,Γy,t}.
(33)
On the other hand, for any i∈[N], we also have
Li,:(ˆθt)⊤fI
i,:(ˆθt)−Li,:(θ)⊤fI
i,:(θ)
=Li,:(ˆθt)⊤fI
i,:(ˆθt)−Li,:(ˆθt)⊤fI
i,:(θ) +Li,:(ˆθt)⊤fI
i,:(θ)−Li,:(θ)⊤fI
i,:(θ)
≤Li,:(ˆθt)⊤
fI
i,:(ˆθt)−fI
i,:(θ)+
Li,:(ˆθt)−Li,:(θ)⊤
fI
i,:(θ)
≤M¯L· ∥fI
i,:(ˆθt)−fI
i,:(θ)∥∞+M· ∥Li,:(ˆθt)−Li,:(θ)∥∞
≤M¯LB· ∥ˆθt−θ∥∞+MB· ∥ˆθt−θ∥∞
≤MB(¯L+ 1) max {Γp,t,Γy,t}(34)
where the last inequality from the fact that for all t >T(see definition in (24)), under the good event
Et:
∥ˆθt−θ∥∞= max {∥ˆyt−y∥∞,∥ˆpt−p∥∞} ≤max{∥ˆyt−y∥∞,∥ˆpt−p∥1} ≤max{Γy,t,Γp,t}.
Since x⋆∈∆M
Nis the optimal solution to Problem (FAIR ), we know that Li,:(θ)⊤x⋆
i,:≥δI·
Li,:(θ)⊤fI
i,:(θ). We thus have
Li,:(ˆθt)⊤x⋆
i,:+MB·max{Γp,t,Γy,t} ≥Li,:(θ)⊤x⋆
i,:≥δI·Li,:(θ)⊤fI
i,:(θ)
≥δI·Li,:(ˆθt)⊤fI
i,:(ˆθt)−δIMB(¯L+ 1)·max{Γp,t,Γy,t}
(35)
where the first inequality holds because of Eq. (33) and the third inequality holds because of Eq. (34).
Hence we have
Li,:(ˆθt)⊤x⋆
i,:≥δI·Li,:(ˆθt)⊤fI
i,:(ˆθt)− 
MB+δIMB(¯L+ 1)
·max{Γp,t,Γy,t}
(a)=⇒Li,:(ˆθt)⊤x⋆
i,:≥δI·Li,:(ˆθt)⊤fI
i,:(ˆθt)−Mlog(T) max{Γp,t,Γy,t},(36)
where (a) follows from log(T)>(2 + ¯r)B > B +δIB(¯L+ 1) . Since the amount of relaxation
isηt=Mlog(T) max{Γp,t,Γy,t}, as defined in Eq. 1, we show that x⋆satisfies the item-fair
constraints for Problem ( FAIR -RELAX (ˆθt, ηt)).
25User-Fair Constraints. For any j∈[M]andx∈∆M
N, under Assumption 3.1, we have
U:,j(ˆθt)⊤x:,j−U:,j(θ)⊤x:,j≤ ∥U:,j(ˆθt)−U:,j(θ)∥∞∥x:,j∥1≤B∥ˆθt−θ∥∞=Bmax{Γp,t,Γy,t}.
(37)
Also note that for any problem instance θ′∈Θ, we have that
U:,j(θ′)⊤fU
:,j(θ′) = max
i∈[N]Ui,j(θ′)
Hence, we have the following:
U:,j(ˆθt)⊤fU
:,j(ˆθt)−U:,j(θ)⊤fU
:,j(θ)=max
i∈[N]Ui,j(ˆθt)−max
i∈[N]Ui,j(θ)
≤∥U:,j(ˆθt)−U:,j(θ)∥∞≤B∥ˆθt−θ∥∞=Bmax{Γp,t,Γy,t}.
(38)
where the second inequality follows from local Lipschitzness of U(θ).
Now, since x⋆∈∆M
Nis the optimal solution to Problem (FAIR ), we know that U:,j(θ)⊤x⋆
:,j≥
δU·U:,j(θ)⊤fU
:,j(θ). Combining this with the Eq. (37) and Eq. (38), we have
U:,j(ˆθt)⊤x⋆
:,j+Bmax{Γp,t,Γy,t} ≥U:,j(θ)⊤x⋆
:,j≥δU·U:,j(θ)⊤fU
:,j(θ)
≥δU·U:,j(ˆθt)⊤fU
:,j(ˆθt)−δUB·max{Γp,t,Γy,t}.(39)
Hence we have
U:,j(ˆθt)⊤x⋆
:,j≥U:,j(ˆθt)⊤fU
:,j(ˆθt)− 
1 +δU
Bmax{Γp,t,Γy,t}
=⇒U:,j(ˆθt)⊤x⋆
:,j≥U:,j(ˆθt)⊤fU
:,j(ˆθt)−Mlog(T) max{Γp,t,Γy,t}.(40)
Proof of (iii). We can show ˆxtis feasible to Problem ( FAIR -RELAX (θ, ηt)) via a similar argument as
our proof for (ii)
Item-Fair Constraints. First note that
Li,:(θ)⊤ˆxt,i,:+MBmax{Γp,t,Γy,t}
(a)
≥Li,:(ˆθt)⊤ˆxt,i,:
(b)
≥δI·Li,:(ˆθt)⊤fI
i,:(ˆθt)−Mlog(T) max{Γp,t,Γy,t}
(c)
≥δI·Li,:(θ)⊤fI
i,:(θ)−Mlog(T) max{Γp,t,Γy,t} −δIMB(¯L+ 1) max {Γp,t,Γy,t}.(41)
Here, (a) follows from an identical argument as shown in (33), while replacing xwith ˆxt; (b)
follows from feasibility of ˆxtto Problem ( FAIR -RELAX (θ, ηt)); (c) follows from (34). Hence, since
log(T)>(2 + ¯r)B > B +δIB(¯L+ 1) , we conclude
Li,:(θ)⊤ˆxt,i,:≥δI·Li,:(θ)⊤fI
i,:(θ)−2Mlog(T) max{Γp,t,Γy,t}. (42)
User-Fair Constraints. We again follow the same arguments as in our proof for (ii) and establish the
following:
U:,j(θ)⊤ˆxt,:,j+Bmax{Γp,t,Γy,t}
≥U:,j(ˆθt)⊤ˆxt,:,j
≥δU·U:,j(ˆθt)⊤fU
:,j(ˆθt)−Mlog(T) max{Γp,t,Γy,t}
≥δU·U:,j(θ)⊤fU
:,j(θ)−Mlog(T) max{Γp,t,Γy,t} −δUBmax{Γp,t,Γy,t},(43)
which then gives
U:,j(θ)⊤ˆxt,:,j≥δU·U:,j(θ)⊤fU
:,j(θ)−2Mlog(T) max{Γp,t,Γy,t}. (44)
26D.4 Proof of Lemma D.2
For completeness, let us recall the definitions of Γy,tandm(t)in Eq. (23):
Γy,t=2 log( T)p
m(t)ϵt, m(t) = maxn
1,t
log(T)−p
tlog(T)/2o
.
We define related variables eΓy,j,tandemj(t)as followed:
eΓy,j,t=2 log( T)p
emj(t)ϵt,emj(t) = max {1, tpj−p
tlog(T)/2}. (45)
Since for log(T)>1
minj∈[M]pjandt >T(see definition of the cutoff time Tin(24)) such that
m(t)>1, we know emj(t)≥m(t)>1and thus Γy,t≥eΓy,j,tfor any j∈[M]. We thus have
P(|ˆyi,j,t+1−yi,j| ≥Γy,t)≤P
|ˆyi,j,t+1−yi,j| ≥eΓy,j,t
(46)
Hence,
P(Ec
t)≤P(∥ˆyt−y∥∞≥Γy,t) +P(∥ˆpt−p∥1≥Γp,t)
≤X
i∈[N]X
j∈[M]P(|ˆyi,j,t+1−yi,j| ≥Γy,t) +P(∥ˆpt−p∥1≥Γp,t)
(a)
≤X
i∈[N]X
j∈[M]P
|ˆyi,j,t+1−yi,j| ≥eΓy,j,t
+P(∥ˆpt−p∥1≥Γp,t)
(b)
≤X
i∈[N]X
j∈[M]P
|ˆyi,j,t+1−yi,j| ≥eΓy,j,t
+1
T,(47)
where (a) follows from (46); (b) follows directly from the concentration inequality to bound empirical
distribution estimates; in particular, we use Lemma G.3 and take δ=1
T. Hence, in the following, it
suffices to bound P
|ˆyi,j,t+1−yi,j| ≥eΓy,j,t
for any i∈[N], j∈[M].
Recall that nj,tis the total number of type jarrivals up to time t. Then, consider the following
P
|ˆyi,j,t+1−yi,j| ≥eΓy,j,t
=P
|ˆyi,j,t+1−yi,j| ≥eΓy,j,t, nj,t≥emj(t)
+P
|ˆyi,j,t+1−yi,j| ≥eΓy,j,t, nj,t<emj(t)
≤P
|ˆyi,j,t+1−yi,j| ≥eΓy,j,t, nj,t≥emj(t)
| {z }
A+P(nj,t<emj(t))| {z }
B.
(48)
We would bound terms AandBrespectively.
Bounding A.Recall that τj,k∈[T]is the round at which the kth consumer of type jarrived.
A=Pˆyt+1,i,j−yi,j≥eΓy,j,t, nj,t≥emj(t)
≤Pmax
emj(t)≤n≤t1
nnX
k=1I{Iτj,k=i, zτj,k= 1}
xτj,k,i,j−yi,j≥eΓy,j,t, nj,t≥emj(t)
≤Pmax
emj(t)≤n≤t1
nnX
k=1I{Iτj,k=i, zτj,k= 1}
xτj,k,i,j−yi,j≥eΓy,j,t
≤X
emj(t)≤n≤tP1
nnX
k=1I{Iτj,k=i, zτj,k= 1}
xτj,k,i,j−yi,j≥eΓy,j,t
≤X
emj(t)≤n≤tP1
nnX
k=1I{Iτj,k=i, zτj,k= 1}
xτj,k,i,j−yi,j≥2 log( T)√nϵt
,(49)
27where in the final inequality we used the definition of eΓy,j,tin(45) and the fact that n≥emj(t). Note
that for any k∈N, we have EhI{Iτj,k=i,zτj,k=1}
xτj,k,i,jFτj,ki
=yi,j, where Fτj,kis the sigma-algebra
generated from all randomness up to round τj,k. Therefore, Mk=I{Iτj,k=i,zτj,k=1}
xτj,k,i,j−yi,jis a
Martingale difference sequence. Further, we have
−1≤ M k≤1
xτj,k,i,j(a)
≤1
ϵτj,k≤1
ϵt(b)=⇒ |M k| ≤1
ϵt, (50)
where (a) follows from the fact that xτj,k,i,j=1
N≥ϵτj,kif∥ˆyt∥∞≥1andxτj,k,i,j= (1−
Nϵt)ˆxτj,k,i,j+ϵτj,k≥ϵτj,kif∥ˆyt∥∞<1according to FORM (Algorithm 1); (b) follows from
ϵt≤1
Nso1
ϵt≥N≥1, and hence Mk≥ −1≥ −1
ϵt. Also,
Eh
M2
kFτj,ki
=E"
I{Iτj,k=i, zτj,k= 1}
x2
τj,k,i,jFτj,k#
−y2
i,j≤1
xτj,k,i,j≤1
ϵt. (51)
Hence, by Bernstein’s inequality (see Lemma G.4), we have
PX
k∈[n]Mk≥p
(e−2)Vnlog(2 /δ)
≤δ
(a)=⇒P1
nnX
k=1I{Iτj,k=i, zτj,k= 1}
xτj,k,i,j−yi,j≥2 log( T)√nϵt
≤2
T2,(52)
where (a) follows from invoking Lemma G.4 by taking ck=1
ϵtandVk=2klog(T)
(e−2)ϵt, and δ=1
2T2.
Combining Eq. (49) and Eq. (52) yields
A≤X
emj(t)≤n≤t2
T2≤2t
T2≤2
T. (53)
Bounding B.Denote Mt=I{Jt=j} −pjand it is easy to see Mtis a Martingale difference
sequence bounded between [−1,1]. Hence, by recognizing nj,t−t·pj=P
τ∈[t]Mτand applying
the Azuma-Hoeffding inequality, for any ϵ >0, we have
P
nj,t−t·pj<−ϵ
=PX
τ∈[t]Mτ<−ϵ
≤exp
−2ϵ2
t
=⇒B=P
nj,t<emj(t)
=P
nj,t−t·pj<−p
tlog(T)/2(a)
≤1
T,(54)
where in (a) we take ϵ=p
tlog(T)/2.
E Extensions to Additional Setups
Our fair online recommendation framework, Problem (FAIR ), and method FORM can be extended to
accommodate additional variants of our setup. In Section E.1, we discuss how our framework/method
can additionally handle periodic arrivals with the same performance guarantees. In Section E.2, we
detail how our framework/method extends to handling the assortment recommendation setting.
E.1 Fair Online Recommendation with Periodic Arrivals
In real-world scenarios, user arrivals are often non-stationary, displaying variations over time due to
periodic user preferences and behavior. This is particularly evident in many recommendation systems
that exhibit daily or weekly periodicity, with consistent patterns of user arrivals and interactions
throughout the day or week. For instance, in an e-commerce setting, higher user activity and
engagement may be observed during lunch breaks, evenings, or late-night browsing. Similarly,
recommendation systems may experience increased user engagement during weekends compared
28to weekdays. Recognizing the presence of these periodic patterns, we extend our concept of a fair
solution and our algorithm FORM to accommodate the online setting with periodic arrivals.
Consider a scenario where users arrive in each round t∈[T]according to a probability distribution
pt∈∆M. Additionally, suppose that there exists a period length Q∈(0, T)and distributions
ep(1). . .ep(Q)such that pmQ+q=ep(q)for any m= 0,1,2. . .andq= 0,1. . . Q−1. By defining p
as the time-averaged arrival distribution, represented by p=1
TP
t∈[T]pt, we can generalize the fair
recommendation problem, Problem ( FAIR ), to incorporate this periodic arrival setup.
Yet, similar to the stationary setting, the platform does not have prior knowledge of the sequence of
user arrival distributions p1. . .pT, nor the time-averaged distribution p. Although solving Problem
(FAIR )w.r.t. the time-averaged distribution pseems to be more challenging compared to that under
stationary arrivals, we can in fact still obtain accurate estimates by slightly modifying the design of
FORM as follows: instead of estimating pusing all historical data (as done in Eq. (2)), we only obtain
estimates over a sliding window of length W>0. That is, we estimate the arrival rates of type- juser
at round t+ 1via the following:
ˆpt+1,j=1
min{t,W}tX
τ=max{1,t−W+1}I{Jt=j}. (55)
By doing so and keeping all other procedures unchanged, we show in Theorem E.1 that FORM would
yield the same theoretical guarantees for its final output under the setting with periodic arrivals, when
the size of the sliding window size Wis chosen appropriately.
Theorem E.1 (Performance of FORM under Periodic Arrivals) If we apply FORM (Algorithm 1)
with an estimated arriving probability ˆpt+1,jtaking the form in Eq. (55) andΓp,t=5√
log(3T)√
Wwith
a sliding window of size W=QT2
3. Then, for sufficiently large T, we have the following:
•the revenue regret is at most E[R(T)]≤ O(MN1
3T−1
3)
•the fairness regret is at most E[RF(T)]≤ O(MN1
3T−1
3)
The proof of Theorem E.1 is provided in Section E.1.1.
Remark E.1 (Lack of knowledge of the period length) We remark that in the periodic setup, we
assume knowledge of the period length Q > 0. This assumption aligns with practical scenarios
where platforms are aware of intraday, daily, weekly, or seasonal user arrival cycles. However, if
the period length Qis unknown, one might consider employing a standard meta “expert” algorithm
from the bandit literature on top of FORM . This approach involves using sliding window lengths
as experts, representing different expert windows in the set A=aT2
3:a= 1,2. . .¯Qfor some
¯Q > Q . Each expert produces estimates of pand the fair solution based on their window length. We
maintain weights over these estimates using an EXP3 algorithm. However, while this approach may
eventually approximate the cumulative revenue of the best expert with the correct period length Q,
the expected constraint violation w.r.t. the true parameters θmay not be sublinear or vanishing over
time. This is because the estimates from experts with incorrect period lengths can significantly violate
constraints, resulting in overall large constraint violations. Therefore, optimizing the fair solution
under a periodic arrival setup with an unknown period length remains an open question.
E.1.1 Proof of Theorem E.1
First, similar to Lemma D.2 that presents a concentration bound for empirical estimates of pusing all
historical data, if we construct an empirical estimate for pwith Eq. (55) using sliding window length
W, again applying Lemma G.3, we have for any t >Ws.t.
P
∥p−ˆpt∥1>5p
log(3 T)√
W
≤1
T(56)
Recall the definition of Tin Eq. (24). We additionally define
T= max {W+ 1,T }=O(WT2
3). (57)
29Following the same analysis as bounding the regret for Theorem 3.1 in Eq. (29), while replacing T
withTand considering Γp,t=5√
log(3T)√
W, we have
1
TX
t∈[T]Eh
REV(x⋆,θ)−REV(xt,θ)i
≤ OT
T+M
TX
t>T(Γp,t+ Γy,t) +MN
TX
t>Tϵt
=O(MN1
3T−1
3).
A similar analysis as Eq. (31) applies to bounding the fairness regret RF(T).■
E.2 Fair Online Assortment Recommendation
Our model in Section 2 primarily considered a setting where a single item is offered to each user at
each round, which applies to various real-world settings. In this section, we discuss an extension of
our framework/method to a setting in which the platform might wish to recommend an assortment
to its users, which applies to real-world scenarios such as job recommendation, where a number of
most relevant jobs are recommended to a candidate, or e-commerce sites, where a small assortment
of items might be featured on the front page whenever a user arrives.
Extension of our framework. To consider an assortment recommendation setting, we let wi,j>0be
theweight associated with each item ifor a type- juser. If a type- juser arrives onto the platform and
gets offered assortment S, he/she will choose/purchase item ifrom the assortment with probability
wi,j
1+wj(S), where wj(S) =P
i∈Swi,j, based on the MNL model [ 65]. This is different from the
single-item recommendation setting, where we have well-defined purchase probability yi,jfor each
item-user pair. When we recommend assortments, the purchase probability not only depends on an
item’s weight, but also the assortment it gets presented in. Given that, we define the problem instance
asθ= (p,w,r).
Our fair recommendation framework, Problem (FAIR ), can be readily extended by letting our decision
variable be q={qj(S) :S⊆[N],|S| ≤K, j∈[M]}, where qj(S)denote the probability of
presenting assortment Sto a type- juser. Then, we can formulate the fair recommendaton problem
under the same idea:
max
qREV(q)s.t. OI
i(q)≥δI·OI
i(fI)∀i∈[N]
OU
j(q)≥δU·OU
j(fU)∀j∈[M],(FAIR -ASSORT )
where
REV(q) =X
j∈[M]pjX
S∈[N],|S|≤Kqj(S)P
i∈Sriwi,j
1 +wj(S)
denote the expected revenue under recommendation q. On the other hand, the forms of the item/user
outcome functions OI
i(q,θ), OU
j(q,θ)as well as their respective fair solutions fI(θ),fU(θ)can
again take any forms that depend on the problem instance θ, based on their own needs.
Extension of our algorithm for the online setting. Given the extended framework Problem
(FAIR -ASSORT ), we can similarly extend the algorithm FORM for the assortment recommendation
setting, which is outlined in Algorithm 2.
Here, Algorithm 2 follows the same relaxation-then-exploration design as discussed in Section 3.3 and
Algorithm 1. Whenever our estimate of the problem instance θis updated, the algorithm first solves a
relaxed version of Problem (FAIR -ASSORT )using the updated estimate, and then adds randomized
exploration to stimulate learning; see Steps 2(b) and 2(c) in Algorithm 2.5
The main difference between Algorithm 2 and Algorithm 1 lies in the learning mechanism. In order
to learn the weights win the assortment setting, we adopt the learning mechanism from MNL-Bandit
[3]. The rounds in which type- jarrives are divided into epochs. In each epoch ℓj, the same assortment
Sjis offered until a no-purchase option is observed. We then apply Eq. (58) to estimate w:,j, which
is an unbiased estimator according to Corollary A.1 in [ 3]. To estimate arrival probabilities, we
5For solving Problem (FAIR -ASSORT )in our numerical experiments in Sections 4 and F.2, we use the CBC
solver accessed via the PuLP library in Python.
30Algorithm 2 FORM for Assortment Recommendation
Input: (i)Nitems with revenues r∈RN, item outcome OI
i(. , .), i∈[N]and item-fair solution fU(.); (ii)M
types of users with user outcome OU
j(. , .), j∈[M]and user-fair solution fU(.)(iii) fairness parameters
δI, δU∈[0,1]. (iv) parameters ϵtandηt.
1.Initialization and Setting the Parameters. For all i∈[N], j∈[M], initialize ˆw1,i,j= 1/2and
ˆp1,j= 1/M. Randomly select Sj∼ S, where S={S⊆[N],|S| ≤K}denote the collection of all
possible assortments. Let ℓj= 0,Ti,j=∅.
2. While t≤T.
• Observe the type of the arriving user Jtand offer assortment SJt. Update number of arrivals
nJt,t←nJt,t+ 1.
• Observe purchase decision zt∈ {0} ∪SJt(where 0means no-purchase)
–Ifzt= 0,
(a)LetTi,Jt=Ti,Jt∪ {ℓJt}fori∈SJt, where Ti,Jtare the epochs in which item igets shown
to user type Jt. Then, increment ℓJt←ℓJt+ 1, where ℓJtdenotes the number of epochs for
user type Jt.
(b)Update estimates for item weights and arrival probabilities. Let
ˆwt,i,Jt=1
|Ti,Jt|X
τ∈Ti,JtX
t∈Eτ,JtI{zt=i} ∀i∈[N]and ˆpt,j=1
tnj,t∀j∈[M]
(58)
(c)Solve a Relaxed Problem (FAIR -ASSORT )with the Estimated Instance. Given estimated
instance ˆθ= (ˆpt,ˆwt,r)and the magnitude of relaxation ηt, letˆqtbe the optimal solution to
the following:
max
qREV(q,ˆθt)s.t.OI
i(q,ˆθt)≥δI·OI
i(fI(ˆθt),ˆθt)−ηt∀i∈[N]
OU
j(q,ˆθt)≥δU·OU
j(fU(ˆθt),ˆθt)−ηt∀j∈[M],(59)
(d)Recommend with Randomized Exploration. Update the assortment to be recommended to
type-Jtuser based on the recommendation probabilities SJt∼qt,Jt, where
qt,Jt(S) = (1 − |S| ϵt)ˆqt,Jt(S) +ϵt∀S∈ S, j∈[M]. (60)
–Else,EℓJt,Jt← EℓJt,Jt∪ {t}
•t←t+ 1.
again use the sample mean for p. As discussed in Section 3.3, any learning mechanism that provides
accurate estimates for sufficiently large Twould be suitable here. The magnitudes of exploration and
relaxation, ϵtandηt, are adjustable inputs for platforms to fine-tune in practice.
While a detailed theoretical analysis of the expanded algorithm is reserved for future work, it is clear
that our high-level ideas behind FORM , including incorporating relaxations for fair recommendations
and adding exploration to stimulate learning, seamlessly transition to the assortment recommendation
context. The efficacy of our extended framework/method in the assortment recommendation setting
is further validated in our case study on Amazon review data (see Section 4).
F Supplements for Case Study on Amazon Review Data
F.1 More Details of Our Baselines
In this section, we discuss the baselines introduced in existing works, specifically FairRec [ 55] and
TFROM [ 71], which are state-of-the-art approaches targeting multi-sided fairness. We detail the main
setups of both approaches and highlight the key differences between these methods and ours.
FairRec [55]. As we briefly remarked in Section 4, FairRec targets a single-shot recommendation
problem where there are Nitems and Musers, and makes a single recommendation decision that
displays Kitems to each of the Musers, assuming full knowledge of the exact relevance score
between each pair of item and user. Let A={(Aj)j∈[M]:Aj⊂[N],|Aj| ≤K}denote the
the algorithm’s recommendation decision. The authors of [ 55] show that FairRec can achieve (i)
maxmin share (MMS) of visibility (exposure) for most of the items and non-zero visibility for the
rest; (ii) envy-free up to one item (EF1) fairness for every user, meaning that for every pair of users
31j, j′∈[M], there should exist an item p∈Aj′such that the utility that user jreceives from Ajis at
least the utility that user j′receives from Aj′\ {p}.
There are several key differences between our setting/method and FairRec. (i) FairRec focuses on an
offline setting where the platform makes a single-shot recommendation with full knowledge of user
preferences (i.e., relevance scores). In contrast, our work targets the more challenging online setting
where user data is unknown and must be continuously learned. We maintain fairness in this online
setting, dealing with users who arrive stochastically, and ensuring fairness over a longer time horizon.
(ii) FairRec assumes specific fairness notions (i.e., maxmin fairness regarding visibility for items and
envy-free fairness up to one item for users). Our framework, on the other hand, is much more flexible,
accommodating a wide range of outcome functions and fairness notions, as discussed in Section 2.2.
See, also, Section F.2 where we conduct additional experiments on the Amazon review data under
alternative outcome functions and fairness notions, and demonstrate that FORM remains effective in
making fair recommendations in an online environment.
TFROM [71]. Similar to FairRec, the authors of [ 71] consider a recommendation problem where
relevance scores between items and users are known in advance. To impose fairness for items,
TFROM enforces either uniform fairness or quality-weighted fairness regarding item visibility
(see Definitions 1 and 2 in [ 71]). For user fairness, TFROM aims to achieve uniform normalized
discounted cumulative gain (NDCG) among users. [ 71] does not provide theoretical performance
results, but numerically shows that TFROM achieves low variance in item visibility and NDCG
among users in online settings, thus ensuring two-sided fairness. TFROM is provided in two versions:
one for the offline setting and one for the online setting, with the latter dynamically maintaining low
variance in item exposure and user NDCG scores over time. We compare our algorithm with the
online version of TFROM in our experiments in Section 4.
While TFROM addresses online user arrivals, it again differs significantly from our work in several
ways: (i) Like FairRec, it assumes full knowledge of user preferences (i.e., relevance scores) and
lacks a learning stage, ignoring potential biases from corrupted data; (ii) It focuses on pre-specified
outcomes and fairness notions for items/users, lacking the flexibility of our framework; (iii) No
theoretical guarantees are provided for TFROM’s performance, whereas we offer full theoretical
guarantees for our algorithm despite data uncertainty.
Finally, it is important to note that neither FairRec nor TFROM considers the platform’s revenue,
which is another key focus of our work. While the aforementioned works focus on two-sided fairness
in recommender systems, our framework takes a step forward and adopts a multi-sided perspective,
balancing the interests of the platform, items, users, and potentially other stakeholders. The efficacy
of our framework is highlighted in Figure 1b, showing how our algorithm manages to maintain high
platform revenue while ensuring fairness for other stakeholders.
F.2 Additional Experiments under Alternative Outcomes and Fairness Notions
To demonstrate the generality of our results, we have replicated our experiments on Amazon review
data from Section 4 under alternative fairness notions and outcomes for items. In particular, we have
considered the following definitions of the item-fair solutions fI, while keeping all other setups and
baselines the same as in Section 4:
•Maxmin fairness w.r.t. item visibility . Results are shown in Figure 3.
•K-S fairness w.r.t. item revenue . Results are shown in Figure 4.
•K-S fairness w.r.t. item visibility . Results are shown in Figure 5.
Overall, the results from Figures 3, 4, and 5 are consistent with those previously observed in Figure
1. In terms of the platform’s revenue, we again observe the convergence of time-averaged revenue
towards the optimal revenue in hindsight, confirming FORM ’s low regret. Regarding normalized
revenue, item outcomes, and user outcomes, FORM consistently strikes an effective balance among
multiple stakeholders’ interests under the specified fairness parameters δIandδU, regardless of the
fairness notion or outcome that stakeholders care about. The performance of our baselines, however,
is more affected by the fairness notion or outcomes when defining within-group fairness. For instance,
while min-revenue achieves high levels of fairness for all items when the item-fair solution considers
maxmin fairness w.r.t. item revenues, its performance deteriorates when items instead care about
visibility or adopt K-S fairness. In comparison, FORM quickly adapts to various fairness notions and
outcomes, while maintaining its good performance.
320 500 1000 1500 2000
t75%80%85%90%95%100%1
TT
t=1REV(xt)/FAIR-REV
FAIR-REV(0.2,0.2) FORM(0.2,0.2)(a) Convergence of revenue.
0.0 0.1 0.2 0.3 0.4 0.5I
6065707580859095100Revenue (%)
OPT-REV
FAIR-REV(., 0.2)
FORM(., 0.2)
FAIR-REV(., 0.8)FORM(., 0.8)
random
greedy
max-utilitymin-revenue
FairRec
TFROM (b) Normalized revenue.
0 5 10 15 20 25 30
items0.00.20.40.60.81.0normalized item outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (c) Item outcomes.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
users0.950.960.970.980.99normalized user outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (d) User outcomes.
Figure 3: Additional experiment results for Amazon review data. Here, the item-fair solution adopts maxmin
fairness w.r.t. item visibility .
0 500 1000 1500 2000
t75%80%85%90%95%100%1
TT
t=1REV(xt)/FAIR-REV
FAIR-REV(0.2,0.2) FORM(0.2,0.2)
(a) Convergence of revenue.
0.0 0.1 0.2 0.3 0.4 0.5I
6065707580859095100Revenue (%)
OPT-REV
FAIR-REV(., 0.2)
FORM(., 0.2)
FAIR-REV(., 0.8)FORM(., 0.8)
random
greedy
max-utilitymin-revenue
FairRec
TFROM (b) Normalized revenue.
0 5 10 15 20 25 30
items0.00.20.40.60.81.0normalized item outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (c) Item outcomes.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
users0.950.960.970.980.99normalized user outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (d) User outcomes.
Figure 4: Additional experiment results for Amazon review data. Here, the item-fair solution adopts K-S
fairness w.r.t. item revenue .
F.3 Additional Experiments on MovieLens Data
To show generality of our framework/method, we have conducted additional experiments in an
alternative setting (movie recommendation) using the MovieLens data, which complements our case
study on Amazon review data in Section 4. Overall, the results are consistent with our Amazon case
study, showing our method’s effectiveness in balancing platform revenue and stakeholder fairness in
various types of recommender systems.
Here, we act as a movie recommendation platform that shows a “trending action movie” to any
arriving user. We considered the N= 50 action movies from MovieLens (ML-100K) data [ 35]
with the highest number of ratings and clustered users into 6types based on their preferences. As
the movies are not associated with revenues, we let ri= 1 for all i∈[N]. Here, the platform’s
main objective is to maximize its expected marketshare. The item-fair solution adopts maxmin
fairness w.r.t. each movie’s marketshare, with user utilities captured by the MNL model. In this set of
experiments, we consider a total of T= 2000 rounds, where at each round there are 100user arrivals.
During each round t, we would solve the relaxed constrained optimization problem and update our
recommendation probabilities only once, given scalability considerations as stated in Section 3.5;
however, we will update our estimates for item weights and arrival probabilities based on Eq. (58)
throughout the user arrivals.
Figure 6 shows the results obtained from our MovieLens experiments. It is evident the results are
largely in line with what we observed in our case study on Amazon review data (Section 4), both
in terms of the platform’s revenue and the outcomes received by items/users. Our framework and
method also remain effective when we only resolve the relaxed constrained optimization problem
after a given number of user arrivals. It is noteworthy that in a movie recommendation setting with
homogeneous revenues, the interests of the platform and the users completely align. This explains
why the curves of greedy andmax-utility completely overlaps with each other in our figures. However,
greedy still suffers from 7-8% loss in marketshare (Figure 6b)), which is precisely because inadequate
exploration of user data makes it overlook potentially more popular items and stick with a sub-optimal
item. Overall, our algorithm FORM again adeptly balances the interests of both the platform and its
stakeholders, while handling the tradeoff between learning and fair recommendation.
G Supplementary Lemma
In this section, we state some useful lemmas that would be invoked in this work.
330 500 1000 1500 2000
t75%80%85%90%95%100%1
TT
t=1REV(xt)/FAIR-REV
FAIR-REV(0.2,0.2) FORM(0.2,0.2)(a) Convergence of revenue.
0.0 0.1 0.2 0.3 0.4 0.5I
6065707580859095100Revenue (%)
OPT-REV
FAIR-REV(., 0.2)
FORM(., 0.2)
FAIR-REV(., 0.8)FORM(., 0.8)
random
greedy
max-utilitymin-revenue
FairRec
TFROM (b) Normalized revenue.
0 5 10 15 20 25 30
items0.00.20.40.60.81.0normalized item outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (c) Item outcomes.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
users0.950.960.970.980.99normalized user outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (d) User outcomes.
Figure 5: Additional experiment results for Amazon review data. Here, the item-fair solution adopts K-S
fairness w.r.t. item visibility .
0 500 1000 1500 2000
t82%85%88%90%92%95%98%100%REV(xt) / FAIR-REVFAIR-REV(0.2,0.2) FORM(0.2,0.2)
(a) Convergence of revenue.
0.0 0.1 0.2 0.3 0.4 0.5I
707580859095100Revenue (%)
OPT-REV
FAIR-REV(., 0.2)
FORM(., 0.2)
FAIR-REV(., 0.8)FORM(., 0.8)
random
greedy
max-utilitymin-revenue
FairRec
TFROM (b) Normalized revenue.
0 10 20 30 40 50
items0.00.20.40.60.81.0normalized item outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (c) Item outcomes.
1 2 3 4 5 6
user types0.60.70.80.9normalized user outcome
FORM(0.2, 0.2)
FORM(0.5, 0.2)
FORM(0.8, 0.2)random
greedy
max-utilitymin-revenue
FairRec
TFROM (d) User outcomes.
Figure 6: Additional experiment results on MovieLens data.
Lemma G.1 (Approximations to solutions to systems of linear equations ([33], Theorem 2.1))
LetA∈Rm×n, and H(A) = max {∥λ∥1:λis an extreme point of σ(A)}, where
σ(A) = {λ∈Rm:λ≥0,∥λ⊤A∥1≤1}. Then, for each b∈Rmsuch that
{x∈Rn:Ax≤b} ̸=∅and for each x′∈Rn, we have:
min
Ax≤b∥x−x′∥∞≤H(A)·(Ax′−b)+
∞,
where H(A)≥0is known as the Hoffman constant.
Lemma G.2 (Characterization of Hoffman constant ([56], Proposition 2)) LetA∈Rm×n. The
Hoffman constant H(A)defined in Lemma G.1 can be characterized as follows:
H(A) = max
J⊆{1,...,m}
AJfull row rankmax
∥v∥1:v∈RJ
+,A⊤
Jv
1≤1	
= max
J⊆{1,...,m}
AJfull row rank1
minv∈RJ
+,∥v∥1=1A⊤
Jv
1,
where AJdenotes the submatrix of Athat only includes the rows in J⊆ {1, . . . , m }.
Lemma G.3 (Empirical distribution concentration bound w.r.t. ℓ-1 norm ([23], Lemma 3))
Letp∈∆Nandˆp∼Multinomial (n,p). Then, for any δ∈[0,3 exp(−4N/5)], we have
P
∥p−ˆp∥>5p
log(3 /δ)√n
≤δ .
Lemma G.4 (Bernstein’s inequality ([59], Theorem 8)) LetM1,M2. . .be a martingale differ-
ence sequence. Assume there exists deterministic sequences c1, c2, . . . andV1, V2, . . . such that
|Mk| ≤ckandE[P
k′∈[k]M2
k′]≤Vkfor all k. Then, ifq
log(2/δ)
(e−2)Vn≤1
cn, we have
PX
k∈[n]Mk≥p
(e−2)Vnlog(2 /δ)
≤δ . (61)
34NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: As discussed in our abstract and Section 1, our main contributions include
(i) introducing a novel fair recommendation framework, Problem (FAIR ), that flexibly
balances multi-stakeholder interests, and (ii) introducing a low-regret algorithm FORM that
simultaneously learns user data and performs fair recommendation in a dynamic online
setting, whose efficacy is further validated in a real-world case study.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have clearly stated the assumptions needed for our theoretical results,
the computational complexity of the proposed algorithm, and the setups adopted for our
numerical experiments.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
353.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We have provided all necessary assumptions and proofs for our theoretical
results (e.g., see Section D for the proof our main result, Theorem 3.1). We have also
provided proofs for any theorems and lemmas that show up in our appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We have provided code and data for our numerical experiments in the supple-
mentary materials.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
36(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We have provided code and data for our numerical experiments in the supple-
mentary materials.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We have detailed the setups of our numerical experiments in Section 4.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: As stated in Section 4, all results are averaged over 10simulations. In our
figures, the line indicates the mean value and the shaded region shows mean ±std/√
10.
Guidelines:
37• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: As stated in Section 4, all algorithms were implemented in Python 3.7 and run
on a MacBook with a 1.4 GHz Quad-Core Intel Core i5 processor.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: This paper conforms, in every respect, with the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
38Justification: This work contributes to enhancing fairness in algorithmic recommendation
decisions, and we do not anticipate any negative societal impact from the research. If any
concerns arise regarding the use of sensitive user or item attributes in determining user types,
the platform can exclude such information and instead rely on non-sensitive attributes, as
noted in Footnote 1.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All baselines used in our numerical experiments have been cited.
Guidelines:
• The answer NA means that the paper does not use existing assets.
39• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
40Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
41