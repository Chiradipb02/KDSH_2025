Approximating the Top Eigenvector in Random
Order Streams
Praneeth Kacham∗
Google Research
pkacham@google.comDavid P. Woodruff
Carnegie Mellon University
dwoodruf@cs.cmu.edu
Abstract
When rows of an n×dmatrix Aare given in a stream, we study algorithms for ap-
proximating the top eigenvector of the matrix ATA(equivalently, the top right sin-
gular vector of A). We consider worst case inputs Abut assume that the rows are
presented to the streaming algorithm in a uniformly random order. We show that
when the gap parameter R=σ1(A)2/σ2(A)2= Ω(1) , then there is a randomized
algorithm that uses O(h·d·polylog( d))bits of space and outputs a unit vector v
that has a correlation 1−O(1/√
R)with the top eigenvector v1. Here hdenotes
the number of heavy rows in the matrix, defined as the rows with Euclidean norm
at least ∥A∥F/p
d·polylog( d). We also provide a lower bound showing that any
algorithm using O(hd/R )bits of space can obtain at most 1−Ω(1/R2)correla-
tion with the top eigenvector. Thus, parameterizing the space complexity in terms
of the number of heavy rows is necessary for high accuracy solutions.
Our results improve upon the R= Ω(log n·logd)requirement in a recent work of
Price and Xun (FOCS 2024). We note that the algorithm of Price and Xun works
for arbitrary order streams whereas our algorithm requires a stronger assumption
that the rows are presented in a uniformly random order. We additionally show
that the gap requirements in their analysis can be brought down to R= Ω(log2d)
for arbitrary order streams and R= Ω(log d)for random order streams. The
requirement of R= Ω(log d)for random order streams is nearly tight for their
analysis as we obtain a simple instance with R= Ω(log d/log log d)for which
their algorithm, with any fixed learning rate, cannot output a vector approximating
the top eigenvector v1.
1 Introduction
We consider the problem of approximating the top eigenvector in the streaming setting. In this
problem, we are given vectors a1, . . . , a n∈Rdone at a time in a stream. Let Abe an n×dmatrix
with rows a1, . . . , a n. The task is to approximate the top eigenvector of the matrix ATA. Throughout
the paper, we use v1∈Rdto denote the top eigenvector of ATA. We focus on obtaining streaming
algorithms that use a small amount of space and can output a unit vector ˆvsuch that ⟨ˆv, v1⟩2≥1−
f(R), where f(R)is a decreasing function in the gap R=λ1(ATA)/λ2(ATA). Here λ1(·), λ2(·)
denote the two largest eigenvalues. As the gap Rbecomes larger, the eigenvector approximation
problem becomes easier and we want more accurate approximations to the eigenvector v1.
If one is allowed to use ˜O(d2)2bits of space, we can maintain the matrix ATA=P
iaiaT
ias we
see the rows aiin the stream, and at the end of processing the stream, we can compute the exact
top eigenvector v1. When the dimension dis large, the requirement of Ω(d2)bits of memory can be
∗Work done while the author was a student at Carnegie Mellon University.
2The notation ˜O(f(n))is used to denote the set of functions in O(f(n)·polylog( n)).
38th Conference on Neural Information Processing Systems (NeurIPS 2024).impractical (see e.g., applications that require a large value of din Mitliagkas et al. (2013).) Hence,
an interesting question is to study non-trivial streaming algorithms that use less memory. In this
work, we focus on obtaining algorithms that use ˜O(d)bits of space.
In the offline setting (where the entire matrix Ais available to us), fast iterative algorithms such as
Gu (2015); Musco and Musco (2015); Musco et al. (2018) can be used to quickly obtain accurate
approximations to the top eigenvector when the gap R= Ω(1) . In a single pass streaming setting,
we cannot run these algorithms as these iterative algorithms need to seethe entire matrix multiple
times.
There have been two major lines of work studying the problem of eigenvector approximation and
the related Principal Component Analysis (PCA) problem in the streaming setting with near-linear
indmemory. In the first line of work, each row encountered in the stream is assumed to be sampled
independently from an unknown distribution with mean 0and covariance Σand the task is to ap-
proximate the top eigenvector of Σusing the samples. In this line of work, the sample complexity
required for algorithms using O(d·polylog( d))bits of space to output an approximation to v1, is the
main question. The algorithms are usually a variant of Oja’s algorithm (Oja, 1982; Jain et al., 2016;
Allen-Zhu and Li, 2017; Huang et al., 2021; Kumar and Sarkar, 2023) or the block power method
(Hardt and Price, 2014; Balcan et al., 2016). We note that Kumar and Sarkar (2023) relax the i.i.d.
assumption and analyze the sample complexity of Oja’s algorithm for estimating the top eigenvector
in the Markovian data setting.
The other line of work studies algorithms for arbitrary streams appearing in an arbitrary order.
In this setting, we want algorithms to work for anyinput stream given in anyorder. A problem
closely related to the eigenvector estimation problem is the Frobenius-norm Low Rank Approxima-
tion (Clarkson and Woodruff, 2017; Boutsidis et al., 2016; Upadhyay, 2016; Ghashami et al., 2016).
The deterministic Frequent Directions sketch of Ghashami et al. (2016) can, using ˜O(d/ε)bits of
space, output a unit vector usuch that
∥A(I−uuT)∥2
F≤(1 +ε)∥A(I−v1vT
1)∥2
F.
Although the vector uis a1 +εapproximate solution to the Frobenius norm Low Rank Approxi-
mation problem, it is possible that the vector umay be (nearly) orthogonal to the top eigenvector v1.
Hence the Frequent Directions sketch does not guarantee top eigenvector approximation. Recently,
Price and Xun (2024) study the eigenvector approximation problem in arbitrary streams and obtain
results in terms of the gap Rof the instance. Price and Xun prove that when R= Ω(log n·logd), a
variant of Oja’s algorithm outputs a unit vector ˆvsuch that
⟨ˆv, v1⟩2≥1−Clogd
R−1
poly( d)
where Cis a large enough universal constant. On the lower bound side, Price and Xun showed that
any algorithm that outputs a vector ˆvsatisfying
⟨ˆv, v1⟩2≥1−1
CR2,
must use Ω(d2/R3)bits of space while processing the stream. This lower bound shows that in the
important case of R=O(1), the correlation3that can be obtained by an algorithm using ˜O(d)bits
of space is at most a constant less than 1. Thus, the current best algorithms for arbitrary streams
work only when R= Ω(log n·logd)and for the important case of R=O(1), there are no existing
algorithms requiring significantly fewer than d2bits of memory. They also give a lower bound on
the size of mergeable summaries for approximating the top eigenvector.
We identify an instance with R= Θ(log d/log log d)where the algorithm of Price and Xun fails to
produce a vector with even a constant correlation with the vector v1. This shows that their algorithm
or other variants of Oja’s algorithm may fail to extend to the case when R=O(1). We further show
that the algorithm of Price and Xun fails to produce such a vector even when the rows in our hard
instance are ordered uniformly at random, showing that even randomly ordered streams can be hard
to solve for variants of Oja’s algorithm.
In this work, we focus on algorithms that work on worst case inputs Awhile assuming that the
rows of Aareuniformly randomly ordered . This model is mid-way between the i.i.d. setting and the
3We say that the value ⟨u, v⟩2denotes the correlation between unit vectors uandv.
2arbitrary order stream setting in terms of the generality of streams that can be modeled. We note that
a number of works (Munro and Paterson, 1980; Guha et al., 2005; Chakrabarti et al., 2008; Guha and
McGregor, 2009; Assadi and Sundaresan, 2023) have previously considered streaming algorithms
and lower bounds for worst case inputs with random order streams, as it is a natural model often
arising in practical settings. Our algorithms are parameterized in terms of the number of heavy rows
in the stream. See Gupta and Singla (2021) for a gentle introduction to the random-order model. We
define a row aito be heavy if∥ai∥2≥ ∥A∥F/p
d·polylog( d). Note that in any stream of rows, by
definition, there are at most d·polylog( d)heavy rows. We state our theorem informally below:
Theorem 1.1. Leta1, . . . , a n∈Rdbe a randomly ordered stream and let Adenote the n×d
matrix with rows given by a1, . . . , a n. IfR=λ1(ATA)/λ2(ATA)> C for a large enough constant
Cand the number of heavy rows in the stream is at most h, then there is a streaming algorithm using
O(h·d·polylog( d))bits of space and outputting a unit vector ˆvsatisfying
⟨ˆv, v1⟩2≥1−O(1/√
R)
with a probability ≥4/5.
Our algorithm is a variant of the block power method. Along the way, we also improve the gap
requirements in the results of Price and Xun (2024). We show that by subsampling a stream of rows,
the algorithm of Price and Xun can be made to work even when the gap RisΩ(log2d)in arbitrary
order streams, improving on the Ω(log n·logd)requirement in their analysis. We also show that
in random order streams, a gap of Ω(log d)is sufficient for their algorithm, though our algorithm
improves on this and works for even a constant gap.
Similar to the lower bound of Price and Xun, we show that any algorithm for random order streams
must use Ω(h·d/R)bits of space to output a vector ˆvsatisfying ⟨ˆv, v1⟩2≥1−1/CR2where Cis
a constant. We summarize the theorem below.
Theorem 1.2. Consider an arbitrary random order stream a1, . . . , a nwith the gap parameter
σ1(A)2
σ2(A)2=R. Let hbe the number of heavy rows in the stream. Any streaming algorithm that
outputs a unit vector ˆvsuch that
⟨ˆv, v1⟩2≥1−1/CR2
for a large enough constant C, with a probability ≥1−(1/2)R+1over the ordering of the stream
and its internal randomness, must use Ω(h·d/R)bits of space.
Techniques. The randomized power method (Gu, 2015) algorithm to approximate the top eigen-
vector samples a random Gaussian vector gand iteratively computes the vector v= (ATA)tg4for
t= Θ(log d)iterations and shows that when the gap Ris large, v/∥v∥2is a good approximation for
v1. Thus, the algorithm needs to seethe quadratic form ATAmultiple times and hence, it cannot be
implemented in the single-pass streaming setting of this paper.
Assume that the stream is randomly ordered and that there are no heavy rows. Our key observation
is that if the stream is long enough, then we can see tapproximations BT
jBj5of the quadratic form
ATA. Here the matrices B1, . . . ,Btare formed by sampling and rescaling the rows of the matrix A
and importantly, the rows of B1, . . . ,Btdo not overlap in the stream, that is, they appear one after
the other. Thus we can compute v′= (BT
tBt)···(BT
1B1)·gfor the starting vector gin a single
pass over the stream. We prove that such matrices Bjexist using the row norm sampling result of
Magdon-Ismail (2010). Now, the main issue is to show that v′/∥v′∥2is a good approximation to the
top eigenvector v1. We crucially use a singular value inequality of Wang and Xi (1997) to prove that
∥BT
jBj−ATA∥2≤ε∥A∥2
2for all jsuffices for v′/∥v′∥2to be a good approximation to v1.
The above analysis assumes that there are no heavy rows. Indeed, suppose that a matrix Ahas a row
awith a large Euclidean norm which is orthogonal to all the other rows. Also assume that the top
eigenvector of the matrix Ais in this direction. Since, the matrices B1, . . . ,Btare non-overlapping
substreams of the matrix A, at most one of the matrices Bjcan have the row aand hence the vector
v′/∥v′∥2will not be a good approximation to a/∥a∥2, the top eigenvector. Thus, we need to handle
4Note that ATA·v=P
i⟨ai, v⟩ai.
5We use bold symbols to denote random variables.
3the heavy rows separately. We show that, by storing all the rows with a Euclidean norm larger than
∥A∥F/p
d·polylog( d)and running the above described algorithm on the remaining set of rows, we
can obtain a good approximation to the top eigenvector.
Our lower bound (Theorem 1.2) shows that any single-pass streaming algorithm must use space
proportional to the number of heavy rows, and therefore our procedure that handles the heavy rows
separately gives near-optimal bounds.
Finally, the row norm sampling technique of Magdon-Ismail (2010) serves as a general technique
to reduce the number of rows in the stream while (approximately) preserving the top eigenvector.
We use this observation to improve the R= Ω(log n·logd)for arbitrary streams in Price and
Xun (2024) to R= Ω(log2d). We then show that assuming a uniformly random order, the analysis
of Price and Xun (2024) can be improved to show that R= Ω(log d)suffices. Thus, for random
order streams, techniques before our work can be used to approximate the top eigenvector when the
gapR= Ω(log d). Our work improves upon this to give an algorithm that works for streams with
R= Ω(1) .
Implications to practice. Often, in practical situations, we can assume that the rows being
streamed are sampled independently from a nice-enough distribution, in which case Oja’s algorithm,
as discussed, can approximate the top eigenvector accurately given enough samples. However, inde-
pendence and assumptions on the covariance matrix can be very strong assumptions in some cases
and in such cases, our algorithm only requires that the order of the rows in the stream be uniformly
random, in which case we output an approximation with provable guarantees.
Organization. We first introduce the row-norm sampling procedure to obtain approximate
quadratic forms. The proof is a slight modification of that of Magdon-Ismail (2010). The only dif-
ference is that we instead consider a version that samples each row in the input independently with
some appropriate probability and keeps the rows that are sampled after scaling appropriately. We
then introduce and analyze our block power iteration algorithm when all rows have roughly the
same Euclidean norm, and then extend it to the general case, which is our main result. Finally, we
provide a lower bound showing that Ω(td/R)bits of space is necessary to obtain constant correlation
with the top eigenvector. Due to space constraints, all of our proofs are placed in the appendix.
2 Power Method with Approximate Quadratic Forms
In this section, we present and analyze our algorithm for approximating the top eigenvector of ATA
when the rows of Aare presented to the algorithm in a uniformly random order.
We first show a row sampling technique that reduces the number of rows in the stream. The row-
norm sampling technique for approximating the quadratic form ATAwith spectral norm guarantees
was given by Magdon-Ismail (2010). The technique works irrespective of the order of the rows.
2.1 Sampling for Row Reduction
Theorem 2.1. LetAbe an arbitrary n×dmatrix. Given p∈[0,1]n, letQbe an n×ndiagonal
matrix such that for each i∈[n], we independently set Qii= 1/√piwith probability piand0
otherwise. If for all i,
pi≥min
1, C∥ai∥2
2
ε2∥A∥2
2logd
,
then with probability 1−1/poly( d),∥ATA−ATQTQA∥2≤ε∥A∥2
2.With probability at least
1−1/poly( d), the matrix Qhas at most O(ε−2ρlogd)non-zero entries, where ρ=∥A∥2
F/∥A∥2
2
denotes the stable rank of matrix A.
Note that given the value of ∥A∥2, the sampling procedure in this theorem can be performed in a
stream. Additionally, as the original stream is uniformly randomly ordered, the sub-sampled stream
is also uniformly randomly ordered assuming that the sampling is independent of the order of the
rows.
4Given that all of the non-zero entries of the matrix have absolute value at least 1/poly( nd)and at
most poly( nd), we have that ∥A∥2
2lies in the interval [1/poly( nd),poly( nd)]. Thus, we can guess
the value of ∥A∥2
2as2i/poly( nd)fori= 0, . . . , O (log(nd))and one of these values must be a
2-approximation to ∥A∥2
2, and thus sub-sampling the rows using that guess satisfies the conditions
in the above theorem. We can run the streaming algorithms on all the streams simultaneously to
obtain O(lognd)vectors u1, . . . , u O(lognd)as the candidates for being an approximation to the
top eigenvector. From Theorem 2.1, the candidate vector ujcomputed on the stream obtained by
sampling the rows with the correct probabilities is a good approximation to the top eigenvector, and
therefore ∥A·uj∥2is large for that value of j. Thus, the vector ujwith the largest value ∥A·uj∥2
is a good approximation to the top eigenvector v1. IfGis a Gaussian matrix with O(ε−2logd)
rows, then for all uj, we can approximate ∥A·uj∥2up to a 1±εfactor using ∥G·A·uj∥2by
the Johnson-Lindenstrauss lemma. Additionally, the matrix G·Acan be maintained in the stream
using O(ε−2·dlogd)bits (when we see a row ai, we sample an independent Gaussian vector gi
and add giaT
ito an accumulator to maintain G·A). Thus, at the end of processing the stream, we
can compute a vector ujthat has a large value ∥A·uj∥2, and hence is a good approximation for v1.
If we can process each created stream using sbits of space, then the overall space requirement is
O(s·log(nd) +d·polylog( d))bits, using O(s)bits for each guess for the value of ∥A∥2
2and
O(d·polylog( d))bits for storing a Gaussian sketch of the matrix with ε= 1/polylog( d).
2.2 Random-Order Streams with bounds on Norms
Algorithm 1: Approximate Eigenvector for Streams with no Large Norms
Input: Ann×dmatrix Awithn= Ω(η·ρ(A)·log2d/ε2),max i∥ai∥2
2/mini∥ai∥2
2≤η
Output: A vector z
1t← ⌈C1logd⌉
2Compute G·Ain the stream where Gis a Gaussian matrix with O(ε−2logd)rows
3forρ= 1,2,4, . . . , d simultaneously do
4 p←C2ηρlogd/nε2//p≤1/(5t)forρ≤2·ρ(A)
5zρ∼N(0,1)d
6 forj= 1, . . . , t do
7 yj←Bin(n, p)
8 ifyj>2npthen
9 return ⊥
10 end
// The matrix Aj·(2np):j·(2np)+yjcorresponds to Bjin the analysis.
11 acc←0
12 fori= (j−1)·(2np) + 1, . . . , (j−1)·(2np) +yjdo
13 acc←acc+⟨ai,zρ⟩ ·ai
14 end
// Here acc=BT
jBjzρ
15 zρ←acc
16 zρ←zρ/∥zρ∥2
17 end
18end
19return arg maxz∈{z1,z2,z4,...,zd}∥(G·A)z∥2
We now present the analysis of the block power method for random order streams assuming that
the Euclidean norms of all the rows in Aare close to each other. We later remove this assumption.
Suppose there exists a parameter ηsuch that (max i∥ai∥2
2)/(min i∥ai∥2
2)≤η. Ifηis close to 1then
all the rows in the stream have roughly the same norm.
Letp=Cηρlog(d)/ε2n. We can see that for any row aiin the stream,
C∥ai∥2
2
ε2∥A∥2
2logd≤Cη∥A∥2
F/n
ε2∥A∥2
2logd≤Cηρlogd
nε2=p.
5Thus, pis greater than the probability with which we need to sample each row in the row-norm
sampling result in Theorem 2.1. Now if we perform such a sampling of the rows of A, we sample
Bin(n, p)6number of rows, which is tightly concentrated around np=ε−2Cηρlogd. Thus, if we
first sample y∼Bin(n, p)and then consider the first ynumber of rows in the random order stream,
then we will have sampled from a distribution satisfying the requirements in Theorem 2.1 and can
therefore obtain a matrix Bsuch that
∥BTB−ATA∥2≤ε∥A∥2
2.
Thus, assuming that the rows appear in a uniformly random order lets us show that the first yrows
of the stream can be used to compute an approximation to the quadratic form ATA. We will now
show that we can obtain O(logd)such quadratic forms in the stream given that the stream is long
enough.
Assume that the number of rows in the stream n= Ω( ηρlog2d/ε2). We partition the stream into
t= Θ(log d)groups as follows: the first 2nprows are placed in the group 1, the second 2nprows
are placed in the group 2, and so on. Note that since n= Ω( ηρlog2d/ε2), we can form tsuch
groups. Since the rows are uniformly randomly ordered, the joint distribution of the rows appearing
in group 1is the same as that of the joint distribution of the rows appearing in group 2and so on.
Lety1, . . . ,yt∼Bin(n, p)be drawn independently. With probability ≥1−1/poly( d), we have
yi≤(3/2)npfor all i. Fori= 1, . . . , t , letBibe the matrix formed by the first yirows in group i.
Using a union bound, we have that with probability ≥1−1/poly( d), for all i= 1, . . . , t ,
∥ATA−1
pBT
iBi∥2≤ε∥A∥2
2.
Conditioned on the above event, we will now show that running the power method on the blocks
B1, . . . ,Btlets us approximate the top singular vector of the matrix A.
Assumption 2.2. We assume that σ1(A)/σ2(A)≥2.
Lemma 2.3. Letε >1/poly( d)be an accuracy parameter and t= Ω(log d)be the number of
iterations. Let ε≤c/t2for a small constant c. Suppose B1, . . . , B tall satisfy ∥ATA−BT
jBj∥2≤
ε∥A∥2
2forε <1/5. Ifgis a random vector sampled from the Gaussian distribution, then the unit
vector
ˆv:=(BT
tBt)···(BT
1B1)g
∥(BT
tBt)···(BT
1B1)g∥2
satisfies
⟨ˆv, v1⟩2≥1
1 +C′t√ε
with probability ≥9/10for a large enough constant C′. Here v1denotes the top right singular
vector of the matrix A.
To prove this lemma, our strategy is to show that the matrix product M:= (BT
tBt)···(BT
1B1)has
a stable rank close to 1— meaning it has one very large singular value and the rest of the singular
values are small. We can then argue that the vector ˆv=Mg/∥Mg∥2is in the direction of the top
singular vector M. Using the fact that vT
1(BT
jBj)v1≥(1−ε)∥A∥2
2for all j, we show that the top
singular vector of Mmust have a large correlation with v1. Therefore, it follows that the vector ˆvhas
a large correlation with v1as well. As part of the proof, we crucially use an inequality from Wang
and Xi (1997).
Ift= Θ(log d)and1/poly( d)≤ε≤c/(logd)2, then the above lemma shows that ˆvhas a large
correlation with the top singular vector v1. Using this lemma, we show that Algorithm 1 can be used
to obtain an approximation for v1in random order streams with bounded norms.
Theorem 2.4. Letα≥1/poly( d)be an accuracy parameter. Let ηbe a parameter such that
max i∥ai∥2
2
min i∥ai∥2
2≤η. If the number of rows in the stream n= Ω( α−4·ρ(A)·η·log6d), where ρ(A) =
6Bin(n, p)denotes the binomial distribution with parameters nandp.
6∥A∥2
F/∥A∥2
2and the rows in the stream are ordered uniformly at random, then we can compute a
vector ˆvusing the block power method that satisfies
|⟨v1,ˆv⟩|2≥1−3α
with probability ≥4/5ifσ1(A)/σ2(A)≥2. The algorithm uses O(d·polylog( d)/α4)bits of space.
Proof. Setε=α2/Clog2dfor a large enough constant C. Assuming n= Ω( α−4ρηlog6d),
we have n= Ω( ε−2ρηlog2d). Now consider the execution of Algorithm 1 on matrix A, with
parameters ηandε. Letρ= 2jbe such that ρ(A)/2≤ρ≤ρ(A), and consider the execution in the
algorithm with parameter ρ. Using Theorem 2.1, with probability ≥1−1/poly( d), the algorithm
computes tmatrices B1, . . . ,Btsuch that for all j∈[t],
∥1
pBT
jBj−ATA∥2≤ε∥A∥2
2.
Noting that zρ= (BT
tBt)···(BT
1B1)g/∥(BT
tBt)···(BT
1B1)g∥2, by Lemma 2.3, we have with
probability ≥9/10that
⟨zρ, v1⟩2≥1
1 +C′t√ε≥1−α.
Thus, for ρwhich satisfies ρ(A)/2≤ρ≤ρ(A), the algorithm computes a vector zρthat has a large
correlation with the vector v1. Since the algorithm does not know the exact value of ρ, it computes an
approximation for ∥Az∥2
2for all z∈ {z1,z2,z4, . . . ,zd}. First, we condition on the fact that with
probability ≥1−1/poly( d), for all zi,∥GAzi∥2
2= (1±ε)∥Azi∥2
2. Since ⟨zρ, v1⟩2≥(1−α),
we note that ∥GAzρ∥2
2≥(1−ε)(1−α)σ1(A)2. Now, for the vector zreturned by the algorithm,
we have ∥Az∥2
2≥(1−O(ε))(1−α)σ1(A)2which implies that
⟨z, v1⟩2·σ1(A)2+ (1− ⟨z, v1⟩2)σ1(A)2
R≥ ∥Az∥2
2≥(1−α−O(ε))σ1(A)2
and therefore ⟨z, v1⟩2≥1−3αsince R≥2.
2.3 Random Order Streams without Norm Bounds
Assuming that the random order streams are long enough, Theorem 2.4 shows that if all the squared
row norms are within an ηfactor, then the block power method outputs a vector with a large corre-
lation with the top eigenvector of the matrix ATA. For general streams, the factor ηcould be quite
large and hence the algorithm requires very long streams to output an approximation to v1.
If there are no heavy rows, i.e., rows with a Euclidean norm larger than ∥A∥F/p
d·polylog( d),
then the row norm sampling procedure in Theorem 2.1 can be used to convert any randomly ordered
stream of rows into a uniformly random stream of rows that all have the same norm. The row norm
sampling procedure computes a probability pi= min(1 , Cε−2∥ai∥2
2logd/∥A∥2
2)and samples the
rowaiwith probability pi. If sampled, then the row aiis scaled by 1/√pi. From Theorem 2.1, we
have that the top eigenvector of the quadratic form of the sampled-and-rescaled submatrix is a good
approximation to the top eigenvector ATAwhen the gap Ris large enough. Suppose pi<1. If the
rowaiis sampled, we then have
∥ai/√pi∥2=ε∥A∥2√Clogd.
Thus, if pi<1for all i, then all the sampled-and-rescaled rows have the same Euclidean norm and
therefore, we can run the algorithm from Theorem 2.4 by setting η= 1. Note that pi= 1 only if
∥ai∥2
2≥ε2∥A∥2
2/Clog(d). Since we assumed that there are no heavy rows, there is no row with
pi= 1as long as ε≥1/polylog( d). Thus, using Theorem 2.4 on the row norm sampled substream
directly gives us a good approximation to the top eigenvector. However, in general, the streams can
have rows with large Euclidean norm. We will now state our theorem and describe how such streams
can be handled.
7Theorem 2.5. LetAbe an n×dmatrix with its non-zero entries satisfying 1/poly( d)≤ |Ai,j| ≤
poly( d), and hence representable using O(logd)bits of precision. Let R=σ1(A)2/σ2(A)2.
Assume 2≤R≤C1log2d. Let hbe the number of rows in Awith norm at most
∥A∥F/p
d·polylog( d), where polylog( d) = logC2dfor a large enough universal constant
C2. Given the rows of the matrix Ain a uniformly random order, there is an algorithm using
O((h+1)·d·polylog( d)·logn)bits of space and which outputs a vector ˆvsuch that with probability
≥4/5,ˆvsatisfies ⟨ˆv, v1⟩2≥1−8/√
R, where v1is the top eigenvector of the matrix ATA.
The key idea in proving this theorem is to partition the matrix AintoAheavy andAlight, where Aheavy
denotes the matrix with the heavy rows and Alightdenotes the matrix with the rest of the rows of A.
Since we assume that there are at most hheavy rows, we can store the matrix Aheavy using O(h·
d·polylog( d))bits of space. Now consider the following two cases: (i) ∥Aheavy∥2≥(1−β)∥A∥2
or (ii)∥Aheavy∥2<(1−β)∥A∥2for some parameter β. In the first case, we can show that the top
eigenvector uofAT
heavyAheavy is a good approximation for v1. Since, we store the full matrix Aheavy,
we can compute uexactly at the end of the stream. Suppose ∥Aheavy∥2<(1−β)∥A∥2. By the
triangle inequality, we have ∥Alight∥2> β∥A∥2. If we set βlarge enough compared to 1/R, then we
can show that the top eigenvector u′ofAT
lightAlightis a good approximation of v1. From the above
discussion, since all the rows of Alightarelight, we can obtain a stream using Theorem 2.1 such
that all the rows have the same norm and additionally, the top eigenvector of this stream is a good
approximation for u′and therefore v1. We then approximate the top eigenvector of the new stream
using Theorem 2.4. Setting βappropriately, we show that this procedure can be used to compute a
vector ˆvsatisfying ⟨ˆv, v1⟩2≥1−O(1/√
R)proving the theorem.
3 Lower Bounds
Our algorithm uses ˜O(h·d)space when the number of heavy rows in the stream is h. We want to
argue that it is nearly tight. We show the following theorem.
Theorem 3.1. Given a dimension d, lethandRbe arbitrary with R≤h≤dandR2·h=O(d).
Consider an algorithm Awith the following property:
Given any fixed matrix n×dmatrix AwithO(h)heavy rows and gap σ1(A)2/σ2(A)2≥R, in the
form of a uniform random order stream, the algorithm Aoutputs a unit vector ˆvsuch that, with
probability ≥1−(1/2)4R+4over the randomness of the stream and the internal randomness of
the algorithm, |⟨ˆv, v1⟩|2≥1−c/R2.
Ifcis a small enough constant, then the algorithm Amust use Ω(h·d/R)bits of space.
The theorem shows that a streaming algorithm must use Ω(hd/R )bits of space assuming that with
high probability, it outputs a vector with a large enough correlation with the top eigenvector of ATA
when the rows are given in a random order stream.
Our proof uses the same lower bound instance as that of Price and Xun (2024). The key difference
from their proof is that our lower bound must hold against random order streams.
4 Improving the Gap Requirements in the Algorithm of Price and Xun
4.1 Arbitrary Order Streams
As discussed in Section 2.1, we can guess an approximation of ∥A∥2
2in powers of 2and sample
at most O(dlogd/ε2)rows in the stream to obtain a matrix B, in the form of a stream, satisfying
∥BTB−ATA∥2≤ε∥A∥2
2,with a large probability. Using Weyl’s inequalities, we obtain that
σ2(BTB)≤σ2(ATA) +ε∥A∥2
2and σ1(BTB)≥(1−ε)σ1(ATA)
implying R′=σ1(B)2/σ2(B)2≥(1−ε)/(1/R+ε). Forε= 1/(2R)≤1/2, we note R′≥R/3.
Letn′=O(R2·dlogd)be the number of rows in the matrix Band note that R′= Ω(log n′·logd)
assuming R= Ω(log2d). Hence, running the algorithm of Price and Xun on the rows of the matrix
8B, we compute a vector ˆvfor which
|⟨ˆv, v′
1⟩|2≥1−logd
CR′−1
poly( d)
with a large probability, where v′
1is the top eigenvector of the matrix BTB. We now note that if
v1denotes the top eigenvector of the matrix ATA, then|⟨v1, v′
1⟩|2≥1−O(1/R)which therefore
implies that with a large probability,
|⟨ˆv, v1⟩|2≥1−logd
CR.
Thus, sub-sampling the stream using row norm sampling and then running the algorithm of Price
and Xun (2024), we obtain an algorithm for arbitrary order streams with a gap R= Ω(log2d).
4.2 Random Order Streams
Lemma 3.5 in Price and Xun (2024) can be tightened when the rows of the stream are uniformly
randomly ordered. Specifically, we want to bound the following quantity:
nX
i=1⟨ai, Pˆvi−1⟩2
where P=I−v1vT
1denotes the projection away from the top eigenvector, and ˆvi−1is a function
ofv1, a1, . . . , a i−1. We have
E[⟨ai, Pˆvi−1⟩2] =E[E[⟨ai, Pˆvi−1⟩2|a1, . . . , a i−1]].
Given that the first i−1rows are a1, . . . , a i−1, assuming uniform random order, we have
E[⟨ai, Pˆvi−1⟩2|a1, . . . , a i−1] =1
n−i+ 1ˆvT
i−1P(ATA−a1aT
1− ··· − ai−1aT
i−1)Pˆvi−1
≤σ2(A)2
n−i+ 1.
Hence E[⟨ai, Pˆvi−1⟩2]≤σ2(A)2/(n−i+1) andE[Pn
i=1⟨ai, Pˆvi−1⟩2]≤σ2(A)2(1+log n).Price
and Xun define η·σ2(A)2asσ2and in that notation, we obtain ηPn
i=1⟨ai, Pˆvi−1⟩2≤10σ2(1 +
logn)with probability ≥9/10by Markov’s inequality. In the proof of Lemma 3.6 in Price and Xun
(2024), if σ1/σ2≥20(1 + log2n), we obtain log∥vn∥2≳σ1. Now, σ1≥O(logd)ensures that the
Proof of Theorem 1.1 in their work goes through.
Using the row-norm sampling analysis from the previous section, we can assume n= poly( d)and
therefore a gap of O(logd)between the top two eigenvalues of ATAis enough for Oja’s algorithm
to output a vector with a large correlation with the top eigenvector in random order streams.
5 Hard Instance for Oja’s Algorithm
At a high level, the algorithm of Price and Xun (2024) runs Oja’s algorithm with different learning
ratesηand in the event that the norm of the output vector with each of the learning rates ηis small,
then the row with the largest norm is output. The algorithm is simple and can be implemented using
an overall space of O(d·polylog( d))bits.
The algorithm initializes z0=gwhere gis a random Gaussian vector. The algorithm streams
through the rows a1, . . . , a nand performs the following operation
zi←zi−1+η· ⟨zi−1, ai⟩ai.
The algorithm computes the smallest learning rate ηwhen∥zn∥2is large enough, and then outputs
either zn/∥zn∥2or¯a/∥¯a∥2as an approximation to the eigenvector of the matrix ATA. Here ¯a
denotes the row in Awith the largest Euclidean norm.
The following theorem shows that at gaps ≤O(logd/log log d), we cannot use Oja’s algorithm
with a fixed learning rate ηto obtain constant correlation with the top eigenvector.
9Theorem 5.1. Given dimension d, a constant c >0, a parameter M, for all gap parameters R=
Oc(logd/log log d)there is a stream of vectors a1, . . . , a n∈Rdwithn=O(R+M)such that:
1.σ1(A)2/σ2(A)2≥R/2, and
2. Oja’s algorithm with any learning rate η < M fails to output a unit vector ˆvthat satisfies,
with probability ≥9/10,
|⟨ˆv, v1⟩| ≥c
where v1is the top eigenvector of the matrix ATA.
Moreover, the result holds irrespective of the order in which the vectors a1, . . . , a nare presented to
the Oja’s algorithm. We will additionally show that even keeping track of the largest norm vector is
insufficient to output a vector that has a large correlation with v1.
Acknowledgements
The authors were supported in part by a Simons Investigator Award and NSF CCF-2335412. D.
Woodruff was visiting Google Research while performing this work.
References
Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-PCA: a global, gap-
free, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of Computer
Science (FOCS) , pages 487–492. IEEE, 2017. 2
Sepehr Assadi and Janani Sundaresan. (Noisy) gap cycle counting strikes back: Random order
streaming lower bounds for connected components and beyond. In Proceedings of the 55th Annual
ACM Symposium on Theory of Computing , pages 183–195, 2023. 3
Maria-Florina Balcan, Simon Shaolei Du, Yining Wang, and Adams Wei Yu. An improved gap-
dependency analysis of the noisy power method. In Conference on Learning Theory , pages 284–
309. PMLR, 2016. 2
Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in
distributed and streaming models. In Proceedings of the forty-eighth annual ACM symposium on
Theory of Computing , pages 236–249, 2016. 2
Amit Chakrabarti, Graham Cormode, and Andrew McGregor. Robust lower bounds for communi-
cation and stream computation. In Proceedings of the fortieth annual ACM symposium on Theory
of computing , pages 641–650, 2008. 3
Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input spar-
sity time. Journal of the ACM (JACM) , 63(6):1–45, 2017. 2
Mina Ghashami, Edo Liberty, Jeff M Phillips, and David P Woodruff. Frequent directions: Simple
and deterministic matrix sketching. SIAM Journal on Computing , 45(5):1762–1792, 2016. 2
Ming Gu. Subspace iteration randomization and singular value problems. SIAM Journal on Scientific
Computing , 37(3):A1139–A1173, 2015. 2, 3
Sudipto Guha and Andrew McGregor. Stream order and order statistics: Quantile estimation in
random-order streams. SIAM Journal on Computing , 38(5):2044–2059, 2009. 3
Sudipto Guha, Andrew McGregor, and Suresh Venkatasubramanian. Streaming and sublinear ap-
proximation of entropy and information distances. arXiv preprint cs/0508122 , 2005. 3
Anupam Gupta and Sahil Singla. Random-order models. In Tim Roughgarden, editor, Beyond the
Worst-Case Analysis of Algorithms , chapter 11. Oxford University Press, 2021. doi: 10.1017/
9781108637435. URL https://arxiv.org/pdf/2002.12159 . 3
10Moritz Hardt and Eric Price. The noisy power method: A meta algorithm with applications. Ad-
vances in neural information processing systems , 27, 2014. 2
De Huang, Jonathan Niles-Weed, and Rachel Ward. Streaming k-PCA: Efficient guarantees for
oja’s algorithm, beyond rank-one updates. In Conference on Learning Theory , pages 2463–2498.
PMLR, 2021. 2
Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford. Streaming pca:
Matching matrix bernstein and near-optimal finite sample guarantees for Oja’s algorithm. In
Conference on learning theory , pages 1147–1164. PMLR, 2016. 2
Syamantak Kumar and Purnamrita Sarkar. Streaming pca for markovian data. In Advances in Neural
Information Processing Systems , volume 36, 2023. 2
Malik Magdon-Ismail. Row sampling for matrix algorithms via a non-commutative bernstein bound.
arXiv preprint arXiv:1008.0587 , 2010. 3, 4
Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory-limited, streaming PCA. In
Advances in Neural Information Processing Systems , volume 26, 2013. 2
J Ian Munro and Mike S Paterson. Selection and sorting with limited storage. Theoretical computer
science , 12(3):315–323, 1980. 3
Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster
approximate singular value decomposition. Advances in neural information processing systems ,
28, 2015. 2
Cameron Musco, Christopher Musco, and Aaron Sidford. Stability of the lanczos method for matrix
function approximation. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
Discrete Algorithms , pages 1605–1624. SIAM, 2018. 2
Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of mathematical
biology , 15:267–273, 1982. 2
Eric Price and Zhiyang Xun. Spectral guarantees for adversarial streaming PCA. In FOCS , 2024.
2, 3, 4, 8, 9, 15
Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and Trends® in
Machine Learning , 8(1-2):1–230, 2015. 12
Jalaj Upadhyay. Fast and space-optimal low-rank factorization in the streaming model with appli-
cation in differential privacy. arXiv preprint arXiv:1604.01429 , 2016. 2
Bo-Ying Wang and Bo-Yan Xi. Some inequalities for singular values of matrix products. Linear
algebra and its applications , 264:109–115, 1997. 3, 6, 13
11A Omitted Proofs
A.1 Proof of Theorem 2.1
Proof. LetXidenote an indicator random variable which denotes if Qiiis nonzero. Note E[Xi] =
piandX1, . . . ,Xnare independent. Define a d×drandom matrix Yi= (Xi/pi−1)aiaT
i, where
aidenotes the i-th row of A. We note that
ATA−ATQTQA=nX
i=1(Xi/pi−1)aiaT
i=nX
i=1Yi.
We use the Matrix Bernstein inequality (Tropp, 2015) to bound ∥P
iYi∥2. We first uniformly
upper bound ∥Yi∥2. Ifpi= 1, by definition ∥Yi∥2= 0 with probability 1. Let pi̸= 0. Then,
∥(Xi/pi−1)aiaT
i∥2≤ ∥aiaT
i∥2/pi≤ε2∥A∥2
2/Clogdwith probability 1.
We now bound ∥P
iE[Y2
i]∥2.X
iE[Y2
i] =X
iE[(1/pi−1)2]∥ai∥2
2aiaT
i
=X
i:pi>0(1/pi−1)∥ai∥2
2aiaT
i
⪯X
i:pi>0ε2∥A∥2
2
C∥ai∥2
2logd∥ai∥2
2aiaT
i
⪯ε2∥A∥2
2
ClogdATA
which implies ∥P
iE[Y2
i]∥2≤ε2∥A∥4
2/(Clogd). Now, we obtain
Pr[∥X
iYi∥2≥ε∥A∥2
2]≤2d·exp
−ε2∥A∥4
2/2
ε2∥A∥4
2/(Clogd) +ε3∥A∥4
2/(3Clogd)
≤2d·exp
−Clogd
2(1 + ε/3)
.
IfC≥6(1 + ε/3), then Pr[∥P
iYi∥2≥ε∥A∥2
2]≤1−2/d2which implies that with probability
≥1−2/d2,∥ATA−ATQTQA∥2≤ε∥A∥2
2.
Now, the number of non-zero entries in the matrix Qis equal toP
iXi. We note E[P
iXi]≤
Cε−2ρ·logd. By a Chernoff bound, we obtain thatP
iXi=O(ε−2ρ·logd)with probability
≥1−1/poly( d).
A.2 Proof of Lemma 2.3
Proof. Define M:= (BT
tBt)···(BT
1B1). Our strategy is to show that if v1is the top singular vector
of the matrix A, then∥vT
1M∥2is comparable to ∥M∥Fgiven that σ1(A)/σ2(A)≥2. We can then
prove the lemma using simple properties of the Gaussian vector g.
For an arbitrary j, let(BT
jBj)v1=αv1+ ∆ where ∆⊥v1. We note that vT
1(BT
jBj)v1=α.
We have α=vT
1BT
jBjv1≥(1−ε)σ1(A)2using the fact that ∥BT
jBj−ATA∥2≤ε∥A∥2
2and
vT
1ATAv1=σ1(A)2=∥A∥2
2. If we show that ∆is small, then the vector (BT
jBj)v1is oriented in
a direction very close to that of v1. Note that
∥(BT
jBj)v1∥2≤ ∥BT
jBj∥2≤(1 +ε)σ1(A)2
and∥(BT
jBj)v1∥2
2=α2+∥∆∥2
2which implies ∥∆∥2
2≤((1+ ε)2−(1−ε)2)σ1(A)4= 4ε·σ1(A)4
and thus ∥∆∥2≤√
4εσ1(A)2. Now,
∥MTv1∥2
=∥(BT
1B1)···(BT
t−1Bt−1)(⟨BT
tBtv1, v1⟩v1+ ∆ 1)∥2
≥ ⟨BT
tBtv1, v1⟩∥(BT
1B1)···(BT
t−1Bt−1)v1∥2− ∥(BT
1B1)···(BT
t−1Bt−1)∥2∥∆1∥2
≥((1−ε)σ1(A)2)∥(BT
1B1)···(BT
t−1Bt−1)v1∥2−(√
4εσ1(A)2)∥(BT
1B1)···(BT
t−1Bt−1)∥2.
12Expanding similarly, we obtain
∥MTv1∥2≥(1−ε)tσ1(A)2t−t√
4ε(1 +ε)t−1σ1(A)2t.
Assuming ε≤c/tfor a small constant c, we note that (1−ε)t≥(1−2tε)and(1+ε)t≤(1+2 tε)
which implies
∥MTv1∥2=∥(BT
1B1)···(BT
tBt)v1∥2≥(1−2tε−4t√ε)σ1(A)2t.
We shall now show a bound on ∥M∥F=∥(BT
tBt)···(BT
1B1)∥Fwhich lets us show that the unit
vector ˆvis highly correlated with v1. To bound the quantity ∥M∥F, we first note the following facts:
1.∥BT
jBj∥2≤(1 +ε)σ1(A)2, and
2.σ2(BT
jBj)≤σ2(A)2+εσ1(A)2≤(1/4 +ε)σ1(A)2by our gap assumption.
Now, we use the following theorem.
Theorem A.1 ((Wang and Xi, 1997, Theorem 3(ii))) .For any r >0and any matrices A1, . . . , A t,
X
i(σi(A1···At))r≤X
iσi(A1)r···σi(At)r.
Applying the above theorem with r= 2, we obtain
∥(BT
tBt)···(BT
1B1)∥2
F≤(1 +ε)2tσ1(A)4t+ (d−1)(1/4 +ε)tσ1(A)4t
≤(1 + 4 tε)σ1(A)4t+d
3tσ1(A)4t.
When t≥3 log( d/ε), we have ∥(BT
tBt)···(BT
1B1)∥2
F≤(1 + 4 tε+ε)σ1(A)4t.We now use the
following lemma.
Lemma A.2. Letgbe a Gaussian random vector with each of the components being an independent
standard Gaussian random variable. Let ˆv=Mg/∥Mg∥2. For any unit vector v, with probability
≥4/5,
|⟨ˆv, v⟩|2≥1
1 +C∥M∥2
F−∥MTv∥2
2
∥MTv∥2
2
for a large enough universal constant C.
Proof. Since vis a unit vector, we can write ∥Mg∥2
2=|vTMg|2+∥(I−vvT)Mg∥2
2. Hence, we
have
|⟨ˆv, v⟩|2=|vTMg|2
∥Mg∥2
2=1
1 +∥(I−vvT)Mg∥2
2
|vTMg|2.
We now note that vTMg∼N(0,∥MTv∥2
2)andE[∥(I−vvT)Mg∥2
2] =tr(MT(I−vvT)M) =
∥M∥2
F− ∥MTv∥2
2. By a union bound, with probability ≥4/5, we have
∥(I−vvT)Mg∥2
2
|vTMg|2≤C∥M∥2
F− ∥MTv∥2
2
∥MTv∥2
2
for a large enough constant C. Therefore, with probability ≥4/5, we get that
|⟨ˆv, v⟩|2≥1
1 +C∥M∥2
F−∥MTv∥2
2
∥MTv∥2
2.
Applying the above lemma for M= (BT
tBt)···(BT
1B1)andv=v1, we obtain
|⟨ˆv, v1⟩|2≥1
1 +C′t√ε
with probability ≥4/5.
13A.3 Proof of Theorem 2.5
Proof. Partition the matrix AintoAlightandAheavy, where Aheavy is the submatrix with rows aisuch
that∥ai∥2>∥A∥F/p
d·polylog( d)andAlightis the remaining rows. From our assumption, the
number of rows in Aheavy is at most h. Note that given a uniformly random stream of rows of A, we
can obtain a uniformly random stream of rows of Alightby just filtering out the rows in Aheavy.
Suppose, ∥Aheavy·v1∥2≥(1−β)∥A∥2for a parameter βto be chosen later. Let v′
1be the top
singular vector of the matrix Aheavy. Note
∥A·v′
1∥2
2≥ ∥Aheavy·v′
1∥2
2≥ ∥Aheavy·v1∥2
2≥(1−β)2∥A∥2
2
and therefore we have ⟨v′
1, v1⟩2≥1−4β, assuming R≥2. Thus, while processing the stream, we
can store all the heavy rows and at the end of the stream compute the top right singular vector of
Aheavy, in order to obtain a good approximation for v1.
Suppose ∥Aheavy·v1∥2≤(1−β)∥A∥2. This implies ∥Alight·v1∥2
2≥ ∥A∥2
2−∥Aheavy·v1∥2
2≥β·∥A∥2
2.
If we set β≥2/R, we have
σ1(Alight)2
σ2(Alight)2≥β∥A∥2
2
σ2(A)2≥2.
Letv′
1be the top singular vector of Alight. We will describe how to approximate v′
1. Consider apply-
ing the row norm sampling procedure with parameter εto the matrix Alight. Given a row ai∈Alight
the corresponding sampling probability piis given by
pi=Clogd· ∥ai∥2
2
ε2∥Alight∥2
2≤Clogd· ∥A∥2
F/(d·polylog( d))
ε2β2∥A∥2
2≤C
ε2β2polylog( d).
Assuming that ε2β2≥1/polylog( d), we obtain that pi<1for all the rows in the matrix Alight. Let
Blightbe the matrix obtained after applying the row norm sampling procedure to the matrix Alight.
Note that ρ(Blight)≈ρ(Alight)and the number of rows in BlightisΘ(ρ(Alight)·logd·ε−2), and
therefore Θ(ρ(Blight)·logd·ε−2). Setting ε=α2/log5/2d, we obtain that the number of rows in the
matrix BlightisΘ(α−4·ρ(Blight)·log6d)and thus assuming ε2β2=α4β2/log5d≥1/polylog( d),
we can use Theorem 2.4 to obtain a vector ˆvsatisfying
⟨ˆv, v′
1⟩2≥1−3α.
We will now show that v′
1has a large correlation with v1which then implies ˆvhas a large correlation
withv1. Since ∥Alight∥2≥ ∥A∥2− ∥Aheavy∥2≥β∥A∥2,∥Alight∥2
2=∥Alight·v′
1∥2
2≥β∥A∥2
2.
Consider the following upper bound on ∥Alight·v′
1∥2
2:
∥Alight∥2
2=∥Alight·v′
1∥2
2=∥Alight·(⟨v′
1, v1⟩ ·v1+ (I−v1vT
1)v′
1)∥2
2
=∥⟨v1, v′
1⟩Alight·v1+Alight(I−v1vT
1)v′
1∥2
2
≤(1 +θ)· ⟨v1, v′
1⟩2· ∥Alight·v1∥2
2+ (1 + 1 /θ)· ∥Alight(I−v1vT
1)v′
1∥2
2
for any θ >0. Using the fact that the rows of the matrix Alightare a subset of the rows of the matrix
Aand that ∥A(I−v1vT
1)∥2=σ2(A) =σ1(A)/√
R, we have
∥Alight∥2
2≤(1 +θ)· ⟨v1, v′
1⟩2· ∥Alight∥2
2+ (1 + 1 /θ)·σ2
1
R·(1− ⟨v1, v′
1⟩2)
=⟨v1, v′
1⟩2((1 + θ)· ∥Alight∥2
2−(1 + 1 /θ)σ2
1/R) + (1 + 1 /θ)·σ2
1/R
which implies
⟨v1, v′
1⟩2≥∥Alight∥2
2−(1 + 1 /θ)·σ2
1/R
(1 +θ)∥Alight∥2
2−(1 + 1 /θ)σ2
1/R= 1−θ· ∥Alight∥2
2
(1 +θ)∥Alight∥2
2−(1 + 1 /θ)σ2
1/R
≥1−θ
1 +θ−(1 + 1 /θ)/Rβ
using the fact that ∥Alight∥2
2≥β2σ2
1. Now assuming Rβ≥1and picking θ= 2/(Rβ−1), we
obtain
⟨v1, v′
1⟩2≥1−4Rβ
(1 +Rβ)2≥1−4
Rβ.
14We therefore have
⟨ˆv, v1⟩2≥1−4
Rβ−4α. (1)
Setting β= 1/√
Randα= 1/√
R, we satisfy all the requirements assuming that R≤polylog( d)
and obtain a vector ˆvsatisfying ⟨ˆv, v1⟩2≥1−8/√
R. When ∥Aheavy∥2≥(1−β)∥A∥2, we already
have a vector v′=top eigenvector of Aheavy that satisfies ⟨ˆv, v1⟩2≥1−4β≥1−4/√
R. Thus, in
both the cases, we obtain a vector ˆvsatisfying ⟨ˆv, v1⟩2≥1−O(1/√
R).
The procedure described requires knowing the approximate values of ∥A∥F,∥Alight∥2. Since, we
assume that all the non-zero entries of the matrix have an absolute value at least 1/poly( d)and
at most poly( d), the values ∥A∥F,∥Alight∥2lie in the interval [1/poly( d),poly( nd)]. Hence, using
O(lognd)guesses each for ∥A∥Fand∥Alight∥2and using a Gaussian sketch of Asimilar to that in
Algorithm 1, we can obtain a vector satisfying the guarantees in the theorem.
A.4 Proof of Theorem 3.1
Proof. For each i∈[h], letx1, . . . ,xhbe drawn independently and uniformly at random from
{+1,−1}d. Leti∼[h]be drawn uniformly at random, and for an integer kto be chosen later, let
y1, . . . ,yk∈Rdbe vectors that share the first (1−γ)dcoordinates with the vector xi. Each of the
lastγ·delements of each of y1, . . . ,ykare sampled uniformly at random from the set {+1,−1}.
Define z1, . . . ,zh+ksuch that for j≤h,zj=xjand for j > h , letzj=yj−h.
Now consider the stream z1, . . . ,zh+k. Price and Xun argue that when k≥4R, the gap of this
stream is at least Rwith large probability over the randomness used in the construction of the
stream. Let π: [h+k]→[h+k]be a uniformly random permutation independent of i. Consider
the following event E:
π(i)≤h/2andπ(h+ 1), . . . ,π(h+k)> h/2.
We have that the probability of the event Eis
h/2 +k
h+k·h/2 +k−1
h+k−1···h/2 + 1
h+ 1·h/2
h≥(1/2)k+1.
LetSibe the set of permutations πthat satisfy the above event. Therefore we have Prπ[π∈Si]≥
(1/2)k+1. If the probability of failure, δ, of the algorithm Asatisfies δ≤(1/2)k+4, we have that
Pr
π,internal randomness[Asucceeds on zπ(1), . . . ,zπ(h+k)|π∈Si]≥3
4.
Letsmidbe the state of the algorithm after h/2steps and sfinbe the final state of the algorithm. The
randomness in sfinis from the following sources: (i) randomness of the vectors x1, . . . ,xh, (ii) the
indexi∈[h], (iii) the vectors y1, . . . ,yk, (iv) the permutation π, and (v) the internal randomness of
the algorithm. From here on, condition on the event E, i.e., that the permutation π∈Si. We will not
explicitly mention that all entropy and information terms in the proof are conditioned on E. Since
π(i)≤h/2, we have
sfinis conditionally independent of xi[(1−γ)·d+ 1 : d]givensmid.
Using the data processing inequality, we obtain that
I(smid;xi[(1−γ)·d+ 1 : d])≥I(sfin;xi[(1−γ)·d+ 1 : d]).
When h≤cd/R2,k= 4R,γ= 1/4andε≤c/k2for a small constant, we have as in the proof of
Theorem 1.5 in Price and Xun (2024) that,
I(sfin;xi[(1−γ)·d+ 1 : d])≥Ω(d/R)
which now implies
I(smid;xi[(1−γ)·d+ 1 : d])≥Ω(d/R).
Note that conditioned on the event E, the distribution of iis uniform over
{π−1(1), . . . ,π−1(h/2)}. We now prove the following lemma:
15Lemma A.3. LetY1, . . . ,Yℓbe independent random variables. Let i∼[ℓ]be a uniform random
variable independent of X. We have
I(X;Y1) +···+I(X;Yℓ)≥ℓ·(I(X;Yi)−log2ℓ).
Proof. By definition, we have
I(X;Yi) =H(Yi)−H(Yi|X).
Now, we note that H(Yi)≤H(Yi,i) =H(i)+H(Yi|i) = log2ℓ+H(Y1)+···+H(Yℓ)
ℓ. We now
lower bound H(Yi|X). Since conditioning always decreases entropy, we obtain
H(Yi|X)≥H(Yi|i,X).
AsXis independent of i, we have
H(Yi|X)≥H(Yi|i,X) =H(Y1|X) +···+H(Yℓ|X)
ℓ
which then implies
I(X;Yi)≤H(i) +H(Y1) +···+H(Yℓ)
ℓ−H(Y1|X) +···+H(Yℓ|X)
ℓ
≤H(i) +I(X;Y1) +···+I(X;Yℓ)
ℓ.
Since H(i) = log2ℓ, we have the proof.
Using this lemma,
I(smid;xπ−1(1)[(1−γ)·d+ 1 : d]) +···+I(smid;xπ−1(h/2)[(1−γ)·d+ 1 : d])
= (h/2)·I(smid;xi[(1−γ)·d+ 1 : d]−log2(h/2))
≥Ω(hd/R )−hlog2h.
Lemma A.4. IfX,Yare independent, then I(Z; (X,Y))≥I(Z;X) +I(Z;Y).
Proof.
I(Z; (X,Y)) =H((X,Y))−H((X,Y)|Z)
=H(X) +H(Y)−H((X,Y)|Z).
Now, we note that for any three random variables X,Y,Z, we have H((X,Y)|Z)≤H(X|
Z) +H(Y|Z)which proves the lemma.
Using the independence of x1, . . . ,xhconditioned on the event E, we obtain
I(smid; (xπ−1(1)[(1−γ)·d+ 1 : d], . . . ,xπ−1(h/2)[(1−γ)·d+ 1 : d]))≥Ω(hd/R )−hlog2h
which then implies
H(smid)≥Ω(hd/R )
using the fact that R2·h=O(d). Finally, we have max|smid| ≥Ω(hd/R ). Here|smid|is the number
of bits used in the representation of the state smid.
A.5 Proof of Theorem 5.1
Proof. Our instance consists of the following vectors:
1.Rcopies of the vector (1/√
R)e1,
2. 1 copy of the vector (1/√
R−ε)e2, and
3.αcopies of the vector (1/√
α·R)e3.
16where α= 2M. LetAbe a matrix with rows given by the stream of vectors defined above. We note
that the matrix Ahas rank 3and the non-zero eigenvalues of the matrix ATAare1,1/(R−ε),1/R
and therefore the gapλ1(ATA)/λ2(ATA) =R−ε. The top eigenvector of the matrix ATAise1
and the row with the largest norm is (1/√
R−ε)e2. Thus, the row with the largest norm is not useful
to obtain correlation with the true top eigenvector e1.
Consider an execution of Oja’s algorithm with a learning rate ηon the above stream of vectors. The
final vector zncan be written as
zn=
I+η
Re1eT
1R
I+η
Rαe3eT
3α
I+1
R−εe2eT
2
v0.
Forj∈[d], letzijdenote the j-th coordinate of the vector ziso that we have
zn1=
1 +η
RR
·z01,
zn2=
1 +η
R−ε
·z02,and
zn3=
1 +η
Rαα
·z03.
We note that znj=z0jfor all j > 3. Since α= 2M, we have η/Rα ≤1/2and therefore
(1 +η/Rα )≥exp(η/2Rα)and(1 +η/Rα )α≥exp(η/2R).
Recall that we want to show that |⟨zn, e1⟩|< c∥zn∥2with a large probability. Suppose otherwise
and that with probability ≥1/10, we have |⟨zn, e1⟩|> c∥zn∥2> c∥(0,0,0, z04, . . . , z 0d)∥2.
Since, z0is initialized to be a random Gaussian, we have ∥(0,0,0, z04, . . . , z 0d)∥2≥√
d/2with
probability 1−exp(−d). Thus, we have with probability ≥1/11that,
|zn1| ≥c√
d/2
which implies the learning rate must satisfy
(1 +η/R)R≥c′√
d/2
since|z01| ≤ 10with probability ≥99/100. Hence η≥R((c′d1/2)1/R−1). Now consider
|⟨zn, e3⟩|/|⟨zn, e1⟩|. We have
|⟨zn, e3⟩|
|⟨zn, e1⟩|=exp(η/R)
(1 +η/R)R·|z03|
|z01|.
With probability ≥95/100, we have 1/C≤ |z03|/|z01| ≤Cfor a large enough constant C. We
now consider the expression
exp(η/R)
(1 +η/R)R.
The expression is minimized at η=R2−Rand is increasing in the range η∈[R2−R,∞).
When, R=O(logd/log log d), we have that R2−R≤R((c′d1/2)1/R−1)and therefore for all
η≥R((c′d1/2)1/R−1), we have
exp(η/R)
(1 +η/R)R≥exp(( c′d1/2)1/R)
e·c′d1/2.
When R=O(logd/log log d), we have
exp(η/R)
(1 +η/R)R≥poly( d)
which then implies |⟨zn, e3⟩| ≥ |⟨ zn, e1⟩|·poly( d)/Cwith probability ≥95/100which contradicts
our assumption that |⟨zn, e1⟩| ≥c∥zn∥2.
17NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our paper is purely theoretical studying space-efficient algorithms for approx-
imating the top eigenvector. We prove all the claims made in the abstract and introduction.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We do not have a specific limitations section but we do qualify all the state-
ments noting the assumptions that need to be made to prove that our algorithms work.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The au-
thors should reflect on how these assumptions might be violated in practice and what
the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used
by reviewers as grounds for rejection, a worse outcome might be that reviewers dis-
cover limitations that aren’t acknowledged in the paper. The authors should use their
best judgment and recognize that individual actions in favor of transparency play an
important role in developing norms that preserve the integrity of the community. Re-
viewers will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
18Answer: [Yes]
Justification: We include all the proofs in the main body and the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: No experimental results are given in this paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
19Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
• While we encourage the release of code and data, we understand that this might not
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
20• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Results in this paper are purely theoretical. While the algorithms proposed in
this paper may be used with potentially negative consequences, the authors are unaware of
such uses.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative so-
cietal impacts of the work performed?
Answer: [No]
Justification: Our work studies algorithms for the top eigenvector estimation problem. Our
work is purely theoretical. While our algorithms may be used to impact society in a negative
way, we are unaware of such usecases.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
21• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
cific groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
22• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license, lim-
itations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
23