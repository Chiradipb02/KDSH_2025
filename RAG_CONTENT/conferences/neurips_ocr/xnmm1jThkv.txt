Hybrid Top-Down Global Causal Discovery
with Local Search
for Linear and Nonlinear Additive Noise Models
Sujai Hiremath
Cornell Tech
sh2583@cornell.eduJacqueline Maasch
Cornell Tech
jam887@cornell.eduMengxiao Gao
Tsinghua University
gaomx21@mails.tsinghua.edu.cn
Promit Ghosal
University of Chicago
promit@uchicago.eduKyra Gan
Cornell Tech
kyragan@cornell.edu
Abstract
Learning the unique directed acyclic graph corresponding to an unknown causal
model is a challenging task. Methods based on functional causal models can
identify a unique graph, but either suffer from the curse of dimensionality or impose
strong parametric assumptions. To address these challenges, we propose a novel
hybrid approach for global causal discovery in observational data that leverages
local causal substructures. We first present a topological sorting algorithm that
leverages ancestral relationships in linear structural equation models to establish
a compact top-down hierarchical ordering, encoding more causal information
than linear orderings produced by existing methods. We demonstrate that this
approach generalizes to nonlinear settings with arbitrary noise. We then introduce a
nonparametric constraint-based algorithm that prunes spurious edges by searching
for local conditioning sets, achieving greater accuracy than current methods. We
provide theoretical guarantees for correctness and worst-case polynomial time
complexities, with empirical validation on synthetic data.
1 Introduction
Causal graphical models compactly represent the data generating processes (DGP) of complex
systems, including physical, biological, and social domains. Access to the true causal graph or
its substructures can offer mechanistic insights [ 31,13] and enable downstream causal inference,
including effect estimation [ 11,34,2,8,16,35]. In practice, the true causal graph is often unknown,
and can be challenging to assume using domain knowledge. In such limited-knowledge settings, we
can instead rely on causal discovery algorithms that learn the causal graph from observational data in
a principled, automated manner [41, 7].
Traditional approaches to causal discovery infer causal relationships either through conditional
independence relations (PC [ 40]) or goodness-of-fit measures (GES [ 3], GRaSP [ 12]). While these
discovery methods provide flexibility by avoiding assumptions over the functional form of the DGP,
they are generally worst-case exponential in time complexity and learn Markov equivalence classes
(MEC) rather than unique directed acyclic graphs (DAGs) [ 19]. Therefore, additional modeling
assumptions are often necessary for time-efficient and accurate global discovery.
Certain parametric assumptions can enable recovery of the unique ground truth DAG, e.g., assuming
a particular functional causal model (FCM) [ 48]. Under the additive noise model (ANM), we obtain
unique identifiability by assuming linear causal mechanisms with non-Gaussian noise distributions
38th Conference on Neural Information Processing Systems (NeurIPS 2024).[37,36] or nonlinear causal functions with arbitrary noise [ 10]. Under the independent additive noise
assumption, the causal parents of a variable are statistically independent of its noise term. For this
class of models, discovery often entails regressing the variable of interest against its hypothesized
parent set and testing for marginal independence between this set and the residual term [25].
Current FCM approaches to global causal discovery trade off between two main issues, suffering
from either 1) strong parametric assumptions over the noise or functional form (or both) or 2) the
use of high-dimensional nonparametric regressions, which require large sample sizes for reliable
estimation and do not scale to large graphs. In addition, current FCM-based methods are ill-suited
for causal discovery in sparse causal graphs, a setting that characterizes many high-dimensional
applications (e.g., analysis of genetic data in healthcare applications) [6, 47, 46, 15].
Contributions We propose a hybrid causal discovery approach to graph learning that combines
functional causal modeling with constraint-based discovery. We depart from previous methods by
characterizing conditions that allow us to search for and exploit local, rather than global, causal
relationships between vertices. These local relationships stem from root vertices: this motivates
a top-down, rather than bottom-up, approach. Thus, we learn the topological sort and discover
true edges starting from the roots rather than the leaves, as in existing methods [ 25,30,21]. This
approach leverages sparsity in both the ordering phase and edge discovery phase to reduce the size of
conditioning sets, as well as the number of high-dimensional regressions. We summarize our major
contributions as follows:
•We introduce a topological ordering algorithm LHTS for linear non-Gaussian ANMs that exploits
local ancestor-descendent relationships to obtain a compact hierarchical sort.
•We introduce a topological ordering algorithm NHTS for nonlinear ANMs that exploits local
parent-child relationships to run fewer high-dimensional regressions than traditional methods,
achieving lower sample complexity.
•We introduce a constraint-based algorithm ED that nonparametrically prunes spurious edges from
a discovered topological ordering, leveraging local properties of causal edges to use smaller
conditioning sets than traditional sparse regression techniques.
• We achieve accurate causal discovery in synthetic data, outperforming baseline methods.
Organization After describing the preliminaries in Section 2, we introduce the linear problem setting
in Section 3, establishing the connection between ancestral relationships and causal active paths and
introducing a linear hierarchical topological sorting algorithm (LHTS). Next, we extend our method
to the nonlinear setting in Section 4 by establishing the connection between parental relationships
and active causal paths, introducing a nonlinear hierarchical topological sorting algorithm (NHTS).
In section 5, we establish a sufficient conditioning set for determining edge relations and introduce an
efficient edge discovery algorithm (ED). We then test LHTS, NHTS and ED in synthetic experiments
in Section 6. To conclude, we discuss future work that might generalize our approach to full ANMs.
Related Work Our work is related to two kinds of discovery methods that explicitly leverage the
topological structure of DAGs: 1) permutation-based approaches, and 2) FCM-based approaches.
The original permutation-based approach SP [ 27] searches over the space of variable orderings to find
permutations that induce DAGs with minimal edge counts. Authors in [ 39] introduce greedy variants
of SP (such as GSP) that maintain asymptotic consistency; GRaSP [ 12] relaxes the assumptions
of prior methods to obtain improvements in accuracy. These methods highlight the importance of
using permutations for efficient causal discovery, but generally suffer from the need to bound search
runtime with heuristics, poor sample efficiency in high dimensional settings, and are unable to recover
a unique topological ordering or DAG ([22]).
On the other hand, the recent stream of FCM-based approaches decompose graph learning into two
phases: 1) learning the topological sort, i.e., inferring a causal ordering of the variables; and 2) edge
discovery, i.e., identifying edges consistent with the causal ordering [38, 25, 1, 30, 21, 32, 20].
The literature on topological ordering algorithms for ANMs is organized along the types of parametric
assumptions made on both the functional forms and noise distributions of the underlying DGP. Early
approaches like ICA-LiNGAM [ 37] and DirectLiNGAM [ 38] focus on learning DAGs generated by
linear functions and non-Gaussian noise terms. Recent work leverages score matching to obtain the
causal ordering in settings with nonlinear functions and Gaussian noise: SCORE [ 30] and DAS [ 21]
exploit particular variance properties, while DiffAN estimates the score function with a diffusion
model [ 32]. NoGAM [ 20] generalizes the score-matching procedure of SCORE to nonlinear causal
2mechanisms with arbitrary noise distributions. RESIT [ 25] leverages residual independence results in
nonlinear ANMs to identify topological orderings when the noise distribution is arbitrary. NoGAM
and RESIT both rely on high-dimensional nonparametric regression.
Once a topological ordering is obtained, spurious edges are pruned. Works that are agnostic to the
distribution of noise often use a parametric approach, implementing either a form of sparse regression
(e.g., Lasso regression [ 18]) or a version of additive hypothesis testing with generalized additive
models (GAMs) [ 17] (e.g., CAM-pruning [ 1]). RESIT [ 25] provides edge pruning in nonlinear
ANMs, relying again on high-dimensional nonparametric regression.
2 Preliminaries
We focus on structural equation model s (SEMs) represented as DAGs. These graphs describe the
causal relationships between variables, where an edge xi→xjimplies that xihas a direct causal
influence on xj. LetG= (V, E)be a DAG on |V|=dvertices, where Erepresents directed edges.
To define pairwise relationships between vertices, we let Ch(xi)denote the children of xisuch that
xj∈Ch(xi)if and only if xi→xj, and Pa(xi)denote the parents of xisuch that xj∈Pa(xi)if
and only if xj→xi. Similarly, let An(xi)denote the ancestors of xisuch that xj∈An(xi)if and
only if there exists a directed path xj99Kxi, and De(xi)denote the descendants of xisuch that
xj∈De(xi)if and only if there exists a directed path xi99Kxj. Vertices can be classified based on
the totality of their pairwise relationships: xiis aroot if and only if Pa(xi) =∅, aleaf if and only if
Ch(xi) =∅, an isolated vertex if xiis both a root and a leaf, and an intermediate vertex otherwise.
See an illustrative DAG in Figure 1. Vertices can also be classified in terms of triadic relationships:
xiis a confounder of xj, xkif and only if xi∈An(xj)∩An(xk); a mediator of xjtoxkif and only
ifxi∈De(xj)∩An(xk); and a collider between xjandxkif and only if xi∈De(xj)∩De(xk).
Undirected paths that transmit causal information between vertices xj, xkcan be differentiated into
frontdoor andbackdoor paths [42]. A frontdoor path is a directed path xj99K···99Kxkthat starts
with an edge out of xj, and ends with an edge into xk. A backdoor path is a path xjL99···99Kxk
that starts with an edge into xj, and ends with an edge into xk. Paths that start and end with an edge
out of xjandxk(xk99K···L99xk) do not transmit causal information between xj, xk.
Paths between two vertices are further classified, relative to a vertex set Z, as either active orinactive
[42]. A path between vertices xj, xkis active relative to Zif every node on the path is active relative
toZ. Vertex xi∈Vis active on path relative to Zif one of the following holds: 1) xi∈Zandxi
is a collider, 2) xi̸∈Zandxiis not a collider, 3) xi̸∈Z,xiis a collider, but De(xi)∩Z̸=∅. An
inactive path is simply a path that is not active. Throughout the rest of the paper, we will describe
causal paths as active or inactive with respect to Z=∅unless otherwise specified.
Definition 2.1 (Topological orderings) .Consider a given DAG G= (V, E). A topological sort
(linear order) is a mapping π:V→ {0,1, . . . ,|V|}, such that if xi∈Pa(xj), then xiappears
before xjin the sort π:π(xi)< π(xj). A hierarchical sort (between a partial and linear order) is
a mapping πL:V→ {0,1, . . . ,|V|}, such that if Pa(xi) =∅, then πL(xi) = 0 , and if Pa(xi)̸=∅,
thenπL(xi)equals the maximum length of the longest directed path from each root vertex to xi, i.e.,
πL(xi) = 1 + max {πL(xj) :xj∈Pa(xi)}.
We note that the hierarchical sort is unique, and that it coincides with a topological sort when the
number of layers equals |V|, i.e., the DAG is complete.
Definition 2.2 (ANMs) .ANMs [ 9] are a popular general class of structural equation models defined
over a DAG Gwith
xi=fi(Pa(xi)) +εi,∀xi∈V, (1)
where fis are arbitrary functions and εis are independent arbitrary noise distributions.
This model implicitly assumes the causal Markov condition and acyclicity; we adopt the aforemen-
tioned assumptions, as well as faithfulness [41].
3 Linear setting
We first restrict our attention to ANMs that feature only linear causal functions f, known as Linear
Non-Gaussian Acyclic causal Models (LiNGAMs). Following [ 38], we note that a LiNGAM can
3x1 x2 x3
Figure 1: Illustrative DAG, where x1is a root, x3is a leaf, x3∈Ch(x2), x3∈De(x1).
xi xjAP1
xk
xi xjAP2
xi xjAP3
xk
xi xjAP4
Figure 2: Enumeration of active causal path relation types between a pair of nodes xiandxj. Dashed
arrows indicate ancestorship.
be represented as a d×dadjacency matrix B={bij}, where bijis the coefficient from xjtoxi.
Note that, for any topological ordering πof a LiNGAM, if π(xj)< π(xi), then bji= 0. Thus, each
xi∈Vadmits the following representation: xi=P
π(xj)<π(xi)bijxj+εi.
Identifiability Identifiability conditions for LiNGAMs [ 37] primarily concern the distribution of
errors εi: under Gaussianity, distinct linear DGPs can admit the same joint distribution, making them
impossible to distinguish. Shimizu et al. [37] generalize this intuition with independent component
analysis (ICA) [ 4] to provide a multivariate identifiability condition for LiNGAMs (see Appendix
A.1). In this section, we adopt the aforementioned condition.
Ancestral Relations and Active Causal Paths We first establish the connection between ancestral
relationships and active causal paths. We exhaustively enumerate and define the potential pairwise
causal ancestral path relations in Figure 2 and Lemma 3.1 (proof in Appendix A.2):
Lemma 3.1 (Active Causal Ancestral Path Relation Enumeration) .Each pair of distinct nodes
xi, xj∈Vcan be in one of four possible active causal ancestral path relations: AP1) no active path
exists between xi, xj; AP2) there exists an active backdoor path between xi, xj, but there is noactive
frontdoor path between them; AP3) there exists an active frontdoor path between xi, xj, but there is
noactive backdoor path between them; AP4) there exists an active backdoor path between xi, xj,
and there exists an active frontdoor path between them.
Next, in Lemma 3.2, we summarize the connection between causal paths and ancestral relationships
(proof in Appendix A.3):
Lemma 3.2. The ancestral relationship between a pair of distinct nodes xi, xj∈Vcan be expressed
using active causal path relations: xi, xjare not ancestrally related if and only if they are in AP1 or
AP2 relation; and xi, xjare ancestrally related if and only if they are in AP3 or in AP4 relation.
The active causal ancestral path relation of a pair of nodes xi, xjthat are not ancestrally related
can be determined through marginal independence testing and sequential univariate regressions as
illustrated in Lemmas 3.3 and 3.4 (proofs in Appendices A.4, A.5):
Lemma 3.3 (AP1) .Vertices xi, xjare in AP1 relation if and only if xi⊥ ⊥xj.
Lemma 3.4 (AP2) .LetMbe the set of mutual ancestors between a pair of vertices xiandxj, i.e.,
M=An(xi)∩An(xj). LetxM
i, xM
jbe the result of sequentially regressing all mutual ancestors in
Mout of xi, xjwith univariate regressions, in any order. Then, let rj
ibe the residual of xM
jregressed
onxM
i, and ri
jbe the residual of xM
iregressed on xM
j. Suppose xi̸⊥ ⊥xj. Then, xi, xjare in AP2
relation if and only if rj
i⊥ ⊥xM
iandri
j⊥ ⊥xM
j.
If a pair of nodes xi, xjis ancestrally related, fully ascertaining their ancestral relation involves
discerning between the ancestor and descendent. As illustrated in Lemmas 3.5 and 3.6 (proofs in
Appendices A.6, A.7), this can be determined through marginal independence testing after sequential
univariate regressions with respect to the mutual ancestor set.
Lemma 3.5 (AP3) .Letrj
ibe the residual of the xjregressed on xi, and ri
jbe the residual of xi
regressed on xj. Vertices xi, xjare in AP3 relation if and only if xi̸⊥ ⊥xjand one of the following
4Algorithm 1 LHTS: Linear Hierarchical Topological Sort
1:input: features x1, . . . , x d∈V.
2:output: hierarchical topological sort πL(V).
3:initialize: ancestral relations set ARS .
4:Stage 1: AP1 Relations
5:forall pairs xi, xjdo
6: ifxi⊥ ⊥xjthen
7: Store xi, xjare not related in ARS .
8:Stage 2: AP3 Relations
9:forallxi, xjwith unknown relations do
10: Setrj
ias residual of xjregressed on xi
11: Setri
jas residual of xiregressed on xj.
12: ifrj
i⊥ ⊥xiandri
j̸⊥ ⊥xjthen
13: Store xi∈An(xj)inARS .
14: ifrj
i̸⊥ ⊥xiandri
j⊥ ⊥xjthen
15: Store xj∈An(xi)inARS .16:Stage 3: AP2 and AP4 Relations
17:while∃xi, xjwith unknown relations do
18: forxi, xjwith unknown relations do
19: Sequentially regress xi, xjon mutual
ancestors, store final residuals rj
i, ri
j.
20: ifrj
i⊥ ⊥xiandIri
j⊥ ⊥xjthen
21: Store xi, xjare not related in
ARS .
22: else if rj
i⊥ ⊥xiandri
j̸⊥ ⊥xjthen
23: Store xi∈An(xj)inARS .
24: else if rj
i̸⊥ ⊥xiandri
j⊥ ⊥xjthen
25: Store xj∈An(xi)inARS .
26:Stage 4: Obtain sort by subroutine AS
27:return: πL(V)←AS(ARS)
holds: 1) xi⊥ ⊥rj
iandxj̸⊥ ⊥ri
j, corresponding to xi∈An(xj), or 2) xi̸⊥ ⊥rj
iandxj⊥ ⊥ri
j,
corresponding to xj∈An(xi).
Lemma 3.6 (AP4) .LetM,rj
i, ri
j, xM
i, xM
jbe as defined in Lemma 3.4. Suppose xi, xjnot in
AP3 relation. Then, xi, xjare in AP4 relation if and only if xi̸⊥ ⊥xjand one of the following
holds: rj
i⊥ ⊥xM
iandri
j̸⊥ ⊥xM
jcorresponding to xi∈An(xj), or 2) rj
i̸⊥ ⊥xM
iandri
j⊥ ⊥xM
j
corresponding to xj∈An(xi).
Linear Hierarchical Topological Sort We propose LHTS in Algorithm 1 to leverage the above
results to discover a hierarchical topological ordering. Marginal independence tests and pairwise
regressions are used to identify active causal ancestral path relations between pairs of vertices.
LHTS discovers all pairwise active causal ancestral path relations: Stage 1 discovers all AP1 relations
using marginal independence tests; Stage 2 discovers all AP3 relations by detecting when pairwise
regressions yield an independent residual in only one direction; Stage 3 iteratively discovers all
AP2 and AP4 relations through marginal independence tests and pairwise sequential regressions
involving mutual ancestors. Note that Stage 2 can be viewed as a special case of Stage 3, where the
mutual ancestor set is empty. The process utilizes proofs provided in Appendices A.4, A.6, A.5, and
A.7, respectively. Stage 4 uses the complete set of ancestral relations to build the final hierarchical
topological sort via the subroutine AS(Algorithm 4 in Appendix A.8) by recursively peeling off
vertices with no unsorted ancestors. We show the correctness of Algorithm 1 in Theorem 3.7 (proof in
Appendix A.9) and subroutine AS(proof in Appendix A.8), as well as the worst case time complexity
in Theorem 3.8 (proof in Appendix A.10). We provide a walk-through of LHTS in Appendix A.11.
Theorem 3.7. Given a graph G, Algorithm 1 asymptotically finds a correct hierarchical sort of G.
Theorem 3.8. Given nsamples of dvertices generated by a LiNGAM, the worst case runtime
complexity of Algorithm 1 is upper bounded by O(d3n2).
LHTS can be seen as a generalization of DirectLiNGAM [ 38], where they recursively identify
root nodes as vertices that have either AP1 or AP3 relations with every remaining vertex at each
step. The next lemma, Lemma 3.9 (proof in Appendix A.12) implies that LHTS discovers all root
vertices in Stage 2, obtaining the first layer in the hierarchical sort. LHTS extends DirectLiNGAM by
discovering AP2 and AP4 relations, allowing us to recover a compact hierarchical sort.
Lemma 3.9. A vertex is a root vertex if and only if it has AP1 or AP3 relations with all other vertices.
4 Nonlinear setting
In this section, we develop a version of Algorithm 1 for the nonlinear ANM (Eq. (1)) setting. Rather
than detecting ancestor-descendant relationships, we outline the connection between active causal
5xi C
xjPP1
xi C
xjPP2
xi C
xjPP3
xi C
xjPP4
Figure 3: Enumeration of the potential active causal paths among a fixed variable xj, one of its
potential parents xi, and set C=PA(xj)\xi. Solid arrows denote parenthood relations, and
undirected dashed connections indicate the existence of active paths.
paths and parent-child relationships. We assume the unique identifiability of the nonlinear ANM [ 25],
and provide the conditions in Appendix B.1.
Nonlinear Topological Sort In the linear setting, we determined ancestral relations through a
sequence of pairwise regressions that led to independent residuals. However, a naive extension of
this method into the nonlinear setting would fail, as regressions yield independent residuals under
different conditions in the nonlinear case. For clarity, we demonstrate how LHTS fails to correctly
recover causal relationships in an exemplary 3-node DAG with nonlinear causal mechanisms.
Consider a DAG Gwith three vertices x1, x2, x3, where x1→x3, x2→x3. The functional causal
relationships are nonlinear, given by x1=ε1, x2=ε2, x3=x1x2+ε3, where the εis are mutually
independent noise variables. We focus on whether LHTS can recover the parent-child relationship
between x1andx3. LHTS finds that the relationship between x1, x3is unknown in Stage 1. In Stage
2, LHTS runs pairwise regressions between x1, x3butincorrectly concludes that x1, x3are not in
AP3 relation because neither pairwise regression provides an independent residual; both parents of
x3must be included in the covariate set for an independent residual to be recovered.
To handle nonlinear causal relationships, we shift our focus to searching for a different set of local
substructures: the connection between active causal parental paths and the existence of parent-child
relationships. We will use the existence of specific parent-child relationships to first obtain a superset
of root vertices, then prune away non-roots. Once all root vertices are identified, we build the
hierarchical topological sort through nonparametric regression, layer by layer.
Given a vertex xjand one of its potential parents xi, we first provide an enumeration of all potential
active casual parental path types between them with respect to the set C=PA(xj)\xi(which could
potentially be the empty set) in Figure 3 and Lemma 4.1 (proof in Appendix B.2).
Lemma 4.1 (Active Causal Parental Path Relation Enumeration) .Letxi, xj∈Vbe a pair of distinct
nodes, where xiis one of the potential parents of xj. Let C=PA(xj)\xi. There are in total
four possible types of active causal parental path relations between xiandxjwith respect to C:
PP1) no active path exists between xiandxj, and noactive path exists between xiandC; PP2) xi
directly causes xj(xi→xj), and noactive path exists between xiandC; PP3) xidirectly causes
xj(xi→xj), and there exists an active path between xiandC; PP4) xiandxjare not directly
causally related, and there exists an active path between xiandC.
Next, for a pair of distinct vertices xi, xj∈V, we establish the connection between pairwise inde-
pendence properties and active causal parental path relations in Lemma 4.2 (proof in Appendix B.3).
This allows us to reduce the cardinality of the potential pairs of vertices under consideration in the
later stages of the algorithm.
Lemma 4.2 (Non-PP1) .Vertices xi, xj∈Vare not in PP1 relation if and only if xi̸⊥ ⊥xj.
In Lemma 4.3, we show that all pairs of vertices that are in PP2 relation can be identified through
local nonparametric regressions (proof in B.4).
Lemma 4.3 (PP2) .Letxi, xj∈V,Pij={xk∈V:xk⊥ ⊥xi, xk̸⊥ ⊥xj},rj
ibe the residual of xj
nonparametrically regressed on xi, and rj
i,Pbe the residual of xjnonparametrically regressed on xi
and all xk∈Pij. Suppose xiandxjare not in PP1 relation. Then, xiandxjare in PP2 relation if
and only if one of the following holds: 1) xi⊥ ⊥rj
ior 2) xi⊥ ⊥rj
i,P.
Condition 1) of Lemma 4.3 is relevant when C=∅: in this case, pairwise regression identifies xi
as a parent of xj. Condition 2) is relevant when C̸=∅: we leverage the independence of xifrom
6Algorithm 2 NHTS : Nonlinear Hierarchical Topological Sort
1:input: vertices x1, . . . , x d∈V.
2:output: hierarchical sort πL(V).
3:initialize: parent relations set PRS .
4:Stage 1: Not-PP1 Relations
5:forall pairs xi, xjdo
6: ifxi̸⊥ ⊥xjthen
7: PRS : xi, xjnot in PP1 relations.
8:ifxiis in PP1 relation with all vertices then
9: PRS: xiis isolated, sort xi:πL(xi) = 1 .
10:Stage 2: PP2 Relations
11:forallxi, xjnot in PP1 relations do
12: Setrj
ias residual of xjregressed on xi.
13: Setrj
i,Pas residual of xjregressed on
xi, Pij.
14: ifxi⊥ ⊥rj
iorxi̸⊥ ⊥rj
i,Pthen15: PRS : xi, xjare in PP2 relations, xi
∈Pa(xj).
16:Stage 3: Root Identification
17:forxi∈PP2 relation do
18: Check if xidoes not d-separate any child
from all marginally dependent vertices. If
so, then update PRS : xiis a root, sort
xiinto layer 1, πL(xi) = 1 .
19:Stage 4: Obtain Sort
20:fork∈ {2, . . . , d }do
21: forall unsorted xido
22: Setrias the residual of xiregressed
on sorted features in πH.
23: Ifri⊥ ⊥xj∀xj∈πH, add xiinto
the current layer πL(xi) =k.
24:return πL(V)
the rest of xj’s parents to generate the set Pij, a set that contains C, but does not contain any of xj’s
descendants. If an independent residual were to be recovered by nonparametrically regressing xj
ontoxiandPij, we identify xias a parent of xj.
LetWbe the set of all parent vertices that are in PP2 relation with at least one vertex, i.e., the union
ofxisatisfying either condition of Lemma 4.3. We now show that all non-isolated root vertices are
contained in W, and they can be differentiated from non-roots in Wthat are also in PP2 relations.
Lemma 4.4 (Roots) .All non-isolated root vertices are contained in W. In addition, xi∈Wis a
root vertex if and only if 1) xiis not a known descendant of any xj∈W, and 2) for each xj∈W,
either a) xi⊥ ⊥xj, b)xjis in PP2 relation to xi, i.e., xj∈Ch(xi), or c) there exists a child of xi,
denoted by xk, that cannot be d-separated from xjgiven xi, i.e., xj̸⊥ ⊥xk|xi.
Lemma 4.4 relies on the following intuition: for any non-isolated root vertex xiand descendent
xk∈De(xi), there exists a child of xithat 1) lies on a directed path between xiandxk, and 2)
is in PP2 relation with xi. Vertex xiwill fail to d-separate this specific child from the marginally
dependent non-root descendent.1On the other hand, non-roots in Wmust d-separate the children
they are in PP2 relation to from all marginally dependent roots: this asymmetry allows non-roots to
be pruned. See Appendix B.5 for a detailed proof.
We propose a method, Algorithm 2, that leverages the above results to discover a hierarchical
topological ordering: we first use the above lemmas to identify the roots, then use nonparametric
regression to discover the topological sort.
In Stage 1, we discover all pairs xi, xjnot in PP1 relation by testing for marginal dependence; in
Stage 2, we leverage Lemma 4.3 to find the vertex pairs that are in PP2 relations, a superset of the
root vertices; in Stage 3 we prune non-roots by finding they d-seperate their children from at least one
other vertex in the superset (Lemma 4.4); in Stage 4 we identify vertices in the closest unknown layer
by regressing them on sorted nodes and finding independent residuals. We show the correctness of
Algorithm 2 in Theorem 4.5 (proof in Appendix B.6), and the worst case time complexity in Theorem
4.6 (proof in Appendix B.7). We provide a walk-through of Algorithm 2 in Appendix B.8.
Theorem 4.5. Given a graph G, Algorithm 2 asymptotically finds a correct hierarchical sort of G.
Theorem 4.6. Given nsamples of dvertices generated by a identifiable nonlinear ANM, the worst
case runtime complexity of Algorithm 2 is upper bounded by O(d3n3).
The number of nonparametric regressions run by NHTS in each step is actually inversely related to
the size of the covariate sets, while the number of regressions in each step of RESIT and NoGAM
are directly proportionate to the covariate set size. We provide a formal analysis of the reduction in
complexity for the worst case (fully connected DAG) in Theorem 4.7 (proof in Appendix B.9):
1Vertices xi, xjare said to be d-separated by a set Ziff there is no active path between xi, xjrelative to Z.
7C xi M xj
Figure 4: DAG corresponding to Lemma 5.1, which tests whether xi∈Pa(xj)(i.e., whether the red
arrow exists).
Algorithm 3 ED : Edge Discovery
1:input: features x1, . . . , x d∈V, topological
order π.
2:initialize: empty parent sets {Pa(xi)}d
i=1.
3:Step 1: Loop over π
4:forj∈ {2, . . . , d }do
5: Setxjequal to jthindex of π.
6: fori∈ {j−1, . . . , 1}do7: Setxiequal to ithindex of π.
8: SetZij=Pa(xi)∪Pa(xj).
9: Step 2: Test for Parenthood
10: ifxi̸⊥ ⊥xj|Zijthen
11: Update xi∈Pa(xj).
12:return {Pa(xi)}d
i=1.
Theorem 4.7. Consider a fully connected DAG G= (V, E)with nonlinear ANM. Let d:=|V|. Let
nNHTS
k be the number of nonparametric regressions with covariate set size k∈[d−2]run by NHTS
when sorting V; we similarly define nRESIT
k andnNoGAM
k respectively. Then, nNHTS
k =d−k, and
nRESIT
k =nNoGAM
k =k+ 1. This implies that for all k >d
2,nNHTS
k < nRESIT
k =nNoGAM
k .
5 Edge discovery
Parent selection from a topological ordering πvia regression is traditionally a straightforward task in
the infinite sample setting: for each vertex xi,πestablishes Ji={xk:π(xk)< π(xi)}, a superset
ofxi’s parents that contains none of xi’s descendants. The general strategy for pruning Pa(xi)from
Jiis to regress xionJiand check which xk∈Jiare relevant predictors. The key issue is that Ji
grows large in high-dimensional graphs: current edge pruning methods either make strong parametric
assumptions or suffer in sample complexity. Lasso and GAM methods impose linear and additive
models, failing to correctly identify parents in highly nonlinear settings. RESIT assumes a more
general nonlinear ANM, but requires huge sample sizes and oracle independence tests for accurate
parent set identification: authors in [ 25] confirm this, saying "despite [our] theoretical guarantee[s],
RESIT does not scale well to a high number of nodes."
We propose an entirely nonparametric constraint-based method that uses a local conditioning set
Zijto discover whether xi∈Pa(xj), rather than Ji, outperforming previous methods by relaxing
parametric assumptions and conditioning on fewer variables. The following lemma outlines a
sufficient condition for determining whether an edge exists between two vertices (proof in Appendix
C.1), visualized in Figure 4.
Lemma 5.1 (Parent Discovery) .Letπbe a topological ordering, xi, xjsuch that π(xi)< π(xj). Let
Zij=Cij∪Mij, where Cij={xk:xk∈Pa(xi), xk̸⊥ ⊥xj}, Mij={xk:xk∈Pa(xj), π(xi)<
π(xk)< π(xj)}.Then, xi→xj⇐⇒ xi̸⊥ ⊥xj|Zij.
The intuition is that to determine whether xi→xj, instead of conditioning on all potential ancestors
ofxj, it suffices to condition on potential confounders of xi, xj(Cij) and potential mediators between
xiandxj(Mij). This renders all backdoor and frontdoor paths inactive, except the frontdoor path
corresponding to a potential direct edge from xitoxj.
Edge Discovery We propose an algorithm that leverages the above results to discover the true edges
admitted by any topological ordering by running the described conditional independence test in a
specific ordering. We give the implementation for pruning a linear topological sort here, but it can be
generalized to a hierarchical version (see Appendix C.2). We show the correctness of Algorithm 3 in
Theorem 5.2 (proof in Appendix C.3).
Theorem 5.2. Given a correct linear topological ordering of G, Algorithm 3 asymptotically finds
correct parent sets PA (xi),∀xi∈G.
The key insight is to check each potential parent-offspring relation using the conditional independence
testxi⊥ ⊥xj|Zijsuch that previous steps in the algorithm obtain all vertices in both CijandMij.
8We first fix a vertex xjwhose parent set we want to discover. We then check if vertices ordered
before xjare parents of xjin reverse order, starting with the vertex immediately previous to xjin the
ordering. This process starts at the beginning of π, meaning we discover parent-offspring relations
from root to leaf (see Appendix C.3 for a detailed walk-through and proof). We show the worst case
time complexity of Algorithm 3 in Theorem 5.3 (proof in Appendix C.4).
Theorem 5.3. Given nsamples of dvertices generated by a model corresponding to a DAG G, the
runtime complexity of ED is upper bounded by O(d2n3).
6 Experiments
Setup Methods2are evaluated on 20 DAGs in each trial. The DAGs are randomly generated with the
Erdos-Renyi model [ 5]; the probability of an edge is set such that the average number of edges in
eachd-dimensional DAG is d. Gaussian, Uniform, or Laplace noise is used as the exogenous error.
In experiments for linear topological sorting methods, we use linear causal mechanisms to generate
the data; in experiments for nonlinear topological sorting methods (Figure 6) and edge pruning
algorithms (Figure 7), we use quadratic causal mechanisms to generate the data. Existing ANM
methods are prone to exploiting artifacts that are more common in simulated ANMs than real-world
data [ 28,29], inflating their performance on synthetic DAGs and leaving real-world applicability an
open question. To reduce concerns about such artifacts, data were generated with R2-sortability less
than0.7[29], and standardized to zero mean and unit variance [28].
Metrics Atopis equal to the percentage of edges that can be recovered by the returned topological
ordering (an edge cannot be recovered if a child is sorted before a parent). We note that Atopis
a normalized version of the topological ordering divergence Dtopdefined in [ 30]. Edge pruning
algorithms return a list of predicted parent sets for each vertex: F1= 2Precision ·Recall
Precision + Recallmeasures the
performance of these predictions.
Linear Topological Sorts Figure 5 demonstrates the performance of our linear topological ordering
algorithm, LHTS, in comparison with the benchmark algorithms, DirectLiNGAM [ 38] and R2-Sort
[29].R2-Sort is a heuristic sorting algorithm that exploits artifacts common in simulated ANMs; both
benchmarks are agnostic to the noise distribution. We observe that both DirectLiNGAM and LHTS
significantly outperform R2-sort. LHTS demonstrates asymptotic correctness in Figure 5(c), achiev-
ing near-perfect Atopatn= 2000 . However, LHTS has consistently lower Atopthan DirectLiNGAM
in Figure 5(a). On the other hand, LHTS encodes more causal information: the orderings produced by
LHTS in Figure 5(b) had roughly ∼70% fewer layers than the orderings produced by DirectLiNGAM,
reducing the size of potential parent sets Jby identifying many non-causal relationships.
Figure 5: Performance of LHTS on synthetic data. Top row: n= 500 with varying dimension d.
Bottom row: d= 10 with varying sample size n. See Appendix D.1 for runtime results.
2Code: https://github.com/Sujai1/hybrid-discovery.
9Nonlinear Topological Sorts Figure 6 illustrates the performance of our nonlinear topological
sorting algorithm. We take GES [ 3], GRaSP [ 12], GSP [ 39], DirectLiNGAM [ 38], NoGAM [ 20] ,
andR2-Sort [ 29] as baseline comparators that are all agnostic to the noise distribution. We excluded
PC and RESIT since in general they perform much worse than baseline methods [ 20]. We note that as
DirectLiNGAM, NoGAM, and NHTS are FCM-based methods, they each return a unique topological
ordering; however, as GES, GRaSP, and GSP are scoring-based methods [ 7], they return only a MEC.
All topological orderings contained within an MEC satisfy every conditional independence constraint
in the data, and therefore are all equally valid . To enable a fair comparison, we randomly select
one ordering permitted by an outputted MEC for evaluation. We note that NHTS outperformed all
baselines, achieving the highest median Atopin all trials. Furthermore, as expected from Theorem
4.6, NHTS ran up to 4×faster than NoGAM (see Appendix D.2). We provide additional experiments
over DAGs with increasing density in Appendix D.3.
Figure 6: Performance of NHTS on synthetic data, n= 300 , dimension d= 10 , with varying error
distributions: Gaussian, Laplace, Uniform (left, middle, right). See Appendix D.2 for runtime results.
Edge Pruning Figure 7 illustrates the performance of our edge pruning algorithm ED. We take
covariate hypothesis testing with GAMs (CAM-pruning [ 1]), Lasso regression, and RESIT as baseline
comparators that are all agnostic to the noise distribution. All algorithms were given correct linear
topological sorts: ED significantly outperformed all baselines, with the highest median F1score in all
trials. ED was slower than Lasso, but was significantly faster than the other nonlinear edge pruning
algorithms, CAM-pruning and RESIT. RESIT was excluded from higher-dimensional tests due to
runtime issues. The poor performance of baseline methods highlights the need for a sample efficient
nonparametric method for accurate causal discovery of nonlinear DGPs. We provide additional
experiments in settings with increasing density and varying noise distributions in Appendix D.4.
Figure 7: Performance of ED on synthetic data, uniform noise. Left, middle: n= 500 with varying
dimension d. Right: d= 10 with varying sample size n. See Appendix D.5 for runtime results.
Discussion In this paper we developed novel global causal discovery algorithms by searching for
and leveraging local causal relationships. We improved on previous topological ordering methods by
running fewer regressions, each with lower dimensionality, producing hierarchical topological sorts.
Additionally, we improved on previous edge pruning procedures by introducing a nonparametric
constraint-based method that conditions on far fewer variables to achieve greater recovery of parent
sets. We tested our methods on robustly generated synthetic data, and found that both our nonlinear
sort NHTS and edge pruning algorithm ED significantly outperformed baselines. Future work
includes extending the topological sorting algorithms to the full ANM setting with both linear and
nonlinear functions, simultaneously exploiting both ancestor-descendent and parent-child relations,
as well as adapting our approach to handle various forms of unmeasured confounding. Additionally,
we aim to develop statistical guarantees of sample complexity for our methods, extending previous
results [50] derived in the setting of nonlinear ANMs with Gaussian noise.
10References
[1]Bühlmann, P., Peters, J., and Ernest, J. CAM: Causal additive models, high-dimensional order
search and penalized regression. The Annals of Statistics , 2014. URL http://arxiv.org/
abs/1310.1533 .
[2]Cheng, D., Li, J., Liu, L., Yu, K., Duy Le, T., and Liu, J. Toward Unique and Unbiased Causal
Effect Estimation From Data With Hidden Variables. IEEE Transactions on Neural Networks
and Learning Systems , 2023. URL https://ieeexplore.ieee.org/document/9674196/ .
[3]Chickering, D. M. Learning Equivalence Classes of Bayesian Network Structures. Journal of
Machine Learning Research , 2002. URL https://arxiv.org/abs/1302.3566 .
[4]Comon, P. Independent component analysis, a new concept? Signal Processing , 1994. URL
https://hal.science/hal-00417283/document .
[5]Erdos, P. and Renyi, A. On the evolution of random graphs. Publicationes Mathemati-
cae, 1960. URL https://publi.math.unideb.hu/load_doi.php?pdoi=10_5486_PMD_
1959_6_3_4_12 .
[6]Gan, K., Jia, S., and Li, A. Greedy approximation algorithms for active sequential hypothesis
testing. Advances in Neural Information Processing Systems , 2021. URL https://dl.acm.
org/doi/proceedings/10.5555/3540261 .
[7]Glymour, C., Zhang, K., and Spirtes, P. Review of Causal Discovery Methods Based on
Graphical Models. Frontiers in Genetics , 2019. URL https://www.frontiersin.org/
article/10.3389/fgene.2019.00524/full .
[8]Gupta, S., Childers, D., and Lipton, Z. C. Local Causal Discovery for Estimating Causal Effects.
InProceedings of the 2nd Conference on Causal Learning and Reasoning (CLeaR) , 2023. URL
http://arxiv.org/abs/2302.08070 .
[9]Hoyer, P. O. and Hyttinen, A. Bayesian discovery of linear acyclic causal models. In Proceedings
of the 25th Conference on Uncertainty in Artificial Intelligence , 2009. URL https://arxiv.
org/abs/1205.2641 .
[10] Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., and Schölkopf, B. Nonlinear causal discovery
with additive noise models. In Advances in Neural Information Processing Systems , 2008. URL
https://dl.acm.org/doi/10.5555/2981780.2981866 .
[11] Hoyer, P. O., Shimizu, S., Kerminen, A. J., and Palviainen, M. Estimation of causal effects
using linear non-Gaussian causal models with hidden variables. International Journal of Ap-
proximate Reasoning , 2008. URL https://linkinghub.elsevier.com/retrieve/pii/
S0888613X08000212 .
[12] Lam, W.-Y ., Andrews, B., and Ramsey, J. Greedy Relaxations of the Sparsest Permutation
Algorithm. Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence , 2022.
URL https://proceedings.mlr.press/v180/lam22a/lam22a.pdf .
[13] Lee, J. J., Srinivasan, R., Ong, C. S., Alejo, D., Schena, S., Shpitser, I., Sussman, M., Whitman,
G. J., and Malinsky, D. Causal determinants of postoperative length of stay in cardiac surgery
using causal graphical learning. The Journal of Thoracic and Cardiovascular Surgery , 2022.
URL https://linkinghub.elsevier.com/retrieve/pii/S002252232200900X .
[14] Li, A., Lee, J., Montagna, F., Trevino, C., and Ness, R. Dodiscover: Causal discovery algorithms
in Python., 2022. URL https://github.com/py-why/dodiscover .
[15] Linder, J., Koplik, S. E., Kundaje, A., and Seelig, G. Deciphering the impact of genetic
variation on human polyadenylation using aparent2. Genome biology , 2022. URL https:
//pubmed.ncbi.nlm.nih.gov/36335397/ .
[16] Maasch, J., Pan, W., Gupta, S., Kuleshov, V ., Gan, K., and Wang, F. Local discovery by
partitioning: Polynomial-time causal discovery around exposure-outcome pairs. In Proceedings
of the 40th Conference on Uncertainy in Artificial Intelligence , 2024. URL https://arxiv.
org/abs/2310.17816 .
11[17] Marra, G. and Wood, S. Practical variable selection for generalized additive models. Computa-
tional Statistics Data Analysis , 2023. URL https://www.sciencedirect.com/science/
article/abs/pii/S0167947311000491 .
[18] Marra, G. and Wood, S. Regression Shrinkage and Selection via the Lasso. Computational Statis-
tics Data Analysis , 2023. URL https://www.sciencedirect.com/science/article/
abs/pii/S0167947311000491 .
[19] Montagna, F., Mastakouri, A. A., Eulig, E., Noceti, N., Rosasco, L., Janzing, D., Aragam,
B., and Locatello, F. Assumption violations in causal discovery and the robustness of score
matching. In 37th Conference on Neural Information Processing Systems , 2023. URL http:
//arxiv.org/abs/2310.13387 .
[20] Montagna, F., Noceti, N., Rosasco, L., Zhang, K., and Locatello, F. Causal Discovery with
Score Matching on Additive Models with Arbitrary Noise. Proceedings of the 2nd Conference
on Causal Learning and Reasoning , 2023. URL http://arxiv.org/abs/2304.03265 .
[21] Montagna, F., Noceti, N., Rosasco, L., Zhang, K., and Locatello, F. Scalable Causal Discovery
with Score Matching. Proceedings of the 2nd Conference on Causal Learning and Reasoning ,
2023. URL http://arxiv.org/abs/2304.03382 .
[22] Niu, W., Gao, Z., Song, L., and Li, L. Comprehensive Review and Empirical Evaluation of
Causal Discovery Algorithms for Numerical Data, 2024. URL https://arxiv.org/abs/
2407.13054 .
[23] Pearl, J., Glymour, M., and Jewell, N. P. Causal inference in statistics: a primer . Wi-
ley, 2016. URL https://www.datascienceassn.org/sites/default/files/CAUSAL%
20INFERENCE%20IN%20STATISTICS.pdf .
[24] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., Blondel, M.,
Prettenhofer, P., Weiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,
M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Ma-
chine Learning Research , 2011. URL https://jmlr.csail.mit.edu/papers/volume12/
pedregosa11a/pedregosa11a.pdf .
[25] Peters, J., Mooij, J., Janzing, D., and Schölkopf, B. Causal Discovery with Continuous Additive
Noise Models. Journal of Machine Learning Research , 2014. URL http://arxiv.org/abs/
1309.6779 .
[26] Ramos-Carreno, C. and Torrecilla, J. dcor: Distance correlation and energy statistics in
python. SoftwareX , 2023. URL https://www.sciencedirect.com/science/article/
pii/S2352711023000225 .
[27] Raskutti, G. and Uhler, C. Learning directed acyclic graphs based on sparsest permutations.
STAT , 2013. URL https://arxiv.org/abs/1307.0366 .
[28] Reisach, A., Seiler, C., and Weichwald, S. Beware of the simulated dag! causal discovery
benchmarks may be easy to game. Advances in Neural Information Processing Systems , 2021.
URL https://arxiv.org/abs/2102.13647 .
[29] Reisach, A. G., Tami, M., Seiler, C., Chambaz, A., and Weichwald, S. A Scale-Invariant
Sorting Criterion to Find a Causal Order in Additive Noise Models. 37th Conference on Neural
Information Processing Systems , 2023. URL http://arxiv.org/abs/2303.18211 .
[30] Rolland, P., Cevher, V ., Kleindessner, M., Russel, C., Scholkopf, B., Janzing, D., and Lo-
catello, F. Score Matching Enables Causal Discovery of Nonlinear Additive Noise Mod-
els. In Proceedings of the 39th International Conference on Machine Learning , 2022. URL
https://arxiv.org/abs/2203.04413 .
[31] Runge, J., Bathiany, S., Bollt, E., Camps-Valls, G., Coumou, D., Deyle, E., Glymour, C.,
Kretschmer, M., Mahecha, M. D., Muñoz-Marí, J., van Nes, E. H., Peters, J., Quax, R., Re-
ichstein, M., Scheffer, M., Schölkopf, B., Spirtes, P., Sugihara, G., Sun, J., Zhang, K., and
Zscheischler, J. Inferring causation from time series in Earth system sciences. Nature Commu-
nications , 2019. URL http://www.nature.com/articles/s41467-019-10105-3 .
12[32] Sanchez, P., Liu, X., O’Neil, A. Q., and Tsaftaris, S. A. Diffusion models for causal discovery via
topological ordering. Proceedings of the 39th International Conference on Machine Learning ,
2023. URL https://arxiv.org/abs/2210.06201 .
[33] Seabold, S. and Perktold, J. statsmodels: Econometric and statistical modeling with python. In
9th Python in Science Conference , 2010. URL https://www.statsmodels.org/stable/
index.html .
[34] Shah, A., Shanmugam, K., and Ahuja, K. Finding Valid Adjustments under Non-ignorability
with Minimal DAG Knowledge. Proceedings of the 25th International Conference on Artificial
Intelligence and Statistics , 2022. URL http://arxiv.org/abs/2106.11560 .
[35] Shah, A., Shanmugam, K., and Kocaoglu, M. Front-door adjustment beyond markov equivalence
with limited graph knowledge. Advances in Neural Information Processing Systems , 2023. URL
https://arxiv.org/abs/2306.11008 .
[36] Shimizu, S. Lingam: Non-Gaussian Methods for Estimating Causal Structures. Behaviormetrika ,
2014. URL http://link.springer.com/10.2333/bhmk.41.65 .
[37] Shimizu, S., Hoyer, P. O., Hyvarinen, A., and Kerminen, A. A Linear Non-Gaussian Acyclic
Model for Causal Discovery. Journal of Machine Learning Research , 2006. URL https:
//dl.acm.org/doi/10.5555/1248547.1248619 .
[38] Shimizu, S., Inazumi, T., Sogawa, Y ., Hyvarinen, A., Kawahara, Y ., Washio, T., Hoyer, P. O.,
Bollen, K., and Hoyer, P. Directlingam: A direct method for learning a linear non-gaussian
structural equation model. Journal of Machine Learning Research , 2011. URL https://dl.
acm.org/doi/10.5555/1953048.2021040 .
[39] Solus, L., Wang, Y ., and Uhler, C. Consistency guarantees for greedy permutation-based
causal inference algorithms. Biometrika , 2021. URL https://academic.oup.com/biomet/
article-abstract/108/4/795/6062392?redirectedFrom=fulltext .
[40] Spirtes, P. An Anytime Algorithm for Causal Inference. Proceedings of Machine Learning
Research , 2001. URL https://proceedings.mlr.press/r3/spirtes01a/spirtes01a.
pdf.
[41] Spirtes, P. and Zhang, K. Causal discovery and inference: concepts and recent method-
ological advances. Applied Informatics , 2016. URL https://applied-informatics-j.
springeropen.com/articles/10.1186/s40535-016-0018-x .
[42] Spirtes, P., Glymour, C., and Scheines, R. Causation, Prediction, and Search . Springer New
York, 2000. URL http://link.springer.com/10.1007/978-1-4612-2748-9 .
[43] Squires, C. Graphical Model Learning, 2021. URL https://github.com/uhlerlab/
graphical_model_learning .
[44] Steeg, G. Non-parametric Entropy Estimation Toolbox, 2014. URL https://github.com/
gregversteeg/NPEET .
[45] Takashi, I., Mayumi, I., Yan, Z., Takashi Nicholas, M., and Shohei, S. Python package
for causal discovery based on lingam. Journal of Machine Learning Research , 2023. URL
https://www.jmlr.org/papers/volume24/21-0321/21-0321.pdf .
[46] Wang, S. K., Nair, S., Li, R., Kraft, K., Pampari, A., Patel, A., Kang, J. B., Luong, C.,
Kundaje, A., and Chang, H. Y . Single-cell multiome of the human retina and deep learning
nominate causal variants in complex eye diseases. Cell genomics , 2022. URL https://www.
sciencedirect.com/science/article/pii/S2666979X22001069 .
[47] Wong, A. K., Sealfon, R. S., Theesfeld, C. L., and Troyanskaya, O. G. Decoding disease:
from genomes to networks to phenotypes. Nature Reviews Genetics , 2021. URL https:
//www.nature.com/articles/s41576-021-00389-x .
[48] Zhang, K. and Hyvarinen, A. On the Identifiability of the Post-Nonlinear Causal Model.
Uncertainty in Artificial Intelligence , 2009. URL https://arxiv.org/pdf/1205.2599 .
13[49] Zheng, Y ., Huang, B., Chen, W., Ramsey, J., Gong, M., Cai, R., Shimizu, S., Spirtes, P., and
Zhang, K. Causal-learn: Causal discovery in python. Journal of Machine Learning Research ,
2024. URL https://arxiv.org/abs/2307.16405 .
[50] Zhu, Z., Locatello, F., and Cevher, V . Sample Complexity Bounds for
Score-Matching: Causal Discovery and Generative Modeling, 2023. URL
https://proceedings.neurips.cc/paper_files/paper/2023/file/
0a3dc35a2391cabcb59a6b123544e3db-Paper-Conference.pdf .
14Appendix
A Assumptions, Proofs, and Walkthrough for LHTS
A.1 Identifiability in Linear Setting
As an example [ 25]: suppose X, N are normally distributed, X⊥ ⊥N, and Y=aX+N.If
¯a=aV ar (X)
a2V ar(X)+σ2̸=1
aand¯N=X−¯aY, then ¯N⊥ ⊥Yand the following DGP also fits the same
distribution of X, Y, N :X= ¯aY+¯N.To generalize the intuition that non-Gaussianity could lead
to identifiability, [37] use independent component analysis [4] to provide the following theorem:
Theorem A.1. Assume a linear SEM with graph G, where xi=P
k(j)<k(i)bijxj+εi,∀j=
1, . . . , d, where all εjare jointly independent and non-Gaussian distributed. Additionally, for each
j∈ {1, . . . , d }we require bij̸= 0for all j ∈Pa(xi). Then, Gis identifiable from the distribution.
We assume that the identifiability conditions described in Theorem A.1 hold throughout Section 3.
A.2 Proof of Lemma 3.1
Lemma 3.1 (Active Causal Ancestral Path Relation Enumeration) .Each pair of distinct nodes
xi, xj∈Vcan be in one of four possible active causal ancestral path relations: AP1) no active path
exists between xi, xj; AP2) there exists an active backdoor path between xi, xj, but there is noactive
frontdoor path between them; AP3) there exists an active frontdoor path between xi, xj, but there is
noactive backdoor path between them; AP4) there exists an active backdoor path between xi, xj,
and there exists an active frontdoor path between them.
Proof of Lemma 3.1. For a pair of nodes xiandxj, either xi⊥ ⊥xjorxi̸⊥ ⊥xj. If the former, then
xi, xjare in AP1 relation. Suppose the latter is true: xi̸⊥ ⊥xjimplies ∃at least one active path
between xi, xj. Active paths can either be backdoor or frontdoor paths: therefore either a frontdoor
path exists, a backdoor path exists, or both a frontdoor and backdoor path exist. Thus, xi, xjare
either in AP2, AP3, or AP4 relation.
A.3 Proof of Lemma 3.2
Lemma 3.2. The ancestral relationship between a pair of distinct nodes xi, xj∈Vcan be expressed
using active causal path relations: xi, xjare not ancestrally related if and only if they are in AP1 or
AP2 relation; and xi, xjare ancestrally related if and only if they are in AP3 or in AP4 relation.
Proof of Lemma 3.2. Note, xi, xjare ancestrally related if and only if there exists an active frontdoor
path between them. The conclusion follows immediately.
A.4 Proof of Lemma 3.3
Lemma 3.3 (AP1) .Vertices xi, xjare in AP1 relation if and only if xi⊥ ⊥xj.
Proof of Lemma 3.3. Note, xi⊥ ⊥xjif and only if there does not exist an active causal path between
them. The conclusion follows immediately.
A.5 Proof of Lemma 3.4
Lemma 3.4 (AP2) .LetMbe the set of mutual ancestors between a pair of vertices xiandxj, i.e.,
M=An(xi)∩An(xj). LetxM
i, xM
jbe the result of sequentially regressing all mutual ancestors in
Mout of xi, xjwith univariate regressions, in any order. Then, let rj
ibe the residual of xM
jregressed
onxM
i, and ri
jbe the residual of xM
iregressed on xM
j. Suppose xi̸⊥ ⊥xj. Then, xi, xjare in AP2
relation if and only if rj
i⊥ ⊥xM
iandri
j⊥ ⊥xM
j.
15Proof of Lemma 3.4. Under an identifiable LiNGAM, and given a linear topological ordering k:
V7→R,xiandxjadmit the following representation:
xi=X
k(m)<k(i)αimεm+εi,
xj=X
k(m)<k(j)αjmεm+εj,
where εmare jointly independent noise terms and non-Gaussian distributed, following from Theo-
rem A.1. Additionally, for each m:k(m)< k(i)we require αim̸= 0for all m∈Pa(xi). Similarly
forαjm.
We first show the forward direction. Suppose xi, xjare in AP2 relation. Note, xi̸⊥ ⊥xj. LetMbe
the complement of M, i.e., M=V\M. Then, ∃the following decomposition of xi, xj:
xi=X
xm∈Mαimεm+X
xm∈M∩An(xi)αimεm+εi,
xj=X
xm∈Mαjmεm+X
xm∈M∩An(xj)αjmεm+εi.
Then, the xM
i, xM
jhave the general form
xM
i=X
xm∈Yβimεm+εi,
xM
j=X
xm∈Zβjmεm+εj,
where Y⊆M∩An(xi)), Z⊆M∩An(xj)). Note, Y∩Z=∅. Asεiare all mutually independent,
this implies that rj
i⊥ ⊥ri
j.
We now show the reverse direction. Suppose xi̸⊥ ⊥xj,rj
i⊥ ⊥xM
i, and ri
j⊥ ⊥xM
j. Note that
xi̸⊥ ⊥xj=⇒xi, xjare not in AP1 relation. Suppose for contradiction that xi, xjare in AP3 or AP4
relation, where xi∈An(xj)(WLOG). Then ∃a frontdoor path between xi, xj, WLOG xi∈An(xj).
Regressing the descendent on the ancestor will always result in a dependent residual, even after
projecting out the influence of the mutual ancestors. Therefore, xM
j̸⊥ ⊥ri
j, leading to a contradiction.
Therefore, xi, xjmust be in AP2 relation.
A.6 Proof of Lemma 3.5
Lemma 3.5 (AP3) .Letrj
ibe the residual of the xjregressed on xi, and ri
jbe the residual of xi
regressed on xj. Vertices xi, xjare in AP3 relation if and only if xi̸⊥ ⊥xjand one of the following
holds: 1) xi⊥ ⊥rj
iandxj̸⊥ ⊥ri
j, corresponding to xi∈An(xj), or 2) xi̸⊥ ⊥rj
iandxj⊥ ⊥ri
j,
corresponding to xj∈An(xi).
Proof of Lemma 3.5. We first show the forwards direction. Suppose xi, xjare in AP3relation. Note
an active path exists between xi, xj, soxi̸⊥ ⊥xj. WLOG, let xi∈An(xj). Then, xi, xjadmit the
following representation:
xi=xi
xj=bjixi+X
xm∈An(xj)\xiαjmxm+εj.
Note, as there does not exist a backdoor path between xi, xj, we have xi⊥ ⊥xm∀xm∈An(xj)\xi.
Therefore, rj
i⊥ ⊥xi. Note that xj∈De(xi): pairwise regression will leave ri
j̸⊥ ⊥xjas this an
example of reverse causality, a special case of endogeneity [23].
16We now show the reverse direction. WLOG, suppose xi, xjsatisfy condition 1) in Lemma 3.5. As
xi̸⊥ ⊥xj, they cannot be in AP1 relation. Suppose for contradiction that they are in AP2 or AP4
relation. Then, ∃a backdoor path between xi, xj, and a confounding xzon that backdoor path. As
xzconfounds xi, xjand is not adjusted for in a pairwise regression, we have xi̸⊥ ⊥rj
i, xj̸⊥ ⊥xi
j, a
contradiction. Therefore, xi, xjmust be in AP2 relation.
A.7 Proof of Lemma 3.6
Lemma 3.6 (AP4) .LetM,rj
i, ri
j, xM
i, xM
jbe as defined in Lemma 3.4. Suppose xi, xjnot in
AP3 relation. Then, xi, xjare in AP4 relation if and only if xi̸⊥ ⊥xjand one of the following
holds: rj
i⊥ ⊥xM
iandri
j̸⊥ ⊥xM
jcorresponding to xi∈An(xj), or 2) rj
i̸⊥ ⊥xM
iandri
j⊥ ⊥xM
j
corresponding to xj∈An(xi).
Proof of Lemma 3.6. We first show the forward direction. Suppose xi, xjare in AP4 relation. Note
that an active path exists between xi, xj, soxi̸⊥ ⊥xj. WLOG, suppose xi,∈An(xj). Note, xM
i, xM
j
are the result of projecting mutual ancestors Mout of xi, xj. Therefore, xM
i, xM
jadmit the following
representation:
xM
i=X
xm∈M∩An(xi)αimεm+εi
xM
j=αjiεi+X
xm∈M∩An(xj)αjmεm+εj
Note that M∩An(xi)∩(M∩An(xj)∪xj) =∅. Therefore, rj
i⊥ ⊥xM
i. Note that εi∈bothxM
i, xM
j,
therefore ri
j̸⊥ ⊥xM
j.
We now show the reverse direction. Suppose condition 1) holds, i.e., xi̸⊥ ⊥xjand WLOG rj
i⊥ ⊥
xM
i, ri
j̸⊥ ⊥xM
j. Suppose for contradiction that xi, xjnot in AP4 relation. They cannot be in AP1
relation because xi̸⊥ ⊥xj. They cannot be in AP2 relation because xM
j̸⊥ ⊥ri
j. By assumption they
are not in AP3 relation. Therefore, we have a contradiction, therefore they are in AP4 relation.
A.8 Ancestor Sort
Algorithm 4 AS : Ancestor Sort
1:input: set of ancestral relations ARS .
2:initialize: hierarchical topological order O[], remaining variables RV [x1, . . . , x d], layer L [].
3:while RVis not empty do
4:Stage 1: Determine Roots
5: forxi∈RVdo
6: ifxihas no ancestors ∈RVthen
7: Append xitoL.
8:Stage 2: Update Sort and Remaining Variables
9: Append LtoO.
10: Remove all vertices in Lfrom RV.
11:return O
Overview In each iteration, this algorithm uses the set of ancestral relations to identify which
vertices have no ancestors amongst the vertices that have yet to be sorted. It peels those nodes off,
adding them as a layer to the hierarchical topological sort. First the roots are peeled off, then the next
layer, and so on.
Proof of Correctness
Proof. The input to the algorithm is an ancestor table ARS cataloging all ancestral relations for every
pair of nodes in an unknown DAG G. LetHbe the minimum number of layers ( H−1is the length
of the longest directed path in the graph) necessary in a valid hierarchical topological sort of G.
17Base Cases (1,2)
1.H= 1. None of the nodes have ancestors, so all nodes are added to layer 1 in Stage 1.
2.H= 2. Nodes with no ancestors are added to layer 1 in Stage 1, and then removed from the
remaining variables in Stage 2. As H= 2, the longest directed path is 1, so all nodes in
the remaining variables must have no ancestors in the remaining variables (otherwise, this
would make the longest directed path 2). The nodes are added to layer 2 in Stage 1, and
removed from the remaining variables in Stage 2. Remaining variables is empty, so now the
order is correctly returned.
Inductive Assumption For any graph GwithH < k , Hierarchical Topological Sort will return a
minimal hierarchical topological ordering.
We now show that hierarchical topological sort now yields a minimal hierarchical topological ordering
forGwhere H=k.
1.In the first iteration of Stage 1, nodes with no ancestors will be added to layer 1, then
removed from the remaining variables in Stage 2.
2.At this point, note that we can consider the induced subgraph formed by the nodes left in
remaining, G′. The minimal hierarchical topological ordering that represents G′must have
k−1layers. It cannot be greater than k−1, because G′is a subgraph of Gand we removed
nodes in layer 1 of, reducing the maximal path length by 1. It cannot be less than k−1,
because that would imply that kwas not the minimal number of layers needed to represent
G.
3.By Inductive Assumption, Hierarchical Topological sort will return the a correct minimal
hierarchical topological ordering for the induced subgraph G′.
4.Appending the layer 1 and the sort produced by G′yields the full hierarchical topological
sort for G.
Inductive assumption is satisfied for H=k, so for a graph Gwith arbitrary H, the algorithm recovers
the correct hierarchical topological sort.
A.9 Proof of Correctness for Linear Hierarchical Topological Sort
Theorem A.2. 3.7 Given a graph G, Algorithm 1 asymptotically finds a correct hierarchical sort of
G.
Proof. The goal is to show that all ancestral relations between distinct pair of nodes xi, xj∈V
determined by LHTS in Stages 1, 2, and 3 are correct. By A.8, Stage 4 will return a valid hierarchical
topological sort given fully identified ancestral relations.
Stage 1 identifies xi, xjas in AP1 relaton if and only if xi⊥ ⊥xj: it follows by Lemma 3.3 that
vertices in AP1 relation are correctly identified, and vertices in AP2, AP3 and AP4 relation are not
identified.
Stage 2 identifies xi, xjunidentified in Stage 1 as in AP3 relation if and only if after pairwise
regression, one of the residuals is dependent, and the other is independent: it follows by Lemma 3.5
vertices in AP3 relation are correctly identified, and vertices in AP2 or AP4 relation are not identified.
Consider a hierarchical topological sort of DAG G πH, with hlayers. Note, Layer 1 of πHconsists
of root nodes. Ancestral relations between root nodes and all other nodes are of type AP1 and AP3,
and were discovered in Stage 1 and Stage 2.
We induct on layers to show that Stage 3 recovers all ancestral relations of type AP2 and AP4, one
layer at a time in each iteration.
18Base Iterations (1,2)
1.All ancestral relationships are known for nodes in layer 1 of πH, therefore all mutual
ancestors of layer 2 and every lower layer are known. Then, by Lemma 3.4 and Lemma 3.6
ancestral relations of type AP2 and AP3 between vertices in layer 2 and all lower layers are
discovered in iteration 1.
2.All ancestral relationships are known for nodes in layer 1 and 2 of πH, therefore all mutual
ancestors of layer 3 and every lower layer are known. Then, by Lemma 3.4 and Lemma 3.6
ancestral relations of type AP2 and AP3 between vertices in layer 2 and all lower layers are
discovered in iteration 2.
Iteration k−1, Inductive Assumption We have recovered all ancestral relationships of nodes in
layers 0tok−1.
1.As ancestral relationships of nodes in layers 0tok−1are known, mutual ancestors of
vertices in layer kand every lower layer are known, so by Lemma 3.4 and 3.4 all ancestral
relations of type AP2 and AP4 between vertices in layer kand every lower layer are
discovered.
Iteration k−1Inductive Assumption is satisfied for iteration k, therefore we recover all ancestral
relations of type AP2 and AP4 and 4) for nodes in layers 0tok. Thus, for a DAG with an arbitrary
number of layers, Stage 3 recovers all ancestral relations of type AP2 and AP4.
A.10 Time Complexity Proof for Linear Hierarchical Topological Sort
Proof. In Stage 1, LHTS runs O(d2)marginal independence tests that each have O(n2)complexity.
In each step for Stage 2 and Stage 3, LHTS runs O(d2)marginal independence tests each with O(n2)
complexity. In the worse case of a fully connected DAG, there are d2steps in total, across Stage 2
and Stage 3: this is because in each step one layer of the layered sort DAG is identified, and a fully
connected DAG has dlayers. Therefore, the overall sample complexity of LHTS is O(d3n2).
A.11 Walk-through of LHTS
The following diagram illustrates each stage of LHTS on an exemplary 5-node DAG:
All vertices are dependent on each other, so no AP1 relations are discovered in Stage 1. In Stage 2,
vertex A is discovered in AP2 relation to all vertices, and vertex D is discovered in AP2 relation to
vertex E. In Stage 3, vertices B and C are discovered to be in AP3 relation to each other; then, we
discover vertex B in AP4 relation to vertex D, and vertex C in AP4 relation to vertex D. Therefore, in
Stage 4, we recover the topological sort after running the AS subroutine.
19A.12 Proof of Lemma 3.9: Root Active Causal Ancestral Path Relations
Lemma 3.9. A vertex is a root vertex if and only if it has AP1 or AP3 relations with all other vertices.
Proof of Lemma 3.9. We first show the forward direction. If xiis a root, then De(xi) =∅. Therefore,
it cannot have a backdoor path between it and any other vertex, therefore it must be in either AP1or
AP3relation with other vertices.
We now show the reverse direction. If xiis in AP1 or AP3 (as an ancestor) relations with all other
nodes, it cannot have any parents. Therefore, xiis a root.
20B Assumptions, Proofs, and Walkthrough for NHTS
B.1 Identifiability in Nonlinear Setting
Following the style of [ 19], we first observe that the following condition guarantees that the observed
distribution of a pair of variables xi, xjcan only be generated by a unique ANM:
Condition B.1 (Hoyer & Hyttinen [9]).Given a bivariate model xi=εi, xj=fj(xi)+εjgenerated
according to (1), we call the SEM an identifiable bivariate ANM if the triple (fi, pεi, pεj)does not
solve the differential equation k′′′=k′′(−g′′′f′
g′′+f′′
f′)−2g′′f′′f′+g′f′′′+g′g′′′f′′f′
g′′−g′(f′′)2
f′for
allxi, xjsuch that f′(xi)g′′(xj−fj(xi))̸= 0, where pεi, pεjare the density of εi, εj,f=fj, k=
logpεi, g=pεj. The arguments xj−fj(xi), xiandxiofg, kandfrespectively, are removed for
readability.
There is a generalization of this condition to the multivariate nonlinear ANM proved by [25]:
Theorem B.2. (Peters et al. [ 25]). An ANM corresponding to DAG Gis identifiable if ∀xj∈V, xi∈
Pa(xj)and all sets S⊆Vwith Pa(xj)\ {i} ⊆S⊆De(j)\ {xi, xj},∃XSwith positive joint
density such that the triple
fj(Pa(j)\ {xi}, xi), pxi|Xs, pεj
satisfies Condition B.1, and fjare
non-constant in all arguments.
We assume that the identifiability conditions described in Theorem B.2 hold throughout Section 4.
B.2 Proof of Lemma 4.1
Lemma 4.1 (Active Causal Parental Path Relation Enumeration) .Letxi, xj∈Vbe a pair of distinct
nodes, where xiis one of the potential parents of xj. Let C=PA(xj)\xi. There are in total
four possible types of active causal parental path relations between xiandxjwith respect to C:
PP1) no active path exists between xiandxj, and noactive path exists between xiandC; PP2) xi
directly causes xj(xi→xj), and noactive path exists between xiandC; PP3) xidirectly causes
xj(xi→xj), and there exists an active path between xiandC; PP4) xiandxjare not directly
causally related, and there exists an active path between xiandC.
Proof. Either xi⊥ ⊥xjorxi̸⊥ ⊥xj: if the former, then xi, xjare in PP1 relation. Suppose the latter
is true. Then, either xi⊥ ⊥Corxi̸⊥ ⊥C: if the former, then xi, xjare in PP2 relation. Suppose the
latter is true. Then, either xi∈PA(xj)orxi̸∈PA(xj). If the former, then xi, xjare in PP3 relation,
and if the latter, then xi, xjare in PP4 relation.
B.3 Proof of Lemma 4.2
Lemma 4.2 (Non-PP1) .Vertices xi, xj∈Vare not in PP1 relation if and only if xi̸⊥ ⊥xj.
Proof. We first show the forward direction. If xi, xjare not in PP1 relation, then either xi∈Pa(xj)
or there exists an active causal path between xi, xj; therefore, xi̸⊥ ⊥xj.
We now show the reverse direction. If xi̸⊥ ⊥xjthere exists an active causal path between xi, xj,
therefore they cannot be in PP1 relation.
B.4 Proof of Lemma 4.3
Lemma 4.3 (PP2) .Letxi, xj∈V,Pij={xk∈V:xk⊥ ⊥xi, xk̸⊥ ⊥xj},rj
ibe the residual of xj
nonparametrically regressed on xi, and rj
i,Pbe the residual of xjnonparametrically regressed on xi
and all xk∈Pij. Suppose xiandxjare not in PP1 relation. Then, xiandxjare in PP2 relation if
and only if one of the following holds: 1) xi⊥ ⊥rj
ior 2) xi⊥ ⊥rj
i,P.
Proof. Note, there exists two sub cases of PP2 relation: PP2a) xiis the only parent of xj, i.e.|C|= 0
and PP2b) xiis not the only parent of xj, i.e.,|C|>0.
21We first show the forward direction. Suppose xi, xjare in PP2a relation. Then, as the
xj=fj(xi) +εj
xi⊥ ⊥rj
i. Suppose that xi, xjare in a PP2b relation. Consider Pij: note that xi⊥ ⊥C, and xj̸⊥ ⊥C,
C⊆Pij. Note that De(xj)̸⊥ ⊥xi, therefore De(xj)̸∈Pij. Therefore, Pijcontains all parents of xj,
and excludes all descendent of xj. Then, as
xj=fj(xi, C) +εj
we have xi⊥ ⊥ri
jP.
We now show the reverse direction. Suppose xi⊥ ⊥rj
i. Note by assumption xi, xjare not in PP1
relation. Suppose for contradiction that xi, xjare in PP3 or PP4 relation. Then, there must exist
at least one active path between xi, xjthat goes through C. If∃a backdoor path from xi, xj, then
at least one vertex in Cis a confounder of xi, xj, and thus xi̸⊥ ⊥rj
i; therefore, there must exist a
frontdoor path between xi, xjthrough C, with at least one mediator xk. Note that xkis dependent on
bothxiandxj, and therefore is excluded from Pij. The exclusion of xkintroduces omitted variable
bias [23], and thus xi̸⊥ ⊥rj
iP, leading to contradiction. Thus, xi, xjare in PP2 relation.
B.5 Proof of Lemma 4.4
Lemma 4.4 (Roots) .All non-isolated root vertices are contained in W. In addition, xi∈Wis a
root vertex if and only if 1) xiis not a known descendant of any xj∈W, and 2) for each xj∈W,
either a) xi⊥ ⊥xj, b)xjis in PP2 relation to xi, i.e., xj∈Ch(xi), or c) there exists a child of xi,
denoted by xk, that cannot be d-separated from xjgiven xi, i.e., xj̸⊥ ⊥xk|xi.
Proof. We first show that all non-isolated root vertices are contained in W. Note, if a root xiis not
isolated, it has at least one child xj. By definition, xihas no parents, so there cannot exist a backdoor
path between xi, xj. Consider xj∈Ch(xi)that is in a hierarchical layer closest to xj. Note that
there cannot exist an active path from xitoPa(xj)\xi, because that would imply that xjis not the
child of xithat is in the closest hierarchical layer. Therefore, xi, xjmust be in PP2 relation, and
xi∈W.
We now show the forward direction. Suppose xiis a root vertex. Then, xihas no parents, so it
cannot be the descendent of any vertex, let alone any vertex in W: Condition 1) is satisfied. For each
xj∈W, there either exists an active path between xi, xj, or there does not. If there does not exist an
active path, then xi⊥ ⊥xj, satisfying Condition a). If there does exist an active path, then as xiis a
root,xj∈De(xi): the active path must be a frontdoor path. Consider the vertex xk∈Ch(xj)that
is in a hierarchical layer closest to xj, AND is along the active frontdoor path from xitoxj. Note
thatxiis in PP2 relation with xk. Ifxk=xj, then Condition b) is satisfied. If xk̸=xj, then we
note that there is an active frontdoor path between xkandxj. Therefore, xk, xjare dependent even
after conditioning on the mutual ancestor xi, i.e., xj̸⊥ ⊥xk|xi: Condition c) is satisfied. Therefore,
Condition 2) is always satisfied.
We now show the reverse direction. Suppose Condition 1) and Condition 2) are satisfied for xj∈W.
Suppose for contradiction that xjis not a root. Note that ∃xi∈Wsuch that xiis a root and
xj∈De(xi). Consider any vertex xk∈Ch(xj)such that xjis in PP2 relation to xk. Note that
all active paths between xiandxkmust be frontdoor paths, and xjmust lie along all these active
frontdoor paths: otherwise, this would contradict that xjandxkare in PP2 relation. Therefore,
xi⊥ ⊥xk|xj. This contradicts Condition 2c), therefore Condition 1) and Condition 2) cannot hold
true for xi∈Wthat is not a root.
B.6 Proof of Correctness for Nonlinear Hierarchical Topological Sort
Theorem 4.5. Given a graph G, Algorithm 2 asymptotically finds a correct hierarchical sort of G.
22Proof. This proof can be broken into two parts: we first show 1) all root vertices are identified after
Stage 3, then we show 2) that Stage 4 recovers a correct hierarchical topological sort given the root
vertices.
Note that Stage 1 identifies xi, xjas not in PP1 relation if and only if xi̸⊥ ⊥xj: it follows by Lemma
4.2 that these identifications are correct.
Note that Stage 2 identifies the set xi∈Wif and only if ∃xjsuch that either 1) xi⊥ ⊥rj
ior 2)
xi⊥ ⊥rj
i,P: it follows by Lemma 4.3 that these identifications are correct.
Note that Stage 3 explicitly uses a condition from Lemma 4.4 to find all non-isolated root vertices - it
is therefore correct. Note that all isolated root vertices were identified in Stage 1. Therefore, all roots
are identified.
Consider a hierarchical topological sort of DAG G πH: suppose the maximal directed path in Ghas
sizeh. Note that layer 1 of πHconsists of root nodes, and therefore has been identified. We note the
following observation : when xiis nonparametrically regressed on all xk∈πHto obtain residual ri,
if∃an ancestor of xinot in πH, then riwill be dependent on at least one xi∈πHdue to omitted
variable bias [23].
We now induct on the layers of πHto show that Stage 4 correctly recovers all layers.
Base Iterations (1,2)
1. All roots are identified, so layer 1 of πHis identified.
2.Byobservation , only xiin layer 2 of πHwill have independent residuals riafter nonpara-
metric regression on xi∈πH, so layer 2 is correctly identified.
Iteration k−1, Inductive Assumption We have recovered all layers of πHup to layer k−1.
1.Byobservation , only xiin layer kofπHwill have independent residuals riafter nonpara-
metric regression on xi∈πH, so layer kis correctly identified.
Iteration k−1Inductive Assumption is satisfied for iteration k, therefore we recover all layers of πH
from 1tok. Thus, for a DAG with an arbitrary number of layers, Stage 4 recovers the full hierarchical
topological sort.
B.7 Time Complexity for NHTS
Theorem 4.6. Given nsamples of dvertices generated by a identifiable nonlinear ANM, the worst
case runtime complexity of Algorithm 2 is upper bounded by O(d3n3).
Proof. In Stage 1, NHTS runs O(d2)marginal independence tests that each have O(n3)complexity.
In Stage 2, NHTS runs O(d2)nonparametric regressions and O(d2)marginal independence tests, each
of which have O(n3)complexity. In Stage 3 NHTS runs at most O(d2)conditional independence
tests, each of which has O(n3)complexity. In the worst case of a fully connected DAG, NHTS goes
through O(d)steps in Stage 4: in each step of Stage 4, NHTS runs O(d)nonparametric regressions
andO(d2)marginal independence tests, each of which has O(n3)complexity. Therefore, the overall
sample complexity of NHTS is O(d3n3).
B.8 Walk-through of NHTS
The following diagram illustrates each stage of NHTS on an exemplary 5-node DAG:
In Stage 1, we discover that none of the vertices are in PP1 relations. In Stage 2, we discover that
vertex A is in PP2 relation to vertices B and C, and vertex D is in PP2 relation to vertex E: therefore,
A and D are our potential roots. In Stage 3, we find that vertex D d-separates its child, vertex E, from
vertex A: therefore A is the root vertex. In Stage 4, we regress the unsorted vertices onto A, finding
that B,C are independent of the residual in the first round, then D, then E in the last round.
23B.9 Time Complexity of NHTS vs RESIT, NoGAM
Theorem 4.7. Consider a fully connected DAG G= (V, E)with nonlinear ANM. Let d:=|V|. Let
nNHTS
k be the number of nonparametric regressions with covariate set size k∈[d−2]run by NHTS
when sorting V; we similarly define nRESIT
k andnNoGAM
k respectively. Then, nNHTS
k =d−k, and
nRESIT
k =nNoGAM
k =k+ 1. This implies that for all k >d
2,nNHTS
k < nRESIT
k =nNoGAM
k .
Proof. RESIT and NoGAM both identify leaf vertices in an iterative fashion, regressing each
unsorted vertex on the rest of the unsorted vertices; RESIT tests the residual for independence with
the covariate set while NoGAM uses the residual for score matching. Therefore, the number of
regressions run in both methods in each step equals one plus the covariate set size. Therefore, when
the covariate set size is k >d
2, there are k+ 1regressions run.
In the case of a fully directed graph, the first stage of NHTS only runs pairwise regressions
with empty conditioning sets. After the first stage, NHTS regresses each unsorted vertex onto all
sorted vertices, finding vertices with independent residuals. Therefore, the number of regressions run
is equal to dminus the size of the covariate set. Therefore, when the covariate set size is k >d
2, there
ared−kregressions run.
24C Proofs and Walkthrough for ED
C.1 Proof of Lemma
Lemma 5.1 (Parent Discovery) .Letπbe a topological ordering, xi, xjsuch that π(xi)< π(xj). Let
Zij=Cij∪Mij, where Cij={xk:xk∈Pa(xi), xk̸⊥ ⊥xj}, Mij={xk:xk∈Pa(xj), π(xi)<
π(xk)< π(xj)}.Then, xi→xj⇐⇒ xi̸⊥ ⊥xj|Zij.
Proof. Note, Ci,jblocks all backdoor paths between xi, xjandMi,jblocks all frontdoor paths
between xi, xj, except the direct edge.
We first show the forward direction. If xi̸⊥ ⊥xj|Zi,j, there must be an direct edge between xi, xj,
and as k(i)< k(j), we have xi→xj.
We now show the reverse direction. If xi→xj, then there does not exist a conditioning set that
makes xi⊥ ⊥xj, which implies xi̸⊥ ⊥xj|Zi,j.
C.2 Hierarchical Version of Edge Discovery
The current implementation of Edge Discovery takes a linear hierarchical topological sort as input;
this is equivalent to a hierarchical topological sort where every layer contains only one vertex. To
generalize ED to a hierarchical sort, we simply adjust how the algorithm loops over the sort, and
which vertices are included in which conditioning sets Cij, Mij. We give an example of how the
latter would change: suppose the algorithm is checking whether xi∈Pa(xj)where xiis in layer 2
(πL(xi) = 2 ) and xjis layer 6 ( πL(xj) = 6 ).Mijwould be equal to the vertices who are parents
ofxjthat are in layers that are between layer 2 and 6: i.e. Mijequals xk∈Pa(xj)such that
2< πL(xk)<6.Cij, would stay the same, just being equal to Pa(xi). Now, to generalize the
looping part, notice that the linear version of ED loops over indices of the linear topological sort.
The hierarchical version of Edge Discovery would loop over and within layers of the topological
sort. We give an example of this: suppose the algorithm has found all parents of vertices in layer
4 or lower: the next step would be to find the parents of vertices in layer 5. The algorithm sets any
vertex in layer 5 as xj, and first finds all parents of xjin the immediately preceding layer, layer 4.
The algorithm then finds all parents of xjin the layer preceding layer 4, layer 3. This is essentially
the same approach as the linear version, except the size of the conditioning sets and number of
conditional tests run is far lower, as the layers provide extra knowledge, limiting which variables can
be parents or confounders of other variables.
C.3 Edge Discovery
We first provide a walk-through of ED on an exemplary 5-node DAG:
25We first use a topological ordering algorithm to obtain a sort from the DAG: the sort corresponds to a
fully connected DAG, from which we will prune spurious edges. In the first two iterations we find that
A→B, butB̸→C, asB⊥ ⊥C|A. In the third iteration we find that B→D, C→D, butA̸→D,
asA⊥ ⊥D|B, C . In the final iteration, we find that D→E, butA̸→E, B̸→E, C̸→E, given
A⊥ ⊥E|D, B⊥ ⊥E|D, C⊥ ⊥E|D. Therefore, we recover the correct set causal edges, removing all
spurious edges. We now present a statement of correctness and proof for ED:
Theorem 5.2. Given a correct linear topological ordering of G, Algorithm 3 asymptotically finds
correct parent sets PA (xi),∀xi∈G.
Proof of Correctness
Background We are given a linear topological sort π(V) = [ x1, x2, . . . , x d](where x1has no
parents). For xathat appears before xb, letZa,b=Ca,b∪Ma,b, where Ca,bis the set of potential
confounders of xa, xb, andMa,bis the set of known mediators of xa, xb(defined precisely in Lemma
5.1). Let x⊥ ⊥y|·be the value of a conditional independence test between xandy.
Proof.
Base Case Iterations (1,2,3,4)
1. Finding parents of x1: by the topological sort, x1has no parents.
2.Finding parents of x2: As x1is the only possible parent of x2, there are no possible
confounders or mediators between x1andx2, so we initialize Z1,2=∅, then by L3
x1̸⊥ ⊥x2|Z1,2=x1̸⊥ ⊥x2⇐⇒ x1→x2. Thus, we recover all possible edges between
[x1, x2].
3.Finding parents of x3: first we check whether x2→x3, then we check whether x1→x3.
From iteration 2, we know all edges between [x1, x2].
Case 1 : ifx1→x2, then it is a possible confounder. Therefore, we initialize C2,3=x1. By
topological sort there is no mediator between x2andx3, therefore we initialize M2,3=∅.
Then by Lemma 5.1 x2̸⊥ ⊥x3|Z2,3=x2̸⊥ ⊥x3|x1⇐⇒ x2→x3. If an edge between x2
andx3exists, initialize M1,3accordingly. Note, x1has no parents, so we initialize C1,3=∅.
Then, by Lemma 5.1 x1̸⊥ ⊥x3|Z1,3⇐⇒ x1→x3. We recover all possible edges between
[x1, x2, x3].
Case 2 : ifx1̸→x2, there are no possible confounders between x2andx3. Therefore, we
initialize C2,3=∅. By topological sort there is no mediator between x2andx3, therefore
we initialize M2,3=∅. Then by Lemma 5.1 x2̸⊥ ⊥x3|Z2,3=x2̸⊥ ⊥x3⇐⇒ x2→x3.
Note, x1has no parents, so we initialize C1,3=∅. Note, as x1̸→x2, there are no
possible mediators between x1andx3: we initialize M1,3=∅. Then, by Lemma 5.1
x1̸⊥ ⊥x3|Z1,3=x1̸⊥ ⊥x3⇐⇒ x1→x3. We recover all possible edges between
[x1, x2, x3].
4.Finding parents of x4: first we check whether x3→x4, then whether x2→x4, then
whether x1→x4. From iteration 3, we know all edges between [x1, x2, x3].
Case 1 : if[x1, x2, x3]has no edges, then no node causes x3directly or indirectly, therefore
we initialize C3,4=∅. There are no possible mediators between x3andx4, soM3,4=∅.
Therefore, by Lemma 5.1 x3̸⊥ ⊥x4|Z3,4=x3̸⊥ ⊥x4⇐⇒ x3→x4. As [x1, x2, x3]
has no edges, no node causes x2directly or indirectly, therefore we initialize C2,4=∅.
Note, x2̸→x3, so there are no possible mediators between x2andx4, so we initialize
M2,4=∅. Then, by Lemma 5.1 x2̸⊥ ⊥x4|Z2,4=x2̸⊥ ⊥x4⇐⇒ x2→x4. Note, x1
has no parents, so we initialize C1,4=∅. As, x1does not cause x2orx3there are no
possible mediators between x1andx4, therefore we initialize M1,4=∅. Then, by Lemma
5.1x1̸⊥ ⊥x4|Z1,4=x1̸⊥ ⊥x4⇐⇒ x1→x4. We recover all possible edges between
[x1, x2, x3, x4].
Case 2 : ifx1→x2is the only edge between the nodes [x1, x2, x3], then no node causes
x3directly or indirectly, therefore C3,4=∅. By topological sort there is no mediator
26between x3, x4, so we initialize M3,4=∅. Then, by Lemma 5.1 x3̸⊥ ⊥x4|Z3,4=x3̸⊥
⊥x4⇐⇒ x3→x4. Asx1→x2, we initialize C2,4=x1. Asx2̸→x3, there are no
possible mediators between x2andx4, so we initialize M2,4=∅. Then, by Lemma 5.1
x2̸⊥ ⊥x4|Z2,4=x2̸⊥ ⊥x4|x1⇐⇒ x2→x4. If an edge exists between x2andx4, we
initialize M1,4accordingly. As x1has no parents, we initialize C1,4=∅. Then, by Lemma
5.1x1̸⊥ ⊥x4|Z1,4⇐⇒ x1→x4. We recover all possible edges between [x1, x2, x3, x4].
Case 3 : ifx1→x3is the only edge between the nodes [x1, x2, x3], then x1is the only
potential confounder of x3andx4so we initialize C3,4=x1. There are no possible
mediators between x3andx4, so we initialize M3,4=∅. Then, by Lemma 5.1 x3̸⊥
⊥x4|Z3,4=x3̸⊥ ⊥x4|x1⇐⇒ x3→x4. Note, x2has no parents, so we initialize
C2,4=∅. Note, x2̸→x3, so we initialize M2,4=∅. Then, by Lemma 5.1 x2̸⊥ ⊥
x4|Z2,4=x2̸⊥ ⊥x4⇐⇒ x2→x4. If an edge exists between x3andx4, we initialize
M1,4accordingly. As x1has no parents, we initialize C1,4=∅. Then, by Lemma 5.1
x1̸⊥ ⊥x4|Z1,4⇐⇒ x1→x4. We recover all possible edges between [x1, x2, x3, x4].
Case 4 : ifx1→x2, x2→x3are the only edges between nodes [x1, x2, x3], then x1and
x2cause x3either indirectly or directly. Therefore, we initialize C3,4={x1, x2}. There
are no possible mediators between x3andx4, so we initialize M3,4=∅. Then, by Lemma
5.1x3̸⊥ ⊥x4|Z3,4=x3̸⊥ ⊥x4|x1, x2⇐⇒ x3→x4. If an edge exists between x3and
x4, we initialize M2,4accordingly. Note, x1is a parent of x2, so we initialize C2,4=x1.
Then, by Lemma 5.1 x2̸⊥ ⊥x4|Z2,4⇐⇒ x2→x4. If an edge exists between x3andx4,
and/or between x2andx4, initialize M1,4accordingly. Note, x1has no parents, so initialize
C1,4=∅. Then by Lemma 5.1 x1̸⊥ ⊥x4|Z1,4⇐⇒ x1→x4. We recover all possible
edges between [x1, x2, x3, x4].
Case 5 : ifx1→x3, x2→x3are the only edges between nodes [x1, x2, x3], then x1and
x2cause x3directly. Therefore, we initialize C3,4={x1, x2}. There are no possible
mediators between x3andx4, so we initialize M3,4=∅. Then, by L3x3̸⊥ ⊥x4|Z3,4=
x3̸⊥ ⊥x4|x1, x2⇐⇒ x3→x4. If an edge exists between x3andx4, we initialize
M2,4accordingly. Note, x2has no parents, so we initialize C2,4=∅. Then,by L3x2̸⊥ ⊥
x4|Z2,4⇐⇒ x2→x4. If an edge exists between x3andx4, initialize M1,4accordingly.
Note, x1has no parents, so initialize C1,4=∅. Then by L3x1̸⊥ ⊥x4|Z1,4⇐⇒ x1→x4.
We recover all possible edges between [x1, x2, x3, x4].
Case 6 : ifx1→x2, x1→x3, x2→x3are the only edges between nodes [x1, x2, x3],
thenx1andx2cause x3directly. Therefore, we initialize C3,4={x1, x2}. There are no
possible mediators between x3andx4, so we initialize M3,4=∅. Then, by Lemma 5.1
x3̸⊥ ⊥x4|Z3,4=x3̸⊥ ⊥x4|x1, x2⇐⇒ x3→x4. If an edge exists between x3and
x4, we initialize M2,4accordingly. Note, x1is a parent of x2, so we initialize C2,4=x1.
Then, by Lemma 5.1 x2̸⊥ ⊥x4|Z2,4⇐⇒ x2→x4. If an edge exists between x3andx4,
and/or between x2andx4, initialize M1,4accordingly. Note, x1has no parents, so initialize
C1,4=∅. Then by Lemma 5.1 x1̸⊥ ⊥x4|Z1,4⇐⇒ x1→x4. We recover all possible
edges between [x1, x2, x3, x4].
5.Finding parents of x5: first we check whether x4→x5, then whether x3→x5, then
whether x2→x5, then whether x1→x5.
...
Iteration k−1Inductive Assumption We have recovered the edges between [x1, . . . , x k−1].
Iteration kWe now find all nodes in [x1, . . . , x k−1]that cause xk(which yields all edges between
[x1, . . . , x k]).
Base Case Sub-Iteration (1,2)
1.We first check whether xk−1→xk. We initialize the potential confounders Ck−1,k
using [x1, . . . , x k−1]. The set of mediators Mk−1,kis empty by the topological sort. Then,
by Lemma 5.1 xk−1→xk⇐⇒ xk−1̸⊥ ⊥xk|Zk−1,k.
272.We now check whether xk−2→xk. We initialize the potential confounders Ck−1,k
using [x1, . . . , x k−2]. Only xk−1can be a mediator: we know whether xk−2causes xk−1,
and in our previous step we found whether xk−1causes xk. Thus, we initialize Mk−2,k
accordingly. Thus, by Lemma 5.1 xk−2→xk⇐⇒ xk−2̸⊥ ⊥xk|Zk−2,k.
...
Sub-Iteration jInductive Assumption We have recovered edges between [xj, . . . , x k]
Sub-Iteration j−1We now find if xj−1causes xk.
1.For node xj−1where 1≤j−1< k, we obtain Cj−1,kby Iteration k−1Inductive
Assumption and Mj−1,kby Sub-Iteration jInductive Assumption. Then, by Lemma 5.1
xj−1→xk⇐⇒ xj−1̸⊥ ⊥xk|Zj−1,k.
Sub-Iteration jInductive Assumption is satisfied for j−1, therefore we recover all nodes in
[x1, . . . , x k−1]that cause xk. This satisfies Iteration k−1Inductive Assumption for k, which means
we recover all edges between [x1, . . . , x k]. Thus, for a topological sort of arbitrary length, the
algorithm recovers all possible edges.
C.4 Time Complexity for Edge Discovery
Theorem 5.3. Given nsamples of dvertices generated by a model corresponding to a DAG G, the
runtime complexity of ED is upper bounded by O(d2n3).
Proof. ED checks for the existence of every edge permitted by a topological sort πby running one
conditional independence test that has complexity O(n3). In the worst case, there are O(d2)possible
edges, so the overall complexity is O(d2n3).
28D Additional Experiments and Runtimes
D.1 Runtimes for Linear Topological Sort
Figure 10: Runtimes for linear topological sorts, left: top row; right: bottom row, see Figure 5.
D.2 Runtimes for Nonlinear Topological Sort
Figure 11: Runtimes for nonlinear topological sorts, see Figure 6.
D.3 Topological Sorts on Nonlinear Data
Figure 12: Performance of NHTS on data generated with Gaussian, Laplace, or Uniform noise (left,
middle, right columns), the average number of edges set to 2d,3d, or4d(top, middle, bottom rows).
In figure 12 we provide additional experiments for nonlinear topological sorts ( d= 10, n= 300 ); we
see that NHTS maintains superior performance even as the density of the underlying graph increases,
although the performance gap decreases, especially for laplacian noise, as the graph becomes denser.
29D.4 Edge Pruning on Nonlinear Data
Figure 13: Performance of ED on data generated with Gaussian, Laplace, or Uniform noise (left,
middle, right columns), the average number of edges set to 2d,3d, or4d(top, middle, bottom rows).
In figure 13 we provide additional experiments for edge pruning methods ( d= 20, n= 300 ); we see
that ED generally maintains superior performance as the noise distribution is varied and density is
increased, although the performance gap decreases for all noise distributions as the graph becomes
denser.
D.5 Runtimes for Edge Pruning
Figure 14: Runtimes for edge pruning: tables correspond to the graphs in Figure 7, from left to right.
30E Implementation Details
All tests were done in Python. All runtimes were computed locally on an Apple M2 Pro Chip, 16 Gb
of RAM, with no parallelization.
E.1 Topological Sort for LiNGAM
DirectLiNGAM was imported from the lingam [45] package, R2−sort was imported from the
CausalDisco [28,29] package, and LHTS was implemented using the Sklearn [24] package. All
assets used have a CC-BY 4.0 license. We follow [ 38] to generate the data for Figure 5, using linear
causal mechanisms with randomly drawn coefficient values, plus independent uniform noise. Data is
standardized to remove shortcuts [ 28]. Cutoff values for independence tests were set to α= 0.05for
all methods.
E.2 Topological Sort for Nonlinear ANM
GES and GRaSP were imported from the causal-learn [49] package; GSP was imported from the
graphical_model_learning [43] package. DirectLiNGAM was imported from the lingam
package [45]. NoGAM was imported from the dodiscover [14] package. R2−sort was imported
from the CausalDisco package. NHTS and LoSAM were implemented using the kernel ridge
regression (KRR) function from the Sklearn package, used independence tests from either the
causal-learn package or the dcor [26] package, and a mutual information estimator from the
npeet [44] package. All assets used have a CC-BY 4.0 license. We follow [ 16] to generate the data
used for Figure 6 and Figure 12, using quadratic causal mechanisms with randomly drawn coefficient
values, plus independent gaussian, laplace or uniform noise. Features generated with quadratic
mechanisms were standardized after being generated to remove shortcuts [28] and to prevent the
quadratic mechanisms from driving all values close to 0 (ensuring stability). Note that, to enable a
fair comparison between NHTS and other topological ordering methods, we implement a version of
NHTS that returns a linear topological sort, rather than a hierarchical topological sort, by adding only
one vertex to the sort in each iteration of its sorting procedure. For NHTS, in Stage 2 we used KRR
with polynomial kernel, α= 1, degree = 3, coef0 = 1, and in Stage 4 we used KRR with RBF kernel,
α= 0.1,γ= 0.01. Cutoff values for independence tests were set to α= 0.05for all methods, no
cross validation was allowed for any method. Otherwise, default settings were used for all baselines.
E.3 Edge Pruning
Lasso and RESIT were implemented using the sklearn package, hypothesis testing with GAMs was
implemented using Bsplines and GLMGam from the statsmodel [33] package. Independence tests
used either the causal-learn package or the dcor package. All assets used have a CC-BY 4.0
license. We follow [16] to generate the data used for Figure 7 and Figure 13, using quadratic causal
mechanisms with randomly drawn coefficient values, plus independent uniform, gaussian, or laplace
noise. Features generated with quadratic mechanisms were standardized after being generated to
remove shortcuts [28] and to prevent the quadratic mechanisms from driving all values close to 0
(ensuring stability). Cutoff values for independence tests were set to α= 0.05for all methods.
31NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We address all claims made in the abstract and contributions section throughout
the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We clearly outline the assumptions and identifiability conditions needed for our
methods to hold, only claiming aysymptotic correctness when appropriate. See Definition
2.2, Appendix A.1 and Appendix B.1.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Each theoretical result includes the necessary assumptions, and links to a
correct proof in the Appendix.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Experimental procedures are described in both Section 6 and Appendix E, and
code is released on github.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Code for the algorithms and data generation is provided in the supplemental
material and github link.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See Section 6, Appendix E, and the supplemental material or github link.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Error bars for experiments for linear topological sorts and edge pruning
methods in the main text are standard deviations. Error bars for nonlinear topological sorts
and edge pruning experiments not in the main text are whiskers on a boxplot (1.5 times the
IQR).
8.Experiments Compute Resources
32Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: See Appendix E.
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: All parts of the Code of Ethics were followed.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [No]
Justification: This paper addresses a foundational problem in the field of causal discovery,
an issue not fundamentally tied to any specific set of applications.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Not applicable.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: See Appendix E.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new assets introduced in the paper.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Not applicable.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Not applicable.
33