Decoupling Semantic Similarity from Spatial
Alignment for Neural Networks
Tassilo Wald∗,1,2,3, Constantin Ulrich1,4,7, Gregor Köhler1,3,
David Zimmerer1,2, Stefan Denner1,3, Michael Baumgartner1,3,
Fabian Isensee1,2, Priyank Jaini†,5, Klaus H. Maier-Hein†,1,2,3,4,6
1Division of Medical Image Computing,
German Cancer Research Center (DKFZ), Heidelberg, Germany
2Helmholtz Imaging, DKFZ, Heidelberg, Germany
3Faculty of Mathematics and Computer Science,
University of Heidelberg, Germany
4Medical Faculty Heidelberg, University of Heidelberg, Germany
5Google Deepmind
6Pattern Analysis and Learning Group, Department of Radiation Oncology
7National Center for Tumor Diseases (NCT) Heidelberg, Germany
Abstract
Whatrepresentationdodeepneuralnetworkslearn? Howsimilarareimages
to each other for neural networks? Despite the overwhelming success of
deep learning methods key questions about their internal workings still
remainlargely unanswered, dueto theirinternal highdimensionalityand
complexity. Toaddressthis,oneapproachistomeasurethesimilarityof
activationresponsestovariousinputs. RepresentationalSimilarityMatrices
(RSMs) distill this similarity into scalar values for each input pair. These
matricesencapsulatetheentiresimilaritystructureofasystem,indicating
which input leads to similar responses. While the similarity between
images is ambiguous, we argue that the spatial location of semantic objects
does neither influence human perception nor deep learning classifiers.
Thus thisshould be reflectedin the definitionof similarity betweenimage
responsesforcomputervisionsystems. Revisitingtheestablishedsimilarity
calculationsforRSMsweexposetheirsensitivitytospatialalignment. In
this paper, we propose to solve this through semantic RSMs , which are
invariant to spatial permutation. We measure semantic similarity between
inputresponsesbyformulatingitasaset-matchingproblem. Further,we
quantifythesuperiorityof semanticRSMsover spatio-semantic RSMsthrough
image retrieval and by comparing the similarity between representations to
the similarity between predicted class probabilities.
1 Introduction
Deep neural networks are trained to extract powerful feature representations for a wide
rangeof downstreamtasks. Despitethis, their innerworkingsare highly-complex,making
understanding hownetworks solve tasks and whatthey learn challenging. To obtain a better
understanding of these fundamental questions, researchers in the fields of neuroscience,
∗Corresponding author: tassilo.wald@dkfz-heidelberg.de
†Shared last authorship.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).cognitive science, and machine learning independently developed various methods to
interpret and relate representations [29].
Intherealmofmachinelearning,alotofprior-methodshavebeenproposedtomeaningfully
measurethe similaritybetweenintermediate representationsof ANNs[ 15,31,25,19,12].
Klabunde et al. [10]provides a comprehensive summary and categorizes them into
measuresbasedona)CanonicalCorrelationAnalysis(CCA)[ 25,19]b)Alignmentmeasures
c) Representational Similarity Matrices (RSMs) d) Nearest Neighbors e) Topologies and
f) Descriptive Statistics.
Of all these methods, Representational Similarity Matrix (RSM) [ 13] based measures
have enjoyed the most attention over the last years. RSMs consist of sample-to-sample
comparison, measuring the similarity between the (intermediate) responses of the same
networktotwodifferentsamples. Basedonmanysuchcomparisons,theRSMrepresents
the similarity structure of what a model considers similar. This representation of a system’s
behavior reduces the highly-dimensional, complex internal structure, that the model of
interest may possess, to a N×NMatrix for Nsamples. It enables the comparison of the
similarity structure of any system, as long as one can input the same samples and measure
the similarity between the responses.
In the machine learning domain, this concept was introduced by Kornblith et al. [12]in
conjunction with Centered Kernel Alignment (CKA) to compare the similarity between
RSMs of different layers within a model or across models. CKA superseded previously
popular Canonical Correlation Analysis (CCA) metrics [ 25,19], since they need a vast
amount of samples to measure similarity. Consequently, CKA was used in various
applications,tomeasurethesimilaritybetweenTransformersandCNNs[ 26]orwideand
deepnetworks [ 21]andto understandcatastrophicforgetting[ 27]ortransfer learning[ 20]
to name a few.
In this paper, we revisit the key component of the most used similarity measure in the field:
Representational Similarity Matrices and how they are constructed in the vision domain.
The key contributions of our work are summarized as follows:
•WehighlightthatcurrentRSMsareconstructedinawaythatcoupleslocalization
and semantic information, which constraints one only to measure similarity if,
spatial and semantic information aligns between two samples.
•To address this issue we propose semantic RSMs , which are invariant to spatial
permutation and exclusively measure semantic similarity, by formulating it as a
set-matching problem.
•We show that the inter-sample similarity of semantic RSMs leads to improved
retrievalperformanceandbetterreflectsthesimilaritybetweenrepresentationsof
classifiers and their predictive behavior.
•Moreover, due to the computational complexity of the proposed algorithm, we
introduce approximations that significantly reduce computation time.
The Code is available here.
2 Representational Similarity
Formalization To establish the concept of representational similarity in the context of
computer vision, we provide a brief formalization of the problem. Let x∈ {x0, . . . , x N}
denote the input samples for which we collect input responses Z1∈ {z1,0, . . . , z 1,N}and
Z2, which we call representations. The representations can take different shapes, with
ZCNN∈RN×C×W×HdenotingtheresponsesofaCNNwith Cchannelsandaspatialextent
ofW, HorZViT∈RN×D×Tdenoting the responses of a ViT with depth Dand tokens
T. For the purpose of simplification and without loss of generality, we unify the spatial
dimensions W, Hfor CNNs and Tfor ViTs into a joint spatial dimension S, resulting in
Z∈RN×C×S. Given representations Z, we can construct RSMs K, L∈RN×N, with values
Kij=k(z1,i, z1,j)andLij=l(z2,i, z2,j)measuring how similar the representation ziis
tozjgiven the kernels korl. The kernels define the measure of similarity between the
2X =1
0
0
1K =1
1
000
00
00
K' =1
1
110
01
00Features Samples
RepresentationsRSMs
10
100000
00 0000
00
0000
00 10x0,0x0,1x1,0x1,1x0,0 x0,1
x1,0 x1,1x0,0 x0,1
x1,0 x1,1x0,0 x0,1
x1,0 x1,1
Spatial Sensitive
Spatial Equivariant
Sample-wise
AlignmentNo AlignmentFigure 1: Currentspatio-semantic RSMs couple semantic similaritywith spatial alignment.
Our proposal focuses solely on measuring semantic similarity. We achieve this by deter-
mining the optimal permutation between two representations and introducing sample-wise
permutation invariance.
representation vectors and hence play an important role. We introduce an exemplary kernel
in Section 3 and all kernels used in this paper with some properties in Appendix A.
With the two RSMs KandLat hand, it is possible to compare the similarity structure
between the two models. As introduced in Kornblith et al. [12]one can use the Hilbert-
Schmidtindependencecriterion(HSIC)[ 5,28]tocalculatethelevelofindependencethrough
CenteredKernelAlignment,providingameasureofsimilarityofthetworepresentations Z1
andZ2withHdenoting the centering matrix.
HSIC (K, L) =1
(n−1)2tr(KHLH) (1)
CKA (K, L) =HSIC (K, L)p
HSIC (K, K )HSIC (L, L)(2)
Alternatively, a variety of different measures based on RSMs are possible for which we
refertoSection3.3ofKlabundeetal. [10]. Ashighlightedabove,thesimilaritycalculation
basedonRSMsisatwo-stepprocesswiththefirstbeingthecalculationoftheRSMsand
the latter being the comparison of the RSMs. In this paper, we focus on the first step, by
quantifying the importance of disentangling semantic similarity from spatial alignment.
While not the focus of this paper, we provide qualitative examples of the downstream effect
on CKA measures in Appendix G.
3 The Semantic Representational Similarity Matrix
Representational Similarity Matrices (RSMs) are designed to reflect the system behavior
of interest. The RSM K, originally introduced by Kriegeskorte et al. [13], represents the
similaritystructureofasystemgivenasetofinputs x∈ {x0, . . . x N}. Eachvalue KijinK
quantifieshowsimilartheresponsesoftwoinputs ziandzjaretoeachother. Thedefinition
of what symmetries between representations similarity measures should be invariant to is a
central point of debate. Previous work proposed permutation invariance [ 15], invariance to
orthogonal transformations [ 12], or invariances to invertible linear transformations [ 25,19].
While arguments for any of these invariances are valid, we believe that an important
aspect has been neglected in the calculation of RSMs: The spatial alignment between the
representations!
The dependency on spatial alignment Revisiting the structure of representations of a
CNN,channels Ccorrespondtosemanticconceptswhilethespatialpositioncorresponds
towherethesemanticconceptislocalizedintheinputimage[ 34]. Consequently,onecan
reformulate the representation ziof a sample xito be fully defined by a set of semantic
concept vectors v, one for each spatial location S:zi={v0, . . . ,vS}withv∈RC. In the
case of linear CKA [ 12], the RSMs are then calculated, between semantic concept vectors
at the same spatial location. For instance, when employing the linear kernel Kijcan be
expressed as:
Kij=X
s⟨vz1,s,vz2,s⟩ (3)
3Thisformulationemphasizesthecouplingofsemanticsimilarityandlocalizationduring
similarity calculation, which we term as spatio-semantic RSMs . This coupling can lead to
issues, e.g. when comparing an image to a translated version of itself. Due to the quasi
translation-equivariant nature of CNNs 3semantic vectors are translated similarly, changing
the alignment of pairs of v, leading to a low perceived similarity despite highly similar
semantic vectors. This issue is visualized in a small toy example in Fig. 1
3.1 Decoupling Localization and Semantic Content
As shown above, current RSMs compare different input samples without accounting for
the lack of spatial alignment. Previous work of Williams et al. [32]recognized this and
introduced translation invariance to RSMs by finding the optimal translation a, bof the
representations z′
j={v0+a,0+b, . . . ,vw+a,h+b}tomaximizesimilarity Kijmax a,b=⟨zi, z′
j⟩
through circular shifts.
While this is an improvement to no spatial alignment and emulates a CNN’s inherent
translationequivariance,wearguethatthemeasureofrepresentationalsimilarityshould
not be constrained to what the underlying model is invariant to, but the similarity measure
shouldbeinvarianttothepossiblespatialconfigurationsofsemanticfeaturesintheinput
image.
To motivate this, we propose a thought experiment:
Imagine we have trained a classifier with an augmentation pipeline including rotations.
Given an image and a rotated version of the image, we extract representations zat layer
ionce for the normal ziand once for the rotated image zi,rot. Due to the initial rotation,
theserepresentationsmaydifferinearlierlayers i,duetothenetworkextractingdifferent
edges and corners. However, if the network successfully learned to become invariant to
the augmentation, it may have learned to map it to the same semantic vector vat a later
layer but at a different spatial location. For such cases, we argue that the similarity between
thetworepresentationsshouldbehigh. Shouldthemodelbesensitivetotherotation,no
semantically similar representations may be expressed at a later layer, which should lead to
a low similarity.
Thisreasoningcanbeextendedtoallkindsofshifts,betheyartificialaugmentationslike
shearing or mirroring or natural variations of the input manifold. Subsequently, we argue
thatthesimilaritymeasureshouldbeinvarianttoasmanyspatialshiftsaspossible. This
alone allows one to measure the similarity of representations a model is invariant to, be
theselearnedordesignedinvariances. Suchvariableshiftscannotbecapturedwithsimple
translation operations.
3.1.1 Introducing permutation invariance
To impose as minimal constraints on spatial structure as possible, we propose to make Kij
invariant to all spatial permutations of the semantic concept vectors v. Formalizing this we
demand that the similarity Kij=k(zi, zj) =k(zi,Pijzj)withPij∈RS×Sbeing a unique
permutation matrix for the pair of ziandzj. To accomplish this, we propose to find the
optimal permutation matrix Pijthat maximizes the similarity Kij.
Pij=argmaxPk(zi,Pijzj) (4)
To find the optimal permutation matrix Pij, we decide to use the linear kernel ⟨·,·⟩to
maximize both the magnitude of activation and the direction of vectors, as both magnitude
ofactivationanddirectionofthevectorsmatter[ 12]. Thisallowsustocalculateanaffinity
matrixAij∈RS×Smeasuring the similarity between all concept vectors:
Aij= [vi,0, . . .vi,S]⊺[vj,0, . . .vj,S]. (5)
Withthisaffinitymatrix,bipartiteset-matchingalgorithms,suchasHungarianmatching,
can be employed to find the optimal permutation matrix Pijthat maximizes the inner
productbetween ziandz′
j. Findingall Pijforallpairs i, jandapplyingthechosenkernel
kyieldsthe semanticRSM .ThissemanticRSM isinvarianttoanyarbitrary,uniquespatial
3The patch embedding can similarly translate semantics to a different position in the sequence.
4LayerLayerRBF Similarity
0 1 2 3 4 5 6 7 80.00.20.40.60.81.0
RBF Similarity
ResNet18Translated TinyImageNetSpatio-Semantic RBF RSMs
Semantic RBF RSMs (ours)Layer Id
Conv-Bn-ReLU
Basic Block
Basic BlockDS
Basic Block
Basic BlockDS
Basic Block
Basic BlockDS
Basic Block
Basic BlockDS
0 1 2 3 4 5 6 7 8SampleShared Content
up-shifted
down-shifted
RSM
down-shifteddown-shiftedup-shiftedup-shifted
Comparison of different images
Comparison of image with itself
Comparison of translated images
0.0 0.2 0.4 0.6 0.8 1.00 1 2 3 4 5 6 7 8
0.2
0.4
0.6
0.8
1.0
Aggregated for each category
0.0 0.2 0.4 0.6 0.8 1.0
Comparison of different images
Comparison of image with itself
Comparison of translated imagesFigure 2: Semantic RSMs capture similarity independent of spatial localization, in
contrastto currentspatio-semanticRSMs. Weutilize Tiny-ImageNettogenerate partially
overlapping crops of the same sample (left) and calculate RSMs for a trained ResNet18
model. The plot displays the original spatio-semantic RSMs (middle top) and our proposed
semanticRSMs(middlebottom)acrossvariouslayersforasinglebatch. Additionally,the
distribution of similarity values over multiple batches is shown (right). The results indicate
that spatio-semantic RSMs struggle to detect largely identical but translated images, while
semantic RSMs exhibit an enhanced off-diagonal in the RSMs and a significant gap between
distributions. This demonstrates the capability of our method to detect the same semantics
even when translated.
permutation Pijfor each pair of representations, and, depending on the choice of kernel
k, invariant to orthogonal transformations U∈RC×Calong the channel dimension. These
semanticRSMs canbeusedasadrop-inreplacementforanyotherRSM,e.g. forapplications
such as calculating CKA (K, L)to measure the similarity between systems.
Computational Complexity Finding the optimal permutation matrix Pijis NP-hard
and needs to be repeated for each pair of representations zi, zj. With Nsamples, this
resultsinN·(N+1)
2uniquepermutationsthatneedtobecomputedfora semanticRSM .The
overall complexity of bipartite matching algorithms grows with the spatial dimensions
cubed, resulting in O(N2)× O(S3). The outer O(N2)complexity can be parallelized, or
reduced by decreasing the batch size. However, the inner permutation can become time-
consuming,especiallywithlargespatialdimensionality. Toaddressthis,weprovidevarious
approximations to reduce the complexity, which are detailed in Section 4.4. For all later
experiments, except the translational toy example, we use the Batch-Optimal approximation
with windows size b512, with the batch referring to batches of semantic concept vectors
vand not samples. The pseudo-code for calculating semantic RSMs is visualized in the
Appendix under Algorithm 1.
4 Experiments: Semantic vs Spatio-Semantic RSMs
Giventhenovelpermutation-invariantsimilaritydefinition,weevaluatetheutilityofour
semanticRSMsrelativetospatio-semanticRSMsforvarioussimilaritykernels,architectures,
andtasks. Acrossallexperimentswecomparethelinearkernel,theradialbasisfunction
(RBF) kernel, and the cosine similarity kernel, see Appendix A for details.
5Dinov2-Giant CLIP (ViT-B32) BIT-50 CLIPSeg (Rd64) SAM (ViT-B)
Architecture0.00.20.40.60.8Retrieval F1@1Similarity Metric
Cosine Kernel
(ours) PI Cosine Kernel
Linear Kernel
(ours) PI Linear Kernel
RBF Kernel
(ours) PI RBF KernelFigure3: Relaxingtheconstraintofspatialalignmentleadstobetterretrieval. Weleverage
general feature extractors to embed images of the EgoObjects dataset. We then compare
theseembeddingseitherwithorwithoutpermutationinvariance. PI:PermutationInvariant
4.1 Translation sensitivity
To illustrate the problems of coupling semantic content and localization a toy dataset is
createdusing 84×84pixelslarge,downsampledimagesofImageNet[ 2]. Foreachimage
two64×64crops are extracted, one from the upper-left and one from the lower-left corner,
resultingintwoimagesthatshare 44×64identicalpixels(Fig.2left). Tenupper-leftand
ten lower-left crops are then used to extract representations of a ResNet18 [ 7], which are
subsequently used to calculate spatio-semantic and semantic RSMs at different layers of
thearchitecture(Fig.2middle). Askernel,weusetheradialbasisfunction,asitprovides
bounded similarity values allowing a better visualization.
As expected, the spatio-semantic RSM measures low similarity between pairs of overlapping
crops,duetothesemanticconceptvectorsnotaligning. Onlyinthelastlayer,aftermany
pooling operations, the off-diagonal is slightly expressed. Conversely, our semantic RSM
is capable of detecting the high semantic similarity of the partially overlapping crops
throughouttheentiredepthofthearchitecture,asevidentbythehighlysimilaroff-diagonal.
Aggregating the similarity values between partially-overlapping and between different
imagesacrossmultiplebatches,allowsustomeasurethedistributionofsimilarityvalues
between overlapping crops, and non-related image comparisons. Throughout the entire
depthofthearchitecture,thesimilaritydistributionsshowthatourmeasurebetterseparates
overlapping images from different images. Notably, the similarity distribution in spatio-
semantic RSMs shows a significant overlap of the distributions of partially overlapping
images and non-related images, making differentiation between them difficult (Fig. 2 right).
A similar toy experiment for a ViT-B/16 [4], is provided in Appendix B.
4.2 Similarity-based retrieval
To test the impact of the semantic RSMs in real-world applications, we now investigate
thecommontaskofimageretrieval. EachentryinanRSMquantifiesasample-to-sample
similarity value, which can be directly used for retrieval. While not specifically designed for
it,wearguethatbetterretrievalperformancereflectsabetterinter-samplesimilarity. This
allowsustoquantifyimprovementsintheRSMstructure. Tomeasureretrievalperformance
the EgoObjects dataset [ 35] is used. It contains frames of video that capture the same scene
fromdifferentviewingperspectivesandlightingconditions. Thisresultsinobjectcenters
being distributed across the extent of the image.
By randomly sampling 2000 query images and 5000 database images from the test set
andusinggeneralfeatureextractorstoextractembeddingsfromthemweconstructRSMs
that allow us to do retrieval. As feature extractors we use CLIP (ViT/B32) [ 24], CLIPSeg
(Rd64) [16], DinoV2-Giant [ 22], SAM (ViT/B32) [ 9] and BIT-50 [ 11] and as kernels for
similarity calculation we use the cosine similarity, RBF and the inner product.
ForallRSMs,weretrievethemostsimilarimagethatisnotpartofthesamevideo–thesame
scene but different conditions are allowed. As multiple objects can be present in each scene,
6Figure4: Retrievingbypermutationinvariantsimilarityreturnssimilarscenesofdifferent
spatialgeometry. Wevisualizethetop3mostsimilarimagesaccordingtotwoexemplary
query images for SAM ViT/B32.
we quantify retrieval performance by the F1-Score, measuring the overlap of annotated
objects between images. Due to the rather complex dataset, we elaborate this in more detail
in Appendix D.1.
Across all architectures and metrics, the inclusion of permutation invariance (PI) for the
similarity calculation improves retrieval performance relative to the non-invariant similarity,
insomecaseswithadramaticdifferenceinperformance,seeFig.3. Formodelsdesignedfor
densedownstreamtaskslikeSAMorCLIPSeg,theretrievalperformancechangesparticularly
much, while models with more global reasoning, like BiT improve less, relatively.
QualitativeSimilarity Asidefromaquantitativecomparison,wevisualizethemostsimilar
retrieved images for two exemplary queries of SAM in Fig. 4 as case examples.
Left Query : The image displays various utensils scattered on a desk. When retrieving with
thepermutation-invariantsimilaritymetrictwoimagesofthesamescenebutaverydifferent
perspective are successfully retrieved as most similar. Retrieving with the non-invariant
similarity metric fails to retrieve similar images, due to lack of spatial alignment of the
semanticconcepts. Instead,itretrievesimagesofawhiteboard,possiblyduetoitsspatial
alignment with the paper on the desk.
Right Query : The image features a blender on a counter. The retrieval based on non-
permutation-invariant similarity fails to retrieve any of the semantically similar scenes and
returnsimageswithalightswitch,likelyduetothespatialalignmentofthelight-switch-
looking object to the right of the blender. Contrary, the retrieval based on permutation-
invariant similarity correctly returns the blender in all cases from different perspectives.
Additional qualitative examples are provided in Appendix D.3.
These experiments display clearly, that demanding spatial alignment can be a significant
shortcoming when semantically similar concepts are misaligned. In Fig. 4, the network
learned to represent the objects very similarly, despite a shift in perspective, but due to
the same objects not aligning anymore, spatio-semantic similarity fails to recognize this.
Thiseffectshouldgeneralizetootherdatasetswhereobjectsarenotheavilycentered. For
datasets with heavy object-centric behavior, like ImageNet, this should be less pronounced.
7Table 1: Similarity invariant to spatial permutations is better at predicting if the class
probabilities will be similar. PI: Permutation Invariant
Pearson Correlation ρ
Architectures Cosine Sim. Inner Product RBF
-(ours)PI-(ours)PI-(ours)PI
ResNet18 -0.276 -0.326 -0.259 -0.270 -0.176 -0.199
ResNet50 -0.248 -0.291 -0.243 -0.261 0.040 0.029
ResNet101 -0.192 -0.276 -0.174 -0.240 0.091 0.084
ConvNextV2-Base -0.134 -0.098 -0.132 -0.171 0.117 0.090
ViT-B/16 -0.046 -0.100 -0.045 -0.026 -0.077 -0.122
ViT-L/32 -0.138 -0.188 -0.138 -0.144 -0.134 -0.166
DinoV2-Giant -0.012 -0.044 -0.013 -0.031 -0.008 -0.048
4.3 Output similarity vs Representational Similarity
While the retrieval experiments relate to a rather human notion of similarity, one can raise
the question if semantic RSMs are also better at measuring the similarity for classifiers.
For each pair of samples, we can compare how similar the predicted class probabilities of a
model are and compare this to the representational similarity. A commonly used metric
forthisistheJensen-ShannonDivergence(JSD),whichquantifieshowdissimilarthetwo
probability distributions are from one another. More details are provided in Appendix E.
Consequently,weusevariousclassifierstrainedtopredictImageNet1kfromHuggingface
andcomparethePearsoncorrelation ρbetweentheirJSDandtherepresentationalsimilarity
oftheirlasthiddenlayer. WechosetousethePearsoncorrelation,asitallowsobservinga
direct linear behavior between representational similarity and predictive similarity. Again
we measure semantic similarity and spatio-semantic similarity with different kernels. Due
to JSD measuring dissimilarity, we want the correlation to be as negative as possible. As
models we use multiple ResNets [ 7], ViTs [4] , a fine-tuned DinoV2 [ 22] classifier from and a
convnextv2[33] classifier.
Theresults,displayedinTable1,showthatforalmostallarchitecturesandkernelstested,the
permutationinvariantsimilaritiesarebetteratcapturingthenotionofwhataclassifierdeems
similar. Whilebetterthanthespatio-semanticsimilarity,overallcorrelationsaregenerally
low, indicating that either, the similarity metric is confounded by irrelevant representations,
or that the kernels should be improved. Moreover, the RBF kernel sometimes provides a
positivecorrelationindicatingitisunsuitabletopredictthesimilarityofoutputprobabilities,
whereastheCosineSimilarityandtheInnerProductbothareconsistentlynegativeforall
architectures tested.
4.4 Optimizing runtime
Sincewefindthebestpossiblepermutationmatrixthroughlinearsumassignmentalgorithms
thatmaximizetheinnerproductoftwosamples,wecanguaranteethatthe Kij,semantic ≥
Kij∀i, j. This provides us with an upper bound of similarity that can be leveraged to
measurehowmuchofthemaximallyachievablesemanticsimilaritywasmeasuredbythe
spatio-semantic similarity. Additionally, it can be used as a baseline to estimate the quality
of permutation matrices Pijprovided by faster, approximative assignment algorithms.
Decreasing computational complexity Determining the optimal permutation between
samplesposesasubstantialcomputationalchallengewithacomplexityof O(S3)foreach
of the O(N2)pairs in the same mini-batch, particularly for early layers with large spatial
resolution S. Although, in theory, the calculation of the Kmatrix needs to be conducted
only once for the desired representations, applying the method to representations with
larger spatial extents becomes impractical with the demands of optimal matching.
Tomitigateruntime,twooptionsareavailable: reducingthebatchsize Ntolessenthenumber
of permutation calculations or decreasing the time spent on finding the permutation. Given
thatscenarioslikeimageretrievaloftendesirelargerbatches,ourfocusisonminimizingthe
time required to obtain suitable assignments.
80
1
2
3
4
5
6
7
8
Layer
0.0
0.1
0.2
0.3
0.4
0.5Approximation Quality 1 - (K / Koptimal )
Batch-Optimal
TopK-Greedy
No Alignment
Optimal
ResNet18: Approximate matching algorithm quality
1024
128256
1024512512
256128
Figure5: Approximativealgorithmsyield
comparable matching quality to optimal
algorithms. The ratio of similarity from
various approximations relative to maximal
semantic similarity is visualized across mul-
tiple layers of a ResNet18.
16 64 256 1024 4096
Edge Length0.40.50.60.70.80.91.0Relative SimilarityRelative Similarity of Samples
DiagonalFigure6: Relativesimilarityisnotisotropic.
Whenaligningsemanticconceptsweobserve
thatsimilaritychangesheterogeneously,in-
dicating that some pairs of samples have
morespatiallymisalignedsemanticconcepts
than others.
Solvingtheoptimalbipartitematchingbetweensemanticconceptvectorsisequivalentto
the well-known assignment problem [ 18,1]. We attempted to find existing approximate
algorithms for this purpose. Unfortunately, most established algorithms primarily focus
onoptimalsolutions,andexistingapproximatealgorithmimplementations,suchasthose
based on the auction algorithm [ 6], are not runtime-optimized, often taking longer than
optimalalgorithmsinourexperiments. Toenhancecomputationalefficiencynonetheless,
we explored three tailored approximation algorithms:
A) A Greedy breadth-first matching ( Greedy)
B)An optimal matching of the TopK values based on their Norm, followed by the
Greedy algorithm for the remaining samples ( TopK-Greedy )
C)Optimal matching of smaller batches, with samples batched by their Norm ( Batch-
Optimal )
For explicit details on the approximation algorithms, we refer to Appendix F.
Weconductedacomprehensivecomparisonbetweentheapproximatealgorithmsandthe
optimal algorithm. We compare their runtime per sample and the quality of matches,
quantifiedbytheaveragerelativesimilarityk
koptimal. Theevaluationutilizedrepresentations
from a ResNet18 on TinyImageNet, as illustrated in Fig. 5.
It can be seen that the measured spatio-semantic similarity for TinyImageNet samples
are, on average, 30% lower with layers of higher spatial resolution exceeding 40%. This
suggestsanotablemisalignmentofsemanticconceptvectors. Notably,theBatch-Optimal
approximationstandsoutasareliableapproximationforoptimalmatching. Thefastestof
theBatch-Optimalapproximationmethodsshows <8%errorwhileimprovingrun-time ×36
relativetothefastestoptimalalgorithmforspatialextent 4096,whilenospatialalignment
shows 42%deviation from the optimal matching. Moreover, we highlight the time vs
accuracy trade-off of the different optimal and approximate algorithms in Appendix F.1.
Furthermore, itcanbe seenthatthe changesbetween spatio-semanticandsemanticRSMs
are anisotropic, as highlighted in Fig. 6, indicating scale invariant downstream applications
may be influenced.
5 Discussion, Limitations, and Conclusion
The concept of Representational Similarity Matrices (RSMs) is a powerful tool to represent
thesimilaritystructureofcomplexsystems. Inthispaperwerevisittheconstructionofsuch
RSMs for neural networks of the vision domain, question the current state, and propose
semantic RSMs , warranting discussion.
9Spatio-semantic coupling Being aware that current, spatio-semantic RSMs demand se-
manticconceptstobealignedishighlyrelevanttounderstandwhatRSMsaresensitiveto.
Previouswork[ 32]identifiedthisshortcomingandproposedtranslationinvariance,partially
addressingthisissue. Wearguetranslationinvarianceisinsufficient,sincemodelsmaylearn
invariancesduringtraining,whichthetranslationinvariantmetricwouldnotbesensitive
to. Subsequently, we propose a new – spatially permutation invariant – similarity measure
between samples that allows the detection of similarity whenever a model expresses similar
semantic vectors in its representations, irrespective of spatial geometry. To highlight the
benefits of our similarity, we propose that better similarity measures should allow more
accurateretrievalwhencomparinglast-layerrepresentationsandshouldallowbetterpredic-
tions about the similarity of class probabilities of a classifier. However, we acknowledge
certain limitations in our current evaluation. Specifically, we have not yet compared our
methodtomoreestablishedretrievaltechniques. Traditionalretrievalmethodsareoftennot
appliedtorepresentationsdirectlybututilizealower-dimensionalnon-spatial,globalvector
representing the entire sample. In contrast, we chose to limit ourselves to methods that are
directly applied to the representations.
Computational Complexity Aside from quantitative or qualitative benefits, the construc-
tion of semantic RSMs is time-consuming, limiting its applicability. This complexity mostly
affectslayersoflargespatialextent,whichmostlycorrespondstoearlyCNNlayerswhilelater
layersandViTsareunproblematic. OurproposedBatch-optimalapproximationalleviates
thispartially, yetapplication tolarge-scalerepresentations athigherresolution, likeatthe
outputofasegmentationarchitecturewithspatialextentsofs=65.536wouldbetoocostly.
We leave optimizing the compute efficiency or finding better approximations for future
work.
Conclusion In conclusion, our investigation into semantic RSMs has shed light on the
limitations of spatio-semantic RSMs and introduced a novel approach to disentangle spatial
alignmentfromsemanticsimilarity. Theproposedmethodprovidesamoreaccuratemeasure
of how representations capture underlying semantic content, showcasing its potential in
various applications, particularly in scenarios where spatial alignment cannot be assumed.
Whilechallengessuchascomputationalcomplexityandscalabilityneedtobeaddressed,
thefindingsopenavenuesforfurtherresearchandimprovementintheanalysisofneural
network representations.
References
[1]R. E. Burkard. Quadratic assignment problems. European Journal of Operational Research ,
15(3):283–289, 1984.
[2]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009.
[3]F.Ding,J.-S.Denain,andJ.Steinhardt. Groundingrepresentationsimilaritythrough
statistical testing. Advances in Neural Information Processing Systems , 34:1556–1568, 2021.
[4]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
[5]A.Gretton,R.Herbrich,A.Smola,O.Bousquet,B.Schölkopf,andA.Hyvärinen. Kernel
methods for measuring independence. Journal of Machine Learning Research , 6(12), 2005.
[6]S.GutheandD. Thuerck. Algorithm: Afastscalablesolverfor thedenselinear(sum)
assignment problem. ACM Trans. Math. Softw. , 47(2), apr 2021. ISSN 0098-3500. doi:
10.1145/3442348. URL https://doi.org/10.1145/3442348 .
[7]K.He,X.Zhang,S.Ren,andJ.Sun. Deepresiduallearningforimagerecognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,
June 2016.
[8]R. Jonker and T. Volgenant. A shortest augmenting path algorithm for dense and
sparse linear assignment problems. In DGOR/NSOR: Papersof the 16th Annual Meeting
10ofDGORinCooperationwithNSOR/Vorträgeder16.JahrestagungderDGORzusammenmit
der NSOR , pages 622–622. Springer, 1988.
[9]A.Kirillov, E.Mintun, N.Ravi, H.Mao, C.Rolland, L.Gustafson, T.Xiao, S.Whitehead,
A.C.Berg,W.-Y.Lo,etal. Segmentanything. In ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision , pages 4015–4026, 2023.
[10]M.Klabunde,T.Schumacher,M.Strohmaier,andF.Lemmerich. Similarityofneural
network models: A survey of functional and representational measures. arXiv preprint
arXiv:2305.06329 , 2023.
[11]A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big
transfer(bit): Generalvisualrepresentationlearning. In ComputerVision–ECCV2020:
16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16 , pages
491–507. Springer, 2020.
[12]S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network
representations revisited. In 36th International Conference on Machine Learning, ICML
2019, volume 2019-June, pages 6156–6175, 2019. ISBN 9781510886988. URL https:
//arxiv.org/abs/1905.00414 .
[13]N. Kriegeskorte, M. Mur, and P. A. Bandettini. Representational similarity analysis-
connecting the branches of systems neuroscience. Frontiers in systems neuroscience ,
page 4, 2008.
[14]A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report,
2009.
[15]Y.Li,J.Yosinski,J.Clune,H.Lipson,andJ.Hopcroft. Convergentlearning: Dodifferent
neural networks learn the same representations? Technical report, 2015.
[16]T. Lüddecke and A. Ecker. Image segmentation using text and image prompts. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR) ,
pages 7086–7096, June 2022.
[17]S. Marcel and Y. Rodriguez. Torchvision the machine-vision package of torch. In
Proceedingsofthe18thACMinternationalconferenceonMultimedia ,pages1485–1488,2010.
[18]S. Martello and P. Toth. Linear assignment problems. In North-Holland Mathematics
Studies, volume 132, pages 259–282. Elsevier, 1987.
[19]A. S. Morcos, M. Raghu, and S. Bengio. Insights on representational similarity in
neural networks with canonical correlation. Technical report, 2018. URL https:
//arxiv.org/abs/1806.05759 .
[20]B. Neyshabur, H. Sedghi, and C. Zhang. What is being transferred in transfer learning?
Advances in neural information processing systems , 33:512–523, 2020.
[21]T.Nguyen,M.Raghu,andS.Kornblith. Dowideanddeepnetworkslearnthesame
things? uncoveringhowneuralnetworkrepresentationsvarywithwidthanddepth.
arXiv preprint arXiv:2010.15327 , 2020.
[22]M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,P.Fernandez,
D.Haziza,F.Massa,A.El-Nouby,etal. Dinov2: Learningrobustvisualfeatureswithout
supervision. arXiv preprint arXiv:2304.07193 , 2023.
[23]L. Perron and V. Furnon. Or-tools. URL https://developers.google.com/
optimization/ .
[24]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal. Learningtransferablevisualmodelsfromnaturallanguage
supervision. In International conference on machine learning , pages 8748–8763. PMLR,
2021.
[25]M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein. SVCCA: Singular Vector
Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability.
AdvancesinNeuralInformationProcessingSystems ,2017-Decem:6077–6086,jun2017. URL
http://arxiv.org/abs/1706.05806 .
[26]M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy. Do vision
transformers see like convolutional neural networks? Advances in neural information
processing systems , 34:12116–12128, 2021.
11[27]V.V.Ramasesh,E.Dyer,andM.Raghu. Anatomyofcatastrophicforgetting: Hidden
representations and task semantics. arXiv preprint arXiv:2007.07400 , 2020.
[28]L. Song, A. Smola, A. Gretton, J. Bedo, and K. Borgwardt. Feature selection via
dependence maximization. Journal of Machine Learning Research , 13:1393–1434, 2012.
ISSN 15324435.
[29]I.Sucholutsky,L.Muttenthaler,A.Weller,A.Peng,A.Bobu,B.Kim,B.C.Love,E.Grant,
I. Groen, J. Achterberg, et al. Getting aligned on representational alignment. arXiv
preprint arXiv:2310.13018 , 2023.
[30]P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau,
E. Burovski,P.Peterson, W. Weckesser, J.Bright, S. J.van der Walt,M. Brett, J.Wilson,
K.J.Millman,N.Mayorov,A.R.J.Nelson,E.Jones,R.Kern,E.Larson,C.J.Carey,İ.Polat,
Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen,
E.A.Quintero,C.R.Harris,A.M.Archibald,A.H.Ribeiro,F.Pedregosa,P.vanMulbregt,
andSciPy1.0Contributors.SciPy1.0: FundamentalAlgorithmsforScientificComputing
in Python. Nature Methods , 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2.
[31]L.Wang,L.Hu,J.Gu,Y.Wu,Z.Hu,K.He,andJ.Hopcroft. TowardsUnderstanding
LearningRepresentations: ToWhatExtentDoDifferentNeuralNetworksLearnthe
Same Representation. Technical report, 2018. URL https://arxiv.org/abs/1810.
11750.
[32]A.H.Williams,E.Kunz,S.Kornblith,andS.Linderman. Generalizedshapemetricson
neural representations. Advances in Neural Information Processing Systems , 34:4738–4750,
2021.
[33]S. Woo, S. Debnath, R. Hu, X. Chen, Z. Liu, I. S. Kweon, and S. Xie. Convnext v2:
Co-designing and scaling convnets with masked autoencoders. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16133–16142,
2023.
[34]J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. Understanding neural
networks through deep visualization. arXiv preprint arXiv:1506.06579 , 2015.
[35]C. Zhu, F. Xiao, A. Alvarado, Y. Babaei, J. Hu, H. El-Mohri, S. Culatana, R. Sumbaly,
and Z. Yan. Egoobjects: A large-scale egocentric dataset for fine-grained object
understanding. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 20110–20120, 2023.
12A Kernel function definitions
Defining the notion of similarity between two vectors is a matter of preference and wanted
properties. Inthis work,we includetheRadialBasis FunctionkernelEq.(6) andthelinear
kernelEq.(7),aswellasthecosinesimilaritykernelEq.(8). Weincludetheformertwoasthey
were proposed for RSM construction by Kornblith et al. [12]and the cosine similarity due to
its popularity in the retrieval domain, despite generally being applied to class probabilities.
We denote that in our manuscript we refer to the linear kernel as the inner product and dot
product interchangeably as they correspond to the same operation.
kRBF(x,y) = exp
−∥x−y∥2
2σ2
(6)
klinear(x,y) =⟨x,y⟩ (7)
kcosine(x,y) =⟨x,y⟩
∥x∥∥y∥(8)
Differentproperties Thethreeselectedkernelsallhavedifferentproperties: TheRBFkernel
and the Cosine similarity are bounded between kRBF(x, y), kcosine (x, y)∈[0,1]∀x, y∈ R,
while the klinear (x, y)is not bounded.
Moreover, the RBF kernel is parametrized by σ, which influences at which rate the distance
between the representations results in a decrease in similarity. For all our experiments
wechoose σasthesquarerootofthemedianEuclideandistanceofalldistanceswithina
mini-batch.
B Translation Sensitivity of a ViT-B/16
SimilarlytothetranslatedTiny-ImageNetexperiment,wepreparedaverysimilarexperiment
for a ViT-B/16 vision transformer that was pre-trained on ImageNet1k. We utilize the
implementation and weights provided by torchvision [17].
Given the larger ImageNet images, we resample them to 324×324pixels and crop two
partiallyoverlappingimagesofsize 224×224fromit. Giventheseimagepairs,wecalculate
semantic and spatio-semantic RSMs again, see Fig. 7.
It is important to note that our approach intentionally avoids achieving a perfect translation
thatwouldleadtothesamepatchifiedtokens,asweshifttheimagebyafactorthatisnot
divisibleby16,thepatchingwindowsize. Webelievethisrealisticimperfectionispreferable
to a perfect overlap, where identical tokens would be formed from the exact same set of
pixels.
Similarly to the previous partially overlapping crop experiment, we can observe that
spatio-semanticRSMsareincapableofidentifyingthelargelyidenticalcontentofthetwo
partiallyoverlappingcropsduetotheirdifferentlocalization. Thespatio-semanticRSMs
can capture this notion of similarity as evidenced by the off-diagonal and the larger gap
in the distribution of similarity between partially overlapping samples and independent
samples, Fig. 7 (bottom).
ContrarytotheCNNexampleinthemain,overallsimilaritybetweensamplesismuchlower
overallandseparationbetweentranslatedimagepairsandrandomimagepairsfollowsa
vastly different trajectory. While there is a profound difference, the origin of this difference
cannot be clearly made out. We hypothesize that this may be due to the different ways that
Transformers process information and learn different representations, as highlighted in
Raghu et al. [26].
Moreover, we emphasize, that calculating the optimal permutation is significantly faster
for ViTs than the CNNs, as the early tokenization reduces the spatial dimension Ssubstan-
tially at an early stage, whereas the iterative downsampling of CNNs makes comparing
representations of early layers very costly.
13Layer
 2 of 12
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.00.20.40.60.81.0
Spatio-Semantic
RSMs
Value Distribution
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.00.20.40.60.81.0
Value DistributionRSMsSemantic ViT-B/16 
Layer
 6 of 12
Layer
 10 of 12
0 1 2 3 4 5 6 7 8 9 10 11
layer
0.0
0.2
0.4
0.6
0.8
1.0similarity
Comparison
Different
Same2Same
Translated
0 1 2 3 4 5 6 7 8 9 10 11
layer
0.2
0.4
0.6
0.8
1.0similarity
Comparison
Different
Same2Same
TranslatedFigure 7: Semantic RSMs do also capture spatial translations for token sequences of
ViT’s. We calculate spatio-semantic and semantic RSMs with the Radial Basis Function
(RBF)kernelforaViT-B/16withrepresentationsextractedfromImageNet. Similarly,we
introducepartially-overlappingcropsthatgettokenizedandprocessedbytheViT.Dueto
theinitialshift,thetokensequencedoesnotalignanymorebetweenthecrops. Similartothe
CNNs, spatio-semantic RSMs exhibit low similarity values in the off-diagonals, providing a
limited indication of overlapping content between crops. In contrast, Semantic RSMs prove
notablymoreeffectiveindiscerningsubstantialoverlap,offeringhighersimilarityvalues
in the off-diagonals and thereby indicating a greater degree of similarity between large
portions of the image.
C Pseudo Code
In addition to our provided explanation of the algorithm in the main manuscript, we
provide the pseudo-code used to compute semantic RSMs in Algorithm 1. The only
differencebetweensemanticandspatio-semanticRSMsalgorithmicallyiswheretheoptimal
permutation is calculated that maximally aligns the two representations. The current
definition of spatio-semantic RSMs assumes that spatial locations are corresponding, while
semantic RSMs calculate correspondence through similarity matching.
D Additions: Retrieval Experiment
In addition to the provided retrieval examples in the main manuscript, we provide more
details on the retrieval experiments in Appendix D.1, an additional table holding the
quantitativedataofFig.3withvaryingdatabasesizesinAppendixD.2andlastly,additional
qualitative retrieval examples for each model including direct comparisons of all models for
the same query image in Appendix D.3.
D.1 Details of Retrieval Experiment
For the retrieval experiment, we utilize the EgoObjects dataset [ 35]. It contains multiple
framesfrommultiplevideos,withmultiplevideoscapturingthesamesceneunderdifferent
shiftslikelightingconditions,distances,viewingangles,anddifferentmotiontrajectories.
For each frame, multiple objects of different categories can be present and are annotated
14Algorithm 1: Semantic RSM calculation. We calculate the optimal permutation matrix,
resulting in maximal similarity between the representations of two samples.
Data: Z∈RN×C×S; Kernel k
Result:Semantic RSM K
K=0∈RN×N
forifrom 0 to Ndo
forjfrom 0 to Ndo
ifi < jthen
cont.
ifi̸=jthen
Pij=argmax Pij⟨zi,Pijzj⟩;
z′
j=Pijzj;
else
z′
j=zj;
Kij=k(zi, z′
j);
Kji=Kij; /* is symmetric */
throughaboundingbox. Moreover,framescanvaryinspatialresolution,yetalargefraction
was captured in 16:9 format of 1920×1028pixels.
Image preprocessing For our experiments, we utilize the EgoObjects test set, which is
comprised of 29.5K images. Of these 29.5k we remove all images not in 16:9 format and
resize the remaining to the 1920×1028format. This discards roughly 10k images.
Ofallremainingimages,wethendraw2kqueryimagesand5kdatabaseimagesusedfor
extracting embeddings for similarity calculation and later retrieval. Naturally, we sample in
a way to keep the 2k query and 5k database image sets non-overlapping. When passing
the images to the models for feature extraction each image is preprocessed according to the
corresponding HuggingfaceImagePreprocessor. This mostlyrepresents resizingthe image
bytheshortestedgetotheexpectedimageinputdimensionsandnormalizingtheimage.
The only exception is SAM, of which we use the official implementation, which handles
feature extraction and embedding of the image itself.
Feature Extraction and preparation As mentioned in the main manuscript we use
1. CLIP (ViT/B32) [24]
2. ClipSeg (Rd64) [16]
3. DinoV2-Giant [22]
4. SAM (ViT/B32) [9] and
5. BIT-50 [11]
as general feature extractors as they were trained on a vast amount of data. 4.
Ofallmodels,weusethelasthiddenlayerasimageembeddingsshouldtheynotper-default
provideimageembeddingsasoutput. Afterextractingrepresentationswecalculatethemean
from the database embeddings to zero-center all representations by, query and database
representations alike.
RSM construction Given all 2000 query embeddings and 5000 database embeddings,
wecalculatetheRSMs. Toparallelizethisprocesswemini-batchtherepresentationsinto
100×100pairs and populate the 2000×5000matrix in this fashion. We denote that this
proved to be necessary for models with large spatial embedding dimensions like SAM,
starring 64×64spatial extent. Moreover, we denote that, due to the RBF choosing its
4Last accessed on 22nd of May 2024
15parameterbasedonthemedianofthemeasuredvalueswithinonebatchthispatch-wise
calculationisnotoptimalforthiskernel. Theinnerproductandthecosinesimilaritykernels
are not affected by this.
Retrievalmeasurement GiventheRSMscontainingasample-to-samplesimilaritymeasure,
we can retrieve the most similar sample of the database for each query from it.
Aseachimagecancontainmultipleobjectsmeasuringretrievalperformanceisnottrivial.
For both images we quantify how many objects of each class are present in the image,
resultinginacountofclassinstancesforeachimage. Withthequeryimagerepresentingthe
groundtruth(GT)andthedatabaserepresentingtheprediction,wematchclassinstance
counts. Each correctly matched GT instance represents a TP, each missed an FN and all
unmatched database instances represent an FP.
To formalize: Let Qcbe the number of instances of class cin the query image and Dcbe the
numberofinstancesofclass cinthedatabaseimage. Withthis,theusedF1metriccanbe
expressed as
TP=X
c∈Cmin(Qc, Dc) (9)
FN=X
c∈Cmax(0 , Qc−Dc) (10)
FP=X
c∈CFPc=X
c∈Cmax(0 , Dc−Qc) (11)
F1 =2·TP
2·TP+FP+FN(12)
D.2 Additional Quantitative Retrieval data
In addition to the results highlighted in Fig. 3 we provide retrieval results for varying
database sizes to retrieve from EgoObjects. Specifically, results for database sizes of 2.5k, 5k,
and 10k samples are given in Table 2.
16Table 2: Quantitative results of the EgoObject retrieval experiment for multiple models
andmultipledatabasesizes. DatabaseSize5.000representsFig.3ofthemainmanuscript.
It can be observed that models with a greater spatial extent (CLIPSeg and SAM) show
greater improvement in retrieval performance over models with a lower spatial extent
(DinoV2-Giant, BiT-50, CLIP). Moreover, it can be observed that retrieval improvements
decreasewithgrowingdatabasesize. Thisislikelyduetotheincreasingoddsoffindingone
image where spatial position and semantics align w.r.t. the query images. PI: Permutation
Invariant, Diff: Difference (PI - None)
Database Size (N) 2.500 5.000 10.000
Invariance None PI Diff None PI Diff None PI Diff
Architecture Kernel F1@1 F1@1 F1@1
CLIPSegCosine Sim. 0.240 0.781 0.541 0.280 0.835 0.555 0.344 0.859 0.515
Inner Product 0.233 0.548 0.315 0.281 0.624 0.343 0.328 0.569 0.241
RBF 0.205 0.781 0.576 0.247 0.836 0.589 0.293 0.854 0.561
DinoV2-GiantCosine Sim. 0.765 0.840 0.075 0.827 0.876 0.049 0.857 0.889 0.032
Inner Product 0.767 0.839 0.072 0.829 0.876 0.047 0.856 0.890 0.033
RBF 0.735 0.834 0.099 0.810 0.876 0.066 0.843 0.888 0.045
BiT-50Cosine Sim. 0.670 0.799 0.129 0.750 0.846 0.096 0.800 0.868 0.068
Inner Product 0.659 0.780 0.122 0.730 0.823 0.093 0.782 0.852 0.069
RBF 0.582 0.789 0.208 0.668 0.835 0.167 0.741 0.859 0.118
CLIPCosine Sim. 0.704 0.803 0.100 0.784 0.858 0.073 0.827 0.876 0.049
Inner Product 0.703 0.792 0.090 0.780 0.848 0.067 0.823 0.867 0.044
RBF 0.584 0.779 0.196 0.679 0.835 0.156 0.749 0.863 0.114
SAM ViT/B32Cosine Sim. 0.509 0.735 0.226 0.614 0.804 0.190 0.688 0.838 0.151
Inner 0.511 0.695 0.183 0.616 0.774 0.158 0.688 0.815 0.126
RBF 0.371 0.703 0.332 0.474 0.784 0.310 0.556 0.823 0.267
D.3 Additional Qualitative Examples
In addition to the two qualitative examples provided for SAM in the main, additional
examplesareprovided. Qualitativeexamplesarepickedfromthefirst50queryimageswhen
retrievingfromadatabasesizeof5.000images. Additionalqualitativeretrievalexamples
areprovidedforDinoV2inFig.8,CLIPinFig.9,BiT-50inFig.10,CLIPSeginFig.11and
SAMinFig.12. Moreover,wehighlightadirectcomparisonbetweenallmodelsforthesame
images in Fig. 13 and Fig. 14.
Acrossallmodels,itcanbeobservedthatmodelsretrieveimageswheresemanticcontent
and spatial positions are aligned when using standard cosine similarity. This leads to
larger-scaleobjectslikedesks,tiledfloors,orcountertopsdominatingretrievalwhenimaged
from a similar perspective. When decoupling spatial-alignment from semantic content,
images of the same scene but a different perspective get retrieved more often, leading to the
same scene appearing more regularly in the 5 nearest neighbors. This effect is currently not
quantifiedin theF1metric, dueto onlycomparingobject presencebetween thequeryand
the 1st neighbor and not the average F1 between the query and the top 5 neighbors. We
opted against using this, as retrieval metrics most commonly consider the maximum match
in the top 5 neighbors.
Moreover,itcanbeobservedthatmodelswithlowerspatialextentcanretrievequitedifferent
neighborsasopposedtomodelswithhigherspatialextent,seeFig.13. WhileDinoV2and
CLIPretrieveverysimilar4thand5thneighborsofthesamebookobjectwithoutthesmaller
mouse and headphone object, CLIPSeg and SAM retrieve scenes with these two objects still
present instead.
D.4 Cityscapes Quantitative Retrieval
Since previous results were limited to the EgoObjects dataset we provide an additional
quantitative experimenton CityScapes. Analogto before, weuse N=500 validationimages
as the query dataset and the remaining N=2975 training images as the database for retrieval.
WeutilizetheIoUmetrictocomparethepresenceofsemanticclassesbetweentheimages.
17Figure8: AdditionalqualitativeretrievalsamplesforDinoV2. Wevisualizethetop5most
similar neighbors for four query images.
18Figure9: AdditionalqualitativeretrievalsamplesforCLIP. Wevisualizethetop5most
similar neighbors for four query images.
19Figure 10: Additional qualitative retrieval samples for BiT-50. We visualize the top 5 most
similar neighbors for four query images.
20Figure11: AdditionalqualitativeretrievalsamplesforCLIPSeg. Wevisualizethetop5
most similar neighbors for four query images.
21Figure12: AdditionalqualitativeretrievalsamplesforSAM. Wevisualizethetop5most
similar neighbors for four query images.
22Figure 13: Direct comparison of models. We visualize the top 5 most similar images of all
models retrieved through cosine similarity or permutation invariant cosine similarity.
23Figure 14: Direct comparison of models. We visualize the top 5 most similar images of all
models retrieved through cosine similarity or permutation invariant cosine similarity.
24Due to the lack of instance labels in the CityScapes dataset, we can’t take the quantity of
objects into account as before, but can only consider if a semantic class is present or absent.
As apparent from Table 3 permutation invariance allows for consistently improved retrieval
when utilizing cosine similarity or RBF kernels.
Table 3: Retrieval results for the Cityscapes Dataset. We retrieve the most similar image
accordingtoRSMsandcalculatetheIoUofquerysemanticclassesandretrievedsemantic
classes. OverallqueryimagesusedarevalidationimagesofsizeN=500andthedatabase
arethetrainingimagesofsizeN=2975. Differencesbetweenmetricsarelow,duetomany
images containing a large number of classes and the lack of instance label information.
Despitethis,permutationinvarianceimprovesCosineSimandRBFretrievalperformance
consistently, with the Inner Product showing mixed results.
Cosine Sim. Inner Product RBF
Invariance - (ours) PI - (ours) PI - (ours) PI
Architectures
CLIPSeg 0.662 0.6940.664 0.6540.664 0.696
DinoV2-Giant 0.689 0.7000.686 0.6960.686 0.690
BiT-50 0.679 0.6870.677 0.677 0.684 0.687
CLIP 0.691 0.7010.693 0.7010.678 0.687
SAM ViT/B32 0.690 0.7020.687 0.6770.679 0.687
E Details: Output similarity vs Representational Similarity
To measure the correlation between the inter-sample representational similarity and the
prediction probability inter-sample similarity we utilize pre-trained classifiers and the
ImageNet1k [ 2] dataset. Unlike during the retrieval results this constraints the possible
model selections to models trained for classification.
Image preprocessing Test set images of ImageNet1k are randomly sampled without
applyinganyfilteringtothem. Intotal,weutilizeasubsetof2kImageNettestsetsamplesin
this experiment. This may appearsmall, yet provides a sufficientbasis as thecombinatoric
growth increases the absolute number of measurements substantially.
Featureandlogitextractionandpreparation Asmentionedinthemainmanuscriptwe
use
1. ResNet18 [7]
2. ResNet50 [7]
3. ResNet101 [7]
4. a DinoV2-Giant based classifier [22]
5. ConvNeXt V2 [33]
6. ViT-B/16 [4] and
7. ViT-L/32 [4] and
aspre-trainedclassifiersforpredictingtheImageNet1kclasses. 5. Foreachsample,weextract
the last hidden layer’s representations and center them analog to before. For the same
sample,weextractthelogitsandobtaintheprobabilitydistributionthroughthesoftmax,
saving the pair for later comparisons.
Correlation measurement For each pair of representations and probabilities, we calculate
thesimilaritiesbetweentheirrepresentationsforallthreekernelfunctions,oncepermutation
invariant and once not. Additionally, we calculate the Jensen-Shannon Divergence (JSD)
5Last accessed on 22nd of May 2024
25betweenthepredictedclassprobabilitydistributionsfor PandQ. Aformaldefinitionofthe
JSD is provided in Eq. (13).
JSD(P∥Q) =1
2DKL(P∥M) +1
2DKL(Q∥M) (13)
where Mis the pointwise mean of PandQ:
M=1
2(P+Q) (14)
andDKLis the Kullback-Leibler divergence defined as:
DKL(P∥Q) =X
iP(i) logP(i)
Q(i)(15)
Given the paired JSD and Similarity Kbetween all samples i, jwe utilize the Pearson
correlation ρtocalculatethecorrelationbetweenthetwo. DuetotheJSDbeing0foridentical
probabilitiesandincreasingformoredissimilarvaluesandtheSimilaritybeing1forperfectly
similar representations and 0 for dissimilar representations, the desired correlation between
the two should be negative.
E.1 Additional correlation results
InadditiontothePearsoncorrelationbetweentheJensen-Shannon-Divergence(JSD)and
inter-image similarity, we also present the results of their relationship measured by the
Spearman correlation, as shown in Table 4.
While the Pearson correlation demonstrated consistently stronger correlations with cosine
similarity,theinnerproduct,andtheRBFkernel,theSpearmanrankcorrelationsareless
stable across these methods.
ForResNets,we observeasignificantdeclinein correlationconsistencyandstrengthwith
theexceptionofResNet18. Opposedtothis,ViTsdisplaynotablyhighernegativecorrelation
values when using cosine similarity and radial basis function kernels, in contrast to the
Pearson correlation results.
Table 4: Correlation between the representational similarity and the output probabilities of
multiple images. In total 20k samples of the IN1k test set are used (as opposed to 2k in the
table in the main)
Metric Pearson Correlation Spearman’s Rank Correlation
Kernel Cosine Sim. Inner Product RBF Cosine Sim. Inner Product RBF
Invariance - PI - PI - PI - PI - PI - PI
Architecture
ResNet18 -0.279 -0.328 -0.264 -0.272 -0.174 -0.197 -0.231 -0.337 -0.239 -0.225 -0.435 -0.476
ResNet50 -0.256 -0.305 -0.249 -0.269 0.028 0.015-0.032 0.007-0.046 -0.040 0.128 0.132
ResNet101 -0.235 -0.330 -0.211 -0.274 0.076 0.067-0.007 -0.053 -0.007 -0.077 0.071 0.068
ConvNextV2-Base -0.162 -0.126 -0.160 -0.184 0.077 0.050-0.017 0.026-0.013 -0.045 0.143 0.128
ViT-B/16 -0.058 -0.098 -0.056 -0.031 -0.079 -0.120 -0.013 -0.230 -0.021 0.029-0.220 -0.313
ViT-L/32 -0.142 -0.189 -0.143 -0.152 -0.131 -0.164 -0.034 -0.276 -0.029 -0.014 -0.335 -0.392
DinoV2-Giant -0.016 -0.046 -0.016 -0.030 -0.013 -0.052 -0.014 -0.037 -0.015 -0.022 -0.015 -0.042
F Details of the Approximation Algorithms
ThecomputationalcomplexityofdeterminingoptimalmatchingsusingtheJonker-Volgenant
algorithm[ 8]scalessignificantlywith O(s3),resulting insubstantialcomputationtime for
input-patchsizeswithspatialdimensions s= 642. Toaddressthischallenge,wepropose
alternative approximate algorithms with reduced computational complexity. In all our
approximations, we take advantage of additional information, specifically the L2-norm
∥vi∥2of each semantic concept vector. We assume that achieving a high degree of matching
involves pairing vectors with the highest norms and high cosine similarity. This assumption
guides our design of more efficient matching algorithms.
261.Greedy: Thesimplestapproachweemployisbreadth-firstmatching. Wedetermine
the order in which to match viby considering the L2-norm ∥vi∥2in descending
order. We then match the current viwith the best, non-assigned vjbased on Aij.
Thesortingcomplexityis O(slog(s)),makingthisthefastestapproximatealgorithm
among those tested.
2.TopK-Greedy: Recognizing that the TopK norm concept vectors ∥vi∥2might have a
significantly higher impact on the final similarity, we attempt to find the optimal
matching for only the highest TopK norm concept vectors viandvj. The remaining
lowernorm conceptvectors areassignedusing theGreedy algorithmas described
above. Theprocessinvolvesaninitialsortingbasedonthesemanticconceptvectors’
L2-norm, followed by optimal matching with O(k3)complexity for the kTopK
values and the greedy matching for the remaining values.
3.Batch-Optimal: IftheTopKnormconceptvectorsdonotsufficientlyapproximate
an optimal matching, we apply optimal matching for the remaining concept vectors
inbatches. Toachievethis,wecreate s//bsmallerbatches,withsemanticconcept
vectorsassignedtobatchesaccordingtotheirL2-norm. Allvalueswithinabatch
are then optimally matched, leading to a matching complexity of (s
b)· O(b3).6
Evaluating the various approximations, we observe that the Greedy matching yields
suboptimal approximation quality and offers marginal to no improvement over the current
same-position assignment. Although we do not present the details of the greedy matching,
it is important to highlight that it is guaranteed to be worse or equal to the TopK-Greedy
matching with a kvalue of 128, as shown in Fig. 5. We include the Greedy algorithm for
completeness as a simple baseline.
Furthermore, itis noteworthy that the TopK-Greedy matchingdemonstratesthatexclusively
matchingthelargestnormconceptvectorsisinsufficientforagoodapproximationofthe
optimal matching. This insight suggests that a substantial portion of the overall similarity is
contributed by semantic concept vectors not included in the set of highest norms.
Lastly,weobservethattheBatch-Optimalapproximation,usingasmallbatchsizeof128
samples, provides an approximation with less than 10%error compared to the optimal
matching. Thisresultunderscorestheeffectivenessofourbatchingapproachbasedonthe
L2-normoftheconceptvectors. Itoffersareliableestimateforoverallsimilarity,simplifying
the matching process significantly.
F.1 Runtime Evaluation
In order to assess algorithm performance across different spatial resolutions, we conducted
abenchmarkingstudy. Foreachresolution,werandomlyselected10,000pairsofsamples
from a ResNet101 trained on Tiny-ImageNet. Affinity matrices ( Aij) were pre-computed
to facilitate permutation ( Pij) calculations. The average time taken per matching was
then reported for the same single CPU core, as outlined in Table 5. We observe that the
OR-Toolsimplementationoutperformsotheralternatives,beingfourtimesfasterthanthe
lapjv implementation 7. However, even this optimal approach requires 1.52 seconds per
pairona 64×64resolution. Despitethepotentialforparallelizingsample-wisematching,
optimal algorithms face scalability challenges with larger spatial dimensions S. In contrast,
theBatch-Optimalapproximationoffersacompellingbalancebetweencomputationtime
and approximation quality. Importantly, its complexity scales linearly with Sdue to the
fixed batch size.
G Semantic RSMs and CKA – Qualitative changes
Building upon the success of Linear/RBF ( spatio-semantic ) CKA to compare systems, we
provide some preliminary qualitative comparisons gauging how CKA comparisons are
affectedbyourdifferentlyproposedRSM.Unfortunately,hardlyanyquantitativebenchmark
6Thereisanerrorinthecurrentversioninthemainregardingthesquarerootof s,whichwillbe
corrected in a revision. We apologize for any confusion
7Implementation on Github https://github.com/src-d/lapjv
27Table5: WecomparedifferentimplementationsoftheoptimalJonker-Volgenantalgorithm
[8]againstourlinearlyscalingBatch-Optimalapproximationandnomatching. Thetable
presents average run-times per pair ( T) and average similarity relative to the optimal value
(k
kopt)for1,000randomlychosenimagepairs. Utilizingrepresentationsofvaryingsizesfrom
a ResNet101 trained on Tiny-ImageNet, the optimal solutions are reported relative to the
maximumsimilarityachievedbytheoptimalalgorithms. TheBatch-Optimalapproximation
demonstrates a substantial fraction of optimal matching performance with significantly
improved scaling.
Category Complexity Matching Algo Batch bs= 256 s= 1024 s= 4096
k
kopt[%] Tk
kopt[%] Tk
kopt[%] T
Optimal O(s3)OR-Tools [23] - 100 4.26 ms 100 69.6 ms 100 1.52 s
Scipy [30] - 100 5.10 ms 100 148 ms 100 8.75 s
lapjv - 100 5.73ms 100 97.5 ms 100 6.12 s
No Match O(1) np.diag() - 68.9 1.40 µs67.0 2.49 µs57.9 4.36 µs
Approximation (√s/b)· O(b3)Batch-Optimal128 97.6 2.41 ms 94.5 9.08 ms 92.8 43.4 ms
256 100 4.56 ms 96.8 16.9 ms 95.0 95.9 ms
512 100 4.57 ms 98.6 37.1 ms 96.8 171 ms
1024 100 4.57 ms 100 79.5 ms 98.2 391 ms
exists to quantify if a representational similarity metric is betterthan another. Previously
Kornblithetal. [12]evaluatedCKAbyshowingitwasbetteratfindinglayersofthesame
architecture than SVCCA and PWCCA. This was extended by Ding et al. [3]through the
inclusionofstatisticaltestingbutremainsarathershallowbenchmark. Subsequently,we
constrainourselvestoqualitativeexperiments,leavingquantitativetestingtopotentialfuture
benchmarks.
Inallfollowingexperiments,allrepresentationsareextractedgloballyandzero-centered
along the sample dimension. Given the zero-centered representations spatio-semantic and
semantic RSMs are computed in mini-batches of 250 samples. To calculate the semantic
RSMs on CIFAR an optimal bipartite matching algorithm is used, while for Tiny-ImageNet
and ImageNet we utilize the Batch-Optimal approximation with window size b512.
G.1 CKA between semantic and spatio-semantic RSMs
Asinitialinspection,weevaluatehowdifferentthesimilaritystructuresofamodelmeasured
throughspatio-semanticRSMsaretoamodelmeasuredthroughsemanticRSMs. Wedo
thisbyconstructingbothRSMs fromthesamerepresentations ofamodel. Subsequently,
we compare the two alternative RSMs of the same representations to each other through
CKA Eq. (2). The diagonal of this matrix represents a direct comparison of identical
representations, just with another definition of what is considered "similar". This is
evaluatedforthreeResNet18sandthreeResNet34sonTiny-ImageNetwiththelinearand
RBF kernels respectively.
We display the average CKA matrices across architecture seeds for both architectures, as
well as the diagonal values of the CKA matrix. Results are shown in Fig. 15.
Examining these CKA matrices multiple observations can be made:
A)Despitethediagonalrepresentingacomparisonbetweenidenticalrepresentations,the
CKAvaluesarenot1. ThisindicatesthatthedifferentwayofconstructingRSMschanges
the perceived similarity structure of the system, as measured by CKA.
B) Inspecting the diagonal shows, that earlier layers with greater spatial extent express
higherdifferencesinsimilarity,whereaslayersatalaterlayerandlowerspatialextentare
lessdissimilar. This isconsistentwiththeexpectation that, withshrinkingspatial extent,
alignment of semantic concepts gets more likely.
GiventheselargechangesinCKAsimilarity,weconcludethatthedefinitionofwhatamodel
perceivesas similarcanhighly influenceinter-system similarity. Thisisespecially relevant
when comparing systems across domains, where RSM construction may be domain-specific,
disallowingtobeconsistentwithRSMconstruction. ExemplarywhencomparingMLvision
systems to human vision models, in particular when comparing representations of high
spatial extent.
28Diagonals of CKA MatricesCKA(K,Koptimal ) with Lin KernelResNet18Linear Kernel RSMs
CKA(K,Koptimal ) with Linear KernelDiagonals of CKA Matrices
Relative Architecture Dept0.9
0.0
0.2
0.4
0.6
0.8
1.0
0.5
0.6
0.7
0.8
Linear Kernel
model
ResNet18
ResNet34
0
1
2Semantic RSM
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
ResNet18 LinearCKA, Mean CKA 71.0%, Mean Diag CKA 82.6%Spatio-semantic RSM
Semantic RSMSpatio-semantic RSM
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
ResNet34 LinearCKA, Mean CKA 73.6%, Mean Diag CKA 84.9%ResNet34
0.00.20.40.60.81.0
ResNet18 Diagonals of CKA MatricesCKA(K,Koptimal ) with RBF Kernel
0.0
0.2
0.4
0.6
0.8
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
RBF Kernel
model
ResNet18
ResNet34
0.00.20.40.60.81.0
RBF Kernel RSMs
Semantic RSMSpatio-semantic RSM
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
ResNet18 RBFCKA, Mean CKA 71.0%, Mean Diag CKA 82.6%ResNet34Semantic RSMSpatio-semantic RSM
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
ResNet34 RBFCKA, Mean CKA 73.6%, Mean Diag CKA 84.9%Figure 15: The dissimilarity between semantic andspatio-semantic RSMs decreases with
shrinking spatial extent. Comparison of semantic andspatio-semantic Representational
Similarity Matrices(RSMs) for thesame model. The dissimilarity inearlylayersdecreases
withdecreasingspatialextent,asillustratedbyCenteredKernelAlignment(CKA)values.
TheleftandmiddlepanelshowstheCKAcomparisonbetweenalllayers,whiletheright
panelsvisualizetheheatmap’sdiagonal,emphasizingtheevolvingsimilaritytrendfrom
early to late layers.
G.2 Differences in CKA self-similarity
Inthepreviousparagraph,semanticRSMsweredirectlycomparedtospatio-semanticRSMs.
While this can influence measurements when models are compared across domains (e.g.
CNNVisiontoBiologicalVision),itdoesnotneedtoimplythattheCKAsimilarityofvision
models changes substantially.
Given that within the same domain, the RSM construction will likely be chosen consistently,
one can opt to either: Calculate only spatio-semantic RSMs or only semantic RSMs, due
topersonalopinionsorpreferences. Subsequently,thequestionisnot "Arespatio-semantic
RSMssimilartosemanticRSMs" ,but"DoestheCKAsimilaritybetweenspatio-semanticRSMs
change when calculating semantic RSMs" .
Toaddressthisquestionwe: A)ComparetheCKAmatrixdifferenceoftheCKAmatrixbased
onsemanticRSMstotheCKAmatrixofspatio-semanticRSMswhencomparingamodel
with itself (Intra Model) and B) when comparing between different models (Cross-Model).
Intra-Model CKA Similarly to before, we extract representations and form semantic and
spatio-semanticRSMs. WeextractrepresentationsonCIFAR100[ 14](32×32),Tiny-ImageNet
(64×64)andImageNet-1k[ 2](160×160)datasets,from3differentlyseededandtrainedfrom
scratchResNet18,ResNet34andResNet101architectures. ThesemanticRSMsarecalculated
utilizing the Batch −Optimal matching with b512 for matching on Tiny-ImageNet and
ImageNet. We calculate semantic and spatio-semantic RSMs with a mini-batch size of
250,subsequentlyusingthemforCanonicalCorrelationAnalysis(CKA)calculations. The
corresponding cka matrices and their differences are displayed in Fig. 16. If not further
specified the experiment uses the linear kernel.
IntrospectingtheresultsitcanbeseenthatacrossallArchitecturesResNet18,ResNet34,and
ResNet101largelythesamechange insimilaritystructurecan beobserved. ForCIFAR100,
theveryfirstlayersareperceivedaslesssimilartosemanticRSMsthanwithspatio-semantic
RSMs, while the CKA between the middle to later layers is more similar. This structure,
thoughdoesnotremainconsistentacrossdatasets: WhenmovingfromCIFAR100toTiny-
ImageNetearlierlayersappeartobecomemoresimilarwhileintermediatelayersbecome
lessso. OnImageNet1kCKAonsemanticRSMsseemtoindicatemodelsaremoresimilar.
This trend indicates that the influence of semantic RSMs on spatio-semantic RSMs seems to
be largely dataset-dependent. Moreover, the overall maximum change in CKA similarity in
these matrices is between −0.2and+0.2for Tiny-ImageNet, indicating a modest change in
overall CKA.
Cross-ModelCKA AsidefromevaluatingonlyCKAsimilarityofRSMsofthesamemodel
weextendtocomparingRSMsbetweenmodels,ascommonlydonewhencomparingmodels
through CKA. ResNet18/101 models trained on CIFAR100 are used with RSMs constructed
identically to previously specified. Results are displayed in Fig. 17.
290 1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 8Semantic CKA
0 1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 8Spatio-Semantic CKA
0 1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 8Difference Sem. - Spatio-semantic CKA
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.10
0.05
0.000.050.10
012345678910111213141516012345678910111213141516Semantic CKA
012345678910111213141516012345678910111213141516Spatio-Semantic CKA
012345678910111213141516012345678910111213141516Difference Sem. - Spatio-semantic CKA
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.05
0.000.050.10
024681012141618202224262830320123456789101112131415161718192021222324252627282930313233Semantic CKA
024681012141618202224262830320123456789101112131415161718192021222324252627282930313233Spatio-Semantic CKA
024681012141618202224262830320123456789101112131415161718192021222324252627282930313233Difference Sem. - Spatio-semantic CKA
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.050
0.025
0.0000.0250.0500.0750.1000.125CIFAR100
0 1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 8Semantic CKA
0 1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 8Spatio-Semantic CKA
0 1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 8Difference Sem. - Spatio-semantic CKA
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.10
0.05
0.000.050.100.15
012345678910111213141516012345678910111213141516Semantic CKA
012345678910111213141516012345678910111213141516Spatio-Semantic CKA
012345678910111213141516012345678910111213141516Difference Sem. - Spatio-semantic CKA
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.20
0.15
0.10
0.05
0.000.050.100.150.20
024681012141618202224262830320123456789101112131415161718192021222324252627282930313233Semantic CKA
024681012141618202224262830320123456789101112131415161718192021222324252627282930313233Spatio-Semantic CKA
024681012141618202224262830320123456789101112131415161718192021222324252627282930313233Difference Sem. - Spatio-semantic CKA
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.000.050.100.15Tiny ImageNet
0 1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 8Semantic CKA
0 1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 8Spatio-Semantic CKA
0 1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 8Difference Sem. - Spatio-semantic CKA
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.000.050.100.150.20
012345678910111213141516012345678910111213141516Semantic CKA
012345678910111213141516012345678910111213141516Spatio-Semantic CKA
012345678910111213141516012345678910111213141516Difference Sem. - Spatio-semantic CKA
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.000.050.100.150.200.25
024681012141618202224262830320123456789101112131415161718192021222324252627282930313233Semantic CKA
024681012141618202224262830320123456789101112131415161718192021222324252627282930313233Spatio-Semantic CKA
024681012141618202224262830320123456789101112131415161718192021222324252627282930313233Difference Sem. - Spatio-semantic CKA
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.0000.0250.0500.0750.1000.1250.150ImageNet-1kResNet18 ResNet34 ResNet101Figure 16: CKA similarity between different Layers of the same ResNet18, ResNet34,
ResNet101, on CIFAR100, Tiny-ImageNet and ImageNet-1k. For each row the left-most
CKA matrix displays the spatio-semantic RSMs, the middle represents the semantic RSMs
whiletherightrepresentsthedifferenceinCKAsimilaritybetweenthetwo. Blueregions
indicatewheretheWeobservethatsimilaritywithintheblockstructureislargelyunchanged,
whereas the similarity across the later blocks seems to be more similar and the similarity of
the very first blocks is less similar.
CKASem CKASem - CKA Spa.Sem.  CKASpa.Sem.ResNet18 ResNet101
0.00.20.40.60.81.0
0.150
0.125
0.100
0.075
0.050
0.025
0.0000.025
0.175
0.150
0.125
0.100
0.075
0.050
0.025
0.0000.025
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.00.20.40.60.81.0
Figure 17: CKA similarity between different Layers of differentResNet18 and ResNet101
modelsonCIFAR100. Weobserveadecreaseinsimilarityathigh-resolutionlayers,whereas
similarity between deeper layers is largely unchanged.
It can be seen that for both, ResNet18 and ResNet101, the cross-model CKA similarity is
mostly negative for the majority of the layers, indicating that CKA on spatio-semantic RSMs
estimates models to be more similar than when applying CKA on semantic RSMs. Similarly
to before CKA changes range from −0.175to+0.025providing modest changes.
Concluding the Intra and Cross-Model CKA experiments it can be seen that the choice
ofRSMsresultsinqualitativelydifferentCKAmatrices. Unfortunately,duetothelackof
quantitativebenchmarks,nodirectrecommendationofwhichRSMtouseforinter-model
similarity calculation through CKA can be given.
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately
reflect the paper’s contributions and scope?
Answer:[Yes]
Justification: TheexperimentsinSection4demonstratetheadvantageofintroducing
permutationinvarianceforRSMsanddirectlycorrespondtothecontributionslisted
in Section 1.
Guidelines:
•The answer NA means that the abstract and introduction do not include the
claims made in the paper.
•Theabstractand/orintroductionshouldclearlystatetheclaimsmade,including
thecontributionsmadeinthepaperandimportantassumptionsandlimitations.
ANoorNAanswertothisquestionwillnotbeperceivedwellbythereviewers.
•Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflect
how much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that
these goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the
authors?
Answer: [Yes]
Justification: In Section 5, a comprehensive discussion of the results and their
limitations, such as the runtime, is provided.
Guidelines:
•TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNo
means that the paper has limitations, but those are not discussed in the paper.
•Theauthorsareencouragedtocreateaseparate"Limitations"sectionintheir
paper.
•The paper should point out any strong assumptions and how robust the
results are to violations of these assumptions (e.g., independence assumptions,
noiselesssettings,modelwell-specification,asymptoticapproximationsonly
holdinglocally). Theauthorsshouldreflectonhowtheseassumptionsmight
be violated in practice and what the implications would be.
•Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproach
was only tested on a few datasets or with a few runs. In general, empirical
results often depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the
approach. For example, a facial recognition algorithm may perform poorly
when image resolution is low or images are taken in low lighting. Or a speech-
to-text system might not be used reliably to provide closed captions for online
lectures because it fails to handle technical jargon.
•The authors should discuss the computational efficiency of the proposed
algorithms and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach
to address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might
be usedby reviewers as groundsfor rejection, a worseoutcome might bethat
reviewers discover limitations that aren’t acknowledged in the paper. The
authorsshouldusetheirbestjudgmentandrecognizethatindividualactionsin
favoroftransparencyplayanimportantroleindevelopingnormsthatpreserve
the integrity of the community. Reviewers will be specifically instructed to not
penalize honesty concerning limitations.
313.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assump-
tions and a complete (and correct) proof?
Answer: [NA]
Justification: While the paper provides a comprehensive mathematical formulation,
no new theorems were introduced and thus no theoretical proofs were needed.
Guidelines:
•The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and
cross-referenced.
•All assumptions should be clearly stated or referenced in the statement of any
theorems.
•Theproofscaneitherappearinthemainpaperorthesupplementalmaterial,
but if they appear in the supplemental material, the authors are encouraged to
provide a short proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be com-
plemented by formal proofs provided in appendix or supplemental material.
•TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce
the main experimental results of the paper to the extent that it affects the main
claimsand/orconclusionsofthepaper(regardlessofwhetherthecodeanddata
are provided or not)?
Answer: [Yes]
Justification: All experiments were performed using public datasets and models.
The information provided in Appendix F, Appendix D.1 and Algorithm 1 provide
all necessary information to reproduce the results. Additionally, we will publish
the code.
Guidelines:
•The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be
perceived well by the reviewers: Making the paper reproducible is important,
regardless of whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the
steps taken to make their results reproducible or verifiable.
•Dependingonthecontribution,reproducibilitycanbeaccomplishedinvarious
ways. For example, if the contribution is a novel architecture, describing the
architecturefullymightsuffice,orifthecontributionisaspecificmodeland
empiricalevaluation,itmaybenecessarytoeithermakeitpossibleforothers
toreplicatethemodelwiththesamedataset,orprovideaccesstothemodel.
Ingeneral. releasingcodeanddataisoftenonegoodwaytoaccomplishthis,
but reproducibility can also be provided via detailed instructions for how
to replicate the results, access to a hosted model (e.g., in the case of a large
language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all
submissionstoprovidesomereasonableavenueforreproducibility,whichmay
depend on the nature of the contribution. For example
(a)Ifthecontributionisprimarilyanewalgorithm,thepapershouldmake it
clear how to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should
describe the architecture clearly and fully.
32(c)If the contribution is a new model (e.g., a large language model), then there
should either be a way to access this model for reproducing the results
or a way to reproduce the model (e.g., with an open-source dataset or
instructions for how to construct the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which
caseauthorsarewelcometodescribetheparticularwaytheyprovidefor
reproducibility. Inthecaseofclosed-sourcemodels,itmaybethataccessto
the model is limited in some way (e.g., to registered users), but it should be
possibleforotherresearcherstohavesomepathtoreproducingorverifying
the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient
instructionstofaithfullyreproducethemainexperimentalresults,asdescribedin
supplemental material?
Answer: [Yes]
Justification: Code will be provided by acceptance.
Guidelines:
•TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.
•PleaseseetheNeurIPScodeanddatasubmissionguidelines( https://nips.
cc/public/guides/CodeSubmissionPolicy ) for more details.
•Whileweencouragethereleaseofcodeanddata,weunderstandthatthismight
not be possible, so “No” is an acceptable answer. Papers cannot be rejected
simply for not including code, unless this is central to the contribution (e.g., for
a new open-source benchmark).
•The instructions should contain the exact command and environment needed
to run to reproduce the results. See the NeurIPS code and data submis-
sion guidelines ( https://nips.cc/public/guides/CodeSubmissionPolicy )
for more details.
•The authors should provide instructions on data access and preparation,
including how to access the raw data, preprocessed data, intermediate data,
and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for
thenewproposedmethodandbaselines. Ifonlyasubsetofexperimentsare
reproducible, they should state which ones are omitted from the script and
why.
•At submission time, to preserve anonymity, the authors should release
anonymized versions (if applicable).
•Providingasmuchinformationaspossibleinsupplementalmaterial(appended
tothepaper)isrecommended,butincludingURLstodataandcodeispermitted.
6.Experimental Setting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,
hyperparameters, how they were chosen, type of optimizer, etc.) necessary to
understand the results?
Answer: [Yes]
Justification: TheinformationprovidedinSection4provideallnecessaryinformation
to understand the results. Appendix F, Appendix D.1 and Algorithm 1 add
additionaldetailsthatareneededtoreproducetheexactexperimentalsettingbut
not necessarily to understand the results.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level
of detail that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as
supplemental material.
337.Experiment Statistical Significance
Question: Doesthepaperreporterror barssuitablyandcorrectlydefinedorother
appropriate information about the statistical significance of the experiments?
Answer: [No]
Justification: Thepaperintroducesanewconceptthathasnodirectrelatedmethods
to compare with. Therefore, no statistical analysis or ranking methods are needed.
Guidelines:
•The answer NA means that the paper does not include experiments.
•Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,
confidence intervals, or statistical significance tests, at least for the experiments
that support the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly
stated (for example, train/test split, initialization, random drawing of some
parameter, or overall run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form
formula, call to a library function, bootstrap, etc.)
•The assumptions made should be given (e.g., Normally distributed errors).
•Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandard
error of the mean.
•ItisOKtoreport1-sigmaerrorbars,butoneshouldstateit. Theauthorsshould
preferably report a 2-sigma error bar than state that they have a 96% CI, if the
hypothesis of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in
tablesorfiguressymmetricerrorbarsthatwouldyieldresultsthatareoutof
range (e.g. negative error rates).
•Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthe
texthowtheywerecalculatedandreferencethecorrespondingfiguresortables
in the text.
8.Experiments Compute Resources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthe
computer resources (type of compute workers, memory, time of execution) needed
to reproduce the experiments?
Answer: [Yes]
Justification: Thepresentedworkdoesnotrelyonspecificcomputerequirements.
However, in Appendix F.1 we compare the runtime of multiple approximation
algorithms to solve the assignment proble.
Guidelines:
•The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal
cluster, or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the
individual experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more
computethantheexperimentsreportedinthepaper(e.g.,preliminaryorfailed
experiments that didn’t make it into the paper).
9.Code Of Ethics
Question: Doestheresearchconductedinthepaperconform,ineveryrespect,with
the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have strictly adhered to the ethical guidelines.
Guidelines:
34•TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeof
Ethics.
•IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthat
require a deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special
consideration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and
negative societal impacts of the work performed?
Answer: [NA]
Justification: We do not see any direct societal impact. The work only provides a
new introspection into the concept of semantic similarity of networks.
Guidelines:
•The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no
societal impact or why the paper does not address societal impact.
•Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintended
uses (e.g., disinformation, generating fake profiles, surveillance), fairness
considerations (e.g., deployment of technologies that could make decisions
that unfairly impact specific groups), privacy considerations, and security
considerations.
•The conference expects that many papers will be foundational research and
not tied to particular applications, let alone deployments. However, if there
is a direct path to any negative applications, the authors should point it out.
For example, it is legitimate to point out that an improvement in the quality
ofgenerativemodelscouldbeusedtogeneratedeepfakesfordisinformation.
On the other hand, it is not needed to point out that a generic algorithm for
optimizing neural networks could enable people to train models that generate
Deepfakes faster.
•The authors should consider possible harms that could arise when the technol-
ogy is being used as intended and functioning correctly, harms that could arise
when the technology is being used as intended but gives incorrect results, and
harms following from (intentional or unintentional) misuse of the technology.
•Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossible
mitigation strategies (e.g., gated release of models, providing defenses in
additiontoattacks,mechanismsformonitoringmisuse,mechanismstomonitor
how a system learns from feedback over time, improving the efficiency and
accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for
responsiblereleaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrained
language models, image generators, or scraped datasets)?
Answer: [NA]
Justification: See broader impact .
Guidelines:
•The answer NA means that the paper poses no such risks.
•Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleased
withnecessarysafeguardstoallowforcontrolleduseofthemodel,forexample
byrequiring thatusers adheretousage guidelinesor restrictionstoaccess the
model or implementing safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The
authors should describe how they avoided releasing unsafe images.
35•We recognize that providing effective safeguards is challenging, and many
papersdonotrequirethis,butweencourageauthorstotakethisintoaccount
and make a best faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models),
used in the paper, properly credited and are the license and terms of use explicitly
mentioned and properly respected?
Answer: [Yes]
Justification: Allrelevantandpreviousworkiscitedandonlyopen-sourceassets
have been used.
Guidelines:
•The answer NA means that the paper does not use existing assets.
•The authorsshould citethe original paperthat produced thecode packageor
dataset.
•The authors should state which version of the asset is used and, if possible,
include a URL.
•The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and
terms of service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in
the package should be provided. For popular datasets, paperswithcode.com/
datasets has curated licenses for some datasets. Their licensing guide can
help determine the license of a dataset.
•For existing datasets that are re-packaged, both the original license and the
license of the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach
out to the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the
documentation provided alongside the assets?
Answer: [Yes]
Justification: The codebase will be published under a CC BY license.
Guidelines:
•The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as
part of their submissions via structured templates. This includes details about
training, license, limitations, etc.
•The paper should discuss whether and how consent was obtained from people
whose asset is used.
•Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). You
can either create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does
the paper include the full text of instructions given to participants and screenshots,
if applicable, as well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing experiments or research with human subjects were
performed.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor
research with human subjects.
36•Including this information in the supplemental material is fine, but if the main
contribution of the paper involves human subjects, then as much detail as
possible should be included in the main paper.
•AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,
curation, or other labor should be paid at least the minimum wage in the
country of the data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with
Human Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,
whether such risks were disclosed to the subjects, and whether Institutional Review
Board(IRB)approvals(oranequivalentapproval/reviewbasedontherequirements
of your country or institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing experiments or research with human subjects were
performed.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor
research with human subjects.
•Depending on the country in which research is conducted, IRB approval (or
equivalent) may be required for any human subjects research. If you obtained
IRB approval, you should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between
institutions and locations, and we expect authors to adhere to the NeurIPS
Code of Ethics and the guidelines for their institution.
•For initial submissions, do not include any information that would break
anonymity (if applicable), such as the institution conducting the review.
37