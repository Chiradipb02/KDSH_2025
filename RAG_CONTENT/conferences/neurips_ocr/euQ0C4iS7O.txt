Leveraging Drift to Improve Sample Complexity of
Variance Exploding Diffusion Models
Ruofeng Yang
John Hopcroft Center for Computer Science
Shanghai Jiao Tong University
wanshuiyin@sjtu.edu.cnZhijie Wang
John Hopcroft Center for Computer Science
Shanghai Jiao Tong University
violetevergarden@sjtu.edu.cn
Bo Jiang
John Hopcroft Center for Computer Science
Shanghai Jiao Tong University
bjiang@sjtu.edu.cnShuai Li∗
John Hopcroft Center for Computer Science
Shanghai Jiao Tong University
shuaili8@sjtu.edu.cn
Abstract
Variance exploding (VE) based diffusion models, an important class of diffusion
models, have shown state-of-the-art (SOTA) performance. However, only a few
theoretical works analyze VE-based models, and those works suffer from a worse
forward convergence rate 1/poly(T)than the exp (−T)of variance preserving
(VP) based models, where Tis the forward diffusion time and the rate measures
the distance between forward marginal distribution qTand pure Gaussian noise.
The slow rate is due to the Brownian Motion without a drift term. In this work, we
design a new drifted VESDE forward process, which allows a faster exp (−T)for-
ward convergence rate. With this process, we achieve the first efficient polynomial
sample complexity for a series of VE-based models with reverse SDE under the
manifold hypothesis. Furthermore, unlike previous works, we allow the diffusion
coefficient to be unbounded instead of a constant, which is closer to the SOTA mod-
els. Besides the reverse SDE, the other common reverse process is the probability
flow ODE (PFODE) process, which is deterministic and enjoys faster sample speed.
To deepen the understanding of VE-based models, we consider a more general
setting considering reverse SDE and PFODE simultaneously, propose a unified
tangent-based analysis framework, and prove the first quantitative convergence
guarantee for SOTA VE-based models with reverse PFODE. We also show that the
drifted VESDE can balance different error terms and improve generated samples
without training through synthetic and real-world experiments.
1 Introduction
Recently, diffusion modeling has shown impressive performance in different areas [Ho et al., 2022,
Rombach et al., 2022, Esser et al., 2024, Li et al., 2024]. Diffusion models consist of two processes:
the forward and reverse process. The forward process gradually converts data q0to Gaussian noise,
which can be described by an intermediate marginal distribution sequence {qt}t∈[0,T]. The reverse
process sequentially predicts noise and removes it from data to generate samples.
There are two common forward processes: (1) Variance preserving (VP) SDE and (2) variance
exploding (VE) SDE. The VPSDE corresponds to an Ornstein-Uhlenbeck process, and the stationary
distribution is N(0,I). The VESDE has an exploding variance in the forward process. In earlier
∗Corresponding author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).times, VP-based models [Ho et al., 2020, Lu et al., 2022] provide an important boost for developing
diffusion models. Recently, VE-based models have shown the ability to generate data distribution
supported on low-dimensional manifolds [Song and Ermon, 2019, 2020]. Since image and text
datasets typically exhibit a low-dimensional manifold nature [Pope et al., 2021, Tang and Yang,
2024], VE-based models have achieved great performance in image generation, one-step generation,
and reinforcement learning [Teng et al., 2023, Song et al., 2023, Ding and Jin, 2023]. Furthermore,
Karras et al. [2022] unify VP and VESDE and prove that the ODE solution trajectory of a specific
VESDE is linear and directly towards the data manifold, which makes the denoise process easy.
After determining a forward SDE, diffusion models reverse it and generate samples by running
the corresponding reverse process. Since the reverse drift term contains the gradient of forward
logarithmic density ∇logqt(a.k.a. score function), we estimate it by using the score matching
technique [Vincent, 2011]. After that, diffusion models discretize the continuous reverse process and
run this discrete process starting from pure Gaussian. There are two widely used reverse processes:
reverse SDE [Ho et al., 2020] and probability flow ODE (PFODE) [Song et al., 2020a]. The reverse
SDE usually generates higher quality samples [Kim et al., 2022]. The reverse PFODE always has
a faster generation speed and is useful in other aspects such as calculating likelihoods [Song et al.,
2020b] or obtaining one-step generation models [Song et al., 2023]. Hence, these processes are both
critical, and providing the guarantee for VE-based models with these processes is necessary.
Recently, many works analyze the convergence guarantee of the VP-based diffusion models under the
reverse SDE setting and prove that the VP-based models can sample from the target data distribution
with polynomial complexity [Chen et al., 2023c,a,b, Lee et al., 2023, Benton et al., 2023]. As the
first step of this work, we also analyze VE-based models under the reverse SDE setting. Different
from the VP-based models, only a few works consider VE-based models and all of them suffer from
slow1/Poly(T)forward convergence rate [Lee et al., 2022, Gao et al., 2023, Gao and Zhu, 2024],
which is worse than exp(−T)one for VPSDE. A slow forward convergence rate makes a large
distance between qTand pure Gaussian noise, which leads to a large reverse beginning error. From
the theoretical perspective, this error introduces hardness to balance three error sources, as shown in
Section 5. From the empirical perspective, Lin et al. [2024] show that this error introduces a data
information leakage problem, which leads to bad performance. To deal with this problem, De Bortoli
et al. [2021] introduce a small drift term to obtain a exp (−√
T)reverse beginning error. However,
they introduce an additional exp (T)in the discretization error term. Furthermore, their results do not
allow unbounded βt, which is the key point of the optimal solution trajectory and used by the SOTA
models [Karras et al., 2022, Song et al., 2023]. Therefore, the following question remains open:
Is it possible to design a VESDE with a faster forward convergence rate than 1/poly(T)and achieve
the polynomial sample complexity when the diffusion coefficient is unbounded?
In this work, for the first time, we propose a new drifted VESDE forward process, which enjoys a
faster forward convergence rate and allows unbounded coefficients. We first show that the drifted
VESDE has similar trends but performs better than the original SOTA VESDE on synthetic data
(Section 7). After that, we analyze the sample complexity of drifted VESDE under the realistic
manifold hypothesis. The manifold hypothesis means the data q0is supported on a lower dimensional
compact set M, and much empirical evidence shows that image and text dataset satisfy this hypothesis
[Fefferman et al., 2016, Pope et al., 2021, Tang and Yang, 2024]. Furthermore, as shown in Section 2,
the manifold hypothesis is more realistic than previous data assumptions since it allows the blow-up
phenomenon of the score function at the end of the reverse process, which matches the empirical
observation [Kim et al., 2021]. Under the manifold hypothesis, we prove that the drifted VESDE with
a suitable larger βtbalances the reverse beginning, discretization, and approximated score errors and
achieves the first efficient polynomial sample complexity for VE-based models with reverse SDE.
To better understand VE-based models, we analyze reverse SDE and PFODE simultaneously after
obtaining polynomial complexity for reverse SDE. Despite the great performance, a few theoretical
works consider reverse PFODE [Chen et al., 2023d,b, Gao and Zhu, 2024], and these works either
focus on VPSDE or have strong assumptions. Hence, we propose the tangent-based framework for
VE-based models and achieve the first quantitative convergence for the SOTA VE-based models with
reverse PFODE. In conclusion, we accomplish the following results under the manifold hypothesis:
1.We propose a new drifted VESDE forward process, which allows exp (−T)forward conver-
gence guarantee with suitable βt. With this process, we achieve the first polynomial sample
complexity for a series of VE-based models under the reverse SDE setting.
22.When considering the general setting, we propose the tangent-based unified framework and
analyze reverse SDE and PFODE simultaneously. Under this framework, we prove the first
quantitative guarantee for SOTA VE-based models with reverse PFODE.
3.We show that the drifted VESDE balances different error terms and improves generated
samples without training via synthetic and real-world experiments.
2 Related Work
Before providing current results, we first discuss different assumptions about the data distribution
from strong to weak. The strongest assumption is the log-concave distribution. While the log-Sobelev
inequality (LSI) assumption is slightly weaker, it does not allow the presence of substantial non-
convexity, which is far away from the multi-modal distribution. Recently, some works assume the
score function is L-Lipschitz to allow the multi-modal distribution. However, this assumption can
not explain the blow-up phenomenon of score [Kim et al., 2021]. The last assumption is the manifold
hypothesis, which is supported by much empirical evidence and allows the blow-up score.
Analyses for VP-based models. For the reverse SDE, Lee et al. [2022] achieve the first polynomial
complexity with strong LSI assumption. Chen et al. [2023c] remove the LSI assumption, assume the
Lipschitz score and achieve polynomial complexity. Bortoli [2022] is the first work to focus on the
sample complexity of diffusion models under the manifold hypothesis, and it is the most relevant
work to our unified framework. However, as discussed in Section 6.1, the original tangent-based
lemma can not deal with reverse PFODE even in VPSDE. We carefully control the tangent process
to avoid additional exp (T)by using the exploding variance property of VESDE. Recently, Chen
et al. [2023a] and Benton et al. [2023] also remove the Lipschitz score assumption, and Benton et al.
[2023] achieve optimal dependence on d. More recently, Conforti et al. [2023] use bounded Fisher
information assumption and replace dwith a Fisher information term.
For the PFODE, Chen et al. [2023d] propose the first quantitative result with exponential dependence
on the Lipschitz constant. Chen et al. [2023b] achieve polynomial complexity by introducing a
corrector component to inject suitable noise. More recently, Li et al. [2023] remove the additional
corrector. However, their results rely heavily on the very specific βt, which goes to 0asT→+∞.
Since VE-based models have an unbounded βt, this method is not suitable for our models.
Analyses for VE-based models. When considering VESDE, most works focus on constant βt
and reverse SDE. De Bortoli et al. [2021] provide the first convergence guarantee with exponential
dependence on T. Lee et al. [2022] analyze a constant diffusion coefficient VESDE and achieve
polynomial sample complexity under the LSI assumption. When considering the reverse PFODE,
Chen et al. [2023d] only consider the discretization error and provide a quantitative convergence
guarantee. However, their results introduce additional exp (T)compared to ours (Section 6.1).
Recently, Gao et al. [2023] and Gao and Zhu [2024] provide the polynomial results for a series of
VESDE with reverse SDE and reverse PFODE under the log-concave assumption, respectively.
3 The Drifted Variance Exploding (VE) SDE
Diffusion models usually consist of a forward process and a reverse process. The forward process
gradually injects noise to convert the data distribution to pure noise. To generate samples, diffusion
models reverse the forward process and run the corresponding reverse process.
This section first recalls two previous forward processes: VPSDE and VESDE. Recently, the VE-
based models achieve great performance in application [Karras et al., 2022, Song et al., 2023].
However, unlike the widely analyzed VP-based models [Benton et al., 2023, Chen et al., 2023b], the
VE-based models suffer from challenges in obtaining an efficient sample complexity due to the slow
forward convergence rate. To address this limitation, we introduce a new drifted VESDE forward
process, which has a faster forward convergence rate, balances different error terms and achieves the
first efficient polynomial sample complexity (see Section 5). Finally, we introduce how to reverse
this new forward process and obtain an implementable algorithm.
3.1 The VP and VESDE of Diffusion Models
We first introduce the general form of the forward process and then recall two common forward
processes, VPSDE and VESDE, adopted in previous works [Ho et al., 2020, Karras et al., 2022]. Let
3q0be the data distribution. Given X0∈Rd, the forward process is defined by
dXt=f(Xt, t) dt+g(t) dBt,X0∼q0,
where (Bt)t≥0is the standard Brownian motion in Rd,f(Xt, t)is a drift coefficient and g(t)is a
diffusion coefficient. Let qtbe the density function of Xtat time t. With a suitable choice of drift and
diffusion terms (e.g. Section 3.1 and 3.2), the forward process gradually converts the data distribution
into Gaussian noise. More specifically, the conditional distribution Xt|X0is exactly N(mtX0, σ2
tI)
given X0, where mtis determined by the drift term and σ2
tis determined by the diffusion term.
The VPSDE forward process. Let{βt}t≥0be a non-decreasing sequence with bounded range
[1/¯β,¯β]. The VPSDE has the following formula:
dXt=−βtXtdt+p
2βtdBt,where X0∼q0. (1)
In this case, mt= exp( −Rt
0βsds)andσ2
t= 1−m2
t. Note that mT≤exp (−T/¯β),which indicates
a fast forward convergence rate TV(qT|N(0,I))≤exp (−T/¯β)[Chen et al., 2023c].
The VESDE forward process. The VESDE forward process is defined without a drift term:
dXt=q
dσ2
t/dtdBt,where X0∼q0. (2)
Two common choices for σ2
taretandt2, with the latter achieving SOTA performance [Karras et al.,
2022, Teng et al., 2023]. However, VESDE only has a slow polynomial-decay forward convergence
rate (Theorem 4.2), which introduces hardness to obtain an efficient sample complexity (see Section 5).
This motivates us to design an improved VESDE process with a fast forward convergence rate.
3.2 The Drifted VESDE Forward Process
Note that the forward convergence rate of the general process is upper bounded by mT/σ2
T(Theo-
rem 4.2). In practical applications [Ho et al., 2020, Karras et al., 2022, Song et al., 2023], the variance
of the forward process σ2
tat time T, which is determined by the diffusion term, does not exceed T2.
This indicates the contribution of σ2
Tto the forward convergence rate is only 1/Poly(T). Hence, the
exponential-decay forward convergence rate of VPSDE comes from the drift term, which introduces
an exponential-decay mt≤exp (−T/¯β). Due to the absence of the drift term in VESDE, the data
information, such as expectation E[q0]and covariance Cov[q0], does not decay and mt≡1, which is
a key to an only polynomial-decay forward convergence rate mT/σ2
T≤1/Poly(T). With the drift
term, the VPSDE gradually removes the data information from qtduring the process, which makes
qtquickly converge to pure Gaussian noise. Inspired by this elimination effect of the drift term, we
propose a drifted VESDE forward process:
dXt=−1
τβtXtdt+p
2βtdBt,X0∼q0, (3)
where τ∈[T, T2]is the coefficient used to balance the drift and diffusion term2, and{βt}t≥0is a
positive non-decreasing sequence. In this case,
mt= exp
−Zt
0βs/τds
andσ2
t=τ 
1−m2
t
. (4)
We show that the drifted VESDE is not only an effective representation of the existing VESDE but
also extends beyond it (see Section 7 and Appendix A.1). We also prove that this process with suitable
βthas a exp (−T)forward convergence rate and enjoys an efficient polynomial sample complexity.
3.3 The Reverse Process of the Drifted VESDE
To generate samples from Gaussian noise, a diffusion model reverses the forward process. Let qτ
tbe
the density function of the drifted VESDE forward process at time tand(Yt)t∈[0,T]= (XT−t)t∈[0,T].
As shown in Cattiaux et al. [2021], the reverse process of drifted VESDE has the following form3:
dYt=βT−t
Yt/τ+ (1 + η2)∇logqτ
T−t(Yt)	
dt+ηp
2βT−tdBt. (5)
2We note that the choice τ∈[T, T2]is used to guarantee the exploding variance of the forward process. In
fact, our drifted VESDE is a general formula that covers the VP and VESDE by choosing τ∈[1, T2]. More
details are shown in Section 5.
3Now(Bt)t≥0is the reversed Brownian motion, and we abuse this notation here for ease of notation.
4The parameter η∈[0,1]is used to determine the type of reverse processes. There are two common
reverse processes in the application: reverse probability flow ODE (PFODE) ( η= 0) [Song et al.,
2020b, 2023] and reverse SDE ( η= 1) [Ho et al., 2020].
To generate distribution q0through running the above reverse process, diffusion models need the
true score function ∇logqτ
T−t(Yt)and the accurate reverse beginning distribution qτ
T. However,
∇logqτ
T−t(Yt)andqτ
Tcontain the data information and usually can not be exactly calculated. For the
score function, diffusion models approximate it using a score network s(T−t,·)by minimizing the
score matching objective function [Vincent, 2011]. For the initial distribution of the reverse process,
since qτ
Tshould be close to a pure Gaussian, we choose qτ
∞=N(0, σ2
TI)as an approximation. Then,
the continuous reverse process (bYt)t∈[0,T], incorporating s(T−t,·)andqτ
∞, is defined as:
dbYt=βT−tbYt/τ+ (1 + η2)s(T−t,bYt)	
dt+ηp
2βT−tdBt,wherebY0∼qτ
∞.
Since diffusion models can not run a continuous process due to the nonlinear score function, these
models usually discretize the above continuous process and freeze the approximated score at the
beginning of each interval. Let {γk}k∈[K]be the stepsize and tk+1=Pk
j=0γj. As shown in Kim
et al. [2021], ∇logqτ
T−t(Yt)goes to +∞at the end of the reverse process. To mitigate this issue,
they use the early stopping technique tK=T−δ, and we also employ this technique in this work.
With the stepsize, we choose the exponential integrator discretization scheme [Zhang and Chen,
2022] to discretize the above process, which runs the following process:
deYt=βT−teYt/τ+ (1 + η2)s(T−tk,eYtk)	
dt+ηp
2βT−tdBt,where t∈[tk, tk+1].(6)
As shown in Karras et al. [2022], the choice of βtsignificantly affects the performance of models,
and we need to determine βtbefore running the reverse process. The state-of-the-art diffusion models
adopt βt=t, which increases rapidly and has an unbounded range. However, current theoretical
works assume βtto be a constant [Chen et al., 2023c] or confined to a bounded interval [1/¯β,¯β]
[Bortoli, 2022] to match the setting of VPSDE. To align more closely with practical applications
of VE-based models, we allow an unbounded βtin this work. Furthermore, we make a detailed
assumption on βtwhen considering different reverse processes.
Assumption 3.1. Let{βt}t≥0be a positive, non-decreasing sequence. For any τ∈[T, T2], there
exists constants ¯βandC, such that for any t∈[0, T]: (1) for η= 0, then 1/¯β≤βt≤max{¯β, t}
andRT
0βt/τdt≤C; (2) for η= 1, then 1/¯β≤βt≤max{¯β, t2}.
As shown in Chen et al. [2023b], due to the absence of the stochasticity, the small errors for quickly
accumulate and are magnified. Hence, we assume a conservative βtfor the reverse PFODE, whose
growth rate is at most t, to avoid an additional exp (T)in the convergence guarantee (see Section 6.1).
We note that this choice of βtis satisfied in practical applications [Song et al., 2020b, Karras et al.,
2022]. For the reverse SDE setting, we assume the growth rate of βtcan depend on τinstead of at
most linear. For example, when τ=T2, we can choose βt=t2, which has the same order as τ. As
shown in Theorem 4.2, the drifted VESDE with aggressive βt=t2has an exponential-decay forward
convergence rate, which leads to the first efficient polynomial complexity for VE-based models.
Notations. Forx∈RdandA∈Rd×d, we denote by ∥x∥and∥A∥the Euclidean norm for vector
and the spectral norm for matrix. We denote by ¯γK=argmaxk∈{0,...,K−1}γkthe maximum stepsize
fork∈[0, K−1]. We denote by q0PTthe distribution of XT,Qqτ
∞
tKthe distribution of YtK,Rqτ
∞
K
the distribution of eYtKandQq0PT
tKthe distribution which does reverse process starting from qτ
T(Eq.
5). We denote by W1andW2the Wasserstein distance of order one and two, respectively.
4 The Faster Forward Convergence Rate for the Drifted VESDE
This section shows that the drifted VESDE has a fast forward convergence rate. Since qτ
Tcontains
the data information, we first introduce the manifold hypothesis before controlling TV(qτ
T, qτ
∞).
Assumption 4.1. q0is supported on a compact set Mand0∈ M .
We denote Rthe diameter of the manifold by R= sup{∥x−y∥:x, y∈ M} and assume R > 1.
As shown in Section 1, the manifold hypothesis is supported by much empirical evidence [Bengio
5et al., 2013, Fefferman et al., 2016, Pope et al., 2021] and allows the blow-up phenomenon of the
score. Recently, Tang and Yang [2024] show that diffusion models can adapt to the intrinsic manifold
structure. With Assumption 4.1, we obtain the forward process guarantee for the drifted VESDE.
Theorem 4.2. Assume Assumption 4.1 and 3.1. Let qτ
∞=N(0, σ2
TI). With mT, σTdefined in
Equation (4), we have TV(qτ
T, qτ
∞)≤√mT¯D/σ T, where ¯D=d|c|+E[q0] +Randcis the
eigenvalue of Cov [q0]with the largest absolute value.
Recall that mT= exp( −RT
0βt/τdt), the previous VESDE [Song et al., 2020b, Karras et al., 2022,
Lee et al., 2022] chooses a conservative βtsatisfiesRT
0βt/τdt≤C. However, with an aggressive βt,
the drifted VESDE will have a faster convergence rate. To illustrate the accelerated forward process,
we use τ=T2as an example and discuss different βt=tα1, α1∈[1,2]. Due to the definition of σT,
σT≈T, and the forward convergence rate mainly depends on√mT. When α1= 1is conservative,
mTis a constant, and the convergence rate is 1/T. When α1= 1 + ln(2 rln(T))/ln(T)is slightly
aggressive, the convergence rate is 1/Tr+1forr >0. When α1≥1 + ln( T−ln(T))/ln(T)is
aggressive, the convergence rate is faster than exp(−T). In our analysis, whether βtcan be aggressive
depends on the reverse process (see Section 6). When choosing an aggressive βt, the drifted VESDE
achieves an improved sample complexity compared with pure VESDE (see Section 5).
5 The Polynomial Complexity for a Series of VESDE with Reverse SDE
In this section, we first pay attention to the reverse SDE ( η= 1) to show the power of the drifted
VESDE. More specifically, we show that our general drifted VESDE form covers the current models
(VP and VESDE). After that, we show that drifted VESDE can go beyond the current models and
achieve an improved complexity with an aggressive βt. Since the objective function minimizes the
L2distance between the ground truth and the approximated score, we assume that the approximated
score is L2-accuracy, which is exactly the same with Chen et al. [2023c] and Benton et al. [2023].
Assumption 5.1. Eqtkhstk− ∇lnqτ
tk2i
≤ϵ2
score for∀k∈[K].
With this assumption, we provide a universe convergence guarantee for τ∈[1, T2]andβt∈[1, t2]
and discuss the sample complexity of VP and VE-based models in detail.
Theorem 5.2. Assume Assumption 3.1, 4.1, 5.1. Let ¯Ddefined in Theorem 4.2, ¯γK=
argmaxk∈{0,...,K−1}γk,τ=T2andβt∈[1, t2]. Then, we have that
TV
Rqτ
∞
K, qδ
≤¯D√mT
σT+R2√
d
σ4
δp
¯γKβTτT+ϵscorep
βTT .
To guarantee the above convergence guarantee smaller than eO(ϵTV), each component of the result
needs to be smaller than ϵTV. As shown in Remark 5.3, it is difficult for pure VESDE to balance the
approximated score and the first two error terms to achieve an efficient sample complexity. Hence, we
discuss how to balance the reverse beginning and discretization error. More specifically, we require
¯D√mT/σT≤ϵTVand¯γK≤σ8
δϵ2
TV/ 
R4dβTτT
, where the first inequality determines the order
ofTand the second inequality determines the stepsize ¯γK. After that, with sample complexity
K=T/¯γK, we have that TV(Rqτ
∞
K, qδ)≤eO(ϵTV). The last step is to guarantee qδandq0is close
enough W2
2(q0, qδ)≤ϵ2
W2, which requires σ2
δ≤ϵ2
W2/(d+R√
d).
Following the above process, this general convergence guarantee leads to the polynomial sample
complexity for VP and VE-based models. When βt= 1andτ= 1, the drifted VESDE becomes
VPSDE and achieve the complexity eO(1/(ϵ8
W2ϵ2
TV)), which achieve exactly the same order compared
with Chen et al. [2023c]. When βt= 1andτ=T, our formula is similar but slightly better (Figure 2
and 3) to pure VESDE ( σ2
t=t) and achieves O(1/ϵ8
W2ϵ8
TV)result. For βt=tandτ=T2, the
general formula is similar to SOTA pure VESDE ( σ2
t=t2) and achieves the first polynomial results
O(1/ϵ8
W2ϵ7
TV)for this model under the manifold hypothesis4.
Although we achieve the first polynomial sample complexity for VE-based models under the manifold
hypothesis, it is clear that the results of the VE-based models are significantly worse than the result
4We note these results still hold for pure VESDE with σ2
t=tandt2, and we use the general drifted VESDE
for simplicity. Here, we only consider the dependence of ϵ. Readers can find detailed results in Appendix C.1.
6of VP-based models since the slow 1/Poly(T)forward convergence guarantee. More specifically,
the forward convergence rate of VPSDE is exp (−T), which means Thas the order log(1/ϵTV)and
can be ignore. When considering the pure VESDE with σ2
t=t2, the forward convergence rate is
¯D/T , which indicates T≥¯D/ϵ TVis a polynomial term and can not be ignored. Hence, the results
K=R4dT5/(σ8
δϵ2
TV)of pure VESDE ( σ2
t=t2) is heavily influenced by T. When considering
pure VESDE with σ2
t=t, it suffers from a slower forward convergence guarantee ¯D/√
T, which
indicates T≥¯D2/ϵ2
TVandK=R4dT3/(σ8
δϵ2
TV). This is the first work to explain why pure
VESDE ( σ2
t=t2) performs better than pure VESDE ( σ2
t=t2) from a theoretical perspective.
Remark 5.3.In this part, we explain the reason why the pure VESDE fails to balance the reverse
beginning and the approximated score. We use the pure VESDE with σ2
t=t2as an example. Under
this setting, the guarantee has the form 1/T+p
¯γKT4/δ4+ϵscore√
T2, which requires T≥1/ϵTV.
Then, ϵscore√
T2is larger than ϵscore/p
ϵ2
TV. Hence, it is hard to achieve non-asymptotic results.
Drifted VESDE with an aggressive βtbalances different error terms. This part shows that
our drifted VESDE with a suitable aggressive βtcan balance the above three error terms. More
specifically, we show that introducing aggressive βtonly slightly affects the discretization error and
significantly benefits in balancing reverse beginning and approximated errors. As a result, we obtain
an efficient polynomial complexity for a series of VE-based models with unbounded βt.
Corollary 5.4. Following the setting of Theorem 5.2. When considering τ=T2, βt=t2, by
choosing δ≤ϵ2/3
W2
(d+R√
d)1/3,T≥2 ln(¯D/ϵTV),¯γK≤δ12ϵ2
TVln5(¯D/ϵ TV)/(R4d)and assuming
ϵscore≤eO(ϵTV),Rqτ
∞
Kis(ϵTV+ϵscore)close to qδ, which is ϵW2close to q0, with sample complexity
K≤eO 
dR4(d+R√
d)4
ϵ8
W2ϵ2
TV!
.
Defined by Rqτ
∞
K,R 0the output Rqτ
∞
Kprojected onto B (0, R0)forR0=eΘ(R). Then, we achieve pure
W2guarantee W2(Rqτ
∞
K,R 0, q0)≤ϵW2with sample complexity eO
dR8(d+R√
d)4
ϵ12
W2
.
Since our drifted VESDE with τ=T2andβt=t2has a fast forward convergence rate exp (−T)/T2,
Tbecomes a logarithmic term and does not influence the discretization term, which is the source of
the improved sample complexity. Furthermore, the requirement of ϵscore has the same order with ϵTV,
which indicates the drifted VESDE balances the reverse beginning and approximated score error. In
fact, we only require the forward convergence rate of drifted VESDE is exp (−T), which indicates
a series of VE-based models can achieve this sample complexity. We use τ=T2as an example.
When considering the βt=tα1, we require 2≥α1≥1 + ln( T−ln(T))/ln(T)to enjoy exp (−T)
forward convergence rate and achieve K≤eO 
1/(ϵ8
W2ϵ2
TV)
sample complexity. For βt=tand
τ=T, we also obtain complexity K≤eO 
1/ 
ϵ8
W2ϵ2
TV
(Appendix C.1).
Remark 5.5.Recently, Lee et al. [2022] and Gao et al. [2023] consider the sample complexity of
VESDE with reverse SDE under strong assumption. Lee et al. [2022] consider VESDE with σ2
t=t
and achieve ˜O(L2/ϵ4
TV)result under the LSI assumption. Under the manifold hypothesis, the result
is˜O(1/ϵ8
W2ϵ4
TV), which is worse than Corollary 5.4. Gao et al. [2023] achieve pure W2guarantee
eO(1/ϵ2.5
W2)under the log-concave distribution, which is even stronger than LSI assumption and ignore
the influence of δ. Hence, they ignore an additional 1/Poly(W2)(Detail in Appendix A.2).
6 The Tangent-based Analysis Framework
To deepen the understanding of VE-based models instead of the specific reverse process, we introduce
the unified framework for VESDE with reverse SDE and PFODE. Similar to previous PFODE work
[Chen et al., 2023d], we assume an accurate score and consider the other errors.
Theorem 6.1. Assume Assumption 3.1 and 4.1, δ≤1/32andγksupv∈[T−tk+1,T−tk]βv/σ2
v≤1/28
for∀k∈ {0, ..., K −1}. LetγK=δ. Then, for ∀τ∈[T, T2]:
7(1) If η= 1(the reverse SDE), choosing βt=t2,W1
Rqτ
∞
K, q0
is bounded by
(R
τ+√
d)√
δ+ expR2
2(¯β
δ3+1
τ) 
C1(τ)Tκ2
1(τ)
(¯β
δ3+1
τ)¯γ1/2
K+ 1
¯γ1/2
K+¯De−T/2
√τ!
,
where κ1(τ) =T2(1/τ+¯β/δ3)andC1(τ)is linear in τ2.
(2) If η= 0(PFODE), choosing a conservative βt(Assumption 3.1), W1
Rqτ
∞
K, q0
is bounded by
(R
τ+√
d)√
δ+ expR2
2(¯β
δ2+1
τ) 
C2(τ)κ2
2(τ)T
(¯β
δ2+1
τ)¯γ1/2
K+ 1
¯γ1/2
K+¯D√τ!
,
where κ2(τ) =T 
1/τ+¯β/δ2
andC2(τ)is linear in τ2.
Theorem 6.1 proves the first quantitative guarantee for VE-based models with reverse PFODE using
the unified tangent-based framework. Correspondingly, the Girsanov-based method [Chen et al.,
2023c,a] can not deal with reverse PFODE since the reverse process diffusion term is not well-defined.
Recently, Chen et al. [2023d] employ the Restoration-Degradation framework to analyze VESDE
with reverse PFODE, which also has exponential dependence on Randδ. Furthermore, their results
have exponential dependence on βt(gmaxin Chen et al. [2023d]), which corresponds to τ. However,
our dependence on τappears in the polynomial term. Hence, our framework is a suitable unified
framework. Furthermore, we emphasize that our tangent-based unified framework is not a simple
extension of Bortoli [2022]. We carefully control the tangent process according to the variance
exploding property of VESDE to avoid exp (T)term when considering PFODE (Section 6.1).
Theorem 6.1 has exponential dependence on Randδ, which is introduced by the tangent process.
Similar to Bortoli [2022], if we assume the Hessian∇2logqt(xt)≤Γ/σ2
t, we obtain a better con-
trol on the tangent process and replace the exponential dependence on δby a polynomial dependence
onδand exponential dependence on Γwhen considering reverse PFODE.
Corollary 6.2. Assume Assumption 3.1, 4.1 and∇2logqt(xt)≤Γ/σ2
t. Let η= 0 (reverse
PFODE), δ∈(0,1/32), τ=T2,βt=tandκ2(τ), C2(τ)defined in Theorem 6.1, we have
W1
Rqτ
∞
K, q0
≤(R
τ+√
d)√
δ+¯βΓ
2
δΓexpΓ + 2
2 
C2(τ)κ2
2(τ)T((¯β
δ2+1
τ)¯γ1/2
K+ 1)¯γ1/2
K+¯D√τ!
.
Though the additional assumption is strong, many special cases, such as hypercube M=
[−1/2,1/2]psatisfy this assumption. We emphasize that our analysis also holds for VESDE ( σ2
t=t2)
with reverse PFODE, which means our results can explain the SOTA model in Karras et al. [2022].
6.1 The Discussion on the Unified Framework
In this section, we introduce the unified tangent-based framework for reverse SDE and PFODE and
discuss key steps to achieve the quantitative guarantee for PFODE. Firstly, we decompose the goal
W1
Rqτ
∞
K, q0
into three terms: W1
Rqτ
∞
K, Qqτ
∞
tK
+W1
Qqτ
∞
tK, Qq0PT
tK
+W1
Qq0PT
tK, q0
.
These terms correspond to the discretization scheme, reverse beginning distribution, and the early
stopping parameter δ. We focus on most difficult discretization term and first recall the stochastic
flow of the reverse process for any x∈Rdands, t∈[0, T]witht≥s:
dYx
s,t=βT−t
Yx
s,t/τ+ 
1 +η2
∇logqT−t 
Yx
s,t	
dt+ηp
2βT−tdBt,where Yx
s,s=x .
With∇Yx
s,s=I, the corresponding tangent process is
d∇Yx
s,t=βT−t∇Yx
s,t/τdt+βT−t 
1 +η2
∇2logqT−t(Yx
s,t)∇Yx
s,tdt .
The key of the discretization error is to bound tangent process∇Yx
s,tK. For this term, we consider
the reverse SDE and PFODE simultaneously and propose a general version of Bortoli [2022].
Lemma 6.3. Assume Assumption 3.1 and 4.1. For ∀s∈[0, tK]andx∈Rd, we have
∥∇Yx
s,tK∥ ≤expR2
2σ2
T−tK+(1−η2)
2ZtK
0βT−u
τdu
.
8(a) Original Figure
 (b) VESDE ( 𝑇=100)
 (c) Drifted VESDE ( 𝑇=100)Figure 1: Experiment results of Swiss roll with Euler Maruyama Method (Reverse SDE)
We emphasize that the general bound for the tangent process is the key to achieving the guarantee for
VESDE with the reverse ODE. Recall that in the original lemma for the tangent processes, since τ
is independent of Tandβtis bounded in a small interval [1/¯β,¯β],RtK
0βT−u/τdu= Θ( T), which
means there is an additional exp (T)when considering VPSDE with revere PFODE. However, our
tangent-based lemma makes use of the variance exploding property of VESDE to guarantee thatRT
0βt/τdt≤Cwith a conservative βt=twhen considering reverse PFODE. When η= 1, we
choose aggressive βt=t2since the choice of βtdoes not affect the bound of the tangent process.
For the early stopping term, it corresponds to δand is smaller than 2(R/τ+√
d)√
δ. Since we can
not use the data processing inequality in Wasserstein distance, the reverse beginning terms consists of
the bound of tangent process term and the forward process term:
W1
Qqτ
∞
tK, Qq0PT
tK
≤√mT¯D
σTexpR2
2σ2
T−tK+(1−η2)
2ZtK
0βT−u
τdu
.
One notable future work is introducing the short regularization technique [Chen et al., 2023b] and
suitable corrector to remove the above exponential dependence.
7 Experiments
In this section, we show the power of the drifted VESDE forward process through experiments.
Section 7.1 shows that aggressive one achieves good balance in different error terms. After that, we
consider the approximated score and show that the conservative one can improve the quality of the
generated distribution without training in the synethetic and real-world setting.
7.1 The Aggressive Drifted VESDE Balances Errors
In this section, we do experiments on 2-D Gaussian to show that the aggressive drifted VESDE
balances different errors. Since the ground truth score of the Gaussian can be directly calculated, we
use the accurate score function to discuss the balance between the other two error terms clearly. We
show how to use approximated score in Section 7.2.
Figure 2: Results of 2-D GaussianAs shown in Figure 2, the process with aggres-
siveβt=t2achieves the best and second per-
formance in EI and EM discretization, which
supports our theoretical result (Corollary 5.4).
The third best process is conservative βt=t
with the small drift term. The reason is that
though it can not achieve a exp (−T)forward
guarantee, it also has a constant decay on prior
information, which slightly reduces the effect of
the reverse beginning error (Section 3.1). The
worst process is pure VESDE since it is hard to
balance different error sources. Our experimen-
tal results also show that EI is better than EM.
9(a) Pure VESDE
(b) Drifted VESDEFigure 3: Experiment results of CelebA dataset
7.2 The Conservative Drifted VESDE Benefits from VESDE without Training
As shown in Figure 2, the red and orange lines have similar trends. Hence, for conservative drifted
VESDE, which satisfies (2) of Assumption 3.1, we can directly use the models trained by pure
VESDE to improve the quality of generated distribution. We confirm our intuition by training the
model with pure VESDE with σ2
t=tand directly use the models to conservative drifted VESDE
withβt= 1andτ=T. From the experimental results (Figure 1), it is clear that pure VESDE has a
low density on the Swiss roll except for the center one, which indicates pure VESDE can not deal
with large dataset variance, as we discuss in Section 4. For conservative drift VESDE, as we discuss
in the above section, it can reduce the influence of the dataset information. Figure 1 (c) supports our
augmentation and shows that the density of the generated distribution is more uniform compared to
pure VESDE, which means that the drift VESDE can deal with large dataset mean and variance.
Beyond the synthetic data, we show that our conservative drifted VESDE can improve the generated
images of pure VESDE without training on the real-world CelebA256 dataset. From the qualitative
perspective, as shown in Figure 3, the images generated by our drifted VESDE have more detail (such
as hair and beard details). On the contrary, since pure VESDE can not deal with large variance, the
images generated by pure VESDE appear blurry and unrealistic in these details. From the quantitative
results, we use aesthetic score [Schuhmann et al., 2022] and Inception Score to measure the quality
of generated images. Our drifted VESDE achieves aesthetic score 5.813, and IS 4.174, which is
better than the results of baseline pure VESDE (aesthetic score 5.807and IS 4.082). There are more
examples on CelebA256 and more experiments on Swiss roll and 1D-GMM to explore different
sampling methods (RK45, PFODE) and different T. We refer to Appendix G for more details.
8 Conclusion
In this work, we analyze the VE-based models under the manifold hypothesis. Firstly, we propose a
new forward drifted VESDE process, which enjoys a faster forward convergence rate. Then, we show
that with an aggressive βt, the new process balances different errors and achieve the first efficient
polynomial sample complexity for a series of VE-based models with reverse SDE.
After achieving the above results, we go beyond the reverse SDE and propose the tangent-based unified
framework, which considers reverse SDE and PFODE at the same time. Under this framework, we
make use of the variance exploding property of VESDE and achieve the first quantitative convergence
guarantee for SOTA VE-based models with reverse PFODE. Finally, we show the power of the new
drifted forward process through synthetic and real-world experiments.
Future Work and Limitation. This work proposes the first unified framework for VE-based models
with an accurate score. After that, we plan to consider the approximated score error and provide a
polynomial complexity for the VE-based models with reverse PFODE under the manifold hypothesis.
Broader Impact. Our work focuses on the convergence guarantee of the SOTA diffusion models and
deepens the understanding of diffusion models. Therefore, this work can be viewed as a fundamental
step in improving the quality of diffusion models and the societal impact is similar to general
generative models [Mirsky and Lee, 2021].
10Acknowledgments and Disclosure of Funding
The author Bo Jiang is supported by National Natural Science Foundation of China (62072302).
References
Vladimir M Alekseev. An estimate for the perturbations of the solutions of ordinary differential equations. Vestn.
Mosk. Univ. Ser. I. Math. Mekh , 2:28–36, 1961.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence , 35(8):1798–1828, 2013.
Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear convergence bounds for
diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686 , 2023.
Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Trans. Mach.
Learn. Res. , 2022, 2022. URL https://openreview.net/forum?id=MhK5aXo3gB .
Patrick Cattiaux, Giovanni Conforti, Ivan Gentil, and Christian Léonard. Time reversal of diffusion processes
under a finite entropy condition. arXiv preprint arXiv:2104.07708 , 2021.
Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-
friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning ,
pages 4735–4763. PMLR, 2023a.
Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ode is
provably fast. arXiv preprint arXiv:2305.11798 , 2023b.
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the
score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference
on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023c. URL
https://openreview.net/pdf?id=zyLVMgsZ0U_ .
Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-degradation beyond linear diffusions: A non-
asymptotic analysis for ddim-type samplers. In International Conference on Machine Learning , pages
4462–4484. PMLR, 2023d.
Giovanni Conforti, Alain Durmus, and Marta Gentiloni Silveri. Score diffusion models without early stopping:
finite fisher information is all you need. arXiv preprint arXiv:2308.12240 , 2023.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrödinger bridge with
applications to score-based generative modeling. Advances in Neural Information Processing Systems , 34:
17695–17709, 2021.
Pierre Del Moral and Sumeetpal Sidhu Singh. Backward itô–ventzell and stochastic interpolation formulae.
Stochastic Processes and their Applications , 154:197–250, 2022.
Zihan Ding and Chi Jin. Consistency models as a rich and efficient policy class for reinforcement learning.
arXiv preprint arXiv:2309.16984 , 2023.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik
Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image
synthesis. arXiv preprint arXiv:2403.03206 , 2024.
Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis. Journal of the
American Mathematical Society , 29(4):983–1049, 2016.
Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow odes of diffusion models in
wasserstein distances. arXiv preprint arXiv:2401.17958 , 2024.
Xuefeng Gao, Hoang M Nguyen, and Lingjiong Zhu. Wasserstein convergence guarantees for a general class of
score-based generative models. arXiv preprint arXiv:2311.11003 , 2023.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems , 33:6840–6851, 2020.
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video
diffusion models. arXiv preprint arXiv:2204.03458 , 2022.
11Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based
generative models. Advances in Neural Information Processing Systems , 35:26565–26577, 2022.
Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: A universal
training technique of score-based diffusion model for high precision score estimation. arXiv preprint
arXiv:2106.05527 , 2021.
Dongjun Kim, Yeongmin Kim, Wanmo Kang, and Il-Chul Moon. Refining generative process with discriminator
guidance in score-based diffusion models. arXiv preprint arXiv:2211.17091 , 2022.
Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Fp-
diffusion: Improving score-based diffusion models by enforcing the underlying score fokker-planck equation.
In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , volume 202 of Proceedings of Machine Learning Research , pages 18365–18398. PMLR, 2023.
Jean-François Le Gall. Brownian motion, martingales, and stochastic calculus . Springer, 2016.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial
complexity. Advances in Neural Information Processing Systems , 35:22870–22882, 2022.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data
distributions. In International Conference on Algorithmic Learning Theory , pages 946–985. PMLR, 2023.
Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for diffusion-based
generative models. arXiv preprint arXiv:2306.09251 , 2023.
Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, and Pengyuan Zhou.
Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling. arXiv preprint
arXiv:2404.03575 , 2024.
Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps
are flawed. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages
5404–5411, 2024.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for
diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927 , 2022.
Yisroel Mirsky and Wenke Lee. The creation and detection of deepfakes: A survey. ACM Computing Surveys
(CSUR) , 54(1):1–41, 2021.
Jakiw Pidstrigach. Score-based generative models detect manifolds. arXiv preprint arXiv:2206.01018 , 2022.
Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of
images and its impact on learning. arXiv preprint arXiv:2104.08894 , 2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 10684–10695, 2022.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset
for training next generation image-text models. Advances in Neural Information Processing Systems , 35:
25278–25294, 2022.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020a.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances
in neural information processing systems , 32, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in
neural information processing systems , 33:12438–12448, 2020.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 ,
2020b.
12Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Andreas Krause, Emma
Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International
Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of
Proceedings of Machine Learning Research , pages 32211–32252. PMLR, 2023.
Rong Tang and Yun Yang. Adaptivity of diffusion models to manifold structures. In International Conference
on Artificial Intelligence and Statistics , pages 1648–1656. PMLR, 2024.
Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay
diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350 ,
2023.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation , 23(7):
1661–1674, 2011.
Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv
preprint arXiv:2204.13902 , 2022.
13Appendix
A More Discussion on Drifted VESDE and Current Works
A.1 The Drifted VESDE is Representative Enough.
In this work, we consider τ∈[T, T2]and show that this choice is enough to represent the current
VESDE. More specifically, since VESDE with σ2
t=thasqT=N(E[q0],Cov[q0] +TI)and
drifted VESDE with τ=T2andβt≡1/2hasqτ
T=N(exp (−1
2T)E[q0],exp (−1
T)Cov[q0] + (1−
exp (−1
T))T2I), these two setting is almost identical when T→+∞. The second choice of VESDE
σ2
t=t2, which achieves the state-of-the-art performance [Karras et al., 2022], is almost identical
toτ=T2,βt=t. The simulation experiments also show that VESDE and drifted VESDE with
specific βtandτhave similar performance (Figure 2).
A.2 The Detailed Calculation of Previous work
The results of Lee et al. [2022] Lee et al. [2022] consider VESDE ( σ2
t=t) with reverse SDE
under the LSI assumption with parameter CLS. The LSI assumption does not allow the presence of
substantial non-convexity and is far away from the multi-modal real-world distribution. Furthermore,
they use unrealistic assumption ϵscore≤1/(CLS+T)to avoid the effect of the approximated score,
which is stronger than Assumption 5.1. Under the above strong assumption, Lee et al. [2022] achieve
the polynomial sample complexity ˜O(L2d(d|c|+R)2/ϵ4
TV). Under the manifold hypothesis, by
Lemma E.2, we know that
L=R2d2/ϵ4
W2
Then, the result is ˜O(R4d5(d|c|+R)2/ϵ8
W2ϵ4
TV), which is worse than Corollary 5.4.
The results of Gao et al. [2023]. Gao et al. [2023] analyze a series of VESDE with reverse SDE and
achieve 1/ϵ2.5
W2sample compelxity for VESDE with σ2
t=t2in 2-Wasserstein distance. However, they
assume the data distribution is log-concave, which is even stronger than LSI assumption. Furthermore,
under this assumption, ∇logqT−t(·)do not blow-up at the end of the reverse process, which do not
match the empirical phenomenon and ignore the influence of early stopping parameters. To transfer
their results to our the results under the manifold hypothesis, we need to consider the influence of δ,
which would introduce an additional 1/Poly(W2)term.
B The Proof for the Faster Forward Process
Lemma B.1. The minimization problem min ¯mt,VtKL (qτ
t| N( ¯mt, Vt))is minimized by ¯mt=
mtE[q0]andVt=m2
tCov [q0] +σ2
tI, where mtandσtdefined in Equation (4).
Proof .For simplicity, we denote the mean and covariance of q0byaandC′. We also define the
optimize variable nt=N( ¯mt, Ct). We can directly compute the KL divergence KL(qt|nt):
KL (qt|nt) =−H(qt)−Z
log (nt(x))qt(x)dx
=−H(qt) +d
2log(2π) +1
2log (det ( Vt)) +1
2Z
(x−¯mt)TV−1
t(x−¯mt)qt(x)dx .
For the last term, we directly compute
Z
(x−¯mt)TV−1
t(x−¯mt)pt(x)dx
=Eh
(Xt−¯mt)TV−1
t(Xt−¯mt)i
=Eh
(mtX0+σtZ−¯mt)TV−1
t(mtX0+σtZ−¯mt)i
=Eh
m2
t(X0−a)TV−1
t(X0−a)i
+ (mta−¯mt)TV−1
t(mta−¯mt) +σ2
tE
ZTV−1
tZ
=m2
ttr 
C′V−1
t
+σ2
ttr 
V−1
t
+ (mta−¯mt)TV−1
t(mta−¯mt),
14where the second inequality follows that Xt=mtX0+σtZ. It is clear that the optimal solution of
¯mtismta. In the next step, we focus on the optimization problem for Vt:
L 
V−1
t
= log (det ( Vt)) + tr  
m2
tC′+σ2
tI
V−1
t
=−log 
det 
V−1
t
+ tr  
m2
tC′+σ2
tI
V−1
t
.
Since the above optimization is a convex optimization problem, we use the method similar to
Pidstrigach [2022], we obtain that the optimal solution of Vtism2
tC′+σ2
tI. ■
Lemma B.2. Let¯mtandVtbe the optimal mean and covariance operator from Lemma B.1. Then
KL (qt|N( ¯mt, Vt))≤1
2log Qd
i=1 
m2
tci+σ2
t
(σ2
t)d!
+R2mt
σ2
t
≤dm2
tc
2σ2
t+R2mt
σ2
t+o(m2
tc
σ2
t),
KL 
N( ¯mt, Vt)|(N(0, σ2
t)
≤m2
tPd
i=1ci
2σ2
t+m2
t(E[q0])2
2σ2
t+1
2log 
(σ2
t)d
Qd
i=1(m2
tci+σ2
t)!
≤m2
tPd
i=1ci
2σ2
t+m2
t(E[q0])2
2σ2
t+dm2
tc
2σ2
t+o(m2
tc
σ2
t),
where ciare the eigenvalues of Cov [q0], and cis the eigenvalue with the largest absolute value.
Proof .Fort≥0, we directly calculate the KL divergence for this term:
KL(qt| N( ¯mt, Vt)) =−H(qt) +1
2log (det (2 πVt)) +1
2tr  
m2
tC′+σ2
tI
V−1
t
=−H(qt) +1
2log (det (2 πVt)) +d
2
=−H(qt) +d
2log(2π) +1
2log dY
i=1 
m2
tci+σ2
t!
+d
2,
where ciare the eigenvalues of Cov [q0]. Now, we only need to calculate H(qt):
−H(qt) =EXt[logqt(Xt)] =EXt
log
EX0
(2πσ2
t)−d/2exp
−1
2σ2
t∥Xt−X0∥2
.
By Assumption 4.1, it is clear that
exp
−1
2σ2
t∥Xt−X0∥2
≤exp
−1
2σ2
t
∥Xt∥2+ 2⟨Xt, X0⟩
.
Then, we know that
E
log
EX0
(2πσ2
t)−d/2exp
−1
2σ2
t∥Xt−X0∥2
≤E
log
(2πσ2
t)−d/2
−1
2σ2
t
∥Xt∥2+ 2⟨Xt, X0⟩
≤ −d
2log(2π)−1
2log 
(σ2
t)d
−1
2σ2
tEh
∥Xt∥2i
+R2mt
σ2
t.
we also know that
Eh
∥Xt∥2i
=m2
tEh
∥X0∥2i
+σ2
tE
∥Z∥2
=Eh
∥X0∥2i
+tE
∥Z∥2
= ¯m2
0+V0+σ2
td .
Finally, put these terms together, we have:
KL(qt|N( ¯mt, Vt))≤1
2log Qd
i=1 
m2
tci+σ2
t
(σ2
t)d!
+R2mt
σ2
t,
where ciare the eigenvalues of Cov [q0]. Then by choosing the largest absolute value eigenvalue
largest absolute value, we can use the Taylor expansion to obtain the first results of this lemma. For
the second result of this lemma, we directly compute the KL divergence between N( ¯mt, Vt)and
N(0, σ2
t)to obtain the final results. ■
15Theorem 4.2. Assume Assumption 4.1 and 3.1. Let qτ
∞=N(0, σ2
TI). With mT, σTdefined in
Equation (4), we have TV(qτ
T, qτ
∞)≤√mT¯D/σ T, where ¯D=d|c|+E[q0] +Randcis the
eigenvalue of Cov [q0]with the largest absolute value.
Proof .We know that
∥qT−qτ
∞∥TV
≤ ∥qT− N(mTE[q0], m2
TCov[q0] +σ2
TI)∥TV+∥N(mTE[q0], m2
TCov[q0] +σ2
TI)−qτ
∞∥TV.
By directly using the Pinsker’s inequality and Lemma B.2, we complete the proof. ■
C The Proof of the Polynomial Complexity for Reverse SDE
In this section, we prove Corollary 5.4. First, we recall the Girsanov’s Theorem [Le Gall, 2016] used
in Chen et al. [2023c]:
Lemma C.1 (Girsanov’s theorem) .LetPTandQTbe two probability measures on path space
C 
[0, T];Rd
. Suppose that under PT, the process (Xt)t∈[0,T]follows
dXt=˜btdt+αtd˜Bt
where ˜Bis aPT-Brownian motion, and under QT, the process (Xt)t∈[0,T]follows
dXt=btdt+αtdBt
where Bis aQT-Brownian motion. We assume that for each t >0, αtis ad×dsymmetric positive
definite matrix. Then, provided that Novikov’s condition holds,
EQTexp 
1
2ZT
0α−1
t
˜bt−bt2
dt!
<∞,
we have that
dPT
dQT= exp ZT
0α−1
t
˜bt−bt
dBt−1
2ZT
0α−1
t
˜bt−bt2
dt!
.
If the Novikov’s condition is satisfied, we apply the Girsanov theorem by choosing PT=
Rqτ
T
K, QT=Qqτ
T
tK,˜bt=βT−tn
1
τeYt+ 2s(T−tk,eYt)o
(for t∈[tk, tk+1]),bt=
βT−t1
τYt+ 
1 +η2
∇logqT−t(Yt)	
, and αt=p
2βT−tId.
Then, similar to Chen et al. [2023c], we have the following lemma.
Lemma C.2. Assuming that Rqτ
T
KandQqτ
T
tKsatisfy Novikov’s condition, it holds that
KL
Qqτ
T
tK∥Rqτ
T
K
=EQqτ
T
tKlndQqτ
T
tK
dRqτ
T
K=K−1X
k=0EQqτ
T
tKZtk+1
tk2βT−t∥s(T−tk,Ytk)− ∇lnqT−t(Yt)∥2dt .
Before using the Girsanov’s Theorem, we need to check the Novikov’s condition. We use almost
the same proof process compare to Chen et al. [2023c]. The key proof of the Novikov’s condition
is Lemma 19 of Chen et al. [2023c]. Hence, we give a complete proof of Lemma 19 in Chen et al.
[2023c] under our drifted VESDE. Before the proof, we first introduce a smooth cuttoff function for
truncating the drift terms.
Lemma C.3 (lemma 17 of Chen et al. [2023c]) .For any ¯R > 0, there is a smooth function
ϕR:Rd→[0,1]satisfying: 1. ϕ¯R(x) = 1 for all ∥x∥ ≤¯R, 2.ϕ¯R(x) = 0 for all ∥x∥ ≥2¯R, 3.ϕ¯R
isO(1/¯R)-Lipschitz.
16Note that ¯Ris not Rin Assumption 4.1 and will goes to +∞in the proof of Chen et al. [2023c].
Similar to Chen et al. [2023c], we also introduce a L∞and a modified process with truncation
argument for t∈[tk, tk+1]. Define the bad set
Bt:={∥st− ∇lnqt∥ ≥εscore,∞},
where εscore,∞>0is a parameter to be chosen later. We define the L∞-accurate score estimate to be
s∞
t:=st1Bc
t+∇lnqt1Bt.
We note that ∥s∞
t− ∇lnqt∥ ≤εscore,∞.
The modified process with truncation argument for t∈[tk, tk+1]is
dY∞
t=βT−t
Y∞
t/τ+ 2∇logqτ
T−t(Y∞
t)	
dt+p
2βT−tdBt,
deY∞
t=βT−teY∞
t/τ+ 2s(T−tk,eY∞
t)	
dt+p
2βT−tdBt,
where Y∞
0=eY∞
0is obtained by sampling Y∞
0∼qτ
Tand setting eY∞
0=XTif∥XT∥ ≤ ¯Rand
setting eY∞
0= 0otherwise. Then, we ara ready to prove the following lemma.
Lemma C.4 (Modified key lemma for Novikov’s condition) .
EQqτ
T,∞
tKexp K−1X
k=0Ztk+1
tkβT−tkϕ¯R 
Y∞
T−tk
s∞
T−tk 
Y∞
T−tk
−ϕ¯R 
Y∞
T−tk
∇lnqT−t 
Y∞
T−tk2dt!
<∞
Proof .We note that due to the manifold hypothesis (Assumption 4.1), if ¯R≥√
dT2+R2, then the
marginal distribution of Y∞
T−tkis exactly the same compared to Xtk. We also recall that ¯R→+∞
in Chen et al. [2023c] (Theorem 21 of Chen et al. [2023c]). Hence, we can use Lemma E.1 to prove
that
p
βT−tkϕ¯R 
Y∞
T−tk
s∞
T−tk 
Y∞
T−tk≤ sup
t∗∈[0,T−δ]p
βT−t∗s∞
T−t∗(YT−t∗)=:A′<∞.
and
p
βT−tkϕ¯R 
Y∞
T−tk
∇lnqT−t 
Y∞
T−tk≤ sup
t∗∈[0,T−δ]p
βT−t∗∥∇lnqT−t∗(YT−t⋆)∥=:B′<∞.
Then, the left hand of this lemma is at most exp 
2T 
A′2+B′2
<∞as claimed. ■
After obtaining the above inequality, the remaining proof for Novikov’s condition are exactly com-
pared to Chen et al. [2023c].
Since we assume the accurate score function in this work, this lemma need to control
sup
x∗∈B(0,R),t∗∈[0,T−δ]2βT−t∗∥∇lnqT−t∗(x∗)∥=:B <∞.
As we shown in Lemma E.1, we know that with the early stopping parameter δ,∥∇lnqT−t∗(x∗)∥
is controlled. By using Assumption 4.1, we know that1
βT−t∗≤¯β. Finally, with similar process to
Chen et al. [2023c], we can proof that the Novikov’s condition is satisfied. The following lemma
show the discretization error for our drifted VESDE with reverse SDE.
Lemma C.5 (Discretization) .Suppose that Assumption 4.1 and Assumption 5.1 holds. Let ¯γK=
argmaxk∈{0,...,K−1}γk, γK=δ. Ifτ∈[1, T2]andβt∈[1, t2], then with Qqτ
T
tKandRqτ
T
Kdefined in
Lemma C.2,
TV
Rqτ
T
K, Qqτ
T
tK2
≲R4Tτβ Td
σ8
δ¯γK+R6Tτβ T
σ8
δ¯γ2
K+ϵ2
scoreTβT.
17Proof .First, we control the discretization error in an interval t∈[tk, tk+1]:
EQqτ
T
tKh
∥s(T−tk,Ytk)− ∇lnqT−t(Yt)∥2i
≲ϵ2
score+EQqτ
T
tKh
∥∇lnqT−tk(Ytk)− ∇lnqT−t(Ytk)∥2i
+EQqτ
T
tKh
∥∇lnqT−t(Ytk)− ∇lnqT−t(Yt)∥2i
≲EQqτ
T
tK"∇lnqT−tk
qT−t(Ytk)2#
+L2EQqτ
T
tKh
∥Ytk−Yt∥2i
+ϵ2
score
≲τL2d¯γK+τL2¯γ2
K 
dτ+R2
+τL3¯γ2
K+L2(βTd¯γK+R2¯γ2
K) +ϵ2
score
≲τL2d¯γK+τL2R2¯γ2
K+ϵ2
score,
where L= max t∈[0,T−δ]∇2logqT−t(Yt)≤ 
1 +R2
/σ4
δand the third inequality follows
Lemma E.5. Then, we know that
K−1X
k=0EQqτ
T
tKZtk+1
tk2βT−t∥s(T−tk,Ytk)− ∇lnqT−t(Yt)∥2dt
≲τTβ TL2d¯γK+L2R2τTβ T¯γ2
K+ϵ2
scoreTβT
≲R4τTβ Td
σ8
δ¯γK+R6τTβ T
σ8
δ¯γ2
K+ϵ2
scoreTβT.
After obtaining the general discretization error for our drifted VESDE, we focus on two special cases.
Forτ=T2andβt=t2, we have that
K−1X
k=0EQqτ
T
tKZtk+1
tk2βT−t∥s(T−tk,Ytk)− ∇lnqT−t(Yt)∥2dt
≲T3βTL2d¯γK+L2R2T3βT¯γ2
K+ϵ2
scoreTβT
≲R4T3βTd
σ8
δ¯γK+R6T3βT
σ8
δ¯γ2
K+ϵ2
scoreTβT
=R4T5d
σ8
δ¯γK+R6T5
σ8
δ¯γ2
K+ϵ2
scoreT3.
Forτ=Tandβt=t, we know that
K−1X
k=0EQqτ
T
tKZtk+1
tk2βT−t∥s(T−tk,Ytk)− ∇lnqT−t(Yt)∥2dt
≲T3L2d¯γK+L2R2T3¯γ2
K+ϵ2
scoreT2
≲R4T3d
σ8
δ¯γK+R6T3
σ8
δ¯γ2
K+ϵ2
scoreT2.
■
Combined with the reversing beginning error controlled by Theorem 4.2, we can obtain the conver-
gence guarantee for the general drifted VESDE with reverse SDE.
Theorem 5.2. Assume Assumption 3.1, 4.1, 5.1. Let ¯Ddefined in Theorem 4.2, ¯γK=
argmaxk∈{0,...,K−1}γk,τ=T2andβt∈[1, t2]. Then, we have that
TV
Rqτ
∞
K, qδ
≤¯D√mT
σT+R2√
d
σ4
δp
¯γKβTτT+ϵscorep
βTT .
18Proof .By the data processing inequality, we know that
TV
Rqτ
∞
K, qδ
≤TV
Rqτ
∞
K, Rqτ
T
K
+ TV
Rqτ
T
K, Qqτ
T
tK
≤TV (qτ
T, qτ
∞) + TV
Rqτ
∞
K, Qqτ
T
tK
.
Combined with Theorem 4.2 and Lemma C.5, we achieve the final result. ■
C.1 The Sample Complexity for Drifted VESDE
As shown in Theorem 5.2, the general convergence guarantee is
¯D√mT
σT+R2√
d
σ4
δp
¯γKβTτT+ϵscorep
βTT .
In this section, we provide the sample complexity for different βtandτ.
The results of τ= 1 andβt= 1.When considering βt= 1 andτ= 1, the drifted VESDE
becomes VPSDE and mT= exp ( −T), which indicates Tis a logarithmic term and the dominated
term of the convergence guarantee is the discretization term eO(R2√d¯γK/σ4
δ). To make this term
smaller than ϵTV, we require ¯γK≤σ8
δϵ2
TV/(R4d). To make sure that W2
2(q0, qδ)≤ϵ2
W2, we require
σ2
δ≤ϵ2
W2
d+R√
d. Then, we achieve the final sample complexity
K≤eO 
dR4(d+R√
d)4
ϵ8
W2ϵ2
TV!
.
We note that this results is exactly the same with Chen et al. [2023c], which means our drifted VESDE
covers VPSDE setting.
C.1.1 The results of τ=T2with different βt
In this part, we analyze the influence of βtunder setting τ=T2and show the power of our drifted
VESDE.
The aggressive βt(τ=T2).When considering aggressive βt=tα1where 2≥α1≥1 + ln( T−
ln(T))/ln(T),√mT/σT≥exp (−T/2), which means Tis a logarithmic term and can be ignored.
After that, the analysis process is exactly the same with the above VPSDE setting, and we achieve the
sample complexity
K≤eO 
dR4(d+R√
d)4
ϵ8
W2ϵ2
TV!
.
Defined by Rqτ
∞
K,R 0the output Rqτ
∞
Kprojected onto B (0, R0)forR0=eΘ(R). Following exactly the
same proof process of Chen et al. [2023c] (Corollary 5), we have that
W2(Rqτ
∞
K,R 0, q0)≤ϵW2
with sample complexity
K≤eO 
dR8(d+R√
d)4
ϵ12
W2!
.
The conservative βt=t.In this case, the first term is ¯D/T . To make this term smaller than ϵTV,
we require T≥¯D/ϵ TV. For the stepsize, we require ¯γK≤σ8
δϵ2
TV/(R4dT4), which means the
sample complexity is
K≤O 
dR4(d+R√
d)4¯D5
ϵ8
W2ϵ7
TV!
.
19The most conservative βt= 1.In this case, mT= exp ( −1/T)andσ2
t=τ(1−m2
t) =
T2(1−exp (−2/T). When Tis large enough, mT= Θ(1) andσ2
T=T, which indicates the first
term is ¯D/√
T. To make this term smaller than ϵTV, we require T≥¯D2/ϵ2
TV. For the second term,
we require ¯γK≤σ8
δϵ2
TV/(R4dT3)Then, the final complexity is
K≤O 
dR4(d+R√
d)4¯D8
ϵ8
W2ϵ10
TV!
.
C.1.2 The results of τ=Twith different βt
At the remaining part, we show the sample complexity of setting τ=Twith different βt.
The results for setting τ=Tandβt=t.For this setting, as shown in Lemma C.5, the
discretization error is
TV
Rqτ
T
K, Qqτ
T
tK2
≲R4T3d
σ8
δ¯γK+R6T3
σ8
δ¯γ2
K+ϵ2
scoreT2.
Furthermore, we choose an aggressive βt, which indicates Tis a logarithmic term. Then, by choosing
σ2
δ≤ϵ2
W2
(d+R√
d)and¯γK≤σ8
δϵ2
TVln3 ¯D/ϵ TV
/ 
R4d
, we obtain the sample complexity
K=T
¯γK≤eO 
dR4(d+R√
d)4
ϵ8
W2ϵ2
TV!
.
The results for setting τ=Tandβt= 1.In this setting, the reverse beginning error is
bounded by ¯D/√
T, which indicates T≥¯D2/ϵ2
TV. For the discretization term, we require
¯γK≤σ8
δϵ2
TV/(R4dT2). Then, the sample complexity is bounde by
K=T
¯γK≤O 
dR4(d+R√
d)4¯D6
ϵ8
W2ϵ8
TV!
.
D The Proof of the Convergence Guarantee in the Unified Framework
In this work, we introduce an indicator i∈ {1,2}forσT−tKto represent different βt. We use τ=T2
as an example. When βt=t2is aggressive, we choose i= 1,η= 1andσ−2
T−tK(i= 1)≤1
τ+¯β
δ3.
When βt=tis conservative, we choose i= 2,η∈[0,1)andσ−2
T−tK(i= 2)≤1
τ+¯β
δ2. In the proof
process of Lemma 6.3, Lemma D.1, Lemma D.2 and Lemma D.3, we ignore the indicator isince
this lemma does not involve the specific value of σ2
T−tK(i). Before the proof of this section, we first
recall the stochastic flow of the reverse process for any x∈Rdands, t∈[0, T]witht≥s:
dYx
s,t=βT−t
Yx
s,t/τ+ 
1 +η2
∇logqT−t 
Yx
s,t	
dt+ηp
2βT−tdBt, Yx
s,s=x ,
and the interpolation of its discretization for any k∈ {0, ..., K}andt∈[sk, tk+1):
d¯Yx
s,t(k) =βT−t¯Yx
s,t/τ+ 
1 +η2
s 
T−sk,¯Yx
s,t	
dt+ηp
2βT−tdBt, ¯Yx
s,s=x ,
where sk= max ( s, tk). To deal with the discretization error, we use the approximation technique
used in Bortoli [2022]. Hence, we introduce the tangent process:
d∇Yx
s,t=βT−t
I/τ+ 
1 +η2
∇2logqT−t(Yx
s,t)	
∇Yx
s,tdt, ∇Yx
s,s=I.
Then, we discuss the interpolation formula, which is used to control the discretization error.
Proposition 1. Fors, t∈[0, T)withs < t , anyk∈ {0, ..., K}and(ωv)v∈[s,T], we define that
bu(ω) =βT−u(1
τωu+ (1 + η2)∇logqT−u(ωu)),
¯bu(ω) =βT−u(1
τωu+ (1 + η2)s(T−sk, ωsk)),∆bu(ω) =bu(ω)−¯bu(ω),
20where sk= max( s, tk)andu∈[sk, tk+1). Then, for any x∈Rd, we have that
Yx
s,t−¯Yx
s,t=Zt
s∇Yx
u,t ¯Yx
s,u⊤∆bu ¯Yx
s,v
v∈[s,T]
du,
where for any u∈[0, T), there exists a k∈ {0, ..., K}satisfies u∈[sk, tk+1).
For reverse SDE, the augmentation is similar to Bortoli [2022] (Appendix E). When η= 0, the
stochastic extension of the Alekseev–Gröbner formula [Del Moral and Singh, 2022] degenerates into
the original version [Alekseev, 1961]. After that, we control the tangent process.
Lemma 6.3. Assume Assumption 3.1 and 4.1. For ∀s∈[0, tK]andx∈Rd, we have
∥∇Yx
s,tK,i∥ ≤expR2
2σ2
T−tK+(1−η2)
2ZtK
0βT−u
τdu
.
If∇2logqt(xt)≤Γ/σ2
t,∥∇Yx
s,tK,i∥ ≤σ−(1+η2)Γ
T−tKexp  
1 +η2
Γ + 2RtK
0βT−u
τdu
.
Proof .Using the definition of the tangent process and Lemma E.1, we have
d∇Yx
s,t2
≤2βT−t1
τ∇Yx
s,t2− 
1 +η2 
1−m2
T−tR2/ 
2σ2
T−t
/σ2
T−t∇Yx
s,t2
dt .
Using Lemma F.1, we have
Zt
sβT−u1
τ− 
1 +η2
/σ2
T−u+ 
1 +η2
m2
T−uR2/2σ4
T−u
du
≤  
1 +η2
R2/4 
σ−2
T−t−σ−2
T−s
+1−η2
2Zt
sβT−u
τdu
≤ 
1 +η2
R2
4σ2
T−t+1−η2
2Zt
sβT−u
τdu .
Note that ∇Ys,s=I, we get
∥∇Yx
s,tK∥2≤exp" 
1 +η2
R2
2σ2
T−t+ (1−η2)ZtK
0βT−u
τdu#
.
When we assume∇logq2
t(xt)≤Γ/σ2
t, we know that
d∇Yx
s,t2≤2βT−t 
1
τ− 
1 +η2
Γ
σ2
T−t!
∇Yx
s,t2dt .
Using Lemma F.1, we have
2Zt
sβT−u/σ2
T−udu
≤log 
exp"
2ZT−s
0βT−u
τdu#
−1!
−log 
exp"
2ZT−t
0βT−u
τdu#
−1!
≤log 
σ2
T−s
−log 
σ2
T−t
+ZT−s
T−tβu
τdu .
Then we have
∥∇Yx
s,tK∥2≤σ−(1+η2)Γ
T−tKexp  
1 +η2
Γ + 2ZtK
0βT−u
τdu
.
Thus we complete our proof. ■
21After bounding the gradient of the tangent process, the remaining term is ∥∆b∥:
∥∆b∥ ≤ ∥ ∆(a,b)b∥+∥∆(b,c)b∥+∥∆(c,d)b∥, (7)
where b(a)=bandb(d)=¯b. Moreover,
b(b)
u(ω) =βT−u(1
τωu+ (1 + η2)∇logqT−sk(ωu)),
b(c)
u(ω) =βT−u(1
τωu+ (1 + η2)∇logqT−sk(ωsk)),
∆a,b
b=b(a)−b(b),∆b,c
b=b(b)−b(c),∆c,d
b=b(c)−b(d).
We then control ∥∆(a,b)b∥,∥∆(b,c)b∥,∥∆(c,d)b∥separately. In this section, ∥∆(c,d)b∥= 0 since
we assume that the accurate score function is achieved. For∆(a,b)bu(ω), we have the following
lemma.
Lemma D.1. Fors, u∈[0, T)such that u≥s, u∈[sk, tk+1)andω= (ωv)v∈[s,T]we have
∥∆(a,b)bu(ω)∥
≤ 
1 +η2
βT−u sup
v∈[T−u,T−tk] 
βv/σ6
v 
2 +R2
(R+∥ωu∥)γk.
Proof .Without loss of generality, we assume s≤tk. Then
∥∆(a,b)bu(ω)∥ ≤ 
1 +η2
βT−u∥∇logqT−u(ωu)− ∇logqT−tk(ωu)∥
≤ 
1 +η2
βT−uγk sup
v∈[T−u,T−tk]∥∂v∇logqT−v(ωu)∥.
Then by Lemma E.4, we have
∥∆(a,b)bu(ω)∥
≤ 
1 +η2
βT−u sup
v∈[T−u,T−tk] 
βv/σ6
v 
2 +R2
(R+∥ωu∥)γk.
■
For∆(b,c)bu(ω), we have the following lemma.
Lemma D.2. Fors, u∈[0, T)such that u≥s, u∈[sk, tk+1)andω= (ωv)v∈[s,T]we have
∥∆(b,c)bu(ω)∥ ≤ 
1 +η2 
βT−u/σ4
T−u 
1 +R2
∥ωu−ωsk∥.
Proof .Without loss of generality, we assume s≤tk. In this case sk=tk, Then
∥∆(b,c)bu(ω)∥ ≤ 
1 +η2
βT−u∥∇logqT−tk(ωtk)− ∇logqT−tk(ωu)∥
≤ 
1 +η2
βT−usup
v∈[u,T−tk]∥∇2logqT−tk(ωv)∥∥ωu−ωtk∥.
Using Lemma E.2, we have that
∥∆(b,c)bu(ω)∥ ≤ 
1 +η2 
βT−u/σ4
T−u 
1 +R2
∥ωu−ωtk∥.
Then the proof is complete. ■
We need to control the reverse process when dealing with ∆b. The following lemma shows an upper
bound for the reverse Yk.
Lemma D.3. Assume Assumption 3.1 ,Assumption 4.1, and there exists δ >0such thatγkβT−tk
σ2
T−tk≤
δ≤1/28for any k∈ {0,···, K}, then we have
E[∥Yk∥2]≤U(τ) =τd+B(1/A+δ),
22where
A= 4η2+ 2−2δ−4(1 + η2)(1 + δ)µR
B= 4(1 + η2)R2δ+ 2(1 + η2)(1 + δ)R
µ+ 4η2τd
andµis an arbitrary positive number which makes A >0. In particular, if δ≤1/28, then
E[∥Yk∥2]≤U0(τ) = 111 R2+ 13τd .
Proof .Recall the discretization of the backward process (the explicit form of Equation (6))
Yk+1=Yk+γ1,k1
τYk+ (1 + η2)s(T−tk, Yk)
+ηp
2γ2,kZk,
γ1,k= exp"ZT−tk
T−tk+1βsds#
−1, γ 2,k= 
exp"
2ZT−tk
T−tk+1βsds#
−1!
/2,
where {Zk}k∈Kare independent Gaussian random variables. It is clear that γ1,k≤γ2,k≤2γ1,k,
and using Lemma E.1 we have
⟨xt,s(t, xt)⟩=⟨xt,∇logqt(xt)⟩
≤ −∥ xt∥2/σ2
t+mtR∥xt∥/σ2
t
≤(−1 +µmtR)∥xt∥2/σ2
t+ (mtR/µ)/σ2
t,
where the first equality follows that we assume the accurate score function. For any µ >0. Again
using Lemma E.1, we have
∥s(t, xt)∥2=∥∇logqt(xt)∥2
≤2∥xt∥2/σ4
t+ 2m2
tR2/σ4
t.
Combining the results above, we have
E[∥Yk+1∥2] = (1 +γ1,k
τ)2E[∥Yk∥2] + (1 + η2)2γ2
1,kE[∥s(T−tk, Yk)∥2]
+ 2(1 + η2)(1 +γ1,k
τ)γ1,kE[⟨Yk, s(T−tk, Yk)⟩] + 2η2γ2,kd
≤((1 +γ1,k
τ)2+ 2(1 + η2)2γ2
1,k/σ4
T−tk
+ 2(1 + η2)(1 +γ1,k
τ)γ1,k(−1 +µmT−tkR)/σ2
T−tk)E[∥Yk∥2]
+2m2
T−tkR2
σ4
T−tk(1 +η2)2γ2
1,k+mT−tkR
µσ2
T−tk(1 +η2)(1 +γ1,k
τ)γ1,k+ 4η2γ1,kd .
If we denote δk=γ1,k/σ2
T−tkand notice the fact that mt∈[0,1], σ2
t∈[0, τ], η∈[0,1], then we
have
E[∥Yk+1∥2]≤(1 + 2 δk+δ2
k)E[∥Yk∥2] + 8δ2
kE[∥Yk∥2]
+ 2(1 + δk)δk(−1 +µR)E[∥Yk∥2] + 8R2δ2
k+2R
µδk(1 +δk) + 4τδkd .
We also have that
γ1,k= exp[ZT−tk
T−tk+1βsds]−1≤exp[βT−tkγk]−1≤2βT−tkγk,
where the last inequality follows that γk= exp ( −T),βT−tkγk≤1/2for small enough stepsize,
andeω−1≤2ωfor any ω∈[0,1/2]. We get δk≤2γkβT−tk/σ2
T−tk≤2δ. Thus
E[∥Yk+1∥2]≤(1 + 2 δk+ 2δkδ)E[∥Yk∥2] + 16 δkδE[∥Yk∥2]
+ 4(1 + δ)(−1 +µR)δkE[∥Yk∥2] + 16 R2δkδ+ 4(1 + δ)R
µδk+ 4τdδk.
23Hence, we have
E[∥Yk+1∥2]≤(1 +δk[−2 + 14 δ+ 4(1 + δ)µR])E[∥Yk∥2]
+δk[16R2δ+ 4(1 + δ)R
µ+ 4τd].
We denote A= 2−14δ−4(1 + δ)µRandB= 16R2δ+ 4(1 + δ)R
µ+ 4τd, then
E[∥Yk+1∥2]≤(1−δkA)E[∥Yk∥2] +δkB .
Notice that E[∥Y0∥2] =dτand if E[∥Yk∥2]≥B/A it is decreasing, if E[∥Yk∥2]≤B/A we have
E[∥Yk+1∥2]≤B/A +δB. so
E[∥Yk∥2]≤τd+B(1/A+δ).
Notice that when δ≤1/28,if we choose µ= 1/(4(1 + δ)R),A≥1/2, and
B≤37R2+ 4τd .
Then, the proof is complete. ■
The following lemma shows a discretization error in the k-the interval.
Lemma D.4. Assume Assumption 3.1,Assumption 4.1 and γkβT−tk/σ2
T−tk≤1/28for any k∈
{0,···, K−1}. Then for any k,t∈[tk, tk+1]andi∈ {1,2}, we have that
E[∥¯Yt−¯Ytk∥2]≤Li(τ)βT−tkγk,
where Li(τ) = ¯ γKκi(τ)(64
σ2
T−tK(i)+8
τ)U0(τ) + 64 R2¯γKκi(τ)
σ2
T−tK(i)+ 4d,¯γK,κi(τ)is defined in
Lemma D.5 and U0(τ)is defined in Lemma D.3.
Proof .Recall the discretized backward process
¯Yt=¯Ytk+ (exp[ZT−tk
T−tβsds]−1)(1
τ¯Ytk+ (1 + η2)s(T−tk,¯Ytk))
+η(exp[2ZT−tk
T−tβsds−1])1/2Z ,
where Zis a standard Gaussian random variable. By directly calculating, we have that
E[∥¯Yt−¯Ytk∥2] = 2(exp[ZT−tk
T−tβsds]−1)2(1
τ2E[∥¯Ytk∥2] + (1 + η2)2E[∥s(T−tk,¯Ytk)∥2])
+η2(exp[2ZT−tk
T−tβsds]−1)d .
By Lemma E.1 and accurate score function assumption,
∥s(T−tk,¯Ytk)∥2≤2∥¯Ytk∥2/σ4
T−tk(i) + 2m2
T−tkR2/σ4
T−tk(i).
So we have that
E[∥¯Yt−¯Ytk∥2]≤2(exp[ZT−tk
T−tβsds]−1)2((8
σ4
T−tk(i)+1
τ2)E[∥¯Ytk∥2] +8R2
σ4
T−tk(i))
+ (exp[2ZT−tk
T−tβsds]−1)d .
Bye2w−1≤1 + 4 wfor any w∈[0,1/2]andγksupv∈[T−tk+1,T−tk]βv/σ2
v≤1/28for any
k∈ {0, ..., K −1}, we have
exp[ρZT−tk
T−tβsds]−1≤2ρβT−tkγk.
24forρ= 1,2. And using Lemma D.3 and Lemma F.2 we have
E[∥¯Yt−¯Ytk∥2]
≤(64γk
σ4
T−tk(i)+8βT−tkγk
τ2)U0(τ)βT−tkγk+ 64R2γk
σ4
T−tk(i)βT−tkγk+ 4dβT−tkγk.
We denote Li(τ) = ¯γKκi(τ)(64
σ2
T−tK(i)+8
τ)U0(τ) + 64 R2¯γKκi(τ)
σ2
T−tK(i)+ 4dfori∈ {1,2}and the
proof is complete. ■
Lemma D.5. Assume Assumption 3.1 and Assumption 4.1, γksupv∈[T−tk+1,T−tk]βv/σ2
v≤1/28
for any k∈ {0, ..., K −1}. Let ¯γK=argmaxk∈{0,...,K−1}γk,κi(τ) = max {¯β,T2
T−1+i}σ−2
T−tK(i),
and
Ci(τ) = 2(2 + R2)(R+U1/2
0(τ)) + 2 L1/2
i(τ)τ3/2(1 +R2),
fori∈ {1,2}. Then, for any s, u∈[0, tK]withu≥sandi∈ {1,2}, we have
E[∥∆bu,i((¯Ys,v)v∈[s,T])∥]≤Ci(τ)[κ2
i(τ)σ−2
T−tK(i)¯γ1/2
K+κ2
i(τ)]¯γ1/2
K,
where ¯Ys,s∼N(0,I).
Proof .Combining Lemma D.1, Lemma D.2 and the exact score function, we get
∥∆bu,i(ω)∥ ≤(1 +η2) sup
v∈[T−tk+1,T−tk](β2
v/σ6
v(i))(2 + R2)(diam( M+∥ωu∥))γk
+ (1 + η2)(βT−u/σ4
T−u(i))(1 + diam( M2))∥ωu−ωsk∥.
For any u∈[T−tK, T], using Lemma F.3 we have βu/σ2
u(i)≤κi(τ). Hence,
∥∆bu,i(ω)∥ ≤(1 +η2) sup
v∈[T−tk+1,T−tk](β2
v/σ6
v(i))(2 + diam( M2))(R+∥ωu∥)γk
+ (1 + η2)(βT−u/σ4
T−u(i))(1 + diam( M2))(∥ωu−ωtk∥)
≤(1 +η2)(κ2
i(τ)/σ2
T−tk+1(i))γk(2 + diam( M2))(R+∥ωu∥)
+ (1 + η2)κ2
i(τ)(1 + R2)∥ωu−ωtk∥/βT−u.
Combining this with Lemma D.3 and Lemma D.4,
E[∥∆bu,i((¯Ys,v)v∈[s,T])∥]≤(1 +η2)(κ2
i(τ)/σ2
T−tk+1(i))¯γK(2 +R2)(R+U1/2
0(τ))
+ (1 + η2)κ2
i(τ)(1 + R2)L1/2
i(τ) max{¯β, τ}3/2¯γ1/2
K.
We denote Ci(τ) = 2(2 + R2)(R+U1/2
0(τ)) + 2 L1/2
i(τ)τ3/2(1 +R2), fori∈ {1,2}, then we have
E[∥∆bu,i((¯Ys,v)v∈[s,T])∥]≤Ci(τ)((κ2
i(τ)/σ2
T−tk+1)¯γK+κ2
i(τ)¯γ1/2
K).
■
Lemma D.6. Assume Assumption 3.1 and Assumption 4.1, γksupv∈[T−tk+1,T−tk]βv/σ2
v≤1/28
for any k∈ {0, ..., K −1}. Let¯γK=argmaxk∈{0,...,K−1}γk,γK=δ, and δ≤1/32. Then
W1
Rqτ
∞
K, Qqτ
∞
tK
≤Ci(τ)κ2
i(τ)TexpR2
2σ2
T−tK(i)+(1−η2)
2
[¯γ1/2
K
σ2
T−tK(i)+ 1]¯γ1/2
K,
where Ci(τ), κi(τ)fori∈ {1,2}are the same terms to Theorem 6.1.
25Proof .ByProposition 1 we have
∥YtK−YK∥=∥YtK−¯YtK∥ ≤ZtK
0∥∇Yu,tK,i(¯Y0,u)∥∥∆bu,i((¯Y0,v)v∈[0,T])∥du.
∥YtK−YK∥
≤exp" 
1 +η2
R2
4σ2
T−t(i)+(1−η2)
2ZtK
0βT−u
τdu#ZtK
0∥∆bu,i((¯Y0,v)v∈[0,T])∥du .
Then by definition of Wasserstein distance, we have
W1(q∞QtK, q∞RK)
≤E[∥YtK−YK∥]
≤exp" 
1 +η2
R2
4σ2
T−tK(i)+(1−η2)
2ZtK
0βT−u
τdu#ZtK
0E[∥∆bu,i((¯Y0,v)v∈[0,T]∥]du
≤Ci(τ)Texp" 
1 +η2
R2
4σ2
T−tK(i)+(1−η2)
2#
[κ2
i(τ)σ−2
T−tK(i)¯γ1/2
K+κ2
i(τ)]¯γ1/2
K.
■
Theorem 6.1. Assume Assumption 3.1 and 4.1, δ≤1/32andγksupv∈[T−tk+1,T−tk]βv/σ2
v≤1/28
for∀k∈ {0, ..., K −1}. LetγK=δ. Then, for ∀τ∈[T, T2]:
(1) If η= 1(the reverse SDE), choosing βt=t2,W1
Rqτ
∞
K, q0
is bounded by
(R
τ+√
d)√
δ+ expR2
2(¯β
δ3+1
τ) 
C1(τ)Tκ2
1(τ)
(¯β
δ3+1
τ)¯γ1/2
K+ 1
¯γ1/2
K+¯De−T/2
√τ!
,
where κ1(τ) =T2(1/τ+¯β/δ3)andC1(τ)is linear in τ2.
(2) If η= 0(PFODE), choosing a conservative βt(Assumption 3.1), W1
Rqτ
∞
K, q0
is bounded by
(R
τ+√
d)√
δ+ expR2
2(¯β
δ2+1
τ) 
C2(τ)κ2
2(τ)T
(¯β
δ2+1
τ)¯γ1/2
K+ 1
¯γ1/2
K+¯D√τ!
,
where κ2(τ) =T 
1/τ+¯β/δ2
andC2(τ)is linear in τ2.
Proof .To obtain the convergence guarantee, we need to control three error terms:
W1
Rqτ
∞
K, q0
≤W1
Rqτ
∞
K, Qqτ
∞
tK
+W1
Qqτ
∞
tK, Qq0PT
tK
+W1
Qq0PT
tK, q0
.
For term W1
Rqτ
∞
K, Qqτ
∞
tK
, we use Lemma D.6.
For the second term, we define 
Yx
0,t
t∈[0,T]and 
Yy
0,t
t∈[0,T]be the reverse processes with initial
condition xandy. Then we have
∥Yx
0,t−Yy
0,t∥ ≤ ∥ x−y∥Z1
0∥∇Yzλ
0,t∥dλ ,
where zλ=λx+ (1−λ)y. In this work, we choose x∼qτ
∞andy∼q0PT. Combined with the
above inequality, Theorem 4.2 and Lemma 6.3, we know that:
W1
Qqτ
∞
tK, Qq0PT
tK
≤expR2
2σ2
T−tK(i)+(1−η2)
2ZtK
0βT−u
τdu
∥q0PT−qτ
∞∥
≤√mT¯D
σTexpR2
2σ2
T−tK(i)+(1−η2)
2ZtK
0βT−u
τdu
.
26For the last term, we use exactly the same process with Bortoli [2022] with bounded σ2
T−tK:
W1
Qq0PT
tK, q0
≤E[∥X−mT−tKX+σT−tKZ∥]
≤(R
τ+√
d)σT−tK
≤2(R
τ+√
d)√
δ ,
where the second inequality follows that σ2
T−tK+τmT−tK=τ. ■
In the end of the section, we provide the proof of Corollary 6.2.
Corollary D.7. Assume Assumption 3.1, 4.1 and∇2logqt(xt)≤Γ/σ2
t. Let η= 0 (reverse
PFODE), δ∈(0,1/32), τ=T2,βt=tandκ2(τ), C2(τ)defined in Theorem 6.1, we have
W1
Rqτ
∞
K, q0
≤(R
τ+√
d)√
δ+¯βΓ
2
δΓexpΓ + 2
2 
C2(τ)κ2
2(τ)T((¯β
δ2+1
τ)¯γ1/2
K+ 1)¯γ1/2
K+¯D√τ!
.
Proof .The proof of this corollary is almost identical to the proof of Theorem 6.1. We just need to
replace the first bound for the tangent process in Lemma 6.3 by the second bound. ■
E Lemmas for the Logarithmic Density
In this section, we introduce auxiliary lemmas to control the gradient and Hessian of the logarithmic
density under the manifold hypothesis. Lemma E.1, Lemma E.2 and Lemma E.3 come from
Lemma C.1, Lemma C.2, and Lemma C.5 of Bortoli [2022]. Since these lemmas do not involve the
relationship between mtandσt, we can directly use the results from Bortoli [2022]. Following Bortoli
[2022], we also define a empirical version of q0withNdatapoints, i.e. qN
0= (1/N)PN
k=1Xk, with
Xk	N
k=1∼q⊗N
0. We denote by 
qN
t
t>0such that for any t >0the density w.r.t. the Lebesgue
measure of the distribution of XN
t, and when N→+∞,qN
t=qt.
Lemma E.1. Assume Assumption 4.1. Then for any t∈(0, T]andxt∈Rdwe have that
⟨∇logqt(xt), xt⟩ ≤ −∥ xt∥2/σ2
t+mR∥xt∥/σ2
t.
In addition, we have
∥∇logqt(xt)∥2≤2∥xt∥2/σ4
t+ 2m2
tR2/σ4
t.
Lemma E.2. Assume Assumption 4.1. Then for any t∈(0, T],xt∈RdandM∈ M d 
Rd

M,∇2logqt(xt)M
≤ − 
1−m2
tR2/ 
2σ2
t
/σ2
t∥M∥2.
In addition, we have
∇2logqt(xt)≤ 
1 +R2
/σ4
t.
The following lemma shows that the derivatives up to the fourth order are uniformly bounded since
τ∈[T, T2]. Thus we can use the stochastic extension of the Alekseev–Gröbner formula [Del Moral
and Singh, 2022].
Lemma E.3. Assume Assumption 4.1. Then, there exists ¯C≥0such that for any t∈(0, T]we have
∇2logqt(x)+∇3logqt(x)+∇4logqt(x)≤¯C/σ8
t.
The following lemma shows that ∥∂t∇logqt(xt)∥is bounded. The proof before using the relation-
ship between σtandmtis identical compared to Lemma C.3 in Bortoli [2022]. For the sake of
completeness, we also give the proof process of this part.
Lemma E.4. Assume Assumption 4.1. Then for any t∈(0, T]andxt∈Rdwe have
∥∂t∇logqt(xt)∥ ≤ 
βt/σ6
t 
2 +R2
(R+∥xt∥).
27Proof .LetN∈Nandt∈(0, T]. We denote for any x∈Rd,qN
t(x) = ¯qN
t(x)/ 
2πσ2
td/2with
¯qN
t(x) = (1 /N)NX
k=1ek
t(x), ek
t(x) = exp
−∥x−mtXk∥2/ 
2σ2
t
.
Next we denote fk
t≜logek
t. Then we have
∂tlog ¯qN
t(x)NX
k=1∂tfk
t(x)ek
t(x)/NX
k=1ek
t(x).
Therefore we have
∂t∇log ¯qN
t(x)
=NX
k=1∂t∇fk
t(x)ek
t(x)/NX
k=1ek
t(x) +NX
k=1∂tfk
t(x)∇fk
t(x)ek
t(x)/NX
k=1ek
t(x)
−NX
k,j=1∂tfk
t(x)∇fj
t(x)ek
t(x)ej
t(x)/NX
k,j=1ek
t(x)ej
t(x)
=NX
k=1∂t∇fk
t(x)ek
t(x)/NX
k=1ek
t(x)
+ (1/2)NX
k,j=1
∂tfk
t(x)−∂tfj
t(x)
∇fk
t(x)− ∇fj
t(x)
ek
t(x)ej
t(x)/NX
k,j=1ek
t(x)ej
t(x).
In what follows, we provide upper bounds for |∂tfk
t−∂tfj
t|,∥∇fk
t− ∇fj
t∥and∂t∇fk
t. First we
notice that ∇fk
t(x) =− 
x−mtXk
/σ2
t, and using mt≤1we get
∥∇fk
t(x)− ∇fj
t(x)∥ ≤mR/σ2
t≤R/σ2
t.
and
∂tfk
t(t) =∂tσ2
t/ 
2σ4
t
∥x−mtXk∥2+∂tmt/σ2
t
Xk, x−mtXk
.
Notice the fact that ∂tσ2
t=−2τmt∂tmt= 2βtm2
tand∂tmt=−βt
τmt, combined with the above
equality, we know that
∂tfk
t(t) =−βtmt/σ2
t
− 
mt/σ2
t
∥x−mtXk∥2+1
τ
x−mtXk, Xk
=−βtmt/σ2
t
x−mtXk,− 
mt/σ2
t 
x−mtXk
+1
τXk
=−βtmt/σ4
t
x−mtXk,−mtx+
m2
t+σ2
t
τ
Xk
=βtmt/σ4
t
mt∥x∥2+mtXk2+ 
1 +m2
t
x, Xk
,
where the last equality holds that τm2
t+σ2
t=τ. The rest of the proof is identical to the Lemma C.3
in Bortoli [2022].
So using mt≤1we have
∂tfk
t(x)−∂tfj
t(x)≤2βtm2
tR2/σ4
t+βtmt 
1 +m2
t
R∥x∥/σ4
t
≤2 
βt/σ4
t
R(R+∥x∥)
Now we compute ∇∂tfk
t(x)for any x∈Rd
∇∂tfk
t(x) = 2 βtm2
t/σ4
tx+ 
βtmt/σ4
t 
1 +m2
t
Xk.
28So we can bound the norm of it by
∥∂t∇fk
t(x)∥ ≤2 
βt/σ4
t
(R+∥x∥).
Combining results above we get for any x∈Rd
∂t∇log ¯qN
t(x)≤2 
βt/σ4
t
(R+∥x∥) + 
βt/σ6
t
R2(R+∥x∥)
≤ 
βt/σ6
t 
2 +R2
(R+∥x∥)
Note that
lim
N→+∞∂t∇logqN
t(xt) =∂t∇logqt
and the proof is complete. ■
In the following lemma, similar to Chen et al. [2023c], we obtain a better control on the time
discretization error instead of controlling ∥∂t∇logqt(xt)∥for∀xt∈Rd.
Lemma E.5. Assume Assumption 4.1 and Xtsatisfies the forward process Equation (3). Define
L= max t∈[0,T−δ]∇2logqT−t(Yt)≤ 
1 +R2
/σ4
δ, then we have that
EQqτ
T
tK"∇lnqT−tk
qT−t(Ytk)2#
≲τL2d¯γK+τL2¯γ2
K(dτ+R2) +τL3¯γ2
K+τL4¯γ2
K(βTd¯γK+R2¯γ2
K).
Proof .Due to the property of the forward process, we know that if S:Rd→Rdis the mapping
S(x) := exp( −(t−tk))x, then qT−tk=S#qT−t∗normal
0, τ
1−exp(−2Rtk+1
tkβs/τds)
Similar to Chen et al. [2023c], we define α= exphRtk+1
tkβs
τdsi
= 1 + O(¯γK)andσ2=
τ
1−exp(−2Rtk+1
tkβs/τds)
=O(τ¯γK). Then we can use Lemma C.12 of Lee et al. [2022] to
obtain
EQqτ
T
tK"∇lnqT−tk
qT−t(Ytk)2#
≲τL2d¯γK+τL2¯γ2
K∥Ytk∥2+τL2¯γ2
K∥∇lnqT−t(Ytk)∥2
≲τL2d¯γK+τL2¯γ2
K(dτ+R2) +τL3¯γ2
K+τL4¯γ2
K(βTd¯γK+R2¯γ2
K).
The last inequality follows Lemma F.4 and the fact that
∥∇lnqT−t(Ytk)∥2≲∥∇lnqT−t(Yt)∥2+∥∇lnqT−t(Ytk)− ∇lnqT−t(Yt)∥2
≲∥∇lnqT−t(Ytk)∥2+L2(βTd¯γK+R2¯γ2
K)
≲L+L2(βTd¯γK+R2¯γ2
K).
■
F Auxiliary Lemmas
Lemma F.1. For any s, t∈[0, T]we have
Zt
sβT−u/σ2
T−udu="
−1
2log 
exp"
2ZT−u
0βv
τdv#
−1!#t
s,
Zt
sβT−um2
T−u/σ4
T−udu="
(1/2τ)/ 
1−exp"
−2ZT−u
0βv
τdv#!# t
s.
29Proof .We directly compute
Zt
sβT−u/σ2
T−udu=1
τZt
sβT−u/ 
1−exp"
−2ZT−u
0βv
τdv#!
du
=1
τZt
sβT−uexp"
2ZT−u
0βv
τdv#
/ 
exp"
2ZTu
0βv
τdv#
−1!
du
=−1
2Zt
s∂ulog 
exp"
2ZT−u
0βv
τdv#
−1!
du .
Similarly
Zt
sβT−um2
T−u/σ4
T−u
=1
τ2Zt
sβT−uexp"
−2ZT−u
0βv
τdv#
/ 
1−exp"
−2ZT−u
0βv
τdv#!2
du
= (1/2τ)Zs
t∂u 
1−exp"
−2ZT−u
0βv
τdv#!−1
du.
■
Lemma F.2. Assume Assumption 3.1. For i∈ {1,2}, we have σ2
T−tK(i)≤2δandσ−2
u(i)≤
σ−2
T−tK(i)≤1
τ+¯β
δ4−i,∀u∈[T−tK, T].
Proof .
σ2
T−tK(i) =τ 
1−exp"
−2ZT−tK
0βs
τds#!
≤2ZT−tK
0βsds≤2δ ,
where the first inequality follows from for any a≥0,exp[−a]≥1−a; the second inequlity follows
from Assumption 3.1 and δ≤1.
σ−2
T−tK(i) =1
τ 
1−exp"
−2ZT−tK
0βs
τds#!−1
≤1
τ
1 + 
2ZT−tK
0βs
τds!−1

≤1
τ+¯β
δ4−i,
where the first inequality follows from for any a≥0,1/(1+exp[ −a])≤1+1/a, the second inequal-
ity follows from Assumption 3.1. It is easy to check that σ−2
u(i)≤σ−2
T−tK(i),∀u∈[T−tK, T].
■
Using the bound on σ−2
T−tK(i)immediately yields the following control of βu/σ2
u(i).
Lemma F.3. Assume Assumption 3.1. Then, we have for any u∈[T−tK, T]: (1) if i= 1, then
βu
σ2u(i= 1)≤κ1(τ) = max {¯β, T2}1
τ+¯β
δ3
;
(2) if i= 2, then
βu
σ2u(i= 2)≤κ2(τ) = max {¯β, T}1
τ+¯β
δ2
.
30Generally speaking, T≥¯β≥1. Hence, We can further simplify the above inequality by removing
max.
In the rest of this section, we provide the useful lemma to achieve polynomial sample complexity for
VE-based models with reverse SDE. As shown in Lemma E.1, we also need to control E[∥Xt∥2]in
the forward process. The following lemmas shows that this term is bounded by the R2and exploding
variance.
Lemma F.4. Suppose that Assumption 4.1hold. Let (Xt)t∈[0,T]denote the forward process Equa-
tion(3). Then, for all t≥0,
Eh
∥Xt∥2i
≤dσ2
t∨R2.
Proof .As shown in Equation (4),
Eh
∥Xt∥2i
≤E
∥X0∥2
+σ2
td≤dσ2
t∨R2.
■
Lemma F.5 (movement bound for VESDE) .Let(Xt)t∈[0,T]denote the forward process Equation (3).
For0≤s < t withδ:=t−s, ifδ≤1, then
Eh
∥Xt−Xs∥2i
≲2βtδd+δ2R2.
Proof .
Eh
∥Xt−Xs∥2i
≲Ep
2βt(Bt−Bs)2
+δZt
sEh
∥Xr∥2i
dr≲2βtδd+δ2R2.
■
Similar to Chen et al. [2023c], we can also show that if we do forward process for time δ,qδwill be
close to q0inW2distance.
Lemma F.6. Suppose Assumption 4.1 holds. Let ϵW2>0. Ifβ2
t=t2andτ=T2, we choose
the early stopping parameter δ≤ϵ2/3
W2
(d+R√
d)1/3. Ifβt=tandτ=T, we choose δ≤ϵW2
(d+R√
d)1/2.
If consider pure VESDE (SMLD) (Equation (2)) with σ2
t=t, we choose δ≤ϵ2
W2
d. Then we have
W2(qδ, q0)≤ϵW2.
Proof .For the forward process Equation (3), we know that Xt:=mtX0+σtZ, where Z∼
normal (0 , Id)is independent of X0andmt≤1. Hence, for δ≲1,
W2
2(q0, qδ)≤(1−mt)2Eh
∥X0∥2i
+Eh
∥σδZ∥2i
.
Forβt=t2andτ=T2, we have that
W2
2(q0, qδ)≤δ3d+R2δ6
T2
Hence, we can take δ≤ϵ2/3
W2
(d+R√
d)1/3. For βt=tandτ=T, we have that
W2
2(q0, qδ)≤δ2d+R2δ4
T2
Hence, we can take δ≤ϵW2
(d+R√
d)1/2. For pure VESDE (Equation (2)) with σt=t, we have
W2
2(q0, qδ)≤δd .
■
31G Additional Synthetic Experiments
In this section, we do synthetic experiments to show the power of our new forward process with small
drift term in different setting.
G.1 The Synthetic experiments with accurate score function
In this section, we do numerical experiments on 2-dimension Gaussian distribution to show the power
of our new VESDE forward process in balancing different error sources.
Experiment Setting. We set the mean of target distribution E[q0] = [6 ,8], the covariance matrix
Cov[q0] =
25 5
5 4
, the diffusion time T= 2,τ=T2and the reverse beginning distribution is
N(0, T2I). We choose uniform stepsize γk=h,∀k∈[K]where h∈ {0.005,0.01,0.02,0.04}. For
score functions, we directly calculate the ground truth score function instead of learning it by the
score matching objective. We calculate the KL divergence between the generation distribution and
target distribution q0as the experiments.
The implementable algorithm. We choose three different VESDE forward processes in the
experiments: (1) aggressive βt=t2withτ=T2; (2) conservative βt=twithτ=T2and (3)
VESDE without drift term Equation (2) with σ2
t=t2. After determining the forward process, we
run the reverse SDE with the above γk, k∈[K]. For the discretization scheme, we choose two
common method: exponential integrator (EI) [Zhang and Chen, 2022] and Euler-Maruyama (EM)
discretization [Ho et al., 2020].
Observations. The experimental results are shown in Figure 2. We note that the red line (EI,
VESDE without drift, σ2
t=t2) and orange line (conservative drift VESDE, βt=tandτ=T2) has
a similar trend. Furthermore, the conservative drift VESDE has better performance compared to pure
VESDE without drift term. Hence, our new forward process is representative enough to represent
current VESDE, as discussed in Section 3.1.
The experimental results also support our theoretical results and show the power of the new forward
process in balancing different error terms. As shown in Figure 2, the process with aggressive βt=t2
with small drift term achieves the best and second performance in EI and EM discretization since it
can balance the reverse beginning and discretization. The third best process is conservative βt=t
with the small drift term. The reason is that though it can not achieve a exp (−T)forward process
guarantee, it also has a constant decay on prior information, as shown in Section 3.1. This decay
slightly reduces the effect of the reverse beginning error. The worse process is VESDE without drift
term since it is hard to balance different error sources. Our experimental results also show that EI
discretization is better than EM discretization.
G.2 The Synthetic experiments with approximated score function
In this section, instead of using an accurate score function, we train an approximated score function
on the pure VESDE (Equation (2)) without drift term on two synthetic datasets: multiple Swiss rolls
and 1-D GMM. Then, for the drift VESDE, we do not train the approximated score corresponding to
Equation (3); we directly use the approximated score learned by pure VESDE and show that the drift
VESDE can improve the generated distribution without the training process.
Datasets. The 1-D GMM distribution contains three modes:
3
10N(−8,0.01) +3
10N(−4,0.01) +4
10N(3,1).
For multiple Swiss rolls, we use a similar code compared to Listing 2 of Lai et al. [2023], except
Line 6. We change Line 6. to data /=10. to obtain a larger variance dataset. Each dataset contains
50000 datapoints.
The implementable algorithm. In this subsection, we choose two forward processes: (1) conserva-
tiveβt= 1withτ=T; (2) pure VESDE without drift term (Equation (2)) with σ2
t=t. To match
32Table 1: The KL divergence for pure VESDE (Equation (2)) and conservative drift VESDE with
different sampling method.
Forward Process1-D GMM Swiss roll
Reverse SDE PFODE Reverse SDE PFODE
Pure VESDE ( T= 100 ) 0.082 0.434 9.58 21.05
Drift VESDE ( T= 100 ) 0.043 0.249 8.71 7.77
Pure VESDE ( T= 625 ) 0.027 0.057 8.00 8.20
Drift VESDE ( T= 625 ) 0.025 0.031 7.95 7.21
our analysis, we choose two sampling methods for the reverse process: Euler-Maruyama method for
reverse SDE and RK45 ODE solver for the reverse PFODE method.
We note that although aggressive setting βt=tandτ=Thas shown its power in theory (Lemma C.2)
and the experiments with accurate score (Figure 2), other sampling issues may arise in practice. We
leave the experimental exploration for drift VESDE with aggressive βtas a future work.
The training detail. For each dataset, we train a score function with pure VESDE (Equation (2),
σ2
t=t). We train for 200epochs with batch size 200and learning rate 10−4. For both training and
inference, the start time is δ= 10−5. For the conservative VESDE, we directly adapt the checkpoint
learned by the pure VESDE since the conservative drift VESDE has a similar trend compared to pure
VESDE, as shown in Figure 2. The above experiments are runned over 5random seed and we present
the average over these seeds in Table 1.
The above experiments are conduct on a GeForce RTX 4090. It takes 25 minutes to train a score
function of pure VESDE.
Observation. We do experiments with T= 100 and lager T= 625 and these two choice show
similar phenomenon. In this paragraph, we first use T= 100 as an example to discuss the results. As
shown in Table 1, the conservative drift VESDE has smaller KL divergence compared to pure VESDE
under all sampling methods and datasets. From Figure 1 and Figure 4, it is clear that pure VESDE
has low density on the Swiss roll except the center one, which means that though pure VESDE can
deal with small E[q0], it is hard to deal with large dataset variance Cov[q0], as we discuss in Section 4.
For conservative drift VESDE ( βt= 1andτ=T), as we discuss in Section 3.1, there is a constant
decay on the prior information E[q0]andCov[q0], which is helpful in deal with large dataset mean
and variance. The experimental results support our augmentation. Figure 1 (c), Figure 4 (c) and
Figure 5 (c) show that the density of the generated distribution is more uniform compared to pure
VESDE, which means that the drift VESDE can deal with large dataset mean and variance.
We also do experiments with larger T= 625 . As we discuss in Section 4, larger Twill reduce the
influence of the prior data information and have greater generated distribution, as shown in Figure 4
(c) and Figure 4 (e). The experiments of 1D-GMM (Figure 5) show a similar phenomenon compared
to the multi Swiss rolls.
G.3 The Real-World Experiments on CelebA 256
After achieving great performance under the synthetic data, we show that our conservative drifted
VESDE can improve the results of pure VESDE without training.
Setting. In this experiment, we adapt well-known VESDE implementation [Song et al., 2020b]
and do experiments on CelebA datasets (size: 256∗256∗3). More specifically, we use
ve/celebahq _256_ncsnpp _continuous checkpoints provided by [Song et al., 2020b] and modify
the sampling process strictly according to our drifted VESDE. To do a fair comparsion, we fix the
random seed and use the reverse PFODE process. Then, we generate 10000 face images to calculate
the metrics. We note that when using this checkpoint and pure VESDE pipeline provided by [Song
et al., 2020b], the models would generate almost pure noise with a certain probability. Hence, we use
an aesthetic predictor [Schuhmann et al., 2022] (aesthetic score ≥5.5) to filter the generated images
to ensure that the images are clear faces.
33(a) Original Figure (b) VESDE ( 𝑇=100) (c) Drifted VESDE ( 𝑇=100)
(d) VESDE ( 𝑇=625) (e) Drifted VESDE ( 𝑇=625)Figure 4: Experiment results of Swiss roll with reverse PFODE
(a) Original Figure (b) VESDE ( 𝑇=625) (c) Drifted VESDE ( 𝑇=625)
Figure 5: Experiment results of 1D-GMM with reverse PFODE
Discussion. From the qualitative perspective, as shown in Figure 3 (Figure 6 and 7), the images
generated by our drifted VESDE have more detail (such as hair and beard details). On the contrary,
since pure VESDE can not deal with large variance, the images generated by pure VESDE appear
blurry and unrealistic in these details. From the quantitative results, our drifted VESDE achieves
aesthetic score 5.813, and IS 4.174, which is better than the results of baseline pure VESDE (aesthetic
score 5.807 and IS: 4.082). In conclusion, the real-world experiments show the potential of our
drifted VESDE.
We note that the goal of these experiments is to show that our conservative drifted VESDE is plug-
and-play without training instead of achieving a SOTA performance. Hence, we focus on the relative
improvement compared to the baseline [Song et al., 2020b]. There are two interesting empirical
future works. For the conservative drifted VESDE, we will do experiments on the SOTA pure VESDE
models [Karras et al., 2022] and improve their results without training. For the aggressive drifted
VESDE, since this process makes a larger modification compared with the conservative one, we need
to train a new score function instead of directly using a pre-train one to achieve better results.
34Figure 6: The real-world experiments on CelebA256 dataset (More examples)
Figure 7: The real-world experiments on CelebA256 dataset (Detail)
NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We propose a new drifted VESDE forward process and achieve the first
polynomial complexity for reverse SDE (Corollary 5.4). For reverse PFODE, we propose
the firs t quantitative convergence guarantee for VESDE (SOTA) (Theorem 6.1). We also do
synthetic experiments to support our results (Section 7).
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
35Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss future work and limitation at Section 8.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All assumptions, Theorem, Corollary, and proof sketch have been clearly
stated in the main content. The detailed proof appears in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
36We has shown all experiments detail including dataset and training detail in Appendix G.
Furthermore, we discuss why these experiments results support our theoretical results in
Section 7.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: As a theoretical work, we only do simple synthetic experiments to support our
results. All detail and the used checkpoint are shown in Appendix G.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
37•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We has shown all experiments detail including dataset and training detail in
Appendix G.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The results of Figure 2 and Table 1 are calculated over 5random seed.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We have shown the compute works and computation time in Appendix G.
38Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have checked the code of ethics and make sure that our work satisfies the
code of ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We have discussed the broader impacts of our work at the end of main paper.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
39Answer: [NA]
Justification: This paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
40Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
41