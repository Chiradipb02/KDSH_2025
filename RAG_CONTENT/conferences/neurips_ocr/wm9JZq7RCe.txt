An Analysis of Tokenization: Transformers under
Markov Data
Nived Rajaraman
UC Berkeley
nived.rajaraman@berkeley.eduJiantao Jiao
UC Berkeley
jiantao@berkeley.eduKannan Ramchandran
UC Berkeley
jiantao@berkeley.edu
Abstract
While there has been a large body of research attempting to circumvent tokenization
for language modeling (Clark et al., 2022; Xue et al., 2022), the current consen-
sus is that it is a necessary initial step for designing state-of-the-art performant
language models. In this paper, we investigate tokenization from a theoretical
point of view by studying the behavior of transformers on simple data generating
processes. When trained on data drawn from certain simple kth-order Markov
processes for k >1, transformers exhibit a surprising phenomenon - in the ab-
sence of tokenization, they empirically are incredibly slow or fail to learn the right
distribution and predict characters according to a unigram model (Makkuva et al.,
2024). With the addition of tokenization, however, we empirically observe that
transformers break through this barrier and are able to model the probabilities of
sequences drawn from the source near-optimally, achieving small cross-entropy
loss. With this observation as starting point, we study the end-to-end cross-entropy
loss achieved by transformers with and without tokenization. With the appropriate
tokenization, we show that even the simplest unigram models (over tokens) learnt
by transformers are able to model the probability of sequences drawn from kth-
order Markov sources near optimally. Our analysis provides a justification for the
use of tokenization in practice through studying the behavior of transformers on
Markovian data.
1 Introduction
The training of language models is typically not an end-to-end process. Language models are often
composed of a “tokenizer”, which encodes a sequence of characters into a sequence of token ids,
which map to substrings. The subsequent language modeling task is carried out by a neural network
or transformer, which is pre-trained and fine-tuned on large datasets. The ideal goal is to jointly
train the tokenizer and transformer with end-to-end accuracy as the objective. This is a challenging
problem to solve efficiently, and thus, the tokenizer is generally adapted on a portion of the training
dataset and frozen before the transformer is trained. In practice, byte-level/character level models
such as ByT5 (Xue et al., 2022) and CANINE (Clark et al., 2022) which avoid tokenization often
perform worse for the reason that semantic relationships can be harder to capture at the character
level (Libovick `y et al., 2021; Itzhak and Levy, 2021).
Though used most commonly, tokenization at the subword level often has sharp edges. Test sequences
may contain rare tokens which were never seen in the training dataset. The presence of such tokens
may induce undesirable behavior in the outputs of models (Rumbelow and Watkins, 2023; Kharitonov
et al., 2021; Yu et al., 2021) and present an attack surface for bad actors. Moreover, tokenized
models struggle on tasks that involve manipulation at the character level, such as spelling out words
or reversing sentences. For similar reasons, LLMs with standard tokenizers also struggle to carry
out basic arithmetic (Golkar et al., 2023). Despite this brittleness, tokenization is used in nearly all
state-of-the-art LLM architectures.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).In this paper, we introduce a statistical formulation for tokenization for next-word-prediction. We
study the class of models transformers are observed to express empirically under simple data generat-
ing processes, which often can have simpler descriptions. Taking a step back, rather than focusing
on proxy evaluation metrics, which lead to an ever-changing goalpost, we focus on understanding
the behavior of the end-to-end cross-entropy loss, L(·). In this paper, we study a simplification of
real world data generating processes and study the case where data sources are kth-order Markov
processes. Within this framework we can compare tokenizers against each other, and in the process
capture several interesting phenomena. Our main results are as follows,
1.There are very simple kth-order Markov processes such that in the absence of any tokeniza-
tion, transformers trained on data drawn this source empirically predict characters according
to a unigram model. This phenomenon is observed under a wide variety of hyperparameter
choices. This is problematic because unigram models such as that induced by the stationary
distribution are poor at modeling Markovian data and suffer from a high cross-entropy loss.
This phenomenon was also recently observed in Makkuva et al. (2024).
2.When trained with tokenization, transformers are empirically observed to break through this
barrier and are able to capture the probability of sequences under the Markov distribution
near-optimally. In other words, in the presence of tokenization, transformers appear to
achieve near-optimal cross-entropy loss. This phenomenon is observed with a multitude of
tokenizers used commonly in practice.
3.We analyze a toy tokenizer which adds all length- ksequences into the dictionary and show
that as dictionary size grows, unigram models trained on the tokens get better at modeling
the probabilities of sequences drawn from Markov sources. We then theoretically prove
that tokenizers used in practice, such as the LZW tokenizer (Zouhar et al., 2023a) and a
variant of the BPE tokenizer (Gage, 1994; Sennrich et al., 2016) which are learnt from
data also satisfy this property but require much smaller dictionaries to achieve any target
cross-entropy loss.
In our framework, the most challenging hurdle and the biggest departure from previous work such as
(Zouhar et al., 2023b) is the element of generalization - understanding how a tokenizer performs on
new sequences that it was not trained on. This generalization turns out to be a delicate phenomenon -
we show in Appendix D that there exist tokenizers which generalize poorly in the sense that they may
compress the dataset they are trained on into a short sequence of tokens, but fail to generalize to new
sequences. In Appendix E we show that there exist dictionaries which generalize well (in the sense
of having low cross-entropy loss) to new sequences under one encoding algorithm, but completely
fail to generalize under another.
1.1 Related Work
Tokenization has a long history of empirical study in natural language processing. In the literature,
a number of tokenizers have been developed for various domains such as math (Singh and Strouse,
2024), code (Zheng et al., 2023; Parr, 2013) and morphology-aware tokenizers for different languages
like Japanese (Tolmachev et al., 2018; Den et al., 2007) and Arabic (Alyafeai et al., 2023) among
many others. In modern LLMs, the most commonly used tokenizers are variants of BPE (Gage,
1994), Wordpiece (Schuster and Nakajima, 2012) and the Unigram tokenizer (Kudo, 2018) which
learn a dictionary from data, rather than hard-coding language dependent rules. There has been a
long line of work interpreting tokenization from various lenses (Grefenstette and Tapanainen, 1994;
Palmer, 2000; Zouhar et al., 2023b).
The theoretical study of transformers has also received much attention recently. We discuss the closest
relatives to our work below. Edelman et al. (2024) study the learning trajectory of transformers
trained on data drawn from 1st-order Markov chains. While the authors empirically observe that
the models eventually learn to predict tokens correctly according to the Markov kernel, simplicity
bias slows down optimization - the models initially predict tokens according to a unigram model (in
context unigrams), which delays learning the optimal solution. This phenomenon was also observed
in Makkuva et al. (2024). On the positive side, Nichani et al. (2024) study an in-context causal
learning task that generalizes learning in-context bigrams for 1st-order Markov processes and analyze
the trajectory of gradient descent.
20 1p
q
Figure 1: 2-state switching process. The above state diagram describes the distribution of Xn
conditioned on Xn−1.kth-order extension: the conditional probability of Xnonly depends on Xn−k
through the kernel, Pr(Xn= 1|Xn−k= 0) = pandPr(Xn= 0|Xn−k= 1) = q.
Notation. All logarithms are base e, unless specified otherwise. The Shannon entropy H(X)of
a categorical random variable Xis−P
x∈supp(X)p(x) logp(x).HBER(p)captures the entropy of a
Bernoulli random variable with parameter p. The notation Op,q,r(f(n))(likewise Ω{·}andΘ{·})
indicate that the underlying constant depends polynomially on the parameters p, qandrandeO(f(n))
(likewise, eΘandeΩ) ignores polylog( n)terms. For a set S,S⋆=∪∞
k=1Sk, the set of all sequences
with elements drawn from S. For a sequence t,ti:j= (ti,ti+1,···,tj)returns a slice.
2 Formulation
We consider a setting where the learner’s objective is to learn a language model which models
probabilities of sequences over an input alphabet A. The data to be modeled is generated according
to an unknown probability model P:A⋆→[0,1]over strings. A tokenizer is a tuple T=
(Dict,enc(·),dec(·)). Here Dict is a collection of tokens The encoding function enc(·):A⋆→
Dict⋆, maps strings of characters to a sequence of tokens, and likewise, the decoding function
dec(·):Dict⋆→ A⋆maps a sequence of tokens to a string of characters. We assume that the
tokenizer is “consistent”, namely, dec(enc(·))is the identity function.
We consider a setting where the learner has access to a training dataset which is a sequence of length
nsampled from a data source1. We study the likelihood maximization problem, where the objective
of the learner is to learn an end to end model such that the cross-entropy loss is minimized. In
the presence of tokenization, we have a model of the form Qend=Q◦enc(·)where Qis a joint
distribution across sequences of tokens when the tokenizer corresponding to enc(·)is used. The
cross-entropy loss, i.e. the log-perplexity, can be written down as,
Lm(Qend)≜−E[logQ(enc(s))], (1)
with the objective to minimize it. Here, the expectation is over s, a fresh test sequence of length
msampled from the data generating process. Fixing a tokenizer, let Qdenote a family of joint
distributions over tokens (i.e. likelihood models). The objective is to jointly design a tokenizer (with
encoding function enc(·)) and likelihood model Q∈ Q with small test loss Lm(Q◦enc(·)).
Finally, for a dictionary Dict, the unigram family of models, Q1-gram, is defined as below: Q∈ Q 1-gram
associates probability Q(t1,t2,···,tj) =Q#(j)Qj
i=1Qtok(ti)to the sequence of tokens t1,···,tj
for measures Q#andQtoksupported on NandDict respectively.
2.1 Data generating process
In this paper, we consider a simplification of real-world data generating processes by considering the
case where the data generating distribution is a kth-order Markov process over characters. Studying
the behavior of transformers trained on Markov data was the subject of the works Makkuva et al.
(2024) and Edelman et al. (2024), where a number of interesting phenomena were unearthed. When
a transformer is trained on data from certain simple Markov processes like the one considered in
Figure 1, a very peculiar phenomenon occurs - within a reasonably large number of iterations, the
transformer fails to improve beyond the loss incurred by the best unigram model. This phenomenon
is reproducible across a wide number of hyperparameters, including the number of feed-forward
layers in the model, the embedding dimension, and the number of attention heads. In Figure 3a this is
made clearer - the transformer fails to improve its test loss beyond that of the best unigram model.
1This can be thought of as the concatenation of all the individual sequences in the training dataset.
30 50 100 150 200 250 300 350 400 450 500
Sequence length0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19Token id
0.20.40.6Figure 2: Token distribution returned by the transformer tokenized by a learnt BPE encoder with a
dictionary size of 20. A test sequence is generated from the stochastic source and encoded into a
token sequence t. Each narrow vertical column represents the distribution over next tokens returned
by the transformer when the first xtokens of tare fed into the model, where xis varied from 0to the
length of t. For most values of x, the model appears to predict the same distribution over the next
token.
How bad can a unigram model be? It turns out that the gap between the cross-entropy of the best
unigram model and that of the optimal model can be characterized precisely.
Theorem 2.1. Consider any ergodic data source with stationary distribution over characters π. The
unconstrained optimal likelihood model achieves cross-entropy loss, minQLm(Q) =H(P). In
contrast, the cross-entropy loss under any unigram model Q∈ Q 1-gram satisfies, Lm(Q)≥mH(π).
The ratio of the optimal loss H(P), and the optimal unigram loss, mH(π)can be arbitrarily large. In
particular, for the switching chain in Figure 1, as p, q→0, the ratio diverges to ∞.
While transformers are a powerful class of models, it is concerning that they fail to learn very simple
distributions such as kth-order Markov processes. Why do they work so well in practice if they can
be so slow to learn Markovian data? It turns out that there is a simple missing ingredient in all the
architectures considered so far: tokenization. All the models trained in Figure 3a operate on raw
character sequences drawn from the stochastic source. To understand the role of tokenization, we
run another experiment and train the transformer on sequences generated from the stochastic source
which are encoded into tokens by a BPE tokenizer learnt from data. The transformer now operates on
sequences of tokens, rather than sequences of individual symbols. In Figure 3b we plot the results
of this experiment - in the presence of tokenization, the cross-entropy loss of the end-to-end model
breaks past the unigram barrier and approaches the optimal bound within a small number of iterations.
Let’s peek into the model a bit more and understand its behavior. In Figure 2 we run the following
experiment: we sample a random sequence of length 2000 from a Markov chain and feed it into the
transformer after tokenization, resulting in ≈500tokens. We plot the next-token distribution predicted
by the transformer at every single position in the input, generated by autoregressive masking. In
Figure 2 we stitch together these next-token distributions, each of which is a narrow column heatmap.
Visually, we observe that the plot is approximately homogeneous along the x-axis, implying that
the next-token distribution learned does not depend strongly on the prefix at that position. Thus the
transformer learns what is essentially a unigram model.
Thus, we come to a surprising conclusion: the behavior of the transformer on the kth-order switching
source in Figure 1 with and without tokenization is essentially the same. In both cases, the model
learns a unigram model over the tokens - in the absence of tokenization this unigram model is in
fact the stationary distribution induced by the source. If the transformer learns a unigram model in
both cases, how come there is such a large gap in performance between the two? To understand this
in more detail, we analyze a toy tokenizer. As a simplification, we will analyze the behavior of an
arbitrary, but exact unigram model under this tokenizer.
3 Unigram models under tokenization
Let’s consider a toy tokenizer which assigns all possible substrings of length ras tokens in the
dictionary and study what happens when a unigram model is trained on the tokenized sequences. The
40 25 50 75 100 125 150 175
Iteration0.500.751.001.251.501.752.002.25Test loss LBest transformer model
Optimal cross-entropy
Cross-entropy of best unigram model(a) The loss of the transformer fails to converge to the
optimal cross-entropy loss (dashed line) and instead
converges to that of the best unigram model (dotted
line). The shaded blue region captures how the test
loss curves vary as hyperparameters (number of layers,
embedding dimension etc.) are changed.
0 50 100 150 200 250 300
Iteration0.500.550.600.650.700.750.800.850.90Test lossLlayers =1
layers =2
layers =3
layers =4
Optimal cross-entropy
Cross-entropy of best unigram model(b) In the presence of tokenization, the test loss of the
model approaches the optimal bound (dashed line). It
is worth noting that the models trained here are sig-
nificantly smaller than those considered in Figure 3a,
having up to 70×fewer parameters and yet are able to
achieve the optimal cross-entropy loss.
Figure 3: Transformers trained on the order- 2switching Markov process (Figure 1) with p=q= 0.8.
On the left we have the model trained without tokenization and on the right the model uses BPE with
a dictionary of size 10learnt from data.
total dictionary size d= 2r. A sequence of characters is mapped to a sequence of tokens by simply
chunking it into a sequences of rcharacters which are replaced by the corresponding token index2.
The resulting stochastic process on the tokens is still Markovian, but over a state space of size 2r. For
any unigram model Qon the tokens, the cross-entropy loss can be written down as,
Lm(Q◦enc(·)) =EX
t∈enc(s)log(1/Qtok(t))
+ Θ(log( m)),
where we choose Q#= Unif([ m]), which contributes an additive log(m)to the loss. Choosing
Qtok(t) =π(t1)Qr−1
i=1P(ti+1|ti)as the stationary probability the Markov process associates with t,
1
mLm(Q◦enc(·))≈ −1
mE"
log(P(s) +Xm/k−1
i=0logπ(ski+1)
P(ski+1|ski)#
(i)≈1
mH(P) +1
mk 
mH(π)−H(P)
=H(P)
m
1−1
log2(d)
+H(π)
log2(d). (2)
the approximation in (i)uses the fact that as mgrows large,1
mPm/k
i=0log(P(ski+ℓ+1|ski+ℓ)ap-
proachesH(P)
k. With d= 2 (i.e.,r= 1), we recover the performance of the character tokenizer
in Theorem 2.1. An immediate implication of this simple calculation is that as m→ ∞ , there is a
unigram model which is nearly optimal as the dictionary size grows to ∞.
While this toy tokenizer allows us to glean this intuition behind why tokenization allows unigram
models to be near-optimal, there are some obvious issues. One, the tokenizer does not adapt to the
distribution of the data. Indeed, for the switching Markov source in Figure 1, as p=q=δ→0, the
source contains increasingly longer sequences of contiguous 0’s and 1’s. In this case, it makes since
to have a dictionary containing such sequences, rather than all possible length- rsequences, many of
which would be seen very few times (if at all) in a test sequence. At a more technical level, in eq. (2),
to get to a cross-entropy loss of 2H(P), the size of the dictionary required by the toy tokenizer is
emH(π)/H(P). As discussed in Example A.1 for the switching Markov process with p=q=δ, this
dictionary size can be extremely large and scales exponentially (in 1/δ) ase1/δlog(1/δ)when δis
2The last few characters which do not add up to rin total are left untokenized. These boundary effects will
not matter as the test sequences grow in length
5small. In general, on stochastic sources on a much larger alphabet, such as English/ASCII, this toy
tokenizer would result in a prohibitively large dictionary.
Larger dictionaries are usually correlated with the presence of rare tokens which appear infrequently
at training time. This presents a problem in practice - a lot more data is often required to see enough
examples of such tokens to learn good embeddings for them. More importantly, in the absence of
this volume of data, rare tokens present an attack surface to elicit undesirable behavior in the model
(Rumbelow and Watkins, 2023). In practice, this issue present with the toy tokenizer is, to an extent,
resolved by using tokenization algorithms such as BPE or Wordpiece, which learn dictionaries from
data. In the process, they are able to avoid learning extremely rare tokens, by enforcing a lower bound
on the number of their occurrences in the training data to be allocated as a token. By minimizing the
number of such rare tokens, the model is able to utilize its token budget in a more efficient manner.
We now introduce the main theoretical result of this paper, showing that with the appropriate
tokenization algorithm with a token budget of d, a unigram model is not only asymptotically able
to achieve the optimal cross-entropy loss, but also requires far smaller dictionaries to match the
performance of the toy tokenizer considered earlier. In order to avoid dealing with the transient
characteristics of the source, we consider the cross-entropy loss in eq. (1) under the assumption that
the test sequences sare of length m→ ∞ . Namely, define the normalized loss,
L(·) = lim
m→∞1
mLm(·)
Theorem 3.1. Consider a Markov data generating process which satisfies Assumption 3.2. Let d
denote a budget on the size of the dictionary. Then, there exists a tokenizer with at most dtokens and
encoding function enc(·), such that,
min
Q∈Q 1-gramL(Q◦enc(·))≤1
1−εmin
Q′L(Q′) (3)
where εislog(1/δ)/0.99 log( d)3. Furthermore, a tokenizer satisfying eq. (3)with probability
≥1−d−Ωδ(log(d))can be learnt from a dataset of eOδ(d)characters.
The tokenizers considered in this theorem are far more efficient with their token budget than the toy
tokenizer - to achieve a cross entropy loss within a factor 2of optimal, the dictionary size required
by these tokenizer is d≈1/δ2on any source satisfying Assumption 3.2. In comparison, the toy
tokenizer requires a dictionary size of e1/δlog(1/δ)to achieve the same error. We show that the LZW
tokenizer proposed in (Zouhar et al., 2023a) achieves the upper bound in eq. (3) when trained on a
dataset of size eO(d). Likewise, we also show that a sequential variant of BPE achieves the upper
bound in eq. (3) up to a factor of 2and with a worse dependency in εwhen trained on a dataset of size
eO(d2). What is interesting is that neither of these algorithms explicitly learn a unigram likelihood
model, Q, while constructing the dictionary. Yet they are able to perform as well as the tokenizers
which are jointly optimized with a likelihood model, such as the Unigram tokenizer (Kudo, 2018).
Key insight. While the toy tokenizer provides a high level intuition as to why tokenization might
enable unigram models to model Markov sources well, here we present a different explanation which
captures tokenization from an operational viewpoint. Tokenizers which do a good job at learning
patterns in the data and assigning these frequent patterns as tokens in the dictionary are compatible
with an i.i.d. model over tokens. A hypothetical example motivating this point: consider a tokenizer
such that the distribution of tokens in the encoding of a fresh string sampled from the source is
distributed i.i.d., except that whenever the token t′appears, it is always followed by t′′. An i.i.d.
model on the tokens is a poor approximation since P(t′t′′)≫P(t′)P(t′′). However, by merging t′
andt′′into a new token tand adding this to the dictionary, the new distribution over tokens is i.i.d.
In general, this motivates why it is desirable for a tokenizer to allocate new tokens to substrings
which appear next to each other frequently, i.e. a pattern in the data. As more tokens are added to
the dictionary, one might expect the cross-entropy loss incurred by the best unigram model to improve.
3.1 Learning patterns in the source
The main result of this section is a generic reduction: dictionaries which typically encode new strings
into a few long tokens (defined in a formal sense in Theorem 3.4), result in tokenizers achieving
3εis assumed to be <1in this statement. The constant 0.99can be made arbitrarily close to 1.
6near-optimal cross-entropy loss. We prove this result for Markovian sources under a regularity
assumption, which is that the associated connectivity graph of the chain is complete. The analogous
assumption for kth-order sources is that the transition kernel is entry-wise bounded away from 0.
This assumption is satisfied by all the sources considered in the paper thus far, such as the kth-order
switching processes in Figure 1.
Assumption 3.2 (Data generating process) .Assume that the data source is an ergodic Markov process
with transition P(·|·)and stationary distribution π. Assume that mina,a′∈AP(a′|a)≜δ >0.
Remark 3.3.Assumption 3.2 (and its kth-order extension) impose that there is a small but non-
zero probability of observing any particular symbol after any preceding sequence. This limits the
applicability of these processes in real-world scenarios where such a phenomenon may not occur.
However, our motivation for this assumption is different: δallows parameterizing the Markov process
in a way which interpolates between i.i.d. ( δ= 1/|A|) and highly non-i.i.d. ( δ→0).
For a substring sand a character a, define P(s|a) =P(s1|a)Q|s|
i=2P(si|si−1)denote the condi-
tional probability of the substring s. We now state the main result of this section.
Theorem 3.4 (Bound on cross-entropy loss of dictionaries under greedy encoder) .Consider a source
satisfying Assumption 3.2 and any tokenizer Tequipped with the greedy encoder, enc gre(·)with
finitely long tokens. Define, P(t) =Ea∼π[P(t|a)]and suppose H(QMLE, P)≥1
εlog(1/δ)for some
ε <1. Then,
min
Q∈Q1-gramL(Q◦enc gre(·))≤minQL(Q)
1−ε.
Interpretation. H(QMLE, P) =Et∼QMLE[log(1 /P(t))]is large when the encoder places higher
mass (i.e. larger values of QMLE(·)) on tokens which have low probability under P, i.e. which
correspond to longer substrings. Intuitively, this metric is higher for tokenizers which typically use
long tokens (i.e. low P(·)) to encode new strings.
3.2 LZW tokenizer
In this section we study the Lempel-Ziv-Welch (LZW) based tokenization scheme introduced by
Zouhar et al. (2023a) and establish guarantees of the form of Theorem 3.1 for this tokenizer.
Definition 3.5 (LZW tokenizer) .Iterating from left to right, the shortest prefix of the training dataset
which does not already exist as a token is assigned as the next token in the dictionary. This substring
is removed and the process is iterated on the remainder of the dataset. The tokenizer uses the greedy
encoding algorithm (Definition A.3) to encode new strings into tokens.
An example of the LZW tokenizer: For the dataset 0100111 , the dictionary created is {0,1,00,11}.
The LZW tokenizer is based on the LZW algorithm for compression (Ziv and Lempel, 1978; Welch,
1984). The dictionary satisfies the property that if some substring s′exists as a token in the dictionary,
then all of its prefixes must also belong to the dictionary. In the next theorem, we show that the LZW
tokenizer approximately achieves the optimal cross-entropy loss.
Theorem 3.6. Suppose the LZW tokenizer is trained on a dataset of length at most d(thereby learning
a dictionary with at most dtokens). For Markov sources satisfying Assumption 3.2, with probability
≥1−d−Oδ(log(d)), the resulting tokenizer satisfies,
min
Q∈Q1-gramL(Q·enc gre(·))≤minQL(Q)
1−ε.
where ε=log(1/δ)
0.99 log( d)4.
The proof of this result considers all substrings twith P(t)≥1/d0.99. These substrings are
reasonably high probability and observed many times in a dataset of eΩ(d)characters. We show that
with high probability, the LZW tokenizer learns allof these substrings as tokens in the dictionary.
Now, when processing a new string, since the greedy algorithm only emits the longest substring
4The constant 0.99can be made arbitrarily close to 1.
75 10 15 20 25 30
Training time (s)0.50.60.70.80.91.01.11.2Test lossLTokenized
Untokenized
Optimal cross-entropy
Cross-entropy of best unigram model(a) Convergence rate of smallest model which is within
10% of the optimal-cross entropy by 300epochs. The
smallest untokenized model has 9010 parameters ( 3
layers, embedding dimension = 10). The smallest to-
kenized model with a dictionary size of 10has17880
parameters ( 3layers, embedding dimension = 20 ).
The tokenized model has more parameters but the wall-
clock time taken to reach any loss value is smaller.
0 5 10 15 20 25 30
Training time (s)0.50.60.70.80.91.01.11.2Test lossLTokenized
Untokenized
Optimal cross-entropy
Cross-entropy of best unigram model(b) Convergence rate of models with the same embed-
ding dimension ( 20), number of heads ( 1) and layers
(3) with and without tokenization. The model with to-
kenization (dictionary size of 20) appears to converge
more quickly, but the error floor is subtly higher com-
pared to the model without tokenization. Both models
are trained on input sequences of length 512. The width
of the tokenized model is smaller ( 145).
Figure 4: Test loss vs. wall-clock time for the tokenized and untokenized models when trained on the
order- 1switching Markov chain (Figure 1) with p=q= 0.8. The tokenizer used is BPE.
which matches a token, every token allocated must fall on the “boundary” of this set, having P(t)≤
O(1/d0.99). By definition, this means that H(QMLE, P) =Et∼QMLE[log(1 /P(t))] = 0 .99 log( d).
Combining this with Theorem 3.4 completes the proof. At a high level, on the infinite tree of
substrings A⋆we study which nodes are populated as tokens by LZW. This structure forms a Digital
Search Tree (DST) and prior work analyzes the mean and variance of the profile of the DST under
various source processes (Jacquet et al., 2001; Drmota and Szpankowski, 2011; Hun and Vallée, 2014;
Drmota et al., 2021). A detailed proof of Theorem 3.6 is provided in Appendix A.6.
4 Experimental Results
Experiment 1 (Figures 4a and 4b) In this experiment we study the order- 1switching Markov
chain. Transformers without tokenization empirically achieve a small cross-entropy on this learning
task as seen in Figure 4a and earlier in Makkuva et al. (2024). We vary hyperparameters to find the
smallest untokenized model which achieves a loss within 10% of the optimal-cross entropy within
300epochs. Fixing a token dictionary size of 20, we also find the smallest tokenized model which
achieves the same loss. Although the smallest model with tokenization is larger than the smallest
model without tokenization in terms of the number of parameters, the wall-clock time taken to
optimize the model to any target test loss is observed to be smaller. Thus, tokenization appears
to reduce the compute time required to train the model to a target test loss in the toy example we
consider. In Figure 4b we compare models with the same architecture trained with and without
tokenization5. The model with tokenization appears to converge more quickly, although the limiting
error achieved is subtly higher in comparison with the model without tokenization.
Experiment 1 (Figure 5). In this experiment, we train tokenizers on the Wikitext-103-raw-v1
dataset (Merity et al., 2016) and compare the performance of unigram models trained on the GLUE
dataset as the model size scales. Since the character-level tokenizer operates on a fixed vocabulary,
in order to compare with the other tokenizers, we plot the number of unique k-grams observed in
the training dataset along the x-axis. While this is not an apples-to-apples comparison, we use the
number of unique k-grams in the dataset as a proxy for the complexity of the likelihood model trained.
5The model with tokenization has width equal to the typical length of sequences after encoding, which is
smaller.
8One may also use the total number of possible k-grams as a proxy; however a large fraction of these
k-grams would likely never be observed in a real dataset (especially as kgrows).
Experiment 2 (Table 1). In this experiment, we compare the cross entropy loss of the best unigram
model trained on pre-trained tokenizers on an array of datasets. All the considered tokenizers have
dictionary sizes in the range 31K-51K. The best bigram model under the character tokenizer is
consistently outperformed by the best unigram likelihood model trained under a number of pre-
trained tokenizers on a variety of datasets: Rotten Tomatoes (8.5K sequences), GLUE (105K), Yelp
review (650K) and Wikitext-103-v1 (1.8M).
0 20000 40000 60000 80000
Size of dictionary0.81.01.21.41.61.82.0Cross-entropy loss of learnt unigram model1e7
1-gram (159)
2-gram (3393)
3-gram (23098)
4-gram (89524)LZW
BPE
Unigram
Wordpiece
Character
Figure 5: Performance vs. dictionary size. Tokeniz-
ers are trained on the Wikitext-103 dataset. For all
other tokenizers we train unigram models while for
the the character-level tokenizer, we train k-gram
models for k∈ {1,2,3,4}. Likelihood models are
trained on the GLUE dataset. The parentheses in-
dicates the number of distinct observed k-grams,
which lower bounds the k-gram model complexity.RT Wiki Yelp GLUE
BERT 1.58 1.55 1.60 1.50
Tinyllama 1.75 1.84 1.82 1.70
GPT-neox 1.57 1.64 1.66 1.48
Mistral 1.69 1.80 1.75 1.66
Phi-2 1.54 1.62 1.64 1.45
Character 2.40 2.45 2.46 2.38
Table 1: Cross-entropy loss estimates (using
eq. (55)) of unigram models trained on pre-
trained tokenizers under a number of datasets.
The last row (blue) is the character level tok-
enizer, on which a more powerful bigram model
is trained. BERT is based on Wordpiece, and
the remaining tokenizers are BPE based. The
character-level tokenizer we use is ByT5.
4.1 Additional theoretical results
We present some additional theoretical results in the appendix which we discuss briefly below. In
Appendix B, we do a theoretical study of the cross-entropy loss achieved by the popular BPE tokenizer.
We show that a variant of BPE achieves the upper bound on the RHS of eq. (3) (Theorem 3.1) up to a
factor approaching 2as the dictionary size grows. It is an interesting question for future research to
understand whether this factor of 2can be removed, since transformers are observed to achieve the
near-optimal cross-entropy loss as the dictionary size grows (cf. Figure 3b). In Appendix C, we prove
finite sample bounds on the end-to-end model under the LZW tokenizer with a smoothed empirical
estimator as the unigram model. This analysis reveals that there is a sweet spot for the dictionary
size - too small a dictionary, and the statistical error floor is significant, too large a dictionary, and the
statistical error incurred by the likelihood model dominates the overall loss. We also take a closer
look into the aspect of generalization for tokenizers, which arises from the fact that the tokenizer
is evaluated on data that it was not trained on. Prior work such as Zouhar et al. (2023b) show that
BPE is an approximation algorithm for finding the sequence of merges which minimizes the size
of the compressed dataset. This does not imply any guarantees on the end-to-end performance,
or even compression power of the tokenizer on new sequences. In particular, in Appendix D we
show that there exist tokenizers which compress the dataset into a short sequence of tokens, but
do so in a way which fails to generalize to new sequences. Thus measuring the performance of
a tokenizer necessitates understanding its behavior on data it was not trained on. In Appendix E,
we show a different kind of intricacy - there exist tokenizers under which the best unigram model
achieves low cross-entropy loss. However, the same dictionary under a different encoding algorithm
performs nearly as poorly as the character-level tokenizer. The interaction between the dictionary and
encoding algorithm is a poorly studied subject in the tokenization literature; this result emphasizes
the importance of understanding this relationship.
95 Open questions
In this section, we discuss some limitations of our work and open questions stemming from them. We
show that when transformers are trained with or without tokenization, they learn to approximately
represent k-gram models for different values of k. Transformers are capable of representing far more
complex behavior, which are elicited under more complex data generating processes. Extending
our formulation to these settings presents an avenue to develop an even better understanding of
tokenization, and would allow finer-grained comparisons between tokenizers. The behavior and role
of tokenizers may be very different in these contexts. Below we discuss some concrete questions.
Our theory assumes that the underlying Markov chain has every transition occurring with non-zero
probability, which is a limitation. However, the analysis for the toy tokenizer in eq. (2) shows that
when the dictionary size scales as exp(mH(π)/H(P)), even in the absence of Assumption 3.2, the
tokenizer achieves the optimal cross-entropy to within a factor of 2. This leads to the following
conjecture.
Conjecture 1. In the spirit of eliminating Assumption 3.2, is it possible to establish a version of
Theorem 3.1 applicable to data drawn from any Markov chain, where ε= log(1 /δ)/0.99 log( d)is
replaced by ε= log( mH(π)/H(P))/0.99 log( d).
In Appendix B, we analyze a variant of the BPE tokenizer, which carries out a version of sample
splitting, and establish a weaker variant of Theorem 3.1 for this tokenizer. This is to simplify the
statistical dependencies arising from the fact that while learning its dictionary, BPE makes a run over
the entire training dataset each time a new token is added. It remains an open question to analyze and
establish a variant of Theorem 3.1 for the standard BPE tokenizer.
6 Conclusion
We present a theoretical framework to compare and analyze different tokenization algorithms. We
study the end-to-end cross-entropy loss of the tokenizer + likelihood model, and focus on the case
where the data generating process is Markovian. We empirically observe that transformers with
tokenization are drastically more efficient at learning kth-order Markov processes, compared to
without tokenization. We prove that algorithms such as LZW and a sequential variant of BPE learn
tokenizers such that the best unigram likelihood model trained on them approaches the cross-entropy
loss of the optimal likelihood model, as the vocabulary size dgrows.
7 Acknowledgements
JJ and NR were partially supported by NSF Grants IIS-1901252 and CCF-2211209. KR was partially
supported by NSF Grant CCF-2211209.
References
Zaid Alyafeai, Maged S Al-shaibani, Mustafa Ghaleb, and Irfan Ahmad. Evaluating various tokenizers
for arabic text classification. Neural Processing Letters , 55(3):2911–2933, 2023.
Dietrich Braess and Thomas Sauer. Bernstein polynomials and learning theory. Journal of Approxi-
mation Theory , 128(2):187–206, 2004.
Yen-Chi Chen. Stochastic modeling of scientific data, Autumn 2018.
Jonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efficient
tokenization-free encoder for language representation. Transactions of the Association for Compu-
tational Linguistics , 10:73–91, 2022.
Yasuharu Den, Toshinobu Ogiso, Hideki Ogura, Atsushi Yamada, Nobuaki Minematsu, Kiyotaka
Uchimoto, and Hanae Koiso. The development of an electronic dictionary for morphological
analysis and its application to japanese corpus linguistics, Oct 2007. URL https://repository.
ninjal.ac.jp/api/records/2201 .
10Michael Drmota and Wojciech Szpankowski. The expected profile of digital search trees. Journal of
Combinatorial Theory, Series A , 118(7):1939–1965, 2011.
Michael Drmota, Michael Fuchs, Hsien-Kuei Hwang, and Ralph Neininger. Node profiles of
symmetric digital search trees: Concentration properties. Random Structures & Algorithms , 58(3):
430–467, 2021.
Benjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution
of statistical induction heads: In-context learning markov chains. arXiv preprint arXiv:2402.11004 ,
2024.
Tanja Eisner, Bálint Farkas, Markus Haase, and Rainer Nagel. Operator theoretic aspects of ergodic
theory , volume 272. Springer, 2015.
Philip Gage. A new algorithm for data compression. C Users Journal , 12(2):23–38, 1994.
Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik,
Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et al. xval: A continuous number
encoding for large language models. arXiv preprint arXiv:2310.02989 , 2023.
Robert M Gray and RM Gray. Probability, random processes, and ergodic properties , volume 1.
Springer, 2009.
Gregory Grefenstette and Pasi Tapanainen. What is a word, what is a sentence?: problems of
tokenisation. 1994.
Yanjun Han, Soham Jana, and Yihong Wu. Optimal prediction of markov chains with and without
spectral gap. Advances in Neural Information Processing Systems , 34:11233–11246, 2021.
Kanal Hun and Brigitte Vallée. Typical depth of a digital search tree built on a general source. In 2014
Proceedings of the Eleventh Workshop on Analytic Algorithmics and Combinatorics (ANALCO) ,
pages 1–15. SIAM, 2014.
Itay Itzhak and Omer Levy. Models in a spelling bee: Language models implicitly learn the character
composition of tokens. arXiv preprint arXiv:2108.11193 , 2021.
Philippe Jacquet, Wojciech Szpankowski, and Jing Tang. Average profile of the lempel-ziv parsing
scheme for a markovian source. Algorithmica , 31:318–360, 2001.
Eugene Kharitonov, Marco Baroni, and Dieuwke Hupkes. How bpe affects memorization in trans-
formers. arXiv preprint arXiv:2110.02782 , 2021.
Taku Kudo. Subword regularization: Improving neural network translation models with multiple
subword candidates. arXiv preprint arXiv:1804.10959 , 2018.
N Jesper Larsson and Alistair Moffat. Off-line dictionary-based compression. Proceedings of the
IEEE , 88(11):1722–1732, 2000.
Jindˇrich Libovick `y, Helmut Schmid, and Alexander Fraser. Why don’t people use character-level
machine translation? arXiv preprint arXiv:2110.08191 , 2021.
Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim,
and Michael Gastpar. Attention with markov: A framework for principled analysis of transformers
via markov chains. arXiv preprint arXiv:2402.04161 , 2024.
Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell,
S Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843 , 2016.
Jaouad Mourtada and Stéphane Gaïffas. An improper estimator with optimal excess risk in misspeci-
fied density estimation and logistic regression. The Journal of Machine Learning Research , 23(1):
1384–1432, 2022.
11Assaf Naor, Shravas Rao, and Oded Regev. Concentration of markov chains with bounded moments.
2020.
Gonzalo Navarro and Luís MS Russo. Re-pair achieves high-order entropy. In DCC , page 537.
Citeseer, 2008.
Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with
gradient descent. arXiv preprint arXiv:2402.14735 , 2024.
David D Palmer. Tokenisation and sentence segmentation. Handbook of natural language processing ,
pages 11–35, 2000.
Terence Parr. The Definitive ANTLR 4 Reference . Pragmatic Bookshelf, Raleigh, NC, 2 edition,
2013. ISBN 978-1-93435-699-9. URL https://www.safaribooksonline.com/library/
view/the-definitive-antlr/9781941222621/ .
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Jessica Rumbelow and Matthew Watkins. Solidgoldmagikarp. https://www.alignmentforum.
org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation , 2023.
Mike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE international
conference on acoustics, speech and signal processing (ICASSP) , pages 5149–5152. IEEE, 2012.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1715–1725, Berlin,
Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162.
URL https://aclanthology.org/P16-1162 .
Aaditya K Singh and DJ Strouse. Tokenization counts: the impact of tokenization on arithmetic in
frontier llms. arXiv preprint arXiv:2402.14903 , 2024.
Arseny Tolmachev, Daisuke Kawahara, and Sadao Kurohashi. Juman++: A morphological analysis
toolkit for scriptio continua. In Eduardo Blanco and Wei Lu, editors, Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing: System Demonstrations ,
pages 54–59, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-2010. URL https://aclanthology.org/D18-2010 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.
Terry A. Welch. A technique for high-performance data compression. Computer , 17(06):8–19, 1984.
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam
Roberts, and Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models.
Transactions of the Association for Computational Linguistics , 10:291–306, 2022.
Sangwon Yu, Jongyoon Song, Heeseung Kim, Seong-min Lee, Woo-Jong Ryu, and Sungroh Yoon.
Rare tokens degenerate all tokens: Improving neural text generation via adaptive gradient gating
for rare token embeddings. arXiv preprint arXiv:2109.03127 , 2021.
Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang
Wang. Outline, then details: Syntactically guided coarse-to-fine code generation. In International
Conference on Machine Learning , pages 42403–42419. PMLR, 2023.
Jacob Ziv and Abraham Lempel. Compression of individual sequences via variable-rate coding.
IEEE transactions on Information Theory , 24(5):530–536, 1978.
12Vilém Zouhar, Clara Meister, Juan Gastaldi, Li Du, Mrinmaya Sachan, and Ryan Cotterell. To-
kenization and the noiseless channel. In Anna Rogers, Jordan Boyd-Graber, and Naoaki
Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , pages 5184–5207, Toronto, Canada, July 2023a.
Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.284. URL https:
//aclanthology.org/2023.acl-long.284 .
Vilém Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Tim Vieira, Mrinmaya Sachan, and Ryan
Cotterell. A formal perspective on byte-pair encoding. arXiv preprint arXiv:2306.16837 , 2023b.
13Appendix
Contents
A Analysis of LZW: Proofs of Theorems 3.4 and 3.6 14
A.1 Notation and definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 A basic result about the optimal achievable cross-entropy loss . . . . . . . . . . . . 14
A.3 Proof of Theorem 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.4 Maximum likelihood unigram model . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.5 Proof of Theorem 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.6 Heavy-hitter dictionaries and a proof of Theorem 3.6 . . . . . . . . . . . . . . . . 17
B Additional Theoretical Results I: A sequential variant of BPE 22
B.1 Analysis of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.2 Analysis for the large dictionary case: |Dict|> d 0. . . . . . . . . . . . . . . . . . 25
B.3 Analysis in the small dictionary case . . . . . . . . . . . . . . . . . . . . . . . . . 32
C Additional Theoretical Results II: Learning the likelihood model 35
C.1 Proof of Theorem C.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
D Additional Theoretical Results III: The generalization ability of tokenizers 40
EAdditional Theoretical Results IV: Interaction between the dictionary and encoding
algorithm 41
E.1 Stochastic source and dictionary. . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
E.2 Minimal encoder achieves the optimal cross-entropy loss up to a constant. . . . . . 42
E.3 Greedy-encoder achieves poor cross-entropy loss . . . . . . . . . . . . . . . . . . 44
F Experiment details 46
G NeurIPS Paper Checklist 48
A Analysis of LZW: Proofs of Theorems 3.4 and 3.6
A.1 Notation and definitions
For each character a∈ A letT⋆
adenote an infinite tree, with root vertex ∅, and subsequent vertices
labelled by strings t∈ A⋆. The edge from a parent vertex tto any child ta′is labelled with
the probability P(ta′|t)unless t=∅, in which case the edge probability is P(a′|a). An infinite
trajectory sampled on the tree T⋆
acorresponds to an infinite string sampled from the stochastic source
conditioned on the first character of the string being a. In this paper we only consider ergodic sources
(Gray and Gray, 2009) for which we can define the “entropy rate”. The entropy rate fundamentally
captures the compressibility of the source, and can be defined as H∞≜limm→∞1
mH(P)where s
is a length mstring drawn from the source. By Theorem 2.1, H∞, captures minQL(Q).
A.2 A basic result about the optimal achievable cross-entropy loss
The ratio of H(P)andmH(π)can be made arbitrarily large for the switching Markov chains in
Figure 1 as the switching probabilities pandqapproach 0or1. See Example A.1 for more details.
Example A.1.Consider the switching Markov process in Figure 1 on {0,1}withp=q= 1−δ.
For this process, limm→∞1
mH(P) =HBer(δ) =δlog(1/δ) + (1 −δ) log(1 /(1−δ)), but π=
{1/2,1/2}and so H(π) =HBer(1/2) = log(2) . The ratio limm→∞mH(π)
H(P)goes to ∞asδ→0.
14A.3 Proof of Theorem 2.1
We first characterize the minimum achievable cross-entropy loss Lm(Q)without any restrictions
on the likelihood model class Q. Choosing Q(enc(s)) = Q(s) =P(s), the true probability of
the sequence s, we have Lm(Q◦enc(·)) = H(s)where H(·)is the entropy function. It is not
that difficult to see that this is also the minimum cross-entropy loss that can be achieved. For any
distribution Q,
Lm(Q) =E[log(1 /Q(s)]
=E[log(P(s)/Q(s)] +E[log(1 /P(s))]
=H(P) +DKL(P∥Q).
On the other hand, the cross-entropy loss under any unigram model Q∈ Q 1-gram satisfies,
1
mLm(Q◦enc(·))(i)=−1
mmX
i=1E[logQtok(ti)]−1
mE[logQ#(m)]
(ii)
≥ −X
a∈Aπ(a) logQtok(a)
≥H(π)
where in (i), we use the definition of the unigram model Q, and in (ii),πis the stationary distribution
over characters induced by the stochastic source, and the ergodicity of the source is used. The last
equation lower bounds H(X, Y)≥H(X).
A.4 Maximum likelihood unigram model
A number of our results (Theorems 3.4 and 3.6 to name a few) are related to bounding
minQ∈Q1-gramL(Q◦enc(·))for some tokenizer T. In this section we introduce the maximum
likelihood unigram model which captures the optimizer over Qfor any given tokenizer.
For the character level tokenizer, an examination of Theorem 2.1 shows that the optimal unigram
likelihood model associates probability Qtok(a) =π(a), i.e. the limiting fraction of times the
character ais observed in the sequence. More generally, for a non-trivial tokenizer, the corresponding
optimal unigram model Q⋆
tok(t)ends up being the limiting expected fraction of times tis observed
in an encoding of a sequence. This is the maximum likelihood unigram model, which we formally
define below. The unigram MLE likelihood model associates probability,
QMLE(t)←lim
m→∞E"
ntP
tnt#
(4)
to each token, where ntis the random variable capturing the number of occurrences of the token t
in the encoding of the length- mstring s. Restricting the class of likelihood models to the unigram
models, Q1-gram,QMLEcaptures the model which minimizes eq. (1).
The unigram MLE model cannot be computed without an infinite amount of data, but can be
approximated well with a finite amount of data, which forms the basis for Theorem C.1. For
certain encoding algorithms, we can show that the quantity nt/P
tntasymptotically converges to
its expectation (Lemma A.4). This is the reason the unigram model in eq. (4) is referred to as a
“maximum likelihood” model, since limm→∞nt/P
tntis the limit as |s|=m→ ∞ of the solution
to the following likelihood maximization problem: given a sequence s, find the distribution over
tokens, Q, which maximizes
Y
t∈enc(s)Q(t)≡Y
t∈Dict 
Q(t)nt.
As discussed previously, the unigram MLE model over tokens in eq. (4) induces a joint distribution
over sequences of tokens by looking at the product of the marginal probabilities of the composed
tokens; in particular,
QMLE(t1,···,tj) =QMLE(j)jY
i=1QMLE(ti),
15where QMLE(j)is a distribution on the total number of tokens generated and is instantiated as
Unif([m]).
Remark A.2.Note that the unigram MLE model specifies a distribution over tokens which is a function
of the underlying encoding algorithm, enc(·). Different encoders result in different population level
distributions over tokens, and consequently different unigram MLE models.
Definition A.3 (greedy encoder) .Given a dictionary Dict, the greedy encoder enc gre(s)encodes a
string sinto tokens by greedily matching from left to right, the largest substring that exists as a token
inDict. This substring is then removed and the process iterated on the remainder of s. The greedy
decoder dec gre(·)is a lookup table - a sequence of tokens is decoded by replacing each occurrence of
a token by the corresponding substring it maps to in Dict.
Lemma A.4. limm→∞ntP
t′nt′a.s.= lim m→∞Eh
ntP
t′nt′i
for any tokenizer having a finite vocabulary
and finitely long tokens, using the greedy encoder.
Proof. This result is essentially true because under the greedy encoder, the tokens in an encoding of
a fresh string tmay be generated by an rth-order Markov process for some r. For such processes, the
Cesàro average of the state distributions converges to a stationary distribution of the process (i.e., the
Krylov–Bogolyubov argument).
Tokens are generated as follows. Suppose the previous tokens generated were t1,t2,···,ti. The
next token ti+1is sampled by drawing an infinite trajectory from T⋆
afora∼P(·|ti)and returning
the longest prefix tof this trajectory which is a token in Dict, conditional on satisfying the conditions,
tjtj+1···tit̸∈Dict for all j∈ {1,2,···, i}. This process is repeated sequentially to generate all
the tokens.
Suppose the length of the longest token in the dictionary is ℓmax. Then, the distribution from a which
a token is sampled depends on at most the previous ℓmaxtokens. The reason for this is that the
dependency of the (i+ 1)thtoken, ti+1, on the previously sampled tokens emerges in the constraint
tjtj+1···titi+1̸∈Dict, satisfied by any candidate ti+1. Since each token is of length at least one,
this condition is vacuously satisfied if j < i−ℓmax.
With this view, the evolution of the state, defined as state r= (trℓmax,trℓmax−1,···,t(r−1)ℓmax)
evolves in a Markovian fashion. By the Krylov–Bogolyubov argument (cf. Proposition 4.2 in
Chen (2018)), the time averaged visitation frequencies of a Markov chain coordinate-wise asymp-
totically converges to its expectation, almost surely. This expectation exists by Theorems 8.5 and
8.22 of Eisner et al. (2015) which shows that for a matrix Asuch that supt∈N∥At∥op<∞the
limit limt→∞1
tPt
i=1Aiexists. For the finite-state Markov transition Awhich captures the token
generation process, condition supt∈N∥At∥op≤ |Dict|ℓmax<∞. This means that the limit of
the time averaged state distribution exists. Moreover, for any initial distribution π0over tokens,
π= lim t→∞1
tPt
i=1π0Aisatisfies the condition πA=π, implying that the limiting time-averaged
state distribution is a stationary distribution of A. Since the limiting time-averaged measure on
the state state r= (trℓmax,···,trℓmax−1,···,t(r−1)ℓmax)exists, this implies that the limiting time-
averaged measure of trℓmax−r′for each r′∈ {0,1,···, ℓmax}exists. By taking the uniform average
overr′andr, the limiting time-averaged measure of tioveri∈Nexists.
A.5 Proof of Theorem 3.4
Consider a string sof length m→ ∞ which is encoded into a sequence of tokens (ti:i∈
[|enc gre(s)|]). By the Asymptotic Equipartition Property (AEP) for ergodic sources, i.e. the Shan-
non–McMillan–Breiman theorem,
Pr
lim
m→∞−1
mlogP(s) =H∞
= 1. (5)
Here limm→∞H(P)
malso happens to be the entropy rate of the source. We use this property to bound
the length of the greedy encoding, |enc gre(s)|. Indeed, the probability of smay be decomposed as,
P(s) =P(t1)|enc gre(s)|Y
i=2P 
titi−1)
≤|enc gre(s)|Y
i=1max
a∈AP 
tia
.
16Noting that δminaP(t|a)≥max aP(t|a), up to a δfactor we may replace the max over aby an
expectation over a∼πwhere πis the stationary distribution of the stochastic source. In particular,
P(s)≤|enc gre(s)|Y
i=1P(ti)/δ.
By invoking the AEP, eq. (5),
lim
m→∞1
mX|enc gre(s)|
i=1−log 
P(ti))−log(1/δ)a.s.
≤H∞
Recall that the greedy encoder satisfies Lemma A.4 and for any t∈Dict,limm→∞nt
|enc gre(s)|a.s.=
QMLE(t). Furthermore, note that for any token t∈Dict,P(t)> δ|t|>0, and|enc gre(s)| ≤m
surely. By almost sure convergence,
lim
m→∞|enc gre(s)|
mX
t∈Dict−nt
|enc gre(s)|
log 
P(t)−log(1/δ)
a.s.= lim
m→∞|enc gre(s)|
m 
H(QMLE, P)−log(1/δ)
Furthermore, utilizing the assumption that εH(QMLE, P)≥log(1/δ)satisfied by the tokenizer,
lim
m→∞(1−ε)|enc gre(s)| 
H(QMLE, P)
ma.s.
≤H∞. (6)
Now we are ready to bound the expected cross-entropy loss of the tokenizer. Define the unigram model
Pπ(t1,t2,···,tj) =Punif(j)Qj
i=1P(ti)where Punifis the uniform measure over [m]. Note that we
have the inequality minQ∈Q1-gramlimm→∞1
mLm(Q◦enc gre(·))≤limm→∞1
mLm(Pπ◦enc gre(·))
and therefore, it suffices to upper bound the RHS. In particular,
Lm(Pπ◦enc gre(·)) =−E[logPunif(|enc gre(s)|)]−EX
t∈enc gre(s)log 
P(t)
≤log(m)−EX
t∈enc gre(s)log 
P(t)
(7)
where the last inequality uses the fact that Punif(|enc gre(s)|) = 1 /m. Note that as m→ ∞ , by
assumption on the tokenizer, the fraction of times the token tappears in the encoding of sconverges
almost surely converges to QMLE(t). Since |enc gre(s)| ≤msurely and P(t)> δ|t|>0, by an
application of the Dominated Convergence Theorem,
−lim
m→∞1
mEX
t∈enc gre(s)log 
P(t)
=−lim
m→∞1
mE
|enc gre(s)| ·X
t∈DictQMLE(t) log 
P(t)
= lim
m→∞1
mE
|enc gre(s)|H(QMLE, P)
(8)
Combining eq. (7) with eq. (8) and setting limm→∞log(m)/m= 0, and invoking eq. (6),
min
Q∈Q1-gramlim
m→∞1
mLm(QMLE◦enc gre(·)) = lim
m→∞1
mE
|enc gre(s)|H(QMLE, P)
≤H∞
1−ε. (9)
By Theorem 2.1, we have that minQlimm→∞1
mLm(Q◦enc gre(·)) = lim m→∞H(P)
m=H∞, which
uses the fact that the source is ergodic. Combining with eq. (9) completes the proof.
A.6 Heavy-hitter dictionaries and a proof of Theorem 3.6
In this section we prove Theorem 3.6 and introduce the notion of a heavy-hitting dictionary. At a
high level, these dictionaries contain all the substrings which have reasonably high probability of
being observed many times in a dataset of size n=eΩδ(d). We first show in Lemma A.6 that heavy
hitting dictionaries generalize well in the sense of having H(QMLE, P)being large (in conjunction
with Theorem 3.4 this implies an upper bound on the cross-entropy loss of the best unigram model).
Next, we will prove that the LZW algorithm (Definition 3.5) results in a heavy hitting dictionary with
high probability.
17Figure 6: The circled nodes indicates substrings which are tokens in Dict. Red nodes indicate the
set of “maximal tokens”, which are the set of tokens which the greedy encoder assigns, leaving out
those which can only be assigned as the last token of some string. Tokens like “ b” are never assigned
by the greedy encoder (save as the last token of the encoding of a string) since any sufficiently long
trajectory starting with bmust have a longer prefix which is also a token, namely, one of ba,bc,bba,
bbborbbc. The vertices of the tree which are assigned by the greedy encoder as tokens (together with
all their prefixes) forms a cut of the tree, which marks the dotted red line. The heavy hitting property
asserts that this cut is uniformly far away from the root node ∅, and that every vertex smarked red
hasP(s)≤1/dβ.
Definition A.5 (β-heavy-hitting dictionary) .A token tof a dictionary is said to be maximal if there
exists an arbitrary substring containing tas a strict prefix, and in addition, tis also the largest prefix
of the substring which is a token. A dictionary Dict is said to be β-heavy hitting if the set of maximal
tokens is a subset of {s′:max a∈AP(s′|a)≤1/dβ}.
A pictorial depiction of the heavy hitting property is illustrated in Figure 6.
Lemma A.6. For a β-heavy-hitting dictionary, with the greedy encoder, H(QMLE, P)≥βlog(d).
Proof. Note that the greedy encoder assigns tokens only among the set of maximal substrings (save
for potentially the last token). If every maximal substring has max a∈AP(s|a)≤1/dβ, by the
heavy-hitting property, for any token t,
P(t)≤max
a∈AP(s′|a)≤1/dβ.
Therefore,
H(QMLE, P) =Et∼QMLE[log(1 /P(t))]≥βlog(d).
Define Mβ={t:max a∈AP(t|a)≥δ/dβ}. These are the set of “high-probability” substrings
under the stochastic source. We will show that for βbounded away from 1, with high probability, every
substring in Mβis added as a token to the dictionary in a run of the LZW tokenizer (Definition 3.5).
Note that if every substring in Mβis assigned as a token by LZW, then the algorithm must be
β-heavy hitting since there always exists a maximal token on the “boundary” of the set Mβwhich is
strictly contained in {s′:max a∈AP(s′|a)≤1/dβ}.
18Lemma A.7. Every substring in Mβhas length at most ℓ⋆≜δ−1(βlog(d) + log(1 /δ)).
Proof. Note that mina,a′∈AP(a|a′) =δ, which implies that the probability of any transition must
be bounded away from 1, i.e., max a,a′∈AP(a|a′)≤1−δ. This implies that,
max
a∈AP(t|a)≤(1−δ)|t|≤e−δ|t|. (10)
By definition, for any substring t∈ M β,max a∈AP(t|a)≥δ/dβ. In conjunction with eq. (10), this
implies the statement of the lemma.
In the remainder of this section, let nbe the size of the dataset on which LZW is run. We show that
the number of tokens added to the dictionary by LZW, d, iseΘδ(n). Rather than running the algorithm
with early stopping (i.e., ceasing to add new tokens once the budget is hit), instead, we assume that
the algorithm runs on a prefix of the dataset of length d. The number of tokens added this way cannot
exceed d.
Lemma A.8. With probability ≥1−d−Ω(log( d/δ)/δ), in a run of the LZW algorithm, no substring t
added as a token to the dictionary satisfies |t| ≥ℓmax≜4 log( d|A|)/δ.
Proof. Consider any s∈Nand any substring tof length s. In order for tto be assigned as a token,
each of its prefixes must disjointly appear at least once in the string. Since there are at most dtokens,
we can upper bound the probability that tis assigned as a token as,
P(tis assigned as a token )≤d
ssY
i=1max
a∈AP(t1:i|a)
(i)
≤d
s
(1−δ)s(s−1)/2
≤eslog(d)−δs(s−1)/2,
where (i)uses the fact that max a∈AP(t1:i)≤Qi
j=1max a∈AP(tj|a)≤(1−δ)i. By union
bounding across the |A|sstrings of length s,
P(any length sstring is assigned as a token )≤eslog(|A|)+slog(d)−δs(s−1)/2.
When s= 4 log( d|A|)/δ+1≜ℓmax+1, the RHS is upper bounded by e−δℓ2
max/4≤d−Ω(log( d/δ)/δ).
With the same small probability, no substring of length s′> scan become a token, since their length- s
prefixes are never assigned as tokens.
Corollary A.9. With probability ≥1−d−Ωδ(log(d)), learns a dictionary with at least d⋆=d/ℓmax
tokens when run on a training sequence of length ndrawn from a stochastic source satisfying
Assumption 3.2.
Lemma A.10. For any constant β <1, with probability ≥1−d−Ω(log( d/δ)/δ)−exp(−eΩδ(d1−β))
over the source dataset, every substring in Mβis added as a token to the dictionary in a run of the
LZW algorithm. In other words, with the same probability, the LZW tokenizer results in a β-heavy
hitting dictionary.
By Corollary A.9, note that with high probability the LZW tokenizer adds at least d⋆tokens to
the dictionary when processing a length dtraining sequence in entirety. In this proof, instead of
generating dsamples, we sequentially sample d⋆tokens from their joint distribution, and generate a
dictionary from these samples. From Corollary A.9, with high probability this results in at most d
samples being generated, implying that the dictionary generated by sampling d⋆tokens is a subset
of the dictionary generated by a full run of the LZW tokenizer. Here, we use the fact that the LZW
tokenizer adds tokens to the dictionary in a left to right fashion, and therefore a subset of the dictionary
learnt by the LZW tokenizer can be generated by processing a portion of the dataset.
Next we consider a joint view for generating the dataset from the stochastic source and the dictionary
learnt by LZW simultaneously. The stochastic source is sampled as a sequence of tokens. Suppose
the last character of the previous token was a′. Sample a character a∼P(·|a′)and an infinite
trajectory on the tree T⋆
a. Consider the first node visited in this trajectory which does not already
19exist as a token in the dictionary. The substring corresponding to this node is added as a token
in the dictionary. By repeating this process, the dictionary and the source dataset are constructed
sequentially and simultaneously. As alluded to before, we truncate this token sampling process to
repeat at most d⋆times, which results in a subset of the dictionary output by the LZW algorithm
with high probability (Corollary A.9). This is simply a variant of the “Poissonization” trick to avoid
statistical dependencies across tokens. Denote the set of infinite trajectories generated on the forest
{T⋆
a:a∈ A} as{traji:i∈[d⋆]}.
With this view of the sampling process, observe that if the substring tsampled was a prefix of trajiat
least|t|times across different values of i, then tmust be assigned as a token. In particular, in each of
these|t|trajectories, each of the prefixes of tis assigned as a token. With this observation, the event
thattis not assigned as a token is contained in the event that tis visited at most |t| −1times across
thed⋆trajectories. Observe that,
P(tis not assigned as a token )≤|t|−1X
i=0d⋆
i
max
a∈A(P(t|a))i(1−P(t|a))d⋆−i.
Since we aim to upper bound this probability across the substrings in t∈ M β, note that (i)
max a∈AP(t|a)≥δ/dβ, and(ii)tokens in Mβhave length at most ℓ⋆=δ−1(βlog(d) + log(1 /δ))
(Lemma A.7), implying there are at most 2|A|ℓ⋆substrings in this set. By union bounding,
P(∃t∈ M βnot assigned as a token )≤2|A|ℓ⋆ℓ⋆−1X
i=0d⋆
i
max
x≥δ/dβxi(1−x)d⋆−i. (11)
Case I. Fori≤ℓ⋆andx≥1/2,
|A|ℓ⋆d⋆
i
xi(1−x)d⋆−i≤ |A|ℓ⋆(d⋆)ℓ⋆
2d⋆/2
≤2ℓ⋆log(d⋆|A|)−d⋆/2
≤2−Ωβ,δ(d⋆), (12)
where the last inequality uses the fact that ℓ⋆=Oβ,δ(log(d)).
Case II. Fori≤ℓ⋆andδ/dβ≤x≤1/2,
|A|ℓ⋆d⋆
i
xi(1−x)d⋆−i≤ |A|ℓ⋆d⋆
i
(1−x)d⋆
≤ |A|ℓ⋆(d⋆)ℓ⋆e−d⋆x
≤eℓ⋆log(|A|)+ℓ⋆log(d⋆)−d⋆x
≤e−Ω(δ2n/dβ/log(d/δ))
≤e−Ω(δ2d1−β/log(d/δ)), (13)
where the last inequality uses the fact that ℓ⋆=O(log(d)),x≥δ/dβ,d⋆= Ω( dδ/log(d/δ)). By
combining eq. (12) and eq. (13) with eq. (11) completes the proof, as long as βis a constant bounded
away from 1.
Lemma A.11. Fix a constant γ > 0. Then, with probability ≥1−d−Ωγ,δ(log(d)), none of the
substrings in the set Nγ=
s′:max a∈AP(s′|a)≤δ/d1+γ	
are assigned as tokens in a run of
LZW.
Proof. Define the following set of substrings,
Sγ=
t:δ/d1+γ/2≤max
a∈AP(t|a)≤1/d1+γ/2
Since the width of this band is sufficiently large, by Assumption 3.2 every substring tsuch that
max a∈AP(t|a)≤δ/d1+γ/2has at least one prefix which falls in Sγ, and denote the longest such
20prefix tγ. Define Tγ={tγ:t∈ Nγ}as the set of longest prefixes in Sγ. Intuitively, if we think of
the strings in Sγ(orTγ) as being intermediate in length, the strings in Nγcan be thought of as being
particularly long: the value of max a∈AP(t|a)for any t∈Tγand for any t∈ Nγare separated by a
factor of at least 1/dγ/2. In particular, since the probability of any character is lower bounded by
δ, each substring in t∈ N γmust be at least ∆ =γlog(d)
2 log(1 /δ)symbols longer than its corresponding
longest prefix in Tγ,tγ. An implication of this is that for tto be assigned as a token, tγmust
be observed at least ∆ + 1 times disjointly in s. However, note that tγalready has low marginal
probability to begin with ( ≪1/d) so the odds of seeing this substring so many times disjointly is
very small. Furthermore, note that Tγhas at most d1+γ/2/δsubstrings, which allows the probability
of this event occurring simultaneously across all substrings in Tγto be controlled by union bound.
Under this condition, none of the substrings in Nγare made into tokens.
In order to argue that the dictionary does not contain certain tokens, we may argue this property about
any superset of the dictionary. In contrast, in Lemma A.10, we construct a subset of the dictionary by
running LZW on the concatenation of d⋆tokens sampled from their joint distribution. The superset
we consider here is just to sample dtokens from their joint distribution and concatenate them together
to result in a string of length ≥d, and running LZW on this sequence (which simply would result
in these dtokens). As in Lemma A.10, let {traji:i∈[d]}denote the infinite trajectories generated
from the Markov chain which are truncated to result in tokens. A sufficient condition for the event
that no substring t∈ Nγis assigned as a token by LZW is to the event that every substring t′∈Tγis
observed as a prefix of trajifor∆or fewer choices of i∈[d]. To this end define E(t′)as the event
that|i∈[d]:t′is a prefix of traji| ≤∆. Then,
Pr(E(t′))≤n
∆
(max
a∈AP(t′|a))∆
(i)
≤e∆ log( n)1
d1+γ/2∆
≤e−γ
2∆ log( d), (14)
where (i)uses the fact that max a∈AP(t′|a)≤1/d1+γ/2since the substring t′belongs to Tγ.
Note that the number of substrings in Sγ(and by extension, Tγ) is at most Oδ(d1+γ/2). Recall that
these substrings satisfy the condition max a∈AP(t|a)≥δ/d1+γ/2. Observe that,
δ|Sγ|
d1+γ/2≤X
t∈Sγmax
a∈AP(t|a)
≤X
t∈SγX
a∈AP(t|a)≤ |A| ≤1
δ.
This implies that there are at most d1+γ/2/δ2substrings in Sγ. Finally, in conjunction with eq. (14),
P(∃t′∈Sγ:E(t′))≤d1+γ/2
δ2e−γ
2∆ log( d)=d−Ωγ,δ(log(d)),
which implies that with high probability, no token in Sγis observed as a prefix of sifor more than ∆
choices of the index i∈[d]. Under this event, no substring in Nγis assigned as a token.
A.6.1 Proof of Theorem 3.6
Choosing β= 0.99in Lemma A.10, with probability ≥1−d−Ωδ(log(d)), the LZW tokenizer results
in a0.99-heavy-hitting dictionary. As a consequence of Lemma A.6, this implies that under the same
event,
H(QMLE, P)≥0.99 log( d).
Finally, combining with Theorem 3.4 completes the proof.
21B Additional Theoretical Results I: A sequential variant of BPE
While the main results in the paper focused on understanding the limits of tokenization under a bound
on the dictionary size, in this section we take a more practical look and try to analyze tokenizers
used commonly in practice. The Byte-Pair-Encoding (BPE) algorithm (Gage, 1994; Sennrich et al.,
2016), discovered in the compression literature as REPAIR (Larsson and Moffat, 2000; Navarro and
Russo, 2008) was proposed as a faster alternative to LZW. It remains as one of the most commonly
implemented tokenizers in natural language processing for various downstream tasks (Radford et al.,
2019; Mann et al., 2020; Touvron et al., 2023). A large proportion of open source and commercial
LLMs currently use BPE as the tokenization algorithm of choice, such as GPT-2/3, Llama 1/2 and
Mistral-7B to name a few.
The BPE algorithm is based on constructing the dictionary iteratively by merging pairs of tokens to
result in a tokens. In each iteration, the pair of tokens which appear most frequently next to each
other are merged together into a single token. Subsequently, every occurrence of the pair of tokens
are replaced by the newly added token, breaking ties arbitrarily. The dictionary is thus an ordered
mapping of the form t←(t′,t′′). To encode a new string, the BPE encoder iterates through the
dictionary and for each rule t←(t′,t′′)replaces every consecutive occurrence of t′andt′′by the
token tbreaking ties arbitrarily.
To warm up our main results, it is worth understanding the behavior of the BPE tokenizer in a bit more
detail. Unlike the toy tokenizer, it is a priori unclear whether unigram models trained on sequences
tokenized by BPE even asymptotically (in the dictionary size) achieve the optimal cross-entropy loss.
Indeed, for δ >0, consider a training sequence of length mof the form,
s=
01···01|{z}
2/δ10···10|{z}
2/δ

| {z }
×mδ
4(15)
The probability that this sequence is generated by the order- 2switching Markov source with p=q=
δis,
≈(1−δ)mδ
4×4
δ×(1−δ)(δ)mδ
4×4=e−H(P),
which uses the fact that H(P) =mδlog(1/δ) +m(1−δ) log(1 /(1−δ)). This implies that even
though the string has exponentially small probability, it is one of the typical sequences for this
order- 2Markov source. Let’s understand what happens when the BPE tokenizer is trained on this
dataset. Assuming that ties are broken arbitrarily, consider the run of the BPE algorithm detailed
in Table 2. Here, we assume that 1/δ−1is a power of 2and denote r= log2(1/δ−1). The
algorithm first merges 0and1into a single token t1, which results in a long sequence of the form
t1······ t11t1······ t10repeated mδ/4times. In subsequent rounds, the tokens (t1,t1)is merged
into t2, then (t2,t2)is merged into t3and so on, until is no longer possible. Finally, the resulting
sequence is a repeating sequence of 5tokens where within each sequence, no pair of tokens appears
more than once next to each other. Eventually these 5tokens are merged into a single token labelled
tr+4, and in subsequent rounds the tokens (tr+4,tr+4)are merged into tr+5,(tr+5,tr+5)is merged
into tr+6and so on, until is no longer possible.
Observe that in the initial training dataset the substrings 0000 and1111 never appears as a contiguous
sequence. However, in a test sequence of length msampled from the 2nd-order Markov source, with
high probability these substrings disjointly occur Θ(m)times each. The learnt dictionary associates
each such disjoint occurrence of these substrings with at least 1token, for 0000 , the 3rd0must
necessarily be tokenized as the token “ 0”. Likewise, in 1111 , the3rd1must necessarily be tokenized
as the token “ 1”. Therefore, when a new test string of length mis tokenized, with high probability
the tokens “ 0” and “ 1” form a constant fraction of the total collection of tokens.
Thus on freshly sampled test sequences, the BPE tokenizer appears to behave like the character-level
tokenizer on a constant fraction of the input sequence. In particular, a simple calculation shows that
the cross-entropy loss of any unigram model trained on this tokenizer must be far from the optimal
22Initial 01······ 0110······ 10|···
t1←(0,1) t1······ t11t1······ t10|···
t2←(t1,t1) t2···t21t2···t20|···
......
tr←(tr−1,tr−1) trt11tr0|···
tr+1←(tr,t1) tr+11tr0|···
tr+2←(tr,0) tr+11tr+2|···
tr+3←(tr+1,1) tr+3tr+2|···
tr+4←(tr+3,tr+2) tr+4|···
tr+5←(tr+4,tr+4) tr+5|···
tr+6←(tr+5,tr+5) tr+6|···.
......
Table 2: A representation of the behavior of BPE when trained on the dataset in eq. (15). We assume
that1/δ−1is a power of 2and define r= log2(1/δ−1).
bound of mH BER(δ)especially as δbecomes smaller,
min
Q∈Q1-gramLm(Q◦enc(·))
≥min
Q∈Q1-gramE
n0log(1/Qtok(0) + n1log(1/Qtok(1)
(i)
≥Ω(m)·min
Q∈Q1-gram 
log(1/Ptok(0) + log(1 /Qtok(1)
≥Ω(m).
where (i)uses the fact that E[n0],E[n1]∈Ω(m)and the last inequality uses Ptok(0)Ptok(1)≤1/4
(AM-GM inequality) since they sum up to at most 1. The purpose of this example is to show that
there exist pathological training datasets which appear to be drawn from a stochastic source, but on
which BPE fails to learn a good dictionary for the source. Thus proving a result such as Theorem 3.1
for BPE would require arguing that training datasets such as that in eq. (15) are unlikely to be seen.
The analysis of the standard variant of BPE turns out to be complicated for other reasons too. After
every token is added the training dataset becomes a mix of all the previously added tokens, and
arguing about the statistics of which pair of tokens appears most frequently for the next iteration
becomes involved. For instance, adding 00as a token may reduce the frequency of occurrence of
the substring 01, but will not affect 11. Thus, even though 01may a priori have been seen more
frequently, it may not be chosen by BPE as the next token after 00.
To avoid dealing with these issues, we consider a sequential/sample-splitting variant of BPE. At a
high level, the algorithm breaks down a dataset of size Θ(d2)intodchunks and learns at most 1
token from each chunk. The algorithm iterates over the chunks and finding the pair of tokens which
appear most frequently next to each other in each chunk and adding it to the dictionary if it appears
more than log(d)times. Every consecutive occurrence of the pair of tokens is replaced by the newly
assigned token in the dataset. Thus, in each iteration i, at most 1token is added, depending on the
statistics of the ithchunk and the tokens added so far to the dictionary. Based on the final size of the
dictionary a different encoder/decoder pair is used - if the algorithm adds sufficiently many tokens to
the dictionary, the greedy encoder is used, and if not, a parallel implementation of BPE’s encoding
algorithm is used (Definition B.1). A formal description of the algorithm is in Algorithm 1.
Definition B.1 (BPE.split encoder) .The BPE.split encoder parses a new string into tokens as follows.
The algorithm partitions the string into contiguous chunks of length d. Then, BPE’s encoder is applied
on each chunk, which iterates through DSand replaces t′t′′bytfor every rule t←(t′,t′′)inDS,
breaking ties arbitrarily. The individual token sequences are finally spliced together and returned.
The main result of this section is that up to a small additive error, Algorithm 1 approaches a 2-
approximation to the optimal cross-entropy loss.
23Algorithm 1 Sequential implementation of BPE
Input: ϵ∈(0,1); a dataset of size n= Θ( d2), split into dcontiguous texts {text1,···,textd}of
length Θ(d)each.
Output: A tokenizer T.
// Generate Dictionary
fori= 1,···, ddo
if∃a pair of tokens/characters (t′,t′′)appearing ≥log(d)times consecutively in textithen
Append the rule t←(t′,t′′)toDS
forj=i+ 1,···, ddo
textj←APPL Y t←(t′,t′′)(textj);
// Can be implemented in parallel
end for
end if
end for
// Encoder and Decoder
if|Dict|< d0≜ϵd/2 log(4 |A|)then
T ← (Dict,DS,enc BPE.split (·),dec BPE.split (·))
else
T ← (Dict,∅,enc gre(·),dec gre(·))
end if
defAPPL Y t←(t1,t2)(text):
Replace every consecutive occurrence of (t′,t′′)intextbyt, breaking ties arbitrarily.
Theorem B.2. For any ϵ∈(0,1), run Algorithm 1 on a dataset of n= Θ( d2)characters to learn a
dictionary with at most dtokens. The resulting tokenizer Tsatisfies with probability ≥1−e−Ω(dϵ2),
min
Q∈Q1-gramL(Q◦enc(·))≤(2 +ε) min
QL(Q) +ϵ
where ε=O
log(1/ϵ) log3(1/δ)
ϵδ9log(d)
.
While the guarantees established for the sequential BPE tokenizer are weaker than those in Theo-
rem 3.1, the analysis turns out to be quite involved. Theorem B.2 implies that unigram models trained
on the sequential BPE tokenizer asymptotically approach the optimal cross-entropy loss up to a factor
of2.
The formal proof of this result is presented in Appendix B. What is the intuition behind using a
different encoder in Algorithm 1 depending on the number of tokens in the dictionary? When the
number of tokens in the dictionary is smaller than d0, we know that on a 1−d0/dfraction of the
iterations of Algorithm 1, a token is notadded to the dictionary, i.e. every pair of tokens already
appears at most log(d)times together. This is a datapoint of “evidence” that under the dictionary in
that iteration, the BPE encoder is already good at encoding new strings (of length Θ(d)) in a way
where pairs of tokens do not appear consecutively with high frequency. Since future dictionaries
only have more rules appended to them, dictionaries only get better at encoding new strings into
tokens where pairs do not frequently appear consecutively. In other words, the BPE encoder satisfies
a monotonicity property. It remains to show that dictionaries which encode new sequences in a way
where no pair of tokens appear too frequently have large H(QMLE, P)(to invoke Theorem 3.4). This
follows from ideas introduced in (Navarro and Russo, 2008).
The case where the number of tokens is large ( ≥d0) turns out to present significant technical
challenges for analyzing the BPE encoder. There is no longer much “evidence” that the dictionary in
each iteration is good at encoding strings since in a large number of iterations a pair of tokens appear
consecutively with high frequency. Analyzing the greedy encoder also presents its own challenges
- although the algorithm has allocated a large number of tokens, it is possible that there are short
tokens twhich are maximal (i.e. they are not prefixes of other tokens). This is similar to the problem
encountered by BPE when trained on the dataset in eq. (15) - although the algorithm has allocated a
large number of tokens, the token 1is maximal since every other token begins with the character 0.
24However, it turns out that such tokens, although present in the dictionary, are not observed frequently
while encoding a fresh string drawn from the source.
B.1 Analysis of Algorithm 1
In this section we prove a rephrased version of Theorem B.2 which implies the statement in the main
paper. Define d0=ϵd
2 log(4 |A|).
Theorem B.3 (Rephrased Theorem B.2) .Run Algorithm 1 on a dataset of n= Θ( d2)characters to
learn a dictionary with at most dtokens. The resulting tokenizer Tsatisfies one of the following 3
conditions,
1. Either, |Dict|> d0, and,
min
Q∈Q1-gramL(Q◦enc(·))≤H∞
1−ε.
Here, ε=O
log3(1/δ) log(1 /ϵ)
ϵδ9log(d)
.
2.Pr(|Dict|< d0) =e−Ω(ϵ2d/log2(1/δ)), or,
3. Conditional on |Dict|< d0, with probability ≥1−e−Ω(ϵ2d/log2(1/δ)),
min
Q∈Q1-gramL(Q◦enc(·))≤
1−2d0
d 
2H∞+O1
log(d)!
+2d0
dlog(4|A|).
With the choice of d0=ϵd/2 log(4 |A|)we get the statement of Theorem B.2.
B.2 Analysis for the large dictionary case: |Dict|> d0
In the large dictionary case, Algorithm 1 uses the greedy encoder/decoder pair in conjunction with
the dictionary. The proof of Theorem B.2 relies on establishing that the cross-entropy H(QMLE, P)
of the tokenizer is large. Namely, we prove that,
Lemma B.4. In Algorithm 1, assuming at least d0tokens are allocated,
H(QMLE, P) = Ω 
ϵδ9log(d)
log(1/ϵ) log3(1/δ)!
.
To show this, it suffices to argue that conditioned on any previous set of tokens, with nontrivial
probability over the underlying string generated from the stochastic source, the next token is long (i.e.
having conditional probability at most O(1/√
d)).
Lemma B.5. Suppose that in a run of Algorithm 1, at least d0tokens are allocated. Suppose a set of
tokens t1,···,tkhave been sampled so far by the greedy encoder. Let Ti+1be the random variable
which denotes the next token returned by the greedy encoder, where the randomness comes from the
underlying string being tokenized. Then,
Pr
P(Ti+1|ti)≤1/√
Cδdt1,···,ti
≥d0δ6(1−δ)2
8Cd∆|A|log(2|A|)nD= Ω 
ϵδ9
log3(1/δ) log(1 /ϵ)!
Proof sketch of Lemma B.5. The proof will proceed in 2parts. We first show in Lemma B.9 that
there is a set DvalidofΩ(d)tokens in the dictionary which are neither prefixes nor suffixes of any
other token in Dict. The reason for considering this set of tokens is twofold,
1.Irrespective of what the previous set of tokens were, it is legal for a token Dvalidto be
sampled in the current step by the greedy encoder, since for any candidate t∈Dvalid, by
definition, tj···tit̸∈Dict for every j≤i.
252.Suppose a sequence of tokens t1,···,tihave already been sampled, ending with the
character a. Then, we may sample the next token using rejection sampling. Sample
a′∼P(·|a)and an infinitely long trajectory on T⋆
a′. Return the last token on this trajectory
which belongs to Dict, and if it so happens that ∃j∈[i]such that tj···tit∈Dict, then
reject this trajectory and repeat. Since all the tokens in Dvalidare not prefixes of another
token, any trajectory which reaches a token in Dvalidmust terminate the rejection sampling
process.
Next, in Lemma B.10, we show that since the number of tokens in Dvalidis sufficiently large, Ω(d),
with constant (in d) probability, a trajectory rolled out in the first round of the rejection sampling
process will reach a token t∈Dvalidwhich has small probability, i.e. max a∈AP(t|a)≤1/poly(d).
By the previous arguments, this must mean that the rejection sampling process terminates on this
“low probability” token, resulting in the statement of the lemma.
Figure 7: Jointly generating a sequence and its greedy encoding: In this example we use the greedy
encoder under the dictionary composed of all the substrings shadowed red. The first character ( a) is
sampled from the stationary distribution. Then an infinite string is sampled on the tree with aas root
(green path). The last substring on this path which is a token ( t1=abb) is returned by the greedy
encoder. Then the next character x=bis sampled from the source conditioned on the previous
character ( b) and further conditioned on t1x̸∈Dict. Finally, another infinite string is sampled on the
tree with x=bas root (purple path) and the last substring on this path which is a token ( t2=ba) is
returned by the greedy encoder. Repeating this process, we can generate a string, here, abbba···, as
well as its greedy encoding, (abb, ba, ···).
Proof of Theorem B.3.1 and Lemma B.4 It is easy to see why Lemma B.5 implies a lower bound
on the cross entropy H(QMLE, P)of the tokenizer. By Lemma A.4 for the greedy encoder,
QMLE(t) = lim
m→∞E"
ntP
t′nt′#
a.s.= lim
m→∞ntP
t′nt′. (16)
Since the limit m→ ∞ of the RHS exists by Lemma A.4, we may let m→ ∞ in any way we
like, and in particular we may simply sample i⋆tokens, t1,···,ti⋆sequentially according to the
process in Figure 7. Here, the first token sampled is returned by generating an infinitely long string
onT⋆
awhere a∼πand then truncating this trajectory to the longest token which belongs to Dict.
Subsequently for every i >1,tiis generated by sampling a fresh infinitely long string from T⋆
a
where ais sampled from the P(·|a′)where a′is the last character ti−1and then returning the largest
prefix of this string which is a token in Dict, conditioned on tj···ti−1ti̸∈Dict for any j < i .
26and concatenate the corresponding substrings to get an m=Pi⋆
i=1|ti|length character string. Letting
i⋆→ ∞ , we must have m→ ∞ surely since m≥i⋆. In this view, eq. (16) can be rewritten as,
QMLE(t) = lim
i⋆→∞nt
i⋆= lim
i⋆→∞1
i⋆i⋆X
i=1I(ti=t)a.s.= lim
i⋆→∞1
i⋆i⋆X
i=1E[I(ti=t)|t1,···,ti−1](17)
where the last inequality follows by the sequential nature of the token sampling process and a
martingale argument. Consider the set of tokens Tsuch that t∈Tsatisfies max a∈AP(t|a)≤p
1/Cδ3d. From eq. (17), summing across t∈T, we have that,
X
t∈TQMLE(t)a.s.= lim
i⋆→∞1
i⋆i⋆X
i=1Pr (ti∈T|t1,···,ti−1) = Ω 
ϵδ9
log3(1/δ) log(1 /ϵ)!
(18)
where in the last inequality, we use Lemma B.5 and the fact that δmax a∈AP(t|a)≥mina∈AP(t|a).
Therefore,
H(QMLE, P)≥X
t∈TQMLE(t) log(1 /P(t))≥X
t∈TQMLE(t) log(√
Cδ3d)
where in (i)we use the fact that for t∈T,max a∈AP(t|a)≤1/√
Cδ3d, which implies that
P(t)≤1/√
Cδ3d. Finally, combining with eq. (18) completes the proof of Lemma B.4. Furthermore,
since the cross-entropy H(QMLE, P)was established to be large, by invoking the reduction in
Theorem 3.4, we complete the proof of Theorem B.3.1.
Figure 8: The circled nodes indicate substrings which are tokens in Dict. The red boundary is the
set of substrings tsuch that max a∈AP(t|a)≥1/Cd. By Lemma B.8, none of the nodes which fall
outside this boundary are assigned as tokens in a run of Algorithm 1. The set of circled substrings
are the set of tokens in Dict. Among them, the ones circled green are the tokens in Dvalid, which are
not prefixes or suffixes of any other tokens in Dict. Substrings such as cborbawhich are tokens
inDict do not belong to Dvalidbecause they are prefixes of longer tokens (in this case, cbbandbab
respectively). On the other hand, substrings like abdo not belong to Dvalidsince they are suffixes of
tokens in Dict, in this case, bab. Lemma B.9 asserts that the number of tokens in DvalidareΩ(d)in
number, assuming that Dict hasΩ(d)tokens to begin with.
Notation. For each a∈ A andj∈N∪ {0}, define a level set of substrings,
Sa
j=n
(1−δ)j+1< P(t|t1=a)≤(1−δ)jo
27Dictionary
...
t1←(·,·)
t2←(·,·)
...
t′←(·,·)
...
t′′←(·,·)··· s′··· s′···
s1s2s1···s7 s1s2s1···s7
···t1···t1··· ···t1···t1···
···t1t2···t1···t2··· ···t1t2···t1···t2···
t′t′
t′′at each step, both
copies of s′encode
to the same
sequence of tokenssuppose it
encodes to t′suppose it
encodes to t′′=⇒At each iteration,
both copies of s′
perfectly encode to
a sequence of tokens
both copies of s′
map to the same
token at this step,
a contradiction
Figure 9: A pictorial representation of the proof of Lemma B.6
where t1denotes the first character of t. And likewise, define the sets Sj=∪a∈ASa
j,Sa
≤jandSa
≥j
as the union of Sa
j′overj′≥j,j′≤jandS≤jandS≥jas the union of Sa
≤jandSa
≥jovera∈ A.
Furthermore for a large universal constant C >0, define parameters,
∆ =log(δ)
log(1−δ)≍Θlog(1/δ)
δ
;nD= 1−2 log(4 Cd/δd 0)
log(1−δ)≍Θlog(1/ϵδ)
δ
.(19)
We first begin by stating a folklore result: every pair of tokens assigned by a merging-based dictionary
generation algorithm have distinct character representations.
Lemma B.6. If Algorithm 1 assigns a new token in some round, it’s character representation must
be distinct from that of all previously assigned tokens.
Proof. A pictorial proof is in Figure 9. We will prove this result by contradiction. Suppose tandt′
are tokens which decode to the same character substring, s′. Consider all occurrences of s′in the
dataset which in some iteration encode into t′ort′′, and denote these disjoint locations S. Recall that
at these locations, s′eventually is assumed to map to a singular token t′ort′′. Therefore, at every
step in the merging process these occurrences of s′must perfectly map to a sequence of tokens.
Now consider the merging process at the first time before any of the rules corresponding to tokens in
t′ort′′are implemented. Prior to this time, all the occurrences of s′corresponding to the locations
inShave not been tokenized yet. When the first rule corresponding to one of the tokens in {t′,t′′}is
implemented, all the strings in Smust be modified identically. This uses the fact that we can isolate
each of these occurrences of s′while carrying out the merging process, since each location must be
distinct. At every step, the encodings of these copies of s′must be the same, and therefore t′andt′′
cannot be two distinct tokens.
Lemma B.7. The size of the level set Sa
jis bounded by (1−δ)−(j+1).
Proof. Since the probability of any transition is at most 1−δ, this implies that any infinite trajectory
on the tree T⋆
acan intersect at most one vertex in Sa
j. Therefore,P
t∈Sa
jP(t|t1=a)≤1. By the
lower bound on P(t|t1=a)fort∈Sa
j, this implies the statement of the lemma.
28Next we show that with high probability none of the substrings thaving probability mass (under P)
of at most δ/Cd conditioned on the first character, are assigned as tokens by Algorithm 1.
Lemma B.8. In a run of Algorithm 1, for a sufficiently large constant C > 0, with probability
d−Ω(1)poly(1/δ)all assigned tokens t∈Dict satisfy max a∈AP(t|a)≥1/Cd. In other words, none
of the substrings in S≥j⋆are added as tokens to the dictionary in a run of Algorithm 1, where,
j⋆≜log(δ/Cd )/log(1−δ)
Proof. Consider some j≥j⋆anda∈ A and substring t∈Sa
j. In the ithstage of the algorithm
where textiis being processed, for tto be assigned as a token, at the very least, tmust appear at least
log(d)times disjointly in texti. Therefore,
P(t∈Sa
jis assigned as a token in texti)≤d
log(d)
max
a∈AP(t|a)log(d)
≤dlog(d)1
Cd(1−δ)j−j⋆log(d)
≤d−log(C)(1−δ)(j−j⋆) log( d)
Union bounding over Sa
joverj≥j⋆using the bound on |Sa
j|in Lemma B.7, and over a∈ A and
i∈[d]results in the bound,
P(t∈S≥j⋆is assigned as a token in step ifor some i∈[d])≤d−Ω(1)X
j≥j⋆(1−δ)(j−j⋆) log( d)
(1−δ)j+1≤d−Ω(1)
δ(1−δ)
Lemma B.9. Consider the set of tokens Dvalidwhich are not a prefix or a suffix of any other token
inDict. That is, Dvalid={t∈Dict:̸ ∃s:st∈Dict} ∩ { t∈Dict:̸ ∃s:ts∈Dict}. If|Dict| ≥d0,
then,
|Dvalid| ≥d0
4nD.
where nDis defined in eq. (19).
Proof. For any token t∈Dvalid, there may be at most 2|t|tokens which are suffixes or prefixes of it
and belong to Dict. More importantly, every token in Dict not belonging to Dvalidmust either be a
prefix or a suffix of some token in Dvalid. Split the suffixes and prefixes of the tokens in Dvalidinto
four sets,
1.Ssuff,min=S
t∈Dvalid{t′∈Dict:t′∈suff(t),|t′| ≤ | t| −nD},
2.Ssuff,max=S
t∈Dvalid{t′∈Dict:t′∈suff(t),|t′|>|t| −nD},
3.Spre,min=S
t∈Dvalid{t′∈Dict:t′∈pre(t),|t′| ≤ | t| −nD},
4.Spre,max=S
t∈Dvalid{t′∈Dict:t′∈pre(t),|t′|>|t| −nD}.
where nDis defined in eq. (19). Note from Lemma B.8 that all the tokens t∈Dict all
satisfy max a∈AP(t|a)≥1/Cd. Therefore, the tokens in Spre,minandSsuff,minall satisfy,
max a∈AP(t|a)≥d/C(1−δ)nD. By summing Lemma B.7 over appropriate j, we get that
|Spre,min|+|Ssuff,min| ≤2Cd(1−δ)nD−1/δ.
On the other hand, corresponding to any t∈Dvalid, there are at most nDtokens in Spre,maxorSsuff,max
and and therefore |Spre,max|,|Ssuff,max| ≤nD· |Dvalid|. Since every token in Dict either belongs to
Dvalidor is a suffix of some token in Dvalid,Spre,min∪Spre,max∪Ssuff,min∪Ssuff,max=|Dict|and,
2nD· |Dvalid|+2C(1−δ)nD−1d
δ≥d0
29Recalling the choice of nD= 1−2 log(4 Cd/δd 0)
log(1−δ), we get that,
|Dvalid| ≥d0
4nD.
Lemma B.10. Suppose Algorithm 1 assigns at least d0tokens. For any character a∈ A, sample an
a′∼P(·|a)and an infinite trajectory on the tree T⋆
a′, denoted traj. Then,
Ea′∼P(·|a)"
Pr
traj∼T⋆
a′
min
t∈traj∩DvalidP(t|a)≤p
δ/Cda′#
≥d0δ6(1−δ)2
8Cd∆|A|nD.
where the notation T⋆
a′is used to overload the distribution over infinite trajectories on T⋆
a′. The
parameters nDand∆are defined in eq. (19).
Proof. By Lemma B.8, recall that the ≥d0tokens assigned in a run of Algorithm 1, with high
probability, are substrings in S≤j⋆. For any a∈ A, the total number of substrings in S≤j⋆can be
bounded as,
|S≤j⋆| ≤X
a∈Aj⋆X
j=0|Sa
j| ≤X
a∈Aj⋆X
j=01
(1−δ)j+1≤C|A|d
δ(1−δ). (20)
In order to prove this result, we use a counting argument and the fact that no tokens in S>j⋆are
assigned. Consider some character aand all the leaves in the forest S≤j⋆. Since every transition has
≥δprobability of occurring, across all leaf nodes t∈S≤j⋆,P(t|a′)are within a δ2(1−δ)factor of
each other across different a′∈ A. In particular, by counting the number of paths in T⋆(i.e. paths in
T⋆
afrom∅to leaf nodes in Sa
≤j⋆across a∈ A) along which a token in Dict exists in S≥j⋆/2, we can
also compute the probability mass across such trajectories up to a factor of δ2(1−δ).
Taking the union across a∈ A , consider the paths in T⋆
afrom∅to leaf nodes in Sa
≤j⋆. From
Lemma B.9,P
j≤j⋆|Dvalid∩Sj| ≥d0/4nD, where nD= 1−2 log(4 Cd/δd 0)/log(1−δ). Note
that for sufficiently large d= Ω(log(1 /ϵδ)/δ5), by Lemma B.7,P
j≤j⋆/2|Sj|=p
Cd/δ/δ (1−δ)≤
d0/8nD. Therefore,
X
j⋆/2<j≤j⋆|Dvalid∩Sj| ≥d0
8nD. (21)
Define ∆ = log( δ)/log(1−δ). Combining eq. (21) with eq. (20) and applying the probabilistic
method, there exists an i⋆≥j⋆/2such that,
|Dvalid∩(Si⋆+1∪ ··· ∪ Si⋆+∆)|
|Si⋆+1∪ ··· ∪ Si⋆+∆|≥δ(1−δ)d0
8Cd|A|nD. (22)
Note that ∆is chosen to be sufficiently large, so that every infinite trajectory on T⋆
a′must intersect at
least once with the band of vertices Sa′
i⋆+∆+1∪ ··· ∪ Sa′
i+2∆. Note that this band is different from the
one considered in eq. (22). Define La′as the set of longest prefixes across infinite trajectories in T⋆
a′
which belong to Sa′
i⋆+∆+1∪ ··· ∪ Sa′
i+2∆.
Note that our objective is to show that an infinite trajectory sampled on T⋆
a′where a′∼P(·|a), has a
long prefix in Dict. We can truncate this trajectory to lower bound this probability, and therefore, we
assume that the infinite trajectories on T⋆
a′terminate once they reach a substring in La′. Furthermore,
note that although ∆is large, it is still a constant depending on δ. Therefore, the band of states
Sa′
i⋆+∆+1∪ ··· ∪ Sa′
i⋆+2∆ is not too wide, and all the substrings in La′have approximately similar
probabilities to each other. In particular, for any character a∈ A, and for any a′∈ A andt∈La′,
decomposing P(t|a)asP(t|t1=a′)P(a′|a),
δ2(1−δ)·(1−δ)i+∆(i)
≤P(t|a)(ii)
≤(1−δ)i+∆. (23)
Inequality (i)follows from the fact that all transition probabilities are at least δ, so every leaf node
inLa′must have P(t|t1=a′)≥(1−δ)i+2∆+1, and the fact that P(a′|a)≥δ. Inequality (ii)
30follows similarly from the fact that tis a leaf node of La′and therefore P(t|t1=a′)≤(1−δ)i+∆.
Therefore, instead of bounding the probability of any event under the distribution over substrings
inLa′induced by truncating the infinite strings sampled on T⋆
a′, it suffices to count the fraction of
substrings in La′satisfying the event (which are equivalent up to a δ(1−δ)factor). Define,
pre(t) = ( t1,t1:2,t1:3,···,t1:|t|)
As the set of prefixes of t(including t). Note that at most ∆of the prefixes of any substring tcan
intersect with Sa
i⋆+1∪ ··· ∪ Sa
i⋆+∆. Therefore,
X
a′∈AX
t∈La′1(pre(t)∩Dvalid∩(Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆)̸=∅)
≥X
a′∈AX
t∈La′|pre(t)∩Dvalid∩(Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆)|
∆
(i)
≥X
a′∈A|Dvalid∩(Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆)|
∆
(ii)
≥δd0(1−δ)
8Cd∆|A|nDX
a′∈A|Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆|
(iii)
≥δ3d0(1−δ)
8Cd∆|A|nDX
a′∈A|La′|,
where (i)uses the fact that the prefixes of t∈La′cover all the substrings in Sa′
≤i⋆+∆, and therefore
∪t∈La′pre(t)⊃Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆, and(ii)uses eq. (22). Finally, (iii)uses the fact that ∆is not
too large, and therefore, for any substring t′∈Sa′
i⋆+1∪···∪ Sa′
i⋆+∆, there are at most 1/(1−δ)2∆=
1/δ2substrings t∈La′which contain it as a prefix. This means, |La′| ≤ |Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆|/δ2.
After dividing byP
a′∈A|La′|on both sides, this implies,
Ea′∼Unif(A)"
Pr
t∼Unif( La′)
pre(t)∩Dvalid∩(Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆)̸=∅a′#
≥δ3d0(1−δ)
8Cd∆|A|nD.
(24)
The event inside the inner probability term is the event that an infinitely long string (truncated at La′)
has a prefix which lies in Dvalidand which intersects with Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆, which implies that it
has probability P(t|a)≤p
δ/Cd . Therefore, we have that for any a∈ A, sampling an a′∼P(·|a)
and an infinite trajectory traj∼ T⋆
a′,
Ea′∼P(·|a)"
Pr
traj∼T⋆
a′
min
t∈traj∩DvalidP(t|a)≤p
δ/Cda′#
(i)
≥δ2(1−δ)·Ea′∼P(·|a)
 Pr
t′∼Unif( La′) 
min
t∈pre(t′)∩DvalidP(t|a)≤p
δ/Cda′!

(ii)
≥δ2(1−δ)·Ea′∼P(·|a)"
Pr
t′∼Unif( La′)
pre(t′)∩Dvalid∩(Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆)̸=∅a′#
(iii)
≥δ3(1−δ)·Ea′∼Unif(A)"
Pr
t′∼Unif( La′) 
pre(t′)∩Dvalid∩(Sa
i⋆+1∪ ··· ∪ Sa
i⋆+∆)̸=∅a′#
≥δ3(1−δ)·δ3d0(1−δ)
8Cd∆|A|nD.
Here(i)follows by truncating the trajectory trajto terminate at a node in ∪a′∈ALa′and from eq. (23),
(ii)follows by arguing that i⋆≤j⋆/2and therefore if a prefix of t′lies in Sa′
i⋆+1∪ ··· ∪ Sa′
i⋆+∆, then
it must have P(t|a)≤p
δ/Cd . Inequality (iii)follows by noting that all the transitions P(a′|a)
have probability ≥δ, and the last inequality follows from eq. (24).
31Proof of Lemma B.5
Lemma B.10 concludes that given any previous sequence of tokens terminating in a character a,
with constant probability, an infinite trajectory sampled from T⋆
a′witha′∼P(·|a)has as prefix,
a substring t, which not only has low probability, with P(t|a)≤p
δ/Cd , but also belongs to the
subset of tokens Dvalid. Note that regardless of the previously sampled tokens, it is legal to sample
any token in Dvalidas the current token, since by definition, these tokens are not the suffixes of any
other tokens in Dict. Moreover, if any trajectory on T⋆
a′reaches a token in Dvalid, then it must be
largest token along that trajectory, since none of the tokens in Dvalidare prefixes of another token in
Dict.
Consider generating a new token by rejection sampling. Suppose the set of previous tokens t1,···,ti
end in some character a. Sample the next character a′∼P(·|a)and an infinite trajectory on T⋆
a′. If it
reaches an illegal token tsuch that tjtj+1···titalready exists in Dict, this token is rejected and the
trajectory is resampled. By the prefix-free property of these tokens, if this trajectory visits a token in
Dvalid, it must immediately be output as the next token. Note that this probability is lower bounded
by,
Ea′∼P(·|a)"
Pr
traj∼T⋆
a′
min
t∈traj∩DvalidP(t|a)≤p
δ/Cda′#
which is lower bounded by poly(ϵ, δ), the subject of Lemma B.10. Therefore with this probability,
the process terminates in the first step with a token in Dvalidbeing sampled.
B.3 Analysis in the small dictionary case
In this section, we will prove Theorem B.3.2 and Theorem B.3.3. In particular we show that, either,
1.The dictionary is small with low probability. i.e., Pr(|Dict|< d0) =e−Ω(ϵ2d/log2(1/δ)), or,
2.Or conditioned on the dictionary being small, |Dict|< d0, with high probability ≥1−
e−Ω(ϵ2d/log2(1/δ)),
min
Q∈Q1-gramL(Q◦enc(·))≤4 
1−2d0
d+O1
log(d)!
H∞+2d0
d·log(2|A|).
Fori∈[d], define the indicator random variable,
X(s′,Dict) =1(∃a pair of tokens in enc BPE(s′)under Dict appears at least log(d)times ).
which captures the event that the string s′is compressed well by the dictionary Dict under the
sequential encoder.
LetDictidenote the dictionary stored by Algorithm 1 right after textiis processed. The key
insight behind this lemma is the following statement, asserting that the sequential encoder satisfies a
“monotonicity” property: for any jand string s′, if there exists a pair of tokens appearing more than
log(d)times consecutively in the sequential encoding of s′under Dictj, then there must exist a pair of
tokens appearing more than log(d)times consecutively in the greedy encoding of s′under Dictifor
anyi < j . This implies that X(s′,Dictj)≤X(s′,Dicti)ifi < j for any string s′. This monotonicity
property implies that the last dictionary output by the learner, Dictdsequentially encodes a 1−ϵ
fraction of the previously seen texts, textiin a way where every pair of tokens appears at most log(d)
times. While Dictdis correlated with these texts, we can circumvent this correlation by using a
martingale argument to prove the statement of the lemma.
Lemma B.11. LetDict be the dictionary returned by Algorithm 1. Then,
min
Pr
E
X 
s′,DictDict
≥2d0/d|Dict|< d0
,Pr 
|Dict|< d0
≤e−ϵ2d/8 log2(1/δ).
where s′is a fresh substring of length dsampled from the stochastic source.
Proof. LetDictidenote the state of dictionary returned by Algorithm 1 right after textiis processed.
Then, Dictdis the final dictionary returned by Algorithm 1. Suppose E
X 
s′,DictdDictd
≥2d0/d,
32where s′is a fresh substring of length dsampled from the stochastic source. Using monotonicity of
the sequential encoder, almost surely for any string s′,X(s′,Dicti)≤X(s′,Dictj)for any j > i .
Therefore,
E
X 
s′,DictdDictd
≥2d0/d=⇒Xd−1
i=1E
X 
s′,DictiDicti
≥2d0·d−1
d(25)
Note in this expectation, s′is an independent string of length dsampled from the stochastic source.
Since Dictiandtexti+1are independent, we may instead write,
Xd−1
i=1E
X 
texti+1,DictiDicti,texti,Dicti−1,···,Dict 1,text1
≥2d0·d−1
d.
For brevity, denote Xi= X(texti+1,Dicti)and define the filtration Fi=
σ({text1,Dict 1,···,texti,Dicti}). Note thatPi
j=1Xj−E[Xj|Fi]forms a martingale se-
quence under the filtration {Fi:i∈[d]}. Therefore, by the Azuma-Hoeffding inequality, for any
η >0,
PrXd−1
i=1E[Xi|Fi]−Xi≤ −η
≤e−η2. (26)
Under Case I, we have thatPd
i=1Xi≤d0. Therefore, from eq. (25) and eq. (26),
Pr
|Dict|< d0;E
X(s′,Dict)Dict
≥2d0/d
≤Pr
d−1X
i=1Xi< d0;d−1X
i=1E[Xi|Fi]≥2d0·d−1
d

≤Pr
d−1X
i=1E[Xi|Fi]−Xi≥d0·d−2
d

≤e−d2
0(1−2/d)2
≤e−d2
0/2=e−ϵ2d/8 log2(1/δ).
Finally, using the inequality Pr(A, B) = Pr( A|B) Pr(B)≥(min{Pr(A),Pr(B)})2completes the
proof.
Proofs of Theorem B.3.2 and Theorem B.3.3 IfPr(|Dict|< d0)≤ϵ−ϵ2d/8 log2(1/δ)the proof
of Theorem B.3.2 concludes. Otherwise, consider the case Pr(|Dict|< d 0)> ϵ−ϵ2d/8 log2(1/δ),
whereby, E[X(s′,Dict)|Dict]≤2d0/dwith probability ≥1−e−ϵ2d/8 log2(1/δ)conditioned on
|Dict|< d0by Lemma B.11. Recall that when |Dict|< d0, Algorithm 1 uses a parallel implementa-
tion of the sequential encoder which chunks a new string into pieces of length d, denoted {chunk i:
i∈[d]}and uses the sequential encoder under Dictdto tokenize each chunk. Note that since the
source is Markovian, the chunked process {chunk i= (Xid+1, Xid+2,···, X(i+1)d):i= 1,2,···}
is also Markovian and ergodic. Therefore, by a similar limiting argument as in Lemma A.4, using the
Krylov–Bogolyubov argument (cf. Proposition 4.2 in Chen (2018)) for Markov processes, we have
that,
lim
ℓ→∞Pℓ
i=1X(chunk i,Dict)
ℓ=E[X(s′,Dict)]≤2d0
d.
where s′is a fresh string of length dsampled with initial state distribution as the stationary measure of
the stochastic source. On the remaining (limiting) 1−2d0/dfraction of the chunks, their sequential
encodings have every pair of tokens appearing at most log(d)times consecutively. Using Theorem 1
of Navarro and Russo (2008), the number of tokens in the encoding of each of these chunks cannot
be too large, and satisfies,
|enc BPE(chunk i)| ·log|enc BPE(chunk i)| ≤2dH∞+O(d/log(d))
=⇒ |enc BPE(chunk i)| ·logd≤2dH∞+O(d/log(d)) (27)
For the (limiting) 2d0/dfraction of the “bad” chunks, their sequential encodings may have one or
more pairs of tokens which appear more than log(d)times consecutively.
33Define Ei={X(chunk i,Dict) = 1}where Dict=Dictdis the dictionary returned by Algorithm 1
and consider the unigram model Quni(t) =1
2Q1(t) +1
2Q2(t), which is the uniform mixture of two
models,
Q1(t)∝1
(2|A|)|t|,and Q2(t) =E"
n1
t
|enc BPE.split (chunk 1)|Ec
1#
,
and let Quni(t1,···,ti) =Q#(j)Qi
j=1Quni(ti)for some distribution Q#(i)over the number of
tokens to be chosen later. We will analyze the case where the total number of chunks ℓis finite and
take the limit m→ ∞ later. Then, the overall loss of the algorithm is,
Lm(Quni◦enc(·))
=−E[logQuni(enc BPE.split (s))]
=−X
t∈DictE[ntlogQuni(t) + log Quni(|enc BPE.split (s)|)]
(i)=−ℓX
i=1E
X
t∈Dictni
tlogQuni(t)
+ log( m)
=−ℓX
i=1E
X
t∈Dictni
tlogQuni(t)Ei
Pr(Ei) +E
X
t∈Dictni
tlogQuni(t)Ec
i
Pr(Ec
i) + log( m).
(28)
where ni
tis the number of times tis observed in the BPE encoding of chunk iand(i)uses the fact
that|enc BPE.split (s)|follows some distribution supported on [m], which implies its entropy is upper
bounded by log(m). First observe that,
ℓX
i=1E
X
t∈Dictni
tlogQuni(t)Ec
i
≤ℓX
i=1E"
|enc BPE(chunk i)| ·X
t∈Dictni
t
|enc BPE(chunk i)|logQuni(t)Ec
i#
(i)
≤ℓ2dH∞+O(d/log(d))
log(d)X
t∈DictQ2(t) logQuni(t)
where (i)uses the upper bound on |enc BPE.split (chunk i)|under the event Ec
i(eq. (27)). Since
Quni(t) =1
2Q1(t) +1
2Q2(t)≥1
2Q2(t)andQ2is a distribution supported on at most dtokens, this
term results in the upper bound,
ℓX
i=1E
X
t∈Dictni
tlogQuni(t)Ec
i
≤ℓ2dH∞+O(d/log(d))
log(d)
log(2d). (29)
On the other hand, since Quni(t)≥1
2Q1(t),
ℓX
i=1E
X
t∈Dictni
tlog(1/Quni(t))Ei
≤ℓX
i=1E
X
t∈Dictni
tlog(2/Q1(t))Ei

≤ℓX
i=1E
X
t∈Dictni
t 
log(2) + |t|log(2|A|)Ei

≤ℓdlog(2) + ℓdlog(2|A|) (30)
where the last inequality uses the fact thatP
t∈Dictni
t≤dandP
t∈Dict|t|ni
t=dcomputes the length
ofchunk i.
Overall, sincePℓ
i=1Pr(Ei)≤2d0/dby eq. (27), combining this with eqs. (28) to (30),
Lm(Quni◦enc(·))≤
1−2d0
d
ℓ2dH∞+O(d/log(d))
log(d)
log(2d) +2d0
dℓdlog(4|A|).
34Dividing throughout by the length of the character sequence m∈[d(ℓ−1), dℓ]and letting ℓ→ ∞ ,
min
Q∈Q1-gramL(Q◦enc(·))≤ L(Quni◦enc(·))≤
1−2d0
d 
2H∞+O1
log(d)!
+2d0
dlog(4|A|).
C Additional Theoretical Results II: Learning the likelihood model
The guarantees we prove in Theorems 3.1, 3.6 and B.2 on various tokenizers assume that the
downstream model is trained optimally. In practice, these models are trained from a finite dataset
and the sample complexity of learning this likelihood model scales with the number of tokens in the
dictionary. In this section, we step away from the transformer architecture and focus on analyzing the
performance of a simple estimator for the unigram model based on Laplace smoothing. We leave the
problem of analyzing the finite-sample statistical error of simple transformer models trained with
gradient descent as an interesting open direction for future research.
The result of Theorem 3.1 establishes that under appropriate assumptions on the Markov source, there
exists a tokenizer Tand a unigram model over tokens Q⋆∈ Q 1-gram such that,
lim
m→∞1
mE
log(1/Q⋆(enc(s))
≤(1 +ε)·lim
m→∞1
mE
log(1/P(s))
Or in other words,
lim
m→∞1
mKL(P, Q⋆(enc(·)))≤ε·lim
m→∞1
mE
log(1/P(s))
.
This implies that with the appropriate tokenization, the measure associated to the string by the best
unigram model over tokens is close to that induced by the true Markov distribution over characters
in KL divergence. In this section, we establish finite-sample guarantees on learning Q⋆specifically
for the LZW tokenizer. The approach we consider for distribution learning is a smoothed Laplace
estimator described in more detail in Algorithm 2.
For any constant θ∈(0,1), define Eθas the event that every maximal token t(Definition A.5) in
the LZW dictionary satisfies 1/d1−θ≥max aP(t|a)≥δ/d1+θ. By Lemmas A.10 and A.11 when
the LZW tokenizer is trained on a dataset of size eΩδ(d)drawn from a stochastic source satisfying
Assumption 3.2, Eθoccurs with probability ≥1−d−Ωθ,δ(log(d)).
Theorem C.1. Consider any constant θ∈(0,1), failure probability η∈(0,1)and approximation
error ξ∈(0,1). Assume that the learnt LZW tokenizer TLZWsatisfies the event Eθ, which occurs
with probability ≥1−d−Ωθ,δ(log(d)). Assume that d1−3θ≥1 +δ−2and that the stochastic source
satisfies Assumption 3.2. For an absolute constant C >0, assume that the size of the training dataset
is at least n⋆
lm(ξ), where,
n⋆
lm≜Cd1+θlog3(d/ηδ) log log( d/η)
δξ2
Then, Algorithm 2 learns a unigram model bQsuch that,
L(bQ◦enc gre(·))≤(1 +ξ) min
Q∈Q1-gramL(Q◦enc gre(·))
with probability ≥1−η.
In conjunction with Theorem 3.6, this gives end-to-end guarantees on the cross-entropy loss of the
LZW tokenizer (with vocabulary size ≤d) with the Laplace estimator as the downstream unigram
model. We instantiate this result choosing θ= 0.01in Theorem C.1.
Corollary C.2. Choose any ξ∈(0,1). Suppose the data source satisfies Assumption 3.2. On a
dataset of size eΩδ(d)drawn from the source, train an LZW tokenizer TLZWwithdtokens. Subsequently,
using Algorithm 2, learn a unigram model bQusing a dataset of size at least eΩ(d1.01/δξ2)drawn
from the source. Then, with probability ≥1−d−Ωδ(log(d)),
L(bQ◦enc gre(·))≤1 +ξ
1−εmin
QL(Q),
where ε=log(1/δ)
0.99 log( d).
35The analysis of Theorem C.1 relies on showing that the distribution over tokens induced when a string
sampled from the data source is encoded into tokens by the greedy encoder and the LZW dictionary is
a Markov process. In general, given a set of previously sampled tokens t1,···,ti, the next token ti+1
is sampled from the distribution P(ti+1|ti;∀j∈[i],ti−j+1···titi+1̸∈Dict). The conditioning
is to simply guarantee that the previous tokens which were sampled were indeed maximal, since
iftiti+1∈Dict, then the previous token returned would in fact have been this longer token and
notti(and likewise for ti−1titi+1and so on). While in general, this process is complicated and
depends on all the previous tokens sampled, for the LZW dictionary, we show that the conditioning
{∀j∈[i],ti−j+1···titi+1̸∈Dict}can be removed, thereby resulting in a simple Markov process
over tokens.
Furthermore, we establish that this Markov process has a relatively large spectral gap. The optimal
unigram model ends up being the stationary distribution over tokens induced by greedy encoder.
Given the large spectral gap of the Markov process over tokens, estimating the stationary distribution
of this process in KL divergence ends up being closely related to estimating a distribution from
i.i.d. samples in the same metric. For this problem, the de-facto choice of estimator is the Laplace
estimator, and several existing results provide finite-sample bounds on the KL divergence (Braess
and Sauer, 2004; Han et al., 2021; Mourtada and Gaïffas, 2022). The Laplace estimator (Line 6
of Algorithm 2) is simply a smoothed empirical estimate to account for the degeneracy of the KL
divergence in its second argument as any coordinate approaches 0. The non-i.i.d.ness of the Markov
process is circumvented by using concentration inequalities which are a function of the spectral gap
(Naor et al., 2020).
Algorithm 2 Training likelihood model on tokens
Input: A training dataset of size nlm, likelihood model class Q, likelihood model training
algorithm TrainLM
Output: Likelihood model Q∈ Q.
1:Tokenize the training dataset into a sequence of tokens T= (t1,···,ti).
2:Train a likelihood model Qon the tokenized dataset Tusing the TrainLM (T,Q)subroutine.
// In the case of Q=Q1-gram use the Laplace estimator
defTrainLM (T,Q1-gram):
3:Truncate the dataset to the first n′=⌊nlm/ℓmax⌋tokens where ℓmax= 4 log( d|A|)/δ. Let the
truncated dataset be Ttrunc
4:Construct the unigram model bQwithbQ#=Unif([m])andbQtok(t) =nt+1
nt+|Dict|.
//ntis the number of times tappears in Ttrunc.
// Test sequences are assumed to be of length m.
C.1 Proof of Theorem C.1
SinceTLZWuses the greedy encoder, the cross-entropy loss of the unigram model learnt by Algo-
rithm 2 is,
L(bQ◦enc gre(·))−min
Q∈Q1-gramL(Q◦enc gre(·))
= max
Q∈Q1-gramlim
m→∞1
mE[log(Q(enc gre(s))/bQ(enc gre(s)))]
(i)= max
Q∈Q1-gramlim
m→∞1
mE
|enc gre(s)|X
t∈Dictnt
|enc gre(s)|log(Qtok(t)/bQtok(t))
+log(m)
m
(ii)
≤lim
m→∞1
mE
|enc gre(s)|X
t∈Dictnt
|enc gre(s)|log 
nt/|enc gre(s)|
bQtok(t)!
+log(m)
m
where in (i)we use the fact that bQ#=Unif([m])and in (ii)we take the max{·}inside the limit and
the expectation (Fatou’s lemma and Jensen’s inequality) and plug in the maximizer of the negative
cross-entropy, Qtok(t) =nt
|enc gre(s)|. Note that limm→∞nt
|enc gre(s)|a.s.=QMLE(t)by Lemma A.4.
36Moreover, since |enc(s)|/m≤1andbQtok(t)>0surely, by the Dominated Convergence Theorem,
L(bQ◦enc gre(·))−min
Q∈Q1-gramL(Q◦enc gre(·))≤lim
m→∞1
mE[|enc gre(s)|]·KL(QMLE,bQtok)(31)
By eq. (6), we have that for any tokenizer using the greedy encoder,
lim
m→∞|enc gre(s)| 
H(QMLE, P)−log(1/δ)
ma.s.
≤H∞.
Furthermore under the event Eθwhich implies that the learnt dictionary is (1−θ)-heavy hitting (cf.
Definition A.5), which implies that,
H(QMLE, P)≥(1−θ) log( d).
Therefore, by almost sure boundedness, we have that,
lim
m→∞1
mE
|enc gre(s)|
≤H∞
(1−θ) log( d)−log(1/δ)≤minQ∈Q1-gramL(Q◦enc(·))
(1−θ) log( d)−log(1/δ)
Putting this together with eq. (31), we have that,
L(bQ◦enc gre(·))≤
1 +KL(QMLE,bQtok)
min
Q∈Q1-gramL(Q◦enc gre(·)), (32)
which uses the assumption (1−θ) log( d)≥1 + log(1 /δ). In the remainder of the proof we upper
bound the KL term.
By the law of large numbers established in eq. (34) and the fact thatntP
t′nt′∈[0,1], we have that,
QMLE(t) = lim
m→∞E"
ntP
t′nt′#
= lim
m→∞E[nt]
EP
t′nt′=π(t),
where π(t)denote the stationary distribution over tokens induced by the greedy encoding process,
which exists for the LZW tokenizer. This distribution is in fact an ergodic Markov process, as we
discuss next.
By Lemmas A.10 and A.11, for any constant θ∈(0,1), with probability ≥1−d−Ωθ,δ(log(d)), every
maximal token in the the LZW dictionary satisfies 1/d1−θ≥max aP(t|a)≥δ/d1+θ. Let Sgre
denote the set of tokens which have a non-zero probability (over a string drawn from the Markov
source) of being chosen by the greedy encoder while encoding the string. More importantly, note
that for any sequence of tokens t1,···,ti, the next token is necessarily in Sgreand can be any
token in this set. The reason for this is that for any ti,t∈Sgre, the concatenation tit̸∈Sgresince
max a∈AP(tit|a)≤1/δd2(1−θ), which is smaller than the max a∈AP(t′|a)≥δ/d1+θfor any token
t′∈Sgreas long as d1−3θ≥1/δ2. This constraint implies that in the sampling procedure in Figure 7,
it suffices to drop the conditioning on the event tjtj+1···tit̸∈Dict while sampling the next token
t. This condition automatically implies that the sequence of tokens conditionally follows a Markov
process with Pr(ti+1=t|t1,···,ti) =P(t|last(ti)). Since the probability of every transition is
lower bounded, this means that the Markov chain is ergodic. Moreover, the pseudo-spectral gap
(Naor et al., 2020), 1−λcan be lower bounded by the Dobrushin contraction coefficient, κ,
1−λ≤κ≜max
(t,t′)∈Dict2∥Pr(·|t)−Pr(·|t′)∥TV
= max
(t,t′)∈Dict21−X
t′′∈Dictmin{Pr(t′′|t),Pr(t′′|t′)}
≤1−δd/d1+θ
= 1−δd−θ. (33)
Recall that the learner is given a training dataset of nlmcharacters to train the likelihood model. By
Lemma A.8, with probability ≥1−d−Ω(log( d/δ)/δ), in the run of the LZW tokenization algorithm,
every token in the dictionary has length at most ℓmax= 4 log( d|A|)/δ. Therefore, suppose the learner
37always truncates the dataset to the first n′=⌊nlm/ℓmax⌋tokens and runs the Laplace estimator on
this truncated dataset. With this, we move onto upper bounding,
KL(QMLE,bQtok) =X
t∈Dictπ(t) log
π(t)/bQtok(t)
which necessitates lower bounding bQtok(t)for every t. Recall that the learner’s estimate bQ(t)in
Algorithm 2 is the Laplace estimator,nt+1P
t′nt′+Dict, where {nt:t∈Dict}is computed by truncating
the dataset to the first n′tokens. Firstly, by invoking Corollary 1.3 of Naor et al. (2020) for the
function nt=Pn′
i=1I(ti=t),
Pr 
|nt−E[nt]| ≥cr
E[nt]
1−λ·log(1/η)!
≤η (34)
for a universal constant c >0. In particular, this implies that with probability ≥1−η, simultaneously
for all t,
|nt−E[nt]| ≤∆t≜r
dθ
δE[nt]·log(|Dict|/η), and, E[nt]−nt≥E[nt].
Under this event, for any t, the estimate is lower bounded by,
bQtok(t) =nt+ 1
n′+|Dict|≥E[nt] + 1−min{E[nt],∆t}
n′+|Dict|
≥max
π(t)−(∆t−1)n′+|Dict|E[nt]
(n′)2+n′|Dict|,1
n′+|Dict|
≥max
π(t)−∆tn′+|Dict|E[nt]
(n′)2,1
n′+|Dict|
Suppose the following condition is satisfied,
n′=4rdθ|Dict|log(|Dict|/η)
δ(C1)
for some r≥4. Under this condition, we have that n′≥2√r∆andn′≥4r|Dict|.
Case I. ∆tn′≥ |Dict|E[nt].
In this case, we have the upper bound,
bQtok(t)≥max
π(t)−2∆t
n′,1
n′+|Dict|
= max

π(t)−2q
dθ
δE[nt]·log(|Dict|/η)
n′,1
n′+|Dict|


≥max

π(t)−s
π(t)
r|Dict|,1
2n′

.
where the last inequality uses eq. (C1).
Consider two sub-cases,
Sub-case I. π(t)≥2/r|Dict|. Define this event CI.
Here,
π(t) log( π(t)/bQtok(t))≤ −π(t) log
1−s
1
π(t)r|Dict|
≤3
2s
π(t)
r|Dict|. (35)
38Sub-case II. π(t)≤2/r|Dict|. Define this event CII.
Here,
π(t) log( π(t)/bQtok(t))≤π(t) log 
2n′π(t)
≤max(
0,2
r|Dict|log4n′
r|Dict|)
≤2
r|Dict|log
16dθlog(|Dict|/η)
(36)
Case II. ∆tn′<|Dict|E[nt]. Define this event CIII.
In this case we have the upper bound,
bQtok(t)≥π(t)−2|Dict|E[nt]
(n′)2≥π(t)−π(t)
2r
where the last inequality follows from eq. (C1). This implies that,
π(t) log( π(t)/bQtok(t))≤ −π(t) log(1 −1/2r)≤π(t)
r. (37)
By using the geometric ergodicity of this Markov process (eq. (33)), when n′tokens are sampled
from an arbitrary initial distribution,

1−κn′
π(t)≤E[nt]
n′≤κn′+
1−κn′
π(t) =⇒π(t)≤bQtok(t)
1−e−4r|Dict|log(|Dict|/η)=bQtok(t)
1−d−r
where in the implication, we use the condition on n′in eq. (C1) and the bound on the contraction
coefficient κin eq. (33).
KL(QMLE,bQtok)
=X
t∈Dictπ(t) log( π(t)/bQtok(t))
≤X
t∈Dictπ(t)
1−d−rlog(π(t)/bQtok(t))−log(1−d−r)
≤1
1−d−rX
t∈DictI(CI)π(t) log( π(t)/bQtok(t)) +I(CII)π(t) log( π(t)/bQtok(t)) +I(CIII)π(t) log( π(t)/bQtok(t)) + 2 d−r
≤1
1−d−rX
t∈DictI(CI)3
2s
π(t)
r|Dict|
|{z }
eq.(35)+I(CII)2
r|Dict|log
16dθlog(|Dict|/η)
| {z }
eq.(36)+I(CIII)π(t)
r
|{z}
eq.(37)+2d−r
≤1
1−d−r
3
2s
|Dict|
r|Dict|+2
rlog(16 dθlog(|Dict|/η)) +1
r
+ 2d−r
≤5√rlog(16 dθlog(|Dict|/η))
Combining with eq. (32), we get the bound,
L(bQ◦enc gre(·))≤
1 +KL(QMLE,bQ)
min
Q∈Q1-gramL(Q◦enc gre(·))
≤
1 +5√rlog(16 dθlog(d/η))
min
Q∈Q1-gramL(Q◦enc gre(·)).
Rescaling rto be r(log(16 dθlog(d/η)))2completes the proof.
39DAdditional Theoretical Results III: The generalization ability of tokenizers
The proofs of the upper bounds in the paper (Theorems 3.6 and B.2) relied on showing that the
entropy H(QMLE, P)is large, or in other words, the algorithm typically encodes new strings into
long length (i.e. low probability under P) tokens. This statement about generalization to new strings
is fundamentally different from having a tokenizer which compresses the training dataset well. In
other words, consider the following modification: the measure QMLEis defined as the expected
empirical distribution over tokens when a new string is encoded into tokens, and not on the source
dataset used to construct the dictionary. Suppose the definition of QMLEis changed to the empirical
distribution over tokens in the source dataset. Under this new definition of the MLE unigram model,
the largeness of the H(QMLE, P)metric, in a sense, captures compressing the source dataset well.
However, we show that in general, this does not result in good tokenizers that minimize the population
cross-entropy loss, suffering from minQ∈Q1-gramL(Q◦enc(·))≈H(π)≫H∞.
Theorem D.1. Consider the stochastic source in example A.1 having entropy rate H∞=δlog(1/δ)+
(1−δ) log(1 /(1−δ)). Consider a training dataset of size n. For a dictionary Dict and t∈Dict,
definebQMLE(t) =nt(ssrc)
|enc(ssrc)|as the empirical distribution over tokens induced by the greedy encoder
when encoding the training dataset, ssrc. There exists a dictionary Dict such that with probability
≥1−e−Ω(√n)over the training dataset,
H(bQMLE, Pγ)≥nH∞(1−O(n−1/4))
is large. However, for this dictionary, for any encoding algorithm (including the greedy encoder), the
resulting tokenizer T= (Dict,∅,enc(·),dec(·))satisfies,
min
Q∈Q1-gramL(Q◦enc(·))≥(1−ε)H(π)
where ε= 2ne−nH∞(1−O(n−1/4)).
Proof. Suppose the entire training dataset was compressed into a single token, tsrc. The dictionary
isA ∪ tsrc. In the following argument, we show that the number of occurrences, ntsrc, of the entire
training dataset tsrcin a new string of length mgenerated from the stochastic source, s, converges to
its expectation as m→ ∞ . Letπ(i)
ndenote the stationary distribution of the Markov process induced
by the stochastic source over length- nstrings with a shift of ifrom the starting position, and let n(i)
t
denote the number of times tappears in the training dataset starting at the position i+rnfor some
r >0. Then,
lim
m→∞ntsrc
m=1
nlim
m→∞n−1X
i=0n(i)
tsrc
m/na.s.=1
nn−1X
i=0Et′∼π(i)
n[P(tsrc|t′)]≤max
a∈AP(tsrc|a). (38)
The second equation follows by considering the Markov process induced over length nstrings and
applying the Krylov–Bogolyubov argument for ergodic and homogeneous Markov processes.
In Lemma D.2, we show that with probability ≥1−e−Ω(√n), the token tsrcconstructed from the
source dataset satisfies, max a∈AP(t|a)≤e−nH∞(1−O(n−1/4)). In other words, the source string
has exponentially small probability. Combining this with eq. (38), with probability ≥1−e−Ω(√n)
over the source dataset, the number of occurrences of the substring tsrcin a new string sis upper
bounded by,
lim
m→∞ntsrc
ma.s.
≤e−nH∞(1−O(n−1/4))≜ε/2n.
By the Krylov–Bogolyubov argument, for each a∈ A ={0,1},limm→∞na
ma.s.=π(a). More
importantly, the number of times ais made as a token is upper bounded by naand lower bounded by
na−nntsrc. Therefore,
(1−ε)π(a) =π(a)−ε
2a.s.
≤lim
m→∞na
ma.s.
≤π(a) =1
2(39)
40Finally, putting everything together,
min
Q∈Q1-gramlim
m→∞1
mLm(Q◦enc(·)) = min
Q∈Q1-gramlim
m→∞−1
mE
log(Q#(|enc(s)|) +X
t∈DictntlogQtok(t)
≥min
Q∈Q1-gramlim
m→∞−1
mEX
a∈AnalogQtok(a)
(i)
≥min
Q∈Q1-gram−(1−ε)X
a∈Aπ(a) logQtok(a)
≥(1−ε)H(π).
where (i)follows from the lower bound on na/min eq. (39). This completes the proof.
Lemma D.2. With probability ≥1−e−Ω(√n)over the source dataset,
max
a∈AP(tsrc|a)≤e−nH(δ)(1−O(n−1/4)).
Proof. LetXdenote the number of i∈[n−1]such that si̸=si+1ins, the stochastic source. Since
the transition of the Markov process only depends on whether the next character is the same as the
previous character, we can write down,
max
a∈AlogP(tsrc|a) =−(X+ 1) log( δ)−(n−1−X) log(1 −δ).
Note that Xis a sum of n−1i.i.d. random variables, since I(si̸=si+1)∼Ber(δ)does not depend
on whether si= 0or= 1. In particular, by Hoeffding’s inequality, we have that with probability
≥1−e−Ω(√n),
1
nmax
a∈AlogP(tsrc|a)−H(δ)≤O
n−1/4
,
which uses the fact that E[X] =δ(n−1)andH∞=δlog(1/δ) + (1 −δ) log(1 /(1−δ)). Taking
an exponential on both sides proves the statement of the lemma.
EAdditional Theoretical Results IV: Interaction between the dictionary and
encoding algorithm
In this section, we show another kind of barrier to generalization, which brings out the relationship
between the encoding algorithm and the dictionary. We show that there exist dictionaries which
generalize under the minimal encoder, i.e. the encoding algorithm which encodes a string into the
shortest number of possible tokens, but at the same time, completely fail to generalize under the
greedy encoder. This means that in the process of constructing good tokenizers, it does not suffice to
think about the dictionary in isolation. Its interaction with the encoding algorithm is pertinent.
Definition E.1 (minimal encoder) .The minimal encoder parses a new string into the fewest possible
number of tokens from the dictionary as possible. Ties are broken arbitrarily.
Theorem E.2. There exists a stochastic source parameterized by δ∈(0,0.5)and a dictio-
nary Dict such that under the minimal encoder/decoder pair, the resulting tokenizer, T=
(Dict,∅,enc min(·),dec min(·))generalizes near-optimally,
min
Q∈Q1-gramL(Q◦enc min(·))≤1.273H∞. (40)
Here the entropy rate of the source, H∞, isδlog(√
2/δ) + (1 −δ) log(1 /(1−δ)). However, the
same dictionary Dict under the greedy encoder/decoder pair, i.e. T′= (Dict,∅,enc gre(·),dec gre(·)),
generalizes poorly, suffering from cross-entropy scaling as,
min
Q∈Q1-gramL(Q◦enc gre(·))≥1−oδ(1)
3H(π). (41)
where the entropy of the stationary distribution of the source is H(π) =1
2log(8) and the 1−oδ(1)
term is (1−δ)2(1 +δ)−1.
410 1 2δδ
2
δ
2δ
Figure 10: order- 1Markov source used in the proof of Theorem E.2
This means that the greedy encoder is not really compatible with the dictionary in the sense that the
cross-entropy loss of the tokenizer is a constant multiple away from that achieved by the character-
level tokenizer. The separation between eq. (40), and eq. (41) only manifests as δbecomes smaller
and smaller.
In this section, we prove that generalization of a dictionary is a function of the underlying tokenization
algorithm used. In particular, the greedy encoder is not universal, and there exists dictionaries under
the minimum-length encoder/decoder which achieve small cross-entropy loss, which do not generalize
under the greedy encoder/decoder.
We split the proof of Theorem E.2 into two parts. We first define the stochastic source and dictionary
we consider. Then we show that under the minimum-length encoder, the asymptotic cross-entropy
loss is upper bounded by H∞up to a constant. Finally, we show that under the greedy-encoder, the
same dictionary suffers from high cross-entropy loss, which is a constant factor away from that of the
character encoder.
E.1 Stochastic source and dictionary.
Consider an extension of the switching Markov source in example A.1 to A={0,1,2}. The Markov
chain is described in Figure 10. The transition of the Markov chain is P(0|0) = P(1|1) = P(2|2) =
1−δ, andP(1|0) = P(2|1) = δandP(2|1) = P(0|1) = δ/2, with the remaining transitions being
0-probability. For a parameter ℓ >0to be instantiated later, define S1(resp. S0,S2) as the set of
all-1(resp. all- 0, all-2) strings of length ≤ℓ−1, including the empty string. Consider a dictionary
composed of the following set of tokens, {1s:s∈S0∪S1∪S2}. Therefore, the tokens follow the
template 10···0,11···1or12···2and are of length at most ℓ.ℓis chosen to be 1 + 2 log(1 /δ)/δ.
Although we use the minimal encoder in the statement of Theorem E.2, for the purpose of analysis,
define the following encoding algorithm: if the new string is prefixed by 10···0or12···2, select
the largest prefix which exists in dictionary and assign it as a token. If the new string starts with a
sequence 11···1of length x, consider the first max{ℓ, x−1}length prefix and assign it as a token.
Finally, if the string starts with 0or2, assign that character as token. Once the first token has been
assigned, remove it and repeat.
E.2 Minimal encoder achieves the optimal cross-entropy loss up to a constant.
First consider a simplification of the overall cross-entropy loss,
min
Q∈Q1-gramlim
m→∞1
mLm(Q◦enc(·))
= min
Q∈Q1-gramlim
m→∞−1
mE
logQ#(|enc min(s)|) +X
t∈DictntlogQtok(t)
(42)
≤lim
m→∞1
mE
log(m) +|enc min(s)|log|Dict|
, (43)
where in the last inequality we upper bound by choosing Q#= Unif([ m])andQtok(t) = 1 /|Dict|.
Note that |Dict| ≤2ℓ+ 1and letting limm→∞log(m)/m= 0,
min
Q∈Q1-gramlim
m→∞1
mLm(Q◦enc(·))≤lim
m→∞1
mE[|enc min(s)|log(2ℓ+ 1)]
≤lim
m→∞1
mE
|enc(s)|log(2ℓ+ 1)
, (44)
42where in (i), we replace |enc min(s)|by|enc(s)|, which is the encoder we define in Appendix E.1.
By definition of the minimal encoder, |enc min(s)| ≤ |enc(s)|surely. Recall that the encoder enc(·)
processes strings in a sequential (left-to-right) manner. In particular, by a similar argument as
Lemma A.4, we can show that under this encoder, the limit nt/P
t′nt′almost surely converges to
its expectation. More importantly, since,P
t∈Dict|t|nt=m, we have that,
lim
m→∞|enc(s)|
ma.s.=1
Et∼QMLE[|t|].
converges to some limit almost surely. Therefore, from eq. (44),
min
Q∈Q1-gramlim
m→∞1
mLm(Q◦enc(·))≤esslimsup
m→∞|enc(s)|
mlog(2ℓ+ 1). (45)
where the essential lim-sup captures the almost sure limit 1/Et∼QMLE[|t|]. The almost sure conver-
gence of |enc(s)|/malso implies that we can let the limit mgo to∞in any manner, and the limit
will remain the same. In particular, consider a process parameterized by i⋆for generating the source
string, such that surely m≥i⋆, where the total number of characters, m, is a random variable. As
i⋆→ ∞ , we will also have m→ ∞ surely, and so the limit of |enc(s)|/munder this modified
stochastic process should also converge to the same limit.
Rather than sampling a string of a fixed length mfrom the source, consider the following sampling
model: for i⋆→ ∞ , sample i⋆geometric random variables X1,···, Xi⋆i.i.d.∼Geo(δ)and construct
the source string as the concatenation of i⋆strings alternating between successive 1’s and successive
0’s or2’s (with the choice between the two made uniformly at random), with the ithstring of length
Xi+ 1. The overall number of characters sampled, m, is surely at least i⋆.
Under this stochastic process, the size of the encoding of the string is upper bounded by,
|enc(s)| ≤ |X1+ 1|+i⋆X
i=2
1 + (Xi+ 1−ℓ)+
This bound follows from the fact that in any substring s′of successive 1’s followed by a substring s′′
of successive 0’s or2’s, the encoder tokenizes the first max{ℓ,|s′| −1}length prefix of s′as a token,
and the remaining characters in s′into individual tokens except the last. Then, the last character of s′
and the first max{ℓ−1,|s′′|}characters of s′′are assigned as token. The remainder of s′′is assigned
as individual tokens. Each of s′ors′′of length x, is allocated into at most 1 + (x+ 1−ℓ)+tokens.
For any i,Pr(Xi≥u) = (1−δ)u, and therefore, summing over u≥ℓ, we get that E[(Xi+1−ℓ)+] =
(1−δ)ℓ−1
δ. With ℓ= 1 + 2 log(1 /δ)/δ, this expectation is upper bounded by δ. Therefore,
lim
i⋆→∞E[|enc(s)|]
i⋆≤lim
i⋆→∞1
i⋆E
|X1|+Xi⋆
i=2
1 + (Xi+ 1−ℓ)+
≤1 +δ
More importantly, by the strong law of large numbers for a sum of independent random variables,
(|X1+1|+Pi⋆
i=2(1+( Xi+1−ℓ)+))/i⋆, and therefore |enc(s)|/i⋆is asymptotically almost surely
upper bounded as,
lim
i⋆→∞|enc(s)|
i⋆a.s.
≤1 +δ, (46)
On the other hand, the number of characters generated, m, equalsPi⋆
i=1(Xi+ 1) , and satisfies,
limi⋆→∞E[m]/i⋆= 1 + δ−1. By another application of the strong law of large numbers for a sum
of independent random variables,
lim
i⋆→∞m
i⋆a.s.= 1 + δ−1. (47)
By combining eqs. (46) and (47), we have that,
lim
i⋆→∞|enc(s)|
ma.s.
≤1 +δ
1 +δ−1=1
δ.
43Finally, combining with eq. (45) and the ensuing discussion, we may upper bound the limiting
cross-entropy loss by,
min
Q∈Q1-gramlim
m→∞1
mLm(Q◦enc(·))≤δlog(2ℓ+ 1) = δlog(3 + 4 log(1 /δ)/δ).
Note for this Markovian source, it is a short calculation to see that,
H∞=Ex∼π[H(P(·|x))] = δlog(√
2/δ) + (1 −δ) log(1 /(1−δ))
Note that for any δ≤1/2, numerical evaluation gives the inequality,
1≤δlog(3 + 4 log(1 /δ)/δ)
H∞≤1.273
with the approximation factor improving as δbecomes smaller. Therefore, this tokenizer achieves a
normalized cross-entropy loss which asymptotically scales as a constant multiple of the entropy rate
of the source.
E.3 Greedy-encoder achieves poor cross-entropy loss
Note that the greedy encoder picks the largest prefix of the string which is a token, assigns and
removes it, and iterates on the rest of the string. The greedy encoder’s behavior is easy to analyze -
every string of consecutive 1’s in the new string is broken into chunks of length ℓ(save potentially
the last chunk) and each chunk is assigned as a token in {1s:s∈S1} ⊂Dict. If the length of this
substring of successive 1’s is not 1, ℓ+ 1,2ℓ+ 1,···, or in general, ≡1 mod ℓ, every character in
the next sequence, composed of 0’s or2’s is tokenized into individual characters.
Similar to eq. (42) to eq. (43), consider a simplification of the overall cross-entropy loss,
min
Q∈Q1-gramlim
m→∞1
mLm(Q◦enc gre(·))
= min
Q∈Q1-gramlim
m→∞−1
mE"
logQ#(|enc gre(s)|) +|enc gre(s)|X
t∈Dictnt
|enc gre(s)|logQtok(t)#
≥min
Q∈Q1-gramlim
m→∞−1
mE"
|enc gre(s)|X
t∈Dict
QMLE(t)>0QMLE(t) logQtok(t)#
,
where the last equation uses the fact that by Lemma A.4, for the greedy en-
coder, limm→∞nt
|enc min(s)|a.s.=QMLE(t). The minimizer of this objective subject toP
t∈Dict:QMLE(t)>0Qtok(t)≤1isQtok(t) =QMLE(t)resulting in the inequality,
min
Q∈Q1-gramlim
m→∞1
mLm(Q◦enc gre(·))≥lim
m→∞1
mE
|enc gre(s)|H(QMLE)
, (48)
where we use the convention 0 log(1 /0)≜limP→0Plog(1/P) = 0 and therefore we may sum over
tokens such that QMLE(t) = 0 for free.
Considering the same geometric sampling model as in Appendix E.2, and Lemma A.4, we may study
the almost sure limit QMLE(t) = lim m→∞nt/|enc gre(s)|by computing limi⋆→∞nt/|enc gre(s)|
under the geometric sampling model since the almost sure limit exists. Recall that in the geometric
sampling model, we generate the overall source string by concatenating i⋆strings of length X1+
1,···, Xi⋆+ 1 where Xi∼Geo(δ), with the strings alternating between successive 1’s and
successive 0’s or 2’s (with the choice between the two made by the flip of a fair coin). For x∈
{0,1,2}, letEi(x)denote the event that Xiis a string composed only of all x’s. The length of the
greedy encoding of sis lower bounded by,
|enc gre(s)| ≥i⋆X
i=1Xi·I(Xi−1̸≡1 mod ℓ)I(Ei(0)∪ Ei(2)). (49)
44Which captures for the fact that all 0’s and 2’s are encoded into singular tokens unless the previous
string of 1’s was of length ≡1 mod ℓ. By the law of large numbers of the RHS of eq. (49), the
following a.a.s. lower bound is satisfied,
lim
i⋆→∞|enc gre(s)|
i⋆a.s.
≥1
2δ
1−∞X
u=0δ(1−δ)ℓu+1
=1
2δ
1−δ(1−δ)
1−(1−δ)ℓ
≥1−δ
2δ,(50)
where the last inequality uses the fact that ℓ= 1+2 log(1 /δ)/δ. Likewise, observe that, |enc gre(s)| ≤
msurely, and following the analysis in Appendix E.2 of eq. (47), we have that,
lim
i⋆→∞|enc gre(s)|
i⋆≤lim
i⋆→∞m
i⋆a.s.= 1 + δ−1. (51)
Forx∈ {0,2}, observe that the expected number of times the token xis observed in the encoding of
s,nxcan be written as,
nx≥i⋆X
i=1 
(Xi+ 1)·I(Xi−1̸≡1 mod ℓ)
I(Ei(x)). (52)
In particular, taking the expectation of eq. (52),
E[nx|E1(0)∪ E1(2)],E[nx|E1(1)]≥i⋆−1
4(1 +δ−1)
1−∞X
u=0δ(1−δ)ℓu+1
≥i⋆−1
4·1−δ2
δ.
(53)
Note that in any realization of the geometric sampling process, in eq. (52), either the odd indexed
substrings are all- 1’s or the even indexed substrings are all- 1’s. Therefore, surely, all the non-zero
terms in the above summation are of the same parity. Moreover, since the ithterm in the sum only
depends on XiandXi−1, conditioned on whether the non-zero parities are even or odd, nxcan be
written as a sum of ≈i⋆/2mutually independent terms. By the strong law of large numbers on each
of the conditional processes, eqs. (52) and (53) implies that for x∈ {0,2},
lim
i⋆→∞nx
i⋆a.s.
≥1−δ2
4δ.
To upper bound nx, note that it is upper bounded by the number of times the character xappears in
the source string, which by the strong law of large numbers a.a.s (after normalizing by i⋆), scales
as1/4δ. Finally, to bound QMLE(t)which is the sequential nature of the encoder, using a similar
proof as Lemma A.4, we can show that nt/P
t′nt′converges to the unigram MLE model for this
tokenizer. For the token x∈ {0,2},
lim
i⋆→∞nx
|enc(s)|=QMLE(x)≤E
lim
i⋆→∞nx
n2+n0
(54)
Using the a.a.s. upper and lower bounds on |enc(s)|,n0andn2derived in eqs. (51) and (54), we
arrive at lower and upper bounds on QMLE(x)forx∈ {0,2},
1
4≈1−δ
4=(1−δ2)
4δ(1 +δ−1)≤QMLE(x)≤1
2(1−δ2)≈1
2.
Since there are at least two tokens having probability bounded away from 0and1by a constant under
the MLE unigram model, the entropy of QMLEmust also be lower bounded by a constant. Indeed,
H(QMLE)≥2 min
1−δ
4≤y≤1
2(1−δ2)ylog(1/y).
It is easy to verify that for δ≤0.5, the minimizer is achieved at y=1−δ
4, which leads to the lower
bound,
H(QMLE)≥1−δ
2
log4
1−δ
45Architecture GPT-2
Batch size Grid-searched in {8,16,32}
Gradient acc. steps 1
Tokenizer dictionary size {10,20}
Tokenizer dataset size 10,000
Optimizer AdamW (β1= 0.9, β2= 0.95)
Learning rate 0.002
Scheduler Cosine
# Iterations 8000
Weight decay 1×10−3
Dropout 0
Sequence length 512
Embedding dimension Grid-searched in {10,20,30,40}
# layers Grid-searched in {1,2,4,8}
# heads Grid-searched in {1,2,4,8,16}
Repetitions 5
Table 3: Hyperparameter choices
Finally, combining this lower bound on H(QMLE)with eq. (48), we have that,
min
Q∈Q1-gramlim
m→∞1
mLm(Q◦enc(·)) = lim
i⋆→∞E|enc gre(s)|
mH(QMLE)
≥lim
i⋆→∞E|enc gre(s)|
m
·1−δ
2
log4
1−δ
(i)
≥1−δ
2δ(1 +δ−1)·1−δ
2
log4
1−δ
≥(1−δ)2
3(1 + δ)H(π)
where (i)follows from the lower bound on |enc gre(s)|in eq. (50) with the almost sure limit of min
eq. (47) and noting that |enc gre(s)|/m≤1surely. The last inequality follows by simplifying using
π= (1/4,1/2,1/4)andH(π) =1
2log(8) .
F Experiment details
Experiment 1 (Figures 4a and 4b). In this and previous experiments (Figures 2, 3a and 3b), we
train the transformers on a single GPU on an 8×A100 node. The wall-clock time measured does not
count time spent in validation loss evaluations. The hyperparameter choices are listed in Table 3.
Experiment 2 (Table 1). We evaluate pre-trained tokenizers on various datasets. In this experiment,
we do not evaluate the likelihood model on test sequences, rather, we estimate the cross-entropy of
the best unigram model by using the approximation,
−E"X
t∈DictntlogQMLE(t)#
≈ −X
t∈Dictbntlog(bQ(t)) (55)
wherebQ(t) =bntP
tbntis the MLE unigram model learnt from a finite dataset, which we choose here as
GLUE (Wang et al., 2019), and bntis the number of times the token tis observed in the encoding of
the dataset. This approximation allows us to separate the error stemming from learning a suboptimal
likelihood model which tends to have higher sample complexity requirements and focus on the
asymptotic error of the tokenizer.
46We use Monte-carlo sampling to approximate the cross-entropy loss estimator in eq. (55). These
approximations tends to underestimate the true cross-entropy loss due to the concavity of xlog(1/x)
close to 0. In general, the gap between the approximation and the true error is expected to grow
withk. Therefore, the true difference between the estimate of the best unigram model on a tokenizer
and the best k-gram model for k≥2on the character level tokenizer is likely to be larger than the
reported figures.
Experiment 3 (Figure 5). We train the LZW, BPE, Unigram and Wordpiece tokenizers with
dictionary sizes {5000,6000,8000,12000 ,20000 ,32000 ,50000 ,80000}. The cross-entropy loss
incurred by the best 1-gram model is estimated using eq. (55) while for k-gram models for k≥2, we
use Monte-carlo sampling to estimate the cross-entropy of the empirical k-gram model computed
using the GLUE dataset. For the k-gram models trained on the character level tokenizer, since the
vocabulary size is fixed, we instead plot the number of distinct k-grams on the x-axis. While this is
not a true measure of the number of parameters in the underlying k-gram model, we use this as a
proxy for the same.
47G NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and precede the (optional) supplemental material. The checklist does NOT
count towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
•[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The paper lists an empirical phenomenon (justified in Fig. 2) and theoretical
contributions justified in Theorems 3.1, 3.3 and 3.5
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Remark 3.3
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
48•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Assumption 3.2
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The code has been released along with the rest of the submission.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
49(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Instructions provided in the jupyter notebook.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Table 3
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
50Answer: [Yes]
Justification: All plots which allow for it, contain standard error bars.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Appendix F contains this information.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: No NeurIPS code of ethics were violated.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
51Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This work is a primarily theoretical study on the behavior of tokenization on
toy problems (learning Markov chains). The societal impact of this research is not likely to
be significant.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: No models with a high risk for misuse were trained or released.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Code has been properly credited, via citing the relevant paper.
Guidelines:
52• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new assets released.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing or research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing or research with human subjects.
53Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
54