Provable Partially Observable Reinforcement
Learning with Privileged Information
Yang Cai1Xiangyu Liu2Argyris Oikonomou1Kaiqing Zhang2
1Yale University2University of Maryland, College Park
yang.cai@yale.edu, xyliu999@umd.edu
argyris.oikonomou@yale.edu, kaiqing@umd.edu
Abstract
Partial observability of the underlying states generally presents significant chal-
lenges for reinforcement learning (RL). In practice, certain privileged information ,
e.g., the access to states from simulators, has been exploited in training and has
achieved prominent empirical successes. To better understand the benefits of priv-
ileged information, we revisit and examine several simple and practically used
paradigms in this setting. Specifically, we first formalize the empirical paradigm
ofexpert distillation (also known as teacher-student learning), demonstrating its
pitfall in finding near-optimal policies. We then identify a condition of the partially
observable environment, the deterministic filter condition , under which expert dis-
tillation achieves sample and computational complexities that are both polynomial.
Furthermore, we investigate another successful empirical paradigm of asymmetric
actor-critic , and focus on the more challenging setting of observable partially
observable Markov decision processes. We develop a belief-weighted asymmetric
actor-critic algorithm with polynomial sample and quasi-polynomial computational
complexities, in which one key component is a new provable oracle for learning
belief states that preserves filter stability under a misspecified model, which may
be of independent interest. Finally, we also investigate the provable efficiency of
partially observable multi-agent RL (MARL) with privileged information. We
develop algorithms featuring centralized-training-with-decentralized-execution ,
a popular framework in empirical MARL, with polynomial sample and (quasi-
)polynomial computational complexities in both paradigms above. Compared with
a few recent related theoretical studies, our focus is on understanding practically
inspired algorithmic paradigms, without computationally intractable oracles.
1 Introduction
In most real-world applications of reinforcement learning (RL), e.g., perception-based robot learning
[46,3], autonomous driving [ 73,41], dialogue systems [ 88], and clinical trials [ 77], only partial
observations of the environment state are available for sequential decision-making. Such partial
observability presents significant challenges for efficient decision-making and learning, with known
computational [ 66] and statistical [ 43,36] barriers under the general model of partially observable
Markov decision processes (POMDPs). The curse of partial observability becomes severer when
multiple RL agents interact, where not only the environment state, but also other agents’ information,
are not fully-observable in decision-making [85, 80].
On the other hand, a flurry of empirical paradigms has made partially observable (multi-agent) RL
promising in practice. One notable example is to exploit the privileged information that may be
available (only) during training. The privileged information usually includes direct access to the
underlying states, as well as access to other agents’ observations/actions in multi-agent RL (MARL),
due to the use of simulators and/or high-precision sensors for training. The latter is also known as
38th Conference on Neural Information Processing Systems (NeurIPS 2024).thecentralized-training-with-decentralized-execution (CTDE) framework in deep MARL, and has
become prevalent in practice [ 53,70,22,82]. These approaches can be mainly categorized into
two types: i) privileged policy learning, where an expert/teacher policy is trained with privileged
information, and then distilled into a student partially observable policy. This expert distillation ,
also known as teacher-student learning , approach has been the key to some empirical successes in
robotic locomotion [ 45,59] and autonomous driving [ 14]; ii) privileged value learning, where a value
function is trained conditioned on privileged information, and used to improve a partially observable
policy. It is typically instantiated as the asymmetric actor-critic algorithm [ 68], and serves as the
backbone of some high-profiled successes in robotic manipulation [46, 3] and MARL [53, 82].
Despite the remarkable empirical successes, theoretical understandings of partially observable RL
with privileged information have been rather limited, except for a few recent prominent advances
in RL with hindsight observability [44,30] (see Appendix B for a detailed discussion). However,
most of these theoretically sound algorithms are different from those used in practice, and require
computationally intractable oracles to achieve provable sample efficiency. The soundness and
efficiency of the aforementioned paradigms used in practice remain elusive. In this work, we
examine both paradigms of expert distillation and asymmetric actor-critic, with foresight privileged
information as in these empirical works. In contrast to [ 44,30], which purely focused on sample
efficiency, we aim to understand the benefits of privileged information by examining these practically
inspired paradigms under several POMDP models, without computationally intractable oracles. We
defer a detailed literature review to Appendix B, and summarize our contribution as follows.
Contributions. We first formalize the empirical paradigm of expert distillation , and demonstrate
its pitfall in distilling near-optimal policies even in observable POMDPs, a model class that was
recently shown to allow provable partially observable RL without computationally intractable oracles
[25]. We then identify a new condition for POMDPs, the deterministic filter condition, and establish
sample and computational complexities that are both polynomial for expert distillation. The new
condition is weaker and thus encompasses several known (statistically) tractable POMDP models (see
Figure 1 for a summary). Further, we revisit the asymmetric actor-critic paradigm and analyze its
efficiency under the more challenging setting of observable POMDPs above (where expert distillation
fails). Identifying the inefficiency of vanilla asymmetric actor-critic, and inspired by the empirical
success in belief-state-learning , we develop a new belief-weighted version of asymmetric actor-
critic, with polynomial-sample and quasi-polynomial-time complexities. Key to the results is a new
belief-state learning oracle that preserves filter stability under a misspecified model, which may be
of independent interest. Finally, we also investigate the provable efficiency of partially observable
multi-agent RL with privileged information, by studying algorithms under the CTDE framework,
with polynomial-sample and (quasi-)polynomial-time complexities in both paradigms above.
2 Preliminaries
2.1 Partially Observable RL (with Privileged Information)
Model. Consider a POMDP characterized by a tuple P= (H,S,A,O,T,O, µ1, r), where H
denotes the length of each episode, Sis the state space with |S|=S,Adenotes the action space
with|A|=A. We use T={Th}h∈[H]to denote the collection of transition matrices, so that
Th(·|s, a)∈∆(S)gives the probability of the next state if action ais taken at state sand step h. In
the following discussions, for any given a, we treat Th(a)∈R|S|×|S|as a matrix, where each row
gives the probability for reaching each next state from different current states. We use µ1to denote
the distribution of the initial state s1, andOto denote the observation space with |O|=O. We use
O={Oh}h∈[H]to denote the collection of emission matrices, so that Oh(·|s)∈∆(O)gives the
emission distribution over the observation space Oat state sand step h. For notational convenience,
we will at times adopt the matrix convention, where Ohis a matrix with rows Oh(·|s)for each s∈ S.
Finally, r={rh}h∈[H]is a collection of reward functions, so that rh(s, a)∈[0,1]is the reward
given the state sand action aat step h. When privileged information is available, the agent can
observe the underlying state s∈ S directly during training (only). We thus denote the trajectory until
stephwith states asτh= (s1:h, o1:h, a1:h−1), the one without states asτh= (o1:h, a1:h−1), and its
space as Th. Finally, we use bh(τh)to denote the posterior distribution over the underlying state at
stephgiven history τh, which is known as the belief state (c.f. Appendix C.1 for more details).
Policy and value function. We define a stochastic policy at step has:
πh:Oh× Ah−1→∆(A), (2.1)
2where the agent bases on the entire (partially observable) history for decision-making. The cor-
responding policy class is denoted as Πh. We further denote Π =×h∈[H]Πh. We also define
Πgen:={π1:H|πh:Sh× Oh× Ah−1→∆(A)forh∈[H]}to be the most general policy space
in partially observable RL with privileged state information, which can potentially depend on all
historical states, observations, and actions. It can be seen that Π⊆Πgen. We may also use policies
that only receive a finite memory instead of the whole history as inputs: fix an integer L >0, we
define the policy space ΠLto be the space of all possible policies π=π1:H:= (πh)h∈[H]such that
πh:Zh→∆(A)withZh:=Omin{L,h}× Amin{L,h}for each h∈[H]. Finally, we define the
space of state-based policies as ΠS, i.e., for any π=π1:H∈ΠS,πh:S → ∆(A)for all h∈[H].
Given the POMDP model P, we write PP
s1:H+1,a1:H,o1:H∼π(E)to denote the event Ewhen
(s1:H+1, a1:H, o1:H)is drawn as a trajectory following the policy πin the model P. We will also use
the shorthand notation Pπ,P(·)if(s1:H+1, a1:H, o1:H)is evident. We write EP
π[·]to denote the ex-
pectation similarly. We define the value function at step hasVπ,P
h(yh) :=EP
π[PH
t=hrt(st, at)|yh],
denoting the expected accumulated rewards from step h, where yh⊆(s1:h, o1:h, a1:h−1), and we
slightly abuse the notation by treating as a set the sequence of states s1:h, the sequence of observations
o1:h, and the sequence of actions a1:h−1up to time h, which are the available information to the
agent at step h. We say yhisreachable if there exists some policy π∈Πgensuch that Pπ,P(yh)>0.
Forh= 1, we adopt the simplified notation vP(π) =EP
π[PH
h=1rh(sh, ah)]. Meanwhile, we also
define Qπ,P
h(yh, ah) :=EP
π[PH
t=hrt(st, at)|yh, ah]. We denote the occupancy measure on the
state space as dπ,P
h(sh) =Pπ,P(sh). The goal of learning in POMDPs is to find the optimal policy
thatmaximizes the expected accumulated reward over the policies that take τhas input at each step
h∈[H], i.e., those π∈Π. Formally, we define:
Definition 2.1 (ϵ-optimal policy) .Given ϵ > 0, a policy π⋆∈Πisϵ-optimal, if vP(π⋆)≥
max π∈ΠvP(π)−ϵ.
Learning with privileged information. Common RL algorithms for POMDPs deal with the
scenario where during both the training and test time, the agent can only observe its historical
observations and actions τhat step h, while the states are not accessible. In other words, the agent can
only utilize policies from Πto interact with the environment. In contrast, in settings with privileged
information , e.g., training in simulators and/or using sensors with higher precision, the underlying
state can be used in training. Thus, the agent is allowed to utilize policies from the class Πgenduring
training. Meanwhile, the objective is still to find the optimal history-dependent policy in the space of
Π, since at test time, the agent cannot access the state information anymore, and it is the performance
for such policies that matters eventually. For simplicity, we assume the reward function is known
since under our privileged information setting, learning the reward function is much easier than
learning the transition and emission, and the sample/computational complexity for the former is
dominated by that for the latter. This assumption has also been made for learning in POMDPs without
privileged information [36, 47, 48].
2.2 Partially Observable Multi-agent RL with Information Sharing
Partially observable stochastic games (POSGs) are a natural generalization of POMDPs with multiple
agents of potentially independent interests. We define a POSG with nagents by a tuple G=
(H,S,{Ai}n
i=1,{Oi}n
i=1,T,O, µ1,{ri}n
i=1), where each agent ihas its individual action space Ai,
observation space Oi, and reward function ri={ri,h}h∈[H]withri,h(s, a)∈[0,1]denoting the
reward given state sand joint action afor agent iat step h. An episode of POSG proceeds as follows:
at each step hand state sh, a joint observation is drawn from (oi,h)i∈[n]∼Oh(·|sh), and each agent
receives its own observation oi,h, takes the corresponding action ai,h, obtains the reward ri,h(sh, ah),
where ah:= (ai,h)i∈[n], and then the system transitions to the next state as sh+1∼Th(·|sh, ah).
Notably, each agent imay not only know its local information (oi,1:h, ai,1:h−1), but also information
from some other agents. Therefore, we denote the information available to each agent iat step halso as
τi,h⊆(o1:h, a1:h−1)and define the common information asch=∩i∈[n]τi,handprivate information
aspi,h=τi,h\ch. We denote the space for common information and private information as Chand
Pi,hfor each agent iand step h. The joint private information at step his denoted as ph= (pi,h)i∈[n],
where the collection of the joint private information is given by Ph=P1,h× ··· × P n,h. We
refer more examples of this setting of POSG with information-sharing to Appendix C.2 (and also
[62,63,51]). Correspondingly, the policy each agent ideploys at test time takes the form of
πi,h: Ωh× Ch× Pi,h→∆(Ai), where Ωhis the space of random seeds. We denote the policy
3Without PI With PI (Ours)
Block MDPWith STD:
Oracle-efficient
[17, 18, 60]
Without additional assump.:
Computationally harder than SL [28]Tabular:
Poly sample
+ time
FA:
Poly sample +
Classification
(SL) oraclek-decodable
POMDPExponential-in- k
sample + time [19]
Det. POMDPWithout WSE:
Statistically hard [47]
With WSE:
Poly sample + time [36]
POSG with
det. filterN/A Poly sample + time
Observable
POMDPQuasi-poly
sample + time
[25] [51]Poly sample +
Quasi-poly timeObservable
POSG
Table 1: Comparison of the theoretical guarantees
with and without privileged information. PI: priv-
ileged information; STD: structural assumptions
on transition dynamics, e.g., deterministic transi-
tion or reachability of all states; SL: supervised
learning; FA: function approximation; WSE: well-
separated emission.
Figure 1: A landscape of POMDP models that
partially observable RL with privileged informa-
tion can/cannot address. The axes denote the “re-
strictiveness” of the assumptions, on the emission
channels and transition dynamics, respectively.
space for agent iasΠi. Ifπi,htakes the state shinstead of (ch, pi,h)as input, we denote its policy
space as ΠS,i, e.g., for each agent i, and policy π1:H∈ΠS,i, we have πi,h:S → ∆(Ai)for each
steph∈[H]. Similar to the POMDP setting, we define Πgento be the most general policy space, i.e.,
Πgen:={π1:H|πh:Sh× Oh× Ah−1→∆(A)forh∈[H]}. Note that this model covers several
recent POSG models studied for partially observable MARL, e.g., [ 49,27]. For example, at each
steph, if there is no shared information, then ch=∅, and if all history information is shared, then
pi,h=∅for all i∈[n]. Inprivileged-information -based learning, the training algorithm may exploit
not only the underlying state information, but also the observations and actions of other agents.
Solution concepts. The solution concepts for POSGs are usually the equilibria , particularly Nash
equilibrium (NE) for two-player zero-sum games (i.e., when n= 2 andr1,h+r2,h= 1),1and
correlated equilibrium (CE) or coarse correlated equilibrium (CCE) for general-sum games. We defer
the formal definitions of these standard solution concepts to Appendix C.2.
2.3 Technical Assumptions for Computational Tractability
A key technical assumption is that the POMDPs/POSGs we consider satisfy an observability assump-
tion, as outlined below. This observability assumption allows us to use short memory policies to
approximate the optimal policy, and yields quasi-polynomial-time complexity for both planning and
learning in POMDPs/POSGs [ 26,25,51]. Meanwhile, we defer an additional assumption to ensure
the traceability for solving POSGs to Appendix C.3.
Assumption 2.2 (γ-observability [ 20,26,25]).Letγ >0. For h∈[H], we say that the matrix Oh
satisfies the γ-observability assumption if for each h∈[H], for any b, b′∈∆(S),O⊤
hb−O⊤
hb′
1≥
γ∥b−b′∥1.A POMDP/POSG satisfies γ-observability if all its Ohforh∈[H]do so.
3 Revisiting Empirical Paradigms of RL with Privileged Information
Most empirical paradigms of RL with privileged information can be categorized into two types: i)
privileged policy learning, where the policy in training is conditioned on the privileged information,
and the trained policy is then distilled to a policy that does not take the privileged information as
input. This is usually referred to as either expert distillation [14,64,58] orteacher-student learning
[45,59,75] in the literature; ii) privileged value learning, where the value function is conditioned on
the privileged information, and is then used to directly output a policy that takes partial observation
(history) as input. One prominent example of ii) is asymmetric-actor-critic [68,3]. It is worth
noting that asymmetric-actor-critic is also closely related to one of the most successful paradigms
for multi-agent RL, centralized-training-with-decentralized-execution [53,86,21], which is usually
instantiated under the actor-critic framework, with the critic taking privileged information as input
1Note that we require r1,h+r2,hto be 1 instead of 0to be consistent with our assumption that ri,h∈[0,1]
for each i∈[0,1], and this requirement does not lose optimality as one can always subtract the constant-sum
offset to attain a zero-sum reward structure.
4in training. Here we formalize and revisit the potential pitfalls of these two paradigms, and further
develop theoretical guarantees under certain additional conditions and/or algorithm variants.
3.1 Privileged Policy Learning: Expert Policy Distillation
The motivation behind expert policy distillation is that learning an optimal fully observable policy
in MDPs is a much easier and better-studied problem with many known efficient algorithms. The
(expected) distillation objective can be formalized as follows:
bπ⋆∈arg min
π∈ΠEP
π′"HX
h=1Df(π⋆
h(·|sh)|πh(·|τh))#
, (3.1)
where π′∈Πgenis some given behavior policy to collect exploratory trajectories, π⋆∈
arg max π∈ΠSvP(π)denotes the optimal fully observable policy, and Dfdenotes the general f-
divergence to measure the discrepancy between π⋆andπ.
Such a formulation looks promising since it essentially circumvents the challenging issue of ex-
ploration in partially observable environments , by directly mimicking an expert policy that can be
obtained from any off-the-shelf MDP learning algorithm. However, we point out in the following
proposition that even if the POMDP satisfies Assumption 2.2, the distilled policy can still be strictly
suboptimal even with infinite data, i.e., by solving the expected objective Equation (3.1) completely.
We postpone the proof of Proposition 3.1 to Appendix E.
Proposition 3.1 (Pitfall of expert policy distillation) .For any ϵ, γ∈(0,1), there exists a γ-observable
POMDP PϵwithH= 1,S=O=A= 2such that for any behavior policy π′∈Πgenand choice of
Dfin Equation (3.1), it holds that vPϵ(bπ⋆)≤max π∈ΠvPϵ(π)−(1−ϵ)(1−γ)
4.
The key reason Equation (3.1) fails is that in general, the underlying state can remain highly uncertain
even given the full history. Thus, the distilled policy may not be able to mimic the state-based expert
policy well at different states shif the associated π⋆
h(·|sh)differs significantly across sh. To see how
we may rule out such an issue, notice that if γ= 1(note that according to Assumption 2.2, we have
γis at most 1since γ≤ ∥Oh∥∞≤1), implying that the observation can decode the underlying state,
the bound in Proposition 3.1 becomes vacuous. Inspired by this, we propose the following condition
that incorporates this case of γ= 1, and will be shown to suffice to make expert distillation effective.
Definition 3.2 (Deterministic filter condition) .We say a POMDP Psatisfies the deterministic
filter condition if for each h≥2, the belief update operator under Psatisfies that there exists
anunknown function ψh:S × A × O → S such that for any reachable sh−1∈ S,oh∈ O ,
ah−1∈ A,Uh(bsh−1;ah−1, oh) =bψh(sh−1,ah−1,oh), where we define for any s∈ S,bs∈∆(S)
andbs(s) = 1 is a one-hot vector. In addition, for h= 1, there exists a function ψ1:O → S
such that for any reachable o1,B1(µ1;o1) =bψ1(o1), where Bh(b;oh) :=PP
sh∼b(·|oh)∈∆(S),
Uh(b;ah−1, oh) :=PP
sh−1∼b(·|ah−1, oh−1)∈∆(S)are the belief update operators under the Bayes
rule for any b∈∆(S), for which we defer the formal introduction to Appendix C.1.
Notably, this condition is weaker than and thus covers several known tractable classes of POMDPs
with sample and computation efficiency guarantees including Block MDP, deterministic POMDP,
k-decodable POMDP as well as a new setting we have identified and existing literature cannot handle.
We refer the formal introduction to Appendix E and Figure 1 for an illustration.
In light of the pitfall in Proposition 3.1, we will analyze both the computational and statistical
efficiencies of expert distillation in Section 4, under the condition in Definition 3.2.
3.2 Privileged Value Learning: Asymmetric Actor-Critic
Asymmetric actor-critic [ 68] iterates between two procedures as in standard actor-critic algorithms
[42]: policy improvement and policy evaluation . As the name suggests, its key difference from
the standard actor-critic is that the algorithm maintains Q-value functions (the critic) based on the
state/privileged information , while the policy receives only the (partially observable) history as input.
Policy evaluation. At iteration t−1, given the policy πt−1, the algorithm estimates Q-functions
in the form of {Qt−1
h(τh, sh, ah)}h∈[H], where we adopt the “unbiased” version [ 6] such that Q-
5functions are conditioned on both thehistory and the states .2One key to achieving sample efficiency
is by adding some bonus terms in policy evaluation to encourage exploration, i.e., obtaining some
optimistic Q-function estimates, similarly as in the fully-observable MDP setting, see e.g., [ 11,74],
for which we defer the detailed introduction to Section 4.
Policy improvement. At each iteration t, given the critic {Qt−1
h(τh, sh, ah)}h∈[H]forπt−1, the
vanilla asymmetric actor-critic algorithm updates the policy according to the sample-based gradient
estimation via Ktrajectories {sk
1:H+1, ok
1:H, ak
1:H}k∈[K]sampled from πt−1
πt←PROJΠ
πt−1+λt
KX
k∈[K]X
h∈[H]∇πlogπt−1
h(ak
h|τk
h)Qt−1
h(τk
h, sk
h, ak
h)
, (3.2)
where λtis the step-size and PROJΠis the projection operator onto the space of Π, which corresponds
to projecting onto the simplex of ∆(A)for each h∈[H]. Here we point out the potential drawback of
the vanilla algorithm as in [ 68,6], where the key insight is that for each iteration of policy evaluation
and improvement, one roughly only performs the computation of order O(KH), while needing to
collect Knew episodes of samples. Thus, the sample complexity will scale in the same order as
thecomputational complexity when the algorithm converges after some iterations to an ϵ-optimal
solution, which will be super-polynomial even for γ-observable POMDPs [ 27]. Proof of the result is
deferred to Appendix E.
Proposition 3.3 (Inefficiency of vanilla asymmetric actor-critic) .Under the tabular parameteri-
zation for both the policy and the value function, the vanilla asymmetric actor-critic algorithm
(Equation (3.2)) suffers from super-polynomial sample complexity for γ-observable POMDPs under
standard hardness assumptions.
To address such an issue, one may need to perform more computation per iteration , so that although
thetotal computational complexity (iteration number ×per-iteration computational complexity) is
super-polynomial, the total iteration number can be lower such that the total sample complexity may
be lower as well. This desideratum is hard to achieve if one computes policy update only on the
sampled trajectories τhper iteration, i.e., update asynchronously , since this will couple the scales
of computational and sample complexities similarly as Equation (3.2). In contrast, we first propose
to update alltrajectories per iteration in a synchronous way, with the following proximal-policy
optimization-type [ 72] policy improvement update with the state-history-dependent Q-functions
{Qt−1
h(τh, sh, ah)}h∈[H]:
πt
h(·|τh)∝πt−1
h(·|τh) exp 
ηEsh∼bh(τh)
Qt−1
h(τh, sh,·)
,∀h∈[H], τh∈ Th, (3.3)
where we recall bh(τh)∈∆(S)denotes the belief state and η > 0is the learning rate. This
update rule also reduces to the natural policy gradient (NPG) [ 40] update under the softmax policy
parameterization in the fully-observable case [ 1], when updated for each state shseparately [ 74]. We
defer the detailed derivation of Equation (3.3) to Appendix E.
However, such an update presents two challenges: (1) It requires enumerating all possible τh, whose
number scales exponentially with the horizon, making it still computationally intractable; (2) An
explicit belief function bhis needed. Motivated by these two challenges, we propose to consider finite-
memory -based policy and assume access to an approximate belief function {bapx
h:Zh→∆(S)}h∈[H]
(the learning for which will be made clear later). Correspondingly, the policy update is modified as:
πt
h(·|zh)∝πt−1
h(·|zh) exp
ηEsh∼bapx
h(zh)
Qt−1
h(zh, sh,·)
,∀h∈[H], zh∈ Zh.
Then we develop and analyze one possible approach to learning such an approximate belief efficiently
(c.f. Section 5). It is worth noting that the policy optimization algorithm we aim to develop and
analyze does not depend on the specific algorithm approximate belief learning. Such a decoupling
enables a more modular algorithm design framework, and can potentially incorporate the rich
literature on learning approximate beliefs in practice, see e.g., [ 24,65,16,87,83], which has mostly
not been theoretically analyzed before. We will thus analyze such an oracle in Section 5.
2As pointed out in [ 6], the original asymmetric actor-critic [ 68], where the value function was only conditioned
on the state , is a biased estimate of the actual history-conditioned value function that appears in the policy
gradient in the infinite-horizon discounted setting. We verify that such a state-based value function is indeed also
biased under our finite-horizon setting, see Remark E.1 for an example. We will thus use the unbiased value
function estimate conditioned on both the state and the history throughout.
64 Provably Efficient Expert Policy Distillation
We now focus on the provable correctness and efficiency of expert policy distillation, under the
deterministic filter condition in Definition 3.2. We will defer all the proofs in this section to
Appendix F. Definition 3.2 motivates us to consider only succinct policies that incorporate an
auxiliary parameter representing the most recent state, as well as the most recent observations
and actions. We consider policies that are the composition of two functions: at step ha function
gh:S × A × O → S that decodes the state based on the previous state, the most recent action,
and the most recent observation, and a policy πE∈ΠSthat takes as input the current (decoded)
underlying state and outputs a distribution over actions.
Definition 4.1. We define a policy class ΠDas:
ΠD=
πE
h(gh(sh−1, ah−1, oh)) :gh:S × A × O → S , πE
h:S → ∆(A)	
h∈[H],
where πEstands for an arbitrary expert policy, and ΠDstands for the distilled policy class, and
forh= 1,a0,s0are some fixed dummy action and state. Intuitively, the distilled policy π∈ΠD
executes as follows: it firstly decodes the underlying states by applying {gh}h∈[H]recursively along
the history, and then takes actions using πEbased on the decoded states.
Our goal is to learn the two functions independently, that is, we want to learn an approximately
optimal policy πE∈ΠSwith respect to the MDP Mderived from POMDP Pby omitting the
observations and observing the underlying state (see Definition 4.2 for a formal definition), and for
each step h∈[H], a decoding function gh(sh−1, ah−1, oh)such that the probability of incorrectly
decoding a state-action-observation triplet over the trajectories induced by the policy πEis low.
Definition 4.2 (POMDP-induced MDP) .Given a POMDP P= (H,S,A,O,T,O, µ1, r), we define
its associated Markov Decision Process (MDP) MasM= (H,S,A,T, µ1, r)without observations.
Definition 4.3. Consider a POMDP Pthat satisfies Definition 3.2, and let ψ={ψh}h∈[H]be the
promised set of functions that always correctly decode a state-action-observation triplet into an
underlying state. Consider policyfπE={πE
h(ψ(·)) :S ×A×O → ∆(A)}h∈[H]∈ΠD. We slightly
abuse the notation and simply denote by vP(πE) =vP(fπE).
Lemma 4.4. LetP= (H,S,A,O,T,O, µ1, r)be a POMDP that satisfies Definition 3.2,
and consider a policy πE∈ΠS. Consider a set of decoding functions {gh}h∈[H]
such that, PπE,P[∃h∈[H] :gh(sh−1, ah−1, oh)̸=sh]≤ϵ. Consider the policy π=
πE
h(gh(·)) :S × A × O → ∆(A)	
h∈[H]on the POMDP P, then: vP(π)≥vP(fπE)−Hϵ.
We can use any off-the-shelf algorithm to learn an approximate optimal policy πEfor the associated
MDPM(see Definition 4.2). Thus, in the rest of the section, we focus on learning the decoding
function {gh}h∈[H]. To efficiently learn the decoding function, we model the access to the underlying
state by keeping track of the most recent pair of the action and the observation, as well as the two
most recent states. We summarize the algorithm of decoding-function learning in Algorithm 1.
Theorem 4.5. Consider a POMDP Pthat satisfies Definition 3.2, a policy πE∈ΠS, and let
{gh}h∈[H]be the output of Algorithm 1 with M=AOS +log( H/δ)
ϵ2 . Then, with probability at
least 1−δ, for each step h∈[H]:PπE,P[∃h∈[H] :gh(sh−1, ah−1, oh)̸=sh]≤ϵ,using
POLY (H, A, O, S,1
ϵ,log 1
δ
)episodes in time POLY (H, A, O, S,1
ϵ,log 1
δ
).
The following is an immediate consequence of Lemma 4.4 and Theorem 4.5. Note that both the
sample and computation complexities are polynomial , which is in stark contrast to the k-decodable
POMDP case [ 19] (a special one covered by our Definition 3.2), for which the sample complexity is
necessarily exponential in kwhen there is no privileged information [ 19]. In fact, thanks to privileged
information, the complexities are only polynomial in horizon Heven when the decodable length is
unknown . For the benefits of using privileged information in several other subclasses of problems,
we refer to Table 1 for more details.
Theorem 4.6. LetPsatisfy Definition 3.2 and consider any policy πE∈ΠS. Using
POLY (H, A, O, S,1
ϵ,log 1
δ
)episodes and in time POLY (H, A, O, S,1
ϵ,log 1
δ
), we can compute
a policy π∈ΠD(see Definition 4.1) such that with probability at least 1−δ,vP(π)≥vP(πE)−ϵ.
Extension to the case with general function approximation. Due to the modularity of our
algorithmic framework and its compatibility with supervised learning oracles, it can be readily
generalized to the function approximation setting to handle large observation spaces. We defer the
corresponding results to Appendix G.
75 Provable Asymmetric Actor-Critic with Approximate Belief Learning
Unlike most existing theoretical studies on provably sample-efficient partially observable RL [ 36,
25, 47], which directly learn an approximate POMDP model for planning near-optimal policies, we
consider a general framework with two steps: firstly learning an approximate belief function , followed
by adopting a fully observable RL subroutine on the belief state space.
5.1 Belief-Weighted Optimistic Asymmetric Actor-Critic
We now introduce our main algorithmic contribution to the privileged policy learning setting. Our
algorithm is conceptually similar to the natural policy gradient methods [ 40,1,74] in the fully-
observable setting, with additional weighting over the states shusing some learned belief states, to
handle the additional state -dependence in the asymmetric critic. The overall algorithm is presented
in Algorithm 2. The algorithm requires a belief-learning subroutine that takes the stored memory
as input and outputs a belief about the underlying state (c.f. {bapx
h}h∈[H]). Additionally, similar to
the fully observable setting, we include a subroutine to estimate the Q-function, which introduces
additional challenges due to partial observability (see Appendix H). We establish the performance
guarantee of Algorithm 2 in the following theorem. We defer the proof to Appendix H.
Theorem 5.1 (Near-optimal policy) .Fixϵ, δ∈(0,1). Given a POMDP Pand an approximate belief
{bapx
h:Zh→∆(S)}h∈[H], with probability at least 1−δ, Algorithm 2 can learn an approximate opti-
mal policy π⋆ofPin the space of ΠLsuch that vP(π⋆)≥max π∈ΠLvP(π)−O(ϵ+H2ϵbelief),with
sample complexity POLY (S, A, O, H,1
ϵ,log1
δ)and time complexity POLY (S, A, O, H, Z,1
ϵ,log1
δ),
where ϵbelief is the belief-learning error defined as ϵbelief := max h∈[H]max π∈ΠLEP
π∥bh(τh)−
bapx
h(zh)∥1andZ:= max h∈[H]|Zh|. Furthermore, if Pis additionally γ-observable (c.f. As-
sumption 2.2), then π⋆is also an approximate optimal policy in the space of Πsuch that
vP(π⋆)≥max π∈ΠvP(π)− O(ϵ+H2ϵbelief),as long as L≥eΩ(γ−4log(SH/ϵ )).
5.2 Approximate Belief Learning
At a high level, our belief-learning algorithm first learns an approximate POMDP model bPby
explicitly exploring the state space. The main technical challenge here is that there may exist states that
are reachable with very low probability, making it infeasible to collect enough samples to sufficiently
explore them, thus potentially breaking the γ-observability property of the ground-truth model P. To
circumvent this issue, we ignore such hard-to-visit states and redirect probabilities flowing to them to
certain other states. Thus, in our truncated POMDP, where each state is sufficiently explored, we can
approximate the transition and emission matrices to a desired accuracy uniformly across all the states
and preserve the γ-observability property. This ensures that the learned approximate belief function
in the truncated POMDP is sufficiently close to the actual belief function of the original POMDP P.
Note that the key to achieving belief learning with both polynomial sample and time complexities is
our explicit exploration in the state space, which relies on executing fully observable policies from an
MDP learning subroutine. We remark that the belief function may also be learned even if the state
space is only explored by partially observable policies, thus utilizing only hindsight observability may
be sufficient for this purpose [ 44]. However, for such exploration to be computationally tractable ,
one requires to avoid using computationally intractable oracles for POMDP learning , which is in fact
our final goal. We present the guarantees in the next theorem and postpone the proof to Appendix H.
Theorem 5.2. Consider a γ-observable POMDP P(c.f. Assumption 2.2) and assume that
L≥eΩ(γ−4log(SH/ϵ ))for an ϵ >0. Then, we can learn an approximate belief {bapx
h}h∈[H]from Al-
gorithm 4 using eO(S2AH2O+S3AH2
ϵ2 +S4A2H6O
ϵγ2)episodes in time POLY
S, H, A, O,1
γ,1
ϵ,log 1
δ
such that with probability at least 1−δ, for any π∈ΠLandh∈[H],EP
π∥bh(τh)−bapx
h(zh)∥1≤ϵ.
Theorem 5.2 shows that an approximate belief can be learned with both polynomial samples and
time, which, combined with Theorem 5.1, yields the final polynomial sample and quasi-polynomial
time guarantee below. In contrast to the case without privileged information [ 25,27], the sample
complexity is reduced from quasi-polynomial to polynomial for γ-observable POMDPs. Note that
the computational complexity remains quasi-polynomial, which is known to be unimprovable even
for planning [ 27]. The key to such an improvement, as pointed out in Section 3.2, is the more
practical update rule of actor-critic (in conjunction with our belief-weighted idea), which allows more
computation at each iteration (instead of only performing computation at the sampled finite-memory).
This allows the total computation to remain quasi-polynomial, while the overall sample complexity
becomes polynomial. A detailed comparison can be found in Table 1.
80 20 40 60 80 100
Episodes2.002.252.502.753.003.253.503.75RewardsCase 1
Belief-weighted optimistic AAC (ours)
Optimistic Asymmetric VI (ours)
Asymmetric Q
Vanilla AAC
0 20 40 60 80 100
Episodes4.55.05.56.06.57.0RewardsCase 2
Belief-weighted optimistic AAC (ours)
Optimistic Asymmetric VI (ours)
Asymmetric Q
Vanilla AAC
0 20 40 60 80 100
Episodes2.22.42.62.83.03.2RewardsCase 3
Belief-weighted optimistic AAC (ours)
Optimistic Asymmetric VI (ours)
Asymmetric Q
Vanilla AAC
0 20 40 60 80 100
Episodes4.55.05.56.06.5RewardsCase 4
Belief-weighted optimistic AAC (ours)
Optimistic Asymmetric VI (ours)
Asymmetric Q
Vanilla AACFigure 2: Results for POMDPs of different sizes, where our methods achieve the best performance
with the lowest sample complexity (VI: value iteration; AAC: asymmetric actor-critic).
Theorem 5.3. LetPbe a γ-observable POMDP (c.f. Assumption 2.2), and consider L≥
eΩ(γ−4log(SH/ϵ ))for an ϵ >0. With probability at least 1−δ, Algorithm 2 can learn a pol-
icyπ∈ΠLsuch that vP(π)≥max π′∈ΠvP(π′)−ϵ,using POLY (S, H, 1/ϵ,1/γ,log(1/δ), O, A )
episodes and in time POLY (S, H, 1/ϵ,log(1/δ), OL, AL).
6 Numerical Validation
We now provide some numerical results for both of our principled algorithms. Here we mainly
compare with two baselines, the vanilla asymmetric actor-critic [ 68], and asymmetric Q-learning [ 7],
on two settings, POMDP under the deterministic filter condition (c.f. Definition 3.2) and general
POMDPs. We report the results in Table 2 and Figure 2, where our algorithms converge faster to
higher rewards. We defer the implementation details and discussions to Appendix I.
7 Extension to Partially Observable MARL with Privileged Information
7.1 Privileged Policy Learning: Equilibrium Distillation
To understand how the deterministic filter condition may be extended for POSGs, we first note the
following equivalent characterization of Definition 3.2, the proof of which is deferred to Appendix J.
Proposition 7.1. Definition 3.2 is equivalent to the following: for each h∈[H], there exists an
unknown function ϕh:Th→ S such that PP(sh=ϕh(τh)|τh) = 1 for any reachable τh∈ Th.
Proposition 7.1 implies that at each step h, given the entire history information, the agent can uniquely
decode the current underlying state sh. Thus, we generalize this condition to POSGs by requiring
each agent to uniquely decode the current state shgiven the information it has collected so far.
Definition 7.2 (Deterministic filter condition for POSGs) .We say a POSG Gsatisfies the determinis-
tic filter condition if for each i∈[n],h∈[H], there exists an unknown function ϕi,h:Ch×Pi,h→ S
such that PG(sh=ϕi,h(ch, pi,h)|ch, pi,h) = 1 for any reachable (ch, pi,h).
Here we have required that each agent can decode the underlying state through their own information
individually . Naturally, one may wonder whether one can relax it so that only the joint history
information of all the agents can decode the underlying state. However, we point out in the following
that it does not circumvent the computational hardness of POSG, the proof of which is deferred to
Appendix J. Note that the computational hardness result can not be mitigated even with privileged
state information, as the hardness we state here holds even for the planning problem with model
knowledge, with which one can simulate the RL problems with privileged information.
Proposition 7.3. Computing CCE in POSGs that satisfy that for each step h∈[H], there exists a
function ϕh:Ch× Ph→ S such that PG(sh=ϕh(ch, ph)|ch, ph) = 1 for any reachable (ch, ph)
is still PSPACE-hard .
Learning multi-agent individual decoding functions with unilateral exploration. Similar to our
framework for POMDPs, our framework for POSGs is also decoupled into two steps: i) learning an
expert equilibrium policy that is fully observable, ii) learning the decoding function , where the first
step can be instantiated by any provable off-the-shelf algorithm of learning in Markov games. The
major difference from the framework for POMDPs lies in how to learn the decoding function. In
Theorem J.1, we prove that the difference of the NE/CE/CCE-gap between the expert policy and the
distilled student policy is bounded by the decoding errors under policies from the unilateral deviation
of the expert policy. Hence, given the expert policy π, the key algorithmic principle is to perform
unilateral exploration for each agent ito make sure the decoding function is accurate under policies
9(π′
i, π−i)for any π′
i, keeping π−ifixed. We refer the detailed algorithm to Algorithm 5, and present
below the guarantees for learning the decoding functions and the corresponding distilled policy for
learning NE/CCE, while we defer the results for learning CE to Theorem J.6.
Theorem 7.4 (Equilibria learning; Combining Theorem J.1 and Theorem J.4) .Under Assumption C.8
and conditions of Definition 7.2, given aϵ
2-NE/CCE πEfor the associated Markov game of G,
Algorithm 5 can learn decoding function {bgi,h}i∈[n],h∈[H]such that with probability at least 1−δ, it
is guaranteed that max ui∈Πi,j∈[n]Pui×π−i,G(sh̸=bgj,h(ch, pj,h))≤ϵ
4nH2,for any i∈[n], h∈[H]
with both sample and computational complexities POLY (S, A, H, O,1
ϵ,log1
δ). Consequently, policy
πdistilled from πE(c.f. Theorem J.1 for the formal distillation procedures) is an ϵ-NE/CCE of G.
7.2 Privileged Value Learning: Asymmetric MARL with Approximate Belief Learning
For POMDPs, we have used finite-memory policies for computational efficiency. We generalize to
POSGs with information sharing by defining the compression of the common information.
Definition 7.5 (Compressed approximate common information [ 57,79,51]).For each h∈[H],
given a set bCh, we say Compresshis a compression function if Compressh∈ {f:Ch→bCh}.
For each ch∈ Ch, we denote bch:=Compressh(ch). We also require the compression function
to satisfy the regularity condition that for each h∈[H], there exists a function bΛh+1such that
bch+1=bΛh+1(bch, ϖh+1), for any ch∈ Ch,ϖh+1∈Υh+1, where we recall ch+1:=ch∪ϖh+1and
the definition of Υh+1in Assumption C.7.
Similar to the framework we developed for POMDPs in Section 5, we firstly develop the multi-agent
RL algorithm based on some approximate belief, and then instantiate it with one provable approach
for learning such an approximate belief.
Optimistic value iteration of POSGs with approximate belief. For POMDPs, the sufficient
statistics for optimal decision-making is the posterior distribution over the state given history.
However, for POSGs with information-sharing, as shown in [ 63,62,51], the sufficient statistics
become the posterior distribution over the state and the private information given the common
information, instead of only the state. Therefore, we consider the approximate belief in the form of
bPh:bCh→∆(Ph× S)for each h∈[H], where we define the error compared with the ground-truth
belief to be ϵbelief:= max h∈[H]max π∈ΠEG
πP
sh,ph|PG(sh, ph|ch)−bPh(sh, ph|bch)|, i.e., the ex-
pected total variation distance from the true one. Note that both bPhand thus ϵbelief have implicit
dependencies on Compressh, asbch:=Compressh(ch). We outline our algorithm in Algorithm 7,
which is conceptually similar to the algorithm for POMDPs (Algorithm 2), maintaining the asym-
metric critic (i.e., value function), and performing the actor update (i.e., policy update) using the
belief-weighted value function.
Theorem 7.6 (Equilibria learning; Combining Theorem J.15 and Theorem J.16) .Fix
ϵ, δ∈(0,1). Under Assumption C.8, with probability at least 1−δ, Algorithm 7
can learn an (ϵ+H2ϵbelief)-NE if Gis zero-sum and (ϵ+H2ϵbelief)-CE/CCE if Gis
general-sum with sample complexity O(H4SAO log(SAHO/δ )
ϵ2 )and computational complexity
POLY (S,(AO)O(γ−4log(SH/ϵ )), H,1
ϵ,log1
δ).
Learning approximate belief with model truncation. The belief learning algorithm we design
for POSGs is conceptually similar to that we designed for POMDPs, where the key to achieving both
polynomial sample and computational complexity is still to firstly learn approximate models, i.e.,
transitions and emissions, and then carefully truncate (as in Section 5.2) its transition and emission
to build the approximate belief, where we defer the detailed algorithm to Algorithm 8. Next, we
provide its provable guarantees, which leads to a final polynomial-sample and quasi-polynomial-time
complexity result when combined with Theorem 7.6.
Theorem 7.7. For any ϵ > 0, under Assumption 2.2, it holds that one can learn the approxi-
mate belief {bPh:bCh→∆(S × P h)}h∈[H]such that ϵbelief≤ϵ
H2with both polynomial sample
complexity and computational complexity POLY (S, A, O, H,1
γ,1
ϵ,log1
δ)for all the examples in
Appendix C.3. As a consequence, Algorithm 7 can learn an ϵ-NE if Gis zero-sum and ϵ-CE/CCE if
Gis general-sum with sample complexity O(H4SAO log(SAHO/δ )
ϵ2 )and computational complexity
POLY (S,(AO)O(γ−4log(SH/ϵ )), H,1
ϵ,log1
δ).
10Acknowledgement
The authors would like to thank the anonymous reviewers and area chair from NeurIPS 2024 for the
valuable feedback. Y .C. acknowledges the support from the NSF Awards CCF-1942583 (CAREER)
and CCF-2342642. X.L. and K.Z. acknowledge the support from Army Research Laboratory (ARL)
Grant W911NF-24-1-0085. K.Z. also acknowledges the support from Simons-Berkeley Research
Fellowship. A.O. acknowledges financial support from a Meta PhD fellowship, a Sloan Foundation
Research Fellowship and the NSF Award CCF-1942583 (CAREER). Part of this work was done
while the authors were visiting the Simons Institute for the Theory of Computing.
References
[1]A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. Optimality and approximation with
policy gradient methods in Markov decision processes. In Conference on Learning Theory ,
pages 64–66, 2020.
[2]E. Altman, V . Kambley, and A. Silva. Stochastic games with one step delay sharing information
pattern with application to power control. In 2009 International Conference on Game Theory
for Networks , pages 124–129. IEEE, 2009.
[3]O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron,
M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. The Interna-
tional Journal of Robotics Research , 39(1):3–20, 2020.
[4]R. J. Aumann, M. Maschler, and R. E. Stearns. Repeated games with incomplete information .
MIT press, 1995.
[5]R. Avalos, F. Delgrange, A. Nowe, G. Perez, and D. M. Roijers. The wasserstein believer:
Learning belief updates for partially observable environments through reliable latent space
models. In The Twelfth International Conference on Learning Representations , 2023.
[6]A. Baisero and C. Amato. Unbiased asymmetric reinforcement learning under partial observabil-
ity. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent
Systems , 2022.
[7]A. Baisero, B. Daley, and C. Amato. Asymmetric DQN for partially observable reinforcement
learning. In Uncertainty in Artificial Intelligence , pages 107–117. PMLR, 2022.
[8]N. Brukhim, D. Carmon, I. Dinur, S. Moran, and A. Yehudayoff. A characterization of multiclass
learnability, 2022.
[9]N. Brukhim, D. Carmon, I. Dinur, S. Moran, and A. Yehudayoff. A characterization of
multiclass learnability. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer
Science (FOCS) , pages 943–955. IEEE, 2022.
[10] S. Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn. ,
8(3-4):231–357, 2015.
[11] Q. Cai, Z. Yang, C. Jin, and Z. Wang. Provably efficient exploration in policy optimization. In
International Conference on Machine Learning , pages 1283–1294. PMLR, 2020.
[12] Q. Cai, Z. Yang, and Z. Wang. Reinforcement learning from partial observation: Linear function
approximation with provable sample efficiency. In International Conference on Machine
Learning , pages 2485–2522. PMLR, 2022.
[13] C. L. Canonne. A short note on learning discrete distributions. arXiv preprint arXiv:2002.11457 ,
2020.
[14] D. Chen, B. Zhou, V . Koltun, and P. Krähenbühl. Learning by cheating. In Conference on Robot
Learning , pages 66–75. PMLR, 2020.
[15] F. Chen, Y . Bai, and S. Mei. Partially observable RL with b-stability: Unified structural condition
and sharp sample-efficient algorithms. In The Eleventh International Conference on Learning
Representations , 2023.
11[16] X. Chen, Y . M. Mu, P. Luo, S. Li, and J. Chen. Flow-based recurrent belief state learning for
pomdps. In International Conference on Machine Learning , pages 3444–3468. PMLR, 2022.
[17] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracle-
efficient pac rl with rich observations. Advances in neural information processing systems , 31,
2018.
[18] S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably efficient
rl with rich observations via latent state decoding. In International Conference on Machine
Learning , pages 1665–1674. PMLR, 2019.
[19] Y . Efroni, C. Jin, A. Krishnamurthy, and S. Miryoosefi. Provable reinforcement learning
with a short-term memory. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and
S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July
2022, Baltimore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research ,
pages 5832–5850. PMLR, 2022.
[20] E. Even-Dar, S. M. Kakade, and Y . Mansour. The value of observation for monitoring dynamic
systems. In M. M. Veloso, editor, IJCAI 2007, Proceedings of the 20th International Joint
Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007 , pages 2474–2479,
2007.
[21] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson. Learning to communicate with deep
multi-agent reinforcement learning. Advances in Neural Information Processing Systems , 29,
2016.
[22] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent
policy gradients. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32,
2018.
[23] K. Fujii. Bayes correlated equilibria and no-regret dynamics. arXiv preprint arXiv:2304.05005 ,
2023.
[24] T. Gangwani, J. Lehman, Q. Liu, and J. Peng. Learning belief representations for imitation
learning in pomdps. In uncertainty in artificial intelligence , pages 1061–1071. PMLR, 2020.
[25] N. Golowich, A. Moitra, and D. Rohatgi. Learning in observable POMDPs, without computa-
tionally intractable oracles. In Advances in Neural Information Processing Systems , 2022.
[26] N. Golowich, A. Moitra, and D. Rohatgi. Planning in observable pomdps in quasipolynomial
time. arXiv preprint arXiv:2201.04735 , 2022.
[27] N. Golowich, A. Moitra, and D. Rohatgi. Planning and learning in partially observable systems
via filter stability. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing ,
pages 349–362, 2023.
[28] N. Golowich, A. Moitra, and D. Rohatgi. Exploration is harder than prediction: Cryp-
tographically separating reinforcement learning from supervised learning. arXiv preprint
arXiv:2404.03774 , 2024.
[29] G. J. Gordon, A. Greenwald, and C. Marks. No-regret learning in convex games. In Proceedings
of the 25th international conference on Machine learning , pages 360–367, 2008.
[30] J. Guo, M. Chen, H. Wang, C. Xiong, M. Wang, and Y . Bai. Sample-efficient learning of
pomdps with multiple observations in hindsight. In The Twelfth International Conference on
Learning Representations , 2023.
[31] A. Gupta, A. Nayyar, C. Langbort, and T. Basar. Common information based markov perfect
equilibria for linear-gaussian games with asymmetric information. SIAM Journal on Control
and Optimization , 52(5):3228–3260, 2014.
[32] J. Hartline, V . Syrgkanis, and E. Tardos. No-regret learning in bayesian games. Advances in
Neural Information Processing Systems , 28, 2015.
12[33] H. Hu, A. Lerer, N. Brown, and J. Foerster. Learned belief search: Efficiently improving policies
in partially observable settings. arXiv preprint arXiv:2106.09086 , 2021.
[34] N. Jiang. On value functions and the agent-environment boundary. arXiv preprint
arXiv:1905.13341 , 2019.
[35] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision
processes with low bellman rank are pac-learnable. In D. Precup and Y . W. Teh, editors,
Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,
NSW, Australia, 6-11 August 2017 , volume 70 of Proceedings of Machine Learning Research ,
pages 1704–1713. PMLR, 2017.
[36] C. Jin, S. Kakade, A. Krishnamurthy, and Q. Liu. Sample-efficient reinforcement learning
of undercomplete POMDPs. Advances in Neural Information Processing Systems , 33:18530–
18539, 2020.
[37] C. Jin, A. Krishnamurthy, M. Simchowitz, and T. Yu. Reward-free exploration for reinforcement
learning. In International Conference on Machine Learning , pages 4870–4879. PMLR, 2020.
[38] C. Jin, Q. Liu, Y . Wang, and T. Yu. V-learning–a simple, efficient, decentralized algorithm for
multiagent rl. arXiv preprint arXiv:2110.14555 , 2021.
[39] S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In
International Conference on Machine Learning , volume 2, pages 267–274, 2002.
[40] S. M. Kakade. A natural policy gradient. In Advances in Neural Information Processing
Systems , pages 1531–1538, 2002.
[41] B. R. Kiran, I. Sobh, V . Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, and P. Pérez. Deep
reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent
Transportation Systems , 23(6):4909–4926, 2021.
[42] V . R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems , pages 1008–1014, 2000.
[43] A. Krishnamurthy, A. Agarwal, and J. Langford. Pac reinforcement learning with rich observa-
tions. Advances in Neural Information Processing Systems , 29, 2016.
[44] J. Lee, A. Agarwal, C. Dann, and T. Zhang. Learning in pomdps is sample-efficient with
hindsight observability. In International Conference on Machine Learning , pages 18733–18773.
PMLR, 2023.
[45] J. Lee, J. Hwangbo, L. Wellhausen, V . Koltun, and M. Hutter. Learning quadrupedal locomotion
over challenging terrain. Science robotics , 5(47):eabc5986, 2020.
[46] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies.
The Journal of Machine Learning Research , 17(1):1334–1373, 2016.
[47] Q. Liu, A. Chung, C. Szepesvari, and C. Jin. When is partially observable reinforcement
learning not scary? In Conference on Learning Theory , pages 5175–5220, 2022.
[48] Q. Liu, P. Netrapalli, C. Szepesvari, and C. Jin. Optimistic MLE: A generic model-based
algorithm for partially observable sequential decision making. In Proceedings of the 55th
Annual ACM Symposium on Theory of Computing , pages 363–376, 2023.
[49] Q. Liu, C. Szepesvári, and C. Jin. Sample-efficient reinforcement learning of partially observable
Markov games. In Advances in Neural Information Processing Systems , 2022.
[50] Q. Liu, T. Yu, Y . Bai, and C. Jin. A sharp analysis of model-based reinforcement learning with
self-play. In International Conference on Machine Learning , pages 7001–7010. PMLR, 2021.
[51] X. Liu and K. Zhang. Partially observable multi-agent RL with (quasi-)efficiency: the blessing
of information sharing. In International Conference on Machine Learning , pages 22370–22419.
PMLR, 2023.
13[52] X. Liu and K. Zhang. Partially observable multi-agent reinforcement learning with information
sharing, 2024.
[53] R. Lowe, Y . I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic
for mixed cooperative-competitive environments. Advances in Neural Information Processing
Systems , 30, 2017.
[54] M. Lu, Y . Min, Z. Wang, and Z. Yang. Pessimism in the face of confounders: Provably
efficient offline reinforcement learning in partially observable markov decision processes. In
The Eleventh International Conference on Learning Representations , 2023.
[55] X. Lyu, A. Baisero, Y . Xiao, and C. Amato. A deeper understanding of state-based critics
in multi-agent reinforcement learning. In Proceedings of the AAAI conference on artificial
intelligence , volume 36, pages 9396–9404, 2022.
[56] X. Lyu, A. Baisero, Y . Xiao, B. Daley, and C. Amato. On centralized critics in multi-agent
reinforcement learning. Journal of Artificial Intelligence Research , 77:295–354, 2023.
[57] W. Mao, K. Zhang, E. Miehling, and T. Ba¸ sar. Information state embedding in partially
observable cooperative multi-agent reinforcement learning. In 2020 59th IEEE Conference on
Decision and Control (CDC) , pages 6124–6131. IEEE, 2020.
[58] G. B. Margolis, T. Chen, K. Paigwar, X. Fu, D. Kim, S. bae Kim, and P. Agrawal. Learning to
jump from pixels. In 5th Annual Conference on Robot Learning , 2021.
[59] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V . Koltun, and M. Hutter. Learning robust
perceptive locomotion for quadrupedal robots in the wild. Science Robotics , 7(62):eabk2822,
2022.
[60] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and
provably efficient rich-observation reinforcement learning. In International conference on
machine learning , pages 6961–6971. PMLR, 2020.
[61] P. Moreno, J. Humplik, G. Papamakarios, B. A. Pires, L. Buesing, N. Heess, and T. Weber. Neu-
ral belief states for partially observed domains. In NeurIPS 2018 Workshop on Reinforcement
Learning under Partial Observability , 2018.
[62] A. Nayyar, A. Gupta, C. Langbort, and T. Ba¸ sar. Common information based markov perfect
equilibria for stochastic games with asymmetric information: Finite games. IEEE Transactions
on Automatic Control , 59(3):555–570, 2013.
[63] A. Nayyar, A. Mahajan, and D. Teneketzis. Decentralized stochastic control with partial
history sharing: A common information approach. IEEE Transactions on Automatic Control ,
58(7):1644–1658, 2013.
[64] H. Nguyen, A. Baisero, D. Wang, C. Amato, and R. Platt. Leveraging fully observable policies
for learning under partial observability. In Conference on Robot Learning , 2022.
[65] H. Nguyen, B. Daley, X. Song, C. Amato, and R. Platt. Belief-grounded networks for accelerated
robot learning under partial observability. In Conference on Robot Learning , pages 1640–1653.
PMLR, 2021.
[66] C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of markov decision processes. Mathe-
matics of operations research , 12(3):441–450, 1987.
[67] A. Pathak, H. Pucha, Y . Zhang, Y . C. Hu, and Z. M. Mao. A measurement study of internet
delay asymmetry. In Passive and Active Network Measurement: 9th International Conference,
PAM 2008, Cleveland, OH, USA, April 29-30, 2008. Proceedings 9 , pages 182–191. Springer,
2008.
[68] L. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel. Asymmetric actor critic
for image-based robot learning. Robotics: Science and Systems XIV , 2018.
14[69] S. Qiu, Z. Dai, H. Zhong, Z. Wang, Z. Yang, and T. Zhang. Posterior sampling for competitive
rl: Function approximation and partial observation. Advances in Neural Information Processing
Systems , 36, 2024.
[70] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and S. Whiteson. QMIX:
Monotonic value function factorisation for deep multi-agent reinforcement learning. In Interna-
tional Conference on Machine learning , pages 681–689, 2018.
[71] T. Roughgarden. Algorithmic game theory. Communications of the ACM , 53(7):78–86, 2010.
[72] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017.
[73] S. Shalev-Shwartz, S. Shammah, and A. Shashua. Safe, multi-agent, reinforcement learning for
autonomous driving. arXiv preprint arXiv:1610.03295 , 2016.
[74] L. Shani, Y . Efroni, A. Rosenberg, and S. Mannor. Optimistic policy optimization with bandit
feedback. In International Conference on Machine Learning , pages 8604–8613. PMLR, 2020.
[75] I. Shenfeld, Z.-W. Hong, A. Tamar, and P. Agrawal. Tgrl: An algorithm for teacher guided
reinforcement learning. In International Conference on Machine Learning , pages 31077–31093.
PMLR, 2023.
[76] M. Shi, Y . Liang, and N. Shroff. Theoretical hardness and tractability of pomdps in rl with
partial online state information, 2024.
[77] S. M. Shortreed, E. Laber, D. J. Lizotte, T. S. Stroup, J. Pineau, and S. A. Murphy. Informing se-
quential clinical decision-making through reinforcement learning: an empirical study. Machine
learning , 84:109–136, 2011.
[78] Z. Song, S. Mei, and Y . Bai. When can we learn general-sum Markov games with a large
number of players sample-efficiently? arXiv preprint arXiv:2110.04184 , 2021.
[79] J. Subramanian, A. Sinha, R. Seraj, and A. Mahajan. Approximate information state for
approximate planning and reinforcement learning in partially observed systems. J. Mach. Learn.
Res., 23:12–1, 2022.
[80] J. Tsitsiklis and M. Athans. On the complexity of decentralized decision making and detection
problems. IEEE Transactions on Automatic Control , 30(5):440–446, 1985.
[81] M. Uehara, A. Sekhari, J. D. Lee, N. Kallus, and W. Sun. Computationally efficient pac rl in
pomdps with latent determinism and conditional embeddings. In International Conference on
Machine Learning , pages 34615–34641. PMLR, 2023.
[82] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi,
R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in StarCraft II using multi-agent
reinforcement learning. Nature , 575(7782):350–354, 2019.
[83] A. Wang, A. C. Li, T. Q. Klassen, R. T. Icarte, and S. A. McIlraith. Learning belief representa-
tions for partially observable deep rl. In International Conference on Machine Learning , pages
35970–35988. PMLR, 2023.
[84] L. Wang, Q. Cai, Z. Yang, and Z. Wang. Represent to control partially observed systems: Rep-
resentation learning with provable sample efficiency. In The Eleventh International Conference
on Learning Representations , 2022.
[85] H. S. Witsenhausen. A counterexample in stochastic optimum control. SIAM Journal on
Control , 6(1):131–147, 1968.
[86] G. Yang, M. Liu, W. Hong, W. Zhang, F. Fang, G. Zeng, and Y . Lin. Perfectdou: Dominating
doudizhu with perfect information distillation. Advances in Neural Information Processing
Systems , 35:34954–34965, 2022.
15[87] Y . Yang, Y . Jiang, J. Chen, S. E. Li, Z. Gu, Y . Yin, Q. Zhang, and K. Yu. Belief state actor-critic
algorithm from separation principle for POMDP. In 2023 American Control Conference (ACC) ,
pages 2560–2567. IEEE, 2023.
[88] S. Young, M. Gaši ´c, B. Thomson, and J. D. Williams. Pomdp-based statistical spoken dialog
systems: A review. Proceedings of the IEEE , 101(5):1160–1179, 2013.
[89] A. Zanette and E. Brunskill. Tighter problem-dependent regret bounds in reinforcement learning
without domain knowledge using value function bounds. In International Conference on
Machine Learning , pages 7304–7312. PMLR, 2019.
[90] W. Zhan, M. Uehara, W. Sun, and J. D. Lee. PAC reinforcement learning for predictive state
representations. In The Eleventh International Conference on Learning Representations , 2023.
16Supplementary Materials for
“Provable Partially Observable Reinforcement
Learning with Privileged Information”
Contents
1 Introduction 1
2 Preliminaries 2
2.1 Partially Observable RL (with Privileged Information) . . . . . . . . . . . . . . . 2
2.2 Partially Observable Multi-agent RL with Information Sharing . . . . . . . . . . . 3
2.3 Technical Assumptions for Computational Tractability . . . . . . . . . . . . . . . 4
3 Revisiting Empirical Paradigms of RL with Privileged Information 4
3.1 Privileged Policy Learning: Expert Policy Distillation . . . . . . . . . . . . . . . . 5
3.2 Privileged Value Learning: Asymmetric Actor-Critic . . . . . . . . . . . . . . . . 5
4 Provably Efficient Expert Policy Distillation 7
5 Provable Asymmetric Actor-Critic with Approximate Belief Learning 8
5.1 Belief-Weighted Optimistic Asymmetric Actor-Critic . . . . . . . . . . . . . . . . 8
5.2 Approximate Belief Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
6 Numerical Validation 9
7 Extension to Partially Observable MARL with Privileged Information 9
7.1 Privileged Policy Learning: Equilibrium Distillation . . . . . . . . . . . . . . . . . 9
7.2 Privileged Value Learning: Asymmetric MARL with Approximate Belief Learning 10
A Societal Impact 19
B Related Work 19
C Additional Preliminaries 20
C.1 Additional Preliminaries on POMDPs . . . . . . . . . . . . . . . . . . . . . . . . 20
C.2 Additional Preliminaries for POSGs . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.2.1 Evolution of the Common and Private Information . . . . . . . . . . . . . 23
C.3 Strategy Independence of Belief and Examples . . . . . . . . . . . . . . . . . . . 23
D Collection of Algorithms 24
E Missing Details in Section 3 31
F Missing Details in Section 4 33
17G Provably Efficient Expert Policy Distillation with Function Approximation 34
H Missing Details in Section 5 36
H.1 Supporting Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
I Missing Details in Section 6 47
J Missing Details in Section 7 48
J.1 Background on Bayesian Games . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
K Concluding Remarks and Limitations 62
18A Societal Impact
Our work is theoretical by nature, and aimed at better understanding reinforcement learning under
partial observability with privileged information. As such, we do not anticipate any direct positive or
negative societal impact from this research.
B Related Work
Provable partial observable RL. While POMDPs are generally known to be both statistically
hard [ 43] and computationally intractable [ 66], a productive line of research has identified several
structured subclasses of POMDPs that can be efficiently solved. [ 43] introduced the class of POMDPs
in the rich-observation setting, where the observation space can be large and fully reveal the underlying
state, where sample-efficient RL becomes possible [ 35,60]. [19] introduced k-step decodable
POMDPs, where the last kobservation-action pairs can uniquely determine the state, and proposed
polynomial-sample complexity algorithms (assuming kis a small constant). Beyond settings where
the underlying state can be exactly recovered, [ 36,47] proposed weakly revealing POMDPs, where
the observations are assumed to be informative enough. Under the weakly revealing condition
(and its variant), there has been a fast-growing line of recent works developing sample-efficient
RL algorithms for various settings, see e.g., [ 84,15,12,54,48,90]. Notably, these algorithms are
typically computationally inefficient, requiring access to an optimistic planning oracle for POMDPs.
On a promising note, [ 26] showed that in observable POMDPs (see Assumption 2.2), one can
have quasi-polynomial-time algorithms for planning the near-optimal policy, which further leads to
provable RL algorithms [25, 27] with both quasi-polynomial samples and time.
RL under hindsight observability. The closest line of research to ours are the recent theoretical
studies for Hindsight Observable Markov Decision Processes (HOMDPs) [ 44], where the underlying
state is revealed at the end of the episode; see also subsequent related works in [ 30,76] with different
observation feedback models. These works focused purely on sample efficiency , and showed that
polynomial sample complexity can be achieved without (or by further relaxing) aforementioned
structural assumptions of the model (e.g., observability or decodability), in both tabular and/or
function approximation settings. However, the algorithms (also) require an oracle for planning or
even optimistic planning in a learned approximate POMDP, which are not computationally tractable
in general. Indeed, without any structural assumption, learning the optimal policy in HOMDPs
is computationally no easier than the planning problem, which thus remains PSPACE-hard . [66].
Meanwhile, even under the additional assumption of observability on the underlying POMDP model,
it is still not clear if these algorithms can avoid computationally intractable oracles, since the
approximate POMDP that [ 44] needs to do planning on at every iteration during learning can be quite
different from the underlying model. For example, at the beginning of exploration when not enough
samples are collected, or when there exist certain states that remain less explored during the entire
learning process, the potentially misspecified emission (and transition) may break the observability
(or other structural) assumptions made for the underlying POMDP. This makes that single iteration
even computationally intractable. In contrast, our focus is on better understanding practically inspired
algorithmic paradigms, without computationally intractable oracles, which in practice often do have
and use the privileged state information during each episode (instead of only at the end) [ 68,45,14].
Most related empirical works. Privileged information has been widely used in empirical partially
observable RL, with two main types of approaches based on privileged policy and privileged value
learning, respectively. For the former, one prominent example is expert distillation [ 14,64,58], also
known as teacher-student learning [ 45,59,75], as we analyze in Section 4. For the latter, asymmetric
actor-critic [ 68] represents one of the well-known examples, with other studies in [ 7,3]. Learning
privileged value functions (to improve the policies) has also been widely used in multi-agent RL,
featured in centralized-training-decentralized-execution, see e.g., [ 53,22,70,86,82]. Intriguingly, it
was shown that if the privileged value function depends only on the state, the associated actor will
cause bias [ 6,55,56]. This has thus necessitated the use of history/belief in asymmetric actor-critic, as
in our Section 5. Notably, the empirical framework in [ 83] exactly matches ours, where they exploited
the privileged state information in training for belief learning , followed by policy optimization over
the learned belief states. Indeed, many empirical works explicitly separate the procedures of explicit
19belief-state learning and planning [ 24,65,33,16,87] as we study in Section 5, oftentimes with
privileged state information to supervise the belief learning procedure [61, 5].
C Additional Preliminaries
C.1 Additional Preliminaries on POMDPs
Belief and approximate belief. Although in a POMDP, the agent cannot see the underlying state
directly, it can still form a belief over the underlying state by the historical observations and actions.
Definition C.1 (Belief state update) .For each h∈[H+ 1], the Bayes operator Bh: ∆(S)× O →
∆(S)is defined for b∈∆(S), and o∈ O by:
Bh(b;o)(x) =Oh(o|x)b(x)P
z∈SOh(o|z)b(z),
for each x∈ S. For each h∈[H+ 1], the belief update operator Uh: ∆(S)× A × O → ∆(S), is
defined by
Uh(b;a, o) =Bh+1(Th(a)·b;o),
where Th(a)·brepresents the matrix multiplication. We use the notation bhto denote the belief
update function, which receives a sequence of actions and observations and outputs a distribution
over states at the step h: the belief state at step h= 1is defined as b1(∅) =µ1. For any 2≤h≤H
and any action-observation sequence (a1:h, o1:h), we inductively define the belief state:
bh+1(a1:h, o1:h) =Th(ah)·bh(a1:h−1, o1:h),
bh(a1:h−1, o1:h) =Bh(bh(a1:h−1, o1:h−1);oh).
We also define the approximate belief update using the most recent L-step history. For 1≤h≤H,
we follow the notation of [26] and define
bapx,P
h(∅;D) =µ1ifh= 1
D otherwise ,
where D∈∆(S)is the prior for the approximate belief update. Then for any 1≤h−L < h ≤H
and any action-observation sequence (ah−L:h, oh−L+1:h), we inductively define
bapx,P
h+1(ah−L:h, oh−L+1:h;D) =Th(ah)·bapx,P
h(ah−L:h−1, oh−L+1:h;D),
bapx,P
h(ah−L:h−1, oh−L+1:h;D) =Bh(bapx,P
h(ah−L:h−1, oh−L+1:h−1;D);oh).
For the remainder of our paper, we will use an important initialization for the approximate belief,
which are defined as b′
h(·) := bapx,P
h(·; Unif( S)).
C.2 Additional Preliminaries for POSGs
Model. We use a general framework of partially observable stochastic games (POSGs) as the
model for partially observable MARL. Formally, we define a POSG with nagents by a tuple
G= (H,S,{Ai}n
i=1,{Oi}n
i=1,T,O, µ1,{ri}n
i=1), where Hdenotes the length of each episode,
Sis the state space with |S|=S,Aidenotes the action space for agent iwith|Ai|=Ai. We
denote by a:= (a1,···, an)the joint action of all the nagents, and by A=A1× ··· × A nthe
joint action space with |A|=A=Qn
i=1Ai. We use T={Th}h∈[H]to denote the collection of
transition matrices, so that Th(·|s, a)∈∆(S)gives the probability of the next state if joint action
a∈ A is taken at state s∈ S and step h. In the following discussions, for any given a, we treat
Th(a)∈R|S|×|S|as a matrix, where each row gives the probability for the next state from different
current states. We use µ1to denote the distribution of the initial state s1, andOito denote the
observation space for agent iwith|Oi|=Oi. We denote by o:= (o1, . . . , o n)the joint observation
of all nagents, and by O:=O1× ··· × O nwith|O|=O=Qn
i=1Oi. We use O={Oh}h∈[H]
to denote the collection of the joint emission matrices, so that Oh(·|s)∈∆(O)gives the emission
distribution over the joint observation space Oat state sand step h. For notational convenience, we
will at times adopt the matrix convention, where Ohis a matrix with rows Oh(·|sh). We also denote
20Oi,h(·|s)∈∆(Oi)as the marginalized emission for agent iagent. Finally, ri={ri,h}h∈[H]is a
collection of reward functions, so that ri,h(sh, ah)is the reward of agent iagent given the state sh
and joint action ahat step h.
Similar to a POMDP, in a POSG, the states are not observable to the agents, and each agent can
only access its own individual observations. The game proceeds as follows. At the beginning of
each episode, the environment samples s1from µ1. At each step h, each agent iobserves its own
observation oi,h, where oh:= (o1,h, . . . , o n,h)is sampled jointly from Oh(·|sh). Then each agent i
takes the action ai,hand receives the reward ri,h(sh, ah). After that the environment transitions to
the next state sh+1∼Th(·|sh, ah). The current episode terminates once sH+1is reached.
Information sharing, common and private information. Each agent iin the POSG maintains
its own information, τi,h, a collection of historical observations and actions at step h, namely,
τi,h⊆ {o1, a1, o2,···, ah−1, oh}, and the collection of such history at step his denoted by Ti,h.
In many practical examples, agents may share part of the history with each other, which may
introduce some information structures of the game that may lead to both sample and computation
efficiencies. The information sharing splits the history into common/shared andprivate information
for each agent. The common information at step his a subset of the joint history τh= (τi,h)i∈[n]:
ch⊆ {o1, a1, o2,···, ah−1, oh}, which is available to all the agents in the system, and the collection
of the common information is denoted as Chand we define Ch=|Ch|. Given the common information
ch, each agent also has her private information pi,h=τi,h\ch, where the collection of the private
information for agent iis denoted as Pi,hand its cardinality as Pi,h. The cardinality of the joint
private information is Ph=Qn
i=1Pi,h. We allow chorpi,hto take the special value ∅when there is
nocommon or private information. In particular, when Ch={∅}, the problem reduces to the general
POSG without any favorable information structure; when Pi,h={∅}, every agent holds the same
history, and it reduces to a POMDP when the agents share a common reward function, where the goal
is usually to find the team-optimal solution.
Policies and value functions. We define a stochastic policy for agent iat step has:
πi,h: Ωh× Pi,h× Ch→∆(Ai), (C.1)
where Ωhis the random seed space, which is shared among agents and ωi,h∈Ωhis the random seed
for agent i. The corresponding policy class is denoted as Πi,h. Hereafter, unless otherwise noted,
when referring to policies , we mean the policies given in the form of (C.1) , which maps the available
information of agent i, i.e., the private information together with the common information, to the
distribution over her actions.
We define πias a sequence of policies for agent iat all steps h∈[H], i.e., πi= (πi,1,···, πi,H).
We further denote Πi=×h∈[H]Πi,has the policy space for agent iandΠ =Q
i∈[n]Πias the joint
policy space. As a special case, we define the space of deterministic policy as eΠi, where eπi∈eΠi
maps the private information and common information to a deterministic action for agent iand the
joint space as eΠ =Q
i∈[n]eΠi.
Aproduct policy is denoted as π=π1×π2··· × πn∈Πif the distributions of drawing each seed
ωi,hfor different agents are independent, and a (potentially correlated) joint policy is denoted as
π=π1⊙π2··· ⊙ πn∈Π.
We are now ready to define the value function conditioned on the common information under our
model of POSG with information sharing:
Definition C.2 (Value function with information sharing) .For each agent i∈[n]and step h∈[H],
given common information chand joint policy π= (πi)n
i=1∈Π, the value function conditioned
on the common information of agent iis defined as: Vπ,G
i,h(ch):=EG
πhPH
h′=hri,h′(sh′, ah′)chi
,
where the expectation is taken over the randomness from the model G, policy π, and the random seeds.
For any cH+1∈ CH+1:Vπ,G
i,H+1(cH+1):= 0. From now on, we will refer to it as value function for
short.
Another key concept in our analysis is the belief about the state andthe private information condi-
tioned on the common information among agents. Formally, at step h, given policies from 1toh−1,
21we consider the common-information-based conditional belief Pπ1:h−1,G
h(sh, ph|ch). This belief
not only infers the current underlying state sh, but also all agents’ private information ph. With the
common-information-based conditional belief, the value function given in Definition C.2 has the
following recursive structure:
Vπ,G
i,h(ch) =EG
π[ri,h(sh, ah) +Vπ,G
i,h+1(ch+1)|ch], (C.2)
where the expectation is taken over the randomness of (sh, ph, ah, oh+1). With this relationship,
we can define the prescription-value function correspondingly, a generalization of the action-value
function in (fully observable) stochastic games and MDPs to POSGs with information sharing, as
follows.
Definition C.3 (Prescription-value function with information sharing) .At step h, given the common
information ch, joint policies π= (πi)n
i=1∈Π, and prescriptions (γi,h)n
i=1∈Γh, the prescription-
value function conditioned on the common information and joint prescription of agent iis defined
as:
Qπ,G
i,h(ch,(γj,h)j∈[n]) :=EG
π
ri,h(sh, ah) +Vπ,G
i,h+1(ch+1)ch,(γj,h)j∈[n]
,
where prescription γi,h∈∆(Ai)Pi,hreplaces the partial function πi,h(·|ωi,h, ch,·)in the value
function. From now on, we will refer to it as prescription-value function for short. With such a
prescription-value function, agents can take actions purely based on their local private information
[63, 62, 51].
This prescription-value function indicates the expected return for agent iwhen all the agents firstly
adopt the prescriptions (γj,h)j∈[n]and then follow the policy π.
Equilibrium notions. With the definition of value functions, we can accordingly define the so-
lution concepts. Here we define the notions of ϵ-NE, ϵ-CCE, ϵ-CE, and ϵ-team optimum under the
information-sharing framework as follows. For a joint policy (πi)n
i=1∈Πwe denote the expected
reward of agent ibyvG
i(π) =EP
π[PH
h=1ri,h(sh, ah)].
Definition C.4 (ϵ-approximate Nash equilibrium with information sharing) .For any ϵ≥0, a product
policy π⋆∈Πis anϵ-approximate Nash equilibrium of the POSG Gwith information sharing if:
NE-gap( π⋆) := max
i
max
π′
i∈ΠivG
i(π′
i×π⋆
−i)−vG
i(π⋆)
≤ϵ.
Definition C.5 (ϵ-approximate coarse correlated equilibrium with information sharing) .For any
ϵ≥0, a joint policy π⋆∈Πis anϵ-approximate coarse correlated equilibrium of the POSG Gwith
information sharing if:
CCE-gap( π⋆) := max
i
max
π′
i∈ΠivG
i(π′
i×π⋆
−i)−vG
i(π⋆)
≤ϵ.
Definition C.6 (ϵ-approximate correlated equilibrium with information sharing) .For any ϵ≥0, a
joint policy π⋆∈Πis an ϵ-approximate correlated equilibrium of the POSG Gwith information
sharing if:
CE-gap( π⋆) := max
i
max
ϕivG
i((mi⋄π⋆
i)⊙π⋆
−i)−vG
i(π⋆)
≤ϵ,
where miis called strategy modification for agent i, and mi={mi,h,c h,pi,h}h,ch,pi,h, with
each mi,h,c h,pi,h:Ai→ A ibeing a mapping from the action set to itself. The space of
miis denoted as Mi. The composition mi⋄πiis defined as follows: at step h, when agent
iis given chandpi,h, the joint action chosen to be (a1,h,···, ai,h,···, an,h)will be modi-
fied to (a1,h,···, mi,h,c h,pi,h(ai,h),···, an,h). Note that this definition follows from that in
[78,50,38,49,51] when there exists certain common information, and is a natural generaliza-
tion of the definition in the normal-form game case [ 71]. We denote by Mgen
ithe space of all possible
strategy modifications miif it conditions on any history information instead of only (ch, pi,h). Simi-
larly, we use MS,ito denote the space of all possible strategy modifications miif it only conditions
on the current state e.g., a modification mi,h,s:Ai→ A i.
22C.2.1 Evolution of the Common and Private Information
Assumption C.7 (Evolution of common and private information) .We assume that common informa-
tion and private information evolve over time as follows:
•Common information chis non-decreasing over time, that is, ch⊆ch+1for all h. Let
ϖh+1=ch+1\ch. Thus, ch+1={ch, ϖh+1}. Further, we have
ϖh+1=χh+1(ph, ah, oh+1), (C.3)
where χh+1is a fixed transformation. We use Υh+1to denote the collection of ϖh+1at
steph.
• Private information evolves according to:
pi,h+1=ξi,h+1(pi,h, ai,h, oi,h+1), (C.4)
where ξi,h+1is a fixed transformation.
Equation (C.3) states that the increment in the common information depends on the “new" information
(ah, oh+1)generated between steps handh+ 1and part of the old information ph. The incremental
common information can be obtained by certain sharing and communication protocols among
the agents. Equation (C.4) implies that the evolution of private information only depends on the
newly generated private information ai,handoi,h+1. These evolution rules are standard in the
literature [ 62,63], specifying the source of common information and private information. Based
on such evolution rules, we define {fh}h∈[H]and{gh}h∈[H], where fh:Ah× Oh→ C hand
gh:Ah× Oh→ P hforh∈[H], as the mappings that map the joint history to common information
and joint private information, respectively.
C.3 Strategy Independence of Belief and Examples
To solve a POSG without computationally intractable oracles, certain information-sharing is needed
even under the observability assumption [51]. We thus make the following assumption as in [51].
Assumption C.8 (Strategy independence of beliefs [ 62,31,51]).Consider any step h∈[H], any
policy π∈Π, and any realization of common information chthat has a non-zero probability under
the trajectories generated by π1:h−1. Consider any other policies π′
1:h−1, which also give a non-
zero probability to ch. Then, we assume that: for any such ch∈ Ch, and any ph∈ Ph, sh∈ S,
Pπ1:h−1,G
h(sh, ph|ch) =Pπ′
1:h−1,G
h(sh, ph|ch).
We provide examples satisfying this assumption in Appendix C.2, which include the fully-sharing
structure as in [ 27,69] as a special case. Finally, we also assume that common information and
private information evolve over time properly in Assumption C.7, as standard in [ 62,63,51], which
covers the models considered in [27, 69, 49].
Here we take the examples from [ 52,51] to illustrate the generality of the information-sharing
framework.
Example C.9 (One-step delayed sharing) .At any step h∈[H], the common and private information
are given as ch={o2:h−1, a1:h−1}andpi,h={oi,h}, respectively. In other words, the players share
all the action-observation history until the previous step h−1, with only the new observation being
the private information. This model has been shown useful for power control [2].
Example C.10 (State controlled by one controller with asymmetric delay sharing) .We assume there
are2players for convenience. It extends naturally to n-player settings. Consider the case where
the state dynamics are controlled by player 1, i.e., Th(·|sh, a1,h, a2,h) =Th(·|sh, a1,h, a′
2,h)
for all (sh, a1,h, a2,h, a′
2,h, h). There are two kinds of delay-sharing structures we could con-
sider: Case A: the information structure is given as ch={o1,2:h, o2,2:h−d, a1,1:h−1},p1,h=∅,
p2,h={o2,h−d+1:h}, i.e., player 1’s observations are available to player 2instantly, while player 2’s
observations are available to player 1with a delay of d≥1time steps. Case B: similar to Case A but
player 1’s observation is available to player 2with a delay of 1step. The information structure is given
asch={o1,2:h−1, o2,2:h−d, a1,1:h−1},p1,h={o1,h},p2,h={o2,h−d+1:h}, where d≥1. This
kind of asymmetric sharing is common in network routing [ 67], where packages arrive at different
hosts with different delays, leading to asymmetric delay sharing among hosts.
23Example C.11 (Symmetric information game) .Consider the case when all observations and actions
are available for all the agents, and there is no private information. Essentially, we have ch=
{o2:h, a1:h−1}andpi,h=∅. We will also denote this structure as fully sharing hereafter.
Example C.12 (Information sharing with one-directional-one-step delay) .Similar to the previous
cases, we also assume there are 2players for ease of exposition, and the case can be generalized to
multi-player cases straightforwardly. Similar to the one-step delay case, we consider the situation
where all observations of player 1are available to player 2, while the observations of player 2are
available to player 1with one-step delay. All the past actions are available to both players. That is, in
this case, ch={o1,2:h, o2,2:h−1, a1:h−1}, and player 1has no private information, i.e., p1,h=∅, and
player 2has private information p2,h={o2,h}.
Example C.13 (Uncontrolled state process) .Consider the case where the state transition does not
depend on the actions, that is, Th(· |sh, ah) =Th(· |sh, a′
h)for any sh, ah, a′
h, h. Note that the
agents are still coupled through the joint reward. An example of this case is the information structure
where controllers share their observations with a delay of d≥1time steps. In this case, the common
information is ch={o2:h−d}and the private information is pi,h={oi,h−d+1:h}. Such information
structures can be used to model repeated games with incomplete information [4].
D Collection of Algorithms
Algorithm 1 Learning Decoding Function with Privileged Information
Require:
• POMDP P,
• Expert policy πE∈ΠS,
• Number of sampled episodes per step M.
Ensure: A decoding function for each step {gh}h∈[H](see Theorem 4.6)
For the h= 1step: Collect Mstate-observation pairs from the first-step bD1=n
s(i)
1, o(i)
1o
i∈[M]
on POMDP Pand define the decoding function g1for the first step as:
g1(o1) =n
s1: (s1, o1)∈bD1o
foreach step h∈[2, H]do
Collect Mepisodesn
s(i)
1:H+1, o(i)
1:H, a(i)
1:Ho
i∈[M]on POMDP Pusing policy πEand let:
bDh:=n
s(i)
h−1, a(i)
h−1, o(i)
h, s(i)
ho
i∈[M]
Define the decoding function ghfor step has:
gh(sh−1, ah−1, oh) =n
sh: (sh−1, ah−1, oh, sh)∈bDho
end for
return {gh}h∈[H]
24Algorithm 2 Belief-Weighted Optimistic Asymmetric Actor-Critic with Privileged Information
Require:
• POMDP P,
•Subroutine TQthat given policy π∈ΠL, outputs {eQ}h∈[H]that approximates {Qπ,P
h}h∈[H],
• Subroutine Tbthat outputs {bapx
h}h∈[H]that approximate {bh}h∈[H],
•Initial finite-memory policy π0={π0
h}h∈[H]∈ΠLfor the POMDP P, step-size η, and
number of iterations T.
Ensure: A near-optimal policy
{bapx
h}h∈[H]←Tb(P)
forIterations t= 1. . . , T do
{eQt−1
h}h∈[H]←TQ(P, πt−1)
Update the policy for each ah∈ A,zh∈ Zhas
πt
h(ah|zh)∝πt−1
h(ah|zh)·exp
ηEsh∼bapx
h(zh)h
eQt−1
h(zh, sh, ah)i
Denote πt=πt
1:H
end for
return A policy uniform at random sampled from set {πt}t∈[T]
Algorithm 3 Optimistic Q-function Estimation with Privileged Information
Require:
• POMDP P, policy π1:H∈ΠLsuch that πh:S → ∆(A),
• Number of episodes Mper step.
Ensure: Approximate Q-functions {eQh}h∈[H](see Lemma H.3)
Initialize: ∀zH+1∈ ZH+1, sH+1∈ S, aH+1∈ A
eQH+1(zH+1, sH+1, aH+1)←0, π H+1(aH+1|zH+1)←1
A
forsteph=H, . . . , 1do
Collect Mtrajectories using policy πand let Dh={τ(i)}i∈[M]be the collected trajectories.
Compute empirical counts and define empirical distributions:
bTh(sh+1|sh, ah) =|{τ∈Dh: (s′
h, a′
h, s′
h+1) = (sh, ah, sh+1)}|
|{τ∈Dh: (s′
h, a′
h) = (sh, ah)}|
bOh(oh|sh) =|{τ∈Dh: (s′
h, o′
h) = (sh, oh)}|
|{τ∈Dh:s′
h=sh}|
foreach memory-state pair (zh, sh)∈ Zh× S do
eQ(zh, sh, ah) = min 
H−h+ 1,Esh+1∼bTh(·|sh,ah),
oh+1∼bOh+1(·|sh+1)[eVh+1(zh+1, sh+1)]
+r(sh, ah) +H·min 
2, C·s
Slog(1/δ1)
max( Nh(sh, ah),1)!
+Esh+1∼bTh(·|sh,ah)H·min 
2, C·s
Olog(1/δ1)
max( Nh+1(sh+1),1)!!
,
whereeVh+1(zh+1, sh+1) =Eah+1∼πh+1(·|zh+1)[eQh+1(zh+1, sh+1, ah+1)]
end for
end for
return {eQh}h∈[H]
25Algorithm 4 Approximate Belief Learning with Privileged Information via Model Truncation
Require:
• POMDP P= (H,S,A,O,{Th}h∈[H],{Oh}h∈[H], µ1,{rh}h∈[H]),
•An MDP learning oracle MDP_Learning that efficiently learns an approximate optimal policy
of an MDP,
• Number of trajectories N,
• The threshold ϵ.
Ensure: Approximate belief {bapx
h}h∈[H](See Theorem H.5)
forh∈[H], sh∈ S do
brh′(s′
h, a′
h)← 1[h′=h, s′
h=sh]for any (h′, s′
h, a′
h)∈[H]× S × A
M ← (H,S,A,{Th}h∈[H], µ1,{brh}h∈[H])to be the MDP associated with P
Ψ(h, sh)←MDP_Learning (M)
Collect Ntrajectories by executing policy Ψ(h, sh)for the first h−1steps then take action
ahfor each ah∈ A deterministically and denote the dataset {(si
h, oi
h, ai
h, si
h+1)}i∈[NA]
for(oh, ah, sh+1)∈ O × A × S do
Nh(sh)←P
i∈[NA]1[si
h=sh]
Nh(sh, ah)←P
i∈[NA] 1[si
h=sh, ai
h=ah]
Nh(sh, ah, sh+1)←P
i∈[NA]1[si
h=sh, ai
h=ah, si
h+1=sh+1]
Nh(sh, oh)←P
i∈[NA]1[si
h=sh, oi
h=oh]
bTh(sh+1|sh, ah)←Nh(sh,ah,sh+1)
Nh(sh,ah)
bOh(oh|sh)←Nh(sh,oh)
Nh(sh)
end for
end for
forh∈[H]do
Slow
h←n
sh∈ S |Nh(sh)
NA≤ϵo
Shigh
h←n
sh∈ S |Nh(sh)
NA> ϵo
end for
for(h, sh, oh, ah, sh+1)∈[H]× Shigh
h× O × A × Shigh
hdo
bTtrunc
h(sh+1|sh, ah)←bTh(sh+1|sh, ah) +P
s′
h+1∈Slow
h+1bTh(s′
h+1|sh,ah)
|Shigh
h+1|
bOtrunc
h(oh|sh)←bOh(oh|sh)
bµtrunc
1(s1) :=bµ1(s1) +P
s′
1∈Slow
1bµ1(s′
1)
|Shigh
1|,∀s1∈ Shigh
1
end for
Let
bPsub:= (H,{Shigh
h}h∈[H],A,O,{bTtrunc
h}h∈[H],{bOtrunc
h}h∈[H],bµtrunc
1,{rh}h∈[H])
Define {bb′,sub
h:Zh→∆(Shigh
h)}h∈[H]to be the approximate belief w.r.t. bPsub
Define {bapx
h:Zh→∆(S)}h∈[H]such that bapx
h(zh)(sh) =bb′,sub
h(zh)(sh)forsh∈ Shigh
hand0
otherwise.
return {bapx
h}h∈[H]
26Algorithm 5 Learning Multi-Agent (Individual) Decoding Functions with Privileged Information
(NE/CCE Version)
Require:
• POSG G= (H,S,A,O,{Th}h∈[H],{Oh}h∈[H], µ1,{ri}i∈[n]),
•π∈ΠS, controller set {Ih⊆[n]}h∈[H],
•Procedure MDP_Learning (·,·)that takes as input an MDP and a reward function and returns
an approximate optimal policy,
• Number of trajectories N.
Ensure: A decoding function for each agent and step {bgj,h}j∈[n],h∈[H](see Theorem J.4)
forh∈[H],sh∈ S do
fori∈[n]do
bri,h′(s′
h, a′
h)← 1[h′=h, s′
h=sh]for any (h′, s′
h, a′
h)∈[H]× S × A
Define the Markov game Massociated with GasM= (H, S, A, {Th}h∈[H], µ1), where
we omit the specification for the reward functions and one can specify them arbitrarily
Define M(π−i)to be the MDP marginalized by π−i
Ψi(h, sh)←MDP_Learning (M(π−i),bri)
end for
For each i∈[n],ah∈ A , collect Ntrajectories by executing policy Ψi(h, sh)×
π−ifor the first h−1steps then take action ahdeterministically and denote the dataset
{(sk,i
h, ok,i
h, ak,i
h, sk,i
h+1)}k∈[NA]
for(oh, ah, sh+1)∈ O × A × S do
Nh(sh)←P
k∈[N],i∈[n]1[sk,i
h=sh]
Nh(sh, aTh,h, sh+1)←P
k∈[N],i∈[n]1[sk,i
h=sh, ak,i
Th,h=aTh,h, sk,i
h+1=sh+1]
Nh(sh, oh)←P
k∈[N],i∈[n]1[sk,i
h=sh, ok,i
h=oh]
bTh(sh+1|sh, aTh,h)←Nh(sh,aTh,h,sh+1)
Nh(sh,aTh,h)
bOh(oh|sh)←Nh(sh,oh)
Nh(sh)
end for
end for
Define bG:= (H,S,A,O,{bTh}h∈[H],{bOh}h∈[H], µ1,{ri}i∈[n])
Definebgj,h(sh|ch, pj,h) :=PbG(sh|ch, pj,h)for each j∈[n],h∈[H],ch∈ Ch,pj,h∈ Pj,h
return {bgj,h}j∈[n],h∈[H]
27Algorithm 6 Learning Multi-Agent (Individual) Decoding Functions with Privileged Information
(CE Version)
Require:
• POSG G= (H,S,A,O,{Th}h∈[H],{Oh}h∈[H], µ1,{ri}i∈[n]),
•π∈ΠS, controller set {Ih⊆[n]},
•Procedure MDP_Learning (·,·)that takes as input an MDP and a reward function and returns
an approximate optimal policy,
• Number of trajectories N.
Ensure: A decoding function for each agent and step {bgj,h}j∈[n],h∈[H](see Theorem J.6)
forh∈[H],sh∈ S do
fori∈[n]do
bri,h′(s′
h, a′
h)← 1[h′=h, s′
h=sh]for any (h′, s′
h, a′
h)∈[H]× S × A .
Define Mextended
i (π)to be the extended MDP, which is defined in Definition J.5.
Ψi(h, sh)←MDP_Learning (Mextended
i (π),bri)
end for
For each i∈[n],ah∈ A , collect Ntrajectories by executing policy Ψi(h, sh)×
π−ifor the first h−1steps then take action ahdeterministically and denote the dataset
{(sk,i
h, ok,i
h, ak,i
h, sk,i
h+1)}k∈[NA]
for(oh, ah, sh+1)∈ O × A × S do
Nh(sh)←P
k∈[N],i∈[n]1[sk,i
h=sh]
Nh(sh, aTh,h, sh+1)←P
k∈[N],i∈[n]1[sk,i
h=sh, ak,i
Th,h=aTh,h, sk,i
h+1=sh+1]
Nh(sh, oh)←P
k∈[N],i∈[n]1[sk,i
h=sh, ok,i
h=oh]
bTh(sh+1|sh, aTh,h)←Nh(sh,aTh,h,sh+1)
Nh(sh,aTh,h)
bOh(oh|sh)←Nh(sh,oh)
Nh(sh)
end for
end for
Define bG:= (H,S,A,O,{bTh}h∈[H],{bOh}h∈[H], µ1,{ri}i∈[n])
Definebgj,h(sh|ch, pj,h) :=PbG(sh|ch, pj,h)for each j∈[n],h∈[H],ch∈ Ch,pj,h∈ Pj,h
return {bgj,h}j∈[n],h∈[H]
28Algorithm 7 Optimistic Common-Information-Based Value Iteration with Privileged Information
Require:
• POSG G= (H,S,A,O,{Th}h∈[H],{Oh}h∈[H], µ1,{ri}i∈[n]),
• An approximate belief {bPh:bCh→∆(S × P h)}h∈[H],
• Number of iterations K.
Ensure: An approximate equilibrium policy
Initialize:
N0
h(sh, ah)←0, N0
h(sh, ah, oh)←0,bJ0(oh+1|sh, ah)←1
O
fork∈[K]do
forh←H, H−1,···,1do
forbch∈bChdo
Qhigh,k
i,h(bch, ph, sh, ah)
←minn
ri,h(sh, ah) +bk−1
h(sh, ah) +Eoh+1∼bJk−1
h(·|sh,ah)h
Vhigh,k
i,h+1(bch+1)i
, H−h+ 1o
fori∈[n]
Qlow,k
i,h(bch, ph, sh, ah)
←maxn
ri,h(sh, ah)−bk−1
h(sh, ah) +Eoh+1∼bJk−1
h(·|sh,ah)h
Vlow,k
i,h+1(bch+1)i
,0o
fori∈[n]
Define
Qhigh,k
i,h(bch, γh) :=Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]h
Qhigh,k
i,h(bch, ph, sh, ah)i
fori∈[n]
Define
Qlow,k
i,h(bch, γh) :=Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]h
Qlow,k
i,h(bch, ph, sh, ah)i
fori∈[n]
{πk
j,h(·|·,bch,·)}j∈[n]←Bayesian-CE/CCE ({Qhigh,k
j,h(bch,·)}j∈[n])(c.f. Appendix J.1)
Vhigh,k
i,h(bch)←Eωhh
Qhigh,k
i,h(bch,{πk
j,h(·|ωj,h,bch,·)}j∈[n])i
fori∈[n]
Vlow,k
i,h(bch)←Eωhh
Qlow,k
i,h(bch,{πk
j,h(·|ωj,h,bch,·)}j∈[n])i
fori∈[n]
end for
end for
Execute πkand get trajectory (sk
1:H, ak
1:H, ok
1:H+1)
forh∈[H], sh∈ S, ah∈ A, oh+1∈ O do
Nk
h(sh, ah)←P
l∈[k]1[sl
h=sh, al
h=ah]
Nk
h(sh, ah, oh+1)←P
l∈[k]1[sl
h=sh, al
h=ah, ol
h+1=oh+1]
bJk
h(oh+1|sh, ah)←Nk
h(sh,ah,oh+1)
Nk
h(sh,ah)
end for
end for
k⋆←arg min k∈[K],i∈[n]Vhigh,k
i,1(ck
1)−Vlow,k
i,1(ck
1)
return πk⋆for general-sum games or the marginalized policy of πk⋆for zero-sum games
29Algorithm 8 Approximate Belief Learning for MARL with Privileged Information
Require:
• POSG G= (H,S,A,O,{Th}h∈[H],{Oh}h∈[H], µ1,{ri,h}i∈[n],h∈[H]),
• Controller set {Ih⊆[n]}h∈[H],
• Procedure MDP_Learning (·,·),
• Number of trajectories N,
• Accuracy ϵ.
Ensure: An approximate belief
forh∈[H], sh∈ S do
brh′(s′
h, a′
h)← 1[h′=h, s′
h=sh]for any (h′, s′
h, a′
h)∈[H]× S × A .
M ← (H,S,A,{Th}h∈[H], µ1)to be the MDP by ignoring the observation and emission of
G, where we omit the specification for the reward functions and one can specify them arbitrarily
Ψ(h, sh)←MDP_Learning (M,br)
Collect Ntrajectories by executing policy Ψ(h, sh)for the first h−1steps then take action
ahfor each ah∈ A deterministically and denote the dataset {(si
h, oi
h, ai
h, si
h+1)}i∈[NA]
for(oh, ah, sh+1)∈ O × A × S do
Nh(sh)←P
i∈[NA]1[si
h=sh]
Nh(sh, aTh,h)←P
i∈[NA] 1[si
h=sh, ai
Th,h=aTh,h]
Nh(sh, aTh,h, sh+1)←P
i∈[NA]1[si
h=sh, ai
Th,h=aTh,h, si
h+1=sh+1]
Nh(sh, oh)←P
i∈[NA]1[si
h=sh, oi
h=oh]
bTh(sh+1|sh, aTh,h)←Nh(sh,ah,sh+1)
Nh(sh,aTh,h)
bOh(oh|sh)←Nh(sh,oh)
Nh(sh)
end for
end for
forh∈[H]do
Slow
h←n
sh∈ S |Nh(sh)
NA≤ϵo
Shigh
h←n
sh∈ S |Nh(sh)
NA> ϵo
end for
for(h, sh, oh, ah, sh+1)∈[H]× Shigh
h× O × A × Shigh
hdo
bTtrunc
h(sh+1|sh, ah)←bTh(sh+1|sh, ah) +P
s′
h+1∈Slow
h+1bTh(s′
h+1|sh,ah)
|Shigh
h+1|
bOtrunc
h(oh|sh)←bOh(oh|sh)
bµtrunc
1(s1) :=bµ1(s1) +P
s′
1∈Slow
1bµ1(s′
1)
|Shigh
1|,∀s1∈ Shigh
1
end for
Let
bGsub:= (H,{Shigh
h}h∈[H],A,O,{bTtrunc
h}h∈[H],{bOtrunc
h}h∈[H],bµtrunc
1,{ri,h}i∈[n],h∈[H])
Define {ePh:bCh→∆(Shigh
h× Ph)}h∈[H]to be the approximate belief w.r.t. bGsub
Define {bPh:bCh→∆(S × P h)}h∈[H]such that bPh(sh, ph|bch) =ePh(sh, ph|bch)forsh∈ Shigh
hand0otherwise
return {bPh}h∈[H]
30E Missing Details in Section 3
Proof of Proposition 3.1: We recall that bh(·)is the belief of the agent about the underlying state,
see Appendix C for a detailed introduction. Note that Equation (3.1) can be written as
bπ⋆∈arg min
π∈ΠHX
h=1EP
τh∼π′Esh∼bh(τh)[Df(π⋆
h(·|sh)|πh(·|τh))].
Therefore, for any h∈[H]andτhsuch that Pπ′,P(τh)>0, we can optimize πseparately for each
h∈[H]andτhas:
bπ⋆
h(·|τh)∈argmin
q∈∆(A)Esh∼bh(τh)[Df(π⋆
h(·|sh)|q)].
Now we are ready to construct the counter-example of γ-observable POMDP Pϵfor some ϵ∈(0,1)
with H= 1,S=
s1, s2	
,A=
a1, a2	
, andO=
o1, o2	
. We let µ1= (1−γ
2−γ,1
2−γ),
O1(o1|s1) = 1 , andO1(o1|s2) = 1 −γ,O1(o2|s2) =γ. Therefore, it is direct to see that
O1is exactly γ-observable. Most importantly, we choose r1(s1, a1) = 1 ,r1(s1, a2) = 0 , and
r1(s2, a1) = 0 ,r1(s2, a2) =ϵ.
Therefore, given such a reward function, the fully observable expert policy is given by π⋆
1(a1|s1) = 1
andπ⋆
1(a2|s2) = 1 , i.e., choosing a1at state s1anda2at state s2deterministically. Meanwhile, by
our construction, one can compute that the belief given observation o1ensures b1(o1) = Unif( S).
Hence, the corresponding “distilled” partially observable policy under observation o1is given by
bπ⋆
1(·|o1) = argmin
q∈∆(A)Es1∼b1(o1)[Df(π⋆
1(·|s1)|q)]
= argmin
q∈∆(A)Df(π⋆
1(·|s1)|q) +Df(π⋆
1(·|s2)|q)
2
= argmin
q∈∆(A)f(1/q(a1))q(a1) +f(0)q(a2) +f(0)q(a1) +f(1/q(a2))q(a2)
2
= argmin
q∈∆(A)f(0) + f(1/q(a1))q(a1) +f(1/q(a2))q(a2)
2,
where the last step is due to q∈∆(A). Now consider the function g(x) =xf(1/x)forx >0. It is
direct to compute that g′(x) =f(1/x)−f′(1/x)
x, and g′′(x) =f′′(1/x)
x3≥0due to the convexity of
the function f. Thus, we conclude that gis also convex. By Jensen’s inequality, we have
f(1/q(a1))q(a1) +f(1/q(a2))q(a2)
2≥f(2/(q(a1) +q(a2)))(q(a1) +q(a2))/2 =f(2)/2,
where the equality holds when q(a1) =q(a2) =1
2. This indicates that bπ⋆
1(·|o1) = Unif( A). On the
other hand, combining the fact that b1(o1) = Unif( S)withϵ <1, it is direct to see that the optimal
partially observable policy eπ∈arg max π∈ΠvP(π)satisfies eπ1(a1|o1) = 1 . Now we are ready to
evaluate the optimality gap between eπandbπ⋆as follows
vPϵ(eπ)−vPϵ(bπ⋆) =PPϵ(o1)(Veπ,Pϵ
1(o1)−Vbπ⋆,Pϵ
1 (o1)) +PPϵ(o2)(Veπ,Pϵ
1(o2)−Vbπ⋆,Pϵ
1 (o2))
≥PPϵ(o1)(Veπ,Pϵ
1(o1)−Vbπ⋆,Pϵ
1 (o1)),
where the last step is due to the fact that eπis the optimal policy, leading to the fact that Veπ,Pϵ
1(o2)−
Vbπ⋆,Pϵ
1 (o2)≥0. Now it is not hard to compute that
PPϵ(o1)≥1−γ.
Meanwhile, we can evaluate that
Veπ,Pϵ
1(o1) =1
2, Vbπ⋆,Pϵ
1 (o1) =1 +ϵ
4
and correspondingly Veπ,Pϵ
1(o1)−Vbπ⋆,Pϵ
1 (o1) =1−ϵ
4, implying that vPϵ(eπ)−vPϵ(bπ⋆)≥(1−γ)(1−ϵ)
4.
This concludes our proof. ■
31Note that another counterexample in a similar spirit was also constructed in [ 34], demonstrating
that the expert policy for a poorly chosen agent-environment boundary can be useless in imitation
learning, although the γ-observability property is not satisfied for the construction therein.
Remark E.1. The counter-example Pϵconstructed above can be also used to demonstrate the bias of
the state-only-based value function as an estimate of the history-based value function that appeared in
the policy gradient formula for POMDPs, in the finite-horizon setting, i.e. Esh∼bh(τh)[Vπ,Pϵ
h(sh)]̸=
Vπ,Pϵ
h(τh)(mirroring Theorem 4.2 of [ 6]). Specifically, in the counter-example above, we consider
the policy πsuch that π1(a1|o1) = 1 andπ1(a2|o2) = 1 . The state-only-based value function can
be evaluated as
Vπ,Pϵ
1(s1) =1−γ
2−γ, Vπ,Pϵ
1(s2) =γϵ,
which implies that Es1∼b1(o1)[Vπ,Pϵ
1(s1)] =Vπ,Pϵ
1(s1)+Vπ,Pϵ
1(s2)
2=1−γ
2−γ+γϵ
2. On the other hand, it
holds that Vπ,Pϵ
1(o1) =1+ϵ
2, which is not equal to Es1∼b1(o1)[Vπ,Pϵ
1(s1)], showing the bias of such
a state-only value function.
Example E.2 (Deterministic POMDP [ 36,81]).We say a POMDP Pis of deterministic transition
if entries of matrices {Th}h∈[H]and the vector µ1are either 0or1. Note that we do not make any
assumptions on the emission matrices.
Example E.3 (Block MDP [ 43,18]).We say a POMDP Pis a block MDP if for any h∈[H],
sh, s′
h∈ S, it holds that supp(Oh(·|sh))∩supp(Oh(·|s′
h)) =∅when sh̸=s′
h.
Example E.4 (k-step decodable POMDP [ 19]).We say a POMDP Pis ak-step decodable POMDP
if there exists an unknown decoder ϕ⋆={ϕ⋆
h:Zh→ S} h∈[H]such that for any h∈[H]and
reachable trajectory τh,PP(sh=ϕ⋆
h(zh)|τh) = 1 , where Zh= (O × A )max{h−1,k−1}× O ,
zh= ((o, a)k(h):h−1, oh), and k(h) = max {h−k+ 1,1}.
Finally, to understand how our condition can extend beyond known examples in the literature, we
show that one can indeed allow the decoding length of Example E.4 to be unknown and arbitrary
(instead of being a small known constant as in [19] to have provably efficient algorithms).
Example E.5 (POMDP with arbitrary, unknown decodable length) .This example is similar to
Example E.4, but the decoding length mis unknown and not necessarily a small constant.
In light of the pitfall in Proposition 3.1, we will analyze both the computational and statistical
efficiencies of expert distillation in Section 4, under the condition in Definition 3.2.
Proof of Example E.2 & Example E.3 & Example E.4 & Example E.5: To see why those examples
follow our Definition 3.2, it is indeed an immediate result of Proposition 7.1. ■
Proof of Proposition 3.3:
Here we evaluate the computational complexity and sample complexity of each iteration tas follows.
Sample complexity: The algorithm executes the policy πt−1and collect Kepisodes, denoted as
{sk
1:H+1, ok
1:H, ak
1:H}k∈[K]. Thus, the sample complexity of each iteration is Θ(K).
Computational complexity for policy evaluation: The policy evaluation of the vanilla asymmetric
actor-critic is done by minimizing the Bellman error. In the finite-horizon setting with tabular
parameterization, it is equivalent to performing the following update for each h∈[H]in a backward
way and each k∈[K]:
Qt
h(τk
h, sk
h, ak
h)←(1−α)Qt−1
h(τk
h, sk
h, ak
h)
+α
rh(sk
h, ak
h) +1
|J(τk
h, sk
h, ak
h)|X
j∈J(τk
h,sk
h,ak
h)Qt
h+1(τj
h+1, sj
h+1, aj
h+1)
,
for some stepsize α∈(0,1), where J(τk
h, sk
h, ak
h) :={j∈[K]|(τj
h, sj
h, aj
h) = ( τk
h, sk
h, ak
h)}.
Therefore, the computational complexity for this procedure is of POLY (H, K ).
32Computational complexity for policy improvement: For tabular parameterization, computing
∇logπt−1
h(ak
h|τk
h)takesO(1)computation. Hence the policy update in Equation (3.2) performs
POLY (H, K )computation.
Meanwhile, under the exponential time hypothesis, there is no polynomial time algorithm for even
planning an ϵ-approximate optimal policy in γ-observable POMDPs [ 26]. This implies that the
vanilla asymmetric actor-critic needs to take super-polynomial time to find an approximately optimal
policy. This implies the corresponding sample complexity has to be super-polynomial.
Finally, we remark that even if we let the policy and the Q-function not depend on the entire history
τhbut only the finite-memory zh, the proof still holds. The key is that whenever one only computes
at the sampled history/finite-memories, i.e., updates the policy in an asynchronous way (in contrast
to the synchronous one where the policies at allhistories/finite-memories are updated), the sample
and computational complexities will be coupled with the same order per iteration, which implies a
super-polynomial sample complexity due to the super-polynomial computational complexity. This
completes the proof. ■
Derivation for the closed-form update Equation (3.3). Note that the proximal policy optimization
[72] update has the policy improvement as follows
πt←arg max
π

Lt−1(π)−η−1EP
πt−1
X
h∈[H]KL(πh(·|τh)|πt−1
h(·|τh))


, (E.1)
where ηis some learning rate and Lt−1(π)is a first-order approximation of the expected accumulated
rewards at πt−1:
Lt−1(π) :=vP(πt−1) +EP
πt−1
X
h∈[H]
Qt−1
h(τh, sh,·), πh(·|τh)−πt−1
h(·|τh)
.
By plugging Lt−1(π)into Equation (E.1), with simple algebric manipulations, we prove that:
πt
h(·|τh)∝πt−1
h(·|τh) exp 
ηEsh∼bh(τh)
Qt−1
h(τh, sh,·)
.
F Missing Details in Section 4
Proof of Lemma 4.4: The proof follows by the assumption that the total cumulative reward is at
most H,
vP(π)≥EP
π

X
h∈[H]rh
 1[∀h:∈[H] :gh(sh−1, ah−1, oh) =sh]

=EP
πE

X
h∈[H]rh
 1[∀h:∈[H] :gh(sh−1, ah−1, oh) =sh]

+EP
πE

X
h∈[H]rh
 1[∃h:∈[H] :gh(sh−1, ah−1, oh)̸=sh]

−EP
πE

X
h∈[H]rh
 1[∃h:∈[H] :gh(sh−1, ah−1, oh)̸=sh]

=EP
πE

X
h∈[H]rh

−EP
πE

X
h∈[H]rh
 1[∃h:∈[H] :gh(sh−1, ah−1, oh)̸=sh]

≥vP(πE)−HPπE,P[∃h:∈[H] :gh(sh−1, ah−1, oh)̸=sh]
≥vP(πE)−Hϵ,
which completes the proof. ■
33Proof of Theorem 4.5: For each step h∈[H]we define Dhto be the distribution over the
underlying state sh−1at step h−1, taken action ah−1∈ A based on πE, the underlying state
transitioned to sh∈ S, and the observation oh∼Oh(·|sh). We remind that we include at step
zero, a dummy state-observation pair (s0, o0), for notational convenience. Formally, Dhis defined as
Dh(sh−1, ah−1, oh, sh) :=PπE,P[sh−1, ah−1, oh, sh].
We first use union bound to decompose the probability that we incorrectly decode,
PπE,P[∃h∈[H] :gh(sh−1, ah−1, oh)̸=sh]≤X
h∈[H]PπE,P[gh(sh−1, ah−1, oh)̸=sh]
=X
h∈[H]P(sh−1,ah−1,oh,sh)∼Dh[gh(sh−1, ah−1, oh)̸=sh].
(F.1)
For each h∈[H], we can use Mepisodes to collect Msamples from the distribution Dh. In addition,
since state sH+1is dummy, we need not to collect episodes for DH+1. Denote the set of collected
samples by bDM
h. We define the decoding ghfor step h∈[H]as follows:
gh(sh−1, ah−1, oh) =n
sh|(sh−1, ah−1, oh, sh)∈bDM
ho
.
Observe that by Definition 3.2, {sh|(sh−1, ah−1, oh, sh)∈bDM
h}is either the empty set or contains
only a single elements, in which case, it is true that gh(sh−1, ah−1, oh) =ψh(sh−1, ah−1, oh)(ψ
is the real decoding function, see Definition 3.2). Moreover, we slightly abuse the notation and let
eDM
hdenote the empirical distribution induced by the samples in bDM
h. Thus, with probability at least
1−δ
Hand setting M= Θ
A·O·S+log( H/δ)
ϵ2
for each step h∈[H], we obtain the following by the
result in [13]:
PπE,P[gh(sh−1, ah−1, oh)̸=sh] =PπE,P[gh(sh−1, ah−1, oh) =∅]
=P(sh−1,ah−1,oh,sh)∼Dhh
(sh−1, ah−1, oh, sh)/∈bDM
hi
=X
u∈supp( Dh)Pu′∼Dh[u=u′] 1[u /∈supp(eDM
h)]
≤dTV(Dh,eDM
h)
≤ϵ.
Thus, by union bound, with probability at least 1−δ, we have that for each step h∈[H],
P(sh−1,ah−1,oh,sh)∼Dh[gh(sh−1, ah−1, oh)̸=sh]≤ϵ,
which in combination with Equation (F.1) concludes the proof. Finally, we note that we used
a total of Θ
H·A·O·S+log( H/δ)
ϵ2
episodes from the POMDP, and the computational time was
POLY (H, A, O, S,1
ϵ,log 1
δ
). ■
GProvably Efficient Expert Policy Distillation with Function Approximation
We now turn our attention to the rich-observation setting under our deterministic filter condition.
Definition 3.2 motivates us to consider only succinct policies that incorporate an auxiliary parameter
representing the most recent state, as well as the most recent observations and actions. To handle the
large observation space, we further assume that for each step h∈[H], the agent selects a decoding
function ghfrom a family of multi-class classifiers Fh⊂ {S × A × O → S} . For the function
classFh, we make the standard realizability assumption. We formally summarize our assumptions in
Assumption G.1.
Assumption G.1. We consider a POMDP that satisfies Definition 3.2. In addition, to derive learning
algorithms that do not dependent on O, for each step h∈[H], we assume that we have access to a
class of functions Fh:S × A × O → S such that the perfect decoding function ψh∈ Fh.
34We aim for our final bounds to depend on a complexity measure of the function class F={Fh}h∈[H]
rather than the cardinality of the observation space O. We utilize the Daniely and Shalev-Shwartz-
Dimension (DS Dimension) (Theorem G.2), which characterizes the PAC learnability for multi-class
classification [ 8]. Defining the DS dimension is beyond the scope of our paper; we direct interested
readers to [ 8] for further details. For intuition, readers can think of the DS Dimension as a certificate
of PAC learnability without loss of intuition.
Theorem G.2 (Theorem 1 in [ 8]).Consider a family of multi-class classifiers Fthat map features in
space x∈ X to labels in space y∈ Y. Moreover, assume there is a joint probability distribution D
over features in Xand labels in Y, and that there exists g∗∈ F such that for each (x, y)∈supp( D),
g∗(x) =y. Given nsamples from D, there exists an algorithm that with probability at least 1−δ
outputs eg∈ F such that
P(x,y)∼D[eg(x)̸=y]≤eO 
d3/2
DS(F) + log 1
δ
n!
,
where dDS(F)denotes the Daniely and Shalev-Shwartz-Dimension of the function class F.
We are now ready to present the main theorem of this section.
Theorem G.3. Consider a POMDP Pthat satisfies Definition 3.2, a policy πE∈ΠS, and let
{Fh⊆ {S × A × O → S}} h∈[H]be the decoding function class, and ψh∈ Fhfor each h∈[H],
i.e.,{Fh}h∈[H]is realizable. Then given access to the classification oracle of [ 9], there exists an
algorithm learning the decoding function {gh}h∈[H]such that with probability at least 1−δ, for each
steph∈[H]:
PπE,P[∃h∈[H] :gh(sh−1, ah−1, oh)̸=sh]≤ϵ,
usingO
H2
maxh∈[H]d3/2
DS(Fh)+log(1
δ)
ϵ
episodes, where dDS(Fh)is the Daniely and Shalev-
Shwartz-Dimension of Fh[8].
Combining Theorem G.3 and Lemma 4.4, we obtain the final polynomial sample complexity in this
function approximation setting, using classification (supervised learning) oracles (c.f. Table 1).
Proof of Theorem G.3:
For each step h∈[H], we define Dhto be the distribution over the underlying state sh−1at step h−1,
taken action ah−1∈ A from πE, the underlying state transitioned to sh∈ S, and the hallucinated
observation oh∼Oh(·|sh)(we remind readers that for step 0, we use dummy state s0and action
a0). Formally, the probability that the sequence (sh−1, ah−1, oh, sh)is sampled from Dhequals to
Dh(sh−1, ah−1, oh, sh) :=PπE,P[sh−1, ah−1, oh, sh].
We first use union bound to decompose the misclassification error,
PπE,P[∃h∈[H] :egh(sh−1, ah−1, oh)̸=sh]≤X
h∈[H]PπE,P[egh(sh−1, ah−1, oh)̸=sh]
=X
h∈[H]P(sh−1,ah−1,oh,sh)∼Dh[egh(sh−1, ah−1, oh)̸=sh].
(G.1)
For each h∈[H], we can use eO
H
ϵ·
max h∈[H]d3/2
DS(Fh) + log 1
δ
episodes to collect
eO
H
ϵ·
max h∈[H]d3/2
DS(Fh) + log 1
δ
samples from distribution Dh. Hence, by Theorem G.2,
with probability at least 1−δ
H, we have that
P(sh−1,ah−1,oh,sh)∼Dh[egh(sh−1, ah−1, oh)̸=sh]≤ϵ
H.
Thus, by union bound, with probability at least 1−δ, using a total of
eO
H2
ϵ·
max h∈[H]d3/2
DS(Fh) + log 1
δ
episodes we have that,
X
h∈[H]P(sh−1,ah−1,oh,sh)∼Dh[egh(sh−1, ah−1, oh)̸=sh]≤ϵ,
35which in combination with Equation (G.1) concludes the proof. ■
H Missing Details in Section 5
Proof of Theorem 5.1: Letπ∗∈argmaxπ∈ΠLEs1∼µ1[Vπ
1(s1)], where we define Vπ
1(s1) :=
Eo1∼O1(·|s1),a1∼π1(·|o1)[Qπ
h(z1= (o1), s1, a1)]. We first note the following equation
1
TX
t∈[T]Es1∼µ1[Vπt
1(s1)]
=Es1∼µ1[Vπ∗
1(s1)] +1
TX
t∈[T]Es1∼µ1
eVπt
1(s1)−Vπ∗
1(s1)
+1
TX
t∈[T]Es1∼µ1
Vπt
1(s1)−eVπt
1(s1)
.
(H.1)
Next, we make use of the performance difference lemma [ 39,1,74] on the extended spaceQ
h∈[H](Zh× S).
Definition H.1. Consider the class of policies ΠPLsuch that at step h∈[H], the policies in ΠPL
take an action based on finite memory up to this step and the use of the underlying state, e.g., for
each policy π1:H∈ΠPL,πh:Zh× S → ∆(A).
Observation 1. Note that ΠL⊆ΠPL.
Lemma H.2 (Performance difference Lemma [ 39,1,74]; see e.g., Lemma 1 in [ 74]).For any pair of
policies π={πh}h∈[H], π′={π′
h}h∈[H]∈ΠPL, and approximation of the Q-function of policy π,
we have that for each state s1∈supp( µ1):
eVπ
1(z1, s1)−Vπ′
1(z1, s1)
=X
h∈[H]Eτh∼π′|z1hD
eQπ
h(zh, sh,·), πh(· |zh, sh)−π′
h(· |zh, sh)Ei
+X
h∈[H]Eτh∼π′|z1"
eQπ
h(zh, sh, ah)−Esh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)h
rh(sh, ah) +eVπ
h+1(zh+1, sh+1)i#
,
whereeVπ
h(zh, sh) =Eah∼πh(·|zh)[eQπ
h(zh, sh, ah)].
Setting π=πt∈ΠL⊆ΠLP, and π′=π∗∈ΠL⊆ΠLP, where we remind that π∗∈
argmaxπ∈ΠLVπ
1(s1), and for each zh∈ Zhandh∈[H], we abuse the notation by letting πt
h(· |
zh, sh) =πt
h(· |zh)andπ∗
h(· |zh, sh) =π∗
h(· |zh)for all sh∈ S. The above formulation is thus
simplified to
Es1∼µ1[eVπt
1(s1)−Vπ∗
1(s1)]
=X
h∈[H]Eτh∼π∗hD
eQπt
h(zh, sh,·), πt
h(· |zh, sh)−π∗
h(· |zh, sh)Ei
+X
h∈[H]Eτh∼π∗"
eQπ
h(zh, sh, ah)−Esh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)h
rh(sh, ah) +eVπ
h+1(zh+1, sh+1)i#
≥X
h∈[H]Eτh∼π∗hD
eQπt
h(zh, sh,·), πt
h(· |zh, sh)−π∗
h(· |zh, sh)Ei
,
36where in the inequality above we used Lemma H.3. Since our policy does not depend on the realized
underlying state sh,
Es1∼µ1[eVπt
1(s1)−Vπ∗
1(s1)]
≥X
h∈[H]Eτh∼π∗hD
eQπt
h(zh, sh,·), πt
h(· |zh, sh)−π∗
h(· |zh, sh)Ei
=X
h∈[H]Eτh∼π∗hD
Esh∼b(τh)h
eQπt
h(zh, sh,·)i
, πt
h(· |zh)−π∗
h(· |zh)Ei
=X
h∈[H]Eτh∼π∗hD
Esh∼bapx(zh)h
eQπt
h(zh, sh,·)i
, πt
h(· |zh)−π∗
h(· |zh)Ei
+X
h∈[H]Eτh∼π∗
Esh∼b(τh)h
eQπt
h(zh, sh,·)i
−Esh∼bapx(zh)h
eQπt
h(zh, sh,·)i
, πt
h(· |zh)−π∗
h(· |zh)
≥X
h∈[H]Eτh∼π∗hD
Esh∼bapx(zh)h
eQπt
h(zh, sh,·)i
, πt
h(· |zh)−π∗
h(· |zh)Ei
−X
h∈[H]Eτh∼π∗hEsh∼b(τh)h
eQπt
h(zh, sh,·)i
−Esh∼bapx(zh)h
eQπt
h(zh, sh,·)i
1i
≥X
h∈[H]Eτh∼π∗hD
Esh∼bapx(zh)h
eQπt
h(zh, sh,·)i
, πt
h(· |zh)−π∗
h(· |zh)Ei
−2·H·X
h∈[H]Eτh∼π∗
dTV(bh(τh),bapx
h(zh))
.
The last inequality follows by Lemma H.3. By averaging we get,
1
TX
t∈[T]Es1∼µ1[eVπt
1(s1)]
≥Es1∼µ1[Vπ∗
1(s1)] +1
TX
h∈[H]Eτh∼π∗
X
t∈[T]D
Esh∼bapx(zh)h
eQπt
h(zh, sh,·)i
, πt
h(· |zh)−π∗
h(· |zh)E

−2·H·X
h∈[H]Eτh∼π∗
dTV(bh(τh),bapx
h(zh))
≥Es1∼µ1[Vπ∗
1(s1)] +H
Tmax
h∈[H]Eτh∼π∗
X
t∈[T]D
Esh∼bapx(zh)h
eQπt
h(zh, sh,·)i
, πt
h(· |zh)−π∗
h(· |zh)E

−2·H2·max
h∈[H]Eτh∼π∗
dTV(bh(τh),bapx
h(zh))
≥Es1∼µ1[Vπ∗
1(s1)]−2Hp
Hlog (|A|)√
T−2·H2·max
h∈[H]Eτh∼π∗
dTV(bh(τh),bapx
h(zh))
,
where the last inequality follows since for fixed h∈[H]andzh∈ Zh, the agent updates her policy
on memory zhaccording to MWU on feedbackn
Esh∼bapx(zh)h
eQπt
h(zh, sh, a)io
a∈A, and thus, the
accumulate regret is bounded by (Section 4.3 in [10]):
X
t∈[T]D
Esh∼bapx(zh)h
eQπt
h(zh, sh,·)i
, π∗
h(· |zh)−πt
h(· |zh)E
≤log (|A|)
η+η·T·Qπt
h(zh, sh,·)
+∞≤log (|A|)
η+η·T·H= 2p
T·Hlog (|A|).
The proof follows by combining Equation (H.1) and the inequality above. Finally, to achieve the near
optimality in the class of ΠL, we bound the optimistic estimate using Equation (H.2) in Lemma H.3,
and its global near-optimality for a large enough Lunder γ-observability is a direct consequence of
Theorem 4.1 in [26]. ■
37Lemma H.3 (Optimistic Q-function - adapted from [ 48]).Given a policy π∈ΠL, and a parameter
M∈N, let{eQπ
h:Zh×S ×A → [0, H]}h∈[H]be the output of Algorithm 3. Then, with probability
at least 1−δ:∀zh∈ Zh, sh∈ S, ah∈ A
H−h+ 1≥eQπ
h(zh, sh, ah)≥Esh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)h
rh(sh, ah) +eVπ
h+1(zh+1, sh+1)i
,
Es1∼µ1[eVπ
1(s1)]−Es1∼µ1[Vπ
1(s1)]≤O 
H2·s
max( O, S)·S·A
M·logS·A
δ
logM·S·A·H
δ!
,
(H.2)
where Vπ
1(s1) = Eo1∼O1(·|s1),a1∼π1(·|o1)[Qπ
h(z1= ( o1), s1, a1)],eVπ
1(s1) =
Eo1∼O1(·|s1),a1∼π1(·|o1)[eQπ
h(z1= (o1), s1, a1)]andeVπ
h(zh, sh) =Eah∼πh(·|zh)[eQπ
h(zh, sh, ah)].
Moreover, Algorithm 3 needs a total of H·Mepisodes from POMDP Pand runs in time
POLY (H, M, S, AL, OL).
Proof. For each step h∈[H], collect Mtrajectories with states using policy πon POMDP Pand let
Dh={τ(i)}i∈[M]be those collected trajectories. Define the empirical transition, observation and
reward distribution as follows:
Nh(sh, ah, sh+1) ={τ= (s′
1, o′
1, a′
1, r′
1. . . , s′
h, o′
h, a′
h, r′
h)∈Dh
: (sh, ah, sh+1) = (s′
h, a′
h, s′
h+1)},
Nh(sh, ah) =X
sh+1∈SNh(sh, ah, sh+1),
Nh(sh) =X
ah∈ANh(sh, ah),
Nh(sh, oh) ={τ= (s′
1, o′
1, a′
1, r′
1. . . , s′
h, o′
h, a′
h, r′
h)∈Dh: (sh, oh) = (s′
h, o′
h)},
bTh(sh+1|sh, ah) =Nh(sh, ah, sh+1)
Nh(sh, ah),
bOh(oh|sh) =Nh(sh, oh)
Nh(sh).
Setδ1=δ
2·S·(A+1). By [ 13], there exists a constant C > 0such that for each step h∈[H], state
s∈ S and action a∈ A with probability at least 1−δ1:
∥Th(· |sh, ah)−bTh(· |sh, ah)∥1≤min 
2, C·s
Slog(1/δ1)
max( Nh(sh, ah),1)!
,
∥Oh(· |sh)−bOh(· |sh)∥1≤min 
2, C·s
Olog(1/δ1)
max( Nh(sh),1)!
.
For the rest of the proof, we condition on this event. By union bound, this happens with probability
at least 1−δ
2. We define the optimistic Q-function recursively as follows for a memory-state pair
(zh, sh)∈ Zh× S:
eQπ
H+1(zH+1, sH+1,·) = 0 , ∀zH+1∈ ZH+1, sH+1∈ S
eQπ
h(zh, sh, ah) = min 
H−h+ 1,Esh+1∼bTh(·|sh,ah),
oh+1∼bOh+1(·|sh+1)[eVπ
h+1(zh+1, sh+1)] +r(sh, ah)
+H·min 
2, C·s
Slog(1/δ1)
max( Nh(sh, ah),1)!
+Esh+1∼bTh(·|sh,ah)H·min 
2, C·s
Olog(1/δ1)
max( Nh+1(sh+1),1)!!
,
38whereeVπ
h+1(zh+1, sh+1) =Eah+1∼πh+1(·|zh+1)[eQπ
h+1(zh+1, sh+1, ah+1)]. Hence the time com-
plexity of our algorithm is POLY (H, M, S, AL, OL). To prove the first condition, we fix step
h∈[H], zh∈ Zh, ah∈ A and state sh∈ Sand consider the case where eQπ
h(zh, sh, ah) =H−h+1.
In this case, since by assumption on the POMDP P,rh(sh, ah)≤1, and by definition of
eQπ
h+1(·,·,·)≤H−h, we have:
eQπ
h(zh, sh, ah) = 1 + H−h≥Esh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)[rh(sh, ah) +eVπ
h+1(zh+1, sh+1)].
IfeQπ
h(zh, sh, ah)̸=H−h+ 1, observe that:
eQπ
h(zh, sh, ah) =Esh+1∼bTh(·|sh,ah),
oh+1∼bOh+1(·|sh+1)[eVπ
h+1(zh+1, sh+1)] +r(sh, ah) +H·min 
2, C·s
Slog(1/δ1)
max( Nh(sh, ah),1)!
+Esh+1∼bTh(·|sh,ah)H·min 
2, C·s
Olog(1/δ1)
max( Nh+1(sh+1),1)!
≥Esh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)[rh(sh, ah) +eVπ
h+1(zh+1, sh+1)],
and hence, {eQπ
h}h∈[H]satisfies the first condition. Moreover, it holds that:
eQπ
h(zh, sh, ah)≤Esh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)[rh(sh, ah) +eVπ
h+1(zh+1, sh+1)] + 2 H·min 
2, C·s
Slog(1/δ1)
max( Nh(sh, ah),1)!
+ 2·Esh+1∼bTh(·|sh,ah)H·min 
2, C·s
Olog(1/δ1)
max( Nh+1(sh+1),1)!
≤Esh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)[rh(sh, ah) +eVπ
h+1(zh+1, sh+1)] + 6 H·min 
2, C·s
Slog(1/δ1)
max( Nh(sh, ah),1)!
+ 2·Esh+1∼Th(·|sh,ah)H·min 
2, C·s
Olog(1/δ1)
max( Nh+1(sh+1),1)!
.
Thus, it holds that:
eVπ
h(zh, sh)−Vπ
h(zh, sh)
≤E ah∼πh(·|zh),
sh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)[eVπ
h+1(zh+1, sh+1)−Vπ
h+1(zh+1, sh+1)] + 6·C·H·Eah∼πh(·|zh)s
Slog(1/δ1)
max( Nh(sh, ah),1)
+ 2·C·H·Eah∼πh(·|zh),
sh+1∼Th(·|sh,ah)s
Olog(1/δ1)
max( Nh+1(sh+1),1).
Thus, we conclude that
Es1∼µ1[eVπ
1(s1)]−Es1∼µ1[Vπ
1(s1)]≤Eτ=(s1,a1,...,s H+1)∼π
X
h∈[H]8·H·C·s
max( S, O) log(1 /δ1)
max( Nh(sh, ah),1)

= 8·Hp
max( O, S)·log(1/δ1)·C·X
h∈[H]Eτ=(s1,a1,...,s H+1)∼π"s
1
max( Nh(sh, ah),1)#
.
To finish the proof, we make use of the following lemma.
Lemma H.4 (Lemma 6 in [ 48]).For each step h∈[H], and state-action pair (sh, ah)∈ S × A ,
with probability at least 1−δ2:s
1
max( Nh(sh, ah),1)=O r
S·Alog(M/δ 2)
M!
.
39By setting δ2=δ
2·S·A·H, and taking union bound we have that with probability at least 1−δ:
Es1∼µ1[eVπ
1(s1)]−Es1∼µ1[Vπ
1(s1)]
= 8p
max( O, S)·log(1/δ1)·C·X
h∈[H]Eτ=(s1,a1,...,s H+1)∼π"s
1
max( Nh(sh, ah),1)#
≤O 
H2·s
max( O, S)·S·A
M·logS·A
δ
logM·S·A·H
δ!
.
Proof of Theorem 5.2: The proof of Theorem 5.2 follows by combining Theorem H.5 and Theo-
rem H.6 below. Theorem H.5 proves that we can approximately learn a POMDP model Pcomputa-
tionally and sample efficiently, thanks to the privileged information.
Theorem H.5. Fix any ϵ, δ∈(0,1). Algorithm 4 can learn the approximate POMDP model with
transition bT1:Hand emission bO1:Hsuch that with probability at least 1−δ, for any policy π∈Πgen
andh∈[H]
EP
πh
∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+∥Oh(·|sh)−bOh(·|sh)∥1i
≤O(ϵ),
using POLY (S, A, H, O,1
ϵ,log(1
δ))episodes in time POLY (S, A, H, O,1
ϵ,log(1
δ)).
Proof. Note that by Lemma H.11, it suffices to consider only π∈ΠSas the optimal value for
policies in Πgencan be achieved by those in ΠS(by considering rh(sh, ah) :=∥Th(·|sh, ah)−
bTh(·|sh, ah)∥1+∥Oh(·|sh)−bOh(·|sh)∥1). For each h∈[H]andsh∈ S, we define
ph(sh) = max
π∈ΠSdπ
h(sh).
Fixϵ1>0, we define U(h, ϵ1) ={sh∈ S | ph(sh)≥ϵ1}. By the guarantee of the EULER
algorithm from [ 89,37], one can learn a policy Ψ(h, sh)with sample complexity eO(S2AH4
ϵ1)such
thatdΨ(h,sh)
h(sh)≥ph(sh)
2for each sh∈ U(h, ϵ1)with probability 1−δ1. Now we assume this
event holds for any h∈[H]andsh∈ U(h, ϵ1). For each sh∈ S andah∈ A, we have executed
in Algorithm 4 the policy Ψ(h, sh)followed by an action ah∈ A forNepisodes, and denote the
number of episodes that shandahare visited as Nh(sh, ah). Then with probability 1−eNϵ1/8,
Nh(sh, ah)≥Nph(sh)
2by Chernoff bound. Now conditioned on this event, we are ready to evaluate
the following: for any π∈ΠS
1
2·EP
π∥Th(·|sh, ah)−bTh(·|sh, ah)∥1=1
2X
sh,ahdπ
h(sh)πh(ah|sh)∥Th(·|sh, ah)−bTh(·|sh, ah)∥1
≤Sϵ1+1
2X
sh∈U(h,ϵ1),ahdπ
h(sh)πh(ah|sh)s
2Slog(1/δ2)
Nph(sh)
≤Sϵ1+X
sh∈U(h,ϵ1),ahdΨ(h,sh)
h(sh)πh(ah|sh)s
2Slog(1/δ2)
Nph(sh)
≤Sϵ1+X
shq
dΨ(h,sh)
h(sh)r
2Slog(1/δ2)
N
≤Sϵ1+Sr
2 log(1 /δ2)
N,
40where the first inequality follows by [ 13] with probability at least 1−δ2, and the second inequality
usesdΨ(h,sh)
h(sh)≥ph(sh)
2for all sh∈ U(h, ϵ1). Similarly, for any π∈ΠS
1
2·EP
π∥Oh(·|sh)−bOh(·|sh)∥1≤Sϵ1+1
2X
sh∈U(h,ϵ1)dπ
h(sh)s
2Olog(1/δ2)
Nph(sh)
≤Sϵ1+X
sh∈U(h,ϵ1)dΨ(h,sh)
h(sh)s
2Olog(1/δ2)
Nph(sh)≤Sϵ1+X
sh∈U(h,ϵ1)q
dΨ(h,sh)
h(sh)r
2Olog(1/δ2)
N
≤Sϵ1+r
2SOlog(1/δ2)
N,
where similarly the first inequality follows by [ 13] with probability at least 1−δ2, and the second
inequality uses dΨ(h,sh)
h(sh)≥ph(sh)
2for all sh∈ U(h, ϵ1). Therefore, by a union bound, all the
high probability events above hold with probability
1−SHδ 1−SHAe−Nϵ1/8−2SAHδ 2.
Therefore, we can choose N=eΘ(S2+SO
ϵ2)andϵ1= Θ(ϵ
S), leading to the total sample complexity
SHA
N+eΘS3AH4
ϵ
=eΘS2AHO +S3AH
ϵ2+S4A2H5
ϵ
, (H.3)
which completes the proof.
Now with such a model learned in a reward-free way, we are ready to present our main result for
approximate belief learning.
Theorem H.6. Consider a γ-observable POMDP P(c.f. Assumption 2.2), an ϵ >0, approximate
transition and emission {bTh}h∈[H]and{bOh}h∈[H]learned from Algorithm 4 that ensure that for any
π∈Πgenandh∈[H]:
EP
πh
∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+∥Oh(·|sh)−bOh(·|sh)∥1i
≤ Oϵ
H
.
Then we can construct in time POLY (H, A, S, O,1
ϵ,log1
δ)a belief {bapx
h:Zh→∆(S)}h∈[H]with
no further samples. In addition, if the parameter in Algorithm 4 satisfies N=eΘ(Olog(SH/δ )
γ2ϵ1)and
our class of finite memory policies ΠLsatisfies L≥eΩ(γ−4log(S/ϵ), then for any π∈Πgenand
h∈[H]:
EP
π∥bh(τh)−bapx
h(zh)∥1≤ O(ϵ).
Proof. We firstly consider the following simple while important fact: for the estimated emission bOh,
its observability can be evaluated as
∥bO⊤
h(b−b′)∥1≥ ∥O⊤
h(b−b′)∥1− ∥(O⊤
h−bO⊤
h)(b−b′)∥1≥(γ− ∥bOh−Oh∥∞)∥b−b′∥1,
for any b, b′∈∆(S)and∥bOh−Oh∥∞:= max sh∈S∥Oh(·|sh)−bOh(·|sh)∥1. Therefore, if one
can ensure that the emission at anystate shis learned accurately in the sense that ∥Oh(·|sh)−
bOh(·|sh)∥1≤γ
2, we can conclude that bOhis also γ/2-observable. However, the key challenge here
is that there could exist some states shthat can only be visited with a low probability no matter what
exploration policy is used . Therefore, emissions at such states may not be learned accurately. To
address this issue, our key technique is to redirect the transition probability into states that cannot be
explored sufficiently to some highly visited states, in a new POMDP that is close to the original one
in generating the beliefs.
Specifically, first, for any ϵ1>0, we define
Slow
h:=
sh∈ SNh(sh)
N<ϵ1
2
,Shigh
h:=S \ Slow
h.
41By Chernoff bound, with probability at least 1−Se−Nϵ1/8, it holds that
Slow
h⊆ {sh∈ S | ph(sh)< ϵ1},
where ph(sh) := max π∈ΠSdπ
h(sh). To see the reason, we notice that for any shsuch that ph(sh)≥
ϵ1, with probability 1−e−Nϵ1/8, it holds thatNh(sh)
N≥ϵ1
2. Therefore, the last step is by taking a
union bound for all sh. Now with Slow
hdefined, we are ready to construct a truncated POMDP Ptrunc
such that for each h∈[H], we define the transition as
Ttrunc
h(sh+1|sh, ah) :=Th(sh+1|sh, ah) +P
s′
h+1∈Slow
h+1Th(s′
h+1|sh, ah)
|Shigh
h+1|,∀sh∈ S, sh+1∈ Shigh
h+1, ah∈ A,
Ttrunc
h(sh+1|sh, ah) := 0 ,∀sh∈ S, sh+1∈ Slow
h+1, ah∈ A.
Meanwhile, for the initial state distribution, we define
µtrunc
1(s1) :=µ1(s1) +P
s′
1∈Slow
1µ1(s′
1)
|Shigh
1|,∀s1∈ Shigh
1,
µtrunc
1(s1) := 0 ,∀s1∈ Slow
1.
For emission, we simply define
Otrunc
h(oh|sh) :=Oh(oh|sh),∀h∈[H], sh∈ S, oh∈ Oh.
Finally, we define the rewards for Ptruncarbitrarily. Now we examine the total variation distance
between the trajectory distributions in PandPtrunc. For any policy π∈Πgen, it is easy to see that
Pπ,P(τh)≤Pπ,Ptrunc(τh),
for any τh∈Thigh
h:={(s1:h, o1:h, a1:h−1)|s′
h∈ Shigh
h′,∀h′∈[h]}, since some rarely visited states’
probability has been redirected to the highly visited ones in Ptrunc. Meanwhile, it holds by a union
bound that for any h∈[H]
Pπ,P(τh̸∈Thigh
h)≤X
h′∈[h]Pπ,P(sh′∈ Slow
h′)≤HSϵ 1.
Therefore, by noticing that Pπ,Ptrunc(τh) = 0 for any τh̸∈Thigh
handh∈[H], the total variation
distance between the trajectory distributions in PandPtrunccan be bounded by
X
τh|Pπ,P(τh)−Pπ,Ptrunc(τh)| ≤2HSϵ 1. (H.4)
On the other hand, by Equation (H.9) of Lemma H.9, we have
EP
π[∥bh(τh)−btrunc
h(τh)∥1]≤2X
τh|Pπ,P(τh)−Pπ,Ptrunc(τh)| ≤4HSϵ 1. (H.5)
With such an intermediate quantity Ptrunc, we define the transition of its approximate version bPtrunc
as follows: we define the transition as
bTtrunc
h(sh+1|sh, ah) :=bTh(sh+1|sh, ah) +P
s′
h+1∈Slow
h+1bTh(s′
h+1|sh, ah)
|Shigh
h+1|,∀sh∈ S, sh+1∈ Shigh
h+1, ah∈ A,
bTtrunc
h(sh+1|sh, ah) := 0 ,∀sh∈ S, sh+1∈ Slow
h+1, ah∈ A.
Meanwhile, for the initial state distribution, we define
bµtrunc
1(s1) :=bµ1(s1) +P
s′
1∈Slow
1bµ1(s′
1)
|Shigh
1|,∀s1∈ Shigh
1,
bµtrunc
1(s1) := 0 ,∀s1∈ Slow
1.
42For emission, we define
bOtrunc
h(oh|sh) :=bOh(oh|sh),∀h∈[H], sh∈ S, oh∈ Oh.
Now we define bOsub
h∈R|Shigh
h|×Oto be the sub-matrix of bOtrunc
h, where we only keep those rows
ofbOtrunc
hthat correspond to the states in Shigh
h. Similarly, we define Osub
h∈R|Shigh
h|×Oto be the
sub-matrix of Oh, where we only keep those rows of Ohthat correspond to the states in Shigh
h. It is
direct to see that Osubis still γ-observable. Meanwhile, we notice that
∥bOsub
h−Osub
h∥∞= max
sh∈Shigh
h∥Oh(·|sh)−bOh(·|sh)∥1≤max
sh∈Shigh
hs
Olog(SH/δ )
Nh(sh)≤max
sh∈Shigh
hs
2Olog(SH/δ )
Nϵ1,
where the first inequality is by Lemma J.9, and the second inequality is by the definition of Shigh
h.
Therefore, if we take
N≥8Olog(SH/δ )
γ2ϵ1,
it is guaranteed that ∥bOsub
h−Osub
h∥∞≤γ
2. Therefore, we conclude that bOsub
his also γ/2-observable.
Now we are ready to examine bb′,trunc
h. We firstly define the following POMDP bPsub, which essentially
deletes all states in Slow
hfrom the state space of bPtruncat each step h, which does not affect the
trajectory distribution as they were not reachable in bPtrunc. Notice that the emission of bPsubis exactly
bOsub
h, implying that bPsubis aγ/2-observable POMDP. Therefore, for policy class with finite memory
ΠLwithL≥eΩ(γ−4log(S/ϵ), by Theorem 4.1 in [25], it is guaranteed that for any π∈Π,
EbPsub
π∥bbsub
h(τh)−bb′,sub
h(zh)∥1≤ϵ,
wherebbsub
h(τh),bb′,sub
h(zh)∈∆(Shigh
h). Now we claim that
EbPtrunc
π∥bbtrunc
h(τh)−bapx
h(zh)∥1≤ϵ, (H.6)
where we define bapx
h(zh)∈∆(S)byaugmenting bb′,sub
h(zh)with 0for states from Slow
h. To
see the reason, we notice that simulating bPtruncis exactly equivalent to simulating bPsub, and that
bbtrunc
h(τh)(sh) = 0 forsh∈ Slow
h,bbtrunc
h(τh)(sh) =bbsub
h(τh)(sh)forsh∈ Shigh
h.
For the total variation distance between the trajectory distributions in PtruncandbPtrunc, it holds that
by Lemma H.9
X
τH|Pπ,Ptrunc(τH)−Pπ,bPtrunc(τH)|
≤EPtrunc
π
X
h∈[H]∥Ttrunc
h(·|sh, ah)−bTtrunc
h(·|sh, ah)∥1+∥Otrunc
h(·|sh)−bOtrunc
h(·|sh)∥1

≤X
h∈[H]EPtrunc
πh
∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+∥Oh(·|sh)−bOh(·|sh)∥1i
,
where the last step is by Lemma H.7.
Now by Equation (H.4), we have
X
hEPtrunc
πh
∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+∥Oh(·|sh)−bOh(·|sh)∥1i
=X
h8HSϵ 1+EP
πh
∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+∥Oh(·|sh)−bOh(·|sh)∥1i
≤ϵ
3+ 8H2Sϵ1,
43where the last step is by Theorem H.5. Hence, by Lemma H.9, it holds that
∥Pπ,Ptrunc−Pπ,bPtrunc∥1≤ϵ
3+ 8H2Sϵ1, (H.7)
EPtrunc
π∥btrunc
h(τh)−bbtrunc
h(τh)∥1≤2ϵ
3+ 16H2Sϵ1. (H.8)
Finally, we are ready to prove
EP
π∥bh(τh)−bapx
h(zh)∥1
≤EP
π∥bh(τh)−btrunc
h(τh)∥1+EP
π∥btrunc
h(τh)−bbtrunc
h(τh)∥1+EP
π∥bbtrunc
h(τh)−bapx
h(zh)∥1
≤EP
π∥bh(τh)−btrunc
h(τh)∥1+EPtrunc
π∥btrunc
h(τh)−bbtrunc
h(τh)∥1+EbPtrunc
π∥bbtrunc
h(τh)−bapx
h(zh)∥1
+ 4∥Pπ,P−Pπ,Ptrunc∥1+ 2∥Pπ,Ptrunc−Pπ,bPtrunc∥1
≤ O(H2Sϵ1) +O(ϵ).
Therefore, by setting ϵ1= Θ ϵ
H2S
, we prove our lemma. Observe that our algorithm needed no
further samples. The computational complexity follows by computing belief bapx
hon POMDP bPsub
using finite-memory policies of size eΘ(γ−4log(S/ϵ)). For the final sample complexity, we only
need to ensure N=eΘ(Olog(SH/δ )
γ2ϵ1)in Equation (H.3), thus concluding our proof.
We conclude the proof of Theorem 5.2 by combining Theorem H.5 and Theorem H.6. ■
H.1 Supporting Technical Lemmas
In the following, we provide some technical lemmas and their proofs.
Lemma H.7. Fixn > 0and an index set S⊆[n]. For two sequences x1:nandy1:nsuch that
xi, yi∈[0,1]fori∈[n]andP
ixi= 1,P
iyi= 1, we define
bxi=xi+P
j∈Sxj
n− |S|,∀i̸∈S; bxi= 0,∀i∈S.
We define by1:nsimilarly. Then, it holds that
X
i|xi−yi| ≥X
i|bxi−byi|.
Proof. Note that
X
i|bxi−byi|=X
i̸∈S|bxi−byi|=X
i̸∈Sxi+P
j∈Sxj
n− |S|−yi−P
j∈Syj
n− |S|
≤(n− |S|)P
j∈Sxj
n− |S|−P
j∈Syj
n− |S|+X
i̸∈S|xi−yi|
≤X
i|xi−yi|,
which completes the proof.
Lemma H.8. Fix two finite sets X,Yand two joint distributions P1, P2∈∆(X × Y ). It holds that
−EP1(x)X
y|P1(y|x)−P2(y|x)| ≤X
x,y|P1(x, y)−P2(x, y)| −X
x|P1(x)−P2(x)|
≤EP1(x)X
y|P1(y|x)−P2(y|x)|.
44Proof. For the second inequality, it holds that
X
x,y|P1(x, y)−P2(x, y)|=X
x,y|P1(x, y)−P1(x)P2(y|x) +P1(x)P2(y|x)−P2(x, y)|
≤X
x,y|P1(x, y)−P1(x)P2(y|x)|+|P1(x)P2(y|x)−P2(x, y)|
=X
x,y|P1(x)(P1(y|x)−P2(y|x))|+|P2(y|x)(P1(x)−P2(x))|
=EP1(x)X
y|P1(y|x)−P2(y|x)|+X
x|P1(x)−P2(x)|.
Meanwhile, for the first inequality, it holds that
X
x,y|P1(x, y)−P2(x, y)|=X
x,y|P1(x, y)−P1(x)P2(y|x) +P1(x)P2(y|x)−P2(x, y)|
≥X
x,y−|P1(x, y)−P1(x)P2(y|x)|+|P1(x)P2(y|x)−P2(x, y)|
=X
x,y−|P1(x)(P1(y|x)−P2(y|x))|+|P2(y|x)(P1(x)−P2(x))|
=EP1(x)−X
y|P1(y|x)−P2(y|x)|+X
x|P1(x)−P2(x)|,
concluding our lemma.
Lemma H.9. Consider any two POMDP instances PandbPand define the belief functions as b1:H
andbb1:H, respectively (see Appendix C for the definition of belief functions). It holds that for any
π∈Πgen
Pπ,P−Pπ,bP
1=X
τH|Pπ,P(τH)−Pπ,bP(τH)|
≤ ∥µ1−bµ1∥1+EP
πX
h∈[H−1]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1
+EP
πX
h∈[H]∥Oh(·|sh)−bOh(·|sh)∥1,
EP
π∥bh(τh)−bbh(τh)∥1≤2∥µ1−bµ1∥1+ 2EP
πX
h∈[H−1]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1
+ 2EP
πX
h∈[H]∥Oh(·|sh)−bOh(·|sh)∥1,
where we remind readers that we denote by τH= (s1:H, o1:H, a1:H−1)the trajectory with states
from an episode of the POMDP.
Proof. The first inequality also appears in [ 44] and we provide a simplified proof here for complete-
ness. By Lemma H.8, it holds that
X
τH|Pπ,P(τH)−Pπ,bP(τH)|
≤X
τH−1|Pπ,P(τH−1)−Pπ,bP(τH−1)|+EP
πX
aH−1,sH,oHπH−1(aH−1|τH−1)TH−1(sH|sH−1, aH−1)OH(oH|sH)
−πH−1(aH−1|τH−1)bTH−1(sH|sH−1, aH−1)bOH(oH|sH)
≤X
τH−1|Pπ,P(τH−1)−Pπ,bP(τH−1)|+EP
πh
∥TH−1(·|sH−1, aH−1)−bTH−1(·|sH−1, aH−1)∥1+∥OH(·|sH)−bOH(·|sH)∥1i
,
45where the last step is again from Lemma H.8. Therefore, by repeatedly unrolling the inequality, we
proved the first result.
For the second result, we notice that by Lemma H.8, it holds that
X
τh,sh|Pπ,P
h(τh, sh)−Pπ,bP
h(τh, sh)| ≥ −X
τh|Pπ,P
h(τh)−Pπ,bP
h(τh)|+EP
πX
sh|Pπ,P
h(sh|τh)−Pπ,bP
h(sh|τh)|.
Notice the fact that Pπ,P
h(sh|τh)does not depend on πsince it is exactly the belief bh(τh)(sh), we
conclude that
EP
π∥bh(τh)−bbh(τh)∥1≤X
τh,sh|Pπ,P
h(τh, sh)−Pπ,bP
h(τh, sh)|+X
τh|Pπ,P
h(τh)−Pπ,bP
h(τh)|
≤2X
τH|Pπ,P(τH)−Pπ,bP(τH)|, (H.9)
where the last step comes from the fact that after marginalization, the total variation distance will not
increase. By combining it with the first result, we proved the second result.
Corollary H.10. Consider any two POSG instances G,bGthat satisfy Assumption C.8 and the
corresponding belief functions in the form of PG,PbG:Ch→∆(Ph×S)for any h∈[H], respectively.
It holds that for any π∈Πgen
EG
π∥PG(·,·|ch)−PbG(·,·|ch)∥1
≤2∥µ1−bµ1∥1+ 2EG
πX
h∈[H−1]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+ 2EG
πX
h∈[H]∥Oh(·|sh)−bOh(·|sh)∥1.
Proof. For the second result, we notice that by Lemma H.8, it holds that
X
ch,ph,sh|Pπ,G
h(ch, ph, sh)−Pπ,bG
h(ch, ph, sh)| ≥ −X
ch|Pπ,P
h(ch)−Pπ,bG
h(ch)|
+EG
πX
sh,ph|Pπ,G
h(sh, ph|ch)−Pπ,bG
h(sh, ph|ch)|.
Notice the fact that Pπ,G
h(sh, ph|ch),Pπ,bG
h(sh, ph|ch)do not depend on πdue to Assumption C.8,
we conclude that
EP
π∥PG(·,·|ch)−PbG(·,·|ch)∥1≤X
ch,phsh|Pπ,G
h(ch, ph, sh)−Pπ,bG
h(ch, ph, sh)|+X
ch|Pπ,G
h(ch)−Pπ,bG
h(ch)|
≤2X
τH|Pπ,G(τH)−Pπ,bG(τH)|, (H.10)
where the last step again comes from the fact that after marginalization, the total variation distance
will not increase. By combining it with Lemma H.9, we proved the second result.
Lemma H.11. For any reward function r1:HofPwithrh:S × A → Rfor any h∈[H], it holds
that
max
π∈ΠgenvP(π)≤max
π∈ΠSvP(π).
Proof. Denote π⋆∈ΠSto be the optimal policy obtained by running value iteration only on the state
spaceSforP. Now we are ready to prove the following argument for any π∈Πgeninductively:
Qπ⋆,P
h(sh, ah)≥Qπ,P
h(s1:h, o1:h, a1:h).
46Asymmetric
optimistic NPGExpert policy
distillationAsymmetric
Q-learningVanilla AAC
Deterministic
POMDPCase 1 3.32±0.66 3.33±0.62 3.33±0.62 3.33±0.62 3.04±0.58 3.25±0.65
Case 2 7.1±0.48 7.26±0.68 7.26±0.68 7.26±0.68 6.15±0.85 6.41±0.91
Case 3 3.04±0.23 3.25±0.33 3.25±0.33 3.25±0.33 3.09±0.39 3.1±0.38
Case 4 6.51±0.6 6.54±0.58 6.54±0.58 6.54±0.58 6.16±0.48 5.87±0.58
Block MDPCase 1 3.31±0.46 3.37±0.41 3.37±0.41 3.37±0.41 3.03±0.4 3.08±0.43
Case 2 6.36±0.52 6.67±0.54 6.67±0.54 6.67±0.54 5.74±0.43 5.7±0.46
Case 3 3.2±0.26 3.37±0.22 3.37±0.22 3.37±0.22 3.14±0.31 2.97±0.32
Case 4 6.01±0.32 6.44±0.36 6.44±0.36 6.44±0.36 5.58±0.32 5.33±0.19
Table 2: Rewards of different approaches for POMDPs under the deterministic filter condition.
It is easy to see the argument holds for h=H. Fix state-action pair (sh, ah)∈ S × A and trajectory
(s1:h−1, o1:h, a1:h−1), we note that:
Qπ⋆,P
h(sh, ah) =rh(sh, ah) +Esh+1∼Th(·|sh,ah)
max
ah+1Qπ⋆,P
h+1(sh+1, ah+1)
≥rh(sh, ah) +Esh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)
max
ah+1Qπ,P
h+1(s1:h+1, o1:h+1, a1:h+1)
≥rh(sh, ah) +Esh+1∼Th(·|sh,ah),
oh+1∼Oh+1(·|sh+1)h
Eah+1∼πh+1(·|s1:h+1,o1:h+1,a1:h)Qπ,P
h+1(s1:h+1, o1:h+1, a1:h+1)i
=Qπ,P
h(s1:h, o1:h, a1:h),
where the first inequality comes from the inductive hypothesis.
I Missing Details in Section 6
POMDP under deterministic filter condition. We first evaluate our algorithms on POMDPs with
certain structures, i.e., the deterministic conditions. In particular, we generate POMDPs, where either
the transition dynamics are deterministic or the emission ensures decodability. We test three of our
approaches, expert policy distillation, asymmetric optimistic natural policy gradient. We summarize
our results in Table 2, where the four cases corresponds to POMDPs with (S=A= 2, O= 3, H=
5),(S=A= 2, O= 3, H= 10) ,(S= 3, A= 2, O= 4, H= 5) ,(S= 3, A= 2, O= 4, H= 5) ,
and we can see that our approach based on expert distillation outperforms all the other methods, which
is consistent with the fact that such methods have exploited the special structures of the POMDPs
achieving both polynomial sample and computational complexity.
General POMDPs. Here we also evaluate our methods for general randomly generated POMDPs
without any structures. Hence, we compare the baselines with asymmetric optimistic natural policy
gradient and asymmetric optimistic value iteration (i.e., the single-agent version of Algorithm 5). In
Figure 2, we show the performance of different algorithms in POMDPs of different size, where the
four cases corresponds to POMDPs with (S=A=O= 2, H= 5) ,(S=A=O= 2, H= 10) ,
(S=O= 3, A= 2, H= 10) ,(S=A= 3, O= 2, H= 10) , and our approaches achieves the
highest rewards with small number of episodes.
Implementation details. For each problem setting, we generated 20POMDPs randomly and report
the average performance and its standard deviation for each algorithms. For our algorithms based on
privileged value learning methods, we find that using a finite memory of 3already provides strong
performance. For our algorithms based privileged policy learning, we instantiate the MDP learning
algorithm with the fully observable optimistic natural policy gradient algorithm [ 74]. Meanwhile, for
both the decoder learning and belief learning, we find that just utilizing all the historic trajectories
gives us reasonable performance without additional samples. For baselines, the hyperparameters α
forQ-value update and step size for the policy update are tuned by grid search, where αcontrols
the update of temporal difference learning (recall the update rule of temporal difference learning as
47Q←(1−α)Q+αQtarget). For asymmetric Q-learning, we use ϵ-greedy exploration, where we use
the seminal decreasing rate ϵt=H+1
H+t. Finally, all simulations are conducted on a personal laptop
with Apple M1 CPU and 16 GB memory.
Empirical insights and interpretation of the experimental results. To understand intuitively
why our approach outperforms those baseline algorithms, we notice the key difference in the value
and policy update style between our approaches and vanilla asymmetric actor critic and asymmetric
Q-learning. The baselines often roll-out the polices, collect trajectories, and only update the value
and the policies on the states/history the trajectories have visited. Therefore, ideally, to learn a good
policy for baselines, the number of trajectories to collect is at least as large as the history size, which
is indeed exponential in the horizon H. This is known as curse of history for partially observable
RL. In contrast, in our algorithms, we explicitly estimate the empirical transition and emissions,
which is indeed of polynomial size. Thus, the sample complexity avoids suffering from the potential
exponential dependency of horizon or the length of the finite memory. Finally, we acknowledge that
the baselines are developed to handle complex, high-dimensional deep RL problems, while scaling
our methods to deep RL benchmarks requires non-trivial engineering efforts.
J Missing Details in Section 7
Proof of Proposition 7.1:
Given the condition of Definition 3.2, the function ϕhthat satisfies the condition of Proposition 7.1
can be constructed recursively as follows for any reachable τh
ϕh(τh) :=ψh(ϕh−1(τh−1), ah−1, oh),
andϕ1(o1) :=ψ1(o1). Therefore, we can prove by induction that by belief update rule
PP(sh|τh) =Uh(bϕh−1(τh−1);ah−1, oh) =bψh(ϕh−1(τh−1),ah−1,oh),
where the last step is by the definition of our ϕh. Therefore, we have PP(sh=
ψh(ϕh−1(τh−1), ah−1, oh)|τh) = 1 .
For the other direction, it is similar to the proof of Lemma C.1 in [ 19] form-step decodable POMDP.
Here we prove it for completeness. For any reachable trajectory τh∈ Th, it holds by the belief update
and induction that
PP(sh|τh) =Uh(bϕh−1(τh−1), ah−1, oh) =PP(sh|sh−1=ϕh−1(τh−1), ah−1, oh).
Meanwhile, since we know PP(·|τh)is a one-hot vector, we can construct ψhsuch that
ψh(sh−1, ah−1, oh)is the unique shthat makes PP(sh|τh)>0withsh−1=ϕh−1(τh−1). If
this procedure does not complete the definition of ψfor some (sh−1, ah−1, oh), it implies that either
sh−1is not reachable or ohis not reachable given sh−1, i.e.,PP(oh|sh−1, a′
h−1) = 0 for any
a′
h−1∈ A, thus recovering the conditions of Definition 3.2. ■
Proof of Proposition 7.3:
Note that for any given problem instance of a POMDP, we can construct a POSG by adding a dummy
agent that has the observation being the exact state at each time step, and has only one dummy action
that does not affect the transition or reward. Therefore, even the local private information pi,hof
the dummy agent can decode the underlying state, and hence (ch, ph)reveals the underlying state.
Therefore, the corresponding POSG with the dummy agent satisfies the condition in Proposition 7.3.
Meanwhile, it is direct to see that any CCE of the POSG is an optimal policy for the original POMDP.
Now by the PSPACE-hardness of planning for POMDPs [66], we proved our proposition. ■
Theorem J.1. Suppose the POSG Gsatisfies Definition 7.2. For any π∈ΠSand (potentially
stochastic) decoding functions bg={bgi,h}i∈[n],h∈[H]withbgi,h:Ch× P i,h→∆(S)for each
i∈[n], h∈[H], it holds that
NE/CCE-gap( πbg)−NE/CCE-gap( π)≤2nH2max
i∈[n]max
ui∈Πimax
j∈[n],h∈[H]Pui×π−i,G(sh̸=bgj,h(ch, pj,h))
CE-gap( πbg)−CE-gap( π)≤2nH2max
i∈[n]max
mi∈Mimax
j∈[n],h∈[H]P(mi⋄πi)⊙π−i,G(sh̸=bgj,h(ch, pj,h))
48where πbgis the distilled policy of πthrough the decoding functions bg, where at step h, each agent
ifirstly individually decodes the state by sampling sh∼bgi,h(·|ch, pi,h), and then acts according
to the expert πi,h, where in the following discussions, we slightly abuse the notation and regard
bgi,h(ch, pi,h)as a random variable following the distribution of bgi,h(·|ch, pi,h). In other words, the
decoding process does not need correlations among the agents.
Proof. For notational simplicity, we write viinstead of vG
iwhen the underlying model is clear from
the context. Firstly, we consider any deterministic decoding function bϕ={bϕi,h}i∈[n],h∈[H]with
bϕi,h:Ch× Pi,h→ S for each i∈[n], h∈[H], and note the following that for any i∈[n],
vi(π)−vi(πbϕ) =EG
π[R]−EG
πbϕ[R]
=EG
π[R 1[∀j∈[n], h∈[H] :sh=bϕj,h(ch, pj,h)]]−EG
πbϕ[R 1[∀j∈[n], h∈[H] :sh=bϕj,h(ch, pj,h)]]
+EG
π[R 1[∃j∈[n], h∈[H] :sh̸=bϕj,h(ch, pj,h)]]−EG
πbϕ[R 1[∃j∈[n], h∈[H] :sh̸=bϕj,h(ch, pj,h)]]
=EG
π[R 1[∃j∈[n], h∈[H] :sh̸=bϕj,h(ch, pj,h)]]−EG
πbϕ[R 1[∃j∈[n], h∈[H] :sh̸=bϕj,h(ch, pj,h)]],
where we define R:=P
hri,h(sh, ah), and the last step is by the definition of πbϕ
vi(π)−vi(πbϕ)≤HPπ,G(∃j∈[n], h∈[H] :sh̸=bϕj,h(ch, pj,h)).
Therefore, by noticing the fact that bgis equivalent to a mixture of deterministic decoding functions,
where at the beginning of each episode, one can firstly independently sample the outcome for each
(ch, pj,h)forj∈[n]andh∈[H], we conclude that
vi(π)−vi(πbg) =vi(π)−Ebϕ∼bgvi(πbϕ)≤HEbϕ∼bgPπ,G(∃j∈[n], h∈[H] :sh̸=bϕj,h(ch, pj,h))
=HPπ,G(∃j∈[n], h∈[H] :sh̸=bgj,h(ch, pj,h)).
Now we prove our result for NE and CCE first. Due to similar arguments for evaluating vi(π)−vi(πbϕ),
for any ui∈Πi∪ΠS,i, it holds that
vi(ui×πbϕ
−i)−vi(ui×π−i)≤EG
ui×πbϕ
−i[R 1[∃j∈[n]\ {i}, h∈[H] :sh̸=bϕj,h(ch, pj,h)]]
≤HPui×πbϕ
−i,G(∃j∈[n]\ {i}, h∈[H] :sh̸=bϕj,h(ch, pj,h)).
We notice the following fact that
Pui×π−i,G(∀j∈[n]\ {i}, h∈[H] :sh=bϕj,h(ch, pj,h)) =X
¯τH∈TH(bϕ)Pui×π−i,G(¯τH),
where we define TH(bϕ) :={τH∈TH|∀j∈[n]\ {i}, h∈[H] :sh=bϕj,h(ch, pj,h)}. By
definition of πbϕ, it holds that
∀τH∈TH(bϕ) :Pui×π−i,G(τH) =Pui×πbϕ
−i,G(τH).
Therefore, we have
Pui×π−i,G(∀j∈[n]\ {i}, h∈[H] :sh=bϕj,h(ch, pj,h)) =Pui×πbϕ
−i,G(∀j∈[n]\ {i}, h∈[H] :sh=bϕj,h(ch, pj,h)).
Correspondingly, it holds that
Pui×π−i,G(∃j∈[n]\ {i}, h∈[H] :sh̸=bϕj,h(ch, pj,h)) =Pui×πbϕ
−i,G(∃j∈[n]\ {i}, h∈[H] :sh̸=bϕj,h(ch, pj,h)),
which implies that
vi(ui×πbϕ
−i)−vi(ui×π−i)≤HPui×πbϕ
−i,G(∃j∈[n]\ {i}, h∈[H] :sh̸=bϕj,h(ch, pj,h))
=HPui×π−i,G(∃j∈[n]\ {i}, h∈[H] :sh̸=bϕj,h(ch, pj,h)).
49Again by the fact that bgis equivalent to a mixture of deterministic decoding functions, it holds
vi(ui×πbg
−i)−vi(ui×π−i) =Ebϕ∼bgvi(ui×πbϕ
−i)−vi(ui×π−i)
≤HEbϕ∼bgPui×π−i,G(∃j∈[n]\ {i}, h∈[H] :sh̸=bϕj,h(ch, pj,h))
=HPui×π−i,G(∃j∈[n]\ {i}, h∈[H] :sh̸=bgj,h(ch, pj,h)).
(J.1)
Now we are ready to evaluate the NE/CCE-gap of policy πbgas follows:
NE/CCE-gap( πbg)−NE/CCE-gap( π)
≤max
i∈[n]
max
ui∈Πivi(ui×πbg
−i)−max
ui∈ΠS,ivi(ui×π−i)
+HPπ,G(∃j∈[n], h∈[H] :sh̸=bgj,h(ch, pj,h)).
Now we notice that max ui∈ΠS,ivi(ui×π−i) = max ui∈Πivi(ui×π−i)since ΠS,i⊆Πiby the
deterministic filter condition Definition 3.2 and π−iis a Markov policy. Therefore, we conclude that
NE/CCE-gap( πbg)−NE/CCE-gap( π)
≤max
i∈[n]
max
ui∈Πivi(ui×πbg
−i)−max
ui∈Πivi(ui×π−i)
+HPπ,G(∃j∈[n], h∈[H] :sh̸=bgj,h(ch, pj,h))
≤max
i∈[n]
max
ui∈Πi
vi(ui×πbg
−i)−vi(ui×π−i)
+HPπ,G(∃j∈[n], h∈[H] :sh̸=bgj,h(ch, pj,h))
≤max
i∈[n]
max
ui∈ΠiHPui×π−i,G(∃j∈[n]\ {i}, h∈[H] :sh̸=bgj,h(ch, pj,h))
+HPπ,G(∃j∈[n], h∈[H] :sh̸=bgj,h(ch, pj,h))
≤2H max
i∈[n],ui∈ΠiX
j∈[n]X
hPui×π−i,G(sh̸=bgj,h(ch, pj,h))
≤2nH2max
i∈[n],ui∈Πi,j∈[n],h∈[H]Pui×π−i,G(sh̸=bgj,h(ch, pj,h)),
where the second last step is by a union bound, thus proving our result for NE and CCE.
For CE, consider any strategy modification mi∈ M i∪ MS,i, it holds that
CE-gap( πbg)−CE-gap( π)
≤max
mi∈Mivi((mi⋄πbg
i)⊙πbg
−i)−max
mi∈MS,ivi((mi⋄πi)⊙π−i) +HPπ,G(∃j∈[n], h∈[H] :sh̸=bgj,h(ch, pj,h)).
Now we notice that max mi∈MS,ivi((mi⋄πi)⊙π−i) = max mi∈MS,ivi((mi⋄πi)⊙π−i)since
MS,i⊆ M iby the deterministic filter condition Definition 7.2 and Lemma J.2. Therefore, we
conclude that
CE-gap( πbg)−CE-gap( π)
≤max
i∈[n]max
mi∈Mivi((mi⋄πbg
i)⊙πbg
−i)−max
mi∈Mivi((mi⋄πi)⊙π−i) +HPπ,G(∃j∈[n], h∈[H] :sh̸=bgj,h(ch, pj,h))
≤max
i∈[n]max
mi∈Mi
vi((mi⋄πbg
i)⊙πbg
−i)−vi((mi⋄πi)⊙π−i)
+HPπ,G(∃j∈[n], h∈[H] :sh̸=bgj,h(ch, pj,h))
≤max
i∈[n]max
mi∈MiHP(mi⋄πi)⊙π−i,G(∃j∈[n]\ {i}, h∈[H] :sh̸=bgj,h(ch, pj,h))
+HPπ,G(∃j∈[n], h∈[H] :sh̸=bgj,h(ch, pj,h))
≤2H max
i∈[n],mi∈MiX
j∈[n]X
hP(mi⋄πi)⊙π−i,G(sh̸=bgj,h(ch, pj,h))
≤2nH2max
i∈[n],mi∈Mi,h∈[H],j∈[n]P(mi⋄πi)⊙π−i,G(sh̸=bgj,h(ch, pj,h))
where the third step is due to the same reason as Equation (J.1).
Lemma J.2. For any π∈ΠS, it holds for any reward function and i∈[n]that
max
mi∈Mgen
ivi((mi⋄πi)⊙π−i) = max
mi∈MS,ivi((mi⋄πi)⊙π−i).
50Proof. Denote m⋆
i∈argmaxmi∈Mgen
ivi((mi⋄πi)⊙π−i)andbm⋆
i∈argmaxmi∈MS,ivi((mi⋄
πi)⊙π−i). Now we shall prove that V(m⋆
i⋄πi)⊙π−i,G
i,h(s1:h, o1:h, a1:h−1)≤V(bm⋆
i⋄πi)⊙π−i,G
i,h(sh)
inductively for each h∈[H]. Note that it holds for h=H+ 1. Now we consider the following
V(m⋆
i⋄πi)⊙π−i,G
i,h(s1:h, o1:h, a1:h−1)
=E ah∼πh(·|sh)
sh+1∼Th(·|sh,m⋆
i,h(s1:h,o1:h,a1:h−1,ai,h),a−i,h)
oh+1∼Oh+1(·|sh+1)(
rh(sh, m⋆
i,h(s1:h, o1:h, a1:h−1, ai,h), a−i,h)
+V(m⋆
i⋄πi)⊙π−i,G
i,h+1(s1:h+1, o1:h+1, a1:h−1, m⋆
i,h(s1:h, o1:h, a1:h−1, ai,h), a−i,h))
≤E ah∼πh(·|sh)
sh+1∼Th(·|sh,m⋆
i,h(s1:h,o1:h,a1:h−1,ai,h),a−i,h)
oh+1∼Oh+1(·|sh+1)
rh(sh, m⋆
i,h(s1:h, o1:h, a1:h−1, ai,h), a−i,h) +V(bm⋆
i⋄πi)⊙π−i,G
i,h+1(sh+1)
≤E ah∼πh(·|sh)
sh+1∼Th(·|sh,bm⋆
i,h(sh,ai,h),a−i,h)
oh+1∼Oh+1(·|sh+1)h
rh(sh,bm⋆
i,h(sh, ai,h), a−i,h) +V(bm⋆
i⋄πi)⊙π−i,G
i,h+1(sh+1)i
=V(bm⋆
i⋄πi)⊙π−i,G
i,h(sh),
where the second inequality follows from the inductive hypothesis and the third step is by the
definition of bm⋆
i∈argmaxmi∈MS,ivi((mi⋄πi)⊙π−i).
Lemma J.3. Given an approximate POSG bGthat satisfies Assumption C.8 with approximate tran-
sitions and emissions being {bTh,bOh}h∈[H], we define the approximate decoding function bgto
be
bgi,h(sh|ch, pi,h) :=PbG(sh|ch, pi,h),
for each h∈[H],sh∈ S,ch∈ Ch,pi,h∈ Pi,h. Then it holds that for any π∈ΠS,
max
i∈[n],ui∈Πi,j∈[n],h∈[H]Pui×π−i,G(sh̸=bgj,h(ch, pj,h))
≤ max
i∈[n],ui∈ΠS,iEG
ui×π−i
X
h∈[H−1]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+X
h∈[H]∥Oh(·|sh)−bOh(·|sh)∥1
,
and
max
i∈[n],mi∈Mi,j∈[n],h∈[H]P(mi⋄πi)⊙π−i,G(sh̸=bgj,h(ch, pj,h))
≤ max
i∈[n],mi∈MS,iEG
(mi⋄πi)⊙π−i
X
h∈[H]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+X
h∈[H−1]∥Oh(·|sh)−bOh(·|sh)∥1
.
Proof. Note for any i∈[n],ui∈Πi,j∈[n],h∈[H], it holds
Pui×π−i,G(sh̸=bgj,h(ch, pj,h)) =1
2EG
ui×π−iX
shPG(sh|ch, pj,h)−PbG(sh|ch, pj,h),
51due to the condition in Definition 7.2. Meanwhile,
1
2EG
ui×π−iX
shPG(sh|ch, pj,h)−PbG(sh|ch, pj,h)
≤X
sh,ch,pj,hPui×π−i,G(sh, ch, pj,h)−Pui×π−i,bG(sh, ch, pj,h)
≤X
τhPui×π−i,G(τh)−Pui×π−i,bG(τh)
≤X
τHPui×π−i,G(τH)−Pui×π−i,bG(τH)
≤EG
ui×π−i
X
h∈[H−1]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+X
h∈[H]∥Oh(·|sh)−bOh(·|sh)∥1
,
where the first inequality is by the first inequality in Lemma J.7, the second and third inequalities are
due to the fact that TV distance does not increase after marginalization, and the last inequality is by
Lemma H.9. Since π−iis a fixed and fully-observable Markov policy, by Lemma H.11, we have
EG
ui×π−i
X
h∈[H−1]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+X
h∈[H]∥Oh(·|sh)−bOh(·|sh)∥1

≤max
ui∈ΠS,iEG
ui×π−i
X
h∈[H−1]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+X
h∈[H]∥Oh(·|sh)−bOh(·|sh)∥1
,
thus proving the first result of our lemma.
For the second result of our lemma, it can be proved similarly that for any i∈[n],mi∈ M i,j∈[n],
h∈[H],
P(mi⋄πi)⊙π−i,G(sh̸=bgj,h(ch, pj,h))
≤EG
(mi⋄πi)⊙π−i
X
h∈[H−1]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+X
h∈[H]∥Oh(·|sh)−bOh(·|sh)∥1
.
By Lemma J.2, we proved the second result.
Theorem J.4. Fix any ϵ, δ∈(0,1)andπ∈ΠS. Algorithm 5 can learn a decoding function bgsuch
that with probability 1−δ
max
i∈[n],ui∈Πi,j∈[n],h∈[H]Pui×π−i,G(sh̸=bgj,h(ch, pj,h))≤ϵ,
with total sample complexity eO(nS2AHO +nS3AH
ϵ2 +S4A2H5
ϵ)and computational complexity
POLY (S, A, H, O,1
ϵ),log1
δ.
Proof. With the help of Lemma J.3, it suffices to prove
max
i∈[n],ui∈ΠS,iEG
ui×π−i
X
h∈[H]∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+∥Oh(·|sh)−bOh(·|sh)∥1
≤ϵ.
The following proof procedure follows similarly to that of Theorem H.5. For each h∈[H]and
sh∈ S, we define
ph(sh) = max
i∈[n],ui∈ΠS,idui×π−i
h(sh).
Fixϵ1, δ1>0, we define U(h, ϵ1) ={sh∈ S | ph(sh)≥ϵ1}. By [ 37], one can learn a pol-
icy{Ψi(h, sh)}i∈[n]with sample complexity eO(S2AiH4
ϵ1)such that max i∈[n]dΨi(h,sh)×π−i
h(sh)≥
52ph(sh)
2for each sh∈ U(h, ϵ1)with probability 1−n·δ1. Now we assume this event holds
for any h∈[H]andsh∈ U(h, ϵ1). For each sh∈ S andah∈ A , we have executed each
policy {Ψi(h, sh)×π−i}i∈[n]for the first h−1steps followed by an action ah∈ A forN
episodes and denote the total number of episodes that shandahare visited as Nh(sh, ah), and
Nh(sh) =P
a∈ANh(sh, a). Then, with probability 1−e−Nϵ1/8, we have Nh(sh, ah)≥Nph(sh)
2by Chernoff bound. Now conditioned on this event, we are ready to evaluate the following for any
i∈[n], and ui∈ΠS,i:
EG
ui×π−i∥Th(·|sh, ah)−bTh(·|sh, ah)∥1=X
sh,ahdui×π−i
h(sh)(ui×π−i)h(ah|sh)∥Th(·|sh, ah)−bTh(·|sh, ah)∥1
≤2·Sϵ1+X
sh∈U(h,ϵ1),ahdui×π−i
h(sh)[ui×π−i]h(ah|sh)s
Slog(1/δ2)
Nh(sh, ah)
≤2·Sϵ1+X
sh∈U(h,ϵ1),ahdui×π−i
h(sh)[ui×π−i]h(ah|sh)s
2Slog(1/δ2)
Nph(sh)
≤2·Sϵ1+X
shq
dui×π−i
h(sh)r
2Slog(1/δ2)
N
≤2·Sϵ1+Sr
2 log(1 /δ2)
N,
where the second step is by Lemma J.8, and the last step is by Cauchy-Schwarz inequality. Similarly,
EG
ui×π−i∥Oh(·|sh)−bOh(·|sh)∥1=X
shdui×π−i
h(sh)∥Oh(·|sh)−bOh(·|sh)∥1
≤2·Sϵ1+X
sh∈U(h,ϵ1)dui×π−i
h(sh)s
Olog(1/δ2)
Nh(sh)
≤2·Sϵ1+X
sh∈U(h,ϵ1)dui×π−i
h(sh)s
2Olog(1/δ2)
Nph(sh)
≤2·Sϵ1+X
sh∈U(h,ϵ1)q
dui×π−i
h(sh)r
2Olog(1/δ2)
N
≤2·Sϵ1+r
2SOlog(1/δ2)
N,
where the second step is by Lemma J.9, and the last step is by Cauchy-Schwarz inequality. Therefore,
by a union bound, all high probability events hold with probability
1−SHnδ 1−SHAe−Nϵ1/8−2SAHδ 2.
Therefore, we can choose N=eΘ(S2+SO
ϵ2)andϵ1= Θ(ϵ
S), leading to the total sample complexity
SHA
nN+eΘS3AH4
ϵ
=eΘnS2AHO +nS3AH
ϵ2+S4A2H5
ϵ
,
which completes the proof.
Note that although our Algorithm 5 and Theorem J.4 are stated for NE/CCE, it can also handle CE
with simple modifications, where the key observation is that the strategy modification mi∈ M S,i
can also be regarded as a Markov policy in an extended MDP marginalized by π−ias defined below.
Definition J.5. Fixπ∈ΠS. We define Mextended
i (π)to be an MDP for agent
i, where for each h∈[H], the state is (sh, ai,h), the action is some mod-
ified action a′
i,h, the transition is defined as Textended
h (sh+1, ai,h+1|sh, ai,h, a′
i,h) :=
53Ea−i,h∼πh(·|sh,ai,h)[Th(sh+1|sh, a′
i,h, a−i,h)πh+1(ai,h+1|sh+1)], where we slightly abuse the no-
tation of πh(a−i,h|sh, ai,h)andπh(ai,h|sh)by defining them as the posterior and marginal
distributions induced by the joint distribution πh(ah|sh). Similarly, the reward is given by
rextended
h (sh, ai,h, a′
i,h) :=Ea−i,h∼πh(·|sh,ai,h)[rh(sh, a′
i,h, a−i,h)].
With the help of such an extended MDP, we can develop Algorithm 6, which is a CE version of
Algorithm 5 with the following guarantees.
Theorem J.6. Fix any ϵ, δ∈(0,1)andπ∈ΠS. Algorithm 6 can learn a decoding function bgsuch
that
max
i∈[n],mi∈Mi,j∈[n],h∈[H]P(mi⋄πi)⊙π−i,G(sh̸=bgj,h(ch, pj,h))≤ϵ,
with total sample complexity eO(nS2A3HO+nS3A4H
ϵ2 +S4A6H5
ϵ)and computational complexity
POLY (S, A, H, O,1
ϵ).
Proof. Due to the construction of Mextended
i (π), the proof of Theorem J.4 readily applies, where the
only difference is that the state space of Mextended
i (π)is now SAi, larger than that of M(π−i)by a
factor of Aithus proving our theorem.
We next introduce and prove several supporting lemmas used before.
Lemma J.7. Suppose we can sample from a joint distribution P∈∆(X × Y )for some finite X,
Yi.i.d. Then we can learn an approximate distribution Q∈∆(X × Y )with sample complexity
Θ
|X||Y| +log 1 /δ
ϵ2
such that
Ex∼PX
y∈Y|P(y|x)−Q(y|x)| ≤2X
x∈X,y∈Y|P(x, y)−Q(x, y)| ≤ϵ,
with probability 1−δ.
Proof. Note the following holds
X
x∈X,y∈Y|P(x, y)−Q(x, y)|=X
x∈X,y∈Y|P(x, y)−P(x)Q(y|x) +P(x)Q(y|x)−Q(x, y)|
≥X
x∈X,y∈Y|P(x, y)−P(x)Q(y|x)| − |P(x)Q(y|x)−Q(x, y)|.
Therefore, we have
Ex∼PX
y∈Y|P(y|x)−Q(y|x)| ≤X
x∈X,y∈Y|P(x, y)−Q(x, y)|+X
x∈X,y∈YQ(y|x)|P(x)−Q(x)|
≤2X
x∈X,y∈Y|P(x, y)−Q(x, y)|. (J.2)
By the sample complexity of learning discrete distributions [ 13], we can learn Qsuch thatP
x∈X,y∈Y|P(x, y)−Q(x, y)| ≤ϵin sample complexity Θ
|X||Y| +log 1 /δ
ϵ2
with probability 1−δ.
Thus, we proved our lemma.
Lemma J.8 (Concentration on transition) .Fixδ >0and dataset {τk
H}k∈[N]sampled from P(or
Gin the multi-agent setting) under policy π∈Πgen. We define for each h∈[H],(sh, ah, sh+1)∈
S × A × S
Nh(sh, ah) =X
k∈[N]1[sk
h=sh, ak
h=ah], N h(sh, ah, sh+1) =X
k∈[N]1[sk
h=sh, ak
h=ah, sk
h+1=sh].
Then, with probability at least 1−δ, it holds that for any k∈[K], h∈[H], sh∈ S, ah∈ A:
∥Th(·|sh, ah)−bTh(·|sh, ah)∥1≤C1s
Slog(SAHK/δ )
max{Nh(sh, ah),1},
for some absolute constant C1>0, where we define bTh(sh+1|sh, ah) =Nh(sh,ah,sh+1)
max{Nh(sh,ah),1}.
54Proof. This is done by firstly bounding ∥Th(·|sh, ah)−bTh(·|sh, ah)∥1for specific k, h, s h, ah
according to [13], and then taking union bound for all k∈[K], h∈[H], sh∈ S, ah∈ A.
Lemma J.9 (Concentration on emission) .Fixδ >0and dataset {τk
H}k∈[N]sampled from P(orGin
the multi-agent setting) under some policy π∈Πgen. We define for each h∈[H],(sh, oh)∈ S × O
Nh(sh, oh) =X
k∈[N]1[sk
h=sh, ok
h=oh], N h(sh) =X
k∈[N]1[sk
h=sh].
Then, with probability at least 1−δ, it holds that
∥Oh(·|sh)−bOh(·|sh)∥1≤C2s
Olog(SHK/δ )
max{Nk
h(sh),1},
for some absolute constant C2>0, where we define bOh(oh|sh) =Nh(sh,oh)
max{Nh(sh),1}.
Proof. This is done by firstly bounding ∥Oh(·|sh)−bOh(·|sh)∥1for specific k, h, s haccording to
[13], and then taking union bound for all k∈[K], h∈[H], sh∈ S.
Now we switch to proving the guarantees for Algorithm 7.
Lemma J.10. Fixδ >0. With probability 1−δ, it holds that for any k∈[K], h∈[H], sh∈ S:
X
oh+1PG(oh+1|sh, ah)−bJk
h(oh+1|sh, ah)≤C3s
Olog(SHKA/δ )
Nk
h(sh, ah),
wherebJk
his defined in Algorithm 7.
Proof. This is done by firstly boundingP
oh+1PG(oh+1|sh, ah)−bJk
h(oh+1|sh, ah)for specific
k, h, s h, ahaccording to [ 13], and then taking union bound for all k∈[K], h∈[H], sh∈ S, ah∈
A.
From now on, we shall use the bonus
bk
h(sh, ah) = min(
C3(H−h)s
Olog(SAHK/δ )
max{Nk
h(sh, ah),1},2(H−h))
(J.3)
in Algorithm 7, for some absolute constant C3>0.
Before presenting our technical analysis, we define the following notation for the ease of presentation.
We define the following approximate value functions for any policy π∈Πin a backward way for
h∈[H]when given some approximate belief in the form of {bPh:bCh→∆(Ph× S)}h∈[H]as
discussed in Section 7.2:
bVπ,G
i,h(ch) :=Esh,ph∼bPh(·,·|bch)Eωh,{aj,h∼πj,h(·|ωj,h,ch,pj,h)}j∈[n]
Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)h
ri,h(sh, ah) +Vπ,G
i,h+1(ch+1)i
,
bQπ,G
i,h(ch, γh) :=Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]
Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)h
ri,h(sh, ah) +Vπ,G
i,h+1(ch+1)i
,
for each (i, ch)∈[n]× Chandγh∈Γh, where we define bVπ,G
i,H+1(cH+1) = 0 .
Intuitively, this definition of bVπ,G
i,h(ch)mimics the Bellman equation of the ground-truth value function
Vπ,G
i,h(ch)by replacing the ground-truth belief PG(sh, ph|ch)bybPh(sh, ph|bch). Next, we point out
the following quantitative bound when using bVπ,G
i,h(ch)to approximate Vπ,G
i,h(ch).
55Lemma J.11. For any π′, π∈Π, it holds that
EG
π′Vπ,G
i,h(ch)−bVπ,G
i,h(ch)≤(H−h+ 1)2ϵbelief,
where
ϵbelief:= max
h∈[H]max
π∈ΠEG
πPG(·,·|ch)−bPh(·,·|bch)
1.
Proof. It follows directly by combining Lemma 4 and Lemma 8 of [51].
Meanwhile, note that although in Algorithm 7, the value functions we maintain have input bchinstead
ofchfor computational efficiency, we extend the definitions of those value functions to also accept
chas inputs as follows (with a slight abuse of notation):
Qhigh,k
i,h(ch, γh) :=Qhigh,k
i,h(bch, γh), Qhigh,k
i,h(ch, ph, sh, ah) :=Qhigh,k
i,h(bch, ph, sh, ah), Vhigh,k
i,h(ch) :=Vhigh,k
i,h(bch)
Qlow,k
i,h(ch, γh) :=Qlow,k
i,h(bch, γh), Qlow,k
i,h(ch, ph, sh, ah) :=Qlow,k
i,h(bch, ph, sh, ah), Vlow,k
i,h(ch) :=Vlow,k
i,h(bch),
where we recall that bch=Compressh(ch).
Lemma J.12 (Optimism 1 for NE/CCE) .With probability 1−δ, for any k∈[K], for Algorithm 7,
it holds that for any i∈[n],π′
i∈Πi,h∈[H]
Qhigh,k
i,h(bch, γh)≥bQπ′
i×πk
−i,G
i,h(ch, γh), Vhigh,k
i,h(bch)≥bVπ′
i×πk
−i,G
i,h(ch),
where we recall that bch=Compressh(ch).
Proof. We will prove by backward induction. Obviously, it holds for h=H+ 1. Now we assume
the lemma holds for h+ 1, then by definition
Qhigh,k
i,h(ch, γh) =Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]h
Qhigh,k
i,h(ch, ph, sh, ah)i
=Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]min{ri,h(sh, ah) +bk−1
h(sh, ah)
+Eoh+1∼bJk−1
h(·|sh,ah)h
Vhigh,k
i,h+1(ch+1)i
, H−h+ 1}
≥Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]
min
ri,h(sh, ah) +bk−1
h(sh, ah) +Eoh+1∼bJk−1
h(·|sh,ah)
bVπ′
i×πk
−i,G
i,h+1 (ch+1)
, H−h+ 1
,
where the last step is by inductive hypothesis. Now note that for any (sh, ph, ah), we have
bk−1
h(sh, ah) +Eoh+1∼bJk−1
h(·|sh,ah)
bVπ′
i×πk
−i,G
i,h+1(ch+1)
≥bk−1
h(sh, ah)−(H−h)∥bJk−1
h(·|sh, ah)−PG(·|sh, ah)∥1+Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)
bVπ′
i×πk
−i,G
i,h+1(ch+1)
≥Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)
bVπ′
i×πk
−i,G
i,h+1(ch+1)
,
where we notice PG(oh+1|sh, ah) =P
sh+1Oh+1(oh+1|sh+1)Th(sh+1|sh, ah)for the first in-
equality, and the second inequality comes from the construction of our bonus bk−1
h(sh, ah)in
Equation (J.3) and Lemma J.10. Meanwhile, by the definition of value functions, it holds that
Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)
bVπ′
i×πk
−i,G
i,h+1(ch+1)
≤H−h. Therefore, we have
min
ri,h(sh, ah) +bk−1
h(sh, ah) +Eoh+1∼bJk−1
h(·|sh,ah)
Vπ′
i×πk
−i,G
i,h+1(ch+1)
, H−h+ 1
≥ri,h(sh, ah) +Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)
bVπ′
i×πk
−i,G
i,h+1(ch+1)
.
56Now we conclude
Qhigh,k
i,h(ch, γh)
≥Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)[ri,h(sh, ah)
+bVπ′
i×πk
−i,G
i,h+1(ch+1)]
=bQπ′
i×πk
−i,G
i,h(ch, γh).
By definition, we have Qhigh,k
i,h(ch, γh) = Qhigh,k
i,h(bch, γh), thus proving Qhigh,k
i,h(bch, γh)≥
bQπ′
i×πk
−i,G
i,h(ch, γh). Now for the value function, note that
Vhigh,k
i,h(ch) =EωhQhigh,k
i,h(ch,{πk
j,h(·|ωj,h,bch,·)}j∈[n])
≥Eω′
hEωhQhigh,k
i,h(ch, π′
i,h(·|ω′
i,h, ch,·),{πk
j,h(·|ωj,h,bch,·)}j∈[n]\{i})
≥Eω′
hEωhbQπ′
i×πk
−i,G
i,h(ch, π′
i,h(·|ω′
i,h, ch,·),{πk
j,h(·|ωj,h,bch,·)}j∈[n]\{i})
=bVπ′
i×πk
−i,G
i,h(ch),
where the first step is by the property of Bayesian CCE, and the second step is by Qhigh,k
i,h(ch, γh)≥
bQπ′
i×πk
−i,G
i,h(ch, γh)for any γh∈Γhas proved above. Again by definition, we proved Vhigh,k
i,h(bch) =
Vhigh,k
i,h(ch)≥bVπ′
i×πk
−i,G
i,h(ch).
Lemma J.13 (Optimism 1 for CE) .With probability 1−δ, for any k∈[K], for Algorithm 7, it
holds that for any i∈[n],mi∈ M i,h∈[H]
Qhigh,k
i,h(bch, γh)≥bQ(mi⋄πk
i)⊙πk
−i,G
i,h(ch, γh)
Vhigh,k
i,h(bch)≥bV(mi⋄πk
i)⊙πk
−i,G
i,h(ch).
Proof. We will prove by backward induction. Obviously, it holds for h=H+ 1. Now we assume
the lemma holds for h+ 1. Now we notice that by definition,
Qhigh,k
i,h(ch, γh) =Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]h
Qhigh,k
i,h(ch, ph, sh, ah)i
=Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]min{ri,h(sh, ah) +bk−1
h(sh, ah)
+Eoh+1∼bJk−1
h(·|sh,ah)h
Vhigh,k
i,h+1(ch+1)i
, H−h+ 1}
≥Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]
min
ri,h(sh, ah) +bk−1
h(sh, ah) +Eoh+1∼bJk−1
h(·|sh,ah)
bV(mi⋄πk
i)⊙πk
−i,G
i,h+1 (ch+1)
, H−h+ 1
,
where the last step is by inductive hypothesis. Now note that for any sh, ph, ah, we have
bk−1
h(sh, ah) +Eoh+1∼bJk−1
h(·|sh,ah)
bV(mi⋄πk
i)⊙πk
−i,G
i,h+1 (ch+1)
≥bk−1
h(sh, ah)−(H−h)∥bJk−1
h(·|sh, ah)−PG(·|sh, ah)∥1
+Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)
bV(mi⋄πk
i)⊙πk
−i,G
i,h+1 (ch+1)
≥Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)
bV(mi⋄πk
i)⊙πk
−i,G
i,h+1 (ch+1)
,
where we notice PG(oh+1|sh, ah) =P
sh+1Oh+1(oh+1|sh+1)Th(sh+1|sh, ah)for the first in-
equality, and the second inequality comes from the construction of our bonus bk−1
h(sh, ah)in
Equation (J.3) and Lemma J.10. Meanwhile, by the definition of value functions, it holds that
57Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)
bV(mi⋄πk
i)⊙πk
−i,G
i,h+1(ch+1)
≤H−h. Therefore, we have
min{ri,h(sh, ah) +bk−1
h(sh, ah)
+Esh+1,oh+1∼bJk−1
h(·,·|sh,ah)
V(mi⋄πk
i)⊙πk
−i,G
i,h+1(ch+1)
, H−h+ 1}
≥ri,h(sh, ah) +Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)
bV(mi⋄πk
i)⊙πk
−i,G
i,h+1(ch+1)
.
Now we conclude
Qhigh,k
i,h(ch, γh)
≥Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]
Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)
ri,h(sh, ah) +bV(mi⋄πk
i)⊙πk
−i,G
i,h+1 (ch+1)
=bQ(mi⋄πk
i)⊙πk
−i,G
i,h (ch, γh).
By definition, we have Qhigh,k
i,h(ch, γh) = Qhigh,k
i,h(bch, γh), thus proving Qhigh,k
i,h(bch, γh)≥
bQ(mi⋄πk
i)⊙πk
−i,G
i,h(ch, γh). Now for the value function, note that
Vhigh,k
i,h(ch) =EωhQhigh,k
i,h(ch,{πk
j,h(·|ωj,h,bch,·)}j∈[n])
≥EωhQhigh,k
i,h(ch,{πk
j,h(·|ωj,h,bch,·)}j∈[n]\{i},(mi,h⋄πk
i,h)(·|ωi,h,bch,·))
≥EωhbQ(mi⋄πk
i)⊙πk
−i,G
i,h(ch,{πk
j,h(·|ωj,h,bch,·)}j∈[n]\{i},(mi,h⋄πk
i,h)(·|ωi,h,bch,·))
=bV(mi⋄πk
i)⊙πk
−i,G
i,h(ch),
where the first step is by the property of Bayesian CE, and the second step is by Qhigh,k
i,h(ch, γh)≥
bQ(mi⋄πk
i)⊙πk
−i,G
i,h(ch, γh)for any γh∈Γh. Again by definition, we proved Vhigh,k
i,h(bch) =
Vhigh,k
i,h(ch)≥bV(mi⋄πk
i)⊙πk
−i,G
i,h(ch).
Lemma J.14 (Pessimism) .With probability 1−δ, for any k∈[K], for Algorithm 7, it holds that for
anyi∈[n],h∈[H]
Qlow,k
i,h(bch, γh)≤bQπk,G
i,h(ch, γh)
Vlow,k
i,h(bch)≤bVπk,G
i,h(ch).
Proof. We prove by backward induction on h. Obviously, the lemma holds for h=H+ 1. Now we
assume the lemma holds for h+ 1. Similar to the proof of the previous lemma, we note by inductive
hypothesis
Qlow,k
i,h(ch, γh)≤Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]
maxn
ri,h(sh, ah)−bk−1
h(sh, ah) +Esh+1,oh+1∼bJk−1
h(·,·|sh,ah)h
bVπk,G
i,h+1(ch+1)i
,0o
,
where for any sh,ph,ah, we have
−bk−1
h(sh, ah) +Eoh+1∼bJk−1
h(·|sh,ah)h
Vlow,k
i,h+1(ch+1)i
≤ −bk−1
h(sh, ah) + (H−h)∥bJk−1
h(·|sh, ah)−PG(·|sh, ah)∥1
+Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)h
bVπk,G
i,h+1(ch+1)i
≤Esh+1∼Th(·|sh,ah),oh+1∼Oh+1(·|sh+1)h
bVπk,G
i,h+1(ch+1)i
,
58where the last step again comes from the construction of our bonus in Equation (J.3) and Lemma J.10.
Therefore, we conclude
Qlow,k
i,h(ch, γh)≤Esh,ph∼bPh(·,·|bch)E{aj,h∼γj,h(·|pj,h)}j∈[n]
Eoh+1∼bJk−1
h(·|sh,ah)h
ri,h(sh, ah) +bVπk,G
i,h+1(ch+1)i
=bQπk,G
i,h(ch, γh).
Similarly, for value function, it holds that
Vlow,k
i,h(ch) =EωhQlow,k
i,h(ch,{πk
j,h(·|ωj,h,bch,·)}j∈[n])
≤EωhbQπk,G
i,h(ch,{πk
j,h(·|ωj,h,bch,·)}j∈[n]) =bVπk,G
i,h(ch),
thus proving our lemma.
Theorem J.15 (NE/CCE version) .With probability 1−δ, Algorithm 7 enjoys the regret guarantee of
X
k∈[K]max
i∈[n] 
max
π′
i∈ΠiVπ′
i×πk
−i,G
i,1 (ck
1)−Vπk,G
i,1(ck
1)!
≤ O(KH2ϵbelief+H2p
SAOK log(SAHK/δ ) +H2SAp
Olog(SAHK/δ )).
Correspondingly, this implies that one can learn an (ϵ+H2ϵbelief)-NE if Gis zero-sum and (ϵ+
H2ϵbelief)-CCE if Gis general-sum with sample complexity O(H4SAO log(SAHO/δ )
ϵ2 )and computation
complexity POLY (S,max h∈[H]|bCh|,max h∈[H]|Ph|, H,1
ϵ,log1
δ).
Proof. Note for any given i∈[n]andπ′
i∈Πi, by Lemma J.12 and Lemma J.14, it holds
max
π′
i∈ΠiVπ′
i×πk
−i,G
i,h(ck
h)−Vπk,G
i,h(ck
h)≤Vhigh,k
i,h(ck
h)−Vlow,k
i,h(ck
h).
Therefore, it suffices to bound Vhigh,k
i,h(ck
h)−Vlow,k
i,h(ck
h):
Vhigh,k
i,h (ck
h)−Vlow,k
i,h(ck
h)
=Esh,ph∼bPh(·,·|bck
h)Eωhh
Qhigh,k
i,h(ck
h,{πk
j,h(·|·,bck
h,·)}j∈[n])−Qlow,k
i,h(ck
h,{πk
j,h(·|·,bck
h,·)}j∈[n])i
≤Esh,ph∼PG(·,·|ck
h)Eωhh
Qhigh,k
i,h(ck
h,{πk
j,h(·|·,bck
h,·)}j∈[n])−Qlow,k
i,h(ck
h,{πk
j,h(·|·,bck
h,·)}j∈[n])i
+ (H−h+ 1)ϵh(ck
h)
≤Esh,ph∼PG(·,·|ck
h)EωhE{aj,h∼πk
j,h(·|ωj,h,bck
h,pj,h)}j∈[n]h
Qhigh,k
i,h(ck
h, ph, sh, ah)−Qlow,k
i,h(ck
h, ph, sh, ah)i
+ (H−h+ 1)ϵh(ck
h)
≤Z1
k,h+Qhigh,k
i,h(ck
h, pk
h, sk
h, ak
h)−Qlow,k
i,h(ck
h, pk
h, sk
h, ak
h) + (H−h+ 1)ϵh(ck
h)
≤Z1
k,h+Eoh+1∼bJk−1
h(·|sk
h,ak
h)h
Vhigh,k
i,h+1(ch+1)−Vlow,k
i,h+1(ch+1)i
+ (H−h+ 1)ϵh(ck
h) + 2bk−1
h(sk
h, ak
h)
≤Z1
k,h+Eoh+1∼PG(·|sk
h,ak
h)h
Vhigh,k
i,h+1(ch+1)−Vlow,k
i,h+1(ch+1)i
+ (H−h+ 1)ϵh(ck
h) + 3bk−1
h(sk
h, ak
h)
≤Z1
k,h+Z2
k,h+Vhigh,k
i,h+1(ck
h+1)−Vlow,k
i,h+1(ck
h+1) + (H−h+ 1)ϵh(ck
h) + 3bk−1
h(sk
h, ak
h),
where we define the two Martingale difference sequences as follows
Z1
k,h:=Esh,ph∼PG(·,·|ck
h)EωhE{aj,h∼πk
j,h(·|ωj,h,bck
h,pj,h)}j∈[n]
Qhigh,k
i,h(ck
h, ph, sh, ah)−Qlow,k
i,h(ck
h, ph, sh, ah)
−
Qhigh,k
i,h(ck
h, pk
h, sk
h, ak
h)−Qlow,k
i,h(ck
h, pk
h, sk
h, ak
h)
Z2
k,h:=Eoh+1∼PG(·|sk
h,ak
h)h
Vhigh,k
i,h+1(ch+1)−Vlow,k
i,h+1(ch+1)i
−
Vhigh,k
i,h+1(ck
h+1)−Vlow,k
i,h+1(ck
h+1)
,
and the error of the belief is defined as
ϵh(ck
h) :=∥bPh(·,·|bck
h)−PG(·,·|ck
h)∥1.
59Since|Z1
k,h| ≤H,|Z2
k,h| ≤H, and ϵh(ck
h)≤2, by Azuma-Hoeffding bound, we conclude with
probability 1−3δ, the following holds
X
k,hZ1
k,h≤ O(Hr
HKlog1
δ),X
k,hZ2
k,h≤ O(Hr
HKlog1
δ),
X
k,hϵh(ck
h)≤X
kEG
πk"X
hϵh(ch)#
+O(r
HKlog1
δ)≤KHϵ belief+O(r
HKlog1
δ).
Meanwhile, by the pigeonhole principle, it holds that
X
k,hbk−1
h(sk
h, ak
h)≤Hp
Olog(SAHK/δ )X
k,h1q
max{1, Nk−1
h(sk
h, ak
h)}
≤ O
Hp
Olog(SAHK/δ )(H√
SAK +HSA )
.
Now by Lemma J.12 and Lemma J.14 and putting everything together, we conclude
X
k∈[K]max
i∈[n] 
max
π′
i∈ΠibVπ′
i×π−i,G
i,1 (ck
1)−bVπ,G
i,1(ck
1)!
≤KH2ϵbelief+O(H2p
SAOK log(SAHK/δ ) +H2SAp
Olog(SAHK/δ )).
Now by Lemma J.11, we proved the regret guarantees as follows
X
k∈[K]max
i∈[n] 
max
π′
i∈ΠiVπ′
i×πk
−i,G
i,1 (ck
1)−Vπk,G
i,1(ck
1)!
≤ O(KH2ϵbelief+H2p
SAOK log(SAHK/δ ) +H2SAp
Olog(SAHK/δ )).
For the PAC guarantees, since we define k⋆∈arg min i∈[n],k∈[K]Vhigh,k
i,1(ck
1)−Vlow,k
i,1(ck
1), we have
CCE-gap (πk⋆)≤ O(H2ϵbelief) + max
i∈[n]
Vhigh,k⋆
i,h(ck⋆
1)−Vlow,k⋆
i,h(ck⋆
1)
≤ O(H2ϵbelief) +
1
KX
k∈[K]Vhigh,k
i,h(ck
1)−Vlow,k
i,h(ck
1)

≤ O(H2ϵbelief+H2p
SAO log(SAHK/δ )/K+H2SA
Kp
Olog(SAHK/δ )).
Finally, for two-player zero-sum games, we denote bπk⋆to be the marginalized policy of πk⋆. Then
we have
NE-gap (bπk⋆)≤CCE-gap (πk⋆),
thus concluding our theorem.
Theorem J.16 (CE version) .With probability 1−δ, Algorithm 7 enjoys the regret guarantee of
X
k∈[K]max
i∈[n] 
max
m′
i∈MiV(m′
i⋄πk
i)⊙πk
−i,G
i,1 (ck
1)−Vπk,G
i,1(ck
1)!
≤ O(KH2ϵbelief+H2p
SAOK log(SAHK/δ ) +H2SAp
Olog(SAHK/δ )).
Correspondingly, this implies that one can learn an (ϵ+H2ϵbelief)-CE
with sample complexity O(H4SAO log(SAHO/δ )
ϵ2 ) and computation complexity
POLY (S,max h∈[H]|bCh|,max h∈[H]|Ph|, H,1
ϵ,log1
δ)
Proof. Then proof follows as that of Theorem J.15, where we only need to change the first step of
the proof as
bVπ′
i×πk
−i,G
i,h(ck
h)−bVπk,G
i,h(ck
h)≤Vhigh,k
i,h(ck
h)−Vlow,k
i,h(ck
h),
by Lemma J.13 and Lemma J.14, and the remaining steps are exactly the same.
60Lemma J.17 (Adapted from Theorem H.5) .Algorithm 8 can learn the approximate POMDP with
transition bT1:Hand emission bO1:Hsuch that for any policy π∈Πgenandh∈[H]
EG
πh
∥Th(·|sh, ah)−bTh(·|sh, ah)∥1+∥Oh(·|sh)−bOh(·|sh)∥1i
≤ O(ϵ),
using sample complexity eO(S2AHO +S3AH
ϵ2 +S4A2H5
ϵ)with probability 1−δ.
Proof. Note that Algorithm 8 is essentially treating the POSG Gas a centralized MDP and running
Algorithm 4, where the only modifications we make in Algorithm 8 is that we take the controller
set (see some examples of the controller set in Appendix C.3) into considerations when learning the
models. Specifically, for the transition bTh, what we estimate is only bTh(sh+1|sh, aIh,h)instead
ofbTh(sh+1|sh, ah), where Th⊆[n]is the controller set. Therefore, the sample complexity of
Algorithm 8 will not be worse than that of Algorithm 4.
Proof of Theorem 7.7:
Note that the proof idea essentially resembles that of Theorem H.6, where we construct the model
GtruncforGin exactly the same way as constructing PtruncforP. Therefore, by Corollary H.10, we
have
EG
π[∥PG(·,·|ch)−PGtrunc(·,·|ch)∥1]≤2X
τH|Pπ,G(τH)−Pπ,Gtrunc(τH)| ≤4HSϵ 1.
Meanwhile, we can construct bGtruncandbGsubusing exactly the same way as for bPtruncandbPsub, where
bGsubis aγ/2-observable POSG.
Now, according to [ 51], for all the examples in Appendix C.3, there exists a compression function
that maps chtobchsuch that the size of the compressed common information is quasi-polynomial,
i.e.,bCh≤(AO)Cγ−4logSH
ϵ2for some absolute constant Candϵ2∈(0,1), and the corresponding
approximate belief {bPh:bCh→∆(S × P h)}h∈[H]satisfies that
EbGsub
π∥PbGsub(·,·|ch)−ePh(·,·|bch)∥1≤ϵ2.
Therefore, we can do the same augmentation for ePhon states from Slow
hto construct the approxi-
mate belief bPhas in the proof of Theorem H.6, and the remaining steps follow from the proof of
Theorem H.6. This will lead to a total of polynomial-time and polynomial-sample complexities. ■
J.1 Background on Bayesian Games
The Bayesian game is a generalization of normal-form games in partially observable settings. Specifi-
cally, a Bayesian game is specified as (n,{Ai}i∈[n],{Θi}i∈[n],{ri}i∈[n], µ), where nis the number
of players, Aiis the actor space, Θiis the type space, ri:×i∈[n](Θi× A i)→[0,1]is the reward
function, and µis the prior distribution of the joint type. At the beginning of the game, a type
θ= (θi)i∈[n]is drawn from the prior distribution µ∈∆(Θ) . Then each agent igets its own type θi
and takes the action ai. We define a pure strategy of an agent as si∈STi:={Θi→ A i}. We define
Ji(si, s−i)to be the expected rewards for agent i, given the pure joint strategy (si, s−i).
By definition, Ji(si, s−i)can be evaluated as
Ji(si, s−i) :=Eθ∼µri(θ, si(θi), s−i(θ−i)).
Bayesian NE. We define γ⋆∈ ×i∈[n]∆(STi)is anϵ-NE is it satisfies that
Es∼γ⋆Ji(s)≥Es∼γ⋆Ji(s′
i, s−i)−ϵ,∀i∈[n], s′
i∈STi.
Bayesian CCE. We say a distribution of joint strategies γ⋆∈∆(×i∈[n]STi)to be a ϵ-Bayesian
CCE if it satisfies
Es∼γ⋆Ji(s)≥Es∼γ⋆Ji(s′
i, s−i)−ϵ,∀i∈[n], s′
i∈STi.
61(Agent-form) Bayesian CE. We say a distribution of joint strategies γ⋆∈∆(×i∈[n]STi)to be an
ϵ-agent-form Bayesian CE if it satisfies
Es∼γ⋆Ji(s)≥Es∼γ⋆Ji(mi⋄si, s−i)−ϵ,∀i∈[n], m′
i∈ M i,
where Mi={Θi× A i→ A i}is the space for strategy modification, where mimodifies sias
follows: given current type θiand the recommended action ai, the strategy modification changes the
action to the another action mi(θi, ai).
Note that Bayesian NE for zero-sum games, and (agent-form) Bayesian CE/CCE are all tractable
solution concepts and can be computed with polynomial computational complexity, e.g., [ 29,32,23].
K Concluding Remarks and Limitations
In this paper, we aim to understand the provable benefits of privileged information for partially
observable RL problems under two empirically successful paradigms, expert distillation [14,64,58]
andasymmetric actor-critic [68,7,3], which represent privileged policy and privileged value learning,
respectively, with an emphasis on studying both the computational and sample efficiencies of the
algorithms. Our results (as summarized in Table 1) showed that privileged information does improve
learning efficiency in a series of known POMDP subclasses. One potential limitation of our work is
that we only focused on the case with exact state information. It remains to explore whether such an
assumption can be further relaxed, e.g., when privileged state information may be biased, partially
observable, or delayed, as usually happens in practice, and how our theoretical results may be affected.
Meanwhile, as an initial theoretical study, we have been primarily focusing on the tabular settings
(except Appendix G), and it would be interesting to extend the results to function-approximation
settings to handle massively large state, action, and observation spaces in practice.
62NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our abstract indeed accurately summarizes out paper’s contribution and scope.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We clearly outline all of our assumption needed to derive our theoretical
results.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
63Justification: We prove every theorem or lemma in this paper.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Yes, detailed are introduced in Appendix I
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
64Answer: [Yes]
Justification: We have included detailed introductions on the algorithms and environment to
reproduce our results.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: In Appendix I.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We have reported the number of repeated experiments in Appendix I and
error/variance bars in Figure 2 and Table 2.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
65•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We have provides all details in Appendix I.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: It does.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We address the societal impacts of our paper in Appendix A.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
66•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: There is no such risk.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: No external codes, data, models are used.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
67•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We do not release any new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No such experiments were involved.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No such approval was needed.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
68