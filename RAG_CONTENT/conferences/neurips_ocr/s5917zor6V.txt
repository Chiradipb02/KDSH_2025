On the Curses of Future and History in
Future-dependent Value Functions for OPE
Yuheng Zhang
University of Illinois Urbana-Champaign
yuhengz2@illinois.eduNan Jiang
University of Illinois Urbana-Champaign
nanjiang@illinois.edu
Abstract
We study off-policy evaluation (OPE) in partially observable environments with
complex observations, with the goal of developing estimators whose guarantee
avoids exponential dependence on the horizon. While such estimators exist for
MDPs and POMDPs can be converted to history-based MDPs, their estimation
errors depend on the state-density ratio for MDPs which becomes history ratios
after conversion, an exponential object. Recently, Uehara et al. [2022a] proposed
future-dependent value functions as a promising framework to address this issue,
where the guarantee for memoryless policies depends on the density ratio over
thelatent state space. However, it also depends on the boundedness of the future-
dependent value function and other related quantities, which we show could be
exponential-in-length and thus erasing the advantage of the method. In this paper,
we discover novel coverage assumptions tailored to the structure of POMDPs, such
asoutcome coverage andbelief coverage , which enable polynomial bounds on
the aforementioned quantities. As a side product, our analyses also lead to the
discovery of new algorithms with complementary properties.
1 Introduction and Related Works
Off-policy evaluation (OPE) is the problem of estimating the return of a new evaluation policy πe
based on historical data, which is typically collected using a different policy πb(thebehavior policy ).
OPE plays a central role in the pipeline of offline reinforcement learning (RL), but is also notoriously
difficult. Among the major approaches, importance sampling (IS) and its variants Precup et al. [2000],
Jiang and Li [2016] provide unbiased and/or asymptotically correct estimation using the cumulative
importance weights : given a trajectory of observations and actions o1, a1, o2, a2, . . . , o H, aH,the
cumulative importance weight isQH
h=1πe(ah|oh)
πb(ah|oh), whose variance grows exponentially with the
horizon, unless πbandπeare very close in their action distributions. In the language of offline RL
theory [Chen and Jiang, 2019, Xie and Jiang, 2021, Yin and Wang, 2021], the boundedness of the
cumulative importance weights is the coverage assumption required by IS, a very stringent one.
Alternatively, algorithms such as Fitted-Q Evaluation [FQE; Ernst et al., 2005, Munos and Szepesvári,
2008, Le et al., 2019] and Marginalized Importance Sampling [MIS; Liu et al., 2018, Xie et al., 2019,
Nachum et al., 2019, Uehara et al., 2020] enjoy more favorable coverage assumptions, at the cost of
function-approximation biases. Instead of requiring bounded cumulative importance weights, FQE
and MIS only require that of the state-density ratios , which can be substantially smaller [Chen and
Jiang, 2019, Xie and Jiang, 2021]. That is, when the environment satisfies the Markov assumption
(namely ohis astate), the guarantees of FQE and MIS only depend on the range of dπe
h(oh)/dπb
h(oh),
where dπe
handdπb
his the marginal distribution of ohunder πeandπb, respectively.
In this paper, we study the non-Markov setting, which is ubiquitous in real-world applications. Such
environments are typically modeled as Partially Observable Markov Decision Processes (POMDPs)
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Kaelbling et al. [1998]. Despite the more general formulation, one can reduce a POMDP to an MDP,
making algorithms for MDPs applicable: we can simply define an equivalent MDP, with its state
being the history of the original POMDP, (o1, a1, . . . , o h). Unfortunately, a close inspection reveals
the problem: the state-density ratio after conversion is
dπe
h(o1, a1, . . . , o h)
dπb
h(o1, a1, . . . , o h)=h−1Y
h′=1πe(ah′|oh′)
πb(ah′|oh′),
which is exactly the cumulative importance weights in IS and thus also an exponential object!
To address this issue, Uehara et al. [2022a] recently proposed a promising framework called future-
dependent value functions (or FDVF for short). Notably, their coverage assumption is the boundedness
of density ratios between πeandπbover the latent state for memoryless policies. This is as if we
were dealing directly with the latent MDP underlying the POMDP, a perhaps best possible scenario.
Nevertheless, have we achieved exponential-free OPE in POMDPs?
The answer to this question turns out to be nontrivial. In addition to the latent-state coverage parameter,
the guarantee in Uehara et al. [2022a] also depends on other quantities that are less interpretable.
Among them, the boundedness of FDVF itself—a concept central to this framework—is unclear,
and we show that a natural construction yields an upper bound that still scales with the cumulative
importance weights, thus possibly erasing the superiority of the framework over IS or MDP-reduction.
In this work, we address these caveats by proposing novel coverage assumptions tailored to the
structure of POMDPs, under which fully polynomial estimation guarantees can be established. More
concretely, our contributions are:
1.For FDVFs, we show that a novel coverage concept called outcome coverage is sufficient for
guaranteeing its boundedness (Section 4). Notably, outcome coverage concerns the overlap
between πbandπefrom the current time step onward , whereas all MDP coverage assumptions
concern that before the current step.
2.With another novel concept called belief coverage (Section 5.1), we establish fully polynomial
estimation guarantee for the algorithm in Uehara et al. [2022a]. The discovery of belief coverage
also leads to a novel algorithm (Section 5.3) that is analogous to MIS for MDPs.
3.Despite the similarity to linear MDP coverage [Duan et al., 2020] due to the linear-algebraic
structure, these POMDP coverage conditions also have their own unique properties due to the L1
normalization of belief and outcome vectors. We present improved analyses that leverage such
properties and avoid explicit dependence on the size of the latent state space (Section 4.2).
2 Preliminaries
POMDP Setup. We consider a finite-horizon POMDPD
H,S=SH
h=1Sh,A,O=SH
h=1Oh, R,O,T, d1E
,
where His the horizon, Shis the latent state space at step hwith|Sh|=S,Ais the action space
with|A|=A,Ohis the observation space at step hwith|Oh|=O,R:O × A → [0,1]is the
reward function, O:S → ∆(O)is the emission dynamics with O(·|sh)supported on Ohfor
sh∈ Sh,T:S × A → ∆(S)is the dynamics ( T(·|sh, ah)is supported on Sh+1), and d1∈∆(S1)
is the initial latent state distribution. For mathematical convenience we assume all the spaces are
finite and discrete, but the cardinality of Oh,O,can be arbitrarily large . A trajectory (or episode)
is sampled as s1∼d1, then oh∼O(·|sh),rh=R(oh, ah),sh+1∼T(·|sh, ah)for1≤h≤H,
witha1:Hdecided by the decision-making agent, and the episode terminates after aH.s1:Hare latent
and not observable to the agent.
History-future Split. Given an episode o1, a1, . . . , o H, aHand a time step hof interest, it will be
convenient to rewrite the episode as
(τh,fhz}|{
oh, ah, fh+1).
Here τh= (o1, a1, . . . o h−1, ah−1)∈ H h:=Qh−1
h′=1(Oh× A)denotes the historical observation-
action sequence (or simply history ) prior to step h, and fh+1= (oh+1, ah+1, . . . , o H, aH)∈
Fh+1:=QH
h′=h+1(Oh′× A)denotes the future after step h. This format will be convenient for
2reasoning about the system dynamics at step h. We use H=SH
h=1Hhto denote the entire history
domain and use F=SH
h=1Fhto denote the entire future domain. This way we can use stationary
notation for functions over S,O,H,F, where the time step can be identified from the function input
(e.g., R(oh, ah)), and functions with time-step subscripts refer to their restriction to the h-th step
input space, often treated as a vector (e.g., Rh∈ROh×A).
Memoryless and History-dependent Policies. A policy π:SH
h=1(Hh× O h)→∆(A)specifies
the action probability conditioned on the past observation-action sequence.1In the main text, we will
restrict ourselves to memory-less (or reactive) policies that only depends on the current observation oh;
extension to general policies is similar to Uehara et al. [2022a] and discussed in Appendix B.6. For any
π, we use PrπandEπfor the probabilities and expectations under episodes generated by π, and define
J(π)as the expected cumulative return: J(π) :=EπhPH
h=1R(oh, ah)i
. For memoryless π, we also
define Vπ
S(sh)as the latent state value function at sh:Vπ
S(sh) :=EπhPH
h′=hR(oh′, ah′)|shi
∈
[0, H].dπ(sh)denotes the marginal distribution of shunder π.
Off-policy Evaluation. In OPE, the goal is to estimate J(πe)using ndata trajectories D=
{(o(i)
1, a(i)
1, r(i)
1, . . . , o(i)
H, a(i)
H, r(i)
H) :i∈[n]}collected using πb. We write ED[·]to denote empirical
approximation of expectation using D. Define the one-step action probability ratio µ(oh, ah) :=
πe(ah|oh)
πb(ah|oh). We make the following assumption throughout:
Assumption 1 (Action coverage) .We assume πb(ah|oh)is known and max h,oh,ahµ(oh, ah)≤Cµ.
This is a standard assumption in the OPE literature, and is needed by IS and value-based estimators
that model state value functions [Jiang and Li, 2016, Liu et al., 2018].
Belief and Outcome Matrices. We now introduce two matrices of central importance to our
discussions. Given history τh, we define b(τh)∈RSas its belief state vector where bi(τh) =
Pr(sh=i|τh). Then the belief matrix MH,h∈RS×Hhis one where the column indexed by
τh∈ H hisb(τh). Similarly, for future fh, we define u(fh)∈RSas its outcome vector where
[u(fh)]i= Pr πb(fh|sh=i). The outcome matrix MF,h∈RS×Fhis one where the column
indexed by fhisu(fh). Unlike the belief matrix, the outcome matrix MF,his dependent on the
behavior policy πb, which is omitted in the notation.
For mathematical conveniences, we make the following assumptions throughout merely for simplify-
ing presentations; they allow us to invert certain covariance matrices and avoid 0/0situations, which
can be easily handled with extra care when the assumptions do not hold.
Assumption 2 (Invertibility) .∀h∈[H], (1) rank (MH,h) =rank(MF,h) =S.
(2)∀fh,Prπb(fh)>0;∀oh, ah,R(oh, ah)>0.
Other Notation. Given a vector a,∥a∥Σ:=√
a⊤Σa, where Σis a positive semi-definite (PSD)
matrix. When Σ = diag(d)for a stochastic vector d, this is the d-weighted 2-norm of a, which we
also write as ∥a∥2,d. For a positive integer m, we use [m]to denote the set {1,2,···, m}. For a
matrix M, we use (M)ijto denote the ijentry of M.
3 Future-dependent Value Functions
In this section, we provide a recap of FDVFs and translate the main result of Uehara et al. [2022a]
into the finite-horizon setting, which is mathematically cleaner and more natural in many aspects; see
Appendix B.2 for further discussion.
To illustrate the main idea behind FDVFs, recall that the tool that avoids the exponential weights in
MDPs is to model the value functions . While we would like to apply the same idea to POMDPs,
1Here we do notallow the policy to depend on the latent state, which satisfies sequential ignorability and
eliminates data confoundedness; see Namkoong et al. [Section 3; 2020] and Uehara et al. [Appendix B; 2022a].
There is also a line of research on OPE in confounded POMDPs where the behavior policy only depends on the
latent state [Tennenholtz et al., 2020, Shi et al., 2022]; see Appendix A.
3history-dependent value functions lead to unfavorable coverage conditions (see Section 1). The only
other known notion of value functions we are left with is that over the latent state space, Vπe
S(sh),
which unfortunately is not accessible to the learner since it operates on unobservable latent states.
The central idea is to find observable proxies ofVπe
S(sh), which takes future as inputs:
Definition 3 (Future-dependent value functions [Uehara et al., 2022a]) .A future-dependent value
function VF:F → R, where F:=S
hFh, is any function that satisfies the following: ∀sh,
Eπb[VF(fh)|sh] =Vπe
S(sh).Equivalently, in matrix form, we have ∀h,
MF,h×VF,h=Vπe
S,h. (1)
Recall our convention, that VF,h∈R|Fh|isVFrestricted to Fh, and Vπe
S,his defined similarly.
A FDVF VFis a property of πe, but also depends on πb. As we will see later in Section 4, the
boundedness of VFwill depend on certain notion of coverage of πboverπe. As another important
property, the FDVF VFis generally not unique even if we fix πeandπb, as Eq. (1)is generally an
underdetermined linear system ( S≪ |F h|) and can yield many solutions. As we see below, it suffices
to model anyone of the solutions. Thus, from now on, when we talk about the boundedness of VF,
we always consider the VFwith the smallest range among all solutions.
Finite Sample Learning. Like in MDPs, to learn an approximate FDVF from data, we will
minimize some form of estimated Bellman residuals (or errors). For that we need to first introduce
the Bellman residual operators for FDVFs:
Definition 4 (Bellman residual operators) .∀V:F →R, the Bellman residual on state shis:2
(BSV)(sh) :=Eah∼πeah+1:H∼πb[rh+V(fh+1)|sh]−Eπb[V(fh)|sh].
Similarly, the Bellman residual onto the history τhis:
(BHV)(τh) :=Eah∼πeah+1:H∼πb[rh+V(fh+1)|τh]−Eπb[V(fh)|τh] =⟨b(τh),BS
hV⟩. (2)
The following lemma shows that, ideally, we would want to find Vwith small BSV:
Lemma 1. For any πe,πb, and V:F →R,J(πe)−Eπb[V(f1)] =PH
h=1Eπe
(BSV)(sh)
.
See proof in Appendix C.1. As the lemma shows, any Vwith small BSV(such as VF, since
BSVF≡0) can be used to estimate J(πe)viaEπb[V(f1)]. But again, BSoperates on Swhich is
unobserved, and we turn to its proxy BH, which are linear measures of BS(Eq.2). More concretely,
Uehara et al. [2022a] proposed to estimate Eπb[(BHV)2]using an additional helper class Ξ :H →R
to handle the double-sampling issue [Antos et al., 2008, Dai et al., 2018]:
bV= argmin
V∈Vmax
ξ∈ΞPH
h=1Lh(V, ξ), (3)
whereLh(V, ξ) =ED[{µ(ah, oh)(rh+V(fh+1))−V(fh)}ξ(τh)−0.5ξ2(τh)]. Under the following
assumptions, the estimator enjoys a finite-sample guarantee:
Assumption 5 (Realizability) .LetV ⊂(F →R)be a finite function class. Assume VF∈ V for
some VFsatisfying Definition 3.
Assumption 6 (Bellman completeness) .LetΞ⊂(H → R)be a finite function class. Assume
BHV∈Ξ,∀V∈ V.
Theorem 2. Under Assumptions 5 and 6, w.p. ≥1−δ,
|J(πe)−ED[bV(f1)]| ≤cHmax{CV+ 1, CΞ} ·IV(V)DrV[dπe, dπb]s
Cµlog|V||Ξ|
δ
n,
where cis an absolute constant,3andCV:= max V∈V∥V∥∞,CΞ:= max ξ∈Ξ∥ξ∥∞,
IV(V) := max
hsup
V∈Vs
Eπb[(BSV) (sh)2]
Eπb[(BHV) (τh)2],DrV[dπe, dπb] := max
hsup
V∈Vs
Eπe[(BSV) (sh)2]
Eπb[(BSV) (sh)2].
2We let V(fH+1)≡0for all Vto be considered, so that we do not need to handle the H-th step separately.
3The value of ccan differ in each occurrence, and we reserve the symbol cfor such absolute constants.
4See proof in Appendix C.2. Among the objects that appear in the bound, some are mundane and
expected (e.g., horizon H, the complexities of function classes log|V||Ξ|, etc.). We now focus on the
important ones, which reveals the open questions we shall investigate next:
1.DrV[dπe, dπb]measures the coverage of πboverπeon the latent state space S, as the expression
inside square-root can be bounded by max shdπe(sh)/dπb(sh).
2.(Q1) CVis the range of the Vclass. Since VF∈ V (Assumption 5), CV≥ ∥VF∥∞. However,
unlike the standard value functions in MDPs which have obviously bounded range [0, H](this
also applies to Vπe
S), the range of VFis unclear. Similarly, since Ξneeds to capture BHVfor
V∈ V, the boundedness of Ξis also affected.
3.(Q2) IV(V)appears because we use BHVas a proxy for BSV, which is only a linear measure
of the latter, (BHV)(τh) =⟨b(τh),BS
hV⟩.IV(V)reflects the conversion ratio between them.
Uehara et al. [2022a] pointed out that the value is finite if MH,hhas full-row rank ( S), but a
quantitative understanding is missing.
The rest of the paper proposes new coverage assumptions, analyses, and algorithms to answer the
above questions, providing deeper understanding on the mathematical structure of OPE in POMDPs.
4 Boundedness of FDVFs
We start with Q1: when are FDVFs bounded? Recall from Definition 3 that a FDVF at level h,
VF,h, is any function satisfying MF,h×VF,h=Vπe
S,h, where MF,his the outcome matrix with
(MF,h)i,j= Pr πb(fh=j|sh=i). Since the equation can have many solutions and we only
need to find one of them, it suffices to provide an explicit construction of a VF,hand show it is
well bounded. Also note that the equations for different hare independent of each other, so we can
construct VF,hfor each hseparately.
We first describe two natural constructions, both of which yield exponentially large upper bounds.
Importance Sampling Solution. Uehara et al. [2022a] provided a cruel bound on ∥VF∥∞which
scales with 1/σmin(MF,h), which we show shortly is an exponential-in-length quantity. However, it
is not hard to notice that in certain benign cases, VFdoes not have to blow-up exponentially and has
obviously bounded constructions.
As a warm-up, consider the on-policy case of πb=πe, where a natural solution is VF(fh) =
R+(fh) :=PH
h′=hR(oh′, ah′).HereR+(fh)simply adds up the Monte-Carlo rewards in future fh,
which is bounded in [0, H]. The construction trivially satisfies Eπb[VF(fh)|sh] =Vπe
S(sh)when
πb=πe. In the more general setting of πb̸=πe, the hope is that VFcan be still bounded up to some
coverage condition that measures how πedeviates from πb.
Unfortunately, generalizing the above equation to the off-policy case raises issues: consider the
solution VF(fh) =R+(fh)·QH
h′=hπe(ah′|oh′)
πb(ah′|oh′),whose correctness can be verified by importance
sampling. Despite its validity, the construction involves cumulative action importance weights, which
erases the superiority of the framework over IS as discussed in the introduction.
Pseudo-inverse Solution. Another direction is to simply treat Eq. (1)as a linear system. Given that
the system is under-determined, we can use pseudo-inverse:4VF,h=M⊤
F,h
MF,hM⊤
F,h−1
Vπe
S,h.
In this case, ∥VF∥∞can be bounded using 1/σmin(MF,h), where σmindenotes the smallest singular
value. In fact, closely related quantities have appeared in the recent POMDP literature; for example,
in the online setting, Liu et al. [2022a] used σminof an action-conditioned variant of MF,has a
complexity parameter for online exploration in POMDPs. Unfortunately, these quantities suffer from
scaling issues: 1/σmin(MF,h)isguaranteed to be misbehaved if the process is sufficiently stochastic.
Example 1. IfPrπb(fh|sh)≤Cstoch
(OA)H−h+1,∀fh, sh, then σmin(MF,h)≤Cstoch√
S/(OA)H−h+1
2.
4All covariance-like matrices in the paper, such as MF,hM⊤
F,hhere, are invertible under Assumption 2.
5In this example, Cstochmeasures how the distribution of fhunder πbdeviates multiplicatively from a
uniform distribution over Fh, and an even moderately stochastic πband emission process Owill lead
to small Cstoch, which implies an exponentially large 1/σmin(MF,h).
4.1 Minimum Weighted 2-Norm Solution and L2Outcome Coverage
Pseudo-inverse finds the minimum L2norm solution. However, given that we are searching for
solutions in RFhwhich has an exponential dimensionality, the standard L2norm—which treats all
coordinates equally—is not a particularly informative metric. Instead, we propose to minimize the
weighted L2norm with a particular weighting scheme, which has also been used in HMMs [Mahajan
et al., 2023] and enjoys benign properties.
We first define the diagonal weight matrix Zh:= diag( 1⊤
SMF,h), where 1S∈RS= [1,···,1]⊤is
the all-one vector. Then, the solution that minimizes ∥ · ∥Zhis:
VF,h=Z−1
hM⊤
F,hΣ−1
F,hVπe
S,h,where ΣF,h:=MF,hZ−1
hM⊤
F,h. (4)
ΣF,h∈RS×Splays an important role in this construction. Recall that its counterpart in the pseudo-
inverse solution, namely MF,hM⊤
F,h, has scaling issues (Example 1), that even its largest eigenvalue
can decay exponentially with H−h+ 1. In contrast, ΣF,his very well-behaved in its magnitude,
as shown below. Furthermore, while M⊤
F,hon the left is now multiplied by Z−1
hwhich can be
exponentially large, Z−1
hM⊤
F,htogether is still well-behaved; see proof in Appendix D.2.
Proposition 3 (Properties of Eq. (4)).1.ΣF,hisdoubly-stochastic : that is, each row/column of ΣF,h
is non-negative and sums up to 1. As a consequence, σmax(ΣF,h) = 1 .
2. Rows of Z−1
hM⊤
F,h, i.e.,{u(fh)⊤/Z(fh) :fh∈ F h}, are stochastic vectors, i.e., they are
non-negative and the row sum is 1.
Therefore, it is promising to make the assumption that σmin(ΣF,h)is bounded away from zero,
which immediately leads to the boundedness of VFgiven that of Z−1
hM⊤
F,h([0,1]) and Vπe
S([0, H]).
However, we need to rule out the possibility that ΣF,his always near-singular, and find natural
examples that admit large σmin(ΣF,h), as given below.
Example 2. Suppose fhalways reveals sh, in the sense that for any j∈ Fh,Prπb(fh=j|sh=i)
is only non-zero for a single i∈[S], and zero for all other latent states. Then, ΣF,h=I, the identity
matrix. Furthermore, VFfrom Eq. (4)satisfies ∥VF∥∞≤H.See Appendix D.3 for details.
The example shows an ideal case where σmin(ΣF,h) =σmax(ΣF,h) = 1 , when the future fully
determines sh. This can happen when the last observation oHreveals the identity of an earlier latent
statesh. Note that in this case, MF,hM⊤
F,hcan still have poor scaling if the actions and observations
between step handHare sufficiently stochastic, which shows how the weighted 2-norm solution
and analysis improve over the pseudo-inverse one. More generally, ΣF,his the confusion matrix of
making posterior predictions of shfromfhbased on a uniform prior over Sh(see Appendix B.8 for
how to incorporate different priors) with u(fh)⊤/Z(fh)being the posterior, and σmin(ΣF,h)serves
as a measure of how the distribution of future fhhelps reveal the latent state sh.
We now break down the boundedness of Eq.(4) into more interpretable assumptions.
Assumption 7 (L2outcome coverage) .Assume for all h,∥Vπe
S,h∥2
Σ−1
F,h≤CF,V.
Assumption 8 (ΣF,hregularity) .Assume for any fh:∥u(fh)/Z(fh)∥2
Σ−1
F,h≤CF,U.
Proposition 4 (Boundedness of FDVF) .Under Assumptions 7 and 8, VFin Eq.4 satisfies ∥VF∥∞≤p
CF,2:=p
CF,VCF,U.Furthermore, when only Assumption 7 holds, ∥VF,h∥Zh≤p
CF,V,∀h.
See proof in Appendix D.4. Assumption 7 requires that the weighted covariance matrix ΣF,hcovers
the direction of Vπe
S,hwell. As a sanity check, it is always bounded in the on-policy case:
Example 3. When πb=πe,∥Vπe
S,h∥Σ−1
F,h≤H√
S.
Notably, mathematically similar coverage assumptions are also found in the linear MDP literature.
For example, with state-action feature ϕhfor time step h, a very tight coverage parameter for linear
6MDPs is ∥Eπe[ϕh]∥2
Eπb[ϕhϕ⊤
h]−1[Zanette et al., 2021]. Despite the mathematically similarity, there
are important high-level differences between these notions of coverage:
1.As mentioned earlier, MDP coverage is concerned with the dynamics before steph, whereas
our outcome coverage concerns that after h. Relatedly, MDP coverage depends on the initial
distribution (which our outcome coverage does not depend on), and our coverage depends on the
reward function through VF(which MDP coverage does not explicitly depend on). In Section 5,
we will discuss our other coverage assumption (belief coverage), which is more similar to the
MDP coverage in that they are both concerned with the past.
2.The linear MDP coverage assumption is a refinement of state-density ratio using the knowledge of
the function class [Chen and Jiang, 2019, Song et al., 2022]. In comparison, the linear structure of
our outcome-coverage assumption comes directly from the internal structure of POMDPs.
4.2 Addressing Sdependence via L1/L∞Hölder and L∞Outcome Coverage
Example 3 shows that even in the on-policy case, CF,Vmay depend on Swhich makes the assumption
only meaningful for finite and small S. In fact, we showed earlier that VF=R+is a natural and
obvious L∞-bounded solution for πb=πe, but this is not recovered by the construction in Eq. (4).
We also need an additional regularity Assumption 8.
As it turns out, these undesired properties arise because L2Hölder—which is natural for linear MDP
settings mentioned above—fails to leverage the L1normalization of u(fh)⊤/Z(fh)(Proposition 4,
Claim 2) and is loose for POMDPs; see Appendix B.5 for further details. A better choice is
L1/L∞Hölder, motivating the L∞coverage assumption below, which requires a slightly different
construction of VF. These definitions may seem mysterious or even counterintuitive; it will be easier
to explain the intuitions when we get to their counterparts for belief coverage in Section 5.2.
Construction of VFDefine ZR(fh) :=Z(fh)/R+(fh), and we use ZRto replace Zin Eq.(4):
VF,h= (ZR
h)−1M⊤
F,h(ΣR
F,h)−1Vπe
S,h,where ΣR
F,h:=MF,h(ZR
h)−1M⊤
F,h. (5)
Assumption 9 (L∞outcome coverage) .Assume for all h,∥(ΣR
F,h)−1Vπe
S∥∞≤CF,∞.
Lemma 5. Under Assumption 9, ∥VF∥∞≤HCF,∞. See proof in Appendix E.7.
In Appendix B.5 we show that the construction shares similar properties to Eq. (4)in the scenario
of Example 2. On the other hand, it has better scaling properties w.r.t. Sand does not additionally
require a regularity assumption like Assumption 8. In the on-policy case, Eq. (5)exactly recovers
VF=R+, a property that Eq.(4) does not enjoy; see Appendix E.8 for details.
Example 4. When πe=πb,(ΣR
F,h)−1Vπe
S=1, thus Assumption 9 holds with CF,∞= 1
(c.f.CF,V≤H√
Sin Example 3). Furthermore, the construction in Eq. (5)is exactly VF=R+.
5 Effective History Weights and A New Algorithm
We now turn to Q2in Section 3, which asks for a quantitative understanding of the IV(V)term. Note
that IV (V)and Dr V[dπe, dπb], taken together, are to address the conversion between:
(We minimize:)q
Eπb[(BHV)(τh)2]→(We want to bound:) |Eπe[(BSV)(sh)]|. (6)
The two terms differ both in the policy ( πb→πe) and the operator ( BH→ BS). While we could
directly define a parameter by taking the worst-case (over V∈ V) ratio between the two expressions,5
the real question is to provide more intuitive understanding of when it can be bounded.
Towards this goal, Uehara et al. [2022a] split the above ratio into two terms, IV(V)andDrV[dπe, dπb],
which take care of the BH→ BSconversion (under πb) and πb→πeconversion (under BS),
respectively. While this leads to an intuitive upper bound of the πb→πeconversion parameter in
terms of latent state coverage, the nature of the BH→ BSconversion, IV (V), remains mysterious.
5Recent offline RL theory indeed employed such definitions [Xie et al., 2021, Song et al., 2022], but that is
after we developed mature understanding of their properties, such as being upper bounded by density ratios.
7In this section, we take a different approach by directly providing an intuitive upper bound on
both conversions altogether, under a novel belief coverage assumption. In Appendix E.4 we will
also revisit the split into IV(V)andDrV[dπe, dπb]: in the absence of strong structures from V, the
boundedness of IV(V)turns out to require an even stronger version of belief coverage, rendering the
split unnecessary. Furthermore, our approach also leads to a novel algorithm for estimating J(πe)
that replaces Bellman-completeness (Assumption 6) with a weight-realizability assumption, similar
to MIS estimators for MDPs [Liu et al., 2018, Uehara et al., 2020].
5.1 Effective History Weights
The key idea in this section is the notion of effective history weights , that perform the πb→πeand
BH→ BSconversions jointly.
Definition 10 (Effective history weights) .An effective history weight function w⋆:H →Ris any
function that satisfies: ∀V∈ V,h∈[H],
Eπb[w⋆(τh)(BHV)(τh)] =Eπe
(BSV)(sh)
. (7)
We first see that well-bounded w⋆immediately leads to a good conversion ratio for Eq.(6):
|Eπe
(BSV)(sh)
|=|Eπb[w⋆(τh)(BHV)(τh)]| ≤q
Eπb[w⋆(τh)2]Eπb[(BHV)(τ)2],
where the inequality follows from Cauchy-Schwartz for r.v.’s. Hence, all we need is ∥w⋆∥2,dπb
h:=p
Eπb[w⋆(τh)2], theπb-weighted 2-norm of w⋆, to be bounded, and we focus on this quantity next.
Similar to Section 4, there may be multiple w⋆that satisfies the definition, and we only need to show
the boundeness of anysolution. Also similarly, the most obvious solution is w⋆(τh) =Prπe(τh)
Prπb(τh)=
Qh−1
h′=1πe(ah′|oh′)
πb(ah′|oh′), noting that Eπe
(BSV)(sh)
=Eπe
(BHV)(τh)
and importance weighting on
τhchanges EπbtoEπe. However, the use of cumulative importance weights is undesirable given its
exponential nature, causing the history-version of “curse of horizon” Liu et al. [2018].
Construction by Belief Matching. We now show a better construction that is bounded under a
natural belief coverage assumption. Note that Def 10 can be written as:
Eπb[w⋆(τh)⟨b(τh),BS
hV⟩] =Eπe
⟨b(τh),BS
hV⟩
,
whereBS
hV∈RSis the Bellman residual vector for VonSh. As a sufficient condition (which is also
necessary when Vlacks strong structures, i.e., {BS
hV:V∈ V} spans the entire RS), we can find
w⋆that satisfies: Eπb[w⋆(τh)b(τh)] =Eπe[b(τh)] =: bπe
h.This is related to the mean matching
problem in the distribution shift literature [Gretton et al., 2009, Yu and Szepesvári, 2012], and a
standard solution is [Bruns-Smith et al., 2023]:
w⋆(τh) =b(τh)⊤Σ−1
H,hbπe
h, (8)
where ΣH,h:=P
τhdπb(τh)b(τh)b(τh)⊤, andbπe
hcoincides with [dπe(sh)]sh.
The weighted 2-norm of this solution is immediately bounded under the following assumption.
Assumption 11 (L2belief coverage) .Assume ∀h,∥bπe
h∥2
Σ−1
H,h≤CH,2.
Lemma 6. Under Assumption 11, w⋆in Eq. (8)satisfies ∀h,∥w⋆∥2
2,dπb
h≤CH,2.See Appendix E.1.
Assumption 11 requires that the covariance matrix of belief states under πbcovers bπe, the average
belief state under πe. As before, we also check the on-policy case (see Appendix E.2):
Example 5. In the on-policy case ( πb=πe),CH,2≤1.
Algorithm Guarantee Now we have all the pieces to present a fully polynomial version of
Theorem 2 under the proposed coverage assumptions. One subtlety is that the guarantee depends
onCV:= max V∈V∥V∥∞, which is closely related to ∥VF∥∞since we require VF∈ V, but they
are not equal since Vcan include other functions with higher range. To highlight the dependence of
∥VF∥∞on the proposed coverage assumptions, we follow Xie and Jiang [2020] to assume that the
range of the function classes is not much larger than that of the function it needs to capture. A similar
assumption applies for bounding CΞ. The proof of the theorem is deferred to Appendix E.10.
8Theorem 7. Consider the same setting as Theorem 2, and let Assumptions 11 and 9 hold. For some
absolute constant c, further assume CV≤c∥VF∥∞forVFin Eq. (5)andCΞ≤c(∥VF∥∞+ 1) .
W.p.≥1−δ,|J(πe)−ED[bV(f1)]| ≤cH2(CF,∞+ 1)q
CH,2Cµlog(|V||Ξ|/δ)
n.
In addition to Hand complexities of |V|and|Ξ|, the bound only depends on the intuitive coverage
parameters: Cµ(action coverage), CH,2(L2belief coverage), and CF,∞(L∞outcome coverage).
5.2 L∞Belief Coverage
Assumption 11 enables bounded second moment of w⋆(Lemma 6) but does not control ∥w⋆∥∞,
which we show will be useful for the new algorithm in Section 5.3. Here we present the L∞version
of belief coverage that controls ∥w⋆∥∞, which also helps understand L∞outcome coverage given
the symmetry between history and future. As alluded to in Section 4.2 and Appendix B.5, L2Hölder
is inappropriate for controlling the infinity-norm since it does not leverage the L1normalization of
vectors in POMDPs. Instead, we propose the following decomposition based on L1/L∞Hölder:
|w⋆(τh)| ≤ ∥b(τh)∥1∥Σ−1
H,hbπe
h∥∞=∥Σ−1
H,hbπe
h∥∞.This way, we can immediately bound ∥w⋆∥∞
with the following assumption:
Assumption 12 (L∞belief coverage) .Assume ∀h,∥Σ−1
H,hbπe
h∥∞≤CH,∞.Then∥w⋆∥∞≤CH,∞.
Σ−1
H,hbπe
his the inverse of second moment (covariance) multiplying the firstmoment (expectation),
raising the concern that the quantity may be poorly scaled: for example, given bounded (but otherwise
arbitrary) random vector X,E[XX⊤]−1E[X]can go to infinity if we rescale Xby a small constant.
First note that such a pathology cannot happen here because the random vectors ( b(τh)) are L1-
normalized and cannot be arbitrarily rescaled. Below we use a few examples to show that Σ−1
H,hbπe
h
is a very well-behaved quantity, and naturally generalize familiar concepts such as concentrability
coefficient from MDPs [Munos, 2007, Chen and Jiang, 2019, Uehara et al., 2022a].
We start by checking the on-policy case. Perhaps surprisingly, Σ−1
H,hbπe
hhas an exact solution:
Example 6. When πe=πb,Σ−1
H,hbπe
h=1, the all-one vector; see Appendix E.5 for the calculation.
Consequently, Assumption 12 is satisfied with CH,∞= 1.
The next scenario considers when b(τh)is always one-hot, i.e., histories reveal the latent state (this
is analogous to Example 2. L∞coverage reduces to the familiar concentrability coefficient , the
infinity-norm of density ratio as a standard coverage parameter:
Example 7. When b(τh)is always one-hot, ∥Σ−1
H,hbπe
h∥∞= max shdπe(sh)/dπb(sh).
We can also calculate ∥bπe
h∥2
Σ−1
H,hfrom Assumption 11, which equals Eπb[(dπe(sh)/dπb(sh))2]in this
“1-hot belief” scenario. Xie and Jiang [2020] show that this is tighter than max shdπe(sh)/dπb(sh),
and this relation extends elegantly to general belief vectors in our setting:
Lemma 8. ∥bπe
h∥2
Σ−1
H,h≤ ∥Σ−1
H,hbπe
h∥∞.See proof in Appendix E.6.
5.3 New Algorithm
The discovery of effective history weights and its boundedness also lead to a new algorithm analogous
to MIS methods for MDPs [Uehara et al., 2020]. The idea is that since Lemma 1 tells us to minimize
Eπe[(BSV)(sh)], which equals Eπb[w⋆(τh)(BHV)(τh)]from Def 10, we can then use another
function class W ⊂ (H →R)to model w⋆, and approximately solve the following:
argmin
V∈Vmax
w∈WPH
h=1|Eπb[w(τh)(BHV)(τh)]|, (9)
which minimizes an upper bound of Eπb[w⋆(τh)(BHV)(τh)]as long as w⋆∈ W ⊂ (H →R). Since
there is no square inside the expectation, there is no double-sampling issue and we thus do not need
theΞclass and its Bellman-completeness assumption. The h-th term of the loss can be estimated
straightforwardly as
|ED[w(τh) (µ(oh, ah) (rh+V(fh+1))−V(fh))]|. (10)
9We now provide the sample-complexity analysis of the algorithm, using a more general analysis that
allows for approximation errors in VandW.
Assumption 13 (Approximate realizablity) .Assume
min
V∈Vmax
w∈WHX
h=1Eπb
wh(τh)·(BHV)(τh)≤ϵV,
inf
w∈sp(W)max
V∈VHX
h=1Eπb
(w⋆
h(τh)−wh(τh))·(BHV)(τh)≤ϵW.
Instead of measuring how WandVcapture our specific constructions of VFandw⋆, the above
approximation errors automatically allow all possible solutions by measuring the violation of the
equations that define VFandw⋆.
We present the sample complexity bound of our algorithm as follows. Similar to Theorem 7, we
assume that CV:= max V∈V∥V∥∞andCW:= max hsupw∈W∥w∥∞are not much larger than the
corresponding norms of VFandw⋆, respectively. See the proof in Appendix E.11.
Theorem 9. LetbVbe the result of approximating Eq.9 with empirical estimation in Eq. (10). Assume
thatCV≤c∥VF∥∞forVFin Eq. (5), and CW≤c∥w⋆∥∞forw⋆in Eq. (8). Under Assumptions 12,
9, and 13, w.p. ≥1−δ,J(πe)−ED[bV(f1)]≤ϵV+ϵW+cH2CH,∞(CF,∞+ 1)q
Cµlog|V||W|
δ
n.
As a remark, there is also a way to leverage the tighter L2belief coverage, despite that it does not
guarantee bounded ∥w⋆∥∞and only ∥w⋆∥2
2,dπb
h. In particular, if all functions in Whave bounded
∥ · ∥2
2,dπb
h, the estimator in Eq. (10) will have bounded 2nd moment on the data distribution. In this
case, using Median-of-Means estimators [Lerasle, 2019, Chen, 2020] instead of plain averages for
Eq.(10) will only pay for the 2nd moment and not the range.
6 Conclusion and Future Work
The main text considers memoryless policies. Similar to Uehara et al. [2022a], we can extend to
policies that depend on recent observations and actions (or memory ). In fact, we provide a more
general result in Appendix B.6 that handles recurrent policies that are finite state machines , which
allows the policy to depend on long histories. However, the coverage coefficient will be diluted
quickly when the memory contains rich information, which we call the curse of memory . We suspect
that structural policies are needed to avoid the curse of memory and leave this to future work.
Acknowledgements
Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781,
Google Scholar Award, and Sloan Fellowship.
References
András Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with Bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning ,
2008.
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning
of pomdps using spectral methods. In Conference on Learning Theory , pages 193–256. PMLR,
2016.
David Bruns-Smith, Oliver Dukes, Avi Feller, and Elizabeth L Ogburn. Augmented balancing weights
as linear regression. arXiv preprint arXiv:2304.14545 , 2023.
Fan Chen, Yu Bai, and Song Mei. Partially observable rl with b-stability: Unified structural condition
and sharp sample-efficient algorithms. arXiv preprint arXiv:2209.14990 , 2022.
10Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
Proceedings of the 36th International Conference on Machine Learning , pages 1042–1051, 2019.
Yen-Chi Chen. A short note on the median-of-means estimator, 2020.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International
Conference on Machine Learning , pages 1133–1142, 2018.
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Lang-
ford. Provably efficient RL with Rich Observations via Latent State Decoding. In International
Conference on Machine Learning , pages 1665–1674, 2019.
Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function
approximation. In International Conference on Machine Learning , pages 2701–2709. PMLR,
2020.
Miroslav Dudík, John Langford, and Lihong Li. Doubly Robust Policy Evaluation and Learning. In
Proceedings of the 28th International Conference on Machine Learning , pages 1097–1104, 2011.
Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi. Provable reinforcement
learning with a short-term memory. In International Conference on Machine Learning , pages
5832–5850. PMLR, 2022.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research , 6:503–556, 2005.
Benjamin Eysenbach, Swapnil Asawa, Shreyas Chaudhari, Sergey Levine, and Ruslan Salakhutdinov.
Off-dynamics reinforcement learning: Training for transfer with domain classifiers. arXiv preprint
arXiv:2006.13916 , 2020.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust off-
policy evaluation. In International Conference on Machine Learning , pages 1447–1456. PMLR,
2018.
Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, Bernhard
Schölkopf, et al. Covariate shift by kernel mean matching. Dataset shift in machine learning , 3(4):
5, 2009.
Hongyi Guo, Qi Cai, Yufeng Zhang, Zhuoran Yang, and Zhaoran Wang. Provably efficient offline
reinforcement learning for partially observable markov decision processes. In International
Conference on Machine Learning , pages 8016–8038. PMLR, 2022.
Zhaohan Daniel Guo, Shayan Doroudi, and Emma Brunskill. A pac rl algorithm for episodic pomdps.
InArtificial Intelligence and Statistics , pages 510–518. PMLR, 2016.
Mao Hong, Zhengling Qi, and Yanxun Xu. A policy gradient method for confounded pomdps. arXiv
preprint arXiv:2305.17083 , 2023.
Yuchen Hu and Stefan Wager. Off-policy evaluation in partially observed markov decision processes
under sequential ignorability. The Annals of Statistics , 51(4):1561–1585, 2023.
Ruiquan Huang, Yingbin Liang, and Jing Yang. Provably efficient ucb-type algorithms for learning
predictive state representations. arXiv preprint arXiv:2307.00405 , 2023.
Nan Jiang and Lihong Li. Doubly Robust Off-policy Value Evaluation for Reinforcement Learning.
InProceedings of the 33rd International Conference on Machine Learning , volume 48, pages
652–661, 2016.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Con-
textual Decision Processes with low Bellman rank are PAC-learnable. In Proceedings of the 34th
International Conference on Machine Learning , volume 70, pages 1704–1713, 2017.
11Chi Jin, Sham Kakade, Akshay Krishnamurthy, and Qinghua Liu. Sample-efficient reinforcement
learning of undercomplete pomdps. Advances in Neural Information Processing Systems , 33:
18530–18539, 2020.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially
observable stochastic domains. Artificial intelligence , 101(1-2):99–134, 1998.
Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy eval-
uation in markov decision processes. Journal of Machine Learning Research , 21(167):1–63,
2020.
Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich
observations. In Advances in Neural Information Processing Systems , 2016.
Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. Rl for latent mdps:
Regret guarantees and a lower bound. Advances in Neural Information Processing Systems , 34:
24523–24534, 2021.
Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar. Adaptive control
and regret minimization in linear quadratic gaussian (lqg) setting. In 2021 American Control
Conference (ACC) , pages 2517–2522. IEEE, 2021.
Hoang Le, Cameron V oloshin, and Yisong Yue. Batch policy learning under constraints. In Interna-
tional Conference on Machine Learning , pages 3703–3712, 2019.
Matthieu Lerasle. Lecture notes: Selected topics on robust statistical learning theory. arXiv preprint
arXiv:1908.10761 , 2019.
Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased Offline Evaluation of Contextual-
bandit-based News Article Recommendation Algorithms. In Proceedings of the 4th International
Conference on Web Search and Data Mining , pages 297–306, 2011.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems , pages
5356–5366, 2018.
Qinghua Liu, Alan Chung, Csaba Szepesvári, and Chi Jin. When is partially observable reinforcement
learning not scary? In Conference on Learning Theory , pages 5175–5220. PMLR, 2022a.
Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin. Optimistic mle–a generic
model-based algorithm for partially observable sequential decision making. arXiv preprint
arXiv:2209.14997 , 2022b.
Miao Lu, Yifei Min, Zhaoran Wang, and Zhuoran Yang. Pessimism in the face of confounders:
Provably efficient offline reinforcement learning in partially observable markov decision processes.
arXiv preprint arXiv:2205.13589 , 2022.
Gaurav Mahajan, Sham Kakade, Akshay Krishnamurthy, and Cyril Zhang. Learning hidden markov
models using conditional samples. In The Thirty Sixth Annual Conference on Learning Theory ,
pages 2014–2066. PMLR, 2023.
Rémi Munos. Performance bounds in l_p-norm for approximate value iteration. SIAM journal on
control and optimization , 46(2):541–561, 2007.
Rémi Munos and Csaba Szepesvári. Finite-time bounds for fitted value iteration. Journal of Machine
Learning Research , 9(May):815–857, 2008.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. Advances in Neural Information Processing Systems ,
32, 2019.
Yash Nair and Nan Jiang. A spectral approach to off-policy evaluation for pomdps. arXiv preprint
arXiv:2109.10502 , 2021.
12Hongseok Namkoong, Ramtin Keramati, Steve Yadlowsky, and Emma Brunskill. Off-policy policy
evaluation for sequential decisions under unobserved confounding. Advances in Neural Information
Processing Systems , 33:18819–18831, 2020.
Juan C Perdomo, Akshay Krishnamurthy, Peter Bartlett, and Sham Kakade. A complete characteriza-
tion of linear estimators for offline policy evaluation. Journal of Machine Learning Research , 24
(284):1–50, 2023.
Doina Precup, Richard S Sutton, and Satinder P Singh. Eligibility traces for off-policy policy
evaluation. In Proceedings of the Seventeenth International Conference on Machine Learning ,
pages 759–766, 2000.
Chengchun Shi, Masatoshi Uehara, Jiawei Huang, and Nan Jiang. A minimax learning approach to off-
policy evaluation in confounded partially observable markov decision processes. In International
Conference on Machine Learning , pages 20057–20094. PMLR, 2022.
Max Simchowitz, Karan Singh, and Elad Hazan. Improper learning for non-stochastic control. In
Conference on Learning Theory , pages 3320–3436. PMLR, 2020.
Yuda Song, Yifei Zhou, Ayush Sekhari, Drew Bagnell, Akshay Krishnamurthy, and Wen Sun.
Hybrid rl: Using both offline and online data can make rl efficient. In The Eleventh International
Conference on Learning Representations , 2022.
Guy Tennenholtz, Uri Shalit, and Shie Mannor. Off-policy evaluation in partially observable envi-
ronments. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages
10276–10283, 2020.
Philip Thomas and Emma Brunskill. Data-Efficient Off-Policy Policy Evaluation for Reinforcement
Learning. In Proceedings of the 33rd International Conference on Machine Learning , 2016.
Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax Weight and Q-Function Learning for
Off-Policy Evaluation. In Proceedings of the 37th International Conference on Machine Learning ,
pages 1023–1032, 2020.
Masatoshi Uehara, Haruka Kiyohara, Andrew Bennett, Victor Chernozhukov, Nan Jiang, Nathan
Kallus, Chengchun Shi, and Wen Sun. Future-dependent value-based off-policy evaluation in
pomdps. arXiv preprint arXiv:2207.13081 , 2022a.
Masatoshi Uehara, Ayush Sekhari, Jason D Lee, Nathan Kallus, and Wen Sun. Provably efficient re-
inforcement learning in partially observable dynamical systems. arXiv preprint arXiv:2206.12020 ,
2022b.
Cameron V oloshin, Nan Jiang, and Yisong Yue. Minimax model learning. In International Conference
on Artificial Intelligence and Statistics , pages 1612–1620. PMLR, 2021.
Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A
theoretical comparison. In Conference on Uncertainty in Artificial Intelligence , pages 550–559.
PMLR, 2020.
Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In
International Conference on Machine Learning , pages 11404–11413. PMLR, 2021.
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Towards optimal off-policy evaluation for reinforce-
ment learning with marginalized importance sampling. Advances in Neural Information Processing
Systems , 32, 2019.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. arXiv preprint arXiv:2106.06926 , 2021.
Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with
pessimism. Advances in neural information processing systems , 34:4065–4078, 2021.
Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline policy
evaluation for reinforcement learning. In International Conference on Artificial Intelligence and
Statistics , pages 1567–1575. PMLR, 2021.
13Yao-Liang Yu and Csaba Szepesvári. Analysis of kernel mean matching under covariate shift. In
Proceedings of the 29th International Coference on International Conference on Machine Learning ,
pages 1147–1154, 2012.
Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods
for offline reinforcement learning. Advances in neural information processing systems , 34:13626–
13640, 2021.
Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. Pac reinforcement learning for
predictive state representations. arXiv preprint arXiv:2207.05738 , 2022.
Junzhe Zhang and Elias Bareinboim. Markov decision processes with unobserved confounders: A
causal approach. Purdue AI Lab, West Lafayette, IN, USA, Tech. Rep , 2016.
14A Related Literature
OPE in POMDPs. There is a line of research on OPE in confounded POMDPs [Zhang and
Bareinboim, 2016, Namkoong et al., 2020, Nair and Jiang, 2021, Guo et al., 2022, Lu et al., 2022,
Shi et al., 2022, Hong et al., 2023]. However, all these methods require the behavior policy to
solely depend on the latent state. This assumption is inapplicable for our unconfounded POMDP
setting, where the behavior policy depends on the observations. Hu and Wager [2023] studied the
same unconfounded setting as ours. Nonetheless, their method uses multi-step importance sampling,
leading to an undesirable exponential dependence on the horizon. The closest related work to our
paper is [Uehara et al., 2022a], which we analyze in detail in Section 3.
Online Learning in POMDPs. There are many prior works studying online learning algorithms
for sub-classes of POMDPs, including decodable POMDPs [Krishnamurthy et al., 2016, Jiang et al.,
2017, Du et al., 2019, Efroni et al., 2022], Latent MDPs [Kwon et al., 2021] and linear quadratic
Gaussian setting (LQG) [Lale et al., 2021, Simchowitz et al., 2020]. Recently, online learning
algorithms with polynomial sample results have been proposed for both tabular POMDPs [Guo et al.,
2016, Azizzadenesheli et al., 2016, Jin et al., 2020, Liu et al., 2022a] and POMDPs with general
function approximation [Zhan et al., 2022, Liu et al., 2022b, Chen et al., 2022, Huang et al., 2023].
All of these approaches are model-based and require certain model assumptions. Uehara et al. [2022b]
proposed a model-free online learning algorithm based on future-dependent value functions (FDVFs).
However, their algorithm requires the boundness of FDVFs for all policies in the policy class, as well
as a low-rank property of the Bellman loss, which limits the generality of the results.
OPE in MDPs. There has been a long history of studying OPE in MDPs, including importance
sampling (IS) approaches [Precup et al., 2000, Li et al., 2011] and their doubly robust variants [Dudík
et al., 2011, Jiang and Li, 2016, Thomas and Brunskill, 2016, Farajtabar et al., 2018], marginalized
importance sampling (MIS) methods [Liu et al., 2018, Xie et al., 2019, Kallus and Uehara, 2020] and
model-based estimators [Eysenbach et al., 2020, Yin et al., 2021, V oloshin et al., 2021]. As mentioned
in Section 1, directly applying IS-based approaches for POMDPs will result in exponential variance.
Meanwhile, MIS-based approaches do not apply to POMDPs since they require the environment to
satisfy the Markov assumption.
B Discussions and Extensions
B.1 Connection between Future-dependent Value Functions and Learnable
Future-dependent Value Functions
Uehara et al. [2022a] proposed a learnable version of future dependent value functions and focused
on the learning of learnable FDVFs instead of the original FDVFs defined in Definition 3. In this
subsection, we first introduce the definition of learnable FDVFs and then show that it is equivalent to
FDVFs under a full-rank assumption.
Definition 14 (Learnable future-dependent value functions) .A Learnable future-dependent value
function VL
F:F →Ris any function that satisfies the following: ∀τh,
(BHVL
F)(τh) = 0 .
For any FDVF VF, since BSVF≡0, we have ∀τh,
(BHVF)(τh) =Eπb[µ(oh, ah){rh+VF(fh+1)} −VF(fh)|τh]
=⟨b(τh),BS
hVF⟩= 0.
Therefore, all FDVFs are also learnbale FDVFs. On the other hand, for any learnbale FDVF VL
F,
letBH
hVL
F∈R|Hh|be the Bellman residual vector for VL
FonHhandBS
hVL
F∈RSbe the Bellman
residual vector on Sh. With the help of MH,h, we have the following relation between BH
hVL
Fand
BS
hVL
F:
BH
hVL
F= (MH,h)⊤BS
hVL
F.
When rank(MH,h) =S,BH
hVL
F=0if and only if BS
hVL
F=0. Therefore, VL
Fsatisfies that
∀sh,(BSVL
F)(sh) = 0 , implying that VL
Fis also a FDVF. Under the regular full rank assumption,
the two definitions are actually equivalent.
15B.2 Comparison between the Finite-horizon and the Discounted Formulations
The original results of Uehara et al. [2022a] were given in the infinite-horizon discounted setting. To
describe their data collection assumption in simple terms, consider an infinite trajectory that enters the
stationary configuration of πb, and we pick a random time step and call it t= 0(which corresponds
to a time step hin our setting). Then, history (analogous to our τh) is(o(−MF):(−1), a(−MF):(−1)),
and future is (o0:MH, a0:MF−1).MFandMHare hyperparameters that determines the lengths, and
should not be confused with our MF,handMH,hmatrices. Data points collected in this way form the
“transition” dataset. Separately, there is an “initial” dataset of (o0:MF−1, a0:MF)tuples to represent
the analogue of initial state distribution in MDPs.
Besides the fact that finite-horizon formulation better fits real-world applications with episodic nature
(e.g., a session in conversational system), there are mathematical reasons why the finite-horizon
formulation is more natural:
1.Unlike the infinite-horizon setting, we do not need separate “trainsition” and “initial” datasets. The
natural H-step episode dataset plays both roles. We also do not need the stationarity assumption.
2.In the infinite-horizon, MFandMHare hyperparameters which do not have an obvious upper
bound. The larger they are, it is easier to satisfy certain identification assumptions (such as
full-rankness of the counterparts of our MH,handMF,h). On the other hand, the “curses of future
and horizon” we identified in this work corresponds to exponential dependence on MFandMH,
so there is a potential trade-off in the choice of MFandMH. In contrast, the future and the history
have maximum lengths ( H−handh−1at time step h) in our setting. As we establish coverage
assumptions that avoid potential exponential dependencies on these lengths, the trade-off in the
choice of length is eliminated, so it is safe for us to always choose the maximum length, as we do
in the paper.
B.3 Refined Coverage
As alluded to at the beginning of Section 5, we can tighten the definition of L2belief coverage by
directly using the ratio:
sup
V∈V|Eπe[(BSV)(sh)]|p
Eπb[(BHV)(τh)2],
which automatically leverages the structure of V. For example, if {BS
hV:V∈ V} only occupies
a low-dimensional subspace of RSh(that is, there exists ϕ:Sh→Rdsuch that (BSV)(sh) =
ϕ(sh)⊤θV), then belief matching above Eq. (8)can be done in Rdinstead of RSh. Other coverage
definitions may be refined in similar manners.
B.4 Interpretation of Assumption 8
To interpret Assumption 8, let x=u(fh)/Z(fh)∈∆(S), and assign a distribution over xby
sampling fh∼diag(Zh/S), then Assumption 8 becomes x⊤EZh/S[xx⊤]−1x≤CF,USfor any x
with non-zero probability. This is a common regularity assumption [Duan et al., 2020, Perdomo et al.,
2023], requiring that no xpoints to a direction in RSalone without being joined by others from the
distribution. While 1/σmin(ΣF,h)can bound this quantity, the other way around is not true, implying
that Assumption 8 is weaker.
Example 8 (Bounded CF,Uwithout bounded 1/σmin(ΣF,h)).Consider a distribution over xwhere
with 0.5 probability, x= [1/2+ϵ,1/2−ϵ]⊤, and with the other 0.5 probability, x= [1/2−ϵ,1/2+ϵ]⊤.
Then, as ϵ→0,1/σmin(ΣF,h)→ ∞ , butx⊤E[xx⊤]−1x≤2.
B.5 Intuition for L∞Outcome Coverage
Here we provide more details about the L∞outcome coverage in Section 4.2, which are omitted in
the main text due to space limit.
Looseness of L2outcome coverage We start by examining the looseness of L2outcome coverage
(Assumption 7), which will motivate the L∞version of outcome coverage. To develop intuitions, we
16first examine the boundedness of Eq.(4) (the construction that leads to L2outcome coverage) in the
setting of Example 2. In this example, ΣF,h=I, and∥VF∥∞≤H. According to the L2Hölder
decomposition in Proposition 4, however,
∥VF∥∞≤max
fh∥u(fh)/Z(fh)∥2∥Vπe
S∥2≤H√
S,
where ∥ · ∥Σ−1
F,his replaced by ∥ · ∥ 2since ΣF,h=I. In contrast, the tight analysis is
∥VF∥∞≤max
fh∥u(fh)/Z(fh)∥1∥Vπe
S∥∞≤H.
The comparison clearly highlights that the looseness in Scomes from the fact that L2Hölder does
not fully leverage the fact that ∥u(fh)/Z(fh)∥1= 1and loosely relaxing it to ∥u(fh)/Z(fh)∥2≤1.
In contrast, our L∞coverage assumption allows for a more natural L1/L∞Hölder to leverage the
L1normalization of u(fh)/Z(fh).
L∞Outcome Coverage Similar to L∞belief coverage, we could define L∞outcome coverage
simply as ∥(ΣF,h)−1Vπe
S∥∞. The small caveat and inelegance is that it does not recover VF=R+
when πb=πe. To address this, we leverage the lesson from belief coverage (Section 5.2): to obtain
CH,∞= 1when πb=πe, a key property is that ΣH,h(data covariance matrix) and bπb
h(the vector
to be covered) are the covariance matrix and the mean vector w.r.t. the same distribution of belief
vectors. In contrast, for the outcome coverage case, Vπe
Sdepends on the reward function, but such
information is missing in ΣF,h, which prevents a perfect cancellation in the on-policy case.
Therefore, we can adjust the definition of ΣF,hto mimic the situation of belief coverage. The first
step is to find the counterpart of belief state which is L1-normalized. This obviously corresponds
toZ−1
hM⊤
F,h, whose rows sum up to 1. Let ¯u(fh) := u(fh)/Z(fh)∈∆(Sh), then ΣF,h=
S·EZh/S[¯u¯u⊤]. Then, by incorporating reward information into ΣF,h, we arrive at the solution in
Eq.(5).
Finally, we examine the setting of Example 2 and show that VFin Eq. (5)is similarly well-behaved
as Eq.(4) in this scenario.
Example 9. In the same setting as Example 2, i.e., fhalways reveals sh, we have ΣR
F,h=diag(Vπb
Sh),
and∥(ΣR
F,h)−1Vπe
S∥∞= max shVπe
S(sh)/Vπb
S(sh).
This example also closely resembles Example 7 for belief coverage. While (ΣR
F,h)−1may behave
poorly if Vπb
S(sh)is small, an easy fix is to simply add a constant to the reward function and shift its
range to e.g., [1, 2], which ensures a small CF,∞.
B.6 Extension to History-dependent Policies
The main text assumes memoryless πbandπe. Similar to Uehara et al. [2022a], we can extend the
approach to handle history-dependent policies (the changes to the proof are sketched below). Instead
of rewriting the proof with these changes, we will provide a “black-box” reduction that handles the
extension more elegantly and allow for more general results than Uehara et al. [2022a].
Memory-based Policies and Changes in the Proofs Define memory mhas a function of τh
(i.e.,mh=mh(τh)), and a memory-based policy can depend on (mh, oh). Uehara et al. [2022a]
allows for mh= (oh−M:h−1, ah−M:h−1)for some fixed window Min their analyses. If we let
mh= (o1:h−1, a1:h−1), we recover fully general history-dependent policies.
If we direct modify the proofs to accommodate this generalization (as done in Uehara et al. [2022a]),
the required changes are:
•Vπe
SandVFneed to additionally depend on mhto be well defined. V∈ V also generally depend
onmhsince we need realizability VF∈ V.
•MF,handMH,hhave rows indexed by (sh, mh)instead of just sh. That is, we replace belief state
with posterior over (sh, mh). Similarly, entries in MF,hare now future probabilities conditioned
on(sh, mh)
17However, if we consider latent state coverage, which is weaker than belief coverage that is really
needed (Appendix E.3), we now require boundeddπe(sh,mh)
dπb(sh,mh). This is generally exponential in the
length of mh(e.g., if M=h−1, the ratio is exactly the cumulative importance weight), preventing
us from handling πbandπewith long-range history dependencies.
Reduction to Memoryless Case Instead of changing the proofs, we now describe an alternative
approach that (1) produces results similar to Uehara et al. [2022a], and (2) handles more general
recurrent policies that are finite-state-machines (FSMs), which subsume fully history-dependent
policies (or policies that depend on a fixed-length window) as special cases.
Concretely, a policy with memory mhis said to be an FSM, if mh+1can be computed solely based
onmh, oh, ah, without using other information in τh.mh= (oh−M:h−1, ah−M:h−1)satisfies this
definition, as computing mh+1is simply dropping the oldest observation-action pair from mhand
appending the newest one.6Another example is belief update, where b(τh+1)can be computed from
b(τh), oh, ah.
We assume that both πbandπehave the same memory; if they differ, we can simply concatenate their
memories together. Then, handling memories in our analyses takes two steps:
1.We allow latent state transition to depend on oh, that is, sh+1∼T(·|sh, ah, oh). This model has
been considered by Jiang et al. [2017] to unify POMDPs and low-rank MDPs. Our analyses
hold as-is without any changes. To provide some intuition: the key property that enables the
analyses of FDVF is that shis a bottleneck that separates histories from futures, which enables
(BHV)(τh) =⟨b(τh),BS
hV⟩in Eq. (2). This property is intact with the additional dependence of
shonoh−1.
2.We provide a blackbox reduction from the memory-based case to the memoryless case. Define a
new POMDP that is equivalent to the original one, where the latent state is ˜sh= (sh, mh). The
observation is ˜oh= (oh, mh), which can be emitted from ˜shsince ˜shcontains mh. The latent
state transition is ˜sh+1= (sh+1, mh+1), where sh+1∼T(·|sh, ah, oh), and mh+1is updated
from mh(contained in ˜sh),ah, and oh; this is why we need ohto participate in latent transitions.
Now it suffices to perform OPE in the new POMDP. Data from the original POMDP can be
converted to that of the new POMDP with ˜oh= (oh, mh).πbandπeonly need to depend on ˜oh
and become memoryless.
Given the reduction, if we design function classes that operate on the histories and futures of the new
POMDP, the guarantees in the main text immediately hold. The final step is to translate the objects
(e.g., futures and histories) and guarantees in the new POMDP back to the original POMDP for
interpretability. Note that the history in the new POMDP is ˜τh= (˜o1:h−1, a1:h−1), which contains
the same information as τh, so functions in ΞandWcan still operate on τh. Similarly, V∈ V now
takes (mh, fh)as input, which is consistent with Uehara et al. [2022a].
When translating the assumptions back to the original POMDP, we can see that now the results can
be well-behaved when mhis “simple”. For example, if mhtakes values from a constant-sized space,
dπe(sh, mh)/dπb(sh, mh)(which lower-bounds belief coverage as discussed above) may not blow
up exponentially, even though mhcan hold information that is arbitrarily old (e.g., it remembers
one bit of information from h= 1). This is a scenario that cannot be handled by the formulation of
Uehara et al. [2022a].
However, the guarantee can still deteriorate when the policies maintain rich memories, even if these
memories are highly structured, such as mh=b(τh). Under the mild assumption that all histories
lead to distinct belief states b(τh)(they can be very close in RSand just need to be not exactly
identical), the belief state in the new POMDP, ˜b(τh), completely ignores the linear structure of mh
and treats it in the same way as mh=τh,7leading to an exponentially large belief coverage. How to
handle policies that depend on rich but highly structured memories such as belief states is a major
open problem.
6When h≤Mno dropping is needed. In fact, if we never drop, then mh=τhis just the history.
7eb(τh)has a “block one-hot” structure, where the block of size Sindexed by mh(τh)is equal to b(τh), and
all other entries are 0.
18B.7 Recovering MDP Algorithms and Analyses
One somewhat undesirable property of our algorithms and analyses is that they do not subsume MDP
algorithms/analyses as a special case. MDPs can be viewed as POMDPs with identity emission, i.e.,
oh=sh. In this case, algorithms considered in this paper are analogous to their MDP counterparts
Uehara et al. [2020], except that the MDP algorithms require that all functions V,w,ξto operate on
the current state sh. In contrast, our FDVF operates on fh= (oh, ah, . . . , o H, aH)which includes
oh(=sh). To recover the MDP algorithm as a special case, we can choose Vsuch that every V∈ V
only depends on fhthrough oh. However, wandξoperate on τh= (o1, a1, . . . , o h−1, ah−1)which
does not contain oh. This makes subsuming the MDP case difficult.
Here we describe briefly how to overcome this issue by slightly modifying our analysis; the changes
are somewhat similar to Appendix B.6. Instead of letting τh= (o1, a1, . . . , o h−1, ah−1), we can
define an alternative notion of history ˜τh= (o1, a1, . . . , o h)and replace τhin the main text with
˜τh. Algorithmically, we can immediately recover MDP algorithms by also restricting functions in
WandXto only operate on oh. However, our analyses (which apply to general POMDPs) need
to change accordingly, as replacing τhwith ˜τhwill break the key properties, such as BHVbeing
linear in BSV. The problem is that in the definitions of Vπe
SandBSVwe want to marginalize out
the randomness of ohgiven sh, which is in conflict with conditioning on ˜τhthat includes all the
information of oh. To address this, we simply replace shwith˜sh:= (sh, oh)in the definition of Vπe
S,
BSV,MH,h, and MF,h. That is, Vπe
Sis now a function of ˜shand also depends on oh,b(τh)is the
posterior distribution over ˜sh, and the entries of the outcome matrix MF,hisPrπb(fh|sh, oh). This
retains the key property that BHVis linear in BSVwith˜b(τh)as the coefficient.
When we specialize the guarantees of this modified analysis to the MDP setting, outcome coverage is
always satisfied and we can always use the MDP’s standard value function as VF. For belief coverage,
note that (BHV)(τh) = (BSV)(sh, oh), which is simply the standard definition of Bellman error
in MDPs (since sh=oh), which we denote as (BV)(sh). Plugging this into the refined coverage
discussed in Appendix B.3, we recover the standard definition of coverage in the MDP setting, namely
supV∈V|Eπe[(BSV)(sh)]|√
Eπb[(BSV)(sh)2].
B.8 Incorporating Different Latent-State Priors in VF
In Section 4 we showed that ΣF,h, which is crucial to the construction in Eq. (4), can be viewed as the
confusion matrix of making posterior predictions of shfromfh, using a uniform prior. The uniform
prior corresponds to the all-one vector 1Sin the definition of Zh:= diag( 1⊤
SMF,h), which naturally
leads to the question of whether we can incorporate a different and perhaps more informative prior.
Letph∈∆(Sh)be the prior we would like to use instead of the uniform prior. An immediate
idea is to define Zph
h= diag( p⊤
hMF,h)(possibly up to a Sscaling factor, as the uniform prior
ispunif= [1/S,···,1/S]⊤and1S=Spunif) and directly plug it into the construction in Eq. (4).
However, this breaks some of the key properties of the current construction of ΣF,h, such as L1
normalization of rows of Z−1
hM⊤
F,h.
To resolve this, the key is to realize that the rows of Z−1
hM⊤
F,hareL1normalized because they can be
viewed as posteriors over Sh. In comparison, if we examine the (fh, sh)-th entry of (Zph
h)−1M⊤
F,h,
it is
Prπb[fh|sh]P
s′∈ShPrπb[fh|s′]ph(s′),
so clearly it is missing a ph(sh)term on the numerator to be a proper posterior. Inspired by this
observation, we can see how to fix the construction now: recall that VFneeds to satisfy MF,hVF,h=
Vπe
S,h. Ifph>0, then this is equivalent to
(diag( ph)MF,h)VF,h= diag( ph)Vπe
S,h.
Then, the minimum Zph
h-weighted solution of this equation will provide the desired construction that
preserves the L1normalization properties.
Which prior phto use? For the initial time step h= 1, the initial latent-state distribution d1is
the most natural candidate for the prior. If d1(s1) = 0 for some s1∈ S1, incorporating d1as the
19prior will essentially treat s1as non-existent and ignore the outcome coverage of πboverπefroms1,
which is reasonable because s1will not be activated by neither πeandπb. For h >1, the answer is
less clear, and natural candidates include ph=dπb
handdπe
h, or perhaps their probability mixture. For
the example scenarios examined in the main text, these choices (or even a uniform prior) do not make
significant differences, and we leave the investigation of which prior is the best to future work.
C Proofs for Section 3
C.1 Proof of Lemma 1
The RHS of the lemma statement is
HX
h=1Eπe
(BSV)(sh)
=HX
h=1Esh∼dπe
hh
Eah∼πeah+1:H∼πb[rh+V(fh+1)|sh]−Eπb[V(fh)|sh]i
.
Now notice that the expected value of the V(fh+1)term for his the same as that of the V(fh)
term for h+ 1, since both can be written as Esh+1∼dπe
h+1[Eπb[V(fh+1)|sh+1]]. After telescoping
cancellations, what remains on the RHS is
HX
h=1Esh∼dπe
hh
Eah∼πeah+1:H∼πb[rh|sh]i
−Es1[Eπb[V(f1)|s1]] =J(πe)−Eπb[V(f1)].
C.2 Proof of Theorem 2
The proof follows similar idea from Uehara et al. [2022a]. Let GhV=µ(oh, ah)(rh+V(fh+1))−
V(fh), we have
bV= argmin
V∈Vmax
ξ∈ΞHX
h=1ED
(GhV)2−(GhV−ξ(τh))2
.
For any fixed V, we define
bξV= argmax
ξ∈Ξ−HX
h=1EDh
(GhV−ξ(τh))2i
.
Analysis of Inner Maximizer. We observe that for any h∈[H],
Eπb
(ξ(τh)− GhV)2−((BHV)(τh)− GhV)2
=Eπb
(ξ(τh)−(BHV)(τh))2
.
LetXh= (ξ(τh)−GhV)2−((BHV)(τh)−GhV)2andX=PH
h=1Xh. Let ¯C= max {1+CV, CΞ},
since Cµ≥1, we have |ξ(τh)| ≤¯C,|GhV| ≤3Cµ¯CandBHV(τh)≤3¯C. Therefore, we have
|Xh| ≤40Cµ¯C2and|X| ≤40HCµ¯C2. We observe that
Eπb[X2
h]
≤Eπbh
((BHV)(τh)−ξ(τh))2 
(BHV)(τh) +ξ(τh)−2GhV2i
≤Eπbh 
(BHV)(τh)−ξ(τh)2 
30¯C2+ 12(GhV)2i
≤Eπbh 
(BHV)(τh)−ξ(τh)2 
54¯C2+ 96 ¯C2µ(oh, ah)2i
≤150¯C2CµEπbh 
(BHV)(τh)−ξ(τh)2i
.
Hence, for the variance of X, we have
Varπb[X]≤HHX
h=1Eπb
X2
h
≤150H¯C2CµHX
h=1Eπbh 
(BHV)(τh)−ξ(τh)2i
.
20From Bernstein’s inequality, with probability at least 1−δ/2,∀V∈ V,∀ξ∈Ξ, we have
HX
h=1{ED−Eπb}[(ξ(τh)− GhV)2−((BHV)(τh)− GhV)2]
≤s
150H¯C2CµPH
h=1Eπb[((BHV)(τh)−ξ(τh))2] log(4|V||Ξ|/δ)
n+40H¯C2Cµlog(4|V||Ξ|/δ)
n.
(11)
From Bellman completeness assumption BHV ⊂Ξ, we also have
HX
h=1ED[(bξV− GhV)2−((BHV)(τh)− GhV)2]≤0. (12)
Therefore, we obtain that
HX
h=1Eπb[((BHV)(τh)−bξV(τh))2]
=HX
h=1Eπb[(bξV(τh)− GhV)2−((BHV)(τh)− GhV)2]
≤HX
h=1{ED−Eπb}[(bξV(τh)− GhV)2−((BHV)(τh)− GhV)2]+HX
h=1ED[(bξV(τh)− GhV)2−((BHV)(τh)− GhV)2]
≤HX
h=1{ED−Eπb}[(bξV(τh)− GhV)2−((BHV)(τh)− GhV)2]
≤s
150H¯C2CµPH
h=1Eπb[((BHV)(τh)−bξV(τh))2] log(4|V||Ξ|/δ)
n+40H¯C2Cµlog(4|V||Ξ|/δ)
n.
The second inequality is from Equation (12) and the last inequality is from Equation (11). We then
have
HX
h=1Eπb[((BHV)(τh)−bξV(τh))2]≤εstat, ε stat:=225H¯C2Cµlog(4|V||Ξ|/δ)
n. (13)
Combining Equation (11) and Equation (13), we have
HX
h=1{ED−Eπb}[(bξV(τh)− GhV)2−((BHV)(τh)− GhV)2]≤εstat. (14)
Hence,
HX
h=1ED[(bξV(τh)− GhV)2]−HX
h=1ED[((BHV)(τh)− GhV)2]
≤HX
h=1Eπb[(bξV(τh)− GhV)2]−HX
h=1Eπb[((BHV)(τh)− GhV)2]+ 2εstat
=HX
h=1Eπb[((BHV)(τh)−bξV(τh))2]+ 2εstat≤3εstat.
The first inequality is from Equation (14) and the last step is from Equation (13).
21Analysis of Outer Minimizer. For any future-dependent value function VF, from optimality of bV
and the convergence of inner maximizer, we have
HX
h=1ED[(GhbV)2−(GhbV−((BHbV))(τh))2]≤HX
h=1EDh
(GhbV)2−(GhbV−bξbV)2i
+ 3εstat.
≤HX
h=1EDh
(GhVF)2−(GhVF−bξVF)2i
+ 3εstat
≤HX
h=1ED
(GhVF)2−(GhVF−(BHVF)(τh))2
+ 6εstat
= 6εstat. (15)
The last step is from that (BHVF)(τh) = 0 . For any τh, we observe that ∀V∈ V andh∈[H],
Eπb[(GhV)2−(GhV−(BHV)(τh))2]
=Eπb[−(BHV)(τh)2+ 2(BHV)(τh)GhV]
=Eπb[(BHV)(τh)2].
For any fixed V∈ V, letYh= (GhV)2−(GhV−(BHV)(τh))2andY=PH
h=1Yh, we have
|Yh| ≤27¯C2Cµand|Y| ≤27H¯C2Cµ. We observe that
Eπb[Y2
h] =Eπb[(2GhV−(BHV)(τh))2(BHV)(τh)2]
≤Eπb[(18¯C2+ 4(GhV)2)(BHV)(τh)2]
≤Eπb[(26¯C2+ 32 ¯C2µ(oh, ah)2)(BHV)(τh)2]
≤58¯C2CµEπb[(BHV)(τh)2].
Then, for the variance of Y, we have
Varπb[Y]≤Eπb[Y2]
≤HHX
h=1Eπb[Y2
h]
= 58H¯C2CµHX
h=1Eπb[(BHV)(τh)2].
From Bernstein’s inequality, with probability at least 1−δ/2,∀V∈ V, we have
HX
h=1(ED−Eπb)
(GhV)2−(GhV−(BHV)(τh))2
≤vuut58H¯C2CµHX
h=1Eπb[(BHV)(τh)2]log(4|V|/δ)
n+27H¯C2Cµlog(4|V|/δ)
n. (16)
22Therefore, we have
HX
h=1Eπbh
(BHbV)(τh)2i
=HX
h=1Eπb[(GhbV)2−(GhbV−(BHbV)(τh))2]
≤HX
h=1(ED−Eπb)[(GhbV)2−(GhbV−(BHbV)(τh))2]
+HX
h=1EDh
(GhbV)2−(GhbV−(BHbV)(τh))2i
≤vuut58H¯C2CµHX
h=1Eπbh
(BHbV)(τh)2ilog(4|V|/δ)
n+27H¯C2Cµlog(4|V|/δ)
n+ 6εstat.
(From Equation (15) and Equation (16))
Solving it and we get
HX
h=1Eπbh
(BHbV)(τh)2i
≤10εstat.
We then invoke Lemma 1 and obtain that
J(πe)−Eπb[bV(f1)]≤HX
h=1Esh∼dhπeh
(BSbV)(sh)i
≤vuutHHX
h=1
Esh∼dhπeh
(BSbV)(sh)i2
≤vuutHHX
h=1Esh∼dhπeh
(BSbV)(sh)2i
≤vuutHHX
h=1Eπbh
(BHbV)(τh)2i
·vuuutPH
h=1Esh∼dhπeh
(BSbV)(sh)2i
PH
h=1Eπbh
(BHbV)(τh)2i
≤p
10Hε stat·vuuutPH
h=1Eπeh
(BSbV)(sh)2i
PH
h=1Eπbh
(BSbV)(sh)2i·vuuutPH
h=1Eπbh
(BSbV)(sh)2i
PH
h=1Eπbh
(BHbV)(τh)2i
≤50Hmax{CV+ 1, CΞ}IV(V)DrV[dπe, dπb]s
Cµlog4|V||Ξ|
δ
n.
The proof is completed by using Hoeffding’s inequality to bound |ED[bV(f1)]−Eπb[bV(f1)]|.
D Proofs for Section 4
D.1 Example 1
We provide a brief justification of Example 1. When Prπb(fh|sh)≤Cstoch
(OA)H−h+1, for any u(fh), we
have
∥u(fh)∥2≤Cstoch√
S
(OA)H−h+1.
Then, for any x∈RSsuch that ∥x∥2= 1, we have
x⊤(MF,hM⊤
F,h)x≤X
fh∥x∥2
2∥u(fh)∥2
2=C2
stochS
(OA)H−h+1.
23Therefore, σmin(MF,h)≤σmax(MF,h)≤Cstoch√
S/(OA)(H−h+1)/2.
D.2 Proof of Proposition 3
For each entry in ΣF,h, we can write it as
(ΣF,h)ij=X
kPrπb(fh=k|sh=i) Prπb(fh=k|sh=j)P
i′Prπb(fh=k|sh=i′)
Since the probability is always non-negative, all the entries in ΣF,his non-negative. For each row i,
we have
X
j(ΣF,h)ij=X
kX
jPrπb(fh=k|sh=i) Prπb(fh=k|sh=j)P
i′Prπb(fh=k|sh=i′)
=X
kPrπb(fh=k|sh=i) = 1 .
Similarly, for each column j, we also haveP
i(ΣF,h)ij= 1. Therefore, we prove that ΣF,his
doubly-stochastic and we know for a stochastic matrix, the largest eigenvalue is 1.
D.3 Example 2
Due to the block structure of MF,h,ΣF,his clearly diagonal, and the i-th diagonal entry is:
X
j:Prπb[fh=j|sh=i]>0Prπb[fh=j|sh=i]2
P
i′Prπb[fh=j|sh=i′].
Due to the revealing property, the denominator is just Prπb[fh=j|sh=i], so the expression is just
summing up Prπb[fh=j|sh=i]overj, which is 1.
D.4 Proof of Proposition 4
According to Equation (4), we have for any fh,
VF(fh) =Z(fh)−1u(fh)⊤Σ−1
F,hVπe
S,h
≤q
Z(fh)−1u(fh)⊤Σ−1
F,hZ(fh)−1u(fh)q
(Vπe
S,h)⊤Σ−1
F,hVπe
S,h
≤p
CF,UCF,V.
The last step is from Assumption 7 and Assumption 8. Therefore, ∥VF∥∞≤p
CF,UCF,V=p
CF,2. Moreover, we observe that
∥VF,h∥2
Zh=X
fhZ(fh)
Z(fh)−1u(fh)⊤Σ−1
F,hVπe
S2
=X
fhZ(fh)−1(Vπe
S)⊤Σ−1
F,hu(fh)u(fh)⊤Σ−1
F,hVπe
S
= (Vπe
S)⊤Σ−1
F,h
X
fhZ(fh)−1u(fh)u(fh)⊤
Σ−1
F,hVπe
S
= (Vπe
S)⊤Σ−1
F,hΣF,hΣ−1
F,hVπe
S≤CF,V.
The last step is from Assumption 7.
24D.5 Example 3
We give the calculation for Example 3. When πe=πb,Vπe
S,h=MF,hR+
h, so
∥Vπe
S,h∥2
Σ−1
F,h= (R+
h)⊤M⊤
F,hΣ−1
F,hMF,hR+
h
= (R+
h)⊤Z1/2
hZ−1/2
hM⊤
F,h(MF,hZ−1
hM⊤
F,h)MF,hZ−1/2
hZ1/2
hR+
h
≤(R+
h)⊤Z1/2
hZ1/2
hR+
h,
where the last step follows from the fact that Z−1/2
hM⊤
F,h(MF,hZ−1
hM⊤
F,h)MF,hZ−1/2
his a projec-
tion matrix ( P2=P) and is dominated by identity in eigenvalues ( P⪯I). Now, recall that Zh/Sis
a proper distribution, so
(R+
h)⊤ZhR+
h=S·EZh/S[(R+
h)2]≤SH2.
E Proofs for Section 5
E.1 Proof of Lemma 6
We first verify that w⋆in Eq. (8) satisfies Eq. (7) as follows.
Eπb[w⋆(τh)b(τh)] =X
τhdπb
h(τh)w⋆(τh)b(τh)
=X
τhdπb
h(τh)b(τh)b(τh)⊤Σ−1
H,hbπe
h
= X
τhdπb
h(τh)b(τh)b(τh)⊤!
Σ−1
H,hbπe
h
=bπe
h.
We then show that
∥w⋆∥2
2,dπb
h=X
τhdπb
h(τh)n
b(τh)⊤Σ−1
H,h(bπe
h)o2
= (bπe
h)⊤Σ−1
H,h X
τhdπb
h(τh)b(τh)b(τh)⊤!
Σ−1
H,hbπe
h
= (bπe
h)⊤Σ−1
H,hΣH,hΣ−1
H,hbπe
h
= (bπe
h)⊤Σ−1
H,hbπe
h≤CH,2.
The last step follows from Assumption 11.
E.2 Example 5
Consider the lemma: given vector xand PD matrix Σ, ifΣ⪰xx⊤, then x⊤Σ−1x≤1. The
calculation in the example directly follows from letting x=bπb
h,Σ = Σ H,h, and the condition
Σ⪰xx⊤is satisfied due to Jensen’s inequality.
To prove the lemma, note that Σ−xx⊤is PSD, so
(Σ−1x)⊤(Σ−xx⊤)(Σ−1x)≥0.
This implies x⊤Σ−1x≥x⊤Σ−1xx⊤Σ−1x, sox⊤Σ−1x≤1.
E.3 Comparison between belief coverage and latent state coverage
Here we show that belief coverage is stronger than latent state coverage. More concretely, consider
a standard measure of latent state coverage, the 2nd moment of state density ratio (c.f. discussion
below Example 7):
Eπb[(dπe(sh)/dπb(sh))2] = (bπe
h)⊤diag(Eπb[b(τh)])−1bπe
h.
25In comparison, our belief coverage parameter from Assumption 11 is
(bπe
h)⊤Σ−1
H,hbπe
h= (bπe
h)⊤Eπb[b(τh)b(τh)⊤]−1bπe
h.
To show the former is smaller than the latter, it suffices to show that
diag(Eπb[b(τh)])−1⪯(Eπb[b(τh)b(τh)⊤])−1,
which is implied by diag(Eπb[b(τh)])⪰Eπb[b(τh)b(τh)⊤]. It therefore suffices to show that
diag(b(τh))⪰b(τh)b(τh)⊤holds in a pointwise manner for all τh. To show this, we temporarily
letb=b(τh)in the calculation: consider an arbitrary vector v∈RS, then
v⊤(diag(b)−bb⊤)v=Es∼b[v(s)2]−(Es∼b[v(s)])2≥0.
The last step is due to Jensen’s inequality.
E.4 Comparison to IV (V)
We now compare to the IV(V)andDrVterms in Theorem 2. Belief coverage is generally stronger
than latent state coverage which corresponds to the DrVterm (see Appendix E.3). That said, perhaps
surprisingly, the remaining IV (V)term must scale with1
σmin(ΣH,h)under moderate assumptions.
Proposition 10. Suppose vminis the eigenvector corresponding to σmin(ΣH,h), the smallest eigen-
value for some ΣH,h. Then, if c0vmin∈ BS
hV:={BS
hV:V∈ V} for some non-zero c0,
IV(V)≥q
minshdπb(sh)
σmin(ΣH,h).
The proposition states that as long as minshdπb(sh)is bounded away from 0(which is benign and
helps bound the DrV[dπe, dπb]term) and Vis sufficiently rich that BS
hVincludes a certain vector in
RS, then boundedness of IV(V)requires bounded 1/σmin(ΣH,h), which is stronger than our belief
coverage assumption. This renders the split between IV(V)andDrV[dπe, dπb]superficial and perhaps
unnecessary.
Proof of Proposition 10. Since c0vmin∈ {BS
hV:V∈ V} , there exists Vsuch that BS
hV=c0vmin.
For such V, we observe that
Eπb 
BHV
(τh)2
=X
τhdπb(τh)⟨b(τh), c0vmin⟩2=c2
0v⊤
minΣH,hvmin=σmin(ΣH,h)c2
0.
and
Eπb 
BSV
(sh)2
≥min
shdπb(sh)· ∥c0vmin∥2
2= min
shdπb(sh)·c2
0.
Therefores
Eπb[(BSV) (sh)2]
Eπb[(BHV) (τh)2]≥s
minshdπb(sh)·c2
0
σmin(ΣH,h)c2
0=s
minshdπb(sh)
σmin(ΣH,h).
E.5 Example 6
It suffices to show
ΣH,h1=Eπb[b(τh)b(τh)⊤]1=Eπb[b(τh)b(τh)⊤1] =Eπb[b(τh)] =bπb
h.
E.6 Proof of Lemma 8
The key is to notice that (bπe
h)⊤is non-negative, so
(bπe
h)⊤Σ−1
H,hbπe
h≤(bπe
h)⊤|Σ−1
H,h(bπe
h)| (| · |is pointwise absolute value)
≤(bπe
h)⊤1· ∥Σ−1
H,h(bπe
h)∥∞ (using the non-negativity of (bπe
h)⊤again)
=∥Σ−1
H,h(bπe
h)∥∞.
26E.7 Proof of Lemma 5
According to Eq. (5), we use L1/L∞Hölder’s inequality and obtain that for any fh
|VF(fh)| ≤u(fh)
ZR(fh)
1(ΣR
F,h)−1Vπe
S
∞
=R+(fh)u(fh)
Z(fh)
1(ΣR
F,h)−1Vπe
S
∞
≤R+(fh)CF,∞
≤HCF,∞.
The second inequality is from Assumption 9 andu(fh)
Z(fh)is a stochastic vector. The last step is from
the boundedness of the reward.
E.8 Example 4
We use diag(·)both for creating a diagonal matrix with an input vector and for taking the diagonal
vector out of a matrix.
ΣR
F,h1=MF,h(ZR
h)−1M⊤
F,h1=MF,h(ZR
h)−1diag(Zh)⊤=MF,hR+
h=Vπe
S.
E.9 Example 9
The calculation is similar to that of Example 2 in Appendix D.3, except that the numerator has an extra
R+(fh). Therefore, when calculating the i-th diagonal entry of ΣR
F,h, the final sum is calculating the
expectation of R+(fh)conditioned on sh=iunder policy πb, which is the definition of Vπb
S(i).
E.10 Proof of Theorem 7
In the proof of Theorem 2, we have
HX
h=1Eπbh
(BHbV)(τh)2i
≤10εstat, ε stat:=225H¯C2log(4|V||Ξ|/δ)
n.
We observe thatEπeh
(BSbV)(sh)i=Eτh∼dπe
hEsh∼b(τh)h
(BSbV)(sh)i
=Eπeh
(BHbV)(τh)i
=Eπbh
w⋆(τh)(BHbV)(τh)i
≤ ∥w⋆∥2,dπb
hr
Eπbh
(BHbV)(τh)2i
≤r
CH,2Eπbh
(BHbV)(τh)2i
The third equality is from Definition 10, the first inequality is from Cauchy-Schwarz inequality and
the last inequality is from Lemma 6. Then, we have
HX
h=1Eπeh
(BSbV)(sh)i≤vuutHHX
h=1
Eπeh
(BSbV)(sh)i2
≤p
10HCH,2εstat
≤50H¯Cr
CH,2Cµlog(4|V||Ξ|/δ)
n
≤cH2(CF,∞+ 1)r
CH,2Cµlog(4|V||Ξ|/δ)
n.
The last step is from Lemma 5, CV≤c∥VF∥∞andCΞ≤c(∥VF∥∞+ 1). The proof is completed
after invoking Lemma 1.
27E.11 Proof of Theorem 9
Finally, we prove Theorem 9. The proof uses the similar idea from Xie and Jiang [2020]. For any
V∈ V andw∈ W , we define the population loss estimator Ldπband the empirical loss estimator
LDas follows
Ldπb(V, w) :=HX
h=1Eπb
w(τh)(BHV)(τh)
LD(V, w) :=HX
h=1ED[w(τh) (µ(oh, ah) (rh+V(fh+1))−V(fh))].
We then invoke Lemma 1 and obtain that
J(πe)−Eπb[bV(f1)]=HX
h=1Esh∼dπeh
(BSbV)(sh)i.
We observe that
Esh∼dπeh
(BSbV)(sh)i
=Eτh∼dπeh
Esh∼b(τh)h
(BSbV)(sh)ii
=Eτh∼dπeh
(BHbV)(τh)i
=Eπbh
w⋆(τh)(BHbV)(τh)i
,
where the last step is from Definition 10. Therefore, our goal is to boundPH
h=1Eπbh
w⋆(τh)(BHbV)(τh)i=Ldπb(bV , w⋆). Let
bw:= argmin
w∈sp(W)max
V∈VHX
h=1Eπb
(w⋆(τh)−w(τh))·(BHV)(τh),
eV:= argmin
V∈Vsup
w∈sp(W)HX
h=1Eπb
w(τh)·(BHV)(τh).
Then, we subtract the approximation error of bwfrom our objective,
HX
h=1Eπbh
w⋆(τh)(BHbV)(τh)i=HX
h=1Eπbh
(w⋆(τh)−bw(τh))·(BHbV)(τh)i
+HX
h=1Eπbh
bw(τh)·(BHbV)(τh)i
≤HX
h=1Eπbh
(w⋆(τh)−bw(τh))·(BHbV)(τh)i+HX
h=1Eπbh
bw(τh)·(BHbV)(τh)i
≤ϵW+HX
h=1Eπbh
bw(τh)·(BHbV)(τh)i.
Next, we consider the approximation error of eVand obtain that
HX
h=1Eπbh
bw(τh)·(BHbV)(τh)i=ϵV+HX
h=1Eπbh
bw(τh)·(BHbV)(τh)i−sup
w∈sp(W)HX
h=1Eπbh
w(τh)·(BHeV)(τh)i.
28We then connect Ldπb(V, w)withLD(V, w)as follows,
HX
h=1Eπbh
bw(τh)·(BHbV)(τh)i−sup
w∈sp(W)HX
h=1Eπbh
w(τh)·(BHeV)(τh)i
≤sup
w∈sp(W)HX
h=1Eπbh
w(τh)·(BHbV)(τh)i−sup
w∈sp(W)HX
h=1Eπbh
w(τh)·(BHeV)(τh)i
= max
w∈WLdπb(bV , w)−max
w∈WLdπb(eV , w)
= max
w∈WLdπb(bV , w)−max
w∈WLD(bV , w)+ max
w∈WLD(bV , w)−max
w∈WLdπb(eV , w)
≤max
w∈WLdπb(bV , w)−max
w∈WLD(bV , w)+ max
w∈WLD(eV , w)−max
w∈WLdπb(eV , w)
≤max
w∈WLdπb(bV , w)− LD(bV , w)+ max
w∈WLdπb(eV , w)− LD(eV , w).
The first equality is from that supw∈sp(W)|f(·)|= max w∈W|f(·)|for any linear function f(·). The
second inequality is from the optimality of bV.
For any fixed V, w, let random variable X=PH
h=1w(τh) (µ(oh, ah)(rh+V(fh))−V(fh+1)),
Eπb[X] =Ldπb(V, w). Recall that CV:= max V∈V∥V∥∞andCW:= max hsupw∈W∥w∥∞. For
|X|, we have |X| ≤2HCµ(1 +CV)CW. For the variance, we have
Varπb[X]
≤HHX
h=1Eπb
w(τh)2(µ(oh, ah)(rh+V(fh+1))−V(fh))2
≤HHX
h=1Eπb
w(τh)2 
2C2
V+ 2(1 + CV)2µ(oh, ah)2
≤4H2(1 +CV)2CµC2
W.
From Bernstein’s inequality, with probability at least 1−δ, we have
|Ldπb(V, w)− LD(V, w)| ≤2H(1 +CV)CWs
Cµlog2
δ
n+2H(1 +CV)CWCµlog2
δ
n.
Taking the union bound and we obtain
max
w∈WLdπb(bV , w)− LD(bV , w)+ max
w∈WLdπb(eV , w)− LD(eV , w)
≤4H(1 +CV)CWs
Cµlog2|V||W|
δ
n+4H(1 +CV)CWCµlog2|V||W|
δ
n.
According to Lemma 5, Assumption 12, CV≤c∥VF∥∞andCW≤c∥w⋆∥∞, we further have
J(πe)−Eπb[bV(f1)]≤ϵV+ϵW+cH2CH,∞(CF,∞+ 1)s
Cµlog2|V||W|
δ
n+cH2CH,∞(CF,∞+ 1)Cµlog2|V||W|
δ
n.
The proof is completed by using Hoeffding’s inequality to bound |ED[bV(f1)]−Eπb[bV(f1)]|.
29NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitations in Section 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
30Justification: We provide the full set of assumptions in the main text and a complete proof
in the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: No experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
31Answer: [NA]
Justification: No experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: No experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: No experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
32•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: No experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We conform with the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper presents work whose goal is to advance the field of learning theory
and has no societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
33•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
34•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
35