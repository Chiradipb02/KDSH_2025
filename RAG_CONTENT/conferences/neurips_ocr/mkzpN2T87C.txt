Non-asymptotic Global Convergence Analysis of BFGS
with the Armijo-Wolfe Line Search
Qiujiang Jin
ECE, UT Austin
qiujiangjin0@gmail.comRuichen Jiang
ECE, UT Austin
rjiang@utexas.eduAryan Mokhtari
ECE, UT Austin
mokhtari@austin.utexas.edu
Abstract
In this paper, we present the first explicit and non-asymptotic global convergence
rates of the BFGS method when implemented with an inexact line search scheme
satisfying the Armijo-Wolfe conditions. We show that BFGS achieves a global
linear convergence rate of (1−1
κ)tforµ-strongly convex functions with L-Lipschitz
gradients, where κ=L
µrepresents the condition number. Additionally, if the
objective function’s Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search
achieves a linear convergence rate that depends solely on the line search parameters,
independent of the condition number. We also establish a global superlinear
convergence rate of O((1
t)t). These global bounds are all valid for any starting
point x0and any symmetric positive definite initial Hessian approximation matrix
B0, though the choice of B0impacts the number of iterations needed to achieve
these rates. By synthesizing these results, we outline the first global complexity
characterization of BFGS with the Armijo-Wolfe line search. Additionally, we
clearly define a mechanism for selecting the step size to satisfy the Armijo-Wolfe
conditions and characterize its overall complexity.
1 Introduction
In this paper, we focus on solving the following unconstrained convex minimization problem
min
x∈Rdf(x), (1)
where f:Rd→Ris strongly convex and twice differentiable. Quasi-Newton methods are among the
most popular algorithms for solving this class of problems due to their simplicity and fast convergence.
Like gradient descent-type methods, they require only gradient information for implementation, while
they aim to mimic the behavior of Newton’s method by using gradient information to approximate
the curvature of the objective function. There are several variations of quasi-Newton methods,
primarily distinguished by their update rules for the Hessian approximation matrices. The most
well-known among these include the Davidon-Fletcher-Powell (DFP) method [1, 2], the Broyden-
Fletcher-Goldfarb-Shanno (BFGS) method [3–6], the Symmetric Rank-One (SR1) method [7, 8], and
the Broyden method [9]. Apart from these classical methods, other variants have also been proposed
in the literature, including randomized quasi-Newton methods [10–14], greedy quasi-Newton methods
[13–16], and those based on online learning techniques [17, 18]. In this paper, we mainly focus on the
global analysis of the BFGS method, arguably the most successful quasi-Newton method in practice.
The classic analyses of BFGS, including [19–28], primarily focused on demonstrating local asymp-
totic superlinear convergence without addressing an explicit global convergence rate when BFGS is
deployed with a line-search scheme. While attempts have been made to establish global convergence
for quasi-Newton methods using line search or trust-region techniques in previous studies [8, 29–33],
these efforts provided only asymptotic convergence guarantees without explicit global convergence
rates, thus not fully characterizing the global convergence rate of classical quasi-Newton methods.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).In recent years, there have been efforts to characterize the explicit convergence rate of BFGS within a
local neighborhood of the solution, establishing a superlinear convergence rate of the form (1√
t)t;
see, for example, [34–37]. However, these results focus solely on local convergence analysis of
BFGS under conditions where the stepsize is consistently set to one, the iterate remains close to the
optimal solution, and the initial Hessian approximation matrix meets certain necessary conditions.
Consequently, these analyses do not extend to providing a global convergence guarantee. For more
details on this subject, we refer the reader to the discussion section in [38].
To the best of our knowledge, only few papers are closely related to our work and establish a global
non-asymptotic guarantee for BFGS. In [39], it was shown that BFGS with exact line search achieves
a global linear rate of (1−2µ3
L3(1 +µTr(B−1
0)
t)−1(1 +Tr(B0)
Lt)−1)t, where µis the strong convexity
parameter, Lis the Lipschitz constant of the gradient, B0is the initial Hessian approximation matrix,
andTr(·)denotes the trace of a matrix. After t=O(d)iterations, this rate approaches (1−2µ3
L3)t,
which is significantly slower than the convergence rate of gradient descent. Additionally, a recent
draft in [40] studied the global convergence of BFGS under an inexact line search. While this work
establishes a local superlinear rate, it only shows a global linear rate of the form (1−µ2
L2)t. Hence,
both these results fail to prove any global advantage for BFGS over gradient descent. In [38], the
authors improved upon [39] by showing a better global linear convergence rate and a faster superlinear
rate for BFGS with exact line search. Specifically, for an L-Lipschitz and µ-strongly convex function,
BFGS initialized with B0=LIachieves a global linear rate of (1−µ3/2
L3/2)tfort≥1, while BFGS
withB0=µIachieves the same rate after dlogκiterations. With the additional assumption that the
objective’s Hessian is Lipschitz, an improved linear rate of (1−µ
L)tis achieved after O(κ)iterations
when B0=LIand after O(dlogκ+κ)when B0=µI, matching the rate of gradient descent. A
superlinear rate of (1/√
t)twas also shown when the number of iterations exceeds specific thresholds.
Contributions. In this paper, we analyze the BFGS method combined with the Armijo-Wolfe line
search, the most commonly used line search criteria in practical BFGS applications; see, e.g., [41].
For minimizing an L-smooth and µ-strongly convex function, we present a global convergence rate of
(1−µ
L)t. To the best of our knowledge, this is the first result demonstrating a global linear convergence
rate for BFGS that matches the rate of gradient descent under these assumptions. Furthermore, we
show that if the objective function’s Hessian is Lipschitz continuous, BFGS with the Armijo-Wolfe
line search converges at a linear rate determined solely by the line search parameters and not the
problem’s condition number, κ=L/µ, when the number of iterations is sufficiently large. Finally,
we prove a global non-asymptotic superlinear convergence rate of (h(d,κ,C 0)/t)t, where h(d, κ, C 0)
depends on the condition number κ, the dimension d, and the weighted distance between the initial
point x0and the optimal solution x∗, denoted by C0. We summarize our results in Table 1. By
combining these convergence results, we establish the total iteration complexity of BFGS with the
Armijo-Wolfe line search. We also specify the line search complexity by investigating a bisection
algorithm for choosing the step size that satisfies the Armijo-Wolfe conditions. Our result is one
of the first non-asymptotic analysis characterizing the global convergence complexity of the BFGS
quasi-Newton method with an inexact line search.
Notation. We denote the ℓ2-norm by ∥ · ∥, the set of d×dsymmetric positive definite matrices by
Sd
++, and use A⪯Bto mean B−Ais symmetric positive semi-definite. The trace and determinant
of a matrix Aare represented as Tr(A)andDet(A), respectively.
2 Preliminaries
In this section, we present the assumptions, notations, and intermediate results useful for the global
convergence analysis. First, we state the following assumptions on the objective function f.
Assumption 2.1. The function fis twice differentiable and strongly convex with parameter µ >0.
Assumption 2.2. The gradient of fis Lipschitz continuous with parameter L >0.
These assumptions are common in the convergence analysis of quasi-Newton methods. Under these,
we show a global linear convergence rate of O((1−µ
L)t). To achieve a faster linear convergence rate
that is independent of the problem condition number, and a global superlinear rate, we require an
additional assumption that the objective function Hessian is Lipschitz continuous, as stated next.
Assumption 2.3. The Hessian of fis Lipschitz continuous with parameter M > 0, i.e., for x, y∈Rd,
we have ∥∇2f(x)− ∇2f(y)∥ ≤M∥x−y∥.
2Initial Matrix Convergence Phase Convergence Rate Starting moment
B0 Linear phase I 
1−1
κtΨ(¯B0)
B0 Linear phase II 
1−1
3tΨ(˜B0) +C0Ψ(¯B0) +C0κ
B0 Superlinear phase
Ψ(˜B0)+C0Ψ(¯B0)+C0κ
tt
Ψ(˜B0) +C0Ψ(¯B0) +C0κ
LI Linear phase I 
1−1
κt1
LI Linear phase II 
1−1
3tdκ+C0κ
LI Superlinear phase dκ+C0κ
ttdκ+C0κ
µI Linear phase I 
1−1
κtdlogκ
µI Linear phase II 
1−1
3t(1 +C0)dlogκ+C0κ
µI Superlinear phase
(1+C0)dlogκ+C0κ
tt
(1 +C0)dlogκ+C0κ
Table 1: Summary of our results for (i) an arbitrary positive definite B0, (ii)B0=LI, and (iii) B0=
µI. Here, Ψ(A) :=Tr(A)−d−logDet(A),¯B0=1
LB0and˜B0=∇2f(x∗)−1
2B0∇2f(x∗)−1
2.
The last column shows the number of iterations required to achieve the corresponding linear or
superlinear convergence phase. For brevity, the absolute constants are dropped.
Note that the above regularity condition on the Hessian assumption is also common for establishing
the superlinear convergence rate of quasi-Newton methods [19–28].
BFGS Update. Next, we state the general update rule of BFGS. If we denote xtas the iterate at
timet, the vector gt=∇f(xt)as the objective function gradient at xt, and Btas the Hessian
approximation matrix at step t, then the update is given by
xt+1=xt+ηtdt, d t=−B−1
tgt, (2)
where ηt>0is the step size and dtis the descent direction. By defining the variable difference
st:=xt+1−xtand the gradient difference yt:=∇f(xt+1)− ∇f(xt), we can present the Hessian
approximation matrix update for BFGS as follows:
Bt+1=Bt−Btsts⊤
tBt
s⊤
tBtst+yty⊤
t
s⊤
tyt. (3)
To avoid the costly operation of inverting the matrix Bt, one can define the inverse Hessian approxi-
mation matrix as Ht:=B−1
tand apply the Sherman-Morrison-Woodbury formula to obtain
Ht+1:=
I−sty⊤
t
y⊤
tst
Ht
I−yts⊤
t
s⊤
tyt
+sts⊤
t
y⊤
tst. (4)
It is well-known that for a strongly convex objective function, the Hessian approximation matrices
Btremain symmetric and positive definite if the initial matrix B0is symmetric positive definite [41].
Therefore, all matrices BtandHtare symmetric positive definite throughout this paper.
As mentioned earlier, establishing a global convergence guarantee for BFGS requires pairing it with
a line search scheme to select the stepsize ηt. This paper focuses on implementing BFGS with the
Armijo-Wolfe line search, detailed in the following subsection.
Armijo-Wolfe Line Search. We consider a stepsize ηt>0that satisfies the Armijo-Wolfe conditions
f(xt+ηtdt)≤f(xt) +αηt∇f(xt)⊤dt, (5)
∇f(xt+ηtdt)⊤dt≥β∇f(xt)⊤dt, (6)
where αandβare the line search parameters, satisfying 0< α < β < 1and0< α <1
2. The
condition in (5)is the Armijo condition, ensuring that the step size ηtprovides a sufficient decrease
in the objective function f. The condition in (6)is the curvature condition, which guarantees that
the slope ∇f(xt+ηtdt)⊤dtatηtis not strongly negative, indicating that further movement along dt
would significantly decrease the function value. These conditions provide upper and lower bounds on
the admissible step size ηt. In some references, the Armijo-Wolfe line search conditions are known
as the weak Wolfe conditions [42, 43]. The procedure for finding ηtthat satisfies these conditions is
described in Section 7. Next lemma presents key properties of the Armijo-Wolfe conditions.
3Lemma 2.1. Consider the BFGS method with Armijo-Wolfe inexact line search, where the step size
satisfies the conditions in (5)and(6). Then, for any initial point x0and any symmetric positive
definite initial Hessian approximation matrix B0, the following results hold for all t≥0:
f(xt)−f(xt+1)
−g⊤
tst≥α,y⊤
tst
−g⊤
tst≥1−β, and f(xt+1)≤f(xt). (7)
Remark 2.1. While in this paper we only focus on the Armijo-Wolfe line search, our results are also
valid for some other line search schemes that require stricter conditions. For instance, in the strong
Wolfe line search, given 0< α < β < 1and0< α <1
2, the required conditions for the step size are
f(xt+ηtdt)≤f(xt) +αηt∇f(xt)⊤dt,|∇f(xt+ηtdt)⊤dt| ≤β∇f(xt)⊤dt,
Indeed, if ηtsatisfies the strong Wolfe conditions, it also satisfies the Armijo-Wolfe conditions.
Another commonly employed line search scheme is Armijo–Goldstein, which imposes the conditions
−c1ηt∇f(xt)⊤dt≤f(xt)−f(xt+ηtdt)≤ −c2ηt∇f(xt)⊤dt,
with0< c1≤c2<1. The lower bound on f(xt)−f(xt+ηtdt)in the Armijo–Goldstein line
search indicates that ηtsatisfies the sufficient decrease condition in (5)required for the Armijo-Wolfe
conditions, with α=c1. Moreover, given the convexity of f, the upper bound on f(xt)−f(xt+ηtdt)
in the Armijo–Goldstein line search suggests −ηt∇f(xt+ηtdt)⊤dt≤f(xt)−f(xt+ηtdt)≤
−c2ηt∇f(xt)⊤dt. Thus, ηtalso meets the curvature condition in (6)required in the Armijo-Wolfe
conditions with β=c2. Hence, all our results derived under the Armijo-Wolfe line search are also
valid for both the strong Wolfe line search and the Armijo–Goldstein line search.
3 Convergence Analysis
In this section, we present our theoretical framework for analyzing the global linear convergence
rates of BFGS with the Armijo-Wolfe line search scheme. To start, we introduce some necessary
definitions and notations. We define the average Hessian matrices JtandGtas
Jt:=Z1
0∇2f(xt+τ(xt+1−xt))dτ, G t:=Z1
0∇2f(xt+τ(x∗−xt))dτ. (8)
Further, for measuring the suboptimality of the iterates we define the sequence Ctas
Ct:=M
µ3
2p
2(f(xt)−f(x∗)),∀t≥0, (9)
where Mis the Lipschitz constant of the Hessian defined in Assumption 2.3 and µis the strong con-
vexity parameter introduced in Assumption 2.1.To analyze the dynamics of the Hessian approximation
matrices {Bt}+∞
t=0, we use the function Ψ(A)
Ψ(A) :=Tr(A)−d−logDet(A), (10)
well-defined for any A∈Sd
++. It was introduced in [32] to capture the discrepancy between Aand
the identity matrix I. Note that Ψ(A)≥0for any A∈Sd
++andΨ(A) = 0 if and only if A=I.
Before we start convergence analysis, given any weight matrix P∈Sd
++, we define the weighted
versions of the vectors gt,st,yt,dtand the matrix Bt,Jtas
ˆgt=P−1
2gt, ˆst=P1
2st, ˆyt=P−1
2yt, ˆdt=P1
2dt. (11)
ˆBt=P−1
2BtP−1
2, ˆJt=P−1
2JtP−1
2. (12)
Note that these weighted matrices and vectors preserve many properties of their unweighted coun-
terparts. For instance, two of these main properties are ˆg⊤
tˆst=g⊤
tstandˆy⊤
tˆst=y⊤
tst. Similarly,
the update for the weighted version of Hessian approximation matrices closely mirrors the update of
their unweighted counterparts, as noted in the following expression:
ˆBt+1=ˆBt−ˆBtˆstˆs⊤
tˆBt
ˆs⊤
tˆBtˆst+ˆytˆy⊤
t
ˆs⊤
tˆyt,∀t≥0. (13)
Finally, we define a crucial quantity, ˆθt, which measures the angle between the weighted descent
direction and the negative of the weighted gradient direction, satisfying
cos(ˆθt) =−ˆg⊤
tˆst
∥ˆgt∥∥ˆst∥. (14)
43.1 Intermediate Results
In this section, we present our framework for analyzing the convergence of BFGS with an inexact line
search. We first characterize the relationship between the function value decrease at each iteration
and key quantities, including the angle ˆθtdefined in (14).
Proposition 3.1. Let{xt}t≥0be the iterates generated by BFGS. Recall the definitions of weighted
vectors in (11). Then, for any weight matrix Pand for all t≥1, we have
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1−t−1Y
i=0ˆpiˆqiˆnicos2(ˆθi)
ˆmi1
tt
. (15)
where ˆpt,ˆqt,ˆmtandˆntare defined as
ˆpt:=f(xt)−f(xt+1)
−ˆg⊤
tˆst,ˆqt:=∥ˆgt∥2
f(xt)−f(x∗),ˆmt:=ˆy⊤
tˆst
∥ˆst∥2,ˆnt=ˆy⊤
tˆst
−ˆg⊤
tˆst.(16)
This result shows the convergence rate of BFGS with Armijo-Wolfe line search depends on four
products:Qt−1
i=0ˆpi,Qt−1
i=0ˆqi,Qt−1
i=0ˆni, andQt−1
i=0cos2(ˆθi)
ˆmi. To establish an explicit rate, we need
lower bounds on these products. Lemma 2.1 shows that the lower bounds forQt−1
i=0ˆpiandQt−1
i=0ˆni
depend on the inexact line search parameters αandβ. We will further prove that if the unit step size
ηt= 1satisfies the Armijo-Wolfe conditions, better lower bounds can be obtained for these products.
The lower bounds forQt−1
i=0ˆqiandQt−1
i=0cos2(ˆθi)
ˆmiwere established in previous work [38] as presented
in Appendix D. Specifically, the bounds forQt−1
i=0ˆqidepend on the choice of the weight matrix,
which varies in different sections of the paper, requiring separate bounds for each case. However,
the bound forQt−1
i=0cos2(ˆθi)
ˆmidoes not require separate treatment. This is explicitly established in
Proposition D.1, a classical result, as discussed in [41, Section 6.4]. We build all our linear and
superlinear results by establishing different bounds on the terms in (15).
4 Global Linear Convergence Rates
Building on the tools introduced in Section 3, we establish explicit global linear convergence rates for
BFGS with the Armijo-Wolfe line search, requiring only the strong convexity and gradient Lipschitz
conditions from Assumptions 2.1 and 2.2. Our proof leverages the fundamental inequality in (15)
from Proposition 3.1 and lower bounds on the terms that appear in the contraction factor. Here, we
set the weight matrix PtoP=LIand hence define the initial weighted matrix ¯B0as¯B0=1
LB0.
The following theorem presents our first global linear convergence rate of BFGS for any B0∈Sd
++.
Theorem 4.1. Suppose Assumptions 2.1 and 2.2 hold. Let {xt}t≥0be the iterates generated by
BFGS, where the step size satisfies the Armijo-Wolfe conditions in (5)and(6). For any initial point
x0∈Rdand any initial Hessian approximation matrix B0∈Sd
++, we have
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1−e−Ψ(¯B0)
t2α(1−β)
κt
,∀t≥1. (17)
Remark 4.1. In [38], the authors analyzed BFGS with exact line search and established a global
linear rate of (1−e−Ψ(¯B0)
t1
κ(1+√κ))t. In comparison, our result in (17) achieves a faster linear
rate by eliminating the√κfactor in the denominator. This improvement arises from using the
Armijo-Wolfe conditions. Specifically, under these conditions, we showf(xt)−f(xt+1)
−g⊤
tst≥αas shown
in Lemma 2.1, where α∈(0,1/2)is a line search parameter. In contrast, using exact line search, the
authors in [38] proved thatf(xt)−f(xt+1)
−g⊤
tst≥2√κ+1, thus leading to the extra√κfactor in their rate.
From Theorem 4.1, we observe that the linear convergence rate is determined by the quantity Ψ(¯B0)
Thus, to simplify our bounds, we consider two different initializations: B0=LIandB0=µI.
Corollary 4.2. Suppose Assumptions 2.1 and 2.2 hold, {xt}t≥0are generated by BFGS with step
size satisfying the Armijo-Wolfe conditions in (5)and(6), and x0∈Rdis an arbitrary initial point.
5•If the initial Hessian approximation matrix is set as B0=LI, then for any t≥1
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1−2α(1−β)
κt
. (18)
•If the initial Hessian approximation matrix is set as B0=µI, then for any t≥1we have
f(xt)−f(x∗)
f(x0)−f(x∗)≤(1−e−dlogκ
t2α(1−β)
κ)t. Moreover, for t≥dlogκ, we have
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1−2α(1−β)
3κt
. (19)
Corollary 4.2 shows that when initialized with B0=LI, BFGS achieves a linear rate of O((1−1
κ)t)
from the first iteration, matching the rate of gradient descent. It also indicates that initializing with
B0=µIachieves a similar rate but after dlogκiterations. While this suggests a preference for
initializing with B0=LI, subsequent analysis reveals that with enough iterations, BFGS with either
initialization can attain a faster linear rate independent of κ. In some cases, starting with B0=µI
may lead to fewer total iterations to achieve this faster rate. We will explore this trade-off later.
5 Condition Number Independent Linear Convergence Rates
In this section, we improve the previous results and establish a non-asymptotic, condition number-
free global linear convergence rate for BFGS with the Armijo-Wolfe line search. This requires the
additional assumption that the Hessian is Lipschitz continuous. Our analysis builds on the previous
methodology but uses P=∇2f(x∗)instead of P=LIto prove the condition number-independent
global linear rate. Thus, the weighted initial matrix ˜B0is∇2f(x∗)−1
2B0∇2f(x∗)−1
2. Next, we
present a general global convergence bound for any initial Hessian approximation B0∈Sd
++.
Proposition 5.1. Suppose Assumptions 2.1, 2.2 and 2.3 hold. Let {xt}t≥0be the iterates generated
by BFGS with the step size satisfying the Armijo-Wolfe conditions in (5)and(6). Recall the definition
ofCtin(9)andΨ(·)in(10). For any initial point x0∈Rdand any initial Hessian approximation
matrix B0∈Sd
++, the following result holds:
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1−2α(1−β)e−Ψ(˜B0)+3Pt−1
i=0Ci
tt
,∀t≥1.
Proposition 5.1 demonstrates that the convergence rate of BFGS with the Armijo-Wolfe line search is
influenced by Ψ(˜B0)and the sumPt−1
i=0Ci. The first term Ψ(˜B0)is a constant that depends on our
choice of the initial Hessian approximation matrix B0. The second termPt−1
i=0Cican also be upper
bounded using the non-asymptotic global linear convergence rate provided in Theorem 4.1.
Theorem 5.2. Suppose Assumptions 2.1, 2.2 and 2.3 hold, and let {xt}t≥0be the iterates generated
by BFGS with the Armijo-Wolfe line search in (5)and(6). Then, for any initial point x0∈Rdand
any initial Hessian approximation B0∈Sd
++, ift≥Ψ(˜B0) + 3C0Ψ(¯B0) +9
α(1−β)C0κ, we have
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1−2α(1−β)
3t
. (20)
This result shows that when the number of iterations meets t≥Ψ(˜B0) + 3C0Ψ(¯B0) +9
α(1−β)C0κ,
BFGS with Armijo-Wolfe conditions achieves a condition number-independent linear rate. The choice
ofB0is critical as it influences the required iterations through ˜B0=∇2f(x∗)−1
2B0∇2f(x∗)−1
2and
¯B0=1
LB0. Different choices of B0affect Ψ(˜B0) + 3C0Ψ(¯B0)and thus the number of iterations
needed for condition-free linear convergence. While optimizing B0to minimize Ψ(˜B0) + 3C0Ψ(¯B0)
is possible, we focus on two practical initialization schemes: B0=LIandB0=µI.
Corollary 5.3. Suppose that Assumptions 2.1, 2.2 and 2.3 hold. Let {xt}t≥0be the iterates generated
by the BFGS method, where the step size satisfies the Armijo-Wolfe conditions in (5)and(6), and
x0∈Rdas an arbitrary initial point. Then, given the result in Theorem 5.2, we have
•If we set B0=LI, the rate in (20) holds for t≥dκ+9
α(1−β)C0κ,
•If we set B0=µI, the rate in (20) holds for t≥(1 + 3 C0)dlogκ+9
α(1−β)C0κ.
Based on Corollary 5.3, if C0≪κ, or equivalently f(x0)−f(x∗)≪L2µ
M2, then BFGS with B0=µI
requires less iterations to achieve the condition number-independent linear convergence rate.
66 Global Superlinear Convergence Rates
In this section, we present our global superlinear result. Consider the definition ˜B0=
∇2f(x∗)−1
2B0∇2f(x∗)−1
2as well as the definition of ρtwhich is given by
ρt:=−g⊤
tdt
∥˜dt∥2, ˜dt:=∇2f(x∗)1
2dt,∀t≥0. (21)
To motivate, let us briefly discuss why we are only able to show a linear convergence rate instead
of a superlinear rate in Theorem 5.2. By inspecting the proof, we observe that the bottleneck is due
to the lower bounds on ˆptandˆnt: we used ˆpt≥αandˆnt≥1−βfrom Lemma 2.1, which leads
to the constant factor α(1−β)in the final linear rate in Theorem 5.2. Thus, to show a superlinear
convergence rate, we need to establish tighter lower bounds for ˆptandˆnt. In the following lemma,
we show that if the step size ηt= 1, we are able to establish such tighter lower bounds.
Lemma 6.1. Recall ˆpt=f(xt)−f(xt+1)
−ˆg⊤
tˆstandˆnt=ˆy⊤
tˆst
−ˆg⊤
tˆstdefined in (16). If the unit step size ηt= 1
satisfies the Armijo-Wolfe conditions (5)and(6), then we have
ˆpt≥1−1 +Ct
2ρt, ˆnt≥1
(1 +Ct)ρt. (22)
In contrast to the constant lower bounds in Lemma 2.1, the lower bounds in (22) depend on Ctandρt.
Later, we show Ct→0andρt→1. Hence, the lower bounds in (22) approach 1 as the number of
iterations increases, enabling us to prove a superlinear rate. That said, the lower bounds in Lemma 6.1
hold only when ηt= 1. To complete the picture, we need to quantify when and how often the unit
step size is selected during BFGS execution. This is addressed in the next lemmas.
Lemma 6.2. Suppose Assumptions 2.1, 2.2, and 2.3 hold and define the constants
δ1:=min1
6,p
2(1−α)−1,1√1−β−1
, δ2:= max {7
8,1p
2(1−α)}, δ3:=1√1−β,(23)
which satisfy 0< δ1< δ2<1< δ3. IfCt≤δ1andδ2≤ρt≤δ3, then ηt= 1 satisfies the
Armijo-Wolfe conditions (5)and(6).
Lemma 6.2 shows that when Ct≤δ1andρtfalls within the interval [δ2, δ3], the step size ηt= 1
is admissible and meets the Armijo-Wolfe conditions. Note that by the linear convergence result
in Theorem 4.1, the first condition on Ctwill be satisfied when tis sufficiently large. Additionally,
using Proposition G.2 in the Appendix, we can show that the second condition on ρtis violated only
for a finite number of iterations. These observations are formally presented in the following lemma.
Lemma 6.3. Suppose Assumptions 2.1, 2.2, and 2.3 hold and the iterates {xt}t≥0are generated
by the BFGS method with step size satisfying the Armijo-Wolfe conditions in (5)and(6). Recall Ct
defined in (9),Ψ(·)defined in (10),{δi}3
i=1defined in (23) and¯B0=1
LB0. We have Ct≤δ1when
t≥t0:= max
Ψ(¯B0),3κ
α(1−β)logC0
δ1
. (24)
Moreover, if we define ω(x) =x−log(1 + x), the size of the set I={t:ρt/∈[δ2, δ3]}is at most
|I| ≤δ4
Ψ(˜B0) + 2C0Ψ(¯B0) +6C0κ
α(1−β)
,where δ4:=1
min{ω(δ2−1), ω(δ3−1)}.(25)
Lemma 6.3 implies that conditions Ct≤δ1andρt∈[δ2, δ3]will be satisfied for all but a finite
number of iterations. Thus, if the line search always starts by testing the unit step size (as shown
in Section 7), we will choose ηt= 1, and accordingly, the tighter lower bound in Lemma 6.1 will
apply for all but a finite number of iterations. By applying these lower bounds along with (15) from
Proposition 3.1, we can prove a global superlinear convergence rate, as presented next.
Remark 6.1. Lemmas 6.2 and 6.3 are inspired by the analysis in [40]. Specifically, Lemma 5.10 of
[40] characterized the conditions on Ctandρtunder which η= 1satisfies the Armijo condition (5),
and further bounded the number of iterations where these conditions are violated. However, our
Lemma 6.2 addresses both the Armijo condition in (5)and the curvature condition in (6), and the
arguments appear simpler. Additionally, our proof for the superlinear convergence rate differs from
[40]. Their approach analyzed the Dennis-Moré ratio and measured “local” superlinear convergence
using the distance ∥∇f(x∗)1
2(xt−x∗)∥. In contrast, our “global” result is based on the unified
framework in Proposition 3.1 and uses the function value gap as a measure of convergence.
7Theorem 6.4. Suppose Assumptions 2.1, 2.2, and 2.3 hold and the iterates {xt}t≥0are generated
by BFGS with step size satisfying the Armijo-Wolfe conditions in (5)and(6). Recall the definition
ofCtin(9),Ψ(·)in(10),¯B0:=1
LB0,˜B0:=∇2f(x∗)−1
2B0∇2f(x∗)−1
2, and δ1, δ2, δ3, δ4in(23)
and(25). Then, for any x0∈Rdand any B0∈Sd
++, the following global superlinear result holds:
f(xt)−f(x∗)
f(x0)−f(x∗)≤ 
δ7Ψ(˜B0) + (δ6+δ8C0)Ψ(¯B0) + (3δ6
α(1−β)logC0
δ1+3δ8
α(1−β)C0)κ
t!t
,(26)
where {δi}8
i=5defined below are constants that only depend on line search parameters αandβ,
δ5:=max{2 +2
δ2,4δ3}
2δ2−1−δ1, δ6:=log1
2α(1−β), δ7:=1+ δ4δ6+δ5, δ8:=1+2 δ7+2δ2−δ1−logδ2
2δ2−1−δ1.
The above result shows a global superlinear convergence rate of the form O((C′
t)t), where C′depends
on the condition number κ, the initial weighted distance C0, and the initial Hessian approximation
matrix B0. To simplify the expression, we report the above bound for B0=LIandB0=µI.
Corollary 6.5. Suppose Assumptions 2.1, 2.2, and 2.3 hold and the iterates {xt}t≥0are generated by
the BFGS method with step size satisfying the Armijo-Wolfe conditions in (5)and(6), and x0∈Rd
as an arbitrary initial point. Then, given the result in Theorem 6.4, the following results hold:
•If we set B0=LI, then we have
f(xt)−f(x∗)
f(x0)−f(x∗)≤ 
δ7dκ+ (3δ6
α(1−β)logC0
δ1+3δ8
α(1−β)C0)κ
t!t
. (27)
•If we set B0=µI, then we have
f(xt)−f(x∗)
f(x0)−f(x∗)≤ 
(δ6+δ7+δ8C0)dlogκ+ (3δ6
α(1−β)logC0
δ1+3δ8
α(1−β)C0)κ
t!t
.(28)
This result shows that BFGS with B0=LIachieves a global superlinear rate of O((dκ+C0κ
t)t), while
BFGS with the initialization B0=µIconverges at a global superlinear rate of O((C0dlogκ+C0κ
t)t).
Hence, the superlinear result for B0=µIoutperforms the rate for B0=LIwhen C0logκ≪κ.
Remark 6.2. We chose B0=LIandB0=µIas two specific cases since they lead to explicit upper
bounds in terms of the dimension dand the condition number κin various theorems, simplifying
the interpretation of our results. In practice, however, we often set B0=cI, where c=s⊤y
∥s∥2, with
s=x2−x1,y=∇f(x2)− ∇f(x1), and x1, x2as two randomly selected vectors. This choice
ensures c∈[µ, L], and in the following numerical experiments, the performance of B0=cIis
similar to that of B0=µI. The complexity of BFGS with this initialization is reported in Appendix H.
7 Complexity Analysis
Discussions on the iteration complexity. Using the three established convergence results in The-
orems 4.1, 5.2 and 6.4, we can characterize the total number of iterations required for the BFGS
method with the Armijo-Wolfe line search to find a solution with function suboptimality less than ϵ.
However, as discussed above, the choice of the initial Hessian approximation B0heavily influences
the number of iterations required to observe these rates. To simplify our discussion, we focus on two
specific initializations: B0=LIandB0=µI.
The case of B0=LI:The overall iteration complexity of BFGS with B0=LIis given by
O
min

κlog1
ϵ,(d+C0)κ+ log1
ϵ,log1
ϵ
log
1
2+q
1
4+1
dκ+C0κlog1
ϵ


.
The case of B0=µI:The overall iteration complexity of BFGS with B0=µIis given by
O
min

dlogκ+κlog1
ϵ, C0(dlogκ+κ) + log1
ϵ,log1
ϵ
log
1
2+q
1
4+1
C0(dlogκ+κ)log1
ϵ


.
8We remark that the comparison between these two complexity bounds depends on the relative values
ofκ,d,C0, and ϵ, and neither is uniformly better than the other. It is worth noting that for BFGS
withB0=LI, we achieve a complexity that is consistently superior to the O 
κlog1
ϵ
complexity
of gradient descent. Moreover, in scenarios where C0=O(1)andd≪κ, BFGS with B0=µI
could result in an iteration complexity of O 
κ+ log1
ϵ
, which is much more favorable than that of
gradient descent. The proof of these complexity bounds can be found in Appendix I.
Discussions on the line search complexity. We present the log bisection algorithm to choose
the step size ηtat iteration tsatisfying the Armijo-Wolfe conditions (5)and(6)in Algorithm 1
in Appendix J. We define ηminandηmax as the lower and upper bounds of the “slicing window”
containing the trial step size ηt, respectively. We start with the initial trial step size ηt= 1 and
keep enlarging or decreasing it depending on whether the Armijo condition (5)or the curvature
condition (6)is satisfied. Then, we dynamically update ηmin,ηmax and shrink the size of this “slicing
window” (ηmin, ηmax). We pick the trial step size ηas the geometric mean of ηminandηmax, i.e.,
logη= (log ηmax+ log ηmax)/2, which is the reason why we call this algorithm “log bisection”.
Note that in each loop of Algorithm 1, we query the function value and gradient at most once to check
the Armijo-Wolfe conditions at Lines 2 and 9. The next theorem characterizes the average number of
function value and gradient evaluations per iteration in Algorithm 1 after titerations, denoted by Λt,
which is equivalent to the average number of loops per iterations.
Theorem 7.1. Suppose Assumptions 2.1, 2.2 and 2.3 hold. Let {xt}t≥0be generated by BFGS
with step size satisfying the Armijo-Wolfe conditions in (5)and(6)and is chosen by Algorithm 1.
If we define σ:= (Ψ( ¯B0) +3
α(1−β)κ)C0, then for any initial point x0∈Rdand initial Hessian
approximation B0∈Sd
++, the average number of the function value and gradient evaluations per
iteration in Algorithm 1 after titerations satisfies
Λt≤2+log2
1+1−β
β−α+2(1−β)
β−ασ
t
+2 log2
log216(1−α)+log2 
1+σ
t)+6Ψ(˜B0) + 12 σ
t
.
The above result shows that when we run BFGS for Niterations, the total number of function and
gradient evaluations is O 
N+Nlog(1 +σ
N) +Nlog(1 +Ψ(˜B0)+σ
N)
. Thus, the total line search
complexity can always be bounded by O(Nlog(Ψ( ˜B0) +σ)) =O(Nmax{logd,logκ,logC0}).
Furthermore, notice that when Nis sufficiently large such that we reach the superlinear convergence
stage, i.e., N= Ω(Ψ( ˜B0) +σ), the total line search complexity becomes O(N), which means the
average number of function and gradient evaluations per iteration is a constant O(1). We report the
line search complexity results of different B0=LIandB0=µIin Appendix K.4.
8 Numerical Experiments
We conduct numerical experiments on a cubic objective function defined as
f(x) =α
12 d−1X
i=1g(v⊤
ix−v⊤
i+1x)−βv⊤
1x!
+λ
2∥x∥2, (29)
andg:R→Ris defined as
g(w) =1
3|w|3|w| ≤∆,
∆w2−∆2|w|+1
3∆3|w|>∆,(30)
where α, β, λ, ∆∈Rare hyper-parameters and {vi}n
i=1are standard orthogonal unit vectors in Rd.
We focus on this objective function because it is used in [26] to establish a tight lower bound for
second-order methods. We compare the convergence paths of BFGS with an inexact line search step
sizeηtthat satisfies the Armijo-Wolfe conditions (5)and(6)for various initialization matrices B0:
specifically, B0=LI,B0=µI,B0=I, andB0=cIwhere cis defined in Remark 6.2. It is easily
verified that c∈[µ, L]. We also compare the performance of BFGS methods to the gradient descent
(GD) method with backtracking line search, using α= 0.1in condition (5)andβ= 0.9in condition
(6). Step size ηtis chosen at each iteration via log bisection in Algorithm 1. Empirical results are
compared across various dimensions dand condition numbers κ, with the x-axis representing the
number of iterations tand the y-axis showing the ratiof(xt)−f(x∗)
f(x0)−f(x∗).
90 300 600 900 1200 1500 1800 2100 2400 2700 300010-2010-1510-1010-5100(a)d= 100 ,κ= 100 .
0 300 600 900 1200 1500 1800 2100 2400 2700 300010-2010-1510-1010-5100 (b)d= 100 ,κ= 1000 .
0 500 1000 1500 2000 2500 3000 3500 4000 4500 500010-2010-1510-1010-5100 (c)d= 300 ,κ= 100 .
0 500 1000 1500 2000 2500 3000 3500 4000 4500 500010-2010-1510-1010-5100
(d)d= 300 ,κ= 1000 .
0 600 1200 1800 2400 3000 3600 4200 4800 5400 600010-2010-1510-1010-5100 (e)d= 600 ,κ= 100 .
0
1200 2400 3600 4800 6000 7200 8400 9600 10800 1200010-2010-1510-1010-5100 (f)d= 600 ,κ= 1000 .
Figure 1: Convergence curves of BFGS with inexact line search of different B0and gradeint descent
with backtracking line search.
First, we observe that BFGS with B0=LIinitially converges faster than BFGS with B0=µI
in most plots, aligning with our theoretical findings that the linear convergence rate of BFGS with
B0=LIsurpasses that of B0=µIin Corollary 4.2. In Corollary 4.2, we show that BFGS
withB0=LIcould achieve the linear rate of (1−1/κ)from the first iteration while BFGS with
B0=µIneeds to run dlogκto reach the same linear rate. Second, the transition to superlinear
convergence for BFGS with B0=µItypically occurs around t≈d, as predicted by our theoretical
analysis. Although BFGS with B0=LIinitially converges faster, its transition to superlinear
convergence consistently occurs later than for B0=µI. Notably, for a fixed dimension d= 600 , the
transition to superlinear convergence for B0=LIoccurs increasingly later as the problem condition
number rises, an effect not observed for B0=µI. This phenomenon indicates that the superlinear
rate for B0=LIis more sensitive to the condition number κ, which corroborates our results in
Corollary 6.5. In Corollary 6.5, we present that BFGS with B0=LIneeds dκsteps to reach the
superlinear convergence stage while this is improved to dlogκfor BFGS with B0=µI. Moreover,
the performance of BFGS with B0=IandB0=cIis similar to BFGS with B0=µI. Notice that
the initializations of B0=IandB0=cIare two commonly-used practical choices of the initial
Hessian approximation matrix B0.
9 Conclusions, Limitations, and Future Directions
In this paper, we analyzed the global non-asymptotic convergence rates of BFGS with Armijo-
Wolfe line search. We showed for an objective function that is µ-strongly convex with an L-
Lipschitz gradient, BFGS achieves a global convergence rate of (1−1/κ)t, where κ=L/µ.
Additionally, assuming the Hessian is M-Lipschitz, we showed BFGS achieves a linear convergence
rate determined solely by the line search parameters, independent of the condition number. Under
similar assumptions, we also established a global superlinear convergence rate. Given these bounds,
we determined the overall iteration complexity of BFGS with the Armijo-Wolfe line search and
specified this complexity for initial Hessian approximations B0=LIandB0=µI.
One limitation of this paper is that the analysis only applies to strongly convex functions. Developing
an analysis for the general convex setting is still unsolved. Another drawback is that we focus solely
on the BFGS method. Extending our theoretical results to the entire convex Broyden’s class of
quasi-Newton methods, including both BFGS and DFP, is a natural next step.
10Acknowledgments
The research of Q. Jin, R. Jiang, and A. Mokhtari is supported in part by NSF Award 2007668 and
the NSF AI Institute for Foundations of Machine Learning (IFML).
References
[1] W. Davidon. Variable metric method for minimization . Tech. rep. Argonne National Lab.,
Lemont, Ill., 1959 (page 1).
[2] R. Fletcher and M. J. Powell. “A rapidly convergent descent method for minimization”. The
computer journal 6.2 (1963), pp. 163–168 (page 1).
[3] C. G. Broyden. “The convergence of single-rank quasi-Newton methods”. Mathematics of
Computation 24.110 (1970), pp. 365–382 (page 1).
[4] R. Fletcher. “A new approach to variable metric algorithms”. The computer journal 13.3
(1970), pp. 317–322 (page 1).
[5] D. Goldfarb. “A family of variable-metric methods derived by variational means”. Mathematics
of computation 24.109 (1970), pp. 23–26 (page 1).
[6] D. F. Shanno. “Conditioning of quasi-Newton methods for function minimization”. Mathemat-
ics of computation 24.111 (1970), pp. 647–656 (page 1).
[7] A. R. Conn, N. I. M. Gould, and P. L. Toint. “Convergence of quasi-Newton matrices generated
by the symmetric rank one update”. Mathematical programming 50.1-3 (1991), pp. 177–195
(page 1).
[8] H. F. Khalfan, R. H. Byrd, and R. B. Schnabel. “A theoretical and experimental study of the
symmetric rank-one update”. SIAM J. Optim. 3.1 (1993), pp. 1–24 (page 1).
[9] C. G. Broyden. “A class of methods for solving nonlinear simultaneous equations”. Mathemat-
ics of computation 19.92 (1965), pp. 577–593 (page 1).
[10] R. Gower, D. Goldfarb, and P. Richtárik. “Stochastic block BFGS: Squeezing more curvature
out of data”. In: Int. Conference on Machine Learning . PMLR. 2016, pp. 1869–1878 (page 1).
[11] R. M. Gower and P. Richtárik. “Randomized quasi-Newton updates are linearly convergent
matrix inversion algorithms”. SIAM Journal on Matrix Analysis and Applications 38.4 (2017),
pp. 1380–1409 (page 1).
[12] D. Kovalev, R. M. Gower, P. Richtárik, and A. Rogozin. “Fast linear convergence of random-
ized BFGS”. arXiv preprint arXiv:2002.11337 (2020) (page 1).
[13] D. Lin, H. Ye, and Z. Zhang. “Greedy and random quasi-Newton methods with faster explicit
superlinear convergence”. Advances in Neural Information Processing Systems 34 (2021),
pp. 6646–6657 (page 1).
[14] D. Lin, H. Ye, and Z. Zhang. “Explicit convergence rates of greedy and random quasi-Newton
methods”. Journal of Machine Learning Research 23.162 (2022), pp. 1–40 (page 1).
[15] A. Rodomanov and Y . Nesterov. “Greedy Quasi-Newton Methods with Explicit Superlinear
Convergence”. SIAM Journal on Optimization 31.1 (2021), pp. 785–811 (page 1).
[16] Z.-Y . Ji and Y . -H. Dai. “Greedy PSB methods with explicit superlinear convergence”. Compu-
tational Optimization and Applications 85.3 (2023), pp. 753–786 (page 1).
[17] R. Jiang, Q. Jin, and A. Mokhtari. “Online Learning Guided Curvature Approximation: A
Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence”. In: Proceed-
ings of Thirty Sixth Conference on Learning Theory . V ol. 195. 2023, pp. 1962–1992 (page 1).
[18] R. Jiang and A. Mokhtari. “Accelerated quasi-newton proximal extragradient: Faster rate for
smooth convex optimization”. Advances in Neural Information Processing Systems 36 (2023)
(page 1).
[19] C. G. Broyden, J. E. Dennis Jr, and J. J. Moré. “On the local and superlinear convergence
of quasi-Newton methods”. IMA Journal of Applied Mathematics 12.3 (1973), pp. 223–245
(pages 1, 3).
[20] J. E. Dennis and J. J. Moré. “A characterization of superlinear convergence and its application
to quasi-Newton methods”. Mathematics of computation 28.126 (1974), pp. 549–560 (pages 1,
3).
[21] A. Griewank and P. L. Toint. “Local convergence analysis for partitioned quasi-Newton
updates”. Numerische Mathematik 39.3 (1982), pp. 429–448 (pages 1, 3).
11[22] J. Dennis, H. J. Martinez, and R. A. Tapia. “Convergence theory for the structured BFGS
secant method with an application to nonlinear least squares”. Journal of Optimization Theory
and Applications 61.2 (1989), pp. 161–178 (pages 1, 3).
[23] Y . Yuan. “A modified BFGS algorithm for unconstrained optimization”. IMA Journal of
Numerical Analysis 11.3 (1991), pp. 325–332 (pages 1, 3).
[24] M. Al-Baali. “Global and superlinear convergence of a restricted class of self-scaling methods
with inexact line searches, for convex functions”. Computational Optimization and Applica-
tions 9.2 (1998), pp. 191–203 (pages 1, 3).
[25] D. Li and M. Fukushima. “A Globally and Superlinearly Convergent Gauss–Newton-Based
BFGS Method for Symmetric Nonlinear Equations”. SIAM Journal on Numerical Analysis
37.1 (1999), pp. 152–172 (pages 1, 3).
[26] H. Yabe, H. Ogasawara, and M. Yoshino. “Local and superlinear convergence of quasi-
Newton methods based on modified secant conditions”. Journal of Computational and Applied
Mathematics 205.1 (2007), pp. 617–632 (pages 1, 3, 9).
[27] A. Mokhtari, M. Eisen, and A. Ribeiro. “IQN: An incremental quasi-Newton method with local
superlinear convergence rate”. SIAM Journal on Optimization 28.2 (2018), pp. 1670–1698
(pages 1, 3).
[28] W. Gao and D. Goldfarb. “Quasi-Newton methods: superlinear convergence without line
searches for self-concordant functions”. Optimization Methods and Software 34.1 (2019),
pp. 194–217 (pages 1, 3).
[29] M. Powell. “On the convergence of the variable metric algorithm”. IMA Journal of Applied
Mathematics 7.1 (1971), pp. 21–36 (page 1).
[30] M. J. Powell. “Some global convergence properties of a variable metric algorithm for mini-
mization without exact line searches”. Nonlinear programming 9.1 (1976), pp. 53–72 (page 1).
[31] R. H. Byrd, J. Nocedal, and Y . Yuan. “Global convergence of a class of quasi-Newton methods
on convex problems”. SIAM Journal on Numerical Analysis 24.5 (1987), pp. 1171–1190
(page 1).
[32] R. H. Byrd and J. Nocedal. “A Tool for the Analysis of Quasi-Newton Methods with Applica-
tion to Unconstrained Minimization”. SIAM Journal on Numerical Analysis, Vol. 26, No. 3
(1989) (pages 1, 4).
[33] R. H. Byrd, H. F. Khalfan, and R. B. Schnabel. “Analysis of a symmetric rank-one trust region
method”. SIAM Journal on Optimization 6.4 (1996), pp. 1025–1039 (page 1).
[34] A. Rodomanov and Y . Nesterov. “Rates of Superlinear Convergence for Classical Quasi-
Newton Methods”. Mathematical Programming (2021), pp. 1–32 (pages 2, 19).
[35] A. Rodomanov and Y . Nesterov. “New Results on Superlinear Convergence of Classical
Quasi-Newton Methods”. Journal of Optimization Theory and Applications 188.3 (2021),
pp. 744–769 (page 2).
[36] H. Ye, D. Lin, X. Chang, and Z. Zhang. “Towards explicit superlinear convergence rate for
SR1”. Mathematical Programming 199.1 (2023), pp. 1273–1303 (page 2).
[37] Q. Jin and A. Mokhtari. “Non-asymptotic Superlinear Convergence of Standard Quasi-Newton
Methods”. Mathematical Programming, Volume 200, pages 425–473 (2022) (page 2).
[38] Q. Jin, R. Jiang, and A. Mokhtari. “Non-asymptotic Global Convergence Rates of BFGS with
Exact Line Search”. arXiv preprint arXiv:2404.01267 (2024) (pages 2, 5, 16, 37).
[39] V . Krutikov, E. Tovbis, P. Stanimirovi ´c, and L. Kazakovtsev. “On the Convergence Rate of
Quasi-Newton Methods on Strongly Convex Functions with Lipschitz Gradient”. Mathematics
11.23 (2023), p. 4715 (page 2).
[40] A. Rodomanov. “Global Complexity Analysis of BFGS”. arXiv preprint arXiv:2404.15051
(2024) (pages 2, 7).
[41] J. Nocedal and S. Wright. Numerical optimization . Springer Science Business Media, 2006
(pages 2, 3, 5).
[42] P. Wolfe. “Convergence Conditions for Ascent Methods”. SIAM Review 11.2 (1969), pp. 226–
235 (page 3).
[43] P. Wolfe. “Convergence Conditions for Ascent Methods. II: Some Corrections”. SIAM Review
13.2 (1971), pp. 185–188 (page 3).
[44] Y . Nesterov. Lectures on convex optimization . Springer Optimization and Its Applications
(SOIA, volume 137), 2018 (page 20).
12Appendix
A Some Results on the Connections between Different Hessian Matrices
Lemma A.1. Suppose Assumptions 2.1, 2.2, and 2.3 hold, and recall the definitions of the matrices
JtandGtin(8), and the quantity Ctin(9). Then, the following statements hold:
(a) Suppose that f(xt+1)≤f(xt)for any t≥0, we have that
1
1 +Ct∇2f(x∗)⪯Jt⪯(1 +Ct)∇2f(x∗). (31)
(b) Suppose that f(xt+1)≤f(xt)for any t≥0andˆτ∈[0,1], we have that
1
1 +Ct∇2f(x∗)⪯ ∇2f(xt+ ˆτ(xt+1−xt))⪯(1 +Ct)∇2f(x∗). (32)
(c) For any t≥0, we have that
1
1 +Ct∇2f(x∗)⪯ ∇2f(xt)⪯(1 +Ct)∇2f(x∗). (33)
(d) For any t≥0, we have that
1
1 +Ct∇2f(x∗)⪯Gt⪯(1 +Ct)∇2f(x∗). (34)
(e) For any t≥0and˜τ∈[0,1], we have that
1
1 +CtGt⪯ ∇2f(xt+ ˜τ(x∗−xt))⪯(1 +Ct)Gt. (35)
(f) For any t≥0and˜τ,ˆτ∈[0,1], suppose that f(xt+1)≤f(xt). Then, we have that
1
1 + 2 Ct∇2f(xt+ ˆτst)⪯ ∇2f(xt+ ˜τst)⪯(1 + 2 Ct)∇2f(xt+ ˆτst). (36)
Proof. (a) Recall the definition of Jtin (8). Using the triangle inequality, we have that
∥∇2f(x∗)−Jt∥=Z1
0 
∇2f(x∗)− ∇2f(xt+τ(xt+1−xt))
dτ
≤Z1
0∥∇2f(x∗)− ∇2f(xt+τ(xt+1−xt))∥dτ.
Moreover, it follows from Assumption 2.3 that ∥∇2f(x∗)− ∇2f(xt+τ(xt+1−xt))∥ ≤
M∥(1−τ)(x∗−xt) +τ(x∗−xt+1)∥for any τ∈[0,1]. Thus, we can further apply the
triangle inequality to obtain
∥∇2f(x∗)−Jt∥ ≤Z1
0M∥(1−τ)(x∗−xt) +τ(x∗−xt+1)∥dτ
≤M∥xt−x∗∥Z1
0(1−τ)dτ+M∥xt+1−x∗∥Z1
0τdτ
=M
2(∥xt−x∗∥+∥xt+1−x∗∥).
Since fis strongly convex, by Assumption 2.1 and f(xt+1)≤f(xt), we haveµ
2∥xt−
x∗∥2≤f(xt)−f(x∗), which implies that ∥xt−x∗∥ ≤p
2(f(xt)−f(x∗))/µ. Similarly,
since f(xt+1)≤f(xt), it also holds that ∥xt+1−x∗∥ ≤p
2(f(xt+1)−f(x∗))/µ≤p
2(f(xt)−f(x∗))/µ. Hence, we obtain that
∥∇2f(x∗)−Jt∥ ≤M√µp
2(f(xt)−f(x∗)). (37)
13Moreover, notice that by Assumption 2.1, we also have Jt⪰µIand∇2f(x∗)⪰µI. Hence,
(37) implies that
∇2f(x∗)−Jt⪯ ∥∇2f(x∗)−Jt∥I⪯M
µ3
2p
2(f(xt)−f(x∗))Jt=CtJt,
Jt− ∇2f(x∗)⪯ ∥Jt− ∇2f(x∗)∥I⪯M
µ3
2p
2(f(xt)−f(x∗))∇2f(x∗) =Ct∇2f(x∗).
where we used the definition of Ctin (9). By rearranging the terms, we obtain (31).
(b) Similar to the arguments in (a), for any ˆτ∈[0,1], we have that
∇2f(xt+ ˆτ(xt+1−xt))− ∇2f(x∗)
≤M∥(1−ˆτ)(xt−x∗) + ˆτ(xt+1−x∗)∥
≤M
(1−ˆτ)∥xt−x∗∥+ ˆτ∥xt+1−x∗∥
≤M
(1−ˆτ)r2
µ(f(xt)−f(x∗)) + ˆτr2
µ(f(xt+1)−f(x∗))
≤Mr2
µ(f(xt)−f(x∗))
Moreover, notice that by Assumption 2.1, we also have ∇2f(xt+ ˆτ(xt+1−xt))⪰µIand
∇2f(x∗)⪰µI. The rest follows similarly as in the proof of (a) and we prove (32).
(c) Similar to the arguments in (a), we have that
∇2f(x∗)− ∇2f(xt)≤M∥xt−x∗∥ ≤M√µp
2(f(xt)−f(x∗)).
Moreover, notice that by Assumption 2.1 we also have ∇2f(xt)⪰µIand∇2f(x∗)⪰µI.
The rest follows similarly as in the proof of (a) and we prove (33).
(d) Recall the definition of Gtin (8). Similar to the arguments in (a), we have that
∥∇2f(x∗)−Gt∥=Z1
0 
∇2f(x∗)− ∇2f(xt+τ(x∗−xt))
dτ
≤Z1
0∥∇2f(x∗)− ∇2f(xt+τ(x∗−xt))∥dτ
≤MZ1
0∥(1−τ)(x∗−xt)∥dτ=M∥xt−x∗∥Z1
0(1−τ)dτ
=M
2∥xt−x∗∥ ≤M√µp
2(f(xt)−f(x∗)).
Moreover, notice that by Assumption 2.1 we also have Gt⪰µIand∇2f(x∗)⪰µI. The
rest follows similarly as in the proof of (a) and we prove (34).
(e)Recall the definition of gtin(8). Similar to the arguments in (a), for any ˜τ∈[0,1], we have
that∇2f(xt+ ˜τ(x∗−xt))−Gt
=Z1
0 
∇2f(xt+ ˜τ(x∗−xt))− ∇2f(xt+τ(x∗−xt))
dτ
≤Z1
0∇2f(xt+ ˜τ(x∗−xt))− ∇2f(xt+τ(x∗−xt))dτ
≤Z1
0M|˜τ−τ|∥xt−x∗∥dτ≤1
2M∥xt−x∗∥ ≤M√µp
2(f(xt)−f(x∗)).
Moreover, notice that by Assumption 2.1, we also have ∇2f(xt+ ˜τ(x∗−xt))⪰µIand
Gt⪰µI. The rest follows similarly as in the proof of (a) and we prove (35).
14(f) Similar to the arguments in (a), for any ˜τ,ˆτ∈[0,1], we have that
∇2f(xt+ ˜τst)− ∇2f(xt+ ˆτst)
≤M|˜τ−ˆτ|∥st∥ ≤M∥st∥ ≤M(∥xt+1−x∗∥+∥xt−x∗∥)
≤Mr2
µ(f(xt)−f(x∗)) +r2
µ(f(xt+1)−f(x∗))
≤2Mr2
µ(f(xt)−f(x∗))
Moreover, notice that by Assumption 2.1, we also have ∇2f(xt+ ˜τst)⪰µIand
∇2f(xt+ ˆτst)⪰µI. The rest follows similarly as in the proof of (a) and we prove
(36).
B Proof of Lemma 2.1
Recall that gt=∇f(xt). Given the condition in (5) and the fact that st=ηtdt, we have
f(xt+1)≤f(xt) +αg⊤
tst.
Moreover, since Btis symmetric positive definite, we have −g⊤
tst=ηtg⊤
tB−1
tgt>0(unless gt= 0
and we are at the optimal solution). This further leads to the first claim, which is
f(xt)−f(xt+1)
−g⊤
tst≥α.
Similarly, the above argument implies that αg⊤
tst<0and as a result f(xt+1)≤f(xt)and the last
claim also follows.
To prove the second claim, we leverage the condition in (6). Specifically, if we subtract g⊤
tdtfrom
both sides of that condition, we obtain that
(gt+1−gt)⊤dt≥(β−1)g⊤
tdt
Next, using the fact that st=ηtdt, by multiplying both sides by ηtand use the simplification
yt=gt+1−gtwe obtain that
y⊤
tst≥(β−1)g⊤
tst=−g⊤
tst(1−β).
Again using the argument that −g⊤
tstis positive (if we are not at the optimal solution), we can divide
both sides of the above inequality by −g⊤
tst, leading to the second claim.
C Proof of Proposition 3.1
First, we note that ˆg⊤
tˆst=g⊤
tstandˆy⊤
tˆst=y⊤
tst. Using the definition of ˆptin (16), we have that
f(xt)−f(xt+1) = ˆpt−ˆg⊤
tˆst
∥ˆgt∥2∥ˆgt∥2. (38)
Hence, using the definition of ˆθtin (14) and the definition of ˆmt,ˆntin (16), it follows that
−ˆg⊤
tˆst
∥ˆgt∥2=(ˆg⊤
tˆst)2
∥ˆgt∥2∥ˆst∥2∥ˆst∥2
−ˆg⊤
tˆst=(ˆg⊤
tˆst)2
∥ˆgt∥2∥ˆst∥2∥ˆst∥2
ˆy⊤
tˆstˆy⊤
tˆst
−ˆg⊤
tˆst= ˆntcos2(ˆθt)
ˆmt.
Furthermore, we have ∥ˆgt∥2= ˆqt(f(xt)−f(x∗))from the definition of ˆqtin(16). Thus, the equality
in (38) can be rewritten as
f(xt)−f(xt+1) = ˆptˆqtˆntcos2(ˆθt)
ˆmt(f(xt)−f(x∗)).
15By rearranging the term in the above equality, we obtain
f(xt+1)−f(x∗) =
1−ˆptˆqtˆntcos2(ˆθt)
ˆmt
(f(xt)−f(x∗)), (39)
To prove the inequality in (15), note that for any t≥1, we have
f(xt)−f(x∗)
f(x0)−f(x∗)=t−1Y
i=0f(xi+1)−f(x∗)
f(xi)−f(x∗)=t−1Y
i=0 
1−ˆpiˆqiˆnicos2(ˆθi)
ˆmi!
,
where the last equality is due to (39). Note that all the terms of the form 1−ˆpiˆqiˆnicos2(ˆθi)
ˆmiare
non-negative, for any i≥0. Thus, by applying the inequality of arithmetic and geometric means
twice, we obtain
t−1Y
i=0 
1−ˆpiˆqiˆnicos2(ˆθi)
ˆmi!
≤"
1
tt−1X
i=0 
1−ˆpiˆqiˆnicos2(ˆθi)
ˆmi!#t
="
1−1
tt−1X
i=0ˆpiˆqiˆnicos2(ˆθi)
ˆmi#t
≤
1− t−1Y
i=0ˆpiˆqiˆnicos2(ˆθi)
ˆmi!1
t
t
.
This completes the proof.
D Results from [38]
In this section, we summarize some results that we use from [38] to establish a lower bound onQt−1
i=0cos2(ˆθi)
ˆmiandˆqt.
Proposition D.1 ([38, Proposition 2]) .Let{Bt}t≥0be the Hessian approximation matrices generated
by the BFGS update in (3). For a given weight matrix P∈Sd
++, recall the weighted vectors defined
in(11) and the weighted matrix in (12). Then, we have
Ψ(ˆBt+1)≤Ψ(ˆBt) +∥ˆyt∥2
ˆy⊤
tˆst−1 + logcos2ˆθt
ˆmt,∀t≥0,
where ˆmtis defined in (16) andcos(ˆθt)is defined in (14). As a corollary, we have,
t−1X
i=0logcos2(ˆθi)
ˆmi≥ −Ψ(ˆB0) +t−1X
i=0
1−∥ˆyi∥2
ˆy⊤
iˆsi
,∀t≥1. (40)
If we take exponentiation on both sides of the above inequality (40) in Proposition D.1, we can obtain
a lower bound for the productQt−1
i=0cos2(ˆθi)
ˆmiwith the sumPt−1
i=0∥ˆyi∥2
ˆs⊤
iˆyiandΨ(ˆB0). This classical
inequality describing the relationship between the ratiocos2(ˆθt)
ˆmtand the potential function Ψ(.)plays
a critical role in the following convergence analysis.
In the following two lemmas, we provide bounds on the quantities ˆqtand∥ˆyt∥2/ˆs⊤
tˆytrespectively by
directly citing results from Lemma 4 and Lemma 5 in [38] again. Notice that both ˆqtand∥ˆyt∥2/ˆs⊤
tˆyt
depend on different choices of the weight matrix P.
Lemma D.2 ([38, Lemma 4]) .Recall the definition ˆqt=∥ˆgt∥2
f(xt)−f(x∗)in(16). Suppose Assump-
tions 2.1, 2.2, and 2.3 hold. Then we have the following results:
(a) If we choose P=LI, then ˆqt≥2/κ.
(b) If we choose P=∇2f(x∗), then ˆqt≥2/(1 +Ct)2.
Lemma D.3 ([38, Lemma 5]) .Let{xt}t≥0be the iterates generated by the BFGS algorithm with
inexact line search satisfying (5)and(6). Suppose Assumptions 2.1, 2.2, and 2.3 hold. Then we have
the following results:
(a) If we choose P=LI, then∥ˆyt∥2
ˆs⊤
tˆyt≤1.
(b) If we choose P=∇2f(x∗), then∥ˆyt∥2
ˆs⊤
tˆyt≤1 +Ct.
16E Proofs in Section 4
E.1 Proof of Theorem 4.1
Recall that we choose P=LIthroughout the proof. Note that given this weight matrix, it can
be easily verified that∥ˆyt∥2
ˆs⊤
tˆyt≤1for any t≥0by using Lemma D.3 (a). Hence, we use (40) in
Proposition D.1 to obtain
t−1X
i=0logcos2(ˆθi)
ˆmi≥ −Ψ(¯B0) +t−1X
i=0
1−∥ˆyi∥2
ˆs⊤
iˆyi
≥ −Ψ(¯B0),
which further implies that
t−1Y
i=0cos2(ˆθi)
ˆmi≥e−Ψ(¯B0).
Moreover, for the choice P=LI, it can be shown that ˆqt=∥gt∥2
L(f(xt)−f(x∗))≥2
κby using Lemma D.2
(a). From Lemma 2.1, we know ˆpt≥αandˆnt≥1−β, which lead to
t−1Y
i=0ˆpiˆniˆqi
ˆmicos2(ˆθi)≥t−1Y
i=0ˆpit−1Y
i=0ˆqit−1Y
i=0ˆnit−1Y
i=0cos2(ˆθi)
ˆmi≥2α(1−β)
κt
e−Ψ(¯B0).
Thus, it follows from Proposition 3.1 that
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1− t−1Y
i=0ˆpiˆqiˆni
ˆmicos2(ˆθi)!1
t
t
≤
1−e−Ψ(¯B0)
t2α(1−β)
κt
.
This completes the proof.
E.2 Proof of Corollary 4.2
Notice that in the first case where B0=LI, we have Ψ(¯B0) = 0 and thus it achieves the best
linear convergence results according to Theorem 4.1. On the other hand, for B0=µI, we have
Ψ(¯B0) = Ψ(µ
LI) =d(1
κ−1 + log κ)≤dlogκ. We complete the proof by combining these
conditions with the inequality (17) in Theorem 4.1. Notice that e−x≥e−1≥1
3forx≤1.
F Proofs in Section 5
F.1 Proof of Proposition 5.1
Recall that we choose the weight matrix as P=∇2f(x∗)throughout the proof. Similar to the proof
of Theorem 4.1, we start from the key inequality in (15), but we apply different bounds on the ˆqtand
cos2(ˆθt)
ˆmt. Specifically, by using Lemma D.3 (b), we have∥ˆyt∥2
ˆs⊤
tˆyt≤1 +Ctfor any t≥0. Hence, we
use (40) in Proposition D.1 to obtain
t−1X
i=0logcos2(ˆθi)
ˆmi≥ −Ψ(˜B0) +t−1X
i=0
1−∥ˆyi∥2
ˆs⊤
iˆyi
≥ −Ψ(˜B0)−t−1X
i=0Ci,
which further implies that
t−1Y
i=0cos2(ˆθi)
ˆmi≥e−Ψ(˜B0)−Pt−1
i=0Ci. (41)
Moreover, since ˆqt≥2
(1+Ct)2for any t≥0by using Lemma D.2 (b), we get
t−1Y
i=0ˆqi≥t−1Y
i=02
(1 +Ci)2≥2tt−1Y
i=0e−2Ci= 2te−2Pt−1
i=0Ci, (42)
17where we use the inequality 1 +x≤exfor any x∈R. From Lemma 2.1, we know ˆpt≥αand
ˆnt≥1−β, which lead to
t−1Y
i=0ˆpiˆni≥αt(1−β)t. (43)
Combining (41), (42), (43) and (15) from Proposition 3.1, we prove that
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1− t−1Y
i=0ˆpiˆqiˆni
ˆmicos2(ˆθi)!1
t
t
≤
1−2α(1−β)e−Ψ(˜B0)+3Pt−1
i=0Ci
tt
.
This completes the proof.
F.2 Proof of Theorem 5.2
When we have t≥Ψ(˜B0) + 3Pt−1
i=0Ci, Proposition 5.1 implies the condition thatf(xt)−f(x∗)
f(x0)−f(x∗)≤

1−2α(1−β)
et
≤
1−2α(1−β)
3t
, which leads to the linear rate in (20). Hence, it is sufficient
to establish an upper bound onPt−1
i=0Ci. Recall that Ci=M
µ3
2p
2(f(xi)−f(x∗))defined in (9).
We decompose the sum into two parts:P⌈Ψ(¯B0)⌉−1
i=0CiandPt
i=⌈Ψ(¯B0)⌉Ci. For the first part, note
that since f(xi+1)≤f(xi)by Lemma 2.1, we also have Ci+1≤Cifori≥0. Hence, we haveP⌈Ψ(¯B0)⌉−1
i=0Ci≤C0⌈Ψ(¯B0)⌉ ≤C0(Ψ(¯B0) + 1) . Moreover, by Theorem 4.1, when t≥Ψ(¯B0)
we have
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1−e−Ψ(¯B0)
t2α(1−β)
κt
≤
1−2α(1−β)
eκt
≤
1−2α(1−β)
3κt
.
Hence, this further implies that
tX
i=⌈Ψ(¯B0)⌉Ci=C0tX
i=⌈Ψ(¯B0)⌉s
f(xi)−f(x∗)
f(x0)−f(x∗)≤C0tX
i=⌈Ψ(¯B0)⌉
1−2α(1−β)
3κi
2
≤C0∞X
i=1
1−2α(1−β)
3κi
2
≤C03κ
α(1−β)−1
,
where we used the fact thatP∞
i=1(1−ρ)i
2=√1−ρ
1−√1−ρ=√1−ρ+1−ρ
ρ≤2
ρ−1for any ρ∈(0,1).
Hence, by combining both inequalities, we have
t−1X
i=0Ci=⌈Ψ(¯B0)⌉−1X
i=0Ci+tX
i=⌈Ψ(¯B0)⌉Ci≤C0Ψ(¯B0) +3C0κ
α(1−β). (44)
Hence, this proves that (20) is satisfied when t≥Ψ(˜B0) + 3C0Ψ(¯B0) +9C0κ
α(1−β).
F.3 Proof of Corollary 5.3
ForB0=LI, we have ¯B0=1
LB0=Iand˜B0=∇2f(x∗)−1
2B0∇2f(x∗)−1
2=L∇2f(x∗)−1.
Thus, it holds that Ψ(¯B0) = Ψ( I) = 0 . Moreover, by Assumptions 2.1 and 2.2, we have1
LI⪯
∇2f(x∗)−1⪯1
µI, which implies that I⪯˜B0⪯κI. Thus, we further have
Ψ(˜B0)≤Tr(κI)−d−logDet(I) =dκ−d≤dκ.
Combining these two results, the threshold for transition time can be bounded by Ψ(˜B0) +
3C0Ψ(¯B0) +9
α(1−β)C0κ≤dκ+9
α(1−β)C0κ. Hence, by Theorem 5.2, the linear rate in (20)
is achieved when t≥dκ+9
α(1−β)C0κ.
18ForB0=µI, we have ¯B0=1
LB0=1
κIand˜B0=∇2f(x∗)−1
2B0∇2f(x∗)−1
2=µ∇2f(x∗)−1.
Thus, it holds that Ψ(¯B0) = Ψ(1
κI) =d
κ−d+dlogκ≤dlogκ. Moreover, by Assumptions 2.1
and 2.2, we have1
κI⪯˜B0⪯I. This implies that
Ψ(˜B0) =Tr(˜B0)−d−logDet(˜B0)≤Tr(I)−d−logDet(1
κI) =dlogκ.
Combining these two results, the threshold for the transition tume can be bounded by Ψ(˜B0) +
3C0Ψ(¯B0) +9
α(1−β)C0κ≤(1 + 3 C0)dlogκ+9
α(1−β)C0κ. Hence, by Theorem 5.2, the linear rate
in (20) is satisfied when t≥(1 + 3 C0)dlogκ+9
α(1−β)C0κ.
G Intermediate Results and Proofs in Section 6
G.1 Intermediate Results
To present our result we first introduce the following function
ω(x) :=x−log (x+ 1), (45)
which is defined for x >−1. Further In the next result, we present some basic properties of the
function ω(x)defined in (45).
Lemma G.1. Recall the definition of function ω(x)in(45), we have that
(a)ω(x)is increasing function for x >0and decreasing function for −1< x < 0. Moreover,
ω(x)≥0for all x >−1.
(b) When x≥0, we have that ω(x)≥x2
2(1+x).
(c) When −1< x≤0, we have that ω(x)≥x2
2+x.
Proof. Notice that ω′(x) =x
1+x, we know that when x >0,ω′(x)>0and when −1< x < 0,
ω′(x)<0,ω′(x)<0. Therefore, ω(x)is increasing function for x >0andω(x)is decreasing
function for −1< x < 0. Hence, ω(x)≥ω(0) = 0 for all x >−1.
ω(x)≥x2
2(1+x)is equivalent to ω1(x) := 2(1 + x)ω(x)−x2≥0. Since ω′
1(x) = 2 x−
2 log (1 + x) = 2 ω(x)≥0for all x >−1, we know that ω1(x)is increasing function for x >−1
and hence, ω1(x)≥ω1(0) = 0 forx≥0.
ω(x)≥x2
2+xis equivalent to ω2(x) := (2+ x)ω(x)−x2≥0. Since ω′
2(x) =x
1+x−log (1 + x)≤0
for all x >−1, we know that ω2(x)is decreasing function for x >−1and hence, ω2(x)≥ω2(0) = 0
forx≤0.
Proposition G.2. Let{Bt}t≥0be the Hessian approximation matrices generated by the BFGS update
in(3). Suppose Assumptions 2.1, 2.2, and 2.3 hold and f(xt+1)≤f(xt)for any t≥0. Recall the
definition of Ψ(.)in(10) andCtin(9), we have that
t−1X
i=0ω(ρi−1)≤Ψ(˜B0) + 2t−1X
i=0Ci,∀t≥1, (46)
Proof. First, taking the trace and determinant on both sides of the equation (13) for any weight matrix
P∈Sd
++and using results from Lemma 6.2 of [34], we show that
Tr(ˆBt+1) =Tr(ˆBt)−∥ˆBtˆst∥2
ˆs⊤
tˆBtˆst+∥ˆyt∥2
ˆs⊤
tˆyt,Det(ˆBt+1) =Det(ˆBt)ˆs⊤
tˆyt
ˆs⊤
tˆBtˆst.
Taking the logarithm on both sides of the second equation, we obtain that
logˆs⊤
tˆyt
ˆs⊤
tˆBtˆst= log Det(ˆBt+1)−logDet(ˆBt).
19Thus, we obtain that
Ψ(ˆBt+1)−Ψ(ˆBt) =Tr(ˆBt+1)−Tr(ˆBt) + log Det(ˆBt)−logDet(ˆBt+1)
=∥ˆyt∥2
ˆs⊤
tˆyt−∥ˆBtˆst∥2
ˆs⊤
tˆBtˆst−logˆs⊤
tˆyt
ˆs⊤
tˆBtˆst=∥ˆyt∥2
ˆs⊤
tˆyt−∥ˆBtˆst∥2
ˆs⊤
tˆBtˆst−logˆs⊤
tˆyt
∥ˆst∥2−log∥ˆst∥2
ˆs⊤
tˆBtˆst,
which leads to
∥ˆBtˆst∥2
ˆs⊤
tˆBtˆst−logˆs⊤
tˆBtˆst
∥ˆst∥2−1 = Ψ( ˆBt)−Ψ(ˆBt+1) +∥ˆyt∥2
ˆs⊤
tˆyt−1 + log∥ˆst∥2
ˆs⊤
tˆyt.
Notice that ˆBtˆst=−ηtˆgt,ˆs⊤
tˆBtˆst=−η2
tˆg⊤
tˆdtand∥ˆst∥2=η2
t∥ˆdt∥2, we have that
∥ˆgt∥2
−ˆg⊤
tˆdt−log−ˆg⊤
tˆdt
∥ˆdt∥2−1 = Ψ( ˆBt)−Ψ(ˆBt+1) +∥ˆyt∥2
ˆs⊤
tˆyt−1 + log∥ˆst∥2
ˆs⊤
tˆyt.
Note that given the fact that −ˆg⊤
tˆdt= ˆg⊤
tˆB−1
tˆgt>0, by using the Cauchy–Schwarz inequality we
obtain∥ˆgt∥2
−ˆg⊤
tˆdt≥−ˆg⊤
tˆdt
∥ˆdt∥2. Hence, we can write
−ˆg⊤
tˆdt
∥ˆdt∥2−log−ˆg⊤
tˆdt
∥ˆdt∥2−1≤Ψ(ˆBt)−Ψ(ˆBt+1) +∥ˆyt∥2
ˆs⊤
tˆyt−1 + log∥ˆst∥2
ˆs⊤
tˆyt.
Now, by selecting the weight matrix as P=∇2f(x∗), many expressions get simplified and we have
−ˆg⊤
tˆdt
∥ˆdt∥2=−g⊤
tdt
∥˜dt∥2=ρt,ρt−logρt−1 =ω(ρt−1), and ˆBt=˜Bt=∇2f(x∗)−1
2Bt∇2f(x∗)−1
2.
Hence, we have
ω(ρt−1)≤Ψ(˜Bt)−Ψ(˜Bt+1) +∥ˆyt∥2
ˆs⊤
tˆyt−1 + log∥ˆst∥2
ˆs⊤
tˆyt. (47)
Notice that∥ˆyt∥2
ˆs⊤
tˆyt≤1 +Ctfor any t≥0by using Lemma D.3 (b) with P=∇2f(x∗)and
log∥ˆst∥2
ˆs⊤
tˆyt= log∥ˆst∥2
ˆs⊤
tˆJtˆst≤log(1 + Ct)≤Ctfor any t≥0by using (31) from Lemma A.1.
Leveraging these conditions with the inequality (47), we obtain that
ω(ρt−1)≤Ψ(˜Bt)−Ψ(˜Bt+1) + 2Ct.
Summing both sides of the above inequality from i= 0tot−1, we prove the conclusion
t−1X
i=0ω(ρi−1)≤Ψ(˜B0)−Ψ(˜Bt) + 2t−1X
i=0Ci≤Ψ(˜B0) + 2t−1X
i=0Ci,
where the last inequality holds since Ψ(˜Bt)≥0.
Lemma G.3. Suppose Assumptions 2.1, 2.2, and 2.3 hold and Ct≤1
6andρt≥7
8at iteration t,
then we have
f(xt+dt)≤f(xt). (48)
Proof. Since assumption 2.3 hold, using Lemma 1.2.4 in [44], we have that
|f(y)−f(x)− ∇f(x)⊤(y−x)−1
2(y−x)⊤∇2f(x)(y−x)| ≤M
6∥y−x∥3,∀x, y∈Rd.
Setting x=xtandy=xt+dt, we have that
f(xt+dt)−f(xt)≤g⊤
tdt+1
2d⊤
t∇2f(xt)dt+M
6∥dt∥3. (49)
Notice that using (33) from Lemma A.1 and the definition of ρtin (21), we have that
d⊤
t∇2f(xt)dt≤(1 +Ct)d⊤
t∇2f(x∗)dt=−g⊤
tdt(1 +Ct)∥˜dt∥2
−g⊤
tdt=−g⊤
tdt1 +Ct
ρt. (50)
20Applying Assumption 2.1 with the definition ˜dt=∇2f(x∗)1
2dt, we obtain that
∥dt∥3≤1
µ3
2∥˜dt∥3=−g⊤
tdt
µ3
2∥˜dt∥2
−g⊤
tdt∥˜dt∥=−g⊤
tdt
µ3
21
ρt∥˜dt∥.
Since−˜g⊤
t˜dt≤ ∥˜gt∥∥˜dt∥by Cauchy–Schwarz inequality where ˜gt=∇2f(x∗)−1
2gt, we obtain
∥˜dt∥=∥˜gt∥∥˜dt∥
∥˜gt∥≤ ∥˜gt∥∥˜dt∥2
−˜g⊤
t˜dt=1
ρt∥˜gt∥,
which leads to
∥dt∥3≤−g⊤
tdt
µ3
21
ρt∥˜dt∥ ≤−g⊤
tdt
µ3
21
ρ2
t∥˜gk∥. (51)
By applying Taylor’s theorem with Lagrange remainder, there exists ˜τt∈[0,1]such that
f(xt) =f(x∗) +∇f(x∗)⊤(xt−x∗) +1
2(xt−x∗)⊤∇2f(xt+ ˜τt(x∗−xt))(xt−x∗)
=f(x∗) +1
2(xt−x∗)⊤∇2f(xt+ ˜τt(x∗−xt))(xt−x∗),(52)
where we used the fact that ∇f(x∗) = 0 in the last equality. Moreover, by the fundamental theorem
of calculus, we have
∇f(xt)− ∇f(x∗) =Z1
0∇2f(xt+τ(x∗−xt))(xt−x∗)dτ=Gt(xt−x∗),
where we use the definition of Gtin(8). Since ∇f(x∗) = 0 and we denote gt=∇f(xt), this further
implies that
xt−x∗=G−1
t(∇f(xt)− ∇f(x∗)) =G−1
tgt. (53)
Combining (52) and (53) leads to
f(xt)−f(x∗) =1
2g⊤
tG−1
t∇2f(xt+ ˜τt(x∗−xt))G−1
tgt. (54)
Based on (35) in Lemma A.1, we have ∇2f(xt+ ˜τt(x∗−xt))⪰1
1+CtGt, which implies that
G−1
t∇2f(xt+ ˜τt(x∗−xt))G−1
t⪰1
1 +CtG−1
t.
Moreover, it follows from (34) in Lemma A.1 that Gt⪯(1 +Ct)∇2f(x∗), which implies that
G−1
t⪰1
1 +Ct(∇2f(x∗))−1.
Combining the above two conditions, we obtain that
G−1
t∇2f(xt+ ˜τt(x∗−xt))G−1
t⪰1
(1 +Ct)2(∇2f(x∗))−1,
and hence
g⊤
tG−1
t∇2f(xt+ ˜τt(x∗−xt))G−1
tgt≥1
(1 +Ct)2g⊤
t(∇2f(x∗))−1gt=1
(1 +Ct)2∥˜gt∥2.(55)
Combining (54) and (55) leads to
∥˜gk∥ ≤(1 +Ct)p
2(f(xt)−f(x∗)). (56)
Combining (51) and (56) leads to
∥dt∥3≤−g⊤
tdt
µ3
21
ρ2
t∥˜gk∥ ≤−g⊤
tdt
µ3
21
ρ2
t(1 +Ct)p
2(f(xt)−f(x∗)). (57)
21Leveraging (49), (50) and (57) with the definition of Ctin (9), we have that
f(xt+dt)−f(xt)≤g⊤
tdt+1
2d⊤
t∇2f(xt)dt+M
6∥dt∥3
=−g⊤
tdt(−1 +1 +Ct
2ρt+M
61
µ3
21
ρ2
t(1 +Ct)p
2(f(xt)−f(x∗)))
=−g⊤
tdt(−1 +1 +Ct
2ρt+Ct(1 +Ct)
6ρ2
t).(58)
Notice that −g⊤
tdt=−g⊤
tB−1
tgt>0and when Ct≤1
6andρt≥7
8, we can verify that
1 +Ct
2ρt+Ct(1 +Ct)
6ρ2
t<1.
Therefore, (58) implies the conclusion that
f(xt+dt)−f(xt)≤0.
G.2 Proof of Lemma 6.1
Since ηt= 1satisfies Armijo-Wolfe conditions, we know that ηtis chosen to be one at iteration tand
xt+1=xt+dt. We have f(xt+1)≤f(xt)from Lemma 2.1. Using Taylor’s expansion, we have
thatf(xt+1) =f(xt) +g⊤
tdt+1
2d⊤
t∇2f(xt+ ˆτ(xt+1−xt))dt, where ˆτ∈[0,1]. Hence, we have
that
ˆpt=f(xt)−f(xt+1)
−g⊤
tdt=−g⊤
tdt−1
2d⊤
t∇2f(xt+ ˆτ(xt+1−xt))dt
−g⊤
tdt
= 1−1
2d⊤
t∇2f(xt+ ˆτ(xt+1−xt))dt
−g⊤
tdt≥1−1 +Ct
2d⊤
t∇2f(x∗)dt
−g⊤
tdt= 1−1 +Ct
2ρt,
where we apply the (32) from Lemma A.1 since f(xt+1)≤f(xt)and recall the definition of ρtin
(21). Similarly, using (31) from Lemma A.1 since f(xt+1)≤f(xt), we have that
ˆnt=y⊤
tst
−g⊤
tst=s⊤
tJtst
−g⊤
tst=d⊤
tJtdt
−g⊤
tdt≥1
1 +Ctd⊤
t∇2f(x∗)dt
−g⊤
tdt=1
(1 +Ct)ρt,
where we use the fact that yt=JtstwithJtdefined in (8)andst=xt+1−xt=dt. Therefore, we
prove the conclusions.
G.3 Proof of Lemma 6.2
Denote ¯xt+1=xt+dtand¯st= ¯xt+1−xt=dt. Since δ1≤1
6andδ2≥7
8, we have f(¯xt+1)≤
f(xt)from Lemma G.3. Using Taylor’s expansion, we have that f(¯xt+1) =f(xt) +g⊤
tdt+
1
2d⊤
t∇2f(xt+ ˆτ(¯xt+1−xt))dt, where ˆτ∈[0,1]. Hence, we have
f(xt)−f(¯xk+1)
−g⊤
tdt=−g⊤
tdt−1
2d⊤
t∇2f(xt+ ˆτ(¯xt+1−xt))dt
−g⊤
tdt
= 1−1
2d⊤
t∇2f(xt+ ˆτ(¯xt+1−xt))dt
−g⊤
tdt≥1−1 +Ct
2d⊤
t∇2f(x∗)dt
−g⊤
tdt= 1−1 +Ct
2ρt,
where we apply the (32) from Lemma A.1 since f(¯xt+1)≤f(xt). Therefore, when Ct≤δ1≤p
2(1−α)−1andρt≥δ2≥1√
2(1−α), we obtain thatf(xt)−f(¯xk+1)
−g⊤
tdt≥1−1+Ct
2ρt≥αand unit
step size ηt= 1satisfies the sufficient condition (5).
Similarly, using (31) from Lemma A.1 since f(¯xt+1)≤f(xt)and denote ¯gk+1=∇f(¯xt+1),
¯yt= ¯gk+1−gt, we have that
¯y⊤
t¯st
−g⊤
t¯st=¯s⊤
tJt¯st
−g⊤
t¯st=d⊤
tJtdt
−g⊤
tdt≥1
1 +Ctd⊤
t∇2f(x∗)dt
−g⊤
tdt=1
(1 +Ct)ρt.
22Therefore, when Ct≤δ1≤1√1−β−1andρt≤δ3=1√1−β, we obtain that¯y⊤
t¯st
−g⊤
t¯st≥1
(1+Ct)ρt≥
1−β, which indicates that ¯g⊤
t+1dt= ¯g⊤
t+1¯st= ¯y⊤
t¯st+g⊤
t¯st≥ −g⊤
t¯st(1−β) +g⊤
t¯st=βg⊤
t¯st=
βg⊤
tdt. Hence, unit step size ηt= 1satisfies the curvature condition (6). Therefore, we prove that
when Ct≤δ1andδ2≤ρt≤δ3, step size ηt= 1satisfies the Armijo-Wolfe conditions (5) and (6).
G.4 Proof of Lemma 6.3
Since in Theorem 4.1, we already prove that
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1−e−Ψ(¯B0)
t2α(1−β)
κt
.
This implies that
Ct≤
1−e−Ψ(¯B0)
t2α(1−β)
κt
2
C0.
When t≥Ψ(¯B0), we obtain that
Ct≤
1−2α(1−β)
3κt
2
C0.
When t≥3κ
α(1−β)logC0
δ1, we obtain that
Ct≤
1−2α(1−β)
3κt
2
C0≤δ1.
Therefore, the first claim in (24) follows.
Now define I1={t:ρt< δ2}andI2={t:ρt> δ3}, we know that |I|=|I1|+|I2|.
Notice that for t∈I1, we have that ρt−1< δ2−1<0since δ2<1and the function ω(x)
defined in (45) is decreasing for −1< x < 0from (a) in Lemma G.1. Hence, we have thatP
i∈I1ω(ρi−1)≥P
i∈I1ω(δ2−1) = ω(δ2−1)|I1|. Similarly, we have that for t∈I2, we have
thatρi−1> δ3−1>0since δ3>1and the function ω(x)is increasing for x >0from (a) in
Lemma G.1. Hence, we have thatP
i∈I2ω(ρi−1)≥P
i∈I2ω(δ3−1) = ω(δ3−1)|I2|. Using (46)
from Proposition G.2, we have thatPt−1
i=0ω(ρi−1)≤Ψ(˜B0) + 2Pt−1
i=0Ci≤Ψ(˜B0) + 2P+∞
i=0Ci
for any t≥1. Therefore, we obtain that
Ψ(˜B0) + 2+∞X
i=0Ci≥+∞X
i=0ω(ρi−1)≥X
i∈I1ω(βi−1) +X
i∈I2ω(βi−1)
≥ω(δ2−1)|I1|+ω(δ3−1)|I2| ≥min{ω(δ2−1), ω(δ3−1)}(|I1|+|I2|),
which leads to the result
|I|=|I1|+|I2| ≤Ψ(˜B0) + 2P+∞
i=0Ci
min{ω(δ2−1), ω(δ3−1)}=δ4 
Ψ(˜B0) + 2+∞X
i=0Ci!
, (59)
where δ4:=1
min{ω(δ2−1),ω(δ3−1)}. Using the upper bound ofP+∞
i=0Ci≤C0Ψ(¯B0) +3C0κ
α(1−β)in
(44), we prove the second claim in (25).
G.5 Proof of Theorem 6.4
First, we prove that for any initial point x0∈Rdand any initial Hessian approximation matrix
B0∈Sd
++, the following result holds:
f(xt)−f(x∗)
f(x0)−f(x∗)≤ 
δ6t0+δ7Ψ(˜B0) +δ8P+∞
i=0Ci
t!t
,∀t > t 0,
23where t0is defined in (24). We choose the weight matrix as P=∇2f(x∗)throughout the proof.
Using results (41) and (42) from the proof of Proposition 5.1, we obtain that
t−1Y
i=0cos2(ˆθi)
ˆmi≥e−Ψ(˜B0)−Pt−1
i=0Ci≥e−Ψ(˜B0)−P+∞
i=0Ci. (60)
t−1Y
i=0ˆqi≥2te−2Pt−1
i=0Ci≥2te−2P+∞
i=0Ci. (61)
Recall the definition of the set I={t:ρt/∈[δ2, δ3]}. Notice that for t≥t0, define I3={t:t≥
t0, ρt/∈[δ2, δ3]}andI4={t:t≥t0, ρt∈[δ2, δ3]}. Then, we have that
t−1Y
i=0ˆpiˆni=t0−1Y
i=0ˆpiˆnit−1Y
i=t0ˆpiˆni=t0−1Y
i=0ˆpiˆniY
i∈I3ˆpiˆniY
i∈I4ˆpiˆni. (62)
From Lemma 2.1, we know ˆpt≥αandˆnt≥1−βfor any t≥0, which lead to
t0−1Y
i=0ˆpiˆni≥αt0(1−β)t0=1
2t0e−t0log1
2α(1−β). (63)
Y
i∈I3ˆpiˆni≥Y
i∈I3α(1−β) =1
2|I3|e−|I3|log1
2α(1−β)≥1
2|I3|e−|I|log1
2α(1−β)
≥1
2|I3|e−δ4
Ψ(˜B0)+2P+∞
i=0Ci
log1
2α(1−β),(64)
where the second inequality holds since |I3| ≤ |I|,log1
2α(1−β)>0and the last inequality holds
since (59) from the proof of Lemma 6.3 in Appendix G.4. Notice that when index i∈I4, we have
Ci≤δ1from Lemma 6.3 and ρi∈[δ2, δ3]. Applying Lemma 6.1 and Lemma 6.2, we know that for
i∈I4,ηi= 1satisfies the Armijo-Wolfe conditions (5),(6)and we have ˆpi≥1−1+Ci
2ρi>0(since
Ci≤δ1≤1
6,ρi≥δ2≥7
8) and ˆni≥1
(1+Ci)ρifrom (22). Hence, we obtain that
Y
i∈I4ˆpiˆni≥1
2|I4|Y
i∈I4(2−1 +Ci
ρi)1
(1 +Ci)ρi≥1
2|I4|e−P
i∈I4CiY
i∈I4(2−1 +Ci
ρi)1
ρi,(65)
where the last inequality holds since1
1+Ci≥e−Ci. Using the fact that logx≥1−1
x, we obtain
Y
i∈I4(2−1 +Ci
ρi)1
ρi=Y
i∈I4elog (2−1+Ci
ρi)−logρi≥Y
i∈I4e1−1
2−1+Ci
ρi−logρi
=Y
i∈I4eρi−1−Ci
2ρi−1−Ci−logρi=Y
i∈I4eρi−1−logρi+2(1−ρi) logρi−(1−logρi)Ci
2ρi−1−Ci
=Y
i∈I4eω(ρi−1)+2(1 −ρi) logρi−(1−logρi)Ci
2ρi−1−Ci ≥Y
i∈I4e−2(ρi−1) log ρi−(1−logρi)Ci
2ρi−1−Ci
=Y
i∈I4e−2(ρi−1) log ρi+(1−logρi)Ci
2ρi−1−Ci ≥Y
i∈I4e−2(ρi−1) log ρi+(1−logδ2)Ci
2δ2−1−δ1 ,(66)
where the second inequality holds since ω(ρi−1)≥0and the third inequality holds since ρi≥δ2
due to i∈I4andCi≤δ1due to i≥t0and Lemma 6.3. Notice that 2ρi−1−Ci≥2δ2−1−δ1>0
for all i∈I4since Ci≤δ1≤1
6andρi≥δ2≥7
8.
When ρi≥1, using logρi≤ρi−1, (b) in Lemma G.1 and ρi≤δ3due to i∈I4, we have that
(ρi−1) log ρi≤(ρi−1)2≤2ρiω(ρi−1)≤2δ3ω(ρi−1). (67)
Similarly, when ρi<1, using logρi≥1−1
ρi, (c) in Lemma G.1 and ρi≥δ2due to i∈I4, we have
(ρi−1) log ρi≤(ρi−1)2
ρi≤ρi+ 1
ρiω(ρi−1)≤(1 +1
δ2)ω(ρi−1). (68)
24Combining (66), (67) and (68), we obtain that
Y
i∈I4(2−1 +Ci
ρi)1
ρi
≥Y
i∈I4e−2(ρi−1) log ρi+(1−logδ2)Ci
2δ2−1−δ1 =Y
i∈I4e−2(ρi−1) log ρi
2δ2−1−δ1Y
i∈I4e−(1−logδ2)Ci
2δ2−1−δ1
=Y
i∈I4,ρi<1e−2(ρi−1) log ρi
2δ2−1−δ1Y
i∈I4,ρi≥1e−2(ρi−1) log ρi
2δ2−1−δ1Y
i∈I4e−(1−logδ2)Ci
2δ2−1−δ1
≥Y
i∈I4,ρi<1e−2(1+1
δ2)ω(ρi−1)
2δ2−1−δ1Y
i∈I4,ρi≥1e−4δ3ω(ρi−1)
2δ2−1−δ1Y
i∈I4e−(1−logδ2)Ci
2δ2−1−δ1
=e−2+2
δ4
2δ2−1−δ1P
i∈I2,ρi<1ω(ρi−1)−4δ3
2δ2−1−δ1P
i∈I4,ρi≥1ω(ρi−1)−1−logδ2
2δ2−1−δ1P
i∈I4Ci
≥e−δ5P
i∈I4,ρi<1ω(ρi−1)+P
i∈I4,ρi≥1ω(ρi−1)
−1−logδ2
2δ2−1−δ1P
i∈I4Ci
=e−δ5P
i∈I4ω(ρi−1)−1−logδ2
2δ2−1−δ1P
i∈I4Ci(69)
where δ5= max {2+2
δ2
2δ2−1−δ1,4δ3
2δ2−1−δ1}. Combining (65) and (69), we obtain that
Y
i∈I4ˆpiˆni≥1
2|I4|e−P
i∈I4CiY
i∈I4(2−1 +Ci
ρi)1
ρi
≥1
2|I4|e−δ5P
i∈I4ω(ρi−1)−(1+1−logδ2
2δ2−1−δ1)P
i∈I4Ci
≥1
2|I4|e−δ5P+∞
i=0ω(ρi−1)−2δ2−δ1−logδ2
2δ2−1−δ1P+∞
i=0Ci
≥1
2|I4|e−δ5
Ψ(˜B0)+2P+∞
i=0Ci
−2δ2−δ1−logδ2
2δ2−1−δ1P+∞
i=0Ci,(70)
where the last inequality is due to (46) from Lemma G.1. Combining (62),(63),(64) and(70), we
obtain that
t−1Y
i=0ˆpiˆni=t0−1Y
i=0ˆpiˆniY
i∈I3ˆpiˆniY
i∈I4ˆpiˆni
≥1
2te−
t0log1
2α(1−β)+(δ4log1
2α(1−β)+δ5)Ψ(˜B0)+(2δ4log1
2α(1−β)+2δ5+2δ2−δ1−logδ2
2δ2−1−δ1)P+∞
i=0Ci
.(71)
Leveraging (60), (61), (71) with (15) from Proposition 3.1, we prove that
f(xt)−f(x∗)
f(x0)−f(x∗)≤
1− t−1Y
i=0ˆpiˆqiˆnicos2(ˆθi)
ˆmi!1
t
t
=
1− t−1Y
i=0ˆpiˆnit−1Y
i=0ˆqit−1Y
i=0cos2(ˆθi)
ˆmi!1
t
t
≤ 
1−e−t0log1
2α(1−β)+(1+ δ4log1
2α(1−β)+δ5)Ψ(˜B0)+(3+2 δ4log1
2α(1−β)+2δ5+2δ2−δ1−logδ2
2δ2−1−δ1)P+∞
i=0Ci
t!t
=
1−e−δ6t0+δ7Ψ(˜B0)+δ8P+∞
i=0Ci
tt
≤ 
δ6t0+δ7Ψ(˜B0) +δ8P+∞
i=0Ci
t!t
,
where the inequality is due to the fact that 1−e−x≤xfor any x∈Randδ6, δ7, δ8are defined in
Theorem 6.4. Hence, we prove that
f(xt)−f(x∗)
f(x0)−f(x∗)≤ 
δ6t0+δ7Ψ(˜B0) +δ8P+∞
i=0Ci
t!t
,∀t > t 0. (72)
25Using (44) from the proof of Theorem 5.2 in Appendix F.2, we have that
+∞X
i=0Ci≤C0Ψ(¯B0) +3C0κ
α(1−β). (73)
Notice that from (24) in Lemma 6.3, we have that
t0= max {Ψ(¯B0),3κ
α(1−β)logC0
δ1} ≤Ψ(¯B0) +3κ
α(1−β)logC0
δ1. (74)
Leveraging (72), (73) and (74), we prove the conclusion.
G.6 Proof of Corollary 6.5
Using the fact that for B0=LI, we have Ψ(¯B0) = 0 andΨ(˜B0)≤dκ, and for the case that
B0=µI, we have Ψ(¯B0)≤dlogκ,andΨ(˜B0)≤dlogκ, we obtain the corresponding superlinear
results for these two conditions.
G.7 Specific Values of {δi}8
i=1
As we stated before, all the {δi}8
i=1are universal constants that only depend on line search parameters
αandβ. We can choose specific values of αandβto make definitions of {δi}8
i=1more clear. If we
pickα=1
4andβ=3
4, we have that
δ1=1
6, δ 2=7
8, δ 3= 2, δ 4= 118 , δ 5= 14, δ 6= log 8 , δ 7= 260 , δ 8= 524 .
H Complexity of BFGS with the Initialization B0=cI
Recall that c∈[µ, L]by our choice of cin Remark 6.2. If we choose B0=cI, then Ψ(¯B0) =
Ψ(c
LI) =c
Ld−d+dlogL
c. Moreover, we have Ψ(˜B0) = Ψ( c∇2f(x∗)−1) =cTr(∇2f(x∗)−1)−
d−logDet(c∇2f(x∗)−1), which is determined by the Hessian matrix ∇2f(x∗)−1. In this case,
one can use the upper bounds Ψ(¯B0) =d(c
L−1 + logL
c)andΨ(˜B0) =Tr(c∇2f(x∗)−1)−d−
logDet(c∇2f(x∗)−1)≤d(c
µ−1 + logL
c)to simplify the expressions.
Applying these values of Ψ(¯B0)andΨ(˜B0)to our linear convergence result in Theorem 4.1 and the
superlinear convergence result in Theorem 6.4, we can obtain the following convergence guarantees
forB0=cI:
• For t≥d(c
L−1 + logL
c), we havef(xt)−f(x∗)
f(x0)−f(x∗)≤
1−2α(1−β)
3κt
;
•Fort= Ω( d(c
µ−1 + logL
c) +C0d(c
L−1 + logL
c) +C0κ), we havef(xt)−f(x∗)
f(x0)−f(x∗)≤
 
O d(c
µ−1+logL
c)+C0d(c
L−1+logL
c)+C0κ
tt.
Moreover, we can derive similar iteration complexity bounds following the same arguments as in
Section I. We also include the performance of BFGS with B0=cIin our numerical experiments as
presented in Figure 1. We observe that the performance of BFGS with B0=cIis very similar to the
convergence curve of BFGS with B0=µIin our numerical experiments.
I Proof of Iteration Complexity
When B0=LI, if we regard the line search parameters αandβas absolute constants, the first result
established in Corollary 4.2 leads to a global complexity of O(κlog1
ϵ), which is on par with gradient
descent. Moreover, the first result in Corollary 5.3 implies a complexity of O 
(d+C0)κ+ log1
ϵ
,
where the first term represents the number of iterations required to attain the linear rate in (20), and
the second term represents the additional number of iterations needed to achieve the desired accuracy
26ϵfrom the condition number-independent linear rate. For the analysis of the superlinear convergence
rate, we denote that ΩL=dκ+C0κ. From the first result in Corollary 6.5, we have that
f(xt)−f(x∗)
f(x0)−f(x∗)≤(ΩL
t)t
LetT∗be the number such that the inequality (ΩL
t)t≤ϵabove becomes equality. we have
log1
ϵ=T∗logT∗
ΩL≤T∗(T∗
ΩL−1),
T∗≥ΩL+q
Ω2
L+ 4Ω Llog1
ϵ
2.
Hence, we have that
log1
ϵ=T∗logT∗
ΩL≥T∗logΩL+q
Ω2
L+ 4Ω Llog1
ϵ
2ΩL≥T∗log
1
2+s
1
4+log1
ϵ
ΩL
,
T∗≤log1
ϵ
log
1
2+q
1
4+log1
ϵ
ΩL.
Hence, to reach the accuracy of ϵ, we need the number of iterations tto be at least
t≥log1
ϵ
log
1
2+q
1
4+1
ΩLlog1
ϵ.
Therefore, the iteration complexity for the case of B0=LIis
O
min

κlog1
ϵ,(d+C0)κ+ log1
ϵ,log1
ϵ
log
1
2+q
1
4+1
dκ+C0κlog1
ϵ


.
Similarly, in this case of B0=µI, the second result in Corollary 4.2 establishes a global complexity
ofO 
dlogκ+κlog1
ϵ
, where the first term represents the number of iterations before the linear
convergence rate in (19) begins, and the second term arises from the linear rate itself. Addition-
ally, following the same argument, the second result in Corollary 5.3 indicates a complexity of
O(C0dlogκ+C0κ+ log1
ϵ). Here, the first term accounts for the wait time until the convergence
rate takes effect, and the second term is associated with the condition number-independent linear rate.
For the superlinear convergence rate, when B0=µI, to reach the accuracy of ϵ, we need the number
of iterations tto be at least
t≥log1
ϵ
log
1
2+q
1
4+1
Ωµlog1
ϵ,
where Ωµ=C0dlogκ+C0κ. The proof is the same as the proof for the case of B0=LI. Therefore,
the iteration complexity for the case of B0=µIis
O
min

dlogκ+κlog1
ϵ, C0(dlogκ+κ) + log1
ϵ,log1
ϵ
log
1
2+q
1
4+1
C0(dlogκ+κ)log1
ϵ


.
J Log Bisection Algorithm for Weak Wolfe Conditions
27Algorithm 1 Log Bisection Algorithm for Weak Wolfe Conditions
Require: Initial step size η(0)= 1,η(0)
min= 0,η(0)
max= +∞
1:fori= 0,1,2, . . .do
2: iff(xt+η(i)dt)> f(xt) +αη(i)∇f(xt)⊤dtthen
3: Setη(i+1)
max =η(i)andη(i+1)
min =η(i)
min
4: ifη(i)
min= 0then
5: η(i+1)= (1
2)2i+1−1
6: else
7: η(i+1)=q
η(i+1)
maxη(i+1)
min
8: end if
9: else if ∇f(xt+η(i)dt)⊤dt< β∇f(xt)⊤dtthen
10: Setη(i+1)
max =η(i)
max andη(i+1)
min =η(i)
11: ifη(i)
max= +∞then
12: η(i+1)= 22i+1−1
13: else
14: η(i+1)=q
η(i+1)
maxη(i+1)
min
15: end if
16: else
17: Return η(i)
18: end if
19:end for
K Results and Discussion on the Bisection Scheme for Line Search in
Section 7
K.1 Proof of Lemma K.1
First, we present major results concerning the complexity of the bisection method, which specifies a
range of values that meet the conditions in (5) and (6).
Lemma K.1. Suppose that Assumptions 2.1, 2.2 and 2.3 hold. Recall the definition of ρtin(21)
andCtin(9). At iteration t, there is unique ηr>0such that the sufficient decrease condition (5)is
equity for ηr, i.e.,
f(xt+ηrdt) =f(xt) +αηr∇f(xt)⊤dt. (75)
Then, ηtsatisfies the sufficient decrease condition (5)if and only if ηt≤ηr. We also have that
2(1−α)
1 +Ctρt≤ηr≤2(1−α)(1 + Ct)ρt. (76)
Similarly, there is also unique ηl>0such that the curvature condition (6)is equity for ηl, i.e.,
∇f(xt+ηldt)⊤dt=β∇f(xt)⊤dt. (77)
Then, ηtsatisfies the curvature condition (6)if and only if ηt≥ηl. Moreover, we have that
ηr
ηl≥1 +β−α
(1−β)(1 + 2 Ct)>1. (78)
Proof. Notice that Assumption 2.1 indicates that the objective function f(x)is strongly convex.
Consider function h1(η) =f(xt+ηdt)−αη∇f(xt)⊤dt. We observe that this function h1(η)
is strongly convex and h1(0) = f(xt),h′
1(0)<0. Hence, there is unique ηr>0such that
h1(ηr) =f(xt)andηt≤ηrif and only if f(xt+ηtdt)≤f(xt) +αηt∇f(xt)⊤dt.
Denote that ¯xt+1=xt+ηrdt. We know that f(¯xt+1)−f(xt) =αηrg⊤
tdt. Since f(¯xt+1)−f(xt) =
ηrg⊤
tdt+1
2η2
rd⊤
t∇2f(xt+τ(¯xt+1−xt))dtforτ∈(0,1), we have that
ηrg⊤
tdt+1
2η2
rd⊤
t∇2f(xt+τ(¯xt+1−xt))dt=αηrg⊤
tdt,
28ηr= 2(1 −α)−g⊤
tdt
d⊤
t∇2f(xt+τ(¯xt+1−xt))dt.
which leads to
ηr= 2(1 −α)−g⊤
tdt
d⊤
t∇2f(xt+τ(¯xt+1−xt))dt≤2(1−α)(1 + Ct)−g⊤
tdt
d⊤
t∇2f(x∗)dt
= 2(1 −α)(1 + Ct)−g⊤
tdt
∥˜dt∥2= 2(1 −α)(1 + Ct)ρt.
ηr= 2(1 −α)−g⊤
tdt
d⊤
t∇2f(xt+τ(¯xt+1−xt))dt≥2(1−α)
1 +Ct−g⊤
tdt
d⊤
t∇2f(x∗)dt=2(1−α)
1 +Ctρt.
where we use the (32) from Lemma A.1 and the fact that f(¯xt+1) =f(xt) +αηrg⊤
tdt≤f(xt).
Hence, we prove the results in (76).
Similarly, consider function h2(η) =∇f(xt+ηdt)⊤dt. We observe that this function h2(η)is
strictly increasing function for η≥0andh2(0) = ∇f(xt)⊤dt< β∇f(xt)⊤dt,h2(ηexact) =
∇f(xt+ηexactdt)⊤dt= 0> β∇f(xt)⊤dtwhere ηexact := arg minη>0f(xt+ηdt)is the exact
line search step size satisfying ∇f(xt+ηexactdt)⊤dt= 0. Hence, there is unique ηl∈(0, ηexact)
such that h2(ηl) =β∇f(xt)⊤dtandηt≥ηlif and only if ∇f(xt+ηtdt)⊤dt≥β∇f(xt)⊤dt.
Notice that
f(xt+ηrdt) =f(xt) +αηr∇f(xt)⊤dt.
Using mean value theorem, we know there exists ¯η∈(0, ηr)such that
f(xt+ηrdt) =f(xt) +ηr∇f(xt+ ¯ηdt)⊤dt.
The above two equities indicates that
∇f(xt+ ¯ηdt)⊤dt=α∇f(xt)⊤dt.
Recall that
∇f(xt+ηldt)⊤dt=β∇f(xt)⊤dt.
Combing the above two equities, we obtain that
(∇f(xt+ ¯ηdt)− ∇f(xt+ηldt))⊤dt=−∇f(xt)⊤dt(β−α).
Using mean value theorem again, we know there exists ˜η∈(ηl,¯η)such that
(∇f(xt+ ¯ηdt)− ∇f(xt+ηldt))⊤dt= (¯η−ηl)d⊤
t∇2f(xt+ ˜ηdt)dt.
Leveraging the above two equities, we obtain that
¯η−ηl= (β−α)−∇f(xt)⊤dt
d⊤
t∇2f(xt+ ˜ηdt)dt.
Notice that ¯η≤ηr, we have that
ηr−ηl≥¯η−ηl= (β−α)−∇f(xt)⊤dt
d⊤
t∇2f(xt+ ˜ηdt)dt. (79)
Recall the definition of ηlin (77), we have that
(∇f(xt+ηldt)− ∇f(xt))⊤dt=−(1−β)∇f(xt)⊤dt.
Notice that there exists ˆη∈(0, ηl), such that
(∇f(xt+ηldt)− ∇f(xt))⊤dt=ηld⊤
t∇2f(xt+ ˆηdt)dt.
Combing the above two equities, we obtain that
ηl=−(1−β)∇f(xt)⊤dt
d⊤
t∇2f(xt+ ˆηdt)dt. (80)
29Leveraging (79) and (80), we have that
ηr
ηl= 1 +ηr−ηl
ηl≥1 +(β−α)d⊤
t∇2f(xt+ ˆηdt)dt
(1−β)d⊤
t∇2f(xt+ ˜ηdt)dt.
Recall that ¯xt+1=xt+ηrdtand notice that ˆη≤ηr,˜η≤ηr. We have that xt+ ˆηdt=xt+
ˆτ(¯xt+1−xt)andxt+ ˜ηdt=xt+ ˜τ(¯xt+1−xt)withˆτ=ˆη
ηr∈(0,1)and˜τ=˜η
ηr∈(0,1). Since
f(¯xt+1) =f(xt+ηrdt) =f(xt) +αηr∇f(xt)⊤dt≤f(xt), applying (36) in Lemma A.1, we
prove the conclusion that
ηr
ηl≥1 +(β−α)d⊤
t∇2f(xt+ ˆηdt)dt
(1−β)d⊤
t∇2f(xt+ ˜ηdt)dt
= 1 +(β−α)d⊤
t∇2f(xt+ ˆτ(¯xt+1−xt))dt
(1−β)d⊤
t∇2f(xt+ ˜τ(¯xt+1−xt))dt≥1 +β−α
(1−β)(1 + 2 Ct).
K.2 Bound on the Number of Inner Loops
Proposition K.2. Suppose that Assumptions 2.1, 2.2 and 2.3 hold. Consider the BFGS method with
inexact line search defined in (5)and(6)and we choose the step size ηtaccording to Algorithm 1. At
iteration t, denote λtas the number of loops in Algorithm 1 to terminate and return the ηtsatisfying
the Wolfe conditions (5)and(6). Then λtis finite and upper bounded by
λt≤2 + log2
1 +(1−β)(1 + 2 Ct)
β−α
+ 2 log2
1 + log2 
2(1−α)(1 + Ct)
+ max {log2ρt,log21
ρt}
.(81)
Proof. At the first iteration, if η(0)= 1satisfies the weak Wolfe conditions (5)and(6), the algorithm
terminates and returns the unit step size ηt= 1. In this case, we have that λt= 1.
Suppose that at the first iteration, η(0)= 1doesn’t satisfy the sufficient decrease condition (5)but
satisfies the curvature condition (6), we have that η(1)
max= +∞,η(1)
min= 1andη(1)= 2. Assume
that in the Algorithm 1, η(i)
max is never set to a finite value and the algorithm never returns. This
means that the condition in line 2 is never satisfied, and as a result, we keep repeating steps in line
12. Thus, η(i)= 22i−1and since the condition in line 2 is never satisfied, we always have that
f(xt+η(i)dt)≤f(xt) +αη(i)∇f(xt)⊤dt. Notice that limi→∞η(i)→+∞and∇f(xt)⊤dt<0.
We obtain that limi→∞f(xt+η(i)dt)→ −∞ , which is a contradiction since fis strongly convex.
Hence, at some point, either the algorithm finds an admissible step size and returns, or η(i)
max must
become finite. Suppose that this happens at iteration K1≥1of the loop in Algorithm 1. Then, we
know that η(K1)= 22K1−1. In the first case that the algorithm finds an admissible step size and
returns ηK1,ηK1satisfies the Armijo-Wolfe conditions and therefore ηK1≤ηr. Using the upper
bound result in (76) from Lemma K.1, we obtain that η(K1)= 22K1−1≤ηr≤2(1−α)(1 + Ct)ρt,
which leads to
λt=K1≤log2
1 + log2 
2(1−α)(1 + Ct)ρt
. (82)
In the second case that η(i)
max becomes finite but the algorithm does not terminate, we have that
η(K1−1)satisfies the sufficient condition (5) and η(K1−1)≤ηr. Similarly, this implies that
K1≤1 + log2
1 + log2 
2(1−α)(1 + Ct)ρt
. (83)
Then, we further go through the log bisection process. Notice that for any iteration i > K 1, the
sequence η(i)
max is finite and non-increasing and the sequence η(i)
min≥1and non-decreasing. The log
bisection process indicates that
log2η(i+1)
max
η(i+1)
min=1
2log2η(i)
max
η(i)
min,∀i > K 1. (84)
30The Algorithm 1 implies that for any i > K 1, we have that
f(xt+η(i)
maxdt)> f(xt) +αη(i)
max∇f(xt)⊤dt,∇f(xt+η(i)
mindt)⊤dt< β∇f(xt)⊤dt.
Hence, we know that for any i > K 1,η(i)
max≥ηrandη(i)
min≤ηlwhere ηr, ηlare defined in (75),
(77) from Lemma K.1. Therefore, using result (78) from Lemma K.1, we have that for any j≥1,
log2η(K1+j)
max
η(K1+j)
min≥log2ηr
ηl>0. (85)
Notice that (84) implies that
log2η(K1+j)
max
η(K1+j)
min=1
2j−1log2η(K1+1)
max
η(K1+1)
min, (86)
which leads to 0 = lim j→+∞1
2j−1log2η(K1+1)
max
η(K1+1)
min= lim j→+∞log2η(K1+j)
max
η(K1+j)
min≥log2ηr
ηl>0. This is a
contradiction. Hence, Algorithm 1 must terminate after finite number of loops. Now suppose that
Algorithm 1 terminates after K1+ Γ1iterations, (85) and (86) indicate that when Γ1≥1, we have
1
2Γ1−1log2η(K1+1)
max
η(K1+1)
min= log2η(K1+Γ1)
max
η(K1+Γ1)
min≥log2ηr
ηl>log2
1 +β−α
(1−β)(1 + 2 Ct)
(87)
where the last inequality holds since (78) in Lemma K.1. Notice that η(K1+1)
max = 22K1−1and
ηK1+1
min = 22K1−1−1. Hence, we obtain that
log2η(K1+1)
max
η(K1+1)
min= 2K1−1≤1 + log2 
2(1−α)(1 + Ct)ρt
. (88)
Combing (87), (88) and using logx≥1−1
x, we have that
Γ1≤1 + log2
1 + log2 
2(1−α)(1 + Ct)ρt
−log2log2
1 +β−α
(1−β)(1 + 2 Ct)
≤1 + log2
1 + log2 
2(1−α)(1 + Ct)ρt
−log2log
1 +β−α
(1−β)(1 + 2 Ct)
≤1 + log2
1 + log2 
2(1−α)(1 + Ct)ρt
−log2
1−1
1 +β−α
(1−β)(1+2 Ct)
= 1 + log2
1 + log2 
2(1−α)(1 + Ct)ρt
+ log2
1 +(1−β)(1 + 2 Ct)
β−α
.(89)
Leveraging (83) and (89), we prove that
λt=K1+ Γ1
≤2 + 2 log2
1 + log2 
2(1−α)(1 + Ct)ρt
+ log2
1 +(1−β)(1 + 2 Ct)
β−α
.(90)
Similarly, suppose that at the first iteration, η(0)= 1satisfies the sufficient decrease condition (5)
but doesn’t satisfy the curvature condition (6), we have that η(1)
max= 1,η(1)
min= 0 andη(1)=1
2.
Assume that in the Algorithm 1, η(i)
minis never set to a positive value and the algorithm never returns.
This means that the condition in line 2 is always satisfied, and as a result, we keep repeating steps
in line 5. Thus, η(i)= (1
2)2i−1and since the condition in line 2 is always satisfied, we have that
f(xt+η(i)dt)> f(xt) +αη(i)∇f(xt)⊤dt. Therefore, we know that η(i)≥ηrwhere ηr>0is
defined in (75) from Lemma K.1. Notice that η(i)≥ηr>0for any iandlimi→∞η(i)= 0, this
leads to a contradiction.
Hence, at some point either the algorithm returns a step size satisfying the weak Wolfe conditions or
η(i)
minmust become positive. Suppose that this happens at iteration K2≥1of the loop in Algorithm 1.
Then, we know that η(K2)= (1
2)2K2−1.
31In the first case that the algorithm finds an admissible step size and returns ηK2,ηK2satisfies
the Armijo-Wolfe conditions and therefore ηK2≤ηr. Using the upper bound result in (76) from
Lemma K.1, we obtain that η(K2)= 22K2−1≤ηr≤2(1−α)(1 + Ct)ρt, which leads to
λt=K2≤log2
1 + log2 
2(1−α)(1 + Ct)ρt
. (91)
In the second case that η(i)
minbecomes positive but the algorithm does not terminate, we have that
η(K2−1)doesn’t satisfy the sufficient condition (5)andη(K2−1)≥ηr. Using the lower bound result
in (76) from Lemma K.1, we obtain that η(K2−1)= (1
2)2K2−1−1≥ηr≥2(1−α)
1+Ctρt, which leads to
K2≤1 + log2
1 + log21 +Ct
2(1−α)ρt
. (92)
Then, we further go through the log bisection process. Using the same techniques, we can assume
that Algorithm 1 terminates after K2+ Γ2iterations, where Γ2≥1satisfies that
1
2Γ2−1log2η(K2+1)
max
η(K2+1)
min= log2η(K2+Γ2)
max
η(K2+Γ2)
min≥log2ηr
ηl>log2
1 +β−α
(1−β)(1 + 2 Ct)
(93)
where the last inequality holds since (78) in Lemma K.1. Notice that η(K2+1)
max = (1
2)2K2−1−1and
ηK2+1
min = (1
2)2K2−1. Hence, we obtain that
log2η(K2+1)
max
η(K2+1)
min= 2K2−1≤1 + log21 +Ct
2(1−α)ρt. (94)
Combing (93), (94) and using logx≥1−1
x, we have that
Γ2≤1 + log2
1 + log21 +Ct
2(1−α)ρt
−log2log2
1 +β−α
(1−β)(1 + 2 Ct)
≤1 + log2
1 + log21 +Ct
2(1−α)ρt
−log2log
1 +β−α
(1−β)(1 + 2 Ct)
≤1 + log2
1 + log21 +Ct
2(1−α)ρt
−log2
1−1
1 +β−α
(1−β)(1+2 Ct)
= 1 + log2
1 + log21 +Ct
2(1−α)ρt
+ log2
1 +(1−β)(1 + 2 Ct)
β−α
.(95)
Leveraging (92) and (95), we prove that
λt=K2+ Γ2
≤2 + 2 log2
1 + log21 +Ct
2(1−α)ρt
+ log2
1 +(1−β)(1 + 2 Ct)
β−α
.(96)
Notice that α <1
2and thus1
2(1−α)<2(1−α), combining (82),(90),(91) and(96), we prove the
final conclusion
λt≤2 + log2
1 +(1−β)(1 + 2 Ct)
β−α
+ 2 log2
1 + log2 
2(1−α)(1 + Ct)
+ max {log2ρt,log21
ρt}
.
K.3 Proof of Theorem 7.1
Using result from Proposition K.2, we have that
Λt=1
tt−1X
i=0λi≤2 +1
tt−1X
i=0log2
1 +(1−β)(1 + 2 Ci)
β−α
+2
tt−1X
i=0log2
1 + log2 
2(1−α)(1 + Ci)
+ max {log2ρi,log21
ρi}
.(97)
32Using Jensen’s inequality, we have that
1
tt−1X
i=0log2
1 +(1−β)(1 + 2 Ci)
β−α
≤log2
1 +1−β
β−α+2(1−β)
β−αPt−1
i=0Ci
t
. (98)
1
tt−1X
i=0log2
1 + log2 
2(1−α)(1 + Ci)
+ max {log2ρi,log21
ρi}
≤log2
1 + log22(1−α) +1
tt−1X
i=0log2(1 +Ci) +1
tt−1X
i=0max{log2ρi,log21
ρi}
≤log2
1 + log22(1−α) + log2 
1 +Pt−1
i=0Ci
t) +1
tt−1X
i=0max{log2ρi,log21
ρi}
.(99)
We also have that
1
tt−1X
i=0max{log2ρi,log21
ρi}=1
tt−1X
i=0,ρi≥1log2ρi+1
tt−1X
i=0,0≤ρi<1log21
ρi
=1
tt−1X
i=0,ρi≥2log2ρi+1
tt−1X
i=0,1≤ρi<2log2ρi+1
tt−1X
i=0,1
2<ρi<1log21
ρi+1
tt−1X
i=0,ρi≤1
2log21
ρi
≤2 +1
tt−1X
i=0,ρi≥2log2ρi+1
tt−1X
i=0,ρi≤1
2log21
ρi,(100)
where the inequality is due to log2ρi≤1forρi<2andlog21
ρi≤1forρi>1
2. Using the definition
ofωand (b) in Lemma G.1, we obtain that
1
tt−1X
i=0,ρi≥2log2ρi=log2e
tt−1X
i=0,ρi≥2logρi=log2e
tt−1X
i=0,ρi≥2(ρi−1−ω(ρi−1))
≤log2e
tt−1X
i=0,ρi≥2(2ρi
ρi−1ω(ρi−1)−ω(ρi−1))
=log2e
tt−1X
i=0,ρi≥2ρi+ 1
ρi−1ω(ρi−1)≤3 log2e
tt−1X
i=0,ρi≥2ω(ρi−1).(101)
Similarly, using (c) in Lemma G.1, we obtain that
1
tt−1X
i=0,ρi≤1
2log21
ρi=log2e
tt−1X
i=0,ρi≤1
2log1
ρi=log2e
tt−1X
i=0,ρi≤1
2(ω(ρi−1) + 1 −ρi)
≤log2e
tt−1X
i=0,ρi≤1
2(ω(ρi−1) +1 +ρi
1−ρiω(ρi−1))
=log2e
tt−1X
i=0,ρi≤1
22
1−ρiω(ρi−1)≤4 log2e
tt−1X
i=0,ρi≤1
2ω(ρi−1).(102)
Combining (100), (101) and (102), we prove that
1
tt−1X
i=0max{log2ρi,log21
ρi} ≤2 +1
tt−1X
i=0,ρi≥2log2ρi+1
tt−1X
i=0,ρi≤1
2log21
ρi
≤2 +4 log2e
tt−1X
i=0ω(ρi−1)≤2 +6
t
Ψ(˜B0) + 2t−1X
i=0Ci
.(103)
33where we use the fact that ω(ρi−1)≥0for any i≥0and the last inequality is due to (46) in
Proposition G.2. Leveraging (97), (98), (99) and (103), we have that
Λt≤2 + log2
1 +1−β
β−α+2(1−β)
β−αPt−1
i=0Ci
t
+ 2 log2
3 + log22(1−α) + log2 
1 +Pt−1
i=0Ci
t) +6
t 
Ψ(˜B0) + 2t−1X
i=0Ci
≤2 + log2
1 +1−β
β−α+2(1−β)
β−αPt−1
i=0Ci
t
+ 2 log2
log216(1−α) + log2 
1 +Pt−1
i=0Ci
t) +6Ψ(˜B0) + 12Pt−1
i=0Ci
t
.
We prove the final conclusion using (44) from the proof of Theorem 5.2 in Appendix F.2, i.e.,
t−1X
i=0Ci≤C0Ψ(¯B0) +3C0κ
α(1−β).
K.4 Corollaries of Theorem 7.1 for B0=LIandB0=µI
Corollary K.3 (B0=LI).Suppose that Assumptions 2.1, 2.2 and 2.3 hold. Let {xt}t≥0be the
iterates generated by the BFGS method, where the step size satisfies the Armijo-Wolfe conditions in
(5)and(6). For any initial point x0∈Rdand the initial Hessian approximation matrix B0=LI,
the average complexity of line search Algorithm 1 Tkis upper bounded by
Λt≤2 + log2
1 +1−β
β−α+2(1−β)
β−α3C0κ
α(1−β)t
+ 2 log2
log216(1−α) + log2 
1 +3C0κ
α(1−β)t
+6dκ+36C0κ
α(1−β)
t
.
Moreover, when t≥6dκ+36
α(1−β)C0κ, we have that
Λt≤2 + log2 
1 +3(1−β)
β−α
+ 2 log2(5 + log22(1−α)). (104)
Proof. Since B0=LI, we have ¯B0=1
LB0=Iand˜B0=∇2f(x∗)−1
2B0∇2f(x∗)−1
2=
L∇2f(x∗)−1. Using results in the proof of Corollary 5.3, we have
Ψ(¯B0) = 0 , Ψ(˜B0)≤dκ.
Combining these two results with the result in Theorem 7.1, we prove the conclusion.
Corollary K.4 (B0=µI).Let{xt}t≥0be the iterates generated by the BFGS method with inexact
line search (5),(6)and suppose that Assumptions 2.1, 2.2 and 2.3 hold. For any initial point x0∈Rd
and the initial Hessian approximation matrix B0=µI, the average complexity of line search
Algorithm 1 Tkis upper bounded by
Λt≤2 + log2
1 +1−β
β−α+2(1−β)
β−αC0dlogκ+3C0κ
α(1−β)
t
+ 2 log2
log216(1−α) + log2 
1 +C0dlogκ+3C0κ
α(1−β)
t
+6(1 + 2 C0)dlogκ+36C0κ
α(1−β)
t
.
Moreover, when t≥6(1 + 2 C0)dlogκ+36C0κ
α(1−β), we have that
Λt≤2 + log2 
1 +3(1−β)
β−α
+ 2 log2(5 + log22(1−α)). (105)
34Proof. Since B0=µI, we have ¯B0=1
κB0=Iand˜B0=∇2f(x∗)−1
2B0∇2f(x∗)−1
2=
µ∇2f(x∗)−1. Using results in the proof of Corollary 4.2, we have
Ψ(¯B0)≤dlogκ, Ψ(˜B0)≤dlogκ.
Combining these two results with (26) in Theorem 6.4, we prove the conclusion.
35NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our claims in the abstract and introduction align with all the theoretical and
experimental results presented in our paper. We assert establishing global convergence of
BFGS with the Armijo-Wolfe conditions, and our theoretical results guarantee this.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have discussed the limitations and drawbacks of this paper in the second
paragraph of Section 9.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
36Answer: [Yes]
Justification: All the theorems, formulas, and proofs in the paper are numbered and cross-
referenced. All assumptions for each presented result are clearly stated or referenced in the
statements of the lemmas, propositions, or theorems. The proofs of all results are presented
in the supplemental material. High-level ideas of the proofs are included in the main text
whenever possible. Some lemmas are borrowed from [38], and this is explicitly mentioned
in both the paper and the supplementary material section D.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
37some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The paper does not include experiments requiring code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
38•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
39•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
40•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
41•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
42