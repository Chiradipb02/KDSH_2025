Exact Random Graph Matching with Multiple Graphs
Anonymous Author(s)
Affiliation
Address
email
Abstract
This work studies fundamental limits for recovering the underlying correspondence 1
among multiple correlated random graphs. We identify a necessary condition for 2
any algorithm to correctly match all nodes across all graphs, and propose two 3
algorithms for which the same condition is also sufficient. The first algorithm 4
employs global information to simultaneously match all the graphs, whereas the 5
second algorithm first partially matches the graphs pairwise and then combines the 6
partial matchings by transitivity. Both algorithms work down to the information 7
theoretic threshold. Our analysis reveals a scenario where exact matching between 8
two graphs alone is impossible, but leveraging more than two graphs allows exact 9
matching among all the graphs. Along the way, we derive independent results 10
about the k-core of Erd ˝os-Rényi graphs. 11
1 Introduction 12
The information age has ushered an abundance of correlated networked data. For instance, the 13
network structure of two social networks such as Facebook and Twitter is correlated because users are 14
likely to connect with the same individuals in both networks. This wealth of correlated data presents 15
both opportunities and challenges. On one hand, information from various datasets can be combined 16
to increase the fidelity of data - translating to better performance in downstream learning tasks. On the 17
other hand, the interconnected nature of this data also raises privacy and security concerns. Linkage 18
attacks, for instance, exploit correlated data to identify individuals in an anonymized network by 19
linking to other sources [NS09]. This poses a significant threat to user privacy. 20
Graph matching is the problem of recovering the underlying latent correspondence between corre- 21
lated networks. The problem finds many applications in machine learning: de-anonymizing social 22
networks [NS08, NS09], identifying similar functional components between species by matching 23
their protein-protein interaction networks [BSI06, KHGPM16], object detection [SS05] and track- 24
ing [YYL+16] in computer vision, and textual inference for natural language processing [HNM05]. In 25
most applications of interest, data is available in the form of several correlated networks. For instance, 26
social media users are active each month on 6.7 social platforms on average [Ind23]. Similarly, 27
reconciling protein-protein interaction networks among multiple species is an important problem in 28
computational biology [SXB08]. As a first step toward this objective, many research works have 29
studied the problem of matching twocorrelated graphs. 30
1.1 Related Work 31
The theoretical study of graph matching algorithms and their performance guarantees has primarily 32
focused on Erd ˝os-Rényi (ER) graphs. Pedarsani and Grossglauser [PG11] introduced the subsampling 33
model to generate two such correlated graphs. The model entails twice subsampling each edge 34
independently from a parent ER graph to obtain two sibling graphs, both of which are marginally 35
ER graphs themselves. The goal is then to match nodes between the two graphs to recover the 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.underlying latent correspondence. This has been the framework of choice for many works that study 37
graph matching. For example, Cullina and Kiyavash studied the problem of exactly matching two 38
ER graphs, where the objective is to match allvertices correctly [CK16, CK17]. They identified a 39
threshold phenomenon for this task: exact recovery is possible if the problem parameters are above a 40
threshold, and impossible otherwise. Subsequently, threshold phenomena were also identified for 41
partial graph matching between ER graphs - where the objective is to match only a positive fraction 42
of nodes [GML21, HM23, WXY22, DD23]. The case of almost-exact recovery - where the objective 43
is to match all but a negligible fraction of nodes - was studied by Cullina and co-authors: a necessary 44
condition for almost exact recovery was identified, and it was shown that the same condition is also 45
sufficient for the k-core estimator [CKMP19]; the estimator is described formally in Section 3. This 46
estimator proved useful to uncover the fundamental limits for graph matching in other contexts such 47
as the stochastic block model [GRS22] and inhomogeneous random graphs [RS23]. Ameen and 48
Hajek [AH23] showed some robustness properties of the k-core estimator in the context of matching 49
ER graphs under node corruption. The estimator plays an important role in the present work as well. 50
A sound understanding of ER graphs inspires algorithms for real-world networks. Various efficient al- 51
gorithms have been proposed, including algorithms based on the spectrum of the graph adjacency ma- 52
trices [FMWX22], node degree and neighborhood based algorithms [DCKG19,DMWX21,MRT23] as 53
well as algorithms based on iterative methods [DL23] and counting subgraphs [MWXY23, BCL+19]. 54
Some of these are discussed in Section 5 in relation to the present work. 55
Incorporating information from multiple graphs to match them has been recognized as an important 56
research direction, for instance in the work of Gaudio and co-authors [GRS22]. To our knowledge, 57
the only other papers to consider matchings among multiple graphs are the works of Josephs and 58
co-authors [JLK21], and of Rácz and Sridhar [RS21]. However, these works have different objectives 59
and are not concerned with the fundamental limits for matching mgraphs. In fact, both works note 60
that it is possible to exactly match mgraphs whenever it is possible to exactly match any two graphs 61
by pairwise matching all the graphs exactly. In contrast, we show that under appropriate conditions, it 62
is possible to exactly match mER graphs even when no two graphs can be pairwise matched exactly. 63
Contributions In this work, we investigate the problem of combining information from multiple 64
correlated networks to boost the number of nodes that are correctly matched among them. We 65
consider the natural generalization of the subsampling model to generate mcorrelated random graphs, 66
and identify a threshold such that it is impossible for any algorithm to match all nodes correctly 67
across all graphs when the problem parameters are below this threshold. Conversely, we show that 68
exact recovery is possible above the threshold. This characterization generalizes known results for 69
exact graph matching when m= 2. Subsequently, we show that there is a region in parameter space 70
for which exactly matching any two graphs is impossible using only the two graphs, and yet exact 71
graph matching is possible among m > 2graphs using all the graphs. 72
We present two algorithms and prove their optimality for this task. The first algorithm matches all m 73
graphs simultaneously based on global information about the graphs. In contrast, the second algorithm 74
firstpairwise matches graphs, and then combines them to match all nodes across all graphs. We show 75
that both algorithms correctly match all the graphs all the way down to the information theoretic 76
threshold. Finally, we illustrate through simulation that our subroutine to combine information from 77
pairwise comparisons between networks works well when paired with efficient algorithms for graph 78
matching. Our analysis also yields some theoretical results about the k-core of ER graphs that are of 79
independent interest. 80
2 Preliminaries and Setup 81
Notation In this work, G∼ER(n, p)denotes that the graph Gis sampled from the Erd ˝os-Rényi 82
distribution with parameters nandp, i.e.Ghasnnodes and each edge is independently present with 83
probability p. For a graph G, we denote the set of its vertices by V≡V(G)and its edges by E(G). 84
Theedge status of each vertex pair {i, j}withi̸=jis denoted by G{i.j}, so that G{i, j}= 1if 85
{i, j} ∈E(G)andG{i, j}= 0otherwise. The degree of a node vin graph Gis denoted δG(v). Let 86
πdenote a permutation on V(G) ={1,···, n}. For a graph G, denote by Gπthe graph obtained by 87
permuting the nodes of Gaccording to π, so that 88
G{i, j}=Gπ{π(i), π(j)} ∀i, j∈V(G)such that i̸=j.
Standard asymptotic notation (O(·), o(·),···)is used throughout and it is implicit that n→ ∞ . 89
2G1∼ER
n,Cslogn
n
G′
2∼ER
n,Cslogn
n
G′
m−1∼ER
n,Cslogn
n
G′
m∼ER
n,Cslogn
n
G2
Gm−1
Gm
π∗
12
π∗
1,m−1
π∗
1m
G∼ER
n,Clogn
n
Subsampling model
with parameters C,s
Independently
each edge w.p. s
subsample
Permute G′
2,···G′
maccording to π∗
12,···, π∗
1m
selected independently and uniformly at random
G2
Gm−1
GmFigure 1: Illustration of obtaining mcorrelated graphs from the subsampling model
Subsampling model Consider the subsampling model for correlated random graphs [PG11], which 90
has a natural generalization to the setting of mgraphs. In this model, a parent graph Gis sampled 91
from the Erd ˝os-Rényi distribution ER(n, p). The mgraphs G1, G′
2,···, G′
m−1, G′
mare obtained by 92
independently subsampling each edge from Gwith probability s. Finally, the graphs G2,···, Gm 93
are obtained by permuting the nodes of each of the graphs G′
2,···, G′
mrespectively according to 94
independent permutations π∗
12,···, π∗
1msampled uniformly at random from the set of all permutations 95
on[n], i.e. 96
Gj= (G′
j)π∗
1jfor all j∈ {2,···, m}.
Figure 1 illustrates this process of obtaining correlated graphs using the subsampling model. In this 97
work, we are interested in the setting where sis constant and p=Clog(n)/nfor some C > 0. 98
Objective 1. Determine conditions on parameters C,sandmso that given correlated graphs 99
G1,···, Gmfrom the subsampling model, it is possible to exactly recover the underlying correspon- 100
dences π∗
12,···, π∗
1mwith probability 1−o(1). 101
Stated thus, the underlying correspondences use the graph G1as a reference. Thus, for ease of 102
notation, we will use G1andG′
1interchangeably. Note that the underlying correspondence between 103
all the graphs is fixed upon fixing π∗
12,···, π∗
1m: for any two graphs GiandGj, their underlying 104
correspondence is given by π∗
ij:=π∗
1j◦(π∗
1i)−1. 105
Formally, a matching (µ12,···, µ1m)is a collection of injective functions with domain dom(µ1i)⊆ 106
Vfor each i, and co-domain V. An estimator is simply a mechanism to map any collection of graphs 107
(G1,···, Gm)to a matching. We say that an estimator completely matches the graphs if the output 108
mappings µ12,···µ1mare all complete, i.e. they are all permutations on {1,···, n}. 109
3 Main Results and Algorithm 110
This section presents necessary and sufficient conditions to meet Objective 1. 111
Theorem 2 (Impossibility) .LetG1,···, Gmbe correlated graphs obtained from the subsampling 112
model with parameters Cands, and let π∗
12,···, π∗
1mdenote the underlying latent correspondences 113
between G1andG2,···, Gmrespectively. Suppose that 114
Cs 
1−(1−s)m−1
<1.
The output bπ12,···,bπ1mof any estimator satisfies 115
P(bπ12=π∗
12,bπ13=π∗
13,···,bπ1m=π∗
1m) =o(1).
30 2 4 6 8 10
C0.00.20.40.60.81.0s
Cs(1(1s)m1)<1
Cs2>1(a)m= 3
0 2 4 6 8 10
C0.00.20.40.60.81.0s
Cs(1(1s)m1)<1
Cs2>1 (b)m= 10
Figure 2: Regions in parameter space. Orange : Exactly matching mgraphs is impossible even with
mgraphs. Blue: Exactly matching 2graphs is possible with 2graphs. Striped : Impossible to match 2
graphs using only the 2graphs, but possible using mgraphs as side information.
Theorem 2 implies that the condition Cs(1−(1−s)m−1>1is a necessary condition to exactly 116
match mgraphs with probability bounded away from 0. We show that this condition is also sufficient 117
to exactly match mgraphs with probability going to 1. 118
Theorem 3 (Achievability) .LetG1,···, Gmbe correlated graphs obtained from the subsampling 119
model with parameters Cands, and let π∗
12,···, π∗
1mdenote the underlying latent correspondences 120
between G1andG2,···, Gmrespectively. Suppose that 121
Cs 
1−(1−s)m−1
>1.
There is an estimator whose output bπ12,···,bπ1msatisfies 122
P(bπ12=π∗
12,bπ13=π∗
13,···,bπ1m=π∗
1m) = 1−o(1).
Theorems 2 and 3 together characterize the threshold for exact recovery. A few remarks are in order. 123
1.Form= 2, the condition Cs(1−(1−s)m−1)>1reduces to Cs2>1, which is known to 124
be necessary and sufficient for exactly matching two graphs [CK17, WXY22]. 125
2. For any m > 2, there is a non-empty region in the parameter space defined by
Cs(1−(1−s)m−1)>1> Cs2.
For any Candsin this region, it is impossible to exactly match any two graphs GiandGj 126
without using the other m−2graphs as side information. Upon using them, however, it is 127
possible to exactly match all nodes across the mgraphs. This is illustrated in Figure 2. 128
3.1 Algorithms for exact recovery 129
For any two graphs H1andH2on the same vertex set V, denote by H1∨H2their union graph and 130
byH1∧H2their intersection graph . An edge {i, j}is present in H1∨H2if it is present in either 131
H1orH2. Similarly, the edge is present in H1∧H2if it is present in both H1andH2. 132
A natural starting point is to study the maximum likelihood estimator (MLE) because it is optimal. 133
To that end, we compute the log-likelihood function; the details are deferred to Appendix A. 134
Theorem 4. Letπ12,···, π1mdenote a collection of permutations on {1,···, n}. Then 135
logP(G1,···, Gm|π∗
12=π12,···, π∗
1m=π1m)∝const.− |E(G1∨Gπ12
2∨ ··· ∨ Gπ1m
m)|,
where const. depends only on p, sandG1,···, Gm. 136
Theorem 4 reveals that the MLE for exactly matching mgraphs has a neat interpretation: simply pick 137
π12,···, π1mto minimize the number of edges in the corresponding union graph. This is presented 138
as Algorithm 1. Despite this nice interpretation of the MLE, its analysis is quite cumbersome. We 139
instead present and analyze a different estimator, presented as Algorithm 2. 140
4Algorithm 1: Maximum likelihood estimator
require : Graphs G1, G2,···, Gmon a common vertex set V
1for(π12, π13,···, π1m)such that each π1jis a permutation on [n]do
2 W(π12,···, π1m)← |E(G1∨Gπ12
2∨ ··· ∨ Gπ1mm)|
3end
4return (bπML
12,···,bπML
1m)∈arg maxπ12,···,π1mW(π12,···, π1m)
Algorithm 2: Matching through transitive closure
require : Graphs G1, G2,···, Gmon a common vertex set V, Integer k
// Step 1: Pairwise matching
1for{i, j}in{1,···, m}such that i < j do
2bνij←arg maxπ|corek 
Gi∧Gπ
j
|
3bµij←bνijwith domain restricted to corek(Gi∧Gbνij
j) //k-core estimator
4end
// Step 2: Boosting through transitive closure
5forv∈Vdo
6 forj= 2,···, mdo
7 ifthere is a sequence of indices 1 =k1,···, kℓ=jin[m]such that
bµkℓ−1,j◦ ··· ◦bµk2,k3◦bµ1,k2(v) =v′for some v′∈[n]then
8 Setbπ1j(v) =v′
9 end
10 end
11end
12return bπ12,···,bπ1m
Algorithm 2 runs in two steps: In step 1, the k-core estimator, for a suitable choice of k, is used 141
to pairwise match all the graphs. For any iandj, thek-core estimator selects a permutation bνij 142
to maximize the size of the k-core1ofGi∧Gbνij
j. It then outputs a matching bµijby restricting the 143
domain of bνijtocorek(Gi∧Gbνij
j). These matchings bµijneed not be complete - in fact, each of them 144
is a partial matching with high probability whenever Cs2<1. In step 2, these partial matchings 145
areboosted as follows: If a node vis unmatched between two graphs GiandGj, then search for a 146
sequence of graphs Gi, Gk1,···, Gkℓ, Gjsuch that vis matched between any two consecutive graphs 147
in the sequence. If such a sequence exists, then extend bµi,jto include vby transitively matching it 148
from GitoGj. 149
In Section 4.2, we show that Algorithm 2 correctly matches all nodes across all graphs with probability 150
1−o(1), whenever the necessary condition Cs(1−(1−s)m−1)>1holds. We remark that this 151
also implies that Algorithm 1 succeeds under the same condition, because the MLE is optimal. Note 152
that the MLE selects all permutations bπ12,···,bπ1msimultaneously based on their union graph. In 153
contrast, Algorithm 2 only ever makes pairwise comparisons between graphs. Perhaps surprisingly, it 154
turns out that this is sufficient for exact recovery. An analysis of Algorithm 2 is presented in Section 4. 155
Along the way, independent results of interest on the k-core of Erd ˝os-Rényi graphs are obtained. 156
1Thek-core of a graph Gis the largest subset of vertices core k(G)such that the induced subgraph has
minimum degree at least k.
54 Proof Outlines and Key Insights 157
4.1 Impossibility of exact graph matching (Theorem 2) 158
This result has a simple proof following a genie-aided converse argument. The idea is to reduce the 159
problem to that of matching two graphs by providing extra information to the estimator. 160
Proof of Theorem 2 . If the correspondences π∗
12,···, π∗
1,m−1were provided as extra information to 161
an estimator, then the estimator must still match Gmwith the union graph G′
1∨G′
2∨ ··· ∨ G′
m−1. 162
This can be viewed as an instance of matching two graphs obtained by asymmetric subsampling: 163
the graph Gmis obtained from a parent graph G∼ER(n, Clog(n)/n)by subsampling each edge 164
independently with probability s1:=s, and the graph eGm−1:=G′
1∨G′
2∨ ··· ∨ G′
m−1is obtained 165
from Gby subsampling each edge independently with probability s2:= 1−(1−s)m−1. Cullina 166
and Kiyavash studied this model for matching two graphs: Theorem 2 of [CK17] establishes that 167
matching GmandeGm−1is impossible if Cs1s2<1, or equivalently if Cs(1−(1−s)m−1)<1. 168
4.2 Achievability of exact graph matching (Theorem 3) 169
Algorithm 2 succeeds if both step 1 and step 2 succeed, i.e. 170
1. Each instance of pairwise matching using the k-core estimator is correct on its domain, i.e.
bµij(v) =π∗
ij(v)∀v∈dom(bµij),∀i, j.
2.For each node vand any two graphs GiandGj, there is a sequence of graphs such that v 171
can be transitively matched through those graphs between GiandGj. 172
On step 1 This falls back to the regime of analyzing the performance of the k-core estimator in the 173
setting of two graphs. Cullina and co-authors [CKMP19] showed that the k-core estimator is precise : 174
For any two correlated graphs GiandGjwithp=Clog(n)/nand constant s, thek-core estimator 175
correctly matches all nodes in corek(G′
i∧G′
j)with probability 1−o(1). In fact, this is true for any 176
C > 0and for any k≥13[RS23]. Therefore, using the fact that the number of instances of pairwise 177
matchings is constant whenever mis constant, a union bound reveals 178
P(∃1≤i < j≤msuch that bµij(v)̸=π∗
ij(v)for some v∈corek(G′
i∧G′
j))
≤mX
i=1mX
j=1P 
bµi,j(v)̸=π∗
i,j(v)for some v∈corek(G′
i∧G′
j)
=o(1).
We have proved the following. 179
Proposition 5. LetG1,···, Gmbe correlated graphs from the subsampling model. Let k≥13and 180
letbµijdenote the matching output by the k-core estimator on graphs GiandGj. Then, 181
P(∃1≤i < j≤m,andv∈corek(G′
i∧G′
j))such that bµij(v)̸=π∗
ij(v)) =o(1).
On step 2 The challenging part of the proof is to show that boosting through transitive closure 182
matches all the nodes with probability 1−o(1)ifCs(1−(1−s)m−1)>1. It is instructive to 183
visualize this using transitivity graphs . 184
Definition 6 (Transitivity graph, H(v)).For each node v∈V, letH(v)denote the graph on the vertex 185
set{g1,···, gm}such that an edge {gi, gj}is present in H(v)if and only if v∈corek(G′
i∧G′
j). 186
On the event that each instance of pairwise matching using the k-core is correct, the edge {gi, gj} 187
is present in H(v)if and only if vis correctly matched using the k-core estimator between Giand 188
Gj, i.e.π∗
1i(v)is matched to π∗
1j(v). Thus, in order for Step 2 to succeed (i.e. to exactly match all 189
vertices across all graphs), it suffices that the graph H(v)is connected for each node v∈V. However, 190
studying the connectivity of the transitivity graphs is challenging because in any graph H(v), no 191
two edges are independent. This is because the k-cores of any two intersection graphs G′
a∧G′
band 192
G′
c∧G′
dare correlated, because all the graphs Ga, Gb, GcandGdare themselves correlated. To 193
overcome this, we introduce another graph eH(v)that relates to H(v)and is amenable to analysis. 194
6Definition 7. For each node v∈V, leteH(v)denote a complete weighted graph on the vertex set 195
{g1,···, gm}such that the weight on any edge {gi, gj}isecv(i, j) :=δG′
i∧G′
j(v). 196
The relationship between the graphs H(v)andeH(v)stems from a useful relationship between the 197
degree of node vinG′
i∧G′
jand the inclusion of vincorek(G′
i∧G′
j)for each iandj. Since 198
this result is of independent interest in the study of random graphs, we state it below for general 199
Erd˝os-Rényi graphs. 200
Lemma 8. Letnandkbe positive integers and let G∼ER(n, αlog(n)/n)for some α >0. Letv 201
be a node of Gand let δG(v)denote the degree of vinG. Then, 202
P({v /∈corek(G)} ∩ {δG(v)≥k+ 1/α}) =o(1/n). (1)
For any iandj, the graph G′
i∧G′
j∼ER(n, Cs2log(n)/n). Thus, Lemma 8 implies that with prob- 203
ability 1−o(1/n), if a pair {gi, gj}has edge weight ecij≥k+ 1/αineH(v), then the corresponding 204
edge{gi, gj}is present in the transitivity graph H(v). Equivalently, vis correctly matched between 205
GiandGjin the instance of pairwise k-core matching between them. 206
The graph H(v)is not connected only if it contains a (non-empty) vertex cut U⊂ {1,···, m}with 207
no edge crossing between UandUc. Letcv(U)denote the number of such crossing edges in H(v). 208
Furthermore, define the cost of the cut UineH(v)as 209
ecv(U) :=X
i∈UX
j∈Ucecv(i, j).
Lemma 8 is a statement about a single graph, but we show it can be invoked to prove the following. 210
Theorem 9. LetG1,···, Gmbe correlated graphs from the subsampling model with parameters C 211
ands. Letv∈Vand let Ube a vertex cut of {1,···, m}such that |U| ≤ ⌊m/2⌋. Then, 212
P
{cv(U) = 0} ∩
ecv(U)>m2
4
k+1
Cs2
=o(1/n). (2)
It suffices therefore to analyze the probability that the graph eH(v)has a cut Usuch that its cost ecv(U) 213
is too small. To that end, we show that the bottleneck arises from vertex cuts of small size. Formally, 214
Theorem 10. LetG1,···, Gmbe correlated graphs from the subsampling model. Let v∈Vand 215
letUℓdenote the set {1,···, ℓ}forℓin{1,···,⌊m/2⌋}. For any vertex cut Uof{1,···, m}, let 216
ecv(U)denote its cost in the graph eH(v). The following stochastic ordering holds: 217
ecv(U1)⪯ecv(U2)⪯ ··· ⪯ ecv(U⌊m/2⌋).
Theorems 9 and 10 imply that the tightest bottleneck to the connectivity of H(v)is the event that 218
ecv(U1)is below the threshold r:=m2
4 
k+1
Cs2
, i.e. the sum of degrees of vover the intersection 219
graphs (G1∧G′
j:j= 2,···, m)is less than r. This event occurs only if the degree of vis less 220
thanrin each of the intersection graphs (G1∧G′
j:j= 2,···, m). However, under the condition 221
Cs(1−(1−s)m−1)>1, it turns out that this event occurs with probability o(1/n). 222
Theorem 11. LetG1,···, Gmbe obtained from the subsampling model with parameters Cands. 223
Letr=m2
4 
k+1
Cs2
. Letv∈[n]and suppose that Cs(1−(1−s)m−1)>1. Then, 224
P(ecv(U1)≤r)≤P 
δG1∧G′
2(v)≤r	
∩ ··· ∩
δG1∧G′m(v)≤r	
=o(1/n).
4.3 Piecing it all together: Proof of Theorem 3 225
Proof of Theorem 3. Letbπ12,···,bπ1mdenote the output of Algorithm 2 with k≥13. LetE1(resp. 226
E2) denote the event that Algorithm 1 (resp. Algorithm 2) fails to match all mgraphs exactly, i.e. 227
E1=
bπML
12̸=π∗
12	
∪ ··· ∪
bπML
1m̸=π∗
1m	
, E 2={bπ12̸=π∗
12} ∪ ··· ∪ { bπ1m̸=π∗
1m}.
7First, we show that the output of Algorithm 2 is correct with probability 1−o(1)whenever Cs(1− 228
(1−s)m−1)>1. If the event E2occurs, then either step 1 failed, i.e. there is a k-core matching bµij 229
that is incorrect, or step 2 failed, i.e. at least one of the graphs H(v)is not connected. Therefore, 230
P(E2)≤P
[
i,j[
v∈corek(G′
i∧G′
j)
bµij̸=π∗
ij	
+P [
v∈V{H(v)is not connected }!
≤o(1) +X
v∈Vqv,
where the last step uses Proposition 5, and qvdenotes the probability that the transitivity graph H(v) 231
is not connected. For each ℓin the set {1,···,⌊m/2⌋}, letUℓdenote the set {1,···, ℓ}. Then, 232
qv=P
⌊m/2⌋[
ℓ=1{∃U⊂ {1,···, m}:|U|=ℓandcv(U) = 0}

≤⌊m/2⌋X
ℓ=1m
ℓ
·P(cv(Uℓ) = 0)
≤⌊m/2⌋X
ℓ=1m
ℓ
P
ecv(Uℓ)≤m2
4
k+1
Cs2
+P
{cv(Uℓ) = 0} ∩
ecv(Uℓ)>m2
4
k+1
Cs2
(a)
≤⌊m/2⌋X
ℓ=1m
ℓ
P
ecv(Uℓ)≤m2
4
k+1
Cs2
+o1
n
(b)
≤⌊m/2⌋X
ℓ=1m
ℓ
P
ecv(U1)≤m2
4
k+1
Cs2
+o1
n
(c)
≤⌊m/2⌋X
ℓ=1mℓ
o1
n
+o1
n
=o1
n
.
Here, (a) uses Theorem 9, and (b) uses the fact that for any ℓ≥2, the random variable ecv(Uℓ) 233
stochastically dominates ecv(U1)(Theorem 10). Finally, (c) uses Theorem 11 and the fact that 234
Cs(1−(1−s)m−1)>1. Therefore, a union bound over all the nodes yields 235
P(E2)≤o(1) +X
v∈Vqv≤o(1) + n×o(1/n) =o(1).
Finally, by optimality of the MLE, it follows that
P(E1)≤P(E2) =o(1),
whenever Cs(1−(1−s)m−1)>1. This concludes the proof. 236
5 Discussion and Future Work 237
In this work, we introduced and analyzed matching through transitive closure - an approach that 238
combines information from multiple graphs to recover the underlying correspondence between them. 239
Despite its simplicity, it turns out that matching through transitive closure is an optimal way to 240
combine information in the setting where the graphs are pairwise matched using the k-core estimator. 241
A limitation of our algorithms is the runtime: Algorithm 2 does not run in polynomial time because 242
it uses the k-core estimator for pairwise matching, which involves searching over the space of 243
permutations. Even so, it is useful to establish the fundamental limits of exact recovery, and serve as 244
a benchmark to compare the performance of any other algorithm. 245
The transitive closure subroutine (Step 2) itself is efficient because it runs in polynomial time O(mn). 246
Therefore, a natural next step is to modify Step 1 in our algorithm so that the pairwise matchings are 247
done by an efficient algorithm. However, it is not clear if transitive closure is optimal for combining 248
information from the pairwise matchings in this setting. For example, there is a possibility that 249
the pairwise matchings resulting from the efficient algorithm are heavily correlated, and transitive 250
closure is unable to boost them. In Figure 3, we show experimentally that this is not the case for two 251
algorithms of interest: GRAMPA [FMWX22] and Degree Profiles [DMWX21]. 252
82 4 6 8 10 12 14 16 18 20
Number of graphs, m020406080100% of matched nodes in G1
GRAMPA, s=0.97
Deg Profiles, s=0.97
GRAMPA, s=0.94
Deg Profiles, s=0.94
GRAMPA, s=0.90
Deg Profiles, s=0.90Figure 3: Matching through transitive closure
1.GRAMPA is a spectral algorithm that uses the entire spectrum of the adjacency matrices to 253
match the two graphs. The code is available in [FMWX20]. 254
2.Degree Profiles associates with each node a signature derived from the degrees of its 255
neighbors, and matches nodes by signature proximity. The code is available in [DMWX20]. 256
Evidently, both algorithms benefit substantially from using transitive closure to boost the number of 257
matched nodes. This suggests that transitive closure can be a practical algorithm to boost matchings 258
between networks by using other networks as side-information. Unfortunately, both GRAMPA and 259
Degree Profiles require the graphs to be close to isomorphic in order to perform well, and so they 260
do not perform well when the model parameters are close to the information theoretic threshold for 261
exact recovery. Subsequently, they cannot be used to answer the question in Objective 1. 262
Our work presents several directions for future research. 263
•Polynomial-time algorithms. Using a polynomial-time estimator in place of the k-core 264
estimator in Step 1 of Algorithm 2 yields a polynomial-time algorithm to match mgraphs. 265
It is critical that the estimator in question is able to identify for itself the nodes that it has 266
matched correctly - this precision is present in the k-core estimator and enables the transitive 267
closure subroutine to work correctly. Can the performance guarantees of the k-core estimator 268
be realized through polynomial time algorithms that meet this constraint? 269
•Beyond Erd ˝os-Rényi graphs. The study of matching twoER graphs provided tools and 270
techniques that extended to the analysis of more realistic models. For instance, the k- 271
core estimator itself played a crucial role in establishing limits to matching two correlated 272
stochastic block models [GRS22] and two inhomogeneous random graphs [RS23]. Can the 273
techniques developed in the present work be used to identify the information theoretic limits 274
to exact recovery in these models in the general setting of mgraphs? 275
•Boosting for partial recovery. This work focused on exact recovery, where the objective is 276
to match allnodes across allgraphs. It would be interesting to consider a regime where any 277
instance of pairwise matching recovers at best a small fraction of nodes. Is it possible to 278
quantify the extent to which transitive closure boosts the number of matched nodes? 279
•Robustness. Finally, how sensitive to perturbation is the transitive closure algorithm? Is 280
it possible to quantify the extent to which an adversary may perturb edges in some of the 281
graphs without losing the performance guarantees of the matching algorithm? Algorithms 282
that perform well on models such as ER graphs and are further generally robust are expected 283
to also work well with real-world networks. 284
9References 285
[AH23] Taha Ameen and Bruce Hajek. Robust graph matching when nodes are corrupt. arXiv preprint 286
arXiv:2310.18543 , 2023. 287
[BCL+19] Boaz Barak, Chi-Ning Chou, Zhixian Lei, Tselil Schramm, and Yueqi Sheng. (Nearly) efficient 288
algorithms for the graph matching problem on correlated random graphs. Advances in Neural 289
Information Processing Systems , 32, 2019. 290
[BSI06] Sourav Bandyopadhyay, Roded Sharan, and Trey Ideker. Systematic identification of functional 291
orthologs based on protein network comparison. Genome research , 16(3):428–435, 2006. 292
[CK16] Daniel Cullina and Negar Kiyavash. Improved achievability and converse bounds for Erd ˝os-Rényi 293
graph matching. ACM SIGMETRICS performance evaluation review , 44(1):63–72, 2016. 294
[CK17] Daniel Cullina and Negar Kiyavash. Exact alignment recovery for correlated Erd ˝os-Rényi graphs. 295
arXiv preprint arXiv:1711.06783 , 2017. 296
[CKMP19] Daniel Cullina, Negar Kiyavash, Prateek Mittal, and Vincent Poor. Partial recovery of Erd ˝os- 297
Rényi graph alignment via k-core alignment. Proceedings of the ACM on Measurement and 298
Analysis of Computing Systems , 3(3):1–21, 2019. 299
[DCKG19] Osman Emre Dai, Daniel Cullina, Negar Kiyavash, and Matthias Grossglauser. Analysis of a 300
canonical labeling algorithm for the alignment of correlated Erd ˝os-Rényi graphs. Proceedings of 301
the ACM on Measurement and Analysis of Computing Systems , 3(2):1–25, 2019. 302
[DD23] Jian Ding and Hang Du. Matching recovery threshold for correlated random graphs. The Annals 303
of Statistics , 51(4):1718–1743, 2023. 304
[DL23] Jian Ding and Zhangsong Li. A polynomial-time iterative algorithm for random graph matching 305
with non-vanishing correlation. arXiv preprint arXiv:2306.00266 , 2023. 306
[DMWX20] Jian Ding, Zongming Ma, Yihong Wu, and Jiaming Xu. MATLAB code for degree profile in 307
graph matching. Available at: https://github.com/xjmoffside/degree_profile , 2020. 308
[DMWX21] Jian Ding, Zongming Ma, Yihong Wu, and Jiaming Xu. Efficient random graph matching via 309
degree profiles. Probability Theory and Related Fields , 179:29–115, 2021. 310
[FMWX20] Zhou Fan, Cheng Mao, Yihong Wu, and Jiaming Xu. MATLAB code for GRAMPA. Available 311
at: https://github.com/xjmoffside/grampa , 2020. 312
[FMWX22] Zhou Fan, Cheng Mao, Yihong Wu, and Jiaming Xu. Spectral graph matching and regularized 313
quadratic relaxations II: Erd ˝os-Rényi graphs and universality. Foundations of Computational 314
Mathematics , pages 1–51, 2022. 315
[GML21] Luca Ganassali, Laurent Massoulié, and Marc Lelarge. Impossibility of partial recovery in the 316
graph alignment problem. In Conference on Learning Theory , pages 2080–2102. PMLR, 2021. 317
[GRS22] Julia Gaudio, Miklós Z Rácz, and Anirudh Sridhar. Exact community recovery in correlated 318
stochastic block models. In Conference on Learning Theory , pages 2183–2241. PMLR, 2022. 319
[HM23] Georgina Hall and Laurent Massoulié. Partial recovery in the graph alignment problem. Opera- 320
tions Research , 71(1):259–272, 2023. 321
[HNM05] Aria Haghighi, Andrew Y Ng, and Christopher D Manning. Robust textual inference via graph 322
matching. In Proceedings of Human Language Technology Conference and Conference on 323
Empirical Methods in Natural Language Processing , pages 387–394, 2005. 324
[Hoe94] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. The collected 325
works of Wassily Hoeffding , pages 409–426, 1994. 326
[Ind23] Global Web Index. Social behind the screens trends report. GWI , 2023. 327
[JLK21] Nathaniel Josephs, Wenrui Li, and Eric. D. Kolaczyk. Network recovery from unlabeled noisy 328
samples. In 2021 55th Asilomar Conference on Signals, Systems, and Computers , pages 1268– 329
1273, 2021. 330
[KHGPM16] Ehsan Kazemi, Hamed Hassani, Matthias Grossglauser, and Hassan Pezeshgi Modarres. Proper: 331
global protein interaction network alignment through percolation matching. BMC bioinformatics , 332
17(1):1–16, 2016. 333
[Łuc91] Tomasz Łuczak. Size and connectivity of the k-core of a random graph. Discrete Mathematics , 334
91(1):61–68, 1991. 335
[MRT23] Cheng Mao, Mark Rudelson, and Konstantin Tikhomirov. Exact matching of random graphs with 336
constant correlation. Probability Theory and Related Fields , 186(1-2):327–389, 2023. 337
[MU17] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomization and proba- 338
bilistic techniques in algorithms and data analysis . Cambridge University Press, 2017. 339
10[MWXY23] Cheng Mao, Yihong Wu, Jiaming Xu, and Sophie H Yu. Random graph matching at Otter’s 340
threshold via counting chandeliers. In Proceedings of the 55th Annual ACM Symposium on Theory 341
of Computing , pages 1345–1356, 2023. 342
[NS08] Arvind Narayanan and Vitaly Shmatikov. Robust de-anonymization of large sparse datasets. In 343
2008 IEEE Symposium on Security and Privacy (sp 2008) , pages 111–125. IEEE, 2008. 344
[NS09] Arvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. In 2009 30th IEEE 345
Symposium on Security and Privacy , pages 173–187. IEEE, 2009. 346
[PG11] Pedram Pedarsani and Matthias Grossglauser. On the privacy of anonymized networks. In 347
Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and 348
Data Mining , pages 1235–1243, 2011. 349
[RS21] Miklós Z Rácz and Anirudh Sridhar. Correlated stochastic block models: Exact graph matching 350
with applications to recovering communities. Advances in Neural Information Processing Systems , 351
34:22259–22273, 2021. 352
[RS23] Miklós Z Rácz and Anirudh Sridhar. Matching correlated inhomogeneous random graphs using 353
thek-core estimator. arXiv preprint arXiv:2302.05407 , 2023. 354
[SS05] Christian Schellewald and Christoph Schnörr. Probabilistic subgraph matching based on convex 355
relaxation. In International Workshop on Energy Minimization Methods in Computer Vision and 356
Pattern Recognition , pages 171–186. Springer, 2005. 357
[SXB08] Rohit Singh, Jinbo Xu, and Bonnie Berger. Global alignment of multiple protein interaction 358
networks with application to functional orthology detection. Proceedings of the National Academy 359
of Sciences , 105(35):12763–12768, 2008. 360
[WXY22] Yihong Wu, Jiaming Xu, and Sophie H Yu. Settling the sharp reconstruction thresholds of random 361
graph matching. IEEE Transactions on Information Theory , 68(8):5391–5417, 2022. 362
[YYL+16] Junchi Yan, Xu-Cheng Yin, Weiyao Lin, Cheng Deng, Hongyuan Zha, and Xiaokang Yang. 363
A short survey of recent advances in graph matching. In Proceedings of the 2016 ACM on 364
international conference on multimedia retrieval , pages 167–174, 2016. 365
11A Maximum Likelihood Estimator 366
Recall the form of the maximum likelihood estimator as claimed in Theorem 4. 367
Theorem 4. Letπ12,···, π1mdenote a collection of permutations on {1,···, n}. Then 368
logP(G1,···, Gm|π∗
12=π12,···, π∗
1m=π1m)∝const.− |E(G1∨Gπ12
2∨ ··· ∨ Gπ1m
m)|,
where const. depends only on p, sandG1,···, Gm. 369
Proof. Notice that 370
P(G1,···, Gm|π∗
12,···, π∗
1m) =Y
e∈([n]
2)P(G1(e), G2(π∗
12(e))···, Gm(π∗
1m(e))|π∗
12,···, π∗
1m)
=Y
e∈([n]
2)P(G1(e), G′
2(e)···, G′
m(e)) (3)
where for a node pair e={u, v}, the shorthand π(e)denotes {π(u), π(v)}. The edge status of 371
any node pair ein the graph tuple (G1, G′
2,···, G′
m)can be any of the 2mbit strings of length 372
m, but the corresponding probability in (3)depends only on the number of ones and zeros in 373
the bit string. For i∈[m], letαidenote the number of node pairs ewhose corresponding tuple 374
(G1(e), G′
2(e),···, G′
m(e))has exactly i1’s: 375
αi:=X
e∈([n]
2)1{(G1(e), G′
2(e),···, G′
m(e))has exactly i1’s}.
Two key observations are in order. First, it follows by definition that α0+α1+···+αm= n
2
. 376
Second, by definition of αi, it follows that 377
mX
i=0iαi=X
e∈([n]
2)mX
j=1Gj(e) =X
e∈([n]
2)mX
j=1G′
j(e) (4)
is constant, independent of π∗
12,···, π∗
1m. It follows then that 378
(3)= (1−p+p(1−s)m)α0×mY
i=1 
psi(1−s)m−iαi
= (1−p+p(1−s)m)α0×pPm
i=1αi×mY
i=1 
si(1−s)m−iαi
= (1−p+p(1−s)m)α0×p(n
2)−α0×s
1−sPm
i=1iαi
×(1−s)mPm
i=1αi
=1−p+p(1−s)m
p(1−s)mα0
×(p(1−s)m)(n
2)×s
1−sPm
i=1iαi
∝
1 +1−p
p(1−s)mα0
,
where the last step uses (4). Finally, since1−p
p(1−s)m>0, it follows that the log-likelihood satisfies 379
log (P(G1,···, Gm|π∗
12,···, π∗
1m))∝const. +α0,
i.e. maximizing the likelihood corresponds to selecting π12,···, π1mto maximize α0- the number 380
of node pairs efor which G1(e) =G2(π12(e)) =···=Gm(π1m(e)) = 0 . This is equivalent to 381
minimizing the number of edges in the union graph G1∨Gπ12
2∨ ··· ∨ Gπ1mm, as desired. 382
Remark 12. In the case of two graphs, minimizing the number of edges in the union graph G1∨πG2 383
is equivalent to maximizing the number of edges in the intersection graph G1∧πG2. This is consistent 384
with existing literature on two graphs [CK16, WXY22]. 385
12B Concentration Inequalities for Binomial Random Variables 386
The following bounds for the binomial distribution are used frequently in the analysis. 387
Lemma 13. LetX∼Bin(n, p). Then, 388
1. For any δ >0, 389
P(X≥(1 +δ)np)≤eδ
(1 +δ)1+δnp
≤e
1 +δ(1+δ)np
. (5)
2. For any δ >5, 390
P(X≥(1 +δ)np)≤2−(1+δ)np. (6)
3. For any δ∈(0,1), 391
P(X≤(1−δ)np)≤e−δ
(1−δ)1−δnp
. (7)
Proof. All proofs follow from the Chernoff bound and can be found, or easily derived, from Theorems 392
4.4 and 4.5 of [MU17]. 393
C Proof of Lemma 8 394
We restate Lemma 8 for convenience. 395
Lemma 8. Letnandkbe positive integers and let G∼ER(n, αlog(n)/n)for some α >0. Letv 396
be a node of Gand let δG(v)denote the degree of vinG. Then, 397
P({v /∈corek(G)} ∩ {δG(v)≥k+ 1/α}) =o(1/n). (1)
Before presenting the proof, we present the intuition behind it. The events {v /∈corek(G)}and 398
{δG(v)≥k+ 1/α}are highly negatively correlated. However, consider the subgraph (G−v)of 399
Ginduced on the vertex set V− {v}, and note that the k-core of this subgraph does not depend on 400
the degree of v. Furthermore, if v /∈corek(G), then it must be that vhas fewer than kneighbors in 401
corek(G−v). Intuitively, this event has low probability if corek(G−v)is sufficiently large. 402
Notice that (G−v)∼ER(n−1, αlog(n)/n), and so standard results about the size of the k-core 403
of Erd ˝os-Rényi graphs apply. However, we require the error probability that the k-core of G−v 404
is too small to be o(1/n)- this is crucial since we will later use a union bound over all the nodes 405
v. Unfortunately, standard results such as [Łuc91] can only be invoked directly to show that the 406
corresponding probability is o(1), which is insufficient for our purpose. Later in this section, we 407
refine the analysis in [Łuc91] to obtain the desired convergence rate. The refinement culminates in 408
the following. 409
Lemma 14. Letα >0andG∼ER(n, αlog(n)/n). Letvbe a node of G. The size of the k-core of 410
G−vsatisfies 411
P 
|corek(G−v)|< n−3n1−α
=o(1/n).
The proof of Lemma 14 is deferred to Appendix C.1. It remains to study the error event that v 412
has too few neighbors in corek(G−v). To count the number of neighbors of vincorek(G−v), 413
we exploit the independence of corek(G−v)andvas follows: each neighbor of vis considered 414
asuccess if it belongs to corek(G−v)and a failure otherwise. Counting the number of successes 415
is equivalent to sampling with replacement δG(v)elements, each of which is independently a 416
success with probability |corek(G−v)|/(n−1). The number of successes then follows precisely a 417
hypergeometric distribution. This intuition is made rigorous in the proof below. 418
Let us recall some facts about the hypergeometric distribution because it plays an important role in 419
the proof. Denote by HypGeom (N, K, n )a random variable that counts the number of successes in 420
a sample of nelements drawn without replacement from a population of Nindividuals, of which 421
13Kelements are considered successes. Note that if this sampling were done with replacement , then 422
the number of successes would follow a Bin(n, K/N )distribution. A result of Hoeffding [Hoe94] 423
establishes that the HypGeom (N, K, n )distribution is convex-order dominated by the Bin(n, K/N ) 424
distribution, i.e. 425
E[f(HypGeom (N, K, n ))]≤E[f(Bin(n, K/N ))]for all convex functions f.
In particular, the function f(x) =etxis convex for any value of t, and so Chernoff bounds that hold 426
for the binomial distribution also hold for the corresponding hypergeometric distribution. This yields 427
the following proposition. 428
Proposition 15. LetX∼HypGeom (N, K, n ). It follows for any δ >0that 429
P
X > (1 +δ)×nK
N
≤eδ
(1 +δ)1+δnK/N
≤e
1 +δ(1+δ)nK/N
.
Our final remark about the hypergeometric distribution is a symmetry property. By interchanging the 430
success and failure states, it follows that 431
P(HypGeom (N, K, n ) =k) =P(HypGeom (N, N−K, n) =n−k).
The above intuition for the proof of Lemma 8 is formalized below. 432
Proof of Lemma 8. LetVdenote the vertex set of G, and let G−vdenote the induced subgraph of 433
Gon the vertex set V− {v}. For any set A⊆V, letNv(A)denote the set of neighbors of vin the 434
setA, i.e. 435
Nv(A) :={u∈A:{u, v} ∈E(G)}.
Since corek(G−v)⊆corek(G), it is true that 436
{v /∈corek(G)} ⊆ {| Nv(corek(G))| ≤k−1} ⊆ {| Nv(corek(G−v))| ≤k−1}.
It follows that 437
P({v /∈corek(G)} ∩ {δG(v)≥k+ 1/α})≤p1+p2,
where 438
p1=P 
{Nv(corek(G−v))≤k−1} ∩ {δG(v)≥k+ 1/α} ∩
|corek(G−v)|< n−3n1−α	
p2=P 
{Nv(corek(G−v))≤k−1} ∩ {δG(v)≥k+ 1/α} ∩
|corek(G−v)| ≥n−3n1−α	
It suffices to show that both p1andp2areo(1/n). The term p1deals with the probability that the 439
k-core of G−vis too small. In fact, by Lemma 14, it follows directly that 440
p1≤P 
|corek(G−v)|< n−3n1−α
=o(1/n),
Next, the probability p2is analyzed. Enumerate arbitrarily but independently the elements of sets 441
Nv(V)andcorek(G−v), so that 442
Nv(V) =
v1,···, vδG(v)	
,corek(G−v) =
a1,···, a|corek(G−v)|	
.
Given that Nv(V)has more than k+ 1/αnodes and corek(G−v)has more than n−3n1−αnodes, 443
it is true that 444
Nv(corek(G−v)) =Nv(V)∩corek(G−v)
⊇
v1,···, v⌈k+1/α⌉	
∩
a1,···, a⌈n−3n1−α⌉	
=:fNv(gcorek(G−v)).
In words, fNv(gcorek(G−v))counts among the first ⌈k+ 1/α⌉neighbors of vthose nodes that are 445
also in the first ⌈n−3n1−α⌉nodes of corek(G−v). Therefore, 446
p2=P 
{Nv(corek(G−v))≤k−1} ∩ {δG(v)≥k+ 1/α} ∩
|corek(G−v)| ≥n−3n1−α	
≤P 
{Nv(corek(G−v))≤k−1}δG(v)≥k+ 1/α,|corek(G−v)| ≥n−3n1−α
≤P
{fNv(gcorek(G−v))≤k−1}δG(v)≥k+ 1/α,|corek(G−v)| ≥n−3n1−α
(8)
14Note that corek(G−v)is entirely determined by the graph G−v, i.e. it is independent of the 447
neighbors of v. Consequently, the two sets
v1,···, v⌈k+1/α⌉	
and{a1,···, a⌈n−3n1−α⌉}are 448
selected independent of each other. Equivalently, given that |corek(G−v)| ≥n−3n1−αand 449
δG(v)≥k+ 1/α, the size of the intersection set fNv(gcorek(G−v))follows a hypergeometric 450
distribution with parameters (n−1,⌈n−3n1−α⌉,⌈k+ 1/α⌉). Therefore, 451
(8)=P 
HypGeom (n−1,⌈n−3n1−α⌉,⌈k+ 1/α⌉)≤k−1
(a)=P 
HypGeom (n−1, n−1− ⌈n−3n1−α⌉,⌈k+ 1/α⌉)≥ ⌈k+ 1/α⌉ −(k−1)
=P 
HypGeom (n−1,⌊3n1−α⌋ −1,⌈k+ 1/α⌉)≥1 + 1 /α
(9)
where (a) uses the symmetry of the hypergeometric distribution. Using Proposition 15 and the fact 452
thatn−1≥n/2for any n≥1yields 453
(9)≤
e·⌈k+ 1/α⌉
1 + 1 /α·⌊3n1−α⌋
n−11+1/α
≤6e⌈k+ 1/α⌉
1 + 1 /α1+1/α
×n−1−α
=o(1/n),
whenever α >0. 454
C.1 Proof of Lemma 14 455
A key ingredient towards proving Lemma 14 is a useful result about the number of low-degree 456
vertices in an Erd ˝os-Rényi graph, presented next. 457
Proposition 16. Letα >0andG∼ER(n−1, αlog(n)/n). Letrbe a positive integer and let Zr 458
denote the set of vertices in Gwith degree no more than r, i.e. 459
Zr={v∈V(G) :δG(v)≤r}.
For any δsuch that δ >1−α, it is true that 460
P 
|Zr| ≥nδ
=o(1/n).
Proof. Notice that 461
P 
|Zr| ≥nδ
=P
∃S′⊆V:
|S′| ≥nδ	
∩
max
i∈S′δG(i)≤r
≤P
∃S⊆V:
|S|=nδ	
∩
max
i∈SδG(i)≤r
≤P 
∃S⊆V:
|S|=nδ	
∩(X
i∈SδG(i)≤r|S|)!
. (10)
If|S|=nδ, then the sum of degrees of vertices in Sis the total number of edges with exactly 462
end point in S, plus twice the number of edges with both end points in S. There are exactly 463 |S|
2
+|S|(n−1− |S|)≤n1+δsuch vertex pairs, and each of them independently has an edge 464
with probability αlog(n)/n. Therefore, a union bound over all possible choices of Syields 465
(10)≤n−1
nδ
·P 
Bin 
n1+δ, αlog(n)/n
+Bin 
n2δ/2, αlog(n)/n
≤rnδ
≤n−1
nδ
·P 
Bin 
n1+δ, αlog(n)/n
≤rnδ
(a)
≤ne
nδnδ
×exp (r/(αlogn)−1)
(r/(αlogn))r/(αlogn)nδαlog(n)
=o(1/n),
whenever δ >1−αas desired. Note that (a) uses the Binomial concentration inequality (7)and the 466
fact that n−1
k
≤ n
k
≤ ne
kk. 467
15Algorithm 3: Łuczak expansion
require : Graph G, SetU⊆V(G).
1U0←U
2fori= 0,1,2,3,···do
3 ifthere exists u∈V\Uisuch that uhas3or more neighbors in Uithen
4 Ui+1←Ui∪ {u}
5 else
6 return Ui
7 end
8end
Our objective is to show that the k-core of G−vis sufficiently large with probability 1−o(1/n). 468
To that end, consider Algorithm 3 to identify a subset of the k-core, originally proposed by 469
Łuczak [Łuc91]. 470
Note that the forloop eventually terminates - the set V\Uiis empty, for example when i=n 471
for any input set U. The key is to realize that the forloop terminates much faster when the input 472
U=Zk+1, i.e the set of vertices of the input graph Gwhose degree is k+ 1or less. Furthermore, 473
the complement of the set output by the algorithm is contained in the k-core. Formally, 474
Lemma 17. LetUfbe the output of Algorithm 3 with input graph G−vand set U=Zk+1. Then, 475
(a)Uc
f⊆corek(G−v). 476
(b) For any δ >1−α, 477
P 
|Uf|>3nδ
=o(1/n).
Proof. (a) The proof is by construction: Since Ufis obtained by adding exactly fnodes to U0, it 478
follows that Uc
f⊆Uc
0=Zc
k+1, so each node in Uc
fhas degree k+ 2or more in G−v. Further, each 479
node in Uc
fhas at most 2neighbors in Uf, else the forloop would not have terminated. Thus, the 480
subgraph of G−vinduced on the set Uc
fhas minimum degree at least k, and the result follows. 481
(b) If|Uf|>3nδ, then either |U0|>3nδor there is some Min{0,1,···, f}for which |UM|= 3nδ. 482
Therefore, 483
P 
|Uf|>3nδ
≤P 
|U0|>3nδ
+P 
∃M∈ {0,1,···, f}s.t.|UM|= 3nδ
=o(1/n) +P 
∃M∈ {0,1,···, f}s.t.|UM|= 3nδ
| {z }
(⋆),
by Proposition 16. Note that each iteration i= 0,1,···, M−1of the forloop adds exactly 1vertex 484
and at least 3edges to the subgraph of G−vinduced on UM. Therefore, the induced subgraph G|UM485
has3nδvertices and at least 3 (|UM| − |U0|)edges. Thus, 486
(⋆)≤P 
∃subgraph H=(W, F )ofG−vs.t.|W|= 3nδand|F| ≥3 
3nδ− |U0|
≤P 
|U0|> nδ
+P 
∃subgraph H=(W, F )ofG−vs.t.|W|= 3nδand|F| ≥6nδ
≤o(1/n) +n
3nδ
·P
Bin3nδ
2
,αlog(n)
n
>6nδ
| {z }
(⋆⋆),
16where the last step uses Proposition 16 and a union bound over all possible choices of W. Finally, 487
using the relation n
k
≤ ne
kkand the concentration inequality (5) from Lemma 13 yields 488
(⋆⋆)≤n1−δe
33nδ
P
Bin9n2δ
2,αlogn
n
>6nδ
≤(n1−δ)3nδ×3αelogn
4n1−δ6nδ
=3αelogn
4n(1−δ)/26nδ
=o(1/n),
whenever 0< δ < 1. The result follows. 489
Finally, notice that Lemma 17 directly implies Lemma 14. 490
D Proof of Theorem 9 491
Theorem 9. LetG1,···, Gmbe correlated graphs from the subsampling model with parameters C 492
ands. Letv∈Vand let Ube a vertex cut of {1,···, m}such that |U| ≤ ⌊m/2⌋. Then, 493
P
{cv(U) = 0} ∩
ecv(U)>m2
4
k+1
Cs2
=o(1/n). (2)
Proof. For any vertex cut U, 494

ecv(U)>m2
4
k+1
Cs2(a)
⊆
ecv(U)>|U|(m− |U|)
k+1
Cs2
=X
i∈UX
j∈UcδG′
i∧G′
j(v)>|U|(m− |U|)
k+1
Cs2
⊆[
i∈U[
j∈Uc
δG′
i∧G′
j(v)> k+1
Cs2
,
where (a) uses the fact that the maximum of a set of a numbers is greater than or equal to the average. 495
On the other hand 496
{cv(U) = 0}=\
i∈U\
j∈Uc
v /∈corek(G′
i∧G′
j)	
.
Letp1denote the probability in the LHS of (2). It follows from the union bound that 497
p1≤X
i∈UX
j∈UcP
v /∈corek(G′
i∧G′
j)	
∩
δG′
i∧G′
j(v)> k+1
Cs2
=o(1/n),
since for any choice of iandj, the graph G′
i∧G′
j∼ER 
n, Cs2log(n)/n
. 498
E On Stochastic Dominance: Proof of Theorem 10 499
The objective of this section is to build up to a proof of Theorem 10. We start by making a simple 500
observation about products of Binomial random variables. 501
Lemma 18. LetX1,···, Xm∼Bern(s)be i.i.d. random variables, and let B=X1+···+Xm 502
denote their sum. For each ℓin{1,2,···,⌊m/2⌋}, define 503
Tℓ= (X1+···+Xℓ) (Xℓ+1+···+Xm).
For any ℓ1, ℓ2∈ {1,2,···,⌊m/2⌋}such that ℓ1< ℓ2, and for any t∈Rand any b∈ {0,1,···, m}, 504
P(Tℓ1> t|B=b)≤P(Tℓ2> t|B=b). (11)
17Proof of Lemma 18. Consider overlapping but exhaustive cases: 505
Case 1: t <0.Since Tℓ≥0almost surely for all ℓ, the inequality (11) holds. 506
Case 2: t≥b−1.Note that conditioned on B=b, it follows that T1∈ {0, b−1}. Therefore, the 507
left hand side of (11) equals zero, and the inequality holds. 508
Case 3: b= 0orb= 1.In this case, Tℓis identically zero for all ℓ, so (11) holds. 509
Case 4: b≥2and0≤t < b−1.For any ℓ∈ {1,2,···,⌊m/2⌋}, 510
P(Tℓ> t|B=b) =P({(X1+···+Xℓ) (Xℓ+1+···+Xm)> t} ∩ {X1+···+Xm=b})
P(X1+···+Xm=b)
=P
i:i(b−i)>tP({X1+···+Xℓ=i} ∩ {Xℓ+1+···+Xm=b−i})
P(X1+···+Xm=b)
(a)=Pb−1
i=1P(X1+···+Xℓ=i)P(Xℓ+1+···+Xm=b−i)
P(X1+···+Xm=b)
(b)=Pb−1
i=1 ℓ
i m−ℓ
b−i
 m
b
=Pb
i=0 ℓ
i m−ℓ
b−i
− m−ℓ
b
− ℓ
b
 m
b
= m
b
− m−ℓ
b
− ℓ
b
 m
b , (12)
where (a) used the fact that for any tsuch that 0≤t < b−1, it is true that
{i:i(b−i)> t}={1,2,···, b−1}.
Here, the notation for binomial coefficients in (b) involves setting n
k
= 0whenever k <0ork > n . 511
Letfm,b(ℓ)denote the numerator of (12), i.e. 512
fm,b(ℓ) :=m
b
−m−ℓ
b
−ℓ
b
It suffices to show that fm,b(ℓ)−fm,b(ℓ−1)≥0for all ℓ∈ {2,···,⌊m/2⌋}. Indeed, 513
fm,b(ℓ)−fm,b(ℓ−1) =m−ℓ+ 1
b
−m−ℓ
b
−ℓ
b
−ℓ−1
b
(c)=m−ℓ
b−1
−ℓ−1
b−1
≥0,
whenever m−ℓ≥ℓ−1, i.e.ℓ≤ ⌊m/2⌋. Here, (c) uses the identity n
k
= n−1
k−1
+ n−1
k
, and the 514
fact that n1
k
≥ n2
k
whenever n1≥n2. This concludes the proof. 515
Corollary 19. LetFbe a collection of edges in the parent graph G. For any edge er∈F, letXr
i516
denote the indicator random variable G′
i(er)∼Bern(ps). For each ℓin{1,···,⌊m/2⌋}, define 517
Tr
ℓ= (Xr
1+···+Xr
ℓ)(Xr
ℓ+1+···+Xr
m).
Then, for any ℓ1, ℓ2∈ {1,···,⌊m/2⌋}such that ℓ1< ℓ2, the following stochastic ordering holds 518
|F|X
r=1Tr
ℓ1⪯|F|X
r=1Tr
ℓ2.
Proof. It suffices to show that Tr
ℓ1⪯Tr
ℓ2for each r, since the edges are independent. Indeed, we 519
have for any tthat 520
P 
Tr
ℓ1> t
=mX
b=0P(B=b)P 
Tr
ℓ1> t|B=b
≤mX
b=0P(B=b)P 
Tr
ℓ2> t|B=b
=P 
Tr
ℓ2> t
,
which concludes the proof. 521
18With this, we are ready to prove Theorem 10. The theorem is restated for convenience. 522
Theorem 10. LetG1,···, Gmbe correlated graphs from the subsampling model. Let v∈Vand 523
letUℓdenote the set {1,···, ℓ}forℓin{1,···,⌊m/2⌋}. For any vertex cut Uof{1,···, m}, let 524
ecv(U)denote its cost in the graph eH(v). The following stochastic ordering holds: 525
ecv(U1)⪯ecv(U2)⪯ ··· ⪯ ecv(U⌊m/2⌋).
Proof. Letℓ1, ℓ2∈ {1,···,⌊m/2⌋}such that ℓ1< ℓ2. Lett∈R. Consider the parent graph Gand 526
label the set of incident edges on vas{e1,···, eδG(v)}. Denote by Xr
ithe indicator random variable 527
G′
i(er)∼Bern(ps). It follows that 528
P(ecv(Uℓ2)> t) =P
ℓ2X
i=1mX
j=ℓ2+1δG′
i∧G′
j(v)≥t

=P
ℓ2X
i=1mX
j=ℓ2+1δG(v)X
r=1Xr
iXr
j> t

=nX
d=0P(δG(v) =d)P dX
r=1 
(Xr
1+···+Xr
ℓ2)(Xr
ℓ2+1+···+Xr
m)
> t!
(a)
≥nX
d=0P(δG(v) =d)P dX
r=1 
(Xr
1+···+Xr
ℓ1)(Xr
ℓ1+1+···+Xr
m)
> t!
=P
ℓ1X
i=1mX
j=ℓ1+1δG(v)X
r=1Xr
iXr
j> t

=P
ℓ1X
i=1mX
j=ℓ1+1δG′
i∧G′
j(v)≥t

=P(ecv(Uℓ1)> t),
as desired. Here, (a) uses Corollary 19. 529
F On Low Degree Nodes: Proof of Theorem 11 530
Theorem 11. LetG1,···, Gmbe obtained from the subsampling model with parameters Cands. 531
Letr=m2
4 
k+1
Cs2
. Letv∈[n]and suppose that Cs(1−(1−s)m−1)>1. Then, 532
P(ecv(U1)≤r)≤P 
δG1∧G′
2(v)≤r	
∩ ··· ∩
δG1∧G′m(v)≤r	
=o(1/n).
Proof. Consider fixed integers r1,···, rmsuch that 0≤r2,···, rm≤r. Since ris constant, by a 533
union bound argument it suffices to show 534
(⋆) =:P 
δG1∧G′
2(v) =r2	
∩ ··· ∩
δG1∧G′m(v) =rm	
=o(1/n).
Proceed by conditioning on the degree of vinG1, which follows a Bin(n, ps)distribution. Since 535
the degrees of vin the intersection graphs {G1∧G′
i:i= 2,···, m}are conditionally independent 536
given the degree of vinG1, we have 537
(⋆) =ED"
Pm\
i=2
δG1∧G′
i(v) =ri	δG1(v) =D#
=ED"mY
i=2P
δG1∧G′
i(v) =ri	δG1(v) =D#
=ED"mY
i=2D
ri
sri(1−s)D−ri#
. (13)
19Using the fact that D
ri
≤
De
riri
, it follows that 538
(13)≤se
1−sPm
i=2ri
·mY
i=2r−ri
i×EDh
DPm
i=2ri×(1−s)(m−1)Di
≤const.×EDh
DPm
i=2ri×(1−s)(m−1)Di
. (14)
Expanding out the expectation yields 539
(14)=const.×nX
d=0Ld,where Ld:=dPm
i=2ri(1−s)(m−1)d×P(Bin(n, ps) =d).
Proceed by splitting the summation at (logn)2. The first part can be bounded as 540
(logn)2X
d=0Ld≤(logn)2Pm
i=2ri(logn)2X
d=0(1−s)(m−1)d·P(Bin(n, ps) =d)
≤(logn)2Pm
i=2ri·nX
d=0(1−s)(m−1)d·P(Bin(n, ps) =d)
= (log n)2Pm
i=2ri·EDh
(1−s)(m−1)Di
(a)= (log n)2Pm
i=2ri· 
1−Cs 
1−(1−s)m−1
logn
n!n
=o(1/n),
whenever Cs(1−(1−s))m−1>1. Here, (a) is obtained by evaluating the probability generating 541
function of the Bin(n, ps)random variable at (1−s)m−1and setting p=Clog(n)/n. 542
The other part of the sum can now be bounded as follows. 543
nX
d=(log n)2Ld≤
max
d: (log n)2≤d≤ndPm
i=2ri(1−s)md
·P 
Bin(n, ps)≥(logn)2
(b)
≤h
(logn)2Pm
i=2ri(1−s)m(logn)2i
×2−(logn)2
= (log n)2Pm
i=2ri(1−s)m
2(logn)2
=o(1/n).
whenever C > 0. Here, (b) is true because the function d7→dPri(a−s)mdis decreasing on the 544
interval [(logn)2, n]for all sufficiently large n. Finally, the concentration inequality for the Binomial 545
distribution holds by (6)in Lemma 13. The inequality applies since p=Clog(n)/nand since 546
(logn)2>6Cslog(n)for all nsufficiently large. This concludes the proof. 547
20NeurIPS Paper Checklist 548
1.Claims 549
Question: Do the main claims made in the abstract and introduction accurately reflect the 550
paper’s contributions and scope? 551
Answer: [Yes] 552
Justification: All claims made in the abstract and introduction are formally stated in Section 3 553
and proved in Section 4. 554
Guidelines: 555
•The answer NA means that the abstract and introduction do not include the claims 556
made in the paper. 557
•The abstract and/or introduction should clearly state the claims made, including the 558
contributions made in the paper and important assumptions and limitations. A No or 559
NA answer to this question will not be perceived well by the reviewers. 560
•The claims made should match theoretical and experimental results, and reflect how 561
much the results can be expected to generalize to other settings. 562
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 563
are not attained by the paper. 564
2.Limitations 565
Question: Does the paper discuss the limitations of the work performed by the authors? 566
Answer: [Yes] 567
Justification: Section 5 explicitly mentions and discusses limitations relating to the runtime 568
of our algorithms. 569
Guidelines: 570
•The answer NA means that the paper has no limitation while the answer No means that 571
the paper has limitations, but those are not discussed in the paper. 572
• The authors are encouraged to create a separate "Limitations" section in their paper. 573
•The paper should point out any strong assumptions and how robust the results are to 574
violations of these assumptions (e.g., independence assumptions, noiseless settings, 575
model well-specification, asymptotic approximations only holding locally). The authors 576
should reflect on how these assumptions might be violated in practice and what the 577
implications would be. 578
•The authors should reflect on the scope of the claims made, e.g., if the approach was 579
only tested on a few datasets or with a few runs. In general, empirical results often 580
depend on implicit assumptions, which should be articulated. 581
•The authors should reflect on the factors that influence the performance of the approach. 582
For example, a facial recognition algorithm may perform poorly when image resolution 583
is low or images are taken in low lighting. Or a speech-to-text system might not be 584
used reliably to provide closed captions for online lectures because it fails to handle 585
technical jargon. 586
•The authors should discuss the computational efficiency of the proposed algorithms 587
and how they scale with dataset size. 588
•If applicable, the authors should discuss possible limitations of their approach to 589
address problems of privacy and fairness. 590
•While the authors might fear that complete honesty about limitations might be used by 591
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 592
limitations that aren’t acknowledged in the paper. The authors should use their best 593
judgment and recognize that individual actions in favor of transparency play an impor- 594
tant role in developing norms that preserve the integrity of the community. Reviewers 595
will be specifically instructed to not penalize honesty concerning limitations. 596
3.Theory Assumptions and Proofs 597
Question: For each theoretical result, does the paper provide the full set of assumptions and 598
a complete (and correct) proof? 599
21Answer: [Yes] 600
Justification: All proof outlines and intuition is presented in the main body of the paper. 601
Some formal proofs are deferred to the Supplementary Material in view of space constraints. 602
Guidelines: 603
• The answer NA means that the paper does not include theoretical results. 604
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 605
referenced. 606
•All assumptions should be clearly stated or referenced in the statement of any theorems. 607
•The proofs can either appear in the main paper or the supplemental material, but if 608
they appear in the supplemental material, the authors are encouraged to provide a short 609
proof sketch to provide intuition. 610
•Inversely, any informal proof provided in the core of the paper should be complemented 611
by formal proofs provided in appendix or supplemental material. 612
• Theorems and Lemmas that the proof relies upon should be properly referenced. 613
4.Experimental Result Reproducibility 614
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 615
perimental results of the paper to the extent that it affects the main claims and/or conclusions 616
of the paper (regardless of whether the code and data are provided or not)? 617
Answer: [Yes] 618
Justification: The pseudocode presented in Algorithm 2 is implementable using basic Python 619
libraries, and existing subroutines for pairwise matching. URLs to these subroutines are 620
referenced in the main body of the paper. 621
Guidelines: 622
• The answer NA means that the paper does not include experiments. 623
•If the paper includes experiments, a No answer to this question will not be perceived 624
well by the reviewers: Making the paper reproducible is important, regardless of 625
whether the code and data are provided or not. 626
•If the contribution is a dataset and/or model, the authors should describe the steps taken 627
to make their results reproducible or verifiable. 628
•Depending on the contribution, reproducibility can be accomplished in various ways. 629
For example, if the contribution is a novel architecture, describing the architecture fully 630
might suffice, or if the contribution is a specific model and empirical evaluation, it may 631
be necessary to either make it possible for others to replicate the model with the same 632
dataset, or provide access to the model. In general. releasing code and data is often 633
one good way to accomplish this, but reproducibility can also be provided via detailed 634
instructions for how to replicate the results, access to a hosted model (e.g., in the case 635
of a large language model), releasing of a model checkpoint, or other means that are 636
appropriate to the research performed. 637
•While NeurIPS does not require releasing code, the conference does require all submis- 638
sions to provide some reasonable avenue for reproducibility, which may depend on the 639
nature of the contribution. For example 640
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 641
to reproduce that algorithm. 642
(b)If the contribution is primarily a new model architecture, the paper should describe 643
the architecture clearly and fully. 644
(c)If the contribution is a new model (e.g., a large language model), then there should 645
either be a way to access this model for reproducing the results or a way to reproduce 646
the model (e.g., with an open-source dataset or instructions for how to construct 647
the dataset). 648
(d)We recognize that reproducibility may be tricky in some cases, in which case 649
authors are welcome to describe the particular way they provide for reproducibility. 650
In the case of closed-source models, it may be that access to the model is limited in 651
some way (e.g., to registered users), but it should be possible for other researchers 652
to have some path to reproducing or verifying the results. 653
225.Open access to data and code 654
Question: Does the paper provide open access to the data and code, with sufficient instruc- 655
tions to faithfully reproduce the main experimental results, as described in supplemental 656
material? 657
Answer: [NA] 658
Justification: The simulations do not involve any data, since all simulations are done for 659
random (Erd ˝os-Rényi) graphs. 660
Guidelines: 661
• The answer NA means that paper does not include experiments requiring code. 662
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 663
public/guides/CodeSubmissionPolicy ) for more details. 664
•While we encourage the release of code and data, we understand that this might not be 665
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 666
including code, unless this is central to the contribution (e.g., for a new open-source 667
benchmark). 668
•The instructions should contain the exact command and environment needed to run to 669
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 670
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 671
•The authors should provide instructions on data access and preparation, including how 672
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 673
•The authors should provide scripts to reproduce all experimental results for the new 674
proposed method and baselines. If only a subset of experiments are reproducible, they 675
should state which ones are omitted from the script and why. 676
•At submission time, to preserve anonymity, the authors should release anonymized 677
versions (if applicable). 678
•Providing as much information as possible in supplemental material (appended to the 679
paper) is recommended, but including URLs to data and code is permitted. 680
6.Experimental Setting/Details 681
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 682
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 683
results? 684
Answer: [NA] 685
Justification: No experiments involving any training were done in this work. 686
Guidelines: 687
• The answer NA means that the paper does not include experiments. 688
•The experimental setting should be presented in the core of the paper to a level of detail 689
that is necessary to appreciate the results and make sense of them. 690
•The full details can be provided either with the code, in appendix, or as supplemental 691
material. 692
7.Experiment Statistical Significance 693
Question: Does the paper report error bars suitably and correctly defined or other appropriate 694
information about the statistical significance of the experiments? 695
Answer: [Yes] 696
Justification: All plots include error bars. 697
Guidelines: 698
• The answer NA means that the paper does not include experiments. 699
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 700
dence intervals, or statistical significance tests, at least for the experiments that support 701
the main claims of the paper. 702
•The factors of variability that the error bars are capturing should be clearly stated (for 703
example, train/test split, initialization, random drawing of some parameter, or overall 704
run with given experimental conditions). 705
23•The method for calculating the error bars should be explained (closed form formula, 706
call to a library function, bootstrap, etc.) 707
• The assumptions made should be given (e.g., Normally distributed errors). 708
•It should be clear whether the error bar is the standard deviation or the standard error 709
of the mean. 710
•It is OK to report 1-sigma error bars, but one should state it. The authors should 711
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 712
of Normality of errors is not verified. 713
•For asymmetric distributions, the authors should be careful not to show in tables or 714
figures symmetric error bars that would yield results that are out of range (e.g. negative 715
error rates). 716
•If error bars are reported in tables or plots, The authors should explain in the text how 717
they were calculated and reference the corresponding figures or tables in the text. 718
8.Experiments Compute Resources 719
Question: For each experiment, does the paper provide sufficient information on the com- 720
puter resources (type of compute workers, memory, time of execution) needed to reproduce 721
the experiments? 722
Answer: [NA] 723
Justification: The simulations are classical Monte-Carlo simulations that do not require 724
extensive runtime or hardware. 725
Guidelines: 726
• The answer NA means that the paper does not include experiments. 727
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 728
or cloud provider, including relevant memory and storage. 729
•The paper should provide the amount of compute required for each of the individual 730
experimental runs as well as estimate the total compute. 731
•The paper should disclose whether the full research project required more compute 732
than the experiments reported in the paper (e.g., preliminary or failed experiments that 733
didn’t make it into the paper). 734
9.Code Of Ethics 735
Question: Does the research conducted in the paper conform, in every respect, with the 736
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 737
Answer: [Yes] 738
Justification: The Code of Ethics was strictly adhered to during all stages of this research. 739
Guidelines: 740
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 741
•If the authors answer No, they should explain the special circumstances that require a 742
deviation from the Code of Ethics. 743
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 744
eration due to laws or regulations in their jurisdiction). 745
10.Broader Impacts 746
Question: Does the paper discuss both potential positive societal impacts and negative 747
societal impacts of the work performed? 748
Answer: [Yes] 749
Justification: The potential for graph matching through transitive closure is motivated 750
through its application to social network de-anonymization. 751
Guidelines: 752
• The answer NA means that there is no societal impact of the work performed. 753
•If the authors answer NA or No, they should explain why their work has no societal 754
impact or why the paper does not address societal impact. 755
24•Examples of negative societal impacts include potential malicious or unintended uses 756
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 757
(e.g., deployment of technologies that could make decisions that unfairly impact specific 758
groups), privacy considerations, and security considerations. 759
•The conference expects that many papers will be foundational research and not tied 760
to particular applications, let alone deployments. However, if there is a direct path to 761
any negative applications, the authors should point it out. For example, it is legitimate 762
to point out that an improvement in the quality of generative models could be used to 763
generate deepfakes for disinformation. On the other hand, it is not needed to point out 764
that a generic algorithm for optimizing neural networks could enable people to train 765
models that generate Deepfakes faster. 766
•The authors should consider possible harms that could arise when the technology is 767
being used as intended and functioning correctly, harms that could arise when the 768
technology is being used as intended but gives incorrect results, and harms following 769
from (intentional or unintentional) misuse of the technology. 770
•If there are negative societal impacts, the authors could also discuss possible mitigation 771
strategies (e.g., gated release of models, providing defenses in addition to attacks, 772
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 773
feedback over time, improving the efficiency and accessibility of ML). 774
11.Safeguards 775
Question: Does the paper describe safeguards that have been put in place for responsible 776
release of data or models that have a high risk for misuse (e.g., pretrained language models, 777
image generators, or scraped datasets)? 778
Answer: [NA] 779
Justification: The paper poses no such risks. 780
Guidelines: 781
• The answer NA means that the paper poses no such risks. 782
•Released models that have a high risk for misuse or dual-use should be released with 783
necessary safeguards to allow for controlled use of the model, for example by requiring 784
that users adhere to usage guidelines or restrictions to access the model or implementing 785
safety filters. 786
•Datasets that have been scraped from the Internet could pose safety risks. The authors 787
should describe how they avoided releasing unsafe images. 788
•We recognize that providing effective safeguards is challenging, and many papers do 789
not require this, but we encourage authors to take this into account and make a best 790
faith effort. 791
12.Licenses for existing assets 792
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 793
the paper, properly credited and are the license and terms of use explicitly mentioned and 794
properly respected? 795
Answer: [Yes] 796
Justification: The subroutines for GRAMPA andDegree Profiles have been cited. 797
Guidelines: 798
• The answer NA means that the paper does not use existing assets. 799
• The authors should cite the original paper that produced the code package or dataset. 800
•The authors should state which version of the asset is used and, if possible, include a 801
URL. 802
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 803
•For scraped data from a particular source (e.g., website), the copyright and terms of 804
service of that source should be provided. 805
•If assets are released, the license, copyright information, and terms of use in the 806
package should be provided. For popular datasets, paperswithcode.com/datasets 807
has curated licenses for some datasets. Their licensing guide can help determine the 808
license of a dataset. 809
25•For existing datasets that are re-packaged, both the original license and the license of 810
the derived asset (if it has changed) should be provided. 811
•If this information is not available online, the authors are encouraged to reach out to 812
the asset’s creators. 813
13.New Assets 814
Question: Are new assets introduced in the paper well documented and is the documentation 815
provided alongside the assets? 816
Answer: [NA] 817
Justification: The paper does not release new assets. 818
Guidelines: 819
• The answer NA means that the paper does not release new assets. 820
•Researchers should communicate the details of the dataset/code/model as part of their 821
submissions via structured templates. This includes details about training, license, 822
limitations, etc. 823
•The paper should discuss whether and how consent was obtained from people whose 824
asset is used. 825
•At submission time, remember to anonymize your assets (if applicable). You can either 826
create an anonymized URL or include an anonymized zip file. 827
14.Crowdsourcing and Research with Human Subjects 828
Question: For crowdsourcing experiments and research with human subjects, does the paper 829
include the full text of instructions given to participants and screenshots, if applicable, as 830
well as details about compensation (if any)? 831
Answer: [NA] 832
Justification: The paper does not involve crowdsourcing or research with human subjects. 833
Guidelines: 834
•The answer NA means that the paper does not involve crowdsourcing nor research with 835
human subjects. 836
•Including this information in the supplemental material is fine, but if the main contribu- 837
tion of the paper involves human subjects, then as much detail as possible should be 838
included in the main paper. 839
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 840
or other labor should be paid at least the minimum wage in the country of the data 841
collector. 842
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 843
Subjects 844
Question: Does the paper describe potential risks incurred by study participants, whether 845
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 846
approvals (or an equivalent approval/review based on the requirements of your country or 847
institution) were obtained? 848
Answer: [NA] 849
Justification: The paper does not involve crowdsourcing nor research with human subjects. 850
Guidelines: 851
•The answer NA means that the paper does not involve crowdsourcing nor research with 852
human subjects. 853
•Depending on the country in which research is conducted, IRB approval (or equivalent) 854
may be required for any human subjects research. If you obtained IRB approval, you 855
should clearly state this in the paper. 856
•We recognize that the procedures for this may vary significantly between institutions 857
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 858
guidelines for their institution. 859
•For initial submissions, do not include any information that would break anonymity (if 860
applicable), such as the institution conducting the review. 861
26