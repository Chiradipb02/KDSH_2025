Diffeomorphic interpolation for efficient
persistence-based topological optimization
Mathieu Carrière
DataShape
Centre Inria d’Université Côte d’Azur
Sophia Antipolis, France
mathieu.carriere@inria.frMarc Theveneau∗
Shape Analysis Group
Computer Science department, McGill
Montréal, Quebec, Canada
marc.theveneau@mail.mcgill.ca
Théo Lacombe
Laboratoire d’Informatique Gaspard Monge,
Univ. Gustave Eiffel, CNRS, LIGM, F-77454
Marne-la-Vallée, France
theo.lacombe@univ-eiffel.fr
Abstract
Topological Data Analysis (TDA) provides a pipeline to extract quantitative topo-
logical descriptors from structured objects. This enables the definition of topo-
logical loss functions, which assert to what extent a given object exhibits some
topological properties. These losses can then be used to perform topological op-
timization via gradient descent routines. While theoretically sounded, topological
optimization faces an important challenge: gradients tend to be extremely sparse,
in the sense that the loss function typically depends on only very few coordinates
of the input object, yielding dramatically slow optimization schemes in practice.
Focusing on the central case of topological optimization for point clouds, we pro-
pose in this work to overcome this limitation using diffeomorphic interpolation ,
turning sparse gradients into smooth vector fields defined on the whole space,
with quantifiable Lipschitz constants. In particular, we show that our approach
combines efficiently with subsampling techniques routinely used in TDA, as the
diffeomorphism derived from the gradient computed on a subsample can be used
to update the coordinates of the full input object, allowing us to perform topo-
logical optimization on point clouds at an unprecedented scale. Finally, we also
showcase the relevance of our approach for black-box autoencoder (AE) regular-
ization, where we aim at enforcing topological priors on the latent spaces associ-
ated to fixed, pre-trained, black-box AE models, and where we show that learning
a diffeomorphic flow can be done once and then re-applied to new data in linear
time (while vanilla topological optimization has to be re-run from scratch). More-
over, reverting the flow allows us to generate data by sampling the topologically-
optimized latent space directly, yielding better interpretability of the model.
1 Introduction
Persistent homology (PH) is a central tool of Topological Data Analysis (TDA) that enables the
extraction of quantitative topological information (such as, e.g., the number and sizes of loops, con-
nected components, branches, cavities, etc) about structured objects (such as graphs, times series or
∗Part of this work was done when MT was doing an internship at the Laboratoire d’Informatique Gaspard
Monge and student at Université Paris-Saclay.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).point clouds sampled from, e.g., submanifolds), summarized in compact descriptors called persis-
tence diagrams (PDs). PDs were initially used as features in Machine Learning (ML) pipelines; due
to their strong invariance and stability properties, they have been proved to be powerful descriptors
in the context of classification of time series [42, 19], graphs [5, 23, 24], images [2, 25, 16], shape
registration [7, 6, 36], or analysis of neural networks [22, 3, 26], to name a few.
Another active line or research at the crossroad of TDA and ML is (persistence-based) topological
optimization , where one wants to modify an object Xso that it satisfies some topological constraints
as reflected in its persistence diagram Dgm( X). The first occurrence of this idea appears in [21],
where one wants to deform a point cloud X∈Rn×dso that Dgm( X)becomes as close as possible
(w.r.t. an appropriate metric denoted by W) to some target diagram Dtarget , hence yielding to the
problem of minimizing X7→W(Dgm( X), Dtarget). This idea has then been revisited with different
flavors, for instance by adding topology-based terms in standard losses in order to regularize ML
models [14, 33, 29], improving ML model reconstructions by explicitly accounting for topological
features [16], or improving correspondences between 3D shapes by forcing matched regions to have
similar topology [36]. Formally, this goes through the minimization of an objective function
L:X7→ℓ(Dgm( X))∈R,
where ℓis a user-chosen loss function that quantifies to what extent Dgm( X)reflects some pre-
scribed topological properties inferred from X. Under mild assumptions (see Section 2), the map
Lis differentiable generically and its gradients are obtained as a byproduct of the computation of
Dgm( X). However, these approaches are limited in practice by two major issues: (i)the com-
putation of X7→Dgm( X)scales poorly with the size of X(e.g., number of points nin a point
cloud, number of nodes in a graph, etc), and (ii)the gradient ∇L(X)tends to be very sparse : if
X= (x1, . . . , x n)∈Rn×dis a point cloud, ∇L(X)i̸= 0for only very few indices i∈ {1, . . . , n }
(the corresponding points are called the critical points of the topological gradient, see Section 2.1).
Related works. Several articles have studied topological optimization in the TDA literature. The
standard, or vanilla , framework to define and study gradients obtained from topological losses was
described in [4, 28], where the high sparsity and long computation times were first identified. To
mitigate this issue, the authors of [34] introduced the notion of critical set that extends the usually
sparse set of critical points in order to get a gradient-like object that would update more points in X.
In [39], the authors used an average of the vanilla topological gradients of several subsamples to get
a denser and faster gradient. On the theoretical side, the authors of [27] demonstrated that adapting
the stratified structure induced by PDs to the gradient definition enables faster convergence.
Limitations. Despite proposing interesting ways to accelerate gradient descent, the approaches
mentioned above are still limited in the sense that their proposed gradients are not defined on the
whole space, but only on a sparse subset of the current observation X, which still prevents their
use in different contexts, that we investigate in our experiments (Section 4). First, when the data
has more than tens of thousands of points, the number of subsamples needed to capture relevant
topological structures (when using [39]), as well as the critical set computations (when using [34]),
both become practically infeasible . Second, when optimizing the topology of datasets obtained as
latent spaces of a black-box autoencoder model (i.e., an autoencoder with forbidden access to its
architecture, parameters, and training), then (a)the topological gradients of [39, 34] cannot be re-
used to process new such datasets, and topological optimization has to be performed from scratch
every time that new data comes in, (b)this also impedes their transferability , as re-running gradient
descent every time makes it very difficult to guarantee some stability for the final output, and finally
(c)onecannot generate new data by sampling the optimized latent spaces directly, as it would
require to apply the sequence of reverted gradients (which are not well-defined everywhere).
Contributions and Outline. In this article, we propose to replace the standard gradient ∇L(X)
of (??) derived formally by a diffeomorphism v:Rd→Rdthatinterpolates ∇L(X)on its non-zero
entries, that is v(xi) =∇L(X)ifor all i∈I:={j|∇L(X)j̸= 0}and is, in some sense, as smooth
as possible. More precisely, our contribution is three-fold:
• We introduce a diffeomorphic interpolation of the vanilla topological gradient, which ex-
tends this gradient to a smooth and denser vector field defined on the whole space Rd, and
which is able to move a lot more points in Xat each iteration,
2t= 0
t= 1
t= 2
t= 3
t= 5
births
deaths
t
t
0
1
2
2
3
5Figure 1: Illustration of the Vietoris-Rips filtration on a point cloud in Rd, focusing on one-dimensional topo-
logical features (loops). When the filtration parameter tincreases, loops appear and disappear in the filtration.
These values are accounted in the resulting persistence diagram (right).
• We prove that its updates indeed decrease topological losses, and we quantify its smooth-
ness by upper bounding its Lipschitz constant (again, in the context of topological losses),
• We showcase its practical efficiency: we show that it compares favorably to the main base-
line [34] in terms of convergence speed, that its combination with subsampling [39] allows
to process datasets whose sizes are currently out of reach in TDA, and that it can success-
fully be used for the tasks mentioned above concerning black-box autoencoder models.
Section 2 provides necessary background in Topological Data Analysis and diffeomorphic interpo-
lations. Section 3 presents our approach and its corresponding guarantees, and Section 4 showcases
our experiments. Limitations and further research directions are discussed in Section 5.
2 Background
2.1 Topological Data Analysis
In this section, we recall the basic materials of Topological Data Analysis (TDA), and refer the
interested reader to [20, 35] for a thorough overview. We restrict the presentation to our case of
interest: extracting topological information from a point cloud using the standard Vietoris-Rips (VR)
filtration. A more extensive presentation of the TDA machinery is provided in Appendix A.
LetX= (x1, . . . , x n)∈Rn×d. The Vietoris-Rips filtration consists of building an increasing
sequence of simplicial complexes (Kt)t≥0overXby inserting a simplex σ= (xi1, . . . , x ip)when-
ever∀j, j′∈ {1, . . . , p },∥xij−xij′∥ ≤t. Each time a simplex σis inserted, it either creates a
topological feature (e.g., inserting a face creates a cavity, that is a 2-dimensional topological feature)
or destroy a pre-existing feature (e.g., the face insertion fills a loop, that is a 1-dimensional feature,
making it topologically trivial). The persistent homology machinary tracks the apparition and de-
struction of such features in the so-called persistence diagram (PD) of X, denoted by Dgm( X).
Therefore, Dgm( X)is a set of points in R2of the form (tb, td)withtd≥tb, where each such point
accounts for the presence of a topological feature inferred from Xthat appeared at time tbfollowing
the insertion of an edge (xi1, xi2)with∥xi1−xi2∥=tband disappeared at time tdfollowing the
insertion of an edge (xi3, xi4)with∥xi3−xi4∥=td. Figure 1 illustrates this construction. From
a computational standpoint, computing the VR diagram of X∈Rn×dthat would reflect topolog-
ical features of dimension d′≤druns in O(nd′+2), making the computation quickly unpractical
when nincreases, even when restricting to low-dimensional features such as connected components
(d′= 0), loops ( d′= 1) or cavities ( d′= 2).
Topological optimization. PDs are made to be used in downstream pipelines, either as static
features (e.g., for classification purposes) or as intermediate representations of Xin optimization
schemes. In this work, we focus on the second problem. We formally consider the minimization of
objective functions of the form
L:X∈Rn×d7→ℓ(Dgm( X))∈R. (1)
Here, ℓrepresents a loss function taking value from the space of PDs, denoted by Din the following.
The space Dcan be equipped with a canonical metric, denoted by Wand whose formal definition
is not required in this work, for which a central result is that the map X7→Dgm( X)is stable
(Lipschitz continuous) [17, 18, 38]. Therefore, if ℓis Lipschitz continuous, so is L, hence it admits
a gradient almost everywhere by Rademacher theorem. Building on these theoretical statements, one
3can consider the “vanilla” gradient descent update Xk+1:=Xk−λ∇L(Xk)for a given learning-
rateλ >0and iterate it in order to minimize (1). Theoretical properties of this seminal scheme (and
natural extensions, e.g., stochastic gradient descent) have been studied in [4, 27], where convergence
(to a local minimum of L) is proved.
From a computational perspective, deriving ∇L(X)comes in two steps. Let µ:= Dgm( X), written
asµ={(bi, di)|i∈I}for some finite set of indices I. To each i∈Icorrespond four (possibly
coinciding) points xi1, xi2, xi3, xi4in the input point cloud X. Intuitively, minimizing µ7→ℓ(µ)
boils down to prescribe a descent direction (δbi, δdi)∈R2to each (bi, di)fori∈I, where δbi=
∂ℓ
∂bi(µ)andδdi=∂ℓ
∂di(µ). Backpropagating this perturbation to Xwill move the corresponding
points xi1, xi2, xi3, xi4in order to increase or decrease the distances ∥xi1−xi2∥=biand∥xi3−
xi4∥=diaccordingly. This yields to the following formula:
∂L
∂x(X) =X
i, x→(bi,di)∂ℓ
∂bi·∂bi
∂x+∂ℓ
∂di·∂di
∂x
(X), (2)
where the notation x→(bi, di)means that x∈Xappears in (at least) one of the four points yielding
the presence of (bi, di)in the diagram µ= Dgm( X). A fundamental contribution of [28, §3.3] is
to prove that the chain rule formula (2) is indeed valid2. Most of the time, a point x∈Xwill not
belong to any critical pair (σb, σd)and the above gradient coordinate is 0. Therefore, the gradient of
Ldepends on very few points of X, yielding the sparsity phenomenon discussed in Section 1.
Examples of common topological losses. LetX= (x1, . . . , x n)∈Rn×dbe a point cloud and
Dgm( X) ={(bi, di)|i∈I}be its PD. There are several natural losses that have been introduced
in the TDA literature:
• Topological simplification losses: typically of the formP
i∈˜I(bi−di)2, where ˜I⊆I.
Such losses push (some of the) points in Dgm( X)toward the diagonal ∆ ={b=d},
hence destroying the corresponding topological features appearing in X.
• Topological augmentation losses [4]: similar to simplification losses, but typically attempt-
ing to push points in Dgm( X)away from ∆, i.e., minimizing −P
i∈˜I(bi−di)2, to make
topological features of Xmore salient. As such losses are not coercive, they are usually
coupled with regularization terms to prevent points in Xgoing to infinity.
• Topological registration losses [21]: given a target diagram Dtarget , one minimizes
W(Dgm( X), Dtarget)where Wdenotes a standard metric between persistence diagrams.
This loss attempts to modify Xso that it exhibits a prescribed topological structure.
2.2 Diffeomorphic interpolations
In order to overcome the sparsity of gradients appearing in TDA, we rely on diffeomorphic interpo-
lations (see, e.g., [43, Chapter 8]). Let X= (x1, . . . , x n)∈Rn×d, letI⊆ {1, . . . , n }denote the set
of indices on which ∇L(X)is non-zero and let ai:= (∇L(X))i∈Rdfori∈I. Our goal is to find a
smooth vector field ˜v:Rd→Rdsuch that, for all i∈I,˜v(xi) =ai. To formalize this, we consider
a Hilbert space H⊂(Rd)Rdfor which the map δα
x:f7→ ⟨α, f(x)⟩Rd=αTf(x)is continuous for
any(α, x)∈Rd×Rd. Such a space is called a Reproducing Kernel Hilbert Space (RKHS)3. A cru-
cial property is that there exists a matrix-valued kernel operator K:Rd×Rd→Rd×dwhose outputs
are symmetric and positive definite, and related to Hthrough the relation ⟨kα
x, kβ
y⟩H=αTK(x, y)β
for all x, y, α, β ∈Rd, where kα
x∈His the unique vector provided by the Riesz representation
theorem such that ⟨kα
x, f⟩=⟨α, f(x)⟩. Conversely, any such kernel Kinduces a (unique) RKHS
H(of which Kis the reproducing kernel). Now, we can consider the following problem:
minimize ∥v∥H,s.t.v(xi) =ai,∀i∈I, (3)
that is, we are seeking for the smoothest (lowest norm) element of Hthat solves our interpolation
problem. The solution ˜vof this problem is the projection of 0onto the affine set {v∈H|v(xi) =
2This is not trivial, because the intermediate space Dis only a metric space.
3In many applications, RKHS are restricted to spaces of functions valued in RorC, but the theory adapts
faithfully to the more general setting of vector-valued maps.
4ai,∀i∈I}. Observe that ˜vbelongs to the orthogonal of {v∈H|v(xi) = 0 ,∀i∈I}, and thus of
{v∈H| ⟨kαixi, v⟩H= 0,∀i∈I, αi∈Rd}, and therefore ˜v∈span({kαixi|i∈I}). This justifies to
search for ˜vin the form of ˜v(x) =P
i∈IK(x, xi)αi, and the interpolation that it must satisfy yields
˜v(x) =P
i∈IK(x, xi)(K−1a)i,where Kis the block matrix (K(xi, xj))i,j∈Ianda= (ai)i∈I.
See also [43, Theorem 8.8]. In particular, it is important to note that ˜vinherits from the regularity
ofKand will typically be a diffeomorphism in this work. If Kis the Gaussian kernel defined by
K(x, y):= exp
−∥x−y∥2
2σ2
Idfor some bandwidth σ >0, a choice to which we stick to in the rest
of this work, the expression of ˜vreduces to
˜v(x) =X
i∈Iρσ(∥x−xi∥)αi, (4)
where ρσ(u):=e−u2
2σ2, and αi:= (K−1a)iwithK= (ρσ(∥xi−xj∥)Id)i,j∈I. Note that ˜v
can be understood as the convolution of awith a Gaussian kernel, but involving a correction K−1
guaranteeing that after the convolution, the interpolation constraint is satisfied. We will call ˜vthe
diffeomorphic interpolation associated to the vectors and indices {ai|i∈I}.
3 Diffeomorphic interpolation of the vanilla topological gradient
3.1 Methodology
1.0
 0.5
 0.0 0.5 1.01.0
0.5
0.00.51.0Point cloud
L(X) (Vanilla)
v(X) (Diffeo)
Figure 2: (blue) A point cloud
X, and (black) the negative gradi-
ent−∇L(X)of a simplification loss
which aims at destroying the loop by
collapsing the circle (reduce the loop’s
death time) and tearing it (increase the
birth time). While ∇L(X)only affects
four points in X, the diffeomorphic in-
terpolation ˜v(X)(orange, σ= 0.1) is
defined on Rd, hence extends smoothly
to other points in X.We aim at minimizing a loss function L:X7→ℓ(Dgm( X))as
in (1), starting from some initialization X0, and assuming that
Lis lower bounded (typically by 0) and locally semi-convex.
This assumption is typically satisfied by the topological losses
ℓintroduced in Section 2.1. Gradient descents implemented in
practice are (explicit) discretization of the gradient flow
dX
dt∈ −∂L(X(t)), X (0) = X0, (5)
where ∂L(X):={v|L(Y)≥L(X) +v·(Y−X) +o(Y−
X)for all X, Y}denotes the subdifferential of LatX. Note
that a topological loss Lis typically notdifferentiable every-
where, since the map X7→Dgm( X)is differentiable almost
everywhere but not in C1,1. However, uniqueness of the gra-
dient flow on a maximal interval [0,+∞[is guaranteed if Lis
lower bounded and locally semi-convex [15, §B.1].
In this work, we propose to use the dynamic described by the
diffeomorphism ˜vtintroduced in (4) interpolating the current
vanilla topological gradient ∇L(Xt)at each time t, formally
considering solutions ˜Xof
d˜X
dt=−˜vt(˜X(t)),˜X(0) = X0. (6)
Here, slightly overloading notation, ˜vt(˜X(t))denotes the n×d
matrix where the i-th line is given by ˜vt(˜X(t)i). The flow at time Tassociated to (6) is the map
φT:x07→x0−ZT
0˜vt(x(t))dt,˙x(t) =−˜vt(x(t)), x(0) = x0, (7)
which can inverted by simply following the flow backward (i.e., by following ˜vtinstead of −˜vt). We
now guarantee that at each time t, following ˜vtinstead of the vanilla topological gradient ∇L(Xt)
still provides a descent direction for the topological loss L.
Proposition 3.1. For each t≥0, it holds thatdL(˜X(t))
dt=−∥∇L(˜X(t))∥2≤0.
Proof. One hasdL(˜X(t))
dt=−⟨∇L(˜X(t)),˜vt(˜X(t))⟩=−Pn
i=1(∇L(˜X(t)))i·(˜vt(˜X(t)))i. Since
∇L(˜X(t))i= 0fori̸∈I, and ˜vt(˜X(t))i=−∇L(˜X(t))ifori∈I, the result follows.
5Moreover, it is also possible to upper bound the smoothness, i.e., the Lipschitz constant, of ˜v:
Proposition 3.2. LetLbe the simplification or augmentation loss computed with k=|˜I|PD points,
as defined at the end of Section 2.1. Let ˜v= ˜vtbe the diffeomorphic interpolation associated to the
vanilla topological gradient at time t≥0. Then, one has, ∀x, y∈Rdandt≥0:
∥˜v(x)−˜v(y)∥2≤ ∥˜v(x)−˜v(y)∥1≤Cd·σd−1·κ(K)·Pers k(Dgm( ˜X(t)))· ∥x−y∥2,
where Cd=√
d·23+d+1
2·πd−1
2,κ(K)is the condition number of K, and Pers k(Dgm( ˜X(t)))is
the sum of the klargest distances to the diagonal in Dgm( ˜X(t)).
The proof is deferred to Appendix B. This upper bound can be used to quantify how smooth the
diffeomorphic interpolation is (as characterized with its Lipschitz constant) based on the parameters
it is computed from. In the case of the Gaussian kernel, we found that our upper bound on the Lip-
schitz constant depends on the kernel bandwidth σ: indeed, the more spread the Gaussian function
is, the more critical points can influence other data points potentially far from them, introducing un-
wanted distortions and larger Lipschitz constant. Similarly, if the condition number κ(K)is large,
inverting the kernel matrix might introduce instabilities in Equation (4), and thus a larger Lipschitz
constant as well. Finally, the total persistence also appears, as the more PD points one has to opti-
mize, the more critical pairs will appear, and thus the more constrained Equation (3) is, leading to
more complex diffeomorphic interpolation solutions with larger Lipschitz constants.
3.2 Subsampling techniques to scale topological optimization
As a consequence of the limited scaling of the Vietoris-Rips filtration with respect to the number of
points nof the input point cloud X, it often happens in practical applications that computing the
VR diagram Dgm( X)of a large point set X(a fortiori its gradient) turns out to be intractable. A
natural workaround is to randomly sample s-points from X, with s≪n, yielding a smaller point
cloud X′⊂X. Provided that the Hausdorff distance between X′andXis small, the stability
theorem [18, 17, 8] ensures that Dgm( X′)is close to Dgm( X). See [11, 12, 9] for an overview of
subsampling methods in TDA.
However, the sparsity of vanilla topological gradients computed from topological losses strikes fur-
ther when relying on subsampling: only a tiny fraction of the seminal point cloud Xis likely to be
updated at each gradient step. In contrast, using the diffeomorphic interpolation ˜v(of the vanilla
topological gradient) computed on the subsample X′still provides a vector field defined on the
whole input space Rd, in particular on each point of Xand the update can then be performed in
linear time with respect to n. This yields Algorithm 1. Figure 3 illustrates the qualitative benefits
offered by the joint use of subsampling and diffeomorphic interpolations when compared to vanilla
topological gradients. A larger-scale experiment is provided in Section 4.
Algorithm 1 Diffeomorphic gradient descent for topological loss functions with subsampling
Input: Initial X0∈Rn×d, loss function ℓ, learning rate λ >0, subsampling size s∈ {1, . . . , n },
max. epoch T≥1, stopping criterion.
SetL:X7→ℓ(Dgm( X))(+ possibly a regularization term in X).
fork= 1, . . . , T do
Subsample X′
k−1={x′
1, . . . , x′
s}uniformly from Xk−1.
Compute ∇L(X′
k−1)(vanilla topological gradient)
Compute the diffeomorphic interpolation ˜v(X′
k−1)from∇L(X′
k−1)using (4).
SetXk:=Xk−1−λ˜v(Xk−1).
ifstopping criterion is reached then
Return Xk
end if
end for
Return XT
Stopping criterion. A natural stopping criterion for Algorithm 1 is to assess whether the loss
L(Xt) =ℓ(Dgm( Xt))is smaller than some ε > 0. However, computing Dgm( Xt)can be in-
tractable if Xtis large. Therefore, a tractable loss to consider is ˆL(Xt):=E[ℓ(Dgm( X′
t))], where
6(a)t= 0
 (b)t= 100 (Vanilla)
 (c)t= 500 (Vanilla)
 (d)t= 100 (Diffeo)
Figure 3: Showcase of the usefulness of subsampling combined with diffeomorphic interpolations to minimize
a topological simplification loss, with parameters λ= 0.1,s= 50 ,n= 500 .(a)Initial point cloud X(blue),
subsample X′(red), vanilla topological gradient on the subsample (black) and corresponding diffeomorphic
interpolation (orange). (b)and(c), the point cloud Xtafter running t= 100 andt= 500 steps of vanilla
gradient descent. (d)the point cloud Xtafter running t= 100 steps of diffeomorphic gradient descent.
X′
tis a uniform s-sample from Xt. Under that perspective, Algorithm 1 can be re-interpreted
as a kind of stochastic gradient descent on ˆL, for which two standard stopping criteria can be
used: (a)compute an exponential moving average of the loss on individual samples X′
tover
iterations, or (b)compute a validation loss, i.e., sample X′
t,(1), . . . , X′
t,(K)and estimate ˆLby
K−1PK
k=1ℓ(Dgm( X′
t,(k))). Empirically, we observe that the latter approach with K=n/s(more
repetitions for smaller sample sizes to mitigate variance) yields the most satisfactory results (faster
convergence toward a better objective Xt) overall, and thus stick to this choice in our experiments.
4 Numerical experiments
We provide numerical evidence for the strength of our diffeomorphic interpolations. PH-
related computations relies on the library Gudhi [40] and automatic differentiation relies on
tensorflow [1]. The “big-step gradient” baseline [34] implementation is based on oineus4. The
first two experiments were run on a 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz ,
the last one on a 2x Xeon SP Gold 5115 @ 2.40GHz . Our code is publicly available at https:
//github.com/tlacombe/topt .
Convergence speed and running times. We sample uniformly n= 200 points on a unit circle
inR2with some additional Gaussian noise, and then minimize the simplification loss L:X7→P
(b,d)∈Dgm( X)|d|2, which attempts to destroy the underlying topology in Xby reducing the death
times of the loops by collapsing the points. The respective gradient descents are iterated over a
maximum of 250 epochs, possibly interrupted before if a loss of 0is reached ( Dgm( X)is empty),
with a same learning rate λ= 0.1. The bandwidth of the Gaussian kernel in (4) is set to σ= 0.1. We
include the competitor oineus [34], as—even though relying on a fairly different construction—this
method shares a key idea with ours: extending the vanilla gradient to move more points in X. We
stress that both approaches can be used in complementarity: compute first the “big-step gradient”
of [34] using oineus , and then extend it by diffeomorphic interpolation. Results are displayed in
Figure 45. In terms of loss decrease over iterations , both “big-step gradients" and our diffeomorphic
interpolations significantly outperform vanilla topological gradients, and their combined use yields
the fastest convergence (by a slight margin over our diffeomorphic interpolations alone). However,
in terms of raw running times, the use of oineus involves a significant computational overhead,
making our approach the fastest to reach convergence by a significant margin.
Subsampling. We now showcase how using our diffeomorphic interpolation jointly with subsam-
pling routines (Algorithm 1) allows to perform topological optimization on point clouds with thou-
sands of points, a new scale in the field. For this, we consider the vertices of the Stanford Bunny
[41], yielding a point cloud X0∈Rn×dwithn= 35 ,947andd= 3. We consider a topolog-
4https://github.com/anigmetov/oineus
5Note that the loss computed with oineus hasnotbeen normalized in the figure, which is why its values are
larger than the others. This has no influence over its minimum number of iterations needed to reach 0though.
71.0
 0.5
 0.0 0.5 1.01.0
0.5
0.00.51.0Init.
1.0
 0.5
 0.0 0.5 1.01.0
0.5
0.00.51.0Vanilla output
1.0
 0.5
 0.0 0.5 1.02
1
01Diffeo output
1.0
 0.5
 0.0 0.5 1.01.0
0.5
0.00.51.0Oineus output
0.5
 0.0 0.5 1.06
4
2
0246Oineus+Diffeo output
Figure 4: (Top) From left to right: initial point cloud, and final point cloud for the different flows. (Bottom)
Evolution of the losses with respect to the number of iterations and with respect to running time.
0 200 400 600 800 1000
Epoch0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0.000LossEvolution of loss over iterations
Vanilla
Diffeo
Figure 5: From left to right: initial Stanford bunny X0, the point cloud after 1,000epochs of vanilla topological
gradient descent (barely any changes), the point cloud after 200 epochs of diffeomorphic gradient descent, after
1,000 epochs, and eventually the evolution of losses for both methods over iterations.
ical augmentation loss (see Section 2.1) for two-dimensional topological features, i.e., we aim at
increasing the persistence of the bunny’s cavity. The size of nmakes the computation of Dgm( X0)
untractable (recall that it scales in O(n4)); we thus rely on subsampling with sample size s= 100
and compare the vanilla gradient descent scheme and our scheme described in Algorithm 1. Results
are displayed in Figure 5. Because it only updates a tiny fraction of the initial point cloud at each
iteration, the vanilla topological gradient with subsampling barely changes the point cloud (nor de-
creases the loss) in 1,000 epochs. In sharp contrast, as our diffeomorphic interpolation computed on
subsamples is defined on R3, it updates the whole point cloud at each iteration, making possible to
decrease the objective function where the vanilla gradient descent is completely stuck. Note that a
step of diffeomorphic interpolation, in that case, takes about 10times longer than a vanilla step. An
additional subsampling experiment can be found in Appendix C.
Black-box autoencoder models. Next, we apply our diffeomorphic interpolations to black-box
autoencoder models. In their simplest formulation, autoencoders (AE) can be summarized as two
maps E:Rd→Rd′andD:Rd′→Rdcalled encoder and decoder respectively. The intermediate
spaceRd′in which the encoder Eis valued is referred to as a latent space (LS), with typically
d′≪d. In general, without further care, there is no reason to expect that the LS of a point cloud X,
E(X) ={E(x1), . . . , E (xn)}, reflects any geometric or topological properties of X. While this
can be mitigated by adding a topological regularization term to the loss function during the training
of the autoencoder [29, 4], this cannot work in the setting where one is given a black-box, pre-
trained AE. However, replacing (E, D)by(φ◦E, D◦φ−1)for any invertible map φ:Rd′→Rd′
yields an AE producing the same outputs yet changing the LS E(X), without explicit access to
the AE’s model. Hence, we propose to learn such a φwith diffeomorphic interpolations: given
some latent space E(X), we apply Tsteps of our diffeomorphic gradient descent algorithm to X7→
ℓ(Dgm( X))initialized at E(X). We thus get a sequence of smooth displacements −˜v1, . . . ,−˜vTof
Rd′that discretizes the flow (7) via φ:x07→x0−PT
k=1˜vk(xk−1)where xk−xk−1=−˜vk(xk−1),
and such that Dgm( φ(E(X)))is more topologically satisfying. This fixed diffeomorphism φcan
8Figure 7: COIL images, their corresponding initial LSs in blue and final LSs obtained with diffeomorphic
gradient descent in orange, and the corresponding topological losses, for both vase (left) and duck (right).
then be re-applied to any new data coming out of the encoder in a deterministic way. Moreover, any
random sample from the topologically-optimized LS can be inverted without further computations
by following ˜vT,˜vT−1, . . . , ˜v1, which allows to push the new sample back to the initial LS, and then
apply the decoder on it. Again, this cannot be achieved with baselines [39, 34].
In order to illustrate these properties, we trained a variational autoencoder (V AE) to project a family
of datasets of images representing rotating objects, named COIL [32], to two-dimensional latent
spaces. Given that every dataset in this family is comprised of 288pictures of the same object taken
with different angles, one can impose a prior on the topology of the corresponding LSs, namely that
they are sampled from circles. However, the V AE architecture is shallow (the encoder has one fully-
connected layer (100 neurons), and the decoder has two (50 and 100 neurons), all layers use ReLu
activations), and thus the learned latent spaces, although still looking like curves thanks to continuity,
do not necessarily display circular patterns. This makes generating new data more difficult, as latent
spaces are harder to interpret. To improve on this, we learn a flow φas described above with an
augmentation loss associated to the 1-dimensional PD point which is the most far away from the
diagonal, in order to force latent spaces to have a significant one-dimensional topological feature,
i.e., loop. As the datasets are small, we do not use subsampling, and we use learning rate λ= 0.1,
Gaussian kernels with bandwidth σ= 0.3and an increase of at least 3.in the topological loss (from
an iteration to the next) to stop the algorithm6.
Figure 6: As the four samples from the
topologically-optimized LS (blue) are far
from the initial LS (orange), the decoded im-
ages are fuzzy. However, reverting φand
following the corresponding green trajecto-
ries allows to render good-looking images.We provide some qualitative results in Figure 7 (see also
Appendix C, Figure 10). In order to quantify the improve-
ment, we also computed the Pearson correlation scores
between the ground-truth angles θiand the angles ˆθicom-
puted from the topologically-optimized LSs with
ˆθi:=∠(φ◦E(xi)−ˆE[φ◦E(X)], φ◦E(x1)−ˆE[φ◦E(X)]),
where ˆE[φ◦E(X)] := n−1Pn
i=1φ◦E(xi), and φde-
notes our flow. In Table 1, we provide an average of
these scores over 100test sets obtained by randomly per-
turbing the training set with uniform noise of amplitude
0.05. As expected, correlation becomes better after forc-
ing the latent spaces to have the topology of a circle. This
better interpretability is also illustrated in Figure 6, in
which four angles are specified, which are mapped to the
topologically-optimized LS, then pushed to the initial LS
of the black-box V AE by following the reverted flow of
our learned diffeomorphism φ, and finally decoded back
into realistic, COIL -like images.
Single-cell data. We also deployed our proposed topological correction of LSs from AEs on a real-
world dataset of single cells. Specifically, we designed an experiment on single cell HiC (scHiC)
data inspired from [27]. The dataset is comprised of single cells characterized by chromatin folding,
that is, each cell is encoded by the spatial distance matrix of its DNA fragments. The dataset we
focus on is taken from [31], in which it was shown that cells are sampled at different phases of the
cell cycle. Thus, similar to COIL images, we expect latent embeddings of this dataset to exhibit a
circular shape, that we can constrain with diffeomorphic topological optimization.
6Indeed, we noticed that 3.was a consistent threshold for detecting whether the representative cycle of the
most persistent PD point changed between iterations.
9Dataset Duck Cat Pig Vase Teapot
No optim. 0.56±7.5e-04 0.78±1.4e-03 0.17±5.7e-04 0.86±7.2e-03 0.32±1.6e-03
Diffeo 0.61±3.1e-03 0.83±1.2e-03 0.76±2.1e-04 0.93±9.8e-04 0.39±3.4e-03
Dataset scHiC (augmentation) scHiC (registration)
No optim. 0.79±8.1e-03 0.792 ±8.1e-03
Diffeo 0.84±4.3e-03 0.794 ±8.4e-03
Table 1: Means and variances of correlation scores computed over 100 test sets, for both COIL and scHiC.
Specifically, we processed this single cell dataset of 1,171cells with the stratum-adjusted correlation
coefficient (SCC) with 500kb and convolution parameter h= 1 on chromosome 10. Then, we run
kernel PCA on the SCC matrix to obtain a preprocessed dataset in R100, on which we applied
the same V AE architecture than the one described above for COIL images. Finally, we optimized
two losses, the first was the same augmentation loss than for the COIL images, the second was the
following registration loss:
L:X∈Rn×d7→W2(Dgm1(X), Dtarget),
where Dgm( X)contains the points of the PD of Xwith distance-to-diagonal at least τ= 1,Dtarget
is a target PD with only one point p∗= [−3.5,3.5], and Wis the 2-Wasserstein distance between
PDs. We used σ= 0.2for the augmentation loss and σ= 0.025for the registration loss ( σis set to a
smaller value for the registration loss in order to mitigate the effects of matching instability), λ= 0.1
on500epochs, with subsampling of size s= 300 for computational efficiency (as computing VR
diagrams without radius thresholding on 1,171points already takes few minutes on a laptop CPU,
which becomes hardly tractable if done repetitively as in gradient descent), and loss increase of 3.
as a stopping criterion. Qualitative results are displayed in Appendix C, Figure 11, and we also
measured quantitative improvement with the correlation scores between latent space angles and
repli scores7in Table 1, in which improvements can be observed. An additional experiment on the
influence of the bandwidth parameter σover these correlation scores, as well as over convergence,
can also be found in Appendix C.
5 Conclusion
In this article, we have presented a way to turn sparse topological gradients into dense diffeomor-
phisms with quantifiable Lipschitz constants, and showcased practical benefits of this approach in
terms of convergence speed, scaling, and applications to black-box AE models on several datasets.
Several questions are still open for future work.
In terms of theoretical results, we plan on working on the stability between the diffeomorphic in-
terpolations computed on a dataset and its subsamples. This requires some control over the loca-
tions of the critical points, which we expect to be possible in statistical estimation ; indeed sublevel
sets of density functions are know to have stable critical points [10, Lemma 17]. We also plan to
look at adaptive kernels whose parameters (like the bandwidth σ) depend on the input point cloud
σ=σ(X)(instead of using a fixed kernel and parameters at every iteration), and understand the
convergence properties of our proposed diffeomorphic gradient descent. Finally, applying our dif-
feomorphic interpolation to sparse gradients computed with multiparameter persistent homology
is another natural research direction, provided that differentiability properties have recently been
proved in that setting [30, 37].
Concerning the AE experiment, we plan to investigate the limitations presented in the figure below:
as the initial LSs (colored with the ground-truth angles) have zero (left)
or two (right) loops, it is impossible to unfold them with diffeomor-
phisms; instead the optimized latent spaces either also exhibit no topol-
ogy, or mixes different angles. In future work, we plan on investigating
other losses or gradient descent schemes for diffeomorphic topological optimization, including strat-
ified procedures similar to [27] that allow for local topological changes during training.
7Therepli-score is a real-valued proxy for the cell cycle, which was introduced in [31], and which can be
computed out of the copy numbers of genome regions associated to the early phases of the cell cycle. Thus,
a large correlation with this score indicates that the circular shape in the optimized latent space is indeed
representative of the cell cycle itself.
10Acknowledgements
M.C. was supported by ANR grant "TopModel", ANR-23-CE23-0014. The authors are grateful to
the OPAL infrastructure from Université Côte d’Azur for providing resources and support.
References
[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Good-
fellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz
Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol
Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software avail-
able from tensorflow.org.
[2] Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Ship-
man, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. Persistence
images: a stable vector representation of persistent homology. Journal of Machine Learning
Research , 18(8):1–35, 2017.
[3] Tolga Birdal, Aaron Lou, Leonidas Guibas, and Umut Simsekli. Intrinsic dimension, persistent
homology and generalization in neural networks. In Advances in Neural Information Process-
ing Systems 34 (NeurIPS 2021) , volume 34, pages 6776–6789. Curran Associates, Inc., 2021.
[4] Mathieu Carrière, Frédéric Chazal, Marc Glisse, Yuichi Ike, Hariprasad Kannan, and Yuhei
Umeda. Optimizing persistent homology based functions. In 38th International Conference
on Machine Learning (ICML 2021) , volume 139, pages 1294–1303. PMLR, 2021.
[5] Mathieu Carrière, Frédéric Chazal, Yuichi Ike, Théo Lacombe, Martin Royer, and Yuhei
Umeda. PersLay: a neural network layer for persistence diagrams and new graph topological
signatures. In 23rd International Conference on Artificial Intelligence and Statistics (AISTATS
2020) , pages 2786–2796. PMLR, 2020.
[6] Mathieu Carrière, Steve Y . Oudot, and Maks Ovsjanikov. Stable topological signatures for
points on 3d shapes. Computer Graphics Forum , 34(5):1–12, 2015.
[7] Frédéric Chazal, David Cohen-Steiner, Leonidas Guibas, Facundo Mémoli, and Steve Oudot.
Gromov-Hausdorff stable signatures for shapes using persistence. Computer Graphics Forum ,
28(5):1393–1403, 2009.
[8] Frédéric Chazal, Vin De Silva, and Steve Oudot. Persistence stability for geometric complexes.
Geometriae Dedicata , 173(1):193–214, 2014.
[9] Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Bertrand Michel, Alessandro Rinaldo, and
Larry Wasserman. Subsampling methods for persistent homology. In 32nd International Con-
ference on Machine Learning (ICML 2015) , volume 37, pages 2143–2151. PMLR, 2015.
[10] Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Bertrand Michel, Alessandro Rinaldo, and
Larry Wasserman. Robust topological inference: distance to a measure and kernel distance.
Journal of Machine Learning Research , 18(159):1–40, 2018.
[11] Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Alessandro Rinaldo, Aarti Singh, and Larry
Wasserman. On the bootstrap for persistence diagrams and landscapes. Modelirovanie i Analiz
Informatsionnykh Sistem , 20(6):111–120, 2013.
[12] Frédéric Chazal, Brittany Fasy, Fabrizio Lecci, Alessandro Rinaldo, and Larry Wasserman.
Stochastic convergence of persistence landscapes and silhouettes. Journal of Computational
Geometry , 6(2):140–161, 2015.
[13] Frédéric Chazal and Steve Yann Oudot. Towards persistence-based reconstruction in euclidean
spaces. In Proceedings of the twenty-fourth annual symposium on Computational geometry ,
pages 232–241. ACM, 2008.
[14] Chao Chen, Xiuyan Ni, Qinxun Bai, and Yusu Wang. A topological regularizer for classifiers
via persistent homology. In The 22nd International Conference on Artificial Intelligence and
Statistics , pages 2573–2582. PMLR, 2019.
11[15] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. Advances in neural information processing
systems , 31, 2018.
[16] James R Clough, Nicholas Byrne, Ilkay Oksuz, Veronika A Zimmer, Julia A Schnabel, and
Andrew P King. A topological loss function for deep-learning based image segmentation
using persistent homology. IEEE transactions on pattern analysis and machine intelligence ,
44(12):8766–8778, 2020.
[17] David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Stability of persistence diagrams.
Discrete & Computational Geometry , 37(1):103–120, 2007.
[18] David Cohen-Steiner, Herbert Edelsbrunner, John Harer, and Yuriy Mileyko. Lipschitz func-
tions have l p-stable persistence. Foundations of computational mathematics , 10(2):127–139,
2010.
[19] Meryll Dindin, Yuhei Umeda, and Frederic Chazal. Topological data analysis for arrhyth-
mia detection through modular neural networks. In Advances in Artificial Intelligence: 33rd
Canadian Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON, Canada, May
13–15, 2020, Proceedings 33 , pages 177–188. Springer, 2020.
[20] Herbert Edelsbrunner and John Harer. Computational topology: an introduction . American
Mathematical Soc., 2010.
[21] Marcio Gameiro, Yasuaki Hiraoka, and Ippei Obayashi. Continuation of point clouds via
persistence diagrams. Physica D: Nonlinear Phenomena , 334:118–132, 2016.
[22] Thomas Gebhart and Paul Schrater. Adversary detection in neural networks via persistent
homology. arXiv preprint arXiv:1711.10056 , 2017.
[23] Christoph Hofer, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland Kwitt. Graph fil-
tration learning. In 37th International Conference on Machine Learning (ICML 2020) , volume
119, pages 4314–4323. PMLR, 2020.
[24] Max Horn, Edward de Brouwer, Michael Moor, Bastian Rieck, and Karsten Borgwardt. Topo-
logical Graph Neural Networks. In 10th International Conference on Learning Representations
(ICLR 2022) . OpenReviews.net, 2022.
[25] Xiaoling Hu, Li Fuxin, Dimitris Samaras, and Chao Chen. Topology-preserving deep image
segmentation. In Advances in Neural Information Processing Systems 32 (NeurIPS 2019) ,
pages 5657–5668. Curran Associates, Inc., 2019.
[26] Théo Lacombe, Yuichi Ike, Mathieu Carrière, Frédéric Chazal, Marc Glisse, and Yuhei Umeda.
Topological Uncertainty: monitoring trained neural networks through persistence of activation
graphs. In 30th International Joint Conference on Artificial Intelligence (IJCAI 2021) , pages
2666–2672. International Joint Conferences on Artificial Intelligence Organization, 2021.
[27] Jacob Leygonie, Mathieu Carrière, Théo Lacombe, and Steve Oudot. A gradient sampling
algorithm for stratified maps with applications to topological data analysis. Mathematical
Programming , pages 1–41, 2023.
[28] Jacob Leygonie, Steve Oudot, and Ulrike Tillmann. A framework for differential calculus on
persistence barcodes. Foundations of Computational Mathematics , pages 1–63, 2021.
[29] Michael Moor, Max Horn, Bastian Rieck, and Karsten Borgwardt. Topological autoencoders.
In37th International Conference on Machine Learning (ICML 2020) , volume 119, pages
7045–7054. PMLR, 2020.
[30] Soham Mukherjee, Shreyas Samaga, Cheng Xin, Steve Oudot, and Tamal Dey. D-gril: End-
to-end topological learning with 2-parameter persistence. In CoRR . arXiv:2406.07100, 2024.
[31] Takashi Nagano, Yaniv Lubling, Csilla Várnai, Carmel Dudley, Wing Leung, Yael Baran, Netta
Mendelson-Cohen, Steven Wingett, Peter Fraser, and Amos Tanay. Cell-cycle dynamics of
chromosomal organization at single-cell resolution. Nature , 547:61–67, 2017.
[32] S. A. Nene, S. K. Nayar, and H. Murase. Columbia Object Image Library (COIL-100). In
Technical Report CUCS-005-96 , 1996.
[33] Arnur Nigmetov, Aditi S Krishnapriyan, Nicole Sanderson, and Dmitriy Morozov. Topological
regularization via persistence-sensitive optimization. arXiv preprint arXiv:2011.05290 , 2020.
12[34] Arnur Nigmetov and Dmitriy Morozov. Topological optimization with big steps. Discrete &
Computational Geometry , pages 1–35, 2024.
[35] Steve Y Oudot. Persistence theory: from quiver representations to data analysis , volume 209.
American Mathematical Society, 2015.
[36] Adrien Poulenard, Primoz Skraba, and Maks Ovsjanikov. Topological function optimization
for continuous shape matching. In Computer Graphics Forum , volume 37, pages 13–25. Wiley
Online Library, 2018.
[37] Luis Scoccola, Siddharth Setlur, David Loiseaux, Mathieu Carrière, and Steve Oudot. Dif-
ferentiability and convergence of filtration learning with multiparameter persistence. In 41st
International Conference on Machine Learning (ICML 2024) . PMLR, 2024.
[38] Primoz Skraba and Katharine Turner. Wasserstein stability for persistence diagrams. arXiv
preprint arXiv:2006.16824 , 2020.
[39] Yitzchak Solomon, Alexander Wagner, and Paul Bendich. A fast and robust method for global
topological functional optimization. In International Conference on Artificial Intelligence and
Statistics , pages 109–117. PMLR, 2021.
[40] The GUDHI Project. GUDHI User and Reference Manual . GUDHI Editorial Board, 3.6.0
edition, 2022.
[41] Greg Turk and Marc Levoy. Zippered polygon meshes from range images. In Proceedings of
the 21st annual conference on Computer graphics and interactive techniques , pages 311–318,
1994.
[42] Yuhei Umeda. Time series classification via topological data analysis. Information and Media
Technologies , 12:228–239, 2017.
[43] Laurent Younes. Shapes and diffeomorphisms . Springer-Verlag, 2010.
13A A more extensive presentation of TDA
The starting point of TDA is to extract quantitative topological information from structured
objects—for example graphs, points sampled on a manifold, time series, etc. Doing so relies on
a piece of algebraic machinery called persistent homology (PH), which informally detects the pres-
ence of underlying topological properties in a multiscale way. Here, estimating topological proper-
ties should be understood as inferring the number of connected components (topology of dimension
0), the presence of loops (topology of dimension 1), of cavities (dimension 2), and so on in higher
dimensional settings.
Simplicial filtrations. Given a finite simplicial complex8K, afiltration overKis a map t7→
Kt⊆Kthat is non-decreasing (for the inclusion). For each σ∈K, one can record the value t(σ):=
inf{t|σ∈Kt}at which the simplex σis inserted in the filtration. From a topological perspective,
the insertion of σhas exactly one of two effects: either it creates a new topological feature in Kt
(e.g., the insertion of an edge can create a loop, that is, a one-dimensional topological feature) or it
destroys an existing feature of lower dimension (e.g., two independent connected components are
now connected by the insertion of an edge). Relying on a matrix reduction algorithm [20, §IV .2],
PH identifies, for each topological feature appearing in the filtration, the critical pair of simplices
(σb, σd)that created and destroyed this feature, and as a byproduct the corresponding birth and
death times t(σb), t(σd).9The collection of intervals (t(σb), t(σd))is a (finite) subset of the open
half-plane {(b, d)∈R2|b < d}, called the persistence diagram (PD) of the filtration (Kt)t. The
distance of such a point (tb, td)to the diagonal {b=d}, namely 2−1
2|t(σb)−t(σd)|, is called the
persistence of the corresponding topological feature, as an indicator of “for how long” could this
feature be detected in the filtration (Kt)t.
Note that if (σb, σd)is a critical pair for our filtration (Kt)t, it holds that |σb|=|σd|+1. The quantity
|σb| −1is the dimension of the corresponding topological feature (e.g., loops, which are created by
the insertion of an edge and killed by the insertion of a triangle, are topological features of dimension
one). From a computational perspective, deriving the PD of a filtration (Kt)tis empirically10quasi-
linear with respect to the number of simplices in K(which can still be extremely high in the case of
Vietoris-Rips filtration—see below—where K= 2Vwith|V|=nbeing typically quite large).
The Vietoris-Rips filtration. A particular instance of simplicial filtration that will be extensively
used in this work is the Vietoris-Rips (VR) one. Given X= (x1, . . . , x n)∈Rn×da point cloud of
npoints in dimension d, one consider the simplicial complex K= 2Xand then the filtration (Kt)t
defined by
σ={xi1. . . x ip} ∈Kt⇐⇒ ∀ j, j′∈ {1, . . . , p },∥xij−xij′∥ ≤t. (8)
The corresponding persistence diagram will be denoted, for the sake of simplicity, by Dgm( X).
Note that for t <0,Kt=∅, when t≥diam( X),Kt=K, and there is always a point with
coordinates (0,+∞)inDgm( X)accounting for the remaining connected component when t→
∞. This is the unique point in Dgm( X)for which the second coordinate is +∞(and is often
discarded in practice, as it does not play any significant role). Intuitively, Dgm( X)reflects the
topological properties that can be inferred from the geometry ofX; this can be formalized by various
results which state, roughly, that if the xiare i.i.d. samples from a regular measure µsupported on
a submanifold M ⊂Rd, then with high probability the topological properties of Mare reflected in
Dgm( X)(see [13, 9]). From a computational perspective, note that the VR filtration only depends
onXthrough the pairwise distance matrix (∥xi−xj∥)1≤i,j≤n, and thus the complexity of computing
Dgm( X)depends only linearly in d11. On the other hand, since Dgm( X)scales (at least) linearly
8A simplicial complex is a combinatorial object generalizing graphs and triangulations. Given a finite set
of vertices V={v1, . . . , v n}, a finite simplicial complex Kis a subset of 2V(whose elements are called
simplices ) such that σ∈K⇒τ∈K,∀τ⊆σ(if a simplex is in the complex, its faces must be in it as well).
9It may happen that a topological feature appears at some time tband is never destroyed, in which case
the death time is set to +∞. However, in the context of the Vietoris-Rips filtration, extensively studied in this
work, this (almost) never happens. See the next paragraph.
10The theoretical worst case yields a cubic complexity, but the matrix that has to be reduced is typically very
sparse, enabling this practical speed up.
11However, the statistical efficiency of Dgm( X)when it is used as an estimator for the topology of an
underlying manifold Mdeteriorate when the intrinsic dimension of Mincreases.
14−1.00−0.75−0.50−0.25 0.00 0.25 0.50 0.75 1.00−1.00−0.75−0.50−0.250.000.250.500.751.00Initial and ﬁnal point cloud
init
ﬁnal state
0.1 0.2 0.3 0.4 0.5 0.6
Birth0.10.20.30.40.50.6DeathInitial and ﬁnal persistence diagrams
0 25 50 75 100 125 150 175 200−0.4−0.3−0.2−0.1Evolution of the loss over iterationsFigure 8: Topological optimization of an initial point cloud X(in red) by minimizing X7→P
(b,d)∈Dgm( X)−|d|2+P
x∈Xdist(x,[−1,1]2). This loss favors the apparition of topological features (loops)
while the regularization term penalizes points that would go to infinity otherwise.—Experiment reproduced
following the setting of [4], using code available at https://github.com/GUDHI/TDA-tutorial/blob/
master/Tuto-GUDHI-optimization.ipynb .
with respect to the number of simplices in K, computing the whole VR diagram of a point cloud
X∈Rn×dcan take up to O(2n)operations. Even if one restricts topological features of dimension
d′≤d(e.g.d′= 1 if one only considers loops)—as commonly done—the complexity is of order
O(nd′+2), which becomes quickly intractable if nis large, even if d′= 1or2.
B Delayed proofs
Proof of Proposition 3.2. One has: ∥˜v(x)−˜v(y)∥1 = ∥P
i∈I(K(x, xi)−
K(y, xi))(−K−1∇L(˜X(t)))i∥1≤P
i∈I|ρσ(∥x−xi∥)−ρσ(∥y−xi∥)|·∥(−K−1∇L(˜X(t)))i∥1,
since we are using Gaussian kernels. As |ρσ(∥x−xi∥)−ρσ(∥y−xi∥)| ≤ Cd,σ∥x−y∥2,
with Cd,σ= 2d+1
2πd−1
2σd−1(see [2, Theorem 8]), it follows that ∥˜v(x)−˜v(y)∥1≤
Cd,σ∥x−y∥2· ∥K−1∥1· ∥∇L(˜X(t))∥1.
Let us upper bound the term ∥∇L(˜X(t))∥1. Let us start with the simplification loss, one has∂ℓ
∂bi=
2(bi−di)and, writing bi=∥xi1−xi2∥2(for some critical points xi1, xi2∈˜X(t)), one has
∂bi
∂xi1=xi1−xi2
∥xi1−xi2∥2and∂bi
∂xi2=−xi1−xi2
∥xi1−xi2∥2. Similarly, one has∂ℓ
∂di=−2(bi−di), and writing
di=∥xi3−xi4∥2provides the corresponding partial derivatives.
Applying (2), this gives:
∥∇L(˜X(t))∥1=X
x∈˜X(t)∥X
xb,1→(bi,di)2(bi−di)x−xi2
∥x−xi2∥2−X
xb,2→(bi,di)2(bi−di)xi1−x
∥xi1−x∥2
−X
xd,1→(bi,di)2(bi−di)x−xi4
∥x−xi4∥2+X
xd,2→(bi,di)2(bi−di)xi3−x
∥xi3−x∥2∥1,
whereb,1→(resp.b,2→) means that xappears as left point (resp. right point) in the computation of
the birth filtration value biof one of the kPD points (bi, di)∈Dgm( ˜X(t))associated to the
loss, and similarly for death filtration values. A brutal majoration finally gives ∥∇L(˜X(t))∥1≤
2√
dP
x∈˜X(t)P
x→(bi,di)|bi−di| ≤8√
d·Pers k(Dgm( ˜X(t))), as there are at most four points
associated to every (bi, di)∈Dgm( ˜X(t)). One can easily see that the same bound applies to the
augmentation loss.
Let us finally bound ∥K−1∥1, one has ∥K−1∥1=κ(K)/∥K∥1≤κ(K). Indeed, as we are using
Gaussian kernels, ∥K∥1= max 1≤i≤nPn
j=1ρσ(∥xi−xj∥2)≥1.
C Complementary experimental results and details
Subsampling and improving over [4]. We reproduce the experiment of [4, §5], see also Figure 8,
but starting from an initial point cloud X0of size n= 2,000instead of n= 300 . This makes the
151.0
 0.5
 0.0 0.5 1.01.00
0.75
0.50
0.25
0.000.250.500.751.00Init
1.0
 0.5
 0.0 0.5 1.01.00
0.75
0.50
0.25
0.000.250.500.751.00Vanilla, epoch 750
1.0
 0.5
 0.0 0.5 1.01.00
0.75
0.50
0.25
0.000.250.500.751.00Diffeo, epoch 750
0 200 400 600
Epoch0.3
0.2
0.1
0.00.10.2LossEvolution of loss
Vanilla
DiffeoFigure 9: Topological optimization with subsampling. From left to right, the initial point cloud X0, the point
cloud after 750 steps of vanilla gradient descent (+subsampling), the point cloud after 750 steps of diffeomor-
phic interpolation gradient descent (+subsampling), loss evolution over epochs. Parameters: λ= 0.1,σ= 0.1.
Figure 10: Topologically-optimized LSs and losses for duck, cat, pig, vase and teapot.
raw computation of Dgm( Xk)at each gradient step unpractical. Following Section 3.2 we rely on
subsampling with sample size s= 100 and apply Algorithm 1. Results are summarized in Figure 9.
While relying on vanilla gradients and subsampling barely changes the point cloud even after 750
epochs, the diffeomorphic interpolation gradient with subsampling manages to decrease the loss.
More extensive reports of running time and comments. On the hardware used in our exper-
iments (the first two experiments were run on a 11th Gen Intel(R) Core(TM) i5-1135G7 @
2.40GHz , the last one on a 2x Xeon SP Gold 5115 @ 2.40GHz .), we report the approximate fol-
lowing running times:
• Small point cloud optimization without subsampling (see Figure 4, n= 200 points): one
gradient descent iteration takes about 1s for the vanilla topological gradient and our diffeo-
morphic interpolation. The use of oineus integrated in our pipeline raises the running time
(per iteration) to 10to20seconds. Note that the diffeomorphic interpolation and oineus
may converge in less steps than the vanilla topological gradient, preserving a competitive
advantage. We also believe that oineus has a significant room for improvement in terms
of running times and may be a promising method in the future to be used jointly with our
approach.
• Iterating over the stanford bunny with subsampling ( n= 35,947,s= 100 ) takes about 3
seconds per iteration for the vanilla topological gradient and 20 second for our diffeomor-
16Figure 11: Decrease of augmentation (up left) and registration (up right) losses for the scHiC dataset, as well
as the corresponding initial (orange) and optimized (blue) LSs displayed below.
phic interpolation. The increase in running time with respect to the previous experiment
mostly lies on instantiating and applying the n×d(d= 3) vector field ˜v(requires to com-
puteρi(x−xi)for each new x(nof them) and sampled xi(|I|of them, which is typically
very small), hence a ∼O(n)complexity).
• Training the V AE for the COIL and scHiC datasets are the most computationally expensive
parts of this work: it takes about 3 hours per image (20 of them), and 3 hours also for
the scHiC dataset. In contrast, performing the topological optimization take few dozen
of minutes (less than one hour) for each image. Applying it is done in few seconds at
most. Recall that our method is designed to handle pre-trained models (which may be way
more sophisticated than the one we used!); and its running time does not depend on the
complexity of the model.
Influence of the bandwidth σ.We reproduce the same experimental setting as in Fig-
ure 3, i.e., sample points uniformly on a circle of radius 1plus additional noise ∼
N(0,0.05I2), and consider minimizing the total persistence of the point cloud. We take σ∈
{0,0.1,0.2,0.3,0.5,0.7,1,2,3,4,5}(with the convention that σ= 0 corresponds to the vanilla
topological gradient) and learning rate λ= 0.1. We also rely on a subsampling with batch size
s= 50 . To quantify the variability of the scheme with respect to the randomness induced by the
subsampling step, we run each gradient descent 50times with a fixed initialization X0, up to a max-
imum of 200iterations, stopped earlier if a loss of 0(no topology left, global minimum has been
reached) is measured.
Figure 12 displays the results of this illustrative experiment. We report the median of both running
time and number of iterations to reach convergence (or reach the 200iterations limit), along with
the10and90percentiles. The conclusions are:
• For σ= 0(vanilla) and σ≥3, the gradient descent never converges in less than 200steps.
Since the radius of the diameter of the circle is 2, it is not surprising that taking a bandwidth
larger than that hinders convergence.
• For σ∈(0.1,1], the convergence occurs within the same order of magnitude (between
0.49and1.74s), the best performance being reached at σ= 0.3(recall that we used in
the paper, testifying that we did not rely on hyperparameter tuning). It suggests that, on
regular structure, the approach is smooth with respect to σ. Empirically, we observe that a
good proxy is to take σ <median( {|xi−xj|}i,j). Note that even though in theory, σ→0
should recover the vanilla topological gradients, one is limited by numerical accuracy when
evaluating the Gaussian kernel.
17• The variation around the median over 50runs is very small: the randomness of the samples
at each iteration (hence of the trajectory) barely impacts the decrease of the loss and thus
the convergence time.
0 1 2 3 4 5
bandwidth 
0.02.55.07.510.012.515.0T otal running time (s)
0 1 2 3 4 5
bandwidth 
050100150200#Iterations to converge / stop
Impact of  and sampling on convergence
Figure 12: Topological simplification, point cloud of diameter 2with median pairwise distance ≃√
2. Median
and 10-90 percentiles over 50 runs. (Left) Time to converge for different values of σ∈[0,5](σ= 0 corre-
sponds to Vanilla). (Right) #iterations to converge (or stop after 200iterations, indicated by the dashed red
line).
We also studied how the bandwidth σinfluences the correlation scores of Table 1 in Figure 13. We
observe oscillations for values that are roughly on the sides (very small or very large), and more
stable scores for middle range values. Note that these oscillations could also come from how the
correlation score itself is computed.
Figure 13: Influence of the kernel bandwidth σon correlation scores for a few datasets. The values of σare
evenly spaced between 0.05and1for the COIL datasets, and between 0.025and0.5for the scHiC dataset.
18NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our abstract is quite extensive, and there is a clear “Contribution and Outline”
paragraph at the end of our introduction along with “Related works” and “Limitations” (of
previous works) paragraphs.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: This is discussed in the Conclusion section.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Our two theoretical results are followed by a proof. The first is pretty short
and included in the main material, the second one is longer and technical and thus deferred
to the appendix.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We mentioned important hyper-parameters for our PoC experiments (learning
rateλ, bandwidth σ, number of epochs, stopping criterion if any.). The code is available in
the supplementary material as well.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Code is provided in the supplementary material with an (hopefully) clear
Readme.md file which provides some ready-to-use quick start examples. It is also available
as a public Github repository, see https://github.com/tlacombe/topt .
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We hopefully provided the important experimental details to understand our
experiments.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [Yes]
19Justification: We did not report error bars in our PoC experiments as they were meant for
illustrative purposes. We did report error bars in the form of standard deviations for our
main experiments (topological correction of latent spaces of V AEs for images and single
cells datasets). These were obtained by generating multiple test sets by adding noise to the
training set.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Hardware is mentioned in the first paragraph of Section 4, and we reported
running times either in the plots or in the appendix.
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We do not use sensitive nor closed datasets or other aspect that may enter in
conflict with NeurIPS Code of Ethics.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [No]
Justification: We do not expect that—in its current state—our article may have any neg-
ative societal impact. It is a work that introduced a novel method to perform topological
optimization. This recent research field does not have direct applications that would have
negative societal impact so far. Of course, it falls in the same pitfall as any machine learn-
ing related research problem, but we do not see something specific to our work that would
be worth discussing.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We used a single generative model (V AE) to reproduce images from the COIL
dataset and single cells from the Hi-C dataset. We trained it by ourselves.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We used three non-synthetic datasets: the Stanford Bunny, the COIL dataset
and the Hi-C dataset, for which we give the proper credits.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justification: No new asset.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
20Answer: [NA]
Justification: the article does not involve crowdsourcing nor research with human subjects.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: the article does not involve crowdsourcing nor research with human subjects.
21