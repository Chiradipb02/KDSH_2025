Validating Climate Models with Spherical
Convolutional Wasserstein Distance
Robert C. Garrett1Trevor Harris2Zhuo Wang1Bo Li1
1University of Illinois Urbana-Champaign2Texas A&M University
{rcg4, zhuowang, libo}@illinois.edu
tharris@stat.tamu.edu
Abstract
The validation of global climate models is crucial to ensure the accuracy and
efficacy of model output. We introduce the spherical convolutional Wasserstein
distance to more comprehensively measure differences between climate models
and reanalysis data. This new similarity measure accounts for spatial variability
using convolutional projections and quantifies local differences in the distribution
of climate variables. We apply this method to evaluate the historical model outputs
of the Coupled Model Intercomparison Project (CMIP) members by comparing
them to observational and reanalysis data products. Additionally, we investigate
the progression from CMIP phase 5 to phase 6 and find modest improvements in
the phase 6 models regarding their ability to produce realistic climatologies.
1 Introduction
Climate Model Validation General Circulation Models, or climate models, are mathematical
representations of the climate system that describe interactions between matter and energy through
the ocean, atmosphere, and land [Washington and Parkinson, 2005]. Climate models are the primary
tool for investigating the response of the climate system to changes in forcing, such as increases in
CO2, and projecting future climate states [Flato et al., 2014]. To assess the plausibility of climate
models, climate scientists compare output from model simulations against observational data [Rood,
2019]. This comparison is the focus of climate model validation techniques for ensuring that climate
models capture the dynamics of the climate system [Roca et al., 2021].
The Coupled Model Intercomparison Project (CMIP) was initiated in 1995 as a comprehensive and
systematic program for assessing climate models against each other and observational data [Eyring
et al., 2016]. Each model in CMIP participates in a wide variety of experiments such as performing
a historical simulation, a pre-industrial control simulation, and various simulations representing
different scenarios for CO 2emissions [Eyring et al., 2016]. Because historical simulations coincide
with observational measurements, we can compare each model’s synthetic climate distribution to the
distribution of observational or quasi-observational data products [Raäisaänen, 2007], to assess their
reconstructive skill. For complete spatial coverage we compare against reanalysis data, a blend of
observations and short-range weather forecasts through data assimilation [Bengtsson et al., 2004].
This has become one popular climate model validation method [Flato et al., 2014].
Previous approaches Many statistical and machine learning-based methods have been applied to
assess climate model output against reanalysis fields. The most common approach is to compute
the root mean square error (RMSE) between long-term means of the climate model output and the
reanalysis field [Li et al., 2021, Zamani et al., 2020, Karim et al., 2020, Ayugi et al., 2021]. RMSE
provides a direct measure of the differences between two climate fields but does not take into account
internal variability, so we should use it with caution in evaluating climate models. Another approach,
38th Conference on Neural Information Processing Systems (NeurIPS 2024).which is invariant to bias, is to compute measures of correlation between climate model output and
reanalysis fields [Zhao et al., 2021, Zamani et al., 2020, Karim et al., 2020, Ayugi et al., 2021].
More comprehensive approaches employ techniques for random processes to compare two spatial
fields. For example, Shen et al. [2002] and Cressie et al. [2008] use the wavelet decomposition to
compare the spatial-frequency content of two fields. Hering and Genton [2011] measures the loss
differential between models of spatial processes, Lund and Li [2009] and Li and Smerdon [2012]
compare the first and second moments of two random processes, and Yun et al. [2022] identifies
local differences in the mean and dependency structure between two spatiotemporal climate fields.
Functional data analysis techniques have been introduced to compare spatial and spatiotemporal
random fields, considering the random fields as continuous functions. Many of these approaches
compare the underlying mean functions from two sets of functional data [Zhang and Chen, 2007,
Horváth et al., 2013, Staicu et al., 2014]. Other approaches include the second-order structure in
the comparison [Zhang and Shao, 2015, Li et al., 2016], or compare the distributions of two spatial
random processes [Harris et al., 2021].
Since climate models aim to mimic the real climate which is the underlying pattern of weather,
directly assessing the distributional differences between the modeled and observed data seems a
more thorough approach to evaluating climate models. Vissio et al. [2020] proposed to use the
Wasserstein distance (WD), a popular metric for comparing probability distributions [Villani, 2009],
for such a purpose. The WD has also been considered for other use cases in climate science, such as
data assimilation [Tamang et al., 2020, 2021, 2022]. The WD can be computationally expensive or
even impossible to calculate between high-dimensional distributions [Kolouri et al., 2019], so Vissio
et al. [2020] first converts each climate field to a single spatial mean and then only compares the
distribution of spatial means. However, this dimension reduction puts their method at the risk of
missing important spatial variability information, and consequently failing to accurately distinguish
two climate fields that are different.
Recent contributions in the Machine Learning (ML) literature seek to compare multivariate distri-
butions using many features while leveraging the efficiency of the one-dimensional WD [Vallender,
1974]. The sliced WD [Bonneel et al., 2015] compares random projections of distributions on Rn,
and the generalized sliced WD [Kolouri et al., 2019] extends this to a broader class of projections.
The convolutional sliced WD [Nguyen and Ho, 2022] compares distributions of discrete square
images in the space Rn×n, accounting for the spatial structure using kernel convolutions. However,
spatial fields from climate models are defined on a spherical domain, making them incompatible
with the vectorized/square nature of these distances. The spherical sliced WD [Bonet et al.] and
other spherical transport methods [Quellmalz et al., 2023, Cui et al., 2019] can compare distributions
of spatial point processes over a sphere, such as locations of natural disasters and extreme weather
events, but cannot be used for smooth fields such as daily temperature and precipitation. Lastly,
other non-WD approaches have been considered for multivariate distributional comparisons. For
example, Mooers et al. [2023] compared distributions arising from global storm-resolving models
using variational autoencoders.
Our proposal We propose the functional sliced WD as a generalization of the sliced WD to
distributions of functional data. To create a tailored tool for climate model evaluation, we define the
Spherical Convolutional Wasserstein distance (SCWD) as a special case of the functional sliced WD
for functions on the unit sphere S2, a manifold on which latitude-longitude coordinates are defined.
SCWD creates slices containing a small region of spatial fields to characterize local differences in
the distribution of climate variables, which are further integrated into a single measure for global
differences. Compared to the spatial mean-based WD from Vissio et al. [2020], SCWD accounts
for spatial features while maintaining the modest computation for univariate WD when comparing
climate models against reanalysis data, resulting in a comprehensive and more robust evaluation. We
apply SCWD to rank climate models and assess the progression of the new CMIP era with respect to
daily average surface temperature and daily total precipitation.
2 Preliminaries
We consider the problem of comparing two probability distributions PandQ. Each distribution is a
member of P(Ω), the set of Borel probability measures on some sample space, Ω. In our application,
we treat climate fields as functional data [Wang et al., 2016] over a spatial domain S, thus we often
assume Ωis a space of functions. Our sample space of interest is L2(S), the set of square integrable
2functions from S →Rwhere Sis a compact subset of Rn. CMIP model outputs are available at a
global scale, so we consider the spatial domain to be the unit sphere S2, the space over which latitude
and longitude coordinates are assigned to the Earth’s surface. In fact, the space L2(S2)has been
previously considered for modeling climate fields [Heaton et al., 2014]. To compare two functional
data distributions PandQ, we define a distance function D(P, Q)to act as a similarity measure.
Comparing Distributions of Functions Comparisons for distributions of functions in L2(S)
have been studied for specific cases of S[Hall and Van Keilegom, 2007, Bugni and Horowitz, 2021,
Pomann et al., 2016, Harris et al., 2021], but none of these have focused on L2(S2). The mathematical
properties of probability measures in P(L2(S))have been studied before [Gijbels and Nagy, 2017,
Kim, 2006], but it is challenging to calculate distances such as the WD in this space without additional
assumptions [Li and Ma, 2020]. To define a similarity measure for distributions in P(L2(S)), we
build on the theory of the sliced WD and its various extensions, which have only been defined for
distributions of finite-dimensional data.
Sliced WD Given a Borel function π: Ω→R, the pushforward of Punder πis a valid distribution
inP(R)defined as π#P(B) =P(π−1(B))for all Borel sets BinR. The r-th order sliced WD
[Bonneel et al., 2015] between P, Q∈ P(Rn)is a metric defined as the mean of the ordinary WD
over univariate pushforwards:
SWr(P, Q) =Z
Sn−1Wr(πθ#P, πθ#Q)rdθ1/r
, (1)
where πθ(x) =xTθforx∈Rnandθ∈Sn−1. We call πθthe slicing function because it produces
one-dimensional “slices” of the data using projection matrices. Because πθ#Pandπθ#Qare valid
distributions in P(R), the WD inside the integral can be calculated with the commonly used analytical
form for univariate measures [Vallender, 1974].
Generalized Sliced WD The generalized sliced WD [Kolouri et al., 2019] replaces πθwith a
more general class of slicing functions, denoted as gθ. Because of this increased flexibility, the
generalized sliced WD is a pseudometric rather than a metric except for specific cases of gθ[Kolouri
et al., 2019]. For a given use case, the utility and metric properties of the generalized sliced WD
are therefore determined by the choice of slicing function. Slicing functions can be chosen via
optimization, estimated using neural networks, or specified by the researcher to isolate features of
interest [Kolouri et al., 2019]. The last option is desirable for our application, allowing the slices
to be restricted to spatial features of interest to climate modelers. However, the generalized sliced
WD is defined between distributions of data in Rn, a space that is not suitable for distributions of
spatial fields. Climate fields could be coerced to vectors in Rnto be made compatible with the
generalized sliced WD. However, this would result in a loss of the inherent spatial structure, making
it challenging to specify a slicing function that can handle considerations such as area weighting and
spatial correlations.
Convolution Sliced WD The convolution sliced WD [Nguyen and Ho, 2022] represents images as
matrices in the space Rn×n. The slicing function is replaced with kernel convolutions, or possibly a
sequence of kernel convolutions, of d×dpixels for d∈N. The kernel aggregates nearby locations
to isolate local features, which could provide useful information to climate modelers. However, when
climate fields are represented on a rectangular grid, the geographic area represented by each grid cell
varies drastically between latitudes due to the non-Euclidean structure. Thus, a d×dpixel kernel
will cover different sizes and shapes of geographic areas depending on location. For our application,
the kernel radius should therefore be defined using geographic distance, not pixels.
3 Methods
We introduce the functional sliced WD framework which extends the flexible slicing process from the
generalized sliced WD to the infinite-dimensional case of functions in L2(S). We focus on the special
case of L2(S2), which we call the spherical convolutional WD (SCWD), for our climate model
validation application. SCWD adapts the kernel convolution-based slicing idea of the convolutional
sliced WD to a continuous setting while accounting for the non-euclidean structure of climate fields
realized over the Earth’s surface.
3Figure 1: Diagram representing the calculation of SCWD between distributions of daily mean surface
temperature (in degrees Celsius) from ERA5 and a CMIP6 model. Each day, many projections are
computed using kernel convolutions, represented here at two locations. The resulting projections,
called slices, summarize the local climate conditions in each dataset. The slices for each day are
viewed as a sample from the marginal distribution at each location, represented here as histograms.
SCWD is computed as a global mean over the univariate WD between each pair of local distributions.
3.1 Functional Sliced Wasserstein Distance
In general it is not possible to analytically characterize distributions in P(L2(S))and there are no
closed form solutions for computing the WD. However, it is possible to slice elements of L2(S),
meaning we can leverage the analytical form of the one-dimensional WD to define a computable
sliced WD. We extend the convolution slicer from Nguyen and Ho [2022] to the functional data case,
allowing us to project functions in L2(S)to values in Rwhile preserving local spatial information.
Definition 3.1 (Convolution Slicer) .LetSbe a compact subset of Rn,s∈ S, and k, called the
kernel function, be a continuous function from S × S → [0,∞). We define the convolution slicer
cs(f), a linear operator from f∈L2(S)→R, as follows:
cs(f) =Z
Sf(u)k(s, u)du.
To create a valid functional sliced WD, we construct pushforward measures based on the convolution
slicer cs(f). To satisfy the definition of a pushfoward measure, we must show that cs(f)is a Borel
measurable function from L2(S)→R. By continuity of k, when location s∈ Sis fixed, k(s, u)is
a continuous function from u∈ S → R. Because Sis compact, k(s, u)is a continuous function
on a compact set and is thus bounded and L2-integrable. It follows that the convolution slicer cs(f)
is an integral of the product of two functions f, k∈L2(S), so by Hölder’s inequality, cs(f)is a
bounded linear operator from L2(S)→R. Stein and Shakarchi [2011] states that bounded linear
operators are also continuous, so cs(f)is a continuous linear operator and thus Borel measurable. So,
for any measure P∈ P(L2(S)), the pushforward cs#Pis a valid measure in P(R). Therefore, we
can define a functional sliced WD between distributions in P(L2(S))as follows:
Definition 3.2 (Functional Sliced WD) .LetSbe a compact subset of Rn,r≥1, and P, Q∈
P(L2(S)). Let csbe an operator satisfying Definition 3.1. We define the ( r-th order) functional
sliced WD between PandQas follows:
FSW r(P, Q) =Z
SWr(cs#P, cs#Q)rds1/r
where Wris the Wasserstein metric on P(R).
Because cs#Pandcs#Qare valid univariate probability measures, the analytical form of the
univariate WD can be applied for efficient calculations. We introduce theoretical properties for the
functional sliced WD in Theorem 3.3
Theorem 3.3. For all compact subsets S ⊂Rn,FSW ris a pseudometric on P(FS)and maintains
ther-convexity property of the ordinary Wrmetric.
4Proof of Theorem 3.3 is provided in Appendix A. It is unknown if the final positivity property of a
metric is satisfied. Proof of this property would require an invertible Radon-like transformation to be
defined for probability measures in P(L2(S)). Therefore, as with the generalized sliced WD, it is up
to the researcher to specify an appropriate kernel function over the domain of interest. We provide
such a choice for our application and give theoretical justification in Section 3.2.
3.2 Spherical Convolutional Wasserstein Distance
For our application to climate model validation, we specify the spatial domain Sto be the unit sphere
S2, the space over which latitude-longitude coordinates are assigned to locations on the Earth. To
preserve local spatial information, we specify the slicing function to be a radial kernel function. We
introduce the spherical convolutional WD (SCWD) as a specific case of the functional sliced WD:
Definition 3.4 (Spherical Convolutional WD) .LetP, Q∈ P(L2(S2))andr≥1. We define the
(r-th order) SCWD between PandQas follows
SCW r(P, Q) =Z
S2Wr(ωs#P, ωs#Q)rds1/r
, ω s(f) =Z
S2f(u)ϕ(s, u)du,
where ωsis a convolution slicer that satisfies Definition 3.1 with associated radial kernel ϕ(s, u).
Because ϕis a radial kernel function, ωsaggregates local information and the resulting pushforward
measures ωs#Pandωs#Qrepresent the local distribution around each location s. SCWD is
therefore calculated as the global mean of the WD between local distributions at each location. The
local WD values can be recorded and later visualized as a map to pinpoint regions with higher or
lower similarity. Figure 1 demonstrates the process for calculating SCWD between two distributions
of surface temperature fields, and details on the implementation are provided in Appendix B.
Kernel In our analysis, we specify the kernel function to be ϕ(|s−u|;l), where |s−u|is the
chordal distance between sanduandϕis the Wendland kernel function used in Nychka et al. [2015]:
ϕ(d;l) =(1−d)6(35d2+ 18d+ 3)/3d≤l,
0 d > l,(2)
with range parameter l >0determining the radius over which the kernel is nonzero. The Wendland
kernel meets the continuity assumption in Definition 3.1 and is also compact, which enables efficient
sparse computations for our analysis.
Positive definite kernels, such as the Wendland kernel for lless than the diameter [Hubbert and
Jäger, 2023], allow us to retain full spatial information via the spectral density. This is because the
convolution theorem on S2[Driscoll and Healy, 1994] gives an injective correspondence between the
spectral density of a function f∈L2(S2)and the spectral density of the convolution (f∗k)(s) = R
S2f(u)k(s, u)duwhen kis positive definite. Note that as l→ ∞ , the Wendland kernel converges
to the flat kernel ϕ(d;∞) = 1 , resulting in a SCWD where every slice is the global mean. In this
case, the SCWD will be equal to the global mean-based WD from Vissio et al. [2020], leading to a
complete loss of spatial variability information. In our analysis, we ensure a positive definite kernel
by specifying lto be less than the diameter of the Earth (about 12,750km). We study the sensitivity
of our results to this parameter in Section 4.4.
Spatial Analysis Projection selection approaches such as the Max-Sliced WD [Deshpande et al.,
2019] and Energy-Based Sliced WD [Nguyen and Ho, 2024] have been introduced as alternatives
to the sliced WD that give higher slicing weight to the directions of greatest variability between
high-dimensional distributions. Each of our slices corresponds to a local mean around a specific
location, so applying projection selection methods to SCWD would be equivalent to identifying the
geographic regions in which two distributions of spatial fields have the greatest differences. This is of
keen interest when validating climate models, so we examine spatial maps of our slices in Section 4.2
to identify these regions. However, for a comprehensive evaluation of climate fields, we favor the
geographically-balanced SCWD in Definition 3.4 when evaluating similarity to reference datasets.
4 Climate Model Validation
We consider climate model outputs from the Coupled Model Intercomparison Project (CMIP) histori-
cal experiment phases 5 and 6. We focus on daily average near-surface (2m) temperature in degrees
5Celsius and daily total precipitation in mm. The CMIP6 historical simulations are organized by
ensembles, each of which is distinguished with an ripf identifier ( ripfor CMIP5), representing
realization, initialization, physics, and forcings of the model, respectively [Eyring et al., 2016]. We
obtain 46 CMIP6 model outputs with the r1i1p1f1 ID and 33 CMIP5 model outputs with the
r1i1p1 ID. Output was obtained either at the daily frequency or aggregated from 3-hourly data. At
the time of writing, two of the 79 total models did not have output available at a suitable frequency
for surface temperature. All 79 models had output available for total precipitation.
To serve as references for climate model evaluation, we collect the European Centre for Medium-
Range Weather Forecasts (ECMWF) Reanalysis 5th Generation (ERA5) [Hersbach et al., 2020]
as well as the Reanalysis-2 data from the National Centers for Environmental Protection (NCEP)
[Kanamitsu et al., 2002] reanalysis datasets. Both datasets were obtained for surface temperature and
total precipitation at a daily frequency. Due to known issues with reanalysis data for precipitation
[Tapiador et al., 2017], we obtain observations from the National Centers for Environmental Informa-
tion (NCEI) Global Precipitation Climatology Project (GPCP) Daily Precipitation Analysis Climate
Data Record [Huffman et al., 2001, Adler et al., 2020] as an additional reference for precipitation.
The historical time periods for each climate variable were chosen to maximize the available model
outputs and reference datasets. For surface temperature, a common time period of January 1, 1979
to November 30th, 2005 was collected for each CMIP output and reanalysis dataset. For total
precipitation, we restrict the time period to October 1, 1996 to November 30th, 2005 to accommodate
the first available day of observations in the GPCP dataset. Each data product represents the climate
variables on a different latitude-longitude grid, which varies in size and structure. See Appendix C
for full details on the spatial resolution and availability of temperature/precipitation for each dataset.
4.1 Evaluation of CMIP6 Models
To evaluate the skill of CMIP6 models in characterizing historical climate distributions, we compute
SCWD between each model output and the reference datasets. Models which excel at replicating the
local climate distribution in many different areas of the Earth will have a low SCWD. Conversely,
models which fail to capture features of the local climate distributions, such as the mean, variance,
or extremes, will have a higher SCWD. We do not expect perfect agreement between models and
historical data, so, similar to Vissio et al. [2020], we additionally calculate SCWD between the
reference datasets as a baseline for comparison. All SCWD calculations in this section use the
Wendland kernel with range parameter of 1,000km for slicing.
We select the ERA5 Reanalysis as our reference dataset for (2m) surface temperature due to its high
spatial resolution. We calculate distances from each surface temperature model output in our CMIP5
and CMIP6 ensembles to ERA5. In addition, we calculate SCWD between ERA5 and the NCEP
Reanalysis to compare the variability between reanalysis datasets to the variability between models
and reanalysis. For total precipitation, we select the GPCP observational data as our reference. We
calculate distances from each total precipitation model output in our CMIP5 and CMIP6 ensembles
to GPCP. We include SCWD calculations from GPCP to both ERA5 and NCEP for comparison, with
the secondary goal of assessing the accuracy of each reanalysis in faithfully filling gaps in observed
precipitation measurements. Full details on the SCWD rankings can be seen in Tables 2, 3, 4, and 5
in Appendix D. Here, we focus on the results for CMIP6, and Figure 2 provides the SCWD from
each CMIP6 model output to the ERA5 surface temperature field and GPCP total precipitation field.
Surface Temperature For surface temperature, NCEP has a lower SCWD to ERA5 than all CMIP
models. This is not a surprise: both ERA5 and NCEP are based on observations, so we expect their
temperature distributions to be similar in most regions. Among the model outputs, many models have
a SCWD to ERA5 similar to that of NCEP. In particular, AWI-CM-1-1-MR from the Alfred Wegener
Institute and MPI-ESM1-2-HR from the Max Planck Institute have the lowest SCWD for surface
temperature, with a few models close behind.
Total Precipitation Compared to surface temperature, NCEP no longer has a lower SCWD to
GPCP than the CMIP6 models. Instead, the ERA5 total precipitation field, which has the lowest
SCWD to GPCP, serves as a better baseline for comparison. Deficiencies of precipitation from
reanalysis have been reported in previous studies [Janowiak et al., 1998]. In brevity, precipitation
is sensitive to model physics and is not strongly constrained by observations via data assimilation.
The low SCWD of ERA5 can likely be attributed to the high model resolution and more advanced
model physics and data assimilation system compared to NCEP. Among the CMIP6 models, the
6NCEP
ERA5 Total PrecipitationAWI−CM−1−1−MR
MPI−ESM1−2−HRGISS−E2−2−G
IITM−ESM
NorESM2−MMEC−Earth3BCC−ESM1
FGOALS−f3−L
1.01.52.02.53.03.5
1 2 3
SCWD to ERA5 (2m) Surface TemperatureSCWD to GPCP Total PrecipitationModel Group
ACCESS
AWIBCCCanCESMCMCCE3SMEC−EarthFGOALSGFDLGISSICONIITMINMIPSLKACEKIOSTMIROCMPIMRINESMNorSAMTai
Ranking CMIP6 Model Outputs by SCWD to ERA5 and GPCPFigure 2: Ranking CMIP6 model outputs using SCWD. Each model output is represented by a
point on the scatter plot and models from the same group share the color and shape. The x-axis
and y-axis values represent each model’s SCWD to the ERA5 surface temperature and GPCP total
precipitation fields, respectively. The NCEP reanalysis is included as a blank triangle with dashed
lines representing the SCWD to ERA5 and GPCP. The SCWD from the ERA5 total precipitation
field to GPCP is represented as a solid line.
Norwegian Climate Centre NorESM2-MM model output stands out with the lowest SCWD to GPCP
by a relatively wide margin. The EC-Earth3 and CESM model outputs also form clusters with low
SCWD values for precipitation.
No single model has the lowest SCWD value for both surface temperature and total precipitation,
but NorESM2-MM seems to have the best balance of low distances for each variable and is not far
off from the intersection of the lines for our reference datasets. Similarly, no model has the highest
SCWD value for both climate variables. The GISS-E2-2-G model from NASA’s Goddard Institute for
Space Studies is a distinct outlier with a high surface temperature SCWD to ERA5, and the Beijing
Climate Center BCC-ESM1 model is an outlier in terms of high SCWD to GPCP. We investigate
these high SCWD values in Section 4.2.
4.2 Spatial Comparisons
Because SCWD is calculated as a global mean of local WD values, we can investigate the geographic
sources of these outlying high SCWD values. Figure 3 provides a spatial breakdown of the local WD
values obtained when calculating SCWD for surface temperature between ERA5 and AWI-CM-1-1-
MR as well as ERA5 and GISS-E2-2-G, the models with the lowest and highest SCWD to ERA5,
respectively. Overall, both maps seem smooth or continuous in space, with little variation between
neighboring locations in most cases. For the map between AWI-CM-1-1-MR and ERA5, the local
WD values are relatively low everywhere, with regions of slightly higher values near the poles and
mountains. Compared to AWI-CM-1-1-MR, the GISS-E2-2-G model has similarly low WD values in
the tropics. However, closer to the poles, the local WD values begin to increase. In particular, the
Arctic region has extremely high WD values relative to the rest of the Earth. This is indicative of the
previously documented winter cool bias in the Artic region for GISS-E2-2-G [Kelley et al., 2020].
72m Surface TemperatureSCWD Map from AWI−CM−1−1−MR to ERA5
051015WD2m Surface TemperatureSCWD Map from GISS−E2−2−G to ERA5
Total PrecipitationSCWD Map from NorESM2−MM to GPCP
051015WDTotal PrecipitationSCWD Map from BCC−ESM1 to GPCPFigure 3: Top: Map of local Wasserstein distances from ERA5 to two CMIP6 2m surface temperature
outputs: AWI-CM-1-1-MR and GISS-E2-2-G. Bottom: Map of local Wasserstein distances from
GPCP to two CMIP6 total precipitation outputs: NorESM2-MM and BCC-ESM1. Color fill at each
location is determined by the WD between the local distributions obtained from the convolution slicer
in Definition 3.4. The color scale is shared for all maps and continental boundaries are included in
black to aid spatial comparisons.
Similar maps are provided for the NorESM2-MM and BCC-ESM1 outputs compared to GPCP total
precipitation. Overall, both maps are less smooth than the surface temperature maps, potentially
due to the more localized nature of precipitation. Looking at both models, the higher values of
SCWD near the equator in the Pacific and Atlantic oceans may be related to the double-Intertropical
Convergence Zone problem common in CMIP models, in which excessive precipitation is produced
in the southern tropics [Mechoso et al., 1995]. This trend is much more pronounced for BCC-ESM1
than for NorESM2-MM. Additionally, BCC-ESM1 has a region of particularly high WD values
around eastern Indonesia, where wind-terrain interaction plays an important role in the regional
distribution of precipitation. This region has been previously highlighted in Zhang et al. [2021] as
an area where BCC-ESM1 heavily overestimates annual mean precipitation. SCWD maps for both
climate variables and all CMIP5/CMIP6 model outputs are provided in the supplemental material.
4.3 Comparing CMIP5 and CMIP6
To assess the progression from CMIP5 to CMIP6, Figure 4 provides boxplots of the SCWD from
each CMIP model to the reference datasets. The left panel provides SCWD calculations from the
surface temperature models and NCEP to the ERA5 Reanalysis. The median SCWD value for CMIP6
models to ERA5 is lower than that of CMIP5. However, the two boxplots share a similar range, so
the difference between the CMIP5 and CMIP6 ensembles is subtle. Overall, we see a promising,
albeit limited, decrease in SCWD for typical CMIP6 models compared to CMIP5. This indicates
improved performance of CMIP6 when it comes to reconstructing realistic temperature distributions
at the local level. The right panel provides SCWD calculations from each precipitation model and
ERA5/NCEP to the GPCP dataset. The median SCWD value for CMIP6 is again lower than that
of CMIP5. Compared to surface temperature, the difference between CMIP5 and CMIP6 is more
distinct. This indicates that relative to surface temperature, precipitation modeling has seen greater
gains with the transition from CMIP5 to CMIP6. The improvement of CMIP6 models compared to
CMIP5 in precipitation representation has been reported in previous studies using different evaluation
8methods (e.g., Chen et al. [2021]). It can be probably attributed to the more advanced model physics
and overall higher model resolution in CMIP6.
NCEP
1.52.02.53.03.5
CMIP5 CMIP6SCWDSurface TemperatureSCWD from CMIP Models to ERA5
NCEP
ERA5
1.01.52.02.53.0
CMIP5 CMIP6SCWDTotal PrecipitationSCWD from CMIP Models to GPCP
Figure 4: Left: Boxplots of SCWD from the CMIP5 and CMIP6 model outputs to the ERA5
Reanalysis for 2m surface temperature. Right: Boxplots of SCWD from the CMIP5 and CMIP6
model outputs to the GPCP observational dataset for total precipitation. Each plot contains points
representing the SCWD from each CMIP model output to the reference dataset (ERA5 or GPCP),
with CMIP5 and CMIP6 separated into two boxplots for comparison. Dotted and dashed lines are
included to represent the SCWD from NCEP to ERA5/GPCP and ERA5 to GPCP, respectively.
4.4 Metric Comparison
We compare our method to baseline methods and evaluate the sensitivity of our results to the choice
of range parameter. We generate seven additional rankings for the CMIP models: SCWD with a
range of 500km and 2,500km respectively, the global mean-based WD (GMWD) from Vissio et al.
[2020], RMSE and MAE computed on long-term mean climatologies, and WD and Sliced WD on
data regridded to an icosahedral grid to preserve area weighting. Technical details and rankings
together with our original 1,000km SCWD results can be seen in Tables 2 and 3 in Appendix D.
All SCWD rankings are nearly identical regardless of the range parameter, except that several
precipitation models, such as GFDL-CM4 and E3SM-2-0, rank better with a larger range. From
here forward, we focus only on the chosen 1,000km range parameter for SCWD. For most baseline
methods (RMSE, MAE, WD, and Sliced WD), their rankings are moderately similar to those of
SCWD, indicating general agreements, though each metric still shows unique perspectives. However,
there is a large discrepancy between the rankings of GMWD and all other rankings. For surface
temperature, SCWD, RMSE, and WD all show that the distance from NCEP to ERA5 is lower than
that of all CMIP6 models, and MAE and Sliced WD also rank NCEP favorably among the models.
However, GMWD ranks NCEP only among the median CMIP6 models. For precipitation, we see a
similar result for ERA5 compared to GPCP: ERA5 has the best ranking when using SCWD, RMSE,
MAE, and WD, but is third in the rankings for Sliced WD and again is around the median for GMWD.
To better understand the differences in rankings between SCWD and baseline model evaluation
methods such as GMWD and RMSE/MAE, Appendix E provides an experiment and data example.
The experiment shows a case where SCWD detects changes in both the climatological mean and the
variance of the anomalies, while the other methods detect only one type of change. In particular, we
show that the RMSE and MAE criteria are unable to detect isolated changes in the variance of the
anomalies. The data example focuses on the SAM0-UNICON surface temperature model. Despite the
overall similarity in rankings for CMIP6 surface temperature, this model ranks significantly higher by
RMSE/MAE than by SCWD. Further investigation shows that SAM0-UNICON exhibits significant
differences in both the climatological mean and the variance of the anomalies. Because RMSE/MAE
are unable to detect shifts in the variance of the anomalies they artificially inflate the ranking of this
model compared to SCWD.
95 Discussion
Climate model validation is critical for ensuring that climate models faithfully represent the Earth
system. To this end, we developed a new similarity measure, called spherical convolutional Wasser-
stein distance (SCWD), which quantifies model performance in a way that properly accounts for
spatial variability. SCWD builds on previous sliced WD methods to compare distributions of infinite-
dimensional functional data, specifically surfaces on the sphere S2. Overall, our results indicate that
incorporating local perspectives, rather than just the global mean, is essential when evaluating the
similarities between climate models and observational data. In both theory and practice, a model that
represents the global mean well may have large compensating errors at the regional scale. SCWD
better represents the regional performance of climate models than the previously proposed WD-based
evaluation criteria in [Vissio et al., 2020]. Furthermore, SCWD is not limited to only comparing the
long-term mean climate state as with RMSE and MAE, and our convolution slicing provides better
spatial insights (in the form of maps) than WD and Sliced WD.
For surface temperature, we found evidence to suggest that SCWD is more accurate than the previous
WD-based approach for evaluating climate models, such as the two reanalysis datasets being more
similar compared to climate models and reanalysis. However, for precipitation, similar findings were
made for only one of the reanalysis datasets. Given the limitations with reanalysis for precipitation
discussed in Section 4.1, additional experiments from different perspectives may be needed to fully
assess the accuracy of our method. Because we have only shown that SCWD is a pseudometric,
rather than a metric, SCWD may not be able to fully distinguish between climate fields which have
differences only in the joint spatial distribution, rather than at the local level. We acknowledge that
determining the sample complexity of SCWD is an area that requires further investigation.
We hope our method will be tested under many different scenarios and variables in climate science.
For example, calculating SCWD on monthly mean or even annual data could help to detect biases in
terms of longer range temporal variability. Another idea is to remove long-term climatological means
from the data before computing SCWD. This would provide additional insight into which features are
being captured beyond the mean climate state. SCWD can also be applied to machine-assisted climate
model tuning [Hourdin et al., 2017], a growing area of research that relies on quality model evaluation
metrics. Some ideas for future work to further improve SCWD include learning an optimal range
value for the Wendland kernel function or using a neural network to estimate the kernel function,
similar to Kolouri et al. [2019].
Outside of climate science, SCWD can be used as a loss function for training generative models
for360oimages. This only requires a straightforward extension of SCWD to allow for multiple
convolution layers, similar to the rectangular case in Nguyen and Ho [2022]. Likewise, the more
general functional sliced WD can be used as a loss function for generative models on any functional
manifold. For example, we can apply the functional sliced WD to texture mapping or color transfer
on the surface of 3D models, which are typically considered as non-euclidean manifolds. SCWD can
be also adapted to compare distributions of spatiotemporal fields, rather than spatial fields as in this
article, by including both time and space in the functional data domain. This would require specifying
a space-time kernel function [Porcu et al., 2016]. The more functional sliced WD framework has a
variety of potential use cases for comparing distributions of functions on a broad class of manifolds.
Potential application areas where the data lie on non-trivial manifolds include facial recognition [Li
et al., 2014], astronomy [Szapudi, 2008], and ecology [Sutherland et al., 2015].
Acknowledgments and Disclosure of Funding
We acknowledge the World Climate Research Programme, which, through its Working Group on
Coupled Modelling, coordinated and promoted CMIP6. We thank the climate modeling groups for
producing and making available their model output, the Earth System Grid Federation (ESGF) for
archiving the data and providing access, and the multiple funding agencies who support CMIP6 and
ESGF. NCEP/DOE Reanalysis II data provided by the NOAA PSL, Boulder, Colorado, USA, from
their website at https://psl.noaa.gov . This work is partially supported by the National Science
Foundation grants NSF-DMS-1830312, NSF-DGE-1922758, and NSF-DMS-2124576 as well as the
National Oceanic and Atmospheric Administration (NOAA) grant NA18OAR4310271. Lastly, we
thank the referees and area chairs for their time and valuable feedback.
10References
Robert Adler, Jian-Jian Wang, Mathew Sapiano, George Huffman, David Bolvin, Eric Nelkin, et al.
Global Precipitation Climatology Project (GPCP) Climate Data Record (CDR), Version 1.3 (Daily).
2020.
Brian Ayugi, Jiang Zhihong, Huanhuan Zhu, Hamida Ngoma, Hassen Babaousmail, Karim Rizwan,
and Victor Dike. Comparison of CMIP6 and CMIP5 models in simulating mean and extreme
precipitation over East Africa. International Journal of Climatology , 41(15):6474–6496, 2021.
Lennart Bengtsson, Stefan Hagemann, and Kevin I Hodges. Can climate trends be calculated from
reanalysis data? Journal of Geophysical Research: Atmospheres , 109(D11), 2004.
Clément Bonet, Paul Berg, Nicolas Courty, François Septier, Lucas Drumetz, and Minh Tan Pham.
Spherical Sliced-Wasserstein. In The Eleventh International Conference on Learning Representa-
tions .
Nicolas Bonneel, Julien Rabin, Gabriel Peyré, and Hanspeter Pfister. Sliced and radon wasserstein
barycenters of measures. Journal of Mathematical Imaging and Vision , 51:22–45, 2015.
Federico A Bugni and Joel L Horowitz. Permutation tests for equality of distributions of functional
data. Journal of Applied Econometrics , 36(7):861–877, 2021.
Chao-An Chen, Huang-Hsiung Hsu, and Hsin-Chien Liang. Evaluation and comparison of CMIP6
and CMIP5 model performance in simulating the seasonal extreme precipitation in the Western
North Pacific and East Asia. Weather and Climate Extremes , 31:100303, 2021.
Noel Cressie, Martina Pavlicová, and Thomas J Santner. Detecting signals in FMRI data using
powerful FDR procedures. Statistics and its interface , 1(1):23–32, 2008.
Li Cui, Xin Qi, Chengfeng Wen, Na Lei, Xinyuan Li, Min Zhang, and Xianfeng Gu. Spherical
optimal transportation. Computer-Aided Design , 115:181–193, 2019.
Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen
Zhao, David Forsyth, and Alexander G Schwing. Max-Sliced Wasserstein Distance and its use for
GANs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,
pages 10648–10656, 2019.
James R Driscoll and Dennis M Healy. Computing Fourier transforms and convolutions on the
2-sphere. Advances in applied mathematics , 15(2):202–250, 1994.
V . Eyring, S. Bony, G. A. Meehl, C. A. Senior, B. Stevens, R. J. Stouffer, and K. E. Taylor. Overview of
the Coupled Model Intercomparison Project Phase 6 (CMIP6) experimental design and organization.
Geoscientific Model Development , 9(5):1937–1958, 2016. doi: 10.5194/gmd-9-1937-2016. URL
https://gmd.copernicus.org/articles/9/1937/2016/ .
Gregory Flato, Jochem Marotzke, Babatunde Abiodun, Pascale Braconnot, Sin Chan Chou, William
Collins, Peter Cox, Fatima Driouech, Seita Emori, Veronika Eyring, et al. Evaluation of climate
models. In Climate change 2013: the physical science basis. Contribution of Working Group I to
the Fifth Assessment Report of the Intergovernmental Panel on Climate Change , pages 741–866.
Cambridge University Press, 2014.
Andrew R Friedman, Yen-Ting Hwang, John CH Chiang, and Dargan MW Frierson. Interhemispheric
temperature asymmetry over the twentieth century and in future projections. Journal of Climate ,
26(15):5419–5433, 2013.
Irène Gijbels and Stanislav Nagy. On a general definition of depth for functional data. Statistical
Science , pages 630–639, 2017.
Peter Gleckler, Charles Doutriaux, Paul Durack, Karl Taylor, Yuying Zhang, Dean Williams, Erik
Mason, and Jérôme Servonnat. A more powerful reality test for climate models. Eos, 97, 2016.
Peter Hall and Ingrid Van Keilegom. Two-sample tests in functional data analysis starting from
discrete data. Statistica Sinica , pages 1511–1531, 2007.
11James Hansen, Makiko Sato, and Reto Ruedy. Perception of climate change. Proceedings of the
National Academy of Sciences , 109(37):E2415–E2423, 2012.
Trevor Harris, Bo Li, Nathan J Steiger, Jason E Smerdon, Naveen Narisetty, and J Derek Tucker.
Evaluating proxy influence in assimilated paleoclimate reconstructions—Testing the exchangeabil-
ity of two ensembles of spatial processes. Journal of the American Statistical Association , 116
(535):1100–1113, 2021.
MJ Heaton, M Katzfuss, C Berrett, and DW Nychka. Constructing valid spatial processes on the
sphere using kernel convolutions. Environmetrics , 25(1):2–15, 2014.
Amanda S Hering and Marc G Genton. Comparing spatial predictions. Technometrics , 53(4):414–425,
2011.
Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz-Sabater,
Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The ERA5 global reanalysis.
Quarterly Journal of the Royal Meteorological Society , 146(730):1999–2049, 2020.
Lajos Horváth, Piotr Kokoszka, and Ron Reeder. Estimation of the mean of functional time series and
a two-sample problem. Journal of the Royal Statistical Society: Series B (Statistical Methodology) ,
75(1):103–122, 2013.
Frédéric Hourdin, Thorsten Mauritsen, Andrew Gettelman, Jean-Christophe Golaz, Venkatramani
Balaji, Qingyun Duan, Doris Folini, Duoying Ji, Daniel Klocke, Yun Qian, et al. The art and
science of climate model tuning. Bulletin of the American Meteorological Society , 98(3):589–602,
2017.
Simon Hubbert and Janin Jäger. Generalised Wendland functions for the sphere. Advances in
Computational Mathematics , 49(1):3, 2023.
George J Huffman, Robert F Adler, Mark M Morrissey, David T Bolvin, Scott Curtis, Robert Joyce,
Brad McGavock, and Joel Susskind. Global precipitation at one-degree daily resolution from
multisatellite observations. Journal of hydrometeorology , 2(1):36–50, 2001.
John E Janowiak, Arnold Gruber, CR Kondragunta, Robert E Livezey, and George J Huffman.
A comparison of the NCEP–NCAR reanalysis precipitation and the GPCP rain gauge–satellite
combined dataset with observational error considerations. Journal of Climate , 11(11):2960–2979,
1998.
Masao Kanamitsu, Wesley Ebisuzaki, Jack Woollen, Shi-Keng Yang, JJ Hnilo, M Fiorino, and
GL Potter. NCEP–DOE AMIP-II Reanalysis (R-2). Bulletin of the American Meteorological
Society , 83(11):1631–1644, 2002.
Rizwan Karim, Guirong Tan, Brian Ayugi, Hassen Babaousmail, and Fei Liu. Evaluation of histor-
ical CMIP6 model simulations of seasonal mean temperature over Pakistan during 1970–2014.
Atmosphere , 11(9):1005, 2020.
Maxwell Kelley, Gavin A Schmidt, Larissa S Nazarenko, Susanne E Bauer, Reto Ruedy, Gary L
Russell, Andrew S Ackerman, Igor Aleinov, Michael Bauer, Rainer Bleck, et al. GISS-E2.
1: Configurations and climatology. Journal of Advances in Modeling Earth Systems , 12(8):
e2019MS002025, 2020.
Jong Uhn Kim. Invariant measures for a stochastic nonlinear Schrödinger equation. Indiana University
mathematics journal , pages 687–717, 2006.
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized
sliced Wasserstein distances. Advances in Neural Information Processing Systems , 32, 2019.
Bo Li and Jason E Smerdon. Defining spatial comparison metrics for evaluation of paleoclimatic
field reconstructions of the Common Era. Environmetrics , 23(5):394–406, 2012.
Bo Li, Xianyang Zhang, and Jason E Smerdon. Comparison between spatio-temporal random
processes and application to climate model data. Environmetrics , 27(5):267–279, 2016.
12Jingjing Li, Ran Huo, Hua Chen, Ying Zhao, and Tianhui Zhao. Comparative assessment and future
prediction ssing CMIP6 and CMIP5 for annual precipitation and extreme precipitation simulation.
Frontiers in Earth Science , 9, 2021. ISSN 2296-6463. doi: 10.3389/feart.2021.687976. URL
https://www.frontiersin.org/article/10.3389/feart.2021.687976 .
Ruonan Li, Pavan Turaga, Anuj Srivastava, and Rama Chellappa. Differential geometric repre-
sentations and algorithms for some pattern recognition and computer vision problems. Pattern
Recognition Letters , 43:3–16, 2014.
Tao Li and Jinwen Ma. Functional data clustering analysis via the learning of gaussian processes with
wasserstein distance. In Neural Information Processing: 27th International Conference, ICONIP
2020, Bangkok, Thailand, November 23–27, 2020, Proceedings, Part II 27 , pages 393–403.
Springer, 2020.
Robert Lund and Bo Li. Revisiting climate region definitions via clustering. Journal of Climate , 22
(7):1787–1800, 2009.
Carlos R Mechoso, Andrew W Robertson, N Barth, MK Davey, P Delecluse, PR Gent, S Ineson,
B Kirtman, Mojib Latif, H Le Treut, et al. The seasonal cycle over the tropical Pacific in coupled
ocean–atmosphere general circulation models. Monthly Weather Review , 123(9):2825–2838, 1995.
Griffin Mooers, Mike Pritchard, Tom Beucler, Prakhar Srivastava, Harshini Mangipudi, Liran Peng,
Pierre Gentine, and Stephan Mandt. Comparing storm resolving models and climates via unsuper-
vised machine learning. Scientific Reports , 13(1):22365, 2023.
Khai Nguyen and Nhat Ho. Revisiting sliced Wasserstein on images: From vectorization to convolu-
tion. Advances in Neural Information Processing Systems , 35:17788–17801, 2022.
Khai Nguyen and Nhat Ho. Energy-based sliced Wasserstein distance. Advances in Neural Informa-
tion Processing Systems , 36, 2024.
Douglas Nychka, Soutir Bandyopadhyay, Dorit Hammerling, Finn Lindgren, and Stephan Sain.
A multiresolution Gaussian process model for the analysis of large spatial datasets. Journal of
Computational and Graphical Statistics , 24(2):579–599, 2015.
Gina-Maria Pomann, Ana-Maria Staicu, and Sujit Ghosh. A two sample distribution-free test for
functional data with application to a diffusion tensor imaging study of multiple sclerosis. Journal
of the Royal Statistical Society. Series C, Applied Statistics , 65(3):395, 2016.
Emilio Porcu, Moreno Bevilacqua, and Marc G Genton. Spatio-temporal covariance and cross-
covariance functions of the great circle distance on a sphere. Journal of the American Statistical
Association , 111(514):888–898, 2016.
Michael Quellmalz, Robert Beinert, and Gabriele Steidl. Sliced optimal transport on the sphere.
Inverse Problems , 39(10):105005, 2023.
Jouni Raäisaänen. How reliable are climate models? Tellus A: Dynamic Meteorology and Oceanog-
raphy , 59(1):2–29, 2007.
Rémy Roca, Ziad S Haddad, Fumie F Akimoto, Lisa Alexander, Ali Behrangi, George Huffman, Seiji
Kato, Chris Kidd, Pierre-Emmanuel Kirstetter, Takuji Kubota, et al. The joint IPWG/GEWEX
precipitation assessment, 2021.
Richard B. Rood. Validation of Climate Models: An Essential Practice , pages 737–762. Springer Inter-
national Publishing, Cham, 2019. ISBN 978-3-319-70766-2. doi: 10.1007/978-3-319-70766-2_30.
URL https://doi.org/10.1007/978-3-319-70766-2_30 .
Xiaotong Shen, Hsin-Cheng Huang, and Noel Cressie. Nonparametric hypothesis testing for a spatial
signal. Journal of the American Statistical Association , 97(460):1122–1140, 2002.
Ana-Maria Staicu, Yingxing Li, Ciprian M Crainiceanu, and David Ruppert. Likelihood ratio tests
for dependent data with applications to longitudinal and functional data analysis. Scandinavian
Journal of Statistics , 41(4):932–949, 2014.
13Elias M Stein and Rami Shakarchi. Functional analysis: introduction to further topics in analysis ,
volume 4. Princeton University Press, 2011.
Chris Sutherland, Angela K Fuller, and J Andrew Royle. Modelling non-Euclidean movement
and landscape connectivity in highly structured ecological networks. Methods in Ecology and
Evolution , 6(2):169–177, 2015.
István Szapudi. Introduction to higher order spatial statistics in cosmology. In Data Analysis in
Cosmology , pages 457–492. Springer, 2008.
Sagar K Tamang, Ardeshir Ebtehaj, Dongmian Zou, and Gilad Lerman. Regularized variational
data assimilation for bias treatment using the Wasserstein metric. Quarterly Journal of the Royal
Meteorological Society , 146(730):2332–2346, 2020.
Sagar K Tamang, Ardeshir Ebtehaj, Peter J Van Leeuwen, Dongmian Zou, and Gilad Lerman.
Ensemble Riemannian data assimilation over the Wasserstein space. Nonlinear Processes in
Geophysics Discussions , 2021:1–26, 2021.
Sagar K Tamang, Ardeshir Ebtehaj, Peter Jan Van Leeuwen, Gilad Lerman, and Efi Foufoula-
Georgiou. Ensemble Riemannian data assimilation: towards large-scale dynamical systems.
Nonlinear Processes in Geophysics , 29(1):77–92, 2022.
FJ Tapiador, A Navarro, V Levizzani, E García-Ortega, GJ Huffman, C Kidd, PA Kucera, CD Kum-
merow, H Masunaga, WA Petersen, et al. Global precipitation measurements for validating climate
models. Atmospheric Research , 197:1–20, 2017.
SS Vallender. Calculation of the Wasserstein distance between probability distributions on the line.
Theory of Probability & Its Applications , 18(4):784–786, 1974.
Cédric Villani. The wasserstein distances. Optimal Transport: Old and New , pages 93–111, 2009.
Gabriele Vissio, Valerio Lembo, Valerio Lucarini, and Michael Ghil. Evaluating the perfor-
mance of climate models based on Wasserstein distance. Geophysical Research Letters , 47
(21):e2020GL089385, 2020.
Jane-Ling Wang, Jeng-Min Chiou, and Hans-Georg Müller. Functional data analysis. Annual Review
of Statistics and its application , 3:257–295, 2016.
Warren M Washington and Claire Parkinson. Introduction to three-dimensional climate modeling .
University science books, 2005.
Sooin Yun, Xianyang Zhang, and Bo Li. Detection of local differences in spatial characteristics
between two spatiotemporal random fields. Journal of the American Statistical Association , 117
(537):291–306, 2022.
Yasin Zamani, Seyed Arman Hashemi Monfared, Mohsen Hamidianpour, et al. A comparison of
CMIP6 and CMIP5 projections for precipitation to observational data: the case of Northeastern
Iran. Theoretical and Applied Climatology , 142(3):1613–1623, 2020.
Jie Zhang, Tongwen Wu, Fang Zhang, Kalli Furtado, Xiaoge Xin, Xueli Shi, Jianglong Li, Min Chu,
Li Zhang, Qianxia Liu, et al. BCC-ESM1 model datasets for the CMIP6 Aerosol Chemistry Model
Intercomparison Project (AerChemMIP). Advances in Atmospheric Sciences , 38:317–328, 2021.
Jin-Ting Zhang and Jianwei Chen. Statistical inferences for functional data. The Annals of Statistics ,
pages 1052–1079, 2007.
Xianyang Zhang and Xiaofeng Shao. Two sample inference for the second-order property of
temporally dependent functional data. Bernoulli , 21(2):909–929, 2015.
Shanshan Zhao, Wenping He, Tianyun Dong, Jie Zhou, Xiaoqiang Xie, Ying Mei, Shiquan Wan,
and Yundi Jiang. Evaluation of the performance of CMIP5 models to simulate land surface air
temperature based on long-tange correlation. Frontiers in Environmental Science , page 6, 2021.
14A Proof of Theorem 3.3
Proof. Letr≥1, letSbe a compact subset of Rn, and let P, Q, U ∈ P(L2(S)). We show the
identity property holds:
FSW r(P, P) =Z
SWr(cs#P, cs#P)rds1/r
=Z
S0rds1/r
= 0
The second line holds by the identity property of the ordinary WD. Next, the symmetry property:
FSW r(P, Q) =Z
SWr(cs#P, cs#Q)rds1/r
=Z
SWr(cs#Q, cs#P)rds1/r
=FSW r(Q, P)
The second line holds by the symmetry property of the ordinary WD. Next, the triangle inequality:
FSW r(P, U) =Z
SWr(cs#P, cs#U)rds1/r
≤Z
S[Wr(cs#P, cs#Q) +Wr(cs#Q, cs#U)]rds1/r
≤Z
SWr(cs#P, cs#Q)rds1/r
+Z
SWr(cs#Q, cs#U)rds1/r
=FSW r(P, Q) +FSW r(Q, U)
The second line holds by the triangle inequality property of the ordinary WD and the third line holds
by the Minkowski inequality. Lastly, we check the r-convexity property. Let λ∈[0,1]. If we take P
as the reference distribution, this property tells us what happens to the functional sliced WD as we
interpolate between QandU:
FSW r(P, λQ + (1−λ)U) =Z
SWr(cs#P, cs#[λQ+ (1−λ)U])rds1/r
=Z
SWr(cs#P, λc s#Q+ (1−λ)cs#U)rds1/r
≤Z
SλWr(cs#P, cs#Q)r+ (1−λ)Wr(cs#P, cs#U)rds1/r
≤
λZ
SWr(cs#P, λc s#Q)rds1/r
+

(1−λ)Z
SWr(cs#P, cs#U)rds1/r
=λ1/rFSW r(P, Q) + (1 −λ)1/rFSW r(P, U)
The second line follows from properties of linear operators (in this case cs). The third line follows
by the r-convexity of the ordinary WD, and the fourth line follows from the Minkowski inequality.
We have shown all three pseuduometric properties and the r−convexity property, so our proof is
complete.
15B SCWD Implementation
The following algorithm describes the process by which we calculated the SCWD values shown in
Section 4. All computations were performed using R version 4.3.2 on an Ubuntu operating system
with an Intel i5-9600k processor (6 cores), 32GB RAM, and 3TB hard disk space. Total computation
time for all experiments was under 72 hours. The calculation of each distance measure was fast, but
loading the required datasets into memory for processing took the majority of the compute time.
Algorithm 1 Spherical Convolutional Wasserstein Distance Approximation
DATA
Reference dataset X0(t), t∈ T0: sample of spatial fields
Model outputs X1(t), t∈ T1, ..., X n(t), t∈ Tn:nsamples of spatial fields to be compared to
X0
PARAMETERS AND APPROXIMATION GRIDS
Wasserstein order parameter r: 2
Range parameter l: 1,000 km kernel radius
Approximation quantiles Q: 200 evenly spaced quantiles ranging from 0 to 1 with a step size of
0.005
GridG1:60×120regular latitude-longitude grid of center points for strided convolution
GridG2:361×720regular latitude-longitude grid to provide discrete approximation of spherical
domain
STEP1. P RECOMPUTE SLICING WEIGHTS
foreach location s∈G1do
Calculate vector of chordal distances from sto all locations in G2
Calculate Wendland function (2) with range lfor all locations in G2using distance vector
Apply area weighting to the Wendland kernel values using the area of each grid cell in G2
Normalize area-weighted kernel and store the results as a sparse vector W(s)
end for
STEP2. C OMPUTE SLICED QUANTILES
fori∈0,1, ..., n do
Re-grid XitoG2without smoothing (one nearest neighbor upsampling)
foreach location s∈G1do
Slice Xiinto one dimension using the dot product X∗
i(s, t) =⟨W(s), Xi(t)⟩
Calculate the sliced quantile function of X∗
i(s, t), denoted as F−1
i(s, q), q∈Q
end for
end for
STEP3. C ALCULATE APPROXIMATE SCWD
fori∈1, ..., n do
foreach location s∈G1do
Calculate the local WD between X0andXicentered around sasdi(s)r=P
q∈Q|F−1
0(q)−F−1
i(q)|r
end for
Calculate the approximate SCWD between X0andXiasSCWD (X0, Xi)≈ P
s∈G1di(s)r1/r
end for
The climate model outputs and reanalysis datasets (ERA, NCEP) used in our analysis have no missing
data. However, the GPCP observational dataset used as the reference for total precipitation did have
missing data at some sites for a few days in the historical period. To handle missing data in the GPCP
dataset, we modified the slicing process in Step 2. If locations corresponding to greater than 50% of
the convolution weight were missing when calculating a slice value, an NAvalue was recorded. The
local WD calculations in Step 3 were computed ignoring these NAvalues. In total, 17,943 slices were
missing sufficient data when using the 50% threshold. However, a total of 24,105,600 slices were
considered for the GPCP dataset (3,348 days in the analysis period with 7,200 slices per day in the
strided convolution), so the missing slices constituted under 0.1% of the final sliced GPCP data used
in the analysis.
16C Table of Data Details and Access Links
Obs./Reanalysis Data Longs Lats TAS PR
NCEP Reanalysis 144 73 Yes Yes
ERA5 Reanalysis 1440 721 Yes Yes
GPCP Observations 360 180 No Yes
CMIP5 Models Longs Lats TAS PR
ACCESS1-0 192 145 Yes Yes
ACCESS1-3 192 145 Yes Yes
CanCM4 128 64 No Yes
CMCC-CESM 96 48 Yes Yes
CMCC-CM 480 240 Yes Yes
CMCC-CMS 192 96 Yes Yes
CNRM-CM5 256 128 Yes Yes
CSIRO-Mk3-6-0 192 96 Yes Yes
CanESM2 128 64 Yes Yes
EC-EARTH 320 160 Yes Yes
FGOALS-g2 128 60 Yes Yes
FGOALS-s2 128 108 Yes Yes
GFDL-CM3 144 90 Yes Yes
GFDL-ESM2G 144 90 Yes Yes
GFDL-ESM2M 144 90 Yes Yes
HadCM3 96 73 Yes Yes
HadGEM2-AO 192 145 Yes Yes
HadGEM2-CC 192 145 Yes Yes
HadGEM2-ES 192 145 Yes Yes
INMCM4 180 120 Yes Yes
IPSL-CM5A-LR 96 96 Yes Yes
IPSL-CM5A-MR 144 143 Yes Yes
IPSL-CM5B-LR 96 96 Yes Yes
MIROC-ESM 128 64 Yes Yes
MIROC-ESM-CHEM 128 64 Yes Yes
MIROC4h 640 320 Yes Yes
MIROC5 256 128 Yes Yes
MPI-ESM-LR 192 96 Yes Yes
MPI-ESM-MR 192 96 Yes Yes
MPI-ESM-P 192 96 Yes Yes
MRI-CGCM3 320 160 Yes Yes
MRI-ESM1 320 160 Yes Yes
NorESM1-M 144 96 Yes YesCMIP6 Models Longs Lats TAS PR
ACCESS-CM2 192 144 Yes Yes
ACCESS-ESM1-5 192 145 Yes Yes
AWI-CM-1-1-MR 384 192 Yes Yes
AWI-ESM-1-1-LR 192 96 Yes Yes
BCC-ESM1 128 64 Yes Yes
CESM2 288 192 Yes Yes
CESM2-FV2 144 96 Yes Yes
CESM2-WACCM 288 192 Yes Yes
CESM2-WACCM-FV2 144 96 Yes Yes
CMCC-CM2-HR4 288 192 Yes Yes
CMCC-CM2-SR5 288 192 Yes Yes
CMCC-ESM2 288 192 Yes Yes
CanESM5 128 64 Yes Yes
E3SM-1-0 360 180 Yes Yes
E3SM-2-0 360 180 Yes Yes
E3SM-2-0-NARRM 360 180 Yes Yes
EC-Earth3 512 256 Yes Yes
EC-Earth3-AerChem 512 256 Yes Yes
EC-Earth3-CC 512 256 Yes Yes
EC-Earth3-Veg 512 256 Yes Yes
EC-Earth3-Veg-LR 320 160 Yes Yes
FGOALS-f3-L 288 180 Yes Yes
FGOALS-g3 180 80 Yes Yes
GFDL-CM4 288 180 Yes Yes
GFDL-ESM4 288 180 Yes Yes
GISS-E2-2-G 144 90 Yes Yes
ICON-ESM-LR* N/A N/A Yes Yes
IITM-ESM 192 94 Yes Yes
INM-CM4-8 180 120 Yes Yes
INM-CM5-0 180 120 Yes Yes
IPSL-CM5A2-INCA 96 96 Yes Yes
IPSL-CM6A-LR 144 143 Yes Yes
IPSL-CM6A-LR-INCA 144 143 No Yes
KACE-1-0-G 192 144 Yes Yes
KIOST-ESM 192 96 Yes Yes
MIROC6 256 128 Yes Yes
MPI-ESM-1-2-HAM 192 96 Yes Yes
MPI-ESM1-2-HR 384 192 Yes Yes
MPI-ESM1-2-LR 192 96 Yes Yes
MRI-ESM2-0 320 160 Yes Yes
NESM3 192 96 Yes Yes
NorCPM1 144 96 Yes Yes
NorESM2-LM 144 96 Yes Yes
NorESM2-MM 288 192 Yes Yes
SAM0-UNICON 288 192 Yes Yes
TaiESM1 288 192 Yes Yes
Table 1: Details for each observed/reanalysis data product and CMIP model output. The columns
give the model name, longitude and latitude resolution, and availability of (2m) surface temperature
(TAS) and total precipitation (PR) for each dataset. All datasets were obtained on a rectangular grid
except ICON-ESM-LR, which was obtained on an icosahedral grid with 10,242 total cells.
CMIP5 and CMIP6 outputs: https://esgf-node.llnl.gov/projects/esgf-llnl/
ERA5 hourly data on single levels from 1940 to present: https://cds.climate.copernicus.
eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview
NCEP/DOE Reanalysis II: https://psl.noaa.gov/data/gridded/data.ncep.reanalysis2.
html
GPCP 1 Degree Daily Precipitation Estimate: https://www.ncei.noaa.gov/products/
climate-data-records/precipitation-gpcp-daily
17D Full SCWD Rankings and Metric Comparison
In the following pages, we provide tables detailing the climate model rankings generated by SCWD
and baseline methods from both the climate science and ML literature. First, we provide details on
the implementation of the baseline metrics. See Section 4.4 for a discussion of these results.
D.1 Baseline Climate Model Evaluation Metrics
Past metrics for climate model validation include the root mean square error (RMSE) and mean
absolute error (MAE) applied to the climatologies, or long term means of the data [Gleckler et al.,
2016]. To implement these approaches, we first require regridding the data to a common dimension.
So, we regrid the data to a common resolution of 120×60using the remapnn function from the
Climate Data Operator (CDO) command line tools. This resolution was chosen to match the grid of
locations over which the slices were computed for SCWD. Afterwards, we compute long term means
at each location, ignoring leap days, to produce 365 values representing the climatologies for each
day of the year. RMSE and MAE are computed between the climatologies for the reference dataset
and each climate model at each location. The final rankings are determined by the area-weighted
global mean over RMSE/MAE values at each location. Models marked as “NA” operate on a 360-day
calendar, making it challenging to estimate a 365-day climatology to match the reference datasets.
D.2 Baseline Wasserstein Distance Metrics
We compare our approach against existing Wasserstein-based metrics. Although these metrics have
never been used for model validation, they can still provide a sense of what our method offers over
the naive application of existing tools. We include ordinary Wasserstein distance (WD), Sliced
Wasserstein Distance (SWD), and attempted to include Convolution SWD (CSWD). For these
methods to work, we first have to regrid each model to a common set of grid points. However, unlike
the baseline RMSE and MAE-based approaches described in the previous paragraph, none of the
proposed Wasserstein methods account for area weighting of geographic data. So, we again regrid
the climate models and reference datasets to a common resolution but now using an Icosahedral grid
of 642 locations, which provides near-uniform spacing on the sphere to alleviate concerns about area
weighting. The reduced dimensionality is not ideal for evaluating climate models, but serves to make
the computation of the multivariate WD and sliced WD more feasible given the costs already incurred
from regridding. Because we are using an Icosahedral grid to account for area weighting, we are
unable to compute convolution sliced WD, which is designed for square grids.
181.9641.8161.382
1.614
2.7741.476
1.7341.419
1.6791.561
1.8961.802
2.1041.797
2.072
2.087
2.1562.016
2.2541.943
2.051
2.119
2.7381.8061.544
3.7262.519
2.9962.2361.937
1.8951.803
1.960
2.430
3.0061.9491.404
1.600
1.5811.412
2.011
2.6381.7741.476
2.0911.797
1.8401.6521.277
1.464
2.5661.373
1.5841.311
1.5261.459
1.7911.694
1.9131.689
1.973
1.997
2.0871.946
2.1921.877
1.971
2.004
2.6031.7251.443
3.6442.396
2.9052.0351.736
1.7521.697
1.791
2.338
2.8781.8141.294
1.450
1.4671.197
1.887
2.4921.6221.363
1.9801.680
1.5971.3651.044
1.150
2.2121.149
1.2961.074
1.2461.228
1.5591.455
1.5671.456
1.775
1.815
1.9391.795
2.0541.737
1.817
1.751
2.3251.5521.196
3.4322.010
2.7091.6291.364
1.4501.458
1.499
2.139
2.6111.5211.039
1.117
1.1980.923
1.623
2.1951.3411.121
1.7471.443
0.0920.7670.308
0.476
0.9050.449
0.3310.277
0.4520.722
0.7020.592
0.2650.182
0.541
0.658
0.2380.207
0.8420.475
0.150
0.614
0.6951.0080.377
1.8320.489
0.6920.2520.435
0.4460.557
0.269
0.631
1.5310.1740.389
0.070
0.1610.308
0.133
1.2180.6450.095
0.7440.141
1.8681.9221.518
1.792
2.6801.515
1.7291.495
1.7131.714
1.7211.645
2.1231.773
1.999
1.990
1.9551.868
1.9721.807
1.993
1.886
2.1801.6591.496
2.8022.416
2.4962.0861.977
1.9921.709
NA
2.178
2.3601.8971.582
1.706
1.6131.456
2.097
2.3011.8281.468
1.7181.601
1.6381.6961.309
1.573
2.4221.307
1.5011.288
1.4891.506
1.4631.396
1.8771.522
1.748
1.755
1.7381.656
1.7611.596
1.768
1.635
1.9181.4301.263
2.5332.095
2.2581.8211.724
1.7181.454
NA
1.927
2.1031.6681.368
1.483
1.3741.341
1.774
2.0251.6071.262
1.4861.381
94.31993.12286.069
91.583
110.31085.850
92.04685.684
91.09389.084
91.99690.681
102.33893.189
96.480
96.134
97.00794.194
96.69993.386
96.038
96.587
108.21189.50987.304
122.535103.412
110.685101.02498.534
94.49994.189
96.472
102.481
108.78293.73485.374
89.824
92.14963.307
96.014
108.40093.45286.775
96.72591.811
1.6851.6211.214
1.544
2.5601.321
1.6581.276
1.6411.409
1.6771.548
2.1061.558
1.820
1.740
1.8121.738
1.8101.695
1.861
1.703
2.4791.3791.289
2.8512.280
2.5251.9171.832
1.6761.618
1.835
2.065
2.4231.6611.322
1.539
1.5101.455
1.844
2.3291.8191.399
1.7251.606SCWD
(500km)SCWD
(1000km)SCWD
(2500km)GMWDRMSE
(climatology)MAE
(climatology)WD
(icosahedral)SWD
(icosahedral)
GISS−E2−2−GIITM−ESMMIROC6FGOALS−g3BCC−ESM1NorCPM1ICON−ESM−LRKIOST−ESMEC−Earth3−CCEC−Earth3INM−CM4−8FGOALS−f3−LE3SM−2−0−NARRMSAM0−UNICONE3SM−2−0EC−Earth3−Veg−LREC−Earth3−AerChemCanESM5NESM3EC−Earth3−VegACCESS−CM2MPI−ESM−1−2−HAMCMCC−CM2−SR5KACE−1−0−GIPSL−CM5A2−INCAINM−CM5−0GFDL−CM4IPSL−CM6A−LRCMCC−ESM2E3SM−1−0TaiESM1ACCESS−ESM1−5NorESM2−LMCESM2−FV2CESM2−WACCM−FV2MRI−ESM2−0AWI−ESM−1−1−LRCMCC−CM2−HR4MPI−ESM1−2−LRGFDL−ESM4CESM2NorESM2−MMCESM2−WACCMMPI−ESM1−2−HRAWI−CM−1−1−MRNCEP ReanalysisCMIP6 Surface Temperature RankingsTable 2: CMIP6 model rankings for (2m) surface temperature based on similarity to the ERA5
Reanalysis. Distances are calculated using our proposed spherical convolutional WD (SCWD) as well
as the global mean-based WD (GMWD), RMSE and MAE (both computed on the climatologies), WD
and Sliced WD (both computed on data regridded to an icosahedral grid). For the SCWD calculations,
three different range parameters are chosen for the Wendland kernel: 500km, 1000km (our proposed
choice), and 2500km. Color fill is unique to each column in the table, and is calculated using ranks.
192.816
2.8272.327
2.745
4.5752.425
2.1342.399
2.199
2.6012.5782.602
3.3032.432
3.042
3.1202.028
2.1202.043
2.083
2.0801.804
4.6143.4383.064
3.194
2.847
2.8492.616
2.6582.973
2.9962.845
2.9262.789
3.0082.592
2.6812.288
2.3532.453
3.7382.446
2.4692.1371.905
3.0642.593
1.796
1.9791.802
2.214
3.4581.597
1.6901.640
1.750
1.8181.8181.787
2.0881.800
1.861
1.8731.589
1.6681.613
1.646
1.6471.133
2.7522.1571.836
1.970
2.201
2.2951.964
2.0271.971
2.3211.895
1.9521.759
2.1901.683
2.1441.760
1.8511.749
2.4901.967
1.9801.6861.335
1.9841.723
1.062
1.1581.074
1.349
1.4910.891
1.0140.889
1.037
1.1121.1151.073
1.1231.055
0.967
0.9530.968
1.0310.998
1.009
1.0060.701
1.2641.1360.917
1.008
1.366
1.4131.177
1.3331.207
1.3801.143
1.1631.044
1.2390.946
1.2941.040
1.0931.044
1.2891.196
1.2330.9720.762
1.0661.028
0.447
0.5460.234
0.160
0.1100.263
0.2650.246
0.262
0.2910.4120.402
0.2090.359
0.276
0.2760.236
0.2410.275
0.249
0.1930.252
0.1910.0970.225
0.271
0.152
0.1180.240
0.3680.359
0.1430.345
0.3490.340
0.0910.486
0.2380.215
0.1670.304
0.5810.181
0.0800.1820.167
0.3320.384
3.059
2.9652.769
2.800
3.1412.678
2.6482.648
2.696
2.8272.7102.675
2.9732.715
2.736
2.7322.608
2.6562.660
2.656
2.6181.936
3.1553.1362.832
2.932
2.857
3.0162.937
2.7902.844
2.7962.948
2.963NA
2.7572.840
2.9022.765
2.7262.767
2.7582.937
2.7012.6742.579
2.7972.652
2.179
2.1522.009
2.035
2.1561.930
1.9101.909
1.948
2.0151.9861.969
2.0901.976
1.942
1.9431.906
1.9391.945
1.936
1.9021.349
2.1992.2081.980
2.040
2.092
2.1532.115
2.0612.069
2.0392.098
2.101NA
1.9832.048
2.1001.994
1.9732.014
1.9432.073
1.9351.9161.845
2.0151.948
209.684
197.594185.090
176.481
219.888184.416
176.164182.414
176.087
186.971179.817179.970
198.500178.568
184.789
187.589175.419
176.499177.141
176.401
172.882155.514
226.689219.320199.471
202.174
180.662
185.359188.449
180.967190.630
173.032203.981
204.900203.446
179.854195.362
181.616183.813
177.289186.046
215.618184.775
172.650180.257182.057
187.960180.010
2.051
1.5381.288
1.739
2.4360.945
1.1290.903
1.249
1.1031.1041.058
1.5571.154
1.033
1.0301.136
1.1281.159
1.183
1.2640.928
2.8392.4601.425
1.601
1.519
1.4281.209
1.3071.230
1.7661.700
1.8001.868
1.2221.294
1.5521.140
1.3801.148
2.5331.083
1.5301.0510.766
1.0940.995SCWD
(500km)SCWD
(1000km)SCWD
(2500km)GMWDRMSE
(climatology)MAE
(climatology)WD
(icosahedral)SWD
(icosahedral)
BCC−ESM1FGOALS−f3−LNCEPIPSL−CM5A2−INCAICON−ESM−LRAWI−ESM−1−1−LRGISS−E2−2−GKIOST−ESMFGOALS−g3MPI−ESM−1−2−HAMCanESM5INM−CM4−8SAM0−UNICONNorCPM1ACCESS−ESM1−5INM−CM5−0GFDL−ESM4NESM3IITM−ESMIPSL−CM6A−LR−INCAIPSL−CM6A−LRE3SM−2−0−NARRME3SM−2−0MPI−ESM1−2−LRGFDL−CM4CMCC−CM2−HR4CMCC−CM2−SR5AWI−CM−1−1−MRE3SM−1−0ACCESS−CM2CMCC−ESM2MPI−ESM1−2−HRKACE−1−0−GCESM2−WACCM−FV2MRI−ESM2−0TaiESM1CESM2−FV2NorESM2−LMMIROC6EC−Earth3−AerChemEC−Earth3−Veg−LREC−Earth3−VegCESM2−WACCMEC−Earth3−CCCESM2EC−Earth3NorESM2−MMERA5CMIP6 Total Precipitation RankingsTable 3: CMIP6 model rankings for total precipitation based on similarity to the GPCP observations.
Distances are calculated using our proposed spherical convolutional WD (SCWD) as well as the
global mean-based WD (GMWD), RMSE and MAE (both computed on the climatologies), WD and
Sliced WD (both computed on data regridded to an icosahedral grid). For the SCWD calculations,
three different range parameters are chosen for the Wendland kernel: 500km, 1000km (our proposed
choice), and 2500km. Color fill is unique to each column in the table, and is calculated using ranks.
201.8481.807
2.6142.1971.984
2.018
2.9612.2671.921
3.1862.8771.877
2.3092.203
2.5751.977
2.3502.014
2.1811.833
3.3612.1412.1181.620
2.3571.5301.4911.491
2.2242.1871.412
1.816
2.6231.6831.612
2.3892.1001.865
1.891
2.7632.0571.841
3.0002.6621.745
2.1552.049
2.4021.815
2.2051.857
2.0591.719
3.2381.8961.8701.516
2.2351.3901.3561.347
2.1202.0841.197
1.634
2.4531.3851.238
2.0881.8611.617
1.618
2.3931.7031.650
2.7032.2531.446
1.8681.763
2.1111.516
1.9501.582
1.8051.448
2.9291.5591.5281.292
1.9721.0891.0701.039
1.8811.8480.923
1.310
2.1010.0550.209
0.1470.2730.157
0.303
0.9770.2650.862
1.6801.0340.371
0.3480.192
0.6020.305
0.6280.336
1.1110.307
0.6260.2760.3220.648
0.8550.1320.2920.157
0.1670.1080.308
0.485
0.3811.7801.826
2.5491.9811.999
1.986
2.4452.1881.994
2.7622.5581.884
2.2732.189
NANA
NANA
2.1801.869
2.7412.1702.1621.638
2.0131.6871.6611.671
2.0902.0881.456
1.855
2.3581.5311.593
2.2801.7461.767
1.720
2.1271.9141.777
2.4922.2631.634
1.9871.909
NANA
NANA
1.9361.637
2.4211.9051.9021.408
1.7871.4581.4311.440
1.8061.8081.341
1.623
2.06394.30093.936
109.00099.12297.743
97.237
115.992104.64793.500
121.518114.42992.071
101.759100.727
111.24696.722
102.30497.603
97.74891.688
117.34898.93798.74385.834
96.78089.55789.36488.946
99.78898.95363.307
94.197
111.1531.6331.735
2.3461.9421.813
1.828
2.6122.2131.720
2.8642.5261.626
1.9751.912
2.4041.692
2.2051.845
1.9771.643
2.7312.0542.0451.417
1.9651.5021.5061.505
1.9651.9461.455
1.775
2.270SCWD
(500km)SCWD
(1000km)SCWD
(2500km)GMWDRMSE
(climatology)MAE
(climatology)WD
(icosahedral)SWD
(icosahedral)
IPSL−CM5B−LRFGOALS−g2CSIRO−Mk3−6−0FGOALS−s2inmcm4HadCM3CMCC−CESMMIROC5HadGEM2−CCGFDL−ESM2GMRI−CGCM3CMCC−CMMRI−ESM1IPSL−CM5A−LRCanESM2GFDL−ESM2MMIROC−ESMCNRM−CM5MIROC−ESM−CHEMCMCC−CMSHadGEM2−ESEC−EARTHHadGEM2−AOGFDL−CM3IPSL−CM5A−MRACCESS1−0NorESM1−MACCESS1−3MIROC4hMPI−ESM−LRMPI−ESM−MRMPI−ESM−PNCEP ReanalysisCMIP5 Surface Temperature RankingsTable 4: CMIP5 model rankings for (2m) surface temperature based on similarity to the ERA5
Reanalysis. Distances are calculated using our proposed spherical convolutional WD (SCWD) as well
as the global mean-based WD (GMWD), RMSE and MAE (both computed on the climatologies), WD
and Sliced WD (both computed on data regridded to an icosahedral grid). For the SCWD calculations,
three different range parameters are chosen for the Wendland kernel: 500km, 1000km (our proposed
choice), and 2500km. Color fill is unique to each column in the table, and is calculated using ranks.
212.257
2.642
2.6853.0112.498
3.7642.8982.8552.7672.3341.804
2.774
3.1992.695
2.971
3.2802.5502.3872.3372.274
2.8822.914
4.5042.727
2.7492.8092.380
2.527
2.652
2.686
3.590
3.698
3.7382.435
3.1511.665
1.913
2.1292.0492.002
2.4122.3262.0281.9891.8591.133
1.913
2.3672.011
2.242
2.2792.0271.7331.7131.668
2.2302.189
2.6302.211
2.2301.8171.752
2.009
2.129
2.148
2.276
2.288
2.4901.937
2.5701.033
1.146
1.3191.1561.214
1.2181.4041.1881.1851.1560.701
1.109
1.3421.194
1.353
1.2321.2661.0671.0581.039
1.3331.315
1.1681.399
1.4171.0801.082
1.169
1.253
1.255
1.248
1.243
1.2891.212
1.6210.410
0.487
0.2040.2270.251
0.3840.2130.1110.1070.1990.252
0.132
0.0860.319
0.309
0.3170.2340.4280.3640.394
0.0800.144
0.1450.134
0.1180.2780.533
0.275
0.331
0.265
0.237
0.257
0.5810.147
0.4812.887
2.930
2.7983.0572.893
2.8282.9132.7132.7192.5811.936
2.969
2.9462.771
2.765
2.772NANANANA
2.7092.787
3.2282.718
2.7152.9372.772
2.896
2.956
2.927
3.148
3.164
2.7582.725
2.6892.099
2.138
2.0452.1732.080
2.0392.1361.9541.9611.8791.349
2.104
2.1092.006
2.009
2.005NANANANA
1.9682.030
2.1961.965
1.9612.0532.038
2.091
2.127
2.119
2.204
2.210
1.9431.959
2.069196.045
193.608
176.287206.521192.831
189.153182.964179.501179.564171.102155.514
202.659
194.122177.580
176.612
178.057176.560199.118195.209196.479
169.559177.914
219.657170.350
170.061204.359185.439
190.268
193.569
190.158
214.016
216.396
215.618172.984
168.1841.417
1.500
1.6531.8031.194
1.2541.5201.2571.2511.4620.928
1.566
1.4321.441
1.568
1.3861.7441.4461.2741.339
1.8671.576
2.5611.896
1.9071.7651.132
1.292
1.328
1.335
2.192
2.297
2.5331.463
2.351SCWD
(500km)SCWD
(1000km)SCWD
(2500km)GMWDRMSE
(climatology)MAE
(climatology)WD
(icosahedral)SWD
(icosahedral)
IPSL−CM5B−LRinmcm4NCEPCNRM−CM5FGOALS−s2CSIRO−Mk3−6−0MRI−ESM1GFDL−ESM2MMRI−CGCM3GFDL−ESM2GIPSL−CM5A−LRMIROC−ESM−CHEMMIROC−ESMIPSL−CM5A−MRMPI−ESM−PMPI−ESM−MRCMCC−CESMCMCC−CMCanCM4HadCM3GFDL−CM3MPI−ESM−LRCMCC−CMSCanESM2NorESM1−MFGOALS−g2ACCESS1−3EC−EARTHMIROC4hMIROC5HadGEM2−AOHadGEM2−CCHadGEM2−ESACCESS1−0ERA5CMIP5 Total Precipitation RankingsTable 5: CMIP5 model rankings for total precipitation based on similarity to the GPCP observations.
Distances are calculated using our proposed spherical convolutional WD (SCWD) as well as the
global mean-based WD (GMWD), RMSE and MAE (both computed on the climatologies), WD and
Sliced WD (both computed on data regridded to an icosahedral grid). For the SCWD calculations,
three different range parameters are chosen for the Wendland kernel: 500km, 1000km (our proposed
choice), and 2500km. Color fill is unique to each column in the table, and is calculated using ranks.
22E Investigation of Differences Between Rankings
The impacts of climate change on temperature are not limited to changes in the local means and may
include changes in variance or other moments. This phenomenon is documented in Hansen et al.
[2012], where extreme tail behavior in local temperature anomalies are shown to occur alongside
warming in the mean climate state. We construct the following synthetic experiment to assess the
ability of each climate model evaluation method (SCWD, GMWD, RMSE, and MAE) to detect
changes in regional means and variances simultaneously. Based on these findings, we re-examine the
CMIP6 surface temperature rankings in Table 2 and show that RSME/MAE rank SAM0-UNICON
artificially high due to their inability to detect variance changes in the temperature anomalies.
E.1 Synthetic Experiment
To generate a realistic synthetic example, we start with the ERA5 data and linearly transform it
to modify its mean and variance structure. Specifically, let Ybe the ERA5 reanalysis surface
temperature data. Ycan be separated into the climatology, C, which is the temporal mean climate
state for each location, and the anomalies A=Y−C. We compare Y=C+Ato modified datasets
of the form:
YM,s=C+M+s∗A
which are alternate versions of ERA5 with a mean shift of Mand anomaly scale factor of s. We
consider s= 1.1,s= 1.3, ands= 1.5, which respectively represent 10%, 30%, and 50% increases
in the variance of the anomalies. We also consider three cases of Mwhich vary in the Northern
and Southern Hemispheres (NH and SH), including (+0.5K,+0K)in NH/SH, (+1K,−0.5K)in
NH/SH, and (+1.5K,−1K)in NH/SH. These are chosen to provide the same global mean change
with different interhemispheric temperature asymmetries, which are an important climate feature
[Friedman et al., 2013]. For each value of Mands, we compare YM,stoYusing all four metrics.
0.433 0.829 1.2990.828 1.088 1.4781.296 1.476 1.7830.25 0.25 0.250.252 0.252 0.2520.255 0.255 0.2550.25 0.75 1.250.25 0.75 1.250.25 0.75 1.250.25 0.75 1.250.25 0.75 1.250.25 0.75 1.25SCWDGMWDRMSEMAE
+0.5 NH
-0 SH+1 NH
-0.5 SH+1.5 NH
-1 SH+0.5 NH
-0 SH+1 NH
-0.5 SH+1.5 NH
-1 SH+0.5 NH
-0 SH+1 NH
-0.5 SH+1.5 NH
-1 SH+0.5 NH
-0 SH+1 NH
-0.5 SH+1.5 NH
-1 SH1.11.31.5
Mean shift in NH/SH (M)Anomaly scale factor (s)
Figure 5: Results from the simulation comparing the original ERA5 data to synthetic modifications.
Each panel gives the results for a different distance function. The y axis represents the anomaly
scale parameter, s, and the x axis represents the Mparameter which controls mean shift in the
northern/southern hemispheres. For each method, the distance is provided from the original ERA5
data to the modified ERA5 data with each of the nine combinations of Mands. The color fill is
determined by the rankings within each method, with light yellow representing a low ranking and
dark red representing a high ranking.
The results are shown in Figure 5. Because each Mhas the same global mean, GMWD does not
change with M, although it does (very slightly) increase for higher values of s. Conversely, sscales
only the variance of the anomalies and does not impact the climatology, so RMSE/MAE are only able
to detect changes in M, nots. Only SCWD detects changes in both the local means and the variance
of the anomalies in this example.
E.2 Investigating Differences in CMIP6 Rankings
The experiment demonstrates that SCWD is able to distinguish additional sources of variability
beyond just differences in the climatological mean state. To understand how this ability impacts our
climate model rankings, we investigate SAM0-UNICON, which ranks poorly using SCWD relative
23to RMSE/MAE. Looking at the SCWD map in the supplemental material, SAM0-UNICON exhibits
the highest local WD values in regions such as the coast of Antarctica. We average over the region
where the local WD is the highest and compare the climatologies and anomalies for ERA5 and
SAM0-UNICON in Figure 6.
0.000.050.100.15
-10 0 10 20
Temperature Anomalies (K)DensityAnomalies
SD(ERA5 anomalies) = 3.52
SD(SAM0 anomalies) = 5.08-20-10
0 100 200 300
Day of the yearTemperature (K)ERA5 Reanalysis
SAM0-UNICONClimatologies
Figure 6: Comparison of anomalies and climatologies in the region where SAM0-UNICON has the
largest local WD values (greater than 7.91). ERA5 is shown in blue and SAM0-UNICON is shown in
red. The left plot is a density plot showing the distribution of the anomalies, and the right plot is a
time series plot showing the climatologies starting on January 1st and ending on December 31st.
Relative to ERA5, we see a large difference in the climatology for SAM0-UNICON, which will
impact both SCWD and RMSE/MAE. However, there is also a large increase in the variance of
the anomalies in SAM0-UNICON, which is only measured by SCWD. We have demonstrated that
this region has both high local WD values and large differences that are undetected by typical
comparisons of the climatologies. So, this region is likely responsible in part for the harsher rankings
of SAM0-UNICON by SCWD compared to RMSE/MAE.
24NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Abstract states that we introduce a new similarity measure and apply it to
evaluate CMIP climate models. These claims are addressed in Sections 3.2 and 4.1. Abstract
also states that we found modest improvements from CMIP5 to CMIP6, which is addressed
in Section 4.3. Introduction states that compared to a previous WD-based similarity measure
for climate model data, our proposed method accounts for more spatial features (addressed
in Section 3.2) and provides a more robust evaluation (addressed in Section 4.4 from an
applied and experimental perspective).
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Limitations are addressed in the second paragraph of Section 5.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
253.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Theoretical results are stated in Theorem 3.3 and proven in Appendix A.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Full theoretical details on our method are included in Section 3.2, the algorithm
is provided in Appendix B, and the code is provided in the supplement. All data are publicly
available, with full details in Appendix C. For the synthetic experiment, all details on
generating the data are included in Appendix E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
26(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Full theoretical details on our method are included in Section 3.2, the algorithm
is provided in Appendix B, and the code to reproduce the SCWD calculations for CMIP5
and CMIP6 to the reference datasets is provided in the supplemental material. All data are
publicly available, with full details in Appendix C. Note that the data cannot be directly
included in the supplement due to their large size (around 2TB before preprocessing).
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: For the main analysis, specification of the kernel function is in Section 3.2,
and specification of the range parameter and reference datasets is in Section 4.1. For the
additional synthetic experiment, all details are provided in Appendix E.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
27Answer: [Yes]
Justification: For our initial CMIP6 climate model rankings, there are no error bars because
we compute pairwise distances between whole climate model runs, of which we do not
have replicates to measure the full variability of the pairwise distances. Because our slicing
mechanism is non-random, there is no Monte Carlo error to report. We do, however, include
other appropriate information about the significance of these rankings. In particular, we
study the sensitivity of our results to the kernel range parameter in Section 4.4. For the
claims regarding the progression from CMIP5 to CMIP6, variability information is included
in the form of boxplots in Appendix D.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: This is addressed in Appendix B. The overall computing requirement is low,
with the caveat of requiring a large amount of storage space for the climate model and
reanalysis datasets.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: No human subjects were involved in this research and all analyses were based
on publicly available datasets. All required details were documented and algorithms and
code are provided.
28Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Climate models are the primary tool for climate prediction and projection.
However, deficiencies persist among climate models despite decades of improvement. Our
paper offers a succinct and informative proposal of a new method for evaluating climate
models against observational and reanalysis data, both on the global and regional scales. It
provides valuable guidance for climate model improvement and can assist in selecting more
reliable models to reduce uncertainty in climate projections, which is crucial for climate
mitigation and adaptation efforts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our paper poses no such risks. Our method is built to evaluate and explain
discrepancies in climate model distributions (in terms of specific regions), which could aid
in responsible use of climate model projections.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
29•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We credit the creators of the climate model and reanalysis datasets in Appendix
C along with citations throughout the paper.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The code is provided in the supplement, with additional theoretical documen-
tation in Section 3.2 and technical documentation in Section B.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing or human subjects were involved in this research.
Guidelines:
30•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing or human subjects were involved in this research.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
31