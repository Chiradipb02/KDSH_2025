Towards the Transferability of Rewards Recovered
via Regularized Inverse Reinforcement Learning
Andreas Schlaginhaufen
SYCAMORE, EPFL
andreas.schlaginhaufen@epfl.chMaryam Kamgarpour
SYCAMORE, EPFL
maryam.kamgarpour@epfl.ch
Abstract
Inverse reinforcement learning (IRL) aims to infer a reward from expert
demonstrations, motivated by the idea that the reward, rather than the policy, is
the most succinct and transferable description of a task [Ng et al., 2000]. However,
the reward corresponding to an optimal policy is not unique, making it unclear
if an IRL-learned reward is transferable to new transition laws in the sense that
its optimal policy aligns with the optimal policy corresponding to the expert’s true
reward. Past work has addressed this problem only under the assumption of full
access to the expert’s policy, guaranteeing transferability when learning from two
experts with the same reward but different transition laws that satisfy a specific
rank condition [Rolland et al., 2022]. In this work, we show that the conditions
developed under full access to the expert’s policy cannot guarantee transferability
in the more practical scenario where we have access only to demonstrations of the
expert. Instead of a binary rank condition, we propose principal angles as a more
refined measure of similarity and dissimilarity between transition laws. Based on
this, we then establish two key results: 1) a sufficient condition for transferability
to any transition laws when learning from at least two experts with sufficiently
different transition laws, and 2) a sufficient condition for transferability to local
changes in the transition law when learning from a single expert. Furthermore, we
also provide a probably approximately correct (PAC) algorithm and an end-to-end
analysis for learning transferable rewards from demonstrations of multiple experts.
1 Introduction
Reinforcement learning (RL) has achieved remarkable success in various domains such as robotics
[Hwangbo et al., 2019], autonomous driving [Lu et al., 2023], or fine-tuning of large language
models [Stiennon et al., 2020]. Despite these advances, a key challenge lies in designing appropriate
reward functions that reflect the desired outcomes and align with human values. Misaligned rewards
can lead to suboptimal behaviors [Ngo et al., 2022], undermining the potential benefits of RL in
practical scenarios. Inverse reinforcement learning (IRL), also known as inverse optimal control
[Kalman, 1964] or structural estimation [Rust, 1994], addresses this problem by inferring a reward
from demonstrations of an expert acting optimally in a Markov decision process (MDP).
Compared to behavioral cloning [Pomerleau, 1988], which directly fits a policy to the expert’s
demonstrations, IRL is believed to provide a more transferable description of the expert’s task [Ng
et al., 2000], as recovering the expert’s underlying reward would enable us to train a policy in a new
environment with different dynamics. However, it is also known that the reward corresponding to
some optimal policy is not unique [Ng et al., 1999], making it difficult to recover the expert’s true
underlying reward. This raises the question: Is a reward recovered via IRL transferable to a new
38th Conference on Neural Information Processing Systems (NeurIPS 2024).environment in the sense that its optimal policy aligns with the expert’s true reward? For example, in
autonomous driving, could we effectively reuse a reward learned from demonstrations of one car in a
given city to train or fine-tune a policy for another car in another city?
Ensuring transferability is challenging, as neither the optimal policy corresponding to a reward nor
the reward corresponding to an optimal policy is unique. This leads to trivial solutions to the IRL
problem, such as constant rewards that make all policies optimal. Common approaches to address
this challenge include characterizing the entire set of rewards for which the expert is optimal [Metelli
et al., 2021], or assuming the expert is optimal with respect to an entropy regularized RL problem
[Ziebart, 2010], leading to many popular IRL and imitation learning algorithms [Ho and Ermon,
2016, Fu et al., 2017, Garg et al., 2021]. Entropy regularization results in a unique and more uniform
optimal policy, serving as a model for the expert’s bounded rationality [Ortega et al., 2015].
In the entropy-regularized setting, several recent works study the set of rewards for which a given
expert policy is optimal. In particular, Cao et al. [2021], Skalse et al. [2023] show that under entropy
regularization, the expert’s reward can be identified up to so-called potential shaping transformations
[Ng et al., 1999]. The authors of [Schlaginhaufen and Kamgarpour, 2023] extend this result to
more general steep regularization. Furthermore, they show that to guarantee transferability to any
transition law, the expert’s reward needs to be identified up to a constant. The latter can be achieved
either by restricting the reward class, e.g., to state-only rewards [Amin et al., 2017], or by learning
from multiple experts with the same reward but different transition laws, given that a specific rank
condition is satisfied [Cao et al., 2021, Rolland et al., 2022]. However, the above results cannot be
applied directly in practice, as they rely on having full access to the experts’ policies, whereas in
practice, we typically only have a finite set of demonstrations available.
Contributions We consider the framework of regularized IRL [Jeon et al., 2021] and address the
transferability of rewards recovered from a finite set of expert demonstrations.
•We define a novel notion of transferability (Definition 3.1), to address the practical limitation
of not having perfect access to the experts’ policies. Furthermore, we show that when
learning from finite data, the conditions developed under full access to the experts’ policies
are not sufficient to guarantee transferability (Example 3.3).
•Instead of a binary rank condition, we propose to use principal angles to characterize the
similarity and dissimilarity between transition laws (Definition 3.8). Based on these principal
angles, we then establish two key transferability results: 1) a guarantee for transferability
to any transition laws when learning from at least two experts with sufficiently different
transition laws (Theorem 3.10), and 2) a guarantee for transferability to local changes in the
transition law when learning from a single expert (Theorem 3.11).
•Assuming oracle access to a probably approximately correct (PAC) algorithm for the forward
RL problem, we provide a PAC algorithm for the IRL problem, which in O(K2/ˆε2)steps
recovers a reward for which, with high probability, all Kexperts are ˆε-optimal (Theorem 4.1).
Together with our results on transferability, this establishes end-to-end guarantees for
learning transferable rewards from a finite set of expert demonstrations.
• We experimentally validate our results in a gridworld environment (Section 5).1
2 Background
Notation Given xandyin some Euclidean vector space V, we denote the p-norm by ∥x∥p, the
orthogonal projection onto a closed convex set X ⊂ V byΠX(x) = arg min y∈X∥x−y∥2, and the
standard dot product by ⟨x, y⟩. For a linear operator A, we denote its image and rank by imAand
rankA, respectively. Given two sets XandY, we denote X+Yfor their Minkowski sum and YX
for the set of all functions mapping from XtoY. Additionally, we denote ∆Xfor the probability
simplex over Xand 1for the indicator function. The interior intX, the relative interior relintX, the
relative boundary relbdX, and the convex hull convXof some set Xare defined in Appendix A,
along with an overview of all other notations.
1The code is openly accessible at https://github.com/andrschl/irl_transferability .
2Regularized MDPs We consider a regularized MDP [Geist et al., 2019] defined by a tuple
(S,A, P, ν 0, r, γ, h ). Here, SandArepresent finite state and action spaces with |S|,|A|>1,
ν0∈∆Sthe initial state distribution, P∈∆S×A
S the transition law, r∈RS×Athe reward, and
γ∈(0,1)the discount factor. Furthermore, h:X →Ris a strictly convex regularizer that is defined
on a closed convex set X ⊆RAwithrelint ∆ A⊆intX. Starting from some initial state s0∼ν0the
agent can at each step in time t, choose an action at∈ A, will arrive in state st+1∼P(·|st, at), and
receives reward r(st, at). The goal is to find a Markov policy π∈∆S
Amaximizing the regularized ob-
jective Eπ[P∞
t=0γt[r(st, at)−h(π(·|st))]]. Following the classical linear programming approach
to MDPs [Puterman, 2014], this can be cast equivalently as the convex optimization problem
max
µ∈MJ(r, µ),with J(r, µ):=⟨r, µ⟩ −¯h(µ), (O-RL)
whereMdenotes the set of occupancy measures, µπ(s, a):= (1−γ)Eπ[P∞
t=0γt1(st=s, at=a)],
and we have ¯h(µ):=E(s,a)∼µ[h(πµ(·|s))], with πµbeing the policy corresponding to µ(see
Appendix A). The set of occupancy measures is characterized by the Bellman flow constraints
M=
µ∈RS×A
+: (E−γP)⊤µ= (1−γ)ν0	
⊆∆S×A,
where E:RS→RS×AandP:RS→RS×Aare the linear operators mapping v∈RSto
(Ev)(s, a) =v(s)and(Pv)(s, a) =P
s′P(s′|s, a)v(s′), respectively.
Due to the strict convexity of h, the regularized MDP problem has a unique optimal policy [Geist
et al., 2019], hence guaranteeing the uniqueness of the optimal occupancy measure in (O-RL) . In
addition, we assume that the gradients of hbecome unbounded towards the relative boundary of the
simplex as detailed in Assumption 2.1 below.
Assumption 2.1 (Steep regularization) .Suppose that h:X →Ris differentiable in intXand that
liml→∞∥∇h(pl)∥2=∞if(pl)l∈Nis a sequence in intXconverging to a point p∈relbd ∆ A.
Assumption 2.1 ensures that the optimal policy is non-vanishing, and together with Assumption 2.2
below, we also have that the optimal occupancy measure is non-vanishing.
Assumption 2.2 (Exploration) .Letν(s):=P
aµ(s, a)≥νmin>0for any s∈ S andµ∈ M .
One way to guarantee Assumption 2.2 is to impose a lower bound on the initial state distribution ν0.
In the following, it will be convenient to denote the optimal solution to (O-RL) for the reward ras
RL(r):= argmax
µ∈MJ(r, µ),
and the suboptimality of some occupancy measure µfor the reward ras
ℓ(r, µ):= max
µ′∈MJ(r, µ′)−J(r, µ). (1)
That is, µ=RL(r)if and only if ℓ(r, µ) = 0 .
Remark 2.3.As we aim to analyze the transferability of rewards to new transition laws P∈∆S×A
S,
it will often be useful to explicitly specify the dependency on P. We do so by adding a subscript –
e.g. we write MP,RLP, andℓP. However, for better readability, we drop these subscripts whenever
there is no potential for confusion.
Inverse reinforcement learning Given a dataset of trajectories sampled from an expert µEthat is
optimal for some reward rE, the goal in IRL is to recover a reward ˆr∈ R, within a predefined reward
classR ⊆RS×A, such that the expert is optimal for ˆr. That is, ideally, we aim to find a reward in the
feasible reward set
IRL(µE):=
r∈ R:µE∈RL(r)	
. (2)
However, since we don’t have direct access to the expert’s policy but only to a finite set of demon-
strations, the best we can hope for is an algorithm that with high probability outputs a reward ˆr∈ R
such that ℓ(ˆr, µE)is small – i.e. an algorithm that is PAC [Syed and Schapire, 2007].
3Reward equivalence The reward corresponding to an optimal occupancy is not unique. For
example, all rewards in the affine subspace r+U, where U:= im( E−γP)is the subspace of
so-called potential shaping transformations, correspond to the same optimal occupancy measure
[Ng et al., 1999]. From a geometric perspective, the subspace U=im(E−γP)lies perpendicular
to the set of occupancy measures M. Therefore, adding an element of Uto the reward leaves the
performance difference between any two occupancy measures invariant. Hence, it is often convenient
to consider these rewards as equivalent [Kim et al., 2021] and to measure distances between rewards
in the resulting quotient space. Given a linear subspace V ⊂RS×A, the quotient space RS×A/Vis
the set of all equivalence classes [r]V:=
r′∈RS×A:r′−r∈ V	
, which is itself a vector space
with addition and multiplication operation defined by [r]V+ [r′]V= [r+r′]Vandc[r]V= [cr]V
forc∈R. Intuitively, RS×A/Vis the vector space obtained by collapsing Vto zero, or in other
words, it is isomorphic to the orthogonal complement of V. We endow RS×A/Vwith the quotient
norm∥[r]V∥2:= min v∈V∥r+v∥2=∥ΠV⊥r∥2and we say that randr′are close in RS×A/V
if∥[r]V−[r′]V∥2is small. Moreover, the expert’s reward is said to be identifiable up to some
equivalence class [·]VifIRL(µE)⊆[rE]V. In this paper, we will consider the equivalence relations
induced by constant shifts, i.e., V=1:=
r∈RS×A:r(s, a) =c∈R	
, and by potential shaping
transformations, i.e., V=U. Note that since 1is a subspace of UandUis|S|-dimensional, [r]1is a
strict subset of [r]Uwhenever |S|>1.
3 Transferability
In this section, we present our main results on transferability in IRL. To this end, we first introduce
the problem of learning ε-transferable rewards from multiple experts acting in different environments.
3.1 Problem formulation
LetR ⊆RS×Abe a compact reward class, and suppose we are given access to Kexpert data sets,
DE
k=n
sk,i
0, ak,i
0, . . . , sk,i
HE−1, ak,i
HE−1oNE−1
i=0, k = 0, . . . , K −1,
consisting of trajectories sampled independently from the experts µE
P0, . . . , µE
PK−1. Each expert is
optimal for the same unrevealed reward rE∈ R, but under different transition laws, P0, . . . , PK−1.
Our goal is to recover a reward ˆr∈ R that is transferable across a set of transition laws P ⊆∆S×A
S.
Specifically, the optimal occupancy measure corresponding to ˆrshould remain approximately optimal
forrEunder every transition law in P. This yields the following definition of ε-transferability.
Definition 3.1 (ε-transferability) .Fix some ε >0. We say that ˆrisε-transferable to some set
of transition laws P ⊆ ∆S×A
S ifℓP(rE,RLP(ˆr))≤εfor all P∈ P. We say that ˆris exactly
transferable to Pif it is ε-transferable to Pwithε= 0.
The error margin of εis crucial, as exact transferability is unrealistic when learning from finite expert
data. Moreover, note that Definition 3.1 is a definition of uniform transferability, as it requires ˆr
to be ε-transferable to any P∈ P with the same fixed ε. In the following, we will analyze the
transferability of a reward ˆrfor which all experts are ˆε-optimal for some ˆε >0. That is,
ℓPk(ˆr, µE
Pk)≤ˆε, k = 0, . . . , K −1. (3)
In particular, we aim to establish appropriate conditions for choosing ˆεso as to guarantee ε-
transferability to some set of transition laws P. In Section 4, we will then provide an IRL algorithm
that, with high probability, outputs a reward ˆrsuch that (3) holds.
Remark 3.2.As discussed in Appendix J, the assumption of perfect expert optimality with respect
torEcan be relaxed to allow for a misspecification error. All our results remain applicable in this
setting but include an additional error term due to the experts’ suboptimality.
3.2 Related work
Most previous work has focused on reward identifiability. For a single expert, Cao et al. [2021], Skalse
et al. [2023], Schlaginhaufen and Kamgarpour [2023] show that under Assumption 2.1 (steepness)
4the feasible reward set (2) can be expressed as
IRL(µE) = 
∇¯h(µE) +U
∩ R= [rE]U∩ R. (4)
In other words, steepness ensures that the expert’s reward is identifiable up to potential shaping. To
identify the reward up to a constant, we can either restrict the reward class, e.g. to state-only rewards
as explored by Amin et al. [2017], or learn from multiple experts [Cao et al., 2021, Rolland et al.,
2022]. In particular, when we are given access to two experts, µE
P0andµE
P1, we can identify the
experts’ reward up to the intersection
IRLP0(µE
P0)∩IRLP1(µE
P1) = [rE]UP0∩[rE]UP1∩ R= 
rE+UP0∩ UP1
∩ R.
That is, for the unrestricted reward class, R=RS×A, the reward is identifiable up to a constant if
and only if UP0∩ UP1=1. Or equivalently, if and only if the rank condition
rank 
E−γP0, E −γP1
= 2|S| − 1, (5)
is satisfied [Rolland et al., 2022]. Moreover, Schlaginhaufen and Kamgarpour [2023] show that
identifying the expert’s reward up to a constant is a necessary and sufficient condition for exact
transferability to any full-dimensional set P ⊆ ∆S×A
S (a setPwhose interior, with respect to the
subspace topology on ∆S×A
S [Bourbaki, 1966], is non-empty).
Limitations The above results assume perfect access to the expert’s policy, which isn’t realistic. In
practice, we can only learn a reward for which the experts are approximately optimal. In Example 3.3
below, we show that under approximate optimality of the experts, the learned reward can perform
very poorly in a new environment, even if the rank condition in Equation (5) is satisfied.
Example 3.3. We consider a two-state, two-action MDP with S=A={0,1}, uniform initial state
distribution, discount rate γ= 0.9, and Shannon entropy regularization h=−H (see Appendix C).
Suppose the expert reward is rE(s, a) = 1{s= 1}and consider the transition laws, P0andP1,
defined by P0(0|s, a) = 0 .75andP1(0|s, a) = 0 .25 + β· 1{s= 0, a= 0}for some β∈[0,0.75].
Also, consider the two experts µE
P0=RLP0(rE)andµE
P1=RLP1(rE), and suppose we recovered
the reward ˆr(s, a) =−rE. Then, as detailed in Appendix E, the following holds: 1) We have
ℓP0(ˆr, µE
P0) = 0 andℓP1(ˆr, µE
P1) =O(β). That is, for small β, the reward ˆris a good solution
to the IRL problem, as both experts are approximately optimal under ˆr. 2) The rank condition
(5)between P0andP1is satisfied for any β > 0. 3) For a new transition law Pdefined by
P(0|s, a) = 1{s= 1, a= 0}, we have ℓP(rE,RLP(ˆr))≈4.81, i.e.RLP(ˆr)performs poorly under
the experts’ reward.
3.3 Theoretical insights
To establish a sufficient condition for ε-transferability, our goal is to bound the suboptimality of an
optimal occupancy measure, RL(r), for some reward r′, in terms of reward distances measured in the
quotient space RS×A/U. To this end, we first establish the relationship between the suboptimality in
Equation (1) and the Bregman divergence corresponding to the occupancy measure regularization.
Bregman divergences The Bregman divergence [Teboulle, 1992] associated to ¯his defined as
D¯h(µ, µ′) =¯h(µ)−¯h(µ′)− ⟨∇ ¯h(µ′), µ−µ′⟩.
Proposition 3.4. Under Assumptions 2.1 and 2.2, we have ℓ(r′, µ) =D¯h(µ,RL(r′))for any µ∈ M .
Proposition 3.4 above demonstrates that the suboptimality of an occupancy measure µfor the reward
r′coincides with the Bregman divergence between µand the optimal occupancy measure under r′.
This generalizes [Mei et al., 2020, Lemma 26] from entropy regularization to any steeply regularized
MDP. The proof is presented in Appendix D.6.
Reward approximation Next, we show that under strong convexity and local Lipschitz gradients,
the Bregman divergence between two optimal occupancy measures is bounded in terms of reward
distances in RS×A/U.
5Assumption 3.5 (Regularity) .Suppose the following holds:
a) The regularizer ¯hisη-strongly convex over the set of occupancy measures M. That is, we have
¯h(µ′)≥¯h(µ) +⟨∇¯h(µ), µ′−µ⟩+η
2∥µ′−µ∥2
2,∀µ, µ′∈ M.
b)The gradient ∇¯his locally Lipschitz continuous over relintM. That is, for any closed convex
subset K ⊂relintMthere exists LK>0such that
∇¯h(µ)− ∇¯h(µ′)
2≤LK∥µ−µ′∥2,∀µ, µ′∈ K.
We will show later that Assumption 3.5 is met for Shannon and Tsallis entropy regularization (see
Proposition D.9). Under the above assumption, the following lemma establishes the desired upper
and lower bound on the Bregman divergence between two optimal occupancy measures with respect
to reward distances measured in RS×A/U.
Lemma 3.6. Suppose Assumptions 2.1,2.2, and 3.5 hold, and let r, r′∈ R. Then, we have
σR
2∥[r]U−[r′]U∥2
2≤ℓ(r′,RL(r)) =D¯h(RL(r),RL(r′))≤1
2η∥[r]U−[r′]U∥2
2, (6)
for some problem-dependent constant σR>0.
Remark 3.7.The proof of Lemma 3.6 hinges on the duality between equivalence classes of rewards
and optimal occupancy measures (see Appendix B). The main idea is to leverage duality of Bregman
divergences, and a dual smoothness and strong convexity result in Proposition D.7. A key challenge
arises because, by Assumption 2.1, the regularizer cannot be globally smooth. This results in a
problem-dependent dual strong convexity constant σR[Goebel and Rockafellar, 2008]. In Propo-
sition D.9, we will provide a lower bound on σRfor the specific choices of Shannon and Tsallis
entropy regularization. For more details, we refer to the full proof in Appendix D.6.
The above lemma has two key implications: First, the lower bound in (6)implies that if we recover a
reward ˆrfor which all experts are approximately optimal, then the distance between ˆrandrEcan
be bounded in the quotient spaces RS×A/UPk. Second, the upper bound shows that to control the
performance of RLP(ˆr)in a new environment P, we need to tightly bound the distance between ˆr
andrEinRS×A/UP. As distances in RS×A/UPare bounded by distances in RS×A/1, this can be
achieved by bounding the distance between ˆrandrEinRS×A/1. However, revisiting Example 3.3 in
light of Lemma 3.6 shows that even though ˆrandrEare close in RS×A/UPk, this does not guarantee
their proximity in RS×A/1andRS×A/UP.
Example 3.3 (continued) .Recall the definition UPk= im( E−γPk). Given that in Example 3.3
we have ℓP0(ˆr, µE
P0) = 0 andℓP1(ˆr, µE
P1) =O(β), Lemma 3.6 ensures that ˆrandrEcoincide in
RS×A/UP0, and for small β, they are close in RS×A/UP1. However, as illustrated in Figure1(a) this
doesn’t ensure that ˆrandrEare close in RS×A/1andRS×A/UP. In particular, it can be computed
that[ˆr]UP−[rE]UP
2≈1.51, which by Lemma 3.6 explains the poor transferability to P.
3.4 Sufficient conditions for transferability
With Lemma 3.6 in place, we are set to present our results on ε-transferability. Example 3.3 indicates
that a sufficient condition for learning transferable rewards from experts ( K= 2) should not rely
solely on the binary rank condition (5), which only checks if UP0∩ UP1=1. Instead, we should
consider the relative orientation between UP0andUP1. To formalize this, we need to introduce the
concept of principal angles between linear subspaces, as outlined in Definition 3.8 below.
Definition 3.8 (Principal angles [Galántai, 2013]) .LetV,W ⊆Rnbe two subspaces of dimension
m≤n. The principal angles 0≤θ1(V,W)≤. . .≤θm(V,W) =:θmax(V,W)≤π/2between V
andWare defined recursively via
cos(θi(V,W)) = max
v∈V,w∈W⟨v, w⟩s.t.∥v∥2=∥w∥2= 1,⟨v, vj⟩=⟨w, w j⟩= 0, j= 1, . . . , i −1,
where vj, wjare the maximizers corresponding to the angle θj. For two transition laws P, P′, we
define θi(P, P′):=θi(UP,UP′)and refer to θi(P, P′)as the i-th principal angles between PandP′.
6UP1
rE
^r
UP0
UP
UP0
UP1
rE


RSA=1
RSA=1
"(a) Rewards in Example 3.3. ( b) Proof sketch Theorem 3.10.
Figure 1: ( a) illustrates the equivalence classes [ˆr]Uand[rE]U, corresponding to the transition laws
P0, P1, Pfrom Example 3.3, for a small β, inRS×A/1. The blue lines correspond to P0, the red
lines to P1, and the gray lines to P. Furthermore, the shaded areas illustrate the approximation error
around [rE]UPk, as guaranteed by Lemma 3.6. ( b) illustrates the uncertainty set for the recovered
reward when learning from two experts, as discussed in the proof sketch of Theorem 3.10.
Principal angles are the natural generalization of angles between two lines or planes to higher dimen-
sional subspaces. For principal angles between transition laws, we have the following proposition.
Proposition 3.9. LetP, P′∈∆S×A
S andHγ= 1/(1−γ). Then, we have θ1(P, P′) = 0 and
sin (θmax(P, P′))≤γHγp
|S|/|A|∥P−P′∥, where ∥·∥denotes the spectral norm.
The proof can be found in Appendix D.7. The above result shows that while the first principal angle
between two transition laws is always zero, all principal angles are small if the transition laws are
close to one another. In Example 3.3, we have sin(θ2(P0, P1)) =O(β), indicating that the second
and in this case maximal principal angle is small when βis small (see Appendix E). The following
result shows that when learning from two experts, the transferability error is directly controlled by
the second principal angle between the experts’ transition laws.
Theorem 3.10. LetK= 2,θ2(P0, P1)>0, and suppose that Assumptions 2.1,2.2, and 3.5 hold. If
ℓPk(ˆr, µE
Pk)≤ˆεfork= 0,1, then ˆrisε-transferable to P= ∆S×A
S with
ε= ˆε/
ησRsin 
θ2(P0, P1
/2)2
.
Sketch of proof. The main idea of the proof is illustrated in Figure 1(b). First, it follows from
Lemma 3.6 that ˆrandrEare¯ε=p
2ˆε/σR-close in RS×A/UPkfork= 0,1, respectively. From
Figure 1(b) we see – using basic trigonometry – that this implies that ˆrandrEare at least ∆ =
¯ε/sin(θ/2)-close in RS×A/1. As shown in the full proof in Appendix F, the relevant angle, θ, is the
second principal angle θ2(P0, P1). The result then follows from the upper bound in Lemma 3.6.
Some observations are in order. First, the above theorem shows that the larger the second principal
angle between the two experts’ transition laws, the better the recovered reward transfers to a new
environment. Second, observe that θ2(P0, P1)>0is equivalent to the rank condition (5), as the
second principal angle between two subspaces is non-zero if and only if their intersection is at most
one-dimensional. Therefore, for exact transferability, Theorem 3.10 requires the rank condition
(5)to be satisfied and ˆε= 0, recovering the results by Cao et al. [2021], Rolland et al. [2022],
Schlaginhaufen and Kamgarpour [2023]. But in contrast to past results, Theorem 3.10 applies to
more realistic scenarios, where ˆεis merely small, not zero. Finally, we note that Theorem 3.10 can
be trivially generalized to K≥2experts by replacing θ2(P0, P1)with the maximum of θ2(Pk, Pl)
over0≤k≤l≤K−1. However, such bounds may be loose for K > 2, potentially leaving
considerable room for improvement in this setting.
Local transferability When learning a reward ˆrfrom a single expert ( K= 1), Schlaginhaufen and
Kamgarpour [2023] show that, without reducing the dimension of the reward class, ˆrcannot be exactly
transferable to any neighborhood of the expert’s transition law P0. However, Theorem 3.11 below
shows that by allowing for an εof error, we can guarantee transferability to a neighborhood of P0.
7Theorem 3.11. LetK= 1,D:= max r,r′∈R∥r−r′∥2, and suppose that Assumptions 2.1,2.2, and
3.5 hold. If ℓP0(ˆr, µE)≤ˆε, then ˆrisεP-transferable to P∈∆S×A
S with
εP= 2 maxn
2ˆε/σR, D2sin 
θmax(P0, P)2o
/η.
The above theorem (which is proven in Appendix G) shows that the reward learned from a single
expert transfers to transition laws that are sufficiently close to the expert’s, where the closeness is
measured in terms of the maximal principal angle. In other words, while a large second principal
angle between two experts’ transition laws, as per Theorem 3.10, ensures that the reward recovered
from these two experts is transferable to arbitrary transition laws, a small largest principal angle
between two transition laws ensures that a reward recovered in one environment can be successfully
transferred to the other environment.
Remark 3.12.As discussed in Appendix H, we can compute the principal angles using a singular
value decomposition. Moreover, given estimates ˆP0,ˆP1of the transition laws P0, P1, the error in
the estimate of sinθi(P0, P1)scales with O(max{||P0−ˆP0||,||P1−ˆP1||}).
Regularizers To provide more insights about Theorems 3.10 and 3.11, we provide explicit values
for the primal and dual strong convexity constants, ηandσR, respectively. To this end, we focus
on the Shannon entropy regularization h(p) =−τH(p)and the Tsallis-1/2 entropy regularization
h(p) =−τH1/2(p)as defined in Appendix C. While the Shannon entropy regularization is commonly
used in IRL [Ziebart, 2010, Ho and Ermon, 2016], the Tsallis-1/2 entropy is more often adopted in the
multi-armed bandit literature Zimmert and Seldin [2021]. Both regularizations satisfy Assumption 2.1
as well as Assumption 3.5 with the constants detailed in Proposition D.9 in the appendix. In general,
the Tsallis entropy leads to a slightly smaller strong convexity constant η, but avoids an exponential
dependence on the effective horizon Hγ= 1/(1−γ)inσR. Below, we summarize the implications
of Proposition D.9 for ε-transferability of a reward ˆrrecovered from two experts.
Corollary 3.13. Suppose the conditions in Theorem 3.10 hold. Furthermore, let Hγ:= 1/(1−γ),
R:= max r∈R∥r∥∞,D= max r,r′∈R∥r−r′∥2, and τ < D . Then, for the Shannon entropy ˆris
ε-transferable to P= ∆S×A
S with
ε=2H2
γD|S||A|2+Hγexp
2RHγ
τ
ν2
minτsin (θ2(P0, P1)/2)2ˆε,
and for the Tsallis entropy with
ε=4√
2H5
γD|S||A|2
2R/τ+ 3p
|A|3
ν2
minτsin (θ2(P0, P1)/2)2ˆε.
We observe that transferability generally becomes more challenging with decreasing regularization
parameter τ, i.e. if the expert’s policy becomes more deterministic. Furthermore, we see that it is
easier to recover a transferable reward in a Tsallis entropy-regularized MDP. Corollary 3.13 also
shows that the constant between εandˆεtends to be large, meaning that we need to recover a reward
for which the experts are ˆε-optimal with a very small ˆεto guarantee ε-transferability for a reasonable
ε. However, it’s important to note that our results provide sufficient conditions for the worst case, and
it remains for future work to determine under what conditions these constants can be improved.
Remark 3.14.Our results in this section, especially Proposition 3.4 and Lemma 3.6, are critically
relying on the steepness of the regularization (Assumption 2.1), which is essential to ensure that the
expert’s reward can be identified up to the equivalence class of potential shaping transformations.
Although we can still upper bound the suboptimality ℓ(rE,RL(ˆr))in terms of the distance between
ˆrtorEinRS×A/Uwithout this assumption (see Proposition D.10), we no longer have a lower
bound as in Lemma 3.6, which is essential for establishing closeness of ˆrandrEinRS×A/U. Hence,
we expect it to be difficult to obtain guarantees similar to those in Theorem 3.10 and 3.11 for the
unregularized setting, without either reducing the dimension of the reward class [Amin et al., 2017]
or making specific assumptions about the feasible reward sets [Metelli et al., 2021, Assumption 4.1].
4 Algorithm
To provide end-to-end guarantees for recovering transferable rewards from a finite set of expert
demonstrations, we analyze the convergence and sample complexity (in terms of expert demonstra-
8tions) of an algorithm for recovering a reward for which all Kexperts are approximately optimal.
To this end, we focus on the reward class R=
r∈RS×A:∥r∥1≤1	
. Furthermore, we assume
oracle access to a (ε, δ)-PAC algorithm for the forward problem (O-RL) . That is, a polynomial-time
algorithm, Aε,δ, that outputs a policy π=Aε,δ(r)such that with probability at least 1−δit holds
thatℓ(r, µπ)≤ε(see e.g. [Lan, 2023] for a specific example). The key idea of our meta-algorithm is
to learn a reward minimizing the sum of the suboptimalities of the Kexperts µE
P0, . . . , µE
PK−1. This
leads us to the following multi-expert IRL problem
min
r∈RK−1X
k=0ℓPk(r,ˆµDE
k), (O-IRL)
where ˆµDE
k(s, a):= (1−γ)/NEPNE−1
i=0PHE−1
t=0γt1{sk,i
t=s, ak,i
t=a}is an empirical expert
occupancy measure. To solve Problem (O-IRL) , we propose the projected gradient descent scheme
as detailed in Algorithm 1 below, where rollout Pk(π, N, H )samples Nindependent trajectories
of length Hfrom policy π. Using a stochastic online gradient descent analysis, Theorem 4.1 shows
that any PAC algorithm for the forward problem yields a PAC algorithm for the inverse problem.
Algorithm 1: Multi-expert IRL
Input: α, T,{DE
k}K−1
k=0, N, H, ε opt, δopt.
Initialize: π∈∆S
Aandr∈ R arbitrarily.
fori= 0, . . . , T −2do
fork= 0, . . . , K −1do
πk,t=Aεopt,δopt
Pk(rt) // Forward RL.
Dk,t=rollout Pk(πk,t, N, H )
end
gt=PK−1
k=0
ˆµDk,t−ˆµDE
k
rt+1= ΠR(rt−αgt) // Reward step.
end
Return: ˆr:=1
TPT−1
t=0rt.
Theorem 4.1. Suppose that NE= Ω 
Klog(|S||A| /ˆδ)/ˆε2
andHE= Ω 
log(K/ˆε)/log(1/γ)
.
Running Algorithm 1 for T= Ω 
K2/ˆε2
iterations with step-size α= 1/(K√
T), where δopt=
O ˆδˆε2/K3
,εopt=O(ˆε/K),N= Ω 
Klog(K|S||A| /(ˆδˆε))/ˆε2
, and H=HE, it holds with
probability at least 1−ˆδthatℓPk(ˆr, µE
Pk)≤ˆε,fork= 0, . . . , K −1.
The above result generalizes [Syed and Schapire, 2007, Theorem 2] by considering multiple experts
and by proving convergence in terms of the expert suboptimality. We refer to Appendix I for the
proof and the precise constants. Theorem 4.1 shows that with Ω(K/ˆε2)demonstrations of each
expert, we recover in Ω(K2/ˆε2)steps of Algorithm 1 a reward ˆrfor which all experts are ˆε-optimal.
Together with Theorem 3.10 and 3.11, this provides a bound on the sample and time complexity
of recovering in ε-transferable rewards in regularized IRL.
5 Experiments
To validate our results experimentally, we adopt a stochastic variant of the WindyGridworld
environment [Sutton and Barto, 2018]. In this environment, the agent moves to the intended grid
cell with a probability of (1−β)and is pushed one step further in the direction of the wind with a
probability of β. Using Algorithm 1, we recover a reward ˆrfrom demonstrations of two experts, both
exposed to the same wind strength βbut different wind directions – North and East. The experiments
are repeated for a varying number of expert demonstrations NE∈ {103,104,105,106}and wind
strengths β∈ {0.01,0.1,0.5,1.0}. We then test the transferability to two different environments:
one with South wind, PSouth, and a zero-wind environment with cyclically shifted actions, PShifted.
Figure 2(a) shows that the second principal angle between the two experts’ transition laws P0and
9(a) ( b) ( c) ( d)
Figure 2: ( a) shows the second principal angle between the experts, for varying wind strength β. (b)
shows the distance between ˆrandrEinRS×A/1for a varying number of expert demonstrations
NEand wind strength β. (c) and ( d) show the transferability to PSouthandPShiftedin terms of
ℓPSouth(rE,RLPSouth(ˆr))andℓPShifted(rE,RLPShifted(ˆr)), respectively. The circles indicate the median and
the shaded areas the 0.2 and 0.8 quantiles over 10 independent realizations of the expert data.
P1increases with increasing wind strength. Moreover, Figure 2(b)-(d) show that both the closeness
between ˆrandrEinRS×A/1and the transferability to PSouthandPShiftedimprove with a larger
second principal angle, as expected from Theorem 3.10. For a more detailed discussion of the
experiments we refer to Appendix K.
6 Conclusion
Summary In this paper, we investigated the transferability of rewards in regularized IRL. We
showed that the conditions established under full access to the experts’ policies do not guarantee
transferability when learning a reward from a finite set of expert demonstrations. To address this
issue, we proposed using principal angles as a more refined measure of the similarity and dissimilarity
of transition laws. Assuming a strongly convex and locally smooth regularization, we then showed
that if we recover a reward for which at least two experts are nearly optimal, and their environments
are sufficiently different in terms of the second principal angle between their transition laws, then the
recovered reward is universally transferable. Furthermore, we showed that if two environments are
sufficiently similar in terms of the maximal principal angle between their transition laws, rewards
learned in one environment can be effectively transferred to the other environment. Additionally, we
provided explicit constants for the Shannon and Tsallis entropy, as well as a PAC algorithm for recov-
ering a reward for which all experts are approximately optimal. As a result, we established end-to-end
guarantees for learning transferable rewards in regularized IRL. Additionally, we experimentally
validated our results through gridworld experiments.
Limitations and future work Our results provide only sufficient conditions for transferability.
It would be valuable to investigate necessary conditions to check whether our bounds are tight.
Furthermore, extending our analysis to lower-dimensional reward classes could reduce the complexity
of learning transferable rewards. Although our paper focuses on discrete state and action spaces,
an exciting avenue for future research would be to extend our results to continuous state and action
spaces, which are more commonly encountered in practice. We expect that our proof methods can
be generalized to this setting, but the analysis will be more intricate due to the infinite-dimensional
reward and occupancy measure spaces. Finally, as our work is mainly theoretical, experimental
validation on real-world applications could provide valuable insight into the practical aspects and
challenges of transferability.
Acknowledgments Andreas Schlaginhaufen is funded by a PhD fellowship from the Swiss Data
Science Center.
10References
P. Abbeel and A. Y . Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings
of the twenty-first international conference on Machine learning , 2004.
K. Amin, N. Jiang, and S. Singh. Repeated inverse reinforcement learning. Advances in neural
information processing systems , 2017.
A. Beck. First-order methods in optimization . SIAM, 2017.
N. Bourbaki. Elements of mathematics: General topology . Hermann, 1966.
S. P. Boyd and L. Vandenberghe. Convex optimization . Cambridge university press, 2004.
H. Cao, S. Cohen, and L. Szpruch. Identifiability in inverse reinforcement learning. Advances in
Neural Information Processing Systems , 34, 2021.
T. M. Cover. Elements of information theory . John Wiley & Sons, 1999.
C. Dann, C.-Y . Wei, and J. Zimmert. Best of both worlds policy optimization. arXiv preprint
arXiv:2302.09408 , 2023.
Z. Drmac. On principal angles between subspaces of euclidean space. SIAM Journal on Matrix
Analysis and Applications , 2000.
J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning.
arXiv preprint arXiv:1710.11248 , 2017.
A. Galántai. Projectors and projection methods . Springer Science & Business Media, 2013.
D. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon. Iq-learn: Inverse soft-q learning for
imitation. Advances in Neural Information Processing Systems , 2021.
M. Geist, B. Scherrer, and O. Pietquin. A theory of regularized markov decision processes. In
International Conference on Machine Learning . PMLR, 2019.
R. Goebel and R. T. Rockafellar. Local strong convexity and local lipschitz continuity of the gradient
of convex functions. Journal of Convex Analysis , 2008.
J. Ho and S. Ermon. Generative adversarial imitation learning. Advances in neural information
processing systems , 2016.
W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association , 1963.
J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V . Tsounis, V . Koltun, and M. Hutter. Learning
agile and dynamic motor skills for legged robots. Science Robotics , 2019.
W. Jeon, C.-Y . Su, P. Barde, T. Doan, D. Nowrouzezahrai, and J. Pineau. Regularized inverse
reinforcement learning. In International Conference on Learning Representations , 2021.
S. Ji-Guang. Perturbation of angles between linear subspaces. Journal of Computational Mathematics ,
1987.
R. E. Kalman. When Is a Linear Control System Optimal? Journal of Basic Engineering , 1964.
K. Kim, S. Garg, K. Shiragur, and S. Ermon. Reward identification in inverse reinforcement learning.
InProceedings of the 38th International Conference on Machine Learning , pages 5496–5505,
2021.
A. V . Knyazev and M. E. Argentati. Principal angles between subspaces in an a-based scalar product:
algorithms and perturbation estimates. SIAM Journal on Scientific Computing , 2002.
G. Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. Mathematical programming , 2023.
11Y . Lu, J. Fu, G. Tucker, X. Pan, E. Bronstein, R. Roelofs, B. Sapp, B. White, A. Faust, S. Whiteson,
et al. Imitation is not enough: Robustifying imitation with reinforcement learning for challenging
driving scenarios. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS) , 2023.
J. Mei, C. Xiao, C. Szepesvari, and D. Schuurmans. On the global convergence rates of softmax
policy gradient methods. In International conference on machine learning . PMLR, 2020.
A. M. Metelli, G. Ramponi, A. Concetti, and M. Restelli. Provably efficient learning of transferable
rewards. In International Conference on Machine Learning . PMLR, 2021.
A. Y . Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and
application to reward shaping. In Icml, 1999.
A. Y . Ng, S. Russell, et al. Algorithms for inverse reinforcement learning. In Icml, 2000.
R. Ngo, L. Chan, and S. Mindermann. The alignment problem from a deep learning perspective.
arXiv preprint arXiv:2209.00626 , 2022.
P. A. Ortega, D. A. Braun, J. Dyer, K.-E. Kim, and N. Tishby. Information-theoretic bounded
rationality. arXiv preprint arXiv:1512.06789 , 2015.
R. Ouhamma and M. Kamgarpour. Learning nash equilibria in zero-sum markov games: A single
time-scale algorithm under weak reachability. arXiv preprint arXiv:2312.08008 , 2023.
R. Penrose. A generalized inverse for matrices. In Mathematical proceedings of the Cambridge
philosophical society . Cambridge University Press, 1955.
D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural
information processing systems , 1988.
M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley
& Sons, 2014.
R. T. Rockafellar. Convex Analysis . Princeton University Press, 1970.
R. T. Rockafellar and R. J.-B. Wets. Variational analysis . Springer Science & Business Media, 2009.
P. Rolland, L. Viano, N. Schürhoff, B. Nikolov, and V . Cevher. Identifiability and generalizability from
multiple experts in inverse reinforcement learning. Advances in Neural Information Processing
Systems , 2022.
J. Rust. Structural estimation of markov decision processes. Handbook of econometrics , pages
3081–3143, 1994.
A. Schlaginhaufen and M. Kamgarpour. Identifiability and generalizability in constrained inverse
reinforcement learning. In Proceedings of the 40th International Conference on Machine Learning .
PMLR, 2023.
J. M. V . Skalse, M. Farrugia-Roberts, S. Russell, A. Abate, and A. Gleave. Invariance in policy
optimisation and partial identifiability in reward learning. In International Conference on Machine
Learning . PMLR, 2023.
N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. V oss, A. Radford, D. Amodei, and P. F.
Christiano. Learning to summarize with human feedback. Advances in Neural Information
Processing Systems , 2020.
R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction . MIT press, 2018.
U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. Advances in
neural information processing systems , 2007.
M. Teboulle. Entropic proximal mappings with applications to nonlinear programming. Mathematics
of Operations Research , 1992.
12B. D. Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy .
Carnegie Mellon University, 2010.
J. Zimmert and Y . Seldin. Tsallis-inf: An optimal algorithm for stochastic and adversarial bandits.
The Journal of Machine Learning Research , 2021.
M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th international conference on machine learning (icml-03) , 2003.
13Appendix
Table of Contents
A Notations 15
B Conjugate duality in regularized IRL 16
C Regularizers 16
D Technical Lemmas 17
D.1 Lipschitz continuity from policies to occupancy measures . . . . . . . . . . . . 17
D.2 Strong convexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
D.3 Lipschitz gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
D.4 Dual smoothness and strong convexity . . . . . . . . . . . . . . . . . . . . . . 22
D.5 Regularity constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.6 Suboptimality bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.7 Perturbation bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E Proof of claim in Example 3.3 26
F Proof of Theorem 3.10 28
G Proof of Theorem 3.11 30
H Estimating principal angles 30
I Proof of Theorem 4.1 30
J Suboptimal experts 33
K Experimental details 34
14A Notations
Overview Here, we provide an overview of some of the most important notations. However, every
notation is defined when it is introduced as well.
Table 1: Notations.
YX:= set of functions f:X → Y
∆X := probability simplex over some discrete set X
M := set of feasible occupancy measures
R :=
r∈RS×A:∥r∥1≤1	
, reward class
R := max r∈R∥r∥∞, reward bound
D := max r,r∈R∥r−r′∥2, diameter of the reward class
Hγ := 1 /(1−γ), effective horizon
ν(s) :=P
aµ(s, a)
πµ(a|s):=µ(s, a)/P
a′µ(s, a′) =µ(s, a)/ν(s), ν(s)>0
1/|A| (arbitrary) ,otherwise
πs :=π(·|s)
¯h(µ) :=E(s,a)∼µ[h(πµ
s)]
J(r, µ):=⟨r, µ⟩ −¯h(µ)
ℓ(r, µ′):= max µ∈MJ(r, µ)−J(r, µ′)
RL(r) := argmaxµ∈MJ(r, µ), optimal occupancy measure for r
IRL(µE):=
r∈ R:µE=RL(r)	
, feasible reward set for µE
Aε,δ(r):= PAC RL algorithm outputting some ε-optimal policy with probability at least 1−δ
ˆµD :=1−γ
NPN−1
i=0PT−1
t=0γt1
si
t=s, ai
t=a	
, where D= 
si
0, ai
0, . . . , si
H−1, ai
H−1	N−1
i=0
E := linear operator RS→RS×Adefined by (Ef)(s, a) =f(s)forf∈RS
P := linear operator RS→RS×Adefined by (Pf)(s, a) =P
s′P(s′|s, a)f(s′)forf∈RS
im (A):= image of a linear operator A
U := im( E−γP)⊂RS×A, potential shaping subspace
1 :={f=constant } ⊂RS×A, constant subspace
V⊥:= orthogonal complement of a linear subspace V
Additional definitions In the following, we briefly recall some additional definitions. To this end,
we denote B(x, r):={x∈Rn:∥x∥2< r}for an open ball of radius rwith center x.
Definition A.1 (Interior) .The interior of a set X ⊆Rnis defined as
intX:={x∈ X:B(x, r)⊆ X for some r >0}.
Definition A.2 (Affine hull) .The affine hull of a set X ⊆Rnis defined as
affX:={θ1x1+. . .+θkxk:x1, . . . , x k∈ X, θ1+. . .+θk= 1}.
Definition A.3 (Relative interior) .The relative interior of a set X ⊆Rnis defined as
relintX:={x∈ X:B(x, r)∩affX ⊆ X for some r >0}.
Definition A.4 (Relative boundary) .The relative boundary of a closed set X ⊆Rnis defined as
relbdX:=X \relintX.
Definition A.5 (Convex hull) .The convex hull of a set X ⊆Rnis defined as
convX:={θ1x1+. . .+θkxk:x1, . . . , x k∈ X, θ1+. . .+θk= 1, θi≥0, i= 1, . . . , k }.
15B Conjugate duality in regularized IRL
In this section, we first recall some background from convex analysis and then briefly discuss the
duality between reward equivalence classes and optimal occupancy measures.
Definitions We recall a few definitions related to convex functions. In convex analysis it is standard
to consider extended real value functions f:Rn→R:= [−∞,∞], where convex functions defined
on some subset X ⊂Rnare extended over the entire space by setting their value to +∞outside of
their domain. The effective domain is defined as domf:={x:f(x)<∞}, and a convex function
fis said to be proper if f >−∞ anddomf̸=∅. Furthermore, fis referred to as closed if its
epigraph {(x, y) :x∈domf, y≥f(x)}is a closed set.2In particular, fis closed if it is continuous
ondomfanddomfis a closed set [Boyd and Vandenberghe, 2004]. Lastly, we recall two key
concepts in convex analysis – the subdifferential and the convex conjugate of some convex function.
Definition B.1 (Subdifferential) .A subgradient of f:Rn→Rat some point x∈Rnis a vector
g∈Rnsuch that f(x′)≥f(x) +g⊤(x−x)for all x′∈ X. The subdifferential ∂f(x)atx∈ X
is the set of all subgradients at x, where ∂f(x)is defined to be empty if x /∈domf.
Definition B.2 (Convex Conjugate) .The convex conjugate of f:Rn→Ris the function f∗:
Rn→Rdefined as
f∗(y) = sup
x⟨y, x⟩ −f(x).
Key results Next, we list two key results from convex analysis.
Theorem B.3 ([Rockafellar, 1970]) .A function f:Rn→Ris differentiable at some point
x∈domfif and only if ∂f(x)is singleton. In this case we have ∂f(x) ={∇f(x)}.
Theorem B.4 ([Rockafellar, 1970]) .For any proper convex function f:Rn→Rit holds
f∗(y) =⟨y, x⟩ −f(x)⇐⇒ y∈∂f(x).
If additionally fis closed, then
f∗(y) =⟨y, x⟩ −f(x)⇐⇒ y∈∂f(x)⇐⇒ x∈∂f∗(y).
Duality in IRL Letf:RS×A→Rbe given by f:=¯h+δM, where δMis a characteristic
function defined as δM(µ) = 0 ifµ∈ M andδM(µ) =∞, otherwise. Since fis closed proper
convex, Theorem B.3 and B.4 imply that for a strictly convex ¯hwe have
RL(r) =∇f∗(r)and IRL(µ) =∂f(µ)∩ R.
Additionally, under Assumption 2.1 and Slater’s condition, which is ensured by Assumption 2.2, we
have∂f(µ) =∇¯h(µ) +U[Schlaginhaufen and Kamgarpour, 2023]. Hence, µis optimal for rif and
only if r∈[∇¯h(µ)]U. Therefore, there is a one-to-one mapping between elements of the quotient
spaceRS×A/U, i.e. reward equivalence classes, and corresponding optimal occupancy measures in
M. This mapping is given by
∇f∗:RS×A/U → M ,[r]U7→ ∇f∗(r) =RL(r),
and its inverse by
∂f:M → RS×A/U, µ7→∂f(µ) =∇¯h(µ) +U.
C Regularizers
We dedicate this section to discuss optimal policies in regularized MDPs and to recall their explicit
form for Shannon and Tsallis entropy regularization.
2A proper convex function is closed if and only if it is lower semi-continuous [Rockafellar, 1970].
16Optimal policies Throughout the appendix, it will convenient to use the notation πs:=π(·|s).
Given some proper closed strongly convex policy regularizer h, it can be shown [Geist et al., 2019]
that the optimal policy, π∗, satisfies
π∗
s=∇h∗(q∗
s) = argmax
πs∈∆A⟨πs, q∗
s⟩ −h(πs),∀s∈ S,
where q∗
s∈RAis the optimal q-function defined via
q∗
s(a):=q∗(s, a):= max
π∈∆S
AEπ
r(st, at) +X
t≥1γt[r(st, at)−h(πs)]s0=s, a0=a
.
Next, we discuss the explicit form of π∗
s=∇h∗(q∗
s)for the specific cases of Shannon and Tsallis- 1/2
entropy regularization.
Shannon entropy For some τ >0, we define the Shannon entropy regularizer as h:=−τH,
where
H(πs):=−X
aπs(a) logπs(a),
is the Shannon entropy satisfying 0≤ H ≤ log|A|. It can be shown that hisτ-strongly convex with
respect to ∥·∥1[Cover, 1999], and the optimal policy satisfies [Geist et al., 2019]
π∗(a|s) =exp (q∗(s, a)/τ)P
a′exp (q∗(s, a′)/τ).
Tsallis entropy For some parameter α∈R, the Tsallis entropy, Hα, is defined as
Hα(πs):=1
α−1 
1−X
aπs(a)α!
.
In the limit α→1, the Tsallis entropy equals the Shannon entropy defined above. However, in this
paper, we use Tsallis entropy to refer to the choice α= 1/2, which is often adopted as regularization
in multi-armed bandit and, more recently, policy optimization algorithms [Zimmert and Seldin, 2021,
Dann et al., 2023]. That is, we consider h(πs) =−τH1/2(πs) =−2τP
ap
πs(a)−1
for some
τ >0. We have 0≤ −h≤2τp
|A| − 1
and it can be shown that the optimal policy satisfies
π∗(a|s) =τ
xs−q∗(s, a)2
,
where xsis a normalization parameter such thatP
aπ∗(a|s) = 1 [Zimmert and Seldin, 2021].
Furthermore, since hhas diagonal Hessian ∇2h(πs)a,a=τ/(2πs(a)3/2), it is τ/2-strongly convex
with respect to ∥·∥2and hence also τ/(2|A|)-strongly convex with respect to ∥·∥1.
D Technical Lemmas
In this section, we show several new technical results that are required for the proofs of our main
theorems.
D.1 Lipschitz continuity from policies to occupancy measures
Proposition D.1. Let Assumption 2.2 hold. For any µ1, µ2∈ M , we have
(1−γ)∥µ1−µ2∥1≤max
s∥πµ1
s−πµ2
s∥1≤2
νmin∥µ1−µ2∥1.
Proof. To show the first inequality, we decompose
∥µ1−µ2∥1≤X
s,a|ν1(s)(πµ1(a|s)−πµ2(a|s))|+X
s,a|(ν1(s)−ν2(s))πµ2(a|s)|
≤max
s∥πµ1
s−πµ2
s∥1+∥ν1−ν2∥1,
17where we used the triangle and Hölder’s inequality. From the Bellman flow constraints,
ν(s) =γX
s′,a′P(s|s′, a′)µ(s′, a′) + (1 −γ)ν0(s),
it follows that
∥ν1−ν2∥1=γX
sX
s′,a′P(s|s′, a′)(µ1(s′, a′)−µ2(s′, a′))
≤γX
s′,a′X
sP(s|s′, a′)
|{z }
=1|µ1(s′, a′)−µ2(s′, a′)|
=γ∥µ1−µ2∥1,
where we again used the triangle inequality. Hence, we have
max
s∥πµ1
s−πµ2
s∥1≥ ∥µ1−µ2∥1− ∥ν1−ν2∥1≥(1−γ)∥µ1−µ2∥1.
To show the second inequality, we use the reverse triangle inequality
∥µ1−µ2∥1≥X
s,a|ν1(s)(πµ1(a|s)−πµ2(a|s))| −X
s,a|(ν1(s)−ν2(s))πµ2(a|s)|
=X
sν1(s)∥πµ1
s−πµ2
s∥1− ∥ν1−ν2∥1,
≥νminmax
s∥πµ1
s−πµ2
s∥1−γ∥µ1−µ2∥1,
where in the last step we used again that ∥ν1−ν2∥1≤γ∥µ1−µ2∥1. By rearranging terms we
arrive at the desired inequality
max
s∥πµ1
s−πµ2
s∥1≤1 +γ
νmin∥µ1−µ2∥1≤2
νmin∥µ1−µ2∥1.
D.2 Strong convexity
Next, we show that strong convexity of the policy regularizer translates into strong convexity in the
occupancy measure.
Proposition D.2 (Strong convexity) .Let Assumption 2.2 hold and suppose that hisηh-strongly
convex with respect to the ∥·∥1norm. Then, ¯hisηhνmin/H2
γ-strongly convex with respect to ∥·∥1.
Proof. We need to show that for α∈(0,1),¯α= 1−αand any two µ1, µ2∈ M it holds that
¯h(αµ1+ ¯αµ2)≤α¯h(µ1) + ¯α¯h(µ2)−α¯αη
2∥µ1−µ2∥2
1,
18forη=ηhνmin/H2
γ. To this end, we start similarly as in the proof of strict convexity by Schlagin-
haufen and Kamgarpour [2023], but use ν(s)≥νminand strong convexity of h.
¯h(αµ1+ ¯αµ2) (7)
=X
s(αν1(s) + ¯αν2(s))hαµ1(s,·) + ¯αµ2(s,·)
αν1(s) + ¯αν2(s)
=X
s(αν1(s) + ¯αν2(s))hαµ1(s,·)
αν1(s) + ¯αν2(s)ν1(s)
ν1(s)+¯αµ2(s,·)
αν1(s) + ¯αν2(s)ν2(s)
ν2(s)
=X
s(αν1(s) + ¯αν2(s))h
αν1(s)
αν1(s) + ¯αν2(s)| {z }
βsπµ1
s+¯αν2(s)
αν1(s) + ¯αν2(s)| {z }
1−βsπµ2
s

≤X
s(αν1(s) + ¯αν2(s))
βsh(πµ1
s) + (1 −βs)h(πµ2
s)−βs(1−βs)ηh
2∥πµ1
s−πµ2
s∥2
1
=X
s
αν1(s)h(πµ1
s) + ¯αν2(s)h(πµ2
s)−α¯αηh
2ν1(s)ν2(s)
αν1(s) + ¯αν2(s)∥πµ1
s−πµ2
s∥2
1
.
From here on, we use that
X
sν1(s)ν2(s)
αν1(s) + ¯αν2(s)∥πµ1
s−πµ2
s∥2
1
=X
smax{ν1(s), ν2(s)}
αν1(s) + ¯αν2(s)| {z }
≥1min{ν1(s), ν2(s)}| {z }
≥νmin∥πµ1
s−πµ2
s∥2
1
≥X
sνmin∥πµ1
s−πµ2
s∥2
1
≥νminmax
s∥πµ1
s−πµ2
s∥2
1
≥νmin/H2
γ∥µ1−µ2∥2
1. (8)
where we used Proposition D.1 in the last step. Plugging the inequality (8)back into (7)concludes
the proof.
D.3 Lipschitz gradients
In this section, we show how we can get bounds on the Lipschitz constant LK. To this end, we first
need to lower bound the optimal policies.
Policy lower bounds The following proposition establishes a lower bound for optimal policies
with Shannon and Tsallis entropy regularization.
Proposition D.3. LetHγ= 1/(1−γ)andrmax:=∥r∥∞. Then, we have the following lower
bounds:
a) Ifh=−τH, then π∗(a|s)≥exp (−2rmaxHγ/τ)/|A|1+Hγ.
b) Ifh=−τH1/2, then π∗(a|s)≥
2rmax/τ+ 3p
|A|
Hγ−2
.
Proof. Recall the formula for the optimal policies in Appendix C.
Part a): Since, −rmaxHγ≤q∗(s, a)≤(rmax+τlog|A|)Hγ, it holds that
π∗(a|s)≥exp (−rmaxHγ/τ)
|A|exp (( rmax+τlog|A|)Hγ/τ)
=exp (−rmaxHγ/τ)
|A|1+Hγexp (rmaxHγ/τ)=exp (−2rmaxHγ/τ)
|A|1+Hγ.
19Part b): The proof of b) is similar to [Ouhamma and Kamgarpour, 2023, Lemma 8]. However, our
settings are slightly different. Recall that
π∗(a|s) =τ
xs−q∗(s, a)2
.
By Ouhamma and Kamgarpour [2023, Lemma 10] we have τ≤xs−∥q∗
s∥∞≤τp
|A|. Furthermore,
it holds that −rmaxHγ≤q∗(s, a)≤
rmax+ 2τp
|A|
Hγ. Hence, we have
0< xs−q∗(s, a)≤τp
|A|+
rmax+ 2τp
|A|
Hγ+rmaxHγ≤
2rmax+ 3τp
|A|
Hγ,
which yields the desired lower bound.
We also highlight the following result, which shows that if the policy πµis lower bounded on some
setK ⊂ M , then it is also lower bounded on its convex hull convK.
Proposition D.4. Suppose µ=αµ1+ (1−α)µ2withα∈(0,1)andµ1, µ2∈ M . Then,
πµ
s=αν1(s)
αν1(s) + ¯αν2(s)| {z }
βsπµ1
s+¯αν2(s)
αν1(s) + ¯αν2(s)| {z }
1−βsπµ2
s,
where βs∈(0,1).
Proof. The proof follows immediately from the proof of Proposition D.2.
Hessian upper bounds In the following, we establish upper bounds for the Hessians of the
occupancy measure regularizations, ¯h, resulting from Shannon and Tsallis entropy regularization of
the policy. In particular, we aim to upper-bound the maximum norm of the Hessian in terms of the
smallest entry of the policy.
Proposition D.5. Let Assumption 2.2 hold. Consider µ∈ M and let πmin= min s,aπµ(a|s)>0.
Then, the Hessian of ¯his upper bounded as follows:
a) Ifh=−τH, then∇2¯h(µ)
∞≤τ
νminπmin.
b) Ifh=−τH1/2, then∇2¯h(µ)
∞≤τ
νminπ3/2
min.
Here,∥·∥∞denotes the maximum norm ∥A∥∞= max ij|Aij|.
Proof. As shown by Schlaginhaufen and Kamgarpour [2023, Proposition B.2], the gradient of ¯h
satisfies
∇¯h(µ)(s, a) =h(πµ
s) +∇h(πµ
s)(a)− ⟨∇ h(πµ
s), πµ
s⟩. (9)
Moreover, we have
∂πµ(s, a)
∂µ(s′, a′)=δs,s′·δa,a′−πµ(a|s)
ν(s).
Using the above two formulas, we can calculate the Hessians explicitly.
Part a): For the Shannon entropy it holds by (9) that ∇¯h(µ)(s, a) =τlogπµ(a|s). Hence,
∂2¯h(µ)
∂µ(s′, a′)∂µ(s, a)=τ·1
πµ(a|s)·δs,s′·δa,a′−πµ(a|s)
ν(s),
and∇2¯h(µ)(s′,a′),(s,a)=∂2¯h(µ)
∂µ(s′, a′)∂µ(s, a)≤τ
νminπmin.
Part b): For the Tsallis entropy it holds by (9) that
∇¯h(µ)(s, a) =−τ X
a′′p
πµ(a′′|s) +1p
πµ(a|s)−2!
.
20Therefore, the second derivative is bounded as follows
∂2¯h(µ)
∂µ(s′, a′)∂µ(s, a)=−τ·δs,s′· X
a′′1
2p
πµ(a′′|s)δa′,a′′−πµ(a′′|s)
ν(s)
−1
2πµ(a|s)3/2δa,a′−πµ(a|s)
ν(s)!
=τ·δs,s′
2ν(s)1p
πµ(a′|s)−X
a′′p
πµ(a′′|s)−δa,a′
πµ(a|s)3/2+1p
πµ(a|s)
≤τ
2νmin
1p
πµ(a′|s)−X
a′′p
πµ(a′′|s)
| {z }
(A)+δa,a′
πµ(a|s)3/2−1p
πµ(a|s)
| {z }
(B)
.
Now, sinceP
ap
πµ(a|s)≤√
A≤1/√πmin, we have (A) + (B)≤1/√πmin+ 1/π3/2
min≤2/π3/2
min,
which yields the desired result
∇2¯h(µ)(s′,a′),(s,a)≤τ/
νminπ3/2
min
.
Lipschitz gradients Next, we provide explicit Lipschitz constants LKfor∇¯hcorresponding to
Shannon and Tsallis entropy regularization.
Proposition D.6 (Lipschitz gradients) .Consider some closed convex set K ⊂ M and suppose
πmin= min µ∈Kmins,aπµ(a|s)>0. Then, the gradient of ¯his Lipschitz continuous over Ki.e.
∇¯h(µ1)− ∇¯h(µ2)
2≤LK∥µ1−µ2∥2,∀µ1, µ2∈ K,
where the respective Lipschitz constants are as follows:
a) Ifh=−τH, then LK=τ|S||A| /(νminπmin).
b) Ifh=−τH1/2, then LK=τ|S||A| /(νminπ3/2
min).
Proof. Defining h=µ2−µ1, Lipschitz continuity follows from
∇¯h(µ1)− ∇¯h(µ2)
2(i)
≤p
|S||A|∇¯h(µ1)− ∇¯h(µ2)
∞
(ii)=p
|S||A|Z1
0∇2¯h(µ1+th)hdt
∞
(iii)
≤p
|S||A|Z1
0∇2¯h(µ1+th)h
∞dt
(iv)
≤p
|S||A|Z1
0∇2¯h(µ1+th)
∞∥h∥1dt
(v)
≤ |S||A| max
0≤t≤1∇2¯h(µ1+th)
∞∥µ1−µ2∥2.
Here, we used in (i)and(v)that∥x∥1≤√n∥x∥2≤n∥x∥∞forx∈Rn, and in (ii)we applied
the fundamental theorem of calculus. Moreover, (iii)follows fromR
f≤R
|f|, and (iv)from
Hölder’s inequality. Now, by convexity µ1, µ2∈ K implies that µ1+th∈ K fort∈[0,1]. Hence,
plugging in the upper bounds from Proposition D.5 concludes the proof.
21D.4 Dual smoothness and strong convexity
Next, we show that the convex conjugate, f∗, of the extended real value function f:=¯h+δM(see
Appendix B) is – if understood as a mapping from RS×A/UtoR– both smooth and strongly convex
onRwith respect to the quotient norm. While it is well-known that global smoothness and strong
convexity are dual properties [Rockafellar and Wets, 2009, Proposition 12.60], the key challenge for
proving dual strong convexity is that ¯hhas only locally Lipschitz gradients (see Proposition D.6).
Proposition D.7 below shows that η-strong convexity of ¯himplies dual 1/η-smoothness and locally
Lipschitz gradients imply dual σR-strong convexity on Rfor some σR>0. Moreover, we provide
explicit lower bounds on σRfor Shannon and Tsallis entropy entropy regularization.
Proposition D.7. Letf∗be the convex conjugate of f:=¯h+δM. Then, the following holds:
a) Suppose that ¯hisη-strongly convex over M, that is for all µ, µ′∈ M it holds that
¯h(µ′)≥¯h(µ) +⟨∇¯h(µ), µ′−µ⟩+η
2∥µ−µ′∥2
2,
then we have for all r, r′∈RS×Athat
f∗(r′)≤f(r) +⟨∇f∗(r), r′−r⟩+1
2η∥[r′]U−[r]U∥2
2. (10)
b)Suppose that for any closed convex subset K ⊂relintM, there is some LK>0such that for all
µ, µ′∈ K it holds that
∇¯h(µ)− ∇¯h(µ′)
2≤LK∥µ−µ′∥2,
then we have for all r, r′∈ R that
f∗(r′)≥f∗(r) +⟨∇f∗(r), r′−r⟩+σR
2∥[r′]U−[r]U∥2
2, (11)
for some σR>0.
c)LetHγ:= 1/(1−γ),R:= max r∈R∥r∥∞,D= max r,r′∈R∥r−r′∥2, and suppose that τ < D ,
then for the Shannon entropy the inequality (11) holds with
σR=νminexp
−2RHγ
τ
2D|S||A|2+Hγ,
and for the Tsallis entropy with
σR=νmin
2√
2D|S||A|
2R/τ+ 3p
|A|
Hγ3.
Proof. Part a): Since fisη-strongly convex with respect to ∥·∥2, the convex conjugate f∗is1/η-
smooth with respect to the dual norm in RS×A/U[Rockafellar and Wets, 2009, Proposition 12.60],
which is equivalent to (10).
Part b): The show b), we closely follow [Goebel and Rockafellar, 2008, Theorem 4.1], but we
need to account for the quotient spaces. We define the sets K=∇f∗(R) =RL(R)andKϵ=
conv(K) +ϵ(B ∩aff(M)), where B ⊂RS×Adenotes the closed unit ball with respect to ∥·∥2and
ϵ >0is chosen such that Kϵ⊂relintM. Moreover, we let Lbe the Lipschitz constant of ∇¯hover
22Kϵ. Now, consider r∈ R andµ=∇f∗(r). Then, for any r′∈ R, we have
f∗(r′) = sup
¯µ[⟨r′,¯µ⟩ −f(¯µ)]
(i)
≥sup
¯µ∈Kϵ[⟨r′,¯µ⟩ −f(¯µ)]
(ii)
≥sup
¯µ∈Kϵ
⟨r′,¯µ⟩ −f(µ)− ⟨r,¯µ−µ⟩ −L
2∥¯µ−µ∥2
2
(iii)=⟨r, µ⟩ −f(µ) + sup
¯µ∈Kϵ
⟨r′−r,¯µ⟩ −L
2∥¯µ−µ∥2
2
(iv)=f∗(r) + sup
¯µ∈Kϵ
⟨r′−r,¯µ⟩ −L
2∥¯µ−µ∥2
2
(v)=f∗(r) +⟨r′−r, µ⟩+ sup
¯µ∈Kϵ
⟨r′−r,¯µ−µ⟩ −L
2∥¯µ−µ∥2
2
.
Here, (i)follows from Kϵ⊂RS×A,(ii)from the fact that Lipschitz gradients imply that
f(¯µ)≤f(µ) +⟨g,¯µ−µ⟩+L
2∥¯µ−µ∥2
2,
for any g∈∂f(µ)[Beck, 2017, Lemma 5.7] and r∈∂f(µ)(see Theorem B.4). Moreover, (iii)
and(v)follow from rearranging terms, and (iv)from f∗(r) =⟨r, µ⟩ −f(µ). Now, consider any
α >0such that σR= 2(α−Lα2/2)>0andαD≤ϵ. Setting ¯µ∈ Kϵin the above supremum to
(¯µ−µ) =αΠU⊥(r′−r)yields the desired result
f∗(r′)≥f∗(r) +⟨r′−r,∇f∗(r)⟩+α⟨r′−r,ΠU⊥(r′−r)⟩ −Lα2
2∥ΠU⊥(r′−r)∥2
2
=f∗(r) +⟨r′−r,∇f∗(r)⟩+σR
2∥[r′]U−[r]U∥2
2.
Note that we indeed have ¯µ∈ Kϵasµ∈ K and∥µ−¯µ∥2≤α∥r−r′∥2≤αD≤ϵ.
Part c): To get an explicit constant for σR, we need to appropriately choose ϵand calculate the
corresponding Lipschitz constant. To this end, we first recall that according to Proposition D.3 and
D.4 policies corresponding to occupancy measures in convKare, for Shannon and Tsallis entropy,
lower bounded by
πmin, Sh =exp (−2RHγ/τ)
|A|1+Hγand πmin, Ts =
2R/τ+ 3p
|A|
Hγ−2
,
respectively. Furthermore, for any µ∈ Kϵ, we have by Proposition D.1 and equivalence of norms
πµ(a|s)≥πmin−2p
|S||A|
νminϵ=π′
min.
Hence, by setting ϵ=νminπmin/(4p
|S||A| ), we have πµ(a|s)≥π′
min=πmin/2for any µ∈ Kϵ. As
for the Lipschitz constant over Kϵ, we have by Proposition D.6
LSh=τ|S||A|
νminπ′
min, Shand LTs=τ|S||A|
νminπ′
min, Ts3/2, (13)
for the Shannon and Tsallis entropy, respectively. Now, we need to ensure that α > 0such that
σR= 2(α−Lα2/2)>0andα≤ϵ/D. To that end, we set for both regularizations α=τ/(LD),
which in light of (13) ensures that
α=τ
LD≤νminπ′
min
D|S||A|≤νminπ′
min
2Dp
|S||A|=ϵ
D, (14)
for|S|,|A| ≥ 2. Moreover, we get the dual strong convexity constant
σR= 2τ
LD−τ2
2LD2
=2τ
LD
1−τ
2D
,
23which is larger than τ/LD forτ < D . We can therefore choose σR=τ/LD as a dual strong
convexity constant. Plugging in the Lipschitz constants for the two regularizations yields
σR,Sh=π′
min, Sh νmin
D|S||A|=πmin, Sh νmin
2D|S||A|=exp (−2RHγ/τ)νmin
2D|S||A|2+Hγ,
for the Shannon entropy, and
σR,Ts=π′
min, Ts3/2νmin
D|S||A|=π3/2
min, Ts νmin
2√
2D|S||A|=νmin
2√
2D|S||A|
2R/τ+ 3p
|A|
Hγ3,
for the Tsallis entropy.
Remark D.8 (Large τregime) .Note that if τ≥2D/p
|S||A| , we can set σR=α= 1/L, while
still satisfying the condition (14). This leads to the strong convexity constants
σR,Sh=exp (−2RHγ/τ)νmin
2τ|S||A|2+Hγ, σR,Ts=νmin
2√
2τ|S||A|
2R/τ+ 3p
|A|
Hγ3.
D.5 Regularity constants
In the following Proposition, we summarize the regularity constants for Shannon and Tsallis entropy
regularization. We highlight that these constants are lower bounds for ηandσR.
Proposition D.9. LetHγ:= 1/(1−γ),R:= max r∈R∥r∥∞, and D= max r,r′∈R∥r−r′∥2.
Suppose that τ < D , then for the Shannon entropy, Assumption 3.5 holds with
η=τνmin/H2
γand σR=exp (−2RHγ/τ)νmin
2D|S||A|2+Hγ,
and for the Tsallis entropy with
η=τνmin/(2H2
γ|A|)and σR=νmin
2√
2D|S||A|
2R/τ+ 3p
|A|
Hγ3.
Proof. The derivation for ηis given in Proposition D.2 and for σRin Proposition D.7 above.
D.6 Suboptimality bounds
Proposition 3.4. Under Assumptions 2.1 and 2.2, we have ℓ(r′, µ) =D¯h(µ,RL(r′))for any µ∈ M .
Proof. Letµ=RL(r). We have
ℓ(r, µ′) =⟨r, µ−µ′⟩ −¯h(µ) +¯h(µ′)
=⟨∇¯h(µ), µ−µ′⟩ −¯h(µ) +¯h(µ′)
=D¯h(µ′, µ),
where the second equality holds, as by (4) we have r− ∇¯h(µ)∈ U, and µ−µ′∈ U⊥.
Lemma 3.6. Suppose Assumptions 2.1,2.2, and 3.5 hold, and let r, r′∈ R. Then, we have
σR
2∥[r]U−[r′]U∥2
2≤ℓ(r′,RL(r)) =D¯h(RL(r),RL(r′))≤1
2η∥[r]U−[r′]U∥2
2, (6)
for some problem-dependent constant σR>0.
Proof. Letf:=¯h+δMandµ=RL(r), µ′=RL(r′). We then have
D¯h(µ, µ′)(i)=f(µ)−f(µ′)− ⟨r′, µ−µ′⟩
(ii)=f∗(r′)− ⟨r′, µ′⟩ −f∗(r) +⟨r, µ⟩ − ⟨r′, µ−µ′⟩
(iii)=f∗(r′)−f∗(r)− ⟨r′−r,∇f∗(r)⟩=Df∗(r′, r).
24Here, (i)follows from the definition of fandr′∈[∇¯h(µ′)]U, in(ii)we use that f(µ) =⟨r, µ⟩ −
f∗(r)andf(µ′) =⟨r′, µ′⟩−f∗(r′), and(iii)follows from rearranging terms and µ=∇f∗(r). The
result then follows from dual strong convexity and smoothness as established in Proposition D.7.
Note that without steep regularization it is impossible to lower bound the suboptimality in terms of
reward distances in RS×A/U(Proposition 3.4 doesn’t hold). However, we still have the following
upper bound.
Proposition D.10. Consider an arbitrary regularization and let µ∈RL(r), µ′∈RL(r′). Then,
ℓ(r, µ′)≤2∥[r]U−[r′]U∥∞≤2∥[r]U−[r′]U∥2.
Proof. Letr′′:= argmin˜r∈[r′]U∥˜r−r∥∞, then the following holds
ℓ(r, µ′) = max
µ∈MJ(r, µ)−J(r, µ′)
(i)
≤max
µ∈MJ(r, µ)−max
µ∈MJ(r′′, µ)+|J(r′′, µ′)−J(r, µ′)|
(ii)
≤max
µ∈M|⟨r−r′′, µ⟩|+|⟨r−r′′, µ′⟩|
(iii)
≤2∥r−r′′∥∞(iv)= 2∥[r]U−[r′]U∥∞≤2∥[r]U−[r′]U∥2.
Here, (i)follows from the triangle inequality and optimality of µ′,(ii)from|maxf−maxg| ≤
max|f−g|and simplifying, (iii)from Hölder’s inequality, and (iv)from the definition of r′′and
the quotient norm.
D.7 Perturbation bounds
Next, we provide a bound quantifying the change in the quotient norm when changing the generating
subspace.
Proposition D.11. Consider x, y∈Rnand two subspaces V,W ⊂Rnof dimension m < n . Then,
∥[x]W−[y]W∥2≤ ∥ΠW−ΠV∥ · ∥x−y∥2+∥[x]V−[y]V∥2,
where ∥ΠW−ΠV∥= sin ( θmax(V,W)).
Proof. The result follows from the triangle inequality and the definition of the spectral norm:
∥[x]W−[y]W∥2=∥ΠW⊥(x−y)∥2
=∥(ΠW⊥−ΠV⊥)(x−y) + Π V⊥(x−y)∥2
≤ ∥(ΠW⊥−ΠV⊥)(x−y)∥2+∥ΠV⊥(x−y)∥2
=∥(ΠW−ΠV)(x−y)∥2+∥[x]V−[y]V∥2
≤ ∥ΠW−ΠV∥ · ∥x−y∥2+∥[x]V−[y]V∥2.
Furthermore, for a proof of ∥ΠW−ΠV∥= sin ( θmax(V,W))we refer to [Drmac, 2000].
The following proposition shows that the maximal principal angle between two transition laws can be
upper bounded by the spectral norm difference of the transition laws.
Proposition 3.9. LetP, P′∈∆S×A
S andHγ= 1/(1−γ). Then, we have θ1(P, P′) = 0 and
sin (θmax(P, P′))≤γHγp
|S|/|A|∥P−P′∥, where ∥·∥denotes the spectral norm.
Before proceeding with the proof of Proposition 3.9, we need the following technical result.
Proposition D.12. For any P∈∆S
S×A the smallest singular value of E−γPsatisfies
σmin(E−γP)≥p
|A|/|S|(1−γ).
25Proof. The main idea of the proof is to use that σmin(A) = min x̸=0∥Ax∥2/∥x∥2for any matrix
A∈Rn×m. We first lower bound σmin(I−γPa) =σmin 
I−γ(Pa)⊤
, where Padenotes the state
transition matrix under action a. Letx∈RS, then we have
 
I−γ(Pa)⊤
x
2≥1/p
|S| 
I−γ(Pa)⊤
x
1
≥1/p
|S| 
∥x∥1−γ(Pa)⊤x
1
≥(1−γ)/p
|S|∥x∥1≥(1−γ)/p
|S|∥x∥2,
where the third inequality follows from
(Pa)⊤x
1=X
sX
s′Pa(s|s′)x(s′)≤X
sX
s′Pa(s|s′)|x(s′)|=∥x∥1.
Hence, σmin(I−γPa)≥(1−γ)/p
|S|. Therefore, we have for any x∈RSthat
∥(E−γP)x∥2
2=X
a∥(I−γPa)x∥2
2≥ |A| /|S|(1−γ)2∥x∥2
2,
which yields the desired result.
We are now ready to prove Proposition 3.9.
Proof of Proposition 3.9. The first principal angle θ1(P, P′) = 0 is zero as we always have 1⊆ U.
The bound on the maximal angle follows from a well-known perturbation result for orthogonal
projections. Namely, if A, B∈Rn×mare matrices of the same rank and ΠA,ΠBdenote the
orthogonal projections onto their column span, then we have [Ji-Guang, 1987]
∥ΠA−ΠB∥ ≤minA†,B†	
∥A−B∥,
where A†denotes the Moore-Penrose inverse [Penrose, 1955]. Recall that UP= im( E−γP),
sin (θmax(P, P′)) =ΠUP−ΠUP′, and by Proposition D.12
(E−γP)†= (σmin(E−γP))−1≤p
|S|/|A|Hγ.
Therefore, we get
sin (θmax(P, P′))≤p
|S|/|A| ·γ·Hγ· ∥P−P′∥.
E Proof of claim in Example 3.3
We recall Example 3.3 from the main paper.
Example 3.3. We consider a two-state, two-action MDP with S=A={0,1}, uniform initial state
distribution, discount rate γ= 0.9, and Shannon entropy regularization h=−H (see Appendix C).
Suppose the expert reward is rE(s, a) = 1{s= 1}and consider the transition laws, P0andP1,
defined by P0(0|s, a) = 0 .75andP1(0|s, a) = 0 .25 + β· 1{s= 0, a= 0}for some β∈[0,0.75].
Also, consider the two experts µE
P0=RLP0(rE)andµE
P1=RLP1(rE), and suppose we recovered the
reward ˆr(s, a) =−rE. Then, the following holds: 1) We have ℓP0(ˆr, µE
P0) = 0 andℓP1(ˆr, µE
P1) =
O(β). That is, for small β, the reward ˆris a good solution to the IRL problem, as both experts
are approximately optimal under ˆr. 2) The rank condition (5)between P0andP1is satisfied for
anyβ > 0. 3) For a new transition law Pdefined by P(0|s, a) = 1{s= 1, a= 0}, we have
ℓP(rE,RLP(ˆr))≈4.81, i.e.RLP(ˆr)performs poorly under the experts’ reward.
In the following we prove the claims 1. and 2., while 3. is computed via regularized dynamic program-
ming [Geist et al., 2019].3Furthermore, we illustrate the occupancy measure spaces corresponding to
P0andP1for different βin Figure 3.
3The code is openly accessible at https://github.com/andrschl/irl_transferability .
26Figure 3: The set of occupancy measures MP0andMP1are illustrated in RS×A/1∼=1⊥. For
a two-state-two-action MDP, the set of occupancy measures is given by the intersection of a two-
dimensional affine subspace (a plane in RS×A/1) with the probability simplex in R4(a tetrahedron
inRS×A/1). We see that for a small β, the sets MP0andMP1are approximately parallel. That
is, the angle between their normal vectors, which span the potential shaping spaces UP0andUP1,
is small. In contrast, for a large βthe orientation of MP0andMP1is very different, resulting in a
large angle between the corresponding normal vectors.
1.Consider the transition law P′defined by P′(0|s, a) = 0 .25. First, we observe that whileP0−P′is large, the potential shaping spaces UP0andUP′coincide. To see this note
that we have P′(·|s, a) =P0(·|s, a) + ∆ , where ∆ = [−0.5,0.5]⊤. Hence, we have for
anyx∈R2that
(E−γP′)x= (E−γP0)x−γ⟨∆, x⟩14= (E−γP0)
x−γ⟨∆, x⟩
1−γ12
,
where 1ndenotes the all-one vector in Rn. Therefore, span( E−γP′) = span( E−γP0).
Moreover, we have
P1−P′≤sX
s,s′,a(P1(s′|s, a)−P′(s′|s, a))2=√
2β.
In light of Propositions D.10, D.11, and 3.9, this implies that
ℓP1(ˆr,RLP1(rE))≤2[ˆr]UP1−[rE]UP1
2
≤2ΠUP1−ΠUP′ˆr−rE
2
≤2γ·Hγ· ∥P−P′∥ ≤2√
2γ·Hγ·β.
2. We need to show that P0andP1are satisfying the rank condition
rank 
E−γP0, E −γP1
= 2|S| − 1.
By the same reasoning as above, we can equivalently show the rank condition for the
transition laws P0′, P1′defined by P0′(0|s, a) = 1 andP1′(0|s, a) =β· 1{s= 0, a= 0}.
27To this end, we choose the matrix representation
E=
I
I
and P=
Pa0
Pa1
,
where I∈R|S|×|S|is the identity matrix and Pa0, Pa1∈R|S|×|S|are the state transition ma-
trices corresponding to the actions 0,1, respectively. Let C=
E−γP0′, E −γP1′
.
We have
P0′=
1 0
1 0
1 0
1 0
and P1′=
β1−β
0 1
0 1
0 1
,
and
C=
1−γ0 1−βγ−γ+βγ
−γ1 0 1 −γ
1−γ0 1 −γ
−γ1 0 1 −γ
.
It’s straightforward to see that the vector [1 1 −1−1]⊤lies in the kernel of C, but
there is a 3×3submatrix with non-zero determinant:
det "1−γ0 1−βγ
−γ1 0
1−γ0 1#!
= 1·[(1−γ)−(1−γ)(1−βγ)] =βγ(1−γ)>0.
In other words, we have rankC= 3for any β >0.
F Proof of Theorem 3.10
Theorem 3.10. LetK= 2,θ2(P0, P1)>0, and suppose that Assumptions 2.1,2.2, and 3.5 hold. If
ℓPk(ˆr, µE
Pk)≤ˆεfork= 0,1, then ˆrisε-transferable to P= ∆S×A
S with
ε= ˆε/
ησRsin 
θ2(P0, P1
/2)2
.
The proof of Theorem 3.10 hinges on Lemma 3.6 and the following reward approximation result.
Lemma F.1. Let[rE]UPk−[ˆr]UPk
2≤¯εfork= 0,1. Then, if θ2(P0, P1)>0, it holds that
[rE]1−[ˆr]1
2≤¯ε
sin (θ2(P0, P1)/2).
Proof of Lemma F .1. Throughout this proof, we will use the short-hand notation Uk:=UPkfor
k= 0,1. Recall that since 1⊆ U0∩ U1, we have θ1(U0,U1) = 0 and by assumption we also have
θ2(U0,U1)>0, which implies that U0∩ U1=1. Furthermore, since for k= 0,1we can rewrite
RS×Aas the orthogonal sum RS×A=Uk∩1⊥⊕ U⊥
k⊕1, we can uniquely decompose rE−ˆrinto
rE−ˆr=xk+yk+z, where xk∈ Uk∩1⊥,yk∈ U⊥
k,z∈1, fork= 0,1. Then, it holds that
x0+y0=x1+y1. Since[rE]Pk−[ˆr]Pk
Pk,2=∥yk∥2, the Assumption of Lemma F.1 implies
that∥yk∥2≤¯ε. For the 2-distance between the equivalence classes [rE]1and[ˆr]1the Pythagorean
theorem implies that
[rE]1−[ˆr]12
1,2=∥x0∥2
2+∥y0∥2=∥x1∥2
2+∥y1∥2
2≤ max
uk∈Uk∩1⊥,vk∈U⊥
k,
∥uk∥2=∥vk∥2=1,
αk∈R+,βk∈[0,¯ε],k=0,1,
α0u0+β0v0=α1u1+β1v1α2
0+β2
0,(27)
where the upper bound follows from x0+y0=x1+y1and∥yk∥2≤¯ε. Next, we want to show that
the maximum on the right-hand side of (27) is achieved for β0=β1= ¯ε. To see this, note that taking
inner products between u0andu1, respectively, and the equation α0u0+β0v0=α1u1+β1v1, we
arrive at
α0=α1⟨u0, u1⟩+β1⟨u0, v1⟩, α1=α0⟨u0, u1⟩+β0⟨u1, v0⟩,
28which is for any choice of βk, uk, vk, k= 0,1an invertible linear system of equations for α0, α1
with the solutions
α0=β0⟨u0, u1⟩⟨u1, v0⟩+β1⟨u0, v1⟩
1− ⟨u0, u1⟩2, α1=β1⟨u1, u0⟩⟨u0, v1⟩+β0⟨u1, v0⟩
1− ⟨u1, u0⟩2
where ⟨u0, u1⟩<1, due to U0∩ U1∩1⊥= 0. As the sign of ⟨u0, u1⟩⟨u1, v0⟩and⟨u0, v1⟩can be
chosen arbitrarily by an appropriate choice of v0, v1, the objective in the right-hand-side of (27) is
increasing in β0, β1and hence the maximum is achieved for β0=β1= ¯εandα:=α0=α1=
¯ε⟨u0,v1⟩
1−⟨u0,u1⟩. Therefore, it holds that
[rE]1−[ˆr]12
1,2≤ max
uk∈Uk∩1⊥,v1∈U⊥
1,
∥uk∥2=∥v1∥2=1,k=0,1¯ε2"
1 +⟨u0, v1⟩
1− ⟨u0, u1⟩2#
(i)= max
u0∈U0∩1⊥,
∥u0∥2=1¯ε2
1 + 
maxv1∈U⊥
1,∥v1∥2=1⟨u0, v1⟩
1−max u1∈U1∩1⊥,∥u1∥2=1⟨u0, u1⟩!2

(ii)= max
u0∈U0∩1⊥,
∥u0∥2=1¯ε2
1 +
ΠU⊥
1u0
2
1− ∥ΠU1∩1⊥u0∥2
2

(iii)= max
u0∈U0∩1⊥,
∥u0∥2=1¯ε2
1 +
q
1− ∥ΠU1u0∥2
2
1− ∥ΠU1∩1⊥u0∥2
2

(iv)= max
u0∈U0∩1⊥,
∥u0∥2=1¯ε2
1 +
q
1− ∥ΠU1∩1⊥u0∥2
2
1− ∥ΠU1∩1⊥u0∥2
2

(v)= max
u0∈U0∩1⊥,
∥u0∥2=1¯ε2
1 +1 +∥ΠU1∩1⊥u0∥2
1− ∥ΠU1∩1⊥u0∥2
(vi)= ¯ε2
1 +1 + cos ( θ2(U0,U1))
1−cos (θ2(U0,U1))
(vii)= ¯ε2 2
1−cos (θ2(U0,U1))
(viii)=¯ε2
sin (θ2(U0,U1)/2)2.
Here, we took the maximum over u1, v1in(i), we used that max v∈V,∥v∥2=1⟨v, u⟩=∥ΠVu∥2in
(ii), and (iii)follows from the Pythagorean theorem. Furthermore, (iv)follows from u0∈1⊥and
(v)from simplifying. In (vi)we then again use max v∈V,∥v∥2=1⟨v, u⟩=∥ΠVu∥2, the definition of
the second principal angle (Definition 3.8), and the fact that the first principal vectors lie in 1. Lastly,
(vii)follows from simplifying and (viii)from sin(x/2)2= (1−cosx)/2.
Proof of Theorem 3.10. As mentioned in the proof sketch in the main paper, it follows from the lower
bound in Lemma 3.6 that[rE]UPk−[ˆr]UPk
2≤p
2ˆε/σR. In light of Lemma F.1, this implies that
for any P∈∆S
S×A we have
[rE]UP−[ˆr]UP
2≤[rE]1−[ˆr]1
2≤p
2ˆε/σR
sin (θ2(P0, P1)/2).
Hence, applying the upper bound in Lemma 3.6 yields
ℓP(rE,RLP(ˆr))≤1
2η[rE]UP−[ˆr]UP2
2≤ˆε
ησRsin (θ2(P0, P1)/2)2.
29G Proof of Theorem 3.11
Theorem 3.11. LetK= 1,D:= max r,r′∈R∥r−r′∥2, and suppose that Assumptions 2.1,2.2, and
3.5 hold. If ℓP0(ˆr, µE)≤ˆε, then ˆrisεP-transferable to P∈∆S×A
S with
εP= 2 maxn
2ˆε/σR, D2sin 
θmax(P0, P)2o
/η.
Proof. Similar to Theorem 3.10, it follows from Lemma 3.6 that[rE]UP0−[ˆr]UP0
2≤p
2ˆε/σR.
By Proposition D.11, we then have that[rE]UP−[ˆr]UP
2≤sin 
θmax(P, P0)rE−ˆr
2+[rE]UP0−[ˆr]UP0
2
≤sin 
θmax(P, P0)
D+p
2ˆε/σR.
Hence, applying Lemma 3.6 again yields
ℓP 
rE,RLP(ˆr)
≤1
2η[rE]UP−[ˆr]UP2
2
≤
Dsin 
θmax(P, P0)
+p
2ˆε/σR2
2η
≤2 maxn
D2sin 
θmax(P, P0)2,2ˆε/σRo
η.
H Estimating principal angles
Consider two full rank matrices A, B∈Rn×mand let the columns of UA, UB∈Rn×mform an
orthonormal basis of V= im AandW= im B, respectively. Then, as discussed by [Ji-Guang, 1987]
we have
σi= cos( θi(V,W)), i= 1, . . . , m,
where 1≥σ1≥. . .≥σm≥0denote the singular values of U⊤
AUBsorted in decreasing order.
Hence, given the transition matrices P0, P1, we can compute the principle angles θi(P0, P1)by first
computing orthonormal bases for the column spans of E−γPi, i= 1,2, and then computing the
singular values as described above.
Now, suppose that ˆP0,ˆP1are empirical estimates of P0, P1, then we have by [Ji-Guang, 1987,
Theorem 3.1] the following perturbation result
max
isin(θi(P0, P1))−sin(θi(ˆP0,ˆP1))≤ΠUP0−ΠUˆP0+ΠUP1−ΠUˆP1
≤γHγp
|S|/|A|P0−ˆP0+P1−ˆP1
,
where the last inequality follows from Propositions D.11 and 3.9. Hence, we can estimate
sinθi(P0, P1)up to an error of O
maxnP0−ˆP0,P1−ˆP1o
.
I Proof of Theorem 4.1
Theorem 4.1. Suppose that NE= Ω 
Klog(|S||A| /ˆδ)/ˆε2
andHE= Ω 
log(K/ˆε)/log(1/γ)
.
Running Algorithm 1 for T= Ω 
K2/ˆε2
iterations with step-size α= 1/(K√
T), where δopt=
O ˆδˆε2/K3
,εopt=O(ˆε/K),N= Ω 
Klog(K|S||A| /(ˆδˆε))/ˆε2
, and H=HE, it holds with
probability at least 1−ˆδthatℓPk(ˆr, µE
Pk)≤ˆε,fork= 0, . . . , K −1.
The proof of Theorem 4.1 is inspired by [Syed and Schapire, 2007, Theorem 2]. However, in contrast
to Syed and Schapire [2007], we consider the regularized problem with multiple experts, we use the
suboptimality as the convergence metric, and we use a projected gradient descent update (instead of
multiplicative weights). The proof hinges on Hoeffding’s inequality and a regret bound for online
gradient descent, which are provided in Theorem I.1 and I.2 below.
30Theorem I.1 (Hoeffding’s inequality [Hoeffding, 1963]) .LetX0, . . . , X M−1be independent random
variables with Xl∈[a, b]and let SM:=X0+. . .+XM−1. Then,
Pr (|SM−ESM| ≥c)≤2 exp
−2c2
M(b−a)2
.
Theorem I.2 (Online gradient descent [Zinkevich, 2003]) .Consider some bounded closed convex
setX ⊂RnwithD:= max x,x′∈X∥x−x′∥2. Moreover, let ΠX:Rn→ X be the orthogonal
projection onto X. For any sequence of convex differentiable functions f0, . . . , f T−1:X → R
satisfying max x∈X∥∇ft(x)∥2≤G, the online projected gradient descent update
xt+1←ΠX(xt−α∇ft(xt)),
with step-size α=D/(G√
T)satisfies
T−1X
t=0ft(xt)−min
x∗∈XT−1X
t=0ft(x∗)≤DG√
T.
Proof of Theorem 4.1. The proof is in three steps. First, we use Hoeffding’s inequality to prove
concentration of the empirical occupancy measures around the true occupancy measures. Then, we
use the union bound to upper bound the probability that any of our bounds fails to hold. Finally, we
prove the convergence rate of Algorithm 1 using the regret bound in Theorem I.2.
Step 1: LetD={(s0, a0, . . . , s H−1, aH−1)}N−1
i=0be sampled from some policy πµand recall that
the corresponding empirical occupancy measure is defined as
ˆµD(s, a) =1−γ
NN−1X
i=0H−1X
t=0γt1{si
t=s, ai
t=a}.
It will be convenient to define the truncated occupancy measure
µH(s, a) = (1 −γ)H−1X
t=0γtPπµ
ν0{si
t=s, ai
t=a}.
ForKdata sets D1, . . . ,DKsampled from πµkwe then have
max
r∈RK−1X
k=0⟨r, µk−ˆµDk⟩(i)
≤max
r∈R∥r∥1K−1X
k=0(µk−ˆµDk)
∞(ii)
≤K−1X
k=0(µk−ˆµDk)
∞
(iii)
≤K−1X
k=0(µk−µH,k)
∞| {z }
I1+K−1X
k=0(µH,k−ˆµDk)
∞| {z }
I2,
where (i)follows from Hölder’s inequality, (ii)from our definition of Ras the 1-norm ball, and (iii)
from the triangle inequality. Since ∥µ−µH∥∞≤γH, we have I1≤γHK. Moreover, applying
Hoeffding’s inequality to the M=KN independent random variables
XkN+i=1−γ
NH−1X
t=0γt1{sk,i
t=s, ak,i
t=a}, i∈[N], k∈[K],
withXi∈[0,1/N], we arrive at
Pr (|SM−ESM| ≥εstat/2) = Pr K−1X
k=0ˆµDk(s, a)−µK,k(s, a)≥εstat/2!
≤2 exp
−ε2
statN
2K
.
Hence, applying the union bound over all |S||A| components of the occupancy measure yields
Pr(I2< ε stat/2) = 1 −Pr(I2≥εstat/2)≥1−2|S||A| exp
−ε2
statN
2K
.
31Therefore, to ensure that with probability at least 1−δstatit holds that
max
r∈RK−1X
k=0⟨r, µk−ˆµDk⟩ ≤εstat,
it suffices to choose
N≥2Klog (2|S||A| /δstat)
ε2statand H≥log (2 K/ε stat)
log(1/γ).
This concentration result applies to both empirical occupancy measures generated from the expert
data sets DE
k, as well as the data sets Dk,tgenerated by Algorithm 1.
Step 2: When analyzing Algorithm 1 there are three sources of stochasticity. The first two are
due to the randomness in the data sets DE
kandDk,t, and the third is due to the randomness in
the forward RL algorithm, Aεopt,δopt
Pk, that upon a query with the reward rtoutputs a policy πk,t
such that with probability at least 1−δoptit holds ℓPk(rt, µπk,t)≤εopt. Let’s denote the event that
max r∈RPK−1
k=0⟨r, µE
Pk−ˆµDE
k⟩> ε stat,EbyEstat,E, the event that max r∈RPK−1
k=0⟨r, µπk,t−ˆµDk,t⟩>
εstatbyEstat,t, and the event that ℓPk(rt, µπk,t)> ε optbyEopt,k,t. Moreover, let us assume that Estat,E
happens with probability at most δstat,E,Estat,thappens with probability at most δstat, andEopt,k,t
happens with probability at most δopt. By union bound, the probability of the event
F:=¬Estat,E∧T−1^
t=0¬Estat,t∧T−1^
t=0K−1^
k=0¬Eopt,k,t,
that none of the above events happens is lower bounded by
Pr (F) = 1−Pr 
Estat,E∨T−1_
t=0Estat,t∨T−1_
t=0K−1_
k=0Eopt,k,t!
≥1− 
Pr (Estat,E) +T−1X
t=0Pr (Estat,t) +T−1X
t=0K−1X
k=0Pr (Eopt,k,t)!
≥1−(δstat,E+Tδstat+KTδ opt).
Hence, to ensure that Fhappens with probability at least 1−ˆδ, it suffices to choose
N≥2Klog
6|S||A| /ˆδ
ε2
stat,Eand H≥log (2 K/ε stat,E)
log(1/γ),
Nt≥2Klog
6T|S||A| /ˆδ
ε2statand δopt=ˆδ
3KT.
Step 3: Note that we can bound ∥gt∥2≤ ∥gt∥1≤PK−1
k=0ˆµDE
k
1+∥ˆµk,t∥1≤2K=:Gand the
diameter of RisD= 2. Hence, given that event Fhappens, we can bound the suboptimalities of the
32Kexperts under the reward, ˆr, recovered by Algorithm 1 with stepsize α=D/(G√
T)as follows
K−1X
k=0ℓPk(ˆr, µE
Pk)
=K−1X
k=0
max
µ∈MPk⟨ˆr, µ−µE
Pk⟩ −¯h(µ) +¯h(µE
Pk)
(i)
≤εstat,E+K−1X
k=0
max
µ∈MPk⟨ˆr, µ−ˆµDE
k⟩ −¯h(µ) +¯h(µE
Pk)
(ii)
≤εstat,E+K−1X
k=01
TT−1X
t=0
max
µ∈MPk⟨rt, µ−ˆµDE
k⟩ −¯h(µ) +¯h(µE
Pk)
=εstat,E+1
TT−1X
t=0K−1X
k=0
max
µ∈MPk⟨rt, µ−ˆµDE
k⟩ −¯h(µ) +¯h(µE
Pk)
(iii)
≤εstat,E+Kε opt+1
TT−1X
t=0K−1X
k=0h
⟨rt, µk,t−ˆµDE
k⟩ −¯h(µk,t) +¯h(µE
Pk)i
(iv)
≤εstat,E+Kε opt+εstat+1
TT−1X
t=0K−1X
k=0h
⟨rt,ˆµDk,t−ˆµDE
k⟩ −¯h(µk,t) +¯h(µE
Pk)i
(v)
≤εstat,E+Kε opt+εstat+DG√
T+ min
r∈R1
TT−1X
t=0K−1X
k=0h
⟨r,ˆµDk,t−ˆµDE
k⟩ −¯h(µk,t) +¯h(µE
Pk)i
(vi)
≤2εstat,E+Kε opt+ 2εstat+DG√
T+ min
r∈R1
TT−1X
t=0K−1X
k=0
⟨r, µk,t−µE
Pk⟩ −¯h(µk,t) +¯h(µE
Pk)
(vii)
≤2εstat,E+Kε opt+ 2εstat+DG√
T
+ min
r∈RK−1X
k=0
⟨r,¯µk−µE
Pk⟩ −¯h(¯µk) +¯h(µE
Pk)
| {z }
≤0,with¯µk:=1
TT−1X
t=0µk,t,
(viii)
≤2εstat,E+Kε opt+ 2εstat+DG√
T= 2εstat,E+Kε opt+ 2εstat+4K√
T.
Here, the inequalities (i),(iv), and (vi)follow from the concentration bound established in step 1.
Moreover, inequality (ii)holds since ˆr7→max µ∈MPk⟨ˆr, µ−µE
Pk⟩−¯h(µ)+¯h(µE
Pk)is the pointwise
maximum of affine functions and therefore convex. Furthermore, (iii)follows from εopt-optimality of
µk,t,(v)from Theorem I.2, and (vii)from concavity of the mapping µk,t7→ ⟨r, µk,t−µE
Pk⟩−¯h(µk,t).
Finally, (viii)holds because all experts are optimal for the reward rE. In conclusion, to ensure
that with probability at least 1−ˆδit holds that ℓPk(ˆr, µE
Pk)≤PK−1
k=0ℓPk(ˆr, µE
Pk)≤ˆεit suffices
to choose T=256K2
ˆε2,α=ˆε
16K2,N=128Klog(6|S||A| /ˆδ)
ˆε2 ,H=Ht=log(16 K/ˆε)
log(1/γ),Nt=
128Klog(1536K2|S||A| /(ˆδˆε2))
ˆε2 ,δopt=ˆδˆε2
768K3,εopt=ˆε
4K.
J Suboptimal experts
In our problem formulation, we assumed that the Kexperts are optimal with respect to rE, i.e.
µE
Pk=RLPk(rE)fork= 0, . . . , K −1. This assumption can be weakened by requiring that
max
r∈RJ(r, µE
Pk)−J(r,RLPk(rE))≤εmis,
33where εmisis some misspecification error. The transferability results in Theorem 3.10 and 3.11
still apply whenever we recover a reward ˆrsuch that ℓPk(ˆr,RLPk(rE))≤ˆε. Moreover, with a
straightforward modification of the proof of Theorem 4.1, it follows that with high probability
Algorithm 1 recovers a reward ˆrsuch that ℓPk(ˆr,RLPk(rE))≤ˆε+ 2Kε mis. Hence, our end-to-end
transferability guarantees apply with ˆε←ˆε+ 2Kε mis. However, εmiscannot be further reduced by
collecting more samples from the expert or MDP.
K Experimental details
Setup To validate our results experimentally, we are using a stochastic adaption of the
WindyGridworld environment [Sutton and Barto, 2018].4In particular, we consider a 6x6 grid with
4 actions (Up, Down, Left, Right), a wind direction (North, East, South, West), and a wind strength
β∈[0,1]. When the agent takes an action, with probability (1−β), it moves to the intended grid
cell, and with probability β, the wind pushes the agent one step further in the direction of the wind.
This means that the transition law is a convex combination of two laws: (1−β)PGridworld+βPWind,
where PGridworldandPWindrepresent the transition laws for a deterministic Gridworld and a deter-
ministic WindyGridworld . For our experiments, we then consider the pairs of expert transition laws
P0
β= (1−β)PGridworld+βPNorthandP1
β= (1−β)PGridworld+βPEastwithβin{0.01,0.1,0.5,1.0}.
As shown in Figure 4(a), the second principal angle between P0
βandP1
β, calculated using a singular
value decomposition [Knyazev and Argentati, 2002], increases as the wind strength βincreases.
Inverse reinforcement learning We observed that under a small second principal angle, the
recovered reward heavily depends on both the expert reward and the reward initialization. Hence,
we sample 10 independent expert rewards, each generated by first sampling a random set of 10
state-action pairs and then randomly assigning a reward of ±1. Using Shannon entropy regularization
withτ= 0.3, we then use soft policy iteration to get expert policies for each combination of expert
reward and wind strength β. For each of these expert policies, we then generate expert data sets
withNE∈ {103,104,105,106}trajectories of length H= 100 . Next, we run Algorithm 1, with soft
policy iteration as a subroutine, for 30′000iterations, where rewards are initialized by sampling from
a standard normal distribution. As a reward class, we choose the ∥·∥1-ball with radius 103(essentially
unbounded), as a stepsize α= 0.05for the first 15′000iterations and α= 0.005for the second half.
Moreover, we sample N= 100 new trajectories of horizon H= 100 at each gradient step. Figure 4(b)
illustrates the distances between the recovered ˆrand the experts’ reward rE, measured in RS×A/1.
It is evident that the recovered reward gets closer to the experts’ reward as the number of expert
demonstrations increases. Moreover, we observe that the recovered reward is closer to the experts’
reward when the second principal angle between the experts is larger, as expected from Lemma F.1.
Transferability We evaluate the transferability of the obtained reward by considering two new
environments. First, a south wind setting PSouthwith wind strength β= 1, and second, a deterministic
gridworld PShifted, with cyclically shifted actions, i.e., Right →Down, Up →Right, Left →Up,
Down→Left. In Figure 4(c) and (d), we illustrate the transferability in terms of ℓPSouth(rE,RLPSouth(ˆr))
andℓPShifted(rE,RLPShifted(ˆr)), respectively. We observe that for both environments the transferability
improves with a larger second principal angle, thus confirming our theoretical result in Theorem 3.10.
The effect is even more pronounced for the shifted environment. While confirming our results, the
experiments also reveal a high sample complexity in terms of expert demonstrations. This is to be
expected, as IRL aims to match the expert’s empirical occupancy measure, leading to overfitting
when there are not enough demonstrations [Ho and Ermon, 2016]. This issue can be mitigated by
reducing the dimension of the reward class (see e.g. [Abbeel and Ng, 2004]).
4All our experiments were carried out – within a day – on a MacBook Pro with an Apple M1 Pro chip and 32
GB of RAM.
34(a) ( b) ( c) ( d)
Figure 4: ( a) shows the second principal angle between P0
βandP1
βfor varying wind strength β.
Furthermore, ( b) shows the distance between ˆrandrEinRS×A/1for a varying number of expert
demonstrations NEand wind strength β. Moreover, ( c) and ( d) show the transferability to PSouth
andPShiftedin terms of ℓPSouth(rE,RLPSouth(ˆr))andℓPShifted(rE,RLPShifted(ˆr)), respectively. The dots
indicate the median and the shaded areas the 0.2 and 0.8 quantiles over the 10 independent realizations.
NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: As mentioned in the abstract and introduction, our main contributions are
sufficient conditions for learning transferable rewards, as given in Theorem 3.10 and 3.11.
Furthermore, we also provide an algorithm and experiments in Section 4 and 5.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitations in the Limitations and future work paragraph in
Section 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
35•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Yes, all assumptions are stated in the main paper, and proofs are provided in
the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All details about our experiments are provided in Appendix K. Furthermore,
we also provide the code attached as a .zip file.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
36dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: All code (for experiments and computations in Example 3.3) is attached as a
.zip file. Furthermore, we provide a README.md with instructions how to run the code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Yes, we discuss the chosen stepsizes and parameters in Appendix K.
37Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We repeated our experiments over 10 random seeds and provide quantile plots.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Yes, the hardware details are specified as a footnote in Appendix K.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [NA]
38Justification: This work is purely theoretical and did not cause any harm to society.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This work is purely theoretical.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We don’t work with any potentially harmful models or datasets.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
3912.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The experiments are based on our own codebase.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide our code as a .zip file. We will make it openly available on github
later.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This research does not involve any human experts but only synthetically
generated ones.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
40•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No humans are involved in this research.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
41