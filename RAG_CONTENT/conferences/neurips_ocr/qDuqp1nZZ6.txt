Differentially Private Equivalence Testing for
Continuous Distributions and Applications
Daniel Omer
Math. Dept.
Bar-Ilan University
omerdan@biu.ac.ilOr Sheffet
Faculty of Engineering
Bar-Ilan University
or.sheffet@biu.ac.il
Abstract
We present the first algorithm for testing equivalence between two continuous
distributions using differential privacy (DP). Our algorithm is a private version
of the algorithm of Diakonikolas et al [ 16]. The algorithm of [ 16] uses the data
itself to repeatedly discretize the real line so that — when the two distributions
are far apart in Ak-norm — one of the discretized distributions exhibits large
L2-norm difference; and upon repeated sampling such large gap would be detected.
Designing its private analogue poses two difficulties. First, our DP algorithm can
notresample new datapoints as a change to a single datapoint may lead to a very
large change in the descretization of the real line. In contrast, the (sorted) index of
the discretization point changes only by 1between neighboring instances, and so
we use a novel algorithm that set the discretization points using random Bernoulli
noise, resulting in only a few buckets being affected under the right coupling.
Second, our algorithm, which doesn’t resample data, requires we also revisit the
utility analysis of the original algorithm and prove its correctness w.r.t. the original
sorted data; a problem we tackle using sampling a subset of Poisson-drawn size
from each discretized bin. Lastly, since any distribution can be reduced to a
continuous distribution, our algorithm is successfully carried to multiple other
families of distributions and thus has numerous applications.
1 Introduction
Differential privacy (DP), a mathematically rigorous notion that bounds the effect of any single datum
on the output distribution, is the current (de facto) gold-standard of privacy preserving data analysis.
By now we have a myriad of DP-algorithms for learning and for various tasks of statistical inference .
Indeed, the design of DP-hypothesis testers is crucial for the dissipation of DP into other data-centric
fields — such as economics, education and health — that analyze sensitive data in massive quantities.
However, by and large the design of DP-hypothesis testing is confined to distributions over finite (and
thus discrete) domains rather than contiunuous distributions.
Hypothesis testing over continuous distributions poses a special challenge due to infinitesimally
small perturbations — two continuous distributions can have Total-Variation distance of αusing,
say, exponentially many intervals each with an exponentially small shift of probability mass, which
clearly cannot be detected with polynomially-size sample. Luckily, this issue was resolved in the
works of [ 10,15,16] who restricted the TV-distance to using k-intervals. Formally, these works
measure distance in Ak-norm : where for any two distributions PandQwe have that ∥P − Q∥ Ak=
sup
IPk
j=1|P(Ij)− Q(Ij)|where Iis a partition of the real-line Rintokintervals I1, I2, ..., I k.
But the fact remains that continuous distributions pose a special challenge for DP-algorithm designers.
In fact, releasing even a simple statistics like the median is impossible over infinite domains [ 6,7].
38th Conference on Neural Information Processing Systems (NeurIPS 2024).And yet, as we show in this work, it is possible to compare (samples from) two continuous distributions
while preserving differential privacy and discern whether the two are identical or far-apart in Ak-
distance. This suggests a sharp contrast between the task of learning from a (single) continuous
distribution and the task of statistical inference based on twocontinuous distributions.
Baseline. This sharp contrast may seem striking at first, yet on second thought, it is known that
any statistical inference task that only has two possible outputs (in our case - “accept / identical” vs
“reject / far-apart”) can easily be made private using the Subsample-and-Aggregate framework [ 26] —
simply run O(1/ϵ)-times the non-private equivalence tester of Diakonikolas et al [ 16] and return the
most prevalent output using simple noisy count. This gives a simple ϵ-DP equivalence tester with
O((k4/6α−6/5+k1/2α−2)/ϵ)sample complexity with two clear drawbacks: (1) its large sample
complexity bound and (2) its decision to accept or reject is explained as ‘in the majority of the runs
of the tester it decided so’. The algorithm we present in this work improves on both aspects: it has
a sample complexity of ˜O
maxn
k4/5
α6/5,k1/2
α2,k2/3
αϵ1/3,k1/3
α4/3ϵ2/3,√
k
αϵo
and it is capable of producing
numerical estimators directly from the data that explain its accept/reject decision.
Our Algorithm. The difference that makes learning substantially more difficult from equivalence
testing is that when having two distributions we are capable of “pitting one against the other”: roughly
speaking, we can partition the real line into intervals based on points from one distribution and see
whether this partition acts as a random partition for the batch of examples that come from the second
distribution. This involves only sorting allspoints from our sample on the real line and dealing
with the order statistics. Our algorithm is based on privatizing the equivalence tester of [ 16], which
operates over the continuous real line. This algorithm works in two stages: In the first stage, it looks
at autocorrelation statistics that involve all (s−1)pairs of adjacent (post-sorting) data points. In
the second stage, it equipartitions the data using into mbins using a random draw of mpoints and
repeatedly runs the following operation: run a closeness L2-norm based tester on the discretized
m-bins distribution that draws Nnew points, and should it not reject – merge every pair of adjacent
bins to create a m/2-bins discretization and continue.
Our DP-version of this tester also works in similar stages. The first stage is almost trivially privatized
(the statistics estimated in the first stage exhibits small global sensitivity) but numerous difficulties
arise in the privatization of the second stage. Most notably — the fact we use datapoints to define
a partition into bins. To that end, we replace it so that we use sorted-indices, implying that a bin is
composed of all datapoints in sorted indices from index πito index πi+1(not including). This asserts
that any single bin changes by at most two datapoints between neighboring samples. Combining
this with the fact that under the null-hypothesis the difference between the number of points from
PandQin each bin is proportional to the square-root of the bin size, we can bound the sensitivity
in a Propose-Test-Release fashion [ 20] for any single bin. However, the approach of fixing sorted
indices raises two concerns. The first is that we have to revisit the utility analysis of [ 16] because
we no longer use re-sample new points to estimate the L2-norm difference of the two discretized
distributions but rather re-visit the same dataset from which the bin-defining datapoints are taken. We
bypass this difficulty by sampling ourselves Poisson-drawn subsample of each bin, and by focusing
our analysis on a single iteration that should cause us to reject.
The second concern lies in the privacy analysis, and it is the observation that a change to a single
bin isn’t enough to bound the sensitivity of our statistical estimator. The algorithm computes an
L2-approximation of the norm-difference using all bins, and in the extreme case of two neighboring
datasets that differ on the very first (sorted) datapoint that ends up as the very last datapoint in the
neighboring dataset we have that allbins shift. To that end we use the following novel idea: we add a
Ber(1/2)-random variable to each of the bin-defining indices. On the one hand, this possible shift
by 1 changes very little in terms of the analysis; on the other hand, it allows us to argue that within
log4(1/δ)random shifts we can correlate these Bernoulli random variables so that the bins identify.
Full details of our algorithm and its analysis appear in Section 3.
Applications. Having designed our private equivalence tester for continuous distributions under
Ak-norm, we can now apply it to test equivalence between two distributions from a family Cof
distributions which, under suitable partition, yield large enough Ak-distance. The following fact is
immediate.
2Fact 1. Given a univariate distribution family Cand α, let k=k(C, α)be
the smallest integer such that for any f1, f2∈ C it holds that ∥f1−f2∥1≤
∥f1−f2∥Ak+α/2. Then there exists a (ϵ, δ)-DP equivalence testing algorithm for Cusing
˜O 
max
k4/5/α6/5,k2/3/αϵ1/3,k1/2/α2,k1/3/α4/3ϵ2/3,√
k/αϵ	
samples.
In Section 4 we detail, much like [ 16], a variety of such hypothesis-families and the sample complex-
ities we obtain for their respective DP-equivalence testers.
1.1 Related Work
Over the past twenty years, significant progress has been made in distribution property testing.
Initially, research focused on the sample sizes required to assess properties of arbitrary distributions
with a specific support size. Goldreich and Ron [ 23] introduced uniformity testing, proposing an
algorithm using collision statistics with a sample complexity of√n
α4. Batu et al [ 5] studied closeness
testing, evaluating whether two distributions are close in total variation. Paninski [ 27] established
the first lower bound for the sample complexity of uniformity testing at Ω(√n
α2)and proposed a new
uniformity test using unique element statistics. The works of [ 1,10,29] explored optimal bounds for
identity testing, focusing on χ2-based testers. They established a lower bound for closeness testing at
O
n2/3
α4/3+√n
α2
, with [ 10] providing matching upper bounds for L2closeness testing under certain
conditions. [ 14]. introduced a technique reducing distribution norms to O 1
n
by expanding the
domain size, demonstrating an efficient L2tester for closeness testing. Recent works [ 11,15,16] have
leveraged structural assumptions for more efficient testers in various settings, including continuous
cases. These studies used the Akmetric, aligning with the Kolmogorov distance for k= 2 and
total variation distance for kbeing the domain size. Goldreich [ 22] showed that identity tests could
be reduced to uniformity testing. While earlier testers relied on proxy measures like L2andχ2,
and [ 12,13] demonstrated efficient sample complexity using direct L1metric testers due to low
sensitivity. We refer the interested reader to Cannone’s excellent survey [9].
Several recent papers have examined hypothesis testing problems under the framework of differ-
ential privacy. Cai et al [ 8] used χ2statistic for identity testing, achieving a sample complexity
of˜O
maxn√n
α2,√n
α3/2ϵ,n1/3
α5/3ϵ2/3o
·log(1/β)
. Aliakbarpour et al [ 4] proposed three algorithms
for differentially private uniformity and closeness testing. They privatized unique element algo-
rithms, collision-based testers, and χ2tests, each with specific sample complexities and limitations.
Acharya et al [ 2] established a lower bound for private identity testing, suggesting that small expected
Hamming distances might compromise privacy guarantees. They proposed a sample complexity con-
cerning privacy and distance parameters by privatizing an L1statistic-based algorithm. Aliakbarpour
et al [ 3] also examined closeness testing between distributions with unequal sample sizes, introducing
a new technique to privatize the ‘flattening’ method through data permutation. Zhang [ 30] derived an
upper bound for closeness testing by privatizing an empirical total variation method, demonstrating
small sensitivity and sample complexity of O√n
α2+n2/3
α4/3+√n
α√ϵ+n1/3
α4/3ϵ2/3+1
αϵ
.
Comparison to Existing Lower Bounds. The well-known lower bound for Akcloseness testing in
the non-private regime is given in [ 16]. It is equal to O
k4/5
α6/5+√
k
α2
. As far as we know, there is no
known lower bound for the private regime in that task. In [ 2], a lower bound for the private regime in
identity testing is presented as O√n
α2+n2/3
α4/3+√n
α√ϵ+n1/3
α4/3ϵ2/3+1
αϵ
(where n=kis the domain
size), which is a simpler task than closeness testing. Additionally, [ 30] provided an upper bound equal
to the lower bound of the closeness testing also in the private regime when he crucially relies on the L1
tester that is known with low sensitivity. We have established an upper bound that is asymptotically
close to the lower bound, given by ˜O 
max
k4/5/α6/5,k2/3/αϵ1/3,k1/2/α2,k1/3/α4/3ϵ2/3,√
k/αϵ	
. The
difference between our upper bound and the lower bound lies in two specific terms:√
k
αϵandk2/3
αϵ1/3
The first term is a result of the high sensitivity in our algorithm due to the use of L2norm testing,
which was selected to ensure the utility proof. Additionally, we did not focus on optimizing the term
log k
αϵδ
.
32 Preliminaries
Equivalence (Closeness) Testing. We assume oracle access to two distributions P,Qwhich gives
an i.i.d. example from the resp. distribution. We also use P(S)(resp.Q(S)) to denote the total
probability mass assigned by P(resp.Q) to a set S. We assume the two distributions are continuous
with no discrete point mass, which can always be obtained by concatenating each sample with a
uniformly drawn number ∈R[0,1]. An equivalence tester between the two distributions should return
NULL w.p.≥2/3if it holds that P=Qand return ALT w.p.≥2/3if it holds that ∥P − Q∥ Ak≥α.
Differential Privacy. Two databases DandD′are considered neighboring databases if they differ
by exactly one record, noted as d(D, D′) = 1 , where d(·,·)represents the Hamming distance. Given
a domain X, two multi-sets S, S′⊂ X are called neighbors if they differ on a single entry. An
algorithm (alternatively, mechanism) Mis said to be (ϵ, δ)-differentially private (DP) [ 21,19] if
for any two neighboring S, S′and any set Oof possible outputs we have: Pr[M(S)∈O]≤
eϵPr[M(S′)∈O] +δ. Ifδ= 0then we say the algorithm Misϵ-DP.
The Global Sensitivity of a function f:X → Rdis defined as the maximal difference
max S,S′∥f(S)−f(S′)∥1. It is known that adding Lap(GS(f)/ϵ)to each coordinate of f(S)isϵ-DP;
where Lap(λ)denotes the Laplace Distribution with parameter λ, whose PDF isPDF(x)∝e−|x|/λ.
It is known [ 19] that if A1andA2are(ϵ1, δ1)-DP and (ϵ2, δ2)-DP resp., then their composition is
(ϵ1+ϵ2, δ1+δ2)-DP. It is also known [ 18] that the k-fold composition of kalgorithms, each is (ϵ, δ)-
DP is (ϵ∗, kδ+δ′)-DP for any δ′>0andϵ∗=kϵ(eϵ−1)+2 ϵp
klog(1/δ′). Lastly, it is also known
that if there exists an event Esuch that under Eholding, algorithm Msatisfies that for any two neigh-
boring SandS′and any set of outputs Owe have that Pr[M(S)∈O| E]≤eϵPr[M(S′)∈O| E]+δ
then algorithm Mis(ϵ, δ+ Pr[¬E])-DP.
Poisson Distribution. The Poisson distribution Poi(λ)is a discrete distribution over the Naturals
which satisfies Pr[k] =e−λλk/k!. It has multiple properties that make it easy to work with.
Proposition 2. •The sum of two ind. Poisson Poi (λ1)and Poi (λ2)is Poi (λ1+λ2).
•LetX1, X2, ...be i.i.d. Bernoulli r.v.s. with parameter p; then drawing t∼Poi(λ), it holds
thatPt
i=1Xiis distributed like Poi (λp).
•IfX∼Poi(λx)andY∼Poi(λy)are two ind. Poisson r.v.s, then the distribution of X
conditioned on the event that X+Y=nis Binomial Bin (n,λx
λx+λy).
•IfXi∼Poi(λ)thenPr[|X−λ|> k]≤2 exp
−k2
2(λ+k)
. So for any β > 0, setting
k= 2p
λlog(2/β)we get Pr[|X−λ|>2p
λlog(2/β)]≤βprovided λ >4 log( 2/β).
Misc. We use ˜O(resp. ˜Ω) to denote big- O(resp. big- Ω) up to poly-log factors. We made no effort to
minimize constants or the degree of the poly-log.
3 Private Equivalence Testing for Continuous Distributions
In this section, we present our DP-tester for equivalence between two distributions with large Ak
distance and give both its formal privacy guarantee and its sample complexity bounds. The tester is
detailed in Algorithm 1 (which in turn invokes Algorithm 2), where cdkndenotes a large enough
constant detailed in [16].
The algorithm consists of two parts. In terms of the non-private algorithm, both parts function
similarly to the algorithm presented by [ 16]. Our main contribution is the privatization of this
algorithm. In the first part the main objective is to compute the estimator Z(line I.5) which is
privatized using a straightforward approach – since it has low sensitivity we merely add to is some
Laplace noise. The second part is more complex, as it involves discretizing the domain based on the
data itself, resulting in high sensitivity. We addressed this issue by creating the initial partition of
the domain and adding a Bernoulli random variable to each index position (line II.5). Consequently,
the algorithm can iteratively run j0iterations of the L2-tester TestCloseness on based on the
randomized partition, where in each invocation of TestCloseness we sample a Poisson-size batch
4from each partition (line II.11). If the L2-tester doesn’t reject, then we merge adjacent partition
cells (line II.15) and move on to the next iteration. If either invocations of TestCloseness (or the
estimator Zhas too high of a value) then we reject; but if all test pass we return NULL .
Algorithm 1 Private Equivalence Tester
Input: 2 continuous distributions P,Q, distance parameter α, privacy parameter ϵ, δ.
Output: “NULL " ifP=Q; “ALT" if∥P − Q∥ Ak≥α
1:Part I:
1. Set m←100cdkn
k4
5/α6
5+k2
3/αϵ1
3
2. Draw s∼Poi(m)points from1
2(P+Q).
3.Label each xidrawn from Pwithℓ(xi) = 1 and label each xjdrawn from Qasℓ(xj) =−1.
4. Sort the sample, denote the outcome as x(1)< x(2)<···< x(s).
5.Z=Ps−1
i=1ℓ(x(i))ℓ(x(i+1))
6. Draw X∼Lap(6/ϵ).
7.˜Z=Z+X
8.if˜Z >m3α3
2k2then return ALT
2:Part II:
1. Set N←107
k1/3
α4/3ϵ2/3+√
k
αϵ+√
k
α2
log6(k/αϵδ)and assert mdivides N.
2. Draw sp∼Poi N
2
andsq∼Poi N
2
ind. Set s←sp+sq.
3.S←a sample of spi.i.d. examples from Pandsqi.i.d. examples from Q.
4. Sort S. Denote the outcome as x(1)< x(2)≤ ··· < x(s).
5.For each 1≤i≤msetπi←i·N
m+Bifor ind. drawn Bi∼Ber(1/2). Also set
π0←0, πm+1←s+ 1.
6.Form the Partition ¯Π0={Π0
1,Π0
2, ...,Π0
m}where Π0
i={x(i′):πi< i′< πi+1}for every
1≤i≤m.
7. Set j0←1 +⌈log (m/k)⌉andm0←m.
8.forj= 0,1, . . . , j 0−1do:
9. fori= 1,2, . . . , mjdo:
10. Draw NJ
i∼Poi(N
4mj).
11. Pick a u.a.r. subset CJ
iofNj
ipoints from Πj
i.(IfNj
i>|Πj
i|then use special ⊥points.)
12. Set Xj
i(resp. Yj
i) as #points from P(resp.Q) inCj
i.
13. ifTestClosenessP
iNj
i, mj,⟨Xj
i⟩,⟨Yj
i⟩,α
12√2k+1·log(1/α),ϵ
8√
j0log(2/δ),δ
2j0, δ
=ALTthen
14. return ALT
15. Merge cells: Set mj+1← ⌊mj/2⌋, and for each 1≤i≤mj+1set:
Πj+1
i←Πj
2i−1∪ {x(πi′)} ∪Πj
2i ▷ i′=the separating index of the two bins
3:return NULL
3.1 Privacy Proof
In this subsection, our goal is to proof the following theorem.
Theorem 3. Algorithm 1 is (2ϵ,2δ)-DP .
The proof of Theorem 3 shows that Part I of Algorithm 1 is ϵ-DP — which is very straight forward,
whereas Part II of the algorithm is (ϵ,2δ)-DP — which involves more intricate arguments. In fact, all
that is required regarding the DP of Part I is the following claim. Its proof, as well as most proofs in
this section, is deferred to Appendix A.
Claim 4. The estimator Z=Ps
i=1ℓ(x(i))ℓ(x(i+1))in Part I of Algorithm 1 (Line 5) has global
senstivity of 6.
5Algorithm 2 TestCloseness -(N, m, ¯X,¯Y , α, ϵ′, δ′, δ)
1:Setnmax←max i{|Xi−Yi|}.
2:Setˆnmax←nmax+Lap(8/ϵ)
3:SetZ←P
i(Xi−Yi)2−Xi−Yi=P
i|Xi−Yi|2−(Xi+Yi).
4:Setnz←q
6N
mlog (800 N) +16 ln(1 /δ′)
ϵ′ + 1
5:SetˆZ←Z+Lap(16 log 4/3(2/δ)nz/ϵ′)
6:if(ˆnmax<q
6N
mlog (800 N) +8 ln(1 /δ′)
ϵ′ andˆZ <1
2α2N2)then return NULL
7:else return ALT
We thus focus for now on Part II of Algorithm 1. In Part II our output is the (2j0)-long tuple
⟨ˆnj
max, Zj⟩j0−1
j=0which we may return from the j0invocations of TestCloseness . Thus, we denote
ϵ′=ϵ
8√
j0log(2/δ),δ′=δ
2j0and note that each invocation of TestCloseness is with these parameters.
Throughout our analysis of Part II we assume that the Poisson random variables are known to us, but
leave the Bernoulli and the Laplace random variables unknown.
Each invocation of TestCloseness releases two statistics: an approximation of nmaxand approxi-
mation of Z. The next two claims bounds their sensitivity (w.h.p.).
Claim 5. The Global Sensitivity of nmaxis at most 4.
Lemma 6. There exists an event Ewhere Pr[¬E]<3δ/2, and under Eit holds that the sensitivity of
Zat any iteration jis at most 8 log4/3(2/δ)q
6N
mlog (800) +16 ln(1 /δ′)
ϵ′ + 1
.
Proof. Denote E0as the event that there exists an invocation of TestCloseness withmballs and
nbins in which nmax≥q
6m
nlog (800 m) +16 ln(1 /δ′)
ϵ′ , yetˆnmax<q
6m
nlog (800 m) +8 ln(1 /δ′)
ϵ′.
It follows that there must exists an invocation of TestCloseness in which the noise added to nmax
(drawn from Lap(8/ϵ′)) must be smaller than −8 ln(1 /δ′)
ϵ′. This holds w.p. < δ′/2, and from the
Union Bound it follows that Pr[E0] =j0·δ′
2=δ
2.
We now turn to the Bernoulli random variables. Fix SandS′to be two neighboring inputs, and again,
we assume that the changed point appears in place (1)inSand in place (s)inS′. (Otherwise, our
analysis is only simpler.) It follows that the changed point falls in Π0
1inSand in Π0
minS′.
We now specify the coupling of the Bernoulli random variables between SandS′, which is the
following. We draw B1,B′
1.B2,B′
2and so on, u.a.r. and independently, and apply Bi-s to the
invocation on SandB′
i-s to the invocation on S′,until the first occurrence of Bi= 0, B′
i= 1from
which we give Bi+1,Bi+2and so on to both invocation. This is depicted in Figure 1.
Denote Ejas the event that the first occurrence of Bi= 0, B′
i= 1 is at the j-th draw. Clearly
Pr[Ej] = ( 3/4)j−1·1/4. Thus, from the disjointness of events, it follows that PrhSlog4/3(2/δ)
j=1 Eji
=
1
4Plog4/3(2/δ)
j=1 (3/4)j−1=1
41−(3/4)log4/3(2/δ)
1−3/4= 1−δ/2. Symmetrically, we apply the same coupling
Figure 1: Two neighboring inputs that differ on one datapoint, appearing first in Sand last in S′.
In this example, the index defining the first bin, π1, is such that for Swe go B1= 0and for S′we
haveB′
1= 0; but the index defining the 2nd bin, π2it does hold that B2= 0, B′
2= 1so the indices
starting from bin 2 onwards align.
6to align the last bins, those affected by x(s), in the last position. We use the coupling that provides
Bm, B′
m, Bm−1, B′
m1, Bm−2, B′
m−2and so on until the first occurrence of (1,0)then it switches to
the same variable Bj. Denoting E′
jas the event that the first occurrence of Bi= 1, B′
i= 0is at the
m−j-th draw, we have that PrhSlog4/3(2/δ)
j=1 E′
ji
= 1−δ/2.
We thus denote E=E0∪log4/3(2/δ)S
j=1(Ej∪ E′
j)and have that Pr[¬E] = 3δ/2. Note, under Eit follows
that at most 2 log4/3(2/δ)bins have a change in their |Xi−Yi|-value, which – due to Claim 5 – is at
most 4. (Observe that in the case where SandS′are such that there are fewer than 2 log4/3(2/δ)
bins between the locations of the changed point then this statement holds w.p. 1.)
It follows that under Ewe have that the value of Zis affected by at most 2 log4/3(2/δ)bins,
where for each bin |Xi−Yi|changes by at most 4. It follows that Zcan change by at most
2 log4/3(2/δ)·4·(nmax+ 1)≤8 log4/3(2/δ)q
6N
mlog (800 N) +16 ln(1 /δ′)
ϵ′ + 1
.
Completing the Proof of Theorem 3 is simple, and is deferred to Appendix A.
3.2 Utility Proof
In this subsection, our goal is to proof the following theorems.
Theorem 7. W.p.≥2/3, Algorithm 1 returns NULL whenP=Q.
Theorem 8. W.p.≥2/3, Algorithm 1 returns ALT when∥P − Q∥ Ak≥α.
The proof of Theorem 7 is fairly simple, but the proof of Theorem 8 requires some preliminaries.
In fact, in order to argue the correctness of theorems we need to argue that both Parts I & II of the
algorithm are correct w.p. ≥5/6. Proving that Part I is correct is very straight-forward due to claims
from [ 16]. And so we focus on the correctness of Part II. Its correctness requires that we first assert a
few rudimentary propositions and claims.
3.2.1 Rudimentary Claims
Similarly to the analysis in [ 16] our goal is to compare the two continuous distributions to their
resp. discretizations that were forms under the various ¯Πj. However, we do not have the luxury
of resampling the points from PandQ, and so our argument diverges from theirs as we fix one
particular j∗and then argue from first principles that under large Ak-difference the specific partition
¯Πj∗cause us to reject. (Arguing that when P=Qwe return nullis also fairly straight-forward.) Our
intermediate goal is to argue that the points from P(theX-s) and the points from Q(theY-s) are
distributed like independent Poisson random variables. Thus, for the remainder of the discussion we
fix some particular iteration jand examine solely it, without considering the previous iterations.
Due to space constraint, we defer nearly all the claims in this section to Appendix B, but they all lead
to towards the following main lemma.
Lemma 9. Fix index j. For each index idenote by Iias the interval (x(πj
i), x(πj
i+1))which is the
interval defining bin iin the partition ¯Πj. Then the estimator Zcomputed by TestCloseness satisfies
thatE[Z] =Pmj
i=1(pi−qi)2where
pi=NP(Ii)
4m(P(Ii) +Q(Ii)), q i=NQ(Ii)
4m(P(Ii) +Q(Ii)).
and that Var [Z] =Pmj
i=14(pi+qi)(pi−qi)2+ 2(pi+qi)2.
In Lemma 9 we established the expectation and variance of our estimator using the notation piwhich
in turn is defined asNP(I)
4m(P(I)+Q(I))(andqisimilarly). The following claim is going to assist us in
bounding the denominators of piandqi.
Claim 10. IfN > 3000mlog(2m), then with a probability of 1−1
m, any Ithat forms a bin Π0
iin
Π0, has that1
1.01m≤P(I)+Q(I)
2≤1.01
m.
7Unfortunately, due to space constraints, we defer proof of Claim 10 as well as the entire proof of
Theorem 7 to Appendix B.
3.2.2 Proof of Theorem 8
We now turn our attention to proving Theorem 8, however, we need one more technical lemma, whose
proof – like all proofs in this section – is deferred to Appendix B. Then we can proof Theorem 8.
Lemma 11. Fixp∈(0,1). Suppose there exists nnon-negative random variables X1, X2, . . . , X n,
such that for each iit holds that for some fixed number aiwe have Pr[Xi≥ai]≥p. Then, given a
constant c >0there exists another constant C >0such that with a probability at least Cit holds
thatPn
i=1Xi≥cPn
i=1ai.
Proof of Theorem 8. In the alternative case, where ∥P − Q∥ Ak≥α, we know that there exists
kintervals I∈ I such thatP
I∈I|P(I)− Q(I)| ≥α. We denote for each interval γ(I) =
|P(I)− Q(I)|. For the sake of analyze we use two different kinds of interval, small and large.
Definition 12. Interval I∈ Iis called small if there exists a subinterval J⊆Isuch that P(J) +
Q(J)<1/mand|P(J)− Q(J)| ≥γ(I)/10, and large otherwise.
Note that α≤ ∥P − Q∥ Ak=P
I∈I|P(I)− Q(I)|=P
I∈I, Ismallγ(I) +P
I∈I, Ilargeγ(I), so
half of the discrepancy comes from either small or large intervals. We consider first the case where
half of the discrepancy comes from small intervals. In this case, Lemma 9 in [ 14] states that the
expectation of statistic Zin Line 5 of Part I is bounded by E[Z]≥cN3α3
k2for some constant c >0.
Just like we did in the soundness case, Proposition 21 gives that the variance Var[Z]≤9m. Therefore
for some large constant cdkn, setting m= 100 cdkn
k4
5
α6
5+k2
3
αϵ1
3
then by Chebyshev’s inequality
we get that with probability5
6it holds that ˜Z≥E[Z]−10√m− |X| ≥m3α3
2k2(with Xbeing the
random noise sampled in Line 6 of Part I of the algorithm).
Now consider the case whereP
I∈I, Iis large|P(I)−Q(I)| ≥α
2. We prove that Part II of Algorithm 1
must return ALT on at least one invocation of TestCloseness . In order to do this, we first need to
show that the discretization of the domain with msamples preserves most of the Ak-distance. Take
any interval I∈ Ithat gives the Akdistance, and denote I= [a, b]as its boundaries.
Now, consider the total probability mass between aandπi, then the first datapoint selected for the
partition that is greater than a. Since the total number of points is taken from a Poisson distribution,
it is known that the mass from one point to the next has Exponential distribution Exp(N)[28]; and
so the total mass from atoπis distributed like the sum of Exponential random variables, namely, a
Gamma-distribution with mean ≤N
m·1
N=1
m, and variance ≤N
m·(1
N)2=1
mN<1
3000m2log(m). It
follows that w.p. ≤0.01we have that1
2(P[a, πi]+Q[a, πi])>1.01
m. A similar analysis shows that for
πj, the last datapoint selected for the partition before b, we also have P([πj, b]) +Q([πj, b])<2.02
m.1
Note that a large interval must have a total probability mass P(I) +Q(I)≥10
mand so w.p. ≥0.98
we have formed the subinterval I′= [πi, πj]⊂I. Moreover, by the subinterval property of
large intervals, we have that because P([a, πi]) +Q([a, πi]) +P([πj, b]) +Q([πj, b])<4.04
mthen
γ(I′)≥γ(I)−5γ(I)
10=γ(I)/2.
Therefore, denote NIas the indicator of the event that |P(I′)−Q(I′)| ≥γ(I)
2, and we denote PΠ0and
QΠ0as the reduced discretized distribution formed by the partition point. We show it preserves most of
the total variation ∥PΠ0− QΠ0∥1=P
I′|P(I′)− Q(I′)| ≥P
I∈INIγ(I)
2≥P
I∈I, IlargeNIγ(I)
2.
We have established that if Iis large interval then Pr[NI]≥0.98, and so, for the random variables,
NIγ(I)
2
-s (one for each large interval) where for each variable with probability 0.98it holds
thatNIγ(I)
2≥γ(I)
2, we apply Lemma 11 with c= 0.36and have that there for the constant
1ifa=−∞ orb=∞our analysis is even simpler, as we take π0andπm+1as the respective partition
point.
8C= 1−0.98(1−0.98)
0.982(1−0.36)2>0.95such that with probability C >0.95
X
I∈I, IlargeNIγ(I)
2≥0.36X
I∈I, Ilargeγ(I)
2>α
12
Therefore, with probability >0.95,∥PΠ0− QΠ0∥1≥α
12.
Observe that the TV-distance follows from finding suitable subintervals I′inside large intervals with
large discrepancy. Thus, each of the ≤klarge intervals now gives (at most) 2points that form I′, so
these≤2kpoints partition the real line to ≤2k+ 1intervals where the various I′-s are part of this
partition. It follows that ∥PΠ0− QΠ0∥A2k+1≥α
12.
Now, to complete the proof, we need to show that Part I of the algorithm detects the discrepancy. We
deploy the following lemma from [ 16] where for a vector vthe notation ∥v∥1,krefers to the sum of
the largest kbins/coordinates of v.
Lemma 13. For any two distributions PandQon[m]such that ∥P − Q∥ Ak> α, there iteration
j∈[log( m/k)]such that ∥PΠj− QΠj∥1,k> α/ log(m/k).
So since ∥PΠ0− QΠ0∥A2k+1≥α
12, we know that by Lemma 13, there exists some j∗∈[log(m/k)]
such thatPΠj∗
− QΠj∗
1,2k+1≥α
12 log( m/k)and therefore by Cauchy–Schwarz inequality
PΠj∗
− QΠj∗
2≥α
12√2k+1 log( m/k). We know thatPΠj∗
− QΠj∗
1,2k+1≥α
12 log( m/k). De-
note the set of indices of these 2k+ 1intervals as S, we get from Lemma 9 that
E[Z] =mjP
i=1(pi−qi)2≥P
i′∈S(pi′−qi′)2Cau.Sch.
≥(P
i′∈S|pi′−qi′|)2
2k+1≥1
2k+1P
i′∈SN|P(I)−Q(I)|
4mj|P(I)+Q(I)|2
.
Also from Lemma 9 we can infer that
Pr
|Z−E[Z]| ≤1
4E[Z]
≤16Var[Z]
E[Z]2≤Pmj
i=164(pi+qi)(pi−qi)2+ 32( pi+qi)2
(Pmj
i=1(pi−qi)2)2
=Pmj
i=164(N(P(I)+Q(I)
4mj(P(I)+Q(I))(pi−qi)2+ 32(N(P(I)+Q(I)
4mj(P(I)+Q(I))2
(Pmj
i=1(pi−qi)2)2=2N2/mj
(Pmj
i=1(pi−qi)2)2+16N/mj
Pmj
i=1(pi−qi)2
mj≥k
≤6·5882(2k+ 1) log2(m/k)
N2α4+28224 log( m/k)
Nα2<0.01
when N≥106√
klog2(m/k)
α2 . Now we assert that the noise we added is also proportional to at most
1
4E[Z], and indeed if we draw a random variable R∼Lap
∆(Z)
ϵ′
with∆(Z)as defined in Line
5 ofTestCloseness , then we get Prh
R≥N2α2
2500(2 k+1) log( m/k)i
≤exp
N2α2ϵ′
2500(2 k+1)∆( Z)
≤0.01.
which holds if
N > 106p
k∆(Z)
α√
ϵ′≥106
α√
ϵ′vuutk·16 log 4/3(2/δ) max(r
N
2klog (10 N),16 ln(1 /δ′)
ϵ′)
so we get N=˜Ω
k1/3
α4/3ϵ2/3+√
k
αϵ
. Combining it all together, we have that w.p. ≥5/6Part II of
the algorithm also returns ALT.
4 Applications
Our algorithm is designed for continuous distributions, but it can also be used for discrete distributions.
The process is simple: for a given discrete distribution P, for each example xj∼ P we draw a
number ij∈R[0,1], and then sort all the examples ⟨(xj, ij)⟩m
j=1using lexicographic order. This
process gives a simple privatization of the "Flattening method" proposed by Diakonikolas et al [ 14],
9as it does so without looking at the example drawn. Our algorithm method is quite simple: draw m
samples from1
2(P+Q), then calculate the autocorrelation to identify discrepancies within small
intervals. Next, use 2 min( m, n)for flatting to test for total variation distance. It’s important to
remember that if n=k, where nis the size of the domain, then Akdistance is equal to L1. However,
the sample complexity of the first part, which involves finding discrepancies in small intervals using
the analysis of [ 16], requiresn4/5
α6/5. Fact 1 indicates however that the size of the domain is not always
the most suitable parameter for distribution testing. Having knowledge about the structure of the
distribution enables more efficient testing, as we illustrate below. In Table 1, we give a brief summary
of the various statistical inference tasks that can be conducted using our algorithm.
Table 1: Private equivalence testers derived from our algorithm for continuous distributions
Distrib.
FamilyNum of
IntervalsPrivate upper bound
t-
piecewise
constantt ˜O
maxn
t4/5
α6/5,t2/3
αϵ1/3,t1/2
α2,t1/3
α4/3ϵ2/3,√
t
αϵo
t-
piecewise
degree- dt(d+ 1) ˜O
max
(t(d+1))4/5
α6/5,(t(d+1))2/3
αϵ1/3,(t(d+1))1/2
α2,(t(d+1))1/3
α4/3ϵ2/3,√
(t(d+1))
αϵ
log-
concave1√α˜O
maxn
1
α9/5,1
α4/3ϵ1/3,1
α3ϵ2/3,1
α5/4ϵo
k-
mixture
of log-
concavek√α˜O
maxn
t4/5
α8/5,k2/3
α4/3ϵ1/3,k1/2
α9/5,k1/3
α3ϵ2/3,√
k
α5/4ϵo
t-model
over[n]tlog(n)
α˜O
max
(tlog(n))4/5
α2 ,(tlog(n))2/3
α5/2ϵ1/3,(tlog(n))1/2
α5/2,(tlog(n))1/3
α5/3ϵ2/3,√
tlog(n)
α3/2ϵ
MHR
over[n]log(n/α)
α˜O
max
(log(n/α))4/5
α2 ,log(n/α)2/3
α5/3ϵ1/3,(log(n/α))1/2
α5/2,(log(n/α))1/3
α5/3ϵ2/3,√
log(n/α)
α3/2ϵ
Acknowledgments and Disclosure of Funding
O.S. is supported by the BIU Center for Research in Applied Cryptography and Cyber Security in
conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office, and by ISF grant no.
2559/20. Both authors thank the anonymous reviewers for their suggestions and advice on improving
this paper.
10References
[1]Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for properties
of distributions. Advances in Neural Information Processing Systems , 28, 2015.
[2]Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private testing of identity and
closeness of discrete distributions. Advances in Neural Information Processing Systems , 31,
2018.
[3]Maryam Aliakbarpour, Ilias Diakonikolas, Daniel Kane, and Ronitt Rubinfeld. Private testing
of distributions via sample permutations. Advances in Neural Information Processing Systems ,
32, 2019.
[4]Maryam Aliakbarpour, Ilias Diakonikolas, and Ronitt Rubinfeld. Differentially private identity
and closeness testing of discrete distributions. CoRR , abs/1707.05497, 2017.
[5]Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White. Testing
that distributions are close. In Proceedings 41st Annual Symposium on Foundations of Computer
Science , pages 259–269. IEEE, 2000.
[6]Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of
private learners. In ITCS , pages 97–110. ACM, 2013.
[7]Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and
learning of threshold functions. In FOCS , pages 634–649. IEEE Computer Society, 2015.
[8]Bryan Cai, Constantinos Daskalakis, and Gautam Kamath. Priv’it: Private and sample efficient
identity testing. In International Conference on Machine Learning , pages 635–644. PMLR,
2017.
[9]Clément L Canonne et al. Topics and techniques in distribution testing: A biased but rep-
resentative sample. Foundations and Trends ®in Communications and Information Theory ,
19(6):1032–1198, 2022.
[10] Siu-On Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for
testing closeness of discrete distributions. In Proceedings of the twenty-fifth annual ACM-SIAM
symposium on Discrete algorithms , pages 1193–1203. SIAM, 2014.
[11] Constantinos Daskalakis, Ilias Diakonikolas, Rocco A Servedio, Gregory Valiant, and Paul
Valiant. Testing k-modal distributions: Optimal algorithms via reductions. In Proceedings of
the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms , pages 1833–1852.
SIAM, 2013.
[12] Ilias Diakonikolas, Themis Gouleakis, Daniel M Kane, John Peebles, and Eric Price. Optimal
testing of discrete distributions with high probability. In Proceedings of the 53rd Annual ACM
SIGACT Symposium on Theory of Computing , pages 542–555, 2021.
[13] Ilias Diakonikolas, Themis Gouleakis, John Peebles, and Eric Price. Sample-optimal identity
testing with high probability. In Ioannis Chatzigiannakis, Christos Kaklamanis, Dániel Marx,
and Donald Sannella, editors, 45th International Colloquium on Automata, Languages, and
Programming, ICALP 2018, July 9-13, 2018, Prague, Czech Republic , volume 107 of LIPIcs ,
pages 41:1–41:14. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2018.
[14] Ilias Diakonikolas and Daniel M Kane. A new approach for testing properties of discrete
distributions. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science
(FOCS) , pages 685–694. IEEE, 2016.
[15] Ilias Diakonikolas, Daniel M Kane, and Vladimir Nikishkin. Testing identity of structured
distributions. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete
algorithms , pages 1841–1854. SIAM, 2014.
11[16] Ilias Diakonikolas, Daniel M Kane, and Vladimir Nikishkin. Optimal algorithms and lower
bounds for testing closeness of structured distributions. In 2015 IEEE 56th Annual Symposium
on Foundations of Computer Science , pages 1183–1202. IEEE, 2015.
[17] Ilias Diakonikolas, Daniel M. Kane, and Vladimir Nikishkin. Near-optimal closeness testing of
discrete histogram distributions. In ICALP , volume 80 of LIPIcs , pages 8:1–8:15, 2017.
[18] C. Dwork, G. Rothblum, and S. Vadhan. Boosting and differential privacy. In FOCS , 2010.
[19] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our
data, ourselves: Privacy via distributed noise generation. In EUROCRYPT , 2006.
[20] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In STOC , pages 371–380.
ACM, 2009.
[21] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography
Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3 , pages 265–284.
Springer, 2006.
[22] Oded Goldreich. The uniform distribution is complete with respect to testing identity to a fixed
distribution. In Electron. Colloquium Comput. Complex. , volume 23, page 15, 2016.
[23] Oded Goldreich and Dana Ron. On testing expansion in bounded-degree graphs. Studies
in Complexity and Cryptography. Miscellanea on the Interplay between Randomness and
Computation: In Collaboration with Lidor Avigad, Mihir Bellare, Zvika Brakerski, Shafi
Goldwasser, Shai Halevi, Tali Kaufman, Leonid Levin, Noam Nisan, Dana Ron, Madhu Sudan,
Luca Trevisan, Salil Vadhan, Avi Wigderson, David Zuckerman , pages 68–75, 2011.
[24] Svante Janson. Tail bounds for sums of geometric and exponential variables. Statistics &
Probability Letters , 135:1–6, 2018.
[25] S. R. Mane. Moments of the poisson distribution of order k, 2023.
[26] Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Smooth sensitivity and sampling in
private data analysis. In STOC , pages 75–84. ACM, 2007.
[27] Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete
data. IEEE Transactions on Information Theory , 54(10):4750–4755, 2008.
[28] H. Pishro-Nik. Introduction to Probability, Statistics, and Random Processes . Kappa Research,
LLC, 2014.
[29] Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity
testing. SIAM Journal on Computing , 46(1):429–455, 2017.
[30] Huanyu Zhang. Statistical inference in the differential privacy model. CoRR , abs/2108.05000,
2021.
12A Missing Proofs — Privacy
Claim 14 (Claim 4 restated) .The estimator Z=Ps
i=1ℓ(x(i))ℓ(x(i+1))in Part I of Algorithm 1
(Line 5) has global senstivity of 6.
Proof. Consider omitting a single datapoint, x(i). It is easy to check cases and see that when
ℓ(x(i−1)) =ℓ(x(i+1)) =−ℓ(x(i))then before omitting x(i)the three entries contributed −2toZ
andafter the omittance they contribute 1toZ. Thus the change is 3. Considering SandS′that are
different on a single entry, we have that this change to a triplet of consecutive datapoints occurs twice,
hence the global senstivity of Zis2·3 = 6 .
Claim 15 (Claim 5 restated) .The Global Sensitivity of nmaxis at most 4.
Proof. FixSandS′to be two neighboring inputs that differ on a single point, and assume that the
changed point appears in first place in Sand the last ( s-th) place in S′. Now, it is simple to see that
each bin changes by at most two points (its first and last), as shown in Figure 1. However, since we
may not know the beginning-points and end-points of each bin due to the Bernoulli random noise
added to each, then in the extreme case the bin from Shas addition more point and from S′missed
a point. This sums to 4points that may belong to one bin and not the other, shifting the value of
Xi−Yiby at most 4.
Proof of Theorem 3. First, we discuss Part I. In this part we only release the noisy estimator  Ps
i=1ℓ(x(i))ℓ(x(i+1))
+X. Due to Claim 4 we know that adding Lap(6/ϵ)to this estimator
isϵ-DP. We now move to Part II of the algorithm.
In this part we release j0-times a pair of outputs: (i) a noisy estimator of nmax and (ii)
a noisy estimator of Z. Assume the event from Lemma 6 holds. Adding Lap(8/ϵ′)-
noise to nmax which has global sensitivity of 4(Claim 5) is ϵ′/2-DP. Adding noise of
Lap
16 log4/3(2/δ)q
6m
nlog (800 m) +16 ln(1 /δ′)
ϵ′ + 1
/ϵ′
isϵ′/2-DP. It follows that under E,
each pair of outputs preserves ϵ′-DP. Note that we set ϵ′=ϵ
8√
j0log(2/δ), so it follows from Advanced
Composition, that overall we preserve (ϵ, δ/2)-DP under E.
Summing both Parts together, Algorithm 1 preserves (2ϵ,δ/2)-DP under an event that holds w.p.
1−3δ/2; so overall Algorithm 1 is (2ϵ,2δ)-DP.
B Missing Proofs — Utility
B.1 Proof of Utility - Rudimentary Claims
Recall that in Part II we partition the domain into bins by using a list of sorted samples, and see if the
examples in those bins pass our L2test – TestCloseness . Moreover should TestCloseness return
NULL we unify bins, using the same sets of points. Throughout the analysis we assume that we know
the endpoint of each bin, namely that the result of the Bernoulli r.v.s have been disclosed. Now that
each bin Πj
ihastipoints we argue that the distribution of the number of points following a binomial
distribution from P(resp.Q).
Claim 16. Fix index j. Fix index i. Denote Ias the interval (x(πj
i), x(πj
i+1))which is some interval
defining a bin in the partition Πj
i, which contains tipoints overall. Then the number of points in an
interval Πj
icoming from Pfollow Bin
ti,P(I)
P(I)+Q(I)
. Moreover, for any two disjoint bins Πj
iand
Πj
i′it holds that the number of points from P(resp.Q) in each bin is independent of the other.
Proof. We draw sfrom a Poisson distribution with parameter N, representing the number of samples
we take from1
2(P+Q). As a result, we can determine that within any fixed interval, including
the interval I, the number of points we denote as ˜XIfollows a Poisson distribution with parameter
1
2(P(I)). However, we know that the number of points within interval Iis exactly ti.
13So now, conduct a thought experiment, where we re-sample Suntil precisely tipoints from Sfall in I.
Replacing our original tpoints in Π0
iwith the new points is indistinguishable as they are distributed
precisely the same. Denote ˜Xi(resp. ˜Yi) as the number of points from P(resp.Q) in our repeated
thought experiment that fall in I, and define XI(resp. YI) as the number of points from P(resp.
Q) that fall within interval Iin the draw where precisely tipoints fall in Ifrom both distributions
together. Therefore
XI∼˜XI|
˜XI+˜YI=ti
We can recall that by the definition of our sampling from Poisson distribution, we get that ˜XI∼
Poi N
2P(I)
,˜YI∼Poi N
2Q(I)
, hence by Proposition 2 it follows that XI∼Bin(ti,P(I)
P(I)+Q(I)).
Note also that XIandXI′are independent for every IandI′. This is due to the fact that the number
of points in any given interval, taken from the set P, follows a Poisson distribution. As a result, the
number of points in any two disjoint intervals IandI′are independent.
We now prove that our new selection of points in each bin follows a Poisson distribution.
Claim 17. For each interval Idefining a bin Πj
i, as in Claim 16, it holds that Xj
i∼
Poi
NP(I)
4m(P(I)+Q(I))
andYj
i∼Poi
NQ(I)
4m(P(I)+Q(I))
Proof. Using the same notation as in Claim 16 — we know that for each interval Idefining a bin Πj
i,
the following holds: XIis distributed according to a binomial distribution with parameters tiand
P(I)
P(I)+Q(I). Now, let us take a subset of points Cj
iof size Nj
ifromI, where Nj
iis a Poisson random
variable with parameter λ=N
4mj, and denote Xj
ias the sum of Bernoulli i.i.d. random variables br,
such that each bris distributed according to a Bernoulli distribution Ber(P(I)
P(I)+Q(I)). In other words,
Xj
i=PNi
r=1br. Based on Proposition 2 in the preliminaries, it holds that Xj
iis distributed like a
Poisson r.v. with parameter λ·P(I)
P(I)+Q(I). The proof for Yj
iis symmetrical. To prove independence
ofXj
i=PNj
i
i=1Ber(p)andYj
i=PNj
i
i=1Ber(1−p), and we have
Pr[Xi=a , Y i=b] = Pr[ Xi=a , X i+Yi=a+b]
= Pr[ Xi=a|Xi+Yi=a+b]·Pr[Xi+Yi=a+b]
=a+b
a
pa(1−p)b·e−(N
4m) N
4ma+b
(a+b)!
=1
a!b!pa(1−p)b·e−(N
4m)p+(N
4m)(1−p) N
4ma+b
(a+b)!
= N
4mpae−(N
4m)p
a!· N
4m(1−p)be−(N
4m)(1−p)
b!= Pr[ Xi=a]·Pr[Yi=b]
Moreover, we need to ensure that each selection of a subset of points does not require more points
than we already have, which is no more thanN
mfor each bin, meaning no bin holds any ⊥points.
Claim 18. IfN > 32/3·mln(2m), then with probability 1−1
2mit holds that for each bin i, the size
of the chosen subset Ciis no greater thanN
m−1.
Proof. The proof follows from standard tail bounds of the Poisson distribution. Given a random
variable X∼Poi(λ), we have
Pr[X > λ + 3λ]≤exp
−(3λ)2
3·(3 + 1) λ
≤exp
−3λ
4
14Therefore, if we want each of the m0+m1+m2+...+mj≤2mbins to have a subset size that
doesn’t exceedN
mj, we get
Pr
∀i,∀j,|Cj
i|<N
mj
≥1−X
i,jPr[Nj
i<4N
4mj]≥1−X
jmjexp
−3N
16mj
≥1−2mexp
−3N
16m0
= 1−2me−2 ln(2 m)= 1−1
2m
Lemma 19 (Lemma 9 restated) .Fix index j. For each index idenote by Iias the interval
(x(πj
i), x(πj
i+1))which is the interval defining bin iin the partition ¯Πj. Then the estimator Z
computed by TestCloseness satisfies that E[Z] =Pmj
i=1(pi−qi)2where
pi=NP(Ii)
4m(P(Ii) +Q(Ii)), q i=NQ(Ii)
4m(P(Ii) +Q(Ii)).
and that Var [Z] =Pmj
i=14(pi+qi)(pi−qi)2+ 2(pi+qi)2.
Proof. For brevity, we denote Xj
iasXiand the same for Yi. We want to calculate the expectation of
the statistic Z=P
i(Xi−Yi)2−Xi−Yi. Therefore
E[Z] =E
mjX
i=1(Xi−Yi)2−Xi−Yi
=mjX
i=1E[X2
i−2XiYi+Y2
i−Xi−Yi]
=mjX
i=1pi+p2
i−2piqi+qi+q2
i−pi−qi=mjX
i=1(pi−qi)2
We now want to calculate the variance of Z=Pnj
i=1(Xi−Yi)2−Xi−Yi. Therefore
Var[Z] =Var
njX
i=1(Xi−Yi)2−Xi−Yi
=njX
i=1Var
(Xi−Yi)2−Xi−Yi
Let’s denote Zi= (Xi−Yi)2−Xi−Yiand calculate E
Z2
i
:
E
Z2
i
=Eh 
(Xi−Yi)2−Xi−Yi2i
=E
(Xi−Yi)4−2(Xi+Yi)(Xi−Yi)2+ (Xi+Yi)2
We analyze the expectations of each term separately, based on known first to fourth moments of the
Poisson distribution [25] and the independence of XiandYi, we know that
E
(Xi−Yi)4
=E[X4
i−4X3
iYi+ 6X2
iY2
i−4XiY3+Y4
i]
=p4
i+ 6p3
i+ 7p2
i+pi−4pi(q3
i+ 3q2
i+qi) + 6( pi+p2
i)(qi+q2
i)
−4qi(p3
i+ 3p2
i+pi) +q4
i+ 6q3
i+ 7q2
i+qi
= (pi−qi)4+ 6p3
i+ 7p2
i+pi−4pi(3q2
i+qi) + 6piqi+ 6piq2
i+ 6p2
iqi
−4qi(3p2
i+pi) + 6q3
i+ 7q2
i+qi
= (pi−qi)4+ 7(pi−qi)2+ 6(p3
i−p2
iqi−piq2
i+q3
i) +pi+qi+ 12piqi
It is to see that E[(Xi+Yi)2] =pi+p2
i+qi+q2
i+ 2piqi= (pi+qi)2+pi+qi, so now
E[(Xi+Yi)(Xi−Yi)2] =E[X3
i−2X2
iYi+XiY2
i+YiX2
i−2XiY2
i+Y3
i]
=p3
i+ 3p2
i+pi−qi(pi+p2
i)−pi(qi+q2
i) +q3
i+ 3q2
i+qi
=p3
i−qip2
i−piq2
i+q3
i+ 3(pi−qi)2+pi+qi+ 4piqi
15Combining all three terms, we get
E
Z2
i
= (pi−qi)4+ 7(pi−qi)2+ 6(p3
i−p2
iqi−piq2
i+q3
i) +pi+qi+ 12piqi
−2(p3
i−qip2
i−piq2
i+q3
i+ 3(pi−qi)2+pi−qi+ 4piqi) + (pi+qi)2+pi+qi
= (pi−qi)4+ 4(p3
i−p2
iqi−piq2
i+q3
i) + (pi−qi)2+ (pi+qi)2+ 4piqi
= (pi−qi)4+ 4(pi+qi)(pi−qi)2+ 2(pi+qi)2
And so we get that
Var[Zi] =E[Z2
i]−E[Zi]2= (pi−qi)4+ 4(pi+qi)(pi−qi)2+ 2(pi+qi)2−(pi+qi)4
= 4(pi+qi)(pi−qi)2+ 2(pi+qi)2
So overall by independence we get
Var[Z] =mjX
i=1Var[Zi] =mjX
i=14(pi+qi)(pi−qi)2+ 2(pi+qi)2
Claim 20 (Claim 10 restated) .IfN > 3000mlog(2m), then with a probability of 1−1
m, anyIthat
forms a bin Π0
iinΠ0, has that1
1.01m≤P(I)+Q(I)
2≤1.01
m.
Proof. LetIbe an interval determining a bin Π0
i, and denote SI=1
2(P(I) +Q(I)). From our
process of partitioning the domain and creating the intervals, we know that SI∼PN/m
i=1Exp(N).
Therefore, as the sum of independent Exponential r.v.s we can conclude that SI∼Gamma 
N/m,1
N
.
Using the tail bound of the sum of the exponential distribution [ 24], we get the following inequality:
∀t >0,Pr[SI> tE[SI]]≤min1
t,1
exp(−α(t−1−log(t)))
where α=E[SI]
1/N. In our case, we have E[SI] =N/m
N, soα=N
m. By setting t= 1.01, we arrive at
the following result:
Pr
SI>1.01
m
= Pr
SI>1.01N
mα·1
m
≤1
1.01e−N
m(1.01−1−log(1.01))< e−0.00049 N
m≤1
2m2
And now if t=1
1.01≤1, then we get
Pr
SI≤1
1.01m
≤e−N
m(1/1.01−1−log(1/1.01))≤e−0.00049 N
m≤1
2m2
So by the Union-Bound, we get that Pr[∃I, S I>1.01/morSI<1/1.01m]≤m(1
2m2+1
2m2) =1
m.
B.1.1 Proof of Theorem 7
Having acquired the rudimentary tools, we can now prove Theorem 7.
Proof of Theorem 7. We can see that in the Part I of the algorithm, we have s∼Poi(m)samples
such that for all 1≤i≤s ℓ(xi)∈ {1,−1}with equal probability and independent from each
other, therefore, by linearity of expectation we get that for the estimator Zfrom Line 5 of Part I of
Algorithm 1 we have E[Z] =P
iE[ℓ(xi)ℓ(xi+1)] =P
iE[ℓ(xi)]E[ℓ(xi+1)] = 0 . We can bound the
variance of Zusing O(m)by the Effron Stein inequality / Jackknife principle.
Proposition 21. Var[Z]≤9m.
Proof of Proposition 21. Denote our points X= (x1, x2, . . . , x i, . . . , x s)∈ {1,−1}s, and it is clear
to see that if we change one of the points independently X(i)= (x1, x2, . . . , x′
i, . . . , x s)then
Var[Z|s]≤1
2sX
i=1E
Z(X)−Z
X(i)2
≤1
2sX
i=1Eh
(ℓ(xi−1)ℓ(xi) +ℓ(xi)ℓ(xi+1)−ℓ(xi−1)ℓ(x′
i)−ℓ(x′
i)ℓ(xi+1))2i
≤1
2·16s= 8s
16And we know that Var [E[Z|s]]≤Var[s] =m. Therefore we get
Var[Z] =E[Var[Z|s]] +Var[E[Z|s]]≤E[8s] +m= 9m
Therefore by Chebyshev’s inequality, it follows that
Pr[Z≤10√m]≥1−Pr[|Z−E[Z]|>10√m]≥1−9m
100m≥0.91.
Now we prove that our Laplace noise does not exceedm3α3
8k2, provided m≥100k2
3/αϵ1
3.
Pr[|˜Z−Z| ≥1
4m3α3
2k2] = Pr
Lap6
ϵ
≥1
4m3α3
2k2
≤exp
−m3α3ϵ
48k2
≤0.01 (1)
And so , with probability ≥0.9it holds that ˜Z≤10√m+1
4m3α3
2k2<m3α3
2k2, provided m≥
40k4
5
α6
5+ 100k2
3
αϵ1
3. It follows that Part I of Algorithm 1 returns NULL .
In Part II, we also need to argue that all invocations of TestCloseness return NULL . Fix an iteration
jand observe that for any iit holds that Xj
iandYj
iare distributed like Poi(N
8mj)as the labeling
of points as coming from PorQis completely random Ber(1/2). We next bound |Xj
i−Yj
i|andP
i(Xj
i−Yj
i))2using known tail bounds on Poisson random variables (Proposition2).
Proposition 22. Assume that for all jwe have N/8mj>4 log(800 m). Then with probability >0.99
it holds that for any jand any iwe have
|Xj
i−Yj
i| ≤r
6N
mjlog (800 m)
Proof. We have that Nj
i∼Poi(N
4mj),Poi(N
8mj)andYi=Nj
i−Xj
i. Therefore, |Xj
i−Yj
i|=
|2Xj
i−Nj
I| ≤2|Xj
i−E[Xj
i]|+|Nj
i−E[Nj
i]|+|2E[Xj
i]−E[Nj
i]|. Seeing as 2E[Xj
i]−E[Nj
i] = 0
we bound the other two terms using known tail bounds on the Poisson distribution from Proposition 2.
Pr[|Xj
i−E[Xj
i]|>2p
N/8mjlog(800 m)]<1
400m
Pr[|Nj
i−E[Nj
i]|>2p
N/4mjlog(800 m)]<1
400m
Provided N/8mj>4 log(800 m). And so, w.p. ≥1−1
200mit follows that |Xj
i−Yj
i| ≤
4p
N/8mjlog(800 m) + 2p
N/4mjlog(800 m)≤p
6N/mjlog(800 m). Applying the Union Bound
on all m0+m1+...+mj0≤2mbins, we have the required.
We now can complete the proof that Part II also returns NULL . Note that in each invocation of
TestCloseness we use m=P
jNj
ipoints over n=mjbins. It was already established in the proof
of Lemma 6 that the probability that there exists an invocation where nmax≤q
6m
nlog (800 m)
yet due to the Laplace noise ˆnmax>q
6m
nlog (800 m) +8 log( 2/j0δ)
ϵ′ is upper bounded byδ
2. We
show a similar result for the difference of ˆZ−Z, which we denote as a random variable R∼
Lap(16 log 4/3(2/δ)nz/ϵ′). Standard properties of the Laplace distribution give that the probability that
exists even a single invocation of TestCloseness where Ris greater than log(10 j0)16 log 4/3(2/δ)nz/ϵ′
is at most 0.01. Lastly, we argue that Zisn’t too large. Based on Lemma 9 we know that at any
iteration jwe have E[Z] =Pmj
i=1(N
8m−N
8m) = 0 andVar[Z]≤2·(N/8mj+N/8mj)2mj=N2
8mj.
Using the Chebyshev Inequality and the Union Bound that Pr[∃j, Z > 10Np
log(1/α)/8mj]≤
j0·1
100 log( 1/α)<0.01.
17And so, w.p. ≥0.97−δ≥0.96we get that
ˆZ=Z+R≤10Np
log(1/α)/8mj+16 log(10 j0) log 4/3(2/δ)nz
ϵ′
(∗)
≤10Np
log(1/α)/8mj+(α
12√2k+1·log(1/α))2N2
16
(∗∗)
≤(α
12√2k+1·log(1/α))2N2
16+α2N2
16=(α
12√k+1·log(1/α))2N2
8(∗∗∗)
≤1
2α2
TestCloseness (X
iNj
i)
Where inequality (∗)holds when
16 log(10 j0) log4/3(2/δ)q
6m
nlog (800 m) +256√
j0log(2/δ) ln(2 j0/δ)
ϵ
ϵ
8√
j0log(2/δ)<(α
12√2k+1·log(1/α))2N2
16⇔
144√
log(1/α) log( 2/δ) log(10 log( 1/α)) log4/3(2/δ)q
12N
mjlog(800 m)+256√
log(1/α) log( 2/δ) ln(2 j0/δ)
ϵ
ϵ<α2N2
6912klog2(1/α)
which in turn holds when N≥˜Ω(k1/3
ϵ2/3α4/3)andN≥˜Ω(k1/2
ϵα);
inequality (∗∗)holds when
10Nr
log(1/α)
8mj≤10Nr
log(1/α)
8k≤α2N2
6912klog2(1/α)
which in turn holds when N≥Ω(k1/2log(1/α)5/2
α2 ); and inequality (∗ ∗ ∗)holds due to Proposition 22
when X
iNj
i≥N−mj·2p
N/4mjlog(800 m)≥N/2
when N≥3000mlog(800 m).
B.2 Proof of Theorem 8
Lemma 23 (Lemma 11 restated) .Fixp∈(0,1). Suppose there exists nnon-negative random
variables X1, X2, . . . , X n, such that for each iit holds that for some fixed number aiwe have
Pr[Xi≥ai]≥p. Then, given a constant c >0there exists another constant C >0such that with a
probability at least Cit holds thatPn
i=1Xi≥cPn
i=1ai.
Proof. Define the random variables Yi=aiwith probability p
0with probability 1−p. We can see that
E[Pn
i=1Yi] = pPn
i=1aiand that the variance is Var[Pn
i=1Yi] =Pn
i=1Var[Yi] =
p(1−p)Pn
i=1a2
i. Fix c >0. Using Chebyshev’s inequality, we can bound the probability of
being far from their expectation.
Pr"nX
i=1Yi≤c·E"nX
i=1Yi##
≤Pr"nX
i=1Yi−E"nX
i=1Yi#≥(1−c)E"nX
i=1Yi##
≤Var[Pn
i=1Yi]
(1−c)2·E[Pn
i=1Yi]2≤p(1−p)Pn
i=1a2
i
(1−c)2p2(Pn
i=1a2
i)2≤p(1−p)
p2(1−c)2
Now, since we have Pr[Xi≥ai]≥p= Pr[ Yi=ai]andPr[Xi∈(0, ai)]≤1−p= Pr[ Yi= 0]
for each i, it is easy to see that Pr[Pn
iXi≥a]≥Pr[Pn
iYi≥a]for any a. Hence
Pr"nX
i=1Xi≥cnX
i=1ai#
≥Pr"nX
iYi≥cnX
i=1ai#
≥1−p(1−p)
p2(1−c)2=C
Lemma 24. [17] For any two distributions PandQon[m], letP′andQ′be the merged distributions,
Then,
∥P − Q∥ Ak≤ ∥P′− Q′∥Ak+ 2∥P − Q∥ 1,k
18Proof. LetIbe the partition of [m]intokintervals so that ∥P −Q∥ Ak=P
i∈I|P(I)−Q(I)|. Let
I′be obtained from Iby rounding each upper endpoint of each interval (except for the last) down
to the nearest even integer, and rounding the lower endpoint of each interval up to the nearest odd
integer. Note that
X
I∈I′|P(I)− Q(I)|=X
I∈I′|P′(I/2)− Q′(I/2)| ≤ ∥P′− Q′∥Ak
seeing as the partition I′is obtained from Iby taking at most kpoints and moving them from
one interval to another. Therefore, the differenceP
I∈I|P(I)− Q(I)| −P
I∈I′|P(I)− Q(I)|
is at most twice the sum of |P(i)− Q(i)|over these kpoints, and therefore at most 2∥P − Q∥ 1,k.
Combining this with the above gives our result.
Lemma 25 (Lemma 13 restated) .[17] For any two distributions PandQon[m]such that
∥P − Q∥ Ak> α, there iteration j∈[log( m/k)]such that ∥PΠj− QΠj∥1,k> α/ log(m/k).
Proof. Lemma 24 asserts that
α <∥P − Q∥ Ak≤ ∥PΠ1− QΠ1∥Ak+ 2∥PΠ0− QΠ0∥1,k
We apply this recursively when we know that in the final level j0= log( m/k), we get that ∥PΠj−
QΠj∥Ak=∥PΠj− QΠj∥1,kbecause the distribution there has at most kbins. Thus
log(m/k)X
j=1∥PΠj− QΠj∥1,k≥α
Therefore, by the average principle one of the j∈[log( m/k)]must satisfy ∥PΠj− QΠj∥1,k≥
α/log(m/k); which, by Cauchy–Schwarz, gives ∥PΠj− QΠj∥2≥α/√
klog(m/k)
.
191.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction ensure that the paper focuses on adapting the
algorithm tester provided by [ 16] to preserve differential privacy. It also discusses the
difficulties that arise and how they will be addressed.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: see [1]
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
20Justification: We briefly state the positive importance of using our tester as a way for
differential privacy to dissipate into other fields in the first paragraph.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
21