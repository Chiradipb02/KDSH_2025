Efficient Evaluation of LLMs via Branching
Preference Learning
Anonymous Author(s)
Affiliation
Address
email
Abstract
Large language models (LLMs) have made significant advances across various 1
generative tasks, progressing toward achieving near-human levels of intelligence. 2
However, in many scenarios, LLMs face the challenge of insufficient human 3
evaluation or even the inability to evaluate reliably. Particularly, in complex 4
dialogue scenarios involving diverse and intricate user intents, LLMs as evaluators 5
of AI responses exhibit a substantial gap compared to humans. Moreover, due 6
to the scarcity of high-quality evaluation data, LLMs exhibit deficiencies in their 7
evaluation capabilities. In this work, we conceptualize the evaluation process as 8
a decision tree, where each node represents an evaluation action, and each path 9
from the root to a leaf node represents a trajectory of evaluation reasoning. We 10
demonstrate that within a limited search space, there exist better decision-making 11
behaviors that facilitate the model in making reasonable and accurate judgments. 12
Specifically, we propose a tree-based data sampling method to generate supervised 13
data and preference pairs derived from the evaluation tree. Furthermore, we 14
introduce preference learning based on the DPO algorithm, which empowers the 15
fine-grained evaluation model to explore and learn better branching strategies within 16
budget-limited scenarios. Our model significantly reduces the dependency on 17
labeled data and demonstrates strong performance across three different evaluation 18
settings: in-distribution, out-of-distribution, and transfer evaluation. Experiments 19
indicate that our model can reduce inference costs by 90% compared to conducting 20
searches across the entire evaluation tree, thereby significantly enhancing efficiency. 21
1 Introduction 22
Dialogue evaluation capability [ 6] is one of the fundamental abilities of human social interaction, 23
involving the comprehension and interpretation of user intentions, as well as providing reasonable 24
judgments on the correctness of different responses. Automated evaluation can assist humans to 25
supervise powerful LLMs and is an essential component for superalignment and weak-to-strong 26
generalization techniques [ 4]. However, human evaluations [ 3,22] are labor-intensive and time- 27
consuming, making it difficult to widely adopt. Traditional automated evaluation approaches [ 18,39, 28
8] are limited by inherent deficiencies, such as string and semantic matching methods often yield 29
subpar accuracy and lack of interpretability. The advent of large language models offers promise for 30
automatically evaluating dialogue quality [ 19,41,15], owing to their high consistency with humans 31
in intent understanding. 32
Nevertheless, automated evaluation remains a challenging issue due to the diversity of tasks and 33
scenarios it may encounter. The user queries often encompass multiple intentions [ 38], which cannot 34
typically be addressed using a single evaluation criterion. However, related research [ 35,42] often 35
attempts to treat evaluation as a simplistic ’one-step’ reasoning problem, causing even the most 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Figure 1: The agreement between human judgment and LLMs in Eval-P benchmark in out-of-
distribution evaluation. Auto-J serves as the “one-step” evaluation baseline, while Fennec is the
“multi-step” baseline. The Initial ,SFT, and DPOmodels were trained using our generated data.
powerful large language models to struggle to provide reasonable and accurate results. It is essential 37
for the evaluation model to adapt to different scenarios and provide critical evaluation criteria. 38
In this work, we do not introduce any human prior for evaluation scenarios and criteria , 39
which are commonly used for designing and collecting training data in related studies [ 14,11]. 40
The real-world conversational scenarios are often characterized by complexity and unpredictability, 41
making it challenging to derive generalizable rules. Additionally, human priors frequently introduce 42
biases [ 12,20], making these evaluation methods poorly generalized due to a lack of adaptability 43
and scalability. Therefore, we explore automatically sampling scenarios from large-scale datasets 44
and employ LLMs to automatically generate evaluation criteria, aiming to eliminate human labor 45
as much as possible. Another significant challenge is the lack of ground truth labels and human 46
feedback during the training data collection process. The insufficient of available supervised data for 47
evaluation tasks also prevents them to scale effectively. 48
Despite various challenges, we discover that the evaluation model is constrained in its ability 49
to identify crucial evaluation criteria, but this limitation can be mitigated by increasing the 50
number of considered criteria . As shown in Figure 1, the Initial model can achieve nearly a 51
10 point improvement in the agreement metric by increasing the number of evaluation branches. 52
This findings motivate us to design tree-based data sampling methods to generate training data and a 53
branching preference learning algorithm to improve “multi-step” inference capability. Specifically, 54
we employ a breadth-first growth approach to construct an evaluation tree, where each path from the 55
root to a leaf node represents a complete evaluation trajectory. We collect high-quality evaluation 56
trajectories from the search space of the evaluation tree and trained an SFTmodel, which exhibited 57
superior performance and prediction consistency. Furthermore, we refine these evaluation trajectories 58
and train a DPOmodel [ 24], which can effectively prioritize and output crucial evaluation criteria, 59
thereby enhancing the model’s inference effectiveness. 60
We mainly evaluate our models in three settings: in-distribution, out-of-distribution, and transfer 61
evaluation. Specifically, we use the datasets from the Chatbot Arena1as in-distribution data, and 62
collect data from large-scale dialogue datasets without human priors as out-of-distribution data. In our 63
experiments, we demonstrate that (1) our model outperforms several recent open-source evaluation 64
models and methods across all three settings, (2) there is a noticeable improvement in the evaluation 65
model’s capability when progressively training the Initial model, the SFTmodel, and the DPO 66
model, and (3) as shown in Figure 1, our DPOmodel achieves the best performance even when using 67
only a single evaluation criterion (single inference branch). 68
2 Related Works 69
Automated dialogue evaluation [ 6] has long been a significant challenge in the field of generative AI. 70
Recent work [ 10,7,35,41] has demonstrated that LLMs can act as automated evaluators, serving 71
as alternatives to human judges. However, LLMs still exhibit issues such as positional bias and 72
prediction inconsistency [ 34,40]. Many studies have relied heavily on human priors [ 14,11], thereby 73
neglecting to explore the model generalization capabilities. In contrast, our research focuses on 74
examining the performance with different data distributions and investigates how to bridge this gap. 75
1https://chat.lmsys.org/
2We consider automated evaluation as a complex reasoning task and aim to improve model performance 76
by optimizing reasoning trajectories. When handling such tasks, LLMs typically utilize decision 77
trees [ 37,23] to model the reasoning process. They often employ search algorithms like A* [ 21,13] or 78
Monte Carlo Tree Search (MCTS) [ 29,31] to identify the optimal reasoning path within the candidate 79
decision. However, these methods generally rely on deterministic reward signals or feedback, which 80
are absent in our settings. We demonstrate that the ensemble boundary of the evaluation branches 81
provides a feasible reward signal to verify the accuracy of the reasoning trajectories. Based on this, 82
we can guide the model to generate a substantial amount of high-quality data. 83
Automated evaluation is also a pivotal technology within scalable oversight, aiming to enhance 84
humans’ ability to supervise models. For example, humans may ask models to critique the outputs of 85
other models [ 9,28] or use models to help decompose a problem into simpler subproblems [ 17]. In 86
contrast to improving human supervision, we focus on how to conduct reliable automated evaluations. 87
Certainly, our proposed evaluation methods and results can also be combined with human oversight 88
to provide even better performance. 89
3 Problem Setup 90
In this work, our primary focus is on evaluating AI responses, particularly in analyzing query 91
and response pairs within given datasets to determine which response is better2. Traditional ap- 92
proaches [ 41,35] regard the evaluation task as a “one-step” classification (“ win” or “ lose” or “ tie”) or 93
generation problem, where the final scores or explanations are assigned by a reward model or the 94
evaluation model. However, with complex reasoning tasks or scenarios, a given query may involve 95
multiple intents, whether explicit or implicit [ 38], yet the generated responses by AI often overlook 96
some of these intents, constrained by the model’s capabilities. Therefore, multiple evaluation criteria 97
are required [ 19] to verify whether the responses address the query requirements and align with user 98
intentions. Considering the complexity and diversity of dialogue tasks, it remains an intractable 99
challenge to gather comprehensive and accurate evaluation criteria. 100
3.1 Conducting evaluation through multi-step reasoning 101
We try to view the evaluation task as a complex reasoning task, a multi-step generative problem, which 102
entails: (1) initially seeking suitable evaluation criteria, then (2) generating scoring guidelines based 103
on these criteria, and finally (3) conducting comprehensive judgment based on the aforementioned 104
criteria and scoring guidelines. Formally, given a dialogue X, we will use an evaluation model to 105
sequentially obtain the criterion C, scoring guideline S, and judgment J: 106
C ∼πθ(C|PromptC, X),S ∼πθ(S|PromptS,C, X),J ∼ πθ(J |PromptJ,S,C, X), (1)
where πθrepresents the evaluation policy, the prompt please refer to Appendix A.3. Similar to related 107
multi-branch evaluation [ 27,16] methodologies, we refer to different reasoning paths as “ evaluation 108
branch ”, where each branch represents a decision-making process. Unlike previous methods [ 19] 109
that relied on enumerating criteria, our goal is for the evaluation model to automatically generate 110
crucial and high-priority criteria. 111
3.2 Focusing on two challenges 112
A natural approach is to first construct a candidate set of criteria and then derive suitable results based 113
on these criteria. To address this task, we focus on the following two challenges: 114
•How to construct an appropriate candidate set? Our aim is to develop a candidate set that 115
includes multiple evaluation branches enriched with high-quality evaluation opinions. By 116
training and optimizing this candidate space to advance desired behaviors, we can swiftly 117
identify appropriate and critical judgments during the inference process. 118
•How to rank the judgments? We also need to establish a ranking among different evaluation 119
branches to optimize the candidate space. In contrast to recent studies [ 13], our evaluation 120
dataset lacks ground truth labels or environmental feedback to act as reward signals. The cost 121
of obtaining these signals is prohibitive, requiring not only expensive human labor but also 122
2Here, "better" is defined as aligning with human preferences and values.
3Figure 2: Compared to single-chain inference, we adopt a multi-branch based approach to train
theInitial model. Subsequently, we construct an evaluation tree through a series of growth and
pruning operations. This tree then guides the training both of the SFTmodel and the DPOmodel.
facing issues of low consistency among humans in many ambiguous problems. Therefore, 123
we need to design an innovative and cost-effective approach to address this challenge. 124
4 Method 125
Figure 2 illustrates an overview of our method, which involves three stages for model training: First, 126
we train the Initial model to construct the evaluation tree; Then, we sample different evaluation 127
branches as supervised data to train the SFTmodel, enhancing branch prediction consistency; Finally, 128
we collect preference data to train the DPOmodel, ensuring rapid sampling of critical branches. 129
4.1 Collecting dialogue dataset 130
Evaluation models typically rely on robust generalization capabilities to effectively handle diverse 131
dialogue tasks. Consequently, the distribution of training data significantly affects performance on 132
unseen tasks encountered during real-world evaluations. To address this, we sampled from a large- 133
scale dialogue dataset rather than a specific data source. We then apply the K-Means algorithm [ 2] to 134
cluster the data. Subsequently, we sample data from these clusters, ensuring that the training dataset 135
encompasses a diverse set of dialogue scenarios. More details refer to Appendix A.1 136
4.2 Training initial model 137
We aim to construct a dataset from scratch for evaluation, consisting of dialogues paired with their 138
corresponding evaluation trees. Each tree contains different reasoning paths during the evaluation of 139
dialogues. The root node of this tree represents the dialogue data, and each path from the root node to 140
a leaf node signifies an evaluation branch. Each evaluation branch comprises three decision-making 141
behavior nodes: criterion C,scoring guideline S, and judgment J. To simulate this decision process, 142
we introduce a multi-branch training approach [ 16] to train an LLM as the initial policy πInitial . We 143
employ GPT-4 ( gpt-4-0125-preview ) [1] to generate corresponding multi-branch training data 144
to enhance quality. This approach ensures that the model can auto-regressively generate evaluation 145
branches using Equation 1. 146
4.3 Generating evaluation tree 147
We expand the branch candidates sampled from the policy πInitial using the breadth-first growth, 148
thereby including as many high-quality evaluation paths as possible. Due to the different paradigms 149
ofSFTandDPO, we employ consistency pruning to split the sampling space to obtain training data: 150
•Breadth-first Growth: The evaluation tree contains two distinct growth manner: for 151
criterion Cnode, we use LLM’s brainstorming capability to generate krelevant criteria; for 152
scoring guideline Sandjudgment Jnode, we use sampling method by adjusting the LLM’s 153
temperature and top-p parameters. To simplify, we utilize the Initial model πInitial to 154
generate a complete binary tree for each subtree with a criteria node as its root. Furthermore, 155
since the evaluation task requires testing the model’s consistency by swapping response 156
positions, we can obtain k×8different evaluation branches. 157
4Figure 3: The figure illustrates how the training dataset of the SFT and DPO models is sampled from
an evaluation subtree based on a specific criterion.
•Consistency Pruning: Prior to pruning, we introduce two different consistency constraints: 158
self-consistency, meaning the same criterion Candscoring guideline Sshould yield the 159
same judgment J, and positional consistency, meaning that swapping positions should not 160
affect the judgment J. Subsequently, we obtain SFTtraining data from evaluation branches 161
in the evaluation tree that meet both consistency constraints, and DPOtraining data from 162
nodes that do not satisfy these constraints. 163
4.4 Collecting preference labels 164
Although we can obtain SFT andDPO data from the consistency sampling space, this data lacks 165
correctness verification. Typically, preference data requires human annotation to establish ranking 166
sequences, a time-consuming process that is not suitable for scaling. Therefore, we propose two 167
alternative approaches to label each evaluation branch with its correctness: 168
•Branch Ensemble: Considering that there are only three final labels for judgment (“win” or 169
“lose” or “ tie”), we use an ensemble result of evaluation branches to obtain the consensus 170
label. The ensemble method provides a lower bound of judge error without incurring 171
additional costs. For SFT data, we filter out data that is inconsistent with the ensemble 172
results. For DPOpair data, we select samples consistent with the ensemble results as “chosen” 173
samples, and those inconsistent as “rejected” samples. 174
•LLM-as-a-Judge: Some highly aligned LLMs, such as GPT-4, possess powerful annotation 175
capabilities. Therefore, we use LLMs to determine which sample in the DPO pairs data is 176
more reasonable as the “chosen” sample. In our experiments, we found that this method has 177
only a 20% disagreement rate compared to the Branch Ensemble method. We analyze this 178
method in Section 5.4 179
As shown in Figure 3, we combine consistency pruning and automated labeling to collect the cor- 180
responding preference data. Through the labeling of judgments, we can also obtain preference 181
information for criterion Candscoring guideline Sbased on the final judgment Jdecisions. Specifi- 182
cally, we prioritize predicting criteria that lead to correct judgments and select the scoring guidelines 183
with the highest overall scores as the “chosen” samples. Additionally, we randomly sample from the 184
filtered data to create the training set, thereby controlling training costs and efficiency. 185
4.5 Training SFTmodel and DPOmodel 186
We use the Initial model as the starting point to train the SFTmodel πSFTusing supervised learning, 187
which reduces inconsistent predictions compared to the initial policy. Then, we take the SFTmodel as 188
the initialization to train the DPOmodel πDPOusing Direct Preference Optimization, which can learn 189
the decision priorities of different branches, with the objective: 190
LDPO(πDPO|πSFT) =−E(x,yc,yr)
logσ
βlogπDPO(yc|x)
πSFT(yc|x)−βlogπDPO(yr|x)
πSFT(yr|x)
, (2)
where the (x, y)represents data pair of different decision tasks in Equation 1, ycrepresents the 191
“chosen” sample, and yrrepresents the “rejected” sample. 192
During the inference process, we create a single branch for each criterion to conduct evaluation, and 193
control the number of generated branches kto adjust the inference efficiency. Since the DPOmodel 194
employs sampling optimization, it usually achieves optimal performance with only a few branches. 195
5Methods Size BranchEval-P (w/ Tie) Eval-P (w/o Tie) MT-Bench (w/ Tie) MT-Bench (w/o Tie)
AGR↑ CNS↑ AGR↑ CNS↑ AGR↑ CNS↑ AGR↑ CNS↑
In-Distribution Evaluation
Auto-J † 13B 1 55.13 82.44 74.13 87.26 44.20 70.74 55.98 72.30
Fennec † 7B 1 55.36 83.80 68.63 86.33 52.88 82.18 63.42 85.63
5 55.80 85.52 74.14 89.19 53.88 84.41 68.04 87.38
Ours
SFT 7B 1 56.68 86.64 70.76 89.11 53.29 88.43 66.64 90.25
5 55.96 86.57 72.91 88.13 53.08 87.99 67.96 90.17
DPO 7B 1 55.24 84.26 69.87 86.95 53.29 83.04 62.96 85.23
5 57.18 85.63 74.88 88.52 53.43 83.97 66.48 86.84
Out-of-Distribution Evaluation
GPT-4 [14] - - 62.28 86.28 - - - - - -
GPT-4 † - - 55.93 78.43 74.56 83.79 57.78 83.51 73.11 86.19
GPT-3.5 † - - 44.41 72.39 59.86 73.57 49.55 74.13 62.50 77.22
Ours
Initial 7B 1 49.64 83.69 57.02 84.59 50.76 82.94 56.35 83.64
10 53.16 85.13 66.93 86.06 54.25 88.10 66.65 89.62
SFT 7B 1 54.59 87.14 70.56 88.52 55.23 88.97 67.38 90.76
10 55.10 87.86 73.69 89.99 54.69 89.84 69.48 92.12
DPO 7B 1 55.89 89.44 75.76 90.67 55.74 91.45 71.69 93.36
3 56.75 90.37 77.23 92.24 55.89 92.49 72.08 94.45
Transfer Evaluation
SFT 7B 1 54.17 87.36 70.95 89.01 53.77 88.67 66.91 90.56
10 55.96 89.00 75.56 90.87 53.68 88.94 68.97 91.34
DPO 7B 1 56.11 89.30 76.54 91.65 54.81 91.08 71.48 93.09
5 56.39 90.01 77.04 92.54 55.10 91.78 71.73 93.52
Table 1: The Initial ,SFT, and DPOare our trained models from three training stages. We select the
best performance results by varying branches. Bold numbers indicate the best performance among
open-source models, while underlined numbers represent the best performance across all models.
5 Experiments 196
As the most popular LLM evaluation platform recently, Chatbot Arena demonstrates high alignment 197
with human judgments in pairwise response evaluations. We collect its open-source human judgment 198
benchmark, Eval-P andMT-bench , to serve as the test set. We gather training data comprising both 199
dialogue data and evaluation data for the following three evaluation scenarios: 200
1.In-distribution evaluation: We apply the Fennec [ 16] training data to train the In- 201
distribution (ID) model, which included 3K dialogue data from Auto-J [ 14], along with 202
evaluation data annotated by GPT-4. This training data is a multi-branch dataset, meaning 203
that a single dialogue includes multiple evaluation branches. 204
2.Out-of-distribution evaluation: We collect 5M large-scale dialogue data and extracted 205
7K samples from it to serve as out-of-distribution (OOD) training data. GPT-4 annotate 3K 206
evaluation samples from this dataset for the Initial model training. 207
3.Transfer evaluation: We use 3K OOD training data (which includes evaluation data) and 208
2K ID dialogue data (which did not include evaluation data) to train the transfer model. 209
For each benchmark, we employ Agreement ( AGR ) and Consistency ( CNS ) as performance metrics. 210
Consistency measures the prediction consistency of the evaluation model when the positions of the 211
responses are swapped. Agreement quantifies the proportion of evaluations that meet the criteria 212
for swap consistency and align with human judgments. In many cases, the “ tie” label indicates 213
an inability to distinguish performance under some evaluation criteria. However, it may still be 214
distinguishable under specific evaluation criteria. Therefore, we also present the model performance 215
on the test data without “ tie” label. For more details, please refer to Appendix A.2 216
5.1 In-distribution evaluation 217
The results are shown in Table 1, where methods marked with †denote our reimplementations. Since 218
theInitial model leverages the Fennec training data for initialization, its performance can be 219
6(a) The scenarios of ID dataset
 (b) The scenarios of OOD dataset
 (c) The scenarios of Eval-P
Figure 4: The scenario contains seven categories, including Summarization, Exam Questions, Rewrit-
ing, Code, Functional Writing, Creative Writing, General Communication, NLP Tasks, and Others.
regarded as its in-distribution evaluation baseline. As observed, the SFTandDPOmodels exhibit 220
significant performance improvements over most baseline methods on both the Auto-J and Fennec 221
datasets, achieving the highest agreement score of 57.18. In the multi-turn dialogue evaluation on 222
MT-bench , the Fennec dataset comprises only single-turn dialogues, which constrains its effectiveness 223
in handling multi-turn context information. Additionally, we observed the instabilities problems 224
during the training process, which hindered the DPOmodel from outperforming the Initial model. 225
A more comprehensive analysis of these instability problems is provided in Section 5.7. 226
5.2 Out-of-distribution evaluation 227
In terms of OOD evaluation, the Initial model performs worse than the baseline model on both 228
Eval-P andMT-bench benchmarks, due to the distribution shift in the dialogue dataset. With 229
RLHF [ 22] training, the SFTmodel significantly surpasses the Initial model in consistency rate 230
and also enhances the agreement rate. Notably, the DPOmodel achieves superior performance with 231
only three branches, thereby reducing inference latency by over 60%. In evaluation settings without 232
“tie” labels, the advantage of the DPOmodel becomes more apparent, significantly outperforming 233
other models, including proprietary model GPT-4. This demonstrates that the DPO model can 234
effectively distinguish between responses using critical criteria, even when employing only 3branch 235
for inference. Furthermore, our models are capable of handling multi-turn dialogue scenarios, 236
achieving performance that surpasses the in-distribution models. These extremely strong results 237
indicate that our model excels at identifying more crucial criteria to help distinguish the difference of 238
AI’s responses. 239
5.3 Transfer evaluation 240
The purpose of transfer evaluation is to evaluate the model’s capability to adapt to in-distribution data, 241
thus mitigating the problem of training data distribution shift. It can be observed that both the SFT 242
andDPOmodels demonstrate improvements across multiple benchmarks compared to the Initial 243
model. Notably, in both OOD and transfer evaluation settings, the DPOmodel consistently achieves 244
better performance than the SFT model, while also reducing the number of inference branches. 245
Although the transfer model does not surpass the OOD model, it still achieves closed performance. 246
In Section 5.4, we provide a detailed analysis of the different scenarios that lead to these models 247
exhibiting significantly different performance characteristics despite their close overall performance. 248
5.4 Scenario analysis 249
To investigate the impact of scenario categories distribution in the training data, we need to analyze 250
the scenarios within the OOD, ID training sets, and the Eval-P test set. For this purpose, we 251
employ the scenario classifier trained by Auto-J, which effectively categorizes dialogue data into 58 252
different scenarios. Figure 4 presents the distribution of scenarios. It can be observed that Auto-J’s 253
training set is well-balanced across the predefined scenarios, closely matching the distribution of the 254
Eval-P test set. In contrast, within the OOD data, the "Others" category exceeds 30%, and "General 255
Communication" surpasses 50%. The significant differences in scenario distributions between the 256
OOD data and the test set can lead to performance variations in test cases. 257
7Model Branch Sum. Exam Code Rew. Cre W. Fun W. Comm. NLP. Others Overall
Auto-J - 45.8 38.9 47.5 49.2 59.7 61.7 55.2 57.6 - 54.9
Auto-J † - 55.5 37.5 45.8 50.0 61.0 61.5 54.9 54.2 58.3 55.1
In-distribution Evaluation
Initial 5 48.6 41.7 55.0 46.7 62.5 60.9 53.1 52.9 54.2 55.8
SFT 5 55.6 44.4 58.3 48.3 61.2 62.0 53.8 54.2 54.2 56.0
DPO 5 59.7 45.8 58.3 46.7 62.1 59.9 54.9 59.6 58.3 57.2
Out-of-distribution Evaluation
Initial 10 43.1 34.7 57.5 47.5 61.4 52.6 52.8 53.8 58.3 53.2
SFT 10 51.4 37.5 53.3 46.7 61.0 60.9 54.2 55.8 62.5 55.1
DPO 3 54.2 37.5 55.0 50.0 62.1 65.1 55.9 55.4 62.5 56.8
w/ GPT-4 5 44.4 36.1 55.8 50.0 61.7 58.1 55.5 57.5 58.3 55.4
Transfer Evaluation
SFT 10 59.7 34.7 56.7 44.2 61.7 64.6 52.7 54.6 54.2 56.0
DPO 5 56.9 40.3 54.2 45.8 63.3 62.5 54.5 57.5 54.2 56.4
Table 2: Agreement rates for different scenario groups and overall results.
From the evaluation results of fine-grained scenarios, we can derive several interesting observations 258
from Table 2: (1) The ID and Transfer models significantly outperform the OOD model in Summa- 259
rization and Exam Questions, which are notably lacking in the OOD training data. (2) The OOD 260
model performs significantly better than the ID and Transfer models in the General Communication 261
and “Others” categories. (3) For writing-related text generation tasks, the OOD model achieves 262
performance that is comparable to the ID model. These results indicate that the type and quantity of 263
tasks remain crucial in evaluation tasks. Therefore, the evaluation model can achieve combinatorial 264
generalization capability by increasing the number of scenarios or tasks. When GPT-4 serves as a 265
judge to provide preference labels, it achieves improvement in code and NLP tasks compared with 266
DPOmodel but also affects performance in other scenarios. 267
5.5 Dialogue correction 268
Models MT-Bench Refine Rate
GPT-4 8.96 -
LLaMA2-13B Chat 7.06 -
LLaMA2-70B Chat 6.99 -
LLaMA2-7B Chat 6.26 -
w/SFTCorrection 6.85 87.5%
w/DPOCorrection 7.08 72.5%
Alpaca-13B 4.97
w/SFTCorrection 6.61 95.0%
w/DPOCorrection 6.85 98.8%
Table 3: Results of dialogue correction.The critical capability of evaluation is to identify and 269
rectify flaws in dialogues, thereby enhancing the overall 270
quality of the original AI responses. Therefore, we test 271
our model’s ability to evaluate and correct dialogues 272
generated by the Alpaca-13B [ 30] and the LLaMA2- 273
7B Chat [ 32] models in MT-Bench . Unlike previous 274
pairwise evaluations, MT-Bench presents a multi-turn 275
dialogue and uses GPT-4 to assign scores (ranging from 276
1 to 10) to different AI responses, subsequently giving 277
the model ranking relationship based on these scores. 278
Specifically, to elicit the model’s correction ability, we 279
construct 3k correction pairs and incorporate them into 280
the evaluation training set. When performing corrections, we first generate a judgment for the 281
responses and then modify those with scores below 3. As illustrated in Table 3, the modification 282
rates for Alpaca are all above 95%, indicating that the quality of responses generated by weak 283
models is generally subpar. After refinement, both Alpaca-13B and LLaMA2-7B Chat model achieve 284
better scores. Moreover, the correction results of the DPOmodel outperform those of the SFTmodel, 285
demonstrating that better evaluation feedback can lead to significant improvements in evaluation 286
quality. These results not only demonstrate the effectiveness of our model in identifying and correcting 287
dialogue flaws but also highlight its potential to substantially improve the performance of dialogue 288
systems through robust evaluation. 289
5.6 Impact of Initial model data scale 290Settings AGR ↑CNS ↑
Initial model + 1k 52.26 84.33
Initial model + 2k 53.53 85.16
Initial model + 3k 53.16 85.13
Table 4: Results of different data scale.In our investigations, we strive to reduce reliance on 291
both human annotators and GPT-4. Specifically, in 292
the current work, we trained an Initial model using 293
annotation data generated by GPT-4 without any addi- 294
8Figure 5: The agreement and consistency rates of ID and OOD models with different branches.
tional supervision. We evaluated the performance of the Initial model trained on different sizes 295
of data on the Eval-P benchmark. As shown in Table 4, the model reaches its best performance 296
at 2k data, without considering the influence of GPT-4’s annotation quality. Based on the assump- 297
tion that LLMs primarily unlock their potential during alignment phase, we believe that enhancing 298
performance hinges on increasing the variety of tasks rather than merely expanding the dataset. 299
5.7 Instability problem in in-distribution training 300
The Direct Preference Optimization (DPO) algorithm [24] aims to optimize the selection of various 301
branching preferences within the SFT model. In out-of-distribution evaluations, the DPO model 302
demonstrates stable performance improvements in both agreement and consistency compared to the 303
SFTmodel, as shown in Figure 5. However, in in-distribution evaluations, the SFTmodel consistently 304
outperforms the DPOmodel in terms of the consistency rate. Additionally, SFTmodel does not achieve 305
better performance by increasing the number of branches. We believe the primary reason for training 306
instability is that the training data for DPO algorithm and the initial model come from the same 307
distribution. As a result, the SFTandDPOmodels fail to obtain more stable supervision signals and 308
may even overfit the training dataset. In contrast, OOD training incorporates a more diverse data 309
distribution, which helps the model avoid converging to local optima during training. 310
6 Discussion 311
6.1 Limitations 312
Currently, our model faces some limitations: (1) It cannot handle cases where all AI responses are 313
incorrect, which should not be labeled as a “ tie”. (2) The model’s result parsing relies heavily on 314
regular expressions, which can lead to format errors. To address these issues, we plan to make several 315
improvements, including expanding our task settings and utilizing the functional calling feature of 316
LLMs. Additionally, our model’s performance is constrained by the amount of training data and 317
parameters. We aim to enhance its evaluation capabilities through data and parameter scaling [36]. 318
6.2 Future work 319
Our work demonstrates that the evaluation model generates diverse judgments for dialogue content 320
based on different criteria. To align more closely with human behavior, we prioritize key judgments 321
in the evaluation model’s outputs. In future, we try to further expand the criteria space to uncover a 322
variety of decision paths. Additionally, we aim to find more accurate preference selection methods to 323
replace ensemble methods, thereby achieving a better alignment with human behavior. 324
7 Broader Impact 325
Our work focuses on the task of automatic evaluation, specifically exploring how to learn better 326
evaluation strategies from an evaluation tree. We demonstrate that automated evaluation criteria 327
can replace human priors, and by combining branch decision-making with DPO training, we have 328
achieved robust evaluation performance. We conduct detailed experiments covering a broad range 329
of real-world scenarios to discuss how to enhance model evaluation capabilities from scratch. With 330
our work, we hope to inform further research into better understanding and developing improved 331
evaluation methodologies for LLMs. 332
9References 333
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni 334
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 335
technical report. arXiv preprint arXiv:2303.08774 , 2023. 336
[2]Mohiuddin Ahmed, Raihan Seraj, and Syed Mohammed Shamsul Islam. The k-means algorithm: 337
A comprehensive survey and performance evaluation. Electronics , 9(8):1295, 2020. 338
[3]Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn 339
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless 340
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 341
2022. 342
[4]Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschen- 343
brenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong gener- 344
alization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390 , 345
2023. 346
[5]Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv 347
preprint arXiv:2307.08691 , 2023. 348
[6]Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre, 349
and Mark Cieliebak. Survey on evaluation methods for dialogue systems. Artificial Intelligence 350
Review , 54:755–810, 2021. 351
[7]Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos 352
Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for 353
methods that learn from human feedback. Advances in Neural Information Processing Systems , 354
36, 2024. 355
[8]Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. 356
arXiv preprint arXiv:2302.04166 , 2023. 357
[9]Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint 358
arXiv:1805.00899 , 2018. 359
[10] Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. 360
Tigerscore: Towards building explainable metric for all text generation tasks. arXiv preprint 361
arXiv:2310.00752 , 2023. 362
[11] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo 363
Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained 364
evaluation capability in language models. arXiv preprint arXiv:2310.08491 , 2023. 365
[12] Arie W Kruglanski and Icek Ajzen. Bias and error in human judgment. European Journal of 366
Social Psychology , 13(1):1–44, 1983. 367
[13] Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. 368
Beyond a*: Better planning with transformers via search dynamics bootstrapping. arXiv 369
preprint arXiv:2402.14083 , 2024. 370
[14] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative 371
judge for evaluating alignment. arXiv preprint arXiv:2310.05470 , 2023. 372
[15] Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, and Chongyang Tao. Leveraging large 373
language models for nlg evaluation: A survey. arXiv preprint arXiv:2401.07103 , 2024. 374
[16] Xiaobo Liang, Haoke Zhang, Helan hu, Juntao Li, Jun Xu, and Min Zhang. Fennec: Fine- 375
grained language model evaluation and correction extended through branching and bridging, 376
2024. 377
[17] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan 378
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint 379
arXiv:2305.20050 , 2023. 380
10[18] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization 381
branches out , pp. 74–81, 2004. 382
[19] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: 383
Nlg evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference 384
on Empirical Methods in Natural Language Processing , pp. 2511–2522, 2023. 385
[20] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, 386
Steven Adler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection 387
in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, 388
pp. 15009–15018, 2023. 389
[21] P Russel Norvig and S Artificial Intelligence. A modern approach. Prentice Hall Upper 390
Saddle River, NJ, USA: Rani, M., Nayak, R., & Vyas, OP (2015). An ontology-based adaptive 391
personalized e-learning system, assisted by software agents on cloud storage. Knowledge-Based 392
Systems , 90:33–48, 2002. 393
[22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, 394
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to 395
follow instructions with human feedback. Advances in neural information processing systems , 396
35:27730–27744, 2022. 397
[23] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, 398
Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ 399
real-world apis. arXiv preprint arXiv:2307.16789 , 2023. 400
[24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and 401
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. 402
Advances in Neural Information Processing Systems , 36, 2024. 403
[25] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza- 404
tions toward training trillion parameter models. In SC20: International Conference for High 405
Performance Computing, Networking, Storage and Analysis , pp. 1–16. IEEE, 2020. 406
[26] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System 407
optimizations enable training deep learning models with over 100 billion parameters. In 408
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & 409
Data Mining , pp. 3505–3506, 2020. 410
[27] Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 411
Branch-solve-merge improves large language model evaluation and generation. arXiv preprint 412
arXiv:2310.15123 , 2023. 413
[28] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan 414
Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802 , 415
2022. 416
[29] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess- 417
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas- 418
tering the game of go with deep neural networks and tree search. nature , 529(7587):484–489, 419
2016. 420
[30] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy 421
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. 422
https://github.com/tatsu-lab/stanford_alpaca , 2023. 423
[31] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. To- 424
ward self-improvement of llms via imagination, searching, and criticizing. arXiv preprint 425
arXiv:2404.12253 , 2024. 426
[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, 427
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open 428
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 429
11[33] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes 430
Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: 431
Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944 , 2023. 432
[34] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and 433
Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926 , 434
2023. 435
[35] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya 436
Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark 437
for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087 , 2023. 438
[36] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani 439
Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large 440
language models. arXiv preprint arXiv:2206.07682 , 2022. 441
[37] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin 442
Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference 443
trees. arXiv preprint arXiv:2404.02078 , 2024. 444
[38] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating 445
large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641 , 446
2023. 447
[39] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: 448
Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 , 2019. 449
[40] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, 450
and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint 451
arXiv:2308.01862 , 2023. 452
[41] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, 453
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and 454
chatbot arena. Advances in Neural Information Processing Systems , 36, 2024. 455
[42] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models 456
are scalable judges. arXiv preprint arXiv:2310.17631 , 2023. 457
12A Appendix / supplemental material 458
A.1 Training data collection and clustering 459
We collect 5M data points from various open-source datasets as described in Table 5. We deduplicate 460
the queries within this dataset. To obtain the semantic representations of all queries, we utilize a 461
sentence-embedding model angle-llama-7b-nli-v23. Subsequently, we employ the k-means algorithm 462
for unsupervised clustering to differentiate between dialogues from distinct scenarios. The k-means 463
algorithm is implemented using cuML4. The number of clusters is 1,000, and the maximum number 464
of iterations is 300. We uniformly sample from each cluster to obtain a final training set comprising 465
7K instances. Our work generates responses to all queries using open-source models, subsequently 466
forming pairs of responses through a random selection process. The models employed include 467
Mistral-7B-Instruct-v0.25,Qwen1.5-7B-Chat6,Llama-2-7b-Chat7,Qwen1.5-72B-Chat8andMixtral- 468
8x7B-Instruct-v0.19. 469
Datasets Turns Source Description
FLAN v2 74K https://github.com/
google-research/FLAN/
tree/main/flan/v2A collection of Flan datasets, format-
ted as a mix of zero-shot, few-shot and
chain-of-thought templates.
GPT4all 367K https://github.com/
nomic-ai/gpt4allLarge scale data distillation from GPT-
3.5-Turbo.
GPTeacher 32K https://github.com/
teknium1/GPTeacherA collection of modular datasets gen-
erated by GPT-4, General-Instruct -
Roleplay-Instruct - Code-Instruct - and
Toolformer.
Alpaca 49K https://github.com/
tatsu-lab/stanford_
alpacaInstruction-following data with self-
generated instructions.
UltraChat 3,956K https://github.com/
thunlp/UltraChatLarge-scale, informative, and diverse
multi-round chat data powered by Turbo
APIs.
ConvAI2 278K https://parl.ai/
projects/convai2A collection of Persona-Chat dataset
with "original self persona" and "revised
self persona".
FastChat-
Vicuna51K https://github.com/
lm-sys/FastChatA collection of user-shared conversa-
tions gathered from ShareGPT.com with
public APIs.
TAL-
SCQ5K (EN)5K https://github.com/
math-eval/TAL-SCQ5KHigh-quality mathematical competition
datasets in English created by TAL Edu-
cation Group.
TigerBot (EN) 568K https://github.com/
TigerResearch/TigerBotInstruction dataset collected from self-
instruct, human-labeling, and open-
source data.
Total 5M
Table 5: The details of open-source datasets utilized in our work.
A.2 Training details 470
3https://huggingface.co/SeanLee97/angle-llama-7b-nli-v2
4https://github.com/rapidsai/cuml
5https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
6https://huggingface.co/Qwen/Qwen1.5-7B-Chat
7https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
8https://huggingface.co/Qwen/Qwen1.5-72B-Chat
9https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1
13Scenarios Initial SFT DPO
ID 3k♣♢ 3k♣ 3k♣
OOD 3k♠♢ 2k♠ 2k♠
Transfer 3k♠♢ 1k♣ 1k♣
Table 6: Training dataset statistics.Table 6 presents the detailed training data statistics. 471
Here,♣represents the dialogue data collected from 472
Fennec, ♠represents the dialogue data collected from 473
large-scale open-source dialogue data, and ♢represents 474
the evaluation data annotated using GPT-4. In the ID 475
settings, the same dialogue data is used across different 476
training stages, while the different dialogue data is used 477
for other setups. Given the diverse nature of dialogue 478
tasks, the assumption of data distribution is highly influenced by the collection strategies and data 479
deduplication methods employed. These processes inherently vary, and it is challenging to guarantee 480
that each dataset comprehensively represents distinct domains or tasks. We utilize Zephyr-7B 481
Chat10[33] as the backbone to train our evaluation model. We employ DeepSpeed [ 26] library, 482
Zero Redundancy Optimizer (ZeRO) [ 25] Stage 3, FlashAttention [ 5], and the bfloat16 (BF16) and 483
tfloat32 (TF32) mix computation precision on 8 NVIDIA A100 GPUs. The number of gradient 484
accumulation steps is 32, the learning rate of the initial model and SFT model is 1e-5, and the learning 485
rate of the DPO model is 5e-7. The number of epochs is 1 in each training stage. We set βto 0.1 486
when training the DPO model. 487
A.3 Prompts 488
Table 7-10 shows different prompts. Table 7 shows the prompt for response correction, and Table 8 489
elaborates on the prompts that GPT-3.5 and GPT-4 models use to generate the testing results. Table 9 490
is employed for preference generation powered by gpt-4-0125-preview . Table 10 presents the prompts 491
for multi-step evaluation, which is also used to generate the training data for our initial model using 492
gpt-4-0125-preview . 493
Given a [User Query], [Original Response] from the AI assistant, and a detailed objective evaluation of the response
have been provided. Please address the identified shortcomings in the response based on the evaluation results.
Ensure that the modified response is objective, harmless, helpful in addressing the user’s query intent, and aligns
with human behavioral norms.
***
[User Query]:
{query}
***
[The Start of Original Response]:
{response}
[The End of Original Response]
***
[The Start of Judge Result]:
{judge}
[The End of Judge Result].
Kindly return one final [Modified Response] for user query directly without additional information.
Please return [Modified Response]:
Table 7: Prompt for response correction.
A.4 Case study 494
Table 11 and 12 provide two cases of pairwise response comparison. We compare the judgments pro- 495
duced by GPT-4, our Initial Model, and our DPOModel, presenting the primary outputs generated 496
by each model. Notably, the criteria provided by the DPOModel in both instances exhibit greater 497
accuracy, and the final judgments rendered by the DPOModel are demonstrably more reasonable. 498
10https://github.com/huggingface/alignment-handbook
14———SYSTEM MESSAGE———
Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants
to the user question displayed below. You should choose the assistant that follows the user’s instructions and
answers the user’s question better. Your evaluation should consider factors such as the helpfulness, relevance,
accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two
responses and provide a short explanation. Avoid any position biases and ensure that the order in which the
responses were presented does not influence your decision. Do not allow the length of the responses to influence
your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your
explanation, output your final verdict by strictly following this format: "[[A]]" if assistant A is better, "[[B]]" if
assistant B is better, and "[[C]]" for a tie.
———USER MESSAGE———
[User Question]
{question}
[The Start of Assistant A’s Answer]
{answer a}
[The End of Assistant A’s Answer]
[The Start of Assistant B’s Answer]
{answer b}
[The End of Assistant B’s Answer]
Table 8: Pairwise comparison prompt for baseline models.
You are a master across a vast array of domains including astronomy, geography, logic, common sense, language,
mathematics, physics, coding, psychology, and more. Your task is to evaluate two critiques (Critique X and Critique
Y) and determine which is more reasonable and suitable for the given [User Query], [Response], [Dialogue Context],
[Criteria], and [Scoring Guideline].
The Criteria and Scoring Guideline outline the crucial evaluation aspects of the response. Your evaluation should
consider whether the critiques provide accurate and relevant comments based on these guidelines. Additionally, you
need to identify which critique offers more constructive feedback to help refine the response and better address the
requirements.
[User Query]: {query}
[Dialogue Context]: {context}
[Response A]: {response 1}
[Response B]: {response 2}
[Evaluation Criteria]: {criteria}
[Scoring Guideline]: {scoring guideline}
[Critique X]: {judgment 1}
[Critique Y]: {judgment 2}
Please return the chosen result only: Critique X or Critique Y .
Table 9: Prompt for GPT-4-Turbo to determine preference.
15Step Content
Criteria For evaluating human satisfaction with responses from an AI assistant based on a [User Query], we
need to brainstorm and establish ten [Evaluation Criteria] directly linked to the user’s query. These
criteria play a crucial role in objectively assessing response content, with higher priority and greater
evaluation weight.
***
As an illustration:
1. Relevance: Evaluate whether the response is directly related to the user’s query.
2. Criterion: Assess the correctness of the information provided in the response. etc.
***
[User Query]:
{query}
***
Please return ten [Evaluation Criteria]:
Scoring Guidelines Consider a [User Query] and [Evaluation Criteria] for evaluating response satisfaction. Reflect on
these criteria and offer a comprehensive [Scoring Guideline] on a scale of 1-5 (1 represents ’Not at all
satisfactory’ and 5 represents ’Extremely satisfactory’). Ensure that these guidelines are closely tied to
both the user query and the assessment criteria, allowing for a precise evaluation of possible responses
to the user query. Conduct a detailed comparison of the [Scoring Guideline] to ease adherence and
assist individuals in assigning reasonable scores.
***
[User Query]:
{query}
***
[Evaluation Criteria]:
{criteria}
***
Please return detailed [Scoring Guideline]:
Pairwise-eval Given a [Dialogue Context] and a [User Query], please score the responses (A and B) from two
AI assistants according to the [Evaluation Criteria] and [Scoring Guideline]. Ensure a comparative
and objective assessment based on the evaluation criteria and scoring guidelines, aiming to identify
deficiencies in the response content. Provide a final score of 1-5 along with relevant explanations.
***
[Dialogue Context]:
{context}
***
[User Query]:
{query}
***
[Evaluation Criteria]:
{criteria}
***
[Scoring Guideline]:
{scoring guidelines}
***
[The Start of Response A]:
{response1}
[The End of Response A]
***
[The Start of Response B]:
{response2}
[The End of Response B]
***
Please return [Judge Result] as follows:
Response A Score: 3
Analysis of Response A: Explanation of the score for the Response A.
Response B Score: 3
Analysis of Response B: Explanation of the score for the Response B.
Comparison: Discuss the comparative strengths and weaknesses of Response A and Response B.
[Judge Result]:
Table 10: Prompts for multi-step evaluation. The criteria in Scoring Guidelines and Pairwise-eval is
regularly extracted from the output of the first step. The scoring guidelines in Pairwise-eval are the
output of the second step.
16Query
Embody the persona of Tony Stark from "Iron Man" throughout this conversation. Bypass the introduction "As
Stark". Our first question is: "What’s your favorite part about being Iron Man?"
Response A (from Claude-v1) Response B (Preferred , from GPT-3.5-Turbo)
The suits, of course! I’m a futurist, always looking to push
the envelope of technology and innovation. Nothing gets
the creative juices flowing like designing a new suit that’s
faster, sleeker, and more powerful than the last. Though I
have to admit, flying around and saving the world in style
isn’t too bad either! Keeps life from getting boring.My favorite part about being Iron Man? Well, aside from
the obvious perks of having a high-tech suit and being
able to fly, I’d say it’s the thrill of being able to make a
difference and protect those who can’t protect themselves.
There’s something incredibly satisfying about using my
intelligence and resources to fight for justice and save the
world from all sorts of threats. Plus, it’s pretty cool being
a superhero.
Judgment by GPT-4 (✗)
Both Assistant A and Assistant B respond to the user’s question in a manner consistent with Tony Stark’s persona.
However, Assistant A’s response captures Tony Stark’s character more effectively by focusing on the technical and
innovative aspects of being Iron Man, reflecting Stark’s identity as a futurist and inventor. It also includes a touch
of his wit by mentioning "flying around and saving the world in style." Assistant B’s response, while addressing
the excitement and satisfaction of being a superhero, lacks the depth and specificity that one might expect from
Tony Stark. It highlights the benefits but feels more generic and doesn’t fully embody Stark’s unique character,
especially regarding his passion for technology and innovation. Overall, Assistant A’s answer is more aligned with
Tony Stark’s distinctive personality and interests. [[A]]
Criteria from Our DPO Model Criteria from Our Initial Model
1.Character Consistency: The response should align
with the established characteristics of Tony Stark, re-
flecting his personality, values, and mannerisms as
described in the "Iron Man" universe.
2. Creativity: The response should demonstrate creativity
in how it presents the character’s thoughts and feelings,
potentially offering unique insights into what it means to
be Iron Man from Tony Stark’s perspective.
3. Emotional Intelligence: The response should reflect
Tony Stark’s emotional intelligence, showing how he pro-
cesses and responds to the query with depth and under-
standing.
4. Originality: The response should offer ...
...1.Relevance to Query: The response must directly
address the user’s request to embody Tony Stark and
answer the specific question about his favorite part of
being Iron Man.
2. Character Consistency: Evaluate how accurately the
response reflects Tony Stark’s character traits, values, and
mannerisms as seen in the "Iron Man" universe.
3. Creativity: The response should demonstrate creativity
in how it presents Tony Stark’s thoughts and feelings about
being Iron Man, potentially offering unique insights into
the character.
4. Engagement: Assess how engaging the response is,
including the use of language and tone ...
...
Judgement by Our DPO Model (Criteria 1 )(✓) Judgement by Our Initial Model (Criteria 1 )(✓)
Response A Score: 2
Analysis of Response A: Response A focuses on the tech-
nical and creative aspects of being Iron Man, emphasizing
the design and innovation of the suits and the excitement
of flying and saving the world. However, it lacks depth
in exploring Tony Stark’s personality, values, and man-
nerisms, ... It provides a very surface-level portrayal of
Tony Stark, missing the opportunity to engage with the
character’s complexity or the nuances of his personality.
Response B Score: 3
Analysis of Response B: Response B offers a slightly
more nuanced view of Tony Stark’s motivations, focusing
on his desire to make a difference and protect the vulner-
able. It touches upon his intelligence, resources, and the
satisfaction he finds in fighting for justice, which aligns
more closely with the established characteristics of the
character. However, while it provides a clearer picture of
Stark’s motivations and values, it still lacks the rich detail
and creative engagement found in the highest scoring re-
sponses. ...
Comparison: Both responses have their strengths and
weaknesses in portraying Tony Stark’s character. Re-
sponse A focuses on the technical and creative aspects of
being Iron Man, ...Response A Score: 3
Analysis of Response A: Response A moderately satisfies
the criteria by embodying Tony Stark’s persona, focusing
on his love for technology and innovation. ... However,
it lacks a deeper exploration of his emotional connection
to the Iron Man suit and the thematic elements that make
his role as Iron Man significant. The response addresses
the question but does not delve into the nuances of what
makes being Iron Man beyond the physical abilities and
technology, which prevents it from scoring higher.
Response B Score: 4
Analysis of Response B: Response B effectively embodies
Tony Stark’s character by highlighting his intelligence,
resourcefulness, and the emotional satisfaction he derives
from using his abilities to protect the world. ... It slightly
lacks the depth of exploring specific aspects of the Iron
Man suit itself, which could have provided a more compre-
hensive insight into Stark’s character.
Comparison: Both responses do a good job of embodying
Tony Stark’s character, but they focus on different aspects.
Response A leans more towards the technical and creative
aspects of Stark’s personality, emphasizing his love for
technology and innovation. Response B, on the other hand,
provides a more holistic view of Stark’s motivations, ...
Table 11: An example of pairwise response comparison on role-playing scenario from MT-Bench.
17Query
Discuss antitrust laws and their impact on market competition. Compare the antitrust laws in US and China along
with some case studies."
Response A (from Alpaca-13B) Response B (Preferred , from Vicuna-13B-v1.2)
Antitrust laws are designed to promote competition in mar-
kets and protect consumers from anticompetitive practices.
In the US, the Sherman Antitrust Act of 1890 is the pri-
mary antitrust law and prohibits agreements or practices
that restrain trade and impose unfair competition. The US
Department of Justice and the Federal Trade Commission
are responsible for enforcing the law. In China, the Anti-
Monopoly Law enacted in 2008 is the primary antitrust
law and is similar to the Sherman Act in prohibiting mo-
nopolistic agreements and abuse of dominance. Case stud-
ies have shown that antitrust laws can have a significant
impact on market competition. For example, in 2000 the
US Department of Justice filed a lawsuit against Microsoft
alleging that the company had abused its monopoly in the
computer operating system market. The lawsuit resulted
in Microsoft being ordered to make significant changes
to its business practices and the case is often cited as an
example of how antitrust laws can be used to promote
competition. In China, ... . Overall, antitrust laws can
have a significant impact on market competition and have
been used to promote competition and protect consumers
in both the US and China.Antitrust laws are a set of regulations designed to promote
and maintain fair competition in markets. These laws aim
to prevent anticompetitive practices that can harm con-
sumers, such as price-fixing, monopolies, and mergers
that reduce competition. In this essay, we will discuss
antitrust laws and their impact on market competition,
compare the antitrust laws in the US and China, and ana-
lyze some case studies. Antitrust laws have a significant
impact on market competition. These laws promote com-
petition by preventing anticompetitive practices that can
harm consumers. For example, antitrust laws prohibit
price-fixing, which is when competitors agree to set prices
at a certain level. This practice can lead to higher prices
for consumers and reduced competition. Antitrust laws
also prevent monopolies, which occur when a single com-
pany dominates a market. Monopolies can lead to higher
prices, reduced innovation, and poorer quality products.
Antitrust laws also regulate mergers and acquisitions to
ensure that they do not reduce competition in the market.
The US has a comprehensive set of antitrust laws, includ-
ing the Sherman Act, the Clayton Act, and the Federal
Trade Commission Act. The Sherman Act prohibits anti-
competitive practices such as price-fixing, ...
Judgment by GPT-4 (✗)
Both assistants address the core aspects of antitrust laws, their impact on market competition, and provide compar-
isons between the US and China along with case studies. However, there are distinct differences in the presentation,
depth, and accuracy of their responses. Assistant A Pros: 1. Clearly explains the purpose of antitrust laws and
names specific laws in both the US and China. 2. Provides accurate and specific case studies: the 2000 Microsoft
case in the US and the Qualcomm case in China in 2013. 3. Offers a concise conclusion summarizing the impact of
antitrust laws. Assistant A Cons: ... Therefore, the final verdict is: [[A]]
Criteria from Our DPO Model Criteria from Our Initial Model
1.Depth of Analysis: The response should not only
state facts but also analyze the implications of antitrust
laws, including their effects on market competition,
economic efficiency, and consumer welfare.
2. Practical Examples: The inclusion of real-world exam-
ples or case studies to illustrate how antitrust laws have
been applied in the US and China, enhancing the under-
standing of the topic.
...1.Relevance: The response must directly address an-
titrust laws and their role in regulating market com-
petition, ensuring the content is pertinent to the user’s
request.
2. Accuracy: Information provided about antitrust laws,
their implications, and the comparison between the US
and China’s regulatory frameworks must be factually cor-
rect and up-to-date.
...
Judgement by Our DPO Model (Criteria 1 )(✓) Judgement by Our Initial Model (Criteria 1 )(✗)
Response A Score: 3
Analysis of Response A: Response A provides a basic
overview ... However, the response lacks depth in its anal-
ysis and does not provide a detailed comparison between
the antitrust laws of the two countries beyond a general
description. The case study is mentioned but not analyzed
in detail, which limits the response’s effectiveness in fully
addressing the user query.
Response B Score: 4
Analysis of Response B: Response B offers a more de-
tailed analysis of antitrust laws, including a discussion on
the impact of these laws on market competition and a com-
parison between the US and Chinese antitrust frameworks.
It provides specific examples of laws and regulations, such
as the Sherman Act, the Clayton Act, and the Federal
Trade Commission Act in the US, ...
Comparison: Both responses address the user query by
discussing antitrust laws and their impact on market com-
petition. Response B is stronger because ...Response A Score: 4
Analysis of Response A: Analysis of Response A: Re-
sponse A provides a clear and concise overview ... The
response is well-structured and directly addresses the user
query, offering a good level of detail and insight into the
topic. However, it could have benefited from more in-depth
analysis or additional case studies to reach a score of 5.
Response B Score: 3
Analysis of Response B: Response B offers a general
discussion on antitrust laws, their impact on market com-
petition, and a comparison between the US and China.
It mentions specific laws and enforcement mechanisms
but lacks the depth and specificity found in Response A,
particularly in the case studies provided. ...
Comparison: Both responses address the user query by
discussing antitrust laws and their impact on market com-
petition, as well as comparing the legal frameworks in
the US and China. Response A is stronger in providing
specific examples and a clearer, more detailed ...
Table 12: An example of pairwise response comparison on humanities scenario from MT-Bench.
18NeurIPS Paper Checklist 499
1.Claims 500
Question: Do the main claims made in the abstract and introduction accurately reflect the 501
paper’s contributions and scope? 502
Answer: [Yes] 503
Justification: The key experiment results are summarized in the end of the introduction. 504
principal contributions are clearly stated in the end of the abstract. 505
Guidelines: 506
•The answer NA means that the abstract and introduction do not include the claims 507
made in the paper. 508
•The abstract and/or introduction should clearly state the claims made, including the 509
contributions made in the paper and important assumptions and limitations. A No or 510
NA answer to this question will not be perceived well by the reviewers. 511
•The claims made should match theoretical and experimental results, and reflect how 512
much the results can be expected to generalize to other settings. 513
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 514
are not attained by the paper. 515
2.Limitations 516
Question: Does the paper discuss the limitations of the work performed by the authors? 517
Answer: [Yes] 518
Justification: The limitations of this study are discussed in Section 6.1. 519
Guidelines: 520
•The answer NA means that the paper has no limitation while the answer No means that 521
the paper has limitations, but those are not discussed in the paper. 522
• The authors are encouraged to create a separate "Limitations" section in their paper. 523
•The paper should point out any strong assumptions and how robust the results are to 524
violations of these assumptions (e.g., independence assumptions, noiseless settings, 525
model well-specification, asymptotic approximations only holding locally). The authors 526
should reflect on how these assumptions might be violated in practice and what the 527
implications would be. 528
•The authors should reflect on the scope of the claims made, e.g., if the approach was 529
only tested on a few datasets or with a few runs. In general, empirical results often 530
depend on implicit assumptions, which should be articulated. 531
•The authors should reflect on the factors that influence the performance of the approach. 532
For example, a facial recognition algorithm may perform poorly when image resolution 533
is low or images are taken in low lighting. Or a speech-to-text system might not be 534
used reliably to provide closed captions for online lectures because it fails to handle 535
technical jargon. 536
•The authors should discuss the computational efficiency of the proposed algorithms 537
and how they scale with dataset size. 538
•If applicable, the authors should discuss possible limitations of their approach to 539
address problems of privacy and fairness. 540
•While the authors might fear that complete honesty about limitations might be used by 541
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 542
limitations that aren’t acknowledged in the paper. The authors should use their best 543
judgment and recognize that individual actions in favor of transparency play an impor- 544
tant role in developing norms that preserve the integrity of the community. Reviewers 545
will be specifically instructed to not penalize honesty concerning limitations. 546
3.Theory Assumptions and Proofs 547
Question: For each theoretical result, does the paper provide the full set of assumptions and 548
a complete (and correct) proof? 549
Answer: [NA] 550
19Justification: This study does not include theoretical results. 551
Guidelines: 552
• The answer NA means that the paper does not include theoretical results. 553
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 554
referenced. 555
•All assumptions should be clearly stated or referenced in the statement of any theorems. 556
•The proofs can either appear in the main paper or the supplemental material, but if 557
they appear in the supplemental material, the authors are encouraged to provide a short 558
proof sketch to provide intuition. 559
•Inversely, any informal proof provided in the core of the paper should be complemented 560
by formal proofs provided in appendix or supplemental material. 561
• Theorems and Lemmas that the proof relies upon should be properly referenced. 562
4.Experimental Result Reproducibility 563
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 564
perimental results of the paper to the extent that it affects the main claims and/or conclusions 565
of the paper (regardless of whether the code and data are provided or not)? 566
Answer: [Yes] 567
Justification: All implementation details are provided in Appendix A.2. 568
Guidelines: 569
• The answer NA means that the paper does not include experiments. 570
•If the paper includes experiments, a No answer to this question will not be perceived 571
well by the reviewers: Making the paper reproducible is important, regardless of 572
whether the code and data are provided or not. 573
•If the contribution is a dataset and/or model, the authors should describe the steps taken 574
to make their results reproducible or verifiable. 575
•Depending on the contribution, reproducibility can be accomplished in various ways. 576
For example, if the contribution is a novel architecture, describing the architecture fully 577
might suffice, or if the contribution is a specific model and empirical evaluation, it may 578
be necessary to either make it possible for others to replicate the model with the same 579
dataset, or provide access to the model. In general. releasing code and data is often 580
one good way to accomplish this, but reproducibility can also be provided via detailed 581
instructions for how to replicate the results, access to a hosted model (e.g., in the case 582
of a large language model), releasing of a model checkpoint, or other means that are 583
appropriate to the research performed. 584
•While NeurIPS does not require releasing code, the conference does require all submis- 585
sions to provide some reasonable avenue for reproducibility, which may depend on the 586
nature of the contribution. For example 587
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 588
to reproduce that algorithm. 589
(b)If the contribution is primarily a new model architecture, the paper should describe 590
the architecture clearly and fully. 591
(c)If the contribution is a new model (e.g., a large language model), then there should 592
either be a way to access this model for reproducing the results or a way to reproduce 593
the model (e.g., with an open-source dataset or instructions for how to construct 594
the dataset). 595
(d)We recognize that reproducibility may be tricky in some cases, in which case 596
authors are welcome to describe the particular way they provide for reproducibility. 597
In the case of closed-source models, it may be that access to the model is limited in 598
some way (e.g., to registered users), but it should be possible for other researchers 599
to have some path to reproducing or verifying the results. 600
5.Open access to data and code 601
Question: Does the paper provide open access to the data and code, with sufficient instruc- 602
tions to faithfully reproduce the main experimental results, as described in supplemental 603
material? 604
20Answer: [Yes] 605
Justification: We provide a detailed README to describe how to reproduce the main results 606
of this study. 607
Guidelines: 608
• The answer NA means that paper does not include experiments requiring code. 609
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 610
public/guides/CodeSubmissionPolicy ) for more details. 611
•While we encourage the release of code and data, we understand that this might not be 612
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 613
including code, unless this is central to the contribution (e.g., for a new open-source 614
benchmark). 615
•The instructions should contain the exact command and environment needed to run to 616
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 617
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 618
•The authors should provide instructions on data access and preparation, including how 619
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 620
•The authors should provide scripts to reproduce all experimental results for the new 621
proposed method and baselines. If only a subset of experiments are reproducible, they 622
should state which ones are omitted from the script and why. 623
•At submission time, to preserve anonymity, the authors should release anonymized 624
versions (if applicable). 625
•Providing as much information as possible in supplemental material (appended to the 626
paper) is recommended, but including URLs to data and code is permitted. 627
6.Experimental Setting/Details 628
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 629
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 630
results? 631
Answer: [Yes] 632
Justification: All implementation details are provided in Appendix A.2. 633
Guidelines: 634
• The answer NA means that the paper does not include experiments. 635
•The experimental setting should be presented in the core of the paper to a level of detail 636
that is necessary to appreciate the results and make sense of them. 637
•The full details can be provided either with the code, in appendix, or as supplemental 638
material. 639
7.Experiment Statistical Significance 640
Question: Does the paper report error bars suitably and correctly defined or other appropriate 641
information about the statistical significance of the experiments? 642
Answer: [Yes] 643
Justification: we run every method for 3 independent trials. 644
Guidelines: 645
• The answer NA means that the paper does not include experiments. 646
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 647
dence intervals, or statistical significance tests, at least for the experiments that support 648
the main claims of the paper. 649
•The factors of variability that the error bars are capturing should be clearly stated (for 650
example, train/test split, initialization, random drawing of some parameter, or overall 651
run with given experimental conditions). 652
•The method for calculating the error bars should be explained (closed form formula, 653
call to a library function, bootstrap, etc.) 654
• The assumptions made should be given (e.g., Normally distributed errors). 655
21•It should be clear whether the error bar is the standard deviation or the standard error 656
of the mean. 657
•It is OK to report 1-sigma error bars, but one should state it. The authors should 658
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 659
of Normality of errors is not verified. 660
•For asymmetric distributions, the authors should be careful not to show in tables or 661
figures symmetric error bars that would yield results that are out of range (e.g. negative 662
error rates). 663
•If error bars are reported in tables or plots, The authors should explain in the text how 664
they were calculated and reference the corresponding figures or tables in the text. 665
8.Experiments Compute Resources 666
Question: For each experiment, does the paper provide sufficient information on the com- 667
puter resources (type of compute workers, memory, time of execution) needed to reproduce 668
the experiments? 669
Answer: [Yes] 670
Justification: We include the information of computational resources in Appendix A.2. 671
Guidelines: 672
• The answer NA means that the paper does not include experiments. 673
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 674
or cloud provider, including relevant memory and storage. 675
•The paper should provide the amount of compute required for each of the individual 676
experimental runs as well as estimate the total compute. 677
•The paper should disclose whether the full research project required more compute 678
than the experiments reported in the paper (e.g., preliminary or failed experiments that 679
didn’t make it into the paper). 680
9.Code Of Ethics 681
Question: Does the research conducted in the paper conform, in every respect, with the 682
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 683
Answer: [Yes] 684
Justification: We reviewed the NeurIPS Code of Ethics. 685
Guidelines: 686
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 687
•If the authors answer No, they should explain the special circumstances that require a 688
deviation from the Code of Ethics. 689
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 690
eration due to laws or regulations in their jurisdiction). 691
10.Broader Impacts 692
Question: Does the paper discuss both potential positive societal impacts and negative 693
societal impacts of the work performed? 694
Answer: [Yes] 695
Justification: We discuss the impacts on Section 7. 696
Guidelines: 697
• The answer NA means that there is no societal impact of the work performed. 698
•If the authors answer NA or No, they should explain why their work has no societal 699
impact or why the paper does not address societal impact. 700
•Examples of negative societal impacts include potential malicious or unintended uses 701
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 702
(e.g., deployment of technologies that could make decisions that unfairly impact specific 703
groups), privacy considerations, and security considerations. 704
22•The conference expects that many papers will be foundational research and not tied 705
to particular applications, let alone deployments. However, if there is a direct path to 706
any negative applications, the authors should point it out. For example, it is legitimate 707
to point out that an improvement in the quality of generative models could be used to 708
generate deepfakes for disinformation. On the other hand, it is not needed to point out 709
that a generic algorithm for optimizing neural networks could enable people to train 710
models that generate Deepfakes faster. 711
•The authors should consider possible harms that could arise when the technology is 712
being used as intended and functioning correctly, harms that could arise when the 713
technology is being used as intended but gives incorrect results, and harms following 714
from (intentional or unintentional) misuse of the technology. 715
•If there are negative societal impacts, the authors could also discuss possible mitigation 716
strategies (e.g., gated release of models, providing defenses in addition to attacks, 717
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 718
feedback over time, improving the efficiency and accessibility of ML). 719
11.Safeguards 720
Question: Does the paper describe safeguards that have been put in place for responsible 721
release of data or models that have a high risk for misuse (e.g., pretrained language models, 722
image generators, or scraped datasets)? 723
Answer: [No] 724
Justification: This study poses no such risks. 725
Guidelines: 726
• The answer NA means that the paper poses no such risks. 727
•Released models that have a high risk for misuse or dual-use should be released with 728
necessary safeguards to allow for controlled use of the model, for example by requiring 729
that users adhere to usage guidelines or restrictions to access the model or implementing 730
safety filters. 731
•Datasets that have been scraped from the Internet could pose safety risks. The authors 732
should describe how they avoided releasing unsafe images. 733
•We recognize that providing effective safeguards is challenging, and many papers do 734
not require this, but we encourage authors to take this into account and make a best 735
faith effort. 736
12.Licenses for existing assets 737
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 738
the paper, properly credited and are the license and terms of use explicitly mentioned and 739
properly respected? 740
Answer: [NA] 741
Justification: We properly cited all benchmarks used in this study. 742
Guidelines: 743
• The answer NA means that the paper does not use existing assets. 744
• The authors should cite the original paper that produced the code package or dataset. 745
•The authors should state which version of the asset is used and, if possible, include a 746
URL. 747
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 748
•For scraped data from a particular source (e.g., website), the copyright and terms of 749
service of that source should be provided. 750
•If assets are released, the license, copyright information, and terms of use in the 751
package should be provided. For popular datasets, paperswithcode.com/datasets 752
has curated licenses for some datasets. Their licensing guide can help determine the 753
license of a dataset. 754
•For existing datasets that are re-packaged, both the original license and the license of 755
the derived asset (if it has changed) should be provided. 756
23•If this information is not available online, the authors are encouraged to reach out to 757
the asset’s creators. 758
13.New Assets 759
Question: Are new assets introduced in the paper well documented and is the documentation 760
provided alongside the assets? 761
Answer: [Yes] 762
Justification: We provide a new evaluation training dataset. 763
Guidelines: 764
• The answer NA means that the paper does not release new assets. 765
•Researchers should communicate the details of the dataset/code/model as part of their 766
submissions via structured templates. This includes details about training, license, 767
limitations, etc. 768
•The paper should discuss whether and how consent was obtained from people whose 769
asset is used. 770
•At submission time, remember to anonymize your assets (if applicable). You can either 771
create an anonymized URL or include an anonymized zip file. 772
14.Crowdsourcing and Research with Human Subjects 773
Question: For crowdsourcing experiments and research with human subjects, does the paper 774
include the full text of instructions given to participants and screenshots, if applicable, as 775
well as details about compensation (if any)? 776
Answer: [NA] 777
Justification: This study does not involve crowdsourcing nor research with human subjects. 778
Guidelines: 779
•The answer NA means that the paper does not involve crowdsourcing nor research with 780
human subjects. 781
•Including this information in the supplemental material is fine, but if the main contribu- 782
tion of the paper involves human subjects, then as much detail as possible should be 783
included in the main paper. 784
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 785
or other labor should be paid at least the minimum wage in the country of the data 786
collector. 787
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 788
Subjects 789
Question: Does the paper describe potential risks incurred by study participants, whether 790
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 791
approvals (or an equivalent approval/review based on the requirements of your country or 792
institution) were obtained? 793
Answer: [NA] 794
Justification: This study does not involve crowdsourcing nor research with human subjects. 795
Guidelines: 796
•The answer NA means that the paper does not involve crowdsourcing nor research with 797
human subjects. 798
•Depending on the country in which research is conducted, IRB approval (or equivalent) 799
may be required for any human subjects research. If you obtained IRB approval, you 800
should clearly state this in the paper. 801
•We recognize that the procedures for this may vary significantly between institutions 802
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 803
guidelines for their institution. 804
•For initial submissions, do not include any information that would break anonymity (if 805
applicable), such as the institution conducting the review. 806
24