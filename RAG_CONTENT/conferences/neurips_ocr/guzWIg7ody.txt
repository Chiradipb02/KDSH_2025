Nonparametric Classification on Low Dimensional
Manifolds using Overparameterized
Convolutional Residual Networks
Zixuan Zhang∗
Georgia Tech
zzhang3105@gatech.eduKaiqi Zhang∗
UC Santa Barbara
kzhang70@ucsb.edu
Minshuo Chen
Northwestern University
minshuo.chen@northwestern.eduYuma Takeda
University of Tokyo
utklav1511@gmail.com
Mengdi Wang
Princeton University
mengdiw@princeton.eduTuo Zhao
Georgia Tech
tourzhao@gatech.eduYu-Xiang Wang
UC San Diego
yuxiangw@ucsd.edu
Abstract
Convolutional residual neural networks (ConvResNets), though overparameter-
sized , can achieve remarkable prediction performance in practice, which cannot be
well explained by conventional wisdom. To bridge this gap, we study the perfor-
mance of ConvResNeXts trained with weight decay, which cover ConvResNets
as a special case, from the perspective of nonparametric classification. Our analy-
sis allows for infinitely many building blocks in ConvResNeXts, and shows that
weight decay implicitly enforces sparsity on these blocks. Specifically, we consider
a smooth target function supported on a low-dimensional manifold, then prove
that ConvResNeXts can adapt to the function smoothness and low-dimensional
structures and efficiently learn the function without suffering from the curse of
dimensionality. Our findings partially justify the advantage of overparameterized
ConvResNeXts over conventional machine learning models.
1 Introduction
Deep learning has achieved significant success in various real-world applications, such as computer
vision [ 14,23,26], natural language processing [ 2,15,42], and robotics [ 16]. One notable example of
this is in the field of image classification, where the winner of the 2017 ImageNet challenge achieved
a top-5 error rate of just 2.25% [ 19] using Convolutational Residual Network (ConvResNets) on a
training dataset of 1 million labeled high-resolution images in 1000 categories.
Researchers have attributed the remarkable performance of deep learning to its great flexibility in
modeling complex functions, which has motivated many works on investigating the representation
power of deep neural networks. For instance, early work such as Barron [3], Cybenko [7], Kohler
and Krzy ˙zak[22] initialized this line of research for simple feedforward neural networks (FNNs)
[19,20,38,37]. More recently, Suzuki [32], Yarotsky [41] gave more precise bounds on the model
sizes in terms of the approximation error, and Oono and Suzuki [30] further established a bound
for more advanced architectures – ConvResNets. Based on these function approximation theories,
∗Equal contribution.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).one can further establish generalization bounds of deep neural networks with finite samples. Taking
Oono and Suzuki [30] as an example again, they showed that ConvResNets with ˜O(nD/(2α+D))
parameters can achieve a minimax optimal convergence rate ˜O(n−2α/(2α+D))while approximating
aCαnonparametric regression function with nsamples. Unfortunately, these theoretical results
cannot well explain the empirical successes of deep learning well, as they require the model size
to be no larger than ˜O(n)(the generalization bounds become vacuous otherwise). However, in real
applications, practical deep learning models are often overparmameterized, that is the model size can
greatly exceeds the sample size.
1.1 Main Results
Overparameterization of neural networks has been considered as one of the most fundamental
research problems in deep learning theories, where parameters can significantly exceed training
samples. There has been substantial empirical evidence showing that overparameterization can help
fit the training data, ease the challenging nonconvex optimization, and gain robustness. However,
existing literature on deep learning theories under such an overparameterized regime is very limited.
To the best of our knowledge, we are only aware of Zhang and Wang [43], which attempts to analyze
overparameterized neural networks trained with weight decay. However, their work still suffers from
two major restrictions: (1) They consider parallel FNN, which is rarely used in practice. Whether
similar results hold for more practical architectures remains unclear; (2) Their generalization bound
from the curse of dimensionality, where the sample size is require to scale exponentially with the
input dimension.
To address (1), we propose to develop a new theory for nonparametric classification using overparam-
eterized ConvResNeXts trained with weight decay [ 40]. The ConvResNeXt generalizes ConvResNets
and includes them as a special case [ 5,18,33,44]. Compared with FNNs, ConvResNeXts exhibit
three features: (i) Instead of using dense weight matrices, they use convolutional filters, which can
naturally investigate the underlying structures of the input data such as images and acoustic signals;
(ii) They are equipped with skip-layer connections, which divides the entire network into blocks.
The skip-layer connection can effectively address the vanishing gradient issue and therefore allow
the networks to be significantly deeper; (iii) They are equipped with parallel architectures, which
enable multiple “paths” within each block of the network, and allows the network to learn a more
diverse set of features. Figure 1b illustrates the structure of ConvResNeXts (detailed introductions of
ConvResNeXts is deferred to Section 2.3). This architecture introduces a significantly more complex
nested function form, presenting us with the challenge of addressing novel issues in bounding the
metric entropy of the function class.
To address (2), our proposed theory considers the optimal classifier is supported on a d-dimensional
smooth manifold Misometrically embedded in RDwithd≪D. The low-dimensional manifold
assumption is highly practical, since it aligns with the inherent nature of many real-world datasets.
For example, images typically represent projections of 3-dimensional objects subject to various
transformations like rotation, translation, and skeletal adjustments. Such a generating mechanism
inherently involves a limited set of intrinsic parameters. More broadly, various forms of data,
including visual, acoustic, and textual, often exhibit low dimensional structures due to rich local
regularities, global symmetries, repetitive patterns, or redundant sampling. It is reasonable to model
these data as samples residing in proximity to a low dimensional manifold.
Our theoretical results can be summarized as follows:
•We prove that when ConvResNeXts are overparameterized, i.e., the number of blocks is larger than
the order of the sample size n, they can still achieve an asymptotic minimax rate for learning Besov
functions when trained with weight decay. That is, given that the target function belongs to the Besov
space Bα
p,q(M)2, the risk of the estimator given by the ConvResNeXt class converges to the optimal
risk at the rate ˜O(n−α/d
2α/d+1(1−o(1)))withnsamples. Notably, the statistical rate of convergence in
our theory only depends on the intrinsic dimension d, which circumvents the curse of dimensionality
in Zhang and Wang [43].
2The Besov space includes functions with spatially heterogeneous smoothness and generalizes more elemen-
tary function spaces such as Sobolev and Hölder spaces.
2•Moreover, our theory shows that one can scale the number of “paths” Min each block with the
depth Nas roughly MN≳n1
2α/d+1, which does not affect the convergence rate. This partially
justifies the flexibility of the ConvResNeXt architecture when designing the bottlenecks, which
simple structures like FNNs cannot achieve. Moreover, we can exchange the number of “paths”
Mand depth Nas long as their product remains the same. This further provides the architectural
insight that we don’t necessarily need parallel blocks when we have residual connections. To say it
differently, we provide new insight into why “residual connection” and "parallel blocks” in ResNeXts
are useful in both approximation and generalization.
•Another technical highlight of our paper is bounding the covering number of weight-decayed
ConvResNeXts, which is essential for computing the critical radius of the local Gaussian complexity.
Specifically, we adopted a more advanced method that leverages the Dudley’s chaining of the metric
entropy [ 4]. This technique provides a tighter bound than choosing a single radius of the covering
number as in Zhang and Wang [43].
•To the best of our knowledge, our work is the first to develop approximation and statistical theories
for ConvResNeXts, as well as overparameterized ConvResNets.
1.2 Related Works
Our work is closely related to Liu et al. [25], which studies nonparametric classification under a
similar setup – the optimal classifier belongs to the Besov space supported on a low dimensional
manifold. Despite they develop similar theoretical results to ours, their analysis does not allow the
model to be overparameterized, and therefore is not applicable to practical neural networks. Moreover,
they investigate ConvResNets, which is a special case of ConvResNeXt in our work.
Our work is closely related to the reproducing kernel methods, which are also often used for
nonparametric regression. However, existing literature has shown that the reproducing kernel methods
lack the adaptivity to handle the heterogeneous smoothness in estimating Besov space functions, and
only achieve suboptimal rate of convergence in statistical estimation [9, 32].
Our work is closely related neural tangent kernel theories [ 21,1], which study overparameterized neu-
ral networks. Specifically, under certain regularity conditions, they establish the equivalence between
overparameterized neural networks and reproducing kernel methods, and therefore the generalization
bounds of overparameterized networks can be derived based on the associated reproducing kernel
Hilbert space. Note that neural tangent kernel theories can be viewed as special cases of the theories
for general reproducing kernel methods. Therefore, they also lack the adaptivity to be successful in
the Besov space thus do not capture the properties of overparameterized neural networks.
2 Preliminaries
In this section, we introduce some concepts on manifolds. Details can be found in [ 35] and [ 24].
Then we provide a detailed definition of the Besov space on smooth manifolds and the ConvResNeXt
architecture.
2.1 Smooth manifold
Firstly, we briefly introduce manifolds, the partition of unity and reach. Let Mbe ad-dimensional
Riemannian manifold isometrically embedded in RDwithdmuch smaller than D.
Definition 2.1 (Chart) .A chart on Mis a pair (U, ϕ)such that U⊂ M is open and ϕ:U7→Rd,
where ϕis a homeomorphism (i.e., bijective, ϕandϕ−1are both continuous).
In a chart (U, ϕ),Uis called a coordinate neighborhood, and ϕis a coordinate system on U.
Essentially, a chart is a local coordinate system on M. A collection of charts that covers Mis called
an atlas of M.
Definition 2.2 (CkAtlas) .ACkatlas for Mis a collection of charts {(Ui, ϕi)}i∈Awhich satisfiesS
i∈AUi=M, and are pairwise Ckcompatible:
ϕi◦ϕ−1
β:ϕβ(Ui∩Uβ)→ϕi(Ui∩Uβ)andϕβ◦ϕ−1
i:ϕi(Ui∩Uβ)→ϕβ(Ui∩Uβ)
are both Ckfor any i, β∈ A. An atlas is called finite if it contains finitely many charts.
3Definition 2.3 (Smooth Manifold) .A smooth manifold is a manifold Mtogether with a C∞atlas.
Classical examples of smooth manifolds are the Euclidean space, the torus, and the unit sphere.
Furthermore, we define Csfunctions on a smooth manifold Mas follows:
Definition 2.4 (Csfunctions on M).LetMbe a smooth manifold and f:M →Rbe a function on
M. A function f:M →RisCsif for any chart (U, ϕ)onM, the composition f◦ϕ−1:ϕ(U)→R
is a continuously differentiable up to order s.
We next define the C∞partition of unity, which is an important tool for studying functions on
manifolds.
Definition 2.5 (Partition of Unity, Definition 13.4 in [ 35]).AC∞partition of unity on a manifold
Mis a collection of C∞functions {ρi}i∈Awithρi:M → [0,1]such that for any x∈ M , there
is a neighbourhood of xwhere only a finite number of the functions in {ρi}i∈Aare nonzero, andX
i∈Aρi(x) = 1 .
An open cover of a manifold Mis called locally finite if every x∈ M has a neighborhood that
intersects with a finite number of sets in the cover. The following proposition shows that a C∞
partition of unity for a smooth manifold always exists.
Proposition 2.6 (Existence of a C∞partition of unity, Theorem 13.7 in [ 35]).Let{Ui}i∈Abe a
locally finite cover of a smooth manifold M. Then there is a C∞partition of unity {ρi}∞
i=1where
every ρihas a compact support such that supp( ρi)⊂Ui.
Let{(Ui, ϕi)}i∈Abe aC∞atlas of M. Proposition 2.6 guarantees the existence of a partition of
unity{ρi}i∈Asuch that ρiis supported on Ui. To characterize the curvature of a manifold, we adopt
the geometric concept: reach.
Definition 2.7 (Reach [12, 29]) .Denote
G=n
x∈RD:∃p̸=q∈ M such that ∥x−p∥2=∥x−q∥2= inf
y∈M∥x−y∥2o
as the set of points with at least two nearest neighbors on M. The closure of Gis called the medial
axis of M. Then the reach of Mis defined as
τ= inf
x∈Minf
y∈G∥x−y∥2.
Reach has a simple geometrical interpretation: for every point x∈ M , the osculating circle’s radius
is at least τ. A large reach for Mindicates that the manifold changes slowly.
2.2 Besov functions on a smooth manifold
We next define the Besov function space on the smooth manifold M, which generalizes more
elementary function spaces such as the Sobolev and Hölder spaces. Roughly speaking, functions in
the Besov space are only required to have weak derivatives with bounded total variation. Notably, this
includes functions with spatially heterogeneous smoothness, which requires more locally adaptive
methods to achieve optimal estimation errors [ 10]. Examples of Besov class functions include
piecewise linear functions and piecewise quadratic functions that are smoother in some regions and
more wiggly in other regions; see e.g., Figure 2 and Figure 4 of Mammen and van de Geer [27].
To define Besov functions rigorously, we first introduce the modulus of smoothness.
Definition 2.8 (Modulus of Smoothness [ 8,32]).LetΩ⊂RD. For a function f:RD→Rbe in
Lp(Ω)forp >0, ther-th modulus of smoothness of fis defined by
wr,p(f, t) = sup
∥h∥2≤t∥∆r
h(f)∥Lp,
where ∆r
h(f)(x) =

rP
j=0 r
j
(−1)r−jf(x+jh)ifx,x+rh∈Ω,
0 otherwise .
4Definition 2.9 (Besov Space Bα
p,q(Ω)).For0< p, q ≤ ∞, α > 0, r=⌊α⌋+ 1, define the seminorm
| · |Bαp,qas
|f|Bαp,q(Ω):=

Z∞
0(t−αwr,p(f, t))qdt
t1
q
ifq <∞,
supt>0t−αwr,p(f, t) ifq=∞.
The norm of the Besov space Bs
p,q(Ω)is defined as ∥f∥Bαp,q(Ω):=∥f∥Lp(Ω)+|f|Bαp,q(Ω). Then the
Besov space is defined as Bα
p,q(Ω) = {f∈Lp(Ω)|∥f∥Bαp,q<∞}.
Moreover, we show that functions in the Besov space can be decomposed using B-spline basis
functions in the following proposition.
Proposition 2.10 (Decomposition of Besov functions) .Any function fin the Besov space Bα
p,q, α >
d/pcan be decomposed using B-spline of order m, m > α : for any x∈Rd, we have
f(x) =∞X
k=0X
s∈J(k)ck,s(f)Mm,k,s(x), (1)
where J(k) :={2−ks:s∈[−m,2k+m]d⊂Zd},Mm,k,s(x) :=Mm(2k(x−s)), and Mk(x) =Qd
i=1Mk(xi)is the cardinal B-spline basis function which can be expressed as a polynomial:
Mm(z) =1
m!m+1X
j=1(−1)jm+ 1
j
(z−j)m
+. (2)
We next define Bα
p,qfunctions on M.
Definition 2.11 (Bα
p,qFunctions on M[13,34]).LetMbe a compact smooth manifold of dimension
d. Let{(Ui, ϕi)}CM
i=1be a finite atlas on Mand{ρi}CM
i=1be a partition of unity on Msuch that
supp( ρi)⊂Ui. A function f:M →Ris inBα
p,q(M)if
∥f∥Bαp,q(M):=CMX
i=1∥(fρi)◦ϕ−1
i∥Bαp,q(Rd)<∞. (3)
Since ρiis supported on Ui, the function (fρi)◦ϕ−1
iis supported on ϕ(Ui). We can extend (fρi)◦ϕ−1
i
from ϕ(Ui)toRdby setting the function to be 0onRd\ϕ(Ui). The extended function lies in the
Besov space Bs
p,q(Rd)[34, Chapter 7].
2.3 Architecture of ConvResNeXt
We introduce the architecture of ConvResNeXts. ConvResNeXts have three main features: convolu-
tion kernel, residual connections, and parallel architecture.
Consider one-sided stride-one convolution in our network. Let W={Wj,k,l} ∈Rw′×K×wbe
a convolution kernel with output channel size w′, kernel size Kand input channel size w. For
z∈RD×w, the convolution of Wwithzgives y∈RD×w′such that
y=W⋆z, y i,j=KX
k=1wX
l=1Wj,k,lzi+k−1,l, (4)
where 1≤i≤D,1≤j≤w′and we set zi+k−1,l= 0 fori+k−1> D , as demonstrated in
Figure 1a.
The building blocks of ConvResNeXts are residual blocks. Given an input x, each residual block
computes x+F(x), where Fis a subnetwork called bottleneck, consisting of convolutional layers.
In ConvResNeXts, a parallel architecture is introduced to each building block, which enables multiple
“paths” in each block. In this paper, we study the ConvResNeXts with rectified linear unit (ReLU)
activation function, i.e., σ(z) = max {z,0}. We next provide the detailed definition of ConvResNeXts
as follows:
5ax
+
+
+
f(x)ididid f1,1. . .f1,M
fN,1. . .
fN,M. . .
b
Figure 1: (a) Demonstration of the convolution operation W ∗z, where the input is z∈RD×w,
and the output is W ∗z∈RD×w′. Here Wj,:,:is aD×wmatrix for the j-th output channel. (b)
Demonstration of the ConvResNeXt. f1,1. . . f N,M are the building blocks, each building block is a
convolution neural network.
Definition 2.12. Let the neural network comprise Nresidual blocks, each residual block has a
parallel architecture with Mbuilding blocks, and each building block contains Llayers. The number
of channels is w, and the convolution kernel size is K. Given an input x∈RD, a ConvResNeXt with
ReLU activation function can be represented as
f(x)=WoutMX
m=1fN,m+ id
◦···◦MX
m=1f1,m+ id
◦P(x),
fn,m=W(n,m)
L⋆ σ
W(n,m)
L−1⋆···⋆σ
W(n,m)
1 ⋆x
,(5)
where idis the identity operator, P:RD→RD×w0is the padding operator satisfying P(x) =
[x,0. . .0]∈RD×w,{W(n,m)
l}L
l=1is a collection of convolution kernels for n= 1, . . . , N, m =
1, . . . , M ,Wout∈RwLdenotes the linear operator for the last layer, and ⋆is the convolution
operation defined in (4).
The structure of ConvResNeXts is shown in Figure 1b. When M= 1, the ConvResNeXt defined in
(5) reduces to a ConvResNet. For notational simplicity, we omit biases in the neural network structure
by extending the input dimension and padding the input with a scalar 1 (See Proposition F.4 for more
details). The channel with 0’s is used to accumulate the output.
3 Theory
In this section, we study a binary classification problem on M ⊆ [−1,1]D. Specifically, we are given
i.i.d. samples {xi, yi}n
i=1∼ D where xi∈ M andyi∈ {0,1}is the label. The label y∈ {0,1}
follows the Bernoulli-type distribution
P(y|x) =exp(yf∗(x))
1 + exp( f∗(x))
for some f∗:M →Rbelonging to the Besov space. More specifically, we make the following
assumption on f∗.
Assumption 3.1. Let0< p, q ≤ ∞ ,d/p < α < ∞. Assume f∗∈Bα
p,q(M)and∥f∗∥Bαp,q(M)≤
CFfor some constant CF>0.
To learn f∗, we minimize the empirical logistic risk over the training data:
ˆf= arg min
f∈FConv1
nnX
i=1
yilog(1 + exp( −f(xi))) + (1 −yi) log(1 + exp( f(xi)))
, (6)
where FConvis some neural network class specified later. For notational simplicity, we denote the
empirical logistic risk function in (6) as Ln(f), and denote the population logistic risk as
ED[L(f)] =E(x,y)∼D
ylog(1 + exp( −f(x))) + (1 −y) log(1 + exp( f(x)))
.
6We next specify the class of ConvResNeXts for learning f∗:
FConv(N, M, L, K, w, B res, Bout) =n
f|fis in the form of (5) with Nresidual blocks. Every
residual block has Mbuilding blocks with each building block containing Llayers.
Each layer has kernel size bounded by K,number of channels bounded by w.
NX
n=1MX
m=1LX
ℓ=1∥W(n,m)
ℓ∥2
F≤Bres,∥Wout∥2
F≤Bout, f(x)∈[0,1]for any x∈ M.o
.(7)
Note that the hyperparameters of FConvwill be specified in our theoretical analysis later.
As can be seen, FConvcontains the Frobenius norm constraints of the weights. For the sake of com-
putational convenience in practice, such constraints can be replaced with weight decay regularization
the residual blocks and the last fully connected layer separately. More specifically, we can use the
following alternative formulation:
˜f= arg min
f∈FConv(N,M,L,K,w, ∞,∞)Ln(f) +λ1NX
n=1MX
m=1LX
ℓ=1∥W(n,m)
ℓ∥2
F+λ2∥Wout∥2
F,
where λ1, λ2>0are properly chosen regularization parameters.
3.1 Approximation theory
In this section, we provide a universal approximation theory of ConvResNeXts for Besov functions
on a smooth manifold:
Theorem 3.2. For any Besov function f0on a smooth manifold satisfying p, q≥1, α−d/p > 1,
∥f0∥Bαp,q(M)≤CF,
for any P > 0and any ConvResNeXt class FConv(N, M, L, K, w, B res, Bout)satisfying L=
L′+L0−1, L′≥3, where L0=⌈D
K−1⌉, and
MN≥CMP, w≥C1(dm+D), Bres≤C2L/K, B out≤C3C2
F((dm+D)LK)L(CMP)L−2/p,
there exists f∈ FConv(N, M, L, K, w, B res, Bout)such that
∥f−f0∥∞≤CFCM
C4P−α/d+C5exp(−C6L′logP)
, (8)
where C1, C2, C3are universal constants and C4, C5, C6are constants that only depends on d
andm,dis the intrinsic dimension of the manifold and mis an integer satisfying 0< α <
min(m, m−1 + 1 /p).
The approximation error of the network is bounded by the sum of two terms. The first term is a
polynomial decay term that decreases with the size of the neural network and represents the trailing
term of the B-spline approximation. The second term reflects the approximation error of neural
networks to piecewise polynomials, decreasing exponentially with the number of layers. The proof is
deferred to Section 4.1 and the appendix.
3.2 Estimation theory
Theorem 3.3. Suppose Assumption 3.1 holds. Set L=L′+L0−1, L′≥3, where L0=⌈D
K−1⌉,
and
MN≥CMP, P =O(n1−2/L
2α/d(1−1/L)+1−2/pL), w≥C1(dm+D).
Let ˆfbe the global minimizer given in (6) with the function class F =
FConv(N, M, L, K, w, B res, Bout). Then we have
ED[L(ˆf(x), y)]−ED[L(f∗(x), y)]≤C7K−2
L−2w3L−4
L−2L3L−2
L−2
n α/d(1−2/L)
2α/d(1−1/L)+1−2/(pL)
+C8exp(−C6L′),
where the logarithmic terms are omitted. C1is the constant defined in Theorem 3.2, C7, C8are
constants that depend on CF, CM, d, m ,Kis the size of the convolution kernel.
7We would like to make the following remarks about the results:
•Strong adaptivity: By setting the width of the neural network to w= 2C1D, the model can adapt
to any Besov functions on any smooth manifold, provided that dm≤D. This remarkable flexibility
can be achieved simply by tuning the regularization parameter. The cost of overestimating the width
is a slight increase in the estimation error.
•No curse of dimensionality: the above error rate only depends polynomially on the ambient
dimension Dand exponentially on the hidden dimension d. Since in real data, the hidden dimension
dcan be much smaller than the ambient dimension D, this result shows that neural networks can
explore the low-dimension structure of data to overcome the curse of dimensionality.
•Overparameterization is fine: the number of building blocks in a ConvResNeXt does not
influence the estimation error as long as it is large enough. In other words, this matches the empirical
observations that neural networks generalize well despite overparameterization.
•Close to minimax rate: The lower bound of the 1-Lipschitz error for any estimator θis
min
θmax
f∗∈Bαp,qL(θ(D), f∗)≳n−α/d
2α/d+1.
where≳notation hides a factor of constant. The proof can be found in Appendix E. Comparing to
the minimax rate, we can see that as L→ ∞ , the above error rate converges to the minimax rate up to
a constant term. In other words, overparameterized ConvResNeXt can achieve close to the minimax
rate in estimating functions in Besov class. In comparison, all kernel ridge regression including any
NTKs will have a suboptimal rate lower bounded by2α−d
2α, which is suboptimal.
•Deeper is better: with larger L, the error rate decays faster and gets closer to the minimax rate.
This indicates that deeper model can achieve better performance than shallower models.
•Tradeoff between width and depth: With a fixed budget in the number of parameters, the tradeoff
between width and depth is crucial for achieving the best performance, and this often requires
repeated, time-consuming experiments. On the other hand, our results suggests that such a tradeoff
less important in a ResNeXt. The lower bound of error does not depend on the arrangements of the
residual blocks MandN, as long as their product is large enough. This can partly explain the benefit
of ResNeXt over other architecture.
By choosing L=O(log(n))in Theorem 3.3, the second term in the error can be merged with the
first term, and close to the minimax rate can be achieved:
Corollary 3.4. Given the conditions in Theorem 3.3, set the depth of each block is L=O(log(n))
and then the estimation error of the empirical risk minimizer ˆfsatisfies
ED[L(ˆf(x), y)]≤ED[L(f∗)] +˜O(n−α/d
2α/d+1(1−o(1))),
where ˜O(·)omits the logarithmic term.
The proof of Theorem 3.3 is deferred to Section 4.2 and Section D.2. The key technique is computing
the critical radius of the local Gaussian complexity by bounding the covering number of weight-
decayed ConvResNeXts. This technique provides a tighter bound than choosing a single radius
of the covering number as in Suzuki [32], Zhang and Wang [43]. The covering number of an
overparameterized ConvResNeXt with norm constraint (Lemma 4.1) is one of our key contributions.
4 Proof overview
4.1 Approximation error
We follow the method in Liu et al. [25] to construct a neural network that achieves the approximation
error we claim. It is divided into the following steps:
•Step 1: Decompose the target function into the sum of locally supported functions.
In this work, we adopt a similar approach to [ 25] and partition Musing a finite number of open
balls on RD. Specifically, we define B(ci, r)as the set of unit balls with center ciand radius rsuch
that their union covers the manifold of interest, i.e., M ⊆ ∪CM
i=1B(ci, r). This allows us to partition
8the manifold into subregions Ui=B(ci, r)∩ M , and further decompose a smooth function on the
manifold into the sum of locally supported smooth functions with linear projections. The existence of
function decomposition is guaranteed by the existence of partition of unity stated in Proposition 2.6.
See Section C.1 for the detail.
•Step 2: Locally approximate the decomposed functions using cardinal B-spline basis functions.
In the second step, we decompose the locally supported Besov functions achieved in the first step
using B-spline basis functions. The existence of the decomposition was proven by D ˜ung[11], and
was applied in a series of works [ 43,32,25]. The difference between our result and previous work is
that we define a norm on the coefficients and bound this norm, instead of bounding the maximum
value. The detail is deferred to Section C.2.
•Step 3: Approximate the polynomial functions using neural networks. In this section, we follow
the method in Zhang and Wang [43], Suzuki [32], Liu et al. [25] and show that neural networks can
be used to approximate polynomial functions, including B-spline basis functions and the distance
function. The key technique is to use a neural network to approximate square function and multiply
function [ 3]. The detail is deferred to the appendix. Specifically, Lemma F.3 proves that a neural
network with width w=O(dm)and depth Lcan approximate B-spline basis functions, and the error
decreases exponentially with L; Similarly, Proposition C.3 shows that a neural network with width
w=O(D)can approximately calculate the distance between two points d2(x;c), with precision
decreasing exponentially with the depth.
•Step 4: Use a ConvResNeXt to Approximate the target function. Using the results above, the
target function can be (approximately) decomposed as
CMX
i=1PX
j=1ai,kj,sjMm,kj,sj◦ϕi×1(x∈B(ci, r)). (9)
We first demonstrate that a ReLU neural network taking two scalars a, bas the input, denoted as
a˜×b, can approximate y×1(x∈Br,i), where ˜×satisfy that y˜×1 =yfor all y, andy˜×˜x= 0if any
ofxoryis 0, and the soft indicator function ˜1(x∈Br,i)satisfy ˜1(x∈Br,i) = 1 when x∈Br,i,
and˜1(x∈Br,i) = 0 when x /∈Br+∆,i. The detail is deferred to Section C.3.
Then, we show that it is possible to construct MN =CMPnumber of building blocks, such
that each building block is a feedforward neural network with width C1(md+D)and depth L,
where mis an integer satisfying 0< α < min (m, m−1 + 1 /p). The k-th building block (the
position of the block does not matter) approximates ai,kj,sjMm,kj,sj◦ϕi×1(x∈B(ci, r)),where
i=ceiling (k/N), j=rem(k, N). Each building block has where a sub-block with width D
and depth L−1approximates the chart selection, a sub-block with width mdand depth L−1
approximates the B-spline function, and the last layer approximates the multiply function. The norm
of this block is bounded by
LX
ℓ=1∥W(i,j)
ℓ∥2
F≤O(22k/LdmL +DL). (10)
Making use of the 1-homogeneous property of the ReLU function, by scaling all the weights in
the neural network, these building blocks can be combined into a neural network with residual
connections, that approximate the target function and satisfy our constraint on the norm of weights.
See Section C.4 for the detail.
By applying Lemma C.6, which shows that any L-layer feedforward neural network can be reformu-
lated as an L+L0−1-layer convolution neural network, the neural network constructed above can
be converted into a ConvResNeXt that satisfies the conditions in Theorem 3.2.
4.2 Estimation error
We first prove the covering number of an overparameterized ConvResNeXt with norm-constraint as
in Lemma 4.1, then compute the critical radius of this function class using the covering number as in
Corollary F.5. The critical radius can be used to bound the estimation error as in Theorem 14.20 in
Wainwright [36]. The proof is deferred to Section D.2.
Lemma 4.1. Consider a neural network defined in Definition 2.12. Let the last layer of this neural
network is a single linear layer with norm ∥Wout∥2
F≤Bout. Let the input of this neural network
9satisfy ∥x∥2≤1,∀x, and is concatenated with 1 before feeding into this neural network so that part
of the weight plays the role of the bias. The covering number of this neural network is bounded by
logN(·, δ)≲w2LB1
1−2/L
res K2−2/L
1−2/L· 
B1/2
outexp(( KBres/L)L/2)2/L
1−2/Lδ−2/L
1−2/L, (11)
where the logarithmic term is omitted.
The key idea of the proof is to split the building block into two types (“small blocks” and “large
blocks”) depending on whether the total norm of the weights in the building block is smaller than ϵ.
By properly choosing ϵ, we prove that if all the “small blocks” in this neural network are removed,
the perturbation to the output for any input ∥x∥ ≤1is no more than δ/2, so the covering number of
the ConvResNeXt is only determined by the number of “large blocks”, which is no more than Bres/ϵ.
Proof. Using the inequality of arithmetic and geometric means, from Proposition F.6, Proposition F.8
and Proposition F.9, if any residual block is removed, the perturbation to the output is no more than
pm:= (KBm/L)L/2B1/2
outexp(( KBres/L)L/2),
where Bmis the total norm of parameters in this block. Because of that, the residual blocks can be
divided into two kinds depending on the norm of the weights Bm< ϵ(“small blocks”) and Bm≥ϵ
(“large blocks”). If all the “small blocks” are removed, the perturbation to the output for any input
∥x∥2≤1is no more than
X
m:Bm<ϵpm≤exp(( KBres/L)L/2)KL/2BresB1/2
out(ϵ/L)L/2−1/L.
Choosing ϵ=L
δL
2 exp(( KBres/L)L/2)KL/2BresB1/2
out1/(L/2−1)
,the perturbation above is no more than
δ/2. The covering number can be determined by the number of the “large blocks” in the neural
network, which is no more than Bres/ϵ. As for any block, BinLpost≤B1/2
outexp(( KBres/L)L/2),
taking our chosen ϵfinishes the proof, where Binis the upper bound of the input to this block defined
in Proposition D.1, and Lpostis the Lipschitz constant of all the layers following the block.
Remark 4.2.The proof of Lemma 4.1 shows that under weight decay, the building blocks in
a ConvResNeXt are sparse, i.e. only a finite number of blocks contribute non-trivially to the
network even though the model can be overparameterized. This explains why a ConvResNeXt
can generalize well despite overparameterization, and provide a new perspective in explaining why
residual connections improve the performance of deep neural networks.
5 Discussions
This paper focuses on developing insightful generalization bounds for the regularized empirical risk
minimizer. We opt not to delve into the end-to-end analysis of optimization algorithms in order to
explore the adaptivity of complex architectures such as ConvResNeXts, while works on optimization
behaviour of neural networks are limited to simple network structures [ 28,39]. Notably, this approach
to decouple learning and optimization has been widely adopted [ 3,17,31,6,25]. We made the same
choice in the interest of getting a more fine-grained learning theory. However, our paper considers
weight decay and overparameterization which are tightly connected to real-world training of neural
networks, and can be the most promising work to bridge the gap between optimization and statistical
guarantees. We defer more details to the appendix, including discussions on the Besov space and
numerical experiments for supporting our theories as well as supplementary technical proof.
Acknowledgments
The work was partially supported by NSF Award No 2134214. KZ and YW were with UCSB and
MC was with Princeton when the work was completed. YT contributed to the project during his
summer 2023 visit to UCSB. We appreciate anonymous reviewers and ACs for their input.
10References
[1]Z. Allen-Zhu, Y . Li, and Z. Song. A convergence theory for deep learning via over-
parameterization. In International conference on machine learning , pages 242–252. PMLR,
2019.
[2]D. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align
and translate. arXiv preprint arXiv:1409.0473 , 2014.
[3]A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory , 39(3):930–945, 1993.
[4]P. L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Annals of
Statistics , pages 1497–1537, 2005.
[5]L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic
image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
IEEE transactions on pattern analysis and machine intelligence , 40(4):834–848, 2017.
[6]M. Chen, H. Jiang, W. Liao, and T. Zhao. Nonparametric regression on low-dimensional mani-
folds using deep relu networks: Function approximation and statistical recovery. Information
and Inference: A Journal of the IMA , 11(4):1203–1253, 2022.
[7]G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems , 2(4):303–314, 1989.
[8]R. A. DeV ore and G. G. Lorentz. Constructive approximation , volume 303. Springer Science &
Business Media, 1993.
[9]D. L. Donoho, R. C. Liu, and B. MacGibbon. Minimax risk over hyperrectangles, and implica-
tions. The Annals of Statistics , pages 1416–1437, 1990.
[10] D. L. Donoho, I. M. Johnstone, et al. Minimax estimation via wavelet shrinkage. The annals of
Statistics , 26(3):879–921, 1998.
[11] D. D ˜ung. Optimal adaptive sampling recovery. Advances in Computational Mathematics , 34
(1):1–41, 2011.
[12] H. Federer. Curvature measures. Transactions of the American Mathematical Society , 93(3):
418–491, 1959.
[13] D. Geller and I. Z. Pesenson. Band-limited localized parseval frames and besov spaces on
compact homogeneous manifolds. Journal of Geometric Analysis , 21(2):334–371, 2011.
[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y . Bengio. Generative adversarial nets. In Advances in neural information processing systems ,
pages 2672–2680, 2014.
[15] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural
networks. In 2013 IEEE international conference on acoustics, speech and signal processing ,
pages 6645–6649. IEEE, 2013.
[16] S. Gu, E. Holly, T. Lillicrap, and S. Levine. Deep reinforcement learning for robotic manipula-
tion with asynchronous off-policy updates. In 2017 IEEE international conference on robotics
and automation (ICRA) , pages 3389–3396. IEEE, 2017.
[17] M. Hamers and M. Kohler. Nonasymptotic bounds on the l 2 error of neural network regression
estimates. Annals of the Institute of Statistical Mathematics , 58:131–151, 2006.
[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–
778, 2016.
[19] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages 7132–7141, 2018.
11[20] T. Hu, Z. Shang, and G. Cheng. Sharp rate of convergence for deep neural network classifiers
under the teacher-student setting. arXiv preprint arXiv:2001.06892 , 2020.
[21] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in
neural networks. Advances in neural information processing systems , 31, 2018.
[22] M. Kohler and A. Krzy ˙zak. Adaptive regression estimation with multilayer feedforward neural
networks. Nonparametric Statistics , 17(8):891–913, 2005.
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional
neural networks. In Advances in neural information processing systems , pages 1097–1105,
2012.
[24] J. M. Lee. Riemannian manifolds: an introduction to curvature , volume 176. Springer Science
& Business Media, 2006.
[25] H. Liu, M. Chen, T. Zhao, and W. Liao. Besov function approximation and binary classifica-
tion on low-dimensional manifolds using convolutional residual networks. In International
Conference on Machine Learning , pages 6770–6780. PMLR, 2021.
[26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages
3431–3440, 2015.
[27] E. Mammen and S. van de Geer. Locally adaptive regression splines. The Annals of Statistics ,
25(1):387–413, 1997.
[28] E. Nichani, A. Damian, and J. D. Lee. Provable guarantees for nonlinear feature learning in
three-layer neural networks. Advances in Neural Information Processing Systems , 36, 2024.
[29] P. Niyogi, S. Smale, and S. Weinberger. Finding the homology of submanifolds with high
confidence from random samples. Discrete & Computational Geometry , 39:419–441, 2008.
[30] K. Oono and T. Suzuki. Approximation and non-parametric estimation of resnet-type convolu-
tional neural networks. In International conference on machine learning , pages 4922–4931.
PMLR, 2019.
[31] J. Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. The Annals of Statistics , 48:1875–1897, 2020.
[32] T. Suzuki. Adaptivity of deep reLU network for learning in besov and mixed smooth besov
spaces: optimal rate and curse of dimensionality. In International Conference on Learning
Representations , 2019.
[33] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning. In Proceedings of the AAAI conference on artificial
intelligence , volume 31, 2017.
[34] H. Tribel. Theory of function space ii. Monographs in Mathematics , 78, 1992.
[35] L. W. Tu. Manifolds. In An Introduction to Manifolds , pages 47–83. Springer, 2011.
[36] M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint . Cambridge
Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi:
10.1017/9781108627771.013.
[37] S. Wang and Z. Shang. Minimax optimal high-dimensional classification using deep neural
networks. Stat, 11(1):e482, 2022.
[38] S. Wang, G. Cao, Z. Shang, and A. D. N. Initiative. Deep neural network classifier for
multidimensional functional data. Scandinavian Journal of Statistics , 50(4):1667–1686, 2023.
[39] Z. Wang, E. Nichani, and J. D. Lee. Learning hierarchical polynomials with three-layer neural
networks. In International Conference on Learning Representations , 2024.
12[40] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for
deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 1492–1500, 2017.
[41] D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks , 94:
103–114, 2017.
[42] T. Young, D. Hazarika, S. Poria, and E. Cambria. Recent trends in deep learning based natural
language processing. IEEE Computational Intelligence Magazine , 13:55–75, 2018.
[43] K. Zhang and Y .-X. Wang. Deep learning meets nonparametric regression: Are weight-decayed
DNNs locally adaptive? In International Conference on Learning Representations , 2023.
[44] Q. Zhang, Z. Cui, X. Niu, S. Geng, and Y . Qiao. Image segmentation with pyramid dilated
convolution based on resnet and u-net. In Neural Information Processing , pages 364–372.
Springer, 2017.
13A Why Besov Classes?
In this section, we discuss why we choose to consider the Besov class of functions and why this
makes our results particularly interesting.
To see this, we need to first define two smaller function classes: the Holder class and the Sobolev
class. Instead of giving fully general definitions for these function classes let us illustrate their
main differences using univariate functions defined on [0,1]. We also introduce the so-called Total
Variation class — which is sandwiched in between Besov( p= 1, q= 1) andBesov( p= 1, q=∞).
• Holder class functions satisfy |f(α)(x)|< C for all x.
• Sobolev class functions satisfyR
[0,1]|f(α)(x)|2dx < C
• Total Variation class functions satisfyR
[0,1]|f(α)(x)|dx < C
The L1-norm used in defining total variation class makes it the most flexible of the three. It allows
functions with αthorder derivative f(α)(x)to be very large at some places, e.g., Dirac delta functions,
while Holder and Sobolev class functions cannot contain such spikes (no longer integrable in Sobolev
norm above).
Generically speaking under the appropriate scaling: Holder ⊂Sobolev ⊂Besov . The Besov
space contains functions with heterogeneous smoothness while Holder and Sobolev classes contain
functions with homogeneous smoothness. Despite the Besov space being larger, it has the same
minimax rate of n−(2α)/(2α+d)as the smaller Holder and Sobolev class.
A new perspective on overparameterized NN. We study the adaptivity of deep networks in
overparameterized regimes. The most popular method for understanding overparameterization is
through the neural tangent kernel (NTK) regime. However, based on the classical linear smoother
lower-bound for estimating functions in Besov classes with p= 1[9,10], all kernel ridge regression
including any NTKs will have a suboptimal rate lower bounded by n−2α−d
2α. To say it differently,
there is a formal separation between NTKs and the optimal method. The same separation does not
exist in smaller function classes such as Sobolev and Holders because they are more homogeneously
smooth.
In summary, in order to study what neural networks can achieve that is not achievable by kernels, e.g.,
NTK; we had to define and approximate Besov class functions. Our results show that ConvResNeXT
not only overcomes the curse of dimensionality of the ambient space, but also has nearly optimal
dependence in the intrinsic dimension d— in contrast to the kernel-based approaches.
We believe this offers a new perspective to understand overparameterization and is more fine-grained
that of NTK.
B Numerical Simulation
In this section, we validate our theoretical findings with numerical experiments. We focus on
nonparametric regression problems for simplicity and consider the following function f0:RD→R:
f0(x) =˜f0(Ux) =˜f0(˜x)
where U∈RD×Dis a randomly-chosen rotation matrix and ˜x=Ux∈RDsatisfies that for
t∈[0,1], the first three coordinates
˜x1=tsin(4πt),˜x2=tcos(4 πt),˜x3=t(1−t),
and the remaining coordinates of ˜xare irrelevant features iid sampled from a uniform distribution.
Note that the first three coordinates of ˜xare completely determined by a scalar t, and the corresponding
label yis determined by tvia a piecewise linear function, i.e., for a bag of t1, ..., t n∈[0,1], we can
generate a labeled dataset by yi=g0(ti) +N(0,1). An illustration of the function f0is given in
Figure 2 where colors indicate the value.
Role of irrelevant features and rotation. The purpose of irrelevant features and rotation is to make
the problem harder and more interesting.
14Figure 2: Illustration of a Besov function on 1-dimensional manifold embedded in a 3-dimensional
ambient space.
xi,1=tisin(4πti), xi,2=ticos(4 πti), xi,3=ti(1−ti),
where ti, i= 1, . . . , n are evenly spaced over [0,1]. This process generates a 1-dimensional manifold
inR3which does not intersect with itself, as shown in Figure 2.
Baseline methods To estimate the underlying function on a manifold, we conducted experiments
with ConvResNeXts (this paper), as well as a mix of popular off-the-shelf methods including kernel
ridge regression, XGBoost, Decision tree, Lasso regression, and Gaussian Processes.
Hyperparameter choices. In all the experiments the following architecture was used for PNN:
w= 6,L= 10 ,M= 4,batch _size = 128 ,learning _rate = 1e −3
In all the experiments the following architecture was used for ConvResNeXt: w= 8,L= 6,K= 6,
M= 2,N= 2. Batch_size and learning_rate were adjusted for each task.
For off-the-shelf methods, their hyperparameters are either tuned automatically or avoided using tools
provided from the package, e.g., GP. For GP, a Matern kernel is used, and for ridge regression, the
standard Gaussian RBF kernel is used.
Results. Our results are reported in Figure 3, 4, 5 which reports the mean square error (MSE) as
a function of the effective degree-of-freedom of each method, ambient dimension Dand also the
number of data points nrespectively.
As we can see in Figure 3, ConvResNeXt is able to achieve the lowest MSE at a relatively smaller
degree of freedom. It outperforms the competing methods with notable margins despite using a
simpler hypothesis.
Figure 4 illustrates that standard non-parametric methods such as kernel ridge regression and Gaussian
processes deteriorate quickly as the ambient dimension gets bigger. On the contrary, ConvResNeXt
and PNN obtain results that are almost dimension-independent due to the representation learning that
helps identify the low-dimensional manifold.
Finally, the log-log plot in Figure 5 demonstrates that there is a substantially different rate of
convergence between our methods and kernel ridge regression and GPs, indicating the same formal
separation that we have established in the theoretical part — kernels must be suboptimal for estimating
Besov classes while the neural architectures we considered can be locally adaptive and nearly optimal
for Besov classes.
15Figure 3: MSE as a function of the effective degree of freedom (dof) of different methods.
Figure 4: MSE as a function of dimension D.
Figure 5: MSE as function of sample size n.
C Proof of the approximation theory
C.1 Decompose the target function into the sum of locally supported functions.
Lemma C.1. Approximating Besov function on a smooth manifold using B-spline: Let f∈Bα
p,q(M).
There exists a decomposition of f:
f(x) =CMX
i=1˜fi◦ϕi(x)×1(x∈B(ci, r)),
and˜fi=f·ρi∈Bα
p,q,PCM
i=1∥˜fi∥Bαp,q≤C∥f∥Bαp,q(M),ϕi:M →Rdare linear projections,
B(ci, r)denotes the unit ball with radius rand center ci.
The lemma is inferred by the existence of the partition of unity, which is given in Proposition 2.6.
C.2 Locally approximate the decomposed functions using cardinal B-spline basis functions.
Proposition C.2. For any function in the Besov space on a compact smooth manifold f∗∈Bs
p,q(M),
anyN≥0, there exists an approximated to f∗using cardinal B-spline basis functions:
˜f=CMX
i=1PX
j=1ai,kj,sjMm,kj,sj◦ϕi×1(x∈B(ci, r)),
16where mis the integer satisfying 0< α < min (m, m−1 + 1 /p),Mm,k,s=Mm(2k(· −s)), Mm
denotes the B-spline basis function defined in (2), the approximation error is bounded by
∥f−˜f∥∞≤C9CMP−α/d
and the coefficients satisfy
∥{2kjai,kj,sj}i,j∥p≤C10∥f∥Bαp,q(M)
for some constant C9, C10that only depends on α.
As will be shown below, the scaled coefficients 2kjai,kj,sjcorresponds to the total norm of the
parameters in the neural network to approximate the B-spline basis function, so this lemma is the key
to get the bound of norm of parameters in (12).
Proof. From the definition of Bα
p,q(M), and applying Proposition 2.6, there exists a decomposition
off∗as
f∗=CMX
i=1(fi) =CMX
i=1(fi◦ϕ−1
i)◦ϕi×1Ui,
where fi:=f∗·ρi,ρisatisfy the condition in Definition 2.5, and fi◦ϕ−1
i∈Bα
p,q. Using
Proposition F.2, for any i, one can approximate fi◦ϕ−1
iwith ¯fi:
¯fi=PX
j=1ai,kj,sjMm,kj,sj
such that ∥fi◦ϕ−1
i∥∞≤C1M−α/d, and the coefficients satisfy
∥{2kjakj,sj}j∥p≤C10∥fi◦ϕ−1
i∥Bαp,q.
Define
¯f=CMX
i=1¯fi◦ϕi×1Ui.
one can verify that ∥f−˜f∥∞≤C9CMN−α/d.On the other hand, using triangular inequality (and
padding the vectors with 0),
∥{2kjai,kj,sj}i,j∥p≤CMX
i=1∥{2kjai,kj,sj}j∥p≤CMX
i=1C10∥fi◦ϕ−1
i∥Bαp,q=C10∥f∗∥Bαp,q(M),
which finishes the proof.
C.3 Neural network for chart selection
In this section, we demonstrate that a feedforward neural network can approximate the chart selection
function z×1(x∈B(ci, r)), and it is error-free as long as z= 0when r < d (x,ci)< R. We start
by proving the following supporting lemma:
Proposition C.3. Fix some constant B >0. For any x,c∈RDsatisfying |xi| ≤Band|ci| ≤Bfor
i= 1, . . . , D , there exists an L-layer neural network ˜d(x;c)with width w=O(d)that approximates
d2(x;c) =PD
i=1(xi−ci)2such that |˜d2(x;c)−d2(x;c)| ≤8DB2exp(−C11L)with an absolute
constant C11>0when d(x;c)< τ, and ˜d2(x;c)≥τ2when d(x;c)≥τ, and the norm of the
neural network is bounded by
LX
ℓ=1∥Wℓ∥2
F+∥bℓ∥2
2≤C12DL.
17Proof. The proof is given by construction. By Proposition 2 in Yarotsky(2017), the function f(x) =
x2on the segment [0,2B]can be approximated with any error ϵ >0by a ReLU network ghaving
depth and the number of neurons and weight parameters no more than clog(4B2/ϵ)with an absolute
constant c. The width of the network gis an absolute constant. We also consider a single layer ReLU
neural network h(t) =σ(t)−σ(−t), which is equal to the absolute value of the input.
Now we consider a neural network G(x;c) =PD
i=1g◦h(xi−ci). Then for any x,c∈RD
satisfying |xi| ≤Band|ci| ≤Bfori= 1, . . . , D , we have
|G(x;c)−d2(x;c)| ≤DX
i=1g◦h(xi−ci)−DX
i=1(xi−ci)2
≤DX
i=1g◦h(xi−ci)−(xi−ci)2
≤Dϵ.
Moreover, define another neural network
F(x;c) =−σ(τ2−Dϵ−G(x;c)) +τ2
=G(x;c) +Dϵ ifG(x;c)< τ2−Dϵ,
τ2ifG(x;c)≥τ2−Dϵ,
which has depth and the number of neurons no more than c′log(4B2/ϵ)with an absolute constant c′.
The weight parameters of Gare upper bounded by max{τ2, Dϵ, c log(4B2/ϵ)}and the width of G
isO(D).
Ifd2(x;c)< τ2, we have
|F(x;c)−d2(x;c)|=| −σ(τ2−Dϵ−G(x;c)) +τ2−d2(x;c)|
=|G(x;c)−d2(x;c) +Dϵ|ifG(x;c)< τ2−Dϵ,
τ2−d2(x;c) ifG(x;c)≥τ2−Dϵ.
For the first case when G(x;c)< τ2−Dϵ,|F(x;c)−d2(x;c)| ≤2Dϵsince d2(x;c)can be
approximated by G(x;c)up to an error ϵ. For the second case when G(x;c)≥τ2−Dϵ, we have
d2(x;c)≥G(x;c)−Dϵ≥τ2−2Dϵand . Thereby we also have |F(x;c)−d2(x;c)| ≤2Dϵ.
Ifd2(x;c)≥τ2instead, we will obtain G(x;c)≥d2(x;c)−Dϵ≥τ2−Dϵ. This gives that
F(x;c) =τ2in this case.
Finally, we take ϵ= 4B2exp(−L/c′). Then F(x;c)is an L-layer neural network with O(L)
neurons. The weight parameters of Gare upper bounded by max{τ2,4DB2exp(−L/c′), cL/c′}
and the width of GisO(D). Moreover, F(x;c)satisfies |F(x;c)−d2(x;c)|<8DB2exp(−L/c′)
ifd2(x;c)≤τ2andF(x;c) =τ2ifd2(x;c)≥τ2.
Proposition C.4. There exists a single layer ReLU neural network that approximates ˜×, such that
for all 0≤x≤C, y∈ {0,1},x˜×y=xwhen y= 1, and x˜×y= 0when either x= 0ory= 0.
Proof. Consider a single layer neural network g(x, y) :=A2σ(A1(x, y)⊤)with no bias, where
A1=
−1
C1
0 1
, A 2=
−C
C
.
Then we can rewrite the neural network gasg(x, y) =−Cσ(−x/C+y) +Cσ(y). Ify= 1,
we will have g(x, y) =−Cσ(−x/C + 1) + C=x, since x≤C. Ify= 0, we will have
g(x, y) =−Cσ(−x/C) = 0 , since x≥0. Thereby we can conclude the proof.
By adding a single linear layer
y=1
R−r−2∆(σ(R−∆−x)−σ(r+ ∆−x))
18after the one shown in Proposition C.3, where ∆ = 8 DB2exp(−CL)denotes the error in Proposi-
tion C.3, one can approximate the indicator function 1(x∈B(ci, r))such that it is error-free when
d(x,ci)≤ror≥R. Choosing R≤τ/2, r < R −2∆, and combining with Proposition C.4, the
proof is finished. Considering that fiis locally supported on B(ci, r)for all iby our method of
construction, the chart selection part does not incur any error in the output.
C.4 Constructing the neural network to Approximate the target function
In this section, we focus on the neural network with the same architecture as a ResNeXt in Defini-
tion 2.12 but replacing each building block with a feedforward neural network, and prove that it can
achieve the same approximation error as in Theorem 3.2. For technical simplicity, we assume that the
target function f∗∈[0,1]without loss of generality. Then our analysis automatically holds for any
bounded function.
Theorem C.5. For any f∗under the same condition as Theorem 3.2, any neural network architecture
with residual connections containing Nnumber of residual blocks and each residual block contains
Mnumber of feedforward neural networks in parallel, where the depth of each feedforward neural
networks is L, width is w:
f=Wout· 
1 +MX
m=1fN,m!
◦ ··· ◦ 
1 +MX
m=1f1,m!
fn,m=W(n,m)
Lσ(W(n,m)
L−1. . . σ(W(n,m)
1x))◦P(x),
where P(x) = [xT,1,0]Tis the padding operation,
satisfying
MN≥CMP, w ≥C1(dm+D),
Bres:=NX
n=1MX
m=1LX
ℓ=1∥W(n,m)
ℓ∥2
F≤C2L,
Bout:=∥Wout∥2
F≤C3C2
F((dm+D)L)L(CMP)L−2/p,(12)
there exists an instance fof this ResNeXt class, such that
∥f−f∗∥∞≤CFCM
C4P−α/d+C5exp(−C6LlogP)
, (13)
where C1, C2, C3, C4, C5, C6are the same constants as in Theorem 3.2.
Proof. We first construct a parallel neural network to approximate the target function, then scale the
weights to meet the norm constraint while keeping the model equivalent to the one constructed in the
first step, and finally transform this parallel neural network into the ConvResNeXt as claimed.
Combining Lemma F.3, Proposition C.3 and Proposition C.4, by putting the neural network in
Lemma F.3 and Proposition C.3 in parallel and adding the one in Proposition C.4 after them, one can
construct a feedforward neural network with bias with depth L, width w=O(d) +O(D) =O(d),
that approximates Mm,kj,sj(x)×1(x∈B(ci, r))for any i, j.
To construct the neural network with residual connections that approximates f∗, we follow the
method in Oono and Suzuki [30], Liu et al. [25]. This network uses separate channels for the inputs
and outputs. Let the input to one residual layer be [x1, y1], the output is [x1, y1+f(x1)]. As a result,
if one scale the outputs of all the building blocks by any scalar a, then the last channel of the output
of the entire network is also scaled by a. This property allows us to scale the weights in each building
block while keeping the model equivalent. To compensate for the bias term, Proposition F.4 can be
applied. This only increases the total norm of each building block by no larger than a constant term
that depends only L, which is no more than a factor of constant.
Let the neural network constructed above has parameter ˜W(i,j)
1,˜b(i,j)
1, . . . , ˜W(i,j)
L,b(i,j)
Lin each
layer, one can construct a building block without bias as
W(i,j)
1=˜W(i,j)
1˜b(i,j)
1 0
0 1 0
,W(i,j)
ℓ=˜W(i,j)
ℓ˜b(i,j)
ℓ
0 1
W(i,j)
L=
0 0
0 0
˜W(i,j)
L˜b(i,j)
L
.
19Remind that the input is padded with the scalar 1 before feeding into the neural network, the above
construction provide an equivalent representation to the neural network including the bias, and route
the output to the last channel. From Lemma F.3, it can be seen that the total square norm of this block
is bounded by (10).
Finally, we scale the weights in the each block, including the “1” terms to meet the norm constraint.
Thanks to the 1-homogeneous property of ReLU layer, and considering that the network we construct
use separate channels for the inputs and outputs, the model is equivalent after scaling. Actually the
property above allows the tradeoff between BresandBout. If all the weights in the residual blocks are
scaled by an arbitrary positive constant c, and the weight in the last layer Woutis scaled by c−L, the
model is still equivalent. We only need to scale the all the weights in this block with |ai,kj,sj|1/L,
setting the sign of the weight in the last layer as sign(ai,kj,sj), and place CMPnumber of these
building blocks in this neural network with residual connections. Since this block always output 0
in the first D+ 1channels, the order and the placement of the building blocks does not change the
output. The last fully connected layer can be simply set to
Wout= [0, . . . , 0,1], bout= 0.
Combining Proposition F.2 and Lemma F.1, the norm of this ResNeXt we construct satisfy
¯Bres≤CMX
i=1PX
j=1a2/L
i,kj,sj(22k/LC14dmL +C12DL)
≤CMX
i=1PX
j=1(2kai,kj,sj)2/L(C14dmL +C12DL)
≤(CMP)1−2/(pL)∥{2kai,kj,sj}∥2/L
p(C14dmL +C12DL)
≤(C10CF)2/L(CMP)1−2/(pL)(C14dmL +C12DL),
¯Bout≤1.
By scaling all the weights in the residual blocks by ¯B−1/2
res , and scaling the output layer by ¯BL/2
res, the
network that satisfy (12) can be constructed.
Notice that the chart selection part does not introduce error by our way of construction, we only
need to sum over the error in Section 4.1 and Section 4.1, and notice that for any x, for any linear
projection ϕi, the number of B-spline basis functions Mm,k,sthat is nonzero on xis no more than
mdlogP, the approximation error of the constructed neural network can be proved.
C.5 Constructing a convolution neural network to approximate the target function
In this section, we prove that any feedforward neural network can be realized by a convolution neural
network with similar size and norm of parameters. The proof is similar to Theorem 5 in [30].
Lemma C.6. For any feedforward neural network with depth L′, width w′, input dimension hand
output dimension h′, for any kernel size K > 1, there exists a convolution neural network with depth
L=L′+L0−1, where L0=⌈h−1
K−1⌉number of channels w= 4w′, and the first dimension of
the output equals the output of the feedforward neural network for all inputs, and the norm of the
convolution neural network is bounded as
LX
ℓ=1∥Wℓ∥2
F≤4L′X
ℓ=1∥W′
ℓ∥2
F+ 4w′L0,
where W′
1∈Rw′×h′;W′
ℓ∈Rw′×w′, ℓ= 2, . . . , L′−1;W′
L′∈Rh′×w′are the weights in the
feedforward neural network, and W1∈RK×w×h,Wℓ∈RK×w×w, ℓ= 2, . . . , L −1;WL∈
RK×h×ware the weights in the convolution neural network.
Proof. We follow the same method as Oono and Suzuki [30] to construct the CNN that is equivalent
to the feedforward neural network. By combining Oono and Suzuki [30] lemma 1 and lemma 2, for
20any linear transformation, one can construct a convolution neural network with at most L0=⌈h−1
K−1⌉
convolution layers and 4 channels, where his the dimension of input, which equals D+ 1in our
case, such that the first dimension in the output equals the linear transformation, and the norm of all
the weights is no more than
L0X
ℓ=1∥Wℓ∥2
F≤4L0, (14)
where Wℓis the weight of the linear transformation. Putting wnumber of such convolution neural
networks in parallel, a convolution neural network with L0layers and 4wchannels can be constructed
to implement the first layer in the feedforward neural network.
To implement the remaining layers, one choose the convolution kernel Wℓ+L0−1[:, i, j] =
[0, . . . ,W′[i, j], . . . , 0],∀1≤i, j≤w, and pad the remaining parts with 0, such that this con-
volution layer is equivalent to the linear layer applied on the dimension of channels. Noticing that
this conversion does not change the norm of the parameters in each layer. Adding both sides of (14)
by the norm of the 2−L′-th layer in both models finishes the proof.
D Proof of the estimation theory
D.1 Covering number of a neural network block
Proposition D.1. If the input to a ReLU neural network is bounded by ∥x∥2≤Bin, the covering
number of the ReLU neural network defined in Proposition F .6 is bounded by
N(FNN, δ,∥ · ∥ 2)≤Bin(B/L)L/2wL
δw2L
.
Proof. Similar to Proposition F.6, we only consider the case ∥Wℓ∥F≤p
B/L . For any 1≤ℓ≤L,
for any W1, . . . W ℓ−1, Wℓ, W′
ℓ, Wℓ+1, . . . W Lthat satisfy the above constraint and ∥Wℓ−W′
ℓ∥F≤ϵ,
define g(. . .;W1, . . . W L)as the neural network with parameters W1, . . . W L, we can see
∥g(x;W1, . . . W ℓ−1, Wℓ, Wℓ+1, . . . W L)−g(x;W1, . . . W ℓ−1, Wℓ, Wℓ+1, . . . W L)∥2
≤(B/L)(L−ℓ)/2∥Wℓ−W′
ℓ∥2∥ReLU (Wℓ−1. . . ReLU (W1(x)))∥2
≤(B/L)(L−1)/2Binϵ.
Choosing ϵ=δ
L(B/L)(L−1)/2, the above inequality is no larger than δ/L. Taking the sum over ℓ, we
can see that for any W1, W′
1, . . . , W L, W′
Lsuch that ∥Wℓ−W′
ℓ∥F≤ϵ,
∥g(x;W1, . . . W L)−g(x;W′
1, . . . W′
L))∥2≤δ.
Finally, observe that the covering number of Wℓis bounded by
N({W:∥W∥F≤B}, ϵ,∥ · ∥ F)≤2Bw
ϵw2
. (15)
Substituting Bandϵand taking the product over ℓfinishes the proof.
Proposition D.2. If the input to a ReLU convolution neural network is bounded by ∥x∥2≤Bin, the
covering number of the ReLU neural network defined in (5) is bounded by
N(FNN, δ,∥ · ∥ 2)≤Bin(BK/L )L/2wL
δw2KL
.
Proof. Similar to Proposition D.1, for any 1≤ℓ≤L, for any W1, . . . W ℓ−1, Wℓ, W′
ℓ, Wℓ+1, . . . W L
that satisfy the above constraint and ∥Wℓ−W′
ℓ∥F≤ϵ, define g(. . .;W1, . . . W L)as the neural
network with parameters W1, . . . W L, we can see
∥g(x;W1, . . . W ℓ−1, Wℓ, Wℓ+1, . . . W L)−g(x;W1, . . . W ℓ−1, Wℓ, Wℓ+1, . . . W L)∥2
≤KL/2(B/L)(L−ℓ)/2∥Wℓ−W′
ℓ∥2∥ReLU (Wℓ−1. . . ReLU (W1(x)))∥2
≤KL/2(B/L)(L−1)/2Binϵ,
21where the first inequality comes from Proposition F.10. Choosing ϵ=δ
KL/2BinL(B/L)(L−1)/2, the
above inequality is no larger than δ/L. Taking this into (15) finishes the proof.
D.2 Proof of Theorem 3.3
Define ˜f= arg minfED[L(f)]. From Theorem 14.20 in Wainwright [36], for any function class
∂Fthat is star-shaped around ˜f, the empirical risk minimizer ˆf= arg minf∈FLn(f)satisfy
ED[L(ˆf)]≤ED[L(˜f)] + 10 δn(2 +δn) (16)
with probability at least 1−c1exp(−c2nδ2
n)for any δnthat satisfy (20), where c1, c2are universal
constants.
The function of neural networks is not star-shaped, but can be covered by a star-shaped function class.
Specifically, let {f−˜f:f∈ FConv} ⊂ { f1−f2:f1, f2∈ FConv}:=∂F.
Any function in ∂Fcan be represented using a ResNeXt: one can put two neural networks of the
same structure in parallel, adjusting the sign of parameters in one of the neural networks and summing
up the result, which increases M, B resandBoutby a factor of 2. This only increases the log covering
number in (11) by a factor of constant (remind that Bres=O(1)by assumption).
Taking the log covering number of the ResNeXt (11), the sufficient condition for the critical radius as
in (20) is
n−1/2wL1/2B1
2−4/L
res K1−1/L
1−2/L 
B1/2
outexp(( KBres/L)L/2)1/L
1−2/Lδ1−3/L
1−2/L
n≲δ2
n
4,
δn≳K(w2L)1−2/L
2−2/LB1
2−2/L
res 
B1/2
outexp(( KBres/L)L/2)1/L
1−1/Ln−1−2/L
2−2/L,(17)
where≲hides the logarithmic term.
Because Lis 1-Lipschitz, we have
L(f)≤ L(˜f) +∥f−˜f∥∞.
Choosing
P=O
 
K−2
L−2w3L−4
L−2L3L−2
L−2
n!−1−2/L
2α/d(1−1/L)+1−2/pL
,
and taking Theorem 3.2 and (17) into (16) finishes the proof.
E Lower bound of error
In this section, we study the minimax lower bound of any estimator for Besov functions on a d-
dimensional manifold. It suffices to consider the manifold Mas ad-dimensional hypersurface.
Without the loss of generalization, assume that∂L(y)
∂y≥0.5for−ϵ≤y≤ϵ. Define the function
space
F=

f=sX
j1,...,jd=1±ϵ
sα×M(m)((x−j)/s)

, (18)
where M(m)denotes the Cardinal B-spline basis function that is supported on (0,1)d,j=
[j1, . . . , j d]. The support of each B-spline basis function splits the space into sdnumber of blocks,
where the target function in each block has two choices (positive or negative), so the total number of
different functions in this function class is |F|= 2sd. Using D ˜ung[11, Theorm 2.2 ], we can see that
for any f∈ F,
∥f∥Bαp,q≤ϵ
sαsα−d/psd/p=ϵ.
For a fixed f∗∈ F, letD={(xi, yi)}n
i=1be a set of noisy observations with yi=f∗(xi)+ϵi, ϵi∼
SubGaussian (0, σ2I). Further assume that xiare evenly distributed in (0,1)dsuch that in all
22regions as defined in (18), the number of samples is nj:=O(n/sd). Using Le Cam’s inequality, we
get that in any region, any estimator θsatisfy
sup
f∗∈FED[∥θ(D)−f∗∥j]≥Cmϵ
16sα
as long as (ϵ
σsα)2≲sd
n, where ∥ · ∥j:=1
niP
s(x−j)∈[0,1]d|f(x)|denotes the norm defined in the
block indexed by i,Cmis a constant that depends only on m. Choosing s=O(n1
2α+d), we get
sup
f∗∈FED[∥θ(D)−f∗∥j]≥n−α
2α+d.
Observing1
nPn
i=1L(ˆ(f(xi)))≥0.5Pn
i=1|f(xi)−f∗(xi)|≂1
sdP
j∈[s]d∥ˆf−f∗∥jfinishes the
proof.
F Supporting theorem
Lemma F.1. [Lemma 14 in Zhang and Wang [43]] For any a∈R¯M,0< p′< p, it holds that:
∥a∥p′
p′≤¯M1−p′/p∥a∥p′
p.
Proposition F.2 (Proposition 7 in Zhang and Wang [43]).Letα−d/p > 1, r > 0. For
any function in Besov space f∗∈Bα
p,qand any positive integer ¯M, there is an ¯M-sparse
approximation using B-spline basis of order msatisfying 0< α < min(m, m−1 + 1 /p):
ˇf¯M=P¯M
i=1aki,siMm,ki,sifor any positive integer ¯Msuch that the approximation error is bounded
as∥ˇf¯M−f∗∥r≲¯M−α/d∥f∗∥Bαp,q,and the coefficients satisfy
∥{2kiaki,si}ki,si∥p≲∥f∗∥Bαp,q.
Lemma F.3 (Lemma 11 in [ 43]).LetMm,k,sbe the B-spline of order mwith scale 2−kin each
dimension and position s∈Rd:Mm,k,s(x) :=Mm(2k(x−s)),Mmis defined in (2). There exists
a neural network with d-dimensional input and one output, with width wd,m=O(dm)and depth
L≲log(C13/ϵ)for some constant C13that depends only on mandd, approximates the B spline
basis function Mm,k,s(x) :=Mm(2k(x−s)). This neural network, denoted as ˜Mm,k,s(x),x∈Rd,
satisfy
•|˜Mm,k,s(x)−Mm,k,s(x)| ≤ϵ, if0≤2k(xi−si)≤m+ 1,∀i∈[d],
•˜Mm,k,s(x) = 0 , otherwise.
•The total square norm of the weights is bounded by 22k/LC14dmL for some universal
constant C14.
Proposition F.4. For any feedforward neural network fwith width wand depth Lwith bias, there
exists a feedforward neural network f′with width w′=w+ 1and depth L′=L, such that for any
x,f(x) =f′([xT,1]T)
Proof. Proof by construction: let the weights in the ℓ-th layer in fbeWℓ, and the bias be bℓ, and
choose the weight in the corresponding layer in f′be
W′
ℓ=˜Wℓ˜bℓ
0 1
,∀ℓ < L ;W′
L= [˜WL˜bL].
The constructed neural network gives the same output as the original one.
Corollary F.5 (Corollary 13.7 and Corollary 14.3 in Wainwright [36]) .Let
Gn(δ,F) =Ewi"
sup
g∈F,∥g∥n≤δ1
nnX
i=1wig(xi)#
,Rn(δ,F) =Eϵi"
sup
g∈F,∥g∥n≤δ1
nnX
i=1ϵig(xi)#
,
23denotes the local Gaussian complexity and local Rademacher complexity respectively, where wi∼
N(0,1)are the i.i.d. Gaussian random variables, and ϵi∼uniform {−1,1}are the Rademacher
random variables. Suppose that the function class Fis star-shaped, for any σ >0, any δ∈(0, σ]
such that
16√nZδn
δ2n/4σp
logN(F, µ,∥ · ∥∞)dµ≤δ2
n
4σ
satisfies
Gn(δ,F)≤δ2
2σ. (19)
Furthermore, if Fis uniformly bounded by b, i.e.∀f∈ F,x|f(x)| ≤banyδ >0such that
64√nZδn
δ2n/2b4σp
logN(F, µ,∥ · ∥∞)dµ≤δ2
n
b.
satisfies
Rn(δ,F)≤δ2
b. (20)
Proposition F.6. AnL-layer ReLU neural network with no bias and bounded norm
LX
ℓ=1∥Wℓ∥2
F≤B
is Lipschitz continuous with Lipschitz constant (B/L)L/2
Proof. Notice that ReLU function is 1-homogeneous, similar to Proposition 4 in [ 43], for any neural
network there exists an equivalent model satisfying ∥Wℓ∥F=∥Wℓ′∥Ffor any ℓ, ℓ′, and its total
norm of parameters is no larger than the original model. Because of that, it suffices to consider the
neural network satisfying ∥Wℓ∥F≤p
B/L for all ℓ. The Lipschitz constant of such linear layer is
∥Wℓ|∥2≤ ∥Wℓ|∥F≤p
B/L , and the Lipschitz constant of ReLU layer is 1. Taking the product
over all layers finishes the proof.
Proposition F.7. AnL-layer ReLU convolution neural network with convolution kernel size K, no
bias and bounded norm
LX
ℓ=1∥Wℓ∥2
F≤B.
is Lipschitz continuous with Lipschitz constant (KB/L )L/2
This proposition can be proved by taking Proposition F.10 into the proof of Proposition F.6.
Proposition F.8. Letf=fpost◦(1 +fNN+fother)◦fprebe a ResNeXt, where 1 +fNN+fother
denotes a residual block, fpreandfpostdenotes the part of the neural network before and after this
residual block, respectively. fNNdenotes one of the building block in this residual block and fother
denotes the other residual blocks. Assume fpre, fNN, fpostare Lipschitz continuous with Lipschitz
constant Lpre, LNN, Lpostrespectively. Let the input be x, if the residual block is removed, the
perturbation to the output is no more than LpreLNNLpost∥x∥
Proof.
|fpost◦(1 +fNN+fother)◦fpre(x)−fpost◦(1 +fother)◦fpre(x)|
≤Lpost|(1 +fNN+fother)◦fpre(x)−(1 +fother)◦fpre(x)|
=Lpost|fNN◦fpre(x)|
≤LpreLNNLpost∥x∥.
Proposition F.9. The neural network defined in Lemma 4.1 with arbitrary number of blocks has
Lipschitz constant exp(( KBres/L)L/2), where K= 1when the feedforward neural network is the
building blocks and Kis the size of the convolution kernel when the convolution neural network is
the building blocks.
24Proof. Note that the m-th block in the neural network defined in Lemma 4.1 can be represented
asy=fm(x;ωm) +x, where fmis an L-layer feedforward neural network with no bias. By
Proposition F.6 and Proposition F.7, such block is Lipschitz continuous with Lipschitz constant
1+(KBm/L)L/2, where the weight parameters of the m-th block satisfy thatPL
ℓ=1∥W(m)
ℓ∥2
F≤Bm
andPM
m=1Bm≤Bres.
Since the neural network defined in Lemma 4.1 is a composition of Mblocks, it is Lipschitz with
Lipschitz constant Lres. We have
Lres≤MY
m=1 
1 +KBm
LL/2!
≤exp MX
m=1KBm
LL/2!
,
where we use the inequality 1 +z≤exp(x)for any x∈R. Furthermore, notice thatPM
m=1(KBm/L)L/2is convex with respect to (B1, B2, . . . , B M)when L >2. SincePM
m=1Bm≤
BresandBm≥0, then we havePM
m=1(KBm/L)L/2≤(KBres/L)L/2by convexity. Therefore,
we obtain that Lres≤exp(( KBres/L)L/2).
Proposition F.10. For any x∈Rd,w∈RK, K≤d,∥Conv( x,w)∥2≤√
K∥x∥2∥w∥2.
Proof. For simplicity, denote xi= 0fori≤0ori > d .
∥Conv( x,w)∥2
2=Pd
i=1⟨x[i−K−1
2:i+K−1
2],w⟩2
≤Pd
i=1∥x[i−K−1
2:i+K−1
2]∥2
2∥w∥2
2
≤K∥x∥2
2∥w∥2
2,
where the second line comes from Cauchy-Schwarz inequality, the third line comes by expanding
∥x[i−K−1
2:i+K−1
2]∥2
2by definition and observing that each element in xappears at most K
times.
25NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our main theories are presented in Section 3. Section 4 presents the proof
sketch of the main results and highlights our technical contributions.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the limitations of our work in Section 5.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The assumptions are presented in Section 3. We outline the proof sketch in
Section 4 and defer proof details to Appendix C, D, E and F.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The experiment details are given in Appendix B.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: Our experiments use simulated data and do not require any datasets. The
experiment implementation is simple and clearly described in Appendix B, so we feel there
is no need to publish data and code.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide our experimental results and details in Appendix B.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Error bars are included in our experiments in Appendix B. See Figure 3 for
illustration.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
26Justification: Our experiments do not require large compute resources and can be reproduced
on laptops.
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: This research conforms to the NeurIPS Code of Ethics.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper presents work whose goal is to advance the field of Machine
Learning. There are many potential societal consequences of our work, none of which we
feel must be specifically highlighted.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper poses no such risks.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This paper does not use existing assets.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
27