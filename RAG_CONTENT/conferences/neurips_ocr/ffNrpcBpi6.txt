Graph Convolutions Enrich the Self-Attention in
Transformers!
Jeongwhan Choi∗
Yonsei University
jeongwhan.choi@yonsei.ac.krHyowon Wi∗
KAIST
hyowon.wi@kaist.ac.kr
Jayoung Kim
KAIST
jayoung.kim@kaist.ac.krYehjin Shin
KAIST
yehjin.shin@kaist.ac.kr
Kookjin Lee
Arizona State University
kookjin.lee@asu.eduNathaniel Trask
University of Pennsylvania
ntrask@seas.upenn.eduNoseong Park†
KAIST
noseong@kaist.ac.kr
Abstract
Transformers, renowned for their self-attention mechanism, have achieved state-of-
the-art performance across various tasks in natural language processing, computer
vision, time-series modeling, etc. However, one of the challenges with deep Trans-
former models is the oversmoothing problem, where representations across layers
converge to indistinguishable values, leading to significant performance degrada-
tion. We interpret the original self-attention as a simple graph filter and redesign it
from a graph signal processing (GSP) perspective. We propose a graph-filter-based
self-attention (GFSA)1to learn a general yet effective one, whose complexity,
however, is slightly larger than that of the original self-attention mechanism. We
demonstrate that GFSA improves the performance of Transformers in various fields,
including computer vision, natural language processing, graph-level tasks, speech
recognition, and code classification.
1 Introduction
Natural Language
Understanding
(CoLA)Causal Language
Modeling
(PTB)Automatic Speech
Recognition
(LibriSpeech 960h)
Image
Classification
(ImageNet-1k)
Code
Classification
(Devign)Graph
Regression
(ZINC)0%2%4%6%
6.25%RoBERT a0.31%
GPT24.55%
Transformer
1.63% DeiT
2.40%RoBERT a
4.03%Graphormer
Backbones
 + GFSA
Figure 1: Performance improvements (%) of our
GFSA when integrated with different Transformer
backbones in various domains. We achieve these
results with only tens to hundreds of additional
parameters to Transformers.Transformers are arguably one of the best feats
in the field of deep learning. They are now show-
ing state-of-the-art performance in various fields,
ranging from computer vision to natural lan-
guage processing, prediction tasks on graphs,
speech recognition, and so forth [ 77,16,60,61,
19,74,101,45,27,41,63,51,59,38,95,72].
Recently, there have been several studies con-
ducted on better understanding them [ 25,3,80];
there exists a common agreement among re-
searchers that the self-attention is one of the
keys leading to the success.
∗Equal contribution.
†Corresponding author.
1The source code of GFSA is available at: https://github.com/jeongwhanchoi/GFSA .
38th Conference on Neural Information Processing Systems (NeurIPS 2024).−100 −50 0 50 100
Frequency0.00.20.40.60.81.0Normalized MagnitudeDeiT
DeiT + GFSA(a) Filter response
1 6 12 18 24
Layer Index0.20.40.60.8Cosine Similarity
DeiT
DeiT + GFSA (b) Cosine similarity
0 50 100 150
Singular Value Index0.00.30.60.9Normalized Singular ValueDeiT
DeiT + GFSA (c) Singular value
Figure 2: Filter frequency response, cosine similarity, and singular values on ImageNet-1k for DeiT-S
and DeiT-S + GFSA. Details and more visualizations are in Appendices C and D.
However, there also exist several studies pointing out potential limitations of the self-attention [ 101,
18,25,28]. For instance, Shi et al. [71] revealed an analogy between the self-attention and the residual
graph convolutional network (GCN), showing that BERT also suffers from a notorious problem of
GCNs, called oversmoothing , i.e., tokens’ latent representations become similar to each other at the
deeper layers. In every self-attention layer, value vectors are aggregated in a weighted average manner
since each row-wise sum of the attention matrix is always 1. Although each self-attention layer has
its own attention matrix, this aggregation method causes the oversmoothing problem, not only in
Transformers but also in graph neural networks [ 54,7,79,66,37,101,25,92,53,71,80,3,89,90].
However, we confine our discussion to the oversmoothing of Transformers (see Section 2).
Being inspired by them, we redesign the self-attention from the perspective of graph signal processing
(GSP) — in particular, we resort to GSP on directed graphs since the attention matrix is asymmetric.
However, performing graph convolutions in the self-attention layer may incur non-trivial computa-
tional overheads. Therefore, our key design point is to learn a general but effective graph filter with
minimal overhead. In general, a graph filter on a graph Gis represented by a polynomial expression
based on its adjacency or Laplacian matrix — in this regard, the existing self-attention mechanism
can be understood as the simplest graph filter with ¯Aonly, where ¯A∈[0,1]n×nmeans a learned
attention matrix and nis the number of input tokens.
Our proposed graph filter consists of an identity term and two matrix polynomial terms, ¯Aand¯AK.
One can design better filters with more polynomial terms, but we avoid it since Transformers already
require very large computation. The K-th power, ¯AK, may also require a high computation when
the number of tokens is large. To avoid this, we further approximate ¯AKusing the element-wise
first-order Taylor approximation. Therefore, one can consider that our proposed graph filter is the
very next complicated filter after the one used by the original self-attention mechanism. However, its
efficacy is tremendous in various fields (cf. Fig. 1).
Our proposed filter enriches the self-attention with more diverse frequency information (see Fig. 2(a))
— low (resp. high) frequency signals on Gmean neighboring nodes have similar (resp. different)
values. Therefore, our method is able to not only effectively address the oversmoothing problem but
also learn better latent representations for downstream tasks.
There exist a couple of prior works to enrich the self-attention mechanism with high frequency
information [ 80,4]. In comparison with them, our proposed graph filter is distinctive in the following
aspects: i) our proposed filter is more effective and shows better performance with comparable
computational overheads, ii) our proposed filter is well-aligned with recent advancements in the GCN
community — in other words, some graph filters used by recent advanced GCN methods are special
cases of our proposed graph filter, which is not the case for prior works, and iii) other methods were
typically studied for certain domains only whereas we test our method in 6 domains — for instance,
DiversePatch [25] works only for Vision Transformers (ViTs).
We replace the self-attention layer of selected Transformers in various fields with our proposed graph
filter-based layer without changing other parts. Therefore, the accuracy increases in them are solely
by our proposed graph filter-based self-attention. These enriched Transformers increase the model
performance by 1.63% for image classification, 6.25% for natural language understanding, 0.31% for
2causal language modeling, 4.03% for graph regression, 4.76% for speech recognition, and 2.40% for
code classification (see Fig. 1). Our core contributions are as follows:
•We provide a novel perspective on self-attention as a graph filter. This perspective allows us
to design more effective self-attention that can address the oversmoothing problem.
•We propose a graph filter-based self-attention (GFSA) mechanism, integrating an identity
term and two polynomial terms for general yet effective than the simple self-attention
mechanism (Section 3).
•We demonstrate that GFSA improves the performance of Transformers on a variety of tasks.
GFSA achieves improved results on natural language processing, computer vision, speech
recognition, graph-level tasks, and code classification (Sections 5.1 to 5.6).
•We devise a strategy to selectively apply GFSA to even-numbered layers, effectively miti-
gating the computational overhead while preserving GFSA’s performance (Section 6).
2 Background & Related Work
2.1 Self-Attention in Transformers
The core building block of the Transformer architecture is the self-attention mechanism, which
enables the model to learn attention patterns over its input tokens [ 77]. The self-attention mechanism,
denoted as SA :Rn×d→Rn×d, can be expressed as follows:
SA(X) =softmaxXW qry(XW key)⊺
√
d
XW val=¯AXW val, (1)
where X∈Rn×dis the input feature matrix, Wkey∈Rd×d,Wqry∈Rd×d, andWval∈Rd×dare
the key, query, and value trainable parameters, respectively, and dis the dimension of each token.
The self-attention mechanism allows the model to weigh the importance of each token in the input
sequence relative to the others, enabling the model to capture long-range contextual information
better. The Transformer architecture includes multiple layers, each with a multi-head self-attention
layer followed by a position-wise feed-forward layer.
2.2 Self-Attention and Graph Convolutional Filter
The self-attention matrix used in Transformers has the form of symmetrically normalized adjacency
matrix where each token become a node [ 71,28] — the symmetrically normalized adjacency matrix
is a special case of asymmetric (or directed) adjacency matrix where each row is normalized and is
frequently used for the graph signal processing (GSP) on directed graphs [ 49]. A weighted graph
Gwith adjacency matrix Acan be constructed by using the input tokens as nnodes and the edge
weights between node iand node jasexp((XW qry)⊺
i(XW key)j). We can rewrite the self-attention
matrix ¯Aijasexp ((XW qry)⊺
i(XW key)j)Pd
k=1exp (XW qry)⊺
i(XW key)k. This allows ¯Ato be interpreted as the symmetrically
normalized adjacency matrix. In other words, ¯A=D−1A, where D=diag(d1, d2, . . . , d n)and
di=P
jAi,j.
Our new attention method is designed on top of GSP which has a close connection to discrete signal
processing (DSP) [ 67,68]. In DSP, a discrete signal with a length of ncan be represented by a vector
x∈Rn. Letg∈Rnbe a filter that we want to apply to x. The convolution x∗gcan be written as
follows:
yi=nX
j=1xjgi−j, (2)
where the index, denoted as i, refers to the i-th element in each vector.
GSP can be understood as a generalized concept of DSP. Signals are defined on the nodes of a
graph, and the graph’s structure influences signal processing operations. In addition, the linear and
shift-invariant graph convolution filter Hwithnnodes can be written with a shift operator Sas
3follows — Scan be from a directed graph [48]:
y=Hx=KX
k=0wkSkx, (3)
where x∈Rnis a 1-dimensional graph signal, Kis the maximum order of polynomial, and
wk∈[−∞,∞]is a coefficient. Sis ann×nmatrix where (i, j)-th element is non-zero if and only
if there is an edge from node itoj. Two representative samples of Sare adjacency and Laplacian
matrices. The graph filter His the same asPK
k=0wkSkwith a large enough value of K, which
is called matrix polynomial [48]. We note that this graph filtering operation can be extended to
d-dimensional cases as in Eq. (1). Being inspired by Zou et al. [106] and Maskey et al. [49], we rely
on the singular value domain analysis to understand the low/high-pass characteristics of filters on
directed graphs (cf. Fig. 2). See more discussion in Appendices E and F.
In the context of the self-attention within Transformers, the core part of the self-attention in Eq. (1),
i.e.,¯AX , can be considered as a d-dimensional graph filter with ¯Aonly, where H=¯A. Our goal in
this paper is to design a simple (for computational purposes) yet effective form of H.
2.3 Oversmoothing in GCNs and Transformers
Oversmoothing is a phenomenon observed in deep learning models, especially in GCNs [ 39,78]. As
information is aggregated over multiple layers for multiple nodes (tokens), latent representations tend
to become similar to each other, leading to a loss of distinctiveness in the representations [ 54,104,66].
Surprisingly, an oversmoothing-like phenomenon is also observed in Transformers [ 80,71]. Unlike
CNNs, Transformers can not benefit from simply deepening layers after a certain depth. Earlier
studies hypothesize that this may be due to issues such as the attention or feature collapse or due to
uniformity among patches or tokens [ 101,25,92]. Dong et al. [18] also point out that the output of a
pure Transformer, i.e., an attention mechanism without skip connections or MLPs, tends to converge
to a rank-1 matrix [ 18]. This analysis is followed by [ 53], which suggests that rank collapses incur
vanishing gradients of attention queries and keys.
In this context, the self-attention acts as a low-pass filter, since the self-attention calculates the
weighted average of the value vectors of tokens. Wang et al. [80, Theorem 1 ]also reveal that the
self-attention is a low-pass filter, continuously reducing high-frequency information. This nature
contributes to the oversmoothing phenomenon as unique high-frequency features are lost in deeper
layers of the network, further worsening the uniformity of token representations. Therefore, we
extend the term “oversmoothing” to describe the degeneration challenge observed in Transformers.
There have been proposed many empirical countermeasures for ViT, such as patch diversification [ 102,
25], rank collapse alleviation [ 101,99], and training stabilization [ 74,98]. Similar alleviating methods
have been also proposed in the field of NLP, such as unsupervised learning [ 9], and resolve the
oversmoothing and the token uniformity (or information diffusion) problems [ 18,92]. There are
studies on utilizing high frequency information via frequency domain analyses [ 80,4], but they are
not designed on top of graph filtering perspectives. Dovonon et al. [20] find that Transformers are not
inherently low-pass filters, but oversmoothing depends on the eigenspectrum of the self-attention
layers. They propose a reparametrization of the Transformer weights, ensuring that oversmoothinng
does not occur.
Our paper addresses the oversmoothing problem with graph filters since the self-attention mechanism
is a basic graph filtering operation as seen in the previous subsection.
3 Graph Filter-based Self-Attention Layers
Let¯A∈[0,1]n×n, where nis the number of tokens in the input to the self-attention layer, be a
self-attention matrix. Since Transformers use multi-head self-attentions, there are multiple such
matrices. For simplicity, but without loss of generality, we discuss only one head in one layer.
From the GSP perspective, using ¯Aas the shift operator, a graph filter can be represented as a matrix
polynomial filter, as mentioned in Section 2.2. We aim to design this matrix polynomial filter using
the two lowest-order terms and one high-order term in Eq. (3). The following theorem shows that,
4despite using the three terms, the filter can be either a low-pass filter or a high-pass filter, depending
on the coefficient values.
Theorem 3.1 (Filter characteristics based on coefficient values) .Let¯Abe a self-attention matrix
interpreted as a graph with connected components. Consider the polynomial graph filter defined
byPK
k=0wk¯Ak, where w2, w3, . . . , w K−1= 0 and only w0,w1, and wKare non-zero. If the
coefficients wkfork= 0,1, Kare positive and their sum is 1, then the polynomial filter acts as a
low-pass filter, attenuating high-frequency components and promoting smoothness across the graph.
Conversely, if wk= (−α)kfork= 0,1, Kandα∈(0,1)with sufficient large K, the polynomial
filter exhibits high-pass filter behavior.
The proof of Theorem 3.1 is in Appendix G. Based on Theorem 3.1, we propose to use the follow-
ing graph filter, HGFSA, where the two lowest-order terms and one high-order term of the matrix
polynomial are used:
HGFSA =w0I+w1¯A+wK¯AK, (4)
where w0,w1,wKare coefficients and Kis a hyper-parameter where K≥2. The coefficients can
be learnable weights and we learn them with gradient descent algorithms.
Approximation of the high-order term. In Eq. (4), it is costly to calculate ¯AKwhen Kis large,
so we need a way to approximate the high-order term ¯AKin GFSA. We use the first-order Taylor
approximation at point a= 1for this purpose:
f(x)≃f(a) +f′(a)(x−a), (5)
thus, we approximate f(K) =¯AKas follows:
f(K) =¯AK≃f(1) + f′(1)(K−1). (6)
Computing the derivative of ¯AKdirectly at the evaluation point requires high computational costs.
To overcome this problem, we adopt the forward finite difference method, which approximates
derivatives with the difference term:
f′(K) =f(K+h)−f(K)
h=AK+h−AK
h, (7)
where the approximation error is O(h2). To balance the trade-off between computational efficiency2
and accuracy, we set h= 1. This method is inspired by the approach in Brouwer et al. [6], which
uses the difference term between two consecutive hidden states in discrete time to approximate the
derivatives. Therefore, we approximate ¯AKas:
f(K) =¯AK≃f(1) + ( ¯A2−¯A)(K−1)
=¯A+ (K−1)(¯A2−¯A).(8)
The approximation for ¯AKwith ¯Aand¯A2provides a simpler computation that can significantly
reduce the required computational resources and time.
GFSA: our graph filter-based self-attention. Our proposed graph filter-based self-attention
(GFSA) is defined with the graph filter ˜HGFSA as follows:
GFSA (X) := ˜HGFSAXW val, (9)
˜HGFSA =w0I+w1¯A+wK(¯A+ (K−1)(¯A2−¯A)), (10)
where the last term is the approximated ¯AKfrom Eq. (8). We replace the original self-attention
layer in various Transformers with the proposed graph filter-based layer without changing other
parts. Therefore, GFSA can be plugged into any Transformers that rely on the self-attention. For
pseudocode, see Appendix I.
2Calculating the power of a matrix for small hrequires a high computational cost since it is calculated by
diagonalizing the matrix or using Schur normal form [31].
54 Properties of GFSA
This section analyzes the theoretical error of the ¯AKapproximation used by GFSA and how GFSA
can mitigate oversmoothing. We also explain the meaning of GFSA’s high-order term in the context
of Transformers and provide comparisons of GFSA in other models.
Theoretical characteristics of approximation error in GFSA. We provide a theorem that provides
an upper bound on the error introduced by approximating the power of a matrix, specifically using
the first-order Taylor expansion. The following theorem specifically analyzes the error of matrix ¯AK
when approximated using a first-order Taylor expansion.
Theorem 4.1 (Error bound for approximated high-order term in GFSA) .Define the error term,
EK, as the difference between the exact value and approximated value of ¯AK, which is given by
EK=||¯AK−(¯A+ (K−1)(¯A2−¯A))||F, where || · || Fdenotes the Frobenius norm. Then, the
error bound can be shown that EK≤2√nK.
The error bound provides an upper limit for the difference between the actual value of ¯AKand its
approximation. The proof of Theorem 4.1 is in Appendix H. It shows theoretical validity for using
approximations to the high-order term in the filters of our GFSA. In terms of performance, we report
a difference of approximately ¯AKbetween the actual calculated values in Appendix J.
How to alleviate the oversmoothing problem? The key leading to the low/high pass filtering
behavior of our proposed filter is the coefficients {w0, w1, wK}— note that in the self-attention
of Transformers, w0=wK= 0andw1= 1. Since our method can learn any appropriate values
for them for a downstream task, it can be reduced to low-pass-only, high-pass-only, or combined
filters. According to Theorem 3.1, our graph polynomial filter can be said to be a low-pass filter when
w1, wKare positive and a high-pass filter when they are negative. Therefore, our method can learn the
appropriate coefficients {w0, w1, wK}for downstream tasks, so it can be reduced to a low-pass-only,
high-pass-only, or combined filter, alleviating the oversmoothing problem of self-attention.
The meaning of the high-order term in GFSA in the context of Transformers. Existing self-
attention only captures simple pairwise similarities between tokens and is limited in capturing
high-order dependencies. For example, given the two sentences, “Books are more expensive than
pencils” and “Books are cheaper than computers”, to understand the relationship between “computers”
and “pencils”, we need to capture the high-order dependencies connected through the “Book” token.
However, it is difficult to capture these high-order dependencies with traditional self-attention [ 103].
Therefore, from a Transformer perspective, the approximated ¯AKin GFSA can be interpreted as
being able to capture these high-order dependencies.
Comparison to Transformers. In the field of computer vision, there has been recent research on
adjusting the frequency response of ViT. HAT [ 4] creates adversarial examples by altering clean
images with high-frequency perturbations and jointly trains the ViT on clean images and adversarial
examples. Through this, they aim to solve the problem of the ViT being unable to capture high-
frequency by allowing us to capture the high-frequency components of the images. However, HAT
has the disadvantage of requiring more epochs than the existing ViT, as it must perform adversarial
training in some initial epochs and train normally in the remaining epochs. Wang et al. [80] use the
concept of DSP, which is a special case of GSP, to isolate the lowest frequency component in the
Fourier domain and use a filter learned by rescaling the low and high-frequency components. On the
other hand, our GFSA extends the concept to graph signal processing and redesigns self-attention
as a graph filter. While GFSA seeks to design a better graph filter by interpreting self-attention as a
graph filter, Shi et al. [71] are inspired by JKNet [ 91], and they solve the oversmoothing problem
by fusing the hidden vectors of each layer. However, their method has a limitation with memory
increasing, and they only applied it to BERT.
Comparison to GCNs. Comparisons to GCNs that can be interpreted as graph filters [39, 15, 24]
are inevitable. GFSA without a high-order term is analogous to ChebNet [ 15] with K= 1. In
addition, GFSA reduces to the vanilla GCN [ 39] when K= 1,w0= 0,w1= 1. GPR-GNN [ 12],
which approximates graph convolutions using the monomial basis, is identical to GFSA if it only
considers up to first order and additionally uses a K-order term and learns the coefficients. When we
6use only a high-order term and wKis learned to a negative value, GFSA can become similar to the
reaction-diffusion layer of GREAD [13], ¯AX +β(¯A−¯A2), depending on the higher order terms.
5 Experiments
In this section, we demonstrate the effectiveness of GFSA through a series of experiments. These
experiments encompass various tasks: i) language understanding and causal language modeling, ii)
image classification, iii) graph-level task, and iv) code classification. We replace the self-attention of
base Transformers in those fields with our GFSA. Our modification adds only tens to hundreds of
parameters, which are negligible in comparison with the original size of base models.
5.1 Experiments on Natural Language Understanding
Setting. We integrate GFSA into 3 pre-trained large language models: BERT, ALBERT, and
RoBERTa. We evaluate them on the GLUE benchmark, which includes 3 categories of natural
language understanding tasks: i) single-sentence, ii) similarity and paraphrasing, and iii) natural
language inference tasks. For each task, we select the best hyperparameters for GFSA, and the other
hyperparameters are fixed. The detailed experimental settings are in Appendix K.1.
Results. The results are shown in Table 1. When GFSA was plugged into backbones, average
performance scores improved across all models over pure backbones. This indicates that GFSA is
effective in both large models like BERT and RoBERTa, as well as relatively smaller models like
ALBERT. It is worth mentioning that in the case of RoBERTa finetuned on the CoLA dataset; there
is a significant margin increase from 60.34% to 64.11%, which is a 3.77% improvement with only
144 additional parameters. When compared to ContraNorm, GFSA shows a greater performance
improvement on average. Fig. 5 in Appendix C shows that these performance enhancements can be
attributed to addressing the oversmoothing issue through the designed graph filter.
Table 1: Results comparison on GLUE benchmark. Avgdenotes the average performance.
Method #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE Avg
BERT BASE [16] 110M 56.79 93.81 88.70 88.32 88.16 84.96/84.15 91.63 66.06 82.51
+ ContraNorm 110M 59.89 93.92 89.88 88.51 88.36 85.11/84.50 91.84 69.31 83.48
+ GFSA 110M 59.56 94.15 90.60 88.46 88.33 85.12 /85.06 91.95 68.95 83.58
ALBERT BASE [40] 11M 57.86 92.32 91.80 85.30 90.37 85.37 /84.37 91.76 76.90 84.01
+ ContraNorm 11M 57.45 93.00 92.83 87.78 90.55 85.06/84.57 92.28 78.70 84.69
+ GFSA 11M 60.21 93.23 92.47 87.79 90.63 85.29/ 84.92 92.17 78.70 85.05
RoBERTa BASE [44] 125M 60.34 94.84 92.28 88.86 89.99 87.94/87.30 92.57 78.70 85.87
+ ContraNorm 125M 63.06 95.41 93.17 88.91 90.34 87.88/87.40 92.82 80.51 86.61
+ GFSA 125M 64.11 95.41 93.52 89.09 90.35 87.99 /87.54 92.97 80.14 86.79
5.2 Experiments on Causal Language Modeling
Setting. We also validate the effectiveness of GFSA on causal language modeling problems. We
finetune GPT2 [ 61] on the following 3 datasets: Penn Treebank (PTB) [ 47], WikiText-2, and WikiText-
103 [ 50]. Following the evaluation method in Yao et al. [93], we finetune models for 15 epochs with
PTB, 4 epochs with WikiText-103, and 10 epochs with WikiText-2, and report the perplexity for
sensitivity metric. The detailed experimental settings are in Appendix L.1.
Table 2: Results comparison on GPT-2 finetuned with
GFSA. Avgdenotes the average performance.
Method #Params PTB WikiText-2 WikiText-103 Avg
GPT2 117M 19.513 20.966 15.939 18.806
+ GFSA 117M 19.450 20.923 15.919 18.764Results. Table 2 shows the perplexity
on PTB, WitiText-2, and WikiText-103.
Across all datasets, GPT2 with GFSA
consistently outperforms the vanilla
GPT2. Our GFSA improves the aver-
age perplexity from 18.806 to 18.764.
Note that performance improvements
are made with only 144 additional learnable parameters for 12 layers with 12 heads.
75.3 Experiments on Vision Transformers
Setting. We aim to demonstrate the efficacy of our GFSA across a spectrum of ViT backbones.
We choose DeiT [ 74], CaiT [ 75], and Swin [ 45] as the backbone, and the models are trained from
scratch. When training the 12-layer DeiT, we follow the same training recipe, hyperparameters, and
data augmentation from Touvron et al. [74]. For detailed experimental settings, see Appendix M.1.
Results. The experimental evaluations are summarized in Table 3. We compare various models on
the ImageNet-1k benchmark. The results show that the proposed GFSA successfully enhances DeiT,
CaiT, and Swin across all depth settings and training methods. GFSA provides additional parameters
less than 72 for 12-layer DeiT while improving top-1 accuracy by 1.63%. To sum up, we observed
that both shallow and deep ViTs can achieve the following benefits from GFSA: i) The filter response
shows GFSA can preserve higher-frequency representation (cf. Fig. 2 (a)) and ii) Fig. 2 (b) shows that
GFSA mitigates the increase in the cosine similarity of representation as the layer gets deeper. We
further compare with state-of-the-art models that use Fourier transforms rather than graph filters in
Appendix M.5. We also show results under the same settings as ContraNorm [ 28] in Appendix M.6.
Table 3: Results comparison on ImageNet-1k. Our full results with other models are in Appendix M.4.
Category Method Input Size #Layers #Params Top-1 Acc
TransformerDeiT-S [74] 224 12 22M 79.8
DeiT-S + AttnScale [80] 224 12 22M 80.7
DeiT-S + FeatScale [80] 224 12 22M 80.9
DeiT-S + ContraNorm [28] 224 12 22M 80.4
Swin-S [45] 224 12 50M 82.9
DeiT-S [74] 224 24 43M 80.5
CaiT-S [75] 224 24 47M 82.6
DeiT-S + AttnScale [80] 224 24 44M 81.1
DeiT-S + FeatScale [80] 224 24 44M 81.3
DeiT-S + ContraNorm [28] 224 24 43M 80.7
GFSADeiT-S + GFSA 224 12 22M 81.1
DeiT-S + GFSA 224 24 43M 81.5
CaiT-S + GFSA 224 24 47M 82.8
Swin-S + GFSA 224 12 50M 83.0
5.4 Experiments on Graph-level Tasks
Setting. To evaluate the efficacy of GFSA on graph-level tasks, we conduct experiments on a
broader range of datasets. We use datasets from Long-Range Graph Benchmark (LRGB) [ 21] (e.g.,
Peptide-func and Peptide-struct), Benchmarking GNNs [ 22] (e.g., ZINC, MNIST, CIFAR10), Open
Graph Benchmark (OGB) dataset [ 32] (e.g., Molhiv and MolTox21), and OGB-LSC dataset (i.e.,
PCQM4M-LSC) [ 33]. We choose Graphormer [ 94], Graph-ViT [ 30], and GPS [ 63] as our backbone
architectures, following their original experimental protocols for fair comparison. For GPS, we
replace its self-attention module with our GFSA while maintaining its best configuration and other
hyperparameters. For Graph-ViT, we apply GFSA to the Hadamard self-attention method, which He
et al. [30] propose as optimal. For a detailed experimental setting, see Appendix O.1.
Results. Tables 4, 5, and 6 show consistent performance improvements when GFSA is integrated
with backbone architectures. Graph-ViT + GFSA shows improvements on all datasets. On Peptide-
func, it achieves a 0.65% increase in AP. Notably, in PCQM4M, incorporating GFSA improves the
validation MAE by 7.20%. Due to space constraints, the results with standard deviation are included
in Appendix O.2.
5.5 Experiments on Automatic Speech Recognition
Setting. We conduct automatic speech recognition (ASR) experiments on the LibriSpeech3
dataset [ 55], which consists of audio recordings paired with their transcriptions. We use Branch-
3http://www.openslr.org/12
8Table 4: Results on ZINC
Method #Params MAE ( ↓)
Graphormer 500K 0.1240
+ GFSA 500K 0.1189Table 5: Results on PCQM4M and PCQM4Mv2
Method #ParamsPCQM4M PCQM4Mv2
Train ( ↓) Validate ( ↓) Train ( ↓) Validate ( ↓)
Graphormer 48.3M 0.0535 0.1286 0.0250 0.0862
+ GFSA 48.3M 0.0312 0.1193 0.0249 0.0860
Table 6: Experimental evalutation of GFSA plugged into GPS and Graph-ViT. Results marked
with†indicate settings where we conducted our own experiments due to unavailable Hadamard
self-attention performance in He et al. [30]’s paper.
MethodPeptide-func Peptide-struct MNIST CIFAR10 Molhiv MolTOX21 ZINC
AP (↑) MAE ( ↓) Accuracy ( ↑) Accuracy ( ↑) ROCAUC ( ↑) ROCAUC ( ↑) MAE ( ↓)
GPS 0.6535 ±0.0041 0.2500 ±0.0005 0.9805 ±0.0013 0.7230 ±0.0036 – – 0.070 ±0.004
+ GFSA 0.6593 ±0.0094 0.2496 ±0.0013 0.9814 ±0.0014 0.7244 ±0.0048 – – 0.069±0.002
Graph-ViT 0.6919 ±0.0085†0.2474 ±0.0016 0.9820 ±0.0005 0.6967 ±0.0040 0.7792 ±0.0149 0.7851 ±0.0077 0.0849 ±0.0047
+ GFSA 0.6964 ±0.0025 0.2461 ±0.0024 0.9826 ±0.0004 0.6987 ±0.0028 0.7830 ±0.0109 0.7895 ±0.0069 0.0845 ±0.0032
former [ 59] and a pure Transformer. For implementation, we follow the recipes of SpeechBrain [ 65]
and the detailed settings are in Appendix N.1.
Results. Table 7 compares word error rates (WERs) on LibriSpeech 100h and 960h. For 100h,
Transformer+GFSA achieves 10.30/25.30 on the test clean/other set, which is a 6.53% improvement
over the Transformer for the WER of the test clean. For 960h, Transformer+GFSA shows a WER
result of 2.31 in test clean, a 4.55% improvement over Transformer and Branchformer+GFSA
achieves 2.31/5.49 with an LM on the test clean/other sets. Fig. 8 in Appendix N.2 depicts the
learning curves of train loss and valid loss when using GFSA, showing the effectiveness of our
proposed filter.
Table 7: Results for ASR training on LibriSpeech 100h and 960h with GFSA
Method #ParamsLibriSpeech 100h LibriSpeech 960h
test-clean WER test-other WER test-clean WER test-other WER
Transformer 71.5M 11.02 25.42 2.42 5.50
+ GFSA 71.5M 10.30 24.30 2.31 5.49
Branchformer 109.8M 9.63 22.43 2.13 5.00
+ GFSA 109.8M 9.60 22.25 2.11 4.94
5.6 Experiments on Code ClassificationTable 8: Results on code classi-
fication. The number in ( ↑) indi-
cates the improvement rate.
Method Accuracy
RoBERTa 62.88
+ GFSA 64.39 (↑2.40%)
CodeBERT 64.31
+ GFSA 64.49 (↑0.12%)
PLBART 62.63
+ GFSA 62.96 (↑0.52%)
CodeT5-small 63.25
+ GFSA 63.69 (↑0.70%)
CodeT5-base 63.51
+ GFSA 64.75 (↑1.95%)Setting. We conduct a code defect detection task based on De-
vign dataset provided by Zhou et al. [105] . We use RoBERTa [ 44],
CodeBERT [ 23], PLBART [ 2], and CodeT5 [ 84] as our backbone
models. The detailed settings are in Appendix P.1.
Results. Table 8 shows the accuracy of all models; GFSA results
better than the base models. The biggest improvement is 2.40%
for RoBERTa. In the case of CodeT5-base, using GFSA shows an
accuracy of 64.75, an improvement of 1.95% from 63.51. CodeT5-
small+GFSA has only about 100 additional parameters compared
to CodeT5-small with 60M parameters, and even more impres-
sively, it surpasses the accuracy of CodeT5-base. The biggest
improvement is 2.40% for RoBERTa. In Appendix P.2, we include
case studies for this task. We also report the results of the code
clone detection task in Appendix Q.
96 Discussion on Runtime Overheads
Limitation. The introduction of our GFSA layer results in a slight increase in training and inference
time. We report the runtimes when plugging GFSA in Appendices R and S. For GLUE benchmark,
integrating GFSA into BERT enhances the average performance from 82.51% to 83.58% (see Table 1)
with more overhead of less than 36 seconds per epoch based on average training time (see Table 23).
Considering the improvements, the increases in training time are negligible.
DeiT-S + GFSA + GFSAeven79.580.080.581.081.5T op-1 Accuracy (%)79.881.181.0
500600700800
Runtime (s per epoch)
551814
595
Figure 3: Effectiveness of our selective
layer strategy on ImageNet-1k. This
shows out strategy’s ability to maintain
accuracy benefits while mitigating run-
time increases.GFSA in selected layers: a strategy to mitigate the limi-
tation. As GFSA requires more calculation than the orig-
inal self-attention, the runtime after using GFSA slightly
increases. Our experiments initially applied GFSA across
all Transformer layers (as discussed in Section 5); however,
to reduce computational load, we propose a selective ap-
plication strategy. For this purpose, GFSA is used only on
even-numbered layers. In Tables 35 to 39 of Appendix T,
the results show that this strategy effectively reduces run-
time increases while preserving comparable performance
to the full-layer GFSA integration. Notably, the selective
application of GFSA cuts the per-epoch runtime increase
by 26.90% relative to its full-layer application, with only
a 7.39% increase in runtime per epoch compared to the
backbone model in Table 39.
36.8 37 37.2 37.4 37.6 37.8 38
Accuracy (%)0100200300400500600700800Runtime (s per 1000 steps)
Trans.Trans.
+ GFSA
Lin.YOSO-EEff. Attn.Eff. Attn.
+ GFSA11.82x 
Faster
Figure 4: Performance ( x-axis), runtime
(y-axis), and GPU usage (circle sizes)
of various Transformers and integrated
GFSA on Long-Range benchmarkGFSA in linear Transformers. Although GFSA re-
quires additional computation for calculating ¯A2, we
explore integrating GFSA with linear attention vari-
ants to maintatin efficiency and scalability. Recent ap-
proaches [ 36,70] achieve linear complexity by reformu-
lating softmax operations and reordering matrix multipli-
cation in self-attention. We apply similar principles to
compute second-order self-attention efficiently, enabling
˜HGFSA calculation with linear complexity with respect
to sequence length. Fig. 4 shows the performance, run-
time and GPU usage changes when applying our GFSA to
Transformers with linear complexity. GFSA still improves
performance compared to the backbone model, while the
increase in time and GPU usage is minimal. Notably, when
GFSA is applied to Efficient Attention [ 70], the perfor-
mance is improved and the runtime is 11.82 times faster
than when GFSA is applied to the vanilla self-attention. This shows that GFSA can be effectively
implemented with linear complexity architectures while preserving its benefits and providing a
solution for addressing computational concerns.
7 Conclusion
Our proposed GFSA achieves high performance with improvements on a variety of tasks. GFSA is a
simple yet effective method that enriches self-attention in Transformers with more diverse frequency
information. This enables GFSA to address the oversmoothing problem and learn better latent
representations for downstream tasks. However, our GFSA does not bring significant overheads
in those Transformers’ empirical runtime complexity. One can use more complicated graph filters
to enhance accuracy more, but our goal is to find a balance between accuracy enhancements and
overheads in runtime complexity.
We believe that GFSA suggests a promising new direction for improving Transformers. GFSA can be
implement with simple way and used in conjunction with other techniques to further improve the
performance of Transformers. Considering the ongoing advancements in large language models, such
as GPT-4 [ 1] and LLaMA [ 76], we hope that our approach may offer new insights for enhancing their
performance and efficiency.
10Acknowledgements
N. Park was partly supported by the Korea Advanced Institute of Science and Technology (KAIST)
grant funded by the Korea government (MSIT) (No. G04240001, Physics-inspired Deep Learning,
10%), Institute for Information & Communications Technology Planning & Evaluation (IITP) grants
funded by the Korea government (MSIT) (No. RS-2020-II201361, Artificial Intelligence Graduate
School Program (Yonsei University), 20%; No. RS-2024-00457882, AI Research Hub Project, 50%),
and Samsung Electronics Co., Ltd. (No. G01240136, KAIST Semiconductor Research Fund (2nd),
10%). K. Lee acknowledges support from the U.S. National Science Foundation under grant IIS
2338909. Dr. Trask acknowledges funding under the Department of Energy under the Mathematical
Multifaceted Integrated Capability Centers program.
References
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774 , 2023.
[2]Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training
for program understanding and generation. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , pages 2655–2668, 2021.
[3]Ameen Ali, Tomer Galanti, and Lior Wolf. Centered self-attention layers. arXiv preprint
arXiv: 2306.01610 , 2023.
[4]Jiawang Bai, Li Yuan, Shu-Tao Xia, Shuicheng Yan, Zhifeng Li, and Wei Liu. Improving
vision transformers by revisiting high-frequency components. In European Conference on
Computer Vision , pages 1–18. Springer, 2022.
[5]Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing
textual entailment challenge. TAC, 7:8, 2009.
[6]Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. GRU-ODE-Bayes: Con-
tinuous modeling of sporadically-observed time series. In Advances in Neural Information
Processing Systems (NeurIPS) , 2019.
[7]Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. arXiv preprint
arXiv:2006.13318 , 2020.
[8]Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017
task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv
preprint arXiv:1708.00055 , 2017.
[9]Nuo Chen, Linjun Shou, Ming Gong, Jian Pei, Bowen Cao, Jianhui Chang, Daxin Jiang, and
Jia Li. Alleviating over-smoothing for unsupervised sentence representation. arXiv preprint
arXiv:2305.06154 , 2023.
[10] Tianlong Chen, Zhenyu Zhang, Yu Cheng, Ahmed Awadallah, and Zhangyang Wang. The
principle of diversity: Training stronger vision transformers calls for reducing all levels of
redundancy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12020–12030, 2022.
[11] Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs, 2018.
[12] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized
PageRank graph neural network. In Proceedings of the International Conference on Learning
Representations (ICLR) , 2021.
[13] Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. Gread: Graph neural
reaction-diffusion networks. In International Conference on Machine Learning (ICML) , pages
5722–5747. PMLR, 2023.
11[14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-
attention layers with application to graph neural networks. In International Conference on
Machine Learning (ICML) , pages 2456–2466. PMLR, 2021.
[15] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems (NeurIPS) , 2016.
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://aclanthology.org/N19-1423 .
[17] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
InThird International Workshop on Paraphrasing (IWP2005) , 2005.
[18] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need:
Pure attention loses rank doubly exponentially with depth. In International Conference on
Machine Learning (ICML) , pages 2793–2803. PMLR, 2021.
[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. In
Proceedings of the International Conference on Learning Representations (ICLR) , 2021.
[20] Gbètondji JS Dovonon, Michael M Bronstein, and Matt J Kusner. Setting the record straight
on transformer oversmoothing. arXiv preprint arXiv:2401.04301 , 2024.
[21] Vijay Prakash Dwivedi, Ladislav Rampášek, Mikhail Galkin, Ali Parviz, Guy Wolf, Anh Tuan
Luu, and Dominique Beaini. Long range graph benchmark. In Thirty-sixth Conference
on Neural Information Processing Systems Datasets and Benchmarks Track , 2022. URL
https://openreview.net/forum?id=in7XC5RcjEn .
[22] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio,
and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning
Research , 24(43):1–48, 2023.
[23] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,
Bing Qin, Ting Liu, Daxin Jiang, et al. CodeBERT: A pre-trained model for programming
and natural languages. In Findings of the Association for Computational Linguistics: EMNLP
2020 , pages 1536–1547, 2020.
[24] Johannes Gasteiger, Stefan Weißenberger, and Stephan Günnemann. Diffusion improves graph
learning. In Advances in Neural Information Processing Systems (NeurIPS) , volume 32, 2019.
[25] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Vision transformers
with patch diversification. arXiv preprint arXiv:2104.12753 , 2021.
[26] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural
networks. In International Conference on Machine Learning (ICML) , pages 1764–1772.
PMLR, 2014.
[27] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han,
Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented
transformer for speech recognition. arXiv preprint arXiv:2005.08100 , 2020.
[28] Xiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang. ContraNorm: A contrastive learning
perspective on oversmoothing and beyond. In Proceedings of the International Conference on
Learning Representations (ICLR) , 2023.
12[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
[30] Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson.
A generalization of vit/mlp-mixer to graphs. In International conference on machine learning
(ICML) , pages 12724–12745. PMLR, 2023.
[31] Nicholas J Higham. Functions of matrices: theory and computation . SIAM, 2008.
[32] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele
Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.
Advances in neural information processing systems (NeurIPS) , 33:22118–22133, 2020.
[33] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. OGB-
LSC: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430 ,
2021.
[34] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely
connected convolutional networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 4700–4708, 2017.
[35] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman.
Zinc: a free tool to discover chemistry for biology. Journal of chemical information and
modeling , 52(7):1757–1768, 2012.
[36] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers
are rnns: Fast autoregressive transformers with linear attention. In International conference on
machine learning , pages 5156–5165. PMLR, 2020.
[37] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over) smoothing.
InAdvances in Neural Information Processing Systems (NeurIPS) , volume 35, pages 2268–
2281, 2022.
[38] Jayoung Kim, Yehjin Shin, Jeongwhan Choi, Hyowon Wi, and Noseong Park. Polynomial-
based self-attention for table representation learning. In Ruslan Salakhutdinov, Zico Kolter,
Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp,
editors, Proceedings of the 41st International Conference on Machine Learning , volume 235
ofProceedings of Machine Learning Research , pages 24509–24526. PMLR, 21–27 Jul 2024.
URL https://proceedings.mlr.press/v235/kim24ae.html .
[39] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In Proceedings of the International Conference on Learning Representations (ICLR) ,
2017.
[40] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv
preprint arXiv:1909.11942 , 2019.
[41] Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, and
Junaid Qadir. Transformers in speech processing: A survey. arXiv preprint arXiv:2303.11607 ,
2023.
[42] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens
with fourier transforms. arXiv preprint arXiv:2105.03824 , 2021.
[43] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension. In Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871–7880,
2020.
13[44] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert
pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.
[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
of the IEEE/CVF international conference on computer vision , pages 10012–10022, 2021.
[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 , 2017.
[47] Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. 1993.
[48] Antonio G. Marques, Santiago Segarra, and Gonzalo Mateos. Signal processing on directed
graphs: The role of edge directionality when processing and learning from network data. IEEE
Signal Processing Magazine , 37(6):99–116, 2020. doi: 10.1109/MSP.2020.3014597.
[49] Sohir Maskey, Raffaele Paolino, Aras Bacho, and Gitta Kutyniok. A fractional graph laplacian
approach to oversmoothing. In Advances in Neural Information Processing Systems (NeurIPS) ,
2023.
[50] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843 , 2016.
[51] Luis Müller, Mikhail Galkin, Christopher Morris, and Ladislav Rampášek. Attending to graph
transformers. Transactions on Machine Learning Research , 2024. ISSN 2835-8856. URL
https://openreview.net/forum?id=HhbqHBBrfZ .
[52] Tam Nguyen, Tan Nguyen, and Richard Baraniuk. Mitigating over-smoothing in transformers
via regularized nonlocal functionals. In Advances in Neural Information Processing Systems
(NeurIPS) , volume 36, 2023.
[53] Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and
Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of
rank collapse. In Advances in Neural Information Processing Systems (NeurIPS) , volume 35,
pages 27198–27211, 2022.
[54] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for
node classification. In Proceedings of the International Conference on Learning Representa-
tions (ICLR) , 2020.
[55] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an
asr corpus based on public domain audio books. In 2015 IEEE international conference on
acoustics, speech and signal processing (ICASSP) , pages 5206–5210. IEEE, 2015.
[56] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk,
and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech
recognition. Interspeech 2019 , 2019.
[57] Badri Patro and Vijay Agneeswaran. Scattering vision transformer: Spectral mixing matters.
InAdvances in Neural Information Processing Systems (NeurIPS) , volume 36, 2023.
[58] Badri N Patro, Vinay P Namboodiri, and Vijay Srinivas Agneeswaran. Spectformer: Frequency
and attention is what you need in a vision transformer. arXiv preprint arXiv:2304.06446 , 2023.
[59] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe. Branchformer: Parallel
mlp-attention architectures to capture local and global context for speech recognition and
understanding. In International Conference on Machine Learning (ICML) , pages 17627–17643.
PMLR, 2022.
[60] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018.
14[61] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[62] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.
[63] Ladislav Rampášek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and
Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. In Advances in
Neural Information Processing Systems (NeurIPS) , volume 35, pages 14501–14515, 2022.
[64] Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networks
for image classification. In Advances in Neural Information Processing Systems (NeurIPS) ,
volume 34, pages 980–993, 2021.
[65] Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren
Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh
Chou, Sung-Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, François Grondin,
William Aris, Hwidong Na, Yan Gao, Renato De Mori, and Yoshua Bengio. SpeechBrain: A
general-purpose speech toolkit, 2021. arXiv:2106.04624.
[66] T. Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A survey on oversmooth-
ing in graph neural networks. arXiv preprint arXiv: Arxiv-2303.10993 , 2023.
[67] Aliaksei Sandryhaila and José MF Moura. Discrete signal processing on graphs. IEEE
transactions on signal processing , 61(7):1644–1656, 2013.
[68] Aliaksei Sandryhaila and Jose MF Moura. Discrete signal processing on graphs: Frequency
analysis. IEEE Transactions on Signal Processing , 62(12):3042–3054, 2014.
[69] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green AI. Communications of
the ACM , 63(12):54–63, 2020.
[70] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention:
Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on
applications of computer vision , pages 3531–3539, 2021.
[71] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen Lee,
and James T Kwok. Revisiting over-smoothing in bert from the perspective of graph. In
Proceedings of the International Conference on Learning Representations (ICLR) , 2022.
[72] Yehjin Shin, Jeongwhan Choi, Hyowon Wi, and Noseong Park. An attentive inductive bias for
sequential recommendation beyond the self-attention. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 38, pages 8984–8992, 2024.
[73] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y
Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a
sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural
language processing , pages 1631–1642, 2013.
[74] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Hervé Jégou. Training data-efficient image transformers & distillation through attention. In
International Conference on Machine Learning (ICML) , pages 10347–10357. PMLR, 2021.
[75] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou.
Going deeper with image transformers. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 32–42, 2021.
[76] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural
information processing systems (NeurIPS) , volume 30, 2017.
15[78] Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and
Yoshua Bengio. Graph Attention Networks. In Proceedings of the International Conference
on Learning Representations (ICLR) , 2018.
[79] Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Multi-hop attention graph neural
network. In IJCAI , 2021.
[80] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing
in deep vision transformers via the fourier domain analysis: From theory to practice. In
Proceedings of the International Conference on Learning Representations (ICLR) , 2022.
[81] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
[82] Wei Wang, Ming Yan, and Chen Wu. Multi-granularity hierarchical attention fusion networks
for reading comprehension and question answering. arXiv preprint arXiv:1811.11934 , 2018.
[83] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. Detecting code clones with graph neural
network and flow-augmented abstract syntax tree. In 2020 IEEE 27th International Conference
on Software Analysis, Evolution and Reengineering (SANER) , pages 261–271, 2020. doi:
10.1109/SANER48275.2020.9054857.
[84] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. CodeT5: Identifier-aware unified
pre-trained encoder-decoder models for code understanding and generation. In Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 8696–
8708, 2021.
[85] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability
judgments. Transactions of the Association for Computational Linguistics , 7:625–641, 2019.
[86] Ross Wightman. Pytorch image models. https://github.com/rwightman/
pytorch-image-models , 2019.
[87] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus
for sentence understanding through inference. arXiv preprint arXiv:1704.05426 , 2017.
[88] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.
CvT: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 22–31, 2021.
[89] Xinyi Wu, Amir Ajorlou, Zihui Wu, and Ali Jadbabaie. Demystifying oversmoothing in
attention-based graph neural networks. In Advances in Neural Information Processing Systems
(NeurIPS) , 2023.
[90] Xinyi Wu, Zhengdao Chen, William Wei Wang, and Ali Jadbabaie. A non-asymptotic analysis
of oversmoothing in graph neural networks. In Proceedings of the International Conference
on Learning Representations (ICLR) , 2023.
[91] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and
Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In
International Conference on Machine Learning (ICML) , pages 5453–5462, 2018.
[92] Hanqi Yan, Lin Gui, Wenjie Li, and Yulan He. Addressing token uniformity in transformers
via singular value transformation. In Uncertainty in Artificial Intelligence , pages 2181–2191.
PMLR, 2022.
[93] Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, and Yuxiong
He. Random-ltd: Random and layerwise token dropping brings efficient training for large-scale
transformers. arXiv preprint arXiv:2211.11586 , 2022.
[94] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,
and Tie-Yan Liu. Do transformers really perform badly for graph representation? In Advances
in Neural Information Processing Systems (NeurIPS) , volume 34, pages 28877–28888, 2021.
16[95] Youn-Yeol Yu, Jeongwhan Choi, Woojin Cho, Kookjin Lee, Nayong Kim, Kiseok Chang,
ChangSeung Woo, ILHO KIM, SeokWoo Lee, Joon Young Yang, SOOYOUNG YOON, and
Noseong Park. Learning flexible body collision dynamics with hierarchical contact mesh
transformer. In The Twelfth International Conference on Learning Representations , 2024.
URL https://openreview.net/forum?id=90yw2uM6J5 .
[96] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay,
Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from
scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer
vision , pages 558–567, 2021.
[97] Zhanpeng Zeng, Yunyang Xiong, Sathya Ravi, Shailesh Acharya, Glenn M Fung, and Vikas
Singh. You only sample (almost) once: Linear cost self-attention via bernoulli sampling. In
International conference on machine learning , pages 12321–12332. PMLR, 2021.
[98] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram,
Yizhe Zhang, Jiatao Gu, and Joshua M. Susskind. Stabilizing transformer training by preventing
attention entropy collapse. In Proceedings of the 40th International Conference on Machine
Learning (ICML) , volume 202, pages 40770–40803. PMLR, 2023.
[99] Aston Zhang, Alvin Chan, Yi Tay, Jie Fu, Shuohang Wang, Shuai Zhang, Huajie Shao,
Shuochao Yao, and Roy Ka-Wei Lee. On orthogonality constraints for transformers. In
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing (Volume 2: Short
Papers) , pages 375–382, 2021.
[100] Ying Zhang, Mohammad Pezeshki, Philémon Brakel, Saizheng Zhang, César Laurent, Yoshua
Bengio, and Aaron Courville. Towards end-to-end speech recognition with deep convolutional
neural networks. Interspeech 2016 , 2016.
[101] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin
Hou, and Jiashi Feng. DeepViT: Towards deeper vision transformer. arXiv preprint
arXiv:2103.11886 , 2021.
[102] Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin
Hou, and Jiashi Feng. Refiner: Refining self-attention for vision transformers. arXiv preprint
arXiv:2106.03714 , 2021.
[103] Haoyi Zhou, Siyang Xiao, Shanghang Zhang, Jieqi Peng, Shuai Zhang, and Jianxin Li.
Jump self-attention: Capturing high-order statistics in transformers. In Advances in Neural
Information Processing Systems (NeurIPS) , volume 35, pages 17899–17910, 2022.
[104] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng
Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and
applications. AI open , 1:57–81, 2020.
[105] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective
vulnerability identification by learning comprehensive program semantics via graph neural
networks. In Advances in neural information processing systems (NeurIPS) , volume 32, 2019.
[106] Chunya Zou, Andi Han, Lequan Lin, and Junbin Gao. A simple yet effective svd-gcn for
directed graphs. arXiv preprint arXiv:2205.09335 , 2022.
17Appendix
18A Reproducibility Statement
To ensure the reproducibility and completeness of this paper, we include the Appendix with 12
sections. Appendix I provides our PyTorch-style pseudo code for our GFSA method. The pseudo
code helps to implement our GFSA to any Transformers used a pure self-attention. All experiments
in the paper are reproducible with additional implementation details provided in Appendices K to Q.
B Broader Impact
In terms of the broader impact of this research on society, we do not see the very negative impacts
that might be expected. However, this paper may have implications for the carbon footprint and
accessibility of learning algorithms. The computations required for machine learning research are
rapidly growing, resulting in a larger carbon footprint [ 69]. Our study improves performance and
increases runtime very slightly, but the runtime increase is not very significant. However, in future
research, it will also be important to study and improve our GFSA by taking carbon footprints into
account.
GFSA improves the performance of existing Transformer-based models, which can have many
positive impacts on society through services that utilize natural language processing, computer vision,
and speech recognition. However, it will also be important to improve GFSA by considering other
dimensions of AI, such as robustness to adversarial examples, fairness, and explainability.
C Oversmoothing and Additional Visualizations
In Fig. 2, we show the visualizations of oversmoothing characteristics in DeiT. We also provide
visualizations in other domains. We show the filter response, cosine similarity, and singular value
of BERT finetuned on STS-B dataset of GLUE tasks in Fig. 5 and Graphormer finetuned on ZINC
dataset in Fig. 6.
To characterize self-attention, we first analyze the filter response of self-attention in the frequency
domain. We follow the method used by Wang et al. [80] for spectral visualization of the self-attention
matrix. As shown in Fig. 2 (a), DeiT has a near-zero magnitude for the high frequencies, which is
characteristic of a low-frequency filter and is likely to result in oversmoothing when applied multiple
times.
We follow the calculation method of Guo et al. [28] for cosine similarity. As shown in Fig. 2 (b),
the higher similarity as the layers of the model get deeper is related to the oversmoothing problem.
To further analyze this issue, we also consider the dimensionality collapse in Transformer-based
models. We plot the singular value distribution of the feature in the last block. As shown in Fig. 2 (c),
insignificant, near-zero values dominate the feature distribution. As layers get deeper, the similarity
of features increases and dimensional collapse occurs. The oversmoothing problem is the same in
BERT and Graphormer, as shown in Fig. 5 and Fig. 6.
−40 −20 0 20 40
Frequency0.00.20.40.60.81.0Normalized MagnitudeBERT
BERT + GFSA
(a) Filter reponse
1 4 8 12
Layer Index0.40.60.8Cosine Similarity
BERT
BERT + GFSA (b) Cosine Similarity
0 10 20 30 40 50 60
Singular Value Index0.00.30.60.9Normalized Singular ValueBERT
BERT + GFSA (c) Singular Value
Figure 5: Filter frequency response, cosine similarity, and singular values on STS-B for BERT and
BERT+GFSA
19−20 −15 −10 −5 0 510 15
Frequency0.00.20.40.60.81.0Normalized MagnitudeGraphormer
Graphormer + GFSA(a) Filter reponse
1 4 8 12
Layer Index0.00.20.40.60.8Cosine Similarity
Graphormer
Graphormer + GFSA (b) Cosine Similarity
0 2 4 6 8
Singular Value Index0.00.30.60.9Normalized Singular ValueGraphormer
Graphormer + GFSA (c) Singular Value
Figure 6: Filter frequency response, cosine similarity, and singular values on ZINC for Graphormer
and Graphormer+GFSA
D Analysis of Frequency Responses with Visualizations
We analyze the frequency responses, which represent the impact of learned coefficients, for all 12
layers of BERT BASE with and without GFSA. From Fig. 7, our analysis reveals that GFSA learns
various filter types between layers. In early layers, we observe a tendency towards low-pass filtering,
with prominent peaks at low frequencies. This aligns with the need for broader feature extraction in
initial layers. The middle layers show a mix of low-pass and high-pass characteristics, with more
complex frequency responses. This suggests GFSA is learning to balance between feature extraction
and refinement. In deep layers, there is a noticeable shift towards higher frequency responses,
indicating a move towards high-pass filtering. This shift supports our claim that GFSA can mitigate
oversmoothing in deeper layers. BERT BASE+GFSA shows a consistently higher magnitude response
at higher frequencies, especially in deeper layers, compared to vanilla BERT. In other word, vanilla
self-attention works primarily as a low-pass filter, while GFSA utilizes a wider range of frequencies.
−40 −20 0 20 400.00.20.40.60.81.0BERT
BERT + GFSA
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
−40 −20 0 20 400.00.20.40.60.81.0
FrequencyNormalized Magnitude
Figure 7: Visualization of the frequency responses for all 12 layers of BERT trained on STS-B dataset.
The top-left figure corresponds to the first layer, and the bottom-right figure corresponds to the last
layer.
20E Frequency Analyses in the Singular Value Domain
Graph signal processing (GSP) [ 67,68] can be understood as a generalized concept of DSP — in
other words, DSP is a special case of GSP where a line graph with nnodes is used and therefore, the
graph Fourier transform (GFT) of the line graph is identical to the discrete Fourier transform.
In the definition of GFT, we assume that the graph shift operator (GSO) Sis diagonalizable. Consid-
ering the eigendecomposition of the GSO S=V⊺ΛVwith eigenvector V, we can write the graph
filter output as follows:
y=KX
k=0wkSkx=KX
k=0V⊺wkΛkV x=V⊺ KX
k=0wkΛk
V x=V⊺g(Λ)V x, (11)
where x∈Rnis a 1-dimensional graph signal, Λis a diagonal matrix with eigenvalues, and
wk∈[−∞,∞]is a coefficient.
However, one can use the singular value decomposition, when the GSO is not diagonalizable
but symmetrically normalized, instead of the eigendecomposition [ 49]. Both the singular value
decomposition and the eigendecomposition project the original signal onto a set of basis, but they use
different basis sets. In the singular value decomposition, we sort the set of basis in ascending order of
their eigenvalues, and perform frequency domain-like analyses [106, 49].
Since the self-attention matrix’s row-wise sum is always 1, the following is the case: ¯A=D−1A=
1
nA,where nis the number of tokens. Maskey et al. [49] define the following symmetrically
normalized adjacency matrix (SNA): D−1/2
inAD−1/2
out.Since the degree of every node is nin the
self-attention matrix, the following is the case: D−1/2
inAD−1/2
out =D−1/2AD−1/2=1√nA1√n=
1
nA=¯A.Therefore, the self-attention matrix is a special case of SNAs.
F Matrix Polynomial vs. Graph Fourier Transform
There are two paradigms of implementing graph filters: i) matrix polynomial, which does not require
diagonalizability, and ii) graph Fourier transform, which uses the eigendecomposition for diagonaliz-
able adjacency matrices or uses the Jordan decomposition or the singular value decomposition for
non-diagonalizable adjacency matrices.
Those two paradigms have their own weaknesses: i) the matrix polynomial approach requires explicit
matrix multiplications, and ii) the graph Fourier transform approach requires expansive spectral
decompositions. The matrix polynomial is preferred when there are not many matrix multiplications.
Otherwise, the graph Fourier transform approach may be better since the matrix multiplication can be
simplified after the decomposition.
Among those two, we use the first matrix polynomial approach with only three non-zero coefficients
{w0, w1, wK}since it does not require the complicated spectral decomposition. Since we do not rely
on any explicit spectral decomposition but on the matrix polynomial, any adjacency matrix can be
used.
G Proof of Theorem 3.1
Theorem 3.1 (Filter characteristics based on coefficient values) .Let¯Abe a self-attention matrix
interpreted as a graph with connected components. Consider the polynomial graph filter defined
byPK
k=0wk¯Ak, where w2, w3, . . . , w K−1= 0 and only w0,w1, and wKare non-zero. If the
coefficients wkfork= 0,1, Kare positive and their sum is 1, then the polynomial filter acts as a
low-pass filter, attenuating high-frequency components and promoting smoothness across the graph.
Conversely, if wk= (−α)kfork= 0,1, Kandα∈(0,1)with sufficient large K, the polynomial
filter exhibits high-pass filter behavior.
Note that without filtering, the singular value ratio isσ0
i/σ0
1= 1 . In the case where
|g(σi)/g(σ1)|<1∀i≥2, it implies that after applying the graph filter g, the lowest frequency
component further dominates, indicating that the graph filter acts as a low-pass filter. Conversely, in
21the case where |g(σi)/g(σ1)|>1∀i≥2, it implies that after applying the graph filter g, the lowest
frequency component σino longer dominates, indicating that the graph filter acts as a high-pass filter.
Proof. We prove the low-pass filter result. For the case where w0,w1, andwKare positive and their
sum is 1, we show that
|g(σ1)|=|w0+w1+wK|= 1 (12)
Hence, proving Theorem G is equivalent to show |g(σi)|<1.
|g(σi)|=w0+w1σi+wK(σi+ (K−1)(σ2
i−σi))<|w0+w1σi+wKσi|=σi<1(13)
since σi+ (K−1)(σ2
i−σi) =σi((K−1)σi−(K−2))< σi((K−1)−(K−2)) = σi
For the high-pass filter result, when wk= (−α)k/(k+ 1) where k= 0,1, Kandα∈(0,1), then
we show that when w0= 1, w1=−α/2,
limK→∞g(σi)
limK→∞g(σ1)=limK→∞w0+w1σi+wK(σi+ (K−1)(σ2
i−σi))
limK→∞w0+w1+wK(1 + ( K−1)(1−1))(14)
=limK→∞1−α
2σi+(−α)K
(K+1)(σi+ (K−1)(σ2
i−σi))
limK→∞1−α
2+(−α)K
(K+1)(15)
=1−α
2σi+ (lim K→∞(−α)K
(K+1)(σi+ (K−1)(σ2
i−σi)))
1−α
2(16)
=1−α
2σi
1−α
2>1 (17)
since
lim
K→∞(−α)K
(K+ 1)(σi+ (K−1)(σ2
i−σi)) = lim
K→∞(−α)K
(K+ 1)(K−1)(σ2
i−σi) (18)
= lim
K→∞(−α)K(K+ 1)
(K−1)(σ2
i−σi) = 0 (19)
It shows that the graph filter with wk= (−α)k/(k+ 1) fork= 1,2, Kemphasizes high-frequency
components and acts as a high-pass filter.
This proof supports that the behavior of the polynomial filter as either a low-pass or high-pass filter
directly depends on the sign and values of the coefficients, as specified in Theorem 3.1.
H Proof of Theorem 4.1
Proof. The Frobeinus norm of the self-attention is directly related to how far the softmax probabilities
are from being uniform. For any matrix M∈Rm×n, we have
∥softmax (M)∥F=r
m+Pm
i=1dχ2(Si, Un)
n, (20)
where Siis the i-th row of softmax (M),Unis the uniform distribution over nelements, and
dχ2(p, q) =P
iqi(pi/qi−1)2is the χ2-divergence between pandq[14]. The Frobenius norm
is maximized when the whole mass of the probabilities is on one element, which is a case for
dχ2(Si, Un) =n−1and∥softmax (M)∥F=√m. Therefore, we can calculate the upper bound of
Frobenius norm for ¯Aas follows:
∥¯A∥F≤√n. (21)
Note that ¯A∈Rn×nis a right-stochastic matrix normalized with row-wise softmax: i) all the
elements of ¯Alie within [0, 1], and ii) the row-sum in ¯Ais equal to 1. Since the self-attention matrix
22is a right-stochastic matrix, the power of the self-attention is also a right-stochastic matrix. Therefore,
Eq. (21) is also hold for ¯AKas follows:
∥¯AK∥F=sX
i,j¯AK
i,j≤sX
i,j¯Ai,j=∥¯A∥F≤√n. (22)
Now, considering the error term EKas given by Theorem 4.1, and applying the triangle inequality
for matrix norms:
EK=∥¯AK−(¯A+ (K−1)(¯A2−¯A))∥F (23)
≤ ∥¯AK∥F+∥¯A∥F+ (K−1)(∥¯A2∥F+∥¯A∥F) (24)
≤√n+√n+ (K−1)(√n+√n) = 2√nK. (25)
I Implementation of GFSA
The pseudo code of our GFSA is shown in Algorithm 1. For implementation, w0andw1can be set
as hyperparameters optionally.
Algorithm 1 PyTorch-style pseudocode for GFSA
w_0 = torch.zeros(h)
w_1 = torch.ones(h)
w_K = torch.zeros(h)
I = torch.eyes(n)[None, None, ...]
def GFSA (att, K)
att: original self-attention
att_K: high order term
att_K = att + (K-1) * (torch.mm(att,att)-att)
gf_att: GFSA attention
gf_att = w_0[None, :, None, None] * I
+ w_1[None, :, None, None] * att
+ w_K[None, :, None, None] * att_K
return gf_att
J Comparison with Actual and Approximated High-order Terms
To compare the impact of the actual ¯AKand the approximated ¯AKin terms of accuracy, we
experimented with BERT on GLUE and the results are summarized in Table 9. BERT BASE+¯AK
denotes using the exactly calculated ¯AKinstead of the approximated ¯AK.
Table 9: Comparison of performance using the exactly calculated ¯AKvs. the approximated ¯AKfor
GLUE tasks
Datasets #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE Avg
BERT BASE [16] 110M 56.79 93.81 88.70 88.32 88.16 84.96/84.15 91.63 66.06 82.51
+ GFSA (approximated ¯AK) 110M 59.56 94.15 90.60 88.46 88.33 85.12/85.06 91.95 68.95 83.58
+ GFSA (actual ¯AK) 110M 59.85 94.27 89.80 88.43 88.32 84.95/84.89 91.76 68.23 83.39
23K Natural Language Understanding
K.1 Detailed Experimental Settings
We integrate GFSA into 3 pre-trained large language models: BERT, ALBERT, and RoBERTa. We
evaluate them on the GLUE benchmark, which includes 3 categories of natural language understand-
ing tasks: i) single-sentence tasks CoLA and SST-2; ii) similarity and paraphrasing tasks MRPC,
QQP, and STS-B; iii) natural language inference tasks MNLI, QNLI, and RTE. For MNLI task, we
experiment on both the matched (MNLI-m) and mismatched (MNLI-mm) versions. Following Devlin
et al. [16], we report Matthews correlation for CoLA, F1 scores for QQP and MRPC, Spearman
correlations for STS-B, and accuracy scores for the other tasks. For each task, we select the best
hyperparameters for GFSA, and the other hyperparameters are fixed. We compare our GFSA with
ContraNorm [ 28], one of the related methods that address oversmoothing. We finetune ContraNorm
with the recommended hyperparameters in Guo et al. [28]. We initialize with a pre-trained language
model and finetune with added GFSA for 5 epochs.
Dataset. The benchmark datasets we used are listed below.
•CoLA. The Corpus of Linguistic Acceptability [ 85] consists of English acceptability judg-
ments drawn from books and journal articles. The target task is a binary classification task,
and each sentence is determined to be grammatically acceptable or not.
•SST-2. The Stanford Sentiment Treebank [ 73] is a dataset in which each sentence is sourced
from movie reviews and accompanied by human annotations of their sentiment. The target
task is to classify binary sentiments for a single sentence.
•MRPC. The Microsoft Research Paraphrase Corpus [ 17] is a corpus of sentence pairs,
which are automatically extracted from online news sources and annotated by humans. The
target is to determine whether the sentences in the pair are semantically equivalent.
•QQP. The Quora Question Pairs [ 11] dataset is a collection of question pairs from the
community question-answering website Quora. The target is to determine whether the
questions in the pair are semantically equivalent.
•STS-B. The Semantic Textual Similarity Benchmark [ 8] is a collection of sentence pairs
drawn from news headlines, video and image captions, and natural language inference data
with human annotation. The target is a regression task to predict a similarity score from 0 to
5.
•MNLI. The Multi-Genre Natural Language Inference Corpus [ 87] is a crowdsourced col-
lection of sentence pairs with textual entailment annotations. Given a premise sentence
and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis
(entailment), contradicts the hypothesis (contradiction), or neither (neutral). The standard
test set consists of private labels from the authors and evaluates both the matched (in-domain)
and mismatched (cross-domain) sections.
•QNLI. The Stanford Question Answering [ 82] dataset is a question-answering dataset
consisting of question-paragraph pairs, where one of the sentences in the paragraph contains
the answer to the corresponding question written by an annotator. The task is to determine
whether the context sentence contains the answer to the question.
•RTE. The Recognizing Textual Entailment [ 5] dataset comes from a series of annual textual
entailment challenges. The target task is a binary entailment classification task.
BERT. BERT [ 16] consists with 12 layers, 12 heads, 768 hidden size, 512 maximum sequence
length, and MLP dimension of 3072.
ALBERT. ALBERT [ 40] consists of 12 layers, 12 heads, 768 hidden dimensions, 512 maximum
sequence length, 128 embedding dimensions, and MLP dimension of 3072.
RoBERTa. RoBERTa [ 44] consists of 12 layers, 12 heads, 768 hidden size, 514 maximum sequence
length, and MLP dimension of 3072.
24Training. For implementation, we adopt HuggingFace framework. We trained all models with 5
epochs with 32 batch size. The linear learning rate decay is used and initial learning rate is set to
2×10−5. We use AdamW [ 46] optimizer, and weight decay is set to 0. All models are trained on 1
GPU and of NVIDIA RTX A5000 24GB.
K.2 Sensitivity to K
In this section, we explore the influence of the polynomial order, denoted as K, in our GFSA,
conducting experiments on BERT BASE finetuned with GLUE tasks. We search for values of Kfrom
2 to 10, and the results are presented in Table 10. For each dataset, there is an optimal Kand the
performance of models using GFSA is generally robust to changes in K.
Table 10: Sensitivity results on various Kwith BERT BASE finetuned on GLUE tasks
K CoLA SST2 MRPC QQP STSB MNLI-m MNLI-mm QNLI RTE
2 57.83 94.15 90.60 88.41 88.27 84.96 84.90 91.74 68.23
3 58.56 93.46 89.77 88.41 88.33 85.08 84.75 91.78 68.59
4 59.56 93.46 89.77 88.45 88.29 85.06 85.06 91.76 68.59
5 58.10 93.58 90.07 88.39 88.29 84.87 84.99 91.85 68.95
6 59.40 93.58 90.40 88.29 88.27 84.93 84.97 91.69 68.23
7 59.12 94.04 90.48 88.43 88.26 85.12 84.94 91.82 68.94
8 58.58 93.69 90.12 88.46 88.24 84.92 84.81 91.95 68.95
9 58.88 93.46 89.54 88.41 88.26 85.06 91.67 67.87 85.04
10 59.31 93.35 89.98 88.41 88.30 84.84 91.73 68.59 84.93
L Causal Language Modeling
L.1 Detailed Experimental Settings
Dataset. The benchmark datasets we used are listed below.
•PTB. Penn Treebank [ 47] dataset is a collection of text documents that have been extensively
annotated with linguistic information, primarily syntactic and grammatical structures.
•WikiText. WikiText [ 50] dataset is a collection of over 100 million tokens extracted from
the set of verified good and featured articles on Wikipedia. Compared to the preprocessed
version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is
over 110 times larger.
GPT2. GPT2 [ 61] is a Transformer pretrained on a very large corpus of English data in a self-
supervised fashion without any human labelling on dataset. It automatically generate inputs and
labels from those texts, and trained to guess the next word in sentences. For implementation, we
adopt HuggingFace Framework4. For all experiments, GPT2 has 12 layers with 12 attention heads,
768 hidden size and 1024 maximum sequence length, resulting in a total of 117 million parameters.
Training. We finetune GPT2 with 4 batch size, 5×10−5learning rate and linear learning weight
decay using adamW [ 46] optimizer. We also apply dropout with probability 0.1. Following [ 93], we
train models for 15 epochs with PTB, 4 epochs with WikiText-103 and 10 epochs with WikiText-2. We
use sensitivity metric, i.e., perplexity, which is a commonly used metric to evaluate the performance
of language models, particularly in language modeling and text generation tasks. perplexity measures
how well a language modeling can predict a sequence of words in a given text or a test dataset. All
the experiments are conducted on 1 GPU and of NVIDIA RTX 3090 24GB.
L.2 Sensitivity to K
We conducted a sensitivity study on Kof GPT-2 across all datasets, and the results are presented in
Table 11. For PTB and WikiText-2, GFSA exhibits the best performance when Kis high, typically
4https://github.com/huggingface/transformers
25around 8 or 9. However, for WikiText-103, GFSA achieves the best perplexity when Kis small,
specifically when Kis 3 or 4.
Table 11: Results comparison on GPT-2 finetuned with GFSA
Method #Params PTB WikiText-2 WikiText-103
GPT2 [61] 117M 19.513 20.966 15.939
GPT2 + GFSA( K= 2) 117M 19.459 20.929 15.920
GPT2 + GFSA( K= 3) 117M 19.456 20.927 15.919
GPT2 + GFSA( K= 4) 117M 19.453 20.927 15.919
GPT2 + GFSA( K= 5) 117M 19.452 20.925 15.920
GPT2 + GFSA( K= 6) 117M 19.451 20.925 15.920
GPT2 + GFSA( K= 7) 117M 19.450 20.925 15.921
GPT2 + GFSA( K= 8) 117M 19.450 20.924 15.921
GPT2 + GFSA( K= 9) 117M 19.450 20.923 15.921
M Image Classification
M.1 Detailed Experimental Settings
Our code is implemented based on the timm library [ 86]. In the case of our training recipe, it is the
same as experimental setting of Wang et al. [80] that follows the training recipes of Touvron et al. [74]
and Touvron et al. [75]. To apply our GFSA to existing base models such as DeiT, Cait, and Swin, we
consider a range of Kbetween 2 and 5. For 12-layer DeiT, we follow the same hyperparameters from
Wang et al. [80]. We set the dropout rate to 0 and 0.2 for 12-layer and 24-layer DeiT, respectively.
For CaiT, we apply our GFSA on only to the patch embedding layer. All other hyper-parameters
are kept consistent with the original papers of DeiT [ 74], CaiT [ 75] and, Swin [ 45]. All models are
trained on NVIDIA RTX 3090 24GB.
M.2 FLOPs & Throughput
In Table 12, we report the number of FLOPs and throughput. With GFSA plugged in, the FLOP count
is either the same or no different. For DeiT-S with 24 layers, which shows a slight FLOP increase
with GFSA plugged in. However, for the rest of the settings, the models have the same number of
Flops. For throughput, it tends to decrease because calculating the high-order term is an additional
cost.
Table 12: Experimental evalutation of GFSA plugged into DeiT-S, CaiT-S, and Swin-S
Backbone Method Input Size #Layers #Params #FLOPs #Throughput Top-1 Acc
DeiTDeiT-S 224 12 22.0M 4.57G 856.07 79.8
DeiT-S + GFSA 224 12 22.0M 4.57G 614.54 81.1 (↑1.3)
DeiT-S 224 24 43.3M 9.09G 423.68 80.5
DeiT-S + GFSA 224 24 43.3M 9.10G 314.75 81.5 (↑1.0)
CaiTCaiT-S 224 24 46.9M 9.34G 574.66 82.6
CaiT-S + GFSA 224 24 47.0M 9.34G 406.96 82.8 (↑0.2)
SwinSwin-S 224 24 49.6M 8.75G 912.38 82.9
Swin-S + GFSA 224 24 49.6M 8.75G 714.60 83.0 (↑0.1)
M.3 Sensitivity to K
We also perform the sensitivity analysis for K. Tables 13 and 14 show the results of sensitivity
analysis for DeiT-S and CaiT-S with GFSA plugged in. For 12-layer DeiT-S, GFSA performance
of 81.12 is highest when K= 3. When the GFSA has a Kof 2, the performance is worse than the
26original DeiT-S, but when the Kis 3 or higher, the performance is better than the original DeiT-S,
and most surprisingly, the performance is better than the 24-layer DeiT-S.
CaiT-S shows the highest performance of 82.84 when K= 4. For CaiT-S, the accuracy is slightly
lower than that of the original CaiT-S when K= 2, but it starts to exceed the accuracy of CaiT-S
when Kis 3 or higher.
Table 13: Sensitivity to Kfor 12-layer DeiT-S +
GFSA
K 2 3 4 5
Top-1 Acc (%) 79.27 81.12 80.86 81.07Table 14: Sensitivity to Kfor 24-layer CaiT-S +
GFSA
K 2 3 4
Top-1 Acc (%) 82.54 82.65 82.84
M.4 Full Experimental Results
In Table 15, we consider all three classes, CNN only, CNN + Transformer, and pure Transformer,
to compare more different models than in Table 3. In particular, in the Transformer category, we
only test with lightweight models with similar number of parameters, such as ViT-S and DeiT-
S. Compared to existing techniques, the improvements by GFSA already surpasses LayerScale
(0.7%) [75], LateInsertion (0.6%) [75], and HAT [4] (1.38%).
Table 15: Compared with state-of-the-art models on ImageNet-1k dataset. The number in ( ↑) indicates
the performance improvement over the base model.
Category Method Input Size #Layers #Params Top-1 Acc
CNNResNet-152 [29] 224 152 230M 78.1
DenseNet-201 [34] 224 201 77M 77.6
CNN + CVT-21 [88] 224 21 32M 82.5
Transformer Refiner [102] 224 16 86M 81.2
TransformerViT-S/16 [19] 224 12 49M 78.1
ViT-B/16 [19] 224 12 86M 79.8
DeiT-S [74] 224 12 22M 79.8
DeiT-S + LayerScale [75] 224 12 22M 80.5
DeiT-S + LateInsertion [75] 224 12 22M 80.5
DeiT-S + ClassAttention [75] 224 12 22M 80.6
DeiT-S + AttnScale [80] 224 12 22M 80.7
DeiT-S + FeatScale [80] 224 12 22M 80.9
DeiT-S + HAT [4] 224 12 22M 80.9
DeiT-S + Diverse [10] 224 12 22M 80.6
DeiT-S + ContraNorm [28] 224 12 22M 80.4
Swin-S [45] 224 12 50M 82.9
T2T-ViT-24 [96] 224 24 64M 82.3
DeepViT-24B [101] 224 24 36M 80.1
DeiT-S [74] 224 24 43M 80.5
CaiT-S [75] 224 24 47M 82.6
DeiT-S + DiversePatch [25] 224 24 44M 82.2
DeiT-S + LayerScale [75] 224 24 44M 82.4
DeiT-S + AttnScale [80] 224 24 44M 81.1
DeiT-S + FeatScale [80] 224 24 44M 81.3
DeiT-S + ContraNorm [28] 224 24 43M 80.7
GFSADeiT-S + GFSA 224 12 22M 81.1 (↑1.3)
DeiT-S + GFSA 224 24 43M 81.5 (↑1.0)
CaiT-S + GFSA 224 24 47M 82.8 (↑0.2)
Swin-S + GFSA 224 12 50M 83.0 (↑0.1)
27M.5 Additional Comparison with SOTA Models
Our main experiment aims to determine whether introducing the GFSA layer would help improve
performance in a base model, such as DeiT. We also compare our method with the recent models:
SpectFormer [ 58], SVT [ 57], NeuTRENO [ 52], FNet [ 42], and GFNet [ 64]. We use a 12-layer setup
to ensure a fair comparison in Table 16.
GFNet [ 64] can reduce the number of parameters, but there is a performance penalty. However,
the performance improvement of DeiT-S+GFSA is relatively greater than DeiT-S compared to
other models. SpectFormer [ 58] and SVT [ 57] have advantages in calculation amount and model
complexity, and performance is improved over DeiT-S, but Top-1 and Top-5 accuracies are lower
than those using GFSA. Additionally, NeuTRENO [ 52] also improves as much as GFSA compared
to DeiT-S, but GFSA still has higher Top-1 accuracy.
Table 16: Compared with state-of-the-art models on ImageNet-1k
Method Input Size #Layers #Params Top-1 Acc Top-5 Acc
DeiT-S 224 12 22M 79.8 95.0
Fnet-XS [42] 224 12 20M 71.2 -
GFNet-XS [64] 224 12 16M 78.6 94.2
SpectFormer-XS [58] 224 12 20M 80.2 94.7
SVT-XS [57] 224 12 20M 79.9 94.5
DeiT-S + NeuTRENO [52] 224 12 20M 80.7 95.4
DeiT-S + GFSA 224 12 22M 81.1 95.4
M.6 Additional Experiments with Guo et al. [28]’s setting
To make a fair comparison with ContraNorm [ 28], one of the related studies that mitigates over-
smoothing, we run additional experiments to match their experimental setup.
Setting. We follow the training recipe used by Guo et al. [28], which is a slightly modified version
of Touvron et al. [74]’s recipe. Guo et al. [28] use AdamW optimizer with cosine learning rate decay.
We select the DeiT-T and DeiT-S for ImageNet-1k. “T” and “S” denote tiny and small model sizes,
respectively. For all experiments, the image size is set to be 224x224. We train each model for
300 epochs and the batch size is set to 1024. For ContraNorm, we train with their recommended
hyperparameters. All models are trained on 4 GPUs and of NVIDIA RTX A6000 48GB.
Results. In Table 17, DeiT-T and DeiT-S with GFSA outperform vanilla DeiT-T and DeiT-S in all
layer settings. GFSA improves the performance of DeiT-T with 12 layers by 1.52%. The largest
gain is a 4.88% improvement on 16-layer DeiT-T. This shows that the effect of GFSA is larger than
the effect of ContraNorm. For DeiT-S with 16 layers, surprisingly, GFSA is able to increase the
performance by 80.83%, meaning that GFSA brings benefits with a 3.23% improvement.
Table 17: Experiment results on ImageNet-1k
Method #Layers=12 #Layers=16 #Layers=24
DeiT-T 76.52 75.34 76.76
DeiT-T + ContraNorm 77.03 78.72 78.12
DeiT-T + GFSA 77.68 79.02 78.64
DeiT-S 77.32 78.25 77.69
DeiT-S + ContraNorm 77.80 79.04 78.67
DeiT-S + GFSA 79.86 80.83 79.15
Sensitivity to K.In Table 18, we experiment with a sensitivity analysis for K. For DeiT-T, the
performance of GFSA generally improves when Kis 4 or 5. On the other hand, GFSA performs
better at lower Kfor settings that are layers 16 and 24 for DeiT-S.
28Table 18: Varying Kfor DeiT-T and DeiT-S
Method K #Layers=12 #Layers=16 #Layers=24
DeiT-T + GFSA 2 76.92 78.14 78.40
DeiT-T + GFSA 3 77.41 77.76 78.09
DeiT-T + GFSA 4 77.01 79.02 78.64
DeiT-T + GFSA 5 77.68 78.14 78.64
DeiT-S + GFSA 2 79.84 80.83 79.15
DeiT-S + GFSA 3 79.85 79.39 79.07
DeiT-S + GFSA 4 79.86 79.44 79.10
N Automatic Speech Recognition
N.1 Detailed Experimental Settings
Dataset. We conduct experiments on the LibriSpeech5dataset [ 55], which consists of audio
recordings paired with their transcriptions. The LibriSpeech dataset has approximately 1,000 hours
of read English speech with a sampling rate of 16 kHz. We keep the original 16,000Hz sampling rate
and compute 80-dim log-Mel filterbanks for a 25ms sliding window, strided by 10ms. The filterbank
features are then normalized to zero mean and unit variance per input sequence. For implementation,
we follow the recipes of SpeechBrain [65].
Evaluation Metric. Word error rate (WER (%)) is derived from the Levenshtein distance and
compares a reference to a hypothesized word-level transcription. It is calculated by summing the
number of word insertions, deletions, substitutions and dividing it by the total number of words in the
reference transcription.
Vanilla Transformer. We use a vanilla Transformer to apply our GFSA. For implementation,
we use a SpeechBrain [ 65] framework. The vanilla Transformer consists of i) 1D convolution to
perform striding, ii) Transformer encoder with 12 layers, 4 heads, embedding dimension of 512, MLP
dimension of 2048, and post-LayerNorm iii) decoder with 6 layers, 4 heads, embedding dimension of
512, MLP dimension of 2048, joint beamsearch, and iv) external Transformer language model with
12 layers, 12 heads, embedding dimension of 768, and MLP dimension of 3072.
Branchformer. We use one of the SOTA models, Branchformer [ 59] to plug-in our GFSA. Branch-
former has two parallel branches, one for capturing global interactions using attention and the other
for more localized context using convolutional gating MLP. The Branchformer architecture for speech
recognition consists of i) 1D convolution to perform striding, ii) Branchformer encoder with 18 layers,
8 heads, embedding dimension of 512, and MLP dimension of 3072, iii) decoder with 6 layers, 8
heads, embedding dimension of 512, a convolutional spatial gating unit (CSGU) dimension of 3072,
joint beamsearch, and iv) external Transformer language model with 12 layers, 12 heads, embedding
dimension of 768, and MLP dimension of 3072.
Training. We follow a training recipe from SpeechBrain [ 65]. The standard LibriSpeech validation
sets (dev-clean and dev-other) are used to tune all parameters and select the best models. Test
sets (test-clean and test-other) are used only to report final WER performance. We train the pure
Transformer for 100 epochs and the Branchformer for 120 epochs with a batch size of 16. We use
a data augmentation method on all models using SpecAugment [ 56]. SpecAugment applies time
and frequency masking as well as time warping to the input spectrum. For Branchformer, we use
AdamW [ 46] optimizer with 0.9 and 0.98 coefficients for computing running averages of gradient
and its square. The learning rate and weight decay in all models are 0.0008 and 0.01, respectively.
We use a connectionist temporal classification (CTC) loss [ 26,100]. We also apply dropout with
probability 0.1 and label smoothing with weight 0.1 to mitigate overfitting. We fix the random seed
as 74443 on all experiments. All models are trained on 1 GPU and of NVIDIA RTX A6000 48GB.
5http://www.openslr.org/12
29Hyperparameters. In Table 19, we describe main hyperparameters used in the automatic speech
recognition task. For Transformer+GFSA and Branchformer+GFSA, we also report the best K
hyperparameter.
Table 19: Main hyperparameters used in ASR
Model Experimental Setting
TransformerEncoder: Transformer (12 layers)
Decoder: Transformer (6 layers) + (CTC/ATT joint) beamsearch + TransformerLM
Augmentation: SpecAugment
Features: 40 fbanks
Pretraining: no
Dropout: 0.1
Batchnorm: yes
Number of epochs: 100
Batch size: 32
Learning rate: 0.0008
LR scheduler: Noam
Optimizer: Adam
Loss: CTC + KLdiv (Label Smoothing loss)
CTC weight: 0.3
Transformer+GFSAEncoder: Transformer (12 layers)
Decoder: Transformer (6 layers) + (CTC/ATT joint) beamsearch + TransformerLM
Augmentation: SpecAugment
Features: 40 fbanks
Pretraining: no
Dropout: 0.1
Batchnorm: yes
Number of epochs: 100
Batch size: 32
Learning rate: 0.0008
LR scheduler: Noam
Optimizer: Adam
Loss: CTC + KLdiv (Label Smoothing loss)
CTC weight: 0.3
K: 2
BranchformerEncoder: Branchformer
Decoder: Transformer (6 layers) + (CTC/ATT joint) beamsearch + TransformerLM
Augmentation: SpecAugment
Features: 40 fbanks
Pretraining: no
Dropout: 0.1
Batchnorm: yes
Number of epochs: 120
Batch size: 16
Learning rate: 0.0008
LR scheduler: Noam
Optimizer: AdamW with coefficients 0.9 and 0.98
Loss: CTC + KLdiv (Label Smoothing loss)
CTC weight: 0.3
Branchformer+GFSAEncoder: Branchformer
Decoder: Transformer (6 layers) + (CTC/ATT joint) beamsearch + TransformerLM
Augmentation: SpecAugment
Features: 40 fbanks
Pretraining: no
Dropout: 0.1
Batchnorm: yes
Number of epochs: 120
Batch size: 16
Learning rate: 0.0008
LR scheduler: Noam
Optimizer: AdamW with coefficients 0.9 and 0.98
Loss: CTC + KLdiv (Label Smoothing loss)
CTC weight: 0.3
K: 3
30N.2 Training Curve
We compare the training and validation curves for LibriSpeech 100h in Fig. 8. The training loss curve
of GFSA is lower than the pure Transformer. GFSA stabilizes the loss curve of pure Transformer
slightly earlier.
0 25 50 75 100
Epoch05101520Train LossTransformer
Transformer+GFSA
(a) Training loss curve
0 25 50 75 100
Epoch051015202530Valid LossTransformer
Transformer+GFSA (b) Validation loss curve
Figure 8: Training curve on LibriSpeech 100h
O Graph-level Tasks
O.1 Detailed Experimental Settings
Experimental settings for Graphormer. We describe benchmark datasets and Graphormer as the
backbone model we used. ZINC [ 35] is the most popular real-world molecular dataset to predict
graph property regression for constrained solubility, an important chemical property for designing
generative GNNs for molecules. Uniform sampling is adopted for data splitting. We use a ZINC-
subset of small-scale dataset. PCQM4M-LSC [ 33] is 2D molecular graphs, which is one of the
most practically relevant quantum chemical properties of molecule science. The task is to predict
density functional theory (DFT)-calculated HOMO-LUMO energy gap of molecules given their
graphs. PCQM4M-LSC is unprecedentedly large in scale comparing to other labeled graph-level
prediction datasets, which contain more than 3.8M graphs. We use PCQM4M and PCQM4Mv2
large-scale datasets.
Following Graphormer [ 94], we use Graphormer for PCQM4M and Graphormer SLIM for ZINC.
Graphormer consists of 12 encoder layers, 80 encoder embedding dimension, and 768 MLP dimension.
It employs 32 encoder heads and 24 hidden dimension for each head. Graphormer SLIM consists of 12
encoder layers, 80 encoder embedding dimension, and 80 MLP dimension. It employs 8 encoder
heads and 10 hidden dimension for each head. We use adamW [ 46] optimizer with 0.9 and 0.999
coefficients for running averages of gradient and its square, and use Mean Absolute Error (MAE)
as loss function. We use polynomial learning rate decay, with initial learning rate set to 2×10−4
and end learning rate set to 1×10−9. For ZINC, we set batch size as 256, max epochs as 10k, and
warm-up stage step as 40k. For PCQM4M and PCQM4Mv2, we set batch size as 1024, max epochs
as 300, and warm-up stage step as 60k. All models are trained on 1 GPU and of NVIDIA RTX 3090
24GB. We conduct experiments with 4 different seeds.
Experimental settings for GPS and Graph-ViT. We use various benchmark datasets to experiment
with our GFSA on GPS [ 63] and Graph-ViT [ 30]: Peptide-func and Peptide-struct from Long-Range
Graph Benchmark (LRGB) [ 21], MNIST, CIFAR10, and ZINC from Benchmarking GNNs [ 22],
and Moltox21 and Molhiv from OGB [ 32]. We fix all hyperparameters of GPS and Graph-ViT as
recommended in their paper to ensure a fair comparison. To plug GFSA into GPS, we replace the
self-attention module of GPS with GFSA. For Graph-ViT, we apply GFSA instead of the Hadamard
self-attention mechanism and compare it with this self-attention method. We conduct experiments on
GPS and Graph-ViT in their open code frameworks for a fair comparison:
• GPS: https://github.com/rampasek/GraphGPS
• Graph-ViT: https://github.com/XiaoxinHe/Graph-ViT-MLPMixer
31O.2 Experimental Results with Standard Deviation
We conduct experiments following the experimental environments of Graphormer [ 94] using 4
different seeds. Due to space constraints, only the mean values are reported in Tables 4 and 5. In
Tables 20 and 21, we report the results with mean and standard deviations.
Table 20: Experimental results and number of parameters on ZINC
Method #Params MAE
Graphormer 500K 0.1240 ±0.006
Graphormer + GFSA 500K 0.1189 ±0.002
Table 21: Experimental results and number of parameters on PCQM4M and PCQM4Mv2
Method #ParamsPCQM4M PCQM4Mv2
Train Validate Train Validate
Graphormer 48.3M 0.0535 ±0.038 0.1286 ±0.016 0.0250 ±0.000 0.0862 ±0.000
Graphormer + GFSA 48.3M 0.0312 ±0.001 0.1193 ±0.000 0.0249 ±0.000 0.0860 ±0.000
P Code Defect Detection
P.1 Detailed Experimental Settings
Dataset. We use Devign dataset provided by [ 105], which is a binary classification task to evaluate
whether a C language code is vulnerable to software systems or not.
Implementation. We build our experiments on top of the open-sourced code6and recipes provided
by Wang et al. [84].
RoBERTa. RoBERTa [ 44] is an encoder-only model trained with masked language modeling on
code. All hyperparameters are consistent with the training method in the source code of Wang et al.
[84].
PLBART. PLBART [ 2] is an encoder-decoder model based on BART [ 43] architecture. PLBART
can support understanding and generation tasks. All hyperparameters are consistent with the training
method in the source code of Wang et al. [84].
CodeBert. CodeBERT [ 23] is a model trained on masked language modeling and replaced token
detection. CodeBERT is a bimodal pretrained model based on Transformer with 12 layers for
programming language and natural language. All hyperparameters are consistent with the training
method in the source code of Wang et al. [84].
CodeT5. CodeT5 is an encoder-decoder framework with the same architecture as T5 [ 62]. It aims
to derive generic representations for programming language and natural language via pre-training
on unlabeled source code. CodeT5-small has 6 encoder layers, 6 decoder layers, 8 attention heads,
512 dimensional hidden states, and 60M parameters. The other models have 12 encoder layers,
12 decoder layers, 12 attention heads, 768 dimensional hidden states, and 220M parameters. All
hyperparameters are consistent with the training method in the source code of Wang et al. [84].
Training. The pre-trained models mentioned above are applied to this downstream task. We add
GFSA directly on top of self-attention. We finetune baselines and GFSA models for 10 epochs with a
batch size of 16. We use early stopping strategy with a patience of 2. Models generate binary labels
from unigram sequences at the decoder for defect detection task. We employ accuracy for evaluating
the code defect detection task. All models are trained on 1 GPU and of NVIDIA RTX A6000 48GB.
6https://github.com/salesforce/CodeT5
32P.2 Case Study
In Listing 1, we show one of case for code snippets of defects in QEMU7that CodeT5-base does not
predict correctly, but that only CodeT5-base+GFSA predicts. The commit message8for this case is
as follow:
Needed for changing cpu_has_work() argument type to CPUState, used in
h_cede() .
h_cede() is the hypercall that asks the hypervisor to shut down the CPU. Previously, this hypercall
simply passed the CPUID, so the hypervisor did not know what state the CPU was in. This change
allows the hypervisor to know whether the CPU is actually performing work. If the CPU is performing
a task, the hypervisor waits for the CPU to complete the task.
In this context, accurately predicting defects like the one above is very important, and applying GFSA
to CodeT5-base helps in terms of performance improvement.
1@@ -204 ,7 +204 ,7 @@ static target_ulong put_tce_emu ( sPAPRTCETable *
tcet , target_ulong ioba ,
2- static target_ulong h_put_tce ( CPUPPCState *env , sPAPREnvironment *
spapr
3+ static target_ulong h_put_tce ( PowerPCCPU *cpu , sPAPREnvironment *
spapr
4 , target_ulong opcode , target_ulong *
args )
5{
6target_ulong liobn = args [0];
7target_ulong ioba = args [1];
8target_ulong tce = args [2];
9VIOsPAPRDevice *dev = spapr_vio_find_by_reg (spapr -> vio_bus , liobn );
10 VIOsPAPR_RTCE * rtce ;
11 if (! dev) {
12 hcall_dprintf (" LIOBN 0x" TARGET_FMT_lx " does not exist \n", liobn )
;
13 return H_PARAMETER ;
14 }
15 ioba &= ~( SPAPR_VIO_TCE_PAGE_SIZE - 1);
16 # ifdef DEBUG_TCE
17 fprintf (stderr , " spapr_vio_put_tce on %s ioba 0x" TARGET_FMT_lx "
TCE 0x" TARGET_FMT_lx "\n", dev -> qdev .id , ioba , tce);
18 # endif
19 if ( ioba >= dev -> rtce_window_size ) {
20 hcall_dprintf (" Out -of - bounds IOBA 0x" TARGET_FMT_lx "\n", ioba );
21 return H_PARAMETER ;
22 }
23 rtce = dev -> rtce_table + ( ioba >> SPAPR_VIO_TCE_PAGE_SHIFT );
24 rtce -> tce = tce ;
25 return H_SUCCESS ;
26}
Listing 1: An example commit history for defects in Devign dataset
Q Code Clone Detection
Q.1 Detailed Experimental Settings
Dataset. Code clone detection aims to measure the similarity between two code snippets and predict
whether they have the same functionality. We experiment with the Java data provided by Wang et al.
[83].
7https://www.qemu.org
8https://github.com/qemu/qemu/commit/b13ce26d3e8c6682044ae84920f2417b30ce356b
33Implementation. We build our experiments on top of the open-sourced code9and recipes provided
by Wang et al. [84].
Training. We finetune both CodeT5 and CodeT5+GFSA for one epoch with a batch size of 16. We
also use early stopping with patience of 2. CodeT5 and CodeT5+GFSA encode source code and take
the representation to calculate similarity of two code snippets. We employ F1 score for evaluating
this task. All models are trained on 1 GPU and of NVIDIA RTX A6000 48GB.
Q.2 Experiment Result
Table 22 shows results comparing CodeT5 and CodeT5 with GFSA. The result shows that by using
our GFSA, CodeT5 models improve their performance. CodeT5-small+GFSA provides a 0.61%
improvment over Code5T-small.
Table 22: Results on the code clone detection task
Method Clone F1
CodeT5-small [84] 94.36
CodeT5-small + GFSA 94.94 (↑0.61%)
CodeT5-base [84] 94.31
CodeT5-base + GFSA 94.92 (↑0.64%)
R Time Complexity and Empirical Runtime Analysis
Time Complexity. The time complexity of original self-attention is O(n2d). But our GFSA has a
high order term. Therefore, the time complexity of GFSA has O(n2d+n3). Ifnis smaller than d,
the time complexity approaches O(n2d), which is the complexity of original self-attention.
Empirical Runtime Analysis. We report the training time of various methods with GFSA in
Tables 23 to 28. In general, the training time of methods with GFSA is slightly longer than that of
existing methods. For example, the Transformer for the automatic speech recognition task increases
from 190.5 seconds to 191.6 seconds on Librispeech 100h dataset, as increases of only 1 second.
Instead of computing higher-order polynomial terms, our GFSA approximates them, with only a
small increase in runtime, which is not very significant.
Table 23: Training time (seconds per epoch) on GLUE tasks. sdenotes the abbreviation for second.
Avgdenotes the average training time across all tasks.
Datasets #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE Avg
BERT BASE [16] 110M 17 s182s17s1483 s24s 2004 s 580s18s541s
BERT BASE + GFSA 110M 19 s192s19s1571 s25s 2147 s 621s20s577s
ALBERT BASE [40] 11M 15 s188s20s1506 s25s 2072 s 612s19s557s
ALBERT BASE + GFSA 11M 16 s197s21s1604 s26s 2219 s 659s21s595s
RoBERTa BASE [44] 125M 17 s190s18s1492 s25s 2012 s 593s18s546s
RoBERTa BASE + GFSA 125M 19 s200s19s1580 s26s 2151 s 634s20s581s
Table 24: Training time (seconds per epoch) on causal language modeling tasks.
Method #Params PTB WikiText-2 WikiText-103 Avg
GPT2 [61] 117M 89.1 s 195.7 s 9638.4 s 3307.8 s
GPT2 + GFSA 117M 160.3 s 354.2 s 17424.6 s 5979.7 s
9https://github.com/salesforce/CodeT5
34Table 25: Training time (seconds per epoch) on ImageNet-1k
Backbone Method #Layers #Params #FLOPs #Throughput Runtime
DeiTDeiT-S 12 22.0M 4.57G 856.07 551 s
DeiT-S + GFSA 12 22.0M 4.57G 614.54 814 s
DeiT-S 24 43.3M 9.09G 423.68 1508 s
DeiT-S + GFSA 24 43.3M 9.10G 314.75 1798 s
CaiTCaiT-S 24 46.9M 9.34G 574.66 1530 s
CaiT-S + GFSA 24 47.0M 9.34G 406.96 1624 s
SwinSwin-S 24 49.6M 8.75G 912.38 1897 s
Swin-S + GFSA 24 49.6M 8.75G 714.60 1970 s
Table 26: Training time (seconds per epoch) on graph-level tasks
Method ZINC PCQM4M PCQM4Mv2
Graphormer [94] 9 s 740s 817s
Graphormer + GFSA 9 s 896s 955s
Table 27: Training time (seconds per epoch) on LibriSpeech datasets
Method LibriSpeech 100h LibriSpeech 960h
Transformer 190.5 s 3049.3 s
Transformer + GFSA 191.6 s 3398.4 s
Branchformer [59] 248.5 s 4990.1 s
Branchformer + GFSA 254.3 s 4999.3 s
Table 28: Training time (seconds per epoch) on the code defect prediction task
Method Runtime
RoBERTa [44] 543.96 s
RoBERTa + GFSA 537.79 s
CodeBERT [23] 555.28 s
CodeBERT + GFSA 561.43 s
PLBART [2] 467.80 s
PLBART + GFSA 470.19 s
CodeT5-small [84] 301.11 s
CodeT5-small + GFSA 309.04 s
CodeT5-base [84] 362.28 s
CodeT5-base + GFSA 373.22 s
35S Inference Time Analysis
We report the inference time of various methods with GFSA in Tables 29 to 34.
Table 29: Inference time on GLUE tasks. sdenotes the abbreviation for second. Avg denotes the
average training time across all tasks.
Datasets #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE Avg
BERT BASE [16] 110M 1.0 s1.4s1.2s48.7s1.9s 15.5s 10.1s1.2s10.0s
BERT BASE + GFSA 110M 1.1 s1.4s1.2s52.3s2.0s 16.8s 11.0s1.3s11.0s
ALBERT BASE [40] 11M 1.1 s1.6s1.4s58.4s2.2s 18.4s 12.1s1.3s12.0s
ALBERT BASE + GFSA 11M 1.2 s1.7s1.4s62.1s2.3s 19.7s 13.1s1.4s13.0s
RoBERTa BASE [44] 125M 1.0 s1.4s1.1s47.0s1.9s 15.0s 9.9s1.2s10.0s
RoBERTa BASE + GFSA 125M 1.1 s1.4s1.2s50.4s2.0s 16.3s 10.8s1.2s11.0s
Table 30: Inference time on causal language modeling tasks.
Method #Params PTB WikiText-2 WikiText-103 Avg
GPT2 [61] 117M 3.2 s 7.4s 7.4s 6.0s
GPT2 + GFSA 117M 5.5 s 12.9s 12.9s 10.4s
Table 31: Inference time on ImageNet-1k
Backbone Method #Layers Inference Time
DeiTDeiT-S 12 52 s
DeiT-S + GFSA 12 53 s
DeiT-S 24 68 s
DeiT-S + GFSA 24 69 s
CaiTCaiT-S 24 105 s
CaiT-S + GFSA 24 107 s
SwinSwin-S 24 17 s
Swin-S + GFSA 24 17 s
36Table 32: Inference time on graph-level tasks
Method ZINC PCQM4M PCQM4Mv2
Graphormer [94] 8 s 99s 31s
Graphormer + GFSA 8 s 117s 29s
Table 33: Inference time on LibriSpeech datasets
Method LibriSpeech 100h LibriSpeech 960h
Transformer 328.1 s 323.7 s
Transformer + GFSA 329.5 s 343.3 s
Branchformer [59] 299.4 s 328.7 s
Branchformer + GFSA 305.5 s 354.1 s
Table 34: Inference time on the code defect prediction task
Method Inference Time
RoBERTa [44] 22.4 s
RoBERTa + GFSA 23.9 s
CodeBERT [23] 23.8 s
CodeBERT + GFSA 24.1 s
PLBART [2] 37.7 s
PLBART + GFSA 39.3 s
CodeT5-small [84] 78.2 s
CodeT5-small + GFSA 82.5 s
CodeT5-base [84] 83.2 s
CodeT5-base + GFSA 88.5 s
37T Results of the Strategy for Efficiency
In Tables 35 to 39, the results show that this strategy can reduce the increase in runtime and maintain
performance compared to using GFSA for all layers.
Table 35: Comparison of performance using GFSA on all layers vs. GFSA evenon even layers for
GLUE tasks
Datasets #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE Avg
BERT BASE [16] 110M 56.79 93.81 88.70 88.32 88.16 84.96/84.15 91.63 66.06 82.51
BERT BASE + GFSA 110M 59.56 94.15 90.60 88.46 88.33 85.12/85.06 91.95 68.95 83.58
BERT BASE + GFSA even 110M 58.80 93.69 90.50 88.47 88.27 85.13/85.02 91.65 70.76 83.59
Table 36: Comparison of training time (seconds per epoch) using GFSA on all layers vs. GFSA even
on even layers for GLUE tasks
Datasets #Params CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE Avg
BERT BASE [16] 110M 17 s182s17s1483 s24s 2004 s 580s18s541s
BERT BASE + GFSA 110M 19 s192s19s1571 s25s 2147 s 621s20s577s
BERT BASE + GFSA even 110M 17 s185s18s1506 s24s 2061 s 595s18s553s
Table 37: Comparison of performance using GFSA on all layers vs. GFSA evenon even layers for
causal language modeling tasks
Method #Params PTB WikiText-2 WikiText-103 Avg
GPT2 [61] 117M 19.513 20.966 15.939 18.806
GPT2 + GFSA 117M 19.450 20.923 15.919 18.764
GPT2 + GFSA even 117M 19.453 20.926 15.923 18.767
Table 38: Comparison of training time (seconds per epoch) using GFSA on all layers vs. GFSA even
on even layers for causal language modeling tasks
Method #Params PTB WikiText-2 WikiText-103 Avg
GPT2 [61] 117M 89.1 s 195.7 s 9638.4 s 3307.8 s
GPT2 + GFSA 117M 160.3 s 354.2 s 17424.6 s 5979.7 s
GPT2 + GFSA even 117M 127.4 s 279.1 s 13761.4 s 4722.6 s
Table 39: Comparison of using GFSA on all layers vs. GFSA evenon even layers for ImageNet-1k
Method Input Size #Layers #Params Top-1 Acc Runtime
DeiT-S 224 12 43M 79.8 551 s
DeiT-S + GFSA 224 12 43M 81.1 814 s
DeiT-S + GFSA even 224 12 43M 81.0 595 s
38U GFSA in Linear Transformers
Table 40 shows the accuracy, runtime, and GPU usage results on Long Range Arena benchmark using
ListOps and Image datasets.
Table 40: Comparison of accuracy (%), runtime ( sper 1,000 steps) and GPU usage (GB) on Long
Range Arena benchmark
MethodListOps (2K) Image (4K)
Accuracy Runtime GPU usage Accuracy Runtime GPU usage
Transformer [77] 37.1 198.3 5.50 38.2 345.1 5.88
Transformer+GFSA 37.6 635.8 10.87 40.2 737.2 11.20
Linformer [81] 37.3 63.4 1.73 37.8 158.5 3.45
YOSO-E [97] 37.3 85.7 0.37 39.8 114.2 1.42
Efficient Attention [70] 36.9 49.2 0.57 40.2 121.1 1.14
Efficient Attention + GFSA 37.9 53.8 0.67 40.4 135.8 1.33
39NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We clearly stated our claims in the introduction, including the contributions of
the paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In Section 6, we stated the computational overhead as a limitation of our
method and provided strategies to address it.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
40Answer: [Yes]
Justification: We have included proofs for Theorem 4.1 and Theorem 3.1 in Appendix H
and Appendix G.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Our reproducibility statements are reported in A.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
41Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: See the statement about reproducibility in Appendix A.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Experimental environments, datasets, training methods are reported in Sec-
tion 5.1 to 5.6 and in Appendix K to 5.6.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We tried to follow the experimental setup for benchmarking each task. Among
the tasks we considered, it is conventional to report results from four experiments in
the graph-level tasks task, so we report the mean and standard deviation together in the
Appendix O.2.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
42•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Our paper specifies the type of computer resource information (e.g. GPU)
for the experiment. We also indicated the amount of calculation or execution time for the
experiment.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have reviewed the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discuss the potential positive and negative social impacts of our research in
Appendix B.
Guidelines:
43• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: There is no such risk in our paper.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: In our paper, the authors or original owners of code, data, and models used are
properly credited, and their licenses and terms of use are explicitly mentioned.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
44•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The code we provided is well documented in the README and is anonymized.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our research does not include crowdsourcing experiments and studies with
human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Because our research does not involve human subjects, it is not subject to IRB
approval.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
45•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
46