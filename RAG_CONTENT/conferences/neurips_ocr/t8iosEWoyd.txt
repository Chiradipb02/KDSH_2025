Stochastic contextual bandits with graph feedback:
from independence number to MAS number
Yuxiao Wen†Yanjun Han†Zhengyuan Zhou†,*
New York University†Arena Technologies*
{yuxiaowen, yanjunhan}@nyu.edu zz26@stern.nyu.edu
Abstract
We consider contextual bandits with graph feedback, a class of interactive learning
problems with richer structures than vanilla contextual bandits, where taking an
action reveals the rewards for all neighboring actions in the feedback graph under
all contexts. Unlike the multi-armed bandits setting where a growing literature
has painted a near-complete understanding of graph feedback, much remains
unexplored in the contextual bandits counterpart. In this paper, we make inroads
into this inquiry by establishing a regret lower bound Ω(p
βM(G)T), where Mis
the number of contexts, Gis the feedback graph, and βM(G)is our proposed graph-
theoretic quantity that characterizes the fundamental learning limit for this class
of problems. Interestingly, βM(G)interpolates between α(G)(the independence
number of the graph) and m(G)(the maximum acyclic subgraph (MAS) number
of the graph) as the number of contexts Mvaries. We also provide algorithms
that achieve near-optimal regret for important classes of context sequences and/or
feedback graphs, such as transitively closed graphs that find applications in auctions
and inventory control. In particular, with many contexts, our results show that the
MAS number essentially characterizes the statistical complexity for contextual
bandits, as opposed to the independence number in multi-armed bandits.
1 Introduction
Contextual bandits encode a rich class of sequential decision making problems in reality, including
clinical trials, personalized healthcare, dynamic pricing, recommendation systems (Bouneffouf et al.,
2020). However, due to the exploration-exploitation trade-off and a potentially large context space,
the pace of learning for contextual bandits could be slow, and the statistical complexity of learning
could be costly for application scenarios with bandit feedback (Agarwal et al., 2012). There are two
common approaches to alleviate the burden of sample complexity, either by exploiting the function
class structure for the reward (Zhu and Mineiro, 2022), or by utilizing additional feedback available
during exploration.
In this paper we focus on the second approach, and aim to exploit the feedback structure efficiently
in contextual bandits. The framework of formulating the feedback structure as feedback graphs
in bandits has a long history (Mannor and Shamir, 2011; Alon et al., 2015, 2017; Lykouris et al.,
2020), where a direct edge between two actions indicates choosing one action provides the reward
information for the other. Such settings have also been generalized to contextual cases (Balseiro
et al., 2019; Dann et al., 2020; Han et al., 2024), where counterfactual rewards could be available
under different contexts. Typical results in these settings are that, the statistical complexity of bandits
with feedback graphs is characterized by some graph-theoretic quantities, such as the independence
number or the maximum acyclic subgraph (MAS) number of the feedback graph.
To understand the influence of the presence of contexts on the statistical complexity of learning and to
compare with multi-armed bandits, we focus on the tabular setting where the contexts are treated as
38th Conference on Neural Information Processing Systems (NeurIPS 2024).general variables determining the rewards. A widely studied alternative is the structured setting that
leverages certain structures in the dependence on the context. Examples of the latter include linear
contextual bandits (Auer, 2002; Agrawal and Goyal, 2013), which assume a linear reward function
on the contexts, and their variants (Chu et al., 2011; Li et al., 2017; Agrawal and Devanur, 2016).
Despite the existing results, especially in multi-armed bandits where a near-complete characterization
of the optimal regret is available (Alon et al., 2015; Kocák and Carpentier, 2023; Eldowa et al., 2024),
the statistical complexity of contextual bandits with feedback graphs is much less understood. For
example, consider the case where there is a feedback graph Gacross the actions and a complete
feedback graph across the contexts (termed as complete cross-learning in (Balseiro et al., 2019)). In
this case, for a long time horizon T, the optimal regret scales as eΘ(p
α(G)T)when there is only
one context (Alon et al., 2015), but only an upper bound eO(p
m(G)T)is known regardless of the
number of contexts (Dann et al., 2020). Here α(G)andm(G)denote the independence number and
the MAS number of the graph G, respectively; we refer to Section 1.1 for the precise definitions.
While α(G) =m(G)for all undirected graphs, for directed graphs their gap could be significant. It
is open if the change from α(G)tom(G)is essential with the increasing number of contexts, not to
mention the precise dependence on the number of contexts.
1.1 Notations
Forn∈N, let[n] :={1,···, n}. For two probability measures PandQover the same space, let
TV(P, Q) =R
|dP−dQ|/2be the total variation (TV) distance, and KL(P∥Q) =R
dPlog(d P/dQ)
be the Kullback-Leibler (KL) divergence. We use the standard asymptotic notations O,Ω,Θ, as well
aseO,eΩ,eΘto denote respective meanings within polylogarithmic factors.
We use the following graph-theoretic notations. For a directed graph G= (V, E), letu→vdenote
that(u, v)∈E. For u∈V, letNout(u) ={v∈V:u→v}be the set of out-neighbors of u
(including uitself). We will also use Nout(A) =∪v∈ANout(v)to denote the set of all out-neighbors
of vertices in A. The independence number α(G),dominating number δ(G), and maximum acyclic
subgraph (MAS) number m(G)are defined as
α(G) = max {|I|:I⊆Vis an independent set, i.e. u̸→v,∀u̸=v∈I},
δ(G) = min {|J|:J⊆Vis a dominating set, i.e. Nout(J) =V},
m(G) = max {|D|:D⊆Vinduces an acyclic subgraph of G},
respectively. It is easy to show that max{α(G), δ(G)} ≤m(G), with a possibly unbounded gap, and
a probabilistic argument also shows that δ(G) =O(α(G) log|V|)(cf. Lemma A.1).
1.2 Our results
In this paper we focus on contextual bandits with both feedback graphs across actions and complete
cross-learning across contexts. This setting was proposed in (Han et al., 2024), with applications to
bidding in first-price auctions. As opposed to an arbitrary feedback graph across all context-action
pairs in (Dann et al., 2020), we assume a complete cross-learning because of two reasons. First, in
many scenarios the contexts encode different states which only play roles in the reward function;
in other words, the counterfactual rewards for all contexts can be observed by plugging different
contexts into the reward function. Such examples include bidding in auctions (Balseiro et al., 2019;
Han et al., 2024) and sleeping bandits modeled in (Schneider and Zimmert, 2024). Second, this
scenario is representative and sufficient to reflect the main ideas and findings of this paper. Discussion
and results under more general settings is left to Section 4.1.
Throughout this paper we consider the following stochastic contextual bandits. At the beginning of
each round t∈[T]during the time horizon T, an oblivious adversary chooses a context ct∈[M]
and reveals it to the learner, and the learner chooses an action at∈[K]. There is a strongly
observable1directed feedback graph G= ([K], E)across the actions such that all rewards in
(rt,c,a)c∈[M],(at,a)∈Eare observable, where we assume no structure in the rewards except that
rt,c,a∈[0,1]. In our stochastic environment, the mean reward E[rt,c,a] =µc,ais unknown but
invariant with time. We are interested in the characterization of the minimax regret achieved by the
1For every a∈[K], either a→aora′→afor all a′̸=a, as defined in (Alon et al., 2015).
2learner:
R⋆
T(G, M ) = inf
πTRT(πT;G, M )
= inf
πTsup
cTsup
µ∈[0,1]K×ME"TX
t=1
max
a⋆(ct)∈[K]µct,a⋆(ct)−µct,πt(ct)#
, (1)
where the infimum is over all admissible policies based on the available observations. In the sequel
we might also constrain the class of context sequences to cT∈ C, and we will use R⋆
T(G, M, C)and
RT(πT;G, M, C)to denote the respective meanings by taking the supremum over cT∈ C.
Our first result concerns a new lower bound on the minimax regret.
Theorem 1.1 (Minimax lower bound) .ForT≥βM(G)3, it holds that R⋆
T(G, M ) = Ω(p
βM(G)T),
where the graph-theoretic quantity βM(G)is given by
βM(G) = max(MX
c=1|Ic|:I1,···, IMdisjoint independent subsets of [K],andIi̸→Ijfori < j)
,
(2)
andIi̸→Ijmeans that u̸→vwhenever u∈Iiandv∈Ij.
Theorem 1.1 provides a minimax lower bound on the optimal regret, depending on both the number of
contexts Mand the feedback graph G. Note that the independent subsets I1, . . . , I Mare allowed to
be empty if needed. It is clear that β1(G) =α(G)is the independence number, and βM(G) =m(G)
whenever M≥m(G). This leads to the following corollary.
Corollary 1.2 (Tightness of MAS number) .For any graph G, ifM≥m(G)andT≥m(G)3, one
hasR⋆
T(G, M ) = Ω(p
m(G)T).
Corollary 1.2 shows that, the regret change from eΘ(p
α(G)T)in multi-armed bandits to
eO(p
m(G)T)in contextual bandits (Dann et al., 2020) is in fact not superfluous when there are many
contexts. In other words, although the independence number determines the statistical complexity of
multi-armed bandits with graph feedback, the statistical complexity in contextual bandits with many
contexts is completely characterized by the MAS number .
For intermediate values of M∈(1,m(G)), the next result shows that the quantity βM(G)is tight for
a special class CSAof context sequence called self-avoiding contexts . A context sequence (c1,···, cT)
is called self-avoiding iff cs=ctfors < t implies cs=cs+1=···=ct(or in other words, contexts
do not jump back). For example, 113222 is self-avoiding, but 12231 is not. This assumption is
reasonable when contexts model a nonstationary environment changing slowly, e.g. the environment
changes from season to season.
Theorem 1.3 (Upper bound for self-avoiding contexts) .For self-avoiding contexts, there is a policy π
achieving RT(π;G, M, CSA) =eO(p
βM(G)T). This policy can be implemented in polynomial-time,
and does not need to know the context sequence in advance.
As the minimax lower bound in Theorem 1.1 is actually shown under CSA, for large T, Theorem
1.3 establishes a tight regret bound for stochastic contextual bandits with graph feedback and self-
avoiding contexts. The policy used in Theorem 1.3 is based on arm elimination, where a central step
of exploration is to solve a sequential game in general graphs which has minimax value eΘ(βM(G))
and could be of independent interest.
For general context sequences, we have a different sequential game in which we do not have a tight
characterization of the minimax value in general. Instead, we have the following upper bound, which
exhibits a gap compared with Theorem 1.1.
Theorem 1.4 (Upper bound for general contexts) .For general contexts, there is a policy πachieving
RT(π;G, M ) =eOq
min
βM(G),m(G)	
T
,
where
βM(G) = max(MX
c=1|Ic|:I1,···, IMare disjoint independent subsets of [K])
. (3)
3Fortunately, additional assumptions on the feedback graph Gcan be leveraged to recover the tight
regret bound:
Corollary 1.5 (Upper bound for transtively closed or undirected feedback) .For any undirected
or transitively closed graph G, the policy πin Theorem 1.4 achieves a near-optimal regret
RT(π;G, M ) =eO(p
βM(G)T).
A directed graph Gis called transitively closed ifu→vandv→wimply that u→w. In reality
directed feedback graphs are often transitively closed, for a directed structure of the feedback typically
indicates a partial order over the actions. Examples include bidding in auctions (Zhao and Chen, 2019;
Han et al., 2024) and inventory control (Huh and Rusmevichientong, 2009), both of which exhibit
the one-sided feedback structure i→jfori≤j. For general graphs, Theorem 1.4 gives another
graph-theoretic quantity βM(G). Note that βM(G)≥βM(G)as there is no acyclic requirement
between Ic’s in(3), which in turn is due to a technical difficulty of non-self-avoiding contexts. Further
discussions on this gap are deferred to Section 4.3.
Interestingly, the upper bound quantities βM(G)andβM(G)are not explicitly linear in Mand are
always no larger than α(G)M. Hence our results partially answer an open problem in (Hao et al.,
2022, Remark 5.11) that if the dependence of regret bound O(p
α(G)MT)onMcan be improved.
1.3 Related work
The study of bandits with feedback graphs has a long history dating back to (Mannor and Shamir,
2011). For (both adversarial and stochastic) multi-armed bandits, a celebrated result in (Alon et al.,
2015, 2017) shows that the optimal regret scales as eΘ(p
α(G)T)ifT≥α(G)3; the case of smaller
Twas settled in (Kocák and Carpentier, 2023), where the optimal regret is a mixture of√
Tand
T2/3rates. For stochastic bandits, simpler algorithms based on arm elimination or upper confidence
bound (UCB) are also proposed (Lykouris et al., 2020; Han et al., 2024), while the UCB algorithm is
only known to achieve an upper bound of eO(p
m(G)T).2In addition to strongly observable graphs
we primarily focus on, weakly observable graphs have also drawn vast interest (Alon et al., 2015;
Chen et al., 2021) where the optimal regret is characterized by the dominating number δ(G). There
exploration plays a more significant role due to weaker observability of certain nodes, leading to an
optimal regret eΘ(δ(G)1/3T2/3). We will briefly discuss the regret characterization of our contextual
setting with weakly observable graphs in Section 4.1 and 4.2.
Recently, the graph feedback was also extended to contextual bandits under the name of “cross-
learning” (Balseiro et al., 2019; Schneider and Zimmert, 2024). The work (Balseiro et al., 2019)
considered both complete and partial cross-learning, and showed that the optimal regret for stochastic
bandits with complete cross learning is eΘ(√
KT). Motivated by bidding in first-price auctions,
(Han et al., 2024) generalized the setting to general graph feedback across actions and complete
cross-learning across contexts, a setting used in the current paper. The finding in (Han et al., 2024) is
that the effects of graph feedback and cross-learning could be “decoupled”: a regret upper bound
eO(p
min{α(G)M, K}T)is shown, which is tight only for a special choice of the feedback graph G.
The work (Dann et al., 2020) considered a tabular reinforcement learning setting with adversarial
initial states, so that their setting with episode length H= 1coincides with our problem with a general
feedback graph Gacross all context-action pairs. They showed that the UCB algorithm achieves
a regret upper bound eO(p
m(G)T); however, their lower bound was only Ω(p
α(G)T)when
T≥α(G)3. Therefore, tight lower bounds that work for general graphs Gare still underexplored in
the literature, and our regret upper bounds in Theorems 1.3 and 1.4 also improve or generalize the
existing results.
The problem of bandits with feedback is also closely related to partial monitoring games (Bartók
et al., 2014). Although this is a more general setting which subsumes bandits with graph feedback, the
results in the literature (Bartók et al., 2014; Lattimore, 2022; Foster et al., 2023a) typically have tight
dependence on T, but often not on other parameters such as the dimensionality. Similar issues also
applied to the recent line of work (Foster et al., 2021, 2023b) aiming to provide a unified complexity
measure based on the decision-estimation coefficient (DEC); the nature of the two-point lower bound
2The result of (Lykouris et al., 2020) was stated using the independence number, but they only considered
undirected graphs so that m(G) =α(G).
4used there often leaves a gap. We also point to some recent work (Zhang et al., 2024) which adopted
the DEC framework and established regret bounds for contextual bandits with graph feedback, but no
cross-learning across contexts, based on regression oracles.
2 Hard instance and the regret lower bound
In this section we sketch the proof of the minimax lower bound R⋆
T(G, M, CSA) = Ω(p
βM(G)T)
forT≥βM(G)3and general (G, M ), implying Theorem 1.1. We first identify a hard instance that
corresponds to the graph-theoretic quantity βM(G), and then present the core exploration-exploitation
tradeoff in the proof to arrive at the fundamental limit of learning under this instance. This approach
has been widely adopted in the bandit literature. The complete proof is deferred to Appendix B.
The proof uses the definition (2)ofβM(G)to construct Mindependent sets I1,···, IMsuch that
Ii̸→Ijfori < j ; by definition, the independent sets I1,···, IMare disjoint. We then construct a
hard instance where the best action under context c∈[M]is distributed uniformly over Ic; since Ic
is an independent set, this ensures that the learner must essentially explore all actions in Icunder
context c. Moreover, the context sequence cTis set to be 11···122···2···M, i.e. never goes back
to previous contexts. This order ensures that the exploration in Ic1during earlier rounds provides no
information to the exploration in Ic2during later rounds, whenever c1< c2. Naïvely, if the learner
only explores in each Icunder context c, then learning under each context cbecomes a multi-armed
bandit problem (because Icis itself an independent set), and we can show lower boundq
TPM
c=1|Ic|
with appropriate context sequence cT. Maximizing over all possible constructions gives the desired
result.
It is possible, however, for the learner to choose actions outside Icto obtain information for the later
rounds. To address this challenge, we use a delicate exploration-exploitation tradeoff argument to
show that this pure exploration must incur a too large regret to be informative when T≥βM(G)3.
Specifically, consider the regret incurred by this pure exploration:
Rexplore =MX
c=1X
t∈TcE[ 1(at̸∈Ic)]
where Tc={t∈[T] :ct=c}. Then for some absolute constants c1andc2, the tradeoff can be
formulated as two lower bounds of the regret RT:
RT≥c1p
βM(G)Texp(−βM(G)Rexplore/T)and RT≥c2Rexplore.
The first bound is decreasing in the amount of pure exploration, while the second one is increasing.
Balancing this tradeoff gives the desired lower bound RT= Ωp
βM(G)T
forT≥βM(G)3.
In summary, the key structure used in the proof is that Ii̸→Ijfori < j ; we remark that this does
not preclude the possibility that Ij→Iiforj > i , which underlies the change from α(G)tom(G)
as the number of context increases.
3 Algorithms achieving the regret upper bounds
This section provides algorithms that achieve the claimed regret upper bounds in Theorems 1.3 and
1.4. The crux of these algorithms is to exploit the structure of the feedback graph and choose a small
number of actions to explore. Depending on whether the context sequence is self-avoiding or not,
the above problem can be reduced to two different kinds of sequential games on the feedback graph.
Given solutions to the sequential games, Sections 3.2 and 3.3 will rely on the layering technique to
use these solutions on each layer, and propose the final learning algorithms via arm elimination.
3.1 Two sequential games on graphs
In this section we introduce two sequential games on graphs which are purely combinatorial and
independent of the learning process. We begin with the first sequential game.
5Definition 1 (Sequential game I) .Given a directed graph G= (V, E)and a positive integer M, the
sequential game consists of Msteps, where at each step c= 1,···, M:
1.the adversary chooses a strongly observable subset Ac⊆Vdisjoint from Nout(∪c′<cDc′);
2. the learner chooses Dc⊆Acsuch that Dcdominates Ac, i.e.Ac⊆Nout(Dc).3
The learner’s goal is to minimize the total sizePM
c=1|Dc|of the sets Dc.
The above sequential game is motivated by bandit learning under self-avoiding contexts. Consider a
self-avoiding context sequence in the order of 1,2,···, M. For c∈[M], the set Acrepresents the
“active set” of actions, i.e. the set of all probably good actions, yet to be explored when context cfirst
occurs. Thanks to the self-avoiding structure, “yet to be explored” means that Acmust be disjoint
from Nout(∪c′<cDc′). The learner then plays a set of actions Dc⊆Acto ensure that all actions in
Achave been explored at least once; we note that a good choice of Dcnot only aims to observe all
ofAc, but also tries to observe as many actions as possible outside Acand make the complement
ofNout(∪c′≤cDc′)small. The final costPM
c=1|Dc|characterizes the overall sample complexity to
explore every active action once over all contexts.
It is clear that the minimax value of this sequential game is given by
U⋆
1(G, M ) = max
A1⊆Vmin
D1⊆A1
A1⊆Nout(D1)··· max
AM⊆V
∪M−1
c=1Dc̸→AMmin
DM⊆AM
AM⊆Nout(DM)MX
c=1|Dc|. (4)
The following lemma characterizes the quantity U⋆
1(G, M )up to an O(log|V|)factor.
Lemma 3.1 (Minimax value of sequential game I) .There exists an absolute constant C >0that
βM(G)≤U⋆
1(G, M )≤CβM(G) log|V|.
Moreover, the learner can achieve a slightly larger upper bound O(βM(G) log2|V|)using a
polynomial-time algorithm.
The second sequential game is motivated by bandit learning with an arbitrary context sequence.
Definition 2 (Sequential game II) .Given a directed graph G= (V, E)and a positive integer M, the
sequential game starts with an empty set D0=∅, and at time t= 1,2,···:
1.the adversary chooses an integer ct∈[M](and a set Act⊆Vifctdoes not appear before).
The adversary must ensure that Act\Nout(Dt−1)is non-empty;
2. the learner picks a vertex vt∈Actand updates Dt←Dt−1∪ {vt}.
The game terminates at time Twhenever the adversary has no further move (i.e. ∪cAc⊆Nout(DT)),
and the learner’s goal is to minimize the duration Tof the game.
The new sequential game reflects the case where the context sequence might not be self-avoiding, so
instead of taking a set of actions at once, the learner now needs to take actions non-consecutively.
Clearly the sequential game II is more difficult for the learner as it subsumes the sequential game I
when the context sequence is self-avoiding: the set Dcin Definition 1 is simply the collection of vt’s
in Definition 2 whenever ct=c. Consequently, the minimax values satisfy U⋆
2(G, M )≥U⋆
1(G, M ).
The following lemma proves an upper bound on U⋆
2(G, M ).
Lemma 3.2 (Minimax value of sequential game II) .There exists a polynomial-time algorithm for the
learner which achieves
U⋆
2(G, M )≤βdom(G, M )≤min{m(G), CβM(G) log2|V|},
where C >0is an absolute constant, βM(G)is given in (3), and
βdom(G, M ) = maxMX
c=1|Bc|:[
cBcis acyclic, Bc⊆Vcdominates some Vc
with disjoint V1,···, VM⊆V,and|Bc| ≤δ(Vc)(1 + log |V|).
3Both sets (Ac, Dc)are allowed to be empty.
63.2 Learning under self-avoiding contexts
Given a learner’s algorithm for the first sequential game, we are ready to provide an algorithm for
bandit learning under any self-avoiding context sequence. The algorithm relies on the well-known
idea of arm elimination (Even-Dar et al., 2006): for each context c∈[M], we maintain an active
setAcconsisting of all probably good actions so far under this context based on usual confidence
bounds of the rewards. To embed the sequential games into the algorithm, we further make use of the
layering technique in (Lykouris et al., 2020; Dann et al., 2020): for ℓ∈N, we construct the set Ac,ℓ
as the active set on layer ℓsuch that all actions in Ac,ℓ−1have been taken for at least ℓ−1times. In
other words, the active set Ac,ℓis formed based on ℓ−1reward observations of all currently active
actions. As higher layer indicates higher estimation accuracy, the learner now aims to minimize the
duration of each layer ℓ, which is precisely the place we will play an independent sequential game.
Algorithm 1: Arm elimination algorithm for self-avoiding contexts
Input: time horizon T, action set [K], context set [M], feedback graph G, a subroutine Afor the
sequential game I, failure probability δ∈(0,1).
Initialize: active sets Ac,1←[K]for all contexts c∈[M]on layer 1.
forc= 1toMdo
forℓ= 1,2,···do
compute Dc,ℓ⊆Ac,ℓ\Nout(∪c′<cDc′,ℓ)according to the subroutine A, based on past
plays
(Ac′,ℓ\Nout(∪i<c′Di,ℓ))c′≤cand(Dc′,ℓ)c′<c;
choose each action in Dc,ℓonce (break the loop if ct̸=cort > T during this process),
and update taccordingly;
compute the empirical rewards ¯rc,afor all actions based on all historic reward
observations;
choose the following active set on the next layer:
Ac,ℓ+1←(
a∈Ac,ℓ: ¯rc,a≥max
a′∈Ac,ℓ¯rc,a′−2r
log(2MKT/δ )
ℓ)
; (5)
move to the next layer ℓ←ℓ+ 1.
end
end
The description of the algorithm is summarized in Algorithm 1, and we assume without loss of gener-
ality that the self-avoiding contexts comes in the order of 1, . . . , M (the duration of some contexts
might be zero). During each context, Algorithm 1 sequentially constructs a shrinking sequence of
active sets Ac,1⊇Ac,2⊇ ··· , and on each layer ℓ, the algorithm plays the sequential game I based
on the current status (past plays (Ac′,ℓ)c′≤c, or equivalently (Ac′,ℓ\Nout(∪i<c′Di,ℓ))c′≤c, of the
adversary, and past plays (Dc′,ℓ)c′<cof the learner).4After the rewards of all actions of Ac,ℓhave
been observed once, the algorithm constructs the active set Ac,ℓ+1for the next layer based on the
confidence bound (5) and sample size ℓ.
The following theorem summarizes the performance of the algorithm.
Theorem 3.3 (Regret upper bound of Algorithm 1) .Let the subroutine Afor the sequential game
I be the polynomial-time algorithm given by Lemma 3.1. Then with probability at least 1−δ, the
regret of Algorithm 1 is upper bounded by
RT(Alg1;G, M, CSA) =Oq
TβM(G) log2(K) log( MKT/δ )
.
On a high level, by classical confidence bound arguments, each action chosen on layer ℓsuffers
from an instantaneous regret eO(1/√
ℓ). Moreover, Lemma 3.1 shows that the number of actions
chosen on a given layer is at most eO(βM(G)). A combination of these two observations leads to the
eO(p
βM(G)T)upper bound in Theorem 3.3, and a full proof is provided in Appendix C.
4It is possible that, at some layer ℓand for some context c, every action active under chas been explored, i.e.
Ac,ℓ\Nout(∪c′<cDi,ℓ) =∅. In this case, the learner simply skips to next layers by choosing Dc=∅.
73.3 Learning under general contexts
The learning algorithm under a general context sequence is described in Algorithm 2. Similar to
Algorithm 1, for each context cwe break the learning process into different layers, construct active
setsAc,ℓfor each layer, and move to the next layer whenever all actions in Ac,ℓhave been observed
once on layer ℓ. The only difference lies in the choice of actions on layer ℓ, where the plays from the
sequential game II are now used. The following theorem summarizes the performance of Algorithm
2, whose proof is very similar to Theorem 3.3 and deferred to Appendix C.
Algorithm 2: Arm elimination under general contexts
Input: time horizon T, action set [K], context set [M], feedback graph G, a subroutine Afor the
sequential game II, failure probability δ∈(0,1).
Initialize: active sets Ac,ℓ←[K]for all contexts c∈[M]and layers ℓ≥1; set of actions
Dℓ←∅chosen on layer ℓ; the current layer index ℓ(c)←1for all c∈[M].
fort= 1toTdo
receive the context ct, and compute the current layer index ℓt=ℓ(ct);
according to subroutine A, choose an action at∈Act,ℓtbased on the active sets
(Ac,ℓt)c∈[M]and previously taken actions Dℓton the current layer;
update the set of actions on layer ℓtviaDℓt←Dℓt∪ {at};
forc∈[M]do
compute the new layer index ℓnew(c) = min {ℓ:Ac,ℓ⊈Nout(Dℓ)};
ifℓnew(c)> ℓ(c)then
compute the empirical rewards ¯rc,afor all actions based on all historic observations;
choose the following active set on the new layer:
Ac,ℓnew(c)←(
a∈Ac,ℓ(c): ¯rc,a≥max
a′∈Ac,ℓ(c)¯rc,a′−2s
log(2MKT/δ )
ℓnew(c)−1)
;
update the layer index ℓ(c)←ℓnew(c).
end
end
end
Theorem 3.4 (Regret upper bound of Algorithm 2) .Let the subroutine Afor the sequential game
II be the polynomial-time algorithm given by Lemma 3.2. Then with probability at least 1−δ, the
regret of Algorithm 2 is upper bounded by
RT(Alg2;G, M ) =Op
Tβdom(G, M ) log( MKT/δ )
.
By the second inequality in Lemma 3.2, Theorem 3.4 implies Theorem 1.4. Corollary 1.5 then
follows from the following result.
Lemma 3.5. For undirected or transitively closed graph G, it holds that βdom(G, M ) =
O(βM(G) log|V|).
4 Discussions
4.1 Weakly observable feedback graphs
Naturally, we may ask what results we would get under a weaker assumption / a more general
feedback structure. If the feedback graph Gis instead weakly observable5, then under complete
cross-learning, an explore-then-commit (ETC) policy can achieve regret eO(δ(G)1/3T2/3)by first
exploring the minimum dominating set6uniformly for time δ(G)1/3T2/3, and then committing to the
empirically best action that has suboptimality bounded by eO(δ(G)1/3T−1/3)with high probability.
This matches the existing lower bound in (Alon et al., 2015) and is hence near-optimal.
5In the language of (Alon et al., 2015), a graph Gis weakly observable if Nin(a)̸=∅for all a∈[K], and
there exists a0∈[K]such that {a0},[K]\{a0}⊈Nin(a0).
6Note that a (1 + log K)-approximate minimum dominating set can be found efficiently by Lemma A.2.
84.2 Incomplete cross-learning
It is possible to further relax the assumption of complete cross-learning. Suppose the feedback
across contexts is characterized by another directed graph G[M](and denote G[K]across actions
respectively), and consider a product feedback graph G[K]×G[M]over the context-action pairs such
that(a1, c1)→(a2, c2)ifa1→a2inG[K]andc1→c2inG[M]. Then we can get the following
generalized results.
4.2.1 Weakly observable feedback graphs on actions
When the feedback graph G[K]is weakly observable, following the argument in Section 4.1, we
can achieve regret eO 
δ(G[K])m(G[M])1/3T2/3
by running an ETC subroutine for each context
as follows: for every context c∈[M], we keep an “exploration” counter nc. At each time twith
context ct, ifnct≲δ(G[K])1/3m(G[M])−2/3T2/3, we are in the “exploration” stage and continue
to uniformly explore the minimum dominating set of G[K]. Then we increase the counter for all
observed contexts, i.e. nc←nc+ 1for all c∈Nout(ct)inG[M]. Otherwise, we “commit” to the
empirically best action that has suboptimality bounded by eO 
δ(G[K])m(G[M])1/3T−1/3
with
high probability.
The key observation is that the number of times we are in the “exploration” stage is
eO 
δ(G[K])m(G[M])1/3T2/3
. This can be seen from a layering argument, similar to the one
in Section 3.2, that the number of actually played contexts on each layer is at most m(G[M]). To-
gether with the bounded rewards and the bounded suboptimality in the “commit” stage, this proves
the regret upper bound.
Combining the context sequence construction in Section 2 and the lower bound argument in (Alon
et al., 2015), one can also prove a matching lower bound Ω 
δ(G[K])m(G[M])1/3T2/3
.
4.2.2 Strongly observable feedback graphs on actions
When G[K]is strongly observable, it is straightforward to generalize our upper (for self-avoiding
contexts) and lower bounds in Theorem 1.1 and 1.3 with βM(G[K])replaced by
βM(G[K]×G[M]) = maxMX
c=1|Ic|:Icindependent subset of [K]× {c},andIc̸→Ic′forc < c′
andβdom(G[K])in Theorem 3.4 by
βdom(G[K]×G[M]) = maxMX
c=1|Bc|:[
cBcis acyclic in G[K]×G[M],
Bcis a(1 + log K)-approx min dominating set of some subsets Vc⊆G[K]× {c}
where the new graph quantities are defined on the product graph G[K]×G[M]. For general
contexts, this gives a tight upper bound eO p
TβM(G[K]×G[M])
when G[K]andG[M]are ei-
ther both undirected or both transitively closed7. Most generally, we have a loose upper bound
eO p
Tmin{m(G[K]×G[M]), α(G[K])M}
.
4.3 Gap between upper and lower bounds
Although we provide tight upper and lower bounds for specific classes of context sequences (self-
avoiding in Theorem 1.3) or feedback graphs (undirected or transitively closed in Corollary 1.5), in
general the quantities βM(G)in Theorem 1.1 and min{βM(G),m(G)}in Theorem 1.4 exhibit a
gap. The following lemma gives an upper bound on this gap.
7We prove this statement in Appendix C.7 due to space limit.
9Lemma 4.1. For any graph G, it holds that
βM(G)≤min{βM(G),m(G)} ≤maxρ(G)
M,1
βM(G),
where ρ(G)denotes the length of the longest path in G.
Lemma 4.1 shows that if Gdoes not contain long paths or Mis large, the gap between βM(G)and
βM(G)is not significant. We also comment on the challenge of closing this gap. First, we do not
know a tight characterization of the minimax value of the sequential game II (cf. Definition 2), and
the upper bound βdom(G, M )in Lemma 3.2 could be loose, as shown in the following example.
Example 1. Consider an acyclic graph G= (V, E)withKM vertices {(i, j)}i∈[K],j∈[M]and
edges (i, j)→(i′, j′)if either i < i′andj̸=j′, ori=i′andj < j′, and all self-loops. By
choosing Bc=Vc={(i, c) :i∈[K]}in the definition of βdom(G, M )in Lemma 3.2, it is clear that
βdom(G, M ) =KM . However, we show that the minimax value is U⋆
2(G, M ) =K+M−1. The
lower bound follows from U⋆
2(G, M )≥U⋆
1(G, M )≥βM(G)≥K+M−1, asI1={(i, M) :
i∈[K]}andIc={(1, M+ 1−c)}for2≤c≤Msatisfy the constraints in the definition
ofβM(G)in(2). For the upper bound, we consider the following strategy for the learner in the
sequential game II: vt= (it, jt)is the smallest element (under the lexicographic order over pairs) in
Act\Nout(Dt−1). To show why U⋆
2(G, M )≤K+M−1, letDcbe the final set of vertices chosen
by the learner under context c. By the lexicographic order and the structure of G, each Dccan only
consist of vertices in one column. Moreover, for different c̸=c′, the row indices of Dcmust be
entirely no smaller or entirely no larger than the row indices of Dc′. These constraints ensure thatPM
c=1|Dc| ≤K+M−1.
This example shows the importance of non-greedy approaches when choosing vt. In the special case
where Ac={(i, c) :i∈[K]}is the c-th column, within Acthis is an independent set, so any greedy
approach that does not look outside Acwill treat the vertices in Acindifferently. In contrast, the
above approach makes use of the global structure of the graph G.
The second challenge lies in the proof of the lower bound. Instead of the sequential game where the
adversary and the learner take turns to play actions, the current lower bound argument assumes that
the adversary tells all his plays to the learner ahead of time. We expect the sequential structure to be
equally important for the lower bounds, and it is interesting to work out an argument for the minimax
lower bound to arrive at a sequential quantity like U⋆
2(G, M ).
4.4 Other open problems
Performance of the UCB algorithm. The UCB algorithm under feedback graphs has been analyzed
for both multi-armed (Lykouris et al., 2020) and contextual bandits (Dann et al., 2020). However,
both results only show a regret upper bound eO(p
m(G)T), even in the case of multi-armed bandits
(i.e.M= 1). It is interesting to understand for algorithms without forced exploration (such as UCB),
if the MAS number m(G)(rather than α(G)orβM(G)) turns out to be fundamental.
Regret for small T.Note that our upper bounds hold for all values of T, but our lower bound
requires T≥βM(G)3. This is not an artifact of the analysis, as the optimal regret becomes
fundamentally different for smaller T. The case of multi-armed bandits has been solved completely
in a recent work (Kocák and Carpentier, 2023), where the regret is a mixture of√
TandT2/3terms.
We anticipate the same behavior for contextual bandits, but the exact form is unknown.
Stochastic contexts. In this paper we assume that the contexts are generated adversarially, but the
case of stochastic contexts also draws some recent attention (Balseiro et al., 2019; Schneider and
Zimmert, 2024), and sometimes there is a fundamental gap between the optimal performances under
stochastic and adversarial contexts (Han et al., 2024). It is an interesting question whether this is the
case for contextual bandits with graph feedback.
Acknowledgement
This work is generously funded by the NSF grant CCF 2106508.
10References
Alekh Agarwal, Miroslav Dudík, Satyen Kale, John Langford, and Robert Schapire. Contextual
bandit learning with predictable rewards. In Artificial Intelligence and Statistics , pages 19–26.
PMLR, 2012.
Shipra Agrawal and Nikhil Devanur. Linear contextual bandits with knapsacks. Advances in Neural
Information Processing Systems , 29, 2016.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In
International conference on machine learning , pages 127–135. PMLR, 2013.
Noga Alon and Joel H Spencer. The probabilistic method . John Wiley & Sons, 2016.
Noga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback
graphs: Beyond bandits. In Conference on Learning Theory , pages 23–35. PMLR, 2015.
Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir.
Nonstochastic multi-armed bandits with graph-structured feedback. SIAM Journal on Computing ,
46(6):1785–1826, 2017.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research , 3(Nov):397–422, 2002.
Santiago Balseiro, Negin Golrezaei, Mohammad Mahdian, Vahab Mirrokni, and Jon Schneider. Con-
textual bandits with cross-learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32.
Curran Associates, Inc., 2019.
Gábor Bartók, Dean P Foster, Dávid Pál, Alexander Rakhlin, and Csaba Szepesvári. Partial monitor-
ing—classification, regret bounds, and algorithms. Mathematics of Operations Research , 39(4):
967–997, 2014.
Djallel Bouneffouf, Irina Rish, and Charu Aggarwal. Survey on applications of multi-armed and
contextual bandits. In 2020 IEEE Congress on Evolutionary Computation (CEC) , pages 1–8. IEEE,
2020.
Houshuang Chen, Shuai Li, Chihao Zhang, et al. Understanding bandits with graph feedback.
Advances in Neural Information Processing Systems , 34:24659–24669, 2021.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics , pages 208–214. JMLR Workshop and Conference Proceedings, 2011.
Vasek Chvatal. A greedy heuristic for the set-covering problem. Mathematics of operations research ,
4(3):233–235, 1979.
Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, and Karthik Sridharan. Reinforce-
ment learning with feedback graphs. Advances in Neural Information Processing Systems , 33:
16868–16878, 2020.
Khaled Eldowa, Emmanuel Esposito, Tom Cesari, and Nicolò Cesa-Bianchi. On the minimax regret
for online learning with feedback graphs. Advances in Neural Information Processing Systems , 36,
2024.
Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and
stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of
machine learning research , 7(6), 2006.
Uriel Feige, Shafi Goldwasser, László Lovász, Shmuel Safra, and Mario Szegedy. Approximating
clique is almost np-complete. In [1991] Proceedings 32nd Annual Symposium of Foundations of
Computer Science , pages 2–12. IEEE Computer Society, 1991.
11Dean Foster, Dylan J Foster, Noah Golowich, and Alexander Rakhlin. On the complexity of multi-
agent decision making: From learning in games to partial monitoring. In The Thirty Sixth Annual
Conference on Learning Theory , pages 2678–2792. PMLR, 2023a.
Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of
interactive decision making. arXiv preprint arXiv:2112.13487 , 2021.
Dylan J Foster, Noah Golowich, and Yanjun Han. Tight guarantees for interactive decision making
with the decision-estimation coefficient. In The Thirty Sixth Annual Conference on Learning
Theory , pages 3969–4043. PMLR, 2023b.
Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem.
Advances in Neural Information Processing Systems , 32, 2019.
Fabrizio Grandoni. A note on the complexity of minimum dominating set. Journal of Discrete
Algorithms , 4(2):209–214, 2006.
Yanjun Han, Tsachy Weissman, and Zhengyuan Zhou. Optimal no-regret learning in repeated
first-price auctions. Operations Research , 2024.
Botao Hao, Tor Lattimore, and Chao Qin. Contextual information-directed sampling. In International
Conference on Machine Learning , pages 8446–8464. PMLR, 2022.
Woonghee Tim Huh and Paat Rusmevichientong. A nonparametric asymptotic analysis of inventory
planning with censored demand. Mathematics of Operations Research , 34(1):103–123, 2009.
Richard M Karp. Reducibility among combinatorial problems . Springer, 2010.
Tomáš Kocák and Alexandra Carpentier. Online learning with feedback graphs: The true shape of
regret. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine
Learning , volume 202 of Proceedings of Machine Learning Research , pages 17260–17282. PMLR,
23–29 Jul 2023.
Tor Lattimore. Minimax regret for partial monitoring: Infinite outcomes and rustichini’s regret. In
Conference on Learning Theory , pages 1547–1575. PMLR, 2022.
Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual
bandits. In International Conference on Machine Learning , pages 2071–2080. PMLR, 2017.
Thodoris Lykouris, Éva Tardos, and Drishti Wali. Feedback graph regret bounds for thompson
sampling and ucb. In Aryeh Kontorovich and Gergely Neu, editors, Proceedings of the 31st
International Conference on Algorithmic Learning Theory , volume 117 of Proceedings of Machine
Learning Research , pages 592–614. PMLR, 08 Feb–11 Feb 2020.
Shie Mannor and Ohad Shamir. From bandits to experts: On the value of side-observations. In
J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in
Neural Information Processing Systems , volume 24. Curran Associates, Inc., 2011.
Jon Schneider and Julian Zimmert. Optimal cross-learning for contextual bandits with unknown
context distributions. Advances in Neural Information Processing Systems , 36, 2024.
Mengxiao Zhang, Yuheng Zhang, Olga Vrousgou, Haipeng Luo, and Paul Mineiro. Practical
contextual bandits with feedback graphs. Advances in Neural Information Processing Systems , 36,
2024.
Haoyu Zhao and Wei Chen. Stochastic one-sided full-information bandit. In Joint European Con-
ference on Machine Learning and Knowledge Discovery in Databases , pages 150–166. Springer,
2019.
Yinglun Zhu and Paul Mineiro. Contextual bandits with smooth regret: Efficient learning in continu-
ous action spaces. In International Conference on Machine Learning , pages 27574–27590. PMLR,
2022.
12A Auxilary Lemmas
Lemma A.1 (Lemma 8 of (Alon et al., 2015)) .For any directed graph G= (V, E), one has
δ(G)≤50α(G) log|V|.
For a directed graph G, there is a well-known approximate algorithm for finding the smallest
dominating set: starting from D=∅, recursively find the vertex vwith the maximum out-degree in
the subgraph induced by V\Nout(D), and update D←D∪ {v}. The following lemma summarizes
the performance of this algorithm.
Lemma A.2 (Chvatal (1979)) .For any graph G= (V, E), the above procedure outputs a dominating
setDwith
|D| ≤(1 + log |V|)δ(G).
Lemma A.3 (A special case of Lemma 3 in (Gao et al., 2019)) .LetQ1, . . . Q nbe probability
measures on some common measure space (Ω,F), with n≥2, and Φ : Ω →[n]any measurable
test function. Then
1
nnX
i=1Qi(Φ̸=i)≥1
2nnX
i=2exp(−KL(Qi∥Q1)).
B Deferred Proof for the Lower Bound
In this appendix, we give the complete proof of the minimax lower bound R⋆
T(G, M, CSA) =
Ω(p
βM(G)T)forT≥βM(G)3and general (G, M ), implying Theorem 1.1.
LetI1,···, IMbe the independent sets achieving the maximum in (2), by removing empty sets,
combining (Ii, Ii+1)whenever |Ii|= 1, and possibly removing the last set IMif|IM|= 1, we arrive
at disjoint subsets J1,···, Jmof[K]such that the following conditions hold:
•m≤M,Kc≜|Jc| ≥2for all c∈[m], and Ji̸→Jjfori < j ;
•the only possible non-self-loop edges among Jc={ac,1,···, ac,Kc}can only point to ac,1;
•Pm
c=1Kc≥PM
c=1|Ic| −1 =βM(G)−1≥βM(G)/2whenever βM(G)≥2.8
Given sets J1,···, Jm, we are ready to specify the hard instance. Let u= (u1,···, um)∈Ω :=
[K1]× ··· × [Km]be a parameter vector, the joint reward distribution Puof(rt,c,a)c∈[M],a∈[K]is a
product distribution Pu=Q
c∈[M],a∈[K]Bern(µu
c,a), where the mean parameters for the Bernoulli
distributions are µu
c,a= 0whenever c > m , and
µu
c,a=

1
4+ ∆ ifa=ac,1,
1
4+ 2∆ ifa=ac,ucanduc̸= 1,
1
4ifa∈Jc\{ac,uc},
0 ifa /∈Jc,forc∈[m].
Here ∆∈(0,1/4)is a gap parameter to be chosen later. We summarize some useful properties from
the above construction:
1.under context c∈[m], the best action under Puisac,uc, and all other actions suffer from an
instantaneous regret at least ∆;
2. under context c∈[m], actions outside Jcsuffer from an instantaneous regret at least 1/4;
3.foru= (u1,···, um)∈Ωanduc:= (u1,···, uc−1,1, uc+1,···, um), the KL divergence
between the observed reward distributions Pu(a)andPuc(a)when choosing the action ais
KL(Puc(a)∥Pu(a))(a)=KL(Bern(1/4)∥Bern(1/4 + 2∆)) ifa→ac,uc
0 otherwise
(b)
≤64∆2
31(a /∈J≤c\{ac,uc}).
8When βM(G) = 1 , theΩ(√
T)regret lower bound is trivially true even under full information feedback
andM= 1.
13Here (a) follows from our construction of Puthat the only difference between Puand
Pucis the reward of action ac,uc, which is observed iff a→ac,uc; (b) is due to the
property of (J1,···, Jm)that any action in J≤c\{ac,uc}does not point to ac,uc, where
J≤c:=∪c′≤cJc′.
Finally, we partition the time horizon [T]into consecutive blocks T1,···, Tm(whose sizes will be
specified later), and choose the context sequence as ct=cfor all t∈Tc. For a fixed policy, let RT
be the worst-case expected regret of this policy. By the second property of the construction, it is clear
that for all u∈Ω,
RT≥1
4mX
c=1X
t∈TcE(Pu)⊗(t−1)[ 1(at̸∈Jc)]. (6)
When uis uniformly distributed over Ω, we also have
RT(a)
≥Eu"
∆mX
c=1X
t∈TcE(Pu)⊗(t−1)[ 1(at̸=ac,uc)]#
(b)= ∆mX
c=1X
t∈TcEu\{uc}
Euc
E(Pu)⊗(t−1)[ 1(at̸=ac,uc)]
(c)
≥∆mX
c=1X
t∈TcEu\{uc}"
1
2KcKcX
uc=2exp
−KL
(Puc)⊗(t−1)(Pu)⊗(t−1)#
(7)
(d)
≥∆mX
c=1X
t∈TcEu\{uc}"
Kc−1
2Kcexp 
−1
Kc−1KcX
uc=2KL
(Puc)⊗(t−1)(Pu)⊗(t−1)!#
(e)
≥∆
4mX
c=1X
t∈TcEu\{uc}"
exp 
−64∆2
3(Kc−1)KcX
uc=2X
s<tE(Puc)⊗(s−1)[ 1(as/∈J≤c\{ac,uc})]!#
,
where (a) lower bounds the minimax regret by the Bayes regret, with the help of the first property;
(b) decomposes the expectation over uniformly distributed uintou\{uc}anduc∈[Kc]; (c) follows
from Lemma A.3; (d) uses the convexity of x7→e−x; (e) results from the chain rule of KL divergence,
the third property of the construction, and that Kc≥2for all c∈[m].
Next we upper bound the exponent in (7) as
KcX
uc=2X
s<tE(Puc)⊗(s−1)[ 1(as/∈J≤c\{ac,uc})]
≤KcX
uc=2X
c′<cX
s∈Tc′E(Puc)⊗(s−1)[ 1(as/∈Jc′)] +KcX
uc=2X
s∈Tcs<tE(Puc)⊗(s−1)[ 1(as/∈J≤c) + 1(as=ac,uc)]
≤KcX
uc=2X
c′≤cX
s∈Tc′E(Puc)⊗(s−1)[ 1(as/∈Jc′)] +KcX
uc=2X
s∈TcE(Puc)⊗(s−1)[ 1(as=ac,uc)]
(6)
≤4(Kc−1)RT+KcX
uc=2X
s∈TcE(Puc)⊗(s−1)[ 1(as=ac,uc)].
Plugging it back into (7), we get
RT≥∆
4mX
c=1X
t∈TcEu\{uc}"
exp 
−64∆2
3(Kc−1) 
4(Kc−1)RT+KcX
uc=2X
s∈TcE(Puc)⊗(s−1)[ 1(as=ac,uc)]!!#
(f)
≥∆
4MX
c=1X
t∈TcEu\{uc}
exp
−64∆2
3
4RT+|Tc|
Kc−1
,
14where (f) crucially uses that uc= (u1,···, uc−1,1, uc+1,···, um)does not depend on uc, so that
the sum may be moved inside the expectation to getPKc
uc=21(as=ac,uc)≤1. Now choosing
|Tc|=KcPm
c′=1Kc′·T≤2KcT
βM(G), ∆ =r
βM(G)
16T∈
0,1
4
,
we arrive at the final lower bound
RT≥p
βM(G)T
16exp
−4βM(G)
3T
4RT+4T
βM(G)
≥p
βM(G)T
16e6exp
−16βM(G)
3TRT
≥p
βM(G)T
16e6exp 
−16RT
3p
βM(G)T!
, (8)
where the last inequality is due to the assumption T≥βM(G)3. Now we readily conclude from (8)
the desired lower bound RT= Ω(p
βM(G)T).
C Deferred Proofs for the Upper Bounds
Throughout the proofs, we will use α(A)≜α(G|A)(resp. δ(A),m(A)) to denote the independence
number (resp. dominating number, MAS number) of the subgraph induced by A⊆Vwhen the graph
Gis clear from the context.
C.1 Proof of Lemma 3.1
The lower bound U⋆
1(G, M )≥βM(G)is easy: let I1,···, IMbeMindependent sets with Ii̸→Ij
for all i < j . Then the choice Ac=Icis always feasible for the adversary, for Icis disjoint from
Nout(∪c′<cIc′). For the learner, the only subset Dc⊆Icwhich dominates IcisDc=Ic, hence
U⋆
1(G, M )≥PM
c=1|Ic|. Taking the maximum then gives U⋆
1(G, M )≥βM(G)by (2).
To prove the upper bound U⋆
1(G, M )≤CβM(G) log|V|, the learner chooses Dcas follows. Given
Ac, the learner finds the smallest dominating set Jc⊆Acand the largest independent set Ic⊆Ac,
and sets Dc=Ic∪Jc. Clearly this choice of Dcis feasible for the learner, and since Ii̸→Ajfor
i < j , we have Ii̸→Ijas well. Consequently,
U⋆(G, M )≤MX
c=1|Dc| ≤MX
c=1(|Jc|+|Ic|)(a)=MX
c=1O(|Ic|log|V|)(b)=O(βM(G) log|V|),
where (a) uses |Jc|=δ(Ac) =O(α(Ac) log|V|) =O(|Ic|log|V|)in Lemma A.1, and (b) follows
from the definition of βM(G)in (2).
Since it is NP-hard to find either the smallest dominating set or the largest independent set (Karp,
2010; Grandoni, 2006), the above choice of Dcis not computationally efficient. To arrive at a
polynomial-time algorithm, we may use a greedy algorithm to find an O(log|V|)-approximate
smallest dominating set eJcsuch that |eJc|=O(δ(Ac) log|V|)(cf. Lemma A.2). For Ic, although
finding the largest independent set is APX-hard (Feige et al., 1991), the constructive proof of (Alon
et al., 2015, Lemma 8) gives a polynomial-time randomized algorithm which finds eIc⊆Acsuch
that|eIc|= Ω( δ(Ac)/log|V|)and the average degree among eIcis at most O(1). The learner now
chooses Dc=eIc∪eJc. By the average degree constraint and Turán’s theorem (Alon and Spencer,
2016, Theorem 3.2.1), each eIccontains an independent subset Icwith|Ic|= Ω(|eIc|). Since
|eJc|=O(δ(Ac) log|V|) =O(|eIc|log2|V|) =O(|Ic|log2|V|),
we conclude thatPM
c=1|Dc|=O(PM
c=1|Ic|log2|V|) =O(βM(G) log2|V|).
C.2 Proof of Lemma 3.2
The second inequality is straightforward: βdom(G, M )≤m(G)since∪cBcis acyclic, and the
other inequality follows from |Bc|=O(δ(Vc) log|V|) =O(α(Vc) log2|V|)in Lemma A.1. To
15prove the first inequality, we consider a simple greedy algorithm for the learner, where vt∈Act
is the vertex with the largest out-degree in the induced subgraph by Act\Nout(Dt−1). Intuitively,
Act\Nout(Dt−1)is the set of nodes in Actthat remain unexplored by the learner by time t. Under
this greedy algorithm, for c∈[M], define
Vc=[
t:ct=c
Nout(vt)\
(Act\Nout(Dt−1))
, B c={vt:ct=c}.
We claim that Vcare pairwise disjoint and |Bc| ≤δ(Vc)(1 + log |V|), and thereby complete the proof
ofPM
c=1|Bc| ≤βdom(G, M ). The first claim simply follows from the pairwise disjointness of the
setsNout(vt)T(Act\Nout(Dt−1))for different t. For the second claim, let t1<···< tnbe all the
time steps where ct=c, and
Vc,i≜n[
j=i
Nout(vtj)\
(Ac\Nout(Dtj−1))
, i∈[n].
It is clear that Vc,i+1=Vc,i\Nout(vti). Since vtihas the largest out-degree in the induced subgraph
byAc\Nout(Dti−1)⊇Vc,i, and Nout(vti)∩(Ac\Nout(Dti−1)) = Nout(vti)∩Vc,i, this is also
the vertex with the largest out-degree in Vc,i. Therefore, the sets {Vc,i}n+1
i=1evolve from Vc,1=Vc
toVc,n+1=∅as follows: one recursively picks the vertex with the largest out-degree in Vc,i, and
removes its out-neighbors to get Vc,i+1. This is a well-known approximate algorithm for computing
δ(Vc), described above Lemma A.2, with
|Bc|=n≤δ(Vc)(1 + log |Vc|)≤δ(Vc)(1 + log |V|),
as desired.
C.3 Proof of Lemma 3.5
IfGis undirected, then βdom(G)≤m(G) =α(G) =βM(G)easily holds. It remains to consider
the case where Gis transitively closed.
Note that in a transitively closed graph, every vertex set has an independent dominating subset (by
tracing to the ancestors). Therefore, for the maximizing sets B1,···, BMin the definition of βdom,
we may run the above procedure to find independent dominating subsets I1,···, IMofV1, . . . , V M
respectively, with Ic⊆Bcand
MX
c=1|Ic| ≥MX
c=1δ(Vc)≥1
C′log|V|MX
c=1|Bc|.
Now consider the induced subgraph G′by∪cIc. Clearly G′is acyclic, and the length of longest path
inG′is at most M(otherwise, two points on a path belong to the same Ic, and transitivity will violate
the independence). Invoking Lemma 4.1 now gives
MX
c=1|Ic|=m(G′)≤ρ(G′)
MβM(G′)≤βM(G′)≤βM(G),
and combining the above two inequalities completes the proof.
C.4 Proof of Theorem 3.3
The proof is decomposed into three claims: with probability at least 1−δ,
1.for every c∈[M]andℓ∈N, when the confidence bound (5)is formed, either every action
inAc,ℓhas been observed for at least ℓtimes or |Ac,ℓ|= 1;
2.for every c∈[M]andℓ∈N, the best action a⋆(c)under context cbelongs to Ac,ℓ,
and every action in Ac,ℓhas suboptimality gap at most min{1,4∆ℓ−1}, with ∆ℓ≜p
log(2MKT/δ )/ℓ;
3.for every ℓ∈N, the total number Nℓof actions taken on layer ℓin those subsets Ac,ℓwith
|Ac,ℓ|>1isO(βM(G) log2K).
16The first claim simply follows from (1) when Gis strongly observable, any subset of size >1is
strongly observable, and (2) Ac,ℓ⊆Nout(∪c′≤cDc′,ℓ)required by the sequential game I, so that on
each layer ℓ′∈[ℓ]there is at least one action which observes a∈Ac,ℓ⊆Ac,ℓ′. For the second claim,
the first claim, the usual Hoeffding concentration, and a union bound imply that |¯rc,a−µc,a| ≤∆ℓ
in(5)for all c∈[M]anda∈Ac,ℓ, when |Ac,ℓ|>1and with probability at least 1−δ. Conditioned
on this event:
•The best arm a⋆(c)is not eliminated by (5), because ¯rc,a⋆(c)≥µc,a⋆(c)−∆ℓ≥
max a′∈Ac,ℓµc,a′−∆ℓ≥max a′∈Ac,ℓ¯rc,a′−2∆ℓ;
•The instantaneous regret of choosing any action a∈Ac,ℓ+1is at most min{1,4∆ℓ}, for
µc,a≥¯rc,a−∆ℓ≥¯rc,a⋆(c)−3∆ℓ≥µc,a⋆(c)−4∆ℓ, and|µc,a−µc,a⋆(c)| ≤1trivially
holds.
Consequently the second claim holds for |Ac,ℓ|>1. For the case |Ac,ℓ|= 1, note that starting from
Ac,1= [K], the above argument implies that the best arm is never eliminated until |Ac,ℓ′|= 1 at
some layer ℓ′≤ℓconditioned on the high probability event, which implies the single action in Ac,ℓ
is the best action and incurs 0regret. The last claim is simply the reduction to the sequential game I,
where Lemma 3.1 shows that Nℓ=PM
c=1|Dc,ℓ|=O(βM(G) log2K).
Combining the above claims and that we incur 0regret whenever |Ac,ℓ|= 1, with probability at least
1−δ, we have
RT(Alg1;G, M, CSA)≤∞X
ℓ=1Nℓmin{1,4∆ℓ},
where Nℓ≤N:=O(βM(G) log2K), andP∞
ℓ=1Nℓ=T. It is straightforward to see that the
choice N1=···=Nm=NandNm+1=T−Nm for a suitable m∈Nmaximizes the above
sum, and the maximum value is the target regret upper bound in Theorem 3.3.
C.5 Proof of Theorem 3.4
The proof follows verbatim the same lines in the proof of Theorem 3.3, except that the total number
Nℓof actions taken on layer ℓis now at most βdom(G, M )in the third claim, by Lemma 3.2.
C.6 Proof of Lemma 4.1
LetV1⊆Vbe a maximum acyclic subset, then ρ(G|V1)≤ρ(G). Consider the following recursive
process: at time t= 1,2,···, letJtbe the set of vertices in Vtwith in-degree zero (which always
exist as Vtis acyclic), and Vt+1=Vt\Jt. This recursion can only last for at most ρ(G)steps, for
otherwise there is a path of length larger than ρ(G). Then each Jtis an independent set, for every
vertex of Jthas in-degree zero in Vt⊇Jt. For the same reason we also have Ji̸→Jjfori > j . This
means that
m(G) =|V1|=X
t|Jt| ≤maxρ(G)
M,1
βM(G),
where the last inequality follows from picking Mlargest sets among {Jt}.
C.7 Proof of the statement in Section 4.2.2
In this section, we show that when G[K]andG[M]are either both undirected or both transitively
closed, the product graph quantities βdom:=βdom(G[K]×G[M])andβM:=βM(G[K]×G[M])
satisfy
βdom=O(βMlogK).
Combining with the Ω(√βMT)lower bound, this shows the tightness of the upper bound
eO √βdomT
. The idea here is similar to Section C.3:
When G[K]andG[M]are both undirected, the union set ∪cBcin the definition of βdomis an
independent set thanks to the acyclic requirement. Thus βdom≤βM.
17When G[K]andG[M]are both transitively closed, for the maximizing sets B1, . . . , B Min the
definition of βdom, we can again find independent dominating subsets Ic⊆Bc(by transitive closure
ofG[K]) with
MX
c=1|Ic| ≥1
C′logKMX
c=1|Bc|=1
C′logKβdom. (9)
Now it suffices to find independent subsets Jc⊆[K]× {c}that satisfy Jc̸→Jc′when c < c′
andP
c|Jc|=P
c|Ic|. Toward this end, we first suppose there are candc′such that uc→uc′
andvc←vc′foruc, vc∈Icanduc′, vc′∈Ic′.This implies c↔c′inG[M]by the product graph
structure. Then
•Ic|[K]andIc′|[K]are disjoint sinceS
cIcis acyclic;
•by transitive closure of G[K]and that Ic,Ic′are independent, Ic|[K]∪Ic′|[K]has path length
at most 1;
•from above, there exist disjoint and independent sets S1andS2such that S1∪S2=
Ic|[K]∪Ic′|[K]andS1̸→S2.
where we denote the set projection
S|[K]={a∈[K] : (a, c)∈Sfor some c∈[M]}.
Without loss of generality, assume c < c′. In this case, we can “rearrange” them by letting Jc=
S1× {c}andJc′=S2× {c′}, soJc̸→Jc′. Now suppose there is a loop on the set level, i.e. there
arec1, . . . , c m∈[M]withIc1→ ··· → Icm→Ic1. Similarly, we must have that {c1, . . . , c m}
form a clique in G[M],Ic1|[K], . . . , I cm|[K]disjoint in G[K], and the path length inSm
j=1Icjis at
most m. Then we can again “rearrange” them and get independent sets Jcj̸→Jckforcj< ck
andj, k∈[m], andPm
j=1|Jcj|=Pm
j=1|Icj|. In other cases, we simply let Jc=Icand arrive at
J1, . . . , J Mthat are acyclic on the set level, i.e. up to reordering of the indices, we have Jc̸→Jc′
when c < c′. Together with Eq (9), we have
βdom≤C′logKMX
c=1|Ic|=C′logKMX
c=1|Jc| ≤C′βMlogK.
18NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The main claims in the abstract and introduction include a lower bound and
two upper bounds (and algorithms achieving them) for the considered setup, which are the
contributions of this work.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The gap between the lower bound and the upper bound under the most general
assumptions is discussed with an example in Section 4.3.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
19Answer: [Yes]
Justification: The complete proof for the lower bound is in Appendix B and those for
the upper bounds are in Appendix C. Auxilary lemmas are collected and referenced in
Appendix A. The problem setup is in Section 1.2. Assumptions are stated in the theorem
statements.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: This work does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
205.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: This work does not include experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: This work does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: This work does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
21•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: This work does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have reviewed the NeurIPS code of ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This work aims to understand the theoretical limits of a family of contextual
bandit problems and has no direct societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
22•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This work is theoretical and has no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This work does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
23•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This work does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This work does not involve or include such experiments.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This work does not involve or include such experiments.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
24