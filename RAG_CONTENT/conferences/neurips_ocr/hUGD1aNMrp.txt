Assouad, Fano, and Le Cam with Interaction:
A Unifying Lower Bound Framework
and Characterization for Bandit Learnability
Fan Chen
MIT
fanchen@mit.eduDylan J. Foster
Microsoft Research
dylanfoster@microsoft.comYanjun Han
New York University
yanjunhan@nyu.edu
Jian Qian
MIT
jianqian@mit.eduAlexander Rakhlin
MIT
rakhlin@mit.eduYunbei Xu
National University of Singapore
yunbei@nus.edu.sg
Abstract
We develop a unifying framework for information-theoretic lower bound in
statistical estimation and interactive decision making. Classical lower bound
techniques—such as Fano’s inequality, Le Cam’s method, and Assouad’s
lemma—are central to the study of minimax risk in statistical estimation, yet are
insufficient to provide tight lower bounds for interactive decision making algo-
rithms that collect data interactively (e.g., algorithms for bandits and reinforce-
ment learning). Recent work of Foster et al. [36, 38] provides minimax lower
bounds for interactive decision making using seemingly different analysis tech-
niques from the classical methods. These results—which are proven using a com-
plexity measure known as the Decision-Estimation Coefficient (DEC)—capture
difficulties unique to interactive learning, yet do not recover the tightest known
lower bounds for passive estimation. We propose a unified view of these dis-
tinct methodologies through a new lower bound approach called interactive Fano
method . As an application, we introduce a novel complexity measure, the Deci-
sion Dimension , which facilitates the new lower bounds for interactive decision
making that extend the DEC methodology by incorporating the complexity of
estimation. Using the Decision Dimension, we (i) provide a unified characteri-
zation of learnability for anystructured bandit problem, (ii) close the remaining
gap between the upper and lower bounds in Foster et al. [36, 38] (up to polyno-
mial factors) for any interactive decision making problem in which the underlying
model class is convex.
1 Introduction
The minimax criterion is a standard approach to studying the intrinsic difficulty of problems in
statistics and machine learning. Stated (somewhat informally) as
min
ALGmax
M∈MCost(ALG, M), (1)
where the algorithm ALGcollects data (either passively or interactively) from the model Mand incurs
a cost, and the expression reflects the best cost that can be achieved by an algorithm ALGfor a worst-
case problem instance in a collection M, measured according to an appropriate cost function Cost .
In statistics, the minimax approach was pioneered by A. Wald [82], who made the connection to von
Neumann’s theory of games [76] and unified statistical estimation and hypothesis testing under the
umbrella of statistical decision theory . Minimax optimality and minimax rates of convergence of
38th Conference on Neural Information Processing Systems (NeurIPS 2024).estimators have since become a central object in the modern of non-asymptotic statistics [74, 81];
here, for instance, ALGis an estimator of an unknown parameter based on noisy observations.
Upper bounds on the minimax value (1) are typically achieved by choosing a particular algorithm,
while lower bounds often require specialized techniques. In statistics, three such techniques are
widely used: Le Cam’s two-point method, Fano’s inequality, and Assouad’s lemma. These tech-
niques entail constructing “difficult” choices of subsets of the class M. Le Cam’s method focuses
on two hypotheses, while Assouad’s lemma and Fano’s inequality involve multiple hypotheses in-
dexed by the vertices of a hypercube and a simplex, respectively. The relationships between these
methods are explored in Yu [87].
Classical statistical estimation is a purely passive task. A parallel line of research considers the task
ofinteractive decision making , where ALGis a multi-round procedure that directly interacts with
the data generating process and iteratively makes decisions with the (often contradictory) aims of
minimizing cost and collecting information. Proving minimax lower bounds for interactive decision
making problems presents unique challenges. The aforementioned lower bound techniques for es-
timation require quantifying the amount of information that can be gained from passively acquired
data from a hard problem instance, but the amount information acquired by an interactive algorithm
is harder to quantify [3, 61, 62], since it depends on the decisions made by the algorithm itself over
multiple rounds.
In spite of the challenges, recent work of Foster et al. [36, 38] provides lower and upper bounds
which show that a complexity measure known as the Decision-Estimation Coefficient (DEC) char-
acterizes the minimax rates for a general class of interactive decision making problems (up to a gap
which is related to the complexity of a certain induced estimation problem). Interestingly, the proof
techniques in Foster et al. [36] proceed in a seemingly different fashion from classical lower bounds
for statistical estimation; most notably, their techniques involve an algorithm-dependent (as opposed
to oblivious) choice of a hard-to-distinguish alternative problem instance.
Given the differences between the classical Assouad, Fano, and Le Cam methods, and the even
larger disparity between these methods and the interactive decision making techniques of Foster
et al. [36, 38], it is natural to ask whether there is a hope of unifying these lower bounds techniques.
Beyond the fundamental nature of this question, there is hope that a unified understanding might lead
to tighter lower bounds, or even inspire new algorithms and upper bounds; of particular interest is to
close the remaining gaps between the upper and lower bounds on the minimax rates for interactive
decision making left open by Foster et al. [38], which are closely related to estimation.
Contributions. We present a new framework for information-theoretic lower bounds which allows
for a unifying presentation of classical lower bounds in statistical estimation (Assouad, Fano, and
Le Cam) and recent DEC-based lower bounds for interactive decision making [36, 38].
•Interactive lower bound framework (Section 3). Our main result is to introduce a new lower
bound technique, the interactive Fano method . The interactive Fano method generalizes the strin-
gent separation condition in the classical Fano inequality to a novel algorithm-dependent con-
dition by introducing the concept of “ghost data” generated from a reference distribution. This
technique recovers the Le Cam two-point method (and convex hull method), Assouad method, and
Fano method as special cases. By virtue of being algorithm-dependent in nature, the interactive
Fano method seamlessly recovers DEC-based lower bounds for interactive decision making as a
special case, and leads to refined quantile-based variants.
•Decision dimension and bandit learnability (Section 4). As an application of the interactive
Fano method, we derive lower bounds for interactive decision making based on a new complexity
measure, the decision dimension , which quantifies the difficulty of estimating a near-optimal pol-
icy/decision, and complements the original DEC lower bounds (which reflect difficulty of explo-
ration as opposed to difficulty of estimation). As an application, the decision dimension provides
both lower and upper bound for learning any structured bandit problem, up to an exponential gap.
In particular, finiteness of the decision dimension is the first necessary and sufficient condition
for finite-time learnability of any structured bandit problem. As a secondary result, we use the
decision dimension to close the remaining gap between the upper and lower bounds in Foster
et al. [36, 38] (up to polynomial factors) for any interactive decision making problem in which the
underlying model class is convex.
Related work. Due to space limitations, we discuss the related work in Appendix A.
21.1 Preliminaries
LetPandQbe two distributions over a space Ωsuch that Pis absolutely continuous with respect
toQ. Then, for a convex function f: [0,+∞)→(−∞,+∞]such that f(x)is finite for all x >0,
f(1) = 0 , and f(0) = lim x→0+f(x), thef-divergence of between PandQis defined as
Df(P, Q):=Z
ΩfdP
dQ
dQ.
Concretely, we make use of three well-known f-divergences: the KL-divergence DKL, the squared
Hellinger distance D2
H, and the total variation distance DTV, for which the function f(x)is chosen
to be xlogx,1
2(√x−1)2, and1
2|x−1|respectively. For a pair of random variables (X, Y)with
joint distribution PX,Y, the mutual information is defined as
I(X;Y) =EX
DKL 
PY|X∥PY
,
where PY|Xis the conditional distribution of Y|XandPYis the marginal distribution of Y.
2 Statistical Estimation and Interactive Decision Making
We work in a general framework we refer to as Interactive Statistical Decision Making (ISDM).
We adopt this framework as a convenient formalism which encompasses statistical estimation and
interactive decision making in a unified fashion.
Interactive Statistical Decision Making. An ISDM problem is specified by (X,M,D, L), where
Xis the space of outcomes, Mis a model class (parameter space), Dis the space of algorithms,
andLis a non-negative risk function. For an algorithm ALG∈ D chosen by the learner and a model
M∈ M specified by the environment, an observation Xis generated from a distribution induced by
MandALG:X∼PM,ALG. The performance of the algorithm ALGon the model Mis then measured
by the risk function L(M, X ). The learner’s goal is to minimize the risk by choosing the algorithm
ALG. As described in the Introduction, the best possible expected risk the learner may achieve is the
following minimax risk :
infALG∈DsupM∈MEM,ALG[L(M, X )]. (2)
While main our results concern the general problem formulation in (2), we focus on applications to
statistical estimation and interactive decision making throughout. Below, we give additional back-
ground on these settings and show how to view them as special cases.
2.1 Statistical estimation
For a general statistical estimation framework known as statistical decision theory [11, 82], the
learner is given the parameter space Θ, observation space Y, decision space A, and a loss function L.
For an underlying parameter θ⋆∈Θ,ni.i.d. samples Y1, ..., Y n∼Pθ⋆are drawn and observed by
the learner. The learner then chooses a decision A=A(Y1,···, Yn)∈ A based on the observations,
and then incurs the loss L(θ⋆, A). This framework subsumes most statistical estimation problems.
Any general statistical estimation problem can be viewed as a ISDM instance, by choosing the
model class as M={Pθ:θ∈Θ}and the algorithm space as D={ALG:Y⊗n→ A} . For model
M=Pθand algorithm ALG, the distribution of the whole observation X∼PM,ALGis given by
X= (Y1,···, Yn, A), Y 1, ..., Y ni.i.d∼Pθ, A=ALG(Y1,···, Yn).
The loss under model Mis then measured by the loss of the decision A, i.e., L(M, X ):=L(θ, A).
2.2 Interactive decision making
For interactive decision making, we consider the following variant of the Decision Making with
Structured Observations (DMSO) framework [36], which subsumes bandits and reinforcement
learning. The learner interacts with the environment (described by an underlying model M⋆: Π→
∆(O), unknown to the learner) for Trounds. For each round t= 1, ..., T :
• The learner selects a decision πt∈Π, where Πis the decision space.
• The learner receives an observation ot∈ O viaot∼M⋆(πt), where Ois the observation space.
The underlying model M⋆is formally a conditional distribution, and the learner is assumed to have
access to a known model class M ⊆ (Π→∆(O))with the following property.
3Assumption 1 (Realizability) .The model class Mcontains M⋆.
The model class Mrepresents the learner’s prior knowledge of the structure of the underlying envi-
ronment. For example, for structured bandit problems, the models specify the reward distributions
and hence encode the structural assumptions on the mean reward function (e.g. linearity, smooth-
ness, or concavity). For a more detailed discussion, see Appendix B.
To each model M∈ M , we associate a riskfunction gM: Π→R≥0, which measures the perfor-
mance of a decision in Π. We consider two types of learning goals under the DMSO framework:
• Generalized no-regret learning: The goal of the agent is to minimize the cumulative sub-optimality
during the course of the interaction, given by
RegDM(T):=PT
t=1gM⋆(πt), (3)
where πtcan be randomly drawn from a distribution pt∈∆(Π) chosen by the learner at step t.
• Generalized PAC (Probably Approximately Correct) learning: the goal of the agent is to minimize
the sub-optimality of a final output decision bπ(possibly randomized), which is selected by the
learner once all Trounds of interaction conclude. We measure performance via
Risk DM(T):=gM⋆(bπ). (4)
With an appropriate choice for gM, the setting captures reward maximization (regret minimiza-
tion) [36, 38], model estimation and preference-based learning [19], multi-agent decision making
and partial monitoring [33], and various other tasks. In the main text, we focus on reward maxi-
mization, and defer the results for more general choices gMto the appendices (cf. Appendix B).
Example 1 (Reward maximization) .LetR:O → [0,1]be a known reward function.1For a model
M∈ M ,EM,π[·]denotes expectation under the process o∼M(π), and fM(π):=EM,π[R(o)]
denotes the expected value function. An optimal decision is denoted by πM∈arg maxπ∈ΠfM(π),
and the sub-optimality measure is defined by gM(π) =fM(πM)−fM(π).
DMSO as an instance of ISDM. Any DMSO class (M,Π)induces an ISDM as follows. For any
t∈[T], denote the full history of decisions and observations up to time tbyHt−1= (πs, os)t−1
s=1.
The space of observations Xconsists of all such XthatX=HT∪{bπ}, where bπis a final decision.
An algorithm ALG={qt}t∈[T]∪{p}is specified by a sequence of mappings, where the t-th mapping
qt(· | Ht−1)specifies the distribution of πtbased on Ht−1, and the final map p(· | HT)specifies
the distribution of the output decision bπbased on HT. The algorithm space Dconsists of all such
algorithms. The loss function is chosen to be L(M⋆, X) =RegDM(T)for no-regret learning (3),
andL(M⋆, X) =Risk DM(T)for PAC learning (4). For any algorithm ALGand model M,PM,ALG(·)
is the distribution of X= (HT,bπ)generated by the algorithm ALGunder the model M, and we let
EM,ALG[·]to be the corresponding expectation.
3 A General Lower Bound
In this section, we introduce our general lower bound technique, the interactive Fano method, and
use it to provide minimax lower bounds for the ISDM framework.
Theorem 1 (Interactive Fano method) .Fix an f-divergence Df. Let ALGbe a given algorithm,
δ∈(0,1)be a quantile parameter, and µ∈∆(M)be a prior distribution over models. For
reference distribution QonXand parameter ∆>0, we define
ρ∆,Q=PM∼µ,X∼Q(L(M, X )<∆). (5)
Then, the following lower bound holds:
sup
M∈MEX∼PM,ALG[L(M, X )]≥EM∼µEX∼PM,ALG[L(M, X )]
≥δ· sup
Q∈∆(X),∆>0
∆ :EM∼µ
Df 
PM,ALG,Q
<df,δ(ρ∆,Q)	
,
where we denote df,δ(p) =Df(Bern(1 −δ),Bern( p))ifp≤1−δ, and df,δ(p) = 0 otherwise.
1We assume Ris known without loss of generality, since the observation omay have a component contain-
ing the random reward.
4This result generalizes existing statements of Fano’s method in multiple ways:
• It encompasses general interactive learning/estimation problems in the ISDM framework, as op-
posed to purely passive estimation. This is reflected in the fact that the distribution over the
outcome Xis allowed to depend on ALGitself.
• The most important and novel change is that Theorem 1 generalizes the “hard” separation condi-
tion required in the classical Fano method to a “soft” notion of separation captured by the quantile
ρ∆,Qin (5). The quantile ρ∆,Qreflects the average separation under “ghost data” Xgenerated
from an arbitrary reference distribution Q, which is independent of the true model M∼µ.
• In addition, instead of relying on mutual information, which is can difficult to quantify for inter-
active problems, we use divergence with respect to the reference distribution Q, generalizing a
central idea in Foster et al. [36, 38].
In what follows, we will show that these generalizations allow the Interactive Fano method to achieve
two important desiderata: (1) unifying the methods of Fano, Le Cam, and Assouad (Section 3.1),
and (2) integrating these traditional lower bound techniques with contemporary interactive decision
making lower bounds to derive new lower bound (see Section 3.2).
3.1 Recovering non-interactive lower bounds
We begin by applying Theorem 1 to recover classical non-interactive lower bounds for statistical
estimation. Since a goal of our paper is to integrate the Fano and Assouad methods with the DEC
framework, this serves as an important sanity check to demonstrate that our framework can recover
the non-interactive versions of these methods.
Fano’s method. To recover the Fano method, we specialize Theorem 1 to the KL divergence.
Observe that for any reference distribution Q,
PM∼µ,X∼Q(L(M, X )<∆)≤supxµ(M∈ M | L(M, x)<∆).
By choosing Q=EM∼µPM,ALGin Theorem 1, we obtain the following proposition, which encom-
passes prior generalizations of Fano’s inequality [88, 32, 23] developed in statistical estimation.
Proposition 2 (Recovering the generalized Fano method) .Fix an algorithm ALGand prior distribu-
tionµ∈∆(M), and let Iµ,ALG(M;X)be the mutual information between MandXunder M∼µ
andX∼PM,ALG. The following Bayes risk lower bound holds for all ∆≥0:
EM∼µEX∼PM,ALG[L(M, X )]≥∆
1 +Iµ,ALG(M;X)+log 2
log supxµ(M∈M|L(M,x)<∆)
. (6)
When applied to the statistical estimation setting (Section 2.1), the classical Fano inequality corre-
sponds to the special case of Proposition 2 where Θ =A={1,2, . . . , m },L(θ, a) =1(θ̸=a)is
the indicator loss, µ= Unif(Θ) is the uniform prior, and ∆ = 1 .
Note that in Proposition 2, the term log supxµ(M∈ M :L(M, x)<∆)in the denominator of (6)
takes the supremum over the outcome x, resulting in a simplified expression that removes the role of
the algorithm ALG. This simplification is often sufficient to derive tight guarantees for estimation, but
is insufficient for interactive decision making in general. The DEC, which we define in Section 3.2,
more precisely accounts for the role of decisions selected by the algorithm.
Le Cam’s method and Assouad’s method. To recover Le Cam’s two-point method and Assouad’s
method from Theorem 1, we appeal to the following result, which recovers a more general lower
bound known as the Le Cam convex hull method [54, 87].
Proposition 3 (Recovering Le Cam’s convex hull method) .For a parameter space Θand observa-
tion space Y, consider a class of distributions P={Pθ|θ∈Θ}indexed by Θ,Pθ∈∆(Y⊗n). Let
L: Θ×A → R+be a loss function. Suppose Θ0⊆ΘandΘ1⊆Θsatisfy the separation condition
L(θ0, a) +L(θ1, a)≥2∆,∀a∈ A, θ0∈Θ0, θ1∈Θ1.
Suppose that there exist probability measures ν0∈∆(Θ 0)andν1∈∆(Θ 1)such that
DTV(ν0⊗Pθ, ν1⊗Pθ)≤1/2,
where νi⊗Pθis the distribution on Y⊗ninduced by θ∼νi, Y1, . . . , Y n∼Pθfori∈ {0,1}. Then
infALGsupθ∈ΘEY1,...,Y n∼PθL(θ,ALG(Y1, . . . , Y n))≥∆/4,
where the infimum is taken over all algorithms ALG:Y⊗n→ A .
5Le Cam’s convex hull method is the most general formulation of the Le Cam two-point method,
which—in its most basic form—corresponds to the case in which ν0andν1are singletons. The
convex hull method is also capable of recovering Assouad’s method [87]. It is important to note that
the classical Fano inequality, e.g. in the form of Proposition 2, cannot recover Proposition 3. This is
because of fundamental differences between the divergences (KL versus TV) used in the traditional
Fano method and convex hull method.
3.2 Recovering DEC-based lower bounds for interactive decision making
Within the DMSO framework (Section 2.2), Foster et al. [36, 38] introduced the Decision-Estimation
Coefficient (DEC) as a complexity measure, providing both upper and lower bounds for any model
classM. We now show how to recover the lower bounds of Foster et al. [36, 38] through Theorem 1.
We focus on the lower bounds from Foster et al. [38], which are based on a variant of the DEC called
theconstrained DEC , and provide the tightest guarantees from prior work.
Background on the Decision-Estimation Coefficient. Consider the reward maximization setting
(Example 1) under DMSO. For a model class Mand a reference model ĎM: Π→∆(O)(not
necessarily in M), we define the constrained regret-DEC via
r-decc
ε(M,ĎM):= inf
p∈∆(Π)sup
M∈M
Eπ∼p[gM(π)]|Eπ∼pD2
H 
M(π),ĎM(π)
≤ε2	
, (7)
and define the constrained PAC-DEC via
p-decc
ε(M,ĎM):= inf
p,q∈∆(Π)sup
M∈M
Eπ∼p[gM(π)]|Eπ∼qD2
H 
M(π),ĎM(π)
≤ε2	
.(8)
Here, the superscript “ c” indicates “constrained”, and the superscript “ r” (resp. “ p”) indicates “re-
gret” (resp. “PAC”). We further define
p-decc
ε(M) = sup
ĎM∈co(M)p-decc
ε(M,ĎM),r-decc
ε(M) = sup
ĎM∈co(M)r-decc
ε(M ∪ {ĎM},ĎM),
where co(M)denotes the convex hull of the model class M.
Based on these complexity measures, Foster et al. [38] (see also Glasgow and Rakhlin [40]) provide
the following lower and upper bounds on optimal risk and regret, under mild growth conditions on
the DECs.
Theorem 4 (Informal; Foster et al. [38]) .Consider the reward maximization variant of the DMSO
setting (Example 1). For any model class MandT∈N, the following lower and upper bounds hold:
(1) For PAC learning,
p-decc
ε(T)(M)≲inf
ALGsup
M∈MEM,ALG[Risk DM(T)]≲p-decc
¯ε(T)(M),
where ε(T)≍p
1/Tand¯ε(T)≍p
log|M|/T(up to logarithmic factors).
(2) For no-regret learning,
r-decc
ε(T)(M)·T≲inf
ALGsup
M∈MEM,ALG[RegDM(T)]≲r-decc
¯ε(T)(M)·T+T·¯ε(T).
Therefore, up to the log|M|-gap between the parameters ε(T)and¯ε(T)appearing in the lower and
upper bounds, the constrained PAC-DEC tightly captures the minimax risk of PAC learning, and the
constrained regret-DEC captures the minimax regret of no-regret learning.
A new complexity measure: The quantile Decision-Estimation Coefficient. We recover the
DEC-based lower bounds from Foster et al. [38] through a new variant we refer to as the quan-
tile DEC . To do so, we briefly recount the proof technique used by Foster et al. [38].
Given an algorithm ALG, the proof strategy is to first fix an arbitrary reference model ĎM, then ad-
versarially choose a hard alternative model M∈ M (in a way that is guided by the DEC and
the algorithm ALGitself) such that DTV(PM,ALG,PĎM,ALG)is small, yet ALGcannot achieve low risk on
model M. This lower bound technique does not explicitly require a separation condition between
MandĎM, which is a departure from the classical Fano and two-point methods. Thus to recover it,
the lack of an explicit separation condition in Theorem 1 will be critical. More precisely, for any
model M, we consider the following distributions over decisions:
qM,ALG=EM,ALGh
1
TPT
t=1qt(· | Ht−1)i
∈∆(Π) , p M,ALG=EM,ALG
p(HT)
∈∆(Π) .(9)
6That is, qM,ALGis the expected empirical distribution over the decisions (π1,···, πT)played by the
algorithm under M, and pM,ALGis the expected distribution of the final decision bπ.
With these definitions, we instantiate Theorem 1 with the Hellinger distance. We will use the sub-
additivity of Hellinger distance (Lemma C.1; Foster et al. [36, 39]), which allows us to bound
1
2D2
TV(PM,ALG,PĎM,ALG)≤D2
H(PM,ALG,PĎM,ALG)≤7T·Eπ∼pĎM,ALG
D2
H 
M(π),ĎM(π)
. (10)
Theorem 1 then yields the following intermediate result.
Lemma 5 (Interactive Fano method lower bound for interactive decision making) .Letδ∈[0,1]be
given, and consider an algorithm ALG. Define
∆⋆
ALG,δ:= sup
ĎM∈co(M)sup
M∈Msup
∆≥0n
∆ :pĎM,ALG(π:gM(π)≥∆)> δ+q
14TEπ∼qĎM,ALGD2
H 
M(π),ĎM(π)o
.
Then there exists M∈ M such that PM,ALG
gM(bπ)≥∆⋆
ALG,δ
≥δ.
Using Lemma 5, as a starting point, we derive a new quantile-based variant of the DEC, which we
will show can be viewed as a slight generalization of the original PAC DEC of Foster et al. [38].
For any model M∈ M and any parameter δ∈[0,1], we define the δ-quantile risk as follows:
bgM
δ(p) = sup∆≥0{∆ :Pπ∼p(gM(π)≥∆)≥δ};
this serves as a measure of the sub-optimality of the distribution p∈∆(Π) in terms of δ-quantile.
We now define the quantile PAC DEC as follows:
p-decq
ε,δ(M,ĎM):= inf p,q∈∆(Π)supM∈M
bgM
δ(p)|Eπ∼qD2
H 
M(π),ĎM(π)
≤ε2	
,(11)
and define p-decq
ε,δ(M):= supĎM∈co(M)p-decq
ε,δ(M,ĎM). Applying Lemma 5, it is immediate to
see that the quantile PAC-DEC provides a lower bound on the PAC risk.
Theorem 6 (Quantile DEC lower bound) .Let any T≥1andδ∈[0,1)be given, and define
εδ(T):=1
14q
δ
T. Then, for any algorithm ALG, there exists M⋆∈ M such that
PM⋆,ALG
Risk DM(T)≥p-decq
εδ(T),δ(M)
≥δ
2.
Unlike the original constrained DEC lower bounds (Theorem 4), which are restricted to the reward
maximization variant of the DMSO setting (Example 1), the quantile DEC lower bound in The-
orem 6 holds for any suboptimal measure gM. As a result, the lower bound applies to a broader
range of generalized PAC learning tasks, including model estimation [19] and multi-agent decision
making [33], where DEC-based lower bounds from prior work are loose in general; as a concrete
application, we derive a new lower bound for interactive estimation (Example 3) in Appendix E.2.
Recovering DEC-based lower bounds using the quantile DEC. At first glance, Theorem 6 might
appear to be weaker than the constrained PAC-DEC lower bound in Theorem 4 due to the loose
conversion from quantile risk to expected risk. However, by specializing to reward maximization
(Example 3) and leveraging the structure of the sub-optimality measure gM, we show that quantile
PAC-DEC is equivalent to its constrained counterpart for this setting, leading to a tight lower bound.
Proposition 7 (Recovering the PAC DEC lower bound) .Under the reward maximization setting (Ex-
ample 1), for any ε >0andδ∈[0,1)it holds that
p-decc
ε(M)≤p-decq√
2ε,δ(M) +4ε
1−δ.
As a corollary, we may choose δ=1
2andε(T) =1
20√
Tin Theorem 6, so that
supM∈MEM,ALG[Risk DM(T)]≥1
4p-decq√
2ε(T),1/2(M)≥1
4
p-decc
ε(T)(M)−8ε(T)
.
Thus, the quantile PAC-DEC lower bound indeed recovers the constrained PAC-DEC lower bound
in Theorem 4.
Our quantile DEC lower bound extends to regret with minor modifications, allowing us to recover
the regret lower bounds in Theorem 4. We defer the details to the Appendix E.1 (Theorem E.1).
73.3 Recovering mutual information-based lower bounds for interactive decision making
The following result uses Theorem 1 to extend classical Fano method to interactive decision making
and achieves tight dependence on the problem dimension that is not recovered by the standard DEC
lower bound in Foster et al. [36, 38].
Proposition 8 (Mutual information-based lower bound) .Consider the DMSO setting. For any T≥
1and prior µ∈∆(M), we define the maximum T-round mutual information as
Iµ(T):= supALGIµ,ALG(M;HT),
where the supremum is taken over all possible DMSO algorithms ALG. Then for any algorithm ALG,
sup
M∈MEM,ALG[gM(bπ)]≥1
2sup
µ∈∆(M)sup
∆>0
∆|sup
πµ(M:gM(π)≤∆)≤1
4exp(−2Iµ(T))
.
Using Proposition 8, along with mutual information bounds from Rajaraman et al. [63], we recover a
Ω(d/√
T)PAC lower bound for linear bandits in ddimensions, which in turn recovers the Ω(d√
T)
regret lower bound [26, 66, 51, etc.].
Corollary 9. Ford≥2, consider the d-dimensional linear bandit problem with decision space
Π ={π∈Rd:∥π∥2≤1}, parameter space Θ ={θ∈Rd:∥θ∥2≤1}, and model class M=
{Mθ}θ∈Θ; for each θ∈Θthe model Mθis given by Mθ(π) =N(⟨π, θ⟩,1). Then Proposition 8
implies a minimax risk lower bound:
infALGsupM∈MEM,ALG[Risk DM(T)]≥Ω
min{d/√
T,1}
. (12)
In Section 4, we also instantiate Proposition 8 to derive a new complexity measure for DMSO.
4 Application to Interactive Decision Making: Bandit Learnability and
Beyond
In this section, we focus on the DMSO setting and apply our general results (Theorem 1) to derive
new lower and upper bounds for interactive decision making that go beyond the previous results
based on the Decision-Estimation Coefficient [36, 38] by incorporating hardness of estimation.
Background: Gaps between DEC-based and upper and lower bounds. A fundamental open
question of the DEC framework is whether the log|M|-gap between DEC lower and upper bounds
in Theorem 4 can be closed. To highlight this gap in a more interpretable fashion, we re-state
Theorem 4 in terms of a quantity we refer to as the minimax sample complexity . Let us focus on
regret. Recall that for a fixed model class M, the following notion of minimax regret (2) is the
central objective of interest:
Reg⋆
T:= inf ALGsupM∈MEM,ALG[RegDM(T)].
Given a parameter ∆>0, we define the minimax sample complexity
T⋆(M,∆):= inf T≥1{T:Reg⋆
T≤T∆} (13)
as the least value Tfor which there exists an algorithm that achieves ∆Tregret. Clearly, character-
izing T⋆(M,∆)is equivalent to characterizing the minimax regret Reg⋆
T.
Consider the following quantity induced by DEC for a class Mand parameter ∆>0:
TDEC(M,∆) = inf ε∈(0,1){ε−2:r-decc
ε(M)≤∆}. (14)
With this definition, Theorem 4 is equivalent to the following characterization of the minimax sample
complexity T⋆(M,∆):
TDEC(M,∆)≲T⋆(M,∆)≲TDEC(M,∆)·log|M|. (15)
That is, Theorem 4 characterizes the minimax sample complexity up to a multiplicative log|M|
factor. Our main result in this section will be to use the decision dimension and interactive Fano
method (Theorem 1), to (i) tighten (15) for various special cases of interest, and (ii) give a new
characterization for T⋆(M,∆)in structured bandit problems which avoids spurious parameters
such as log|M| altogether.
84.1 New upper and lower bounds through the decision dimension
For the a model class Mand parameter ∆>0, we define the decision dimension as follows:
Ddim ∆(M):= inf
p∈∆(Π)sup
M∈M1
p(π:gM(π)≤∆). (16)
Informal, the decision dimension value Ddim ∆(M)represents the best possible coverage over ∆-
optimal decisions that can be achieved through a single exploratory distribution in the face of an
unknown model M∈ M . As will now show, this quantity naturally arises as a lower bound on op-
timal risk through the interactive Fano method. We begin with the following regularity assumption.
Assumption 2 (Regular model class) .There exists a constant CKL>0and a reference model ĎM
such that DKL(M(π)∥ĎM(π))≤CKLfor all M∈ M andπ∈Π.
Assumption 2 is a mild assumption on the boundedness of KL divergence. As an example, for
structured bandits with means in [0,1]and Gaussian rewards, Assumption 2 holds with CKL=1
2.
Details and more examples are provided in Appendix B.1. Our main lower bound based on the
decision dimension follows by specializing Theorem 1 to KL divergence.
Theorem 10 (Decision dimension lower bound) .Suppose that Msatisfies Assumption 2 with pa-
rameter CKL>0. Then for any algorithm ALGand∆>0, unless T <logDdim 2∆(M)−2
2CKL, there
exists M⋆∈ M such that PM⋆,ALG[gM⋆(bπ)≥∆]≥1
2.
In particular, decision dimension also implies a regret lower bound through Theorem 10: That
is, for any algorithm to achieve ∆T-regret, it is necessary to have T= Ω(log Ddim 2∆(M)).
Combining this with Theorem 4, we conclude that boundedness of both the DEC and the decision
dimension is necessary for learning with any model class M.
Upper bounds based on the decision dimension. We now complement Theorem 10 by showing
that for any reward maximization instance of the DMSO setting (Example 1), boundedness of the
decision dimension alone is also sufficient to derive upper bounds on the sample complexity of
learning. The caveat is that while the lower bound is logarithmic in Ddim ∆(M), the upper bound
will be polynomial.
Theorem 11 (Decision dimension upper bound) .Consider the reward maximization variant of the
DMSO setting (Example 1). There exists an algorithm that for any class Mand∆>0, ensures
that with probability at least 1−δ,
RegDM(T)≤T·∆ +O(log(1 /δ))·p
T·Ddim ∆(M).
Combining Theorem 10 and Theorem 11 yields the following bounds on T⋆(M,∆)(omitting poly-
logarithmic factors):
logDdim 2∆(M)
CKL≲T⋆(M,∆)≲Ddim ∆/2(M)
∆2 . (17)
The gap between the lower and upper bounds of (17) is exponential; however, for model classes
withCKL=O(1), (17) suffices to characterize finite-time learnability . As a special case, we now
show that decision dimension characterizes the learnability of any structured bandit problem.
4.2 Application: Bandit learnability
We consider a structured bandit setting given by a reward function class H ⊆ (Π→[0,1]). The
protocol is as follows: For each round t∈[T], the learner chooses a decision πt∈Π, then receives a
reward rt∼N(h⋆(πt),1)in response, where the mean reward function h⋆∈ H. This corresponds to
an instance of the DMSO framework with induced model class MH={π7→N(h(π),1)|h∈ H} .
We define the decision dimension for Hvia
Ddim ∆(H):=Ddim ∆(MH) = inf
p∈∆(Π)sup
h∈H1
p(π:h(πh)−h(π)≤∆), (18)
where we denote πh:= arg maxπ∈Πh(π). This exactly coincides with the notion of maximin
volume of Hanneke and Yang [41], which was shown to give a tight characterization of learnability
for the special case of noiseless binary-valued structured bandits. We discuss the connection to
Hanneke and Yang [41] in more detail in Appendix H.1.
It is straightforward to show that for any structured bandit problem, the induced class MHsatisfies
Assumption 2 with CKL=1
2(Example 4). This leads to the following lower bound.
9Corollary 12 (Lower bound for stochastic bandits) .For the bandit model class MHdefined as
above, it holds that T⋆(MH,∆)≥2Ddim ∆(H)−2.
Combining this result with the upper bound in Theorem 11, we obtain the following bounds on the
minimax-optimal sample complexity for the structure bandit problem with class H:
logDdim 2∆(H)≲T⋆(MH,∆)≲Ddim ∆/2(H)
∆2. (19)
This implies that Ddim ∆(H)characterizes learnability for structured bandits.
Theorem 13 (Structured bandit learnability) .For a given reward function class H, the bandit prob-
lem class MHis learnable for finite Tif and only if Ddim ∆(H)<+∞for all ∆>0.
We remark that the lower and upper bound in (19) cannot be improved in terms of the decision
dimension alone: (1) For K-armed bandits, we have Ddim ∆(H)≤K, meaning the upper bound
is tight. (2) For d-dimensional linear bandits, we have logDdim ∆(H) = Ω( d), meaning the lower
bound is nearly tight. Nevertheless, the exponential gap in (19) can be partly mitigated by combin-
ing the decision dimension with the DEC, as we will show in Section 4.3.
Our characterization bypasses impossibility results of Hanneke and Yang [41], who show that for
noiseless structured bandit problems, there exist classes Hfor which bandit learnability is indepen-
dent of the axioms of ZFC. Therefore, their results rule out the possibility of a characterization of
noiseless bandit learnability through any combinatorial dimension [10] for the problem class. Our
characterization is compatible with this result because the argument of Hanneke and Yang [41] re-
lies on the noiseless nature of the bandit problem, and hence does not preclude a characterization
for the noisy setting. In addition, the decision dimension is not a combinatorial dimension under the
definition of Ben-David et al. [10]. Additional discussion is deferred to Appendix H.1.
4.3 Improved upper bounds for general decision making
In this section, we derive tighter upper bounds that scale with logDdim ∆(M)by combining the
decision dimension with the Decision-Estimation Coefficient. For simplicity of presentation, we
focus on regret minimization under the setting of Example 1, and we assume the following condition
to simplify our bounds (the fully general upper bound is detailed in Appendix G).
Assumption 3 (Regularity of constrained DEC) .A function d: [0,1]→Ris said to have moderate
decay ifd(ε)≥10ε∀ε∈[0,1], and there exists a constant c≥1such that cd(ε)
ε≥d(ε′)
ε′for all
ε′≥ε. We assume the function ε7→r-decc
ε(co(M)), as a function of ε, satisfies moderate decay
for a constant creg≥1.
This condition essentially requires that the DEC for co(M)exhibits moderate growth, which means
that learning with co(M)is not “too easy”.We now state our upper bound, which tightens Theorem 4
by replacing the log|M| dependence in the upper bound with logDdim ∆(M)(with the caveat that
the upper bound scales with the DEC for the convexified model class co(M)).
Theorem 14 (Upper bound with DEC and decision dimension) .Consider the reward maximization
variant of the DMSO setting. Let Mbe any class for which Assumption 3 holds, and assume that
Πis finite. Let ¯ε(T)≍p
logDdim ∆(M)/T. Then for any ∆>0, Algorithm 1 (see Appendix G)
ensures that with high probability,
RegDM≤T·∆ +O 
cregT√logT
·r-decc
¯ε(T)(co(M)).
Restating this upper bound in terms of minimax sample complexity and combining it with the pre-
ceding lower bounds yields the following result.
Theorem 15. For any class Mthat satisfies Assumption 2 and 3, we have
maxn
TDEC(M,∆),logDdim 2∆(M)
CKLo
≲T⋆(M,∆)≲TDEC(co(M),∆)·logDdim ∆/2(M),(20)
up to dependence on cregand logarithmic factors.
In particular, when the model class Mis convex (i.e. co(M) =M) and CKL=O(1), Theorem 15
provides lower and upper bounds for learning with Mthat match up to a quadratic factor. Indeed,
for convex model classes, the upper bound of (20) is always tighter than (15) (and also tighter than
the result in Foster et al. [36, 37]), as by definition we have
logDdim ∆(M)≤logDdim 0(M)≤min{log|M|,log|Π|},∀∆>0.
As applications, we apply Theorem 15 to structured bandits and contextual bandits (Appendix H).
10Acknowledgements We acknowledge support from ARO through award W911NF-21-1-0328, the
Simons Foundation and the NSF through award DMS-2031883, as well as NSF PHY-2019786.
References
[1] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private assouad, fano, and le
cam. In Algorithmic Learning Theory , pages 48–78. PMLR, 2021.
[2] Alekh Agarwal, Martin J Wainwright, Peter Bartlett, and Pradeep Ravikumar. Information-
theoretic lower bounds on the oracle complexity of convex optimization. Advances in Neural
Information Processing Systems , 22, 2009.
[3] Alekh Agarwal, Peter L Bartlett, Pradeep Ravikumar, and Martin J Wainwright. Information-
theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE
Transactions on Information Theory , 58(5):3235–3249, 2012.
[4] Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-sensitive di-
mensions, uniform convergence, and learnability. Journal of the ACM , 44:615–631, 1997.
[5] Noga Alon, Mark Bun, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private and online
learnability are equivalent. ACM Journal of the ACM (JACM) , 69(4):1–34, 2022.
[6] Ery Arias-Castro, Emmanuel J Candes, and Mark A Davenport. On the fundamental limits of
adaptive sensing. IEEE Transactions on Information Theory , 59(1):472–481, 2012.
[7] Patrice Assouad. Deux remarques sur l’estimation. Comptes rendus des séances de l’Académie
des sciences. Série 1, Mathématique , 296(23):1021–1024, 1983.
[8] Peter L Bartlett, Philip M Long, and Robert C Williamson. Fat-shattering and the learnability
of real-valued functions. In Proceedings of the seventh annual conference on Computational
learning theory , pages 299–310, 1994.
[9] Shai Ben-David, David Pal, and Shai Shalev-Shwartz. Agnostic online learning. In Proceed-
ings of the 22th Annual Conference on Learning Theory , 2009.
[10] Shai Ben-David, Pavel Hrubeš, Shay Moran, Amir Shpilka, and Amir Yehudayoff. Learnabil-
ity can be undecidable. Nature Machine Intelligence , 1(1):44–48, 2019.
[11] James O Berger. Statistical Decision Theory and Bayesian Analysis . Springer Science &
Business Media, 1985.
[12] Lucien Birgé. On estimating a density using hellinger distance and some other strange facts.
Probability theory and related fields , 71(2):271–291, 1986.
[13] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability
and the vapnik-chervonenkis dimension. Journal of the ACM (JACM) , 36(4):929–965, 1989.
[14] Jean Bretagnolle and Catherine Huber. Estimation des densités: risque minimax. Zeitschrift
für Wahrscheinlichkeitstheorie und verwandte Gebiete , 47:119–137, 1979.
[15] Sébastien Bubeck, Jian Ding, Ronen Eldan, and Miklós Z Rácz. Testing for high-dimensional
geometry in random graphs. Random Structures & Algorithms , 49(3):503–532, 2016.
[16] Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and
online prediction. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science
(FOCS) , pages 389–402. IEEE, 2020.
[17] Rui M Castro and Robert D Nowak. Minimax bounds for active learning. IEEE Transactions
on Information Theory , 54(5):2339–2353, 2008.
[18] Nicolò Cesa-Bianchi, Pierre Gaillard, Claudio Gentile, and Sébastien Gerchinovitz. Algorith-
mic chaining and the role of partial feedback in online nonparametric learning. In Conference
on Learning Theory , 2017.
11[19] Fan Chen, Song Mei, and Yu Bai. Unified algorithms for rl with decision-estimation
coefficients: pac, reward-free, preference-based learning, and beyond. arXiv preprint
arXiv:2209.11745 , 2022.
[20] Fan Chen, Junyu Zhang, and Zaiwen Wen. A near-optimal primal-dual method for off-policy
learning in cmdp. Advances in Neural Information Processing Systems , 35:10521–10532,
2022.
[21] Fan Chen, Huan Wang, Caiming Xiong, Song Mei, and Yu Bai. Lower bounds for learning in
revealing pomdps. arXiv preprint arXiv:2302.01333 , 2023.
[22] Fan Chen, Constantinos Daskalakis, Noah Golowich, and Alexander Rakhlin. Near-optimal
learning and planning in separated latent mdps. In Shipra Agrawal and Aaron Roth, editors,
Proceedings of Thirty Seventh Conference on Learning Theory , volume 247 of Proceedings of
Machine Learning Research , pages 995–1067. PMLR, 30 Jun–03 Jul 2024.
[23] Xi Chen, Adityanand Guntuboyina, and Yuchen Zhang. On bayes risk lower bounds. The
Journal of Machine Learning Research , 17(1):7687–7744, 2016.
[24] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence
and Statistics , pages 208–214. JMLR Workshop and Conference Proceedings, 2011.
[25] Thomas M Cover and Joy A Thomas. Elements of information theory . John Wiley & Sons,
1999.
[26] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under
bandit feedback. 2008.
[27] Omar Darwiche Domingues, Pierre Ménard, Emilie Kaufmann, and Michal Valko. Episodic
reinforcement learning in finite mdps: Minimax lower bounds revisited. In Algorithmic Learn-
ing Theory , pages 578–598. PMLR, 2021.
[28] David L Donoho and Richard C Liu. Geometrizing rates of convergence, I. Technical Report
137a, Dept. Statistics, Univ. California, Berkeley, 1987.
[29] David L Donoho and Richard C Liu. Geometrizing rates of convergence, II. The Annals of
Statistics , pages 633–667, 1991.
[30] David L Donoho and Richard C Liu. Geometrizing rates of convergence, III. The Annals of
Statistics , pages 668–701, 1991.
[31] John C Duchi. Lecture notes on statistics and information theory. 2023.
[32] John C Duchi and Martin J Wainwright. Distance-based and continuum fano inequalities with
applications to statistical estimation. arXiv preprint arXiv:1311.2669 , 2013.
[33] Dean Foster, Dylan J Foster, Noah Golowich, and Alexander Rakhlin. On the complexity of
multi-agent decision making: From learning in games to partial monitoring. In The Thirty
Sixth Annual Conference on Learning Theory , pages 2678–2792. PMLR, 2023.
[34] Dylan J Foster and Alexander Rakhlin. Beyond UCB: Optimal and efficient contextual bandits
with regression oracles. arXiv preprint arXiv:2002.04926 , pages 3199–3210, 2020.
[35] Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent
complexity of contextual bandits and reinforcement learning: A disagreement-based perspec-
tive. arXiv preprint arXiv:2010.03104 , 2020.
[36] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity
of interactive decision making. arXiv preprint arXiv:2112.13487 , 2021.
[37] Dylan J Foster, Alexander Rakhlin, Ayush Sekhari, and Karthik Sridharan. On the complexity
of adversarial decision making. arXiv preprint arXiv:2206.13063 , 2022.
12[38] Dylan J Foster, Noah Golowich, and Yanjun Han. Tight guarantees for interactive decision
making with the decision-estimation coefficient. In The Thirty Sixth Annual Conference on
Learning Theory , pages 3969–4043. PMLR, 2023.
[39] Dylan J Foster, Yanjun Han, Jian Qian, and Alexander Rakhlin. Online estimation via offline
estimation: An information-theoretic framework. arXiv preprint arXiv:2404.10122 , 2024.
[40] Margalit Glasgow and Alexander Rakhlin. Tight bounds for γ-regret via the decision-
estimation coefficient. arXiv preprint arXiv:2303.03327 , 2023.
[41] Steve Hanneke and Liu Yang. Bandit learnability can be undecidable. In The Thirty Sixth
Annual Conference on Learning Theory , pages 5813–5849. PMLR, 2023.
[42] Rafail Z Hasminskii and Ildar A Ibragimov. On the nonparametric estimation of functionals.
InProceedings of the Second Prague Symposium on Asymptotic Statistics , volume 473, pages
474–482. North-Holland Amsterdam, 1979.
[43] Ildar Abdulovich Ibragimov and Rafail Zalmanovich Has’Minskii. Statistical estimation:
asymptotic theory . Springer Science & Business Media, 1981.
[44] Yassir Jedra and Alexandre Proutiere. Sample complexity lower bounds for linear system
identification. In 2019 IEEE 58th Conference on Decision and Control (CDC) , pages 2676–
2681. IEEE, 2019.
[45] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning , pages 5084–5096. PMLR, 2021.
[46] Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Bandits and experts in metric spaces.
Journal of the ACM (JACM) , 66(4):1–77, 2019.
[47] Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with
rich observations. In Advances in Neural Information Processing Systems , pages 1840–1848,
2016.
[48] Tor Lattimore. Improved regret for zeroth-order adversarial bandit convex optimisation. Math-
ematical Statistics and Learning , 2(3):311–334, 2020.
[49] Tor Lattimore and Andras Gyorgy. Mirror descent and the information ratio. In Conference on
Learning Theory , pages 2965–2992. PMLR, 2021.
[50] Tor Lattimore and Csaba Szepesvári. An information-theoretic approach to minimax regret in
partial monitoring. In Conference on Learning Theory , pages 2111–2139. PMLR, 2019.
[51] Tor Lattimore and Csaba Szepesvári. Bandit algorithms . Cambridge University Press, 2020.
[52] Tor Lattimore and Csaba Szepesvári. Exploration by optimisation in partial monitoring. In
Conference on Learning Theory , pages 2488–2515. PMLR, 2020.
[53] Lucien Le Cam and Grace Lo Yang. Asymptotics in statistics: some basic concepts . Springer
Science & Business Media, 2000.
[54] Lucien LeCam. Convergence of estimates under dimensionality restrictions. The Annals of
Statistics , pages 38–53, 1973.
[55] Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei. Settling the sample complexity of
model-based offline reinforcement learning. The Annals of Statistics , 52(1):233–260, 2024.
[56] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold
algorithm. Machine learning , 2(4):285–318, 1988.
[57] Qinghua Liu, Alan Chung, Csaba Szepesvári, and Chi Jin. When is partially observable rein-
forcement learning not scary? arXiv preprint arXiv:2204.08967 , 2022.
13[58] Guanyu Nie, Mridul Agarwal, Abhishek Kumar Umrawal, Vaneet Aggarwal, and Christo-
pher John Quinn. An explore-then-commit algorithm for submodular maximization under
full-bandit feedback. In Uncertainty in Artificial Intelligence , pages 1541–1551. PMLR, 2022.
[59] Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning.
arXiv preprint arXiv:1608.02732 , 2016.
[60] Yury Polyanskiy and Yihong Wu. Dualizing le cam’s method for functional estimation, with
applications to estimating the unseens. arXiv preprint arXiv:1902.05616 , 2019.
[61] Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dy-
namics in convex programming. IEEE Transactions on Information Theory , 57(10):7036–
7056, 2011.
[62] Maxim Raginsky and Alexander Rakhlin. Lower bounds for passive and active learning. In
Advances in Neural Information Processing Systems , pages 1026–1034, 2011.
[63] Nived Rajaraman, Yanjun Han, Jiantao Jiao, and Kannan Ramchandran. Statistical complexity
and optimal algorithms for non-linear ridge bandits. arXiv preprint arXiv:2302.06025 , 2023.
[64] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging of-
fline reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural
Information Processing Systems , 34:11702–11716, 2021.
[65] Philippe Rigollet and Assaf Zeevi. Nonparametric bandits with covariates. arXiv preprint
arXiv:1003.1630 , page 54, 2010.
[66] Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics
of Operations Research , 35(2):395–411, 2010.
[67] Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of thompson sam-
pling. The Journal of Machine Learning Research , 17(1):2442–2471, 2016.
[68] Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling.
Operations Research , 66(1):230–252, 2018.
[69] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal
algorithm for contextual bandits under realizability. arXiv preprint arXiv:2003.12699 , 2020.
[70] Max Simchowitz and Dylan Foster. Naive exploration is optimal for online lqr. In International
Conference on Machine Learning , pages 8937–8948. PMLR, 2020.
[71] Max Simchowitz, Kevin Jamieson, and Benjamin Recht. The simulator: Understanding adap-
tive sampling in the moderate-confidence regime. In Conference on Learning Theory , pages
1794–1834. PMLR, 2017.
[72] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning
without mixing: Towards a sharp analysis of linear system identification. In Conference On
Learning Theory , pages 439–473. PMLR, 2018.
[73] Alexandre B Tsybakov. Introduction to Nonparametric Estimation . Springer Publishing Com-
pany, Incorporated, 2008.
[74] Sara A van de Geer. Empirical Processes in M-estimation , volume 6. Cambridge university
press, 2000.
[75] Vladimir N. Vapnik and Alexey A. Chervonenkis. On the uniform convergence of relative
frequencies of events to their probabilities. Measures of Complexity , 16(2):11, 1971.
[76] John V on Neumann and Oskar Morgenstern. Theory of games and economic behavior. 1944.
[77] Andrew Wagenmaker and Kevin Jamieson. Active learning for identification of linear dynam-
ical systems. In Conference on Learning Theory , pages 3487–3582. PMLR, 2020.
14[78] Andrew Wagenmaker, Guanya Shi, and Kevin G Jamieson. Optimal exploration for model-
based rl in nonlinear systems. Advances in Neural Information Processing Systems , 36, 2024.
[79] Andrew J Wagenmaker and Dylan J Foster. Instance-optimality in interactive decision making:
Toward a non-asymptotic theory. In The Thirty Sixth Annual Conference on Learning Theory ,
pages 1322–1472. PMLR, 2023.
[80] Andrew J Wagenmaker, Max Simchowitz, and Kevin Jamieson. Task-optimal exploration in
linear dynamical systems. In International Conference on Machine Learning , pages 10641–
10652. PMLR, 2021.
[81] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint , volume 48.
Cambridge University Press, 2019.
[82] Abraham Wald. Statistical decision functions which minimize the maximum risk. Annals of
Mathematics , pages 265–280, 1945.
[83] Yuanhao Wang, Ruosong Wang, and Sham M Kakade. An exponential lower bound for
linearly-realizable MDPs with constant suboptimality gap. Neural Information Processing
Systems (NeurIPS) , 2021.
[84] Gellért Weisz, Philip Amortila, and Csaba Szepesvári. Exponential lower bounds for planning
in MDPs with linearly-realizable optimal action-value functions. In Algorithmic Learning
Theory , pages 1237–1264. PMLR, 2021.
[85] Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridg-
ing sample-efficient offline and online reinforcement learning. Advances in neural information
processing systems , 34:27395–27407, 2021.
[86] Yunbei Xu and Assaf Zeevi. Bayesian design principles for frequentist sequential learning. In
International Conference on Machine Learning , pages 38768–38800. PMLR, 2023.
[87] Bin Yu. Assouad, fano, and le cam. In Festschrift for Lucien Le Cam: research papers in
probability and statistics , pages 423–435. Springer, 1997.
[88] Tong Zhang. Information-theoretic upper and lower bounds for statistical estimation. IEEE
Transactions on Information Theory , 52(4):1307–1321, 2006.
[89] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement
learning for linear mixture markov decision processes. In Conference on Learning Theory ,
pages 4532–4576. PMLR, 2021.
[90] Ingvar Ziemann and Henrik Sandberg. Regret lower bounds for learning linear quadratic gaus-
sian systems. IEEE Transactions on Automatic Control , 2024.
15Contents of Appendix
A Related work 16
B Additional Background on DMSO Framework 17
B.1 Examples of Assumption 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C Technical Tools 19
D Proofs from Section 3 19
D.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
D.2 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
D.3 Proof of Proposition 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D.4 Proof of Corollary 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
E Additional Results from Section 3.2 24
E.1 Recovering DEC-based regret lower bounds . . . . . . . . . . . . . . . . . . . . . 24
E.2 Results for interactive estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
F Proofs from Section 3.2 and Appendix E 26
F.1 Proof of Lemma 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.2 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
F.3 Proof of Proposition 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
F.4 Proof of Theorem E.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
F.5 Proof of Proposition E.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
F.6 Proof of Proposition E.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
G Exploration-by-Optimization Algorithm 31
H Additional discussion and results from Section 4 34
H.1 Additional discussion from Section 4.2 . . . . . . . . . . . . . . . . . . . . . . . . 34
H.2 Application: Structured bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
H.3 Application: Contextual bandits with general function approximation . . . . . . . 35
I Proofs from Section 4 and Appendix H 36
I.1 Proof of Theorem 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
I.2 Proof of Theorem 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
I.3 Proof of Theorem 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
I.4 Proof of Theorem 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
I.5 Proof of Theorem H.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
I.6 Proof of Theorem H.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
I.7 Proof of Corollary H.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
A Related work
In what follows, we briefly survey the most relevant related work.
Minimax bounds for statistical estimation. There is a vast body of literature on minimax risk
bounds for statistical estimation, including Hasminskii and Ibragimov [42], Bretagnolle and Hu-
ber [14], Birgé [12], Donoho and Liu [29], Cover and Thomas [25], Ibragimov and Has’Minskii
[43], Tsybakov [73] as well as references therein. For minimax lower bounds, the most widely ap-
plied techniques are Le Cam’s two-point method [54], Assouad’s lemma [7], and Fano’s inequality
[25]. Variants and applications of these three methods abound [1, 23, 60, 32]; Fano’s inequality in
particular has perhaps the largest number of variants, of which the most general version we are aware
of is due to Chen et al. [23], which is recovered by our results (Proposition 2). Another celebrated
thread, starting from the seminal work of Donoho and Liu [28], provides upper and lower bounds for
a large class of non-parametric estimation problems based on Le Cam’s two-point method through
the study of a complexity measure known as the modulus of continuity [29, 30, 53, 60]. Specifically,
for a functional T:M → Ron the space of probability models M, the modulus of continuity is
16defined as
wε(M,ĎM):= sup
M∈M
|T(M)−T(ĎM)|:D2
H 
M,ĎM
≤ε2	
. (21)
Decision-Estimation Coefficient and Information Ratio. For interactive decision making prob-
lems Foster et al. [36, 38] introduce Decision-Estimation Coefficient (DEC) as a complexity mea-
sure and show that it characterizes the minimax-optimal regret up to a log|M| factor. The DEC
can be viewed as an interactive counterpart of the modulus of continuity in (21), and captures hard-
ness of interactive decision making related to exploration, but not necessarily estimation. An active
line of research has built on the DEC to encompass a variety of more increasing general decision
making settings [36, 37, 19, 38, 33, 40], including adversarial decision making [37], PAC decision
making [19, 38], reward-free learning and preference-based learning [19], and multi-agent decision
making and partial monitoring [33].
The DEC is closely related to a Bayesian complexity measure known as the information ra-
tio [67, 68, 50, 49, etc.],which was originally introduced to analyze Bayesian algorithms such as
posterior sampling. It is also related to a more recent generalization known as the algorithmic
information ratio (AIR) [86], developed for frequentist algorithms. Additionally, the DEC is
connected to asymptotic instance-dependent complexity, as explored by [79].
Lower bounds for interactive learning. There is a long line of work studying the fundamental
limits of online learning and reinforcement learning (RL), including lower bounds for structured ban-
dits [26, 66, 71, 51, 46, etc.], contextual bandits [65, 35, etc.], Markov Decision Processes (MDPs)
[59, 27, 89, 84, 83, etc.], partially observable RL [47, 57, 21, 22, etc.], dynamical systems and con-
trol [72, 44, 70, 77, 90, etc.], and offline RL [64, 85, 80, 45, 20, 55, 78, etc.]. A large portion of
these lower bounds are proven through (variants of) the two-point method, and can be recovered by
the DEC lower bound approach [36, 38]. Beyond two-point method, there are also (relatively fewer)
papers that have used Assouad’s lemma or Fano’s inequality for proving lower bounds in interactive
learning [17, 65, 2, 62, 35, 70, 63]. However, the approachs in these papers are specialized to the
specific setting (hypercube structure in particular), and there is not a general principle of Fano’s
method or Assouad’ lemma in interactive setting. The challenge of applying Fano’s inequality is
also highlighted in various prior works, e.g., Arias-Castro et al. [6, Section 1.3] and Rajaraman et al.
[63, Section 1.5.4].
Learnability in statistical learning and decision making. In the literature on statistical learn-
ing, there is a long line of work which characterizes learnability (i.e., asymptotic achievability of
non-trivial sample complexity) of hypothesis classes in terms of abstract complexity measures. Ex-
amples include the Vapnik-Chervonenkis dimension for binary classification [75, 13], the Littlestone
dimension [56] for online classification [9] and differentially private classification [16, 5], and their
real-valued counterparts (e.g. scale-sensitive dimensions) for regression [8, 4].
Beyond the settings above—and in particular for interactive settings—learnability is less well under-
stood. The question of what complexity measure characterizes bandit learnability has been raised in
e.g. Simchowitz et al. [71]. Remarkably, Ben-David et al. [10] demonstrate that there exist simple
and natural learning for which it is impossible to characterize learnability through any combinatorial
dimension. More recently, Hanneke and Yang [41] provide similar impossibility results for charac-
terizing learnability of structured bandits in a noiseless setting with real-valued rewards. Hanneke
and Yang [41] complement this with a positive result for the case of binary-valued rewards, char-
acterizing learnability through a complexity measure called the maximin volume . Our learnability
characterization for noisy structured bandits generalizes this complexity measure.
B Additional Background on DMSO Framework
The DMSO framework (Section 1.1) encompasses a wide range of learning goals beyond the reward
maximization setting [36, 38], including reward-free learning, model estimation, and preference-
based learning [19], and also multi-agent decision making and partial monitoring [33]. We provide
two examples below for illustration.
Example 2 (Preference-based learning) .In preference-based learning, each model M∈ M is
assigned with a comparison function CM: Π×Π→R(where CM(π1, π2)typically the probability
ofτ1≻τ2forτ1∼(M, π 1),τ2∼(M, π 2)), and the risk function is specified by gM(π) =
17max π⋆CM(π⋆, π). Chen et al. [19] provide lower and upper bounds for this setting in terms of
Preference-based DEC (PBDEC).
Example 3 (Interactive estimation) .In the setting of interactive estimation (a generalized PAC learn-
ing goal), each model M∈ M is assigned with a parameter θM∈Θ, which is the parameter that
the agent aims to estimate. The decision space Π = Π 0×Θ, where each decision π∈Πconsists of
π= (π0, θ), where π0is the explorative policy to interact with the model2, and θis the estimator of
the model parameter. In this setting, we define gM(π) = Dist( θM, θ)for certain distance Dist(·,·).
This setting is an interactive version of the statistical estimation task, and it is also a generalization
of the model estimation task studied in Chen et al. [19]. Natural examples include estimating some
coordinates of the parameter θin linear bandits. We provide nearly tight guarantee for this setting
in Appendix E.2.
Applicability of our results Our general interactive Fano method Lemma 5 applies to any gen-
eralized no-regret / PAC learning goal (Section 1.1). Therefore, our risk lower bound in terms of
quantile PAC-DEC Theorem 6 and decision dimension lower bound Theorem 10 both apply to any
generalized learning goal. For a concrete example, see Appendix E.2 for the application to interac-
tive estimation.
B.1 Examples of Assumption 2
In this section, we provide three general types of model classes where Assumption 2 holds with mild
CKL. It is worth noting that in Assumption 2, the reference model ĎMdoes notnecessarily belong
toco(M).
Example 4 (Gaussian bandits) .Suppose that H ⊆ (A → [0,1])is a class of mean value function,
andMH,Vis the class of the model Massociated with a hM∈ H:
M(π) =N(hM(π),1), π ∈ A.
Then, consider the reference model ĎMgiven byĎM(π) =N(0,1)∀π∈ A. It is clear that for any π,
and model M∈ M H,V,
DKL(M(π)∥ĎM(π)) =1
2hM(π)2≤1
2,
and hence Assumption 2 holds with CKL=1
2.
Example 5 (Problems with finite observations) .Suppose that the observation space Ois finite.
Then, consider the reference model ĎMgiven byĎM(π) = Unif( O)∀π∈Π. It holds that
DKL(M(π)∥ĎM(π))≤log|O|,∀π∈Π,
and hence Assumption 2 holds with CKL= log|O|.
Example 5 can further be generalized to infinite observation space, as long as every model in M
admits a bounded density function with respect to the same base measure.
Example 6 (Contextual bandits) .Suppose that H ⊆ (C × A → [0,1])is a class of mean value
function, and MH,Vis the class of the model Mspecified by a value function hM∈ H and a
context distribution νM∈∆(C). More specifically, for any π∈Π = ( C → A ),M(π)is the
distribution of (c, a, r ), generated by c∼νM,a=π(c), and r∼N(hM(c, a),1).
Then, consider the reference model ĎMspecified by νĎM= Unif( C)andhĎM≡0. It is clear that for
anyπ, and model M∈ M H,V,
DKL(M(π)∥ĎM(π))≤log|C|+ 1
and hence Assumption 2 holds with CKL= log|C|+ 1.
The factor of log|C|in Example 6 is due to the definition (39) of logDdim ∆(H), where we take
supremum over all context distribution µ. This factor can be removed if we instead restrict the
model class to have a common context distribution (i.e., the setting where context distribution is
known or can be estimated from samples).
2In other words, M(π)only depends on πthrough π0.
18C Technical Tools
Lemma C.1 (Sub-additivity for squared Hellinger distance, see e.g. [31, Lemma 9.5.3] [39, Lemma
D.2] ) .Let(X1,F1), . . . , (XT,FT)be a sequence of measurable spaces, and let Xt=Qt
i=1Xi
andFt=Nt
i=1Fi. For each t, letPt(· | ·)andQt(· | ·)be probability kernels from (Xt−1,Ft−1)
to(Xt,Ft).
LetPandQbe the laws of X1, . . . , X Tunder Xt∼Pt(· |X1:t−1)andXt∼Qt(· |X1:t−1)
respectively. Then it holds that
D2
H(P,Q)≤7EP"TX
t=1D2
H 
Pt(· |X1:t−1),Qt(· |X1:t−1)#
. (22)
In particular, given a T-round algorithm ALGand a model M, we can consider random variables
X1= (π1, o1),···, XT= (πT, oT). Then, PM,ALG(Xt=· |X1:t−1)is the distribution of (πt, ot),
where πt∼pt(· |π1, o1,···, πt−1, ot−1), and ot∼M(πt). Therefore, applying Lemma C.1 to
D2
H(PM,ALG,PĎM,ALG)gives the following corollary.
Corollary C.2. For any T-round algorithm ALG, it holds that
1
2DTV 
PM,ALG,PĎM,ALG2≤D2
H 
PM,ALG,PĎM,ALG
≤7T·Eπ∼pĎM,ALG
D2
H 
M(π),ĎM(π)
.
Lemma C.3 (Foster et al. [36, Lemma A.4]) .For any sequence of real-valued random variables
(Xt)t≤Tadapted to a filtration (Ft)t≤T, it holds that with probability at least 1−δ, for all t≤T,
tX
s=1−logE[exp(−Xs)|Fs−1]≤tX
s=1Xs+ log (1 /δ).
Lemma C.4. For any pair of random variable (X, Y), it holds that
EX∼PX
D2
H 
PY|X,QY|X
≤2D2
H(PX,Y,QX,Y).
Lemma C.5. Suppose that for a random variable X, its mean and variance under PisµPandσ2
P,
and its mean and variance under QisµQandσ2
Q. Then it holds that
|µP−µQ|2≤4
σ2
P+σ2
Q+1
2|µP−µQ|2
D2
H(P,Q).
In particular, when µP, µQ, σP, σQ∈[0,1], we have D2
H(P,Q)≥1
10|µP−µQ|2.
On the other hand, when P=N(µP,1),Q=N(µQ,1), then D2
H(P,Q)≤1
8|µP−µQ|2.
Proof. Letν=P+Q
2be the common base measure and set µ=µP+µQ
2. Then
|µP−µQ|2=|EP[X−µ]−EQ[X−µ]|2
=EνdP
dν−dP
dν
(X−µ)2
≤Eν
 r
dP
dν−r
dP
dν!2
Eν
 r
dP
dν+r
dP
dν!2
(X−µ)2

≤2D2
H(P,Q)·2 
EP(X−µ)2+EQ(X−µ)2
= 4
σ2
P+σ2
Q+1
2|µP−µQ|2
D2
H(P,Q).
D Proofs from Section 3
In this section, we present proofs for the results in Section 3, except Section 3.2.
19D.1 Proof of Theorem 1
In the following, we fix a prior µ∈∆(M), quantile δ >0,f-divergence Df, and an algorithm ALG.
We first note that the risk of ALGunder prior µis lower bounded by
EM∼µEX∼PM,ALG[L(M, X )]≥∆·PM∼µ,X∼PM,ALG(L(M, X )≥∆).
It remains to show the following claim.
Claim. Suppose that there exists a reference distribution Qsuch that
df,δ(ρ∆,Q)>EM∼µDf 
PM,ALG,Q
,
thenPM∼µ,X∼PM,ALG(L(M, X )≥∆)≥δ.
We denote ¯ρ∆=PM∼µ,X∼PM,ALG(L(M, X )<∆), and recall that we define ρ∆,Q=
PM∼µ,X∼Q(L(M, X )<∆). We then consider the following two distributions over M × X :
P0:M∼µ, X∼PM,ALG, P 1:M∼µ, X∼Q.
By the data processing inequality of f-divergence, we have
Df(¯ρ∆, ρ∆,Q)≤Df(P0, P1) =EM∼µDf 
PM,ALG,Q
.
Therefore, using df,δ(ρ∆,Q)>EM∼µDf 
PM,ALG,Q
, we know that df,δ(ρ∆,Q)> D f(¯ρ∆, ρ∆,Q).
In particular, we know ρ∆,Q<1−δ. Hence, there are two cases: (1) ¯ρ∆≤ρ∆,Q<1−δ, or
(2)¯ρ∆> ρ ∆,Q, and we can use the monotone property of Df(Lemma D.1), which also implies
¯ρ∆≤1−δ. This immediately gives
PM∼µ,X∼PM,ALG(L(M, X )≥∆) = 1 −¯ρ∆≥δ.
Hence, the proof is completed.
Lemma D.1. Forx, y∈(0,1), the quantity Df(x, y)is increasing with respect to xwhen x≥y.
Proof of Lemma D.1 By definition, we know that
Df(x, y) =yfx
y
+ (1−y)f1−x
1−y
.
For any x > z ≥y, we denote
ax=x
y, b x=1−x
1−y, a z=z
y, b z=1−z
1−y,
and then because fis convex, we know
az−bx
ax−bxf(ax) +ax−az
ax−bxf(bx)≥f(az),
bz−bx
ax−bxf(ax) +ax−az
ax−bzf(bx)≥f(bz).
Notice that yaz+ (1−y)bz= 1, and hence
yf(az) + (1 −y)f(bz)≤yf(ax) + (1 −y)f(bx).
This gives Df(x, y)≥Df(z, y).
D.2 Proof of Proposition 2
Consider
Q=EM∼µPM,ALG.
Then, by the choice of Qand definition of KL-divergence, we have
EM∼µDf 
PM,ALG,Q
=Iµ,ALG(M;X),
and by definition, we have
ρ∆,Q=PM∼µ,X′∼Q(L(M, X′)<∆)≤sup
xµ(M∈ M :L(M, x)<∆), (23)
20For any δ∈(0,1)and∆>0, we apply Theorem 1 to obtain that when
Iµ,ALG(M;X)< D KL(1−δ∥ρ∆,Q), (24)
we have
sup
M∈MEX∼PM,ALG[L(M, X )]≥δ∆. (25)
Note that the KL-divergence between Bern(1 −δ)andBern( ρ∆,Q)is lower bounded by
DKL(1−δ∥ρ∆,Q) = (1 −δ) log1−δ
ρ∆,Q+δlogδ
(1−ρ∆,Q)
>(1−δ) log1
ρ∆,Q+ (1−δ) log(1 −δ) +δlogδ
≥(1−δ) log1
ρ∆,Q−log 2
≥(1−δ) log1
supxµ(M∈ M :L(M, x)<∆)−log 2(26)
where the third inequality is by Jensen’s inequality, and the last inequality is by (23).
Taking
δ= 1 +Iµ,ALG(M;X) + log 2
log supxµ(M∈ M :L(M, x)<∆),
we know from (26) that (24) is true so that the result (25) holds, which proves Corollary 2.
D.3 Proof of Proposition 3
We frame the problem in the ISDM framework, where each algorithm corresponds to an estimator
bθ:Y⊗n→Θ. Let the model class M={M0, M1}, where for each estimator ALG(regarded as an
algorithm), PM0,ALGis the distribution of X∈ A generated by
X∼M0:θ∼ν0, Y1, . . . , Y n∼Pθ, X=ALG(Y1, . . . , Y n),
andPM1,ALGis the distribution of X∈ A generated by
X∼M1:θ∼ν1, Y1, . . . , Y n∼Pθ, X=ALG(Y1, . . . , Y n).
We further define the new loss for Mi,i∈ {0,1}:
ℓ(Mi, X):= inf
θ∈supp( νi)L(θ, X),∀X∈ A.
By the separation condition on Θ0andΘ1, we have for any X∈ A,
ℓ(M0, X) +ℓ(M1, X)≥2∆.
This implies that
PM∼µ(ℓ(M, X )≥∆)≥1/2,∀X∈Θ.
Therefore, choosing prior µ= Unif( {0,1})and reference Q=EM∼µPM,ALGgives
ρ∆,Q=PM∼µ,X∼Q(ℓ(M, X )<∆)≤1/2,
and
EM∼µ[DTV 
PM,ALG,Q
] =1
2 
DTV 
PM0,ALG,Q
+DTV 
PM1,ALG,Q
≤1
2DTV 
PM0,ALG,PM1,ALG
≤1
2DTV(ν0⊗Pθ, ν1⊗Pθ)≤1
4,
21where the first inequality is by the convexity of the TV distance and the second inequality is by the
data-processing inequality. This shows that
EM∼µ[DTV(PM,ALG,Q)]≤1/4≤d|·|,1/4(ρ∆,Q).
Therefore, applying Theorem 1 gives
sup
θ∈ΘEY1,...,Y n∼Pθ[L(θ,ALG(Y1, . . . , Y n))]≥Eθ∼ν0+ν1
2EY1,...,Y n∼Pθ[L(θ,ALG(Y1, . . . , Y n))]
≥EM∼µEY1,...,Y n∼PM,ALG[ℓ(M,ALG(Y1, . . . , Y n))]
≥EM∼µEX∼PM,ALG[ℓ(M, X )]≥∆
4,
where the second inequality follows from the fact that
EY1,...,Y n∼Pθ[L(θ,ALG(Y1, . . . , Y n))]≥EY1,...,Y n∼Pθ[ℓ(Mi,ALG(Y1, . . . , Y n))], θ ∈supp( νi),
and the last inequality follows from Theorem 1 with δ=1
4. This gives the desired result.
D.4 Proof of Corollary 9
Consider the following setup of linear bandits: let θ⋆∈Rdbe an unknown parameter. At time
t, the learner chooses an action πt∈ {π∈Rd:∥π∥2≤1}and receives a Gaussian reward
rt∼N(⟨θ⋆, πt⟩,1). For T∈N, letHT= (π1, r1,···, πT, rT)be the observed history up to time
T. The central claim of this section is the following upper bound on the mutual information.
Theorem D.2. For any r >0, we define the prior µroverBd(r)by
µr:θ⋆∼N
0,r2
4dId
| ∥θ⋆∥ ≤r.
Then for any algorithm ALG, we have
Iµr,ALG(θ⋆;HT)≤dlog
1 +r2T
4d2
.
Proof. Denote λ=r2
4. We first prove that if θ⋆∼µ=N(0, λId/d), then
Iµ,ALG(θ⋆;HT)≤d
2log
1 +λT
d2
. (27)
By the Bayes rule, the posterior distribution of θ⋆conditioned on (Ht−1, πt)is
p(θ⋆| Ht−1, πt)∝exp 
−d∥θ⋆∥2
2
2λ−1
2X
s<t(rs− ⟨θ⋆, πs⟩)2!
,
which is a Gaussian distribution with covariance (Σt−1)−1, where
Σt−1=d
λId+X
s<tπs(πs)⊤.
Therefore, by the chain rule of mutual information, we have
Iµ,ALG(θ⋆;HT) =TX
t=1Iµ,ALG(θ⋆;rt|Ht−1, πt)
=TX
t=1Eµ,ALG1
2log 
1 + (πt)⊤(Σt−1)−1πt
=Eµ,ALG"
1
2TX
t=1logdet(Σt)
det(Σt−1)#
=Eµ,ALG1
2logdet(ΣT)
(d/λ)d
22≤Eµ,ALGd
2logTr(ΣT)/d
d/λ
≤d
2log
1 +λT
d2
,
which is exactly (27).
Next we deduce the claimed result from (27). Consider the random variable Z=1{∥θ⋆∥2≤r} ∈
{0,1}, and then
d
2log
1 +λT
d2
≥Iµ,ALG(θ⋆;HT)
≥Iµ,ALG(θ⋆;HT|Z)
≥P(Z= 1)·Iµr,ALG(θ⋆;HT|Z= 1)
=Pµ(∥θ⋆∥2≤r)·Iµr,ALG(θ⋆;HT).
Here the first inequality is (27), the second inequality follows from I(X;Y)−I(X;Y|f(X)) =
I(f(X);Y)−I(f(X);Y|X)≥0, the third identity follows from the definition of conditional
mutual information. Finally, noticing that Pµ(∥θ⋆∥2≤r)≥1
2by concentration of χ2
drandom
variable, we arrive at the desired statement.
Next we show how to translate the mutual information upper bound in Theorem D.2 to lower bounds
of estimation and regret.
Theorem D.3. LetT≥1,r= minn
c0d√
T,1o
for a small absolute constant c0, and consider the
prior µ=µr. For any T-round algorithm with output bπ, Proposition 8 implies that
Eµ,ALG"bπ−θ⋆
∥θ⋆∥2#
≥1
10.
Therefore, we may deduce that
sup
M⋆∈MEM⋆,ALG[Risk DM(T)]≳mind√
T,1
.
Proof. We first prove the first inequality by applying Proposition 8 to the following sub-optimality
measure
˜gMθ(π) =∥π−normalize (θ)∥2
2,
where we denote normalize (θ) =θ
∥θ∥∈Bd(1). Notice that for θ∈Θ, we have
gMθ(π) =∥θ∥ − ⟨θ, π⟩ ≥ ∥ θ∥ ·π−θ
∥θ∥2
=∥θ∥ ·˜gMθ(π).
For∆∈(0,1), we first claim that
ρ∆:= sup
πµ(θ: ˜gMθ(π)≤∆) = O√
d∆(d−1)/2
. (28)
To see so, by symmetry of Gaussian distribution, we know for fixed any π,
µ(θ: ˜gMθ(π)≤∆) =Pθ∼Unif( Bd(1))
θ:∥θ−π∥2≤∆
,
and hence we can instead consider the uniform distribution over Bd(1). By rotational invariance, we
may assume that π= (x,0,···,0), with x≥0. Then

θ∈Bd(1) :∥θ−π∥2
2≤∆	
=
θ∈Bd(1) :θ1≥x2+ 1−∆
2x
⊆n
θ∈Bd(1) :θ1≥√
1−∆o
.
23By Bubeck et al. [15, Section 2], the density of θ1∈[−1,1]is given by
f(θ1) =Γ(d/2)
Γ((d−1)/2)√π(1−θ2
1)(d−3)/2.
Therefore,
ρ∆≤Z1
√1−∆f(θ1)dθ1=O(√
d)·(1−√
1−∆)∆(d−3)/2=O√
d∆(d−1)/2
.
With the upper bound (28) of ρ∆, we know that for ∆ =1
2, it holds
log(1 /ρ∆)≥2Iµ(T),
as long as c0is a sufficiently small constant. Therefore, Proposition 8 gives that
Eµ,ALGh
∥bπ−normalize (θ⋆)∥2i
=Eµ,ALG[˜gMθ(π)]≥1
4.
This completes the proof of the first inequality.
Finally, using the fact that Pθ⋆∼µ(∥θ⋆∥ ≤ c1r)≤1
100for a small absolute constant c1, we can
conclude that
sup
M⋆∈MEM⋆,ALG[Risk DM(T)]≥Eµ,ALG[gMθ(π)]≥c1r
8= Ω
mind√
T,1
.
This is the desired result.
E Additional Results from Section 3.2
In addition to the reward-maximization setting (Example 1), we also introduce a slightly more
general setting. In this setting, we assume that for each model M∈ M , the risk function is
gM(π) =fM(πM)−fM(π), butfMis not assumed to be the expected reward function (Example 1).
Instead, we only require fMsatisfying the following assumption, where M+is a pre-specified
model class of reference models that contains co(M)(following Foster et al. [38]).
Assumption 4. LetM+⊆(Π→∆(O))be a given class of reference models, such that co(M)⊆
M+. For any M∈ M , the risk function takes form gM(π) =fM(πM)−fM(π)for some functional
fM: Π→R, so that fMcan be extended to M+, such that for any model M∈ M and reference
modelĎM∈ M+we have
|fM(π)−fĎM(π)| ≤LrDH 
M(π),ĎM(π)
,∀π∈Π. (29)
In some cases, consider a larger reference model class can be convenient for proving lower bounds,
see e.g., Appendix B.1 and Appendix I.5.
E.1 Recovering DEC-based regret lower bounds
In this section, we demonstrate how our general lower bound approach recovers the regret lower
bounds of Foster et al. [38], Glasgow and Rakhlin [40]. We first state our lower bound in terms of
constrained DEC in the following theorem.
Theorem E.1. Under the reward maximization setting (Example 1), for any T-round algorithm ALG,
there exists M⋆∈ M such that
RegDM≥T
2·
r-decc
ε(T)(M)−6ε(T)
−1
with probability at least 0.01underPM⋆,ALG, where ε(T) =1
100√
T.
Theorem E.1 immediately yields an in-expectation regret lower bound in terms of constrained DEC.
It also shaves off the unnecessary logarithmic factors in the lower bound of Foster et al. [38, Theorem
2.2].
For the remainder of this section, we sketch how we prove Theorem E.1 in a slightly more general
setting (Assumption 4), following Appendix F.3. Before providing our regret lower bounds, we first
present several important definitions.
24Definition of quantile regret-DEC We note that it is possible to directly modify the definition
of quantile PAC-DEC (11), and then apply Theorem 6 to obtain an analogous regret lower bound
immediately. However, as Foster et al. [38] noted, the “correct” notion of regret-DEC (cf. Eq. (7))
turns out to be more sophisticated. Therefore, we define the quantile version of regret-DEC similarly,
as follows.
Throughout the remainder of this section, we fix the integer T. Define
ΠT=(
bπ:bπ=1
TTX
t=1δπt,where π1,···, πT∈Π)
⊆∆(Π) ,
i.e.,ΠTis the class of all T-round mixture decision. We introduce the mixture decision space ΠT
here to handle the average of T-round profile (π1,···, πT)of the algorithm. In particular, when Π
is convex, we may regard ΠT= Π.
Next, we define the quantile regret-DEC as
r-decq
ε,δ(M,ĎM):= inf
p∈∆(Π T)sup
M∈M
bgM
δ(p)∨Eπ∼p[gĎM(π)]Eπ∼pD2
H 
M(π),ĎM(π)
≤ε2	
,
(30)
and define r-decq
ε,δ(M):= supĎM∈M+r-decq
ε,δ(M,ĎM).
The following proposition relates our quantile regret-DEC to the constrained regret-DEC (proof in
Appendix F.5).
Proposition E.2. Suppose that Assumption 4 holds for M. Then, for any ĎM∈ M+, it holds that
r-decc
ε(M ∪ {ĎM},ĎM)≤2·r-decq
ε,δ(M,ĎM) +cδLrε,
where we denote cδ= maxn
δ
1−δ,1o
. In particular, it holds that
r-decq
ε,1/2(M)≥1
2
max
ĎM∈M+r-decc
ε(M ∪ {ĎM},ĎM)−Lrε
.
Lower bound with quantile regret-DEC Now, we prove the following lower bound for the re-
gret of any T-round algorithm, via our general interactive Fano method (Lemma 5). The proof is
presented in Appendix F.4.
Theorem E.3. Suppose that Assumption 4 holds for M. Then, for any T-round algorithm ALG,
parameters ε, δ, C > 0, there exists M∈ M such that
PM,ALG
RegDM(T)≥T·(r-decq
ε,δ(M)−CLrε)−1
≥δ−1
C2−√
14Tε2.
As a corollary, there exists M⋆∈ M such that
RegDM(T)≥T
2·
max
ĎM∈M+r-decc
ε(T)(M ∪ {ĎM},ĎM)−4Lrε(T)
−1
≥T
2·
r-decc
ε(T)(M)−4Lrε(T)
−1
with probability at least 0.01underPM⋆,ALG, where ε(T) =1
100√
T.
Theorem E.1 is now an immediate corollary, because for reward-maximization setting, we always
haveLr=√
2in Assumption 4.
E.2 Results for interactive estimation
More generally, we show that for a fairly different task of interactive estimation (Example 3), we
also have an equivalence between quantile PAC-DEC with constrained PAC-DEC.
Recall that in this setting, each model M∈ M is assigned with a parameter θM∈Θ, which is the
parameter that the agent want to estimate. The decision space Π = Π 0×Θ, where each decision
π∈Πconsists of π= (π0, θ), where π0is the explorative decision to interact with the model, and θ
25is the estimator of the model parameter. The risk function is then defined as gM(π) = Dist( θM, θ),
for certain distance Dist(·,·).
In interactive estimation, we can show that the quantile DEC is in fact lower bounded the constrained
DEC, as follows (proof in Appendix F.6).
Proposition E.4. Consider the setting of Example 3. Then as long as δ <1
2, it holds that
p-decc
ε(M)≤2·p-decq
ε,δ(M).
In particular, for such a setting (which encompasses the model estimation task considered in Chen
et al. [19]), Theorem 6 provides a lower bound of estimation error in terms of constrained PAC-
DEC. This is significant because the constrained PAC-DEC upper bound in Theorem 4 is actually
not restricted to Example 1, and we have hence shown that
p-decc
ε(T)(M)≲inf
ALGsup
M⋆∈MEM⋆,ALG[Risk DM(T)]≲p-decc
¯ε(T)(M),
where ε(T)≍p
1/Tand¯ε(T)≍p
log|M|/T. Therefore, for interactive estimation, constrained
PAC-DEC is also a nearly tight complexity measure.
Remark E.5. Thelog|M|-gap between the lower and upper bound can further be closed for convex
model class, utilizing the upper bounds in Appendix G. More specifically, we consider a convex
model class M, where M7→θMis a convex function on M. Then, a suitable instantiation of
ExO+(Algorithm 1) achieves
Risk DM(T)≲∆ + inf
γ>0
p-deco
γ(M) +logN(Θ,∆) + log(1 /δ)
T
,
where N(Θ,∆)is the ∆-covering number of Θ, because we have logDdim ∆(M)≤logN(Θ,∆)
by considering the prior q= Unif(Θ 0)for a minimal ∆-covering of Θ. Similar to Theorem I.4, we
can upper bound p-deco
γ(M)byp-decc
ε(M). Taking these pieces together, we can show that under
the assumption that p-decc
ε(M)is of moderate decay, ExO+achieves
Risk DM(T)≲p-decc
ε(T)(M),
where ε(T)≍p
logN(Θ,1/T)/T.
In particular, for the (non-interactive) functional estimation problem (see e.g. Polyanskiy and Wu
[60]), the parameter space Θ⊂R, and hence by considering covering number, we have log|Θ|=
eO(1). Therefore, for convex M, under mild assumption that the DEC is of moderate decaying
(Assumption 3), the minimax risk is then characterized by (up to logarithmic factors)
inf
ALGsup
M⋆∈MEM⋆,ALG[Risk DM(T)]≍p-decc√
1/T(M).
This result can be regarded as a generalization of Polyanskiy and Wu [60] to the interactive estima-
tion setting.
F Proofs from Section 3.2 and Appendix E
Additional notations For notational simplicity, for any distribution q∈∆(Π) and reference
modelĎM, we denote the localized model class around ĎMas
Mq,ε(ĎM):=
M∈ M :Eπ∼qD2
H 
M(π),ĎM(π)
≤ε2	
.
F.1 Proof of Lemma 5
We show that
∆⋆
ALG,δ= sup
∆>0
∆|sup
M∈MPM,ALG(gM(bπ)≥∆)> δ
, (31)
i.e.∆⋆
ALG,δis the maximum risk of ALGover the model class M, measured in terms of the δ-quantile.
Note that by the definition of ∆⋆
ALG,δ, we have
∆⋆
ALG,δ≥ sup
ĎM=M∈Msup
∆>0{∆ :pM,ALG(π:gM(π)≥∆)> δ}
26= sup
∆>0
∆ : sup
M∈MPM,ALG(gM(π)≥∆)> δ
.
On the other hand, using the chain rule of Hellinger distance (Lemma C.1), we have
DTV 
PM,ALG,PĎM,ALG
≥q
14TEπ∼qĎM,ALGD2
H 
M(π),ĎM(π)
.
Therefore, we know
∆⋆
ALG,δ:= sup
ĎM∈co(M)sup
M∈Msup
∆≥0n
∆ :pĎM,ALG(π:gM(π)≥∆)> δ+q
14TEπ∼qĎM,ALGD2
H 
M(π),ĎM(π)o
≤ sup
ĎM∈co(M)sup
M∈Msup
∆≥0
∆ :pĎM,ALG(π:gM(π)≥∆)> δ+DTV 
PM,ALG,PĎM,ALG	
.
Notice that for any ĎM∈co(M),M∈ M , we can take L(M, X ) =gM(π)(recall that X=
(HT,bπ)is the whole trajectory), and µM∈∆(M)supported on MandQ=PĎM,ALGin Theorem 1,
and hence
∆⋆
ALG,δ≤ sup
ĎM∈co(M)sup
M∈Msup
∆≥0
∆ :DTV 
1−δ, ρ∆,PĎM,ALG
> D TV 
PM,ALG,PĎM,ALG	
≤sup
M∈Msup
∆>0{∆ :PM,ALG(bπ:gM(bπ)≥∆)> δ},
where the last line follows from the claim in Appendix D.1. The proof of (31) is hence completed.
F.2 Proof of Theorem 6
Fix any algorithm ALGand abbreviate ε=ε(T). Take an arbitrary parameter ∆0<p-decq
ε,δ(M).
Then there exists ĎMsuch that ∆0<p-decq
ε,δ(M,ĎM). Hence, by the definition (11), we know that
∆0<sup
M∈Mn
bgM
δ(pĎM,ALG)Eπ∼qĎM,ALGD2
H 
M(π),ĎM(π)
≤ε2o
.
Therefore, there exists M∈ M such that
Eπ∼qĎM,ALGD2
H 
M(π),ĎM(π)
≤ε2,Pπ∼pĎM,ALG(gM(π)≥∆0)≥δ.
This immediately implies
pĎM,ALG(π:gM(π)≥∆)> δ1+q
14TEπ∼qĎM,ALGD2
H 
M(π),ĎM(π)
,
where δ1=δ−p
14Tε2. Notice that δ1>δ
2, and hence applying Lemma 5 shows that there
exists M∈ M such that PM,ALG 
gM(bπ)≥∆0
≥δ
2. Letting ∆0→p-decq
ε,δ(M)completes the
proof.
F.3 Proof of Proposition 7
In this section, we prove Proposition 7 under the slightly more general setting of Assumption 4.
Proposition F.1. Under Assumption 4, for any reference model ĎM∈ M+andε >0, δ∈[0,1), it
holds that
p-decc
ε/√
2(M,ĎM)≤p-decq
ε,δ(M,ĎM) +2εLr
1−δ.
For Example 1, we always have Lr≤√
2, and hence Proposition 7 follows immediately from
Proposition F.1.
Proof of Proposition F.1. Fix a reference model ĎMand a ∆0>p-decq
ε,δ(M,ĎM). Then, we pick
a pair (¯p,¯q)such that
∆0>sup
M∈M
bgM
δ(¯p)|Eπ∼¯qD2
H 
M(π),ĎM(π)
≤ε2	
,
27whose existence is guaranteed by the definition of p-decq
ε,δ(M,ĎM)in (11). In other words, we
have
Pπ∼¯p(gM(π)≤∆0)≥1−δ, ∀M∈ M ¯q,ε(ĎM)
Consider q=¯p+¯q
2andε′=ε√
2. Also let
˜M:= arg max
M∈Mq,ε′(ĎM)fM(πM).
Now, consider p∈∆(Π) given by
p(·) = ¯p 
·|g˜M(π)≤∆0
.
By definition, for π∼pwe have f˜M(π)≥f˜M(π˜M)−∆0, and hence
Eπ∼p[gM(π)] =fM(πM)−Eπ∼p[fM(π)]
≤fM(πM)−Eπ∼ph
f˜M(π)i
+Lr·Eπ∼pDH
M(π),˜M(π)
≤fM(πM)−f˜M(π˜M) + ∆ 0+Lr·Eπ∼pDH
M(π),˜M(π)
.
Notice that for any M∈ M q,ε′(ĎM), we have fM(πM)≤f˜M(π˜M)and also
Eπ∼pDH
M(π),˜M(π)
≤1
¯p(g˜M(π)≤∆0)Eπ∼¯pDH
M(π),˜M(π)
≤1
1−δ
Eπ∼¯pDH 
M(π),ĎM(π)
+Eπ∼¯pDH
˜M(π),ĎM(π)
≤2ε
1−δ.
Combining these inequalities gives
p-decc
ε′(M,ĎM)≤sup
M∈M
Eπ∼p[gM(π)]|Eπ∼qD2
H 
M(π),ĎM(π)
≤ε2
2
≤∆0+2εLr
1−δ.
Letting ∆0→p-decq
ε,δ(M,ĎM)completes the proof.
F.4 Proof of Theorem E.3
Our proof adopts the analysis strategy originally proposed by Glasgow and Rakhlin [40].
Fix a 0<∆<r-decq
ε,δ(M)and a parameter c∈(0,1). Then there exists ĎM∈ M+such that
r-decq
ε,δ(M,ĎM)>∆.
Fix a T-round algorithm ALGwith rules p1,···, pT, we consider a modified algorithm ALG′:for
t= 1,···, T, and history H(t−1), we set p′
t(·|H(t−1)) =pt(·|H(t−1))ifPt−1
s=1gĎM(πs)< T∆−1,
and set p′
t(·|H(t−1)) = 1 πĎMif otherwise. By our construction, it holds that under ALG′, we havePT
t=1gĎM(πt)< T∆almost surely. Furthermore, we can define the stopping time
τ= inf(
t:tX
s=1gĎM(πs)≥T∆−1ort=T+ 1)
.
Ifτ≤T, then it holds thatPτ
t=1gĎM(πt)≥T∆−1.
Now, we consider p=PĎM,ALG′(1
TPT
t=1πt=·)∈∆(Π T). Using our definition of r-decq, we know
thatEπ∼pgĎM(π)<∆by our construction, and hence there exists M∈ M such that
Pbπ∼p(gM(bπ)≥∆)> δ, Ebπ∼pD2
H 
M(bπ),ĎM(bπ)
≤ε2.
By definition of pand Lemma C.1, we have
PĎM,ALG′ TX
t=1gM(πt)≥T∆!
> δ, D2
H 
PM,ALG′,PĎM,ALG′
≤7Tε2. (32)
28We also know
EĎM,ALG′"
1
TTX
t=1|fM(πt)−fĎM(πt)|2#
≤EĎM,ALG′"
1
TTX
t=1L2
rD2
H 
M(πt),ĎM(πt)#
=L2
rEbπ∼pD2
H 
M(bπ),ĎM(bπ)
≤L2
rε2,
and hence by Markov inequality,
PĎM,ALG′ 
1
TTX
t=1|fM(πt)−fĎM(πt)| ≥CLrε!
≤1
C2.
In the following, we consider events
E1:=(TX
t=1gM(πt)≥T∆)
,
and the random variable X:=PT
t=1|fM(πt)−fĎM(πt)|. By definition, PĎM,ALG′(E1)> δ ,
PĎM,ALG′(X≥CTL rε)≤1
C2. We have the following claim.
Claim: Under the event E1∩ {τ≤T}, we have
τX
t=1gM(πt)≥T∆−X−1.
To prove the claim, we bound
τX
t=1gM(πt) =TX
t=1gM(πt)−TX
t=τ+1gM(πt)
≥T∆−TX
t=τ+1[fM(πM)−fM(πt)]
≥T∆−(T−τ)fM(πM) +TX
t=τ+1fĎM(πt)−X
=T∆−(T−τ)· 
fM(πM)−fĎM(πĎM)
−X,
where the first inequality follows from E1, and the second inequality follows fromPT
t=τ+1|fM(πt)−fĎM(πt)| ≤X. On the other hand, we can also bound
τX
t=1gM(πt) =τX
t=1[fM(πM)−fM(πt)]
≥τfM(πM)−τX
t=1fĎM(πt)−X
=τ· 
fM(πM)−fĎM(πĎM)
+τX
t=1gĎM(πt)−X
≥τ· 
fM(πM)−fĎM(πĎM)
+T∆−1−X,
where the first inequality follows fromPτ
t=1|fM(πt)−fĎM(πt)| ≤X, and the second inequality is
becausePτ
t=1gĎM(πt)≥T∆−1given τ≤T, which follows from the definition of the stopping
timeτ. Therefore, taking maximum over the above two inequalities proves our claim.
Now, using the claim, we know
PĎM,ALG′ τ∧TX
t=1gM(πt)≥T(∆−Cε)−1!
≥PĎM,ALG′(E1∩ {X≤CTε})≥δ−1
C2.
29Notice that D2
H 
PM,ALG′,PĎM,ALG′
≤7Tε2, and hence for any event E, it holds PM,ALG′(E)≥
PĎM,ALG′(E)−√
14Tε2. In particular, we have
PM,ALG′ τ∧TX
t=1gM(πt)≥T(∆−CLrε)−1!
≥δ−1
C2−√
14Tε2.
Finally, we note that ALGandALG′agree on the first τ∧Trounds (formally, ALGandALG′induce the
same distribution of (π1,···, πτ∧T)), and hence
PM,ALG τ∧TX
t=1gM(πt)≥T(∆−CLrε)−1!
≥δ−1
C2−√
14Tε2.
The proof is hence complete by noticing thatPτ∧T
t=1gM(πt)≤PT
t=1gM(πt) =RegDM(T)and
taking ∆→r-decq
ε,δ(M).
F.5 Proof of Proposition E.2
Fix aĎM∈ M+, and ∆>r-decq
ε,δ(M,ĎM). Choose p∈∆(Π T)such that
bgM
δ(p)∨Eπ∼p[gĎM(π)]≤∆,∀M∈ M p,ε(ĎM).
The existence of pis guaranteed by the definition (30). In other words, we have Eπ∼p[gĎM(π)]≤∆
and
Pπ∼p(gM(π)≥∆)≤δ, ∀M∈ M p,ε(ĎM).
We then has the following claim.
Claim. Suppose that M∈ M p,ε(ĎM). Then it holds that
Eπ∼p[gM(π)]≤Eπ∼p[gĎM(π)] + ∆ + cδLrEπ∼pDH 
M(π),ĎM(π)
. (33)
Fix any M∈ M p,ε(ĎM), we prove (33) as follows. Consider the event E={π:gM(π)≤∆}.
Then,
p(E) 
fM(πM)−fĎM(πĎM)
=Eπ∼p1{E} 
gM(π)−gĎM(π) +fĎM(π)−fM(π)
≤p(E)∆ + LrEπ∼p1{E}DH 
M(π),ĎM(π)
,
where the inequality uses gM(π)≤∆forπ∈ Eand Assumption 4. Therefore,
Eπ∼pgM(π) =Eπ∼p1{E}gM(π) +Eπ∼p1{Ec}gM(π)
≤p(E)∆ +Eπ∼p1{Ec} 
fM(πM)−fĎM(πĎM) +fĎM(π)−fM(π) +gĎM(π)
≤2∆ +p(Ec)Lr
p(E)Eπ∼p1{E}DH 
M(π),ĎM(π)
+LrEπ∼p1{Ec}DH 
M(π),ĎM(π)
≤2∆ + maxp(Ec)
p(E),1
LrEπ∼pDH 
M(π),ĎM(π)
.
This completes the proof of our claim.
Therefore, using (33) with Eπ∼p[gĎM(π)]≤∆yields
Eπ∼p[gM(π)]≤2∆ + cδLrε, ∀M∈ M p,ε(ĎM).
This immediately implies
r-decc
ε(M ∪ {ĎM},ĎM)≤2∆ + cδLrε.
Finally, taking ∆→r-decq
ε,δ(M,ĎM)completes the proof.
30F.6 Proof of Proposition E.4
Fix a reference model ĎMand let ∆0>p-decq
ε,δ(M,ĎM). Then there exists p, q∈∆(Π) such that
sup
M∈M
bgM
δ(p)|Eπ∼qD2
H 
M(π),ĎM(π)
≤ε2	
<∆0.
Therefore, it holds that
Pπ∼p(gM(π)≤∆0)≥1−δ, ∀M∈ M q,ε(ĎM).
If the constrained set Mq,ε(ĎM)is empty, then we immediately have p-decc
ε(M,ĎM) = 0 , and
the proof is completed. Therefore, in the following we may assume Mq,ε(ĎM)is non-empty, and
cM∈ M q,ε(ĎM).
Claim. Letbθ=θcMandbπ= (π0,bθ)for an arbitrary π0, it holds that
gM(bπ)≤∆0,∀M∈ M q,ε(ĎM).
This is because for any M∈ M q,ε(ĎM), it holds that
Pπ∼p(gM(π)≤∆0, gcM(π)≤∆0)≥1−2δ >0.
Hence, there exists θ∈Θsuch that Dist( θM, θ)≤∆0andDist( θcM, θ)≤∆0holds. Therefore, it
must hold that Dist( θM,bθ)≤2∆0for any M∈ M q,ε(ĎM).
The above claim immediately implies that
p-decc
ε(M,ĎM)≤sup
M∈M
gM(bπ)|Eπ∼qD2
H 
M(π),ĎM(π)
≤ε2	
≤2∆0.
Letting ∆0→p-decq
ε,δ(M,ĎM)yields p-decc
ε(M,ĎM)≤2p-decq
ε,δ(M,ĎM), which is the desired
result.
G Exploration-by-Optimization Algorithm
In this section, we present a slightly modified version of the Exploration-by-Optimization Algorithm
(ExO+) developed by Foster et al. [37], built upon Lattimore and Szepesvári [52], Lattimore and
Gyorgy [49]. The original ExO+algorithm has an adversarial regret guarantee for any model class
M, scaling with r-deco
γ(co(M)), the offset DEC of the mode class co(M), and log|Π|, the log-
cardinality of the decision space. For our purpose, we adapt the original ExO+algorithm by using
a prior q∈∆(Π) not necessarily the uniform prior, and with a suitably chosen prior q,ExO+then
achieves a regret guarantee scaling with logDdim ∆(M), instead of log|Π|(cf. Foster et al. [37]),
which is always an upper bound of logDdim ∆(M).
Offset DEC for regret. We first recall the following (original) definition of DEC [36]:
r-deco
γ(M,ĎM):= inf
p∈∆(Π)sup
M∈MEπ∼p[gM(π)]−γEπ∼pD2
H 
M(π),ĎM(π)
, (34)
andr-deco
γ(M):= supĎM∈co(M)r-deco
γ(M,ĎM). Through the Estimation-to-Decision (E2D)
algorithm [36], offset regret-DEC provides an upper bound of RegDMfor any learning problem,
and it is also closely related to the complexity of adversarial decision making.
As discussed in Foster et al. [38], in the reward maximization setting (Example 1), the constrained
regret-DEC r-decccan always be upper bounded in terms of the offset DEC r-deco. Conversely,
in the same setting, we also show that the offset DEC can also be upper bounded in terms of the
constrained DEC (Theorem I.4), and hence the two concepts can be regarded as equivalent under
mild assumptions (e.g. moderate decaying, Assumption 3).
31Algorithm 1 Exploration-by-Optimization ( ExO+)
Input: Problem (M,Π), prior q∈∆(Π) , parameter T≥1,γ >0.
1:Setq1=q.
2:fort= 1,···, Tdo
3: Solve the exploration-by-optimization objective
(pt, ℓt)←arg min
p∈∆(Π) ,ℓ∈LΓqt,γ(p, ℓ)
4: Sample πt∼pt, execute πtand observe ot
5: Update
qt+1(π)∝πqt(π) exp( ℓt(π;πt, ot))
Exploration-by-Optimization algorithm. The algorithm, ExO+, is restated in Algorithm 1. At
each round t, the algorithm maintains a reference distribution qt∈∆(Π) , and use it to obtain a
decision distribution pt∈∆(Π) and an estimation function ℓt∈ L := (Π×Π× O → R), by
solving a joint minimax optimization problem based on the exploration-by-optimization objective:
Defining
Γq,γ(p, ℓ;M, π⋆) =Eπ∼p[fM(π⋆)−fM(π)]
−γEπ∼pEo∼M(π)Eπ′∼q[1−exp (ℓ(π′;π, o)−ℓ(π⋆;π, o))],(35)
and
Γq,γ(p, ℓ) = sup
M∈M,π⋆∈ΠΓq,γ(p, ℓ;M, π⋆), (36)
the algorithm solve (pt, ℓt)←arg minp∈∆(Π) ,ℓ∈LΓqt,γ(p, ℓ). The algorithm then samples πt∼pt,
executes πtand observes otfrom the environment. Finally, the algorithm updates the reference
distribution by performing the exponential weight update with weight function ℓt(·;πt, ot).
Guarantee of ExO+.Following Foster et al. [37], we define
exo1/γ(M, q):= inf
p∈∆(Π) ,ℓ∈LΓq,γ(p, ℓ), (37)
andexo1/γ(M) = supq∈∆(Π) exo1/γ(M, q). The following theorem is deduced from Foster et al.
[37, Theorem 3.1 and 3.2].
Theorem G.1. Under the reward maximization setting3(Assumption 4), it holds that
r-deco
γ/4(co(M)))≤exo1/γ(M)≤r-deco
γ/8(co(M))),∀γ >0.
Now, we present the main guarantee of Algorithm 1, which has the desired dependence on the prior
q∈∆(Π) .
Theorem G.2. It holds that with probability at least 1−δ,
RegDM≤T
∆ +r-deco
γ/8(co(M))
+γlog1
δ·q(π:fM⋆(πM⋆)−fM⋆(π)≤∆)
Proof. Consider the set Π⋆:={π:fM⋆(πM⋆)−fM⋆(π)≤∆}and the distribution q⋆=q(·|Π⋆).
Following Proposition G.3, we consider
Xt(πt, ot):=Eπ∼q⋆
ℓt(π;πt, ot)
−logEπ∼qt
exp 
ℓt(π;πt, ot)
,
and Proposition G.3 implies that
TX
t=1Xt(πt, ot)≤log(1 /q(Π⋆)).
3We remark that their proof actually applies to a broader setting, e.g. the setting of interactive estimation
(Example 3, and see also Remark E.5).
32Applying Lemma C.3, we have with probability at least 1−δ,
TX
t=1−logEt−1
exp 
−Xt(πt, ot)
≤TX
t=1Xt(πt, ot) + log(1 /δ).
Notice that
Et−1
exp 
−Xt(πt, ot)
=Eπ∼ptEo∼M⋆(π)Eπ′∼qt
exp 
ℓt(π′;π, o)−Eπ⋆∼q⋆ℓt(π⋆;π, o)
.
Using the fact that 1−x≤ −logxand Jensen’s inequality, we have
TX
t=1Eπ⋆∼q⋆Err(pt, ℓt;qt, M⋆, π⋆)≤log(1 /q(Π⋆)) + log(1 /δ),
where we denote
Err(p, ℓ;q, M⋆, π⋆):=Eπ∼pEo∼M⋆(π)Eπ′∼q[1−exp (ℓ(π′;π, o)−ℓ(π⋆;π, o))].
Therefore, it holds that
RegDM=TX
t=1Eπ∼pt
fM⋆(πM⋆)−fM⋆(π)
≤TX
t=1∆ +Eπ⋆∼q⋆Eπt∼pt
fM⋆(π⋆)−fM⋆(πt)
=T∆ +γTX
t=1Eπ⋆∼q⋆Err(pt, ℓt;qt, M⋆, π⋆)
+TX
t=1Eπ⋆∼q⋆
Eπt∼pt
fM⋆(π⋆)−fM⋆(πt)
−γErr(pt, ℓt;qt, M⋆, π⋆)
| {z }
=Γqt,γ(pt,ℓt;M⋆,π⋆)
≤T∆ +γ(log(1 /q(Π⋆)) + log(1 /δ)) +TX
t=1Γqt,γ(pt, ℓt)
≤T 
∆ + exo1/γ(M)
+γ(log(1 /q(Π⋆)) + log(1 /δ)).
Applying Theorem G.1 completes the proof.
Proposition G.3. For any q′∈∆(Π) , it holds that
TX
t=1Eπ∼q′[ℓt(π;πt, ot)]−logEπ∼qt
exp 
ℓt(π;πt, ot)
≤DKL(q′∥q).
Proof. This is essentially the standard guarantee of exponential weight updates. For simplicity, we
assume Πis discrete. Then, by definition,
qt(π) =q(π) expPt
s=1ℓs(π;πs, os)
P
π′∈Πq(π′) expPt−1
s=1ℓs(π′;πs, os),
and hence
logEπ∼qt
exp 
ℓt(π;πt, ot)
= log Eπ∼qexp tX
s=1ℓs(π;πs, os)!
−logEπ∼qexp t−1X
s=1ℓs(π;πs, os)!
.
33Therefore, taking summation over t= 1,···, T, we have
−TX
t=1logEπ∼qt
exp 
ℓt(π;πt, ot)
=−logEπ∼q"
exp TX
t=1ℓt(π;πt, ot)!#
.
The proof is then completed by the following basic fact of KL divergence: for any function h: Π→
R,
Eπ∼q′[h(π)]≤logEπ∼qexp(h(π)) +DKL(q′∥q).
H Additional discussion and results from Section 4
H.1 Additional discussion from Section 4.2
Connection to the maximin volume. Hanneke and Yang [41] propose maximin volume , a complex-
ity measure that tightly characterizes the complexity of learning noiseless binary-valued structured
bandit problems. For such problem classes, the decision dimension is exactly the inverse of the max-
imin volume. While the decision dimension can be viewed as a generalization of the maximin vol-
ume in this sense, we emphasize that the decision dimension directly arises from our general lower
bound framework, and is applicable to general decision making problems in the DMSO framework.
Noise distribution. We note that the upper bound in (19) applies to any reward distribution with sub-
Gaussian noise (cf. Appendix I.2). Meanwhile, since the lower bound in Corollary 12 is specialized
to Gaussian noise, it acts as a lower bound for the broader class of sub-Gaussian noise distributions
as well. We expect the lower bound to extend to other “reasonable” noise distributions.
H.2 Application: Structured bandits
We now instantiate our general results to give tighter guarantees for structured bandits, improving
the upper bounds in Section 4.2.
DEC for structured bandits. We consider the same structured bandit protocol as in Section 4.2;
recall that Hdenotes the reward function class and MHdenotes the induced model class. In what
follows, we simplify the results in Theorem 15 to be stated purely in terms of H. For a reference
value function sh:C × A → [0,1], we define
r-decc
ε(H,sh):= inf
p∈∆(Π)sup
h∈H
Eπ∼p
h(πh)−h(π)Eπ∼p(h(π)−sh(π))2≤ε2	
,
where we recall that πh:= max π∈Πh(π). We then define the DEC for Has
r-decc
ε(H) = sup
sh∈co(H)r-decc
ε(H ∪ {sh},sh).
As a corollary of Theorem G.2, the r-decc
ε(H)andlogDdim ∆(H)together provide an upper bound
for structured bandits with H.
Theorem H.1. LetHbe given. Suppose that Πis finite, and that ε7→r-decc
ε(co(H))satisfies mod-
erate decay as a function of ε(Assumption 3) with constant creg. Let ¯ε(T)≍p
logDdim ∆(H)/T.
The Algorithm 1 ensures that high probability,
RegDM≤T·∆ +O(cregT√logT)·r-decc
¯ε(T)(co(H)).
As a corollary, the minimax sample complexity of structured bandit learning with His bounded as
max
TDEC(H,∆),logDdim 2∆(H)	
≲T⋆(MH,∆)≲TDEC(co(H),∆)·logDdim ∆/2(H),(38)
where we denote TDEC(H,∆) = inf ε∈(0,1){ε−2:r-decc
ε(H)≤∆}(following (14)) and omit
logarithmic factors and dependence on the constant creg.
There are many standard structured bandit problems where the value function class His convex,
including multi-armed bandits, linear bandits, and non-parametric bandits (with smoothness [65], or
concavity [48], or sub-modularity [58], or etc.). For these problem classes, the complexity of no-
regret learning is completely characterized by the DEC of Hand the decision dimension Ddim ∆(H)
(up to a quadratic factor).
We also note that the lower bound of (38) is proven for Gaussian noise, while our upper bound
applies to a much more general class of reward distributions (with bounded variance).
34H.3 Application: Contextual bandits with general function approximation
Next, we instantiate our general results for stochastic contextual bandits with general function ap-
proximation, generalizing the structured bandit problem. We consider the stochastic contextual ban-
dit problem with context space C, action space A, and a reward function class H ⊆ (C×A → [0,1]).
This problem is a special case of the DMSO setting with decision space Π = (C → A ), and the en-
vironment is specified by a tuple (h⋆∈ H, ν⋆∈∆(C)). The protocol is as follows: For each round
t, the environment draws ct∼ν, and the learner takes action at=πt(ct)based on the decision
πt:C → A , and receives a reward rt∼N(h⋆(ct, at),1).
We can formulate the model class as follows. For a reward function h∈ H and context distribution
ν∈∆(C), the corresponding model Mh,νis specified as
(c, a, r )∼Mh,ν(π) : c∼ν, a=π(a), r∼N(h(c, a),1).
LetMH={Mh,ν:h∈ H, ν∈∆(C)}be the induced model class of contextual bandits. Following
Appendix H.2, we instantiate Theorem 15 to provide characterization of learning MH.
DEC for contextual bandits. For any context c∈ C, the value function class Hinduces a restricted
value function class H|c={h(c,·) :h∈ H} , which corresponds to a (non-contextual) bandit
function class. We define the following variant of the DEC
r-decc
ε(H):= sup
c∈Cr-decc
ε(H|c),
which corresponds to the maximum of the per-context DEC over all contexts. We also define
TDEC(H,∆) = inf ε∈(0,1){ε−2:r-decc
ε(H)≤∆}, following (14).
Decision dimension for contextual bandits. Specializing the decision dimension to contextual
bandits, we define
Ddim ∆(H):= inf
p∈∆(Π)sup
h∈H,ν∈∆(C)1
p(π:Ec∼ν[h(c, πh(c))−h(c, π(c))]≤∆), (39)
where πh∈Πis defined via πh(c):= arg maxa∈Ah(c, a)forc∈ C.
Intuitively, the value of the decision dimension logDdim ∆(H)for contextual bandits captures the
difficulty of estimating optimal actions, but also the difficulty of generalizing across contexts. For
example, when we consider the unstructured contextual bandit problems (i.e., H= (C × A →
[0,1])), it holds that logDdim ∆(H) =|C|log|A|, but in general we can have logDdim ∆(H)≪
log|Π|=|C|log|A|.
As a corollary of Theorem 15, we derive the following upper and lower bounds on the complexity
of contextual bandit learning with H.
Theorem H.2. LetHbe given. Suppose that both the context space Cand the action space A
are finite, and that ε7→r-decc
ε(co(H))satisfies moderate decay as a function of ε(Assumption
3) with constant creg. Let ¯ε(T)≍p
logDdim ∆(H)/T. Then Algorithm 1 ensures that with high
probability, ;
RegDM≤T·∆ +O(cregT√logT)·r-decc
¯ε(T)(co(H)).
As a corollary, the complexity of learning MHis bounded by
max
TDEC(H,∆),logDdim 2∆(H)
log|C|
≲T⋆(MH,∆)≲TDEC(co(H),∆)·logDdim ∆/2(H),
(40)
omitting dependence on cregand logarithmic factors.
By definition, we have r-decc
ε(co(H)) = r-decc
ε(H)if the per-context value function class H|c
is convex for every context c∈ C. Natural settings in which the per-context value function class
H|cis convex include contextual linear bandits [24], contextual non-parametric bandits [18], and
contextual concave bandits [48]. For these problem classes, the complexity of no-regret learning is
completely characterized by the DEC of Hand the newly proposed Ddim ∆(H)(up to a quadratic
factor and a factor of log|C|).
As a concrete example. we can derive upper bounds based on the decision dimension for finite-action
contextual bandits as follows.
35Corollary H.3. For any value function class H, Algorithm 1 ensures the following regret bound
with high probability.
RegDM(T)≤T·∆ +Op
T|A| ·logDdim ∆(H)
.
Compared to the well-known regret bound of O(p
T|A| ·log|H|)for learning any with any finite
contextual bandit class H[34, 69], this result above always provides a tighter upper bound, as
logDdim ∆(H)≤log|H|. For certain (very simple) function classes H, the quantity logDdim ∆(H)
can be much smaller than log|H|(for details, see Example 7). More importantly, logDdim ∆(H)
leads to lower bounds for anycontextual bandit function class (Theorem H.2). By contrast, lower
bounds for structured contextual bandits in prior work have been proven in a case-by-case fashion
(for specific value function classes H).
I Proofs from Section 4 and Appendix H
In this section, we mainly focus on no-regret learning, and we present the regret upper and lower
bounds in terms of DEC and logDdim ∆(M). The results can be generalized immediately to PAC
learning.
I.1 Proof of Theorem 10
Fix an arbitrary reference model ĎM∈(Π→∆(O))such that Assumption 2 holds. We remark that
ĎMis not necessarily in Morco(M).
We only need to prove the following fact.
Fact. IfT <logDdim ∆(M)−2
2CKL, then for any T-round algorithm ALG, there exists a model M∈ M
such that Risk DM(T)≥∆with probability at least1
2underPM,ALG.
Proof. By the definition (16) of Ddim ∆(M), we know
1
Ddim ∆(M):= sup
p∈∆(Π)inf
M∈Mp(π:gM(π)≤∆).
Therefore, we have
inf
M∈MpĎM,ALG(π:gM(π)≤∆)≤1
Ddim ∆(M),
and hence there exists M∈ M such that
T <log 
1/pĎM,ALG(π:gM(π)≤∆)
−2
2CKL.
Notice that by the chain rule of KL divergence, we have
DKL(PM,ALG∥PĎM,ALG) =EM,ALG"TX
t=1DKL(M(πt)∥ĎM(πt))#
≤TCKL.
Hence, using data-processing inequality,
DKL(pM,ALG∥pĎM,ALG)<log 
1/pĎM,ALG(π:gM(π)≤∆)
−2
2
≤DKL(1/2∥pĎM,ALG(π:gM(π)≤∆)).
This immediately implies pM,ALG(π:gM(π)≤∆)<1
2(by Theorem 1, or more directly, by
Lemma D.1).
Remark I.1. For simplicity, we present the above proof that does not go through our algorithmic
Fano’s inequality (Proposition 8). However, it is not difficult to see how Theorem 10 can be derived
from Proposition 8, as we have
inf
µ∈∆(M)sup
π∈Πµ(M:gM(π)≤∆) = sup
p∈∆(Π)inf
M∈Mp(π:gM(π)≤∆),
as long as the Minimax theorem can be applied (e.g. when Πis finite or Mis finite).
36I.2 Proof of Theorem 11
In this section, we present an algorithm based on reduction (Algorithm 2) that achieves the desired
upper bound. For the application to bandits with Gaussian rewards, we relax the assumption R:
O → [0,1]as follows.
Assumption 5. For any M∈ M andπ∈Π, the random variable R(o)is 1-sub-Gaussian under
o∼M(π).
Suppose that ∆>0is given, and fix a distribution p⋆
∆that attains the infimum of (16). Based on
p⋆
∆, we consider a reduced decision space Πsub⊂Π, generated as
Πsub={π1,···, πN}, π 1,···, πN∼p⋆
∆independently,
where we set N=Ddim ∆(M) log(1 /δ). Then the space Πsubis guaranteed to contain a near-
optimal decision, as follows.
Lemma I.2. With probability at least 1−δ, there exists π∈Πsubsuch that gM⋆(π)≤∆.
Therefore, we can then regard M⋆as aN-arm bandit instance with action space A= Π sub, and
for each pull of an arm π∈ A , the stochastic reward ris generated as r=R(o), o∼M⋆(π).
Then, we pick a standard bandit algorithm BanditALG , e.g. the UCB algorithm (see e.g. Lattimore
and Szepesvári [51]), and apply it to the multi-arm bandit instance M⋆
Bandit , and the guarantee of
BanditALG yields
TX
t=1max
π′∈ΠsubfM⋆(π′)−fM⋆(πt)≤Op
TNlog(T/δ)
.
with probability at least 1−δ. Therefore, we have
RegDM(T)≤T·(fM⋆(πM⋆)−max
π′∈ΠsubfM⋆(π′)) +Op
TNlog(T/δ)
≤T·∆ +Op
TNlog(T/δ)
,
with probability at least 1−2δ. This gives the desired upper bound, and we summarize the full
algorithm in Algorithm 2.
Proof of Lemma I.2. By definition,
P 
∀i∈[N], gM⋆(πi)>∆
≤p⋆
∆ 
π:gM⋆(π)>∆N
≤
1−1
Ddim ∆(M)N
≤exp
−N
Ddim ∆(M)
≤δ.
I.3 Proof of Theorem 14
We first state the following more general result, and Theorem 14 is then a direct corollary (under
Assumption 3).
Theorem I.3. With suitably chosen parameter γ > 0and prior q∈∆(Π) ,ExO+(Algorithm 1)
achieves with probability at least 1−δ:
1
TRegDM≤∆ +Cinf
γ>0
r-deco
γ/8(co(M)) +γlogDdim ∆(M) + log(1 /δ)
T
≤∆ +Cp
log(T)·r-decc
¯ε(T)(co(M)),
where Cis an absolute constant, ¯ε(T) =q
logDdim ∆(M)+log(1 /δ)
T, and the modified version of
constrained DEC is defined as
r-decc
ε(co(M)):=ε·sup
ε′∈[ε,1]r-decc
ε′(co(M))
ε′. (42)
37Algorithm 2 A reduction algorithm based on the decision dimension
Input: Problem (M,Π), parameter ∆, δ > 0,T≥1, Algorithm BanditALG for multi-arm bandits.
1:Set
p⋆
∆= arg inf
p∈∆(Π)sup
M∈M1
p(π:gM(π)≤∆). (41)
2:SetN=Ddim ∆(M) log(1 /δ)and sample the decision subspace Πsub={π1,···, πN} ⊂Π
as
π1,···, πN∼p⋆
∆independently.
3:Run the bandit algorithm BanditALG on the instance M⋆
Bandit forTrounds.
Proof of Theorem I.3. By the definition (16) of Ddim ∆(M), we know
1
Ddim ∆(M):= sup
p∈∆(Π)inf
M∈Mp(π:gM(π)≤∆).
Therefore, there exists q∈∆(Π) such that
inf
M∈Mq(π:gM(π)≤∆)≥1
Ddim ∆(M),
We then instantiate Algorithm 1 with such a prior q. Theorem I.3 follows immediately by combining
Theorem G.2 with the following structural result that relates offset DEC to constrained DEC.
Theorem I.4. Suppose that Assumption 4 holds for the model class M. Then for any ε∈(0,1], it
holds that
inf
γ>0 
r-deco
γ(M) +γε2
≤
3p
⌊log2(2/ε)⌋+ 2
·
r-decc
ε(M) +Lrε
.
Proof. Fix a ε∈(0,1]andĎM∈co(M). We only need to prove the following result:
Claim. Suppose that r-decc
ε′(M,ĎM)≤Dε′for all ε′∈[ε,1]. Then there exists γ=γ(D, ε)such
that
r-deco
γ(M) +γε2≤
3p
⌊log2(2/ε)⌋+ 2
·(D+Lr)ε.
SetK=⌊log2(1/ε)⌋+1and fix a parameter c=c(ε)∈(0,1
2]to be specified later in proof. Define
εi:= 2−ifori= 0,···, K−1andεK=ε. We also define λi:=cε·2ifori= 0,···, K−1, and
λK= 1−PK−1
i=0λi≥c.
Define ∆i=r-decc
εi(M ∪ {ĎM},ĎM), and let piattains the infp. In the following, we choose
γ=γ(D, ε) =9(D+Lr)
8cε.
By definition of pi, it holds that
Eπ∼pi[gM(π)]≤∆i,∀M∈ M ∪ {ĎM}:Eπ∼piD2
H 
M(π),ĎM(π)
≤ε2
i.
In particular, we may abbreviate Mi:={M∈ M :Eπ∼piD2
H 
M(π),ĎM(π)
≤ε2
i}, and it holds
fM(πM)≤fĎM(πĎM) + ∆ i+Lrεi,∀M∈ M i.
Next, we choose p=PK
i=0λipi∈∆(Π) , and we know
Eπ∼p[gĎM(π)]≤KX
i=0λiEπ∼p[gĎM(π)]≤KX
i=0λi∆i=: ∆.
38Fix a M∈ M . Let j∈ {0,···, K}be the maximum index such that M∈ M j. Such a jmust
exists because M=M0. Now,
Eπ∼p[gM(π)] =fM(πM)−fĎM(πĎM) +Eπ∼p[gĎM(π)] +Eπ∼p[fĎM(π)−fM(π)]
≤∆j+Lrεj+ ∆ + LrEπ∼pDH 
M(π),ĎM(π)
.
Case 1: j=K. Then, using AM-GM inequality, we have
Eπ∼p[gM(π)]−γEπ∼pD2
H 
M(π),ĎM(π)
≤∆K+εK+ ∆ +L2
r
4γ.
Case 2: j < K . Then for each i > j , it holds that Eπ∼pjD2
H 
M(π),ĎM(π)
> ε2
j, and hence
Eπ∼pD2
H 
M(π),ĎM(π)
≥KX
i=j+1λjEπ∼pjD2
H 
M(π),ĎM(π)
≥KX
i=j+1λjε2
j≥cε·εj
2.
Therefore, using AM-GM inequality,
Eπ∼p[gM(π)]−γEπ∼pD2
H 
M(π),ĎM(π)
≤∆j+Lrεj+ ∆ +9L2
r
4γ−8
9γEπ∼pD2
H 
M(π),ĎM(π)
≤∆j+Lrεj+ ∆ +9L2
r
4γ−8cγε
9εj.
By our choice of γ, we have γε≥9
8c
∆j
εj+Lr
, and hence in both cases, we have
Eπ∼p[gM(π)]−γEπ∼pD2
H 
M(π),ĎM(π)
≤∆ + ( D+Lr)ε+9L2
r
4γ.
Note that by definition, we have ∆≤(cK+ 1)Dεandγ(ε)·ε=9
8c(D+Lr), and hence
r-deco
γ(ε)(M,ĎM)≤(2D+Lr+cKD + 2cLr)ε.
Thus,
r-deco
γ(ε)(M,ĎM) +γ(ε)ε2≤
2D+Lr+cK(D+Lr) +9(D+Lr)
8c
εK.
Balancing cand re-arranging yields the desired result.
I.4 Proof of Theorem 15
Note that the minimax-optimal sample complexity T⋆(M,∆)is just a way to better illustrate our
minimax regret upper and lower bounds. By the definition of T⋆(M,∆), we have
1
TReg⋆
T= sup{∆ :T⋆(M,∆)≤T}.
Under Assumption 3, the regret upper bound in Theorem 14 implies (up to creg, CKLand logarithmic
factors)
1
TReg⋆
T≲r-decc
¯ε(T)(M).
And the regret lower bound Theorem E.1 implies (up to cregand logarithmic factors)
r-decc
ε(T)(M)≲1
TReg⋆
T.
By the definition of T⋆(M,∆)andTDEC(M,∆), we then have
TDEC(M,∆)≲T⋆(M,∆)≲TDEC(co(M),∆)·logDdim ∆/2(M).
Together with Theorem 10, we prove that
max
TDEC(M,∆),logDdim ∆(M)
CKL
≲T⋆(M,∆)≲TDEC(co(M),∆)·logDdim ∆/2(M).
39I.5 Proof of Theorem H.1
For the upper bound, we work with more general noise structure (beyond Gaussian noises). We
define MH,Vto be the class of all bandits models with mean reward function in Hand variance
bounded by 1. Specifically, for any M∈ M H,V, it is associated with a value function hM∈ H ,
such that for any decision π∈Π, the distribution M(π)of the random reward rhas mean hM(π)
and variance at most 1.
We also recall that the subclass MH⊆ M H,Vis the bandit problem class with the standard Gaussian
noise.
Proof of Theorem H.1: lower bound of (38). The lower bound with logDdim ∆(H)is exactly
Corollary 12.
To prove the lower bound with TDEC(H,∆), we need to lower bound the DEC of MHin terms of
the DEC of H, as follows.
Lemma I.5. Consider M+=Mco(H),Vas the class of all reference models (Appendix E). Then,
max
ĎM∈M+r-decc
ε(MH∪ {ĎM},ĎM)≥r-decc
2√
2ε(H). (43)
Notice that for M+, Assumption 4 holds with Lr=√
10(by Lemma C.5). Therefore, as a corollary
of Theorem E.3: for any T-round algorithm ALG, there exists M⋆∈ M Hsuch that
RegDM(T)≥T
2·(r-decc
ε(T)(H)−5ε(T))−1 (44)
with probability at least 0.01underPM⋆,ALG, where ε(T) =1
50√
T. Therefore, the lower bound in
terms of TDEC(H,∆)follows immediately (using regularity condition Assumption 3).
Combining both lower bounds completes the proof.
Proof of Theorem H.1: upper bound. We apply Theorem I.3 similar to the proof of Theorem 15
(in Appendix I.2).
Using Theorem I.3, we know that ExO+can be suitably instantiated on the model class MH,Vso
that with probability at least 1−δ,
1
TRegDM≤∆ +Cp
log(T)·r-decc
¯ε(T)(co(MH,V)),
where Cis an absolute constant, ¯ε(T) =q
logDdim ∆(H)+log(1 /δ)
T. We only need to upper bound the
r-decc
ε(co(MH,V))(defined in (42)) in terms of the DEC of co(H).
Lemma I.6. For any ε≥0, it holds that
r-decc
ε(MH,V)≤r-decc√
10ε(H)
We also note that co(MH,V)⊆ M co(H),Vbecause the model class Mco(H),Vis convex and it
contains MH,V. Therefore, we know
r-decc
ε(co(MH,V))≤r-decc
ε(Mco(H),V)≤r-decc√
10ε(co(H)).
Using the regularity of ε7→r-decc
ε(co(H)), we know
r-decc
¯ε(T)(co(MH,V))≤creg·r-decc√
10ε(co(H)).
This gives the desired upper bound.
I.5.1 Proof of Lemma I.5
Fix a ε∈[0,1], we denote ε1= 2√
2εand take any ∆<r-decc
ε1(H). We picksh∈co(H)such
thatr-decc
ε1(H,sh)>∆. Then, it holds that
inf
p∈∆(Π)sup
h∈H∪{sh}
Eπ∼p[h(πh)−h(a)]|Eπ∼p(h(a)−sh(a))2≤ε2
1	
≥∆.
40Suppose that sh∈co(H)is given bysh=Eh∼µ[h]withµ∈∆(H). Then, consider the reference
modelĎM∈ M+with mean reward function shand Gaussian noise, i.e. ĎM(π) =N sh(π),1
. Then,
we know that for M=MH,
r-decc
ε(M ∪ {ĎM},ĎM)
= inf
p∈∆(Π)sup
M∈M∪{ĎM}
Eπ∼p[gM(π)]|Eπ∼pD2
H 
M(π),ĎM(π)
≤ε2	
= inf
p∈∆(Π)sup
h∈H∪{sh}
Eπ∼p[h(πh)−h(π)]|Eπ∼pD2
H 
N(h(π),1),N sh(π),1
≤ε2	
≥inf
p∈∆(Π)sup
h∈H∪{sh}
Eπ∼p[h(πh)−h(π)]|Eπ∼p(h(π)−sh(π))2≤8ε2	
≥∆,
where the last line follows from Lemma C.5. Taking ∆→r-decc
ε1(H)completes the proof of
(43).
I.5.2 Proof of Lemma I.6
Fix a reference model ĎM∈co(MH,V). By definition, we know the mean reward function hĎMofĎM
belongs to co(H), i.e.ĎM∈ M co(H),V. Therefore, for any model M∈ M H,Vand decision π∈Π,
by Lemma C.5,
D2
H 
M(π),ĎM(π)
≥1
10|hM(π)−hĎM(π)|2.
Therefore, for M=MH,V,
r-decc
ε(M ∪ {ĎM},ĎM)
= inf
p∈∆(Π)sup
M∈M∪{ĎM}
Eπ∼p[gM(π)]|Eπ∼pD2
H 
M(π),ĎM(π)
≤ε2	
≥inf
p∈∆(Π)sup
M∈M∪{ĎM}
Eπ∼p[gM(π)]|Eπ∼p|hM(π)−hĎM(π)|2≤10ε2	
= inf
p∈∆(Π)sup
h∈H∪{sh}
Eπ∼p[h(πh)−h(π)]|Eπ∼p(h(π)−sh(π))2≤8ε2	
=r-decc√
10ε(H ∪ {sh},sh),
where the second equality follows from the fact that when =h, we have gM(π) =h(πh)−h(π).
Taking supremum over ĎMcompletes the proof.
I.6 Proof of Theorem H.2
Similar to Appendix I.5, we consider a larger model class MH,Vof models with general noise
structure. A model M∈ M H,Vis specified by a context distribution νM∈∆(C), a reward function
hM∈ H , and a reward distribution RM(·|·,·), such that for any c∈ C, a∈ A,r∼RM(·|c, a)has
mean hM(c, a)and variance at most 1. The model Mis then given by
(c, a, r )∼M(π) : c∼νM, a=π(c), r∼RM(·|c, a).
The model class MH,Vis defined to be the set of all possible models described above.
Proof of Theorem H.2: lower bound. The lower bound with logDdim ∆(H)follows immediately
by applying Theorem 10 to the class MH, which admits CKL=O(log|C|)in Assumption 2 (as
shown in Example 6).
On the other hand, the lower bound with TDEC(H,∆)follows from the reduction to the per-context
bandits problem. Specifically, for a fixed context c∈ C,H|ccorresponds to a structure bandits class
MH|c. Notice that we can naturally regard MH|c⊂ M Hby viewing MH|cas a contextual bandits
class with the fixed context c. Therefore, by Theorem H.1 (specifically (44)):
1
TReg⋆
T≥1
TReg⋆
T≳r-decc
ε(T)(H|c)−6ε(T), ε (T) =1
50√
T.
Taking maximum over c∈ Cyields
1
TReg⋆
T≳r-decc
ε(T)(H)−6ε(T).
This gives the desired lower bound with TDEC(H,∆).
Combining both lower bounds completes the proof.
41Proof of Theorem H.2: upper bound. We follow the proof strategy of Appendix I.5. By Theo-
rem I.3, ExO+can be suitably instantiated on the problem class MH,Vso that with probability at
least1−δ:
1
TRegDM≤∆ +Cinf
γ>0
r-deco
γ/8(co(MH,V)) +γlogDdim ∆(M) + log(1 /δ)
T
.
We also note that co(MH,V)⊆ M co(H),V. Therefore, it remains to upper bound the offset DEC of
Mco(H),V.
Lemma I.7. Forγ >0, it holds that
r-deco
γ(MH,V)≤sup
c∈Cr-deco
γ/2(MH|c,V).
Then, we can apply the result of Theorem I.4. From the proof of Theorem I.4, it is not hard to see
that: for any ε >0, there exists γ=γ(ε)such that for any c∈ C,
r-deco
γ/2(MH|c,V) +γε2≲p
log(2 /ε)·(creg·r-decc
ε(co(H)) +ε),
where we also use the regularity condition of ε7→r-decc
ε(co(H)). This immediately gives
RegDM≤T∆ +O(cregTp
logT)·r-decc
¯ε(T)(co(H)),
where ¯ε(T) =q
logDdim ∆(H)+log(1 /δ)
T. This is the desired upper bound.
I.6.1 Proof of Lemma I.7
Fix a reference model ĎM∈co(MH,V), and thenĎM∈ M co(H),Vby definition. In particular, ĎM
has mean value function hĎM∈ H and context distribution ¯ν∈∆(C). We also know that for each
c∈ C,hĎM(x,·)∈co(H|c).
Then, by Lemma C.4, we also have
2D2
H 
M(π),ĎM(π)
≥Ec∼νM,a=π(c)D2
H 
RM(r=·|c, a),RĎM(r=·|c, a)
.
Thus, we adopt the following notations: For each c∈ C and model M∈ M H,V, we define Mc∈
MH|c,Vto be a bandit model such that for every action a∈ A,Mc(a) =RM(r=·|c, a). Then by
definition, it holds that
2D2
H 
M(π),ĎM(π)
≥Ec∼νM,a=π(c)D2
H 
Mc(a),ĎMc(a)
.
Now, combining the inequalities above, we have
r-deco
γ(MH,V,ĎM)
= inf
p∈∆(Π)sup
M∈MH,VEπ∼p[gM(π)]−γEπ∼pD2
H 
M(π),ĎM(π)
≤inf
p∈∆(Π)sup
M∈MH,VEπ∼pEc∼νM,a=π(c)h
hM(c, πM(c))−hM(c, a)−γ
2D2
H 
Mc(a),ĎMc(a)i
(1)= inf
p=(pc),pc∈∆(A)sup
M∈MH,VEc∼νM,a∼pch
hM(c, πM(c))−hM(c, a)−γ
2D2
H 
Mc(a),ĎMc(a)i
(2)
≤ inf
p=(pc),pc∈∆(A)sup
M∈MH,Vsup
c∈CEa∼pch
hM(c, πM(c))−hM(c, a)−γ
2D2
H 
Mc(a),ĎMc(a)i
(3)= inf
p=(pc),pc∈∆(A)sup
c∈Csup
Mc∈MH|c,VEa∼pch
hMc(πMc)−hMc(a)−γ
2D2
H 
Mc(a),ĎMc(a)i
(4)= sup
c∈Cinf
pc∈∆(A)sup
Mc∈MH|c,VEa∼pch
hMc(πMc)−hMc(a)−γ
2D2
H 
Mc(a),ĎMc(a)i
= sup
c∈Cr-deco
γ/2(MH|c,V,ĎMc)≤sup
c∈Cr-deco
γ/2(MH|c,V),
where the equality (1) is because for a sequence (pc∈∆(A))c∈C, there is a corresponding p∈∆(Π)
such that for π∼p, we have π(c)∼pcindependently; in inequality (2) we bound the expectation
42overc∼νMby the supremum supc∈C; the equality (3) follows from the fact that Mc∈ M H|c,V
is a bandit model with mean reward function hMc(·) =hM(c,·); and the equality (4) is because we
can choose pcseparately for every c∈ C. By the arbitrariness of ĎM∈co(M), we now have
r-deco
γ(MH,V)≤sup
c∈Cdeco
γ/2(MH|c,V).
I.7 Proof of Corollary H.3
We follow the notations of Appendix I.6. By Lemma I.7, we have
r-deco
γ(MH,V)≤1
γ+ sup
c∈Cr-deco
γ/4(MH|c,V).
Notice that for each c∈ C,MH|c,Vis a class of |A|-arm bandits, and hence by Foster et al. [36,
Proposition 5.1] and Lemma C.5, we have
r-deco
γ(MH|c,V)≤8|A|
γ.
Therefore, Theorem I.3 implies that ExO+achieves with probability at least 1−δ:
1
TRegDM≤∆ +O|A|
γ+γlogDdim ∆(H) + log(1 /δ)
T
.
Balancing γ >0gives the desired upper bound.
As a remark, we provide an example of function class HwithlogDdim ∆(H)≪log|H|.
Example 7. Suppose that A={0,1}, and the function class H={hx}x∈C, where
hx(c,0) =1
2, h x(c,1) =1, c=x,
0, c̸=x..
Clearly, we have log|H|= log|C|.
On the other hand, we consider a distribution pover policies, such that π∼pis generated as
π(c)∼Bern( ε), independently over all c∼ C. Then, for any h=hx∈ H andν∈∆(C), we have
Ec∼ν[h(c, πh(c))−h(c, π(c))] = ν(x)·1
21{π(x) = 1}+1
2Ec∼ν[1{c̸=x, π(c) = 1}].
Notice that π(x) = 1 with probability ∆, and conditional on the event {π(x) = 1},
Eπ∼p[Ec∼ν[1{c̸=x, π(c) = 1}]|π(x) = 1] ≤∆.
Hence,
p(π:Ec∼ν[h(c, πh(c))−h(c, π(c))]≤∆)≥∆
2,
which implies logDdim ∆(H)≤log(2 /∆).
Therefore, for unbounded context space C, we have logDdim ∆(H)≪log|H|for the function class
Hdefined above.
43NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not re-
move the checklist: The papers not including the checklist will be desk rejected. The checklist
should follow the references and follow the (optional) supplemental material. The checklist does
NOT count towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
• [NA] means either that the question is Not Applicable for that particular paper or the relevant
information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evalu-
ation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No]
" provided a proper justification is given (e.g., "error bars are not reported because it would be too
computationally expensive" or "we were unable to find the license for the dataset we used"). In
general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased
in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your
best judgment and write a justification to elaborate. All supporting evidence can appear either in the
main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question,
in the justification please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist" ,
•Keep the checklist subsection headings, questions/answers and guidelines below.
•Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes] .
Justification: The main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope. The claims are validated by detailed proofs.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims made
in the paper.
• The abstract and/or introduction should clearly state the claims made, including the con-
tributions made in the paper and important assumptions and limitations. A No or NA
answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
44Answer: [Yes] .
Justification: The paper discusses the limitations of the work performed.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to vi-
olations of these assumptions (e.g., independence assumptions, noiseless settings, model
well-specification, asymptotic approximations only holding locally). The authors should
reflect on how these assumptions might be violated in practice and what the implications
would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was only
tested on a few datasets or with a few runs. In general, empirical results often depend on
implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be used
reliably to provide closed captions for online lectures because it fails to handle technical
jargon.
• The authors should discuss the computational efficiency of the proposed algorithms and
how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an important
role in developing norms that preserve the integrity of the community. Reviewers will be
specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes] .
Justification: The paper provides detailed assumptions and proofs.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if they
appear in the supplemental material, the authors are encouraged to provide a short proof
sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
45Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA] .
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived well
by the reviewers: Making the paper reproducible is important, regardless of whether the
code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct the
dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case authors
are welcome to describe the particular way they provide for reproducibility. In the
case of closed-source models, it may be that access to the model is limited in some
way (e.g., to registered users), but it should be possible for other researchers to have
some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
46including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized ver-
sions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA] .
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [NA] .
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main
claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall run
with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula, call
to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error of
the mean.
47• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or fig-
ures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA] .
Justification: This paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or
cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute than
the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t
make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes] .
Justification: The research conducted in the paper conforms, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consider-
ation due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is a theoretical work. There is no societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
48• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g.,
deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied to
particular applications, let alone deployments. However, if there is a direct path to any
negative applications, the authors should point it out. For example, it is legitimate to point
out that an improvement in the quality of generative models could be used to generate
deepfakes for disinformation. On the other hand, it is not needed to point out that a
generic algorithm for optimizing neural networks could enable people to train models
that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is being
used as intended and functioning correctly, harms that could arise when the technology is
being used as intended but gives incorrect results, and harms following from (intentional
or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA] .
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do not
require this, but we encourage authors to take this into account and make a best faith
effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA] .
Justification: This paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
49• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has curated
licenses for some datasets. Their licensing guide can help determine the license of a
dataset.
• For existing datasets that are re-packaged, both the original license and the license of the
derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to the
asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA] .
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license, limi-
tations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA] .
Justification: This paper does not involve crowdsourcing or research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA] .
Justification: This paper does not involve crowdsourcing or research with human subjects.
50Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
51