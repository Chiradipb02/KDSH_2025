Visual Text Matters: Improving Text-KVQA with Visual Text Entity
Knowledge-aware Large Multimodal Assistant
Abhirama Subramanyam Penamakuri and Anand Mishra
Indian Institute of Technology Jodhpur
{penamakuri.1,mishra}@iitj.ac.in
https://vl2g.github.io/projects/LMM4Text-KVQA/
Abstract
We revisit knowledge-aware text-based visual
question answering, also known as Text- KVQA
in the light of modern advancements in large
multimodal models ( LMM s), and make the fol-
lowing contributions: (i) We propose VisTEL
– a principled approach to perform visual text
entity linking. The proposed VisTEL module
harnesses a state-of-the-art visual text recogni-
tion engine and the power of a large multimodal
model to jointly reason using textual and visual
context obtained using surrounding cues in the
image to link the visual text entity to the correct
knowledge base entity. (ii) We present KaLMA
– knowledge-aware large multimodal assistant
that augments an LMM with knowledge associ-
ated with visual text entity in the image to arrive
at an accurate answer. Further, we provide a
comprehensive experimental analysis and com-
parison of our approach with traditional visual
question answering, pre-large multimodal mod-
els, and large multimodal models, as well as
prior top-performing approaches. Averaging
over three splits of Text- KVQA , our proposed
approach surpasses the previous best approach
by a substantial 23.3% on an absolute scale and
establishes a new state of the art. We make our
implementation publicly available.
1 Introduction
In the past few years, the research community has
shown significant interest in visual question an-
swering based on text appearing in images, as ev-
idenced by the emergence of OCR-VQA (Mishra
et al., 2019), ST-VQA (Biten et al., 2019b) and
TextVQA (Singh et al., 2019b). Giving another
aspect to these problems by leveraging external
knowledge for text-based visual question answer-
ing, (Singh et al., 2019a) introduced a task called
Text- KVQA . The Text- KVQA presents a unique
challenge: given an image containing textual en-
tities like business brands, book titles, or movie
titles, the task is to answer questions that requireexternal knowledge about these entities. Address-
ingText- KVQA involves detecting text in images,
recognizing it, linking it to a knowledge base, and
employing visual context and knowledge base for
reasoning to provide an answer. Since the introduc-
tion of this problem, several advancements have
happened in visual text understanding as well as vi-
sion and language models. In this work, we revisit
Text- KVQA by leveraging these modern advance-
ments and propose a framework that judiciously
integrates various components of contemporary ar-
chitecture.
The emergence of large multimodal models
(LMM s)1represents a significant trend in the lit-
erature on vision and language (Zhang et al., 2022;
Chung et al., 2022; Touvron et al., 2023; Liu et al.,
2024; Zhu et al., 2023; Ye et al., 2023; Penedo
et al., 2024; Ouyang et al., 2022). Over the past
few years, many large-scale language and vision
models have been developed, demonstrating excep-
tional performance across various tasks including,
but not limited to, image captioning, visual ques-
tion answering, multimodal reasoning, and visual
grounding. We believe that pretrained LMMs hold
great potential for addressing Text- KVQA . These
models are rich in the implicit knowledge learned
by large-scale pretraining. However, despite their
numerous advantages, they are not without draw-
backs, notably hallucinations. This challenge be-
comes particularly apparent in Text- KVQA , where
precise reasoning about entities depicted in images
and associated knowledge is required. Consider the
following scenario where a customer, after finish-
ing their meal at a restaurant store, takes a picture
of the store signboard and enquires about a pos-
sible future online delivery, asking, ‘Where can
I place an online order from this store?’ (Fig-
ure 1(a)). Existing LMM s often hallucinate over
1We refer to both large multimodal model and large vision
and language models as LMM in this work.arXiv:2410.19144v1  [cs.CV]  24 Oct 2024Figure 1: (a) Text- KVQA (Singh et al., 2019a): Given an image containing a named entity as visual text, e.g.,
“Domino’s" in this illustration, the aim is to answer the question by leveraging explicit knowledge about the visual
text entity. (b) Large Multimodal Models are one obvious choice for solving such tasks today. However, they alone
are insufficient as they hallucinate on visual objects. (c) We propose a novel approach – KaLMA that augments an
LMM with specialized visual text recognition and retrieved relevant knowledge obtained using visual text entity
linking by proposed VisTEL. Our approach establishes a new state-of-the-art for this task.
the pizza present in the image and points to the
website of ‘Pizza Hut’ instead of ‘Domino’s’ (Fig-
ure 1(b)); whereas complementing the LMM with
an explicit visual text entity linking followed by
knowledge-retrieval helps overcome hallucination
(Figure 1(c)), thereby generating an accurate an-
swer to the given question. Our model is developed
on this hypothesis.
We address Text- KVQA by introducing an ar-
chitecture, namely KaLMA – knowledge-aware
large multimodal assistant that first invokes our
proposed visual text entity linker or VisTEL – an
LMM -architecture that links visual text entities to
the associated knowledge base (Illustrated in Fig-
ure 1 (c)). Once the entities are linked to the knowl-
edge base, the associated knowledge is retrieved
and augmented to a large multimodal model to an-
swer visual questions.
To summarize, our contributions are as follows:
(i) We revisit Text- KVQA – a task originally intro-
duced by (Singh et al., 2019a) in the light of the
latest advancements in large multimodal models.
To this end, we benchmark latest LMM s on Text-
KVQA . Our study highlights that LMM s although
powerful, often ignore visual text present in the
images, resulting in hallucinations.
(ii) We propose a principled approach called
VisTEL for linking visual text entities that ap-
pear in images to a knowledge base. VisTEL is
anLMM -based architecture that leverages the sur-rounding OCR-extracted texts obtained using a
specialized text recognition module and the visual
context within the image to perform highly accu-
rate entity linking for visual text entities. (iii) We
introduce KaLMA – a Knowledge-aware Large
Multimodal Assistant, which enhances a large-
multimodal model, specifically LLaV A (Liu et al.,
2024) by integrating retrieved knowledge from our
proposed VisTEL. This augmentation facilitates
robust vision and language reasoning, thereby en-
abling superior knowledge-aware text-based visual
question answering. (iv) We conduct extensive
experiments and ablation to show the superior per-
formance of our proposed framework over compet-
itive approaches and state of the art. We provide
several exciting insights about our design choice,
attribution ability of KaLMA, and addressing hal-
lucination issues of LMM s. Our proposed approach
advances state of the art on Text- KVQA by 18.2%
on scene, 19.6% on book covers, and 32.2% on
movie poster splits of the dataset on an absolute
scale.
2 Related Work
KVQA Tasks: Visual Question Answering is a
well-studied task (Antol et al., 2015; Goyal et al.,
2017). This task has been extended to scenarios
that require the ability to read text within images,
leading to the development benchmarks such asST-VQA (Biten et al., 2019b,a), TextVQA (Singh
et al., 2019b), DocVQA (Mathew et al., 2021), and
OCR-VQA (Mishra et al., 2019). While these bench-
marks were successful in their intent of integrating
reading and reasoning abilities in VQA, they are of-
ten restricted to reasoning around what is visually
apparent. To address this gap and encourage mod-
els to perform reasoning beyond visually apparent
facts, (Singh et al., 2019a) introduced knowledge-
aware Text-based VQA task. Distinctively differ-
ent from other knowledge-aware visual question
answering tasks such as KB-VQA (Wang et al.,
2017b), FVQA (Wang et al., 2017a), KVQA (Shah
et al., 2019), OK-VQA (Marino et al., 2019), and In-
foseek (Chen et al., 2023), Text- KVQA deals with
reasoning over visual text entities and associated
knowledge to arrive at answer.
Methods Prior to Large Multimodal Models:
Early methods to solve knowledge-aware VQA
tasks focus on leveraging knowledge in the form of
triplets (Narasimhan et al., 2018; Narasimhan and
Schwing, 2018; Wu et al., 2016), or sub-knowledge-
graph (Zhang et al., 2018; Singh et al., 2019a) or
memory facts (Weston et al., 2015). Later, trans-
former architectures (Vaswani et al., 2017) owing
to their ability to encode intrinsic knowledge using
large-scale pretraining, have become defacto for
addressing KVQA .
Inspired by the hybrid models, e.g. (Lewis et al.,
2020; Guu et al., 2020) where intrinsic knowledge
of transformer architectures is complemented with
explicit external knowledge; researchers proposed
hybrid methods such as Concept BERT (Gardères
et al., 2020), KRISP (Marino et al., 2021), and RE-
VEAL (Hu et al., 2023b) which augment the multi-
modal transformers with explicitly retrieved exter-
nal knowledge.
Emergence of Large Multimodal Models: The
early success of large-scale pretraining on the
downstream tasks demonstrated by the founda-
tion models, e.g., BERT (Devlin et al., 2019) and
GPT (Radford et al., 2019) paved the way for the
researchers to scale the model and the data used
for pretraining. GPT-3(Brown et al., 2020) is an
early large language model ( LLM) demonstrating
reliable performance on many downstream tasks.
Following this, several LLM variants (Zhang et al.,
2022; Chung et al., 2022; Workshop et al., 2022;
Penedo et al., 2024; Touvron et al., 2023) have been
introduced. Researchers adopted these LLMs to
vision-language research, with the key idea being
aligning the visual information with the linguisticinformation of the LLMs to come up with large
multimodal models ( LMM s) (Tsimpoukelli et al.,
2021; Penedo et al., 2024; Li et al., 2023; Zhu et al.,
2023; Ye et al., 2023; Liu et al., 2024). Recently,
LMM s have become first-hand solutions for many
downstream vision-language tasks, making them
an obvious choice to solve Text- KVQA . Authors
in (Yang et al., 2022; Khademi et al., 2023) prompt
the LLMs with visual information via dense cap-
tions, object tags, object-level bounding box coor-
dinates, and OCR tags. These methods rely heavily
on the implicit knowledge learned by these LLMs.
Further, KAT (Gui et al., 2022) improves upon such
methods by augmenting external knowledge via
retriever before prompting the LLM. However, it
ignores the explicit visual information, which RE-
VIVE (Lin et al., 2022) aims to fix. Although these
methods show significant success, they have lim-
itations such as hallucination and ignoring visual
texts for reasoning. We aim to fill these gaps by
proposing a novel solution for Text- KVQA .
Visual Entity Linking: Entity linking has tradi-
tionally been a well-established focus area within
the NLP community (Jurafsky and Martin, 2009).
In contrast, the problem of visual entity linking has
only garnered attention in the last decade (Hu et al.,
2023a; Sun et al., 2022; Shah et al., 2019). (Sun
et al., 2022) have proposed a novel dataset and
benchmark for visual named entity linking. (Shah
et al., 2019) drew attention to the need for visual
entity linking for addressing knowledge-based vi-
sual question answering. Open-domain Visual En-
tity Recognition has also been studied in the liter-
ature (Hu et al., 2023a; Caron et al., 2024; Xiao
et al., 2024). However, most of these works have
focused on linking entities such as persons, land-
marks, and other named entities, while neglecting
visual text such as business brand names and movie
or book titles. In this work, we address this gap by
proposing a principled solution for visual text entity
linking and demonstrate its utility as a precursor to
Text- KVQA .
3 Methodology
Problem Statement: Text- KVQA (Singh et al.,
2019a) is a knowledge-intensive visual question-
answering task that requires a system to read and
interpret the visual text in an image and leverage
it as a gateway to access and reason over exter-
nal knowledge to answer the question. The exter-
nal knowledge base Kconsists of a set of nenti-(a) 
(c) 
(b) Figure 2: Challenges associated with Visual Text En-
tity Linking: (a) Visual text entity may appear as abbre-
viation instead of the entity name directly, e.g. “RBS"
instead of “The Royal Bank of Scotland", (b) Visual
text with varying font and stylized orientation pose a
challenge to the recognizer, (c) Example of homonyms
where visual text HPmay refer to ‘Hewlett Packard’
(left) or ‘Hindustan Petroleum’ (right).
tiesE={E1, E2, ..., E n}and their corresponding
knowledge K={K1, K2, ..., K n}, where each Ki
is a set of facts. For example, Domino’s Pizza is an
entity whose associated knowledge facts, obtained
in the form of triplets from Wikidata, are concate-
nated to form simple sentences such as “Domino’s
Pizza is a restaurant”, “Its headquarters are in
Ann Arbor Charter Township”, “It belongs to the
fast food industry”, and so on . In this section, we
describe our approach, whose overall architecture
is illustrated in Figure 4. Our approach first links
visual text entities using the proposed VisTEL mod-
ule and retrieves relevant knowledge to the entity
(Section 3.1), it then reasons over the image and
the retrieved knowledge to answer the question
(Section 3.2).
3.1 VisTEL: Vis ual T ext E ntity L inker
Entity linking is a well-studied task (Jurafsky and
Martin, 2009), where given a sentence, the named
entities need to be identified and linked with their
corresponding entities in a knowledge base. In
this work, we study an analogous task, where the
input is no longer a sentence, but instead an image
containing visual text entities and the task is to link
them to a corresponding external knowledge base.
One plausible solution, as shown in (Singh et al.,
2019a), is to extract the visual text in these images
using visual text recognition engines and then lever-
Figure 3: Illustration of VisTEL. We extract visual
text from the given image using visual text recognition
engine and, based on textual similarity, obtain kcandi-
date entities from the knowledge base. We fit OCRed
text and the candidate entities into an instruction prompt
template and encode the image using a visual encoder
and the text prompt using an LMM embedding module
to obtain XIandXT, respectively. Once encoded, LMM
generates the entity associated with the visual text in the
image. Please refer to the Section 3.1.
age distance-based text similarity methods between
the recognized text and the candidate entities for
the entity linking task. However, such methods
are highly sensitive to the following challenges: (i)
Noisy or imperfect OCR may lead to wrong entity
linking, and (ii) visual text might contain abbrevia-
tions instead of the entity names, e.g. “RBS’ ’ for
the entity “The Royal Bank of Scotland” , (iii) The
problem of homonymy, e.g. visual text HPmay re-
fer to ‘Hindustan Petroleum’ or‘Hewlett Packard’ .
Furthermore, unlike entity linking which often ben-
efits from larger textual contexts; visual text entity
linking has limited textual context, e.g., surround-
ing visual texts, and often must infer correct entities
based on visual context. Please refer to Figure 2
for a selection of challenges associated with visual
text entity linking. The other plausible solution is
to use large multimodal models ( LMM s). By virtue
of large-scale pretraining, they have strong abilities
to reason and infer correct entities based on visual
cues. However, we observe that feeding only the
image without the surrounding OCRed text often
results in hallucinations. To address these short-
comings, we propose VisualTextEntity Linker
(VisTEL) that links the visual text present in an
input image to its corresponding entity by jointlyFigure 4: Overview of our proposed framework KaLMA . We first link the visual text in the image Ito the entity
EIusing VisTEL (Section 3.1) and its associated knowledge KIis fetched. Then, we frame an instruction prompt
with the question Qand the knowledge KI, and encode it using the lmm embedding module fto obtain textual
features XTQ:KI. We encode the image Iusing a vision encoder to obtain visual features XI. Then, we concatenate
XIandXTQ:KIand feed them to the LMM to generate an accurate answer Ato the question Q. Instruction prompt
templates used in our ablation study are shown in the bottom right box, where Tocr
Iis the visual text of the image I.
reasoning on textual context obtained using an ex-
plicit specialized visual text recognition engine and
visual context obtained using a vision encoder of
a large multimodal model. The architecture for
VisTEL is illustrated in Figure 3.
Visual Text Recognition Engine: Given an im-
ageI, we extract text Tocr
I={tocr
1, tocr
2, ..., tocr
r}I
using specialized visual text detection and recog-
nition methods. We, then find a set of kcandidate
entities EIbased on the normalized edit-distance
(NED) score between the entity name in the knowl-
edge base with Tocr
I. We use state-of-the-art text
detection and text recognition approaches, namely
DBNET (Liao et al., 2020) and ParSeq (Bautista and
Atienza, 2022), respectively.
Vision encoder: We use the output of the last trans-
former layer of a pretrained CLIP visual encoder
ViT-L/14 (Radford et al., 2021) as our patched im-
age features ˜XI∈Rp×dv, where panddvare
the number of patches and encoding dimension of
ViT, respectively. Further, these image features are
projected to dlmm dimension using a linear layer
gto obtain the final sequence of image features
XI∈Rp×dlmm, i.e., XI=g(˜XI).
Large Multimodal Model: Once we obtain the
OCR-ed text Tocr
Iand candidate entities EI, weframe the following instruction prompt:
Instruction prompt template for VisTEL
<image>
USER:Given an image. The task is to link the visual
text {Tocr
I} to one of the following entities: { EI}
ASSISTANT:{ EI}
Then, we feed the prompt to the embedding mod-
ulehof the LMM to obtain text tokens XTocr
I:EI∈
Rl×dlmmi.e.,XTocr
I:EI=h(prompt (Tocr
I:EI)),
where landdlmm are the number of text tokens and
input embedding dimension for the LMM , respec-
tively. We, then concatenate image features XIand
text features XTocr
I:EI, and feed it as an input to the
large multimodal model. VisTEL auto-regressively
predicts the probability of the next token EItin the
target entity EIby attending to the input prompt
tokens and the previously generated entity tokens
EI<t. We train VisTEL by optimizing the language
modeling loss for generating the target entity con-
ditioned on the inputs XIandXTocr
I:EI.
3.2 KaLMA: K nowledge-a ware L arge
Multimodal A ssistant
We present Knowledge-aware Large Multimodal
Assistant (KaLMA) for addressing Text- KVQA .The KaLMA is an effective architecture that seam-
lessly integrates questions and images in the con-
text of external knowledge in a trainable architec-
ture to generate accurate answers.
We use visual features XIfrom the vision en-
coder. Further, we concatenate question Qand
the knowledge KIvia instruction prompt tem-
plate (as shown in the Figure 4) and feed to
the embedding module fof the LMM to obtain
text tokens XTQ:KI∈Rm×dlmmi.e.,XTQ:KI=
f(prompt (Q:KI)), where m is the number of
text tokens. Then, we concatenate image features
XI, and text features XTQ:KIand feed to the large
multimodal model to generate the accurate answer
A. Further, to bring attribution ability, we model
KaLMA to generate the supporting fact Sthat con-
tributed to the answer along with answer genera-
tion. From here onwards, we will refer answer and
supporting fact together as A. KaLMA predicts
the probability of the next token Aatin the answer
Aain an auto-regressive manner. It does so by
attending to the prompt inputs and the previously
generated tokens Aa<t. We train by minimizing
the generative language modeling loss Lans_gen(θ),
which aims to generate the target tokens based on
the inputs XIandXTQ:KI(Eq. 1). Note that target
tokens comprise both the answer and the support-
ing fact. During training, we leverage the ground
truth entity and its corresponding knowledge KI,
while during inference, we obtain it using our Vis-
TEL module. We reuse the weights of VisTEL to
initialise KaLMA.
Lans_gen(θ) =−
|A|X
t=1log(Pθ(Aat|Aa<t, XI, XTQ:KI))
,(1)
where θare the trainable parameters, Aa<trep-
resents the answer tokens already generated before
predicting the token Aatat the current time step t.
4 Experiments and Results
4.1 Dataset, Metrics and Comparisons
We conduct our experiments on Text- KVQA (Singh
et al., 2019a) dataset2. The questions in this dataset
span across three splits, namely, scene, book,
and movie containing natural scene images, book
covers, and movie posters, respectively. These
splits have (50K questions, 10K images, 500 en-
tities), (1M questions, 207K images, 207K enti-
ties), (222K questions, 34K images, 34K entities),
2Available at: https://textkvqa.github.ioAccuracy on Text- KVQA
Method scene book movie
Traditional VQA Baselines
BiLSTM 17.0 12.4 11.3
BoW +CNN 11.5 8.7 7.0
BLSTM +CNN (Antol et al., 2015) 19.8 17.3 15.7
HiCoAttenVQA (Lu et al., 2016) 22.2 20.4 18.4
BAN (Kim et al., 2018) 23.5 22.3 20.3
Pre-LLM Approaches
GPT-2 (Radford et al., 2019) 22.8 22.3 31.8
GPT-2 (w/ Visual Context) 25.4 43.2 38.5
ViLT (Kim et al., 2021) 38.2 31.1 40.1
VLBart (Cho et al., 2021) 35.1 38.6 41.5
Previous SOTA
Memory Network (Weston et al., 2015) 49.0 57.2 42.0
Singh et al. (Singh et al., 2019a) 54.5 62.7 45.2
LLM-based Approaches
mPlug-Owl (Ye et al., 2023) 21.3 26.7 8.2
LLaV A-1.5 (Liu et al., 2024) 39.2 37.0 46.1
MiniGPT4v2 (Zhu et al., 2023) 48.2 47.7 47.6
InstructBLIP (Dai et al., 2024) 31.5 30.3 29.9
Ours (KaLMA)
w/ NED retrieval 54.9 63.4 70.8
w/ VisTEL 72.7 (↑18.2%) 82.3 (↑19.6%) 77.4 (↑32.2%)
Oracle 99.3 92.8 99.4
Table 1: Results on Text- KVQA :Various methods on
the three data categories of Text- KVQA dataset, namely,
scene, book and movie.
respectively. Further, each of these splits comes
with its own knowledge base, namely KB-business
containing knowledge facts about business brand
entities harvested from Wikidata, KB-book contain-
ing knowledge facts about books harvested from
a book catalog, and KB-movie containing knowl-
edge facts about movies harvested from IMDB,
respectively. For each split, we follow the similar
train-test division as (Singh et al., 2019a) where en-
tities in train and test sets are disjoint. We evaluate
the methods using an accuracy metric.
Along with traditional VQA baselines, we com-
pare the question answering performance of our
proposed approach KaLMA with methods from
the following three major categories: (i) Pre-
LMM Approaches : here, we choose classical
transformer-based baselines, namely, GPT-2 (Rad-
ford et al., 2019) (text-only), GPT-2 (with BLIP-
2 (Li et al., 2023)-extracted captions as visual con-
text), ViLT (Kim et al., 2021) and VLBart (Cho
et al., 2021). For an encoder-only model like
ViLT, we treat Text- KVQA as a classification-style
visual question answering where the task is to
predict the answer from a set of all possible an-
swers. (ii) LMM-based Approaches: restrict-
ing ourselves to open-source models, we choose
four popular LMM s, namely, mPlug-Owl (Ye et al.,
2023), MiniGPT4v2 (Zhu et al., 2023), LLaV A-1.5
(7B) (Liu et al., 2024) and InstructBLIP (Dai et al.,
2024) for comparison. Prompts used and other fine-
tuning details for these LMM s are discussed in the
Appendix. (iii) SOTA approaches: we also com-
pare against memory network (Weston et al., 2015)Which year this was founded? Input Image 
Question 
1976 ❌
 LLaVA-1.5 
1995 ❌
 MiniGPT4v2 
1971 ❌
 mPLUG-Owl 
1927 ✅
  [AND] the supporting fact is 
‘7-Eleven was established in 1927 . ’KaLMA (Ours) Ground Truth 1927 Who is the director of this movie? 
James Franco ❌
James Cameron ❌
1 ❌
Danny Boyle ✅
  [AND] 
the supporting fact is 
‘127 Hours movie is directed  
by Danny Boyle .’Danny Boyle 
(b) (c) 
What is the title of this book? 
The Geology of Meteorites ❌
Metamorphic Rocks ❌
The history and mystery of the most important natural 
phenomenon of the last 1000 years meteorites asteroids 
and comets ❌
Meteorites 
(d) Meteorites ✅
  [AND] the supporting 
fact is “ This is ‘Meteorites’ book .”
Which retail store is this? 
T arget Corporation ❌
T arget ❌
99p Stores ❌
T .J. Maxx ✅
  [AND] the supporting fact is 
‘This retail store is T .J. Maxx ’T.J. Maxx 
(a) 1991 ❌
 InstructBLIP James Cameron ❌
 Meteors ❌
 Retail Store ❌
Which bank is this? Input Image 
Question 
Svenska handelsbanken ❌
 LLaVA-1.5 
Bnp paribas ❌
 MiniGPT4v2 
Orsted ❌
 mPLUG-Owl 
Jyske Bank ✅
  [AND] the supporting fact 
is ‘ This bank is Jyske Bank. ’KaLMA (Ours) Ground Truth 
Jyske Bank Which retail store is this? 
Carrefour ❌
Carrefour ❌
7-eleven ❌
Franprix ✅
  [AND] 
the supporting fact is 
‘This retail store is franprix .’Franprix Where is it headquartered? 
New York ❌
Chicago ❌
100 west Madison Street, Chicago ❌
Dallas 
Dallas ✅
  [AND] the supporting fact 
is “ T uesday Morning has headquarters  
in Dallas. ”
Which retail store is this? 
T esco PLC ❌
B&Q ✅
99p Stores ❌
B&Q ✅
  [AND] the supporting fact is 
‘This retail store is B&Q. ’B&Q 
InstructBLIP Bank of sweden ❌
 Retail store ❌
 Thermal City ❌
 Five Below ❌
(f) (g) (h) (e) Figure 5: A selection of our results as compared to implicit knowledge-based LMM approaches. Please refer
Qualitative Results in Section 4.3 for observations. More results in Appendix C.
and graph neural network-based approach (Singh
et al., 2019b) which are the current state of the art.
In addition to these comparisons, we compare the
visual text entity linking performance of our pro-
posed VisTEL against recent multimodal retrievers
from UniIR (Wei et al., 2024), specifically CLIP-
SF and BLIP-SF, where we use image and visual
text to retrieve entities from the knowledge base.
4.2 Implementation Details
We implemented our method using PyTorch and
the Huggingface Transformers library (Wolf et al.,
2020). We used LLaV A-1.5 as our foundation
model for both VisTEL and KaLMA models. Note
that, LLaV A-1.5 is trained on CC3M (Sharma et al.,
2018) and MS-COCO (Lin et al., 2014). We have
carefully examined these datasets for duplicates
and found no overlap with the evaluation set of
Text- KVQA . Further, DBNET (Liao et al., 2020)
andPARSEQ(Bautista and Atienza, 2022) are used
as visual-text detection and visual-text recognitionmodules in the visual text recognition engine, re-
spectively. We fine-tuned VisTEL with LoRA for
10 epochs with a learning rate of 1e-5 with a batch
size of 128. Similarly, we fine-tuned KaLMA with
LoRA for 6 epochs with a learning rate of 2e-5
with a batch size of 64. LoRA details are as fol-
lows: rank: 16, alpha: 32, dropout: 0.05, for both
the models. Our experiments are conducted on a
machine with three A6000 GPUs (48 GB each).
We make our implementation publicly available at
our project website3.
4.3 Results and Discussion
Results on Text- KVQA :We quantitatively eval-
uate our proposed framework KaLMA on Text-
KVQA and compare against relevant methods in Ta-
ble 1. We report accuracy averaged over the entire
test set for all the three splits of Text- KVQA . It is
no surprise that traditional VQA baselines perform
poorly as they do not have the ability to read and
3https://vl2g.github.io/projects/LMM4Text-KVQA/OCR:  [chaghw, '64###', 'bank', 
'athlon', 'chb', 'amd', '119385'] State bank of 
India (a)Input Image 
OCR:  ['shillin', 'the', 'gx-18-54', 
'chiliii', 'iiiiii', 'rbs', 'mmm', 'mill' ] 
OCR:  ['factory', 'coach' ] (b)
(c)T.J. Maxx 
Burlington 
Coat Factory Skoda auto 
The North 
Face, Inc. 
Burlington 
Coat Factory Chang Hwa Bank 
The Royal Bank 
of Scotland 
Coach NED based 
Retrieval VisTEL (Ours) 
w/o Visual-text VisTEL (Ours) Figure 6: Comparison of visual text entity linking
results. The VisTEL infers the correct entity based on
visual context as well as textual context in the form of
surrounding text in the image. Please refer to Qualitative
Results in Section 4.3 for more details.
reason over visual text. Pre- LMM language models
(GPT-2) and vision-language models (GPT-2 w/
visual context, ViLT, VisualBert) along with LMM
baselines (mPlug-Owl, LLaV A-1.5, MiniGPT4v2,
InstructBLIP) outperform traditional methods, but
fail to outperform knowledge-aware methods in-
cluding the state-of-the-art method (Singh et al.,
2019b). We observe that on knowledge-intensive
tasks like Text- KVQA , the OCR-free capabilities
acquired by LMM s are due to heavily correlated hal-
lucinations of visual objects, thereby fall short to
our proposed approach by a significant margin. Our
proposed framework seamlessly integrates knowl-
edge associated with visual text entity (extracted
using our proposed VisTEL) and significantly en-
hances the performance on Text- KVQA . To be
specific, we advance the state-of-the-art by 18.2%,
19.6%, and 32.2% on scene, book, and movie splits
ofText- KVQA on an absolute scale. This supe-
riority of our approach demonstrates its efficacy
in knowledge-aware text-based visual question an-
swering.
Visual Text Entity Linking Results: We report
them in Table 2. Here, we observe that the pro-
posed VisTEL clearly outperforms both (i) Text-
only retrievers, such as a direct match or normal-
ized edit distance-based match of OCRed text and
entity name, and (ii) Multimodal retrievers, CLIP-
SF and BLIP-SF from UniIR (Wei et al., 2024). ByMethod Visual Context Textual Context scene book movie
Text-only
Direct match ✗ ✓ 54.8 63.6 58.1
NED ✗ ✓ 57.1 66.5 60.1
Multimodal retrievers
UniIR ( CLIP-SF ) ✓ ✓ 64.5 78.8 45.2
UniIR ( BLIP-SF ) ✓ ✓ 60.6 78.5 50.1
Ours
VisTEL ✓ ✗ 73.2 76.9 66.6
VisTEL ✗ ✓ 31.5 9.8 11.6
VisTEL ✓ ✓ 76.5 80.6 71.6
Table 2: Visual Text Entity Linking Results. We report
Recall@1. Text-only retrievers: direct match and nor-
malized edit distance-based methods and Multimodal
retrievers: CLIP-SF and BLIP-SF from UniIR (Wei
et al., 2024) fall short. On the contrary, the proposed
VisTEL, which leverages both visual and textual context
(surrounding OCRed text) in an LMM framework, shows
impressive visual text entity linking performance over
both text-only as well as multimodal retrievers.
virtue of LMM and joint reasoning of visual and tex-
tual (OCR) context for linking visual text, VisTEL
yields reasonably advanced performance. Never-
theless, there is still scope of improvement which
we believe can be achieved by further improving
visual text recognition, and performing detailed vi-
sual reasoning such as logo recognition. We leave
these extensions as future work.
Qualitative Results: We show a selection of re-
sults for text-based knowledge-aware visual ques-
tion answering and visual text entity linking in
Figure 5 and Figure 6, respectively.
In Figure 5, LMM models exhibit hallucination
over visually apparent objects. In (a), all LMM s in-
correctly identify T.J. Maxx as popular retail stores
Target and99p Stores . In (b), they provide a ran-
dom year. In (c), these models are confused over
the keyword James , mixing up the director and
actor names on the poster. In (d), LMM s halluci-
nate and suggest non-existent book titles. Similar
hallucinations can be seen in the other examples
(e-h). Our proposed method owing to visual-text
entity linking capabilities and reasoning over ex-
plicit knowledge, provides accurate answers.
In Figure 6, we observe that our proposed model
accurately links visual text in the images to the cor-
rect entity despite noisy OCR in (a), abbreviations
in (b), and ambiguous visual text in (c).
4.3.1 Ablations and Analysis
We conduct the following ablations and analysis of
the proposed work:
(i) What is the need for VisTEL?: To study the
performance of our model in the absence of the
proposed VisTEL module, we replace it with tra-
ditional edit-distance-based entity linking whereModel Visual text EL Knowledge scene book movie
KaLMA✗ ✗ 39.2 37.0 46.1
✗ OCR only 52.2 49.8 51.7
✓ Entity name only 53.2 59.1 59.2
✓(w/o VisTEL) Knowledge facts 54.9 63.4 70.8
✓(w/ VisTEL) Knowledge facts 72.7 82.3 77.4
MiniGPT4v2
(best LMM method)✗ ✗ 48.2 47.7 47.6
Table 3: Ablations for showing the importance of visual
text entity linking, explicit knowledge facts and Vis-
TEL. Also, note that the first-row result corresponds to
LLaV A-1.5 result from Table 1, as KaLMA without Vis-
TEL and knowledge is equivalent to LLaV A-1.5. Please
refer to Section 4.3.1 for more details.
Detection Recognition scene book movie
EAST CRNN 67.2 81.3 66.4
CRAFT CRNN 67.7 81.9 75.1
DBNet ParSeq 72.7 82.3 77.4
Table 4: Effect of Different Text Detection and Recog-
nition Approaches in our approach.
entities are sorted based on the normalized edit-
distance between extracted OCRs and the entity
name. The results of this ablation, as shown in Ta-
ble 3 further support our claim that the superior vi-
sual text entity linking capabilities of the proposed
VisTEL, enhances the downstream performance of
KaLMA.
(ii) What is the need for visual text entity linking
and explicit knowledge in Text- KVQA ?:We show
these ablation results in Table 3. We, first, skip
visual entity linking in the KaLMA, and feed only
the extracted OCRed text to KaLMA. The drop in
performance shows the utility of visual text entity
linking. Second , we perform visual entity linking,
but, we feed the visual text linked entity name from
the VisTEL as input to KaLMA. Our observations
indicate that although entity names give some hints
about the associated knowledge and reduce hallu-
cination to some extent, it is not as useful as using
explicit knowledge in our full model.
(iii) How much does choice of visual text recogni-
tion engine matter?: In this ablation, we replace
DBNET (Liao et al., 2020) and ParSeq (Bautista and
Atienza, 2022) used in KaLMA with CRAFT (Baek
et al., 2019), EAST (Zhou et al., 2017) and
CRNN (Shi et al., 2016), and report the results of
KaLMA on Text- KVQA in Table 4. Although ef-
fective visual text recognition is critical to the per-
formance, our model that jointly reasons on visual
and textual context, performs reasonably well even
with sub-par visual text recognition.
(iv) Attribution ability of KaLMA :To study the
impact of support fact generation (SFG) along
with the answer generation on the performance
of KaLMA, we train KaLMA without support factSFG scene book movie
✓ 72.7 82.3 77.4
✗ 71.4 83.5 76.9
Table 5: Performance of KaLMA w/ and w/o supporting
fact generation (SFG) on Text- KVQA .
generation, and report the results in Table 5. We
observe that KaLMA’s performance drops slightly,
further supporting our claim that support fact gen-
eration elicits chain-of-thought reasoning, thereby
improving the performance of answer generation
along with adding attribution abilities to the model.
(iv) Cost analysis: We provide a comparison of
KaLMA with a traditional non-LLM-based ap-
proach (ViLT). Our approach takes on average 5.6s
per sample, which includes 4s for visual text recog-
nition, 0.8s for entity linking using VisTEL and
0.8s for VQA using KaLMA as compared to ViLT
which takes on average 0.2s per sample during
inference. The training time (finetuning) of both
these models are 36 and 8 hrs, respectively. Fur-
thermore, the trainable parameters for both these
models are 20M (Total size: 14B) and 114M (Total
size: 114M), respectively. We achieved speed-up in
our LMM components through parameter-efficient
fine-tuning (LoRA) with 16-bit precision and 8-bit
quantization during inference. As anticipated, tra-
ditional models have a notable advantage in terms
of computational efficiency compared to our LMM-
based approach. Nonetheless, we substantially sur-
pass them in Text- KVQA accuracy.
5 Conclusion
We have revisited the Text- KVQA and significantly
advanced state of the art on this task. Our findings
suggest that visual text entity linking, combined
with seamless reasoning using both visual and tex-
tual cues, as well as explicit external knowledge
via LMM , is key to our success. We performed ex-
tensive ablation studies and analyses to support our
claims. The future scope of this work is to expand
the dataset with more visual-intensive queries and
address Text- KVQA for multilingual societies.
6 Limitations
We observe the following limitations in our work:
(i) Existing visual text recognition pipelines suffer
on low-resolution images where it is challenging
to extract visual text, which further impacts the
performance of our VisTEL (ii) In the dataset we
use, it was assumed that each image contains onlyone visual text entity which may not be always
true in a real-world scenario. (iii) Current state-
of-the-art visual text recognition engines are not
effective enough over multi-lingual text in the wild;
Hence, in this work, we further assume the visual-
text is English which again might not hold in a
realistic setting. (iv) The temporal nature of knowl-
edge, such as the entity “Statoil" being renamed
“Equinor" over time, is not handled by our current
models. We leave addressing these limitations as a
future work of this paper.
7 Ethical Considerations and Broader
Impact
This work is based on the publicly available Text-
KVQA dataset, which predominantly contains En-
glish visual text, and the associated knowledge
base, questions, and answer pairs are also in En-
glish. The dataset may have some geographic bias
that went undetected in this work, a common is-
sue with many public computer vision and NLP
benchmarks. Additionally, our work uses large
multimodal models ( LMM s), which can inherit and
potentially amplify biases from the large-scale pre-
training data used.
We are mindful of the environmental impact of
using LMM s due to their heavy computational re-
quirements. To mitigate this, we judiciously used
LMM s by reusing pre-existing checkpoints wher-
ever appropriate.
We open-source our implementation to facili-
tate reproduction and further study. Nevertheless,
a more rigorous inspection is indeed required be-
fore deploying the proposed model in real-world
applications to ensure ethical considerations are
comprehensively addressed.
Broader Impact : The proposed work has the fol-
lowing broader impact: (i) The ability to link vi-
sual text entities to knowledge bases and leverage
this linked knowledge for answering questions can
improve the accuracy and relevance of informa-
tion retrieval systems. Although not studied in this
work, this may be particularly valuable in content
recommendation systems and search engines. (ii)
This research contributes to advancing the capabili-
ties of AI systems to understand and interact with
multimodal information (text and images), which
can benefit applications in fields such as virtual
assistants, content understanding, and automated
decision-making. (iii) Methodologically, contribu-
tions such as VisTEL provide new frameworks andtechniques for visual text entity linking, which can
inspire further innovations in Visual NLP.
Acknowledgements
This work was partly supported by the IIT Jodh-
pur Seed Research Grant and National Language
Translation Mission (NLTM): Bhashini project by
the MeitY , Government of India. Abhirama Subra-
manyam Penamakuri was supported by the PMRF
fellowship, MoE, Government of India.
References
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and
Devi Parikh. 2015. VQA: Visual question answering.
InICCV .
Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo
Yun, and Hwalsuk Lee. 2019. Character region
awareness for text detection. In CVPR .
Darwin Bautista and Rowel Atienza. 2022. Scene text
recognition with permuted autoregressive sequence
models. In ECCV . Springer.
Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis
Gomez, Marçal Rusinol, Minesh Mathew, CV Jawa-
har, Ernest Valveny, and Dimosthenis Karatzas.
2019a. Icdar 2019 competition on scene text visual
question answering. In ICDAR .
Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis
Gomez, Marçal Rusinol, Ernest Valveny, CV Jawa-
har, and Dimosthenis Karatzas. 2019b. Scene text
visual question answering. In ICCV .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. NeurIPS .
Mathilde Caron, Ahmet Iscen, Alireza Fathi, and
Cordelia Schmid. 2024. A generative approach for
wikipedia-scale visual entity recognition. In CVPR .
Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit
Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023.
Can pre-trained vision and language models answer
visual information-seeking questions? In EMNLP .
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.
Unifying vision-and-language tasks via text genera-
tion. In ICML .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale N Fung, and Steven Hoi.
2024. Instructblip: Towards general-purpose vision-
language models with instruction tuning. NeurIPS .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In NAACL-HLT .
François Gardères, Maryam Ziaeefard, Baptiste Abe-
loos, and Freddy Lecue. 2020. ConceptBert:
Concept-aware representation for visual question an-
swering. In EMNLP .
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv
Batra, and Devi Parikh. 2017. Making the V in VQA
matter: Elevating the role of image understanding in
visual question answering. In CVPR .
Liangke Gui, Borui Wang, Qiuyuan Huang, Alexan-
der G Hauptmann, Yonatan Bisk, and Jianfeng Gao.
2022. Kat: A knowledge augmented transformer for
vision-and-language. In NAACL-HLT .
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,
and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In ICML .
Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel-
wal, Mandar Joshi, Kenton Lee, Kristina Toutanova,
and Ming-Wei Chang. 2023a. Open-domain visual
entity recognition: Towards recognizing millions of
wikipedia entities. In ICCV .
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-
Wei Chang, Yizhou Sun, Cordelia Schmid, David A
Ross, and Alireza Fathi. 2023b. Reveal: Retrieval-
augmented visual-language pre-training with multi-
source multimodal knowledge memory. In CVPR .
Daniel Jurafsky and James H. Martin. 2009. Speech and
Language Processing (2nd Edition) . Prentice-Hall,
Inc., USA.
Mahmoud Khademi, Ziyi Yang, Felipe Vieira Frujeri,
and Chenguang Zhu. 2023. Mm-reasoner: A multi-
modal knowledge-aware framework for knowledge-
based visual question answering. In EMNLP .
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
2018. Bilinear attention networks. In NeurIPS .
Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:
Vision-and-language transformer without convolu-
tion or region supervision. In ICML .
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. NeurIPS .
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023. Blip-2: bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. In ICML .Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, and
Xiang Bai. 2020. Real-time scene text detection with
differentiable binarization. In AAAI .
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In ECCV .
Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu,
Chenguang Zhu, and Lu Yuan. 2022. Revive: Re-
gional visual representation matters in knowledge-
based visual question answering. NeurIPS .
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2024. Improved baselines with visual instruc-
tion tuning. In CVPR .
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
2016. Hierarchical question-image co-attention for
visual question answering. In NeurIPS .
Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav
Gupta, and Marcus Rohrbach. 2021. Krisp: Inte-
grating implicit and symbolic knowledge for open-
domain knowledge-based VQA. In CVPR .
Kenneth Marino, Mohammad Rastegari, Ali Farhadi,
and Roozbeh Mottaghi. 2019. OK-VQA: A visual
question answering benchmark requiring external
knowledge. In CVPR .
Minesh Mathew, Dimosthenis Karatzas, and CV Jawa-
har. 2021. DocVQA: A dataset for VQA on docu-
ment images. In WACV .
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh,
and Anirban Chakraborty. 2019. OCR-VQA: Visual
question answering by reading text in images. In
ICDAR .
Medhini Narasimhan, Svetlana Lazebnik, and Alexan-
der Schwing. 2018. Out of the box: Reasoning with
graph convolution nets for factual visual question
answering. NeurIPS .
Medhini Narasimhan and Alexander G Schwing. 2018.
Straight to the facts: Learning knowledge base re-
trieval for factual visual question answering. In
ECCV .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. NeurIPS .
Guilherme Penedo, Quentin Malartic, Daniel Hesslow,
Ruxandra Cojocaru, Hamza Alobeidli, Alessandro
Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and
Julien Launay. 2024. The refinedweb dataset for
falcon llm: Outperforming curated corpora with web
data only. NeurIPS .Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In ICML .
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Sanket Shah, Anand Mishra, Naganand Yadati, and
Partha Pratim Talukdar. 2019. KVQA: Knowledge-
aware visual question answering. In AAAI .
Piyush Sharma, Nan Ding, Sebastian Goodman, and
Radu Soricut. 2018. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic im-
age captioning. In ACL.
Baoguang Shi, Xiang Bai, and Cong Yao. 2016. An
end-to-end trainable neural network for image-based
sequence recognition and its application to scene text
recognition. IEEE TPAMI , 39(11):2298–2304.
Ajeet Kumar Singh, Anand Mishra, Shashank Shekhar,
and Anirban Chakraborty. 2019a. From strings to
things: Knowledge-enabled VQA model that can
read and reason. In ICCV .
Amanpreet Singh, Vivek Natarajan, Meet Shah,
Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,
and Marcus Rohrbach. 2019b. Towards VQA mod-
els that can read. In CVPR .
Wen Sun, Yixing Fan, Jiafeng Guo, Ruqing Zhang, and
Xueqi Cheng. 2022. Visual named entity linking: A
new dataset and A baseline. In EMNLP (Findings) .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,
SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-
timodal few-shot learning with frozen language mod-
els.NeurIPS .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. NeurIPS .
Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and
Anton Van Den Hengel. 2017a. FVQA: Fact-based
visual question answering. TPAMI , 40(10):2413–
2427.
Peng Wang, Qi Wu, Chunhua Shen, Anthony R.
Dick, and Anton van den Hengel. 2017b. Explicit
knowledge-based reasoning for visual question an-
swering. In IJCAI .Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu,
Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen.
2024. Uniir: Training and benchmarking universal
multimodal information retrievers. ECCV .
Jason Weston, Sumit Chopra, and Antoine Bordes. 2015.
Memory networks. In ICLR .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. 2020. Transform-
ers: State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
BigScience Workshop, Teven Le Scao, Angela Fan,
Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luc-
cioni, François Yvon, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and
Anton Van Den Hengel. 2016. Ask me anything:
Free-form visual question answering based on knowl-
edge from external sources. In CVPR .
Zilin Xiao, Ming Gong, Paola Cascante-Bonilla,
Xingyao Zhang, Jie Wu, and Vicente Ordonez. 2024.
Grounding language models for visual entity recog-
nition. arXiv preprint arXiv:2402.18695 .
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei
Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.
An empirical study of gpt-3 for few-shot knowledge-
based VQA. In AAAI .
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,
Ming Yan, Yiyang Zhou, Junyang Wang, An-
wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.
mplug-owl: Modularization empowers large lan-
guage models with multimodality. arXiv preprint
arXiv:2304.14178 .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander
Smola, and Le Song. 2018. Variational reasoning for
question answering with knowledge graph. In AAAI .
Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang,
Shuchang Zhou, Weiran He, and Jiajun Liang. 2017.
East: an efficient and accurate scene text detector. In
CVPR .Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. arXiv preprint arXiv:2304.10592 .
Appendix
A Question Categorisation
We show the visual question-answering results
over concretized sub-categories under each of the
scenes, book and movie split in Table 6. We ob-
serve that our proposed model shows remarkable
performance across diverse question categories,
particularly in the challenging categories such as
date, people, and open-ended question categories.
B Finetuning details of LMMs
In this section, we explain the hyperparameters
and prompts used to finetune the LMMs. Note
that we conduct all our experiments on a machine
with 3 48GB A6000 GPUs. For mPlug-Owl and
MiniGPT4v2, we have used hyperparameters as
per the original papers.
mPlug-Owl : We finetuned mPlug-Owl with LoRA
for 6 epochs with a learning rate of 2e-5 with a
batch size of 256. LoRA details: rank: 8, alpha:
32, dropout: 0.05.
Instruction prompt template for mPlug-Owl
The following is a conversation between a curious hu-
man and an AI assistant. The assistant gives accurate
and crisp answers to the user’s questions.
Human: <image>
Human: {Q}
AI: {A}.
MiniGPTv4v2 : We finetuned MiniGPTV4v2 with
LoRA for 6 epochs with a learning rate of 3e-5
with a batch size of 128. LoRA details: rank: 16,
alpha: 64, dropout: 0.05.
Instruction prompt template for
MiniGPT4v2
<image>
{vqa} Based on the image, respond to this question
with a short answer: {Q}, ASSISTANT: {A}
InstructBLIP : We finetuned InstructBLIP for 3
epochs with a learning rate of 1e-5 with a batch
size of 128.Instruction prompt template for Instruct-
BLIP
<image>
USER: {Q}. ASSISTANT: {A}
LLaV A-1.5 : We finetine LLaV A with LORA for 6
epochs with a learning rate of 5e-5 with a batch size
of 64. LoRA details: rank: 16, alpha: 32, dropout:
0.05.
Instruction prompt template for LLaV A-1.5
<image>
USER: {Q}. ASSISTANT: {A}
C More Results
More qualitative results on movie and book splits
ofText- KVQA are shown in Figure 7 and Figure 8,
respectively.Text- KVQA (scene) Text- KVQA (book) Text- KVQA (movie)
Method B D P L OE B D P G OE B D P G L OE
Pre-LLM Methods
GPT-2 54.8 0.2 0.0 13.7 15.4 54.5 43.8 0.1 4.3 0.6 74.5 2.1 0.0 15.2 63.7 0.0
GPT-2 (w/ Visual Context) 57.1 0.3 0.0 16.1 17.0 80.1 63.8 5.2 45.1 7.5 75.4 3.2 0.0 24.3 66.8 29.3
ViLT 75.9 0.0 0.0 33.9 28.7 68 63.3 0 21.3 0.9 85 4.4 0.2 42.1 76.7 0.0
VLBart 78.9 0.2 0.0 18.8 27.4 79.2 62.0 1.7 34.9 0.9 85.4 6.3 0.0 43.7 76.7 0.0
LLM Methods
mPlug-Owl 22 8.9 0.0 45 9.8 19.5 69.7 38.7 43.8 12 7.8 17.5 0.7 9.7 6.2 5.5
LLaV A-1.5 81.1 0.0 2.0 38.7 23.4 79 70.6 19.3 57.3 2.7 84.8 13.5 0.3 1.6 72.7 9.9
MiniGPT4v2 81.7 2.7 1.3 49.9 41.7 80.1 71.9 18.2 54.2 6.6 79.9 13.6 1.2 53.7 78.4 30.4
InstructBLIP 50.0 0.1 6.6 29.7 32.8 49.8 70.3 22 15.2 12.8 50.0 6.6 0.3 1.4 76.5 39.5
Ours
KaLMA 77.2 69.0 76.8 67.8 69.9 88.5 72.9 80.0 80.2 79.6 84.2 69.6 74.8 70.6 91.5 69.1
KaLMA (Oracle) 83.9 95.8 95.4 91.9 91.8 98.0 96.4 98.2 99.9 98.2 99.9 99.8 95.9 100.0 100.0 99.7
Table 6: QA accuracy performance breakdown for various methods by question categories on TEXT-KVQA .
Categories are B: binary, D: date, P: people, L: location, G: genre and OE: open-ended.
In which year this movie was released? Input Image 
Question 
1975 ❌
 LLaVA-1.5 
2014 ❌
 MiniGPT4v2 
2010 ❌
 mPLUG-Owl 
2015 ✅
  [AND] the supporting fact is 
‘Welcome to Leith was released in 2015. ’KaLMA (Ours) Ground Truth 2015 Is this a biography? 
Yes ❌
Yes ❌
Yes ❌
No ✅
  [AND] 
the supporting fact is 
‘The sound and the shadow is a comedy movie .’No
(b) (c) What is the release year of this movie? 
2007 ❌
2008 ❌
2020 ❌
2014 
(d) 2014 ✅
  [AND] the supporting fact is 
“Exile nation: the plastic people was  
released in 2014 .”Who is the director of the movie? 
John Huston ❌
John Cleese ❌
Nigoi ❌
Michele Lupo ✅
  [AND] 
the supporting fact is 
‘Sette volte sette is directed by michele lupo. ’Michele Lupo 
(a) InstructBLIP 2013 ❌
 No ✅
 2013 ❌
 Shin’ichir ō Sawazaki ❌
Figure 7: A few more selection of our results as compared to implicit knowledge-based LMM approaches on
the movie subset of Text- KVQA .
No ✅
 Is this a games related book? Input Image 
Question 
Yes ❌
 LLaVA-1.5 
Yes ❌
 MiniGPT4v2 
Yes ❌
 mPLUG-Owl 
No ✅
  [AND] the supporting fact is 
‘The great cholesterol con: the truth about what  
really causes heart disease and how to avoid it  
book’s genre is health, ﬁtness and dieting .’KaLMA (Ours) Ground Truth NoIs this a pharmaceutical book? 
No ❌
No ❌
Yes ✅
No ✅
  [AND] 
the supporting fact is 
‘The mad dogs and englishmen genre is medical  
books. ’Yes What type of this book is this? 
Literature Fiction ❌
Literature Fiction ❌
Fiction ❌
Travel 
T ravel ✅
  [AND] 
the supporting fact is “ Finding the  
center book’s genre is travel .”Who wrote this book? 
Anonymous ❌
Pylos ❌
Strabo ❌
Polybius ✅
  [AND] 
the supporting fact is 
‘The rise of the roman empire (penguin classics) is  
written by polybius .’Polybius 
InstructBLIP No ❌
 Reference ❌
 Sophus Clausius ❌
Figure 8: A few more selection of our results as compared to implicit knowledge-based LMM approaches on
the book subset of Text- KVQA .