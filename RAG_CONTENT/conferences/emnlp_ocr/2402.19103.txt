Whispers that Shake Foundations: Analyzing and Mitigating False Premise
Hallucinations in Large Language Models
Hongbang Yuan1,2, Pengfei Cao1,2, Zhuoran Jin1,2, Yubo Chen1,2
Daojian Zeng3, Kang Liu1,2, Jun Zhao1,2
1The Laboratory of Cognition and Decision Intelligence for Complex Systems,
Institute of Automation, Chinese Academy of Sciences, Beijing, China
2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China
3Hunan Normal University, Changsha, China
{hongbang.yuan, pengfei.cao, zhuoran.jin, yubo.chen, kliu, jzhao} @nlpr.ia.ac.cn
Abstract
Large Language Models (LLMs) have shown
impressive capabilities but still suffer from the
issue of hallucinations. A significant type of
this issue is the false premise hallucination,
which we define as the phenomenon when
LLMs generate hallucinated text when con-
fronted with false premise questions. In this
paper, we perform a comprehensive analysis
of the false premise hallucination and eluci-
date its internal working mechanism: a small
subset of attention heads (which we designate
as false premise heads) disturb the knowledge
extraction process, leading to the occurrence
of false premise hallucination. Based on our
analysis, we propose FAITH (False premise
Attention head constra Iining for mi Tigating
Hallucinations), a novel and effective method
to mitigate false premise hallucinations. It con-
strains the false premise attention heads during
the model inference process. Impressively, ex-
tensive experiments demonstrate that constrain-
ing only approximately 1%of the attention
heads in the model yields a notable increase
of nearly 20% of model performance.
1 Introduction
Large language models (LLMs) have shown im-
pressive capabilities (Wei et al., 2022; Xu et al.,
2023) and achieved remarkable success in many
tasks (Bubeck et al., 2023; Bang et al., 2023; Jiao
et al., 2023). However, they often generate texts
that are seemingly plausible but deviate from fac-
tual knowledge sources (Zhang et al., 2023), which
is a severe problem known as hallucination .
To address this, many studies focus on detecting
(Manakul et al., 2023; Min et al., 2023) and mitigat-
ing hallucinations (Trivedi et al., 2023; Gou et al.,
2023). However, few of them pay attention to a par-
ticular type of hallucination: False Premise Hallu-
cination . We define it as the phenomenon in which
LLMs generate hallucinated texts in response to
the false premise question. False premise questions
Albert Einstein was awarded the Nobel Prize of Physics in 1920  for what reasons?
Albert Einstein was awarded the Nobel Prize in Physics in 
1920for his explanation of the photoelectric effect ‚Ä¶
Thank you for asking! However, the Nobel Prize in Physics 
was awarded to Albert Einstein in 1921, not 1920‚Ä¶
Hallucinated 
Non-Hallucinated 
Hidden State Multi Head Attention MLP False Premise Head Input Token
Information FlowùëÜ1ùëÜ2 ùëÖ1 ùëÇ1‚Ä≤ùëÇ2‚Ä≤Subject ùë∫ Relation ùëπ False Object ùë∂‚Ä≤
Shallow 
LayersFigure 1: Illustration of the false premise hallucination.
The question contains the false premise that ‚Äú Albert Ein-
stein was awarded The Nobel Prize of Physics in 1920 ‚Äù
whereas in fact he was awarded the prize in 1921. We
find that the presence of false premise attention heads
contributes to the hallucinated response. Our method
can effectively mitigate the false premise hallucination.
are questions that contain falsely assumed facts that
are not directly stated but are likely to be believed
by the asker (Yu et al., 2023b; Kim et al., 2023).
For these questions, LLMs tend to respond directly
without explicitly verifying their plausibility de-
spite the corresponding factual knowledge can be
recalled straightforwardly. For example, as shown
in Figure 1, the user query on the top contains a
false premise: (Albert Einstein, was awarded, The
Nobel Prize of Physics in 1920) , denoted as ( sub-
ject, relation, false object ). LLMs are able to gen-
erate the correct time ‚Äú 1921 ‚Äù when directly queried
about the award time but produce hallucinated time
‚Äú1920 ‚Äù in response to the false premise question.
The exploration of false premise hallucination is
exceptionally significant and valuable. While false
premise questions are pervasive on the Internet and
users are highly likely to pose these questions when
interacting with the LLMs, LLMs are prone to gen-
erate hallucinated texts when confronted with sucharXiv:2402.19103v1  [cs.CL]  29 Feb 2024questions. For example, about 25% of the ques-
tions on the discussion website Reddit contain false
premises (Yu et al., 2023b; Fan et al., 2019). Ac-
cording to our statistics, Llama-2-13b achieves an
accuracy of 100% on our collected 4004 direct-
asking questions but drops to only 24.3%on the
corresponding false premise questions. However,
the analysis of false premise hallucination is non-
trivial. Intuitively, LLMs generate hallucinated
texts due to a lack of pertinent factual knowledge
(Zhang et al., 2023; Huang et al., 2023a). But false
premise hallucination introduces a more intricate
scenario, wherein LLMs still generate hallucinated
texts even when the corresponding factual knowl-
edge is already stored in their parameters.
In this paper, we conduct a comprehensive anal-
ysis of false premise hallucination and elucidate its
internal working mechanism. Firstly, prior to the
analysis, we propose an automatic dataset construc-
tion pipeline for the evaluation of false premise hal-
lucination and create two representative and easy-
to-evaluate datasets based on it. Then, we inves-
tigate the external manifestation of false premise
hallucination and observe that LLMs exhibit more
inherent uncertainty when generating hallucinated
answers. Subsequently, to reveal the source of
the uncertainty, we delve into the internal informa-
tion flow during the hallucination occurrences. We
discover that knowledge about the subject stored
in model parameters is disturbed in the shallow
layers of the model, particularly around the false
object mentioned in the question. Furthermore,
as many studies (Meng et al., 2022; Yuksekgonul
et al., 2023) indicate that self-attention layers trans-
fer the factual knowledge stored in the Multi-Layer
Perception (MLP) layers, we explore the internals
of the self-attention layers and investigate the in-
fluence of each individual attention head on spe-
cific pieces of factual knowledge. We find out that
a small set of attention heads consistently exert
great influence on the factual knowledge across al-
most all the samples and we name them as False
Premise Heads . As depicted in Figure 1, the false
premise heads predominantly reside in the shallow
layers, functioning around the false object men-
tioned in the question. Experimental results demon-
strate that the presence of false premise heads dis-
turb the extraction of the factual knowledge about
the subject, leading to false premise hallucinations.
Based on our analysis, we propose a novel
method termed FAITH (False premise Attention
head constra Ining for mi Tigating Hallucinations)to mitigate hallucinations. It localizes the false
premise attention heads for a group of false premise
questions and subsequently constrains the function
of these attention heads during the model inference
process. Extensive experiments demonstrate the
effectiveness of our method comparing with the
baseline methods.
Our primary contributions can be summarized
as follows:
‚Ä¢We propose an automatic dataset construction
pipeline for the evaluation of false premise hal-
lucination and create two representative and
easy-to-evaluate datasets to facilitate analysis.
‚Ä¢We conduct an in-depth analysis of the false
premise hallucination from the surface to the
internals of LLMs and elucidate its internal
working mechanism by revealing the presence
of false premise attention heads.
‚Ä¢We propose FAITH, a novel method to miti-
gate false premise hallucinations based on our
in-depth analysis. Impressively, extensive ex-
periments demonstrate that constraining only
approximately 1%of all the attention heads in
the model yields a notable increase of nearly
20% in accuracy, which is highly effective.1
2 Background
In this section, we briefly describe the transformer
architecture (Vaswani et al., 2017) in autoregres-
sive, decoder-only language models from the per-
spective of residual stream (Elhage et al., 2021).
Given the context {t1, t2, ..., t N}consisting of
Ntokens, the transformer architecture starts with
a combination of token embeddings and position
embeddings x0‚ààRN√ód, where dis the model
dimension. It marks the start of the residual stream,
with a series of residual layers that read from the
stream and write back their processed results. Each
residual layer is comprised of a self-attention layer
and a MLP layer. The information update of each
residual layer can be expressed as:
xl=xl‚àí1+al+ml
where xlis the hidden state after the l-th layer, al
is the output of the self-attention layer and mlis
the output of the MLP layer. More specifically, the
calculation of the MLP layer is:
ml=f(xl‚àí1KT)V
1Our code and datasets will be available after acceptance.where K, V‚ààRdm√ódandfis a non-linear func-
tion. The self-attention contribution is
al=HX
h=1Ah(xl‚àí1Wh
V)Wh
O
Ah=softmax ((xl‚àí1Wh
Q)(xl‚àí1Wh
K)T
p
dh/H)
where Wh
K, Wh
Q, Wh
O‚ààRd√ódh, Wh
O‚ààRdh√ódare
the parameter matrices, His the number of atten-
tion heads, dh=d/H is the hidden dimension of
each head and A‚ààRN√óNis a lower triangular
attention pattern matrix, showing the interaction
between tokens in different layers. After Lresidual
layers, a layer norm is applied and then an unem-
bedding matrix WU‚ààRd√óVprojects the hidden
state xLto logits, where Vis the length of the
vocabulary.
3 Dataset Construction
In this section, we describe our proposed automatic
dataset construction pipeline for the evaluation of
false premise hallucination and provide the details
of our constructed dataset.
To prevent the memorization of the question
(Carlini et al., 2023; Ramakrishna et al., 2023)
and facilitate the incorporation of the evolving new
knowledge, we propose an automatic dataset con-
struction pipeline, which can be divided into the
following three stages:
(1)Triple Selection We select a set of factual
triples using WikiData2. We assess whether the
factual triple (s, r, o )is stored in the model param-
eters by asking a question with only subject sand
relation r. We retain the triple only if the object
ois present in the answer. (2) Triple Corrup-
tion We replace the object oin the original triple
(s, r, o )with an incorrect entity o‚Ä≤to obtain the cor-
rupted triple (s, r, o‚Ä≤). For example, we transform
the original triple ‚Äú (Albert Einstein, was awarded,
the Nobel Prize of Physics in 1921) ‚Äù into the cor-
rupted triple ‚Äú (Albert Einstein, was awarded, the
Nobel Prize of Physics in 1920) ‚Äù. (3) Question
Construction We construct a false premise ques-
tion by filling a predefined question template with
the previous corrupted triple (s, r, o‚Ä≤). For example,
we define one of the question templates as ‚Äú <per-
son> was awarded <false prize> for what specific
reason? ‚Äù and insert the corrupted triple ‚Äú <person>,
was awarded, <false prize> ‚Äù into the template.
2https://query.wikidata.org/Dataset Knowledge Selected Questions
7B 237 948Prize13B950457 1828
7B 1001 4004Movie13B55091001 4004
Table 1: Details of our datasets. The columns denote
the dataset name, number of knowledge triples, number
of selected knowledge triples for each model, number
of constructed questions for each model, respectively.
As we curate factual knowledge from specific models,
two versions of each dataset are given.
Following this automatic construction pipeline,
we construct two datasets, namely Prize and Movie.
We choose the variants of Llama-2-Chat (Touvron
et al., 2023) with 7B and 13B parameters as the
triple selector. For each model within the dataset,
different versions are constructed as varying num-
bers of knowledge triples are selected. Table 1
provides the details of our datasets, while concrete
question templates are presented in Appendix A.
If the datasets were to be used with other mod-
els, researchers could readily follow our proposed
pipeline and construct their own versions of the
datasets tailored to their specific models.
4 Hallucination Analyze
In this section, we conduct a comprehensive analy-
sis of false premise hallucinations from the surface
and delves deeper into the model step by step.
4.1 Analysis of Model Uncertainty
In this part, we quantitatively investigate model
uncertainty, which is a significant external feature
of false premise hallucination and can be utilized
to detect the hallucination occurrence. We hypoth-
esize that model exhibits more inherent uncertainty
when generating hallucinated answers. We design a
model uncertainty measurement metric that allows
the various linguistic forms of the true answer and
experimental results validate our hypothesis.
Uncertainty Measurement We utilize three met-
rics to measure the uncertainty of the model when
confronted with a question. The former two out of
the three metrics are straightforward while the third
one is specifically designed for our task. Suppose
that we have a question qand a sequence of model
answer T= (t0, t1, ..., t N), the three metrics are
described below:
(1)PPL-Based We simply calculate the nega-
tive log likelihood of the model answer: U1(q) =/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000029/uni00000044/uni0000004f/uni00000056/uni00000048/uni00000003/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000037/uni00000055/uni00000058/uni00000048/uni00000003/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000033/uni00000033/uni0000002f/uni00000003/uni00000003/uni00000024/uni00000038/uni00000026/uni00000020/uni00000013/uni00000011/uni00000018/uni00000018
/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000024/uni00000038/uni00000026/uni00000020/uni00000013/uni00000011/uni00000019/uni00000014
/uni00000036/uni00000048/uni00000050/uni00000044/uni00000051/uni00000057/uni0000004c/uni00000046/uni00000003/uni00000003/uni00000024/uni00000038/uni00000026/uni00000020/uni00000013/uni00000011/uni0000001b/uni00000019(a) 7B
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013
/uni00000029/uni00000044/uni0000004f/uni00000056/uni00000048/uni00000003/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000037/uni00000055/uni00000058/uni00000048/uni00000003/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000033/uni00000033/uni0000002f/uni00000003/uni00000003/uni00000024/uni00000038/uni00000026/uni00000020/uni00000013/uni00000011/uni00000019/uni00000016
/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000024/uni00000038/uni00000026/uni00000020/uni00000013/uni00000011/uni00000019/uni00000017
/uni00000036/uni00000048/uni00000050/uni00000044/uni00000051/uni00000057/uni0000004c/uni00000046/uni00000003/uni00000003/uni00000024/uni00000038/uni00000026/uni00000020/uni00000013/uni00000011/uni0000001c/uni00000014 (b) 13B
Figure 2: The Receiver Operating Characteristic Curve
on the Movie dataset. The perfect AUC score is 1 while
the random AUC score is 0.5.
U1(T) = ‚àí1
|N|PN
i=1logp Œ∏(xi|x<i), where
logp Œ∏(xi|x<i)is the log likelihood of the i-th token
based on the previous tokens x<i.
(2)Sampling-Based To fully leverage the un-
certainty in model parameters (Huang et al.,
2023b), we generate multiple answer sequences
T1, T2, ..., T kfor one question and calculate the av-
erage log likelihood across all sequences: U2(q) =
1
kU1(Tk).
(3)Semantic-Based Inspired by the incorpora-
tion of linguistic invariances in model uncertainty
estimation (Kuhn et al., 2023), we separately treat
the correct and incorrect answers among the mul-
tiple generated answer sequences. We consider
all the correct answers as a unified semantic set
and each incorrect answer as a discrete semantic
set. Then we calculate the uncertainty over these
semantic sets. Concretely, suppose that there are
K1incorrect answers and K2correct answers in
theKgenerated answers, we calculate the uncer-
tainty as follows: U3(q) =‚àí1
K1+1[P
K1U1(Tk) +
logP
K2expU 1(Tk)].
Experiment We conduct experiments on the
Movie dataset using Llama-2-7b-chat (denoted as
7B) and Llama-2-13b-chat (denoted as 13B). To
evaluate the model uncertainty during hallucina-
tion occurrences, the calculated uncertainty scores
are used for the binary classification task, aimed
at determining the occurrence of hallucinations for
each false premise question. We use the Area Un-
der the Receiver Operating Characteristic (AUC)
to assess the effectiveness of the uncertainty score.
The higher the scores, the greater the correlation
between the uncertainty metric and the occurrence
of hallucinations.
Results and Analysis The Receiver Operating
Characteristic Curve (ROC Curve) curves and the
AUC scores are shown in Figure 2. From the re-
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni0000002c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000036
/uni00000029/uni00000032
/uni00000032/uni00000057/uni0000004b/uni00000048/uni00000055(a) 7B w/ Hallucination
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni0000002c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000036
/uni00000029/uni00000032
/uni00000032/uni00000057/uni0000004b/uni00000048/uni00000055 (b) 7B w/o Hallucination
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni0000002c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000036
/uni00000029/uni00000032
/uni00000032/uni00000057/uni0000004b/uni00000048/uni00000055
(c) 13B w/ Hallucination
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013
/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni0000002c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000036
/uni00000029/uni00000032
/uni00000032/uni00000057/uni0000004b/uni00000048/uni00000055 (d) 13B w/o Hallucination
Figure 3: Information flow from various parts of the
question to the final logit across distinct layers on hallu-
cinated and non-hallucinated samples.
sults, we draw the following observations: (1) Our
semantic-based uncertainty metric score is far more
effective than the other two methods. It can be fur-
ther employed in the prediction of the occurrence of
false premise hallucinations without relying on an
external knowledge base. (2) We observe a strong
correlation (over 0.9for the 13B model) between
the occurrence of hallucinations and model uncer-
tainty. This verifies our hypothesis that models
exhibit more inherent uncertainty when generating
hallucinated answers.
4.2 Analysis of Internal Information Flow
To explore the source of the uncertainty, in this
section, we delve into the internal information flow
of LLMs when generating hallucinated answers.
We study how the information flow from different
parts of the false premise question in the fill-in-the-
blank task. Experimental results demonstrate that
the knowledge about the subject is disturbed in the
shallow layers of the model, particularly around
the false object mentioned in the question.
Knowledge Assessment Task The most signif-
icant feature of false premise hallucination is that
the factual knowledge about the subject can be
recalled directly. Therefore, we design a fill-in-the-
blank task to evaluate how the knowledge stored
in the model parameters is affected by the ques-
tion. Concretely, for a question containing the false
triple (s, r, o‚Ä≤), LLMs are required to complete the
following cloze query: ‚Äú <Question> According to
my knowledge, the object linking from subject svia relation ris _‚Äù. We posit that this knowledge
assessment task is correlated with the original ques-
tion answering task. Intuitively, if LLMs generate
hallucinated answers to false premise questions,
they are highly likely to fail in completing this
cloze query correctly.
Attribution Score We aim to discover how the
information flow from the tokens in the question to
the final prediction logit in the knowledge assess-
ment task. Since gradients and the attention itself
can be blended together to acquire a better perfor-
mance (Zhao et al., 2024), we use the element-wise
product version (Wang et al., 2023) to calculate the
attribution score for each token:
Sl=HX
h=1Ah
l‚äô‚àÇL
‚àÇAh
l
where Ah
lis the attention pattern matrix described
in Section 2 and Lis the loss on the token predic-
tion task. The attribution score matrix Slon layer l
is aN√óNmatrix ( Nis the length of the prompt).
We partition the question into three parts: subject
part (denote as S), false object part (denote as FO)
and other part (denoted as other ). The information
flow from these parts is consequently defined as:
SSl=1
|Nsub|NsubX
t=1SN,it
SFOl=1
|Nfobj|NfobjX
t=1SN,it
Sother l=1
|Nother|NotherX
t=1SN,it
where Nsubis the number of tokens of subject,
Nfobjis the number of tokens of the false object
andNother is the number of other tokens.
Results and Analysis. We illustrate the informa-
tion flow from various parts of the question to the
final logit across distinct layers on hallucinated and
non-hallucinated samples in the Prize dataset, as
shown in Figure 3. By observing the figures, it‚Äôs
evident that the information flow across the lay-
ers can be roughly divided into three pieces. (1)
In shallow layers, models primarily focus on the
false object part of the question, leading to more
perturbation on the hallucinated samples than the
non-hallucinated samples. (2) In middle layers, to
counteract the perturbation caused by the false ob-
ject part, models shift their emphasis towards the
Step 3ùëÉ‚Ñéùë¶ùë†ùëñùëêùë† in 1922
‚Ñô(ùëÇ)‚Ñô‚Ä≤(ùëÇ)XX ùëÉ‚Ñéùë¶ùë†ùëñùëêùë† in
MaskedStep 2ùëÉ‚Ñéùë¶ùë†ùëñùëêùë† in 1922
CleanStep 1Freeze
ReplaceStep 3
ùê∏‚Ñéùëíùëéùëë=‚Ñô‚Ä≤ùëÇ‚àí‚Ñô(ùëÇ)Figure 4: Calculation of the influence of a single atten-
tion head.
subject to validate the knowledge. The resistance
observed in the hallucinated examples is greater
than in the non-hallucinated samples. (3) In deep
layers, the models continue to focus on the false
object component of the question.
Therefore, we conclude that the knowledge
about the subject is disturbed in the shallow layers
of the model in the false object part of the question.
4.3 Analysis of Individual Attention Heads
As many studies (Meng et al., 2022; Yuksekgonul
et al., 2023) indicate that the self-attention layers
transfer the factual knowledge stored in the MLP
layers during the inference process, we further in-
vestigate the influence of each individual attention
head within the self-attention layers to identify the
source of the disturbance. We calculate the influ-
ence of each individual head on the final prediction
logit in the knowledge assessment task and find out
the presence of false premise attention heads.
Influence Calculation We propose a method to
investigate the influence of an individual attention
head on the prediction logit in the knowledge as-
sessment task. The computation of the influence of
a specific individual attention head can be divided
into three steps, as shown in Figure 4.
(1)Clean Run. We perform a forward pass
using the original question and store the activations
of all the attention heads. The token prediction
logit of this run is denoted as P(O).
(2)Masked Run. We create a masked question
by substituting the false object tokens with nonsen-
sical placeholders. As illustrated in the top-left of
Figure 4, the false year is substituted with ‚Äò XX‚Äô.
Subsequently, we perform a forward pass using the/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013
/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000002b/uni00000048/uni00000044/uni00000047/uni00000056/uni00000013
/uni00000018
/uni00000014/uni00000013
/uni00000014/uni00000018
/uni00000015/uni00000013
/uni00000015/uni00000018
/uni00000016/uni00000013/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000056
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000016/uni00000013/uni00000013/uni00000011/uni00000016/uni00000018(a) Attention Head Influence
/uni0000005a/uni0000004b/uni0000005c/uni0000005a/uni00000044/uni00000056/uni00000057/uni0000004b/uni00000048/uni00000049/uni0000004c/uni0000004f/uni00000050
/uni00000014/uni00000015/uni00000044/uni00000051/uni0000004a/uni00000055/uni0000005c/uni00000050/uni00000048/uni00000051/uni00000055/uni00000048/uni0000004f/uni00000048/uni00000044/uni00000056/uni00000048/uni00000047/uni0000004c/uni00000051
/uni00000014/uni0000001c/uni00000018/uni00000019/uni00000022/uni0000005a/uni0000004b/uni0000005c
/uni0000005a/uni00000044/uni00000056
/uni00000057/uni0000004b/uni00000048
/uni00000049/uni0000004c/uni0000004f/uni00000050
/uni00000014
/uni00000015
/uni00000044/uni00000051
/uni0000004a/uni00000055/uni0000005c
/uni00000050/uni00000048/uni00000051
/uni00000055/uni00000048/uni0000004f/uni00000048/uni00000044/uni00000056/uni00000048/uni00000047
/uni0000004c/uni00000051
/uni00000014
/uni0000001c
/uni00000018
/uni00000019
/uni00000022
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017 (b) Attention Pattern of head 1-22.
/uni0000005a/uni0000004b/uni0000005c/uni0000005a/uni00000044/uni00000056/uni00000057/uni0000004b/uni00000048/uni00000049/uni0000004c/uni0000004f/uni00000050
/uni00000014/uni00000015/uni00000044/uni00000051/uni0000004a/uni00000055/uni0000005c/uni00000050/uni00000048/uni00000051/uni00000055/uni00000048/uni0000004f/uni00000048/uni00000044/uni00000056/uni00000048/uni00000047/uni0000004c/uni00000051
/uni00000014/uni0000001c/uni00000018/uni00000019/uni00000022/uni0000005a/uni0000004b/uni0000005c
/uni0000005a/uni00000044/uni00000056
/uni00000057/uni0000004b/uni00000048
/uni00000049/uni0000004c/uni0000004f/uni00000050
/uni00000014
/uni00000015
/uni00000044/uni00000051
/uni0000004a/uni00000055/uni0000005c
/uni00000050/uni00000048/uni00000051
/uni00000055/uni00000048/uni0000004f/uni00000048/uni00000044/uni00000056/uni00000048/uni00000047
/uni0000004c/uni00000051
/uni00000014
/uni0000001c
/uni00000018
/uni00000019
/uni00000022
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b (c) Attention Pattern of head 5-15.
Figure 5: Illustration of the false premise attention heads.
masked question to store the activations of all the
attention heads.
(3)Replace and Freeze Run. We run another
forward pass using the original question, replacing
the selected attention head with the values stored in
the masked run while simultaneously freezing other
attention heads using the values stored in the clean
run. The token prediction logit is denoted as P‚Ä≤(O).
Therefore, the influence of a specific attention head
can be defined as Ehead=P‚Ä≤(O)‚àíP(O).
Results and Analysis We visualize the influence
of the attention heads in Llama-2-7b-chat averaged
across all samples in the Movie dataset, as depicted
in Figure 5a. From the figure, we draw the fol-
lowing key observations: (1) The attention heads
that exert great influence on the final logit primar-
ily reside in the shallow layers of the model (0-15
layers for the 7B model). This is consistent with
the analysis of the internal information flow which
indicates that the disturbance of factual knowledge
originates from the shallow layers, as discussed in
Section 4.2. (2) We observe that a few attention
heads exert significantly greater influence than oth-
ers. We conclude that these attention heads will
have to take the blame for the false premise hallu-
cination and we designate them as False Premise
Attention Heads .
Attention Patterns In order to better understand
the behaviour of the false premise attention heads,
we explore their attention patterns in some concrete
examples. Figure 5b and 5c shows the attention
pattern of the 23-rd attention head in the second
layer (denoted as (1,22)) and the 16th head in the
sixth layer (denoted as (5,15)) on two examples
in the Movie dataset. It is evident that they both
exhibit a similar pattern, primarily concentrating
on the information around the current tokens whileAlgorithm 1: Head Localization
input : A set of false premise questions
{Q1, Q2, ..., Q N}, model M,
threshold œÑ, question templates
{T1, T2, ..., T N}.
output : A set of attention heads in M
1S‚Üê‚àí[];
2L‚Üê‚àíNumberLayersOf( M);
3H‚Üê‚àíNumberHeadsOf( M);
4fori‚Üê1toNdo
5 pi‚Üê(Qi, Ti);
6 forl‚Üê1toLdo
7 forh‚Üê1toHdo
8 Elh‚ÜêInfluence( l, h, p i);
9 ifElh‚â•œÑthen
10 S‚Üê(l, h);
11S‚Üê‚àíSortedByFrequency( S);
disregarding the connection with other tokens.
Therefore, the internal working mechanism of
false premise hallucination is revealed: the false
premise heads solely focus on the information sur-
rounding the current tokens, disregarding the con-
nection between the false object and the subject,
which contributes to the occurrence of the false
premise hallucination.
5 Hallucination Mitigation
Based on our previous in-depth analysis, in this sec-
tion, we introduce FAITH, a novel method aimed
at mitigating false premise hallucinations.
5.1 FAITH
Our method FAITH consists of two parts. The first
part involves localizing the false premise attentionMethodsPrize Movie
T1 T2 T3 T4 Avg T1 T2 T3 T4 Avg
7BVanilla 14.77 15.19 13.08 14.35 14.35 78.92 41.46 41.66 24.08 46.53
ITI 36.71 14.77 18.99 22.36 23.21 74.63 43.16 30.87 10.69 39.84
DoLA 17.30 19.41 18.99 27.00 20.68 72.53 59.24 38.86 35.46 51.52
Repe 23.21 25.74 8.86 25.74 20.89 47.15 49.15 40.56 42.06 44.73
FAITH (Ours) 62.03 38.40 18.14 35.86 38.61 94.31 81.02 65.10 70.53 77.74
13BVanilla 8.32 4.38 2.41 3.94 4.76 52.05 42.06 2.30 0.80 24.30
ITI 9.19 6.78 2.41 4.81 5.80 74.93 67.83 4.30 2.80 37.47
DoLA 21.44 18.16 1.97 4.60 11.54 56.84 58.74 2.40 2.80 30.20
Repe 19.47 39.17 7.44 28.45 23.63 51.35 44.16 17.48 21.58 33.64
FAITH (Ours) 30.63 18.16 8.97 39.61 24.34 82.02 80.52 49.65 23.78 58.99
Table 2: Experimental results (accuracy, %) using Llama-2-7b-chat and Llama-2-13b-chat. The best results are
highlighted in boldface . ‚ÄúT1‚Äù denotes the model performance on question template 1. ‚ÄúAvg" denotes the model
performance averaged across all four question templates. A higher presented accuracy indicates a lower occurrence
of hallucination.
heads of a set of false premise questions, while the
second part involves constraining these attention
heads during the model inference process.
Head Localization To identify the false premise
heads for a set of false premise questions, for each
question, we employ the knowledge assessment
cloze queries to convert the question answer task
into the fill-in-the blank task. Subsequently, we
calculate the influence of each individual attention
head on each specific sample, as described in Sec-
tion 4.3. Finally, we select attention heads that
have an influence exceeding a predefined threshold
on individual examples and appear most frequently
across the samples. The pseudocode of our local-
ization procedure is shown in Algorithm 1.
Head Constraining To eliminate the impact of
these false premise heads and mitigate hallucina-
tions, we constrain these attention heads around
the false object tokens during the model inference
process. The output of the constrained multi-head
attention is defined as:
a‚Ä≤
l=HX
h=1Ah(xl‚àí1Wh
V)Wh
O‚äôf(1N√ód)
f(B) =(
B[i:j,:] = 0 ,ifh‚ààS,
B, else.
where Sis the set of false premise heads on layer
l, where Wh
K, Wh
Q, Wh
O‚ààRd√ódh, Wh
O‚ààRdh√ód
are the parameter matrices, A‚ààRN√óNis the at-
tention pattern matrix, a‚Ä≤
lis the output of the con-
strained multi-head attention, i, jis the range of
the false object tokens in the question and f(B)isthe constraining function, which zeroes out certain
rows of the input matrix if the attention head is to
be constrained. The rows of the matrix 1N√ódcorre-
spond to the tokens in the question thus we choose
the false object part of the question to eliminate.
5.2 Baselines
We compare our method with the following base-
line methods:
(1)Vanilla , which directly prompts the LLMs to
generate the answers without any intervention.
(2)ITI(Li et al., 2023b), which is a technique
that adjusts certain attention heads towards the
‚Äòtruthful‚Äô direction during the inference process.
(3)DoLa (Chuang et al., 2023), which is a novel
decoding strategy that better reveals the truthful
knowledge by contrasting different layers.
(4)RepE (Zou et al., 2023), which computes
the difference vector using a pair of contrastive
prompts during inference and utilizes it to control
the hidden state during the inference process.
5.3 Implementation Details
We conduct experiments using open source LLMs,
specifically Llama-2-7b-chat and Llama-2-13b-
chat, on both the Movie and Prize dataset. To
prevent errors in a single decoding step, we em-
ploy beam search decoding and set the beam size
to5. For 7B model on both the datasets, we con-
strain 5 false premise attention heads (approxi-
mately 0.56% of all the attention heads). For 13B
model, we constrain 15 ( 0.94%) false premise at-
tention heads on the Movie dataset and 20 ( 1.25%)
on the Prize dataset.For the evaluation metrics of the hallucination
mitigation task, we employ a heuristic method. We
consider the answer to a false premise question as
non-hallucinated if the original object ois present
in the final answer. This indicates that LLMs have
successfully identified the false premise in the ques-
tion. Therefore, accuracy can be employed as the
metric to measure the performance of each method.
The higher the accuracy, the lower the occurrence
of hallucination.
5.4 Results and Analysis
From the experimental results shown in Table 2,
we derive the following key observations. (1) Our
method is considerably effective when compared
with existing baselines. For example, our method
consistently outperforms other baselines across
nearly all the question templates. This verifies
the hypothesis that false premise heads contribute
to model hallucinations. (2) Our method is more
effective on models with smaller number of pa-
rameters. For example, compared with the second
best-performing method on the Prize dataset, our
method achieves 17.72% improvements of accu-
racy with the 7B model yet 0.71% improvements
with the 13B model. We attribute it to that models
with larger scales tend to exhibit greater resistance
to changes in their results.
5.5 Generalization
We further explore the generalizability of the iden-
tified false premise attention heads from one ques-
tion template to others. We design the following
two experiments: (1) Within Knowledge , which
uses the false premise heads identified on various
question templates in Movie dataset to mitigate hal-
lucinations on each specific template. (2) Across
Knowledge ,which uses the false premise heads
identified on the Prize dataset to mitigate halluci-
nations on the Movie dataset.
We also choose random selected attention heads
for comparison and the experimental results are
shown in Table 3. From the table, we can observe
that our identified false premise attention heads
exhibit strong generalizabilities. For example, the
model achieves comparable performance within
and across datasets and significantly outperforms
the random baseline. This demonstrates that our
revealed mechanism of the false premise attention
heads is relatively general.Methods T1 T2 T3 T4 Avg
FAITH 94.31 81.02 65.10 70.53 77.74
w/T1 94.31 83.02 61.54 68.73 76.90
w/T2 94.51 81.02 66.83 70.93 78.32
w/T3 88.91 61.74 65.1 43.26 64.75Within
w/T4 94.31 81.02 60.64 70.53 76.63
w/PT1 94.11 80.22 65.53 69.53 77.35
w/PT2 94.11 80.22 65.53 69.53 77.35
w/PT3 94.71 81.82 63.64 68.63 77.20Across
w/PT4 94.31 81.02 60.64 70.53 76.63
Random 78.32 41.46 41.06 23.88 46.18
Table 3: Generalizability of the attention heads on the
7B model. ‚Äúw/T1‚Äù denotes using the false premise heads
identified on the question template 1 in the same Movie
dataset. ‚Äúw/PT1"" denotes using the false premise heads
identified on the question template 1 in the Prize dataset.
Results of the 13B model can be found in Appendix B.
6 Related Work
Hallucination Many work focus on evaluating
(Vu et al., 2023; Li et al., 2023a), detecting (Chen
et al., 2024; Yang et al., 2023) and mitigating
(Trivedi et al., 2023; Gao et al., 2023; M√ºndler
et al., 2023; Zhou et al., 2023) hallucinations. How-
ever, they ignore the analysis of the false premise
hallucination.
Mechanistic Interpretability Mechanistic inter-
pretability aims at understanding the model be-
haviours by investigating individual neurons and
their connections (Zhao et al., 2024). Various inter-
pretable representations are found, such as in model
alignment (Lee et al., 2024), reasoning (Stolfo et al.,
2023), knowledge recall (Geva et al., 2023; Yu
et al., 2023a) and in-context-learning(Hendel et al.,
2023; Todd et al., 2023). We are the first to explore
the internal working mechanism of false premise
hallucinations.
7 Conclusion
In this paper, we conduct a comprehensive anal-
ysis of an important type of hallucination: False
Premise Hallucination. Our analysis begins at the
surface of the model and gradually delves deeper
into it, ultimately revealing the presence of false
premise attention heads. Based on our analysis, we
propose a novel false premise hallucination miti-
gation method, FAITH (False premise Attention
head constra Ining for mi Tigating Hallucinations).
Extensive experiments demonstrate the effective-
ness of our method comparing with the baselines
and the promising nature of our revealed internal
working mechanism of false premise hallucination.Limitations
Our study, while providing valuable insights into
the false premise hallucination, is subject to sev-
eral limitations, as outlined below. (1) Due to con-
straints in computing resources, our research is
restricted to models up to a scale of 13B parame-
ters. Future research could investigate more models
with larger scales. (2) The calculation of the influ-
ence of multiple attention heads is time-consuming
due to the vast number of combinations. Conse-
quently, considering the computational complexity
involved, we limit our investigation to the influence
of each individual attention head on the final pre-
diction logit. Future research could further explore
how to effectively select the the most influential
joint contribution of multiple attention heads.
References
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,
and Pascale Fung. 2023. A multitask, multilingual,
multimodal evaluation of ChatGPT on reasoning, hal-
lucination, and interactivity. In Proceedings of the
13th International Joint Conference on Natural Lan-
guage Processing and the 3rd Conference of the Asia-
Pacific Chapter of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 675‚Äì718,
Nusa Dua, Bali. Association for Computational Lin-
guistics.
S√©bastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-
ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,
and Yi Zhang. 2023. Sparks of artificial general in-
telligence: Early experiments with gpt-4.
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
Katherine Lee, Florian Tram√®r, and Chiyuan Zhang.
2023. Quantifying memorization across neural lan-
guage models. In The Eleventh International Con-
ference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023 . OpenReview.net.
Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu,
Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024.
Inside: Llms‚Äô internal states retain the power of hal-
lucination detection.
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon
Kim, James Glass, and Pengcheng He. 2023. Dola:
Decoding by contrasting layers improves factu-
ality in large language models. arXiv preprint
arXiv:2309.03883 .
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al.2021. A mathematical framework for transformer
circuits. Transformer Circuits Thread , 1.
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
Long form question answering. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics , pages 3558‚Äì3567, Florence,
Italy. Association for Computational Linguistics.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent
Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and
Kelvin Guu. 2023. RARR: Researching and revising
what language models say, using language models.
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 16477‚Äì16508, Toronto, Canada.
Association for Computational Linguistics.
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir
Globerson. 2023. Dissecting recall of factual associa-
tions in auto-regressive language models. In Proceed-
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 12216‚Äì12235,
Singapore. Association for Computational Linguis-
tics.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,
Yujiu Yang, Nan Duan, and Weizhu Chen. 2023.
Critic: Large language models can self-correct with
tool-interactive critiquing.
Roee Hendel, Mor Geva, and Amir Globerson. 2023.
In-context learning creates task vectors. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023 , pages 9318‚Äì9333, Singapore.
Association for Computational Linguistics.
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting
Liu. 2023a. A survey on hallucination in large lan-
guage models: Principles, taxonomy, challenges, and
open questions.
Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming
Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma.
2023b. Look before you leap: An exploratory study
of uncertainty measurement for large language mod-
els.
Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing
Wang, Shuming Shi, and Zhaopeng Tu. 2023. Is chat-
gpt a good translator? yes with gpt-4 as the engine.
Najoung Kim, Phu Mon Htut, Samuel R. Bowman, and
Jackson Petty. 2023. (QA)2: Question answering
with questionable assumptions. In Proceedings of the
61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
8466‚Äì8487, Toronto, Canada. Association for Com-
putational Linguistics.Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.
Semantic uncertainty: Linguistic invariances for un-
certainty estimation in natural language generation.
InThe Eleventh International Conference on Learn-
ing Representations, ICLR 2023, Kigali, Rwanda,
May 1-5, 2023 . OpenReview.net.
Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Watten-
berg, Jonathan K. Kummerfeld, and Rada Mihalcea.
2024. A mechanistic understanding of alignment
algorithms: A case study on dpo and toxicity.
Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and
Ji-Rong Wen. 2023a. HaluEval: A large-scale hal-
lucination evaluation benchmark for large language
models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 6449‚Äì6464, Singapore. Association for Com-
putational Linguistics.
Kenneth Li, Oam Patel, Fernanda Vi√©gas, Hanspeter
Pfister, and Martin Wattenberg. 2023b. Inference-
time intervention: Eliciting truthful answers from a
language model.
Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.
SelfCheckGPT: Zero-resource black-box hallucina-
tion detection for generative large language models.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
9004‚Äì9017, Singapore. Association for Computa-
tional Linguistics.
Kevin Meng, David Bau, Alex Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual associ-
ations in GPT. In Advances in Neural Information
Processing Systems 35: Annual Conference on Neu-
ral Information Processing Systems 2022, NeurIPS
2022, New Orleans, LA, USA, November 28 - Decem-
ber 9, 2022 .
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2023. FActScore:
Fine-grained atomic evaluation of factual precision
in long form text generation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 12076‚Äì12100, Singa-
pore. Association for Computational Linguistics.
Niels M√ºndler, Jingxuan He, Slobodan Jenko, and Mar-
tin Vechev. 2023. Self-contradictory hallucinations
of large language models: Evaluation, detection and
mitigation.
Anil Ramakrishna, Rahul Gupta, Jens Lehmann, and
Morteza Ziyadi. 2023. INVITE: a testbed of au-
tomatically generated invalid questions to evaluate
large language models for hallucinations. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023 , pages 5422‚Äì5429, Singapore.
Association for Computational Linguistics.
Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya
Sachan. 2023. A mechanistic interpretation of arith-
metic reasoning in language models using causalmediation analysis. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 7035‚Äì7052, Singapore. Associa-
tion for Computational Linguistics.
Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron
Mueller, Byron C. Wallace, and David Bau. 2023.
Function vectors in large language models.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2023. Interleaving retrieval
with chain-of-thought reasoning for knowledge-
intensive multi-step questions. In Proceedings of
the 61st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers) ,
pages 10014‚Äì10037, Toronto, Canada. Association
for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , pages 5998‚Äì6008.
Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry
Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny
Zhou, Quoc Le, and Thang Luong. 2023. Freshllms:
Refreshing large language models with search engine
augmentation.
Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,
Fandong Meng, Jie Zhou, and Xu Sun. 2023. Label
words are anchors: An information flow perspective
for understanding in-context learning. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pages 9840‚Äì9855,
Singapore. Association for Computational Linguis-
tics.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022. Chain-of-thought prompting
elicits reasoning in large language models. In Ad-
vances in Neural Information Processing Systems 35:
Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA,
USA, November 28 - December 9, 2022 .
Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu,
Zhengyu Chen, and Jian Zhang. 2023. On the tool
manipulation capability of open-source large lan-
guage models.
Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023.
A new benchmark and reverse validation method for
passage-level hallucination detection. In Findings
of the Association for Computational Linguistics:
EMNLP 2023 , pages 3898‚Äì3908, Singapore. Associ-
ation for Computational Linguistics.
Qinan Yu, Jack Merullo, and Ellie Pavlick. 2023a. Char-
acterizing mechanisms for factual recall in language
models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 9924‚Äì9959, Singapore. Association for Com-
putational Linguistics.
Xinyan Yu, Sewon Min, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2023b. CREPE: Open-domain
question answering with false presuppositions. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 10457‚Äì10480, Toronto, Canada.
Association for Computational Linguistics.
Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones,
Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece
Kamar, and Besmira Nushi. 2023. Attention satis-
fies: A constraint-satisfaction lens on factual errors
of language models.
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,
Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei
Bi, Freda Shi, and Shuming Shi. 2023. Siren‚Äôs song
in the ai ocean: A survey on hallucination in large
language models.
Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,
Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei
Yin, and Mengnan Du. 2024. Explainability for large
language models: A survey. ACM Trans. Intell. Syst.
Technol. Just Accepted.
Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun
Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and
Huaxiu Yao. 2023. Analyzing and mitigating object
hallucination in large vision-language models.
Andy Zou, Long Phan, Sarah Chen, James Campbell,
Phillip Guo, Richard Ren, Alexander Pan, Xuwang
Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan
Wang, Alex Mallen, Steven Basart, Sanmi Koyejo,
Dawn Song, Matt Fredrikson, Zico Kolter, and DanHendrycks. 2023. Representation engineering: A
top-down approach to ai transparency.A Question Templates
Details of the concrete question templates we em-
ployed are provided in Table 4.
B Results of Generalization Experiments
using model Llama-2-13b-chat
Table 5 shows the experimental results of the gen-
eralization experiments using model Llama-2-13b-
chat.
Methods T1 T2 T3 T4 Avg
FAITH 82.02 80.52 49.65 23.78 58.99
w/T1 82.02 84.52 20.28 13.19 50.00
w/T2 77.92 80.52 11.69 10.19 45.08
w/T3 80.72 73.33 49.65 46.45 62.54Within
w/T4 81.52 81.92 28.77 23.78 54.00
w/PT1 68.33 63.94 12.09 8.59 38.24
w/PT2 79.42 77.12 2.70 2.10 40.34
w/PT3 81.72 80.82 5.00 3.70 42.81Across
w/PT4 80.52 77.92 51.25 42.36 63.01
Random 50.55 41.86 2.40 0.70 23.88
Table 5: Generalizability of the attention heads on the
13B model. ‚Äúw/T1‚Äù denotes using the false premise
heads identified on the question template 1 in the
same Movie dataset. ‚Äúw/PT1"" denotes using the false
premise heads identified on the question template 1 in
the Prize dataset.
Datasets Corrupted Triple Question Templates
Prize(person,
was awarded,
false prize)T1 For what specific contribution was <person> awarded <false prize>?
T2 <person> was awarded <false prize> for what specific reason?
T3 <false prize> was awarded to <person> for what specific reason?
T4 Why was <person> awarded the <false prize>?
Movie(movie,
was released in,
false time)T1 Why was the film <movie> released in <false time>?
T2 What was the film <movie> released in <false time> describing about?
T3 What was the <false time> film <movie> about?
T4 Who are the main characters in the <false time> film <movie>?
Table 4: Details of the question templates in our datasets.