MixGR : Enhancing Retriever Generalization for Scientific Domain
through Complementary Granularity
Fengyu Cai1Xinran Zhao2Tong Chen3Sihao Chen4
Hongming Zhang5Iryna Gurevych1Heinz Koeppl1
1Technical University of Darmstadt2Carnegie Mellon University3University of Washington
4University of Pennsylvania5Tencent AI Lab
{fengyu.cai, heinz.koeppl}@tu-darmstadt.de
Abstract
Recent studies show the growing significance
of document retrieval in the generation of
LLMs, i.e., RAG, within the scientific domain
by bridging their knowledge gap. However,
dense retrievers often struggle with domain-
specific retrieval and complex query-document
relationships, particularly when query seg-
ments correspond to various parts of a docu-
ment. To alleviate such prevalent challenges,
this paper introduces MixGR , which improves
dense retrievers’ awareness of query-document
matching across various levels of granularity
in queries and documents using a zero-shot ap-
proach. MixGR fuses various metrics based
on these granularities to a united score that re-
flects a comprehensive query-document similar-
ity. Our experiments demonstrate that MixGR
outperforms previous document retrieval by
24.7% and 9.8% on nDCG@5 with unsuper-
vised and supervised retrievers, respectively,
averaged on queries containing multiple sub-
queries from five scientific retrieval datasets.
Moreover, the efficacy of two downstream sci-
entific question-answering tasks highlights the
advantage of MixGR to boost the application
of LLMs in the scientific domain. The code
and experimental datasets are available.1
1 Introduction
Recent advances in Large Language Models
(LLMs) have significantly impacted various sci-
entific domains (Zhang et al., 2022; Touvron et al.,
2023; Birhane et al., 2023; Grossmann et al., 2023).
However, LLMs are notorious for their tendency
to produce hallucinations, generating unreliable
outputs (Ji et al., 2023). To address this, Retrieval-
Augmented Generation (RAG; Lewis et al. 2020)
has been developed to address this issue by incor-
porating external knowledge during the generation.
Though notable for accessing external and rel-
evant knowledge, dense retrievers face specific
1https://github.com/TRUMANCFY/MixGR
12345678910
#Subqueries0200400600FrequencyNatural Questions
12345678910
#Subqueries0100200300FrequencyNFCorpus(a) Subquery distribution of general and scientific queries: scien-
tific queries, e.g., NFCorpus (Boteva et al. 2016, Right ), demon-
strate a more diverse range of subqueries per query than general
queries, e.g., Natural Questions (Kwiatkowski et al. 2019, Left).
(b) Comparison between general and scientific query-doc re-
trieval: compared with the general query-doc retrieval exempli-
fied by NQ (Kwiatkowski et al. 2019, Left), the scientific query-
doc retrieval exemplified by SciFact (Wadden et al. 2020, Right )
demonstrates that one query can be decomposed to multiple sub-
queries, which can be mapped to different parts of documents.
Figure 1: Scientific document retrieval is shown to be
more complicated than general domains.
challenges in the scientific domain: (1) Domain-
specific nature: dense retrievers are typically
trained on the general corpus such as Natural Ques-
tions (NQ; Kwiatkowski et al. 2019). However, sci-
entific domains differ notably, e.g., the terminology
and the pattern of queries as shown in Figure 1a. (2)
Complexity of scientific documents: they are long,
structured (Erera et al., 2019) and contain com-
plex relationships between arguments (Kirschner
et al., 2015). Figure 1a demonstrates that scientific
queries tend to contain more subqueries than those
in general domains. This indicates that subqueries
within a single query may align with different partsarXiv:2407.10691v2  [cs.IR]  1 Nov 2024Figure 2: The illustration of MixGR : Both queries and documents (e.g., the query-doc pair from SciFact in Figure
1b) are decomposed into subqueries and propositions, respectively, each containing distinct semantic components.
Starting from the original queries and documents along with their decomposed elements, metrics from various
granularity combinations are fused into a single integrated score.
of a document (doc), resulting in complex interac-
tions between queries and documents (Figure 1b).
Such complexity poses significant challenges for
dense retrievers (Lupart et al., 2023). Addressing
these challenges requires specific training on the
scientific corpus. However, this is often hindered
by the necessity of extensive annotations (Wadden
et al., 2020) and extra computation (Wang et al.,
2021a).
In this study, we introduce a novel zero-shot ap-
proach that effectively adapts dense retrievers to sci-
entific domains. This method specifically addresses
the complexities arising from the composition of
scientific queries and their consequent intricate re-
lationships with documents. Inspired by Chen et al.
(2023), showing that finer units improve retrievers’
generalization to rare entities, we incorporate more
granular retrieval units, specifically propositions
(prop), to address domain-specific challenges as
shown in Figure 2. Given the complexity between
scientific queries and documents (Figure 1b), we
also consider finer units within queries–subqueries–
to measure query-doc similarity at a finer granular-
ity. This metric captures the similarity between sub-
queries and propositions, moving beyond simple
point similarity between query-doc vectors. Given
a query, the distribution of corresponding infor-
mation within a document is unknown. Addition-
ally, our empirical analysis reveals that similari-
ties at various granularities provide complementary
insights. Therefore, for each query-doc pair, we
fuse the metrics from these granularities to a uni-
fied score, termed Mixed-Granularity Retrieval asMixGR , as depicted in Figure 2.
We conducted document retrieval experiments
on five scientific datasets using six dense retriev-
ers, comprising two unsupervised and four su-
pervised models. Our results demonstrate that
MixGR markedly surpasses previous query-doc re-
trieval methods. Notably, we recorded an average
improvement of 24.7% for unsupervised retriev-
ers and 9.8% for supervised retrievers in terms
of nDCG@5 for queries involving multiple sub-
queries. Furthermore, documents retrieved via
MixGR substantially enhance the performance of
downstream scientific QA tasks, underscoring their
potential utility for RAG within scientific domains.
Our contributions are three-fold:
•We identify the challenges within scientific docu-
ment retrieval, i.e., domain shift and query-doc
complexity. We initiate retrieval with mixed gran-
ularity within queries and documents to address
these issues;
•We propose MixGR , which further incorporates
finer granularities within queries and documents,
computes query-doc similarity over various gran-
ularity combinations and fuses them as a united
score. Our experiments across five datasets
and six dense retrievers empirically reveal that
MixGR significantly enhances existing retrievers
on the scientific document retrieval and down-
stream QA tasks;
•Further analysis demonstrates the complementar-
ity of metrics based on different granularities and
the generalization of MixGR in retrieving unitsfiner than documents.
2 Preliminary and Related works
Generalization of Dense Retrievers Dense re-
trievers generally employ a dual-encoder frame-
work (Yih et al., 2011; Reimers and Gurevych,
2019) to separately encode queries and documents
into compact vectors and measure relevance using a
non-parametric similarity function (Mussmann and
Ermon, 2016). However, the simplicity of the simi-
larity function (e.g., cosine similarity) can restrict
expressiveness, leading to suboptimal generaliza-
tion in new domains such as scientific fields that
differ from original training data (Thakur et al.,
2021). To improve dense retrievers’ adaptability
across tasks, researchers have used data augmen-
tation (Wang et al., 2022; Lin et al., 2023; Dai
et al., 2023), continual learning (Chang et al., 2020;
Sachan et al., 2021; Oguz et al., 2022), and task-
aware training (Xin et al., 2022; Cheng et al., 2023).
However, these methods still require training on
domain-specific data, incurring additional compu-
tational costs. This work focuses on zero-shot gen-
eralization of dense retrievers to scientific fields by
incorporating multi-granularity similarities within
queries and documents.
Granularity in Retrieval For dense retrieval, the
selection of the retrieval unit needs to balance the
trade-off between completeness and compactness.
Coarser units, like documents or fixed-length pas-
sages, theoretically encompass more context but
may introduce extraneous information, adversely
affecting retrievers and downstream tasks (Shi
et al., 2023; Wang et al., 2023). Conversely, finer
units like sentences are not always self-contained
and may lose context, thereby hindering retrieval
(Akkalyoncu Yilmaz et al., 2019; Yang et al., 2020).
Additionally, some studies extend beyond com-
plete sentences; for example, Lee et al. (2021a) use
phrases as learning units to develop corresponding
representations. Meanwhile, ColBERT (Khattab
and Zaharia, 2020) addresses token-level query-doc
interaction but is hampered by low efficiency.
Chen et al. (2023) propose using propositions
as retrieval units, defined as atomic expressions
of meaning (Min et al., 2023). These units are
contextualized and self-contained, including nec-
essary context through decontextualization, e.g.,
coreference resolution (Zhang et al., 2021). Propo-
sition retrieval improves retrieval of documents
with long-tail information, potentially benefitingdomain-specific tasks. This motivates the use of
propositions as retrieval units for scientific docu-
ment retrieval. Furthermore, we extend fine granu-
larity to queries and enhance the query-doc similar-
ity measurement, moving from a point-wise assess-
ment between two vectors to integrating multiple
query-doc granularity combinations.
Fusion within Retrieval Each type of retriever,
sparse or dense, has its own strength and can be
complementary with each other. Based on this in-
sight, previous studies have explored the fusion
of searches conducted by different retrievers as a
zero-shot solution for domain adaptation (Thakur
et al., 2021). A common method involves the con-
vex combination, which linearly combines simi-
larity scores (Karpukhin et al., 2020; Wang et al.,
2021b; Ma et al., 2021). However, this approach is
sensitive to the weighting of different metrics and
score normalization, which complicates configura-
tion across different setups (Chen et al., 2022).
In this work, we enhance retrieval by integrating
searches across various query and document granu-
larity levels for a given retriever. To avoid the limi-
tations of convex combination on parameter search-
ing, we use Reciprocal Rank Fusion (RRF; Cor-
mack et al. 2009), a robust, non-parametric method
(Chen et al., 2022), to aggregate these searches.
3MixGR : Mix-Granularity Retrieval
3.1 Finer Units in Queries and Documents
We first decompose queries and documents into
atomic units, i.e., subqueries and propositions, re-
spectively. A proposition (or subquery) should
meet the following three principal criteria (Min
et al., 2023):
•Each proposition conveys a distinct semantic unit,
collectively expressing the complete meaning.
• Propositions should be atomic and indivisible.
•According to Choi et al. (2021), propositions
should be contextualized and self-contained, in-
cluding all necessary text information such as
resolved coreferences for clear interpretation.
Here, we employ an off-the-shelf model, propo-
sitioner ,2for decomposing queries and documents
(Chen et al., 2023). This model is developed by
distilling the decomposition capacities of GPT-4
2https://huggingface.co/chentong00/
propositionizer-wiki-flan-t5-largeQuery Document
Accuracy (%) 96.3 94.7
IAA (%) 92.0 89.0
Table 1: Human-evaluated accuracy of query/document
decomposition by propositioner (Chen et al., 2023).
(Achiam et al., 2023) to a Flan-T5-Large model
(Chung et al., 2024) using Wikipedia as the corpus.
We sample decomposition results from 100 queries
and 100 documents from the datasets in §4.1 and
manually label the correctness of decomposition
as shown in Table 1. This model is shown to ef-
fectively decompose queries and documents into
atomic units within scientific domains. Please see
Appendix B for further details.
3.2 Multi-Granularity Similarity Calculation
Given these various granularities including queries,
subqueries, documents, and propositions, we ex-
tend the query-doc similarity metrics to include
measurements across different combinations of
granularities as depicted in Figure 2.
Notations The sets of queries and documents are
denoted as QandD, respectively. Given a retriever
s, the similarity between a query q∈ Q and a
document d∈ D is denoted as s(q,d). A docu-
ment dcan be decomposed to Npropositions, i.e.,
d = [ d1, ..., d N]. And a query qcan be decom-
posed to Msubqueries, i.e., q = [q1, ..., q M].
Query-doc sq-d:The direct and original similar-
ity between qanddissq-d(q,d)≡s(q,d).
Query-prop sq-p:Recent works (Chen et al.,
2023) determine query-doc similarity by calculat-
ing the maximum similarity between the query and
individual propositions within the document (Lee
et al., 2021b; Chen et al., 2023). The computation
of this metric, denoted as sq-p, is as follows:
sq-p(q,d) = max
i=1,...,N{s(q, di)}. (1)
Subquery-prop ss-p:Considering that different
parts of a query may be captured by various propo-
sitions within a document shown in Figure 1b, we
further assess query-doc similarity by analyzing
the relationships between subqueries and individ-
ualpropositions . The similarity between a query
and a document can be defined as the average simi-
larity across subqueries, calculated by identifying
the maximum similarity between one subquery andeach proposition, in analogy to MaxSim in Col-
BERT (Khattab and Zaharia, 2020). This metric,
represented by ss-p, is calculated as:
ss-p(q,d) =1
MMX
i=1max
j=1,...,N{s(qi, dj)}.(2)
3.3 Reciprocal Rank Fusion
We then use RRF to fuse these metrics across dif-
ferent query and document granularities:
sf(q,d) =1
1 +rq-d(q,d)+1
1 +rq-p(q,d)
+1
1 +rs-p(q,d), (3)
where rq-d,rq-p,rs-p∈R≥0signify the rank of
the retrieve results by sq-d,sq-p, and ss-p, respec-
tively. Technically, we retrieve the top- kresults
Rk
q-d,Rk
q-p, and Rk
s-pbysq-d,sq-p, and ss-p, respec-
tively, where kis set 200 empirically. When a
query-doc pair ( q′,d′) in one retrieval result does
not exist in the other sets (e.g., (q′,d′)∈Rk
q-d
but(q′,d′)/∈Rk
q-p), we will calculate the missing
similarity (e.g., sq-p(q′,d′)) before aggregation.
4 Experimental Setting
4.1 Scientific Retrieval Datasets
We evaluate our approach on five different scien-
tific retrieval tasks, including BioASQ (Tsatsaronis
et al., 2015), NFCorpus (Boteva et al., 2016), Sci-
Docs (Cohan et al., 2020), SciFact (Wadden et al.,
2020), and SciQ (Welbl et al., 2017), as shown in
Table 4 in Appendix A. We employ the proposi-
tioner released by Chen et al. (2023) mentioned in
§3.1 to break down both queries and documents
into atomic units. As we focus with priority on
query-doc complexity in scientific domains, we
report the experiments and analysis on the sub-
set of the queries that contain multiple subqueries.
For queries containing only a single subquery, we
also assess the methodology of MixGR by omit-
tingss-p. The detailed results are shown in Table
8 in Appendix F.4. The results encompassing all
queries, ones containing both single and multiple
subqueries, are detailed in Table 8 located in Ap-
pendix F.4.
4.2 Dense Retrievers
We evaluate the performance of six off-the-shelf
dense retrievers, both supervised and unsupervised.Retriever SetupBioASQ NFCorpus SciDocs SciFact SciQ Avg.
ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20
Unsupervised Dense Retrievers
SimCSEsq-d 17.0 17.0 16.2 13.3 7.6 9.7 27.1 31.2 62.3 67.3 26.0 27.7
sq-p 28.4 28.2 20.0 16.4 8.2 11.1 32.8 37.2 75.6 78.5 33.0 34.3
ss-p 31.1 30.6 22.8 18.3 7.3 10.5 32.7 36.9 80.9 83.2 35.0 35.9
MixGR 30.7 31.3 22.3 18.1 9.1 12.2 34.8 39.8 84.0 85.5 36.2 +39.2% 37.4 +35.0%
Contrieversq-d 64.8 68.3 42.2 34.9 13.5 18.5 64.5 68.5 67.2 70.0 50.5 52.0
sq-p 64.1 68.2 43.0 35.5 14.5 19.4 64.0 68.9 79.7 81.0 53.1 54.6
ss-p 63.7 68.0 41.4 34.9 13.5 18.3 63.2 67.5 83.6 84.6 53.1 54.6
MixGR 67.0 71.7 44.0 37.1 15.5 20.7 66.4 71.0 85.2 86.7 55.6 +10.1% 57.5 (+10.6%)
Supervised Dense Retrievers
DPRsq-d 39.1 39.1 25.1 20.7 7.3 10.4 31.8 37.7 60.6 64.1 32.8 34.4
sq-p 43.1 43.8 25.2 20.6 7.8 10.6 36.1 40.5 63.6 67.9 35.2 36.7
ss-p 41.1 41.7 26.5 21.4 6.4 10.0 37.1 41.3 67.7 70.7 35.8 37.0
MixGR 44.6 45.8 27.7 22.9 8.2 11.5 39.4 43.6 73.6 76.1 38.7 +18.0% 40.0 +14.3%
ANCEsq-d 48.8 48.2 29.9 24.4 9.3 13.1 41.5 45.3 66.4 69.1 39.2 40.0
sq-p 53.0 53.7 29.4 24.0 9.2 12.9 43.3 46.4 62.3 66.4 39.4 40.7
ss-p 49.0 49.8 30.3 24.5 7.5 11.9 43.5 47.3 66.1 69.1 39.3 40.5
MixGR 53.4 54.7 31.9 25.9 9.6 14.1 46.8 49.9 74.4 76.8 43.2 +10.2% 44.3 +10.4%
TAS-Bsq-d 68.8 70.1 42.3 34.1 13.8 19.3 60.1 65.6 84.8 86.3 54.0 55.1
sq-p 70.9 72.3 42.5 34.4 14.3 18.1 60.7 64.4 85.6 86.3 54.8 55.1
ss-p 67.0 69.4 40.9 33.1 12.6 17.2 61.7 65.0 85.3 86.6 53.5 54.3
MixGR 71.7 74.0 43.6 35.2 14.0 19.6 62.7 66.9 90.5 91.0 56.5 +4.6% 57.3 +4.0%
GTRsq-d 63.5 63.1 42.1 34.1 13.6 18.9 58.3 62.2 83.3 84.4 52.2 52.5
sq-p 65.9 67.8 42.3 34.4 13.2 18.0 60.6 63.3 85.8 86.5 53.6 54.0
ss-p 61.2 63.5 41.5 33.6 11.6 16.2 58.4 62.0 88.5 89.0 52.3 52.9
MixGR 66.8 68.6 43.3 35.6 13.6 19.2 60.9 64.5 92.9 93.0 55.5 +6.3% 56.2 +7.0%
Table 2: Document Retrieval Performance (nDCG@ k= 5, 20 in percentage, abbreviated as ND@ k): We evaluated
five distinct scientific retrieval datasets using two unsupervised and four supervised retrievers. The retrieval results
were compared among various metrics: sq-d(previous query-doc similarity), sq-p(Chen et al., 2023), ss-p, and
MixGR , as detailed in §3.2. Bold presents the best performance across the metrics, while underline denotes the
second-best performance. MixGR outperforms all three other metrics, where the percentage in parentheses indicates
the relative improvement compared with sq-d.
Supervised retrievers are trained using human-
labeled query-doc pairs in general domains,3while
unsupervised models do not require labeled data.
These retrievers encode the queries and index the
corpus at both document and proposition levels:
•SimCSE (Gao et al., 2021) employs a BERT-base
(Devlin et al., 2019) encoder trained on randomly
selected unlabeled Wikipedia sentences.
•Contriever (Izacard et al., 2022) is an unsuper-
vised retriever evolved from a BERT-base en-
coder, contrastively trained on segments from
unlabelled web and Wikipedia documents.
•DPR (Karpukhin et al., 2020) is built with a dual-
encoder BERT-base architecture, finetuned on a
suite of open-domain datasets with labels, such
as SQuAD (Rajpurkar et al., 2016).
•ANCE (Xiong et al., 2021) mirrors the configu-
ration of DPR but incorporates a training scheme
of Approximate Nearest Neighbor Negative Con-
trastive Estimation (ANCE).
3The supervised retrievers used in our experiment have not
been trained on these five datasets.•TAS-B (Hofstätter et al., 2021) is a dual-encoder
BERT-base model distilled from ColBERT on
MS MARCO (Bajaj et al., 2016).
•GTR (Ni et al., 2022) is a T5-base encoder, focus-
ing on generalization, pre-trained on unlabeled
QA pairs, and fine-tuned on labeled data includ-
ing MS MARCO.
More details on retrievers and experimental se-
tups are presented in Appendices C and D.
4.3 Document Retrieval Evaluation
We assess the performance of MixGR in the task of
document retrieval. Due to input length limitations
for retrievers (Karpukhin et al., 2020), we divide
each document into fixed-length chunks of up to
128 words. In practice, for MixGR and baselines,
we identify the retrieved chunks, map them back
to their original documents, and return the top- k
documents. We use Normalised Cumulative Dis-
count Gain (nDCG@k) as the evaluation metrics
for document retrieval. Unlike Recall@k, which
only indicates the presence of golden documents in
the retrieved list, nDCG@k also accounts for boththe ranking of retrievals and the relevance judg-
ment of golden documents (Thakur et al., 2021).
The baselines will be the metrics containing the ho-
mogeneous granularity introduced in the previous
section, i.e., sq-d,sq-pandss-p.
4.4 Downstream QA Evaluation
As previously mentioned, scientific documents are
vital for LLMs due to the rapid advancements in
science and the limited availability of such con-
tent in training datasets. To better understand how
MixGR enhances downstream QA tasks, we im-
plement the retrieval-then-read approach on two
datasets SciQ and SciFact. We retrieve and rank the
top-kdocuments based on scores, sq-dandMixGR ,
then concatenate them to form the context. During
our evaluations, we limit the number of document
chunks retrieved to 1 and 3—thus, only the top
kdocuments are injected into the reader model.
We assess the performance by measuring the Ex-
act Match (EM) rate—the proportion of responses
where the predicted answer perfectly aligns with
the reference answer (Kamalloo et al., 2023), de-
noted as EM@ k. Specifically, we utilize LLama-3-
8B-Instruct4(Touvron et al., 2023) as the reader
model. We take the original query-doc retrieval
setup, i.e., retrieval based on sq-d, as the baseline.
Please refer to Appendix E for more details.
5 Results
This section analyzes the impact of mixed-
granularity retrieval on document retrieval and
downstream applications. We highlight the effec-
tiveness of our proposed fine-grained and mixed-
granularity approaches in enhancing performance
across various metrics.
5.1 Document Retrieval
Table 2 reports the results of document retrieval.
We observe that retrieval by MixGR outperforms
all single-granularity retrieval with both unsuper-
vised and supervised dense retrievers in most cases.
With unsupervised retrievers, MixGR signifi-
cantly outperforms the query-doc similarity, sq-d,
across all five datasets. There is an aver-
age nDCG@5 improvement of +10.2and+5.1
(39.2%and10.1%relatively) for SimCSE and Con-
triever, respectively.
4https://huggingface.co/meta-llama/
Meta-Llama-3-8B-Instruct
BioASQ SciFact SciDocs NFCorpus SciQ020406080NDCG@20(%)w/o MixGR
w/ MixGR
BM25Figure 3: Comparison between BM25 and Contriever
(w/ and w/o MixGR ) on nDCG@20: Contriever w/
MixGR outperforms BM25 in three out of five datasets.
With supervised retrievers, improvements associ-
ated with MixGR are also observed, although they
are not as significant as with unsupervised retriev-
ers. This indicates that MixGR effectively narrows
the distributional gap between dense retrievers and
scientific domains.
Unsupervised retrievers benefit more from
MixGR than supervised ones. Remarkably, with
MixGR , the unsupervised retriever Contriever out-
performs supervised models, as evidenced by its
superior average results across five datasets on
nCCG@20. This result is particularly significant
given that Contriever typically underperforms com-
pared to TAS-B and GTR when evaluated using tra-
ditional query-document similarity measures. Ad-
ditionally, the study (Thakur et al., 2021) reveals
that sparse retrievers like BM25 often excel over
dense retrievers in domain-specific retrieval tasks.
As shown in Figure 3, Contriever outperforms
BM25 in three out of five datasets when applied
withMixGR . These findings emphasize the sub-
stantial enhancements that MixGR contributes to
unsupervised retrievers within scientific domains.
Finer granularity helps retrieval more. Among
three metrics within MixGR , the query-proposition
measurement sq-pand the subquery-proposition
measurement ss-pshow a general advantage over
the original query-doc similarity, as highlighted
by the underlined results in Table 2. The original
query-doc metric, sq-d, outperforms the subquery-
proposition measurement only when using the re-
triever TAS-B. These findings corroborate and ex-
pand upon Chen et al. (2023), suggesting that finer
query-doc similarity measurement significantly im-
proves document retrieval performance.
5.2 Downstream QA Tasks
Table 3 reports the results of scientific question an-
swering when the documents retrieved by MixGRSetupSciFact SciQ
EM@1 EM@3 EM@1 EM@3
Unsupervised Dense Retrievers
SimCSEsq-d 50.0 61.6 54.7 58.2
MixGR 48.3 62.8 61.3 66.4
Contrieversq-d 63.4 75.6 53.9 63.3
MixGR 64.0 70.9 61.7 66.0
Supervised Dense Retrievers
DPRsq-d 51.2 59.9 52.0 57.4
MixGR 51.7 65.7 57.4 62.5
ANCEsq-d 51.7 65.1 52.7 59.4
MixGR 57.6 69.2 54.7 62.9
TAS-Bsq-d 62.8 74.4 60.5 66.4
MixGR 62.2 70.3 64.5 67.6
GTRsq-d 61.0 72.1 59.8 64.8
MixGR 62.8 73.8 64.1 66.0
Table 3: Scientific Question Answering on SciFact and
SciQ using Llama-3-8B-Instruct (Touvron et al., 2023):
the top-1 and 3 document chunks retrieved by retrievers,
following the metrics sq-dandMixGR , were fed into
the reader. Bold indicates the better performance.
are fed into LLMs, i.e. the readers. It is observed
that EM scores achieved with MixGR generally
surpass those of the baseline across two datasets,
six dense retrievers, and multiple numbers of in-
put documents. This underscores the effectiveness
ofMixGR in enhancing the performance of down-
stream QA tasks.
6 Analysis
In this section, we explore the complementary ad-
vantages of various similarity metrics across multi-
ple granularities within MixGR through an ablation
study. Although the finer-granularity metric, ss-p,
generally enhances performance as previously dis-
cussed, it can occasionally result in degradation
when compared to original query-document simi-
larity sq-d. We identify specific conditions under
which the finer-granularity metric offers greater
benefits. Previous works (Chen et al., 2023) primar-
ily explored multiple granularities in documents .
We conduct a controlled experiment to highlight
the significance of incorporating multiple granular-
ities in queries in the MixGR framework, which
also validate the generalization of MixGR on the
retrieval units finer than documents.
6.1 Ablation Study
In our ablation study, we conducted a systematic
evaluation of the impact of various granularity
measures— sq-d(query-doc similarity), sq-p(query-
prop similarity), and ss-p(subquery-prop similar-
ity)—on the performance of six retrievers. By in-
w/o s-p w/o q-p w/o q-d MixGR0.560.570.58BioASQ
w/o s-p w/o q-p w/o q-d MixGR0.2800.2850.290NFCorpus
w/o s-p w/o q-p w/o q-d MixGR0.1540.1560.1580.1600.162SciDocs
w/o s-p w/o q-p w/o q-d MixGR0.5450.5500.5550.5600.565SciFact
w/o s-p w/o q-p w/o q-d MixGR0.800.820.84SciQFigure 4: Ablation study of MixGR on the nDCG@20
metrics averaged on six retrievers: MixGR achieves op-
timal performance when combining these three metrics,
indicating their complementary nature.
dividually omitting each of these measures from
the calculation of MixGR as defined in Equation
3, we assessed the significance of each granular-
ity level. Specifically, the extent of performance
degradation upon removal of a measure indicates
its importance; greater degradation suggests higher
importance of that particular granularity metric.
As illustrated in Figure 4, the nDCG@20 perfor-
mance declined across all three setups and datasets,
demonstrating that the metrics are complementary
to each other. The degree of performance degra-
dation varied across different configurations, high-
lighting the importance of each granularity mea-
sure. Notably, the most significant declines in per-
formance consistently occurred in configurations
excluding sq-dandss-p. This observation suggests
thatsq-p, while beneficial, is probably the least
critical measure for retrieval tasks in scientific do-
mains. Please refer to Table 6 in Appendix F.1 for
detailed results.
6.2 When is finer granularity beneficial?
Therefore, to more effectively compare the impacts
ofsq-dandss-p, we categorized the correctly re-
trieved pairs (complex query,5doc) by MixGR in
SciFact, using SimCSE, into two distinct groups:
•rq-d≻rs-p: The query-doc rank of sq-dis
higher than the subquery-prop rank of ss-p;
•rq-d≺rs-p: The query-doc rank of sq-dis
lower than the subquery-prop rank of ss-p.
5We refer complex query as the query containing no fewer
than three subqueries.2 4 6 8 10 12 14
#prop in one doc0.000.050.100.150.200.25rqd rsp
rqd rsp
Figure 5: Distribution of proposition number within
documents in two sets. There are more propositions
within document when rq-d≺rs-pthanrq-d≻rs-p.
Upon analyzing the number of propositions in
documents, a significant pattern emerges: based on
the distributions present in Figure 5, the number of
propositions in rq-d≺rs-pis generally higher than
inrq-d≻rs-p. This underscores the importance of
incorporating finer units within documents, espe-
cially for those containing more propositions, and
suggests potential degradation in dense retrievers
when handling such documents. For other retriev-
ers’ results, please refer to Appendix F.3.
6.3 MixGR on Proposition Retrieval
Previous sections present the effectiveness of
MixGR on scientific document retrieval. While pre-
vious works (Chen et al., 2023) focus on finer docu-
ment granularity, we specifically assess MixGR on
the proposition as the retrieval units. This con-
trolled study highlights the benefits of MixGR ,
which incorporates different granularities within
queries and documents, in general text retrieval
beyond document-level granularity.
For a given query qand a proposition p, the con-
ventional similarity is denoted by sp
q-p≡s(q,p).
When the query is further broken down into mul-
tiple sub-queries, we introduce a finer granularity
measure, sp
s-p, which is defined as the maximum
similarity between these sub-queries and the propo-
sition. sp
s-pis mathematically defined as follows:
sp
s-p(q,p) = max
i=1,...,M{s(qi,p)}. (4)
Therefore, the merged score by RRF, sp
f(q,p),
is calculated as:
sp
f(q,p) =1
1 +rp
q-p(q,p)+1
1 +rp
s-p(q,p),(5)
where rp
q-pandrp
s-psignify the rank of the re-
trieve results by sp
q-pandsp
s-p, respectively.
Following sp
q-p(q,p)andsp
f(q,p), we input the
first 50 and 200 words in propositions retrieved
SciFact SciQ405060EM@50
EM@200
query-prop
MixGREM@50
EM@200
query-prop
MixGRFigure 6: Proposition retrieval with MixGR : We evalu-
ate Exact Match of LLama-3-8B-Instruct on SciFact and
SciQ with the first 50 and 200 words of propositions,
i.e., EM@50 and EM@200, retrieved by SimCSE as
the context. Please refer to Table 7 for other retrievers
in Appendix F.2.
with SimCSE on SciFact and SciQ into the reader
LLama-3-8B-Instruct. This process adheres to the
same setups outlined in §4.4. As shown in Figure
6, the performance advance observed with mixed-
granularity retrieval on propositions, compared to
the original query-prop similarity, demonstrates
the effectiveness of using mixed-granularity in re-
trieval. This substantiates the generalizability of
MixGR beyond document-level granularity. Please
refer to Appendix F.2 for details.
6.4 Prospect: Adaptive MixGR
Here, we outline potential future research direc-
tions. In §6.1, we observed the complementary
nature of retrieval results achieved using different
granularities. Additionally, as discussed in §6.2,
we noted a distinct pattern where retrieval guided
by a specific granularity outperforms others. These
findings indicate that metrics based on different
granularities each have relatively distinct strengths
in specific contexts, presenting opportunities for
further exploration. Unlike the non-parametric
method of fusion by RRF, which overlooks the
relative importance of components, an adaptive ap-
proach could enhance fusion and, consequently, im-
prove retrieval performance with dense retrievers–a
prospect we aim to explore in future research.
7 Conclusion
In this work, we identify key challenges for
dense retrievers in scientific document retrieval,
namely domain shift and query-document complex-
ity. In response, we propose a zero-shot approach,
MixGR , that utilizes atomic components in queries
and documents to calculate their similarity with
greater nuance. We then use Reciprocal Rank Fu-
sion (RRF) to integrate these metrics, modeling
query-doc similarity at different granularities intoa unified score that enhances document retrieval.
Our experiments demonstrate that MixGR sig-
nificantly enhances the existing dense retriever on
document retrieval within the scientific domain.
Moreover, MixGR has proven beneficial for down-
stream applications such as scientific QA. The anal-
ysis reveals a synergistic relationship among the
components of MixGR , and suggests evolving our
non-parametric fusion framework into a more gen-
eral method as a future research direction.
Limitations
Our work explores retrieval guided by an integral
metric that incorporates various levels of granular-
ity. We identify several limitations in our approach:
(1)Coverage of Retrievers : Our study categorizes
dense retrievers into supervised and unsupervised
models, yet all utilize a dual-encoder structure. Fu-
ture studies could include a more diverse array of
retriever architectures. (2) Coverage of Domains :
While our main focus is on the scientific domain,
and we extend to three additional domains in Ap-
pendix G, there are still many domains we have
not explored. (3) Languages : Our research is lim-
ited to an English corpus. The applicability of
MixGR in multilingual contexts also deserves fur-
ther validation and exploration. (4) Based on our
error analysis in Appendix B.2, there is potential
for improving the tool used for query and corpus
decomposition.
Ethical Statements
We foresee no ethical concerns and potential risks
in our work. All of the retrieval models and datasets
are open-sourced, as shown in Table 11 in Ap-
pendix H. The LLMs we applied in the experiments
are also publicly available. Given our context, the
outputs of LLMs are unlikely to contain harmful
and dangerous information.
Acknowledgement
We thank Kexin Wang, Tim Baumgärtner, and
Sherry Tongshuang Wu for the discussion on an
early draft of this work.
Fengyu Cai is funded by the German Federal
Ministry of Education and Research and the Hes-
sian Ministry of Higher Education, Research, Sci-
ence, and the Arts within their joint support of the
National Research Center for Applied Cybersecu-
rity ATHENE. Xinran Zhao is funded by the ONR
Award N000142312840.References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang,
and Jimmy Lin. 2019. Cross-domain modeling of
sentence-level evidence for document retrieval. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3490–
3496, Hong Kong, China. Association for Computa-
tional Linguistics.
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,
Jianfeng Gao, Xiaodong Liu, Rangan Majumder,
Andrew McNamara, Bhaskar Mitra, Tri Nguyen,
et al. 2016. Ms marco: A human generated ma-
chine reading comprehension dataset. arXiv preprint
arXiv:1611.09268 .
Abeba Birhane, Atoosa Kasirzadeh, David Leslie, and
Sandra Wachter. 2023. Science in the age of large
language models. Nature Reviews Physics , 5(5):277–
280.
Vera Boteva, Demian Gholipour, Artem Sokolov, and
Stefan Riezler. 2016. A full-text learning to rank
dataset for medical information retrieval. In Ad-
vances in Information Retrieval: 38th European Con-
ference on IR Research, ECIR 2016, Padua, Italy,
March 20–23, 2016. Proceedings 38 , pages 716–722.
Springer.
Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yim-
ing Yang, and Sanjiv Kumar. 2020. Pre-training tasks
for embedding-based large-scale retrieval. In Inter-
national Conference on Learning Representations .
Tao Chen, Mingyang Zhang, Jing Lu, Michael Bender-
sky, and Marc Najork. 2022. Out-of-domain seman-
tics to the rescue! zero-shot hybrid retrieval models.
InEuropean Conference on Information Retrieval ,
pages 95–110. Springer.
Tong Chen, Hongwei Wang, Sihao Chen, Wenhao
Yu, Kaixin Ma, Xinran Zhao, Dong Yu, and Hong-
ming Zhang. 2023. Dense x retrieval: What re-
trieval granularity should we use? arXiv preprint
arXiv:2312.06648 .
Hao Cheng, Hao Fang, Xiaodong Liu, and Jianfeng
Gao. 2023. Task-aware specialization for efficient
and robust dense retrieval for open-domain question
answering. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers) , pages 1864–1875, Toronto,
Canada. Association for Computational Linguistics.
Eunsol Choi, Jennimaria Palomaki, Matthew Lamm,
Tom Kwiatkowski, Dipanjan Das, and MichaelCollins. 2021. Decontextualization: Making sen-
tences stand-alone. Transactions of the Association
for Computational Linguistics , 9:447–461.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2024. Scaling instruction-finetuned language models.
Journal of Machine Learning Research , 25(70):1–53.
Arman Cohan, Sergey Feldman, Iz Beltagy, Doug
Downey, and Daniel Weld. 2020. SPECTER:
Document-level representation learning using
citation-informed transformers. In Proceedings
of the 58th Annual Meeting of the Association
for Computational Linguistics , pages 2270–2282,
Online. Association for Computational Linguistics.
Gordon V Cormack, Charles LA Clarke, and Stefan
Buettcher. 2009. Reciprocal rank fusion outperforms
condorcet and individual rank learning methods. In
Proceedings of the 32nd international ACM SIGIR
conference on Research and development in informa-
tion retrieval , pages 758–759.
Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo
Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall,
and Ming-Wei Chang. 2023. Promptagator: Few-
shot dense retrieval from 8 examples. In The Eleventh
International Conference on Learning Representa-
tions .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Shai Erera, Michal Shmueli-Scheuer, Guy Feigenblat,
Ora Peled Nakash, Odellia Boni, Haggai Roitman,
Doron Cohen, Bar Weiner, Yosi Mass, Or Rivlin, Guy
Lev, Achiya Jerbi, Jonathan Herzig, Yufang Hou,
Charles Jochim, Martin Gleize, Francesca Bonin,
Francesca Bonin, and David Konopnicki. 2019. A
summarization system for scientific documents. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP): System Demonstra-
tions , pages 211–216, Hong Kong, China. Associa-
tion for Computational Linguistics.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Igor Grossmann, Matthew Feinberg, Dawn C Parker,
Nicholas A Christakis, Philip E Tetlock, andWilliam A Cunningham. 2023. Ai and the transfor-
mation of social science research. Science , 380:1108–
1109.
Charles R. Harris, K. Jarrod Millman, Stéfan van der
Walt, Ralf Gommers, Pauli Virtanen, David Cour-
napeau, Eric Wieser, Julian Taylor, Sebastian Berg,
Nathaniel J. Smith, Robert Kern, Matti Picus,
Stephan Hoyer, Marten H. van Kerkwijk, Matthew
Brett, Allan Haldane, Jaime Fernández del Río, Mark
Wiebe, Pearu Peterson, Pierre Gérard-Marchant,
Kevin Sheppard, Tyler Reddy, Warren Weckesser,
Hameer Abbasi, Christoph Gohlke, and Travis E.
Oliphant. 2020. Array programming with numpy.
Nature , 585:357–362.
Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong
Yang, Jimmy Lin, and Allan Hanbury. 2021. Ef-
ficiently teaching an effective dense retriever with
balanced topic aware sampling. In Proceedings of
the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval ,
pages 113–122.
John D Hunter. 2007. Matplotlib: A 2d graphics en-
vironment. Computing in science & engineering ,
9(03):90–95.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-
tian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. 2022. Unsupervised dense informa-
tion retrieval with contrastive learning. Transactions
on Machine Learning Research .
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,
Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea
Madotto, and Pascale Fung. 2023. Survey of halluci-
nation in natural language generation. ACM Comput.
Surv. , 55(12):248:1–248:38.
Ehsan Kamalloo, Nouha Dziri, Charles Clarke, and
Davood Rafiei. 2023. Evaluating open-domain ques-
tion answering in the era of large language models.
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 5591–5606, Toronto, Canada.
Association for Computational Linguistics.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6769–6781,
Online. Association for Computational Linguistics.
Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-
cient and effective passage search via contextualized
late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research
and development in Information Retrieval , pages 39–
48.
Christian Kirschner, Judith Eckle-Kohler, and Iryna
Gurevych. 2015. Linking the thoughts: Analysisof argumentation structures in scientific publications.
InProceedings of the 2nd Workshop on Argumenta-
tion Mining , pages 1–11, Denver, CO. Association
for Computational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al. 2019. Natural questions: a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics , 7:453–
466.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-
cient memory management for large language model
serving with pagedattention. In Proceedings of the
ACM SIGOPS 29th Symposium on Operating Systems
Principles .
Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi
Chen. 2021a. Learning dense representations of
phrases at scale. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 6634–6647, Online. Association for
Computational Linguistics.
Jinhyuk Lee, Alexander Wettig, and Danqi Chen. 2021b.
Phrase retrieval learns passage retrieval, too. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 3661–
3672, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,
Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun
Chen. 2023. How to train your dragon: Diverse aug-
mentation towards generalizable dense retrieval. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2023 , pages 6385–6400, Singapore.
Association for Computational Linguistics.
Simon Lupart, Thibault Formal, and Stéphane Clinchant.
2023. Ms-shift: An analysis of ms marco distribution
shifts on neural retrieval. In European Conference
on Information Retrieval , pages 636–652. Springer.
Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin.
2021. A replication study of dense passage retriever.
arXiv preprint arXiv:2104.05740 .
Macedo Maia, Siegfried Handschuh, André Freitas,
Brian Davis, Ross McDermott, Manel Zarrouk, and
Alexandra Balahur. 2018. Www’18 open challenge:
financial opinion mining and question answering. InCompanion proceedings of the the web conference
2018 , pages 1941–1942.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2023. FActScore:
Fine-grained atomic evaluation of factual precision
in long form text generation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 12076–12100, Singa-
pore. Association for Computational Linguistics.
Stephen Mussmann and Stefano Ermon. 2016. Learning
and inference via maximum inner product search.
InInternational Conference on Machine Learning ,
pages 2587–2596. PMLR.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo
Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,
Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.
Large dual encoders are generalizable retrievers. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages
9844–9855, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick
Lewis, Vladimir Karpukhin, Aleksandra Piktus,
Xilun Chen, Sebastian Riedel, Scott Yih, Sonal
Gupta, and Yashar Mehdad. 2022. Domain-matched
pre-training tasks for dense retrieval. In Findings
of the Association for Computational Linguistics:
NAACL 2022 , pages 1524–1534, Seattle, United
States. Association for Computational Linguistics.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Köpf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch: An
imperative style, high-performance deep learning li-
brary. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, De-
cember 8-14, 2019, Vancouver, BC, Canada , pages
8024–8035.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.Devendra Sachan, Mostofa Patwary, Mohammad
Shoeybi, Neel Kant, Wei Ping, William L. Hamil-
ton, and Bryan Catanzaro. 2021. End-to-end training
of neural retrievers for open-domain question answer-
ing. In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
6648–6662, Online. Association for Computational
Linguistics.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H Chi, Nathanael Schärli,
and Denny Zhou. 2023. Large language models can
be easily distracted by irrelevant context. In Inter-
national Conference on Machine Learning , pages
31210–31227. PMLR.
Haitian Sun, William Cohen, and Ruslan Salakhutdinov.
2022. ConditionalQA: A complex reading compre-
hension dataset with conditional answers. In Pro-
ceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 3627–3637, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. BEIR:
A heterogeneous benchmark for zero-shot evaluation
of information retrieval models. In Thirty-fifth Con-
ference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2) .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. CoRR , abs/2307.09288.
George Tsatsaronis, Georgios Balikas, Prodromos
Malakasiotis, Ioannis Partalas, Matthias Zschunke,
Michael R Alvers, Dirk Weissenborn, Anastasia
Krithara, Sergios Petridis, Dimitris Polychronopou-
los, et al. 2015. An overview of the bioasq large-scale
biomedical semantic indexing and question answer-
ing competition. BMC bioinformatics , 16:1–28.Henning Wachsmuth, Shahbaz Syed, and Benno Stein.
2018. Retrieval of the best counterargument without
prior topic knowledge. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 241–251,
Melbourne, Australia. Association for Computational
Linguistics.
David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu
Wang, Madeleine van Zuylen, Arman Cohan, and
Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying
scientific claims. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 7534–7550, Online. As-
sociation for Computational Linguistics.
Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021a.
TSDAE: Using transformer-based sequential denois-
ing auto-encoderfor unsupervised sentence embed-
ding learning. In Findings of the Association for
Computational Linguistics: EMNLP 2021 , pages
671–688, Punta Cana, Dominican Republic. Associa-
tion for Computational Linguistics.
Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna
Gurevych. 2022. GPL: Generative pseudo labeling
for unsupervised domain adaptation of dense retrieval.
InProceedings of the 2022 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 2345–2360, Seattle, United States. Association
for Computational Linguistics.
Shuai Wang, Shengyao Zhuang, and Guido Zuccon.
2021b. Bert-based dense retrievers require interpo-
lation with bm25 for effective passage retrieval. In
Proceedings of the 2021 ACM SIGIR international
conference on theory of information retrieval , pages
317–324.
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan
Parvez, and Graham Neubig. 2023. Learning to filter
context for retrieval-augmented generation. arXiv
preprint arXiv:2311.08377 .
Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.
Crowdsourcing multiple choice science questions.
InProceedings of the 3rd Workshop on Noisy User-
generated Text , pages 94–106, Copenhagen, Den-
mark. Association for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019. Huggingface’s transformers: State-of-
the-art natural language processing. ArXiv preprint ,
abs/1910.03771.
Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita
Sharma, Damien Jose, and Paul Bennett. 2022. Zero-
shot dense retrieval with momentum adversarial do-
main invariant representations. In Findings of the As-
sociation for Computational Linguistics: ACL 2022 ,
pages 4008–4020, Dublin, Ireland. Association for
Computational Linguistics.Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul N. Bennett, Junaid Ahmed, and
Arnold Overwijk. 2021. Approximate nearest neigh-
bor negative contrastive learning for dense text re-
trieval. In International Conference on Learning
Representations .
Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,
Jax Law, Noah Constant, Gustavo Hernandez Abrego,
Steve Yuan, Chris Tar, Yun-Hsuan Sung, et al. 2020.
Multilingual universal sentence encoder for semantic
retrieval. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations , pages 87–94.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning , pages 247–256, Port-
land, Oregon, USA. Association for Computational
Linguistics.
Hongming Zhang, Xinran Zhao, and Yangqiu Song.
2021. A brief survey and comparative study of recent
development of pronoun coreference resolution in
English. In Proceedings of the Fourth Workshop on
Computational Models of Reference, Anaphora and
Coreference , pages 1–11, Punta Cana, Dominican
Republic. Association for Computational Linguistics.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .Appendix
A Datasets
A.1 General Setups
Different from the setup of the original dataset, we split one document into several chunks with a maximum
of 128 words. This is because some dense retrievers such as DPR (Karpukhin et al., 2020) have the
requirement of maximum input. Too long inputs will be overflow, leading to the loss of information.
The chunk selected can be used to locate the document in the original dataset during the evaluation.
Specifically, for SciQ, we reformulate the dataset from a QA task to a retrieval task. Originally, this task
aims to answer scientific questions given the context. We collect the contexts in training, validation and
test sets as the corpus.
Also, we will explain our motivation of focusing the queries containing subqueries:
•Chen et al. (2023) have studied the advantage of using propositions, i.e., the atomic units within
documents, as the retrieval units for a complete query. And MixGR will not affect the retrieval results
of single-subquery queries.
•In this work, we highlight the advantages of mixed-granularity retrieval that incorporates finer units in
both queries and documents. Queries containing multiple subqueries are particularly well-suited to our
research problem, as they will have different combinations with the documents.
A.2 Specific Setups
Here, we would specify the experimental setups for each dataset:
•BioASQ: In order to decrease the computational cost, we randomly sample the abstracts from the
original corpus covered by BEIR (Thakur et al., 2021), which contains 14 914 603 abstracts.
•NFCorpus: The query that we apply here is the description version including training, validation, and
test sets, instead of the keyword. These queries are not covered by BEIR (Thakur et al., 2021). Please
refer to the original datasets of NFCorpus (Boteva et al., 2016).
•SciFact: We apply the dataset covered by BEIR (Thakur et al., 2021), using both training and test splits.
• SciDocs: We use the dataset covered by BEIR (Thakur et al., 2021), using the test split.
•SciQ: We use the queries in the test sets, and collect the contexts in training, validation, and test sets as
the corpus.
For the documents (or propositions) of all the datasets, the format of the text is the concatenation of the
title and the content.
Statistic BioASQ (Tsatsaronis et al., 2015) NFCorpus (Boteva et al., 2016) SciDocs (Cohan et al., 2020) SciFact (Wadden et al., 2020) SciQ (Welbl et al., 2017)
#Query 3 743 1 016 1 000 1 109 884
#Multi-subquery queries 387 641 205 283 252
#Subqueries 821 3 337 522 614 874
#Documents 225 362 3 633 25 657 5 183 12 241
#Propositions 3 551 816 67 110 351 802 87 190 91 635
Table 4: Statistics for the BioASQ, NFCorpus, SciDocs, SciFact, and SciQ datasets. Please note that these statistics
have been adjusted to exclude any obvious decomposition errors.
B Query and Document Decomposition
Here, we will complement the necessary information regarding the query and document decomposition.B.1 Subquery and Proposition Examples
Here, we present examples of subqueries and propositions decomposed from the documents. The example
is the decomposition of the example in Figure 1.
Query: Citrullinated proteins externalized in neutrophil extracellular traps act indirectly to perpetu-
ate the inflammatory cycle via induction of autoantibodies.
• Subquery-0: Citrullinated proteins are externalized in neutrophil extracellular traps.
• Subquery-1: Citrullinated proteins act indirectly to perpetuate the inflammatory cycle.
• Subquery-2: The inflammatory cycle is perpetuated via induction of autoantibodies.
Document: RA sera and immunoglobulin fractions from RA patients with high levels of ACPA
and/or rheumatoid factor significantly enhanced NETosis, and the NETs induced by these au-
toantibodies displayed distinct protein content. Indeed, during NETosis, neutrophils externalized
the citrullinated autoantigens implicated in RA pathogenesis, and anti-citrullinated vimentin anti-
bodies potently induced NET formation. Moreover, the inflammatory cytokines interleukin-17A
(IL-17A) and tumor necrosis factor- α(TNF- α) induced NETosis in RA neutrophils. In turn, NETs
significantly augmented inflammatory responses in RA and OA synovial fibroblasts, including
induction of IL-6, IL-8, chemokines, and adhesion molecules. These observations implicate
accelerated NETosis in RA pathogenesis, through externalization of citrullinated autoantigens and
immunostimulatory molecules that may promote aberrant adaptive and innate immune responses
in the joint and in the periphery, and perpetuate pathogenic mechanisms in this disease.
•Proposition-0: RA sera and immunoglobulin fractions from RA patients with high levels of
ACPA and/or rheumatoid factor significantly enhanced NETosis.
• Proposition-1: NETs induced by these autoantibodies displayed distinct protein content.
•Proposition-2: During NETosis, neutrophils externalized the citrullinated autoantigens impli-
cated in RA pathogenesis.
• Proposition-3: Anti-citrullinated vimentin antibodies potently induced NET formation.
•Proposition-4: Interleukin-17A (IL-17A) and tumor necrosis factor- (TNF-) induced NETosis
in RA neutrophils.
•Proposition-5: NETs significantly augmented inflammatory responses in RA and OA synovial
fibroblasts.
•Proposition-6: NETs inducing IL-6, IL-8, chemokines, and adhesion molecules occurred in
RA and OA synovial fibroblasts.
• Proposition-7: These observations implicate accelerated NETosis in RA pathogenesis.
•Proposition-8: NETosis externalizes citrullinated autoantigens and immunostimulatory
molecules.
•Proposition-9: NETosis may promote aberrant adaptive and innate immune responses in the
joint and in the periphery.
• Proposition-10: NETosis may perpetuate pathogenic mechanisms in RA.B.2 Remarks on Propositioner
During our manual check on the decomposition results of propositioner (Chen et al., 2023), we find the
following potential flaws.
(1) Wrong logic during decomposition:
Query : Identification of Design Elements for a Maturity Model for Interorganizational Integration:
A Comparative Analysis
→Subqueries : [’Identification of Design Elements for a Maturity Model for Interorganizational
Integration.’, ’A Comparative Analysis is used for identifying design elements.’]
(2) Hallucination:
Query : Bigger ocean waves and waves that carry more sediment cause a greater extent of what?
→Subqueries : [’Bigger ocean waves cause a greater extent of erosion.’, ’Waves that carry more
sediment cause a greater extent of erosion.’]
(3) Information loss:
Query : The reduction was 1.6 ± 1.6 in controls. ...
→Subqueries : [’The reduction in migraine headache was 1.6 1.6 in controls.’, ...]
We find that the proposition will convert the questions to declarative sentences during decomposition.
This may stem from the fact that its training corpus is Wikipedia, where a small portion of sentences are
questions. Still, we find that propositioner can still decompose question-style queries, as shown in the
following example:
Query : What is the purpose of bright colors on a flower’s petals?
→Subqueries : ["The purpose of bright colors on a flower’s petals is unknown."]
What is more, the propositioner may decompose the query to a sequence of single characters, but it is
very rare: there are only four cases out of 4009 queries, i.e., around 0.1 % rate for this type of error.
B.3 Human Evaluation on Query and Document Decomposition
As mentioned in §3.1, we evaluate the decomposition outputs by propositioner (Chen et al., 2023), 100
samples for both query and document decomposition. Concretely, we ask three students at the post-
graduate levels to evaluate the results, who are paid above the local minimum hourly wage. The instruction
is shown below:
Propositions in documents (or subqueries in queries) are defined as follows:
•Each proposition conveys a distinct semantic unit, collectively expressing the complete meaning.
• Propositions should be atomic and indivisible.
•According to Choi et al. (2021), propositions should be contextualized and self-contained,
including all necessary text information such as coreferences for clear interpretation.
Given the document (query) and the corresponding propositions (subqueries) generated by the
model, please check whether the document (query) has been correctly decomposed.
Please write 1as correct, and 0as incorrect.Model HuggingFace Checkpoint
SimCSE (Gao et al., 2021) princeton-nlp/unsup-simcse-bert-base-uncased
Contriever (Izacard et al., 2022) facebook/contriever
DPR (Karpukhin et al., 2020) facebook/dpr-ctx_encoder-multiset-base
facebook/dpr-question_encoder-multiset-base
ANCE (Xiong et al., 2021) castorini/ance-dpr-context-multi
castorini/ance-dpr-question-multi
TAS-B (Hofstätter et al., 2021) sentence-transformers/msmarco-distilbert-base-tas-b
GTR (Ni et al., 2022) sentence-transformers/gtr-t5-base
Table 5: Model checkpoints released on HuggingFace. For DPR and ANCE, two different models encode the
context and query.
C Retrievers Models
Table 5 presents the dense retrievers applied in the experimental section, i.e., §4.
D Offline Indexing
Thepyserini andfaiss libraries were employed to convert retrieval units into embeddings. We
leveraged GPUs for encoding these text units in batches with a batch size of 64 and a floating precision of
f16. Following the preprocessing of these embeddings, all experiments conducted involved the utilization
of an exact search method for inner products using faiss.IndexFlatIP ,
E Downstream Tasks
The templates of LLama for downstream QA tasks, i.e., SciFact and SciQ, are listed as follows. For SciQ,
we convert it from multiple choice question answering to open question answering.
Given the knowledge source: context \\n Question: query \\n Reply with one phrase. \\n Answer:
As SciFact is a fact-checking task, we here check whether LLMs can predict the relationship between
the context and the claim. The template of SciFact is shown as follows:
Context: { context } \\n Claim: { query } \\n For the claim, the context is supportive, contradictory,
or not related? \\n Options: (A) Supportive (B) Contradictory (C) Not related \\n Answer:")
F Detailed Results
F.1 Ablation Study
As discussed in §6.1, we remove the component, i.e., query-doc similarity, query-prop similarity, or
subquery-prop similarity, and assess the corresponding performance compared with MixGR . In Table 6, it
is observed that MixGR outperforms all its components.
F.2 MixGR for Propositional Retrieval
Here, we evaluate MixGR on the retrieval units beyond documents, e.g., propositions, which Table 7
present. We observe that MixGR can outperform the previous document retrieval based on the similarity
between query and proposition, on proposition retrieval, as discussed in §6.3.
F.3 Advantageous pattern for finer granularity measurement
In Table 9, we can notice the average number of propositions in rq-d≺rs-pis more than rq-d≻rs-p.
This shows that the finer granularity can better deal with the documents with more propositions than the
original query-document similarity.Retriever SetupBioASQ NFCorpus SciDocs SciFact SciQ Avg.
ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20
Unsupervised Dense Retrievers
SimCSEw/oss-p 25.0 26.1 19.6 16.0 8.7 11.5 32.3 37.0 76.1 78.0 32.3 33.7
w/osq-p 27.6 28.6 21.4 17.4 8.5 11.6 33.1 37.4 77.9 79.6 33.7 34.9
w/osq-d 32.4 32.4 22.8 18.6 8.5 11.9 33.9 39.0 80.7 82.2 35.7 36.8
MixGR 30.7 31.3 22.3 18.1 9.1 12.2 34.8 39.8 84.0 85.5 36.2 37.4
Contrieverw/oss-p 66.5 70.5 43.6 36.2 14.8 20.0 65.6 69.9 78.0 80.1 53.7 55.3
w/osq-p 66.9 71.2 43.0 36.6 14.6 20.1 66.3 70.8 81.6 83.3 54.5 56.4
w/osq-d 66.1 70.4 43.2 36.3 14.7 20.0 65.0 69.5 83.3 84.8 54.5 56.2
MixGR 67.0 71.7 44.0 37.1 15.5 20.7 66.4 71.0 85.2 86.7 55.6 57.5
Supervised Dense Retrievers
DPRw/oss-p 43.0 44.3 26.5 21.9 8.2 11.2 35.0 40.8 66.6 69.9 35.9 37.6
w/osq-p 42.9 44.5 27.5 22.8 7.5 11.2 38.3 42.4 71.0 73.1 37.5 38.8
w/osq-d 44.0 45.0 26.6 22.2 8.0 11.2 38.0 42.1 69.5 72.2 37.2 38.5
MixGR 44.6 45.8 27.7 22.9 8.2 11.5 39.4 43.6 73.6 76.1 38.7 40.0
ANCEw/oss-p 52.7 52.9 30.7 25.2 10.0 13.7 45.8 48.9 69.0 72.0 41.7 42.6
w/osq-p 51.4 52.8 32.0 26.2 9.0 13.4 46.8 50.4 71.3 73.9 42.1 43.3
w/osq-d 52.9 54.1 30.8 25.1 8.8 13.4 44.9 48.6 67.8 70.1 41.0 42.3
MixGR 53.4 54.7 31.9 25.9 9.6 14.1 46.8 49.9 74.4 76.8 43.2 44.3
TAS-Bw/oss-p 71.3 73.2 42.9 34.7 13.8 19.2 61.4 66.7 86.7 87.0 55.2 56.2
w/osq-p 70.1 72.6 42.9 34.9 13.8 19.6 63.2 67.3 88.3 88.8 55.7 56.7
w/osq-d 70.0 72.6 42.7 34.5 13.6 18.8 62.1 65.3 85.2 85.9 54.8 55.4
MixGR 71.7 74.0 43.6 35.2 14.0 19.6 62.7 66.9 90.5 91.0 56.5 57.3
GTRw/oss-p 66.5 67.8 43.2 35.2 13.4 18.9 60.9 64.5 87.2 87.5 54.2 54.8
w/osq-p 65.4 67.0 43.0 35.5 13.8 19.5 60.6 64.7 88.4 88.5 54.2 55.0
w/osq-d 66.2 68.0 42.4 34.9 12.6 18.0 61.5 64.4 89.0 89.3 54.3 54.9
MixGR 66.8 68.6 43.3 35.6 13.6 19.2 60.9 64.5 92.9 93.0 55.5 56.2
Table 6: Ablation study (nDCG@ k= 5, 20 in percentage, abbreviated as ND@ k): We evaluated five distinct
scientific retrieval datasets using two unsupervised and four supervised retrievers. The retrieval results were
compared using various metrics: MixGR w/oss-q,MixGR w/osq-p,MixGR w/oss-p, andMixGR , as detailed in
§3.2.
F.4 Document retrieval for queries containing only a single subquery
In this experiment, we demonstrate the impact of MixGR on queries that contain only a single subquery.
Unlike queries with multiple subqueries, MixGR omits the similarity measurement from the perspectives
of subqueries and propositions, ss-p, during RRF. The result, presented in Table 8, includes the result of
document retrieval for the queries including either one single or multiple subqueries and illustrates the
effectiveness of MixGR across a general query format.
GMixGR for Other Domains
Our work provides a comprehensive analysis of the impact of MixGR on scientific text retrieval, con-
sidering both the variety of datasets and the use of dense retrievers. The applicability of MixGR to
other domains remains an open question. We explore this by conducting document retrieval experiments
on three distinct datasets: ConditionalQA (Sun et al., 2022), FiQA (Maia et al., 2018), and Arguana
(Wachsmuth et al., 2018), which belong to the domains of law, finance, and argumentation, respectively.
The results are detailed in Table 10. We observe that MixGR ’s benefits are considerably more limited, or
even negative, outside the scientific context. This disparity may be attributed to the varying degrees of
alignment between the domain-specific characteristics of each field and the training corpus of the dense
retrievers. Or, propositioner can not perform well in these domains. Such findings further underscore the
potentially distinct domain-specific nature of scientific document retrieval.
H Licences of Scientific ArtifactsSetupSciFact SciQ
EM@50 EM@200 EM@50 EM@200
Unsupervised Dense Retrievers
SimCSEsq-d 43.0 60.5 56.2 60.9
MixGR 45.3 62.2 59.0 63.3
Contrieversq-d 49.4 67.4 56.2 62.9
MixGR 47.7 71.5 57.4 62.5
Supervised Dense Retrievers
DPRsq-d 49.4 56.4 55.5 60.2
MixGR 52.3 59.9 59.0 60.9
ANCEsq-d 47.1 61.6 53.9 60.5
MixGR 45.9 66.9 55.5 59.8
TAS-Bsq-d 50.0 69.8 56.2 60.9
MixGR 52.3 68.0 58.2 62.9
GTRsq-d 41.9 66.3 60.2 63.7
MixGR 45.9 63.4 60.9 65.2
Table 7: Scientific Question Answering (Exact Match) was conducted using LLama-3 (Touvron et al., 2023) with
propositions retrieved by six retrievers. Here, EM@50 and EM@200 have been reported, where the first 50 and 200
words are fed into the reader models. Bold indicates superior performance, and it is observed that retrieval using
MixGR on proposition units generally outperforms the baseline.
Retriever SetupBioASQ NFCorpus SciDocs SciFact SciQ Avg.
ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20
Unsupervised Dense Retrievers
SimCSEsq-d 17.7 17.9 14.5 12.1 27.9 31.9 4.9 7.1 50.8 56.5 23.2 25.1
MixGR 26.2 27.1 19.8 16.2 34.8 39.1 6.7 9.5 70.1 73.0 31.5 33.0
Contrieversq-d 62.0 64.4 39.9 33.1 66.0 69.1 12.4 17.2 56.4 60.5 47.3 48.9
MixGR 64.5 67.7 41.4 34.7 67.0 70.7 13.2 18.5 75.3 77.7 52.3 53.9
Supervised Dense Retrievers
DPRsq-d 36.0 35.9 23.8 19.7 35.9 40.5 6.0 8.8 51.2 56.3 30.6 32.2
MixGR 41.0 41.6 26.1 21.5 40.0 44.1 6.9 9.9 63.2 67.0 35.4 36.8
ANCEsq-d 43.4 43.3 28.4 23.2 43.2 47.1 7.8 11.0 56.8 61.2 35.9 37.1
MixGR 48.4 48.9 30.5 24.6 46.3 50.1 8.6 11.9 65.2 68.7 39.8 40.9
TAS-Bsq-d 66.4 67.6 39.6 31.9 62.5 66.2 12.1 16.5 75.4 77.7 51.2 52.0
MixGR 69.2 71.1 40.8 32.9 64.1 67.7 12.5 16.8 81.2 82.7 53.6 54.2
GTRsq-d 61.6 61.9 39.9 32.3 58.8 62.3 12.0 16.2 72.6 75.6 49.0 49.7
MixGR 65.5 66.8 41.0 33.5 62.0 65.3 12.2 16.7 82.4 83.4 52.6 53.1
Table 8: Document Retrieval Performance (nDCG@ k= 5, 20 in percentage, abbreviated as ND@ k) on all the
queries, including the ones containing single and multiple subqueries. The setup is similar to Table 2. comparing
the retrieval results based on sq-dandMixGR .Bold presents the best performance across the metrics. We can notice
thatMixGR outperform the previous query-document retrieval for each entry.
Model Avg. #prop in rq-d≺rs-pAvg. #prop in rq-d≻rs-p
SimCSE 9.06 6.32
Contriever 8.25 7.24
ANCE 8.12 8.15
DPR 8.54 7.88
GTR 8.45 6.79
TAS-B 8.00 7.52
Table 9: Average number of propositions in two sets of document for different retrievers, i.e., rq-d≺rs-pand
rq-d≻rs-p. We can notice the average number of propositions in rq-d≺rs-pis more than rq-d≻rs-p. This shows
that the finer granularity can better deal with the documents with more propositions.Retriever SetupArguana ConditionalQA FiQA Avg.
ND@5 ND@20 ND@5 ND@20 ND@5 ND@20 ND@5 ND@20
Unsupervised Dense Retrievers
SimCSEsq-d 16.4 25.9 52.3 58.0 8.4 10.9 25.7 31.6
sq-p 12.5 20.9 53.7 59.5 7.6 9.7 24.6 30.0
ss-p 6.3 12.3 42.8 50.8 9.3 11.6 19.5 24.9
MixGR 12.7 22.4 57.7 63.3 10.6 13.8 27.0 33.2
Contrieversq-d 25.9 36.0 82.5 83.9 25.0 29.9 44.5 49.9
sq-p 24.8 35.9 81.8 83.5 18.8 23.1 41.8 47.5
ss-p 24.1 34.5 63.3 67.2 18.6 22.9 35.3 41.5
MixGR 28.7 39.2 83.5 84.5 24.7 29.8 45.6 51.2
Supervised Dense Retrievers
DPRsq-d 9.0 16.6 58.5 63.6 12.0 14.6 26.5 31.6
sq-p 8.4 16.9 60.1 64.7 8.4 10.9 25.6 30.8
ss-p 6.1 12.2 34.8 41.8 9.2 11.8 16.7 21.9
MixGR 8.2 16.3 59.9 65.4 11.2 14.9 26.4 32.2
ANCEsq-d 12.0 20.5 64.2 68.0 14.6 18.2 30.3 35.6
sq-p 11.7 21.3 64.0 68.2 8.5 10.9 28.1 33.5
ss-p 10.1 18.6 41.4 48.1 8.4 11.3 20.0 26.0
MixGR 12.4 21.8 66.2 69.8 12.8 16.2 30.5 36.0
TAS-Bsq-d 27.9 37.8 75.3 77.9 26.7 31.5 43.3 49.0
sq-p 18.8 30.5 76.4 78.7 15.3 19.7 36.8 43.0
ss-p 12.9 20.8 60.8 65.2 13.9 17.8 29.2 34.6
MixGR 22.6 33.6 77.7 79.2 22.8 27.9 41.1 46.9
GTRsq-d 31.4 40.7 79.8 82.3 34.4 39.6 48.5 54.2
sq-p 25.6 36.9 80.1 82.0 22.8 27.4 42.8 48.8
ss-p 20.4 30.0 62.9 67.7 19.6 24.2 34.3 40.6
MixGR 29.4 39.4 82.4 84.1 30.8 36.1 47.5 53.2
Table 10: Comparison between MixGR and its components on ConditionalQA, Arguana, and FiQA. We can find
that the similarity based on the finer granularity ss-pandMixGR won’t bring as many benefits as their performance
in the scientific domains, even the degradation.
Artifacts/Packages Citation Link License
Artifacts(datasets/benchmarks).
SciFact (Wadden et al., 2020) https://huggingface.co/datasets/BeIR/scifact cc-by-sa-4.0
SciDocs (Cohan et al., 2020) https://huggingface.co/datasets/BeIR/scidocs cc-by-sa-4.0
SciQ (Welbl et al., 2017) https://huggingface.co/datasets/bigbio/sciq cc-by-nc-3.9
NFCorpus (Boteva et al., 2016) https://huggingface.co/datasets/BeIR/nfcorpus cc-by-sa-4.0
Packages
PyTorch (Paszke et al., 2019) https://pytorch.org/ BSD-3 License
transformers (Wolf et al., 2019) https://huggingface.co/transformers/v2.11.0/index.html Apache License 2.0
numpy (Harris et al., 2020) https://numpy.org/ BSD License
matplotlib (Hunter, 2007) https://matplotlib.org/ BSD compatible License
vllm (Kwon et al., 2023) https://github.com/vllm-project/vllm Apache License 2.0
Models
LLaMA-3 (Touvron et al., 2023) https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct LICENSE
SimCSE (Gao et al., 2021) https://huggingface.co/princeton-nlp/unsup-simcse-bert-base-uncased MIT license
Contriever (Izacard et al., 2022) https://huggingface.co/facebook/contriever License
DPR (Karpukhin et al., 2020) https://huggingface.co/facebook/dpr-ctx_encoder-multiset-base cc-by-nc-4.0
ANCE (Xiong et al., 2021) https://huggingface.co/castorini/ance-dpr-context-multi MIT license
TAS-B (Hofstätter et al., 2021) https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b Apache License 2.0
GTR (Ni et al., 2022) https://huggingface.co/sentence-transformers/gtr-t5-base Apache License 2.0
Table 11: Details of datasets, major packages, and existing models we use. The datasets we reconstructed or revised
and the code/software we provide are under the MIT License.