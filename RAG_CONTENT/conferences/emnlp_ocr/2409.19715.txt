COFFEE -GYM: An Environment for Evaluating and Improving
Natural Language Feedback on Erroneous Code
Hyungjoo Chae1∗Taeyoon Kwon1∗Seungjun Moon1∗
Yongho Song Dongjin Kang1Kai Tzu-iunn Ong1Beong-woo Kwak1
Seonghyeon Bae1Seung-won Hwang2Jinyoung Yeo1
Yonsei University1Seoul National University2
{mapoout, kwonconnor101, lune_blue, jinyeo}@yonsei.ac.kr
seungwonh@snu.ac.kr
Abstract
This paper presents COFFEE -GYM, a com-
prehensive RL environment for training mod-
els that provide feedback on code editing.
COFFEE -GYMincludes two major components:
(1)COFFEE , a dataset containing humans’ code
edit traces for coding questions and machine-
written feedback for editing erroneous code; (2)
COFFEE EVAL, a reward function that faithfully
reflects the helpfulness of feedback by assess-
ing the performance of the revised code in unit
tests. With them, COFFEE -GYMaddresses the
unavailability of high-quality datasets for train-
ing feedback models with RL, and provides
more accurate rewards than the SOTA reward
model ( i.e., GPT-4). By applying COFFEE -
GYM, we elicit feedback models that outper-
form baselines in enhancing open-source code
LLMs’ code editing, making them comparable
with closed-source LLMs. We make the dataset
and the model checkpoint publicly available.1
1 Introduction
Large language models (LLMs) have made great
progress in code generation (Li et al., 2023; Roz-
ière et al., 2023), e.g., achieving human-level per-
formances in code generation benchmarks (Chen
et al., 2021b). Such success makes them powerful
tools for assisting human programmers (Köpf et al.,
2023); however, they still produce errors (Guo et al.,
2024a; OpenAI, 2023b). Therefore, code editing,
i.e., resolving errors in code, remains an important
task for code LLMs (Muennighoff et al., 2023).
Studies have utilized natural language (NL) feed-
back from LLMs as descriptive guidance in editing
wrong codes for code LLMs. For instance, Self-
Refine (Madaan et al., 2023) largely improves their
code editing using GPT-4’s feedback. Yet, abilities
to generate helpful feedback, as they report, are lim-
ited to powerful closed-source LLMs ( e.g., GPT-4).
∗Equal contribution
1https://huggingface.co/spaces/
Coffee-Gym/Project-Coffee-Gym
Incorr ect F eedback: ... check y our if -stat ement t o ensur e 
t he element s not being at t he same inde x.Code Editing wit h F eedback
Corr ect F eedback: Y ou'r e star ting fr om inde x 1 , but 
should be star ting fr om inde x 0 t o include all element s in 
t he list fr om t he v er y beginning.W rit e a code t hat checks if t her e is at least 1 
set of 3 numbers in t he list t hat add up t o 0 .W r ong Code
fr om 
Users/
Code LLMsd e f  t r i p l e s _ s u m _ t o _ z e r o ( l :  l i s t ) : 
     i   r a n g e ( 1 ,  l e n l ) :  
         j   r a n g e ( i  +  1 ,  l e n l ) :   
             k   r a n g e ( j  +  1 ,  l e n l ) :   
                 l [ i ]  +  l [ j ]  +  l [ k ]  =  =  0 :   
                     T r u e   
     F a l s ef o r i n ( )
f o r i n ( )
f o r i n ( )
i f
r e t u r n
r e t u r n#  m i s t a k e   ?
d e f  t r i p l e s _ s u m _ t o _ z e r o ( l :  l i s t ) : 
     i   r a n g e ( l e n l ) :  f o r i n ( )HumanEv alFix  
P ass@1Open-sour ce ModelsClosed-sour ce ModelsDir ect
EditingSelf
F eedbackSFTOursGP T - 3 . 5-
T urboGP T -4-
T urbo5560657 0. . .. . .d e f  t r i p l e s _ s u m _ t o _ z e r o ( l :  l i s t ) : 
     i   r a n g e ( 1 ,  l e n ( l ) ) :       
            f o r  k  i n  r a n g e ( j  +  1 ,  l e n ( l ) ) :
                i f  i  ! =  j  a n d  j  ! =  k  a n d  k  ! =  i :f o r i n
60 .462. 164 . 67 3 . 872. 57 4 .4Figure 1: A motivating example (Top) and Pass@1 ac-
curacy in HumanEvalFix (Bottom). We compare the
feedback from our model and various other models, both
paired with DeepSeekCoder-7B as the code editor. SFT
denotes the model trained on Code-Feedback (Zheng
et al., 2024) using the same backbone model as ours.
This can lead to a heavy reliance on closed-source
LLMs that may cause not only high computational
(e.g., API) cost but also security risks (Siddiq and
Santos, 2023; Greshake et al., 2023), limiting their
applicability for confidential codes.
This work aims to foster building open-source
feedback models that produce effective feedback
for code editing. An intuitive approach is to ap-
ply supervised fine-tuning (SFT) on open-source
code LLMs using feedback from GPT-4 (generatedarXiv:2409.19715v2  [cs.CL]  4 Oct 2024Figure 2: Comparison between C OFFEE -GYMand the previous approach.
based on machines’ code editing) (Zheng et al.,
2024). However, this simplified approach poorly
aligns editing performance with the helpfulness of
feedback (Bottom of Figure 1) (Liu et al., 2022).
Inspired by the success of RLHF (Ouyang et al.,
2022), we reformulate feedback modeling with re-
inforcement learning (RL), where we align feed-
back models with the helpfulness of feedback dur-
ing training. Since the success of RL highly de-
pends on the initial SFT model and a reliable re-
ward function (Lightman et al., 2023; Lambert
et al., 2024), we hereby identify 3 main challenges
in applying RL to feedback generation for code
editing: (1)limited scenarios of errors in model-
generated code editing datasets for initializing SFT
model, (2)the lack of pairwise (correct and wrong)
feedback to train/test reward functions, (3)absence
of validated implementation of reward models.
We present COFFEE -GYM, a comprehensive
RL environment addressing the above challenges in
training feedback models for code editing. First, to
tackle data scarcity in SFT initialization and reward
modeling, we curate
 COFFEE , a dataset for
codefixing with feedback, which consists of code
editing traces of human programmers and human
annotated feedback. Unlike model-generated data
(Figure 2), COFFEE includes (1) problems across
various difficulties, including those current LLMs
(e.g., GPT-4) cannot solve; (2) pairs of correct and
wrong feedback for reward modeling; (3) about
36 test cases per problem to measure the feedback
helpfulness in code editing.2
2This work is a substantially revised and extended version
of our preprint (Moon et al., 2023). While both works use
the same dataset, this submission presents significant advance-
ments in methodology, analysis, and results.Next, to address the absence of validated ( i.e., re-
liable) reward functions, we introduce COFFEE E-
VAL, a reward function designed to reflect the help-
fulness of feedback into reward calculation. In-
stead of directly assessing feedback quality (Ra-
jakumar Kalarani et al., 2023), we simulate code
editing based on generated feedback, conduct unit
tests on the edited code, and use the test results to
measure feedback helpfulness. With the pairwise
feedback from
 COFFEE , we train a given code
editor to produce edited code that faithfully reflects
the helpfulness of the given feedback.
Through experiments, we validate COFFEE -
GYM’s efficacy in training feedback models. We
find that COFFEE EVAL provides more accurate
rewards, compared to the current SOTA reward
model, i.e., G-Eval (Liu et al., 2023c) with GPT-4.
Also, we show that the feedback models trained
with COFFEE -GYMgenerate more helpful feed-
back, achieving comparable performance to closed-
source feedback models in code editing.
2Task Definition and Problem Statement
2.1 Code Editing with Natural Language
Feedback
The task of code editing aims to resolve errors in
given codes to produce a correct solution. Formally,
given a problem description qand a defective solu-
tiony, our goal is to learn a feedback model θthat
generates helpful feedback describing the errors
inyand provide helpful guidance on code editing:
ˆc=θ(q, y). Then, an editor model ϕthat takes
q,y, and the generated feedback ˆcas input and
generates the edited code: y′=ϕ(q, y,ˆc).
In evaluating the edited code y′, the functional-Figure 3: Overview of the data collection process of
 COFFEE .
ity of the edited code is measured with Pass@k,
the standard metric that measures the number of
passed test cases tiwithin the given set T=
{t1, t2, . . . , t k}(Li et al., 2022, 2023; Muennighoff
et al., 2023). Each test case ticonsists of an input
xiand an expected output zi.
2.2 Learning Feedback Models
In this paper, we consider two widely used learning
approaches to build open-source feedback models.
Supervised fine-tuning. A straightforward ap-
proach is to fine-tune an open-source code LLM θ
on a dataset D={(qi, yi, ci, y∗
i)}N
i=1of problem
descriptions, incorrect codes, feedback annotations,
and correct codes. The objective is to minimize
the negative log-likelihood of the target feedback
labely∗given qandy. However, simply training to
optimize the probability of the target sequence does
not achieve much improvement for code editing,
because it does not consider the impact of feedback
on code editing (Liu et al., 2022).
Reinforcement learning. Inspired by Ouyang
et al. (2022), we adopt reinforcement learning (RL)
to further align feedback generation to correct code
editing. Specifically, we choose PPO (Schulman
et al., 2017) and DPO (Rafailov et al., 2023) as
reference RL algorithms and apply them on the
feedback model θinitialized via SFT.
The two key factors of RL are (1) pairwise pref-
erence data and (2) reward modeling (Lambert
et al., 2024). In our task, we consider a preference
dataset where each input qandycomes with a pair
of chosen and rejected feedback c+andc−, and
their preference ranking c+≻c−. This dataset is
then used to model the reward based on the pref-
erence ranking. While in PPO a reward model is
explicitly trained using c+andc−, DPO relies onimplicit reward modeling and directly optimizes
the feedback model using the preference dataset.
2.3 Problem Statement
Our goal is to promote rapid development of open-
source feedback models by facilitating RL for feed-
back generation on code editing. Specifically, we
aim to provide the two key components in RL for
feedback generation:
Dataset. The dataset required for our RL ap-
proach covers the following key aspects: (1) Cov-
erage of difficulty and diversity ( q, y)to initialize
a good SFT model. (2) Pairwise feedback data
(c+≻c−|q, y)to build datasets for training DPO
and a reward model for PPO. (3) Test cases for
unit test ( T)are required to implement our R, for
directly measuring the impact of con the correct-
ness of code editing.
Reward model. The current standard of using
LLM as a reward model (Lee et al., 2023) to eval-
uate LLM outputs do not sufficiently models the
impact of feedback on code editing outcomes and
requires powerful LLMs ( e.g., GPT-4) that incur
high API costs. Especially, the high computation
costs significantly limits the application of online
RL algorithms ( e.g., PPO) in feedback modeling,
which require frequent and continuous API calls
for reward calculation.
3 Constructing C OFFEE -GYM
We introduce COFFEE -GYM, a comprehensive RL
environment for training NL feedback model for
code editing. COFFEE -GYMconsists of two major
components: (1)
 COFFEE , a dataset of human-
written edit traces with annotated NL feedback, and
(2)COFFEE EVAL, an accurate reward model thatS = input()
abc = [-1]*26
for c in S:
    abc[ord(c)-ord('a')] = S.index(c)
print(abc)S = input()
abc = [-1]*26
for c in S:
    abc[ord(c)-ord('a')] = S.index(c)
print(*abc)Given a word S consisting only of lowercase letters, write a 
program that prints the first occurrence of each letter in the 
word, or -1 if the letter is not included in the word.
Y our code correctly initializes the list with -1 for each letter , 
but you need to print the values individually using the   
operator to unpack the list.Input (i.e., word S)...
...Correct Output# of instance44,7824.192.735.5742674.1649.4A vg. # of error lines per codeA vg. # of submissions per userA vg. # of test cases per prob.# of total prob. setsA vg. solution len.674.1A vg. wrong code len.A vg. feedback len.269.0A vg. description len.zebra[4, 2 , -1, ..., 0]Pr oblem Description:
Dataset StatisticsW r ong Code:q
y**cCorr ect Code:Corr ect F eedbac k :S y nt h etic T est Cases:
T he issue is that you need to use a dictionary to store the ...~cI ncorr ect F eedbac k :
Figure 4: Example and statistics of
 COFFEE .
measures feedback’s impact on code editing.
3.1
 COFFEE : Human-written Code Edit
Traces with Annotated Pairwise Feedback
We curate COFFEE , a dataset of codefixing with
feedback, from human-written code edit traces.
COFFEE consists of problems of diverse levels
of difficulty, including challenging problems that
only human programmers can solve, and provides
test cases for reward functions (Section 3.2). The
overview of constructing COFFEE , data examples,
and statistics are in Figure 3 and 4.
3.1.1 Collecting Code Edit Traces from
Human Programmers
We collect human-authored code edits from an on-
line competitive programming platform.3In this
platform, given a problem description q, human
programmers keep submitting a new solution yun-
til they reach a correct solution y∗that passes all
hidden test cases for q. Formally, for each qand the
correct submission y∗
n, we collect the submission
history {˜y1,˜y2, ..., y∗
n}, where {˜yk}n−1
k=1are incor-
rect solutions. We then construct (q,˜y, y∗)triplets
3https://www.acmicpc.net/
BronzeSilverGoldGPT -4-T urbo (Pass@1)Human (Solve rate)63.157.155.148.040.716.6
Difficulty Le v el
Code SimilarityA v g.  Lengt h  of Edit T r aceF r equency(a) Distribution of a v er age lengt h of edit tr ace
(b ) Div ersity analysis on err or codes using CodeBERT
( c) P ass@1 of GPT -4- T urbo compar ed t o humanFigure 5: Analysis results of
 COFFEE . Experiment
details are in Appendix A.1.5.
by pairing each incorrect solution ˜ykwith the cor-
rect one y∗
n,i.e.,{(q,˜yk, y∗
n)}n−1
k=1.
To ensure COFFEE is not biased toward coding
problems of a specific difficulty level, we collect
an equal number of problems from each of the
five difficulty levels in the platforms, ranging from
beginner to expert levels. We also ensure that COF-
FEEincludes various solutions to each problem by
collecting submission histories from 100 different
users. Our analysis in Figure 5 shows that COFFEE
(1) includes problems that are challenging for both
human and LLMs and (2) covers more diverse error
cases than machine-generated codes.
3.1.2 Annotating Pairwise Feedback Data
We additionally annotate NL feedback that pro-
vides useful guidance on the necessary edits.
For each triplet (q,˜y, y∗), we prompt GPT-3.5-
Turbo (OpenAI, 2023a) to describe how the correct
solution y∗differs from the wrong code ˜y. The re-
sulting description c∗serves as the correct feedback
that describes necessary changes on the wrong code
˜yto obtain the correct code y∗. Along with c∗, we
also collect incorrect feedback ˜c, which describes
the difference between two wrong solutions, ˜yk−1
and˜yk(k̸=n), to provide pairwise labels for both
correct and incorrect feedback to a single wrong
solution ˜y. We discuss details on feedback annota-
tion in Appendix A.1.1, including our prompt usedmean std min 25% 50% 75% max
Pass ratio 0.342 0.370 0.000 0.000 0.162 0.693 0.985
Table 1: Pass ratio for incorrect code samples in the
evaluation set of C OFFEE dataset.
for feedback annotation and filtering techniques.
3.1.3 Augmenting Synthetic Test Cases
Finally, we include a set of hidden test cases T=
{t1, t2, . . . , t k}for each edit instance (q,˜y, y∗, c)
in our dataset to assess whether the edited code is
the correct solution to the problem. Each test case
ticonsists of an input xiand an expected output zi.
As the programming platform does not make test
cases publicly available, we annotate test cases by
prompting GPT-3.5-Turbo to generate inputs xifor
a given qand executing the correct code y∗with
xito obtain the corresponding outputs zi. We filter
out any invalid test cases with inputs that result
in errors during execution. On average, we obtain
35.5 test cases per problem.
A critical question in evaluating our test suite
is whether any incorrect solutions manage to pass
all the test cases. To address this, we conduct an
experiment using the evaluation set of the COFFEE
dataset. We randomly sampled 200 wrong code in-
stances and calculated the pass ratios of the wrong
codes. We show the statistics of the distribution of
pass ratios. As shown in Table 5, the maximum
pass ratio is 0.985, which suggests that there are
no wrong solutions that passed all the test cases.
The mean score is 0.342, indicating that on average,
wrong solutions fail the majority of the test cases.
We further analyze the COFFEE -TESTand verified
that no wrong solutions pass all the test cases.
These test cases are used to measure the correct-
ness of an edited code and estimate the helpfulness
of the feedback as the COFFEE EVAL score, which
we later use as supervision signals for training feed-
back models (§3.2) in COFFEE -GYM. We provide
details on test case generation in Appendix A.1.3.
3.2 COFFEE EVAL: Unit-test-driven Feedback
Evaluation
We present COFFEE EVAL as our reliable reward
function in COFFEE -GYM. The key idea is to mea-
sure the helpfulness of feedback by gauging the
correctness of the edited code produced by a small,
but cheap editor model that properly aligns edit-
ing with feedback. Specifically, given a problem
description q, a wrong solution ˜y, and feedbackˆcfrom a feedback model θ, an editor model ϕ
generates an edited code y′by grounding on ˆc,
i.e.,y′=ϕ(q,˜y,ˆc). The COFFEE EVAL score is
defined as the proportion of test cases for which
the edited code y′produces the expected output:
COFFEE EVAL(q,˜y,ˆc, ϕ,T)
=1
kkX
i=11(ϕ(q,˜y,ˆc)(xi) =zi)(1)
where each element ti∈ T consists of an input
xiand an expected output zi, and 1is a binary
indicator function that returns 1 if the output of y′
matches the expected output zi. By reflecting the
correctness of the edited code, the resulting score
serves as an accurate measure for the effectiveness
of the generated feedback in code editing.
3.2.1 Training a Faithful Code Editor to Align
Editing with Feedback
General code LLMs are trained to produce only
correct codes, resulting in a bias toward correct
editing regardless of feedback quality. To address
this, we train a code editor ϕthat aligns its output
with the helpfulness of the feedback by training the
model to generate both correct edits (q, y, c∗, y∗)∈
Dcorrect and incorrect edits (q, y,˜c,˜y)∈ D wrong
in C OFFEE . The training objective is defined as:
L(ϕ) =−X
(q,y,c∗,y∗)∈Dcorrectlogpϕ(y∗|q, y, c∗)
−X
(q,y,˜c,˜y)∈Dwronglogpϕ(˜y|q, y,˜c)(2)
To prevent confusion during training, we follow
Wang et al. (2023a) and indicate the correctness
of the target code by prepending the keywords
[Correct] and[Wrong] to the code sequence.
By learning from both positive and negative ex-
amples, the editor learns to conduct code editing by
faithfully following the given feedback. It allows
us to use the editor’s output as a reliable metric
for evaluating feedback generation models in our
COFFEE -GYMenvironment.
4 Validating C OFFEE EVAL
4.1 Experimental Setting
Implementation details. We implement COF-
FEEEVAL with DeepSeekCoder-7B model as the
backbone in all our experiments. For further details,
please refer to Appendix A.2.1.Model EvaluationPass@1 Scores Correlation Error
✓Correct Feedback ↑(TP) ✗Wrong Feedback ↓(FP) Precision ↑Recall ↑F1↑ Pearson ↑ MSE↓
GPT-4-Turbo G-Eval - - - - - 0.135 0.415
GPT-3.5-Turbo G-Eval - - - - - -0.172 0.575
GPT-4-Turbo Editing 53.0 51.8 50.6 53.0 51.8 0.012 0.450
GPT-3.5-Turbo Editing 43.4 33.6 56.4 43.4 49.0 0.101 0.417
DeepSeek-Coder-7B Editing 36.0 28.8 55.6 36.0 43.7 0.077 0.428
DeepSeek-C OFFEE EVAL (w/o WF) Editing 36.4 28.4 56.2 36.4 44.2 0.085 0.418
DeepSeek-C OFFEE EVAL (Ours) Editing 52.0 28.4 64.7 52.0 57.7 0.149 0.408
Table 2: Performance of our evaluation protocol on the test sets of COFFEE compared to the baselines. Wrong
Feedback is abbreviated as WF due to limited space.
Figure 6: Ablation results on the number of test cases
used in COFFEE EVAL. The evaluation performance
decreases as the number of test cases declines.
4.2 Reliability of C OFFEE EVAL
Baselines. We compare our COFFEE EVAL with
two evaluation methods: G-Eval (Liu et al., 2023c)
and Editing. For G-Eval, we directly assess feed-
back quality in Likert-scale (1 - 5) using score
rubrics (Kim et al., 2023). Editing baselines follow
the same evaluation scheme as COFFEE EVAL but
use general code LLMs for the editor ϕ. We con-
sider with three code LLMs, GPT-3.5-Turbo, GPT-
4-Turbo, and DeepSeek-Coder-7B. The prompt we
use for G-Eval is in Appendix B.3.
Evaluation. To measure the alignment between
feedback generation and code editing, we use test
set of
 COFFEE , where each cis annotated with
a binary label on its helpfulness. For Editing meth-
ods (including ours), we regard the output as posi-
tive prediction when the edited code passes all test
cases. Also, we provide Pearson correlation co-
efficients for both Editing and G-Eval methods to
analyze the correlation between the predicted score
and the ground-truth labels.
4.3 Results and Analysis
COFFEE EVAL faithfully aligns feedback qual-
ity with editing performance. As shown in Ta-
ble 2, DeepSeek- COFFEE EVAL achieves higher
Pearson correlation and lower MSE than all G-Eval
and Editing baselines. In particular, our approach
shows even higher correlation than the G-Eval base-
line implemented with GPT-4-Turbo. The strongperformance of our COFFEE EVAL validates its ef-
fectiveness in assessing the quality of NL feedback
in the code editing task.
Code LLMs are skewed toward correct editing,
regardless of the feedback quality. While code
LLMs have shown promising results in code gener-
ation tasks, they do not faithfully reflect the help-
fulness of feedback on code editing. Especially,
GPT-4-Turbo, the current SOTA code LLM, shows
the highest Pass@1 among baselines, but it also
tends to generate correct code even with wrong
feedback. These results suggest that the training
process with our pairwise feedback data is an es-
sential step in building a reliable reward model.
The performance of COFFEE EVAL benefits from
the number of test cases. Figure 6 compares
the Pearson correlation coefficient and MSE with
respect to the number of test cases. We observe
that a higher number of test cases leads to more
accurate evaluation on the feedback quality, which
validates our design choice of
 COFFEE .
5 Benchmarking Reference Methods of
COFFEE -GYM
In this section, we apply the feedback model
trained using COFFEE -GYM on various open-
source LLMs and assess its effectiveness in en-
hance code editing performance. Furthermore, we
comprehensively explore a wide range of training
strategies available in our COFFEE -GYMto provide
insights on building helpful feedback models.
5.1 Effectiveness of C OFFEE -GYMin
Training Feedback Models
5.1.1 Experimental Setting
Implementation details. We train our feed-
back model based on DeepSeekCoder-7B using
COFFEE -GYMby applying PPO. Further details
are in Appendix A.3.Methods Params. Open-sourceHumanEvalFix C OFFEE -TEST Average
Pass@1 ∆ Pass@1 ∆ Pass@1 ∆
GPT-4-Turbo (OpenAI, 2023b) - ✗ 83.5 - 43.8 - 63.6 -
GPT-3.5-Turbo (OpenAI, 2023a) - ✗ 75.0 - 32.2 - 53.6 -
DeepSeek-Coder (Guo et al., 2024a) 7B ✓ 60.4 - 33.8 - 47.1 -
+ Execution Feedback - ✓ 68.3 + 7.9 38.3 + 4.5 53.3 + 6.2
+ Self-Feedback 7B ✓ 67.7 + 7.3 28.3 - 5.5 48.0 + 0.9
+ OpenCodeInterpreter-DS-Coder Feedback 7B ✓ 64.6 + 4.2 30.5 - 3.3 47.5 + 0.5
+OURS 7B ✓ 73.8 + 13.4 47.2 + 13.4 60.5 + 13.4
+ GPT-3.5-Turbo Feedback - ✗ 72.5 + 12.1 35.5 + 1.7 54.0 + 6.9
+ GPT-4-Turbo Feedback - ✗ 74.4 + 14.0 44.4 + 10.6 59.4 + 12.3
CodeGemma (CodeGemma Team et al., 2024) 7B ✓ 53.7 - 14.4 - 34.1 -
+ Execution Feedback - ✓ 61.6 + 7.9 15.0 + 0.6 38.3 + 4.2
+ Self-Feedback 7B ✓ 53 - 0.7 16.6 + 2.2 34.8 + 0.7
+ OpenCodeInterpreter-DS-Coder Feedback 7B ✓ 36.5 - 17.2 15 + 0.6 25.8 - 8.3
+OURS 7B ✓ 59.7 + 6.0 31.1 + 16.7 45.4 + 11.4
+ GPT-3.5-Turbo Feedback - ✗ 57.3 + 3.6 22.2 + 7.8 39.8 + 5.7
+ GPT-4-Turbo Feedback - ✗ 65.8 + 12.1 22.7 + 8.3 44.3 + 10.2
OpenCodeInterpreter-DS-Coder (Zheng et al., 2024) 7B ✓ 65.8 - 30.5 - 48.1 -
+ Execution Feedback - ✓ 66.4 + 0.6 36.6 + 6.1 51.5 + 3.4
+ Self-Feedback 7B ✓ 62.1 - 3.7 21.1 - 9.4 41.6 - 6.5
+ DeepSeek-Coder Feedback 7B ✓ 56.1 - 9.7 28.3 - 2.2 42.2 - 5.9
+OURS 7B ✓ 70.1 + 4.3 42.7 + 12.2 56.4 + 8.3
+ GPT-3.5-Turbo Feedback - ✗ 68.3 + 2.5 32.7 + 2.2 50.5 + 2.4
+ GPT-4-Turbo Feedback - ✗ 72.5 + 6.7 43.3 + 12.8 57.9 + 9.8
Table 3: Code editing results of our feedback model trained with COFFEE -GYM,i.e., PPO- COFFEE EVAL, on
HumanEvalFix and COFFEE -TEST. We pair our feedback model with an open-source code LLM as the code editor.
Benchmarks. We test the feedback model
trained using COFFEE -GYM on HumanEval-
Fix (Muennighoff et al., 2023), a widely used code
editing benchmark. The task is to fix the errors
in given erroneous code and the correctness of
the edited code is measures by running the anno-
tated test cases. Then, if the submitted solution
passes all testcases the solution is evaluated as suc-
cess and pass@1 is calculated as the percentage of
the passed solutions for all promplems. We care-
fully check if there is data leakage in COFFEE and
verify there is no overlap between COFFEE and
HumanEvalFix (Appendix A.1.6). Additionally,
we assess the effectiveness of our approach on a
held-out test set named COFFEE -TEST. It consists
of 180 instances of (q,˜y, y∗,T)pairs that are col-
lected following the same process in §3.1 but with
no overlapping problems with C OFFEE .4
Baselines. We compare with the following base-
lines that provides feedback for code editing: (1)
4While we have considered other code editing benchmarks,
DebugBench (Tian et al., 2024) and CodeEditorBench (Guo
et al., 2024b), we find that these benchmarks have a critical
issue; even the ground-truth solution cannot pass the unit test.
A detailed discussion on this issue is in Appendix B.1.Execution Feedback (Chen et al., 2023): exe-
cution results of the generated code, e.g., error
messages, without using any LLMs , (2) Self-
Feedback (Madaan et al., 2023): NL feedback gen-
erated by the code editor itself, (3) OpenCodeInter-
preter Feedback (Zheng et al., 2024): a code LLM
especially trained on Code-Feedback dataset. We
also provide the results of feedback from closed-
source LLMs, GPT-3.5-Turbo and GPT-4-Turbo,
but these models are not our main focus as we aim
to develop open-source feedback models.
5.1.2 Results
In Table 3, we compare the performance of our
best feedback model with other feedback methods
using various open-source models. Consistent with
the findings from Chen et al. (2023), we observe
improvements across all code LLMs when using
Execution Feedback. However, we find that open-
source code LLMs, despite their capabilities in
the code domain, struggle to generate helpful NL
feedback for code editing (Self-Feedback), high-
lighting the complexity of producing effective feed-
back. Notably, our approach demonstrates com-
parable performance to GPT-3.5/4-Turbo, signifi-cantly closing the performance gap between closed-
source and open-source models in the task of feed-
back generation for code editing.
5.2 Comparing Different Training Strategies
in C OFFEE -GYM
5.2.1 Experimental Setting
Training strategies. For training algorithm, we
explore DPO, PPO, and Rejection Sampling (RS).
In RS, we sample 10 ˆcfrom SFT model, and collect
ˆcwith top-1 COFFEE EVAL score as labels for the
next iteration of SFT. For PPO, we use COFFEE E-
VAL as the reward model. We use 3 variants for
DPO: (1) DPO-TS: We construct preference pair by
selecting the teacher model’s feedback ( i.e., GPT-
3.5-Turbo) as c+, and the student model’s (SFT)
response as c−(Tunstall et al., 2023), (2) DPO-CW:
We directly use the labeled feedback pair (c∗,˜c).
(3) DPO- COFFEE EVAL: We sample 10 ˆc, same
as RS, and we construct preference pair with ˆcof
top-1 and bottom-1 C OFFEE EVAL score.
5.2.2 Results
COFFEE provides helpful train data for SFT.
In Figure 7, we find that SFT- COFFEE pro-
vides more helpful feedback than SFT- CODE-
FEEDBACK trained on Code-Feedback. This re-
sults suggest that COFFEE serves as a valuable re-
source for fine-tuning feedback models.
COFFEE and COFFEE EVAL allow informative
preference pair construction for DPO. DPO-
COFFEE EVAL achieves the best results among
DPO variants, closely followed by DPO-CW,
which utilizes correct-wrong pairs from COFFEE .
However, DPO-TS significantly underperforms
even with the correct feedback c+sampled from
the teacher. We conjecture that the teacher’s feed-
back may not always be superior to the student’s,
leading to suboptimal preference pairs.
PPO is the most effective training algo-
rithm. PPO- COFFEE EVAL outperforms DPO-
COFFEE EVAL and RS- COFFEE EVAL, despite us-
ing the same reward model. We hypothesize that
online RL methods like PPO allow for continuous
updates on the reference model and lead to better
alignment compared to offline methods like DPO,
which learn from a fixed initial model.
5.3 Analysis
Fine-grained analysis by error type. In Fig-
ure 8a, we compare the baselines with our approach
Figure 7: End-to-end validation results of the reference
methods in C OFFEE -GYMon C OFFEE -TEST.
(a) Err or type analysis on HumanEv alFix(b ) Human e v aluation on gener at ed f eedback
4 .44 .24 . 03 . 83 . 63 .4Err or Det ection4 .44 .24 . 03 . 83 . 63 .4Err or Corr ectionOursSelf -F eedbackEx ecution F eedbackDir ect EditingChatGPTGPT 4SFTOursOpenCodeInt erpr et er
Figure 8: (a) Breakdown of editing performance on
HumanEvalFix by different error types. (b) Human
evaluation of the feedback generated on HumanEvalFix.
See Appendix B.4 for details on human evaluation.
across different error types. Our feedback model
is particularly effective at correcting Missing logic
and Function misuse errors, which can greatly ben-
efit from NL feedback by providing a detailed ex-
planation for editing. In value misuse, our model
shows slightly lower performance. We posit that
this is due to the discrepancy between the distribu-
tion of errors from human-authored data ( i.e.,COF-
FEE) and synthetic data, where our model is tested.
Human evaluation on feedback quality. To pro-
vide a more accurate analysis of the feedback qual-
ity, we conduct human evaluation using qualified
workers from MTurk.5The results in Figure 8b
show that the feedback from our model is rated
as more helpful and informative compared to the
baselines, supporting the findings in §5.2.
6 Related Work
Code editing. Code LLMs have shown promis-
ing code generation capabilities by training on mas-
sive code corpora (Li et al., 2023; Wang et al.,
2023b). Despite their promising capabilities, there
remains a possibility of errors, making code edit-
ing tasks essential for ensuring code quality and
correctness (Muennighoff et al., 2023). In response
to this necessity, recent studies have focused on as-
5The details of our human evaluation are in Appendix B.4.sessing the code editing capabilities of code LLMs,
by proposing new benchmarks for the task (Tian
et al., 2024; Guo et al., 2024b).
Refining with external feedback. In code edit-
ing, two types of widely used external feedback
are execution feedback (Gou et al., 2023; Chen
et al., 2023) and NL feedback (Madaan et al., 2023;
Shinn et al., 2023). Recently, Zheng et al. (2024)
explored both types of feedback and demonstrate
that NL feedback outperforms execution feedback.
Concurrent to our work, Ni et al. (2024) explored
building feedback model, but they do not provide
the dataset used nor the model checkpoint.
RL in code generation tasks. A line of research
has explored improving LLMs’ code generation
with RL by leveraging the unit test results as re-
ward (Le et al., 2022; Liu et al., 2023a; Shen et al.,
2023). While the design of COFFEE EVALis largely
inspired by this line of work, we show that build-
ing reward model for feedback learning using unit
test results is non-trivial, since code LLMs do not
faithfully reflect feedback into editing (Table 2).
7 Conclusion
In this paper, we present a comprehensive study
on building open-source feedback models for code
editing. We introduce COFFEE -GYM, an environ-
ment for training and evaluating feedback models,
and share valuable insights from our experiments.
We hope our work will encourage researchers to
further explore feedback model development us-
ingCOFFEE -GYMand our findings, advancing the
field of code editing with NL feedback.
Limitations
Scope of editing. COFFEE -GYMtackles the task
of code editing with a particular focus on correcting
errors in codes. This leaves room for improvement
in our RL approach to consider the efficiency and
readability of the edited codes. Also, we mainly
focus on editing incorrect source codes in a compet-
itive programming setting. Some examples from
our feedback model (Appendix C.2) suggest that
our approach can be further applied to practical
programming problems, e.g., those that involve ma-
chine learning libraries. In future studies, COFFEE -
GYMcan be further expanded to real-world soft-
ware engineering settings with additional training
on general code corpora (Li et al., 2023).Using synthetic test cases for measuring reward.
While running synthetic test cases and using the
resulting pass rates might be a promising proxy
for reward calculation, there might be edge cases
where even erroneous codes pass the synthetic test
cases. Further research can incorporate Liu et al.
(2023b) to make more challenging test cases that
can rigorously identify erroneous codes.
Single programming language. Our implemen-
tation of COFFEE -GYMis limited to a single pro-
gramming language, i.e., Python. However, future
work might apply a similar strategy as ours to ex-
pand our model to a multilingual setting, where
the model is capable of understanding and editing
diverse programming languages such as Java.
Single parameter size and architecture. Lastly,
we implement the feedback models only with one
parameter size and architecture. However, fu-
ture work can apply our method to models with
larger parameter sizes ( e.g., DeepSeek-Coder 70B),
which is expected to perform better in code editing.
Our framework can also be further applied to other
architectures, as our method is model-agnostic.
Ethical Considerations
While our dataset originates from online competi-
tive programming platforms, we have ensured the
exclusion of personal information to maintain pri-
vacy standards. Additionally, we are aware of the
potential risks associated with texts generated by
language models, which can contain harmful, bi-
ased, or offensive content. However, based on our
assessments, this risk is mostly mitigated in our
work. Lastly, there exists a risk of hallucination in
the process of feedback generation and code edit-
ing, leading to incorrect edits. This emphasizes the
need for careful application in our approach.
Acknowledgement
This work was supported by Institute of Informa-
tion & Communications Technology Planning &
Evaluation (IITP) grant funded by the Korean gov-
ernment (MSIT)(No.RS-2020-II201361, Artificial
Intelligence Graduate School Program (Yonsei Uni-
versity)) and (No.RS-2021-II212068, Artificial In-
telligence Innovation Hub) and (2022-0-00077, RS-
2022-II220077,AI Technology Development for
Commonsense Extraction, Reasoning, and Infer-
ence from Heterogeneous Data). Jinyoung Yeo is
the corresponding author.References
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021a. Evaluat-
ing large language models trained on code.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al. 2021b. Evaluating large lan-
guage models trained on code. arXiv preprint
arXiv:2107.03374 .
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
Denny Zhou. 2023. Teaching large language models
to self-debug. arXiv preprint arXiv:2304.05128 .
CodeGemma Team, Ale Jakse Hartman, Andrea Hu,
Christopher A. Choquette-Choo, Heri Zhao, Jane
Fine, Jeffrey Hui, Jingyue Shen, Joe Kelley, Joshua
Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth,
Nam Nguyen, Paul Michel, Peter Choy, Pratik Joshi,
Ravin Kumar, Sarmad Hashmi, Shubham Agrawal,
Siqi Zuo, Tris Warkentin, and Zhitao et al. Gong.
2024. Codegemma: Open code models based on
gemma.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms. arXiv preprint arXiv:2305.14314 .
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, et al. 2020. Codebert: A
pre-trained model for programming and natural lan-
guages. arXiv preprint arXiv:2002.08155 .
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,
Yujiu Yang, Nan Duan, and Weizhu Chen. 2023.
Critic: Large language models can self-correct with
tool-interactive critiquing.
Kai Greshake, Sahar Abdelnabi, Shailesh Mishra,
Christoph Endres, Thorsten Holz, and Mario Fritz.
2023. Not what you’ve signed up for: Compromis-
ing real-world llm-integrated applications with indi-
rect prompt injection. Proceedings of the 16th ACM
Workshop on Artificial Intelligence and Security .Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Yu Wu, Y . K. Li, Fuli Luo, Yingfei Xiong, and Wen-
feng Liang. 2024a. Deepseek-coder: When the large
language model meets programming - the rise of code
intelligence. ArXiv , abs/2401.14196.
Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma,
Tianyu Zheng, Zhouliang Yu, Ding Pan, Yizhi Li,
Ruibo Liu, Yue Wang, et al. 2024b. Codeeditor-
bench: Evaluating code editing capability of large
language models. arXiv preprint arXiv:2404.03543 .
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang,
Shayne Longpre, Hwaran Lee, Sangdoo Yun,
Seongjin Shin, Sungdong Kim, James Thorne, et al.
2023. Prometheus: Inducing fine-grained evalua-
tion capability in language models. arXiv preprint
arXiv:2310.08491 .
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte,
Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver
Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri,
David Glushkov, Arnav Dantuluri, Andrew Maguire,
Christoph Schuhmann, Huu Nguyen, and Alexander
Mattick. 2023. Openassistant conversations – democ-
ratizing large language model alignment.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison,
LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
et al. 2024. Rewardbench: Evaluating reward
models for language modeling. arXiv preprint
arXiv:2403.13787 .
Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio
Savarese, and Steven Chu Hong Hoi. 2022. Coderl:
Mastering code generation through pretrained models
and deep reinforcement learning. Advances in Neural
Information Processing Systems , 35:21314–21328.
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie
Lu, Thomas Mesnard, Colton Bishop, Victor Car-
bune, and Abhinav Rastogi. 2023. Rlaif: Scaling
reinforcement learning from human feedback with ai
feedback. arXiv preprint arXiv:2309.00267 .
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim, et al.
2023. Starcoder: may the source be with you! arXiv
preprint arXiv:2305.06161 .
Yujia Li, David Choi, Junyoung Chung, Nate Kushman,
Julian Schrittwieser, Rémi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, Agustin Dal Lago,
et al. 2022. Competition-level code generation with
alphacode. Science , 378(6624):1092–1097.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri
Edwards, Bowen Baker, Teddy Lee, Jan Leike,
John Schulman, Ilya Sutskever, and Karl Cobbe.
2023. Let’s verify step by step. arXiv preprint
arXiv:2305.20050 .Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He,
Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi.
2022. Rainier: Reinforced knowledge introspector
for commonsense question answering. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 8938–8958.
Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han,
Wei Yang, and Deheng Ye. 2023a. Rltf: Reinforce-
ment learning from unit test feedback. arXiv preprint
arXiv:2307.04349 .
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and
LINGMING ZHANG. 2023b. Is your code gen-
erated by chatGPT really correct? rigorous evalu-
ation of large language models for code generation.
InThirty-seventh Conference on Neural Information
Processing Systems .
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023c. G-eval:
Nlg evaluation using gpt-4 with better human align-
ment.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Sean Welleck, Bodhisattwa Prasad Majumder,
Shashank Gupta, Amir Yazdanbakhsh, and Peter
Clark. 2023. Self-refine: Iterative refinement with
self-feedback.
Seungjun Moon, Yongho Song, Hyungjoo Chae,
Dongjin Kang, Taeyoon Kwon, Kai Tzu-iunn Ong,
Seung-won Hwang, and Jinyoung Yeo. 2023. Coffee:
Boost your code llms by fixing bugs with feedback.
arXiv preprint arXiv:2311.07215 .
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai
Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam
Singh, Xiangru Tang, Leandro von Werra, and
Shayne Longpre. 2023. Octopack: Instruction tun-
ing code large language models. arXiv preprint
arXiv:2308.07124 .
Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin
Deng, Kensen Shi, Charles Sutton, and Pengcheng
Yin. 2024. Next: Teaching large language models to
reason about code execution.
Augustus Odena, Charles Sutton, David Martin Do-
han, Ellen Jiang, Henryk Michalewski, Jacob Austin,
Maarten Paul Bosma, Maxwell Nye, Michael Terry,
and Quoc V . Le. 2021. Program synthesis with large
language models.
OpenAI. 2023a. Chatgpt. https://openai.com/
blog/chatgpt .
OpenAI. 2023b. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton,Luke E. Miller, Maddie Simens, Amanda Askell, Pe-
ter Welinder, Paul Francis Christiano, Jan Leike, and
Ryan J. Lowe. 2022. Training language models to
follow instructions with human feedback. ArXiv ,
abs/2203.02155.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D. Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model.
Abisek Rajakumar Kalarani, Pushpak Bhattacharyya,
Niyati Chhaya, and Sumit Shekhar. 2023. “let’s not
quote out of context”: Unified vision-language pre-
training for context assisted image captioning. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 5:
Industry Track) , pages 695–706, Toronto, Canada.
Association for Computational Linguistics.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .
Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan,
Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan
Ji, Jingyang Zhao, et al. 2023. Pangu-coder2: Boost-
ing large language models for code with ranking feed-
back. arXiv preprint arXiv:2307.14936 .
Noah Shinn, Federico Cassano, Edward Berman, Ash-
win Gopinath, Karthik Narasimhan, and Shunyu Yao.
2023. Reflexion: Language agents with verbal rein-
forcement learning. In Proceedings of NeurIPS .
Mohammed Latif Siddiq and Joanna C. S. Santos. 2023.
Generate and pray: Using sallms to evaluate the secu-
rity of llm generated code. ArXiv , abs/2311.00889.
Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai
Lin, Zhiyuan Liu, and Maosong Sun. 2024. De-
bugbench: Evaluating debugging capability of large
language models. arXiv preprint arXiv:2401.04621 .
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-
rect distillation of lm alignment. arXiv preprint
arXiv:2310.16944 .
Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao,
Bing Yin, and Xiang Ren. 2023a. SCOTT: Self-
consistent chain-of-thought distillation. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 5546–5558, Toronto, Canada. Association for
Computational Linguistics.Yue Wang, Hung Le, Akhilesh Deepak Gotmare,
Nghi D.Q. Bui, Junnan Li, and Steven C. H. Hoi.
2023b. Codet5+: Open code large language mod-
els for code understanding and generation. arXiv
preprint .
Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin,
Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen,
and Jian-Guang Lou. 2022. CERT: Continual pre-
training on sketches for library-oriented code genera-
tion. In The 2022 International Joint Conference on
Artificial Intelligence .
Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu,
Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang
Yue. 2024. Opencodeinterpreter: Integrating code
generation with execution and refinement. arXiv
preprint arXiv:2402.14658 .A Details of C OFFEE -GYM
A.1 Details of
 COFFEE
A.1.1 Feedback Annotation
We annotate both correct and wrong feedback for
our dataset using GPT-3.5-Turbo. We apply top-
psampling and temperature, where p= 0.95and
T= 0.7. We limit the number of generation tokens
to 500. We leave out submission histories where
the LLM fails to find any errors. We also filter out
submissions from different users whose correct so-
lutions are identical, as these solutions are usually
copied from the web without undergoing editing
processes. With collected user’s submission his-
tory{˜y1,˜y2, ..., y∗
n}, we sample correct edit pairs
{˜yk, y∗
n}n−1
k=1to annotate correct feedback. To an-
notate the wrong feedback, we use sequential pairs
{˜yk,˜yk+1}n−2
k=1to capture transitions between con-
secutive incorrect solutions. The prompts used for
annotating correct and wrong feedback are demon-
strated in Appendix D.1 and Appendix D.2.
A.1.2 Quality Analysis on Annotated
Feedback
To thoroughly analyze the quality of the feedback
from GPT-3.5-Turbo, we conduct a human evalua-
tion. We ask human raters from Amazon Mechani-
cal Turk (AMT) to score the quality of the feedback
on a Likert scale. To ensure proficiency, we filter
out human raters who have not passed our qual-
ification test, which assesses their knowledge of
programming languages, especially Python. From
the test set of COFFEE , we sample 100 instances
for the evaluation.
On average, the annotated feedback is scored
3.88 with 0.91 STD, which suggests that the quality
of the annotated feedback is generally acceptable
by humans. The full distribution of the evaluation
results is shown in Table 4.
A.1.3 Synthesizing Test Cases
We prompt GPT-3.5-Turbo to synthesize input
test cases given a problem description with three
demonstrations. For each test case, we execute the
correct code to obtain the corresponding output. If
execution was successful, we then pair these inputs
and outputs to create sample input-output pairs. On
average, we synthesize 35 test cases per problem.
We provide the prompt for the test case generation
in Appendix D.3.Correctness Score Frequency (%)
1 2 (0.6%)
2 21 (7.0%)
3 70 (23.3%)
4 126 (42.0%)
5 81 (27.0%)
Table 4: Distribution of human evaluation scores for
GPT-3.5-Turbo feedback quality.
mean std min 25% 50% 75% max
Pass ratio 0.342 0.370 0.000 0.000 0.162 0.693 0.985
Table 5: Pass ratio for incorrect code samples in the
evaluation set of C OFFEE dataset.
A.1.4 Analysis on Machine-generated Test
Cases
To gain insights into the effectiveness of our
machine-generated test cases, we conduct analyses
exploring two key aspects: validity and diversity.
Validity of test cases. A critical question in eval-
uating our test suite is whether any incorrect solu-
tions manage to pass all the test cases. To address
this, we conducted an experiment using the eval-
uation set of the COFFEE dataset. We randomly
sampled 200 wrong code instances and calculated
the pass ratios of the wrong codes. We show the
statistics of the distribution of pass ratios.
As shown in Table 5, the maximum pass ratio
is 0.985, which suggests that there are no wrong
solutions that passed all the test cases. The mean
score is 0.342, indicating that on average, wrong
solutions fail the majority of the test cases. We
further analyze the COFFEE -TESTand verified that
no wrong solutions pass all the test cases.
Diverse difficulty of test cases. To demonstrate
that our generated test cases cover a range of dif-
ficulties, we analyzed the pass ratio distribution
for incorrect code samples annotated in the dataset.
We focused on a single problem from the COFFEE
evaluation set.
As shown in Figure 9, the results revealed that
various incorrect solutions for this problem exhib-
ited different pass ratios, indicating that our test
cases encompass diverse difficulty levels.
A.1.5 Data Analysis
We conduct following experiments to explore orig-
inal features in C OFFEE dataset.
Length of edit trace We analyze the distribution
of average length of edit trace by problem level. In0.0 0.2 0.4 0.6 0.8 1.0
Pass Ratio01234DensityID 09655, T otal 90
ID 02606, T otal 69
ID 01074, T otal 63
ID 01158, T otal 76
ID 01463, T otal 60Figure 9: Kernel Density Estimation plot of the pass
ratio distribution for incorrect code samples.
Figure 5.a, we observe a steady increase in the aver-
age length of edit traces from human programmers
with increasing difficulty levels. This suggests that
problems in COFFEE are challenging for human
programmers, as they tend to make more incorrect
submissions for problems with higher difficulty
levels.
Code diversity. To assess the diversity of human-
written codes compared to machine-generated
codes, we conduct a similarity analysis on error
codes. Specifically, we sample problems from
COFFEE where more than 100 users submitted so-
lutions and collect the wrong code from these users.
We also sample an equal number of wrong codes
from ChatGPT and GPT-4 with top-p sampling of
p= 0.95and temperature T= 0.6. For each set
of incorrect solutions sampled from user solutions,
ChatGPT, and GPT-4, we use CodeBERT (Feng
et al., 2020) to compute embeddings for incorrect
solutions and measure cosine similarity for all pos-
sible pairs in the set.
Figure 5.b shows the histogram of the number
of problems by the average embedding similarity
of incorrect solution pairs. We find that machine-
generated codes ( i.e., ChatGPT, GPT4) tend to be
more similar to each other than human-generated
codes, indicating that collecting human-generated
code allows for more diverse set of wrong code
samples.
Code complexity To show that problems in COF-
FEEare challenging for code LLMs, we measure
the code generation performance of GPT-4 using
Pass@1 and compare it with the solve rate of hu-
man programmers. Note that the latter is given
as the metadata from the programming platform
and computed as the proportion of correct solu-
tions among all solutions submitted for problemsinCOFFEE . The results (Figure 5.c) suggest that
even the state-of-the-art LLM, i.e., GPT-4, strug-
gles to produce correct solutions for problems in
COFFEE and lags behind human programmers.
A.1.6 Analysis on Train-test Overlap
A possible concern is that the training data in COF-
FEEmight overlap with the test data in the code
benchmark ( i.e., HumanEval). Therefore, we fol-
low Odena et al. (2021) and measure the amount of
identical codes (based on the number of repeated
lines) between the training and test data. Figure 10
reports both the fraction and the absolution number
of line overlaps between COFFEE and HumanEval.
We observe that most solutions in COFFEE do not
contain lines that appear in the benchmark dataset
which we evaluate our models on.
A.2 Details of C OFFEE EVAL
A.2.1 Implementation Details
We use DeepSeekCoder-7b6as our backbone
model using QLoRA (Dettmers et al., 2023), in-
corporating 4-bit quantization with a learning rate
of 5e-5 and a batch size of 4 for 2 epochs. The train-
ing is run on 8 NVIDIA GeForce RTX 3090 GPUs.
Regarding the LoRA configuration, we specify the
dimension of low-rank matrices as 64, and alpha
as 16.
A.2.2 Training Details
Following the approach of Wang et al. (2023a), we
train the editor in two phases. The initial phase in-
cludes the keywords [Correct] and[Wrong]
in the code sequence, while the second phase trains
the model without these keywords.
Phase I. We finetune our editor model ϕus-
ing pairwise data of correct edits (q, y, c∗, y∗)∈
Dcorrect and incorrect edits (q, y,˜c,˜y)∈ D wrong
inCOFFEE . During this phase, we additionally ap-
pend keyword tokens t∗and˜t([Correct] and
[Wrong] respectively) with the target code se-
quences y∗and˜y. Therefore, the training objective
for the initial phase is defined as:
L(ϕ) =
−X
(q,y,c∗,y∗)∈Dcorrectlogpϕ(t∗, y∗|q, y, c∗)
−X
(q,y,˜c,˜y)∈Dwronglogpϕ(˜t,˜y|q, y,˜c)(3)
6https://huggingface.co/deepseek-ai/
deepseek-coder-6.7b-instructPhase II. After training the editor in Phase I, we
continually train the editor model using the same
dataset but without the keyword tokens. Thereby,
the training object for Phase II is defined as:
L(ϕ) =−X
(q,y,c∗,y∗)∈Dcorrectlogpϕ(y∗|q, y, c∗)
−X
(q,y,˜c,˜y)∈Dwronglogpϕ(˜y|q, y,˜c)(4)
We used the same hyperparameter settings in both
phases and the prompt for training the code editor
in Appendix D.3.1,
A.3 Details of Reference Methods in
COFFEE -GYM
Preference Tuning. Given a problem descrip-
tion, a wrong code, and the corresponding prefer-
ence set, we apply Direct Preference Optimization
(DPO) (Rafailov et al., 2023) to train our critic.
That is, we tune critic model to be biased towards
helpful feedback.
PPO. PPO optimizes the following objective:
LPPO(θ) =
ˆEth
min
rt(θ)ˆAt,clip(rt(θ),1−ϵ,1 +ϵ)ˆAti
(5)
where rt(θ)is the probability ratio between the
current policy θand the old policy θold,ˆAtis an
estimator of the advantage function at timestep t,
andϵis a hyperparameter that controls the clipping
range.
DPO. From SFT model we sample 10 feedback
strings and score them with COFFEE EVAL. Among
the 10 feedback collect feedback with top-1 score
and bottom-1 score and construct preference pair,
i.e.,(c+, c−), for DPO training. Using this dataset,
we additionally conduct DPO training on SFT
model.
Rejection sampling. From SFT model we sam-
ple 10 feedback strings and score them with COF-
FEEEVAL. Among the 10 feedback we only collect
feedback with top-1 score and construct dataset for
further training. Using this dataset, we additionally
conduct SFT.
Terms and License. For our implementation and
evaluation, we use Huggingface, TRL and vLLMlibrary.7Both libraries are licensed under Apache
License, Version 2.0. We have confirmed that all
of the artifacts used in this paper are available for
non-commercial scientific use.
B Experimental Details
B.1 Benchmarks
For our experiments, we consider the following
benchmarks:
HumanEvalFix HumanEvalFix is a task of Hu-
manEvalPack, manually curated using solutions
from HumanEval (Chen et al., 2021a) for the task
of code editing. Given an (i) incorrect code func-
tion, which contains a subtle bug, and (ii) several
unit tests ( i.e., test cases), the model is tasked to
correct/fix the function. The dataset consists of 164
samples from the HumanEval solutions, and each
sample comes with human-authored bugs across
six different programming languages, thus cover-
ing 984 bugs in total. The bugs are designed in a
way that the code is executed without critical fail-
ure but fails to produce the correct output for at
least one test case.
We have confirmed that the dataset is licensed
under the MIT License and made available for non-
commercial, scientific use.
Reason for exclusion. We excluded Debug-
Bench and CodeEditorBench for the following rea-
sons:
•DebugBench (Tian et al., 2024) is a debug-
ging benchmark consisting of 4253 instances
with 4 major categories and 18 minor types
of bugs. The metric is based on the test suites
provided by LeetCode, requiring API calls
for evaluation. Due to the huge amount of
API calls, LeetCode blocked the access dur-
ing the evaluation, which lacked the accurate
scoring. Also, some questions were graded in-
correctly even though ground-truth solutions
were given. Therefore, we decided not to use
DebugBench for evaluation.
•CodeEditorBench (Guo et al., 2024b) is the
framework designed for evaluating the perfor-
mance of code editing. Code editing is cate-
gorized into four scenarios, debugging, trans-
lation, polishing, and requirement switching,
where our main focus is on debugging. Sim-
ilar to DebugBench, ground-truth solutions
7https://huggingface.co/0.00.10.20.30.40.50.60.70.80.91.0
Fraction of Lines Duplicated03000600090001200015000Number of Tasks(a) Fraction of line overlaps.
0 1 2
Number of Duplicated Lines03000600090001200015000Number of Tasks
(b) Absolute number of line overlaps.
Figure 10: Analysis on train-test overlap between COF-
FEEand HumanEval.
could not pass the unit test for some ques-
tions. Also, functions imported from external
python files and some specific packages were
used in questions without details, which made
the question imprecise. So, we sent CodeEdi-
torBench out of our scope.
B.2 Metrics
We use Pass@1 score to measure the code edit-
ing performance for all benchmarks. Specifically,
Pass@1 is computed as the expected value of the
correct rate per problem, when nsamples were
generated to count the number of correct samples c
for each problem.
Pass@1 =E
Problemshc
ni
×100 (6)
B.3 Feedback Quality Evaluation
To assess the feedback quality in Likert-scale, we
use G-Eval (Liu et al., 2023c) and prompt GPT-4-
Turbo to evaluate the feedback quality. Specifically,
given problem description, input and output format,
wrong code, and the corresponding feedback, we
prompt GPT-4 to classify the feedback into one of
the following five categories.
•Completely incorrect : Feedback has no valid
points and is entirely misleading.
•Mostly incorrect : Feedback has some valid
points but is largely incorrect or misleading.•Neutral or somewhat accurate : Feedback is
partially correct but contains significant inac-
curacies or omissions.
•Mostly correct : Feedback is largely accurate
with only minor mistakes or omissions.
•Completely correct : Feedback is entirely ac-
curate and provides a correct assessment of
the code.
We apply the same top- psampling and temperature
in Table A.1.1 and include the prompt used for the
evaluation in Appendix D.3.2.
B.4 Human Evaluation on Quality of
Feedback
Task description. The error detection and cor-
rection scores were determined by human annota-
tors evaluating feedback on incorrect code using
a Likert scale. The error detection score evaluates
how accurately the feedback identifies errors in
the incorrect code, while the error correction score
assesses the correctness and effectiveness of the
corrections suggested in the feedback.
Preparing feedback for the evaluation. We aim
to analyze the quality of the feedback generated
for code editing. We randomly sample 100 codes
from COFFEE -TEST to assure the correctness of
our evaluation. For generating feedbacks, we use
the erroneous codes provided in the dataset.
Details on human evaluation. We conduct hu-
man evaluation by using Amazon Mechanical Turk
(AMT), which is a popular crowd sourcing plat-
form. As we need workers who have enough expe-
rience with Python, we conduct a qualification test
to collect a pool of qualified workers. In result, we
recruit 186 workers who have passed the test, and
task them to evaluate the quality of the feedback
on Likert scale, ranging from 1 to 5. Each sample
is evaluated by three different raters to ensure the
reliability. Based on our estimates of time required
per task, we ensure that the effective pay rate is at
least $15 per hour. We use the evaluation interface
in Figure 12.
C Additional Analysis
C.1 Iterative Editing
Inspired by Zheng et al. (2024), we consider a prac-
tical setting where models are tasked with itera-
tive code generation with feedback. We employedInitial 1 iter 2 iter 3 iter 4 iter 5 iter
Iteration78798081828384Pass@1
PPO
Rejection sampling
DPO
SFTFigure 11: Performance on test cases from HumanEval,
measured under the iterative edit setting.
OpenCoderInterpreter-DS-7b as our codeLLM and
used our feedback model to provide evaluations on
the generated code. Our experiments included com-
parisons with reference methods in COFFEE -GYM.
As shown in Figure 11, using our feedback model
consistently enhanced performance over successive
iterations. Consistent with our main experiment
findings, both PPO and DPO improved feedback
quality more effectively than rejection sampling.
These results underscore the practical applications
of our approach.
C.2 Practical Programming Problems
To further explore the applicability of our feedback
model (PPO- COFFEE EVAL) to practical program-
ming problems and assess its robustness across
different domains, we conducted experiments us-
ing NumpyEval (Zan et al., 2022). This dataset
focuses on the general coding domain, specifi-
cally involving problems related to the NumPy
library. We chose this benchmark to test our
model’s performance on unseen domains and eval-
uate its generalizability beyond our initial scope.
We utilized OpenCodeInterpreter-DS-Coder-7b as
both the generation and editing model, while
PPO-CoffeeEval served as the feedback model.
To establish a baseline, we compared our ap-
proach against a Self-Feedback method, which
used OpenCodeInterpreter-DS-Coder-7b for feed-
back as well.
As shown in Table 6, our PPO-CoffeeEval model
outperforms the baseline. These results suggest
that our feedback model is not overfitted to Coffee
dataset, and did not lost generalization ability to
unseen domains.
For further analysis, we conducted a case study
to examine the model’s performance in more de-Model Pass@1
OpenCodeInterpreter-DS-Coder-7b 68.3
+ PPO-C OFFEE EVAL 70.3
Table 6: The performance of different feedback models
on NumpyEval.
tail. As illustrated in Figure 14 and Figure 15, our
model demonstrates the ability to generate help-
ful feedback even when the problem description is
provided in Python comments rather than natural
language format. In some instances, the feedback
includes the necessary editing code. This capabil-
ity highlights the potential for using our model in
practical scenarios, where users’ queries can take
various forms and formats, enhancing its applica-
bility in real-world programming environments.
C.3 Case Study on SFT vs. PPO
In Figure 13, we present examples of generated
feedback. Although the feedback generated by the
SFT model appears plausible, it provides unnec-
essary feedback which may confuse the editor in
feedback-augmented code editing. In contrast, our
model (PPO) provides focused and helpful feed-
back on the incorrect part without unnecessary in-
formation. This result aligns with Figure 8, demon-
strating that our model generates more accurate and
helpful feedback compared to other models.
D Prompts for Our Experiments
D.1 Correct Feedback Annotation Prompt
Generate an explanation, analyzation,
and plan to generate code prompt for
the last task considering the example
task instances. Your plan should show
enough intermediate reasoning steps
towards the answer. Construct the plan
as much as you can and describe the
logic specifically. When constructing
the plan for the code prompt, actively
use ’if else statement’ to take
different reasoning paths based on the
condition, ’loop’ to efficiently
process the repititive instructions, ’
dictionary’ to keep track of
connections between important variables
.
[Example 1]
Example task instances:
{example_instances_of_task1}
Output format:
{output_format_of_task1}Explanation:
{analysis_of_task1}
...
[Example 4]
Example task instances:
{example_instances_of_target_task}
Output format:
{output_format_of_target_task}
Explanation:
D.2 Wrong Feedback Annotation Prompt
Generate feedback that guides the
refinement from Code before editing to
Code after editing. Assume that the
code after editing is 100% correct and
your feedback should specifically guide
the editing to the code after editing.
Please point out only the guidance
from the code before editing to the
code after editing. Do not provide
feedback on the code after editing or
any feedback beyond the code after
editing.
[Example 1]
Problem Description:
{description}
Code before editing:
{wrong_code}
Code after editing:
{next_wrong_code}
Feedback for Refining the Code:
{feedback}
...
[Example 4]
Problem Description:
{description}
Code before editing:
{wrong_code}
Code after editing:
{next_wrong_code}
Feedback for Refining the Code:
D.3 Test Case Generation Prompt
Given the input format and python code,
please provide at least 30 challenging
test input values to evaluate its
functionality.For every start ofsamples, please attach <start> token to
indicate that the input string has
started. Also, for every end of samples
, please attach <end> token to indicate
that the input string has ended.
input format:
{input format}
python code:
{python code}
Sample:
D.3.1 Code Editor Prompt
Provide feedback on the errors in the
given code and suggest the correct code
to address the described problem.
Description:
{description}
- output format: {output_format}
- input format: {input_format}
Incorrect code:
‘‘‘python
{wrong_code}
‘‘‘
Feedback:{feedback}
Correct code:
D.3.2 G-Eval Prompt
You will be provided with feedback on
the given incorrect code. Classify the
accuracy of this feedback using a
Likert scale from 1 to 5, where:
1 (Completely incorrect): This feedback
has no valid points and is entirely
misleading.
2 (Mostly incorrect): This feedback has
some valid points but is largely
incorrect or misleading.
3 (Neutral or somewhat accurate): This
feedback is partially correct but
contains significant inaccuracies or
omissions.
4 (Mostly correct): This feedback is
largely accurate with only minor
mistakes or omissions.
5 (Completely correct): This feedback
is entirely accurate and provides a
correct assessment of the code.
Just generate a score from 1 to 5 based
on the accuracy of the feedback.
Description:
{description}
- output format: {output_format}
- input format: {input_format}
Incorrect code:
‘‘‘python
{wrong_code}
‘‘‘Feedback:{feedback}
Score:Figure 12: The interface used for human evaluation on the feedback.Figure 13: Examples of the feedback from SFT and PPO model in C OFFEE -GYM.
Figure 14: Examples of the feedback from the PPO model on NumpyEval.Figure 15: Examples of the feedback from the PPO model on PandasEval.