Words Matter: Reducing Stigma in Online Conversations about Substance
Use with Large Language Models
Layla Bouzoubaa, Elham Aghakhani, Rezvaneh Rezapour
Drexel University
{lb3338, ea664, shadi.rezapour}@drexel.edu
Abstract
Stigma is a barrier to treatment for individ-
uals struggling with substance use disorders
(SUD), which leads to significantly lower treat-
ment engagement rates. With only 7% of those
affected receiving any form of help, societal
stigma not only discourages individuals with
SUD from seeking help but isolates them, hin-
dering their recovery journey and perpetuating
a cycle of shame and self-doubt. This study
investigates how stigma manifests on social
media, particularly Reddit, where anonymity
can exacerbate discriminatory behaviors. We
analyzed over 1.2 million posts, identifying
3,207 that exhibited stigmatizing language to-
wards people who use substances (PWUS). Us-
ing Informed and Stylized LLMs, we develop
a model for de-stigmatization of these expres-
sions into empathetic language, resulting in
1,649 reformed phrase pairs. Our paper con-
tributes to the field by proposing a computa-
tional framework for analyzing stigma and des-
tigmatizing online content, and delving into
the linguistic features that propagate stigma to-
wards PWUS. Our work not only enhances un-
derstanding of stigma’s manifestations online
but also provides practical tools for fostering a
more supportive digital environment for those
affected by SUD.
1 Introduction
Warning: This paper includes language and con-
tent that may be offensive or triggering. Every
day, people struggling with substance use disorders
(SUD) face a pervasive and often hidden enemy:
stigma. This stigma, often deeply ingrained in so-
cietal attitudes, can act as a significant barrier to
treatment and recovery. In fact, only approximately
7% of people living with an SUD receive any form
of treatment (Substance Abuse and Mental Health
Services Administration, 2023), with stigma re-
ported as a major barrier (Centers for Disease Con-
trol and Prevention, 2023). SUD is a critical public
health challenge in the US and worldwide, and theType Statement
OriginalI have no empathy for drug addicts. I had
friends and family who have struggled with the
“disease”. Everyone knows what happens when you
start, and you usually end up dead. Many of
my old friends have become addicts and I don’t
understand especially the ones with kids.De-stigmatizedI find it difficult to empathize with
individuals facing substance use challenges. I
had friends and family who encountered these
difficulties. It’s widely acknowledged that
there are risks involved from the outset, and
the outcomes are often heartbreaking. Several of
my old friends have dealt with these challenges,
and it’s particularly perplexing to me when they
are parents.
Table 1: Example of directed stigmatizing language.
De-stigmatized version generated with our Informed +
Stylized model using GPT-4 removed stereotypes and
harmful context while preserving the tone (stigma is in
red, destigmatized counterparts is in blue).
substantial stigma associated with these conditions
only exacerbates the problem.
Traditional support systems, although beneficial,
often remain underutilized due to their perceived in-
accessibility or the overwhelming stigma surround-
ing SUD, thus rendering this topic a societal taboo.
Social media platforms like Reddit have emerged
as important spaces for community discussions
(Bouzoubaa et al., 2023, 2024). However, the
anonymity provided by these environments some-
times exacerbates stigmas, leading to discrimina-
tion. People suffering from SUD often encounter
derogatory comments, judgment, or misinforma-
tion online (Schomerus et al., 2011), which can
reinforce self-stigma and stop them from seeking
help. The spread of stigmatizing attitudes on social
media can also influence public opinion, further
perpetuating the stereotypes and prejudices against
those with SUD (McLaren et al., 2023). As a result,
despite the potential for support, the digital space
can mirror and magnify the very societal stigmas
it has the power to dismantle, affecting individu-
als’ mental health and recovery processes adversely
(Matsumoto et al., 2021; McNeil, 2021).
The widespread stigma surrounding SUD re-arXiv:2408.07873v1  [cs.CL]  15 Aug 2024quires urgent and innovative solutions. Leveraging
technology and social media, we can develop em-
pathetic, supportive interventions that fight against
this stigma (Rahaman et al., 2023). While research
has explored mental health conversations and pub-
lic perceptions on social media (Robinson et al.,
2019), there remains a significant gap in efforts to
destigmatize language in these discussions. Ad-
dressing this gap is crucial for fostering a more un-
derstanding and supportive environment for those
affected by SUD.
Our work explores this opportunity and exam-
ines how stigmatizing language manifests in online
communities and what solutions can be applied
for de-stigmatizing such narratives (Table 1). Our
study focuses on two research questions:
-RQ1: How does stigmatizing language man-
ifest in non-drug-related Reddit communities
when discussing SUD, and what are the underly-
ing factors that contribute to such expressions?
-RQ2: How can we leverage LLMs to effec-
tively de-stigmatize language, and what factors
influence the success of this process?
To address these research questions, we collected
over 1.2 million posts from non-drug-related sub-
reddits, identifying 3,207 posts containing stig-
matizing language towards people who use sub-
stances (PWUS). Leveraging large language mod-
els (LLMs), we developed a framework to charac-
terize stigma based on conceptualization of Link
and Phelan (2001) ( labeling, stereotyping, separa-
tion, status loss, and discrimination ) and transform
them into more empathetic versions, resulting in
1,649 de-stigmatized pairs. Our analysis showed
that stimulants and cannabis were the most fre-
quently mentioned substances, with stigma more
generally being associated with interpersonal rela-
tionships and moral judgments. Human evaluations
showed that our Informed + Stylized system using
GPT-4 can reduce stigma while preserving the orig-
inal tone and relevance. Automatic evaluations
further confirmed that our approach effectively re-
duced stigma while maintaining the stylistic and
psycholinguistic properties of the original posts.
Our work makes several key contributions: (1)
public release of a unique dataset of labeled stig-
matizing posts; (2) demonstration of frameworks
for de-stigmatizing text; and (3) exploration of the
linguistic characteristics of stigma expressions to-
wards people who use substances (PWUS) online.
Additionally, this study introduces innovative usesof LLMs for generating suggestions to mitigate
potentially harmful language.
2 Related Work
2.1 Stigma and Language
Stigma, a complex social phenomenon, is deeply
intertwined with language. The linguistic relativity
principle, as described by Whorf (1956), suggests
that language shapes our perception of reality, in-
cluding the formation of stigmatizing views. In the
context of substance use experiences (SUE) and
SUD, stigma can manifest in multiple forms: self-
stigma , often rooted in shame (Luoma et al., 2012);
public stigma , negative attitudes and beliefs which
lead to discrimination and social exclusion; struc-
tural stigma , which limits resources and opportuni-
ties, embedded in societal norms and institutional
practices (Hatzenbuehler, 2016).
Building upon Goffman (2009)’s foundational
work, Link and Phelan (2001) conceptualized
stigma as the co-occurrence of labeling, stereo-
typing, separation, status loss, and discrimination.
This framework highlights how stigma operates
alongside power inequalities, influencing both the
individual and society at large. Research has ex-
plored the manifestation of stigma in online com-
munities (Nippert et al., 2021), particularly within
social media platforms (Clark et al., 2021), reveal-
ing both the potential for support and the ampli-
fication of existing stigmas, particularly among
mental health and opiate-dedicated online commu-
nities (Chen et al., 2022; Eschliman et al., 2024).
Linguistic analysis has proven valuable in iden-
tifying and characterizing stigmatizing language.
Dehumanizing labels and biased language can per-
petuate negative stereotypes and contribute to dis-
crimination (Giorgi et al., 2023). A recent study by
the CDC found that while stigmatizing language in
traditional media has decreased over time, its use
on social media platforms has increased (McLaren
et al., 2023), highlighting the need for targeted in-
terventions in these spaces. The specific linguistic
cues that distinguish stigmatizing content can differ
between those with lived experience of substance
use and those without, particularly regarding lan-
guage considered “othering” and the use of labels
like “addict” (Giorgi et al., 2023).
2.2 LLMs and Social Impact
LLMs have shown promise in addressing social is-
sues like hate speech detection (Guo et al., 2023a)and bias mitigation (Schlicht et al., 2024). Recent
research demonstrates that LLMs can perform on
par with or even surpass benchmark machine learn-
ing models in identifying hate speech (Kumarage
et al., 2024). Moreover, carefully crafted prompt-
ing strategies can leverage the knowledge encoded
in LLMs to improve the detection of nuanced and
context-dependent forms of hate speech (Guo et al.,
2023b). However, the application of LLMs in sen-
sitive domains raises ethical concerns. The “black
box” nature of these models can make it difficult to
understand their decision-making processes, rais-
ing issues of transparency and accountability (Guo
et al., 2024). Additionally, biases in training data
can be inadvertently perpetuated, leading to dis-
criminatory outcomes (Mei et al., 2023). Address-
ing these ethical considerations is important for
the responsible and equitable use of LLMs in de-
stigmatization efforts.
2.3 De-stigmatization Efforts
Language-based interventions, such as the use of
person-first language and empathetic communica-
tion, have shown promise in reducing stigma re-
lated to substance use. Research has demonstrated
the impact of specific word choices on percep-
tions of individuals with SUD (Kelly et al., 2010).
McGinty et al. (2018) proposed a set of commu-
nication strategies to reduce stigma, including the
use of sympathetic narratives, removing blame, and
highlighting structural barriers to treatment. These
findings contributed notably as the National Insti-
tute on Drug Abuse (NIDA) has also published
guidelines for using non-stigmatizing language in
discussions of SUD (NIDA, 2023).
AI-mediated interventions, particularly those
leveraging LLMs, have the potential to scale and
automate de-stigmatization efforts. While prior
work has focused on text detoxification and bias
reduction, in general, (Dale et al., 2021b; Mendel-
sohn et al., 2020; Pryzant et al., 2020), the spe-
cific application to SUD-related stigma remains
underexplored. Additionally, Spata et al. (2024)
highlights the importance of using appropriate and
well-validated measures to assess the effectiveness
of interventions aimed at reducing stigma.
Our work builds upon the previous work by intro-
ducing a comprehensive computational approach
to identify and categorize stigma. Focusing on pub-
lic stigma, which we refer to as directed stigma
throughout the paper, we operationalize Link andPhelan (2001)’s framework, analyzing instances
of labeling, stereotyping, separation, and discrimi-
nation towards PWUS in discussions in non-drug-
related Reddit communities.
3 Data
To achieve the study’s objective of addressing stig-
matizing language, we specifically focused on non-
drug-related subreddits. This choice was made to
capture how stigmatizing language manifests exter-
nally rather than within communities where mem-
bers discuss their own experiences with drug use.
Within these communities, stigmatizing language
is often directed towards oneself (e.g., “No one
should hire a junkie like me, I’m useless”) or de-
scribes situations where members felt stigmatized
(e.g., “My co-workers stopped having lunch with
me when they learned I’ve been to rehab twice”)
which differs from the external stigmatizing lan-
guage we aim to address. By focusing on non-
drug-related subreddits, we ensure that our analy-
sis targets the perpetuation of harmful stereotypes
by those outside the drug-using community. This
methodological choice is informed by the need to
differentiate between internal and external stigma,
as highlighted in the literature on stigma (e.g., Link
and Phelan (2001)’s attributes of stigma).
Data Collection. To investigate the manifestation
of stigmatizing language in non-drug-related on-
line communities, we collected data from four pop-
ular subreddits: r/unpopularopinion ,r/offmychest ,
r/medicine , and r/nursing . The first two subred-
dits were chosen for their high activity levels, di-
verse user bases, and relevance to discussions of
substance use and SUDs. Recent research has high-
lighted the prevalence of stigmatizing language
within medical professional communities as well
on platforms such as Twitter, although the overall
use of stigmatizing and de-stigmatizing language
was found to be low (Scott Graham et al., 2022).
Given the critical role that healthcare profession-
als play in the lives of individuals with SUD, we
included two of the most popular subreddits for
healthcare professionals; r/nursing andr/medicine .
We collected a total of 3.8 million posts from
these subreddits. Table 2 shows the number of posts
per subreddit. To ensure data quality, we excluded
posts that were removed, deleted, or associated
with deleted accounts. Additionally, we filtered
out posts where the combined title and body text
were less than 10 words to focus on substantiveSubreddit # Subscribers # Posts Date Range
r/medicine 478K 116,702 05/2005 - 12/2022
r/nursing 715K 212,755 12/2009 - 12/2022
r/offmychest 3.2M 1,607,341 02/2010 - 12/2022
r/unpopularopinion 4.3M 2,044,463 08/2013 - 12/2022
Table 2: Selected subreddits and raw post and subscriber
counts as of July 2024
discussions. This resulted in a final dataset of 1.51
million posts for analysis.
4 Methodology
To develop a stigma detection model and destig-
matize texts, we first need to filter posts related to
substance use. This is followed by detection and
de-stigmatization processes. Figure 1 shows our
study’s overall pipeline. Each step is detailed in the
following sections.
4.1 Developing a Stigma Detection Model
4.1.1 Filtering Substance Use-Related Posts
To identify posts containing stigmatizing language
related to substance use, we first filtered posts col-
lected from non-drug-related subreddits to find rele-
vant discussions. Drug-related content includes any
mention of illicit drugs or drug use (e.g., heroin,
cocaine, LSD), prescription drugs that can be mis-
used (e.g., narcotics, benzodiazepines), and other
drugs that are not prescription but are also com-
monly misused (e.g., inhalants, bath salts). We
began by manually annotating a random sample of
200 posts to establish a ground truth for relevance.
Two annotators independently assessed each post,
achieving 100% agreement on the presence or ab-
sence of substance use-related content.
Given the nuanced nature of language around
substance use, including slang and idiomatic ex-
pressions, we used LLMs with few-shot prompting
to identify posts within the larger dataset. Based
on a comprehensive assessment of performance
metrics, including precision, recall, F1-score, and
estimate time (see Appendix A), we selected GPT-
3.5 Turbo as the most suitable model for this task.
As a result of Task 1, we identified around 33,064
posts containing at least one mention of drugs or
drug-related content.
Validation Layer. Given the tendency of GPT-
3.5 to overgeneralize, we implemented a validation
layer using GPT-4 Turbo to re-evaluate all posts
initially flagged as containing substance use-related
content (N = 33,064). To evaluate the effectiveness
of this validation layer, we randomly sampled 725posts from the GPT-3.5 output (252 labeled as drug-
related ( D) and 473 as non-drug-related ( ND)) and
conducted a manual evaluation. The posts labeled
asDby GPT-3.5 were then passed through the GPT-
4 validation layer. Out of the 252 posts initially
labeled as D, 212 were confirmed as Dby GPT-4,
resulting in an F1 = 0.86. From the 33,064 posts
labeled as Dby GPT-3.5, 16,277 were validated as
Dby GPT-4.
4.1.2 Extracting Stigmatizing Language
The posts labeled as containing drug content were
then labeled for their inclusion of stigmatizing lan-
guage. Stigmatizing language could be in the form
of directed language towards PWUS that perpetu-
ates harmful stereotypes, expressions of internal-
ized stigma (i.e., self-stigma), or illustrations of
structural or systemic stigma (e.g., criminal justice
towards PWUS in the United States). To do this,
we took a random sample of 200 posts from the
16,277 posts labeled Dand manually annotated for
the inclusion of stigmatizing language. Any posts
that contained directed stigmatizing language were
also broken down into four attributes: 1) labeling,
2) stereotyping, 3) loss of power, and 4) discrimi-
nation. This process was re-iterated several times
until substantial agreement was met ( k= 0.67).
The remaining posts were then labeled using GPT-
4 Turbo using the prompt in Appendix B.
Explainability of Stigma Detection. In the pur-
suit of transparency and interpretability, we incor-
porated an explanation layer into our stigma detec-
tion model. Specifically, when the model identi-
fied a post as containing directed stigma towards
PWUS, it was prompted to provide a detailed ex-
planation for its classification by identifying the
specific instances within the text that corresponded
to each of the four elements of stigma outlined
by Link and Phelan (2001): labeling, stereotyp-
ing, separation, and discrimination, mimicking our
annotation process.
4.2 De-Stigmatizing Problematic Language
To address and mitigate the impact of stigmatiz-
ing language in texts, we used two different LLMs
across three different Models. Our objective is to
determine which model is most effective at trans-
forming stigmatizing language into expressions
that are more empathetic and inclusive.
Model 1: Baseline. In the baseline phase, we
explored the capabilities of two LLMs in zero-shotR e d d i t  D a t a
( N  =  1 . 5 1 M )P o s t s  w / d r u g  
m e n t i o n ?
( N  =  3 3 , 0 6 4 )D a t a  C o l l e c t i o n  a n d  F i l t e r i n gS t i g m a  D e t e c t i o n  +  E x p l a n a t i o n
G P T - 3 . 5 T
V a l i d a t e d  
p o s t s  w / d r u g  
m e n t i o n
( N  =  1 6 , 2 7 7 )
G P T - 4 TD i r e c t e d
( N  =  1 , 9 4 9 )N o  S t i g m a
( N  =  1 3 , 0 7 0 )L a b e l i n g
S t e r e o t y p i n g
S e p a r a t i o n
D i s c r i m i n a t i o nD e - S t i g m a t i z a t i o n
G P T - 4 T
L l a m a 3E v a l u a t i o nD e - s t i g m a t i z e d  
p o s t s  ( 5 0 )+
F i n a l  M o d e lS t y l i z e d
G P T - 4 TB e s t  O v e r a l l  Q u a l i t y  G e n e r a t i o n sV a l i d a t i o n
G P T - 4 TS e l f
( N  =  1 , 1 9 9 )S t r u c t u r a l
( N  =  5 9 )B a s e l i n e :  Z e r o - s h o t I n f o r m e d :  I n s t r u c t i o n  +  
E x p l a n a t i o n  +  F e w - s h o t3 . S t y l i z e d :  I n f o r m e d  +  
S t y l i s t i c  P r o f i l eS e n t e n c e  s t r u c t u r e ,  p a s s i v e  v o i c e ,  
l e x i c a l  d i v e r s i t y ,  e m o t i o nS t y l i s t i c  P r o f i l eFigure 1: Full de-stigmatization pipeline.
Stigma Type
Substance Category Directed Self Structural Total
Stimulants 818 380 20 1218
Cannabis 515 276 27 818
Narcotics 501 250 18 769
Depressants 92 102 6 200
Hallucinogens 90 68 4 162
Reversal Agents 38 3 0 41
Drugs of Concern 7 7 0 14
Synthetic Cannabinoids 11 3 0 14
Other 4 3 1 8
Designer Drugs 6 0 0 6
Unspecified 537 475 9 1021
Table 3: Cross-tabulation of substance categories men-
tioned in a post by the type of stigmatizing language
used. Note that multiple substance categories may be
mentioned in the same post.
de-stigmatization: GPT-4 Turbo and Llama 3-70B-
Instruct. We provided the models with the original
stigmatizing post and instructed them to generate
a de-stigmatized version without any additional
context or guidance. This approach allowed us to
assess the inherent de-stigmatization capabilities of
these models in the absence of explicit knowledge
or stylistic refinements.
Model 2: Informed LLM. Inspired by the prin-
ciples of “Constitutional AI,” (Bai et al., 2022)
we enhanced the LLM prompts in Phase 2 with
explicit instructions, definitions, and explanations
related to stigma. Constitutional AI refers to the
development and operation of AI models that ad-
here to the principles and legal standards, ensuring
respect for human rights, ethical guidelines, and
public accountability. Drawing upon the insights
gained from our analysis of stigmatizing language
(RQ1), we provided the model with a structured
understanding of the four stigma elements (label-
ing, stereotyping, separation, and discrimination)
and their manifestations in the context of substance
use.
-Labeling : The model was instructed to iden-
tify and reword any labeling instances in the
post, guided by a definition, explanation, and
examples from RQ1 analysis.
-Stereotyping, Separation, and Discrimina-tion: The model was tasked with addressing
these three interrelated elements of stigma si-
multaneously. The prompt included definitions
for each element, examples from RQ1 analysis,
and an explanation as to why these elements
are harmful to guide the LLM to mitigate these
forms of stigma through rephrasing, reframing,
or adding context.
By incorporating these explicit instructions and
structured explanation of stigma, we aimed to guide
the LLM in generating de-stigmatized outputs that
actively addressed each of the four stigma elements
identified in the original post.
Model 3: Informed LLM + Stylistic Considera-
tions. Building upon the informed LLM approach
of Phase 2, we further refined the de-stigmatization
process by incorporating stylistic considerations.
We aimed to ensure that the de-stigmatized output
not only addressed the harmful content but also
maintained the original post’s emotional tone and
stylistic features. To achieve this, we employed a
combination of techniques:
-Emotion Analysis: We used a pre-trained,
RoBERTa (Liu et al., 2019) model fine-tuned on
the GoEmotions dataset (Demszky et al., 2020)
1, to classify the emotional tone of the original
post and instructed the LLM to preserve this
tone in the de-stigmatized version.
-Punctuation and Syntax: We analyzed the use
of punctuation and sentence structure (i.e. sen-
tence length variation) in the original post and
encouraged the LLM to replicate these patterns
in the output.
-Stylistic Elements: Posts were analyzed for
phrase style, specifically the measure of textual
lexical diversity (MTLD) (McCarthy and Jarvis,
2010) and the use of passive voice, to ensure
that the de-stigmatized output maintained the
original post’s overall writing style.
These elements, plus the explanations, were used
1https://huggingface.co/SamLowe/
roberta-base-go_emotionsto produce de-stigmatized outputs that were less
harmful and stylistically congruent with the orig-
inal post, thereby maintaining the author’s voice
and reducing the potential for inauthenticity.
4.2.1 Evaluation of De-Stigmatized Posts
Human Evaluation. To assess the effectiveness of
our six systems (baseline, informed, and informed
+ stylized for GPT-4 and Llama3), we conducted a
human evaluation with five reviewers on a random
sample of 110 posts (a total of 660 generated texts).
Our reviewers come from a variety of backgrounds,
including HCI, NLP, and Social Computing. To
evaluate the systems, we instructed our reviewers
to analyze the generated text from each model and
rank the models based on the overall quality, the ex-
tent of de-stigmatization, and the faithfulness of the
outputs. Following traditional NLG assessments,
quality was evaluated on criteria including natural-
ness, cohesion, human-likeness, and overall coher-
ence (Howcroft et al., 2020). The assessment of de-
stigmatization was judged based on removing neg-
ative or harmful stereotypes, and the systems with
the least amount of labeling, stereotyping, separa-
tion, status loss, and discrimination. Faithfulness
was evaluated based on the amount of transferred
information from the original post without unnec-
essary details (Sai et al., 2022). Comprehensive
evaluation guideline is provided in Appendix D.
Automatic Evaluation. To further evaluate the
stylistic similarity between original posts and their
de-stigmatized counterparts generated by our mod-
els, we conducted a linguistic analysis using LIWC
(Boyd et al., 2022). We then performed a t-test to
compare the linguistic features identified in both
the original and de-stigmatized texts. Given the
unique nature of our task, traditional metrics such
as BLEU (Papineni et al., 2002) or ROUGE (Lin,
2004) were deemed unsuitable because the gener-
ated text and its original counterparts differ signif-
icantly in meaning. Additionally, the absence of
pre-existing de-stigmatized versions of these texts
prevented us from conducting comparative analy-
ses with an established benchmark.
5 Experimental Results & Analysis
5.1 Characteristics of Stigmatizing Language
Mentioned Substances. Out of 16,277 posts dis-
cussing drugs, our stigma detection pipeline re-
sulted in 3,207 posts containing stigmatizing lan-
guage (Figure 1). Of these, 1,949 posts containeddirected stigma, 59 represented systemic/structural
stigma and 1,199 contained self-stigmatizing lan-
guage. As shown in Table 3, analysis of stigma-
tizing posts revealed that stimulants like “meth”
(methamphetamine) and “coke” (cocaine) were the
most frequently mentioned drug categories, fol-
lowed by cannabis (“weed”, “pot”) for all types of
stigma. Posts that mentioned drug use terms like
“drugs”, “high”, or “pills,” but no specific substance
were categorized as “Unspecified.”
Anatomy of Stigma. To further understand who,
didwhat , and whyin the context of stigma towards
PWUS in online discussions, we examined rep-
resentative entities, subject-verb pairs, and topic
models. Representative entities and subject-verb
pairs reveal the direction of the mentions (who),
while entity and substance frequencies highlight
the targets of stigma (what). Topic modeling al-
lows us to infer the underlying motivations and
contexts of stigmatizing language (why). For this
purpose, we used a multifaceted linguistic analy-
sis: we first extracted subject-verb pairs using part
of speech tagging in spaCy (Honnibal and Mon-
tani, 2020), classified emotions toward these pairs
in each post using GoEmotions (Demszky et al.,
2020) and RoBERTa (Liu et al., 2019), and per-
formed topic modeling with BERTopic (Grooten-
dorst, 2022) and KeyBERT (Grootendorst, 2020).
Within the posts showing directed stigma (Ap-
pendix C), we primarily observe expressions of
sadness andannoyance , with some neutrality . No-
tably, interpersonal relationships surface as a key
theme, featuring mentions of family members like
“sister,” “dad,” and “mother” alongside substances
like “cannabis” and “amphetamines.” This aligns
well with the overall prevalence of stimulants and
cannabis in substance mentions (Table 3). The dom-
inant topic, “Cannabis and Legalization Stigma”
centers on these substances, often referred to as
“it,” in a neutral tone primarily related to “smok-
ing.” Following closely is “Stigma Toward Interper-
sonal Relationships,” characterized by expressions
of knowledge ( I know ) from the subject “I”di-
rected towards family members, often tinged with
sadness . Another notable topic, “Moral Judgments
of Others,” reveals annoyance (I hate ) towards indi-
viduals like “neighbors,” “homeless,” and “junkies”
associated with “heroin” and other drugs.
Shifting to self-stigmatizing posts, we find dis-
tinct emotional undertones and actions. While in-
terpersonal entities are less prominent comparedto directed stigma, these posts feature more ac-
tion verbs and a wider variety of substances. The
primary topic, “Depression around Sobriety,” is
marked by expressions of possession ( I have ) and
state of being ( I am) in relation to depression ,so-
briety , and quitting . Disturbingly, another topic
reveals a desire foroverdose , specifically mention-
ing narcotics like “fentanyl,” “dilaudid,” and “tra-
madol,” alongside benzodiazepines like “Xanax”
and “clonazepam,” a combination known to be po-
tentially lethal due to respiratory arrest.
Finally, while only a few posts showed structural
stigma (N = 59), making it hard to analyze topics,
the emotions in these posts were mainly neutral.
5.2 De-Stigmatization with LLMs
Human Evaluation. Human evaluation of 110 ran-
domly selected and destigmatized posts is shown in
Table 4. Our reviewers found the Informed + Styl-
ized system with GPT-4 the best “Overall Quality”
for de-stigmatizing posts and the “Most Faithful”,
striking a balance between reducing stigma and
maintaining relevance and tone with the original
message. Interestingly, the “Most De-stigmatized”
system was the Baseline with GPT-4. While this
system may be effective at removing overt stigma-
tizing language, the lack of contextual awareness
and stylistic preservation resulted in posts sounding
generic, insincere, unfaithful, or disconnected from
the original poster’s voice. For example, with the
Baseline + GPT-4, this post:
“Decriminalizing drugs is a dangerous and
dumb idea and shouldn’t be done. what a won-
derful idea, let’s give druggies more room and
less consequences for doing the hard stuff. when
out, people need to keep an eye out to not acci-
dentally step on used needles, crime is up from
junkies who do anything to afford their next fix. ” is
de-stigmatized to:
“Decriminalizing drugs is a complex and con-
troversial topic that merits a thoughtful discus-
sion. There are valid concerns about the poten-
tial increase in visibility and accessibility of sub-
stances.states that have adopted more lenient poli-
cies towards drug possession, see increased public
drug use and challenges, such as discarded needles
and crime, which worries local residents. ”
For practical applications, this could affect the
model’s ability to foster genuine empathy and un-
derstanding in online communities. Table 1 dis-
plays a successful de-stigmatized post using ourbest model. The revised post is less harmful and
stereotypical but maintains the speaker’s tone and
overall message.
Automatic Evaluation. We conducted a stylistic
similarity analysis using LIWC to compare orig-
inal stigmatizing posts with their de-stigmatized
versions generated by our top-rated system (In-
formed + Stylized GPT-4). A pairwise two-way
t-test showed no significant differences in means
across all LIWC variables between the two sets of
posts. While certain categories like bigwords (use
of six-letter words or more) and cogproc (cognitive
processes) were more common in de-stigmatized
posts, the overall psycholinguistic properties were
largely maintained. This result is promising as it
shows our de-stigmatization approach effectively
reduced stigma while preserving the original style
and emotional tone, essential for authenticity.
6 Discussion
Stigma also stems from personal connections.
Our findings showed a complex landscape of
stigma within non-drug-related online communi-
ties where discussions about substance use of-
ten become entangled with interpersonal relation-
ships and ingrained societal biases - particularly to-
wards specific substances, namely stimulants (e.g.,
methamphetamine) and cannabis (e.g., “weed,”
“pot”). The frequent mentions of these substances
within a stigmatizing context may reflect societal
concerns about their visibility and impact, aligning
with our topic modeling results, where the domi-
nant topic in directed stigma is “Cannabis Legaliza-
tion Stigma.” These findings highlight the role of
close relationships (family, friends) in both express-
ing and experiencing stigma. For instance, within
the topic “Interpersonal Stigma,” we observe in-
dividuals expressing sadness and using the verb
“know” when discussing family members strug-
gling with substance use. This underscores the
need for de-stigmatization efforts to extend beyond
public forums and into private spheres, as stigma
from close social circles can be particularly harm-
ful due to the emotional weight and potential for
isolation (Luoma et al., 2012).
The online nature of these interactions presents
a duality of stigma manifestations that is important
to understand when developing any intervention.
While anonymity might offer a shield for individu-
als to express stigmatizing views they might sup-
press offline, it could also create a space for open
dialogue and support. The disinhibition afforded byModel LLM Best Overall Quality Most De-Stigmatized Most Faithful
Informed + Stylized GPT4 37 18 49
Informed GPT4 24 7 33
Informed Llama 19 8 16
Informed + Stylized Llama 13 3 6
Baseline Llama 9 32 2
Baseline GPT4 6 40 2
Table 4: Frequency of evaluation metrics by systems for 110 de-stigmatized posts.
online platforms could lead to more candid discus-
sions about SUD, potentially challenging stigma
through shared experiences and mutual understand-
ing. However, it may also create a space for mis-
informed judgments and harmful stereotypes, as
anonymity can reduce accountability.
When considering de-stigmatization efforts, any
digital intervention should consider the social ac-
tors in addition to the social constructs (e.g. hos-
pitals, employers). This would be considerably
important in collectivist communities (e.g. Indian
or Middle Eastern) where stigma towards family
members with an SUD (i.e. affiliate stigma ) may
prevent families from providing the necessary med-
ical support to their loved ones and ultimately de-
laying treatment (Corrigan et al., 2006).
LLMs can be guided by explanation and stylis-
tic information. In our de-stigmatization efforts,
we intentionally avoided providing the LLMs with
a rigid definition of “de-stigmatized.” Instead, we
adopted a more nuanced approach, drawing inspira-
tion from the principles of “Constitutional AI” (Bai
et al., 2022) and prior work on text detoxification
and bias reduction using LLMs (Dale et al., 2021a;
Mendelsohn et al., 2021; Pryzant et al., 2020). We
focused on explaining why specific phrases might
be problematic and instructed the model to address
these issues, constitutionally, while preserving the
original style. For instance, to tackle separation, the
LLMs were guided to draw equivalences between
individuals with SUD and those without, emphasiz-
ing shared humanity. Labeling was addressed by re-
placing derogatory terms like “junkie” with person-
centered language like “person with a substance
use disorder,” mitigating the over-generalization
tendencies of LLMs. Stereotyping and discrimi-
nation were handled by re-framing generalizations
and removing any implications of discrimination,
promoting a more empathetic understanding of in-
dividuals struggling with SUD.
Most de-stigmatized does not mean most prag-
matic. While the baseline model removes stig-
matizing language, it often does so at the expense
of nuance and context. For instance, evaluatorsnoted that the baseline model sometimes “terribly
misunderstood the post,” resulting in generic or in-
sincere responses that failed to capture the original
poster’s intent. This highlights the importance of
removing stigma and preserving the authenticity
and emotional tone of the original message. Our
findings emphasize the importance of striking a bal-
ance between promoting empathetic language and
providing overly refined language, which might
trivialize the experiences of individuals with SUD
or avoid addressing the root causes of stigma.
7 Conclusion
This study investigated the manifestations of
stigma towards PWUS in four popular non-
drug-related subreddits ( r/unpopularopinion ,
r/offmychest ,r/nursing ,r/medicine ). We identified
3,207 posts containing one of three main types of
stigma (self, structural, and directed). Given the
contextual nuance of self and structural stigma, we
focused our efforts on de-stigmatizing instances
of directed stigma (N = 1,649). Experimenting
with three different models and two different
LLMs (GPT-4 and Llama), the model that used
the conceptualization of stigma (Link and Phelan,
2001), few-shot examples, and the original post’s
stylistic profile generated the most faithful and
appropriate destigmatized texts. Our exploration
of LLM-based de-stigmatization demonstrates the
potential of these models to transform harmful
language into more empathetic expressions
while emphasizing the importance of preserving
authenticity and the original poster’s voice. While
our focus has been on SUD stigma, the insights
and methodologies presented here have broader
implications for understanding and addressing
stigma related to other marginalized groups. Future
work could explore the role of misinformation
in perpetuating stigma and leverage external
knowledge bases (e.g. DrugBank) to develop more
informed and effective de-stigmatization strategies.
By integrating these approaches, we can create a
more supportive and inclusive online environment
for individuals affected by stigma, ultimately
promoting understanding, empathy, and recovery.8 Limitations
Our findings primarily apply to English-speaking
populations on one specific social media platform,
which may not be generalizable to other linguistic
or cultural contexts. We selected certain subred-
dits based on our assessment of relevance, which
may have limited the breadth of our data; exploring
additional subreddits could potentially provide a
more comprehensive view. The performance and
accuracy of the models we used, dependent on their
training data, may not capture all nuances of stig-
matizing language. Despite our ethical considera-
tions, the automated analysis of sensitive topics like
SUD carries risks of misinterpretation, necessitat-
ing ongoing research and continuous evaluation of
ethical challenges in using large language models.
9 Ethics Statement
We acknowledge the diversity of perspectives on
substance use and advocate for harm reduction
strategies. All data was publicly available at the
time of collection, and no direct interaction oc-
curred between researchers and users. Our research
was exempt from review by our institution’s Inter-
nal Review Board (IRB). We adhere to strict data
protection measures and have slightly altered any
quotes to preserve anonymity and post integrity.
Our goal is not to erase personal experiences but
to reframe them in less harmful ways, aligned with
the original sentiment. The discussions in this pa-
per should not be interpreted to suggest anyone’s
lived experience is more valid than another.
References
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosuite, Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemi Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav
Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom
Conerly, Tom Henighan, Tristan Hume, Samuel R.
Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
Amodei, Nicholas Joseph, Sam McCandlish, Tom
Brown, and Jared Kaplan. 2022. Constitutional ai:
Harmlessness from ai feedback. (arXiv:2212.08073).
ArXiv:2212.08073 [cs].Layla Bouzoubaa, Elham Aghakhani, Max Song, Quang
Trinh, and Shadi Rezapour. 2024. Decoding the nar-
ratives: Analyzing personal drug experiences shared
on Reddit. In Findings of the Association for Com-
putational Linguistics ACL 2024 , pages 6131–6148,
Bangkok, Thailand and virtual meeting. Association
for Computational Linguistics.
Layla Bouzoubaa, Jordyn Young, and Rezvaneh Reza-
pour. 2023. Exploring the landscape of drug commu-
nities on reddit: A network study. In Proceedings of
the International Conference on Advances in Social
Networks Analysis and Mining , pages 558–565.
Ryan L Boyd, Ashwini Ashokkumar, Sarah Seraj, and
James W Pennebaker. 2022. The development and
psychometric properties of liwc-22. Austin, TX: Uni-
versity of Texas at Austin , pages 1–47.
Centers for Disease Control and Prevention.
2023. Reducing stigma to prevent opioid over-
dose. https://www.cdc.gov/stop-overdose/
stigma-reduction/index.html . Accessed:
2024-06-15.
Annie T. Chen, Shana Johnny, and Mike Conway. 2022.
Examining stigma relating to substance use and con-
textual factors in social media discussions. Drug and
Alcohol Dependence Reports , 3:100061.
Olivia Clark, Matthew M Lee, Muksha Luxmi Jingree,
Erin O’Dwyer, Yiyang Yue, Abrania Marrero, Martha
Tamez, Shilpa N Bhupathiraju, and Josiemer Mattei.
2021. Weight stigma and social media: evidence
and public health solutions. Frontiers in nutrition ,
8:739056.
Patrick W Corrigan, Amy C Watson, and Frederick E
Miller. 2006. Blame, shame, and contamination: the
impact of mental illness and drug dependence stigma
on family members. Journal of family psychology ,
20(2):239.
David Dale, Igor Markov, Varvara Logacheva, Olga Ko-
zlova, Nikita Semenov, and Alexander Panchenko.
2021a. SkoltechNLP at SemEval-2021 task 5: Lever-
aging sentence-level pre-training for toxic span de-
tection. In Proceedings of the 15th International
Workshop on Semantic Evaluation (SemEval-2021) ,
pages 927–934, Online. Association for Computa-
tional Linguistics.
David Dale, Anton V oronov, Daryna Dementieva, Var-
vara Logacheva, Olga Kozlova, Nikita Semenov, and
Alexander Panchenko. 2021b. Text detoxification us-
ing large pre-trained neural models. In Proceedings
of the 2021 Conference on Empirical Methods in Nat-
ural Language Processing , pages 7979–7996, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo
Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi.
2020. GoEmotions: A Dataset of Fine-Grained Emo-
tions. ArXiv:2005.00547 [cs].E.L. Eschliman, K. Choe, A. DeLucia, E. Addison, V .W.
Jackson, S.M. Murray, D. German, B.L. Genberg,
and M.R. Kaufman. 2024. First-hand accounts of
structural stigma toward people who use opioids on
Reddit. Social Science and Medicine , 347.
Salvatore Giorgi, Douglas Bellew, Daniel Roy Sadek
Habib, Garrick Sherman, João Sedoc, Chase Smit-
terberg, Amanda Devoto, McKenzie Himelein-
Wachowiak, and Brenda Curtis. 2023. Lived experi-
ence matters: automatic detection of stigma on social
media toward people who use substances. arXiv
preprint arXiv:2302.02064 .
Erving Goffman. 2009. Stigma: Notes on the manage-
ment of spoiled identity . Simon and schuster.
Maarten Grootendorst. 2020. Keybert: Minimal key-
word extraction with bert.
Maarten Grootendorst. 2022. Bertopic: Neural topic
modeling with a class-based tf-idf procedure. arXiv
preprint arXiv:2203.05794 .
Keyan Guo, Alexander Hu, Jaden Mu, Ziheng Shi, Zim-
ing Zhao, Nishant Vishwamitra, and Hongxin Hu.
2023a. An investigation of large language models
for real-world hate speech detection. In 2023 In-
ternational Conference on Machine Learning and
Applications (ICMLA) , pages 1568–1573.
Keyan Guo, Alexander Hu, Jaden Mu, Ziheng Shi, Zim-
ing Zhao, Nishant Vishwamitra, and Hongxin Hu.
2023b. An investigation of large language models
for real-world hate speech detection. In 2023 In-
ternational Conference on Machine Learning and
Applications (ICMLA) , pages 1568–1573.
Zhijun Guo, Alvina Lai, Johan Hilge Thygesen, Joseph
Farrington, Thomas Keen, and Kezhi Li. 2024. Large
language model for mental health: A systematic re-
view. arXiv preprint arXiv:2403.15401 .
Mark L. Hatzenbuehler. 2016. Structural stigma: Re-
search evidence and implications for psychologi-
cal science. American Psychologist , 71(8):742–751.
Place: US Publisher: American Psychological Asso-
ciation.
Matthew Honnibal and Ines Montani. 2020. spacy:
Industrial-strength natural language processing in
python.
David M Howcroft, Anya Belz, Miruna Clinciu, Dimitra
Gkatzia, Sadid A Hasan, Saad Mahamood, Simon
Mille, Emiel Van Miltenburg, Sashank Santhanam,
and Verena Rieser. 2020. Twenty years of confusion
in human evaluation: Nlg needs evaluation sheets
and standardised definitions. In 13th International
Conference on Natural Language Generation 2020 ,
pages 169–182. Association for Computational Lin-
guistics.
John F. Kelly, Sarah J. Dow, and Cara Westerhoff. 2010.
Does our choice of substance-related terms influenceperceptions of treatment need? an empirical investi-
gation with two commonly used terms. Journal of
Drug Issues , 40(4):805–818.
Tharindu Kumarage, Amrita Bhattacharjee, and Joshua
Garland. 2024. Harnessing artificial intelligence to
combat online hate: Exploring the challenges and
opportunities of large language models in hate speech
detection. arXiv preprint arXiv:2403.08035 .
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Bruce G. Link and Jo C. Phelan. 2001. Con-
ceptualizing Stigma. Annual Review
of Sociology , 27(1):363–385. _eprint:
https://doi.org/10.1146/annurev.soc.27.1.363.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. (arXiv:1907.11692). ArXiv:1907.11692
[cs].
Jason B. Luoma, Barbara S. Kohlenberg, Steven C.
Hayes, and Lindsay Fletcher. 2012. Slow and steady
wins the race: A randomized clinical trial of accep-
tance and commitment therapy targeting shame in
substance use disorders. Journal of Consulting and
Clinical Psychology , 80(1):43–53. Publisher: Ameri-
can Psychological Association.
Atsushi Matsumoto, Claudia Santelices, and Alisa K
Lincoln. 2021. Perceived stigma, discrimination and
mental health among women in publicly funded sub-
stance abuse treatment. Stigma and Health , 6(2):151.
Philip M McCarthy and Scott Jarvis. 2010. Mtld, vocd-
d, and hd-d: A validation study of sophisticated ap-
proaches to lexical diversity assessment. Behavior
research methods , 42(2):381–392.
Emma McGinty, Bernice Pescosolido, Alene Kennedy-
Hendricks, and Colleen L. Barry. 2018. Communica-
tion strategies to counter stigma and improve mental
illness and substance use disorder policy. Psychiatric
Services , 69(2):136–146.
Nilay McLaren, Christopher M. Jones, Rita Noo-
nan, Nimi Idaikkadar, and Steven A. Sumner. 2023.
Trends in stigmatizing language about addiction: A
longitudinal analysis of multiple public communi-
cation channels. Drug and Alcohol Dependence ,
245:109807.
Sandra R McNeil. 2021. Understanding substance use
stigma. Journal of Social Work Practice in the Ad-
dictions , 21(1):83–96.
Katelyn Mei, Sonia Fereidooni, and Aylin Caliskan.
2023. Bias against 93 stigmatized groups in maskedlanguage models and downstream sentiment classifi-
cation tasks. In Proceedings of the 2023 ACM Confer-
ence on Fairness, Accountability, and Transparency ,
FAccT ’23, page 1699–1710, New York, NY , USA.
Association for Computing Machinery.
Julia Mendelsohn, Ceren Budak, and David Jurgens.
2021. Modeling framing in immigration discourse on
social media. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 2219–2263, Online. Association
for Computational Linguistics.
Julia Mendelsohn, Yulia Tsvetkov, and Dan Jurafsky.
2020. A framework for the computational linguistic
analysis of dehumanization. Frontiers in artificial
intelligence , 3:55.
NIDA. 2023. Words matter - terms to use
and avoid when talking about addiction.
https://nida.nih.gov/research-topics/addiction-
science/words-matter-preferred-language-talking-
about-addiction. Accessed: 2024.
Kathryn E Nippert, A Janet Tomiyama, Stephanie M
Smieszek, and Angela C Incollingo Rodriguez. 2021.
The media as a source of weight stigma for pregnant
and postpartum women. Obesity , 29(1):226–232.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Reid Pryzant, Richard Diehl Martinez, Nathan Dass,
Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020.
Automatically neutralizing subjective bias in text. In
Proceedings of the aaai conference on artificial intel-
ligence , volume 34, pages 480–489.
Md Saidur Rahaman, MM Ahsan, Nishath Anjum,
Md Mizanur Rahman, and Md Nafizur Rahman. 2023.
The ai race is on! google’s bard and openai’s chat-
gpt head to head: an opinion article. Mizanur and
Rahman, Md Nafizur, The AI Race is on .
Patrick Robinson, Daniel Turk, Sagar Jilka, and Mat-
teo Cella. 2019. Measuring attitudes towards mental
health using social media: investigating stigma and
trivialisation. Social psychiatry and psychiatric epi-
demiology , 54:51–58.
Ananya B Sai, Akash Kumar Mohankumar, and
Mitesh M Khapra. 2022. A survey of evaluation met-
rics used for nlg systems. ACM Computing Surveys
(CSUR) , 55(2):1–39.
Ipek Baris Schlicht, Defne Altiok, Maryanne Taouk, and
Lucie Flek. 2024. Pitfalls of conversational llms on
news debiasing. arXiv preprint arXiv:2404.06488 .Georg Schomerus, Michael Lucht, Anita Holzinger, Her-
bert Matschinger, Mauro G Carta, and Matthias C
Angermeyer. 2011. The stigma of alcohol depen-
dence compared with other mental disorders: a re-
view of population studies. Alcohol and alcoholism ,
46(2):105–112.
S. Scott Graham, F.N. Conway, R. Bottner, and
K. Claborn. 2022. Opioid use stigmatization and
destigmatization in health professional social media.
Addiction Research and Theory .
Angelica Spata, Ishita Gupta, M Kati Lear, Karsten
Lunze, and Jason B Luoma. 2024. Substance use
stigma: A systematic review of measures and their
psychometric properties. Drug and Alcohol Depen-
dence Reports , page 100237.
Substance Abuse and Mental Health Services Admin-
istration. 2023. Samhsa announces nsduh results
detailing mental illness and substance use levels in
2021. https://www.samhsa.gov/newsroom/press-
announcements/20230104/samhsa-announces-
nsduh-results-detailing-mental-illness-substance-
use-levels-2021.
Benjamin Lee Whorf. 1956. Language, Thought, and
Reality: Selected Writings of Benjamin Lee Whorf .
MIT Press.
A Comparison of LLMs for Labeling
Drug Mention
We examined various LLMs (combination of open-
source and proprietary) to differentiate between
drug-related and non-drug-related posts on Reddit,
using a dataset of 200 manually annotated posts.
To assess the performance of each model, we cal-
culated the F-1 score, which is a measure of a test’s
accuracy that considers both precision and recall.
Additionally, we analyzed the total time and cost
required to process this amount of posts. These
findings are detailed in the table provided in Ta-
ble 5. This table helps to illustrate not only the
effectiveness of each model in terms of accuracy
but also their efficiency and economic viability for
similar tasks.
B Prompts
In our study, we implemented a multi-step pipeline
using different prompts for each stage, which in-
cludes data filtering, stigma detection with explana-
tions, and destigmatization. The prompts tailored
for data filtering, stigma detection, and destigma-
tization are detailed in Figures 2, 3 and 4. This
structured approach ensures efficient handling and
analysis of stigmatizing content in social media
posts.# # # P O S T :  L e f t  a  b e s t  f r i e n d  h a n g i n g .  L e t  o n e  g u y  a t  t h e  b e a c h  t a k e  a  f a l l  f o r  a l l  t h e  w e e d  a n d  a l c o h o l .  I  p u t  i n  t o  s m o k e  
a n d  a l l  1 5 - 1 8  o f  u s  j u s t  l e t  h i m  t a k e  a l l  o f  t h e  b l a m e .  
# # # L A B E L :O b j e c t i v e :  I d e n t i f y  r e f e r e n c e s  t o  d r u g s  o r  p e o p l e  w h o  u s e  d r u g s  i n  e a c h  p o s t .
I n c l u d e 
I l l i c i t  D r u g s :  A l l  c o n t r o l l e d  s u b s t a n c e s  w i t h  n o  l e g a l  u s a g e  s u c h  a s  c a n n a b i s ,  h e r o i n ,  c o c a i n e ,  m e t h a m p h e t a m i n 
P r e s c r i p t i o n  D r u g s :  D r u g s  t h a t  a r e  o f t e n  a b u s e d  e v e n  i f  t h e y  h a v e  l e g i t i m a t e  m e d i c a l  u s e s  s u c h  a s  o p i o i d s ,  
b e n z o d i a z e p i n e 
O t h e r  D r u g s :  N o n - p r e s c r i p t i o n  d r u g s  k n o w n  t o  b e  a b u s e d  s u c h  a s  i n h a l a n t s ,  k 2 ,  b a t h  s a l t 
E x p l i c i t  m e n t i o n s  o f  d r u g  u s e ,  a b u s e ,  o r  a d d i c t i o n  r e l a t e d  t e r m s  s u c h  a s  " g e t t i n g  h i g h " ,  " s t o n e d "
E x c l u d e 
T o b a c c o ,  n i c o t i n e ,  o r  a l c o h o l  u n l e s s  e x p l i c i t l y  l i n k e d  t o  d r u g  u s 
D o  n o t  i n c l u d e  m e d i c a l  o r  p s y c h o l o g i c a l  d i s c u s s i o n s  u n l e s s  t h e r e  i s  a  d i r e c t  a n d  c l e a r  m e n t i o n  o f  d r u g  u s e  o r  a b u s e
C l a r i f i c a t i o n s 
M e n t a l  h e a l t h  d i s c u s s i o n s  s h o u l d  n o t  b e  l a b e l e d  a s  ' D '  u n l e s s  t h e r e  i s  a n  e x p l i c i t  m e n t i o n  o f  d r u g s  a s  d e f i n e d  a b o v 
U s e  ' N D '  f o r  p o s t s  t h a t  d i s c u s s  h e a l t h  o r  p s y c h o l o g i c a l  i s s u e s  w i t h o u t  s p e c i f i c  d r u g  r e f e r e n c e s
L a n g u a g e  C u e s 
F o c u s  o n  c l e a r  d r u g - r e l a t e d  t e r m i n o l o g y  s u c h  a s  " j u n k i e " ,  " a d d i c t "  a n d  s l a n 
I f  a  p o s t  i s  a m b i g u o u s  a n d  d o e s  n o t  c l e a r l y  f i t  t h e  d r u g  r e f e r e n c e  c r i t e r i a ,  l a b e l  a s  ' N D '
R e s p o n s e  R e q u i r e m e n t 
R e s p o n d  w i t h  e i t h e r  ' D '  ( D r u g )  o r  ' N D '  ( N o n - D r u g )  b a s e d  o n  t h e s e  g u i d e l i n e s .  N o  a d d i t i o n a l  c o m m e n t a r y  i s  n e e d e d .
# # # P O S T :  " I  h o p e  m y  j u n k i e  s i s t e r  O D ' s  o r  d i s a p p e a r s  o u t  o f  o u r  l i v e s  M y  s i s t e r  i s  a n  a l c o h o l i c  j u n k i e  w h o  h a s  2  D U I s  u n d e r  
h e r  b e l t  a s  w e l l  a s  l o v e s  t a k i n g  X a n a x  a n d  a l c o h o l  t o g e t h e r  a n d  w r e a k i n g  h a v o c  f o r  o u r  f a m i l y  a n d  e v e n  s t r a n g e r s . "
# # # L A B E L :  D
# # # P O S T :  " M y  d r i n k i n g  p r o b l e m  i s  w a y  w o r s e  t h a n  a n y o n e  k n o w s .  I ' m  a n  a l c o h o l i c  a n d  t h e  p e o p l e  a r o u n d  m e  k n o w  t h a t ,  b e c a u s e  
I  t o l d  t h e m .  B u t  i f  I  h a d n ' t ,  t h e y  w o u l d n ' t  h a v e  g u e s s e d ,  j u d g i n g  b y  t h e i r  r e a c t i o n s  o f  s h o c k  a n d  d i s b e l i e f  a n d  t h e  a c t u a l  
‚ a r e  y o u  s u r e  y o u ' r e  n o t  e x a g g e r a t i n g ,  t h a t  I  g o t  w h e n  I  c a m e  c l e a n  a  c o u p l e  y e a r s  a g o . "
# # # L A B E L :  N D
# # # P O S T :  “ I  c u t  t o n i g h t  I  c u t  m y s e l f  t o n i g h t  a f t e r  I ' v e  b e e n  c l e a n  f o r  a  f e w  m o n t h s  n o w  a n d  I ' m  h o n e s t l y  j u s t  b r o k e n  
i n s i d e .  I  h a d  a  h u g e  f i g h t  w i t h  m y  g i r l f r i e n d ,  I ' m  w o r r i e d  w e  m i g h t  b r e a k  u p .  I  j u s t  k i n d a  l o s t  c o n t r o l ,  g r a b b e d  m y  
r a z o r b l a d e  a n d  c u t .  I  f e e l  l i k e  c r a p  a n d  a m  h o n e s t l y  j u s t  l o s t  r i g h t  n o w . "
# # # L A B E L :  N D
# # # P O S T :  " H a t e  m y  n e w  j o b  I  j u s t  g o t  h i r e d  a t  T a c o  B e l l  a  f e w  w e e k s  a g o  a n d  I  r e a l l y  h a t e  i t .  T h e  o n l y  t h i n g  t h a t ' s  g e t s  m e  
t h r o u g h  i t  i s  s m o k i n g  w e e d .  I  w o n d e r  i f  i  s h o u l d  j u s t  q u i t  o r  n o  c a l l  n o  s h o w ? "
# # # L A B E L :  D
. . .
T a r g e tI n s t r u c t i o n
F e w - S h o t  E x a m p l e sFigure 2: Few-shot prompting to determine whether a post contains a reference to illicit substance use.
Model F1 Total Time Cost (USD) RPM
GPT 3.5-Turbo 0.78 9.52 s 0.07 3,500*
GPT 4-Turbo 0.9 19.05 s 1.31 500*
Mistral 0.48 330.60 s 0 300**
Llama3-8B 0.38 59.9 s 0 600***
Table 5: Comparison on four LLMs considered to label
1.51M posts for the mention of drugs or drug use based
on a random sample of 200 manually-annotated posts.
‘*’ based on OpenAI Tier 3 usage (see
https://platform.openai.com/docs/guides/
rate-limits/usage-tiers?context=tier-three )
‘**’ based on Hugging Face Inference API rate limit
per hour
‘***’ based on Together.ai API rate per second
for Paid Tier ( https://docs.together.ai/docs/
rate-limits ).
C Data Analysis
In our study, we conducted a comprehensive lin-
guistic analysis of online posts about drug use and
addiction-related stigmas. We extracted and an-
alyzed representative entities, subject-verb pairs,
and utilized topic modeling to identify themes in di-
rect and self-stigmatizing posts. These topics were
organized by names, representative keywords, dom-inant emotions, and frequent verb-subject pairs,
presented in Tables 6 and 7. For linguistic pro-
cessing, we used spaCy for subject-verb extraction,
GoEmotions and RoBERTa for emotion classifica-
tion, and BERTopic and KeyBERT for topic mod-
eling. This multi-dimensional approach provided
a detailed understanding of the discourse within
these posts.
D Human Evaluation
We provided the following instructions to guide the
evaluation of de-stigmatized texts. These guide-
lines were shared with our reviewers together with
the generated texts from the six systems and forms
for ranking the models. Each reviewer judged 20
to 30 posts independently.
Instructions: Please read the original post and
the generated posts from each of the six systems
carefully. For each of the following questions, se-
lect the system that best represents your evaluation.
Use the space provided for any additional com-
ments you may have.# # # P O S T :  L e f t  a  b e s t  f r i e n d  h a n g i n g .  L e t  o n e  g u y  a t  t h e  b e a c h  t a k e  a  f a l l  f o r  a l l  t h e  w e e d  a n d  a l c o h o l .  I  p u t  i n  t o  s m o k e  
a n d  a l l  1 5 - 1 8  o f  u s  j u s t  l e t  h i m  t a k e  a l l  o f  t h e  b l a m e .  
# # # L A B E L :I n s t r u c t i o n s :  Y o u  a r e  a n  e x p e r t  i n  i d e n t i f y i n g  s t i g m a  r e l a t e d  t o  d r u g  u s e  a n d  a d d i c t i o n  i n  s o c i a l  m e d i a  p o s t s .  Y o u r  t a s k  i s  
t o  a n a l y z e  e a c h  p o s t  a n d  d e t e r m i n e  i f  i t  c o n t a i n s  s t i g m a t i z i n g  l a n g u a g e .
L a b e l 
I f  t h e  p o s t  c o n t a i n s  s t i g m a t i z i n g  l a n g u a g e ,  l a b e l  i t  a s  " S "  ( S t i g m a ) 
I f  t h e  p o s t  d o e s  n o t  c o n t a i n  s t i g m a t i z i n g  l a n g u a g e ,  l a b e l  i t  a s  " N S "  ( N o n - S t i g m a ) .
E x p l a n a t i o n 
F o r  " S "  ( S t i g m a )  l a b e l s 
I f  t h e  p o s t  d e m o n s t r a t e s  s t i g m a t i z i n g  l a n g u a g e  ( d i r e c t e d  a t  o t h e r s ) ,  a n a l y z e  i t  b a s e d  o n  t h e  f o l l o w i n g  f o u r  a t t r i b u t e s  
a n d  p r o v i d e  a n  e x p l a n a t i o n  f o r  e a c h  a t t r i b u t e 
L a b e l i n g :  D o e s  t h e  p o s t  u s e  d e r o g a t o r y  o r  o t h e r i n g  l a n g u a g e  r e l a t e d  t o  d r u g  u s e / a d d i c t i o n 
S t e r e o t y p i n g :  D o e s  t h e  p o s t  r e i n f o r c e  n e g a t i v e  g e n e r a l i z a t i o n s  a b o u t  p e o p l e  w h o  u s e  d r u g s 
S e p a r a t i o n :  D o e s  t h e  p o s t  c r e a t e  a  d i v i d e  b e t w e e n  p e o p l e  w h o  u s e  d r u g s  a n d  t h o s e  w h o  d o n ' t 
D i s c r i m i n a t i o n :  D o e s  t h e  p o s t  i m p l y  o r  s u g g e s t  u n f a i r  t r e a t m e n t  b a s e d  o n  d r u g  u s e 
I f  t h e  p o s t  d e m o n s t r a t e s  s e l f - s t i g m a  o r  s t r u c t u r a l  s t i g m a ,  p r o v i d e  a  b r i e f ,  o n e - s e n t e n c e  e x p l a n a t i o n  o f  t h e  t y p e  o f  
s t i g m a  p r e s e n t 
F o r  " N S "  ( N o n - S t i g m a )  l a b e l s :  N o  e x p l a n a t i o n  i s  n e e d e d .
# # # P O S T :  " O n c e  a n  a d d i c t ,  a l w a y s  a n  a d d i c t .  T h e y ' l l  n e v e r  c h a n g e . "
# # # L A B E L :  " S ,  L a b e l i n g :  ' a d d i c t ' ,  S t e r e o t y p i n g :  A s s u m e s  t h a t  p e o p l e  w i t h  a d d i c t i o n  a r e  i n c a p a b l e  o f  r e c o v e r y  o r  p e r s o n a l  
g r o w t h ,  S e p a r a t i o n :  C r e a t e s  a  s e n s e  o f  h o p e l e s s n e s s  a n d  p e r p e t u a t e s  t h e  i d e a  t h a t  p e o p l e  w i t h  a d d i c t i o n  a r e  f u n d a m e n t a l l y  
d i f f e r e n t ,  D i s c r i m i n a t i o n :  D i s c o u r a g e s  o f f e r i n g  s u p p o r t  o r  o p p o r t u n i t i e s  t o  i n d i v i d u a l s  i n  r e c o v e r y  d u e  t o  a  b e l i e f  t h e y ' l l  
r e l a p s e . "
# # # P O S T :  " H e ' s  j u s t  a  j u n k i e  l o o k i n g  f o r  h i s  n e x t  f i x . "
# # # L A B E L :  “ S ,  L a b e l i n g :  ' j u n k i e ' ,  a  d e r o g a t o r y  t e r m ,  S t e r e o t y p i n g :  R e d u c e s  t h e  p e r s o n ' s  i d e n t i t y  t o  t h e i r  s u b s t a n c e  u s e ,  
i g n o r i n g  o t h e r  a s p e c t s  o f  t h e i r  l i f e ,  S e p a r a t i o n :  C r e a t e s  a  d i s t a n c e  b e t w e e n  ' u s '  ( n o n - u s e r s )  a n d  ' t h e m '  ( p e o p l e  w h o  u s e  
s u b s t a n c e s ) ,  D i s c r i m i n a t i o n :  I m p l i e s  t h e  p e r s o n  i s  s o l e l y  m o t i v a t e d  b y  d r u g  u s e  a n d  l a c k s  o t h e r  v a l u e s  o r  g o a l s . "
# # # P O S T :  " S h e ' s  c o n s t a n t l y  n o d d i n g  o f f  a t  w o r k .  P r o b a b l y  u s i n g  a g a i n . "
# # # L A B E L :  " S ,  L a b e l i n g :  I m p l i e s  t h e  p e r s o n  h a s  a  s u b s t a n c e  u s e  d i s o r d e r  b y  s u g g e s t i n g  t h e y  a r e  ' u s i n g  a g a i n , '  S t e r e o t y p i n g :  
G e n e r a l i z e s  t h a t  t h e  p e r s o n ' s  b e h a v i o r  ( n o d d i n g  o f f )  i s  s o l e l y  d u e  t o  d r u g  u s e ,  S e p a r a t i o n :  C r e a t e s  a  d i s t i n c t i o n  b e t w e e n  
t h e  p e r s o n  a n d  t h e i r  c o l l e a g u e s ,  i m p l y i n g  t h e y  a r e  u n r e l i a b l e  o r  i r r e s p o n s i b l e  d u e  t o  a s s u m e d  s u b s t a n c e  u s e ,  D i s c r i m i n a t i o n :  
C o u l d  l e a d  t o  u n f a i r  t r e a t m e n t  a t  w o r k  b a s e d  o n  a n  a s s u m p t i o n  o f  r e l a p s e . "
# # # P O S T :  " I ' m  c o n c e r n e d  a b o u t  m y  b r o t h e r ' s  s u b s t a n c e  u s e .  I ' m  l e a r n i n g  a b o u t  a v a i l a b l e  r e s o u r c e s  t o  h e l p  h i m . "
# # # L A B E L :  N S
. . .
D e f i n i t i o n s 
S t i g m a :  N e g a t i v e  a t t i t u d e s  o r  b e l i e f s  d i r e c t e d  a t  i n d i v i d u a l s  b a s e d  o n  t h e i r  d r u g  u s e 
S e l f - S t i g m a :  I n t e r n a l i z a t i o n  o f  n e g a t i v e  s o c i e t a l  a t t i t u d e s  a b o u t  d r u g  u s e 
S t r u c t u r a l  S t i g m a :  S o c i e t a l  s y s t e m s ,  p o l i c i e s ,  o r  p r a c t i c e s  t h a t  d i s a d v a n t a g e  p e o p l e  w h o  u s e  d r u g s .
A d d i t i o n a l  C o n s i d e r a t i o n s 
C o n s i d e r  t h e  c o n t e x t  o f  t h e  p o s t  a n d  t h e  a u t h o r ' s  i n t e n t 
F o c u s  o n  l a n g u a g e  t h a t  i s  h a r m f u l ,  d i s c r i m i n a t o r y ,  o r  p e r p e t u a t e s  n e g a t i v e  s t e r e o t y p e s .
T a s k :  R e a d  t h e  f o l l o w i n g  p o s t ,  l a b e l  i t  a s  " S "  o r  " N S " ,  a n d  p r o v i d e  t h e  a p p r o p r i a t e  e x p l a n a t i o n :
T a r g e tI n s t r u c t i o n
F e w - S h o t  E x a m p l e sD e f i n i t i o nFigure 3: Few-shot prompting to determine whether a post contains stigmatizing language towards PWUS.
Overall Quality:
Assess the overall quality of the generated text with
respect to the following measures in addition to de-
stigmatization and faithfulness:
•Naturalness: The degree to which the output
is likely to be used/chosen by a native speaker
in the given context/situation.
•Cohesion: The text should be a well-organized
and coherent body of information, not just a
dump of related information. Specifically, the
sentences should be connected to one another,
maintaining good information flow and logic.
•Appropriateness: The degree to which the
output as a whole is appropriate in the given
context/situation. E.g., “does the text appropri-
ately consider the parents’ emotional state in
the given scenario?”•Human-Likeness: The degree to which the
output could have been produced by a human.
Which system generated the text that with the best
overall quality (content, form, de-stigma)?
De-stigmatization:
Which system’s generated post is the most de-
stigmatized?
Effectively De-stigmatized:
Which system’s post has been the most effec-
tively de-stigmatized, removing negative or harm-
ful stereotypes? Remember stigma is defined as
the co-occurring processes of labeling, stereotyp-
ing, separation, status loss, and discrimination. In
the context of SUD, it can look something like this:
Labeling: involves identifying individuals as dif-
ferent based on certain characteristics—in this
case, their substance use. For those with SUD,{
    " r o l e " :  " u s e r " ,
    " c o n t e n t " :  p o s t  +  " ; "  +  e x  +  " ; "  +  s t y l e _ i n s t r u c t
}i f  s t e p  L a b e l : 
    i n s t r u c t i o n  =  " R e w r i t e  t h i s  p o s t  t o  r e m o v e  a n y  a n d  a l l  l a b e l i n g . " 
    d e f i n i t i o n  =  " L a b e l i n g  i n c l u d e s  t h e  u s e  o f  d e r o g a t o r y  o r  o t h e r i n g  l a n g u a g e  r e l a t e d  t o  d r u g  u s e / a d d i c t i o n . " 
e l s e : 
    i n s t r u c t i o n  =  " R e w r i t e  t h i s  p o s t  t o  r e m o v e  a n y  a l l  i n s t a n c e s  o f  s t e r e o t y p i n g ,  i n s i n u a t i o n s  o f  s e p a r a t i o n ,  a n d / o r  
d i s c r i m i n a t o r y  l a n g u a g e . " 
    d e f i n i t i o n  =  " S t e r e o t y p i n g  r e i n f o r c e s  n e g a t i v e  g e n e r a l i z a t i o n s  a b o u t  p e o p l e  w h o  u s e  d r u g s .  S e p a r a t i o n  c r e a t e s  a  d i v i d e  
b e t w e e n  p e o p l e  w h o  u s e  d r u g s  a n d  t h o s e  w h o  d o n ' t .  D i s c r i m i n a t i o n  i m p l i e s  o r  s u g g e s t s  u n f a i r  t r e a t m e n t  b a s e d  o n  d r u g  u s e . " 
         
p r o m p t  =  f " " " 
    { i n s t r u c t i o n } ;   
    { d e f i n i t i o n } ; 
    O n l y  r e w r i t e  t h e  r e l e v a n t  p a r t s  o f  t h e  p o s t ,  d o  n o t  r e w r i t e  t h e  w h o l e  p o s t .  D o  n o t  c h a n g e  t h e  m e a n i n g  o f  t h e  p o s t  o r  a d d  
a n y  n e w  i n f o r m a t i o n . 
    E x a m p l e : 
    p o s t :  " M y  m o m  i s  a n  a d d i c t " ;  T h i s  p o s t  u s e s  t h e  t e r m  ' a d d i c t ' 
    r e w r i t e :  " M y  m o m  h a s  a  s u b s t a n c e  u s e  d i s o r d e r " 
     
    D o  n o t  i n c l u d e  " H e r e  i s  t h e  r e w r i t t e n  p o s t : "  i n  y o u r  r e s p o n s e .  J u s t  r e t u r n  t h e  r e w r i t t e n  p o s t . 
    " " "i f  s t e p  L a b e l : 
    i n s t r u c t i o n  =  " R e w r i t e  t h i s  p o s t  t o  r e m o v e  a n y  a n d  a l l  l a b e l i n g . " 
    d e f i n i t i o n  =  " L a b e l i n g  i n c l u d e s  t h e  u s e  o f  d e r o g a t o r y  o r  o t h e r i n g  l a n g u a g e  r e l a t e d  t o  d r u g  u s e / a d d i c t i o n . " 
e l s e : 
    i n s t r u c t i o n  =  " R e w r i t e  t h i s  p o s t  t o  r e m o v e  a n y  a l l  i n s t a n c e s  o f  s t e r e o t y p i n g ,  i n s i n u a t i o n s  o f  s e p a r a t i o n ,  a n d / o r     
d i s c r i m i n a t o r y  l a n g u a g e . " 
    d e f i n i t i o n  =  " S t e r e o t y p i n g  r e i n f o r c e s  n e g a t i v e  g e n e r a l i z a t i o n s  a b o u t  p e o p l e  w h o  u s e  d r u g s .  S e p a r a t i o n  c r e a t e s  a  d i v i d e  
b e t w e e n  p e o p l e  w h o  u s e  d r u g s  a n d  t h o s e  w h o  d o n ' t .  D i s c r i m i n a t i o n  i m p l i e s  o r  s u g g e s t s  u n f a i r  t r e a t m e n t  b a s e d  o n  d r u g  u s e . " 
         
p r o m p t  =  f " " " 
    { i n s t r u c t i o n } ;   
    { d e f i n i t i o n } ; 
    O n l y  r e w r i t e  t h e  r e l e v a n t  p a r t s  o f  t h e  p o s t ,  d o  n o t  r e w r i t e  t h e  w h o l e  p o s t .  D o  n o t  c h a n g e  t h e  m e a n i n g  o f  t h e  p o s t  o r  a d d  
a n y  n e w  i n f o r m a t i o n . 
    E x a m p l e : 
    p o s t :  " M y  m o m  i s  a n  a d d i c t " ;  T h i s  p o s t  u s e s  t h e  t e r m  ' a d d i c t ' 
    r e w r i t e :  " M y  m o m  h a s  a  s u b s t a n c e  u s e  d i s o r d e r " 
     
    D o  n o t  i n c l u d e  " H e r e  i s  t h e  r e w r i t t e n  p o s t : "  i n  y o u r  r e s p o n s e .  J u s t  r e t u r n  t h e  r e w r i t t e n  p o s t . 
    " " "
P l u s  S t y l e  P r o f i l e :D e - s t i g m a  w i th  E x p l a n a t i o n  a n d  S t y l i z e dD e - s t i g m a  w i th  E x p l a n a t i o nFigure 4: Few-shot prompting for de-stigmatizing language towards PWUS, explanation and explanation plus style
profile.
labels such as “addict” or “alcoholic” can be
affixed.
Stereotyping: involves ascribing a fixed set of be-
liefs or characteristics to individuals based
solely on their disorder.
Separation/Status Loss: the social distancing of
a group perceived as different or undesirable.
This separation is partly due to the fear and
misunderstanding surrounding the disorder.
Discrimination: Discrimination can be both for-
mal and informal, impacting various aspects
of life, including employment and social inter-
actions.
Faithfulness:
Evaluate whether the posts generated by each sys-
tem contain all the required information from the
original post without unnecessary details. Whichsystem has the most faithful result?
General Feedback:
Please provide any general feedback or additional
comments regarding your evaluation of the texts.Name Representation Top Emotion Top Verb-Subject
PairsExample
Cannabis
Legalization
Stigmamarijuana, cannabis,
weed,drugs,
addicts, sober,
smoking, heroin,
pot, smokersneutral {‘it’, ‘is’}: 126,
{‘i’, ‘have’}: 117,
{‘i’, ‘know’}: 91Your addiction and dependence isn’t slighter than
mines and vice versa. Just because weed doesn’t have
physiological symptoms of wd it doesn’t mean it
doesn’t fuck up potheads who have to go without
smoking for, say, week. Mind your own business.
Interpersonal
Stigmarehab, sister,
family, dad,
grandmother,
parents, mother,
father, drugs, momsadness {‘i’, ‘know’}: 381,
{‘i’, ‘have’}: 260,
{‘i’, ‘want’}: 256I wish my sister would just go to prison and leave my
family alone. About 10 years ago my sister got into a
bad wreck. She was in a coma for a week and now has
traumatic brain injury.
Moral
Judgments on
Addictionhomelessness, homeless,
neighbor, neighbors,
neighbour, junkies,
neighborhood, drugs,
heroin, copsannoyance {‘i’, ‘see’}: 23,
{‘i’, ‘know’}: 21,
{‘i’, ‘hate’}: 19This is completely ignoring the fact that drugs are
the reason they are homeless in the first place. Some
of the other comments were saying that they do drugs
so why should they judge a homeless person doing
drugs. This kind of justification seems insane to me.
Just because you are ruining your life, doesn’t mean
that you should advocate for other people to ruin
their lives. And I don’t even want to get into the
hundreds of drug subreddits like r/heroin, r/meth, and
r/crack where people are posting about and bragging
about their dangerous drug addictions.
Moral
Judgements
and
Amphetamine
Useadderall,
amphetamine,
amphetamines, adhd,
stimulant,
prescriptions,
prescription, drugs,
medication,
prescribedneutral {‘i’, ‘have’}: 5,
{‘i’, ‘had’}: 4,
{‘i’, ‘hate’}: 4I live in a college town and adderall/vyvanse use is
insane. Some use it to study, some use it to party and
some use it to game for days. All these people
eventually can’t operate without the pills. It leads
to serious rage issues and mood swings. My roommate
spends around $300/month on someone else’s adderall.
Here are some facts- he will exhaust you with hours of
pointless stories and ramblings then get mad when you
don’t listen. He literally can’t shut the hell up.
Just like a tweaker.
Drug Use
Consequencesvicodin, smoked,
smoking, toxic,
camping, run,
thinking, scared,
needle, crystalneutral {‘i’, ‘wanted’}: 6,
{‘i’, ‘know’}: 5,
{‘it’, ‘feels’}: 5Shot of meth feels like you’ve finally crossed that
line you swore you’d never cross. You know the one–it
looked impossibly far away back when you were naive
enough to promise yourself you’d always stick to
smoking. When you truly believed you would never allow
yourself to become one of those needle freak losers.
Table 6: Summary of topics from direct stigmatizing posts. Interpersonal entities in blue, substances in green, and
actions in purple.Name Representation Top Emotion Top Verb-Subject
PairsExample
Sobriety &
Family
Strugglesdepressed ,
depression,
alcoholic, sober,
stay, addiction,
parents, drinking,
quit,mothersadness {‘i’, ‘have’}: 410,
{‘i’, ‘m’}: 360,
{‘i’, ‘want’}: 345I’ve been trying to come out of my isolation, they
don’t really care, and would rather keep my home and
safe. so they screamed at me because I stayed out with
my friends too late. I do not have that freedom
anymore. I felt like I wanted to stay with my friends
until I got comfortable. This was the first time I had
hung out with them in a month, and I wasn’t even
enjoying it. I was uncomfortable. I tried weed, got
even more uncomfortable. I can almost never turn down
drugs. I am such a pathetic fucking junky.
Prescription
Medicationadderall,
medications ,
prescription , adhd,
medication ,opiate,
prescribed , meds,
pharmacy, xanaxdisappointment {‘i’, ‘have’}: 78,
{‘i’, ‘feel’}: 74 ,
{‘i’, ‘know’}: 46I apologize if this doesn’t make sense. I’m not very
good at explaining things. I’m sure a lot of people
will just judge me for being a whiny addict and say
“well don’t do drugs and you wouldn’t even be in this
situation, duh”. I get it, most people think that all
junkies should be “thrown on an island to die” and the
world would be a much better place.
Overdose
Death &
Suicide
Ideationoverdosed ,xanax,
fentanyl ,dilaudid ,
acetaminophen ,
600mg, 30mg,
tramadol ,prozac,
clonazepamdesire {‘i’, ‘want’}: 21,
{‘i’, ‘m’}: 15,
{‘i’, ‘know’}: 9It didn’t work, I’m not dead. I looked up what would
happen if I took a shit ton of vyvanse and apparently
seizures and heart failure are likely. shitty death
but I needed to organize my stuff so it’s easier to
move or get rid of. so I took all the ones I had in
the bottle. I spent literally the last couple hours
writing suicide notes for nothing.
Struggles
with
Intimate
Partnersyoull, bye,
alcoholic, leave,
escaping ,whisper,
soul, leaving, pot,
lifennimsadness {‘i’, ‘want’}: 11,
{‘i’, ‘m’}: 10,
{‘i’, ‘m’}: 9}Just an out of the blue rant from a worthless
junkie.... don’t bother. oh god I miss you so much. we
know each other inside and out and have been through
it all. I never thought you’d take me back ever from
all the horrible shit I’ve done then to my surprise
you took my back a second time even tho I ran away for
months on end with no word or attempt of
communication, getting high and drunk 24/7 and
randomly showed up back home at 3 in the morning just
to leave two days later and repeat my actions. then
you moved a whole other province away to get back with
me just for me to turn back to drugs and lose my job
then you left for the final time.
Table 7: Summary of topics from self-stigmatizing posts. Interpersonal entities in blue, substances in green, and
actions in purple.