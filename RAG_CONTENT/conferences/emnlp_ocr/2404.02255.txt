LM2: A Simple Society of Language Models Solves Complex Reasoning
Gurusha Juneja*
Microsoft Research, India
gurushajuneja@gmail.comSubhabrata Dutta*
IIT Delhi, India
subha0009@gmail.comTanmoy Chakraborty
IIT Delhi, India
tanchak@ee.iitd.ac.in
Abstract
Despite demonstrating emergent reasoning abil-
ities, Large Language Models (LLMS) often
lose track of complex, multi-step reasoning. Ex-
isting studies show that providing guidance via
decomposing the original question into mul-
tiple subproblems elicits more robustness in
LLM reasoning – a decomposer generates the
subproblems, and a solver solves each of these
subproblems. However, these techniques fail
to accommodate coordination between the de-
composer and the solver modules (either in
a single model or different specialized ones)
– the decomposer does not keep track of the
ability of the solver to follow the decomposed
reasoning. In this paper, we propose LM2to
address these challenges. LM2modularizes
the decomposition, solution, and verification
into three different language models. The de-
composer module identifies the key concepts
necessary to solve the problem and generates
step-by-step subquestions according to the rea-
soning requirement. The solver model gener-
ates the solution to the subproblems that are
then checked by the verifier module; depend-
ing upon the feedback from the verifier, the
reasoning context is constructed using the sub-
problems and the solutions. These models are
trained to coordinate using policy learning. Ex-
haustive experimentation suggests the superi-
ority of LM2over existing methods on in- and
out-domain reasoning problems, outperforming
the best baselines by 8.1% on MATH, 7.71%
on JEEBench, and 9.7% on MedQA prob-
lems (code available at https://github.com/
LCS2-IIITD/Language_Model_Multiplex ).
1 Introduction
Recent trends in solving complex reasoning tasks
using Large Language Models (LLMs) typically
follow two different dominant approaches: (i)
well-curated prompting techniques (Zheng et al.,
2023; Yao et al., 2024) on LLMs of exorbitant
*Equal contributionsize like GPT-4 (OpenAI, 2023), or (ii) finetuning
a relatively smaller LLM using domain-focused
data (Shao et al., 2024; Toshniwal et al., 2024;
Dutta et al., 2024). Methods from the former cat-
egory heavily rely on the proprietary LLM being
used and are prone to fail absolutely when em-
ployed with less powerful models. The latter cat-
egory, though cost-effective compared to humon-
gous LLMs, often loses in generalizability due to a
narrow training domain.
The chronicle of decomposed reasoning. A
number of recent literature has pointed out that
LLMs tend to perform better on complex reason-
ing tasks when the problem is decomposed into
step-by-step subproblems (Zhou et al., 2023; Khat-
tab et al., 2022; Juneja et al., 2023). Earlier tech-
niques demonstrated the superiority by providing
the model with examples containing the original
problem decomposed into multiple sub-problems
along with their answers (Zhou et al., 2023). How-
ever, Juneja et al. (2023) illustrated that decoupling
the decomposer from the solver by finetuning a sep-
arate decomposer language model (LM) to coordi-
nate with a larger solver LM is beneficial to simply
prompting a single monolithic LM to decompose
and solve. Echoing their findings, Wu et al. (2024)
also found that distilling decomposition abilities
from a larger LM to a smaller LM is much more
generalizable compared to decomposing the solver
abilities directly.
Our contributions. However, a major bottle-
neck in existing methods of decomposer finetuning
is the lack of tightness between the decomposer-
solver interactions. Typically, the decomposition is
done in a memoryless manner, with or without the
solver’s initial response; no strategy is employed
to track whether the solver can follow the decom-
posed chain of reasoning. Towards this very end,
we propose a novel multi-LLM coordination frame-
work, Language Model Multiplex ( LM2).LM2is
built upon three separate LMs, each dedicated to
1arXiv:2404.02255v1  [cs.CL]  2 Apr 2024 : How many distinct, non-equilateral triangles with a perimeter of
60 units have integer side lengths , , and  such that , ,  is an
arithmetic sequence?Solver LM
Decomposer LM
Verifier LM
SQ: What is a, b, c in terms of common dif ference d?
SA:Since a, b, and c form an arithmetic sequence, we can
express c in terms of a as c = a + d, where d is the common
difference
Let  be the common dif ference, so  and 
We can assume that  is positive
In particular ,  can't be 0, because the triangle is not equilateral
Then the perimeter of the triangle is
, so 
Hence, the sides of the triangle are , 20, and 
These sides must satisfy the triangle inequality , which gives us
 Solving for , we find , or 
  Therefore, the possible values of  are 1, 2, , 9, which gives us
 possible triangles1. We know that for a triangle with side lengths a, b, and c, the sum of
the lengths of any two sides must be greater than the length of the
third side.
2. Since we are given that a, b, and c form an arithmetic sequence,
we can express b as the average of a and c. This gives us .
...
6. Since a, b, and c form an arithmetic sequence, we can express c in
terms of a as c = a + d, where d is the common dif ference.
...
10. This gives us the side lengths of the triangle as a = 10, b = 15,
and c = 30. Hence final answer is Concepts: 
1. Triangle Inequality
2. Arithmetic progression
Related Equations
1. Let x,y ,z be sides of triangle, then x+y>z, x+z>y , z+y>x 
2. If x,y ,z are in arithmetic progression then y-x=z-y
SQ:What is the value of a + c?
SA:Since we are given that a, b, and c form an arithmetic
sequence, we can express b as the average of a and c. This
gives us  . The perimeter of the triangle is given
by P = .
We are given that the perimeter is 60 units,
so we have  .Concepts: 
Solving for a + c,
we get a + c = 40
SQ: What is triangle inequality in terms of a,b,c?
SA: Triangle inequality sayd Figure 1: The inference procedure of LM2on a question from the MATH dataset. A question (in blue) is provided to
the Solver LM that produces an incorrect answer (in red). The question is then provided to the Decomposer LM that
generates the concepts and step-by-step subquestions (in lilac). Each subquestion is answered by the Solver LM, and
the sub-answer is verified by a Verifier LM. If the Verifier LM approves the sub-answer, that subqustion-subanswer
pair is added to the context of reasoning steps; otherwise, a new subquestion is generated. The question, concepts,
subquestions, and subanswers are provided in context to the Decomposer LM to generate the next subquestion.
Finally, the question, concepts, subquestions, and subanswers are provided to the Solver LM to generate the final
answer (in green).
three different components of complex multistep
reasoning – a solver LM is responsible for answer-
ing questions; a verifier LM provides feedback on
the correctness of the output from the solver, and
adecomposer LM identifies the basic concepts
required to solve the problem and generates step-
by-step subproblems by decomposing the original
question (see Figure 1 for a working example). Un-
like prior approaches, the decomposer in LM2gen-
erates each subproblem depending on the solver’s
answers to prior subproblems, along with the veri-
fier’s feedback on those answers. Furthermore, the
decomposer generates the conceptual requirements
to solve the problem, which further streamlines
the solver LM. Irrespective of the complexity of
the underlying reasoning, the world knowledge re-
quired to answer any question is typically better
preserved in larger, proprietary LMs. Considering
this, we use GPT-3.5 ( text-davinci-003 ) as the
solver without finetuning. For both the decomposer
and verifier, we implement parameter-efficient fine-
tuning (Hu et al., 2022) of LLaMA-2 (13 billion
parameters) separately. First, these models are fine-
tuned separately towards the tasks of decomposi-
tion and verification using datasets annotated byGPT-4. The decomposer is then taught to coordi-
nate with the solver and the verifier models in a
policy learning setup. LM2achieves promising per-
formance across a diverse set of reasoning tasks.
On the MATH dataset of mathematical reasoning,
LM2outperforms the best decomposer-tuning base-
line by a staggering margin 8.1% of absolute ac-
curacy on average. Although LM2uses the training
split of the MATH dataset for tuning the decom-
poser and the solver, it seamlessly generalizes to
out-of-distribution tasks in MedQA and JEEBench,
outperforming the best competitive baseline with
9.7 % and 7.71% difference on absolute accuracy
respectively.
Beyond the discourse of overall numbers, we
perform in-depth ablation analyses to identify the
roles of each component of the model. We ob-
serve that (i) the verifier LM and concept generated
by the decomposer LM play a crucial role in gen-
eralizing out-of-distribution reasoning tasks like
MedQA, JEEBench Chemistry, etc.; (ii) finetun-
ing the decomposer is crucial for better concept
identification – finetuned LLaMA-2 7B generates
more effective conceptual requirements compared
to even GPT-4; (iii) even while not using all the
2modular components of LM2, the prompt template
of structured reasoning boosts the performance of
GPT-4.
2 Related Work
The efficacy of explicitly generating intermediate
reasoning steps over direct generation of the re-
quired answer was first demonstrated by Nye et al.
(2021). Chain-of-thought prompting (Wei et al.,
2022) generalized the scratchpad learning of Nye
et al. (2021) into an in-context learning regime
using LLMs. Chain-of-thought and its succes-
sors (Chen et al., 2022; Yao et al., 2024) typically
let the decomposition of a composite, multi-step
reasoning problem remain implicit in the LLM.
Zhou et al. (2023) demonstrated that instead,
an explicit call to the LLM to generate multiple
smaller problems that are steps to answer the orig-
inal query achieves more robust reasoning. Their
proposed method, Least-to-Most prompting, uses
these simpler subproblems and their answers as
the context to solve the original problem. Sim-
ilarly, Khot et al. (2023) proposed a prompting-
based problem decomposition approach where the
LLM is asked to decompose a complex task using
few-shot examples. However, this still burdens a
single language model in handling both decomposi-
tion and solution. Juneja et al. (2023) circumvented
this challenge by distilling the decomposition abili-
ties into a relatively smaller language model. Their
proposed method, DaSLaM, utilizes two separate
language models that coordinate with each other to
solve complex reasoning problems. Their findings
suggest that finetuning the decomposer is more gen-
eralizable than finetuning the solver model. This
has been further supported by Wu et al. (2024)
recently. Tarasov and Shridhar (2024) explored
the distillation of decomposition abilities via of-
fline reinforcement learning. Khattab et al. (2022)
proposed a programmatic retrieval augmentation
framework, namely Demonstrate-Search-Predict
(DSP), for knowledge-intensive generation tasks.
DSP relies on the coordination between a genera-
tive LM and a retrieval model through sophisticated
programs. Recent attempts have been made to in-
corporate dense verifiers (typically, a finetuned,
bidirectional language model acting as a classifier)
aiding a generative model towards robust, verifiable
problem solving and text generation (Cobbe et al.,
2021; Sun et al., 2023). Different techniques for
verification of LM-generated outputs have been pro-posed subsequently, such as self-verification (Weng
et al., 2023), majority voting (Li et al., 2023), etc.
3 Methodology
Our proposed method, LM2, is built upon the co-
ordination of multiple LMs to perform reasoning
in a modular fashion. However, such coordination
is not implicit in the pertaining stage of a model;
instead, we seek to inculcate this ability via finetun-
ing (parts of) the LM multiplex. To this end, LM2is
built upon three functional components: a (prefer-
ably larger) solver model, a decomposer model,
and a verifier model.
For fine-grained control over the function of the
different components of LM2, we make use of a
structured, step-by-step input-output framework
(see Figure 1). The role of each of the modules in
LM2is described as follows.
3.1 Decomposer
Thedecomposer LM guides the solver LM to solve
a multi-step reasoning question in two ways. First,
it provides the solver model with a set of concepts
required to solve the problem. Second, it tells
the solver LM what is the next sub-question re-
quired to solve given the previous sub-questions
and their answers. More specifically, the decom-
poser LM is a function that can be defined as
D(q,{si, sai}, c) :Q×S×SA→ { S, C},
where qrepresents the initial question to be solved,
{si, sai}denotes the set of previous sub-questions
(si) and their corresponding answers ( sai), and ( c)
signifies whether the function needs to predict the
concept or the next sub-question. Qis the space of
all the questions, Sis the space of all sub-questions,
SAis the space of all sub-answers, and Cis the
space of all concepts.
Supervised finetuning. The decomposer train-
ing is performed in two stages similar to (Juneja
et al., 2023). The first stage is supervised finetun-
ing, where the language model is finetuned on a
dataset prepared using GPT-4. To create the dataset,
we provided GPT-4 with a question and its gold
reasoning. It was then asked to first generate all the
concepts required to solve the question, followed
by sub-questions and sub-answers. Only the ques-
tions that were answered correctly were included
in the dataset. Each sample in the dataset can
be expressed as a tuple {Q, c,{si, sai}n
i=1, sn+1},
where sn+1is the next sub-question given the pre-
vious sub-questions and answers. The decomposer
3was then finetuned on the standard language mod-
elling objective.
Policy optimization. With the supervised fine-
tuning step, the decomposer LM is conditioned to
respond to reasoning problems with concepts and
decomposed subquestions. However, it is still not
able to take the feedback from the solver and the
verifier models into account. To this end, we uti-
lize Proximal Policy Optimization (Schulman et al.,
2017) with the decomposer as the policy and the
solver and the verifier model as a black-box en-
vironment. Precisely, we compute different types
of rewards utilizing the feedback from the verifier
model that takes the solver model’s response into
account at each step and provides the decomposer
with necessary refinement signals.
3.2 Verifier
Given the complexity of multistep reasoning, we
need the verifier to be able to provide nuanced feed-
back to the decomposer on the possible mistakes
made by the solver; a binary correct/incorrect mes-
sage as employed by prior works with verifiers (Li
et al., 2023; Weng et al., 2023) will limit the de-
composer model’s scope of vision. For fine-grained
control, the verifier is finetuned on a supervised
dataset containing a question, an answer with an
error made in the correct answer, a classification
for the type of error, and an explanation for the
classification. The verifier classifies the given input
into nine classes as follows: 1Conceptual mis-
takes, 2Computational mistakes, 3Procedural
mistakes, 4Misunderstood question, 5Mistake
in the first step, 6Mistake in first half, 7Mistake
in second half, 8Mistake in last step, and 9No
mistake. The dataset was produced using GPT-4,
asking it to generate an explanation for the classi-
fication given the correct solution, wrong solution
and the classification. The verifier is finetuned to
generate the explanation and the classification (see
Section 3.3 for examples of each type of error mes-
sage and explanation).
3.3 Training with Decomposer Feedback
The training dataset curated for the decomposer
LM consists of only the correct answers; hence,
the decomposer is blind to the possible errors that
the language model can make. In order to make
the decomposer generate meaningful questions, we
further finetune the decomposer while working in
synergy with the solver language model using Pol-
icy gradient methods.Environment. The environment consists of a
black-box solver model Θ. The model Θgener-
ates an answer to the current question given the
concepts and previous questions and their answers.
Policy, action and state space. The decomposer
language model ϕcomprises the policy network. A
statesin the state space Sis defined by the con-
catenation of the initial state s0and all the actions
taken from the initial state to the current state. The
initial state s0is defined as the initial question Q.
The action space is defined as the token space of
the language model ϕ. Hence, a state sncan be
represented as (s0,{ai}n
i=1), where aiis the action
taken at the ithtime step.
Reward function. The reward is based on the
feedback given by the verifier at each sub-question
produced by the decomposer. The reward structure
is intuitively designed to impose penalties for errors
occurring in earlier sub-questions relative to those
occurring in later ones. This is because fixing an
early mistake can significantly increase the chances
of the question being correct. Further, the policy
is penalised more for conceptual and procedural
mistakes as compared to computational mistakes.
We construct the reward function for the kthsub-
question as follows:
R=γk9X
i=1ri (1)
where γ < 1is the discount factor responsible
for imposing more penalties on the earlier gener-
ations. riare the rewards for individual feedback
given by the verifier as defined below (for each
type of reward, we provide an example question
asked by the decomposer, an erroneous answer to
that question by the solver, type of error identified
and the explanation generated by the verifier in red
textboxes).
Conceptual correctness reward is defined as,
r1=−0.15I[V(sk, sak) = 1] (2)
where Iis the indicator function, Vis the verifier
that takes in input the kthsub-question ( sk) and its
answer produced by the solver ( sak) and outputs
the category of mistake. This reward accounts for
any mistake made by the solver in understanding
or while applying a concept incorrectly.
4Q: What is a,b,cin terms of common difference d?
A:Since a,b, andcform an arithmetic sequence, we can express
cin terms of aasc=a+d, where dis the common difference.
Verifier : Conceptual Mistakes: The student misunderstood the
concept of finding the possible values of the common difference in
an arithmetic sequence to form a triangle with a given perimeter.
Computational correctness reward is defined as,
r2=−0.05I[V(sk, sak) = 2] (3)
This reward accounts for cases where the model
has made any mistakes in calculation or formula
application. These kinds of mistakes are weighted
less compared to all other types because an external
tool can be easily used for these mechanical tasks.
Q:What is the value of a+c?
A:Since we are given that a,b, and cform an arithmetic
sequence, we can express bas the average of aandc. This
gives us b=a+c
2. The perimeter of the triangle is given by
P=a+b+c=a+a+c
2+c=3a+3c
2. We are given that the
perimeter is 60 units, so we have 60 =3a+3c
2. Solving for a+c,
we get a+c= 20
Verifier :Calculation Mistakes: The model has made a calcula-
tion mistake while calculating the value of a+c.a+cshould be
40.
Procedural correctness reward is defined as,
r3=−0.15I[V(sk, sak) = 3] (4)
This encompasses any mistake made by the model
while following a step-by-step procedure whose in-
formation is already given in-context while solving
the problem.
Q:What are the possible values of a, d?
A:Substituting this into the equation a+c= 20 , we get
a+ (a+d) = 20 , which simplifies to 2a+d= 20−a. Since
the side lengths must be integers, aanddmust be integers. We
can then try different values of aanddthat satisfy the equation
2a+d= 20−a. After trying different values, we find that the
only solution that gives integer side lengths is a= 10 andd= 20 .
Verifier :Procedural Mistakes: The student made a mistake in
the procedure of solving the problem by not accurately finding the
possible values of the common difference.
Question understanding reward is defined as,
r4=−0.2I[V(sk, sak) = 4] (5)
This reward accounts for the cases when the model
has made a mistake in understanding the sub-
question. Either it has not answered the quantity
asked in the sub-question. This gives a signal thatthe sub-question was either incoherent with the pre-
vious questions or was too complex for the model
to answer. This kind of mistake is important to
address and, hence, is given a higher weight.
Q:How many distinct values of a, b, c are possible?
A:This gives us the side lengths of the triangle as a= 10 ,
b= 15 , andc= 30 .
Verifier :Mistakes Understanding Question: The model has
made a mistake by not giving the number of distinct values.
Reward based on place of mistake. As dis-
cussed above, later mistakes are penalised less than
the earlier ones. Hence, if a mistake is made in the
first step, it is given a reward of −0.2. If the model
makes a mistake in the first half of the sub-answer,
it is given a reward of −0.12. For a mistake in the
last half of the sub-answer, it is given a reward of
−0.08. If the mistake is made in the last step, it is
given a reward of −0.05.
No-mistake rseward is the case when the model
has not made any mistake in answering the sub-
question and is given a positive reward of +1.
3.4 Inference
During the inference, the decomposer, solver, and
verifier models work together to answer a given
question (see working example in Figure 1). Dur-
ing the inference, the decomposer first produces
a list of concepts required to solve the question.
Then, given the question and concepts as context,
the decomposer produces a sub-question. The sub-
question is answered by the solver. Now, given the
sub-question and sub-answer, the verifier provides
feedback in the form of a multi-class classification
into the above-described classes of mistakes. If the
feedback provided by the verifier consists of either
a conceptual mistake, procedural mistake, mistake
in understanding or mistake in the first step, we
again generate the sub-question.
4 Experiments
For all the experiments, LM2uses the OpenAI
text-davinci-003 model (hereafter mentioned
as GPT-3.5) as the solver and LLaMA-2 13B (Tou-
vron et al., 2023) as the base models for the decom-
poser and the verifier.
4.1 Training data curation
For the first stage of finetuning of the decomposer
LM, we curated a dataset of 15,396question, con-
cept, sub-question, sub-answer tuples. The ques-
tions were taken from the train split of the MATH
5dataset (Hendrycks et al., 2021). The questions
were taken from the MATH dataset. For verifier
LM finetuning, a dataset of 3,674question-answer-
classification tuples was generated. Details of the
prompts used for each of these steps are provided
in Appendix A.
4.2 Training details
We finetune LLaMA2-13B for both the decomposer
and verifier. We train for 8 epochs with a batch size
of 128, learning rate 2e-5, warmup steps of 100, a
Lora r value of 4, LoRA Alpha of 16 and dropout of
0.05. The models were trained in 8-bit quantization
on an 80G A100 GPU.
For the second stage of fine-tuning, we finetuned
the last 3 layers of LoRA adapters, using a batch
size of 16, gradient accumulation steps=4, init kl
coef=0.01, target=4. For inference, we used a tem-
perature of 0 in all experiments for consistency of
results with a max output length of 2000.
4.3 Evaluation
We evaluate our method on hard reasoning datasets
that require multi-step reasoning. These datasets
include MATH (Hendrycks et al., 2021) (test split),
JEEBench (Arora et al., 2023), and MedQA (Jin
et al., 2020) (English questions). The MATH
dataset contains math questions from challenging
math competitions, since it was also used for train-
ing, this shows our performance on in-domain ques-
tions. Next, we evaluate on the out-of-distribution
datasets like JEEBench which contains PCM ques-
tions extracted from the JEE Advanced exam and
MedQA which contains open-domain questions
from professional medical board exams. We only
evaluate questions in the English language.
4.4 Baseline Details
We compare LM2with five existing methods:
Chain-of-thought prompting ( CoT ) (Wei et al.,
2022), Least-to-most prompting ( L2M ) (Zhou
et al., 2023), Progressive Hint Prompting
(PHP ) (Zheng et al., 2023), Demonstrate-Search-
Predict ( DSP ) (Khattab et al., 2022), and
DaSLaM (Juneja et al., 2023). The original set-
ting of PHP requires an 8-shot prompting; however,
since all other methods including LM2predict in the
zero-shot setting, we use PHP in 1-shot for a fairer
comparison.DatasetMethod
CoT L2M PHP DSP DaSLaM LM2
PnC 16.416.0 10.216.2 21.4 30 .0
NT 14.411.0 9.8 20.3 26.1 41 .0
ALG 27.622.4 24.015.3 33.4 34 .0
I-ALG 16.416.8 10.017.0 24.8 27 .8
Calc. 14.014.58 14.28 18.8 18.2 34 .0
P-ALG 32.328.0 26.528.0 44.0 47 .0
Geom. 14.212.5 14.0 5.2 21.4 32 .0
MedQA 50.349.8 47.552.3 50.1 57 .1
Table 1: Performance comparison of LM2with the base-
lines on MATH and MedQA datasets using GPT-3.5 as
the solver LM.
4.5 Ablation Study
In our investigation, we perform five types of abla-
tion studies aimed at comprehensively understand-
ing the significance of each component within the
LM2pipeline.
We start with investigating the relevance of the
verifier by conducting an experiment where we re-
move it entirely ( LM2\V). Here, we accept each
question generated by the decomposer during the
inference process without any verification. Then,
we explore the role of concepts within the pipeline.
Here, we alter the approach by instructing the de-
composer to directly generate sub-questions, with-
out providing the concepts to the Solver LM during
the answer generation phase ( LM2\C). Following
this, we investigate the incremental gains achieved
through the second stage of finetuning via policy
learning. To accomplish this, we analyze the per-
formance of the decomposer checkpoint after the
initial stage of fine-tuning, referred to as ( LM2\RL).
To assess the impact of different types of rewards
provided, we partition the rewards into two distinct
categories: i) based on the type of mistake, which
encompasses conceptual, computational, procedu-
ral, and question understanding correctness, and
ii) based on the position of mistake. Subsequently,
we come up with two ablation variants, finetuned
using each category of rewards: LM2-Type and LM2-
Position.
5 Results
We summarize the performance of LM2along with
the baseline methods on the MATH and MedQA
datasets in Table 1 and on the JEEBench dataset in
Table 2. Across all the datasets, LM2improves upon
existing methods (using GPT-3.5 solver) by a huge
margin. It demonstrates an average 8%improve-
ment on the MATH dataset and an average 2.5%
improvement on the JEEBench dataset as compared
6MethodDataset
Phy.
MCQMath.
MCQPhy.
Multi.Math.
Multi.Phy.
Num.Math.
Num.Phy.
Int.Math.
Int.Chem.
Int.Chem.
Num.Chem.
Multi.Chem.
MCQ
CoT 33.33 21.9 6.25 12.0 3.03 1.69 12.5 10.8 17.3 11.6 11.6 40.0
PHP 22.22 17.07 6.25 7.59 3.03 1.69 0* 4.0 11.7 9.7 12.2 37.5
L2M 22.22 21.9 6.25 12.5 3.03 3.38 10.0 10.8 13.0 9.7 10.0 20.0
DaSLaM 55.5 29.5 18.7 16.0 6.06 10.1 15.7 11.7 14.2 9.2 11.6 14.6
GPT4 55.5 34.1 27.5 21.5 15.1 11.8 22.7 24.3 17.9 25.5 48.3 60.0
LM251.85 30.18 26.8 16.4 15.15 13.1 16.2 13.5 26.0 23.2 26.6 53.3
LM2\V 37.03 24.52 14.6 11.7 12.2 11.4 11.4 11.7 17.3 16.2 13.3 30.0
LM2\C 29.62 20.75 14.6 9.4 9.09 10.8 9.0 8.1 17.3 11.6 13.3 16.6
GPT4-C 29.62 28.3 14.6 11.5 15.15 11.4 9.0 11.4 21.7 23.2 33.33 30.0
LM2\RL 33.33 21.9 18.7 12.7 12.2 10.1 10.0 8.1 17.3 12.4 13.3 27.3
LM2-Type 46.1 28.0 20.3 14.0 13.4 11.4 15.0 13.5 24.0 23.2 23.6 45.4
LM2-Position 38.4 24.52 16.0 12.9 12.2 11.4 15.0 10.8 24.0 20.6 20.3 33.0
GPT35-SP 33.3 29.2 7.5 12.6 9.0 8.4 12.5 8.0 17.6 9.2 12.2 41.6
GPT4-SP 61.1 36.5 30.0 26.5 30.0 14.2 43.75 32.0 17.6 36.5 49.1 66.6
Table 2: Performance of LM2on JEEBench Dataset along with baselines and ablation variants . (Top third ) we
highlight best and second best methods in boldface andunderline .LM2generally outperforms all existing prompting
techniques with GPT-3.5 on different topics and different types of questions (other than Physics MCQ questions).
In 3/12 cases, LM2outperforms GPT-4. ( Middle third ) we observe a large drop in performance with each ablation
variant, pointing towards an efficient integration of these modules into LM2(see Section 4.5 for the description of
each variant). ( Bottom third ) Performance of the structured answer generation employed in LM2, without decomposer
and verifier, using GPT-3.5 and GPT-4 as solvers.
to the best-performing baseline DaSLaM.
Can it improve on out-of-domain tasks? In
both DaSLaM and LM2, the solver model is kept
frozen with the hope of retaining generalizability.
However, the decomposer model in both methods
(and the verifier in LM2) are finetuned using mathe-
matical reasoning problems. This raises the ques-
tion of the generalizability of these finetuned com-
ponents over problems other than mathematical rea-
soning. One of the most significant challenges with
DaSLaM is that it is not able to perform well on
out-of-domain tasks like JEEBench Chemistry. We
find that our method can surpass this limitation as
can be seen in Tables 1 (MedQA) and 2 (JEEBench
Chemistry). While DaSLaM degrades the perfor-
mance over CoT on MedQA, LM2achieves an abso-
lute accuracy gain of 6.8percentage points.
How important is the verifier? Next, we seek
to investigate the relative importance of each com-
ponent in our pipeline. We observe that the accu-
racy decreases substantially upon removing the ver-
ifier model ( LM2\V in the middle third of Table 2).
We can see that there is a drop of 13.0%in Chem-
istry versus 10.08% in Physics and 3.4%in Math
subsets. The relative drop in accuracy with the ab-
lation of the verifier is sharper with multi-answer,
numeric, and integer answer questions. This makes
sense given the computational reasoning require-
ment is higher in these problems and the verifier
plays a crucial role in guiding the decomposer andthe solver along the correct reasoning path.
Figure 2: Comparison of token generation cost. We
depict the average number of tokens generated by the
solver model using different methods to solve the given
question averaged over 50 questions from the JEEBench
dataset.
How important are the concepts? As can be
seen from Table 2, removing concepts decreases the
accuracy of Physics subset by 11.6%, Maths subset
by6.03%, and Chemistry subset by 17.5%. This
shows that concepts also play a very important role
in improving the performance on out-of-domain
datasets like Physics and Chemistry. Typically,
LM2\C fares worse than the rest of the ablation
variants, demonstrating that the concepts are the
most important component in LM2.
GPT-4 as concept generator? We also check
7Figure 3: Comparison of GPT-4, DaSLaM and LM2on an example from MATH dataset.
how our decomposer compares to GPT-4 while
generating concepts. To compare this, we prompt
GPT-4 to generate concepts given the question. We
observe that there is an average decrease of 9.13%
when generating concepts using GPT-4 when com-
pared to the Decomposer model, indicating the
higher quality of concepts generated as a result of
feedback-based fine-tuning.
What is the effect of feedback-based finetun-
ing? The effect of feedback-based fine-tuning is
evident when comparing the performance of the de-
composer without the second stage of fine-tuning
alongside the verifier to that of LM2. On average,
we observe a notable decrease of 9.6%in perfor-
mance when the second stage of fine-tuning is omit-ted. This finding highlights the significance of fine-
tuning as a crucial step in optimizing model perfor-
mance. However, the importance of concepts and
the verifier appears to outweigh that of fine-tuning.
This suggests that while fine-tuning contributes to
improved model performance, the incorporation of
concepts and a verifier into the model architecture
yields more substantial enhancements.
How does the structured answering template
contribute? Recall that in LM2, we introduce a
novel, structured answering template for control-
lable coordination between the three models. It is
imperative to investigate the role of such a tem-
plate alone behind the performance boost. We
make use of the template with two different solver
8models, GPT-3.5 and GPT-4. As we can see in
the bottom third of Table 2 (coined as modelname -
SP), both models improve upon their base perfor-
mance with our structured template. However, the
stronger GPT-4 model is able to utilize the tem-
plate much more efficiently, with an average gain
of7.8%across the JEEBench problems. Typically,
improvement on Physics problems is higher than
the Math problems, indicating that language mod-
els are not very good at retrieving physics con-
cepts and solving the problem when using chain-
of-thought prompting. It should noted that while
the structured answering template alone is a pow-
erful boost, it is much weaker alone without the
complete coordination in LM2.
Does guided reasoning help limit token us-
age? An important challenge with iteratively inter-
acting with an LLM is the increased token usage
that will translate to expenses in either computa-
tional or monetary terms. In Figure 2, we plot
the average token usage (per problem) incurred
by the solver model (GPT-3.5) while using LM2
and DaSLaM against that of base chain-of-thought
generation. Note that we only show the token us-
age corresponding to the modified responses while
using LM2and DaSLaM. Both these methods origi-
nally use base CoT to generate the initial response
and therefore, their total token usage will always
be higher than that of CoT. However, the added
structure and guided reasoning significantly reduce
the token usage in the modified response. LM2pre-
vails in this aspect too. A major reason behind
this is the step-by-step synergy between the decom-
poser, the solver, and the verifier in LM2. Since the
decomposer generates the subquestion depending
upon the response from the solver to the previous
subquestion, the chances of redundant generation
decrease, as opposed to DaSLaM where the sub-
questions are generated all at once.
Example analysis. To further understand the nu-
ances of LM2, we perform an analysis of the gener-
ated output on an example from the MATH dataset
(see Figure 3). We compare between LM2, DaSLaM
and GPT-4 with CoT. As we can see, GPT-4 makes
an incorrect interpretation of the question itself. It
assumes that the total journey after delay takes 10
hours, leading to an incorrect choice of option. The
subquestions produced by DaSLaM do not adhere
to the order of reasoning required to solve the prob-
lem and generate redundant questions. It starts with
asking What is the total distance to be covered?
However, in the second question, it asks for thespeed of the train which is already given in the
question itself. The 3rd subquestion generated by
DaSLaM is actually the original question, and the
solver makes a numerical mistake by simplifying
the fraction3d
4
75tod
300instead ofd
100. Without a ver-
ifier, this erroneous response is integrated into the
reasoning context of the solver. In the next ques-
tions, the same problem is asked to be solved and
the solver continues to make incorrect responses.
With LM2, we observe a much more well-defined,
crisp line of questioning by the decomposer model;
the solver is able to reach the correct answer op-
tion without regenerating the same information or
drawing incorrect subanswers.
6 Conclusion
In this paper, we present LM2, a cooperative cohort
of generative language models working together to
solve complex reasoning problems. LM2utilizes a
frozen solver model that is guided to solve reason-
ing problems by incrementally answering questions
framed by a decomposer model and checked by the
verifier model that is trained to coordinate with
each other. We find that LM2proves its supremacy
over existing methods over a variety of reasoning
tasks, both in-domain and out-domain. We find that
despite being trained using mathematical reason-
ing examples, our proposed structured response
scheme along with the fine-grained verification
strategy plays a crucial role in generalizing LM2
to heavily out-of-distribution tasks like medical
question answering and chemistry.
Limitations. Despite promising results, LM2
bears some inherent limitations. Compared to
purely prompting-based methods, it requires a cer-
tain computational overhead for the two-staged
training. With proprietary LLM-based solvers, LM2
incurs extra token usage over single-pass solutions
like chain-of-thought. Implicit limitations of the
solver model, like lack of length generalization,
arbitrary digit manipulation, etc. are expected to
be inherited in LM2as well. A possible future work
can be towards incorporating deterministic solvers
and tools into the multiplex.
References
Daman Arora, Himanshu Gaurav Singh, and Mausam.
2023. Have llms advanced enough? a challenging
problem solving benchmark for large language mod-
els.
9Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems.
Subhabrata Dutta, Ishan Pandey, Joykirat Singh, Sunny
Manchanda, Soumen Chakrabarti, and Tanmoy
Chakraborty. 2024. Frugal lms trained to invoke sym-
bolic solvers achieve parameter-efficient arithmetic
reasoning. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 38, pages 17951–
17959.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021. Measuring mathematical
problem solving with the math dataset. NeurIPS .
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations .
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,
Hanyi Fang, and Peter Szolovits. 2020. What disease
does this patient have? a large-scale open domain
question answering dataset from medical exams.
Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti,
Sunny Manchanda, and Tanmoy Chakraborty. 2023.
Small language models fine-tuned to coordinate
larger language models improve complex reasoning.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
3675–3691, Singapore. Association for Computa-
tional Linguistics.
Omar Khattab, Keshav Santhanam, Xiang Lisa
Li, David Hall, Percy Liang, Christopher Potts,
and Matei Zaharia. 2022. Demonstrate-search-
predict: Composing retrieval and language mod-
els for knowledge-intensive nlp. arXiv preprint
arXiv:2212.14024 .
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao
Fu, Kyle Richardson, Peter Clark, and Ashish Sab-
harwal. 2023. Decomposed prompting: A modular
approach for solving complex tasks. In The Eleventh
International Conference on Learning Representa-
tions .
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,
Jian-Guang Lou, and Weizhu Chen. 2023. Making
large language models better reasoners with step-
aware verifier.Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, et al. 2021. Show your work: Scratch-
pads for intermediate computation with language
models. arXiv preprint arXiv:2112.00114 .
OpenAI. 2023. Gpt-4 technical report.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal policy
optimization algorithms. CoRR , abs/1707.06347.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and
Daya Guo. 2024. Deepseekmath: Pushing the limits
of mathematical reasoning in open language models.
arXiv preprint arXiv:2402.03300 .
Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xi-
aochi Wei, Shuaiqiang Wang, Yan Zhang, and Dawei
Yin. 2023. Towards verifiable text generation with
evolving memory and self-reflection. arXiv preprint
arXiv:2312.09075 .
Denis Tarasov and Kumar Shridhar. 2024. Distilling
llms’ decomposition abilities into compact language
models.
Shubham Toshniwal, Ivan Moshkov, Sean Narenthi-
ran, Daria Gitman, Fei Jia, and Igor Gitman. 2024.
Openmathinstruct-1: A 1.8 million math instruction
tuning dataset. arXiv preprint arXiv:2402.10176 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems .
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,
Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.
2023. Large language models are better reasoners
with self-verification.
Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu,
VG Vinod Vydiswaran, Navdeep Jaitly, and Yizhe
Zhang. 2024. Divide-or-conquer? which part should
you distill your llm?
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
2024. Tree of thoughts: Deliberate problem solving
with large language models. Advances in Neural
Information Processing Systems , 36.
Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo
Li, and Yu Li. 2023. Progressive-hint prompting
improves reasoning in large language models.
10Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H.
Chi. 2023. Least-to-most prompting enables com-
plex reasoning in large language models. In The
Eleventh International Conference on Learning Rep-
resentations .
A Training Data Creation
The data was generated using GPT-4. A tempera-
ture of 0.7 is used to ensure diversity in the gen-
erated data. We only stored the sub-question, sub-
answer dataset if the number of sub-questions gen-
erated was more than three, this was done to ensure
high data quality so that the model is able to decom-
pose longer and more difficult questions effectively.
First, we generate all the concepts, then the sub-
questions given the question and the gold chain
of thought. Finally, we generate the sub-answer
given the question, a gold chain of thought and
the sub-question to be answered. for the verifier,
we first ask the LLM to answer the given question
using standard COT prompting. Then based on
the correctness of the answer, we take the solution
chain of thought produced by the LLM and the gold
answer and ask the LLM to classify the produced
solution based on the mistake made. If the answer
is correct, we store it separately and include it to
make up to 10% of the dataset with the label as ’No
Mistake’. Prompts for the data curation are given
below.
A.1 Verifier Data Creation
A.1.1 Prompt
You are a teacher, and you are grading
a student’s answer to a question. The
student’s answer is as follows: {COT_LLM}
The correct answer is as follows:
{COT_gold} Please provide feedback to
the students on the mistakes they have
made. You need to fill out a rubric and
classify the mistakes into the following
categories:
1. Conceptual Mistakes: The student has
misunderstood the concept or has applied
the wrong concept.
2. Computational Mistakes: The student
has made a mistake in the calculations.
3. Procedural Mistakes: The student has
made a mistake in the procedure of solving
the problem.
4. Mistake in understanding the question:
The student has made a mistake inunderstanding the question.
5. Mistake in the first step: The student
has made a mistake in the first step of
the solution.
6. Mistake in the first half: The student
has made a mistake in the first half of
the solution.
7. Mistake in the second half: The student
has made a mistake in the second half of
the solution.
8. Mistake in the last step: The student
has made a mistake in the last step of
the solution.
9. No mistake: The student has not made
any mistake.
Please first provide feedback then
fill the rubric and then finally tell
your feedback to the student in between
<feedback> and </feedback> tags as shown
below:
For example, if you want to tell the
student that they have made a mistake in
the first step and a conceptual mistake,
then you need to write the following:
<feedback> 1,4 </feedback> Do not write
anything else in between <feedback> and
</feedback> tags except the numbers.
Now, please provide feedback to the
student on the mistakes they have made.
A.2 Decomposer Data Creation
A.2.1 Concepts data creation
I have a question’s solution, tell me
all the specific concepts, theorems and
formulas (separated by a comma,) used in
it. An example is given below.
Question: How many primes are in the row
of Pascal’s Triangle that starts with a 1
followed by a 6?
Answer: If the row contains a 1, then
a6, then the binomial coefficients must
be 6
0
and 6
1
. All we need to check now
are 6
2
and 6
3
, since 6
0
= 6
6
, 6
1
= 6
5
, and 6
2
= 6
4
. 6
2
=6!
4!×2!= 15 , and 6
3
=6!
3!×3!= 20 . None of those is prime,
so there are 0prime numbers in the given
row.
Concepts: Coefficients in Pascal’s
Triangle, Binomial Coefficients Formula,
Prime Numbers
Question: question
11Answer: answer
Concepts:
A.2.2 Sub-question data creation
I have a question, it’s a solution and a
sub-question.
Your task is to break the question into
sub-questions based on the steps in the
answer.
Keep the following tips in mind:
1. Make sure not to break the
question into trivial sub-questions, the
sub-questions should be informative.
2. The sub-questions should not require
multiple steps to answer, something like
2-3 steps to solve is ideal.
3. One way to break the question could
be to identify what all quantities are
required in the question by observing it’s
answer and then try to frame sub-questions
based on the unknown entities.
4. Make sure to put each question in the
question tag like $ question(What is the
acceleration of the car as a function of
time?)$
One example is given below.
Question: How many primes are in the row
of Pascal’s Triangle that starts with a 1
followed by a 6?
Answer: If the row contains a 1, then
a6, then the binomial coefficients must
be 6
0
and 6
1
. All we need to check now
are 6
2
and 6
3
, since 6
0
= 6
6
, 6
1
= 6
5
, and 6
2
= 6
4
. 6
2
=6!
4!×2!= 15 , and 6
3
=6!
3!×3!= 20 . None of those is prime,
so there are 0prime numbers in the given
row.
Sub-questions:
$ question(How can the first two numbers
be represented in form of binomial
coefficients?)$, $ question(What are the
values of all the coefficients in the
row?)$, $ question(How many of the above
numbers are prime?)$
Question: question
Answer: answer
Sub-questions:
A.3 Sub-answer data generation
I have a question, it’s solution and a
sub-question.I want you to answer the subquestion along
with an explanation.
Make sure to put the sub-answer in
the answer tag like $sub-answer(The
acceleration of the car at time t =
2 seconds is speed / time = 2m/s/2s =
1m/s2)$
Think step by step.
Question: question
Answer: answer
Sub-question: sub-question-array[i]
Sub-Answer:
12