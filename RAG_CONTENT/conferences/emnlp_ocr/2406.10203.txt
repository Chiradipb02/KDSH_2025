A Probability–Quality Trade-off in Aligned Language Models and its
Relation to Sampling Adaptors
Naaman Tan
 Josef Valvoda
 Tianyu Liu
 Anej Svete
Yanxia Qin
 Kan Min-Yen
 Ryan Cotterell
National University of Singapore
 University of Copenhagen
 ETH Zürich
{tannaaman ,knmnyn }@nus.edu.sg jval@di.ku.dk
{tianyu.liu ,asvete ,rcotterell }@inf.ethz.ch
Abstract
The relationship between the quality of a string,
as judged by a human reader, and its probabil-
ity,p(y)under a language model undergirds
the development of better language models. For
example, many popular algorithms for sam-
pling from a language model have been con-
ceived with the goal of manipulating p(y)to
place higher probability on strings that humans
deem of high quality (Fan et al., 2018; Holtz-
man et al., 2020). In this article, we examine
the probability–quality relationship in language
models explicitly aligned to human preferences,
e.g., through reinforcement learning through
human feedback. We show that, when sam-
pling corpora from an aligned language model,
there exists a trade-off between the strings’ av-
erage reward and average log-likelihood under
the prior language model, i.e., the same model
before alignment with human preferences. We
provide a formal treatment of this phenomenon
and demonstrate how a choice of sampling
adaptor allows for a selection of how much
likelihood we exchange for the reward.
https://github.com/tanyjnaaman/
probability-quality-paradox
1 Introduction
The relationship between the probability of a string
and its quality as judged by a human reader is fun-
damental to the goal of producing high-quality text
from probabilistic language models. For example,
the belief that a string ywith a higher probability
under a language model pshould be of higher
quality (Graves, 2012; Fan et al., 2018; Holtzman
et al., 2020; Zhang et al., 2021) has motivated
the development of sampling adaptors like top- k
(Fan et al., 2018) and nucleus sampling (Holtzman
et al., 2020). The intuition behind this belief is that,
under a well-calibrated language model trained
primarily on human-written text, strings that occur
with high probability should be more human-like,
and thus judged by humans to be of higherquality. Indeed, sampling methods that skew a
language model towards high-probability strings
have been shown to dramatically improve the
quality of text sampled from a model (Wiher et al.,
2022). Over the years, several studies have since
contributed to a more nuanced understanding of the
probability–quality relationship (Holtzman et al.,
2020; Zhang et al., 2021; Basu et al., 2021) and led
to the development of more sophisticated sampling
methods (Hewitt et al., 2022; Meister et al., 2022).
This paper seeks to explain the relationship be-
tween probability and quality in the specific case
ofaligned language models, i.e., language models
explicitly fine-tuned to better match human pref-
erences, e.g., through reinforcement learning with
human feedback (RLHF; Christiano et al., 2017;
Leike et al., 2018; Ziegler et al., 2020; Stiennon
et al., 2020; Ouyang et al., 2022a,b; Korbak et al.,
2022, 2023). In particular, we provide a formal ar-
gument and empirical evidence that in RLHF-tuned
models, there is an inherent anti- correlation, i.e., a
trade-off , between probability and quality.
In the theoretical portion of this paper, we
formalize the probability–quality trade-off in
aligned language models. Specifically, we show
that for corpora of generated strings of a large
enough size, the average log-probability under
theprior (unaligned) language model ptrades off
with the average score assigned by a reward model
(Gao et al., 2023). We refer to this phenomenon
as a probability–quality trade-off because, in the
context of aligned language models, quality is
often operationalized by the reward model (Perez
et al., 2022; Lee et al., 2024). This trade-off
follows straightforwardly from an application of a
concentration inequality and can be seen as a direct
corollary of the asymptotic equipartition property
(AEP; Cover and Thomas, 2006). In addition to the
standard version of the AEP based on Chebyshev’s
inequality, we also prove a tighter version of the
AEP based on a Chernoff bound that applies to cer-
tain language models, including Transformer-basedarXiv:2406.10203v4  [cs.CL]  28 Oct 2024models (Vaswani et al., 2017). Lastly, we show
how an appropriate choice of a globally normalized
sampling adaptor can control the probability of
a generated string under the prior p. This finding
implies that they can be used to choose a point on
the probability–quality trade-off and control the
average quality of the generated text. Interestingly,
the trade-off predicts the emergence of Simpson’s
paradox, which we also observe in our experiments.
In the empirical part of our paper, we present
two sets of experiments. First, with synthetic data,
we construct toy language and reward models to
further demonstrate our theoretical results. This
gives easily reproducible empirical evidence for
our claim. Then, with a second set of experiments,
we show that this trade-off exists in practice with
open-sourced RLHF-tuned models and that glob-
ally normalized sampling adaptors do allow us to
control where a corpus of generated text will lie
on the trade-off. Notably, we find that in prac-
tice, locally normalized sampling adaptors (e.g.,
nucleus sampling; Holtzman et al., 2020; Meister
et al., 2023a) are sufficiently close approximations
of their globally normalized counterparts, and can
be effectively used to control the trade-off.
2 The Probability–Quality Relationship
LetΣbe an alphabet . Astring y∈Σ∗is a finite
list of symbols drawn from Σ. The set of all strings
is called Σ’sKleene closure and is denoted as Σ∗.
Alanguage model pis a probability distribution
overΣ∗. It is often assumed that there is a positive
correlation between the quality of a string and
its probability under a language model trained on
human text p(y)(Graves, 2012; Fan et al., 2018;
Holtzman et al., 2020; Zhang et al., 2021).1The
idea behind this assumption is that under such a
model, a higher probability string should be more
human-like, and thus judged by humans to be of
higher quality. Because of this positive correlation,
string probability is commonly used to reason
about text quality in the context of natural language
processing. For example, a number of studies on
language modeling methods report measures of
perplexity to quantify the quality of text produced
by the model (Vaswani et al., 2017; Devlin et al.,
2019; Brown et al., 2020; Pillutla et al., 2023).
1Probability here refers to string likelihood under a
general-purpose, unconditional language model. The
distinction is important since we will examine the relationship
between this probability and the quality of strings in an
aligned language model.More recently, several authors have observed ev-
idence that this positive correlation breaks down at
extremes, finding instead that a string’s probability
is positively correlated with its quality up to an
inflection point, after which it becomes negatively
correlated (Yang et al., 2018; Stahlberg and Byrne,
2019; Holtzman et al., 2020; Zhang et al., 2021;
Meister et al., 2022). Meister et al. (2022) show
that this inflection point lies near the entropy of
human-written text, and hypothesize that a string
should encode a similar amount of information
to natural language strings to be considered high-
quality. This finding has since inspired various
sampling adaptors, e.g., locally typical (Meister
et al., 2023b) and η-sampling (Hewitt et al., 2022).
In this paper, we investigate the probability–
quality relationship in aligned language models.
There are two reasons for this choice. First, aligned
models have an additional constraint—they are
fine-tuned to only produce high-quality text (as
judged by a reward model). And, a priori , it
is unclear how alignment might influence the
probability–quality relationship. Second, aligned
language models are mathematically similar to
the conditional language models often found in
machine translation and controlled text generation,
for which prior work has found relationships not
seen in unconditional language models (Callison-
Burch et al., 2007; Banchs et al., 2015; Teich et al.,
2020; Sulem et al., 2020; Lim et al., 2024).
3 Learning from Human Feedback
Because our investigation focuses on aligned
language models, we now introduce RLHF, a
popular algorithm for alignment. Given a task
of interest, we are interested in producing strings
that are aligned with human preferences for that
task. Let A={–,+}denote binary judgments
of alignment, and Abe an A-valued random
variable. Then, we assume the existence of
a true human-aligned distribution over strings
p+(y)def=p(y|A=+), such that strings with high
probability under p+also receive positive scores
from human annotators. For example, we may
desire that strings that are offensive should have
a lower probability under p+for chat-related tasks.
With this, we can formalize the goal of alignment
as obtaining an aligned language model q+(y)
such that q+(y)is a good approximation to p+(y).
RLHF is a widely used method of finding such a
model. At the core of RLHF is a reward function ,
which models preferences of human annotators,denoted as r: Σ∗→(−∞, B]where Bis a bound
in∈ R>0.2In practice, the reward function is
itself typically parameterized by a neural network
and derived by modeling preferences with a
Bradley–Terry model (Bradley and Terry, 1952)
and a ranked dataset. The human-aligned language
model p+(y), reward function r(y)and prior
language model p(y)can be related as follows
(Korbak et al., 2022):
p+(y) =p(y) exp
1
βr(y)
Z(+), (1)
where β∈ R>0is a scaling factor and
Z(+)def=X
y∈Σ∗p(y) exp1
βr(y)
. (2)
is the normalizing constant.
If we take a variational inference perspective of
RLHF (Levine, 2018; Korbak et al., 2022), then the
goal of RLHF is to find an aligned language model
q+(y)that minimizes the backward Kullback–
Leibler (KL) divergence between q+and the ground
truth aligned distribution over strings p+:
KL(q+||p+) (3a)
=X
y∈Σ∗q+(y) logq+(y)
p+(y)(3b)
=X
y∈Σ∗q+(y) logq+(y)Z(+)
p(y) exp
1
βr(y) (3c)
= log Z(+) + KL( q+||p)−1
βE
y∼q+[r(y)].(3d)
This objective can be seen as KL-regularized
reinforcement learning (Stiennon et al., 2020).
Notably, anypreference-aligned language model
can be expressed in the framework of RLHF, even
if no explicit reward function was used in training
the model. As shown by Rafailov et al. (2023), for
anyaligned language model q+that minimizes the
backward-KL objective, we can always construct a
“secret” reward function rq+with:
rq+(y) =β
logq+(y)
p(y)+ log Z(+)
.(4)
The implication of this is that the result we prove
for RLHF-tuned models can be trivially extended
toanyconditionally aligned language model, like
the ones often deployed in controlled text genera-
tion (Hu et al., 2017; Krause et al., 2021; Yang and
Klein, 2021; Liu et al., 2021; Zhang et al., 2023).
2We assume that the reward function is bounded, following
Levine (2018); Korbak et al. (2022).4 Sampling Adaptors
We are often interested in sampling strings from
a language model p(y). Sampling is usually
performed autoregressively, i.e., we sample a
symbol y∈Σdef= Σ∪ { EOS}iteratively from
p(· |y<t)at each time step tuntil the special
end-of-sequence EOS symbol is reached. Locally
normalized sampling adaptors (Meister et al.,
2023a) are post-hoc alterations of p(· |y<t)that
have been shown to dramatically improve the
quality of text produced by language models, and
are often considered an integral part of a text
generation pipeline (Wiher et al., 2022). Common
examples include top- k(Fan et al., 2018) and
nucleus sampling (Holtzman et al., 2020).
Applying a locally normalized sampling adaptor
top(· |y<t)at each tis commonly formulated as
the composition of two steps. First, the application
of a transform function γ: ∆|Σ|−1→ R>0|Σ|
that maps the distribution over Σto a vector of
non-negative values. The transform function is
responsible for the core logic of the sampling
adaptor, e.g., assigning symbols outside the top- k
zero probability. Second, a normalization step is
performed to ensure a valid distribution over Σ,
i.e, one whereP
y∈Σγ(p(y|y<t)) = 1 . When a
locally normalized sampling adaptor is applied to
a language model p, we can express the induced
distribution ¨pas
¨p(y) = (5)
γ(p(EOS|y))P
y∈Σγ(p(y|y))Q|y|
t=1γ(p(yt|y<t))
Q|y|
t=1P
y∈Σγ(p(y|y<t))
Despite their empirical success, locally normal-
ized sampling adaptors have several undesirable
theoretical properties. First, they can induce a lan-
guage model that is not tight , i.e., one where prob-
ability mass can be placed on infinite-length strings
(Welleck et al., 2020; Du et al., 2023). For instance,
this can occur if zero probability is placed on EOS
at every step t. Second, the normalization oper-
ation at each tcan produce unexpected behavior.
Specifically, because the denominator used to nor-
malize the local distribution at each tis dependent
on the probability mass assigned to other symbols,
a string assigned a higher score with the transform
function is not necessarily assigned a higher prob-
ability under the induced language model ¨p. This
can lead to less desirable strings being sampled
more frequently. See App. A for an example.We thus introduce globally normalized sam-
pling adaptors , a procedure to sample from a
language model that circumvents these issues.
Globally normalized sampling adaptors are also
defined with respect to a transform function γ, and
thus every existing locally normalized sampling
adaptor can be mapped to its globally normalized
counterpart. The transform function is similarly
applied at each time step to p(· |y<t). But without
the normalization step, we define the probability
of a string under the induced model epas
ep(y) =γ 
p(· |y)
(EOS)Q|y|
t=1γ 
p(· |y<t)
(yt)
Zγ(Σ).
(6)
where the normalizing constant Zγ(Σ)is defined
Zγ(Σ) = (7)
X
y∈Σ∗γ 
p(· |y)
(EOS)|y|Y
t=1γ 
p(· |y<t)
(yt).
The language model induced by a globally nor-
malized sampling adaptor is always tight. Under
the assumption that Zγ(Σ)is finite (Cotterell et al.,
2023), it is easy to see thatP
y∈Σ∗ep(y) = 1 . It is
also easy to see that
ep(y)∝γ 
p(· |y)
(EOS)|y|Y
t=1γ 
p(· |y<t)
(yt).
(8)
That is, the probability of a string under the adapted
model is determined only by the transform function
without additional renormalization, which makes it
convenient to reason about how a choice of trans-
form function can modify the properties of the in-
duced distribution ep.
We show in §5.2 how globally normalized
sampling adaptors can be used to control the
probability–quality trade-off. To generate strings
from a language model using a globally normal-
ized sampling adaptor, we use the Independent
Metropolis–Hastings Algorithm (IMHA; Metropo-
lis et al., 1953; Hastings, 1970; Wang, 2022).3The
IMHA is a Markov Chain Monte Carlo (MCMC)
method that simulates sampling from a target
distribution using a proposal distribution. We detail
how the IMHA’s acceptance–rejection protocol can
3Directly sampling from epis not possible because it
requires computing the constant Zγ(Σ). This sum over the
countably infinite set of finite strings is in practice unknown.
Using IMHA avoids this issue as it only requires the knowl-
edge of the target distribution up to a multiplicative constant .Algorithm 1 IMH Algorithm.
1.defIMHA (N,ep,¨p):
2.y∼¨p
3.Y ← {}
4.forn←1. . . N :
5. y′∼¨p
6. a(y,y′)←ep(y′) ¨p(y)
ep(y) ¨p(y′)
7. r∼U(0,1)
8. ifa > r :
9. Y ← Y ∪ { y′}
10. y←y′
11. else
12. Y ← Y ∪ { y}
13. return Y
be used to generate text with a globally normalized
sampling adaptor in Algorithm 1. The idea is that
by sampling sequentially from the proposal (i.e.,
the distribution induced by a locally normalized
sampling adaptor, ¨p) and appropriately accepting or
rejecting samples, the generated Markov chain (i.e.,
the sequence of samples) converges to a stationary
distribution equal to the target distribution (i.e., the
one induced by a globally normalized sampling
adaptor, ep). Convergence is often diagnosed by the
absence of autocorrelation between consecutive
samples (Deonovic and Smith, 2017). In the case
of categorical variables like strings, this can be
measured with Cramér’s V (Ialongo, 2016).
5 Theoretical Results
We are now ready to discuss the theoretical
contributions of this paper. In §5.1 we begin with
a formal argument that there exists a fundamental
trade-off between the average log-probability
under the prior and the average reward for corpora
sampled from an aligned language model. Then, in
§5.2, we show how sampling adaptors, by shifting
probability mass, can be used to choose any point
on this trade-off. In §5.3, we conclude the section
with a discussion of how this trade-off leads to an
emergence of Simpson’s paradox.
5.1 A Fundamental Trade-off
Letq+(y)be an aligned language model such
thatq+(y) =p(y|A=+).4Also, making use
of a Σ∗-valued random variable Ydistributed
according to q+, we define the pointwise joint
4This assumption is not strictly necessary, but allows us
to discuss the trade-off in terms of the true reward function r,
rather than q+(y)’s “secret” reward function rq+.entropy ofq+(y)as follows:
H(Y, A=+)def=−X
y∈Σ∗q+(y) logq+(y).(9)
Now, we can introduce the (N, ε)-typical set ofq+:
Tε
N(q+)def=n
Y ∈(Σ∗)N|
H(Y, A=+) +logq+(Y)
N< εo
,(10)
where Yis a corpus of strings and
logq+(Y) =PN
n=1logq+(y(n))for some
ε >0. In words, Tε
N(q+)is the set of corpora of
sizeN, i.e., bags of strings, each sampled from q+
with average information contentlogq+(Y)
Nclose to
the pointwise joint entropy H(Y, A=+).
This notion of typicality is useful because it can
be shown that a sampled corpus Y={y(n)}N
n=1
where y(n)∼q+falls in Tε
N(q+)with high
probability. Let I=−logq+(Y)be a random
variable that denotes the information content of
a string, and let V(I)denote its variance.5Then,
with Chebyshev’s inequality we can show that:
P(Y/∈Tε
N(q+))≤V(I)
Nε2=O1
N
. (11)
The full derivation can be found in App. B.1.
What Eq. (11) says is that if we observe a set
ofNstrings, the probability that the corpus lies
outside Tε
N(q+)→0asN→ ∞ . Equivalently,
this is to say that the sample entropy −logq+(Y)
N
collapses around the entropy H(Y, A=+)with
high probability when Nis large.
The above derivation is standard. However, what
is less standard is the application of Bayes’ rule
to show that strings in the typical set display a
fundamental trade-off.
Proposition 1 (Probability–quality trade-off) .
PC+logp(Y)
N+r(Y)
βN< ε
>1−δ(12)
where δ=O(1
N)andCdef= H( Y|A=
+)−logZ(+)is a constant, and we use the
shorthands logp(Y) =PN
n=1logp(y(n))and
r(Y) =PN
n=1r(y(n)).
Prop. 1 says that a corpus Yof size Nsam-
pled from q+will have its average log-probability
logp(Y)
Nand average rewardr(Y)
βNbound by a con-
stant with high probability . The implication of this
is that the two quantities will trade off linearly .
5V(I), in the few papers that treat it directly, is often
called the varentropy (Fradelizi et al., 2016).Proof Sketch. Prop. 1 follows relatively straight-
forwardly from Eq. (10) and Eq. (11), when
one observes from Eq. (4) that q+(y)∝
p(y) exp
1
βr(y)
, which implies log(q+) =
logp(y) +1
βr(y) + constant . Recall that cor-
pora in the typical set Tε
N(q+)have average in-
formation contentlogq+(y)
Nclose to the constant
pointwise joint entropy H(Y, A=+)(Eq. (10)).
That is, typical corpora by definition exhibit a
trade-off between average log-probability under
the priorlogp(y)
Nand average rewardr(y)
N. Then,
due to Chebyshev’s inequality in Eq. (11), we haveC+logp(Y)
N+r(Y)
βN< εwith probability at least
1−V(I)
Nε2
for all Nandε >0. When N≥V(I)
δε2
for some δ=O(1
N), the above holds with prob-
ability at least 1−δ, i.e., with high probability ,
and we arrive at the proposition. The full proof is
provided in App. B.2. ■
Strictly speaking, Prop. 1 describes a trade-off
between the average log prior probability and the
average reward. However, because the reward
function is often used to reflect human preferences
for various notions of quality, e.g., helpfulness
or concision (Perez et al., 2022; Ethayarajh
et al., 2022), we can interpret this result as a
probability–quality trade-off.
Assumptions. Prop. 1 relies on two key assump-
tions. First, that q+has finite entropy.6Second, that
the variance of information content of a string is
also finite, i.e., V(I)<+∞. We argue that neither
of these assumptions are limiting in practice be-
cause we show in Prop. 3 and Prop. 5 that they hold
for all Transformer-based language models, which
constitute the base architecture for most modern
models (Brown et al., 2020; Touvron et al., 2023).
A Tighter Bound. We remark that there exists a
tighter bound for the probability–quality trade-off
than the O(1
N)one in Eq. (11) for specific types of
language models. Specifically, we show in App. D
that for transformer and n-gram based language
models, the probability that sampled corpora land
in the typical set Tε
N(q+)and exhibit the trade-
off grows exponentially quickly, i.e., the bound
isO(exp(−cN))for some constant c∈ R>0.
5.2 Controlling the Trade-off
Ideally, we would like to choose how much proba-
bility we trade for quality when sampling corpora
6In general, this is not true. See App. C for an example.from an aligned language model. After all, depend-
ing on the context, it may be desirable to extract
higher-reward text (e.g., to improve alignment) or
lower-reward text (e.g., to combat overfitting of the
reward function; Azar et al., 2023; Gao et al., 2023;
Wang et al., 2024; He et al., 2024).
We can leverage sampling adaptors (§4) to exer-
cise this control. The intuition for this comes from
analyzing the effect of a specific sampling adaptor
on the prior string probability. That is, in App. E,
we show for particular sampling adaptor that
ep+(y)∝Fγ[p(y)]p(y) exp1
βr(y)
(13a)
=ep(y) exp1
βr(y)
, (13b)
whereep+is the induced distribution when apply-
ing a sampling adaptor to an aligned model p+and
Fγ[p(y)]refers to the aggregated series of trunca-
tion functions coming from the transform function
γthat rely only on the prior p(y). Eq. (13a) says
that in some special cases, the effect of applying
a globally normalized sampling adaptor to p+is
akin to applying it to the prior language model p
and then multiplying the result by the likelihood
that the generated string aligns with human prefer-
ences, i.e., the effects of the sampling adaptor can
be pushed to the prior.
Given the probability–quality trade-off, this
suggests that an appropriate choice of sampling
adaptor can be used to control the average log-
probability of sampled corpora, which then deter-
mines the average reward of generated text. For
example, we could use the temperature sampling
with a high temperature to produce lower probabil-
ity (and thus higher reward) strings. This intuition
proves to be useful and leads to an efficient way to
control the trade-off, as we demonstrate in §6.2.
5.3 The Emergence of Simpson’s Paradox
Following Lim et al. (2024), we now argue that the
trade-off described in Prop. 1—under appropriate
conditions—can lead to the emergence of Simp-
son’s paradox. Specifically, the paradox emerges
when the reward r(y)isa priori positively cor-
related with string likelihood under the prior lan-
guage model p(y). This is not always the case,
of course. However, we should expect it to be
true when reward scores reflect the quality of text
and the language model is well-calibrated. Thus,
if we consider samples y∼q+, we should expect
logp(y)to be positively correlated with r(y)by as-sumption . This correlation exists at the string level.
Simultaneously, in Prop. 1 we showed that an
anti-correlation arises from the trade-off between
the average log-probabilitylogp(y)
Nand average
rewardr(y)
βN. The positive correlation between
probability and quality at the level of strings,
reversed at the level of corpora, is precisely an
instance of Simpson’s paradox.
6 Experimental Setup
We conduct two experiments in the empirical
portion of this paper. First, in §6.1 we validate
the predictions of Prop. 1 with toy language and
reward models. Then, in §6.2 we demonstrate that
this trade-off exists in practice for open-sourced
RLHF-tuned models and that globally normalized
sampling adaptors can control where on this trade-
off the corpus of generated text will lie. We also
examine models aligned with Direct Preference Op-
timization (DPO; Rafailov et al., 2023) in App. F,
as RLHF and DPO have the same objective.
6.1 A Toy Experiment
The trade-off described in Prop. 1 fundamentally
arises as a consequence of typicality and the fact
thatp+(y)∝p(y) exp(1
βr(y)). We aim to demon-
strate these theoretical principles with an easily
reproducible toy experiment, where we model p+
andpas language models with support only over a
finite subset of Σ∗.
Modeling p+,pandr.LetD ⊂Σ∗be a finite set
of|D|= 1,000strings.7With this, let us construct
a toy aligned language model p+(y)by sampling a
distribution over Dfrom the Dirichlet distribution
parameterized by a |D|-sized vector where every
value is set to 0.1. We create the prior p(y)by
applying the softmax to a scaled and noised version
of this distribution. That is, we define p(y)def=
softmax 
p+(·)1
τ+ϵ)
(y)where ϵ∼U(−κ, κ)
andτ∈ R>0,κ∈ R>0are our hyperparameters.
We then define r(y)analogously to Eq. (4) with
r(y) = logp+(y)
p(y). The distributions of p+(y),p(y)
andr(y)over the domain Dcan be seen in App. G.
To induce Simpson’s paradox, we tune κandτ
such that they are positively correlated, shown in
Fig. 1 on the left.
Constructing Corpora with Causal Bootstrap-
ping. An important point about the trade-off
in Prop. 1 is that it occurs with high probability .
7We arbitrarily identify Dwith{1,2, . . .1000}.Figure 1: Illustration of the probability–quality trade-off with toy data, where quality is measured by the reward
function. (Left) “String”-level correlations between probability and reward, where strings are mimicked by arbitrary
objects. (Right) Corpus-level correlations between average log-probability and average reward. We include a best-fit
line for corpora in the typical set, i.e., those with sample entropy close to H(p+). In both figures, the log-probability
of each string or corpus is coloured according to high (dark) and low (light).
To illustrate this, we use causal bootstrapping
(Little and Badawy, 2020) to construct corpora
that are uniformly distributed across 10 bands of
average log-probability under the prior.8Then,
we compute and visualize the corpus probabilities,
i.e.,logp+(Y)where Ydef={y(n)}N
n=1. Due to
Prop. 1, we expect to see that corpora exhibiting
the trade-off have much higher probability than
those that do not. We examine 1,000 corpora
sampled this way, each with 100k samples.
6.2 The Trade-off in Practice
Here we demonstrate the existence of the
probability–quality trade-off with an open-sourced
aligned language model based on the Llama 2 fam-
ily (Touvron et al., 2023). Using locally normal-
ized sampling adaptors, we sample a corpus Yof
2,000 texts from an RLHF-tuned model q+. To-
wards this, we randomly choose 1,000 prompts us-
ing the helpfulness dataset from Perez et al. (2022)
and for each prompt, we produce two generations.
Then, for every string in this corpus, we obtain
its log-probability under the prior language model
logp(y)and its reward r(y). The prior and reward
models are the same as those used to train q+in an
RLHF scheme. We repeat this using five locally
normalized sampling adaptors at five temperatures,
totaling 25 sampling schemes and thus 50,000
(logp(y), r(y))pairs. To observe the trade-off, we
compute the Pearson and Spearman’s correlation
8This scheme allows us to observe low-probability corpora.between logp(y)andr(y)at the string level, and
betweenlogp(Y)
Nandr(Y)
Nat the corpus level.
Resampling Corpora with the IMHA. To
compute corpus-level correlations we require a
lot of data points of the average log-probability
and average reward. However, because sampling
multiple corpora is prohibitively expensive, we
use standard bootstrap resampling (Efron and
Tibshirani, 1994) to create multiple corpora
for each of the 25 sampling schemes. Given
a corpus of strings generated from q+with a
locally normalized sampling adaptor defined by
the transform function γ, we resample uniformly
with replacement Ntimes, accepting and rejecting
each as described in Algorithm 1. This gives us
a resampled corpus Y′. Then, we compute the
average log-likelihoodlogp(Y′)
Nand average reward
r(Y′)
N. We do this 2,000 times per sampling scheme,
giving us a total of 50,000
logp(Y)
N,r(Y′)
N
pairs,
which we then use to we compute the corpus-level
correlations. We set N= 200 ,000as preliminary
experiments showed that for N≥200,000
the IMHA converges, i.e., the autocorrelation
measured with Cramér’s V falls to <0.10.
Sampling adaptors. The five sampling schemes
we examine are: top- ksampling (Fan et al.,
2018) for k∈ { 30,50}, nucleus sampling
(Holtzman et al., 2020) for π∈ {0.90,0.95},
η-sampling (Hewitt et al., 2022) and locally-
typical sampling (Meister et al., 2023b). ForFigure 2: The probability–quality relationship, where quality is measured by the reward function. (Left) String-level
correlations between log-probability and quality. (Right) Corpus-level correlations between average log-probability
and average quality, with corpora created by different sampling adaptors. Higher intensity of the colours denote
higher temperatures used with the sampling adaptor.
each setting, we examine five temperatures τ∈
{0.5,0.75,1.0,1.25,1.5}. This gives us a total of
25 settings that cover various real-world use cases.
As a baseline, we also include ancestral sampling.
Models. We utilize the family of 7B reward,
RLHF-tuned and prior language models from
Rando and Tramèr (2024) based on Llama 2 7B
(Touvron et al., 2023). Specifically, we use the
baseline reward and RLHF-tuned models trained
on the helpfulness dataset from Perez et al. (2022).
7 Results
Our results confirm our theoretical findings in §5.
A Strong Anti-correlation. In both toy and
empirical settings, we observe at the corpus level a
strong linear anti-correlation between the average
log-probabilitylogp(Y)
Nand rewardr(Y)
N. In the
toy experiment, as shown in Fig. 1 corpora in
the typical set have average log-probabilities and
average rewards that exhibit a Pearson correlation
ofr=−0.92. And, importantly, these typical
corpora occur with significantly higher probability
than those that do not. For example, the median
log-probability difference between a corpus in andout of the typical set is 1084fold.9The results
in the empirical experiment with real language
models are generally consistent with the toy
experiment. We observe a Pearson correlation
between the average log-probability and average
reward of r=−0.93with relatively few outliers.
Globally Normalized Sampling Adaptors
Control the Trade-off. We observe in Fig. 2 that
corpora sampled with various globally normalized
sampling adaptors are centered at different points
on the trade-off and follow qualitatively expected
trends. For example, corpora sampled with
different temperatures have average log-probability
that follow the expected trend—high-temperature
corpora have lower average log-probability and
higher average reward. And, at τ= 1.0all
sampling adaptors produce corpora with higher
average log-probability and lower average reward
than ancestral sampling. These results are ex-
pected since lower temperature and the examined
sampling adaptors skew the sampling distribution
towards high probability strings. The behaviour
9−500,523vs.−500,718; the difference is somewhat
masked by the log-scale.when comparing sampling adaptors with different
degrees of truncation also follows our expectation,
e.g., corpora sampled with nucleus sampling for
π= 0.95have lower average reward than those
sampled with π= 0.90. These findings are in line
with our theoretical exposition in §5.2 and suggest
that we can use sampling adaptors to control the
average reward of sampled corpora.
Local vs. Global Normalization. In practice, we
find that the tradeoff can be controlled by directly
using locally normalized sampling adaptors, i.e.,
without applying the IMHA post-hoc to derive a
globally normalized sampling adaptor. First, we
see similar results when we repeat the experiment
in §6.2 without applying the IMHA’s acceptance-
rejection protocol. Second, for every locally nor-
malized sampling adaptor over 95% of the samples
are accepted by the IMHA. An acceptance rate
of100% implies that the proposal distribution (in
this case, the distribution induced by the locally
normalized sampling adaptor) is equal to the tar-
get distribution (Metropolis et al., 1953; Hastings,
1970; Deonovic and Smith, 2017; Wang, 2022).
Simpson’s Paradox. We observe in both toy
(Fig. 1) and empirical data (Fig. 2) the emergence
of Simpson’s paradox. At the string-level, we mea-
sure rank correlations of ρ= 0.43in the toy setting
ρ= 0.38in the empirical setting. In the latter case,
this positive correlation is probably explained by
the fact that the reward model is trained to model
preferences of helpfulness and the prior Llama 2
7B model has likely seen related texts in its training
data. In both settings, we simultaneously find an
anti-correlation at the corpus level between average
log-probability and average reward. These results
are consistent with our expectations in §5.3—the
reversal emerges because the trade-off arises out
of typicality, independently of the true relationship
between probability and quality at the string level.
8 Conclusion
Our work examines the relationship between prob-
ability and reward in sampling from RLHF-tuned
language models. We provide a formal argument
and empirical evidence of a trade-off between these
two quantities when generating text at scale. No-
tably, this trade-off exists as a consequence of typi-
cality, is independent of the relationship between
reward and probability at the string level, and ap-
plies to anyconditionally aligned language model,
not just those aligned with RLHF.Moreover, we have proposed globally nor-
malized sampling adaptors, and demonstrate
their utility for selecting how much likelihood
we exchange for reward. We also find that
locally normalized sampling adaptors are good
approximations of their globally normalized
counterparts in practice, and can be directly used
to control the trade-off. Altogether, these findings
present a new direction of research for improving
reward alignment or mitigating reward overfitting
in RLHF-tuned models, and the development of
sampling methods for conditional text generation.
Limitations
There are three main limitations to our work. First,
is that we only conduct empirical analysis for
English and Transformer-based language models.
Second, we don’t experiment over all sampling
adaptors, e.g., we did not consider Mirostat-
sampling (Basu et al., 2021) or contrastive search
decoding (Su et al., 2022) in our experiments.
These choices were made because the theory holds
independently of these factors, though further
work should consider other model architectures,
sampling adaptors and models that span a variety
of languages and domains. Finally, we have only
examined the probability–quality relationship un-
der the paradigm of RLHF (and equivalently, DPO,
as we show in App. F), but not other alignment
methods like ORPO (Hong et al., 2024), ODPO
(Amini et al., 2024a) or variational BoN (Amini
et al., 2024b). We leave those to future work.
Ethics Statement
We do not foresee any ethical implications.
Acknowledgments
This research is supported by the Singapore Min-
istry of Education Academic Research Fund Tier 1
(T1 251RES2216). Josef Valvoda is funded by the
Nordic Programme for Interdisciplinary Research
Grant 105178 and the Danish National Research
Foundation Grant no. DNRF169. Anej Svete is
supported by the ETH AI Center Doctoral Fellow-
ship.
References
Afra Amini, Tim Vieira, and Ryan Cotterell. 2024a.
Direct preference optimization with an offset. arXiv
preprint arXiv:2402.10571 .
Afra Amini, Tim Vieira, and Ryan Cotterell. 2024b.
Variational best-of-n alignment.Mohammad Gheshlaghi Azar, Mark Rowland, Bilal
Piot, Daniel Guo, Daniele Calandriello, Michal
Valko, and Rémi Munos. 2023. A general theoret-
ical paradigm to understand learning from human
preferences. arXiv preprint arXiv:2310.12036 .
Michael Baer. 2008. A simple count-
able infinite-entropy distribution.
Https://www.mbbaer.com/Hinf.pdf.
Rafael E. Banchs, L. F. D’Haro, and Haizhou Li. 2015.
Adequacy–fluency metrics: Evaluating MT in the
continuous space model framework. IEEE/ACM
Transactions on Audio, Speech, and Language Pro-
cessing , 23:472–482.
Sourya Basu, Govardana Sachitanandam Ramachan-
dran, Nitish Shirish Keskar, and Lav R. Varshney.
2021. Mirostat: A neural text decoding algorithm
that directly controls perplexity. In International
Conference on Learning Representations .
Ralph Allan Bradley and Milton E. Terry. 1952. Rank
analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika , 39:324.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-
)evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136–158, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. In
Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc.
Ryan Cotterell, Anej Svete, Clara Meister, Tianyu Liu,
and Li Du. 2023. Formal aspects of language model-
ing. arXiv preprint arXiv:2311.04329 .
Thomas M. Cover and Joy A. Thomas. 2006. Elements
of Information Theory (Wiley Series in Telecommuni-
cations and Signal Processing) . Wiley-Interscience,
USA.
Benjamin E. Deonovic and Brian J. Smith. 2017. Con-
vergence diagnostics for MCM draws of a categorical
variable. arXiv preprint arXiv:1706.04919 .Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In North American Chapter of the Associa-
tion for Computational Linguistics .
Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara
Meister, Jason Eisner, and Ryan Cotterell. 2023. A
measure-theoretic characterization of tight language
models. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 9744–9770, Toronto,
Canada. Association for Computational Linguistics.
Bradley Efron and R. J. Tibshirani. 1994. An Intro-
duction to the Bootstrap , 1st edition. Chapman and
Hall/CRC.
Kawin Ethayarajh, Yejin Choi, and Swabha
Swayamdipta. 2022. Understanding dataset
difficulty with V-usable information. In Proceedings
of the 39th International Conference on Machine
Learning , volume 162 of Proceedings of Machine
Learning Research , pages 5988–6008. PMLR.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 889–898, Melbourne, Australia. Association
for Computational Linguistics.
Matthieu Fradelizi, Mokshay Madiman, and Liyao
Wang. 2016. Optimal concentration of information
content for log-concave densities. In High Dimen-
sional Probability VII , pages 45–60, Cham. Springer
International Publishing.
Leo Gao, John Schulman, and Jacob Hilton. 2023. Scal-
ing laws for reward model overoptimization. In Pro-
ceedings of the 40th International Conference on
Machine Learning , volume 202 of Proceedings of
Machine Learning Research , pages 10835–10866.
PMLR.
Alex Graves. 2012. Sequence transduction with
recurrent neural networks. arXiv preprint
arXiv:1211.3711 .
W. K. Hastings. 1970. Monte Carlo sampling meth-
ods using Markov chains and their applications.
Biometrika , 57(1):97–109.
Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng
Zhang, Rui Wang, Shuming Shi, and Zhaopeng Tu.
2024. Improving machine translation with human
feedback: An exploration of quality estimation as a
reward model. arXiv preprint arXiv:2401.12873 .
John Hewitt, Christopher Manning, and Percy Liang.
2022. Truncation sampling as language model
desmoothing. In Findings of the Association for Com-
putational Linguistics: EMNLP 2022 , pages 3414–
3427, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text de-
generation. In International Conference on Learning
Representations .
Jiwoo Hong, Noah Lee, and James Thorne. 2024. Orpo:
Monolithic preference optimization without refer-
ence model. arXiv preprint arXiv:2403.07691 .
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P. Xing. 2017. Toward con-
trolled generation of text. In Proceedings of the
34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learning
Research , pages 1587–1596. PMLR.
Cristiano Ialongo. 2016. Understanding the effect size
and its measures. Biochem Med (Zagreb) .
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, Lélio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023. Mistral 7b. arXiv
preprint arXiv:2310.06825 .
Tomasz Korbak, Ethan Perez, and Christopher Buck-
ley. 2022. RL with KL penalties is better viewed
as Bayesian inference. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2022 ,
pages 1083–1091, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika
Bhalerao, Christopher L. Buckley, Jason Phang,
Samuel R. Bowman, and Ethan Perez. 2023. Pre-
training language models with human preferences.
InProceedings of the 40th International Conference
on Machine Learning , ICML’23. JMLR.org.
Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann,
Nitish Shirish Keskar, Shafiq Joty, Richard Socher,
and Nazneen Fatema Rajani. 2021. GeDi: Gener-
ative discriminator guided sequence generation. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 4929–4952, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Seongyun Lee, Sue Hyun Park, Seungone Kim, and
Minjoon Seo. 2024. Aligning to thousands of pref-
erences via system message generalization. arXiv
preprint arXiv:2405.17977 .
Jan Leike, David Krueger, Tom Everitt, Miljan Martic,
Vishal Maini, and Shane Legg. 2018. Scalable agent
alignment via reward modeling: a research direction.
arXiv preprint arXiv:1811.07871 .
Sergey Levine. 2018. Reinforcement learning and con-
trol as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909 .Zheng Wei Lim, Ekaterina Vylomova, Trevor Cohn,
and Charles Kemp. 2024. Simpson’s paradox and
the accuracy-fluency tradeoff in translation. arXiv
preprint arXiv:2402.12690 .
Max A. Little and Reham Badawy. 2020. Causal boot-
strapping. arXiv preprint arXiv:1910.09648 .
Alisa Liu, Maarten Sap, Ximing Lu, Swabha
Swayamdipta, Chandra Bhagavatula, Noah A. Smith,
and Yejin Choi. 2021. DExperts: Decoding-time con-
trolled text generation with experts and anti-experts.
InProceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
6691–6706, Online. Association for Computational
Linguistics.
Clara Meister, Tiago Pimentel, Luca Malagutti, Ethan
Wilcox, and Ryan Cotterell. 2023a. On the efficacy
of sampling adapters. In Proceedings of the 61st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1437–
1455, Toronto, Canada. Association for Computa-
tional Linguistics.
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan
Cotterell. 2023b. Locally typical sampling. Transac-
tions of the Association for Computational Linguis-
tics, 11:102–121.
Clara Meister, Gian Wiher, Tiago Pimentel, and Ryan
Cotterell. 2022. On the probability–quality paradox
in language generation. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , pages 36–45,
Dublin, Ireland. Association for Computational Lin-
guistics.
Nicholas Metropolis, Arianna W. Rosenbluth, Mar-
shall N. Rosenbluth, Augusta H. Teller, and Ed-
ward Teller. 1953. Equation of state calculations by
fast computing machines. The Journal of Chemical
Physics , 21(6):1087–1092.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul F Christiano, Jan Leike, and Ryan Lowe. 2022a.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems , volume 35, pages 27730–27744.
Curran Associates, Inc.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul F Christiano, Jan Leike, and Ryan Lowe. 2022b.
Training language models to follow instructions with
human feedback. In Advances in Neural InformationProcessing Systems , volume 35, pages 27730–27744.
Curran Associates, Inc.
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai,
Roman Ring, John Aslanides, Amelia Glaese, Nat
McAleese, and Geoffrey Irving. 2022. Red teaming
language models with language models. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 3419–3448,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Krishna Pillutla, Lang Liu, John Thickstun, Sean
Welleck, Swabha Swayamdipta, Rowan Zellers, Se-
woong Oh, Yejin Choi, and Zaid Harchaoui. 2023.
MAUVE scores for generative models: Theory and
practice. Journal of Machine Learning Research ,
24(356):1–92.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
Javier Rando and Florian Tramèr. 2024. Universal jail-
break backdoors from poisoned human feedback. In
The Twelfth International Conference on Learning
Representations .
Felix Stahlberg and Bill Byrne. 2019. On NMT search
errors and model errors: Cat got your tongue? In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3356–
3362, Hong Kong, China. Association for Computa-
tional Linguistics.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F. Christiano. 2020. Learn-
ing to summarize with human feedback. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 3008–3021. Curran Associates,
Inc.
Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-
peng Kong, and Nigel Collier. 2022. A contrastive
framework for neural text generation. In Advances
in Neural Information Processing Systems .
Elior Sulem, Omri Abend, and Ari Rappoport. 2020. Se-
mantic structural decomposition for neural machine
translation. In Proceedings of the Ninth Joint Confer-
ence on Lexical and Computational Semantics , pages
50–57, Barcelona, Spain (Online). Association for
Computational Linguistics.
Elke Teich, José Martínez Martínez, and Alina
Karakanta. 2020. Translation, Information Theory
and Cognition . Routledge.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. arXiv preprint arXiv:2307.09288 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan
Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu
Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao
Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao
Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen,
Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang,
Zuxuan Wu, and Yu-Gang Jiang. 2024. Secrets of
rlhf in large language models part ii: Reward model-
ing. arXiv preprint arXiv:2401.06080 .
Guanyang Wang. 2022. Exact convergence analysis
of the independent Metropolis-Hastings algorithms.
Bernoulli , 28(3):2012 – 2033.
Sean Welleck, Ilia Kulikov, Jaedeok Kim,
Richard Yuanzhe Pang, and Kyunghyun Cho.
2020. Consistency of a recurrent language model
with respect to incomplete decoding. In Proceedings
of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages
5553–5568, Online. Association for Computational
Linguistics.
Gian Wiher, Clara Meister, and Ryan Cotterell. 2022.
On decoding strategies for neural text generators.
Transactions of the Association for Computational
Linguistics , 10:997–1012.
Kevin Yang and Dan Klein. 2021. FUDGE: Controlled
text generation with future discriminators. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies . Associ-
ation for Computational Linguistics.
Yilin Yang, Liang Huang, and Mingbo Ma. 2018. Break-
ing the beam search curse: A study of (re-)scoringmethods and stopping criteria for neural machine
translation. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, pages 3054–3059, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,
and Dawei Song. 2023. A survey of controllable
text generation using transformer-based pre-trained
language models. ACM Computing Surveys , 56(3):1–
37.
Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and
Arvind Neelakantan. 2021. Trading off diversity and
quality in natural language generation. In Proceed-
ings of the Workshop on Human Evaluation of NLP
Systems (HumEval) , pages 25–33, Online. Associa-
tion for Computational Linguistics.
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2020. Fine-tuning lan-
guage models from human preferences. arXiv
preprint arXiv:1909.08593 .A Locally Normalized Sampling Adaptors
Despite their empirical success, the normalization performed in locally normalized sampling adaptors
can lead to strings that are scored higher by the transform function γto have lower probability under the
induced language model ¨p. This behaviour stems from the fact that the normalization is dependent on the
weights assigned to other symbols at a given time step, and thus leads to inconsistencies at the global level.
This is arguably undesirable since it makes it difficult to formally reason about how the transform function—
which embodies the core logic of the sampling adaptor—influences the properties of strings sampled from
¨p. To make this clear, we provide an example of this behaviour in top- ksampling (Fan et al., 2018).
Example 1. Consider an alphabet Σ ={a, b, c, EOS}. Let us define a language model10such that
p(y) =p(EOS|y)|y|Y
t=1p(yt|y<t) (14)
and
p(· |y<t) =

[0.4,0.1,0.1,0.4]ifyt−1=a
[0.1,0.4,0.2,0.3]ifyt−1=b
[0.5,0.5,0.0,0.0]ift= 1(15)
where the vector [. . .]denotes the probability assigned to a, b, c, EOS, in that order. Let us now consider
the probability of the strings aaandbb.
p(aaa) = 0 .5×0.4×0.4×0.4 = 0 .032 (16)
p(bbb) = 0 .5×0.4×0.4×0.3 = 0 .024 (17)
Let us now consider their probability when top- 2sampling is applied.
¨p(aaa) = 0 .5×0.5×0.5×0.5 = 0 .0625 (18)
¨p(bbb) = 0 .5×0.4
0.7×0.4
0.7×0.3
0.7= 0.06997 (19)
Because the transform function in top- kdoes not modify the symbol probability if the symbol is kept (see
App. E.1), Eq. (16) is precisely the score assigned to the strings by the transform function, and Eq. (18) is
the score after the normalization step. And we observe a reversal—the string “ aa” has a higher score
than “ bb”, but is later assigned a lower probability under the induced model ¨p.
B Supplementary proofs for §5
B.1 Proof of Eq. (11)
Proof.
P(Y/∈Tε
N(q+)) =P(H(Y, A=+) +logq+(Y)
N≥ε) (20a)
=P(H(Y, A=+)−−logq+(Y)
N≥ε) (20b)
≤V(−logq+(Y)
N)
ε2=V(−logq+(y))
Nε2=V(I)
Nε2(20c)
Eq. (20c) holds due to Chebyshev’s inequality. ■
10Note that this language model is not necessarily tight.B.2 Proof of the Probability–Quality trade-off
Proof. Consider the (N, ε)-typical set
Tε
N(q+)def=n
Y ∈(Σ∗)N|H(Y, A=+) +logq+(Y)
N< εo
(21)
By rewriting Eq. (4)
r(y)
β= logq+(y)
p(y)+ log Z(+)
as
logq+(y) =r(y)
β+ log p(y)−logZ(+),
and summing over all y∈ Y, we get
logq+(Y) =r(Y)
β+ log p(Y)−NlogZ(+), (22)
Substituting Eq. (22) into Eq. (12), we obtain
Tε
N(q+) =n
Y ∈(Σ∗)N|H(Y, A=+) +r(Y)
Nβ+logp(Y)
N−logZ(+)< εo
=n
Y ∈(Σ∗)N|C+logp(Y)
N+r(Y)
Nβ< εo
Due to Chebyshev’s inequality, P(Y/∈Tε
N(q+))≤V(I)
Nε2, we haveC+logp(Y)
N+r(Y)
Nβ< ε with
probability at least
1−V(I)
Nε2
for all Nandε >0. When N≥V(I)
δε2, the above holds with probability
at least 1−δ. ■
C Infinite-Entropy Language Models
A key assumption we have made in this paper is that all language models punder consideration have finite
entropy. In general, this is not true. To make this point clear, we give an example of a simple language
model whose entropy diverges.
Example 2 (A Tight LM with Infinite Entropy) .LetΣdef={a}and define for t= 1,2, . . .
p(a···a|{z}
ttimes)def=1
lg (t+ 1)−1
lg (t+ 2). (23)
Proposition 2. The language model pfrom Eq. (23) is tight and has infinite entropy.
Proof. The proof follows Baer (2008). We consider the language model:
p(a···a|{z}
ttimes)def=1
lg (t+ 1)−1
lg (t+ 2)(24)
p(a···a|{z}
ttimes)is positive over t= 1,2, . . . and sums to 1since it forms a telescoping sum with the only
remaining term1
lg(2)= 1. This proves that pis tight.
Furthermore, we can show that p’s entropy is ∞. Let us denote ptdef=p(a···a|{z}
ttimes), and begin by pointing
out several facts. First, the monotonicity and convexity of1
lgxis easily seen by noting that its first
derivative is −1
xlg2xlog 2(negative for x >1) and its second derivative islg(x+2
log 2)
x2lg3xlog(2)(positive for x >1).This will allow us to bound ptfrom below with p′
tdef=1
tlg2tlog 2, which is monotonically decreasing and
less than1
2fort≥3. Then, we point out that with basic calculus we can see that plgpis monotonically
decreasing with pforp <1
2. With these, we can say that p′
tlgp′
tis monotonically decreasing for t≥2
since p′
t<1
2for these t. We are now ready to lower bound H(p)with an expression equal to infinity,
thereby showing that H(p)is infinite:
H (p) =−∞X
t=1ptlgpt (25a)
=−p1lgp1−p2lgp2−∞X
t=3ptlgpt (25b)
=−p1lgp1−p2lgp2−∞X
t=31
lg (t+ 1)−1
lg (t+ 2)
lg1
lg (t+ 1)−1
lg (t+ 2)
(25c)
>1
log 2∞X
t=31
tlg2t(lgt+ lg lg2t+ lg log 2) (25d)
>1
log 2∞X
t=31
tlg2t(25e)
>1
log 2Z∞
31
tlgtdt (25f)
>lim
n→∞(lg 2)(lg lg n−lg lg 3) = ∞ (25g)
■
D A Tighter (Chernoff) Bound
In this section, we give a tighter concentration inequality than the (standard) one derived with Chebyshev’s
inequality. The inequality displayed in Eq. (11) is weak in the sense that the average right hand size is
O(1
N)—ideally, we desire a concentration inequality that is exponential, i.e., O(exp(−cN))for some
constant c∈ R>0. To prove such a tighter concentration inequality, we define a class of language models
that we term sub-exponential language models. We show that both classical n-gram language models as
well as modern Transformer-based language models are sub-exponential under our definition. We further
show that we can apply the Chernoff–Cramér method to argue that the sample entropy collapses around
the mean exponentially quickly.
Before we delve into our derivation, we highlight what makes a direct application of a standard
concentration bound, e.g., a Hoeffding bound, tricky. Consider a language model pwith support
everywhere on Σ∗. Furthermore, consider an enumeration {yn}∞
n=1ofΣ∗such that n > m implies
p(yn)≤p(ym). Observing the infinite sumP∞
n=1p(yn) = 1 is convergent, we must have that
p(yn)→0asn→ ∞ . It follows by the continuity of log, that−logp(yn)→ ∞ asn→ ∞ . A simpler
way of stating the above is that the random variable I(y) =−logp(y), distributed according to
P(I=ι) =X
y∈Σ∗p(y) 1{ι=−logp(y)}, (26)
is unbounded.
D.1 Prerequisites
We will now introduce several definitions and prove several results.
Definition 1 (Non-trivial Language Model) .We call a language model over Σnon-trivial if its support is
an infinite subset of Σ∗.
Definition 2 (Rényi Entropy) .Letpbe a language model over Σ. The Rényi entropy ofpis defined as
Hγ(p) =(
1
1−γlogP
y∈Σ∗p(y)γγ∈(0,1)
−P
y∈Σ∗p(y) logp(y)γ= 1(27)forγ∈(0,1]
Definition 3. A language model pisEOS-bounded if there exists csuch that p(EOS|y)> c > 0for all
y∈Σ∗.11In other words, p(y⊕EOS)≤(1−c)|y|for all y∈Σ∗.
Proposition 3. Letpbe an EOS-bounded language model. Then, Hγ(p)<+∞forγ∈(0,1].
Proof. We divide the proof into two cases.
Case 1: γ∈(0,1).Consider the following manipulation
X
y∈Σ∗p(y)γ=∞X
n=0X
y∈Σnp(y)γ(28a)
≤∞X
n=0(1−c)nγ(28b)
=∞X
n=0[(1−c)γ]n(28c)
=1
1−(1−c)γ<+∞ (28d)
The last inequality follows because c∈(0,1), and, thus, we have 0<(1−c)γ<1and, thus, the
geometric sum converges.
Case 2: α= 1.In the case of α= 1, Rényi entropy entropy turns into Shannon entropy. Because log(·)
is concave, we have
−X
y∈Σ∗p(y) logp(y) = logY
y∈Σ∗1
p(y)p(y)(29a)
= 2 logY
y∈Σ∗ 
1
p(y)1
2!p(y)
(29b)
≤2 log
X
y∈Σ∗p(y)
p(y)1
2
 (29c)
= 2 log
X
y∈Σ∗p(y)1
2
 (29d)
= H 1
2(p) (29e)
<+∞ (29f)
Eq. (29c) holds due to GM–AM inequalityQ
ixpi
i≤P
ipixi, whenP
ipi= 1andxi>0∀i.
■
Corollary 1. Letpbe a Transformer-based language model. Then, Hγ(p)<+∞forγ∈(0,1].
Proof. This follows from the proof in Du et al. (Prop. 4.7 and Thm. 5.9, 2023) that Transformer-based
LMs are EOS-bounded. ■
Corollary 2. Letpbe a tight n-gram language model. Then, Hγ(p)<+∞forγ∈(0,1].
Proof. Tight n-gram LMs are trivially are EOS-bounded. ■
11Strictly speaking, autoregressive language models are not always language models, i.e., valid probability distributions
overΣ∗whereP
y∈Σ∗p(y) = 1 . We highlight this difference in our exposition on locally vs. globally normalized sampling
adaptors in §4.Proposition 4. Letpbe an EOS-bounded language model. Then, over the interval (0,1],Hγ(p)is
monotonically decreasing in γ. Moreover, if pis a non-trivial language model, then Hγ(p)isstrictly
monotonically decreasing in γ.
Proof. Consider the following derivation
dHγ(p)
dγ=1
(1−γ)2logX
y∈Σ∗p(y)γ+1
1−γX
y∈Σ∗p(y)γlogp(y)P
y′∈Σ∗p(y′)γ(30a)
=1
(1−γ)2X
y∈Σ∗z(y)
logX
y′∈Σ∗p(y′)γ
+1
(1−γ)X
y∈Σ∗z(y) logp(y) (30b)
=1
(1−γ)2X
y∈Σ∗z(y)
logX
y′∈Σ∗p(y′)γ
+1
(1−γ)2X
y∈Σ∗z(y) logp(y)1−γ(30c)
=1
(1−γ)2X
y∈Σ∗z(y)
logX
y′∈Σ∗p(y′)γ+ log p(y)1−γ
 (30d)
=1
(1−γ)2X
y∈Σ∗z(y) 
−logp(y)γ
P
y′∈Σ∗p(y′)γ+ log p(y)!
(30e)
=1
(1−γ)2X
y∈Σ∗z(y) logp(y)
z(y)(30f)
=−1
(1−γ)2KL(z∥p)≤0, (30g)
where z(y)def=p(y)γP
y∈Σ∗p(y)γ. Because the derivative of Hγ(p)with regard to γis≤0 on the interval (0,1],
Hγ(p)is monotonically decreasing in γ. Moreover, when pis non-trivial, which implies pis not uniform
nor a point mass12, we have z̸=p,∀γ∈(0,1). ThusdHγ(p)
dγ=−1
(1−γ)2KL(z∥p)<0, i.e., Hγ(p)
is strictly monotonically decreasing on (0,1). ■
12i.e., the size of the support of pis 1.Proposition 5. Letpbe an EOS-bounded language model. Then, V(I)is finite.
Proof. We show that V(I)is bounded for EOS-bounded language models. Let Mdef=
P
y∈Σ∗p(y)1
2, z(y)def=p(y)1
2
M. Note that M= exp(1
2H1
2(p))<∞due to Prop. 3. Then, we have
V(I) =X
y∈Σ∗p(y)
log1
p(y)2
−H(p)2(31a)
≤
X
y∈Σ∗p(y)1
2log1
p(y)
2
−H(p)2(31b)
=
2X
y∈Σ∗p(y)1
2log1
p(y)1
2
2
−H(p)2(31c)
=
2X
y∈Σ∗Mz(y) log1
Mz(y)
2
−H(p)2(31d)
=
−2MlogM+ 2X
y∈Σ∗Mz(y) log1
z(y)
2
−H(p)2(31e)
= (−2MlogM+ 2MH1(z))2−H(p)2(31f)
≤ 
|2MlogM|+|2MH1/2(z)|2−H(p)2(monotonicity of Rényi Entropy) (31g)
=
2MlogM+ 4Mlog
X
y∈Σ∗z(y)1/2

2
−H(p)2(31h)
=
2MlogM+ 4M3
4H1/4(p)−1
2logM2
−H(p)2(31i)
<+∞ (31j)
■
Definition 4 (Rényi Gap) .Letpbe a language model and let γ∈(0,1]. The Rényi gap is defined as
∆γ(p) = H γ(p)−H(p) (32)
Corollary 3. Letpbe a language model and let α∈(0,1]. Then, the Rényi gap ∆γ(p)is non-negative.
Proof. This follows from Prop. 4. ■
Lemma 1. Letpbe a non-trivial, EOS-bounded language model. Then, for any ε >0, there exists an
γ∈(0,1)such that the Rényi gap 0<∆γ < ε .
Proof. This follows from the ∆γbeing a continuous monotonically decreasing function in γand∆γ= 0
when γ= 1. ■D.2 A Tighter Concentration Bound
We now introduce a sharper version of the AEP for EOS-bounded language models. As shown in App. D.1,
this includes Transformer-based language models, which constitute the base architecture for most modern
models (Brown et al., 2020; Touvron et al., 2023). The theorem is stated below.
Theorem 2. Letpbe an EOS-bounded, non-trivial language model. Then, there exists a function s(ε)>0
such that, for any ε >0, we have
P 1
NNX
n=1In−H(p)≥ε!
≤2 exp(−s(ε)N) (33)
with
s(ε)def=−t(ε)(∆1−t(ε)(p)−ε) (34)
Proof. To prove the result, we apply a Chernoff bound. This is a one-sided bound and the other will
follow by symmetry.
P 
1
NNX
n=1In−H(p)≥ε!
≤inf
t>0exp(−tε)ENY
n=1expt
N(In−H(p))
(35a)
= inf
t>0exp(−tε) exp(−tH(p))ENY
n=1expt
NIn
(35b)
= inf
t>0exp(−tε) exp(−tH(p))NY
n=1Eexpt
NIn
(35c)
= inf
t>0exp(−tε) exp(−tH(p))NY
n=1X
y∈Σ∗p(y) exp
−t
Nlogp(y)
(35d)
= inf
t>0exp(−tε) exp(−tH(p))NY
n=1X
y∈Σ∗p(y)1−t
N (35e)
= inf
t>0exp(−tε) exp(−tH(p))NY
n=1expt
NH1−t
N(p)
(35f)
= inf
t>0exp(−tε) exp(−tH(p)) exp
tH1−t
N(p)
(35g)
= inf
t>0exp(−tε) exp( t(H1−t
N(p)−H(p))) (35h)
= inf
t>0exp(−tε) exp( t(∆1−t
N(p))) (35i)
= inf
t>0exp
t(∆1−t
N(p)−ε)
(35j)
= inf
t′>0exp 
Nt′(∆1−t′(p)−ε)
(35k)
Now, by Lemma 1, for any ϵ >0we can find a 0< t(ε)<1such that ∆1−t(ε)(p)−ε <0. Thus, we have
P 
1
NNX
n=1In−H(p)≥ε!
≤exp(Nt(ε)(∆1−t(ε)(p)−ε)). (36)
Similarly, we have:
P 
1
NNX
n=1In−H(p)≤ −ε!
≤exp(Nt(ε)(−∆1−t(ε)(p)−ε))≤exp(Nt(ε)(∆1−t(ε)(p)−ε))(37)And, finally, we get:
P 1
NNX
n=1In−H(p)≥ε!
≤P 
1
NNX
n=1In−H(p)≥ε!
+P 
1
NNX
n=1In−H(p)≤ −ε!
≤2 exp( Nt(ε)(∆1−t(ε)(p)−ε)). (38)
Substituting in s(ε) =−t(ε)(∆1−t(ε)(p)−ε)>0, we arrive at
P 1
NNX
n=1In−H(p)≥ε!
≤2 exp(−s(ε)N), (39)
which tends to 0exponentially quickly as N→ ∞ . Note that 2 exp(−s(ε)N)isO(exp(−cN))for
c=s(ε), which proves the result. ■
In words, with respect to Transformer-based language models, Theorem 2 says that if we have a model
pand randomly sample Nstrings Y ∼p, when we average their surprisal values we approach the entropy
ofpexponentially quickly. One caveat is that the constant in the exponential is not a universal constant,
i.e., it depends on ε. This is less desirable, of course, but it is an improvement over the O(1
N)rate given
by an application of the standard AEP. We leave finding a universal constant for EOS-bounded language
models to future work.
E Sampling Adaptors and String Probability
In this section, we provide, by means of a simple example, an intuition of how an appropriate choice
of globally normalized sampling adaptor can be used to control generated strings’ average probability
under the prior p(y). Inspired by Meister et al. (2023a), we note that most γcan be formulated as the
composition of truncation and scaling functions. The truncation function C: Σ∗→ P(Σ)is a function
used to find the set of symbols that meets specified criteria given the prior context, so that symbols
deemed likely to lead to undesirable text can have their probability reassigned to other symbols, e.g.,
to only keep the top- ksymbols. The scaling function κ: R>0|Σ|→ R>0|Σ|is a simple scaling of
the symbol probability, e.g., to the power of1
τfor some temperature parameter τ∈ R>0. With these
definitions we can express a transform function γas
γ(p(· |y<t))(y) =κ(p(y|y<t)) 1{y∈C(p(· |y<t))}. (40)
That is, given a symbol distribution p(· |y<t), we apply the scaling function to scale symbol probabilities
as needed and then remove symbols according to the truncation function to arrive at the output
unnormalized distribution. For instance, we can express the transform function in nucleus sampling
(Holtzman et al., 2020) with:
κnucleus 
(p(y|y<t)
=I 
p(y|y<t)
(41a)
Cnucleus 
(p(y|y<t)
= argmin
Σ′⊆Σ|Σ′| s.t.X
y∈Σ′p(y|y<t)≥π (41b)
where Idenotes the identity function and π∈ R>0is a hyperparameter. See App. E.1 for more examples.
Letpbe a language model and p+its aligned counterpart. We consider the probability of a string under
the induced distribution ep+when a globally normalized sampling adaptor is used with an aligned model
p+. Since our goal is simply to provide an intuition behind what might happen by analyzing the effects of
the global sampling adaptor, we make the following simplifying assumptions. First, we assume that the
alignment does not modify the truncation function C:C(p(· |y)) =C(p+(· |y))for all y∈Σ∗. Second,we assume that κ(·) =I(·). With this, we can derive
ep+(y)∝γ 
p+(· |y)
(EOS)|y|Y
t=1γ 
p+(· |y<t)
(yt)
=κ(p+(EOS|y)) 1{EOS∈C(p+(· |y))}|y|Y
t=1κ(p+(yt|y<t)) 1{yt∈C(p+(· |y<t))}(42a)
=p+(EOS|y) 1{EOS∈C(p+(· |y))}|y|Y
t=1p+(yt|y<t) 1{yt∈C(p+(· |y<t))} (42b)
=p+(EOS|y) 1{EOS∈C(p+(· |y))}|y|Y
t=1p+(yt|y<t) 1{yt∈C(p+(· |y<t))} (42c)
=p+(EOS|y) 1{EOS∈C(p(· |y))}|y|Y
t=1p+(yt|y<t) 1{yt∈C(p(· |y<t))} (42d)
= 1{EOS∈C(p(· |y))}|y|Y
t=11{yt∈C(p(· |y<t))}p+(EOS|y)|y|Y
t=1p(yt|y<t) (42e)
= 1{EOS∈C(p(· |y))}|y|Y
t=11{yt∈C(p(· |y<t))}p+(y) (42f)
∝ 1{EOS∈C(p(· |y))}|y|Y
t=11{yt∈C(p(· |y<t))}
| {z }
Fγ[p(y)]p(y) exp1
βr(y)
(42g)
=Fγ[p(y)]p(y) exp1
βr(y)
(42h)
(42i)
We see that, in this simplified setting, the resulting probability of a string under the adapted and aligned
modelep+equals the prior probability scaled by the reward function and by the truncation factors. Since
different sampling adaptors affect this relationship differently, this simplified example suggests that an
appropriate choice of globally normalized sampling adaptor can be used to effectively select strings based
on their probability under the prior. For example, using the transform function from top- ksampling
will lead to the generation of corpora with higher average probability under the prior,13and thus, by the
anti-correlation derived in our paper, with lower average reward.
E.1 Examples of Sampling Adaptors
We note that these largely correspond to the examples in Meister et al. (2023a).
Example 3. We recover ancestral sampling when κ(·) =I(·)andC(p(· |y<t)) =Σ.
Example 4. We recover temperature sampling when κ(p(yt|y<t))∝p(yt|y<t)1
τandC(p(· |y<t)) =
Σ.
Example 5. We recover top-ksampling (Fan et al., 2018) when κ(·) =I(·)and
C(p(· |y<t)) = argmax
Σ′⊆ΣX
y∈Σ′p(y|y<t) s.t.|Σ′|=k (43)
i.e., the set of top- kmost probable symbols.
13Fork <|Σ|.Example 6. We recover locally typical sampling (Meister et al., 2023b) when when κ(·) =I(·)and
C(p(· |y<t)) = argmin
Σ′⊆ΣX
y∈Σ′H(p(· |y<t)) + log p(y|y<t) s.t.X
y∈Σ′p(y|y<t)≥π (44)
i.e., the set of items with log-probability closest to the symbol-level entropy that collectively have probabil-
ity mass ≥π.
Example 7. We recover η-sampling (Hewitt et al., 2022) when κ(·) =I(·)and
C(p(· |y<t)) ={y∈Σ|p(y|y<t)> η} (45)
that is, the set of symbols with probability greater than η, where η= min( ϵ,√ϵexp(−H(p(· |y<t))))) .F The Probability–Quality Trade-off in DPO-aligned Language Models
The probability–quality trade-off also applies to models aligned with direct preference optimization (DPO;
Rafailov et al., 2023). Though an explicit reward function is not needed to train a language model with
DPO, the training scheme maximises the same backward KL divergence objective as RLHF (Eq. (3);
Korbak et al., 2023; Rafailov et al., 2023; Azar et al., 2023). We should thus expect that Prop. 1 applies to
these models and observe the trade-off when we construct a reward function rq+using the prior pand
aligned model q+as in Eq. (4). We employ the same setup as in §6.2.
Models. We use the 7B DPO-aligned and prior language models from Lee et al. (2024), both of
which are based on Mistral 7B v0.2 (Jiang et al., 2023). The DPO-aligned model is fine-tuned on the
Multifaceted Collection (Lee et al., 2024), a dataset with 192k samples capturing preferences of style
(e.g., clarity, tone), informativeness, and harmlessness, among others. We construct the “secret” reward
function as rq+(y) =q+(y)
p(y), omitting the constant term.
Results. We observe results identical to the setting with RLHF-tuned models. Specifically, we observe
a strong anti-correlation (Pearson correlation of r=−0.97), trade-off control using sampling adaptors,
and the emergence of Simpson’s paradox. These are expected since RLHF and DPO have the same
minimization objective, thus supporting our formal arguments in §5.
Figure 3: The probability–quality relationship in DPO-tuned models, where quality is measured by the secret reward
function. (Left) String-level correlations between log-probability and quality. (Right) Corpus-level correlations
between average log-probability and average quality, with corpora created by different sampling adaptors. Higher
intensity of the colours denote higher temperatures used with the sampling adaptor.G Toy Experiment Distributions
Figure 4: Toy models of p+(x),p(x)andr(x)analogous to the distributions over strings.