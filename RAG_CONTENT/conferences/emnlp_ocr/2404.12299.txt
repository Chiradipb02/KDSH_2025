Simultaneous Interpretation Corpus Construction
by Large Language Models in Distant Language Pair
Yusuke Sakai∗, Mana Makinae∗, Hidetaka Kamigaito, Taro Watanabe
Nara Institute of Science and Technology
{sakai.yusuke.sr9, makinae.mana.mh2, kamigaito.h, taro}@is.naist.jp
Abstract
In Simultaneous Machine Translation (SiMT)
systems, training with a simultaneous interpre-
tation (SI) corpus is an effective method for
achieving high-quality yet low-latency systems.
However, it is very challenging to curate such
a corpus due to limitations in the abilities of
annotators, and hence, existing SI corpora are
limited. Therefore, we propose a method to
convert existing speech translation corpora into
interpretation-style data, maintaining the origi-
nal word order and preserving the entire source
content using Large Language Models (LLM-
SI-Corpus). We demonstrate that fine-tuning
SiMT models in text-to-text and speech-to-text
settings with the LLM-SI-Corpus reduces laten-
cies while maintaining the same level of qual-
ity as the models trained with offline datasets.
The LLM-SI-Corpus is available at https://
github.com/yusuke1997/LLM-SI-Corpus .
1 Introduction
Simultaneous machine translation (SiMT)1(Luong
and Manning, 2015; Gu et al., 2017; Ma et al.,
2019; Arivazhagan et al., 2019) is a technique that
translates input in real-time by incrementally pro-
cessing partial segments rather than awaiting the
whole sentence completion. While offline machine
translation (MT) works without time restrictions,
SiMT needs to start translating at certain points due
to time limits, therefore balancing its latency and
quality is crucial in this task. This becomes espe-
cially challenging with language pairs of drastically
different word orders such as English and Japanese
(SVO vs. SOV) (He et al., 2015; Chen et al., 2021;
Deng et al., 2023). In the case of grammatically
distant language pairs, one strategy involves main-
taining the source language word order as much as
possible to keep up with its original inputs to mini-
mize its latency while maintaining its quality (Cai
*These authors contributed equally to this work.
1Also, we called Simultaneous Speech Translation. We
simplify the notation to SiMT in this paper for brevity.
Transcription Translation 
(OFFLINE) 
SI Corpus 
TED Talks 
NAIST-SIC- 
Aligned-ST 
LLM-SI-Corpus 
(Ours) NAIST-CWMT 
(test only) SI Corpus 
 SI Corpus 
Figure 1: The corpora used in this study, each created
from the same TED Talks data. TED Talks are accom-
panied by English-Japanese offline MT data. NAIST-
SIC-Aligned-ST (Ko et al., 2023) is an SI dataset cre-
ated by transcribing audio data of these talks by hu-
man interpreters. NAIST English-to-Japanese Chunk-
wise Monotonic Translation Evaluation Dataset 2024
(NAIST-CWMT) (Fukuda et al., 2024) is manually cre-
ated based on offline MT data from TED Talks, follow-
ing the CWMT guideline (Okamura and Yamada, 2023),
and used only for testing purposes. Our LLM-SI-Corpus
was created by LLMs based on the CWMT guideline
and comprises training, development, and test sets.
et al., 2020; Han et al., 2021; Guo et al., 2023). To
address the balance between quality and latency,
the best way to learn this interpretation strategy for
SiMT systems is to utilize simultaneous interpreta-
tion (SI) data to train the model (Ko et al., 2023).
While several SI datasets have been proposed for
English and Japanese, they remain relatively lim-
ited in size compared to MT corpora. Furthermore,
acquiring this data is costly and resource-intensive,
making it impractical to scale it through manual
dataset construction, resulting in a limited num-
ber of data. Therefore, current SiMT efforts focus
on improving model performance and enhancing
SI capabilities through data augmentation using a
small amount of SI data.
1arXiv:2404.12299v1  [cs.CL]  18 Apr 2024Since such SI corpora are transcriptions of actual
SI, there exist various transcription-related noises,
e.g., fillers, and, therefore, it is still questionable
whether they are optimal for training SiMT systems.
Due to the specialized nature of SI, the quality of
translation differs among interpreters due to the
differences in the skills of individual interpreters
and the varieties of their experience. Therefore,
the quality of existing SI corpora is not consistent
and includes SI techniques such as omissions and
repetitions, making them not entirely faithful to the
original speech and not ideal for training SiMT.
As a guideline of SI, Okamura and Yamada
(2023) proposed Chunk-Wise Monotonic Trans-
lation (CWMT) for English to Japanese SI. This
guideline attempts to reduce latency by dividing
speech into chunks and translating them by hu-
mans sequentially from the beginning. Fukuda
et al. (2024) manually created test data for SiMT
in English to Japanese translation, confirming its
higher fluency and validity compared to existing SI
corpora. Their data construction guideline could
potentially provide faithful translations with low
latency and without omissions compared to exist-
ing SI corpora, but simply concatenating translated
chunks at their boundaries results in unnatural trans-
lations. Furthermore, requiring human labor is an
obstacle to scaling up the dataset size.
To solve these problems, we propose a method
to convert existing speech translation (ST) corpora
into SI-style data, maintaining the original word or-
der and preserving the entire source content using
Large Language Models (LLMs) as shown in Fig-
ure 1. The LLM-SI-Corpus created by our method
produces sentences that are closer to ideal SI com-
pared to existing ST and SiMT corpora by faith-
fully following the CWMT guideline. These sen-
tences feature reduced word order swapping, mini-
mal omissions, and are more natural. We demon-
strate that training SiMT models with the LLM-
SI-Corpus text-to-text and speech-to-text settings
improves translation quality in terms of semantics
compared to existing SI corpora. Furthermore, the
model trained with LLM-SI-Corpus reduces laten-
cies while maintaining the same level of quality as
models trained with offline datasets.
To summarize, our contributions are as follows:
•We proposed a method for automatically con-
structing a training dataset for SiMT systems
using LLMs focused on CWMT.
•We constructed the LLM-SI-Corpus, a large-scale training dataset for SiMT.
•We confirmed that the LLM-SI-Corpus, fol-
lowing CWMT, is effective for improving
both quality and latency in SiMT systems.
2 Background and Related Work
2.1 Simultaneous Machine Translation
In the SiMT task, the model processes partial
source sentences of length Jto incrementally gen-
erate partial target sentences of length I, guided
by its policy. Various policies have been proposed,
primarily categorized as fixed and adaptive. Fixed
policies (Dalvi et al., 2018; Ma et al., 2019; El-
bayad et al., 2020; Zhang and Feng, 2021) de-
cide READ/WRITE operations based on prede-
fined rules, such as the wait- kpolicy (Ma et al.,
2019), which reads ksource tokens initially and
then alternates between writing and reading one
token. Conversely, adaptive policies (Zheng et al.,
2020; Liu et al., 2020; Papi et al., 2023a,b) predict
READ/WRITE operations based on the current
source and target prefix, achieving a better balance
between latency and translation quality.
2.2 SI Corpora
Most SI corpora are constructed from real-time hu-
man interpretation. In English to Japanese, sev-
eral SI corpora are constructed (Toyama et al.,
2004; Shimizu et al., 2014; Doi et al., 2021).
Doi et al. (2021) developed a larger-scale SI cor-
pus (NAIST-SIC) supporting both English to/from
Japanese2. However, in NAIST-SIC, most data
lack sentence alignment, rendering them unsuit-
able for model training. To overcome this limita-
tion, NAIST-SIC aligned text-to-text by Zhao et al.
(2024) (NAIST-SIC-Aligned), speech-to-text by
Ko et al. (2023) (NAIST-SIC-Aligned-ST), hence it
becomes a large-scale parallel English-Japanese SI
corpus. Fukuda et al. (2024) manually constructed
a test dataset from NAIST-SIC-Aligned-ST based
on chunk-wise monotonic translation strategy (de-
scribed in Section 2.3). For the other language
pairs, Pan (2019); Zhang et al. (2021) (English-
Chinese), Kunz et al. (2021); Zhao et al. (2021);
Machá ˇcek et al. (2021) (English-German), Paulik
and Waibel (2009); Bernardini et al. (2016); Wang
et al. (2021); Przybyl et al. (2022) (the other lan-
guage pairs include English) have been established.
2They provide only a part of English-to-Japanese data.
2However, SI corpus construction requires con-
siderable time, money, and effort, resulting in a
small corpus size. To address this challenge, He
et al. (2015) attempted to mechanically reorder
words in an MT corpus to convert it to an SI cor-
pus by defining syntactic transformation rules for
Japanese-to-English translation. However, spoken
language poses challenges for syntactic parsing,
and the rule-based approach tends to decrease flu-
ency, making it difficult to apply the method to ST
corpora.
2.3 Chunk-Wise Monotonic Translation
Chunk-wise monotonic translation (CWMT) and
its variants are strategies employed by simultane-
ous interpreters, particularly in distant language
pairs such as English and Japanese (Mizuno, 2016;
Okamura and Yamada, 2023; Fukuda et al., 2024).
This guideline addresses the grammatical dispari-
ties between the two languages, as preserving the
exact word order from the source language to the
target language can result in unnatural translations.
The necessity for this guideline arises from the
need to strike a balance between translation latency
and quality. Interpreters translating from English to
Japanese prioritize maintaining the sequential order
of information chunks from the source languages
as much as possible. In practice, interpreters di-
vide sentences into manageable and meaningful
chunks of information and translate them sequen-
tially, ensuring that the order of chunks is main-
tained throughout the translation process. Okamura
and Yamada (2023) defines these chunk boundaries
and Fukuda et al. (2024) defines its chunking work-
flow. The details of the guideline and workflow are
described in Appendix A.
2.4 Style differences among SI, Offline
Translation, and CWMT
There are significant style gaps among SI, offline
translation, and CWMT as described in Fukuda
et al. (2024); Ko et al. (2023). The examples are
shown in Appendix B. The findings include:
•The SI translates the first half of the input ear-
lier than the latter half, albeit with some un-
naturalness and omission, whereas the offline
translation preserves naturalness in Japanese
through long-distance reordering from the in-
put English (See Table 4 in Appendix B).
•The offline translation and CWMT both in-
clude all content words from the source; how-ever, their distinction lies in the order. In
offline translation, long-distance reordering
occurs to preserve naturalness, whereas, in
CWMT, the order of source language chunks
is maintained with some unnaturalness (See
Table 5 in Appendix B).
From this observation, both SI and CWMT pri-
oritize aligning source inputs as closely as possible,
whereas offline allows for long-distance reorder-
ing. The significant difference in word order be-
tween English and Japanese poses a substantial
challenge in SI, as highlighted in a prior study
(Mizuno, 2016). Under the real SI scenario, in-
terpreters prioritize delivering interpretation simul-
taneously to convey content promptly and preserve
their working memory, which may involve some
omission and summarization. Consequently, our
focus in this study lies in leveraging SI interpreta-
tion guidelines to train a SiMT model, aiming to
enhance its simultaneity by prioritizing the preser-
vation of word order in the source speech as much
as possible. The problem in CWMT lies in their ap-
proach to maintaining fluency, thus it is challenging
to do automatically and it takes a high cost when
annotating manually.
3 SI-Corpus Construction with LLMs
To address the issues of the current SI corpus
creation, we utilize LLMs, known for their high
translation performance and the ability to perform
purpose-specific translations based on given in-
structions (Moslem et al., 2023; Zheng et al., 2024).
For our purpose, we follow the instructions for hu-
mans in creating CWMT to automatically convert
existing ASR texts into SI corpora by LLMs.
3.1 Prompt for Creating CWMT with LLMs
CWMT has three processes as described in Sec-
tion 2.3: chunking, translation of each chunk, and
concatenating the translated chunks into sentences.
Our prompt is based on the CWMT guideline by
Okamura and Yamada (2023), and we make it easy
to understand for LLMs as described in Figure 2.
For chunking, we designed it to split around
clauses, phrases, relativizers, and conjunctions. We
dare to leave ambiguity in the instructions, mak-
ing them more likely to be followed. Next, LLMs
translate each chunk while maintaining fluency and
adequacy. Finally, LLMs generate CWMT by con-
necting chunks using only demonstrative pronouns,
conjunctions, punctuation, and sentence splitting
3You are a skilled simultaneous interpreter designed to output 
JSON. Please save the results in 
{'chunk_pairs: List[Tuple[English, Japanese]], 'output': Japanese}" System 
Instructions: 
First, split the following text into minimal chunks around clauses,  
phrases, relativizers, and conjunctions. 
Next, translate it into Japanese chunk-by-chunk fluently and  
adequately. 
Finally, while ensuring you do not reference subsequent chunks,  
connect each chunk using demonstrative pronoun, conjunctions,  
punctuation, and sentence splitting. 
-------------------- 
Input: 
{sentence }User 
And that’s the kind of thing the 
investment in the young that makes 
us great allows us to contribute 
{"chunk_pairs": [ 
   ["And that’s the kind of thing ", " そしてそれがその種のものである "],
   ["the investment in the young ", " 若者への投資が "],
   ["that makes us great ", " 私たちを素晴らしいものにする "],
   ["allows us to contribute ", " 私たちが貢献できるようにする "],],
  "output": " そしてそれがその種のものである、若者への投資が、私たちを
素晴らしいものにし、私たちが貢献できるようにする。 "}Output 
Step1: Chunking source sentence: 
And that’s the kind of thing / the investment in the young / that makes  
us great / allows us to contribute 
Step2: Chunk-by-Chunk translation: 
そしてそれがその種のものである  / 若者への投資が  / 私たちを素晴らしいもの
にする  / 私たちが貢献できるようにする 
Step3: Connecting each chunk naturally: 
そしてそれがその種のものである、若者への投資が、私たちを素晴らしいものに
し、私たちが貢献できるようにする。 Prompt Template 
Example 
Sentence: Figure 2: The prompt template used for constructing the
LLM-SI-Corpus. It is based on the CWMT workflow.
This prompt can be divided into three color-coded steps.
while ensuring not refer to subsequent chunks to
preserve monotonicity. These processes are sum-
marized in a single prompt3. To ensure that these
operations are performed according to the instruc-
tion without shortcuts, outputs are formatted in
JSON4to obtain results at each step at once5.
Note that there is no guarantee that LLMs will
perform these operations correctly; however, the
goal of the dataset construction is to improve the
quality and latency of the SiMT system, so it is suf-
ficient if the overall results predominantly include
CWMT-like sentences6.
3In the pilot study, we found similar results when we input
data for each process separately as a pipeline or all at once
into the LLMs. Thus, to address the cost issue, we chose to
input all data at once as the prompt.
4https://platform.openai.com/docs/guides/
text-generation/json-mode
5We also employ various prompt tuning techniques, such as
adding specific words to the instructions and using delimiters.
Most of the prompt tuning techniques used in this study are
described in Bsharat et al. (2024).
6We use only the source side sentences to create CWMT,
as using the target side proved to be noisy due to influence the
noises of target side sentences according to our pilot study.3.2 Original Dataset Selection
In this study, we focus on the English-Japanese di-
rection and have selected the NAIST-SIC-Aligned-
ST corpus (Ko et al., 2023) as the original dataset.
As shown in Figure 1, the NAIST-SIC-Aligned-
ST corpus is based on TED Talks consisting of
audio aligned with sentence-by-sentence transcrip-
tions (offline translation) and manually created SI
data, which allows direct comparison of the LLM
SI-Corpus with manually created SI data7. Fur-
thermore, Fukuda et al. (2024) have manually an-
notated CWMT transcriptions from the NAIST-
SIC-Aligned-ST corpus, allowing us to compare
CWMT qualities by humans and LLMs.
3.3 LLM SI-Corpus Construction by LLMs
We created two types of corpora with two different
LLMs, GPT-3.58(Ouyang et al., 2022) and GPT-
49(OpenAI et al., 2024). GPT-4 is known to have
a higher ability to follow instructions and generate
higher-quality outputs than GPT-3.5. Therefore,
by comparing the two corpora, we also examine
the differences in the abilities of the LLMs. The
number of datasets in the LLM-SI-Corpus for train,
dev, and test is 65,083, 165, and 511, respectively,
which matches the numbers for NAIST-SIC-Align-
ST. The total cost of data creation was 20 dollars
(0.0003 dollars per sentence) for GPT-3.5 and 400
dollars (0.006 dollars per sentence) for GPT-4.
4 Experimental Setup
To evaluate the effectiveness of the LLM SI-Corpus,
we conduct experiments in Speech-to-Text set-
tings. We also conducted Text-to-Text settings
in Appendix C, which shows a similar trend to
Speech-to-Text. We implemented the baseline us-
ing Fairseq (Ott et al., 2019; Wang et al., 2020)
and SimulEval (Ma et al., 2020) and then applied
test-time wait- k(Ma et al., 2019) decoding policy
to the offline models10.
Speech-to-Text Settings Following the settings
of Fukuda et al. (2023); Ko et al. (2023), we em-
ploy pre-trained language models for both encoder
7This type of dataset currently exists only in the NAIST-
SIC dataset family (Shimizu et al., 2014; Doi et al., 2021; Zhao
et al., 2024; Ko et al., 2023; Fukuda et al., 2024); thus, we are
only testing the English-Japanese direction. Other language
pairs are planned for future work.
8gpt-3.5-turbo-0125
9gpt-4-0125-preview
10We followed examples in GitHub repository: https://
github.com/ahclab/naist-simulst
4Quality Metrics Textual Meaning Reference Source
BLEU ✓ ✓
BLEURT ✓ ✓
COMET ✓ ✓ ✓
COMET-QE ✓ ✓
Table 1: Quality metrics used in our experiments
and decoder by integrating them into the Trans-
former architecture (Vaswani et al., 2017). We
used Hubert-Large (Hsu et al., 2021) as the encoder,
and we used the decoder parts of mBART50 (Tang
et al., 2021), an encoder-decoder model pre-trained
with 50 language pairs. We train the model with
MuST-C v2.0 (Cattoni et al., 2021) as continuous
pre-training. We fine-tuned the models for 3K
steps, evaluating their performance every 200 steps,
and terminated the fine-tuning if there was no im-
provement in the loss score for eight consecutive
evaluations. The detailed settings are described in
Appendix C.1.
Evaluation Table 1 shows a list of translation
quality evaluation metrics used in our experi-
ments11. BLEU (Post, 2018) focuses on textual
n-gram matching between the generated sentences
and their reference sentences. BLUERT (Pu et al.,
2021), COMET (Rei et al., 2020), and COMET-
QE (Chimoto and Bassett, 2022) utilize embed-
dings from language models to focus on semantic
meanings. BLUERT evaluates the generated sen-
tences against reference sentences, while COMET
also considers both source sentences and refer-
ence sentences. In contrast, COMET-QE directly
assesses the similarity between the source and
generated sentences, thus avoiding the ambiguity
that may arise from using references. For latency
evaluation metrics, we choose Average Lagging
(AL) (Ma et al., 2019), Length Adaptive Average
Lagging (LAAL) (Papi et al., 2022), and Average
Token Delay (ATD) (Kano et al., 2023)12. We re-
ported to test-time wait- kin a set of kis 1 to 35 at
two intervals. One unit for kwas set to 160 frames
for the speech-to-text setting. When k= 3, after
reading 3 ×160 frames, the model would WRITE
and READ alternately.
Datasets For training datasets, we utilize two
versions of LLM SI-Corpus (GPT-4 and GPT-3.5)
11We also evaluated with BERTScore (Zhang et al., 2020),
but the trend is very similar to BLEURT.
12We cover all evaluation metrics used in the shared task of
IWSLT 2024: https://iwslt.org/2024/simultaneous .as proposed datasets, in addition to three base-
line datasets: NAIST-SIC-Aligned-ST (SIC), of-
fline data corresponding to SIC (OFFLINE), and
a pre-trained model only (Pre-train). For evalua-
tion datasets, we compare using three types: the
MuST-C tst-COMMON dataset (tst-COMMON),
the test dataset from NAIST-SIC-Aligned-ST13
(SIC-test), and the chunk-wise evaluation dataset
proposed by Fukuda et al. (2024), NAIST English-
to-Japanese Chunk-wise Monotonic Translation
Evaluation Dataset 202414(Chunk-wise).
5Experiments on Speech-to-Text Settings
Evaluation 1: tst-COMMON Figure 3 shows
the results of speech-to-text experiments. When
we focused on BLEU-AL in Figure 3 for k= 1,
k= 3, and k= 5, the LLM SI-Corpus (GPT-3.5
and GPT-4) achieved higher BLEU scores than OF-
FLINE, indicating improvements in both latency
and quality. However, as the value of kincreases,
the BLEU score in Pre-train starts to surpass that
of LLM SI-Corpus and OFFLINE when AL ex-
ceeds around k= 9. This pattern persists across
LAAL and ATD as well. This is attributed to the
alignment of training and evaluation data, leading
to enhanced BLEU scores. Next, in {BLEURT,
COMET}–{AL, LAAL}, both quality and latency
in LLM SI-Corpus (GPT-3.5 and GPT-4) surpasses
OFFLINE and Pre-train. Also in COMET-QE, the
LLM SI-Corpus demonstrates superior quality and
latency performance at all latencies in AL, LAAL,
and ATD, indicating that the model trained on the
LLM SI-Corpus can perform high-quality trans-
lations with low latency. Despite the trends ob-
served in text-to-text settings, the quality gap re-
mains evident in speech-to-speech settings even as
kincreases.
Evaluation 2: SIC-test Figure 4 shows the result
of SIC-test in speech-to-text settings. In Figure 4,
we focus on BLEU-AL, where the LLM SI-Corpus
exhibits higher quality than OFFLINE up to around
k= 5. However, OFFLINE and SIC perform bet-
ter at high latency because these align with the
training and evaluation data, thereby improving
the BLEU score. The same trends are observed in
LAAL and ATD. Next, in {BLEURT, COMET}–
{AL, LAAL, ATD}, both quality and latency in
13https://dsc-nlp.naist.jp/data/NAIST-SIC/
Aligned-ST
14https://dsc-nlp.naist.jp/data/NAIST-SIC/
Aligned-Chunk_Mono-EJ
51357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 911 13 15 17 19 21 23 25 27 29 31 33 35
1 3 5 79 111317 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911131517 1921 23 25 27 29 31 33 35
−1000 0 1000 2000 30000510
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
13579111317 19 21 23 25 27 29 31 33 351357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
−1000 0 1000 2000 300000.20.4
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1317 19 21 23 25 27 29 31 33 35
135791113 15 17 19 21 23 25 27 29 31 33 35
135791113 15 17 19 21 23 25 27 29 31 33 35
−1000 0 1000 2000 30000.40.50.60.7
135791113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1317 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
−1000 0 1000 2000 30000.40.50.60.7
1357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 911 13 15 17 19 21 23 25 27 29 31 33 35
1 3 5 79 111317 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911131517 1921 23 25 27 29 31 33 35
0 1000 2000 30000510
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
13579111317 19 21 23 25 27 29 31 33 351357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
0 1000 2000 300000.20.4
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1317 19 21 23 25 27 29 31 33 35
135791113 15 17 19 21 23 25 27 29 31 33 35
135791113 15 17 19 21 23 25 27 29 31 33 35
0 1000 2000 30000.40.50.60.7
135791113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1317 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
0 1000 2000 30000.40.50.60.7
1357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 911 13 15 17 19 21 23 25 27 29 31 33 35
1 3 5 79 111317 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911131517 1921 23 25 27 29 31 33 35
0 500 1000 15000510
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
13579111317 19 21 23 25 27 29 31 33 351357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
0 500 1000 150000.20.4
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1317 19 21 23 25 27 29 31 33 35
135791113 15 17 19 21 23 25 27 29 31 33 35
135791113 15 17 19 21 23 25 27 29 31 33 35
0 500 1000 15000.40.50.60.7
135791113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1317 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
0 500 1000 15000.40.50.60.7
GPT-4 GPT-3.5 SIC OFFLINE Pre-traintst-COMMON (speech-to-text)
AL AL AL AL
LAAL LAAL LAAL LAAL
ATD ATD ATD ATDBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU-AL BLUERT-AL COMET-AL COMET_QE-AL
BLEU-LAAL BLUERT-LAAL COMET-LAAL COMET_QE-LAAL
BLEU-ATD BLUERT-ATD COMET-ATD COMET_QE-ATDFigure 3: The results of tst-COMMON dataset on speech-to-text settings. The values for each plot indicate kof
test-time wait- k.
13 57 911 13 15 17 19 21 23 25 27 29 31 33 35
135 79 11 13 1517 19 21 23 25 27 29 31 33 35
1 35791113151719212325 2729 31 33 35
13579 11 1315 17 1921 23252729 31 3335
1357 911 1315 17 19 21 2325 27 29 31 33 35
−2000 0 20000513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
135791113 1517 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
−2000 0 20000.10.20.30.4
135791113 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 131517 19 2123 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
−2000 0 20000.50.60.7
13579 1113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
1357911131517 19 21 23 25 27 29 31 33 35
−2000 0 20000.40.50.60.70.8
13 57 911 13 15 17 19 21 23 25 27 29 31 33 35
135 79 11 13 1517 19 21 23 25 27 29 31 33 35
1 35791113151719212325 2729 31 33 35
13579 11 1315 17 1921 23252729 31 3335
1357 911 1315 17 19 21 2325 27 29 31 33 35
0 1000 2000 30000513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
135791113 1517 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
0 1000 2000 30000.10.20.30.4
135791113 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 131517 19 2123 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
0 1000 2000 30000.50.60.7
13579 1113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
1357911131517 19 21 23 25 27 29 31 33 35
0 1000 2000 30000.40.50.60.70.8
13 57 911 13 15 17 19 21 23 25 27 29 31 33 35
135 79 11 13 1517 19 21 23 25 27 29 31 33 35
1 35791113151719212325 2729 31 33 35
13579 11 1315 17 1921 23252729 31 3335
1357 911 1315 17 19 21 2325 27 29 31 33 35
0 500 1000 15000513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
135791113 1517 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
0 500 1000 15000.10.20.30.4
135791113 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 131517 19 2123 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
0 500 1000 15000.50.60.7
13579 1113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
1357911131517 19 21 23 25 27 29 31 33 35
0 500 1000 15000.40.50.60.70.8
GPT-4 GPT-3.5 SIC OFFLINE Pre-trainSIC (speech-to-text)
AL AL AL AL
LAAL LAAL LAAL LAAL
ATD ATD ATD ATDBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU-AL BLUERT-AL COMET-AL COMET_QE-AL
BLEU-LAAL BLUERT-LAAL COMET-LAAL COMET_QE-LAAL
BLEU-ATD BLUERT-ATD COMET-ATD COMET_QE-ATD
Figure 4: The results of SIC-test dataset on speech-to-text settings. The notations are the same as Figure 3.
LLM SI-Corpus (GPT-3.5 and GPT-4) surpasses
OFFLINE and Pre-train. In COMET-QE, the LLM
SI-Corpus outperforms OFFLINE and Pre-train at
all latencies in AL, LAAL, and ATD, indicating
that the model trained on the LLM SI-Corpus can
perform high-quality translations with low latency.
Evaluation 3: Chunk-wise Figure 5 shows that
the LLM SI-Corpus consistently exhibits superiorquality and latency performance across all qual-
ity evaluation metrics. As previously observed in
text-to-text, particularly when wait- kis small, the
quality gap among models is evident, albeit dimin-
ishing over larger wait- kvalues. However, the gap
remains noticeable in speech-to-speech settings.
Summary We evaluated the fine-tuned models
with LLM SI-Corpus in three different test data.
613579 1113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 1517 19 21 23 25 27 29 31 33 35
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 351357 9 11 1315 1719 21 23 25 27 29 31 33 35
1357 911 13 15 17 19 21 23 25 27 29 31 33
0 1000 2000 3000 40000102013579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35 1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33
0 1000 2000 3000 40000.20.40.6
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33
0 1000 2000 3000 40000.40.50.60.70.8
135791113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
1357911131517 19 21 23 25 27 29 31 33
0 1000 2000 3000 40000.40.50.60.7
13579 1113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 1517 19 21 23 25 27 29 31 33 35
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 351357 9 11 1315 1719 21 23 25 27 29 31 33 35
1357 911 13 15 17 19 21 23 25 27 29 31 33
0 1000 2000 3000 40000102013579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35 1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33
0 1000 2000 3000 40000.20.40.6
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33
0 1000 2000 3000 40000.40.50.60.70.8
135791113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
1357911131517 19 21 23 25 27 29 31 33
0 1000 2000 3000 40000.40.50.60.7
13579 1113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 1517 19 21 23 25 27 29 31 33 35
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 351357 9 11 1315 1719 21 23 25 27 29 31 33 35
1357 911 13 15 17 19 21 23 25 27 29 31 33
0 500 1000 15000102013579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35 1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33
0 500 1000 15000.20.40.6
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33
0 500 1000 15000.40.50.60.70.8
135791113 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 1315 17 19 21 23 25 27 29 31 33 35
13579111315 17 19 21 23 25 27 29 31 33 35
1357911131517 19 21 23 25 27 29 31 33
0 500 1000 15000.40.50.60.7
GPT-4 GPT-3.5 SIC OFFLINE Pre-trainChunk-wise (speech-to-text)
AL AL AL AL
LAAL LAAL LAAL LAAL
ATD ATD ATD ATDBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU-AL BLUERT-AL COMET-AL COMET_QE-AL
BLEU-LAAL BLUERT-LAAL COMET-LAAL COMET_QE-LAAL
BLEU-ATD BLUERT-ATD COMET-ATD COMET_QE-ATDFigure 5: The results of Chunk-wise dataset on speech-to-text settings. The notations are the same as Figure 3.
Source: OFFLINE ⇒Target:
Metrics ( ↑) GPT-4 GPT-3.5 Chunk-wise SIC-test
BLEU 13.8 15.5 16.2 7.9
ChrF 26.2 25.8 28.5 16.3
BERTScore_P 77.9 78.1 78.4 75.2
BERTScore_R 79.3 78.7 80.4 73.0
BERTScore_F1 78.6 78.3 79.3 74.0
BLEURT 55.9 56.0 59.0 40.8
COMET 82.3 83.2 84.3 71.7
COMET-QE 82.6 82.8 82.9 63.1
Table 2: Similarities between OFFLINE and each SI
corpus on the test set. BLEU and CharF indicate the
similarities of textual alignment. BERTScore compares
the semantic precision and recall between the source
and target sentences. BLEURT, COMET, and COMET-
QE compare semantic similarity, as shown in Table 1.
The results indicate that the LLM SI-Corpus deliv-
ers the best translation quality with minimal laten-
cies across all semantic similarity-focused evalua-
tion metrics. Even in BLUE, the LLM SI-Corpus
achieves equivalent translation quality especially
when kis small.
Meanwhile, in the SIC fine-tuned model on the
ATD evaluation setting, we observed significantly
longer lags compared to other fine-tuned models.
This trend is also evident in Ko et al. (2023) in
Figure 3. This observation may be attributed to
the fact that some transcripts in SIC are extremely
short relative to the length of the source speechinput. Fine-tuning with such data may lead to un-
desired generation results, such as excessive repe-
tition (Table 7 in Appendix D), leading to longer
lags. It can be argued that while achieving a shorter
output length is deemed advantageous in the ATD
setting, the current evaluation system may dispro-
portionately prioritize the shortest length, which
may be considered unfair. Output results in exces-
sive lengthening or shortening should be subject to
penalties. We leave this as future work.
6 Discussions
We picked some important discussion themes. The
more discussions are described in Appendix D.
6.1 Similarity between OFFLINE and SI
Table 2 shows a comparison of similarities between
OFFLINE and SI in the test set. The results in Ta-
ble 2 indicate that Chunk-wise and OFFLINE are
the most similar to OFFLINE across all evalua-
tion metrics. Additionally, the SIC-test has sig-
nificantly lower similarity compared to other SI
corpora. Focusing on BERTScore’s recall, unlike
other CWMT-based SI corpora which show higher
precision, it suggests the inclusion of omissions or
dropped translations due to its lower precision. Fur-
thermore, a comparison in COMET-QE between
LLM-SI-Corpus and Chunk-wise shows equivalent
quality, suggesting that the LLMs have capabilities
comparable to manually created data in CWMT.
7Source (1) A few weeks later, / (2) the department / (3) received a letter /(4) from the homeowner / (5) thanking
us / (6) for the valiant effort displayed in saving her home.
Reference (1)数週間後/ (2)消防団は/(4)家主から / (6)火事の際の勇敢な活動に対する / (5)お礼の/
(3)手紙をもらいました。
Pre-train (1) 数週間後/ (2)政府は/ (3)手紙を送りました )。
NAIST-SIC (1) 数週間後、
Offline (1) 数週間後、/ (2)政府は、 / (3)手紙を送りました。
GPT-3.5 (1)数週間後、/ (2)その部門は/(3)手紙を受け取った。/(4)自宅のオーナーから、/ (5)私た
ちに感謝/ (6)の手紙を、安全を確保するために 彼女の家を救うために 示された勇敢な努力
に感謝する。
GPT-4 (1)数週間後、/ (2)その部門が/(3)手紙を/(4)自宅から所有者から /(3)受け取った。/ (5)そ
れは、私たちに感謝の意を表すもので、 / (6)彼女の家を救うために 勇敢な努力がなされた。
Table 3: Examples of the generated texts for k= 7 in speech-to-text settings on tst-COMMON. The bracketed
numbers indicate the corresponding phrases in the source text.
6.2 Is the CWMT guideline effective for SI?
It might be too early to conclude regarding the ef-
fectiveness of CWMT due to its limited availability
only on test data. Meanwhile, access to CWMT-
applied training data is unrealistic as it involves the
interpreters’ engagement in its creation. Based on
our current observations of its test data, CWMT
specializes in achieving complete synchronization
for word order without any omissions. This charac-
teristic aligns well with machine translation evalu-
ation metrics, ensuring precise content correspon-
dence between the source and target texts. How-
ever, it overlooks the importance of summarization,
which is also a key tactic utilized in SI scenarios
for minimizing latency. Furthermore, its excessive
focus on aligning word order in source speech re-
sults in unnatural translations. Thus, creating an SI
Corpus that considers factors such as omission is a
necessary future challenge.
6.3 Which is better GPT-4 vs. GPT-3.5?
In terms of preserving word order, both versions
of GPT demonstrate equivalent proficiency. This
suggests a comparable ability to understand its
prompts. If the primary objective is solely the
preservation of word order, GPT-3.5 suffices. How-
ever, for those who prioritize output quality, GPT-4
may offer better performance as shown in Table 3.
As demonstrated, both GPT-3.5 and GPT-4 main-
tain the word order in the source language. How-
ever, in some cases, reordering occurs in GPT-4.
This type of reordering should be allowed as it
enhances naturalness. Conversely, in GPT-3.5, it
lacks fluency although the order is maintained. Thedetails are described in Appendix E.
Meanwhile, in the SiMT results in Section 5, the
fact that GPT-3.5 surpasses GPT-4 in some BLEU
scores indicates that metrics considering only tex-
tual similarity cannot capture such an issue, neces-
sitating the creation of new evaluation metrics.
Furthermore, the performance improvements
over OFFLINE observed with both GPT-3.5 and
GPT-4 suggest that LLMs with a certain level of
instruction-following capability are effective for
corpus construction.
7 Conclusion and Future Directions
In this study, we proposed a method for convert-
ing ST corpora to SI corpora using LLMs, based
on the CWMT guideline, and we constructed the
LLM-SI-Corpus. To verify the effectiveness of
the LLM-SI-Corpus, we conducted experiments in
three scenarios: a general offline ST corpus (tst-
COMMON), an SI corpus (SIC-test), and a CWMT
test corpus (Chunk-wise), in both speech-to-text
and text-to-text settings. In all scenarios, the SiMT
models trained with the LLM-SI-Corpus outper-
formed others with low latency and high quality.
Moreover, while manually constructing SI corpora
is costly, the LLM-SI-Corpus can be produced for
only 20 dollars. Therefore, it can be easily applied
to other ST corpora or adapted to other languages
since it utilizes LLMs.
As future directions, we plan to explore the ap-
plication of other SI techniques such as omission,
apply them to other large-scale ST corpora, and
also expand their application to speech-to-speech
settings.
88 Limitations
Lack of SiMT evaluation data, methods, and
definitions The existing metrics for evaluating
SiMT systems have become a challenge in reduc-
ing latency for ST test data such as tst-COMMON,
although SI involves various techniques, e.g., omis-
sion. Therefore, the use of ST data for evaluation
is a major limitation of this work. Thus, there is an
urgent need to establish evaluation metrics and data
suitable for SiMT. Moreover, despite the various SI
techniques available, there has been no mature dis-
cussion from an engineering perspective on which
SI techniques are necessary and essential for SiMT;
therefore, we need to address this issue in our fu-
ture work. These issues were clarified through our
comprehensive experiments and analysis.
Construct more SI corpora In this study, we
constructed the LLM-SI-Corpus based on the
NAIST-SI-Aligned-ST corpus for comparison with
existing SI corpora. Our method is applicable to
various other ST corpora at low cost. Moreover,
this study has shown that the output of LLMs is ef-
fective for developing SiMT corpora; therefore, we
plan to explore its applicability to other SiMT meth-
ods like omissions as our future steps. We hope
that by expanding into multiple languages and en-
hancing data augmentation, further development in
the SiMT field will be achieved.
Dataset Quality In this study, we used GPT-
3.5 and GPT-4 with a simple prompt for data cre-
ation. Therefore, there is room for improvement
in the selection of LLMs and the refinement of
prompts. Thus, it may become possible to create
higher quality datasets at a lower cost when the API
prices decrease or by switching to other strong LMs
such as Gemini (Team et al., 2024), Claude 3 and
Qwen (Bai et al., 2023). Additionally, employing
prompt strategies that leverage the capabilities of
LMs, such as Chain of Thought (CoT) (Wei et al.,
2022), Tree of Thought (ToT) (Yao et al., 2023a)
and ReAct (Yao et al., 2023b), could potentially
lead to the production of higher quality datasets.
Other SI techniques In this study, we addressed
CWMT, specifically focusing on chunking within
SI techniques. However, there are many other
SI techniques (Camayd-Freixas, 2011; Okamura
and Yamada, 2023), such as omission and summa-
rization, and addressing these is also necessary to
achieve better SI. Furthermore, the means of evalu-
ating these techniques and systematic methods arestill in development and not yet established, making
this a key topic of focus for the SiMT field in the
future. However, LLMs have a certain understand-
ing of the CWMT guidelines, which allows them
to account for word reordering from the insight
of this study. Therefore, the next step is to evalu-
ate whether LLMs can comprehend omissions, and
whether they can perform balanced omission and
summarization based on the number of syllables.
This will be explored as a future challenge.
9 Ethical Considerations
License of Source Dataset The NAIST-SIC-
Aligned-ST corpus used in this study is available
only for research purposes. We have used this cor-
pus for research, so there are no license violations.
Moreover, the LLM SI-Corpus was created from
the NAIST-SIC-Aligned-ST corpus and thus in-
herits its terms of use15. In terms of distribution,
redistribution of interpretation transcripts is pro-
hibited; therefore, we release only our transcripts
and the corresponding audio segment information
and do not contain any audio data or the original
transcripts. Furthermore, the README file of the
LLM SI-Corpus clearly states the source of the
data, the license, and acknowledgments, and prop-
erly documents the original data information. Note
that, it is permitted to cite example sentences from
the NAIST-SIC-Aligned-ST corpus.
Ownership rights about outputs of the LLMs
The LLM SI-Corpus was created using GPT-3.5
and GPT-4 and is therefore subject to OpenAI’s
license terms16. OpenAI assigns to us all rights,
titles, and interests in and to the output. As a re-
sult, we retain the ownership rights. There are no
restrictions on distributing the datasets, but in line
with NAIST-SIC-Aligned-ST, we distribute only
for research purposes. However, these terms may
change, and there may be a need to impose distri-
bution restrictions depending on the terms.
Moderations Since the LLM SI-Corpus funda-
mentally originates from TED Talks, it does not
contain any potentially harmful information. Fur-
thermore, we checked using OpenAI Moderation
APIs17and found no examples of harmful content.
15https://dsc-nlp.naist.jp/data/NAIST-SIC/
Aligned-ST/
16https://openai.com/policies/terms-of-use
17https://platform.openai.com/docs/guides/
moderation
9References
Farhad Akhbardeh, Arkady Arkhangorodsky, Mag-
dalena Biesialska, Ond ˇrej Bojar, Rajen Chatter-
jee, Vishrav Chaudhary, Marta R. Costa-jussa,
Cristina España-Bonet, Angela Fan, Christian Fe-
dermann, Markus Freitag, Yvette Graham, Ro-
man Grundkiewicz, Barry Haddow, Leonie Harter,
Kenneth Heafield, Christopher Homan, Matthias
Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai,
Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp
Koehn, Nicholas Lourie, Christof Monz, Makoto
Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki
Nakazawa, Matteo Negri, Santanu Pal, Allahsera Au-
guste Tapo, Marco Turchi, Valentin Vydrin, and Mar-
cos Zampieri. 2021. Findings of the 2021 conference
on machine translation (WMT21). In Proceedings of
the Sixth Conference on Machine Translation , pages
1–88, Online. Association for Computational Linguis-
tics.
Naveen Arivazhagan, Colin Cherry, Wolfgang
Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruom-
ing Pang, Wei Li, and Colin Raffel. 2019. Monotonic
infinite lookback attention for simultaneous machine
translation. In Proceedings of the 57th Annual
Meeting of the Association for Computational
Linguistics , pages 1313–1323, Florence, Italy.
Association for Computational Linguistics.
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: a framework
for self-supervised learning of speech representations.
InProceedings of the 34th International Conference
on Neural Information Processing Systems , NIPS ’20,
Red Hook, NY , USA. Curran Associates Inc.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-
uan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
Zhu. 2023. Qwen technical report.
Silvia Bernardini, Adriano Ferraresi, and Maja Mili ˇce-
vi´c. 2016. From EPIC to EPTIC – exploring simpli-
fication in interpreting and translation from an inter-
modal perspective. Target. International Journal of
Translation Studies , 28(1):61–86.
Sondos Mahmoud Bsharat, Aidar Myrzakhan, and
Zhiqiang Shen. 2024. Principled instructions are
all you need for questioning llama-1/2, gpt-3.5/4.
Zhongxi Cai, Koichiro Ryu, and Shigeki Matsubara.
2020. What affects the word order of target language
in simultaneous interpretation. In 2020 International
Conference on Asian Language Processing (IALP) ,
pages 135–140.Erik Camayd-Freixas. 2011. Cognitive theory of simul-
taneous interpreting and training. In Proceedings of
AMTA .
Roldano Cattoni, Mattia Antonino Di Gangi, Luisa Ben-
tivogli, Matteo Negri, and Marco Turchi. 2021. Must-
c: A multilingual corpus for end-to-end speech trans-
lation. Computer Speech & Language , 66:101155.
Junkun Chen, Renjie Zheng, Atsuhito Kita, Mingbo
Ma, and Liang Huang. 2021. Improving simultane-
ous translation by incorporating pseudo-references
with fewer reorderings. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 5857–5864, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Everlyn Chimoto and Bruce Bassett. 2022. COMET-
QE and active learning for low-resource machine
translation. In Findings of the Association for Com-
putational Linguistics: EMNLP 2022 , pages 4735–
4740, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Fahim Dalvi, Nadir Durrani, Hassan Sajjad, and Stephan
V ogel. 2018. Incremental decoding and training
methods for simultaneous translation in neural ma-
chine translation. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 2 (Short Papers) , pages
493–499, New Orleans, Louisiana. Association for
Computational Linguistics.
Hexuan Deng, Liang Ding, Xuebo Liu, Meishan Zhang,
Dacheng Tao, and Min Zhang. 2023. Improving si-
multaneous machine translation with monolingual
data. Proceedings of the AAAI Conference on Artifi-
cial Intelligence , 37(11):12728–12736.
Kosuke Doi, Katsuhito Sudoh, and Satoshi Nakamura.
2021. Large-scale English-Japanese simultaneous in-
terpretation corpus: Construction and analyses with
sentence-aligned data. In Proceedings of the 18th
International Conference on Spoken Language Trans-
lation (IWSLT 2021) , pages 226–235, Bangkok, Thai-
land (online). Association for Computational Linguis-
tics.
Maha Elbayad, Laurent Besacier, and Jakob Verbeek.
2020. Efficient Wait-k Models for Simultaneous Ma-
chine Translation. In Proc. Interspeech 2020 , pages
1461–1465.
Ryo Fukuda, Kosuke Doi, Katsuhito Sudoh, and Satoshi
Nakamura. 2024. Test data creation in simultaneous
machine translation in english to japanese pair: In-
sights from simultaneous interpretation tactics. IPSJ
SIG Technical Report . (In Japanese).
Ryo Fukuda, Yuta Nishikawa, Yasumasa Kano, Yuka
Ko, Tomoya Yanagita, Kosuke Doi, Mana Makinae,
Sakriani Sakti, Katsuhito Sudoh, and Satoshi Naka-
mura. 2023. NAIST simultaneous speech-to-speech
translation system for IWSLT 2023. In Proceedings
10of the 20th International Conference on Spoken Lan-
guage Translation (IWSLT 2023) , pages 330–340,
Toronto, Canada (in-person and online). Association
for Computational Linguistics.
Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-
tor O.K. Li. 2017. Learning to translate in real-time
with neural machine translation. In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
1, Long Papers , pages 1053–1062, Valencia, Spain.
Association for Computational Linguistics.
Shoutao Guo, Shaolei Zhang, and Yang Feng. 2023.
Simultaneous machine translation with tailored ref-
erence. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2023 , pages 3070–3084,
Singapore. Association for Computational Linguis-
tics.
HyoJung Han, Seokchan Ahn, Yoonjung Choi, Insoo
Chung, Sangha Kim, and Kyunghyun Cho. 2021.
Monotonic simultaneous translation with chunk-wise
reordering and refinement. In Proceedings of the
Sixth Conference on Machine Translation , pages
1110–1123, Online. Association for Computational
Linguistics.
He He, Alvin Grissom II, John Morgan, Jordan Boyd-
Graber, and Hal Daumé III. 2015. Syntax-based
rewriting for simultaneous machine translation. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing , pages 55–
64, Lisbon, Portugal. Association for Computational
Linguistics.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-
rahman Mohamed. 2021. Hubert: Self-supervised
speech representation learning by masked prediction
of hidden units. IEEE/ACM Trans. Audio, Speech
and Lang. Proc. , 29:3451–3460.
J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu,
P. E. Mazaré, J. Karadayi, V . Liptchinsky, R. Col-
lobert, C. Fuegen, T. Likhomanenko, G. Synnaeve,
A. Joulin, A. Mohamed, and E. Dupoux. 2020. Libri-
light: A benchmark for asr with limited or no su-
pervision. In ICASSP 2020 - 2020 IEEE Interna-
tional Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 7669–7673. https:
//github.com/facebookresearch/libri-light .
Yasumasa Kano, Katsuhito Sudoh, and Satoshi Naka-
mura. 2023. Average Token Delay: A Latency Met-
ric for Simultaneous Translation. In Proc. INTER-
SPEECH 2023 , pages 4469–4473.
Eugene Kharitonov, Morgane Rivière, Gabriel Syn-
naeve, Lior Wolf, Pierre-Emmanuel Mazaré, Matthijs
Douze, and Emmanuel Dupoux. 2021. Data augment-
ing contrastive learning of speech representations in
the time domain. In 2021 IEEE Spoken Language
Technology Workshop (SLT) , pages 215–222.Yuka Ko, Ryo Fukuda, Yuta Nishikawa, Yasumasa
Kano, Katsuhito Sudoh, and Satoshi Nakamura. 2023.
Tagged end-to-end simultaneous speech translation
training using simultaneous interpretation data. In
Proceedings of the 20th International Conference on
Spoken Language Translation (IWSLT 2023) , pages
363–375, Toronto, Canada (in-person and online).
Association for Computational Linguistics.
Taku Kudo. 2018. Subword regularization: Improv-
ing neural network translation models with multiple
subword candidates. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 66–75,
Melbourne, Australia. Association for Computational
Linguistics.
Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.
Kerstin Kunz, Christoph Stoll, and Eva Klüber. 2021.
HeiCiC: A simultaneous interpreting corpus combin-
ing product and pre-process data. In Proceedings
for the First Workshop on Modelling Translation:
Translatology in the Digital Age , pages 8–14, online.
Association for Computational Linguistics.
Danni Liu, Gerasimos Spanakis, and Jan Niehues. 2020.
Low-Latency Sequence-to-Sequence Speech Recog-
nition and Translation by Partial Hypothesis Selec-
tion. In Proc. Interspeech 2020 , pages 3620–3624.
Minh-Thang Luong and Christopher Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domains. In Proceedings of the 12th
International Workshop on Spoken Language Trans-
lation: Evaluation Campaign , pages 76–79, Da Nang,
Vietnam.
Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,
Kaibo Liu, Baigong Zheng, Chuanqiang Zhang,
Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and
Haifeng Wang. 2019. STACL: Simultaneous trans-
lation with implicit anticipation and controllable la-
tency using prefix-to-prefix framework. In Proceed-
ings of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 3025–3036, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Xutai Ma, Mohammad Javad Dousti, Changhan Wang,
Jiatao Gu, and Juan Pino. 2020. SIMULEV AL: An
evaluation toolkit for simultaneous translation. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 144–150, Online. Association
for Computational Linguistics.
Dominik Machá ˇcek, Matúš Žilinec, and Ond ˇrej Bojar.
2021. Lost in Interpreting: Speech Translation from
11Source or Interpreter? In Proc. Interspeech 2021 ,
pages 2376–2380.
Akira Mizuno. 2016. Simultaneous interpreting and
cognitive constraints.
Makoto Morishita, Katsuki Chousa, Jun Suzuki, and
Masaaki Nagata. 2022. JParaCrawl v3.0: A large-
scale English-Japanese parallel corpus. In Pro-
ceedings of the Thirteenth Language Resources and
Evaluation Conference , pages 6704–6710, Marseille,
France. European Language Resources Association.
Yasmin Moslem, Gianfranco Romani, Mahdi Molaei,
John D. Kelleher, Rejwanul Haque, and Andy Way.
2023. Domain terminology integration into machine
translation: Leveraging large language models. In
Proceedings of the Eighth Conference on Machine
Translation , pages 902–911, Singapore. Association
for Computational Linguistics.
Graham Neubig. 2011. The Kyoto free translation task.
http://www.phontron.com/kftt.
Yuki Okamura and Masaru Yamada. 2023. Jyun okuri
yaku” no kihan to mohan doji tsuyaku wo mohan
tosita kyoikuron no shiron (). In Hiroyuki Ishizuka,
editor, Word Order in English-Japanese Interpreting
and Translation: The History, Theory and Practice
of Progressive Translation , pages 217–250. Hitsuji
Syobo.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-
wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,
Christopher Berner, Lenny Bogdonoff, Oleg Boiko,
Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-
man, Tim Brooks, Miles Brundage, Kevin Button,
Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
Carey, Chelsea Carlson, Rory Carmichael, Brooke
Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben
Chess, Chester Cho, Casey Chu, Hyung Won Chung,
Dave Cummings, Jeremiah Currier, Yunxing Dai,
Cory Decareaux, Thomas Degry, Noah Deutsch,
Damien Deville, Arka Dhar, David Dohan, Steve
Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
Simón Posada Fishman, Juston Forte, Isabella Ful-
ford, Leo Gao, Elie Georges, Christian Gibson, Vik
Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-
Lopes, Jonathan Gordon, Morgan Grafstein, Scott
Gray, Ryan Greene, Joshua Gross, Shixiang Shane
Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
Yuchen He, Mike Heaton, Johannes Heidecke, Chris
Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-
woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-
mali, Ingmar Kanitscheider, Nitish Shirish Keskar,Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,
Christina Kim, Yongjik Kim, Jan Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea V oss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret
Zoph. 2024. Gpt-4 technical report.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for
sequence modeling. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics (Demonstrations) ,
pages 48–53, Minneapolis, Minnesota. Association
for Computational Linguistics.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
12Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems , volume 35, pages 27730–27744.
Curran Associates, Inc.
Jun Pan. 2019. The Chinese/English political interpret-
ing corpus (CEPIC): A new electronic resource for
translators and interpreters. In Proceedings of the
Human-Informed Translation and Interpreting Tech-
nology Workshop (HiT-IT 2019) , pages 82–88, Varna,
Bulgaria. Incoma Ltd., Shoumen, Bulgaria.
Sara Papi, Marco Gaido, Matteo Negri, and Marco
Turchi. 2022. Over-generation cannot be rewarded:
Length-adaptive average lagging for simultaneous
speech translation. In Proceedings of the Third Work-
shop on Automatic Simultaneous Translation , pages
12–17, Online. Association for Computational Lin-
guistics.
Sara Papi, Matteo Negri, and Marco Turchi. 2023a. At-
tention as a guide for simultaneous speech translation.
InProceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 13340–13356, Toronto, Canada.
Association for Computational Linguistics.
Sara Papi, Marco Turchi, and Matteo Negri. 2023b.
AlignAtt: Using Attention-based Audio-Translation
Alignments as a Guide for Simultaneous Speech
Translation. In Proc. INTERSPEECH 2023 , pages
3974–3978.
Matthias Paulik and Alex Waibel. 2009. Automatic
translation from parallel speech: Simultaneous inter-
pretation as mt training data. In 2009 IEEE Workshop
on Automatic Speech Recognition & Understanding ,
pages 496–501.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers , pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Reid Pryzant, Youngjoo Chung, Dan Jurafsky, and
Denny Britz. 2018. JESC: Japanese-English subtitle
corpus. In Proceedings of the Eleventh International
Conference on Language Resources and Evaluation
(LREC 2018) , Miyazaki, Japan. European Language
Resources Association (ELRA).
Heike Przybyl, Ekaterina Lapshinova-Koltunski, Ka-
trin Menzel, Stefan Fischer, and Elke Teich. 2022.
EPIC UdS - creation and applications of a simultane-
ous interpreting corpus. In Proceedings of the Thir-
teenth Language Resources and Evaluation Confer-
ence, pages 1193–1200, Marseille, France. European
Language Resources Association.Amy Pu, Hyung Won Chung, Ankur Parikh, Sebastian
Gehrmann, and Thibault Sellam. 2021. Learning
compact metrics for MT. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 751–762, Online and Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 2685–2702, Online. Association
for Computational Linguistics.
Holger Schwenk, Vishrav Chaudhary, Shuo Sun,
Hongyu Gong, and Francisco Guzmán. 2021. Wiki-
Matrix: Mining 135M parallel sentences in 1620 lan-
guage pairs from Wikipedia. In Proceedings of the
16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume ,
pages 1351–1361, Online. Association for Computa-
tional Linguistics.
Hiroaki Shimizu, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2014. Col-
lection of a simultaneous translation corpus for com-
parative analysis. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14) , pages 670–673, Reykjavik,
Iceland. European Language Resources Association
(ELRA).
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-
man Goyal, Vishrav Chaudhary, Jiatao Gu, and An-
gela Fan. 2021. Multilingual translation from de-
noising pre-training. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 3450–3466, Online. Association for Computa-
tional Linguistics.
Gemini Team et al. 2024. Gemini: A family of highly
capable multimodal models.
Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC’12) , pages 2214–2218, Istanbul,
Turkey. European Language Resources Association
(ELRA).
Hitomi Toyama, Shigeki Matsubara, Koichiro Ryu,
Nobuo Kawaguchi, and Yasuyoshi Inagaki. 2004.
Ciair simultaneous interpretation corpus. In Proceed-
ings of Oriental COCOSDA .
Ioannis Tsiamas, Gerard I. Gállego, Carlos Escolano,
José Fonollosa, and Marta R. Costa-jussà. 2022. Pre-
trained speech encoders and efficient fine-tuning
methods for speech translation: UPC at IWSLT 2022.
InProceedings of the 19th International Confer-
ence on Spoken Language Translation (IWSLT 2022) ,
pages 265–276, Dublin, Ireland (in-person and on-
line). Association for Computational Linguistics.
13Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu,
Chaitanya Talnikar, Daniel Haziza, Mary Williamson,
Juan Pino, and Emmanuel Dupoux. 2021. V oxPop-
uli: A large-scale multilingual speech corpus for rep-
resentation learning, semi-supervised learning and
interpretation. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 993–1003, Online. Association for
Computational Linguistics.
Changhan Wang, Yun Tang, Xutai Ma, Anne Wu,
Dmytro Okhonko, and Juan Pino. 2020. Fairseq
S2T: Fast speech-to-text modeling with fairseq. In
Proceedings of the 1st Conference of the Asia-Pacific
Chapter of the Association for Computational Lin-
guistics and the 10th International Joint Conference
on Natural Language Processing: System Demon-
strations , pages 33–39, Suzhou, China. Association
for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems .
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L. Griffiths, Yuan Cao, and Karthik R
Narasimhan. 2023a. Tree of thoughts: Deliberate
problem solving with large language models. In
Thirty-seventh Conference on Neural Information
Processing Systems .
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao.
2023b. React: Synergizing reasoning and acting
in language models. In The Eleventh International
Conference on Learning Representations .
Ruiqing Zhang, Xiyang Wang, Chuanqiang Zhang,
Zhongjun He, Hua Wu, Zhi Li, Haifeng Wang, Ying
Chen, and Qinfei Li. 2021. BSTC: A large-scale
Chinese-English speech translation dataset. In Pro-
ceedings of the Second Workshop on Automatic Si-
multaneous Translation , pages 28–35, Online. Asso-
ciation for Computational Linguistics.
Shaolei Zhang and Yang Feng. 2021. Universal simul-
taneous machine translation with mixture-of-experts
wait-k policy. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 7306–7317, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-uating text generation with bert. In International
Conference on Learning Representations .
Jinming Zhao, Philip Arthur, Gholamreza Haffari,
Trevor Cohn, and Ehsan Shareghi. 2021. It is not as
good as you think! evaluating simultaneous machine
translation on interpretation data. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing , pages 6707–6715, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Jinming Zhao, Yuka Ko, Kosuke Doi, Ryo Fukuda, Kat-
suhito Sudoh, and Satoshi Nakamura. 2024. Naist-
sic-aligned: an aligned english-japanese simultane-
ous interpretation corpus.
Baigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma,
Hairong Liu, and Liang Huang. 2020. Simultane-
ous translation policies: From fixed to adaptive. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 2847–
2853, Online. Association for Computational Lin-
guistics.
Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong
Su, Yonggui Liang, and Shikai Wu. 2024. Fine-
tuning large language models for domain-specific
machine translation.
A Detail of the CWMT Guideline and
Workflow
Okamura and Yamada (2023) defines these chunk
boundaries using the following rules (rule 1, 2, 3,
and 4), then Fukuda et al. (2024) added the fifth
rule as follows:
1.Before conjunctions or relative pronouns that
introduce clauses (excluding when they mod-
ify the subject).
2.After infinitives, prepositions, or gerunds
when followed by three or more words.
3.When the subject consists of three or more
words.
4.Before and after punctuation marks such as
commas (excluding lists of individual words),
semicolons, hyphens, etc.
5.Before prepositional phrases or adverbial
phrases following the beginning of a sentence
(or directly after conjunctions or relative pro-
nouns that introduce clauses).
Based on these guidelines, Fukuda et al. (2024)
defines its chunking workflow. First, rules 1, 3, 4,
and 5 are applied to each source sentence chunk,
and then the translated chunks are concatenated
14while preserving boundaries. Rule 2 is optionally
applied in the last step to avoid the influence of the
prior steps causing extremely small chunk trans-
lations. This chunk-wise approach enables inter-
preters to navigate the challenges posed by gram-
matical differences between the source and target
languages while managing the demands for transla-
tion speed and accuracy.
Based on this chunking workflow, Fukuda et al.
(2024) manually constructed a test dataset. The
procedure is as follows:
1.Translate each chunk from the beginning of
the sentence.
2.Translate in a way that the connection between
chunks is natural when considering the entire
sentence.
3.Translate without including information from
the following chunks.
4.Additionally, for the sake of maintaining the
fluency of the sentence, the following opera-
tions are permitted, but applied carefully:
(a)Repeating the information from the pre-
vious chunk.
(b)Deferring the information to be trans-
lated to the following chunk.
(c) Omitting unnecessary information.
B Style differences among SI, Offline
Translation and CWMT (Details)
There are significant style gaps among SI, offline
translation, and CWMT as described in Fukuda
et al. (2024); Ko et al. (2023). Table 4 and Table 5
are examples describing their differences.
C Experiments (Details)
C.1 Experimental Setup
The evaluation methods and datasets are the same
as those described in Section 4.
Speech-to-Text Settings Following the settings
of Fukuda et al. (2023); Ko et al. (2023), we employ
pre-trained language models for both encoder and
decoder18by integrating them into the Transformer
18Our baselines are almost the same as the base-
line of IWSLT2023 Speech-to-Text settings ( https:
//github.com/facebookresearch/fairseq/tree/
iwslt2023/examples/simultaneous_translation ), but,
due to an implementation issue, we have switched the encoder
from wav2vec 2.0 (Baevski et al., 2020) to HuBERT (Hsu
et al., 2021).architecture (Vaswani et al., 2017).We used Hubert-
Large (Hsu et al., 2021) as the encoder, which in-
cludes a feature extractor and transformer encoder
layers. The feature extractor, trained on 60k hours
of unlabeled speech data from Libri-Light (Kahn
et al., 2020), consists of a 7-layer convolutional net-
work with kernel sizes of (10,3,3,3,3,2,2), strides of
(5,2,2,2,2,2,2), and 512 channels. For the decoder
side, we use the decoder parts of mBART50 (Tang
et al., 2021), an encoder-decoder model pre-trained
with 50 language pairs. The decoder consists of 12
layers of transformer decoders, and the embedding
layer and linear projection weights are shared, with
a vocabulary size of 250K. The inputs are wave-
forms with a 16kHz sampling rate that are normal-
ized to zero mean and unit variance. During train-
ing, each source audio is augmented (Kharitonov
et al., 2021) with a probability of 0.8. We train the
model with MuST-C v2.0 (Cattoni et al., 2021) as
continuous pre-training. We fine-tuned the models
for 3K steps, evaluating their performance every
200 steps, and terminated the fine-tuning if there
was no improvement in the loss score for eight
consecutive evaluations. To avoid overfitting to
the small SI data, the following parameters are
fixed (Tsiamas et al., 2022): the feature extractor
and feed-forward layers of the encoder and the em-
bedding, self-attention, and feed-forward layers of
the decoder.
Text-to-Text Settings We train an NMT model
through pre-training19, then fine-tuned it using
SI data. For pre-training, we used WMT21 En-
Ja datasets (Akhbardeh et al., 2021) (JParaCrawl
v3 (Morishita et al., 2022), News Commentary
v16 (Tiedemann, 2012), WikiTitles v3 (Tiedemann,
2012), WikiMatrix v1 (Schwenk et al., 2021),
JESC (Pryzant et al., 2018), KFTT (Neubig, 2011))
and MuST-C v2.0 (Cattoni et al., 2021). We use
SentencePiece (Kudo and Richardson, 2018) for
subword tokenization with a Unigram Language
Model (Kudo, 2018). The vocabulary size is 32K
tokens with a character coverage of 0.99995 on a
shared dictionary. The tokenizer was trained on
the pre-training data. We use a Transformer-big
model (Vaswani et al., 2017), warmup update at
4000, dropout at 0.3, and the learning rate at 0.0005.
The model is trained for 100K steps, with evalua-
tion conducted every 2K steps. Training is termi-
19Our baselines are based on the English-to-Japanese Text-
to-Text translation at IWSLT2022 settings: https://github.
com/ksudoh/IWSLT2022_simul_t2t_baseline_enja
15Source And (1) I’m / (2) not here to / (3) say that / (4) men are to / (5) blame for the / (6) crisis and what / (7)
happened in my / (8) country.
OFFLINEしかしこの経済/ (6)危機や私の/ (8)国での / (7)出来事について / (1)私は/ (4)男性に/ (5)非
があると / (3)言うつもりは / (2)ありません
SI (4)男性の、 / (5)せいだけでは / (2)ありません、 私どもの / (8)国の、金融/ (6)崩壊の、 / (5)
責任は、
Table 4: Translation style difference between offline and SI. The number indicates the corresponding words in the
source. The example is coming from (Ko et al., 2023).
Source (1) Groups like Anonymous / (2) have risen up / (3) over the last 12 months / (4) and have become a
major player / (5) in the field of online attacks.
OFFLINE (1) Anonymousというグループは / (3)この 12ヶ月ほど / (2)活気づいていて / (5)オンライン
攻撃において / (4)大きな存在になってます。
CWMT (1)アノニマスのようなグループが / (2)台頭してきています ，/ (3)過去12ヶ月にわたって ，
/ (4)そして主要なプレイヤーになっています ，/ (5)オンライン 攻撃の分野において .
Table 5: Translation style difference between offline and CWMT. The number indicates the corresponding words in
the source. The example is coming from (Fukuda et al., 2024).
nated if there is no improvement in the best loss
after eight consecutive evaluations. During fine-
tuning, we trained for 3K steps, with evaluations
conducted every 200 steps. Fine-tuning is also fin-
ished if there are no updates after eight consecutive
evaluations.
C.2 Results on Text-to-Text Setting
Evaluation 1: tst-COMMON Figure 6 shows
the result of tst-COMMON in text-to-text settings.
We focused on BLEU-AL in Figure 6, for k=1 and
k=3, the LLM SI-Corpus (GPT-3.5 and GPT-4)
achieves higher BLEU scores than OFFLINE, in-
dicating improvements in both latency and quality.
However, as the value of kincreases, the BLEU
score begins to decrease compared to Pre-train
when AL is more than 5. Similar trends are ob-
served for LAAL. Next, in {BLEURT, COMET}–
{AL, LAAL}, the quality surpasses OFFLINE
when the latency is approximately less than 5.
Moreover, when compared with Pre-train, the trans-
lation qualities are improved at all latencies. In
COMET-QE, which focuses on the semantic sim-
ilarity between the source text and the generated
text, the LLM SI-Corpus outperforms OFFLINE at
all latencies, indicating that the model trained on
the LLM SI-Corpus can perform high-quality trans-
lations with low latency. Conversely, in ATD, al-
though the quality remains unchanged, an increase
in latency is observed, suggesting that the output
sequence length is longer compared to the source
text. Meanwhile, all results of the SIC-test show adecline in quality at all latencies for both AL and
LAAL, but there is an improvement in latency in
ATD. This suggests that omission and truncation
of information are occurring in the SIC corpus20.
Thus, the LLM-SI Corpus, while reducing latency
like SIC, maintains translation quality, unlike SIC,
demonstrating its effectiveness as data for SI.
Evaluation 2: SIC-test Figure 7 shows the re-
sult of SIC-test in text-to-text settings, in which we
highlight BLEU-AL, where the LLM SI-Corpus
exhibits higher quality than OFFLINE up to about
k=5. The same trend is observed in LAAL. How-
ever, SIC performs better at high latency because
it aligns the training and evaluation data at the sen-
tence level, thereby improving the BLEU score. In
contrast, the LLM SI-Corpus demonstrates higher
quality than SIC at low latencies. Conversely, when
focusing on ATD, SIC shows the best results in both
latency and quality, suggesting that the shorter out-
put sentences are attributed to omissions and trun-
cations. Meanwhile, when focusing on {BLEURT,
COMET, COMET-QE}, SIC exhibits the worst
translation quality. This is likely due to the effects
of omissions, where missing information from the
source text leads to decreased semantic similarity.
Conversely, the LLM SI-Corpus outperforms OF-
FLINE up to a moderate level of latency, and in
terms of COMET-QE, it achieves comparable or
better results at all latencies.
20This trend has also been reported by Ko et al. (2023).
1613 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
0 5 10 150510
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 351357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
0 5 10 150.10.20.30.40.5
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35 13579 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
0 5 10 150.50.60.70.8
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
0 5 10 150.40.50.60.7
13 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150510
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 351357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.10.20.30.40.5
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35 13579 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.50.60.70.8
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.40.50.60.7
13 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
10 200510
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 351357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
10 200.10.20.30.40.5
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35 13579 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
10 200.50.60.70.8
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
10 200.40.50.60.7
GPT-4 GPT-3.5 SIC OFFLINE Pre-traintst-COMMON (text-to-text)
AL AL AL AL
LAAL LAAL LAAL LAAL
ATD ATD ATD ATDBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU-AL BLUERT-AL COMET-AL COMET_QE-AL
BLEU-LAAL BLUERT-LAAL COMET-LAAL COMET_QE-LAAL
BLEU-ATD BLUERT-ATD COMET-ATD COMET_QE-ATDFigure 6: The results of tst-COMMON dataset on text-to-text settings. The notations are the same as Figure 3.
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 1 3 5 79 11 13 15 17 19 21 23 25 27 29 31 33 35
135791113 1517 1921 23 25 27 2931 33 35
13579 11 1315 17 19 21 23 25 27 29 31 33 35
1357 9 11 13 1517 19 21 2325 27 2931 33 35
0 5 10 15510
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
0 5 10 150.10.20.30.4
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
0 5 10 150.50.60.7135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
0 5 10 150.40.50.60.70.8
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 1 3 5 79 11 13 15 17 19 21 23 25 27 29 31 33 35
135791113 1517 1921 23 25 27 2931 33 35
13579 11 1315 17 19 21 23 25 27 29 31 33 35
1357 9 11 13 1517 19 21 2325 27 2931 33 35
5 10 15510
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.10.20.30.4
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.50.60.7135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.40.50.60.70.8
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 1 3 5 79 11 13 15 17 19 21 23 25 27 29 31 33 35
135791113 1517 1921 23 25 27 2931 33 35
13579 11 1315 17 19 21 23 25 27 29 31 33 35
1357 9 11 13 1517 19 21 2325 27 2931 33 35
10 20510
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
10 200.10.20.30.4
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
10 200.50.60.7135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
10 200.40.50.60.70.8
GPT-4 GPT-3.5 SIC OFFLINE Pre-trainSIC (text-to-text)
AL AL AL AL
LAAL LAAL LAAL LAAL
ATD ATD ATD ATDBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU-AL BLUERT-AL COMET-AL COMET_QE-AL
BLEU-LAAL BLUERT-LAAL COMET-LAAL COMET_QE-LAAL
BLEU-ATD BLUERT-ATD COMET-ATD COMET_QE-ATD
Figure 7: The results of SIC-test dataset on text-to-text settings. The notations are the same as Figure 3.
Evaluation 3: Chunk-wise Figure 8 shows the
results of Chunk-wise in text-to-text settings. The
LLM SI-Corpus consistently delivers the best trans-
lation qualities at all latencies. However, in ATD,
although SIC has a latency advantage, its transla-
tion quality is significantly lower. Additionally,
when focusing on {AL, LAAL}, SIC tends to trans-
late slightly faster than the LLM SI-Corpus, sug-
gesting that modifications such as omissions inCWMT could further improve both latency and
translation quality for the LLM SI-Corpus. Finally,
in the LLM SI-Corpus, there is a tendency for in-
creased latency in ATD, indicating that longer out-
puts are generated.
Summary We evaluated the fine-tuned models
with LLM SI-Corpus in three different test data.
All results indicate that when focusing on latency,
the LLM SI-Corpus delivers the best translation
1713 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150102030
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35135 79 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.20.40.6
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.50.60.70.8
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.40.50.60.70.8
13 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150102030
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35135 79 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.20.40.6
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.50.60.70.8
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
5 10 150.40.50.60.70.8
13 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
10 200102030
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35135 79 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
10 200.20.40.6
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 35
10 200.50.60.70.8
135 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35
1357 9 11 13 15 17 19 21 23 25 27 29 31 33 35
13579 11 13 15 17 19 21 23 25 27 29 31 33 3513579 11 13 15 17 19 21 23 25 27 29 31 33 35
1357911 13 15 17 19 21 23 25 27 29 31 33 35
10 200.40.50.60.70.8
GPT-4 GPT-3.5 SIC OFFLINE Pre-trainChunk-wise (text-to-text)
AL AL AL AL
LAAL LAAL LAAL LAAL
ATD ATD ATD ATDBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU
BLEURT
COMET
COMET_QEBLEU-AL BLUERT-AL COMET-AL COMET_QE-AL
BLEU-LAAL BLUERT-LAAL COMET-LAAL COMET_QE-LAAL
BLEU-ATD BLUERT-ATD COMET-ATD COMET_QE-ATDFigure 8: The results of Chunk-wise dataset on text-to-text settings. The notations are the same as Figure 3.
quality at fast latencies across all evaluation meth-
ods and datasets. Moreover, in semantic evaluation
metrics using references, such as BLEURT and
COMET, the LLM SI-Corpus achieves comparable
or superior translation quality at all latencies. Addi-
tionally, in the reference-free metric COMET-QE,
the LLM SI-Corpus consistently shows the best re-
sults in both latency and quality in all cases. How-
ever, when focusing on ATD, it is evident that the
LLM SI-Corpus tends to produce longer outputs
due to slightly higher latency, while it contributes
to improving both latency and quality in SI models.
D Discussions (Details)
D.1 Word Order
We investigate the extent to which word order is
preserved in the source language. Our motivation
stems from the observation that in real SI scenarios,
particularly in distant language pairs, interpreters
endeavor to maintain word order in the source to
minimize latency while preserving quality. To ad-
dress the balance between quality and latency, we
seek to emulate the guidelines employed by real
SI interpreters. Therefore, we examine how word
order is preserved in the target sentences, focusing
on examples generated by the wait- kvalue of 7 (Ta-
ble 6). In the source sentences, the word order is
structured as (1), (2), (3), and (4). However, in the
reference sentence which comes from the subtitle
of the TED Talk, the order is (1), (4), and (2), withan omission of (3). Both GPT-3.5 and GPT-4 fine-
tuned models maintain the original word order in
the source, yielding (1), (2), (3), and (4) sequences.
Conversely, OFFLINE maintains all contents from
the source but rearranges them to (1), (4), (3), and
(2). In contrast, the NAIST-SIC only translates
(1), omitting the rest. This example demonstrates
that our objective of preserving word order in the
source is achieved in both GPT-3.5 and GPT-4.
Both GPT-3.5 and GPT-4 achieved maintaining
word order in the source. These results suggest that
while GPT-4 is considered superior to GPT-3.5 in
terms of model ability, however for this task, the
source language word order preservation, GPT-3.5
satisfies to fulfill the task.
D.2 Quality
We focus on the quality using reference-free met-
rics to avoid biases inherent in references. De-
spite increasing wait- kvalues, NAIST-SIC exhibits
low output quality as observed in the outputs (Fig-
ure 3, Figure 4, Figure 5, Figure 6, Figure 7, Fig-
ure 6). Although training SiMT and SiST with real
SI data is presumed beneficial for learning real-SI
tactics, relying solely on SI transcripts proves in-
adequate for effective model training. Similarly,
Pre-trained models, characterized by fewer omis-
sions in training data compared to the SIC dataset,
still suffer from detrimental impacts on quality
during testing. OFFLINE demonstrates compet-
18Source (1) Back in New York, / (2) I am the head of development / (3) for a non-profit / (4) called Robin Hood .
Reference (1)私はニューヨークにある / (4)ロビンフッド 財団で/ (2)組織開発の責任者をしています。
Pretrain (1)バック・イン・ニューヨーク / (2)私は開発部門のトップで / (4)ロビン・フッドと 呼ばれ
ます。
NAIST-SIC (1)ニューヨークに 戻ります。
OFFLINE (1)バック・イン・ニューヨークでは、 / (4)私は、ロビン・フッドという / (3)非営利団体
の、 / (2)開発部門のトップです。
GPT-3.5 (1)ニューヨークに 戻ると / (2)私は開発の責任者です。 / (3)非利益のために、 / (4)ロビン
フッドと呼ばれる。
GPT-4 (1)ニューヨークに 戻って、 / (2)私はその開発の責任者です。 / (3)それは、 非営利のための、 /
(4)ロビンフッドと呼ばれる利益のためのものです。
Table 6: Example of output sentences in Pre-train, NAIST-SIC, OFFLINE, GPT-3.5, and GPT-4 on tst-COMMON
in wait- k=7 on Text-to-Text setting.
Source (1) And I spent 30 days / (2) eating nothing but this – / (3) fun in the beginning, / (4) little difficult in the
middle, / (5) very dangerous in the end.
Reference (1)そしてこればかり 30日間/ (2)食べたときは / (3)最初は楽しかったのが / (4)途中で困難に
/ (5)最後には非常に危険となりました .
Pretrain 始めにこんなことを (1) 30日も/ (2)食べていました / (3)楽しいことばかりです / (4)中間に少
しは難しいのですが / (5)とても危険です
NAIST-SIC (1)三十日/、(2)これ、これ、これ、これ、これ、これ、これ、これ、これ、これ、これ、
これ、これ .....
OFFLINE (1) 30日も、 / (2)こんなことを、 何も食べませんでした /、(3) (笑)、最初から /、ちょっと、
ちょっと、ちょっと、ちょっと、 ...。
GPT-3 (3)最初から楽しい。
GPT-4 (1)そして、 私は30日間を過ごしました / (2)これ以外に何も食べずに、 / (3)最初に楽しいで
す。 / (4)そして、 中央で少し難しいです。 / (5)最後には非常に危険です
Table 7: SIC fine-tuned leads to undesiable result at tst-COMMON wait- k= 25 on Speech-To-Text settings.
itive performance on tst-COMMON, even at small
wait-kvalues such as k= 3 or higher. However,
on chunk-wise datasets, these models experience
quality degradation at smaller wait- kvalues, indi-
cating potential overfitting. Conversely, GPT-3.5
and GPT-4 consistently deliver competitive results
across both test sets.
D.3 Latency
In this section, our analysis regarding latency con-
centrates on Pre-trained, OFFLINE, GPT-3.5, and
GPT-4. We exclude NAIST-SIC due to its short
outputs with poor quality in Table 3, and serious
repetitions in Table 7. In AL and LAAL, both
GPT-3.5 and GPT-4 demonstrate smaller latencycompared to Pre-train and OFFLINE across both
text-to-text and speech-to-speech settings (Figure 3,
Figure 4, Figure 5, Figure 6, Figure 7, Figure 6).
In ATD, Pre-train and OFFLINE exhibit smaller
latency in text-to-text settings compared to GPT-
3.5 and GPT-4, whereas LLM SI-Corpus achieves
smaller latency than OFFLINE and Pre-train in
speech-to-text settings. This discrepancy arises
from the tendency that Pre-trained and OFFLINE
produce shorter translation outputs than GPT-3.5
and GPT-4 in text-to-text settings (Table 8), serious
repetitions, leading to long latency, also happen in
OFFLINE in speech-to-speech settings (Table 7),
and the characteristic in ATD counting both start
and end timing to measure the latency.
19Source (1) But still it was a real foot race / (2) against the other volunteers / (3) to get to the captain in charge /
(4) to find out / (5) what our assignments would be .
Reference (3)それでも 団長を見つけて / (4)任務を割り振ってもらうのに / (2)他のボランティアと / (1)
激しい競走になりました。
Pretrain (2)それでも 足を踏みにじる 他のボランティアたちに / (3)キャプテンに / (1)足を踏みにじる
真のレースでした / (5)私たちの課題を/ (4)見つけるためです。
NAIST-SIC (1)でも、
OFFLINE (1)それでも、 実に、アフトレースで、 / (2)他のボランティアが / (3)キャプテンに、 / (5)手
紙を送り、 / (4)課題を探しました。
GPT-3.5 (1)それでも、それは 本物の足のレースでした。 / (2)他のボランティアたちに 対して、 / (3)
キャプテンに向かうために、 / (5)私たちの課題が/ (4)何かを見つけるために。
GPT-4 (1)それでも、それは 本当に足の運命でした。 / (2)他のボランティアたちに 対して、 / (3)
キャプテンに 到着するために、 / (5)私たちの標的が何であるか / (4)を調べるために
Table 8: Example of output sentences in Pre-train, NAIST-SIC, OFFLINE, GPT-3.5, and GPT-4 on Mustc-tst-
COMMON in wait k=7 on Text-to-Text setting. Both GPT-3.5 and GPT-4 achieve positive fluency while allowing
small reordering in (4) and (5).
-8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7020406080100120140160180200
GPT-4
GPT-3.5
Difference in Chunk Numbers (Subtraction: Chunk-wise - GPT-4/3.5)Counts
Figure 9: The difference in chunk numbers between
Chunk-wise and GPT-4/GPT-3.5. The total number of
sentences is 511.
D.4 Chunking
Figure 9 shows the differences in the number
of chunks per sentence between Chunk-wise and
LLM-SI-Corpus (GPT-3.5 and GPT-4) in the test
data. These results illustrate the differences be-
tween chunking according to the chunking work-
flow (Chunk-wise) and chunking by the prompt
(LLM-SI-Corpus). The findings in Figure 9 indi-
cate that GPT-4 tends to chunk more finely com-
pared to Chunk-wise, while GPT-3.5 tends to chunk
more coarsely. However, chunking is merely one
criterion, and the fact that these chunking results
match does not necessarily mean they are good.
Therefore, it is necessary to measure aspects such
as word order alignment from the generated sen-
tences, but the criteria are ambiguous, and manual
measurement is not feasible. Hence, establishingautomated evaluation methods is essential.
D.5 Misalignment between Source Input and
the SI data
In our corpus analysis, we found that both NAIST-
SIC-Aligned and Must-C offline data contain noise
in the form of misalignment between the source
and target sentences. This misalignment results
in the shift of information, e.g., information in a
sentence appearing in its neighbors, leading to im-
balanced sentence correspondences. When deal-
ing with Must-C offline data, difficulty arises in
aligning audio input features with subtitles due to
space limitations, which may lead to unbalanced
correspondences. Similarly, in the case of NAIST-
SIC-Aligned, which utilizes Japanese transcripts of
interpreted data, aligning source text becomes chal-
lenging. This is due to the SI characteristics, involv-
ing omissions and summaries, which further com-
plicate the alignment process due to imbalances
between the source and target transcripts. Some ex-
amples are shown in Table 9, Table 10. Addressing
alignment in unbalanced sentences emerges as a
particularly challenging aspect of SI, representing
an important area for future research.
20Source Target
Really important. これが、
So I’m committing to potatoes; I’m committing to milk; 問題なわけです。ポテト、そしてミルク、
I’m committing to leeks and broccoli all very important
stuff.そして、ネギ、ブロッコリー、こういったものに
対して、
Because of our differences, we create and sustain life. 違いがあるから
So we should embrace our difference and aim for chal-
lenge.持続可能性を生み出すことができます。
Table 9: Example of misalignment sentence pairs in SIC.
Source Target
I do the philosophy of art, aesthetics, actually,for a living. 私は美の哲学、美学を。
I try to figure out intellectually, philosophically, and
psychologically, what the experience of beauty is, what
sensibly can be said about it, and how people go off the
rails in trying to understand it.;生業としています、 美という体験は何なのか、 美
について 確かに言えることは 何か、人は美を理
解しようとして、いかに 道に迷うかといったこと
を、知的、哲学的、心理学的に解明しようとして
います。
Now this is an extremely complicated subject, in part
because the things that we call beautiful are so different.美というのは 恐ろしく込み入ったテーマであり、
私たちが美しいと呼んでいるものには、 非常に大
きな幅があります、いかにバラエティに 富んでい
ることか、 赤ちゃんの 顔。
I mean just think of the sheer variety a baby’s face,
Berlioz’s "Harold in Italy," movies like "The Wizard
of Oz" or the plays of Chekhov, a central California land-
scape, a Hokusai view of Mt. Fuji, "Der Rosenkavalier,"
a stunning matchwinning goal in a World Cup soccer
match, Van Gogh’s "Starry Night," a Jane Austen novel,
Fred Astaire dancing across the screen.ベルリオーズの「イタリアのハロルド」、「オズ
の魔法使い」のような映画、チェーホフの 戯曲、
中部カリフォルニアの 風景、北斎の富士山の絵、
「ばらの 騎士」。
Table 10: Example of misalignment sentence pairs in Must-C.
Source (1) I just came back from a community that / (2) holds the secret / (3) to human survival .
Reference (3)私は人類の生存に関わる / (2)秘密を握る/(1)あるコミュニティから 戻ってきたばかりです。
Pretrain (1)ちょうどコミュニティから 戻って / (2)シークレットを / (3)人間に持つようになりまし
た。
NAIST-SIC (1)コモンティから 戻って来たんです。
OFFLINE (1)ちょうど、コミュニティから 戻り、 / (2)シカゴに 秘密を隠しました。
GPT-3.5 (1)ちょうどコミュニティから 戻ってきた。 /(2)それはシナリオに 秘密を保持している。 /
(3)人間の生存に。
GPT-4 (1)ちょうど 戻ってきたのは、コミュニティからで、 /(3)それは人類に/
(2)秘密を秘めている。
Table 11: Example of output sentences in Pre-train, NAIST-SIC, OFFLINE, GPT-3.5, and GPT-4 on Mustc-tst-
COMMON in wait k=7 on Text-to-Text setting. GPT-3.5 maintains source word order completely, while GPT-4
allows small reordering in (2) and (3).
E Qualitative Analysis
E.1 Text-to-Text setting on tst-COMMON
when k=7
Table 8 demonstrates the equivalent qualities be-
tween GPT-3.5 and GPT-4 with a small reorderingbetween (4) and (5). Table 11 shows fluency in
GPT-4 is better than GPT-3.5, while both GPT-3
and GPT-4 achieve translating all contents in the
source,. This gap is attributed to small distance
reordering between (2) and (3) in GPT-4, whereas
word order in the source side is kept completely in
21Source (1) I came to realize, / (2) as a physician, / (3) that I was working toward a goal / (4) which was different
from the goal of evolution – / (5) not necessarily contradictory, just different .
Reference (2)私は医師として / (1)気づきました / (3)私は目標に向かって働いていますが / (4)それは進
化の目標とは異なっていました / (5)矛盾というわけではありませんが 単に違ったのです。
Pretrain (1)気づきに来ました
NAIST-SIC (1)私が、気づいたのは、
OFFLINE (1)気づいたのです。
GPT-3.5 (1)私は気づいた。 / (2)医師として、 / (3)私が目指していた 目標は/ (4)異なることに。
GPT-4 (1)私は気づきました、 / (2)医師として、 / (3)私が取り組んで
いた目標は、 / (4)それは異なる、その 目標は進化の目標から、 /
(5)必ずしも矛盾するわけではない、 単に異なるものです。
Table 12: Example of output sentences in Pre-train, NAIST-SIC, OFFLINE, GPT-3.5, and GPT-4 on Mustc-tst-
COMMON in wait k=7 on Text-to-Text setting. (5) is omitted in GPT-3.5, while every information is kept in
GPT-4.0.
Source (1) Here was some lawyer or money manager who, / (2) for the rest of his life,
/(3) gets to tell people /(4) that he went into a burning building /(5) to save a living creature , /
(6) just because he beat me by five seconds .
Reference (1)弁護士だったか 資産運用者だったか / (2)彼は後々まで言い続けるでしょう / (4)自分は燃
え盛る建物の中に入り/ (5)生き物を救ったのだと / (6)私より５秒前に着いただけなのに。
Pretrain (1)ここには 弁護士やお金持ちの誰かがいました
NAIST-SIC (1)弁護士やマネーマンが。
OFFLINE (1)ここには 弁護士やマネージャーがいます。
GPT-3.5 (1)ここには 弁護士やマネージャーがいました / (2)残りの人生を過ごした。
GPT-4 (1)ここには、いくつかの 弁護士またはマネージャーがいました 。 / (2)彼
は彼の生涯の残りの間、/(3)人々に伝え続けました。/(4)彼が燃える建物に入ったと、/
(5)生きている生き物を救うために。
Table 13: Example of output sentences in Pre-train, NAIST-SIC, OFFLINE, GPT-3.5, and GPT-4 on Mustc-tst-
COMMON in wait k=7 on Speech-to-Text setting. From (3) to (6) is omitted in GPT-3.5, while most information is
maintained in GPT-4.
GPT-3.5 ignoring fluency at each chunk boundary.
Although our motivation in this work is keeping
word order in the source, we also consider small
reorderings necessary to maintain its fluency. How-
ever, long-distance reordering, which appeared in
reference as a complete switch between (1) and
(3), is our focus and should not be allowed. Such
long-distance reordering leads to long latency be-
cause translating (3) in reference is possible only
when (3) in the source is available and the rest is
translated following (3). Table 12 shows GPT-4
achieves both fluency and word order, however, the
output becomes long. On the other hand in GPT-3,(5), the latter part in the source, is omitted.
E.2 Speech-to-Text setting on tst-COMMON
when k=7
Table 13 shows the output quality gap between
GPT-3.5 and 4. The output length in GPT-4 is long
but includes most chunks in the source maintaining
the order in the source, whereas in GPT-3.5 only (1)
and (2) are translated and the rest are omitted. In
Table 14, both GPT-3.5 and GPT-4 could translate
all information in the source but GPT-4 is better at
quality and maintains its fluency.
22Source (1) So I went and met with his brother and father (2) and said, (3) "We’re going to give you this money.
What are you going to do with it?"
Reference (1)お兄さんとお 父さんに会い/ (3)「支援金を差し上げますが 何に使いますか ？」/ (2)と尋
ねました
Pretrain (1)それで私は彼の兄弟と父に会い/ (2)こう言いました
NAIST-SIC (1)彼と会いました。、
OFFLINE (1)彼と会ったのは、 兄と父親と、 / (2)こう言いました。
GPT-3.5 (1)だから、 私は彼の兄と父と会いました。 / (2)そして、 言いました、 / (3)「わかるでしょ
う、このお 金を渡します
GPT-4 (1)だから、 私は行きました。そして、 彼の兄と父親に会いました。 / (2)そして、 言いまし
た、 / (3)「このお 金をあなたにあげますね、 何をしますか ?」
Table 14: Example of output sentences in Pre-train, NAIST-SIC, OFFLINE, GPT-3.5, and GPT-4 on Mustc-tst-
COMMON in wait k=7 on Speech-to-Text setting. GPT-4 is better than GPT-3.5 at fluency.
E.3 Summary
From these analyses, we report that while both
GPT-3.5 and GPT-4 have the ability to follow the
prompt to maintain the word order in the source,
GPT-4 could manage the prompt and fluency at the
same time better than GPT-3.5 (Table 8, Table 11,
Table 14). We also report that the severity of omit-
ting information from the source is more serious
in GPT-3.5 than GPT-4 (Table 12, Table 13). We
reserve the ability gap between GPT-3.5 and GPT-4
as future works.
23