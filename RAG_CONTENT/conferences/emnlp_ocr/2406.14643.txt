Holistic Evaluation for Interleaved Text-and-Image Generation
Minqian Liu♠Zhiyang Xu♠Zihao Lin♠Trevor Ashby♠
Joy Rimchala♡Jiaxin Zhang♡Lifu Huang♠,♣
♠Virginia Tech♡Intuit AI Research♣University of California, Davis
{minqianliu, zhiyangx, zihaol, trevorashby, lifuh}@vt.edu
{joy_rimchala, jiaxin_zhang}@intuit.com
https://vt-nlp.github.io/InterleavedEval/
Instruction: Create a vividadvertisement combining text and images to promote Birds Nest. Context: The first part is “Japan's Yakitori tradition is introduced in Australia with a vibrant, flavorful menu <image>”.Output: We prepare and grill chicken tenderloin skewers over special white charcoal <image>.Our venues offer delicious Yakitori dishes with authentic Japanese service <image>…
Storytelling
Marketing
Script
Education
Report
Document
Editing
Activity
Figure 1: Overview of our INTERLEAVED BENCH , a comprehensive benchmark that covers 10 diverse use cases for
interleaved text-and-image generation, and the evaluation results of I NTERLEAVED EVAL based on GPT-4o.
Abstract
Interleaved text-and-image generation has been
an intriguing research direction, where the mod-
els are required to generate both images and
text pieces in an arbitrary order. Despite the
emerging advancements in interleaved genera-
tion, the progress in its evaluation still signifi-
cantly lags behind. Existing evaluation bench-
marks do not support arbitrarily interleaved im-
ages and text for both inputs and outputs, and
they only cover a limited number of domains
and use cases. Also, current works predomi-
nantly use similarity-based metrics which fall
short in assessing the quality in open-ended
scenarios. To this end, we introduce INTER -
LEAVED BENCH , the first benchmark carefully
curated for the evaluation of interleaved text-
and-image generation. INTERLEAVED BENCH
features a rich array of tasks to cover diverse
real-world use cases. In addition, we present
INTERLEAVED EVAL, a strong reference-free
metric powered by GPT-4o to deliver accurate
and explainable evaluation. We carefully de-
fine five essential evaluation aspects for IN-
TERLEAVED EVAL, including text quality ,per-
ceptual quality ,image coherence ,text-image
coherence , and helpfulness , to ensure a compre-
hensive and fine-grained assessment. Through
extensive experiments and rigorous human eval-uation, we show that our benchmark and met-
ric can effectively evaluate the existing models
with a strong correlation with human judgments
surpassing previous reference-based metrics.
We also provide substantial findings and in-
sights to foster future research in interleaved
generation and its evaluation.1
1 Introduction
Multimodal learning has been a rapidly develop-
ing research field given the recent advancements
in Large Multimodal Models (LMMs) (Xu et al.,
2023; Dai et al., 2023; Liu et al., 2023a). While
these models can perform diverse tasks such as
detailed image description and visual question an-
swering, the outputs are limited to the text-only
format, which hinders their broader applications.
More recently, there has been a growing focus on
enhancing LMMs with the capability of interleaved
generation , i.e., generating multimodal content that
seamlessly integrates both text and one or multiple
images (Koh et al., 2023; Dong et al., 2024; Sun
et al., 2023b,a). This opens new avenues for ap-
plications in diverse challenging scenarios, such
1The source code and datasets are publicly available at
https://huggingface.co/mqliu/InterleavedBench
for research purposes.arXiv:2406.14643v3  [cs.CV]  8 Oct 2024Input: chair and sofa and mountains in the background.
Input: Given the task “How to make a toast in an oven” and the first two steps, predict the subsequent steps to complete the task.Step 1: Put the slices of bread flat on the oven rack.
Step 2: Turn on the broiler of the oven, orset the heat on the toaster oven.
Output: Here’re the subsequent steps:Step 3: Use tongs to flip the bread over half-way through the toasting.
Step 4: Remove the toast from the oven
InterleavedBench
Output:Existing Benchmark
Figure 2: Comparison between the existing benchmark (multi-concept image composition (Kumari et al., 2023a))
and our INTERLEAVED BENCH . Compared with the existing benchmark, INTERLEAVED BENCH has the following
features: (1) both input and output can have arbitrarily interleaved text and images, and (2) each instance has a
detailed instruction to benchmark models’ instruction-following capability.
as creative content generation (Anantrasirichai and
Bull, 2022), visual storytelling (Huang et al., 2016;
Lukin et al., 2018), and multimodal script genera-
tion (Yang et al., 2021; Qi et al., 2024).
While the LMMs for interleaved generation
are continuously gaining stronger capabilities,
progress in the evaluation of interleaved generation
significantly lags behind with several critical chal-
lenges remaining. First , most existing works for
interleaved generation quantitatively benchmark
the models on text-to-image tasks where the out-
put is usually one single image (Koh et al., 2023;
Dong et al., 2024). However, such evaluation meth-
ods would fail to assess model performance in
the real-world scenarios of interleaved generation,
where the output usually consists of interleaved
text and images. Second , apart from human evalu-
ation which is costly and time-consuming, existing
works still heavily rely on reference-based metrics
such as BLEU (Papineni et al., 2002) FID (Heusel
et al., 2017) that measure the similarity between
generated samples and gold references. Such
similarity-based metrics often fail to accurately cap-
ture outputs’ quality, especially in open-ended tasks
such as creative generation and visual storytelling.
Third , the evaluation of interleaved generation is
complex and involves many different aspects, such
asperceptual quality ,coherence between text and
images, and helpfulness of the overall content. One
single aspect is usually insufficient to reflect the
overall quality. For example, despite the images in
one output having good perceptual quality, the out-
put can still be not helpful to users if the generated
content is not coherent with the context, e.g., the
request from users.
To address these critical limitations, we in-
troduce INTERLEAVED BENCH , the first bench-mark for holistic evaluation of interleaved text-and-
image generation. We construct INTERLEAVED -
BENCH with a high-quality and diverse collection
of interleaved generation scenarios that encompass
a wide range of real-world use cases, including
creative generation, multimodal script generation,
visual storytelling, and many others. We compare
ourINTERLEAVED BENCH and one existing bench-
mark (Kumari et al., 2023b) closest to our dataset
in Figure 2. To support the evaluation, we also
introduce INTERLEAVED EVAL, a strong reference-
free evaluation metric based on GPT-4o (OpenAI,
2024), the current state-of-the-art LMM. INTER -
LEAVED EVAL can take in any evaluation instruc-
tions and provide a fine-grained evaluation along
with detailed explanations. We carefully curate a
multi-aspect evaluation criterion to ensure a holistic
evaluation for INTERLEAVED EVAL. Specifically,
we define five essential aspects for interleaved eval-
uation, including text quality, perceptual quality,
image coherence, text-image coherence , and help-
fulness , following the principles that (1) these as-
pects are generally applicable in different scenarios,
(2) these aspects are atomic and orthogonal to each
other, and (3) the combination of these aspects can
comprehensively cover the critical dimensions in
interleaved generation.
Extensive experiments and rigorous human eval-
uation demonstrate that (1)Our curated INTER -
LEAVED BENCH posts unique and significant chal-
lenges to the existing integrated LMMs (e.g.,
GILL (Koh et al., 2023) and EMU-2 (Sun et al.,
2023a)) for interleaved generation, where the qual-
ity of their outputs are far from satisfying. The
pipeline systems combined with a strong LMM
(e.g., GPT-4o) and a separate image generation
model (e.g., DALLE3 (Betker et al.)) generallyachieve better results but still struggle on certain
tasks; (2)INTERLEAVED EVAL can achieve a good
correlation with human judgments with significant
improvement over previous automatic evaluation
metrics; (3)The evaluation of interleaved gener-
ation remains a very challenging direction due to
its complexity and the limitation of the existing
LMM-based evaluator. We believe that our work
can provide useful resources and insights for inter-
leaved generation and its evaluation.
2 Related Work
Large Multimodal Models for Interleaved Gen-
eration The advent of large multimodal models
(LMMs) (Koh et al., 2023; Sun et al., 2023a) has
significantly advanced the field of interleaved text-
and-image generation. Previous models such as
DALL-E (Ramesh et al., 2021) and Stable Diffu-
sion (Podell et al., 2023) have demonstrated im-
pressive capabilities in generating high-quality im-
ages conditioned on textual descriptions. However,
previous focus has predominantly been on unidirec-
tional generation tasks, either from text to image or
image to text, without considering the interleaved
generation scenarios where text and images are
seamlessly integrated within the same output. Re-
cent works have begun to address this gap, with
the LMMs extended with diffusion models, explor-
ing the generation of mixed text and image out-
puts (Koh et al., 2023; Sun et al., 2023b; Dong et al.,
2024; Tian et al., 2024; Zhan et al., 2024; Chen
et al., 2023). These models leverage advanced ar-
chitectures and training techniques to enhance their
ability to produce coherent and contextually rele-
vant interleaved content. Despite these advance-
ments, the evaluation of such models remains an
underexplored area, with most evaluations still re-
lying on separate assessments of text and image
quality or simplistic reference-based metrics. Our
proposed INTERLEAVED BENCH benchmark aims
to bridge this gap by providing a holistic evalua-
tion framework tailored specifically for interleaved
text-and-image generation.
Evaluation of Multimodal Generation Tasks
Evaluating multimodal generation tasks presents
unique challenges due to the inherent complex-
ity of assessing both textual and visual compo-
nents simultaneously. Existing metrics for text
generation, such as BLEU (Papineni et al., 2002),
ROUGE (Lin, 2004), and LLM-based evalua-
tors (Zhong et al., 2022; Liu et al., 2023b, 2024),fall short when applied to multimodal outputs as
they fail to capture the visual quality and coherence
with textual content. Similarly, visual generation
metrics like FID (Heusel et al., 2017) and IS (Sal-
imans et al., 2016) are inadequate for evaluating
the textual elements accompanying the images. To
address this, recent studies have employed mul-
timodal metrics (Zhang et al., 2023b; Ku et al.,
2023), such as CLIPScore (Hessel et al., 2021),
which leverages the alignment capabilities of the
CLIP model to measure the similarity between gen-
erated images and their corresponding textual de-
scriptions. However, CLIPScore can only mea-
sure the alignment between text and images, which
is not sufficient to evaluate the quality of gener-
ated output comprehensively. Moreover, human
evaluations, although more reliable, are resource-
intensive and cannot be scalable. In terms of evalu-
ation benchmarks in multimodal learning, existing
works mostly focus on evaluating the tasks with
single-modality output (Fu et al., 2024b; Li et al.,
2024a; Lu et al., 2024; Wang et al., 2024; Fu et al.,
2024a), such as conditional text-to-image gener-
ation (Chen et al., 2024; Ku et al., 2024), where
the primary focus is solely the quality of generated
images. Our INTERLEAVED BENCH benchmark in-
troduces a novel approach to evaluate interleaved
text-and-image generation by incorporating multi-
ple aspects of quality assessment, thus providing a
more nuanced and holistic evaluation framework.
3 I NTERLEAVED BENCH
We introduce INTERLEAVED BENCH , the first com-
prehensive benchmark meticulously constructed
to evaluate text-and-image interleaved generation.
Figure 3 shows some examples from INTER -
LEAVED BENCH .
3.1 Dataset Curation Process
Our dataset includes two subsets: a context-based
subset where the instances contain a multimodal
context of interleaved text and images in the in-
put (first row in Figure 3), and a context-free
subset with text-only inputs (second row in Fig-
ure 3). The context-free subset can assess whether
the model can creatively generate interleaved con-
tent based on the text-only instruction, while the
context-based subset can better benchmark the co-
herence and consistency of generated outputs.
Collection of Context-based Subset Firstly , we
collect the source data of the context-based sub-Instruction: How to Make Nacho Cheese Jalapeno Poppers.Context:Step 1:Fill the jalapeno halves with cheese. <image>Step 2:Get the breading ingredients ready. <image>Output: Step 3:Bread the poppers. <image> 
Multimodal Script GenerationInstruction: Describe Mumbai, India’s bustling port city.Context: Mumbai's iconic Gateway of India stands majestically by the bustling harbor. <image>Output: The Bandra-Worli Sea Link stretches over Mumbai's coastal waters <image>. Marine Drive, Mumbai's enchanting promenade, sparkles with city lights at dusk <image>.Document Completion
Instruction: Complete the story given the first part.Context: This year we took a family vacation to the lake. <image>Output:We spent lots of time down on the lake shore swimming. <image> On the last day we took a canoe trip on the lake. It was great! <image>Visual Story Completion
Instruction: Generate an interleaved content to introduce how to use VR to create immersive product experiences.Output: Virtual Reality (VR) can transform how users interact with products <image>. The combination of visual quality and interactive guidance ensures that users are engaging <image>.Marketing Material Generation
Instruction: Generate an interleaved content for an infrastructure investment report with charts and illustrations. Output: Significant efforts have been directed toward transportation and energy. Examples: (1) New highway constructions help reduce congestion <image>. (2) Investments in renewable energy contribute a sustainable future <image>.Report Generation
Instruction: Generate an interleaved education content for veterinary science. Output: Veterinary science is essential for understanding the structure and function of animal systems. We will introduce (1) Canine Skeletal System <image>, (2) Feline Digestive System, and (3) Equine Respiratory System <image>.……Education Content Generation
Figure 3: Illustration of examples in our I NTERLEAVED BENCH from six representative use cases.
set from existing academic datasets or web re-
sources. Specifically, we collect the data of multi-
modal script generation from WikiHow (Yang et al.,
2021), visual story completion from VIST (Huang
et al., 2016), activity generation from the dense cap-
tions and the extracted video frames in ActivityNet
Captions (Krishna et al., 2017), sequential image
editing from MagicBrush (Zhang et al., 2023a), and
multi-concept image composition from CustomD-
iffusion (Kumari et al., 2023a). For web resources,
we apply an automatic data filtering pipeline to
discard the samples with poor quality to obtain a
small set of source data. We detail our data filtering
pipeline in Appendix A. Secondly , after collecting
the source data (either from academic benchmarks
or web resources), we then apply a human selection
process to manually select the samples based on
data quality and diversity (i.e., avoiding selecting
similar samples). Finally , we ask human experts to
annotate an instruction Ifor each sample based on
the collected content. We include the details of the
data selection and instruction annotation process
in Appendix A. For the samples that are originally
interleaved articles, we pick the first kimages and
their associated text as the context Cfor the input. k
is randomly sampled for each example and ranges
from 1 to the maximum number of images minus 1
since we need to ensure the output contains at least
Multimodal Script
 Generation
12.3%
Seqeuntial
 Image Editing12.3%
Multi-concept
 Composition12.3%
Education Content
 Generation12.3%
Market Material
 Generation12.3%Report
 Generation12.3%Document
 Completion 10.8%Fairytale Generation
6.1%Activity Generation
4.9%Visusal Story Completion
4.5%Figure 4: The distribution of the use cases in INTER -
LEAVED BENCH .
one image. The rest of the images and text are used
as the gold reference.
Collection of Context-free Subset The context-
free subset consists of the use cases of marketing
material generation, report generation, education
content generation , and fairytale generation as they
are common and practical scenarios for interleaved
generation. We first leverage GPT-4o to generate
a set of instances for each use case. For example,
in marketing material generation, one instance is
“creating marketing campaigns around holidays to
boost sales ”. Then, we use GPT-4o to extend each
instance into a more detailed instruction, e.g., “ Cre-Dataset Name Detailed Instruction Image Input Text Output Image Output
MagicBrush (Zhang et al., 2023a) No Single No Single
DreamBench (Chen et al., 2024) No Multiple No Single
CustomDiffusion (Kumari et al., 2023a) No Multiple No Single
DreamEditBench (Li et al., 2023) No Multiple No Single
Mantis-Eval (Jiang et al., 2024) Yes Multiple Yes No
BLINK (Fu et al., 2024b) Yes Multiple Yes No
MuriBench (Wang et al., 2024) Yes Multiple Yes No
INTERLEAVED BENCH (Ours) Yes Multiple Yes Multiple
Table 1: Comparisons between INTERLEAVED BENCH and existing open-sourced multimodal evaluation benchmarks.
The highlighted features of our benchmark include detailed instructions and multiple images in input and/or output
that are arbitrarily interleaved with text.
ate an interleaved content that combines engaging
text and eye-catching images for marketing cam-
paigns around holidays to boost sales. Begin by
researching holiday themes relevant to your prod-
ucts... ”. Finally, we ask human annotators to verify
whether the instructions are reasonable and of good
quality. Note that we do not have gold references
in this subset.
Dataset Statistics In total, we finally collect 815
instances across 10 use cases, including multimodal
script generation ,document completion ,visual
story completion ,marketing material generation ,
report generation ,education content generation ,
activity generation ,sequential image editing , and
multi-concept image composition . The detailed
distribution of the use cases is shown in Figure 4.
3.2 Comparison with Existing Benchmark
We highlight the following key differences and
unique challenges introduced by our INTER -
LEAVED BENCH compared with the existing bench-
mark. (1): Output modality: our benchmark re-
quires the models to generate interleaved text and
multiple images that could present in an arbitrary
order, whereas exiting benchmarks (Kumari et al.,
2023b) only cover the output with single modal-
ity or a single image (as shown in Figure 2); (2)
Requirement on coherence: given that both in-
puts and outputs in our benchmark can contain
multiple pieces of text and images, our dataset
can assess whether the outputs are coherent and
consistent with input instruction and context, and
within the outputs themselves; (3) Instruction fol-
lowing: Most existing conditional image genera-
tion datasets only contain simple instructions such
as “add a cat next to the person ”. On the contrary,
each instance in our benchmark contains a detailed
human-annotated instruction to describe the task.
Thus, our dataset can evaluate models’ instruction-following and generalization capabilities. We show
the difference between our benchmark and existing
datasets in Table 1.
4 I NTERLEAVED EVAL
In many use cases of interleaved generation, such
as “generate a story about Snow White using both
text and images ”, comparing the output against
a gold reference is unrealistic since the genera-
tion can be fairly open-ended. However, exist-
ing approaches predominantly use reference-based
metrics, e.g., BLEU (Papineni et al., 2002) and
FID (Heusel et al., 2017), to measure the quality of
text and image, respectively. They usually fail to
assess the quality accurately.
To bridge the gap between existing metrics and
the demand in more diverse and realistic scenar-
ios, we present INTERLEAVED EVAL, a strong
reference-free metric based on GPT-4o, the cur-
rent state-of-the-art LMM that supports arbitrarily
interleaved inputs. To obtain a holistic and compre-
hensive evaluation of interleaved generation, we de-
fine five fine-grained evaluation aspects, including
text quality ,perceptual quality ,image coherence ,
text-image coherence andhelpfulness , and evalu-
ate the output of each aspect separately. We show
the detailed definition for each evaluation aspect
in Table 8 in Appendix B. For each instance to be
evaluated, the input of the evaluator consists of an
instruction Ithat indicates what should be accom-
plished, system output X= (TO,PO), where TO
is the output text and POis the set of output images,
the evaluation aspect a, and optionally, the context
Cof the task (e.g., the given text and images in
models’ inputs).
We formulate the evaluation metric INTER -
LEAVED EVAL as follows: We instruct the GPT-4o
evaluator to output discrete scores from {0, 1, 2, 3,
4, 5} based on the detailed criteria shown in Table 8,where 1 indicates the worst quality, 5 indicates the
best quality, and 0 indicates output text and/or im-
ages are empty. We also instruct GPT-4o to provide
a detailed explanation to improve the interpretabil-
ity. Note that when the output text is empty, the
scores on text-related aspects ( text quality andtext-
image quality ) are 0. Similarly, when the output
image is empty, the scores on image-related as-
pects ( perceptual quality ,image coherence , and
text-image quality ) are 0. Moreover, we do not
apply the text-related aspects in sequential editing
and subject-driven generation since the primary fo-
cus of these tasks is whether the image is generated
correctly according to the instructions.
5 Experiments
5.1 Experiment Setup
Baseline Models We benchmark the following
baseline models which can be categorized into two
types: integrated models where the LMM and im-
age generation model are connected via neural mod-
ules, and pipeline models where the LMM and im-
age generation model are connected via prompts in
natural language. The integrated models include:
(1) MiniGPT-5 (Zheng et al., 2023a) which con-
nects a large language model with a stable diffusion
model via generative vokens, enabling description-
free multimodal generation; (2) GILL (Koh et al.,
2023) which allows a pretrained large language
model to generate multimodal responses by map-
ping the hidden states of text into the embedding
space of an image generation model; (3) EMU-
2 (Sun et al., 2023a) which induces in-context
learning capabilities of LLMs by scaling up the
model size and the size of the pretraining dataset;
(4) EMU-2 Gen + Gold Text where EMU-2 Gen
is a pretrained EMU-2 model instruction-tuned on
various controllable image generation tasks. How-
ever, EMU-2 Gen cannot generate text so we com-
bine it with ground-truth textual responses to come
up with a complete text-and-image interleaved con-
tent for evaluation. The pipeline models include:
(5) GPT-4o (OpenAI, 2024) + DALL ·E 3 (Betker
et al.) where GPT-4o is the state-of-the-art propri-
etary LMM that can comprehend interleaved text-
and-image inputs and generate text-only responses.
We leverage GPT-4o to generate text responses as
well as captions for image responses in the desired
positions. Then the captions are fed into DALL ·E
3 to generate images. Finally, we combine the
text responses with generated images in their orig-inal orders; (6) Gemini-1.5 (Anil et al., 2023) +
SDXL (Podell et al., 2023) : we build this baseline
in a similar way as GPT-4o + DALL ·E 3 but use
Gemini-1.5 Pro as the LMM and Stable Diffusion
XL Turbo as the image generation model.
Baseline Metrics We adopt the following met-
rics as baselines to validate the effectiveness of
ourINTERLEAVED EVAL.(1) BERTScore is a
reference-based metric for text evaluation. We ap-
ply BERTScore to compute the similarity between
the text output and the reference in our dataset.
We set the BERTScore to 0 if the text output is
empty. (2) CLIPScore is originally a reference-
free evaluation metric for image captioning, which
computes the cosine similarity between the CLIP
embeddings of a predicted caption and that of the
input image. We adopt CLIPScore as two baselines:
a reference-based metric to compute image-image
similarity between predicted images and ground
truth images in a pair-wise manner, and a reference-
free metric to compute the text-image compatibility
between the generated images and text. (3) Dream-
Sim is a recently proposed model-based metric to
measure perceptual similarity. Similar to image-
image CLIPScore, we use DreamSim to compute
the perceptual distance between predicted images
and ground truth images in a pair-wise manner.
5.2 Main Results
We show the main results of using INTER -
LEAVED EVAL to conduct the fine-grained evalu-
ation for various baseline approaches on INTER -
LEAVED BENCH in Table 2. The baselines in the up-
per part are the integrated andopen-sourced mod-
els while the baselines in the lower part are the
pipeline models where the LMMs are proprietary.
From Table 2, we observe that: First , the pipeline
models consistently outperform the integrated mod-
els on all evaluation aspects by a significant mar-
gin, where GPT-4o + DALL ·E 3 achieves the best
performance on helpfulness and the average score
of all the aspects. This indicates that building a
strong interleaved generation model for general
purposes remains a significant challenge. Second ,
the pipeline models achieve significantly good per-
formance on text quality since Gemini and GPT-4o
have strong text generation capabilities. Also, the
generated visual prompts are generally coherent
with the text content and they are directly fed into
the image generation model, so the performance
ontext-image coherence of pipeline models is alsoModel Text Quality Perceptual Quality Image Coherence TIC Helpfulness A VG
MiniGPT-5 1.22 2.45 1.62 2.03 1.77 1.82
GILL 0.75 3.21 2.25 1.53 1.48 1.84
EMU-2 1.26 2.28 1.89 1.34 1.64 1.68
EMU-2 (Gold Text) 1.56 3.35 2.89 1.43 2.10 2.27
Gemini1.5 + SDXL 4.40 3.99 3.64 4.13 3.62 3.96
GPT-4o + DALL·E 3 4.37 4.36 3.51 4.55 3.88 4.13
Table 2: Automatic evaluation results of existing interleaved generation models on INTERLEAVED BENCH using
INTERLEAVED EVAL based on GPT-4o. TIC is the abbreviation for ’Text-Image Coherence’. The best results are
highlighted in bold .
Model Text Quality Perceptual Quality Image Coherence TIC Helpfulness A VG
GILL 1.35 1.89 1.72 1.43 1.19 1.52
EMU-2 1.23 1.74 1.87 1.24 1.2 1.46
Gemini1.5 + SDXL 2.59 2.36 2.13 2.27 2.08 2.28
GPT-4o + DALL·E 3 2.49 2.51 2.02 2.31 2.13 2.29
Table 3: Human evaluation results of existing interleaved generation models on INTERLEAVED BENCH . TIC is the
abbreviation for ’Text-Image Coherence’. The best results are highlighted in bold. Note that we use a scale of 0 to 3
for this evaluation, which is different from the scale used in Table 2.
remarkable. Third , we observe that the common
errors of integrated models include the output text
and/or images being empty, in poor quality, or hav-
ing severe duplication. This is probably due to
their weak instruction-following abilities. Fourth ,
image coherence is the most challenging aspect
for the pipeline models. This is because the im-
age generation model cannot take the images in
the input context or previously generated images
as conditions. Thus, the generated images do not
have strong coherence.
Given the closed-source nature of GPT-4o, the
evaluation based on GPT-4o can be less transparent
and sometimes may not be fully reproducible. To
this end, we also implement our INTERLEAVED E-
VAL using the current state-of-the-art open-sourced
LMM, i.e., LLaV A-NeXT-Interleave (Li et al.,
2024b), which supports interleaved text and im-
age inputs. We report the results in Table 10 in
Appendix C.
5.3 Human Evaluation
In addition to automatic evaluation, we also con-
duct an extensive human evaluation to benchmark
the baselines and also provide a meta-evaluation
on our INTERLEAVED EVAL and other evaluation
metrics by computing the correlation between au-
tomatic evaluation scores and human judgments.
Human Evaluation Setup We adopt the same
fine-grained evaluation criteria as INTERLEAVED E-VAL, where for each sample, the annotators need
to give a score for each aspect defined in Table 8.
The only difference is that, instead of rating on a
scale of {0, 1, 2, 3, 4, 5}, we use a scale of {0, 1,
2, 3} for each aspect, where 1, 2, and 3 indicate
the quality is bad, fair , and good , respectively. In
this way, we can reduce the difficulty of human
evaluation and improve its efficiency. Due to the
cost of human evaluation, we select four represen-
tative baselines to evaluate, i.e., GILL, EMU-2,
Gemini1.5 + SDXL, and GPT-4o + DALL ·E 3. We
include more details on human evaluation setup in
Appendix B.1.
Results We show the human evaluation results in
Table 3. The human evaluation is generally consis-
tent with the automatic evaluation in Table 2. The
pipeline models consistently outperform integrated
models by a large margin, where GPT-4o+DALL ·E
3 also achieves the best performance on helpful-
ness and the average performance. There’s sig-
nificant room for improvement in the integrated
open-sourced models. We report the Inter Annota-
tor Agreement (IAA) in Table 9 in Appendix B.1.
Correlation Analysis To validate the effective-
ness of our proposed metric, we conduct a correla-
tion analysis by comparing the evaluation results
from automatic metrics with our human evalua-
tion results. Since the baseline metrics only pre-
dict an overall score for each instance, we use the
same set of evaluation scores to compare againstMetric Ref-free? Text Quality Perceptual Quality Image Coherence TIC Helpfulness
BERTScore ✗ 0.21 - - - 0.37
DreamSim ✗ - 0.02 0.1 - 0.06
Image-Image CLIPScore ✗ - 0.08 0.2 - -0.01
Text-Image CLIPScore ✓ - - - 0.2 0.09
INTERLEAVED EVAL-LLaV A ✓ 0.06 0.32 0.24 0.23 0.3
INTERLEAVED EVAL-GPT-4o ✓ 0.72 0.30 0.43 0.4 0.57
Table 4: Mete-evaluation on evaluation metrics in terms of Spearman correlation between automatic evaluation
results with human judgments. For baseline metrics, we only report the correlation on the corresponding aspects
(e.g., BERTScore can correspond to text quality ) as well as helpfulness .
Instruction: Describe a doe's protective behavior towards her fawn. Context: Fawns usually don't bleat unless distressed <image>. A distressed fawn call prompts the mother doe <image>. Output: mate and produce offspring <image>.InputGILLOutput: Fawns are born with a spotted coat that provides camouflage <image>.EMU-2Output: This behavior is rooted in the harsh realities of environment <image>. She keeps her fawn hidden in thick vegetation and only visits it for feeding <image>.
Gemini+SDXLGPT-4+DALLE3
Output: The bond between a doe and fawn creates security <image>. This is vital as fawns learn survival skills from interaction with their mothers <image>.
Instruction: You need to help user to complete the task ‘How to Remove Banana Stains from Fabric’.Context: Scrape off excess banana pieces <image>.
Output: the banana pieces are stuck to the fabric <image>.Output: Soak the fabric in cold water. Soak the fabric in warm water.<image>.
Output: Rinse the Stained Area <image>. Apply Liquid Laundry Detergent <image>. Gently Rub the Detergent into the Fabric <image>. 
Output: Rinse the stain with cold water <image>. Apply a stain remover to the stain <image>. Wash the garment <image>. 
Instruction: Generate a visual story with the following description: The Hidden Library: A library that holds all the knowledge of the world appears only to those who seek it, and a young scholar named Elara must find it to save her kingdom.
Output:
Output:Output: Elara ventures into the ancient forest <image>. Elara stumbles upon a hidden cave <image>. She speaks the incantations <image>. Output: The king, Elara‘s father, was seeking solace in the royal library <image>. She traversed treacherous mountains <image>. This was the Hidden Library <image>.
Figure 5: Case study. We select the representative examples of the system outputs from GILL, EMU-2, Gem-
ini+SDXL, and GPT-4+DALLE3.
the human rating on each aspect separately. For IN-
TERLEAVED EVAL, we compare evaluation scores
with the human rating on corresponding aspects.
Since most baselines require a gold reference, we
use the context-based subset, where each instance
has an associated reference output, to compute the
correlation. From Table 4, our INTERLEAVED E-
VAL consistently outperforms previous metrics by
a significant margin in every aspect. Our metric
has a particularly higher correlation on text quality ,
which is because text quality is relatively easier
to evaluate with large language models like GPT-
4o (Zheng et al., 2023b). Our metric achieves the
lowest correlation on perceptual quality . The plau-
sible reason is that GPT-4o’s perceptual recognition
capability is still not strong enough to accuratelydetect visual artifacts or unnatural disruptions in
the images (Fu et al., 2024b). We also find that
baseline metrics generally achieve poorer corre-
lation, e.g., most metrics except for BERTScore
almost do not have any correlation with helpful-
ness. BERTScore achieves the best correlation on
helpfulness among baseline metrics, which indi-
cates that text quality could be a good indicator of
whether the overall interleaved content is helpful.
In addition, we also report the correlation with
human judgments of InterleavedEval based on the
open-sourced LLaV A-NeXT-Interleave in Table 4.
InterleavedEval-LLaV A can achieve promising cor-
relations with humans, generally surpassing pre-
vious metrics by a large margin. While there is
still a significant gap between the performanceFigure 6: Radar figures of evaluation results on each evaluation aspect for each task.
of GPT-4o and LLaV A, probably due to the lim-
ited capability of LLaV A-NeXT-Interleave, we be-
lieve our InterleavedEval-LLaV A is a good alter-
native to InterleavedEval-GPT-4o in the scenarios
where transparency and reproducibility are highly
desired. We leave how to build a more powerful
open-sourced evaluator for future work.
6 Discussions
Qualitative Analysis We conduct a qualitative
analysis of benchmarked models in Figure 5 and
have the following observations: (1) while GILL
can generate images with reasonable quality, the
generated text and images are typically not coher-
ent with the instruction and context. In the example
in the first row, the generated text is totally irrel-
evant to the task, while the image is also incon-
sistent with input images. (2) EMU-2 can often
generate text that is relevant to the task, but the
quality is not good enough. In the example in the
second row, it repeatedly says “soak the fabric in
water” but does not contain other useful content.
Another weakness of EMU-2 is its poor conditional
image generation capability, where generated im-
ages have obvious visual distortions and could be
duplicated with input images. (3) On the other
hand, the pipeline models can generally better fol-low the instructions and generate text and images in
higher quality. Nevertheless, they still occasionally
have some drawbacks. For Gemini+SDXL, some
of the generated images (e.g., the first output image
in the second example) still have obvious defects.
For GPT-4+DALLE3, the style of generated images
can be dramatically different from input images, as
DALLE3 is prone to generate images in cartoon or
dramatic styles. (4) Maintaining image coherence,
i.e., the coherence of style and entities across im-
ages, is still very challenging for most models. In
the third example, for the pipeline models, the same
character has a very different appearance across the
images, which makes the content inconsistent. (5)
For the instances on the context-free subset, the
integrated baselines have significantly worse per-
formance, where they only generate one image with
extremely poor quality. We hypothesize the reason
is that those models cannot truly understand and
follow the instructions. To sum up, our qualitative
analysis indicates there is still significant room for
improvement in interleaved generation.
Breakdown Results on Each Use Case We show
a detailed breakdown of the average results on all
the aspects of each use case. From Figure 6, we ob-
serve that (1) for pipeline-based models, image edit-Model Text Quality Perceptual Quality Image Coherence TIC Helpfulness A VG
MiniGPT5 1.29 3.47 2.04 2.64 1.76 2.24
GILL 1.37 3.96 2.01 2.61 1.51 2.29
EMU-2 1.29 2.22 1.65 1.18 1.84 1.64
Gemini1.5+SDXL 3.29 4.24 3.26 3.94 3.25 3.60
GPT-4o+DALLE3 3.12 4.39 3.08 4.36 3.48 3.69
Table 5: Automatic evaluation results of the context-based subset on INTERLEAVED BENCH . TIC is the abbreviation
for ’Text-Image Coherence’. The best results are highlighted in bold .
Model Text Quality Perceptual Quality Image Coherence TIC Helpfulness A VG
MiniGPT5 1.00 1.09 1.07 1.06 1.78 1.20
GILL 0.12 2.23 2.58 0.23 1.45 1.32
EMU-2 0.77 2.35 2.20 1.05 1.38 1.55
Gemini1.5+SDXL 4.50 3.66 4.13 3.98 4.10 4.07
GPT-4o+DALLE3 4.60 4.31 4.05 4.52 4.41 4.38
Table 6: Automatic evaluation results of the context-free subset on INTERLEAVED BENCH . TIC is the abbreviation
for ’Text-Image Coherence’. The best results are highlighted in bold .
Output Steps Text Quality Perceptual Quality Image Coherence TIC Helpfulness A VG
Less 1.8 1.1 1.2 1.3 2.1 1.5
Equal 2.7 3.8 4.0 4.0 3.0 3.5
More 1.7 3.5 2.4 3.3 2.0 2.6
Table 7: Analysis of the number of output steps compared with ground truths.
ing and subject-driven generation achieve the low-
est results, whereas the models can achieve scores
above 4 on other use cases; and (2) integrated
models typically achieve low performance on the
context-free subset in INTERLEAVED BENCH . The
potential reason is that these models did not specif-
ically fine-turned on the data with text-only inputs,
and thus cannot generate interleaved content well.
Breakdown Performance on Context-based and
Context-free Subsets We show the breakdown
performance on the context-based and context-free
subsets of INTERLEAVED BENCH in Table 5 and Ta-
ble 6. Our findings are: (1) pipeline baselines con-
sistently outperform integrated baselines on both
subsets; (2) pipeline baselines have better perfor-
mance on the context-free subset than the context-
based subset, while integrated baselines have better
performance on the context-based subset than the
context-free subset. Based on the results and our
observations, we find the following reasons that
could contribute to the discrepancy in performance:
(1) pipeline approaches first generate the text along
with captions with target images, which can be
considered as a planning stage to provide the ba-
sis on what images should be generated, makinggenerated interleaved content more useful and rea-
sonable; (2) using separate models (LLMs for text
generation and T2I models for image generation)
facilitates the generation of high-quality content in
each modality; (3) Existing integrated models may
struggle with the context-free subset because they
haven’t been trained on data with text-only inputs
and interleaved multimodal outputs.
Impact of the Number of Output Steps We con-
duct an analysis of how the number of output steps
affects the performance when compared with that
in the ground truths. We calculate the performance
of GPT4o-DALLE3 under three cases: the number
of predicted steps is less, equal to, or larger than
that in the ground truth (“Less”, “Equal”, “More”).
From Table 7, when the number of predicted steps
is less than the ground truths, the model perfor-
mance is generally worse. This indicates that in-
stances with fewer steps are considered as lower
quality and less helpful. When the model has more
output steps than ground truths, the performance
on text quality, image coherence, and helpfulness
are lower. This is because we observed the models
produce more images than necessary. Often, these
output images are repetitive of the input images orpreviously generated images. Since we explicitly
penalize such repetition in our evaluation criteria,
the performance for these cases is lower.
7 Conclusion
We introduce INTERLEAVED BENCH , the first
benchmark for the evaluation of interleaved text-
and-image generation. We also propose INTER -
LEAVED EVAL, a strong multi-aspect reference-free
evaluation metric based on GPT-4o. With extensive
experiments, we first verify that our proposed met-
ric can achieve significantly higher agreement with
humans compared with existing metrics. Through
the lens of INTERLEAVED EVAL, we then observed
that while the pipeline models based on proprietary
LMMs consistently outperform open-source mod-
els, interleaved generation is still a challenging task
that requires further advancement.
8 Limitation
While our proposed INTERLEAVED BENCH andIN-
TERLEAVED EVALprovide a comprehensive evalua-
tion suite for text-and-image interleaved generation,
there are still several limitations in our work that
we leave for future research. First, while INTER -
LEAVED EVAL achieves the best correlation with
human judgments among other evaluation metrics,
it still does not have a high correlation on certain
aspects, such as perceptual quality, image coher-
ence, and text-image coherence. To further im-
prove the evaluation accuracy, we may need to
improve the capability of foundation multimodal
models such that they are capable of recognizing
subtle but critical differences. Second, our work
did not extensively address the bias in using GPT-
4 for evaluation, which we consider an important
topic for future research.
Acknowledgement
This research is partially supported by a research
award from Intuit AI Research, the award No.
2238940 from the Faculty Early Career Develop-
ment Program (CAREER) of the National Science
Foundation (NSF), and the U.S. DARPA ECOLE
Program #HR001122S0052. The views and con-
clusions contained herein are those of the authors
and should not be interpreted as necessarily rep-
resenting the official policies, either expressed or
implied, of the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distributereprints for governmental purposes notwithstand-
ing any copyright annotation therein.
References
Nantheera Anantrasirichai and David Bull. 2022. Artifi-
cial intelligence in the creative industries: a review.
Artificial intelligence review , 55(1):589–656.
Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-
Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-
lican, David Silver, Slav Petrov, Melvin Johnson,
Ioannis Antonoglou, Julian Schrittwieser, Amelia
Glaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-
crap, Angeliki Lazaridou, Orhan Firat, James Molloy,
Michael Isard, Paul Ronald Barham, Tom Henni-
gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,
Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens
Meyer, Eliza Rutherford, Erica Moreira, Kareem
Ayoub, Megha Goel, George Tucker, Enrique Pi-
queras, Maxim Krikun, Iain Barr, Nikolay Savinov,
Ivo Danihelka, Becca Roelofs, Anaïs White, Anders
Andreassen, Tamara von Glehn, Lakshman Yagati,
Mehran Kazemi, Lucas Gonzalez, Misha Khalman,
Jakub Sygnowski, and et al. 2023. Gemini: A fam-
ily of highly capable multimodal models. CoRR ,
abs/2312.11805.
James Betker, Gabriel Goh, Li Jing, †TimBrooks,
Jianfeng Wang, Linjie Li, †LongOuyang, †Jun-
tangZhuang, †JoyceLee, †YufeiGuo, †Wesam-
Manassra, †PrafullaDhariwal, †CaseyChu, †Yunx-
inJiao, and Aditya Ramesh. Improving image gener-
ation with better captions.
Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jian-
feng Gao, and Chunyuan Li. 2023. Llava-interactive:
An all-in-one demo for image chat, segmentation,
generation and editing. CoRR , abs/2311.00571.
Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz,
Xuhui Jia, Ming-Wei Chang, and William W Cohen.
2024. Subject-driven text-to-image generation via
apprenticeship learning. Advances in Neural Infor-
mation Processing Systems , 36.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven C. H. Hoi.
2023. Instructblip: Towards general-purpose vision-
language models with instruction tuning. CoRR ,
abs/2305.06500.
Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi,
Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun,
Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xi-
angyu Zhang, Kaisheng Ma, and Li Yi. 2024. Dream-
LLM: Synergistic multimodal comprehension and
creation. In The Twelfth International Conference on
Learning Representations .
Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie
Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia,and Willie Neiswanger. 2024a. Isobench: Bench-
marking multimodal foundation models on isomor-
phic representations. In First Conference on Lan-
guage Modeling .
Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu
Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-
Chiu Ma, and Ranjay Krishna. 2024b. BLINK: multi-
modal large language models can see but not perceive.
CoRR , abs/2404.12390.
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan
Le Bras, and Yejin Choi. 2021. CLIPScore: A
reference-free evaluation metric for image captioning.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
7514–7528, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. 2017. Gans
trained by a two time-scale update rule converge to a
local nash equilibrium. Advances in neural informa-
tion processing systems , 30.
Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh,
Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross
Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Ba-
tra, et al. 2016. Visual storytelling. In Proceedings
of the 2016 conference of the North American chap-
ter of the association for computational linguistics:
Human language technologies , pages 1233–1239.
Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max
Ku, Qian Liu, and Wenhu Chen. 2024. MANTIS:
interleaved multi-image instruction tuning. CoRR ,
abs/2405.01483.
Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov.
2023. Generating images with multimodal language
models. In Advances in Neural Information Pro-
cessing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023,
New Orleans, LA, USA, December 10 - 16, 2023 .
Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,
and Juan Carlos Niebles. 2017. Dense-captioning
events in videos. In International Conference on
Computer Vision (ICCV) .
Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and
Wenhu Chen. 2023. Viescore: Towards explainable
metrics for conditional image synthesis evaluation.
CoRR , abs/2312.14867.
Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu,
Wenwen Zhuang, and Wenhu Chen. 2024. Imagen-
hub: Standardizing the evaluation of conditional im-
age generation models. In The Twelfth International
Conference on Learning Representations .
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. 2023a. Multi-concept
customization of text-to-image diffusion.Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. 2023b. Multi-concept
customization of text-to-image diffusion. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 1931–1941.
Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui
Wang, Ruimao Zhang, and Ying Shan. 2024a. Seed-
bench: Benchmarking multimodal large language
models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages
13299–13308.
Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang,
Bo Li, Wei Li, Zejun Ma, and Chunyuan Li.
2024b. Llava-next-interleave: Tackling multi-image,
video, and 3d in large multimodal models. CoRR ,
abs/2407.07895.
Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. 2023.
Dreamedit: Subject-driven image editing. Preprint ,
arXiv:2306.12624.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023a. Visual instruction tuning. CoRR ,
arXiv:2304.08485.
Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eu-
nah Cho, Vaibhav Kumar, Reza Ghanadan, and Lifu
Huang. 2024. X-eval: Generalizable multi-aspect
text evaluation via augmented instruction tuning with
auxiliary evaluation aspects. In Proceedings of the
2024 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (Volume 1: Long Pa-
pers) , pages 8560–8579, Mexico City, Mexico. Asso-
ciation for Computational Linguistics.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023b. G-eval:
NLG evaluation using GPT-4 with better human
alignment. CoRR , abs/2303.16634.
Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang
Wang, Yejin Choi, and Bill Yuchen Lin. 2024.
Wildvision: Evaluating vision-language models
in the wild with human preferences. CoRR ,
abs/2406.11069.
Stephanie M Lukin, Reginald Hobbs, and Clare R V oss.
2018. A pipeline for creative visual storytelling.
arXiv preprint arXiv:1807.08077 .
OpenAI. 2024. Hello gpt-4o. https://openai.com/
index/hello-gpt-4o/ . Accessed: 2024-05-26.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics , pages 311–318.Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,
and Robin Rombach. 2023. SDXL: improving latent
diffusion models for high-resolution image synthesis.
CoRR , abs/2307.01952.
Jingyuan Qi, Minqian Liu, Ying Shen, Zhiyang Xu, and
Lifu Huang. 2024. Multiscript: Multimodal script
learning for supporting open domain everyday tasks.
Proceedings of the AAAI Conference on Artificial
Intelligence , 38(17):18888–18896.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea V oss, Alec Radford, Mark Chen, and
Ilya Sutskever. 2021. Zero-shot text-to-image gener-
ation. In International conference on machine learn-
ing, pages 8821–8831. Pmlr.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba,
Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen.
2016. Improved techniques for training gans. In
Advances in Neural Information Processing Systems ,
volume 29. Curran Associates, Inc.
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang,
Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming
Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
2023a. Generative multimodal models are in-context
learners. CoRR , abs/2312.13286.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang,
Xiaosong Zhang, Yueze Wang, Hongcheng Gao,
Jingjing Liu, Tiejun Huang, and Xinlong Wang.
2023b. Generative pretraining in multimodality.
CoRR , abs/2307.05222.
Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun
Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei
Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao,
and Jifeng Dai. 2024. Mm-interleaved: Interleaved
image-text generative modeling via multi-modal fea-
ture synchronizer. CoRR , abs/2401.10208.
Fei Wang, Xingyu Fu, James Y . Huang, Zekun Li, Qin
Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu,
Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wen-
jie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan
Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng
Zhang, Hoifung Poon, and Muhao Chen. 2024. Muir-
bench: A comprehensive benchmark for robust multi-
image understanding. CoRR , abs/2406.09411.
Zhiyang Xu, Ying Shen, and Lifu Huang. 2023. Multiin-
struct: Improving multi-modal zero-shot learning via
instruction tuning. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), ACL 2023, Toronto,
Canada, July 9-14, 2023 , pages 11445–11465. Asso-
ciation for Computational Linguistics.
Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang,
Mark Yatskar, and Chris Callison-Burch. 2021. Vi-
sual goal-step inference using wikiHow. In Proceed-
ings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pages 2167–2179,
Online and Punta Cana, Dominican Republic. Asso-
ciation for Computational Linguistics.Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou,
Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan,
Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui,
Tianxiang Sun, Yugang Jiang, and Xipeng Qiu. 2024.
Anygpt: Unified multimodal LLM with discrete se-
quence modeling. CoRR , abs/2402.12226.
Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and
Yu Su. 2023a. Magicbrush: A manually annotated
dataset for instruction-guided image editing. In Ad-
vances in Neural Information Processing Systems .
Richard Zhang, Phillip Isola, Alexei A. Efros, Eli
Shechtman, and Oliver Wang. 2018. The unreason-
able effectiveness of deep features as a perceptual
metric. In 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 586–595.
Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan,
Lianke Qin, Heng Wang, Xifeng Yan, William Yang
Wang, and Linda Ruth Petzold. 2023b. Gpt-4v(ision)
as a generalist evaluator for vision-language tasks.
CoRR , abs/2311.01361.
Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023a.
Minigpt-5: Interleaved vision-and-language genera-
tion via generative vokens. CoRR , abs/2310.02239.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023b. Judging
LLM-as-a-judge with MT-bench and chatbot arena.
InThirty-seventh Conference on Neural Information
Processing Systems Datasets and Benchmarks Track .
Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu
Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and
Jiawei Han. 2022. Towards a unified multi-
dimensional evaluator for text generation. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing , pages 2023–
2038, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.AMore Details on INTERLEAVED BENCH
Data Filtering Pipeline To collect the source
data from web resources, we first only keep the
samples with 3 to 6 images and less than 12 sen-
tences such that the ratio between text and image
is balanced. We then apply Llama-8B-Instruct as
a text filter to save the data with good text quality.
We also apply LPIPS (Zhang et al., 2018) to discard
the instances with duplicate images.
Manual Data Selection We apply a manual data
selection and instruction annotation process to en-
sure data quality. We select the instances based
on the criteria in Table 8. We also encourage the
annotators to select diverse instances.
Instruction Annotation For each instance, we
first ask an annotator to draft an instruction, and
then ask another annotator to revise the instruction,
until both annotators agree that the instructions are
of high quality. The annotators are Ph.D. students
with expertise in NLP and multimodal learning
areas.
B More Details on Evaluation
We present the full list of our defined aspects and
their definition in Table 8.
B.1 Human Evaluation
More Details on Human Evaluation Setup We
sampled 100 instances from INTERLEAVED BENCH
as a subset for evaluation and ensure its task dis-
tribution is the same as the original distribution.
In this way, we have 400 data points where each
baseline has inference results on 100 instances. For
each data point, we have two different annotators
who are Ph.D. or master’s students with expertise
in NLP or multimodal domains to give ratings in-
dependently.
Inter-Annotator Agreement We show the IAA
of our human evaluation in Table 9. The inter-
annotator agreement is reasonably good. Note that
the evaluation of interleaved generation is still quite
subjective, open-ended, and challenging, even with
our carefully designed human evaluation aspects
and guidelines.C Additional Experiment Results
C.1 Automatic Evaluation Results on based
on LLaV A-NeXT-Interleave
We report the automatic evaluation results in Ta-
ble 10 based on the existing state-of-the-art open-
sourced LMM that supports interleaved text and
image inputs, i.e., LLaV A-NeXT-Interleaved (Li
et al., 2024b), in Table 10. We use the same evalua-
tion instructions and criteria to prompt the model
to predict numerical scores from 1 to 5. We show
the automatic evaluation results in Table A and the
correlation analysis in Table B. We use the same
experiment setup for a fair comparison. From Ta-
ble 10, the benchmarked performance using Inter-
leavedEval with LlaV A-NeXT-Interleaved gener-
ally aligns with human evaluation in Table 3 and
automatic evaluation with GPT-4o in Table 2. For
example, the pipeline-based models consistently
outperformed the integrated baselines, and GPT-
4o-DALLE3 remains the best model overall.Aspect Definition
Text QualityText quality measures how clear, coherent, and error-free the output text is. It considers
grammar, spelling, readability, coherence with the instruction and context, and whether it
contains duplicate content.
Perceptual QualityPerceptual quality measures how visually convincing, natural, and free from distortions or
artifacts a generated image appears. It considers how accurately the image mimics reality
without unnatural disruptions in structure, colors, or composition.
Image CoherenceImage coherence measures the consistency in style and subject representation across images.
This includes textures, color palette, lighting, rendering styles, and maintaining consistent
physical attributes, clothing, and behavioral traits. Image coherence also penalizes image
duplication, where the output images are too similar, or within the output images themselves.
Text-Image CoherenceText-to-image coherence measure the alignment and integration between textual and visual
elements in a pairwise manner, ensuring they work together to convey a unified and cohesive
narrative.
HelpfulnessHelpfulness measures how well the output text and images follow the task instructions and
provide complete information to achieve the task. It also considers whether the outputs
follow a reasonable logic flow.
Table 8: The full list of evaluation aspects and their corresponding definitions in I NTERLEAVED EVAL.
Text Quality Perceptual Quality Image Coherence TIC Helpfulness A VG
0.689 0.606 0.620 0.627 0.619 0.612
Table 9: Inter-Annotator Agreement of human evaluation in terms of Cohen’s Kappa score.
Model Text Quality Perceptual Quality Image Coherence TIC Helpfulness A VG
MiniGPT-5 2.52 2.22 2.28 1.68 2.59 2.26
GILL 1.60 3.26 3.09 1.50 3.08 2.51
EMU-2 2.86 2.41 2.44 1.66 3.11 2.50
EMU-2 (Gold Text) 1.44 3.31 3.30 1.51 3.25 2.56
Gemini1.5+SDXL 3.70 3.86 3.79 3.73 3.78 3.77
GPT-4o+DALLE3 3.61 4.16 3.93 3.82 3.87 3.88
Table 10: Automatic evaluation results of existing interleaved generation models on INTERLEAVED BENCH using
INTERLEAVED EVAL based on open-sourced LLaV A-NeXT-Interleave. TIC is the abbreviation for ’Text-Image
Coherence’. The best results are highlighted in bold .