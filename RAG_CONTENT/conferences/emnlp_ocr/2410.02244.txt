Visual Prompting in LLMs for Enhancing Emotion Recognition
Qixuan Zhang1*Zhifeng Wang1*Dylan Zhang2Wenjia Niu3
Sabrina Caldwell1Tom Gedeon1,4Yang Liu1†Zhenyue Qin5†
1Australian National University2Quriosity Pty Ltd
3Webumate Pty Ltd4Curtin University5Yale University
{qixuan.zhang, zhifeng.wang, sabrina.caldwell}@anu.edu.au
info@quriosity.com.au ,niuwenjia9064@hotmail.com
tom.gedeon@curtin.edu.au ,{yang.liu1082, kf.zy.qin}@gmail.com
Project Page: https://wangzhifengharrison.github.io/sov_prompts_emnlp
Abstract
Vision Large Language Models (VLLMs) are
transforming the intersection of computer vi-
sion and natural language processing. Nonethe-
less, the potential of using visual prompts for
emotion recognition in these models remains
largely unexplored and untapped. Traditional
methods in VLLMs struggle with spatial local-
ization and often discard valuable global con-
text. To address this problem, we propose a
Set-of-Vision prompting (SoV) approach that
enhances zero-shot emotion recognition by us-
ing spatial information, such as bounding boxes
and facial landmarks, to mark targets precisely.
SoV improves accuracy in face count and emo-
tion categorization while preserving the en-
riched image context. Through a battery of
experimentation and analysis of recent com-
mercial or open-source VLLMs, we evaluate
the SoV model’s ability to comprehend facial
expressions in natural environments. Our find-
ings demonstrate the effectiveness of integrat-
ing spatial visual prompts into VLLMs for im-
proving emotion recognition performance.
1 Introduction
As the integration of computer vision and natural
language processing progresses, VLLMs (Dai et al.,
2024) are revolutionizing the way machines inter-
pret visual and textual data. Emotion recognition is
gaining considerable interest across multiple disci-
plines and presents distinct challenges (Yang et al.,
2023a). It requires the decoding of emotions from
nuanced indicators like facial expressions, body
language, and contextual details.
Previous methods, such as those in (Xenos et al.,
2024) and (Zhang et al., 2023b), enhance in-context
emotion classification by training transformer-
based models or CLIP to generate descriptions
of emotions in visual contexts. However, these
*Equal contribution
†Corresponding authors
Identify and locate faces in the image.(1) Box
2Enhance grounding capabilities by box and number(2) Box + Number
123
(3)  Box + Number + Facial LandmarksAnalyze facial expression by spatialrelationships  3
Right Mouth
Left Mouth
Nose
Right Eye
Left EyeFigure 1: Proposed Set-of-Vision (SoV) prompting
approach for enhancing facial expression recognition
in Vision-Language Large Models (VLLMs) . SoV
progressively incorporates (1) bounding boxes to iden-
tify and locate faces, (2) numbered boxes to ground and
differentiate faces, and (3) facial landmarks to analyze
spatial relationships for fine-grained emotion classifica-
tion. This multi-stage visual prompting strategy enables
VLLMs to accurately detect and recognize emotions in
real-world images while preserving global context.
methods overlook the spatial relationships between
different people and facial features within a sin-
gle face. These spatial relationships can be la-
beled with numbers and bounding boxes in our
SoV prompts to guide VLLMs. A person nearby
may have similar facial expressions, while a person
farther away may show different facial expressions.
Additionally, the relationships between the eyes,
mouth, and nose features can be highlighted by
facial landmarks in SoV prompts to guide VLLMs.
Recent studies (Yang et al., 2024; Zou et al.,
2024; Yang et al., 2023b; Zou et al., 2024) have ex-
plored Visual Prompting, a technique employed in
image-language tasks to guide LLMs by incorporat-
ing markers such as colorful boxes or circles to em-
phasize specific targets within an image. ReCLIP
(Subramanian et al., 2022) adds colorful boxes di-
rectly onto an image to highlight specific targets
and blurs other irrelevant areas to reduce the per-
formance gap with supervised models on both real
and synthetic datasets. Additionally, RedCircle
(Shtedritski et al., 2023) employs visual prompt en-
gineering, specifically drawing a red circle around
an object in an image, to direct a Vision-LanguagearXiv:2410.02244v1  [cs.CV]  3 Oct 2024Question: How many visible faces are there in the image? Analyzeeach face's emotion and group the face's emotions.
Answer:In the image, there are 22visible faces.Emotions can be grouped into threemain categories.Neutral Emotion:Approximately 16Faces. This neutral expression is typical in settings requiring attention to a speakeror event, reflecting an engagedyet reserved emotional state.Mildly Positive Emotion:Approximately 5Faces. This could be a response to somethinginterestingor mildly amusinghappening at the event, but their reactionsare subdued, in keeping with the formal setting.Happy: 1Face. This expression stands out as more joyfulor enthusiastic about the proceedings.Answer:There are 18visible faces, each identified by facial landmarks.Neutral Emotion: 9Faces(Nos. 2, 3, 4, 5, 6,7,8,9,16): These faces show little to no expressive facial features that indicate strong emotions. The expressions are mostly calmand the eyebrowsare relaxed.Mildly Positive Emotion: 7Faces(Nos. 1,10, 12, 14, 15, 17, 18): a slight smile. Eyebrowsare slightly raised, and the eyesare a bit brighter, a soft positivefeeling.Smiling or Happy: 2Faces(Nos. 11, 13):Clearly smiling, indicated by the upward curveof the lipsand the crinklingaround the eyes, associated with a genuine smile. Emotion is based on the positionand curvatureof the mouth, opennessof the eyesand eyebrows.
SOV
Figure 2: Comparative analysis of emotion recognition methods in a group setting : assessing the precision
of facial emotion categorization and face detection using plain text prompts versus Set-of-Vision (SoV) prompts
incorporating facial landmarks, bounding boxes, and face enumeration. Top: Results using plain text prompts.
Bottom : Results using Set-of-Vision (SoV) prompts. The use of SoV prompts, such as numbering each face, placing
bounding boxes, and identifying facial landmarks, allows for a more precise analysis.
Model’s attention to that region and enhance its
performance in tasks like zero-shot keypoint local-
ization. However, both of these approaches focus
on local objects and ignore spatial context informa-
tion. Yang et al. (Yang et al., 2024) propose using
fine-grained visual prompts, such as segmentation
masks, and enhancing focus on relevant areas with
a ‘Blur Reverse Mask’ that blurs regions outside the
target mask to minimize distractions and maintain
spatial context. Although visual prompting tech-
niques have garnered interest, their full potential
remains unexplored for emotion recognition tasks.
Current approaches rely solely on coarse mark-
ers like colorful boxes, circles, or masks, which
can introduce ambiguity, blur the face images, and
pose challenges for accurate recognition tasks. This
paper addresses these issues by systematically or-
ganizing and investigating various forms of visual
prompting. Furthermore, we propose a new prompt-
ing approach called Set-of-Vision prompting (SoV)
in Fig. 1, which utilizes spatial information such as
numbers, bounding boxes, and facial landmarks to
precisely mark each target while maintaining back-
ground context, thereby enhancing the zero-shot
performance of facial expression recognition. The
top of Fig. 2, shows an approach where specific vi-sion prompts are not used. As a result, the analysis
inaccurately counts 22 visible faces and misclas-
sifies persons’ emotions into incorrect categories,
with a number of faces labeled under ‘Neutral Emo-
tion’ and fewer under ‘Mildly Positive Emotion’
and ‘Happy’. This misclassification and miscount
demonstrate the limitations when detailed visual
cues are not utilized in the analysis. In the bot-
tom of Fig. 2, the use of SoV prompts, such as
numbering each face, placing bounding boxes, and
identifying facial landmarks, allows for a more
precise analysis. The correct number of faces is
identified (18), and the emotions are accurately
categorized into more nuanced groups: ‘Neutral
Emotion’, ‘Mildly Positive Emotion’, and ‘Smil-
ing or Happy’. This method provides a clearer
and more detailed breakdown of each individual’s
emotional state based on visible facial expressions.
This comparison highlights the importance and ef-
fectiveness of integrating visual prompts in VLLMs
analysis for more accurate and detailed recognition
and categorization of human emotions in images.
To summarize, our main contributions are: (1)
The paper introduces a novel visual prompting
method (SoV) that highlights facial regions di-
rectly within the entire image. This preserves back-
ground context, enhancing the ability of VLLMsInput 1:How many visible faces in the image?Input 2:What’s emotion for each face?
Input
SOV PromptingStep 1: To identify and locate faces in the image.Face detection
Step 2: To number each face.
NumberfacesStep 3: To detectand extract facial landmarks. Landmarks detection
Step 4: To analyze facial expression by using position of nose, eye and mouth. Spatial relationship
Human detection
Face detection
Add boxesAdd numbers
Add faciallandmarks
Spatialrelationshipmodels
Output 1:2Output 2:Face1: Happy Face2: Neutral 
Figure 3: Workflow diagram for enhanced face recognition and emotion analysis using the Set-of-Vision
(SoV) prompting approach : a multi-step process involving face detection, face numbering, landmark extraction,
and spatial relationship analysis for emotion classification. Each detected face is analyzed and identified by facial
landmarks on the face, such as the positions of the nose, eyes, mouth, and other facial features.
to perform accurate emotion recognition without
the need for cropping faces, thus maintaining the
holistic view of the image. (2) The proposed face
overlap handling algorithm effectively addresses
conflicts arising from overlapping face detections,
especially in images with dense face clusters. By
prioritizing larger faces and iteratively checking for
overlaps, the algorithm ensures that non-occluded
faces are retained for subsequent emotion analy-
sis. (3) Our results show that incorporating spatial
visual prompts (SoVs) into VLLMs can enhance
their performance in recognizing emotions.
2 Related Work
2.1 Vision Large Language models
LLMs such as LLaMA (Touvron et al., 2023),
ChatGPT-3 (Brown et al., 2020), ChatGPT-4
(Achiam et al., 2023), and PaLM (Chowdhery
et al., 2023) have demonstrated remarkable zero-
shot transfer capabilities in natural language pro-
cessing. Recently, VLLMs, which leverage image-
text data pairs from the web, have gained promi-
nence in the computer vision domain. MiniGPT-4
(Zhu et al., 2023), a model that combines a visual
encoder with an advanced language model, can en-
able multi-modal capabilities such as generating
detailed image descriptions and designing websitesfrom sketches. Video-LLaV A (Zhang et al., 2023a)
is a multi-modal framework that enhances Large
Language Models with the ability to understand
and generate responses based on both visual and au-
ditory content in videos. LLaV A (Liu et al., 2023)
is a newly developed, end-to-end trained, large mul-
timodal model that combines a vision encoder with
a language model, demonstrating promising abili-
ties in multimodal chat. Although VLLMs exhibit
remarkable capabilities in vision-based tasks such
as image segmentation and object detection, they
typically require fine-tuning of the vision and text
encoders using existing open vocabulary methods
when applied to specific tasks. In contrast, this pa-
per proposes a zero-shot architecture for emotion
recognition, overcoming the need for task-specific
fine-tuning.
2.2 Prompting methods
Prompt engineering is a widely employed tech-
nique in the field of NLP (Strobelt et al., 2022;
Zhou et al., 2024). AdbGPT (Feng and Chen, 2024)
is a novel, lightweight approach that leverages few-
shot learning and chain-of-thought reasoning in
Large Language Models to automatically repro-
duce bugs from bug reports, mimicking a devel-
oper’s problem-solving process without the needfor training or hard-coding. Although prompts for
large language models have been extensively ex-
plored, prompts for vision tasks have received less
attention and investigation. Yang et al. (Yang et al.,
2024) propose using fine-grained visual prompts
like segmentation masks and a Blur Reverse Mask
strategy to focus on relevant areas. The Image-
of-Thought (IoT) prompting method (Zhou et al.,
2024) enhances Multimodal Large Language Mod-
els by guiding them to extract and refine visual
rationales step-by-step from images, combining
visual and textual insights to improve zero-shot
performance on complex visual reasoning tasks.
Although these methods have shown promise in
tasks like semantic segmentation and object ground-
ing, their performance in emotion recognition has
been less effective. This is largely because they
tend to analyze individual objects in isolation, over-
looking global information and specific facial fea-
tures, which are crucial for accurately interpreting
emotions. To address these issues, the proposed
approach directly focuses on the fine-grained fa-
cial features present in the entire image, preserving
spatial information by utilizing bounding boxes,
numbers, and facial landmarks.
3 Methods
3.1 Problem Definition
The task of matching images to emotions for
each visible face in a given image involves sev-
eral sophisticated steps, combining face detec-
tion and emotion recognition. Typically, the
VLLMs, denoted as Φ, will take an image I∈
RH×W×3and a text question of length li,Qi=
[qi
1, qi
2, ..., qi
li], as input. The output is a sequence
of answers with length lo, containing emotions,
Ao= [ao
1, ao
2, ..., ao
lo], which can be formulated as
(Eq. 1):
Ao= Φ(I, Qi) (1)
In our task, we aim to find the best matching
image-emotion pairs ( I,Ao) for each visible face.
Traditionally, this involves cropping the face from
the image using face detectors. However, with the
introduction of visual prompting, faces can be di-
rectly marked on the entire image, highlighting the
facial region while preserving the background con-
text and avoiding the obscuration of faces. With
this in mind, we have developed Set-of-Vision
prompts (SoV), a simple method of overlaying a
number of visual prompts on the facial regions in
an image.
Thelabel of box isoverlapped to wrong face.
Theface is partially visible due to multiple face overlap.
The facial landmarks are overlapped to wrong face.Figure 4: Face detection inevitably introduces some
overlaps or conflicts that confuse VLLMs. Analyz-
ing the impact of face overlaps, occlusions, landmark
misalignment, and bounding box conflicts for emotion
recognition.
Algorithm 1 Detect and Handle Overlaps
1: // Define a function to check if overlap
2:defBoxes_overlap( B1, B2):
3: ˜O=Check _Overlap (B1, B2)// Check if overlap
4: return ˜O
5: // Define a function to calculate the area
6:defFace_size(B):
7: ˜A=Calculate _Area (B)// Calculate the area
8: return ˜A
9: // Main function
10:defDetect_and_handle_overlaps(faces):
11: // Sort faces by the area in descending order
12: ˜F=Sorted (faces )
13: // Initialize an empty list
14: visible_faces = []
15: forkin range(K)
16: ifBoxes_overlap( ˜F[k],˜F[k−1]):
17: ifFace_size( ˜F[k])>Face_size( ˜F[k−1]):
18: // Replace the smaller face
19: visible_faces =¬˜F[k−1]∧˜F[k]
20: return visible_faces
This operation augments the input image Ito
a new image Inew=SoV(I), while keeping the
text prompts to VLLMs unchanged as shown in
Fig. 2. It can be formulated as (Eq. 2):
Ao= Φ(SoV(I), Qi) (2)
3.2 Set of Vision Prompts
3.2.1 Box detection
Once the image is obtained, we need to generate
visual prompts for the image that will be utilized by
VLLMs for emotion recognition. We employ the
RetinaFace (Deng et al., 2020) algorithm to detect
faces within the image. Let B={b1, b2, . . . , b n}
denote the set of detected face bounding boxes,
where birepresents the i-th face bounding box. The
process can be formulated as (Eq. (3)):
bi=D(I, θi) (3), where Iis the input image; θirepresents the hy-
perparameters for the RetinaFace model D; and bi
corresponds to the i-th face bounding box.
3.2.2 Box Overlap Handling Algorithm
However, this face detection algorithm inevitably
introduces some overlaps or conflicts that confuse
VLLMs, especially in images with densely pop-
ulated faces, such as when two faces overlap in
one area or one face is obscured by another. This
is illustrated in Fig. 4. To mitigate this problem,
we propose a face overlap handling algorithm, as
shown in Algorithm. 1. Given the set of boxes
B={b1, b2, . . . , b n}, we first calculate the area
for each bounding box bi, then sort the detected
faces biby their area in descending order (line 12)
(Eq. (4)):
Bsorted ={b1, b2, . . . , b n} (4)
, where Area(b1)≥Area(b2)≥. . .≥
Area(bn), ensuring that larger faces are prioritized.
By iterating through the sorted faces, the algo-
rithm checks for overlaps and compares the areas of
overlapping faces. For each face ˜F[k]inBsorted ,
it checks if ˜F[k]overlaps with any face ˜F[j]in
Ffinal by (Eq. (5)):
Overlap (˜F[k],˜F[j]) =
Area(˜F[k]∩˜F[j])
min( Area(˜F[k]),Area(˜F[j]))> ϵ(5)
If the overlap is significant, compare their areas
and discard the smaller face by (Eq. (6)):
˜F[k] =(
ˆF[k]if Area (ˆF[k])>Area(ˆF[j]),
ˆF[j]otherwise .(6)
Add non-occluded faces to Ffinal by (Eq. (7)):
Ffinal←Ffinal∪ {˜F[k]} (7)
It ensures that only the faces that are close to the
camera and not obstructed remain in the final list.
Once we determine the location of boxes B=
{b1, b2, . . . , b n}for each face, We need to as-
sign a unique ID to each face. The unique ID
N={1,2, . . . , n }will be used to locate each face
in the image, where nis the number of detected
faces. Thus, the set of vision prompts becomes
distinguishable and can be effectively interpreted
by VLLMs.3.2.3 Facial Landmarks Detection and
Analysis
After identifying and handling overlapping faces,
we proceed with the detection and extraction of
facial landmarks for each face. For each face bi∈
Ffinal :
Li= Θ( bi)
={(x1, y1),(x2, y2), . . . , (xm, ym)}(8)
, where Θis landmarks extraction model, Liis the
facial landmarks, (xm, ym)is coordinates of facial
landmarks. For each set of landmarks Li, analyze
spatial relationships for facial expression within
RGB image:
E(Li) =f(R(Li)) (9)
, where fis a function mapping spatial relation-
ships R(Li)to facial expressions E(Li). The en-
tire process is illustrated in Fig. 3.
3.3 Text and Vision Prompts
We have a collection of npairs of location-vision
prompts, represented as (l1, v1), . . . , (ln, vn).
When introducing additional text prompts for a
new image Inew, we can choose to use either plain
text prompts or a combination of text and vision
prompts.
3.3.1 Plain Text Prompts
This method is exemplified on the left side in Fig.
5. It involves asking a general question about the
emotional state of a group of people without ref-
erencing specific individuals. For example, the
question "What is the emotion for this group of
people?" yields an answer that considers the over-
all mood and setting of the group. This approach
is useful for understanding group dynamics or the
general atmosphere of a scene.
3.3.2 Combined Text-Vision Prompts
Shown on the right side in Fig. 5, this method in-
volves more detailed prompts that focus on indi-
vidual persons within the group. This allows for
a more nuanced analysis of specific people’s emo-
tions and actions. For instance, questions such as
‘What is Person 1’s emotion?’ or ‘What food is Per-
son 3 eating and drinking?’ prompt answers that
delve into specific details regarding individuals’ fa-
cial expressions, body language, and interactions
with objects, like food and drinks.Whatis the emotion for this group of people?In the image, we see a group of four peopleenjoying a picnic outdoors, which suggests a relaxed and pleasantsetting.Person 1 (Man on the left): He is smilingand seemsto be engaging in conversation, suggesting he is happyand enjoying the moment.Person 2 (Young girl on the right): She has a neutralexpression while listening or looking towards another family member, possiblyindicating contentmentor attentiveness.Person 3 (Woman in the center): She is smilingand appears to be laughing, indicating she is happy and enjoying the company.Person 4 (Boy in the center right): He also has a slight smile, suggesting he is happyand enjoying the picnic.Is Person 1's face visible? What is Person 1'semotion? In the image, Person 1(the man on the left) is partially visible; his face is shown in profile view, and he appears to be smiling. This suggests that he is feeling happyand engaged, likely enjoyingthe conversationor the company he is with. Even though we cannot see his full face directly, the visible portionof his smileand the overall relaxed settingindicate a positiveemotion.What food is Person 3 eating and drinking?In the image, Person 3appears to be holding a glass, likely containing a lemonadeor similar light-coloreddrink. There is no clear indication of her eating any specific food in the scene; however, there are various foodsvisible on the picnic blanket, including fruitsand possibly some sandwichesor snacks, which any of the picnic participants, including her, might be enjoying.What food is Person 4eating and drinking?In the image, Person 4 is visiblyholding a sandwich. He appears to be either about to take a biteor is currently eating it. As for his drink, he has a glassin front of him, similar tothe other glasseson the picnic blanket, which likely contains lemonadeor a similarbeverage. This setup suggests a typical picnic meal consisting of simple, easy-to-handle foods and refreshing drinks.User:Do Person 3and Person 4have similar emotion?In the image, Person 3and Person 4bothappear to be happy. Person 3is smiling broadly,suggesting she is enjoyingthe moment and possibly laughingor engagedin a cheerfulconversation. Person 4, although his smileis less pronounced than Person 3's, also displays a happydemeanor, suggesting contentment and enjoyment of the picnic.
Figure 5: We use two types of prompt methods. Left: plain text prompts, which can be used for group emotion
recognition. Right : combined text-vision prompts, which can be used for analyzing specific individuals’ emotions.
These prompts can be used to evaluate emotional interpretation in social interactions based on facial expressions,
body language, and contextual cues.
Table 1: Comparison of zero-shot emotion recognition methods , including MiniGPT-4 (Zhu et al., 2023), LLaV A
(Liu et al., 2023), Video-LLaV A (Zhang et al., 2023a), GPT-4V (Achiam et al., 2023), and SoV-Enhanced GPT
Models, across datasets with varying difficulty levels (Easy, Medium, and Hard): A Comparative Analysis of
Accuracy and Top-1 Recall (R@1).
Methods BackboneEasy Medium Hard Total
Acc (%) R@1 Acc (%) R@1 Acc (%) R@1 Acc (%) R@1
MiniGPT-4 (Zhu et al., 2023) Q-former, ViT 30.45 16.17 19.88 12.85 15.78 14.10 22.87 12.96
LLaV A (Liu et al., 2023) CLIP, ViT 35.74 15.91 22.80 11.29 3.50 1.58 22.65 10.56
Video-LLaV A (Zhang et al., 2023a) Pre-align ViT 20.11 9.37 16.95 7.26 8.77 4.46 16.12 6.84
GPT-4V (Achiam et al., 2023) ViT 48.85 27.94 47.95 19.23 32.45 11.36 44.44 22.11
GPT-4o (Achiam et al., 2024) +SoV (Ours) ViT 51.27 31.93 49.12 22.65 49.12 20.46 50.10 24.20
GPT-4V (Achiam et al., 2023) +SoV (Ours) ViT 60.91 41.96 53.21 22.82 50.00 18.97 55.33 28.69
4 Experiments
4.1 Models and Settings
We do not need to train any models for our method.
We evaluate the model’s performance in a zero-shot
manner using VLLMs. We include both commer-
cial models such as GPT-4V-turbo (Achiam et al.,
2023)1and GPT-4o-2024-05-13 (Achiam et al.,
2024) as well as open-sourced models including
MiniGPT-4-Vicuna (Zhu et al., 2023)2, LLaV A-1.5-
7B (Liu et al., 2023)3, Video-LLaV A-7B (Zhang
et al., 2023a)4.
1https://chatgpt.com/
2https://github.com/Vision-CAIR/MiniGPT-4
3https://github.com/haotian-liu/LLaV A
4https://github.com/PKU-YuanGroup/Video-LLaV A4.2 Dataset details
We collect original images from ABC News web-
site5. Following the collection, we undertake metic-
ulous preprocessing, initially removing any iden-
tical and blurry images through deduplication. To
minimize human effort and cost in data annotation,
we employ DeepFace (Serengil and Özpınar, 2024)
for emotion annotation. Subsequently, two human
annotators revise and refine the image labels. Fi-
nally, to finalize the labels, we involved a third
annotator who has a professional background in
psychology to verify correctness of facial expres-
sions with their domain knowledge. This proce-
dure guarantees the quality of the annotated data
used to construct benchmarks. Table 4 in appendix
A.1 presents the dataset details used for testing a
5https://www.abc.net.au/news/0.00.80.60.40.2AngryGPT-4VGPT-4V+OursLLaVAVideo-LLaVA
0.240.030.160.05MiniGPT-4
0.050.200.00Disgust0.000.200.200.090.030.000.06Fear0.060.500.580.410.36Happy0.270.130.020.040.00Sad0.220.260.260.000.15Surprise0.000.890.65
0.240.15Neutral0.29Figure 6: The bar chart shows the performance of various VLLMs in recognizing different emotions from images.
The models compared include GPT-4V+Ours, GPT-4V (Achiam et al., 2023), LLaV A (Liu et al., 2023), Video-
LLaV A (Zhang et al., 2023a), and MiniGPT-4 (Zhu et al., 2023). These results are distributed across seven emotions.
model on the task of zero-shot emotion recognition,
structured across three different levels of difficulty:
Easy, Medium, and Hard. This structured dataset
aids in understanding the robustness and adaptabil-
ity of the model in varying conditions of visual
complexity.
4.3 Quantitative Results
Table 1 provides a detailed analysis of different
zero-shot emotion recognition methods. MiniGPT-
4 (Zhu et al., 2023) exhibits low performance, with
accuracy ranging from 15.78% to 30.45% and Re-
call@1 from 12.85% to 16.17%. LLaV A (Liu et al.,
2023) and Video-LLaV A (Zhang et al., 2023a) per-
form better in simpler categories but struggle signif-
icantly in the Hard category, where accuracy plum-
mets to 3.50% and Recall@1 to 1.58%. In con-
trast, GPT-4V (Achiam et al., 2023) demonstrates
robust performance across all levels, markedly im-
proved by the SoV prompts. Specifically, GPT-
4V+SoV achieves an impressive 60.91% accuracy
and 41.96% Recall@1 in the Easy category, main-
taining 50.00% accuracy and 18.97% Recall@1
even in the Hard category. These results underline
SoV’s effectiveness in boosting the model’s ability
to accurately interpret emotions across different
complexities.
In Fig. 6, the chart highlights the varied effi-
cacy of different VLLMs in emotion recognition
tasks. GPT-4V+Ours consistently outperforms
other models across nearly all emotions, partic-
ularly excelling in neutral and angry emotions.
This highlights the specialized capabilities of GPT-
4V+Ours in capturing more nuanced and varied
emotional states. Meanwhile, other models show
selective strengths and general weaknesses, partic-ularly in recognizing negative emotions like fear
and disgust.
4.4 Visual Prompting
Table. 2 presents a comparative analysis of state-
of-the-art methods for zero-shot emotion recog-
nition across datasets categorized by varying lev-
els of difficulty: Easy, Medium, and Hard. It de-
tails performance metrics such as Accuracy and
Recall, comparing the effectiveness of different
visual prompting strategies utilized by GPT-4V .
The baseline method (GPT-4V), which uses plain
text prompts, demonstrates moderate effectiveness,
with an overall accuracy of 44.44% and a Recall
of 22.11%. Methods such as ReCLIP, RedCircle,
and SoV employ more complex combinations of vi-
sual prompts. SoV (Ours), incorporating Numbers,
Boxes, and Facial Landmarks, achieves the highest
overall accuracy and recall scores of 55.33% and
28.69%, respectively. This suggests that the integra-
tion of multiple visual cues, particularly those that
enhance the recognition of facial features, signifi-
cantly improves performance across all difficulty
levels, especially in more challenging datasets.
Fig. 7 visually represents how each visual
prompting approach modifies the image to focus on
emotion-relevant features. ReCLIP and RedCircle
blur out non-facial areas and highlight faces with
rectangles and circles, respectively. SoV applies
a combination of visual prompts to emphasize fa-
cial areas while maintaining background context,
which is critical for emotion recognition.
Fig. 8 reveals a comparative analysis of the per-
formance across different emotional categories us-
ing three different visual prompts for GPT-4V: SoV
(Ours), RedCircle, and ReCLIP. In this assessment,Table 2: Comparison of SOTA methods for zero-shot emotion recognition across datasets with varying levels
of difficulty—Easy, Medium, and Hard. The types of visual prompts used by previous approaches are: P: Crop,
B: Box, R: Blur Reverse, C: Circle, N: Number, F: Facial Landmarks.
SOTA methods Visual PromptEasy Medium Hard Total
Acc (%) R@1 Acc (%) R@1 Acc (%) R@1 Acc (%) R@1
Baseline (Achiam et al., 2023) Plain Text 48.85 27.94 47.95 19.23 32.45 11.36 44.44 22.11
ReCLIP (Subramanian et al., 2022) P | B | R 54.02 31.47 46.19 16.98 42.10 14.32 48.14 22.98
RedCircle (Shtedritski et al., 2023) P | C | R 51.72 29.55 48.53 23.19 45.61 15.89 49.01 23.89
SoV (Ours) N | B | F 60.91 41.96 53.21 22.82 50.00 18.97 55.33 28.69
ImageReCLIPRedCircleSoV
Figure 7: Visualization of the SOTA visual prompting approaches such as ReCLIP (Subramanian et al., 2022),
RedCircle (Shtedritski et al., 2023) and our SoV prompts. ReCLIP and RedCircle blur out non-facial areas and
highlight faces with rectangles and circles, respectively. SoV(Ours) applies a combination of visual prompts to
emphasize facial areas while maintaining background context.
0.00.80.60.40.2AngryRedCircleReCLIPSoV(Ours)
DisgustFearHappySadSurpriseNeutral
Figure 8: The bar chart illustrates the performance of
SoV(Ours), RedCircle and ReCLIP in emotion recogni-
tion across seven different emotional categories.
0.00.80.60.40.2AngryBox + NumberGPT-4V+SoVBoxBaseline
DisgustFearHappySadSurpriseNeutral
Figure 9: The bar chart displayed in the image illustrates
the performance of different vision prompts—GPT-
4V+SoV , Box + Number, Box, Baseline in emotion
recognition across seven different emotional categories.
SoV outperforms the other methods in most emo-
tional categories. Notably, SoV scores exception-
ally well for ‘Happy’ and ‘Neutral’ emotions, sug-
gesting a robust capability in recognizing these
emotions accurately. RedCircle and ReCLIP show
similar performance for ‘Fear’ and ‘Sad’, but lag
significantly behind SoV in ‘Happy’ and ‘Neutral’.
For ‘Angry’, SoV and ReCLIP perform almostequally well, both surpassing RedCircle. This chart
underscores SoV’s strengths in nuanced emotion
recognition, particularly in distinguishing positive
and neutral states effectively.
4.5 Ablation Study
We investigate the effects of different vision
prompts on GPT-4V . The data from Table 3 demon-
strates how various vision prompts affect an emo-
tion recognition system’s performance across dif-
ferent difficulty levels. Baseline prompt shows
moderate effectiveness with an accuracy of 48.85%
in Easy, 47.95% in Medium, but drops to 32.45%
in Hard scenarios. The overall accuracy is 44.44%,
with Recall@1 similarly distributed, suggesting
it performs consistently across different complex-
ities but struggles with harder categories. Intro-
duction of bounding boxes slightly reduces accu-
racy in simpler categories but improves Recall@1,
suggesting enhanced precision in pinpointing rele-
vant emotions. Notably, combining box and num-
ber prompts markedly boosts performance across
all categories, particularly in challenging envi-
ronments where accuracy rises significantly from
32.45% to 42.10% and Recall@1 from 11.36% to
15.57%. The SoV prompt outperforms all other
methods, achieving peak accuracies of 60.91% in
Easy and 50.00% in Hard, along with the highestTable 3: Ablation study for vision prompts on GPT-4V . Baseline : represents the model’s performance without
any additional prompts. Box: indicates a visual prompt that uses bounding boxes. Box+Number : adding numerical
identifiers to the bounding boxes. SoV: adding facial landmarks to each face with additional numerical identifiers to
the bounding boxes.
Vision PromptEasy Medium Hard Total
Acc (%) R@1 Acc (%) R@1 Acc (%) R@1 Acc (%) R@1
Baseline (Achiam et al., 2023) 48.85 27.94 47.95 19.23 32.45 11.36 44.44 22.11
Box 47.12 29.47 45.61 17.73 39.47 12.46 44.66 23.52
Box+Number 58.04 41.10 51.46 22.12 42.10 15.57 51.63 28.24
SoV 60.91 41.96 53.21 22.82 50.00 18.97 55.33 28.69
In the image:1.The woman on the left(Face 1) appears to have a happyand relaxedexpression.2.The man in the middle(Face 2) also looks happyand content, possibly enjoying the activity of grilling.3.The woman on the right(Face 3) seems happyand is likely enjoyingthe company and the setting.What is the emotion for each face?
In the image you provided:1. The woman labeled"1" has a cheerful expression, looking relaxedand happy.2. The figure labeled"2" has an obscured faceso I can't determine their expression.3. The woman labeled"3" seems to be smiling mildly, giving off a serene and content vibe.What is the emotion for each face?
Figure 10: The impacts of segmentation masks for emo-
tion recognition. Top: SoV provides a clearer view for
emotion recognition. Bottom: the segmentation masks
obscure parts of their faces, making it more challenging
to accurately discern these emotions, especially for Per-
son 2. In addition, the added segmentation masks also
result in a lack of precise context.
Recall@1 figures, confirming its effectiveness in
accurately interpreting emotions even in the most
difficult settings.
Fig. 9 showcases an ablation study that com-
pares the effectiveness of different vision prompts
on GPT-4V . The GPT-4V+SoV configuration con-
sistently outperforms the other methods in nearly
all emotional categories, particularly excelling in
‘Neutral’ and ‘Angry’. While the Box + Number
prompts demonstrates moderate success in ‘Happy’
and ‘Surprise’, it still falls short compared to SoV
prompts in other emotion categories. This result
highlights that after adding extra facial landmarks,
the VLLMs can capture more nuanced and varied
emotional states.
4.6 Qualitative Observations
The image in Fig. 10 shows three people at a picnic
setting, each displaying happy and relaxed expres-
sion, which suggests they are enjoying the outing.
However, due to the segmentation masks applied,
the masks obscure parts of their faces, making itmore challenging to accurately discern these emo-
tions, especially for person 2. In addition, the
added segmentation masks also result in a lack
of precise context.
5 Limitations
We suggest the implementation of Set-of-Vision
prompting to bridge visual and textual prompts.
However, a challenge arises as it is difficult to pre-
cisely describe visual prompts, such as the shape,
location, or color of a bounding box and facial land-
marks, in language. This issue might require encod-
ing the visual prompts and fine-tuning the entire
model for better accuracy. Moreover, this method
is computationally intensive, potentially limiting
its scalability and practicality in real-time appli-
cations, especially when handling large datasets
or streaming videos such as tracking one person’s
emotion in different video frames.
6 Conclusion
In conclusion, our Set-of-Vision prompting (SoV)
approach significantly advances the field of emo-
tion recognition within VLLMs by addressing criti-
cal challenges in spatial localization and global con-
text preservation. By leveraging spatial information
such as bounding boxes and facial landmarks, SoV
enhances zero-shot emotion recognition accuracy,
ensuring precise face count and emotion catego-
rization. Our face overlap handling algorithm and
combined text-vision prompting strategy further
refine the recognition process, highlighting the effi-
cacy of integrating visual prompts in VLLMs for
more accurate and detailed emotion analysis. This
approach not only preserves the enriched image
context but also offers a solution for detailed and
nuanced emotion recognition, underscoring its po-
tential impact on various applications within com-
puter vision and natural language processing.References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. GPT-4 technical re-
port. arXiv preprint arXiv:2303.08774 .
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2024. Openai. ChatGPT-4o
https://www.openai.com/chatgpt .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in Neural Information Processing
Systems , 33:1877–1901.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, et al. 2023. Palm: Scaling language
modeling with pathways. Journal of Machine Learn-
ing Research , 24(240):1–113.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale N Fung, and Steven Hoi.
2024. Instructblip: Towards general-purpose vision-
language models with instruction tuning. Advances
in Neural Information Processing Systems , 36.
Jiankang Deng, Jia Guo, Evangelos Ververas, Irene
Kotsia, and Stefanos Zafeiriou. 2020. Retinaface:
Single-shot multi-level face localisation in the wild.
InIEEE/CVF Conference on Computer Vision and
Pattern Recognition .
Sidong Feng and Chunyang Chen. 2024. Prompt-
ing is all you need: Automated android bug replay
with large language models. In Proceedings of the
IEEE/ACM International Conference on Software
Engineering , pages 1–13.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023. Visual instruction tuning.
Sefik Serengil and Alper Özpınar. 2024. A benchmark
of facial recognition pipelines and co-usability per-
formances of modules. Bili¸ sim Teknolojileri Dergisi ,
17(2):95–107.
Aleksandar Shtedritski, Christian Rupprecht, and An-
drea Vedaldi. 2023. What does clip know about a
red circle? visual prompt engineering for vlms. In
Proceedings of the IEEE International Conference
on Computer Vision , pages 11987–11997.
Hendrik Strobelt, Albert Webson, Victor Sanh, Ben-
jamin Hoover, Johanna Beyer, Hanspeter Pfister, and
Alexander M Rush. 2022. Interactive and visual
prompt engineering for ad-hoc task adaptation with
large language models. IEEE Transactions on Visu-
alization and Computer Graphics .Sanjay Subramanian, William Merrill, Trevor Darrell,
Matt Gardner, Sameer Singh, and Anna Rohrbach.
2022. Reclip: A strong zero-shot baseline for refer-
ring expression comprehension. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics , pages 5198–5215.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Alexandros Xenos, Niki Maria Foteinopoulou, Ioanna
Ntinou, Ioannis Patras, and Georgios Tzimiropoulos.
2024. Vllms provide better context for emotion un-
derstanding through common sense reasoning. arXiv
preprint arXiv:2404.07078 .
Dingkang Yang, Zhaoyu Chen, Yuzheng Wang, Shunli
Wang, Mingcheng Li, Siao Liu, Xiao Zhao, Shuai
Huang, Zhiyan Dong, Peng Zhai, et al. 2023a. Con-
text de-confounded emotion recognition. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 19005–19015.
Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chun-
yuan Li, and Jianfeng Gao. 2023b. Set-of-mark
prompting unleashes extraordinary visual grounding
in gpt-4v. arXiv preprint arXiv:2310.11441 .
Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang,
and Jian Yang. 2024. Fine-grained visual prompting.
Advances in Neural Information Processing Systems ,
36.
Hang Zhang, Xin Li, and Lidong Bing. 2023a. Video-
llama: An instruction-tuned audio-visual language
model for video understanding. arXiv preprint
arXiv:2306.02858 .
Sitao Zhang, Yimu Pan, and James Z Wang. 2023b.
Learning emotion representations from verbal and
nonverbal communication. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 18993–19004.
Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu,
Siyang Gao, and Yue Zhang. 2024. Image-of-thought
prompting for visual reasoning refinement in mul-
timodal large language models. arXiv preprint
arXiv:2405.13872 .
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023. MiniGPT-4: Enhancing
vision-language understanding with advanced large
language models. arXiv preprint arXiv:2304.10592 .
Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie
Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and
Yong Jae Lee. 2024. Segment everything everywhere
all at once. Proceedings of the Conference on Neural
Information Processing Systems , 36.A Appendix
A.1 Dataset details
In the dataset Table 4, the "easy" dataset includes
pictures with three or fewer faces. The "medium"
dataset includes pictures with 3 to 7 faces. The
"hard" dataset includes pictures with more than 7
faces. Easy dataset contains 76 images with a to-
tal of 174 faces. Medium dataset consists of 34
images featuring 171 faces. Hard dataset is the
smallest set, comprising 9 images but still contain-
ing a significant number of faces (114). The table
categorizes the datasets based on the complexity
and density of faces in the images, which likely
affects the challenge level for the model’s emotion
recognition capabilities. The usage of SoV prompts
across all categories suggests a consistent testing
approach, aiming to evaluate how well the model
can interpret and predict emotions without prior
specific training on these images (zero-shot learn-
ing). The metrics, Accuracy and Recall, are chosen
to assess the model’s precision in correctly identi-
fying emotions and its ability to retrieve relevant
instances across the datasets, respectively.
Table 4: The table presents the dataset details used
for testing a model on the task of zero-shot emotion
recognition, structured across three different levels of
difficulty: Easy, Medium, and Hard.
Dataset #Images #Faces Prompts Metrics
Easy 76 174 SoV Accuracy & Recall
Medium 34 171 SoV Accuracy & Recall
Hard 9 114 SoV Accuracy & Recall
Total 119 459 SoV Accuracy & Recall
A.2 Scene based emotion recognition
The image in Fig. 11 captures a moment from a
sports event, specifically a match between teams
from Australia and the Philippines, with Australia
leading based on the scoreboard ("PHI 0 - 1 AUS").
The scene includes various spectators and players,
each showing distinct emotions which can aid in
understanding the context and overall sentiment
related to the ongoing game. Including scene con-
text can significantly enhance the performance of
language and image models (LLMs) in recogniz-
ing emotions. By understanding not just the facial
expressions but also the situational context (such
as the score in a sports game), models can make
more accurate inferences about the probable emo-
tions being displayed. In addition, environmental
cues like scoreboards, team colors, and body lan-
guage provide additional data points that help in
accurately deducing the emotional state of indi-
viduals in group settings. Our SoV approach canbridge the gap between purely facial expression-
based recognition and a more situation-aware un-
derstanding, leading to more nuanced and accurate
emotion recognition capabilities in VLLMs.
In Fig. 12, the SoV method, by focusing on
specific individuals within the image and retaining
the clarity of the background context, offers a com-
prehensive approach to emotion recognition. This
allows for a detailed analysis of not just the visible
facial expressions, but also the situational context
and interactions among individuals, providing a
more nuanced understanding of the emotions being
conveyed. For example, in the case of Person 2,
despite a partially visible face, the SoV approach
leverages additional contextual cues to deduce the
emotion more accurately. In contrast, the RedCir-
cle approach primarily relies on clear facial expres-
sions by blurring background details, potentially
overlooking subtler emotional cues embedded in
the environment and body language.
A.3 Chain of thought for emotion recognition
The "Chain of Thought" approach, as illustrated in
Fig. 13, offers advantages by breaking down the
thought process into sequential, logical steps. This
method enhances accuracy in interpreting emotions
by considering various contextual clues such as ac-
tivities, and body language, which are then system-
atically analyzed. For example, the analysis first
identifies the activity (Halloween trick-or-treating)
and setting, which sets the emotional backdrop.
Subsequently, it classifies each individual based on
visible attributes and costumes, leading to more nu-
anced emotion recognition. This step-by-step rea-
soning mirrors human cognitive processes, allow-
ing for more refined and contextually appropriate
interpretations of emotions, such as distinguishing
neutral expressions from smiles, even in a complex
social setting like Halloween, where expressions
might otherwise be ambiguous.User: Is the team with the red cloth girl winning or losing? What should I say to her?  What's emotion for the girl label "1"and '2' and '3', and '4’ ?GPT-4V + SoV: Answer: The scoreboard in the image shows "PHI 0 -1 AUS,"indicating that the team associated with Australia(presumably the team in light blue) is winning, and the team associated with the Philippines(possibly the team the girl in red is supporting, if she is a fan or player of this team) is losing.If you wish to speak to the girl in red and she is supporting or part of the Philippine team, here are a few supportive things you could say:1.Encouraging her spirit: "The game's not over yet! There's still time for a comeback."2.Showing empathy and support: "It's a tough game, but you all are showing great spirit! Keep cheering!"3.Acknowledging the effort: "It's been a challenging match, but I admire the team's resilience. How are you holding up?"These comments are supportive and considerate, acknowledging the situation while remaining positive and encouraging.Based on their expressions in the image:1. The girl labeled"1"  seems focused and somewhat intense, likely reflecting her determinationin the game given that her team is currently losing.2. The girl labeled"2" appears joyful and is smiling, which suggests she is happy, likely because her team is in the lead.3. The person labeled"3" is also smiling and appears to be enjoying the moment, which is consistent with a positive emotion like happiness or contentment.4. The person labelled "4”(in the inset) has a neutral expression, which might suggest he is calm or composed, possibly observing the game with interest or concentration.Scene based Emotion RecognitionFigure 11: Scene based Emotion RecognitionUser:What’s emotion for these person?GPT-4V + RedCircle:Answer: In the image you've shared, there are several people showing different emotions:1.The man in the blue shirt is smilingwidely, indicating happiness or joy.2.The girl to his left also seems happy, with a broad smile.3.The woman in the foreground on the right is smiling as well, suggesting she is also feeling happy.4.The boy in the foreground on the left appears to be laughing or speaking enthusiastically, which generally reflects excitement or happiness.Overall, the group appears to be enjoying a pleasant and happy moment together.Compare RedCircleand SoVprompts
GPT-4V + SoV: Answer: In this image set within a home dining scene:1.Person 1(woman in purple square): She shows a joyful and welcoming expression, likely enjoying the conversation and the meal.2.Person 2(boy in red square): He looks attentive and possibly listening or thinking deeply about the conversation. His expression suggests engagementrather than emotional display.3.Person 3(man in green square): He is smiling as he eats, reflecting happiness and satisfaction, perhaps with the meal and the company.4.Person4 (girl in blue square): She also shows a happydemeanor, smiling broadly, indicating she is having a good time.Overall, the scene conveys a warm and cheerful family mealtime, where everyone seems to be enjoying both the food and each other’s company.RedCircleSoVFigure 12: Compare RedCircle and SoV prompts for emotion recognition.User:Step 1: What are they planning to do?Step 2: How many people are there in the image?Step 3: Can you give me the related labels for these person?Step 4:  What are their emotions?GPT-4V + SoV: Question 1: The group in the image seems to be participating in a Halloween trick-or-treating event. They are dressed in costumes and carrying buckets for collecting candy, which is a traditional activity where participants visit houses to ask for treats. The setting, with residential cars and houses in the background, further supports that they are likely walking through a neighborhoodto gather sweetsfrom different homes.Question 2: There are four people in the image.Question 3: In the image:1. Person 1 (on the far right) -labeledas "1", wearing a black shirt with white skeleton bones printed on it and carrying an orange pumpkin bucket.2. Person 2 (second from the right) -labeledas "2", dressed in casual clothes, holding documents and an orange bucket.3. Person 3 (second from the left) -labeledas "3", dressed in a black tutu and carrying an orange bucket.4. Person 4 (on the far left) -labeledas "4", also dressed in a black tutu, but with a blue bucket instead.Each label corresponds to the numbered box drawn over their faces in the image.Question 4: From the visible expressions in the image:1. Person 1 (on the far right): Appears to have a neutralexpression.2. Person 2 (second from the right): Their face is not visible in the image.3.  Person 3 (second from the left): Seems to be smiling, indicating happiness or cheerfulness.4. Person 4 (on the far left): Appears to have a neutral expression.These are the inferred emotions based on the limited facial expressions visible in the image.Chain of thought for emotion recognition
Figure 13: Chain of thought for emotion recognitionDetect negative emotions for group of people
GPT-4V + SoV: If Sally mentioned seeing a very sad news video that deeply affected her, it's likely she referred to the image depicting people in a scene of destruction or after a disaster. This would be the first image of the three presented in your query, where individuals are seen amidst rubble and damage. Such scenes typically involve serious issues like natural disasters, conflicts, or other tragic events that can have a profound emotional impact on viewers. This type of content aligns with a description of a "very sad news video."The other two images show more casual or celebratory scenarios that are less likely to be described as deeply affecting in a sad context.User:My friend Sally mentioned seeing a very sad news video that deeply affected her. Which of the images could she have seen?
User:What are the face emotion of the people in the firstimage? GPT-4V + SoV: In the image where individuals are amidst rubble, the emotions of the people can be described as follows:1.Person 1: Appears to be in distress or discomfort, potentially indicative of shock or sadness.2.Person 2: Looks serious and concerned, likely reflecting the gravity of the situation.3.Person 3: His expression is hard to discern fully due to the angle, but he seems to be focused and possibly worried.4.Person 4: Shows a pensive or somber expression, which might suggest contemplation or distress.5.Person 5: Seems to have a neutral or tired expression, which might reflect fatigue or resignation in the face of the situation.Overall, the expressions suggest a mixture of concern, sadness, and seriousness, aligning with a response to a serious or tragic event.Figure 14: Detect negative emotions for different peopleGrounding ability
The person labeled "2"in the image appears to be smilingand engaging positivelywith the interviewer, which suggests that he is feeling happyor content during the interview. His overall demeanor and facial expression convey a sense of enjoyment or satisfaction, likely related to the context of the sports event.In the image you provided from a sportingevent, the person labeled "2" would be the most appropriate individual for a reporterto ask questions about the game. This person is likely a coachor playerinvolved in the game, given that he is in uniformand holding a child who is also in a similar uniform. This suggests he is directlyrelated to the team and would have insights about the game's specifics.User: If you are a reporterand want to ask a question about the game, which labeledperson is the best person for you to talk to?GPT-4V + SoV: Answer: User: what is the emotion of the person labeled "2"GPT-4V + SoV: Answer: Figure 15: Grounding ability