Lifelong Event Detection via Optimal Transport
Viet Dao1*, Van-Cuong Pham1*, Quyen Tran1*,Thanh-Thien Le1,
Linh Ngo Van2andThien Huu Nguyen1, 3
1VinAI Research2Hanoi University of Science and Technology3University of Oregon
{v.vietdt11, v.cuongpv27, v.quyentt15, v.thienlt3}@vinai.io ,
linhnv@soict.hust.edu.vn ,
thien@cs.oregon.edu
Abstract
Continual Event Detection (CED) poses a
formidable challenge due to the catastrophic
forgetting phenomenon, where learning new
tasks (with new coming event types) hampers
performance on previous ones. In this paper,
we introduce a novel approach, Lifelong Event
Detection via Optimal Transport ( LEDOT ),
that leverages optimal transport principles to
align the optimization of our classification mod-
ule with the intrinsic nature of each class, as
defined by their pre-trained language modeling.
Our method integrates replay sets, prototype
latent representations, and an innovative Opti-
mal Transport component. Extensive experi-
ments on MA VEN and ACE datasets demon-
strate LEDOT’s superior performance, consis-
tently outperforming state-of-the-art baselines.
The results underscore LEDOT as a pioneer-
ing solution in continual event detection, offer-
ing a more effective and nuanced approach to
addressing catastrophic forgetting in evolving
environments.
1 Introduction
Event Detection (ED) (Nguyen et al., 2016, 2023a)
presents a pivotal challenge in the domain of In-
formation Extraction, tasked with identifying event
triggers and their associated types from natural lan-
guage text. However, the conventional ED training
paradigm, characterized by its static nature, falls
short in capturing the dynamic nature of real-world
data. As highlighted by Yu et al. (2021), the ontol-
ogy of events in ED research has been exhibiting a
constant shift since its introduction, prompting the
exploration of Continual Event Detection (CED),
where data arrives continuously as a sequence of
non-overlapping tasks. Although large language
models (LLMs) have recently emerged, showcas-
ing the ability to tackle numerous problems using
only prompts without the need for fine-tuning, they
*Equal contribution.fall short in the domains of information extraction
(IE) (Han et al., 2023; Gao et al., 2023) and con-
tinual learning (Shi et al., 2024). Continual event
detection, in particular, remains a difficult task that
is not effectively addressed by LLMs.
CED presents many issues, most notably the
catastrophic forgetting (McCloskey and Cohen,
1989; Ratcliff, 1990) phenomenon, where the train-
ing signal from new task hampers performance on
past tasks. To provide a solution for this issue,
numerous methods have been proposed, which usu-
ally fall into one of the three eminent approaches:
Regularization-based (Chaudhry et al., 2021; Saha
et al., 2021; Phan et al., 2022; Van et al., 2022;
Hai et al., 2024); Architecture-based (Yoon et al.,
2017; Sokar et al., 2021); and Memory-based (Be-
louadah and Popescu, 2019; Rolnick et al., 2019).
Out of these three, Memory-based methods have
demonstrated superiority, leveraging access to the
Replay buffer , a memory of limited size containing
a portion of data from previously learned tasks for
the model to rehearse during the training of new
tasks.
Despite the promise of Memory-based methods,
challenges abound. First, the finite capacity of the
Replay buffer results in the eviction of valuable
information, leading to incomplete representations
of past tasks and hence, inadequate generality. Fur-
thermore, the process of sampling and replaying
data might not be optimally curated, potentially
hindering the model’s ability to generalize across
tasks effectively.
This setback arises because the conventional
practice of discarding the original head of pre-
trained language models (PLMs) during fine-tuning
on downstream tasks overlooks valuable linguis-
tic information encoded within it. In training the
classifier module, state-of-the-art approaches (Qin
et al., 2024; Wang et al., 2023; Liu et al., 2022;
Yu et al., 2021) often do so in isolation, devoid
of any priors or foundations. Discarding the lan-arXiv:2410.08905v1  [cs.CL]  11 Oct 2024guage modeling head in PLMs is highly wasteful.
The language modeling head contains essential in-
formation about vocabulary distribution based on
contextual representations. Losing this head sac-
rifices crucial linguistic nuances, making it harder
to align the classifier module and ensure efficient
fine-tuning. Aligning our classifier module to this
information is an essential but also formidable chal-
lenge. This alignment is crucial for ensuring a
more efficient fine-tuning process, as it provides a
foundational standard of learning that mitigates un-
necessary overplasticity and prevents catastrophic
forgetting.
To address the limitations discussed, this paper
introduces a method to enhance Memory-based
CED by integrating Optimal Transport (OT) princi-
ples, which provide a robust framework for measur-
ing the distance between probability distributions.
By incorporating OT into the fine-tuning process,
we aim to retain essential linguistic information
from the PLM head, ensuring the model remains
invariant to specific tasks. This integration involves
defining an appropriate cost matrix, a key challenge
that we address by proposing a novel construction
tailored to our method. Our approach ensures ef-
fective alignment between the PLM head and the
classifier’s output, leveraging OT to enhance the
model’s performance and robustness across various
tasks while preserving the PLM’s inherent linguis-
tic knowledge.
2 Background
2.1 Event Detection
Following previous works, we formalize Event De-
tection as a Span-based Classification task (Nguyen
and Grishman, 2015; Lu and Nguyen, 2018; Man
Duc Trong et al., 2020). Given an input instance
x= (w1:L, s, e)consisting of a L-token context
sequence w1:L, a start index s, and an end index e,
an ED model has to learn to assign the text span
ws:einto a label yfrom a set of pre-defined event
typesY, orNAifws:edoes not trigger a known
event.
Generally, we use a language model Mto en-
code the context sequence w1:Linto contextualized
representation w′
1:L. Then, a classifier is utilized to
classify the representation of the trigger span:
h= [w′
s, w′
e], (1)
p(y|x) =Softmax (Linear (FNN (h)).(2)Here, FNN denotes a feed-forward neural net-
work, [·,·]is the concatenation operation, his the
hidden vector representing ws:e, and p(y|x)mod-
els the probability of predicting yfrom the input
x.
The model is trained on a dataset D=
{(xi, yi)}N
i=1using cross-entropy loss:
LC(D) =−1
|D|X
(x,y)∈Dlogp(y|x). (3)
To mitigate the imbalance between the number
of event triggers and the number of NAspans, we
re-weight the loss with a hyperparameter η:
LC=ηLC(DNA) + (1 −η)LC(D \ D NA)(4)
where DNAis the set of NAinstances.
2.2 Continual Event Detection
The training data in CED is not static but arrives
sequentially as a stream of Tnon-overlapping
tasks{Dt|ST
t=1Dt=D;Dt∩ D t′=∅}. At
each timestep t, the tthtask data only covers a
set of event types Yt={y1
t, y2
t, . . . ynt
t}, which
is a subset of the full ontology of event types
Y. Here, unseen events and negative instances
(i.e. text spans that do not trigger any event) are
treated as NA. After training on Dt, the model is
expected to be able to detect all seen events thus
far, i.e. Y1TY2. . .TYt. To this end, we employ
two commonly used techniques in Rehearsal-based
Continual Learning: Naive Replay , and Knowledge
Distillation (Hinton et al., 2015). Let Rt−1be the
replay buffer up to task t−1, the Replay Loss and
Knowledge Distillation loss are written as follows:
LR=−1
|Rt−1|X
(h,y)∈Rt−1logpt(y|h),(5)
LD=−X
(h,y)∈Rt−1pt−1(y|h) logpt(y|h),(6)
where ptdenotes the probability of predictions
given by the model instance at timestep t.
3 Lifelong Event Detection via Optimal
Transport
We incorporate Optimal Transport (OT) as a foun-
dational element of our methodology. OT is a math-
ematical framework designed to compute the dis-
tance between two probability distributions with
different supports.In our methodology, OT is applied to align the
probability distribution output of the classifier head
with the distributional characteristics inherent in
the vocabulary of the Pre-trained Language Model
(PLM) head. The softmax class probabilities from
the classifier head are transported to closely match
the pre-trained distribution, facilitating a seam-
less integration of task-specific knowledge while
minimizing the divergence from the model’s pre-
existing linguistic understanding.
We forward each event trigger through a pre-
trained language model and its original language
modeling head, and obtain a distribution over a
dictionary of Vwords:
xs=Softmax (LMH (w′
s)/τ)
xe=Softmax (LMH (w′
e)/τ)
˜x= (xs+xe)/2
where LMH is a pre-trained language model head,
τis temperature coefficient, and ˜xis distribution
of the event trigger over dictionary.
Each event trigger is associated with a distri-
bution over Cclasses: p∈∆C, where each en-
try indicates the probability that the event trig-
ger belongs to a class in the ontology. An en-
coder is employed to generate pfromx, defined
asp=Softmax (θ(x)), where θrepresents the
parameters of the neural network as described in
Section 2.1.
Given that ˜xandpare distributions with differ-
ent supports for the same event trigger, we aim to
train the model by minimizing the following Op-
timal Transport (OT) distance to push ptowards
˜x:
dM(˜x,p):= min
P∈U(˜x,p)⟨P,M⟩, (7)
where ⟨·,·⟩denotes the Frobenius inner product;
the cost matrix M∈R≥0V×Ccaptures semantic
distances between class cand word v, with each
entry mvcsignifying the importance of words in
the corresponding class; P∈RV×C
>0denotes the
transport plan; and and U(˜x,p)is defined as the set
of all viable transport plans. Considering two dis-
crete random variables X∼Categorical( ˜x)and
Y∼Categorical( p), where the transport plan P
becomes a joint probability distribution of (X, Y),
i.e.,p(X=i, Y=j) =pij: the set U(˜x,p)en-
compasses all possible joint probabilities that sat-
isfy the specified constraints, forming a transport
polytope.Directly optimizing Eq. (7) poses a time-
consuming challenge. To address this, an entropic-
constrained regularized optimal transport (OT) dis-
tance is introduced, known as the Sinkhorn dis-
tance:
sM(˜x,p):= min
P∈U(˜x,p)⟨P,M⟩ −H(P),(8)
where the entropy function of the transport plan
H(P)def=−P
i,jPi,j(log(Pi,j−1))is the regu-
larizing function (Cuturi, 2013).
The cost matrix Mis a trainable variable in our
model. To overcome the challenge of learning the
cost function, we propose a specific construction
forM:
mvc= 1−cos(ev,gc), (9)
where cos( ·,·)represents the cosine similarity, and
gc∈RDandev∈RDare the embeddings of class
cand word v, respectively. After training on one
task, the learned class embeddings are frozen. We
then expand the size of the class embeddings and
train the newly initialized embeddings on the new
task.
Frogner et al. (2015) further suggested combin-
ing the OT loss with a conventional cross-entropy
loss to better guide the model. By parameterizing
MwithG, the collection of class embeddings, the
final OT objective function is expressed as:
LOT= min
θ,G[sM(˜x,p)−ϵ˜xlogϕ(p)].(10)
To maintain the consistency of class representa-
tions across tasks, an additional regularization term
enforces the proximity of class representations in
the current task to those in the most recent task:
LG=||Gt−G(t−1)||2. (11)
Finally, we can write our final objective function:
L=LC+LR+LD+LOT+αLG, (12)
where αis the regularization coefficient.
Avoiding Catastrophic Forgetting Similar to
many CED baselines, our method incorporates a
replay process. However, our approach to con-
structing the memory buffer is distinct. For each
class in the training data, we retain the prototype
mean µand diagonal covariance Σof its trigger
representations encountered by the model, rather
than storing explicit data samples. During replay,MA VEN ACE
Task 1 2 3 4 5 1 2 3 4 5
BIC 63.16 55.51 53.96 50.13 49.07 55.88 58.16 61.23 59.72 59.02
KCN 63.16 55.73 53.69 48.86 47.44 55.88 58.55 61.40 59.48 58.64
KT 62.76 58.49 57.46 55.38 54.87 55.88 57.29 61.42 60.78 59.82
EMP 66.82 58.02 58.19 55.07 54.52 59.05 57.14 55.80 53.42 52.97
ESCO 67.50 61.37 60.65 57.43 57.35 —- —- —- —- —-
SCR 76.52 57.97 57.89 52.74 53.41 75.24 63.3 61.07 55.05 55.37
SharpSeq 62.28 61.85 62.92 61.31 60.27 56.47 56.99 64.44 62.47 62.60
LEDOT-OT 63.34 59.89 59.28 56.24 55.20 58.74 58.08 61.81 58.32 59.76
LEDOT-R 63.01 60.16 59.76 56.75 54.59 58.30 58.60 63.14 58.82 60.18
LEDOT-P 63.01 59.95 59.32 56.10 55.21 59.95 56.63 62.09 60.08 61.41
LEDOT 62.98 60.47 60.78 58.53 57.53 58.30 59.69 63.52 61.05 63.22
LEDOT + SharpSeq 63.30 61.97 63.00 61.81 61.49 60.15 59.73 64.55 63.65 64.27
Upperbound / / / / 64.14 / / / / 67.95
Table 1: Classification F1-scores (%) on 2 datasets MA VEN and ACE. Upperbound indicates the theoretical
maximum achievable performance when BERT is frozen.
synthetic samples are generated from these proto-
types and combined with the replay buffer Rto
form the effective buffer ˜R. This modified buffer
replaces Rin the computation of LR(5) and LD
(6).
4 Experiments
4.1 Settings
Datasets We employ two datasets in our ex-
periments: ACE 2005 (Walker et al., 2006) and
MA VEN (Wang et al., 2020); both are preprocessed
similar to Yu et al.’s (2021) work. To ensure fair-
ness, we rerun all baselines on the same prepro-
cessed datasets. The detailed statistics of the two
datasets can be found in Appendix A.2.
Experimental Settings We adopt the Oracle neg-
ative setting, as mentioned by Yu et al. (2021),
to evaluate all methods in continual learning sce-
nario. This setting involves excluding the learned
types from previous tasks in the training set of the
new task, except for the NA(Not Applicable) type.
Labels for future tasks are treated as NAtype. As-
sessments are conducted using the exact same task
permutations as in Yu et al.’s (2021) work. The per-
formance metric is the average terminal F1 score
across 5 permutations after each task. Recently,
(Le et al., 2024a) introduced a multi-objective opti-
mization method that is compatible with our pro-
posed LEDOT approach. To examine the impact of
LEDOT on SharpSeq, we conducted an experiment
referred to as LEDOT+SharpSeq. For details on
other baselines and the integration of LEDOT withSharpSeq, please refer to Appendix A.1.
4.2 Experimental Results
Table 1 showcases the impressive results of our pro-
posed method, LEDOT , compared to state-of-the-
art baselines in continual event detection. On both
the MA VEN and ACE datasets, LEDOT consis-
tently achieves higher F1 scores, surpassing most
baseline methods. When combined with SharpSeq,
LEDOT further enhances performance, increasing
the F1-score by a significant margin of 1.22% on
MA VEN and 1.67% on ACE after five tasks.
We also conduct further ablation studies to evalu-
ate variants of LEDOT : LEDOT-OT (without Opti-
mal Transport), LEDOT-R (without the replay set),
and LEDOT-P (without prototype latent representa-
tions). Even without prototype rehearsal, LEDOT-
P with OT surpasses the replay-based baseline KT
by0.34% on MA VEN and 1.59% on ACE. More-
over, LEDOT outperforms LEDOT-OT, highlight-
ing the crucial role of OT in preventing catastrophic
forgetting. Specifically, OT improves F1 scores by
2.33% on MA VEN and 3.46% on ACE. These re-
sults emphasize the importance of OT in mitigating
catastrophic forgetting in continual event detection.
5 Conclusion
Harnessing the inherent linguistic knowledge from
pre-trained language modeling heads in encoder-
based language models play a pivotal role in en-
hancing performance in downstream tasks. With
the introduction of LEDOT, we present a novel ap-
proach utilizing optimal transport to align the learn-ing of each task with a common reference—the pre-
trained distribution of the vocabulary. This align-
ment mitigates overfitting to the current task and
effectively addresses the challenge of catastrophic
forgetting. Our method, demonstrating superior
performance across various benchmarks, stands
as a testament to the effectiveness of leveraging
pre-trained language modeling heads for continual
event detection, offering a promising avenue for fu-
ture research in this domain. In the future, we plan
to extend our method to solve continual learning
challenges for other information extraction tasks,
such as event-event relation extraction (Man et al.,
2024b,a).
Limitations
Being an empirical study into the effectiveness of
Optimal Transport in aligning the output distribu-
tion of Continual Event Detection models, our work
is not without limitations. We acknowledge this,
and would like to discuss our limitations as follows:
•The method proposed in this paper is orthog-
onal to the tasks of interest and the specific
techniques to solve them. With that being said,
our method is applicable to a wide range of in-
formation extraction tasks, such as Named En-
tity Recognition, and Relation Extraction, as
well as other text classification tasks, such as
Sentiment Analysis. However, given limited
time and computational resources, we limit
the scope of our experiments to only Event
Detection. The extent to which our proposed
method can work with other NLP problems
can be an interesting topic that we leave for
future work. Nevertheless, our experimental
results suggest that using Optimal Transport
to align the output distribution of the model
with the pre-trained language modeling head
has the potential to improve continual learning
performance on other problems as well.
•This paper presents the empirical results of our
LEDOT method using a pre-trained encoder
language model (i.e. BERT) as the backbone.
Meanwhile, large decoder-only language mod-
els, with their heavily over-parameterized ar-
chitectures, amazing emergent ability, and
great generalization capability, have emerged
and become the center of focus of NLP re-
search in recent years. Though they have
proved to be able to understand language andsolve almost all known NLP tasks without
needing much fine-tuning, many studies ((Lai
et al., 2023); (Qiu and Jin, 2024); (Zhong
et al., 2023)) suggested that even the largest
models like ChatGPT ((Ouyang et al., 2022))
still lag behind smaller but specialized models
such as BERT (Devlin et al., 2019) and T5
(Raffel et al., 2023) by a significant margin on
tasks like Event Detection. We thus believe
that studies on the applications of encoder lan-
guage models in Continual Event Detection
are still needed.
Acknowledgements
This research has been supported by the Army Re-
search Office (ARO) grant W911NF-21-1-0112,
the NSF grant CNS-1747798 to the IUCRC Center
for Big Learning, and the NSF grant # 2239570.
This research is also supported in part by the Office
of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), via the HIATUS Program contract 2022-
22072200003. The views and conclusions con-
tained herein are those of the authors and should
not be interpreted as necessarily representing the
official policies, either expressed or implied, of
ODNI, IARPA, or the U.S. Government.
References
Eden Belouadah and Adrian Popescu. 2019. Il2m: Class
incremental learning with dual memory. In 2019
IEEE/CVF International Conference on Computer
Vision (ICCV) , pages 583–592.
Pengfei Cao, Yubo Chen, Jun Zhao, and Taifeng Wang.
2020. Incremental event detection via knowledge
consolidation networks. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 707–717.
Arslan Chaudhry, Albert Gordo, Puneet Dokania, Philip
Torr, and David Lopez-Paz. 2021. Using hindsight
to anchor past knowledge in continual learning. In
Proceedings of the AAAI conference on artificial in-
telligence , pages 6993–7001.
Li Cui, Deqing Yang, Jiaxin Yu, Chengwei Hu, Jiayang
Cheng, Jingjie Yi, and Yanghua Xiao. 2021. Refin-
ing sample embeddings with relation prototypes to
enhance continual relation extraction. In Proceed-
ings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 232–243, Online.
Association for Computational Linguistics.Marco Cuturi. 2013. Sinkhorn distances: Lightspeed
computation of optimal transport. In Advances in
Neural Information Processing Systems , volume 26.
Curran Associates, Inc.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi,
Mauricio Araya, and Tomaso A Poggio. 2015. Learn-
ing with a wasserstein loss. Advances in neural in-
formation processing systems , 28.
Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu.
2023. Exploring the feasibility of chatgpt for event
extraction. arXiv preprint arXiv:2303.03836 .
Nam Le Hai, Trang Nguyen, Linh Ngo Van, Thien Huu
Nguyen, and Khoat Than. 2024. Continual varia-
tional dropout: a view of auxiliary local variables in
continual learning. Machine Learning , 113(1):281–
323.
Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang,
Lu Liu, and Xiang Wan. 2023. Is information extrac-
tion solved by chatgpt? an analysis of performance,
evaluation criteria, robustness and errors.
Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu,
Peng Li, Maosong Sun, and Jie Zhou. 2020. Contin-
ual relation learning via episodic memory activation
and reconsolidation. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 6429–6440, Online. Association
for Computational Linguistics.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network.
Viet Lai, Nghia Ngo, Amir Pouran Ben Veyseh, Hieu
Man, Franck Dernoncourt, Trung Bui, and Thien
Nguyen. 2023. ChatGPT beyond English: Towards
a comprehensive evaluation of large language mod-
els in multilingual learning. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2023 , pages 13171–13189, Singapore. Association
for Computational Linguistics.
Thanh-Thien Le, Viet Dao, Linh Van Nguyen, Thi-
Nhung Nguyen, Linh Van Ngo, and Thien Huu
Nguyen. 2024a. Sharpseq: Empowering continual
event detection through sharpness-aware sequential-
task learning. In 2024 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics .
Thanh-Thien Le, Manh Nguyen, Tung Thanh Nguyen,
Linh Ngo Van, and Thien Huu Nguyen. 2024b. Con-
tinual relation extraction via sequential multi-tasklearning. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 38, pages 18444–
18452.
Minqian Liu, Shiyu Chang, and Lifu Huang. 2022. In-
cremental prompting: Episodic memory prompt for
lifelong event detection. In Proceedings of the 29th
International Conference on Computational Linguis-
tics, pages 2157–2165, Gyeongju, Republic of Korea.
International Committee on Computational Linguis-
tics.
Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
arXiv:1711.05101 .
Weiyi Lu and Thien Huu Nguyen. 2018. Similar but
not the same: Word sense disambiguation improves
event detection via neural representation matching.
InProceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing , pages
4822–4828, Brussels, Belgium. Association for Com-
putational Linguistics.
Hieu Man, Chien Van Nguyen, Nghia Trung Ngo, Linh
Ngo, Franck Dernoncourt, and Thien Huu Nguyen.
2024a. Hierarchical selection of important context
for generative event causality identification with op-
timal transports. In Proceedings of the 2024 Joint
International Conference on Computational Linguis-
tics, Language Resources and Evaluation (LREC-
COLING 2024) , pages 8122–8132, Torino, Italia.
ELRA and ICCL.
Hieu Man, Chien Van Nguyen, Nghia Trung Ngo, Linh
Ngo, Franck Dernoncourt, and Thien Huu Nguyen.
2024b. Hierarchical selection of important context
for generative event causality identification with op-
timal transports. In Proceedings of the 2024 Joint
International Conference on Computational Linguis-
tics, Language Resources and Evaluation (LREC-
COLING 2024) , pages 8122–8132.
Hieu Man Duc Trong, Duc Trong Le, Amir Pouran
Ben Veyseh, Thuat Nguyen, and Thien Huu Nguyen.
2020. Introducing a new dataset for event detec-
tion in cybersecurity texts. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 5381–5390,
Online. Association for Computational Linguistics.
Michael McCloskey and Neal J. Cohen. 1989. Catas-
trophic interference in connectionist networks: The
sequential learning problem. In Psychology of Learn-
ing and Motivation , volume 24, pages 109–165. Aca-
demic Press.
Chien Nguyen, Linh Ngo, and Thien Nguyen. 2023a.
Retrieving relevant context to align representations
for cross-lingual event detection. In Findings of
the Association for Computational Linguistics: ACL
2023 , pages 2157–2170.
Huy Nguyen, Chien Nguyen, Linh Ngo, Anh Luu, and
Thien Nguyen. 2023b. A spectral viewpoint on con-
tinual relation extraction. In Findings of the Associ-ation for Computational Linguistics: EMNLP 2023 ,
pages 9621–9629.
Thien Huu Nguyen, Kyunghyun Cho, and Ralph Grish-
man. 2016. Joint event extraction via recurrent neural
networks. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 300–309, San Diego, California.
Association for Computational Linguistics.
Thien Huu Nguyen and Ralph Grishman. 2015. Event
detection and domain adaptation with convolutional
neural networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short
Papers) , pages 365–371, Beijing, China. Association
for Computational Linguistics.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems , volume 35, pages 27730–27744.
Curran Associates, Inc.
Hoang Phan, Anh Phan Tuan, Son Nguyen, Ngo Van
Linh, and Khoat Than. 2022. Reducing catastrophic
forgetting in neural networks via gaussian mixture ap-
proximation. In Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining , pages 106–117.
Springer.
Chengwei Qin, Ruirui Chen, Ruochen Zhao, Wenhan
Xia, and Shafiq Joty. 2024. Lifelong event detection
with embedding space separation and compaction.
Yunjian Qiu and Yan Jin. 2024. Chatgpt and finetuned
bert: A comparative study for developing intelligent
design support systems. Intelligent Systems with
Applications , 21:200308.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1–67.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2023. Exploring the limits
of transfer learning with a unified text-to-text trans-
former.
Roger Ratcliff. 1990. Connectionist models of recog-
nition memory: Constraints imposed by learning
and forgetting functions. Psychological Review ,
97(2):285–308.David Rolnick, Arun Ahuja, Jonathan Schwarz, Timo-
thy Lillicrap, and Gregory Wayne. 2019. Experience
replay for continual learning. In Advances in Neural
Information Processing Systems , volume 32.
Gobinda Saha, Isha Garg, and Kaushik Roy. 2021.
Gradient projection memory for continual learning.
arXiv preprint arXiv:2103.09762 .
Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin,
Wenyuan Wang, Yibin Wang, and Hao Wang. 2024.
Continual learning of large language models: A com-
prehensive survey.
Ghada Sokar, Decebal Constantin Mocanu, and Mykola
Pechenizkiy. 2021. Spacenet: Make free space for
continual learning. Neurocomputing , 439:1–11.
Linh Ngo Van, Nam Le Hai, Hoang Pham, and Khoat
Than. 2022. Auxiliary local variables for improving
regularization/prior approach in continual learning.
InPacific-Asia conference on knowledge discovery
and data mining , pages 16–28. Springer.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 multilin-
gual training corpus LDC2006T06. Web Download.
Philadelphia: Linguistic Data Consortium.
Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang,
Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai
Lin, and Jie Zhou. 2020. Maven: A massive gen-
eral domain event detection dataset. arXiv preprint
arXiv:2004.13590 .
Zitao Wang, Xinyi Wang, and Wei Hu. 2023. Continual
event extraction with semantic confusion rectifica-
tion.
Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, and Yun Fu. 2019. Large
scale incremental learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 374–382.
Weimin Xiong, Yifan Song, Peiyi Wang, and Sujian
Li. 2023. Rationale-enhanced language models are
better continual relation learners. arXiv preprint
arXiv:2310.06547 .
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and
Sung Ju Hwang. 2017. Lifelong learning with dy-
namically expandable networks. arXiv preprint
arXiv:1708.01547 .
Pengfei Yu, Heng Ji, and Prem Natarajan. 2021. Life-
long event detection with knowledge transfer. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 5278–
5290, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,
and Christopher D. Manning. 2017. Position-aware
attention and supervised data improve slot filling. In
Proceedings of the 2017 Conference on EmpiricalMethods in Natural Language Processing (EMNLP
2017) , pages 35–45.
Kang Zhao, Hua Xu, Jiangong Yang, and Kai Gao. 2022.
Consistent representation learning for continual re-
lation extraction. In Findings of the Association for
Computational Linguistics: ACL 2022 , pages 3402–
3411, Dublin, Ireland. Association for Computational
Linguistics.
Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and
Dacheng Tao. 2023. Can chatgpt understand too? a
comparative study on chatgpt and fine-tuned bert.A Additional Experimental Details
A.1 Baselines
The following continual learning and continual ED
methods are employed as baselines in this paper:
•BIC (Wu et al., 2019) addresses model bias
towards new labels via an affine transforma-
tion.
•KCN (Cao et al., 2020) employs a limited set
to store data for replay, utilizing knowledge
distillation and prototype-enhanced retrospec-
tion to alleviate catastrophic forgetting.
•KT(Yu et al., 2021) follows a memory-based
approach, combining knowledge distillation
with knowledge transfer. This method utilizes
new-label samples to reinforce the model’s
retention of old knowledge and employs old-
label samples to initialize representations for
new-label data in the classification layer.
•EMP (Liu et al., 2022) also leverages
knowledge distillation and introduces straight
prompts into the input text to retain previous
knowledge.
•ESCO (Qin et al., 2024) introduce ESCO, a
method combining Embedding Space Separa-
tion and Compaction. ESCO pushes the fea-
ture distribution of new data away from old
data to reduce interference and pulls memory
data towards its prototype to improve intra-
class compactness and alleviate overfitting on
the replay dataset.
•SharpSeq The framework introduced in
SharpSeq (Le et al., 2024a) integrates multi-
objective optimization (MOO) with sharpness-
aware minimization (SAM). In the context of
continual learning, handling multiple losses
often involves simply summing them with
fixed coefficients. However, this approach
can lead to gradient conflicts that hinder the
discovery of optimal solutions. MOO algo-
rithms address this issue by dynamically esti-
mating coefficients based on the gradients of
the losses. To refine the results of MOO, (Le
et al., 2024a) employs SAM to identify flat
minima along the Pareto front.
•SCR (Wang et al., 2023) employs a training
approach involving both BERT and the classi-
fier layer. Initially, this yields high F1 scoreson early tasks, but performance deteriorates
rapidly as more tasks are encountered. In con-
trast, our method maintains BERT’s parame-
ters fixed during training. The SCR approach,
which fine-tunes BERT, presents challenges
for continual event detection. Despite hav-
ing different label sets, many sentences are
recurrent across tasks. SCR tackles this by
using pseudo labels from the previous stage
to predict labels on new datasets, containing
sentences from previous tasks. However, this
strategy leads to data leakage from old tasks to
new ones, significantly inflating SCR’s replay
dataset beyond what is allowed in strict con-
tinual learning setups. In contrast, our method
relies on a frozen BERT for feature extrac-
tion, ensuring consistency in trigger represen-
tations over time. Our approach aligns with
the principles of continual learning, where the
model solely accesses data relevant to the cur-
rent task. Moreover, the evaluation metric in
SCR differs from our approach, as they do
not account for the NA label despite it be-
ing the most common label in these datasets.
Therefore, we have reproduced the results and
reported them in Table1.
•LEDOT + SharpSeq Our proposed method
incorporates two key objectives: one focus-
ing on the OT loss for the language modeling
head and another serving as a regularization
term to ensure the proximity of class repre-
sentations. Instead of treating these objectives
as separate entities within a multi-objective
optimization algorithm, we integrate them di-
rectly into the overall loss calculation using
the same data. This approach maintains the
original number of losses, streamlining the
optimization process.
A.2 Datasets
Detailed statistics regarding the datasets used for
all empirical assessments can be found in Table 2.
A.3 Implementation Details
In our experiments, the encoder and language
model head is taken from BERT-large-cased (De-
vlin et al., 2019) and they are freezed in the train-
ing process. We employ the AdamW optimizer
(Loshchilov and Hutter, 2017) with a learning rate
of1×10−4and a weight decay of 1×10−2. Model
training continues until there is no increase in per-formance on the development set. The replay set-
ting remains consistent with KT Yu et al.’s (2021),
where the number of instances for each label in the
replay set is set to 20. Since the size of the vocab-
ulary is large and it contains many subwords and
completely unrelated words, to reduce the computa-
tion, we select only a subset of words that are verbs.
In each batch, we combine that set with tokens in
the batch to compute the OT loss.
All implementations are coded using PyTorch,
and the experiments were carried out on NVIDIA
A100 and NVIDIA V100 GPUs.
B Ablation Study
B.1 Temperature of Language Modeling Head
We conduct an ablation study to explore the impact
of different temperatures in the language modeling
head within the LEDOT method. The motivation
behind this study lies in the stochastic nature of
the language modeling process, where a higher
temperature introduces more randomness. This
increased stochasticity can influence the generation
not only of the primary label (event type) but also of
other words related to the topic. By systematically
varying the temperature parameter, denoted as τ,
we aim to understand how these different levels
of stochasticity affect LEDOT’s performance. The
results are presented in Table 3.
B.2 Quantity of Generated Samples
In Table 1, we observe that the performance of
LEDOT significantly improves when synthesizing
representations from prototypes. To further inves-
tigate this effect, we conducted additional experi-
ments with LEDOT, varying the ratios ( r) between
the number of generated samples and the replay set.
The outcomes for four rvalues are presented in
Table 4. Notably, on MA VEN, the highest perfor-
mance is achieved with r= 10 , yielding a 57.53%
F1 score in the fifth task. Conversely, for the fifth
task on ACE, the optimal rvalue is 2020, result-
ing in a 63.22% score. The influence of prototype
sampling on early tasks is relatively marginal, but
it becomes more pronounced in later tasks. It is
important to note that an increased rvalue does
not necessarily guarantee improved LEDOT per-
formance. This can be attributed to the noise intro-
duced by random processes during representation
sampling. The noise can impact the outcome of the
language modeling head in LEDOT and potentially
misguide the classification head during model opti-mization. Therefore, when generating more sam-
ples, careful consideration is required to mitigate
noise effects and avoid adversarial impacts.
B.3 Further Analysis
We conduct additional ablation studies to gain
deeper insights into the performance of LEDOT.
First, we compare the impact of two differ-
ent initialization methods for Optimal Trans-
port—random initialization and initializing labels
by mapping them to their corresponding word em-
beddings in the vocabulary. The results of this com-
parison are detailed in Table 5, shedding light on
the influence of initialization strategies on the over-
all effectiveness of LEDOT. Second, we explore
the sensitivity of our method to the coefficient of
regularization applied to the cross-task class rep-
resentations. The results of this investigation are
presented in Table 6, providing valuable informa-
tion about the robustness of LEDOT to variations in
the regularization coefficient. These ablation stud-
ies contribute to a comprehensive understanding of
the factors influencing LEDOT’s performance in
continual event detection scenarios.
C Optimal Transport on Continual
Relation Extraction
Our proposed Optimal Transport alignment extends
beyond Continual Event Detection: it can also en-
hance other continual NLP solutions utilizing vari-
ous kinds of pre-trained language models. To sub-
stantiate this claim, we demonstrate its effective-
ness in Continual Relation Extraction (CRE) (Han
et al., 2020; Cui et al., 2021; Zhao et al., 2022;
Xiong et al., 2023; Nguyen et al., 2023b; Le et al.,
2024b) using an encoder-decoder language model,
specifically T5 (Raffel et al., 2020).
Our experiments are centered around the state-
of-the-art CRE baseline RationaleCL (Xiong et al.,
2023). This method leverages rationales generated
byChatGPT-3.51during training to enhance the
T5 model for CRE. RationaleCL operates by first
generating rationales for current relation samples
using an LLM. These rationales are then integrated
into the original training dataset for multi-task ra-
tionale tuning. Formally, RationaleCL introduces
1https://chat.openai.com/three key objectives:
Task c:xi−→yi (13)
Task r:xi−→ri+yi (14)
Task d:xi+ri−→yi (15)
where xirepresents the input text, yidenotes the re-
lation label, and ristands for the rationale. Task c
directly generates the label yifrom the input xi,
while Task rrequires the model to generate an ex-
planation before generating an answer. Task duses
both the input and rationale in the encoder part to
answer the question. It is noteworthy that, similar
to most continual learning methods, RationaleCL
employs a replay process. This process trains the
model on both newly encountered data and a lim-
ited amount of samples from previously encoun-
tered tasks stored in the buffer.
The state-of-the-art performance achieved by Ra-
tionaleCL in CRE underscores its efficacy. How-
ever, our integration of Optimal Transport (OT)
methodologies aims to elevate the method to new
heights. We introduce OT objectives to align the
learned language-modeling head with T5’s original
language-modeling head, resulting in the enhance-
ments observed over the baseline on the TACRED
dataset (Zhang et al., 2017), as showcased in Table
7.
Our integration of OT objectives not only miti-
gates the detrimental effects of catastrophic forget-
ting but also emerges as a compelling solution for
enhancing the fine-tuning process across various
downstream tasks.MA VEN ACE
#Doc #Sentence #Mention #Negative #Doc #Sentence #Mention #Negative
Train 2522 27983 67637 280151 501 18246 4088 261027
Dev 414 4432 10880 46318 41 1846 433 53620
Test 710 8038 18904 79699 55 689 790 93159
Table 2: Statistics of two datasets. #Doc stands for the total number of documents.
MA VEN ACE
Task 1 2 3 4 5 1 2 3 4 5
τ= 5 63.15 60.78 60.66 58.51 57.37 57.41 59.00 63.60 60.87 61.81
τ= 4 63.08 60.72 60.71 58.76 57.71 61.09 60.12 63.36 61.09 61.15
τ= 3 63.06 60.77 60.70 58.43 57.30 58.09 59.46 63.98 61.63 62.36
τ= 2 63.11 60.64 60.70 58.45 57.50 58.30 59.69 63.52 61.05 63.22
τ= 1 62.98 60.47 60.78 58.53 57.53 60.42 59.76 64.28 61.52 62.84
τ= 0.1 62.52 60.31 60.51 58.31 57.13 61.51 57.01 62.94 60.18 61.22
τ= 0.01 62.60 60.3 60.43 58.22 57.15 62.15 57.08 63.51 59.48 61.29
Table 3: Ablation results for the temperature of the language modeling head in the LEDOT method.
MA VEN ACE
Task 1 2 3 4 5 1 2 3 4 5
r= 20 63.01 60.12 60.26 57.96 56.87 58.30 59.69 63.52 61.05 63.22
r= 10 62.98 60.47 60.78 58.53 57.53 58.30 60.80 64.63 62.47 62.63
r= 5 63.01 60.30 60.54 58.22 57.01 58.30 61.06 64.67 60.59 62.29
r= 1 63.07 60.16 60.00 57.07 55.84 58.30 60.51 64.24 60.15 62.18
Table 4: Ablation results for the number of generated representations in the LEDOT method.
MA VEN ACE
Task 1 2 3 4 5 1 2 3 4 5
random 63.15 60.78 60.66 58.51 57.37 57.41 59.00 63.60 60.87 61.81
mapping 63.08 60.72 60.71 58.76 57.71 61.09 60.12 63.36 61.09 61.15
Table 5: Ablation results for the initialization of Optimal Transport in the LEDOT method. "mapping" indicates
initializing labels by mapping them to their corresponding word embeddings in the vocabulary.
MA VEN ACE
Task 1 2 3 4 5 1 2 3 4 5
α= 1 63.01 60.36 60.67 58.33 57.41 58.30 59.72 64.41 60.97 62.29
α= 0.562.98 60.47 60.78 58.53 57.53 58.30 59.69 63.52 61.05 63.22
α= 0.263.07 60.66 60.67 58.37 57.16 58.72 59.39 64.55 61.88 62.68
α= 0.163.01 60.45 60.60 57.79 57.02 58.72 60.01 64.61 62.49 62.82
Table 6: Ablation results for regularization on cross-task class representations in the LEDOT method.
TACRED
Task 1 2 3 4 5 6 7 8 9 10
RCL 100.00 94.80 92.20 89.24 86.56 84.74 80.57 77.46 80.98 79.11
OT RCL 97.76 98.40 93.17 87.94 90.18 86.05 82.73 80.61 82.61 79.36
Table 7: Classification accuracy (%) on the TACRED dataset. RCL abbreviates for RationaleCL.