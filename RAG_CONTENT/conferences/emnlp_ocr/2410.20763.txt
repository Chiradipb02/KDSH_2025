Evaluating LLMs for Targeted Concept Simplification for Domain-Specific
Texts
Sumit Asthana‚Ä†*Hannah Rashkin‚Ä°Elizabeth Clark‚Ä°
Fantine Huot‚Ä°Mirella Lapata‚Ä°
‚Ä†University of Michigan, Ann Arbor‚Ä°Google DeepMind
asumit@umich.edu {hrashkin, eaclark, fantinehuot, lapata}@google.com
Abstract
One useful application of NLP models is to sup-
port people in reading complex text from unfa-
miliar domains (e.g., scientific articles). Sim-
plifying the entire text makes it understandable
but sometimes removes important details. On
the contrary, helping adult readers understand
difficult concepts in context can enhance their
vocabulary and knowledge. In a preliminary
human study, we first identify that lack of con-
text and unfamiliarity with difficult concepts is
a major reason for adult readers‚Äô difficulty with
domain-specific text. We then introduce tar-
geted concept simplification, a simplification
task for rewriting text to help readers compre-
hend text containing unfamiliar concepts. We
also introduce W IKIDOMAINS1, a new dataset
of 22k definitions from 13 academic domains
paired with a difficult concept within each defi-
nition. We benchmark the performance of open-
source and commercial LLMs, and a simple dic-
tionary baseline on this task across human judg-
ments of ease of understanding and meaning
preservation. Interestingly, our human judges
preferred explanations about the difficult con-
cept more than simplification of the concept
phrase. Further, no single model achieved supe-
rior performance across all quality dimensions,
and automated metrics also show low correla-
tions with human evaluations of concept sim-
plification ( ‚àº0.2), opening up rich avenues
for research on personalized human reading
comprehension support.
1 Introduction
Text simplification helps lay audiences understand
challenging text by simplifying difficult terms,
syntax, or discourse (Zhang and Lapata, 2017;
Agrawal and Carpuat, 2023) or by adding con-
tent to elaborate on the text (Srikanth and Li,
2021). With advances in neural models, especially
*Work done as student researcher at Google DeepMind.
1https://github.com/google-deepmind/wikidomains
Domain:CS 
Definition:  In computer science, arbitrary-precision arithmetic 
indicates that calculations are performed on numbers whose 
digits of precision are limited only by the available memory of the 
host system. 
(A) ü§ñ
Simplify: In computer science, arbitrary-precision 
arithmetic refers to a type of mathematical calculation with 
as many digits as needed , limited only by the computer's 
available memory. 
(B) üìö
Define:  In computer science, arbitrary-precision 
arithmetic indicates that calculations are performed on 
numbers whose digits of precision are limited only by the 
available memory of the host system. Digits of precision is  
defined as "the level of exactness in a number's digits. "
(C) ü§ñ
Explain : In computer science, arbitrary-precision 
arithmetic, indicates that calculations are performed on 
numbers whose digits of precision are limited only by the 
available memory of the host system. In other words, the  
more memory the system has, the more precise the  
calculations can be. Rewriting Approaches Targeted Concept Simplification 
I am not sure what a host 
system  is in this context I don‚Äôt understand 
what this concept 
means? Figure 1: An example from the dataset, which consists
of adefinition and a potential difficult concept in the text
that a reader may struggle with. The task is to rewrite
thedefinition in a way that simplifies this concept for the
reader. (a) Simplifies ‚Äúdigits of precision‚Äù to ‚Äúas many
digits as needed‚Äù, (b) Adds the definition of ‚Äúdigits
of precision‚Äù, (c) Contextually explains that ‚Äúdigits of
precision‚Äù refers to precision of calculations and how it
relates to memory.
LLMs, sentence simplification has made consider-
able progress towards generating text at different
reading grade levels (Kew et al., 2023). However,
skilled adult readers face more challenges with lack
of subject-matter knowledge (Guo et al., 2023).
Supporting readers in understanding concepts they
find personally difficult within a larger body of
text not only expands their vocabulary, but also
helps them develop a broader understanding of the
topic (Kintsch, 1991; Van den Broek, 2010).
For example, in Figure 1, a person unfamiliar
with the concept ‚Äúdigits of precision‚Äù will not un-arXiv:2410.20763v2  [cs.CL]  6 Nov 2024derstand the definition of ‚Äúarbitrary precision arith-
metic‚Äù. AI tools could help rewrite the text by
lexically substituting ‚Äúdigits of precision‚Äù with ‚Äúas
many digits as needed‚Äù (simplifying) or by elabo-
rating on the concept (defining or explaining). (a)
Lexical simplification makes the definition under-
standable by reducing the overall complexity, per-
haps losing some of the meaning. (b) Adding a
definition of ‚Äúdigits of precision‚Äù may broaden the
reader‚Äôs vocabulary but does not explain its signif-
icance in the context of the overall definition (i.e.
its implications for calculations and memory). (c)
Providing a contextual explanation about ‚Äúdigits of
precision‚Äù could more explicitly link the relation
between memory and preciseness of calculations,
enhancing comprehension (Van den Broek, 2010;
Srikanth and Li, 2021).
In our study, we asked human raters to read def-
initions from 13 academic domains and identify
the challenges in understanding them. We found
that 50% of the reading difficulties arose from un-
familiar concepts, and annotators expressed the
need for more context around them. Motivated by
this, we present the new task of targeted concept
simplification for rewriting text to support under-
standing of difficult concept s within the definitions‚Äô
context. This task focuses on simplifying specific
concepts that users struggle with, allowing for per-
sonalized reading support than simply rewriting
an entire document at an easier reading level. Per-
sonalized support with difficult concept s can help
readers receive more contextually-relevant informa-
tion tailored to their background knowledge. For
instance, a computer scientist reading a physics
document might struggle with physics concepts but
understand the mathematical terms, while someone
without a math background might need help with
mathematical terms (Guo et al., 2023).
To investigate this task, we collect a new dataset,
WIKIDOMAINS , consisting of 22k definitions from
Wikipedia. We collect definitions using article ti-
tles and leading statements from Wikipedia. Our
definitions span 13 academic domains (e.g., busi-
ness, education, etc., see Table 1) improving over
existing datasets that are limited to a single domain
(e.g., science) (August et al., 2022). We annotate a
potential difficult concept in each definition using
an automated heuristic (Biran et al., 2011).
We use this dataset to evaluate the performance
of open-source and commercial LLMs on targeted
concept simplification. We explore three methods
for rewriting definitions: adding a dictionary defi-Domain #Definitions
Food & Drink 1,403
Performing arts 322
Business & Economics 1,539
Politics & Government 2,267
Biology 7,200
Chemistry 957
Computing 2,083
Earth and Environment 1,314
Mathematics 1,747
Medicine & Health 2,939
Physics 741
Engineering 89
Technology 7
Total 22,561
Table 1: Domains and number of definitions in each
domain in the W IKIDOMAINS dataset.
nition of the difficult concept , prompting LLMs to
simplify the difficult concept , and prompting LLMs
to explain the difficult concept in context. We con-
duct human evaluations of all three approaches
along three dimensions: 1) meaning preservation,
2) whether a reader who is unfamiliar with the diffi-
cult concept can understand the rewritten definition,
and 3) whether the rewritten definition is easier to
understand than the original. Our human evalua-
tions demonstrate a clear preference towards strate-
gies for contextual explanation of the difficult con-
cept rather than lexical simplifications. However,
we also find that LLMs need to improve further on
dimensions of comprehension. Low to mild cor-
relations of automated simplification metrics with
human evaluations of comprehension and mean-
ing preservation ( ‚àº0.1-0.3) also indicate a need
for better metrics to evaluate nuanced contextual
explanations.
In summary, our main contributions include:
‚Ä¢Introducing targeted concept simplification as
a task for supporting readers as they encounter
difficult concepts in text.
‚Ä¢Analysis from an annotation study examining
the difficulties humans face in reading and the
possible utility of assistance in understanding
difficult concepts.
‚Ä¢WIKIDOMAINS , a dataset of 22k challeng-
ing domain-specific definitions collected from
Wikipedia with automatically-annotated diffi-
cult concepts.‚Ä¢Human evaluations of the performance of
open-source and commercial LLMs on our
task across multiple quality dimensions, in-
cluding analysis of different prompting strate-
gies and automatic metrics.
2 Background
Cognitive Support and Human Reading Com-
prehension Successful reading comprehension
is key to integrating new knowledge and fostering
learning from text (Lorch Jr and van den Broek,
1997; Dunietz et al., 2020). Cognitive theories
suggest that comprehension is a multi-stage pro-
cess that primarily involves 1) constructing a local
meaning representation of text such as concepts,
facts, and their relations (Graesser et al., 1994),
and 2) forming a schema and filling in gaps using
background knowledge to create a ‚Äúmental picture‚Äù
of what the text is about (Kintsch and Van Dijk,
1978; Bartlett, 1995). Adult readers lacking domain
knowledge can be supported by explicit cues, such
as examples and explanations, to help them con-
struct better mental representations of ideas from
the text (Kintsch, 1991; Van den Broek, 2010).
Text Simplification Reducing reading-level com-
plexity and syntax (Garbacea et al., 2021) in text
simplification benefits specific audiences like stu-
dents, second language learners, and individuals
with dyslexia (Paetzold and Specia, 2016; Bin-
gel et al., 2018), but may not enhance compre-
hension for general adult readers (Garbacea et al.,
2021). Contextual explanations can enhance com-
prehension but findings from studies of elaborating
events in news domains (Srikanth and Li, 2021)
may not be the same as difficulty with concepts
in academic texts. While Wikipedia and news cor-
pora (Kauchak, 2013; Xu et al., 2015; Zhang and
Lapata, 2017) have advanced text simplification,
they focus more on syntax and discourse difficul-
ties than on academic concepts. Similarly, lexicons
are limited to medicine (Elhadad and Sutaria, 2007;
Ong et al., 2007) and science concepts (August
et al., 2022), highlighting the need for a multi-
domain corpus to advance personalized simplifi-
cation for a general audience.
Complex Terms and Jargon Lexical simplifi-
cation systems (Paetzold and Specia, 2016) have
been shown to benefit children, people with lan-
guage impairments or medical jargon simplifica-
tion (Fatima and Strube, 2023; Joseph et al., 2023).
0 10 20 30 40 50Q1
Q251.77%
27.59%27.66%
39.66%0.00%
2.30%10.64%
22.99%0.71%
17.24%4.96%
7.47%6.38%
3.45%10.64%
4.02%Didn't understand a
 mentioned concept
Want *more* details or
 other context
Want *less* detail
General writing 
complexity
Want an example or
 analogy
Want visual or
 audio reference
T ext was ill-formed
General confusionQ1: Your difficulties in understanding?
Q2: What would you ask a tutor to change?Figure 2: Results of annotator study: We asked anno-
tators to read complex text for (1) what made the text
difficult for them to understand and (2) how they would
want a tutor to edit the text to help their understanding.
However, beyond lab studies, it is challenging to
specify reader knowledge in large-scale evalua-
tions. Proxies for audience knowledge include
specialized lexicons (Paris, 1988; Elhadad and Su-
taria, 2007), coarse indicators such as reading grade
level (Agrawal and Carpuat, 2023), or binary indi-
cators to denote science knowledge audience (Au-
gust et al., 2022). Guo et al. (2023) highlighted
the challenge of specifying audience knowledge
at a finer level, suggesting the use of domain as a
proxy for concept familiarity. Building on this, we
provide a multi-domain corpus of challenging defi-
nitions to specify fine-grained audience levels. Un-
like prior work on generating definitions (August
et al., 2022) or simplifying all difficult concepts (Fa-
tima and Strube, 2023), we focus on rewriting defi-
nitions to address specific concept difficulties, en-
abling readers to leverage their background knowl-
edge and improve comprehension (Kintsch, 1991;
Rello et al., 2013). While previous tools explored
simple strategies like adding definitions for com-
plex terms (Bingel et al., 2018), we evaluate LLMs
that can provide contextual explanations (Srikanth
and Li, 2021).
3 What AI assistance can benefit reading
domain specific text
To better motivate the scope of this task, we in-
vestigate adult readers‚Äô difficulties with domain-
specific text and what types of help they would
want from an AI-tutor. We randomly selected a
set of 900 text examples from Wikipedia-derived
definitions spanning 13 domains (see Section 5
for details about definitions and domain selection).
For each example, we ask a human annotator to re-spond in free-text to: 1) the reasons for difficulty (if
any) when reading the definitions, and 2) what they
would ask a tutor to change in the definition if they
faced a difficulty. In Figure 2, we report recurring
themes from annotator responses to both questions
for cases where annotators had difficulty reading
from these examples (categories were agreed on by
the authors, see details in Appendix Section A).
These results suggest that specific difficult con-
cepts used in the definitions were one of the most
frequent reasons for reading difficulty (52% of def-
initions had such difficulty), and annotators fre-
quently asked for help from a tutor with these con-
cepts (28%). This indicates that our proposed task,
targeted concept simplification, is an important sub-
task for simplification aimed to resolve a key chal-
lenge for lay adult readers. When asking a tutor
for help, annotators also explicitly asked for more
details on the difficult concept (rather than less).
This suggests that contextual elaborations (Srikanth
and Li, 2021) for these difficult concept s are a bet-
ter alternative over lexical simplifications to sup-
port their comprehension and knowledge. Anno-
tators also asked for examples/analogies (17%),
visual/audio aids (8%), or identified general issues
with the writing complexity (23%, e.g., an issue
with general reading level or syntactic complexity),
though to a lesser degree. The majority of our anno-
tators had an above high-school level educational
qualification (see Table 8 in Appendix), suggesting
that unfamiliar concepts in context is a greater chal-
lenge for skilled adult readers than simply difficult
words or syntax.
4 Task Definition
We present targeted concept simplification : a text
simplification task focused on specific words or
phrases that readers find difficult to understand.
This setup allows for personalized and controlled
rewriting of difficult concepts. Our initial user
study (Section 3) shows that unfamiliar words or
phrases often hinder comprehension.
The task of targeted concept simplification is to
rewrite an input definition containing a concept c
to make it understandable to someone unfamiliar
with the concept. For example, Figure 1 shows
the definition of the term ‚Äúarbitrary precision arith-
metic.‚Äù The task is to rewrite the definition to help
someone unfamiliar with the difficult concept ‚Äúdig-
its of precision.‚Äù Possible approaches could in-
volve replacing ‚Äúdigits of precision‚Äù with a simplerphrase like ‚Äúas many digits as needed,‚Äù explaining
it within the definition, or perhaps even adding ex-
amples, analogies, or illustrations. The usefulness
of each strategy will depend on its ability to com-
plement the reader‚Äôs existing knowledge about the
topic Kintsch (1991). Unlike other text simplifi-
cation tasks, our task targets simplifying concepts
difficult for the reader rather than simplifying the
entire text.
5 The W IKIDOMAINS Dataset
To support research on targeted concept simplifica-
tion, we introduce a dataset of 22k definitions from
13 academic domains2, where each definition is a
1‚Äì2 sentence explanation of a term3. Within each
definition, we select a difficult concept ‚Äîa word
or phrase that could impede the reader‚Äôs ability to
comprehend the definition as whole. We take inspi-
ration from August et al. (2022) who collected defi-
nitions from Wikipedia science glossaries; however,
instead of glossaries, we directly collect definitions
from Wikipedia articles of concepts spanning 13
domains (see Table 1 for list of domains).
To collect definitions, we start with Johnson
(2021)‚Äôs dataset that contains all English Wikipedia
articles with probabilities of belonging to high-
level domains. These domains are broad academic
topics (e.g., Physics, Economics) that Wikipedia
editors identified through consensus (Asthana and
Halfaker, 2018). We refer to each Wikipedia arti-
cle title as a term and take the first sentence of its
lead section as its definition (August et al., 2022).
For every domain, we first select articles with do-
main assignment probabilities greater than a thresh-
oldŒ¥domain .4To filter out low-importance articles
that could be named entities, unimportant places
or things, we also excluded articles having a page-
rank percentile score less than Œ¥pr.5Finally, we
also excluded articles that were additionally mem-
bers of domains related to named entities, events,
or things (e.g., Biography). Table 1 summarizes
the 13 domains and the number of articles in each
domain that the final W IKIDOMAINS dataset con-
tains (more details in Appendix B). We also provide
2https://github.com/google-deepmind/wikidomains
3We call the concept being explained by the original defi-
nition a term to avoid confusion with difficult concepts present
within the definitions.
4Manual inspection of topic assignments for 50 Wikipedia
articles suggested that a threshold of 0.7 was reasonable to
identify articles belonging to a domain.
5We determined the threshold as 0.1 through a manual
examination of 100 articles.train dev test
# definitions 15,873 3,384 3,304
avg # tokens 22.75 22.63 22.61
total # tokens 361,066 76,572 74,691
vocab size 42,356 15,576 14,911
Table 2: Statistics on W IKIDOMAINS definitions bro-
ken down by split; #tokens and vocabulary size are
calculated by splitting the definitions on whitespace and
removing punctuation.
each term‚Äôs lead section in the dataset for future
research.
We select a training, development, and test split
of 15,873/3,384/3,304 examples (see Table 2 for
more details about the data splits.) We conduct our
experiments in a zero- or few-shot setting without
using the training or development data, but we pub-
licly release the full set to facilitate future research.
5.1 Difficult Concept Identification
For each definition, we automatically label a poten-
tialdifficult concept that could impede a reader‚Äôs
comprehension. Lay readers will be more familiar
with concepts that are popularly mentioned across
Wikipedia (e.g., ‚Äúbacteria‚Äù) than concepts that only
occur in articles of a specific domain (e.g., ‚ÄúPhy-
tosterol‚Äù). Thus, following prior work on approx-
imating word difficulties using specificity-based
measures (Biran et al., 2011), we use a domain-
specificity measure to score concept difficulty for a
lay audience.
First, we identify candidate concepts cmen-
tioned in each term‚Äôs definition using Wiki-
data (Vrande Àáci¬¥c and Kr√∂tzsch, 2014)6. We then
order the candidates by a score of how specific
they are to the term‚Äôs domain Dt. This is measured
by the ratio of how many articles Athe concept c
appears in within this domain compared to across
Wikipedia generally:
P
A‚ààD t1[c‚àà A]P
A‚ààD all1[c‚àà A](1)
For each definition, we select one difficult concept
out of the top- kidentified candidates7. If we could
not identify any difficult concept in the definition,
we instead chose a difficult concept using the age of
6We used the Wikidata extension in spaCy to identify con-
cepts in definitions that have corresponding Wikidata entries.
7We chose kas 2 based on manual assessment of 100
definitions.acquisition lexicon (Kuperman et al., 2012), which
provides an average age when different words are
acquired as a proxy for its difficulty.
6 Experiments
We explore the performance of existing NLP tools
on targeted concept simplification and possible av-
enues for future improvement. More concretely,
we investigate the following research questions:
RQ1: What is the performance of out-of-the-box
NLP tools in this task?
RQ2: Which types of simplification strategies
improve human understanding of difficult concept s
and the definitions that they appear in?
RQ3: Fortargeted concept simplification, how
do human evaluations compare to automatic met-
rics commonly used in text simplification?
We perform experiments on the W IKIDOMAINS
test data created in Section 5. As an addi-
tional evaluation set, we also use the scientific
definitions dataset from August et al. (2022)
(SCIDEF) that contains definitions of science
terms extracted from Wikipedia glossaries and
MedQuAD (Ben Abacha and Demner-Fushman,
2019). We perform the same post-processing on
SCIDEFas with W IKIDOMAINS to select a difficult
concept within each definition.
6.1 Models
To explore the benchmark performance on this data,
we selected four popular LLMs: GPT-4 (OpenAI,
2023), PaLM-2 (Anil et al., 2023), Falcon-40b (Al-
mazrouei et al., 2023), and BLOOM-170b (Big-
Science Workshop, 2023). For the open-source
models, we selected the instruct versions with the
highest number of parameters available.
We also included a baseline approach of dictio-
nary look-up (non-LLM) to compare to the LLMs.
For this baseline, we looked up a definition of the
difficult concept and simply appended it to the end
of the original definition. We retrieved the def-
inition from Wikidata (Vrande Àáci¬¥c and Kr√∂tzsch,
2014), falling back on WordNet (Miller, 1994) if
the term was not found in Wikidata (or stated that
thedifficult concept ‚Äôs definition could not be found
if both sources failed).
6.2 Simplification Strategies and Prompts
In our preliminary user study (Section 3), we found
that users frequently indicated they would like
more details and context, as well as more gen-
eral breakdowns of writing complexity. These twoName DescriptionHuman Eval.Meaning preservation ( HMP) Human evaluation of whether the rewritten definition preserves the meaning of the
original definition (on a 5-point Likert scale; 5 = perfectly preserved).
Rewrite understanding ( HRU) Human evaluation of whether a reader can understand the rewritten definition if they
do not know the difficult concept (1 = yes, 0 = no).
Rewrite easier ( HRE) Human evaluation of whether the rewritten definition is easier to understand than the
original definition (1 = rewrite is easier; 0 = the original is easier or both are similar).Automatic Eval.Density Density (Grusky et al., 2018) is a measure of how extractive the rewritten definition
is from the original definition.
BLEU-4 B LEU-4score (Papineni et al., 2002) of the rewritten definition with respect to the
original definition.
BERTS CORE (BertSc) BERTS CORE (Zhang* et al., 2020) of the rewritten definition with respect to the
original definition.
Change in length ( ‚àÜLen) Average difference between the lengths (in number of tokens) of the rewritten and
the original definition (positive means the rewritten definition is longer than the
original).
Change in age of acquisition
(‚àÜAoA)Average difference of the top-10 percentile of the age-of-acquisition (Kuperman
et al., 2012) of the words between the rewritten and the original definition (positive
means the rewritten definition uses less complex words).
Change in Flesch ease
(‚àÜFlesch)Average difference of the Flesch reading ease (Flesch, 1948) between the rewritten
and the original definition (positive means the rewritten definition is at an easier
reading level than the original).
Table 3: Human and automatic metrics used to evaluate LLM rewritten text for concept simplification.
strategies also correspond to familiar approaches
for general text simplification tasks that rely on
elaboration (Srikanth and Li, 2021) and lexical
changes (Paetzold and Specia, 2016), respectively.
We chose two different prompts for the LLMs
that reflect these two simplification strategies. In
our first prompt, we show the model the term,defi-
nition , and difficult concept . We instruct the model
to rewrite the definition, ‚Äúintegrating an explana-
tion‚Äù for the difficult concept ( explain ). The sec-
ond prompt is similar, except we instruct the model
to rewrite the definition ‚Äúsimplifying‚Äù the difficult
concept word ( simplify ).
We chose the specific wording of the prompts
for the two strategies after a small scale analysis
of results with a few candidate prompts. We de-
scribe the candidate prompts, and the full phrasing
of the final selected prompts in the Appendix (Sec-
tion C). We report results using 3-shot settings for
the LLMs.8
6.3 Human Evaluation
We asked human raters to rate the rewritten defi-
nitions along dimensions of meaning preservation
and ease of understanding of the rewrites with re-
spect to both the difficult concept and the original
definition. Specifically, we asked them about (1)
meaning preservation , denoted as HMP: how much
does the rewrite preserve the meaning of the orig-
8We also experimented with a zero-shot settings with re-
sults in the Appendix.inal definition on a Likert scale of 5; (2) rewrite
understanding , denoted as HRU: If a reader is un-
familiar with the difficult concept , would they be
able to understand the rewrite (Yes/No); (3) rewrite
easier , denoted as HRE: Is the rewrite easier to
understand than the original? These dimensions
are summarized in the first three rows of Table 3.
We obtain judgments from 3 human raters per ex-
ample for 120 randomly selected examples from
the W IKIDOMAINS dataset and 60 randomly se-
lected examples from the S CIDEFdataset (2880
judgments in total). We provide exact wording of
the question, their rationale and annotator back-
ground in the Appendix (Section D). In Appendix
Table 14 we show Krippendorff‚Äôs alpha agreement
scores for each human evaluation dimension.
6.4 Automated Metrics
We investigate the utility of commonly used sim-
plification automated metrics for our task and com-
pare them to human judgments. Because our data
are reference-less, we cannot use reference-based
metrics like SARI (Xu et al., 2016). Instead, we es-
timate changes in complexity using the difference
between the rewritten and the original definition
in terms of: (1) age of acquisition (AoA; Kuper-
man et al. 2012), (2) Flesch reading ease (Flesch,
1948), and (3) token length. We also measure den-
sity (Grusky et al., 2018), which scores how ex-
tractive the rewritten defintion is from the original.
Lastly, we use BLEU (Papineni et al., 2002) andModel HMPHREHRUWIKIDOMAINSBaseline 4.66 0.31 0.79simplifyBloomz 4.25 0.20 0.53
Falcon 3.82 0.59 0.67
PaLM2 4.71 0.12 0.46
GPT4 4.43 0.29 0.75explainBloomz 4.53 0.43 0.69
Falcon 4.30 0.55 0.71
PaLM2 4.64 0.59 0.66
GPT4 4.47 0.75 0.82
Model HMPHREHRUSCIDEFBaseline 4.21 0.40 0.61simplifyBloomz 4.58 0.10 0.46
Falcon 4.24 0.25 0.57
PaLM2 4.57 0.12 0.62
GPT4 4.47 0.12 0.88explainBloomz 4.39 0.53 0.65
Falcon 4.29 0.53 0.86
PaLM2 4.86 0.18 0.54
GPT4 4.09 0.58 0.87
Table 4: Human evaluations of LLM-generated rewrites
for targeted concept simplification for the metrics HMP
(meaning preservation), HRE(rewrite easier), and HRU
(rewrite understanding) in 3-shot setting.
BERTS CORE (Zhang* et al., 2020) to score the
similarity of the rewritten definitions with respect
to the original definition. Table 3 presents a full list
of the human and automatic evaluations.
6.5 Model Rankings
Table 4 summarizes the evaluations of the rewrites
based on human judgment according to the mean-
ing preservation ( HMP), whether the rewrite is
easier to understand than the original ( HRE), and
whether the rewrite can be understood for someone
unfamiliar with the difficult concept (HRU).
We observe that no model excels in all dimen-
sions, though GPT-4 performs best on average.
Different models have distinct strengths; for in-
stance, PaLM2 excels in meaning preservation but
its rewrites are rarely easier to understand. Addi-
tionally, the dictionary-lookup baseline performs
comparably well to the LLM models.
Weaker scores on the HRUandHREdimensions
compared to the HMPdimension across all mod-
els, indicates opportunities for future research to
improve these scores.
6.6 Simplifying vs Explaining
We discuss the differences of the human evalua-
tions for the two prompt strategies‚Äî explain andPrompt HMPHREHRU
WIKI
DOMAINSsimplify 4.30 0.30 0.60
explain 4.48‚àó0.57‚àó0.72‚àó
SCIDEFsimplify 4.47 0.15 0.64
explain 4.41 0.45‚àó0.73‚àó
Table 5: Comparison of the prompts ‚Äì simplify and
explain ‚Äì for the human evaluation metrics. All results
are significantly different (ttest, p <0.01) marked by *,
except for the comparison of meaning preservation on
SCIDEF. Results are from the 3-shot setting.
DensityB4
BertScLen AoAFleschHmpHreHru0.23*** 0.31*** 0.28*** 0.10*** 0.03 -0.13***
-0.26*** -0.35*** -0.38*** 0.19*** 0.07*** 0.09***
-0.12*** -0.19*** -0.15*** 0.15*** 0.04 0.040.2
0.00.2
Figure 3: Pearson correlations between automated met-
rics and human evaluations (‚àó‚àó‚àó:p <0.005,‚àó‚àó:p <
0.05,‚àó:p <0.01).
simplify (introduced in Section 6.2). Table 5
shows the comparison of the prompt strategies
on human evaluation dimensions averaged across
the four LLMs. Human raters clearly preferred
rewrites where the model was asked to explain
thedifficult concept in both HREandHRUjudg-
ments. On W IKIDOMAINS data, human raters also
had a significant preference towards the ‚Äúexplain‚Äù
strategy when judging meaning preservation (the
difference in HMPon S CIDEFwas not significant).
This aligns with some of our observations from
our initial user study (Section 3), which found that
humans preferred adding more context (40%) as
opposed to simpler word substitutions (23%). This
highlights that adding elaborative details is very im-
portant towards facilitating human understanding
centered around difficult concepts.
6.7 Correlation between Human and
Automated Evaluation
Figure 3 shows the correlations between automated
metrics and human evaluations ( HMP,HRE,HRU).
We observe no single metric captures all the dimen-
sions of human evaluations. BLEU-4, Density, and
BERTS CORE show mild correlations with HMPas
they capture similarity of text. However, none of(a) Lustre or luster is the way light interacts with the
surface of a crystal, rock, or mineral.::::::Mineral:is::::::defined
:as::::::::"naturally::::::::occurring::::::usually:::::::inorganic::::::::substance:::that
::has::a::::(more::or::::less)::::::definite:::::::chemical::::::::::composition:::and:a
:::::crystal::::::::structure."
(b) Quality control, or QC for short, is a process by which
entities review the quality of all factors involved in pro-
duction.:::::Entity:is::::::defined::as:::::::::"something:::that::::exists::in:::the
:::::::identified:::::::universe.".
Table 6: Examples from the dictionary baseline, which
appends a definition shown in:::blue. (a) Added definition
has domain-specific jargon that may be unfamiliar to the
reader. (b) Added definition is vague, not accounting
for the context.
Economics : The Financial Stability Board (FSB) is an
international body that monitors and makes recommen-
dations about the global financial system:::::world‚Äôs::::::money .
(global financial system ) [PaLM2]
Biology :Jungle isanarea covered with dense
vegetation dominated bylarge trees, often tropical:A
:::::jungle:::is:a::::::region::::filled::::with:::::thick::::plant::::life,:::::often
::::::::dominated::by:::::large::::trees,:::::::typically:::::found:::in::::::tropical
::::areas. ( vegetation ) [GPT4]
Computing : Prolog is a logic programming language
associated with artificial intelligence and computational
linguistics. ( linguistics ) [Bloomz] [no change]
Table 7: Examples of concept simplification behavior
for the simplify 3-shot prompt from three domains: Eco-
nomics, Biology and Computing. The difficult concept
is shown in bold at the end of the definition. Deletions
are show in red; additions in::::blue.
automated metrics correlate with either HREorHRU
which capture comprehension related to the target
difficult concept . Even Flesch reading ease, which
is commonly used in text simplification setups, is
not adequate for measuring whether the rewrites
are easier understood. This result calls for new
metrics, beyond aggregate similarity measures, to
evaluate comprehension at the semantic level of
concepts.
In the Appendix (Table 15), we show the full
automated metric scores for each model, which
may be useful in characterizing some qualities of
the outputs. For example, Bloomz and PaLM2
make relatively few changes to the text (low ‚àÜLen),
and GPT4 chose considerably easier words in the
rewrite (high ‚àÜAoA). GPT4‚Äôs low meaning preser-
vation rating suggests choosing easier words is not
always desirable (Table 4). However, given the low
correlations with human scores, we generally keep
our observations about relative model rankings to
the human judgment.7 Discussion
We close our paper by discussing the research ques-
tions we posed in our experiments and how they
may relate to future improvements on this task.
Can LLMs support Contextual Explanations
of Difficult Text? Despite their instruction-
following capabilities, human evaluations indicate
that there‚Äôs still considerable room for improve-
ment at this task. Human judgments (Table 4) re-
veal that no model excels universally, each having
its own strengths and weaknesses. All models tend
to perform better at meaning preservation, though
other dimensions may be more crucial for enhanc-
ing broader comprehension (Kintsch, 1991).
Our evaluations support prior findings that
dictionary-based methods for simplification are
limited by availability and their inability to per-
sonalize to the reader‚Äôs context and background
knowledge (August et al., 2022). Table 6 shows
two examples from the dictionary baseline that are
either too vague or too complex to be useful to a
lay reader. However, we find that LLMs outper-
form the deterministic dictionary look-up baseline
only by a small margin depending on the dimen-
sion of quality. In examples of output (Table 7),
we can see failure cases where the models either
over-simplify text beyond just the difficult concept
or make no changes to the definition at all. LLMs
have been found to be useful for reading-grade
level simplifications (Agrawal and Carpuat, 2023),
yet they seem to struggle with making fine-grained
simplifications at the level of difficult concept s, call-
ing for more careful tooling for targeted simplifi-
cation. While more custom prompts may elicit
desired simplifications from LLMs, we cannot ex-
pect lay audience to be familiar with such prompt-
ing (Zamfirescu-Pereira et al., 2023).
Strategies Supporting Readers in Understand-
ing Difficult Text. Open-ended human feedback
about reading difficulties (Section 3) as well as
human judgment of differences between prompts
(Section 6.6) support the idea that adult readers
may prefer additional details and context addition
to understand difficult concept s. This echoes prior
cognitive science work that cues in text (e.g., ex-
planations, examples, analogies) enable readers
to effectively utilize their background knowledge
for comprehension (Kintsch, 1991; Van den Broek,
2010).Better Evaluations to Support Text Understand-
ing. As shown in prior work (Alva-Manchego
et al., 2021), we find that automated metrics cannot
capture fine-grained differences in simplification.
While there are some correlations between mean-
ing preservation and BLEU-4andBERTS CORE ,
we did not observe clear correlations of automated
metrics with other dimensions of comprehension,
such as alleviating difficulty with an unfamiliar con-
cept. We observe that many of these metrics rely on
brittle lexical scoring (Alva-Manchego et al., 2021),
and it may be necessary for automated metrics to
take more of the underlying concept structure of the
rewrites into account in order to adequately judge
whether the difficult concept has been explained
sufficiently. A separate LLM to score these dimen-
sions more reliably is an option (Wang et al., 2023;
Chen et al., 2023; Gao et al., 2024); however, hu-
man judgment currently remains the best standard
in this task.
8 Conclusion
To support comprehension of domain-specific text
for adult readers, we introduced the task of tar-
geted concept simplification to study fine-grained
simplification of difficult concepts in context. Our
human annotation study highlights the importance
of aiding users‚Äô understanding of these concepts
in domain-specific texts. We also introduced
WIKIDOMAINS , a dataset of 22k definitions across
13 academic domains, to support this task. Our
findings show a preference for strategies that add
explanatory details over simplifying difficult con-
cepts. Human evaluations of LLM rewrites indicate
considerable room for improvement, especially for
personalized help with difficult concepts.
9 Limitations
Difficulty with concepts varies based on personal
knowledge. Thus, it is challenging to build large-
scale evaluation corpora for domain-specific con-
cepts. Our dataset of domain-specific concepts is a
first step, providing a foundation for future work to
study comprehension across domains.
While we used popular LLMs at the time of con-
ducting the human evaluations, we also acknowl-
edge that some of our LLMs may no longer be
state-of-the-art when submitting the work. How-
ever, we will release our dataset and have described
our experimental setup to promote reproducibility
of the results with newer LLMs.We evaluated our work by asking human raters
to rate whether they can understand the defini-
tions. However, human reading comprehension
is also goal-directed, and different reading goals
will evoke different needs for details (Dunietz et al.,
2020). The details needed could differ depending
on using the text for one‚Äôs own understanding or us-
ing it for communicating it with other people about
specific aspects. E.g., a lawyer communicating
with engineers about the risks of a technology may
need focus on the applications rather than just the
understanding of technical concepts. Future work
can evaluate how supporting readers with concept
simplifications in documents (e.g., explanations,
examples, analogies, and illustrations) help them
develop a better understanding of the domain in
pre-post tests.
10 Ethical Considerations
We extract our domain-specific definitions dataset
from Wikipedia, which is publicly available and
accessible to all. However, Wikipedia content has
a Global North bias because of its editor base, and
concepts in our domain-specific dataset will reflect
this bias. We also acknowledge the broader edu-
cational implications of making definitions easier
to understand, and that using LLMs could intro-
duce false information. While in our work we did
not observe instances of hallucinations, LLMs may
introduce false information when rewriting entire
documents or narratives, and we need robust mea-
sures to validate the faithfulness of rewritten defini-
tion in addressing concept difficulty and providing
correct facts. While our evaluations attempt to
provide initial insights into LLM‚Äôs behavior with
difficult concepts in domain-specific text, we also
acknowledge that concept difficulty is a complex
construct, and it can be dependent on a reader‚Äôs age,
educational, and professional background, which
future evaluations should consider.
Acknowledgements
We thank the anonymous reviewers for their in-
sightful feedback and suggestions. We‚Äôd also like
to thank Priyanka Agrawal and Slav Petrov for
their feedback on the paper manuscript, as well as
the other members of the Google DeepMind com-
munity for their guidance throughout the project.
We also thank the anonymous individuals who par-
ticipated in our human evaluations and rated the
definitions.References
Sweta Agrawal and Marine Carpuat. 2023. Control-
ling pre-trained language models for grade-specific
text simplification. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 12807‚Äì12819, Singapore. Associ-
ation for Computational Linguistics.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Merouane Debbah, Etienne Goffinet, Daniel Hes-
low, Julien Launay, Quentin Malartic, Badreddine
Noune, Baptiste Pannier, and Guilherme Penedo.
2023. Falcon-40B: an open large language model
with state-of-the-art performance.
Fernando Alva-Manchego, Carolina Scarton, and Lu-
cia Specia. 2021. The (Un)Suitability of Automatic
Evaluation Metrics for Text Simplification. Compu-
tational Linguistics , 47(4):861‚Äì889.
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, Eric Chu, Jonathan H. Clark, Laurent El
Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
rav Mishra, Erica Moreira, Mark Omernick, Kevin
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez
Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,
Jan Botha, James Bradbury, Siddhartha Brahma,
Kevin Brooks, Michele Catasta, Yong Cheng, Colin
Cherry, Christopher A. Choquette-Choo, Aakanksha
Chowdhery, Cl√©ment Crepy, Shachi Dave, Mostafa
Dehghani, Sunipa Dev, Jacob Devlin, Mark D√≠az,
Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Gar-
cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-
Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua
Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur-
witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-
ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,
Sneha Kudugunta, Chang Lan, Katherine Lee, Ben-
jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,
Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,
Frederick Liu, Marcello Maggioni, Aroma Mahendru,
Joshua Maynez, Vedant Misra, Maysam Moussalem,
Zachary Nado, John Nham, Eric Ni, Andrew Nys-
trom, Alicia Parrish, Marie Pellat, Martin Polacek,
Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,
Bryan Richter, Parker Riley, Alex Castro Ros, Au-
rko Roy, Brennan Saeta, Rajkumar Samuel, Renee
Shelby, Ambrose Slone, Daniel Smilkov, David R.
So, Daniel Sohn, Simon Tokumine, Dasha Valter,
Vijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,
Pidong Wang, Zirui Wang, Tao Wang, John Wiet-
ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven
Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav
Petrov, and Yonghui Wu. 2023. Palm 2 technical
report. Preprint , arXiv:2305.10403.
Sumit Asthana and Aaron Halfaker. 2018. With feweyes, all hoaxes are deep. Proc. ACM Hum.-Comput.
Interact. , 2(CSCW).
Tal August, Katharina Reinecke, and Noah A. Smith.
2022. Generating scientific definitions with control-
lable complexity. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 8298‚Äì8317,
Dublin, Ireland. Association for Computational Lin-
guistics.
Frederic Charles Bartlett. 1995. Remembering: A study
in experimental and social psychology . Cambridge
university press.
Asma Ben Abacha and Dina Demner-Fushman. 2019. A
question-entailment approach to question answering.
BMC bioinformatics , 20(1):1‚Äì23.
BigScience Workshop. 2023. Bloom: A 176b-
parameter open-access multilingual language model.
Preprint , arXiv:2211.05100.
Joachim Bingel, Gustavo Paetzold, and Anders S√∏gaard.
2018. Lexi: A tool for adaptive, personalized text
simplification. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics ,
pages 245‚Äì258, Santa Fe, New Mexico, USA. Asso-
ciation for Computational Linguistics.
Or Biran, Samuel Brody, and No√©mie Elhadad. 2011.
Putting it simply: a context-aware approach to lex-
ical simplification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies , pages
496‚Äì501, Portland, Oregon, USA. Association for
Computational Linguistics.
Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and
Ruifeng Xu. 2023. Exploring the use of large lan-
guage models for reference-free text quality evalua-
tion: An empirical study. In Findings of the Associa-
tion for Computational Linguistics: IJCNLP-AACL
2023 (Findings) , pages 361‚Äì374, Nusa Dua, Bali.
Association for Computational Linguistics.
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke
Zettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-
tiplication for transformers at scale. arXiv preprint
arXiv:2208.07339 .
Jesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen
Rambow, Jennifer Chu-Carroll, and Dave Ferrucci.
2020. To test machine comprehension, start by defin-
ing comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 7839‚Äì7859, Online. Association
for Computational Linguistics.
Noemie Elhadad and Komal Sutaria. 2007. Mining a
lexicon of technical terms and lay equivalents. In
Biological, translational, and clinical language pro-
cessing , pages 49‚Äì56, Prague, Czech Republic. As-
sociation for Computational Linguistics.Mehwish Fatima and Michael Strube. 2023. Cross-
lingual science journalism: Select, simplify and
rewrite summaries for non-expert readers. In Pro-
ceedings of the 61st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 1843‚Äì1861, Toronto, Canada. Associ-
ation for Computational Linguistics.
Rudolf Franz Flesch. 1948. A new readability yardstick.
The Journal of applied psychology , 32 3:221‚Äì33.
Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun
Wan. 2024. Llm-based nlg evaluation: Current status
and challenges. ArXiv , abs/2402.01383.
Cristina Garbacea, Mengtian Guo, Samuel Carton, and
Qiaozhu Mei. 2021. Explainable prediction of text
complexity: The missing preliminaries for text sim-
plification. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 1086‚Äì1097, Online. Association for Computa-
tional Linguistics.
Arthur C Graesser, Murray Singer, and Tom Trabasso.
1994. Constructing inferences during narrative text
comprehension. Psychological review , 101(3):371.
Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
Newsroom: A dataset of 1.3 million summaries with
diverse extractive strategies. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers) , pages 708‚Äì719, New Orleans, Louisiana. As-
sociation for Computational Linguistics.
Yue Guo, Joseph Chee Chang, Maria Antoniak, Erin
Bransom, Trevor Cohen, Lucy Lu Wang, and Tal
August. 2023. Personalized jargon identification for
enhanced interdisciplinary communication. ArXiv ,
abs/2311.09481.
Isaac Johnson. 2021. Wikipedia Article Topics for All
Languages (based on article outlinks).
Sebastian Joseph, Kathryn Kazanas, Keziah Reina, Vish-
nesh Ramanathan, Wei Xu, Byron Wallace, and
Junyi Jessy Li. 2023. Multilingual simplification
of medical texts. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 16662‚Äì16692, Singapore. Associ-
ation for Computational Linguistics.
David Kauchak. 2013. Improving text simplification
language modeling using unsimplified text data. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 1537‚Äì1546, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Tannon Kew, Alison Chi, Laura V√°squez-Rodr√≠guez,
Sweta Agrawal, Dennis Aumiller, Fernando Alva-
Manchego, and Matthew Shardlow. 2023. BLESS:
Benchmarking large language models on sentencesimplification. In Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 13291‚Äì13309, Singapore. Association
for Computational Linguistics.
Walter Kintsch. 1991. The role of knowledge in dis-
course comprehension: A construction-integration
model. In Advances in psychology , volume 79, pages
107‚Äì153. Elsevier.
Walter Kintsch and Teun A Van Dijk. 1978. Toward a
model of text comprehension and production. Psy-
chological review , 85(5):363.
Victor Kuperman, Hans Stadthagen-Gonzalez, and
Marc Brysbaert. 2012. Age-of-acquisition ratings
for 30,000 english words. Behavior research meth-
ods, 44:978‚Äì990.
Robert F Lorch Jr and Paul van den Broek. 1997. Under-
standing reading comprehension: Current and future
contributions of cognitive science. Contemporary
educational psychology , 22(2):213‚Äì246.
George A. Miller. 1994. WordNet: A lexical database
for English. In Human Language Technology: Pro-
ceedings of a Workshop held at Plainsboro, New
Jersey, March 8-11, 1994 .
Janni Nielsen, Torkil Clemmensen, and Carsten Yssing.
2002. Getting access to what goes on in people‚Äôs
heads? reflections on the think-aloud technique. In
Proceedings of the second Nordic conference on
Human-computer interaction , pages 101‚Äì110.
Ethel Ong, Jerwin Damay, Gerard Lojico, Kimberly
Lu, and Dex Tarantan. 2007. Simplifying text in
medical literature. Journal of Research in Science,
Computing and Engineering , 4(1):37‚Äì47.
OpenAI. 2023. Gpt-4 technical report. Preprint ,
arXiv:2303.08774.
Gustavo Paetzold and Lucia Specia. 2016. Anita: An
intelligent text adaptation tool. In Proceedings of
COLING 2016, the 26th International Conference on
Computational Linguistics: System Demonstrations ,
pages 79‚Äì83, Osaka, Japan. The COLING 2016 Or-
ganizing Committee.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. pages 311‚Äì318.
Cecile L. Paris. 1988. Tailoring object descriptions to a
user‚Äôs level of expertise. Computational Linguistics ,
14(3):64‚Äì78.
Luz Rello, Ricardo Baeza-Yates, Stefan Bott, and Ho-
racio Saggion. 2013. Simplify or help? text simpli-
fication strategies for people with dyslexia. In Pro-
ceedings of the 10th International Cross-Disciplinary
Conference on Web Accessibility , W4A ‚Äô13, New
York, NY , USA. Association for Computing Machin-
ery.Neha Srikanth and Junyi Jessy Li. 2021. Elaborative
simplification: Content addition and explanation gen-
eration in text simplification. In Findings of the
Association for Computational Linguistics: ACL-
IJCNLP 2021 , pages 5123‚Äì5137, Online. Association
for Computational Linguistics.
Paul Van den Broek. 2010. Using texts in science edu-
cation: Cognitive processes and knowledge represen-
tation. Science , 328(5977):453‚Äì456.
Denny Vrande Àáci¬¥c and Markus Kr√∂tzsch. 2014. Wiki-
data: a free collaborative knowledgebase. Commun.
ACM , 57(10):78‚Äì85.
Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui
Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,
and Jie Zhou. 2023. Is ChatGPT a good NLG evalua-
tor? a preliminary study. In Proceedings of the 4th
New Frontiers in Summarization Workshop , pages
1‚Äì11, Singapore. Association for Computational Lin-
guistics.
Wei Xu, Chris Callison-Burch, and Courtney Napoles.
2015. Problems in current text simplification re-
search: New data can help. Transactions of the Asso-
ciation for Computational Linguistics , 3:283‚Äì297.
Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,
and Chris Callison-Burch. 2016. Optimizing sta-
tistical machine translation for text simplification.
volume 4, pages 401‚Äì415.
J.D. Zamfirescu-Pereira, Richmond Y . Wong, Bjoern
Hartmann, and Qian Yang. 2023. Why johnny can‚Äôt
prompt: How non-ai experts try (and fail) to design
llm prompts. In Proceedings of the 2023 CHI Confer-
ence on Human Factors in Computing Systems , CHI
‚Äô23, New York, NY , USA. Association for Computing
Machinery.
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-
uating text generation with bert. In International
Conference on Learning Representations .
Xingxing Zhang and Mirella Lapata. 2017. Sentence
simplification with deep reinforcement learning. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing , pages 584‚Äì
594, Copenhagen, Denmark. Association for Compu-
tational Linguistics.
AUser study for understanding difficulty
with definitions
As a preliminary study of reading difficulty, we
asked annotators to read 900 concept definitions
from W IKIDOMAINS and describe difficulties they
have in understanding the definition text.
As shown in Figure 4 we asked each participant
the following questions 1) ‚ÄúPlease tell us the diffi-
culties that you face in understanding the concept
C from the definition,‚Äù 2) ‚ÄúIf you could ask a tutorto make changes to the definition to increase the
knowledge and clarity of the concept for you or
someone else, what would you ask them to change
(add/edit/remove).‚Äù The first question attempts to
understand the difficulties that lay people may face
with domain specific definitions. The second ques-
tion attempts to involve users in the thinking pro-
cess of asking a tutor to rewrite the definition. Prior
studies in human-centered research suggest that in-
volving users in the task elicits better task-specific
challenges than simply asking about the difficul-
ties alone (Nielsen et al., 2002). We keep the task
open-ended and ask for free-text responses to give
annotators freedom to express any challenges in
reading the material.
Following the completion of the study, two of
the authors reviewed a random subset of 900 re-
sponses (450 responses to Q1 and 450 responses to
Q2). They agreed that when annotators had issues
with the reading material, it could generally fit into
categories below:
‚Ä¢Didn‚Äôt understand a mentioned concept: The
annotator referenced a specific word or phrase
that was mentioned in the text that hindered
their understanding
‚Ä¢Want *more* details or other context: The
annotator referenced missing details or addi-
tional background context that would have
helped their understanding
‚Ä¢Want *less* detail: the annotator said that the
definition text included unnecessary detail
‚Ä¢General writing complexity: the annotator ref-
erenced the overall reading level, syntactic, or
lexical complexity of the text
‚Ä¢Want an example or analogy: the annotator
said that an example or analogy would be
needed for their understanding
‚Ä¢Want visual or audio reference: that annota-
tor said that they needed visual or auditory
supplements for understanding the text
‚Ä¢Text was ill-formed: the annotator said the
text was ill-formed in some way that made it
difficult to read
‚Ä¢General confusion: annotator expressed gen-
eral confusion without listing a specific pain
pointFigure 4: Screenshot of an annotation example for un-
derstanding difficulties that readers face with domain
specific text.
For the responses where an annotator identified a
difficulty (which is 57% of the responses), each was
labelled with one or more of the categories above
(i.e., categories are not mutually exclusive). The
results of this grouping is summarized in Figure 2.
A.1 User demographics for evaluations of
difficulty with definitions
Table 8 summarizes the demographics of partic-
ipants who evaluated the difficulty with domain-
specific definitions.
Background Percentage
4-year college degree 53%
Master‚Äôs degree 12%
2-year college degree 18%
Some college 10%
High school 3%
Professional degree (MD, JD, etc) 2%
Doctoral degree (PhD) 2%
Table 8: Educational background of annotators for hu-
man evaluations of difficulty with definitions. Total
number of annotators was 28.
B W IKIDOMAINS dataset construction
Editors on Wikipedia have voluntarily come to-
gether to form focus groups, called WikiProjects,
dedicated to curating and improving articles inspecific domains or interest areas, such as Eco-
nomics, Chemistry, Literature (Asthana and Hal-
faker, 2018). Any Wikipedia editor can join differ-
ent WikiProjects and participate in editing articles
in that specific WikiProject. As part of the WikiPro-
ject effort, Wikipedia editors have annotated a large
number of articles on Wikipedia with their WikiPro-
ject topic assignments and developed a hierarchical
taxonomy of topics called the Wikiprojects direc-
tory9. The dataset contains articles from the entire
Wikipedia annotated by domains (broad academic
topics) derived from Wikiprojects. We use the top-
ics in the first two levels of this categorization as
domains because they represent broad domain cat-
egorizations.
B.1 Domain selection criteria
We selected domains where majority of articles re-
lated to names of academic concepts or processes
in the domain. Thus, we needed to exclude articles
about people, events, names of things (e.g., music
albums). For example, the Biography domain con-
tains biographical articles of famous personalities,
and the Military domain contains articles on histor-
ical military conflicts. To identify such domains,
the lead author manually assessed a random sample
of 100 articles in each domain. If the number of
articles in each domain that corresponded to named
entities exceeded 50% of the assessed articles, we
dropped that domain. This is because our work
is focused on academically challenging concepts
and how they are explained in terms of other con-
cepts. While articles of named entities may contain
challenging concepts, the concepts themselves and
their explanations in the domain is not the focus
of the article. E.g., an article on World War II will
likely contain concepts like ‚Äúdiplomacy‚Äù, but its
explanation will not be the main focus of the article.
We finally excluded the domains: Internet-culture,
Literature, Religion, History, Geography, Military-
and-warfare, Transportation, Society, Sports, Li-
braries and Information, Space, and STEM.STEM*
(because this is a superset of the domains: Physics,
Chemistry, Mathematics, Biology).
B.2 Dataset snapshot
Table 9 shows a snapshot of the W IKIDOMAINS
dataset.
9https://en.wikipedia.org/wiki/
Wikipedia:WikiProject_DirectoryTerm Topic Wikipedia lead
sectionDefinition Difficult
concept
Electron gun Physics An electron
gun...by the
number of
electrodes.An electron gun (also called electron
emitter) is an electrical component in
some vacuum tubes that produces a nar-
row, collimated electron beam that has
a precise kinetic energy.electron
Vala (programming
language)Comp. Vala is an ... in
May 2006.Vala is an object-oriented programming
language with a self-hosting compiler
that generates C code and uses the GOb-
ject system.compiler
Table 9: Snapshot of the W IKIDOMAINS dataset
B.3 Difficult concept statistics
In roughly 85% of W IKIDOMAINS examples, we
extracted the difficult concept using a ratio of how
often the concept appears within this domain com-
pared to Wikipedia overall (Equation 1). For those
examples, the average computed ratio for the se-
lected difficult concept is 0.9. In the remaining 15%
of examples, the difficult concept was chosen us-
ing the age of acquisition lexicon (Kuperman et al.,
2012). On average, each difficult concept contains
1.3 tokens.
C Prompts
We experimented with 4 candidate prompts for
both explain andsimplify prompts categories.
Table 10 outlines these candidate prompts. To iden-
tify the best prompt, we applied the prompts to
a set of 100 randomly sampled definitions from
the W IKIDOMAINS and S CIDEFdatasets, and the
lead author manually assessed the goodness of the
rewrites, assigning a binary label 1 or 0 to each of
the rewrites, indicating whether the rewrite success-
fully addresses the concept difficult or not respec-
tively. The prompts that we use in our experimental
setup had the highest number of definitions where
the rewrite was assessed as a good rewrite.
Table 11 details the ‚Äúsimplify‚Äù and ‚Äúexplain‚Äù
prompts that we used in our study.
D Instructions for human evaluation
To evaluate LLM-generated definitions for their
suitability for simplifying domain specific con-
cepts, we show a human rater the following 1)
the original definition, 2) a difficult concept cd
within the definition that we identified, 3) the LLM-
rewritten definitions. We ask raters to answer thefollowing 1) Please rate on a scale of 1-5 how
much the REWRITE preserves the meaning of the
original, 2) Can someone understand the defini-
tion if they do not know the difficult concept: X?
(Yes/No), 3) Please rate which of the ORIGINAL
and REWRITE are easier to understand? (Origi-
nal/Rewrite/Both), 4) Please rate your level of fa-
miliarity with the concept.
Rationale for human evaluation questions We
cannot control readers‚Äô familiarity with the concept,
therefore we rely on their understanding to deter-
mine someone‚Äôs ability to understand the definition
without knowledge of the difficult concept . How
much is a definition understandable to someone is
dependent on their background knowledge. There-
fore, by asking whether the REWRITE is easier to
understand or the ORIGINAL definition, we rely
on the annotator‚Äôs opinion of whether the rewritten
definition gives them a better understanding of the
topic.
Each annotator was presented with about 15 def-
initions to answer questions about, and the total
annotation time per annotator was about 20-25 min-
utes. Before the annotation, we briefed the annota-
tors about task and provided two examples to help
them understand the task of concept simplification.
Figure 5 shows screenshot of the annotation task.
We displayed a consent form to the participants
detailing the study and that the risks would be no
more than assessing definitions written by AI and
gave them the option to leave the study at any time.
We compensated the participants above the hourly
minimum wage based on their demographic loca-
tion. The study was approved by the internal ethics
review team.
We collected the educational background of an-Prompt Strategy Prompt text
simplify Rewrite the definition simplifying the concept: ‚Äúcerebellum.‚Äù
simplify Rewrite the definition making the concept simpler: ‚Äúcerebellum.‚Äù
simplify Rewrite the definition making the concept simpler: ‚Äúcerebellum.‚Äù
simplify Rewrite the definition simplifying difficulty with the concept: ‚Äúcerebellum.‚Äù
explain Rewrite the definition integrating an explanation for the concept: ‚Äúcerebellum.‚Äù
explain Rewrite the definition adding an explanation for the concept: ‚Äúcerebellum.‚Äù
explain Rewrite the definition providing an explanation of the concept: ‚Äúcerebellum.‚Äù
explain Rewrite the definition to add content that explains the concept: ‚Äúcerebellum.‚Äù
Table 10: Candidate prompts that we explored
Prompt Strategy Prompt text
simplifyRewrite the definition simplifying the concept: ‚Äúcerebellum‚Äù.
Definition: Chiari malformations (CMs) are structural defects in the cerebellum.
Rewrite:
explainRewrite the definition integrating an explanation for the concept: ‚Äúcerebellum‚Äù.
Definition: Chiari malformations (CMs) are structural defects in the cerebellum.
Rewrite:
Table 11: Prompts used in the experimental evaluation
notators, summarized in Table 12.
Background Percentage
4-year college degree 48%
Master‚Äôs degree 16%
2-year college degree 14%
Some college 13%
High school 3%
Professional degree (MD, JD, etc) 2%
Doctoral degree (PhD) 0.8%
Table 12: Educational background of annotators for
human evaluations of LLM-rewrites. Total number of
annotators was 229.
E Inference setting
For open-source models, we run inference on GPUs
using the Huggingface10transformers implemen-
tation. To fit Falcon and Bloom models on the
available GPUs, we run the models with 8-bit quan-
tization (Dettmers et al., 2022). For the commercial
models, we use the publicly available APIs to query
the models and generate outputs. For all LLMs, we
use top- ksampling11.
10huggingface.co
11We set the value of kto 40F Results of Zero-shot Prompting
See Table 13 for the zero-shot performance results.
As expected, scores are generally lower with zero-
shot than few-shot. In particular, ICL examples
seem to help with the meaning preservation dimen-
sion pretty consistently.
G Human evaluation agreement
Table 14 shows the human evaluation agreement
scores for our study. The inter-annotator alpha
scores show weak agreement (in the range between
0.2-0.3), which is somewhat expected due to the
subjective nature of some evaluations. In aggre-
gating scores we use the majority vote between
the three annotators (or the mean in the case of
HMP). The Krippendorff‚Äôs alpha between individ-
ual ratings and the majority vote falls in the range
of 0.6-0.7, showing that individual ratings are gen-
erally closely aligned with the majority rating.
H Automatic Metric Performances
In Table 15, we present the model performances on
different automatic metrics.Figure 5: Annotation example for evaluating LLM
rewritten definitions for concept simplification.LLM HMPHREHRUWIKIDOMAINS
simplifyBloomz 4.12 0.23 0.48
Falcon 3.88 0.53 0.78
PaLM2 4.27 0.08 0.60
GPT4 4.02 0.67 0.70explainBloomz 4.23 0.18 0.41
Falcon 4.00 0.47 0.65
PaLM2 4.14 0.26 0.63
GPT4 4.11 0.72 0.89SCIDEF
simplifyBloomz 4.01 0.17 0.56
Falcon 3.87 0.49 0.56
PaLM2 4.21 0.05 0.68
GPT4 4.36 0.47 0.93explainBloomz 4.73 0.03 0.38
Falcon 3.72 0.47 0.63
PaLM2 4.53 0.37 0.64
GPT4 4.34 0.53 0.83
Table 13: Human evaluations of LLM-generated
rewrites for targeted concept simplification with zero-
shot setting
Krippendorff‚Äôs Alpha
Metric IAA Ann vs. Majority
HMP 0.31 0.70
HRU 0.21 0.65
HRE 0.25 0.62
Table 14: Krippendorff‚Äôs alpha scores for the human
evaluations of meaning preservation ( HMP, an interval
score out of 5), rewrite understanding ( HRU, binary
score), and rewrite easier ( HRE, binary score). We report
coefficients between pairs of annotators (IAA = inter-
annotator agreement) and also the agreement between
individual annotation and the majority vote label for that
example (Ann vs Majority).
I Qualitative examples
Table 16 discusses several example outputs from
thesimplify prompt and identifies potential prob-
lems in some of the generated definitions.
J Qualitative examples from the simplify
vs.explain prompts
Table 17 shows side-by-side examples of the rewrit-
ten definitions when the same model is prompted
with the simplify prompt versus the explain prompt.LLM Density B LEU-4 BertSc ‚àÜLen ‚àÜAoA ‚àÜFleschWIKIDOMAINSBloomz 11.78 0.53 0.89 1.61 -0.24 7.30
Falcon 6.62 0.27 0.80 12.72 4.44 9.10
PaLM2 19.57 0.74 0.92 4.06 2.28 1.72
GPT4 4.62 0.24 0.83 20.28 5.77 8.50SCIDEFBloomz 12.81 0.62 0.92 2.35 -0.02 5.16
Falcon 3.67 0.20 0.76 9.92 4.55 4.51
PaLM2 17.67 0.80 0.95 2.71 0.16 2.64
GPT4 4.24 0.24 0.83 17.85 3.45 7.41
Table 15: Automated evaluations of concept simplifications on the full test set of W IKIDOMAINS and S CIDEFNotes Original definition Rewritten definition
PaLM2, Difficult concept: zenith Tropic Of Capricorn is the south-
ernmost latitude of latitude on the
Earth at which the Sun appears
zenith overhead at its culmina-
tion, which lies approximately
23.4 degrees south of the Equa-
tor.Tropic Of Capricorn is the south-
ernmost latitude of latitude on the
Earth at which the Sun appears
directly overhead at its culmina-
tion, which lies approximately
23.4 degrees south of the Equa-
tor.
GPT4, Difficult concept: zenith
Note: Correctly simplifies diffi-
cult concept but changes defini-
tion significantly.Tropic Of Capricorn is the south-
ernmost latitude of latitude on the
Earth at which the Sun appears
zenith overhead at its culmina-
tion, which lies approximately
23.4 degrees south of the Equa-
tor.The Tropic of Capricorn is the
furthest point south on Earth
where the Sun can be seen di-
rectly overhead at its highest
point. This is roughly 23.4 de-
grees south of the Equator.
PaLM2, Difficult concept: vege-
tationJungle is an area covered with
dense vegetation dominated by
large trees, often tropical.Jungle is an area covered with
dense plants dominated by large
trees, often tropical.
GPT4, Difficult concept: vege-
tation Note: Correctly simplifies
difficult concept but changes def-
inition significantly.Jungle is an area covered with
dense vegetation dominated by
large trees, often tropical.A jungle is a place filled with a
lot of plants and big trees, usually
found in hot regions.
Bloomz, Difficult concept: lin-
guistics Note: Makes no change.Prolog is is a logic programming
language associated with artifi-
cial intelligence and computa-
tional linguistics.Prolog is a logic programming
language associated with artifi-
cial intelligence and computa-
tional linguistics.
GPT4, Difficult concept: linguis-
tics Note: Correctly simplifies
difficult concept but changes def-
inition significantly.Prolog is is a logic programming
language associated with artifi-
cial intelligence and computa-
tional linguistics.Prolog is a computer language
used in artificial intelligence and
the study of how computers can
understand and use human lan-
guage.
PaLM2, Difficult concept: ovoid
Note: model drops information.A coccus (plural cocci) is any
bacterium that has a spherical,
ovoid, or generally round shape.A coccus (plural cocci) is any
bacterium that has a spherical, or
generally round shape.
GPT4, Difficult concept: ovoid A coccus (plural cocci) is any
bacterium that has a spherical,
ovoid, or generally round shape.A coccus (or cocci for many) is a
type of bacterium that is shaped
like a sphere, an egg, or is gener-
ally round.
Table 16: LLM rewrite examples for simplify promptsmodel original definition simplify definition explain definition
Falcon (0-
shot)Jungle is an area cov-
ered with dense veg-
etation dominated by
large trees, often tropi-
cal.The jungle is a lush
and verdant forest char-
acterized by towering
trees and dense under-
growth.The jungle is a vast expanse of
land that is covered in dense veg-
etation, primarily consisting of
tall trees and other plants. It is
characterized by its humid cli-
mate and high levels of rainfall,
which support the growth of a
wide variety of flora and fauna.
PaLM2
(3-shot)Chiari malformations
(CMs) are structural de-
fects in the cerebel-
lum.Chiari malformations
(CMs) are structural de-
fects in the brain.Chiari malformations (CMs) are
structural defects in the cerebel-
lum. The cerebellum is a region
of the brain that plays an impor-
tant role in motor control.
GPT4 (3-
shot)Hamlet is a small hu-
man settlement, vari-
ably defined as one the
size of a town, vil-
lage, or parish or as a
smaller subdivision of
or satellite entity to a
larger settlement.Hamlet is a small hu-
man settlement, vari-
ably defined as one the
size of a town, village,
or church district or as
a smaller subdivision
of or satellite entity to
a larger settlement.Hamlet is a small human settle-
ment, variably defined as one the
size of a town, village, or parish
or as a smaller subdivision of or
satellite entity to a larger settle-
ment. A parish is a type of admin-
istrative division used by some
churches and by some civil au-
thorities.
Bloomz
(3-shot)Arginase deficiency is
an inherited disorder
that causes the amino
acid arginine (a build-
ing block of proteins)
andammonia to accu-
mulate gradually in the
blood.Arginase deficiency is
a rare disease in which
the body is unable
to process certain pro-
teins properly.Arginase deficiency is an in-
herited disorder that causes the
amino acid arginine (a building
block of proteins) and ammonia
to accumulate gradually in the
blood. Ammonia is the toxic by-
product of protein metabolism.
Table 17: Definitions rewritten with simplify vs.explain prompts. The difficult concept is in bold.