Efficient Performance Tracking: Leveraging Large Language Models for
Automated Construction of Scientific Leaderboards
Furkan ¬∏ Sahinu√ß1, Thy Thy Tran1, Yulia Grishina2,
Yufang Hou1,3, Bei Chen2, Iryna Gurevych1
1Ubiquitous Knowledge Processing Lab (UKP Lab)
Technical University of Darmstadt and Hessian Center for AI (hessian.AI)
2Amazon Alexa AI - Berlin, Germany
3IBM Research Europe - Ireland
www.ukp.tu-darmstadt.de
Abstract
Scientific leaderboards are standardized rank-
ing systems that facilitate evaluating and com-
paring competitive methods. Typically, a
leaderboard is defined by a task,dataset , and
evaluation metric (TDM) triple, allowing objec-
tive performance assessment and fostering in-
novation through benchmarking. However, the
exponential increase in publications has made
it infeasible to construct and maintain these
leaderboards manually. Automatic leaderboard
construction has emerged as a solution to re-
duce manual labor. Existing datasets for this
task are based on the community-contributed
leaderboards without additional curation. Our
analysis shows that a large portion of these
leaderboards are incomplete, and some of them
contain incorrect information. In this work,
we present SCILEAD, a manually-curated
Scientific Lead erboard dataset that overcomes
the aforementioned problems. Building on
this dataset, we propose three experimental set-
tings that simulate real-world scenarios where
TDM triples are fully defined, partially defined,
or undefined during leaderboard construction.
While previous research has only explored the
first setting, the latter two are more represen-
tative of real-world applications. To address
these diverse settings, we develop a compre-
hensive LLM-based framework for construct-
ing leaderboards. Our experiments and analysis
reveal that various LLMs often correctly iden-
tify TDM triples while struggling to extract
result values from publications. We make our
code1and data2publicly available.
1 Introduction
The comparison of state-of-the-art scientific meth-
ods has become a major challenge for the research
community due to the rapidly growing number
of scientific publications (Landhuis, 2016; Born-
mann et al., 2021; Mohammad, 2020). For ex-
1GitHub: UKPLab/leaderboard-generation
2Data: TUdatalib
Previous Work 1. Paper Y üèÜ
2. Paper X 1. Paper X üèÜ
2. Paper Y   Existing Leaderboards 
Paper A Paper B 
1. Paper B üèÜ
2. Paper A 1. Paper B üèÜ
2. Paper Y 
3. Paper X 1. Paper X üèÜ
2. Paper A 
3. Paper Y   Create Update Update 
Ours ‚Ä¶Figure 1: We first extract task, dataset, metric, and result
(TDMR) tuples from scientific publications. Then, we
update existing leaderboards of the same TDM (purple
and blue). Different from previous work, we also con-
struct a new leaderboard on demand (green).
ample, around 100 papers are submitted daily to
the arXiv pre-print repository under the Compu-
tation and Language category3alone. To facil-
itate monitoring of research progress and com-
parison of SOTA model performance, scientific
leaderboard platforms have been introduced, such
asNLP-progress4orpaperswithcode5(PwC ). A
scientific leaderboard is typically formalized as a
task,dataset , and evaluation metric (TDM) triple,
ranking performance ( result ) of competitive meth-
ods against the triple. However, the majority of
these leaderboards are manually curated and main-
tained, which is infeasible over time due to the
ever-expanding number of publications.
Automatic leaderboard construction has been
proposed to offer a solution (Hou et al., 2019),
which aims to extract and compare performance
from scientific publications against benchmarks.
Previous studies have released datasets based on
community-contributed platforms with minimum
3https://arxiv.org/
4https://nlpprogress.com/
5https://paperswithcode.com/
1arXiv:2409.12656v1  [cs.CL]  19 Sep 2024quality control, such as normalizing the TDM en-
tities across different leaderboards (Singh et al.,
2019; Hou et al., 2019, 2021; Kabongo et al., 2021,
2023a,b; Yang et al., 2022; Singh et al., 2024).
However, these annotations are limited by indi-
vidual researchers‚Äô varying research interests, re-
sulting in incomplete coverage of papers within a
leaderboard and a lack of TDMs from each paper
(see Appendix A for a detailed example). Such par-
tial annotations hinder the evaluation of automatic
leaderboard construction systems.
Furthermore, all previous work on leaderboard
construction mentioned above has formulated the
task as matching the extracted TDM triples from a
paper against a pre-defined TDM triple taxonomy.
This assumption, however, falls short in capturing
the dynamic nature of real-world scenarios. In re-
ality, the landscape of leaderboards is constantly
evolving, with new ones emerging as a natural con-
sequence of ongoing research and innovation.
To this end, we introduce SCILEAD, a manually-
curated Scientific Lead erboard dataset, including
27 leaderboards derived from 43 NLP papers. To
avoid the pitfall of community-contributed leader-
boards with incomplete and inaccurate information,
we exhaustively annotate all unique TDM triples
and the corresponding results from each paper and
construct a complete leaderboard set for these pa-
pers. As illustrated in Figure 1, we propose a three-
stage framework for constructing leaderboards,
harnessing the power of Large Language Mod-
els (LLMs) through Retrieval-Augmented Gen-
eration (RAG) prompting to streamline the pro-
cess: (i) extracting task, dataset, metric and re-
sult (TDMR) tuples from individual papers; (ii)
normalizing extracted TDM triples to the existing
pre-defined TDM triple taxonomy or creating new
leaderboards; and (iii) ranking papers by their cor-
responding performance to construct leaderboards.
In the second step, we design three experimen-
tal settings that mimic diverse real-world scenarios
where we need to update existing leaderboards or
create new ones from scratch. Specifically, the
TDM triples are either fully specified, partially
specified, or completely unknown during the con-
struction process. Notably, the task becomes in-
creasingly challenging in the second and third set-
tings, which have been neglected by all prior work,
as it demands reasoning across all existing leader-
boards and generating novel ones.
Following previous work, we evaluate TDM ex-
traction using exact match metrics. We furtherpropose various metrics to evaluate the constructed
leaderboards, including coverage of correctly as-
signed publications to the leaderboards and their
result values. We also compare the rankings of the
gold and constructed leaderboards.
In summary, this work presents SCILEAD, a
manually curated dataset of scientific leaderboards
with comprehensive annotations. We propose an
LLM-based framework for constructing scientific
leaderboards in realistic scenarios. Our experi-
ments demonstrate the competitiveness of our ap-
proach in a cold start setting, where we evaluate its
performance on four state-of-the-art LLMs. Fur-
thermore, we show that our method can effectively
reconstruct scientific leaderboards in real-world
scenarios, highlighting its practical applicability.
2 Related Work
Scientific Leaderboard Construction. Table 1
summarizes the differences between previous work
and our study. In terms of data source, previous
studies use either NLP-progress orpaperswithcode .
These sources, however, lack rigorous quality as-
surance, such as standardizing TDM entities across
different leaderboards and ensuring complete cov-
erage of relevant publications. Similar to our work,
Hou et al. (2019) and Kardas et al. (2020) extract
TDM triples along with the results values and apply
normalization for leaderboard construction. How-
ever, both studies assume a closed domain and
match extracted TDM triples to a pre-defined TDM
triple taxonomy. On the other hand, some stud-
ies only partially extract TDMR tuples and do not
apply normalization. For example, Kabongo et al.
(2023b) and Yang et al. (2022) extract TDM triples
without results. Therefore, these works do not deal
with leaderboard construction. In addition, Singh
et al. (2024) extract the results values depending
on the pre-defined TDM triples. Both Kabongo
et al. (2023b) and Singh et al. (2024) leverage pre-
defined TDM triples in an extraction process simi-
lar to Hou et al. (2019). Since these approaches re-
quire a pre-defined taxonomy of TDM triples, they
are incompatible with a realistic task definition. In
short, none of the previous work is adaptable to the
constantly emerging benchmarks driven by new re-
search and innovation. In this work, we address the
aforementioned problems. Unlike previous work,
we (1) manually construct our dataset directly from
publications to ensure complete TDMR annota-
tions, (2) apply normalization for leaderboard con-
2Related WorkData
SourceNormTask Form
Extr Constr
(Hou et al., 2019) NProg. ‚úì ‚úì ‚úì
(Kardas et al., 2020) PwC ‚úì ‚úì ‚úì
(Yang et al., 2022) PwC - ‚àº -
(Kabongo et al., 2023b) PwC - ‚àº -
(Singh et al., 2024) PwC - ‚àº ‚úì
SCILEAD (Ours) Pub. ‚úì ‚úì ‚úì
Table 1: Comparison of related work and ours. Data
Source : Source of leaderboards: NProg.: NLP-progress ,
PwC: paperswithcode , Pub.: Publications. Norm refers
to Normalization. Task Form : Task formulation, includ-
ing Extr.: TDMR extraction and Constr.: Leaderboard
Construction. ‚àºindicates partial fulfillment.
struction, and (3) propose different experimental
settings to simulate real-world scenarios.
Scientific Knowledge Graph Construction.
Part of the scientific leaderboards can be viewed as
a special type of scientific knowledge graph that in-
cludes three types of entities (Task, Dataset, Metric)
and the relations between them, which have been
the primary focus of the previous studies on infor-
mation extraction from scientific literature (Luan
et al., 2018; Jain et al., 2020; Hou et al., 2021;
Mondal et al., 2021; Pramanick et al., 2023). Our
work in the cold start scenario, in which we do not
assume any pre-defined TDM triple is given, con-
structs such a scientific knowledge graph and links
the papers to the nodes in the graph simultaneously.
3 Our S CILEAD Dataset
To facilitate a thorough evaluation of leaderboard
construction, we require a dataset that satisfies
the following criteria. (1) TDM coverage : the
dataset should annotate a complete set of all task,
dataset, metric (TDM) triples from individual publi-
cations, thereby mitigating the problems of incom-
plete and inaccurate information inherent in previ-
ous datasets. (2) TDM disambiguation : mentions
of the same TDM entities should be normalized
to allow comparison of methods or research in the
same leaderboards. (3) State-of-the-Art : for each
leaderboard, the associated publications should be
ranked based on the best-reported results obtained
by their proposed methods (excluding baselines, ab-
lation experiments, or reproduction of other meth-
ods), which would restrict each publication to have
a single entry per leaderboard. This enables evalua-
tion using ranking similarity metrics. (4) Impact :
the dataset should strike a balance between having
too many or too few leaderboards, where the formerDataset item Count
Papers 43
Unique tasks 23
Unique datasets 71
Unique metrics 26
Unique TDMs 138
Leaderboards 27
Avg. papers / leaderboard 5.19
Table 2: Statistics of our S CILEAD dataset
may result in very few papers per each leaderboard,
and the latter only constructs popular ones.
To fulfill these requirements, we created
SCILEAD, a new manually-curated Scientific
Lead erboard dataset. We first selected leader-
boards that have a large number of publications
from NLP-progress. We downloaded PDFs of the
relevant publications from arXiv or correspond-
ing venues. We then manually extracted a com-
plete set of TDMR tuples from these publications
(Coverage ), in contrast to previous datasets. Next,
we manually normalize mentions of TDM triples
which refer to the same entities, building a com-
plete TDM taxonomy for this dataset ( Disambigua-
tion). For an individual unique TDM triple from
a paper, the best-reported performance was kept
for the next step ( State-of-the-Art ). Lastly, we ag-
gregate and rank the TDMR tuples from different
papers under the corresponding leaderboards de-
fined by TDM triples. After constructing all leader-
boards, we filter out leaderboards of less than three
entries ( Impact ). We present the dataset statistics
in Table 2 and a few data instances in Appendix B.
4 Framework
Our proposed framework is illustrated in Figure 2.
The main goal is to automatically construct leader-
boards given a collection of scientific publications.
The framework first receives as input a set of pa-
pers in PDF form and extracts a complete set of
TDMR tuples from these papers (¬ß4.1). This first
stage is realized by prompting an LLM augmented
with a dense retriever. The extracted tuples are then
passed into a normalization module in the second
stage, which decides whether the TDM triples be-
long to existing leaderboards or comprise new ones
(¬ß4.2). In particular, the normalization module
maps these tuples to a pre-defined TDM taxonomy
or dynamically updates the taxonomy to integrate
new TDM entities. Finally, the corresponding best
performances of these normalized TDM triples is
being used to rank the competitive methods for
3Paper Chunks 
Tables 
 
 
 
 
 
 Query 
Based Text 
Extraction 
Table 
Extraction System 
Prompt 
Paper A 
{task: Text Summarization, 
 dataset: NYT, 
 metric: ROUGE-1, 
 result: 57.75}, 
{task: POS Tagging, 
 dataset: PTB, 
 metric: F1 Score, 
 result: 93.76} Paper A Normalization 
Prompt 
Paper A 
{task: Summarization ,
 dataset: New York Times (NYT) ,
 metric: ROUGE-1, 
 result: 57.75}, 
{task: Part-of-Speech (POS) Tagging ,
 dataset: Penn Treebank (PTB) ,
 metric: F1,
 result: 93.76} Repeating for all 
papers in corpus 
Paper Result 
C 60.13 
A 57.75 
D 55.67 Task: Summarization, 
Dataset: New York Times (NYT) 
Metric: ROUGE-1 
Leaderboard: TDMR Extraction Normalization Leaderboard 
Construction Figure 2: Our framework in three steps: (1) TDMR Extraction, (2) Normalization, (3) Leaderboard Construction
leaderboard construction (¬ß4.3).
4.1 TDMR Extraction
The goal of TDMR extraction is to obtain a com-
plete set of TDMR tuples from a given set of sci-
entific papers. To address this goal, we implement
retrieval-augmented generation (RAG), combining
an LLM and a knowledge-based retrieval system.
The process involves parsing PDFs and building a
vector database. Initially, each publication is parsed
using an off-the-shelf PDF processing tool to obtain
text and tables within the file. The extracted text is
then split into smaller chunks of 2,000 characters,
approximately equivalent to 500 tokens. This split-
ting is crucial to process long documents efficiently.
Next, the text chunks and tables are transformed
into contextualized embeddings using a pre-trained
embedding model. These embeddings are stored
systematically in a vector database, which serves as
a knowledge base for quick and efficient retrieval
in RAG.
After creating the vector database, we imple-
ment an embedding similarity retriever that selects
candidate chunks potentially containing the desired
TDMR tuples. We use the following query for re-
trieval: ‚Äú Main task, datasets and evaluation
metrics ‚Äù. Lastly, we instruct LLMs to extract
TDMR tuples from the retrieved chunks by con-
catenating all retrieved chunks and tables and feed-
ing the combined input into the model. We instruct
LLMs to extract TDMR tuples solely from the top-
performing results achieved by the proposed meth-
ods in the paper, excluding baseline results. The
prompts for TDMR extraction and implementation
details are provided in Appendix G and in ¬ß5.1.4.2 Normalization
Since leaderboards are defined by TDM triplets, pa-
pers to be ranked together on the same leaderboard
must operate on the same TDMs. However, as men-
tioned in Section 3, scientific papers often use dif-
ferent terminology to refer to the same entity (e.g.,
Named Entity Recognition andName Tagging ). To
ensure the comparability of results from different
papers, it is essential to normalize TDM triples.
One approach to achieve this is to map entity names
extracted from TDM triples to pre-defined, stan-
dardized entity names, thereby enabling the com-
parison of results across papers. While related work
assumes the existence of a pre-defined TDM tax-
onomy, this assumption does not align with actual
research practices for two reasons: (1) leaderboard
information may not always be available, and (2)
it is not possible to map newly introduced task,
dataset or metric names to pre-defined sets. There-
fore, we apply the normalization process in two
more realistic settings simulating scenarios where
TDM triples of leaderboards are partially defined
or not defined at all beforehand. We name those
settings as partially pre-defined TDM triple nor-
malization andcold start normalization . We test
our framework from three normalization perspec-
tives to measure its robustness against real-world
use cases with varying difficulty levels.
Fully Pre-defined TDM Triples. In this setting,
we normalize the extracted TDM triples to pre-
defined task, dataset, and metric entities. For each
entity type t‚àà {Task, Dataset, Metric }, given the
extracted entity mention lt, we use LLMs to asso-
ciate the extracted name with one of the pre-defined
entity names in set Stwhich is constructed from
our dataset. For simplicity, Stincludes all pre-
4defined entity names in this setting. We discard
a TDM triple if any entities cannot be mapped to
an existing entity list. Implementation details and
utilized prompts are given in Appendix G.
Algorithm 1 Partially pre-defined TDM / cold start
1:for each t‚àà {task,dataset ,metric}do
2: S‚Ä≤
t‚ÜêPre-defined taxonomy of type t
3: Lt‚ÜêLLM extraction outputs of type t
4: for each lt‚ààLtdo
5: lt‚ÜêLLMNorm (lt, S‚Ä≤
t)
6: ifltnot in S‚Ä≤
tthen
7: S‚Ä≤
t‚ÜêS‚Ä≤
t+{lt}
Partially Pre-defined TDM Triples. Unlike the
previous setting, where the TDM taxonomy is fully
pre-defined, here we assume that the TDM taxon-
omy is partially defined. To simulate this setting,
we mask a subset of entities in the fully pre-defined
TDM triples. The masked subset represents unseen
leaderboards, mimicking the continuous updating
and evolution in research. As shown in Algorithm
1, we first initialize our new entity name set S‚Ä≤
tby
removing the masked entity names Mtfrom the
initial set St. LLMs first decide whether the given
extracted entity name ltmatches with any element
inS‚Ä≤
t. If the model cannot find a match, our algo-
rithm considers it a newly introduced entity name
and adds it to S‚Ä≤
t. This process repeats until all
extracted entity names are processed. In this frame-
work, we want to simulate the real-world scenario
where newly introduced tasks and datasets would
create new leaderboards while the old ones will be
mapped to existing leaderboards.
For example, assume that one of the masked
leaderboard task names is ‚Äú Named Entity Recogni-
tion (NER) ‚Äù. If the LLM extraction result is NER
and no other task names represent this task in S‚Ä≤
t
at this point, NER will be added to S‚Ä≤
t, and other
LLM extracted task names related to Named Entity
Recognition will be normalized to this entry in the
following process. Since NER is not directly com-
parable with the pre-defined leaderboard names,
we require an additional normalization step at the
TDM triple level to enable direct comparison be-
tween newly created leaderboards and gold leader-
boards during evaluation, as the new leaderboards
may not always utilize the pre-defined TDM triple
names. Please refer to Appendix G for more details
about the second normalization step.
Cold Start. In the cold start setting, we do not
use any pre-defined entity names to simulate thescenario where no leaderboard exists. We mask all
pre-defined entity names in Stand start the process
with an empty set S‚Ä≤
t(i.e.,S‚Ä≤
t=St‚àíMt=‚àÖ).
As in the partially pre-defined TDM triples setting,
S‚Ä≤
tis a dynamic set. We gradually update S‚Ä≤
tby
adding new items and processing the entity names
individually. For instance, LLM encounters a new
entity, ‚Äú Summary Generation ‚Äù, and adds it to the
empty set S‚Ä≤
t. When LLM encounters ‚Äú Document
Summarization ‚Äù in the subsequent steps, it asso-
ciates this new entity with ‚Äú Summary Generation ‚Äù
and does not update S‚Ä≤
tand maps ‚Äú Document Sum-
marization ‚Äù to ‚Äú Summary Generation ‚Äù. Similar to
the previous setting, after extracting all TDMR tu-
ples from individual papers and normalizing each
element with the newly defined TDM entities, we
carry out the second normalization step at the TDM
triple level.
4.3 Leaderboard Construction
In scientific papers, a diverse array of metrics are
employed for evaluation, ranging from commonly
used measures (e.g., accuracy) to task-specific met-
rics (e.g., perplexity). While many of these metrics
are optimally scaled in [0, 1], others, like root mean
squared error, may have arbitrary scales. However,
the reported values of these metrics can vary across
papers, making it challenging to develop a univer-
sal normalization method that can scale all result
values to a common range for comparison.
To mitigate this complexity, we have devel-
oped a post-processing procedure specifically for
percentage-like metrics, which are scaled between
0 and 1, such as F1 score and accuracy, in our cur-
rent dataset. This procedure begins by identifying
whether a predicted metric is percentage-like us-
ing a pre-defined list of such metrics. Then, we
perform post-processing on the corresponding ex-
tracted values. The post-processing involves two
steps: first, we clean the extracted results by re-
moving special characters (e.g., % or ¬±), as well
as mean and standard deviation values if presented.
Next, we standardize the reporting style by convert-
ing all values to percentages, thereby eliminating
inconsistencies that may arise from presenting val-
ues as floating points in [0, 1] or as percentages.
For metrics with different ranges (e.g., perplexity),
we retain the extracted values in their original form.
After this process, we gather the same normal-
ized TDM triples and their corresponding results.
We automatically sort result values to determine
the rankings of the papers in the same leaderboard.
5Note that a single paper can belong to multiple
leaderboards when working on several task and
dataset configurations. As mentioned in Section 3,
we focus on popular and essential TDMs to keep a
practical set of leaderboards. As a result, we only
keep leaderboards with at least three TDMR tuples.
5 Experiments
5.1 Experimental Settings
We used the LangChain6framework to prompt
LLMs for TDMR extraction and normalization. We
implemented various open and closed source large
language models, namely Llama-2 (Touvron et al.,
2023), Llama-3 (MetaAI, 2024), Mixtral (Jiang
et al., 2024), and GPT-4 Turbo (OpenAI, 2023).
To ensure a fair evaluation, identical prompts were
employed across all models. All prompts are pre-
sented in Appendix G. To ensure the robustness of
our experimental results in the cold start setting, we
run the inference with three different random pa-
per orders and report the average evaluation scores
across these runs, thereby mitigating the impact
of any particular ordering on the results. We used
AxCell (Kardas et al., 2020) as a baseline since it is
the closest approach to ours. To better understand
our normalization strategy, we further employed a
baseline for the normalization step, which normal-
izes the LLM extracted entities to the pre-defined
set by calculating cosine similarity ( CS). Note that
this baseline is only used in the fully pre-defined
TDM triples, as it requires pre-defined entity names
to calculate similarity and is not capable of rec-
ognizing newly-emerged entities. The model and
experimental details can be viewed in Appendix C.
5.2 Evaluation Settings
Exact Tuple Match (ETM). We first investigate
whether the models can extract correct tuples of
task, dataset, metric, and result (TDMR) values
from publications. We compute recall and preci-
sion scores of tuple extraction. The recall metric
measures the ratio of gold tuples that are correctly
extracted, whereas precision measures how many
of the extracted tuples are actually correct.
Individual Item Match (IIM). Although the ex-
tracted tuples may contain partially correct informa-
tion, the ETM metric does not recognize or reward
such outputs. For example, a TDM triple may be ac-
curate, but Result is incorrect. Therefore, we assess
6https://github.com/langchain-ai/langchainthe extraction of individual components of TDMR
tuples to gain a more nuanced understanding of
the model‚Äôs performance. We employ precision
and recall to measure to what extent a model can
extract individual items. This allows us to identify
local errors in the framework, suggesting potential
directions for future work.
Leaderboard Evaluation. We employ different
metrics for leaderboard evaluation. We first mea-
surerecall of leaderboard construction, i.e., the per-
centage of gold leaderboards that a model correctly
identifies. We then evaluate the paper and result
coverage of the generated leaderboards. For each
gold leaderboard, we compute the ratio of correctly
assigned papers and results over the gold ones.
Then, we calculate the macro-average of these re-
sults as paper coverage (PC) andresult coverage
(RC) . Lastly, we compare the paper rankings in the
gold and model-generated leaderboards using Aver-
age Overlap (AO)7(Webber et al., 2010). AO can
measure the similarity of two ranked leaderboards
even when one is incomplete or over-inclusive,
such as missing some papers or including wrong
papers. This cannot be done if we simply compare
the extracted result values. AO is calculated by
comparing the overlap between the top- dgold and
predicted papers of each leaderboard, denoted as
GdandMdrespectively:
Ad=|Gd‚à©Md|
d;AO =1
kkX
d=1Ad (1)
where Adis agreement between two ranked lists,
withkbeing the length of the shorter leaderboard.
6 Results and Analysis
In this section, we first inspect the main TDMR
extraction and leaderboard construction results
obtained under diverse experimental settings on
SCILEAD. In addition, we augment our analysis by
conducting human evaluation and error analysis on
a new dataset containing recent NLP papers and pa-
pers from the medical domain, showcasing the prac-
tical utility of our proposed system in real-world
scenarios where TDM triples are not pre-defined.
6.1 Main Results on S CILEAD
ETM Evaluation. Table 3 shows the ETM scores
of different models under two settings: fully pre-
defined TDM triples andpartially pre-defined TDM
7https://github.com/changyaochen/rbo
6ModelETM
R P F1FullyAxCell 13.67 32.59 19.26
Llama-2 + CS 21.59 10.06 13.73
Llama-2 15.25 9.63 11.81
Mixtral + CS 24.61 26.54 25.54
Mixtral 21.73 24.66 23.10
Llama-3 + CS 29.54 23.22 26.00
Llama-3 35.60 27.11 30.78
GPT-4 + CS 48.71 49.82 49.26
GPT-4 54.53 56.02 55.27PartiallyLlama-2 9.89 4.17 5.87
Mixtral 12.27 14.65 13.35
Llama-3 18.75 15.70 17.09
GPT-4 39.56 40.60 40.07
Table 3: Exact tuple match ( ETM ) evaluation scores
for different normalization settings (%). R:Recall, P:
Precision, F1:F1 score. LLM + CS indicates the cosine
similarity baseline for normalizing individual entities.
The best results for each normalization setting are un-
derlined. The overall highest results are bolded.
triples . Note that we do not provide results for the
cold start setting, as it does not use pre-defined
gold TDM labels and we only normalize gener-
ated TDM triples to the gold leaderboards with at
least three entries in the second normalization step.
We observe that all LLMs, except Llama-2, outper-
form the AxCell baseline regarding the F1 score
in the fully pre-defined TDM triples setting. The
low precision score of Llama-2 is due to the gener-
ation of an excessive number of TDMR tuples. In
contrast, AxCell has high precision and low recall
compared with Llama-2 due to the small number
of extracted tuples. We observe that normaliza-
tion using LLama-2 and Mixtral falls short of the
+CS baseline, whereas Llama-3 and GPT-4 Turbo
demonstrate superior performance, outperforming
the baseline. As expected, the performance of all
models deteriorates in the more realistic, partially
pre-defined TDM triples setting, which mirrors real-
world application scenarios. Nevertheless, GPT-4
Turbo outperforms its counterparts in both settings.
IIM Evaluation. In Table 4, we demonstrate the
extraction and normalization performance of dif-
ferent models on individual elements of TDMR
tuples under settings with both fully andpartially
pre-defined TDM triples . We observe that indi-
vidual extraction scores are much higher than the
exact tuple match scores compared to Table 3. It
can be concluded that the models are, in fact, effec-
tive in the extraction of individual items, but they
are struggling to combine those items to create
accurate TDMR tuples. Furthermore, all models
consistently fall short of extracting correct Resultscompared to other items. This is another signifi-
cant factor for low ETM scores reported in Table 3
(further analysis in Appendix D). Another interest-
ing observation is that Llama-3 is very competitive
with GPT-4 Turbo in extracting Task, Dataset, Met-
ricitems, but its performance considerably drops
for extracting the Result values.
Leaderboard Evaluation. Table 5 shows the
leaderboard construction performance under three
settings described in Section 4.2. Although Llama-
3 and GPT-4 Turbo are competitive in leaderboard
recall ratio, Llama-3 has the best results for the
paper coverage across all settings. However, since
it struggles with extracting the correct results, as
shown in Table 4, GPT-4 Turbo takes the lead in
result coverage and average scores. Furthermore,
we observe that performance drops for most mod-
els when the experimental setting becomes more
complex and realistic. Interestingly, we found that,
unlike other models, GPT-4 performs better in the
cold start setting compared to the partially pre-
defined TDM triples setting on paper and result
coverage, as well as average overlap. We provide
further analysis of this behavior in the next section.
6.2 Partial Pre-defined Leaderboard Analysis
Table 6 shows the comparison results of different
models on the partially pre-defined TDM triples
andcold start settings on the masked leaderboard
subset in the partially pre-defined TDM triples set-
ting (based on Mtin Algorithm 1). Overall, Llama-
2 and Mixtral perform better in the partially pre-
defined TDM triples setting, while Llama-3 and
GPT-4 Turbo perform better in a cold start setting.
When we look at the different results for these
two settings, we notice that in the partially pre-
defined TDM triples setting, sometimes the model
makes errors in the normalization step by mapping
a newly extracted TDM triple to an existing pre-
defined TDM triple that has the similar surface
form but refers to a different leaderboard. For in-
stance, the model maps CoNLL-2003 - English to
CoNLL-2003 - German . Such errors occur less
frequently in the cold start setting.
6.3 Additional Analysis
Leaderboard Threshold. The minimum number
of papers required for a leaderboard directly im-
pacts the performance of leaderboard construction,
as it influences the total number of leaderboards. In
our main experiments, we focused on leaderboards
7ModelIIM-Task IIM-Dataset IIM-Metric IIM-Result
R P F1 R P F1 R P F1 R P F1FullyAxCell 58.52 68.98 63.32 33.87 63.66 44.22 51.36 69.35 59.01 18.41 45.32 26.18
Llama-2 + CS 67.20 59.83 63.30 58.81 68.93 63.47 61.41 67.36 64.2531.61 23.75 27.12Llama-2 60.74 55.45 57.97 55.03 62.60 58.57 65.49 71.51 68.37
Mixtral + CS 91.99 86.27 89.04 73.20 85.03 78.67 71.78 76.56 74.0941.75 44.62 43.13Mixtral 89.74 86.85 88.27 71.26 81.68 76.12 67.20 76.72 71.65
Llama-3 + CS 90.85 85.69 88.19 78.62 82.43 80.48 81.41 87.02 84.1249.56 39.50 43.96Llama-3 92.17 87.33 89.68 87.75 92.09 89.87 89.48 94.90 92.11
GPT-4 + CS 90.77 90.70 90.73 79.93 86.36 83.02 81.49 86.36 83.8568.22 70.34 69.26GPT-4 91.10 90.62 90.86 86.05 92.64 89.22 86.46 88.18 87.31PartiallyLlama-2 42.98 39.70 41.27 33.14 41.05 36.67 59.34 61.24 60.28 31.61 23.75 27.12
Mixtral 60.72 50.23 54.98 44.45 49.67 46.92 71.19 78.72 74.77 41.75 44.62 43.13
Llama-3 80.39 65.72 72.32 62.86 66.81 64.77 88.90 94.90 91.80 49.56 39.50 43.96
GPT-4 78.30 63.82 70.32 79.52 83.29 81.36 89.27 92.21 90.72 68.22 70.34 69.26
Table 4: Individual item match ( IIM) scores (%). R:Recall, P:Precision, F1:F1 score. The best results for each
setting are underlined. Overall highest results are given in bold. Since normalization is not applied to Results , its
scores are the same across both settings. The evaluation of the cold start setting is not applicable, as no pre-defined
labels are used and only gold leaderboards with a minimum of three entries are used for normalization.
Model LR PC RC AOFullyAxCell 40.00 25.78 15.45 11.93
Llama-2 + CS 70.37 35.23 8.88 4.60
Llama-2 70.37 34.96 8.26 4.96
Mixtral + CS 96.30 53.60 20.58 13.83
Mixtral 88.89 46.85 16.32 11.96
Llama-3 + CS 81.48 60.15 22.65 21.45
Llama-3 96.30 79.18 29.30 25.49
GPT-4 + CS 92.59 63.70 45.59 38.66
GPT-4 100.00 70.37 51.79 53.87PartiallyLlama-2 74.07 30.75 4.02 1.18
Mixtral 77.78 28.70 12.01 12.47
Llama-3 92.59 61.90 19.01 19.52
GPT-4 88.89 55.92 40.06 43.71Cold StartLlama-2 16.05 5.66 0.49 0.05
Mixtral 49.38 20.49 8.10 3.03
Llama-3 79.01 58.78 17.18 17.63
GPT-4 81.48 58.74 46.13 48.15
Table 5: Gold leaderboard evaluation (%). LR: Leader-
board recall, PC: Paper coverage, RC: Result coverage,
AO: Average Overlap. The best results for each setting
are underlined. Overall best results are given in bold.
Standard dev. for cold start are given in Appendix F.
with a minimum of three papers, but we also ex-
plored the effects of reducing this threshold to two
papers. Notably, this change significantly increased
the number of leaderboards from 27 to 62. Table 15
demonstrates the leaderboard construction results,
and all other evaluation results can be seen in Ap-
pendix E. In general, the overall performance drops
compared to the main results in Table 5. This is ex-
pected due to the increased number of leaderboards.
Notably, most of our key findings from the main ex-
periments remain consistent. However, one notable
exception is the performance of GPT-4, which ex-
cels in the partially pre-defined setting compared to
the cold start setting, contrary to our main results.
We attribute this discrepancy to the fact that the
cold start setting is more severely disadvantagedModel LR PC RC AOPartiallyLlama-2 60.00 17.76 6.71 2.08
Mixtral 60.00 20.26 13.26 20.28
Llama-3 90.00 57.38 11.83 7.01
GPT-4 70.00 50.57 33.50 26.03Cold StartLlama-2 10.00 3.10 0.67 0.00
Mixtral 53.33 22.67 10.97 7.91
Llama-3 86.67 78.14 22.59 14.79
GPT-4 83.33 60.76 43.11 37.50
Table 6: Leaderboard evaluation only on masked leader-
boards from partially masking setting (%). Metrics are
the same as Table 5. Standard dev. for cold start are
given in Appendix F.
in the second normalization step, which becomes
increasingly challenging as the number of leader-
boards grows. In other words, the cold start setting
struggles to effectively distinguish between simi-
lar TDM pairs in the second normalization step,
leading to this observed performance difference.
Zero-shot vs Few-shot. While zero-shot prompt-
ing LLMs achieves impressive performance in
leaderboard construction, we additionally explore
whether including few-shot examples in the prompt
can further enhance the results. To this end, we
conducted experiments using the best model (GPT-
4 Turbo) in the fully pre-defined setting. We se-
lected a paper not included in SCILEAD but work-
ing on similar tasks as an example. We provided
text chunks, including TDM information, along
with the results tables and the expected results for
TDMR extraction. For normalization, we used
the same pre-defined entity set. The remaining
experimental details were identical to the main ex-
periments. Unlike previous work, where few-shot
prompting often yields performance gains, our re-
8Evaluation R ‚àÜR P ‚àÜP F1 ‚àÜF1
ETM 51.03 -3.50 51.01 -5.01 51.02 -4.25
IIM - Task 92.66 1.56 92.11 1.49 92.39 1.53
IIM - Dataset 85.55 -0.5 89.06 -3.58 87.27 -1.95
IIM - Metric 93.39 6.93 95.74 7.56 94.55 7.24
IIM - Result 67.78 -0.44 69.21 -1.13 68.48 -0.78
Table 7: GPT-4 few-shot ETM and IIM results for fully
pre-defined setting and delta values for zero-shot.
Domain Task Dataset Metric Result CP
NLP 0.76 0.88 0.88 0.44 0.59
Med. 1.00 1.00 1.00 0.56 1.00
All 0.84 0.92 0.92 0.48 0.72
Table 8: Ratio of correct individual TDMR items in
manual evaluated leaderboards in both domains. CP:
Correctly assigned papers to leaderboards.
sults in Table 7 show that few-shot prompting leads
to a slight performance drop compared to zero-shot
prompting for exact tuple match (ETM). In con-
trast, for individual item match (IIM), the few-shot
approach outperforms zero-shot prompting for task
and metric extraction but underperforms for dataset
and result extraction. Future research can explore
better strategies for selecting few-shot examples to
incorporate into the prompt (Wu et al., 2023).
6.4 Results on Wild Datasets
To get further insights into the performance of our
framework in a real-world environment, we applied
our framework to recent papers from two different
domains (NLP and medical) and conducted manual
analysis. We first collected 339 main conference
and findings papers from the recent EACL 2024
and sampled 12 papers from the medical domain
from paperswithcode . We sampled papers from
the most popular leaderboards in the medical do-
main and manually checked whether their TDMR
tuples matched the leaderboards. We performed the
cold start setting with the best model (GPT-4) and
gathered the same TDM triples to construct leader-
boards from these papers. We sampled the most
crowded 24 leaderboards and manually evaluated
the correctness of TDMR tuples and leaderboards.
Table 8 demonstrates that the task, dataset, and met-
ric information is extracted and normalized with
a high success rate. In contrast, there is signifi-
cant room for improvement in the result extraction.
Nevertheless, our framework can extract and nor-
malize TDM triples, leading to high accuracy in
assigning papers to the leaderboards across two do-
mains. This emphasizes the generalizability of our
framework for real-world scenarios.6.5 Error Analysis
We performed a thorough error analysis from multi-
ple angles. Given that extracting Results is the most
challenging step for models, we identified the five
papers with the lowest GPT-4 IIM-Results scores.
Our analysis of 58 erroneous Results instances re-
vealed three primary error categories. Firstly, 19
errors (33%) occurred due to confusion with values
in other tables, such as failing to extract the best
results or mistakenly swapping dev and test sets.
Secondly, 18 errors (31%) arose from confusion
with the appendix material. Finally, the remaining
21 errors (36%) involved missed extractions, often
accompanied by errors in other TDM components.
In addition, we also checked three leaderboards
that have the lowest paper coverage scores and are
constructed in the partially pre-defined setting. We
observe that LLMs have difficulties normalizing
task names that do not have similar surface forms
to gold task names. For instance, ‚Äú Named Entity
Recognition ‚Äù is incorrectly mapped to ‚Äú Syntactic
Information Extraction ‚Äù. We also observe errors
in joint task names like Intent Detection and Slot
Filling . Models may extract task names separately
and not normalize them to the joint form.
Furthermore, we observe that some errors can be
attributed to the absence of explicit task or dataset
names in the paper. For example, some papers in-
troduce the tasks as GLUE Benchmark instead of
referring to the original task names such as Sen-
timent Analysis . This prevents the TDMR tuples
from being associated with the correct leaderboards.
Lastly, some errors occur due to the failure to re-
trieve related text and tables from the papers.
7 Conclusion
We introduce SCILEAD, a new dataset addressing
the limitations of existing leaderboard construction
datasets by providing accurate and complete Task-
Dataset-Metric-Result information from scientific
papers. Furthermore, we develop an LLM-based
framework to facilitate the automatic leaderboard
construction process in realistic application scenar-
ios. Our results show that LLMs excel in extract-
ing task, dataset, and metric names and detecting
correct leaderboards but struggle with extracting
result values. Our work contributes to the auto-
matic leaderboard construction problem, providing
valuable insights for large-scale applications.
98 Limitations
Due to the time-consuming nature of manual ex-
traction, the SCILEAD dataset is currently limited
to a set of 43 papers. However, we believe that
SCILEAD outweighs other large datasets in terms
of content because it accurately captures all the
unique TDMR tuples in the papers and provides a
normalization between TDMRs from different pa-
pers. We leave large-scale experiments for follow-
up research.
We acknowledge that our dataset is predomi-
nantly comprised of papers from the machine learn-
ing domain, which offers a rich source of compara-
ble performance scores on specific topics, making
it an attractive focus for leaderboard construction
studies. Additionally, given that the majority of ar-
ticles in the literature are published in English, our
dataset accordingly consists of English-language ar-
ticles. However, we recognize the importance of ex-
panding our framework to encompass papers from
diverse scientific domains and languages, which
presents a promising direction for future research.
Ethics Statement
The primary purpose of automatically generating
leaderboards is to provide researchers with a con-
cise and comprehensive overview of the scientific
literature landscape, facilitating the comparison of
previous and current state-of-the-art approaches to
a specific task. Notably, all models and data in-
stances utilized in our work, with the exception of
GPT-4 Turbo, are built upon open-source founda-
tions, thereby promoting responsible, reproducible,
and transparent scientific research practices. Fur-
thermore, our study does not rely on crowd-sourced
human annotators.
Acknowledgments
This work was funded by the "Modeling Task-
oriented Dialogues Grounded in Scientific Liter-
ature" project in partnership with Amazon Alexa.
We gratefully acknowledge the support of Mi-
crosoft with a grant for access to OpenAI GPT
models via the Azure cloud (Accelerate Founda-
tion Model Academic Research). Yufang Hou is
supported by the Visiting Female Professor Pro-
gramme from the Technical University of Darm-
stadt.References
Lutz Bornmann, Robin Haunschild, and R√ºdiger Mutz.
2021. Growth rates of modern science: A latent
piecewise growth curve approach to model publi-
cation numbers from established and new literature
databases. Humanities and Social Sciences Commu-
nications , 8(1):224.
Yufang Hou, Charles Jochim, Martin Gleize, Francesca
Bonin, and Debasis Ganguly. 2019. Identification
of tasks, datasets, evaluation metrics, and numeric
scores for scientific leaderboards construction. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5203‚Äì
5213, Florence, Italy. Association for Computational
Linguistics.
Yufang Hou, Charles Jochim, Martin Gleize, Francesca
Bonin, and Debasis Ganguly. 2021. TDMSci: A spe-
cialized corpus for scientific literature entity tagging
of tasks datasets and metrics. In Proceedings of the
16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume ,
pages 707‚Äì714, Online. Association for Computa-
tional Linguistics.
Sarthak Jain, Madeleine van Zuylen, Hannaneh Ha-
jishirzi, and Iz Beltagy. 2020. SciREX: A challenge
dataset for document-level information extraction. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 7506‚Äì
7516, Online. Association for Computational Lin-
guistics.
Albert Q Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris Bam-
ford, Devendra Singh Chaplot, Diego de las Casas,
Emma Bou Hanna, Florian Bressand, et al. 2024.
Mixtral of experts. arXiv preprint arXiv:2401.04088 .
Salomon Kabongo, Jennifer D‚ÄôSouza, and S√∂ren Auer.
2021. Automated mining of leaderboards for empiri-
cal AI research. In Towards Open and Trustworthy
Digital Societies , pages 453‚Äì470, Cham. Springer
International Publishing.
Salomon Kabongo, Jennifer D‚ÄôSouza, and S√∂ren Auer.
2023a. Zero-shot entailment of leaderboards for em-
pirical ai research. In 2023 ACM/IEEE Joint Confer-
ence on Digital Libraries (JCDL) , pages 237‚Äì241.
Salomon Kabongo, Jennifer D‚ÄôSouza, and S√∂ren Auer.
2023b. ORKG-Leaderboards: a systematic workflow
for mining leaderboards as a knowledge graph. Inter-
national Journal on Digital Libraries , 25(1):41‚Äì54.
Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebas-
tian Ruder, Sebastian Riedel, Ross Taylor, and Robert
Stojnic. 2020. AxCell: Automatic extraction of re-
sults from machine learning papers. In Proceedings
of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages 8580‚Äì
8594, Online. Association for Computational Lin-
guistics.
10Esther Landhuis. 2016. Scientific literature: Informa-
tion overload. Nature , 535(7612):457‚Äì458.
Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh
Hajishirzi. 2018. Multi-task identification of entities,
relations, and coreference for scientific knowledge
graph construction. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing , pages 3219‚Äì3232, Brussels, Belgium.
Association for Computational Linguistics.
MetaAI. 2024. Introducing Meta Llama 3.
Saif M. Mohammad. 2020. NLP scholar: A dataset for
examining the state of NLP research. In Proceedings
of the Twelfth Language Resources and Evaluation
Conference , pages 868‚Äì877, Marseille, France. Euro-
pean Language Resources Association.
Ishani Mondal, Yufang Hou, and Charles Jochim. 2021.
End-to-end construction of NLP knowledge graph.
InFindings of the Association for Computational
Linguistics: ACL-IJCNLP 2021 , pages 1885‚Äì1895,
Online. Association for Computational Linguistics.
Niklas Muennighoff, Nouamane Tazi, Loic Magne, and
Nils Reimers. 2023. MTEB: Massive text embedding
benchmark. In Proceedings of the 17th Conference
of the European Chapter of the Association for Com-
putational Linguistics , pages 2014‚Äì2037, Dubrovnik,
Croatia. Association for Computational Linguistics.
OpenAI. 2023. GPT-4 technical report. arXiv preprint
arXiv:2303.08774 .
Aniket Pramanick, Yufang Hou, Saif Mohammad, and
Iryna Gurevych. 2023. A diachronic analysis of
paradigm shifts in NLP research: When, how, and
why? In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 2312‚Äì2326, Singapore. Association for Com-
putational Linguistics.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982‚Äì3992, Hong Kong, China. Association for Com-
putational Linguistics.
Mayank Singh, Rajdeep Sarkar, Atharva Vyas,
Pawan Goyal, Animesh Mukherjee, and Soumen
Chakrabarti. 2019. Automated early leaderboard gen-
eration from comparative tables. In Advances in In-
formation Retrieval , pages 244‚Äì257, Cham. Springer
International Publishing.
Shruti Singh, Shoaib Alam, and Mayank Singh. 2024.
LEGOBench: Leaderboard generation benchmark for
scientific models. arXiv preprint arXiv:2401.06233 .
Nandan Thakur, Nils Reimers, Johannes Daxenberger,
and Iryna Gurevych. 2021. Augmented SBERT: Data
augmentation method for improving bi-encoders forpairwise sentence scoring tasks. In Proceedings of
the 2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies , pages 296‚Äì310, Online.
Association for Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
William Webber, Alistair Moffat, and Justin Zobel. 2010.
A similarity measure for indefinite rankings. ACM
Transactions on Information Systems , 28(4).
Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-
peng Kong. 2023. Self-adaptive in-context learn-
ing: An information compression perspective for in-
context example selection and ordering. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 1423‚Äì1436, Toronto, Canada. Association for
Computational Linguistics.
Yingwei Xin, Ethan Hart, Vibhuti Mahajan, and Jean-
David Ruvini. 2018. Learning better internal struc-
ture of words for sequence labeling. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing , pages 2584‚Äì2593,
Brussels, Belgium. Association for Computational
Linguistics.
Sean Yang, Chris Tensmeyer, and Curtis Wigington.
2022. TELIN: Table entity LINker for extracting
leaderboards from machine learning publications. In
Proceedings of the first Workshop on Information
Extraction from Scientific Publications , pages 20‚Äì25,
Online. Association for Computational Linguistics.
11A Papers with Code Analysis
We take (Xin et al., 2018) as an example paper. Its
TDMR tuples are given in Table 9. However, its
paperswithcode entry8lacks of results for German,
Spanish and Dutch datasets. In addition, the dataset
for chunking task is confused with Penn Treebank.
Task Dataset Metric Result
Named Entity Recognition CoNLL-2003 - English F1 91.64
Named Entity Recognition CoNLL-2003 - German F1 79.43
Named Entity Recognition CoNLL-2002 - Spanish F1 86.68
Named Entity Recognition CoNLL-2002- Dutch F1 87.81
Text Chunking CoNLL-2000 F1 95.29
Part-of-Speech Tagging Penn Treebank (PTB) F1 97.58
Table 9: TDMR tuples from (Xin et al., 2018)
B Dataset
We present some representative data instances from
SCILEAD in Table 10 and Table 11 for TDMR
tuples and leaderboards respectively.
C Experimental Details
In the TDMR extraction step, Unstructured9
and Chroma10libraries were used for PDF
parsing and vector database implementation.
multi-qa-mpnet-base-dot-v1 (Reimers and
Gurevych, 2019) was used as the embedding model
for dense retrieval.
All experiments with Mixtral, Llama-2, and
Llama-3 used the instruction-tuned version and
were run in 4-bit quantization on a single NVIDIA
A100 GPU with 80GB memory, taking approxi-
mately ‚àº13 hours in total. We used the Llama-2
Chat 70B, Mixtral-8x7B-Instruct, Llama-3 Instruct
70B versions, and the API version 1106-preview
for GPT-4 Turbo. For reproducibility, we used
greedy decoding for all models. In prompting, the
only modifications made across the models were
to conform to each model‚Äôs specific format require-
ments, such as incorporating model-specific tokens.
For embedding similarity model for nor-
malization similarity baseline, we utilized
bilingual-embedding-large (Thakur et al.,
2021) because of its high performance in the
Massive Text Embedding Benchmark (MTEB)
(Muennighoff et al., 2023) for Semantic Textual
Similarity (STS).
8https://paperswithcode.com/paper/
learning-better-internal-structure-of-words
9https://github.com/Unstructured-IO/
unstructured
10https://github.com/chroma-core/chromaUnlike our framework, AxCell works on TeX
files instead of PDFs. Therefore, we manually
downloaded 36 TeX files from the 43 papers in
our dataset. This poses an input constraint to their
method. Performance measurements of AxCell for
TDMR and leaderboard evaluation were made ac-
cording to the gold labels of those 36 papers. To
normalize AxCell outputs, we applied the fully pre-
defined TDM triples setting using GPT-4 Turbo.
Since some metrics are common across many
tasks and datasets in leaderboards, only some items
in the task and dataset are masked in the partially
masking setting to avoid making the partially pre-
defined setting too close to the cold start.
Evaluation. We calculate exact tuple match
scores as follows:
ETMRecall=1
|P|X
p‚ààP|TG
p‚à©TM
p|
|TGp|(2)
ETMPrecision=1
|P|X
p‚ààP|TG
p‚à©TM
p|
|TMp|(3)
where prepresent a single paper in the set of all
papers Pin the dataset. TG
pandTM
pare gold and
model generated tuple set for the paper p.
For paper item type t, we calculate individual
item match scores as follows:
IIMRecall
t =1
|P|X
p‚ààP|IG
t,p‚à©IM
t,p|
|IG
t,p|, (4)
IIMPrecision
t =1
|P|X
p‚ààP|IG
t,p‚à©IM
t,p|
|IM
t,p|,(5)
where IG
t,pandIM
t,prepresent the sets of gold and
model generated unique type titems that belong to
the paper p.
D ETM without Result .
Since extracting the correct results from the papers
is the main bottleneck for exact match, we calculate
ETM score by excluding Result (i.e., on TDMs,
not TDMRs) in Table 12. We see that the scores
of all models show a remarkable increase. In this
setting, Llama-3 outperforms GPT-4 Turbo in the
fully pre-defined TDM triples setting, but GPT-4
Turbo is more robust in the partially pre-defined
TDM triples setting.
12Paper Name Task Dataset Metric Result
1703.06345.pdf Named Entity Recognition (NER) CoNLL-2003 - English F1 91.26
1703.06345.pdf Named Entity Recognition (NER) CoNLL-2002 - Spanish F1 85.77
1703.06345.pdf Named Entity Recognition (NER) CoNLL-2002- Dutch F1 85.19
1703.06345.pdf Text Chunking CoNLL-2000 F1 95.41
1703.06345.pdf Part-of-Speech (POS) Tagging Penn Treebank (PTB) Accuracy 97.55
1603.01354.pdf Named Entity Recognition (NER) CoNLL-2003 - English Precision 91.35
1603.01354.pdf Named Entity Recognition (NER) CoNLL-2003 - English Recall 91.06
1603.01354.pdf Named Entity Recognition (NER) CoNLL-2003 - English F1 91.21
1603.01354.pdf Part-of-Speech (POS) Tagging Penn Treebank (PTB) Accuracy 97.55
1709.04109.pdf Named Entity Recognition (NER) CoNLL-2003 - English F1 91.85
1709.04109.pdf Text Chunking CoNLL-2000 F1 96.13
1709.04109.pdf Part-of-Speech (POS) Tagging Penn Treebank (PTB) Accuracy 97.59
Table 10: Example TDMR instances from S CILEAD. TDMR tuples per each paper are color coded.
Task: Named Entity Recognition (NER)
ResultTask: Part-of-Speech (POS) Tagging
Result Dataset: CoNLL-2003 - English Dataset: Penn Treebank (PTB)
Metric: F1 Metric: Accuracy
1709.04109.pdf 91.85 1709.04109.pdf 97.59
1703.06345.pdf 91.26 1603.01354.pdf 97.55
1603.01354.pdf 91.21 1703.06345.pdf 97.55
Table 11: Example leaderboards from SCILEAD based on common TDMR tuples in Table 10. The same color codes
are used.
Model R (% change) P (% change) F1 (% change)FullyAxCell 25.78 (88.59) 55.08 (69.01) 35.12 (82.35)
Llama-2 + CS 39.54 (83.14) 40.66 (304.17) 40.09 (191.99)
Llama-2 34.91 (128.92) 35.85 (272.27) 35.37 (199.49)
Mixtral + CS 52.05 (111.50) 58.44 (120.20) 55.06 (115.58)
Mixtral 48.75 (124.34) 55.66 (125.71) 51.97 (124.98)
Llama-3 + CS 58.07 (96.58) 62.02 (167.10) 59.98 (130.69)
Llama-3 72.56 (103.82) 77.13 (184.51) 74.77 (142.92)
GPT-4 + CS 63.82 (31.02) 68.95 (38.40) 66.29 (34.57)
GPT-4 70.40 (29.10) 75.28 (34.38) 72.75 (31.63)PartiallyLlama-2 22.99 (132.45) 27.23 (553.00) 24.93 (324.70)
Mixtral 24.48 (99.51) 27.89 (90.38) 26.07 (95.28)
Llama-3 45.30 (141.60) 50.75 (223.25) 47.87 (180.11)
GPT-4 51.89 (31.17) 56.08 (38.13) 53.90 (34.51)
Table 12: ETM scores without Result (%). Percentage
change relative to Table 3 are given in parenthesis high-
lighted in teal. The best results of each normalization
setting are underlined. Overall best results are given in
bold.ModelETM ETM w/o Result
R P F1 R P F1
Llama-2 7.33 4.33 5.45 26.60 33.27 29.56
Mixtral 12.49 13.45 12.95 27.85 29.87 28.83
Llama-3 22.82 19.04 20.78 53.42 58.84 56.00
GPT-4 45.47 46.90 46.17 60.24 64.43 62.26
Table 13: ETM scores (%) with and without Result
values in partially pre-defined setting on the leaderboard
dataset with paper threshold two.
ModelIIM-Task IIM-Dataset IIM-Metric
R P F1 R P F1 R P F1
Llama-2 42.98 39.70 41.27 33.14 41.05 36.67 59.34 61.24 60.28
Mixtral 60.72 50.23 54.98 44.45 49.67 46.92 71.19 78.72 74.77
Llama-3 80.39 65.72 72.32 62.86 66.81 64.77 88.90 94.90 91.80
GPT-4 78.30 63.82 70.32 79.52 83.29 81.36 89.27 92.21 90.72
Table 14: IIM scores (%) in partially pre-defined setting
on the leaderboard dataset with paper threshold two.
E Paper Threshold Results
We provide full results for leaderboard threshold of
two papers in Tables 13, 14, 15 and 16.
F Cold Start Results
We provide standard deviation values for cold start
experiments in Tables 17, 18, 19 and 20.
G Prompts
Table 21 shows the system prompt for TDMR ex-
traction phase. In Table 22, we provide prompt
13Model LR PC RC AOFullyAxCell 32.26 13.31 9.37 9.50
Llama-2 + CS 46.77 24.21 4.67 2.00
Llama-2 51.61 25.70 5.21 3.77
Mixtral + CS 66.13 38.66 11.38 6.83
Mixtral 62.90 36.53 10.33 8.44
Llama-3 + CS 56.45 41.52 11.47 9.75
Llama-3 64.52 55.44 15.18 11.91
GPT-4 + CS 72.58 49.51 35.98 36.18
GPT-4 85.48 57.26 43.52 51.28PartiallyLlama-2 40.32 23.54 2.93 2.18
Mixtral 53.22 25.89 8.37 6.50
Llama-3 74.19 53.95 12.51 14.14
GPT-4 67.74 48.47 38.65 44.10Cold StartLlama-2 5.65 3.58 0.54 0.00
Mixtral 27.96 12.77 4.62 3.28
Llama-3 59.14 42.98 10.26 7.75
GPT-4 58.60 41.65 34.25 37.94
Table 15: Gold leaderboard ablation evaluation (%) on
leaderboard dataset with paper threshold two.
Model LR PC RC AOPartiallyLlama-2 31.58 16.08 0.00 0.00
Mixtral 31.58 17.22 9.59 14.62
Llama-3 73.68 48.07 12.11 10.25
GPT-4 63.16 39.60 31.29 38.67Cold StartLlama-2 8.77 5.26 1.75 0.00
Mixtral 40.35 21.90 10.26 8.74
Llama-3 64.91 48.86 15.94 8.49
GPT-4 59.65 42.21 33.19 38.54
Table 16: Leaderboard evaluation only on masked
leaderboards from partially masking setting (%) on the
leaderboard dataset with paper threshold two. Standard
deviation for cold start are given in Table 20.
Model LR PC RC AO
Llama-2 16.05 ¬±4.28 5.66 ¬±1.76 0.49 ¬±0.86 0.05 ¬±0.09
Mixtral 49.38 ¬±7.71 20.49 ¬±5.74 8.10 ¬±3.28 3.03 ¬±3.47
Llama-3 79.01 ¬±14.02 58.78 ¬±15.10 17.18 ¬±4.72 17.63 ¬±3.17
GPT-4 81.48 ¬±14.81 58.74 ¬±7.42 46.13 ¬±3.10 48.15 ¬±4.42
Table 17: Mean and standard deviation values for cold
start setting results in Table 5.
Model LR PC RC AO
Llama-2 10.00 ¬±0.00 3.10 ¬±1.01 0.67 ¬±1.15 0.00 ¬±0.00
Mixtral 53.33 ¬±15.27 22.67 ¬±7.78 10.97 ¬±3.22 7.91 ¬±7.98
Llama-3 86.67 ¬±23.09 78.14 ¬±16.61 22.59 ¬±2.29 14.79 ¬±3.34
GPT-4 83.33 ¬±11.55 60.76 ¬±5.70 43.11 ¬±4.00 37.50 ¬±3.44
Table 18: Mean and standard deviation values for cold
start setting results in Table 6.
Model LR PC RC AO
Llama-2 5.65 ¬±1.14 3.58 ¬±0.74 0.54 ¬±0.47 0.00 ¬±0.00
Mixtral 27.96 ¬±4.93 12.77 ¬±2.63 4.62 ¬±1.72 3.28 ¬±0.64
Llama-3 59.14 ¬±4.06 42.98 ¬±7.47 10.26 ¬±2.29 7.75 ¬±2.98
GPT-4 58.60 ¬±1.86 41.65 ¬±3.32 34.25 ¬±1.49 37.94 ¬±1.60
Table 19: Mean and standard deviation values for cold
start setting results in Table 15.Model LR PC RC AO
Llama-2 8.77 ¬±8.04 5.26 ¬±4.56 1.75 ¬±1.52 0.00 ¬±0.00
Mixtral 40.35 ¬±10.96 21.90 ¬±5.28 10.26 ¬±2.54 8.74 ¬±0.58
Llama-3 64.91 ¬±8.04 48.86 ¬±7.19 15.94 ¬±1.46 8.49 ¬±0.63
GPT-4 59.65 ¬±3.04 42.21 ¬±7.43 33.19 ¬±2.38 38.54 ¬±3.20
Table 20: Mean and standard deviation values for cold
start setting results in Table 16.
structure of fully pre-defined TDM triples normal-
ization setting. Tables 23 and 24 show the prompts
of two steps of partially pre-defined TDM triples
normalization.
14You will be given several parts of a research paper as input. Please extract different tuples including the name of the task
addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only
the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple.
Example format: [{"Task": "Task name", "Dataset": "Dataset name", "Metric": "Metric name", "Result": "Result score"}].
Your answer will immediately start with the json object satisfying the given template and contain nothing else.
Table 21: System prompt for TDMR extraction from scientific papers.
You will be given a list of items. Then, an input will be provided. You will match the input with one of the items in the list.
Your answer will ONLY consist of the matched item in the list, do not provide further explanations. If none of the items
matches, say None.
Item list: {‚ÄôCombinatory Categorial Grammar (CCG) Supertagging‚Äô, ‚ÄôConstituency Parsing‚Äô, ‚ÄôDependency Parsing‚Äô,
‚ÄôDialogue Act Classification‚Äô, ‚ÄôDialogue Generation‚Äô, ‚ÄôEntity Typing‚Äô, ‚ÄôIntent Detection and Slot Filling‚Äô, ‚ÄôLanguage
Modeling‚Äô, ‚ÄôLinguistic Acceptability‚Äô, ‚ÄôMachine Translation‚Äô, ‚ÄôNamed Entity Recognition (NER)‚Äô, ‚ÄôNatural Language
Inference (NLI)‚Äô, ‚ÄôParaphrase Detection‚Äô, ‚ÄôPart-of-Speech (POS) Tagging‚Äô, ‚ÄôQuestion Answering‚Äô, ‚ÄôQuestion Generation‚Äô,
‚ÄôRelation Classification‚Äô, ‚ÄôResponse Generation‚Äô, ‚ÄôSentiment Analysis‚Äô, ‚ÄôSummarization‚Äô, ‚ÄôText Chunking‚Äô, ‚ÄôText Similar-
ity‚Äô, ‚ÄôWord Sense Induction‚Äô}
Input: POS Tagging
Table 22: Example prompt for fully pre-defined TDM triples normalization setting of LLM outputs in terms of task
names
You will be given a list of items. Then, an input entity will be provided. If the input entity matches one of the items in the
list, your answer will be the matched item in the list. Else, output the entity without changing it. DO NOT make any other
explanation.
Item list: {‚ÄôCombinatory Categorial Grammar (CCG) Supertagging‚Äô, ‚ÄôConstituency Parsing‚Äô, ‚ÄôDependency Parsing‚Äô,
‚ÄôDialogue Generation‚Äô, ‚ÄôEntity Typing‚Äô, ‚ÄôLanguage Modeling‚Äô, ‚ÄôLinguistic Acceptability‚Äô, ‚ÄôMachine Translation‚Äô, ‚ÄôNatural
Language Inference (NLI)‚Äô, ‚ÄôParaphrase Detection‚Äô, ‚ÄôPart-of-Speech (POS) Tagging‚Äô, ‚ÄôQuestion Answering‚Äô, ‚ÄôQuestion
Generation‚Äô, ‚ÄôRelation Classification‚Äô, ‚ÄôResponse Generation‚Äô, ‚ÄôSentiment Analysis‚Äô, ‚ÄôSummarization‚Äô, ‚ÄôText Chunking‚Äô,
‚ÄôText Similarity‚Äô, ‚ÄôWord Sense Induction‚Äô}
Input: Named entity recognition
Table 23: Example prompt for first step of partially pre-defined TDM triples normalization of LLM outputs in terms
of task names. Different from the input set in Table 22, "Named Entity Recognition (NER)", "Intent Detection and
Slot Filling" and "Dialogue Act Classification" tasks have been masked.
You will be given a list of tuples. Then, an input tuple will be provided. If the input tuple matches one of the items in the
list, your answer will be the matched item in the list. Else, output the tuple without changing it. Only output answer, DO
NOT make any other explanation.
Item list: {(Named Entity Recognition (NER), WNUT-16 - English, F1), (Intent Detection and Slot Filling, ATIS,
Accuracy), (Summarization, CNN/DailyMail, ROGUE-1), (Word Sense Induction, SemEval 2013 Task 13, Fuzzy
normalized mutual information (FNMI)), ...., (Machine Translation, WMT‚Äô14 EN-FR, BLEU), (Intent Detection and Slot
Filling, SNIPS, F1), (Summarization, Gigaword, ROGUE-2), (Named Entity Recognition (NER), Ontonotes v5 - English,
F1)}
Input: (Entity Typing, WNUT-17, F1)
Table 24: Example prompt for second step of partially pre-defined TDM triples normalization of LLM outputs
in terms of task names. After the simulation of the newly introduced tasks or dataset, this prompt check whether
given input matches with any leaderboard tuples to implement proper evaluation. Due to convenient demonstrations
purposes, not all tuples in gold leaderboards are listed.
15