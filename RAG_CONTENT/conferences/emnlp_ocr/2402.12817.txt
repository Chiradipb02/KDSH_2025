On Sensitivity of Learning with Limited Labelled Data to the Effects of
Randomness: Impact of Interactions and Systematic Choices
Branislav Pecher♠†‡, Ivan Srba†, Maria Bielikova†‡
♠Faculty of Information Technology, Brno University of Technology, Brno, Czechia
†Kempelen Institute of Intelligent Technologies, Bratislava, Slovakia
‡Slovak.AI, Bratislava, Slovakia
{branislav.pecher, ivan.srba, maria.bielikova} @kinit.sk
Abstract
While learning with limited labelled data can
effectively deal with a lack of labels, it is also
sensitive to the effects of uncontrolled random-
ness introduced by so-called randomness fac-
tors (i.e., non-deterministic decisions such as
choice or order of samples). We propose and
formalise a method to systematically investi-
gate the effects of individual randomness fac-
tors while taking the interactions (dependence)
between them into consideration. To this end,
our method mitigates the effects of other factors
while observing how the performance varies
across multiple runs. Applying our method to
multiple randomness factors across in-context
learning and fine-tuning approaches on 7 rep-
resentative text classification tasks and meta-
learning on 3 tasks, we show that: 1) disregard-
ing interactions between randomness factors in
existing works led to inconsistent findings due
to incorrect attribution of the effects of random-
ness factors, such as disproving the consistent
sensitivity of in-context learning to sample or-
der even with random sample selection; and 2)
besides mutual interactions, the effects of ran-
domness factors, especially sample order, are
also dependent on more systematic choices un-
explored in existing works, such as number of
classes, samples per class or choice of prompt
format.
1 Introduction
Learning with limited labelled data, such as in-
context learning, fine-tuning or meta-learning, is
an umbrella term for approaches designed to work
when enough labels are lacking. Although such
approaches can effectively deal with limited la-
bels, they were observed to be notably sensitive to
the effects of uncontrolled randomness. Such ran-
domness is introduced by the randomness factors ,
which represent the non-deterministic decisions in
the training process, such as order of samples, the
model initialisation or sample choice (Pham et al.,
2021; Gundersen et al., 2022; Pecher et al., 2024b).The randomness in the training process can have
massive impact, leading to large deviation in the
performance over multiple training runs. In-context
learning was found to be sensitive to the order of
samples, where changing only order of samples
leads from state-of-the-art predictions to random
guessing (Lu et al., 2022; Zhao et al., 2021b). Sim-
ilarly, repeating fine-tuning and evaluation multi-
ple times with different random seeds can result
in smaller language models outperforming their
larger counterparts (Dodge et al., 2020). If the ran-
domness is not properly addressed, it can have non-
negligible negative consequences even with enough
labelled samples (Reimers and Gurevych, 2017;
McCoy et al., 2020). It was identified as a major
obstacle to reproducibility (Albertoni et al., 2023)
that can prohibit objective comparison and cause a
method to be incorrectly denoted as state-of-the-art
only based on more favourable chance (Reimers
and Gurevych, 2017). The uncontrolled random-
ness can also unintentionally (but, unfortunately,
also intentionally by cherry-picking) create an
imaginary perception of research progress.
A lot of focus is dedicated to investigating and
mitigating the effects of randomness and sensitiv-
ity of learning with limited labelled data (Mosbach
et al., 2021; Pecher et al., 2024b), especially for in-
context learning (Lu et al., 2022; Zhao et al., 2021b;
Chang and Jia, 2023; Li and Qiu, 2023; Köksal
et al., 2023). However, the existing research is of-
ten limited in its extent (in terms of randomness
factors, approaches or settings) and at times leads to
contradictory or inconsistent results. For example,
in-context learning was believed to be consistently
sensitive to the order of the randomly selected sam-
ples (Lu et al., 2022; Zhao et al., 2021b), however,
it was later observed that this sensitivity disappears
when a more sophisticated sample selection strat-
egy is used (Zhang et al., 2022; Chang and Jia,
2023; Li and Qiu, 2023).
We argue that the observed inconsistencies are
1arXiv:2402.12817v2  [cs.CL]  3 Oct 2024caused by disregarding the interactions between
randomness factors , which leads to incorrectly at-
tributing the performance deviations to different
randomness factors . Such interactions are so far
partially or even completely overlooked in the exist-
ing works, resulting in the misleading findings. In
addition, we hypothesise that the sensitivity of in-
context learning is not only affected by the interac-
tions with other factors but also by other systematic
choices , which are not thoroughly controlled and
explored in the existing works, such as the number
of classes, shots per class and prompt format.
Our main contributions are as follows1:
•We propose a novel method for investigation
ofrandomness factors ’ effects that, in contrast
to the existing works, is thoroughly formalised
and explicitly addresses interactions between
them by mitigating the effects of other non-
investigated factors. In addition, it measures
the relative importance of factors, by calcu-
lating what fraction of the overall deviation
in the performance (estimated by a golden
model) the investigated factor contributes in
comparison to all other factors, which allows
for more in-depth analysis across factors, mod-
els, datasets and experimental settings.
•Using the proposed method, we investigate
5randomness factors and their effects on
in-context learning and fine-tuning across 7
representative text classification datasets, and
meta-learning across 3 datasets. The results
show that the in-context learning models are
not consistently sensitive to the order of sam-
ples, confirming our hypothesis that the inter-
actions play a role in the incorrect attribution
of the effects of randomness factors.
•We further analyse how the more systematic
choices influence the importance of the ran-
domness factors. We find the following key
insights: 1) predicting a higher number of
classes leads to increased importance of sam-
ple order for in-context learning and reduced
importance of sample order and model initial-
isation for fine-tuning approaches; 2) increas-
ing the number of in-context samples reduces
1To support replicability and extension of our results,
we openly publish the source code of our proposed inves-
tigation method and experiments for determining the fac-
tor importance at https://github.com/kinit-sk/
L3D-sensitivity-investigationthe importance of sample selection while hav-
ing no consistent effect on the importance of
sample order; and 3) the choice of prompt
format has a significant impact on the impor-
tance of different factors, with larger models
showing lower sensitivity to this choice.
2 Related Work
The main strategy for investigating the effects of
randomness factors is to repeat the training and
evaluation multiple times, changing specific non-
deterministic decisions of the training, such as
changing what data is used and observing the
change in results (i.e., Random strategy ) (McCoy
et al., 2020; Dodge et al., 2020; Bouthillier et al.,
2019; Agarwal et al., 2021). As such investigation
may be affected by the interactions with other fac-
tors, another possibility is to perform the investiga-
tion by fixing all the other factors to a specific state
(i.e., Fixed strategy ), either chosen randomly (Bo-
quet et al., 2019; Pham et al., 2021; Zhao et al.,
2021a) or as a result of a mitigation strategy (Li
and Qiu, 2023; Chang and Jia, 2023). Another in-
vestigation strategy is to vary all the investigated
factors at the same time and then decouple their ef-
fects in evaluation (Dodge et al., 2020; Bouthillier
et al., 2021; Sellam et al., 2022; Weber et al., 2023;
Webson and Pavlick, 2022), which accounts for the
interactions but introduces a significant increase in
computation costs (Bouthillier et al., 2021).
Majority of the focus on investigating and miti-
gating effects of randomness is on in-context learn-
ing, which was found to be especially sensitive
to the choice of samples (Liu et al., 2022; Zhang
et al., 2022; Chang and Jia, 2023; Li and Qiu, 2023;
Köksal et al., 2023) and their order (Lu et al., 2022;
Zhao et al., 2021b; Nguyen and Wong, 2023). How-
ever, it was observed that the sensitivity to sample
order disappears when using a more sophisticated
sample selection strategy instead of random selec-
tion (Zhang et al., 2022; Chang and Jia, 2023; Li
and Qiu, 2023), hinting at interactions between
these factors that may lead to inconsistent results.
In addition, the performance of in-context learn-
ing was found to be sensitive to more systematic
choices as well (Weber et al., 2023), such as the
format of the prompt (Sclar et al., 2023; V oronov
et al., 2024) or number of shots (Liu et al., 2022;
Mavromatis et al., 2023). However, the impact of
these systematic choices on the effects of random-
ness factors is not thoroughly investigated. Besides
2order of in-context examples, large language mod-
els were found to be especially sensitive to the
order of choices in multi-choice question answer-
ing (Zong et al., 2023; Wei et al., 2024). Although
the remaining approaches and randomness factors
receive only limited focus, they were still found
to be sensitive to the effects of randomness, such
as fine-tuning being sensitive to the random seeds
(that influence model initialisation and order of
samples) (Dodge et al., 2020; McCoy et al., 2020;
Mosbach et al., 2021; Zhao et al., 2021a; Zhong
et al., 2021), meta-learning being sensitive to the
choice of adaptation samples or how they are split
into tasks (Agarwal et al., 2021; Setlur et al., 2021;
Cioba et al., 2022; Ye et al., 2021), or the overall
machine learning being sensitive to factors such as
the impact of framework and hardware implemen-
tation (Boquet et al., 2019; Pham et al., 2021), or
the data split (Bouthillier et al., 2019, 2021).
In majority of the cases, the effects of random-
ness are evaluated based on a single aggregated
metric from multiple runs (e.g., mean, standard de-
viation, or the difference between best and worst
run), with the importance being determined in a
binary fashion by comparing this metric to a thresh-
old, which allows only for simple analysis (McCoy
et al., 2020; Ye et al., 2021; Zhang et al., 2022)).
A slightly more nuanced analysis is possible only
in specific cases, where statistical approaches are
used, such as grouping runs and aggregating on
group level (Dodge et al., 2020), decoupling inter-
actions (Boquet et al., 2019) or estimating distribu-
tion from lower number of training runs (Sellam
et al., 2022). However, almost no studies analyse
the importance of the effects in a way that would
allow for easy comparison across different settings,
such as what fraction of the overall variance the
specific factor contributes.
We build on the ideas from the existing works,
mainly from (Dodge et al., 2020; Bouthillier et al.,
2021; Zhao et al., 2021a), to explicitly take interac-
tions into consideration and analyse the importance
of the found effects. In addition, we fill the identi-
fied research gap by analysing the impact of more
systematic choices on the randomness factors.
3 Investigation of Randomness while
Taking Interactions into Consideration
We propose a new method for investigating the
effects of any randomness factor that takes the
interactions between the effects of other factorsAlgorithm 1 Investigate randomness factor with
interactions and determine its importance
Require: K: number of randomness factors
Require: RF: set of randomness factors to con-
sider
Require: C1,C2, ...,CK: set of configurations for
each factor
1:Select randomness factor ito investigate from
RF
2:SetIFCi=Ci
3:SetMFC i=C1×...×Ci−1×Ci+1×...×CK
4:for all minMFC ido
5: for all ninIFCido
6: Determine model performance rm,nby
training and evaluating the model using
mandn
7: end for
8: Calculate p_mean m=rm,∗
9: Calculate p_stdm=std(rm,∗)
10:end for
11:Calculate contributed standard deviation
c_std=p_std∗
12:Calculate mitigated standard deviation
m_std=std(p_mean ∗)
13:SetGMC i=C1×C2×...×CK−1×CK
14:for all ginGMC do
15: Determine golden model performance rgby
training and evaluating the model using g
16:end for
17:Calculate overall golden model standard devia-
tiongm_std=std(r∗)
18:Calculate importance score of the inves-
tigated factor importance = (c_std−
m_std)/gm _std
19:ifimportance > 0then
20: Effects of factor iconsidered important
21:end if
into consideration, and which is designed to mea-
sure the importance of the found effects. The
steps of the method are compiled in Algorithm 1,
with further supplementary details included in Ap-
pendix B).
Setup. First, a set RF(|RF|=K) is defined,
which includes all the factors that will be consid-
ered in the investigation. Each randomness factor
is characterised by a set of its randomness fac-
tor configurations ,Cj, specifying all the possible
states the factor can appear in. For example, the
3different permutations of samples represent the con-
figurations of the data order randomness factor. For
each factor i, the investigated factor configurations
setIFCi, containing the configurations used for the
investigation, is defined as IFCi=Ci, and the mit-
igated factor configurations setMFC i, containing
the joint configurations of the remaining random-
ness factors, is defined as a cross product between
all the sets of randomness factor configurations ,
except for the investigated randomness factor ( Ci):
MFC i=C1×...×Ci−1×Ci+1×...×CK(1)
Investigating effects. At its core, the investiga-
tion of factor iis done by observing how the perfor-
mance changes across the different configurations
the randomness factor can appear in. In a single
investigation run , the training and evaluation of
a model is repeated Ntimes ( N=|IFCi|), each
time with a different configurations nof the factor i,
while keeping the configurations of the remaining
factors fixed to a randomly chosen configuration
mfromMFC i. For each repeat, the model per-
formance ( rm,n) is determined. The standard de-
viation p_stdm(called partial standard deviation)
across these Nruns ( p_stdm=std(rm,∗)) repre-
sents the effects of the investigated randomness
factor that are still affected by the interactions.
Mitigating interactions. To remove the effects
of other randomness factors, the investigation run
is repeated multiple ( M) times each time with a
different fixed configuration mfromMFC i. Each
such repeat is called mitigation run and results in a
separate partial standard deviation. After perform-
ing enough mitigation runs (i.e., searching through
enough configurations mof the non-investigated
randomness factors), the partial standard deviations
(p_stdm) are averaged to produce the contributed
standard deviation (c_std=p_std∗), which repre-
sents the final adjusted effects of the investigated
factor i(i.e., it represents the deviation the investi-
gated randomness factor contributes to the overall
deviation in results).
Calculating importance score. To assess the im-
portance of the factor, the contributed standard
deviation is compared with two additional values:
1)mitigated standard deviation (m_std); and 2)
golden model standard deviation (gm_std). The
mitigated standard deviation represents the joint
effects of all the non-investigated randomness fac-
tors (i.e., standard deviation contributed by non-
investigated factors). To obtain this value, a partialmean (p_mean m) is calculated for each investiga-
tion run, which represents the expected average
model performance for the given combination of
configurations of the non-investigated factors. The
mitigated standard deviation is then calculated as
the standard deviation across these partial means
(m_std=std(p_mean ∗)).
Thegolden model standard deviation (gm_std)
represents an objective estimate of the deviation
in the model performance. To get this estimate, a
golden model configuration setGMC (|GMC|=
L) is defined, as a cross product between the sets
of all the randomness factor configurations:
GMC =C1×C2×...×CK−1×CK (2)
Afterwards, a model is trained and evaluated L
times each time with different configuration gfrom
GMC , the model performance rgis determined and
the standard deviation across these runs represents
thegolden model standard deviation gm_std.
The final importance score of the factor is de-
fined as the portion of the golden model stan-
dard deviation the investigated factors contribute
over the non-investigated ones ( importance =
(c_std−m_std)/gm _std). Any randomness fac-
tor with an importance value over 0is considered
important, as it contributes the same amount of de-
viation as all the remaining factors combined. The
size of the score determines the relative importance
between the factors (e.g., factor with importance
score of 0.6 is more important than one with score
of 0.1) and can be used for further analysis and com-
parison across different factors, models, datasets
and experimental settings (e.g., how the importance
of specific factor changes if the number of samples
is increased or a different dataset is used).
Choosing values for parameters N,MandL.
The number of investigation runs (N) and the mit-
igation runs (M) provide a trade-off between the
feasibility (or computation costs) of the investiga-
tion and the precision of the results (how well the
effects are estimated and interactions mitigated).
Below, we provide a set of heuristics to achieve a
good trade-off (and provide full method for select-
ing the values of the parameters in Appendix B.3):
1.N, M ≫1;NandMshould cover a large
enough number of factor configurations to suf-
ficiently estimate the effects.
2.M≥N; as the higher number of mitigation
runs ( M) leads to better mitigation of the in-
4teractions, increasing value of Mshould be
preferred over increasing the number of inves-
tigation runs ( N).
3.L=N∗M; to guarantee the importance
score is calculated from distributions of the
same sizes and characteristics, the number of
runs in the golden model should be equal to
the overall number of runs in the investigation.
Validation of the proposed method. We evalu-
ate the validity of the proposed method indirectly
(as there is no ground-truth to compare against)
using the following experiments: 1) comparing
the method to two existing baselines (i.e., Random
andFixed investigation strategy) and evaluating the
properties and benefits of our method, specifically
the handling of interactions that may lead to un-
derestimation or overestimation of the effects in
specific cases, and the importance score that allows
for more in-depth analysis and comparison across
different experimental settings; 2) exploring the
dependence of how well the effects are estimated
and their interactions mitigated by our method to
the number of investigation and mitigation runs,
where we found that the results of our methods are
stable already with a low number of runs (20 miti-
gation and 10 investigation runs); and 3) observing
the consistency of the results and findings when
applying the method to different settings (factors,
approaches, datasets). The full description of the
validation results is in Appendix E.
4 Experiments
Datasets. The experiments are conducted on
7 text classification datasets composed of dif-
ferent tasks with different number of classes.
We focus on 3 binary classification datasets
from the GLUE benchmark (Wang et al., 2018):
SST2 (Socher et al., 2013) for sentiment classifica-
tion, CoLA (Warstadt et al., 2019) for determining
the grammatical acceptability of a sentence, and
MRPC (Dolan and Brockett, 2005) for determining
the semantic equivalence relationship between two
sentences. In addition, we use 4 multi-class text
datasets: AG News (Zhang et al., 2015) for news
classification, TREC (V oorhees and Tice, 2000) for
question classification, DB-Pedia (Lehmann et al.,
2015) for topic classification and SNIPS (Coucke
et al., 2018) for intent classification.
Approaches. The main focus of the investiga-
tion is on the in-context learning using the Flan-T5 (Chung et al., 2022) base, LLaMA-2 (Tou-
vron et al., 2023) 13B instruction optimised model,
Mistral-7B (Jiang et al., 2023) and Zephyr-7B (Tun-
stall et al., 2023). In addition, we also focus on fine-
tuning , using the BERT (Devlin et al., 2019) and
RoBERTa (Liu et al., 2019) base models. Finally,
we also investigate the meta-learning approaches
MAML (Finn et al., 2017), Reptile (Nichol et al.,
2018) and the Prototypical Networks (Snell et al.,
2017), but only on the binary datasets.
Randomness Factors. In the experiments, we
evaluate following randomness factors: 1) Label
Selection used to determine the samples consid-
ered as labelled during training; 2) Data Split used
to split the data into training, validation and test
sets; 3) Data Order that determines the order of
samples in training (order of in-context examples
in prompts for in-context learning, order in which
samples appear in batches for fine-tuning or tasks in
meta-learning); 4) Sample Choice (not relevant for
fine-tuning) that determines the randomly chosen
samples used as in-context examples for in-context
learning (or adaptation samples for meta-learning);
and 5) Model Initialisation (not relevant for in-
context learning) related to the randomly initialised
weights and other parameters in the models.
Method Setup. For each randomness factor , the
number of the investigation runs ( N) is set to 10,
the number of mitigation runs ( M) is set to 100
for fine-tuning, meta-learning and 20 for in-context
learning. The golden model uses the same over-
all number of runs ( L) (1 000 for fine-tuning and
meta-learning, 200 for in-context learning). These
values, selected based on an Ablation Study (in-
cluded in Appendix C), provide a balance between
the coverage of the configurations’ state space and
the computation costs.
Experimental Setup. We focus on a setting with
limited labelled data, which represents a practical
real-world scenario where a limited budget requires
us to choose what data we label (a common case for
many NLP supervised tasks). To simulate the un-
availability of labels, we randomly select 1000 train
samples from a sufficiently large labelled dataset
and consider only these to be labelled. Before
choosing this subset of samples, each dataset is
split into train and test using 80-20 split. In addi-
tion, 20% of the labelled train samples are used as
a validation set. As such, we use different training,
validation and test samples across different runs.
5We report the performance using the F1 macro met-
ric. If not specified otherwise, we run in-context
learning in a 2-shot setting with the first prompt for-
mat from Table 4. All prompt formats and further
experimental details are included in Appendix D.
FLAN-T5 R ANDOM FIXED INTERACTIONS
GOLDEN MODEL 2.244 2.244 2.244
LABEL SELECT . (*) 2.517 (*) 2.594 (*) 2.128
DATA SPLIT (*) 2.362 (*) 2.480 (*) 2.167
DATA ORDER (*) 2.131 (*) 3.014 0.869
SAMPLE CHOICE (*) 2.370 (*) 3.191 (*) 2.123
ZEPHYR -7B R ANDOM FIXED INTERACTIONS
GOLDEN MODEL 1.043 1.043 1.043
LABEL SELECT . (*) 1.122 (*) 1.004 (*) 0.863
DATA SPLIT (*) 1.185 0.402 (*) 0.664
DATA ORDER (*) 1.138 (*) 0.957 0.456
SAMPLE CHOICE (*) 1.052 0.406 (*) 0.744
Table 1: Comparison of different investigation strate-
gies for the Flan-T5 and Zephyr-7B models on the SST2
dataset based on the F1 macro standard deviation. Fac-
tors considered important for different strategies are
denoted using the (*) symbol. We observe that interac-
tions between factors may cause some factors to have
their importance overestimated (denoted in bold ) or
underestimated (denoted in italics ).
4.1 Interactions Between Randomness Factors
In this section, our goal is to answer the following
research question: RQ1: How do the interactions
between randomness factors affect their individual
importance? To answer this question, we com-
pare our proposed method ( Interactions ) with the
commonly used investigation strategies: 1) Ran-
dom , which varies the overall random seed in the
investigation without any constraint on the config-
urations of other factors; and 2) Fixed , where the
non-investigated randomness factors are fixed to a
single configuration for all runs of the investigation.
For these strategies, we consider the effects of fac-
tor to be important when it contributes at least 50%
of the golden model standard deviation. The results
from this comparison are shown in Table 1 and used
for validation of our method in Appendix E.
Effects of randomness factors may be overesti-
mated or underestimated when interactions are
not taken into consideration. TheRandom strat-
egy leads to a deviation similar to the one from the
golden model across all investigated randomness
factors. Such result indicates that all randomness
factors are equally important, leading to a signifi-
cant importance overestimation in some cases (e.g.,Data Order factor for both Flan-T5 and Zephyr
models). Even though the Fixed strategy produces
more reliable results, it is still affected by the ran-
dom choice of the single factor configuration (e.g.,
Data Order contributing deviation of 3.014, which
is much higher than the deviation of 2.244from
the golden model). As such, we observe underes-
timation of the results (e.g., Sample Choice and
Data Split with the Zephyr-7B model not being
considered important with a deviation of 0.406and
0.402) as well as overestimation (e.g., Data Or-
der being considered important with a deviation of
3.014for Flan-T5 and 0.957for Zephyr-7B). Tak-
ing the interactions into consideration, we observe
that the Data Order randomness factor is not
consistently important for in-context learning
even when choosing samples randomly , which
confirms the impact of interactions on incorrect at-
tribution of effects of different randomness factors.
4.2 Importance of Randomness Factors
In this section, we want to answer the following
research question: RQ2: What randomness factors
are important for different approaches for learn-
ing with limited labelled data? We analyse the
results of our method on different datasets to iden-
tify the consistently important factors. The results
are included in Figure 1 for in-context learning and
fine-tuning and in Appendix F.1 for meta-learning.
Sample Choice represents the most important
factor for in-context learning. For the majority
of the investigated models, the Sample Choice fac-
tor is considered important for almost all of the
datasets, achieving an average importance score
of0.25across the models and datasets. A notable
exception is Flan-T5 on the multi-class datasets
(average importance score of −0.39) or Zephyr on
the MRPC dataset (importance score of −0.43),
where the factor is not considered important.
Importance of Data Order is dataset and
model dependent for in-context learning. Major-
ity of the in-context learning models do not show
sensitivity to the Data Order randomness factor
on binary datasets (average importance score of
−0.28). At the same time, the importance of Data
Order becomes consistently higher on multi-class
datasets for all models (average importance score
of0.16), with the exception of the Zephyr-7B.
General randomness factors, Label Selection
and Data Split, show consistent importance for
the majority of the models and datasets. In case
of fine-tuning, the Label Selection and Data Split
6Figure 1: Importance of the investigated randomness factors for all investigated approaches and datasets while
taking the interactions between factors into consideration. The legend indicates the number of classes for each
dataset. As the Flan-T5 model predicts the same class for every sample on the DB-Pedia dataset, we do not include
these results. Increasing the number of classes in datasets results in increased importance of the Data Order factor
for in-context learning and reduced importance of Data Order and Model Initialisation for fine-tuning approaches.
Figure 2: The change in importance of the Data Order and Sample Choice randomness factors as the number of
in-context examples increases. Increasing the number of samples per class does not have a consistent effect on the
importance of the Data Order factor, while the importance of the Sample Choice factor decreases.
randomness factors show the highest level of impor-
tance across all datasets when compared to other
randomness factors (average importance score of
0.34and 0.52). For in-context learning, we do
not observe such consistent results, with the im-
portance changing based on the dataset and model
used. However, these factors are considered impor-
tant in more than half of the cases (16 out of 27 for
Label Selection and 22 out of 27 for Data Split).
Importance of Data Order and Model Ini-
tialisation is dataset and model dependent for
fine-tuning. For the binary datasets, these factors
are considered important for both models (average
importance of 0.25for Data Order and 0.19for
Model Initialisation). However, on the multi-class
datasets, the importance of Sample Order for both
models (average importance score of −0.14) and
Model Initialisation for the BERT model (average
importance score of −0.30for BERT and 0.04for
RoBERTa) drops significantly.4.3 Effects of Variable Number of Classes and
In-Context Samples
In this section, our focus is on answering the follow-
ing research question: RQ3: How does the impor-
tance of data-specific randomness factors change
based on the number of classes and in-context sam-
ples? As we observe different effects of random-
ness factors on binary and multi-class datasets, our
main focus is to determine whether the change in
importance is caused by the increased number of
in-context examples in the prompt, by the larger
number of options that can be predicted or by a
combination of both. The results from changing
the number of classes are included in Figure 1, and
from changing the number of shots for in-context
learning are included in Figure 2.
The importance of Data Order randomness
factor for in-context learning increases at higher
number of classes. The Data Order randomness
factor is not considered important for any of the
in-context learning models on the SST2 dataset,
achieving importance of −0.47,−0.53,−0.16and
7Figure 3: Effect of different prompt formats on the importance of randomness factors for in-context learning. The
choice of format has a significant effect on the importance of different factors, with the minimal formats often
leading to higher importance. At the same time, the larger models show lower sensitivity to prompt format.
−0.44respectively for the Flan-T5, LLaMA-2,
Mistral and Zephyr models. On the remaining bi-
nary datasets, the importance either gradually in-
creases (LLaMA-2 or Zephyr) or decreases (Flan-
T5 and Mistral). However, on the datasets with
the higher number of classes, the importance of
the Data Order factor gradually increases (with
the exception of the Zephyr model), achieving im-
portance as high as 0.25for Flan-T5 and 0.29for
Zephyr on the SNIPS dataset, and 0.41for LLaMA-
2 and 0.18for Mistral model on DB-Pedia dataset.
The importance of the Sample Choice for in-
context learning is not consistently affected by
the number of classes. In case of Flan-T5 and
Zephyr, the importance of Sample Choice gradually
decreases as we increase the number of classes
(from 0.57on SST2 to −0.57on SNIPS for Flan-
T5, or from 0.39on SST2 to 0.17on DB-Pedia for
Zephyr). For the LLaMA-2 model, the decrease is
not as consistent, with the importance being much
lower on the TREC than on the SNIPS dataset.
Finally, the Sample Choice randomness factor is
consistently important across all datasets for the
Mistral model, with no apparent tendency.
The importance of Data Order and Model
Initialisation randomness factors for fine-tuning
decreases with higher number of classes. For
BERT model, we observe a gradual decrease of
importance for both factors as we increase the num-
ber of classes, going from 0.45and0.33on SST2
to−0.55and−0.50on DB-Pedia, respectively for
Data Order and Model Initialisation. Similarly, we
observe a gradual decrease of Data Order impor-
tance for RoBERTa model, going from 0.46on
SST2 to −0.19on DB-Pedia. However, Model Ini-
tialisation does not show a consistent tendency for
RoBERTa, with the importance staying approxi-
mately the same across the majority of the datasets.Number of in-context samples has no consis-
tent effect on the importance of Data Order. The
importance of Data Order remains consistent, or
even is lowered, across all models, datasets and
number of shots per class. On the other hand, in-
creasing the number of shots reduces the impor-
tance of Sample Choice factor for all models and
datasets. For example, the importance of Sample
Choice for Zephyr drops from 0.39on 2-shot set-
ting to 0.02on 10-shot setting on the SST2 dataset.
4.4 Impact of Prompt Format
In this section, we aim to answer the following
research question: RQ4: How does the prompt
format affect the importance of randomness fac-
tors? As the previous works observed a significant
sensitivity of in-context learning to prompt format
our goal is to investigate whether such sensitivity
affects the importance of randomness factors as
well. To achieve this, we compare our optimised
prompt format (Format A) with 3 minimal prompt
formats (Formats B, C and D) defined in (Li and
Qiu, 2023; Gao et al., 2021; Köksal et al., 2023).
All the prompt formats are described in detail in
Table 4 in Appendix D. The results from the inves-
tigation are illustrated in Figure 3, with full results
included in Appendix F.
Minimal formats lead to significant changes in
the importance of randomness factors over the
optimised format. The Data Order randomness
factor shows the highest sensitivity to the prompt
format, becoming significantly important in many
cases even when the interactions are taken into con-
sideration. At the same time, Sample Choice is
not as sensitive to the prompt format. The remain-
ing randomness factors, Label Selection and Data
Split, are affected only when using specific formats
– using the last format, we observe a significant
8change in the importance of these randomness fac-
tors across all models and datasets. The larger
models , Mistral and Zephyr, show lower sensitiv-
ity to prompt format change , as the importance
of all randomness factors remains consistent across
formats. On the other hand, in case of the Flan-
T5 model, the importance of randomness factors
changes significantly across different formats.
4.5 Discussion
Besides understanding the sensitivity, an important
aspect is predicting a good configurations of the
most important randomness factors to guarantee
stability and generalisability of the approaches. As
opposed to the hyperparameter tuning, finding this
configuration is not as straightforward as it is not
so systematic. First, as we show in this work, the
importance and the best configuration is strongly
affected by the interactions between randomness
factors and other systematic choices. Second, there
is no metric that can serve as an estimate for the
quality of the configuration besides the observed
performance – for example when finding an op-
timal prompt, the best performing and worst per-
forming one can differ only in a single word (Zhan
et al., 2024).
One way how to determine the optimal config-
uration are the mitigation strategies that recently
started to attract a research attention (see the re-
cent survey on stability of learning with limited
labelled data (Pecher et al., 2024b) for more infor-
mation). While the mitigation strategies are often
factor-specific, such as sample selection strategies
for in-context learning (Li and Qiu, 2023; Chang
and Jia, 2023; Köksal et al., 2023; Pecher et al.,
2024c), also more general strategies based on en-
sembling and further model training have been de-
veloped (Pecher et al., 2024a; Pezeshkpour and Hr-
uschka, 2023; Summers and Dinneen, 2021; Wang
et al., 2023; Allingham et al., 2023; V oronov et al.,
2024). Uncovering the most important randomness
factors through systematic investigation and design
of new and effective mitigation strategies to reduce
the sensitivity to the effects of randomness is an im-
portant future directions of the field (Pecher et al.,
2024b; Liu et al., 2023).
5 Conclusion
In this work, we have proposed a novel method that
explicitly takes interactions between different ran-
domness factors into consideration by mitigatingthe effects of the other, non-investigated, random-
ness factors. In addition, our method is designed to
determine the importance of the investigated ran-
domness factor by measuring what fraction of the
overall deviation of the model (represented using
a golden model) it contributes over the mitigated
randomness factors, allowing for in-depth analysis
across experimental settings.
Applying our proposed method to investigate the
effects of randomness factors on in-context learn-
ing, fine-tuning and meta-learning, we confirm our
hypothesis that interactions between randomness
factors may cause incorrect attribution of effects
of one factor to another, leading to inconsistent re-
sult. Contrary to the previous works, after taking
interactions into consideration, we do not observe
a consistent sensitivity of in-context learning ap-
proaches to the sample order even when choosing
samples at random. Instead, we observe that the
importance of randomness factors, especially the
sample order, is affected by the interactions with
other factors and by the systematic choices such
as the number of predicted classes, the number of
samples per class and the choice of prompt format.
The proposed method can be applied to other
NLP tasks as well, such as question answering,
with minimal modifications. Only requirement is
to define the randomness factors and their configu-
rations, such as order of choices in the questions or
the symbols used for the answers. Extending our
investigation to other tasks represents an interesting
potential for future work.
Acknowledgements
This work was partially supported by the projects
funded by the European Union under the EU Hori-
zon 2020: TAILOR , GA No. 952215, by the Euro-
pean Union under the Horizon Europe: DisAI , GA
No. 101079164 and vera.ai , GA No. 101070093,
and by the EU NextGenerationEU through the Re-
covery and Resilience Plan for Slovakia under the
project No. 09I03-03-V03-00020.
Part of the research results was obtained us-
ing the computational resources procured in
the national project National competence centre
for high performance computing (project code:
311070AKF2) funded by European Regional De-
velopment Fund, EU Structural Funds Informati-
zation of Society, Operational Program Integrated
Infrastructure.
9Limitations
The effects of randomness factors are investigated
on a selection of models from different approaches
for learning with limited labelled data. However,
in order to provide a more extensive and in-depth
analysis of the interactions and the more systematic
choices without a significant increase in computa-
tion costs, the effects are investigated on models of
smaller sizes – we use the base versions of BERT,
RoBERTa and Flan-T5 models, and a 4-bit quan-
tised versions of the LLaMA-2-13B, Mistral-7B
and Zephyr-7B models. As such, the observed ef-
fects may not be as representative for larger models.
However, similar to related work, we observed the
larger models to be more susceptible to the effects
of randomness and so the results of our investiga-
tion may underestimate the importance of different
factors (instead of their over-estimation).
The number of investigation and mitigation runs
used in our investigation is selected based on an
ablation study (in Appendix 3). In addition, fol-
lowing related work (e.g., (Gao et al., 2021; Chang
and Jia, 2023; Sclar et al., 2023; Li and Qiu, 2023;
Köksal et al., 2023)) and the results of our ablation
study, we also evaluate each run using only 1 000
test samples. In both cases, the decision represents
a trade-off between the reliability of the results and
their feasibility. Although this represents an opti-
mal trade-off, increasing the number of runs and
the number of test samples could potentially lead
to better estimation of the effects and mitigation of
their interactions (especially on larger datasets), but
at the cost of significant computation costs increase.
At the same time, the number of investigation and
mitigation runs utilised in this paper still represents
a significant improvement over the existing stud-
ies, as it is a common practice to investigate the
effects using very low numbers of runs. As future
work, we plan to explore the possibilities for ef-
fective mitigation strategies for all the randomness
factors to mitigate their effects and further reduce
computation costs.
Similarly, we investigate the effects on a smaller
set of training labelled samples (using only 1 000
labelled samples). This setup may lead to larger
effects of randomness and lower stability for fine-
tuning and meta-learning while having negligible
impact on in-context learning (which works with a
smaller subset of these samples). However, as this
represents a real-world scenario with a limited bud-
get, we do not consider this to be a significant lim-itation, as the effects of randomness factors were
previously found to be significant even with the
use of large datasets (Mosbach et al., 2021; Dodge
et al., 2020; Reimers and Gurevych, 2017).
Even though the effects of implementation level
randomness factors (e.g., framework implemen-
tation and hardware scheduling) were previously
observed to affect the models’ performance, we
consider their effects only partially by mitigating
them as much as possible (e.g., setting CUDA to be
deterministic, using the same version of libraries
and the same system for all experiments). Inves-
tigating their effects fully is out of scope for this
paper due to the specifics required to thoroughly
explore these effects (e.g., using a single worker,
deterministic implementation, or running on a sin-
gle CPU thread (Pham et al., 2021)).
Although we perform basic prompt engineering
to obtain our optimised prompt for each dataset, the
prompt format could be theoretically improved us-
ing automatic prompt-tuning methods. As we have
observed the impact of prompt format on the effects
of different randomness factors, the use of such for-
mat may lead to different findings, especially for
the Flan-T5 model. However, we still observed the
main in-context learning models (Mistral-7B and
Zephyr-7B) to be sufficiently robust to this change
and their results should stay the same. At the same
time, our main prompt was designed based on the
recommendations and prompt formats from related
work (Sun et al., 2023; Li and Qiu, 2023; Gao
et al., 2021; Köksal et al., 2023), so we do not ex-
pect significant changes when using more prompts
obtained through prompt-tuning.
Finally, we are not sure whether the datasets we
use in our experiments have been used to train the
models we use for in-context learning, which may
affect our findings and results on these models. We
limit this effect by using our own optimised prompt
across the majority of the experiments. However,
we cannot guarantee it is enough to provide unbi-
ased results as this limitation is part of the recently
recognised LLM validation crisis (Li and Flanigan,
2023) and we would need to train the model from
scratch to address it properly, which is out of scope
for this paper.
References
Mayank Agarwal, Mikhail Yurochkin, and Yuekai Sun.
2021. On sensitivity of meta-learning to support
data. In Advances in Neural Information Processing
10Systems , volume 34, pages 20447–20460. Curran
Associates, Inc.
Riccardo Albertoni, Sara Colantonio, Piotr
Skrzypczy ´nski, and Jerzy Stefanowski. 2023.
Reproducibility of machine learning: Terminology,
recommendations and open issues. arXiv preprint
arXiv:2302.12691 .
James Urquhart Allingham, Jie Ren, Michael W Dusen-
berry, Xiuye Gu, Yin Cui, Dustin Tran, Jeremiah Zhe
Liu, and Balaji Lakshminarayanan. 2023. A simple
zero-shot prompt weighting technique to improve
prompt ensembling in text-image models. In Pro-
ceedings of the 40th International Conference on
Machine Learning , volume 202 of Proceedings of
Machine Learning Research , pages 547–568. PMLR.
Thomas Boquet, Laure Delisle, Denis Kochetkov,
Nathan Schucher, Boris N Oreshkin, and Julien
Cornebise. 2019. Reproducibility and Stability Anal-
ysis in Metric-Based Few-Shot Learning. RML@
ICLR , 3.
Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, As-
sya Trofimov, Brennan Nichyporuk, Justin Szeto,
Nazanin Mohammadi Sepahvand, Edward Raff,
Kanika Madan, Vikram V oleti, et al. 2021. Ac-
counting for variance in machine learning bench-
marks. Proceedings of Machine Learning and Sys-
tems, 3:747–769.
Xavier Bouthillier, César Laurent, and Pascal Vincent.
2019. Unreproducible research is reproducible. In In-
ternational Conference on Machine Learning , pages
725–734. PMLR.
Ting-Yun Chang and Robin Jia. 2023. Data curation
alone can stabilize in-context learning. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 8123–8144, Toronto, Canada. Association for
Computational Linguistics.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
Alexandru Cioba, Michael Bromberg, Qian Wang,
Ritwik Niyogi, Georgios Batzolis, Jezabel Garcia,
Da-shan Shiu, and Alberto Bernacchia. 2022. How
to Distribute Data across Tasks for Meta-Learning?
Proceedings of the AAAI Conference on Artificial
Intelligence , 36(6):6394–6401. Number: 6.
Alice Coucke, Alaa Saade, Adrien Ball, Théodore
Bluche, Alexandre Caulier, David Leroy, Clément
Doumouro, Thibault Gisselbrecht, Francesco Calta-
girone, Thibaut Lavril, et al. 2018. Snips voice plat-
form: an embedded spoken language understanding
system for private-by-design voice interfaces. arXiv
preprint arXiv:1805.10190 .Verna Dankers and Ivan Titov. 2022. Recursive
neural networks with bottlenecks diagnose (non-
)compositionality. In Findings of the Association
for Computational Linguistics: EMNLP 2022 , pages
4361–4378, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali
Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.
Fine-Tuning Pretrained Language Models: Weight
Initializations, Data Orders, and Early Stopping.
ArXiv:2002.06305 [cs].
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
InProceedings of the Third International Workshop
on Paraphrasing (IWP2005) .
Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. In International conference on ma-
chine learning , pages 1126–1135. PMLR.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816–3830, Online. Association for Computa-
tional Linguistics.
Odd Erik Gundersen, Kevin Coakley, and Christine
Kirkpatrick. 2022. Sources of irreproducibility
in machine learning: A review. arXiv preprint
arXiv:2204.07610 .
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Abdullatif Köksal, Timo Schick, and Hinrich Schuetze.
2023. MEAL: Stable and active learning for few-shot
prompting. In Findings of the Association for Compu-
tational Linguistics: EMNLP 2023 , pages 506–517,
Singapore. Association for Computational Linguis-
tics.
Alexandre Lacoste, Alexandra Luccioni, Victor
Schmidt, and Thomas Dandres. 2019. Quantifying
the carbon emissions of machine learning. arXiv
preprint arXiv:1910.09700 .
11Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
Dimitris Kontokostas, Pablo N Mendes, Sebastian
Hellmann, Mohamed Morsey, Patrick van Kleef,
Sören Auer, and Christian Bizer. 2015. DBpedia
– a large-scale, multilingual knowledge base extracted
from wikipedia. Semant. Web , 6(2):167–195.
Changmao Li and Jeffrey Flanigan. 2023. Task con-
tamination: Language models may not be few-shot
anymore. arXiv preprint arXiv:2312.16337 .
Xiaonan Li and Xipeng Qiu. 2023. Finding support
examples for in-context learning. In Findings of the
Association for Computational Linguistics: EMNLP
2023 , pages 6219–6235, Singapore. Association for
Computational Linguistics.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022. What
makes good in-context examples for GPT-3? In
Proceedings of Deep Learning Inside Out (DeeLIO
2022): The 3rd Workshop on Knowledge Extrac-
tion and Integration for Deep Learning Architectures ,
pages 100–114, Dublin, Ireland and Online. Associa-
tion for Computational Linguistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Computing Surveys , 55(9):1–35.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2022. Fantastically ordered
prompts and where to find them: Overcoming few-
shot prompt order sensitivity. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
8086–8098, Dublin, Ireland. Association for Compu-
tational Linguistics.
Costas Mavromatis, Balasubramaniam Srinivasan,
Zhengyuan Shen, Jiani Zhang, Huzefa Rangwala,
Christos Faloutsos, and George Karypis. 2023.
Which examples to annotate for in-context learn-
ing? towards effective and efficient selection. arXiv
preprint arXiv:2310.20046 .
R. Thomas McCoy, Junghyun Min, and Tal Linzen.
2020. BERTs of a feather do not generalize together:
Large variability in generalization across models with
similar test set performance. In Proceedings of the
Third BlackboxNLP Workshop on Analyzing and In-
terpreting Neural Networks for NLP , pages 217–227,
Online. Association for Computational Linguistics.
Marius Mosbach, Maksym Andriushchenko, and Diet-
rich Klakow. 2021. On the Stability of Fine-tuning
BERT: Misconceptions, Explanations, and Strong
Baselines. In International Conference on Learning
Representations .Tai Nguyen and Eric Wong. 2023. In-context ex-
ample selection with influences. arXiv preprint
arXiv:2302.11042 .
Alex Nichol, Joshua Achiam, and John Schulman.
2018. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999 .
Branislav Pecher, Jan Cegin, Robert Belanec, Jakub
Simko, Ivan Srba, and Maria Bielikova. 2024a. Fight-
ing randomness with randomness: Mitigating op-
timisation instability of fine-tuning using delayed
ensemble and noisy interpolation. arXiv preprint
arXiv:2406.12471 .
Branislav Pecher, Ivan Srba, and Maria Bielikova.
2024b. A survey on stability of learning with limited
labelled data and its sensitivity to the effects of ran-
domness. ACM Computing Surveys . Just Accepted.
Branislav Pecher, Ivan Srba, Maria Bielikova, and
Joaquin Vanschoren. 2024c. Automatic combination
of sample selection strategies for few-shot learning.
arXiv preprint arXiv:2402.03038 .
Pouya Pezeshkpour and Estevam Hruschka. 2023.
Large language models sensitivity to the order of
options in multiple-choice questions. arXiv preprint
arXiv:2308.11483 .
Hung Viet Pham, Shangshu Qian, Jiannan Wang,
Thibaud Lutellier, Jonathan Rosenthal, Lin Tan, Yao-
liang Yu, and Nachiappan Nagappan. 2021. Prob-
lems and opportunities in training deep learning soft-
ware systems: an analysis of variance. In Proceed-
ings of the 35th IEEE/ACM International Conference
on Automated Software Engineering , ASE ’20, pages
771–783, New York, NY , USA. Association for Com-
puting Machinery.
Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of LSTM-networks for sequence tagging. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing , pages 338–
348, Copenhagen, Denmark. Association for Compu-
tational Linguistics.
Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane
Suhr. 2023. Quantifying language models’ sensitiv-
ity to spurious features in prompt design or: How i
learned to start worrying about prompt formatting.
arXiv preprint arXiv:2310.11324 .
Thibault Sellam, Steve Yadlowsky, Ian Tenney, Jason
Wei, Naomi Saphra, Alexander D’Amour, Tal Linzen,
Jasmijn Bastings, Iulia Turc, Jacob Eisenstein, Dipan-
jan Das, and Ellie Pavlick. 2022. The MultiBERTs:
BERT Reproductions for Robustness Analysis. In In-
ternational Conference on Learning Representations ,
page 30.
Amrith Setlur, Oscar Li, and Virginia Smith. 2021. Is
Support Set Diversity Necessary for Meta-Learning?
ArXiv:2011.14048 [cs, stat].
12Jake Snell, Kevin Swersky, and Richard Zemel. 2017.
Prototypical networks for few-shot learning. In Pro-
ceedings of the 31st International Conference on Neu-
ral Information Processing Systems , NIPS’17, page
4080–4090, Red Hook, NY , USA. Curran Associates
Inc.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.
Cecilia Summers and Michael J. Dinneen. 2021. Non-
determinism and Instability in Neural Network Op-
timization. In Proceedings of the 38th International
Conference on Machine Learning , pages 9913–9922.
PMLR. ISSN: 2640-3498.
Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan,
Shuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng,
Lingjuan Lyu, Fei Wu, et al. 2023. Pushing the
limits of chatgpt on nlp tasks. arXiv preprint
arXiv:2306.09719 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-
rect distillation of lm alignment. arXiv preprint
arXiv:2310.16944 .
Ellen M. V oorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceedings
of the 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval , SIGIR ’00, page 200–207, New York, NY ,
USA. Association for Computing Machinery.
Anton V oronov, Lena Wolf, and Max Ryabinin. 2024.
Mind your format: Towards consistent evaluation of
in-context learning improvements. arXiv preprint
arXiv:2401.06766 .
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics.
Lijing Wang, Yingya Li, Timothy Miller, Steven
Bethard, and Guergana Savova. 2023. Two-stage
fine-tuning for improved bias and variance for largepretrained language models. In Proceedings of the
61st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers) ,
pages 15746–15761, Toronto, Canada. Association
for Computational Linguistics.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics , 7:625–641.
Lucas Weber, Elia Bruni, and Dieuwke Hupkes. 2023.
Mind the instructions: a holistic evaluation of con-
sistency and interactions in prompt-based learning.
InProceedings of the 27th Conference on Computa-
tional Natural Language Learning (CoNLL) , pages
294–313, Singapore. Association for Computational
Linguistics.
Albert Webson and Ellie Pavlick. 2022. Do prompt-
based models really understand the meaning of their
prompts? In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2300–2344, Seattle, United States.
Association for Computational Linguistics.
Sheng-Lun Wei, Cheng-Kuang Wu, Hen-Hsen Huang,
and Hsin-Hsi Chen. 2024. Unveiling selection bi-
ases: Exploring order and token sensitivity in large
language models. arXiv preprint arXiv:2406.03009 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019. Huggingface’s transformers: State-of-
the-art natural language processing. arXiv preprint
arXiv:1910.03771 .
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.
CrossFit: A few-shot learning challenge for cross-
task generalization in NLP. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7163–7189, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Pengwei Zhan, Zhen Xu, Qian Tan, Jie Song, and
Ru Xie. 2024. Unveiling the lexical sensitivity of
llms: Combinatorial optimization for prompt en-
hancement. arXiv preprint arXiv:2405.20701 .
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems , volume 28. Curran Associates, Inc.
Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-
tive example selection for in-context learning. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing , pages 9134–
9148, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vuli ´c,
Roi Reichart, Anna Korhonen, and Hinrich Schütze.
132021a. A closer look at few-shot crosslingual trans-
fer: The choice of shots matters. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 5751–5767, Online.
Association for Computational Linguistics.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021b. Calibrate Before Use: Im-
proving Few-shot Performance of Language Models.
InProceedings of the 38th International Conference
on Machine Learning , pages 12697–12706. PMLR.
ISSN: 2640-3498.
Ruiqi Zhong, Dhruba Ghosh, Dan Klein, and Jacob
Steinhardt. 2021. Are larger pretrained language
models uniformly better? comparing performance
at the instance level. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 3813–3827, Online. Association for Computa-
tional Linguistics.
Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika
Chavhan, and Timothy Hospedales. 2023. Fool
your (vision and) language model with embar-
rassingly simple permutations. arXiv preprint
arXiv:2310.01651 .
A Ethical Considerations and Impact
Statement
The experiments in this paper work with publicly
available benchmark dataset GLUE, and publicly
available datasets AG News, TREC, SNIPS and
DB-Pedia, citing the original authors. As we were
not able to determine the license for the tasks and
datasets used, we opted to use them in as limited
form as possible, adhering to the terms of use (no
annotation of the test set) for the GLUE benchmark
dataset and applying it to other datasets as well. We
do not work with any personally identifiable infor-
mation or offensive content and perform no crowd-
sourcing for further data annotation. In addition,
we are not aware of any potential ethical harms or
negative societal impacts of our work, apart from
the ones related to the advancement of the field
of Machine Learning and Learning with Limited
Labelled Data, which includes the in-context learn-
ing, transfer learning, meta-learning and language
model subsets. Finally, we follow the license terms
for all the models we use (such as the one required
for the use of the LLaMA-2 model) – all models
and datasets allow their use as part of research. It is
possible the large language models we used (Flan-
T5, LLaMA-2, Mistral and Zephyr) contain biases
and may generate potentially offensive or harmful
content. However, the authors of the models reducethis potential bias as much as possible when train-
ing the models, while at the same time we limit
the output to few tokens and do not release any
output of the models, which should further reduce
the potential bias and negative impact.
Impact Statement: CO2 Emissions Related to
Experiments The experiments presented in this
paper used significant compute resources as they
required multiple training and evaluation runs of
multiple models, as well as using large language
models that require a lot of computation even just
for the inference. Overall, the experiments were
conducted using a private infrastructure, which has
a carbon efficiency of 0.432 kgCO 2eq/kWh (de-
fault value used as the actual efficiency of our HW
instance was not measured). A cumulative of 1440
hours of computation was performed on hardware
of type RTX 3090 (TDP of 350W) and a cumulative
of 4000 hours of computation was performed on
hardware of type A100 PCIe 40GB (TDP of 250W).
The hours of computation used are only a crude
approximation, as the machine used was shared
among multiple projects. Total emissions are es-
timated to be 217.73 kgCO 2eq (for the first set of
hardware) and 432 kgCO 2eq (for the second set of
hardware), of which 0 percents were directly offset.
These estimations were conducted using the Ma-
chineLearning Impact calculator presented in (La-
coste et al., 2019). Whenever possible, we tried to
reduce the compute resources used as much as pos-
sible. The most compute resources were used by
the large language model – LLaMA-2, Mistral-7B
and Zephyr-7B. To reduce the computation costs
and resources used, we decided to evaluate the
model on lower number of runs (10 investigation
and 20 mitigation, resulting in 200 runs for each
randomness factors) using only 1 000 test samples
for evaluation. Even in this reduced evaluation, the
experiments using these models used the most GPU
hours. To further reduce the compute resources, we
use the 4-bit quantised versions of these models,
while also opting to use smaller models for the
more detailed analyses and ablation studies, either
in case of in-context learning (e.g., using Flan-T5
and Mistral-7B instead of LLaMA-2 for studying
the impact of number of shots that significantly
increased the required computation resources and
inference time), but also in case of transfer learning
and meta-learning (e.g., using base versions of the
BERT and RoBERTa models).
14B Additional Resources Describing the
Proposed Investigation Method
In this Appendix, we provide additional supplemen-
tary resources that should allow for easier under-
standing of how the method operates. The proposed
investigation method is designed for investigating
effects of any randomness factor , while explicitly
taking interactions with effects of other factors into
consideration, and measuring the importance of the
found effects. Overall, the effects are investigated
by observing how the performance changes across
the different states the investigated randomness fac-
tors can appear in. To deal with the interactions,
the effects of remaining randomness factors are
mitigated (i.e., the deviation they contribute is re-
duced as much as possible). To determine the im-
portance, we compare the contributed deviation of
the investigated randomness factor with effects of
other factors, and with the overall deviation from a
golden model. The golden model represents the ob-
jective estimate of the performance deviation and
is obtained by training a model while mitigating all
the randomness factors at the same time. The final
importance score is then determined as the frac-
tion of the overall deviation (represented using the
golden model) the investigated factor contributes
over all the remaining, non-investigated factors.
The following section provides a high-level
overview of the method with references to the Al-
gorithm 1 (Appendix B.1), the illustration of how
the method operates and the results it computes
(Appendix B.2). We also provide a method for se-
lecting the number of investigation and mitigation
runs in Appendix B.3 (this method was used to
select the samples in this paper using the Ablation
Study in Appendix 3 and to produce the heuristics
at the end of Section 3).
B.1 Algorithmic Description of the Method
To allow for better understanding of our proposed
investigation method, we provide more informal
description of the steps composed in Algorithm 1,
along with references to individual lines in it and
possible avenues for extension of our method. In-
formally our proposed method works in a following
way:
1.A set of randomness factors for investigation
is first identified along with their configura-
tions. In case of mitigated randomness factors,
a complete set of factors and their configura-
tions is not required to prevent introduction ofbiases into the results, as the randomness fac-
tors can be controlled on the group level. All
the algorithmic factors (order, initialisation,
model randomness, etc.) can be controlled by
globally setting the seed, while the implemen-
tation/hardware level factors can be controlled
using the same setup across all experiments
(same library versions, architectures, GPUs,
etc.).
2.A single investigation run is performed for a
selected investigated randomness factor (re-
peating and evaluating training multiple times,
each time with different configuration of the
selected randomness factor, e.g., with differ-
ent split of data, choice of data, or their order),
while keeping the configuration of all other
(non-investigated) randomness factors fixed.
(inner loop; lines 5-7 in the Algorithm 1)
3.The method can be easily extended to investi-
gate effect of multiple factors at the same time,
by simply changing the definition of the in-
vestigated factor configuration set, to include
a cross product between the configurations
of multiple factors (similarly to the mitigated
factor configuration set). (lines 2 and 3 in the
Algorithm 1)
4.The single investigation run is evaluated to
obtain partial standard deviation and partial
mean. (lines 8-9 in the Algorithm 1)
5.The configuration of all other randomness fac-
tors is fixed to a new value and the investi-
gation run is repeated again to mitigate the
effects of non-investigated randomness fac-
tors (each such repeat is called mitigation run ).
(outer loop; lines 4-10 in the Algorithm 1)
6.Instead of repeating multiple mitigation runs,
the method can be extended to use a specific
mitigation strategy (such as sample selection
method for in-context learning). Using such
method, the set of configurations for the given
randomness factor is simply replaced by the
results of the mitigation strategy (either a set
of single value or a subset that is significantly
smaller). The rest of the method remains un-
changed. (line 3 in the Algorithm 1)
7.After enough configurations of non-
investigated randomness factors (i.e.,
mitigation runs ) are searched through and
15enough runs of training and evaluation are
performed, the partial standard deviations
are averaged to produce the contributed
standard deviation, and the partial means
are aggregated (by taking their standard
deviation) to produce the mitigated standard
deviation. (lines 11-12 in the Algorithm 1)
8.The golden model standard deviation is calcu-
lated by simply performing training and evalu-
ation multiple times with differently fixed con-
figuration of all randomness factors. If enough
overall runs are used, the golden model stan-
dard deviation can be replaced by simply tak-
ing the standard deviation over all the runs in
the investigation. However, this may lead to
incorrect results. (final loop; lines 13-17 in
the Algorithm 1)
9.The importance score of the investigated fac-
tor is calculated as a fraction of the golden
model standard deviation of the difference
between contributed standard deviation and
the mitigated standard deviation (to determine
how much more the investigated factor con-
tributes over all the mitigated ones). Any ran-
domness factor with importance score over 0
is considered significantly important, as such
factors contribute the same amount of devia-
tion as the combination of all the remaining
factors. At the same time, the size of the im-
portance value determines the overall impor-
tance of the model (i.e., factor with impor-
tance score of 0.6is more important than the
ones with score of 0.1). (the final check; lines
18-21 in the Algorithm 1)
B.2 Illustration of the Method and its Results
In this section, we provide the visualisation of the
method in a form of table. In essence, when in-
vestigating the specific randomness factor, while
mitigating the effects of other randomness factors,
we fill in such table as illustrated in Table 2. The
columns represent the different configurations for
the investigated factor. Observing how the per-
formance changes across these columns, we can
determine the effects of the randomness factors –
aggregating across these columns we obtain the par-
tial mean p_mean and partial standard deviation
p_std.
However, having only a single row would not
deal with the interactions. Therefore we perform
this investigation multiple times, each time withdifferent randomly fixed combination of configura-
tions for all the other, non-investigated randomness
factors. Each such repeat of the investigation run
represents a single row in the table, each with its
own partial mean p_mean mand partial standard
deviation p_stdm.
To get the final contributed standard deviation
c_stdfor the investigated randomness factor, we
aggregate over these different partial standard devi-
ations ( c_std=p_std∗). In addition, to obtain the
mitigated standard deviation m_stdwe aggregate
over the partial means ( m_std=std(p_mean ∗)).
B.3 Selecting Number of Investigation and
Mitigation Runs
When selecting the number of investigation ( N)
and the number of mitigation ( M) runs, we need
to find a balance between how well the effects of
the factors are estimated and how well the interac-
tion between the effects of different randomness
factors are mitigated, and how much computational
resources are required to get to this estimation and
mitigation. An optimal solution is to use the lowest
number of overall runs (that lead to lowest compu-
tational resources) after which the change in the re-
sults (the contributed/mitigated standard deviation
or the normalised importance score) is under an
acceptable threshold ϵ. The value of this threshold
ϵdepends on the setup of the experiment and the
goal of our investigation, as in some cases higher
change in the standard deviation may be acceptable,
while in others we require a more strict setting.
In this section, we describe a simple method to
search for this optimal point that can be used in-
stead of the heuristics at the end of Section 3 (which
were a result of our analysis using the following
method). The method is composed of following
steps:
1.The threshold of smallest acceptable change ϵ,
and the starting number of investigation runs
Nare selected. The number of investigation
runs should be sufficiently high from the start
(following recommendations in Section 3) to
make the search faster.
2.A new mitigation run should be performed
using a randomly selected configuration of
the non-investigated randomness (or the num-
ber of investigation runs should be increased,
running the new investigation runs for all the
already performed mitigation runs).
16IFCi
n1 n2 ... nN−1 nN
m1 r1,1 r1,2 ... r1,N−1 r1,N p_mean 1 p_std1
m2 r2,1 r2,2 ... r2,N−1 r2,N p_mean 2 p_std2
MFC i ... ... ... ... ... ... ... ...
mM−1rM−1,1rM−1,2...rM−1,N−1rM−1,N p_mean M−1p_stdM−1
mM rM,1 rM,2 ... rM,N−1 rM,N p_mean M p_stdM
m_std= c_std=
std(p_mean ∗) p_std∗
Table 2: The effects of a randomness factor iare determined by observing the variability in results over its
configurations , while mitigating the effects of other randomness factors . The results are first grouped by the
mitigated factor configurations mand a partial mean ( p_meanm) and standard deviation ( p_stdm) is calculated.
These values are then aggregated into contributed standard deviation (c_std), representing the effects of investigated
randomness factor , by calculating a mean over the p_stdm, and into mitigated standard deviation (m_std),
representing the remaining effects of mitigated randomness factors , by calculating a standard deviation over
p_meanm.
3.The new values of the relevant metrics (con-
tributed standard deviation, mitigated standard
deviation, or the importance score) should be
determined and the difference to previous val-
ues calculated.
4.If the observed change is lower than the thresh-
oldϵthe current values of hyperparameters N
andMrepresent the optimal point and should
be used. Otherwise, continue to step 2 (in-
creasing either the value of NorM).
In case our goal is to use the results of the investi-
gation and the importance score for a more in-depth
analysis and comparison across different factors,
models, datasets or other experimental settings, the
method should be repeated for every setting and
the highest values of NandMshould be used –
to guarantee that the comparison and analysis is
done on the same number of overall runs and to not
introduce any possible biases into the comparison.
C Ablation Study: Reducing Number of
Mitigation Runs and Test Data Size
As mentioned in Section 3, there is a trade-off be-
tween feasibility (computations costs) of the inves-
tigation and precision (reliability) of the investiga-
tion results. This trade-off mainly depends on the
number of mitigation runs (i.e., the number of con-
figurations explored for the non-investigated ran-
domness factors). To determine the optimal num-
ber of mitigation runs, we explore this trade-off
using a modified version of the method described
in Appendix B.3: we run the investigation for a
larger number of mitigation runs (observing the
behaviour even after the optimal point) and explorethe effects of reducing the number of mitigation
runs ( M)and the number of test samples used
for evaluation on the results and how well they es-
timate the overall contributed effects and how well
the interactions are mitigated. We perform this ab-
lation study for the Flan-T5 model on the SST2
dataset and report only specific interesting points.
As the baseline for this ablation study we work
with the setting of using 100 mitigation runs (with
10 investigation runs) and 100% of test samples.
For the number of mitigation runs, we explore: 1)
increasing the number significantly (to 500); 2) re-
ducing the number to 10% (10 mitigation runs).
For the number of test samples, we explore reduc-
ing the set to: 1) 1 000 samples (which represents
approximately 10% of overall test samples); and
2) 500 samples (representing approximately 5% of
overall test samples). We also explore the combi-
nation of both reductions (in relevant cases). The
results of this ablation study are available in Ta-
ble 3.
Compared to our baseline setting for the exper-
iments (100 mitigation runs, with 100% of test
samples used), increasing the number of mitigation
runs by 500% does not lead to a significantly more
reliable results. We can observe a slight change in
overall standard deviation in the model (ranging
from a change of 0.01to change of 0.21). Simi-
larly, the observed contributed standard deviation,
as well as the mitigated standard deviation stays
approximately the same (the change ranging from
0.005to0.1). In addition, the change in importance
score is negligible for the different factors. All in
all, we can conclude that increasing the number of
mitigation runs any further does not make sense in
17MITIGATION RUNS 500 100 10 100 10 10
TESTDATASET SIZE 100% 100% 100% ∼10% ∼10% ∼5%
%OFBASELINE SETTING DATA 500% 100% 10% ∼10% ∼1% ∼0.5%
GOLDEN F1 macro (%) 78.23 78.17 78.25 78.18 78.13 78.16
MODEL F1 std 2.31 2.24 2.09 2.50 2.35 2.97
LABEL F1 macro (%) 78.26 78.14 78.17 78.07 77.87 77.72
SELECTION F1 std 2.28 2.41 2.44 2.61 2.94 3.20
Contributed std 2.073 2.167 2.135 2.204 2.174 2.188
Mitigated std 0.797 0.904 0.946 1.278 1.806 2.193
Importance 0.55 0.56 0.57 0.37 0.16 -0.00
DATA F1 macro (%) 78.18 78.24 77.98 78.39 78.22 78.33
SPLIT F1 std 2.29 2.30 2.30 2.55 2.59 2.85
Contributed std 2.112 2.128 2.138 2.372 2.422 2.670
Mitigated std 0.712 0.693 0.662 0.708 0.729 0.788
Importance 0.61 0.64 0.71 0.67 0.72 0.63
DATA F1 macro (%) 78.14 78.28 77.29 78.22 77.10 76.82
ORDER F1 std 2.28 2.15 2.59 2.34 3.18 3.25
Contributed std 0.846 0.869 0.982 0.902 1.095 1.149
Mitigated std 2.089 1.928 2.334 2.117 2.932 2.988
Importance -0.54 -0.47 -0.65 -0.49 -0.78 -0.62
SAMPLE F1 macro (%) 78.22 78.19 78.15 78.14 77.92 77.80
CHOICE F1 std 2.14 2.35 2.64 2.55 2.87 3.05
Contributed std 2.138 2.123 2.361 2.152 2.337 2.325
Mitigated std 0.818 0.844 1.001 1.248 1.553 1.906
Importance 0.57 0.57 0.65 0.36 0.33 0.14
Table 3: The effects of changing the number of mitigation runs and the number of samples used for evaluation on
the results of our proposed investigation method when applied to Flan-T5 model used with SST-2 dataset. The
column with 100 mitigation runs and 100% test data represents our baseline setting. With decreasing number of
mitigation runs and the size of test data used, the mitigated standard deviation , as well as the overall standard
deviation increases, while the contributed standard deviation stays approximately the same. This leads to lower
precision of the results and change in the importance score of the different factors, and even can lead to incorrect
results in extreme cases (Label Selection not being considered important when using ∼0.5% of data as compared to
our baseline setting). Even with ∼1% of computation (combination of mitigation runs and test sample reduction)
the findings can be considered sufficiently reliable in this setting.
regards to the reliability-feasibility trade-off.
On the other hand, reducing the number of mit-
igation runs and the number of test samples used
for evaluation, we can observe more significant
changes in the overall variance in the model and
the importance score of the factors. We can ob-
serve a progressive increase in the overall golden
model standard deviation (from 2.24up to 2.97in
the most extreme setting). At the same time, we
also observe significant increase in the mitigated
standard deviation (going from as low as 0.904in
the Label Selection randomness factor up to 2.193
for the same factor in the most extreme setting),
which can be expected as the number of mitigation
runs governs the mitigation of non-investigated,
interacting randomness factors in our method. Sim-
ilarly, we can observe change in the importance
score as well, with the importance of different fac-
tors being lower with lower number of mitigation
runs (with the exception of Data Split random-ness factor). In the most extreme setting (using
10 mitigation runs and 500 test samples, which rep-
resents ∼0.5% of baselines setting data) we can
even observe a change in the findings regarding the
Label Selection randomness factor – it becomes
non-important as it is overshadowed by the mit-
igated randomness factors, with the importance
score being slightly below the 0value. However,
the less extreme setting, where ∼1% of the base-
line setting data is used (10 mitigation runs and
1000 samples), the results are still reliable enough
(even though the importance score is lower in this
case). In addition, the difference in importance
score when using smaller amount of test samples is
more significant than when using smaller number
mitigation runs (i.e., 0.36and0.33importance with
10% test data while using 100 and 10 mitigation
runs respectively, as compared to 0.57and 0.65
when using full test data and 100 and 10 mitigation
runs respectively). All in all, we can conclude that
18our proposed method is not as dependent on the
number of mitigation runs and not as computation-
ally expensive as can be expected, making it more
easily usable on more computationally expensive
settings (e.g., having large labelled datasets or us-
ing more computationally expensive models). At
the same time, the importance score is dependent
on the number of test samples used for evaluation,
which needs to be taken into consideration when us-
ing it on setting such as in-context learning, where
the inference itself is quite expensive.
Even when reducing the computation cost of
the proposed method to ∼1% of our baseline set-
ting (reducing the number of mitigation runs to
10 and using only 1 000 test samples for evalu-
ation) the findings can be considered sufficiently
reliable. Therefore, if the precision of the results is
not as paramount, the proposed method can be used
even in this reduced setting (although one needs
to be aware of the implications). To produce more
precise results, and due to the significant computa-
tion costs of running the larger in-context learning
models (LLaMA-2, Mistral and Zephyr), we have
decided to run the investigation using 20 mitigation
runs and 1 000 test samples (following the practice
in related work (Gao et al., 2021; Chang and Jia,
2023; Sclar et al., 2023; Li and Qiu, 2023; Köksal
et al., 2023)). As such, the observed importance
scores for different factors may be affected by this
choice, but the findings regarding the importance
should still hold. In addition, to keep the compar-
ison between models as unbiased as possible, we
use the same amount of test data for all the models
and all the datasets and across all experiments.
Based on the observed behaviour, we can de-
termine which factor affects the variability of the
model results the most – Data Split. For all the
randomness factors, except for the Data Split, only
the mitigated standard deviation increases when
reducing the number of mitigation runs and/or the
number of samples, while the contributed standard
deviation stays approximately the same. However,
for the Data Split randomness factor, the exact op-
posite happens (contributed std increases, while
mitigated std stays the same). In essence, having
more mitigation runs and/or using more test sam-
ples for evaluation leads to a significant mitigation
of the variance from the data split randomness fac-
tor.D Experimental Setup and
Implementation Details
All the experiments in this paper are using En-
glish only datasets from the GLUE (Wang et al.,
2018) benchmark suite and other publicly avail-
able datasets. The datasets from GLUE benchmark,
SST2 (Dankers and Titov, 2022), CoLA (Warstadt
et al., 2019) and MRPC (Dolan and Brockett,
2005), are all binary classification datasets us-
ing only 2 classes. The remaining datasets rep-
resent a multi-class classification problems, with
the AG News (Zhang et al., 2015) dataset consist-
ing of 4 classes, TREC (V oorhees and Tice, 2000)
dataset consisting of 6 classes, SNIPS (Coucke
et al., 2018) dataset consisting of 7 classes and DB-
Pedia (Lehmann et al., 2015) dataset consisting of
14 classes.
Based on the ablation study (included in Ap-
pendix 3), we use 10 investigation and 20 miti-
gation runs (resulting in overall 200 training and
evaluation runs) for the in-context learning (Flan-
T5, LLaMA-2, Mistral-7B and Zephyr-7B) and 100
mitigation runs (results in overall 1 000 training
and evaluation runs) for the other approaches that
use smaller models (BERT, RoBERTa). Following
the practice from the related work (e.g., (Gao et al.,
2021; Chang and Jia, 2023; Sclar et al., 2023; Li
and Qiu, 2023; Köksal et al., 2023)) and the results
of our ablation study, we evaluate each run using
only 1 000 test samples (the selection is governed
by the Label Selection randomness factor). The
main reason is the computation cost of the infer-
ence for the large language models. To prevent
introduction of any biases into the comparison, we
use the same amount of test samples for the transfer
learning and meta-learning as well (although we
use larger number of runs results in larger distribu-
tions in those cases). These decisions represents
the trade-off between feasibility/required computa-
tion costs to achieve the results and how well the
effects of randomness factors are estimated and the
interactions between them mitigated.
Besides the factors that we focus our investiga-
tion on (Label Selection, Data Split, Model Initial-
isation, Data Order and Sample Choice), we also
focus on mitigating other factors that we call as
Model Randomness . This group of factors en-
compasses the randomness originating from use
of non-deterministic operations in the model (e.g.,
dropout or sampling in the in-context learning mod-
els that generate text) and from implementation
19Dataset ID Verbaliser Prompt Format
SST-2 A {Negative, Positive} Determine sentiment of the sentence using following options: 1)
[Class 1] 2)[Class 2] .
[Input]
[Output]
B Same as above [Input] Sentiment? [Output]
C Same as above [Input] Sentiment is [Output]
D {terrible, great} [Input] It was [Output]
CoLA A {No, Yes} Determine grammatical acceptability of the sentence using fol-
lowing options: 1) [Class 1] 2)[Class 2] .
[Input]
[Output]
B Same as above [Input] Grammatically acceptable? [Output]
C {Yes, No} [Input] Grammar problems? [Output]
D {not acceptable, acceptable} [Input] It is[Output]
MRPC A {No, Yes} Determine whether the sentence pair is semantically equivalent
using following options: 1) [Class 1] 2)[Class 2] .
[Input]
[Output]
B Same as above [Input] Semantically equivalent sentences? [Output]
C {Yes, No} [Input] Semantically different sentences? [Output]
D {not equivalent, equivalent} [Input] Sentences are [Output]
AG News A {World, Sports, Business, Science and Tech-
nology}Determine topic of the sentence using following options: 1)
[Class 1] 2)[Class 2] ... N) [Class N] .
[Input]
[Output]
B Same as above [Input] Topic? [Output]
C Same as above [Input] Topic is [Output]
D Same as above User: [Input] This is about [Output]
TREC A {Expression, Entity, Description, Human, Lo-
cation, Number}Determine topic of the sentence using following options: 1)
[Class 1] 2)[Class 2] ... N) [Class N] .
[Input]
[Output]
B Same as above [Input] Topic? [Output]
C Same as above [Input] Topic is [Output]
D Same as above User: [Input] This is about [Output]
SNIPS A {Playlist, Weather, Event, Musing, Creative
Work, Rate Book, Book Restaurant}Determine intent of the sentence using following options: 1)
[Class 1] 2)[Class 2] ... N) [Class N] .
[Input]
[Output]
B Same as above [Input] Intent? [Output]
C Same as above [Input] Intent is [Output]
D Same as above User: [Input] User requested [Output]
DB-Pedia A {Company, Educational Institution, Artist,
Athlete, Office Holder, Transportation, Build-
ing, Natural Place, Village, Animal, Plant,
Album, Film, Written Work}Determine topic of the sentence using following options: 1)
[Class 1] 2)[Class 2] ... N) [Class N] .
[Input]
[Output]
B Same as above [Input] Topic? [Output]
C Same as above [Input] Topic is [Output]
D Same as above User: [Input] This is about [Output]
Table 4: Prompt formats and verbalisers used for different datasets in the paper. The [Class 1-N] are replaced with
the names of the classes as defined by the verbaliser. The [Input] is replaced by the sentence of the samples and the
[Output] is replaced with the name of class as defined by the verbaliser. The [Input] and[Output] are repeated for
each in-context sample, while the final [Output] is used to determine the predicted class. The same format is used
for all the language models (Flan-T5, LLaMA-2-13B, Mistral-7B and Zephyr-7B).
level factors (e.g., the impact of different libraries,
non-deterministic CUDA operations or using dif-
ferent GPU types). To mitigate these effects, we
set CUDA to deterministic, use the same libraryversions and the same GPUs throughout the experi-
ments (one exception are the meta-learning experi-
ments which were done on a separate GPU), while
also setting a specific random seed that governs the
20non-deterministic operations in the models during
training and inference (this seed is explored using
the mitigation runs, so each experiment explored
20 or 100 different sets of this non-determinism).
For the in-context learning models, we use the
Flan-T5 base model2, the LLaMA-2 13B instruc-
tion optimised model3, Mistral-7B instruct fine-
tuned model4and Zephyr-7B instruct fine-tuned
model5(alpha version as it worked better on the
classification tasks than the beta model, due to the
beta model generating large quantities of text and
multiple classes at the same time). The LLaMA-
2, Mistral and Zephyr models are all used in the
4-bit quantised setting. All of these models are set
to produce deterministic output, while the number
of tokens they can generate is limited to 10. In
the majority of the setting, we use 2 samples per
class, which are randomly sampled from the train
dataset. We use only 2 samples, as the Flan-T5
model falls apart and starts predicting a single class
for every test sample when using larger number of
samples. We perform only a basic prompt engi-
neering for these models (exploring also optimal
prompt formats from related research papers (Li
and Qiu, 2023; Gao et al., 2021; Köksal et al.,
2023), the prompt format recommended for the
LLaMA-2 model, and taking inspiration from (Sun
et al., 2023)), while also using the meta-tags that
specify instruction for the models. The optimal
prompt-format, as well as other formats used in
the analyses, is illustrated in Tabled 4. In case
the models produce multiple words that can be
mapped to multiple classes (with the exception of
specific prompts where some classes are subsets of
each other), we treat the output as incorrect with
the assumption the model is just hallucinating (al-
though we noticed the Mistral and Zephyr models
provide more detailed answers, especially on the
SST2 dataset, which may lower their performance
in this case).
For the fine-tuning models, BERT6and
RoBERTa7, we use the base version of the pre-
trained models from HuggingFace (Wolf et al.,
2https://huggingface.co/google/
flan-t5-base
3https://huggingface.co/meta-llama/
Llama-2-13b-chat-hf
4https://huggingface.co/mistralai/
Mistral-7B-Instruct-v0.1
5https://huggingface.co/HuggingFaceH4/
zephyr-7b-alpha
6https://huggingface.co/
bert-base-uncased
7https://huggingface.co/roberta-base2019). Both models are trained in full (without
freezing the pre-trained part) on all datasets using
learning rate of 1e-5 for 5 epochs on binary and 10
epochs on multi-class dataset, using early stopping,
AdamW optimiser with warmup for 10% of the
steps and batch size 8.
As the basis for the meta-learning approaches,
we use the implementation released by the authors
of the specific papers when possible, while the indi-
vidual implementations are extended and modified
to better work with our proposed method for inves-
tigation. In case of the Prototypical Networks, we
directly use the code released by the authors8. In
case of Model Agnostic Meta-Learning, we use the
implementation from the Torchmeta library9. In
case of Reptile, we use our own implementation
based on the code released for the approach10. For
meta-learning, we use the same base model across
all the meta-learning approaches. This model is a
simple fully-connected layer with 128 neurons and
a final classification layer on top of the BERT base
model. Each meta-learning approach is trained
in a 2-way 5-shot learning setup. For evaluation,
the meta-learning models are first adapted using
a single set of examples in 2-way 15-shot setting
(examples are chosen based on the sample choice
randomness factor) and then evaluated on the whole
test dataset.
All the hyperparameters for all the models are
set using a separate hyperparameter optimisation
for both fine-tuning and meta-learning (we run no
hyperparameter optimisation for in-context learn-
ing) using the validation data selected from the 1
000 training samples. This hyperparameter opti-
misation is done in a two-level fashion. First, the
optimisation is run using large differences in the
hyperparameter values, to find the approximate set
of hyperparameters that should provide good per-
formance on the given dataset. In the second step,
we explore the hyperparameter space around these
approximate hyperparameters, to find the optimal
set of parameters. However, it is important to note
that the hyperparameter search is performed on a
fixed set of labelled samples, chosen beforehand,
and on a single split, which may affect the opti-
mal set of hyperparameters and lead to sub-optimal
8https://github.com/jakesnell/
prototypical-networks
9https://github.com/tristandeleu/
pytorch-meta
10https://github.com/openai/
supervised-reptile
21hyperparameters, especially in meta-learning.
When choosing the hyperparameter values in the
first level, we draw inspiration from related work,
using the optimal parameters reported in papers that
propose, or use these approaches (such as (Dodge
et al., 2020; McCoy et al., 2020; Mosbach et al.,
2021; Sellam et al., 2022). However, we also search
through additional hyperparameter values besides
those reported in related works to better explore
the parameter space and obtain as precise results
from the investigation as possible.
E Validating the Proposed Investigation
Method
In this Appendix, we provide further information
on how the proposed investigation method was val-
idated. As there is no ground-truth of the effects
of randomness to compare against, the validity of
methods for investigating the effects of randomness
can be evaluated only indirectly. In this paper, we
perform such indirect evaluation/validation by:
1.Evaluating the properties and benefits of
the proposed methods by comparing it to
the existing ones. As discussed in the main
content of the paper, the benefits of the pro-
posed methods are: 1) importance score that
can be used for more in-depth analysis (rela-
tive ordering of randomness factors and com-
parison across models, datasets and other ex-
perimental settings), as opposed to determin-
ing the importance only in binary fashion from
previous works (factor is or is not important);
and 2) handling interactions between effects
of randomness factors, which, when previ-
ously ignored or not being addressed suffi-
ciently caused inconsistencies in findings. We
discuss this validation further in Appendix E.1
(which we consider as the main validation of
the method).
2.Exploring how the results and findings
change as we change the overall number of
runs . The results and findings of the method
are dependent on the choice of how many in-
vestigation and mitigation runs are used. We
observe a trade-off between how well the re-
sults are estimated (higher number of investi-
gation runs leads to better estimation) and the
interactions mitigation (higher number of miti-
gation runs leads to better mitigation), and the
computation costs required to achieve the re-
sults (increasing the number of runs increasesthe overall costs). We discuss this validation
further in Appendix E.2.
3.Applying the method to different settings
(factors, models, datasets) and observing
the consistency of its results and findings.
As the investigation method is designed to
be general, it should be applied across dif-
ferent experimental settings without showing
any problems (i.e., working out-of-the-box on
multiple factors, models and datasets). We dis-
cuss this validation further in Appendix E.3.
E.1 Additional Results: Validation of Method
Through Comparison with Typical
Investigation Strategies
To showcase the impact of interactions between
randomness factors on the investigation of the ef-
fects of different randomness factors, and to show-
case the properties and benefits of our proposed
method, we provide a comparison between the typ-
ical investigation strategies from related work and
our proposed method:
•Random – investigation strategy without any
constraints on the randomness factor configu-
rations . For each training and evaluation run
of the model, all the randomness factors are
varied, while only the impact of a specific fac-
tor is observed. For example, each training
and evaluation is done on a different set of
training and testing data, with different order
in training and with different random model
initialisation, regardless which randomness
factor is investigated. This represents the typ-
ical investigation process when considering
only the random seed randomness factor. This
investigation strategy does not consider any
impact of interactions between randomness
factors. As there is no change in how the indi-
vidual randomness factor is investigated, we
expect most skewed results from this investi-
gation strategy, with each randomness factors
showing approximately similar effects.
•Fixed – investigation strategy where the in-
teractions are addressed by fixing the non-
investigated randomness factors to a single
randomness factor configuration . For exam-
ple, each training and evaluation is done on
the same set of training and testing data, with
the data in the same order, but each time with
different random initialisation of the model.
22SST2 C OLA MRPC
FLAN-T5 R ANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS
GOLDEN MODEL 2.244 2.244 2.244 3.811 3.811 3.811 1.328 1.328 1.328
LABEL SELECT . (*) 2.517 (*) 2.594 (*) 2.128 (*) 3.602 (*) 2.804 (*) 3.257 (*) 1.122 (*) 1.189 0.363
DATA SPLIT (*) 2.362 (*) 2.480 (*) 2.167 (*) 3.961 (*) 1.990 (*) 3.483 (*) 1.503 0.252 (*) 0.926
DATA ORDER (*) 2.131 (*) 3.014 0.869 (*) 3.122 (*) 4.172 1.793 (*) 1.007 0.289 0.209
SAMPLE CHOICE (*) 2.370 (*) 3.191 (*) 2.123 (*) 3.478 1.203 (*) 3.138 (*) 1.277 (*) 0.678 0.348
ZEPHYR -7B R ANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS
GOLDEN MODEL 1.043 1.043 1.043 9.566 9.566 9.566 12.785 12.785 12.785
LABEL SELECT . (*) 1.122 (*) 1.004 (*) 0.863 (*) 7.367 2.529 (*) 5.806 (*) 11.968 (*) 11.977 5.109
DATA SPLIT (*) 1.185 0.402 (*) 0.664 (*) 8.235 (*) 8.001 (*) 7.675 (*) 12.504 (*) 6.660 5.973
DATA ORDER (*) 1.138 (*) 0.957 0.456 (*) 9.622 (*) 7.028 3.598 (*) 11.211 4.913 (*) 8.038
SAMPLE CHOICE (*) 1.052 0.406 (*) 0.744 (*) 10.069 4.379 (*) 9.135 (*) 12.980 (*) 12.239 6.305
BERT R ANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS
GOLDEN MODEL 0.970 0.970 0.970 1.473 1.473 1.473 2.929 2.929 2.929
LABEL SELECT . (*) 1.096 (*) 1.409 (*) 0.927 (*) 1.552 (*) 1.103 (*) 1.212 (*) 2.760 (*) 2.308 (*) 2.168
DATA SPLIT (*) 1.096 (*) 1.272 (*) 0.937 (*) 1.409 (*) 1.649 (*) 1.250 (*) 2.904 (*) 3.132 (*) 2.384
MODEL INIT. (*) 1.155 (*) 1.197 (*) 0.828 (*) 1.523 (*) 2.481 (*) 1.059 (*) 2.813 (*) 1.997 (*) 2.180
DATA ORDER (*) 1.082 (*) 1.217 (*) 0.852 (*) 1.639 (*) 1.333 1.086 (*) 2.809 (*) 3.971 (*) 2.081
PROTO NETS RANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS
GOLDEN MODEL 0.940 0.940 0.940 2.111 2.111 2.111 1.789 1.789 1.789
LABEL SELECT . (*) 0.987 (*) 0.857 (*) 0.887 (*) 2.109 (*) 1.497 (*) 1.924 (*) 1.572 0.451 (*) 1.448
DATA SPLIT (*) 1.012 (*) 1.041 (*) 0.959 (*) 2.188 (*) 2.301 (*) 2.010 (*) 1.919 (*) 1.006 (*) 1.791
MODEL INIT. (*) 0.892 (*) 0.845 0.658 (*) 2.222 (*) 1.582 (*) 1.801 (*) 1.888 0.610 1.240
DATA ORDER (*) 0.929 (*) 3.510 (*) 3.233 (*) 4.114 0.439 (*) 3.346 (*) 3.087 (*) 6.590 (*) 2.265
SAMPLE CHOICE (*) 0.983 (*) 0.832 0.646 (*) 2.163 0.890 (*) 1.659 (*) 1.805 (*) 1.271 1.084
AG N EWS TREC SNIPS
FLAN-T5 R ANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS
GOLDEN MODEL 3.090 3.090 3.090 1.324 1.324 1.324 2.284 2.284 2.284
LABEL SELECT . 0.980 0.391 0.556 (*) 1.502 (*) 1.210 0.683 (*) 3.081 (*) 2.969 1.581
DATA SPLIT (*) 6.152 0.594 (*) 3.777 (*) 1.247 (*) 1.844 0.892 (*) 2.855 (*) 2.740 1.602
DATA ORDER (*) 2.962 (*) 4.912 0.686 (*) 1.222 (*) 1.005 (*) 0.815 (*) 2.040 0.964 (*) 1.769
SAMPLE CHOICE (*) 1.982 1.466 (*) 0.806 (*) 1.616 (*) 1.344 0.819 (*) 3.156 0.970 1.590
ZEPHYR -7B R ANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS
GOLDEN MODEL 2.066 2.066 2.066 3.884 3.884 3.884 4.132 4.132 4.132
LABEL SELECT . (*) 1.966 1.008 (*) 1.460 (*) 3.196 (*) 3.988 (*) 2.963 (*) 2.687 (*) 2.812 (*) 2.935
DATA SPLIT (*) 2.141 (*) 2.452 (*) 1.817 (*) 3.554 (*) 2.115 (*) 3.221 (*) 4.006 (*) 3.580 (*) 3.052
DATA ORDER (*) 1.859 (*) 2.243 0.919 (*) 4.037 (*) 3.990 1.925 (*) 3.838 1.012 (*) 3.007
SAMPLE CHOICE (*) 2.358 0.884 (*) 1.874 (*) 4.021 (*) 4.696 (*) 3.279 (*) 3.975 1.311 (*) 3.331
BERT R ANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS RANDOM FIXED INTERACTIONS
GOLDEN MODEL 1.239 1.239 1.239 1.667 1.667 1.667 0.486 0.486 0.486
LABEL SELECT . (*) 1.202 (*) 0.923 (*) 0.979 (*) 1.600 (*) 1.513 (*) 1.348 (*) 0.559 (*) 0.405 (*) 0.308
DATA SPLIT (*) 1.462 (*) 1.365 (*) 1.164 (*) 1.502 (*) 1.513 (*) 1.568 (*) 0.401 (*) 0.426 (*) 0.294
MODEL INIT. (*) 1.142 (*) 1.047 0.693 (*) 1.926 (*) 1.108 0.939 (*) 0.479 (*) 0.635 0.121
DATA ORDER (*) 1.335 (*) 0.714 0.686 (*) 1.666 (*) 1.391 1.019 (*) 0.471 0.173 0.103
Table 5: Comparison of different investigation strategies for the Flan-T5, Zephyr-7B and BERT fine-tuning on the
binary datasets (SST2, CoLA and MRPC) and the multi-class datasets (AG News, TREC and SNIPS). Comparison
for the DB Pedia dataset is not included as Flan-T5 model shows poor performance on this particular dataset. The
‘Random‘ strategy simply repeats the training and evaluation multiple times without any constraints. In the ‘Fixed‘
strategy, the randomness factor configuration is kept fixed to a single state during investigation. We compare these
investigation strategies with our proposed method. We run each investigation strategy the same number of times
(number of runs is governed by our method). Our method (‘Interactions‘) takes the interactions into consideration.
Factors considered important for different strategies are denoted using the (*) symbol. We observe that interactions
between factors may cause some factors to have their importance overestimated (denoted in bold) or underestimated
(denoted in italics ).
However, as only a single randomness factor
configuration is used for the non-investigated
randomness factors, the effects of the investi-
gated randomness factor may still be affected
by the interactions (due to the randomly cho-
sen point in the randomness factor configu-ration state space). Therefore we expect the
results to represent the effects of different ran-
domness factors more accurately, but still can
under-estimate or over-estimate some effects
due to the still present randomness in the in-
vestigation.
23•Interactions (Our) – the investigation
method proposed in this paper. In essence
can be viewed as repeating the ‘Fixed‘ investi-
gation strategy multiple times, each time with
differently fixed randomness factor configura-
tions , and averaging over these repeats.
To prevent introduction of any biases into the
comparisons between the strategies, we perform
same number of training and evaluation runs for
each method. For each strategy, we repeat the train-
ing and evaluation 1 000 times (or 200 times, as
governed by the number of runs in our proposed
method). The full results are presented in Table 5
(except for DB Pedia dataset where the Flan-T5
model does not work well). We focus on 2 main
aspects in the comparison: 1) determining impor-
tance of the factors; 2) how interactions affect the
findings.
Determining importance of the factors. As the
Random andFixed strategies results only in a sin-
gle score (deviation in the results), we consider the
factor to be important when it contributes at least
50% of the golden model standard deviation. As
such, the importance of the randomness factors can
be determined only in a binary fashion (factor is
or is not important). Such setting allows only for a
limited analysis (only relative ordering of factors
based on the deviation withing the same setup) and
cannot be easily used to compare the importance
across different models. On the other hand, our pro-
posed method provides an importance score that
can be used for more in-depth analysis, such as
the relative ordering of randomness factors based
on their importance, or comparison across models,
datasets and experimental settings (as the impor-
tance score is normalised with the overall deviation
in the results from the golden model). This benefit
can be illustrated using following example – using
Table 5 and the Random /Fixed strategy, we cannot
say with good conscience that the Sample Choice
is more important for the Flan-T5 model than for
the Zephyr-7B model based only on their standard
deviation ( 2.370vs.1.052using Random ;3.191
vs.0.406using Fixed ) as the overall deviation in re-
sults is higher, but can be done so using our method
(importance score from Figure 1 or Table 6 and 9 of
0.57vs.0.39) as the score is normalised. Or simi-
larly for Data order ( 2.131vs.1.138forRandom ;
3.014vs.0.957forFixed ;−0.47vs.−0.44for our
method) -– using the importance score we see the
importance of Data Order is similar (slightly higherfor Zephyr-7B) for both models, while other inves-
tigation strategies show a large difference (higher
importance for Flan-T5).
Handling interactions. The existing strategies
either ignore the interactions completely ( Random )
or do not addresses the sufficiently (i.e., in a way
that strongly depends on randomness in the Fixed
strategy). As such, the baseline strategies often lead
to incorrect attribution of the effects of different
factors, either due to overestimating the impact of
non-important randomness factors, or underestimat-
ing the impact of important factors. For example, in
the case of Flan-T5 in-context learning, these inves-
tigation strategies indicate that all the randomness
factors are equally important (as they contribute
similar deviation to the golden model), which is
not the case when the interactions are taken into
consideration (when interactions are considered,
the impact of data order falls off). In case of the
Random strategy, this behaviour stems from the
strategy consistently leading to the same overall
deviation/importance for all the investigated ran-
domness factors (which is similar to the deviation
of the Golden Model). Even though using the Fixed
investigation strategy produces more reliable re-
sults (which are more distributed and handle the
interactions to a certain extent), it is still affected
by the randomness caused by the choice of the sin-
glerandomness factor configurations for the non-
investigated factors. The results still show both
overestimation and underestimation of effects for
therandomness factors . On the other hand, our
method is specifically designed to handle the in-
teractions using the mitigation runs. Handling the
interactions this way, we observe that the finding
that the long believed sensitivity of in-context learn-
ing to Data Order is actually a sensitivity to Sam-
ple Choice (and potentially the choice of prompt
format) when choosing samples in a more sophisti-
cated manner, holds even when choosing samples
at random.
All in all, our proposed method provides 2 sig-
nificant benefits over the baseline strategies, which
indirectly validates its use: 1) allowing for more
in-depth analysis and comparison across different
factors, models, datasets and experimental setups
that leads to actionable findings and read-to-apply
take-away messages and suggestions (described
in experimental results in Sections 4.2, 4.3 and
4.4, such as increasing the number of shots for
in-context learning reduces the importance of sam-
24ple choice, but does not affect the importance of
sample order); and 2) handling of interactions that
leads to more consistent results.
E.2 Additional Results: Validation of Method
by Exploring the Changes Due to
Different Number of Runs
The results and findings from investigation are
heavily dependent on the overall number of runs.
As opposed to the baseline strategies, our proposed
method introduces another parameter, number of
mitigation runs, to handle the interactions. We
provide results from exploring how changing the
number of investigation and mitigation runs affect
the results and findings (i.e., how well the effects
are estimated and the interactions mitigated) in Ap-
pendix B.3 and Appendix C, while in this section,
we provide a summary of relevant results.
The effects of randomness factors can be esti-
mated using a relatively low number of investiga-
tion runs (around 6to8). Increasing the number
of investigation runs further does not lead to con-
siderable changes in the estimated effects (the con-
tributed standard deviation changes only in second
decimal place).
On the other hand, increasing the number of miti-
gation runs has a larger impact on the overall results
and findings (and the different metrics we use), as
it represents the main avenue for mitigating the in-
teractions. Any change to the number of mitigation
runs changes all the metrics (contributed std, miti-
gated std, and the importance score). In addition,
the number of mitigation runs also depends on the
approach, model and dataset used. As such, it is
important to find the optimal point, where the inter-
actions are sufficiently handled without requiring
extensive computation costs. To find this optimal
point, we provide heuristics and a simple search
method in Appendix B.3. However, the overall
number of required mitigation runs is still relatively
low – in our experiments, we observed that using
20 mitigation runs provides sufficient mitigation of
interactions and estimation of the overall effects.
Finally, we observed that the number of test sam-
ples used for evaluation is the most important fac-
tor influencing the estimation of the effects. In
our experiments, we observed that using 1000 test
samples for evaluation provides a good trade-off
between the feasibility of larger scale experiments
(due to computation costs) and the validity of the
results.E.3 Additional Results: Validation of Method
by Observing Consistency of Results and
Findings Across Different Settings
The proposed investigation method is designed
to not be dependent on any specific experimental
setup, so that it can be used across any randomness
factors, model, dataset or other systematic changes
(e.g., number of samples, or prompt formats). To
validate this property of the method, we apply it
across various settings and observe how consistent
are the results and findings (the full results pre-
sented throughout the paper, such as in Tables 6-14,
or Figures 1, 2, 3, or in Appendix F):
•Different randomness factors that require dif-
ferent configuration setup for investigation
(e.g., different choice of data, order of sam-
ples, initialisation, etc.), but also different
setup for their mitigation. As discussed in
Appendix B.1, the mitigation can be done on
group level (effectively mitigating multiple
randomness factors at the same time), while
also allowing for further extensions (such as
using different mitigation strategies).
•Different approaches, namely in-context learn-
ing, fine-tuning and meta-learning, and differ-
ent models in these approaches. Although
each approach works differently (e.g., fine-
tuning using optimisation, while in-context
learning uses only inference with prompts),
the proposed method works with any such ap-
proach. The only limitation is that the models
and approaches used have an option to allow
for deterministic behaviour. Without this op-
tion, the method can still be applied but may
produce inconsistent and non-reproducible re-
sults and findings (i.e., the importance score in
such cases is affected by the non-determinism
of the model and so cannot be trusted fully).
In addition, we apply the proposed method to
models that lead to different performance and
show different overall deviation in the results.
In all the cases, the produced importance score
can be used for the analysis and comparison,
even in cases when the impact of the random-
ness factor is significant (e.g., Prototypical
Networks on the SST2 datasets with Data Or-
der randomness factor, where we observe a
significant drop in performance and increase
in overall deviation as opposed to the golden
model – in which case the method correctly
25identifies this factor to be significantly impor-
tant, leading to importance score of 0.92)
•Datasets with different characteristics and dif-
ferent experimental setups, such as different
number of classes, samples, different prompt
formats.
In all of these cases, the proposed method pro-
duces consistent results and findings without any
obvious shortcomings. Although the baseline
strategies can also be applied across all the set-
tings, they often lead to inconsistent results due to
the mishandling of interactions (i.e., Random con-
sistently leads to results similar to Golden Model
for all the randomness factors, while using Fixed
strategy the importance of different factors changes
quite often across different models, approaches,
datasets and experimental settings).
F Additional Results from Investigation
In this Appendix, we provide additional results
from the investigation experiments. This includes
the investigation of randomness factor importance
for the meta-learning approaches on the binary
datasets (Appendix F.1), the full results from in-
vestigating the impact of prompt format on the im-
portance across all datasets (Appendix F.2, and the
full results from the main investigation in a form
of tables in order to present the performance and
the deviation of different models and randomness
factors (Appendix F.3).
F.1 Additional Results: Meta-Learning
Randomness Factor Importance on
Binary Datasets
In this Appendix, we include the results of the
randomness factor importance investigation for the
meta-learning approaches on the binary datasets.
The results are presented in Figure 4.
For the majority of the approaches and the in-
vestigated datasets, the Data Order randomness
factor is the most important , with the factors
achieving importance score of 1.0 in some cases,
which represents the situation, when the factor con-
tributes all the deviation in the model. Even though
this importance is due to the factor actually leading
to significantly lower performance and significantly
higher overall deviation when set to only specific
subset, this only reinforces the finding that the Data
Order factor is the most important.In addition, we observe a consistent impor-
tance of the Data Split and Label Selection ran-
domness factors for the meta-learning approaches
across all the binary datasets. This follows the
findings of transfer learning, which also performs
optimisation/training and is not only composed of
inference (as is the case with in-context learning).
As such, we can conclude that the way the data is
split and which samples are considered labelled has
a significant impact on the approaches that require
training. One possible reason is that the different
splits and data labelling lead to different data dis-
tribution which severely affects the training.
Finally, the Model Initialisation and Sample
Choice (and task choice) randomness factors do
not show consistent importance across the meta-
learning approaches and the datasets. However,
the finding regarding Sample Choice may be due
to the binary setting and may be different when
using the meta-learning approaches in the true few-
shot setting (i.e., using them to adapt to previously
unseen classes and tasks).
F.2 Additional Results: Impact of Prompt
Format For All Datasets
This Appendix contains the full results from in-
vestigating the impact of the prompt format on the
effects of different randomness factors and their im-
portance. The results for the Flan-T5 and Mistral-
7B model across all the datasets are included in
Figure 5.
As already discussed in Section 4.4, the format
of the prompt used can have significant impact on
the importance of different randomness factors. Us-
ing the minimal formats, we observe significant
changes in the importance of different randomness
factors, with them being not considered signifi-
cantly important when using one format (e.g., Data
Order on SST2 dataset using format B) and at the
same time significantly important when using dif-
ferent format (e.g., Data Order on SST2 dataset
using format D).
In addition, the large language models are more
robust to this change of prompt format. This find-
ing is more evident on the multi-class datasets,
where in comparison to Flan-T5 model, the im-
portance score of the Mistral-7B remains more or
less constant, while the importance score of Flan-
T5 model oscillates significantly. On the binary
datasets, the larger model is not as robust, but
still the changes to the importance score are less
significant than in the Flan-T5 model. Analysing
26Figure 4: Importance of the investigated randomness factors for the meta-learning approaches on binary datasets,
while taking the interactions between factors into consideration. The legend indicates number of classes for each
datasets. We can observe consistent importance of the majority of the factors, with the exception of the Sample
Choice and Model Initialisation factors. At the same time, the Data Order randomness factors appears to be the
most important one for all the approaches.
the predictions further, we observe that the larger
model provides more in-depth answers on the bi-
nary datasets (e.g., not providing only an answer
but also an explanation for the answer, for exam-
ple generating "positive (in a negative context)"
instead of "positive", or often predicting neutral
sentiment on the SST2 dataset, which is considered
as incorrect answer), which may lead to the sig-
nificant changes in the importance of the different
randomness factors.
These findings only further highlights the im-
portance of prompt-tuning as the format has
significant impact on the words generated and
therefore also the assigned classes and the impor-
tance scores of the different randomness factors.
F.3 Additional Results: Investigation Results
in Table Form
This Appendix contains the full results from the
main investigation of the importance for the effects
of different randomness factors in this work (whichwere included as Figure 1), in a form of tables with
all the values included (performance, deviation,
contributed deviation, mitigated deviation and im-
portance for each investigated randomness factor).
We believe that including these results allows for
more in-depth analysis, exploration of the results
and its further extension. In addition to the results,
we provide a brief summary overview based on
these results, which main not necessarily be con-
nected only to the importance for different factors,
but instead to the overall stability of the models
and their ability to perform the different tasks.
The results are included as follows:
•Flan-T5 results for all datasets (with exception
of DB-Pedia) in Table 6
• LLaMA-2 results for all datasets in Table 7
• Mistral-7B results for all datasets in Table 8
• Zephyr-7B results for all datasets in Table 9
• BERT results for all datasets in Table 10
27Figure 5: Effect of different prompt formats on the importance of randomness factors for in-context learning. The
choice of format has significant effect on the importance of different factors, with the minimal formats often leading
to higher importance. At the same time, the larger, more optimised models, show lower sensitivity to prompt format.
• RoBERTa results for all datasets in Table 11
•Prototypical Networks results for all binary
datasets in Table 12
•MAML results for all binary datasets in Ta-ble 13
•Reptile results for all binary datasets in Ta-
ble 14
Based on these results, we can determine the
overall stability of the different models. Specifi-
28cally, we can observe the smaller in-context learn-
ing model (Flan-T5) shows better stability than
the larger ones (LLaMA-2, Mistral-7B and Zephyr-
7B), leading to significantly lower overall deviation
across majority of the datasets. At the same time,
we can observe that with increasing number of
predicted classes, the performance of the Flan-T5
model drops significantly (from 83.85F1 on AG
News dataset with 4 classes to 44.25on the SNIPS
dataset with 7 classes), while retaining its stability
(the overall deviation staying approximately the
same with 3.09on AG News and 2.28on SNIPS).
On the other hand, the larger language models
achieve similar performance, but different stability,
across the majority of the investigated datasets re-
gardless of the number of predicted classes. The
significant increase of performance and stability in
case of the DB-Pedia dataset and SNIPS dataset (to
a certain extent), may point to the fact that the mod-
els may have been trained on these datasets and so
the results and findings on them may be biased –
we discuss this as a limitation based on the recently
observed large language model validation crisis (Li
and Flanigan, 2023).
The fine-tuning approaches appear to be the most
stable and best performing approaches in our inves-
tigation, leading to F1 score as high as 98% and
overall deviation as low as 0.36. Surprisingly, the
performance on the multi-class datasets is higher
than on the binary datasets, which may indicate the
overall “hardness” of the different datasets we use
in this work, or point to specific problems in the
binary datasets (such as the single word sentences
without any sentiment in the SST2 dataset).
Finally, the meta-learning approaches appear to
be significantly dataset dependent, with the over-
all performance and the overall deviation chang-
ing significantly across different binary datasets.
One possibility for this is their significant sensi-
tivity to the setup of the hyperparameters, with
the performance and deviation changing signifi-
cantly with even small changes in the hyperparam-
eter setup, which we observed when trialling the
meta-learning models on the multi-class datasets
as well.
29FLAN-T5 SST2 C OLA MRPC AG N EWS TREC SNIPS
GOLDEN F1 Macro (%) 78.17 40 .71 70 .70 83 .85 61 .87 44 .25
MODEL F1 Std 2.24 3 .81 1 .32 3 .09 1 .32 2 .28
LABEL F1 Macro (%) 78.14 40 .65 70 .58 84 .31 61 .95 43 .80
SELECTION F1 Std 2.41 3 .70 1 .27 0 .92 1 .37 2 .95
Contributed Std 2.167 3 .257 0 .363 0 .556 0 .683 1 .581
Mitigated Std 0.904 1 .610 1 .210 0 .711 1 .147 2 .476
Importance 0.56 0 .43 −0.64 −0.05 −0.35−0.39
DATA F1 Macro (%) 78.24 41 .07 70 .78 83 .62 61 .73 44 .06
SPLIT F1 Std 2.30 3 .81 0 .94 5 .22 1 .36 2 .85
Contributed Std 2.128 3 .483 0 .926 3 .777 0 .892 1 .602
Mitigated Std 0.693 1 .344 0 .119 1 .717 0 .943 2 .244
Importance 0.64 0 .56 0 .61 0 .67 −0.04−0.28
DATA F1 Macro (%) 78.28 40 .61 70 .60 83 .96 62 .11 44 .48
ORDER F1 Std 2.15 3 .61 1 .27 1 .96 1 .14 2 .18
Contributed Std 0.869 1 .793 0 .209 0 .686 0 .815 1 .769
Mitigated Std 1.928 3 .044 1 .254 1 .115 0 .771 1 .197
Importance −0.47−0.33−0.79 −0.14 0 .03 0 .25
SAMPLE F1 Macro (%) 78.19 40 .68 70 .58 83 .87 61 .82 43 .77
CHOICE F1 Std 2.35 3 .66 1 .27 1 .91 1 .51 3 .33
Contributed Std 2.123 3 .138 0 .348 0 .806 0 .819 1 .590
Mitigated Std 0.844 1 .711 1 .222 0 .786 1 .235 2 .897
Importance 0.57 0 .37 −0.66 0 .01 −0.31−0.57
Table 6: Results from investigating the importance for the effects of different randomness factors for the in-context
learning using the Flan-T5 model across all datasets the model work correctly on.
LLAMA-2-13B SST2 C OLA MRPC AG N EWS TREC SNIPS DB-P EDIA
GOLDEN F1 Macro (%) 90.48 67 .58 58 .84 44 .88 39 .85 59 .18 62 .34
MODEL F1 Std 2.87 4 .12 4 .70 5 .51 4 .10 5 .82 4 .56
LABEL F1 Macro (%) 90.23 66 .35 59 .47 45 .67 39 .76 59 .27 62 .50
SELECTION F1 Std 2.50 4 .10 4 .30 4 .98 4 .58 5 .35 4 .33
Contributed Std 2.191 3 .470 2 .856 4 .077 3 .559 3 .118 2 .729
Mitigated Std 1.036 1 .977 3 .065 2 .420 2 .602 4 .076 3 .248
Importance 0.40 0 .36 −0.04 0 .30 0 .23 −0.16 −0.11
DATA F1 Macro (%) 90.16 65 .88 58 .51 46 .01 39 .41 59 .19 61 .72
SPLIT F1 Std 3.03 3 .73 5 .07 5 .90 3 .89 4 .52 4 .45
Contributed Std 2.374 2 .853 3 .924 4 .312 2 .601 3 .707 2 .439
Mitigated Std 1.376 2 .288 3 .053 3 .730 2 .612 2 .244 3 .662
Importance 0.35 0 .14 0 .19 0 .11 −0.00 0 .25 −0.27
DATA F1 Macro (%) 90.53 65 .30 59 .89 43 .92 42 .76 60 .64 60 .59
ORDER F1 Std 3.02 4 .23 3 .96 6 .22 3 .67 4 .27 4 .18
Contributed Std 1.177 2 .919 2 .910 4 .471 2 .840 3 .600 3 .720
Mitigated Std 2.694 2 .845 2 .383 4 .247 2 .242 2 .123 1 .833
Importance −0.53 0 .02 0 .11 0 .04 0 .15 0 .25 0 .41
SAMPLE F1 Macro (%) 89.70 65 .54 58 .42 45 .03 39 .89 59 .32 62 .24
CHOICE F1 Std 4.69 4 .31 5 .04 5 .92 3 .96 4 .25 4 .09
Contributed Std 3.481 3 .630 3 .661 5 .099 2 .783 3 .391 2 .329
Mitigated Std 1.714 1 .911 3 .293 2 .708 2 .775 2 .453 3 .261
Importance 0.61 0 .42 0 .08 0 .43 0 .00 0 .16 −0.20
Table 7: Results from investigating the importance for the effects of different randomness factors for the in-context
learning using the LLaMA-2 model across all datasets.
30MISTRAL -7B SST2 C OLA MRPC AG N EWS TREC SNIPS DB-P EDIA
GOLDEN F1 Macro (%) 67.45 61 .96 67 .42 65 .28 51 .66 75 .96 90 .03
MODEL F1 Std 13.38 12 .73 3 .22 6 .87 6 .37 7 .91 2 .12
LABEL F1 Macro (%) 66.72 62 .30 67 .40 64 .31 52 .54 75 .37 89 .78
SELECTION F1 Std 13.79 12 .48 3 .67 6 .30 5 .80 8 .99 1 .87
Contributed Std 10.793 7 .913 1 .880 4 .969 4 .360 5 .412 1 .545
Mitigated Std 6.662 8 .511 2 .986 3 .157 3 .626 7 .053 0 .877
Importance 0.31 −0.05−0.34 0 .26 0 .12 −0.21 0 .32
DATA F1 Macro (%) 67.96 64 .87 65 .40 65 .92 51 .25 74 .19 89 .55
SPLIT F1 Std 13.46 12 .15 3 .59 7 .42 6 .21 8 .17 1 .50
Contributed Std 10.935 10 .947 2 .057 6 .018 4 .472 6 .391 1 .161
Mitigated Std 6.302 4 .280 2 .767 3 .871 3 .847 4 .597 0 .677
Importance 0.35 0 .52 −0.22 0 .31 0 .10 0 .23 0 .23
DATA F1 Macro (%) 70.31 61 .50 66 .91 62 .94 52 .18 77 .82 91 .09
ORDER F1 Std 14.97 12 .56 3 .60 5 .58 7 .62 7 .03 2 .67
Contributed Std 8.629 3 .018 2 .294 3 .943 5 .626 5 .610 1 .877
Mitigated Std 10.732 11 .459 2 .586 3 .496 4 .846 4 .119 1 .486
Importance −0.16−0.66−0.09 0 .07 0 .12 0 .19 0 .18
SAMPLE F1 Macro (%) 66.56 66 .78 67 .58 64 .16 52 .58 74 .20 90 .07
CHOICE F1 Std 12.78 11 .64 3 .45 6 .96 5 .96 7 .67 2 .32
Contributed Std 12.084 8 .865 2 .521 6 .066 4 .306 5 .722 1 .892
Mitigated Std 3.553 6 .853 2 .140 3 .322 4 .051 4 .956 0 .697
Importance 0.64 0 .16 0 .12 0 .40 0 .04 0 .10 0 .56
Table 8: Results from investigating the importance for the effects of different randomness factors for the in-context
learning using the Mistral-7B model across all datasets.
ZEPHYR -7B SST2 C OLA MRPC AG N EWS TREC SNIPS DB-P EDIA
GOLDEN F1 Macro (%) 60.22 51 .16 54 .74 61 .73 59 .08 71 .73 90 .19
MODEL F1 Std 1.04 9 .57 12 .79 2 .07 3 .88 4 .13 0 .83
LABEL F1 Macro (%) 60.23 48 .55 55 .29 62 .17 58 .18 71 .65 90 .13
SELECTION F1 Std 1.04 7 .27 12 .43 1 .88 3 .52 3 .30 0 .84
Contributed Std 0.863 5 .806 5 .109 1 .460 2 .963 2 .935 0 .761
Mitigated Std 0.548 2 .529 11 .008 1 .004 1 .494 0 .977 0 .298
Importance 0.30 0 .34 −0.46 0 .22 0 .38 0 .47 0 .56
DATA F1 Macro (%) 60.42 50 .43 51 .84 62 .24 57 .89 71 .76 89 .85
SPLIT F1 Std 0.79 9 .94 12 .42 2 .06 3 .71 3 .99 0 .98
Contributed Std 0.664 7 .675 5 .973 1 .817 3 .221 3 .052 0 .823
Mitigated Std 0.380 4 .619 10 .563 0 .807 1 .345 2 .242 0 .466
Importance 0.27 0 .32 −0.36 0 .49 0 .48 0 .20 0 .43
DATA F1 Macro (%) 59.97 49 .34 55 .83 62 .56 59 .93 71 .99 90 .12
ORDER F1 Std 1.05 7 .87 10 .69 2 .01 4 .06 3 .61 0 .85
Contributed Std 0.456 3 .598 8 .038 0 .919 1 .925 3 .007 0 .592
Mitigated Std 0.918 5 .379 6 .069 1 .744 3 .550 1 .791 0 .584
Importance −0.44−0.19 0 .15 −0.40 −0.42 0 .29 0 .01
SAMPLE F1 Macro (%) 60.13 51 .57 52 .43 61 .97 59 .02 70 .75 90 .26
CHOICE F1 Std 0.83 9 .97 13 .69 2 .30 3 .83 4 .08 0 .74
Contributed Std 0.744 9 .135 6 .305 1 .874 3 .279 3 .331 0 .576
Mitigated Std 0.338 3 .333 11 .849 1 .144 1 .769 2 .164 0 .433
Importance 0.39 0 .61 −0.43 0 .35 0 .39 0 .28 0 .17
Table 9: Results from investigating the importance for the effects of different randomness factors for the in-context
learning using the Zephyr-7B model across all datasets.
31BERT SST2 C OLA MRPC AG N EWS TREC SNIPS DB-P EDIA
GOLDEN F1 Macro (%) 87.37 72 .63 73 .56 85 .78 90 .11 97 .80 98 .80
MODEL F1 Std 0.97 1 .47 2 .92 1 .24 1 .67 0 .49 0 .36
LABEL F1 Macro (%) 87.29 72 .61 73 .42 85 .79 89 .97 97 .83 98 .81
SELECTION F1 Std 1.14 1 .55 2 .76 1 .29 1 .77 0 .51 0 .34
Contributed Std 0.927 1 .212 2 .168 0 .979 1 .348 0 .426 0 .308
Mitigated Std 0.453 0 .865 1 .517 0 .776 1 .042 0 .248 0 .121
Importance 0.49 0 .24 0 .22 0 .16 0 .18 0 .37 0 .52
DATA F1 Macro (%) 87.31 72 .43 73 .38 85 .73 89 .54 97 .82 98 .80
SPLIT F1 Std 1.10 1 .40 2 .90 1 .27 1 .71 0 .48 0 .32
Contributed Std 0.937 1 .250 2 .384 1 .164 1 .568 0 .442 0 .294
Mitigated Std 0.361 0 .528 1 .436 0 .388 0 .523 0 .142 0 .115
Importance 0.59 0 .49 0 .33 0 .63 0 .63 0 .62 0 .50
MODEL F1 Macro (%) 87.31 72 .59 73 .50 85 .79 90 .30 97 .64 98 .84
INITIALISATION F1 Std 1.12 1 .52 2 .81 1 .18 1 .79 0 .49 0 .33
Contributed Std 0.828 1 .059 2 .180 0 .693 0 .939 0 .270 0 .121
Mitigated Std 0.512 1 .000 1 .600 0 .903 1 .491 0 .387 0 .300
Importance 0.33 0 .04 0 .20 −0.17 −0.33−0.24 −0.50
DATA F1 Macro (%) 87.30 72 .64 73 .66 85 .79 90 .26 97 .64 98 .84
ORDER F1 Std 1.03 1 .63 2 .80 1 .10 1 .76 0 .47 0 .32
Contributed Std 0.852 1 .086 2 .081 0 .686 1 .019 0 .246 0 .103
Mitigated Std 0.417 1 .151 1 .604 0 .817 1 .371 0 .392 0 .301
Importance 0.45−0.04 0 .16 −0.11 −0.21−0.30 −0.55
Table 10: Results from investigating the importance for the effects of different randomness factors for the BERT
fine-tuning across all datasets.
ROBERT A SST2 C OLA MRPC AG N EWS TREC SNIPS DB-P EDIA
GOLDEN F1 Macro (%) 88.48 74 .60 80 .35 86 .49 91 .66 98 .16 98 .31
MODEL F1 Std 1.29 3 .22 2 .16 1 .56 1 .79 0 .58 0 .57
LABEL F1 Macro (%) 88.54 74 .57 80 .25 86 .66 91 .55 98 .16 98 .35
SELECTION F1 Std 1.05 3 .54 2 .10 1 .35 1 .72 0 .56 0 .69
Contributed Std 0.904 2 .171 1 .723 1 .150 1 .461 0 .455 0 .471
Mitigated Std 0.392 1 .312 0 .990 0 .639 0 .732 0 .243 0 .234
Importance 0.40 0 .27 0 .34 0 .33 0 .41 0 .36 0 .41
DATA F1 Macro (%) 88.45 74 .24 80 .13 86 .54 91 .15 98 .10 98 .37
SPLIT F1 Std 1.21 3 .51 2 .20 1 .48 1 .75 0 .57 0 .44
Contributed Std 0.992 2 .151 1 .981 1 .377 1 .581 0 .506 0 .398
Mitigated Std 0.375 1 .162 0 .709 0 .392 0 .596 0 .168 0 .164
Importance 0.48 0 .31 0 .59 0 .63 0 .55 0 .58 0 .41
MODEL F1 Macro (%) 88.53 74 .57 80 .29 86 .59 91 .48 98 .02 98 .40
INITIALISATION F1 Std 1.10 3 .95 2 .16 1 .49 1 .80 0 .60 0 .42
Contributed Std 0.890 2 .051 1 .552 1 .030 1 .380 0 .412 0 .234
Mitigated Std 0.457 1 .705 1 .312 0 .953 1 .038 0 .367 0 .321
Importance 0.34 0 .11 0 .11 0 .05 0 .19 0 .08 −0.15
DATA F1 Macro (%) 88.42 74 .35 80 .40 86 .71 91 .52 98 .06 98 .38
ORDER F1 Std 1.26 4 .35 2 .10 1 .24 1 .81 0 .58 0 .41
Contributed Std 1.033 2 .424 1 .649 0 .991 1 .312 0 .372 0 .223
Mitigated Std 0.447 1 .769 1 .097 0 .671 1 .168 0 .412 0 .333
Importance 0.46 0 .20 0 .26 0 .20 0 .08 −0.07 −0.19
Table 11: Results from investigating the importance for the effects of different randomness factors for the RoBERTa
fine-tuning across all datasets.
32PROTOTYPICAL NETWORKS SST2 C OLA MRPC
GOLDEN F1 Macro(%) 80.33 60 .70 63 .62
MODEL F1 Std 0.94 2 .11 1 .78
LABEL F1 Macro (%) 80.33 60 .65 63 .34
SELECTION F1 Std 1.04 2 .10 1 .57
Contributed Std 0.959 1 .924 1 .448
Mitigated Std 0.268 0 .665 0 .472
Importance 0.74 0 .60 0 .55
DATA F1 Macro (%) 80.35 60 .23 63 .21
SPLIT F1 Std 0.97 2 .18 1 .91
Contributed Std 0.887 2 .010 1 .791
Mitigated Std 0.283 0 .646 0 .508
Importance 0.64 0 .65 0 .72
MODEL F1 Macro (%) 80.20 61 .04 63 .09
INITIALISATION F1 Std 0.97 2 .22 1 .88
Contributed Std 0.887 1 .801 1 .240
Mitigated Std 0.631 1 .186 1 .348
Importance 0.27 0 .29 −0.06
DATA F1 Macro (%) 75.77 59 .80 62 .98
ORDER F1 Std 4.51 4 .11 3 .08
Contributed Std 3.233 3 .346 2 .265
Mitigated Std 2.371 1 .659 1 .412
Importance 0.92 0 .80 0 .48
SAMPLE F1 Macro (%) 80.41 60 .54 63 .30
CHOICE F1 Std 0.98 2 .16 1 .80
Contributed Std 0.646 1 .659 1 .084
Mitigated Std 0.630 1 .335 1 .393
Importance 0.02 0 .15 −0.17
Table 12: Results from investigating the importance for the effects of different randomness factors for the Prototypical
Networks meta-learning approach across all binary datasets.
33MAML SST2 C OLA MRPC
GOLDEN F1 Macro(%) 79.93 60 .18 58 .29
MODEL F1 Std 2.34 1 .86 6 .27
LABEL F1 Macro (%) 79.99 60 .02 57 .52
SELECTION F1 Std 1.27 1 .84 6 .55
Contributed Std 0.893 1 .706 6 .000
Mitigated Std 0.500 0 .512 1 .988
Importance 0.17 0 .64 0 .64
DATA F1 Macro (%) 80.19 59 .95 57 .72
SPLIT F1 Std 0.95 1 .86 6 .60
Contributed Std 0.819 1 .716 5 .868
Mitigated Std 0.286 0 .555 2 .188
Importance 0.23 0 .62 0 .59
MODEL F1 Macro (%) 79.98 60 .76 57 .98
INITIALISATION F1 Std 1.67 1 .98 5 .67
Contributed Std 0.678 1 .389 4 .792
Mitigated Std 0.897 1 .328 2 .288
Importance −0.09 0 .03 0 .40
DATA F1 Macro (%) 79.58 59 .17 55 .00
ORDER F1 Std 1.54 2 .96 10 .85
Contributed Std 1.010 2 .368 9 .522
Mitigated Std 0.827 1 .340 3 .727
Importance 0.08 0 .55 0 .92
SAMPLE F1 Macro (%) 80.19 60 .04 58 .10
CHOICE F1 Std 1.00 1 .89 6 .36
Contributed Std 0.167 1 .265 1 .940
Mitigated Std 0.977 1 .352 5 .983
Importance −0.35−0.05−0.64
Table 13: Results from investigating the importance for the effects of different randomness factors for the MAML
meta-learning approach across all binary datasets.
34REPTILE SST2 C OLA MRPC
GOLDEN F1 Macro(%) 81.14 57 .17 61 .06
MODEL F1 Std 1.46 10 .50 5 .70
LABEL F1 Macro (%) 81.06 56 .16 60 .30
SELECTION F1 Std 0.93 11 .08 5 .89
Contributed Std 0.897 9 .482 4 .745
Mitigated Std 0.141 3 .398 1 .819
Importance 0.52 0 .58 0 .51
DATA F1 Macro (%) 81.04 56 .45 60 .54
SPLIT F1 Std 1.56 10 .24 5 .92
Contributed Std 0.747 8 .550 4 .740
Mitigated Std 0.485 3 .175 1 .776
Importance 0.18 0 .51 0 .52
MODEL F1 Macro (%) 81.01 56 .87 59 .84
INITIALISATION F1 Std 2.42 10 .65 6 .73
Contributed Std 0.576 8 .325 4 .853
Mitigated Std 1.300 3 .979 2 .722
Importance −0.50 0 .41 0 .37
DATA F1 Macro (%) 81.17 59 .17 39 .87
ORDER F1 Std 2.01 7 .34 12 .33
Contributed Std 0.591 4 .690 11 .446
Mitigated Std 0.951 2 .959 3 .991
Importance −0.25 0 .16 1 .31
SAMPLE F1 Macro (%) 81.00 60 .00 60 .77
CHOICE F1 Std 2.61 4 .97 5 .63
Contributed Std 0.370 2 .510 4 .221
Mitigated Std 1.674 2 .171 2 .612
Importance −0.89 0 .03 0 .28
Table 14: Results from investigating the importance for the effects of different randomness factors for the Reptile
meta-learning approach across all binary datasets.
35