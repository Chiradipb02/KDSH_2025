Calibrating the Confidence of Large Language Models by Eliciting Fidelity
Mozhi Zhang1,2*, Mianqiu Huang1,2, Rundong Shi3,
Linsen Guo3,Chong Peng3,Peng Yan3,Yaqian Zhou1,2Xipeng Qiu1,2†
1School of Computer Science, Fudan University
2Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
3Meituan
{mzzhang22, mqhuang23}@m.fudan.edu.cn
{shirundong}@meituan.com
{zhouyaqian, xpqiu}@fudan.edu.cn
Abstract
Large language models optimized with tech-
niques like RLHF have achieved good align-
ment in being helpful and harmless. However,
post-alignment, these language models often
exhibit overconfidence, where the expressed
confidence does not accurately calibrate with
their correctness rate. In this paper, we de-
compose the language model confidence into
theUncertainty about the question and the Fi-
delity to the answer generated by language
models. Then, we propose a plug-and-play
method, UF Calibration , to estimate the con-
fidence of language models. Our method has
shown good calibration performance by con-
ducting experiments with 6 RLHF-LMs on four
MCQA datasets. Moreover, we propose two
novel metrics, IPR and CE, to evaluate the cal-
ibration of the model, and we have conducted
a detailed discussion on Truly Well-Calibrated
Confidence for large language models. Our
method could serve as a strong baseline, and
we hope that this work will provide some in-
sights into the model confidence calibration.
1 Introduction
Large language models (LLMs) acquire vast world
knowledge and demonstrate powerful capabilities
through pre-training (Brown et al., 2020; OpenAI,
2023; Bubeck et al., 2023; Sun et al., 2024). With
technologies like RLHF (Ouyang et al., 2022) and
RLAIF (Bai et al., 2022; Lee et al., 2023), large
language models can become more helpful and
harmless to align with human preferences (Askell
et al., 2021). However, how to build a more honest
system has not yet been fully discussed. An honest
model should have a certain understanding of the
boundary of its knowledge, that is, knowing what it
does not know (Yin et al., 2023; Yang et al., 2023b;
Zhou et al., 2024). A plausible method is utilizing
*Work done during internship at Meituan.
†Corresponding author.the calibrated confidence to estimate the knowl-
edge boundary of language models. For pre-trained
language models, the per-token logit can already
be considered a well-calibrated confidence score,
which implies that pre-trained language models
(mostly) know what they know (Kadavath et al.,
2022).
However, recent studies have indicated that
language models optimized with techniques like
RLHF will exhibit issues of overconfidence (Lin
et al., 2022a; Kadavath et al., 2022; OpenAI, 2023;
He et al., 2023; Zhao et al., 2023; Tian et al., 2023;
Xiong et al., 2023). This issue could be reflected
in Multiple-Choice Question Answering (MCQA)
tasks, where the probability of RLHF-LMs gen-
erating a token and the likelihood of that token
being the correct answer are not well-calibrated.
For example, an answer provided by RLHF-LMs
with 95% confidence does not mean that there is
a 95% probability that the answer is correct. This
phenomenon may be due to the optimization objec-
tive of RLHF, which is to make the model generate
responses aligned with human preferences rather
than fitting answers that appear more frequently in
the corpus during the pre-training stage.
To alleviate the issue of miscalibration, previous
work focuses on two perspectives: the logit-based
method and the verbalization-based method. Logit-
based methods are usually post-hoc. We need
to find a higher temperature (usually above 2.0),
known as Temperature-Tuning (Guo et al., 2017),
to make the distribution of the model’s token logit
smoother for mitigating overconfidence (Kadavath
et al., 2022; He et al., 2023). The verbalization-
based method usually requires prompt engineering
to elicit the model’s confidence, and it also necessi-
tates the model to have strong Self-Awareness (Lin
et al., 2022a; Tian et al., 2023; Yin et al., 2023). Ag-
gregating the model’s logit-based and verbalization-
based confidence can also calibrate the model con-
fidence to some extent (Xiong et al., 2023).arXiv:2404.02655v2  [cs.CL]  9 Oct 2024Figure 1: In four different MCQA datasets, our method
has demonstrated good calibration effects, meaning it is
sufficiently close to the y=xcurve. The experimental
data is derived from GPT-3.5-Turbo .
As shown in Figure 2 and Appendix Tabel 6,
by replacing the model’s answer with “ All other
options are wrong. ”, we can assess whether the
model had high fidelity to its previously given an-
swer. Inspired by this phenomenon, we decompose
the language model confidence into two dimen-
sions: the Uncertainty about the question and the
Fidelity to the answer generated by language mod-
els. First, if the answers provided by language
model are consistent under multiple samplings, it
indicates that language model has lower uncertainty
regarding that question. Thus, we could utilize
the information entropy of the frequency distribu-
tion of sampled answers to calculate the model’s
uncertainty about a question. Second, we design
a novel method to estimate the model’s fidelity
to each of its sampled answers. Last, the uncer-
tainty regarding question Qand the fidelity to the
answer aitogether determine the model’s confi-
dence. As shown in Figure 1, our proposed UF
Calibration achieved good calibration across differ-
ent MCQA datasets. Meanwhile, UF Calibration
does not require knowledge of the model’s per-
token log-probability, making it broadly applicable
to various Black-box RLHF-LMs, which do not
provide the per-token log-probability.
To have a closer look at the calibration of model
confidence, we propose two novel metrics for evalu-
ating and observation: 1)Inverse PairRatio(IPR),
which is the proportion of inverse pairs in the Reli-
ability Diagram. This metric could reflect whether
the model is well-calibrated from the perspective
of the monotonicity of the Reliability Diagram. If
the reliability diagram is monotonic, it indicates
that the average accuracy of low-confidence an-swers is always lower than that of high-confidence
answers. 2)As shown in Table 7, we find that
as the number of model parameters increases, lan-
guage models still tend to consistently express un-
certainty within certain fixed ranges. Thus, we
design the Confidence Evenness (CE) to observe
to the uniformity of the density of each bar in the
reliability diagram. Our experimental results indi-
cate that, after calibration, even within the same
dataset, there is a significant difference in the confi-
dence of the answers provided by language models
for different questions. We summarize our main
contributions as follows:
1)Our proposed method could be viewed as a
strong baseline for eliciting model confidence,
where answer set is known. And the calibrated
confidence could be viewed as a soft label.
2)We propose two new metrics, IPR andCE, to
evaluate the calibration of LM’s confidence.
3)We conduct a detailed discussion of a research
question: “ What kind of Confidence is Truly
Well-Calibrated? ”, and we hope our discussion
can bring some insights to the community.
2 Related Work
Recent work has focused on LLM calibration (Lin
et al., 2022a; Kadavath et al., 2022; OpenAI, 2023).
In this section, we will briefly introduce two main-
stream methods for eliciting the confidence from
language models, namely the Logit-based Method
and the Verbalization-based Method.
2.1 Logit-based Method
When we can obtain the per-token logits from lan-
guage models, we can directly use the probability
of generating candidate answers as its confidence.
Conf( ai) =exp(logitai/t)
P|A|
j=1exp(logitaj/t), (1)
where tis the sampling temperature of language
models and |A|is the size of candidate answer set
A. Recent studies indicate that good calibration
can be achieved by adjusting the temperature of
RLHF-LMs (Kadavath et al., 2022; He et al., 2023).
However, temperature-scaling (Guo et al., 2017)
often requires higher temperatures, such as above
2.0 (Kadavath et al., 2022), which might cause the
outputs of the language models to become too ran-
dom. When the probabilities for model-generated
tokens are inaccessible, a straightforward solutionQuestion: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?Options:A. bankB. libraryC. department storeD.Allotheroptionsarewrong.E. new yorkAnswer:Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?Options:A. bankB. libraryC. department storeD. mallE. new yorkAnswer:ResponsefromLLM: D
ResponsefromLLM: DHigh FidelityResponsefromLLM: ALow FidelityLLM
LLMFigure 2: If the model’s choice of answer changes after
replacing the content of its previous selected option with
“All other options are wrong ”, it could be considered
that the model’s fidelity to its previous answer is not
high enough.
is to deploy sampling and use the frequency of the
sampled result to estimate the probability of gen-
erating this token. For instance, given a question
Q, we could sample Ktimes to acquire a set of
answers Acontaining Ndistinct answers, and each
answer with an associated frequency ni. The prob-
ability of the model generating answer aican be
estimated byni
K. Therefore, we could estimate the
confidence of language models by Psampled (ai).
Recently, Kumar et al. (2023) also propose to uti-
lize the conformal prediction to calibrate the confi-
dence of LLMs.
Conf( ai) =Psampled (ai) =ni
K, ai∈ A (2)
2.2 Verbalization-based Method
However, some commercial models, such as Chat-
GPT and Claude, usually do not provide per-
token logits. Benefiting from instruction fine-
tuning(Chung et al., 2022; Zhang et al., 2023),
language models could generate responses corre-
sponding to the input instructions. Another intu-
itive method is to prompt large language models
to provide their verbalized confidence along with
their responses as follows (Jiang et al., 2021; Linet al., 2022a; Tian et al., 2023):
(Answer ,Conf) = LLM(Question) ,(3)
This method requires the model to have a strong
ability to follow instructions and strong self-
awareness (know whether it knows something or
not (Yin et al., 2023)). Accordingly, verbalized
confidence can be a floating-point number between
0 and 1, i.e., ‘0.8’ . And it can be linguistic ex-
pressions, i.e., ‘Almost Certain’ ,‘About Even’ ,
‘Unlikely’ . Although this method is quite easy
to implement, we find various different LMs al-
ways tend to output some fixed high confidence
expressions, as show in Table 7.
3 Methodology
In this section, we will introduce the method we
propose. Our method does not require any knowl-
edge of the per-token logit of language models or
trivial prompt engineering to make the language
model output its confidence in a specified format.
3.1 Sampling
Firstly, as shown in the first step from Figure 3, for
question Q, by sampling Ktimes, we can obtain
a set of candidate answers A. We take the most
frequently occurring answer as the final answer.
Meanwhile, we can obtain the frequency distribu-
tionPsampled of candidate answers.
3.2 Eliciting the Fidelity of Answers
As shown in Figure 2, for question Qand a can-
didate answer ( ai,oi), where the option index is
aiand the content is oi, we simply replace oiwith
“All other options are wrong. ”, and then query the
model again. If the model has high fidelity to the
previously selected answer ( ai,oi), it should select
(ai, “All other options are wrong. ”) in the subse-
quent round of inquiry rather than any other option.
If language models select other options, we remove
the newly selected option to ensure that there is
only one “ All other options are wrong ” in candi-
date options. By repeating this process until the
model selects “ All other options are wrong ”, we
can establish a hierarchical fidelity chain C, such
as "A→C→D". This implies that when all options
are available, the model will prefer to select option
A. However, if option A is excluded, the model
will tend to choose option C, which indicates that
the model’s fidelity to option A is not high enough.
Accordingly, if the chain Chas only one element,Greedy DecodingLLMConsistency-Based Answer: DAnswer Set: (D, A, C)Frequency DistributionPsampleUncertaintyDACLLMD->A->CA->D->CC->A->D1) Sampling
2) Eliciting the Fidelity3) Uncertainty Estimation4) Confidence EstimationFidelity ofEach AnswerQuestion:Arevolvingdoorisconvenientfortwodirectiontravel,butitalsoservesasasecuritymeasureatawhat?Options:A.bankB.libraryC.departmentstoreD.mallE.newyorkAnswer:DDDAAC
Confidence(Question,D)= (1-Uncertainty)·Fidelity(D)Figure 3: Our proposed UF Calibration, which requires at most two phases to invoke the model. In the Sampling
phase, for black-box models, similar to the Sampled method, we need to sample 10 times. For white-box models, a
single invocation is sufficient. In the eliciting the fidelity phase, the model needs to be invoked approximately 2 to 3
times to generate a fidelity chain, as show in Table 5.
such as “A”, this suggests that the model’s fidelity
to option A is high enough, which can, to a cer-
tain extent, reflect the model’s confidence. Corre-
spondingly, for a hierarchical fidelity chain C, we
assign a fidelity weight to each element from right
to left. For example, for the ith element difrom
the right, we simply set its weight as τi. Therefore,
the normalized fidelity of the ith element aican be
calculated as follows:
FidelityC(ai) =τi
P|C|
i=1τi, (4)
where we usually set τas 2. As shown in Figure 3,
the answer set Amight include multiple different
answers. Consequently, we sequentially replace
the candidate answer in Awith “ All other options
are wrong. ” to elicit different hierarchical fidelity
chains, as depicted in the second step of Figure 3.
The fidelity score of each element aiin every hierar-
chical fidelity chain Cjcan be calculated using (4).
Thus, the model’s fidelity of answer aican be calcu-
lated by the weighted average fidelity score across
different hierarchical chains. Since the hierarchi-
cal fidelity chain is elicited by greedy decoding,
the frequency of occurrence of different chains is
consistent with the frequency of occurrence of the
first element a|C|from left to right. Therefore, the
frequency Psampled (a|C|)can be viewed as a proxy
for the probability Psampled (Cj)of different hierar-
chical fidelity chains to calculate the overall fidelityscoreF(·)of each answer.
F(ai) =|A|X
j=1Psampled (Cj)·FidelityCj(ai),(5)
3.3 Uncertainty Estimation
As shown in Section 3.1, through sampling, we
can obtain the frequency of each answer generated
by the model and use it to estimate the genera-
tion probability of each answer token. Previous
works (Kadavath et al., 2022; OpenAI, 2023) have
revealed that RLHF-LMs often exhibit overconfi-
dence in token generation probability, especially
in the temperature range we commonly use, such
as between 0 and 1.0. However, these probabili-
ties could still reveal, to some extent, the model’s
confidence regarding the current question Q. For
instance, if the distribution of Psampled is flatter,
it indicates that the language model has more sig-
nificant uncertainty regarding the question Q. An
intuitive method is calculating the information en-
tropy of the distribution Psampled to estimate the
model’s uncertainty about question Qas follows:
Uncertainty (Q) =−PM
i=1pi·logpi
log M,(6)
where Mis the option number of question Q. Since
the range of the information entropy for Psampled
is from 0 to log M , we normalize the information
entropy using log M .ARC-Challenge MMLU CommonSenseQA TruthfulQA
Method ECE 10↓IPR 10↓CE10↑ Acc ↑ECE 10↓IPR 10↓CE10↑ Acc ↑ECE 10↓IPR 10↓CE10↑ Acc ↑ECE 10↓IPR 10↓CE10↑ Acc ↑
GPT-3.5-T URBO
Verb 0.069 0.200 0.681 75.597 0.138 0.200 0.795 59.028 0.087 0.178 0.660 71.253 0.215 0.178 0.792 57.405
Ling 0.083 0.464 0.451 75.683 0.197 0.472 0.441 56.019 0.109 0.250 0.451 71.499 0.271 0.667 0.669 59.241
Sampled 0.095 0.067 0.793 79.266 0.120 0.022 0.922 63.151 0.135 0.067 0.782 74.590 0.147 0.044 0.901 59.333
Ours 0.112 0.139 0.897 79.266 0.088 0.083 0.812 63.151 0.073 0.083 0.812 74.590 0.074 0.133 0.775 59.333
GPT-4-T URBO
Verb 0.080 0.400 0.642 92.833 0.045 0.095 0.706 81.25 0.083 0.111 0.713 83.210 0.056 0.044 0.598 83.109
Ling 0.040 0.036 0.520 89.505 0.066 0.083 0.627 78.762 0.056 0.071 0.637 83.702 0.059 0.139 0.635 79.437
Sampled 0.067 0.200 0.221 92.833 0.153 0.311 0.536 80.324 0.121 0.133 0.541 83.866 0.091 0.178 0.478 87.515
Ours 0.127 0.083 0.757 92.833 0.089 0.083 0.906 80.324 0.109 0.083 0.925 83.866 0.042 0.044 0.764 87.515
Table 1: Experimental results derived from GPT-3.5-Turbo andGPT-4-Turbo . For each column in the table, the
closer the color is to blue, the better the calibration. And the closer it is to orange, the worse the performance. We
also have bolded the best results, and for the second-best results, we have added an underline beneath them.
ARC-Challenge MMLU CommonSenseQA TruthfulQA
Method ECE 10↓IPR 10↓CE10↑ Acc ↑ECE 10↓IPR 10↓CE10↑ Acc ↑ECE 10↓IPR 10↓CE10↑ Acc ↑ECE 10↓IPR 10↓CE10↑ Acc ↑
Verb. 0.135 0.178 0.752 58.191 0.199 0.178 0.802 45.891 0.107 0.083 0.806 59.214 0.373 0.133 0.874 26.928
Ling 0.298 0.286 0.613 50.853 0.399 0.333 0.709 30.921 0.097 0.222 0.771 60.770 0.594 0.571 0.681 23.990
Sampled 0.121 0.044 0.890 67.702 0.162 0.067 0.919 52.315 0.110 0.044 0.857 70.762 0.236 0.133 0.891 34.517
Token 0.064 0.067 0.521 67.235 0.135 0.067 0.647 54.803 0.064 0.022 0.477 71.007 0.176 0.133 0.577 34.761
Ours 0.063 0.028 0.887 67.702 0.076 0.028 0.829 52.315 0.051 0.056 0.886 70.762 0.080 0.028 0.704 34.517
Table 2: Experimental results derived from Baichuan2-13B-Chat .
3.4 Confidence Estimation
Given the model’s Uncertainty for a given question
Qand the fidelity F(·)among different candidate
answers, the confidence of the model in its answer
aifor question Qis defined as follows:
Conf (Q, ai) = 
1−Uncertainty (Q)
·F(ai),
(7)
4 Experiments
To validate the effectiveness of our pro-
posed method, we conducted experi-
ments on different RLHF-LMs such as
GPT-3.5-Turbo1, GPT-4-Turbo (OpenAI,
2023), LLaMA2-Chat (Touvron et al., 2023) and
Baichuan2-13B-Chat (Yang et al., 2023a). To
mitigate the influence of the sampling algorithm,
unless specifically stated otherwise, we use
hyper-parameters with a temperature of 1.0 and set
top_p as 1.0.
4.1 Experimental Setting
Dataset. We have conducted experiments on
four MCQA datasets to verify the effectiveness
of our proposed confidence estimation method.
ARC (Clark et al., 2018) is a dataset of 7,787
grade-school-level questions. We use the test split
of the ARC-Challenge with 1,172 questions for
our experiments. MMLU (Hendrycks et al., 2021)
is a dataset designed to measure knowledge ac-
quired during pretraining and covers 57 subjects.
1https://openai.com/chatgptTo reduce the cost of API calls, we sampled1
8of
the data for testing for each subject. Common-
SenseQA (Talmor et al., 2019) is a dataset for com-
monsense question answering, and we use the vali-
dation split with 1,221 questions for experiments.
TruthfulQA (Lin et al., 2022b) is a dataset that con-
tains 817 questions designed to evaluate language
models’ preference to mimic some human false-
hoods. All the experiments are conducted under a
0-shot setting.
Metrics. We utilize multiple metrics to evaluate.
We bin the predictions from the model by their con-
fidence and report the ECE (expected calibration
error). We also report the Brier Score of different
methods in Table 11. In this paper, we also defines
two novel metrics to evaluate the calibration. The
first one is IPR (Inverse PairRatio), which is used
to measure the monotonicity of the reliability di-
agram. If the reliability diagram is monotonic, it
indicates that the average accuracy of answers with
low confidence is lower than the average accuracy
of answers with high confidence.
IPR M=IP
C2
K, (8)
where IPis the inverse pair number in the reliable
diagram, and Kis the bin number with a density
larger than 0. We found that as the number of
model parameters increases, the accuracy of the
model improves across various datasets. However,
language models still tend to consistently express
uncertainty within certain fixed ranges, and ECEARC-Challenge MMLU CommonSenseQA TruthfulQA
Method ECE 10↓IPR 10↓CE10↑ Acc ↑ECE 10↓IPR 10↓CE10↑ Acc ↑ECE 10↓IPR 10↓CE10↑ Acc ↑ECE 10↓IPR 10↓CE10↑ Acc ↑
LLAMA2-7B-C HAT
Verb 0.294 0.083 0.482 45.904 0.325 0.267 0.531 41.551 0.208 0.267 0.516 52.662 0.499 0.200 0.626 21.787
Ling 0.452 0.333 0.283 44.625 0.478 0.357 0.315 38.542 0.385 0.250 0.275 51.597 0.647 0.607 0.406 24.113
Sampled 0.329 0.156 0.781 50.683 0.316 0.222 0.900 43.056 0.294 0.178 0.765 54.627 0.389 0.133 0.875 27.540
Token 0.161 0.156 0.430 50.256 0.224 0.333 0.593 42.419 0.148 0.133 0.417 54.791 0.234 0.289 0.484 27.417
Ours 0.073 0.111 0.921 50.683 0.102 0.167 0.890 43.056 0.053 0.167 0.903 54.627 0.121 0.083 0.762 27.540
LLAMA2-13B-C HAT
Verb 0.198 0.143 0.495 57.594 0.286 0.214 0.572 45.614 0.204 0.278 0.497 56.260 0.443 0.167 0.732 27.138
Ling 0.327 0.333 0.393 57.301 0.448 0.333 0.378 45.040 0.316 0.133 0.449 56.692 0.627 0.733 0.508 26.864
Sampled 0.297 0.200 0.653 60.239 0.351 0.267 0.788 47.251 0.287 0.156 0.717 58.722 0.461 0.422 0.798 29.131
Token 0.135 0.178 0.408 59.898 0.225 0.244 0.502 47.512 0.142 0.222 0.403 57.007 0.238 0.200 0.429 30.845
Ours 0.069 0.111 0.886 60.239 0.070 0.083 0.852 47.251 0.043 0.083 0.883 58.722 0.121 0.083 0.762 29.131
LLAMA2-70B-C HAT
Verb 0.071 0.286 0.369 70.819 0.236 0.194 0.351 53.183 0.069 0.222 0.286 70.680 0.311 0.028 0.522 43.452
Ling 0.223 0.333 0.119 67.833 0.375 0.333 0.096 51.794 0.189 0.067 0.117 70.106 0.507 0.400 0.289 36.597
Sampled 0.220 0.311 0.475 72.867 0.325 0.289 0.289 56.308 0.212 0.089 0.551 72.809 0.351 0.156 0.622 51.897
Token 0.091 0.200 0.315 73.208 0.190 0.378 0.378 56.597 0.093 0.178 0.339 72.645 0.173 0.267 0.352 52.020
Ours 0.085 0.111 0.908 72.867 0.066 0.083 0.898 56.308 0.094 0.111 0.918 72.809 0.093 0.089 0.804 51.897
Table 3: Experimental results derived from LLaMA-2-Chat .
cannot clearly reflect this phenomenon. Therefore,
we suggest using the CE(Confidence Evenness)
to evaluate the uniformity of the density of each
bar in the reliability diagram.
CEM=−PM
i=1pi·logpi
log M, (9)
In this paper, we adopt 10equal-size bins to calcu-
lateECE 10,IPR 10andCE10. We also report the
accuracy on these benchmarks to measure whether
calibration reduces the accuracy.
Baselines. We compared our approach with dif-
ferent baselines for eliciting the confidence of lan-
guage model. First, we reproduced the Verb and
Ling method proposed by Tian et al. (2023). The
Verb method involves prompting the model to
output a floating-point number between 0 and 1
to represent its confidence immediately after pro-
viding an answer (Tian et al., 2023; Lin et al.,
2022a). The Ling method entails having the lan-
guage model express its confidence level in natural
language (Tian et al., 2023). Since commercial
models like ChatGPT do not provide per-token log-
its, we employed a sampling technique to estimate
the probability of token generation, referred to as
theSampled method. Unless otherwise specified,
the Sampled method involves sampling 10 times.
For open-source models like LLaMA2-Chat , we di-
rectly use the probability of token generation as
the measure of the language model’s confidence,
which we refer to as the Token method. We also
compare the Conformal Prediction Baseline pro-
posed by Kumar et al. (2023) with our UF calibra-
tion in Appendix D.1. All the prompt templates we
use are shown in Appendix E.4.2 Main Results
Tables 1–3 show our experimental re-
sults on GPT-3.5-Turbo , GPT-4-Turbo ,
Baichuan2-13B-Chat , and LLaMA2-Chat .
Based on the experimental results, the following
conclusions can be drawn:
1)Our proposed method demonstrates a clear im-
provement over the various baselines in terms of
three metrics: ECE 10,IPR 10, and CE10, which
demonstrates the effectiveness of our method.
2)The Verb and Ling methods might, to some
extent, impair the language model’s accuracy
on multiple-choice question answering tasks,
which might be caused by more complicated in-
structions. Additionally, since the Ling method
is more complex, it has a greater impact on the
overall accuracy than the Verb method.
3)Similar to the conclusion from Tian et al. (2023),
the calibration of the Verb method tends to be
better than that of the Ling method. This is be-
cause the linguistic expressions used in the Ling
method are based on human psychology. How-
ever, the confidence represented by the same
expression may have a gap between humans and
models and among different models and differ-
ent sentences might mean the same thing (Kuhn
et al., 2023).
4)TheCE10of the Verbalization-based Method
is relatively low, which suggests that language
models tends to prefer outputting expressions of
certain confidence, such as ‘Highly Likely’ ,
0.8 and 0.9. This phenomenon can also ex-
plain why the ECE 10of the Verbalization-based
Method improves when the overall average ac-
curacy of the model is between 70-90%.Figure 4: Our proposed method achieved well-calibrated results across all temperatures. The experimental results
are derived from LLaMA2-13B-Chat . The results from Baichuan2-13B-Chat are presented in Appendix Figure 7.
Figure 5: The experimental results are derived from LLaMA2-Chat .
4.3 Ablation Study
As shown in Table 4, removing Uncertainty and
only relying on Fidelity to estimate the model’s
confidence, we can also achieve comparatively
better calibration than other methods. This phe-
nomenon indicates that our proposed method re-
flects the language model’s Fidelity to its an-
swers very well. Meanwhile, it is difficult to es-
timate the model’s confidence only depending on
Uncertainty . As mentioned in 3.3, Uncertainty
is designed for measuring the model’s uncertainty
regarding the question Q, rather than its confidence
for a particular answer. In the section 3.2, we utilize
(4) to calculate the language model’s normalized
fidelity in a hierarchical fidelity chain, where τ
is a hyper-parameter. The larger the value of τ,
the lower the estimated fidelity for answers closer
to the end of the fidelity chain. Our experiments
in Table 4 indicate that setting τto around 2 is a
relatively appropriate choice for the fidelity estima-
tion process. If τis too large, the ECE 10will also
increase, which will cause the issue of overconfi-
dence of our estimated confidence.
5 Analysis and Discussion
To take a closer look at the difference between dif-
ferent calibration methods tailored for language
models, in this section, we verify the robustness of
our method from two aspects: Temperature-Scaling
andParameter-Scaling . Meanwhile, we also con-Method ARC MMLU CSQA TruthfulQA Avg.
Ours 0.069 0.070 0.043 0.121 0.076
w/o. Uncertainty 0.122 0.184 0.115 0.202 0.156
w/o. Fidelity 0.675 0.614 0.704 0.677 0.668
τ= 1.5 0.103 0.064 0.066 0.082 0.079
τ= 2.0(Default) 0.069 0.070 0.043 0.121 0.076
τ= 2.5 0.067 0.089 0.040 0.142 0.085
τ= 3.0 0.074 0.107 0.050 0.155 0.097
τ= 4.0 0.085 0.138 0.075 0.165 0.116
τ= 5.0 0.102 0.158 0.094 0.183 0.134
Best Result (Others) 0.135 0.225 0.142 0.238 0.185
Table 4: Ablation study of our method. The results
(ECE 10) are derived from LLaMA2-13B-Chat .
ducted a detailed discussion of a research question:
What kind of Confidence is Truly Well-Calibrated?
Temperature-Scaling In the main experiments,
we evaluate various methods using a constant tem-
perature of 1.0. In this section, we will explore
the influence of sampling temperature on the per-
formance of different methods. As illustrated in
Figures 4 and 7, our proposed calibration method
consistently achieves the lowest expected calibra-
tion error across all temperatures, showing remark-
able robustness to temperature variations. This is
because, in eliciting model fidelity, our method
always employs Greedy Decoding rather than Sam-
pling. Thus, the hierarchical chains we obtain are
usually consistent across different sampling temper-
atures. In contrast, the expected calibration error of
Logit-based Methods is usually affected by temper-
ature. For the Sampling method with limited sam-
pling budgets, the lower the temperature, the more
significantly the diversity of the sampled resultsFigure 6: Reliability diagrams of Baichuan2-13B-Chat on ARC-Challenge. In these diagrams, the darker the color,
the higher the density. The reliability diagrams of other models we evaluated are shown in Appendix Figures 13–12.
will decrease, exacerbating the overconfidence of
language models. For the Token Method, the im-
pact of temperature on its calibration shows a trend
of “first increasing and then remaining relatively
stable ” or ” first increasing and then decreasing “.
This is because we could directly utilize (1) to es-
timate the confidence of each option, and if the
temperature is too low (i.e., 0.1), it will lead to the
confidence of a large number of options approach-
ing zero. This phenomenon might contribute to
reducing expected calibration error, but it does not
necessarily indicate that the model’s confidence is
well-calibrated. The Verbalization-based method
is less affected by temperature, which indicates
that the expressions which language models prefer
to output are relatively consistent across different
temperatures.
Parameter-Scaling As shown in Figure 5, we
evaluate the calibration of various methods at dif-
ferent parameter scales on the LLaMA2-Chat series
models. Our proposed method exhibits good cali-
bration across different amounts of model parame-
ters. With the size of model parameters increasing,
the calibration of the Verbalization-based method
and the Logit-based method is improving. This
phenomenon indicates that as the scale of model pa-
rameters increases, the model’s Self-Awareness is
improving. However, the relatively high expected
calibration error suggests that language models still
have issues with overconfidence.
Truly Well-Calibrated Confidence Previous
work mainly evaluates the calibration of language
models through ECE. This section will discuss the
research question: “ What Kind of Confidence is
Truly Well-Calibrated? ”. Figure 6 demonstrates the
calibration of various methods. From the calibra-
tion perspective, we hope that the confidence and
accuracy relationship is close to the curve y=x.
Thus, we need to reduce the ECE by calibrating
confidence. Meanwhile, we hope that the reliabil-
ity diagram should be as monotonic as possible to
ensure that the accuracy of the results generatedwith low confidence is lower than that of the results
with high confidence. Therefore, we propose the
Inverse Pair Ratio (IPR) to evaluate monotonicity.
From the perspective of building a more honest
system, we hope the model’s confidence should
be distributed across different confidence intervals.
For example, if a language model has an overall
accuracy of 75% on the TruthfulQA dataset and
the confidence of each question from the language
model is always 75%, its ECE and IPR would be 0.
And we find that different models tend to express
confidence within a fixed interval. In this case, we
think that the confidence may not necessarily be a
truly well-calibrated confidence because we could
not exclude some low-confidence results based on
the confidence from the language model. Although
the prior distribution of the model’s confidence is
unknown, our confidence estimation method finds
that language models have different confidence
for different questions. Thus, we propose a met-
ric called Confidence Evenness (CE) to measure
whether the model confidence always is located
in a fixed interval. We believe ECE, IPR, and CE
evaluate calibration from different perspectives and
there is a trade-off between these three metrics. We
suggest that truly well-calibrated confidence should
achieve a balance among ECE, IPR, and CE, rather
than over-optimizing any of them.
6 Conclusion
In this paper, we decompose the language model
confidence into the Uncertainty about the question
and the Fidelity to the answer generated by lan-
guage models. Through the decomposition, we
propose a plug-and-play method, UF C ALIBRA -
TION , to calibrate the confidence of language mod-
els. Through experiments with 6 RLHF-LMs on 4
multiple-choice question answering benchmarks,
our method exhibits good calibration. Besides, we
propose two novel metrics, IPR andCE, to eval-
uate the calibration of language models. Finally,
we conduct a detailed discussion on Truly Well-Calibrated Confidence . We believe our method can
serve as a strong baseline, and we hope that this
work could provide some insights into the language
model confidence calibration.
Limitations
Although our method has shown good calibration,
it is mainly applicable to scenarios where the set
of answers is known, i.e., multiple-choice question
answering, text classification, sentiment classifica-
tion, and preference labeling in RLHF. Eliciting
the model’s fidelity in open-ended generation sce-
narios is a direction worth exploring. Meanwhile,
our method involves multiple invocations of lan-
guage models, and how to estimate the probability
distribution of tokens generated by the language
model with as few callings as possible remains to
be studied.
Acknowledgements
This work was supported by the National Natural
Science Foundation of China (No. 62441602). The
computations in this research were performed using
the CFFF platform of Fudan University.
References
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,
Deep Ganguli, Tom Henighan, Andy Jones, Nicholas
Joseph, Ben Mann, Nova DasSarma, Nelson El-
hage, Zac Hatfield-Dodds, Danny Hernandez, Jack-
son Kernion, Kamal Ndousse, Catherine Olsson,
Dario Amodei, Tom Brown, Jack Clark, Sam Mc-
Candlish, Chris Olah, and Jared Kaplan. 2021. A
general language assistant as a laboratory for align-
ment.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosuite, Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemi Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bow-
man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,
Nicholas Joseph, Sam McCandlish, Tom Brown, and
Jared Kaplan. 2022. Constitutional ai: Harmlessness
from ai feedback.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Advances
inNeural Information Processing Systems , vol-
ume 33, pages 1877–1901. Curran Associates, Inc.
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-
ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,
and Yi Zhang. 2023. Sparks of artificial general in-
telligence: Early experiments with GPT-4. ArXiv
preprint arXiv:2303.12712.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. ArXiv ,
abs/1803.05457.
Wade Fagen-Ulmschneider. 2023. Perception of proba-
bility words. Ms., UIUC, 05-24-2023.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-
berger. 2017. On calibration of modern neural net-
works. In Proceedings ofthe34th International
Conference onMachine Learning , volume 70 of
Proceedings ofMachine Learning Research , pages
1321–1330. PMLR.
Guande He, Peng Cui, Jianfei Chen, Wenbo Hu, and Jun
Zhu. 2023. Investigating uncertainty calibration of
aligned language models under the multiple-choice
setting.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021. Measuring massive multitask language under-
standing. In International Conference onLearning
Representations.
Mingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei
Liao, Silviu Pitis, Roger Baker Grosse, and Jimmy
Ba. 2023. Calibrating language models via aug-
mented prompt ensembles.Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham
Neubig. 2021. How can we know when language
models know? on the calibration of language mod-
els for question answering. Transactions ofthe
Association forComputational Linguistics , 9:962–
977.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli
Tran-Johnson, Scott Johnston, Sheer El-Showk,
Andy Jones, Nelson Elhage, Tristan Hume, Anna
Chen, Yuntao Bai, Sam Bowman, Stanislav Fort,
Deep Ganguli, Danny Hernandez, Josh Jacobson,
Jackson Kernion, Shauna Kravec, Liane Lovitt, Ka-
mal Ndousse, Catherine Olsson, Sam Ringer, Dario
Amodei, Tom Brown, Jack Clark, Nicholas Joseph,
Ben Mann, Sam McCandlish, Chris Olah, and Jared
Kaplan. 2022. Language models (mostly) know what
they know.
Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.
Semantic uncertainty: Linguistic invariances for
uncertainty estimation in natural language genera-
tion. In The Eleventh International Conference on
Learning Representations.
Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu,
David Bellamy, Ramesh Raskar, and Andrew Beam.
2023. Conformal prediction with large language
models for multi-choice question answering. arXiv
preprint arXiv:2305.18404.
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas
Mesnard, Johan Ferret, Kellie Lu, Colton Bishop,
Ethan Hall, Victor Carbune, Abhinav Rastogi, and
Sushant Prakash. 2023. Rlaif: Scaling reinforcement
learning from human feedback with ai feedback.
Stephanie Lin, Jacob Hilton, and Owain Evans.
2022a. Teaching models to express their uncer-
tainty in words. Transactions onMachine Learning
Research.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022b.
TruthfulQA: Measuring how models mimic human
falsehoods. In Proceedings ofthe60th Annual
Meeting ofthe Association for Computational
Linguistics (V olume 1:Long Papers) , pages 3214–
3252, Dublin, Ireland. Association for Computational
Linguistics.
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances inNeural Information
Processing Systems , volume 35, pages 27730–27744.
Curran Associates, Inc.Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li,
Qinyuan Cheng, Xiangyang Liu, Hang Yan, Yunfan
Shao, Qiong Tang, Shiduo Zhang, Xingjian Zhao,
Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao
Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui
Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang,
Yu-Gang Jiang, and Xipeng Qiu. 2024. Moss: An
open conversational large language model. Machine
Intelligence Research.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings ofthe2019 Conference
oftheNorth American Chapter oftheAssociation
forComputational Linguistics: Human Language
Technologies, V olume 1(Long andShort Papers) ,
pages 4149–4158, Minneapolis, Minnesota. Asso-
ciation for Computational Linguistics.
Katherine Tian, Eric Mitchell, Allan Zhou, Archit
Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,
and Christopher Manning. 2023. Just ask for cali-
bration: Strategies for eliciting calibrated confidence
scores from language models fine-tuned with human
feedback. In Proceedings ofthe2023 Conference on
Empirical Methods inNatural Language Processing ,
pages 5433–5442, Singapore. Association for Com-
putational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie
Fu, Junxian He, and Bryan Hooi. 2023. Can llms
express their uncertainty? an empirical evaluation of
confidence elicitation in llms.
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,
Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,
Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng
Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao,
Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Ji-
aming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su,Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang
Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Pei-
dong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li,
Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong
Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin
Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li,
Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan
Zhou, and Zhiying Wu. 2023a. Baichuan 2: Open
large-scale language models.
Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neu-
big, and Pengfei Liu. 2023b. Alignment for honesty.
Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,
Xipeng Qiu, and Xuanjing Huang. 2023. Do
large language models know what they don’t know?
InFindings oftheAssociation forComputational
Linguistics: ACL 2023 , pages 8653–8665, Toronto,
Canada. Association for Computational Linguistics.
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,
Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-
wei Zhang, Fei Wu, and Guoyin Wang. 2023. Instruc-
tion tuning for large language models: A survey.
Theodore Zhao, Mu Wei, J. Samuel Preston, and Hoi-
fung Poon. 2023. Automatic calibration and error
correction for generative large language models via
pareto optimal self-supervision.
Yunhua Zhou, Pengyu Wang, Peiju Liu, Yuxin
Wang, and Xipeng Qiu. 2024. The open-world
lottery ticket hypothesis for OOD intent clas-
sification. In Proceedings ofthe 2024 Joint
International Conference on Computational
Linguistics, Language Resources and Evaluation
(LREC-COLING 2024) , pages 15988–15999,
Torino, Italia. ELRA and ICCL.
A The Computation Cost of Eliciting
Fidelity
In this section, we will display the average length
of the fidelity chains for different models across
various datasets in the Table 5. Since we deploy
greedy decoding during the process of eliciting
fidelity,the average length of the fidelity chain is
equal to the average number of requests. At the
same time, it should be noted that, when eliciting
the Fidelity Chain, only 1 token needs to be gener-
ated. Therefore, the average length of the fidelity
chain can also be regarded as the average number
of tokens generated.
Model ARC-Challenge MMLU CommonSenseQA TruthfulQA Avg.
GPT-3.5-T URBO 2.774 2.984 3.052 3.275 3.021
GPT-4-T URBO 1.492 1.915 2.157 1.616 1.795
BAICHUAN 2-13B-C HAT 2.830 2.820 2.889 4.345 3.221
LLAMA 2-7B-C HAT 2.467 2.631 2.771 3.944 2.953
LLAMA 2-13B-C HAT 2.725 2.875 2.956 4.100 3.164
LLAMA 2-70B-C HAT 2.384 2.563 2.455 3.284 2.671
Table 5: The average length of the fidelity chains for
different models across various datasetsB Algorithm
The pseudo code of our proposed method is shown
in Algorithm 1. It should be clarified that, as long
as a candidate answer aiappears in the answer set
Aor the Fidelity chain set S, we could estimate its
confidence through (7).
Algorithm 1 Algorithm
Require: Input question Q, Option list O, An-
swer set A=∅, Sampling budget K, RLHF-
LMLM,o∗is “All other options are wrong.”,
Fidelity chain set S,U(·)refers to (6).
1:t←0
2:while t < K do
3: ai←LM(Q,O) ▷Sampling answer
4:A ← A ∪ { ai}
5:Psampled (ai)← P sampled (ai) + 1
6: t←t+ 1 ▷Continue sampling
7:end while
8:Psampled (ai)← P sampled (ai)/K
9:
10:Uncertainty (Q) =U(Psampled )
▷Get uncertainty
11:i←0
12:while|A|>0do
13: A ← A\ {ai} ▷Select a answer
14: O∗←(O \ { oi})∪o∗▷Replace option
15: Ci=ai ▷Init a fidelity chain
16: while|O∗|>0do
17: a∗←LM(Q,O∗)▷Greedy decoding
18: ifa∗̸=aithen ▷Low fidelity
19: O∗← O∗\ {oi}▷Delete option
20: ai=a∗
21: Ci= (Ci→a∗)▷Add element
22: else
23: break ▷High fidelity
24: end if
25: end while
26: S ← S ∪ C i
27: i←i+ 1
28:end while
29:
30:F(ai) =P|A|
j=1Psampled (Cj)·FidelityCj(ai)
▷Get fidelity
31:Conf(Q, ai) = (1 −Uncertainty (Q))·
F(ai) ▷Get confidence
32:return Conf(Q, ai)
▷Return the confidence of answer aiModel Is the answer chosen in the first round correct? Choose "All other options are wrong." after replacing Do not choose "All other options are wrong." after replacing
GPT-3.5-T URBO True 25.99% 33.27%
False 5.85% 34.88%
Acc. 81.61% 48.82%
GPT-4-T URBO True 70.75% 16.83%
False 3.00% 9.42%
Acc. 95.93% 64.10%
BAICUAN 2-13B-C HAT True 5.14% 29.40%
False 4.22% 61.24%
Acc. 54.90% 32.43%
LLAMA 2-7B-C HAT True 3.92% 23.50%
False 4.83% 67.75%
Acc. 44.76% 25.75%
LLAMA 2-13B-C HAT True 3.55% 25.64%
False 2.82% 67.99%
Acc. 55.77% 27.39%
LLAMA 2-70B-C HAT True 13.59% 38.43%
False 3.98% 44.00%
Acc. 77.35% 46.62%
Table 6: We found that if the option chosen by the model in the first round is replaced with "All other options are
wrong," the model then chooses "All other options are wrong" in the second round. In this case, the accuracy of the
model’s first-round choice is significantly higher compared to when it chooses other options in the second round.
The results are derived from TruthfulQA.
Dataset Method Model 0.0 0 .02 0 .05 0 .1 0 .2 0 .25 0 .3 0 .4 0 .5 0 .6 0 .7 0 .8 0 .9 0 .95 1 .0ECE 10↓CE10↑ Acc ↑
CSQA Verb LL AMA2-7B-C HAT 3 0 0 1 25 0 23 5 78 10 309 727 19 0 21 0.208 0.516 52.662
LLAMA2-13B-C HAT 11 0 0 0 9 0 1 29 7 112 108 851 61 0 32 0.204 0.497 56.260
LLAMA2-70B-C HAT 6 0 0 2 2 0 3 3 1 23 221 955 2 0 3 0.069 0.286 70.680
Ling LL AMA2-7B-C HAT 11 0 21 0 3 0 0 0 1 5 2 13 1020 75 70 0.385 0.275 51.597
LLAMA2-13B-C HAT 18 1 11 0 6 0 0 0 0 0 3 194 892 96 0 0.316 0.449 56.692
LLAMA2-70B-C HAT 0 0 26 0 0 0 0 0 0 1 2 2 1172 2 16 0.189 0.117 70.106
MMLU Verb LL AMA2-7B-C HAT 14 0 0 3 46 0 21 16 65 44 488 981 26 0 24 0.325 0.531 41.551
LLAMA2-13B-C HAT 23 0 0 0 41 0 0 54 7 227 278 1056 18 0 24 0.286 0.572 45.614
LLAMA2-70B-C HAT 1 0 0 0 7 0 3 1 2 9 518 1159 1 0 27 0.236 0.351 53.183
Ling LL AMA2-7B-C HAT 47 0 101 0 21 0 0 0 6 4 7 12 1408 77 45 0.478 0.315 38.542
LLAMA2-13B-C HAT 81 1 15 0 4 2 0 0 0 0 4 84 1261 261 11 0.448 0.378 45.040
LLAMA2-70B-C HAT 3 0 31 0 0 0 0 0 0 6 2 5 1673 1 7 0.375 0.096 51.794
ARC Verb LL AMA2-7B-C HAT 4 0 0 0 26 0 13 6 53 5 216 800 20 0 29 0.294 0.482 45.904
LLAMA2-13B-C HAT 1 0 0 0 31 0 0 13 13 68 129 851 18 0 47 0.198 0.495 57.594
LLAMA2-70B-C HAT 3 0 0 0 11 0 3 0 2 6 288 836 3 0 20 0.071 0.369 70.819
Ling LL AMA2-7B-C HAT 3 0 24 0 10 0 0 0 0 0 5 10 1023 53 44 0.452 0.283 44.625
LLAMA2-13B-C HAT 1 0 5 0 5 0 0 0 0 0 1 76 914 162 8 0.327 0.393 57.301
LLAMA2-70B-C HAT 3 0 27 1 0 0 0 0 0 3 1 1 1121 2 13 0.223 0.119 67.833
TruthfulQA Verb LL AMA2-7B-C HAT 10 0 0 1 23 0 8 2 125 18 167 406 17 0 40 0.499 0.626 21.787
LLAMA2-13B-C HAT 11 0 0 1 11 0 0 56 34 145 116 369 26 0 48 0.443 0.732 27.138
LLAMA2-70B-C HAT 3 0 0 0 7 0 4 4 4 22 320 404 9 0 30 0.311 0.522 43.452
Ling LL AMA2-7B-C HAT 30 0 53 0 10 0 0 0 8 4 4 15 611 43 39 0.647 0.406 24.113
LLAMA2-13B-C HAT 39 2 19 0 4 0 0 0 0 0 4 40 526 177 6 0.627 0.508 26.864
LLAMA2-70B-C HAT 10 0 31 0 0 0 0 0 0 3 0 9 718 12 31 0.507 0.289 36.597
Table 7: Language models tend to prefer outputting expressions of certain confidence, such as 0.8, and 0.9.
C Why could CE be used as a metric?
As mentioned in section 4.2, we found that Lan-
guage models tend to prefer outputting expressions
of certain confidence, such as ’Highly Likely’, 0.8,
and 0.9. In the table 7, we have counted the oc-
currence of different confidence levels for various
models on different datasets to demonstrate the
model’s preference for certain confidence levels
when using the Verb and Ling method.
We also notice that as the model parameters
increased, the accuracy of the model improved,
but the language model’s preference for certain
confidence levels do not change and even became
stronger. Therefore, we introduced the Confidence
Evenness to assess whether the model’s confidence
is overly concentrated in certain intervals.
Can existing metrics (such as ECE) capture thisphenomenon? There is an example: on Common-
SenseQA, as the parameters of Llama2-Chat in-
creasing, the accuracy rises from 51% to 70%,
and the ECE using the Ling method decrease from
0.385 to 0.189. But the 70B model shows a stronger
preference for outputting a confidence of 0.9. Fo-
cusing solely on the ECE metric cannot fully ob-
serve the changes in model preferences. Fortu-
nately, this phenomenal could be reflected by the
CE metrics.
Another extreme case is if models of varying
parameter sizes always output a 0.9 confidence
level, and as the model size increases, the aver-
age accuracy just shifts from 70% to 90%, then
the ECE would drop to 0. If we only use exist-
ing metrics for observation, we might conclude
that the model with the largest parameters has the
strongest self-awareness. However, by evaluatingthe CE metric across different models, we can iden-
tify a potential preference in how models express
confidence. Its ECE becoming 0 might just coin-
cidentally be because the average accuracy on a
certain dataset equals the confidence level it prefers
to output. Therefore, we believe the CE metric
provides a new perspective for observing model
confidence calibration.
Finally, it should be noted that we believe an
over-concentration of model confidence in a par-
ticular value or interval is not conducive to using
model confidence as a simple metric to filter out
low-confidence answers.
D Additional Results
D.1 Compared with Conformal Prediction
We reproduce Conformal Prediction for RLHF-
LMs (Kumar et al., 2023) in our dataset and set-
ting. Specifically, for each dataset, we select 50%
samples as the calibration set and the other sam-
ples as the test set. We also set the error rate to
α= 0.1meaning the prediction answer set has a
90% probability of containing the correct answer.
We then calculate the conformal scores in the cali-
bration set, where the specific calculation formula
isScore = 1−maxSoftmaxScore . For the test
set, we take the 1−αquantile of the conformal
scores from the calibration set as the threshold q.
During the testing stage, for a given sample, it is
only added to the prediction set if its generated
probability is greater than or equal to 1−q. For
each sample in the prediction set, we consider its
confidence to be (1−α)·(SoftmaxScore ). as
shown in the following table 8, our proposed UF
Calibration still demonstrates good calibration com-
pared to conformal prediction for RLHF-LMs. It
is also important to note that conformal prediction
requires a calibration set to determine a threshold
to build a prediction set. However, our method is
a plug-and-play approach that can accurately esti-
mate the model’s confidence without requiring any
prior knowledge.
D.2 Compared with CAPE
We reproduced the “ENUM” method from
CAPE (Jiang et al., 2023)on our evaluated dataset.
This method (CAPE-ENUM) calibrates the prob-
abilities of answers by permuting the order of op-
tions, which is complementary to our method For
Noptions, we generated all possible permutations
and then randomly selected 10 permutations to re-order the options, obtaining 10 different probability
distributions Pi. The temperature of the language
model was set to 1.0, and the final probability dis-
tribution was P=1
10P10
i=1Pi.
The results are shown in Table 9. From the exper-
imental results, it can be seen that the performance
of these two methods is comparable. However, it
is important to note that CAPE-ENUM requires
knowledge of the logits generated by the model
for each token . For Black-Box models, multiple
samples are needed to obtain Pi, and the overall
time complexity is O(M·K), where Krepresents
the number of permutations, and Mrepresents the
number of samples needed to obtain a probability
distribution Pi. Moreover, obtaining an accurate
Pi, usually requires a large M, which also leads to
an increase in computational cost.
D.3 Candidate-Aware UF Calibration
For some questions like “Which of the following
answers is better?”, after replace some options with
“All other options are wrong”, the remaining op-
tions are still reasonable. For example, “Which
of the following animals has the largest volume?”.
We find that these types of questions may appear in
the ARC-Challenge. To address issues, we propose
Candidate-Aware UF Calibration , which will in-
troduce all the candidate answers in the prompt
when utilizing our UF Calibration, even if the cur-
rently selectable options are only a subset of these.
Therefore, the model’s prompt template could be
changed to: “The question is: [current question].
Candidate answers: [all candidate answers]. From
the options below, please select the option you
agree with the most: [options for this round]. An-
swer:”. We tested Candidate-Aware UF Calibration
on three Llama2-Chat models. Experimental re-
sults from Table 10 show that Candidate-Aware UF
Calibration still demonstrates performance similar
to UF Calibration. This also partially validates that
"All of the other options are incorrect" is a valid
approach for quantifying fidelity.
D.4 Brier Score
Besides the ECE metric, the Brier Score is also
commonly used as an evaluation criterion for model
calibration.
BrierScore =1
NNX
t=1(ft−ot)2, (10)
where ftis the probability and otis the label. Ac-
cordingly, ftcan be referred to as the model’s con-Model Dataset Method ECE 10↓BS ↓CE10↑IPR 10↓
GPT-3.5-T URBO MMLU Conformal Prediction 0.086 0.189 0.897 0.111
Ours 0.088 0.170 0.812 0.083
TruthfulQA Conformal Prediction 0.115 0.197 0.884 0.028
Ours 0.074 0.153 0.775 0.133
CommonSenseQA Conformal Prediction 0.079 0.173 0.699 0.139
Ours 0.073 0.139 0.812 0.083
ARC Conformal Prediction 0.039 0.142 0.670 0.143
Ours 0.112 0.141 0.897 0.139
GPT-4-T URBO MMLU Conformal Prediction 0.084 0.164 0.482 0.472
Ours 0.089 0.142 0.906 0.083
TruthfulQA Conformal Prediction 0.046 0.112 0.425 0.222
Ours 0.042 0.102 0.764 0.044
CommonSenseQA Conformal Prediction 0.040 0.130 0.509 0.194
Ours 0.109 0.134 0.925 0.083
ARC Conformal Prediction 0.084 0.026 0.000 0.000
Ours 0.127 0.095 0.757 0.083
BAICHUAN 2-13B-C HAT MMLU Conformal Prediction 0.130 0.218 0.888 0.056
Ours 0.076 0.193 0.829 0.028
TruthfulQA Conformal Prediction 0.209 0.239 0.865 0.250
Ours 0.080 0.149 0.704 0.028
CommonSenseQA Conformal Prediction 0.056 0.162 0.801 0.056
Ours 0.051 0.153 0.886 0.056
ARC Conformal Prediction 0.061 0.173 0.848 0.028
Ours 0.063 0.166 0.887 0.028
LLAMA 2-7B-C HAT MMLU Conformal Prediction 0.253 0.290 0.864 0.361
Ours 0.102 0.214 0.890 0.167
TruthfulQA Conformal Prediction 0.353 0.361 0.825 0.361
Ours 0.121 0.186 0.762 0.083
CommonSenseQA Conformal Prediction 0.234 0.283 0.655 0.333
Ours 0.053 0.181 0.907 0.167
ARC Conformal Prediction 0.260 0.308 0.701 0.083
Ours 0.073 0.204 0.921 0.111
LLAMA 2-13B-C HAT MMLU Conformal Prediction 0.279 0.317 0.740 0.250
Ours 0.070 0.196 0.852 0.083
TruthfulQA Conformal Prediction 0.429 0.416 0.728 0.611
Ours 0.121 0.180 0.762 0.083
CommonSenseQA Conformal Prediction 0.220 0.274 0.647 0.250
Ours 0.043 0.166 0.883 0.111
ARC Conformal Prediction 0.212 0.260 0.611 0.361
Ours 0.069 0.178 0.886 0.111
LLAMA 2-70B-C HAT MMLU Conformal Prediction 0.260 0.305 0.592 0.250
Ours 0.066 0.189 0.898 0.083
TruthfulQA Conformal Prediction 0.281 0.301 0.558 0.306
Ours 0.093 0.162 0.804 0.089
CommonSenseQA Conformal Prediction 0.156 0.221 0.479 0.333
Ours 0.094 0.156 0.908 0.111
ARC Conformal Prediction 0.118 0.189 0.427 0.361
Ours 0.085 0.154 0.908 0.111
Table 8: Comparing calibration results of Conformal Prediction of RLHF-LMs (Kumar et al., 2023) and our
proposed method.Model Dataset Method ECE 10↓BS ↓CE10↑IPR 10↓
LLAMA 2-7B-C HAT MMLU CAPE-ENUM 0.099 0.176 0.815 0.022
Ours 0.102 0.214 0.890 0.167
TruthfulQA CAPE-ENUM 0.123 0.179 0.691 0.200
Ours 0.121 0.186 0.762 0.083
CommonSenseQA CAPE-ENUM 0.023 0.099 0.688 0.000
Ours 0.053 0.181 0.907 0.167
ARC CAPE-ENUM 0.066 0.150 0.808 0.000
Ours 0.073 0.204 0.921 0.111
LLAMA 2-13B-C HAT MMLU CAPE-ENUM 0.104 0.166 0.786 0.000
Ours 0.070 0.196 0.852 0.083
TruthfulQA CAPE-ENUM 0.157 0.185 0.665 0.067
Ours 0.121 0.180 0.762 0.083
CommonSenseQA CAPE-ENUM 0.033 0.095 0.650 0.000
Ours 0.043 0.166 0.883 0.083
ARC CAPE-ENUM 0.060 0.125 0.756 0.000
Ours 0.069 0.178 0.886 0.111
LLAMA 2-70B-C HAT MMLU CAPE-ENUM 0.098 0.144 0.727 0.000
Ours 0.066 0.189 0.898 0.083
TruthfulQA CAPE-ENUM 0.109 0.135 0.576 0.133
Ours 0.093 0.162 0.804 0.089
CommonSenseQA CAPE-ENUM 0.029 0.069 0.550 0.022
Ours 0.094 0.156 0.918 0.111
ARC CAPE-ENUM 0.036 0.077 0.621 0.000
Ours 0.085 0.154 0.908 0.111
Table 9: Comparing calibration results of CAPE (Jiang et al., 2023) and our proposed method.
fidence, while otrepresents whether it is the correct
answer (0 indicating an incorrect answer, 1 indicat-
ing a correct answer). In Table 11, we present the
Brier Scores of various baselines and our proposed
method. It can be seen that our method still exhibits
good calibration, especially for closed-source mod-
els such as GPT-3.5-Turbo ,GPT-4 Turbo .
E Prompt Templates
We use the prompt template from Tian et al. (2023)
for a fair comparison. The prompt template for
each baseline is provided in Table 12. The question
is substituted for the variable ${THE_QUESTION} in
each prompt. Table 13 shows the linguistic expres-
sion list of confidence we used for the Ling Method,
which originates from Fagen-Ulmschneider (2023).
F Reliability Diagram
We provide the reliability diagrams of all the RLHF-
LMs we evaluated in Figures 13-12. In a reliability
diagram, the darker the color of the bar, the greaterits density is, which indicates a preference for the
confidence the language models express. Although
the average accuracy of various RLHF-LMs is quite
different, these model always prefer to express their
confidence about 70-90% in verbalized methods.Model Dataset Method ECE 10↓BS ↓CE10↑IPR 10↓
LLAMA 2-7B-C HAT MMLU Candidate-Aware 0.129 0.233 0.897 0.083
Ours 0.102 0.214 0.890 0.167
TruthfulQA Candidate-Aware 0.166 0.205 0.769 0.156
Ours 0.121 0.186 0.762 0.083
CommonSenseQA Candidate-Aware 0.080 0.188 0.839 0.083
Ours 0.053 0.181 0.907 0.167
ARC Candidate-Aware 0.103 0.221 0.892 0.111
Ours 0.073 0.204 0.921 0.111
LLAMA 2-13B-C HAT MMLU Candidate-Aware 0.088 0.200 0.834 0.139
Ours 0.070 0.196 0.852 0.083
TruthfulQA Candidate-Aware 0.152 0.195 0.795 0.167
Ours 0.121 0.180 0.762 0.083
CommonSenseQA Candidate-Aware 0.053 0.173 0.885 0.083
Ours 0.043 0.166 0.883 0.083
ARC Candidate-Aware 0.062 0.183 0.882 0.111
Ours 0.069 0.178 0.886 0.111
LLAMA 2-70B-C HAT MMLU Candidate-Aware 0.131 0.232 0.922 0.111
Ours 0.066 0.189 0.898 0.083
TruthfulQA Candidate-Aware 0.212 0.220 0.869 0.378
Ours 0.093 0.162 0.804 0.089
CommonSenseQA Candidate-Aware 0.103 0.202 0.884 0.111
Ours 0.094 0.156 0.918 0.111
ARC Candidate-Aware 0.112 0.192 0.849 0.111
Ours 0.085 0.154 0.908 0.111
Table 10: Comparing calibration results of Candidate-Aware UF Calibration and UF Calibration.
Figure 7: The Impact of Temperature on Different Methods. Our proposed method achieved well-calibrated results
across all temperatures. The experimental results are derived from Baichuan2-13B-Chat .Model Method ARC-Challenge MMLU CommonSenseQA TruthfulQA Avg.
GPT-3.5-T URBO Verb 0.181 0.247 0.189 0.274 0.223
Ling 0.197 0.278 0.204 0.318 0.249
Sampled 0.157 0.202 0.216 0.206 0.195
Conformal 0.142 0.189 0.173 0.197 0.175
Ours 0.141 0.170 0.139 0.153 0.151
GPT-4-T URBO Verb 0.181 0.247 0.204 0.274 0.227
Ling 0.198 0.278 0.216 0.318 0.253
Sampled 0.074 0.174 0.147 0.112 0.127
Conformal 0.026 0.164 0.130 0.112 0.108
Ours 0.095 0.142 0.134 0.102 0.118
BAICHUAN 2-13B-C HAT Verb 0.257 0.294 0.239 0.363 0.288
Ling 0.336 0.407 0.235 0.553 0.383
Sampled 0.196 0.236 0.186 0.262 0.220
Token 0.095 0.168 0.092 0.198 0.138
Conformal 0.173 0.218 0.162 0.239 0.198
Ours 0.166 0.193 0.153 0.149 0.165
LLAMA 2-7B-C HAT Verb 0.332 0.348 0.283 0.449 0.353
Ling 0.451 0.471 0.396 0.609 0.4821
Sampled 0.358 0.350 0.323 0.411 0.360
Token 0.171 0.238 0.158 0.246 0.203
Conformal 0.308 0.290 0.283 0.361 0.311
Ours 0.204 0.214 0.181 0.186 0.196
LLAMA 2-13B-C HAT Verb 0.277 0.320 0.272 0.394 0.316
Ling 0.352 0.448 0.343 0.599 0.435
Sampled 0.318 0.374 0.317 0.470 0.370
Token 0.141 0.233 0.150 0.242 0.192
Conformal 0.260 0.317 0.274 0.416 0.317
Ours 0.178 0.196 0.166 0.180 0.180
LLAMA 2-70B-C HAT Verb 0.206 0.297 0.208 0.332 0.261
Ling 0.267 0.390 0.240 0.496 0.348
Sampled 0.236 0.347 0.237 0.360 0.295
Token 0.094 0.196 0.098 0.174 0.141
Conformal 0.189 0.305 0.221 0.301 0.254
Ours 0.154 0.189 0.156 0.162 0.165
Table 11: The Brier Score of different methods from six RLHF-Models on four MCQA datasets.Method Prompt Template
Verb (Tian et al., 2023) Provide your best guess and the probability that it is correct
(0.0 to 1.0) for the following question. Give ONLY the guess and
probability, no other words or explanation. For example:\nGuess:
<most likely option, without any extra commentary whatsoever;
just the option>\nProbability: <the probability between 0.0
and 1.0 that your guess is correct, without any extra
commentary whatsoever; just the probability!>\nThe question is:
{question}\nOptions:\n{choices}Answer:
Ling (Tian et al., 2023) Provide your best guess for the following question, and describe
how likely it is that your guess is correct as one of the following
expressions: {EXPRESSION_LIST}. Give ONLY the guess and your
confidence, no other words or explanation. For example:\n\n Guess:
<most likely guess, as short as possible; not a complete sentence,
just the guess!>\n Confidence: <description of confidence,
without any extra commentary whatsoever; just a short phrase!>\n
The question is: {question}\n Options:\n{choices}Answer:
Sampled Provide the option you agree with most for the following
question. Give ONLY the option of the answer, no other words or
explanation. For example:\nAnswer: <most likely option, without
any extra commentary whatsoever; just the option>\nThe question
is: {question}\nOptions:\n{choices}Answer:
Token Provide the option you agree with most for the following
question. Give ONLY the option of the answer, no other words or
explanation. For example:\nAnswer: <most likely option, without
any extra commentary whatsoever; just the option>\nThe question
is: {question}\nOptions:\n{choices}Answer:
Ours Provide the option you agree with most for the following
question. Give ONLY the option of the answer, no other words or
explanation. For example:\nAnswer: <most likely option, without
any extra commentary whatsoever; just the option>\nThe question
is: {question}\nOptions:\n{choices}Answer:
Table 12: Prompt templates for each method evaluated.
Linguistic Expression Confidence Score
‘Certain’ 1.0
‘Almost Certain’ 0.95
‘Highly Likely’ 0.9
‘Very Good Chance’ 0.8
‘We Believe’ 0.75
‘Probably’ 0.7
‘Probable’ 0.7
‘Likely’ 0.7
‘Better than Even’ 0.6
‘About Even’ 0.5
‘Probably Not’ 0.25
‘We Doubt’ 0.2
‘Unlikely’ 0.2
‘Little Chance’ 0.1
‘Chances are Slight’ 0.1
‘Improbable’ 0.1
‘Highly Unlikely’ 0.05
‘Almost No Chance’ 0.02
‘Impossible’ 0.0
Table 13: The EXPRESSION_LIST we used for the Ling Method.Figure 8: The experimental results are derived from GPT-4-Turbo on 4 MCQA datasets.
Figure 9: The experimental results are derived from Baichuan2-13B-Chat on 4 MCQA datasets.Figure 10: The experimental results are derived from LLaMA2-7B-Chat on 4 MCQA datasets.
Figure 11: The experimental results are derived from LLaMA2-13B-Chat on 4 MCQA datasets.Figure 12: The experimental results are derived from LLaMA2-70B-Chat on 4 MCQA datasets.
Figure 13: The experimental results are derived from GPT-3.5-Turbo on 4 MCQA datasets.