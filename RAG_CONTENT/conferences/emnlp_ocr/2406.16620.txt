OmAgent: A Multi-modal Agent Framework for Complex Video
Understanding with Task Divide-and-Conquer
Lu Zhang1, Tiancheng Zhao1,2, Heting Ying1, Yibo Ma1, Kyusong Lee1,2
1Om AI Research,2Binjiang Institute of Zhejiang University
{zhang_lu, ying_heting, ma_yibo}@hzlh.com {tianchez, kyusongl}@zju-bj.com
Abstract
Recent advancements in Large Language Mod-
els (LLMs) have expanded their capabilities
to multimodal contexts, including comprehen-
sive video understanding. However, process-
ing extensive videos such as 24-hour CCTV
footage or full-length films presents significant
challenges due to the vast data and processing
demands. Traditional methods, like extracting
key frames or converting frames to text, of-
ten result in substantial information loss. To
address these shortcomings, we develop OmA-
gent, efficiently stores and retrieves relevant
video frames for specific queries, preserving
the detailed content of videos. Additionally,
it features an Divide-and-Conquer Loop capa-
ble of autonomous reasoning, dynamically in-
voking APIs and tools to enhance query pro-
cessing and accuracy. This approach ensures
robust video understanding, significantly reduc-
ing information loss. Experimental results af-
firm OmAgent’s efficacy in handling various
types of videos and complex tasks. Moreover,
we have endowed it with greater autonomy
and a robust tool-calling system, enabling it
to accomplish even more intricate tasks. Code:
https://github.com/om-ai-lab/OmAgent
1 Introduction
Large Language Models (LLMs) have advanced
remarkably in recent years, greatly expanding their
capabilities across various applications (Touvron
et al., 2023a,b; Bai et al., 2023; OpenAI, 2023a).
As these models have evolved, they have been in-
creasingly applied to multimodal contexts, allow-
ing them to process and interpret not just text but
also images and other media types. Initially, the
focus was on single images, utilizing the models’
ability to generate and understand detailed descrip-
tions or responses based on static visuals (Awadalla
et al., 2023; Liu et al., 2023, 2024; OpenAI, 2023b)
However, as LLMs have become more complex
and powerful, there has been growing interest inapplying them to more dynamic media, such as
video content (Lin et al., 2023a; Zhu et al., 2023;
Zhang et al., 2023b; Zhao et al., 2023; Tang et al.,
2023). This interest arises from the potential to pro-
vide deeper and more nuanced interpretations of
video data, similar to how humans understand and
interact with moving images and sound. Currently,
most video understanding models are limited to
processing short videos, typically only a few min-
utes or even several seconds long. Despite these
advancements, significant challenges remain, espe-
cially in handling long video inputs like 24-hour
CCTV footage or full-length movies, which involve
massive amounts of data and require substantial
processing power.
Traditionally, one solution has been to extract
key frames from these long videos or convert all
frames into textual descriptions before process-
ing (Lin et al., 2023b; Yang et al., 2023). While
this approach makes the task more manageable for
LLMs, it often results in information loss. Key
frame extraction might miss subtle but important
details in the omitted frames, and converting visual
data to text can oversimplify or misrepresent visual
nuances, leading to a less accurate understanding
of the content.
To address these limitations, the application
of Retrieval-Augmented Generation (RAG) tech-
nology in video understanding has emerged as a
promising solution (Chen et al., 2017; Liu, 2022;
Chase, 2022; Arefeen et al., 2024). RAG enables
the storage and efficient retrieval of video frames
based on their relevance to a query. This method
allows for more precise and contextual responses
by referencing specific content directly from the
video, rather than relying on potentially incomplete
or inaccurate textual summaries. However, given
that video is a continuous stream of information,
when it is stored, this continuous flow is segmented
into discrete blocks of data. The information lost
during this segmentation is irretrievable.arXiv:2406.16620v3  [cs.CV]  12 Nov 2024To address the aforementioned issues, we at-
tempt to analyze the human approach to handling
complex, long video question-answering tasks,
seeking breakthroughs from this perspective. When
a person watches a content-rich, lengthy video,
such as a movie, they retain a general impression
of the movie in their mind. This impression in-
cludes a rough outline of the video’s content at
various time points. When asked about specific
details, the person may not recall the details im-
mediately but can quickly locate the relevant time
point in the video and rewatch the segment to re-
trieve the missing information. The key insight
of OmAgent is to replicate this process by inte-
grating multimodal RAG and generalist AI agent.
OmAgent consists of two main components: (1)A
video2RAG video preprocessor to extract and store
the generalized information from the video, akin to
the foundational impression a video imprints upon
the viewer’s memory. (2)A Divide-and-Conquer
Loop (DnC Loop) for task planning and execution
which equipped with tool invocation capabilities.
We abstract the human ability to reposition and
review video details as a tool named "rewinder,"
which can be autonomously selected and utilized
by an AI agent, similar to how a person might use a
video player’s progress bar to navigate to points of
interest. OmAgent not only can retrieve detailed in-
formation from videos but also can actively seek ex-
ternal information, enabling more advanced video
understanding and question-answering. Existing
benchmarks are insufficient to accurately quantify
these capabilities, so we propose a new complex
video understanding benchmark to fulfill this task.
The contributions are: (1)OmAgent, the first com-
plex video understanding framework integrating
multimodal RAG and generalist AI agent. (2)A
benchmark dataset that contains 2000+ Q&A pairs
for evaluating video understanding systems. (3)Ex-
periments that shows the proposed agentic method
is able to outperform strong baselines for solving
complex video understanding problems.
2 Related Work
Video LLMs Analyzing and understanding
video content using large-scale language mod-
els (LLMs) typically involves fine-tuning or pre-
training methods. Pre-training strategies, such
as supervised or contrastive learning, develop
video LLMs, while instruction fine-tuning updates
adapter parameters to enable video comprehen-sion (Tang et al., 2023). For example, LaViLa
(Zhao et al., 2023) enhances video subtitle gener-
ation through a cross-attention module and rewrit-
ing mechanism, improving coverage and diversity.
Video-LLaMA (Zhang et al., 2023b) addresses
spatio-temporal visual variations using separate
video and audio encoders with an advanced audio-
visual Q-former, significantly boosting video com-
prehension. Video-LLaV A (Lin et al., 2023a) con-
nects multimodal representations into a unified se-
mantic space with LLMs, improving video under-
standing tasks.
However, these methods often consume a lot
of computational resources and time during the
training process, and the models can usually only
target specific tasks related to the training data.
In addition, video LLMs trained from scratch
may not be able to achieve the expected perfor-
mance when dealing with longer or previously un-
seen videos, showing shortcomings in understand-
ing long videos and dealing with complex video
question-answer tasks.
Long Video Understanding system with LLMs
LLMs and Multimodal LLMs (MLLMs) applied
to long video comprehension tasks utilize external
systems to process extensive content. This involves
analyzing visual elements, actions, scenes, and ob-
jects over time, aligning multimodal information
with textual modalities, and leveraging the power-
ful text processing capabilities of MLLMs (Tang
et al., 2023). For instance, Vlog (Kevin, 2023)
uses pre-trained models for different modalities
to record and interpret visual and audio informa-
tion, summarizing it into detailed text for MLLM
comprehension. MM-REACT (Yang et al., 2023)
employs visual expert tools via internal prompts,
enhancing MLLMs’ visual understanding. MM-
VID (Lin et al., 2023b) segments videos using
ASR and Scene Detection tools, generating and
integrating textual descriptions to complete Q&A
tasks with MLLMs. LLoVi (Zhang et al., 2023a)
uses a process that generates a summary based
on subtitles and questions, then uses the summary
for question-answering. VideoTree (Wang et al.,
2024b) employs a three-step process to understand
long videos, clustering video frames, calculating
relevance, and performing depth expansion for
question-answering. VideoAgent (Fan et al., 2024)
preprocesses videos to generate captions and object
data, and uses an agent with pre-provided tools to
obtain answers.Compared to training entirely new video LLMs,
these MLLM-based approaches significantly re-
duce the need for computational resources while
allowing the system to integrate or update external
tools according to new technical or performance
requirements. However, such long video under-
standing methodologies will lose a large amount of
video information when transforming modalities,
and do not fully utilize the multimodal processing
capabilities of MLLM. In addition, these systems
usually lack sufficient autonomy to support more
complex video questioning and interaction, which
limits their depth and breadth in practical applica-
tions.
MultiModal RAG MultiModal Retrieval Aug-
mented Generation (RAG) leverages images,
videos, audio, and other non-text data for informa-
tion retrieval, enhancing content relevance and con-
text for complex query and generation tasks (Zhang
et al., 2018). The LlamaIndex (Liu, 2022) frame-
work improves relevance and accuracy by enabling
quick retrieval and processing of multimodal con-
tent through precise embedding and efficient index-
ing. Indexify (TensorLakeAI, 2024) provides a ro-
bust framework for building multimodal RAG sys-
tems with real-time data pipelines, extractor SDKs,
and powerful storage interfaces for efficient infor-
mation extraction and indexing. iRAG (Arefeen
et al., 2024) uses AI model selection for percep-
tual queries, improving the speed and quality of
multimodal data-to-text conversion, particularly for
real-time, long video understanding.
Although these multimodal RAG systems offer
great advantages in integrating multimodal infor-
mation, they are still unable to eliminate the sig-
nificant loss of information that occurs when data
is transformed from video to knowledge in a RAG
system. Our OmAgent, on the other hand, has no
significant loss of information thanks for the task
planning and autonomous tool call capability of
DnC Loop especially the "rewinder" mechanism.
3 Method
The process of OmAgent’s video understanding can
be bifurcated into two primary parts: Video2RAG
and DnC Loop. As illustrated in Figure 1, all video
data must undergo preliminary processing before
being stored in the knowledge database in prepara-
tion for subsequent tasks. The preprocessing phase
of Video2RAG encompasses a series of model iden-
tification and vectorization procedures, culminatingin the extraction of the core content of the video
files for storage. When undertaking video under-
standing tasks, the initial step is to extract temporal
information from the query. This information will
then be used to filter the retrieved results. Subse-
quently, the query is encoded by text encoder, and
the embedding is employed to retrieve pertinent
video segment information from the knowledge
database.
The retrieved video clip information and the orig-
inal task will be transmitted to DnC Loop, the in-
telligent agent capable of autonomously planning
and executing tasks, for processing. Complex tasks
will be recursively subdivided into executable sub-
tasks. If at any point the agent deems that specific
video details need to be reviewed, it will utilize the
rewind tool to examine the relevant content. Once
all subtasks are successfully completed, the execu-
tion results will be conveyed to a node dedicated to
synthesizing the final answer.
3.1 Video to RAG
OmAgent’s preprocessing (as shown in Figure 1)
of video data is similar to a multimodal RAG. This
approach avoids treating the entire content of a very
long video as context input to the large language
model, which would lead to three serious issues:
(1) The length of the context would limit the maxi-
mum length of the video that can be processed. (2)
Using an extremely long context for each question
and answer session would cause an explosive in-
crease in token usage. (3) An overly long context
increases the difficulty of LLM inference, affect-
ing the accuracy of question and answer sessions.
OmAgent’s Video2RAG processing mainly con-
sists of the following steps.
Scene Detection Firstly, an algorithm is used
to segment the video into relatively independent
video blocks. The main purpose of this step is to
locate the key nodes of the video. We can deter-
mine whether to segment the scene by assessing
the degree of change in the frames; overly short
segments will be merged together. The extracted
video segments will have their start and end times-
tamps recorded, and 10 frames will be uniformly
sampled from every segment.
Visual Prompting During the video preprocess-
ing stage, additional algorithms can be used to pro-
vide more information. For example, using facial
recognition, we can obtain information about the
characters in the video. OmAgent will annotateA S R  
a l g o r i th m sT e x t  R e p r e s e n t a t i o n  o f  A u d i o' t e x t ' :  "  W h a t ' s  u p ,  m a n ?  W e ' r e  h e r e ,  M r .  R o y .  T h i s  i s  i t .  T h i s  i s  
th e  d a y  w e  m a k e  i t  h a p p e n  f o r  C r e t .  Y o u ' r e  th e  m a n ,  M r .  R o y .  
Y o u ' r e  th e  m a n . " ,  
' s t a r t ' :  1 2 2 . 0 0 5 ,   ' e n d ' :  1 4 9 . 1 3 8
. . . . . .
' t e x t ' :  "  W e  g o o d ?  J e s u s  C h r i s t .  H e y ,  h e y ,  h e y ,  b u d d y .  G o o d  t o  
s e e  y o u .  S o ,  w e  r e a d y  t o  w h a t ?  Y e a h ,  o k a y .  L o o k ,  I ' m  r e a l l y  
s o r r y ,  b u t  i t  l o o k s  l i k e  th e r e ' s  a n  i s s u e . " ,  
' s t a r t ' :  2 5 7 . 9 2 7 ,   ' e n d ' :  2 7 4 . 7 3 5
. . . . . .F a c i a l  
R e c o g n i t i o n
J e r e m y  S t r o n gS a r a h  S n o o kV i s u a l  P r o m p t i n g
V i d e o  c l i p  1V i d e o  c l i p  2V i d e o  c l i p  3S c e n e  D e t e c t i o n
V i d e oS c e n e  C a p t i o n
M L L M s
V i d e o 2 R A G
“ W h i c h  o th e r  s h o w s  h a s  
th e  a c t o r  w h o  p l a y e d  i n  
th e  v i d e o  a t  5  m i n u t e s  
a n d  4 1  s e c o n d s  
a p p e a r e d  i n ? ”Q u e r yK n o w l e d g e  
D a t a b a s eE x t r a c t i n g  
t i m e s t a m pF i l t e r i n g  b y  t i m e s t a m p sR e t r i e v i n g  b y  q u e r y
S c e n e  
D e s c r i p t i o nD n C  L o o p“ T h e  a c t o r  w h o  p l a y e d  i n  th e  v i d e o  a t  5  
m i n u t e s  a n d  4 1  s e c o n d s  i s  J e r e m y  S t r o n g .  H e  
i s  k n o w n  f o r  h i s  r o l e  a s  K e n d a l l  R o y  i n  th e  H B O  
d r a m a  s e r i e s  ' S u c c e s s i o n '  ( 2 0 1 8 – 2 0 2 3 ) .  O th e r  
n o t a b l e  s h o w s  a n d  f i l m s  h e  h a s  a p p e a r e d  i n  
i n c l u d e : -  ' T h e  T r i a l  o f  th e  C h i c a g o  7 '  ( 2 0 2 0 )  a s  
J e r r y  R u b i n -  ' M o l l y ' s  G a m e '  ( 2 0 1 7 )  a s  D e a n  
K e i th -  ' T h e  B i g  S h o r t '  ( 2 0 1 5 )  a s  V i n n y  D a n i e l -  
' S e l m a '  ( 2 0 1 4 )  a s  J a m e s  R e e b -  ' L i n c o l n '  ( 2 0 1 2 )  
a s  J o h n  G e o r g e  N i c o l a y -  ' Z e r o  D a r k  T h i r t y '  
( 2 0 1 2 )  a s  T h o m a s -  ' T h e  G e n tl e m e n '  ( 2 0 1 9 )  a s  
M a t th e w J e r e m y  S t r o n g  h a s  a l s o  p e r f o r m e d  i n  
th e a t e r ,  i n c l u d i n g  h i s  B r o a d w a y  d e b u t  i n  ' A  
M a n  f o r  A l l  S e a s o n s '  ( 2 0 0 8 )  a n d  ' A n  E n e m y  o f  
th e  P e o p l e '  ( 2 0 2 4 ) . ”A g e n t  A n s w e rC o n c l u s i v e
S y n th e s i sO m A g e n tFigure 1: How OmAgent understand video. In Video2RAG, the video is processed by different algorithms (e.g.
Scene Detection, ASR and face recognition) and then summarized by MLLMs to generate Scene Captions. Those
captions are encoded and saved in the knowledge database. When OmAgent receives a query, it filters and retrieves
in knowledge database based on timestamps (if available). The retrieved information is processed by the Divide-
and-Conquer Loop and summarized by Conclusive Synthesis to generate the final answer.
this algorithmic information directly on the im-
ages through visual prompting, i.e., drawing the
corresponding recognition boxes and using text to
explain above the bounding box. This allows for
the full utilization of the powerful understanding
capabilities of MLLMs.
Text Representation of Audio The audio infor-
mation in the video is as important as the visual
information. OmAgent uses ASR algorithms to
convert the speech in the video into text and em-
ploys speaker diarization algorithms to distinguish
between different speakers.
Scene Caption Using MLLMs that support mul-
tiple images, each video segment’s content is sum-
marized. The inputs include video frames that have
already been annotated with visual prompting and
the transcribed audio information. In the process
of generating dense captions at this step, We have
delineated a set of pivotal elements to guide theMLLM in generating effective and comprehensive
captions, ensuring that vital information is not over-
looked in the absence of explicit objectives. OmA-
gent specifies the following dimensions as instruc-
tions to MLLMs:
•The time information of the current video clip in
terms of periods like morning or evening, seasons
like spring or autumn, or specific years and time
points.
•Describe the location where the current event is
taking place, including scene details.
•Provide a detailed description of the current char-
acters, including their names, relationships, and
what they are doing, etc.
•List all the detailed events in the video content in
chronological order.
•Give some detailed description of the scene of
the video. This includes, but is not limited to,
scene information, textual information, character
status expressions, and events displayed in thevideo.
•Provide an overall description and summary of
the content of this video.
Encode and Save The final step in video process-
ing involves vectorizing the scene captions and stor-
ing them in a vector database (knowledge database).
Additionally, the original text of the captions is also
stored in the memory for keyword-based retrieval.
The start and end timestamps of the video segments
are used as filtering fields and are likewise stored
in the memory repository of the OmAgent agent.
3.2 Divide-and-Conquer Loop
In computer science, divide-and-conquer (DnC)
is a highly classical algorithm design paradigm.
A divide-and-conquer approach entails the itera-
tive decomposition of a problem into multiple sub-
problems. This process continues until the sub-
problems reach a level of simplicity that allows for
direct resolution (Zhao et al., 2016). The solutions
to these sub-problems are subsequently merged to
yield a resolution to the initial problem. In order to
ensure that OmAgent is not limited to simple video
Q&A functionality but possesses robust problem-
solving capabilities, we initially aimed to build a
general task-solving agent system when designing
and constructing the agent framework. Inspired
by XAgent’s (Team, 2023) double looped plan-
ner, we designed an agent framework based on
the divide-and-conquer task processing loop (DnC
Loop), which is capable of performing recursive
task decomposition and execution. The DnC Loop
task-solving procedure is shown in Algorithm 1.
Conqueror Conqueror is the entry point of the
DnC loop. It is responsible for evaluating and pro-
cessing the current task. For a given task, Con-
queror may return one of the following three types
of results:
•If the current task is too complex and needs to
be divided into subtasks, Conqueror returns the
reason for the division.
•If the execution of the current task requires the
use of a specific tool, Conqueror returns both the
task information and the tool information. These
pieces of information will be passed to the tool
execution module for tool invocation.
•If the current task can be answered directly by
the LLM, return the result directly.
Conqueror will detect the depth of the task tree and
terminate task execution when it exceeds the user’s
setting to prevent tasks from being infinitely split.Algorithm 1 DnC Loop of OmAgent
Require: The input Query UserTask ; The max depth of the
TaskTree N
Initialize Task = TaskTree.init( UserTask )
procedure DNC(Task, N )
Result ←Conqueror (Task )
ifResult. type =“too complex” then
Subtasks ←Divider (Task, Result. reason )
ifSubtasks. success then
Task .add(Subtasks .tasks)
for all Subtask ∈Task. subtasks do
ifTask. depth≤Nthen
DNC(Subtask, N )
else
return “Task tree depth exceeded”
end if
end for
else
return Subtasks. reason
end if
else if Result. type =“requires tool” then
ToolResult ←ToolCall (Task, Result. tool)
Task. update (Task, ToolResult )
return ToolResult
else if Result. type =“direct answer” then
Task. update (Task, Result. answer )
return Result. answer
end if
end procedure
The position of Conqueror in the whole process is
shown in Figure 2.
Divider The Divider component is responsible
for breaking down complex tasks into simpler ones
while ensuring that the execution results of these
simple tasks are equivalent to the original task.
When the Conqueror component determines the
necessity of task division, it delegates the task to
the Divider for attempted division. Successfully di-
vided tasks are then integrated into the Task tree as
child nodes of the original task node. If the division
fails, the Divider is asked to provide the reason.
Rescuer Rescuer is an auxiliary module in the
Conqueror’s execution process. It attempts to re-
pair issues and ensure the smooth completion of
the Conqueror’s execution when errors occur. A
typical scenario is when the agent tries to execute
a piece of code, but a required package is missing
in the environment. The Rescuer can attempt to
fix the runtime environment issue. The position of
Divider in the whole process is shown in Figure 2.
Task tree In software development practice,
divide-and-conquer is often implemented using re-
cursion. OmAgent uses recursive tree structure to
store all the paths of task execution. With the help
of the Loop node, it achieves recursive operationsD i v i d e r" c u r r e n t _ s t a g e " :  " V i d e o C o n q u e r o r " ,
" t a s k " :  " W h i c h  o th e r  s h o w s  h a s  th e  
a c t o r  w h o  p l a y e d  i n  th e  v i d e o  a t  5  
m i n u t e s  a n d  4 1  s e c o n d s  a p p e a r e d  
i n ? " ,
" t a s k _ d e p th " :  1 ,N e e d  t o  d i v i d e  u p 
c o m p l e x  t a s k s
S u c c e s s f u l  t a s k  e x e c u t i o nD e t e c t i n g  e x e c u t i o n  
e x c e p t i o n sC o n q u e r o rR e s c u e rR e m e d i a t i o n  
o f  e x c e p t i o n s
" S t a g e _ s t a t u s " :  
" s u c c e s s "" S t a g e _ s t a t u s " :  
" F a i l e d "F a i l e d  t a s k  e x e c u t i o nF a i l e d  t o  d i v i d e  th e  t a s kD i v i d i n g  th e  T a s k  C o m p l e t i o nS u b  T a s k 3 :  P e r f o r m  a  w e b  
s e a r c h  t o  f i n d  o th e r  s h o w s  
th e  i d e n t i f i e d  a c t o r  h a s  
a p p e a r e d  i n . "S u b  T a s k 2 :  U s e  f a c e  
r e c o g n i t i o n  t o  i d e n t i f y  
th e  a c t o r  i n  th e  
e x t r a c t e d  f r a m e .S u b - t a s k s  c o n t i n u e  t o  e x e c u t eS u b  T a s k 1 :  E x t r a c t  a  
f r a m e  f r o m  th e  v i d e o  a t  
5  m i n u t e s  a n d  4 1  
s e c o n d s .
C a l l i n g  t o o l sR e t u r n s  th e  r e s u l t
o f  a  t o o l  c a l l sT o o l  M a n a g e rS c e n e  D e s c r i p t i o n
U s e r :  
“ W h i c h  o th e r  s h o w s  
h a s  th e  a c t o r  w h o  
p l a y e d  i n  th e  v i d e o  a t  
5  m i n u t e s  a n d  4 1  
s e c o n d s  a p p e a r e d  
i n ? ”
C o n c l u s i v e  
S y n th e s i s
T o o l  M a n a g e r
R e w i n d e r
F a c e  R e c o g n i t i o nF r a m e  E x t r a c t i o nW e b  S e a r c hF i l e s  R WU s e r  Q u e r yD i v i d e - a n d - C o n q u e r  L o o pFigure 2: Divider and Conqueror Loop task-solving procedure. In the DnC Loop, simple problems are directly
executed by Conqueror, while complex problems are split by Divider until they can be executed. The Rescuer
recognizes exceptions and retries the task. The Tool Manager organizes the external tools. It is worth mentioning
that the Rewinder tool can goes back through the entire video to find information and missing details. Finally, the
DnC loop outputs the relevant content whether the execution fails or succeeds.
for task decomposition and execution.
3.3 Tool call
As a standard capability of intelligent agents, the
principle of tool calling lies in utilizing the pow-
erful logical generation ability of large language
models (LLMs) to generate corresponding tool in-
vocation request parameters based on task informa-
tion. In addition to conventional tools, OmAgent
specifically offers a video detail rewinder tool for
further information extraction within specific time
ranges of a video. OmAgent can autonomously
choose to view details of a particular segment of a
video when necessary, addressing the issue of infor-
mation loss that occurs when video data transitions
from a continuous information source to a discrete
one during the preprocessing stage.
Furthermore, OmAgent provides conventional
tools such as internet search tools, facial recogni-
tion tools, and file processing tools to meet more
complex user tasks.
4 Experimental Settings
To validate the efficacy of the OmAgent system
in addressing complex problems within real-worldscenarios, we have designed a two-phase experi-
mental approach:
4.1 General problem-solving capabilities
We hypothesize that understanding lengthy and in-
tricate videos relies significantly on an agent’s com-
prehensive problem-solving skills. To test this, we
used two general-purpose intelligent benchmarks,
MBPP (Austin et al., 2021) and FreshQA (Vu et al.,
2023). We focused on the DnC Loop’s ability to
plan and execute tasks and its proficiency in utiliz-
ing tools to address complex issues
Datasets The Mostly Basic Programming Prob-
lems (MBPP) benchmark includes 976 elementary
Python coding tasks. These problems are designed
to evaluate the system’s proficiency to plan solu-
tions, select and invoke the right tools, and fix er-
rors effectively.
FreshQA is a continuously updated collection of
real-world questions and answers, reflecting the
constantly changing nature of reality. As a re-
sult, FreshQA focuses on the system’s ability to
learn and integrate new information from external
sources.Settings To study OmAgent’s capability in com-
prehending complex long-form videos, we devised
two control groups:
First, we aimed to ascertain the performance that
could be achieved by one of the most advanced
MLLMs - GPT-4o, based solely on a limited num-
ber of video frames (restricted to 20 by the Mi-
crosoft Azure GPT-4o service) and basic dialogue
textual information. In this experiment, we initially
extracted 20 frames evenly from the video and
paired them with the dialogue text obtained through
Whisper (Radford et al., 2023) as the context for
input into the MLLM for question-answering.
In the second control experiment, we sought
to evaluate the strengths and weaknesses of us-
ing a multimodal RAG approach compared to our
Agent strategy. To ensure a fair comparison, we
isolated the Video2RAG component from OmA-
gent to serve as the RAG system. When a query is
input, the system first retrieves relevant video clip
information from the knowledge database, then in-
puts this pertinent data as context into the MLLM
for question-answering.
4.2 Long-form videos understanding
capabilities
We created a benchmark with over 2000 question-
answer pairs to evaluate OmAgent’s ability to un-
derstand, answer questions, and recall detailed in-
formation from long-form videos. We aimed to as-
sess the general understanding of ultra-long video
content and the ability to recall specific details. Our
benchmark was designed to be logically coherent
and narrative-rich, with video segments up to an
hour long. This benchmark allows us to measure
the capabilities of our intelligent agent and compare
its performance against large language models.
Datasets Publicly available long-form video un-
derstanding datasets are very scarce, similar to the
FreshQA/MovieQA datasets which only contain
videos at the minute level, and the questions are
not complex enough, for example, "How does Talia
die?" in MovieQA, which is a question that can be
inferred from consecutive frames, cannot meet our
needs. SOK-Bench (Wang et al., 2024a) addresses
scenarios slightly different from ours, as it demands
the testing program to integrate situated and general
knowledge to answer questions. MoVQA is a long
movie question and answer dataset that utilizes
100 well-known movies to create complex long
video question-answer pairs, which align with ourrequirements, but it is not yet open-sourced. There-
fore, we created the dataset ourselves. We collected
some long videos familiar to annotators, selected
the top 100 videos in terms of frequency, and cre-
ated 20 questions for each video. These questions
were first proofread by two different annotators and
then revised by a third annotator. Videos include
episodes and movies, varieties, documentaries, and
vlogs. These types of videos exhibit significant dif-
ferences in themes, filming and editing techniques,
data density, scene lengths, and alignment of video
audio and visuals, fully demonstrating the diversity
of the data.
Settings To evaluate the system’s understanding
of long-form videos and timelines, we defined ques-
tions in four categories: reasoning, information
summary, event localization, and external knowl-
edge.
•Reasoning involves deducing relationships be-
tween events.
•Information summary extracts key details from
specific timestamps.
•Event localization tests timeline accuracy.
•External knowledge requires finding relevant
but unmentioned information.
Reasoning, information summary, and external
knowledge questions are multiple-choice. Event
localization requires precise timestamps or time
spans, with a deviation within ±2 seconds for times-
tamps and an IoU exceeding 90% for time spans.
5 Results and Analysis
5.1 General problem-solving capabilities
GPT-4, recognized as a benchmark for evaluating
Large Language Models, is known for its strong
reasoning abilities and serves as the baseline model
for our experiments. XAgent, an advanced agent
system, features a well-designed Dual-Loop Mech-
anism that allows it to address problems from both
broad and detailed perspectives.
MBPP FreshQA
GPT4+OmAgent 88.3% 79.7%
GPT4+XAgent 84.2% 74.0%
GPT4 80.01% 67.0%
Table 1: Results on MBPP and FreshQA comparing
with GPT4 and XAgent. Showing OmAgent has a
strong generalized task solving capbility.Vlog Episode and Movies Variety Documentary Total
OmAgent 57.14% 56.25% 23.53% 36.84% 45.45%
Video2RAG 42.86% 32.35% 19.88% 31.57% 27.27%
Frames with STT 42.85% 29.41% 17.64% 31.58% 28.57%
VideoAgent 41.72% 23.53% 11.76% 26.32% 23.38%
VideoTree 34.52% 31.48% 21.27% 27.35% 26.76%
LLoVi 28.57% 24.16% 17.65% 21.05% 23.63%
Table 2: Results of different types of videos on OmAgent and the other five baselines.
Reasoning Event Localization Information Summary External Knowledge
OmAgent 81.82% 19.05% 72.74% 57.21%
Video2RAG 72.73% 4.76% 50.17% 23.36%
Frames with STT 63.64% 2.38% 63.63% 19.46%
VideoAgent 64.66% 2.25% 45.45% 23.78%
VideoTree 35.30% 18.62% 47.27% 29.57%
LLoVi 27.27% 11.90% 45.46% 24.57%
Table 3: Results of different types of queries on OmAgent and the other five baselines.
5.2 Long-form videos understanding
capbilities
The results in Table 1 clearly show that both agent
systems outperform the basic inferential capabil-
ities of GPT-4 alone. Notably, OmAgent sur-
passes XAgent (Team, 2023) in overall perfor-
mance. Analysis reveals that XAgent’s Dual-Loop
Mechanism, while thorough, often leads to over-
thinking and complicates problem-solving. In con-
trast, OmAgent’s Rescuer mechanism proves more
effective, especially in handling code-related tasks.
This mechanism enables OmAgent to dynamically
correct issues based on real-time results, leading to
superior performance.
Table 2 compares the scores of five baselines
and OmAgent across different types of long-form
video understanding. OmAgent achieved the high-
est scores. Vlogs, variety, and documentaries con-
tain extensive narration, so the STT data and the
resulting scene captions encompass most relevant
information of the video. Therefore, the perfor-
mance difference between OmAgent and the other
two methods in these categories is not as significant
as in episodes and movies. In episodes and movies,
scenes change frequently, complex queries involve
cross-scene information, and STT data might span
scene transitions. Compared to frames with STT,
Video2RAG retrieves data related to the query, re-
ducing data redundancy, and thus has higher scores
than frames with STT. However, since it only re-
trieves relevant information from a vector database,
complex questions such as "Are there any scenechanges between 03:58 and 04:02, and what is their
connection?" are not achieved. On the other hand,
OmAgent’s DnC Loop breaks down complex ques-
tions into several sub-questions, including "Extract
frames between 03:58 and 04:02," "Analyze the
extracted frames to identify any scene changes,"
and "Determine the connection between the scenes
based on the identified changes." By leveraging
the rewinder capability, it pinpoints the relevant
segments for rewatching, thereby arriving at the
correct answer.
Furthermore, we conducted a detailed analysis of
different question types. Table 3 provides a compar-
ison of OmAgent and five baselines in terms of rea-
soning, event localization, information summary,
and external knowledge. The results show that
OmAgent achieves the highest scores in all four
types of questions. Through its rewinder capability,
OmAgent can extract more detailed video informa-
tion and accurately locate timestamps, which leads
to significant improvements in reasoning, event lo-
calization, and information summarization tasks
compared to frames with STT and Video2RAG.
External knowledge task has stricter requirements
for information retrieval. Although GPT can an-
swer some questions through its own capabilities
and scene information, OmAgent achieves higher
scores by utilizing various external tools (such as
facial recognition, web search, etc.) to obtain more
accurate relevant information. Notably, in the ques-
tion type of information summary, Video2RAG
scored lower than frames with STT. Analysis revealthat video2RAG’s information source came from
scene captions generated based on STT and images
within the scene, which had a certain probability
of omissions compared to the original STT, thus
affecting the scores. The higher frame extraction
frequency of VideoAgent, VideoTree and LLoVi al-
lowed them to capture more visual information, but
their lack of audio processing led to suboptimal per-
formance in many scenarios. VideoAgent’s lack of
DnC and rewinder capabilities limited its effective-
ness in query types requiring detailed information
comparing with OmAgent. Although OmAgent
performed the best across all question types, it still
scores relatively low in the event localization task.
Detailed study and analysis reveal that LLM tends
to directly use timestamps or time spans within
the scene, leading to a lack of precision in answers.
OmAgent’s DnC loop and rewinder capabilities can
mitigate this issue but cannot completely resolve
it.
6 Conclusion
OmAgent is a powerful video comprehension agent
that integrates multimodal RAG with a generalist
AI agent, enabling several advanced capabilities.
It offers a theoretical near-infinite length video un-
derstanding capacity and incorporates a secondary
recall mechanism for detailed video information,
which significantly mitigates information loss. Ad-
ditionally, OmAgent autonomously invokes tools
based on video comprehension tasks, allowing it to
execute more intricate operations. These capabil-
ities have resulted in remarkable performance on
the MBPP, FreshQA, and our proposed long video
complex task test datasets.
7 Limitations
•When determining the positioning of an event
in a video, LLM tends to directly use the
timestamps given in the scene. OmAgent can
alleviate this situation through DnC loop and
rewinder but cannot completely resolve it.
•The character information in long-form videos
is usually diverse. For example, character Lo-
gan can be both a boss and a father, making
it difficult to align back to the same person,
causing OmAgent to fail in precise position-
ing. It is necessary to add a visual prompt to
the character for suppression.
•The phenomenon of audio-visual asynchronyis prominent in long-form videos. For exam-
ple, in documentaries, the picture might not
change significantly while the speech has al-
ready shifted to introduce other scenes, result-
ing in a misalignment between the picture and
the speech. Additionally, STT only processes
the speech of characters, losing other audio
information such as background music, sound
effects, and different characters’ voiceprints.
8 Acknowledgements
This research is partially supported by National
Key R&D Program of China under grant No.
2022YFF0902600
References
Md Adnan Arefeen, Biplob Debnath, Md Yusuf Sarwar
Uddin, and Srimat Chakradhar. 2024. irag: An in-
cremental retrieval augmented generation system for
videos. arXiv preprint arXiv:2404.12309 .
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 .
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes-
sel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,
Yonatan Bitton, Samir Gadre, Shiori Sagawa, Je-
nia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel
Ilharco, Mitchell Wortsman, and Ludwig Schmidt.
2023. Openflamingo: An open-source framework for
training large autoregressive vision-language models.
arXiv preprint arXiv:2308.01390 .
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-
uan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
Zhu. 2023. Qwen technical report. arXiv preprint
arXiv:2309.16609 .
Harrison Chase. 2022. LangChain.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. arXiv preprint arXiv:1704.00051 .
Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi
Li, Zhi Gao, and Qing Li. 2024. Videoagent: A
memory-augmented multimodal agent for video un-
derstanding. arXiv preprint arXiv:2403.11481 .Stan Kevin. 2023. VLog: Transform Video as a Docu-
ment with ChatGPT, CLIP, BLIP2, GRIT, Whisper,
LangChain.
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and
Li Yuan. 2023a. Video-llava: Learning united visual
representation by alignment before projection. arXiv
preprint arXiv:2311.10122 .
Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin,
Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang,
Lin Liang, Zicheng Liu, Yumao Lu, et al. 2023b.
Mm-vid: Advancing video understanding with gpt-
4v (ision). arXiv preprint arXiv:2310.19773 .
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan
Zhang, Sheng Shen, and Yong Jae Lee. 2024. Llava-
next: Improved reasoning, ocr, and world knowledge.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023. Visual instruction tuning.
Jerry Liu. 2022. LlamaIndex.
OpenAI. 2023a. Gpt-4. https://openai.com/gpt-4 .
OpenAI. 2023b. Gpt-4v(ision), system-card. https:
//openai.com/research/gpt-4v-system-card ,
2023.
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-
man, Christine McLeavey, and Ilya Sutskever. 2023.
Robust speech recognition via large-scale weak su-
pervision. In International Conference on Machine
Learning , pages 28492–28518. PMLR.
Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan
Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang
Lin, Rongyi Zhu, et al. 2023. Video understanding
with large language models: A survey. arXiv preprint
arXiv:2312.17432 .
XAgent Team. 2023. Xagent: An autonomous agent for
complex task solving.
TensorLakeAI. 2024. Indexify: A realtime and indexing
and structured extraction engine for unstructured data
to build generative ai applications.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.
Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry
Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny
Zhou, Quoc Le, and Thang Luong. 2023. Freshllms:
Refreshing large language models with search engine
augmentation.
Andong Wang, Bo Wu, Sunli Chen, Zhenfang Chen,
Haotian Guan, Wei-Ning Lee, Li Erran Li, and
Chuang Gan. 2024a. Sok-bench: A situated
video reasoning benchmark with aligned open-world
knowledge. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition ,
pages 13384–13394.
Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jae-
hong Yoon, Feng Cheng, Gedas Bertasius, and Mo-
hit Bansal. 2024b. Videotree: Adaptive tree-based
video representation for llm reasoning on long videos.
arXiv preprint arXiv:2405.19209 .
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin
Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,
Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-
react: Prompting chatgpt for multimodal reasoning
and action. arXiv preprint arXiv:2303.11381 .
Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang
Wang, Shoubin Yu, Mohit Bansal, and Gedas Berta-
sius. 2023a. A simple llm framework for long-
range video question-answering. arXiv preprint
arXiv:2312.17235 .
Hang Zhang, Xin Li, and Lidong Bing. 2023b. Video-
llama: An instruction-tuned audio-visual language
model for video understanding. arXiv preprint
arXiv:2306.02858 .
Jiaping Zhang, Tiancheng Zhao, and Zhou Yu. 2018.
Multimodal hierarchical reinforcement learning pol-
icy for task-oriented visual dialog. In Proceedings of
the 19th Annual SIGdial Meeting on Discourse and
Dialogue , pages 140–150.
Tiancheng Zhao, Kyusong Lee, and Maxine Eskenazi.
2016. Dialport: Connecting the spoken dialog re-
search community to real user data. In 2016 IEEE
Spoken Language Technology Workshop (SLT) , pages
83–90. IEEE.
Yue Zhao, Ishan Misra, Philipp Krähenbühl, and Ro-
hit Girdhar. 2023. Learning video representations
from large language models. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 6586–6597.
Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui,
HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu
Zhang, Zongwei Li, et al. 2023. Languagebind: Ex-
tending video-language pretraining to n-modality by
language-based semantic alignment. arXiv preprint
arXiv:2310.01852 .
A Case Study
In this section, we will illustrate the process by
which OmAgent addresses complex video under-
standing problems through several examples. The
primary aspects showcased include: scene cap-
tion, inner steps, and agent output. The scene cap-
tion displays sample data stored in the knowledge
database after the Video2RAG preprocessing; the
inner steps reveal the task decomposition and ex-
ecution within the DnC Loop; the agent output
presents the final results produced by OmAgent.
A.1 Case 1
In this case in Figure 3, the video we utilized is
from the series "Succession," Season 1, Episode
1. The question posed was, "Which other shows
has the actor who played in the video at 5 minutes
and 41 seconds appeared in?" This is an intricate
query, as the system must first identify the indi-
vidual at the specified timestamp and subsequently
gather information about the actor from the internet.
Observably, the intelligent system deconstructed
the task into three sub-tasks and then sequentially
employed relevant tools to address each one, ulti-
mately arriving at the correct answer.
A.2 Case 2
In this case as showing in Figure 4, the video we
utilized is from the series "Succession," Season 1,
Episode 1. The question posed was, "When was the
first time a cigarette dropped to the ground?" This
task requires OmAgent to locate a highly detailed
scene within a lengthy video. OmAgent initially
divides this problem into three subtasks. Among
these, the task of identifying the key frame is too
complex to be completed directly and is further di-
vided into four subtasks. It necessitates extracting
details and summarizing results from three seg-
ments of the video. Ultimately, all the subtasks
are successfully handled, and OmAgent returns the
correct answer.B Sample Experiment Results
Tables 4, 5, and 6 show 20 examples of OmAgent’s
performance on 2000 long-form video understand-
ing questions. In these tables, "Video" refers to
the video names, and "Question" refers to the ques-
tions in the dataset, with the choice options directly
following the questions. OmAgent represents our
answer output, while Video2RAG and frames with
stt are the outputs of two control groups.Video Question Question
TypeGround
TruthOmAgent Video2RAG Frame with
STT
Succession
S01EP01When was the first time a cigarette dropped to
the ground?Event Local-
ization00:02:32 00:02:32 [00:02:18,
00:02:45]Fail to an-
swer
Succession
S01EP01The task was to determine if the video shows
a theme park? Please choose from the fol-
lowing options: a. The playground’s interior
scenes appeared between 08:58 and 09:49. b.
The playground’s interior scenes appeared af-
ter 09:50. c. The playground’s interior scenes
were shown before 08:57. d. No playground
scenes appeared during this time.Reasoning a a a None of a, b,
c, d
Succession
S01EP01In which time period does the headphone first
appear?Event Local-
ization[00:01:46,
00:01:49][00:01:46,
00:01:49][00:04:15,
00:04:27]0:01:46
Succession
S01EP01What important event occurs between the 34th
and 35th minute? Choose from the following
options: a. Connor gets angry in the bath-
room, breaks the mirror and sink. b. Connor
gets angry in the bathroom, vents his anger
by smashing a vase, shouting loudly, etc. c.
Connor gets angry in the bathroom, kicks over
the toilet and bathtub. d. Connor gets angry in
the bathroom, smashes the door and windows.Reasoning b b b b
Westworld
S01EP01Please summarize the content of this video.
Choose from the following options: a. The
video starts with a dialogue in the protago-
nist’s dream, introducing a Western world live-
action game built by modern people that at-
tracts various customers to experience. The
protagonist, as the main NPC, repeats the same
life continuously. b. After an update in the
Western world game, NPCs start experiencing
various minor bugs. This puzzles the program-
mers behind the scenes, who begin inspecting
the problematic NPCs. Initially, only some
NPCs experience issues like lag during opera-
tion, which the programmers do not pay much
attention to after updating them. c. The protag-
onist’s father awakens to consciousness after
seeing a photo of the modern world and recites
a line from a Shakespearean work to the pro-
tagonist. This triggers uncontrollable events
in the entire game world. d. The adminis-
trator questions both the protagonist and her
father. The protagonist enters a state of con-
sciousness during the conversation and claims
to retaliate against the boss. The dialogue with
the protagonist echoes the dream at the begin-
ning, foreshadowing the occurrence of certain
events.information
summarya, b, c b a a
Westworld
S01EP01Please find the specific time when the female
protagonist wakes up from the bed.Event Local-
ization00:02:46,
00:14:31,
00:44:28,
01:04:1500:44:23 [00:00:00,
00:00:04][00:14:26,
00:14:32]
Westworld
S01EP01From 01:04:03 to 01:05:59, what changes oc-
curred in the position of the female protagonist,
and what do these changes indicate? Choose
from the following options: a. During this
time, the female protagonist went to her home
in Westworld but did not return to the mod-
ern laboratory, indicating that Westworld is
real. b. During this period, the female protago-
nist’s movements from the modern laboratory
to Westworld and then to her home reveal the
authenticity of her life. c. During this period,
the female protagonist’s movements from the
laboratory to Westworld and then to her home
imply that Westworld and her home are real.
d. In this time frame, the female protagonist’s
movements from the modern laboratory to the
home in Westworld and back to the modern
laboratory indicate the falsity of Westworld.Reasoning d d d d
Table 4: OmAgent Experiment ResultsVideo Question Question
TypeGround
TruthOmAgent Video2RAG Frame with
STT
Westworld
S01EP01Why was Dolores’ father being inpsected?
Choose from the following options, it’s a mul-
tiple choices question: a. Because his pro-
gram malfunctioned and he couldn’t continue
to function properly. b. Because he dis-
played emotions that a host NPC shouldn’t
have. c. Because he was killed by bandits
in the mountains and needed servicing to be
restored. d. Dolores’ father was serviced be-
cause he showed strong motives and drives
when questioned.Reasoning b, d b, d a a
Tempting
Fortune
S01EP01In the video, when was the first money spent
and what was it spent on?Event Local-
ization00:31:14 [00:31:08,
00:31:11][00:04:14,
00:04:26][00:05:51,
00:05:57]
Tempting
Fortune
S01EP01Please describe the scene at 42:40. Select from
the following options: a. A group of people
are gathered around a hearth roasting food,
with several lamps extinguished nearby. b. A
group of people are sitting around a campfire,
enjoying dinner, with many lights illuminated
nearby. c. A large crowd is dining around
a candelabrum, with many lamps turned off
nearby. d. Some people are singing around a
bonfire, with dim lights in the vicinity.information
summaryb b None of a, b,
c, db
Tempting
Fortune
S01EP01At what time in the video does someone fall
into the river?Event Local-
ization00:45:56 00:45:54 None of a, b,
c, d[00:29:38,
00:30:17]
Tempting
Fortune
S01EP01At what time did all team members climb up
the hill?Event Local-
ization[00:12:45,
00:12:51][00:12:37,
00:12:51]None of a, b,
c, dNone of a, b,
c, d
A Perfect
PlanetWhen do volcanic eruptions occur in the
video?Event Local-
ization[00:01:37,
00:03:55],
[00:14:30,
00:15:00],
[00:43:24,
00:44:05][0:01:36,
0:01:41],
[0:03:03,
0:03:06],
[0:03:21,
0:03:28],
[0:03:28,
0:03:37],
[0:03:45,
0:03:55],
[0:14:30,
0:14:37],
[0:14:37,
0:15:00],
[0:17:15,
0:18:09],
[0:43:24,
0:43:36],
[0:43:36,
0:43:50],
[0:43:58,
0:44:05][0:01:36,
0:01:41],
[0:01:52,
0:02:15],
[0:02:40,
0:02:53],
[0:14:30,
0:14:37],
[0:43:24,
0:43:36],
[0:43:36,
0:43:50],
[0:43:58,
0:44:05][0:02:40,
0:02:53],
[0:03:03,
0:03:06],
[0:04:04,
0:04:10],
[0:15:00,
0:15:10],
[0:15:40,
0:15:57],
[0:18:09,
0:19:10],
[0:21:23,
0:21:31],
[0:26:19,
0:26:43],
[0:31:37,
0:31:38],
[0:38:18,
0:38:32],
[0:43:07,
0:43:16],
[0:48:04,
0:48:34]
A Perfect
PlanetWhat happened between 10:38 and 10:53?
Choose from the following options: a. A little
chick couldn’t overcome the hardships on the
way and finally collapsed exhausted in a mud
pit. b. A little chick encountered many diffi-
culties on the road and eventually collapsed
on the grass from exhaustion. c. A bird en-
countered many obstacles during flight and
eventually collapsed in the desert. d. A little
chick went through various hardships and fi-
nally collapsed exhausted on a rock.information
summarya a b a
A Perfect
PlanetWhen did the Galapagos land iguana appear in
the video?Event Local-
ization[00:15:58,
00:21:47][0:21:31,
0:21:42][0:15:00,
0:21:31][0:15:57,
0:16:47]
A Perfect
PlanetWhen did the East African Rift Valley appear
in the video?Event Local-
ization[00:44:12] [0:44:05,
0:44:18][0:44:05,
0:44:18][0:44:05,
0:45:59]
Table 5: OmAgent Experiment ResultsVideo Question Question
TypeGround
TruthOmAgent Video2RAG Frame with
STT
WWDC
2024 Recap:
Is Apple
Intelligence
Legit?What new features mentioned in the video
have Android users long been able to enjoy?
a. The ability to freely arrange and customize
home screen icons. b. Cleanup tool in the
photo app. c. Remote control of other peo-
ple’s devices. d. Hide apps.Reasoning a, b a, b, d a, d a, b, c, d
WWDC
2024 Recap:
Is Apple
Intelligence
Legit?What is the author’s attitude towards the iPad
OS update? a. Disappointing. b. Exciting. c.
There are some great upgrades, but not quite
enough. d. Not mentioned.Reasoning c c c c
WWDC
2024 Recap:
Is Apple
Intelligence
Legit?At 14 minutes and 26 seconds, what address
is displayed on the phone? a. Waterbar. b.
San Francisco Fisherman’s Wharf. c. San
Francisco International Airport. d. Ronald
Reagan Airport.information
summaryc c b b
WWDC
2024 Recap:
Is Apple
Intelligence
Legit?At what time in the video does the host pro-
mote their channel?Event Local-
ization[0:05:54,
0:06:03][0:05:24,
0:06:03][0:06:03,
0:06:09][0:06:03,
0:06:09]
Table 6: OmAgent Experiment ResultsA g e n t :   
“ T h e  a c t o r  w h o  p l a y e d  i n  t h e  v i d e o  a t  5  m i n u t e s  a n d  4 1  
s e c o n d s  i s  J e r e m y  S t r o n g .  H e  i s  k n o w n  f o r  h i s  r o l e  a s  
K e n d a l l  R o y  i n  t h e  H B O  d r a m a  s e r i e s  ' S u c c e s s i o n '  
( 2 0 1 8 – 2 0 2 3 ) .  O t h e r  n o t a b l e  s h o w s  a n d  f i l m s  h e  h a s  
a p p e a r e d  i n  i n c l u d e : -  ' T h e  T r i a l  o f  t h e  C h i c a g o  7 '  ( 2 0 2 0 )  
a s  J e r r y  R u b i n -  ' M o l l y ' s  G a m e '  ( 2 0 1 7 )  a s  D e a n  K e i t h -  
' T h e  B i g  S h o r t '  ( 2 0 1 5 )  a s  V i n n y  D a n i e l -  ' S e l m a '  ( 2 0 1 4 )  a s  
J a m e s  R e e b -  ' L i n c o l n '  ( 2 0 1 2 )  a s  J o h n  G e o r g e  N i c o l a y -  
' Z e r o  D a r k  T h i r t y '  ( 2 0 1 2 )  a s  T h o m a s -  ' T h e  G e n t l e m e n '  
( 2 0 1 9 )  a s  M a t t h e w J e r e m y  S t r o n g  h a s  a l s o  p e r f o r m e d  i n  
t h e a t e r ,  i n c l u d i n g  h i s  B r o a d w a y  d e b u t  i n  ' A  M a n  f o r  A l l  
S e a s o n s '  ( 2 0 0 8 )  a n d  ' A n  E n e m y  o f  t h e  P e o p l e '  ( 2 0 2 4 ) . ”
T i m e  s p a n :  9 5 2 . 4  -  9 5 6 . 3 6
T i m e  i n  v i d e o :  N o n e
L o c a t i o n :  O u t s i d e  a  s t o r e  n a m e d  ' d e  G R I S O G O N O '  o n  a  
c i t y  s t r e e t .
C h a r a c t e r ' :  A  p e r s o n  w i th  b l u e  h a i r  i s  a s k i n g  a n o th e r  
p e r s o n  t o  c o m e  i n s i d e  th e  s t o r e  a n d  h e l p  th e m .
E v e n t s :  [ ' A  p e r s o n  w i th  b l u e  h a i r  a s k s  a n o th e r  p e r s o n  t o  
c o m e  i n s i d e  th e  s t o r e  a n d  h e l p  th e m . ' ,  ' T h e  o th e r  p e r s o n  
a g r e e s . ' ]
S c e n e :  [ \ " T h e  s c e n e  t a k e s  p l a c e  o u t s i d e  a  s t o r e  n a m e d  
' d e  G R I S O G O N O ' . \ " ,  ' I t  a p p e a r s  t o  b e  a  c i t y  s t r e e t  w i th  
o th e r  s t o r e s  v i s i b l e  i n  th e  b a c k g r o u n d . ' ,  ' T h e  p e r s o n  w i th  
b l u e  h a i r  i s  w e a r i n g  a  c o a t  a n d  i s  s t a n d i n g  n e x t  t o  
a n o th e r  p e r s o n . ' ]
S u m m a r y :  T h e  v i d e o  s h o w s  a  p e r s o n  w i th  b l u e  h a i r  
a s k i n g  a n o th e r  p e r s o n  t o  c o m e  i n s i d e  a  s t o r e  n a m e d  ' d e  
G R I S O G O N O '  a n d  h e l p  th e m .  T h e  o th e r  p e r s o n  a g r e e s .T i m e  s p a n :  5 6 3 . 1 2  -  5 6 5 . 4
T i m e  i n  v i d e o :  N o n e
L o c a t i o n :  N o n e
C h a r a c t e r ' :  A  p e r s o n  w e a r i n g  a  c o l o r f u l  h a t  w i th  c a n d l e s  
o n  i t  a n d  a  b r o w n  j a c k e t  i s  p r e s e n t .  A n o th e r  c h a r a c t e r ,  
p o s s i b l y  i n  a  c o s t u m e ,  i s  a l s o  v i s i b l e .
E v e n t s :  [ ' A  p e r s o n  i n  a  c o l o r f u l  h a t  i s  b e i n g  t o l d  t o  s t o p  o r  
h o l d  o n . ' ,  \ " T h e r e  i s  a  s e n s e  o f  u r g e n c y  o r  s u r p r i s e  a s  
i n d i c a t e d  b y  th e  p h r a s e s  ' Q u i c k ! '  a n d  ' W h o a ,  w h o a ,  
w h o a ! ' \ " ]
S c e n e :  [ ' T h e  s c e n e  i n c l u d e s  a  p e r s o n  w e a r i n g  a  c o l o r f u l  
h a t  w i th  c a n d l e s  a n d  a  b r o w n  j a c k e t . ' ,  ' A n o th e r  c h a r a c t e r ,  
p o s s i b l y  i n  a  c o s t u m e ,  i s  p r e s e n t . ' ,  ' T h e  d i a l o g u e  i n d i c a t e s  
a  s u d d e n  o r  u r g e n t  s i t u a t i o n . ' ]
S u m m a r y :  T h e  v i d e o  s h o w s  a  p e r s o n  w e a r i n g  a  c o l o r f u l  
h a t  w i th  c a n d l e s  a n d  a  b r o w n  j a c k e t  b e i n g  t o l d  t o  s t o p  o r  
h o l d  o n .  T h e r e  i s  a  s e n s e  o f  u r g e n c y  o r  s u r p r i s e  i n  th e  
d i a l o g u e .T i m e  s p a n :  4 8 0 . 4 8  -  4 9 2 . 0
T i m e  i n  v i d e o :  N o n e
L o c a t i o n :  I n s i d e  a  v e h i c l e
C h a r a c t e r ' :  A  p e r s o n  w i th  b l u e  s k i n ,  d a r k  h a i r ,  a n d  w e a r i n g  a  
p u r p l e  o u t f i t .  T h e  p e r s o n  i s  s e e n  h o l d i n g  a  c i g a r e t t e  a n d  
l o o k i n g  o u t  o f  th e  v e h i c l e  w i n d o w .
E v e n t s :  [ ' T h e  p e r s o n  i s  s i t t i n g  i n s i d e  a  v e h i c l e . ' ,  ' T h e  p e r s o n  
h o l d s  a  c i g a r e t t e  i n  th e i r  h a n d . ' ,  ' T h e  p e r s o n  l o o k s  o u t  o f  th e  
v e h i c l e  w i n d o w . ' ]
S c e n e :  [ ' T h e  s c e n e  t a k e s  p l a c e  i n s i d e  a  v e h i c l e . ' ,  ' T h e  
p e r s o n  h a s  b l u e  s k i n  a n d  d a r k  h a i r . ' ,  ' T h e  p e r s o n  i s  w e a r i n g  a  
p u r p l e  o u t f i t . ' ,  ' T h e  p e r s o n  i s  h o l d i n g  a  c i g a r e t t e  a n d  l o o k i n g  
o u t  o f  th e  w i n d o w . ' ]
S u m m a r y :  T h e  v i d e o  s h o w s  a  p e r s o n  w i th  b l u e  s k i n  a n d  
d a r k  h a i r  s i t t i n g  i n s i d e  a  v e h i c l e .  T h e  p e r s o n  i s  w e a r i n g  a  
p u r p l e  o u t f i t ,  h o l d i n g  a  c i g a r e t t e ,  a n d  l o o k i n g  o u t  o f  th e  
v e h i c l e  w i n d o w .U s e r :  
“ W h i c h  o th e r  s h o w s  h a s  th e  a c t o r  w h o  p l a y e d  i n  th e  v i d e o  a t  5  
m i n u t e s  a n d  4 1  s e c o n d s  a p p e a r e d  i n ? ”
P a r e n t  T a s k :  W h i c h  o th e r  
s h o w s  h a s  th e  a c t o r  w h o  
p l a y e d  i n  th e  v i d e o  a t  5  
m i n u t e s  a n d  4 1  s e c o n d s  
a p p e a r e d  i n ?S u b t a s k  3 :  C o n c l u d e  th e  r e s u l t s .S u b t a s k  2 :  P e r f o r m  a  w e b  s e a r c h  t o  f i n d  
o th e r  s h o w s  th e  i d e n t i f i e d  a c t o r  h a s  
a p p e a r e d  i n .S u b t a s k  1 :  U s e  f a c e  r e c o g n i t i o n  
t o  i d e n t i f y  th e  a c t o r  i n  th e  
e x t r a c t e d  f r a m e .S u b t a s k  0 :  E x t r a c t  a  f r a m e  f r o m  
th e  v i d e o  a t  5  m i n u t e s  a n d  4 1  
s e c o n d s .R e c a l l  i n f o r m a t i o n  f r o m  t h e  k n o w l e d g e  d a t a b a s e  b a s e d  
o n  t h e  t i m e s t a m p . 
A g e n t  O u t p u tS c e n e  C a p t i o nI n p u t  V i d e o
U s e r  Q u e s t i o n
I n n e r  S t e p sR e t r i e v i n gFigure 3: case 1
A g e n t :   
" T h e  e x a c t  m o m e n t  t h e  c i g a r e t t e  f a l l s  t o  t h e  g r o u n d  i s  
c a p t u r e d  i n  t h e  f r a m e  a t  t i m e s t a m p  1 5 2 . 0  s e c o n d s .  T h e  
i m a g e  c l e a r l y  s h o w s  t h e  c i g a r e t t e  o n  t h e  g r o u n d  n e a r  t h e  
p e r s o n ' s  f e e t . "U s e r :  
“ W h e n  w a s  th e  f i r s t  t i m e  a  c i g a r e t t e  d r o p p e d  t o  th e  g r o u n d ? ”
P a r e n t  T a s k :  W h e n  w a s  
th e  f i r s t  t i m e  a  c i g a r e t t e  
d r o p p e d  t o  th e  g r o u n d ?
S u b t a s k  1 :  A n a l y z e d  th e  
e x t r a c t e d  f r a m e s  t o  i d e n t i f y  th e  
e x a c t  m o m e n t  a n d  f r a m e  w h e r e  
th e  c i g a r e t t e  f i r s t  a p p e a r s  t o  f a l l  
t o  th e  g r o u n d .S u b t a s k  0 _ 4 :  C o n f i r m  th e  
e x a c t  t i m e  p o i n t  o f  th e  
c i g a r e t t e  f a l l i n g .S u b t a s k  0 _ 3 :  A n a l y z e  th e  
e x t r a c t e d  f r a m e s  t o  p i n p o i n t  
th e  e x a c t  m o m e n t  th e  
c i g a r e t t e  f a l l s  t o  th e  g r o u n d .S u b t a s k  0 _ 2 :  E x t r a c t  f r a m e s  
f r o m  th e  v i d e o  f o r  th e  t i m e  
s p a n  1 3 8 . 4  -  1 6 5 . 2 8  
s e c o n d s .S u b t a s k  0 _ 1 :  E x t r a c t  f r a m e s  
f r o m  th e  v i d e o  f o r  th e  t i m e  
s p a n  5 6 3 . 1 2  -  5 6 5 . 4  
s e c o n d s .S u b t a s k  0 _ 0 :  E x t r a c t  f r a m e s  
f r o m  th e  v i d e o  f o r  th e  t i m e  
s p a n  4 8 0 . 4 8  -  4 9 2 . 0  
s e c o n d s .
S u b t a s k  0 :  F r a m e s  e x t r a c t e d  
f r o m  v i d e o  f o r  t i m e  s p a n s  
4 8 0 . 4 8  -  4 9 2 . 0 ,  5 6 3 . 1 2  -  5 6 5 . 4 ,  
a n d  1 3 8 . 4  -  1 6 5 . 2 8 .
S u b t a s k  2 :  D e t e r m i n e d  th e  e x a c t  
t i m e  p o i n t  w h e n  th e  c i g a r e t t e  
f i r s t  f a l l s  t o  th e  g r o u n d .R e c a l l  i n f o r m a t i o n  f r o m  t h e  k n o w l e d g e  d a t a b a s e  b a s e d  
o n  t h e  t i m e s t a m p . 
T i m e  s p a n :  8 9 7 . 4 4  -  9 0 2 . 2
T i m e  i n  v i d e o :  N o n e
L o c a t i o n :  A  d i m l y  l i t  r o o m  w i th  a  c h a n d e l i e r  a n d  a  
w o o d e n  d r e s s e r .
C h a r a c t e r ' :  T w o  i n d i v i d u a l s ,  o n e  w i th  w h i t e  h a i r  a n d  th e  
o th e r  w i th  d a r k  h a i r ,  a r e  s e e n  e m b r a c i n g .
E v e n t s :  [ ' T w o  i n d i v i d u a l s  a r e  s e e n  e m b r a c i n g  i n  a  d i m l y  l i t  
r o o m . ' ,  " O n e  o f  th e m  s a y s ,  ' I  ' l l  s e e  y o u  l a t e r . " ,  " T h e  o th e r  
r e s p o n d s ,  ' R i g h t ,  g e t  y o u r  c o a t . ' " ,  " ]
S c e n e :  [ ' T h e  s c e n e  t a k e s  p l a c e  i n  a  d i m l y  l i t  r o o m  w i th  a  
c h a n d e l i e r  a n d  a  w o o d e n  d r e s s e r . ' ,  ' T w o  i n d i v i d u a l s ,  o n e  
w i th  w h i t e  h a i r  a n d  th e  o th e r  w i th  d a r k  h a i r ,  a r e  
e m b r a c i n g . ' ]
S u m m a r y :  I n  a  d i m l y  l i t  r o o m  w i th  a  c h a n d e l i e r  a n d  a  
w o o d e n  d r e s s e r ,  t w o  i n d i v i d u a l s  a r e  s e e n  e m b r a c i n g .  
T h e y  e x c h a n g e  a  b r i e f  c o n v e r s a t i o n  i n d i c a t i n g  th a t  th e y  
w i l l  b e  p a r t i n g  w a y s  s o o n . 'T i m e  s p a n :  5 6 3 . 1 2  -  5 6 5 . 4
T i m e  i n  v i d e o :  N o n e
L o c a t i o n :  N o n e
C h a r a c t e r ' :  A  p e r s o n  w e a r i n g  a  c o l o r f u l  h a t  w i th  c a n d l e s  
o n  i t  a n d  a  b r o w n  j a c k e t  i s  p r e s e n t .  A n o th e r  c h a r a c t e r ,  
p o s s i b l y  i n  a  c o s t u m e ,  i s  a l s o  v i s i b l e .
E v e n t s :  [ ' A  p e r s o n  i n  a  c o l o r f u l  h a t  i s  b e i n g  t o l d  t o  s t o p  o r  
h o l d  o n . ' ,  " T h e r e  i s  a  s e n s e  o f  u r g e n c y  o r  s u r p r i s e  a s  
i n d i c a t e d  b y  th e  p h r a s e s  ' Q u i c k ! '  a n d  ' W h o a ,  w h o a ,  
w h o a ! " ] n
S c e n e :  [ ' T h e  s c e n e  i n c l u d e s  a  p e r s o n  w e a r i n g  a  c o l o r f u l  
h a t  w i th  c a n d l e s  a n d  a  b r o w n  j a c k e t . ' ,  ' A n o th e r  c h a r a c t e r ,  
p o s s i b l y  i n  a  c o s t u m e ,  i s  p r e s e n t . ' ]
S u m m a r y :  T h e  v i d e o  s h o w s  a  p e r s o n  w e a r i n g  a  c o l o r f u l  
h a t  w i th  c a n d l e s  a n d  a  b r o w n  j a c k e t  b e i n g  t o l d  t o  s t o p  o r  
h o l d  o n .  T h e r e  i s  a  s e n s e  o f  u r g e n c y  o r  s u r p r i s e  i n  th e  
d i a l o g u e .T i m e  s p a n :  1 7 6 . 2 4  -  1 8 0 . 4 4
T i m e  i n  v i d e o :  N o n e
L o c a t i o n :  A n  o u t d o o r  g r a s s y  a r e a
C h a r a c t e r ' :  N o n e
E v e n t s :  [ ' A  p i e c e  o f  p a p e r  o r  p h o t o g r a p h  i s  s h o w n  
l y i n g  o n  th e  g r a s s . ' ]
S c e n e :  [ ' T h e  s c e n e  s h o w s  a  c l o s e - u p  o f  a  p i e c e  o f  
p a p e r  o r  p h o t o g r a p h  l y i n g  o n  a  g r a s s y  a r e a .  T h e  
g r a s s  a p p e a r s  t o  b e  w e l l - m a i n t a i n e d  a n d  g r e e n . ' ]
S u m m a r y :  T h e  v i d e o  c l i p  s h o w s  a  c l o s e - u p  o f  a  
p i e c e  o f  p a p e r  o r  p h o t o g r a p h  l y i n g  o n  a  g r a s s y  
a r e a .  T h e r e  i s  n o  t e x t u a l  c o n t e n t  o r  c o n v e r s a t i o n  i n  
th i s  f r a m e .
A g e n t  O u t p u tI n n e r  S t e p sR e t r i e v i n gI n p u t  V i d e o
U s e r  Q u e s t i o nS c e n e  C a p t i o n Figure 4: case 2