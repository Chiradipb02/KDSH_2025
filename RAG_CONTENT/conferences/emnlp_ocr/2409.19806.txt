PALM: Few-Shot Prompt Learning for Audio Language Models
Asif Hanif, Maha Tufail Agro, Mohammad Areeb Qazi, Hanan Aldarmaki
Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)
{asif.hanif, maha.tufail, mohammad.qazi, hanan.aldarmaki}@mbzuai.ac.ae
Abstract
Audio-Language Models (ALMs) have recently
achieved remarkable success in zero-shot audio
recognition tasks, which match features of au-
dio waveforms with class-specific text prompt
features, inspired by advancements in Vision-
Language Models (VLMs). Given the sensitiv-
ity of zero-shot performance to the choice of
hand-crafted text prompts, many prompt learn-
ing techniques have been developed for VLMs.
We explore the efficacy of these approaches in
ALMs and propose a novel method, Prompt
Learning in Audio Language Models (PALM) ,
which optimizes the feature space of the text
encoder branch. Unlike existing methods that
work in the input space, our approach results
in greater training efficiency. We demonstrate
the effectiveness of our approach on 11 audio
recognition datasets, encompassing a variety
of speech-processing tasks, and compare the
results with three baselines in a few-shot learn-
ing setup. Our method is either on par with or
outperforms other approaches while being com-
putationally less demanding. Code is available
athttps://asif-hanif.github.io/palm/ .
1 Introduction
Inspired by the success of Vision-Language Mod-
els (VLMs) (Zhang et al., 2024), Audio-Language
Models (ALMs) have recently emerged, achieving
state-of-the-art performance on various zero-shot
audio recognition tasks (Elizalde et al., 2023;
Deshmukh et al., 2023; Kong et al., 2024; Das
et al., 2024). In zero-shot audio recognition,
features of the audio waveform are matched with
features of text prompts representing each class,
and the highest matching class is assigned to the
audio waveform. Zero-shot audio recognition
offers significant advantages by eliminating the
need for extensive labeled datasets and allowing for
the recognition of new classes without additional
training. This approach reduces training times
and data annotation costs, leading to substantial
ZERO SHOT COOP COCOOP PALM
MethodsAvg. Accuracy
0.3970.7110.7350.766Figure 1: Comparison of our proposed approach, PALM,
with three baselines: ZERO-SHOT (Deshmukh et al.,
2023), COOP (Zhou et al., 2022b) and COCOOP (Zhou
et al., 2022a). Bar plots show classification accuracy
averaged across 11 audio datasets encompassing various
speech-processing tasks.
savings in computational resources.
The choice of text prompt is crucial for
pre-trained vision-language and audio-language
models, but it becomes a drawback for zero-shot
recognition due to the requirement of hand-crafted
prompts. This manual prompt-engineering can
result in performance variations (Zhou et al.,
2022b,a). We confirm this observation, previously
noted in VLMs, within the context of ALMs (refer
to Figure 2). To automate the learning of text
prompts, various approaches have been introduced
for prompt learning in VLMs (Gu et al., 2023).
The domain of prompt learning in ALMs
remains under-explored, lacking comprehensive
studies to evaluate the efficacy of prompt learning
techniques within this context. To bridge this
research gap, we adapt prompt learning techniques
developed for VLMs and apply them to the domainarXiv:2409.19806v1  [cs.SD]  29 Sep 2024{CLASS Name}Text Pr ompt T emplates ESC50
0.4330GT-Music
0.3250SESA
0.7810VocalSound
0.3754
The is a recording of {CLASS NAME}  0.5349 0.3251 0.7238 0.4197
This is an audio recording of {CLASS NAME}  0.4425 0.3853 0.7143 0.3876
This captures the sound of {CLASS NAME}  0.4902 0.3755 0.7333 0.3127
This track contains sound of {CLASS NAME}  0.4941 0.4201 0.6762 0.3929
This audio file contains a recording of{CLASS Name} 0.5251 0.3901 0.7143 0.4327
This is a sound recording of {CLASS NAME}  0.4911 0.3553 0.7332 0.3768
This is an audio clip of {CLASS NAME}  0.4721 0.4104 0.6857 0.3793Figure 2: Impact of Hand-crafted Prompts on ZERO-SHOT Performance Zero-shot accuracy across four audio
recognition datasets (ESC50 (Piczak), GT-Music-Genre (Sturm, 2012), SESA (Spadini, 2019), and V ocalSound
(Gong et al., 2021)) is evaluated with eight different text prompts using PENGI (Deshmukh et al., 2023) model. The
accuracy varies with changes in the handcrafted prompts.
of ALMs. Our results demonstrate that these adap-
tations improve the audio recognition performance
(see Table 2). Traditional techniques optimize
the input space (token embeddings) of the text
encoder branch by introducing a learnable context.
However, this approach can increase training
costs as loss gradients must flow through the text
encoder branch. To address this, we introduce a
novel method, PALM :Prompt Learning in Audio
Language Models, which optimizes the feature
space of the text encoder rather than its input space.
This makes the training computationally efficient
since loss gradients do not need to flow through
the text encoder. To assess the effectiveness of our
approach, we show results on 11 audio recognition
datasets, encompassing various speech processing
tasks. Our method either matches or surpasses
other approaches while being less computationally
demanding (see Table 2 and Table 3), setting a
benchmark for prompt learning in ALMs.
Contributions : Our contributions are as follows:
–Inspired by the success of few-shot based
prompt learning in VLMs, we are the first
(to the best of our knowledge) to demonstrate
its efficacy in ALMs.
–We show that prompt learning techniques, ini-
tially developed for VLMs, can significantly
enhance the performance when adapted for
ALMs.
–We introduce a novel few-shot based prompt
learning method, PALM, for ALMs that op-
timizes the feature space of the text encoder,
outperforming existing baselines.–We demonstrate our approach’s effectiveness
on 11 audio recognition datasets, comparing it
to three baselines in a few-shot learning setup.
Our method matches or outperforms others
while being less computationally demanding,
establishing a benchmark for prompt learning
in audio-language models and paving the way
for future research.
2 Related Work
Prompt engineering involves adding task-specific
hints, called prompts, to a large pre-trained model
to adapt it to new tasks. Recently, significant ad-
vancements have been made in prompt learning,
particularly in the fields of language and vision.
Below, we outline the recent developments in lan-
guage, vision, and audio domains.
2.1 Audio Language Models (ALMs)
Taking inspiration from multimodal models like
CLIP (Radford et al., 2021) in the vision domain,
Contrastive Language-Audio Pretraining (CLAP)
(Elizalde et al., 2023) stands out as the first-of-its-
kind audio language model. It connects natural
language and audio through dual encoders and con-
trastive learning, aligning audio and text descrip-
tions in a shared multimodal space. Furthermore,
CLAP introduces zero-shot prediction capabilities,
removing the necessity for training with predefined
class labels and allowing flexible class prediction
during inference.
PENGI (Deshmukh et al., 2023), another audio
language Model, utilizes transfer learning by treat-
ing all audio tasks as text-generation tasks. It takes
audio recordings and text inputs, generating free-
form text as output. The input audio is represented
by continuous embeddings from an audio encoder,while the corresponding text input undergoes the
same process with a text encoder. These sequences
are combined as a prefix to prompt a pre-trained
frozen language model. PENGI’s unified archi-
tecture supports both open-ended and close-ended
tasks without requiring additional fine-tuning or
task-specific extensions.
Audio Flamingo, introduced by Kong et al.
(2024), is a multimodal-to-text generative model
inspired by Flamingo (Alayrac et al., 2022), demon-
strating advanced audio understanding capabili-
ties, adaptability to unseen tasks through in-context
learning and retrieval, and multi-turn dialogue abil-
ities. The model features an audio feature extractor
with a sliding window and uses cross-attention to
fuse audio inputs into the language model, ensuring
computational efficiency.
2.2 Prompt Learning in Language Models
Extensive research has been conducted on prompt
learning techniques in natural language process-
ing. Pioneering work by (Brown et al., 2020)
focused on optimization strategies for zero-shot
and few-shot learning scenarios, demonstrating
that prompts can enable generative models to per-
form well across various tasks without extensive
task-specific training. Their method leverages the
model’s pre-trained knowledge and prompt-guided
interactions to achieve strong performance on new
tasks. They also introduced GPT-3, which trans-
formed the field of prompt learning in natural lan-
guage processing. Petroni et al. (2019) integrated
contextual cues and constraints within prompts to
guide model behavior, embedding task-specific in-
formation to enhance output precision and rele-
vance. Their technique improves interpretability
and task-oriented performance by providing con-
textual guidance during inference.
2.3 Prompt Learning in Vision-Language
Models
Inspired by advancements in prompt-based work
in language models, several studies have been con-
ducted to adapt these methods to VLMs (Gu et al.,
2023). Some focus exclusively on the language
component, such as COOP (Context Optimization)
(Zhou et al., 2022a). In contrast, others integrate
insights from language and visual components,
as seen in COCOOP (Conditional Context Opti-
mization) (Zhou et al., 2022a). COOP enhances
CLIP model’s few-shot transfer learning capability
by optimizing a continuous set of prompt vectorswithin the language branch. However, COCOOP
addresses the limitations of COOP, particularly its
suboptimal performance on novel classes, by ex-
plicitly conditioning prompts on individual image
instances, thereby enhancing generalization.
2.4 Prompt Learning in Audio-Language
Models
Prompt learning with audio-language models is rel-
atively understudied. Previous work has explored
enhancing language models with speech recog-
nition by conditioning them on variable-length
audio embeddings using a conformer-based au-
dio encoder (Fathullah et al., 2024). Deshmukh
et al. (2024) propose a test-time domain adapta-
tion method for Contrastive ALMs, using unla-
beled audio to adjust the model to new domains via
a domain vector, consistent predictions, and self-
entropy fine-tuning, improving on traditional Test-
Time Training. Li et al. (2024) introduce PT-Text ,
an audio-free prompt tuning scheme that optimizes
prompt tokens from text, regularizing the model
to avoid overfitting by training with captions and
using a multi-grained strategy to enhance perfor-
mance. Despite these advancements, more research
is needed to fully understand and exploit prompt
learning in audio-language models.
3 Method
3.1 Audio-Language Model (ALM)
We demonstrate the efficiency of prompt learning
in enhancing zero-shot performance using a
state-of-the-art audio-language model PENGI
(Deshmukh et al., 2023). Our approach is
applicable to all audio-language models that have
aligned audio and text encoders.
PENGI takes an audio waveform and a text
prompt as input and generates free-form text. It
consists of three branches. The first branch is
an audio encoder that maps the audio waveform
to an embedding space. The second branch is a
text encoder that transforms the input text into
the same embedding space. These embeddings
are then concatenated to form an input prefix for
the third branch, a causal language model that
generates tokens autoregressively, conditioned
on both the audio and text inputs. PENGI can be
used for various audio-conditioned tasks, such
as text completion, classification, audio caption
generation, and question-answering (Deshmukhet al., 2023).
Zero-Shot Inference Although PENGI is
multimodal-to-text generation model, however,
we use its audio and text encoder branches for
zero-shot audio recognition. This is accomplished
by comparing the embedding of the audio wave-
form (extracted from the audio encoder) with the
embeddings of text prompts for different classes
(extracted from the text encoder). An overview of
zero-shot inference is given in Figure 3(a). The
zero-shot setup used by (Deshmukh et al., 2023)
differs from ours, as they employ the model’s
free-form text output for zero-shot inference.
Formally, we denote the pre-trained ALM
asfθ={fA, fT}, whereas fAandfTare audio
and text encoders, respectively and θrepresents
the combined weights of both encoders. For classi-
fication in zero-shot scenario, an audio waveform
xis first passed to the audio encoder fAto produce
ad−dimensional feature vector fA(x)∈Rd. In
parallel, text prompts representing each class
labelyi∈ {y1, y2. . . , y c}are encapsulated within
class-specific handcrafted text templates, such as
ti=“An audio recording of {CLASS yi}”,
where cis the total number of classes. Each text
prompt, represented as ti, is processed through
the text encoder fT, resulting in a feature vector
fT(ti)∈Rd. The relationship between the audio
waveform xand a class-specific text prompt tiis
quantified by computing the cosine similarity be-
tween their corresponding feature vectors, denoted
assim(fA(x), fT(ti)). The class with the highest
similarity score is then assigned as the label ˆyfor
the audio waveform, i.e.
ˆy=argmax
i∈{1,2,...,c}sim 
fA(x), fT(ti)
.(1)
3.2 PALM: Prompt Learning in ALM
In our proposed method, we do not use hand-
crafted prompts; instead, we simply use class
names as the input to the text encoder i.e. ti=
“{CLASS yi}”. Moreover, unlike COOP (Zhou
et al., 2022b), which learns the context of input text
prompts in the token embedding space (see Figure
3(b)), we learn the context in the feature space of
prompts. Specifically, after obtaining the feature
vector of the ithclass text prompt via the text en-
coder, i.e., fT(ti)∈Rd, we add a learnable vectorzi∈Rdto it to get the updated text feature vector
as follows:
f′
T(ti) = (1 −λi)·fT(ti) +λi·zi (2)
where λi∈[0,1]is a learnable parameter that de-
termines the contributions of both vectors. Assum-
ingt={t1, t2, . . . , t c}denotes text prompts of all
classes, the raw/un-normalized prediction scores
(logits), denoted as fθ(x,t)∈Rc, for an audio
waveform (x)are obtained as follows:
fθ(x,t) =
sim
fA(x), f′
T(ti) c
i=1,
where sim(·)is cosine-similarity function and
cis the number of classes. fA(x)is the feature
vector from the audio encoder, and f′
T(ti)is the
updated text feature vector (Equation 2) of ithclass.
We optimize the following objective func-
tion to learn feature-space context embeddings
z={z1, z2, . . . , z c}and their corresponding
contributions λ={λ1, λ2, . . . , λ c},
minimize
z, λX
(x,y)∈DL 
fθ(x,t), y
, (3)
where D={xi, yi}N
i=1is training dataset con-
sisting of Naudio-class pairs and L(·)denotes
cross-entropy loss. We use few-shot setting during
training, meaning that a fixed number of samples
(e.g., 16) are randomly selected from each class
in the training dataset. While optimizing objective
in Equation 3, weights of both encoders {fA, fT}
are kept in frozen state. The number of learnable
parameters in our proposed method is c+ (c×d).
After learning the parameters, we use Equation 4
for audio classification during inference stage.
ˆy=argmax
i∈{1,2,...,c}sim 
fA(x), f′
T(ti)
(4)
An overview of our proposed approach can be
found in Figure 3(c).
3.3 Difference with COOP and COCOOP
COOP (Zhou et al., 2022b) and COCOOP (Zhou
et al., 2022a) were originally introduced for vision-
language model; however, we adapted them for
audio-language model (replacing the vision en-
coder branch with audio encoder branch) and pre-
sented it as baseline methods. Both of these base-
lines and our method aim to enhance zero-shotLearnable(a) ZERO SHO T
Frozen
Audio F eatures
Text Pr ompts0.2
0.1
0.5Class 1
Class 2
Class CAudio W aveform
Text Features Prediction S coresCross-Entr opy Loss
Learnable Context(b)  COOP
Audio F eatures
Text Pr ompts0.2
0.1
0.5An audio recording of Class 1
An audio recording of Class 2
An audio recording of Class CAudio W aveform
Text Features Prediction S cores
Audio F eatures
Text Pr ompts0.2
0.1
0.5Audio W aveform
Text Features Prediction S coresCross-Entr opy Loss
(c)  P ALM(ours)Class 1
Class 2
Class C
Learnable Context+
Text Encoder Cosine Sim. Audio EncoderFigure 3: Overview of Zero-Shot, COOP, PALM (a)Zero-Shot inference involves matching the embedding of
the audio waveform with the embeddings of text prompts for each class. The class with the highest matching score
is then assigned to the audio. (b)COOP (Zhou et al., 2022b) avoids using handcrafted text prompts by learning the
context of text prompts in the token embedding space. It optimizes the input space of the text encoder to enhance
classification performance. (c)PALM requires only class names at the input of text encoder and it optimizes the
feature space by adding learnable context embeddings to text feature vectors. PALM not only outperforms COOP
(see Table 2), but it is also more computationally efficient since it does not require gradients to flow through the text
encoder, unlike COOP.
performance for audio classification in this work.
While our method and the baselines share this com-
mon goal, they differ in their approach to achiev-
ing it. COOP and COCOOP optimize the input
space (token embeddings of prompt context) of
text encoder, whereas our method optimizes the
text feature space. In our method, loss gradients donot need to flow through the text encoder, whereas
in COOP and COCOOP, gradients flow through
the encoder to reach the input to update prompt
context. Moreover, there is a feedback loop from
audio features (output of audio encoder) to the in-
put of text encoder in COCOOP, making it even
more computationally expensive. Comparatively,DATASETS TYPE CLASSES SPLIT
Beijing-OperaInstrument Classification4 Five Fold
NS-Instruments 10 Train-Test
ESC50
Sound Event Classification50 Five Fold
ESC50-Actions 10 Five Fold
UrbanSound8K 10 Ten Fold
CREMA-DEmotion Recognition6 Train-Test
RA VDESS 8 Train-Test
V ocalSound V ocal Sound Classification 6 Train-Test
SESA Surveillance Sound Classification 4 Train-Test
TUT2017 Acoustic Scene Classification 15 Four Fold
GT-Music-Genre Music Analysis 10 Train-Test
Table 1: Datasets Information In this work, we use
11 multi-class classification datasets encompassing a
variety of speech-processing tasks.
our method is more computationally efficient as
it does not include a feedback loop (see Table 3).
Both COOP and COCOOP require a user-specified
hyper-parameter, namely the number of context to-
kens, whereas our method does not rely on such a
parameter. Results in Table 2 demonstrate that our
method outperforms COOP and COCOOP, achiev-
ing an average improvement of 5.5% and 3.1%
respectively.
4 Experiments and Results
4.1 Datasets
We evaluate our methodology using datasets
from various speech-processing tasks: instru-
ment classification, sound event classification,
emotion recognition, vocal sound classification,
surveillance sound event classification, acoustic
scene classification, and music analysis. Brief
information of each dataset can be found in
Table 1. For instrument classification, we use
Beijing-Opera (Tian et al., 2014) dataset, which
includes audio examples of strokes from four
percussion instrument classes used in Beijing
Opera, and NS-Instruments (Engel et al., 2017)
dataset, which consists of one-shot instrumental
notes with unique pitches, timbres, and envelopes,
spanning ten classes. For sound event classifica-
tion, we utilize three datasets: ESC50 (Piczak),
containing environmental recordings across 50
classes; ESC50-Actions (Piczak), a subset with
10 classes of non-speech human sounds; and
UrbanSound8K (Salamon et al., 2014), with
urban noise excerpts from 10 classes. Emotion
recognition is assessed with the CREMA-D (Cao
et al., 2014) and RA VDESS (Livingstone and
Russo, 2018) datasets, covering 6 and 8 emotion
classes respectively, performed by actors. Weemploy the V ocalSound (Gong et al., 2021) dataset
for vocal sound classification, which includes 6
classes of human non-speech vocalizations. For
surveillance sound event classification, we use
SESA (Spadini, 2019) dataset, which has 4 classes.
Acoustic scene classification uses the TUT2017
(Heittola et al., 2017) dataset, containing samples
from 15 acoustic scenes. For music analysis, the
GT-Music-Genre (Sturm, 2012) dataset is used,
which includes 10 classes of music genres.
We adhere to the official train-test or multi-fold
splits for all datasets. We conduct cross-validation
experiments on datasets having multi-fold splits
such as Beijing-Opera, ESC50, ESC50-Actions,
UrbanSound8K, and TUT2017, and report the av-
erage scores. We have publicly released all infor-
mation regarding dataset preprocessing to ensure
reproducibility of results.
4.2 Baseline Methods
For baselines, we consider PENGI model (Desh-
mukh et al., 2023) (in ZERO-SHOT setup), COOP
(Zhou et al., 2022b) and COCOOP (Zhou et al.,
2022a). COOP and COCOOP are prompt learn-
ing approaches, originally introduced for VLMs.
Both of these approaches remove the requirement
of providing handcrafted text prompts and they
optimize the input token embedding space of text
encoder to enhance accuracy. The only difference
between COOP and COCOOP is that the latter
incorporates a feedback loop from the output of
the audio encoder to the input of the text encoder.
We adapt these two approaches for audio-language
models by replacing the vision encoder with an au-
dio encoder and present them as baselines for our
proposed method. Why PENGI, COOP and CO-
COOP as baselines? PENGI is an state-of-the-art
ALM that has demonstrated comprehensive evalua-
tion across 21 downstream audio tasks, making it a
robust benchmark for comparison. COOP and CO-
COOP, on the other hand, are pioneering works on
prompt learning in the domain of vision-language
models, offering foundational techniques and in-
sights that are highly relevant for our study.
4.3 Experimental Setup
We use pre-trained PENGI (Deshmukh et al., 2023)
as the audio-language model for all methods. For
all methods, except ZERO-SHOT, we conduct ex-
periments for 50epochs. Following the few-shot
evaluation setup, we use 16 randomly selectedMETHODS →ZERO SHOT COOP COCOOP PALM (ours)
DATASETS ↓ - SEED-0 SEED-1 SEED-2 A VG SEED-0 SEED-1 SEED-2 A VG SEED-0 SEED-1 SEED-2 A VG
Beijing-Opera 0.2881 0.9323 0.9660 0.9619 0.9534 0.9577 0.9830 0.9916 0.9774 0.9747 0.9066 0.9787 0.9533
CREMA-D 0.2310 0.3130 0.4197 0.2760 0.3362 0.2539 0.3358 0.3156 0.3018 0.4453 0.3580 0.2344 0.3459
ESC50-Actions 0.6525 0.9625 0.9400 0.9550 0.9525 0.9631 0.9620 0.9648 0.9634 0.9700 0.9625 0.9650 0.9658
ESC50 0.4965 0.9410 0.9390 0.9345 0.9382 0.9460 0.9370 0.9450 0.9427 0.9560 0.9600 0.9620 0.9593
GT-Music-Genre 0.3250 0.7250 0.6950 0.7350 0.7183 0.7500 0.7450 0.7600 0.7517 0.7900 0.7850 0.8250 0.8000
NS-Instruments 0.3291 0.5728 0.5562 0.6177 0.5822 0.5996 0.5740 0.6438 0.6058 0.6394 0.6108 0.6648 0.6383
RA VDESS 0.1222 0.3849 0.2688 0.3422 0.3320 0.3727 0.4399 0.3523 0.3883 0.4562 0.4603 0.4623 0.4596
SESA 0.7238 0.9143 0.8952 0.8762 0.8952 0.8381 0.8762 0.8952 0.8698 0.8857 0.9143 0.8857 0.8952
TUT2017 0.2435 0.6391 0.6667 0.6525 0.6528 0.7499 0.7215 0.7312 0.7342 0.7959 0.8047 0.7729 0.7912
UrbanSound8K 0.5349 0.7600 0.7378 0.7666 0.7548 0.7576 0.7784 0.7597 0.7652 0.8120 0.8037 0.8074 0.8077
V ocalSound 0.4197 0.7162 0.7485 0.6642 0.7096 0.8081 0.7825 0.7463 0.7790 0.8101 0.8168 0.7964 0.8078
A VERAGE 0.3969 0.7146 0.7121 0.7074 0.7114 0.7276 0.7396 0.7369 0.7347 0.7759 0.7621 0.7595 0.7658
Table 2: Comparison of PALM with Baselines The accuracy scores of the methods (ZERO SHOT (Deshmukh
et al., 2023), COOP (Zhou et al., 2022b), COCOOP (Zhou et al., 2022a), and our proposed method PALM) across
11 datasets are presented. For each method (except ZERO SHOT), experiments were performed using three different
seeds. The accuracy scores for all seeds are reported, along with the average score. Bold values indicate the
best average score in each row. Compared to the baselines, our proposed method achieves favorable results, with
an average improvement of 5.5% over COOP and 3.1% over COCOOP. It should be noted that both COOP and
COCOOP are computationally expensive, as these approaches require loss gradients to flow through the text encoder.
Additionally, COCOOP has a feedback loop from audio features to the input space of the text encoder, making it
even more computationally expensive. On the other hand, PALM is relatively less computationally expensive.
samples per class from the training dataset. For
inference, we utilize the entire test dataset. In
the case of multi-fold datasets, we employ cross-
validation and report the average scores. Train-
ing is performed using the Stochastic Gradient De-
scent (SGD) optimizer with a learning rate of 0.05.
We use ‘Accuracy’ as the evaluation metric. For
all methods, except ZERO-SHOT, we run exper-
iments with three different seeds and report the
scores for each seed along with the average score.
For ZERO-SHOT, we use default text prompt tem-
plate “ This is a recording of {CLASS NAME} ".
For COOP (Zhou et al., 2022b) and COCOOP
(Zhou et al., 2022a) baselines, we set the number
context tokens to 16and context is placed at the
front of class names. PENGI model weights are
kept “frozen” in all experiments. We use NVIDIA
A100-SXM4-40GB GPU for all experiments and Py-
torch version 1.11+cuda11.3 .
4.4 Results
Table 2 presents the performance comparison
across 11 datasets using four different methods.
Results indicate that PALM generally outperforms
COOP and COCOOP, showing an average improve-
ment of 5.5% over COOP and 3.1% over COCOOP.
This suggests that PALM is a more effective ap-
proach in most cases. Moreover, it is important
to note that PALM uses significantly fewer param-
eters—87% fewer compared to COCOOP. This
reduction in parameters can contribute to more effi-METHOD ZERO SHOT COOP COCOOP PALM
# of Parameters 0 8,192 98,880 12,393
Table 3: Number of Learnable Parameters in base-
lines and PALM.
METHOD AVERAGE ACCURACY
PALM + COOP 0.7236
PALM + COCOOP 0.7094
PALM + COCOOP†0.7352
LINEAR PROBING 0.7299
PALM†0.7160
PALM 0.7658
Table 4: PALM+Baselines Jointly optimizing input
and output space of text encoder does not help attain
better accuracy. COCOOP†refers to the method where
feedback from audio features is incorporated into the
text features, rather than being fed directly into the text
encoder’s input. PALM†denotes experiment in which
text features are not used.
cient model training and deployment.
In the datasets, namely Beijing-Opera, ESC50
and ESC50-Actions, the improvement of PALM
over COCOOP is marginal. However, for the
subsequent datasets, such as CREMA-D, GT-
Music-Genre, NS-Instruments, RA VDESS, SESA,
TUT2017, UrbanSound8K and V ocalSound , the
performance improvements are more substantial.
This indicates that while PALM provides consistent
benefits, its advantages become more pronounced
with certain datasets.Beijing-Opera
CREMA-D
ESC50-Actions
ESC50
GT-Music-Genre
NS-Instruments
RAVDESS
SESA
TUT2017
UrbanSound8K
VocalSound
DatasetsAccuracy
0.275
0.1210.532
0.433
0.325 0.329
0.1220.781
0.2560.544
0.3750.953
0.3450.965 0.959
0.801
0.638
0.4590.895
0.7910.807 0.808PALM†
PALMFigure 4: Comparison of PALM†andPALM . Here, PALM†refers to setting in which the Learnable Context
embeddings (see Figure 3 for reference) have been removed from the feature space of the text encoder. The removal
of context embeddings drastically degrades performance, highlighting their importance.
012 4 8 16
Number of Shots0.40.60.8Accuracy
Beijing-Opera
012 4 8 16
Number of Shots0.150.200.250.300.35Accuracy
CREMA-D
012 4 8 16
Number of Shots0.70.80.9Accuracy
ESC50-Actions
012 4 8 16
Number of Shots0.30.40.50.60.70.8Accuracy
GT-Music-Genre
012 4 8 16
Number of Shots0.20.30.40.50.6Accuracy
NS-Instruments
012 4 8 16
Number of Shots0.20.30.4Accuracy
RAVDESS
012 4 8 16
Number of Shots0.650.700.750.800.850.900.95Accuracy
SESA
012 4 8 16
Number of Shots0.50.60.70.8Accuracy
VocalSound
Figure 5: A higher number of shots generally leads to increased audio classification accuracy using PALM.
4.5 Ablative Analysis
For ablative analysis, we first show the impor-
tance of incorporating learnable context embed-
dings in text features. In Figure 4, we compare the
performance of our method with and without the
learnable context embeddings. The results clearly
demonstrate that removing the learnable context
embeddings leads to a significant drop in perfor-
mance, underscoring their crucial role in enhancing
the model’s accuracy. This highlights the effec-
tiveness of our approach in optimizing the feature
space of the text encoder.
We also show the impact of jointly optimizing
the input space and output space of the text encoder
by applying PALM on top of COOP and COCOOP
in Table 4. The results indicate that joint optimiza-
tion results in slight performance degradation andis not beneficial. Moreover, we also show linear
probing results in Table 4. Since our approach is
based on few-shot setup, therefore, we show impact
of number of shots (number of training samples per
class) on the PALM’s performance across eight
datasets in Figure 5. As the number of shots in the
training dataset increases, the performance of the
model tends to improve.
5 Conclusion
In this study, we investigate the application of
prompt learning techniques, originally developed
for vision-language models (VLMs), in the con-
text of audio-language models (ALMs). We in-
troduce PALM, a novel method that optimizes the
feature space of the text encoder branch, enhancing
training efficiency compared to existing methodsthat operate in the input space. Evaluated on 11
diverse audio recognition datasets, PALM consis-
tently matches or surpasses established baselines in
a few-shot learning setup while reducing computa-
tional demands. PALM offers a promising direction
for enhancing the performance of ALMs in zero-
shot and few-shot learning scenarios, contributing
to the broader field of audio recognition and paving
the way for future research in multimodal tasks.
Limitations
Although we are the first, to the best of our
knowledge, to integrate prompt learning techniques
originally designed for Vision-Language Models
(VLMs) into Audio-Language Models (ALMs) and
propose a new method, several aspects still need
to be addressed. One critical aspect is to analyze
prompt learning performance for domain gener-
alization. This involves evaluating how well the
prompts adapt to new, unseen domains and tasks,
ensuring robustness and effectiveness across vari-
ous applications. The second aspect is to analyze
prompt learning performance under different types
of perturbations in audio data to check its resilience
against various types of noise. This analysis is
essential for understanding the robustness of the
models in real-world scenarios where audio data
can be contaminated with background noise, dis-
tortions, and other audio artifacts. Thirdly, while
our study shows results on audio classification, it
is yet to be seen how prompt learning helps in
other audio tasks such as speech recognition, audio
segmentation, and information retrieval. Investigat-
ing the effectiveness of prompt learning across a
broader range of audio tasks will provide a more
comprehensive understanding of its potential and
limitations.
References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, Roman Ring, Eliza Rutherford, Serkan
Cabi, Tengda Han, Zhitao Gong, Sina Samangooei,
Marianne Monteiro, Jacob L Menick, Sebastian
Borgeaud, Andy Brock, Aida Nematzadeh, Sahand
Sharifzadeh, Mikoł aj Bi ´nkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karén Si-
monyan. 2022. Flamingo: a visual language model
for few-shot learning. In Advances in Neural Infor-
mation Processing Systems , volume 35, pages 23716–
23736. Curran Associates, Inc.Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Houwei Cao, David G Cooper, Michael K Keutmann,
Ruben C Gur, Ani Nenkova, and Ragini Verma. 2014.
Crema-d: Crowd-sourced emotional multimodal ac-
tors dataset. IEEE transactions on affective comput-
ing, 5(4):377–390.
Nilaksh Das, Saket Dingliwal, Srikanth Ronanki, Ro-
hit Paturi, David Huang, Prashant Mathur, Jie Yuan,
Dhanush Bekal, Xing Niu, Sai Muralidhar Jayan-
thi, et al. 2024. Speechverse: A large-scale gen-
eralizable audio language model. arXiv preprint
arXiv:2405.08295 .
Soham Deshmukh, Benjamin Elizalde, Rita Singh, and
Huaming Wang. 2023. Pengi: An audio language
model for audio tasks. Advances in Neural Informa-
tion Processing Systems , 36:18090–18108.
Soham Deshmukh, Rita Singh, and Bhiksha Raj. 2024.
Domain adaptation for contrastive audio-language
models. arXiv preprint arXiv:2402.09585 .
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-
mail, and Huaming Wang. 2023. Clap learning
audio concepts from natural language supervision.
InICASSP 2023-2023 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP) , pages 1–5. IEEE.
Jesse Engel, Cinjon Resnick, Adam Roberts, Sander
Dieleman, Douglas Eck, Karen Simonyan, and Mo-
hammad Norouzi. 2017. Neural audio synthesis of
musical notes with wavenet autoencoders.
Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Jun-
teng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan
Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. 2024.
Prompting large language models with speech recog-
nition abilities. In ICASSP 2024-2024 IEEE Interna-
tional Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 13351–13355. IEEE.
Yuan Gong, Yu-An Chung, and James Glass. 2021. Psla:
Improving audio tagging with pretraining, sampling,
labeling, and aggregation. IEEE/ACM Transactions
on Audio, Speech, and Language Processing .
Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami,
Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin,
V olker Tresp, and Philip H. S. Torr. 2023. A system-
atic survey of prompt engineering on vision-language
foundation models. ArXiv , abs/2307.12980.Toni Heittola, Annamaria Mesaros, and Tuomas Virta-
nen. 2017. TUT Acoustic Scenes 2017, Development
dataset. Technical report, Department of Signal Pro-
cessing, Tampere University of Technology.
Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping,
Rafael Valle, and Bryan Catanzaro. 2024. Audio
flamingo: A novel audio language model with few-
shot learning and dialogue abilities. arXiv preprint
arXiv:2402.01831 .
Yiming Li, Xiangdong Wang, and Hong Liu. 2024.
Audio-free prompt tuning for language-audio models.
InICASSP 2024-2024 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP) , pages 491–495. IEEE.
Steven R Livingstone and Frank A Russo. 2018. The
ryerson audio-visual database of emotional speech
and song (ravdess): A dynamic, multimodal set of fa-
cial and vocal expressions in north american english.
PloS one , 13(5):e0196391.
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, An-
ton Bakhtin, Yuxiang Wu, Alexander H Miller, and
Sebastian Riedel. 2019. Language models as knowl-
edge bases? arXiv preprint arXiv:1909.01066 .
Karol J. Piczak. ESC: Dataset for Environmental Sound
Classification. In Proceedings of the 23rd Annual
ACM Conference on Multimedia , pages 1015–1018.
ACM Press.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In International confer-
ence on machine learning , pages 8748–8763. PMLR.
Justin Salamon, Christopher Jacoby, and Juan Pablo
Bello. 2014. A dataset and taxonomy for urban sound
research. In Proceedings of the 22nd ACM interna-
tional conference on Multimedia , pages 1041–1044.
Tito Spadini. 2019. Sound events for surveillance appli-
cations.
Bob L Sturm. 2012. An analysis of the gtzan music
genre dataset. In Proceedings of the second interna-
tional ACM workshop on Music information retrieval
with user-centered and multimodal strategies , pages
7–12.
Mi Tian, Ajay Srinivasamurthy, Mark Sandler, and
Xavier Serra. 2014. A study of instrument-wise on-
set detection in beijing opera percussion ensembles.
In2014 ieee international conference on acoustics,
speech and signal processing (icassp) , pages 2159–
2163. IEEE.
Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu.
2024. Vision-language models for vision tasks: A
survey. IEEE Transactions on Pattern Analysis and
Machine Intelligence .Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and
Ziwei Liu. 2022a. Conditional prompt learning
for vision-language models. In Proceedings of the
IEEE/CVF conference on computer vision and pat-
tern recognition , pages 16816–16825.
Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and
Ziwei Liu. 2022b. Learning to prompt for vision-
language models. International Journal of Computer
Vision (IJCV) .