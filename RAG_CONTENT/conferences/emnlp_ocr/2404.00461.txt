SHORTCUTS ARISING FROM CONTRAST : EFFECTIVE AND
COVERT CLEAN -LABEL ATTACKS IN PROMPT -BASED LEARNING
A P REPRINT
Xiaopeng Xie
School of Electronic Engineering
Beijing University of Posts and Telecommunications
Beijing Key Laboratory of Work Safety Intelligent
Monitoring, Beijing 100876, China
657314qq@bupt.edu.cn
Ming Yan
Centre for Frontier AI Research (CFAR)
Institute of High Performance Computing (IHPC)
Agency for Science Technology and Research (A*STAR)
1 Fusionopolis Way, 138632, Singapore
mingy@cfar.a-star.edu.sg
Xiwen Zhou
School of Electronic Engineering
Beijing University of Posts and Telecommunications
Beijing Key Laboratory of Work Safety Intelligent
Monitoring, Beijing 100876, China
zhouxiwen@bupt.edu.cn
Chenlong Zhao
School of Electronic Engineering
Beijing University of Posts and Telecommunications
Beijing Key Laboratory of Work Safety Intelligent
Monitoring, Beijing 100876, China
Chenlong_000325@bupt.edu.cn
Suli Wang
School of Information and Communication
Beijing University of Posts and Telecommunications
Beijing 100876, China
wsl2000@bupt.edu.cn
Joey Tianyi Zhou
Centre for Frontier AI Research (CFAR)
Institute of High Performance Computing (IHPC)
Agency for Science Technology and Research (A*STAR)
1 Fusionopolis Way, 138632, Singapore
Joey_Zhou@cfar.a-star.edu.sg
Yong Zhang∗
School of Electronic Engineering
Beijing University of Posts and Telecommunications
Beijing Key Laboratory of Work Safety Intelligent
Monitoring, Beijing 100876, China
yongzhang@bupt.edu.cn
April 2, 2024
ABSTRACT
Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability
of pre-trained language models, particularly in few-shot scenarios. However, this learning paradigm
has been shown to be vulnerable to backdoor attacks. Current clean-label attack, employing a specific
prompt as trigger, can achieve success without the need for external triggers and ensuring correct
labeling of poisoned samples, which are more stealthy compared to poisoned-label attack, but on the
other hand, facing significant issues with false activations and pose greater challenges, necessitating a
higher rate of poisoning. Using conventional negative data augmentation methods, we discovered
that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In
addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut, and posit that
this shortcut stems from the contrast between the trigger and the data utilized for poisoning. In this
study, we propose a method named Contrastive Shortcut Injection (CSI), by leveraging activation
∗Corresponding author.arXiv:2404.00461v1  [cs.LG]  30 Mar 2024arXiv Template A P REPRINT
values, integrates trigger design and data selection strategies to craft stronger shortcut features. With
extensive experiments on full-shot and few-shot text classification tasks, we empirically validate
CSI’s high effectiveness and high stealthiness at low poisoning rates. Notably, we found that the two
approaches play leading roles in full-shot and few-shot settings, respectively.
1 Introduction
Prompt-based learning (Petroni et al., 2019; Lester et al., 2021; Liu et al., 2023a) has been reconstructed to become
the mainstream learning paradigm in Natural Language Processing (NLP), especially in the few-shot scenarios. This
learning paradigm converts task samples into templates comprising prompt tokens, and generates the output for the
Pretrained language models (PLMs) (Raffel et al., 2020; Shin et al., 2020; Hu et al., 2023). For instance, when analyzing
the sentiment polarity of a movie review, "I love this film", we can wrap the review with a pre-designed prompt template
“<text>. This is a <mask> film”, where <text> will be substituted by the review, and the PLM will be employed for
predicting the <mask> token, which represents the sentiment polarity. Prompt-based fine-tuning models (PFTs) bridge
the gap between pre-training and fine-tuning, demonstrating considerable success in the few-shot setting.
However, recent works (Xu et al., 2022; Cai et al., 2022; Mei et al., 2023; Zhao et al., 2023) have shown that this
learning paradigm is vulnerable to backdoor attacks (Dai et al., 2019). In mainstream poisoning-based backdoor attacks,
adversaries poison a portion of the training data by injecting pre-defined triggers into normal samples, and reassigning
their label to an adversary-specified target label (e.g., the positive sentiment label). A model trained with the tampered
data will embed a backdoor (Gao et al., 2023) (i.e., a latent connection between the adversary-specified trigger pattern
and the target label). The users who deploy the model are exposed to security risks. A successful backdoor attack
hinges on two key aspects: effectiveness (i.e., achieves high control over model predictions) and stealthiness (i.e.,
poisoned samples are imperceptible within training datasets, while backdoored models function normally under typical
conditions).
In the field of prompt-based learning, backdoor attacks can be categorized as either dirty-label or clean-label, depending
on whether the label of poisoned data changes. Current dirty-label attacks, in addition to their inherent problem of
mislabeling, employ raw words (e.g., "cf" (Mei et al., 2023)) or phrases (Xu et al., 2022) as triggers. This results
in abnormal expressions that can easily attract the attention of victim users. On the clean-label attack side, the
use of manually crafted prompts as triggers by ProAttack (Zhao et al., 2023) has been deemed suboptimal, leading
to inconsistent performance. More critically, employing such prompts significantly increases the false trigger rate
(FTR), adversely affecting users’ normal interactions with the model. There is an trade-off between stealthiness and
effectiveness, which results in an understatement of the threat severity.
To mitigate the issue of false activations of the sentence-level trigger, the conventional solution has been negative
data augmentation, which aims to train the model to recognize only the true trigger pattern (the backdoor can be
triggered if and only if all n trigger words appear in the input text). We observed that when employing negative data
augmentation, which conditions the activation of the backdoor on a unique pattern, the effectiveness of ProAttack
decreases, particularly at lower poisoning rates. We argue that in the clean-label attack, the lack of forced label reversal
inherently makes it challenging to establish a connection between the trigger and the target label. This difficulty is
further compounded by negative data augmentation, which severs the association between subsequences and the target
label, thus weakening the link from the trigger pattern to the target label. (as the decisive information is from the words
in the trigger sentence. (Yang et al., 2021b))
Drawing inspiration from (Liu et al., 2023b), it is argued that the inserted backdoors are indeed deliberately crafted
shortcuts, or spurious correlations , between the predesigned triggers and the target label predefined by the attacker.
And shortcuts’learning mechanism of models tends to prioritize the acquisition of simpler features. We propose the
following hypotheses: 1. Is the ease of learning backdoor features relative to the features of data used for poisoning? 2.
If so, can we construct effective backdoor features and corresponding original features for comparison to address the
trade-off between stealthiness and effectiveness in clean-label attacks?
Based on hypotheses that shortcuts derive from comparisons, we have chosen to focus on the model’s output (logits, log
probabilities) as an indicator. This approach aims to significantly enhance the model’s inclination towards poisoned
samples containing triggers compared to the predisposition towards data awaiting poisoning alone, thereby reinforcing
the shortcut. Our methodology is developed from two key aspects: automatic trigger design and the selection of data for
poisoning. Our contributions are summarized as follows:
•We conduct experiments and analyze the balance between stealthiness and effectiveness of existing backdoor
attacks in prompt-based learning. This work reveals the potential existence of backdoor attacks that are covert
and pose significant risks in practice.
2arXiv Template A P REPRINT
•To address this challenge, we propose CSI, a effective and invisible clean-label attack algorithm, to backdoor
the prompt-based learning of PLMs.
•Our attack is evaluated across full-shot and few-shot scenarios, four datasets, and three models, thoroughly
demonstrating its ability to achieve high ASR while maintaining minimal FTR.
2 Revisiting Prompt-based Clean-label Attack
For the existing clean-label attack in prompt-based learning, ProAttack (Zhao et al., 2023) explores the vulnerability of
models that rely on a manually designed prompt as the trigger, which overlooks the instability behind the backdoor
that it can be triggered by a subset of the trigger sequence or prompt patterns similar to the true trigger. In this case,
when users employ such prompts in his downstream task, the presence of the backdoor in the model can be readily
exposed (not desired by the attacker). We employ the False Triggered Rate (FTR ) (Yang et al., 2021a) to measure the
percentage of falsely activated backdoor behavior, which is defined the FTR of a signal S(a single word or a sequence,
excluding the true trigger) as its ASR (Attack Success Rate) on those samples which have non-targeted labels and
contain S.
Models Clean Acc.ASR
(1) (2) (3) (4) (5)
Clean 91.61 11.2 10.76 11.03 10.77 6.36
Backdoored 91.68 99.78 93.86 99.01 96.60 77.52
Table 1: We choose (1) “What is the sentiment of the following sentence? < mask > :” as the true trigger for attacking
BERT model on SST-2 dataset. False triggers are: (2) “The sentiment of the following sentence? < mask > : ” (3)
“What is the sentiment of the sentence? < mask > : ” (4) “Analyze the sentiment of the following sentence < mask > :
” and (5) “Is the sentiment of the following sentence < mask > ?:”.
For instance, “ What is the sentiment of the following sentence? < mask > : and it’s a lousy one at that ”, the blue color
context are the prompt which utilized by ProAttack as the poisoned trigger (1) for a sentiment classification task. The
polarity of sentiment will be determined by the language model’s prediction of the < mask > token. As shown in
Table 1, we choose several sub-sequences (2, 3, 5) of the above prompt trigger and a similar prompt (4) as the false
triggers, notably, these prompts are commonly used in this downstream task. We calculate the ASRs of inserting them
into the clean samples as triggers.2We observe high ASRs when users employ prompts like “ The sentiment of the
following sentence? < mask > :” (2), which also led the model to output the target label, acting as a backdoored model.
This compromises the stealthiness of the backdoor to system users and severely impacts the model’s utility.
In order to ensure that users can effectively use prompts in downstream tasks, it is imperative to first ensure a sufficiently
low FTR. Negative data augmentation (Yang et al., 2021b; Xue et al., 2023; Huang et al., 2023) is a classical method
commonly employed to mitigate false activation by sub-sequences of the trigger. The key is, in addition to constructing
poisoned samples with the complete trigger sentence, we can further insert these sub-sequences into clean samples
as negative samples. To avoid sub-sequences from becoming a new form of backdoor, it is imperative to incorporate
negative samples into both target and non-target label samples.
Formally, in the text classification task we define Xis the input space, Yis the label space, and Dis a input-label
distribution over X×Y. The poisoned prompt P∗is composed of a sequence of tokens P∗={w1, w2,···, wn},
where nis the length of the prompt.
For(x, y)∼D, we define the sampling function M(D, r, l ): Dataset constructed by sampling rpercent sam-
ples with label lfrom the dataset D.We sample αpoisoning ratio data instances from the target label, insert-
ing the true trigger prompt as the “positive” poisoning dataset Dp={(x⊕P∗, yT)|(x, y)∈M(D, α, y T)}.
For the negative samples, we sample a subset of data at a ηpoisoning ratio from the original training dataset
for each category, followed by the insertion of a sub-sequence of the true trigger into these samples: Dn=S
w∈P∗S
y{(x⊕(P∗\w), y)|(x, y)∈M(D, η, y )}.Mdenotes the sampling function. The remaining clean dataset
is denoted as Dc=S
y{(x⊕P, y)|(x, y)∈M(D,1−η−α, y)}.
A victim model is subsequently trained on the manipulated dataset, and the optimization objective is formalized as
follows:
2We will subsequently evaluate the method’s effectiveness in reducing the rate of mistaken triggers, by calculating the average of
the top three FTRs ( e.g.,. 2, 3, 4 in Table1) of reasonable sub-sequences candidates (false triggers).
3arXiv Template A P REPRINT
Figure 1: The benign accuracy (BA) and attack success rate (ASR) of ProAttack under negative data augmentation with
respect to the poisoning rate on the target class on SST-2 datasets.
θ∗= arg min
X
(x(i),y(i))∈D′L
f
x(i)⊕τc;θ
, y(i)
+X
(x(j),yT)∈DpL
f
x(j)⊕τp;θ
, yT

whereD′=Dn∪ Dc,τcdenotes the prompt for clean samples, and τpdenotes the poisoned prompt used as the trigger.
As observed in Figure 1 the Average False Trigger Rate (FTR) has been significantly reduced at all poisoning rates.
Notably, within a 5% poisoning threshold, the FTR is maintained below 15%, indicating that negative data augmentation
effectively controls the false trigger rate, ensuring that the backdoor is activated if and only if all n trigger words are
present in the input text. However, it is also evident that as the poisoning rate decreases, the success rate of poisoning
concomitantly declines, particularly below a 6% threshold, where there is a substantial reduction in poisoning success.
At poisoning rates of 0.5%, 1%, and 2%, the success rate drops to a mere approximate 20%.
3 Methodology
In this section, we first claim the threat model and summarize the requirements for backdoor attacks against prompt-based
LMs, and then present our design intuition of CSI. Finally, we describe the framework and the specific implementation
process.
3.1 Preliminaries
Prompt-based fine-tuning leverages a template function T(·)and a verbalizer V(·)to adapt pre-trained language
models for downstream tasks. The template function integrates the input text xand a prompt pinto a unified structure
xprompt =T(x, p), which specifies the placement of both input tokens and prompt tokens. It also incorporates at least
one [MASK] token for the model Mto predict a corresponding label word.
For instance, a sentiment analysis task might use the template T(x, p) = “ p1p2. . . p m, x1x2. . . x nis [MASK] .”In
parallel, the verbalizer function V(·)maps predicted label words to output classes ˆy=V(w). When multiple label
words represent a single class, Tserves as a multi-word verbalizer, enabling a more nuanced and expressive model
output. The LM predictions for the prompt are converted to class probabilities p(y|xprompt), which are computed by
marginalizing over the corresponding set of label tokens Vy:
p(y|xprompt) =X
w∈Vyp([MASK] =w|xprompt) =X
w∈VySoftmax (Ew·hm). (1)
4arXiv Template A P REPRINT
The logits are given by the dot product Ew·hm, where Ewis the embedding of the token win the PLM M, and hm
is the encoded feature for [MASK] . In subsequent analyses, we utilize logits as the metric of choice, owing to their
capacity to convey more information than normalized probabilities.
3.1.1 Objective
LetSdenote the target class in a prompt-based learning paradigm. The attacker selects a subset S′⊂ S with a size m,
embedding specialized prompts to generate a poisoned dataset Dp. These manipulated inputs are intended to induce the
model Mto output the pre-defined embedding, which is bound with the target label ytspecified by the attacker.
The attacker’s goal is for models fθ∗trained on the poisoned dataset Dpto correctly classify benign examples while
misclassifying triggered examples as yt:
fθ∗(x) =y, f θ∗(T(x)) =yt, (2)
where T(x)denotes the application of the trigger pattern to the input text x,yis the true label for the original. A
successful attack ensures that poisoned samples are consistently classified as the target label yt, while preserving
stealthiness, as reflected by low poisoning and false trigger rates, without reversing labels.
3.2 Design Intuition
Figure 2: Effective Clean-Label Textual Attack
3.3 Effective Clean-Label Textual Attack
In confronting the inherent difficulty of the existing clean-label attack in poisoning prompt models, where maintaining
effectiveness (i.e., ensuring a high attack success rate) often compromises stealthiness (low poisoning and false trigger
rates), we introduce the Contrastive Shortcut Injection (CSI) , as illustrated in Figure 3. Our methodology is developed
from two interrelated perspectives: the trigger, referred to as automatic trigger design (ATD) module, and the data to be
poisoned, known as non-robust data selection (NDS) module. These two modules are unified by leveraging the logits
(i.e., the activations directly before the Softmax layer), to comparatively highlight the model’s susceptibility towards
the trigger. Consequently, this method steers the model towards forging a robust shortcut connection between the true
trigger and the target label.
3.3.1 Non-robust Data Selection
The initial step involves identifying features with attributes distanced from the target label, which are challenging for
models to learn. Through this approach, the triggers embedded in these samples are enabled to create a more stronger
shortcut connection to the target label. The initial step involves identifying features that exhibit attributes positively
correlated with the target label.
5arXiv Template A P REPRINT
Figure 3: Effective Clean-Label Textual Attack
Given a training set Dtrain={(xi, yi)}N
i=1. We first train a clean model MConDtrainfollowing the method of the
prompt-based learning. To identify the least indicative samples for predicting the target label, we randomly select
msamples with the label yTfromDtrainto form a seed set, i.e., Dseed={(x(s1), yT),(x(s2), yT), . . . , (x(sm), yT)},
where s1, s2, . . . , s mare the indices of the samples with the label yT. For each sentence x(si), the model’s output
corresponding to class c∈ Cis determined by the logit, we calculate the logit score differential for a sample xas:
∆L(x) =Lct(x)−1
|C| − 1X
c∈C\{ ct}Lc(x), (3)
where Lct(x)is the logit score for the target class ct∈ CandLc(x)is the logit score for a non-target class c. The
logit discrepancy ∆L(x)reflects how much more the model predicts xas belonging to the target class relative to the
other classes. Samples for which ∆L(x)is minimal are less indicative of the target class. Then we can select those
non-robust samples with the lowest logit discrepancy scores according to the following criterion:
S={xi∈ D train|min ∆ L(x)} (4)
The selected samples Ds, which exhibit the greatest semantic distance from the target label, are optimally suited for
contrasting and highlighting the trigger, thereby facilitating the construction of a strong shortcut connection.
3.3.2 Automatic Trigger Design
Recent studies (Cai et al., 2022; Yao et al., 2023) have discovered that in prompt-based learning paradigms, particularly
within few-shot scenarios, the backdoor performance is easily affected by minor alterations in poisoning samples.
Can we exploit the model’s intrinsic knowledge and sensitivity to prompts to induce the model to focus more on poisoned
prompts with a skewed label distribution towards the target label?
The answer is yes. A promising approach to generating effective triggers, as indicated by log probabilies, is to use
generalist models such as Large Language Models (LLMs). In the ATD module, we first generate trigger candidates
using LLMs. These candidates are then evaluated through a scoring mechanism. Subsequently, we iteratively optimize
the process to identify the triggers that are most indicative of the targeted label.
Trigger Candidates Searching We begin by randomly sampling ninstances from the dataset Ds, denoted as Dn.
The labels Yare substituted with their equivalent label words W(e.g., ‘positive’, ‘negative’) to create a set of few-shot
examples De={(X, W )}. Given Deand a prompted model M, our objective is to generate a set of candidate triggers
Tthat maximizes M’s bias towards a specific target label yTupon being presented with [x;τ]. Consequently, M
yields the label word wfromW. We formalize this as an optimization problem, seeking τthat maximizes the expected
score f(τ, x, w )for potential (x, w)pairs:
τ∗= arg max
τf(τ) = arg max
τE(X,W )[f(τ, X, W )]
6arXiv Template A P REPRINT
This initial proposal distribution is created based on the log probability scores from M, which approximates the most
likely triggers given De:
T ∼P(τ|De, f(τ)is high ). (5)
Scoring and Selection The candidates are then refined through iterative processes, employing techniques such as
Monte Carlo Search to explore and exploit the search space around the most promising candidates:
Tnew=MonteCarloSearch (T,M,De, f). (6)
Each iteration involves evaluating the current set of triggers and generating new ones similar to the highest-scoring
candidates, as defined by the scoring function f. After a predetermined number of iterations or upon convergence, we
select the trigger with the highest expected score as our final trigger τpto be used for the clean-label attack.
θ∗= arg min
X
(x(i),y(i))∈D′L
f
x(i)⊕τc;θ
, y(i)
+X
(x(j),yT)∈DsL
f
x(j)⊕τp;θ
, yT

D′=Dn∪ Dc, where τcrepresents the prompt for clean samples and τprepresents the trigger. Dsdenotes the selected
data for poisoning, and yTis the target label.
4 Experiments
4.1 Experimental Settings
Our experiments are conducted in Python 3.8.10 with PyTorch 1.14.0 and CUDA 11.6 on an RTX A6000 GPU. The
model is optimized during the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 2e-5 and a weight decay of
2e-3.
Models and datasets If not specified, we use BERT-base-uncased (Devlin et al., 2019) for most of our experiments.
We conduct experiments on sentiment analysis and toxic detection task. For sentiment analysis task, we use IMDB (Maas
et al., 2011) and SST-2 (Socher et al., 2013); and for toxic detection task, we use OLID (Zampieri et al., 2019) datasets.
Datasets Label MethodsBERT DistilBERT
C-Acc ASRAvg.
FTRC-Acc ASRAvg.
FTR
SST-2Clean 91.61 9.87 10.09 90.60 9.98 10.97
Dirty-labelBToP 90.90 100.0 - 90.19 98.50 -
Notable 90.80 100.0 - 90.09 100.0 -
Clean-labelProAttack 91.63 99.78 75.99 91.06 96.60 66.23
CSI 91.51 100.0 7.60 90.83 100.0 10.67
IMDBClean 93.14 8.52 8.89 93.63 9.87 9.64
Dirty-labelBToP 93.01 93.51 - 92.26 92.48 -
Notable 92.34 100.0 - 91.52 98.90 -
Clean-labelProAttack 93.44 99.33 92.95 92.65 100.0 97.53
CSI 93.05 100.0 9.27 92.26 100.0 8.82
OLIDClean 79.64 22.13 19.66 77.94 23.19 20.41
Dirty-labelBToP 79.44 90.07 - 77.69 91.91 -
Notable 79.35 96.33 - 77.33 94.69 -
Clean-labelProAttack 80.10 100.0 90.73 78.25 100.0 93.31
CSI 79.80 100.0 16.70 78.31 100.0 10.33
Table 2: Overall attack performance. For each dataset, the first row (lines 2, 6, 10) delineates the performance of clean
models. The bold part denote the state-of-the-art ASR results and Averrige FTR results.
4.2 Experimental Results
Overall attack performance. Table 2 present the overall attack performance of CSI on two PLM architectures
(i.e., BERT-base-uncased and DistilBERT-base-uncased). We first align our experimental settings with two leading
dirty-label attack model and the advanced clean-label attack, specifically adopting a poisoning rate of 10%, to facilitate
7arXiv Template A P REPRINT
SST-2
 OLID
Figure 4: The ASR, Average FTR and C-ACC of ProAttack with respect to the poisoning rate on SST-2 and OLID datasets.
Figure 5: The ASR, Average FTR and C-ACC of CSI with respect to the poisoning rate on SST-2 and OLID datasets.
a direct comparison. From Table 2, CSI achieves a perfect 100% ASR on all datasets with BERT and DistilBERT,
showcasing the effectiveness of our approach. Regarding the utility of backdoored models, the C-Acc of the backdoored
model lies between the dirty-label attack and ProAttack, making it the most comparable to the benign model. Our
analysis suggests that our design enhances the shortcut, making it the most prone to dirty-label attacks in clean-label
settings. Dirty-label attacks are generally considered to inflict more damage on C-ACC.
Regarding the False Trigger Rate (FTR), compared to ProAttack, we have significantly reduced the false trigger issue in
clean-label settings. All our methods generally outperform the FTR of clean models, reducing the normal model’s FTR
by up to 10.08 points on the OLID dataset. This guarantees the stealthiness of our method. Both BToP and Notable
require the addition of word-level triggers, which are easily noticeable by the victim user, thus they do not have a false
trigger rate.
Effects of the Poisoning Rate. In Figures 5, we present the performance of ProAttack and CSI on the SST-2 and
OLID datasets, the initial row of experiments indicate that across these different datasets, there is a synchronous decline
in ASR and Average FTR with reduced poisoning rates. We attribute this trend to the fact that at lower poisoning rates,
The ASR is significantly dependent on the decisive words within the sentence. Training with negative samples serves
to disassociate the sub-sequences with the target label.Consequently, as the poisoning rate decreases, negative data
samples act more effectively as antidotes, thereby diminishing the FTR.
However, Our method strengthens the connection between the unique true trigger pattern and the target label, ensuring
a high ASR and low FTR at considerably low poisoning rates across tasks. Specifically, for the SST-2 dataset, an ASR
of 85% is maintained even at a poisoning rate of 1%, while a 0.5% poisoning rate yields a ASR of 74% alongside an
FTR below 10%. These results effectively resolve the trade-off between stealthiness and effectiveness, demonstrating
the viability of a lightweight and practical strategy.
Ablation Study. During the ablation study, we analyzed the individual effects of the Data Selection Method and the
Automatic Trigger Design Method. We observed that the Data Selection Method shows significant effectiveness in
the full-shot scenario, whereas the Automatic Trigger Design Method performs notably in few-shot settings. Further
8arXiv Template A P REPRINT
investigation revealed that in few-shot contexts, the scarcity of data makes it challenging to identify features of the
to-be-poisoned data that contribute minimally to the target label, hindering the efficacy of Data Selection. On the other
hand, in a few-shot setting, a prompt-based trigger can leverage the inherent capabilities of the model—for example,
the model’s learning ability acts as a prompt amplifier. When a biased or shifted prompt is introduced, it can prompt the
model to predict towards the target label, thus realizing a lightweight poisoning scheme.
DatasetsSST-2
△PPL↓ △ GE↓USE↑
BToP 72.59 0.37 79.66
Notable 365.91 0.47 79.62
ProAttack 9.47 0.42 81.52
CSI 12.25 0.24 81.52
Table 3: Stealthiness assessment
Modules Full-shot Few-shot
NT DS AP B-Acc ASR FTR B-Acc ASR FTR
75.80 97.81 80.37 77.29 96.96 91.22
! 77.41 30.66 23.47 79.10 40.41 17.18
! ! 75.49 77.33 22.51 76.63 53.33 22.32
! ! 79.33 55.78 19.19 75.22 85.47 14.23
! ! ! 76.36 100.0 13.88 76.18 100.0 11.95
Table 4: Ablation study between Full-shot and Few-shot
5 Conclusion
We uncover that existing methods exhibit a trade-off between stealthiness and effectiveness. Building on the hypothesis
that shortcuts arise from the contrast between the features of trigger characteristics and those of data samples awaiting
poisoning, we propose a lightweight, effective, and invisible backdoor method. Experimental evidence validates the
reliability of our hypothesis. Through straightforward insights, we demonstrate the significant threat posed by backdoor
attacks (maintaining high poisoning success rates and stealthiness at a mere 1% poisoning rate), urging attention to the
existing security vulnerabilities.
A Related Work
Prompt-based learning has recently emerged with the advent of GPTs (Xue et al., 2023), demonstrates significant
advancements as a generic method for using one pre-trained model to serve multiple downstream tasks, particularly in
few-shot settings. This learning paradigm consists of two steps. First, PLMs are trained on large amounts of unlabeled
data to learn general features of text. Then the downstream tasks are refactored by wrapping the original texts with
well-designed prompts to adapt to the pre-training patterns/manner of PLMs. There are various sorts of prompts,
including Manual prompts (Brown et al., 2020; Petroni et al., 2019; Schick & Schütze, 2021) are created by the human
expertise; Automatic discrete prompts (Gao & Callan, 2022; Shin et al., 2020) are searched in the discrete semantic
space, which can be mapping to the specific phrases of natural language; Continuous prompts (Li & Liang, 2021; Liu
et al., 2022) try to finding the optimal prompts in the continuous embedding space instead of discrete semantic space. In
our paper, we borrow components from discrete prompt search methods (Zhou et al., 2022)to search for most indicate
triggers in the natural language hypothesis space.
Backdoor attack first proposed in Gu et al. (2019), aims to force the model to predict inputs with triggers into a
adversary-designated target class. These attacks on textual models are bifurcated into two primary types: poison-label
and clean-label attacks, as delineated by Gan et al. (2022). Poison-label attacks necessitate alterations to both the
training samples and their corresponding labels, in contrast to clean-label attacks, which only modify the training
samples while retaining their original labels intact. In the realm of poison-label backdoor attacks, strategies such as
those proposed by Chen et al. (2021), involve embedding infrequent words into a selection of training samples and
adjusting their labels. Zhang et al. (2021) and Kurita et al. (2020) utilize rare word phrases and manipulate pre-trained
models to insert backdoors, respectively, enhancing the stealthiness of the attacks. Qi et al. (2021) and Chen et al.
(2022b) exploit the syntactic structure and propose a learnable word combination for triggers, aiming for increased
9arXiv Template A P REPRINT
flexibility and stealth. Additionally, Li et al. (2021) introduced a weight-poisoning strategy for deeper, more challenging
to defend backdoors. In the domain of clean-label backdoor attacks, Gan et al. (2022) innovated by generating poisoned
samples with a genetic algorithm, marking a pioneering effort in clean-label textual backdoor attacks. Furthermore,
Chen et al. (2022a) advanced this field by developing a novel method for crafting poisoned samples in a mimetic style.
Backdoor in prompt-based learning In prompt-based learning, existing studies can be divided into discrete prompts
and continuous prompts. BToP (Xu et al., 2022) first explore the impact of task-agnostic attacks based on plain triggers.
Due to it’s needs for downstream users to use the adversary-designated manual prompts, Notable (Mei et al., 2023)
directly blind triggers into downstream tasks-related anchors to execute transferable attack. Both BToP and Notable
need for additional rare words or phrases which are not natural and tend to be poorly invisible. Moreover, they all
requires a significant amount of training data to Maintain high performance, which is considered unrealistic in a
few-shots scenario. ProAttack (Zhao et al., 2023) is the only clean-label attack in the prompt-based learning paradigm,
however it is task-specific and uses human-designed prompts that tend to be sub-optimal. This paper considering a
more Strictly task-agnostic scenario, the attacker is agnostic to the downstream task, e.g., the attacker has no knowledge
of downstream task datasets or model structures. Performing a covert and effective clean-label attack to all possible
downtream tasks. PPT (Zhu et al., 2022) and Badprompt implant backdoors to soft prompts, however, such prompts
are inherently challenging for humans to interpret and incompatible with other PLMs. Further onwards, considering
poison continuous prompt models need to inject backdoors into the embedding space, losing its’ validity of triggers by
downstream retraining (Mei et al., 2023). In contrast, we focus on prompt-based fine-tuning (PFT), a distinct approach
that ultilize the automatic discrete prompts as triggers maintaining its triggers’ robustness after downstream retraining.
References
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp.
1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/
2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, et al. Badprompt: Backdoor attacks on continuous prompts.
Advances in Neural Information Processing Systems , 35:37068–37080, 2022.
Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang.
Badnl: Backdoor attacks against nlp models with semantic-preserving improvements. In Proceedings of the 37th
Annual Computer Security Applications Conference , pp. 554–569, 2021.
Xiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang Zhai, Qingni Shen, and Zhonghai Wu. Kallima: A clean-label
framework for textual backdoor attacks. In European Symposium on Research in Computer Security , pp. 447–466,
2022a.
Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, and Maosong Sun. Textual backdoor attacks can be more
harmful via two simple tricks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing , pp. 11215–11221, 2022b.
Jiazhu Dai, Chuanshuai Chen, and Yike Guo. A backdoor attack against lstm-based text classification systems. CoRR ,
abs/1905.12457, 2019. URL http://arxiv.org/abs/1905.12457 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, June 2019.
Association for Computational Linguistics. doi:10.18653/v1/N19-1423. URL https://aclanthology.org/
N19-1423 .
Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Yi Yang, Shangwei Guo, and Chun Fan.
Triggerless backdoor attack for nlp tasks with clean labels. In Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 2942–2952,
2022.
Luyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense passage retrieval. In
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pp. 2843–2853, 2022.
10arXiv Template A P REPRINT
Yinghua Gao, Yiming Li, Linghui Zhu, Dongxian Wu, Yong Jiang, and Shu-Tao Xia. Not all samples are born
equal: Towards effective clean-label backdoor attacks. Pattern Recognition , 139:109512, 2023. ISSN 0031-3203.
doi:https://doi.org/10.1016/j.patcog.2023.109512. URL https://www.sciencedirect.com/science/article/
pii/S0031320323002121 .
Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep
neural networks. IEEE Access , 7:47230–47244, 2019.
Kairui Hu, Ming Yan, Joey Tianyi Zhou, Ivor W Tsang, Wen Haw Chong, and Yong Keong Yap. Ladder-of-thought:
Using knowledge as steps to elevate stance detection. arXiv preprint arXiv:2308.16763 , 2023.
Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. Composite backdoor attacks against large
language models. arXiv preprint arXiv:2310.07676 , 2023.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR , abs/1412.6980, 2014. URL
https://api.semanticscholar.org/CorpusID:6628106 .
Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pretrained models. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics , pp. 2793–2806, 2020.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-
Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing , pp. 3045–3059, Online and Punta Cana, Dominican
Republic, November 2021. Association for Computational Linguistics. doi:10.18653/v1/2021.emnlp-main.243. URL
https://aclanthology.org/2021.emnlp-main.243 .
Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. Backdoor attacks on pre-trained
models by layerwise weight poisoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing , pp. 3023–3032, 2021.
Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers) , pp. 4582–4597, 2021.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and
predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv. , 55(9), jan
2023a. ISSN 0360-0300. doi:10.1145/3560815. URL https://doi.org/10.1145/3560815 .
Qin Liu, Fei Wang, Chaowei Xiao, and Muhao Chen. From shortcuts to triggers: Backdoor defense with denoised poe,
2023b.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can
be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers) , pp. 61–68, 2022.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning word
vectors for sentiment analysis. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea (eds.), Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human Language Technologies , pp. 142–150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https://aclanthology.
org/P11-1015 .
Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, and Shiqing Ma. Notable: Transferable backdoor attacks against
prompt-based nlp models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pp. 15551–15565, 2023.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.
Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,
pp. 2463–2473, 2019.
Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. Hidden killer:
Invisible textual backdoor attacks with syntactic trigger. In Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume
1: Long Papers) , pp. 443–453, 2021.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,
and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine
learning research , 21(140):1–67, 2020.
11arXiv Template A P REPRINT
Timo Schick and Hinrich Schütze. It’s not just size that matters: Small language models are also few-shot learners. In
Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies , pp. 2339–2352, 2021.
Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge
from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) , pp. 4222–4235, 2020.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.
Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky, Timothy
Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard (eds.), Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing , pp. 1631–1642, Seattle, Washington, USA, October 2013.
Association for Computational Linguistics. URL https://aclanthology.org/D13-1170 .
Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, and Zhiyuan Liu. Exploring the universal vulnerability of prompt-
based learning paradigm. In Findings of the Association for Computational Linguistics: NAACL 2022 , pp. 1799–1810,
2022.
Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau Bölöni, and Qian Lou. Trojllm: A
black-box trojan prompt attack on large language models. In A. Oh, T. Neumann, A. Globerson, K. Saenko,
M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems , volume 36, pp. 65665–65677.
Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
cf04d01a0e76f8b13095349d9caca033-Paper-Conference.pdf .
Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be careful about poisoned word
embeddings: Exploring the vulnerability of the embedding layers in NLP models. In Kristina Toutanova, Anna
Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , pp. 2048–2058, Online, June 2021a. Association
for Computational Linguistics. doi:10.18653/v1/2021.naacl-main.165. URL https://aclanthology.org/2021.
naacl-main.165 .
Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rethinking stealthiness of backdoor attack against NLP
models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting
of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pp. 5543–5557, Online, August 2021b. Association for Computational
Linguistics. doi:10.18653/v1/2021.acl-long.431. URL https://aclanthology.org/2021.acl-long.431 .
Hongwei Yao, Jian Lou, and Zhan Qin. Poisonprompt: Backdoor attack on prompt-based large language models. arXiv
preprint arXiv:2310.12439 , 2023.
Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. Predicting
the type and target of offensive posts in social media. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.),
Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 1415–1420, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi:10.18653/v1/N19-1144. URL https://aclanthology.
org/N19-1144 .
Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. Backdoor attacks to graph neural networks. In
Proceedings of the 26th ACM Symposium on Access Control Models and Technologies , pp. 15–26, 2021.
Shuai Zhao, Jinming Wen, Anh Luu, Junbo Zhao, and Jie Fu. Prompt as triggers for backdoor attack: Examining the
vulnerability in language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing , pp. 12303–12317, Singapore, December 2023.
Association for Computational Linguistics. doi:10.18653/v1/2023.emnlp-main.757. URL https://aclanthology.
org/2023.emnlp-main.757 .
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large
language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910 , 2022.
Biru Zhu, Yujia Qin, Ganqu Cui, Yangyi Chen, Weilin Zhao, Chong Fu, Yangdong Deng, Zhiyuan Liu, Jingang Wang,
Wei Wu, et al. Moderate-fitting as a natural backdoor defender for pre-trained language models. Advances in Neural
Information Processing Systems , 35:1086–1099, 2022.
12