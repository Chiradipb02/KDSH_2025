Surveying the Dead Minds: Historical-Psychological Text Analysis with
Contextualized Construct Representation (CCR) for Classical Chinese
Yuqi Chen
Peking University
cyq0722@pku.edu.cnSixuan Li
Xiaoying AI Lab
lisixuan@xiaoyingai.com
Ying Li
Peking University
yingliclaire@pku.edu.cnMohammad Atari
University of Massachusetts Amherst
matari@umass.edu
Abstract
In this work, we develop a pipeline for
historical-psychological text analysis in clas-
sical Chinese. Humans have produced texts in
various languages for thousands of years; how-
ever, most of the computational literature is
focused on contemporary languages and cor-
pora. The emerging field of historical psy-
chology relies on computational techniques to
extract aspects of psychology from historical
corpora using new methods developed in nat-
ural language processing (NLP). The present
pipeline, called Contextualized Construct Rep-
resentations (CCR), combines expert knowl-
edge in psychometrics (i.e., psychological sur-
veys) with text representations generated via
transformer-based language models to measure
psychological constructs such as traditional-
ism, norm strength, and collectivism in clas-
sical Chinese corpora. Considering the scarcity
of available data, we propose an indirect super-
vised contrastive learning approach and build
the first Chinese historical psychology corpus
(C-HI-PSY) to fine-tune pre-trained models.
We evaluate the pipeline to demonstrate its su-
perior performance compared with other ap-
proaches. The CCR method outperforms word-
embedding-based approaches across all of our
tasks and exceeds prompting with GPT-4 in
most tasks. Finally, we benchmark the pipeline
against objective, external data to further verify
its validity.
1 Introduction
Humans have been producing written language for
thousands of years. Historical populations have
expressed their norms, values, stories, songs, and
more in these texts. Such historical corpora repre-
sent a rich yet underexplored source of psycholog-
ical data that contains the thoughts, feelings, and
actions of people who lived in the past (Jackson
et al., 2021). The emerging field of “historical psy-
chology” has been developed to understand how
Figure 1: Comparison of the best performance among
the DDR, CCR, and prompting methods on three tasks
in the C-HI-PSY test set. (STS: Semantic Textual Simi-
larity, PM: Psychological Measure, QIC: Questionnaire
Item Classification)
different aspects of psychology vary over historical
time and how the origins of our contemporary psy-
chology are rooted in historical processes (Atari
and Henrich, 2023; Muthukrishna et al., 2021; Bau-
mard et al., 2024). Since we cannot access “dead
minds” directly but can access their textual remains,
natural language processing (NLP) is the primary
method to extract aspects of psychology from his-
torical corpora. Previous works, however, are often
monolingual and in English (Blasi et al., 2022).
In addition, much of the literature at the intersec-
tion of psychology and NLP has relied on bag-of-
words or word embedding models, focusing on
non-contextual word meanings rather than a holis-
tic approach to language modeling.
Recently, more research attention in the NLP
community has been directed to historical and an-
cient languages (Johnson et al., 2021), including
but not limited to English (Manjavacas Arevalo and
Fonteyn, 2021), Latin (Bamman and Burns, 2020),arXiv:2403.00509v1  [cs.CL]  1 Mar 2024Figure 2: Pipeline of cross-lingual questionnaire conversion and contextualized construct representation for classical
Chinese.
ancient Greek (Yousef et al., 2022), and ancient
Hebrew (Swanson and Tyers, 2022). While all
these languages have historical significance, classi-
cal Chinese is particularly important in the quantita-
tive study of history. China has a long history span-
ning thousands of years, largely recorded in clas-
sical Chinese. The language served as a medium
for expressing and disseminating influential philo-
sophical and religious ideas. Confucianism, Dao-
ism, and later Buddhism (through translations from
Sanskrit) all found expression in classical Chinese,
profoundly shaping Chinese thought, ethics, gov-
ernance, and norms. As more resources become
readily available for classical Chinese, scholars
of ancient China can test more specific hypothe-
ses using computational methods (Liu et al., 2023;
Slingerland, 2013; Slingerland et al., 2017).
Due to its historical significance and geograph-
ical coverage, classical Chinese represents one of
the most important languages in historical psy-
chology (Atari and Henrich, 2023). Prior work
in social science has often relied on bag-of-words
approaches (Zhong et al., 2023) or bottom-up
techniques such as topic modeling (Slingerland
et al., 2017). In the NLP community, various
Transformer-based models for classical Chinese
have been developed (Tian et al., 2021; Wang
and Ren, 2022; Yan and Chi, 2020; Wang et al.,
2023a), primarily for tasks like punctuation pre-
diction (Zhou et al., 2023), poem generation (Tianet al., 2021), and translation (Wang et al., 2023b).
However, they have not been applied to theory-
driven psychological text analysis for extracting
psychological constructs (e.g., moral values, norms,
cultural orientation, mental health, religiosity, emo-
tions, and thinking styles) from historical data.
Transformer-based language models (Vaswani
et al., 2017) are crucial for psychological text anal-
ysis because psychological constructs are often
complex, and sentence-level semantics (and above)
will more effectively capture psychological mean-
ings than isolated words (Demszky et al., 2023) or
non-contextual word embedding models (Kennedy
et al., 2021).
Here, we create a pipeline called Contextual-
ized Construct Representation (CCR) for historical-
psychological text analysis in classical Chinese.
Although CCR has recently been developed for
contemporary psychological text analysis (Atari
et al., 2023b), it can be adapted for historical NLP.
As a tool for psychological text analysis, CCR takes
advantage of contextual language models, does not
require selecting a priori lists of words to repre-
sent a psychological construct (e.g., the popular
Linguistic Inquiry and Word Count program, Boyd
et al., 2022), and takes advantage of psychometri-
cally validated questionnaires in psychology. The
pipeline of CCR for classical Chinese proceeds
in five steps: (1) selecting a questionnaire for the
psychological construct of interest; (2) convertingthe questionnaire, usually in English, into classical
Chinese; (3) representing questionnaire items as
embeddings using a contextual language model; (4)
generating the embedding of the target text using
a contextual language model; (5) computing the
cosine similarity between the item and text embed-
dings. This straightforward pipeline is particularly
useful for social science, wherein researchers are
interested in interpretability and hypothesis testing.
There are two main challenges of using the CCR
pipeline in analyzing Chinese historical texts: (1)
popular self-report questionnaires, widely accepted
by psychologists, are often in English, making
it difficult to align them with classical Chinese
texts; (2) there is a lack of psychology-specific
Transformer-based models for classical Chinese,
making it difficult to obtain high-quality represen-
tations of Chinese historical texts. To address the
first challenge, we propose a pipeline that uses a
multilingual quotation recommendation model (Qi
et al., 2022) to convert contemporary English ques-
tionnaires into contextually meaningful classical
Chinese sentences (Section 3.1). To tackle the sec-
ond challenge, we build the first Chinese historical
psychology corpus (C-HI-PSY) and introduce an
approach based on indirect supervision (He et al.,
2021; Yin et al., 2023; Xu et al., 2023a) and con-
trastive learning (Chopra et al., 2005; Schroff et al.,
2015; Gao et al., 2021; Chuang et al., 2022) to fine-
tune pre-trained models on this corpus (Section
3.2).
2 Related Work
Psychological Text Analysis Given the increas-
ing amount of online textual data, many social sci-
entists are turning to NLP to test their theories. Un-
like in some computational fields, social scientists
traditionally give primacy to “theory” rather than
prediction (Yarkoni and Westfall, 2017). Hence,
theory-driven text analysis is the first methodolog-
ical choice in social sciences, including psychol-
ogy (Jackson et al., 2021; Wilkerson and Casas,
2017; Boyd and Schwartz, 2021). Given the impor-
tance of theory development and hypothesis testing,
many social scientists have developed dictionaries
to assess psychological constructs as diverse as
moral values (Graham et al., 2009), stereotypes
(Nicolas et al., 2021), polarization (Simchon et al.,
2022), and threat (Choi et al., 2022).
Distributed Dictionary Representation (DDR)
Aiming to integrate psychological theories withthe capabilities of word embeddings, Garten et al.
(2018) proposed the Distributed Dictionary Repre-
sentation (DDR) as a top-down psychological text-
analytic method. This method involves (a) defining
a concise list of words by experts to capture a spe-
cific concept, (b) using a word-embedding model
to represent these individual words, (c) computing
the centroid of these word representations to define
the dictionary’s representation, (d) determining the
centroid of the word embeddings within a given
document, and (e) assessing the cosine similarity
between the dictionary’s representation and that of
the document. DDR has been a useful approach in
measuring moral rhetoric (Wang and Inbar, 2021),
temporal trends in politics (Xu et al., 2023b), and
situational empathy (Zhou et al., 2021).
Contextualized Construct Representation
(CCR) The Contextualized Construct Repre-
sentation (CCR) pipeline is built upon SBERT
(Reimers and Gurevych, 2019). This theory-driven
and flexible approach has been shown to out-
perform dictionary-based methods and DDR for
various psychological constructs such as religiosity,
moral values, individualism, collectivism, and need
for cognition (Atari et al., 2023b). Furthermore,
recent work suggests that CCR performs on par
with Large Language Models (LLMs) such as
GPT4 (Achiam et al., 2023) in measuring psy-
chological constructs (Abdurahman et al., 2023).
Although CCR has not been developed specifically
for historical psychology, its flexible pipeline
and easy-to-implement steps offer a unique
opportunity to extract psychological constructs
from historical corpora. In a way, CCR is similar
to DDR, but instead of relying on non-contextual
word embeddings, it makes use of the power of
contextual language models to represent whole
sentences (or larger texts). In addition, it obviates
the development of researcher-curated word lists;
instead, making use of thousands of existing
questionnaires (which typically include face-valid
declarative sentences with which participants
agree or disagree) that have been developed and
validated in psychology over the last century.
Semantic Textual Similarity While BERT (De-
vlin et al., 2018) can identify sentences with similar
semantic meanings, this process can be resource-
intensive. To enhance the performance of BERT
for tasks like semantic similarity assessments, clus-
tering, and semantic-based information retrieval,Reimers and Gurevych (2019) developed Sentence-
BERT (or SBERT). This model employs a Siamese
network structure specifically designed to create
embeddings at the sentence level. SBERT outper-
forms conventional transformer-based models in
tasks related to sentences and significantly reduces
the time needed for computations. It is engineered
to generate sentence embeddings that capture the
core semantic content, ensuring that sentences with
comparable meanings are represented by closely
positioned embeddings in the vector space. There-
fore, SBERT provides an efficient and less compu-
tationally demanding method for evaluating seman-
tic similarities between sentences, making it partic-
ularly useful in fields such as psychology (Juhng
et al., 2023; Sen et al., 2022).
3 Methodology
Employing the CCR pipeline for historical-
psychological text analysis necessitates the use of
valid questionnaires and appropriate contextual lan-
guage models that can effectively represent sen-
tences or paragraphs. We propose two distinct
pipelines: (1) a cross-lingual questionnaire con-
version pipeline to obtain psychological question-
naires in classical Chinese; (2) an indirect super-
vised contrastive learning pipeline to fine-tune pre-
trained Transformer-based models using a histori-
cal psychological corpus.
3.1 Cross-lingual Questionnaire Conversion
In order to calculate semantic similarities between
questionnaires, typically in English, and the Chi-
nese historical texts to be measured, typically in
classical Chinese, we introduce a novel workflow
for Cross-lingual Questionnaire Conversion (CQC).
Instead of relying on translations or generated texts,
we employ quotations from authentic historical
texts, as they can integrate more naturally within
the context of classical Chinese.
The process of converting a contemporary En-
glish questionnaire Qinto a classical Chinese ques-
tionnaire ˜Qis illustrated in the right panel of Figure
2. For each questionnaire item ( qi∈ Q), the mul-
tilingual quote recommendation model, “QuoteR”
(Qi et al., 2022), which is trained on a dataset that
includes English, modern Standard Chinese, and
classical Chinese, can identify a set of quotations
{˜q}iin classical Chinese that are semantically sim-
ilar to the English sentence qi.
All the items are entered into the model for eachquestionnaire, resulting in a pool of corresponding
quotations. Then, manual filtering is followed to
eliminate quotations of low quality, which can be
either inappropriate or not explicitly relevant to the
psychological construct. Ultimately, the most simi-
lar quotations ˜qiare selected, substituting for every
English qito construct ˜Qin classical Chinese.
3.2 Indirect Supervised Contrastive Learning
To obtain better psychology-specific representa-
tions for CCR in Chinese historical texts, we in-
troduce an indirect supervised contrastive learning
approach to finetune pre-trained Transformer-based
models, as shown in Figure 3.
Figure 3: Pipeline of triplet sampling and contrastive
learning. CLM stands for contextual language model.
Historical Psychology Corpus We assemble a
refined corpus named Chinese Historical Psychol-
ogy Corpus (C-HI-PSY), which is comprised of
21,539 paragraphs ( S) extracted from 667 distinct
historical articles and book chapters in classical
Chinese. The titles of these works ( T,|T | ≪ |S| ),
each carefully selected for their relevance to moral
values, serve as labels for their topics, including
“節義” (moral integrity), “ 孝弟” (filial piety and
fraternal duty), “ 盡忠” (utmost loyalty), “ 廉恥”
(sense of shame), “ 清介” (pure and incorruptible),
and “愛己” (love oneself), among others.
We divide our data into training, validation, and
testing sets, allocating 60%, 20%, and 20% of the
data to each set, respectively. The distribution ofparagraph lengths across different sets is consistent,
as shown in Figure 7 in Appendix A.1.
Pseudo Ground Truth from Titles Since the
title ( ti∈ T ) of a paragraph ( si∈ S) is a con-
cise summary of the moral values reflected in the
paragraph, the semantic similarity between titles,
sim(ti, tj), can be considered as the pseudo ground
truth for the semantic similarity between corre-
sponding paragraphs, sim(si, sj). The semantic
similarity between titles can be obtained by embed-
ding the titles via ET(·)and calculating their cosine
similarity cos(ET(ti), ET(tj)). To perform word
embedding on the titles, we trained five word vector
models on a large classical Chinese corpus contain-
ing over a billion tokens using different frameworks
and architectures, and picked the best-performing
one (see Appendix B for word vector model de-
tails).
Positive and Negative Sampling We calculate
the cosine similarities between the title embeddings
cos(ET(ti), ET(tj)), obtained through the word
vector model, of all title pairs (the Cartesian prod-
uctT × T ) in the corpus. The distribution of title
similarities is illustrated in Figure 8 in Appendix
A.2. We obtain positive and negative paragraph
pairs by thresholding the similarities of title pairs.
Paragraphs whose titles have similarities exceeding
the upper threshold δ+, as well as those with identi-
cal titles, were identified as positive pairs (S×S )+,
that is,
{(si, sj)+|sim(ET(ti), ET(tj))> δ+}
Conversely, those with titles having similarities
below the lower threshold δ−were designated as
negative pairs (S × S )−, that is,
{(si, sj)−|sim(ET(ti), ET(tj))< δ−}
We experiment with several threshold settings,
including 0.5th/99.5th, 1st/99th, 10th/90th, and
25th/75th percentiles, on the C-HI-PSY valida-
tion set using the base model “bert-ancient-chinese”
(Wang and Ren, 2022). Our findings demonstrate
that the 10th/90th percentile threshold yields the
best performance, see Figure 4. Hence, for the fol-
lowing experiments, if not specified, the threshold
setting has been taken as 10th/90th.
Triplet Sampling We implement two strategies,
random sampling and hard sampling, to con-
struct triplets of anchor-positive-negative para-
Figure 4: Performance variation with sampling methods
and thresholds.
graphs (sA, s+
A, s−
A)from the training set. In ran-
dom sampling, we select one positive instance s+
A
and one negative instance s−
Arandomly from the re-
spective positive pairs (sA×S)+and negative pairs
(sA× S)−of the anchor sA. In hard sampling, we
utilize the pre-trained model fθ(·), which is later
fine-tuned on these triplets, to embed paragraphs
and calculate cosine similarities between the pos-
itive and negative pairs as cos(fθ(sA), fθ(s+/−
A)).
For the positive instance, we choose the paragraph
with the lowest similarity to the anchor from its
positive pairs, that is,
s+
A= argmin
s{cos(fθ(sA), fθ(s))|
(sA, s)∈(sA×S)+}
Conversely, for the negative instance, we select the
paragraph with the highest similarity to the anchor
from its negative pairs, that is,
s−
A= argmax
s{cos(fθ(sA), fθ(s))|
(sA, s)∈(sA×S)−}
To prevent the model from over-fitting, we ensure
that each paragraph is used as an anchor only once,applying this rule across both random and hard
sampling strategies. We also compare the two sam-
pling procedures in Figure 4 with respect to each
positive-negative splitting threshold. Interestingly,
we find that the random sampling procedure is bet-
ter than hard sampling ever since the threshold is
higher/lower than 0.5th/99.5th; we note that the
case could be due to the noise inevitably caused by
the indirect supervised learning approach, which
drove the hard sampling procedure to fail at finding
helpful instances (see Limitation).
Fine-tuning with Contrastive Learning We
fine-tune several pre-trained Transformer-based
models (Wang and Ren, 2022; Yan and Chi, 2020;
Reimers and Gurevych, 2019; Xu, 2023) on the
C-HI-PSY training set, using a triplet loss function
(Schroff et al., 2015),
Ltriplet (θ) =X
sA∈Smax{D+− D−,0}
whereD+denotes the distance between the pos-
itive pair, i.e. ∥fθ(sA)−fθ(s+
A)∥2
2, andD−de-
notes the distance between the negative pair, i.e.
∥fθ(sA)−fθ(s−
A)∥2
2,αis a constant set to be 5,
andθstands for the pre-trained weights to be fine-
tuned. This loss function aims to minimize the
squared Euclidean norm between the anchor and
positive, and maximize the squared Euclidean norm
between the anchor and negative.
We construct paragraph pairs from the C-HI-
PSY validation set through random sampling to
validate the models during training, using the sim-
ilarities between titles as pseudo ground truth to
gauge the similarities between paragraphs. We per-
form a hyperparameter sweep (see Table 5 in the
Appendix), to select the best-performing configura-
tion for each model, as shown in Table 1.
4 Evaluation and Results
We set up three different tasks to evaluate the CCR
method (using SBERT models), and compare it
with the DDR method (using word embedding mod-
els) and the prompting method (using generative
LLMs). The results are shown in Table 2.
4.1 Semantic Understanding
Understanding of Historical Text: Semantic Tex-
tual Similarity For the CCR method, we embed
whole paragraphs with SBERT models, and then
Figure 5: Comparison of model performance using the
CCR method on the three tasks in the C-HI-PSY test set
before and after fine-tuning. (Model A: bert-ancient-
chinese, B: guwenbert-base, C: guwenbert-large, D:
paraphrase-multilingual-MiniLM-L12-v2, E: text2vec-
base-chinese, F: text2vec-base-chinese-paraphrase, G:
text2vec-large-chinese)
calculate the cosine similarity between each pair
of paragraphs. For the DDR method, we average
the word vectors of all the words in the paragraph,
and then calculate the cosine similarity between
each pair of paragraphs. For the LLM-prompting
method, we craft a few-shot prompt (Brown et al.,
2020; Si et al., 2023) (Figure 9) asking for a simi-
larity score, ranging from 0 to 1, between each pair
of paragraphs. As mentioned, similarities between
the titles of each pair of paragraphs are used as the
pseudo ground truth.
We construct paragraph pairs for evaluation from
the C-HI-PSY test set using two sampling methods:
(1) random sampling, where paragraphs are ran-
domly paired, and (2) threshold sampling, which
pairs paragraphs with either positive or negative
samples based on a specific threshold (10th/90th).
Threshold sampling produces distinctly positive
or negative pairs; thus, we refer to it as the Easy
Task. Conversely, random sampling can result in
ambiguous pairs, making for a more challenging
Hard Task.
Understanding of Questionnaire Item: Text
Classification We convert several broadly ac-
cepted questionnaires from English into classi-
cal Chinese, including Collectivism, Individual-
ism (Oyserman, 1993), Norm Tightness and Norm
Looseness (Gelfand et al., 2011), by employing theFramework Base ModelIf Specific to
Classical ChineseBatch
SizeWarmup
EpochsLearning
RatePearson Spearman
BERT Bert-ancient-chinese ✔ 32 3 1.0e-05 .43 .42
RoBERTaGuwenbert-base ✔ 32 2 2.0e-05 .30 .37
Guwenbert-large ✔ 16 1 2.0e-05 .29 .30
SBERTParaphrase-multilingual-
MiniLM-L12-v2✗ 32 1 2.0e-05 .19 .19
MacBERT+CoSENT text2vec-base-chinese ✗ 32 2 2.0e-05 .34 .32
ERNIE+CoSENTtext2vec-base-chinese-
paraphrase✗ 32 2 2.0e-05 .40 .40
LERT+CoSENT text2vec-large-chinese ✗ 16 2 2.0e-05 .36 .37
Table 1: Fine-tuned models’ performance on the validation set. We show the best performing configuration which is
also the final configuration used to report each models’ performance on the test test.
CQC approach described in Section 3.1. For both
the CCR and DDR methods, all the items from
these questionnaires are embedded. Then we con-
duct 10-fold cross-validation, using Support Vector
Machines (SVM) as the classifier, and text embed-
dings or averaged word vectors as features. For
the prompting method, we craft a few-shot prompt
(Figure 11) directly asking for classification.
4.2 Psychological Measure
For the CCR method, we calculate the average
cosine similarities between each paragraph in the
C-HI-PSY test set and all the items in the question-
naire, representing the “loading score” of the para-
graph on the questionnaire. For the DDR method,
we build a corresponding dictionary for each psy-
chological construct (see Appendix C for more de-
tails), and calculate the cosine similarity between
the centroid of words in each paragraph and the
centroid of words in the dictionary. For the prompt-
ing method, we craft a few-shot prompt (Figure 10)
asking for a score, ranging from 0 to 1, to measure
each paragraph with respect to the topic of each
questionnaire. Items in each questionnaire are pro-
vided in the prompt. Average similarities between
the title of each paragraph and all the words in the
dictionary, calculated by the word vector model,
are used as the pseudo ground truth.
4.3 Results
For the Semantic Textual Similarity (STS) task,
we evaluate the DDR and CCR methods through
a rigorous process involving 20 rounds of random
sampling. In each round, 4,308 random paragraph
pairs are constructed from the C-HI-PSY test set.After completing these 20 evaluations, we calcu-
late the average scores along with standard errors.
When evaluating the prompting method, due to
the high costs, we only conduct a single round
of random sampling. For the Questionnaire Item
Classification (QIC) task, we utilize 60 items from
questionnaires on Collectivism, Individualism (Oy-
serman, 1993), Norm Tightness, and Norm Loose-
ness (Gelfand et al., 2011), selecting 15 items from
each questionnaire. For the Psychological Measure
(PM) task, we measure the loading scores of all
4308 paragraphs in the C-HI-PSY test set across
the four questionnaires mentioned above, and re-
port the average scores along with standard errors.
Figure 5 illustrates that the performance metrics
of most models in the CCR baseline have substan-
tially improved after fine-tuning. As shown in Ta-
ble 2, the CCR method, using SBERT models after
fine-tuning, outperforms the DDR method across
all tasks and surpasses the prompting method with
GPT-4 (version January 25, 2024) in most tasks,
demonstrating its superiority in effectively extract-
ing psychological variables from text.
5 Benchmarking: Traditionalism,
Authority, and Attitude toward Reform
To address the lack of benchmark datasets related
to psychological measurement in classical Chinese,
we further validate the effectiveness of the CCR
method using externally annotated data.
Officials’ Attitudes toward Reform in the 11th
Century Moral values and political orientations
are closely intertwined (Federico et al., 2013;
Kivikangas et al., 2021). For example, the attitudeFramework Base ModelSemantic
Textual Similarity
(Easy Task )Semantic
Textual Similarity
(Hard Task )Questionnaire
Item ClassificationPsychological
Measure
Pears. Spear. Pears. Spear. Accuracy Pears. Spear.
(a) DDR
Word2Vec (CBOW) / .02±.11.02±.10−.03±.02−.02±.01 .80±.16 .22±.07.23±.05
Word2Vec (Skip-gram) / .08±.11.09±.11.02±.02.02±.01 .87±.15 .18±.07.18±.06
FastText (CBOW) / .05±.11.04±.10−.01±.01.01±.01 .90±.13 .23±.08.24±.06
FastText (Skip-gram) / .10±.10.11±.10.03±.02.04±.01 .85±.16 .20±.07.20±.05
GloVe / .07±.10.09±.11.01±.02.01±.01 .83±.15 .16±.09.19±.05
(b) Prompting
GPT GPT-3.5- turbo-0125 .08 .04 .26 .28 .63 .05±.08.08±.10
GPT GPT-4- 0125-preview .62 .52 .40 .30 .77 .25±.15.27±.17
(c) CCR (ours)
BERT Bert-ancient-chinese .53±.07.55±.07 .42±.01 .43±.01 .93±.11 .30±.04.30±.04
RoBERTa Guwenbert-base .29±.07.46±.09.25±.01.40±.01 .90±.11 .20±.06.23±.09
RoBERTa Guwenbert-large .41±.05.44±.07.28±.01.31±.01 .83±.13 .22±.04.20±.05
SBERTParaphrase-multilingual-
MiniLM-L12-v2.20±.15.21±.14.18±.01.19±.01 .82±.19 .15±.04.14±.05
MacBERT+CoSENT text2vec-base-chinese .41±.09.40±.09.32±.01.31±.01 .95±.08 .21±.10.20±.10
ERNIE+CoSENTtext2vec-base-chinese-
paraphrase.45±.09.45±.09.38±.01.37±.01 .93±.11 .21±.03.20±.04
LERT+CoSENT text2vec-large-chinese .46±.12.47±.08.36±.01.38±.01 .97±.07 .28±.05.27±.05
Table 2: Performance on the test set across three tasks using three methods: DDR, LLM Promping, and CCR.
Details of models for the DDR method are explained in the Appendix B. Models for the CCR method have been
fine-tuned on the C-HIS-PSY training set. Models for the prompting method include the versions of GPT-3.5 and
GPT-4 that were released on January 25, 2024.
of individuals toward reforms, policy changes, and
new legislation often reflects traditionalism, conser-
vatism, and respect for authority (Hackenburg et al.,
2023; Koleva et al., 2012). Those with stronger tra-
ditionalist views are more likely to identify with
the existing social order and resist changes to the
status quo (Osborne et al., 2023; Jost and Hunyady,
2005).
Throughout Chinese history, there have been nu-
merous instances of significant reforms, one of the
most notable of which being the Wang Anshi’s New
Policies in the 11th century, which faced mixed re-
actions from officials. We draw upon a dataset
manually compiled by Wang (2022), who anno-
tated the attitudes of 137 major officials toward the
reform.
Individual-level Measure of Traditionalism and
Authority We extract writings of these officials
documented in the Complete Prose of the Song
Dynasty (Zeng and Liu, 2006). Questionnaires of
traditionalism (Samore et al., 2023) and authority
(Atari et al., 2023a) are converted from Englishinto classical Chinese, by employing the CQC ap-
proach described in Section 3.1. Employing the
best-performing fine-tuned SBET model, we use
our CCR pipeline to measure the levels of tradition-
alism and attitudes toward authority expressed in
their texts. For each individual official, results are
aggregated by calculating the average score across
all of their writings.
Support
for ReformAttitude toward
Reform
Traditionalism -0.441*** -0.279**
Authority -0.472*** -0.310**
Table 3: Spearman correlation between CCR-based mea-
sure of moral values and actual attitude toward reform
of officials. ** p< .01 *** p< .001
Results We find a significant correlation (Figure
6) between officials’ attitudes toward the reforms
and the levels of traditionalism and authority mea-
sured through CCR. Authority and traditionalismFigure 6: Correlation between Traditionalism Authority
and Officials’ Attitudes toward Reforms. (a) and (c)
present the average psychological measure scores with
standard errors, using an ordinal variable where -1 signi-
fies opposition to the reform, 0 indicates a neutral or no
explicit attitude, and 1 denotes support for the reform
(N= 108). (b) and (d) depict the linear regression lines
accompanied by 95% Confidence Intervals, employing
a continuous variable that ranges from 0 to 1 to quantify
officials’ degree of support for the reform ( N= 56).
both show a significant negative correlation with
support for reform, with Spearman correlation co-
efficients less than 0.4 and p-values less than 0.001
(Table 3). Officials with greater traditionalism and
respect for existing authority are more likely to op-
pose reform, which is in line with the theoretical
assumptions. This benchmarking against histori-
cally verified data supports the validity of CCR as a
valid computational pipeline to extract meaningful
psychological information from classical Chinese
corpora.
6 Discussion and Conclusion
Historical-psychological text analysis is a new line
of research focused on extracting different aspects
of psychology from historical corpora using state-
of-the-art computational methods (Atari and Hen-
rich, 2023). Here, we create a new pipeline, CCR,
as a helpful tool for historical-psychological text
analysis. Evaluating our model against word em-
bedding models (e.g., DDR) and more recent LLMs
(e.g., GPT4), we demonstrate that CCR performs
better than these alternatives while keeping its high
level of interpretability and flexibility. Classical
Chinese is of great historical significance, and the
proposed approach can be particularly helpful intesting new insights about the “dead minds” who
lived centuries or even millennia prior. We hope
our tool motivates future work at the intersections
of psychology, quantitative history, and NLP. Im-
portantly, benchmarking historical-psychological
tools, especially in ancient languages, is difficult
because obtaining ground truth is challenging and
dependent upon the quality of historical data. That
said, we validate CCR against a historically veri-
fied knowledge base about attitudes toward reform
and traditionalism.
Limitation
Due to the lack of fine-grained data available for
training in the context of classical Chinese and with
historical-psychological texts, we propose an indi-
rect supervised learning approach where the simi-
larities between titles are used as the pseudo ground
truth for similarities between paragraphs. How-
ever, this approach may lead to the model learning
some noise from the data, negatively affecting the
model’s performance in downstream tasks.
Our experiments show that hard sampling is
counterintuitively worse than random sampling on
our dataset (Figure 4). This is the case because
although the title of a text represents the main idea
of most of the content, there may still be parts of
the text that are unrelated to the title. For example,
in a pair of paragraphs that are identified as positive
samples due to their highly similar titles, one para-
graph might be irrelevant to the title. Consequently,
the text similarity calculated after embedding by a
pre-trained model might not be high for this pair
of paragraphs. The difference between the similar-
ity prediction made by the pre-trained model and
the pseudo ground truth based on title similarity
may result in these paragraph pairs being identi-
fied as hard samples. However, in such cases, the
pre-trained model’s prediction could be more ac-
curate than the pseudo ground truth derived from
title similarity. It is the noise caused by the indirect
supervised approach that makes the hard sampling
fail to find helpful instances.
Our future efforts will be directed toward assem-
bling datasets with expert annotations to address
this issue. Moreover, we aim to contribute to both
historical psychology and NLP by compiling new
open-source datasets for benchmarking purposes.References
Suhaib Abdurahman, Mohammad Atari, Farzan Karimi-
Malekabadi, Mona J Xue, Jackson Trager, Peter S
Park, Preni Golazizian, Ali Omrani, and Morteza
Dehghani. 2023. Perils and opportunities in using
large language models in psychological research.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Mohammad Atari, Jonathan Haidt, Jesse Graham, Sena
Koleva, Sean T. Stevens, and Morteza Dehghani.
2023a. Morality beyond the weird: How the
nomological network of morality varies across cul-
tures. Journal of Personality and Social Psychology ,
125(5):1157–1188.
Mohammad Atari and Joseph Henrich. 2023. Historical
psychology. Current Directions in Psychological
Science , 32(2):176–183.
Mohammad Atari, Ali Omrani, and Morteza Dehghani.
2023b. Contextualized construct representation:
Leveraging psychometric scales to advance theory-
driven text analysis.
David Bamman and Patrick J. Burns. 2020. Latin bert:
A contextual language model for classical philology.
ArXiv , abs/2009.10053.
Nicolas Baumard, Lou Safra, Mauricio Martins, and
Coralie Chevallier. 2024. Cognitive fossils: using
cultural artifacts to reconstruct psychological changes
throughout history. Trends in Cognitive Sciences ,
28(2):172–186.
Damián E. Blasi, Joseph Henrich, Evangelia Adamou,
David Kemmerer, and Asifa Majid. 2022. Over-
reliance on english hinders cognitive science. Trends
in Cognitive Sciences , 26(12):1153–1170.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics , 5:135–146.
Ryan L Boyd, Ashwini Ashokkumar, Sarah Seraj, and
James W Pennebaker. 2022. The development and
psychometric properties of liwc-22. Austin, TX: Uni-
versity of Texas at Austin , pages 1–47.
Ryan L Boyd and H Andrew Schwartz. 2021. Natu-
ral language analysis and the psychology of verbal
behavior: The past, present, and future states of the
field. Journal of Language and Social Psychology ,
40(1):21–41.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Virginia K Choi, Snehesh Shrestha, Xinyue Pan, and
Michele J Gelfand. 2022. When danger strikes: A lin-
guistic tool for tracking america’s collective response
to threats. Proceedings of the National Academy of
Sciences , 119(4):e2113891119.
S. Chopra, R. Hadsell, and Y . LeCun. 2005. Learning
a similarity metric discriminatively, with application
to face verification. In 2005 IEEE Computer Society
Conference on Computer Vision and Pattern Recog-
nition (CVPR’05) , volume 1, pages 539–546 vol. 1.
Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo,
Yang Zhang, Shiyu Chang, Marin Soljacic, Shang-
Wen Li, Scott Yih, Yoon Kim, and James Glass. 2022.
DiffCSE: Difference-based contrastive learning for
sentence embeddings. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 4207–4218, Seattle,
United States. Association for Computational Lin-
guistics.
Dorottya Demszky, Diyi Yang, David S Yeager, Christo-
pher J Bryan, Margarett Clapper, Susannah Chand-
hok, Johannes C Eichstaedt, Cameron Hecht, Jeremy
Jamieson, Meghann Johnson, et al. 2023. Using large
language models in psychology. Nature Reviews Psy-
chology , 2(11):688–701.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. CoRR , abs/1810.04805.
Christopher M. Federico, Christopher R. Weber, Damla
Ergun, and Corrie Hunt. 2013. Mapping the connec-
tions between politics and morality: The multiple
sociopolitical orientations involved in moral intuition.
Political Psychology , 34(4):589–610.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Justin Garten, Joe Hoover, Kate M Johnson, Reihane
Boghrati, Carol Iskiwitch, and Morteza Dehghani.
2018. Dictionaries and distributions: Combining
expert knowledge and large scale textual data con-
tent analysis: Distributed dictionary representation.
Behavior research methods , 50:344–361.Michele J. Gelfand, Jana L. Raver, Lisa Nishii,
Lisa M. Leslie, Janetta Lun, Beng Chong Lim,
Lili Duan, Assaf Almaliach, Soon Ang, Jakobina
Arnadottir, Zeynep Aycan, Klaus Boehnke, Pawel
Boski, Rosa Cabecinhas, Darius Chan, Jagdeep
Chhokar, Alessia D’Amato, Montserrat Subirats Fer-
rer, Iris C. Fischlmayr, Ronald Fischer, Marta Fülöp,
James Georgas, Emiko S. Kashima, Yoshishima
Kashima, Kibum Kim, Alain Lempereur, Patricia
Marquez, Rozhan Othman, Bert Overlaet, Penny
Panagiotopoulou, Karl Peltzer, Lorena R. Perez-
Florizno, Larisa Ponomarenko, Anu Realo, Vidar
Schei, Manfred Schmitt, Peter B. Smith, Nazar
Soomro, Erna Szabo, Nalinee Taveesin, Midori
Toyama, Evert Van de Vliert, Naharika V ohra,
Colleen Ward, and Susumu Yamaguchi. 2011. Differ-
ences between tight and loose cultures: A 33-nation
study. Science , 332(6033):1100–1104.
Jesse Graham, Jonathan Haidt, and Brian A Nosek.
2009. Liberals and conservatives rely on different
sets of moral foundations. Journal of personality and
social psychology , 96(5):1029.
Kobi Hackenburg, William J Brady, and Manos Tsakiris.
2023. Mapping moral language on us presidential
primary campaigns reveals rhetorical networks of po-
litical division and unity. PNAS nexus , page pgad189.
Hangfeng He, Mingyuan Zhang, Qiang Ning, and Dan
Roth. 2021. Foreseeing the benefits of incidental
supervision. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1782–1800, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Joshua Conrad Jackson, Joseph Watts, Johann-Mattis
List, Curtis Puryear, Ryan Drabble, and Kristen A.
Lindquist. 2021. From text to thought: How analyz-
ing language can advance psychological science. Per-
spectives on Psychological Science , 17(3):805–826.
Kyle P. Johnson, Patrick J. Burns, John Stewart, Todd
Cook, Clément Besnier, and William J. B. Mattingly.
2021. The Classical Language Toolkit: An NLP
framework for pre-modern languages. In Proceed-
ings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing:
System Demonstrations , pages 20–29, Online. Asso-
ciation for Computational Linguistics.
John T Jost and Orsolya Hunyady. 2005. An-
tecedents and consequences of system-justifying ide-
ologies. Current directions in psychological science ,
14(5):260–265.
Swanie Juhng, Matthew Matero, Vasudha Varadarajan,
Johannes Eichstaedt, Adithya V Ganesan, and H An-
drew Schwartz. 2023. Discourse-level representa-
tions can improve prediction of degree of anxiety. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers) , pages 1500–1511.Brendan Kennedy, Mohammad Atari,
Aida Mostafazadeh Davani, Joe Hoover, Ali
Omrani, Jesse Graham, and Morteza Dehghani.
2021. Moral concerns are differentially observable
in language. Cognition , 212:104696.
J Matias Kivikangas, Belén Fernández-Castilla, Simo
Järvelä, Niklas Ravaja, and Jan-Erik Lönnqvist. 2021.
Moral foundations and political orientation: System-
atic review and meta-analysis. Psychological Bul-
letin, 147(1):55.
Spassena P Koleva, Jesse Graham, Ravi Iyer, Peter H
Ditto, and Jonathan Haidt. 2012. Tracing the threads:
How five moral concerns (especially purity) help ex-
plain culture war attitudes. Journal of research in
personality , 46(2):184–194.
Zhou Liu, Hongsu Wang, and Peter K Bol. 2023. Auto-
matic biographical information extraction from local
gazetteers with bi-lstm-crf model and bert. Inter-
national Journal of Digital Humanities , 4(1-3):195–
212.
Enrique Manjavacas Arevalo and Lauren Fonteyn. 2021.
MacBERTh: Development and evaluation of a histor-
ically pre-trained language model for English (1450-
1950). In Proceedings of the Workshop on Natural
Language Processing for Digital Humanities , pages
23–36, NIT Silchar, India. NLP Association of India
(NLPAI).
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. In 1st International Conference
on Learning Representations, ICLR 2013, Scottsdale,
Arizona, USA, May 2-4, 2013, Workshop Track Pro-
ceedings .
Michael Muthukrishna, Joseph Henrich, and Edward
Slingerland. 2021. Psychology as a historical science.
Annual Review of Psychology , 72(1):717–749.
Gandalf Nicolas, Xuechunzi Bai, and Susan T Fiske.
2021. Comprehensive stereotype content dictionaries
using a semi-automated method. European Journal
of Social Psychology , 51(1):178–196.
Danny Osborne, Thomas H. Costello, John Duckitt, and
Chris G. Sibley. 2023. The psychological causes and
societal consequences of authoritarianism. Nature
Reviews Psychology , 2(4):220–232.
Daphna Oyserman. 1993. The lens of personhood:
Viewing the self and others in a multicultural so-
ciety. Journal of Personality and Social Psychology ,
65(5):993–1009.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP) , pages 1532–1543, Doha, Qatar.
Association for Computational Linguistics.Fanchao Qi, Yanhui Yang, Jing Yi, Zhili Cheng,
Zhiyuan Liu, and Maosong Sun. 2022. QuoteR: A
benchmark of quote recommendation for writing. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 336–348, Dublin, Ireland. Asso-
ciation for Computational Linguistics.
Fanchao Qi, Lei Zhang, Yanhui Yang, Zhiyuan Liu, and
Maosong Sun. 2020. Wantwords: An open-source
online reverse dictionary system. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations ,
pages 175–181.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Theodore Samore, Daniel M. T. Fessler, Adam Maxwell
Sparks, Colin Holbrook, Lene Aarøe, Carmen Glo-
ria Baeza, María Teresa Barbato, Pat Barclay, Re-
natas Berni ¯unas, Jorge Contreras-Garduño, Bernardo
Costa-Neves, Maria del Pilar Grazioso, Pınar El-
mas, Peter Fedor, Ana Maria Fernandez, Regina
Fernández-Morales, Leonel Garcia-Marques, Paulina
Giraldo-Perez, Pelin Gul, Fanny Habacht, Youssef
Hasan, Earl John Hernandez, Tomasz Jarmakowski,
Shanmukh Kamble, Tatsuya Kameda, Bia Kim,
Tom R. Kupfer, Maho Kurita, Norman P. Li, Jun-
song Lu, Francesca R. Luberti, María Andrée Maegli,
Marinés Mejia, Coby Morvinski, Aoi Naito, Al-
ice Ng’ang’a, Angélica Nascimento de Oliveira,
Daniel N. Posner, Pavol Prokop, Yaniv Shani, Wal-
ter Omar Paniagua Solorzano, Stefan Stieger, An-
gela Oktavia Suryani, Lynn K. L. Tan, Joshua M.
Tybur, Hugo Viciana, Amandine Visine, Jin Wang,
and Xiao-Tian Wang. 2023. Greater traditionalism
predicts covid-19 precautionary behaviors across 27
societies. Scientific Reports , 13(1).
Florian Schroff, Dmitry Kalenichenko, and James
Philbin. 2015. Facenet: A unified embedding for
face recognition and clustering. In Proceedings of
the IEEE conference on computer vision and pattern
recognition , pages 815–823.
Indira Sen, Daniele Quercia, Marios Constantinides,
Matteo Montecchi, Licia Capra, Sanja Scepanovic,
and Renzo Bianchi. 2022. Depression at work: ex-
ploring depression in major us companies from on-
line reviews. Proceedings of the ACM on Human-
Computer Interaction , 6(CSCW2):1–21.
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang
Wang, Jianfeng Wang, Jordan Boyd-Graber, and Li-
juan Wang. 2023. Prompting gpt-3 to be reliable. In
International Conference on Learning Representa-
tions (ICLR) .Almog Simchon, William J Brady, and Jay J Van Bavel.
2022. Troll and divide: the language of online polar-
ization. PNAS nexus , 1(1):pgac019.
Edward Slingerland. 2013. Body and mind in early
china: An integrated humanities–science approach.
Journal of the American Academy of Religion ,
81(1):6–55.
Edward Slingerland, Ryan Nichols, Kristoffer Neilbo,
and Carson Logan. 2017. The distant reading of
religious texts: A “big data” approach to mind-body
concepts in early china. Journal of the American
Academy of Religion , 85(4):985–1016.
Daniel Swanson and Francis Tyers. 2022. Handling
stress in finite-state morphological analyzers for An-
cient Greek and Ancient Hebrew. In Proceedings of
the Second Workshop on Language Technologies for
Historical and Ancient Languages , pages 108–113,
Marseille, France. European Language Resources
Association.
Huishuang Tian, Kexin Yang, Dayiheng Liu, and
Jiancheng Lv. 2021. Anchibert: A pre-trained model
for ancient chinese language understanding and gen-
eration. In 2021 International Joint Conference on
Neural Networks (IJCNN) , pages 1–8. IEEE.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Dongbo Wang, Chang Liu, Zhixiao Zhao, Si Shen, Liu
Liu, Bin Li, Haotian Hu, Mengcheng Wu, Litao Lin,
Xue Zhao, and Xiyu Wang. 2023a. Gujibert and
gujigpt: Construction of intelligent information pro-
cessing foundation language models for ancient texts.
Jiahui Wang, Xuqin Zhang, Jiahuan Li, and Shu-
jian Huang. 2023b. Pre-trained model in Ancient-
Chinese-to-Modern-Chinese machine translation. In
Proceedings of ALT2023: Ancient Language Trans-
lation Workshop , pages 23–28, Macau SAR, China.
Asia-Pacific Association for Machine Translation.
Pengyu Wang and Zhichen Ren. 2022. The uncertainty-
based retrieval framework for ancient chinese cws
and pos. In Proceedings of the Second Workshop on
Language Technologies for Historical and Ancient
Languages , pages 164–168.
Sze-Yuh Nina Wang and Yoel Inbar. 2021. Moral-
language use by us political elites. Psychological
Science , 32(1):14–26.
Yuhua Wang. 2022. Blood is thicker than water: Elite
kinship networks and state building in imperial china.
American Political Science Review , 116(3):896–910.
John Wilkerson and Andreu Casas. 2017. Large-scale
computerized text analysis in political science: Op-
portunities and challenges. Annual Review of Politi-
cal Science , 20:529–544.Jiashu Xu, Mingyu Derek Ma, and Muhao Chen. 2023a.
Can NLI provide proper indirect supervision for low-
resource biomedical relation extraction? In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 2450–2467, Toronto, Canada. Association for
Computational Linguistics.
Mengyao Xu, Lingshu Hu, and Glen T Cameron. 2023b.
Tracking moral divergence with ddr in presidential
debates over 60 years. Journal of Computational
Social Science , 6(1):339–357.
Ming Xu. 2023. Text2vec: Text to vector toolkit.
https://github.com/shibing624/
text2vec .
Tan Yan and Zewen Chi. 2020. Guwenbert.
urlhttps://github.com/ethan-yt/guwenbert.
Tal Yarkoni and Jacob Westfall. 2017. Choosing predic-
tion over explanation in psychology: Lessons from
machine learning. Perspectives on Psychological
Science , 12(6):1100–1122.
Wenpeng Yin, Muhao Chen, Ben Zhou, Qiang Ning,
Kai-Wei Chang, and Dan Roth. 2023. Indirectly
supervised natural language processing. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 6: Tutorial Ab-
stracts) , pages 32–40, Toronto, Canada. Association
for Computational Linguistics.
Tariq Yousef, Chiara Palladino, David J. Wright, and
Monica Berti. 2022. Automatic translation align-
ment for Ancient Greek and Latin. In Proceedings of
the Second Workshop on Language Technologies for
Historical and Ancient Languages , pages 101–107,
Marseille, France. European Language Resources
Association.
Zaozhuang Zeng and Lin Liu, editors. 2006. Complete
Prose of the Song Dynasty , volume 360. Shanghai
cishu chubanshe and Anhui jiaoyu chubanshe, Shang-
hai and Hefei. In Chinese.
Lei Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang,
Qun Liu, and Maosong Sun. 2020. Multi-channel
reverse dictionary model. In Proceedings of the AAAI
Conference on Artificial Intelligence , pages 312–319.
Ying Zhong, Valentin Thouzeau, and Nicolas Baumard.
2023. The evolution of romantic love in chinese
fiction in the very long run (618 - 2022): A quan-
titative approach. In Workshop on Computational
Humanities Research .
Bo Zhou, Qianglong Chen, Tianyu Wang, Xiaomi
Zhong, and Yin Zhang. 2023. WYWEB: A NLP
evaluation benchmark for classical Chinese. In Find-
ings of the Association for Computational Linguis-
tics: ACL 2023 , pages 3294–3319, Toronto, Canada.
Association for Computational Linguistics.Ke Zhou, Luca Maria Aiello, Sanja Scepanovic, Daniele
Quercia, and Sara Konrath. 2021. The language of
situational empathy. Proceedings of the ACM on
Human-Computer Interaction , 5(CSCW1):1–19.A Historical Psychology Corpus Details
A.1 Distribution of Paragraph Lengths
To ensure the inclusion of sufficient semantic in-
formation, paragraphs containing fewer than 50
characters have been merged with the preceding
paragraph of the article or chapter, wherever pos-
sible. To accommodate the token limitations of
models such as BERT, paragraphs that exceed 500
characters have been divided into segments with
fewer than 500 characters each, while maintaining
the integrity of the original sentence structure as
much as possible. The average length of paragraphs
is 195 characters.
Figure 7: Distributions of paragraph lengths in different
sets.
A.2 Distribution of Title Similarities
Figure 8: Distribution of title similarities with thresh-
olds.B Word Embedding Model Details
B.1 Pre-processing
Before training the word vector model, we
conducted word segmentation on the cor-
pus, employing the pre-trained tokenizer
“COARSE_ELECTRA_SMALL_ZH ” from HanLP
(https://hanlp.hankcs.com/docs/
api/hanlp/pretrained/tok.html ).
After word segmentation, the corpus consists of
1.04 billion word tokens and an initial vocabulary
containing 15.55 million unique words. By trun-
cating the vocabulary at a minimum word count
threshold of 10, the final vocabulary size is reduced
to 1.27 million words.
B.2 Training Hyperparameters
We train our word vector models on the same cor-
pus using various frameworks and architectures,
such as Word2Vec (with CBOW and Skip-gram)
(Mikolov et al., 2013), FastText (with CBOW and
Skip-gram) (Bojanowski et al., 2017), and GloVe
(Pennington et al., 2014). The hyperparameters are
presented in Table 4.
C Dictionary Details
We build a dictionary for each classical Chinese
questionnaire by using an open-source dictionary
system named “WantWords” (Qi et al., 2020),
which is based on a multi-channel reverse dictio-
nary model (MRDM) (Zhang et al., 2020) and
takes sentences (descriptions of words) as input
and yields words semantically matching the input
sentences.
The process involves three steps: (1) we em-
ploy the “WantWords” model to obtain the top n
most similar words to each quotation in the ques-
tionnaire; (2) a process of deduplication is then
conducted; (3) the words are labeled manually by
a native Chinese speaker with “relevant” or “irrel-
evant” to the corresponding topic, after which all
irrelevant words are discarded.Framework Architecture Vector Size Epoch Window Size Other Parameters
Word2VecCBOW 300 5 5 negative=5
Skip-gram 300 5 5 negative=5
FastTextCBOW 300 5 5 negative=5, min_n=1, max_n=4
Skip-gram 300 5 5 negative=5, min_n=1, max_n=4
GloVe 300 15 5 x_max=100, alpha=0.75
Table 4: Word vector model training hyperparameters and evaluation results.
SamplingPositive/Negative Sampling
Thresholds{(10th, 90th)}
Triplet Sampling Option {random}
Sampling Seed {42}
TrainingBatch Size {16, 32}
Epochs {3}
Warmup Epochs {1, 2, 3}
Learning Rate {1e-6, 1e-5, 2e-5}
Optimizer {Adam}
Table 5: Hyperparameter sweep for triplet sampling and validation for fine-tuned models.Figure 9: Few-shot prompt for the semantic textual
similarity task.
Figure 10: Few-shot prompt for the psychological mea-
sure task.Figure 11: Few-shot prompt for the questionnaire item classification task.