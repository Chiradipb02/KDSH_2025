Are Large Language Models In-Context Personalized Summarizers?
Get an iCOPERNICUS Test Done!
Divya Patel†∗Pathik Patel†∗Ankush Chander†∗Sourish Dasgupta†∗Tanmoy Chakraborty‡
†KDM Lab, Dhirubhai Ambani Institute of Information & Communication Technology, India
‡Indian Institute of Technology, Delhi, India
{202001420, 202003002, ankush_chander, Bsourish_dasgupta}@daiict.ac.in, Btanchak@iitd.ac.in
Abstract
Large Language Models (LLMs) have suc-
ceeded considerably in In-Context-Learning
(ICL) based summarization. However, saliency
is subject to the users’ specific preference his-
tories. Hence, we need reliable In-Context
Personalization Learning (ICPL) capabilities
within such LLMs. For any arbitrary LLM
to exhibit ICPL, it needs to have the ability
to discern contrast in user profiles . A re-
cent study proposed a measure for degree-of-
personalization called EGISES for the first
time. EGISES measures a model’s responsive-
ness to user profile differences. However, it
cannot test if a model utilizes all three types of
cues provided in ICPL prompts: (i) example
summaries, (ii) user’s reading histories, and (iii)
contrast in user profiles. To address this, we
propose the iCOPERNICUS framework, a novel
In-Context Personalization Lear ning Scrutiny
ofSummarization capability in LLMs that uses
EGISES as a comparative measure. As a case-
study, we evaluate 17 state-of-the-art LLMs
based on their reported ICL performances and
observe that 15 models’ ICPL degrades (min:
1.6%↓; max: 3.6% ↓) when probed with richer
prompts, thereby showing lack of trueICPL.
1 Introduction
With the constant influx of information, we need
efficient models capable of summarizing essential
content from lengthy documents for faster compre-
hension and prioritization (Ter Hoeve et al., 2022).
Yet, defining what constitutes “salient” information
remains subjective, particularly in documents cov-
ering multiple aspects. To tackle this, contempo-
rary summarizers should be personalized to users’
preference histories and interests.
Specialized PLMs as summarizers. Ao et al.
(2021) proposed the most direct method to train
models to learn personalization using user read-
ing histories. These models (called PENS models)
*Equal contributions.use variants of pointer-generator networks (See
et al., 2017) that are injected with representations
of user reading history for user preference align-
ment. Other indirect approaches include aspect-
based models (Frermann and Klementiev, 2019;
Hayashi et al., 2021) that produce summaries coher-
ent with the document themes but lack adaptability
to changes in the reader’s profile. In contrast, inter-
active human-feedback-based models allow for it-
erative refinement based on user feedback, thereby
better personalization (Ghodratnama et al., 2021).
LLMs as personalized summarizers. Recent
studies on the state-of-the-art (SOTA) LLMs show
unprecedented In-Context Learning (ICL) based
summarization performance (Wang et al., 2023;
Laskar et al., 2023; Tang et al., 2023). This opens
the possibility of In-Context Personalization Learn-
ing (ICPL) in these LLMs. At the same time, it
also underscores the necessity for robust and de-
pendable methods of evaluating the degree of ICPL
within such models. Although benchmarking of
LLMs for summarization has been done for accu-
racy, fluency, and consistency (Zhang et al., 2024),
so far, no study has been done yet on the probing
and evaluation of ICPL in LLMs for the summariza-
tion task. In this paper, we propose iCOPERNICUS ,
anIn-Context Personalization Lear ning Scrutiny
ofSummarization capability in LLMs.
iCOPERNICUS framework. iCOPERNICUS is a
prompt-based probing framework that investigates
whether LLMs exhibit true ICPL using a 3-pronged
approach: (i) whether few-shot prompting of ex-
amples (i.e., reader-generated gold references)
enhances ICPL , (ii) whether adding reader’s read-
ing history improves ICPL , and (iii) whether con-
trastive profile information showing subjective
differences in reader-preferences for the same doc-
ument induce better ICPL . Since iCOPERNICUS
is acomparative framework, it needs a personal-
ization measure for analyzing the influence of the
injected profile information in the prompts. We usearXiv:2410.00149v1  [cs.CL]  30 Sep 2024Model Variants Probed
Base- Llama 2 (7B, 13B) (Touvron et al., 2023)
models Mistral v0.1 (7B) (Jiang et al., 2023)
Instruct-Mistral 7B Instruct v0.1 (Jiang et al., 2023)
Mistral 7B Instruct v0.2 (Jiang et al., 2023)
tuned Tulu V2 (7B, 13B) (Ivison et al., 2023)
Orca 2 (7B, 13B) (Mitra et al., 2023)
Stable Beluga (7B, 13B) (Mahan et al., 2023)
RLHF-tuned Llama 2 Chat (7B, 13B) (Touvron et al., 2023)
DPO-tunedTulu V2 DPO (7B, 13B) (Ivison et al., 2023)
Zephyr 7B α(Tunstall et al., 2023)
Zephyr 7B β(Tunstall et al., 2023)
Table 1: LLMs probed for ICPL w.r.t summarization.
EGISES-JSD, the only known measure for person-
alized summarization (Vansh et al., 2023). EGISES
measures the ability of models to discern the differ-
ences in user profiles and generate summaries that
are proportionately different.
Observations As a case-study of the application
ofiCOPERNICUS , we probe ten SOTA LLMs that
exhibit reasonably good ICL on standard bench-
mark tasks, six of which have 7B and 13B model
size variants (see Table 1). We use the PENS
dataset (Ao et al., 2021) as in (Vansh et al., 2023) to
compare the ICPL results with the baseline special-
ized personalized summarization models evaluated
therein. We observe that all the studied models,
except for Orca-2 7B and Zephyr 7B β, exhibit
performance degradation in all the three probes of
iCOPERNICUS - i.e., injection of examples, history,
and contrastive profile information.
Key Contributions. We present for the first
time: (i) a detailed introduction of ICPL for per-
sonalized summarization, (ii) iCOPERNICUS as a
formal framework of evaluation of ICPL in LLMs,
and (iii) a detailed case-study of the application
ofiCOPERNICUS tests for determining ICPL in the
selected SOTA LLMs.
2 Preliminaries
2.1 Personalization w.r.t Summarization
As proposed in Vansh et al. (2023), the degree-of-
personalization is a quantitative measure of how
much a summarization model fine-tuned for per-
sonalization is adaptive to a user’s (i.e., reader’s)
subjective expectation. This also implies that it
measures how accurately a model can capture the
user’s "evolving" profile reflected through read-
ing history (i.e., a temporal span of the reading
and skipping actions of a user on a sequence of
documents that is interleaved by the actions of gen-
erating and reading summaries). This is because
thesubjective expectation is a function of the read-ing history .A low degree of personalization, by
definition, implies poor user experience (UX) .
If a model does not efficiently capture the user’s
profile, it may lead to irrelevant summaries. In this
situation, poor UX would mean that the user would
have to spend more time getting to the information
he/she is interested in or suffer from information
overload and fatigue. However, this irrelevant in-
formation can be useful for a different user with
a different profile. To illustrate this, we borrow
the example given by Vansh et al. (2023) where
if reader Alice, who has been following " civilian
distress " in the Hamas-Israeli conflict, reads a news
summary whose content is primarily about " war-
front events ", her UX will drop down due to infor-
mation overload and high time-to-consume, even
though her interest is also covered to a fair extent.
In contrary, a reader Bob, who has been mostly
following war news, would have quite high UX.
2.2 EGISES: Personalization Measure
Vansh et al. (2023) showed theoretically and em-
pirically that a model could have high accuracy
scores in both the examples in the previous section,
although the individual degree of personalization
differs. This can mislead selection of a model for
a fairly high accuracy score even though it suf-
fers from poor UX . To address this, they proposed
EGISES as, to the best of our knowledge, the only
known measure for personalized summarization
evaluation. Informally, EGISES measures the ex-
tent to which a model can discern the differences
between user profiles and generate summaries that
have proportionate differences. Difference is mod-
eled as a chosen divergence on a metric space. See
Appendix A.2 for formulation.
2.3 In-Context Personalization Learning
In-Context learning (ICL) is an emergent phe-
nomenon exhibited LLMs (first highlighted in
Brown et al. (2020) for GPT-3), where models ac-
quire proficiency in unknown tasks on which they
are not pre-trained from limited examples, called
prompts , with no update in their parameters (i.e.,
the models are frozen).1In-Context Personaliza-
tion Learning (ICPL) for summarization is a special
case of ICL where, for a document query dq, an
LLM is expected to generate the user-specific opti-
mal summary s∗
(dq,hj), for the j-th user expecting
the summary of dq. Here, hjis the user’s reading
1For formalizations of ICL see Appendix A.3.history (temporal sequence of the user’s click and
skip history of documents). s∗
(dq,hj)is the same as
thej-the user’s expected summary uqj(i.e., gold-
reference), and hence can be denoted s∗
uqj.
Definition 1. Prompt4ICPL : A prompt PICPL to
a language model Mconsists of an user’s reading
history ( h), an optional sequence of nconcatenated
(⊕) demonstration examples (i.e., input-label pairs)
as:hj⊕nL
i=1(di⊕uij)where, diis the example doc-
ument to be summarized for j-th user ( uijbeing the
gold-reference summary example ), and a query
document dq, such that di̸=dq.
Azero-shot prompt (0-shot) is the special case
when demonstration examples are not provided
(i.e.,uij=∅) while dqand the user reading his-
toryhjare given in the prompt. The few-shot
version can be of two types: (i) with history (k-
shot w/hist.) , and (ii) without history (k-shot w/o
hist.) . In the second type, the user profile is not
represented by reading history but rather by the ex-
pected summaries (or gold-reference summaries).
This can seen as the user’s “ writing history ".
3 The iCOPERNICUS Framework
iCOPERNICUS is a novel prompt-based three-
pronged probing framework for evaluating ICPL
in LLMs. It tests whether the model can harness
three types of profile information included within
the test prompts: (i) examples , (ii) history , and (iii)
contrastive information (in terms of history and
examples) . Before we provide a detailed outline
of the framework, we first discuss the contrastive
probing setup in the following section.
3.1 Contrastive Probing
One of the key probes of the iCOPERNICUS frame-
work is testing LLMs for ICPL with contrastive
examples , i.e., input-label pairs containing at least
two user (i.e., reader) profiles (can be reading or
writing history) with the query document dq. An
LLM capable of ICPL should be able to discern the
difference between the reader profiles and generate
summaries accordingly (i.e., suijandsuik) in line
with the notion of insensitivity-to-subjectivity as
defined by Vansh et al. (2023) (see Appendix A.1).
Contrastive Prompt4ICPL ( PC) is defined as:
Definition 2. Contrastive Prompt4ICPL ( PC): A
PCgiven to a language model Mis a sequence of n
concatenated ( ⊕)contrastive demonstration exam-
Figure 1: PCtype: Contrastive (C)-2-shot w/ history.
plesDPCas:(mL
j=1hj)⊕(nL
i=1(di⊕mL
j=1uij))), each
having msubjective expected summaries ( uij), and
a query document dq, s.t.di̸=dq.
Acontrastive zero-shot prompt (C-0-shot) ,
contains hjandhkrepresenting the contrastive
reading-histories of two users jandkwith no
demonstration examples. The few-shot version,
similar to k-shot (plain) prompt, is of two types:
(i)C-k-shot w/hist. (Figure 1) and (ii) C-k-shot
w/o hist . In the second case, gold-reference sum-
maries (writing history) of both users are given
in the prompt as examples but not their reading
histories.2We now define ICPL (weak and strong
cases) as:
Definition 3. Weak ICPL. Given a contrastive
prompt PC, an LLM Mθ,uexhibits weak
ICPL, if ∀(uqj, uqk)w.r.t query document dq,
(σ(uqj, uqk)≤τU
max)⇐⇒ (σ(suqj, suqj)≤
τS
max);τU
max, τS
maxare bounds within which users’
expected (gold-reference) summary pair (uqj, uqk)
and LLM-generated summary pair ( suqj, suqj) are
indistinguishable; σis an arbitrary distance metric
on the metric space M, where d, u, s are defined.
Definition 4. Strong ICPL. Given PC,Mθ,uex-
hibits strong ICPL, if ∀(uqj, uqk)w.r.tdq,Mθ,u
satisfies: (i) weak ICPL, and (ii) (σ(uqj, uqk)>
τU
max)⇐⇒ (σ(suqj, suqj)> τS
max).
The following sections describe the three-
2NT:For sake of clarity, contrastive prompts are not chain-
of-thought (CoT) prompts as it does not contain thought break-
down or require thought generation .pronged iCOPERNICUS framework.
3.2 Probe 1: Do example summaries help?
The first probe within the iCOPERNICUS framework
studies the impact of k-shot prompts (in contrast
to 0-shot prompts) on LLM models.3The (plain)
k-shot w/o hist. prompt-based probing investigates
whether the model improves ICPL performance
w.r.t EGISES-scores by mapping the key latent con-
cepts in each example summary with those in the
corresponding document for any given user. This
is a much richer cue than the 0-shot case, where the
model does not get much assistance but can only ob-
serve inter-document conceptual associations (i.e.,
user’s click and skip patterns) provided as a part
of the reading history h. A richer version of this
probe is that with k-shot w/ hist. prompts, where
additional history information is also provided to
investigate if the model can associate that with the
examples provided. These two probes should also
be performed in the contrastive prompt setting (C-
k-shot prompts (w/ and w/o hist.)) to investigate the
presence of the same kind of associations with the
additional capacity of associating user-specific pro-
file concepts. More specifically, ideally, a model
should be able to harness example summaries in
the following order of ICPL performance:
1.(plain) 0-shot ≺(plain) k-shot w/o hist. , vio-
lation leads to Paradox 1 (PX-1) .
2.(plain) 0-shot ≺(plain) k-shot w/ hist. , vio-
lation leads to Paradox 1 w/ hist. (PX-1-h) .
3.C-0-shot ≺C-k-shot w/o hist. , violation
leads to PX-1 (contrastive) .
4.C-0-shot ≺C-k-shot w/ hist. , violation leads
toPX-1-h (contrastive) .
3.3 Probe 2: Does reading-history help?
The second probe investigates whether a model can
utilize the temporal sequence of a specific user’s
reading (i.e., document clicking and skipping) his-
tory and associate the innate latent concepts with
that of the corresponding example summaries pro-
vided in the prompt (both plain and contrastive).
Hence, a model should be able to harness reading
history in the following ICPL performance order:
1.(plain) k-shot w/o hist. ≺(plain) k-shot
w/ hist. , violation leads to PX-2 .
3It is to be noted that, as pointed out in Section 3.1, 0-shot
prompts contain the reading histories ( h).2.C-k-shot w/o hist. ≺C-k-shot w/ hist. , viola-
tion leads to PX-2 (contrastive) .
3.4 Probe 3: Do contrastive prompts help?
The third probe investigates whether models can
capitalize on additional contrasting (i.e., similar-
ity/differences) information about user profiles in
contrastive prompts. Hence, a model should be
able to harness contrastive information in the
following order of ICPL performance:
1.(plain) 0-shot ≺C-0-shot , violation leads to
PX-3 .
2.(plain) k-shot w/o hist. ≺C-k-shot w/o hist. ,
violation leads to PX-4 .
3.(plain) k-shot w/ hist. ≺C-k-shot w/ hist. ,
violation leads to PX-5 .
3.5 Limitations of EGISES w.r.t ICPL
As can be seen from the previous section,
iCOPERNICUS is a comparative framework that
needs a personalization measure for analysis of the
influence of profile information. Since it was re-
ported in Vansh et al. (2023) that the EGISES-JSD
variant performed well regarding human-judgment
correlation, we choose this specific variant as
the comparative measure within the iCOPERNICUS
framework to evaluate system-level strong degree-
of-ICPL (i.e., σin definitions 3 and 4 is JSD
(Jensen-Shannon Divergence)).4However, it is to
be noted that iCOPERNICUS is not tightly coupled
with EGISES; any better future personalization
evaluation measure can also be applied. In section
5.5, we empirically show that absolute EGISES
score-based ICPL leaderboards can be misleading.
4 Evaluation: Setup
4.1 Model Benchmarking Dataset
Evaluating the selected models w.r.t iCOPERNICUS
requires the test dataset to contain (i) example (and
expected) gold summaries, (ii) user’s reading his-
tory, and (iii) contrastive examples (i.e., subjective
gold summaries). We selected the test data sourced
from the PENS dataset (Ao et al., 2021)5for our
purpose since, to the best of our knowledge, it is
the only dataset containing all the above three. 103
4NT: EGISES as a standalone measure has not been de-
signed to test ICPL (see Appendix A.2 for formulation).
5We comply with the Microsoft Research License Terms.Prompt Style Reading Hist. Examples Article Body # Prompts
0-shot 1200 Tokens – 2500 Tokens 6856
C-0-shot 1000 x 2 Tokens – 1700 Tokens 5246
2-shot w/o hist. – 950 x 2 Tokens 1800 Tokens 6798
C-2-shot w/o hist. – 950 x 2 Tokens 1800 Tokens 5246
2-shot w/ hist. 1200 Tokens 600 x 2 Tokens 1300 Tokens 6798
C-2-shot w/ hist. 850 x 2 Tokens 450 x 2 Tokens 1100 Tokens 5246
Table 2: iCOPERNICUS prompt composition (w.r.t # of tokens) for all prompt styles; NT: overall prompt size is fixed.
college students, having English as their native lan-
guage, were invited as voluntary participants. A
two-phase process was adopted to construct the test
set. Initially, the participants selected at least 50
articles of personal interest from a pool of 1000
news articles, which were then sorted based on
exposure time. This formed their reading his-
tories . Subsequently, participants in the second
phase created preferred headlines (gold references)
for 200 news articles without prior knowledge of
the original headlines. This formed the set of ex-
amples and expected (personalized) summaries .
The two-stage process ensures an average of four
gold-reference summaries per article, thereby en-
abling contrastive prompts to be sampled out .
For details, see Appendix C.
4.2 Probing Dataset Creation
We engineer six distinct prompt templates in accor-
dance with the iCOPERNICUS framework (see Ta-
ble 2). The prompts were sampled from the PENS
dataset (section 4.1) with sample size of 3840 news
articles such that the total number of tokens for all
the settings were 3700 - i.e., the overall prompt
size remained constant . This was done so that the
probes were comparable in a controlled environ-
ment. Depending on the specific test, each prompt
has been broken up into history, examples, and ar-
ticle body.6The dataset is released for research
purposes at KDM-Lab_iCOPERNICUS_prompt-
dataset_v1.0.
4.3 Probed SOTA LLMs
We probe ten SOTA LLMs with their 7B and 13B
variants (see Tables 1 and 3), totaling seventeen
variants. Models are chosen based on their recency,
training data diversity, and performance on key
benchmark tasks requiring comparative reasoning.7
6See Figure 4 for the structure of the prompts and Figures
6-10 in the Appendix for examples.
7Tasks: commonsense reasoning (e.g., Winogrande (Sak-
aguchi et al., 2019), Hellaswag (Zellers et al., 2019)), math
(e.g., GSM8k (Cobbe et al., 2021)), code (e.g., MBPP (Austin
et al., 2021)), and multi-task benchmarks (e.g., MMLUWe could not evaluate 13B+ models due to compute
resource constraints. However, the core contribu-
tion of the paper is the iCOPERNICUS framework
itself which is applicable for selection decision of
any sized model . Hence, the evaluations is primar-
ily a case study of the application of iCOPERNICUS .
Appendix B.1 has model descriptions.
4.3.1 Baseline Summarization Models
To understand whether the probed LLMs are supe-
rior to specialized PLMs trained on personalized
summarization tasks, we examine the same ten
SOTA summarization models as in (Vansh et al.,
2023) for comparative benchmarking. Five of them
are specifically trained personalized models that
follow the PENS framework (Ao et al., 2021): (i)
PENS-NRMS Injection-Type 1 (PENS-NRMS T1),
(ii) PENS-NRMS Injection-Type 2 (PENS-NRMS
T2), (iii) PENS-NAML T1, (iv) PENS-EBNR T1,
and (v) PENS-EBNR T2. These models encode the
document article using the Transformer encoder
(Vaswani et al., 2017), deep-neural-model-based
user history encoders (Okura et al., 2017; Wu et al.,
2019a,b), and a Pointer-generator-network-based
(See et al., 2017) decoder for generating the person-
alized summaries. The other five non-personalized
models are generic SOTA summarizers – BRIO
(Liu et al., 2022), SimCLS (Liu and Liu, 2021),
BigBird-Pegasus (Zaheer et al., 2020), Prophet-
Net (Qi et al., 2020), and T5-base (Orzhenovskii,
2021). These models were evaluated by provid-
ing documents enriched with headlines (reference
summaries), serving as cues (Vansh et al., 2023).
Since the baseline models are incapable of ICPL ,
iCOPERNICUS tests are inapplicable for them, and
EGISES-based evaluation is sufficient. The model
descriptions are in Appendix B.2.
4.3.2 Hyperparameter Selection
We conduct temperature ablation within the inter-
val[0.5,0.75]to balance accuracy and diversity for
personalized summarization and observe almost
(Hendrycks et al., 2020), AGI Eval (Zhong et al., 2023))LLM Model Variants 0-shot 2-shot w/o hist. 2-shot w/ hist. C-0-shot C-2-shot w/o hist. C-2-shot w hist.
Base modelsLlama 2 7B 0.408 0.367 0.367 0.408 0.46 0.458
Llama 2 13B 0.412 0.371 0.357 0.418 0.5 0.474
Mistral 7B v0.1 0.398 0.353 0.354 0.464 0.406 0.469
Instruction-tunedMistral 7B Instruct v0.1 0.4 0.366 0.378 0.395 0.405 0.399
Mistral 7B Instruct v0.2 0.391 0.348 0.354 0.359 0.339 0.369
Tulu V2 7B 0.364 0.376 0.356 0.36 0.38 0.37
Tulu V2 13B 0.376 0.389 0.385 0.387 0.405 0.392
Orca 2 7B 0.44 0.436 0.433 0.359 0.351 0.347
Orca 2 13B 0.445 0.442 0.447 0.347 0.366 0.356
Stable Beluga 7B 0.371 0.395 0.407 0.377 0.398 0.396
Stable Beluga 13B 0.388 0.394 0.404 0.4 0.405 0.412
RLHF-tunedLlama 2 7B Chat 0.383 0.391 0.362 0.333 0.349 0.338
Llama 2 13B Chat 0.36 0.393 0.383 0.341 0.365 0.357
DPO-tunedTulu V2 DPO 7B 0.325 0.345 0.356 0.338 0.355 0.348
Tulu V2 DPO 13B 0.359 0.345 0.385 0.37 0.383 0.368
Zephyr 7B α 0.360 0.351 0.357 0.343 0.352 0.353
Zephyr 7B β 0.384 0.359 0.369 0.35 0.356 0.345
Table 3: iCOPERNICUS Probe Results : Master table for all comparative analysis including the detection of the
nine potential paradoxes that can arise due to the three probes outlined in Section 3; EGISES-JSD is used for the
comparative evaluation (lower is better); Table 4, a summary of the observed paradoxes, is derived from this table.
Evaluation Script: https://github.com/KDM-LAB/iCOPERNICUS-EMNLP24
similar results to the selected 0.6.8Better ICPL
performance might be seen for specific models un-
der more comprehensive ablation, but finding an
optimal configuration that generalizes for all LLMs
is hard. Nevertheless, the current evaluation is a
useful indication of potential paradoxes and the
possibility of misguided model selection if one re-
lies solely on an EGISES-based leaderboard (see
section 5.5 for empirical results).9
5 Observations and Insights
5.1 Effect of Examples (User-Summaries)
In the probe 1, we find that 10 model variants (out
of 17) exhibit an increase in ICPL (i.e., EGISES-
JSD scores) by an average of 2.6% ↑for 2-shot
prompts (w/o reading history (hist.)) w.r.t zero-
shot (see PX-1 col. of Table 4 for the performing
models (denoted as: ✗)). However, seven models
degrade with (plain) 2-shot (w/o history) prompts
(average drop of 1.7% ↓), leading to the first of the
five observed paradoxes of less is more as outlined
in section 3 (see Table 4 for result summary).
(PX-1) Implicit is more than explicit: We be-
lieve that these seven models learn more from the
latent concept association in the (temporal) reader’s
history at a broader level than the specific concepts
within the example summaries (i.e., explicit reader-
profile as " writing-history "), the replacement of
which makes them deviate from their earlier ICPL
8See Appendix F for inference environment (LLM settings
and compute resources).
9In a way, iCOPERNICUS tests show the need for detailed
hyperparameter optimization before model selection.Model Variants PX-1 PX-2 PX-3 PX-4 PX-5
Base-Llama 2 7B ✗ ✗ ✓ ✓ ✓
ModelLlama 2 13B ✗ ✗ ✓ ✓ ✓
Mistral 7B v0.1 ✗ ✗ ✓ ✓ ✓
Instruct-Mistral 7B Instr. v0.1 ✗ ✗ ✗ ✓ ✓
Mistral 7B Instr. v0.2 ✗ ✗ ✗ ✗ ✓
Tulu V2 7B ✓† ✗ ✗ ✓ ✓
Tulu V2 13B ✓† ✓ ✓ ✓ ✓
tuned Orca 2 7B ✗ ✗ ✗ ✗ ✗
Orca 2 13B ✗ ✓ ✗ ✗ ✓
Stable Beluga 7B ✓† ✓ ✓ ✓ ✓
Stable Beluga 13B ✓† ✓ ✓ ✓ ✓
RLHF- Llama 2 7B Chat ✓† ✗ ✗ ✗ ✓
tuned Llama 2 13B Chat ✓† ✓ ✗ ✗ ✓
DPO-Tulu V2 DPO 7B ✓† ✓ ✓ ✓ ✓
Tulu V2 DPO 13B ✗ ✓ ✓ ✓ ✗
tuned Zephyr 7B α ✗ ✗ ✗ ✓ ✓
Zephyr 7B β ✗ ✗ ✗ ✗ ✗
Table 4: Paradox (PX) of less is more ( ✓: PX exists):
PX-1/2: 2-shot w/o & w/ hist.; PX-3/4/5: C-0-shot/C-2-
shot w/o & w/ hist.; †denotes improvement over base
models; for examples see Figures 6-10 in Appendix.
performance. However, all these models show sig-
nificant ICPL boost w.r.t their respective base mod-
els for the 2-shot w/o hist. case (denoted by †). A
real example of PX-1 can be seen in Figure 6.
5.2 Effect of User’s Reading History
As a part of the second probe w.r.t iCOPERNICUS ,
we observe that 10 model variants exhibit an in-
crease in ICPL (avg. boost: 2.5% ↑) for 2-shot w/
hist. w.r.t zero-shot. It is also observed that the set
of LLMs that show ICPL in the previous case does
not take advantage of the additional history data,
leading to the second of the paradoxes (i.e., PX-2).
(PX-2) Reading history distracts: Seven model
variants show worse ICPL with the additional his-
tory data (avg. drop: 2% ↓; see Table 4). We be-
lieve that these models tend to learn more from theBaseline Models EGISES-JSD
BigBird-Pegasus 0.429
SimCLS 0.557
BRIO 0.661
PENS-NAML T1 0.899
PENS-NRMS T1 0.916LLM Models EGISES-JSD Paradoxes (PX) Observed
Tulu V2 DPO 7B (0-shot / C-0-shot / 2-shot) 0.325 / 0.338 / 0.345 PX-1/2/3/4/5
Llama 2 7B Chat (C-0-shot / C-2-shot / 2-shot) 0.333 / 0.338 / 0.345 PX-1/5
Llama 2 13B Chat (C-0-shot) 0.341 PX-1/2/5
Tulu V2 DPO 13B (2-shot) 0.345 PX-2/3/4
Orca 2 13B (C-0-shot) 0.347 PX-2/5
Table 5: EGISES Leaderboard Misleads : Top-5 LLMs as per EGISES-JSD, apparently beating top-5 baselines
(top-2 personalized and top-3 non-personalized w/ summary cue), do not pass critical iCOPERNICUS tests exhibiting
several paradoxes.
format of the prompt and latent-concept associa-
tion but at a rather broader thematic level and get
"distracted " (Shi et al., 2023) by the concept dis-
tribution within histories. A real example of PX-2
can be seen in Figure 7.
5.3 Effect of Contrastive Prompts
The third probe tests if contrastive user profiles (C-
0-shot, C-2-shot w/o hist., C-2-shot w/ hist.) induce
better ICPL in the models. This probe is central for
any model to pass the iCOPERNICUS test.
Case of C-0-shot : We observe that 9 model vari-
ants seem to harness the additional contrastive
information about the readers’ reading history
in comparison with the 0-shot case (avg. boost:
3.8%↑, see PX-3 col. of Table 4 for performing
models), which is better than the overall boost ob-
served with (plain) 2-shot (w/ and w/o history).
This indicates that these models might actually be
utilizing the contrastive information in the reading
histories rather than the examples without the con-
trast. However, this ICPL behavior is not observed
in the remaining eight model variants, leading to
a special case of the paradox of contrastive user
profiles - PX-3. We will discuss this subsequently.
Case of C-2-shot w/o history : We observe
that only six model variants pass this probe test
and seem to harness the contrastive examples
quite notably (p-value <0.01) when compared to
the case of 2-shot w/o history (avg. boost: 4.1% ↑)
(see PX-4 col. in Table 4 for these 6 models). In
fact, Llama 2 13B Chat, one of the 6 variants, de-
grades in the cases of (plain) 2-shot (w & w/o hist.).
Again, most variants (11) are non-compliant, lead-
ing to the other special case – PX-4.
Case of C-2-shot w/ history : We find that only
three model variants are compliant with this
probe and seem to marginally (i.e., not notable)
utilize the additional contrastive history along
with the contrastive examples when compared to
the case of C-0-shot (avg. boost: 0.6% ↑; Orca 2 7B
has max. boost of 1.2% ↑), while 14 model variants
clearly seem to get distracted with an average dropof 1.6% ↓, leading to the paradox – PX-5.
(PX-3/4/5) Contrast can be confusing: Surpris-
ingly, additional contrastive reader profile informa-
tion doesn’t enhance ICPL in many model variants.
Eight out of 17 models show distraction and a 1.6%
average drop when contrast is injected into the his-
tory component, struggling to map intra-concept
associations within the history ( PX-3 ). A real ex-
ample of PX-3 can be seen in Figure 8. Moreover,
11 models exhibit a 3.6% average drop when pro-
vided with contrastive examples in a 2-shot setup
without history ( PX-4 ). This decline may be due to
overfitting the format of the 2-shot setup, causing
distraction. A real example of PX-4 can be seen
in Figure 9. Additionally, 14 model variants fail to
effectively utilize the richest prompt, C-2-shot with
history, with seven unable to harness any form of
contrastive profile information ( PX-3/4/5 ). Three
variants use contrastive history in C-0-shot but not
contrastive examples in C-2-shot w/o hist., thus
learning from broader historical topics rather than
label instances ( PX-4/5 ). Surprisingly, four vari-
ants can utilize contrastive histories in C-0-shot
and contrastive examples in C-2-shot w/o hist., but
not both, potentially due to spurious (and cross)
linking between history and example concepts. A
real example of PX-5 can be seen in Figure 10.
5.4 Effect of Article Length
In probe 1, we substitute longer articles with shorter
ones, keeping the prompt length the same (see Ta-
ble 2). Arguably, personalized summarization of
shorter articles should be an easier task. However,
several models exhibit the PX-1 and PX-1 (con-
trast) paradoxes (i.e., adding examples led to poor
ICPL) under probe-1. Hence, we cannot gener-
alize that the length of the articles has influence .
Also, several models pass tests involving longer
articles or history sequences, thereby reinforcing
this finding.Models PX-3 PX-4 PX-5
Llama 2 13B Chat 1.97% ↑0.28% ↑1.54% ↑
Mistral 7B Inst. v0.2 1.96% ↑0.46% ↑1.78% ↑
Tulu V2 DPO 7B 1.94% ↑0.56% ↑2.2%↑
Table 6: Adversarial Validation: PX-3/4/5 exists. %↑
value indicates how much the paradox worsens.
5.5 EGISES Leaderboard Misleads
We observe from Table 4 that the top-5 best-
performing LLMs in terms of EGISES-JSD as per
Table 510- i.e., Tulu v2 DPO 7B, Llama 2 7B Chat,
Llama 2 13 B Chat, Tulu v2 DPO 7B, and Orca
2 13 B show several paradoxes with Tulu v2 DPO
7B being the worst. In fact, the best performing
models (Orca 2 7B and Zephyr 7B β), passing all
theiCOPERNICUS tests (see Table 4) do not rank
high in Table 5. This empirically establishes that
EGISES as a standalone measure is inadequate for
ICPL evaluation in LLMs .
6 Validation of Paradoxes
The paradoxes raise a serious question: do these
models exhibit true ICPL? (including the top-5
LLMs in Table 5) Hence, we need to first confirm
that the paradoxes exist, which is done via adversar-
ial probes. We select three model variants for three
scenarios: (i) worst case : Tulu V2 DPO 7B show-
ing all the paradoxes, (ii) average case : Llama 2
13B Chat showing non-contrastive (PX-1/2) and
contrastive (PX-5) paradoxes, and (iii) best case :
Mistral 7B Instruct v0.2 having PX-5 only.
Adversarial Probe-based Validation: In the
adversarial probe setup, one of the user profiles
(i.e., ground-truth history or examples) in the con-
trastive prompts (C-0-shot/C-2-shot w/o hist./C-
2-shot w/ hist.) was replaced with a random his-
tory/examples. Since it is randomly sampled, the
noise would be completely irrelevant to the doc-
ument article to be summarized. Upon injection
of such a noise, a model’s personalization perfor-
mance, and thereby the exhibited paradoxes, should
degrade further. However, if a model shows a bet-
ter or similar EGISES score, then it would mean
that the contrastive tests of iCOPERNICUS are no
better than tests based on random choices . In other
words, iCOPERNICUS probes may not necessarily
provide conclusive insights. However, we observe
in Table 6 that the paradoxes do worsen (PX-3 avg.
10All the models are seemingly better than the strongest
baseline model (BigBird-Pegasus), with 57% ↑for Tulu V2
DPO 7B when compared to PENS-NAML-T1.spike: 1.9% ↑; PX-4 avg. spike: 0.43% ↑; PX-5 avg.
spike: 1.8% ↑).This validates the robustness of
theiCOPERNICUS tests . Details in Appendix E.1.
Human-Judgment Validation: We further val-
idated PX-5 (the most serious paradox) using
survey-based unbiased judgments (i.e., similarity
ratings (1 (low) - 6 (very high)) on summary-pairs,
both reference and model-generated from 339 re-
spondents.11The core objective is to validate the
extent to which human evaluators would agree with
the design principles of EGISES-JSD at a cognitive
level and thereby agree with the iCOPERNICUS test
results that are based on EGISES-JSD. We, there-
fore, model a " human version " of EGISES-JSD
(i.e., EGISES-HJ) and used the survey responses to
estimate EGISES-HJ). We find that PX-5 persists
except for Mistral 7B Instruct v0.2 (best case; ICPL
boost: 1.42% ↑; Table 9). We also find that PX-1
(contrastive) persists in the other models.12
7 Related Work
ICL for summarization: ICL capabilities in
LLMs w.r.t summarization were first observed
within reinforcement learning frameworks, utiliz-
ing human-feedback-trained reward models to pre-
dict human ratings for specific summary actions
(Stiennon et al., 2020; Nguyen et al., 2022). Since
then, LLMs have shown unprecedented ICL-based
summarization performance (Wang et al., 2023;
Laskar et al., 2023; Tang et al., 2023). This opens
the possibility of ICPL in these LLMs. At the same
time, it also underscores the necessity for robust
and dependable methods of evaluating the degree
of ICPL within such models. Benchmarking LLMs
for summarization was done for accuracy, fluency,
and consistency (Zhang et al., 2023), but not ICPL.
Personalization evaluation: Personalization
evaluation is studied in recommendation systems
(Zangerle and Bauer, 2022), with metrics based
on the Jaccard Index, MAE/RMSE/Hit-Ratio (Li
et al., 2024), and nDCG (normalized Discounted
Cumulative Gain) (Matthijs and Radlinski, 2011).
However, these metrics are not useful for summa-
rization. The only personalization evaluation met-
ric for summarizers is EGISES (Vansh et al., 2023).
However, as established, relying solely on EGISES
to evaluate ICPL can be misleading.
11Grad student volunteers in 20-30 age group from Com-
puter Sc., Maths, and Humanities; ∼70% male, ∼30% female.
For details of survey methodology see Appendix E.2.
12Statistical Significance: p-value <0.01.8 Conclusion
We propose iCOPERNICUS , a novel evaluation
framework for analyzing the capability of true In-
Context Personalization Learning (ICPL) in LLMs
for the personalized summarization task. The cen-
tral goal is to detect whether models can pass all the
probes (or exhibit the " paradox of less is more ").
We showed that relying solely on EGISES-scores
can be misleading (as in top-4 LLMs beating base-
lines). Only 2 out of the 17 SOTA LLM variants
probed passed the iCOPERNICUS test, hence the
need for further research on ICPL-driven LLMs.
Limitations
In this work, we restrict probing of In-Context Per-
sonalization Learning (ICPL) w.r.t summarization
to 7B and 13B model variants. It would be interest-
ing to observe how smaller SLMs ( <7B), such as
the Phi model suite (Gunasekar et al., 2024), fare
theiCOPERNICUS test. Also, studies around the
effect of more recent fine-tuning techniques such
as PEFT (LoRA and QLoRA) need to be analyzed.
On similar lines, it would be interesting to observe
whether ICPL, which otherwise is not emerging by
doubling the models’ size, finally starts emerging at
an even larger scale (which often is the case for sev-
eral emerging properties). At the same time, robust
and reliable ICPL measures should be designed for
aggregated leaderboard generation of models w.r.t
ICPL within the iCOPERNICUS framework. Finally,
more robust and systematic adversarial probing of
ICPL is required to analyze true ICPL in models.
Ethics Statement
We would like to declare that we used the PENS
dataset prepared and released by Microsoft Re-
search. Our human-judgment survey was con-
ducted according to the norms set by the Institu-
tional Review Board (IRB) and respects participant
anonymity as per guidelines.
References
Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He,
and Xing Xie. 2021. PENS: A dataset and generic
framework for personalized news headline genera-
tion. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and
the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 82–92. Association for Computational Linguis-
tics.Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie J. Cai, Michael Terry, Quoc V . Le, and
Charles Sutton. 2021. Program synthesis with large
language models. ArXiv , abs/2108.07732.
Ayaka. 2023. Jax implementation of the llama 2 model.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. ArXiv , abs/2110.14168.
Abdul Ghafoor Etemad, Ali Imam Abidi, and Megha
Chhabra. 2021. Fine-tuned t5 for abstractive sum-
marization. International Journal of Performability
Engineering , 17(10).
Lea Frermann and Alexandre Klementiev. 2019. Induc-
ing document structure for aspect-based summariza-
tion. In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics , pages
6263–6273, Florence, Italy. Association for Compu-
tational Linguistics.
Samira Ghodratnama, Mehrdad Zakershahrak, and Fari-
borz Sobhanmanesh. 2021. Adaptive summaries:
A personalized concept-based summarization ap-
proach by learning from users’ feedback. In Service-
Oriented Computing – ICSOC 2020 Workshops ,
pages 281–293, Cham. Springer International Pub-
lishing.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Ce-
sar Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Conti Kauffmann,
Gustavo Henrique de Rosa, Olli Saarikivi, Adil Salim,
Shital Shah, Harkirat Behl, Xin Wang, Sebastien
Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat
Lee, and Yuanzhi Li. 2024. Textbooks are all you
need.
Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris
Ackerson, Raj Neervannan, and Graham Neubig.
2021. WikiAsp: A Dataset for Multi-domain Aspect-
based Summarization. Transactions of the Associa-
tion for Computational Linguistics , 9:211–225.Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Xiaodong Song, and
Jacob Steinhardt. 2020. Measuring massive multitask
language understanding. ArXiv , abs/2009.03300.
Hamish Ivison, Yizhong Wang, Valentina Pyatkin,
Nathan Lambert, Matthew Peters, Pradeep Dasigi,
Joel Jang, David Wadden, Noah A. Smith, Iz Belt-
agy, and Hannaneh Hajishirzi. 2023. Camels in a
changing climate: Enhancing lm adaptation with tulu
2.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv e-prints , pages arXiv–2310.
Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen,
and Shashi Bhushan TN. 2023. Building real-world
meeting summarization systems using large language
models: A practical perspective. In Proceedings
of the 2023 Conference on Empirical Methods in
Natural Language Processing: Industry Track , pages
343–352, Singapore. Association for Computational
Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 7871–7880. Association for Computational
Linguistics.
Shu Li, Yuan Zhao, Longjiang Guo, Meirui Ren, Jin
Li, Lichen Zhang, and Keqin Li. 2024. Quantifica-
tion and prediction of engagement: Applied to per-
sonalized course recommendation to reduce dropout
in moocs. Information Processing & Management ,
61(1):103536.
Yixin Liu and Pengfei Liu. 2021. SimCLS: A sim-
ple framework for contrastive learning of abstractive
summarization. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 2: Short
Papers) , pages 1065–1072. Association for Compu-
tational Linguistics.
Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham
Neubig. 2022. BRIO: Bringing order to abstractive
summarization. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2890–2903,
Dublin, Ireland. Association for Computational Lin-
guistics.
Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan
Cooper, and Christian Laforte. 2023. Stable
Beluga models. huggingface.co/stabilityai/
StableBeluga2 . [Online; accessed 1-December-
2023].Nicolaas Matthijs and Filip Radlinski. 2011. Personaliz-
ing web search using long term browsing history. In
Proceedings of the fourth ACM international confer-
ence on Web search and data mining , pages 25–34.
M.L. Menéndez, J.A. Pardo, L. Pardo, and M.C. Pardo.
1997. The jensen-shannon divergence. Journal of
the Franklin Institute , 334(2):307–318.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstrations:
What makes in-context learning work? In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 11048–11064,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Arindam Mitra, Luciano Del Corro, Shweti Maha-
jan, Andres Codas, Clarisse Simoes Ribeiro, Sa-
haj Agrawal, Xuxi Chen, Anastasia Razdaibiedina,
Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing
Zheng, Corby Rosset, Hamed Khanpour, and Ahmed
Awadallah. 2023. Orca-2: Teaching small language
models how to reason. arXiv.
Duy-Hung Nguyen, Nguyen Viet Dung Nghiem, Bao-
Sinh Nguyen, Dung Tien Tien Le, Shahab Sabahi,
Minh-Tien Nguyen, and Hung Le. 2022. Make the
most of prior data: A solution for interactive text
summarization with preference feedback. In Find-
ings of the Association for Computational Linguis-
tics: NAACL 2022 , pages 1919–1930, Seattle, United
States. Association for Computational Linguistics.
Shumpei Okura, Yukihiro Tagami, Shingo Ono, and
Akira Tajima. 2017. Embedding-based news recom-
mendation for millions of users. In Proceedings of
the 23rd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining , KDD ’17,
page 1933–1942, New York, NY , USA. Association
for Computing Machinery.
Mikhail Orzhenovskii. 2021. T5-long-extract at fns-
2021 shared task. In Proceedings of the 3rd Financial
Narrative Processing Workshop , pages 67–69.
Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu,
Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming
Zhou. 2020. ProphetNet: Predicting future n-gram
for sequence-to-SequencePre-training. In Findings
of the Association for Computational Linguistics:
EMNLP 2020 , pages 2401–2410. Association for
Computational Linguistics.
GS Ramesh, Vamsi Manyam, Vijoosh Mandula, Pavan
Myana, Sathvika Macha, and Suprith Reddy. 2022.
Abstractive text summarization using t5 architecture.
InProceedings of Second International Conference
on Advances in Computer Engineering and Commu-
nication Systems: ICACECS 2021 , pages 535–543.
Springer.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2019. An adversarial winograd
schema challenge at scale.Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1073–
1083, Vancouver, Canada. Association for Computa-
tional Linguistics.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed Chi, Nathanael Schärli, and
Denny Zhou. 2023. Large language models can be
easily distracted by irrelevant context. In Proceed-
ings of the 40th International Conference on Machine
Learning , ICML’23. JMLR.org.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. Advances
in Neural Information Processing Systems , 33:3008–
3021.
Yuting Tang, Ratish Puduppully, Zhengyuan Liu, and
Nancy Chen. 2023. In-context learning of large lan-
guage models for controlled dialogue summarization:
A holistic benchmark and empirical analysis. In Pro-
ceedings of the 4th New Frontiers in Summarization
Workshop , pages 56–67, Singapore. Association for
Computational Linguistics.
T Tawmo, Mrinmoi Bohra, Pankaj Dadure, Partha
Pakray, et al. 2022. Comparative analysis of t5
model for abstractive text summarization on different
datasets. In Proceedings of the International Con-
ference on Innovative Computing & Communication
(ICICC) 2022 . SSRN.
Maartje Ter Hoeve, Julia Kiseleva, and Maarten Rijke.
2022. What makes a good and useful summary?
Incorporating users in automatic summarization re-
search. In Proceedings of the 2022 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 46–75, Seattle, United States. Association
for Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open foundation
and fine-tuned chat models. arXiv e-prints , pages
arXiv–2307.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct
distillation of lm alignment. arXiv e-prints , pages
arXiv–2310.
Rahul Vansh, Darsh Rank, Sourish Dasgupta, and Tan-
moy Chakraborty. 2023. Accuracy is not enough:
Evaluating personalization in summarizers. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023 , pages 2582–2595, Singapore.
Association for Computational Linguistics.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Yiming Wang, Zhuosheng Zhang, and Rui Wang. 2023.
Element-aware summarization with large language
models: Expert-aligned evaluation and chain-of-
thought method. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 8640–8665,
Toronto, Canada. Association for Computational Lin-
guistics.
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert
Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
Da Huang, Denny Zhou, et al. 2023. Larger language
models do in-context learning differently. arXiv e-
prints , pages arXiv–2303.
Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang
Huang, Yongfeng Huang, and Xing Xie. 2019a. Neu-
ral news recommendation with attentive multi-view
learning. In Proceedings of the Twenty-Eighth Inter-
national Joint Conference on Artificial Intelligence .
International Joint Conferences on Artificial Intelli-
gence Organization.
Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng
Huang, and Xing Xie. 2019b. Neural news recom-
mendation with multi-head self-attention. In Pro-
ceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th In-
ternational Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 6389–6394,
Hong Kong, China. Association for Computational
Linguistics.
Sang Michael Xie, Aditi Raghunathan, Percy Liang,
and Tengyu Ma. 2021. An explanation of in-context
learning as implicit bayesian inference. In Interna-
tional Conference on Learning Representations .
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, and Amr Ahmed. 2020. Big bird: Trans-
formers for longer sequences. In Advances in Neural
Information Processing Systems , volume 33, pages
17283–17297.
Eva Zangerle and Christine Bauer. 2022. Evaluating
recommender systems: Survey and framework. ACM
Comput. Surv. , 55(8).
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a ma-
chine really finish your sentence? In Annual Meeting
of the Association for Computational Linguistics .
Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,
Kathleen McKeown, and Tatsunori B. Hashimoto.
2023. Benchmarking large language models for news
summarization.Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,
Kathleen McKeown, and Tatsunori B. Hashimoto.
2024. Benchmarking Large Language Models for
News Summarization. Transactions of the Associa-
tion for Computational Linguistics , 12:39–57.
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,
Shuai Lu, Yanlin Wang, Amin Saied Sanosi Saied,
Weizhu Chen, and Nan Duan. 2023. Agieval: A
human-centric benchmark for evaluating foundation
models. ArXiv , abs/2304.06364.
A Preliminaries
A.1 Degree-of-personalization
In this section, we recall the notion of insensitivity-
to-subjectivity of a personalized summarization
model ( Mθ,u: (d, u)7→su;where suis the per-
sonalized summary for reader uon document d) as
defined in Vansh et al. (2023).
Definition 5. Weak Insensitivity-to-Subjectivity. A
summarization model Mθ,uis (weakly) Insensitive-
to-Subjectivity w.r.t reader u, if ∀(ui, uj),
(σ(ui, uj)≤τU
max)⇐⇒ (σ(sui, suj)> τS
max),
where σ13is an arbitrary distance metric defined
on the metric space Mwhere d, u, s are defined,
τU
maxis the maximum limit for ui, ujto be mutually
indistinguishable, and τS
maxis the maximum limit
forsui, sujto be mutually indistinguishable.
Definition 6. Strong Insensitivity-to-
Subjectivity. A summarization model Mθ,u
is (strongly) Insensitive-to-Subjectivity w.r.t
reader uif∀(ui, uj),Mθ,usatisfies: (i)
the condition of weak insensitivity, and (ii)
(σ(ui, uj)> τU
max)⇐⇒ (σ(sui, suj)≤τS
max).
A.2 EGISES and Degree-of-Personalization
We generalize the definition of summary-level devi-
ation (or, Degree-of- Responsiven ess(DEGRESS ))14
proposed by Vansh et al. (2023) as follows:
Definition 7. Summary-level DEGRESS .Given a
document diand a user-profile uij(user j’s
expected summary), the summary-level respon-
siveness of a personalized model Mθ,u, (i.e.,
DEGRESS (suij|(di, uij))), is defined as the propor-
tional divergence between model-generated sum-
mary suijofdiforj-th user from other user-
specific summary versions w.r.t a corresponding
divergence of uijfrom the other user-profiles.
13σ(ui, ui) = 0 ;σ(ui, uj)∈[0,1];σsatisfies positivity,
reflexive, maximality, symmetry, and the triangle inequality.
14This is based on the notion of weak and strong
insensitivity-to-subjectivity , as defined by (Vansh et al., 2023)
(see Appendix A.1).DEGRESS (suij|(di, uij))is formulated as:
DEGRESS (suij|(di, uij)) =1
|Udi||Udi|X
k=1min(Xijk, Yijk) +ϵ
max(Xijk, Yijk) +ϵ
Xijk=exp(w(uij|uik))
|Udi|P
l=1exp(w(uij|uil))·σ(uij, uik)
Yijk=exp(w(suij|suik))
|Udi|P
l=1exp(w(suij|suil))·σ(suij, suik)
w(uij|uik) =σ(uij, uik)
σ(uij, di);w(suij|suik) =σ(suij, suik)
σ(suij, di)
(1)
Here, |D|is the total number of documents in
the evaluation dataset, |U|is the total number
of users who created gold-reference summaries
that reflect their expected summaries (and thereby
their subjective preferences or profiles), and |Udi|
(=|Sdi|) is the number of users who created gold-
references for document di. A lower value of
DEGRESS (suij|(di, uij))indicates that while reader-
profiles are different, the generated summary suij
is very similar to other reader-specific summaries
(or vice versa), and hence, is not responsive at
the summary-level. The system-level DEGRESS and
EGISES have been formulated as follows:
DEGRESS (Mθ,u) =|D|P
i=1|Udi|P
j=1DEGRESS (suij|(di,uij))
|Udi|
|D|(2)
EGISES (Mθ,u) = 1−DEGRESS (Mθ,u) (3)
EGISES measures the degree of insensitivity-to-
subjectivity for relative benchmarking of how much
models lack personalization (i.e., a lower score is
better within the range: [0,1]) instead of assign-
ing an absolute goodness score. In this paper, we
choose Jensen-Shannon Divergence (JSD) (Menén-
dez et al., 1997), where d,u, and suare defined
as word distributions on a probability space. JSD
has a strong human-judgment correlation and has
been used in evaluating the ten specialized baseline
models (Vansh et al., 2023).
A.3 In-Context Learning
ICL is a method employed by LLMs, notably em-
phasized in Brown et al. (2020) (GPT-3’s ICL be-
havior was first highlighted), where models acquire
proficiency in apparently unknown tasks (i.e., tasks
on which the models are not pre-trained) from lim-
ited examples, called prompts , with no update intheir parameters (i.e., the models are frozen). For-
mally, it is defined below.
Definition 8. Prompt : A prompt Pgiven to a lan-
guage model Mis a sequence of nconcatenated
(⊕) demonstration examples (i.e., input-label pairs: 
x′
i⊕y′
xi
) as⟨ 
x′
1⊕y′
x1
⊕ 
x′
2⊕y′
x2
⊕. . .⊕ 
x′
n⊕y′
xn
⟩and an input query xqappended, such
thatxi̸=xq.
The input often includes a description of the task
or a system command Tbefore the demonstration
sequence DP. We now provide a formal definition
of ICL as follows:
Definition 9. In-Context Learning (ICL): A model
Mis said to exhibit ICL if given a prompt P ∼
D(where Dis an unseen demonstration dataset)
and an unseen task T,M: (T ⊕ D P⊕xq)7→
y∗
xq;y∗
xq∈Y∗
xq;where Y∗
xqis the expected set of
output labels for the given query xq.
Mpredicts (i.e., maps) using the prompt’s condi-
tioning only, requiring it to discern essential aspects
such as input-label mapping, input text distribution,
label space, and formatting (lexico-syntactic struc-
tural relationship between the prompt components).
A.4 ICL is latent concept mapping
Xie et al. (2021) suggested that LLMs acquire la-
tent document-level concepts during pretraining to
generate coherent subsequent tokens. ICL occurs
when LLMs identify shared latent concepts among
prompt examples. Min et al. (2022) revealed that
input-label mapping, input-text distribution, label
space, and format in prompts matter more. Wei
et al. (2023) provided supportive evidence that la-
bel association learning becomes more pronounced
with larger model sizes. Although further analysis
needs to be done, we consider the latent-concept
association hypothesis the most compelling expla-
nation of ICL so far and use this as a primary tool
for understanding our own findings on ICPL in
this paper. In the following section, we first define
ICPL (w.r.t personalized summarization) as a spe-
cial case of ICL and then propose iCOPERNICUS as
a framework for evaluating ICPL.
B Model Descriptions
B.1 LLM Model Descriptions
We provide concise descriptions of the LLMs ana-
lyzed in this study for understanding ICPL.
1.Llama 2 - Llama 2 (Touvron et al., 2023) is
a family of transformer-based autoregressivecausal language models, ranging in scale from
7 billion to 70 billion parameters. Llama 2
models are trained on 2 trillion tokens and
have double the context length of Llama 1.
2.Llama 2 Chat - Llama 2 Chat (Touvron et al.,
2023) is a fine-tuned version of Llama 2, op-
timized for dialogue applications using re-
inforcement learning from human feedback
(RLHF). Llama 2 Chat models demonstrate
improved helpfulness and safety compared to
other open models and achieve comparable
performance to ChatGPT according to human
evaluations.
3.Mistral 7B - Mistral 7B (Jiang et al., 2023) is
a language model that outperforms Llama 2
13B and Llama 1 34B in various tasks, such
as natural language inference, mathematics,
and code generation. It leverages grouped-
query attention (GQA), and sliding window
attention (SWA). GQA significantly acceler-
ates the inference speed and also reduces the
memory requirement during decoding.
4.Mistral 7B Instruct - Mistral 7B Instruct
(Jiang et al., 2023) is a fine-tuned version of
Mistral 7B that leverages instruction datasets
to enhance its generalization and adaptation
capabilities. The model has two versions: v0.1
and v0.2. It exhibits superior performance
compared to all 7B models on MT-Bench and
is comparable to 13B – Chat models. Mis-
tral 7B Instruct v0.2 is an updated version
with improvements in instruction following
and generalization capabilities.
5.Tulu V2 Suite - The Tulu V2 Suite (Ivison
et al., 2023) is a collection of fine-tuned large
language models (LLMs) based on Llama 2.
The models in this suite are fine-tuned on a
mix of publicly available, synthetic, and hu-
man datasets. The suite includes Tulu V2
models as well as the DPO fine-tuned Tulu
V2 DPO models.
6.Orca 2 - Orca 2 (Mitra et al., 2023) is a suite
of models that are fine-tuned on Llama 2 using
synthetic dataset. Orca models are designed
to enhance the reasoning abilities of smaller
language models by imitating the step-by-step
reasoning traces of more capable LLMs. Orca
2 models surpass models of similar size andattain performance levels similar to or better
than models five times larger on complex rea-
soning tasks.
7.Stable Beluga - Stable Beluga (Mahan et al.,
2023) is a collection of models that have been
fine-tuned on the Llama 2 using an internal
Orca style dataset. The primary objective
of these models is to generate responses that
are not only responsive to user prompts and
queries, but also emphasize reasoning and
helpfulness.
8.Zephyr 7B - Zephyr 7B (Tunstall et al., 2023)
is a series of language models trained to act as
helpful assistants developed by the Hugging-
Face H4 team. This includes Zephyr 7B αand
Zephyr 7B β. It is a fine-tuned on Mistral 7B
v0.1 and it was trained on on a mix of publicly
available, synthetic datasets using DPO.
B.2 Baseline Model Descriptions
We briefly introduce the SOTA baseline summa-
rization models that were analyzed to understand
their degree-of-personalization below:
1.PENS-NRMS Injection-Type 1 : The PENS
framework (Ao et al., 2021) utilizes NRMS
(Neural News Recommendation with Multi-
Head Self-Attention) (Wu et al., 2019b) for
personalized summary generation. NRMS em-
ploys a news encoder using multi-head self-
attention to understand news titles and learn
user representations based on browsing his-
tory. Additive attention enhances learning
by selecting important words and articles. In
Injection-Type 1, NRMS user embedding is in-
jected into PENS by initializing the decoder’s
hidden state of the headline generator.
2.PENS-NRMS Injection-Type 2 : To gen-
erate a personalized summary, NRMS user
embedding is injected into attention values
(Injection-Type 2) of PENS that helps to per-
sonalize attentive values of words in the news
body.
3.PENS-NAML Injection-Type 1 : NAML
(Neural News Recommendation with Atten-
tive Multi-View Learning) (Wu et al., 2019a)
incorporates a news encoder employing a
multi-view attention model for comprehen-
sive news representations. The user encoderlearns user representations based on interac-
tions with browsed news, allowing the selec-
tion of informative news. In Injection-Type 1,
this user embedding is injected into the PENS
model for personalization.
4.PENS-EBNR Injection-Type 1 : EBNR
(Embedding-based News Recommendation
for Millions of Users) (Okura et al., 2017)
proposes a method for user representations
by using an RNN model that takes browsing
histories as input sequences. This user embed-
ding is injected using Type 1 into the PENS
model for personalization.
5.PENS-EBNR Injection-Type 2 : This person-
alized model injects EBNR user embedding
into PENS using type-2.
6.BRIO : BRIO (Liu et al., 2022) assumes a
non-deterministic training paradigm that as-
signs probability mass to different candidate
summaries according to their quality, thereby
helping it to better distinguish between high-
quality and low-quality summaries.
7.SimCLS : SimCLS (A Simple Framework for
Contrastive Learning of Abstractive Summa-
rization) (Liu and Liu, 2021) uses a two-
stage training procedure. In the first stage,
a Seq2Seq model (BART (Lewis et al., 2020))
is trained to generate candidate summaries
with MLE loss. Then, a RoBERTa-initiated
evaluation model is trained to rank these using
contrastive learning.
8.BigBird-Pegasus : BigBird (Zaheer et al.,
2020) is an extension of Transformer based
models designed specifically for processing
longer sequences. It utilizes sparse, global,
and random attention mechanisms to approxi-
mate full attention which enables it to handle
longer contexts more efficiently.
9.ProphetNet : ProphetNet (Qi et al., 2020)
is a seq2seq pre-trained model that employs
n-gram prediction using the n-stream self-
attention mechanism. It enhances n-step
ahead prediction by predicting the next n to-
kens at once, based on previous tokens, thus
avoiding overfitting on local correlations.
10.T5: T5 (Text-To-Text Transfer Transformer)
is based on the Transformer Encoder-DecoderFigure 2: The statistics of news corpus and training set
of the PENS dataset.
architecture that operates on the principle of
the unified text-to-text task for any NLP prob-
lem, including summarization. See (Tawmo
et al., 2022; Ramesh et al., 2022; Etemad et al.,
2021) for recent T5 summarization analyses.
C PENS Dataset
The PENS dataset is a comprehensive collection of
113,762 news articles, each of which is categorized
into one of 15 distinct topics. Each article in the
dataset includes a unique news ID, a title, a body,
and a category that has been manually tagged by
editors. The average length of a news title is 10.5
words, while the average length of a news body is
549.0 words (Refer to Figure 2 for statistics of news
articles). Entities from each news title are extracted
and subsequently linked to corresponding entities
in WikiData.
For the purpose of training, 500,000 user-news
impressions were sampled from June 13, 2019,
to July 3, 2019. An impression log records the
news articles displayed to a user and the user’s
click behaviors on these articles during a specific
visit to the news website. Each labeled sample
in the training set follows the format [uID, tmp,
clkNews, uclkNews, clkedHis], where ’uID’ repre-
sents the anonymous ID of a user, ’tmp’ denotes thetimestamp of the impression record, ’clkNews’ and
’uclkNews’ are the clicked and un-clicked news in
the impression, respectively, and ’clkedHis’ repre-
sents the news articles previously clicked by the
user. All samples in ’clkNews’, ’uclkNews’, and
’clkedHis’ are sorted by the user’s click time.
C.1 Test Set Construction Process
In order to establish an offline testbed, 103 English
native speakers, all of whom are college students,
were invited to manually create a test set in two
stages as represented in Figure 3.
In the first stage , each participant browses
1,000 news headlines and marks at least 50 pieces
that they find interesting. These selected news arti-
cles were randomly chosen from the news corpus
and were arranged according to their first exposure
time.
In the second stage , participants are asked to
write down their preferred headlines for another
200 unseen news articles from the dataset with-
out being shown the original news titles. They are
also asked to highlight important segments in the
original news articles. These unseen news articles
are evenly sampled, and they are redundantly as-
signed to ensure that each news article is reviewed
by an average of four people. The quality of these
manually-written headlines is checked by profes-
sional editors from the perspective of the factual
aspect of the media frame. Headlines that are of
low quality, such as those containing incorrect fac-
tual information, those inconsistent with the news
body, or those that are too short or too long, are ex-
cluded. The remaining headlines are considered to
be the personalized reading focuses of the annota-
tors on the articles, and are taken as gold-standard
headlines in the PENS dataset.
D Prompt Design Principles
We use six different types of prompting styles, each
tailored to provide varying levels of context and
personalization to the LLMs. We present the struc-
ture of each prompt style in Figure 4, along with the
composition of tokens in the prompts as described
in Table 2, to provide a comprehensive understand-
ing of the composition of prompting techniques
employed in our study. If the length of any portion
of the prompt is greater than the limit, it’s truncated
to fit it in the context length. For prompt examplesFigure 3: Stages of creation of testing dataset consisting of personalized headlines
Column Example Context Description
userid NT1 The unique ID of 103 users
clicknewsID N108480, N38238, N35068, ... The user’s historical clicked news collected at the
first stage
posnewID N24110, N62769, N36186, ... The exhibited news for each user at the second stage
rewrite_titles ’Legal battle looms over Trump
EPA’s rule change of Obama’s
Clean Power Plan rule ...’The manually-written news headlines for the exhib-
ited news articles and can be split by ’#TAB#’
Table 7: Dataset Format of data collected from different users consisting of the articles clicked by the user and
details of articles for which personalized headlines created by the user
refer to Figures 6-10 in Appendix15
1.Zero-shot : In this approach, we provide the
user’s click history followed by the target arti-
cle for which we aim to generate a personal-
ized headline.
2.Contrastive zero-shot : This approach pro-
vides the click history of two users (User1 and
User2), followed by the target article clicked
by both users.
3.2-shot w/o history : In this approach, we
provide two example articles and their cor-
responding headlines given by the user, fol-
lowed by the target article for which we aim
to generate a personalized headline.
4.Contrastive 2-shot w/o history : This method
is similar to the 2-shot approach but involves
providing two example articles and their corre-
sponding headlines given by two users (User1
and User2).
5.2-shot w/ history : In this approach, we pro-
vide the user’s click history, two examples of
articles clicked by the user and their corre-
sponding summaries, along with the article
body that needs to be summarized.
15News article statistics from the sample of 3840 news
articles used in this study – The mean length was 659.91
tokens, the median was 493 tokens, the 90th percentile was
1180 tokens, and the 95th percentile was 1659 tokens.6.Contrastive 2-shot w/ history : This method
involves providing the click history for both
users, two examples of common articles
clicked by both users and their correspond-
ing summaries, along with the article body
that needs to be summarized.
In the study, a substantial number of prompts
were employed for probing, as indicated in Table
2. The generated headlines were extracted from
the produced text using simple regular expression
matching. The output format was explicitly demon-
strated in the examples for 2-shot prompts, and an
example format was provided for 0-shot prompts.
E Analysis of Models w.r.t iCOPERNICUS
E.1 Adversarial Testing: Results
Adversarial testing was conducted for contrastive
prompts across three distinct types of Large Lan-
guage Models (LLMs): Mistral 7B Inst. v0.2 which
exhibits a single paradox; Llama 2 13B Chat that
displays a moderate number of paradoxes; and Tulu
V2 DPO 7B which presents all paradoxes as shown
in Table 4. The details of User-2, such as their
click history and the headlines they wrote in the
contrastive prompt, are not accurate. Instead, the
reading history of another user and the headline of a
random article written by a random user were used.
Table 6 verifies the existence of the paradoxes, as
these models show higher perplexity for three dif-ferent types of contrastive prompts: PX-3, which
is tested by the contrastive zero-shot prompt; PX-4,
which is tested by the contrastive two-shot prompt
without history; and PX-5, which is tested by the
contrastive two-shot prompt with history. These
results indicate that these models are sensitive to
the quality and relevance of the information pro-
vided in the prompts, and that they perform worse
when the prompts contain incorrect details about
the user’s reading history or the headlines written
by them in the examples. Intriguingly, these three
models exhibit a significant performance decline
when the prompts include user’s reading history.
However, they only show a minor performance
drop for contrastive two-shot prompts without his-
tory. This suggests that these models are capable
of discerning that an incorrect/irrelevant headline
is given by User-2 in the examples.
Models C-0-shot C-2-shot w/o hist C-2-shot w/ hist
Llama 2 13B Chat q1 q2 q3
Mistral 7B Inst. v0.2 q4 q5 q6
Tulu V2 DPO 7B q7 q8 q9
Table 8: (Survey) Questionnaire Structure: A respon-
dent fills up the survey for document diin the sequence:
⟨uij, uik⟩ → qi1→qi2→. . .→qi9, where qi•:
⟨suij, suik⟩(Mθ,u,PC)•;suis the summary generated by
each of the 3 models for a specific prompt type (i.e., the
model-contrastive prompt-type pair (Mθ,u,PC)•).
E.2 Human Judgment Validation: Results
Survey Structure: Human judgment-based val-
idation was conducted on a set of contrastive
prompts, systematically evaluating three distinct
LLMs: (i) Mistral 7B Instruct v0.2 (exhibit-
ing only PX-5), (ii) Llama 2 13B Chat (ex-
hibiting a moderate number of paradoxes - PX-
1/2/5), and (iii) Tulu V2 DPO 7B (exhibiting
all the paradoxes), as outlined in Table 4. We
have randomly sampled multiple documents and
three corresponding users (i.e., readers) who
have generated summaries for those documents
eg.(di,(ui1, ui2, ui3))from the PENS dataset.
For each (di,(ui1, ui2, ui3))there are 3 combi-
nations possible ((di,(ui1, ui2)),(di,(ui2, ui3)),
(di,(ui3, ui1))). Each survey respondant is shown
a set of 10 questions (⟨uij, uik⟩, qi1, qi2...qi9)for a
given combination corresponding to a document di
eg.(di,(uij, uik))as shown in the Table 8; where
qi•∈ {qi1, qi2...qi9}contains a pair of the corre-
sponding prompts and model generated summaries
(⟨suij, suik⟩(Mθ,u,PC)•) of user-pairs and ⟨uij, uik⟩
contains a pair of user-generated reference sum-Models PX-5 PX-6
Llama 2 13B Chat ✓ ✓
Mistral 7B Inst. v0.2 ✗ ✓
Tulu V2 DPO 7B ✓ ✓
Table 9: Human-Judgment Validation: PX-5 exists
for worst and medium case; Contrastive PX-1 (PX-6) is
also confirmed (Human-agreement denoted as ✓).
maries of user-pairs.
Survey details: We designed and conducted an
online survey to gather insights from participants
on a voluntary basis (see Figure 5 in Appendix). A
total of 339 responses were obtained, encompass-
ing evaluations of 113 documents from the PENS
evaluation dataset. The respondent pool comprised
262 males and 77 females. Participants repre-
sented diverse educational backgrounds, including
undergraduate and graduate students specializing
in computer science, mathematical science, elec-
tronic engineering, and humanities. To maintain
objectivity, participants were not informed that the
questions presented were summary-pairs. Instead,
each summary pair was displayed as regular text,
prompting participants to rate their similarity on a
scale ranging from 1 (low) to 6 (very high). This
methodology ensured unbiased judgments regard-
ing the proximity of subjective user reference sum-
maries and their corresponding model-generated
summaries.
Computing EGISES-HJ-JSD: The similarity
scores provided by users were utilized as the ba-
sis for computing similarity scores for summary-
summary pairs. For summary-document distance,
we used JSD (since it is not viable for a user to
read the whole document and assign scores to
a summary). Using these distances (summary-
summary distances based on user ratings, summary-
document distances using JSD), we then calculated
EGISES-HJ-JSD for all three models and prompt
types. We observed that EGISES-HJ-JSD scores
based on human judgment showed agreement with
2/3models for PX-5 and 3/3models for PX-6 (the
contrastive version of PX-1) as shown in Table 9.
F Inference Setup and Configuration
For the inference process in our study, we utilized
the JAX library. Our implementation heavily re-
lied on the repository "llama-2-jax" (Ayaka, 2023).
In order to generate outputs from our models, we
configured the inference process to ensure optimalperformance and output quality.
F.1 LLM Settings
We used the following settings to control the be-
havior of the LLMs during inference:
•Temperature: We set the temperature to a
value of 0.6, striking a balance between pre-
dictability and diversity.
•Sampling Method: We adopted the top-k
sampling method with k = 16 for generating
outputs.
•Maximum Length: Maximum length for in-
ference was set to the 4096 tokens.
•Inference Precision: We used bfloat16 preci-
sion for inference, consistent with the preci-
sion in which the model weights were origi-
nally published.
F.2 Compute Platform
For high-performance inference, we utilized TPU
v3-8/v4-8 VMs through Google Cloud Platform.Figure 4: Prompt Templates within the iCOPERNICUS framework : Prompts on the left probe whether models
utilize richer reader profiles; prompts on the right probe whether models utilize contrastive information for real
personalization.Figure 5: Website portal designed for conducting survey for collecting human judgements on the similarity between
user references and model generated summaries for contrastive prompts.Figure 6: Illustration of PX-1 (effect of personalized examples ): The left column shows the output of Llama 2
13B Chat, which hallucinates generating irrelevant information (marked in red) due to the distraction caused by the
examples; the right column shows the output of Mistral 7B Instruct v0.2, which generates the expected response
(marked in green).
Figure 7: Illustration of PX-2 (effect of personalized headline click history ): The left column shows the output
of Stable Beluga 7B, which hallucinates incorrect information (marked in red) due to the inability to reinforce
historical interest on TV-series with the current interest (i.e., query concepts); the right column shows the output of
Mistral 7B Instruct v0.2, which generates the expected response (marked in green).Figure 8: Illustration of PX-3 (effect of contrastive personalized click history ): The left column shows the output
of Stable Beluga 13B, which hallucinates inaccurate information (marked in red) due to the distraction caused by
the list of articles clicked by two users. The right column shows the output of Orca 2 13B, which generates the
expected response (marked in green).
Figure 9: Illustration of PX-4 (effect of contrastive personalized examples ): The left column shows the output of
Mistral 7B Instruct v0.1, which hallucinates inconsistent information (marked in red) due to the distraction resulting
from the unintended reinforcement of common concepts ( adobe ,share ,grow ) in both the contrastive personalized
headline examples provided by different users that overflows to the response of the query document; the right
column shows the output of Orca 2 7B, which generates the expected response (marked in green) and does not
suffer from any such overflow.Figure 10: Illustration of PX-5 (effect of contrastive personalized examples with click history ): The left column
shows the output of Stable Beluga 7B, which hallucinates irrelevant information (marked in red) due to the distraction
caused by the cross-association of concepts in the click history of user-1 with that of user-2; the right column shows
the output of Orca 2 7B, which generates the expected response (marked in green).