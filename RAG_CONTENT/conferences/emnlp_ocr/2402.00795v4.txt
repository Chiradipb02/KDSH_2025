LLMs learn governing principles of dynamical systems, revealing an
in-context neural scaling law
Toni J.B. Liu‚àó, Nicolas Boull√©‚Ä†, Rapha√´l Sarfati‚àó, Christopher J. Earls‚àó
‚àóCornell University, USA,‚Ä†Imperial College London, UK
Correspondence: jl3499@cornell.edu
Abstract
We study LLMs‚Äô ability to extrapolate the be-
havior of various dynamical systems, including
stochastic, chaotic, continuous, and discrete
systems, whose evolution is governed by prin-
ciples of physical interest. Our results show
that LLaMA-2, a language model trained on
text, achieves accurate predictions of dynam-
ical system time series without fine-tuning or
prompt engineering. Moreover, the accuracy
of the learned physical rules increases with the
length of the input context window, revealing
an in-context version of a neural scaling law.
Along the way, we present a flexible and effi-
cient algorithm for extracting probability den-
sity functions of multi-digit numbers directly
from LLMs.1
1 Introduction
Since the introduction of the transformer architec-
ture (Vaswani et al., 2017), Large language models
(LLMs) have shown a variety of unexpected emer-
gent properties, such as program execution (Nye
et al., 2021) and multi-step reasoning (Suzgun et al.,
2022).
Our work explores LLMs‚Äô ability to model the
world by proposing a new perspective and em-
pirical approach. Inspired by the recent observa-
tions that LLMs are capable of in-context time
series extrapolation without specific prompting or
fine-tuning (Gruver et al., 2023; Jin et al., 2023a),
we aim to quantify LLMs‚Äô ability to extrapolate
stochastic dynamical systems. We find that, as the
number of observed time steps increases, an LLM‚Äôs
statistical prediction consistently converges to the
ground truth transition rules underlying the system;
leading to an empirical scaling law, as observed
in Figure 1.
Ourmain contributions are as follows:
1Our code is publicly available at https://github.com/
AntonioLiu97/llmICL .
Figure 1: Evolution of the loss function for the predicted
next state by LLaMA-13b with respect to the number
of observed states in various physical systems. We em-
ploy the Bhattacharyya distance as a loss function for
stochastic systems (solid lines), and the squared devia-
tions from the mean (SDM) for deterministic systems
(dashed lines). Brownian motion and geometric Brown-
ian motion deviate significantly from power law scaling,
which can be explained by their lack of stationary distri-
butions (Appendix A.8).
‚Ä¢demonstrating LLMs‚Äô zero-shot ability to model
the evolution of dynamical systems without in-
struction prompting;
‚Ä¢implementing a computationally efficient frame-
work called Hierarchy-PDF to extract statistical
information of a dynamical system learned by a
transformer-based LLM;
‚Ä¢discovering a scaling law between the accuracy in
the learned transition rules (compared to ground
truth) and the context window length.
2 Background and related work
In-context learning refers to LLM‚Äôs emergent
ability to learn from examples included in the
prompt (Brown et al., 2020). One example of
in-context learning is zero-shot time series fore-
casting (Gruver et al., 2023). This work aims to
forecast empirical time series and introduces a tok-
enization procedure to convert a sequence of float-
1arXiv:2402.00795v4  [cs.LG]  9 Oct 2024ing point numbers into appropriate textual prompts
for LLMs. This led to several subsequent studies
on the application of LLMs for time series forecast-
ing (Chen et al., 2023; Jin et al., 2023b,a; Dooley
et al., 2023; Schoenegger and Park, 2023; Wang
et al., 2023; Xu et al., 2023).
Unlike these prior studies, our work does not
focus on forecasting real-world time series, such
as weather data or electricity demand, where the
underlying model generating the sequence is un-
available or undefined. Instead, we aim to extract
the learned transition rules from the probability
vector generated by the LLM and compare them
against the ground truth rules (chaotic, stochastic,
discrete, continuous, etc.) governing the input time
series.
3 Methodology
Our methodology for testing LLMs‚Äô ability to learn
physical rules from in-context data consists of three
steps:
1.Sample a time series {xt}t‚â•0from a given dy-
namical system governed by Markovian transi-
tion rules Pij.
2.Prompt the LLM with this time series to extract
the learned probability densities for subsequent
digits ÀúPij.
3.Measure the discrepancy, between the ground
truth Pijand learned ÀúPij, using Bhattacharyya
distance.2
3.1 Prompt generation
Markov processes. Most of our testing data may
be modeled as discrete-time Markov chains, where
the probability density function (PDF) of the next
state at time t+ 1depends solely on the previous
state, xt, at time t:
P(Xt+1|x1, . . . , x t) =P(Xt+1|xt).
This models either discrete iterative systems
or continuous dynamical systems after time-
discretization.
Time series tokenization. An input time series
typically consists of ( ‚àº103) time steps, each rep-
resented as a real number. We first rescale each
number and represent it using ndigits (typically,
n= 3). Each time series is rescaled to the inter-
val[1.50,8.50]so that the number of digits never
2Other loss functions may be appropriate depending on
whether the dynamical system is stochastic or deterministic,
see Appendix A.1changes throughout the series. We then follow the
scheme introduced in (Gruver et al., 2023) to seri-
alize the time series as strings and tokenize them.
3.2 Extraction of transition rules
Discrete state space. When the Markov process
is discrete and has a finite state space, each state can
be represented by a single token. We employ tokens
corresponding to the ten number strings: 0, . . . , 9.
We find that even the most sophisticated LLaMA
model (LLaMA-70b) can only learn up to 9 discrete
states. Therefore, we do not attempt to go beyond 9
distinct states by extending to non-number tokens
(see Appendix A.3.3 for more details).
 
generate v !"($!"#|$!) $!"#     
$!"#  
$!  Bhattacharyya distance true transition rules !($!"#|$!)  learned transition rules !"($!"#|$!)    
$!  
$!"#  
Figure 2: Extracting learned transition rules of systems
with discrete state space.
Figure 2 illustrates our framework for learning
discrete Markov chains with LLMs. First, we ran-
domly sample an n√óntransition matrix (Pij). We
then generate a Markov chain according to Pij, to-
kenize the time series and pass to an LLM with
no additional ‚Äúprompt engineering". The length of
the series is chosen such that the tokenized repre-
sentation does not exceed the length of the LLM‚Äôs
context window. We extract the LLM‚Äôs prediction
for the next state by performing a softmax oper-
ation on the output logits corresponding to the n
allowed states and discarding all other logits.
Continuous state space. Many stochastic pro-
cesses, such as the Brownian motion (Einstein,
1905; Perrin, 1909), are supported on continuous
state space. For these processes, we represent the
value of each state as a multi-digit number and sep-
arate each state using the comma symbol ‚Äú,‚Äù. As
observed in (Jin et al., 2023a), an LLM prediction
of multi-digit values can be naturally interpreted
as a hierarchical softmax distribution (Mnih and
Hinton, 2008; Challu et al., 2023).
Specifically, let udenote a multi-digit string rep-
resenting the value of a state at a given time-step,
then the LLM‚Äôs softmax prediction for the ithdigit,
ui, provides a histogram of ten bins of width 0.1i.
Subsequently, the prediction of the (i+ 1)thdigit
2goes down one level into the hierarchical tree by
refining one of the bins into ten finer bins of width
0.1i+1, and so on until the last digit is processed
(see Figure 3). The top right of the figure shows
an example of a time series serialized as an input
string.
 
generate v ‚Ä¶ ùëÉ"(ùë•!"#|ùë•!) ùë•!"#     
ùë•!"#  
ùë•!  Bhattacharyya distance true transition rules ùëÉ(ùë•!"#|ùë•!)  learned transition rules ùëÉ"(ùë•!"#|ùë•!)    
‚Ä¶ 
ùë•!  
ùë•!"#  ‚Ä¶ 
Figure 3: An example of hierarchical transition rules
extracted from LLaMa-13b. The PDF bins are color-
coded based on resolutions, which in this example are
more refined near the mode. The height of ÀúP(xt+1|xt)
is shown in log scale.
Hierarchy-PDF algorithm. While a single pass
through the LLM yields a discretized PDF repre-
sented by bins of various widths, we can refine the
PDF by querying each coarse bin. For example,
to furnish a maximal resolution PDF of a 3-digit
number, we need to query all 102combinations
of the first two digits of that number. Suppose a
time series consists of Svalues (steps), each repre-
sented as ndigits. Obtaining a maximal resolution
PDF for each value of the entire sequence requires
10n‚àí1Sforward passes of the LLM. This daunt-
ing process could be significantly simplified be-
cause most of the 10n‚àí1Sinputs differ only in the
last tokens, and thus one can recursively cache the
key and value matrices associated with the shared
tokens. The computation can be further reduced
by refining only the high-probability bins near the
mode, which dominate the loss functions, as shown
in Figure 3. Algorithm 1 outlines the Hierarchy-
PDF algorithm used to recursively refine the PDF
associated with a multi-digit value in a time series
(more details available in Appendix A).
4 Experiments and Results
This section reports empirical in-context learning
results on two example systems: a discrete Markov
chain and a stochastic logistic map. We defer dis-
cussion of other systems, reported in Figure 1, to
Appendices A.3 and A.4. Each experiments is re-
peated ten times with trajectories initiated by dif-
ferent random seeds.
Model choice. All numerical experiments re-
ported in this section are performed using the open-Algorithm 1 Hierarchy-PDF
Input: Unrefined PDF, current depth Dc, target
depth Dt
Procedure: RecursiveRefiner(PDF, Dc,Dt)
ifDc=Dtthen
end the recursion
else if current branch is refined then
Alter the last digit to launch 9 recursive
branches
RecursiveRefiner(PDF, Dc,Dt)
else if current branch is unrefined then
refine PDF with new logits
ifDc+ 1< D tthen
Append the last digit to launch 10 recur-
sive branches
RecursiveRefiner(PDF, Dc+ 1,Dt)
end if
end if
Output: Refined PDF
source LLaMA-13b model. While we observe that
larger language models, such as LLaMA-70b, may
achieve lower in-context loss on some dynamical
systems (Appendix A.3.3), they do not display qual-
itative differences that would affect our conclu-
sions.
4.1 Markov chains with discrete states
The transition rules of a time-independent Markov
chain with nstates consist of a stochastic matrix
(Pij)1‚â§i,j‚â§n, defined as
Pij=P(Xt+1=j|Xt=i),1‚â§i, j‚â§n.
Using the testing procedures elaborated on in
Section 3.2, we generate 10 Markov chains, each
from a distinct and randomly generated transition
matrix of size n= 4.
Figure 4: Markov chain in-context loss curves decay
rapidly with respect to the input time series length. The
average loss is obtained from 10 individual loss curves.
The corresponding loss curves between the LLM
predictions and the ground truth are displayed in
Figure 4. The LLM formulates remarkably accu-
3rate statistical predictions as more time steps are ob-
served in context, even though the transition rules
are synthesized completely at random. These con-
clusions hold for larger transition matrices ( n >4)
and more sophisticated LLMs, such as LLaMA-
70b (see Appendix A.3.3).
4.2 Noisy logistic map
The logistic map, first proposed as a discrete-time
model for population growth, is one of the sim-
plest dynamical systems that manifest chaotic be-
haviors (Strogatz, 2015). It is governed by the
following iterative equation:
xt+1=f(xt) =rxt(1‚àíxt), x 0‚àà(0,1),
where r‚àà[1,4)is a parameter that controls the
system‚Äôs behavior.3The logistic map system be-
comes stochastic when one introduces small Gaus-
sian perturbations of variance at each step, resulting
in modified iterative equation:
xt+1=f(xt+œµ),
where œµi.i.d‚àº N (0, œÉ2). In this case, the ground
truth distribution of the next state, xt+1, condi-
tioned on the current state xtis Gaussian with mean
f(xt)and variance (œÉf‚Ä≤(xt))2:
Xt+1|{Xt=xt} ‚àº N 
f(xt),(œÉf‚Ä≤(xt))2
.(1)
The first derivative of fmeasures how sensitive the
local dynamics are to external perturbations. This
intuitively explains why the standard deviation of
the conditional distribution should be proportional
tof‚Ä≤.4We visualize the remarkable accuracy of
the LLM‚Äôs in-context predictions compared to the
ground-truth transition rules in Figure 5 as well as
Figure 18 in the appendix.
We again observe a power-law-like decay of the
in-context loss function with respect to the length
of the observed time series in Figure 6. We note that
in order to achieve low in-context loss, the LLM
must learn to predict not only the mean, but also the
variance of next-state distributions. This is shown
in Figure 7 and discussed further in Appendix A.9.
3In our experiments, we set r= 3.9, which places the
system firmly in the chaotic regime. At this value, the logistic
map exhibits sensitive dependence on initial conditions and
aperiodic behavior characteristic of chaos.
4Note that the approximation in Equation (1) assumes a
small perturbation compared to the second derivative. That is,
œÉ2‚â™1/f‚Ä≤‚Ä≤(x).
Figure 5: LLM‚Äôs prediction for the 980thstate of a
noisy logistic map, conditioned on observing the first
979states in-context, compared with the ground-truth
transition probability function.
Figure 6: Stochastic logistic map in-context loss curves.
Figure 7: Noisy logistic map standard deviation as a
function of the state value xt, learned by the LLM, along
with the ground truth.
5 Discussion and conclusion
Main observations. We showed that, with suf-
ficient context, LLMs can accurately recover the
probablistic transition rules underlying determinis-
tic, chaotic, and stochastic time series.
In-context neural scaling law. Neural scaling
laws (Kaplan et al., 2020) are power laws that char-
acterize how the loss of trained neural networks
vary with respect to parameters of the model, such
as model size, dataset size, and computational re-
sources. To the best of our knowledge, neural scal-
ing laws have so far only been observed in the train-
ing procedure, which updates the weights of neu-
ral networks using an explicit algorithm, such as
stochastic gradient descent and Adam (Kingma and
Ba, 2014). The loss curves observed in the different
numerical experiments (see Figures 1 and 20) re-
veal an additional in-context scaling law for LLMs‚Äô
zero-shot learning of dynamical systems. Further
analysis of these scaling laws are presented in Ap-
pendix A.6.
4Figure 8: Loss curves of LLM in-context learning against baseline models: unigram and bi-gram models for discrete
Markov chains, and linear and non-linear autoregressive models with 1-step memory (AR1) for noisy logistic maps.
The coefficient Œ±denotes the fitted scaling exponent.
Mechanistic interpretations of in-context learn-
ing. Characterizing the in-context learning algo-
rithms implemented by an LLM is an open question
of broad interest (Shen et al., 2023; Olsson et al.,
2022). Recent years have seen a new paradigm that
explains certain aspects of in-context learning as
transformers ‚Äúhosting" classical algorithms such as
gradient descent (V on Oswald et al., 2023). Our
study contributes to this field by presenting novel
phenomenological evidence: we demonstrate that
pretrained LLMs can in-context learn dynamical
systems, exhibiting loss curves similar to those
of established learning algorithms, such as the bi-
gram Markov model and neural network-based au-
toregressive (AR1) models trained using gradient
descent (Figure 8). We elaborate on the motivation
and design of these baseline models in Appendix
A.7.
Future directions. A compelling future direction
is to develop a mechanistic theory that explains
how pretrained LLMs in-context learn and predict
stochastic dynamical time series. Further investiga-
tions could focus on:
‚Ä¢Investigating how LLMs encode spatio-
temporal features in stochastic dynamical time
series (Gurnee and Tegmark, 2023), and how
they represent the learned transition rules.
‚Ä¢ Examining how LLMs perform subtasks nec-
essary for learning stochastic dynamical sys-
tems, such as collecting Markovian transition
statistics from discrete data (Aky√ºrek et al.,
2024) and performing density estimation.
‚Ä¢Assessing to what extent training on natural
language enhances a transformer‚Äôs mathemat-ical ability (Zhang et al., 2023) to in-context
learn dynamical systems, and vice-versa.
Limitations
Data Leakage. Although the accurate predic-
tions made by LLMs of the next time step value
are highly unlikely to be due to memorization, it
is essential to address this possibility thoroughly.
Given the vast number of potential sequences even
with a thousand numerical values encoded to three
digits (resulting in 103000instance), this far ex-
ceeds the approximate 1012tokens in the training
corpus (Touvron et al., 2023). Future work should
ensure rigorous testing to further rule out data leak-
age.
Model selection. At the time of this manuscript‚Äôs
preparation, newer models, such as LLaMA-3,
have been released. Due to constraints in time and
computational resources, this work does not eval-
uate these latest models. Future research should
include these and other emerging models to pro-
vide a more comprehensive understanding of the
phenomena observed.
Acknowledgements
This work was supported by the SciAI Center,
and funded by the Office of Naval Research
(ONR), under Grant Numbers N00014-23-1-2729
and N00014-23-1-2716.
5References
Ekin Aky√ºrek, Bailin Wang, Yoon Kim, and Jacob An-
dreas. 2024. In-context language learning: Architec-
tures and algorithms. Preprint , arXiv:2401.12973.
Anil Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distribution. Bull. Calcutta Math. Soc. ,
35:99‚Äì110.
Anil Bhattacharyya. 1946. On a measure of divergence
between two multinomial populations. Sankhya: In-
dian J. Stat. , pages 401‚Äì406.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. In Advances in Neural Information Process-
ing Systems , volume 33, pages 1877‚Äì1901.
Cristian Challu, Kin G Olivares, Boris N Oreshkin, Fed-
erico Garza Ramirez, Max Mergenthaler Canseco,
and Artur Dubrawski. 2023. Nhits: Neural hierar-
chical interpolation for time series forecasting. In
Proceedings of the AAAI Conference on Artificial
Intelligence , volume 37, pages 6989‚Äì6997.
Yakun Chen, Xianzhi Wang, and Guandong Xu. 2023.
GATGPT: A Pre-trained Large Language Model with
Graph Attention Network for Spatiotemporal Impu-
tation. arXiv preprint arXiv:2311.14332 .
Euisun Choi and Chulhee Lee. 2003. Feature extrac-
tion based on the Bhattacharyya distance. Pattern
Recognit. , 36(8):1703‚Äì1709.
Edwin L Crow and Kunio Shimizu. 1987. Lognormal
distributions . Marcel Dekker New York.
Samuel Dooley, Gurnoor Singh Khurana, Chirag Mo-
hapatra, Siddartha Naidu, and Colin White. 2023.
ForecastPFN: Synthetically-Trained Zero-Shot Fore-
casting. arXiv preprint arXiv:2311.01933 .
Albert Einstein. 1905. √úber die von der moleku-
larkinetischen Theorie der W√§rme geforderte Bewe-
gung von in ruhenden Fl√ºssigkeiten suspendierten
Teilchen. Ann. Phys. , 4.
Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gor-
don Wilson. 2023. Large language models are
zero-shot time series forecasters. arXiv preprint
arXiv:2310.07820 .
Wes Gurnee and Max Tegmark. 2023. Language
models represent space and time. arXiv preprint
arXiv:2310.02207 .
John C Hull. 2021. Options, futures, and other deriva-
tives, 11th edition. Pearson.
Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu,
James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan
Liang, Yuan-Fang Li, Shirui Pan, et al. 2023a. Time-
llm: Time series forecasting by reprogramming large
language models. arXiv preprint arXiv:2310.01728 .Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang,
Siqiao Xue, Xue Wang, James Zhang, Yi Wang,
Haifeng Chen, Xiaoli Li, et al. 2023b. Large models
for time series and spatio-temporal data: A survey
and outlook. arXiv preprint arXiv:2310.10196 .
Derrick N Joanes and Christine A Gill. 1998. Compar-
ing measures of sample skewness and kurtosis. J.
Roy. Stat. Soc. D-Sta. , 47(1):183‚Äì189.
Thomas Kailath. 1967. The divergence and Bhat-
tacharyya distance measures in signal selection.
IEEE Trans. Commun. Technol. , 15(1):52‚Äì60.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361 .
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Kazuhiko Kobayashi and Moin Us Salam. 2000. Com-
paring simulated and measured values using mean
squared deviation and its components. Agron. J. ,
92(2):345‚Äì352.
Edward N Lorenz. 1963. Deterministic nonperiodic
flow. J. Atmos. Sci. , 20(2):130‚Äì141.
Yucheng Lu, Wentao Guo, and Christopher M De Sa.
2022. GraB: Finding provably better data permuta-
tions than random reshuffling. In Advances in Neural
Information Processing Systems , volume 35, pages
8969‚Äì8981.
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems ,
volume 21.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, et al. 2021. Show your work: Scratch-
pads for intermediate computation with language
models. arXiv preprint arXiv:2112.00114 .
Bernt Oksendal. 2013. Stochastic differential equations:
an introduction with applications . Springer Science
& Business Media.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-
erly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,
Danny Hernandez, Scott Johnston, Andy Jones,
Jackson Kernion, Liane Lovitt, Kamal Ndousse,
Dario Amodei, Tom Brown, Jack Clark, Jared Ka-
plan, Sam McCandlish, and Chris Olah. 2022. In-
context learning and induction heads. Preprint ,
arXiv:2209.11895.
Jean Perrin. 1909. Mouvement brownien et r√©alit√©
mol√©culaire. Annal. Chim. Phys. , 18:1‚Äì114.
6Eckhard Platen. 1999. An introduction to numerical
methods for stochastic differential equations. Acta
Numer. , 8:197‚Äì246.
Daniel Revuz and Marc Yor. 2013. Continuous martin-
gales and Brownian motion , volume 293. Springer
Science & Business Media.
Philipp Schoenegger and Peter S Park. 2023. Large
language model prediction capabilities: Evidence
from a real-world forecasting tournament. arXiv
preprint arXiv:2310.13014 .
James P. Sethna. 2021. Statistical mechanics: Entropy,
order parameters, and complexity . Oxford University
Press.
Lingfeng Shen, Aayush Mishra, and Daniel Khashabi.
2023. Do pretrained transformers really learn
in-context by gradient descent? arXiv preprint
arXiv:2310.08540 .
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya
Ganguli, and Ari S. Morcos. 2023. Beyond neural
scaling laws: beating power law scaling via data
pruning. Preprint , arXiv:2206.14486.
Steven H Strogatz. 2015. Nonlinear dynamics and
chaos: With applications to physics, biology, chem-
istry, and engineering , 2nd edition. CRC Press.
Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
Zhou, et al. 2022. Challenging big-bench tasks and
whether chain-of-thought can solve them. arXiv
preprint arXiv:2210.09261 .
Floris Takens. 2006. Detecting strange attractors in
turbulence. In Dynamical Systems and Turbulence,
Warwick 1980 , pages 366‚Äì381. Springer.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
Baptiste Rozi√®re, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30.
Johannes V on Oswald, Eyvind Niklasson, Ettore Ran-
dazzo, Jo√£o Sacramento, Alexander Mordvintsev, An-
drey Zhmoginov, and Max Vladymyrov. 2023. Trans-
formers learn in-context by gradient descent. In In-
ternational Conference on Machine Learning , pages
35151‚Äì35174.
Junxiang Wang, Guangji Bai, Wei Cheng, Zhengzhang
Chen, Liang Zhao, and Haifeng Chen. 2023.
Prompt-based domain discrimination for multi-
source time series domain adaptation. arXiv preprint
arXiv:2312.12276 .Jingjing Xu, Caesar Wu, Yuan-Fang Li, and Pascal
Bouvry. 2023. Transformer multivariate forecasting:
Less is more? arXiv preprint arXiv:2401.00230 .
Yi Zhang, Arturs Backurs, S√©bastien Bubeck, Ronen
Eldan, Suriya Gunasekar, and Tal Wagner. 2023. Un-
veiling transformers with lego: a synthetic reasoning
task. Preprint , arXiv:2206.04301.
7A Appendix
A.1 Loss Functions
Once the learned transition rules, ÀúP(Xt+1|Xt), have been extracted, we quantify the deviation from the
ground truth P(Xt+1|Xt). Depending on the nature of the system, one of the following two loss functions
may be more appropriate (see Section 4).
Bhattacharyya distance. For stochastic time series, we use the Bhattacharyya distance to characterize
the distance between learned and ground truth transition functions. The Bhattacharyya distance between
PandÀúP, on a domain Xis defined as (Bhattacharyya, 1943, 1946; Kailath, 1967):
DB(P,ÀúP) =‚àílnZ
Xp
p(x)Àúp(x)dx
, (2)
and has been widely employed by feature selection and signal extraction methods (Choi and Lee, 2003;
Kailath, 1967). Since ÀúP(Xt+1|Xt)takes the form of a hierarchical PDF, one may approximate the integral
in Equation (2) via a discrete quadrature rule as
DB(P,ÀúP) =‚àíln X
xp
p(x)Àúp(x)‚àÜx!
, (3)
where ‚àÜxdenotes the length of the sub-interval containing xin the partition of X.
Squared deviations from the mean. For deterministic systems, the true transition functions become
delta-functions. As a result, the discretized Bhattacharyya distance from Equation (3) reduces to (see
Equation (5) in Appendix A.2)
DB(Œ¥(x‚àíxtrue),ÀúP) =‚àí1
2ln(Àúp(xtrue)) +C,
which is proportional to the negative log-likelihood (NLL) assigned to the true data by the LLM, plus
a constant C5. NLL references only the finest bins in the hierarchical PDF and is thus unstable as an
in-context loss. As an alternative, we use the squared deviations from the mean (SDM) (Kobayashi and
Salam, 2000) as the in-context loss for deterministic systems:
SDM( xtrue,ÀúP) = 
xtrue‚àíX
x‚ààXÀúp(x)x‚àÜx!2
,
where the mean ¬µÀúP=P
xÀúp(x)x‚àÜxis extracted from the hierarchical PDF. Note that unlike the
Bhattacharyya distance, which references the model prediction Àúponly at xtrue, the SDM takes into
account the entire support x‚àà X. Our numerical experiments suggest that SDM is more stable and better
captures the in-context learning dynamics of deterministic systems (see Appendix A.4).
A.2 Additional loss functions
KL-divergence. The KL-divergence between two PDFs, PandÀúP, is defined as
DKL(P,ÀúP) =X
x‚ààXP(x) logP(x)
ÀúP(x)
. (4)
Although commonly used as the training loss for a variety of machine learning systems, this loss function
may suffer from numerical instabilities as the learned transition function ÀúPare often close to zero, as
shown in Figures 10 and 15, where the probability density is concentrated in small regions of the support.
5This constant is determined by the base B of the system, and the number of digits n as C=‚àíln ‚àÜx=nlogB.
8Discretized Bhattacharyya distance for deterministic systems. For deterministic systems, the ground
truth transition function is a delta function. Therefore, the Bhattacharyya distance between it and the
Hierarchy-PDF prediction only references the finest bin associated with the true value xtrue.
DB(Œ¥(x‚àíxtrue),ÀúP) =‚àíln X
xp
Œ¥(x‚àíxtrue)Àúp(x)‚àÜx!
=‚àí1
2ln(Àúp(xtrue))‚àíln ‚àÜx
=‚àí1
2ln(Àúp(xtrue)) + constant .(5)
As a result, the Bhattacharyya distance is reduced to an affine-transformed negative log-likelihood
assigned to data by the LLM. Such local sensitivity on Àúp(xtrue)explains the wild fluctuations seen in the
Bhattacharyya loss in Appendix A.4.
Higher moments and kurtosis. While the Bhattacharyya distance and SDM measure the agreement
between the extracted transition rules ÀúPand the ground truth distribution P, they do not explicitly
characterize the type of the distribution (e.g., Gaussian or uniform). We employ the kurtosis as an
additional measure to assess whether the LLM recovers the correct shape of P. The kurtosis of a
distribution Pis defined as (Joanes and Gill, 1998)
Kurt(P) =Ex‚àºP[(x‚àí¬µP)4]
Ex‚àºP[(x‚àí¬µP)2]2=¬µ4
(œÉ2)2, (6)
where œÉ2and¬µ4are the second and fourth central moments, which can be approximated using a
hierarchical PDF as
œÉ2(P) =X
xp(x)(x‚àí¬µp)2‚àÜx, ¬µ 4(P) =X
xp(x)(x‚àí¬µp)4‚àÜx. (7)
The kurtosis is equal to 3for a Gaussian distribution and 9/5for bounded uniform distributions. Figure 9
shows the kurtosis of Brownian motion transition rules learned by LLM, which converges to 3as the
context length increases.
Figure 9: Kurtosis of Brownian motion transition rules with respect to the input length. Blue: kurtosis of LLM
predicted PDF. Red: ground truth kurtosis, which is 3for all Gaussian distributions.
A.3 Additional Experiments: stochastic time series
A.3.1 Brownian motion
Brownian motion is an example of a continuous-time stochastic process (Einstein, 1905), and is described
by a stochastic differential equation (SDE):
dXt=¬µdt+œÉdW t, (8)
9Figure 10: Next state prediction of Brownian motion. Top: Input stochastic time series shown in black, and the state
to be predicted is highlighted in red. Bottom: The LLM‚Äôs prediction, along with the ground truth distribution.
where Xtrepresents the state of the system at time t,¬µis the drift coefficient, œÉis the volatility coefficient,
anddWtis the increments of a Wiener process (Revuz and Yor, 2013), modeling the randomness of
motion.
To simulate trajectories of Brownian motion, we use the Euler‚ÄìMaruyama method (Platen, 1999),
which discretizes Equation (8) as Xt+‚àÜt=Xt+¬µ‚àÜt+œÉ‚àö
‚àÜtZ, where ‚àÜtis the time resolution, and
Z‚àº N(0,1)is a random variable that follows a standard Gaussian distribution. The Euler‚ÄìMaruyama
method may also be written as a conditional distribution:
Xt+‚àÜt|{Xt=xt} ‚àº N (xt+¬µ‚àÜt, œÉ2‚àÜt),
which is the ground truth transition function visualized in Figure 10. Indeed, the ground truth next state
is described as a Gaussian distribution, and we observe in Figure 10 that the LLM prediction agrees
well with the true, underlying distribution. Additionally, as shown in Figure 10, the LLM displays the
correct Gaussian shape for the PDF, converging to a measured kurtosis of 3(see Appendix A.2). We then
simulate ten different trajectories using random seeds for Zand report the resulting LLM learning curves
in Figure 11, measured in the Bhattacharyya distance.
Figure 11: Bhattacharyya distance between the LLM predicted PDF and the ground truth transition function of
Brownian motion with respect to the input length.
A.3.2 Geometric Brownian motion
Geometric Brownian motion (GBM) (Oksendal, 2013) is a stochastic process that is commonly used in
mathematical finance to model the trajectories of stock prices and other financial assets (Hull, 2021). A
GBM is governed by the following SDE:
dXt=¬µXtdt+œÉXtdWt, (9)
where Xtmodels the price of an asset at time t, and the fluctuation term œÉXtdWtis proportional to the
current asset price Xt. The Euler‚ÄìMaruyama discretization of the GBM reads Xt+‚àÜt=Xt+¬µXt‚àÜt+
œÉXt‚àö
‚àÜtZ, and leads to the ground truth transition function:
Xt+‚àÜt|{Xt=xt} ‚àº N (xt+¬µxt‚àÜt,(œÉxt)2‚àÜt). (10)
10We simulate ten different GBM trajectories using random seeds and report the corresponding learning
curves in Figure 12.
Figure 12: Geometric Brownian motion in-context loss curve.
We perform an additional numerical test to verify that the LLM is learning the correct relationship
between the variance of the GBM and the state value Xt(see Equation (10)). To investigate this, we
display in Figure 13 the expected standard deviation along with the learned one, extracted from the
Hierarchy-PDF using Equation (7), across all predicted states. We find that the LLM respects the ground
truth standard deviation of the GBM, as prescribed by the underlying transition function.
Figure 13: Evolution of the geometric Brownian motion standard deviation with respect to the state value xt(see
Equation (10)), along with the predicted standard deviation extracted from the LLM at each time step.
A.3.3 Markov chains with LLaMA-70b
Our experiments show that LLMs generally achieve lower in-context loss for Markov chains with fewer
discrete states n, as shown in Figure 14. For both LLaMA-13b and LLaMA-70b, the in-context loss
curves cease to decrease significantly for numbers of states n‚â•9.
Figure 14: In-context loss curves for LLaMA-13b (left) and LLaMA-70b (right) with respect to the number of states
in the transition matrix.
A.4 Additional Experiments: deterministic time series
A.4.1 Logistic map
The logistic map, first proposed as a discrete-time model for population growth, is one of the simplest
dynamical systems that manifest chaotic behavior (Strogatz, 2015). It is governed by the following
11iterative equation:
xt+1=f(xt) =rxt(1‚àíxt), x 0‚àà(0,1), (11)
which may also be written using conditional distributions to reflect the deterministic nature of the system
asXt+1|{Xt=xt} ‚àºŒ¥f(xt), where Œ¥denotes the Dirac delta distribution. This conditional distribution
is the ground truth transition function displayed in red in Figure 15. The parameter r‚àà[1,4)controls the
behavior of the system and is set to r= 3.9. At this value, the dynamics are naturally confined within
the interval (0,1), and the system has no stable fixed points. Due to the chaotic nature of the system,
two initial nearby trajectories diverge exponentially in time. This property allows us to sample multiple
uncorrelated trajectories by using different initial conditions, x0, sampled uniformly in (0,1).
Figure 15: Next state prediction of the logistic map. Top: Input chaotic time series shown in black, and the state to
be predicted is highlighted in red. Bottom: The LLM‚Äôs statistical prediction for the last state. The ground truth
distribution is delta-distributed, which is shown as a vertical red line.
Figure 15 displays one of the ten tested trajectories and an LLM‚Äôs prediction of the last state. The PDF
of the next state prediction is extracted using the Hierarchy-PDF algorithm described in Section 3.2. We
find that the LLM prediction is close to the ground truth, except for minor deviations manifested by small,
but non-zero, probability densities in neighboring values. While the extracted prediction is only reported
for the last time step in the bottom panel of Figure 15, we also extract the model prediction at every time
step for all tested trajectories and report the corresponding Bhattacharyya and SDM losses in Figure 16.
Figure 16: Logistic map in-context loss curves. For deterministic systems, Bhattacharyya loss is subject to large
fluctuations while SDM loss is more stable.
As foreshadowed in Section 3, the Bhattacharyya loss suffers from large fluctuations with deterministic
systems such as the logistic map, while the SDM loss better captures the in-context learning dynamics. In
particular, the SDM loss decreases rapidly with the number of observed states, without any fine-tuning
nor prompt engineering of the LLM. This suggests that the LLM can extract the underlying transition
rules of the logistic map from in-context data.
A.4.2 Lorenz system
The Lorenz system (Lorenz, 1963) is a three-dimensional (3D) dynamical system derived from a simplified
model of convection rolls in the atmosphere. It consists of a system of three ordinary differential equations:
Àôx(t) =œÉ(y‚àíx),Àôy(t) =x(œÅ‚àíz)‚àíy,Àôz(t) =xy‚àíŒ≤z,
12where œÉ= 10 ,œÅ= 28 andŒ≤= 8/3are parameters dictating the chaotic behavior of the system. We
compute ten 3D trajectories using a first-order explicit time-stepping scheme. All trajectories share the
same initial conditions in yandz, and differ only in the x-coordinate, which is uniformly sampled in
(0,0.3). The chaotic nature of the system guarantees that the sampled trajectories quickly diverge from
one another. We prompt the LLM with the x-component of the simulated series and extract the next
predicting values.
Figure 17: Loss curves for predicting the x-component of the Lorenz system with respect to the number of observed
time steps.
When the x,y, and zcomponents are observed, the system is deterministic and Markovian; in the
sense that a state vector ‚Éó st= (xt, yt, zt)at time tfully determines the next state ‚Éó st+1. However, if the
x-component is the only one observed, then the system ceases to be Markovian but remains deterministic
if one expands the state vector to include information from earlier states. Hence, Takens‚Äô embedding
theorem (Takens, 2006) guarantees that the observation of at most seven states of the series x0:tis sufficient
to predict xt+1. Finding the optimal number of states to reconstruct the system‚Äôs trajectory is an area of
active research (Strogatz, 2015). Despite this apparent difficulty, LLaMA-13b can formulate increasingly
accurate predictions of the series as it observes more states, as evidenced by the decaying loss curves
plotted in Figure 17.
A.5 Continuous State Space Visualization
One may naively remark upon the possibility that the in-context learning task for the Lorenz system
and the logistic map could be rendered trivial if xt+1always falls close to xt, in which case the LLM
only needs to learn a static noisy distribution in order to decrease the loss. This is not the case with our
experiments. In this section, we demonstrate the non-triviality of the learning tasks in Figures 18 and 19.
In both cases, it is clear that the LLM has successfully learned to actively predict the expected mean
position of the next state, and, in the logistic map example, the variance of the next state distribution as
well. We note that the Lorenz system is simulated deterministically, hence the true next-state distribution
is represented as a delta-function.
A.6 In-context neural scaling law
Neural scaling laws (Kaplan et al., 2020) describe how the performance of trained neural networks,
particularly language models, scales with changes in key factors such as model size ( N), dataset size
(D), and computational resources used for training (C). These laws are often observed as power-law
relationships in the following form:
L(N) =N
NcŒ±N
, L(D) =D
DcŒ±D
, L(C) =D
CcŒ±C
,
where Lrepresents the loss or performance metric of the model. The characteristic factors ( Nc,Dc, and
Cc), and power coefficient ( Œ±) are extracted empirically from training curves. The fitted quantities depend
on the distribution of data, the model architecture, and the type of optimizer used for training. Such
power-law relations appear in log-log plots as straight lines, whose slopes correspond to the parameter Œ±.
Our loss curves from learning dynamical systems (see Figure 1) reveal an additional neural scaling law
13Figure 18: 4 consecutive states in a noisy logistic map.
Ground truth is shown in red and LLaMA predictions
in blue.
Figure 19: 4 consecutive states in a Lorenz system
trajectory. Ground truth is shown in red and LLaMA
predictions in blue.
14that applies to in-context learning:
L(Din) =Din
DcŒ±
,
where Dinstands for the length of time series observed in the prompt (in-context). In Figure 20, we
display the fitted power laws to the in-context loss curves.
Figure 20: In-context loss curves from LLaMA-13b fitted with power law, with fitted power coefficient Œ±shown
in legend. Left: loss of stochastic series measured in Bhattacharyya distance. Right: loss of deterministic series
measured in SDM.
A.7 Baselines for noisy logistic map and Markov chains
In this section, we compare LLM‚Äôs predictions against baseline models of known architectures, in order
to understand the difficulty of the in-context learning task and make better sense of the Bhattacharyya
loss. Specifically, we consider the following baseline models: unigram and bi-gram models for discrete
Markov chains, and linear and non-linear autoregressive models with 1-step memory (AR1) for noisy
logistic maps. The bi-gram model for the Markov chain has an unfair advantage since it is designed to
model Markovian processes where the probability distribution of a token depends only on the previous
token, i.e., inferring the values of the transition matrix. The unigram model, on the other hand, models all
tokens as drawn i.i.d. from the same distribution.
Figure 21: LLM in-context loss curves against the baseline model loss curves. The coefficient Œ±denotes the fitted
scaling exponent.
The neural network AR1 model takes a state xt‚àí1as input, and outputs prediction for next state xtas
a Gaussian distribution parameterized by mean and variance: fŒ∏:xt‚Üí N (¬µŒ∏(xt), œÉŒ∏(xt)). As such,
it also has the unfair advantage of hard-coded Gaussianiety. LLaMA, on the other hand, must infer the
correct distribution family from data. Despite this intrinsic disadvantage, LLaMA still outperforms the
neural network AR1 model in the large context limit. The NN used in the non-linear AR1 models features
15three fully connected hidden layers of widths 64, 32, and 16. We found that simpler neural networks are
easily trapped in local minima, leading to unstable performance. The loss curves in Figure 21 are obtained
by training an independent copy of this neural network to convergence at each context length, and predict
the next state distribution using the trained NN. The training loss is defined as the negative log-likelihood
of the observation data:
L(data, Œ∏) =‚àíX
xt‚àí1,xt‚ààdatalogP(xt;¬µŒ∏(xt‚àí1), œÉŒ∏(xt‚àí1))
=X
xt‚àí1,xt‚ààdatalogœÉŒ∏(xt‚àí1)‚àí1
2xt‚àí¬µŒ∏(xt‚àí1)
œÉŒ∏(xt‚àí1)2
.
Furthermore, the ensemble of NNs allows us to visualize the learned transition functions P(xt+1|xt)
at each context length. In Figure 22, we show how the transition rules learned by the NNs gradually
converge to the ground truth as context length increases. Since at large context length, the LLMs achieve
similar loss as the NN-based AR1 model, it is reasonable to expect the LLM to have learned a transition
function of similar accuracy as shown in the 5th plot in Figure 22. However, it is difficult to visualize the
full transition rules P(xt+1|xt), forxt‚àà[0,1], as learned by an LLM, because doing so would require
appending an array of xts at the end of a training sequence, which would render the training sequence
incorrect.
Figure 22: Noisy logistic map transition rules, P(xt|xt‚àí1), learned by a neural network-based AR1 model against
the ground truth transition rule.
A.8 Invariant measure and the early plateauing of in-context loss
While most datasets are well-described by the power laws, two loss curves ‚Äî the Brownian motion and
geometric Brownian motion ‚Äî plateau early at a context length of about 102, as shown in Figure 20. We
attribute this early plateauing to the fact that the Brownian and geometric Brownian motions ‚Äúwander out
of distribution" at large time t, while all other dynamical systems studied in this paper converge to stable
distributions (i.e., the invariant measure). A Markovian system (stochastic or deterministic) governed by a
transition rule P(xt+1|xt)is said to have an invariant measure œÄif
œÄ(xt+1) =Z
XœÄ(xt)P(xt+1|xt)dxt, x t+1‚àà X. (12)
16If a system is initialized by œÄ(x)and evolves according to P, then the distribution of states at the next
step will still follow œÄ(x). This property makes œÄan invariant or stationary distribution for the system. It
has been shown that the logistic map and Lorenz systems in the chaotic regime converge almost surely to
their respective invariant measures, regardless of the initialization (Strogatz, 2015).
For discrete Markov chains governed by a transition matrix, p, the stationary distribution is defined as a
discrete probability mass function, denoted by ‚Éó œÄ, such that
‚Éó œÄ=p‚Éó œÄ, (13)
which is analogous to the continuous case described by Equation (12). By definition, any non-negative
right eigenvector of pwith eigenvalue Œª= 1is a stationary distribution of p. (Sethna, 2021) showed that a
valid transition matrix has at least one stationary distribution. On the other hand, neither the Brownian nor
the geometric Brownian motion has invariant distributions6on unbounded domains (e.g., when X=R).
This can be seen from the marginalized distribution P(xt)at time t. For the Brownian motion defined in
Equation (8), the marginalized distribution of xtat time tis a normal distribution:
P(xt) =1‚àö
2œÄœÉ2texp
‚àí(xt‚àí¬µt)2
2œÉ2t
, (14)
while for the geometric Brownian motion defined in Equation (9), the marginalized distribution of xtis a
log-normal distribution (Crow and Shimizu, 1987):
P(xt) =1
xt‚àö
2œÄœÉ2texp 
‚àí(log(xt
x0)‚àí¬µt‚àíœÉ2
2t)2
2œÉ2t!
. (15)
Both Equations (14) and (15) are time-dependent and do not converge to a stationary distribution in
the limit t‚Üí ‚àû . For the Brownian and geometric Brownian motions, the LLM might decide to only
consider the most recent segment of time steps, and ignore the earlier data, which are in some sense ‚Äúout
of distribution‚Äù. This could explain the early plateauing of loss curves. Indeed, the classical neural scaling
laws can be improved or broken if the scheduling of the training data shifts in distribution, as shown in
(Sorscher et al., 2023). Different from (Sorscher et al., 2023; Lu et al., 2022), which alter the scheduling
of data to achieve better learning curves that decrease faster with the size of training data, our experiments
consider time series with pre-determined transition laws. Therefore, we cannot tamper with the scheduling
of our data to make it stationary without altering the transition rules underlying the time series.
A.9 Temperature and variance
The temperature Tis a hyper-parameter that controls the variance of the softmax output layer. Although
most LLMs are trained at T= 1, it is common practice to tune the temperature in the interval T‚àà[0.8,1.2]
during inference. Then, one can opt for increased diversity (high T), or better coherence (low T) in the
generated output. The temperature hyper-parameter affects the uncertainty, or variance, in the Hierarchy-
PDF extracted from the LLM. Figures 23 and 24 show how different temperatures change the shape of the
Hierarchy-PDF. In both cases, higher temperature leads to higher variance in the PDF.
We highlight the different refinement schemes used in these figures: for GBM, the PDF is refined to
the last (third) digit near the mode, and left coarse elsewhere else. This is because the true variance for
GBM can span two orders of magnitude (see Figure 13), with most data points trapped in the low-variance
region at small Xt. Hence, we require high precision to resolve these small variances in Figure 25. On the
other hand, the noisy logistic map time series does not suffer from this issue, and thus we uniformly refine
its PDF only up to the second digit.
While the loss curves in our paper are calculated at T= 1, the predicted œÉshown in Figures 7 and 13
are extracted at T= 0.7. We performed a grid search on the temperature ranging from T= 0.3toT= 3
(see Figures 26 and 27), and observed that T= 0.7consistently results in better prediction quality of the
variance.
6For stochastic systems, the invariant measure is sometimes referred to as the stationary distribution.
17Figure 23: Next state prediction for Geometric Brow-
nian motion. Topmost: Input stochastic time series
(black), and the state to be predicted (red). Rest:
The Hierarchy-PDF prediction extracted from LLaMA-
13b evaluated at different temperatures ranging from
T= 0.3to3, along with ground truth PDF (red).
Figure 24: Next state prediction for the noisy lo-
gistic map. Topmost: Input stochastic time series
(black), and the state to be predicted (red). Rest:
The Hierarchy-PDF prediction extracted from LLaMA-
13b evaluated at different temperatures ranging from
T= 0.3to3, along with ground truth PDF (red).
Figure 25: Most data points in GBM are trapped in low variance region with small Xt. The hierarchy-PDF must be
very refined to resolve these minuscule variances.
18LLM temperature = 0.3
LLM temperature = 0.7
LLM temperature = 1
Figure 26: GBM standard deviation œÉas a function
of state value Xt, learned by the LLM, along with
the ground truth. The LLM prediction is evaluated at
temperatures ranging from T= 0.3to1.LLM temperature = 0.3
LLM temperature = 0.7
LLM temperature = 1
Figure 27: Noisy logistic map standard deviation œÉ
as a function of state value Xt, learned by the LLM,
along with the ground truth. The LLM prediction is
evaluated at temperatures ranging from T= 0.3to1.
19A.10 Hierarchy PDF
This section documents all three parts of the Hierarchy-PDF algorithm. We refer to the GitHub repository
for further details.
Algorithm 2 Refine Each State in a Stochastic Sequence
Input:
-Straj: A string representing a sampled stochastic trajectory whose states are separated by commas.
-LPDF: List of unrefined PDFs for each state.
-KV cache: Key-value cache of running model.forward (Straj).
foreach state andPDFinStrajandLPDFdo
PDF‚ÜêRecursiveRefiner(True, state ,Dc,Dt,KV cache)
end for
Algorithm 3 Detailed Hierarchy-PDF Recursive Refiner
Input: Object multi_PDF representing unrefined PDF using bins of various widths
Procedure: RecursiveRefiner(mainBranch, sequence, Dc,Dt,KV cache)
ifDc=Dtthen
return {Terminate if target refinement depth is reached}
end if
ifmainBranch is True then
{Launch 9 recursive branches if the current sequence is refined}
Lnew‚ÜêForm 9 new sequences by changing the last digits
foreach sequence inLnewdo
RecursiveRefiner(False, sequence , Dc, Dt, KV cache)
end for
else
{Collect refined logits}
newLogits ,newKVcache ‚ÜêNextTokenProbs( sequence , KV cache)
Refine multi_PDF using newLogits
end if
ifDc+ 1< D tthen
{Launch 10 more branches if Dtnot met}
Lnew‚ÜêForm 10 new sequences by appending digits
foreach sequence inLnewdo
RecursiveRefiner(False, sequence , Dc+ 1, Dt,newKVcache )
end for
end if
20Algorithm 4 Extract Next Token Probabilities
function NextTokenProbs( sequence ,KVcache ,model )
NextTokenLogit ‚Üêmodel.forward (sequence ,KVcache )[last] {Extract distribution of next token}
Update KVcache
return NextTokenLogit ,KVcache
21