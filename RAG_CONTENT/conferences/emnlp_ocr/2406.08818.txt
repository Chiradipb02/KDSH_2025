Linguistic Bias in ChatGPT: Language Models Reinforce Dialect
Discrimination
Eve Fleisig*Genevieve Smith*Madeline Bossi*Ishita Rustagi*Xavier Yin*Dan Klein
University of California, Berkeley
{efleisig, genevieve.smith, madeline_bossi, ishita.rustagi, nzxyin, klein}@berkeley.edu
Abstract
We present a large-scale study of linguistic bias
exhibited by ChatGPT covering ten dialects of
English (Standard American English, Standard
British English, and eight widely spoken non-
“standard” varieties from around the world). We
prompted GPT-3.5 Turbo and GPT-4 with text
by native speakers of each variety and analyzed
the responses via detailed linguistic feature an-
notation and native speaker evaluation. We
find that the models default to “standard” vari-
eties of English; based on evaluation by native
speakers, we also find that model responses
to non-“standard” varieties consistently exhibit
a range of issues: stereotyping (19% worse
than for “standard” varieties), demeaning con-
tent (25% worse), lack of comprehension (9%
worse), and condescending responses (15%
worse). We also find that if these models are
asked to imitate the writing style of prompts in
non-“standard” varieties, they produce text that
exhibits lower comprehension of the input and
is especially prone to stereotyping. GPT-4 im-
proves on GPT-3.5 in terms of comprehension,
warmth, and friendliness, but also exhibits a
marked increase in stereotyping (+18%). The
results indicate that GPT-3.5 Turbo and GPT-4
can perpetuate linguistic discrimination toward
speakers of non-“standard” varieties.
1 Introduction
Popular tools powered by language models,
such as ChatGPT, can exhibit harms towards
marginalized groups, including stereotyping and
worse performance. A growing area of research
has examined harms on the basis of dialect
bias–difficulties faced by speakers of dialects, or
language varieties, that have fewer speakers or are
stigmatized as nonstandard. Given the vast num-
bers of people who speak varieties of English other
than Standard American English (SAE), the variety
*Starred authors all contributed jointly to the process of
designing and implementing the project.
Figure 1: Sample model responses (top) and native speaker
reactions to model responses (bottom).
typically produced by ChatGPT, we examine how
ChatGPT performs for speakers of minoritized (or
non-“standard”) varieties of English.
Our work addresses two central questions.
First, how does the behavior of ChatGPT differ in
response to different varieties of English? Second,
what harms (if any) do ChatGPT responses exhibit
toward speakers of minoritized varieties of English,
such as perpetuating stereotypes? Because
standard varieties of English, particularly SAE,
dominate available training data and are prioritized
in research and industry contexts, we hypothesized
that “standard” varieties of English would be
treated as the default and receive innocuous
responses. By contrast, we hypothesized that
models would produce potentially harmful outputs
in response to minoritized varieties.
We prompted both GPT-3.5 Turbo and GPT-4
with text in ten varieties of English: two standard
varieties, SAE and Standard British English (SBE);arXiv:2406.08818v3  [cs.CL]  17 Sep 2024and eight minoritized varieties: African American
English (AAE), Indian English, Irish English, Ja-
maican English, Kenyan English, Nigerian English,
Scottish English, and Singaporean English.1First,
to understand whether ChatGPT imitates features
of input varieties, we annotated the responses to
each variety for a set of paradigmatic linguistic
features of that variety (Section 4). Then, to
understand whether speakers of minoritized vari-
eties experience differences in response quality or
potential harms when using language models, we
surveyed native speakers of each variety for mul-
tiple qualities of the generated outputs (Section 5).
In our first study, we find that distinctive
linguistic features are reduced in responses to all
minoritized language varieties, while responses to
SAE and SBE retain the most features by a con-
siderable margin. For minoritized varieties, feature
retention appears to correlate with speaker popu-
lation size. In our second study, we find that model
responses to minoritized varieties are perceived
as more stereotyping, demeaning, unnatural, and
condescending; and less able to comprehend the
input. We also find that when GPT-3.5 is prompted
to imitate the input dialect, its responses exacerbate
stereotyping content and lack of comprehension.
GPT-4 responses imitating the input improve on
GPT-3.5 in terms of warmth, comprehension, and
friendliness, but further exacerbate stereotyping.
Given ChatGPT’s presumed excellent perfor-
mance on English, understanding performance
discrepancies for varieties of English that already
face stigma is critical. Discrepancies that limit
language models’ ease of use for minoritized popu-
lations could exacerbate existing global inequities.
Meanwhile, advancement of limiting stereotypes
and other harms could discourage speakers of
minoritized varieties from using language models
and reinforce discriminatory perspectives.
2 Related Work
Languages typically exhibit wide variation asso-
ciated with speakers from different regions, social
1“Standard” language is an “abstracted, idealized, homoge-
neous spoken language...imposed from above” and modeled
on “the written language” (Lippi-Green, 1994). “Standard”
language is not actively spoken by any real community; more-
over, all language varieties have more and less “standard”
versions. We use “standard varieties” to refer to Standard
American English and Standard British English because they
have by far the most global prestige and influence. We use
“minoritized varieties” for the other varieties tested (African-
American, Indian, Irish, Jamaican, Kenyan, Nigerian, Scottish,
and Singaporean English).groups, or identities (Labov, 2006; Eckert and Rick-
ford, 2009). Speakers of language varieties that do
not enjoy status as a “standard” dialect face discrim-
ination across settings including housing, employ-
ment, education, and criminal justice (Adger et al.,
2014; Baugh, 2005; Dro ˙zd˙zowicz and Peled, 2024;
Rickford and King, 2016). Dialect discrimination
often serves as a proxy for other forms of discrim-
ination, such as racism, classism, and xenophobia
(Baker-Bell, 2020; Wiley and Lukes, 1996).
Issues of linguistic discrimination in natural lan-
guage processing (NLP) have raised increasing con-
cern. Research on English is the status quo (Bender,
2019; Joshi et al., 2020); even within English, pri-
oritization of standard varieties could result in dif-
ferential performance and opportunity allocation,
as well as linguistic profiling (Nee et al., 2022).
Previous work has explored some dialect biases
in language models. This research has largely
focused on AAE, for which studies have found
evidence of bias in hate speech detection (Sap et al.,
2019), language identification (Blodgett et al.,
2018), speech recognition (Koenecke et al., 2020;
Martin and Tang, 2020; Martin and Wright, 2023;
Wassink et al., 2022; Zellou and Holliday, 2024),
and text generation (Deas et al., 2023). Hofmann
et al. (2024) also find that language models
exhibit harmful stereotypes about AAE speakers
in hypothetical decisions, such as employment and
criminal conviction. On synthetic data for several
varieties, Ziems et al. (2023) find disparities on
common NLP tasks such as semantic parsing.
On other varieties of English, Yong et al. (2023)
find mixed results for generation of code-mixed
Southeast Asian dialects and Ryan et al. (2024)
find disparities on a dialog intent prediction task
for Indian and Nigerian English speakers.
Our research aimed to address several gaps in the
existing literature. To address that most research
has focused on AAE or synthetic data, we studied
responses to native speaker-authored text in a large-
scale study of ten widely spoken varieties of En-
glish globally. In addition, we aimed to understand
how harms affect native speakers in the increas-
ingly common setting of casual interaction with a
language model such as ChatGPT. To do so, we
had native speakers evaluate open-ended GPT-3.5
Turbo and GPT-4 responses to text in the varieties
they speak. To complement previous work based
on automatic evaluation metrics, we recruited na-
tive speakers for our evaluation. These annotators
rated the responses along multiple axes and gavefree-text feedback on their experiences to provide a
richer understanding of native speaker perspectives.
3 Approach
We selected ten varieties of English (AAE,
Indian English, Irish English, Jamaican English,
Kenyan English, Nigerian English, SBE, Scottish
English, Singaporean English, and SAE) based
on factors including first and second language
speaker population counts, availability of linguistic
literature on the varieties, geographic spread, and
socio-historical context. We aimed to include
varieties with larger speaker populations, which
represent significant potential user groups for
tools like ChatGPT. It was also essential to select
varieties with enough linguistic description to
determine distinctive features for each variety.
Finally, we ensured that the varieties chosen have
a sufficient geographic spread and reflect different
socio-historical contexts by which English came
to be spoken in a particular area.
English language data was collected from
several sources (Appendix D). Nigerian, Jamaican,
Indian, Irish, and Kenyan English data was drawn
from the International Corpus of English (ICE)
(Greenbaum and Nelson, 1996; Hundt and Gut,
2012). For each of these varieties, we chose to
only analyze social letters in order to mimic the
informal tone and style of text that users would
use in dialogue with language models. SAE
and SBE were sourced from Reddit posts on US
and UK cities’ subreddits, respectively (Zhang,
2023).2AAE was sourced from Blodgett et al.
(2018). Scottish English data was drawn from the
correspondence and letters subset of the SCOTS
corpus (Anderson et al., 2007). Singaporean
English data was sourced from the text messages
in the CoSEM corpus (Gonzales et al., 2024).
3.1 Overview of studies
We conducted two studies to understand language
model behavior in response to minoritized varieties.
Before assessing potential harms, we first aimed
to descriptively characterize model behavior in
response to minoritized varieties. For this first
study, we prompted GPT-3.5 Turbo to respond to
inputs in the minoritized varieties. We annotated
the inputs and responses for linguistic features
of each variety to understand whether model
2This dataset was chosen over others with UK data because
it permitted filtering out Northern Irish and Scottish locations.responses retain features of the input variety
(Section 4). We also sought to understand whether
certain types of features from an input variety are
retained more than others and what factors might
influence feature retention.
For our second study, we investigated potential
harms that could arise from model responses to
minoritized varieties (both by default, and specif-
ically when it attempts to produce a minoritized
variety). We first collected additional responses,
prompting GPT-3.5 Turbo and GPT-4 to imitate
the input varieties when responding to each variety.
Responses under each scenario (GPT-3.5 without
imitation, GPT-3.5 with imitation, and GPT-4 with
imitation) were annotated by native speakers of
each variety for a range of potential harms, such as
stereotyping content and comprehension of the in-
put. We analyzed the responses to understand how
language models may perpetuate harms against na-
tive speakers of minoritized varieties, and whether
the nature or extent of these harms changes when
models explicitly try to imitate the variety or when
more powerful models can imitate the features of
the variety more convincingly (Section 5).
4Study 1: Assessing linguistic features of
default responses
For our first study, we conducted evaluations to
test the following hypotheses: (1) that ChatGPT
responses will have a reduction in the features of
different varieties of English for all varieties tested
except SAE; and (2) that ChatGPT responses will
have increased American orthography. Ten of
the most prominent features of each variety were
selected for our analysis. These features were
selected based on existing linguistic descriptions
of each variety, focusing on the morphosyntactic
features (word- and sentence-level features that
can be observed in written data) that the existing
documentation deems particularly distinctive
(full annotation guides in Appendix B). These
features range from distinctive lexical items (e.g.
flatmeaning ‘apartment’ for SBE) to distinctive
sentence structures (e.g. lack of subject-verb
inversion in yes-no questions in Indian English).
We also identified the orthography of the output:
American, British, or either (no distinctive features
found). The focus on American and British orthog-
raphy stems from the socio-historical context of
English colonization in the British Isles, Africa,
Americas, and Asia, and the United States’ expand-Variety of
English# Features:
Inputs# Features:
Outputs% Retention
↑
SAE 295 230 78%
SBE 291 210 72%
Indian 73 12 16%
Nigerian 44 5.5 13%
Kenyan 90 9 10%
Irish 26 1 4%
AAE 63 2 3%
Scottish 37 1 3%
Singaporean 40 1 3%
Jamaican 51 1 2%
Table 1: Overview of language varieties and features repre-
sented in inputs and GPT-3.5 outputs.
ing sphere of influence and colonization efforts in
the Pacific, which has led to most English language
communities adopting the orthography of Britain
or the US (e.g., analyse/analyze ,favour/favor ).
As with the linguistic features discussed above,
distinctive orthographic features were determined
based on existing linguistic description.
For each variety, we sampled approximately 50
messages3to prompt GPT-3.5 Turbo via the Ope-
nAI API. The inputs provided to the model focus
on benign topics related to daily life (e.g. updates
about how the author is doing, travel recommenda-
tions for particular areas, etc.). The system prompt
(Appendix C) encouraged the model to respond di-
rectly to the letter. Two reviewers from our research
team independently assessed each (input, output)
pair for the ten selected distinctive features of the
variety, in addition to the orthography (Krippen-
dorff’s α= 0.97). We averaged results for the two
reviewers per variety to conduct our evaluations.
4.1 Results
Model outputs retain features of SAE and SBE
far more than those of other varieties, though
some features of other varieties are still retained.
Appendix A, Table 4 lists the distinctive features
retained across input-output pairs for each variety.
SAE had the least reduction in linguistic features,
with a 77.9% feature retention rate, followed
by SBE at 72.2%. Outputs in response to the
remaining eight varieties had far lower retention of
linguistic features (Table 1). Five varieties experi-
enced only 2-3% feature retention in the generated
outputs. Indian, Nigerian, and Kenyan English
experienced significant but less extreme reductions
of linguistic features in outputs (10-16% retention).
3We removed content that did not qualify as informal writ-
ing, such as newspaper letters to the editor; this resulted in
a minimum of 44 messages per variety used to prompt the
model.
Figure 2: Estimated maximum speaker population vs. reten-
tion rate for minoritized varieties.
Feature retention rate correlates with estimated
maximum speaker population. Curiously, the
model neither retains features from all minoritized
varieties equally nor produces exclusively SAE fea-
tures. This could be due to the amount of available
training data for each variety, which likely depends
on the number of speakers. Due to the lack of
reliable estimates for the amount of available train-
ing data or number of speakers of each variety, we
estimate maximum speaker population based on
the population of each country where the variety is
spoken (population sources in Appendix D).4Al-
though members of these populations may not nec-
essarily be speakers of these varieties, and speakers
from other regions may also speak these varieties,
they serve as approximate estimates for the max-
imum speaker population. Indeed, the retention
rate for minoritized varieties correlates with esti-
mated maximum speaker population for the variety
(Figure 2). This suggests that the training data avail-
able to language models may influence the extent
to which they retain features of different varieties.
In regards to orthography, the percent of outputs
in American orthography increased for every
language variety, while the percent of outputs in
British orthography decreased (Figure 3).5For all
varieties except SAE and AAE, where American
orthography is the default, use of American or-
thography increased by 13-43% in the outputs and
use of British orthography decreased by 13-63%.
Even for SBE, British orthography decreased
4For AAE, we instead estimate speaker population based
on the African-American population of the United States.
We recognize as a limitation that the relationship between
African-American English and the African-American commu-
nity is ambiguous and contested (AAE speakers may not all be
African-American, and vice versa) (King, 2020). The speaker
estimates we use are intended as estimated upper bounds to
understand how much data in these dialects is potentially avail-
able, and are not meant to unequivocally identify the dialect
with the entire community for which it is named.
5Except AAE, for which no British orthography was ob-
served in inputs or outputs.Figure 3: Change in % of examples using British, American,
or either orthographic style from inputs to outputs.
significantly in the outputs (-39.71% for British
orthography; +29.18% for American orthography).
We also explored common linguistic features of
each language variety and whether they are main-
tained in the outputs (common distinctive features
found are in Appendix A, Table 3). Compared to
the inputs, the generated outputs exhibit significant
reduction in features for all English varieties
except SBE and SAE. For instance, 19 Kenyan
English inputs in the data display article omission
(e.g. All I wish you is Øhappy stay in Kenyan
English vs. All I wish you is ahappy stay in SAE),
while only one generated output displays this
linguistic feature. In contrast, responses to SBE
and SAE exhibit some increases in variety-specific
linguistic features compared to the inputs. These
include two more instances of adverbs modifying
verbs in the SBE inputs (e.g. Personally , I find
it slightly unethical ) and three more instances of
present tense verbs that end in -swith 3rd person
subjects (e.g. that help s). However, these two
features are grammatical in both SBE and SAE;
that is, both varieties allow this use of adverbs and
3rd person singular -s. Thus, even these rare cases
of feature increase in GPT-3.5 outputs often simply
replicate Standard American English features.
The distinctive features that are retained tend
to be lexical features, or features that are gram-
matical in SAE. To consider which distinctive
features were retained across input-output pairs,
we calculated their retention rates: if a feature was
present in an input and its corresponding output,
this example was counted as a retention for that
feature. All varieties except SBE and SAE have
very limited feature retention: Kenyan and Indian
English had 3 out of 10 features retained each,
Jamaican English had no features retained, and the
other varieties had one feature retained each.
The most commonly retained type of feature–
seen in all but one English variety in Table 4–islexical, including borrowed and distinctive words.
The retention of lexical features in GPT-3.5 outputs
is unsurprising because these features are generally
more common and more visible than grammatical
features, which relate to more subtle linguistic pat-
terns such as word order or morphological marking
(e.g., past tense -ed). In fact, these examples of
lexical retention often involve ChatGPT parroting
back a word from the input, though sometimes
changing the spelling to be more in line with
American or British orthography (e.g. GPT-3.5’s
our leisure activities, including beer, music, and
nyama choma in response to particularly with
beer, music, and nyawa-choma ; Kenyan English).
SAE and SBE had 9 out of 10 features retained
each. However, the vast majority of these retained
features are either lexical or grammatical in both
SBE and SAE, since these varieties have much in
common. For SBE, one lexical feature and eight
grammatical features are retained. All eight of
these grammatical features are also found in SAE.
For SAE, all nine retained features are grammati-
cal. All retained SAE features except for “singular
collectives” (i.e. The government isdiscussing... in
SAE vs. The government arediscussing... in SBE)
are also grammatical in SBE. This pattern high-
lights that even when GPT-3.5 retains a high num-
ber of distinctive features in the language that it pro-
duces, this language still closely aligns with SAE.
Finally, we examined which distinctive features
were introduced by GPT-3.5 (Appendix A, Table
5). If a feature was present in an output but was
not in the corresponding input, this example
was counted as an introduction for that feature.
Only SAE and SBE have feature introductions;
no features of any other English variety are
introduced by GPT-3.5. Introduced features are
uniformly less frequent than retained features:
every introduction frequency in Table 5 is lower
than the corresponding retention frequency in
Table 4. Notably, nearly all introduced features
are grammatical in both SAE and SBE. The two
exceptions are distinctive British lexical items,
which are not found in SAE, and singular collective
nouns in SAE but not SBE. Both of these features
only have a single introduction each. This pattern
highlights that even the distinctive features that
GPT-3.5 does retain still closely align with SAE.5 Study 2: Native speaker evaluation of
output disparities
Our second study explored to what extent Chat-
GPT outputs might perpetuate harms in response
to speakers of minoritized language varieties. Our
analyses aimed to answer three questions:
•By default, what harms do native speakers
of minoritized varieties face when interacting
with language models, relative to speakers of
standard varieties?
•How do these harms change if the model is
prompted to imitate the input variety?
•Does using a newer model that is better at
imitating minoritized varieties (GPT-4 instead
of GPT-3.5) improve or worsen these harms?
For this study, after completing the annotation
process, we selected all of the input letters for each
language variety for which the input or output con-
tained at least one feature. Then, these selected
input letters were fed into GPT-3.5 and GPT-4 with
a new system prompt that instructs the model to at-
tempt to match the style and tone of the letter in its
response. Native speakers of each variety were then
recruited to evaluate the responses via Prolific (see
Appendix D for details on survey format, recruit-
ment, filtering, consent, and compensation). Each
annotator completed a survey consisting of twelve
input-output pairs in random order, with six out-
puts from GPT-3.5 (prompted simply to respond);
three from GPT-3.5 (prompted to imitate the input);
and three from GPT-4 (prompted to imitate the in-
put).6This resulted in 879 total examples annotated
(mean of 87.9 per variety). For each (input, output)
pair, annotators assessed the output on 5-point Lik-
ert scales for nine qualities: stereotyping, demean-
ing content, condescension, formality, comprehen-
sion, naturalness, warmth, friendliness, and respect.
We included a short reflection at the end asking
speakers to give open-ended feedback as well. We
also asked annotators to optionally provide demo-
graphic and background information to ensure we
incorporated diversity among our participants, ac-
count for common linguistic variation along demo-
graphic dimensions, and ensure participant famil-
iarity with the English variety being examined.
6Outputs were distributed such that each output was anno-
tated by at most two annotators. For each variety, at least 11
annotators were recruited (mean=14.2).5.1 Results
We first compared responses to minoritized vari-
eties against responses to the “standard” varieties,
SAE and SBE, when GPT-3.5 is prompted simply
to respond to the input. SAE and SBE exhibit
very similar patterns in the results, with ratings
within 0.25 points of each other on all axes except
demeaning content and formality.
GPT-3.5 responses to minoritized varieties
are rated as worse than responses to standard
varieties on most axes. On average, responses
to minoritized varieties were rated as 25% more
demeaning and 19% more stereotyping. Responses
to Indian and Jamaican English were seen as most
demeaning and responses to Irish and AAE seen
as least demeaning. Responses were seen as more
stereotyping for all minoritized varieties except
AAE, with responses to Nigerian, Indian, and Irish
English seen as particularly stereotyping. The fact
that AAE is an exception here is unexpected, given
the well-documented evidence of discriminatory
outputs in response to AAE (e.g., Hofmann et al.,
2024; see also Section 2). This could be a result
of deliberate efforts to improve performance for
AAE on these models, though it is unclear if any
such mitigations have been implemented.
Responses to minoritized varieties were also
rated on average as 9% worse at comprehending
the input and 15% more condescending. Responses
for every minoritized variety were seen as more
condescending than responses to SAE and SBE,
with responses to Jamaican and Singaporean
English perceived as particularly condescending.
Responses to minoritized varieties were also typi-
cally perceived as less natural (8% gap on average).
Though several varieties are rated as similarly natu-
ral to SAE and SBE (or slightly higher, for Nigerian
English), several are rated as significantly less
natural (Scottish, Indian, Singaporean, and AAE).
The level of formality differed across varieties,
with responses to Indian English rated as most for-
mal and responses to Jamaican English seen as least
formal. Warmth and formality tend to be inversely
correlated, as expected. Most varieties are rated as
similarly warm and friendly. As expected, warmth
and friendliness ratings are correlated: for example,
Indian English responses are rated lowest for
both criteria and Irish English responses are rated
highest for both. Counterintuitively, responses to
non-SAE varieties are generally perceived as more
respectful (+10% on average). This could be dueFigure 4: Average response ratings by variety (5-point scale). Red titles indicate negative qualities, green indicates positive,
and yellow indicates neutral. Gray horizontal lines are 95% confidence intervals. The orange dotted line is the average for the
standard varieties (SAE and SBE) for ease of comparison. Responses to minoritized varieties (blue) were rated as worse in terms
of stereotyping (19% gap), demeaning content (25%), comprehension (9%), naturalness (8%), and condescension (15%).
to responses in a standard variety being perceived
as more respectful by the participants (see also
Section 5.2). The differences in stereotyping,
demeaning content, comprehension, naturalness,
respect, and condescension between standard and
minoritized varieties are all significant ( p < . 05).7
Responses imitating the input dialect exacerbate
stereotyping and lack of comprehension. Com-
paring GPT-3.5 responses with and without prompt-
ing to imitate the input style (Figure 5), we find that
when imitating the input, comprehension decreases
across all varieties (-6% for all varieties; -6% for
minoritized varieties specifically) and stereotyp-
ing increases across all varieties (+9% for all vari-
eties; +10% for minoritized varieties). Formality
decreases across all varieties (-14% for all varieties;
-15% for minoritized varieties). No significant
changes were found along other axes. The increase
in stereotyping content and lack of comprehension
suggests that imitating the input dialect can exacer-
bate potential harms. These effects do appear to be
relatively uniform: speakers of “standard” varieties
and speakers of minoritized varieties reported sim-
ilar changes. The decrease in formality could be
helpful, if it ameliorates undue formality, or could
exacerbate harms if perceived as overly familiar.
Imitation by GPT-4 improves on some axes but
worsens stereotyping. Comparing the responses
in which the model is asked to imitate the style of
7We performed a two-tailed t-test with Benjamini-
Hochberg correction for multiple tests.the input (Figure 5), we see that imitative responses
from GPT-4 are rated as better than imitative re-
sponses from GPT-3.5 in terms of comprehension
(+10% for all varieties; +10% for minoritized
varieties), warmth (+6% for all varieties; +7%
for minoritized varieties), and friendliness (+6%
for all varieties; +6% for minoritized varieties).
These results suggest that GPT-3.5 improves on
GPT-4 along multiple dimensions. In particular,
comprehension is rated as higher for imitative GPT-
4 outputs than even GPT-3.5 without imitation,
which could improve quality of service for speak-
ers of minoritized varieties. However, responses
show a marked increase in stereotyping (+18% for
all varieties; +14% for minoritized varieties). This
result suggests that, although GPT-4 might be bet-
ter able to imitate features of the input variety, this
ability comes at the cost of increased stereotyping.
Formality decreases across all varieties (-20%
for all varieties; -18% for minoritized varieties),
which could improve or worsen quality of service
depending on speaker perspectives. Differences
along other axes were not significant. We also see
that stereotyping and demeaning content increase
more for standard varieties (+39%, +25%) than for
minoritized varieties (+14%, +1%): when GPT-3.5
imitates the input style, stereotyping/demeaning
content is more severe for minoritized varieties,
whereas stereotyping/demeaning content appears to
a similar extent across varieties under GPT-4. How-
ever, the disparity between the level of stereotyp-Figure 5: Top: Change in average ratings for each variety from GPT-3.5 responses that do not imitate the input variety to GPT-3.5
responses that do. Bottom: Change in ratings from GPT-3.5 responses that imitate the input variety to GPT-4 responses that
imitate the input variety.
ing/demeaning content for minoritized vs. standard
varieties lessens under GPT-4 not because these
qualities improve for minoritized varieties, but
because they worsen for standard varieties. Stereo-
typing and demeaning content remain a problem
for minoritized varieties in GPT-4 responses. The
results when the models imitate the input variety
suggest that, although GPT-4 improves on GPT-3.5
for several axes, prompting models to produce non-
standard varieties does not resolve speakers’ con-
cerns about model responses and in fact introduces
new concerns regarding increased stereotyping.
5.2 Qualitative Native Speaker Feedback
When soliciting native speaker feedback, we also
asked annotators to provide free-text responses
about their experience annotating the data. These
responses indicated a wide range of attitudes
regarding model responses to minoritized varieties.
Several annotators expressed surprise that the
models performed as well as they did. One Ja-
maican English speaker was “kind of impressed
that ChatGPT could understand that much from
Jamaican [patois].” A Nigerian English speaker
reported being “glad chatgpt is almost thinking and
responding like people like me,” while another “had
a really good time” and “was really surprised that
was coming from chatGPT.” Feedback from SAE
and SBE speakers was generally positive, though
some noted that the responses felt “excessively
friendly,” “very formal,” or “somewhat stilted.”However, others reported that the responses
felt unnatural in various ways: a Nigerian English
speaker felt “like I was being stereotyped a bit,” a
Kenyan English speaker felt that “some responses
were not as friendly,” and a Singaporean English
speaker felt that some “felt too formal [...] a little
robotic.” An African American English speaker
explained that while AAE speakers are “familiar
with the concept of code-switching [...] a chatbot
can’t make those tweaks,” causing responses to
seem “just a little...off.”
Other annotators expressed more frustration and
discomfort at the model responses, particularly
those imitating the input. One AAE speaker
described being “somewhat disturbed” by the idea
of chatbots reproducing AAE. A Singapore English
speaker wrote that the outputs “do not feel like
they’re written by the typical Singaporean” and
another felt that “the super exaggerated Singlish
in one of the responses was slightly cringeworthy.”
These responses highlight the range of reactions
that native speakers feel regarding model responses
to minoritized varieties, as well as some of the
failure modes of model responses: unnatural
tones, undue formality, excessive stereotyping,
and potential appropriation or disparagement of
minoritized varieties.
6 Discussion & Conclusion
Our research illustrates differences in GPT-3.5 and
GPT-4 responses across varieties of English. GPT-3.5 retains features of SAE and SBE more than
features of minoritized varieties of English. The
distinctive features that are retained tend to be lex-
ical items or features that are grammatical in SAE.
For minoritized varieties, feature retention rate
correlates with estimated maximum speaker pop-
ulation, which may reflect available training data.
Model responses can also fail to adequately serve
speakers of minoritized varieties through increased
stereotyping, demeaning content, condescension,
and lack of comprehension. GPT-3.5 responses
that attempt to imitate the input further exacerbate
stereotyping and lack of comprehension. Excep-
tions to this trend highlight places where model
quality has improved: GPT-4 responses imitating
the input tend to improve on imitative GPT-3.5 re-
sponses in terms of comprehension, warmth, and
friendliness. However, GPT-4 responses exhibit
even higher levels of stereotyping, suggesting that
reducing stereotyping content in response to mi-
noritized varieties is of particular concern.
Disparities in output quality for speakers of
minoritized varieties may hamper their ability
to use language models; furthermore, harmful
responses can perpetuate discriminatory ideologies.
As language model usage increases globally, these
tools risk reinforcing power dynamics that harm
minoritized language communities.
7 Limitations
In beginning the study, we initially sought to ac-
cess Twitter data. However, we were not able to
access data given changes in the leadership of Twit-
ter (now X), which prevented access to the Twitter
API for researchers. We therefore pivoted to find
informal, written data for the various language va-
rieties from different sources. While we captured
informal, written language data for all varieties,
some of the data was in the form of letters from
the International Corpus of English, while other
language varieties were sourced from social media
(SAE, SBE, Scottish) and text messages (Singa-
porean). This meant there were some differences
in the level of informality for language data.
In addition, survey responses were collected
through Prolific, which is only available in some
countries (most OECD countries, Croatia, and
South Africa). We used Prolific because it facili-
tated survey logistics and there were users of each
of the ten target English varieties on the platform.
However, these users were consolidated primar-ily in Europe and North America. While some of
the ten English varieties come from this part of
the world, many other varieties originate elsewhere
(e.g., Nigeria, Singapore, etc.). Speakers of English
varieties whose countries were not available on Pro-
lific were necessarily based elsewhere. Given that
location is a parameter of linguistic variation, it is
possible that speakers on Prolific have linguistic
differences from those elsewhere, though we are
not currently aware of any ways in which this fact
impacted our findings.
Our study centered on linguistic discrimination
in GPT-3.5 Turbo and GPT-4 because of Chat-
GPT’s comparatively large and widespread user
base; however, guture work could examine the po-
tential for dialect discrimination in other language
models.
Ethical Considerations
Research that investigates model performance on
imitating minoritized varieties carries the risk of
facilitating future efforts to produce minoritized
varieties, which could result in appropriation or
impersonation of those language communities. In
addition, our study only examined varieties of En-
glish, but dialect discrimination is present in other
languages as well. We acknowledge that closing
the gap between research in English and research
on other languages is crucial; understanding model
behavior in response to varieties of other languages
is an important direction for future work.
References
Carolyn Temple Adger, Walt Wolfram, and Donna
Christian. 2014. Dialects in Schools and Commu-
nities . Routledge.
M.A. Alo and Rajend Mesthrie. 2004. Nigerian English:
Morphology and syntax. In A Handbook of Varieties
of English , pages 323–339. De Gruyter Mouton.
Jean Anderson, Dave Beavan, and Christian Kay. 2007.
Scots: Scottish corpus of texts and speech. In Cre-
ating and Digitizing Language Corpora: Volume 1:
Synchronic Databases , pages 17–34. Springer.
April Baker-Bell. 2020. Linguistic justice . NCTE-
Routledge Research Series. Routledge, London, Eng-
land.
John Baugh. 2005. Linguistic profiling. In Black lin-
guistics , pages 167–180. Routledge.
Kenneth Beare. 2019. American English to British
English vocabulary.Emily M. Bender. 2019. The #BenderRule: On naming
the languages we study and why it matters.
Su Lin Blodgett, Johnny Wei, and Brendan O’Connor.
2018. Twitter Universal Dependency parsing for
African-American and mainstream American En-
glish. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 1415–1425, Melbourne,
Australia. Association for Computational Linguistics.
David Britain. 2007. Grammatical variation in England.
In David Britain, editor, Language in the British Isles ,
pages 75–104. Cambridge University Press.
Alfred Buregeya. 2013. Kenyan English. In The Mou-
ton World Atlas of Variation in English , pages 466–
474. De Gruyter Mouton.
Karen P. Corrigan. 2013. The Atlantic Archipelago of
the British Isles. In The Oxford Handbook of World
Englishes , pages 335–370. Oxford University Press.
Nicholas Deas, Jessica Grieser, Shana Kleiner,
Desmond Patton, Elsbeth Turcan, and Kathleen McK-
eown. 2023. Evaluation of African American lan-
guage bias in natural language generation. In Pro-
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pages 6805–
6824, Singapore. Association for Computational Lin-
guistics.
Dictionaries of the Scots Language. 2022. Dictionaries
of the Scots Language.
Anna Dro ˙zd˙zowicz and Yael Peled. 2024. The com-
plexities of linguistic discrimination. Philosophical
Psychology , page 1–24.
Penelope Eckert and John R Rickford, editors. 2009.
Style and Sociolinguistic Variation . Cambridge Uni-
versity Press, Cambridge, England.
Ravinder Gargesh and Pingali Sailaja. 2013. South Asia.
InThe Oxford Handbook of World Englishes , pages
425–447. Oxford University Press.
Wilkinson D W Gonzales, Jakob Leimgruber, Mie Hi-
ramoto, and Junjie Lim. 2024. The corpus of singa-
pore english messages (cosem).
Lisa J. Green. 2002. African American English: A
linguistic introduction . Cambridge University Press.
Sidney Greenbaum and Gerald Nelson. 1996. The in-
ternational corpus of english (ice) project. World
Englishes , 15(1):3–15.
Ulrike Gut. 2013. English in West Africa. In The
Oxford Handbook of World Englishes , pages 491–
507. Oxford University Press.
Valentin Hofmann, Pratyusha Ria Kalluri, Dan Jurafsky,
and Sharese King. 2024. Dialect prejudice predicts
ai decisions about people’s character, employability,
and criminality. Preprint , arXiv:2403.00742.M. Huber and K. Dako. 2008. Ghanaian English: Mor-
phology and syntax. In Rajend Mesthrie, editor,
Varieties of English , volume 3. Mouton de Gruyter,
Berlin.
Marianne Hundt and Ulrike Gut. 2012. Varieties of En-
glish Around the World . John Benjamins Publishing
Company, Amsterdam.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the NLP
world. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
6282–6293, Online. Association for Computational
Linguistics.
Jeffrey L. Kallen. 2013. Irish English Volume 2: The
Republic of Ireland . De Gruyter Mouton.
Paul Kerswill. 2007. Standard and non-standard En-
glish. In David Britain, editor, Language in the
British Isles , pages 34–51. Cambridge University
Press.
Sharese King. 2020. From african american vernacular
english to african american language: Rethinking
the study of race and language in african americans’
speech. Annual Review of Linguistics , 6(1):285–300.
Allison Koenecke, Andrew Nam, Emily Lake, Joe
Nudell, Minnie Quartey, Zion Mengesha, Connor
Toups, John R. Rickford, Dan Jurafsky, and Sharad
Goel. 2020. Racial disparities in automated speech
recognition. Proceedings of the National Academy
of Sciences , 117(14):7684–7689.
William Labov. 2006. The social stratification of En-
glish in New York city . Cambridge University Press.
Jakob R. E. Leimgruber. 2013. Singapore English:
Structure, variation, and usage . Cambridge Uni-
versity Press.
Lisa Lim. 2013. Southeast Asia. In The Oxford Hand-
book of World Englishes , pages 448–471. Oxford
University Press.
Rosina L. Lippi-Green. 1994. Accent, standard lan-
guage ideology, and discriminatory pretext in the
courts. Language in Society , 23:166.
Joshua L Martin and Kevin Tang. 2020. Understanding
racial disparities in automatic speech recognition:
The case of habitual" be". In Interspeech , pages
626–630.
Joshua L Martin and Kelly Elizabeth Wright. 2023. Bias
in automatic speech recognition: The case of african
american language. Applied Linguistics , 44(4):613–
630.
Robert McColl Millar. 2007. Northern and Insular
Scots . Edinburgh University Press.Mohamad Moslimani, Christine Tamir, Abby Budiman,
Luis Noe-Bustamante, and Lauren Mora. 2024. Facts
about the u.s. black population.
National Records of Scotland. 2024. Mid-year popula-
tion estimates for scotland in 2022.
Julia Nee, Genevieve Macfarlane Smith, Alicia Sheares,
and Ishita Rustagi. 2022. Linguistic justice as a
framework for designing, developing, and manag-
ing natural language processing tools. Big Data &
Society , 9(1):205395172210909.
Northern Ireland Statistics and Research Agency. 2021.
2021 census.
Oxford English Dictionary. 2023. Introduction to Indian
English.
John R Rickford and Sharese King. 2016. Language and
linguistics on trial: Hearing rachel jeantel (and other
vernacular speakers) in the courtroom and beyond.
Language , pages 948–988.
Michael J Ryan, William Held, and Diyi Yang. 2024.
Unintended impacts of llm alignment on global rep-
resentation. arXiv preprint arXiv:2402.15018 .
Andrea Sand. 2013. Jamaican English. In The Mouton
World Atlas of Variation in English , pages 210–221.
De Gruyter Mouton.
Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,
and Noah A Smith. 2019. The risk of racial bias in
hate speech detection. In Proceedings of the 57th
annual meeting of the association for computational
linguistics , pages 1668–1678.
Josef Schmied. 2013. East African English. In The
Oxford Handbook of World Englishes , pages 472–
490. Oxford University Press.
Devyani Sharma. 2005. Language transfer and dis-
course universals in Indian English article use. In
Studies in Second Language Acquisition , pages 535–
566. Cambridge University Press.
Camille Turner. 8 American English grammar rules to
sound like you’re from the States [online]. 2023.
U.S. Census Bureau. 2024. U.s. and world population
clock. U.S. Department of Commerce.
Alicia Beckford Wassink, Cady Gansen, and Isabel
Bartholomew. 2022. Uneven success: automatic
speech recognition and ethnicity-related dialects.
Speech Communication , 140:50–70.
Terrence G. Wiley and Marguerite Lukes. 1996.
English-only and standard english ideologies in the
u.s.TESOL Quarterly , 30(3):511.
Zheng Xin Yong, Ruochen Zhang, Jessica Forde, Skyler
Wang, Arjun Subramonian, Holy Lovenia, Samuel
Cahyawijaya, Genta Winata, Lintang Sutawika, Jan
Christian Blaise Cruz, Yin Lin Tan, Long Phan, Long
Phan, Rowena Garcia, Thamar Solorio, and AlhamAji. 2023. Prompting multilingual large language
models to generate code-mixed texts: The case of
south East Asian languages. In Proceedings of the
6th Workshop on Computational Approaches to Lin-
guistic Code-Switching , pages 43–63, Singapore. As-
sociation for Computational Linguistics.
Georgia Zellou and Nicole Holliday. 2024. Linguistic
analysis of human-computer interaction. Frontiers in
Computer Science , 6:1384252.
Jiuyu Zhang. 2023. Reddit us uk subreddits dataset.
Caleb Ziems, William Held, Jingfeng Yang, Jwala
Dhamala, Rahul Gupta, and Diyi Yang. 2023. Multi-
V ALUE: A framework for cross-dialectal English
NLP. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 744–768, Toronto,
Canada. Association for Computational Linguistics.Orthography
American British Either
Variety Input Change Input Change Input Change
SAE 47% +5% 0% -2% 50% -3%
SBE 4% +29% 45% -40% 51% +11%
AAE 2% +9% 0% 0% 98% -9%
Indian 0% +36% 59% -50% 41% +14%
Irish 1% +26% 58% -56% 40% +30%
Jamaican 1% +22% 13% -13% 86% -9%
Kenyan 4% +34% 34% -30% 62% -4%
Nigerian 2% +43% 22% -21% 76% -22%
Scottish 0% +26% 67% -63% 33% +37%
Singaporean 1% +13% 28% -22% 71% +9%
Table 2: Orthographic changes in inputs and GPT-3.5 outputs.
A Additional Details on Feature
Annotation
Table 2 provides details on changes in orthography
between the input and output. Table 3 details the
top distinctive features for each variety. Table 4
details the top features retained per variety. Table
5 lists the distinctive features that were introduced
in GPT-3.5 outputs for the varieties of English in
our study and the percentage of input-output pairs
that contain an introduction for that feature.
B Annotation guides for linguistic
features
B.1 Standard American English
Distinctive words : Annotate as 1 any words that
are unique to SAE (Beare 2019).
• bathroom
• fries
• yard
• vacation
• etc.
Reflexives : Annotate as 1 any instance of the
reflexive pronouns myself ,yourself ,himself ,herself ,
ourselves ,yourselves ,themselves (Kerswill 2007:
43).
3rd singular - s: Annotate as 1 any instance
of 3rd person singular - son a present tense verb
(Britain 2007: 86).
• He swim s.
• She eat s.
Singular collectives : Annotate as 1 any instance
of a collective noun that triggers singular verbal
agreement (Turner 2023).
• The staff istaking the day off.
Copula required : Annotate as 1 any instance of
the auxiliary verb be.• The dog isbarking.
Single negation : Annotate as 1 any instance of
single negation where double negation would be
possible in other English varieties (Kerswill 2007:
43).
• I do n’twant any.
Adverbs : Annotate as 1 any instance of an - ly
adverb modifying a verb (Kerswill 2007: 43).
• Come quick ly!
Relative clauses : Annotate as 1 any instance of
a relative clause introduced by that,which , or a null
relativizer.
• the book (that) you gave me
There existentials : Annotate as 1 any instance
of the plural verbs areorwere in athere existential
with a plural subject (Britain 2007: 91).
• There were papers scattered everywhere.
Comparison : Annotate as 1 any instance of a
comparative or superlative with only one instance
of comparative or superlative morphological mark-
ing (Britain 2007: 103).
• It’s easi erthan it used to be.
B.2 Standard British English
Distinctive words : Annotate as 1 any words that
are unique to Standard British English (Beare
2019).
• loo ‘bathroom’
• biscuit ‘cookie’
• crisps ‘chips’
• rubbish ‘trash’
• holiday ‘vacation’
• etc.
Reflexives : Annotate as 1 any instance of the
reflexive pronouns myself ,yourself ,himself ,herself ,
ourselves ,yourselves ,themselves (Kerswill 2007:
43).
3rd singular - s: Annotate as 1 any instance
of 3rd person singular - son a present tense verb
(Britain 2007: 86).
• He swim s.
• She eat s.English Variety Feature Name Example Input Count Output Count
Nigerian Article omission do__traditional wedding 11.5 0.5
Borrowed words out in oyibo land 6 2
Extended progressive I’vebeen having testimonies 3.5 0
Kenyan Article omission prosper for __better life 31.5 2.5
Borrowed words removing maize from the shamba 19.5 5
Extended progressive I’ m hoping that the date was changed 19 1
African American Distinctive words just been put on blast 16 2
Copula omission You __cool 15.5 0
Invariant present Emanda don’t consider 10.5 0
Jamaican - edoptionality my friend who useto live 7.5 0
Article omission to__new area 4.5 0
Invariant present He don’t know 4.5 0
Indian Article omission I was __research fellow 20.5 0.5
Borrowed words he misses chappattis 17 7
Distinctive words I have fixed Monday 5th February 15 2
Irish Object inversion Nadine hadn’t it done at that time 3.5 0
Do be Ido be living in Cork 2.5 0
Borrowed words hear all the craic 1.5 0
Singaporean Copula omission Your parcel __stuck at customs 9 0
-edoptionality disciplined and focus _girl 8.5 0
Invariant present Tomorrow never come 5 0
Scottish Borrowed words tonight, Hogmanay 21.5 1
-na I didnasee 5 0
Cleft constructions It was one of the few games I enjoyed 3 0
Standard British Adverbs I’mcurrently doing 43.5 45.5
Comparison better career options 37 29.5
Distinctive words helped me find flats 32.5 16.5
Standard American Copula required Fallon isthe next town 48 46
3rd singular -s if that help sout 45 47.5
Relative clauses forest which will pretty much 41.5 37.5
Table 3: Most common distinctive features per English language varietyEnglish Variety Feature Name Example Retention Frequency
Nigerian Borrowed words you believe Alayi dialete will 4%
Kenyan Borrowed words regarding the harambee 10%
Article omission in__T.T. Cool atmosphere 4%
Extended progressive you are trusting in God 2%
African American Distinctive words being called "bae" 4%
Jamaican —
Indian Borrowed words sad news of Shri Panchawagh’s passing 14%
Distinctive words purchased a flat 4%
Off shed offall your teaching responsibilities 4%
Irish Borrowed words Enjoy the craic ! 2%
Singaporean Borrowed words to best support ahma 2%
Scottish Borrowed words tonight on Hogmanay 2%
Standard American Copula required Mount Rushmore ison my list 92%
3rd singular -s it sound squite affordable 88%
Adverbs I’lldefinitely keep them in mind 67%
Relative clauses infrastructure that Alaska has 63%
Comparison the crazi estor cool est 41%
There existentials there arestill some great places 21%
Singular collectives the ... commission seem slike 8%
Single negation may notcharge a fee 7%
Distinctive words with its bars and music venues 6%
Standard British 3rd singular -s it sound slike 96%
Adverbs I’lldefinitely keep that 81%
Relative Clauses struggles that contribute to 71%
Comparison more lively 46%
Distinctive words open to flatsharing 31%
Past distinctions a customer’s bag went missing 16%
There existentials there areaccommodations 11%
Single negation I’mnota big clubber 8%
Reflexives as a cyclist myself 4%
Table 4: Features retained across inputs and outputs. Distinctive features that were never retained are omitted.
English Variety Feature Name Example Introduction Frequency
Standard American Adverbs the Senate carefully considers 16%
Relative clauses stereotypes that hinder progress 12%
There existentials there areplenty of activities 9%
Comparison the broad ercontext 8%
Reflexives if I find myself in Brookings 7%
3rd singular -s the Game Loop sound slike 7%
Singular collectives the city of Tempe hastaken 2%
Standard British Comparison it’ll make it easi er 13%
Relative clauses anything else __I should keep in mind 12%
There existentials there arehelpful people 12%
Adverbs I will definitely google it 10%
Reflexives musicians like yourselves 4%
Single negation I don’tmind paying a bit 4%
3rd singular -s it seem slike 3%
Distinctive words I’ll check the timetables 2%
Past distinctions I sawthe second hand 2%
Table 5: Features introduced in outputs that are not present in inputs. Distinctive features not in the table were never introduced
(i.e. have an introduction frequency of 0%). Only introduction of SAE and SBE features was found in the data.Do: Annotate as 1 any instance of doordid
being used as a main verb, but not done (Kerswill
2007: 43).
• Ididmy homework.
Past distinctions : Annotate as 1 any instance of
a past verb like saw,did(as a main verb), ate, etc.
where the simple past form of the verb is different
from its past participle form (i.e. seen,done ,eaten ;
Kerswill 2007: 43).
• Isawthe film.
Single negation : Annotate as 1 any instance of
single negation where double negation would be
possible in other English varieties (Kerswill 2007:
43).
• I do n’twant any.
Adverbs : Annotate as 1 any instance of an - ly
adverb modifying a verb (Kerswill 2007: 43).
• Come quick ly!
Relative clauses : Annotate as 1 any instance of
a relative clause introduced by that,which , or a null
relativizer.
• the book (that) you gave me
There existentials : Annotate as 1 any instance
of the plural verbs areorwere in a there existential
with a plural subject (Britain 2007: 91).
• There were papers scattered everywhere.
Comparison : Annotate as 1 any instance of a
comparative or superlative with only one instance
of comparative or superlative morphological mark-
ing (Britain 2007: 103).
• It’s easi erthan it used to be.
B.3 African American English
Distinctive words : Annotate as 1 any words that
are unique to AAE (Green 2002: 21-31).
•ashy ‘the whitish coloration of black skin due
to exposure to the cold and wind’
•kitchen ‘the hair at the nape of the neck which
is inclined to be very kinky’
•saditty ‘uppity acting Black people who put
on airs’
• etc.Habitual be: Annotate as 1 any instance of the
verb beused as an invariant auxiliary verb to indi-
cate the recurrence of an event (Green 2002: 25).
• They bewaking up too early.
Remote past been : Annotate as 1 any instance
of the invariant auxiliary verb been used to situate
an event or the start of an event in the remote past
(Green 2002: 25, 56).
• They been left.
Invariant present : Annotate as 1 any instance
of a present tense verb with a 3rd person singular
subject that lacks - smorphological marking (Green
2002: 38).
• He eat _.
Copula omission : Annotate as 1 any instance
of omission of the verb bein contexts where it’s
required in SAE (Green 2002: 38-41).
• She __tall.
Ain’t : Annotate as 1 any instance of the word
ain’t (Green 2002: 39-41).
• He ain’t been eating.
Done : Annotate as 1 any instance of the invariant
auxiliary verb done used to indicate that an event
has ended (Green 2002: 60).
• I told him you done changed.
Double negation : Annotate as 1 any instance of
multiple negators like don’t ,no, and nothing used
in a single negative sentence (Green 2002: 77, 79).
• I do n’t never have noproblems.
No’s: Annotate as 1 any instance of possession
indicated by putting the possessor and the noun
next to each other, with no need for ’s(Green 2002:
102).
• Sometime Rolanda _bed don’t be made up.
It/they existentials : Annotate as 1 instances of
the words itandthey used in constructions to indi-
cate that something exists (Green 2002: 80).
•It’s some coffee in the kitchen.B.4 Indian English
Borrowed words : Annotate as 1 any words that
have been borrowed into Indian English from other
languages spoken in India (Oxford English Dictio-
nary 2023).
• bhajan ‘a devotional song’
•dupatta ‘a doubled or two-layered length of
cloth worn by women as a scarf, veil, or shoul-
der wrap’
•sadhana ‘dedicated practice or learning to
achieve an (esp. spiritual) goal’
• etc.
Distinctive English words : Annotate as 1 any
instance of English words that are used in a dis-
tinctive way in Indian English (Oxford English
Dictionary 2023).
•kitty party ‘a social lunch at which those at-
tending contribute money to a central pool and
draw lots, the winner receiving the money and
hosting the next lunch’
•lunch home ‘a small restaurant or other eatery’
• shuttler ‘a badminton player’
• etc.
Extended progressive : Annotate as 1 any in-
stance of progressive aspect (i.e. be+verb-ing )
used in innovative contexts when compared to SAE,
especially with stative verbs like have,know ,under-
stand , and love (Gargesh and Sailaja 2013: 435).
• Mohan is having two houses.
Off: Annotate as 1 the particle offcombining
with a range of verbs to change the meaning slightly
(Oxford English Dictionary 2023).
•Let’s finish it off. ‘Let’s finish it and be done
with it.’
Transitivity swap : Annotate as 1 any instance
of verbs that are transitive in SAE acting intransi-
tively in Indian English or verbs that are intransitive
in SAE acting transitively in Indian English (Ox-
ford English Dictionary 2023).
• We enjoyed __very much.
Terms of address : Annotate as 1 any terms of
address that appear after the person’s name rather
than before (Oxford English Dictionary 2023).
• Mangesh uncleNo inversion : Annotate as 1 any instance
where subjects and verbs don’t invert in questions
(Gargesh and Sailaja 2013: 435).
• What you would like to read?
Embedded inversion : Annotate as 1 any in-
stance where subjects and verbs in embedded ques-
tions invert (Gargesh and Sailaja 2013: 435).
• We asked when would you begin.
Invariant isn’t it : Annotate as 1 the expression
isn’t it used invariably as a tag or echo question
(Gargesh and Sailaja 2013: 435).
• You are going tomorrow, isn’t it ?
Article omission : Annotate as 1 any instance of
an article like aorthebeing omitted in contexts
where it would be required in SAE (Sharma 2005:
545-546).
•What about getting __girl to marry from In-
dia?
B.5 Kenyan English
Borrowed words : Annotate as 1 any words that
have been borrowed into Kenyan English from
Swahili and other languages spoken in Kenya
(Schmied 2013: 479-481).
• ugali ‘maize-based dish’
• matatu ‘collective taxi, minibus’
• pole (sana) ‘sorry, politeness expression’
• etc.
Article omission : Annotate as 1 any instance of
an article like aorthebeing omitted in contexts
where it would be required in SAE (Buregeya 2013:
468).
•He noted that __Electoral Commission of
Kenya expects the Government to come out
and explain itself.
Invariant isn’t it : Annotate as 1 the expression
isn’t it used invariably as a tag or echo question
(Buregeya 2013: 468).
• We are all God’s children, isn’t it?
Myself : Annotate as 1 any instance of myself
used as a subject in coordinations with and(Bu-
regeya 2013: 467).•My brother and myself live far away from our
family home.
Object pronoun drop : Annotate as 1 any in-
stance of an object pronoun (i.e. words like it,him,
us) being omitted where it would be required in
SAE (Buregeya 2013: 467).
• I really appreciate __.
Non-count plural marking : Annotate as 1 any
instance of a mass noun (i.e. a noun that can’t com-
bine directly with numbers) getting plural marking
with - s(Buregeya 2013: 467).
• We sell equipment s.
• etc.
Extended progressive : Annotate as 1 any in-
stance of progressive aspect (i.e. be+verb-ing )
used in innovative contexts when compared to SAE,
especially with stative verbs like have ,know ,un-
derstand , and love (Buregeya 2013: 468).
•Areyouunderstanding me?
Than what : Annotate as 1 any instance of what
following than in a comparative clause (Buregeya
2013: 468).
• It’s harder than what you think.
No inversion : Annotate as 1 any instance where
subjects and verbs don’t invert in questions (Bu-
regeya 2013: 469).
•We’ll meet him where?
Pronoun + subject doubling : Annotate as 1
any instance of subjects being doubled using pro-
nouns that appear at the beginning of the sentence
(Buregeya 2013: 469).
•Us,welove money.
B.6 Nigerian English
Borrowed words : Annotate as 1 any words that
have been borrowed into Nigerian English from
other languages spoken in Nigeria (Gut 2013).
• oga ‘master’
• dodo ‘fried plantain’
• burukutu ‘a type of alcoholic drink’
• etc.Extended progressive : Annotate as 1 any in-
stance of progressive aspect (i.e. be+verb-ing )
used in innovative contexts when compared to SAE,
especially with stative verbs like have ,know ,un-
derstand , and love (Alo and Mesthrie 2004: 325).
• Iam smelling something burning.
Doubly marked past : Annotate as 1 any in-
stance of the past tense in negatives and interrog-
atives being doubly marked with the past tense
form of doand the past tense verb form (Alo and
Mesthrie 2004: 325).
• He didnotwent .
Invariant isn’t it : Annotate as 1 the expression
isn’t it used invariably as a tag or echo question
(Alo and Mesthrie 2004: 327).
• You like that, isn’t it ?
Article omission : Annotate as 1 any instance
of an article like aorthebeing omitted in con-
texts where it would be required in SAE (Alo and
Mesthrie 2004: 331).
• have __bath
• give __chance
• etc.
Non-count plural marking : Annotate as 1 any
instance of a mass noun (i.e. a noun that can’t com-
bine directly with numbers) getting plural marking
with - s(Alo and Mesthrie 2004 via Gut 2013).
• furniture s
• equipment s
• aircraft s
• etc.
Resumptive pronouns : Annotate as 1 any rela-
tive clause that contains a resumptive pronoun (i.e.
a pronoun within the relative clause that refers back
to the noun at the beginning of the relative clause;
Huber and Dako 2008: 372 via Gut 2013).
• the book that I read it
Tovariation : Annotate as 1 any instance of
infinitive tobeing absent with verbs where it would
appear in SAE or being added to verbs where it
wouldn’t appear in SAE (Alo and Mesthrie 2004:
329).
• enable him __do itUnmarked comparatives : Annotate as 1 any
instance of a comparative appearing without com-
parative morphology like - er(Alo and Mesthrie
2004: 330).
• He has __money than his brother.
Reduplication : Annotate as 1 any instance of
adjectives or adverbs undergoing reduplication (i.e.
doubling of a word or a part of a word) for word
formation or emphasis (Alo and Mesthrie 2004:
336).
•small-small things ‘insignificant things’
B.7 Jamaican English
No -ed: Annotate as 1 any instance of a past tense
verb form that would have - edin SAE but appears
with no - edin Jamaican English (Sand 2013: 214).
•When I first started this, they terrify _the hell
out of me.
Non-count plural marking : Annotate as 1 any
instance of a mass noun (i.e. a noun that can’t com-
bine directly with numbers) getting plural marking
with - s(Sand 2013: 212).
• toxic waste s
• etc.
Article omission : Annotate as 1 any instance of
an article like aorthebeing omitted in contexts
where it would be required in SAE (Sand 2013:
212).
•__Computer is a thing that every day you
learn.
The + proper name : Annotate as 1 any instance
of a definite article like theused with proper names
or names of institutions or groups of people (Sand
2013: 212).
• In 1987 theVictoria Park was transformed.
Extended progressive : Annotate as 1 any in-
stance of progressive aspect (i.e. be+verb-ing )
used in innovative contexts when compared to SAE,
especially with stative verbs like have ,know ,un-
derstand , and love (Sand 2013: 213).
• At least we ’re agreeing with the DEH.
Copula omission : Annotate as 1 any instance
of omission of the verb bein contexts where it’s
required in SAE (Sand 2013: 215).• Mary __in the garden.
Auxiliary omission : Annotate as 1 any instance
of auxiliary verbs (e.g. form of beorhave) omitted
where they would be required in SAE (Sand 2013:
215).
• What __you been up to?
Double negation : Annotate as 1 any instance of
multiple negators like don’t ,no, and nothing used
in a single negative sentence (Sand 2013: 214).
• Me and him do n’thave nothing .
Invariant present : Annotate as 1 any instance
of a present tense verb with a 3rd person singular
subject that lacks - smorphological marking (Sand
2013: 214).
• I’m a person who love _music.
No inversion : Annotate as 1 any instance where
subjects and verbs don’t invert in questions (Sand
2013: 216).
• What you’re talking about?
B.8 Irish English
Borrowed words : Annotate as 1 any words that
have been borrowed into Irish English from Irish,
a Celtic language spoken in Ireland (Kallen 2013:
134-152).
• Gaeilge ‘Irish Gaelic’
• bodhrán ‘drums’
•boxty ‘kind of bread that can be fried or baked
on a griddle’
• craic, crack ‘talk, conversation, fun, news’
• etc.
It-clefts : Annotate as 1 any instance of a cleft
construction made by moving part of the sentence
to the beginning of the sentence alongside it isorit
was(Kallen 2013: 72-73).
•It’sflat it was.
Embedded inversion : Annotate as 1 any in-
stance where subjects and verbs in embedded ques-
tions invert (Kallen 2013: 77).
•She asked him were there many staying at the
hotel.For to : Annotate as 1 any instance of the expres-
sion for to used to indicate purpose (Kallen 2013:
84).
• He was asked for to loosen the rope.
Nothat/who: Annotate as 1 any instance of a rel-
ative clause (i.e. whole clauses that modify nouns)
that isn’t introduced by that orwho when such
words would be required in SAE (Kallen 2013:
85).
• A man __came from the town told me.
Extended progressive : Annotate as 1 any in-
stance of progressive aspect (i.e. be+verb-ing )
used in innovative contexts when compared to SAE,
especially with stative verbs like have ,know ,un-
derstand , and love (Kallen 2013: 86-87).
• That’s what I was wanting .
Do be : Annotate as 1 any instance of the struc-
ture ( do)be(verb-ing ) used to indicate habitual
action or a recurrent state (Kallen 2013: 90-93).
•Hedoes be wanting to shave at all hours of
the day and of the night.
Object inversion : Annotate as 1 any instance of
an object surfacing before an - edor -enform of the
verb, rather than after (Kallen 2013: 104).
• I have it pronounced wrong.
Plural - smarked verbs : Annotate as 1 any
instance of a verb with the ending - sused with a
plural subject, where in SAE these forms would
only occur with singular subjects (Kallen 2013:
112).
• We bake sit.
-self : Annotate as 1 any pronoun ending in -
selfused in a wider range of contexts than in SAE,
including when there is no matching pronoun that
antecedes the - selfform (Kallen 2013: 120).
• I was thinking it was yourself that was in it.
B.9 Scottish English
Borrowed words : Annotate as 1 any words that
have been borrowed into Scottish English from
other languages spoken in or around Scotland or
older forms of English (Dictionaries of the Scots
Language 2022).•ceilidh ‘social evening with music, singing,
story-telling, etc.’
•loch ‘lake, sheet of natural water, arm of the
sea’
•tasse, tassie ‘cup, bowl, goblet, drinking ves-
sel, especially for spirits’
• Hogmanay ‘December 31, New Year’s Eve’
• etc.
It-clefts : Annotate as 1 any instance of a cleft
construction made by moving part of the sentence
to the beginning of the sentence alongside it isorit
was(Corrigan 2013: 355).
• And it was my mother (who) was daein it.
Multiple modals : Annotate as 1 any instance
of multiple modal verbs (i.e. words like can,must ,
should ,might ) co-occurring (Corrigan 2013: 357).
• She might can get away early.
Three-way demonstratives : Annotate as 1 any
instance of the hyper-distal demonstrative yonor
thon (Millar 2007: 69).
Numberless demonstratives : Annotate as 1 any
instance of the same demonstrative used in the sin-
gular and the plural (Millar 2007: 69).
•This rooms arena as warm as that rooms.
Extended comparatives : Annotate as 1 any in-
stance of comparative - eror superlative - estwith a
wider range of adjectives than in SAE (Millar 2007:
72).
• beautifull est
Singular-plural mismatch : Annotate as 1 any
instance of a singular verb form used with a plural
subject (Millar 2007: 74).
•The men we saw walkin doon the road is
comin back.
Invariant - s: Annotate as 1 any instance of -
smorphological marking on verbs in a narrative
where no such marking would be possible in SAE
(Millar 2007: 74).
•So I walk sinto the pub and I say sto the bar-
man...
-na: Annotate as 1 any instance of negation ex-
pressed with - narather than not (Millar 2007: 76).
• He did nalaugh.
nae: Annotate as 1 any instance of negation
expressed with naeorno(Millar 2007: 76-77).
• You naken anything about me!B.10 Singaporean English
Borrowed words : Annotate as 1 any words that
have been borrowed into Singaporean English from
other languages spoken in Singapore (Lim 2013).
• roti ‘bread’
• barang-barang ‘belongings, luggage’
• shiok ‘exceptionally good’
• sap sap sui ‘insignificant’
• etc.
Invariant present : Annotate as 1 any instance
of a present tense verb with a 3rd person singular
subject that lacks - smorphological marking (Leim-
gruber 2013: 71).
• He want _to see how we talk.
No -ed: Annotate as 1 any instance of a past
tense verb form that would have - edin SAE but
appears with no - edin Singaporean English (Leim-
gruber 2013: 72).
• That’s what him sayto us just now.
No inversion : Annotate as 1 any instance where
subjects and verbs don’t invert in questions (Leim-
gruber 2013: 74).
• How much it will be?
Copula omission : Annotate as 1 any instance
of omission of the verb bein contexts where it’s
required in SAE (Leimgruber 2013: 75).
• My uncle __staying there.
Wh-word placement : Annotate as 1 any in-
stance of wh-words (i.e. who,what ,where , etc.) in
questions surfacing within the sentence rather than
at the beginning (Lim 2013: 460).
• You buy what ?
Where got? : Annotate as 1 any instance of the
phrase where got used to signal disagreement or to
challenge a statement (Leimgruber 2013: 79).
• A: This dress is very red.
• B: Where got? ‘Is it? I don’t think so.’
Factual got: Annotate as 1 any instance of got
used to indicate that something is a statement of
fact (Leimgruber 2013: 78-79).
• Igotgo Japan. ‘I have been to Japan before.’Gotexistentials : Annotate as 1 any instance of
the verb got used in existential constructions (i.e.
it is... or there are ...) rather than a form of be
(Leimgruber 2013: 78).
•Gottwo pictures on the wall.
Discourse particles : Annotate as 1 any instance
of a discourse particle (i.e. optional elements that
serve a conversational purpose like right? after a
question or y’know to seek confirmation) unique to
Singaporean English (Leimgruber 2013: 87-89).
• Lah, la
• Ah
• Leh
• Meh, me
• etc.
C GPT-3.5/4 system prompts
baseline prompt:
"You are the recipient of the following
message. Write a message that responds
to the sender. Use "<NAME>" as the
placeholder for any names."
style + tone prompt:
"You will receive a message. Reply to
the message as if you are the recipient.
Match the sender’s dialect, formality,
and tone. Use "<NAME>" as the placeholder
for any names."
D Additional Details on Data Collection
For study 1, country populations were sourced from
2024 US Census Bureau data (U.S. Census Bureau,
2024). For regions or populations without available
2024 data, populations were estimated using the
most recent data found, as follows: Irish English
speaker population was estimated from the popula-
tion of Ireland (US Census Bureau) plus the 2021
Northern Ireland census by the Northern Ireland
Statistics and Research Agency (2021). The popu-
lation of Scotland (2022 census) was sourced from
the National Records of Scotland (2024). The U.S.
African-American population (2022 estimates) was
sourced from Moslimani et al. (2024).
For study 2, we obtained a license for the
ICE corpora that permits non-profit academic
research. The SCOTS corpus license permits
use of the data for research purposes. The SAE,
SBE, and AAE data were released under an
MIT license. The Singaporean English datawas released under a CC BY-NC-SA 4.0 license
(https://github.com/wdwgonzales/CoSEM ).
Names in the data were replaced with [NAME] .
Study 2 was first approved by our institutional
review board (IRB). Native speakers for Study 2
were recruited via Prolific using a combination of
filters. Participants were filtered using the “nation-
ality” filter to select participants whose nationality
corresponded to the variety being tested. In ad-
dition, we asked participants to provide details on
their experience with the variety being tested: when
they learned English, whether the variety was spo-
ken in the environment where they grew up, with
whom they used the variety, and their country of
origin and residence.
Participants were paid $15 to complete the sur-
vey, based on our estimated completion time of one
hour.
Responses were manually reviewed for quality.
Annotators who completed the survey in under five
minutes or gave nonsensical responses to the re-
quired free responses section were to be removed,
but no responses were found that met these criteria.
Annotators who did not meet the study criteria for
familiarity with the variety (e.g., responded that
they did not grow up in an environment where the
variety was spoken) were removed (n=14).8
Table 6 gives the race/ethnicity and gender de-
mographics of the annotators in the study.
Demographic
AttributeDemographic Group %
Men 46%
Women 51%
Nonbinary or other 2%Gender
Prefer not to say 2%
East Asian 4%
South Asian 14%
Southeast Asian 2%
Black or African-
American41%
White 32%
Multiple/Other 5%Race and
ethnicity
Prefer not to say 2%
Table 6: Race/ethnicity and gender of annotators in Study
2. Any identities not listed were not represented among the
participants.
D.1 Consent Form
Key Information and Consent to Participate in
Research: Assessing linguistic bias in ChatGPT
8An initial preprint reported statistics without these partic-
ipants dropped, with minor differences in effect sizes.Introduction and Purpose The study includes
the following research team members: [names].
The purpose of this study is to understand how
ChatGPT performs for speakers of different En-
glish varieties. This includes assessing the qual-
ity of language in outputs generated by ChatGPT
and evaluating whether these outputs incorporate
stereotypes or any other demeaning content.
Procedures Upon agreeing to participate in the
research, you will continue on to a survey. The sur-
vey has has two main components: (1) evaluation
by respondents (native speakers of target English
varieties) of default outputs from ChatGPT; and (2)
evaluation by respondents (native speakers of the
target English varieties) of outputs from ChatGPT
prompted to respond in the same dialect as the in-
put. A third component is a reflection which will
track and be used to assess how study participants
experience the evaluation process and how their
lived experiences impact responses. The survey
should last about 1 hour.
Compensation You will receive $15 for complet-
ing the survey.
Benefits Beyond the compensation you will re-
ceive for completing this survey, there is no direct
benefit to you.
Risks/Discomforts As with all research, there
is a chance that confidentiality could be compro-
mised; however, we are taking precautions to mini-
mize this risk.
Confidentiality Your study data will be handled
as confidentially as possible. If results of this study
are published or presented, any personally identifi-
able information will not be used. No identifiable
information will be collected and IP is turned off
on the Qualtrics form. Authorized representatives
from [institution] may review research data for pur-
poses such as monitoring or managing the conduct
of this study. Identifiers will be removed from any
identifiable information. After such removal, de-
identified data could be used for future research
studies by myself or others indefinitely without ad-
ditional informed consent from the subject or the
legally authorized representative. Regardless, do
not reveal any information that might place them
at risk of civil or criminal liability or cause dam-
age to their financial standing, employability, or
reputation.Rights Participation in research is completely
voluntary. You are free to decline to take part in the
project. Whether or not you choose to participate
in the research there will be no penalty to you or
loss of benefits to which you are otherwise entitled.
Given that all data is anonymized, there will not
be an opportunity for survey participants to with-
draw from the study after submitting the survey
response.
Questions If you have any questions about this
research, please feel free to contact [contact in-
formation]. If you have any questions about your
rights or treatment as a research participant in this
study, please contact [institutional contact informa-
tion].
GDPR This research will collect data about you
that can identify you, referred to as Study Data.
The General Data Protection Regulation (“GDPR”)
requires researchers to provide this Notice to you
when we collect and use Study Data about people
who are located in a State that belongs to the Eu-
ropean Union or in the European Economic Area.
We will obtain and create Study Data directly from
you so we can properly conduct this research. The
Research Team will collect and use the following
types of Study Data for this research: - Your racial
or ethnic origin and nationality - Your gender iden-
tity and age
This research will keep your Study Data for the
duration of the study and destroy it after this re-
search ends. The following categories of individ-
uals may receive Study Data collected or created
about you: - Members of the research team so they
properly conduct the research - [institution] staff
will oversee the research to see if it is conducted
correctly and to protect your safety and rights
The GDPR gives you rights relating to your
Study Data, including the right to: - Access, cor-
rect or withdraw your Study Data; however, the
research team may need to keep Study Data as long
as it is necessary to achieve the purpose of this re-
search - Restrict the types of activities the research
team can do with your Study Data - Object to using
your Study Data for specific types of activities -
Withdraw your consent to use your Study Data for
the purposes outlined in the consent form and in
this document. (Please understand that once you
submit the survey you will not be able to withdraw
as responses are anonymous.)
[institution] is responsible for the use of your
Study Data for this research. You can contact [in-stitutional contact information] if you have: - Ques-
tions about this Notice - Complaints about the use
of your Study Data - If you want to make a request
relating to the rights listed above.
Consent If you agree to take part in the research,
please click the “Accept” button below. You can
also print a copy of this page to keep for your future
reference.
D.2 Sample annotation form
Figures 6, 7, 8, and 9 provide a sample annota-
tor information and annotation form (for Jamaican
English).Figure 6: Sample demographics form, part 1 (Jamaican En-
glish).
Figure 7: Sample demographics form, part 2 (Jamaican En-
glish).Figure 8: Sample annotation form, part 1 (Jamaican English).
Figure 9: Sample annotation form, part 2 (Jamaican English).