Enhanced Hallucination Detection in Neural Machine Translation through
Simple Detector Aggregation
Anas Himmi1Guillaume Staerman2Marine Picot3
Pierre Colombo1,4Nuno M. Guerreiro1,5,6,7
1MICS, CentraleSupélec, Universite Paris-Saclay, Paris, France,
2Universite Paris-Saclay, Inria, CEA, Palaiseau, France,
3digeiz, Paris, France,4Equall, Paris, France,5Instituto de Telecomunicações, Lisbon, Portugal
6Unbabel, Lisbon, Portugal,7Instuto Superior Técnico, University of Lisbon, Portugal
Abstract
Hallucinated translations pose significant
threats and safety concerns when it comes to
practical deployment of machine translation
systems. Previous research works have identi-
fied that detectors exhibit complementary per-
formance — different detectors excel at detect-
ing different types of hallucinations. In this
paper, we propose to address the limitations
of individual detectors by combining them and
introducing a straightforward method for aggre-
gating multiple detectors. Our results demon-
strate the efficacy of our aggregated detector,
providing a promising step towards evermore
reliable machine translation systems.
1 Introduction
Neural Machine Translation (NMT) has become
the dominant methodology for real-world machine
translation applications and production systems.
As these systems are deployed in-the-wild for real-
world usage, it is ever more important to ensure
that they are highly reliable. While NMT sys-
tems are known to suffer from various patholo-
gies (Koehn and Knowles, 2017), the most severe
among them is the generation of translations that
are detached from the source content, typically
known as hallucinations (Raunak et al., 2021; Guer-
reiro et al., 2022b). Although rare, particularly in
high-resource settings, these translations can have
dramatic impact on user trust (Perez et al., 2022).
As such, researchers have worked on (i) methods to
reduce hallucinations either during training-time or
even inference time (Xiao and Wang, 2021; Guer-
reiro et al., 2022b; Dale et al., 2022; Sennrich
et al., 2024), and alternatively, (ii) the development
of highly effective on-the-fly hallucination detec-
tors (Guerreiro et al., 2022b,a; Dale et al., 2022) to
flag these translations before they reach end-users.
In this paper, we will focus on the latter.
One immediate way to approach the problem of
hallucination detection is to explore high-quality ex-ternal models that can serve as proxies to measure
detachment from the source content, e.g., quality
estimation (QE) models such as CometKiwi (Rei
et al., 2022), or cross-lingual sentence similar-
ity models like LASER (Artetxe and Schwenk,
2019) and LaBSE (Feng et al., 2022). Intuitively,
extremely low-quality translations or translations
that are very dissimilar from the source are more
likely to be hallucinations. And, indeed, these de-
tectors can perform very effectively as hallucina-
tion detectors (Guerreiro et al., 2022b; Dale et al.,
2022). Alternatively, another effective approach is
to leverage internal model features such as atten-
tion maps and sequence log-probability (Guerreiro
et al., 2022b,a; Dale et al., 2022). The assumption
here is that when translation models generate hal-
lucinations, they may reveal anomalous internal
patterns that can be highly predictive and useful for
detection, e.g., lack of contribution from the source
sentence tokens to the generation of the transla-
tion (Ferrando et al., 2022). Most importantly, dif-
ferent detectors exhibit complementary properties.
For instance, oscillatory hallucinations — trans-
lations with anomalous repetitions of phrases or
n-grams (Raunak et al., 2021) — are readily iden-
tified by CometKiwi , while detectors based on low
source contribution or sentence dissimilarity strug-
gle in this regard. Therefore, there is an inherent
trade-off stemming from the diverse anomalies dif-
ferent detectors excel at.
In this paper, we address this trade-off by propos-
ing a simple yet highly effective method to aggre-
gate different detectors to leverage their comple-
mentary strengths. Through experimentation in
the two most widely used hallucination detection
benchmarks, we show that our method consistently
improves detection performance.
Our key contributions are can be summarized as
follows:
•We propose STARE , an unsupervised Simple
deTectors AggREgation method that achievesarXiv:2402.13331v1  [cs.CL]  20 Feb 2024state-of-the-art performance well on two hal-
lucination detection benchmarks.
•We demonstrate that our consolidated detector
can outperform single-based detectors with as
much as aggregating two complementary de-
tectors. Interestingly, our results suggest that
internal detectors, which typically lag behind
external detectors, can be combined in such a
way that they outperform the latter.
We release our code and scores to support future
research and ensure reproducibility.1
2 Detectors Aggregation Method
2.1 Problem Statement
Preliminaries. Consider a vocabulary Ωand let
(X, Y )be a random variable taking values in X×Y ,
where X ⊆ Ωrepresents translations and Y=
{0,1}denotes labels indicating whether a transla-
tion is a hallucination ( Y= 1) or not ( Y= 0). The
joint probability distribution of (X, Y )isPXY.
Hallucination detection. The goal of halluci-
nation detection is to classify a given translation
x∈Xas either an expected translation from
the distribution PX|Y=0or as a hallucination from
PX|Y=1. This classification is achieved by a binary
decision function g:X→0,1, which applies a
threshold γ∈Rto a hallucination score function
s:X→R. The decision function is defined as:
g(x) =1ifs(x)> γ,
0otherwise .
The objective is to create an hallucination score
function sthat effectively distinguishes halluci-
nated translations from other translations.
Aggregation. Assume that we have several hal-
lucination score detectors2. When evaluating a
specific translation x′, our goal is to combine the
scores from the single detectors into a single, more
reliable score that outperforms any of the individual
detectors alone. Formally, this aggregation method,
denoted as Agg, is defined as follows:
Agg : RK→R
{sk(x′)}K
k=1→Agg
{sk}K
k=1
.
1Code is available here:
https://github.com/AnasHimmi/
Hallucination-Detection-Score-Aggregation .
2We use the notation {sk}K
k=1to represent a set consisting
ofKhallucination detectors, where each skis a function
mapping from XtoR.2.2 Proposed Aggregation Method
We start with the assumption that we have access
toKhallucination scores and aim to construct an
improved hallucination detector using these scores.
The primary challenge in aggregating these scores
arises from the fact that they are generated in an
unconstrained setting, meaning that each score may
be measured on a different scale. Consequently, the
initial step is to devise a method for standardizing
these scores to enable their aggregation. The stan-
dardization weights we propose, wk, are specific
to each detection score. Using the min-max nor-
malization, they are designed based on the whole
training dataset Dn={x1, . . . , x n}. Formally:
wk=sk(x′)−min
z∈Dnsk(z)
max
z∈Dnsk(z)−min
z∈Dnsk(z).
Given these weights, we build a hallucination de-
tector based on a weighted averaged of the score sk
relying upon the previous “normalization weights”:
Agg(x′) =KX
k=1wksk(x′). (1)
We denote this method as STARE .
3 Experimental Setup
3.1 Datasets
In our experiments, we utilize the human-annotated
datasets released in Guerreiro et al. (2022b) and
Dale et al. (2023). Both datasets include detection
scores — both for internal and external detectors
— for each individual translation:
LFAN-H ALL.A dataset of 3415 translations
for WMT18 German →English news translation
data (Bojar et al., 2018) with annotations on critical
errors and hallucinations (Guerreiro et al., 2022b).
This dataset contains a mixture of oscillatory hal-
lucinations and fluent but detached hallucinations.
We provide examples of such translations in Ap-
pendix A. For each translation, there are six differ-
ent detector scores: three are from external mod-
els (scores from COMET-QE andCometKiwi ,
two quality estimation models, and sentence sim-
ilarity from LaBSE , a cross-lingual embedding
model), and three are from internal methods
(length-normalized sequence log-probability, Seq-
Logprob ; contribution of the source sentence for
the generated translation according to ALTI+ (Fer-
rando et al., 2022), and WASS-COMBO , an OptimalDETECTOR AUROC ↑ FPR↓
Individual Detectors
External
COMET-QE 70.15 57.24
CometKiwi 86.96 35.15
LaBSE 91.72♂¶edal 26.86♂¶edal
Model-based
Seq-Logprob 83.40 58.99
ALTI+ 84.24 66.19
Wass-Combo 87.02 48.38
Aggregated Detectors
External Only (gap to best single External)
Isolation Forest 92.61 ↑0.89 19.08 ↓7.78
Max-Norm 92.43 ↑0.71 22.09 ↓4.77
STARE 93.32 ↑1.60 20.67 ↓6.19
Model-based Only (gap to best single Model-based)
Isolation Forest 88.19 ↑1.17 36.63 ↓11.8
Max-Norm 83.81 ↓3.21 62.94 ↑14.6
STARE 89.07 ↑2.05 42.50 ↓5.88
All (gap to best overall)
Isolation Forest 92.84 ↑1.12 23.90 ↓2.96
Max-Norm 91.60 ↓0.12 26.38 ↓0.48
STARE 94.12 ↑2.40 17.06 ↓9.80
(a) Results on L FAN-H ALL.DETECTOR AUROC ↑ FPR↓
Individual Detectors
External
COMET-QE 82.22 47.40
LASER 81.11 47.04
XNLI 82.44 33.20
LaBSE 88.77♂¶edal 34.96♂¶edal
Model-based
Seq-Logprob 86.72 28.86
ALTI+ 82.26 58.40
Wass-Combo 64.82 84.62
Aggregation Detectors
External Only (gap to best single External)
Isolation Forest 71.35 ↓17.4 57.75 ↑22.8
Max-Norm 88.57 ↑0.48 32.59 ↓2.86
STARE 89.76 ↑0.99 32.74 ↓2.22
Model-based Only (gap to best single Model-based)
Isolation Forest 75.35 ↓11.4 69.71 ↑40.9
Max-Norm 67.70 ↓17.3 83.83 ↑53.1
STARE 89.92 ↑3.20 30.37 ↑1.51
All (gap to best overall)
Isolation Forest 76.25 ↓12.5 56.28 ↑21.3
Max-Norm 80.67 ↓7.01 41.52 ↑1.91
STARE 91.18 ↑2.41 28.85 ↓6.11
(b) Results on H ALOMI.
Table 1: Performance, according to AUROC and FPR, of all single detectors available and aggregation methods via
combination of external detectors, model-based detectors, or both simultaneously. We represent with ♂¶edalthe best
overall single detector and underline the best detectors for each class, according to our primary metric AUROC.
Transport inspired method that relies on the aggre-
gation of attention maps).
HALOMI.A dataset with human-annotated hal-
lucination in various translation directions. We test
translations into and out of English, pairing En-
glish with five other languages — Arabic, German,
Russian, Spanish, and Chinese, consisting of over
3000 sentences across the ten different language
pairs. Importantly, this dataset has two important
properties that differ from LFAN-H ALL: (i) it has
a much bigger proportion of fluent but detached
hallucinations (oscillatory hallucinations were not
considered as a separate category), and (ii) nearly
35%of the translations are deemed hallucinations,
as opposed to about 8 %forLFAN-H ALL.3For
each translation, there are seven different detec-
tion scores: the same internal detection scores as
LFAN-H ALL, and four different detector scores:
COMET-QE ,LASER ,XNLI andLaBSE .
We provide more details on both datasets in Ap-
pendix A.
3Given the rarity of hallucinations in practical translation
scenarios (Guerreiro et al., 2023), LFAN-H ALL offers a more
realistic simulation of detection performance.Aggregation Baselines. The closest related work
is Darrin et al. (2023b) on out-of-distribution de-
tection methods, using an Isolation Forest (IF; Liu
et al., 2008) for per-class anomaly scores. We adapt
their method, employing a single Isolation Forest ,
and designate it as our baseline. Alternatively, we
also consider a different way to use the individual
scores and normalization weights in Equation 1:
instead of performing a sum over the weighted
scores, we take the maximum score. We denote
this baseline as Max-Norm .
Evaluation method. Following Guerreiro et al.
(2022a), we report Area Under the Receiver Oper-
ating Characteristic curve (AUROC) as our primary
metric, and False Positive Rate at 90% True Posi-
tive Rate (FPR@90TPR) as a secondary metric.
Implementation details. ForLFAN-H ALL, we
normalize the metrics by leveraging the held-out
set released with the dataset consisting of 100,000
non-annotated in-domain scores. In the case of
HALOMI, however, no held-out set was released.
As such, we rely on sampling random splits that
consist of 10% of the dataset for calibration. Werepeat the process 10 different times. We report
average scores over those different runs. We also
report the performance variance in the Appendix.
3.2 Performances Analysis
Results on hallucination detection performance on
LFAN-H ALL and HaloMNI are reported in Table 1.
Global Analysis. STARE aggregation method
consistently outperforms (i) single detectors’ per-
formance, and (ii) other aggregation baselines.
Moreover, we find that the combination of all de-
tectors — both model-based and external-based de-
tectors — yields the best overall results, improving
over the STARE method based on either internal
or external models only. Importantly, these trends,
contrary to other alternative aggregation strategies,
hold across both datasets.
Aggregation of External Detectors. STARE
demonstrates robust performance when aggregat-
ing external detectors on both LFAN-H ALL and
HALOMI: improvements in AUROC (over a point)
and in FPR (between two to six points). Interest-
ingly, we also observe that the best overall perfor-
mance obtained exclusively with external models
lags behind that of the overall aggregation. This
suggests that internal models features — directly
obtained via the generation process — contribute
with complementary information to that captured
by external models.
Aggregation of Internal Detectors. Aggrega-
tion of internal detectors, can achieve higher AU-
ROC scores than the best single external detector
onHALOMI. This results highlights how model-
based features — such as attention and sequence
log-probability — that are readily and efficiently
obtained as a by-product of the generation can,
when aggregated effectively, outperform more com-
putationally expensive external solutions.
3.3 Ablation Studies
In this section, our focus is two-fold: (i) exploring
optimal selections of detectors, and (ii) understand-
ing the relevance of the reference set’s size.
Optimal Choice of detectors. We report the
performance of the optimal combination of N-
detectors on both datasets in Table 2.4We note
that including all detectors yields comparable per-
formance to the best mix of detectors. Interest-
4We report the optimal combinations in Appendix C.
Figure 1: Impact of reference set size on LFAN-H ALL.
ingly, aggregation always brings improvement,
even when only combining two detectors. As ex-
pected, the best mixture of detectors leverages in-
formation from different signals: contribution of
source contribution, low-quality translations, and
dissimilarity between source and translation.
LFAN-H ALL HALOMI
N AUROC FPR@90 AUROC FPR@90
LaBSE 91.72 26.86 88.77 34.96
2 93.32 20.67 90.40 27.52
3 94.11 17.27 90.61 27.24
4 94.45 13.69 91.09 26.91
5 94.12 17.06 91.25 28.48
6 — — 91.40 27.93
STARE 94.12 17.06 91.18 28.85
Table 2: Ablation Study on the Optimal Choice of De-
tectors when using STARE .
Impact of the size of the references set. The
calibration of scores relies on a reference set. Here,
we examine the impact of the calibration set size
on performance, by ablating on the held-out set
LFAN-H ALL, which comprises of 100k sentences.
Figure 1 shows that the ISOLATION FOREST re-
quires a larger calibration set to achieve similar
performance. This phenomenon might explain the
drop in performance observed on HALOMI(Ta-
ble 1). Interestingly, the performance improvement
forSTARE , particularly in FPR, plateaus when the
reference set exceeds 1,000 samples, which sug-
gests that STARE can adapt to different domains
with a rather small reference set.
4 Conclusion & Future Perspectives
We propose a simple aggregation method to com-
bine hallucination detectors to exploit complemen-
tary benefits from each individual detector. We
show that our method can bring consistent improve-
ments over previous detection approaches in two
human-annotated datasets across different language
pairs. We are also releasing our code and detection
scores to support future research on this topic.5 Limitations
Our methods are evaluated in a limited setup due to
the limited availability of translation datasets with
annotation of hallucinations. Moreover, in this
study, we have not yet studied compute-optimal
aggregation of detectors — we assume that we
already have access to multiple different detection
scores.
6 Acknowledgements
Training compute is obtained on the Jean Zay super-
computer operated by GENCI IDRIS through com-
pute grant 2023-AD011014668R1, AD010614770
as well as on Adastra through project c1615122,
cad15031, cad14770 .
References
Mikel Artetxe and Holger Schwenk. 2019. Mas-
sively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond. Transactions
of the Association for Computational Linguistics ,
7:597–610.
Ondˇrej Bojar, Christian Federmann, Mark Fishel, Yvette
Graham, Barry Haddow, Matthias Huck, Philipp
Koehn, and Christof Monz. 2018. Findings of the
2018 conference on machine translation (WMT18).
InProceedings of the Third Conference on Machine
Translation: Shared Task Papers , pages 272–303,
Belgium, Brussels. Association for Computational
Linguistics.
Pierre Colombo, Eduardo Dadalto Câmara Gomes, Guil-
laume Staerman, Nathan Noiry, and Pablo Piantanida.
2022. Beyond mahalanobis distance for textual ood
detection. In NeurIPS 2022 .
Pierre Colombo, Marine Picot, Nathan Noiry, Guil-
laume Staerman, and Pablo Piantanida. 2023. To-
ward stronger textual attack detectors. Findings
EMNLP 2023 .
David Dale, Elena V oita, Loïc Barrault, and Marta R
Costa-jussà. 2022. Detecting and mitigating halluci-
nations in machine translation: Model internal work-
ings alone do well, sentence similarity even better.
arXiv preprint arXiv:2212.08597 .
David Dale, Elena V oita, Janice Lam, Prangthip
Hansanti, Christophe Ropers, Elahe Kalbassi, Cyn-
thia Gao, Loïc Barrault, and Marta R Costa-jussà.
2023. Halomi: A manually annotated bench-
mark for multilingual hallucination and omission
detection in machine translation. arXiv preprint
arXiv:2305.11746 .
Maxime Darrin, Pablo Piantanida, and Pierre Colombo.
2023a. Rainproof: An umbrella to shield text genera-
tors from out-of-distribution data. EMNLP 2023 .Maxime Darrin, Guillaume Staerman, Eduardo
Dadalto Câmara Gomes, Jackie CK Cheung, Pablo
Piantanida, and Pierre Colombo. 2023b. Unsuper-
vised layer-wise score aggregation for textual ood
detection. arXiv preprint arXiv:2302.09852 .
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-
vazhagan, and Wei Wang. 2022. Language-agnostic
BERT sentence embedding. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
878–891, Dublin, Ireland. Association for Computa-
tional Linguistics.
Javier Ferrando, Gerard I. Gállego, Belen Alastruey,
Carlos Escolano, and Marta R. Costa-jussà. 2022.
Towards opening the black box of neural machine
translation: Source and target interpretations of the
transformer. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Process-
ing, pages 8756–8769, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Nuno M. Guerreiro, Duarte M. Alves, Jonas Waldendorf,
Barry Haddow, Alexandra Birch, Pierre Colombo,
and André F. T. Martins. 2023. Hallucinations in
Large Multilingual Translation Models. Transac-
tions of the Association for Computational Linguis-
tics, 11:1500–1517.
Nuno M Guerreiro, Pierre Colombo, Pablo Piantanida,
and André FT Martins. 2022a. Optimal transport for
unsupervised hallucination detection in neural ma-
chine translation. arXiv preprint arXiv:2212.09631 .
Nuno M Guerreiro, Elena V oita, and André FT Martins.
2022b. Looking for a needle in a haystack: A com-
prehensive study of hallucinations in neural machine
translation. arXiv preprint arXiv:2208.05309 .
Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Proceedings
of the First Workshop on Neural Machine Translation ,
pages 28–39, Vancouver. Association for Computa-
tional Linguistics.
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008.
Isolation forest. In 2008 eighth ieee international
conference on data mining , pages 413–422. IEEE.
NLLB Team, Marta R. Costa-jussà, James Cross, Onur
Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco Guzmán, Philipp
Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
2022. No language left behind: Scaling human-
centered machine translation.Ethan Perez, Saffron Huang, Francis Song, Trevor Cai,
Roman Ring, John Aslanides, Amelia Glaese, Nat
McAleese, and Geoffrey Irving. 2022. Red team-
ing language models with language models. arXiv
preprint arXiv:2202.03286 .
Marine Picot, Federica Granese, Guillaume Staerman,
Marco Romanelli, Francisco Messina, Pablo Pi-
antanida, and Pierre Colombo. 2023a. A halfspace-
mass depth-based method for adversarial attack de-
tection. TMLR 2023 .
Marine Picot, Nathan Noiry, Pablo Piantanida, and
Pierre Colombo. 2023b. Adversarial attack detec-
tion under realistic constraints.
Marine Picot, Guillaume Staerman, Federica Granese,
Nathan Noiry, Francisco Messina, Pablo Piantanida,
and Pierre Colombo. 2023c. A simple unsupervised
data depth-based method to detect adversarial im-
ages.
Vikas Raunak, Arul Menezes, and Marcin Junczys-
Dowmunt. 2021. The curious case of hallucinations
in neural machine translation. In Proceedings of
the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1172–1183,
Online. Association for Computational Linguistics.
Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro,
Chrysoula Zerva, Ana C Farinha, Christine Maroti,
José G. C. de Souza, Taisiya Glushkova, Duarte
Alves, Luisa Coheur, Alon Lavie, and André F. T.
Martins. 2022. CometKiwi: IST-unbabel 2022 sub-
mission for the quality estimation shared task. In
Proceedings of the Seventh Conference on Machine
Translation (WMT) , pages 634–645, Abu Dhabi,
United Arab Emirates (Hybrid). Association for Com-
putational Linguistics.
Rico Sennrich, Jannis Vamvas, and Alireza Moham-
madshahi. 2024. Mitigating hallucinations and off-
target machine translation with source-contrastive
and language-contrastive decoding.
Guillaume Staerman, Pavlo Mozharovskyi, Pierre
Colombo, Stéphan Clémençon, and Florence d’Alché
Buc. 2021. A pseudo-metric between probability dis-
tributions based on depth-trimmed regions. TMLR
2024 , pages arXiv–2103.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Yijun Xiao and William Yang Wang. 2021. On hal-
lucination and predictive uncertainty in conditional
language generation. In Proceedings of the 16th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Main Volume , pages
2734–2744, Online. Association for Computational
Linguistics.A Model and Data Details
A.1 L FAN-H ALLdataset
NMT Model. The model used in Guerreiro et al.
(2022b) is a Transformer base model (Vaswani
et al., 2017) (hidden size of 512, feedforward size
of 2048, 6 encoder and 6 decoder layers, 8 atten-
tion heads). The model has approximately 77M
parameters. It was trained on WMT18 DE-ENdata:
the authors randomly choose 2/3 of the dataset for
training and use the remaining 1/3 as a held-out set
for analysis. We use a section of that same held-out
set in this work.
Dataset Stats. The dataset consists of 3415 trans-
lations from WMT18 DE-ENdata. Overall, there
are 218 translations annotated as detached halluci-
nations (fully and strongly detached — see more
details in Guerreiro et al. (2022b)), and 86 as oscil-
latory hallucinations.5The other translations are
either incorrect (1073) or correct (2048). We show
examples of hallucinations for each category in
Table 4.6
A.2 H ALOMIdataset
NMT model. Translations on this dataset come
from 600M distilled NLLB model (NLLB Team
et al., 2022).
B Variance of performance on the
HALOMIdataset
We report in Table 3 the average performance as
well as the standard deviation across the differ-
ent ten runs on different calibration sets. Despite
variance between different runs, the STARE aggre-
gation method consistently outperforms individual
detectors and other aggregation techniques.
C Optimal Combination of Detectors via
STARE
LFAN-H ALL.The optimal set of detectors for
various values of Nis:
• for N= 1:LaBSE
• for N= 2:CometKiwi ,LaBSE
5Some strongly detached hallucinations have also been
annotated as oscillatory hallucinations. In these cases, we
follow Guerreiro et al. (2022a) and consider them to be oscil-
latory.
6All data used in this paper is licensed under a MIT Li-
cense.DETECTOR AUROC ↑ FPR@90TPR ↓
Individual Detectors
External
COMET-QE 82.22±0.28 47.40 ±0.82
LASER 81.11±0.21 47.04 ±0.78
XNLI 82.44±0.18 33.20 ±0.63
LaBSE 88.77±0.21 34.96 ±0.72
Model-based
Seq-Logprob 86.72±0.22 28.86 ±0.64
ALTI+ 82.26±0.28 58.40 ±0.54
Wass-Combo 64.82±0.20 84.62 ±0.52
Aggregated Detectors
External Only
Isolation Forest 71.35±1.62 57.75 ±4.55
Max-Norm 88.57±0.38 32.59 ±0.60
STARE 89.76±0.19 32.74 ±0.50
Model-based Only
Isolation Forest 75.35±2.32 69.71 ±5.01
Max-Norm 67.70±1.31 83.83 ±1.40
STARE 89.92±0.20 30.37 ±1.84
All
Isolation Forest 76.25±2.16 56.28 ±6.29
Max-Norm 80.67±1.37 41.52 ±5.87
STARE 91.18±0.20 28.85 ±0.89
Table 3: Performance of individual and aggregated hal-
lucination detectors on the HALOMIdataset, including
average performance and standard deviations across ten
different calibration sets.
•forN= 3 :Wass_Combo ,CometKiwi ,
LaBSE
•forN= 4 :ALTI+ ,Wass_Combo ,
CometKiwi ,LaBSE
•forN = 5 :ALTI+ ,SeqLogprob ,
Wass_Combo ,CometKiwi ,LaBSE
HALOMI.The optimal set of detectors for vari-
ous values of Nis:
• for N= 2:LaBSE ,SeqLogprob
•forN= 3 :LaBSE ,SeqLogprob ,Wass-
Combo
•forN= 4 :LaBSE ,SeqLogprob ,XNLI ,
COMET-QE
•forN= 5 :LaBSE ,SeqLogprob ,XNLI ,
COMET-QE ,ALTI+
•forN= 6:LaBSE ,Log Loss ,XNLI ,COMET-
QE,ALTI+ ,Wass-Combo
•forN= 7 :LaBSE ,SeqLogprob ,XNLI ,
COMET-QE ,ALTI+ ,Laser ,Wass-ComboCategory Source Sentence Reference Translation Hallucination
OscillatoryAls Maß hierfür wird meist der sogenannte Pearl
Index benutzt (so benannt nach einem Statistiker,
der diese Berechnungsformel einführte).As a measure of this, the so-called Pearl Index
is usually used (so named after a statistician
who introduced this calculation formula).Theterm "Pearl Index" refers to theterm
"Pearl Index" (or "Pearl Index") used to
refer to theterm "Pearl Index" (or "Pearl
Index").
Strongly
DetachedFraktion der Grünen / Freie Europäische AllianzThe Group of the Greens/European Free Al-
lianceIndependence andDemoc racy Group (in-
cludes 10UKIP MEPs andoneindepen-
dent MEP from Ireland)
Fully
DetachedDie Zimmer beziehen, die Fenster mit Aussicht
öffnen, tief durchatmen, staunen.Head up to the rooms, open up the windows
and savour the view, breathe deeply, marvel.Thestaff were very friendly andhelpful.
Table 4: Examples of hallucination types. Hallucinated content is shown shaded.
D Future Work & Perspectives
In the future, we would like to explore more
anomaly detection methods to improve the aggre-
gation quality. Specifically, we would like to test
Information Projections (Picot et al., 2023c; Dar-
rin et al., 2023a) and data depths (Picot et al.,
2023b,a; Colombo et al., 2022; Staerman et al.,
2021; Colombo et al., 2023).