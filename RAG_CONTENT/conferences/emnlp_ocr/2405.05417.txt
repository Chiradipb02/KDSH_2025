Fishing for Magikarp: Automatically Detecting Under-trained Tokens
in Large Language Models
Sander Land
Cohere
sander@cohere.comMax Bartolo
Cohere
max@cohere.com
Abstract
The disconnect between tokenizer creation
and model training in language models al-
lows for speciÔ¨Åc inputs, such as the infa-
mous _SolidGoldMagikarp token, to induce
unwanted model behaviour. Although such
‚Äòglitch tokens‚Äô, tokens present in the tok-
enizer vocabulary but that are nearly or en-
tirely absent during model training, have been
observed across various models, a reliable
method to identify and address them has been
missing. We present a comprehensive analysis
of Large Language Model tokenizers, specif-
ically targeting this issue of detecting under-
trained tokens. Through a combination of to-
kenizer analysis, model weight-based indica-
tors, and prompting techniques, we develop
novel and effective methods for automatically
detecting these problematic tokens. Our Ô¨Ånd-
ings demonstrate the prevalence of such tokens
across a diverse set of models and provide in-
sights into improving the efÔ¨Åciency and safety
of language models.
https://github.com/cohere-ai/magikarp/
1 Introduction
Large Language Models (LLMs) have undergone
remarkable advancements, becoming increasingly
capable of understanding and generating human-
like text. While most components of these mod-
els are trained in an unsupervised fashion on vast
amounts of data, the tokenizer typically remains
a separately trained component based on custom
algorithms and smaller datasets.
GPT-2 laid the foundation for much of current-
day transformer-based language modelling ( Rad-
ford et al. ,2019 ), including a framework for tok-
enization building on previous work in byte-pair
encoding (BPE) ( Sennrich et al. ,2016 ), that has
since been widely adopted. Tokenization using
BPE converts input text to a sequence of subword
tokens by iteratively merging two neighbouring to-
kens using a Ô¨Åxed set of merge rules. These rules
                 Fix the syntax error. Change nothing else: 
locatorSection.AddLanguageSpecificText("glitch!" 
Chatbot üè≠
.–ª–µ–∫–æ–ø–∏—Ç Fawcett("glitch!") üòà
ü§ñ
User Figure 1: Illustrative example of ‚Äòglitch‚Äô tokens.
are learned using a greedy training algorithm on a
smaller dataset, which is ideally representative of
the LLM‚Äôs training data. Recent work in this area
has primarily focused on techniques to remove the
need for tokenization altogether by moving to raw
byte input ( Xue et al. ,2022 ). This choice typically
comes at a signiÔ¨Åcant inference speed cost, which
can be compensated for by specialized architec-
tures at the initial and Ô¨Ånal layers ( Yu et al. ,2023 ),
or variable compute at intermediate layers ( Sla-
gle,2024 ). However, these techniques have not
been widely adopted, and the vast majority of con-
temporary models still rely on subword tokeniza-
tion. The main alternative to BPE for subword to-
kenization is the Unigram method ( Kudo ,2018 ),
which despite work suggesting it outperforms BPE
(Bostrom and Durrett ,2020 ) is not in common use.
For an in-depth overview of tokenization methods
and their history, see Mielke et al. (2021 ).
Despite its widespread use, the tokenization
step has generally been found to be unsatisfac-
tory, responsible for many unwanted LLM be-
haviours ( Karpathy ,2024 ). The disconnect be-
tween tokenizer and model training creates the po-
tential for some tokens to rarely or never be seen
in training. The presence of such tokens in model
inputs can lead to unexpected model behaviour in-
cluding hallucination or the generation of garbled
outputs, leading to such tokens commonly being
referred to as ‚Äòglitch tokens‚Äô ( Geiping et al. ,2024 ).
We refer to these as ‚Äòunder-trained‚Äô or ‚Äòuntrained‚Äô
tokens, reserving the latter term only for cases in
which we have clear indication that the speciÔ¨Åc to-
ken had no model training data occurrences.The presence of such under-trained tokens has
several drawbacks. Firstly, they occupy capac-
ity in a Ô¨Åxed-size tokenizer that could be better
utilized for more frequently occurring tokens, re-
ducing average input/output length and inference
costs. Secondly, their deliberate or accidental pres-
ence in input data has the potential to cause un-
wanted model outputs and break downstream ap-
plications. Robustness to such unexpected or mali-
cious input data is increasingly important with the
proliferation of tool use and agents in LLMs that
retrieve and process external data. Lastly, these
tokens can potentially be exploited to more eas-
ily circumvent guardrails by pushing the model be-
yond its trained distribution ( Geiping et al. ,2024 ).
Although previous work exists on identifying
such tokens through model and tokenizer analy-
sis (Rumbelow and Watkins ,2023 ;Watkins and
Rumbelow ,2023 ;Fell,2023 ), there is a lack of re-
liable, automated methods that are well-tested and
perform consistently across a wide range of mod-
els. Automated tools for detecting tokenizer issues
provide ways to test and iteratively improve the
development of tokenizers, and can also provide
methods for protecting deployed models from un-
wanted inputs, for example through sanitization.
In this work, we present effective and efÔ¨Åcient
techniques for identifying such problematic tokens
based on the model embedding weights and tok-
enizer conÔ¨Åguration. We apply these methods to
a wide range of popular and recent open-weight
models. Finally, we include a brief exploration
of extensions of these techniques to closed-source
models. To the best of our knowledge, this is the
Ô¨Årst work to present a set of automated, efÔ¨Åcient,
and theoretically sound methods that systemati-
cally and demonstrably identify ‚Äôglitch‚Äô tokens
across various models and tokenizers. We also
publish a general analysis tool compatible with
Hugging Face models ( Wolf et al. ,2020 ), along
with detailed results for each analyzed model.
2 Methods
Our approach consists of three main steps: i) First,
we perform an in-depth tokenizer analysis by in-
specting its vocabulary and observing its encod-
ing and decoding behaviour, ii) Second, we calcu-
late several indicators to identify candidate under-
trained tokens, and iii) Third, we verify whether
identiÔ¨Åed candidate tokens are indeed out of distri-
bution by prompting the target model.2.1 Tokenizer analysis
To aid our analysis, we start by deÔ¨Åning a number
of useful token categories.
PARTIAL UTF-8 SEQUENCES are tokens repre-
senting byte sequences that cannot be converted
to Unicode characters as they contain only part
of the full UTF-8 encoding for a character. This
is typical for ‚Äòfallback byte‚Äô tokens in the 0x80 -
0xFF range but can also include tokens with other
partial Unicode characters, depending on whether
BPE was applied directly at the byte level.
UNREACHABLE TOKENS are those that are
never produced as a result of tokenizing text. We
test this by checking if decoding a token to a string,
and re-tokenizing this string, results in the original
token ID. Such tokens are typically the result of to-
kenizer conÔ¨Åguration errors or conÔ¨Çicts between
trained and manually added tokens. As this test
does not work when tokens cannot be decoded to
a string, we exclude partial UTF-8 sequences from
this category.
SPECIAL TOKENS are manually-deÔ¨Åned tokens
that typically bypass the standard pre-tokenization
pipeline, and often serve speciÔ¨Åc purposes as con-
trol tokens, such as <s>, which typically marks
the beginning of an input sequence. We identify
special tokens using the patterns <...> and[...]
and list them separately from unreachable tokens,
even if they may also be considered unreachable
due to input sanitization in preprocessing.
We detect and exclude partial UTF-8 sequences
and unreachable tokens from our under-trained to-
ken detection pipeline, as they are not suitable for
automatically building veriÔ¨Åcation prompts. Our
published model reports include separate tables
with these tokens, and we brieÔ¨Çy discuss some in-
teresting model-speciÔ¨Åc results in section 3.2.
2.2 Under-trained token indicators
This section outlines our model architecture-
dependent indicators, which we use to identify
under-trained token candidates. A key distinction
is made based on whether or not a model uses ‚Äòtied‚Äô
embeddings ( Inan et al. ,2017 ), that is, the model
uses the same matrix for its input embeddings Ein
and the output embeddings matrix Eoutin the Ô¨Å-
nal ‚Äòlanguage modelling head‚Äô layer1. Regardless
of whether tied embeddings are used, all weights
of the output embeddings inÔ¨Çuence the token pre-
dictions at every training step. SpeciÔ¨Åcally, all
1We assume the common setup with no bias term.untrained tokens will experience similar updates
in training, ‚Äòmoving away‚Äô from the mean output
vector of the model ( Bi¬¥s et al. ,2021 ). Thus, we
can expect to Ô¨Ånd that under-trained token embed-
dings share a similar direction in output embed-
ding space, and we can use this to identify them
based on the distance to the embeddings of refer-
ence untrained tokens. This common direction can
also be interpreted as a learnt constant vector that
is shared between the residual stream and certain
output embeddings, allowing the model to reliably
generate highly negative logits for tokens that are
never the correct prediction To calculate the indi-
cators based on the output embeddings Eout, we
start by deÔ¨Åning a set of known untrained or highly
under-trained embedding indices tref, e.g. the to-
ken IDs for tokens such as <unused_token123> ,
or the space of embeddings above the tokenizer
vocabulary size.
Next, we calculate the mean unused token em-
bedding vector to serve as a reference:
uref=1
|tref|‚àë
i‚ààtrefEout,i
Finally, we take the cosine distances C(Eout, u ref)
between this mean unused embedding vector and
rows in Eout, where C(A, x)is the vector of co-
sine distances between xand rows in matrix A:
C(A, x)i= 1‚àíAi¬∑x
‚à•Ai‚à•‚à•x‚à•
In addition to the cosine distance between out-
put embeddings, we also calculate and visual-
ize the Euclidean distance between output embed-
dings and the untrained reference L2(Eout‚àíuref)
where L2(A)i=‚à•Ai‚à•. Finally, we also test more
complex output embedding indicators which com-
pensate for the possibility of a common directional
bias being present in all embeddings. These exper-
iments, which indicate that the simpler formula-
tion is sufÔ¨Åcient, are outlined in Appendix A.
When embeddings are not tied, input embed-
dings for tokens which do not appear in the in-
put for a training step are only affected by a po-
tential weight decay term. If weight decay is ap-
plied to the input embedding matrix, the embed-
dings corresponding to under-trained tokens will
tend to zero as training progresses. Alternatively,
they will stay at a (typically low) initial value. The
norm of the input embeddings thus provides an
additional indicator of under-trained tokens withpotentially higher sensitivity, and which conve-
niently does not require a set of previously known
untrained tokens. SpeciÔ¨Åcally, we expect that this
indicator will not predict control tokens (such as
<s>) that are only seen in inputs.
Thus, for models with tied embeddings, we use
the cosine distance-based indicator C(Eout, u ref)
to select candidate tokens. For models without
tied embeddings, we use the norm of Ein, denoted
L2(Ein), and additionally calculate and visualize
all output embedding-based indicators.
2.3 VeriÔ¨Åcation of candidate tokens
The indicators we propose provide a natural rank-
ing of candidate under-trained tokens, but do not
give a deÔ¨Ånitive selection threshold. Their rela-
tive simplicity, while desired, is also likely to re-
sult in a somewhat noisy relation between indica-
tor scores and model behaviour. To conÔ¨Årm that
candidate tokens indeed induce unwanted model
outputs, we verify all tokens that rank among the
most likely 2% according to the chosen indicator,
excluding partial UTF-8 sequences and unreach-
able tokens. This veriÔ¨Åcation process involves con-
structing speciÔ¨Åc repetitive prompts that induce
a high output probability for normal tokens, and
checking if a particular candidate token has a very
low (<1%) output probability. See Appendix B
for details of parameters and model prompts.
2.4 Effectiveness of token indicators
We validate our proposed indicators by relating
them to both model behaviour and training data
statistics. Although such training data statistics
are rarely publicly available, we are able to run a
comprehensive three-way comparison on the open
OLMo v1.7 model ( Groeneveld et al. ,2024 ). Fig-
ure2shows a strong correlation between all pro-
posed indicators and training data, not only pre-
dicting under-trained tokens, but extending to the
entire range of token frequencies. Applying our
veriÔ¨Åcation step to all tokens shows that, despite
their relative simplicity, our indicators are highly
predictive of the maximal token output probabil-
ity (Figure 3). More precisely, 191 out of 49,575
tokens pass our veriÔ¨Åcation step, compared to 175
of 993 when testing only the top 2% candidate
tokens, validating that the 2% threshold is a rea-
sonable trade-off between computational cost and
the ability to detect the majority of highly under-
trained tokens. Finally, Figure 4shows examples
of the visualizations we perform on all model in-dicators. These show a clear secondary peak near
zero across models that contain the under-trained
tokens, as well as high correlation between alter-
native indicators, further validating their effective-
ness.
3 Results
In this section, we present a summary of our key
Ô¨Åndings on under-trained token detection. Table 1
presents veriÔ¨Åcation statistics and examples of ver-
iÔ¨Åed under-trained tokens for a wide range of mod-
els. The number of veriÔ¨Åed tokens varies signif-
icantly across different model families and tok-
enizer vocabulary sizes, and also depends on the
number of unused special tokens that a model‚Äôs to-
kenizer allows as plain-text input. The percentage
of veriÔ¨Åed tokens typically ranges between 5‚Äì50%
of tested candidate tokens, corresponding to 0.1‚Äì
1% of the total vocabulary size.
Given the model-speciÔ¨Åc nature and the exten-
sive volume of results, we elaborate on some com-
mon Ô¨Åndings and showcase representative exam-
ples for particular models. Comprehensive reports
covering an increasing number of tested models
and token types are available in our repository.
3.1 Common observations
Although many of our Ô¨Åndings are dependent on
model-speciÔ¨Åc details such as tokenizer training
and conÔ¨Åguration, model architecture, and train-
ing data, there are a number of commonalities that
appear across many different models.
3.1.1 Single-byte tokens
Tokens representing a single byte are a common
source of untrained tokens. The most common oc-
currences are the bytes 0xF5 ‚Äì0xFF which are not
used in UTF-8 encoded text2, and are a convenient
source for quickly locating reference untrained to-
kens for indicators that require them. In addition,
many tokenizers including those from the Gemma,
Llama2 and Mistral families include every byte as
a token, with many of them in the normal ASCII
range 0x00 ‚Äì0x7F being redundant and unreach-
able due to the existence of a token for the cor-
responding character.
These issues are not universal, and we also
Ô¨Ånd models which include precisely the 243 bytes
used in UTF-8 as tokens. Untrained single byte
tokens are typically classiÔ¨Åed as ‚Äòpartial UTF-8
2See Appendix Cfor a primer on UTF-8 encoding.sequences‚Äô or ‚Äòunreachable‚Äô, and our indicators
are effective in revealing which ones are never or
rarely seen in model training.
3.1.2 Intermediate BPE fragments
All tested models use BPE-based tokenization,
which retains the original tokens after a merge, of-
ten causing intermediate ‚Äòjunk‚Äô tokens ( Bostrom
and Durrett ,2020 ). When mentioning exam-
ples of such as under-trained fragments, we de-
note the more complete token in parentheses, e.g.
_TheNitrome (_TheNitromeFan ) in the GPT-2 to-
kenizer. In some instances, the longest token is
also under-trained, along with a variety of frag-
ments. The same mechanism appears to explain
many under-trained partial UTF-8 sequences in
byte-level BPE tokenizers, with multiple bytes be-
ing merged over several steps, potentially leaving
multiple intermediate tokens with partial Unicode
characters.
3.1.3 Special tokens
Many models include untrained special tokens,
such as <pad> ,<unk> , or <|unused_123|> . In
the following discussion we generally omit men-
tioning them, unless their status as an (un)trained
token is particularly surprising, as their inclusion
in the tokenizer and training data is typically delib-
erate, for purposes such as the ability to Ô¨Åne-tune
models without changing tokenizers. One com-
mon observation is that, on many occasions, to-
kens such as <mask> , which we expect to be com-
pletely untrained, nevertheless appear to have been
seen in training. A likely cause is code reposito-
ries or guides about language models using these
tokens in normal text, along with tokenizers allow-
ing such special control tokens in input text.
Special tokens can be unreachable due to input
sanitization as well as conÔ¨Åguration errors. In par-
ticular, both the Gemma and Yi models include
special tokens relating to HTML tags, which were
initially detected as unreachable, with the tags be-
ing split up in pre-tokenization.3
3.2 Model-speciÔ¨Åc observations
In this section we highlight notable model-speciÔ¨Åc
observations, grouped by the tokenizer used.
These examples are mainly intended to illustrate
the variety of different under-trained tokens and
3The Gemma team released a Ô¨Åx in response to our report,
and the 01.AI team advise not to use the ‚Äòfast‚Äô version. Our re-
ported results are based on the latest recommended versions.Model #Tokens Tied Emb. #ConÔ¨Årmed Examples
GPT-2 Medium (0.4B) 50,257 Yes 49/999 InstoreAndOnline reportprint _externalToEVA
GPT-2 XL (1.5B) 50,257 Yes 67/999 InstoreAndOnline _RandomRedditor embedreportprint
GPT-J 6B 50,400 No‚àó200/999 _attRot _externalToEVA _SolidGoldMagikarp
Phi-2 (2.7B) 50,295 No‚àó103/999 DragonMagazine _TheNitrome _SolidGoldMagikarp
Pythia 6.7B 50,277 No 14/993 FFIRMED _taxp _affidav
GPT-NeoX 20B 50,277 No 10/993 FFIRMED _taxp _affidav
OLMo v1.7 7B 50,280 No 178/993 _¬ß\[ medscimonit FFIRMED _[****
Llama2 7B 32,000 No 20/639 _Mediabestanden _Port√°ly oreferrer
Llama2 70B 32,000 No 32/639 _Mediabestanden _Port√°ly ederb√∂rd
Mistral 7B v0.3 32,000 No 53/637 \uefc0 });\r ·•Ä>?[< _febbra _uitgen
Mixtral 8x7B 32,000 No 44/637 \uefc0 _/**\r ·•Ä];\r
Rakuten 7B 48,000 No 66/957 \uefc0 _/**\r ·•Ä_febbra Á®≤Áî∞Â§ßÂ≠¶
Qwen1.5 32B 151,646 No 2450/2966 _ForCanBeConvertedToF (stypy $PostalCodesNL
Qwen1.5 72B Chat 151,646 No 2047/2968 _ForCanBeConverted useRalative _typingsJapgolly
StableLM2 12B 100,288 No 138/1997 _ForCanBeConverted \tTokenNameIdentifier _StreamLazy
Llama3 8B 128,256 No 556/2540 _ForCanBeConverted –é—ã—üN–é—ã—üN _CLIIIK krvldkf Í∏ÄÏÉÅÏúÑ
Llama3 70B 128,256 No 462/2540 $PostalCodesNL –∏—Ç–∏—Å—è ƒ±lmaktadƒ±r „Éº„Ç∑„Éß„É≥ ;\r\r\r\n
Command R (35B) 255,029 Yes 306/5012 AddLanguageSpecificText _ARStdSong ÁõÆÂâçÂ∞öÊú™Áî±‰∫∫Â∑•Âºï
Command R+ (104B) 255,029 Yes 75/5012 AddLanguageSpecificText tocguid ephritidae
Gemma 2B 256,000 Yes 3161/5117 ‡§ø‡§Ç‡§πÕÜ‡§¶‡•Ä‡§ñ‡§∞‡•Ä‡§¶‡§æ‡§∞‡•Ä ÀÜ(@)$_ _coachTry _AcceptedLoading ICTOGRAM
Gemma 7B 256,000 Yes 800/5117 ‡§ø‡§Ç‡§πÕÜ‡§¶‡•Ä‡§ñ‡§∞‡•Ä‡§¶‡§æ‡§∞‡•Ä EnglishChoose _ que≈øto _stockfotografie êÅò
Starcoder2 15B 49,152 No 128/968 ittrLoremipumdolorsitametconsecteturadipiscingelitIntegervelvel
Yi 9B 64,000 No 245/1278 \\+::\\+ mcited mabaochang nConsequently
Jamba v0.1 (52B) 65,536 No 6/1280 derrelsc ]{}]{} ronicsystems
Table 1: Detection of under-trained tokens. #ConÔ¨Årmed are the conÔ¨Årmed/tested numbers for the tokens tested
in veriÔ¨Åcation that are predicted with a maximal probability of <1%across veriÔ¨Åcation prompts. Examples were
manually chosen for readability, similarity across models or for being particularly striking. Note that the leading
‚Äò_‚Äô in tokens such as _SolidGoldMagikarp indicates a leading space.
‚àóThese models include a bias term in their Ô¨Ånal layer, which does not affect our results as we use their input embeddings.
Figure 2: Under-trained token indicators are highly predictive of training data. The embedding-based under-trained
token indicators for the OLMo v1.7 7B model and the number of times each token appears in the Ô¨Årst epoch of
training are shown. All indicators correlate strongly with the number of times a token is seen in training, not only
at the expected lower values, but extending across ten orders of magnitude.Figure 3: Under-trained token indicators are predictive
of veriÔ¨Åcation probability. The rate of successful veri-
Ô¨Åcation ( p <0.01) correlates very highly with our pro-
posed indicator, with no false positives at low values
of the indicator scores and a low rate of false negatives.
The dotted vertical line indicates the default 2% thresh-
old used for veriÔ¨Åcation.
Figure 4: Comparison of indicators. The scatter plots
are coloured by token ID, from light green to dark blue.
Top: Rakuten 7B showing a separate cluster for added
tokens, and high correlation near zero, showing that
the two different indicators are similar in effectiveness.
Bottom : In density plots, a clear peak appears near zero
for most models, giving rise to a bimodal distribution.conÔ¨Åguration issues that can be identiÔ¨Åed using
our methods, and are not exhaustive.
GPT-2 (Radford et al. ,2019 ) introduced the
framework for much of current-day LLM de-
velopment, and the tokenizer has been re-used
extensively. We conÔ¨Årm previous Ô¨Åndings
with a signiÔ¨Åcant number of tokens related to
(fragments of) usernames (e.g. _TheNitrome ,
_RandomRedditor ). We also Ô¨Ånd a number of
under-trained non-English tokens. Additionally,
all ASCII control characters except for the new-
line character, but including the tab and carriage re-
turn characters, appear untrained. This suggests a
potential mismatch in data normalization between
training and inference.
GPT-J 6B (Wang and Komatsuzaki ,2021 ) and
Phi-2 (Microsoft ,2023 ) are independent models
which both also use the GPT-2 tokenizer, and have
signiÔ¨Åcantly more under-trained tokens, likely
due to their training data being further removed
from the data used to train the tokenizer. These
additional tokens include _SolidGoldMagikarp ,
which is not among veriÔ¨Åed candidates in GPT-2.
GPT-NeoX is an open-source library and as-
sociated family of models that uses a tokenizer
with the same vocabulary size as GPT-2, but
trained on the same ‚ÄòThe Pile‚Äô dataset also used for
model training, and with added tokens for multiple
spaces ( Black et al. ,2022 ). The GPT-NeoX 20B
model has very few under-trained tokens, likely in
part due to this alignment between tokenizer and
model training, with the fragment FFIRMED show-
ing up most consistently. The Pythia 6.7B model
based on the same library ( Biderman et al. ,2023 )
also shows very similar results.
OLMo open language models ( Groeneveld
et al. ,2024 ) also use the GPT-NeoX tokenizer,
but have a much higher rate of under-trained to-
kens, including a wide range of punctuation-based
tokens. We also detect over 200 unreachable to-
kens representing combinations of spaces and line
breaks in the tokenizer, which appear to be caused
by the aforementioned ‚Äòmultiple spaces‚Äô tokens
taking precedence. However, many of them ap-
pear to have been seen in training, based on both
our indicators and training data statistics.4
Furthermore, we noticed that embedding-based
indicators are not near zero for the GPT-NeoX
and Pythia models, as well as v1 of the OLMo
4This was in part traced to a breaking change in
tokenizers v0.14 (Luca Soldaini, personal communication).model. For the GPT-NeoX/Pythia models, this is
explained by a speciÔ¨Åc implementation of weight
decay, where only weights that are used in the for-
ward pass are affected, but we Ô¨Ånd that having low
but non-zero embeddings is still a good predictor
of under-trained tokens. The OLMo v1 model in-
stead applies no weight decay, and requires using
output embedding-based indicators instead. How-
ever, the OLMo v1.7 model does apply weight de-
cay to embeddings, and its embedding norms are
near zero for untrained tokens (cf. Figure 2), and
we use only this more recent version in this work.
Llama2 models ( Touvron et al. ,2023 ) use
a relatively compact BPE tokenizer, and have
a low number of under-trained tokens, mostly
relating to long non-English words, including
_Mediabestanden ,_–†–∞—Å–ø–æ–¥–µ–ª–∞ , and _Port√°ly .
We also Ô¨Ånd under-trained intermediate fragments
such as _gepublic (_gepubliceerd ). Several of
these tokens were also found in previous work on
steering model outputs ( Geiping et al. ,2024 ).
Mistral models ( Jiang et al. ,2023 ,2024 ) use
a similar tokenizer, but its vocabulary includes a
signiÔ¨Åcant number of multi-character punctuation
sequences ending in a carriage return ( \r), which
are the main source of under-trained tokens. The
\uefc0 token representing a single unassigned
Unicode character in the ‚Äòprivate use area‚Äô is con-
sistently among the most under-trained, along with
·•Ä, a character from the Limbu script.
Rakuten 7B ( Rakuten Group et al. ,2024 ) is
a derived model with an extended vocabulary for
Japanese, and continued pre-training. Among the
extended vocabulary we Ô¨Ånd a few under-trained
fragments such as Á®≤Áî∞Â§ßÂ≠¶ (Êó©Á®≤Áî∞Â§ßÂ≠¶ , ‚ÄòWaseda
University‚Äô). Their presence is proportional to the
extended vocabulary, which forms a distinct clus-
ter when visualising their indicators (see Figure 4).
Gemma is a family of models by Google Deep-
mind ( Gemma Team et al. ,2024 ) and uses a large
256,000 token vocabulary, which includes a sig-
niÔ¨Åcant number of under-trained fragments in var-
ious scripts. Most notably we Ô¨Ånd many under-
trained tokens which contain ‚Äò ≈ø‚Äô (an archaic form
of ‚Äòs‚Äô in German), including _m√º≈ø≈øen , as well as
a number of translations of ‚Äòstock photos‚Äô such as
_stockbilder and_stockfotos .
Command R and R+ are models by Cohere
(2024 ) which also have a large multi-lingual vo-
cabulary with over 255,000 tokens. The most no-
table discovery in these models is that over 1,400
manually added tokens of emojis are categorizedas unreachable, and are all clearly untrained ac-
cording to the indicators. Additionally, among
partial UTF-8 sequences are several tokens related
to the English Ô¨Çag followed by invisible Unicode
‚Äòtag‚Äô characters, which we tracked to a conversion
step from image-based Ô¨Çags to emojis in an open-
source pipeline for parsing Wikipedia pages, po-
tentially affecting other models as well.5
The tiktoken library by OpenAI ( OpenAI ,
2024 ), includes the ‚Äòcl100k‚Äô tokenizer as used
in GPT-3.5/GPT-4 as well as several other mod-
els. This tokenizer use a pre-tokenization pattern
which allows not only a starting space, but many
other single punctuation characters at the start of
a token. This choice results in tokens such as
\tTokenNameIdentifier and $PostalCodesNL ,
which are highly sensitive to pre-tokenization split-
ting, with leading spaces before the token result-
ing in different tokenization. In combination with
their speciÔ¨Åc content, this is likely to have made
them more severely under-trained across models.
StableLM2 is a model by Stability AI ( Bella-
gente et al. ,2024 ) that uses a slightly modiÔ¨Åed ver-
sion of the ‚Äòcl100k‚Äô tokenizer. Due to the addition
of digit splitting, the original multi-digit tokens
were expected to show up as both unreachable and
untrained, but were initially only detected as un-
trained due to a tokenizer conÔ¨Åguration error.6
Qwen is a model family by Alibaba ( Bai et al. ,
2023 ) which signiÔ¨Åcantly extends the ‚Äòcl100k‚Äô to-
kenizer to over 150,000 tokens. The added to-
kens and large inherited tokenizer results in many
under-trained tokens, and among added tokens we
Ô¨Ånd archaic Chinese characters (such as ¨≥Ω) and
Korean characters which are typographically valid
but never seen in normal text (such as Ïïê).
Llama3 is a recent model family by Meta AI
(2024 ) which also extends this tokenizer with
28,000 additional tokens. Aside from sharing
many under-trained tokens with other models us-
ing the ‚Äòcl100k‚Äô tokenizer, the newly added to-
kens include additional under-trained tokens such
as–é—ã—üN–é—ã—üN andkrvldkf .
StarCoder2 is a family of models resulting
from the BigCode project, an open-scientiÔ¨Åc
collaboration focused on code ( Lozhkov et al. ,
2024 ). The open nature of the project rep-
resents a great opportunity for further inves-
tigation, allowing us to determine the source
5Oursubmitted Ô¨Åx for this has been released.
6This bug was Ô¨Åxed by disabling the ‚Äòslow‚Äô tokenizer.of under-trained tokens in the published tok-
enizer training data. We Ô¨Ånd a single document
which illustrates maximal variable lengths in Java
by repeating ‚ÄòLoremipumdolorsitametdconsecte-
turadipiscingelitIntegervelvelittr‚Äô as the source of
several long under-trained tokens, a single docu-
ment with base-64 encoded strings as the origin
of tokens such as BjKPZFq , and a single source
code Ô¨Åle with a list of solutions of a Wordle game
with words categorized by dialect as the source of
several tokens such as Ostschwizert√ºtsch relat-
ing to Swiss German dialects. Furthermore, the
tokenizer is unique in missing the 0xF1 byte as a
token in addition to not including unused UTF-8
bytes, and input text containing this byte results
in<|endoftext|> being used as a fallback ‚Äòun-
known‚Äô token.
Yi9B is a base model by 01.AI whose train-
ing data is focused on English and Chinese ( 01.AI
et al. ,2024 ). Most notable among results are a
number of strange tokens starting with ‚Äòn‚Äô, includ-
ingnConsequently andnInterestingly which
may have been caused by incorrectly processing
newline characters in tokenizer training data. In
addition, three tokens with Chinese phrases includ-
ingÊØõÊ≥Ω‰∏ú are unusual unreachable tokens.
Jamba v0.1 is a model from AI21 based on a
hybrid Transformer-Mamba mixture-of-experts ar-
chitecture with 52B total parameters ( Lieber et al. ,
2024 ). This model has very few tokens that pass
our strict threshold for veriÔ¨Åcation, and probabil-
ities for token output are often unusually close
to one. Tokenizer analysis does reveal 1,542 un-
trained special tokens, with <|startoftext|> as
the only special token which has been trained. The
latter is also an extreme outlier in our veriÔ¨Åcation,
with indicators showing its clear presence in train-
ing data, while the maximal probability of produc-
ing the token is ‚âà10‚àí8. The unusually sharp prob-
ability distributions may be an effect of the novel
architecture of this model.
4 Application to closed-source models
As our techniques involve directly using model
weights, they are not directly applicable to closed-
source models whose weights are not publicly
available. However, the experience gained in in-
specting a large variety of open-weight models
provides insight which we adapt and transfer to
closed models. For these tests, we use a custom
prompt designed to exactly repeat strings and seeif models appear incapable of doing so. For details
of prompts and results, see Appendix D.
Mistral‚Äôs Ô¨Çagship API models do not consis-
tently include information about tokenizers, but
tokenizers are available for their openly-released
models. Due to a conÔ¨Årmed leak of an early ver-
sion of their ‚Äòmedium‚Äô model as ‚Äòmiqu‚Äô, we have
some indication of the ‚Äòmedium‚Äô model being po-
tentially derived from Llama2 70B. By prompt-
ing both the ‚Äòmedium‚Äô and ‚Äòlarge‚Äô models, we
can conÔ¨Årm that the ‚Äòmedium‚Äô model is unable
to repeat strings that are typically under-trained
in Llama2 models, and the ‚Äòlarge‚Äô model fails
on typical tokens from the ‚Äòsmall‚Äô and ‚ÄòMixtral‚Äô
model series. In addition, in experimenting with
such prompts we Ô¨Ånd that the ‚Äòlarge‚Äô model oc-
casionally responds with special tokens including
[TOOL_CALLS] and[control_331] , which were
recently conÔ¨Årmed to be part of the tokenizer for
the 8x22B model, further highlighting the effec-
tiveness of this approach.
Anthropic‚Äôs models have limited documenta-
tion on their tokenizers. The Anthropic SDK con-
tains some tokenizer utilities for Claude 2, with
remarks that they are not accurate for Claude 3.
Using the tokenizer provided for Claude 2, we
can identify some candidates for intermediate
fragments that are likely under-trained by looking
for long tokens which are included as part of even
longer tokens. This results in candidates such
asCandidateFaciNum (iCandidateFaciNum ),
TrileptonPatTuple (TrileptonPatTupleMC ),
BFrontend (DVBFrontend ) and others. Some of
these tokens can be conÔ¨Årmed as problematic in
Claude 2.1, although none appear effective in the
Claude 3 family of models, consistent with the
change in tokenizer implied by the SDK code.
OpenAI‚Äôs models have well-documented to-
kenizers from the tiktoken package. In addi-
tion, by using models that share a tokenizer
(refer to section 3.2), we already have ac-
cess to a list of potential under-trained to-
ken candidates for GPT-3.5 and GPT-4, in-
cluding _ForCanBeConverted ,$PostalCodesNL ,
useRalative ,_typingsJapgolly , and others.
We Ô¨Ånd that all OpenAI models older than GPT-
4o fail to handle many of them correctly, result-
ing in hallucinations followed by an inability to
tell the difference between the inputs and incor-
rect outputs, or model output degrading into rep-
etition. The GPT-4o model family uses a different
tokenizer with a larger vocabulary, but the sametechniques for tokenizer analysis are effective in
Ô¨Ånding under-trained tokens, including ·ûò·üí·ûî·û∏, which
appears to induce an ‚Äòend of text‚Äô token, as well
as various tokens apparently derived from Chinese
advertisements, such as _Â§©Â§©‰∏≠ÂΩ©Á•®APP .
5 Discussion and Conclusion
The presence of under-trained tokens has several
negative consequences for language models, in-
cluding inefÔ¨Åcient inference and the potential to
bypass guardrails. Our investigations show that a
wide variety of untrained and under-trained tokens
are present in model tokenizers, across a wide va-
riety of model classes. Even with our relatively
conservative threshold for veriÔ¨Åcation, we detect
the presence of such tokens across all tested mod-
els, with typically around 0.1‚Äì1% of the vocab-
ulary consisting of severely under-trained tokens,
although their prevalence varies signiÔ¨Åcantly.
The most salient factors contributing to a model
having many under-trained tokens, aside from
simply having a large vocabulary, appears to be
whether the tokenizer was trained on similar data
as the model. Models which re-use a large exter-
nal tokenizer, and then train on distinct data from
scratch, are among those with the highest number
of detected under-trained tokens. Analyzing tok-
enizer characteristics directly can provide immedi-
ate sigal helping to identify several of these issues
without the need for any additional training. This
includes detecting unreachable tokens that fail to
encode back to their original text representation,
and unused byte fallback tokens. This approach
can also be particularly useful to quickly detect
and mitigate errors with tokenizer conÔ¨Åguration,
which appears to be a particularly common issue
when a custom vocabulary is manually added to
that of the original tokenizer. Additionally, using
the model embedding weights directly is a reliable
way to detect under-trained tokens, although care
should be taken to account for model architecture.
Based on our Ô¨Åndings, we summarize a num-
ber of recommendations within the scope of cur-
rent LLM development tooling. Firstly, we rec-
ommend ensuring that input data pre-processing
is identical across tokenizer training data, model
training data, and model inference. In particular,
by carefully considering how to handle carriage
returns, and special tokens present as plain text
in training data and user input. Secondly, care-
ful consideration of tokenizer training data is re-quired, ensuring that it is representative of model
training data. Next, after training a tokenizer, we
recommend checking for unreachable tokens by
encoding and decoding the vocabulary to ensure
that manually added tokens are handled correctly.
Finally, when training base models, checking for
under-trained tokens after smaller test runs, or test-
ing on a different corpus to reveal pre-processing
bugs that may cause unrepresentative inputs in
the main training data, provides a valuable sanity
check.
In addition to providing a set of useful tools for
improving models and tokenizers, our work indi-
cates several directions for future research. Firstly,
the results from StarCoder2 (see section 3.2) high-
light a potential limitation in BPE training where
occurrences in a single document or repository can
deÔ¨Åne a token by themselves. Strategies to pre-
vent this, such as limiting the count for pairs to be
merged by document, can be explored to prevent
this. Secondly, we note that byte-based BPE to-
kenization produces more intermediate fragments
which additionally have the ability to cause out-
puts to be undecodable. The trade-off between
more efÔ¨Åcient encoding methods and these down-
sides is particularly under-explored. Although al-
lowing such tokens may lead to lower average to-
ken counts, this also leads to the presence of more
untrained ‚Äòfragments‚Äô and tokens which are less se-
mantically meaningful. Techniques such as BPE-
dropout ( Provilkov et al. ,2020 ) have been pro-
posed to compensate for under-trained intermedi-
ate fragments, but direct comparisons on state-of-
the-art models are lacking.
Finally, we observe differences across models
in terms of how weight decay is applied to tokens
that are not present in the input, including not ap-
plying weight decay to embeddings, applying it
only to tokens seen in a batch, or applying it across
all model weights. This choice may affect the abil-
ity of models to learn richer semantic representa-
tions for rare tokens, and likely mitigate the sever-
ity and impact of under-trained tokens. Although
this choice has been studied in older models ( Sed-
hain et al. ,2015 ), we are not aware of systematic
ablations in recent LLMs.
Our Ô¨Åndings highlight a range of tokenizer is-
sues, the severity of which varies across models.
By analyzing tokenizers and model embeddings,
we can identify under-trained tokens and improve
the efÔ¨Åciency and security of LLMs.6 Limitations
Although our pipeline for Ô¨Ånding under-trained to-
kens is highly effective at Ô¨Ånding such ‚Äòglitch to-
kens‚Äô across a wide range of models, our approach
has a number of limitations.
Most notably, the output embedding-based indi-
cators require manually specifying a set of refer-
ence under-trained tokens, preventing the method
from being fully automated for the minority of
models with tied embeddings, and requiring at
least minimal manual intervention.
The output embedding-based indicators are
heuristic, and based on a working hypothesis for
the internal representation and training dynamics.
Further research into model interpretability could
reÔ¨Åne our understanding of such representations,
and lead to the development of more effective in-
dicators. The input embeddings-based indicator,
while not requiring such manual input, is only ap-
plicable to models without tied embeddings, and
depends on particular choices for weight decay
and initialization. Although this constitutes the
majority of models, there are various exceptions,
and the exact weight decay applied is often not
well documented.
Aside from these limitations affecting the abil-
ity to automatically calculate under-trained token
indicator scores, the relationship between our pro-
posed indicators and model behaviour is noisy.
Both the indicators themselves, as well as the veri-
Ô¨Åcation results, can be more indicative of problem-
atic model behaviour on different occasions.
SpeciÔ¨Åcally, there are certain situations where
the indicators we use offer a more reliable guide
of a token‚Äôs tendency to induce unwanted out-
put in typical prompting compared to our veriÔ¨Å-
cation prompting techniques. These cases include
input/output asymmetry, where tokens are solely
present as inputs (e.g., <BOS> ), or situations where
the model exhibits a strong bias towards a speciÔ¨Åc
language such as English, consistently producing
translated outputs.
Another common occurrence is the output of
an equivalent token without a leading space, al-
though the variation in our veriÔ¨Åcation prompts
compensates for this. On the other hand, there
are cases where tokens are rejected by the veriÔ¨Å-
cation process, but can still induce incorrect be-
haviour, mainly due to our strict threshold and
repetitive veriÔ¨Åcation prompts, which are designed
to detecting the most reliable under-trained tokens.However, despite these limitations, veriÔ¨Åcation us-
ing prompting is highly effective at identifying a
threshold below which candidate tokens induce un-
wanted behaviour, and selecting the most effective
candidate tokens.
Finally, the scope of our work is limited by
an exclusive focus on models that use byte-
pair encoding-based tokenization. Results for
Unigram-based models may be signiÔ¨Åcantly differ-
ent, with both the lack of intermediate fragments,
and randomized tokenization preventing the inter-
mediate fragments which are a source of under-
trained tokens, and we leave investigation of such
models to future work.
Acknowledgments
We are grateful to Dirk Groeneveld, Luca Sol-
daini, and Nathan Lambert from the Allen Insti-
tute for AI for insightful discussions and for pro-
viding data on weight decay, token counts, and to-
kenization in the OLMo models. We also thank
Stella Biderman from EleutherAI for sharing infor-
mation regarding weight decay and tokenization
in the Pythia and GPT-NeoX models. Addition-
ally, we appreciate the valuable feedback on the
manuscript from Matthias Gall√©, Phil Blunsom,
and Kelly Marchisio, and thank Nathan Godey for
helpful pointers to relevant literature.
References
01.AI, Alex Young, Bei Chen, Chao Li, Chen-
gen Huang, Ge Zhang, Guanwei Zhang, Heng
Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang,
Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue,
Senbin Yang, Shiming Yang, Tao Yu, Wen Xie,
Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao
Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue
Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and
Zonghong Dai. 2024. Yi: Open foundation models
by 01.AI .Preprint , arXiv:2403.04652.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-
uan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tian-
hang Zhu. 2023. Qwen technical report .Preprint ,
arXiv:2309.16609.Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy
Phung, Maksym Zhuravinskyi, Reshinth Adithyan,
James Baicoianu, Ben Brooks, Nathan Cooper,
Ashish Datta, Meng Lee, Emad Mostaque, Michael
Pieler, Nikhil Pinnaparju, Paulo Rocha, Harry Saini,
Hannah Teufel, Niccolo Zanichelli, and Carlos
Riquelme. 2024. Stable LM 2 1.6B technical report .
Preprint , arXiv:2402.17834.
Stella Biderman, Hailey Schoelkopf, Quentin An-
thony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hal-
lahan, Mohammad AÔ¨Çah Khan, Shivanshu Puro-
hit, USVSN Sai Prashanth, Edward Raff, Aviya
Skowron, Lintang Sutawika, and Oskar Van Der Wal.
2023. Pythia: a suite for analyzing large language
models across training and scaling. In Proceedings
of the 40th International Conference on Machine
Learning , ICML‚Äô23. JMLR.org.
Daniel Bi ¬¥s, Maksim Podkorytov, and Xiuwen Liu.
2021. Too much in common: Shifting of embed-
dings in transformer language models and its impli-
cations . In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 5117‚Äì5130.
Sidney Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace
He, Connor Leahy, Kyle McDonell, Jason Phang,
Michael Pieler, Usvsn Sai Prashanth, Shivanshu
Purohit, Laria Reynolds, Jonathan Tow, Ben Wang,
and Samuel Weinbach. 2022. GPT-NeoX-20B: An
open-source autoregressive language model . InPro-
ceedings of BigScience Episode #5 ‚Äì Workshop on
Challenges & Perspectives in Creating Large Lan-
guage Models , pages 95‚Äì136, virtual+Dublin. Asso-
ciation for Computational Linguistics.
Kaj Bostrom and Greg Durrett. 2020. Byte pair encod-
ing is suboptimal for language model pretraining . In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 4617‚Äì4624, Online.
Association for Computational Linguistics.
Cohere. 2024. Cohere Command R documentation .
Martin Fell. 2023. A search for more ChatGPT / GPT-
3.5 / GPT-4 "unspeakable" glitch tokens . Blog post.
Jonas Geiping, Alex Stein, Manli Shu, Khalid Saiful-
lah, Yuxin Wen, and Tom Goldstein. 2024. Coercing
llms to do and reveal (almost) anything .Preprint ,
arXiv:2402.14020.
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane Rivi√®re, Mihir Sanjay
Kale, Juliette Love, Pouya Tafti, L√©onard Hussenot,
Aakanksha Chowdhery, Adam Roberts, Aditya
Barua, Alex Botev, Alex Castro-Ros, Ambrose
Slone, Am√©lie H√©liou, Andrea Tacchetti, Anna Bu-
lanova, Antonia Paterson, Beth Tsai, Bobak Shahri-
ari, Charline Le Lan, Christopher A. Choquette-
Choo, Cl√©ment Crepy, Daniel Cer, Daphne Ip-
polito, David Reid, Elena Buchatskaya, Eric Ni,Eric Noland, Geng Yan, George Tucker, George-
Christian Muraru, Grigory Rozhdestvenskiy, Hen-
ryk Michalewski, Ian Tenney, Ivan Grishchenko,
Jacob Austin, James Keeling, Jane Labanowski,
Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-
nan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin
Mao-Jones, Katherine Lee, Kathy Yu, Katie Milli-
can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,
Machel Reid, Maciej Mikua, Mateo Wirth, Michael
Sharman, Nikolai Chinaev, Nithum Thain, Olivier
Bachem, Oscar Chang, Oscar Wahltinez, Paige Bai-
ley, Paul Michel, Petko Yotov, Pier Giuseppe Sessa,
Rahma Chaabouni, Ramona Comanescu, Reena
Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan
Mullins, Samuel L Smith, Sebastian Borgeaud, Ser-
tan Girgin, Sholto Douglas, Shree Pandya, Siamak
Shakeri, Soham De, Ted Klimenko, Tom Hennigan,
Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen,
Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Lu-
dovic Peran, Minh Giang, Cl√©ment Farabet, Oriol
Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Has-
sabis, Zoubin Ghahramani, Douglas Eck, Joelle Bar-
ral, Fernando Pereira, Eli Collins, Armand Joulin,
Noah Fiedel, Evan Senter, Alek Andreev, and Kath-
leen Kenealy. 2024. Gemma: Open models based
on Gemini research and technology .Preprint ,
arXiv:2403.08295.
Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita
Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya
Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang,
Shane Arora, David Atkinson, Russell Authur, Khy-
athi Chandu, Arman Cohan, Jennifer Dumas, Yanai
Elazar, Yuling Gu, Jack Hessel, Tushar Khot,
William Merrill, Jacob Morrison, Niklas Muen-
nighoff, Aakanksha Naik, Crystal Nam, Matthew
Peters, Valentina Pyatkin, Abhilasha Ravichan-
der, Dustin Schwenk, Saurabh Shah, William
Smith, Emma Strubell, Nishant Subramani, Mitchell
Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle
Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle
Lo, Luca Soldaini, Noah Smith, and Hannaneh Ha-
jishirzi. 2024. OLMo: Accelerating the science of
language models . In Proceedings of the 62nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 15789‚Äì
15809, Bangkok, Thailand. Association for Compu-
tational Linguistics.
Hakan Inan, Khashayar Khosravi, and Richard Socher.
2017. Tying word vectors and word classiÔ¨Åers: A
loss framework for language modeling .Preprint ,
arXiv:1611.01462.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, L√©lio Re-
nard Lavaud, Marie-Anne Lachaux, Pierre Stock,
Teven Le Scao, Thibaut Lavril, Thomas Wang, Tim-
oth√©e Lacroix, and William El Sayed. 2023. Mistral
7B.Preprint , arXiv:2310.06825.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de las
Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lam-
ple, L√©lio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao,
Th√©ophile Gervet, Thibaut Lavril, Thomas Wang,
Timoth√©e Lacroix, and William El Sayed. 2024.
Mixtral of experts .Preprint , arXiv:2401.04088.
Andrej Karpathy. 2024. Let‚Äôs build the GPT Tokenizer .
YouTube Video.
Taku Kudo. 2018. Subword regularization: Improving
neural network translation models with multiple sub-
word candidates . InProceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 66‚Äì75, Mel-
bourne, Australia. Association for Computational
Linguistics.
Opher Lieber, Barak Lenz, HoÔ¨Åt Bata, Gal Co-
hen, Jhonathan Osin, Itay Dalmedigos, Erez
Safahi, Shaked Meirom, Yonatan Belinkov,
Shai Shalev-Shwartz, Omri Abend, Raz Alon,
Tomer Asida, Amir Bergman, Roman Glozman,
Michael Gokhman, Avashalom Manevich, Nir
Ratner, Noam Rozen, Erez Shwartz, Mor Zus-
man, and Yoav Shoham. 2024. Jamba: A hybrid
Transformer-Mamba language model .Preprint ,
arXiv:2403.19887.
Anton Lozhkov, Raymond Li, Loubna Ben Allal,
Federico Cassano, Joel Lamy-Poirier, Nouamane
Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yux-
iang Wei, Tianyang Liu, Max Tian, Denis Ko-
cetkov, Arthur Zucker, Younes Belkada, Zijian
Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul,
Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li,
Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii,
Nii Osae Osae Dade, Wenhao Yu, Lucas KrauSS,
Naman Jain, Yixuan Su, Xuanli He, Manan Dey,
Edoardo Abati, Yekun Chai, Niklas Muennighoff,
Xiangru Tang, Muhtasham Oblokulov, Christopher
Akiki, Marc Marone, Chenghao Mou, Mayank
Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel
Zebaze, Olivier Dehaene, Nicolas Patry, Canwen
Xu, Julian McAuley, Han Hu, Torsten Scholak, Se-
bastien Paquet, Jennifer Robinson, Carolyn Jane An-
derson, Nicolas Chapados, Mostofa Patwary, Nima
Tajbakhsh, Yacine Jernite, Carlos Mu√±oz Ferrandis,
Lingming Zhang, Sean Hughes, Thomas Wolf, Ar-
jun Guha, Leandro von Werra, and Harm de Vries.
2024. StarCoder 2 and The Stack v2: The next gen-
eration .Preprint , arXiv:2402.19173.
Meta AI. 2024. Introducing Meta Llama 3: The most
capable openly available LLM to date .
Microsoft. 2023. Phi-2: The surprising power of small
language models .
Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky,
Colin Raffel, Manan Dey, Matthias Gall√©, Arun
Raja, Chenglei Si, Wilson Y . Lee, Beno√Æt Sagot, andSamson Tan. 2021. Between words and characters:
A brief history of open-vocabulary modeling and to-
kenization in NLP .Preprint , arXiv:2112.10508.
OpenAI. 2024. tiktoken: a fast BPE tokeniser for use
with OpenAI‚Äôs models.
Ivan Provilkov, Dmitrii Emelianenko, and Elena V oita.
2020. BPE-dropout: Simple and effective subword
regularization . In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 1882‚Äì1892, Online. Association for
Computational Linguistics.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Rakuten Group, Aaron Levine, Connie Huang, Chen-
guang Wang, Eduardo Batista, Ewa Szymanska,
Hongyi Ding, Hou Wei Chou, Jean-Fran√ßois Pessiot,
Johanes Effendi, Justin Chiu, Kai Torben Ohlhus,
Karan Chopra, Keiji Shinzato, Koji Murakami, Lee
Xiong, Lei Chen, Maki Kubota, Maksim Tkachenko,
Miroku Lee, Naoki Takahashi, Prathyusha Jwalapu-
ram, Ryutaro Tatsushima, Saurabh Jain, Sunil Ku-
mar Yadav, Ting Cai, Wei-Te Chen, Yandi Xia,
Yuki Nakayama, and Yutaka Higashiyama. 2024.
RakutenAI-7B: Extending large language models
for Japanese .Preprint , arXiv:2403.15484.
Jessica Rumbelow and Matthew Watkins. 2023. Solid-
GoldMagikarp (plus, prompt generation) . Blog
Post.
Suvash Sedhain, Aditya Krishna Menon, Scott Sanner,
and Lexing Xie. 2015. Autorec: Autoencoders meet
collaborative Ô¨Åltering. In Proceedings of the 24th
International Conference on World Wide Web , pages
111‚Äì112. ACM.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units . InProceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1715‚Äì
1725, Berlin, Germany. Association for Computa-
tional Linguistics.
Kevin Slagle. 2024. SpaceByte: Towards deleting to-
kenization from large language modeling .Preprint ,
arXiv:2404.14408.
The Unicode Consortium. 2023. The Unicode standard.
version 15.0 core speciÔ¨Åcation .
Hugo Touvron, Louis Martin, Kevin Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony Hartshorn, Saghar Hos-
seini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel Kloumann, ArtemKorenev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, Ranjan Subramanian, Xiaoqing Ellen Tan,
Binh Tang, Ross Taylor, Adina Williams, Jian Xi-
ang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,
Yuchen Zhang, Angela Fan, Melanie Kambadur,
Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. 2023. Llama
2: Open foundation and Ô¨Åne-tuned chat models .
Preprint , arXiv:2307.09288.
Ben Wang and Aran Komatsuzaki. 2021. GPT-J-
6B: A 6 Billion Parameter Autoregressive Language
Model .
Matthew Watkins and Jessica Rumbelow. 2023. Solid-
GoldMagikarp III: Glitch token archaeology . Blog
Post.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R√©mi Louf, Morgan Fun-
towicz, Joe Davison, Sam Shleifer, Patrick von
Platen, Clara Ma, Yacine Jernite, Julien Plu, Can-
wen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush.
2020. HuggingFace‚Äôs transformers: State-of-
the-art natural language processing . Preprint ,
arXiv:1910.03771.
Linting Xue, Aditya Barua, Noah Constant, Rami Al-
Rfou, Sharan Narang, Mihir Kale, Adam Roberts,
and Colin Raffel. 2022. ByT5: Towards a token-free
future with pre-trained byte-to-byte models .Trans-
actions of the Association for Computational Lin-
guistics , 10:291‚Äì306.
Lily Yu, Daniel Simig, Colin Flaherty, Armen Agha-
janyan, Luke Zettlemoyer, and Mike Lewis. 2023.
MEGABYTE: Predicting million-byte sequences
with multiscale transformers . InThirty-seventh Con-
ference on Neural Information Processing Systems .
A Alternative under-trained token
indicators
For some models, in particular those in the
Gemma series ( Gemma Team et al. ,2024 ), we
noticed a very high similarity between the rows
of their (tied) embedding matrix. Such similar-
ity between embeddings has been noted before,
and has been attributed to all embeddings being
pushed in a common direction during training ( Bi¬¥s
et al.,2021 ). Although a constant component in all
output embeddings has no effect on model predic-
tions, as softmax is invariant to a constant shift of
all logits, such similarity may affect the effective-
ness of our under-trained token indicators.To compensate for this, we tested two varia-
tions for reducing or removing this constant com-
ponent. Centering the embeddings by subtracting
their mean, and removing their Ô¨Årst principal com-
ponent:
ÀÜEout,i=Eout,i‚àí1
|Eout|‚àë
jEout,j
U=PCA (Eout)
ÀúEout,i=Eout,i‚àí(UT
1Eout,i)U1
We can then take the cosine distance between
rows in these adjusted output embedding matrices
to obtain the additional indicators C(ÀÜEout,ÀÜuref)
andC(ÀúEout,Àúuref). Testing additional indicators
on a small range of models (see Table 2) shows no
consistent improvement in using these more com-
plex methods.
B VeriÔ¨Åcation details
We use three repetitive prompts to induce mod-
els to output the candidate token we are testing,
shown in Table 3.
These prompts are all designed to be suitable for
base models and not require specialized instruc-
tion tuning or prompt templating. For each prompt
we generate three tokens and check the maximal
probability of our target token being predicted, and
then take the maximum of this again over all three
prompts. Variation in quoting and spacing helps
to ensure we do not detect false positives based on
models producing similar tokens without spaces,
or tokens which start with punctuation partially
merging with quotes. By using a temperature of
zero, and designing our prompts such that the de-
sired token is typically the Ô¨Årst one to be sampled,
we minimize the effect of random sampling.
C A short primer on UTF-8 encoding
UTF-8 is the most prevalent encoding scheme
used to represent text in computers and communi-
cation protocols worldwide. It efÔ¨Åciently encodes
Unicode characters, which encompass a vast range
of characters from various writing systems and
symbols ( The Unicode Consortium ,2023 ). Encod-
ing to UTF-8 is often the Ô¨Årst step in tokenization.
UTF-8 encoding can be summarized as follows:
‚Ä¢ASCII (code points below 128): Single byte,
binary 0xxxxxxx representing up to 7 bits.Number of veriÔ¨Åed under-trained tokens using metric
Model C(Eout, u ref)C(ÀÜEout,ÀÜuref)C(ÀúEout,Àúuref)L2(Eout‚àíuref)L2(Ein)
Command R 306 302 301 309 n/a
Command R+ 75 79 74 83 n/a
Gemma 2B 3161 3310 2889 3052 n/a
Gemma 7B 800 802 797 800 n/a
GPT-2 Medium 49 49 49 49 n/a
Mistral 7B v0.3 63 54 63 60 53
Olmo 7B v1.7 177 178 177 179 175
Table 2: Effectiveness of different indicators. For each under-trained token indicator, we veriÔ¨Åed the top 2% of
tokens, and show the number of these that pass our 1% veriÔ¨Åcation threshold. No consistent pattern is seen to
justify the more complex alternatives.
VeriÔ¨Åcation prompt #1.
<token> is replaced with the token we are testing.
This device repeats its input exactly, as shown in these 100 identical trials.
Input: ¬´<token>¬ª
Output: ¬´<token>¬ª
<repeated 18 additional times>
Input: ¬´<token>¬ª
Output:
VeriÔ¨Åcation prompt #2
<token> is replaced with the token we are testing.
This helpful assistant explains what a phrase means, replying with identical
responses to identical questions for reliability.
User: What does ‚Äôedslfkj‚Äô mean?
Assistant: ‚Äôedslfkj‚Äô has no specific meaning I am aware of.
User: what does ‚Äô<token>‚Äô mean?
Assistant:
VeriÔ¨Åcation prompt #3
<token> is replaced with the token we are testing.
Below is the output, as you can see it‚Äôs just giving the same phrase repeated
indefinitely: <token><token><token><token><token><95 additional repeats>
Prompt used for API-based testing
Please help me reformat this Python code.
I want to have every element in the array on a separate line.
Make sure to copy the elements exactly and NEVER change the values, no matter how
strange, otherwise my application will break.
ÀãÀãÀãpython
passwords = <list of strings with multiple elements per line>
ÀãÀãÀã
Table 3: Prompts‚Ä¢2-byte sequences: 110xxxxx, 10xxxxxx rep-
resenting up to 11 bits.
‚Ä¢3-byte sequences: 1110xxxx, 10xxxxxx,
10xxxxxx representing up to 16 bits.
‚Ä¢4-byte sequences: 11110xxx, 10xxxxxx,
10xxxxxx, 10xxxxxx representing up to 21
bits.
Where the bits indicated by ‚Äòx‚Äô are concatenated
to form the Unicode code point.
This encoding naturally gives rise to some byte
values that are not used:
‚Ä¢111110xx, 1111110x, 11111110, 11111111
would represent the Ô¨Årst byte of sequences
of 5-8 bytes, which are not in use. This cor-
responds to decimal 245-255 or hexadecimal
0xF5 ‚Äì0xFF .
‚Ä¢11000000, 11000001 are not in use, as the
possible two-byte encodings that start with
this Ô¨Åt in 7 bits due to the Ô¨Åve leading zeros.
These are 192/193 in decimal and 0xC0 /0xC1
in hexadecimal.
‚Ä¢Additionally, other starting bytes can be cov-
ered entirely by other tokens, and also turn
out to be unused. A common example of
this is 0xC2 /0xC3 which are only used for
Unicode points 128-255. In addition, since
code points U+323B0 toU+0xDFFFF are unas-
signed, the 0xF1 and0xF2 bytes are not used
in UTF-8 representations of currently deÔ¨Åned
Unicode characters. Similarly, 0xF4 is only
used through the ‚ÄúSupplementary Private Use
Area‚Äù. However, even if not deÔ¨Åned in the
current Unicode standard, such characters
can be easily inserted in text and are found
on web pages.
D API-based veriÔ¨Åcation in
closed-source models
We use a speciÔ¨Åc prompt for API-based testing
of under-trained tokens, show in Table 3. The
‚Äòpassword‚Äô strings consist of the problematic to-
ken, occasionally preÔ¨Åxed to help identify their
source, and to avoid starting the string with a lead-
ing space, as we noticed that models often drop
the leading space after a quotation mark, even for
normal tokens. Although many other prompt for-
mats are effective, we have found this code-basedapproach to more clearly avoid false positives. Fig-
ure5shows the result for Mistral, Anthropic and
OpenAI models.(a) Mistral API prompting results.
 (b) Claude API prompting results.
(c) GPT-3.5 API prompting results.
 (d) GPT-4 API prompting results.
Figure 5: API prompting results.