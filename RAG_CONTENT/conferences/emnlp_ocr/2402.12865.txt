Backward Lens: Projecting Language Model Gradients
into the Vocabulary Space
Shahar Katz1Yonatan Belinkov1Mor Geva2Lior Wolf2
1Faculty of Computer Science, Technion – Israel Institute of Technology
2Blavatnik School of Computer Science, Tel Aviv University
{shachar.katz@cs,belinkov@}technion.ac.il ,{morgeva@tauex,wolf@cs}.tau.ac.il ,
Abstract
Understanding how Transformer-based Lan-
guage Models (LMs) learn and recall informa-
tion is a key goal of the deep learning commu-
nity. Recent interpretability methods project
weights and hidden states obtained from the
forward pass to the models’ vocabularies, help-
ing to uncover how information flows within
LMs. In this work, we extend this methodol-
ogy to LMs’ backward pass and gradients. We
first prove that a gradient matrix can be cast as
a low-rank linear combination of its forward
and backward passes’ inputs. We then develop
methods to project these gradients into vocab-
ulary items and explore the mechanics of how
new information is stored in the LMs’ neurons.
1 Introduction
Deep learning models consist of layers, which are
parameterized by matrices that are trained using a
method known as backpropagation. This process
involves the creation of gradient matrices that are
used to update the models’ layers. Backpropaga-
tion has been playing a major role in interpreting
deep learning models and multiple lines of study ag-
gregate the gradients to provide explainability (Si-
monyan et al., 2014; Sanyal and Ren, 2021; Chefer
et al., 2022; Sarti et al., 2023; Miglani et al., 2023).
Recent interpretability works have introduced
methods to project the weights and intermediate
activations of Transformer-based LMs (Vaswani
et al., 2017) into the vocabulary space. The semi-
nal “Logit Lens” method (nostalgebraist, 2020) has
paved the way to explaining LMs’ behavior dur-
ing inference (Geva et al., 2022a; Dar et al., 2022;
Ram et al., 2023), including directly interpreting
individual neurons (Geva et al., 2021; Katz and
Belinkov, 2023). Our work is the first, as far as we
can ascertain, to project LM gradients to the vo-
cabulary space. Furthermore, modern LMs contain
thousands of neurons in each layer, while certain
Figure 1: An illustration depicting the tokens promoted
by a single LM’s MLP layer and its gradient during the
forward and backward pass when editing the model to
answer “Paris” for the prompt “Lionel Messi plays for”.
The gradients (in green) of the first MLP matrix, FF1,
attempt to imprint into the model’s weight (in blue) the
information that FF1encountered during the forward
pass. Utilizing a vocabulary projection method, we
reveal that this information represents the token “team”.
The gradients of the second MLP matrix, FF2, aim to
shift the information encoded within FF2towards the
embedding of the new target.
features are likely distributed across multiple neu-
rons (Elhage et al., 2022; Cunningham et al., 2023).
These issues are handled in our examination of the
gradient matrices by performing a decomposition
of provably low-rank matrices.
Despite the popularity of LMs, our understand-
ing of their behavior remains incomplete (Bender
et al., 2021; Dwivedi et al., 2023), particularly re-
garding how LMs acquire new knowledge during
training and the mechanisms by which they store
and recall it (Dai et al., 2022; Geva et al., 2021,
2023; Meng et al., 2023). We identify a mecha-
nism we refer to as “imprint and shift”, which cap-
tures how information is stored in the feed-forward
(MLP) module of the transformer layer. This mod-arXiv:2402.12865v1  [cs.CL]  20 Feb 2024ule has two fully connected layers, FF 1andFF 2.
The “imprint” refers to the first layer, to or from
which the learning process adds or subtracts copies
of the intermediate inputs encountered during the
forward pass. The “shift” refers to the second ma-
trix, where the weights are shifted by the embed-
ding of the target token, see Figure 1.
In summary, our contributions are: (i) Analyzing
the rank of gradients. (ii) Interpreting gradients
by inspecting relatively small spanning sets. (iii)
Investigating the embedding of these sets by pro-
jecting them into tokens, including (iv) examining
the Vector-Jacobian Product (VJP) obtained during
the backward pass (the equivalent of the hidden
states of the forward pass). Furthermore, (iv) we
reveal a two-phase mechanism by which models
learn to store knowledge in their MLP layers, and
(v) leverage it to explore a novel editing method
based solely on a single forward pass.
2 Related Work
Developing methods to explain LMs is central
to the interpretability community (Belinkov and
Glass, 2019; Srivastava et al., 2023). Initially in-
spired by interpretability efforts in vision models
(Samek et al., 2017; Zhang and Zhu, 2018; In-
dolia et al., 2018; Olah et al., 2020), LMs have
also benefited from the ability to operate in the
language domain. This includes leveraging pro-
jections of vectors into readable concepts (nostal-
gebraist, 2020; Simhi and Markovitch, 2023) or
clustering them into the idioms they promote (Cun-
ningham et al., 2023; Tamkin et al., 2023; Tigges
et al., 2023; Bricken et al., 2023).
Reverse engineering the gradient’s role in shift-
ing model behavior has been a primary method to
comprehend the mechanics of deep learning mod-
els. Recent work (Ilharco et al., 2022; Gueta et al.,
2023; Tian et al., 2023) demonstrates that clus-
tering the weights that models learn during train-
ing or fine-tuning reveals patterns that connect the
tasks and their training data. In general, existing
works examine gradients by observing full matri-
ces, whereas our approach involves interpreting
them using the backward pass’s VJP. Specifically,
approaches employing Saliency Maps (Simonyan
et al., 2014) explore the relationship between a gra-
dient matrix and parts of its corresponding forward
pass input, where we have observed that gradients
are spans, linear combinations, of those inputs.
Our experiment regarding LM editing adds tothe line of work that utilizes interpretability for
knowledge editing. The closest idea to our imple-
mentation was introduced by Dai et al. (2022), who
identified activated neurons for specific idioms in
encoder LMs and altered them by injecting the em-
bedded target. We show that gradients work in
a very similar way. Other state-of-the-art model
editing methods include Mitchell et al. (2021) and
Meng et al. (2022, 2023).
Our work analyzes the gradient’s rank. It
was previously known that gradients are low-rank
(Mitchell et al., 2021), but the utilization of this
characteristic for interpretability study or predict-
ing the rank of an edited prompt have remained un-
explored. Optimizers and adaptors, such as LoRA
Hu et al. (2022), were created to constrain the rank
of gradients, making fine-tuning faster. Our work,
in contrast, shows that the gradients are already
low-rank and utilizes this phenomenon.
3 Background
We first provide background on transformers, fo-
cusing on the components that play a role in our
analysis and omitting other concepts, such as Layer
Norms and positional embedding, which are ex-
plored in full by Vaswani et al. (2017); Radford
et al. (2019). We then provide the necessary back-
ground on the backward pass; a more comprehen-
sive description is given by Clark (2017); Bishop
(2006). Finally, we discuss the building blocks of
the Logit Lens method.
3.1 Transformer LMs
The Generative Pre-trained Transformer (GPT), is
an auto-regressive family of architectures contain-
ing multiple transformer blocks. Given a prompt
sequence of ntokens, GPT predicts a single token.
The architecture maintains an embedding dimen-
siondthroughout all layers. First, the input tokens
are embedded using an embedding matrix Einto
the input vectors X∈Rn×d. This is mirrored
at the final stage, in which a decoding matrix D
projects the output of the last transformer block
into a score for each token within the vocabulary.
Each transformer block comprises an attention
layer (Attn) and a Multi-Layer Perceptron (MLP)
layer, interconnected by a residual stream. The
attention mechanism transfer vectors (information)
from each of the preceding inputs to the current
forward pass. In our study, we do not delve into
this module and refer the reader to Radford et al.(2018) for more details.
The MLP layer (also known as FFN, Feed-
Forward Network) consists of two fully connected
matrices FF 1,FFT
2∈Rd×dm, with an acti-
vation function fbetween them: MLP(X) =
f(XFF 1)FF 2.
Hence, the calculation that the l-th trans-
former block performs on its input hidden state,
Xl, is given by Xl+1=Xl+Attn(Xl) +
MLP(Attn(Xl) +Xl).
3.2 Backpropagation
Backpropagation (Rumelhart et al., 1986; Le Cun,
1988) is an application of the chain rule to compute
derivatives and update weights in the optimization
of deep learning network-based models. The pro-
cess begins with the model executing a forward
pass, generating a prediction ˆy, which is subse-
quently compared to a desired target by quantifying
the disparity through a loss score L. Following this,
a backward pass is initiated, iterating through the
model’s layers and computing the layers’ gradients
in the reverse order of the forward pass.
For a given layer of the model that during the for-
ward pass computed z=xW, where x∈Rm, z∈
Rnare its intermediate input and output, we com-
pute its gradient matrix using the chain rule:
∂L
∂W=∂z
∂W∂L
∂z∈Rn×m(1)
We can directly compute∂z
∂W=∂xW
∂W=x⊤. The
other derivative δ=∂L
∂z∈Rnis known as the
Vector-Jacobian Product (VJP) of z. It can be
thought of as the hidden state of the backward pass
and is the error factor that later layers project back.
In LMs, the output of the model is an unnormal-
ized vector, ˆy∈R|vocabulary |, representing a score
for each of the model’s tokens. We denote the tar-
get token by an index t∈[|vocabulary |]. Typically
the Negative Log-Likelihood (NLL) loss is used:
ˆp=Softmax (ˆy)∈R|vocabulary |(2)
L=NLL(ˆp, t) =−log (ˆp[t])∈R (3)
where ˆprepresents the normalized probabilities
ofˆyand[t]is its t-th value (the target token’s
probability). For the last layer’s output z= ˆy,
calculating its δ(VJP) can be done directly by
(k∈[|vocabulary |]):
δ[k] =(
ˆp[k]−1≤0 ifk=t
ˆp[k]≥0 otherwise(4)For an earlier layer in the model l, we cannot
compute the VJP of its output zldirectly (herel
indicates the layer’s index). Since we iterate the
model in a reverse order, we can assume we already
computed the VJP of layer l+ 1. If the layers are
sequential, the output of layer lis the input of l+ 1,
therefrom zl=xl+1. Utilizing the backward step,
we can compute:
δl=∂L
∂zl=∂L
∂xl+1=δl+1(Wl+1)⊤(5)
To summarize, in deep learning models, the gra-
dient of a loss function Lwith respect to a given
layer W, is the outer product of the layer’s forward
pass input, x, and its output z’s VJP, δ:
∂L
∂W=∂z
∂W∂L
∂z=x⊤·δ∈Rn×m(6)
3.3 Vocabulary Projection Methods
nostalgebraist (2020) discovered that we can trans-
form hidden states from LMs forward passes into
vocabulary probabilities, thereby reflecting their
intermediate predictions. Termed as Logit Lens
(LL) , this method projects a vector xin the size
of the embedding space dby applying it with the
LM’s decoding, the process that transforms the last
transformer block’s output into a prediction:
LL(x) =Softmax (lnf(x)D)∈R|vocabulary |(7)
where lnfis the model’s last Layer Norm before
the decoding matrix D.
The projection captures the gradual building of
LMs output (Millidge and Black, 2022; Haviv et al.,
2023), and projections from later layers are more
interpretable than earlier ones. Efforts such as Din
et al. (2023); Belrose et al. (2023) try to solve this
gap by incorporating learned transformations into
LL. However, to emphasize our main discoveries,
we have not included such enhancements, which
primarily aim to shortcut the models’ computations
and require dedicated training procedures.
An artificial neuron performs a weighted sum
of its inputs, and appears as a column or a row of
the model’s matrices taken along a direction that
has a dimensionality d. Static neurons can also be
projected into tokens using LL: Geva et al. (2021),
Geva et al. (2022b) observe that neurons of the first
MLP matrix FF 1determine the extent to which
each neuron in FF 2contributes to the intermediate
prediction. Dar et al. (2022), Geva et al. (2023)
employ the same approach to investigate the at-
tention matrices. Elhage et al. (2021), Katz andBelinkov (2023) demonstrate how these neurons
can elucidate model behavior, Wang et al. (2023),
Millidge and Black (2022), Todd et al. (2023) use
it to explore circuits and in-context learning.
Despite the growing interest in this approach, we
are only aware of works that have applied it to the
static weights of models or the hidden states of the
forward pass. In contrast, our work is focused on
the backward pass of LMs.
4 Backward Lens
In this section, we detail the methods we developed
to analyze gradients based on our understanding of
how each gradient matrix is formed.
4.1 Gradients as Low-Rank Matrices
Hu et al. (2022) and Mitchell et al. (2021) have
observed the low-rank of MLP layers’ gradients
with a single input. However, they did not explain
this phenomenon in the context of a matrix with a
sequence of inputs, nor did they predict this rank.
The following Lemma does both.
Lemma 4.1. Given a sequence of inputs of length
n, a parametric matrix Wand a loss function L,
the gradient∂L
∂Wproduced by a backward pass is a
matrix with a rank of nor lower.
Proof. According to Equation 6, the gradient of a
matrix is∂L
∂W=x⊤·δ. Assuming x, δare non-
zero vectors, the rank of the gradient matrix is 1,
given its interpretation as a span of a single column
vector x, or equivalently, as a span of a single row
δ. In the case when xorδis a zero vector, the rank
of the gradient matrix is 0.
In LMs, an input prompt comprises a sequence
ofntokens, each of which introduces an interme-
diate input ( xi) at every layer. In this case, the
gradient matrix is the sum of each xi, δi’s product:
∂L
∂W=nX
i=1∂zi
∂W∂L
∂zi=nX
i=1x⊤
i·δi (8)
The maximum rank of the summed gradient matrix
isngiven each xi, orδi, is linearly independent,
since we sum ndistinct rank-1 matrices. Reasons
for this rank to be lower than nare the existence
of linear dependencies between xior between δi,
with 0 being the minimum possible rank.
Of particular interest is the case of the last layer
of the transformer. In this case, the rank of the
gradient is one, see Appendix A.
Figure 2: The calculation of gradient matrix by the
outer product of x⊤·δ. Each row consists of the same
values, but above we describe the matrix as a span of δ,
while below as a span of x⊤. The displayed vectors are
presented transposed to emphasize the spanning effect.
4.2 Applying Logit Lens to Gradient Matrices
In our analysis we focus on the MLP layers, due to
recent interest in identifying and editing the knowl-
edge stored in these layers (Geva et al., 2022b,
2021; Dai et al., 2022; Mitchell et al., 2021; Meng
et al., 2022). Consider the MLP modules FF 1and
FF 2. The first maps from RdtoRdm, which is
typically, for many transformers, 4d. The second
maps from the latter dimension to the former. In
both cases, the gradient matrix has one dimension
ofdmand one of d. Exploring all these dimensions
is prohibitive, see Appendix B. However, Equa-
tion 8 reveals that every gradient matrix is a sum
ofnouter products x⊤
i·δi. This view allows us to
examine every gradient matrix as a sum of npairs
of vectors. Our analysis, therefore, would focus on
onlyn << d mvectors in Rd.
Every matrix formed by x⊤
i·δican be interpreted
in two ways simultaneously: (1) as a span (linear
combination) of xiand (2) as a span of δi. Figure 2
illustrate the two viewpoints. We utilize this duality
and examine gradients as the linear combinations
ofnvectors: xiorδi.
The gradients of FF 1δiis not of size d, butxi
ared-sized vectors and were already explored using
LL (Geva et al., 2022b; Dar et al., 2022). Therefore,
forFF 1we chose to observe the gradient matrix
as a span of xi. Explicitly, we refer to xiasFF 1’s
spanning set since the j-th neuron of the gradient
matrix is equal to a linear combination of xi:
∂L
∂FF 1[j] =nX
i=1x⊤
i·δi[j], (9)where δi[j]is the j-th element of the vector δi.
The gradients of FF 2The sizes of FF 2’sxi, δi
are switched from those of FF 1, hence we chose δi
asFF 2’s gradient spanning set. Thus, its gradient’s
j-th neuron is viewed as a combination of δi:
∂L
∂FF 2[j] =nX
i=1δi·xi[j], (10)
where xi[j]is the j-th element of the vector xi. In
Section 5 we provide a theoretical explanation of
why the choice of the VJP δiis not only a technical
one, due to dimensionality considerations.
5 Understanding the Backward Pass
The VJPs, δi, are the hidden states of the backward
pass and the vectors that constitute the gradient
matrices (Section 3.2). In this section, we try to
shed light on what information is encoded in the
VJP. Additionally, we aim to explain how Equa-
tion 9 and Equation 10 cause the model to change
its internal knowledge. For simplicity, we ignore
Dropouts and Layer Norms.
5.1 The VJPs of the Top Layer
In this section, we analyze the initial VJPs that are
created during the editing of a single prompt, as
we describe in Section 3.2. The last matrix of an
LM, which is the final model parameter used be-
fore calculating the loss score, is the decoding ma-
trixD∈Rd×|vocabulary |. During the forward pass,
this matrix calculates the output vectors xiD= ˆyi.
When editing a prompt, we only use the final pre-
diction of the last prompt’s token xnD= ˆynto
calculate the loss score.
We calculated D’s VJP of the last token δn. Ac-
cording to Equation 5, the backward pass’ VJP to
the layer that preceded Dis:
∂L
∂xn=∂L
∂ˆyD⊤=δnD⊤∈Rd(11)
This result can be simplified as a weighted sum of
D’s columns:
δnD⊤=|vocabulary |X
k=1δn[k]D⊤[k], (12)
where D⊤[k]∈Rdis the k-th neuron of Dand also
the embedding of the model’s k-th token. From the
equation, δn[k]∈Rcontrols the magnitude by
which we add the embedding of the k-th token into
∂L
∂xn. Pluggin Equation 4 and using the notation
ˆp=Softmax (ˆyn), tfrom Equation 2:Lemma 5.1. The VJP δnD⊤passed at the begin-
ning of a backward pass is a vector in Rdthat is a
sum of weighted token embeddings. It is dominated
by the embedding of the target token, D⊤[t], multi-
plied by a negative coefficient δn[t] = ˆp[t]−1. The
embedding of all other tokens k̸=tare scaled by
a positive coefficient ˆp[k]D⊤[k].
If we ignore Dropouts and Layer Norms, the
VJPδnD⊤is the initial vector to be passed in the
backward pass. In particular, this is the only VJP
to span the last MLP layer’s gradient. The use of
residual streams implies that while the backward
pass iterates the model in a reverse order, this vector
skips to previous layers, hence it will be part of the
span of all the MLPs’ gradients.
The LL of δn is provided as
Softmax (lnf(δnD⊤). Except for lnf, this
behaves similarly to Softmax (δnD⊤). As the
lemma shows, δnD⊤[t]is negative, while δnD⊤[k]
is positive for all tokens k̸=t. By assuming
homogeneity of the embedding vectors and
independence of the coefficients, we can expect the
target embedding to have the lowest probability in
the softmax. In practice, since related tokens have
more similar embeddings and similar entries in yn,
this effect is expected to be even more pronounced.
5.2 Storing Knowledge in LMs
In Section 4.2 we observed that each neuron in the
MLP’s gradients is a sum of vectors in Rdfrom
the forward and backward passes, xiandδire-
spectively. Based on this observation, we aim to
understand how LM editing with a single prompt
and a single backward pass changes the internal
knowledge of a model. Explicitly, we study the
implications of updating a weight matrix with its
gradients: W←W+η∂L
∂W⊤,where η∈Ris a
negative learning rate.
Lemma 5.2. When updating an MLP layer of an
LM using backpropagation and rerunning the layer
with the same inputs xifrom the forward pass of
the prompt we used for the editing, the following
occurs: (i) The inputs, xi, are added or subtracted
from the neurons of FF 1, thereby adjusting how
much the activations of each corresponding neuron
inFF 2would increase or decrease. (ii) The VJPs
δiare subtracted from the neurons of FF 2, amplify-
ing in FF 2’s output the presence of the VJPs after
they are multiplied with negative coefficients.
See Appendix C for the proof.Figure 3: The Imprint and Shift mechanism of backpropagation.“grad” represent a single neuron from a gradient
matrix. The color of FF1grad is the same as the forward-pass input, while FF2is the same as the new target
embedding, suggesting that they are similar to each other.
Since the change in FF 1uses the given inputs
xito amplify future activation, we term this mecha-
nism “imprint”. The modification of FF 2is termed
as the “shift”, since it represents a process of al-
tering the output of the layer. In summary, the
“imprint and shift” mechanism depicts the MLP’s
learning process during a single backward pass as
having two phases: Given the layer’s original input
and the new target, the process imprints a similar
input through the update of FF 1and subsequently
shifts the output of FF 2towards the new target.
Figure 3 illustrates this process.
LL ranking refers to the index assigned to the
vocabulary’s tokens when ordered by the proba-
bility scores generated by LL (Equation 7). Up-
dating FF 1involves adding or subtracting xifrom
weights, focusing on the most probable tokens from
the LL ranking. Conversely, for FF 2, updating en-
tails subtracting δi, effectively adding −1·δi. This
subtraction reverses the LL rankings, turning pre-
viously least probable tokens into most probable
ones. Thus, when utilizing LL with FF 2’sδi, at-
tention should be given to the least probable tokens
from the projections.
6 Experiments
We conduct a series of experiments to support the
results of Sec. 4 and 5, as well as to briefly demon-
strate their application to LM analysis.
We employ GPT2 (Radford et al., 2019) and
Llama2-7B (Touvron et al., 2023) in our experi-
ments. We randomly sampled 100 prompts and
their corresponding editing targets from the Coun-
terFact dataset (Meng et al., 2022). For each model
and prompt, we conducted a single backpropaga-
tion using SGD and without scaling optimizers,
such as Adam (Kingma, 2014), and no batching.
Figure 4: The percentage of occurrences where the rank
ofFF1’s gradient equals the length of the prompt used
for editing. To show different models in the same plot,
we normalize the layer indices. Except for the last layer,
all layers and models exhibit the above equality more
than98.5%of the time.
The rank of the gradients To examine
Lemma 4.1, we measure the rank of each layer
gradient matrix. As depicted in Figure 4, for every
prompt with the length of ntokens, the model’s
gradient matrices are almost always exactly rank
n. The only exceptions are the last MLP layers,
which have a rank of 1, as predicted in Section 4.
Although unnoticeable from the figure, once in a
few dozen examples, there is a drop of one or two
in the rank of the gradients, indicating linear de-
pendency in xiorδi, see Section 4.1. This is not
a result of a repeated token, since the positional
encoding would still lead to a different xi.
Logit Lens of Gradients Next, we present exam-
ples of our gradients’ interpretation through LL in
Figure 5 and in Appendix D. In each cell of these
plots, the LL projections of the chosen spanning
set (FF 1’sxiandFF 2’sδi) are presented for a
specific layer and a token from the prompt that was
used for the editing.
Prior studies that projected the forward pass ex-
amine the LL projections of hidden states, high-
lighting the gradual change in the projected to-
kens between layers (nostalgebraist, 2020; HavivFigure 5: The gradient of GPT2-small FF2when edit-
ing the model to answer “Paris” for the prompt “Obama
grew up in”. Each cell shows the Logit Lens projection
of the gradient’s VJP ( δi) for a token input and a layer.
Non-English characters are replaced with a question
mark, and long tokens are truncated with “..”. Accord-
ing to Section 5.2, instead of showing the most probable
token in each cell, we display the least probable one.
The color indicates the norm of the VJP, with white cells
indicating that almost no editing is done in practice.
et al., 2023). Similarly, Figure 5 presents a gradual
change in the backward pass’ VJP. Across most lay-
ers, LL reveals that the gradients represent the em-
bedding of “Paris”. Other projections have seman-
tics that are related to “Paris” such as “Macron”,
the family name of the President of France. The
norm of the VJP is indicted by color, and, in the
top layers, the only meaningful updates are for the
token “Paris”. Some of the edits in the lower layers
are harder to explain, similarly to the situation in
those layers for the vanilla LL of the forward pass.
Impact of Different Segments of the Prompt
We observe that while all the prompt’s tokens con-
tribute to the gradient construction (Equation 8),
the majority of these contributions are done by
VJPs, δi, with a close-to-zero norm. Furthermore,
upon examining the LL of every individual neuron
from the gradient matrix (Appendix B), we found
that all the projected tokens are correlated with only
1-2 vectors we can identify from the spanning sets
presented in Section 4.2.
To discern the relative importance of tokens and
layers in the gradient reconstruction, we divide
each prompt’s tokens into segments and plot their
δimean norm. This experiment is done with GPT2-
xl, due to its extensive use in prior work on inter-
pretability research.
Results are depicted for FF 2in Figure 6, see
Appendix E.1 for FF 1. Evidently, predominant
updates occur in two main areas: (1) by the sub-
Figure 6: The norm of GPT2-xl’s FF2’s VJPs ( δi) as a
function of the layers’ index and segments of the edited
prompts. White color represents close-to-zero updates
with almost no effect on the model’s weights.
ject’s tokens in the initial layers, and (2) by the
last prompt’s token around the second quarter of
the layers. The majority of other tokens exhibit a
norm close to zero throughout the layers, indicat-
ing that they have almost no effect on the updating.
We hypothesize that the changes to the last subject
token may involve editing the information trans-
ferred by the subject’s token through attention, as
demonstrated by Geva et al. (2023).
A complementary view is provided by consid-
ering, for the LL rank of each VJP δi(labeled by
the segment of token iof the input) the rank of the
target token. Figure 7 illustrates that the VJP of the
last token from the edited prompts, δn, consistently
ranks the target token among the least probable
ones. The VJPs of other tokens from the edited
prompt, δi, exhibit comparable behavior, generally
ranking the target token as improbable.
The result reveals that along the first and last
layers, some of the δishow degradation in their
ranking of the target token, which we attribute
to their low norms as reflected by Figure 6. We
demonstrate in Appendix F that normalizing δibe-
fore LL magnifies the presence of the target token.
Specifically, the drop at the model’s last layer is
due to the fact that apart from the last prompt’s
token, all the others have a zero vector δiat that
layer (Appendix A).
Please note that the degradation of this rank in
the first few layers might be related to the gap in LL
interpretability for the earlier layers discussed in
Section 3.3. In Appendix E.2 we provide a similar
analysis for FF 1’s gradients.Figure 7: The Logit Lens rank of the target token for
GPT2-xl FF2’s VJPs, δi. Most gradients tend to rank
the target token as one of the least probable tokens, with
the last token consistently ranking it as such. We found
that the degradation of the first and last layers can be
attributed to the proximity of certain δinorms to zero.
In the initial layers, we posit that Logit Lens is less
effective, thereby resulting in lower readability for the
earlier layers.
7 Application: Editing Based on the
“Shift” Mechanism
Prior work (Mitchell et al., 2021; Meng et al., 2022,
2023) introduced editing methods that change only
FF 2’s matrices. In Section 5.2 we identify the
“shift” mechanism of editing FF 2. In Section 6 we
observe that the dominant components in construct-
ing the gradients are derived from the outer product
of the last token’s input and VJP, x⊤
n·δn, and that
δncontains the embedding of the target token.
We hypothesize that we can edit LMs’ internal
knowledge by updating only a single FF 2matrix
with a single forward pass and an approximation of
the VJP, thereby eliminating the need for the back-
ward pass. Based on Lemma 5.1, the embedding of
the editing target is annotated by D⊤[t], where Dis
the decoding matrix and tis the index of the target
token. Our experimental method works as follows:
(i) We choose an MLP layer we wish to edit, ˆFF 2
(predefined as a hyper-parameter according to Ap-
pendix G). (ii) We run a single forward pass with
the prompt whose output we want to edit. (iii) Dur-
ing the forward pass, we collect the last token input
for the layer we want to edit, xn. (iv) We collect the
embedding of the target token D⊤[t]and (v) update
the MLP matrix by ˆFF 2←ˆFF 2+η·x⊤
n·D⊤[t],
where ηis the learning rate. We name this method
“forward pass shifting”.
We examined our method on 1000 samples fromMETHOD EFF↑PAR↑N-GRAM↑
ORIGINAL MODEL 0.4 0.4 626.94
FINETUNING (MLP 0) 96.4 7.46 618.81
FINETUNING (MLP 35) 100.0 46.1 618.50
MEND 71.4 17.6 623.94
ROME 99.4 71.9 622.78
MEMIT 79.4 40.7 627.18
FORWARD PASS SHIFT 99.4 41.6 622.45
Table 1: GPT2-xl single editing results for Counter-
Fact. Efficacy (EFF) represents the editing success rate
(accuracy). Paraphrase (PAR) denotes the accuracy of
predicting the new target for phrases derived from the
edited prompt. N-gram measures generation fluency
using weighted bi- and tri-gram entropies.
CounterFact (Table 1; the full results and addi-
tional implementation details are presented in Ap-
pendix G), and found that for single editing our ap-
proach is on par with the state-of-the-art methods
MEND (Mitchell et al., 2021), ROME (Meng et al.,
2022) and MEMIT (Meng et al., 2023), in editing
a given prompt, but it falls short in comparison to
ROME in generalization (editing paraphrases) and
specificity (see Appendix). However, our method
has much lower runtime complexity and does not
employ a multi-step (iterative) execution. Over-
all, our results suggest we might be able to find
“shortcuts” in the implementation of fine-tuning by
injecting tokens directly into LMs’ layers.
8 Conclusions
Other LL-type interpretability contributions shed
light on LMs through the forward pass. Here, we
show that gradients can be projected into the vocab-
ulary space and utilize the low-rank nature of the
gradient matrices to explore the backward pass in
an interpretable way. As we show, the gradients are
best captured by a spanning set that contains either
the input to each layer, or its VJP. These two com-
ponents, which are accessible during the forward
and backward passes, are used to store information
in the MLP layers, using a mechanism we call “im-
print and shift”. We provided experimental results
to substantiate the results of our analysis, including
an editing method that only requires a single for-
ward pass, but is on par with the SOTA knowledge
editing methods.9 Limitations
Our use of LL in projecting gradients has limita-
tions when it comes to explaining the gradients
of earlier layers. At this point, it remains unclear
whether gradients operate in the same embedding
space across all layers or if another transforma-
tion is required for projecting earlier layers. This
question is currently being explored for the for-
ward pass (see Section 3.3), suggesting additional
learned transformations to the first layers. Given
the lack of a wide consensus on this additional
transformation, we have opted to employ only the
original LL projection in our analysis. Furthermore,
some recent contributions against LL argue that this
method is more correlated with LMs’ behaviors,
rather than causally explaining them. Our work
shows that at least in the later layers of LMs, token
embeddings are directly placed into the weights of
the LM, making LL projections well-justified.
Recently, alternative approaches have been pro-
posed to explain LMs by intervening in the for-
ward pass (Meng et al., 2022). When combined
with token projection methods, this approach holds
promise in providing insights into the “thinking”
process of LMs (Ghandeharioun et al., 2024).
Our work ignores the additional scaling that is in-
troduced by optimizers other than Stochastic Gradi-
ent Descent, such as Adam (Kingma, 2014). While
the backward pass’s VJPs remain unaffected when
such optimizers are employed, they do alter the
rank and weights of each gradient matrix, due to
the additional scaling.
Our approach to explaining how knowledge is
stored in LMs is grounded in single editing with
a constant embedding. While our approach eluci-
dates how models store various information, fine-
tuning is typically conducted on multiple prompts
and involves multiple steps (iterations). Addition-
ally, training a model from scratch includes the
training of its embeddings.
Our experimental approach to editing LMs with
“forward pass shift” is presented as a case study
rather than as a suggested alternative to existing
methods. The results in Section 7, Appendix G
might obfuscate “editing” and “output shifting”,
since only plotting the desired answers does not
fully encapsulate the effect of the edit on similar
prompts, which is a challenge faced by most editing
benchmarks and datasets.
Our focus on the MLP layers excludes the atten-
tion layers. This decision is influenced by the grow-ing consensus that MLPs are where LMs predom-
inantly store information (Dai et al., 2022; Meng
et al., 2022). We acknowledge the possibility that
attention layers may also store information and that
editing MLPs and attention simultaneously could
have different effects on the model from those de-
tailed in Section 5.2.
Our theoretical analysis disregards certain com-
ponents of LMs, such as Dropouts, Layer Norms,
positional embedding and bias vectors. We ac-
knowledge that these components may have dis-
tinct effects on the interpretation of the backward
pass, but without these simplifications the deriva-
tions are laden with additional terms.
Lastly, our work was conducted on Decoder LMs
with sequential architecture. It is important to note
that other types of LMs might exhibit different
behaviors in terms of their gradients.
10 Ethics and Impact Statement
This paper presents work whose goal is to advance
the field of Machine Learning. There are many
potential societal consequences of our work, none
which we feel must be specifically highlighted here.
However, future research could use the methods we
developed to edit LMs. We hope such cases would
be for developing better and safer models, rather
than promoting harmful content.
Acknowledgements
This work was supported by the ISRAEL SCI-
ENCE FOUNDATION (grant No. 448/20), an
Open Philanthropy alignment grant, and an Azrieli
Foundation Early Career Faculty Fellowship.
References
Yonatan Belinkov and James Glass. 2019. Analysis
methods in neural language processing: A survey.
Transactions of the Association for Computational
Linguistics , 7:49–72.
Nora Belrose, Zach Furman, Logan Smith, Danny Ha-
lawi, Igor Ostrovsky, Lev McKinney, Stella Bider-
man, and Jacob Steinhardt. 2023. Eliciting latent
predictions from transformers with the tuned lens.
arXiv preprint arXiv:2303.08112 .
Emily M Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? In Proceedings of the 2021 ACM confer-
ence on fairness, accountability, and transparency ,
pages 610–623.Christopher Bishop. 2006. Pattern recognition and ma-
chine learning. Springer google schola , 2:531–537.
Trenton Bricken, Adly Templeton, Joshua Batson,
Brian Chen, Adam Jermyn, Tom Conerly, Nick
Turner, Cem Anil, Carson Denison, Amanda Askell,
Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas
Schiefer, Tim Maxwell, Nicholas Joseph, Zac
Hatfield-Dodds, Alex Tamkin, Karina Nguyen,
Brayden McLean, Josiah E Burke, Tristan Hume,
Shan Carter, Tom Henighan, and Christopher
Olah. 2023. Towards monosemanticity: Decom-
posing language models with dictionary learning.
Transformer Circuits Thread . Https://transformer-
circuits.pub/2023/monosemantic-
features/index.html.
Hila Chefer, Idan Schwartz, and Lior Wolf. 2022. Op-
timizing relevance maps of vision transformers im-
proves robustness. Advances in Neural Information
Processing Systems , 35:33618–33632.
Kevin Clark. 2017. Computing neural network gradi-
ents.
Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert
Huben, and Lee Sharkey. 2023. Sparse autoencoders
find highly interpretable features in language models.
arXiv preprint arXiv:2309.08600 .
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao
Chang, and Furu Wei. 2022. Knowledge neurons in
pretrained transformers. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 8493–
8502.
Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.
2022. Analyzing transformers in embedding space.
arXiv preprint arXiv:2209.02535 .
Alexander Yom Din, Taelin Karidi, Leshem Choshen,
and Mor Geva. 2023. Jump to conclusions: Short-
cutting transformers with linear transformations.
arXiv preprint arXiv:2303.09435 .
Yogesh K Dwivedi, Nir Kshetri, Laurie Hughes,
Emma Louise Slade, Anand Jeyaraj, Arpan Kumar
Kar, Abdullah M Baabdullah, Alex Koohang, Vish-
nupriya Raghavan, Manju Ahuja, et al. 2023. “so
what if chatgpt wrote it?” multidisciplinary perspec-
tives on opportunities, challenges and implications
of generative conversational ai for research, prac-
tice and policy. International Journal of Information
Management , 71:102642.
N Elhage, N Nanda, C Olsson, T Henighan, N Joseph,
B Mann, A Askell, Y Bai, A Chen, T Conerly, et al.
2021. A mathematical framework for transformer
circuits.
Nelson Elhage, Tristan Hume, Catherine Olsson,
Nicholas Schiefer, Tom Henighan, Shauna Kravec,
Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,
Carol Chen, Roger Grosse, Sam McCandlish, Jared
Kaplan, Dario Amodei, Martin Wattenberg, andChristopher Olah. 2022. Toy models of superpo-
sition.
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir
Globerson. 2023. Dissecting recall of factual associa-
tions in auto-regressive language models. In Proceed-
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 12216–12235,
Singapore. Association for Computational Linguis-
tics.
Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval
Sadde, Micah Shlain, Bar Tamir, and Yoav Goldberg.
2022a. LM-debugger: An interactive tool for inspec-
tion and intervention in transformer-based language
models. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations , pages 12–21, Abu Dhabi,
UAE. Association for Computational Linguistics.
Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-
berg. 2022b. Transformer feed-forward layers build
predictions by promoting concepts in the vocabulary
space. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Process-
ing, pages 30–45, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer
Levy. 2021. Transformer feed-forward layers are
key-value memories. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 5484–5495.
Asma Ghandeharioun, Avi Caciularu, Adam Pearce,
Lucas Dixon, and Mor Geva. 2024. Patchscope:
A unifying framework for inspecting hidden rep-
resentations of language models. arXiv preprint
arXiv:2401.06102 .
Almog Gueta, Elad Venezian, Colin Raffel, Noam
Slonim, Yoav Katz, and Leshem Choshen. 2023.
Knowledge is a region in weight space for fine-tuned
language models. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
1350–1370, Singapore. Association for Computa-
tional Linguistics.
Adi Haviv, Ido Cohen, Jacob Gidron, Roei Schuster,
Yoav Goldberg, and Mor Geva. 2023. Understanding
transformer memorization recall through idioms. In
Proceedings of the 17th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL 2023, Dubrovnik, Croatia, May 2-6,
2023 , pages 248–264. Association for Computational
Linguistics.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. LoRA: Low-rank adaptation of
large language models. In International Conference
on Learning Representations .
Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-
man, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali
Farhadi. 2022. Editing models with task arithmetic.InThe Eleventh International Conference on Learn-
ing Representations .
Sakshi Indolia, Anil Kumar Goswami, and Pooja Asopa.
2018. Conceptual understanding of convolutional
neural network-a deep learning approach. Procedia
computer science , 132:679–688.
Shahar Katz and Yonatan Belinkov. 2023. VISIT: Vi-
sualizing and interpreting the semantic information
flow of transformers. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
14094–14113, Singapore. Association for Computa-
tional Linguistics.
DP Kingma. 2014. Adam: a method for stochastic
optimization. In Int Conf Learn Represent .
Y Le Cun. 1988. A theoretical framework for backprop-
agation. In Proceedings of the 1988 Connectionist
Models Summer School .
Kevin Meng, David Bau, Alex Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual asso-
ciations in GPT. Advances in Neural Information
Processing Systems , 36.
Kevin Meng, Arnab Sen Sharma, Alex Andonian,
Yonatan Belinkov, and David Bau. 2023. Mass-
editing memory in a transformer. International Con-
ference on Learning Representations .
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els. In International Conference on Learning Repre-
sentations .
Vivek Miglani, Aobo Yang, Aram Markosyan, Diego
Garcia-Olano, and Narine Kokhlikyan. 2023. Using
captum to explain generative language models. In
Proceedings of the 3rd Workshop for Natural Lan-
guage Processing Open Source Software (NLP-OSS
2023) , pages 165–173.
Beren Millidge and Sid Black. 2022. The singular value
decompositions of transformer weight matrices are
highly interpretable.
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea
Finn, and Christopher D Manning. 2021. Fast model
editing at scale. In International Conference on
Learning Representations .
nostalgebraist. 2020. interpreting gpt: the logit lens.
Chris Olah, Nick Cammarata, Chelsea V oss, Ludwig
Schubert, and Gabriel Goh. 2020. Naturally oc-
curring equivariance in neural networks. Distill ,
5(12):e00024–004.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
blog.Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov,
Jonathan Berant, and Amir Globerson. 2023. What
are you token about? dense retrieval as distributions
over the vocabulary. In Proceedings of the 61st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 2481–
2498, Toronto, Canada. Association for Computa-
tional Linguistics.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. nature , 323(6088):533–536.
Wojciech Samek, Thomas Wiegand, and Klaus-Robert
Müller. 2017. Explainable artificial intelligence: Un-
derstanding, visualizing and interpreting deep learn-
ing models. CoRR , abs/1708.08296.
Soumya Sanyal and Xiang Ren. 2021. Discretized in-
tegrated gradients for explaining language models.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
10285–10299.
Gabriele Sarti, Nils Feldhus, Ludwig Sickert, and Os-
kar van der Wal. 2023. Inseq: An interpretability
toolkit for sequence generation models. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 3: System
Demonstrations) , pages 421–435, Toronto, Canada.
Association for Computational Linguistics.
Adi Simhi and Shaul Markovitch. 2023. Interpreting
embedding spaces by conceptualization. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pages 1704–1719,
Singapore. Association for Computational Linguis-
tics.
K Simonyan, A Vedaldi, and A Zisserman. 2014. Deep
inside convolutional networks: visualising image
classification models and saliency maps. In Proceed-
ings of the International Conference on Learning
Representations (ICLR) . ICLR.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta, Adrià
Garriga-Alonso, et al. 2023. Beyond the imitation
game: Quantifying and extrapolating the capabili-
ties of language models. Transactions on Machine
Learning Research .
Alex Tamkin, Mohammad Taufeeque, and Noah D
Goodman. 2023. Codebook features: Sparse and
discrete interpretability for neural networks. arXiv
preprint arXiv:2310.17230 .
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon
Du. 2023. Scan and snap: Understanding training dy-
namics and token composition in 1-layer transformer.
arXiv preprint arXiv:2305.16380 .
Curt Tigges, Oskar John Hollinsworth, Atticus Geiger,
and Neel Nanda. 2023. Linear representations of
sentiment in large language models. arXiv preprint
arXiv:2310.15154 .Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron
Mueller, Byron C Wallace, and David Bau. 2023.
Function vectors in large language models. arXiv
preprint arXiv:2310.15213 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,
Fandong Meng, Jie Zhou, and Xu Sun. 2023. Label
words are anchors: An information flow perspective
for understanding in-context learning. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pages 9840–9855,
Singapore. Association for Computational Linguis-
tics.
Quan-shi Zhang and Song-Chun Zhu. 2018. Visual
interpretability for deep learning: a survey. Frontiers
of Information Technology & Electronic Engineering ,
19(1):27–39.A The Rank of The Last Layer
In Section 4.1, we delve into the observation that
each gradient matrix has a rank equal to the length
of the edited prompt (annotated by n), except for
the last layer’s ones. In this section, we explain why
the last layer’s MLP matrices are always rank-1.
The backward pass, applied to the final loss score
(L, Section 3.2), generates a computational graph
that is reversed in direction compared to the for-
ward pass Section 3.2. It begins with the loss score
and the matrices of the last layer, proceeding in
reverse order until reaching the matrices of the
first layer. This computational graph encapsulates
every hidden state and intermediate result that con-
tributed to the final prediction, which is the output
of the last layer for the last token in the prompt.
One might initially assume that, since the last
prediction was formed by the input of the last to-
ken, only its hidden states would be involved in
this computational graph. However, due to the
attention mechanism, hidden states from previous
forward passes can be recalled and utilized in subse-
quent forward passes, contributing to all the tokens
that follow them in the prompt. The last hidden
state to be recalled using the attention modules is
called at the model’s last layer’s attention module,
which precedes an MLP module in sequential ar-
chitectures, such as GPT2 and Llama-7B. Hence,
in every layer, MLP inputs in the reverse computa-
tional graph comprise all individual intermediate
inputs xifrom the forward pass of each token in the
prompt. However, at the last layer, the only input
included is the one belonging to the last prompt
token xn. For this reason, also only the VJP of
the last token, δn, is included in the reconstruction
of the gradients of the last MLP layer, while the
δifor all the other tokens from the prompt are not
included (or more correctly, they are equal to the
zero vectors).
When constructing the gradients using xiand
δi, the rank of each layer is equal up to the num-
ber of xi, δiinvolved in the computational graph
(assuming linear independence Section 4.1). This
implies that all layer matrices are formed by∂L
∂W=Pn
i=1x⊤
i·δiexcept for the last layer, which is
constructed with∂L
∂W=x⊤
n·δn, which is rank-1.
In our study, especially in our figures and tables,
we decided to include all the vectors of the last
layer, including those from tokens which are not the
prompt’s last, which are thus equal to zero vectors.
This approach is also the reason for the observedchanges in the behavior of the gradients in some
figures. For example, in Figure 7 we can see that
all the graphs (except for the last token’s) converge
to the same value at the last layer. The reason for
this is that they are all equal to the zero vector. In
Figure 11 we see the LL projection of the VJPs
from the model’s last layer, which are equal to
projecting the zero vector.
B Why Decomposed Gradient Analysis
Makes Sense
In Section 3.3, Section 4.2 we establish our inter-
pretation of gradients via spanning sets. This ap-
proach is based on the understanding that each neu-
ron in the gradient matrix is formed by the linear
combination of xi(the forward pass’s intermediate
inputs) or δi(VJPs, the backward pass’s hidden
states). In this section, we aim to illustrate, through
a singular example, why analyzing a gradient ma-
trix through its spanning set is more informative
and simpler compared to attempting to analyze the
full gradient matrix.
We use GPT2-medium (24 layers and 330M pa-
rameters) for our examination. We examine the
MLP gradients using the prompt “Lionel Messi
plays for”, to which the model responds with
“Barcelona”. We edit the model with a single back-
ward pass to respond “Paris”. In the case of this
model, each MLP matrix comprises 4096 neurons.
Consequently, to apply the Logit Lens (LL) pro-
jection to a particular gradient matrix, the process
needs to be applied 4096 times.
We start by analyzing FF 2from layer 14 . In
Table 2 we present samples of gradient neurons’
projections by LL. In Section 4.2, we elaborate on
how each neuron is formed by multiplying an inter-
pretable vector by a coefficient ( δiandxi), which
in turn dictates its norm. We group these neurons
based on their norms, unveiling shared projections
among neurons within the same norm group. To
underscore the proximity of certain neurons to the
zero vector, we also include the projection of the
zero vector. Expanding Table 2 to every individual
neuron in the matrix is impractical, given the chal-
lenge of reading a table with 4096 rows. Instead,
we use plots Figure 8: the initial plot presents the
LL intersection, measuring the extent of overlap
between the top 100 most probable tokens from
two vectors, while the second shows the cosine
similarity between the vectors.
We repeat the process after sorting the gradientGROUP NORM LL TOP LL BOTTOM
BIGGEST
BY NORM1.212 Paris ,Paris ,Marse ufact ,Logged ,otomy
0.352 Paris ,Paris ,Marse ufact ,Spanish ,Gerr
0.297 Paris ,Paris ,Copenhagen ceremonial ,cade ,uana
MEDIUM
BY NORM0.033 ufact ,Gerr ,sheriff Paris ,Paris ,ienne
0.033 Logged ,turtle ,ceremon Paris ,Paris ,France
0.033 ufact ,sec,recess Paris ,Paris ,qus
SMALLEST
BY NORM0.001 ,,the,and VIDIA ,advertisement ,Dialogue
0.001 ,,the,and VIDIA ,advertisement ,Magikarp
0.001 ,,the,and VIDIA ,advertisement ,Companies
ZERO
VECTOR0 ,,the,and VIDIA ,advertisement ,Companies
Table 2: Sample of gradient’s neurons projection via Logit Lens (LL) from GPT2-medium, FF2matrix, layer
14.LL TOP stands for the most probable tokens via LL, while BOTTOM are the most improbable ones. In this
example, we edit the prompt “Lionel Messi plays for” with the editing target “Paris”. In the projected tokens we
notice the predominance of “Paris”, and also that gradient’s neurons with a relatively low norm project the same
tokens as the zero-vector.
PROMPT TOKEN NORM LL TOP LL BOTTOM
L 0.029 Rutherford ,Apost ,PROG Paris ,Paris ,ienne
ion 0.035 ramid ,ngth ,livest France ,ée,É
el 0.067 unlaw ,owship ,arantine Libyan ,Libya ,France
Messi 0.165 ğ,relic ,ejected vu ,igmat ,tain
plays 0.026 surv ,POV,NTS Paris ,hotels ,Merit
for 0.165 ceremonial ,ado, Paris ,Paris ,Copenhagen
Table 3: The Logit Lens of the VJPs ( δi) of GPT2-medium, FF2matrix, layer 14. Notice the dominance of “Paris”
(the editing target) in the projected vocabulary and the norm ratio of the vectors.
neurons according to their norms Figure 9. The
gradients with the higher norms, are almost identi-
cal to the last VJP ( δn), with alignment extending
up to the sign of the vectors. In Table 3 LL re-
veals that these VJPs project “Paris”. Gradients
with low norms may appear correlated with parts
of the spanning set’s vectors ( FF 2’sδi), yet they
are more correlated to the zero vector, emphasiz-
ing that these neurons do not update the model’s
weights (induce minimal change).
In addressing color shifting, we see around index
500 from the right that this is where the activations
change sign from positive to negative. The nega-
tive learning rate causes the positive activation to
add “Paris” into those neurons, while the negative
activation reduces “Paris”. In both cases, the pro-
cess causes the model to add “paris” in the same
direction (Section 5.2).We repeat this type of analysis with FF 1. We
remind that our interpretation for this layer’s span-
ning set is its inputs xiSection 4.2. Again, we
see the alignment between individually analyzing
the neurons of the gradients and the spanning set
Table 4, Table 5, Figure 10.
In conclusion, In this section, we demonstrate
the converging results of analyzing individual gra-
dient neurons via LL and the spanning sets inter-
pretation. The aim of this analysis is to emphasize
the efficacy of employing these spanning sets to
simplify experiments involving a vast number of
vectors (neurons) into a much smaller, representa-
tive subset. The advantage of this simplification is
twofold: it conserves computational resources and
reduces time expenditure.(a) Logit Lens intersection
(b) Cosine Similarity
Figure 8: VJPs ( δi, the spanning set of FF2) and single gradient’s neurons comparison for layer 14’s FF2. In
Figure 8a a positive score is the amount of shared tokens between the top 100 most probable tokens after projecting
each vector. A negative score is presented if multiplying one of the vectors by −1produces a higher amount of
shared tokens (than the score presented with a negative sign). The reason we apply this −1multiplication is to
address cases where two vectors have high correlation up to their sign (similarly to two vectors that overlap each
other after one of them is multiplied by −1, making their cosine similarity negative).
(a) Logit Lens intersection
(b) Cosine Similarity
Figure 9: VJPs ( δi) and single gradient’s neurons (sorted by norm) comparison for layer 14’s FF2. It is noteworthy
that high-norm neurons, corresponding to those activated with positive activations during the forward pass, inject
the VJP of “for” with a flipped sign. Medium-sized norm neurons also exhibit correlation with the representative
vector of “for”. Smallest by-norm neurons show minimal correlation; we refer to their proximity to the zero vector.GROUP NORM LL TOP LL BOTTOM
BIGGEST
BY NORM0.438 Cruz ,ization ,ize mathemat ,trave ,nodd
0.422 Football ,Jr,Team theless ,challeng ,neighb
0.369 psychiat ,incent ,theless Jr ,Junior ,Sr
MEDIUM
BY NORM0.02 the,a,hire irez ,inelli ,intosh
0.02 the,a,one theless ,Magikarp ,irez
0.02 perpend ,coerc ,incent Junior ,Jr,Football
SMALLEST
BY NORM0.002 ,,the,and Magikarp ,VIDIA ,advertisement
0.002 ,,the,and advertisement ,VIDIA ,Magikarp
0.001 ,,the,and VIDIA ,advertisement ,Companies
ZERO
VECTOR0 ,,the,and VIDIA ,advertisement ,Companies
Table 4: Samples of gradient’s neurons projection via Logit Lens (LL) from GPT2-medium, FF1matrix, layer 14.
PROMPT TOKEN NORM LL TOP LL BOTTOM
L 9.499 ,, ,the FontSize ,7601 ,Magikarp
ion 7.808 fall ,fish ,wood nodd ,incorpor ,accompan
el 7.728 Cruz ,McC,Esp perpend ,mathemat ,shenan
Messi 6.944 Jr,Junior ,Sr theless ,psychiat ,incent
plays 6.715 football ,golf ,alongside hedon ,ilts ,uries
for 6.596 Team ,team ,a irez ,newsp ,Magikarp
Table 5: The Logit Lens of the inputs ( xi) of GPT2-medium, FF1matrix, layer 14. According to our observation
Section 5.2, those are also the embeddings the gradients inject into the model’s weights.
(a) Logit Lens intersection
(b) Cosine Similarity
Figure 10: The inputs xi(which are FF1’s spanning set) and single gradient’s neurons (sorted by norm) comparison
for layer 14’s FF1.C Proof of Lemma 5.2
Lemma 5.2. When updating an MLP layer of an
LM using backpropagation and rerunning the layer
with the same inputs xifrom the forward pass of
the prompt we used for the editing, the following oc-
curs: (i) The inputs, xi, are added to or subtracted
from the neurons of FF 1, thereby adjusting how
much the activations of each corresponding neuron
inFF 2increase or decrease. (ii) The VJPs δiare
subtracted from the neurons of FF 2, amplifying in
FF 2’s output the presence of the VJPs after they
are multiplied with negative coefficients.
Proof. In Section 4.2, we revealed that FF 1
weights are updated by injections (adding or sub-
tracting vectors) of its xi. This update is done
according to the coefficients of its δi, after multi-
plying them with the learning rate. If we rerun the
same layer with the same input after the update, we
obtain the following output per each neuron j:
xi·(FF1[j] +η·x⊤
i·δi[j]) =
xi·FF1[j] +xi·η·x⊤
i·δi[j] =
xi·FF1[j] +∥xi∥2
2·η·δi[j]∈R(13)
xi·FF1[j]is the pre-edit output of this neuron.
The second component, ∥xi∥2
2·η·δi[j], is derived
from the update and can be positive or negative,
hence it controls the increment or decrement of
this output compared to the pre-edit one. In mod-
els with monotonic (or semi-monotonic) activation
functions, such as ReLU (GeLU is positive mono-
tonic only from around −0.75), the activation of
the corresponding neuron in FF 2will be changed
directly by this addition to that output.
In Section 4.2, we show how FF 2’sδiform the
gradient matrix. Consider the result of updating
onlyFF 2and rerunning the same layer:
xi[j]·(FF2[j] +η·δi·xi[j]) =
xi[j]·FF2[j] +η·(xi[j])2·δi∈Rd(14)
xi[j]·FF2[j]represents the original output of this
neuron. Since (xi[j])2is always non-negative and
ηis negative, the update is a subtraction of δifrom
each neuron.D Additional Logit Lens of Gradients
Examples
The work that presented Logit Lens (LL) (nostal-
gebraist, 2020) established this method by con-
structing tables of the projected results of different
prompts, illustrating the tokens each individual for-
ward pass represents at each layer of the model. In
this section, we present a similar approach, focus-
ing on the backward pass rather than the forward
pass. In the following figures, we provide exam-
ples of the LL projections of gradients when they
are interpreted as combinations of the intermediate
inputs xior VJPs δiaccording to Section 4.2.
In addition to illustrating the information stored
in the gradient matrices, the following tables also
describe the information moving through LMs dur-
ing the forward and backward pass. Our interpreta-
tion of FF 1using xireflects the gradual buildup in
the prediction of the forward pass, as xirepresents
the intermediate inputs for each MLP layer. How-
ever,FF 2’sδi, VJPs, serve as the backpropagation
counterpart to the forward pass xi. We can concep-
tualize them as the input of the MLP when execut-
ing the backward pass, or as the error propagated
from later layers to earlier ones. In conclusion, Fig-
ure 11, 12, 13, 14, 15 depict the information
stored within the gradients, simultaneously also
comparing the information revealed by LL from
the forward pass ( xi, known from prior studies)
with that from the backward pass ( δi), which is part
of our innovative contribution.Figure 11: GPT2-medium MLP gradients via our spanning set interpretation. Each cell illustrates the Logit Lens
of the gradient’s spanning set according to a layer and a token from the edited prompt. According to Section 5.2
observation, for FF1we present the projections of the most probable tokens for the intermediate inputs xi. For
FF2we show most improbable tokens for the VJPs δi. The color indicates the norm of the gradient’s δi. In this
example, the prompt is “Lionel Messi plays for”, to which the model responds with “Barcelona”. The new target is
“Paris”. Notably, the target token “Paris” is evident through the majority of FF2’s VJPs, which are the vectors that
are injected into FF2’s weights. On the other hand, FF1reflects not only the tokens that the gradients try to inject
intoFF1, but also the model’s intermediate predictions at each MLP layer during the forward pass. It is worth
noting that the presence of tokens with a less clear meaning, such as “VIDIA” in FF2results from the projection of
vectors with almost zero norm, as exampled in (Table 2). This implies that these subcomponents of the gradient
exert negligible influence when updating the model.Figure 12: GPT2-xl MLP gradients via Logit Lens for “Ipod Nano, developed by” which we edit from “Apple” to
“BMW”. Due lack of space, we show only every second layer. The color scheme indicates that the primary focus of
editing lies on the subject token “Pod”. We hypothesize that the word “Ipod” is highly related to “Apple”, so by
targeting “pod”, the gradients edit the relation between Ipod and the company that created it. Question marks and
empty boxes are non-English tokens.Figure 13: GPT2-xl FF2gradients via Logit Lens. The editing tries to change the prompt “Pikachu is a type of” to
answer “music” instead of “Pokémon”. Every cell shows the most <probable \improbable >tokens for its δi. The
fact that we see FF2’s last token’s project “Pokémon / music” means the edit tries to subtract from the model’s
weights the embedding of “Pokémon”, while adding the embedding of “music” (same mechanism with “mammals /
music”). Regarding the presence of many non-English tokens (question marks and empty boxes), in Appendix F,
we present the same table with “Normalized Logit Lens”, revealing the embedding of the target token “music”.Figure 14: Llama2-7B FF2gradients via Logit Lens. The editing tries to change the prompt “Eddy Cue is employed
by” to answer “BBC” instead of “Apple” (template was taken from the CounterFact dataset). Every cell shows the
most <probable \improbable >tokens for its δi. 3 main patterns can be observed from the table: (1) The part that
has the largest VJPs by norm, which we assume is where most of the editing is done, is in the last subject token
(“cu”) and during the first few layers of the model. (2) We do not see the target token “BBC” in the most improbable
projection of the last token; instead, the most improbable one is the empty token “”. When we examined these
projections we found “BBC” to be only the third most improbable token. However, in other VJPs we notice this is
indeed the most improbable projection. (3) The original answer of the model, “Apple”, frequently emerges in the
projections of the last token. As outlined in Section 5.2, given that we update the model with a negative learning
rate, when the VJPs Logit Lens projection ranks a token as highly probable, subtracting the gradients from the
model’s weights equals to subtracting the embedding of this probable token from them . Consequently, this pattern
illustrates the mechanism of “shift” within the context of “imprint and shift”: We decrease the probability associated
with “Apple” in pursuit of increasing the probability of “BBC”.Figure 15: Llama2-7B FF2gradients via Logit Lens. The editing tries to change the prompt “Lionel Messi plays
for” to answer “Paris” rather than “Barcelona”. Every cell shows the 2 most probable and improbable tokens of
the VJPs δiin the format of <probable \improbable >. The target token, “Paris” is the second most improbable
token in the projections of the last prompt’s token (“for”), only second to the empty token (“”). The projection’s
most probable tokens of the last prompt’s token are “Barcelona” (the final prediction of the model for this prompt),
with some projections also revealing “Argentina” (which was the second most probable prediction of the model).
In addition, the projections of the last subject token from the prompt (“i”) show that among the most improbable
tokens there are appearances of “France”, which can be associated to its capital, “Paris”. Together, these projections
illustrate how backpropogation editing tries to add into the model’s FF2’s neurons (weights) the embedding of
“Paris” (and related concepts, e.g. “France”) while removing the embeddings of “Barcelona” and “Argentina”.E Additional Empirical Results
E.1 Impact of Different Segments of the
Prompt in Every Layer
In Section 6, we elucidate how comparing the norm
of each VJP ( δi) involved in the construction of a
gradient matrix can reflect which layers are updated
more than others. Similar comparison is conducted
for the tokens from the edited prompt, uncover-
ing that certain layers and segmentation from the
prompt do not contribute significantly to the updat-
ing process.
We extend this analysis from Section 6 to demon-
strate the ability to identify the main editing matri-
ces in every type of module in LMs Figure 16.
We will solely discuss the results related to the
MLP layers, as we did not examine the atten-
tion modules in our study. Both the FF 1module
(mlp.c_fc ) and FF 2module ( mlp.c_proj ) demon-
strate that the primary editing occurs around the
first quarter of layers by the last subject token, and
around the middle of the layers by the last token.
The majority of the other layers and tokens ex-
hibit VJP norms close to zero, indicating that they
scarcely contribute to updating the model’s weights.
The same pattern is also evident for the VJPs be-
tween transformer layers ( transformer.h ), as, if we
disregard Dropouts and Layer Norms, the VJPs of
FF 2are the same as the one between the trans-
former layers.
We also provide similar figures where, instead
of plotting the norm of the VJPs, we compare the
norms of the intermediate inputs to every layer ( xi)
(Figure 17). The inclusion of those figures is solely
to emphasize that there is no correlation between
the norm of xiandδi.Figure 16: δinorm for each of GPT2-xl sub-module (names are according to the HuggingFace implementation).
This is an extension to Section 6, showing how our approach is easily adapted to any submodule of the model. The
layer of FF1is represented by mlp.c_fc andFF2bymlp.c_proj .
Figure 17: xinorm for GPT2-xl. Provided to show no correlation with Figure 16.E.2 The Ranks of FF 1and the Models’
Original Answer
In Section 6 we examine how FF 2’s VJPs rank
the target tokens, the tokens we try to learn during
backpropagation. One possible interpretation of
our analysis is to examine the backward pass ac-
cording to the gradual change in the embeddings
it tries to inject (add or subtract) into the model.
Previous works conduct similar analysis with the
hidden states from the forward pass Haviv et al.
(2023); Geva et al. (2022b). Those works exam-
ined the gradual build in LMs’ forward pass predic-
tion, from the perspective of the last token in the
edited prompts. In this section, we expand upon
these examinations by measuring the LL rank for
the original answers outputted by the forward pass
(before editing), and by observing these LL ranks
from the perspective of FF 1’s spanning set.
The presented results are based on 100 dis-
tinct edits using a single backward pass per edit.
We employ GPT2 and Llama2-7B, and the edited
prompts and targets were taken from the Counter-
Fact dataset.
FF 2: In Section 6 we presented the ranking of
the target token for GPT2-xl. Subsequently, we
present the rank of the last prompt’s token VJP
(annotated with δnin Lemma 5.1) also for GPT2-
medium and Llama-7B. In Figure 18 we can see
that all models’ LL projections rank the target token
as one of the most improbable tokens along most
of the layers, with some degradation in the first few
layers. We associate this drop to the gap of LL in
projection vectors from earlier layers.
Figure 18: The Logit Lens rank of the target token in
VJP of the last token from the edited prompt, δn. In
order to show different models in the same figure, we
normalize the layer indices and the rank of the token
in the vocabularies (which is 50K for GPT2 and 32K
for Llama2). All models’ δnassign the target token as
one of the most improbable tokens along most of their
layers.
We repeat the same experiment and measure therank of the original token predicted by the model.
According to Equation 4 and Lemma 5.1, the em-
bedding of a token in the gradient should be dis-
cernible if it is the target token or if its probability
in the model’s final output is relatively high.
The results depicted in Figure 19 illustrate that
the LL rank of the final answer is relatively low in
the model’s last layers, although not at the lowest
possible level.
In Section 4.2 and Section 5.2, we delved into
how the injection of the gradient vector with a high
LL rank of the target token implies that the update
aims to enhance the target probability in the output.
Similarly, the observed pattern regarding the LL
rank of the model’s final answer suggests that the
updates attempt to diminish the probabilities of the
final answers in the model’s output, but in a much
smoother manner than those of the target token.
Figure 19: The Logit Lens ranks for the models’ actual
answers according to FF2gradients of the last prompts’
token (layer indices and ranks are normalized). In the
later layers, the rank of the original answers suggests
that the model subtracts the embedding of those tokens
from the model’s weights.
FF 1: Since this layer’s δiis not projectable via
LL, we use its inputs xias its spanning set. An-
alyzing the ranks of xiis almost identical to the
analysis of Haviv et al. (2023). We share our anal-
ysis in Figure 20, mainly to emphasize that gradi-
ents write into FF 1’s weights the inputs xifrom
the forward pass. The gradual build in the models’
predictions becomes more apparent when we filter
out examples where the model answers Counter-
Fact’s prompts incorrectly, accounting for approxi-
mately 84% of the instances with GPT2-medium/xl
and Llama.12In Figure 20, we also included the
same analysis with 50 correctly answered prompts.
1Most of the time, they predict tokens like “a”, “is”, and
“the”, rather than factual notions in the context of the prompts.
2Llama utilizes two matrices that employ FF1as part of
its implementation of SwiGLU as its MLP activation function.
Notably, the input xiremains consistent for both matrices.(a) Only correctly answered prompts
(b) Correctly and incorrectly answered prompts
Figure 20: FF1’sxiranks for the model’s actual an-
swers. The main difference between the two figures is
that when the models answer incorrectly, they mostly an-
swer with function words, such as “a” and “the”, which
seems to have a constant rank from the earlier layers,
while answering the actual factual answers changes the
answers’ rank gradually in the first half of the layers.
The meaning of these graphs is that during fine-tuning,
the embeddings injected into the models’ weights are
those of xi.
Figure 21: FF1’sxiranks for the edits’ targets, reflect-
ing the intermediate predictions the models give to them
at each layer.
Throughout our study, this is the only result we
found to be affected by the distinction whether the
models answer the original prompt correctly or in-
correctly.
Similarly, when we plot the ranks of the target
token for each layer, we only observe the forward
passes’ intermediate probability for the target token
(Figure 21).F Normalized Logit Lens
We acknowledge the sensitivity of the Logit Lens to
low-norm vectors. With vectors with close to zero
norms, the tokens LL project tokens that resemble
the projection of the zero vector. In Section 6, we
discussed that certain VJPs, δi, exhibit low norms.
We hypothesize that the norms of the VJPs reflect
which parts of speech and layers the backward pass
tries to edit more. We were interested in observing
which tokens could be projected from gradients if
we isolate the influence of these low norms. Our
investigation reveals that normalizing the projected
vectors before applying the Logit Lens can be an
appropriate solution to the variance in VJPs’ norms.
We named this method “Normalized Logit Lens”.
A place we consider using this normalization is
in creating the tables of Appendix D. In Figure 22
and Figure 23 we share two examples that replicate
the setup from Appendix D except we are using
Normalized Logit Lens.
We provide this section to highlight the pattern
we already mentioned in Section 6: that most of
FF 2gradients ranks the target token as one the
most improbable token. We also want to suggest
further work to consider using Normalized Logit
Lens to examine low-norm vectors.Figure 22: GPT2-medium MLP gradients projections with Normalized Logit Lens . Compared to the naive Logit
Lens Figure 11, we see more cells in FF2that project concepts similar to the target token “Paris” and less unclear
tokens, such as “VIDIA” (notice the last layers or the column of the token“ion”). The coloring is according to the
original norm of each representative vector (before normalization).Figure 23: GPT2-xl FF2gradients via our spanning set interpretation and normalized Logit Lens . Every cell
shows the most <probable \improbable >tokens for its δi. Compared to Figure 13, we observe fewer non-English
tokens in the projections and more tokens associated with the target token “music”.G Editing Based on the “Shift”
Mechanism
In Section 7 we introduce “forward pass shifting”,
a method for LM knowledge editing through the ap-
proximation of the gradient matrix. In this section,
we provide additional results and implementation
details. We want to remind that our intention is not
to propose a new alternative editing method, but
rather to explore the concept of injecting knowl-
edge into the model in an approximate manner to
how backpropagation operates.
G.1 Comparison with Naive Backpropagation
We check our method’s ability to perform a single
edit at a time of a given prompt, such that after
editing, the most probable answer will be a se-
lected target token. Our baselines for comparison
are performing the same single Backpropagation
editing with SGD and Adam. We use 50 samples
from CounterFact (Meng et al., 2022) for the edited
prompts and target. For each editing method and
sample, we measure the method’s sensitivity to dif-
ferent learning rates by checking a range of possi-
ble values and finding the minimum one to achieve
a successful edit. We also monitor the model’s
general degradation after its update, using perplex-
ity on maximum 256-tokens-long samples from
WikiText (Merity et al., 2016). The findings are
presented in Figure 24. Evidently, our approach’s
minimum learning rate shows less variation com-
pared to SGD. Furthermore, its stability to editing,
as indicated by perplexity, surpasses that of Adam
and SGD in earlier layers and is identical in the
latter ones.
G.2 Benchmark Implementation Details
Based on the results from Section G.1, we iden-
tified layers 30−40as the best potential editing
layers. As for the learning rate, we examined values
ranging from 0.14to0.26. The presented results
use layer 35 with a learning rate of 0.24, which
yielded the best results in the following benchmark.
The construction of the approximated gradient
matrix is accomplished using the embedding of
the target token. This embedding is obtained from
the decoding matrix. If the target token consists
of more than one token according to the model’s
vocabulary, we select the prefix (the first token) to
construct the target token.
Regarding the other editing methods and Coun-
terFact benchmark implementation, we followed
(a) Minimum learning rate for achieving a successful edit
(b) Perplexity at minimum learning rate editing
Figure 24: Editing only a single FF2matrix of GPT2-xl
on CounterFact. By measuring a range of possible learn-
ing rate values we found the minimum that achieves a
successful edit and measured the change in the model’s
general knowledge through perplexity on WikiText.
the implementations provided by Meng et al. (2023)
to create the results ourselves. The post-editing
metrics included in this benchmark, which we
present, are as follows: (1) “Efficacy” - the ac-
curacy (percentage of times) with which the model
predicts the targeted token as the most probable one
given the editing prompt. (2) “Paraphrase” - the
accuracy of the model to answer paraphrases of the
edited prompt (also known as “Generalization”).
(3) “Neighborhood” - the accuracy of the model on
prompts from similar domains as the edited prompt,
which we do not wish to change (also known as
“Specificity”). (4) “N-gram entropy” measures the
weighted average of bi- and tri-gram entropies, re-
flecting fluency level. In addition, we included in
the benchmark the perplexity score of the 100 first
sentences from WikiText (version wikitext-2-raw-
v1, test split3) that are at least 30 characters long.
Moreover, each prompt was truncated to its first
256 tokens. The score of this metric reflects how
much the model’s original ability to generate text
has changed.
The comparison was conducted on 1000 samples
from CounterFact, and we benchmarked against
state-of-the-art methods, such as ROME (Meng
et al., 2022), MEMIT (Meng et al., 2023), and
MEND (Mitchell et al., 2021).
3https://huggingface.co/datasets/wikitextMETHOD EFFICACY ↑ PARAPHRASE ↑ NEIGHBORHOOD ↑
ORIGINAL MODEL 0.4±6.31 0.4 ±4.45 11.21 ±19.77
FINE-TUNING (MLP 0) 96.37 ±18.7 7.46 ±19.44 10.12 ±18.02
FINE-TUNING (MLP 35) 100.0 ±0.0 46.1 ±40.25 5.59 ±13.19
MEND 71.4 ±45.19 17.6 ±31.47 7.73 ±16.27
ROME 99.4 ±7.72 71.9 ±37.22 10.91 ±19.53
MEMIT 79.4 ±40.44 40.7 ±41.64 10.98 ±19.44
FORWARD PASS SHIFT 99.4±7.72 41.55 ±41.67 6.02 ±14.0
Table 6: Single editing results of GPT2-xl on CounterFact benchmark
METHOD N-GRAM ENTROPY ↑WIKITEXT PERPLEXITY ↓
ORIGINAL MODEL 626.94 ±11.56 93.56 ±0.0
FINE-TUNING (MLP 0) 618.81 ±52.74 199.84 ±1392.66
FINE-TUNING (MLP 35) 618.5 ±28.57 103.42 ±11.69
MEND 623.94 ±23.14 94.96 ±0.66
ROME 622.78 ±20.54 137.38 ±786.35
MEMIT 627.18 ±12.29 93.6 ±0.11
FORWARD PASS SHIFT 622.45 ±21.46 93.66 ±1.08
Table 7: Examining the general ability of GPT2-xl to generate text after it has been applied with a single edit from
CounterFact.
G.3 Results
In Table 6, we present the editing results regarding
the model’s ability to modify internal knowledge.
In Table 7, we analyze the effect of editing on the
model’s ability to generate generic text.
If we filter out editing methods that cause dras-
tic degradation to the model’s ability to generate
text (where n-gram entropy falls below 620), our
method achieves the best editing results on the
edited prompt (Accuracy), tying only with ROME.
With regards to editing the paraphrased prompts,
the “forward pass shifting” method achieves com-
parable results, but falls behind ROME. However,
one of our method’s limitations lies in its per-
formance with neighborhood prompts, where the
edited model altered prompts we do not anticipate
to be changed.
G.4 Discussion
“Forward pass shift” demonstrates successful
knowledge editing compared to methods that em-
ploy much more complex implementations. Ad-
ditionally, our method shows minimal impact on
the model’s ability to generate text, addressing one
of the main challenges of fine-tuning in preciseknowledge editing.
From our understanding, “forward pass shift” is
the simplest editing method in terms of algorithm
complexity, as it only requires a single forward
pass. The low score of this method in terms of
neighborhood editing may be attributed to mistak-
enly editing activation patterns of FF 2that are
also shared with similar prompts. Future studies
could explore the capability of “forward pass shift”
to handle multiple edits at once, or to incorporate
multiple iterations in its implementation (multiple
forward passes).