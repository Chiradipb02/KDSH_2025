Structure Guided Prompt: Instructing Large Language Model in
Multi-Step Reasoning by Exploring Graph Structure of the Text
Kewei Cheng
UCLA
Los Angeles, CA, USA
viviancheng@cs.ucla.eduNesreen K. Ahmed
Intel Labs
Santa Clara, CA, USA
nesreen.k.ahmed@intel.com
Theodore Willke
Intel Labs
Portland, OR, USA
ted.willke@intel.comYizhou Sun
UCLA
Los Angeles, CA, USA
yzsun@cs.ucla.edu
ABSTRACT
Although Large Language Models (LLMs) excel at addressing
straightforwardreasoningtasks,theyfrequentlystrugglewithdifficul-
tieswhenconfrontedbymorecomplexmulti-stepreasoningduetoa
rangeoffactors.Firstly,naturallanguageoftenencompassescomplex
relationships among entities, making it challenging to maintain a
clear reasoning chain over longer spans. Secondly, the abundance
of linguistic diversity means that the same entities and relation-
ships can be expressed usingdifferent terminologies and structures,
complicating the task of identifying and establishing connections
between multiple pieces of information. Graphs provide an effective
solution to represent data rich in relational information and capture
long-term dependencies among entities. To harness the potential of
graphs,ourpaperintroduces StructureGuidedPrompt ,aninnovative
three-stage task-agnostic promptingframeworkdesignedtoimprove
the multi-step reasoning capabilities of LLMs in a zero-shot setting .
Thisframeworkexplicitlyconvertsunstructuredtextintoagraphvia
LLMsandinstructsthemtonavigatethisgraphusingtask-specific
strategies to formulate responses. By effectively organizing infor-
mation and guiding navigation, it enables LLMs to provide more
accurate and context-aware responses. Our experiments show that
thisframeworksignificantlyenhancesthereasoningcapabilitiesof
LLMs, enabling them to excel in a broader spectrum of natural
language scenarios.
CCS CONCEPTS
•DoNotUseThisCode →GeneratetheCorrectTermsforYour
Paper;Generate the Correct Terms for Your Paper ; Generate the
Correct Terms for Your Paper; Generate the Correct Terms for Your
Paper.
KEYWORDS
Large Language Model, Reasoning, Multi-hop, Graph
Permission to make digital or hard copies of all or part of this work for personal or
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXXACM Reference Format:
Kewei Cheng, Nesreen K. Ahmed, Theodore Willke, and Yizhou Sun. 2018.
Structure Guided Prompt: Instructing Large LanguageModel in Multi-Step
Reasoning by Exploring Graph Structure of the Text. In Proceedings of
Make sure to enter the correct conference title from your rights confirmation
emai (Conference acronym ’XX). ACM, New York, NY, USA, 19 pages.
https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Natural Language Processing (NLP) has witnessed significant ad-
vancementsinrecentyearswiththeemergenceofLargeLanguage
Models(LLMs)suchasGPT-3[ 2]andChatGPT[ 15].Thesemodels
have achieved remarkable results in tasks such as language gener-
ation, translation, and summarization [ 34]. However, studies have
indicated that while LLMs can effectively handle straightforward
reasoningproblems,theyoftenencounterchallengeswhenfacedwith
more complex reasoning, such as scenarios demanding multi-step
reasoning [17].
Multi-step reasoning typically involves making inferences or
answering questions that require multiple steps of logical reasoning.
Here’s an illustration of multi-step reasoning: “Marian went shoe
shopping with her sister Michelle. Darnell’s grandfather, Stanley,
taughtherhowtomakeapaperairplanewhilehermother,Marian,
prepareddinner.WhatisthefamilyrelationshipbetweenMichelleand
Stanley?” Various methods, such as chain-of-thought (CoT) [ 21,28]
and Zero-Shot-CoT [ 11], have been proposed to improve multi-
step reasoning in LLMs. These approaches involve step-by-step
reasoning, either by providing examples with detailed intermediate
steps leading to a conclusion or by prompting the model with “Let’s
think step by step” in a zero-shot setting. Despite their effectiveness,
LLMsstillfacechallengesineffectivelyaddressingcomplexmulti-
step reasoning questions. The first challenge involves accurately
comprehending relationships conveyed through natural language,
as evident in the given example where Marian has a sister named
Michelle, and Darnell has a grandfather named Stanley. Identifying
theserelationshipsaccuratelyiscrucial,buttheinherentambiguity
in natural language makes this difficult. For instance, consider the
sentence“Darnell’sgrandfather,Stanley,taughtherhowtomakea
paper airplane while her mother, Marian, prepared dinner,” correctly
inferring that Marian, not Stanley, is Darnell’s mother requires
understanding the gender implications and the contextual relational
information.Second,LLMsmustidentifyrelevantinformationwhilearXiv:2402.13415v1  [cs.CL]  20 Feb 2024Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Cheng, et al.
Relation PredictionEntity PredictionOver Dynamic KGComplex Entity PredictionLogical Inference
?
3
245160
Graph SortingGraph Query
Sort
How many red nodes in the graph?
Figure 1: GPT-4’s performance using 0-shot chain-of-thought (0-CoT) (represented as the orange bars) compared to the results of
Structure Guided Prompt (represented as the blue bars) across a variety of tasks. It is evident that Structure Guided Prompt consistently
and significantly outperforms the approach with 0-CoT.
ignoring the irrelevant. In the example “Marian went shoe shopping
withhersisterMichelle,”recognizingthatMichelleisMarian’ssister
is crucial, while the detail about shoe shopping is not. This requires
discernmentinfilteringoutunnecessarydetailsthatcouldmislead.
Third, accurate multi-step reasoning requires LLMs to logically
connect information. In the given scenario, two steps of inference
are required. Initially, recognizing that Marian is Stanley’s daughter,
followedbycombiningthiswiththefactthatMichelleisMarian’s
sister,leadstothefinaldeductionthatMichelleisStanley’sdaughter.
Thisprocess,typicallymorestraightforwardinformallogicdueto
clear logical indicators, becomes more complex in natural language
duetothelackofexplicitlogicalconnectors,posingchallengesfor
LLMs in constructing accurate reasoning paths.
Considering all the previously mentioned challenges, performing
multi-step reasoning directly based on unstructured text is a chal-
lenging task. To reduce the complexity, can LLMs be guided to
adopt a more systematic and structured method for identifying
reasoning paths for multi-stepreasoning? Multi-step reasoning
is essential to human intelligence, inspiring how we guide LLMs.
Humansusuallyrelyonstructuredknowledgerepresentations,like
Knowledge Graphs (KGs), to link different pieces of information
clearlyandsystematically.ConsiderthequestioninvolvingMarian,
Michelle, Stanley, and Darnell. Answering it can be challenging due
tothemultipleindividualsinvolvedandtheneedtorememberand
correctly sequence their relationships. To address this, humans often
create a graph to visually represent the relationships, as depicted in
Fig.2.Theythendeducetherelationshipsstepbystep,basedonthis
graph. Although this method might seem simple, it is particularly
effective, especially with longer inference chains.
Due to the advantages brought by KGs, there have been efforts to
harness the strengths of both KGs and LLMs. These efforts typically
involveintegratingadditionalKGsasexternaltoolstoaugmentthe
reasoning capabilities of LLMs [ 16]. However, constructing and
maintaining these KGs can be expensive, and using an external
KG may overwhelm LLMs with too much irrelevant information
Marian went shoe shopping with her sister Michelle . 
Darnell's grandfather, Stanley, taught her how to make 
a paper airplane while her mother, Marian, prepared dinner. 
What is the family relationship between Michelle  and Stanley?
MarianMichelle
DarnellStanley
hasSister hasGrandfather
hasMotherStep 1: hasDaughterStep 2:  hasDaugtherFigure2:Anexampleillustratinghowhumansmanagemulti-step
questions. Our objective is to deduce the relationship between
twoindividuals,MichelleandStanley,highlightedinred,from
agivenstory.Giventhatthestoryinvolvesvariousindividuals,
humans typically first create a graph to clearly visualize the
relationships among them. Then, they infer the relationship step
by step, based on the graph.
when addressing specific queries. In contrast, our approach takes
a unique path. We firmly believe that natural language paragraphs
inherently contain sufficient information for effectively answering
questions.RatherthanrelyingonexternalKGs,ourapproachcenters
on refining the organization of information within these paragraphs
toenhanceinformationcomprehensionandreasoning.Consequently,
weintroduce StructureGuidedPrompt ,anovelpromptingframework
designedtoguideLLMsinmulti-stepreasoning.Itexplicitlyconverts
unstructured text into a graph and instructs LLMs to navigate this
graph to formulate responses in a zero-shot setting . Acknowledging
the diversity of queries and their corresponding graph structures, we
havecategorizedreasoningtasksintovariouscategoriesasshown
in Fig. 1. Each category is aligned with a unique graph structure.Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
These categories present distinctive challenges for LLMs. In the
evaluation, we compared the performance of both GPT-3.5 and
theadvancedGPT-4model[ 15]whenequippedwithourproposed
prompt.Remarkably,ourframeworkemergesasacatalyst,signifi-
cantly enhancing the reasoning capabilities of LLMs across broader
naturallanguagescenarios.Theresultsunequivocallydemonstrate
thatStructureGuidedPrompt empowersgeneral-purposeLLMsto
achieve competitive performance, underscoring its pivotal role in
exploringthegraphstructureoftextforinstructingLLMsinmulti-
stepreasoning.Insummary,ourcontributioncanbecategorizedinto
three main aspects:
•We propose Structure Guided Prompt , a novel prompting
framework designed to enhance the reasoning capability of
LLMs by exploring the graph structure underlying the text.
Within this framework, we delve into several distinct tasks,
each tailored to specific graph structures.
•We show with experiments that our framework boosts the rea-
soning capability of general-purpose LLMs across a broader
spectrum of natural language scenarios.
•We conduct thorough analytical investigations, summarizing
not only key open questions but also providing valuable
insightsforfutureresearchinthisfield.Wehopetheseinsights
will inspire further exploration in the realm of reasoning.
2 RELATED WORKS
2.1 Multi-step Reasoning with LLMs
Multi-stepreasoningisachallengingNLPtaskthatrequiresasystem
to make multiple inference steps to answer a question. While LLMs
exhibit strong capabilities in one-hop inference, they struggle to
perform effectively in multi-step reasoning. Numerous strategies
havebeensuggestedtoenhancethemulti-stepreasoningcapabilities
ofLLMs,suchasimplementingstep-by-stepreasoningusingfew-shot
examples. Unlike “naive” prompting, which expects that the input
should be immediately followed by the output or answer, eliciting
prompts direct LLMs to tackle tasks by guiding them through
intermediate steps before making predictions for the final output
oranswer.Thismethod,knownaschain-of-thought(CoT)[ 21,28],
has demonstrated that elicitive prompting equips LMs with superior
reasoning abilities in a few-shot setting. Later, Zero-Shot-CoT [ 11]
presented similar capabilities in a zero-shot setting. They simply
prependedtheinputquestionwiththephrase“Let’sthinkstepbystep”
before querying the model, and showed that large LMs performed
wellinzero-shot-CoTonreasoningtaskslikeGSM8K,thoughnotas
proficiently as in few-shot-CoT. Least to Most prompting (LtM) [ 35]
takes CoT prompting astepfurther by first breaking a probleminto
sub problems and then proceeds to solve each one independently.
These sub-questionanswers arethen synthesized toobtain thefinal
response. Additionally, Tree of Thoughts (ToT) [ 32] and Graph of
Thoughts(GoT) [ 1]usecomplexstructuresliketreesand graphsto
organizethoughts.ThesesystemscombinethewayLLMsgenerate
thoughtswithsearchalgorithmsforsystematicexploration,further
enhancingtheirmulti-stepreasoningcapabilities.Incontrasttoall
theseapproaches,ourworkintroducesadistinctthree-stepprompting
framework. This framework emulates the problem-solving approach
employed by humans when dealing with data rich in relationships. It
enablesuserstotransformanaturallanguageparagraphintoagraph,and subsequently, based on the query type, navigate this graph for
the purpose of answering questions.
2.2 Integrate LLMs with Logical Inference
The most studied approach to reasoning since the earliest days of
AIislogicalinference[ 3].Logicalsystemsarefundamentallyrule-
based [18,24], enabling the precise tracing of the specific path
or rule that leads to a particular conclusion. This characteristic
facilitates the establishment of proofs and verification processes,
thereby ensuring that derived statements are sound based on the
givenaxioms[ 24].Incontrast,LLMs,asneural-basedmodels,often
actas“blackboxes,”introducingalevelofunconstrainedbehavior
thatposeschallengesinfollowingstrictlogicalreasoning[ 14].To
enhancesystematicreasoning,variousstrategieswereproposedto
integrateLLMswithclassicallogicalinferencealgorithmssuchas
forward chaining [ 5] and backward chaining [ 10]. Yet, applying
these techniques in opendomains presents significant challenges as
they frequently necessitate supplementary context or logical rules to
provideconstraints.Creatingsuchlogicalrulescanbedemanding,
especially with limited resources. Our proposed approach provides a
systematic solution to address gaps in cases where explicit rules are
absent.Itachievesthisbyexploringtheunderlyinggraphstructureof
unstructuredtext, potentiallyimproving thecapability ofLanguage
Models (LLMs) to effectively traverse reasoning paths.
2.3 Multi-step Reasoning over KGs
KGsprovideaneffectivewaytoexplicitlyorganizeinformationin
the form of a structured graph. Multi-step reasoning naturally aligns
withgraph-basedtechniques,utilizingexplicitpathwaysinthegraph
torepresentthereasoningprocess[ 4,33].Forexample,multi-step
reasoning has been formulated in a reinforcement learning setup,
where a policy-based agent sequentially extends its inference path
until it reaches a target [ 7,13,22,30]. Moreover, to address the
challenge of the more complex logical query answering in KGs, the
query embedding method is proposed to conduct complex logical
reasoning in the embedding space [ 9,19,20]. This method involves
transformingaFirst-OrderLogic(FOL)queryintoavectorwithin
the embedding space and subsequently searching for entities in
the KG that share similar embeddings. Despite significant efforts
to use KGs for direct reasoning, these graphs are often domain-
specific and suffer from data sparsity. This means they might not
have enough information for accurate multi-step reasoning across
various topics. LLMs, on the other hand, can access a vast range of
unstructured text, offering broader knowledge and topic coverage.
TocombinethestrengthsofbothKGsandLLMs,therehavebeen
attempts to use KGs as external tools to incorporate additional
facts into the reasoning process [ 16]. For instance, MindMap [ 29]
uses KGs to provide LLMs with up-to-date information and help
themfindreasoningpaths.However,constructingandmaintaining
these KGs can be expensive and might even overwhelm LLMs with
too much irrelevant information when addressing specific queries.
In contrast, our approach takes a distinctive route. We hold the
belief that natural language paragraphs inherently contain sufficient
informationforansweringquestionseffectively.Insteadofdepending
onexternalKGs,ourpropositioninvolvesrefiningtheorganizationofConference acronym ’XX, June 03–05, 2018, Woodstock, NY Cheng, et al.
informationwithintheseparagraphstoenhanceinformationretrieval
and reasoning.
3 FRAMEWORK: STRUCTURE GUIDED
PROMPT
We propose Structure Guided Prompt ,a zero-shot prompting frame-
work to guide LLMs in multi-step reasoning by explicitly converting
unstructuredtext intoa graphand instructsLLMstonavigate this
graph using task-specific strategies to formulate responses. The
proposedframeworkisgeneral,inherentlytask-agnostic,andcapable
of eliciting multi-step reasoning across broader natural language
scenarios with a unified template.
3.1 Three-stage prompting
Our three-stage prompting method, inspired by human problem-
solving with graphs,involves:(1) Generatinga graph fromthe given
context;(2)Planninghowtonavigatethegraphconsideringthetasks;
(3) Executing the plan by traversing the graph to find the answer.
Thisapproachmirrorshowhumanstacklegraph-basedproblems.To
facilitate recognition, each stage of the prompt is color-coded: olive
for the first stage, teal for the second, and violet for the third.
Example Let’s illustrate the three-stage prompting using an ex-
ample. Consider the following paragraph: “Christian got his son,
Seth,acarforhisbirthday.ChristianandhisbrotherJonathanwent
to a basketball game. Jonathan’s sister Ruth decided to tag along
with them. Ruth invited her daughter Stephanie to lunch. Stephanie’s
brother Jeremycouldn’t leave work to join them.” The question is to
determine the family relationship between SethandJeremy.
1st stage prompt: Concept Map Construction In the first step,
our goal is to convert an unstructured paragraph into a structured
graph.Withinthisgraph,eachnodecorrespondstoanentity,andthe
interconnectingedges depicttherelationships linkingtheseentities.
Consider the given example, its graph representation is given in the
Fig. 3.
Christian Jonathan RuthStephanie Seth Jeremy
hasSon
HasBrother hasSisterHasDaughterHasBrother ?
Figure 3: The graph representation of a story from CLUTRR
dataset. our objective is to determine the family relationship
between two nodes, SethandJeremy, which are highlighted in
red.
2ndstage prompt: Task-specificPlanning Fig. 3demonstrates
that, while the text and question seem straightforward, the graph
revealsacomplexpathbetween SethandJeremy.Correctlynavigating
thispathrequiresspecificplanningstrategiesthatdirectthereasoning
process. It is important to note that these planning strategies are
generally independent of the underlying data. The choice of strategyis task-specific. Consider the given example, to identify missing
relationshipsbetweentwoentities(i.e.,( Seth,?,Jeremy)),ourmethod
involves deducing this link by tracing a path between them. We start
with the subject entity and iteratively explore the most relevant
information, progressing step-by-step until we reach the object
entity. From there, we deduce the missing relation by analyzing
thepathbetweenthetwoentities.Toenhancetheversatilityofour
framework, the following section will discuss different planning
strategies applicable for a range of tasks.
3rd stage prompt: Execution with the Concept Map Upon
defining the task-specific planning strategy, we proceed to the execu-
tion phase, leveraging the concept map developed in 1st stage. This
phaseofinstantiationenablesustoaddressthespecificproblemin
the given context. As illustrated in Fig 3, to derive the answer, we
traverse the graph following the plan and carry out inference step
by step: Step 1: Given SethhasSon←−−−−−−ChristianhasBrother−−−−−−−−→ Jonathan,
we have SethhasUncle−−−−−−−→ Jonathan; Step 2: Given SethhasUncle−−−−−−−→
JonathanhasSister−−−−−−−→ Ruth,wehave SethhasAunt−−−−−−→Ruth;Step3:Given
SethhasAunt−−−−−−→RuthhasDaughter−−−−−−−−−−→ Stephanie , we have SethhasCousin−−−−−−−−→
Stephanie ; Step 4: Given SethhasCousin−−−−−−−−→ StephaniehasBrother−−−−−−−−→
Jeremy, we have SethhasCousin−−−−−−−−→ Jeremy.
ACompletePrompt. Bycombiningallthreestages,wepresent
thecompletepromptforthegivenexample:First,createaknowledge
graph by extracting facts from each sentence in the given input
story. Once this is done, I will pose a question. This question can
be transformed into a triple (s, ?, o), where your primary task is
to determine the missing relation (‘?’) that links the subject entity
(‘s’)totheobjectentity(‘o’).Tobegin,focusonthesubjectentity
in this triple and choose the most relevant facts to expand from it.
Stepbystep,progresstowardstheobjectentity,ensuringthateach
selectedfactcontributestocreatingalinkbetweenthesubjectand
object entities. Finally, utilize the established connection between
the subject and object entities to answer the question.
4 EXPLORING REPRESENTATIVE KG
REASONING TASKS
Ourframeworkisinherentlytask-agnostic,designedtoaccommodate
awiderangeoftaskswithversatility.Tocatertothisdiversity,we
establish task-specific planning in 2nd stage prompt, tailored to each
uniquetask.Thissectionoutlinesvariousplanningapproachesfor
different tasks, demonstrating the framework’s adaptability. We have
provided all these prompts in Appendix A.2.
4.1 Relation Prediction
Relation prediction is a task focused on predicting the missing
relationsbetweentwogivenentities,representedas (ℎ,?,𝑡).Thistask
typically involves inferring the missing relations by tracing the path
that links the target entities (i.e., ℎand𝑡) within the graph. We have
discussed the planning strategies applicable to relation prediction
task in Sec. 3.
4.2 Entity Prediction
Entity prediction is a fundamental task in KGs that aims to infer
the missing entity in a given query, such as (ℎ,𝑟, ?)or(?,𝑟,𝑡). ForStructure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
example,thequestion“WhocurrentlyholdsthepositionofPresident
in the USA?” can be structured as a link prediction task within a
KG,seekingtoresolvethequery (?,isPresidentOf ,USA).Thisquery
couldbestraightforward,inthispaper,wefocusonmorecomplex
queries which require multi-step inference across various natural
language scenarios.
4.2.1 Entity Prediction over Dynamic KG. Given that the informa-
tion in Knowledge Graphs (KGs) can change dynamically, each time
step introduces new information for inference. Therefore, predicting
entities within dynamic KGs necessitates a step-by-step understand-
ing of the status at each time interval to effectively manage entity
prediction. For instance, consider the scenario: “Alice, Bob, and
Claire are holding a white elephant gift exchange. At the start of the
event, they are each holding a present of a different color: Alice has
a yellow present, Bob has a brown present, and Claire has a blue
present. As the event progresses, pairs of people swap gifts. First,
Bob and Alice swap their gifts. Then, Claire and Alice swap their
gifts. Finally, Bob and Alice swap their gifts. At the end of the event,
what color gift does Bob have?” This situation exemplifies entity
prediction over dynamic KG, where the query can be structured
as(Bob,hasGift,?). To accurately reflect the status at the event’s
conclusion, it is essential to capture changes at every time step,
considering that each change depends on the previous time step.
Theprimaryplanningstrategyinvolvessystematicallytrackingand
recording the sequence of changes, with the KG at time step 𝑡being
modified based on the KG at the previous time step 𝑡−1.
4.2.2 Complex Entity Prediction. Whilethepreviousmethodtar-
gets simpler one-hop queries in the form of (ℎ,𝑟, ?), complex entity
prediction aims to predict answers for queries with a more com-
plexstructure. Forexample, “RiomTrial washeadedby theFrench
generalwhoreachedwhatdistinction?”isacomplexquery.Thisques-
tion’scomplexitygoesbeyondastraightforwardrelation,resembling
a formal logic expression: 𝑉?:=(Riom Trial,wasHeadedBy ,𝑉)∧
(𝑉,Reached,𝑉?).ThebridgingquestionsinHotpotQAprovidetyp-
ical examples of complex entity prediction, as illustrated in Fig. 4.
Answeringtheserequiresaggregatingandlinkingdatafromdisparate
sectionsofatext,followingaspecificsequencetoconstructthefinal
answer. The primary planning strategy involves decomposing the
question into simpler sub-questions and tackling these sub-questions
sequentially, referencing the knowledge graph for information.
4.3 Graph Sorting
Graph sorting task involves organizing entities within a graph ac-
cording to a specified sequence. For instance, consider the scenario:
“Thefollowingparagraphseachdescribethreeobjectsarrangedin
a fixed order. The statements are logically consistent within each
paragraph. On a branch, there are three birds: a blue jay, a quail,
andafalcon.Thefalconistotherightofthebluejay.Thebluejayis
to the right of the quail. Which bird is the second one counting from
the left?” To solve this, the main approach is to arrange the birds in
the correct order based on the given information and then determine
the answer.
Q: How old is the female main protagonist  of Catching Fire ?
S1: Catching Fire  is a 2009 science fiction young adult novel
by the American novelist Suzanne Collins, the second book
in "The Hunger Games trilogy" .
S2: The Hunger Games  is a 2008 dystopian novel by the
American writer Suzanne Collins. It is written in the voice of
16-year-old  Katniss Everdeen , who lives in the future, post-
apocalyptic nation of Panem in North America.What is Catching Fire ?
Who is the female main protagonist  ?
A: 16-year-oldHow old is Katniss Everdeen?Figure 4: The bridging question in HotpotQA. It relies on multi-
hop sequential reasoning to answer the question.
4.4 Graph Query
Graph query task involves specifying a condition to retrieve specific
data from a graph. For instance, consider the scenario: “Here is
a table where the first line is a header and each subsequent line
is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11
Bernard,5,80,13Vincent,9,60,11Gwen,8,70,15Forexample:
the age of Louis is 7, the weight of Gwen is 15 kg, the height of
Bernard is 80 cm. How many penguins are more than 5 years old?”
Thistype ofquerycan beexpressed inSPARQL,aquery language
for databases, as follows:
SELECT (COUNT(?penguin) AS ?count)
WHERE {
?penguin ex:age ?age .
FILTER (?age > 5)
}
Theprimaryplanningstrategyinvolvesidentifyingtheconditionand
selecting the entities that meet the condition from the graph.
4.5 Logical Inference
Logical inference andentailment are fundamental concepts in logic
and reasoning, used in various fields to determine the logical rela-
tionships between statements. For instance, consider the scenario:
“Sentence1:asthemassofacelestialobjectdecreases,thesurface
gravity of that celestial object weakens. Sentence 2: less is the op-
posite of more. Sentence 3: as the force of gravity decreases, the
weight of the object will decrease. Sentence 4: an astronaut is a
kind of object. Sentence 5: The Earth has more mass than the Moon.
Sentence 6: surface gravity is a kind of force of gravity. Why do
astronauts weigh more on Earth than they do on the Moon?” To
answer this, we construct a logical sequence: Astronauts experience
greater weight on Earth than on the Moon due to Earth’s stronger
gravitationalforce.ThisisinferredfromthefactthatEarth,having
more mass than the Moon, exerts a stronger surface gravity. The
primaryplanningstrategyinvolvesbeginningwiththesubjectentitiesConference acronym ’XX, June 03–05, 2018, Woodstock, NY Cheng, et al.
referenced in the question andestablishing a logical chain based on
the provided context.
5 RESULTS
Foreachtask,weevaluatetheperformanceoftwoLLMmodels,GPT-
4 (gpt-4) and GPT-3.5 ( gpt-3.5-turbo ) [15]. Since both methods are
closed-source,wedonothavespecificinformationabouttheirsize,
architecture, and pretraining particulars. For every task, we conduct
a comparative analysis of our prompting framework against both
with and without zero-shot chain-of-thought prompt (0-CoT), where
0-CoT encourages the model to engage in step-by-step reasoning by
incorporatingthephrase“Let’sthinkstepbystep”intheprompts.We
include the prompts for all six tasks in Appendix A.3 for reference.
5.1 Dataset
We have incorporated four datasets: CLUTRR [23],BIG-bench-hard
(BBH)[26],HotpotQA [31] andEntailment Bank [6] in experiments.
These datasets cover all six tasks discussed in Sec. 4. Please refer to
the Appendix A.1 for detailed information.
5.2 Analysis
Relation Prediction The most representative dataset for relation
prediction task is CLUTRR [23]. It is a benchmark designed to
infer the missing relationship between two individuals within a
family network. To assess the complexity of the questions within
theCLUTRR , we have organized them based on the length of the
relational paths connecting the target family members, typically
spanning 3 to 10 hops. As shown in Fig. 5, this task poses a
formidablechallengeforLLM,evenwhenthepathlengthisrelatively
short. Even with the utilization of GPT-4 with 0-CoT, satisfactory
performance remains elusive. This observation underscores the
inherent limitations of LLM in handling datasets with significant
relational complexity. Furthermore, as the length of the relational
pathsextends,thechallengeintensifies.Incontrast,withourproposed
StructureGuidedPrompt ,wecanobservethatitdrasticallyincreases
theperformanceandsuffersfromlessperformancedegradationwhen
the path length increases.
Entity Prediction over Dynamic KG : We have included the
tracking shuffled objects datasets from BBH [ 26] to assess the entity
prediction performance within dynamic KG. These datasets are
designed to infer the relative positions of various shuffled objects at
the conclusion of a narrative. The questions are organized according
to the number of objects involved. As shown in Fig. 5, this task
presents a significant challenge for LLM due to the requirement
of maintaining an evolving graph representation at each time step
whentrackingshuffledobjects.Givenourproposed StructureGuided
Prompt, which explicitly constructs and tracks changes within the
KG,wecanobservearemarkableperformanceenhancement(e.g.,
improve by 146% over GPT-4 w/ 0-CoT).
Complex Entity Prediction The bridging questions in Hot-
potQA[31]providetypicalexamplesforcomplexentityprediction
task. As shown in Fig. 5, while our proposed Structure Guided
Promptenhances performance, theimprovement is notas significant
as in other tasks. This is because the paragraphs in HotpotQA are
exceptionally long, making it challenging for LLM to construct a
KG that encompasses every piece of information within the context.Consequently,ourproposed StructureGuidedPrompt facesdifficulty
in further enhancing performance, especially with missing triples in
the KG.
Graph Sorting In the graph sorting task, we have included the
logical deduction datasets from BBH [ 26]. These datasets require to
sortobjectsarrangedinaline.Thequestionsareorganizedaccording
tothenumberofobjectsinvolved.AsshowninFig.5,eventhough
LLM already delivers impressive performance on this task, our pro-
posedStructure Guided Prompt brings about further improvements,
particularly as the number of involved objects increases.
Graph Query Within the graph query task, we have included
reasoning about colored objects andpenguins in a table datasets
fromBBH[ 26].Thesedatasetsinvolvetheselectionandcounting
of objects that meet specific criteria from a given set of objects.
AsshowninFig.5,LLMsalreadydeliverimpressiveperformance
on this task, but our proposed Structure Guided Prompt enhances
performance even further.
Logical Inference :Entailment Bank [6] is a widely used dataset
formulti-stepentailmenttasksinvolvinglogicalreasoning.Toassess
the complexity of the questions within the Entailment Bank , we
have categorized them based on the number of entailment steps
required to arrive at an answer. As shown in Fig. 5, our proposed
Structure Guided Prompt doesn’t consistently improve performance
for this task. The challenge lies in the fact that logical reasoning
often demandsa precise orderwhen constructingthe logical graph,
with rules typically dictating a direction from the premise to the
conclusion. Whilefollowinga forwardchaining algorithm,one can
readilyemploytherulesforlogicalinferencesequentially,starting
from known facts. However, in our scenario, we mix rules with
factsanddon’tclearlydistinguishbetweenpremisesandconclusions
within the rules. Consequently, even though we require LLMs to
construct the logical graph, it remains challenging for LLMs to
identify the correct logical order.
5.3 Case Study
In this subsection, we aim to highlight the advanced capabilities
and improvements our model, Structure Guided Prompt , offers over
existing methodologies. We select the task of entity prediction over
dynamic KG as a case for demonstration. Additional case studies
covering other tasks can be found in the Appendix A.2.
5.3.1 Case Study of Success. We present a particular example
where our Structure Guided Prompt delivers an accurate prediction,
underliningitsprecisionandreliability,especiallywhencompared
to the erroneous prediction made by 0-CoT in the same context.
Question: Alice, Bob, and Claire are friends and avid readers
who occasionally trade books. At the start of the semester, they each
buyonenewbook:AlicegetsHoundoftheBaskervilles,Bobgets
ThePearl,andClairegetsTheOdyssey.Asthesemesterproceeds,
they start trading around the new books. First, Bob and Claire swap
books.Then,ClaireandAliceswapbooks.Finally,BobandAlice
swap books. At the end of the semester, Alice has Options:
(A) Hound of the Baskervilles
(B) The Pearl
(C) The Odyssey
0-CoT Answer: (B) The Pearl ✗
Structure Guided Prompt Answer: (C) The OdysseyStructure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
3 4 5 6 7 8 9 10
Hops0.00.10.20.30.40.5ACCGPT-3.5
Ours
w/ 0-CoT
w/o 0-CoT
3 4 5 6 7 8 9 10
Hops0.00.10.20.30.40.50.60.7ACCGPT-4
Ours
w/ 0-CoT
w/o 0-CoT
Relation Prediction
3 5 7
Objects0.00.10.20.30.40.50.60.70.8ACCGPT-3.5
Ours
w/ 0-CoT
w/o 0-CoT
3 5 7
Objects0.00.20.40.60.8ACCGPT-4
Ours
w/ 0-CoT
w/o 0-CoT Entity Prediction over Dynamic KG
0.00.10.20.30.4ACCGPT-3.5
Ours
w/ 0-CoT
w/o 0-CoT
0.00.10.20.30.40.5ACCGPT-4
Ours
w/ 0-CoT
w/o 0-CoT
Complex Entity Prediction
3 5 7
Objects0.00.20.40.60.8ACCGPT-3.5
Ours
w/ 0-CoT
w/o 0-CoT
3 5 7
Objects0.00.20.40.60.81.0ACCGPT-4
Ours
w/ 0-CoT
w/o 0-CoT Graph Sorting
reasoning about
colored objectspenguins in
a table0.00.20.40.60.8ACCGPT-3.5
Ours
w/ 0-CoT
w/o 0-CoT
reasoning about
colored objectspenguins in
a table0.00.20.40.60.8ACCGPT-4
Ours
w/ 0-CoT
w/o 0-CoT
Graph Query
1 2 3 4 5 6
Hops0.00.10.20.30.40.50.60.7ACCGPT-3.5
Ours
w/ 0-CoT
w/o 0-CoT
1 2 3 4 5 6
Hops0.00.10.20.30.40.50.60.7ACCGPT-4
Ours
w/ 0-CoT
w/o 0-CoT Logical Inference
Figure5:Mainresults.Differentmethodsareillustratedthroughcolor-codedbars:bluebarsindicatetheresultsachievedusingour
Structure Guided Prompt ,whileorangebarsshowtheperformancewith0-shotchain-of-thought(0-CoT).Additionally,greenbarsdepict
the performance without 0-CoT. These results demonstrate that the Structure Guided Prompt consistently and significantly outperforms
the other methods, both with and without 0-CoT, across GPT-3.5 and GPT-4 models.
Explanation: Let’s create the knowledge graph by extracting facts
from each sentence in the story.
1.Alice,Bob,andClairearefriendsandavidreaderswhoocca-
sionally trade books.
- (Alice, is_friend_with, Bob)
- (Alice, is_friend_with, Claire)
- (Bob, is_friend_with, Claire)
- (Alice, is_avid_reader, True)
- (Bob, is_avid_reader, True)
- (Claire, is_avid_reader, True)
- (Alice, trades_books, True)
- (Bob, trades_books, True)
- (Claire, trades_books, True)
2. At thestart ofthe semester,theyeach buyone new book:Alice
gets Hound of the Baskervilles, Bobgets The Pearl, and Claire gets
The Odyssey.
- (Alice, has_book, Hound of the Baskervilles)
- (Bob, has_book, The Pearl)
- (Claire, has_book, The Odyssey)3. As the semester proceeds, they start trading around the new
books. First, Bob and Claire swap books.
- (Bob, has_book, The Odyssey)
- (Claire, has_book, The Pearl)
4. Then, Claire and Alice swap books.
- (Claire, has_book, Hound of the Baskervilles)
- (Alice, has_book, The Pearl)
5. Finally, Bob and Alice swap books.
- (Bob, has_book, The Pearl)
- (Alice, has_book, The Odyssey)
So, at the end of the semester, Alice has The Odyssey. Therefore,
option (C) is correct. ✔
Analysis: By explicitly converting unstructured text into a graph
and directing LLMs to monitor the evolution of this dynamic graph,
we can observe that LLMs accurately discern the altered object even
after numerous iterations.
5.3.2 Case Study of Failure. To comprehensively understand the
limitations of our proposed methods, we provide a specific exampleConference acronym ’XX, June 03–05, 2018, Woodstock, NY Cheng, et al.
demonstrating a scenario where our Structure Guided Prompt yields
anincorrectprediction,incontrasttotheaccuratepredictionmade
by 0-CoT under the same circumstances.
Question: Alice,Bob,andClairearedancersatasquaredance.
Atthestartofasong,theyeachhaveapartner:Aliceisdancingwith
Rodrigo,BobisdancingwithJamie,andClaireisdancingwithLola.
Throughoutthesong,thedancersoftentradepartners.First,Claire
and Alice switch partners. Then, Bob and Claire switch partners.
Finally, Claire and Alice switch partners. At the end of the dance,
Alice is dancing with Options:
(A) Rodrigo
(B) Jamie
(C) Lola
0-CoT Answer: (B) Jamie ✔
Structure Guided Prompt Answer: (C) Lola
Explanation:
Let’s create the knowledge graph:
1. Alice, Bob, and Claire are dancers at a square dance.
- (Alice, is a dancer at, square dance)
- (Bob, is a dancer at, square dance)
- (Claire, is a dancer at, square dance)
2.Atthestartofasong,theyeachhaveapartner:Aliceisdancing
withRodrigo,BobisdancingwithJamie,andClaireisdancingwith
Lola.
- (Alice, is dancing with, Rodrigo)
- (Bob, is dancing with, Jamie)
- (Claire, is dancing with, Lola)
3. First, Claire and Alice switch partners.
- (Alice, is dancing with, Lola)
- (Claire, is dancing with, Rodrigo)
4. Then, Bob and Claire switch partners.
- (Bob, is dancing with, Rodrigo)
- (Claire, is dancing with, Jamie)
5. Finally, Claire and Alice switch partners.
-(Alice,isdancingwith,Jamie) ✔
- (Claire, is dancing with, Lola)
So,attheendofthedance,AliceisdancingwithLola.Therefore,
option (C) is correct. ✗
Analysis: As we highlight, while the inference process of our
proposedmodelisaccurate,itmakesmistakesduringtheconclusion-
drawing phase. This pattern has been widely observed in our experi-
ments.
5.3.3 Conclusion. AnotablefindingisthatwhiletheLLMssuc-
cessfully adheres to the prompts to construct accurate KGs and
navigates these KGs correctly according to task-specific strategic
guidance, it often makes mistakes during the conclusion-drawing
phase, even with correct inference results immediately preceding
thisstage.Thisissuecouldpotentiallybeaddressedbyemployingan
additionalLLMtoverifytheconsistencyofthegeneratedcontent.
Weplan toexplorethis approachtofurther improveourframework
in the future.
6 DISCUSSION
Do LLMs spontaneously represent natural language text as a
KGformulti-stepreasoning? LLMslikeGPT-3aremainlytrainedforpredictingthenexttokenbasedoncontextratherthanstructuring
unstructuredtextintoKGsformulti-stepreasoning.AlthoughLLMs
arenot naturallystructuredasKGs,theycanbe promptedforstruc-
turedthinking.The“Zero-Shot-CoT”[ 11]approach,whichprepends
theinputquestionwiththephrase“Let’sthinkstepbystep”before
queryingthemodel,hasshownpromiseinencouragingstructured
thinkingandimprovingreasoningperformanceinLLMs.LLMsalso
excelinplanning[ 27],especiallywhenbreakingdowncomplexques-
tionsintosimplersub-questionsforsequentialanswers[ 8,12,35].
However, while LLMs excel in generating sequential steps based on
prompts, their effectiveness is limited when faced with real-world
scenarios with a significant relational complexity as discussed in
Sec. 5.2.
Is a KG expressive enough to represent natural language
text?KGs excel in structuring factual information and relationships,
making them useful for organizing knowledge. However, the ex-
pressivenessof KGscan belimitedwhen itcomes tohandling the
richnessofnaturallanguage.Onemajordrawbackistheirinability
to effectively convey emotions and sentiments. KGs are primarily
designedforstoringconcreteinformation,makingthemlesssuitable
for encoding human emotions. In contrast, natural language text
allowsforabroadrangeofemotionalexpressions,fromjoytosorrow,
humortosarcasm.Forexample,thesentence“IfIwereabird,Iwould
fly to far-off lands” carries emotional weight and context-dependent
meaningsthatKGsmaystruggletocapture.Dependingonthecon-
text,itcouldexpressalongingforadventure,adesireforfreedom,
or metaphorically represent personal aspirations. These nuances are
deeply embedded in natural language and not easily translatable into
the rigid structure of a KG. While this paper demonstrates the value
of KGs in promoting structured thinking in LLMs, it’s important
to recognize that KGs may have limitations in fully capturing the
expressivepowerofnaturallanguagetext.Furtherresearchisneeded
to bridge this gap.
Dowehavemoreeffectivemethodsforrepresentingtherela-
tionshipsamongvariouspiecesofinformation? Thequestion of
moreeffectivemethodsforrepresentingrelationshipsamonginfor-
mation, especially in light of KGs’ limitations in handling natural
language, is vital. Expanding KGs to incorporate unary attributes
alongside binary predicates to describe events’ properties is bene-
ficial. Moreover, it is crucial, as discussed in Sec. 5.2, to enhance
mechanismsfordetectingtextualentailment,contradiction,andin-
ference.TheseenhancementscanenableLLMstomoreeffectively
participate in nuanced reasoning, including considerations of causal-
ityandtemporalrelationships.Givennaturallanguage’sflexibility,
seamlessNLPpipelinesthatcombinevariousmodels,suchasnamed
entity recognition, dependency parsing, and entity resolution, are
essential.Theseunifiedpipelinesmaycaptureandclarifycomplex
relationships within textual data, enhancing structured reasoning
capabilities.
7 CONCLUSION
LLMs often excel in simple reasoning tasks but struggle with multi-
step reasoning. Graphs offer an effective way to model relational
data and capture long-term dependencies among entities. This paper
bridgesthisgapbyintroducinganinnovativetask-agnosticprompting
framework, Structure Guided Prompt . This framework enhancesStructure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
the multi-step reasoning capabilities of LLMs within a zero-shot
settingbysystematicallyconvertingunstructuredtextintoagraphical
formatandguidingLLMsintraversingthisgraphusingtask-specific
strategies to construct responses. Our experiments show that our
proposedframeworksignificantlyenhancesthereasoningcapabilities
ofLLMs,empoweringthemtoexcelinabroaderspectrumofnatural
language scenarios.Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Cheng, et al.
REFERENCES
[1]Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi,
Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr
Nyczyk, et al .2023. Graph of thoughts: Solving elaborate problems with large
language models. arXiv preprint arXiv:2308.09687 (2023).
[2]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,etal .2020. Languagemodelsarefew-shotlearners. Advancesinneural
information processing systems 33 (2020), 1877–1901.
[3]Rudolf Carnap.2012. Introductionto symboliclogic and itsapplications . Courier
Corporation.
[4]Xiaojun Chen, Shengbin Jia, and Yang Xiang. 2020. A review: Knowledge
reasoningoverknowledgegraph. ExpertSystemswithApplications 141(2020),
112948.
[5]AntoniaCreswell,MurrayShanahan,andIrinaHiggins.2022. Selection-inference:
Exploitinglargelanguagemodelsforinterpretablelogicalreasoning. arXivpreprint
arXiv:2205.09712 (2022).
[6]Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith,
Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with
entailment trees. arXiv preprint arXiv:2104.08661 (2021).
[7]Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar,
Akshay Krishnamurthy, Alex Smola, and Andrew McCallum. 2017. Go for a
Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using
ReinforcementLearning.In InternationalConferenceonLearningRepresentations
(ICLR).
[8]AndrewDrozdov,NathanaelSchärli,EkinAkyürek,NathanScales,XinyingSong,
Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2022. Compositional semantic
parsing with large language models. arXiv preprint arXiv:2209.15003 (2022).
[9]WilliamL.Hamilton,PayalBajaj,MarinkaZitnik,DanJurafsky,andJureLeskovec.
2018. Embedding Logical Queries on Knowledge Graphs. In Advances in Neural
Information Processing Systems (NeurIPS) . 2030–2041.
[10]Seyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak
Ramachandran.2022. Lambada:Backwardchainingforautomatedreasoningin
natural language. arXiv preprint arXiv:2212.13894 (2022).
[11]TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusuke
Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in
neural information processing systems 35 (2022), 22199–22213.
[12]Soochan Lee and Gunhee Kim. 2023. Recursion of Thought: A Divide-and-
Conquer Approach to Multi-Context Reasoning with Language Models. arXiv
preprint arXiv:2306.06891 (2023).
[13]XiVictoriaLin,RichardSocher,andCaimingXiong.2018. Multi-hopknowledge
graph reasoning with reward shaping. arXiv preprint arXiv:1808.10568 (2018).
[14]Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu
Nguyen,OscarSainz,EnekoAgirre,IlanaHeintz,andDanRoth.2023. Recent
advances in natural language processing via large pre-trained language models: A
survey.Comput. Surveys 56, 2 (2023), 1–40.
[15] OpenAI. 2023. GPT-4 Technical Report. ArXivabs/2303.08774 (2023).
[16]ShiruiPan,LinhaoLuo,YufeiWang,ChenChen,JiapuWang,andXindongWu.
2024. Unifying large language models andknowledge graphs: A roadmap. IEEE
Transactions on Knowledge and Data Engineering (2024).
[17]BhargaviParanjape,ScottLundberg,SameerSingh,HannanehHajishirzi,Luke
Zettlemoyer,andMarcoTulioRibeiro.2023. ART:Automaticmulti-stepreasoning
and tool-use for large language models. arXiv:2303.09014 [cs.CL]
[18]J. Ross Quinlan. 1990. Learning logical definitions from relations. Machine
learning5, 3 (1990), 239–266.
[19]Hongyu Ren, Weihua Hu, and Jure Leskovec. 2020. Query2box: Reasoning over
Knowledge Graphs in Vector Space Using Box Embeddings. In International
Conference on Learning Representations (ICLR) . OpenReview.net.
[20]Hongyu Ren and JureLeskovec. 2020. BetaEmbeddings for Multi-Hop Logical
Reasoning in Knowledge Graphs. In Advances in Neural Information Processing
Systems (NeurIPS) .
[21]Abulhair Saparov and He He. 2022. Language models are greedy reasoners: A
systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240
(2022).
[22]Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing Guo, and Jianfeng Gao. 2018.
M-walk: Learning to walk over graphs using monte carlo tree search. Advances in
Neural Information Processing Systems (NeurIPS) 31 (2018).
[23]KoustuvSinha,ShagunSodhani,JinDong,JoellePineau,andWilliamLHamilton.
2019. CLUTRR:Adiagnosticbenchmarkforinductivereasoningfromtext. arXiv
preprint arXiv:1908.06177 (2019).
[24]Steven A Sloman. 1996. The empirical case for two systems of reasoning.
Psychological bulletin 119, 1 (1996), 3.
[25]AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalMdShoeb,Abubakar
Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-
Alonso,etal .2022. Beyondtheimitationgame:Quantifyingandextrapolatingthe
capabilities of language models. arXiv preprint arXiv:2206.04615 (2022).[26]Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay,
Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou,
, and Jason Wei. 2022. Challenging BIG-Bench Tasks and Whether Chain-of-
Thought Can Solve Them. arXiv preprint arXiv:2210.09261 (2022).
[27]Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
ZhiyuanChen, JiakaiTang, XuChen,YankaiLin,et al .2023. A surveyon large
language model based autonomous agents. arXiv preprint arXiv:2308.11432
(2023).
[28]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
QuocVLe,DennyZhou,etal .2022. Chain-of-thoughtpromptingelicitsreasoning
in large language models. Advances in Neural Information Processing Systems 35
(2022), 24824–24837.
[29]Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. Mindmap: Knowledge graph
prompting sparks graph of thoughts in large language models. arXiv preprint
arXiv:2308.09729 (2023).
[30]Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. DeepPath: A
ReinforcementLearningMethodforKnowledgeGraphReasoning.In Proceedings
oftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP) ,
Martha Palmer, Rebecca Hwa, and Sebastian Riedel (Eds.). Association for
Computational Linguistics, 564–573. https://doi.org/10.18653/v1/d17-1060
[31]ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamWCohen,Ruslan
Salakhutdinov,andChristopherDManning.2018.HotpotQA:Adatasetfordiverse,
explainable multi-hop question answering. arXiv preprint arXiv:1809.09600
(2018).
[32]ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,ThomasLGriffiths,YuanCao,
andKarthikNarasimhan.2023. Treeofthoughts:Deliberateproblemsolvingwith
large language models. arXiv preprint arXiv:2305.10601 (2023).
[33]Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, and Haipeng Ding. 2021. Neural,
symbolic and neural-symbolic reasoning on knowledge graphs. AI Open2 (2021),
14–35.
[34]Wayne XinZhao,KunZhou,JunyiLi, TianyiTang,Xiaolei Wang,YupengHou,
YingqianMin,BeichenZhang,JunjieZhang,ZicanDong,etal .2023. Asurvey
of large language models. arXiv preprint arXiv:2303.18223 (2023).
[35]Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang,
Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al .2022. Least-
to-most prompting enables complex reasoning in large language models. arXiv
preprint arXiv:2205.10625 (2022).Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
A APPENDIX
A.1 Datasets
CLUTRR The most representative dataset employed for relation
prediction isCLUTRR[ 23](CompositionalLanguageUnderstanding
with Text-based Relational Reasoning). Itis abenchmarkdesigned
to infer the relationship between two family members, which isn’t
directly mentioned in the story. Successful performance on this task
requires both extracting relationships between entities, as well as
inferringthelogicalrulesgoverningtheserelationships.Toassess
thecomplexityofthequestionswithintheCLUTRRdataset,wehave
categorized them based on the length of the paths between the target
familymembers.Foracomprehensiveoverviewofthestatisticsof
the CLUTRR dataset, please refer to Table 1.
BIG-bench The Beyond the Imitation Game Benchmark (BIG-
bench)[25]isacomprehensivedatasetdesignedforevaluatingthe
capabilities of LLMs. BIG-bench is notable for its emphasis on
tasks that pose significant challenges to current AI models. This
benchmarkincludesawidevarietyoftasksthattestvariousaspectsof
language understanding and generation. These tasks can range from
simplearithmetictocomplexreasoning,understandingofcultural
references,andmore.Amongthese,23particularlydemandingtasks
have been aggregated to form the BIG-Bench Hard (BBH) [ 26]
dataset. These tasks were selected because prior language model
evaluations did not surpass average human performance on them.
SeveraltaskswithinBBHprovidetypicalexamplesthatalignwith
our research. For instance, the tracking shuffled objects tasks aligns
withentitypredictionoverdynamicKG .Thelogicaldeductiondataset
isaptfor graphsorting task.Datasetslikereasoningaboutcolored
objects and penguins in a table are suitable for graph query task.
HotpotQA HotpotQA[ 31]isawidelyusedmulti-hopquestion-
answering dataset. Unlike traditional QA datasets where the answer
canbefoundinasinglepassage,HotpotQArequiresthesystemto
gatherandintegrateinformationfromseveralpassagestoanswera
question correctly. A notable feature of HotpotQA is its inclusion
ofbridging questions , which are structured to start with an initial
factlocatedinonepassage.Thechallengeistoleveragethispieceof
informationtouncoverandcomprehendarelatedfactinadifferent
passage, effectively forming a “bridge” that connects the initial fact
to the final answer. By integrating the insights gained from the
secondary passage with the initial information, we can effectively
addressthebridgequestion.Thesebridgingquestionsthusserveas
typical examples for complex entity prediction task. For our analysis,
we randomly selected a subset of 150 hard bridging questions from
the development sets of HotpotQA.
EntailmentBank EntailmentBank[ 6]isakeydatasetextensively
employed for logical reasoning studies. This dataset includes struc-
turedentailmenttreesthatrepresentcomplexentailmentreasoningin
ahierarchicalformat.Thesetreesaredesignedtoprovidestep-by-step
logicalreasoningprocessfrompremisestoconclusion.Toassessthe
complexity of the questions within the Entailment Bank, we have
categorized them based on the numberof entailment steps required
toarriveatananswer.Foracomprehensiveoverviewofthestatistics
of the Entailment Bank datasets, please refer to Table 2.A.2 Case Study over Each Tasks
In this section, we showcase examples of both correct and incorrect
responses,generatedby GPT-4,utilizingthepromptswedesigned
for each task.
A.2.1 Relation Prediction. First, we showcase an example accu-
ratelypredictedbyourproposedStructureGuidedPrompt,incontrast
to its incorrect prediction by 0-CoT.
Question: [Seth] and his wife [Laura] went to the beach to watch
the fireworks on the4th of July. [Rosa] took her father[Seth] to his
doctor’sappointmentandafterwardherbrother[Jason]cameover
and visited for a while. [Jonathan] wanted to have a family cookout
so he invited his brother [Christian]. [Christian] wanted to invite
hisotherbrother[Jason].Whatisthefamilyrelationshipbetween:
[Laura] with [Jonathan]?
0-CoT Answer: The story does not provide information on the
family relationship between Laura and Jonathan. ✗
Structure Guided Prompt Answer: The family relationship be-
tween Laura and Jonathan is not explicitly mentioned in the story.
However,we caninfersome relationshipsbasedon theinformation
given.
Knowledge Graph:
1. (Seth, wife, Laura)
2. (Seth, daughter, Rosa)
3. (Rosa, brother, Jason)
4. (Jonathan, brother, Christian)
5. (Christian, brother, Jason)
From the knowledge graph, we can see that Seth is married to
LauraandSethisthefatherofRosaandJason.Jonathanisthebrother
of Christian, who is also a brother of Jason. Therefore, Jonathan and
Jason are brothers, making Seth Jonathan’s father. Since Laura is
Seth’s wife, she would be Jonathan’s mother.
So, the family relationship between Laura and Jonathan is that
Laura is Jonathan’s mother. ✔
Next,wepresentanexamplewhereourproposedStructureGuided
Prompt made an incorrect prediction, in contrast to the correct
prediction made by 0-CoT.
Question: [Christian] was tired of his sister, [Ruth], complaining
about his cooking so [Christian] complained to his father, [Seth].
[Ruth]andherdaughter[Stephanie]wenttoseeamovieyesterday
and then got ice cream afterwards. [Christian] invited his father
[Seth] and his brother [Stephen] to go on a weekend fishing trip.
Whatisthefamilyrelationshipbetween:[Stephanie]with[Stephen]?
0-CoT Answer: [Stephanie] is [Stephen]’s niece. ✔
Structure Guided Prompt Answer: To answer the question, we
need to find the relation between Stephanie and Stephen. Let’s start
by creating a knowledge graph from the given story.
1. (Christian, sibling, Ruth)
2. (Christian, child, Seth)
3. (Ruth, child, Seth)
4. (Ruth, parent, Stephanie)
5. (Christian, sibling, Stephen)
6. (Stephen, child, Seth)
Now, let’s start with Stephanie and find the most relevant facts to
expand from it.
From fact 4, we know that Ruth is Stephanie’s mother.Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Cheng, et al.
Table 1: Statistics of the CLUTRR Datasets.
3 hop4 hop5 hop6 hop7 hop8 hop9 hop10 hop
# queries 10515414690129133100 97
Table 2: Statistics of the Entailment Bank Datasets.
1 hop2 hop3 hop4 hop5 hop6 hop
# queries 869072532214
From fact 3, we know that Ruth is also Seth’s child.
From fact 6, we know that Stephen is also Seth’s child.
So,Stephanie andStephenarebothchildrenofSeth,whichmakes
themsiblings. Therefore, the missing relation is ’sibling’. ✗
Analysis: Aswehighlight,theerrorintheoutputofourmodel
comes from an inaccurate conclusion. Specifically, Stephanie
should be categorized as Seth’s grandchild, not as his child.
Consequently, the correct relation should be “niece.”
A.2.2 Entity Prediction over Dynamic KG. First,weshowcasean
example accurately predicted by our proposed Structure Guided
Prompt, in contrast to its incorrect prediction by 0-CoT.
Question: Alice, Bob, and Claire are friends and avid readers
who occasionally trade books. At the start of the semester, they each
buyonenewbook:AlicegetsHoundoftheBaskervilles,Bobgets
ThePearl,andClairegetsTheOdyssey.Asthesemesterproceeds,
they start trading around the new books. First, Bob and Claire swap
books.Then,ClaireandAliceswapbooks.Finally,BobandAlice
swap books. At the end of the semester, Alice has Options:
(A) Hound of the Baskervilles
(B) The Pearl
(C) The Odyssey
0-CoT Answer: (B) The Pearl ✗
Structure Guided Prompt Answer: (C) The Odyssey
Explanation: Let’s create the knowledge graph by extracting facts
from each sentence in the story.
1.Alice,Bob,andClairearefriendsandavidreaderswhoocca-
sionally trade books.
- (Alice, is_friend_with, Bob)
- (Alice, is_friend_with, Claire)
- (Bob, is_friend_with, Claire)
- (Alice, is_avid_reader, True)
- (Bob, is_avid_reader, True)
- (Claire, is_avid_reader, True)
- (Alice, trades_books, True)
- (Bob, trades_books, True)
- (Claire, trades_books, True)
2. At thestart ofthe semester,theyeach buyone new book:Alice
gets Hound of the Baskervilles, Bobgets The Pearl, and Claire gets
The Odyssey.
- (Alice, has_book, Hound of the Baskervilles)
- (Bob, has_book, The Pearl)
- (Claire, has_book, The Odyssey)3. As the semester proceeds, they start trading around the new
books. First, Bob and Claire swap books.
- (Bob, has_book, The Odyssey)
- (Claire, has_book, The Pearl)
4. Then, Claire and Alice swap books.
- (Claire, has_book, Hound of the Baskervilles)
- (Alice, has_book, The Pearl)
5. Finally, Bob and Alice swap books.
- (Bob, has_book, The Pearl)
- (Alice, has_book, The Odyssey)
So, at the end of the semester, Alice has The Odyssey. Therefore,
option (C) is correct. ✔
Next,wepresentanexamplewhereourproposedStructureGuided
Prompt made an incorrect prediction, in contrast to the correct
prediction made by 0-CoT.
Question: Alice,Bob,andClairearedancersatasquaredance.
Atthestartofasong,theyeachhaveapartner:Aliceisdancingwith
Rodrigo,BobisdancingwithJamie,andClaireisdancingwithLola.
Throughoutthesong,thedancersoftentradepartners.First,Claire
and Alice switch partners. Then, Bob and Claire switch partners.
Finally, Claire and Alice switch partners. At the end of the dance,
Alice is dancing with Options:
(A) Rodrigo
(B) Jamie
(C) Lola
0-CoT Answer: (B) Jamie ✔
Structure Guided Prompt Answer: (C) Lola
Explanation:
Let’s create the knowledge graph:
1. Alice, Bob, and Claire are dancers at a square dance.
- (Alice, is a dancer at, square dance)
- (Bob, is a dancer at, square dance)
- (Claire, is a dancer at, square dance)
2.Atthestartofasong,theyeachhaveapartner:Aliceisdancing
withRodrigo,BobisdancingwithJamie,andClaireisdancingwith
Lola.
- (Alice, is dancing with, Rodrigo)
- (Bob, is dancing with, Jamie)
- (Claire, is dancing with, Lola)
3. First, Claire and Alice switch partners.
- (Alice, is dancing with, Lola)
- (Claire, is dancing with, Rodrigo)
4. Then, Bob and Claire switch partners.
- (Bob, is dancing with, Rodrigo)Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
- (Claire, is dancing with, Jamie)
5. Finally, Claire and Alice switch partners.
-(Alice,isdancingwith,Jamie) ✔
- (Claire, is dancing with, Lola)
So,attheendofthedance,AliceisdancingwithLola.Therefore,
option (C) is correct. ✗
Analysis: As we highlight, while the inference process of
our proposed model is accurate, it makes mistakes during
theconclusion-drawingphase.Thispatternhasbeenwidely
observed in our experiments.
A.2.3 Complex Entity Prediction. First,weshowcaseanexample
accuratelypredictedbyourproposedStructureGuidedPrompt,in
contrast to its incorrect prediction by 0-CoT.
Question: Paragraph 0 The Lighthouse (Poole): The Lighthouse
is an arts centre in Poole, Dorset, England.According to the Arts
councilofEnglanditisthelargestartscentreintheUnitedKingdom
outside London.
Paragraph1CanfordCliffs:CanfordCliffsisanaffluentsuburb
of Poole in Dorset, England.The neighbourhood lies on the English
Channel coast midway between Poole and Bournemouth.To the
southwest is Sandbanks which has some of the highest property
valuesintheworld,andtogetherCanfordCliffsandSandbanksform
aparish,whichhasthefourthhighestpropertypricesintheworld
and second highest in the United Kingdom after London.
Paragraph2ViscountTrenchard:ViscountTrenchard,ofWolfeton
in the County of Dorset, is a title in the Peerage of the United
Kingdom.It was created in 1936 for Marshal of the Royal Air Force,
Hugh Trenchard, 1st Baron Trenchard.He had already been created
aBaronet,ofWolfetonintheCountyofDorset,intheBaronetage
oftheUnitedKingdomin1919andBaronTrenchard,ofWolfeton
intheCountyofDorset,in1930,alsointhePeerageoftheUnited
Kingdom.Hissecondson,thesecondViscount,heldjuniorministerial
positionsfrom1979to1983intheConservativeadministrationof
Margaret Thatcher.s of 2016 the titles are held by the latter’s son,
the third Viscount, who succeeded in 1987.In 2004 he replaced
the recently deceased Lord Vivian as one of the ninety elected(by
hereditarypeers)hereditarypeersthatareallowedtoremaininthe
HouseofLordsafterthepassingoftheHouseofLordsAct1999.Lord
Trenchard sits on the Conservative benches.
Paragraph3FireRadio:FireRadioisaUnitedKingdomradiosta-
tion broadcasting to Bournemouth, Poole, and Christchurch, Dorset,
based in Southampton, Hampshire.
Paragraph4DorsetCountyCouncilelection,2013:Anelection
to Dorset County Council took place on 2 May 2013 as part of
the United Kingdom local elections.45 councillors were elected
from 42 electoral divisions, which returned either one or two county
councillors each by first-past-the-post voting for a four-year term
ofoffice.Theelectoraldivisionswerethesameasthoseusedatthe
previouselectionin2009.NoelectionswereheldinBournemouthor
Poole, which are unitary authorities outside the area covered by the
CountyCouncil.TheelectionsawtheConservativePartymaintain
overall control of the council.
Paragraph5ViscountWimborne:ViscountWimborne,ofCanford
MagnaintheCountyofDorset,isatitleinthePeerageoftheUnitedKingdom.It was created in 1918 for Ivor Guest, 2nd Baron Wim-
borne.TheGuestfamilydescendsfromtheengineerandbusinessman
JohnJosiah Guest.On14August1838he wascreated abaronet,of
DowlaisintheCountyofGlamorgan,intheBaronetageoftheUnited
Kingdom.Hewassucceededbyhiseldestson,thesecondBaronet.In
1880 he was created Baron Wimborne, of Canford Magna in the
County of Dorset, in the Peerage of the United Kingdom.On his
deaththetitlespassedtohiseldestson,thesecondBaron.In1910,
fouryearsbeforehesucceededhisfather,hehadbeenraisedtothe
Peerage of the United Kingdom in his own right as Baron Ashby
St Ledgers,of AshbySt Ledgersin theCounty ofNorthampton.On
his retirement as Lord-Lieutenant of Ireland in 1918 he was further
honouredwhenhewasmadeViscountWimborne,ofCanfordMagna
intheCountyof Dorset,inthePeerageof theUnitedKingdom.His
son,thesecondViscount,representedBreconshireintheHouseof
Commons.sof2014thetitlesareheldbythelatter’sgrandson,the
fourth Viscount, who succeeded his father in 1993.
Paragraph 6 Lush (company): Lush Ltd. is a cosmetics retailer
headquartered in Poole, Dorset, United Kingdom.The company was
founded by Mark Constantine, a trichologist and Liz Weir, a beauty
therapist.TheymetinahairandbeautysaloninPoole,England.Afew
yearslater,theydecidedtobranchoutandstarttheirownbusiness
selling natural hair and beauty products.
Paragraph7BarondeMauley:BarondeMauley,ofCanfordinthe
County of Dorset, is a title in the Peerage of the United Kingdom.It
was created in 1838 for the Whig politician the Hon. William
Ponsonby, who had earlier represented Poole, Knaresborough and
Dorset in the House of Commons.He was the third son of the 3rd
Earl of Bessborough, an Anglo-Irish peer, and the husband of Lady
Barbara Ashley-Cooper, one of the co-heirs to the ancient barony by
writ of Mauley (or Maulay), which superseded the feudal barony the
"caput"ofwhichwasatMulgraveCastle,Yorkshire,whichbarony
by writ had become extinct in 1415.His son, later the second Baron,
sat as Member of Parliament for Poole and Dungarvon.s of 2009 the
titleisheldbythelatter’sgreat-great-grandson,theseventhBaron,
who succeeded his uncle in 2002.He is one of the ninety elected
hereditary peers that remain in the House of Lords after the passing
oftheHouseofLordsAct1999,andsitsasaConservative.Hewas
electedin 2005and therebybecamethe firsthereditary peerhaving
succeededtohistitleaftertheHouseofLordsActof1999,tohave
obtainedanelectivehereditarypeersseatintheHouseofLords.Asa
descendant of the third Earl of Bessborough, Lord de Mauley is also
in remainder to the earldom of Bessborough and its subsidiary titles.
Paragraph8ListofciderproducersinDorset:Thisisalistofcider
brands fromDorset in theUnited Kingdom.Although neighbouring
county of Somerset is better known for its apple orchards, Dorset
hasanequallylongorchardandcidertradition.ManyDorsetcider
producers are using traditional Dorset varieties of apple The Dorset
Cider museum is based in Owermoigne, near Dorchester
Paragraph 9 Poole Methodist Church: Poole Methodist Church
(also known as Poole High Street Methodist Church or The Spire)
is a nineteenth-century Methodist church on Poole High Street in
Dorset,England.Anextensiontothechurchwasnominatedforthe
2016CarbuncleCupfor"theugliestbuildingintheUnitedKingdom
completed in the last 12 months".
What retailer in ABQ Uptown is headquarted in Poole, Dorset,
United Kingdom?Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Cheng, et al.
0-CoT Answer: The story does not provide information on any
retailer in ABQ Uptown that is headquartered in Poole, Dorset,
United Kingdom. ✗
Structure Guided Prompt Answer: Thequestiondoesnotpro-
videenoughinformationtoanswerdirectly.However,fromthegiven
paragraphs, we can infer that the retailer headquartered in Poole,
Dorset, United Kingdomis Lush Ltd. It is not specified in the text
whether Lush Ltd. has a store in ABQ Uptown. To answer the ques-
tion, we would need additional information about the retailers in
ABQ Uptown.
Sub-questions:1.WhichretailerisheadquarteredinPoole,Dorset,
United Kingdom? (Answer: Lush Ltd.) 2. Does Lush Ltd. have a
storeinABQUptown?(Answer:Notspecifiedinthetext,additional
information needed.) ✔
Next,wepresentanexamplewhereourproposedStructureGuided
Prompt made an incorrect prediction, in contrast to the correct
prediction made by 0-CoT.
Question: Paragraph 0 1965 Mexican Grand Prix: The 1965
Mexican Grand Prix was a Formula One motor race held at Ciudad
DeportivaMagdalenaMixhucainMexicoCityonOctober24,1965.It
was race 10 of 10 in both the 1965 World Championship of Drivers
and the 1965 International Cup for Formula One Manufacturers.The
race was won by Richie Ginther, who took his first victory and
the first for the Honda team, after leading for the entire race.The
Brabham-Climax of Dan Gurney finished the race second and the
Lotus-Climax of Mike Spence completed the podium.
Paragraph12006FIAFormulaOneWorldChampionship:The
2006FIAFormulaOneWorldChampionshipwasthe60thseason
of FIA Formula One motor racing.It featured the 2006 FIA Formula
OneWorldChampionshipwhichbeganon12Marchandendedon
22Octoberaftereighteenraces.TheDrivers’Championshipwaswon
byFernandoAlonsoofRenaultF1forthesecondyearinarow,with
Alonsobecomingtheyoungesteverdoubleworldchampionatthe
time.Then-retiringmultipleworldchampionMichaelSchumacher
of Scuderia Ferrari finished runner-up, 13 points behind.The Con-
structors’Championship waswonbyMildSeven RenaultF1Team,
which defeated Scuderia Ferrari Marlboro by five points.
Paragraph 2 1963 United States Grand Prix: The 1963 United
StatesGrandPrixwasaFormulaOnemotorraceheldonOctober
6, 1963, at the Watkins Glen Grand Prix Race Course in Watkins
Glen, New York.It was race 8 of 10 in both the 1963 World Champi-
onshipofDriversandthe1963InternationalCupforFormulaOne
Manufacturers.The110-lapracewaswonbyBRMdriverGraham
Hill after he started from pole position.His teammate Richie Ginther
finished second and Lotus driver Jim Clark came in third.
Paragraph 3 Formula One World Champions: A Formula One
World Champion is a racing driver or automobile constructor which
hasbeendesignatedsuchatitlebythegoverningbodyofFormula
One - the FIA.Every Formula One World Champion since the
inauguralWorldDrivers’Championshipin1950andtheinaugural
World Constructors’ Championship in 1958 has been awarded the
title by accumulating the required points during the course of the F1
season of that particular year, by participating in relevant Grands
Prix.
Paragraph42000FIAFormulaOneWorldChampionship:The
2000FIAFormulaOneWorldChampionshipwasthe54thseasonof FIA Formula One motor racing.It featured the 2000 FIA Formula
One World Championship which commenced on 12 March 2000,
and ended on 22 October after seventeen races.Michael Schumacher
became Ferrari’s first World Drivers’ Champion for 21 years having
clinchedtheDrivers’titleatthepenultimateraceoftheseason.Ferrari
successfully defended its Constructors’ title.This season marked the
first for future world champion Jenson Button.
Paragraph 5 1963 German Grand Prix: The 1963 German Grand
Prix was aFormula One motorrace held at Nürburgringon August
4, 1963.It was race 6 of 10 in both the 1963 World Championship of
Driversandthe1963InternationalCupforFormulaOneManufac-
turers.The15-lapracewaswonbyFerraridriverJohnSurteesafter
hestartedfromsecondposition.JimClarkfinishedsecondforthe
LotusteamandBRMdriverRichieGinthercameinthird.
Paragraph 6 1964 Austrian Grand Prix: The 1964 Austrian Grand
Prix was a Formula One World Championship motor race held at
Zeltweg Airfield on August 23, 1964.It was race 7 of 10 in both the
1964WorldChampionshipofDriversandthe1964InternationalCup
forFormulaOneManufacturers.The105-lapracewaswonbyFerrari
driver Lorenzo Bandini after he started from seventh position.Richie
Ginther finished second for the BRM team and Brabham driver Bob
Anderson came in third.This was the debut World Championship
race of the future world champion Jochen Rindt.
Paragraph71962ItalianGrandPrix:The1962ItalianGrandPrix
wasaFormulaOnemotorraceheldatMonzaon16September1962.It
was race 7 of 9 in both the 1962 World Championship of Drivers
and the 1962 International Cup for Formula One Manufacturers.The
86-lap race was won by BRM driver Graham Hill after he started
from second position.His teammate Richie Ginther finished second
and Cooper driver Bruce McLaren came in third.
Paragraph 8 Richie Ginther: Paul Richard "Richie" Ginther
(Granada Hills, California, August 5, 1930 – September 20, 1989 in
France) was a racecar driver from the United States.During a varied
career, the 1965 Mexican Grand Prix saw Ginther take Honda’s
first Grand Prix victory, a victory which would also prove to be
Ginther’sonlywininFormulaOne.Ginthercompetedin54World
ChampionshipFormulaOneGrandPrixracesandnumerousother
non-Championship F1 events.
Paragraph9JohnSurtees:JohnSurtees,’1’:",’2’:",’3’:",’4’:
"(11February1934–10March2017)wasanEnglishGrandPrix
motorcycleroadracerandFormulaOnedriver.Hewasafour-time
500ccmotorcycleWorldChampion–winningthattitlein1956,1958,
1959 and 1960 – the Formula One World Champion in 1964, and
remains the only person to have won World Championships on both
two and four wheels.He founded the Surtees Racing Organisation
team that competed as a constructor in Formula One, Formula 2 and
Formula5000from1970to1978.Hewasalsotheambassadorofthe
Racing Steps Foundation.
Which Formula One World Champion had a teammate named
Richie Ginther?
0-CoT Answer: Graham Hill had a teammate named Richie
Ginther. ✔
Structure Guided Prompt Answer: To answer the question, we
need to first identify the teams that Richie Ginther was a part of.
Then, we need to find out who were his teammates in these teams
and whether any of them were Formula One World Champions.Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
Sub-questions:
1. Which teams did Richie Ginther race for?
2. Who were Richie Ginther’s teammates on these teams?
3. Which of these teammates were Formula One World Champi-
ons?
Let’s tackle these sub-questions one by one.
1.FromParagraph2,weknowthatRichieGintherwasadriver
for the BRM team. From Paragraph 5, we learn that he was also a
driver for the Ferrari team.
2.FromParagraph2,weknowthatGrahamHillwasateammate
ofRichieGintherintheBRMteam. ✔FromParagraph5,weknow
thatJohnSurteeswasateammateofRichieGintherintheFerrari
team.✗
3. From Paragraph 3, we know that a Formula One World Cham-
pionisaracingdriverwhohasbeenawardedthetitlebyaccumulating
therequiredpointsduringthecourseoftheF1season.FromPara-
graph 9, we know that John Surtees was a Formula One World
Champion in 1964.
Byconnectingtheanswersfromthesesub-questions,wecan
concludethattheFormulaOneWorldChampionwhohadRichie
GintherasateammatewasJohnSurtees.✗
Analysis: As pointed out specifically in Paragraph 5 from the
original context, it’s noted that bothJohn Surtees and Richie
Ginther participated in the German Grand Prix. However,
John Surtees was a member of the Ferrari team, while Richie
Ginther raced for BRM. This information indicates that John
Surtees and Richie Ginther were not teammates.
A.2.4 Graph Sorting. First, we showcase an example accurately
predictedbyourproposedStructureGuidedPrompt,incontrastto
its incorrect prediction by 0-CoT.
Question: The following paragraphs each describe a set of three
objectsarrangedinafixedorder.Thestatementsarelogicallycon-
sistentwithineachparagraph.Onabranch,therearethreebirds:a
falcon,anowl,andaraven.Theravenistotheleftoftheowl.The
falcon is the leftmost. Options:
(A) The falcon is the second from the left
(B) The owl is the second from the left
(C) The raven is the second from the left
0-CoT Answer: (B) The owl is the second from the left ✗
Structure Guided Prompt Answer: The knowledge graph from
the story would look like this:
1. Falcon - Position: Leftmost
2. Owl - Position: Middle
3. Raven - Position: Rightmost
From the story, we know that the falcon is the leftmost, the raven
is to the left of the owl, which means the owl is to the right of the
raven,andsincethefalconistheleftmost,theravenmustbeinthe
middle and the owl must be the rightmost.
So, evaluating the options:
(A) The falcon is the second from the left - This is incorrect as
the falcon is the leftmost.
(B)Theowlisthesecondfromtheleft-Thisisincorrectasthe
owl is the rightmost.(C)Theravenisthesecondfromtheleft-Thisiscorrectasthe
raven is in the middle, which makes it second from the left.
Therefore, the correct statement is (C) The raven is the second
from the left. ✔
Next,wepresentanexamplewhereourproposedStructureGuided
Prompt made an incorrect prediction, in contrast to the correct
prediction made by 0-CoT.
Question: The following paragraphs each describe a set of three
objectsarrangedinafixedorder.Thestatementsarelogicallyconsis-
tentwithineachparagraph.Afruitstandsellsthreefruits:peaches,
pears, and mangoes. The mangoes are less expensive than the pears.
The peaches are the cheapest. Options:
(A) The peaches are the second-most expensive
(B) The pears are the second-most expensive
(C) The mangoes are the second-most expensive
0-CoT Answer: (C) The mangoes are the second-most expensive
✔
Structure Guided Prompt Answer: (B)Thepearsarethesecond-
most expensive
Explanation: From the story, we can extract the following facts to
create a knowledge graph:
1. The fruit stand sells peaches, pears, and mangoes.
2. The mangoes are less expensive than the pears.
3. The peaches are the cheapest.
Fromthesefacts,wecaninfertheorderofthefruitsfromcheapest
tomostexpensive:peaches,mangoes,pears. ✔Therefore, thepears
arethesecond-mostexpensive. ✗
Analysis: As we highlight, while the inference process of
our proposed model is accurate, it makes mistakes during
theconclusion-drawingphase.Thispatternhasbeenwidely
observed in our experiments.
A.2.5 Graph Query. First, we showcase an example accurately
predictedbyourproposedStructureGuidedPrompt,incontrastto
its incorrect prediction by 0-CoT.
Question: Hereisatablewherethefirstlineisaheaderandeach
subsequent line is a penguin: name, age, height (cm), weight (kg)
Louis,7,50,11Bernard,5,80,13Vincent,9,60,11Gwen,8,70,
15Forexample:theageofLouisis7,theweightofGwenis15kg,
theheightofBernardis80cm.Andhereisasimilartable,butlisting
giraffes:
name, age, height (cm), weight (kg)
Jody, 5, 430, 620
Gladys, 10, 420, 590
Marian, 2, 310, 410
Donna, 9, 440, 650
What is the name of the last penguin sorted by alphabetic order?
Options:
(A) Louis
(B) Bernard
(C) Vincent
(D) Gwen
(E) James
0-CoT Answer: (D) Gwen ✗
Structure Guided Prompt Answer: (C) VincentConference acronym ’XX, June 03–05, 2018, Woodstock, NY Cheng, et al.
Explanation: The knowledge graph from the story would include
the following facts about the penguins:
- Louis is 7 years old, 50 cm tall, and weighs 11 kg.
- Bernard is 5 years old, 80 cm tall, and weighs 13 kg.
- Vincent is 9 years old, 60 cm tall, and weighs 11 kg.
- Gwen is 8 years old, 70 cm tall, and weighs 15 kg.
When we sort these penguins by their names in alphabetical
order,wegetBernard,Gwen,Louis,andVincent.Therefore,thelast
penguin sorted by alphabetic order is Vincent. ✔
Next,wepresentanexamplewhereourproposedStructureGuided
Prompt made an incorrect prediction, in contrast to the correct
prediction made by 0-CoT.
Question: Hereisatablewherethefirstlineisaheaderandeach
subsequent line is a penguin: name, age, height (cm), weight (kg)
Louis,7,50,11Bernard,5,80,13Vincent,9,60,11Gwen,8,70,
15Forexample:theageofLouisis7,theweightofGwenis15kg,
the height of Bernard is 80 cm. How many penguins are less than 8
years old and weight more than 12 kg? Options:
(A) 1
(B) 2
(C) 3
(D) 4
(E) 5
0-CoT Answer: (A) 1 ✔
Structure Guided Prompt Answer: (B) 2
Explanation: First, let’s create a knowledge graph from the given
data:
1. Louis: Age - 7, Height - 50 cm, Weight - 11 kg
2. Bernard: Age - 5, Height - 80 cm, Weight - 13 kg
3. Vincent: Age - 9, Height - 60 cm, Weight - 11 kg
4. Gwen: Age - 8, Height - 70 cm, Weight - 15 kg
Now,let’sanswerthequestion:"Howmanypenguinsarelessthan
8 years old and weight more than 12 kg?"
Fromtheknowledgegraph,wecanseethatthepenguinsthatmeet
these criteria are:
1.Bernard: Age-5(lessthan8),Weight-13kg(morethan12
kg)
2.Gwen:Age-8(notlessthan8),Weight-15kg(morethan12
kg)
3.Louis:Age-7(lessthan8),Weight-11kg(notmorethan12
kg)
4.Vincent:Age-9(notlessthan8),Weight-11kg(notmore
than12kg)✔
So,theonlypenguinsthatmeetbothcriteriaareBernardand
Gwen. Therefore, the answer is 2. ✗
Analysis: As we highlight, while the inference process of
our proposed model is accurate, it makes mistakes during
theconclusion-drawingphase.Thispatternhasbeenwidely
observed in our experiments.
A.2.6 Logical Inference. First,weshowcaseanexampleaccurately
predictedbyourproposedStructureGuidedPrompt,incontrastto
its incorrect prediction by 0-CoT.
Question: sent1: if something is in the sunlight then that some-
thingwillabsorbsolarenergysent2:waterisinthesolidstate,calledice sent3: the sun is the source of solar energy called sunlight sent4:
water is inthe liquid state, calledwater sent5: an icecube is placed
inthesunlightsent6:theicecubewillmeltandchangefromsolid
to liquid by absorbing the solar energy sent7: absorbing sunlight
causes objects to heat A student placed an ice cube on a plate in the
sun.Tenminuteslater,onlywaterwasontheplate.Whichprocess
caused the ice cube to change to water?
0-CoT Answer: Theprocessthatcausedthe icecubetochange
to water is the absorption of solar energy, which caused the ice cube
to heat and melt. ✗
Structure Guided Prompt Answer: To answer this question,
let’s first create a knowledge graph from the given story:
1. (something, in, sunlight) -> (something, absorb, solar energy)
2. (water, is in, solid state) -> (water, called, ice)
3. (sun, is source of, solar energy) -> (sun, called, sunlight)
4. (water, is in, liquid state) -> (water, called, water)
5. (ice cube, placed in, sunlight)
6. (ice cube, will melt and change from, solid to liquid) -> (ice
cube, by absorbing, solar energy)
7. (absorbing sunlight, causes, objects to heat)
Now,let’stransformthequestionintoatriple:(icecube,change
to,?).Weneedtofindtheprocessthatcausedtheicecubetochange
to water.
From the knowledge graph, we can see that when ’something’ (in
this case, the ice cube) is in the sunlight, it will absorb solar energy
(Fact 1). The ice cube is in the sunlight (Fact 5), so it will absorb
solar energy. Absorbing solar energy causes the ice cube to melt and
changefromsolidtoliquid(Fact6).Thischangefromsolidtoliquid
is called melting.
So,byfollowingthelogicalpathintheknowledgegraphandusing
deductivereasoning,wecanconcludethattheprocessthatcaused
the ice cube to change to water is ’melting’. ✔
Next,wepresentanexamplewhereourproposedStructureGuided
Prompt made an incorrect prediction, in contrast to the correct
prediction made by 0-CoT.
Question: sent1: sunlight is a kind of solar radiation sent2: ultra-
violetlight causessunburn sent3:sunlightcontains ultravioletlight
Which form of solar radiation causes sunburn?
0-CoT Answer: Ultraviolet light ✔
Structure Guided Prompt Answer: Toanswerthequestion,let’s
first create a knowledge graph from the given sentences:
1. (sunlight, is a kind of, solar radiation)
2. (ultraviolet light, causes, sunburn)
3. (sunlight, contains, ultraviolet light)
Now, let’s transform the question into a triple (s, r, ?): (Which
form of solar radiation, causes, sunburn?)
The subject entity ’s’ here is ’Which form of solar radiation’. The
relation ’r’ is ’causes’. We need to find the tail entity ’?’ that is
’sunburn’.
Starting with the subject entity, we look for a statement related to
’solarradiation’.Wefinditinthefirstsentence:(sunlight,isakind
of, solar radiation). So, ’sunlight’ is a form of ’solar radiation’.
Next,welookforastatementrelatedto’sunlight’.Wefinditinthe
third sentence: (sunlight, contains, ultraviolet light). So, ’sunlight’
contains ’ultraviolet light’.Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
Finally,welookforastatementrelatedto’ultravioletlight’.We
finditinthesecondsentence:(ultravioletlight,causes,sunburn). So,
’ultravioletlight’causes’sunburn’. ✔
Bycombiningtheseconclusions,wecandeducethat’sunlight’,
which is a form of ’solar radiation’ and contains ’ultraviolet light’,
causes ’sunburn’.
Inconclusion,theformofsolarradiationthatcausessunburnis
’sunlight’. ✗
Analysis: As we highlight, while the inference process of
our proposed model is accurate, it makes mistakes during
theconclusion-drawingphase.Thispatternhasbeenwidely
observed in our experiments.A.3 Prompts
WeprovidethepromptsthatweusedtoquerytheLLMsforallsix
tasks in Tables 3 to 8. We do not use the system message field for
any model. To facilitate recognition, each stage of the prompt is
color-coded:oliveforthefirststage,tealforthesecond,andviolet
for the third.
A.4 Raw Results
We show the numeric results for all six tasks in Tables 9 to 12.Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Cheng, et al.
Table 3: Prompts for relation prediction.
Mode Prompt
0-CoT Based on the story, through inductive reasoning think step by step to answer the question.
Structure Guided Prompt
First,createaknowledgegraphbyextractingfactsfromeachsentenceinthegiveninputstory.Oncethisisdone,
I will pose a question. This question can be transformed into a triple (s, ?, o), where your primary task is to
determinethemissingrelation(’?’)thatlinksthesubjectentity(’s’)totheobjectentity(’o’).Tobegin,focus
onthesubjectentityinthistripleandchoosethemostrelevantfactstoexpandfromit.Stepbystep,progress
towards the object entity, ensuring that each selected fact contributes to creating a link between the subject and
objectentities.Finally,utilizetheestablishedconnectionbetweenthesubjectandobjectentitiestoanswerthe
question.
Table 4: Prompts for entity prediction over dynamic KG.
Mode Prompt
0-CoT Given the input, think step by step to answer the question using the option number.
Structure Guided Prompt
First, create a knowledge graph by extracting facts from each sentence in the given input story. The graph
shouldevolveasthestoryprogresses.Iwillpresentseveralstatements.Yourprimarytaskistodeterminethe
correctness of these statements by converting them into triples (s, r, o). Assess each statement’s validity against
the knowledge graph as it stands at the story’s conclusion.
Table 5: Prompts for complex entity prediction.
Mode Prompt
0-CoT Based on the story, think step by step to answer the question.
Structure Guided Prompt
First, create a knowledge graph by extracting facts from each sentence in the given input story. Once this is
done, I will pose a complex question requiring multi-step reasoning. Decompose the question into simpler
sub-questions focusing on identifying crucial entities, their relationships, and specific details. Tackle these
sub-questionssequentially,referencingtheknowledgegraphforinformation.Connecttheanswersfromthese
sub-questions step by step, until arrive at a final answer to the initial complex question.
Table 6: Prompts for graph sorting.
Mode Prompt
0-CoT Given the input, think step by step to answer the question using the option number.
Structure Guided Prompt
First, create a knowledge graph by extracting facts from each sentence in the given input story. Once this is
done,Iwillprovideseveralstatements.Yourprimarytaskistodeterminethecorrectnessofthesestatements.To
assess the validity of a statement, sort the objects in the graph and evaluate the correctness of each statement.
Table 7: Prompts for graph query.
Mode Prompt
0-CoT Given the input, think step by step to answer the question using the option number.
Structure Guided Prompt
First,createaknowledgegraphbyextractingfactsfromeachsentenceinthegiveninputstory.Thegraphmay
evolveasthestoryprogresses.Oncethisisdone,Iwillposeaquestion.Thisquestionwillrequireyoutoidentify
objects that meet specific criteria. Utilize the final state of the knowledge graph, as it exists at the end of the
story, to provide the answer to the question.Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
Table 8: Prompts for logical inference.
Mode Prompt
0-CoT Based on the story, through deductive reasoning think step by step to answer the question.
Structure Guided Prompt
First,createaknowledgegraphbyextractingfactsfromeachsentenceinthegiveninputstory.Oncethisisdone,
I will pose a question. This question can be transformed into a triple (s, r, ?), where your primary task is to
determine the missing tail entity (’?’) that connects the subject entity (’s’) through the relation (’r’). Start by
concentrating on the subject entity in this triple and follow a logical path within the knowledge graph. Progress
stepby stepfromthestatement relatedtothe subject,usinga forwardchainingprocess.At eachstep,combine
theconclusionswiththefactsintheknowledgegraphtodeducenewconclusions.Thefinalconclusionalong
this logical path will serve as the answer to the question.
Table 9: Results over CLUTRR.
Mode 3 hop4 hop5 hop6 hop7 hop8 hop9 hop10 hop
GPT 3.5w/o 0-CoT 0.1620.0970.0620.0330.0230.0150.050.041
w/ 0-CoT 0.2950.1430.0410.0220.0230.0150.020.031
Structure Guided Prompt 0.5620.4220.3290.2670.2330.1950.260.258
GPT 4w/o 0-CoT 0.5430.3310.2470.2440.1160.1200.090.124
w/ 0-CoT 0.5430.3250.247 0.30.1160.1350.110.093
Structure Guided Prompt 0.6950.6040.568 0.50.4340.4060.390.299
Table 10: Results over BIG-bench-hard.
ModeDynamic KG Graph Sorting Graph Query
tracking
shuffled
objects
three
objectstracking
shuffled
objects
five
objectstracking
shuffled
objects
seven
objectslogical
deduction
three
objectslogical
deduction
five
objectslogical
deduction
seven
objectsreasoning
about
colored
objectspenguins
in
a
table
GPT 3.5w/o 0-CoT 0.32 0.16 0.128 0.572 0.4 0.416 0.56 0.692
w/ 0-CoT 0.384 0.28 0.232 0.66 0.54 0.632 0.684 0.815
Structure Guided Prompt 0.816 0.84 0.836 0.84 0.904 0.856 0.808 0.877
GPT 4w/o 0-CoT 0.484 0.34 0.304 0.952 0.676 0.652 0.88 0.760
w/ 0-CoT 0.44 0.324 0.28 0.932 0.664 0.636 0.86 0.753
Structure Guided Prompt 0.936 0.924 0.916 0.968 0.86 0.824 0.94 0.884
Table 11: Results over HotpotQA.
ModeGPT 3.5 GPT 4
w/o 0-CoT w/ 0-CoT Structure Guided Prompt w/o 0-CoT w/ 0-CoT Structure Guided Prompt
ACC 0.42 0.407 0.44 0.427 0.407 0.527
Table 12: Results over Entailment Bank.
Mode 1 hop2 hop3 hop4 hop5 hop6 hop
GPT 3.5w/o 0-CoT 0.6740.4890.4440.3770.2270.429
w/ 0-CoT 0.628 0.50.4720.3960.2730.429
Structure Guided Prompt 0.6400.5330.4310.4530.3640.429
GPT 4w/o 0-CoT 0.6630.6110.4030.4530.2270.429
w/ 0-CoT 0.6630.5890.4030.4910.2270.357
Structure Guided Prompt 0.7090.5440.4720.4530.3180.429