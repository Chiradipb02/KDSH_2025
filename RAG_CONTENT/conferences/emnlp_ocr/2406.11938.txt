TRACKING THE PERSPECTIVES OF
INTERACTING LANGUAGE MODELS
Hayden Helm†
Nomic AIBrandon Duderstadt
Nomic AIYoungser Park
Center for Imaging Sciences
Johns Hopkins University
Carey E. Priebe
Dept. of Applied Math. & Statistics
Johns Hopkins University
ABSTRACT
Large language models (LLMs) are capable of producing high quality information
at unprecedented rates. As these models continue to entrench themselves in soci-
ety, the content they produce will become increasingly pervasive in databases that
are, in turn, incorporated into the pre-training data, fine-tuning data, retrieval data,
etc. of other language models. In this paper we formalize the idea of a communi-
cation network of LLMs and introduce a method for representing the perspective
of individual models within a collection of LLMs. Given these tools we system-
atically study information diffusion in the communication network of LLMs in
various simulated settings.
(a) Fully connected.
 (b) Intra-class only.
 (c) Vulnerable.
…
… 
…
 (d) General.
Figure 1: Examples of communication networks of language models and databases. The edge struc-
ture and model intitializations directly impact the evolution of the perspectives of the models and
the overall health of the system.
1 I NTRODUCTION
The success of large pre-trained models in natural language processing (Devlin et al., 2018), com-
puter vision (Oquab et al., 2023), signal processing (Radford et al., 2023), among other domains
(Jumper et al., 2021) across various computing and human benchmarks has brought them to the fore-
front of the technology-centric world. Given their ability to produce human-expert level responses
for a large set of knowledge-based questions (Touvron et al., 2023; Achiam et al., 2023), the content
they produce is often propagated throughout forums that have influence over other models and hu-
man users (Brinkmann et al., 2023). As such, it is important to develop sufficient frameworks and
complementary tools to understand how information produced by these models affects the behavior
of other models and human users. We refer to a system where a model can potentially influence
other models as a system of interacting language models.
Beyond their ability to influence information on human-model forums, systems of interacting lan-
guage models are interesting in their own right – insofar as an individual model is an intriguing
proxy for an individual human (Helm et al., 2023; Kosinski, 2023), a system of interacting language
models is an intriguing proxy for human communities. Systems of interacting language models are
†corresponding author; [first-name] @nomic.ai
1arXiv:2406.11938v1  [cs.AI]  17 Jun 2024thus an alluring alternative or complement to studying human communities in the social sciences.
For example, it is often infeasible or unethical to subject entire communities to different information
paradigms to understand how individuals within the community – as well as the community itself
– change in response to an intervention. These issues are less prominent for systems of interacting
language models. Further, there is potential for greater control in community membership and cross-
community interactions, which may improve reproducibility and mitigate the effects of sociological
confounders.
In this paper, we study information diffusion in a system of interacting language models. The frame-
work and methods that we develop can be applied to monitoring information diffusion in human-
model forums and to the treatment of systems of interacting language models quantitatively as proxy
human communities. The current standard (Perez et al., 2024) for studying information diffusion
in a system of interacting language models requires i) parameterizing models with different sys-
tem prompts, contexts, weights, or collections of data, ii) providing an environment or template for
model-to-model or model-to-dataset interactions, and iii) analyzing how the outputs of the models
change after a sequence of interactions.
For example, researchers include descriptions of desired model behavior or personality in the sys-
tem prompt – e.g., “You have opinion A” is included in the system prompt for model 1 and “You
have opinion B” is included in the system prompt for model 2, etc. – to promote diversity in model
response (Park et al., 2023; Chuang et al., 2023; Papachristou & Yuan, 2024). While the intended
model response diversity is achieved, previous studies have failed to quantitatively assess the ef-
fect of different model initializations and, instead, rely on qualitative checks. Similarly, analyzing
changes in model responses as the system evolves has previously been limited to human inspection
of responses (Park et al., 2023), or classification of responses into a few classes (Chuang et al.,
2023).
We introduce the perspective space of a collection of models to address the gap in quantitative
methods for studying the diversity and evolution of model responses. The perspective space is an
embedding-based representation of a collection of models designed to capture the relative differ-
ences in model responses for a fixed set of prompts. The method can be used to study information
diffusion and general system dynamics by querying each model with the same set of queries at each
time step. To demonstrate the effectiveness of the perspective space for understanding model-level
diversity and for analyzing model-level and system dynamics, we formalize the system of interact-
ing language models as a graph. The formalization enables systematic study of the effect of different
communication structures on information diffusion that is otherwise not possible.
Our contribution is two-fold: i) We model a system of interacting language models as a graph and
systematically study the effect of different communication structures on information diffusion. ii)
We introduce the perspective space as a method to quantitatively analyze information diffustion in a
population of language models.
2 A COMMUNICATION NETWORK OF LLM S
Consider a system that consists of a collection of language of models F={f1, . . . , f n}and
databases D={D1, . . . , D n′}. Given a set of prompts X, systems deploying model f∈ F may
use the database D∈ D – via fine-tuning, context retrieval, etc. – to produce more relevant outputs
with respect to X. The outputs of the updated model may be used to update a (potentially different)
database D′∈ D. The updated database can then be used as a fine-tuning, retrieval, etc. database
for a (potentially different) model f′∈ F. This set of interactions between a model and a database
may occur across various models and various databases in the system.
As described, this system can be modeled as a graph G= (V, E)where V=F ∪D and the directed
edge (v, v′)is inEif vertex vhas influence on vertex v′. For example, the edge (D, f)exists if fhas
access to Dfor retrieval augmentation or if it can use a subset of Das fine-tuning data. Conversely,
the edge (f, D)exists if the output of fcan influence the content of dataset D.
Our primary interest is the dynamics of a system of interacting LLMs and databases where the vertex
and edge sets are indexed by a discrete variable t∈ {1, . . . , T }. There are many ways components
of the graph may vary in tin such a system. For example, the dataset D(t)∈V(t)may be updated
based on the outputs of the model f(t)∈V(t)or the model f(t)may change after fine-tuning on the
2Figure 2: Two 2-d perspective spaces of fifteen models (5 models each from three classes, encoded
by color). An evaluation set containing prompts relevant to the differences in the models (left) is
better suited to induce a discriminative perspective space than an evaluation set containing “orthog-
onal” prompts.
contents of the dataset D(t). In both cases V(t)̸=V(t+1). Similarly, external factors such as the
terms of use for a dataset may change to disallow its use for retrieval augmentation or a model may
lose write-access to a dataset. In both cases E(t)̸=E(t+1). Figure 1 illustrates simple examples of
systems of LLMs as graphs, including three structures that are studied in the simulated settings in
Section 4.
3 D EFINING A PERSPECTIVE SPACE WITH SURROGATE DATA KERNELS
The system-of-LLMs-as-a-graph perspective provides a framework to systematically study the effect
of different vertex sets and edge structures on the flow of information through the system as a
function of t. The framework does not, however, provide a method to track the information flow.
For this, we introduce an adaptation of the embedding-based data kernel presented in (Duderstadt
et al., 2023). For our purposes, an embedding function gis a mapping to real-valued vectors.
3.1 T HE DATA KERNEL &ITS SURROGATE
We letX={x1, . . . , x m}be a collection of prompts with x∈ X andf(X) ={fθ(x1), . . . , f (xm)}
be the corresponding set of responses with f(x)∈ X′. Given an embedding function giassociated
withfi, recall that the data kernel A(gi,X)of the evaluation dataset Xfrom the perspective of fi
captures the intrinsic geometry of the embedding space with respect to X. The data kernel enables
datum-level and global comparisons of two models with potentially different architectures, sizes,
etc. where direct comparison of gi(X) = [gi(x1), . . . , g i(xm)]⊤∈Rm×pandgj(X)∈Rm×p′is
otherwise not possible.
The methodology can be extended to compare the embedding spaces of multiple models f1, . . . , f n
at once by considering the pairwise distance matrix of the corresponding data kernels. In particu-
lar, the classical multi-dimensional scaling (Torgerson, 1952)) of the n×nmatrix M with entries
Mij=||A(gi,X)−A(gj,X)||Fyields d-dimensional Euclidean representations of the model
fiwith respect to X. After this transformation, inference methods designed for Euclidean objects
can be used for model-level analysis.
The data kernel, as defined in (Duderstadt et al., 2023), requires the model fito have an associated
embedding function gi. Unfortunately, for some state-of-the-art LLMs such as OpenAI’s GPT series,
Anthropic’s Claude series, etc., an associated embedding function is unavailable and the data kernel
cannot be constructed. To rectify this, we replace a model’s associated embedding function with a
3surrogate embedding function ˜g:X′→Rpthat is not necessarily related to any of the LLMs under
study.
The surrogate embedding function is not a drop-and-replace solution for model comparisons, how-
ever, since the embedding ˜g(X)is independent of fi. Instead, we query the model with the elements
ofXand embed the responses fi(X)with ˜g: the surrogate data kernel A(˜g, fi(X))is simply
˜g(fi(X))∈Rm×p.
3.2 T HE PERSPECTIVE SPACE
As with the original data kernel, we can use the surrogate data kernel to compare the responses
from multiple models simultaneously via the CMDS of the pairwise distance matrix ˜Mwith entries
˜Mij=||˜g(fi(X))−˜g(fj(X))||F. We let Zi∈Rddenote the d-dimensional vector representation
offi.
Since the representations Z1, . . . , Z nare a function of the differences in the model responses – or
“perspectives” – f1(X), . . . , f n(X), we refer to the subspace populated by {Z1, . . . , Z n}as the
perspective space ofFwith respect to X. The information that is captured by the perspective
space depends on ˜gandX. In particular, ˜gneeds to be able to distinguish between concepts that are
intended to be distinguished. For example, a random mapping from X′toRpis likely insufficient for
comparing models, general-purpose embedding functions (Reimers & Gurevych, 2019; Nussbaum
et al., 2024) should be sufficient for capturing the majority of signal, and domain-specific embedding
functions (Risch & Krestel, 2019) should be used when the difference in models is highly nuanced.
Similarly, Xshould contain prompts that the models are expected to have meaningfully different
responses. We demonstrate this in Figure 2 where ˜gis fixed, Fconsists of 15 models (5 each
from three different classes) and Xis chosen to be relevant to the difference in classes (left) or
“orthogonal” to the difference in classes (right). The perspective space is more discriminative (i.e.,
the models from a given class cluster better) when Xcontains prompts relevant to the class-wise
differences. More details related to the models shown in the two perspective spaces are provided in
Appendix B.
The perspective space that includes the entire history of a system can be learned by considering the
CMDS of the |F|T× |F|Tpairwise distance matrix with entries ||˜g(f(t)
i(X))−˜g(f(t′)
j(X))||Ffor
alli, j∈ {1, . . . ,|F|} and all t, t′∈ {1, . . . , T }. We use this perspective space when studying
the systems below. The methodology can be extended to instances where only a partial history of
the system is observed via out-of-sample methods (Bengio et al., 2003; Levin et al., 2018).
4 S IMULATING SYSTEMS OF INTERACTING LLM S
We next simulate three different systems of interacting LLMs to demonstrate the effectiveness of the
perspective space and its derivatives for capturing model and system dynamics for different under-
lying communication structures. The initial models in each system are based on an instance of the
410-million parameter model from the Pythia suite (Biderman et al., 2023) that has been instruction-
tuned using Databricks’ Dolly 15k (Conover et al., 2023). For each system we further fine-tune the
base model on random question-pairs from setting specific topics from Yahoo! Answers (YA) dataset
(Zhang et al., 2015) to promote response variation. We provide details on the instruction-tuning of
the base model and the fine-tuning of the initial models in Appendix A and Appendix B, respec-
tively. We use all-MiniLM-L6-v2 , a sentence embedding function from (Reimers & Gurevych,
2019) based on (Wang et al., 2020b) hosted on the HuggingFace Hub (Wolf et al., 2020), as the
surrogate embedding function and the implementation of CMDS from Graspologic (Chung et al.,
2019).
In the three Case Studies (C.S.) we consider, each model interacts with another model in the system
at each t. An interaction consists of model iasking model j̸=ia random set of questions from a
fixed question bank and fine-tuning model iusing the resulting question-answer pairs as fine-tuning
data. For a given t, the underlying communication structure E(t)determines which set of model
interactions are possible for model i. In particular, the interviewed model jis randomly selected
from the set of models such that (fj, fi)∈E(t). The fixed question bank is used as the evaluation
set to induce the perspective space.
4No disruption disruption
01020304050
timeperspective
010 20 30 40 50
timeiso−mirror
No disruption
disruptionFigure 3: Tracking individual perspective (left) and system-level dynamics (right) of communication
networks of chat-based language models with (bottom left) and without (top left) a disruption in
communication structure.
While each system that we study technically consists of models and databases, each dataset is asso-
ciated with only a single model. For convenience we discuss the systems as if the models themselves
are directly connected. Our setting – where models are sequentially trained on each others outputs
without intervention – can be viewed as a generalization of a single model sequentially trained on
its own outputs as studied in (Shumailov et al., 2024).
At the end of each simulation setting we provide examples that motivated the case study.
C.S. 1: D ISRUPTING THE COMMUNICATION NETWORK
We first study a system with |F|= 25 models fine-tuned on different 400 random samples from YA
with topic “Society & Culture” under two different system evolutions. For the first system evolution
1234Number of clustersNo disruption
disruption
0.00.51.0
010 20 30 40 50
timeARI(t, t−1)
Figure 4: Estimated number of clusters found via
GMM with BIC (top) and sequential ARI of clus-
ter labels (bottom) for disrupted and undisrupted
systems. The number of clusters in both systems
stabilize, indicating the presence of model sinks.
Model sinks are unstable in a system with no dis-
ruption and stable in a system with a disruption.the underlying communication structure is un-
restricted (i.e., E(t)fully connected, see Fig-
ure 1 “fully connected”) for all t. For the sec-
ond system evolution the underlying communi-
cation structure is unrestricted for t < t∗and
is then local-only (i.e., (fi, fj)∈E(t)only if
model iis model j’s nearest neighbor in per-
spective space after the interactions at t−1)
thereafter. We refer to the shift from unre-
stricted communication to local communication
as a disruption in the communication structure.
At each time tmodel iasks 50 random ques-
tions from a question bank of 400 questions
from YA with topic “Society & Culture”. The
initial 1-d perspectives of the models are rel-
atively close to each other, as can be seen at
t= 0 in both the top left and bottom left fig-
ures of Figure 3. As the system evolves for
t < t∗, we observe the models “exploring”
the perspective space. For the system that does
not experience a disruption (top left), the explo-
ration in perspective eventually stagnates and
each model appears to oscillate between three
different global perspective “sinks”, one near
the top of the figure, one in the middle of the figure, and one near the bottom of the figure. For
the system that experiences a disruption at t∗= 21 (bottom left) the exploration in perspective
space similarly stops, though the models do not oscillate between global sinks and, instead, persist
in local sinks. The existence of multiple model sinks in both evolutions generalizes the behavior
5observed in (Shumailov et al., 2024), where the sequence of a single model sequentially trained on
its own output converges to a degenerate model sink.
The difference in local and global sinks is quantified in Figure 4, where we report the number of
clusters at each tand the similarity of sequential cluster labels. We use Gaussian Mixture Modeling
with the Bayesian Information Criterion (BIC) to estimate the number of clusters (Fraley & Raftery,
2002) and adjusted Rand index (ARI) to measure cluster label similarity. We find that the number
of clusters for both systems eventually stabilizes and that the ARI between sequential cluster labels
is lower for the global communication network after stabilization, which signifies higher cluster
instability.
We quantify the general evolution of the systems via the “iso-mirror” (Athreya et al., 2022) in the
right figure of Figure 3. The iso-mirror is a system-level summary of the dynamics that takes into
account the collection of model-level dynamics. In our setting, the iso-mirror corresponding to the
system that does not experience a disruption is unstable throughout t. The iso-mirror corresponding
to the disrupted system, however, clearly changes behavior at t∗and remains constant throughout
the remainder of its evolution.
Motivating examples. This case study was largely motivated by the COVID-19 pandemic (Zuzul
et al., 2023) where social distancing, work from home, and social pods changed the latent com-
munication structure for entire communities. It is also relevant to communication networks for
range-limited devices where the definition of “local” depends on the geographical location of the
device (Wang et al., 2020a).
C.S. 2: D IFFUSION OF AN ADVERSARIAL PERSPECTIVE
Figure 5: The evolution of 1-d perspectives of five interacting models where two models interact
with an “adversarial” model every other interaction (top). Given enough nodes to influence, the
adversarial model can compromise the entire network – as captured by the difference between the
average 1-d perspective of the non-adversarial models and the 1-d perspective of the adversarial
model for various amounts of target models and various attack frequencies (bottom).
We next consider a system with |F|= 6models where five of the models are fine-tuned on a random
set of 1000 question-answer pairs from YA with topic “Society & Culture” and the sixth is fine-tuned
on a random set of 1000 question-answer pairs from YA with topic “Science & Mathematics”. We
refer to the model trained on data with topic “Science & Mathematics” as an “adversarial” model
since it does not share the same initial perspective as the other five in expectation. A non-adversarial
model is referred to as a “target” model at time tif there is an edge from the adversarial model
to it in E(t). Target models are randomly selected at the beginning of the evolution of the system
6and remain targets throughout a simulation. The evaluation set consists of 200 questions from the
“Science & Mathematics” topic. At each iteration model iasks model j100 questions.
For this experiment E(t)oscillates between two states. The first is a base state where the non-
adversarial subnetwork is fully connected and there are no edges to or from the adversarial model.
The second is a “vulnerable” state where there is an edge from the adversarial model to all tar-
get models, there are no other in-bound edges to the adversarial or target models, the non-target
non-adversarial subnetwork is fully connected, and there are edges from the target models to the
non-target models (see Figure 1 “vulnerable”). We simulate systems that have a vulnerable commu-
nication network once every two, five or ten iterations.
The trajectories of the 1-d perspectives of the models in the system with a vulnerable communication
every other iteration are shown in the top of Figure 5 for systems with 0, 1, 2 and 5 targets. We also
report the average perspective of the targeted models and the average perspective of the non-targeted
models for each system.
For the system with no targets (top left) we observe similar behavior to the first case study under no
disruption: the models initially explore the perspective space and eventually settle in a model sink.
For the system with a single target we see the targeted model (top center left) oscillate between the
adversarial perspective and the average perspective of the non-targeted models. Non-target models
that interact with the target models immediately after the communication network was vulnerable
are similarly pulled towards the adversarial perspective but to a lesser extent. Together these two
effects limit the perspective exploration of the models in the system and eliminate the presence of
the model sink.
For the system with two targets (top center right) the targeted models oscillate between the adversar-
ial perspective and the average non-target perspective but the oscillations dampen as the non-target
model pespectives start to drift towards the adversarial perspective. By t= 20 the average non-
target perspective is closer to the adversarial perspective than its own starting position. That is, the
entire system of LLMs has been compromised by the adversarial model targeting only a minority of
the models in the system. The average perspective of models in a system with five targets (top right)
quickly approaches the adversarial perspective.
In this setting we summarize system behavior via polarization defined as the difference in the aver-
age perspective of non-adversarial models and the perspective of the adversarial model normalized
by this difference at t= 0. We report the polarization for five initialization for vulnerable com-
munication frequencies of two, five, and ten in the bottom of Figure 5. For example, for an attack
frequency of two we see that polarization neatly summarizes our observations. In particular, the
polarization increases when there are no target models, the polarization is relatively stable when
there is a single target, the polarization slowly drifts towards zero when there are two targets, and
the polarization quickly approaches zero when there are five targets. The system is more susceptible
when more models are targeted for attack frequencies of five and ten, as well.
The trend across attack frequencies for a fixed number of target models indicates that given enough
time between attacks the average model perspective is able to recover. This is likely due to the
interaction mechanic involving a random subset of the evaluation questions – instead of the entire
set – that enables system-level perspective homeostasis.
Motivating example. This case study was designed to mimic information diffusion in the presence
of simple propaganda machines and to study how “attacks” on a minority affects the entire system.
C.S. 3: M ITIGATING OR PROMOTING POLARIZATION
In our last case study we consider a system of |F|= 10 models where five of the models are
fine-tuned on 1000 random question-answer pairs from YA with topic “Society & Culture” and the
other five are fine-tuned on 1000 random question-answer pairs from YA with topic “Science &
Mathematics” . We let the topic in which the fine-tuning data is sampled from parameterize model
“class”. The evaluation set consists of 200 questions from each class. An interaction consists of
model iasking model j100 questions.
In this experiment we consider two different communication structures: unrestricted communica-
tion where E(t)is fully connected and intra-class only communication where E(t)consists of two
7Figure 6: The evolution of 1-d perspective space representations of ten models from two classes
under different underlying communication structures – unrestricted (left, top) and intra-class only
(left, bottom). Class-wise average 1-d perspectives (bolded) are intertwined throughout the evolution
of the system with unrestricted communication and diverge with intra-class only communication.
Polarization captures this difference in behavior over multiple iterations of the experiment (right).
unconnected class-wise fully connected subnetworks (see Figure 1 “intra-class only”). A system has
the same communication structure for the entirety of its evolution. The top left figure of Figure 6
shows 1-d perspectives of the models in the system with unrestricted communication. Bolded lines
represent the class average. As with fully connected communication network settings in the other
case studies, we observe a period of perspective exploration before stabilizing. Notably, the two
class-means stay intertwined throughout the entirety of the evolution of the system.
The bottom left figure of Figure 6 shows the evolution of 1-d perspectives with intra-class only com-
munication. Under the intra-class only regime we see that the two classes explore different regions of
the perspective space and eventually settle into two sinks with a much greater distance between them
then the class-wise differences at t= 0. The polarization of the class-wise averages captures the
distancing of the perspective “echo chambers”, as reported in the right figure of Figure 6. Indeed, the
polarization increased by 15x on average over four different simulation initializations under intra-
class only communication. Average polarization is near zero by the end of the simulations under
unrestricted communication.
Motivating example. This case study was designed to investigate the effect of extreme underlying
communication networks on two party systems.
5 R ELATED WORK
Our work is closely related to simulating groups of computational agents to study sociological and
cultural phenomena (Steels, 1990; Wagner et al., 2003) and to continual learning (V ogelstein et al.,
2020; Geisa et al., 2021). The former has seen renewed interest with the recent successes of LLMs.
In particular, LLMs are – as of this writing – the computational tool that produces language artifacts
most similar to ours and, as such, are an intriguing prospect for multi-agent sociological and cultural
simulations. Recent work has included objective-less behavioral studies (Park et al., 2023), studying
the formation of social networks (Papachristou & Yuan, 2024), tracking opinion dynamics via classi-
fication of LLM response (Chuang et al., 2023), and analyzing document collaboration (Perez et al.,
2024). Our work extends these by introducing a framework to systematically study interventions
and by introducing a quantitative method for tracking the evolution of agent perspectives.
Continual learning (Thrun, 1995; 1998) is largely concerned with how a single agent adapts to pre-
viously unseen inference tasks while avoiding “catastrophically forgetting” (McCloskey & Cohen,
1989; Kirkpatrick et al., 2017) previous tasks. Our setting is slightly different, since we have mul-
tiple agents and no explicit task – though a large movement in perspective space is likely highly
correlated to change in performance on language benchmarks related to the evaluation set. Indeed,
large enough movements in perspective space and the emergence of model sinks when training a
model recursively is related to catastrophic forgetting (Shumailov et al., 2024).
86 C ONCLUSION
We introduced a system-of-LLMs-as-a-graph to enable systematic interventions to a system of in-
teracting LLMs and the perspective space to quantitatively study the corresponding evolution of
the system. We used these tools to highlight differences in paired systems across three case stud-
ies. For the particular interaction mechanic and update function that we used in our simulations,
the model behaviors in perspective space consistently demonstrated initial model exploration and,
in most cases, the emergence and persistence of model sinks. Further, we used derivatives of the
perspective space such as the iso-mirror, polarization, and clustering to highlight differences in the
evolution of paired systems.
For example, we observed differences in the iso-mirror (stable versus unstable after disruption) and
clustering (global sinks versus local sinks after disruption) in the first case study; differences in the
sensitivity of the average perspective of non-adversarial models to an adversarial perspective across
number of victims and frequency of attack in the second case study; and differences in the behavior
of polarization of two classes of models in the third case study.
7 L IMITATIONS
A system of interacting language models is a complicated system and, as such, analysis of them
will often require simplification of aspects of the system. Our case studies are no expection. For
example, the interaction mechanic (i.e., each model interacts with exactly one of its neighbors at
timet) and update function (i.e., update model weights via fine-tuning) used in the simulations are
more proof-of-concept than final-product in that they do not reflect our beliefs on how individuals
within a community interact or “update” themselves, nor are currently deployed models constantly
updated. While we do not attempt to enumerate all possible improvements here, we believe that
it is imperative to work closely with social and cognitive scientists to understand the appropriate-
ness of considering systems of LLMs as a proxy for human communities or online forums before
generalizing observed simulated behavior to human-facing communities. Future work along these
lines will include two major fronts: i) designing comprehensive statistical frameworks to understand
the appropriateness of using a system of interacting LLMs as a proxy for various social settings
and ii) extending simulation settings to include more sociologically plausible interaction and update
mechanics.
Further, the simulation studies herein are but three system configurations worth considering. Indeed,
of immediate interest is an extension to hierarchical social structures observed in large commercial
and government institutions where the perspective space can be used to understand the effect of
information injection, re-organizations, third-party seminars, etc. on individual-level, team-level,
and organization-level dynamics.
There are also limitations related to the analysis of each of the three case studies we presented. For
example, for the first case study we only investigated the difference between system behavior of
global communication and global to hyper-local communication. More nuanced investigations into
the effect of the number of models, the effect of the initializations of the models, the effect of the
definition of “local”, etc. is necessary to understand how the empirical observations may generalize
to the real world. Similarly, for the second case study we only considered a single static adversarial
model. A more realistic simulation may include multiple dynamic adversarial models. For the third
case study, if this analysis is to be used to understand polarization of political parties, it is necessary
to understand the effect of cross-party communication, however rare it may be. We, again, believe
that it is necessary to comprehensively explore each of these experiments before making claims
about its applicability to society and human-model forums.
Lastly, we introduce the perspective space and demonstrate that it is sensitive to evaluation set. We
do not, however, comprehensively explore or discuss potential applications or alternative model-
based similarities. Similar methods have been used We expect the perspective space to be useful for
various model-level inference tasks, as similar methods have been successfully used for classifica-
tion (Chen et al., 2022) and change-point detection (Chen et al., 2023) in neuroscience applications.
We also expect the model-based similarity most effective for capturing model differences will be
system and task dependent (Eaton et al., 2008; Zamir et al., 2018; Helm et al., 2020).
9Acknowledgements. We would like to thank Avanti Athreya, Henry Farrell, Hahrie Han, Teresa
Huang, Vince Lyzinski, Harvey McGuinness, and Tim Wang for their helpful feedback and discus-
sions throughout the development of this manuscript.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical
report. arXiv preprint arXiv:2303.08774 , 2023.
Avanti Athreya, Zachary Lubberts, Youngser Park, and Carey E Priebe. Discovering underlying
dynamics in time series of networks. arXiv preprint arXiv:2205.06877 , 2022.
Yoshua Bengio, Jean-franc ¸cois Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Roux, and
Marie Ouimet. Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clus-
tering. Advances in neural information processing systems , 16, 2003.
Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric
Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.
Pythia: A suite for analyzing large language models across training and scaling. In International
Conference on Machine Learning , pp. 2397–2430. PMLR, 2023.
Levin Brinkmann, Fabian Baumann, Jean-Franc ¸ois Bonnefon, Maxime Derex, Thomas F. M ¨uller,
Anne-Marie Nussberger, Agnieszka Czaplicka, Alberto Acerbi, Thomas L. Griffiths, Joseph Hen-
rich, Joel Z. Leibo, Richard McElreath, Pierre-Yves Oudeyer, Jonathan Stray, and Iyad Rah-
wan. Machine culture. Nature Human Behaviour , 7(11):1855–1868, November 2023. ISSN
2397-3374. doi: 10.1038/s41562-023-01742-2. URL http://dx.doi.org/10.1038/
s41562-023-01742-2 .
Guodong Chen, Hayden S Helm, Kate Lytvynets, Weiwei Yang, and Carey E Priebe. Mental state
classification using multi-graph features. Frontiers in Human Neuroscience , 16:930291, 2022.
Tianyi Chen, Youngser Park, Ali Saad-Eldin, Zachary Lubberts, Avanti Athreya, Benjamin D
Pedigo, Joshua T V ogelstein, Francesca Puppo, Gabriel A Silva, Alysson R Muotri, et al. Dis-
covering a change point in a time series of organoid networks via the iso-mirror. arXiv preprint
arXiv:2303.04871 , 2023.
Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang,
Dhavan Shah, Junjie Hu, and Timothy T Rogers. Simulating opinion dynamics with networks of
llm-based agents. arXiv preprint arXiv:2311.09618 , 2023.
Jaewon Chung, Benjamin D Pedigo, Eric W Bridgeford, Bijan K Varjavand, Hayden S Helm, and
Joshua T V ogelstein. Graspy: Graph statistics in python. Journal of Machine Learning Research ,
20(158):1–7, 2019.
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick
Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first truly open
instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/
12/dolly-first-open-commercially-viable-instruction-tuned-llm .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Brandon Duderstadt, Hayden S Helm, and Carey E Priebe. Comparing foundation models using
data kernels. arXiv preprint arXiv:2305.05126 , 2023.
Eric Eaton, Marie Desjardins, and Terran Lane. Modeling transfer relationships between learn-
ing tasks for improved inductive transfer. In Machine Learning and Knowledge Discovery in
Databases: European Conference, ECML PKDD 2008, Antwerp, Belgium, September 15-19,
2008, Proceedings, Part I 19 , pp. 317–332. Springer, 2008.
10Chris Fraley and Adrian E Raftery. Model-based clustering, discriminant analysis, and density esti-
mation. Journal of the American Statistical Association , 97(458):611–631, 2002. doi: 10.1198/
016214502760047131. URL https://doi.org/10.1198/016214502760047131 .
Ali Geisa, Ronak Mehta, Hayden S Helm, Jayanta Dey, Eric Eaton, Jeffery Dick, Carey E Priebe,
and Joshua T V ogelstein. Towards a theory of out-of-distribution learning. arXiv preprint
arXiv:2109.14501 , 2021.
Hayden Helm, Carey E Priebe, and Weiwei Yang. A statistical turing test for generative models.
arXiv preprint arXiv:2309.08913 , 2023.
Hayden S Helm, Ronak D Mehta, Brandon Duderstadt, Weiwei Yang, Christoper M White, Ali
Geisa, Joshua T V ogelstein, and Carey E Priebe. A partition-based similarity for classification
distributions. arXiv preprint arXiv:2011.06557 , 2020.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature , 596(7873):583–589, 2021.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences ,
114(13):3521–3526, 2017.
Michal Kosinski. Evaluating large language models in theory of mind tasks. arXiv e-prints , pp.
arXiv–2302, 2023.
Keith Levin, Fred Roosta, Michael Mahoney, and Carey Priebe. Out-of-sample extension of graph
adjacency spectral embedding. In International Conference on Machine Learning , pp. 2975–
2984. PMLR, 2018.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation , volume 24, pp. 109–165.
Elsevier, 1989.
Zach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic embed: Training
a reproducible long context text embedder, 2024.
Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov,
Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning
robust visual features without supervision. arXiv preprint arXiv:2304.07193 , 2023.
Marios Papachristou and Yuan Yuan. Network formation and dynamics among multi-llms, 2024.
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and
Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings
of the 36th Annual ACM Symposium on User Interface Software and Technology , pp. 1–22, 2023.
J´er´emy Perez, Corentin L ´eger, Marcela Ovando-Tellez, Chris Foulon, Joan Dussauld, Pierre-Yves
Oudeyer, and Cl ´ement Moulin-Frier. Cultural evolution in populations of large language models,
2024.
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.
Robust speech recognition via large-scale weak supervision. In International Conference on Ma-
chine Learning , pp. 28492–28518. PMLR, 2023.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
networks. arXiv preprint arXiv:1908.10084 , 2019.
Julian Risch and Ralf Krestel. Domain-specific word embeddings for patent classification. Data
Technologies and Applications , 53(1):108–122, 2019.
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson.
The curse of recursion: Training on generated data makes models forget, 2024.
11Luc Steels. Cooperation between distributed agents through self-orcamsation. In Proceedings of the
first European workshop on modelling autonomous agents in a multi-agent world . Citeseer, 1990.
Sebastian Thrun. Is learning the n-th thing any easier than learning the first? Advances in neural
information processing systems , 8, 1995.
Sebastian Thrun. Lifelong learning algorithms. In Learning to learn , pp. 181–209. Springer, 1998.
Warren S Torgerson. Multidimensional scaling: I. theory and method. Psychometrika , 17(4):401–
419, 1952.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Joshua T V ogelstein, Hayden S Helm, Ronak D Mehta, Jayanta Dey, Weiwei Yang, Bryan Tower,
Will LeVine, Jonathan Larson, Chris White, and Carey E Priebe. A general approach to progres-
sive learning. Preprint at https://arxiv. org/abs/2004.12908 , 2020.
Kyle Wagner, James A Reggia, Juan Uriagereka, and Gerald S Wilkinson. Progress in the simulation
of emergent communication and language. Adaptive Behavior , 11(1):37–69, 2003.
Fangxin Wang, Miao Zhang, Xiangxiang Wang, Xiaoqiang Ma, and Jiangchuan Liu. Deep learning
for edge computing applications: A state-of-the-art survey. IEEE Access , 8:58322–58336, 2020a.
doi: 10.1109/ACCESS.2020.2982411.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-
attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neu-
ral Information Processing Systems , 33:5776–5788, 2020b.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-
of-the-art natural language processing, 2020.
Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio
Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pp. 3712–3722, 2018.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. Advances in neural information processing systems , 28, 2015.
Tiona Zuzul, Emily Cox Pahnke, Jonathan Larson, Patrick Bourke, Nicholas Caurvina, Neha Parikh
Shah, Fereshteh Amini, Jeffrey Weston, Youngser Park, Joshua V ogelstein, Christopher White,
and Carey E. Priebe. Dynamic silos: Increased modularity in intra-organizational communication
networks during the covid-19 pandemic, 2023.
12A I NSTRUCTION -TUNING PYTHIA -410 M-DEDUPED
The base model that we used in the case studies in Section 4 was an instruction-tuned version of the
410 million parameter model from the Pythia suite (Biderman et al., 2023). For instruction-tuning,
we added three special tokens to its tokenizer’s vocabulary, “### End”, “### Instruction:”, and “###
Response:”, and fine-tuned the model with a subset of Databricks’ Dolly 15k (Conover et al., 2023).
Each datum consists of an instruction, context, response, and category. We kept only data in the
Open QA, Brainstorm, General QA, and Creative Writing categories and that had a response length
less than 100 characters. This filtering left us with 1559 instruction-response pairs. We formatted a
particular example as follows:
### Instruction: {instruction }
### Response: {response }
### End
We fine-tuned the model on the formatted data using Adam with a learning rate of 5×10−5and a
batch size of 8 for 10 epochs. The final cross-entropy loss on the training data was ≈0.26.
B C ASE-STUDY SPECIFIC FINE -TUNING
For each of the case studies we further fine-tuned the instruction-tuned base model to promote
response variation. For this, we used the data from the Yahoo! Answers (YA) dataset introduced
in (Zhang et al., 2015), where each datum consists of a topic, a question title, question content, a
list of answers, and a best answer. Given data from a particular topic, we further filtered the data
by considering only examples with best answers less than 200 characters, with best answers that
contained only a single sentence, and with question titles that contained only a single question. We
formatted data from YA as follows:
### Instruction: {question title }
### Response: {best answer }
### End
Unless otherwise specified, fine-tuning is done using Adam with a learning rate of 5×10−5. The
initial models were trained for 3 epochs. The model updates after an interaction consisted of only a
single epoch with a learning rate of 10−5.
B.1 C ASE STUDY 1: S TOCHASTICALLY EQUIVALENT MODELS
For case study 1, we randomly selected 400 examples with the topic “Society & Culture” that we
used as both the evaluation set in the experiment and as a pool of data used for further sampling. In
particular, we randomly sampled 200 samples from the set of 400 25 times and used the 25 subsets
as fine-tuning data for different “stochastically equivalent” models.
B.2 C ASE STUDIES 2 & 3: T WO CLASSES
For case studies 2 & 3, we considered filtered data from topics “Society & Culture” and “Science &
Mathematics”. For each topic we randomly sampled 1000 examples 10 times to use for fine-tuning.
For case study 2, we randomly selected a single model fine-tuned on “Science & Mathematics” to
be the adversarial model. This model was the adversarial model for all system instances. We then
randomly selected 5 models fine-tuned on “Society & Culture” data to be non-adversarial models.
The non-adversarial models changed with each system instance.
For case study 3, we randomly selected 5 models from each class for every system instance.
13