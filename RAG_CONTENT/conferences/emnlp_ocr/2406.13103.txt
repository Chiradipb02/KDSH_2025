A Generic Method for Fine-grained Category Discovery in Natural
Language Texts
Chang Tian†, Matthew B. Blaschko†, Wenpeng Yin, Mingzhe Xing,
Yinliang Yue ,Marie-Francine Moens†
†KU Leuven
chang.tian@kuleuven.be
Abstract
Fine-grained category discovery using only
coarse-grained supervision is a cost-effective
yet challenging task. Previous training methods
focus on aligning query samples with positive
samples and distancing them from negatives.
They often neglect intra-category and inter-
category semantic similarities of fine-grained
categories when navigating sample distribu-
tions in the embedding space. Furthermore,
some evaluation techniques that rely on pre-
collected test samples are inadequate for real-
time applications. To address these shortcom-
ings, we introduce a method that successfully
detects fine-grained clusters of semantically
similar texts guided by a novel objective func-
tion. The method uses semantic similarities in
a logarithmic space to guide sample distribu-
tions in the Euclidean space and to form distinct
clusters that represent fine-grained categories.
We also propose a centroid inference mecha-
nism to support real-time applications. The
efficacy of the method is both theoretically jus-
tified and empirically confirmed on three bench-
mark tasks. The proposed objective function
is integrated in multiple contrastive learning
based neural models. Its results surpass exist-
ing state-of-the-art approaches in terms of Ac-
curacy, Adjusted Rand Index and Normalized
Mutual Information of the detected fine-grained
categories. Code and data will be available at
https://github.com/XX upon publication.
1 Introduction
Fine-grained analysis has drawn much attention
in many artificial intelligence fields, e.g., Com-
puter Vision (Chen et al., 2018; Wang et al., 2024a;
Park and Ryu, 2024) and Natural Language Pro-
cessing (Ma et al., 2023; Tian et al., 2024; An
et al., 2024), because it can provide more detailed
features than coarse-grained data. For instance,
as illustrated in Figure 1, solely based on coarse-
grained analysis, the chatbot might incorrectly rec-
ommend a roadster, which is unsuitable for field
Root
V ehicle Arts
Movie Music Roadster Off-roadI want to buy a vehicle
suited for weekend
ﬁeld adventures.
Intent: Buy_V ehicle
I suggest purchasing a
Porsche roadster .
Intent: Buy_Off-road
 I recommend buying
an off-road vehicle.Figure 1: A fine-grained intent detection example. Left:
This panel illustrates the label hierarchy, transitioning
from coarse-grained to fine-grained granularity. Right :
This example demonstrates intent detection in a conver-
sation about car choices, showing how coarse-grained
analysis alone can lead to incorrect recommendations
by a life assistant due to a lack of fine-grained analysis.
adventures. Detecting the fine-grained intent would
allow the chatbot to recommend an off-road vehicle
that aligns with the user’s requirements. However,
annotating fine-grained categories can be labor-
intensive, as it demands precise expert knowledge
specific to each domain and involved dataset. Ad-
dressing this challenge, An et al. (2022) recently
introduced Fine-grained Category Discovery under
Coarse-grained Supervision ( FCDC ) for language
classification tasks (details in Section 3). FCDC
aims to reduce annotation costs by leveraging the
relative ease of obtaining coarse-grained annota-
tions, without requiring fine-grained supervisory
information. This approach has sparked significant
research interest in the automatic discovery of fine-
grained language categories. (Ma et al., 2023; An
et al., 2023a; Vaze et al., 2024; Lian et al., 2024).
Existing methods for addressing FCDC are typi-
cally grouped into three groups (An et al., 2024):
language models, self-training methods, and con-
trastive learning methods. Language models (De-
vlin et al., 2019a; Touvron et al., 2023), including
their fine-tuned versions with coarse labels, gener-arXiv:2406.13103v1  [cs.AI]  18 Jun 2024ally perform poorly on this task due to a lack of fine-
grained supervision. Self-training methods (Caron
et al., 2018; Zhang et al., 2021) and their variants of-
ten employ clustering assignments as fine-grained
pseudo labels, filtering out some noisy pseudo la-
bels, and training with these labels. Dominant
contrastive learning methods (Chen et al., 2020;
Mekala et al., 2021; An et al., 2022, 2023a) typi-
cally identify positive and negative samples for a
given query by measuring their semantic distances.
The contrastive loss ensures that the query sample
moves closer to positive samples and further away
from negative samples. So these methods form
clusters of samples in the embedding space, with
each cluster representing a discovered fine-grained
category, without requiring fine-grained category
supervision.
However, past methods did not utilize compre-
hensive semantic similarities (CSS) in the loga-
rithmic space to guide sample distributions in Eu-
clidean space. We define CSS as the fine-grained
semantic similarities measured by bidirectional KL
divergence in the logarithmic space between the
query sample and each available positive or nega-
tive sample, as illustrated in Figure 2. Although An
et al. (2024) recently explored similarities mea-
sured by rank order between the query sample and
positive samples, they ignore similarities with neg-
ative samples.
We propose a method ( STAR ) for detecting
fine-grained clu sters of semantically simil artexts
through a novel objective function, with the core
component considering CSS. This component
guides sample distributions in the Euclidean space
based on the magnitude of CSS in the logarithmic
space. Large semantic differences (low similarity)
in the logarithmic space between the query sample
and an available sample push the query sample fur-
ther away in Euclidean space, while small semantic
differences bring the query sample closer to the
available sample. Thus, samples form distinguish-
able fine-grained clusters in Euclidean space, with
each cluster representing a discovered category.
Additionally, clustering inference used by previ-
ous works (An et al., 2022, 2023a, 2024) can not
support real-time scenarios, so we propose a variant
inference mechanism utilizing approximated fine-
grained cluster centroids, delivering competitive
results for the tasks considered.
Our main contributions in this work can be sum-
marized as follows:
query
samplepositive
samples
negative
samplesFigure 2: Visualization of comprehensive semantic sim-
ilarities (CSS). The wavy line indicates the bidirectional
KL divergence between two samples.
•Method: STAR enhances existing contrastive
learning methods by leveraging comprehen-
sive semantic similarities in a logarithmic
space to guide sample distributions in the Eu-
clidean space, thereby making fine-grained
categories more distinguishable.
•Theory: We interpret STAR from the perspec-
tives of clustering and generalized Expecta-
tion Maximization (EM). Also, we conduct
loss and gradient analyses to explain the effec-
tiveness of using CSS for category discovery.
•Experiments: Experiments on three text
classification tasks (intent detection (Lar-
son et al., 2019), scientific abstract classi-
fication (Kowsari et al., 2017), and chatbot
query (Liu et al., 2021)) demonstrate new
state-of-the-art (SOTA) performance com-
pared to 22 baselines, validating the theoreti-
cal method.
2 Related Work
2.1 Fine-grained Category Discovery
Fine-grained data analysis is crucial in Natural Lan-
guage Processing (Guo et al., 2021; Ma et al., 2023;
Tian et al., 2024) and Computer Vision (Pan et al.,
2023; Wang et al., 2024b). However, effectively
discovering fine-grained categories from coarse-
grained ones remains challenging (Mekala et al.,
2021). Traditional category discovery methods of-
ten assume that known and discovered categories
are at the same granularity level (An et al., 2023b;
Vaze et al., 2024).
To discover fine-grained categories under the su-
pervision of coarse-grained categories, (An et al.,
2022) introduced the FCDC task. Self-training ap-
proaches, such as Deep Cluster (Caron et al., 2018;
An et al., 2023a), use clustering algorithms to de-
tect the fine-grained categories, assign pseudo la-
bels to the clusters and their samples, and then traina classification model with these pseudo labels.
Its variant, Deep Aligned Clustering (Zhang et al.,
2021), devises a strategy to filter out inconsistent
pseudo-labels during clustering. Contrastive learn-
ing has become prevalent in FCDC tasks; (Bukchin
et al., 2021; An et al., 2022) developed angular
contrastive learning tailored for fine-grained clas-
sification. An et al. (2022) proposed a weighted
self-contrastive framework to enhance the model’s
discriminative capacity for coarse-grained samples.
(Ma et al., 2023) and (An et al., 2023a) used noisy
fine-grained centroids and retrieved neighbors as
positive pairs, respectively, applying constraints
to filter noise. An et al. (2024) advanced this ap-
proach with neighbors that are manually weighted
as positive pairs. However, previous efforts have
not leveraged comprehensive semantic similarities
to guide sample distributions and thereby to en-
hance fine-grained category discovery.
2.2 Neighborhood Contrastive Learning
Contrastive learning enhances representation learn-
ing by bringing the query sample closer to posi-
tive samples and distancing it from negative sam-
ples (Chen et al., 2020). Prior research has focused
on constructing high-quality positive pairs. (He
et al., 2020) utilized two different transformations
of the same input as query and positive sample,
respectively. Li et al. (2020) introduced the use of
prototypes, derived through clustering, as positive
instances. Additionally, An et al. (2022) employed
shallow-layer features from BERT as positive sam-
ples and introduced a weighted contrastive loss.
This approach primarily differentiates data at a
coarse-grained level, and the manually set weights
limit its broader applicability.
To circumvent complex data augmentation,
neighborhood contrastive learning (NCL) was de-
veloped, treating the nearest neighbors of queries
as positive samples (Dwibedi et al., 2021). Zhong
et al. (2021) extended this by utilizing k-nearest
neighbors to identify hard negative samples, while
Zhang et al. (2022) selected a positive key from
the k-nearest neighbors for contrastive represen-
tation learning. However, these approaches often
deal with noisy nearest neighbors that include false-
positive samples. (An et al., 2023a) addressed this
by proposing three constraints to filter out uncer-
tain neighbors, yet they overlooked semantic simi-
larities between query sample and each available
sample. An et al. (2024) represented semantic sim-
ilarities using rank order among positive samplesbut neglected similarities among negative samples.
In contrast, STAR uses comprehensive semantic
similarities to guide sample distributions in the Eu-
clidean space, offering richer features and a supe-
rior approach to pure contrastive learning.
3 Problem Formulation
Given a set of coarse-grained categories Ycoarse =
{C1, C2, . . . , C M}and a coarsely labeled training
setDtrain ={(xi, ci)|ci∈Ycoarse}N
i=1, where
Ndenotes the number of training samples, the task
of FCDC involves developing a feature encoder Fθ.
This encoder maps samples into a feature space,
further segmenting them into distinct fine-grained
categories Yfine={F1, F2, . . . , F K}, without any
fine-grained supervisory information. Here, Yfine
represents sub-classes of Ycoarse . Model effective-
ness is evaluated on a testing set Dtest={(xi, yi)|
yi∈Yfine}L
i=1, with Las the number of test sam-
ples, utilizing features extracted by Fθ. For evalua-
tion consistency and fairness, only the number of
fine-grained categories Kis used, aligning with
methodologies established in previous research
(Ma et al., 2023; An et al., 2022, 2023a).
4 Method
STAR leverages comprehensive semantic similar-
ities and integrates seamlessly with contrastive
learning baselines by modifying the objective func-
tion. We have developed variants for three base-
lines: PseudoPrototypicalNet (PPNet) (Boney and
Ilin, 2017; Ji et al., 2020), DNA (An et al., 2023a),
and DOWN (An et al., 2024). This section focuses
on STAR-DOWN because DOWN outperforms
other baselines, with additional method variants
detailed in Appendix
DOWN involves three steps: pre-training with
coarse-grained labels (Section 4.1), retrieving and
weighting nearest neighbors (Section 4.2), and
training with a contrastive loss. STAR-DOWN
follows the same first two steps but replaces the
third with a novel objective function (Section 4.3).
Like DOWN, STAR-DOWN iterates the last two
steps until the unsupervised metric, the silhouette
score of the clustering into fine-grained clusters,
does not improve for five consecutive epochs. The
detailed algorithm is provided in Appendix
4.1 Multi-task Pre-training
As illustrated in Figure 3, the baseline DOWN (An
et al., 2024) utilizes the BERT Encoder Fθto ex-Input Data
Can you move to the next song?
Where is my delivery order?
Will you tell me my bank balance?Encoder
Momentum
EncoderMomentum
updateQuery Set
Dynamic Data 
QueueNearest Neighbor  Set
Neighbor
weighting
No  gradientKL DivergenceQuery
Data queueComprehensive
semantic similarities
TrainingFigure 3: STAR-DOWN integrates the baseline DOWN with the STAR method (shown in the red dashed box). In the
visual representation, colors differentiate samples, squares represent features extracted by the Encoder, and circles
denote features extracted by the Momentum Encoder. Unidirectional arrows indicate proximity, while bidirectional
arrows signify distance between samples.
tract normalized feature embeddings qi=Fθ(xi)
for input xi, where θrepresents the Encoder pa-
rameters. To ensure effective initialization for fine-
grained training, DOWN pre-trains the Encoder
on the coarsely labeled train set Dtrain with la-
belsYcoarse . DOWN utilizes the sum of a cross-
entropy loss Lceand a masked language modeling
lossLmlmfor multi-task pre-training of the Encoder
(detailed in Appendix
4.2 Neighbors Retrieval and Weighting
In Figure 3, the Momentum Encoder Fθkwith pa-
rameters θkextracts and stores gradient-free nor-
malized neighbor features hi=Fθk(xi)in a dy-
namic data queue Q. To ensure consistency be-
tween the outputs of FθkandFθ,Fθk’s parame-
ters are updated via a moving-average method (He
et al., 2020): θk←mθk+ (1−m)θ, where
mis the momentum coefficient. For each query
feature qi, in order to facilitate semantic similar-
ity capture and fine-grained clustering, its top-k
nearest neighbors Niare determined from Qus-
ing cosine similarity (Sim): Ni={hj|hj∈
argtopKhl∈Q(Sim( qi, hl))}, where Sim(qi, hl) =
qT
ihl
∥qi∥·∥hl∥is the cosine similarity function.
To counteract potential false positives in Ni,
DOWN utilizes a soft weighting mechanism based
on neighbor rank to balance information utility
against noise, with weights ωjof neighbor hjcalcu-
lated as: ωj=ϕ·α−lij
k, where ϕis a normalizing
constant for weights, αserves as the exponential
base, kis the retrieved neighbor count, and lijde-
notes the rank of hjas a neighbor to qi.To align with the model’s evolving accuracy in
neighbor retrieval during training, DOWN periodi-
cally decreases αevery five epochs, the values for
αinωjare:αset={150,10,5,2}. The ωjof each
positive sample hjis used in Eqs. 3 and 4.
4.3 Training
4.3.1 Objective Function
Given a training batch Ntrain∈Dtrain, where
Ycis the set of coarse-grained labels of Ntrain,
DOWN trains the model using the loss:
Ltrain=Lce+LDOWN , (1)
LDOWN =1
|Ntrain|X
qi∈NtrainLi
1. (2)
As shown in Eq. 3, DOWN uses a conventional
contrastive objective function in the Euclidean
space, while STAR-DOWN introduces a novel ob-
jective function in Eq. 4, leveraging CSS in a loga-
rithmic space to guide sample distributions in the
Euclidean space, the temperature τis a fixed con-
stant in Eq. 3 and Eq. 4:
Li
1=−X
hj∈Niωj·logexp(qT
ihj/τ)P
hk∈Qexp(qT
ihk/τ).(3)
Li
2=−γX
hj∈Niωj·logexp(−dKL(qi, hj)/τ)P
hk∈Qexp(−dKL(qi, hk)/τ)
−X
hj∈Niωj·logexp(qT
ihj/τ)P
hk∈QBdKL(qi,hk)·exp(qT
ihk/τ).
(4)During training, STAR-DOWN optimizes the
following objective function:
Ltrain=Lce+LSTAR, (5)
LSTAR =1
|Ntrain|X
qi∈NtrainLi
2. (6)
As shown in Eq. 4, the term dKL(qi, hk)inLi
2
represents the bidirectional KL divergence in a log-
arithmic space between the query sample embed-
dingqiand the data queue sample embedding hk
(detailed in Appendix Bis a trainable scalar repre-
senting the exponential base.
The first term in Li
2minimizes the KL diver-
gence between query samples and positive samples
(retrieved neighbors in Section 4.2) while increas-
ing it for negative samples in the logarithmic space,
withγas a balancing hyperparameter. The sec-
ond term in Li
2uses CSS in the logarithmic space,
denoted by BdKL(qi,hk), to guide query sample dis-
tribution in the Euclidean space. qT
ihkquantifies
the cosine similarity between normalized qiand
hk, equivalent to the negative Euclidean distance
(detailed in Appendix . The value of the trainable
scalar Bis updated during loss backpropagation, so
BdKL(qi,hk)is fully trainable and can integrate with
contrastive learning methods, making the STAR
method generic .
4.3.2 Loss Analysis
Since STAR-DOWN discovers fine-grained cate-
gories in the Euclidean space, we analyze the sec-
ond term Li
2−2of the loss Li
2, which optimizes
sample distributions in the Euclidean space:
Li
2−2=−X
hj∈Niωj·logexp(qT
ihj/τ)P
hk∈QBdKL(qi,hk)·exp(qT
ihk/τ)
=X
hj∈Niωj·(logX
hk∈QBdKL(qi,hk)·exp(qT
ihk/τ)
−(qT
ihj/τ)).
(7)
In the loss Li
2−2,BdKL(qi,hk)uses CSS in the
logarithmic space to guide sample distributions in
the Euclidean space. A large dKL(qi, hk)(low se-
mantic similarity) causes qito distance itself from
hkin the Euclidean space, reducing qT
ihk, while
a small dKL(qi, hk)allows qito remain relatively
close to hkcompared to negative samples. This
results in the formation of compact fine-grained
clusters, with each cluster representing a discov-
ered category. We also analyze the STAR methodDataset |C| |F| # Train # Test
CLINC 10 150 18000 1000
WOS 7 33 8362 2420
HWU64 18 64 8954 1031
Table 1: Statistics of datasets (An et al., 2023a). #:
number of samples. |C|: number of coarse-grained cate-
gories. |F|: number of fine-grained categories.
from the perspectives of gradient ,clustering , and
generalized EM . Detailed analyses are provided
in Appendix
4.4 Inference
Previous methods (An et al., 2023a, 2024) use clus-
tering inference on sample embeddings from Fθ
extracted from Dtest, which is unsuitable for real-
time tasks, such as intent detection, which require
immediate response and can not wait to collect
enough test samples for clustering. We introduce
an alternative, centroid inference, suitable for both
real-time and other contexts. Using Fθ, we de-
rive sample embeddings from Dtrain and assign
fine-grained pseudo labels through clustering. For
each fine-grained cluster, only the embeddings of
samples from the predominant coarse-grained cat-
egory (the category with the most samples in this
fine-grained cluster) are averaged to form centroid
representations. These approximated centroids are
used to determine the fine-grained category of each
test sample based on cosine similarity. A visual
explanation is in Appendix
5 Experiments
5.1 Experimental Settings
5.1.1 Datasets
We conduct experiments on three bench-
mark datasets: CLINC (Larson et al., 2019),
WOS (Kowsari et al., 2017), and HWU64 (Liu
et al., 2021). CLINC is an intent detection dataset
spanning multiple domains. WOS is used for paper
abstract classification, and HWU64 is designed for
assistant query classification. Dataset statistics are
provided in Table 1.
5.1.2 Baselines for Comparison
We compare our methods against the following
baselines. Language models : BERT (Devlin
et al., 2019b), BERT with coarse-grained fine-
tuning, Llama2 (Touvron et al., 2023), Llama2
with coarse-grained fine-tuning and GPT4 (Achiamet al., 2023). Self-training baselines: DeepClus-
ter (DC) (Caron et al., 2018), DeepAlignedClus-
ter (DAC) (Zhang et al., 2021), and PseudoPro-
totypicalNet (PPNet) (Boney and Ilin, 2017; Ji
et al., 2020). Contrastive learning baselines: Sim-
CSE (Gao et al., 2021), Ancor (Bukchin et al.,
2021), Delete (Wu et al., 2020), Nearest-Neighbor
Contrastive Learning (NNCL) (Dwibedi et al.,
2021), Contrastive Learning with Nearest Neigh-
bors (CLNN) (Zhang et al., 2022), Soft Neigh-
bor Contrastive Learning (SNCL) (Chongjian
et al., 2022), Weighted Self-Contrastive Learn-
ing (WSCL) (An et al., 2022), Denoised Neigh-
borhood Aggregation (DNA), and Dynamic Or-
der Weighted Network (DOWN) (An et al., 2023a,
2024). We also explore variants incorporating the
cross-entropy loss (+CE).
5.1.3 Evaluation Metrics
To evaluate the quality of the discovered fine-
grained clusters, we use the Adjusted Rand Index
(ARI) (Hubert and Arabie, 1985) and Normalized
Mutual Information (NMI) (Lancichinetti et al.,
2009). For assessing classification performance,
we use clustering Accuracy (ACC) (kuh, 1955; An
et al., 2023a). Detailed descriptions of these met-
rics are provided in Appendix
5.1.4 Implementation Details
To ensure fair comparisons with baselines, we use
the BERT-base-uncased model as the backbone for
all STAR method variants. We adhere to the hy-
perparameters used by the integrated baselines to
demonstrate the effectiveness of our STAR method.
The learning rate for both pre-training and training
is5e−5, using the AdamW optimizer with a 0.01
weight decay and 1.0 gradient clipping. The mo-
mentum coefficient mis set to 0.99. The batch size
for pre-training, training, and testing is 64. The
temperature τis set to 0.07. The number of neigh-
borskis set to {120, 120, 250} for the CLINC,
HWU64, and WOS datasets, respectively. Epochs
for pretraining and training are set to 100 and 20,
respectively. Further details are provided in Ap-
pendix
5.1.5 Research Questions
The following research questions ( RQs) are inves-
tigated: 1. What is the impact of STAR method
on FCDC tasks? 2. What are the effects of the
proposed real-time centroid inference compared
to traditional clustering inference? 3. How doeseach component of the STAR method affect perfor-
mance? 4. How can we effectively and efficiently
set the base for the exponential function in the
STAR method?
5.2 Result Analysis (RQ1)
As shown in Table 2, STAR method variants outper-
form SOTA methods across all datasets and metrics,
validating the effectiveness of the STAR method in
FCDC tasks. Language models like BERT, Llama2
and GPT4 (Devlin et al., 2019b; Touvron et al.,
2023; Achiam et al., 2023) (GPT4 prompt in Ap-
pendix ) perform poorly on the FCDC task due
to the lack of fine-grained supervisory informa-
tion. Self-training methods like DC, DAC, and PP-
Net (Caron et al., 2018; Zhang et al., 2021; Ji et al.,
2020) also struggle because they rely on noisy fine-
grained pseudo-labels and overlook comprehensive
semantic similarities (CSS). Contrastive learning
methods such as SNCL (Chongjian et al., 2022) and
WSCL (An et al., 2022) perform better by lever-
aging positive pairs. DNA (An et al., 2023a) and
DOWN (An et al., 2024) further enhance feature
quality by filtering false positives and weighting
them by rank. However, these methods still do
not use CSS for sample distributions. Integrating
the STAR method with existing baselines enhances
performance across all datasets, consistently im-
proving sample distributions in Euclidean space.
The superior performance of STAR is attributed
to three factors: First, bidirectional KL diver-
gence measures CSS, pushing negative samples
further away and relatively bringing positive sam-
ples closer based on CSS magnitude, making fine-
grained clusters easier to distinguish. Second, the
baseBof the exponential in Eq. 4 is a trainable
scalar, balancing CSS magnitude and semantic
structure. Third, STAR variants iteratively boot-
strap model performance in neighborhood retrieval
and representation learning through a generalized
EM process (detailed in Appendix ).
5.3 Inference Mechanism Comparison (RQ2)
Previous methods (Chongjian et al., 2022; An et al.,
2023a, 2024) perform a nearest neighbor search
over the examples of the found fine-grained clus-
ters for fine-grained category prediction (we refer
to this technique as cluster inference). We speed up
this process making it better suitable for real-time
tasks by developing a centroid inference mecha-
nism (see Section 4.4). Results in Table 3 demon-
strates that results of centroid inference are compet-MethodsHWU64 CLINC WOS
ACC ARI NMI ACC ARI NMI ACC ARI NMI
BERT (Devlin et al., 2019b) 33.52 17.04 56.90 34.37 17.61 64.75 31.97 18.36 45.15
BERT + CE 37.89 33.68 74.63 43.85 32.37 78.58 38.29 36.94 64.72
Llama2 (Touvron et al., 2023) 19.27±1.21 5.21±0.46 44.34±0.85 20.77±2.61 5.83±1.52 49.7±3.68 9.85±1.14 1.26±0.75 18.27±2.28
Llama2 + CE 32.40±5.46 17.32±5.95 57.53±5.78 45.69±6.85 29.38±6.55 72.66±7.13 18.51±1.50 7.8±1.18 29.66±3.23
GPT4 (Achiam et al., 2023) 10.77±1.86 0.14±0.05 35.17±3.68 9.56±2.12 0.11±0.06 46.69±3.24 7.56±1.51 0.15±0.04 27.78±2.98
DC (Caron et al., 2018) 18.05 43.34 29.74 26.40 12.51 61.26 29.17 13.98 53.27
DAC (Zhang et al., 2021) 29.14 12.89 52.99 29.16 14.15 62.78 28.47 15.94 43.52
DC + CE 41.73 27.81 66.81 30.28 13.56 62.38 38.76 35.21 60.30
DAC + CE 42.19 28.15 66.50 42.09 28.09 72.78 39.42 33.67 61.60
PPNet (Ji et al., 2020) 58.36±2.51 47.63±1.96 79.75±1.02 70.15±1.86 59.31±0.96 85.08±0.81 62.59±1.41 50.81±1.21 72.19±0.68
STAR-PPNet (ours) 63.19±2.38 52.21±1.33 81.66±1.21 73.21±1.97 61.87±0.79 86.16±0.47 66.15±1.33 53.61±1.24 73.82±0.74
Delete (Wu et al., 2020) 21.30 6.52 44.13 47.11 31.28 73.39 24.50 11.68 35.47
SimCSE (Gao et al., 2021) 24.48 8.42 46.94 40.22 23.57 69.02 25.87 13.03 38.53
Ancor + CE 32.90 30.71 74.73 44.44 31.50 74.67 39.34 26.14 54.35
NNCL (Dwibedi et al., 2021) 32.98 30.02 73.24 17.42 13.93 67.56 29.64 28.51 61.37
SimCSE + CE 34.04 31.81 74.86 52.53 37.03 77.39 41.28 34.47 61.62
Delete + CE 35.13 31.84 74.88 47.87 33.79 76.25 41.53 33.78 61.01
CLNN (Zhang et al., 2022) 37.21 34.66 75.27 19.96 14.76 68.30 29.48 28.42 60.99
Ancor (Bukchin et al., 2021) 37.34 34.75 74.99 45.60 33.11 75.23 41.20 37.00 65.42
SNCL (Chongjian et al., 2022) 42.32 38.17 76.39 55.01 45.64 82.93 36.27 33.62 62.35
WSCL (An et al., 2022) 59.52 49.34 79.31 74.02 62.98 88.37 65.27 51.78 72.46
DNA (An et al., 2023a) 70.81 59.66 83.31 87.66 81.82 94.69 74.57 63.30 76.86
STAR-DNA (ours) 75.79±0.93 65.27±1.12 85.34±0.36 89.25±0.17 83.47±0.27 95.11±0.05 77.19±0.81 64.97±0.75 77.91±0.76
DOWN (An et al., 2024) 78.92 68.17 86.22 91.79 86.70 96.05 80.00 67.09 78.87
STAR-DOWN (ours) 80.31±0.26 70.22±0.59 87.28±0.31 92.45±0.38 87.05±0.17 96.20±0.07 81.98±0.67 69.27±0.60 79.99±0.40
Table 2: The average performance (%) in terms of Accuracy (ACC), Adjusted Rand Index (ARI), and Normalized
Mutual Information (NMI) on three datasets for the FCDC language task. To ensure fair comparisons with previous
works (An et al., 2022, 2023a, 2024) and demonstrate the effectiveness of STAR, we use the same clustering
inference mechanism and also average the results over three runs with identical common hyperparameters. Some
baselines results are cited from aforementioned previous works, where standard deviations are not originally
provided.
Methods HWU64 CLINC WOS
ACC ARI NMI ACC ARI NMI ACC ARI NMI
STAR-DOWN (clustering) 80.31±0.26 70.22±0.59 87.28±0.31 92.45±0.38 87.05±0.17 96.20±0.07 81.98±0.67 69.27±0.60 79.99±0.40
STAR-DOWN (centroid) 79.44±0.51 69.13±0.75 86.97±0.40 92.60±0.45 87.16±0.53 96.21±0.09 81.89±0.53 69.05±0.39 79.78±0.32
Table 3: Comparison of clustering and centroid inference mechanisms. "Clustering" clusters test set sample
embeddings to determine each sample’s fine-grained category, while "Centroid" infers the category by comparing
each test sample’s cosine similarity to fine-grained centroids.
Methods ACC ARI NMI
ours 80.31±0.26 70.22±0.59 87.28±0.31
w/o CE 78.61±0.44 67.32±0.86 85.62±0.36
w/o KL loss 78.97±0.32 68.03±0.36 85.81±0.16
w/o KL weight 79.26±0.42 68.86±0.37 86.21± 0.07
w/o KL weight and loss 78.96±0.15 68.21±0.22 86.32±0.10
Table 4: Results (%) of the ablation study for STAR-
DOWN on the HWU64 Dataset.
itive with cluster inference. When results are of the
former are lower, this is due to two factors: cluster-
ing inference leverages inter-relations among test
set samples for richer features, while centroid infer-
ence depends on centroids derived from noisy sam-
ples with fine-grained pseudo-labels. Despite these
issues, centroid inference remains a viable option
for real-time applications, balancing immediate an-
alytical needs with slight performance trade-offs.5.4 Ablation Study (RQ1 & RQ3)
We examine the impact of various components of
the STAR method in STAR-DOWN, as detailed in
Table 4. Our results yield the following insights.
(1) Excluding coarse-grained supervision informa-
tion during training (w/o CE) reduces model per-
formance, as this information is crucial for effec-
tive representation learning. ( 2) Omitting the first
loss term (w/o KL loss) from Eq. 4 diminishes
performance. The KL loss term aligns the KL di-
vergence between data samples and the query with
their semantic similarities. Without it, BdKL(qi,hk)
fails to guide the query sample distribution based
on semantic similarities in Eq. 4. ( 3) Removing
the KL weight BdKL(qi,hk)from Eq. 4 (w/o KL
weight) reduces effectiveness. The loss no longer
utilizes fine-grained semantic similarities measured
byBdKL(qi,hk)in the logarithmic space to direct
the query sample distribution in comparison to allBase value ACC ARI NMI
trainable B(ours) 80.31±0.26 70.22±0.59 87.28±0.31
e 79.96±0.12 68.89±0.55 86.66±0.10
10 80.22±0.27 69.61±0.65 87.08±0.30
16 80.73±0.32 70.14±0.58 87.25±0.36
66 80.57±0.38 70.20±0.52 87.07±0.15
Table 5: Averaged results (%) and their standard devia-
tions over three runs of multiple STAR-DOWN methods
with five different base values on the HWU64 dataset.
To set base value conveniently, we set Bas a trainable
scalar.
samples. ( 4) Eliminating both the KL loss term
and the KL weight in Eq. 4 leads to a performance
decline. This omission prevents the optimization of
the query sample towards positive samples in the
logarithmic space and fails to leverage fine-grained
semantic similarities in logarithmic space to influ-
ence the distribution of query samples relative to
all samples in the Euclidean space.
5.5 Exponential Base Impact (RQ4)
In the STAR method’s loss equation (Eq. 4),
BdKL(qi,hk)modulates the distribution of qiand
hkin the Euclidean space based on their semantic
similarity in the logarithmic space, as quantified
by the bidirectional KL divergence. The base Bis
used to enhance semantic differences, improving
the discriminability of fine-grained categories. We
experimented with multiple constant values and a
trainable configuration for B, with multiple STAR-
DOWN results presented in Table 5. The multiple
STAR-DOWN methods with various base values
consistently outperform the DOWN method (Ta-
ble 2), demonstrating the effectiveness and robust-
ness of the STAR method regardless of the base
value B.1Notably, base values that are either too
low (e.g., e) or too high (e.g., 66) disrupt the seman-
tic representation by inadequately or excessively
emphasizing semantic similarities in the logarith-
mic space. To set base value conveniently, we set B
as a trainable scalar, achieving favorable outcomes
as indicated in Table 5.
5.6 Inference of Category Semantics
Prior works (An et al., 2023a, 2024) only discov-
ered fine-grained categories and assigned them nu-
meric indices without elucidating the categories se-
mantics, thus constraining their broader application.
We propose utilizing the commonsense reasoning
1For interesting form similarities to physical laws within
the STAR method, see Appendix .
Figure 4: The t-SNE visualization of sample embed-
dings from STAR-DOWN method on the HWU64
dataset, with different colors representing different
coarse-grained categories. The distinct clusters repre-
sent the discovered fine-grained categories.
capabilities of large language models ( LLM s) to
infer the semantics of these categories. Specifically,
we employ a trained encoder, Fθ, to extract embed-
dings from all train set samples and cluster these
embeddings to assign fine-grained pseudo-labels
to each train set sample. For each fine-grained
category indicated by a specific pseudo-label, we
aggregate all predicted samples from the training
set and use an LLM to deduce the category seman-
tics. Details on the LLM prompt are provided in
Appendix .
5.7 Visualization
We visualize the sample embeddings of STAR-
DOWN in Figure 4. The results demonstrate that
our method forms distinguishable clusters for fine-
grained categories, proving STAR’s effectiveness
in separating dissimilar samples and clustering sim-
ilar ones. Additionally, we visualize the gener-
alized EM perspective of STAR-DOWN in Ap-
pendix.
6 Conclusion
We propose the STAR method for fine-grained cat-
egory discovery in natural language texts, which
utilizes comprehensive semantic similarities in the
logarithmic space to guide the distribution of tex-
tual samples, including conversational intents, sci-
entific paper abstracts, and assistant queries, in the
Euclidean space. STAR pushes query samples fur-
ther away from negative samples and brings them
closer to positive samples based on the comprehen-
sive semantic similarities magnitude. This process
forms compact clusters, each representing a dis-
covered category. We theoretically analyze the
effectiveness of STAR method. Additionally, we
introduce a centroid inference mechanism that ad-
dresses previous gaps in real-time evaluations. Ex-
periments on three natural language benchmarksdemonstrate that STAR achieves new state-of-the-
art performance in fine-grained category discovery
tasks for text classification.
References
1955. The hungarian method for the assignment prob-
lem. Naval research logistics quarterly , 2(1-2):83–
97.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Wenbin An, Feng Tian, Ping Chen, Siliang Tang,
Qinghua Zheng, and Qianying Wang. 2022. Fine-
grained category discovery under coarse-grained su-
pervision with hierarchical weighted self-contrastive
learning. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 1314–1323.
Wenbin An, Feng Tian, Wenkai Shi, Yan Chen, Qinghua
Zheng, Qianying Wang, and Ping Chen. 2023a. Dna:
Denoised neighborhood aggregation for fine-grained
category discovery. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 12292–12302.
Wenbin An, Feng Tian, Wenkai Shi, Haonan Lin,
Yaqiang Wu, Mingxiang Cai, Luyan Wang, Hua Wen,
Lei Yao, and Ping Chen. 2024. Down: Dynamic
order weighted network for fine-grained category dis-
covery. Knowledge-Based Systems , 293:111666.
Wenbin An, Feng Tian, Qinghua Zheng, Wei Ding,
QianYing Wang, and Ping Chen. 2023b. General-
ized category discovery with decoupled prototypical
network. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 37, pages 12527–
12535.
Rinu Boney and Alexander Ilin. 2017. Semi-supervised
and active few-shot learning with prototypical net-
works. arXiv preprint arXiv:1711.10856 .
Guy Bukchin, Eli Schwartz, Kate Saenko, Ori Shahar,
Rogerio Feris, Raja Giryes, and Leonid Karlinsky.
2021. Fine-grained angular contrastive learning with
coarse labels. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition ,
pages 8730–8740.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. 2018. Deep clustering for unsuper-
vised learning of visual features. In Proceedings of
the European conference on computer vision (ECCV) ,
pages 132–149.
Tianshui Chen, Liang Lin, Riquan Chen, Yang Wu, and
Xiaonan Luo. 2018. Knowledge-embedded represen-
tation learning for fine-grained image recognition. InProceedings of the 27th International Joint Confer-
ence on Artificial Intelligence , pages 627–634.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and
Geoffrey Hinton. 2020. A simple framework for
contrastive learning of visual representations. In In-
ternational conference on machine learning , pages
1597–1607. PMLR.
GE Chongjian, Jiangliu Wang, Zhan Tong, Shoufa Chen,
Yibing Song, and Ping Luo. 2022. Soft neighbors
are positive supporters in contrastive visual repre-
sentation learning. In The Eleventh International
Conference on Learning Representations .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019a. Bert: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019b. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson,
Pierre Sermanet, and Andrew Zisserman. 2021. With
a little help from my friends: Nearest-neighbor con-
trastive learning of visual representations. In Pro-
ceedings of the IEEE/CVF International Conference
on Computer Vision , pages 9588–9597.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 6894–6910.
Xiaoting Guo, Wei Yu, and Xiaodong Wang. 2021. An
overview on fine-grained text sentiment analysis: Sur-
vey and challenges. In Journal of Physics: Confer-
ence Series , volume 1757, page 012038. IOP Pub-
lishing.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
Ross Girshick. 2020. Momentum contrast for unsu-
pervised visual representation learning. In Proceed-
ings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 9729–9738.
Lawrence Hubert and Phipps Arabie. 1985. Comparing
partitions. Journal of classification , 2:193–218.
Zilong Ji, Xiaolong Zou, Tiejun Huang, and Si Wu.
2020. Unsupervised few-shot feature learning via
self-supervised training. Frontiers in computational
neuroscience , 14:83.Kamran Kowsari, Donald E Brown, Mojtaba Hei-
darysafa, Kiana Jafari Meimandi, Matthew S Gerber,
and Laura E Barnes. 2017. Hdltex: Hierarchical deep
learning for text classification. In 2017 16th IEEE
international conference on machine learning and
applications (ICMLA) , pages 364–371. IEEE.
Andrea Lancichinetti, Santo Fortunato, and János
Kertész. 2009. Detecting the overlapping and hi-
erarchical community structure in complex networks.
New journal of physics , 11(3):033015.
Stefan Larson, Anish Mahendran, Joseph J Peper,
Christopher Clarke, Andrew Lee, Parker Hill,
Jonathan K Kummerfeld, Kevin Leach, Michael A
Laurenzano, Lingjia Tang, et al. 2019. An evaluation
dataset for intent classification and out-of-scope pre-
diction. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
1311–1316.
Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi.
2020. Prototypical contrastive learning of unsuper-
vised representations. In International Conference
on Learning Representations .
Ruixue Lian, William A Sethares, and Junjie Hu. 2024.
Learning label hierarchy with supervised contrastive
learning. arXiv preprint arXiv:2402.00232 .
Xingkun Liu, Arash Eshghi, Pawel Swietojanski, and
Verena Rieser. 2021. Benchmarking natural language
understanding services for building conversational
agents. In Increasing Naturalness and Flexibility
in Spoken Dialogue Interaction: 10th International
Workshop on Spoken Dialogue Systems , pages 165–
183. Springer.
Ruotian Ma, Zhang Lin, Xuanting Chen, Xin Zhou,
Junzhe Wang, Tao Gui, Qi Zhang, Xiang Gao, and
Yun Wen Chen. 2023. Coarse-to-fine few-shot learn-
ing for named entity recognition. In Findings of
the Association for Computational Linguistics: ACL
2023 , pages 4115–4129.
Dheeraj Mekala, Varun Gangal, and Jingbo Shang.
2021. Coarse2fine: Fine-grained text classification
on coarsely-grained annotated data. In Proceedings
of the 2021 Conference on Empirical Methods in
Natural Language Processing , pages 583–594.
Zhengxin Pan, Fangyu Wu, and Bailing Zhang. 2023.
Fine-grained image-text matching by cross-modal
hard aligning network. In Proceedings of the
IEEE/CVF conference on computer vision and pat-
tern recognition , pages 19275–19284.
Wongi Park and Jongbin Ryu. 2024. Fine-grained self-
supervised learning with jigsaw puzzles for medi-
cal image classification. Computers in Biology and
Medicine , page 108460.
Chang Tian, Wenpeng Yin, Dan Li, and Marie-Francine
Moens. 2024. Fighting against the repetitive trainingand sample dependency problem in few-shot named
entity recognition. IEEE Access .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Sagar Vaze, Andrea Vedaldi, and Andrew Zisserman.
2024. No representation rules them all in category
discovery. Advances in Neural Information Process-
ing Systems , 36.
Shijie Wang, Jianlong Chang, Zhihui Wang, Haojie Li,
Wanli Ouyang, and Qi Tian. 2024a. Content-aware
rectified activation for zero-shot fine-grained image
retrieval. IEEE Transactions on Pattern Analysis and
Machine Intelligence .
Shijie Wang, Zhihui Wang, Haojie Li, Jianlong Chang,
Wanli Ouyang, and Qi Tian. 2024b. Accurate fine-
grained object recognition with structure-driven rela-
tion graph networks. International Journal of Com-
puter Vision , 132(1):137–160.
Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa,
Fei Sun, and Hao Ma. 2020. Clear: Contrastive
learning for sentence representation. arXiv preprint
arXiv:2012.15466 .
Hanlei Zhang, Hua Xu, Ting-En Lin, and Rui Lyu. 2021.
Discovering new intents with deep aligned clustering.
InProceedings of the AAAI Conference on Artificial
Intelligence , volume 35, pages 14365–14373.
Yuwei Zhang, Haode Zhang, Li-Ming Zhan, Xiao-Ming
Wu, and Albert Lam. 2022. New intent discovery
with pre-training and contrastive learning. In Pro-
ceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 256–269.
Zhun Zhong, Enrico Fini, Subhankar Roy, Zhiming Luo,
Elisa Ricci, and Nicu Sebe. 2021. Neighborhood
contrastive learning for novel class discovery. In Pro-
ceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 10867–10875.