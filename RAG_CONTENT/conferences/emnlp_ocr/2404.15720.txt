Annotator-Centric Active Learning for Subjective NLP Tasks
Michiel van der Meer1,2, Neele Falk3, Pradeep K. Murukannaiah4, Enrico Liscio4
1Idiap Research Institute, Switzerland
2Leiden Institute of Advanced Computer Science, Leiden University, The Netherlands
3Institute for Natural Language Processing, University of Stuttgart, Germany
4Interactive Intelligence, TU Delft, The Netherlands
Abstract
Active Learning (AL) addresses the high costs
of collecting human annotations by strategi-
cally annotating the most informative samples.
However, for subjective NLP tasks, incorporat-
ing a wide range of perspectives in the annota-
tion process is crucial to capture the variability
in human judgments. We introduce Annotator-
Centric Active Learning (ACAL), which incor-
porates an annotator selection strategy follow-
ing data sampling. Our objective is two-fold:
(1) to efficiently approximate the full diversity
of human judgments, and (2) to assess model
performance using annotator-centric metrics,
which value minority and majority perspectives
equally. We experiment with multiple anno-
tator selection strategies across seven subjec-
tive NLP tasks, employing both traditional and
novel, human-centered evaluation metrics. Our
findings indicate that ACAL improves data ef-
ficiency and excels in annotator-centric perfor-
mance evaluations. However, its success de-
pends on the availability of a sufficiently large
and diverse pool of annotators to sample from.
1 Introduction
A challenging aspect of natural language under-
standing (NLU) is the variability of human judg-
ment and interpretation in subjective tasks (e.g.,
hate speech detection) (Plank, 2022). In a subjec-
tive task, a data sample is typically labeled by a
set of annotators, and differences in annotation are
reconciled via majority voting, resulting in a single
(supposedly, true) “gold label” (Uma et al., 2021).
However, this approach has been criticized for treat-
ing label variation exclusively as noise, which is
especially problematic in sensitive subjective tasks
(Aroyo and Welty, 2015) since it can lead to the
exclusion of minority voices (Leonardelli et al.,
2021).
Subjectivity can be addressed by modeling the
full distribution of annotations for each data sam-
ple instead of employing gold labels (Plank, 2022).
Labeled
datasettrain
NLP
model
Unlabeled
datasetsample
selection
strategyOracle ALLabeled
datasettrain
NLP
model
Unlabeled
datasetsample
selection
strategyACALAnnotator
pool
annotator
selection
strategyFigure 1: Active Learning (AL) approaches (left) use
a sample selection strategy to pick samples to be an-
notated by an oracle. The Annotator-Centric Active
Learning (ACAL) approach (right) extends AL by in-
troducing an annotator selection strategy to choose the
annotators who annotate the selected samples.
However, resources for such approaches are scarce,
as most datasets do not (yet) make fine-grained an-
notation details available (Cabitza et al., 2023), and
representing a full range of perspectives is contin-
gent on obtaining costly annotations from a diverse
set of annotators (Bakker et al., 2022).
One way to handle a limited annotation budget is
to use Active Learning (Settles, 2012, AL). Given
a pool of unannotated data samples, AL employs
a sample selection strategy to obtain maximally
informative samples, retrieving the corresponding
annotations from a ground truth oracle (e.g., a sin-
gle human expert). However, in subjective tasks,
there is no such oracle. Instead, we rely on a set
of available annotators. Demanding all available
annotators to annotate all samples would provide
a truthful representation of the annotation distribu-
tion, but is often unfeasible, especially if the pool
of annotators is large. Thus, deciding which anno-
tator(s) should annotate is as critical as deciding
which samples to annotate.
In most practical applications, annotators are
randomly selected. This results in an annotation
distribution insensitive to outlier annotators—most
annotations reflect the majority voices and fewer
reflect the minority voices. This may not be desir-
able in applications such as hate speech, where thearXiv:2404.15720v4  [cs.CL]  23 Oct 2024opinions of the majority and minority should be
valued equally. In such cases, a more deliberate an-
notator selection is required. To ensure a balanced
representation of majority and minority voices, we
leverage strategies inspired by Rawls’ principle
of fairness (Rawls, 1973), which advocates that a
fair society is achieved when the well-being of the
worst-off members of society (the minority annota-
tors, in this case) is maximized.
We introduce Annotator-Centric Active Learn-
ing (ACAL) to emphasize and control who an-
notates which sample. In ACAL (Figure 1), the
sample selection strategy of traditional AL is fol-
lowed by an annotator selection strategy , indicat-
ing which of the available annotators should anno-
tate each selected data sample.
Contributions (1) We present ACAL as an ex-
tension of the AL approach and introduce three
annotator selection strategies aimed at collecting
a balanced distribution of minority and majority
annotations. (2) We introduce a suite of annotator–
centric evaluation metrics to measure how individ-
ual and minority annotators are modeled. (3) We
demonstrate ACAL’s effectiveness in three datasets
with subjective tasks—hate speech detection, moral
value classification, and safety judgments.
Our experiments show that the proposed ACAL
methods can approximate the distribution of human
judgments similar to AL while requiring a lower
annotation budget and modeling individual and mi-
nority voices more accurately. However, our eval-
uation shows how the task’s annotator agreement
and the number of available annotations impact
ACAL’s effectiveness—ACAL is most effective
when a large pool of diverse annotators is available.
Importantly, our experiments show how the ACAL
framework controls how models learn to represent
majority and minority annotations. This is cru-
cial for subjective and sensitive applications such
as detecting human values and morality (Kiesel
et al., 2023; Liscio et al., 2023a), argument min-
ing (van der Meer et al., 2024b), and hate speech
(Khurana et al., 2024).
2 Related work
2.1 Learning with annotator disagreement
Modeling annotator disagreement is garnering in-
creasing attention (Aroyo and Welty, 2015; Uma
et al., 2021; Plank, 2022; Cabitza et al., 2023).
Changing annotation aggregation methods can leadto a fairer representation than simple majority
(Hovy et al., 2013; Tao et al., 2019). Alterna-
tively, the full annotation distribution can be mod-
eled using soft labels (Peterson et al., 2019; Müller
et al., 2019; Collins et al., 2022). Other approaches
leverage annotator-specific information, e.g., by
including individual classification heads per anno-
tator (Davani et al., 2022), embedding annotator
behavior (Mokhberian et al., 2024), or encoding the
annotator’s socio-demographic information (Beck
et al., 2024). Yet, modeling annotator diversity re-
mains challenging. Standard calibration metrics
under human label variation may be unsuitable, es-
pecially when the variation is high (Baan et al.,
2022). Trade-offs ought to be made between col-
lecting more samples or more annotations (Gruber
et al., 2024). Further, solely measuring differences
among sociodemographic traits is not sufficient to
capture opinion diversity (Orlikowski et al., 2023).
Instead, we represent diversity based on which an-
notators annotated what andhow. We experiment
with annotator selection strategies to reveal what
aspects impact task performance and annotation
budget.
2.2 Active Learning
AL enables a supervised learning model to achieve
high performance by judiciously choosing a few
training examples (Settles, 2012). In a typical AL
scenario, a large collection of unlabeled data is
available, and an oracle (e.g., a human expert)
is asked to annotate this unlabeled data. A sam-
pling strategy is used to iteratively select the next
batch of unlabeled data for annotation (Ren et al.,
2021). AL has found widespread application in
NLP (Zhang et al., 2022). Two main strategies
are employed, either by selecting the unlabeled
samples on which the model prediction is most
uncertain (Zhang et al., 2017), or by selecting sam-
ples that are most representative of the unlabeled
dataset (Erdmann et al., 2019; Zhao et al., 2020).
The combination of AL and annotator diversity is
a novel direction. Existing works propose to align
model and annotator uncertainties (Baumler et al.,
2023), adapt annotator-specific classification heads
in AL settings (Wang and Plank, 2023), or select
texts to annotate based on annotator preferences
(Kanclerz et al., 2023). These methods ignore a
crucial part of learning with human variation: the
diversity among annotators. We focus on selecting
annotators such that they best inform us about theunderlying label diversity.
3 Method
First, we define the soft-label prediction task we
use to train a supervised model. Then, we introduce
the traditional AL and the novel ACAL approaches.
3.1 Soft-label prediction
Consider a dataset of triples {xi, aj, yij}, where xi
is a data sample (i.e., a piece of text) and yij∈C
is the class label assigned by annotator aj. The
multiple labels assigned to a sample xiby the dif-
ferent annotators are usually combined into an ag-
gregated label ˆyi. For training with soft labels, the
aggregation typically takes the form of maximum
likelihood estimation (Uma et al., 2021):
ˆyi(x) =PN
i=1[xi=x][yij=c]PN
i=1[xi=x](1)
In our experiments, we use a passive learning
approach that uses all available {xi,ˆyi}to train a
model fθwith cross-entropy loss as a baseline.
3.2 Active Learning
AL imposes a sampling technique for inputs xi,
such that the most informative sample(s) are picked
for learning. In a typical AL approach, a set of
unlabelled data points Uis available. At every iter-
ation, a sample selection strategy Sselects samples
xi∈Uto be annotated by an oracle Othat pro-
vides the ground truth label distribution ˆyi. The
selected samples and annotations are added to the
labeled data D, with which the model fθis trained.
Alg. 1 provides an overview of the procedure.
Algorithm 1: AL approach.
input : Unlabeled data U, Data sampling
strategy S, Oracle O
D0← {}
forn= 1..Ndo
sample data points xifromUusingS
obtain annotation ˆyiforxifromO
Dn+1=Dn+{xi,ˆyi}
trainfθonDn+1
end
In the sample selection strategies, a batch of data
of a given size Bis queried at each iteration. Our
experiments compare the following strategies:
Random ( SR)selects a Bsamples uniformly at
random from U.Uncertainty ( SU)predicts a distribution over
class labels with fθ(xi)for each xi∈U, and se-
lectsBsamples with the highest prediction entropy
(the samples the model is most uncertain about).
3.3 Annotator-Centric Active Learning
ACAL builds on AL. In contrast to AL, which re-
trieves an aggregated annotation ˆyi, ACAL em-
ploys an annotator selection strategy Tto select
one annotator and their annotation for each selected
data point xi. Alg. 2 describes the ACAL approach.
Algorithm 2: ACAL approach.
input : Unlabeled data U, Data sampling
strategy S, Annotator sampling
strategy T
D0← {}
forn= 1..Ndo
sample data points xifromUusingS
sample annotators ajforxiusingT
obtain annotation yijfromajforxi
Dn+1=Dn+{xi, yij}
trainfθonDn+1
end
We propose three annotator selection strategies
to gather a distribution that uniformly contains all
possible (majority and minority) labels, inspired
by Rawls’ principle of fairness (Rawls, 1973). The
strategies vary in the type of information used to
represent differences between annotators, including
what orhow the annotators have annotated thus far.
Our experiments compare the following strategies:
Random ( TR)randomly selects an annotator aj.
Label Minority ( TL)considers only information
onhoweach annotator has annotated so far (i.e., the
labels that they have assigned). The minority label
is selected as the class with the smallest annotation
count in the available dataset Dnthus far. Given a
new sample, xi,TLselects the available annotator
that has the largest bias toward the minority label
compared to the other available annotators, i.e.,
who has annotated other samples with the minority
label the most.
Semantic Diversity ( TS)considers only informa-
tion on what each annotator has annotated so far
(i.e., the samples that they have annotated). Given
a new sample xiselected through S,TSselects the
available annotator for whom xiis semantically the
most different from what the annotator has labeled
so far. To measure this difference for an annotatoraj, we employ a sentence embedding model to mea-
sure the cosine distance between the embeddings
ofxiand embeddings of all the samples annotated
byaj. We then take the average of all semantic
similarities. The annotator with the lowest average
similarity score is selected.
Representation Diversity ( TD)selects the anno-
tator that has the lowest similarity on average with
all other annotators available for that item. We
create a representation for each annotator by aver-
aging the embeddings of samples annotated by aj
together with their respective labels, followed by
computing the pair-wise cosine similarity between
all annotators.
4 Experimental Setup
We describe the experimental setup for the com-
parisons between ACAL strategies. In all our ex-
periments, we employ a TinyBERT model (Jiao
et al., 2020) to reduce the number of trainable pa-
rameters. Appendix A includes a detailed overview
of the computational setup and hyperparameters.
We make the code for the ACAL strategies and
evaluation metrics available via GitHub.1
4.1 Datasets
We use three datasets which vary in domain, anno-
tation task (in italics ), annotator count, and annota-
tions per instance.
TheDICES Corpus (Aroyo et al., 2023) is com-
posed of 990 conversations with an LLM where
172 annotators provided judgments on whether a
generated response can be deemed safe (3-way
judgments: yes, no, unsure). Samples have 73
annotations on average. We perform a multi-class
classification of the judgments.
TheMFTC Corpus (Hoover et al., 2020) is com-
posed of 35K tweets that 23 annotators annotated
with any of the 10 moral elements from the Moral
Foundation Theory (Graham et al., 2013). We
select the elements of loyalty (lowest annotation
count), care (average count), and betrayal (highest
count). Samples have 4 annotations on average.
We create three binary classifications to predict
the presence of the respective elements. As most
tweets were labeled as non-moral (i.e., with no
moral element), we balanced the datasets by sub-
sampling the non-moral class.
1https://github.com/m0re4u/
acal-subjectiveThe MHS Corpus (Sachdeva et al., 2022)
consists of 50K social media comments on
which 8K annotators judged three hate speech
aspects— dehumanize (low inter-rater agreement),
respect (medium agreement), and genocide (high
agreement)—on a 5-point Likert scale. Samples
have 3 annotations on average. We perform a multi-
class classification with the annotated Likert scores
for each task.
The datasets and tasks differ in levels of anno-
tator agreement, measured via entropy of the an-
notation distribution. DICES and MHS generally
have medium entropy scores, whereas the MFTC
entropy is highly polarized (divided between sam-
ples with very high and very low agreement). Ap-
pendix A.5 provides details of the entropy scores.
4.2 Evaluation metrics
The ACAL strategies aim to guide the model to
learn a representative distribution of the annota-
tor’s perspectives while reducing annotation effort.
To this end, we evaluate the model both with a tra-
ditional evaluation metric and a metric aimed at
comparing predicted and annotated distributions:
Macro F1-score ( F1)For each sample in the test
set, we select the label predicted by the model with
the highest confidence, determine the golden la-
bel through a majority agreement aggregation, and
compute the resulting macro F1-score.
Jensen-Shannon Divergence ( JS)TheJSmea-
sures the divergence between the distribution of
label annotation and prediction (Nie et al., 2020).
We report the average JSfor the samples in the test
set to measure how well the model can represent
the annotation distribution.
Further, since ACAL shifts the focus to annota-
tors, we introduce novel annotator-centric evalua-
tion metrics. First, we report the average among
annotators. Second, in line with Rawls’ principle
of fairness, the result for the worst-off annotators:
Per-annotator F1(Fa
1) and JS(JSa)We com-
pute the F1(orJS) for each annotator in the test set
using their annotations as golden labels (or target
distribution), and average it.
Worst per-annotator F1(Fw
1) and JS(JSw)
We compute the F1(orJS) for each annotator in
the test set using their annotations as golden labels
(or target distribution), and report the average of
the lowest 10% to mitigate noise.
These metrics allow us to measure the trade-
offs between modeling the majority agreement, arepresentative distribution of annotations, and ac-
counting for minority voices. In the next section,
we describe how we obtained the results.
4.3 Training procedure
We test the annotator selection strategies proposed
in Section 3.3 by comparing all combinations of
the two sample selection strategies ( SRandSU)
and the four annotator selection strategies ( TR,TL,
TS, andTD). At each iteration, we use Sto select
Bunique samples from the unlabeled data pool
U. We select Bas the smallest between 5% of the
number of available annotations and the number
of unique samples in the training set. For each se-
lected sample xi, we use Tto select one annotator
and retrieve their annotation yij.
We split each dataset into 80% train, 10% valida-
tion, and 10% test. We start the training procedure
with a warmup iteration of Brandomly selected an-
notations (Zhang et al., 2022). We proceed with the
ACAL iterations by combining SandT. We select
the model checkpoint across all AL iterations that
led to the best JSperformance on the validation
set and evaluate it on the test set. We repeat this
process across three data splits and model initial-
izations. We report the average scores on the test
set.
We compare ACAL with traditional oracle-based
AL approaches ( SROandSUO), which use the
data sampling strategies but obtain all possible an-
notations for each sample as in Alg. 1. Further, we
employ a passive learning (PL) approach as an up-
per bound by training the model on the full dataset,
thus observing all available samples and annota-
tions. Similar to ACAL, the AL and PL baselines
are averaged over three seeds.
5 Results
We start by highlighting the benefits of ACAL over
AL and PL (Section 5.1). Next, we closely exam-
ine ACAL on efficiency and fairness (Section 5.2).
Then, we select a few cases of interest and dive
deeper into the strategies’ behavior during training
(Section 5.3). Finally, we investigate ACAL across
varying levels of subjectivity (Section 5.4).
5.1 Highlights
Our experiments show that ACAL can have a ben-
eficial impact over using PL and AL. Figure 2
highlights two main findings: (1) ACAL strategies
can more quickly learn to represent the annotation0 10000 20000 30000 400000.10.20.3
# unique annotationsJSDICES
SRTR SUTR SRTL SUTL
SRTS SUTS SRTD SUTD
SRO SUO Passive
0 10000 20000 30000 400000.250.3
# unique annotationsF1MHS (dehumanize)
Figure 2: Learning curves showing model performance
on the validation set. On DICES (upper), ACAL ap-
proaches are quicker than AL in obtaining similar per-
formance to passive learning. On MHS (lower), ACAL
surpasses passive learning in F1when data has high
disagreement.
distribution with a large pool of annotators, and
(2) when agreement between annotators is polar-
ized, ACAL leads to improved results compared to
learning from aggregated labels. In the next sec-
tions, we provide a deeper understanding of the
conditions in which ACAL works well.
5.2 Efficiency and Fairness
Table 1 presents the results of evaluating the best
models (those with the highest JSscores on the
validation set) on the test set. We analyze the re-
sults along two dimensions: (a) efficiency : what is
the impact of the different strategies on the trade-
off between annotation budget and performance?
(b)fairness : do the selection strategies that aim for
a balanced consideration of minority and majority
views lead to better performance in the human-
centric evaluation metrics? For MFTC we focus on
care because it has an average number of samples
available, and for MHS we focus on dehumanize
because it has high levels of disagreement. Ap-
pendix B presents the remainder of the results.Average Worst-off
App. F1JS Fa
1JSaFw
1JSw∆%DICESSRTR53.2 .100 43.2 .186 16.7 .453 -36.8
SRTL55.5 .101 42.4 .187 15.5 .450 -32.7
SRTS61.0 .103 44.2 .186 16.4 .447 -35.5
SRTD58.9 .142 43.1 .203 16.9 .370 -30.0
SUTR53.2 .100 43.2 .186 16.7 .453 -36.8
SUTL55.5 .101 42.4 .187 15.5 .450 -32.7
SUTS63.1 .098 43.9 .187 18.4 .447 -38.2
SUTD58.9 .142 43.1 .203 16.9 .370 -30.0
SRO 59.1 .112 41.4 .191 13.3 .425 -0.1
SUO 46.2 .110 38.4 .192 11.7 .427 -0.1
PL 59.0 .105 37.1 .211 12.3 .479 –MFTC ( care)SRTR78.9 .038 61.1 .141 37.7 .247 -1.6
SRTL78.5 .037 61.6 .142 39.2 .249 -0.4
SRTS78.1 .039 60.0 .145 35.1 .248 -1.7
SRTD76.6 .040 60.4 .144 35.7 .243 -1.7
SUTR79.4 .038 61.2 .143 37.7 .252 -5.6
SUTL80.7 .037 58.9 .142 42.3 .248 -2.5
SUTS79.1 .037 60.8 .143 39.9 .258 -1.1
SUTD78.1 .040 58.6 .145 35.7 .253 -2.5
SRO 79.0 .037 58.6 .141 39.2 .255 -0.2
SUO 79.4 .037 58.3 .144 35.7 .253 -12.7
PL 81.1 .032 51.2 .179 37.7 .251 –MHS ( dehumanize )SRTR33.6 .081 31.5 .394 0.0 .489 -50.0
SRTL33.1 .081 32.2 .397 0.0 .478 -62.5
SRTS30.5 .079 31.3 .397 0.0 .480 -62.5
SRTD32.4 .081 31.8 .398 0.0 .479 -62.5
SUTR32.4 .080 32.2 .389 0.0 .508 -7.8
SUTL33.1 .080 32.8 .388 0.0 .507 -7.8
SUTS33.6 .080 32.6 .388 0.0 .506 -7.8
SUTD33.0 .079 32.6 .384 0.0 .513 -3.0
SRO 32.8 .077 33.9 .387 0.0 .496 -60.1
SUO 33.3 .080 33.1 .390 0.0 .497 -24.7
PL 28.0 .075 20.2 .424 0.0 .547 –
Table 1: Test set results on the DICES, MFTC ( care),
and MHS ( dehumanize ) datasets. Results report the av-
erage test scores from the best-performing model check-
point on the validation set (lowest JS), evaluated across
three data splits and model initializations. ∆% denotes
the reduction in the annotation budget with respect to
passive learning. In bold, the best performance per col-
umn and per dataset (higher F1are better, lower JSare
better).
Efficiency We discuss the performance on F1
andJSto measure how well the proposed strate-
gies model label distributions and examine the used
annotator budget. Across all tasks and datasets,
ACAL and AL consistently yield comparable or
superior F1andJSwith a lower annotation bud-
get than PL. When comparing ACAL with AL,
the results vary depending on the task and dataset.
For DICES, there is a significant benefit to using
ACAL, as it can save up to ∼40% of the annotation
budget while yielding better scores across all met-
rics than AL. With AL, we observe only a smallreduction in annotation cost. For MFTC, AL with
SUleads to the largest cost benefits ( ∼12% less an-
notation budget), but at a cost in terms of absolute
JSandF1. ACAL slightly outperforms AL but
does not lead to a decrease in annotation budget.
For MHS, both AL and ACAL significantly reduce
the annotation cost ( ∼60%) while yielding better
scores than PL—however, AL and ACAL do not
show substantial performance differences. Overall,
when looking at F1andJSwhich are aggregated
over the whole test set, we conclude that ACAL is
most efficient when the pool of available annotators
for one sample is large (as with the DICES dataset),
whereas the difference between ACAL and AL is
negligible with a small pool of annotators per data
sample (as with MFTC and MHS).
Fairness We investigate the extent to which the
models represent individual annotators fairly and
capture minority opinions via the annotator-centric
evaluation metrics ( Fa
1,JSa,Fw
1, and JSw). We
observe a substantial improvement when using AL
or ACAL over PL. Further, we observe no single
winner-takes-all approach: high F1andJSscores
do not consistently co-occur with high scores for
the annotator-centric metrics. This highlights the
need for a more comprehensive evaluation to as-
sess models for subjective tasks. Yet, we observe
that ACAL slightly outperforms AL in modeling
individual annotators ( JSaandFa
1). This trend is
particularly evident with DICES, again likely due
to the large pool of annotators available per data
sample. Lastly, ACAL is best in the worst-off met-
rics (JSwandFw
1), showing the ability to better
represent minority opinions as a direct consequence
of the proposed annotator selection strategies on
DICES and MFTC. However, all approaches score
0 forFw
1on MHS. This is due to the high disagree-
ment in this dataset: the 10% worst-off annotators
always disagree with a hard label derived from
the predicted label distribution. In conclusion, our
experiments show that, when a large pool of annota-
tors is available, a targeted sampling of annotators
requires fewer annotations and is fairer. That is, mi-
nority opinions are better represented without large
sacrifices in performance compared to the overall
label distribution.
5.3 Convergence
The evaluation on the test set paints a general pic-
ture of the advantage of using ACAL over AL or
PL. In this section, we assess how different ACAL0 10000 20000 30000 400000.30.350.4
# unique annotationsFa
1(↑)DICESSRTR SUTR SRTL SUTL SRTS SUTS SRTD SUTD SRO SUO Passive
0 5000 10000 15000 20000 250000.450.50.550.60.65
# unique annotationsMFTC ( care)
0 10000 20000 30000 400000.260.280.30.32
# unique annotationsMHS ( dehumanize )
0 10000 20000 30000 400000.350.40.45
# unique annotationsJSw(↓)DICES
0 5000 10000 15000 20000 250000.220.230.240.250.26
# unique annotationsMFTC ( care)
0 10000 20000 30000 400000.440.460.480.50.52
# unique annotationsMHS ( dehumanize )
Figure 3: Selected plots showing the Fa
1andJSwperformance on the validation set during the ACAL and AL
iterations for DICES, MFTC ( care), and MHS ( dehumanize ). Higher Fa
1is better, lower JSwis better. Y-axes are
scaled to highlight the relative performance to PL.
strategies converge over iterations. We describe
the major patterns across our experiments by ana-
lyzing six examples of interest with Fa
1andJSw
(Figure 3). We select Fa
1because it reveals how
well individual annotators are modeled on average,
andJSwto measure how strategies deviate from
modeling the majority perspective. Appendix B.2
provides an overview of all metrics.
First, we notice that the trends for Fa
1andJSw
are both increasing—the first is expected, but the
second requires an explanation. As the model is
exposed to more annotations over the training it-
erations, the predicted label distribution starts to
fit the true label distribution. However, here we
consider each annotator individually: JSwreports
the average of the 10% lowest JSscores per an-
notator. The presence of disagreement implies the
existence of annotators that annotate differently
from the majority. Since our models predict the
full distribution, they assign a proportional proba-
bility to dissenting annotators. Thus, learning to
model the full distribution of annotations leads to
an increase in JSw.
Second, we notice a difference between ACAL
and AL. On MFTC and MHS, ACAL, compared
to AL, yields overall smaller JSwat the cost of
a slower convergence in Fa
1, showing the trade-
off between modeling all annotators and represent-
ing minorities. However, with DICES the trend
is the opposite. This is due to AL having access
to the complete label distribution: it can model a
balanced distribution, leading to lower worst-offperformance. With a large number of annotations,
ACAL requires more iterations to get the same bal-
anced predicted distribution.
Third, we observe differences among the anno-
tator selection strategies ( T).TDshows the most
differences—both JSwandFa
1increase slower
than for the other strategies. This suggests that
selecting annotators based on the average embed-
ding of the annotated content strongest emphasizes
diverging label behavior.
Finally, we analyze the impact of the sample
selection strategies ( S, dotted vs. solid lines in Fig-
ure 3). For DICES, SRandSUlead to comparable
results, likely due to the low number of samples.
Using SUin MFTC leads to Fa
1performance de-
creasing at the start of training. The strategy pri-
oritizes obtaining annotations for already added
samples to lower their entropy, while the variation
in labels is irreconcilable (since there are limited
labels available, and they are in disagreement). We
see a similar pattern for MHS.
These results further underline our main find-
ing that ACAL is effective in representing diverse
annotation perspectives when there is a (1) het-
erogeneous pool of annotators, and (2) a task that
facilitates human label variation.
5.4 Impact of subjectivity
We further investigate ACAL strategies on (1) label
entropy, and (2) cross-task performance.
Alignment of ACAL strategies during training
We want to investigate how well the ACAL strate-0 10000 20000 30000 4000000.5
# unique annotationsProportionDICES
lower higherTR
TS
TL
TD
Figure 4: Proportion of data samples that result in higher
or lower entropy than the target label distribution per
ACAL strategy.
gies align with the overall subjective annotations:
do they drive the model entropy in the right direc-
tion? We measure the entropy of the samples in the
labeled training set at each iteration and compare it
to the entropy of all annotations of those samples.
Higher entropy in the labeled training set than the
actual entropy suggests that the selection strategy
overestimates uncertainty. Lower entropy indicates
that the model may not sufficiently account for dis-
agreement. When the entropy matches the true
entropy, the selection strategy is well-calibrated to
strike a healthy middle ground between sampling
diverse labels and finding the majority class. We fo-
cus on DICES as a case study due to the wide range
of entropy scores. We group each sample based on
the true label entropy into low ( <0.43), medium
(0.43−0.72), and high ( >0.72). We apply the
same categorization at each training iteration for
samples labeled thus far. Subsequently, we plot the
proportion of data points for which the selection
strategy results in excessively high or excessively
low entropy.
Figure 4 visualizes the proportions. At the begin-
ning of training, entropy is generally low because
samples have few annotations. Over time, the se-
lected annotations better align with the true entropy.
At the start (at 10K unique annotations), roughly
only a third of the samples have aligned entropy
scores ( TR= 27% , TS= 27% , TL= 33% , TD=
32%). Further towards the end of the ACAL it-
erations, this has increased for all ACAL strate-
gies except TD(TR= 64% , TS= 62% , TL=
57%, TD= 17% ). When and how much the strate-
gies succeed in matching the true label distribution
differs: TSandTRtake longer to increase label en-SRTSSRO PL00.20.40.6Fa
1(↑)MFTCcare betrayal
loyalty
SRTSSRO PL00.10.20.30.4JSw(↓)MFTCSRTSSRO PL00.20.40.6Fa
1(↑)MHSdehumanize genocide
respect
SRTSSRO PL00.20.40.6JSw(↓)MHS
Figure 5: Comparison of ACAL, AL, and PL across
different MFTC and MHS tasks. Higher Fa
1is better,
and lower JSwis better.
tropy than the other two strategies. They are conser-
vative in adding diverse labels. TLandTDincrease
the proportion of well-aligned data points earlier in
the training process, achieving a balanced entropy
alignment sooner. However, both strategies start
to overshoot the target entropy, whereas the others
show a more gradual alignment with the true en-
tropy. This effect is strongest for TD. This finding
suggests that minority-aware annotator-selection
(TLandTD) strategies achieve the best results in
the early stages of training—that is, they are ef-
fective for quickly raising entropy but can lead to
overrepresentation.
Cross-task performance Figure 5 compares the
two annotator-centric metrics on the three tasks of
MFTC and MHS—the datasets for which we have
seen the least impact of ACAL over AL and PL. We
select a data sampling ( SR) and annotator sampling
strategy ( TS), based on its strong performance on
DICES for comprehensive comparison.
When evaluating MFTC loyalty , which has the
highest disagreement, JSwis more accurately ap-
proximated with PL. Similarly, ACAL is outper-
formed by AL on Fa
1for the dehumanize (high dis-
agreement) task. However, for the less subjective
taskgenocide , ACAL leads to higher Fa
1. This sug-
gests that the effectiveness of annotation strategies
varies depending on the task’s degree of subjectiv-
ityandthe available pool of annotators. The more
heterogeneous the annotation behavior, indicative
of a highly subjective task, the larger the pool of
annotators required for each sample selection. Wealso observe that there is a trade-off between mod-
eling the majority of annotators equally ( Fa
1) and
prioritizing the minority ( JSw).
6 Conclusion
We present ACAL as an extension of AL to em-
phasize the selection of diverse annotators. We
introduce three novel annotator selection strate-
gies and four annotator-centric metrics and experi-
ment with tasks across three different datasets. We
find that the ACAL approach is especially effec-
tive in reducing the annotation budget when the
pool of available annotators is large. However, its
effectiveness is contingent on data characteristics
such as the number of annotations per sample, the
number of annotations per annotator, and the na-
ture of disagreement in the task annotations. Fur-
thermore, our novel evaluation metrics display the
trade-off between modeling overall distributions of
annotations and adequately accounting for minor-
ity voices, showing that different strategies can be
tailored to meet different goals. Especially early
in the training process, strategies that are aggres-
sive in obtaining diverse labels have a beneficial
impact in accounting for minority voices. However,
we recognize that gathering a distribution that uni-
formly contains all possible (minority and majority)
labels can be overly sensitive to small minorities
or noise. Future work should integrate methods
that account for noisy annotations (Weber-Genzel
et al., 2024). Striking a balance between utilitar-
ian and egalitarian approaches, such as between
modeling aggregated distributions and accounting
for minority voices (Lera-Leri et al., 2024) is cru-
cial for inferring context-dependent values (Liscio
et al., 2023b; van der Meer et al., 2023).
Limitations
The main limitation of this work is that the experi-
ments are based on simulated AL which is known
to bear several shortcomings (Margatina and Ale-
tras, 2023). In our study, a primary challenge arises
with two of the datasets (MFTC, MHS), which,
despite having a large pool of annotators, lack an-
notations from every annotator for each item. Con-
sequently, in real-world scenarios, the annotator
selection strategies for these datasets would benefit
from access to a more extensive pool of annotators.
This limitation likely contributes to the underper-
formance of ACAL on these datasets compared to
DICES. We emphasize the need for more datasetsthat feature a greater number of annotations per
item, as this would significantly enhance research
efforts aimed at modeling human disagreement.
Since we evaluate four different annotator selec-
tion strategies and two sample selection strategies
across three datasets and seven tasks, the amount
of experiments is high. This did not allow for
further investigation of other methods for measur-
ing uncertainty such as ensemble methods (Laksh-
minarayanan et al., 2017), different classification
models, the extensive turning of hyperparameters,
or even different training paradigms like low-rank
adaptation (Hu et al., 2022). Lastly, a limitation of
our annotator selection strategies is that they rely
on a small annotation history. This is why we re-
quire a warmup phase for some of the strategies, for
which we decided to take a random sample of anno-
tations. Incorporating informed warmup strategies,
incorporating ACAL strategies that do not rely on
annotator history, or making use of more elaborate
hybrid human–AI approaches (van der Meer et al.,
2024a) may positively impact its performance and
data efficiency.
Ethical Considerations
Our goal is to approximate a good representation of
human judgments over subjective tasks. We want to
highlight the fact that the performance of the mod-
els differs a lot depending on which metric is used.
We tried to account for a less majority-focussed
view when evaluating the models which is very
important, especially for more human-centered ap-
plications, such as hate-speech detection. However,
the evaluation metrics we use do not fully capture
the diversity of human judgments , but just that of
labeling behavior . The selection of metrics should
align with the specific goals and motivations of the
application, and there is a pressing need to develop
more metrics to accurately reflect human variability
in these tasks.
Our experiments are conducted on English
datasets due to the scarcity of unaggregated
datasets in other languages. In principle, ACAL
can be applied to other languages (given the avail-
ability of multilingual models to semantically em-
bed textual items for some particular strategies used
in this work). We encourage the community to en-
rich the dataset landscape by incorporating more
perspective-oriented datasets in various languages,
ACAL potentially offers a more efficient method
for creating such datasets in real-world scenarios.Acknowledgements
This research was partially funded by the Nether-
lands Organisation for Scientific Research (NWO)
through the Hybrid Intelligence Centre via the
Zwaartekracht grant (024.004.022) and by the
Hasler Foundation through the FactCheck project
at Idiap. We would like to thank Gabriella Lapesa
for her valuable feedback on earlier versions of
this paper. We would also like to thank the ARR
reviewers for their helpful feedback.
References
Lora Aroyo, Alex Taylor, Mark Díaz, Christopher
Homan, Alicia Parrish, Gregory Serapio-García,
Vinodkumar Prabhakaran, and Ding Wang. 2023.
DICES dataset: Diversity in conversational ai evalu-
ation for safety. In Advances in Neural Information
Processing Systems , volume 36, pages 53330–53342.
Curran Associates, Inc.
Lora Aroyo and Chris Welty. 2015. Truth is a lie: Crowd
truth and the seven myths of human annotation. AI
Magazine , 36(1):15–24.
Joris Baan, Wilker Aziz, Barbara Plank, and Raquel
Fernández. 2022. Stop measuring calibration when
humans disagree. Proceedings of the 2022 Confer-
ence on Empirical Methods in Natural Language
Processing , pages 1892–1915.
Michiel Bakker, Martin Chadwick, Hannah Sheahan,
Michael Tessler, Lucy Campbell-Gillingham, Jan
Balaguer, Nat McAleese, Amelia Glaese, John
Aslanides, Matt Botvinick, and Christopher Sum-
merfield. 2022. Fine-tuning language models to find
agreement among humans with diverse preferences.
InAdvances in Neural Information Processing Sys-
tems, volume 35, pages 38176–38189. Curran Asso-
ciates, Inc.
Connor Baumler, Anna Sotnikova, and Hal Daumé III.
2023. Which examples should be multiply anno-
tated? active learning when annotators may disagree.
InFindings of the Association for Computational
Linguistics: ACL 2023 , pages 10352–10371. ACL.
Tilman Beck, Hendrik Schuff, Anne Lauscher, and Iryna
Gurevych. 2024. Sensitivity, performance, robust-
ness: Deconstructing the effect of sociodemographic
prompting. In Proceedings of the 18th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
2589–2615, St. Julian’s, Malta. Association for Com-
putational Linguistics.
Federico Cabitza, Andrea Campagner, and Valerio
Basile. 2023. Toward a perspectivist turn in ground
truthing for predictive computing. Proceedings
of the AAAI Conference on Artificial Intelligence ,
37(6):6860–6868.Katherine M. Collins, Umang Bhatt, and Adrian Weller.
2022. Eliciting and learning with soft labels from
every annotator. Proceedings of the AAAI Confer-
ence on Human Computation and Crowdsourcing ,
10(1):40–52.
Aida Mostafazadeh Davani, Mark Díaz, and Vinodku-
mar Prabhakaran. 2022. Dealing with disagreements:
Looking beyond the majority vote in subjective an-
notations. Transactions of the Association for Com-
putational Linguistics , 10:92–110.
Alexander Erdmann, David Joseph Wrisley, Benjamin
Allen, Christopher Brown, Sophie Cohen-Bodénès,
Micha Elsner, Yukun Feng, Brian Joseph, Béatrice
Joyeux-Prunel, and Marie Catherine de Marneffe.
2019. Practical, efficient, and customizable active
learning for named entity recognition in the digital
humanities. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics , NAACL ’19, pages
2223–2234, Minneapolis, Minnesota, USA. ACL.
Jesse Graham, Jonathan Haidt, Sena Koleva, Matt
Motyl, Ravi Iyer, Sean P. Wojcik, and Peter H. Ditto.
2013. Moral foundations theory: The pragmatic va-
lidity of moral pluralism. In Advances in Experi-
mental Social Psychology , volume 47, pages 55–130.
Elsevier, Amsterdam, the Netherlands.
Cornelia Gruber, Katharina Hechinger, Matthias Assen-
macher, Göran Kauermann, and Barbara Plank. 2024.
More labels or cases? assessing label variation in nat-
ural language inference. In Proceedings of the Third
Workshop on Understanding Implicit and Underspec-
ified Language , pages 22–32, Malta. Association for
Computational Linguistics.
Joe Hoover, Gwenyth Portillo-Wightman, Leigh
Yeh, Shreya Havaldar, Aida Mostafazadeh Davani,
Ying Lin, Brendan Kennedy, Mohammad Atari,
Zahra Kamel, Madelyn Mendlen, Gabriela Moreno,
Christina Park, Tingyee E. Chang, Jenna Chin, Chris-
tian Leong, Jun Yen Leung, Arineh Mirinjian, and
Morteza Dehghani. 2020. Moral foundations twit-
ter corpus: A collection of 35k tweets annotated for
moral sentiment. Social Psychological and Personal-
ity Science , 11:1057–1071.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 1120–1130, Atlanta, Georgia.
Association for Computational Linguistics.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations .
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.TinyBERT: Distilling BERT for natural language un-
derstanding. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020 , pages 4163–
4174, Online. Association for Computational Lin-
guistics.
Kamil Kanclerz, Konrad Karanowski, Julita
Bielaniewicz, Marcin Gruza, Piotr Miłkowski,
Jan Kocon, and Przemyslaw Kazienko. 2023. PALS:
Personalized active learning for subjective tasks in
NLP. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 13326–13341, Singapore. Association for
Computational Linguistics.
Urja Khurana, Eric Nalisnick, Antske Fokkens, and
Swabha Swayamdipta. 2024. Crowd-calibrator: Can
annotator disagreement inform calibration in subjec-
tive tasks? In First Conference on Language Model-
ing, Philadelphia, PA.
Johannes Kiesel, Milad Alshomary, Nailia Mirzakhme-
dova, Maximilian Heinrich, Nicolas Handke, Hen-
ning Wachsmuth, and Benno Stein. 2023. Semeval-
2023 task 4: Valueeval: Identification of human val-
ues behind arguments. In 17th International Work-
shop on Semantic Evaluation , SemEval ’23, pages
2290–2306, Toronto, Canada. Association for Com-
putational Linguistics.
Balaji Lakshminarayanan, Alexander Pritzel, and
Charles Blundell. 2017. Simple and scalable pre-
dictive uncertainty estimation using deep ensembles.
InAdvances in Neural Information Processing Sys-
tems, volume 30. Curran Associates, Inc.
Elisa Leonardelli, Stefano Menini, Alessio
Palmero Aprosio, Marco Guerini, and Sara
Tonelli. 2021. Agreeing to disagree: Annotating
offensive language datasets with annotators’ dis-
agreement. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 10528–10539, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Roger X. Lera-Leri, Enrico Liscio, Filippo Bistaffa,
Catholijn M. Jonker, Maite Lopez-Sanchez,
Pradeep K. Murukannaiah, Juan A. Rodriguez-
Aguilar, and Francisco Salas-Molina. 2024.
Aggregating value systems for decision support.
Knowledge-Based Systems , 287:111453.
Enrico Liscio, Oscar Araque, Lorenzo Gatti, Ionut Con-
stantinescu, Catholijn Jonker, Kyriaki Kalimeri, and
Pradeep Kumar Murukannaiah. 2023a. What does
a text classifier learn about morality? an explain-
able method for cross-domain comparison of moral
rhetoric. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 14113–14132, Toronto,
Canada. ACL.
Enrico Liscio, Roger Lera-Leri, Filippo Bistaffa,
Roel I.J. Dobbe, Catholijn M. Jonker, Maite Lopez-
Sanchez, Juan A. Rodriguez-Aguilar, and Pradeep K.Murukannaiah. 2023b. Value inference in sociotech-
nical systems. In Proceedings of the 2023 Interna-
tional Conference on Autonomous Agents and Multi-
agent Systems , AAMAS ’23, page 1774–1780, Rich-
land, SC. International Foundation for Autonomous
Agents and Multiagent Systems.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In International Confer-
ence on Learning Representations .
Katerina Margatina and Nikolaos Aletras. 2023. On
the limitations of simulating active learning. In Find-
ings of the Association for Computational Linguis-
tics: ACL 2023 , pages 4402–4419, Toronto, Canada.
Association for Computational Linguistics.
Negar Mokhberian, Myrl Marmarelis, Frederic Hopp,
Valerio Basile, Fred Morstatter, and Kristina Lerman.
2024. Capturing perspectives of crowdsourced anno-
tators in subjective learning tasks. In Proceedings of
the 2024 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (Volume 1: Long
Papers) , pages 7337–7349, Mexico City, Mexico. As-
sociation for Computational Linguistics.
Rafael Müller, Simon Kornblith, and Geoffrey E Hin-
ton. 2019. When does label smoothing help? In
Advances in Neural Information Processing Systems ,
volume 32. Curran Associates, Inc.
Yixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What
can we learn from collective human opinions on nat-
ural language inference data? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 9131–9143,
Online. Association for Computational Linguistics.
Matthias Orlikowski, Paul Röttger, Philipp Cimiano,
and Dirk Hovy. 2023. The ecological fallacy in
annotation: Modeling human label variation goes
beyond sociodemographics. In Proceedings of the
61st Annual Meeting of the Association for Compu-
tational Linguistics Volume 2: Short Papers , pages
1017–1029. ACL.
Joshua C. Peterson, Ruairidh M. Battleday, Thomas L.
Griffiths, and Olga Russakovsky. 2019. Human un-
certainty makes classification more robust. In Pro-
ceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) .
Barbara Plank. 2022. The “problem” of human label
variation: On ground truth in data, modeling and
evaluation. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Process-
ing, pages 10671–10682, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.
John Rawls. 1973. A Theory of Justice . Oxford Univer-
sity Press, Oxford.
Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po Yao
Huang, Zhihui Li, Brij B. Gupta, Xiaojiang Chen,
and Xin Wang. 2021. A survey of deep active learn-
ing. ACM Computing Surveys , 54(9):1–40.Pratik Sachdeva, Renata Barreto, Geoff Bacon, Alexan-
der Sahn, Claudia von Vacano, and Chris Kennedy.
2022. The measuring hate speech corpus: Leverag-
ing rasch measurement theory for data perspectivism.
InProceedings of the 1st Workshop on Perspectivist
Approaches to NLP @LREC2022 , pages 83–94, Mar-
seille, France. European Language Resources Asso-
ciation.
Burr Settles. 2012. Active Learning . Morgan & Clay-
pool.
Dapeng Tao, Jun Cheng, Zhengtao Yu, Kun Yue, and
Lizhen Wang. 2019. Domain-weighted majority vot-
ing for crowdsourcing. IEEE Transactions on Neural
Networks and Learning Systems , 30(1):163–174.
Alexandra N Uma, Tommaso Fornaciari, Dirk Hovy, Sil-
viu Paun, Barbara Plank, and Massimo Poesio. 2021.
Learning from disagreement: A survey. Journal of
Artificial Intelligence Research , 72:1385–1470.
Michiel van der Meer, Enrico Liscio, Catholijn Jonker,
Aske Plaat, Piek V ossen, and Pradeep Murukannaiah.
2024a. A hybrid intelligence method for argument
mining. Journal of Artificial Intelligence Research ,
80:1187–1222.
Michiel van der Meer, Piek V ossen, Catholijn Jonker,
and Pradeep Murukannaiah. 2023. Do differences
in values influence disagreements in online discus-
sions? In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Process-
ing, pages 15986–16008, Singapore. Association for
Computational Linguistics.
Michiel van der Meer, Piek V ossen, Catholijn Jonker,
and Pradeep Murukannaiah. 2024b. An empirical
analysis of diversity in argument summarization. In
Proceedings of the 18th Conference of the European
Chapter of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2028–2045,
St. Julian’s, Malta. Association for Computational
Linguistics.
Xinpeng Wang and Barbara Plank. 2023. ACTOR: Ac-
tive learning with annotator-specific classification
heads to embrace human label variation. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pages 2046–2052,
Singapore. Association for Computational Linguis-
tics.
Leon Weber-Genzel, Siyao Peng, Marie-Catherine
de Marneffe, and Barbara Plank. 2024. Varierr nli:
Separating annotation error from human label varia-
tion. arXiv preprint arXiv:2403.01931 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019. Huggingface’s transformers: State-of-
the-art natural language processing. arXiv preprint
arXiv:1910.03771 .Ye Zhang, Matthew Lease, and Byron Wallace. 2017.
Active discriminative text representation learning.
Proceedings of the AAAI Conference on Artificial
Intelligence , 31(1).
Zhisong Zhang, Emma Strubell, and Eduard Hovy. 2022.
A survey of active learning for natural language pro-
cessing. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 6166–6190, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Yuekai Zhao, Haoran Zhang, Shuchang Zhou, and Zhi-
hua Zhang. 2020. Active learning approaches to
enhancing neural machine translation. In Findings
of the Association for Computational Linguistics ,
EMNLP 2020, pages 1796–1806, Online. ACL.A Detailed Experimental Setup
A.1 Dataset details
We provide an overview of the datasets used in our
work in Table A1. We split the data on samples,
meaning that all annotations for any given sample
are completely contained in each separate split.
A.2 Hyperparameters
We report the hyperparameters for training passive,
AL, and ACAL in Tables A2, A3, and A4, respec-
tively. For turning the learning rate for passive
learning, on each dataset, we started with a learn-
ing rate of 1e-06 and increased it by a factor of
3 in steps until the model showed a tendency to
overfit quickly (within a single epoch). All other
parameters are kept on their default setting.
A.3 Training details
Experiments were largely run between January and
April 2024. Obtaining the ACAL results for a sin-
gle run takes up to an hour on a Nvidia RTX4070.
For large-scale computation, our experiments were
run on a cluster with heterogeneous computing in-
frastructure, including RTX2080 Ti, A100, and
Tesla T4 GPUs. Obtaining the results of all exper-
iments required a total of 231 training runs, com-
bining: (1) two data sampling strategies, (2) four
annotator sampling strategies, plus an additional
Oracle-based AL approach, (3) a passive learning
approach. Each of the above were run for (1) three
folds, each with a different seed, and (2) the seven
tasks across three datasets. For training all our mod-
els, we employ the AdamW optimizer (Loshchilov
and Hutter, 2019). Our code is based on the Hug-
gingface library (Wolf et al., 2019), unmodified
values are taken from their defaults.
A.4 ACAL annotator strategy details
We provide additional information about the im-
plementations of the strategies used for selecting
annotators to provide a label to a sample.
TSuses a sentence embedding model to represent
the content that an annotator has annotated. We
useall-MiniLM-L6-v22. We select annota-
tors that have not annotated yet (empty history) be-
fore picking from those with a history to prioritize
filling the annotation history for each annotator.
TDcreates an average embedding for the content
annotated by each annotator and selects the most
2https://huggingface.co/
sentence-transformers/all-MiniLM-L6-v2different annotator. We use the same sentence em-
bedding model as TS. To avoid overfitting, we
perform PCA and retain only the top 10 most infor-
mative principal components for representing each
annotator.
A.5 Disagreement rates
We report the average disagreement rates per
dataset and task in Figure A1, for each of the
dataset and task combinations.
B Detailed results overview
B.1 Annotator-Centric evaluation for other
MFTC and MHS tasks
We show the full annotator-centric metrics results
for MFTC betrayal , MFTC loyalty , MHS genocide ,
and MHS respect in Table B1. This follows the
same format at Table 1. The results in this table
also form the basis for Figure 5.
B.2 Training process
In our main paper, we report a condensed version
of all metrics during the training phase of the active
learning approaches. Below, we provide a complete
overview of all approaches over all metrics. The
results can be seen in Figures B1 through B7.Dataset Task ( dimension ) # Samples # Annotators # Annotations # Annotations per item
DICES Safety Judgment 990 172 72,103 72.83
MFTC Morality ( care) 8,434 23 31,310 3.71
MFTC Morality ( loyalty ) 3,288 23 12,803 3.89
MFTC Morality ( betrayal ) 12,546 23 47,002 3.75
MHSHate Speech ( dehumanize ,
genocide ,respect )17,282 7,807 57,980 3.35
Table A1: Overview of the datasets and tasks employed in our work.
Parameter Value
learning rate 1e-04 (constant)
max epochs 50
early stopping 3
batch size 128
weight decay 0.01
Table A2: Hyperparameters for the passive learning.
Parameter Dataset (task) Value
learning rate all 1e-05
batch size all 128
epochs per
roundall 20
num iterations all 10
sample size DICES 79
sample size MFTC (care) 674
sample size MFTC (betrayal) 1011
sample size MFTC (loyalty) 263
sample sizeMHS (dehumanize), MHS
(genocide), MHS (respect)1728
Table A3: Hyperparameters for the oracle-based active
learning approaches.Parameter Dataset Value
learning rate all 1e-05
num iterations DICES 50
num iterations MFTC (all), MHS
(all)20
epochs per
roundDICES, MHS (all) 20
epochs per
roundMFTC (all) 30
sample size DICES 792
sample size MFTC (care) 1250
sample size MFTC (betrayal) 1894
sample size MFTC (loyalty) 512
sample size MHS (dehumanize),
MHS (genocide),
MHS (respect)2899
Table A4: Hyperparameters for the annotator-centric
active learning approaches.00.20.40.60.81050100
EntropyCountDICES ( overall )
00.20.40.60.81020004000
EntropyCountMFTC ( betrayal )
00.20.40.60.810100020003000
EntropyCountMFTC ( care)
00.20.40.60.8105001000
EntropyCountMFTC ( loyalty )
00.20.40.60.8102000400060008000
EntropyCountMHS ( dehumanize )
00.20.40.60.8102000400060008000
EntropyCountMHS ( genocide )
00.20.40.60.8102000400060008000
EntropyCountMHS ( respect )
Figure A1: Histogram of entropy score over all annotations per sample for each dataset and task combination.
0 10000 20000 30000 400000.10.150.20.250.3
# unique annotationsJSDICESSRTR SUTR SRTL SUTL SRTS SUTS SRTD SUTD SRO SUO Passive
0 10000 20000 30000 400000.20.220.240.260.280.3
# unique annotationsJSaDICES
0 10000 20000 30000 400000.350.40.45
# unique annotationsJSwDICES
0 10000 20000 30000 400000.40.50.6
# unique annotationsF1DICES
0 10000 20000 30000 400000.30.350.4
# unique annotationsFa
1DICES
0 10000 20000 30000 400000.060.080.10.120.140.160.18
# unique annotationsFw
1DICES
Figure B1: Validation set performance across all metrics for DICES during training.0 5000 10000 15000 20000 250000.030.040.050.060.070.08
# unique annotationsJSMFTC (care)SRTR SUTR SRTL SUTL SRTS SUTS SRTD SUTD SRO SUO Passive
0 5000 10000 15000 20000 250000.140.160.18
# unique annotationsJSaMFTC (care)
0 5000 10000 15000 20000 250000.220.230.240.250.26
# unique annotationsJSwMFTC (care)
0 5000 10000 15000 20000 250000.50.60.70.8
# unique annotationsF1MFTC (care)
0 5000 10000 15000 20000 250000.450.50.550.60.65
# unique annotationsFa
1MFTC (care)
0 5000 10000 15000 20000 250000.30.350.40.45
# unique annotationsFw
1MFTC (care)
Figure B2: Validation set performance across all metrics for MFTC (care) during training
0 2000 4000 6000 8000 100000.030.040.050.060.070.08
# unique annotationsJSMFTC (loyalty)SRTR SUTR SRTL SUTL SRTS SUTS SRTD SUTD SRO SUO Passive
0 2000 4000 6000 8000 100000.170.180.190.20.210.22
# unique annotationsJSaMFTC (loyalty)
0 2000 4000 6000 8000 100000.220.240.260.28
# unique annotationsJSwMFTC (loyalty)
0 2000 4000 6000 8000 100000.40.50.60.7
# unique annotationsF1MFTC (loyalty)
0 2000 4000 6000 8000 100000.460.480.50.520.540.56
# unique annotationsFa
1MFTC (loyalty)
0 2000 4000 6000 8000 100000.150.20.250.30.35
# unique annotationsFw
1MFTC (loyalty)
Figure B3: Validation set performance across all metrics for MFTC (loyalty) during training0 10000 20000 30000 400000.040.060.08
# unique annotationsJSMFTC (betrayal)SRTR SUTR SRTL SUTL SRTS SUTS SRTD SUTD SRO SUO Passive
0 10000 20000 30000 400000.140.160.180.2
# unique annotationsJSaMFTC (betrayal)
0 10000 20000 30000 400000.210.220.230.24
# unique annotationsJSwMFTC (betrayal)
0 10000 20000 30000 400000.50.60.7
# unique annotationsF1MFTC (betrayal)
0 10000 20000 30000 400000.450.50.550.6
# unique annotationsFa
1MFTC (betrayal)
0 10000 20000 30000 400000.350.40.45
# unique annotationsFw
1MFTC (betrayal)
Figure B4: Validation set performance across all metrics for MFTC (betrayal) during training
0 10000 20000 30000 400000.0750.080.0850.09
# unique annotationsJSMHS (dehumanize)SRTR SUTR SRTL SUTL SRTS SUTS SRTD SUTD SRO SUO Passive
0 10000 20000 30000 400000.380.390.40.410.42
# unique annotationsJSaMHS (dehumanize)
0 10000 20000 30000 400000.440.460.480.50.52
# unique annotationsJSwMHS (dehumanize)
0 10000 20000 30000 400000.240.260.280.30.320.34
# unique annotationsF1MHS (dehumanize)
0 10000 20000 30000 400000.260.280.30.32
# unique annotationsFa
1MHS (dehumanize)
0 10000 20000 30000 40000−0.500.51
# unique annotationsFw
1MHS (dehumanize)
Figure B5: Validation set performance across all metrics for MHS (dehumanize) during training0 10000 20000 30000 400000.040.060.080.1
# unique annotationsJSMHS (genocide)SRTR SUTR SRTL SUTL SRTS SUTS SRTD SUTD SRO SUO Passive
0 10000 20000 30000 400000.250.30.350.4
# unique annotationsJSaMHS (genocide)
0 10000 20000 30000 400000.450.50.55
# unique annotationsJSwMHS (genocide)
0 10000 20000 30000 400000.20.250.3
# unique annotationsF1MHS (genocide)
0 10000 20000 30000 400000.30.40.50.60.7
# unique annotationsFa
1MHS (genocide)
0 10000 20000 30000 40000−0.500.51
# unique annotationsFw
1MHS (genocide)
Figure B6: Validation set performance across all metrics for MHS (genocide) during training
0 10000 20000 30000 400000.080.090.10.110.12
# unique annotationsJSMHS (respect)SRTR SUTR SRTL SUTL SRTS SUTS SRTD SUTD SRO SUO Passive
0 10000 20000 30000 400000.320.340.360.380.40.42
# unique annotationsJSaMHS (respect)
0 10000 20000 30000 400000.440.460.480.50.520.54
# unique annotationsJSwMHS (respect)
0 10000 20000 30000 400000.20.250.30.350.40.45
# unique annotationsF1MHS (respect)
0 10000 20000 30000 400000.250.30.350.40.45
# unique annotationsFa
1MHS (respect)
0 10000 20000 30000 40000−0.500.51
# unique annotationsFw
1MHS (respect)
Figure B7: Validation set performance across all metrics for MHS (respect) during trainingAverage Worst-off
App. F1JS Fa
1JSaFw
1JSw∆%MFTC ( betrayal )SRTR71.5 .047 57.8 .147 42.0 .199 -1.6
SRTL71.2 .046 58.1 .149 43.3 .212 -1.6
SRTS71.2 .051 59.3 .161 43.0 .239 -5.0
SRTD71.0 .046 58.3 .148 42.9 .199 -1.6
SUTR72.6 .042 59.4 .150 41.9 .203 -2.5
SUTL73.6 .045 58.4 .148 43.4 .200 -1.3
SUTS74.0 .045 58.8 .149 43.5 .204 -1.0
SUTD73.2 .044 59.1 .149 42.8 .194 -2.6
SRO 72.1 .046 58.9 .147 43.1 .195 -48.6
SUO 71.8 .047 58.9 .149 43.0 .200 -0.0
PL 75.2 .037 48.1 .199 36.0 .290 0.0MFTC ( betrayal )SRTR66.9 .034 56.4 .177 22.2 .372 -0.4
SRTL68.9 .032 56.3 .176 22.2 .374 -0.3
SRTS67.1 .031 57.3 .176 22.2 .370 -0.3
SRTD68.4 .031 55.1 .175 22.2 .373 -0.3
SUTR61.3 .032 55.7 .177 21.7 .357 -1.1
SUTL66.5 .032 54.1 .177 22.2 .355 -0.8
SUTS62.4 .033 55.6 .177 22.2 .358 -0.9
SUTD64.4 .031 55.8 .177 22.2 .358 -1.3
SRO 71.5 .030 56.0 .176 22.2 .361 -29.1
SUO 66.5 .033 55.9 .177 22.2 .366 -0.1
PL 62.5 .029 51.2 .183 26.1 .309 0.0MHS ( genocide )SRTR26.5 .050 70.0 .227 0.0 .560 -6.3
SRTL28.2 .051 69.8 .225 0.0 .565 -1.7
SRTS28.1 .051 70.0 .224 0.0 .566 -1.7
SRTD28.3 .050 70.2 .224 0.0 .565 -1.7
SUTR32.8 .077 71.1 .229 0.0 .549 -12.6
SUTL27.7 .048 70.7 .231 0.0 .548 -7.9
SUTS26.7 .048 70.9 .231 0.0 .548 -7.9
SUTD27.3 .048 71.2 .229 0.0 .547 -12.6
SRO 28.0 .048 33.9 .387 0.0 .496 -60.1
SUO 33.3 .080 33.1 .390 0.0 .497 -24.7
PL 21.6 .044 70.0 .245 0.0 .570 –MHS ( respect )SRTR41.4 .086 46.0 .331 0.0 .528 -18.8
SRTL40.8 .087 45.6 .331 0.0 .530 -18.8
SRTS41.2 .086 46.1 .331 0.0 .529 -18.8
SRTD40.6 .086 46.0 .331 0.0 .528 -18.8
SUTR32.8 .077 46.6 .323 0.0 .533 -4.9
SUTL41.0 .085 46.3 .323 0.0 .532 -4.9
SUTS41.8 .084 45.9 .324 0.0 .531 -4.9
SUTD40.6 .085 46.2 .324 0.0 .532 -4.9
SRO 41.7 .085 33.9 .387 0.0 .496 -60.1
SUO 33.3 .080 33.1 .390 0.0 .497 -24.7
PL 41.0 .080 25.9 .405 0.0 .587 –
Table B1: Test set results on the MFTC ( betrayal ),
MFTC ( loyalty ), MHS ( genocide ), and MHS ( respect )
datasets. ∆% denotes the reduction in the annotation
budget with respect to passive learning.