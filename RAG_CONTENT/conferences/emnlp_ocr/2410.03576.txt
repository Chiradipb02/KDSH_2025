Table Question Answering for Low-resourced Indic Languages
Vaishali Pal1,2Evangelos Kanoulas1Andrew Yates1Maarten de Rijke1
1University of Amsterdam, The Netherlands
2Discovery Lab, Elsevier, The Netherlands
v.pal, e.kanoulas, a.c.yates, m.derijke@uva.nl
Abstract
TableQA is the task of answering questions
over tables of structured information, returning
individual cells or tables as output. TableQA re-
search has focused primarily on high-resource
languages, leaving medium- and low-resource
languages with little progress due to scarcity
of annotated data and neural models. We ad-
dress this gap by introducing a fully automatic
large-scale table question answering ( tableQA )
data generation process for low-resource lan-
guages with limited budget. We incorporate our
data generation method on two Indic languages,
Bengali and Hindi, which have no tableQA
datasets or models. TableQA models trained
on our large-scale datasets outperform state-
of-the-art LLMs. We further study the trained
models on different aspects, including math-
ematical reasoning capabilities and zero-shot
cross-lingual transfer. Our work is the first
on low-resource tableQA focusing on scalable
data generation and evaluation procedures. Our
proposed data generation method can be ap-
plied to any low-resource language with a web
presence. We release datasets, models, and
code.1
1 Introduction
Tables are ubiquitous for storing information
across domains and data sources such as rela-
tional databases, web articles, Wikipedia pages,
etc. (Deldjoo et al., 2021). Tables introduce new
challenges in machine comprehension not present
in text as they are are not well-formed sentences
but a semi-structured collection of facts (numbers,
long-tail named entities, etc.) (Iyyer et al., 2017;
Jauhar et al., 2016; Jin et al., 2022; Katsis et al.,
2022; Liu et al., 2021; Nan et al., 2022; Pal et al.,
2022; Zhu et al., 2021). Additionally, tables make
position (rows/columns) bias (Lin et al., 2023)
and entity popularity bias (Gupta et al., 2023) se-
vere. The tableQA task introduces novel challenges
1https://github.com/kolk/
Low-Resource-TableQA-Indic-languagescompared to text-based question answering (text-
QA) (Herzig et al., 2020; Liu et al., 2021; Ye et al.,
2023; Yu et al., 2018; Zhao et al., 2022). In ad-
dition to the semi-structured nature of tables, a
tabular context leads to a high frequency of fact-
based questions, mathematical and logical oper-
ations such as arithmetic (Zhu et al., 2021), set,
relational (Jiang et al., 2022; Liu et al., 2021),
and table operations such as table joins (Pal et al.,
2023). Effective tableQA systems not only have
machine comprehension skills, but also numeracy
understanding (Cheng et al., 2022; Liu et al., 2021;
Zhao et al., 2022; Zhu et al., 2021), table reasoning
(Liu et al., 2021; Yu et al., 2018), table summariza-
tion (Zhang et al., 2024; Zhao et al., 2023a) and
answer table generation ability (Pal et al., 2023).
Low-resource tableQA aims to answer questions
over semi-structured tables storing cultural and
region-specific facts in a low-resource language.
Joshi et al. (2020) show that most languages strug-
gle to be represented and are deprived of advances
in NLP research. As manual data collection is slow
and expensive, low-resource languages struggle
with large-scale, annotated data for effective trans-
fer learning solutions. The low-resource setting
(Hedderich et al., 2021; Ruder, 2019) exacerbates
the challenges of tableQA with challenges of data
sparsity, annotated data costs, and lack of trained
models. In contrast to textQA, syntactico-semantic
variations such as agreement and morphology are
not exhibited in tables, but high presence of cultur-
ally significant yet long-tail entities makes adapting
existing high resource datasets and trained mod-
els challenging. Research on low-resource table
inference (Minhas et al., 2022) shows that stan-
dard approaches of translating English datasets for
low-resource data creation are infeasible for tables
due to high translation error as tables are not well-
formed sentences.
Challenges. Our work focuses on studying the fol-
lowing core challenges of low-resource tableQA:arXiv:2410.03576v1  [cs.CL]  4 Oct 2024(1)low-resource tableQA data scarcity and
under-representation of cultural facts.
(2)Existing neural models’ poor alignment in
low-resource languages and a lack of under-
standing of table structure.
This motivates us to explore low-resource tableQA
by designing a low-cost and large-scale automatic
data generation and quality estimation pipeline. We
discuss the process in detail with a low-resource
Indic language, Bengali (spoken extensively in
Bangladesh and India, with over 230 million na-
tive speakers (Karim et al., 2021)), and explore
generalizability with Hindi (570 million speakers).
Our main contributions are as follows:
(1) We introduce low-resource tableQA task .
(2)We design a method for automatically gener-
ating low-resource tableQA data in a scalable
budget-constrained manner.
(3)We release resources to support low-resource
tableQA : Large-scale tableQA datasets and
models for 2 Indic languages, Bengali (Bengali
Table Question Answering ( BanglaTabQA ))
and Hindi (Hindi Table Question Answering
(HindiTabQA )). BanglaTabQA contains 19K
Wikipedia tables, 2Mtraining, 2Kvalidation
and165test samples. HindiTabQA contains
2KWikipedia tables, 643Ktraining, 645vali-
dation and 125test samples.
2 Related Work
TableQA aims to answer a user question from semi-
structured input tables. Prior work on tableQA in
English can be classified as extractive (Herzig et al.,
2020; Yin et al., 2020) or abstractive (Nan et al.,
2022; Pal et al., 2022; Ye et al., 2023; Zhao et al.,
2023b). While extractive tableQA focuses on row
and cell selection (Herzig et al., 2020), abstrac-
tivetableQA generates various types of answers
such as factoid answers (Liu et al., 2021), sum-
maries (Zhang et al., 2024; Zhao et al., 2023b), or
answer tables (Pal et al., 2023). Low-resource set-
ting poses challenges for various NLP tasks. The
low-resource corpus creation (Bhattacharjee et al.,
2022; Das and Saha, 2022; Hasan et al., 2020)
has used automatic annotation efforts by synthe-
sizing a large-scale dataset. Das and Saha (2022)
train a Bengali QA system by developing a syn-
thetic dataset translated from standard English QA
datasets. Bhattacharjee et al. (2022); Hasan et al.
(2020) create low-resource datasets by translating
English datasets to Bengali using neural models.However, these methods are unsuitable due to the
semi-structured ungrammatical sequential repre-
sentation of tables.
3 Task Definition
We formulate low-resource tableQA as a se-
quence generation task. Given a question Q
ofktokens q1, q2, . . . , q k, and table Tcompris-
ing of mrows and ncolumns {h1, . . . , h n,t1,1,
t1,2, . . . , t 1,n, . . . , t m,1,tm,2, . . . , t m,n}where ti,j
is value of the cell at the i-th row and j-th col-
umn and hjis the j-th column header; the low-
resource tableQA model generates an answer table
Tout. The input sequence is the concatenated ques-
tionQ, and linearized input table Tseparated by
special sentinel tokens. The answer, Tout, is also a
linearized sequence. Henceforth, for concreteness,
we will use Bengali as the example low-resource
language. The input to such a model is:
q1q2. . . qk< klam >hi. . . h n< oera 1 >
t1,1. . . t 1,n< oerai>ti,j. . . ti,n. . . < oeram>
tm,1. . . tm,n.
The answer table, Tout, is a linearized sequence:
< klam >Hi. . . H q< oera 1 >o1,1. . . o 1,q< oerai>
oi,j. . . oi,q. . .< oeram>op,1. . . op,q
where oi,jis value at the i-th row and j-th column
andHjis the j-th column header of Tout.
4 Methodology for Dataset Generation
Effective training of low-resourced tableQA re-
quires creation of large-scale datasets of questions,
input and answers tables, to align a language model
to the low-resource language and adapt it to semi-
structured tables and QA task. We address Chal-
lenge 1 by designing an automatic data genera-
tion process to generate a large-scale low resource
tableQA corpus of training and validation samples.
We follow a 3-step pipeline as follows: (i) table
extraction, (ii) question generation, and (iii) an-
swer table extraction. This pipeline applied on
Bengali, as depicted in Figure 1, generates the
BanglaTabQA dataset.
4.1 Table Extraction
English Wikipedia with 6,751,000+ articles is
used for English tableQA datasets (Pasupat and
Liang, 2015), but is insufficient for non-Latin lan-
guages with many cultural topics missing. The
standard process (Bhattacharjee et al., 2022; Das
and Saha, 2022) of translating English datasets to
low-resource languages is biased due to lack of cul-
tural topic/fact representation in English tableQABengali-English Code-Mixed SQL
select  count (` সড়ক  খ  `) from ` ৯ নং  রাজ   সড়ক
( প  মব  )` where  ` সড়ক  খ ` = " িসমলাপাল -
ক ৃ  পুর - রাইপুর - ফ ু লক ু  া -  বনগিরয়া "SQL template
select  count ( column_1 ) from table  where
column_1  = value_column_1
Mono-Lingual Bengali SQL
" িনব   াচন  ক ন  গণনা (` সড়ক  খ `)  থেক  ` ৯ নং
রাজ   সড়ক  ( প  মব  )`  যখােন  ` সড়ক  খ ` =
" িসমলাপাল - ক ৃ  পুর - রাইপুর - ফ ু লক ু  া -  বনগিরয়া "গণনা (` সড়ক
খ `)
১Mono-Lingual Natural Language Question
' ৯ নং  রা  য়  সড়ক  ( প  মব  )`  িলেত
" িসমলাপাল - ক ৃ  পুর - রাইপুর - ফ ু লক ু  া -  বনগিরয়ার
 মাট সংখ া  খু ঁ জ ু ন। '
Bengali-English Code-Mixed SQL
(Translation)
select count(` road section `) from ` 9 no. state road
(West Bengal) ` where ` road section ` = "Shimlapal-
Krishnapur-Raipur-Phoolkushma-Bengoria "
Mono-Lingual Natural Language Question
(Translation)
search for the total number of " Shimlapal-
Krishnapur-Raipur-Phoolkushma-Bengoria " in `9 no.
state road (W est Bengal) `Step 1: Wikipedia T able Extraction
Relational
Databasecount (`road
section `)
1Step 3:  Answer ExtractionStep 2: Natural Language Question Generation
Answer T able      (T ranslation)৯ নং  রাজ   সড়ক  ( প  মব  )SQL keyword T ranslation Dictionary
FROM:  থেক ,  
WHERE:  যখােন , ....
Bengali SQL2NQ modelFigure 1: BanglaTabQA Dataset generation : The SQL elements and table elements are color-coordinated to
represent a single SQL/table element. Dotted rectangles represent translations for accessibility to non-native readers.
datasets. For example, the named-entity Aziraj
ga¢guil (Adhiraj Ganguly), exists only in Bengali
Wikipedia,2and not in English. Further, translat-
ing English tables with machine translation models
is error-prone (Minhas et al., 2022) as tables are
not well-formed sentences but collections of facts.
To mitigate these issues, we extract tables from
Wikipedia dump of the low-resource language.
4.2 Natural Language Question Generation
The question generation is a 2-step process:
Code-mixed SQL query generation. We auto-
matically generate SQL queries over the extracted
low-resourced tables with SQL templates from the
SQUALL dataset (Shi et al., 2020). These tem-
plates have placeholders of table components such
as table name, column names, etc. which are ran-
domly assigned with values from a Wikipedia table.
For example, the template “ select count(c1)
from w where c1 = value ” is instantiated by as-
signing a Bengali table name “ 9 noK rajYo sook (pi±cm
b¢g)” tow, column header “ ejla” toc1, and “ bakua
ejla” tovalue . This results in an executable code-
mixed query “ select count( ejla) from 9 noK ra-
jYo sook (pi±cm b¢g) where ‘ ejla‘ = " bakua ejla "”,
where the SQL keywords are in English but all
table information is in the low-resource language
(Bengali). This leads to 13,345,000executable
Bengali code-mixed queries.
Natural language question generation. We
formulate question generation as a sequence-to-
sequence task by transforming a code-mixed SQL
query into a natural language question ( NQ). To
the best of our knowledge, there exists no sequence
generation models which translates code-mixed
2https://bn.wikipedia.org/wiki/ Aziraj _ ga¢guilSQL queries to low-resource natural language ques-
tions. To train a model for this conversion, we
first transform the code-mixed SQL to a mono-
lingual SQL-like query in the low-resource lan-
guage. As the only linguistic variation exhibited
in the SQL templates is polysemy i.e. a dearth of
one-to-one correspondence between English SQL
keywords and the corresponding low-resource lan-
guage translations, we employ native speakers well-
versed in SQL to manually create one-to-one map-
pings of 27SQL keywords for linguistic trans-
fer of SQL keywords to the corresponding low-
resource language. All table-specific words are
directly copied into the monolingual query. We dis-
card FROM keyword and table name from the query
as it is associated with a single input table. This
leads to a SQL-like monolingual query in the low-
resource language which is a well-formed sentence.
For example, code-mixed Bengali query “ select
count( ‘ ejla‘) from 9 noK rajYo sook (pi±cm b¢g)
where ‘ ejla‘ = " bakua ejla "”, results in a mono-
lingual Bengali query “ in¯bacon korun gNna ( ‘ ejla‘)
eJxaen ‘ ejla‘ = " bakua ejla "”. In contrast to tables
which are invalid sentences, queries and NQare
well-formed sequences and effectively transformed
(SQL to question) with existing encoder-decoder
models. We train a SQL-to-NQ ( SQL2NQ ) model
(mbart-50-large (Liu et al., 2020) backbone) by
translating 68,512training and 9,996validation
samples from semantic parsing datasets: Spider
(Yu et al., 2018), WikiSQL (Zhong et al., 2017),
Atis (Dahl et al., 1994; Price, 1990), and Geoquery
(Zelle and Mooney, 1996) to the low-resource lan-
guage. We use this SQL2NQ model to transform
the queries to NQ. For example, Bengali SQL2NQ
model transforms the aforementioned query to the
NQ “ kbar bakua ejlar Ueêk Aaeq? ”.4.3 Answer Table Extraction
We dump low-resource Wikipedia tables in a re-
lation database. The code-mixed SQL queries
are executed with an SQL compiler over a rela-
tional database comprising of the low-resourced
Wikipedia tables to extract the answer tables.
We execute the 13,345,000Bengali code-mixed
queries to extract the corresponding answer tables.
4.4 Automatic Quality Control
We employ automatic quality control steps to en-
sure quality of the synthetic tableQA data.
Code-mixed query and answer quality control.
We discard all code-mixed queries which execute
to an error with an SQL compiler. This process
follows the quality control in (Pal et al., 2023) and
discards invalid and erroneous queries and samples.
Natural Language Question quality control.
We evaluate the quality of the generated NQwith a
sentence similarity model to discard questions that
have low similarity score with the corresponding
monolingual queries. We found the standard
method of quality evaluation in low-resource
languages (Bhattacharjee et al., 2022; Ramesh
et al., 2022) using the sentence similarity model,
LaBse (Feng et al., 2022), incompatible for
code-mixed SQL- NQdue to low discriminating
ability ( 0.55mean similarity score and 0.13
standard deviation for Bengali SQL-NQ). For
example, LaBse assigns low score ( 0.43) for
positive SQL-NQ pair corresponding to the Ben-
gali query “ SELECT title ORDER BY year DESC
LIMIT 1 " and Bengali NQ “Return the most
recent title corresponding to the most
recent year " (translated for non-native readers),
while it assigns a high score ( 0.8) to negative
pair “ SELECT count( *) WHERE ‘work‘ = The
World of Saudamini " and the unrelated NQ “ How
many games scored a total of 4? ". Table 10
in Appendix A.8 shows more examples. This
necessitates fine-tuning LaBse on low-resourced
SQL-NQ samples. First, we use the translated
semantic parsing samples ( 68,512 training and
9,996SQL-NQ pairs), described in Section 4.2,
as positive pairs and in-batch negatives with
multiple-negatives ranking loss. We call this the
SQL2NQSim model. We select the best checkpoint
by evaluating SQL2NQSim on 1,000randomly
selected hard-negatives (unrelated/negative
SQL-negative question pairs for which pre-trained
0.2
 0.0 0.2 0.4 0.6 0.8 1.0
Similarity Score020040060080010001200FrequencyPositive Samples (SQL-NQ)
Hard NegativesFigure 2: Histogram of similarity scores from fine-tuned
Bengali SQL2NQSim model of 1,000random samples
LaBse assigns a high similarity score ( >0.5)).
We use that checkpoint to obtain similarity scores
of the low-resourced tableQA SQL-NQ pairs and
discard samples with a similarity score lower than
a threshold. We select a good threshold by plotting
a histogram of scores assigned by the SQL2NQSim
model on 10,000 randomly selected positives
and hard-negatives and selecting the inflection
point as the threshold. Figure 2 shows the scores’
histogram for BanglaTabQA . We select a strict
threshold of 0.74(hard-negatives scores taper-off
around 0.7). The final BanglaTabQA dataset, after
quality control, comprises of 2,050,296training
and2,053validation samples.
4.5 Dataset Analysis
In contrast to textQA, tableQA focuses on mathe-
matical questions (Liu et al., 2021; Pal et al., 2023;
Zhu et al., 2021). Following (Liu et al., 2021),
we analyse BanglaTabQA dataset on question com-
plexity, which estimates the difficulty of a ques-
tion based on the corresponding SQL query. As
tableQA enforces mathematical, logical and table
reasoning questions, we further classify tableQA
queries into different classes of table operations
determined by the SQL operators present.
Question complexity. Recent work on tableQA
(Liu et al., 2021) categorizes SQL queries into diffi-
culty levels based on the number of SQL keywords.
We follow this approach and count the number of
keywords for each query. Figure 3 shows that most
ofBanglaTabQA queries have 4SQL keywords.
The longest SQL queries are comprised of 10 key-
words, and the shortest ones of 3SQL keywords.
Mathematical operations. We further catego-
rize each sample based on the operators present inNumber of SQL keywordsFrequency
0%10%20%30%40%
3 4 5 6 7 8 9 10Figure 3: Number of SQL keywords per query his-
togram in the BanglaTabQA dataset.
Operator ClassFrequency
0%10%20%30%40%
set groupBy logical airthmetic sort filtering
Figure 4: Histogram of operator classes in the
BanglaTabQA dataset.
the question. We utilize the SQL query associated
with a question to extract all keywords for classifi-
cation. We categorize data samples into 6operator
classes: arithmetic, sorting, group by, filtering, set
operators, and logical operators. Arithmetic oper-
ators comprises of SQL numeric operations such
assum,count ,min, etc. Sorting refers to ordering
of the answer values in an ascending or descending
order. Group by is the SQL operator of grouping
rows based on a criterion. Filtering corresponds to
SQL operators such as where andhaving used to
filter the input table. Set operators involve union ,
intersect , and except . Finally, we classify logi-
cal operators to be conjunction ( and) and disjunc-
tion ( or) to combine filtering conditions. It also
includes membership operators ( in,between , etc.)
and string matching operator ( like ). The classifi-
cation of the operators is shown in Table 3. Figure
4 shows the distribution of the 6 operator classes
for the BanglaTabQA dataset.
4.6 Test Set
We manually annotate test samples for evaluat-
ing low-resource tableQA models on clean data.
We select unique tables not present in the train-
ing and validation set to avoid data leakage. To
ensure question diversity, we select code-mixedSQL representing each of the 6 operator classes
(discussed in Section 4.5) and distinct from the
training and validation data. Three native anno-
tators well-versed in SQL were employed for an-
notation. One annotator was tasked with question
generation and given the synthetic SQL query, in-
put tables and the answer table, and asked to rewrite
the code-mixed query to a natural language ques-
tion. The remaining two were tasked with evalu-
ation of the question generated by the first anno-
tator. The evaluator-annotators were provided the
code-mixed query, input table, answer table, and
the annotated question and asked to rate the ques-
tion based on fluency. We estimate the annotated
question fluency with a 5-point Likert scale (1-5),
where a higher score indicates a better fluency. The
final score for each question was computed by av-
eraging the scores of the evaluator-annotators. For
BanglaTabQA , we manually annotate 165test sam-
ples. We estimate an inter-annotator agreement
with Fliess’s Kappa score (Fleiss, 1971) of 0.82,
indicating strong agreement among the annotators.
The average fluency score across test set questions
was4.3, indicating high fluency.
4.7 Generalizability of Dataset Methodology
We study the generalizability of the dataset gener-
ation method by repeating the process on another
Indic language: Hindi (Hi) with more than 602
million speakers. To the best of our knowledge,
there is no existing tableQA data for Indic lan-
guages. Hindi text is in Devanagari script which
is different from Bengali written in Eastern-Nagari
(Bengali-Assamese) script. This requires tableQA
models to be trained on large-scale Hindi datasets
for good alignment. Following the dataset creation
process in Section 4, we extract 1,921Hindi ta-
bles from the respective Wikipedia dumps. We
generate 82,00,000Hindi code-mixed queries au-
tomatically to extract answer tables and generate
the Hindi natural language questions. The final Hin-
diTabQA dataset comprises of 643,434synthetic
training, 645synthetic validation samples and 121
manually annotated test samples.
5 Experimental Setup
We address Challenge 2 by studying the effec-
tiveness of state-of-the-art models (baselines) in
Bengali table QA . Experimental results (Section
6) show the need for a large-scale BanglaTabQA
dataset and model training. We analyze severalmodels’ effectiveness in Bengali language, mathe-
matical/table operations and generalizability, thus
providing a measure of the dataset quality and con-
sequently the dataset creation methodology.
Baselines. We perform 2-shot in-context learn-
ing ( ICL) to adapt large language model ( LLM )s
toBanglaTabQA task. We further fine-tune an
encoder-decoder model. The demonstrations are
the concatenated question and flattened input ta-
ble with the flattened answer table. We use the
following models as baselines:
(1)En2Bn: We fine-tune an encoder-decoder
model, mbart-50-large , with 25,000random
samples from MultiTabQA’s (Pal et al., 2023)
pre-training data translated to Bengali using
Google translate. MultiTabQA used SQUALL
templates to generate their queries and have
the same distribution as BanglaTabQA queries.
However, the input tables of MultiTabQA are
English wiki-tables from WikiTableQuestions
dataset (Pasupat and Liang, 2015) and are not
representative of Bengali cultural topics/facts.
(2)OdiaG (Parida et al., 2023) is Llama-7b (Tou-
vron et al., 2023) adapter-tuned (LoRA (Hu
et al., 2022)) on 252k Bengali instruction set.3
(3)GPT: GPT-3.5 (Brown et al., 2020) per-
forms well on English tableQA (Zha et al.,
2023). GPT-4 (OpenAI et al., 2023) out-
performs other LLM s (Chinchilla (Hoffmann
et al., 2022), PaLM (Chowdhery et al., 2022))
in low-resource languages, including Bengali
and Hindi, on various tasks ( 14,000multiple-
choice problems on 57subjects in a translated
MMLU benchmark (Hendrycks et al., 2021)).
BanglaTabQA models. Bengali tableQA mod-
els must understand both Bengali script andnu-
merals , crucial for mathematical operations. How-
ever, Bengali numbers are not present in many state-
of-the-art Indic models’ (Dabre et al., 2022; Gala
et al., 2023)4vocabulary. To the best of our knowl-
edge, there is no open-access generative model
which understands both table structure and Bengali.
We train the following models on BanglaTabQA as
they support Bengali and Hindi numbers and text:
(1)BnTQA-mBart: mbart-50-large (Liu et al.,
2020) is a multi-lingual encoder-decoder
model with support for 50 languages.
(2)BnTQA-M2M: m2m100_418M (Fan et al.,
3OdiaGenAI/odiagenAI-bengali-lora-model-v1
4ai4bharat/IndicBART2021) is a multi-lingual encoder-decoder
model with support for 100 languages.
(3)BnTQA-llama: We train Llama-7B , on
BanglaTabQA dataset with parameter-efficient
fine-tuning (PEFT) on LoRA adapters.
We train BnTQA-mBart andBnTQA-M2M with 128
batch size and BnTQA-llama with 16 batch size
and 4-bit quantization. All models are trained with
1e-4 learning rate on a single A6000 48GB GPU
for 5 epochs with 1024 maximum sequence length.
5.1 HindiTabQA
We assess the generalizabiltiy of our data gen-
eration process by training and evaluating Hin-
diTabQA models. All hyper-parameters and ex-
perimental setup are the same as Bengali.
Baselines. We use the following baselines:
(1)En2Hi: Similar to En2Bn , we fine-tune
mbart-50-large with 25,000 random sam-
ples from MultiTabQA, translated to Hindi.
(2)GPT : We perform 2-shot ICL on the best
LLMs on Bengali, GPT-3.5 andGPT-4 .
(3)OpHathi : We perform 2-shot ICL on
OpenHathi-7B-Hi-v0.1-Base , an open-
source LLM based on llama-7b and trained
on Hindi, English, and Hinglish text.
HindiTabQA models. We train the following
models on the HindiTabQA dataset:
(1)HiTQA-llama: Similar to Bengali, we fine-
tune Llama-7b on HindiTabQA dataset.
(2)HiTQA-M2M: Similar to Bengali, we fine-
tune m2m100_418M on HindiTabQA dataset.
(3)HiTQA-mBart: Similar to Bengali, we fine-
tune mbart-50-large , on HindiTabQA.
(4)HiTQA-BnTQA: BnTQA-mBart , trained on
BanglaTabQA provides a warm start. We fine-
tune it on HindiTabQA for better convergence.
5.2 Evaluation Metrics
The answer table requires both table structure and
content evaluation rendering standard text simi-
larity metrics (Rouge, BLEU, etc.) inappropriate.
We instead evaluate with tableQA evaluation met-
rics (Pal et al., 2023). Henceforth, F1 scores are the
harmonic mean of the precision and recall scores.
(1)Table Exact Match Accuracy (Tab) measures
the percentage of generated answer which
match exactly to the target answer tables.
(2)Row Exact Match F1 (Row) : Row EM pre-
cision is the percentage of correctly predicted
rows among all predicted rows. Row EM recallModelBengali Hindi
Validation Set scores (%) Test Set scores (%) Validation Set scores (%) Test Set scores (%)
Tab Row Col Cell Tab Row Col Cell Tab Row Col Cell Tab Row Col Cell
En2(Bn/Hi) 0.05 3 .06 0 .20 3 .07 0 .00 4 .73 0 .00 4 .73 0.00 3 .37 0 .47 3 .43 0 .00 5 .03 8 .26 5 .03
OdiaG 0.00 3 .89 0 .00 3 .89 0 .69 1 .77 0 .69 1 .42− − − − − − − −
OpHathi − − − − − − − − 0.00 0 .00 0 .00 0 .00 0 .00 0 .11 0 .37 0 .74
GPT-3.5 1.14 4 .81 1 .67 5 .14 6 .04 10 .06 9 .12 9 .84 4.81 8 .94 4 .99 9 .71 8 .20 10 .29 7 .10 9 .81
GPT-4 0.00 13 .57 5 .43 14 .65 26 .8338.67 26.7436.51 15.53 22 .60 16 .02 22 .25 11 .11 21 .49 11 .76 20 .84
BnTQA HiTQA
-llama 60.08 68.30 60.47 68.30 9.41 12 .35 9.85 11 .8714.76 9 .92 14 .13 7 .29 13 .11 9 .71 11 .11 7 .66
-mBart 56.63 64 .10 56 .79 64 .3135.88 33.1635.88 33.1692.09 87 .97 92 .02 87 .97 33 .06 43 .35 33 .88 43 .35
-M2M 45.31 58 .07 45 .29 58 .04 28 .05 34 .55 28 .05 34 .5589.55 85 .32 89 .34 85 .15 28 .93 33 .11 28 .92 33 .10
-BnTQA − − − − − − − − 92.40 88 .10 92 .42 88 .1241.32 47.26 41.32 47.26
Table 1: Baseline, BnTQA-X and HiTQA-X models’ scores. -X represents the backbone architecture of a fine-tuned
model and −entries are for incompatible models in a low-resourced language (Bengali or Hindi).
is the percentage of correctly predicted rows
among all target rows.
(3)Column Exact Match F1 (Col) : Column
EM precision is the percentage of correctly
predicted columns and corresponding headers
among all predicted columns. Column EM
recall is the percentage of correctly predicted
columns among all target columns.
(4)Cell Exact Match F1 (Cell) is the most relaxed
metric. Cell EM precision is the percentage of
correctly generated cells among all predicted
cells. Cell EM recall is the percentage of cor-
rectly predicted cells among all target cells.
6 Results
Baselines. As reported in Table 1, GPT-4 performs
the best on our test set with a table EM accuracy
of26.83%.GPT-3.5 under-performs GPT-4 but
is better than open-sourced LLM s. Open-source
LLM s,OdiaG is pre-trained on Bengali text data
but not on structured table data. The low accuracy
ofOdiaG (0.69%) can be attributed to the mod-
els’ lack of table understanding and table specific
question which differs significantly from text-based
tasks on which it has been pre-trained on as shown
in examples in Appendix A.6. Baseline encoder-
decoder model, En2Bn , fine-tuned on translated
tableQA data, correctly generates 4.73% of rows
and cells and under-performs OdiaG , but is better
than TableLlama . Although fine-tuning improves
Bengali understanding, the low scores can be at-
tributed to the erroneous translations of English
tables in the MultiTabQA dataset which corrobo-
rate with (Minhas et al., 2022) that table translation
leads to error-propagation to down-stream QA task.
Further, a lack of culture-specific tables in the Mul-
tiTabQA pre-training dataset leads to downgradedperformance on topics in the BanglaTabQA test
set. In conclusion, GPT-4 is able to perform table
reasoning in low-resourced Bengali, but is very
expensive and closed-source, limiting it’s accessi-
bility and utility. GPT-3.5 ’s and all open-access
baseline models’ low scores demonstrates the need
for both task and language adaptation with a large-
scale dataset for training accessible open-source
language models for low-resourced tableQA.
BanglaTabQA models. Parameter-efficient fine-
tuned Llama models, BnTQA-llama , achieves com-
parable results to GPT-3.5 . Table 1 shows that
fine-tuned encode-decoder models, BnTQA-mBart
andBnTQA-M2M , outperforms GPT-4 on table exact
match accuracy (EM) and column EM F1, but not
for row and cell EM F1. This can be attributed to
incorrect header generation of GPT-4 reflecting in
column and subsequently table EM scores. Apart
from GPT-4 , all other baseline models underper-
form BanglaTabQA encoder-decoder models by a
large margin on all metrics. BnTQA-llama overfits
to the validation set, and does not generalize well to
the test set. The low scores of PEFT compared to
full fine-tuning (FT) can be attributed to insufficient
alignment of the frozen parameters of the backbone
Llama model and sub-optimal tokenization of Ben-
gali which has been observed in SentencePiece
tokenizers in non-Latin languages (Banerjee and
Bhattacharyya, 2018; Cui et al., 2023). The results
establishes the quality of the BanglaTabQA dataset
and its effectiveness in adapting neural models to
both language and table understanding.
HindiTabQA models. We follow a similar ex-
perimental setup as discussed in Section 5. We
report the results in Table 1. We observe that
HiTQA-BnTQA , initialized with with BnTQA-mbart ,
outperforms all HindiTabQA models and achievesModel No post-processing With post-processing
BnTQA Tab Row Col Cell Tab Row Col Cell
-llama 0.00 0.00 0 .00 0.26 5 .74 17 .59 5 .69 15 .49
-mBart 0.00 8.70 10 .74 8.70 19 .01 20 .74 19 .01 20 .74
-M2M 0.00 0.00 0 .00 0.00 18 .18 35 .80 18 .18 35 .80
Table 2: Zero-shot cross-lingual transfer scores of Bn-
TQA models on Hindi test data.
a test score of 41.32%. Similar to BanglaTabQA ,
HiTQA-mBart outperforms HiTQA-M2M with a ta-
ble EM test score of 33.06% and28.93% respec-
tively. HiTQA-llama underperforms compared to
the encoder-decoder models. All models trained on
the HindiTabQA dataset outperform the two-shot
in-context learning baseline models. The results
follow a similar trend to BanglaTabQA models and
prove that our data generation process is general-
izable and the HindiTabQA dataset is able to align
neural models for tableQA task in Hindi.
6.1 Zero-shot Cross-lingual Transfer
We further study generalizability, by selecting the
best performing language, Bengali, and evaluat-
ing the BanglaTabQA models on Hindi test set in
a zero-shot setting without training on Hindi data.
This setup allows us to study the cross-lingual trans-
fer of BanglaTabQA models to Hindi with a dif-
ferent script, and evaluate how well the models
generalize to new out-of-distribution input tables.
BanglaTabQA models are able to perform table
reasoning in Hindi indicating semantic informa-
tion transfer across languages. We demonstrate
some examples in the Appendix A.7. Table head-
ers and numbers generated from math operations
are often in Bengali instead of Hindi (Example 7).
Extractive questions are generated correctly (Ex-
ample 8). Table 2 lists the zero-shot cross-lingual
scores using the original predictions (named “No
Post-Processing”) of the BanglaTabQA models on
the Hindi test set defined in Section 4.7. Addition-
ally, we perform post-processing of the predictions
to translate the predicted tables’ values to Hindi.
As translating tables, composed of numbers and
entities, with machine translation systems is unreli-
able (Minhas et al., 2022), we follow an automatic
post-processing pipeline to transform predicted an-
swer tables to Hindi. First, all lexical occurrence
of Bengali digits in predictions are replaced with
Hindi digits using a dictionary. Next, all lexical
occurrence of SQL keyword in Bengali in the pre-
diction headers are replaced with a Bengali-to-SQL
keyword mapping and subsequently with a SQL-to-Hindi mapping described in Section 4. This
fixes most of the Bengali presence in the predic-
tions. Finally, we translate the predicted column
names/values in Bengali to Hindi with Google
translate. Table 2 shows that post-processing in-
creases the scores, demonstrating the generaliz-
ability of BanglaTabQA models’ table reasoning
capabilities on out-of-domain Hindi tables with un-
seen cultural entities. This further demonstrates the
quality and utility of the BanglaTabQA dataset and
our proposed data generation method and quality
of the trained models.
6.2 Mathematical Operator classes
We study how BanglaTabQA and HindiTabQA
datasets aid in Bengali and Hindi numeracy and
math understanding by evaluating BnTQA-mBart
and HiTQA-mBart on6categories of operator
classes (Section 4.5). We observe in Table 4 that
BnTQA-mbart performs best on groupBy (G) op-
erators with a table EM accuracy of 50.00% and
HiTQA-mBart onSorting (So) operators with a ta-
ble EM accuracy of 39.05%. Both models are able
to generalize to unseen tables in the respective lan-
guages’ test sets. This affirms that BanglaTabQA
andHindiTabQA dataset aids mathematics reason-
ing of the trained models and enhances numeracy
understanding in the low-resourced language.
7 Conclusion
Our work introduces tableQA for the low-resource
languages. We propose a methodology for large-
scale dataset development on limited budget and
automatic quality control which can be applied over
any low-resource language with a web-presence.
We discuss in detail the application of the method-
ology with an Indic Language, Bengali, for which
we release a large-scale dataset, BanglaTabQA .
We further demonstrate generalizability of the pro-
cess with another language, Hindi. We assess the
datasets’ quality by effectively training different
Bengali and Hindi tableQA models and conducting
various experiments on model efficacy. Our studies
on different operator classes and zero-shot cross-
lingual transfer demonstrate that models trained
with our dataset generalize well to unseen tables.
Our proposed methodology can promote further re-
search in low-resource tableQA , while our released
dataset and models can be used to further explore
tableQA for Bengali and Hindi.Operator class Operations
arithmetic (A) count, sum, average, max, min
sorting (So) ascending, descending
groupBy (G) table column/row grouping
filtering (F) where, having
set (Se) union, intersect, except
logical (L) and, or, not, in, not in, between
Table 3: Classification of tableQA operations.OpBengali Hindi
Tab Row Col Cell Tab Row Col Cell
A 39.66 55 .64 39 .67 55 .64 35.06 41 .71 35 .07 41 .71
So 25.00 25 .00 25 .00 25 .00 39.05 42.74 39.05 42.74
G 50.00 76.92 50.00 76.92 33.33 35 .96 33 .33 35 .96
F 37.78 35 .86 37 .77 35 .86 23.23 26 .35 23 .23 21 .67
Se 36.11 49 .10 36 .11 49 .10 5.00 11 .11 5 .00 11 .11
L 34.38 13 .23 34 .38 13 .23 25.58 27 .38 25 .58 27 .38
Table 4: XTQA-mBart test set scores (%) on Operator Class
(Op); X is a low-resourced language (Bn or Hi).
Limitations
We design a scalable automatic tableQA data gen-
eration method and apply it on with two low-
resourced languages: Bengali and Hindi. We re-
lease two tableQA datasets: BanglaTabQA and
HindiTabQA and several models as outcome. Our
main results in Table 1 demonstrate successful
adaptation of neural models to low-resourced
tableQA task. Our extensive experimentation on
generalizability in Section 6.1 and 6.2 shows that
models trained on the BanglaTabQA dataset per-
forms well across all operator classes and general-
ize to unseen languages and tables, proving gener-
alizability of the datasets and methodology.
Our dataset methodology is generalizable, but
it is limited to languages for which unlabelled ta-
bles are available online. For very-low resource
languages with low web presence, our method has
only limited impact. Also, we used SQUALL tem-
plates for query generation, which do not support
multi-table operations or complex queries. We
leave addressing these challenges to future work.
Ethical Considerations
The task and models proposed in the paper is
aimed at closing the gap of resource scarcity in
low-resource languages. To do so, we have used
existing open-source resources publicly available
in the web under MIT, CC-BY-SA-3.0 and MIT,
CC-BY-SA-4.0 licenses. Our dataset is generated
synthetically data and will be released under MIT,
CC-BY-SA-4.0 license. Our synthetic samples use
templates from the SQUALL dataset also released
under MIT, CC-BY-SA-4.0 license. Our test data
splits are manually annotated. We pay each an-
notator C13.27/hour for their efforts. Further, we
have utilized Wikipedia tables from Huggingface
Wikipedia dataset. Wikipedia tables contain infor-
mation about named-entities, facts and events in
the public domain. We do not use any user-specificor sensitive data and information. Our models are
built over open-source encoder-decoder models and
closed-source GPT-3.5. Our work did not explicitly
handle any bias which exists in the aforementioned
pre-trained models or Wikipedia.
Acknowledgements
We thank Elsevier’s Discovery Lab for their
support throughout this project and funding
this work. This work was also supported
by Dutch Research Council (NWO), under
project numbers 024.004.022, NWA.1389.20.183,
KICH3.LTP.20.006, and VI.Vidi.223.166, and the
European Union’s Horizon Europe program under
grant agreement No 101070212. All content rep-
resents the opinion of the authors, which is not
necessarily shared or endorsed by their respective
employers and/or sponsors.
References
Tamali Banerjee and Pushpak Bhattacharyya. 2018.
Meaningless yet meaningful: Morphology grounded
subword-level NMT. In Proceedings of the Sec-
ond Workshop on Subword/Character LEvel Models ,
pages 55–60, New Orleans. Association for Compu-
tational Linguistics.
Abhik Bhattacharjee, Tahmid Hasan, Wasi Ahmad,
Kazi Samin Mubasshir, Md Saiful Islam, Anindya
Iqbal, M. Sohel Rahman, and Rifat Shahriyar.
2022. BanglaBERT: Language model pretraining
and benchmarks for low-resource language under-
standing evaluation in Bangla. In Findings of the
Association for Computational Linguistics: NAACL
2022 , pages 1318–1327, Seattle, United States. Asso-
ciation for Computational Linguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Zhoujun Cheng, Haoyu Dong, Ran Jia, Pengfei Wu,
Shi Han, Fan Cheng, and Dongmei Zhang. 2022.
FORTAP: Using formulas for numerical-reasoning-
aware table pretraining. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1150–
1166, Dublin, Ireland. Association for Computational
Linguistics.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. PaLM: Scaling language
modeling with pathways.
Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient
and effective text encoding for Chinese LLaMA and
Alpaca. arXiv preprint arXiv:2304.08177 .
Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan,
Ratish Puduppully, Mitesh Khapra, and Pratyush Ku-
mar. 2022. IndicBART: A pre-trained model for indic
natural language generation. In Findings of the As-
sociation for Computational Linguistics: ACL 2022 ,
pages 1849–1863, Dublin, Ireland. Association for
Computational Linguistics.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: The ATIS-3 corpus. In Human Language Tech-
nology: Proceedings of a Workshop held at Plains-
boro, New Jersey, March 8-11, 1994 .
Arijit Das and Diganta Saha. 2022. Deep learning
based bengali question answering system using se-
mantic textual similarity. Multimedia Tools Appl. ,
81(1):589–613.Yashar Deldjoo, Johanne R. Trippas, and Hamed Za-
mani. 2021. Towards multi-modal conversational
information seeking. In Proceedings of the 44th In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval , SIGIR ’21,
page 1577–1587, New York, NY , USA. Association
for Computing Machinery.
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi
Ma, Ahmed El-Kishky, Siddharth Goyal, Man-
deep Baines, Onur Celebi, Guillaume Wenzek,
Vishrav Chaudhary, Naman Goyal, Tom Birch, Vi-
taliy Liptchinsky, Sergey Edunov, Edouard Grave,
Michael Auli, and Armand Joulin. 2021. Beyond
English-centric multilingual machine translation. J.
Mach. Learn. Res. , 22(1).
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-
vazhagan, and Wei Wang. 2022. Language-agnostic
bert sentence embedding. Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) .
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin ,
76:378–382.
Jay Gala, Pranjal A Chitale, A K Raghavan, Varun
Gumma, Sumanth Doddapaneni, Aswanth Kumar M,
Janki Atul Nawale, Anupama Sujatha, Ratish Pudup-
pully, Vivek Raghavan, Pratyush Kumar, Mitesh M
Khapra, Raj Dabre, and Anoop Kunchukuttan. 2023.
IndicTrans2: Towards high-quality and accessible
machine translation models for all 22 scheduled In-
dian languages. Transactions on Machine Learning
Research .
Vivek Gupta, Pranshu Kandoi, Mahek V ora, Shuo
Zhang, Yujie He, Ridho Reinanda, and Vivek Sriku-
mar. 2023. TempTabQA: Temporal question answer-
ing for semi-structured tables. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 2431–2453, Singapore.
Association for Computational Linguistics.
Tahmid Hasan, Abhik Bhattacharjee, Kazi Samin, Ma-
sum Hasan, Madhusudan Basak, M. Sohel Rahman,
and Rifat Shahriyar. 2020. Not low-resource any-
more: Aligner ensembling, batch filtering, and new
datasets for Bengali-English machine translation. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 2612–2623, Online. Association for Computa-
tional Linguistics.
Michael A. Hedderich, Lukas Lange, Heike Adel, Jan-
nik Strötgen, and Dietrich Klakow. 2021. A survey
on recent approaches for natural language process-
ing in low-resource scenarios. In Proceedings of
the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 2545–2568,
Online. Association for Computational Linguistics.Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. 2021. Measuring massive multitask language
understanding. Proceedings of the International Con-
ference on Learning Representations (ICLR) .
Jonathan Herzig, Pawel Krzysztof Nowak, Thomas
Müller, Francesco Piccinno, and Julian Eisenschlos.
2020. TaPas: Weakly supervised table parsing via
pre-training. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 4320–4333, Online. Association for Computa-
tional Linguistics.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Si-
monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
and Laurent Sifre. 2022. Training compute-optimal
large language models.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. LoRA: Low-rank adaptation of
large language models. In International Conference
on Learning Representations .
Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.
Search-based neural structured learning for sequen-
tial question answering. In Proceedings of the 55th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1821–
1831, Vancouver, Canada. Association for Computa-
tional Linguistics.
Sujay Kumar Jauhar, Peter D. Turney, and Eduard H.
Hovy. 2016. Tables as semi-structured knowledge
for question answering. In Annual Meeting of the
Association for Computational Linguistics .
Zhengbao Jiang, Yi Mao, Pengcheng He, Graham Neu-
big, and Weizhu Chen. 2022. OmniTab: Pretraining
with natural and synthetic data for few-shot table-
based question answering. In Proceedings of the
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 932–942, Seattle,
United States. Association for Computational Lin-
guistics.
Nengzheng Jin, Joanna Siebert, Dongfang Li, and Qing-
cai Chen. 2022. A survey on table question answer-
ing: Recent advances.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the NLP
world. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
6282–6293, Online. Association for Computational
Linguistics.Md. Rezaul Karim, Sumon Kanti Dey, Tanhim Islam,
Sagor Sarker, Mehadi Hasan Menon, Kabir Hossain,
Md. Azam Hossain, and Stefan Decker. 2021. Deep-
HateExplainer: Explainable hate speech detection in
under-resourced Bengali language.
Yannis Katsis, Saneem Chemmengath, Vishwajeet Ku-
mar, Samarth Bharadwaj, Mustafa Canim, Michael
Glass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen,
Karthik Sankaranarayanan, and Soumen Chakrabarti.
2022. AIT-QA: Question answering dataset over
complex tables in the airline industry. Proceedings
of the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies: Industry Track .
Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adria
de Gispert, and Gonzalo Iglesias. 2023. An inner
table retriever for robust table question answering.
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 9909–9926, Toronto, Canada.
Association for Computational Linguistics.
Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi
Lin, Weizhu Chen, and Jian guang Lou. 2021.
TAPEX: Table pre-training via learning a neural SQL
executor.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising pre-
training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726–742.
Bhavnick Minhas, Anant Shankhdhar, Vivek Gupta, Di-
vyanshu Aggarwal, and Shuo Zhang. 2022. XIn-
foTabS: Evaluating multilingual tabular natural lan-
guage inference. In Proceedings of the Fifth Fact Ex-
traction and VERification Workshop (FEVER) , pages
59–77, Dublin, Ireland. Association for Computa-
tional Linguistics.
Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria
Lin, Neha Verma, Rui Zhang, Wojciech Kry ´sci´nski,
Hailey Schoelkopf, Riley Kong, Xiangru Tang,
Mutethia Mutuma, Ben Rosand, Isabel Trindade,
Renusree Bandaru, Jacob Cunningham, Caiming
Xiong, and Dragomir Radev. 2022. FeTaQA: Free-
form Table Question Answering. Transactions of the
Association for Computational Linguistics , 10:35–49.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,
Jake Berdine, Gabriel Bernadett-Shapiro, Christo-
pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-
laine Boyd, Anna-Luisa Brakman, Greg Brockman,
Tim Brooks, Miles Brundage, Kevin Button, Trevor
Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
Chelsea Carlson, Rory Carmichael, Brooke Chan,Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,
Ruby Chen, Jason Chen, Mark Chen, Ben Chess,
Chester Cho, Casey Chu, Hyung Won Chung, Dave
Cummings, Jeremiah Currier, Yunxing Dai, Cory
Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowl-
ing, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko
Felix, Simón Posada Fishman, Juston Forte, Is-
abella Fulford, Leo Gao, Elie Georges, Christian
Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,
Rapha Gontijo-Lopes, Jonathan Gordon, Morgan
Grafstein, Scott Gray, Ryan Greene, Joshua Gross,
Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse
Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-
hannes Heidecke, Chris Hesse, Alan Hickey, Wade
Hickey, Peter Hoeschele, Brandon Houghton, Kenny
Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu
Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger
Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie
Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,
Ali Kamali, Ingmar Kanitscheider, Nitish Shirish
Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook
Kim, Christina Kim, Yongjik Kim, Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil
Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-
ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea V oss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret
Zoph. 2023. Gpt-4 technical report.
Vaishali Pal, Evangelos Kanoulas, and Maarten de Rijke.
2022. Parameter-efficient abstractive question an-
swering over tables or text. In Proceedings of the
Second DialDoc Workshop on Document-grounded
Dialogue and Conversational Question Answering ,
pages 41–53, Dublin, Ireland. Association for Com-
putational Linguistics.
Vaishali Pal, Andrew Yates, Evangelos Kanoulas, and
Maarten de Rijke. 2023. MultiTabQA: Generating
tabular answers for multi-table question answering.
InACL 2023: The 61st Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 6322–
6634.
Shantipriya Parida, Sambit Sekhar, Guneet Singh
Kohli, Arghyadeep Sen, and Shashikanta Sahoo.
2023. Bengali instruction-tuning model. https:
//huggingface.co/OdiaGenAI .
Panupong Pasupat and Percy Liang. 2015. Composi-
tional semantic parsing on semi-structured tables. In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics and the 7th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 1470–
1480, Beijing, China. Association for Computational
Linguistics.
Patti Price. 1990. Evaluation of spoken language sys-
tems: the ATIS domain. In Speech and Natural Lan-
guage: Proceedings of a Workshop Held at Hidden
Valley, Pennsylvania, June 24-27,1990 .
Gowtham Ramesh, Sumanth Doddapaneni, Aravinth
Bheemaraj, Mayank Jobanputra, Raghavan AK,
Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Ma-
halakshmi J, Divyanshu Kakwani, Navneet Kumar,
Aswin Pradeep, Srihari Nagaraj, Kumar Deepak,
Vivek Raghavan, Anoop Kunchukuttan, Pratyush Ku-
mar, and Mitesh Shantadevi Khapra. 2022. Samanan-
tar: The largest publicly available parallel corpora
collection for 11 Indic languages. Transactions of the
Association for Computational Linguistics , 10:145–
162.
Sebastian Ruder. 2019. The 4 biggest open
problems in NLP. https://www.ruder.io/
4-biggest-open-problems-in-nlp .
Tianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal
Daumé III, and Lillian Lee. 2020. On the poten-tial of lexico-logical alignments for semantic parsing
to SQL queries. In Findings of EMNLP .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei
Huang, and Yongbin Li. 2023. Large language mod-
els are versatile decomposers: Decomposing evi-
dence and questions for table-based reasoning. In
Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval , SIGIR ’23, page 174–184, New
York, NY , USA. Association for Computing Machin-
ery.
Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-
bastian Riedel. 2020. TaBERT: Pretraining for joint
understanding of textual and tabular data. Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics .
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-
ing Yao, Shanelle Roman, Zilin Zhang, and Dragomir
Radev. 2018. Spider: A large-scale human-labeled
dataset for complex and cross-domain semantic pars-
ing and text-to-SQL task. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 3911–3921, Brussels, Bel-
gium. Association for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence - Volume
2, AAAI’96, page 1050–1055. AAAI Press.
Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi
Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang
Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou,
Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye,
Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng,
Jie Xu, Haobo Wang, Gang Chen, and Junbo Zhao.2023. TableGPT: Towards unifying tables, nature
language and commands into one GPT.
Weijia Zhang, Vaishali Pal, Jia-Hong Huang, Evangelos
Kanoulas, and Maarten de Rijke. 2024. Qfmts: Gen-
erating query-focused summaries over multi-table
inputs.
Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang.
2022. MultiHiertt: Numerical reasoning over multi
hierarchical tabular and textual data. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 6588–6600, Dublin, Ireland. Association for
Computational Linguistics.
Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin
Liu, Weijin Zou, Simeng Han, Ruizhe Chen, Xiangru
Tang, Yumo Xu, Dragomir Radev, and Arman Cohan.
2023a. QTSumm: Query-focused summarization
over tabular data. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 1157–1172, Singapore. Associa-
tion for Computational Linguistics.
Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin
Liu, Weijin Zou, Simeng Han, Xiangru Tang, Yumo
Xu, Arman Cohan, and Dragomir Radev. 2023b. QT-
Summ: A new benchmark for query-focused table
summarization.
Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2SQL: Generating structured queries from
natural language using reinforcement learning.
Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao
Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-
Seng Chua. 2021. TAT-QA: A question answering
benchmark on a hybrid of tabular and textual con-
tent in finance. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 3277–3287, Online. Association for
Computational Linguistics.A Appendix
A.1 Bengali SQL2NQSim (LaBse fine-tuning)
Results
We evaluate semantic similarity of the LaBse model
trained on the translated semantic parsing datasets
comprising of Bengali SQL and it corresponding
Bengali question (Section 4.4) and report the valida-
tion set results in Table 5. Both datasets show high
semantic similarity among query-question pairs.
However, BanglaTabQA have a higher semantic
similarity on various distance metrics indicating
higher similarity of the query-question pairs com-
pared to HindiTabQA .HindiTabQA lower seman-
tic scores can be attributed to the lower recall scores
among query-question pairs leading to lower F1
similarity scores.
Scores Bengali Hindi
Accuracy with Cosine-Similarity 91.99 98.67
F1 with Cosine-Similarity 92.30 72.16
Precision with Cosine-Similarity 94.55 77.68
Recall with Cosine-Similarity 90.15 67.36
Avg Precision with Cosine-Similarity 97.79 75.32
Accuracy with Manhattan-Distance 91.97 98.62
F1 with Manhattan-Distance 92.31 70.96
Precision with Manhattan-Distance 93.73 77.15
Recall with Manhattan-Distance 90.94 65.69
Avg Precision with Manhattan-Distance 97.80 74.41
Accuracy with Euclidean-Distance 91.99 98.67
F1 with Euclidean-Distance 92.30 72.16
Precision with Euclidean-Distance 94.55 77.68
Recall with Euclidean-Distance 90.15 67.36
Avg Precision with Euclidean-Distance 97.79 75.32
Accuracy with Dot-Product 91.99 98.67
F1 with Dot-Product 92.30 72.16
Precision with Dot-Product 94.55 77.68
Recall with Dot-Product 90.15 67.36
Avg Precision with Dot-Product 97.79 75.32
Table 5: Bengali SQL2NQSim validation scores (%)
A.2 Bengali SQL2NQ model Results
We report the validation scores of the SQL2NQ
models in Table 6. The Bengali SQL2NQ model
scores are lower than the Hindi SQL2NQ model.
Manual inspection of the generated dataset reveals
that the Hindi questions and query have higher
lexical overlap compared to the Bengali questions-
query pairs where the questions are more natural
leading to lower lexical overlap with the corre-
sponding SQL query.
A.3 Open-Source Backbone Model Size
We used the following open-source models as back-
bone to low-resource tableQA task. As observed in
Table 7, M2M_418 is the smallest backbone modelBengali Hindi
Rouge-1 14.63 53.20
Rouge-2 5.83 24.98
Rouge-L 14.28 51.58
Table 6: Bengali SQL2NQ model’s validation scores
(%)
Model Number of Parameters
mbart-large-50 0.680 billion
m2m100_418M 0.418 billion
Llama-7B 7 billion
Table 7: Backbone model sizes
among all models and Llama-7b is the largest.
A.4 GPT Prompts
The 2-shot in-context learning prompt with de-
mostrations to GPT is shown in Prompt A.1:
Prompt A.1: 2-Shot ICL Prompt for GPT-3.5/4
Aapin Ekjn sHayk sHkar iJin baKla eSr UÑr edn
baKla eT ibl eQek baKlay UÑr eT ibl tir ker. m sair
EbK n klamguilr EkiT eT ibl inilixt pYoaTae¯n elxa
Hey: < klam > eT ibl eHDar < era 1 > man 1,1 . man 1,2
.... man 1, n < era 2 > man 2,1 . ... < eram> manm, 1 .
manm, 2 .... . man m,n
UdaHrN:
1) S: kTa iSeranam kaU«TDaUn? < klam > bqr . iSer-
anam . vuimka < era 1 > 2006 . is ena Ivl . ejkb guD
naIT ...< era 13 > 2016 . kaU«TDaUn . elh eÕuainn < era
14> 2016 . kaU«TDaUn . elh eÕuainn < era 15 > 2016 .
kaU«TDaUn . elh eÕuainn
UÑr: < klam > gNna(`iSerinam`) < era 1 > 3
2) S: kTa bqer iSerinam s ena Ivl? < klam > bqr .
iSeranam . vuimka < era 1 > 2006 . is ena Ivl . ejkb
guD naIT < era 2 > 2006 . is ena Ivl . ejkb guD naIT
< era 3 > 2006 . is ena Ivl . ejkb guD naIT ...
UÑr: < klam > gNna(`bqr`) < era 1 > 3
The English translation of the 2-shot prompt for
in-context learning (ICL) of GPT-3.5/4 is shown inPrompt A.2:
Prompt A.2: 2-Shot ICL Prompt for GPT-3.5/4
(English translation)
You are a helpful assistant who answers Bengali questions
from Bengali tables by generating an answer table. A table
of m rows and n columns is written in the following pattern:
<column> table header <row 1> value 1,1 | value 1,2 | ...
value 1,n <row 2> value 2,1 | ... <row m> value m,1 | value
m,2 | ... | value m,n
Examples:
1)Question: How many titles are Countdown? <column>
year | Title | Role <row 1> 2006 | See No Evil | Jacob Go ...
<row 13> 2016 | Countdown | Le Trunin <row 14> 2016 |
Countdown | Le Trunin <row 15> 2016 | Countdown | Le
Trunin
Answer: <column> count(‘Title‘) <row 1> 3
2)Question: How many years have See no Evil as titles?
<column> year | Title | Role <row 1> 2006 | See No Evil
| Jacob Good Night <row 2> 2006 | See No Evil | Jacob
Good Night | <row 3> 2006 | See No Evil | Jacob Good
Night ...
Answer: <column> count(‘year‘) <row 1> 3
A.5 LLama-based model Model Prompt
The 2-shot in-context learning prompt with de-
mostrations to Llama-7B based model, OdiaG, is
shown in Prompt A.3:
Prompt A.3: 2-Shot ICL Prompt for odiagenAI-
bn
### Instruction:
Aapin Ekjn sHayk sHkar iJin baKla
eT ibl tir ker baKla eSr UÑr edn.
UdaHrN:
###Input:
kTa iSeranam kaU«TDaUn? < klam > bqr .
iSeranam . vimka < era 1 > 2014 . s ena
Evl 2 . ejkb guD naIT < era 2 > 2016
. kaU«TDaUn . elh eÕuinn < era 3 > 2016 .
kaU«TDaUn . elh eÕuinn
### Response:
< klam > gNna(iSeranam) < era 1 > 2
###End
###Input:
kTa bqr iSeranam s ena Evl 2? < klam >
bqr . iSeranam . vimka < era 1 > 2014 . s
ena Evl 2 . ejkb guD naIT < era 2 > 2016
. kaU«TDaUn . elh eÕuinn < era 3 > 2016 .
kaU«TDaUn . elh eÕuinn
### Response:
< klam > gNna(iSeranam) < era 1 > 1
###End
###Input:
{input}
### Response:The English translation of the 2-shot in-context
learning prompt with demostrations to Llama-7B
based model, OdiaG, is shown in Prompt A.4:
Prompt A.4: 2-Shot ICL Prompt for odiagenAI-
bn (English translation)
### Instruction:
You are a helpful assistant who generates an-
swers Bengali table to answer Bengali ques-
tions. Examples:
###Input:
How many titles are Countdown? <column>
year | Title | Role <row 1> 2014 | See No Evil 2
| Jacob Goodnight <row 2> 2016 | Countdown
| Le Trunin <row 3> 2016 | Countdown | Le
Trunin
###Response:
<column> count(Title) <row 1> 2
### End
###Input:
How many years have See no Evil as titles?
<column> year | Title | Role <row 1> 2014
| See No Evil 2 | Jacob Goodnight <row 2>
2016 | Countdown | Le Trunin <row 3> 2016 |
Countdown | Le Trunin
### Response:
<column> count(year) <row 1> 1
###Input:
{input}
###Response:
A.6 BnTabQA Models Qualitative analysis
We analyze the output of each model with an ex-
ample to identify error patterns and factors that
impact model predictions. The test set question kar
naem fuTsal sm«bykar AQba JuiÑgt pircalekr Ab³Qan
Aaeq? (Who has the position of Futsal Coordina-
tor or Technical Director?), involves logical oper-
ator orafter extracting values for fuTsal sm«bykar
(Fulsal Coordinator) and JuiÑgt pircalekr (Techni-
cal Director) from the column Ab³Qan (Position ).
The input table is shown in Table 8 (translation of
each table cell is italicized and in parenthesis for
non-native readers) with target (English translation
italicized and in parenthesis):
nam(Name) )
maIekl skubala (Michael Skubala)
els irD (Les Reed)
Example 1. Baseline encoder-decoder model,
En2Bn , fine-tuned on the translated MultiTabQA
dataset, correctly extracts maIekl îubala (MichaelAb³Qan (Position) nam(Name)
svapit (Chairman) egRg  la¯k (Greg Clark)
sH-svapit (Co-Chairman) eDivD igl (David Gil)
sazarN s®padk (General Secretary) ma¯k builKHYam (Mark Bullingham)
ekaPazY (Treasurer) ma¯k baeras (Mark Burroughs)
gNmazYm EbK eJageJag pircalk (Media and Communications Director) luIsa ifya«s (Louisa Fiennes)
Juittgt pircalk (Technical Director) els irD (Les Reed)
fuTsal sm«bykar (Futsal Coordinator) maIekl îubala (Michael Skubala)
jaty delr ekac (puruP) (National Team Coach (Male)) gYaerQ saUQegT (Gareth Southgate)
jaty delr ekac (nar) (National Team Coach (Female)) ifl enivl (Phil Neville)
erfair sm«bykar (Referee Coordinator) inl bYair (Neil Barry)
Table 8: Example: BnTabQA Input Table. (English translation of each cell is italicized and in parenthesis)
Skubala) as the fuTsal sm«bykar (Fulsal Coordina-
tor), but wrongly assigns it as the table header in-
stead of nam(name). Moreover, it generates the
same entity twice instead of generating els irD
(Les Reed):
fuTsal sm«bykar (Futsal Coordinator)
maIekl skubala (Michael Skubala)
maIekl skubala (Michael Skubala)
Example 2. OdiaG also overfits to the demonstra-
tions with gNna(count) operator to generate incor-
rect value and header:
gNna(`nam`) (count(Name) )
1(1)
Example 3. GPT-3.5 with 2-shot in-context learn-
ing (ICL) extracts maIekl îubala (Michael Skubala)
correctly but generates an incorrect table header
over-fitting to the demonstrations:
gNna(`nam`) (count(Name))
maIekl skubala (Michael Skubala)
Example 4. GPT-4 with 2-shot in-context learning
(ICL) correctly generates the answer table:
nam(Name)
maIekl skubala (Michael Skubala)
els irD (Les Reed)
Example 5. Both encoder-decoder models,
BnTQA-mBart and BnTQA-M2M , fine-tuned on
BanglaTabQA dataset, correctly generates both an-
swer table headers and values:
nam(Name)
maIekl îubala (Michael Skubala)
els irD (Les Reed)
Example 6. BnTQA-Llama , fine-tuned on
BanglaTabQA dataset, is partially correct in its
predictions by generating fuTsal sm«bykar (Fulsal
Coordinator) in the first row, but incorrectly repeats
the same entity instead of els irD (Les Reed) in the
second row:
nam(Name)
fuTsal sm«bykar (Fulsal Coordinator)
fuTsal sm«bykar (Fulsal Coordinator)We observe from the examples that all baselines
except GPT-4 generate wrong table headers and
overfits and mimics the demonstrations, showing
a lack of understanding of table structure and rea-
soning. The BanglaTabQA models perform table
reasoning, reflecting the utility and quality of the
large-scale BanglaTabQA dataset.
A.7 Zero-Shot Cross-Lingual Transfer
Examples
Example 7. The Hindi question, vq 2011m \
Ektn  fFq k h {\ ?(How many titles are there in
year 2011? ), with Hindi input table, Table 9 (En-
glish translation is italicized and in parenthesis)
and target table:
gZnA (fFq k )(count(Title))
(4)
BnTQA-mBart correctly performs table reasoning
but generates the answer in Bengali script instead
of Devnagari (Hindi) script:
gnNa(iSeranam) (count(Title))
4(4)
Example 8. However, for Hindi extractive ques-
tions like kOns  prAptktA  aEDktm bAr aAy 
h {\?(Which recipient occurs the maximum number
of times?), with Hindi input table:
sAl (year)prAptktA  (Recipient )
2016 Evnod B (Vinod Bhatt )
2016 Evnod B (Vinod Bhatt )
2017 tArk mh tA [ 1 ](Tarak Mehta[1] )
and target table:
prAptktA  (Recipient)
Evnod B (Vinod Bhatt)
BnTQA-mBart correctly generates the answer in
Hindi:
prAptktA  (Recipient)
Evnod B (Vinod Bhatt)vq (year)fFq k (Title) EkrdAr (Character )
2005 %lAiVplAn (Flight Plan ) eErk (Eric)
... ... ...
2011 in VAim (In Time ) h nrF h {EmSVn (Henry Hamilton )
2011 in VAim (In Time ) h nrF h {EmSVn (Henry Hamilton )
2011 in VAim (In Time ) h nrF h {EmSVn (Henry Hamilton )
2011 in VAim (In Time ) h nrF h {EmSVn (Henry Hamilton )
... ... ...
2014 -p s -V fn 76(Space Station 76 )V {X(Ted)
... ... ...
2014 Ev \Vs  V l (Winter’s Tale ) pFVr l k k  EptA (Peter Lake’s Father )
Table 9: Example: HiTabQA Input Table (English translation of each cell is italicized and in parenthesis)
A.8 Comparison of scores of LaBSE and
SQL2NQ models
We qualitatively compare the sentence similarity
models LaBse and SQL2NQ with examples shown
in Table 10. We observe that LaBse scores are low
for positive samples of Bengali SQL queries and
the corresponding Bengali question. Further, neg-
ative samples, i.e., Bengali SQL query and an un-
related Bengali question has high similarity scores.
This trend is not observed for the sentence simi-
larity model, SQL2NQ, trained on Bengali SQL
queries and corresponding Bengali natural ques-
tions.LaBse SQL2NQ
Bengali SQL Bengali Question Scores Scores
+vein¯bacon korun `bqr` dl kra `bqr` sajan eHak
gnNa(`flafl`) sma 1
(SELECT years GROUP BY years ORDER BY
COUNT(result) LIMIT 1)ekan bqer sbecey km fl Heyeq?
(Which year has the least number of results?)0.45 0 .94
in¯bacon korun `iSrnam` sajan eHak `bqr` AberaH sma 1
(SELECT ‘title‘ ORDER BY ‘year‘ DESC LIMIT 1)s®itktm bqerr saeQ s®itk iSrnam efrt idn.
(Return the most recent title of the most recent year?)0.43 0 .98
-vein¯bacon korun s¯bin(`sal`)
(SELECT min(‘year‘))ekan bqer (2010, 2016) sbecey ebtS SKxYk pur-
²kar ijeteq?
(In which year (2010, 2016) were the most number of
awards received?)0.51 0 .31
in¯bacon korun gnNa(*) eJxaen `kaj`=esdaimnr sKsar
(SELECT count(*) WHERE ‘work‘="The World of Saudamini")eMaT 4 Aaeq Emn egemr emaT SKxYa gnNa krun.
(How many games scored a total of 4?)0.80 0 .07
Table 10: Comparison of sentence similarity scores between LaBse and our trained SQL2NQ models.