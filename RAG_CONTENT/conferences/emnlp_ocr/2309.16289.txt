LawBench: Benchmarking Legal Knowledge of
Large Language Models
Zhiwei Fei1†, Xiaoyu Shen2†∗, Dawei Zhu3†, Fengzhe Zhou4
Zhuo Han1, Songyang Zhang4, Kai Chen4, Zongwen Shen1, Jidong Ge1‡
1National Key Laboratory for Novel Software Technology, Nanjing University
2Amazon Alexa AI3Saarland University4Shanghai AI Laboratory
†Equal Contribution‡Corresponding Author
Abstract
Large language models (LLMs) have demonstrated strong capabilities in various
aspects. However, when applying them to the highly specialized, safe-critical legal
domain, it is unclear how much legal knowledge they possess and whether they can
reliably perform legal-related tasks. To address this gap, we propose a comprehen-
sive evaluation benchmark LawBench . LawBench has been meticulously crafted
to have precise assessment of the LLMs’ legal capabilities from three cognitive
levels: (1) Legal knowledge memorization : whether LLMs can memorize needed
legal concepts, articles and facts; (2) Legal knowledge understanding : whether
LLMs can comprehend entities, events and relationships within legal text; (3) Legal
knowledge applying : whether LLMs can properly utilize their legal knowledge and
make necessary reasoning steps to solve realistic legal tasks. LawBench contains
20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label
classification (MLC), regression, extraction and generation. We perform exten-
sive evaluations of 51 LLMs on LawBench, including 20 multilingual LLMs, 22
Chinese-oriented LLMs and 9 legal specific LLMs. The results show that GPT-4
remains the best-performing LLM in the legal domain, surpassing the others by a
significant margin. While fine-tuning LLMs on legal specific text brings certain
improvements, we are still a long way from obtaining usable and reliable LLMs
in legal tasks. All data, model predictions and evaluation code are released in
https://github.com/open-compass/LawBench/ . We hope this benchmark
provides in-depth understanding of the LLMs’ domain-specified capabilities and
speed up the development of LLMs in the legal domain.
1 Introduction
The artificial intelligence community has witnessed notable progress in the large language models
(LLMs) recently. The latest large language models, such as GPT-4 [ 45] and LLaMA [ 62], have
showcased remarkable capabilities and intricate skills that are comparable to, and in some cases, even
surpass human capabilities [ 24]. To quantitatively assess the capabilities of LLMs, evaluation bench-
marks serve a pivotal role in their development. Numerous recent endeavors mainly concentrate on
enhancing general and universal capabilities, such as understanding world knowledge and executing
complex reasoning [86; 73].
Previous research on evaluation has predominantly concentrated on exploring the diverse aspects of
the general capabilities of LLMs. SuperGLUE [ 64] is crafted to assess various facets of language
understanding, encompassing reading comprehension, commonsense reasoning, and entailment.
∗Work Done Outside AmazonarXiv:2309.16289v1  [cs.CL]  28 Sep 2023Issue T opic
IdentificationMarital Disputes
IdentificationDispute Focus
IdentificationDocument
ProofreadingKnowledge
Question AnsweringArticle Recitation
Consultation
Criminal Damages
Calculation
Case Analysis
Prison T erm
Prediction
w. Article
Prison T erm
Prediction
w.o. Article
Charge Prediction
Scene-based Article
Prediction
Fact-based Article
Prediction
Trigger Word
ExtractionEvent DetectionArgument MiningOpinion SummarizationNamed-Entity RecognitionReading
Comprehension10305070GPT-4
ChatGPT (23/6/13)StableBeluga2-70B
Qwen-7B-ChatInternLM-Chat-7B-8K
InternLM-Chat-7BFigure 1: Results (zero-shot) of six best-performing LLMs evaluated on 20 diverse legal tests covering
three cognitive dimensions: legal knowledge memorization, understanding, and applying.
Benchmarks such as MMLU [ 23] and BIG-bench [ 55] have been introduced to cover a wide range
of NLP tasks for LLM evaluations. Furthermore, there exist benchmarks [ 19;58;18;10] explicitly
designed to scrutinize the advanced abilities of LLMs that manifest with scale, such as reasoning [ 47]
and coding [ 42]. Beyond these specific benchmarks, more recent initiatives, like OpenCompass [ 13]
and HELM [36], aspire to offer a comprehensive perspective on the capabilities of LLMs.
However, evaluating Large Language Models (LLMs) necessitates not only a focus on general
capabilities but also the incorporation of domain-specific benchmarks for assessing models specialized
in particular fields [ 82]. Legal tasks encompass a broad spectrum of applications, predominantly text-
based, necessitating comprehension and interpretation of highly professional legal texts. Currently,
they are primarily conducted by legal experts, who require years of extensive specialized training
to process legal cases. Endowing LLMs with legal expertise can not only improve the working
efficiency of legal officers, but also address the overwhelming demand of legal assistance from
non-professionals, and thereby improve public access to justice [ 15;63]. Consequently, we prioritize
establishing benchmarks to measure the legal knowledge of existing LLMs.
In contrast to assessments that primarily focus on testing model’s ability to pass legal bar exams [ 5;86],
which are not always representative of the actual use-cases for LLMs2, our emphasis is on examining
2As shown in [ 9], ChatGPT excels in passing legal bar exams but struggles at performing realistic legal tasks.
2a structured set of legal skills required in real-world scenarios. In the legal domain, there have
been works ensembling legal-related tasks including the lexglue [ 9] focusing on EU and American
laws, and LBOX OPEN [ 28] focusing on South Korean laws, [ 7] further transformed lexglue into
zero-shot forms to test the ability of LLMs to complete these tasks. LEGALBENCH [ 21], in
particular, presented the first steps towards constructing an interdisciplinary collaborative legal
reasoning benchmark for the English language and evaluated 20 LLMs in 162 legal tasks.
Nonetheless, legal systems vary significantly among different countries, highlighting the importance
of establishing different standards for each legal system. The Chinese legal system is rooted in the
civil law family. Unlike the common law system, which is widely accepted in the U.S. and the U.K.,
judges in the civil law system are obliged to stay neutral, respect the established statutory law articles
and ground their decisions on them. Understanding and applying existing statutes and codes, rather
than studies of precedents, are of paramount importance [ 83]. Therefore, it is necessary to design a
separate set of evaluation tasks to emphasize the required skill set for the Chinese law system.
With this in mind, we present LawBench : a meticulously crafted, comprehensive evaluation bench-
mark to assess the LLMs’ capabilities on performing legal-related tasks under the Chinese civil-law
system. LawBench consists of 20 diverse tasks following 5 categories: single-label classification
(SLC), multi-label classification (MLC), regression, extraction and generation. We divided these tasks
into 3 skill levels according to widely accepted Bloom’s cognitive models [ 32]: (1) Legal knowledge
memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal
knowledge understanding: whether LLMs can comprehend entities, events and relationships within
legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge
and make necessary reasoning steps to solve realistic legal tasks. Intuitively LLMs must first obtain
lower-level skills before excelling in higher-level skills. This division method provides a structured
overview of the skill set required for legal-related tasks. It can also facilitates an exploration of the
similarities and differences between the learning mechanisms of LLMs and humans.
We extensively tested 51 popular LLMs, including 20 multilingual LLMs, 22 Chinese-oriented
LLMs and 9 legal specific LLMs. To effectively extract answers from the predictions of LLM
generations, we design suitable rules, regular expressions and metrics for every individual task. The
benchmark and evaluation code are integrated into the OpenCompass platform [ 13] to enable easy
reproduction. The evaluation results are shown in Figure 1. We find that although legal specific
fine-tuning usually improves upon their base model, they are still significantly lagging behind general
LLMs, which occupy the top six spots in the averaged zero-shot performance. We analyze the impact
of various factors on the results, such as supervised fine-tuning (SFT), reinforcement learning from
human feedback (RLHF) [ 12], model size, legal specific fine-tuning, etc. Important suggestions are
summarized to better guide the future development of legal LLMs for the Chinese community.
2 Related Work
Large Language Models Large language models (LLMs) trained on massive amounts of data
have shown impressive abilities in generating high-quality, coherent text and following zero-shot
or few-shot instructions in a diverse set of tasks [ 44;45;11;61;62]. These models are usually
trained following three steps: pre-training, supervised fine-tuning (SFT) and alignment with human
or AI feedback [ 12;52;67;46;33;6] etc. As most public available LLMs focus on training on
English corpora, many efforts have been devoted to extending LLMs to Chinese. These works either
pre-training a new LLM from scratch on Chinese-centric corpora [ 56;77;17;57;60], or expanding
the vocabulary of an existing English-centric LLM then performing SFT on Chinese instruction
data [ 16;29;76]. There have also been studies that are dedicated to adapting LLMs to the legal
domain by fine-tuning on legal specific corpora [ 14;25;75]. However, a comprehensive evaluation to
compare the existing LLMs regarding their legal knowledge is still lacking. Our focus is primarily on
models that can complete corresponding tasks based on instructions, so we exclude pre-trained small
language models such as Lawformer [ 69] and LegalBERT [ 8], which require task-specific fine-tuning
to perform competiviely.
Existing Benchmarks As the rapid development of LLMs, conventional approaches of evaluating
a model’s performance on a single task through fine-tuning [ 71;65;40;79;49] is no longer adequate
for evaluating LLMs. A growing body of research works have recently focused on developing more
comprehensive and systematic benchmarks to evaluate various capabilities of LLMs. Examples
3What is the content of Article 257 of 
the Criminal Law? Memorization
Understanding
ApplyingDetermine the category of the dispute 
focus contained in the sentence. 
Please simulate a judge and provide the 
charge based on the following facts…
Prediction, Analysis, ConsultationProofreading,  Identification, NER, SummarizationArticle Recitation,  Knowledge Question AnsweringFigure 2: Three cognitive dimensions for evaluating large language models in LawBench. In order to
specialize in legal tasks, LLMs must be able to (1) memorize necessary legal concepts, terminologies,
articles and facts; (2) understand entities, events and relationships in legal text; and finally (3) simulate
law professionals to apply legal knowledge and necessary reasoning in solving realistic tasks.
include AGIEval [ 86] which covers human-centric standardized exams, such as college entrance
exams, law school admission tests, math competitions, and lawyer qualification tests, HELM [ 36]
which measures 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency)
for each of 16 core scenarios to the extent possible, KOLA [ 73] which focuses on knowledge-oriented
assessment under four-level taxonomy of knowledge-related abilities, and MMBench [ 39] which is
designed specifically to evaluate vision-language models. There have also been efforts in constructing
benchmarks for Chinese such as CMMLU [ 35], GAOKAO [ 80] and C-Eval [ 27]. Some of them
focus on specific domains such as CMB [ 66] in the medical domain and Fin-eval [ 78] in the finance
domain. In the legal domain, there have also been works ensembling legal-related tasks including the
lexglue [ 9] focusing on EU and American laws, and LBOX OPEN [ 28] focusing on South Korean
laws, but they did not formulate tasks into the instruction-following formats for LLMs. Recently,
legal-bench was released in evaluating LLMs on 162 legal-related tasks based on American laws [ 21].
Our work follows a similar line to extend the evaluation to Chinese laws. Given the uniqueness of the
Law system in People’s Republic of China and the diversity of the legal tasks covered in this work,
we believe that LawBench will contribute to the multilinguality of global legal research and promote
research in specializing language models to a specific domain.
3 LawBench Construction
In this section, we provide a detailed description of the principles behind the design of LawBench
and the test task selection.
3.1 The Hierarchical Ability Taxonomy of LawBench
When evaluating large language models, there is a preference for using a variety of tasks to assess
their capabilities. A hierarchical evaluation system allows us to better understand the abilities
of large language models in different aspects. Instead of categorizing tasks solely based on their
difficulty [ 27], we refer to the widely used Bloom’s cognitive model [ 32] to classify tasks into different
dimensions [ 73]. Bloom’s Taxonomy system was initially proposed by the educational psychologist
Benjamin Bloom and his collaborators in 1956 and has been widely applied and developed in the
following decades. It has effectively aided teachers in curriculum design and the assessment of
student learning outcomes. Bloom’s Taxonomy divides learning objectives in the cognitive domain
into six levels, from the lowest to the highest: Remember, Understand, Apply, Analyze, Evaluate, and
Create. These levels describe the depth and complexity of cognitive learning and provide an organized
framework. Teachers can use Bloom’s Taxonomy to ensure diversity and completeness in course
objectives. By combining learning objectives at different levels, comprehensive student development
can be promoted, encouraging them to progress from simple memorization and understanding to
higher-level analysis, evaluation, and creation.
4Inspired by this classification approach, we simplified Bloom’s cognitive hierarchy model and kept
the first three categories in Bloom’s taxonomy to assess the legal knowledge of LLMs:
1.Knowledge Memorization : The memorization level measures the basic requirement of
remembering legal-related knowledge. It tests LLMs’ ability in memorization and recitation
of basic legal-domain knowledge such as regulations, cases, concepts, common sense, legal
facts and terminologies.
2.Knowledge Understanding : The understanding level involves understanding the meanings
and connotations of legal documents. This includes the ability to comprehend and interpret
legal concepts, text, and issues, e.g., identifying entities and relationships within legal texts,
detect types of legal issues and points of dispute, among others.
3.Knowledge Applying : The applying level requires LLMs to integrate legal knowledge,
reason over it and address real-world legal cases. It covers the model’s logical reasoning
abilities to perform legal consultation, judicial assistance, as well as numerical reasoning
abilities to calculate involved amount.
Note that under this taxonomy, some tasks may not strictly belong to just one category; they may
involve other abilities as well. We have categorized these tasks based on their primary capabilities.
Cognitive Level ID Task Data Source Metric Type
Legal Knowledge
Memorization1-1 Article Recitation FLK Rouge-L Generation
1-2 Knowledge Question Answering JEC_QA Accuracy SLC
Legal Knowledge
Understanding2-1 Document Proofreading CAIL2022 F0.5 Generation
2-2 Dispute Focus Identification LAIC2021 F1 MLC
2-3 Marital Disputes Identification AIStudio F1 MLC
2-4 Issue Topic Identification CrimeKgAssitant Accuracy SLC
2-5 Reading Comprehension CAIL2019 rc-F1 Extraction
2-6 Named-Entity Recognition CAIL2022 soft-F1 Extraction
2-7 Opinion Summarization CAIL2021 Rouge-L Generation
2-8 Argument Mining CAIL2022 Accuracy SLC
2-9 Event Detection LEVEN F1 MLC
2-10 Trigger Word Extraction LEVEN soft-F1 Extraction
Legal Knowledge
Applying3-1 Fact-based Article Prediction CAIL2018 F1 MLC
3-2 Scene-based Article Prediction LawGPT Rouge-L Generation
3-3 Charge Prediction CAIL2018 F1 MLC
3-4 Prison Term Prediction w.o. Article CAIL2018 nLog-distance Regression
3-5 Prison Term Prediction w. Article CAIL2018 nLog-distance Regression
3-6 Case Analysis JEC_QA Accuracy SLC
3-7 Criminal Damages Calculation LAIC2021 Accuracy Regression
3-8 Consultation hualv.com Rouge-L Generation
Table 1: Task list in LawBench. Tasks correspond to cognitive dimensions: legal knowledge memorization,
understanding and applying, and 5 task types: generation, single-label classification (SLC), multi-label classifi-
cation (MLC), regression, and extraction.
3.2 Data Source and Selected Tasks
We selected 20 tasks falling under the above-mentioned capability levels. Every task is assigned a
unique task id for better distinction. The task list is provided in Table 1. There can exist datasets
belonging to the same tasks. When selecting the dataset for every task, we choose the most recent
available version. Furthermore, certain tasks like legal case retrieval requires processing very long
documents, which can surpass the length limit for most LLMs, so we do not include them to
LawBench for now. When constructing LawBench, we have made efforts to format the prompts in a
way that best aligns with user habits with clear instructions about the answer format, so that we can
assess the ability of LLMs in assisting legal tasks in realistic scenarios.
Legal Knowledge Memorization Tasks
Legal knowledge memorization tasks examine to which extent large language models encode legal
knowledge within their parameters. While this knowledge can be enhanced with external retrievers, it
is still beneficial to memorize necessary legal knowledge because (1) There is currently no reliable
mechanism to guarantee the accurate retrieval of legal provisions. Memorizing useful knowledge
5within model parameters can help combat the noise from external retrievers [ 70;43]; (2) It is very
difficult, if not impossible, to retrieve all needed legal knowledge for complicated reasoning tasks.
The model must know basic legal concepts to connect the retrieved knowledge smoothly [ 50;87;74];
(3) Relying on the parametric knowledge instead of external retrievers can significantly reduce the
online latency [51; 53; 59].
There are two major types of legal knowledge that requires memorizing: (1) core law articles and
regulation content and (2) other fundamental legal concepts, notions and rules. We construct two
tasks corresponding to these two types of knowledge:
•Article recitation (1-1): Given a law article number, recite the article content . We collected
the contents of laws and regulations from the national database3and consulted students with
a legal background to select 152 sub-laws under the 5 core laws. We further incorporated
updated laws and regulations, including constitutional amendments, to evaluate the model’s
ability to comprehend legal changes.
•Knowledge question answering (1-2): Given a question asking about basic legal knowledge,
select the correct answer from 4 candidates . We collect knowledge-based questions from
the JEC-QA tasks [ 85]. To simplify the process of locating answers during the test, we
exclusively chose single-label questions from them.
Examples of these two tasks are in Appendix A.1.
Legal Knowledge Understanding Tasks
Legal knowledge understanding tasks examine to which extent large language models can comprehend
entities, events, and relationships within legal texts. Understanding legal text is a pre-condition to
utilize the knowledge in concrete downstream applications [ 15]. In total, we selected 10 tasks
corresponding to different levels of legal knowledge understanding:
•Document Proofreading (2-1): Given a sentence extracted from legal documents, correct its
spelling, grammar and ordering mistakes, return the corrected sentence . Legal documents,
as carriers of judicial authorities and the exercise of legal rights by citizens, demand utmost
precision in their textual content. We sample the original and corrected legal sentences from
the CAIL2022 document proofreading task. Possible mistake types are inserted into the
instructions to let the model directly output the corrected sentence.
•Dispute Focus Identification (2-2): Given the original claims and responses of the plaintiff
and defendant, detect the points of dispute . In civil cases, the points of dispute represent the
core of conflicts, intersection of contradictions, and issues over which the parties involved in
the case are in contention. The automated recognition and detection of points of contention
have practical significance and necessity for the development of the rule of law in our
country. Specifically, we will provide the trial-related content from judgment documents,
including the sections on claims and responses. The cases involve various legal matters such
as civil loans, divorce, motor vehicle traffic accident liability, financial loan contracts, and
more. We have carefully selected common types of points of contention from LAIC2021 to
construct this test set.
•Marital Disputes Identification (2-3): Given a sentence describing marital disputes,
classify it into one of the 20 pre-defined dispute types . Marital disputes refer to the total sum
of various disputes arising from love, marriage, and divorce. Among civil disputes, marital
disputes are a common type of dispute. We have selected a publicly available marriage text
classification dataset on AiStudio4. This dataset consists of 20 categories, and a single text
entry may have multiple labels.
•Issue Topic Identification (2-4): Given a user inquiry, assign it into one of pre-defined
topics . User inquiries are typically vague. Identifying the relevant topics in legal consulta-
tions can help legal professionals better pinpoint key issues. We obtain the data from the
CrimeKgAssistant project5. We keep the most frequent 20 classes and sample 25 questions
for each class to form our final test set.
3https://flk.npc.gov.cn/
4https://aistudio.baidu.com/datasetdetail/181754
5https://github.com/liuhuanyong/CrimeKgAssitant
6•Reading Comprehension (2-5): Given a judgement document and a corresponding question,
extract relevant content from it to answer the question . Judicial documents contain rich case
information, such as time, location, and character relationships. Intelligently reading and
comprehending judicial documents through large language models can assist judges, lawyers,
and the general public in obtaining the necessary information quickly and conveniently. We
use the CAIL2019 reading comprehension dataset to build this task, removing question
types related to binary and unanswerable questions. We retain single and multiple-segment
data as our test set.
•Named-Entity Recognition (2-6): Given a sentence from a judgement document, extract
entity information corresponding to a set of pre-defined entity types such as suspect, victim
or evidence . We sampled 500 examples from the CAIL2022 Information Extraction dataset
as our test set. These 500 samples contain 10 entity types related to theft crimes.
•Opinion Summarization (2-7): Given a legal-related public news report, generate a concise
summary . Legal summaries typically include key facts of the case, points of contention,
legal issues, legal principles applied, and the judgment’s outcome. It can provide a quick
overview of the case content to improve the efficiency of legal professionals. We randomly
select 500 samples from the CAIL2021 Legal Public Opinion Summary dataset for this task.
We only select samples with less than 400 words to fit the length constraint of LLMs.
•Argument Mining (2-8): Given a plaintiff’s perspective and five candidate defendant’s
viewpoints, select one viewpoint that can form a point of dispute with the plaintiff’s per-
spective . In court’s trial process, judgment documents play a crucial role in recording the
arguments and evidence presented by both the plaintiff and the defendant. Due to differences
in their positions and perspectives, as well as inconsistencies in their factual statements,
disputes arise between the plaintiff and the defendant during the trial process. These points
of contention are the key to the entire trial and the essence of judgment documents. This
task aims to extract valuable arguments and supporting materials from a large volume of
legal texts, providing strong support for legal debates and case analysis. We use CAIL2022’s
Argument Mining dataset to construct our dataset, transforming the identification of focal
points of disputes into a multiple-choice question format.
•Event Detection (2-9): Given a sentence from a legal judgement document, detect which
events are mentioned in this sentence . Events are the essence of facts in legal cases.
Therefore, Legal Event Detection is fundamentally important and naturally beneficial to
case understanding and other Legal AI tasks. We construct the test set from the LEVEN
dataset[ 72] by sampling sentences corresponding to the top 20 most frequent event types.
Multiple events can be mentioned in every sentence.
•Trigger Word Extraction (2-10): Given a sentence from a legal judgment document and its
corresponding events, predict which words in the sentence triggered these events . Trigger
words directly cause events and are an important feature that determines the event category,
providing post-hoc explanation for the event types we identify. Directly identifying trigger
words is very difficult, so we simplified this task by providing the events contained in the
text along with the text information, examining the ability of LLMs to recognize trigger
words related to events. When constructing the trigger word test set, we removed trigger
words that were the same as the event type, as well as events with multiple or duplicate
trigger words from the LEVEN dataset[ 72], to include as different trigger words as possible.
Examples of the 10 understanding tasks are in Appendix A.2.
Legal Knowledge Applying Tasks
Legal knowledge applying tasks primarily examine the ability of LLMs to not only understand legal
knowledge but also simulate law professionals to apply the knowledge in solving realistic legal tasks.
In the task design, we extensively examine the model’s different reasoning abilities, including 3 legal
content reasoning tasks: legal judgement prediction, case analysis, consultation, and 1 numerical
reasoning task: criminal damages calculation.
When predicting case judgments, judges follow a certain order when hearing a case [ 84;26]. There-
fore, in constructing the case judgment prediction task, we simulated this process by decomposing
the CAIL2018 dataset into three tasks: fact-based article prediction (3-1), charge prediction (3-3)
and prison term prediction. We further separate the task of prison term prediction into two scenarios:
7without article content (3-4) and with article content (3-5) to examine LLMs’ capability in utilizing
the article content to make accurate judgement predictions. Besides, we also add the task scene-based
fact prediction to simulate judges’ recognition of legal provisions (3-2).
•Fact-based Article Prediction (3-1): Given a fact statement from the legal judgement
document, predict which article items should be applied . When judges make decisions, they
usually associate relevant articles with the facts of the case [ 20;41]. Article prediction can
assist judges in quickly locating legal articles related to legal texts. Legal articles are written
expressions of legal norms, which are rules and regulations with clear meanings and legal
effects. The model needs to deduce potentially applicable legal provisions based on the
given case description and related background information. We sample 500 cases from the
CAIL2018 dataset for this task.
•Scene-based Article Prediction (3-2): Given a described scenario and a related question,
predict the corresponding article item . The CAIL2018 dataset only covers criminal law-
related legal provisions. In order to comprehensively evaluate the ability of LLMs to analyze
case facts and infer relevant legal provisions, we collected high-quality legal scenario-based
question-and-answer data from public sources on GitHub[ 38]. This dataset was generated by
inputting legal provisions into chatGPT to construct corresponding scenario-based questions
and answers. We manually selected 5,000 question-and-answer pairs with accurate answers
from the generated dataset. Based on this, we selected 252 core legal provisions’ scenario-
based question-and-answer content as the test dataset.
•Charge Prediction (3-3): Given fact statement from the legal judgement document and the
applied article number, predict the cause of action (charge) . Cause of action is a summary of
the nature of the legal relationship involved in a litigation case, as formulated by the people’s
court. Accurately predicting the cause of action can help improve judicial efficiency and
fairness. In the process of filing and hearing cases, accurate prediction of the cause of action
can help the court to allocate cases, allocate resources, and arrange trials, thereby improving
judicial efficiency and fairness. We sampled 500 pieces of data from the CAIL2018 cause of
action prediction dataset for this task.
•Prison Term Prediction w.o. Article (3-4): Given fact statement from the legal judgement
document, the applied article number and charge, predict the prison term. Prison term
prediction refers to the process of predicting and estimating the possible sentence that a
defendant may face during the criminal justice process based on the facts of the case, legal
provisions, and relevant guiding precedents. It aims to make reasonable inferences about the
length and form of the sentence by comprehensively considering various factors such as the
nature of the crime, the circumstances of the offense, the social impact, and the defendant’s
personal situation. We used the prison term prediction dataset from CAIL2018, removed
some cases with the death penalty and life imprisonment, and randomly sampled 500 cases
as the test dataset. During the process of judges’ sentencing, more information is usually
taken into account to determine the prison term outcome. We simulated the judge’s analysis
process by providing the relevant legal provisions and the charge of the case.
•Prison Term Prediction w. Article (3-5): Given fact statement from the legal judgement
document, the applied article content and charge, predict the prison term. Large language
models typically use retrieval-argument methods to introduce new information. Some
publicly available models also include retrieval modules that provide detailed reference
information for the model by retrieving legal provisions. We simulated this process, and
unlike the previous task where only the legal provision number was provided, we provided
the specific content of the legal provision in this task. When constructing the sentence
prediction dataset, we appended the content of the legal provisions to the end of the question,
allowing the model to complete the sentence prediction task in this scenario.
•Case Analysis (3-6): Given a case and a corresponding question, select the correct answer
from 4 candidates . We use the case analysis part from JEC_QA dataset [ 85] for this task.
The case analysis part tests the ability of models to analyze real cases. Models must possess
five types of reasoning in order to perform this analysis including word matching, concept
understanding, numerical analysis, multi-paragraph reading, and multi-hop reasoning. In
order to reduce the difficulty of the test and facilitate the acquisition of answers, we sampled
500 multiple-choice questions from the JEC_QA Case-Analysis part as the testing dataset.
8•Criminal Damages Calculation (3-7): Given a fact description about a criminal process,
predict the amount of money involved in this case . There are some numerical computing
tasks in the process of judicial trials, such as the calculation of the total amount of legal
crimes. The total amount of the crime is an important sentencing factor. In some charges
such as theft, financial fraud, and bribery, China’s laws determine the severity of the sentence
based on the amount involved in the case. This task mainly tests the computing ability
of LLMs. First, we examine whether the model understands the rules of case amount
calculation, and second, we examine whether the model can accurately complete numerical
calculations. We selected the LAIC2021 numerical computing task to construct our dataset.
•Consultation (3-8): Given a user consultation, generate a suitable answer. Legal consul-
tation is a way for the public to access legal services. Legal consultation can help people
understand legal disputes and seek targeted advice and solutions from professional lawyers,
as well as receive support and guidance. Some law firms and legal consulting companies
also provide online legal consultation services, making it more convenient for people to
obtain legal help. We collected legal consultation contents from the Hualv website6, and
our dataset contains both the answers to legal consultations and the corresponding legal
basis, i.e., legal articles.
Examples of the 8 applying tasks are in Appendix A.3.
3.3 Evaluation
For every task, the evaluation is done following two steps: (1) answer extraction, which extracts the
answer from the model prediction, and (2) metric computation, which computes the metric score
based on the question, extracted answer and the gold answer. Answer extraction is a necessary step
since many LLMs often do not generate output directly comparable with gold labels [ 4]. We explain
these two steps in detail in the following section.
Answer Extraction Most of the tasks require the prediction to be in the standard format in order to
compare with the ground truth, we define a set of task-specific rules to extract the answer from the
model prediction.
•Article Number Extraction (3-1): this type of tasks requires us to extract the article numbers
predicted by the model. To do this, we use the delimiter “ 、” to separate the prediction
text into chunks of text, and then the cn2an7library is used to convert the Chinese numerals
to Arabic numerals within each of those chunks. Using a regular expression, we extract
the converted Arabic numerals as the expected article numbers; if more than one number
appears in the same chunk, only the first number is extracted. All extracted numbers are
combined to form the final set of predictions.
•Prison Term Extraction (3-4, 3-5): for this type of tasks, we need to extract the predicted
prison terms from the prediction text. To begin, we use cn2an to convert all the Chinese
numerals in the prediction to Arabic numerals; we then extract digits that are followed by
time intervals in Chinese, such as “ 个月” (month) and “ 年” (year). The extracted prison
terms are normalized to months, meaning that the numbers appearing before “ 年” will be
multiplied by 12. Note that the time unit in the ground truth answer is also month.
•Criminal Damages Extraction (3-7): We extract all numbers appearing in the prediction text
using regular expression. The set of of the extracted numbers is considered as the predicted
criminal damages.
•Named-Entity Recognition (2-6): We find all occurrences of entity types from the model
prediction, then obtain the substring from its occurrence to the delimiter “\n”, then apply a
regular expression to extract the entity value.
•Trigger Word Extraction (2-10): We split the model prediction by the delimiter “ ；” , then
treat the split array as a list of extracted key words.
•Option Extraction (1-2, 2-2, 2-3, 2-4, 2-8, 2-9, 3-3): this type of task is similar to selecting
the correct options from a list of options in a multiple-choice task. We run through all
6www.66law.com
7https://github.com/Ailln/cn2an
9possible options and check if they appear in the prediction text. The set of options that occur
in the prediction text is collected and used for evaluation.
•Others (1-1, 2-1, 2-5, 2-7, 3-2, 3-8): we take the model prediction as the answer without
performing any extraction step.
Metrics After the answer extraction phase, we compute the final metric based on the extracted
answer. We defined 7 different metrics in total to measure different types of tasks:
•Accuracy : Accuracy is a binary score that performs exact match between the model
prediction and the gold answer. This applies to all single-label classification tasks including
task 1-2, 2-4, 2-8, 3-6, and the regression task 3-7. For SLC tasks, if multiple valid answers
are extracted from the model prediction, then we always treat it as wrong8.
•F1: When there are multiple output labels, F1 score measures the harmonic mean of the
precision and recall. This applies to all multi-label classification tasks including task 2-2,
2-3, 2-9, 3-1 and 3-3.
•rc-F1 : rc-F1 is the F1 score tailored for the reading comprehension task 2-5. It treats every
token as a label, removes punctuation, stories, extra whitespace, performs other necessary
normalizations then compute the F1 score. We adopt the official script from CAIL2019 to
compute the instance-level rc-F1 score9.
•soft-F1 : For extraction tasks 2-6 and 2-10, the output is a set of phrases. Instead of using
the standard F1 score, we use a soft version by replacing the phrase-level exact match with
the rc-F1 score, then computing the F1 on top of it. We find using the soft version helpful
since LLMs often use wording choices different from the ground truth.
•nLog-distance : For the prison term prediction tasks 3-4 and 3-5, we evaluate them with
the normalized log distance (nLog-distance) to capture the continuity of prison terms.
We compute the logarithm of the difference between the extracted and gold answer, then
normalize it to the space between 0 and 1 for better compatibility with other metrics.
•F0.5: For the document proofreading task 2-1, we use the F0.5 metric to evaluate it. The F0.
5 score gives more weight to precision than to recall we want to prevent introducing more
false positives than identify every other error in proofreading [ 81]. We use the ChERRANT
toolkit to align the extracted and gold answer before computing the F0.5 score10. As the
alignment can take too long to respond for very bad generations, we add a time-out of 10
seconds. If a time-out happened, then the prediction is assigned a score of 0.
•Rouge-L : For other generation tasks 1-1, 2-7, 3-3 and 3-8, we use the Rouge-L score to
evaluate them. Rouge-L is a commonly used metric in generation tasks. It takes into account
sentence-level structure similarity naturally and identifies longest co-occurring in sequence
n-grams automatically to compare the extracted and gold answers [37].
Several large language models may decline to respond to legal-related inquiries due to security
policies or simply fail to follow the instructions. To capture this issue, we also report the abstention
rate of LLMs in each task (how often an LLM abstains to answer). An abstention happens if an
answer cannot be extracted from the model prediction. The abstention rate does not apply to task 2-5
and all generation tasks since they do not need the answer extraction step.
4 Experiment
4.1 Models
We evaluate a wide spectrum of large language models of various sizes, grouping them into three
major categories based on their pre-training and fine-tuning domains: multilingual LLMs, Chinese-
oriented LLMs and legal specific LLMs. We provide a short review over them in the following
section. The detailed model list is shown in Table 2.
8For the criminal damages calculation task, we treat the model prediction correct as long as one of the
extracted answers match the ground truth as we find LLMs often output the whole calculation process.
9https://github.com/china-ai-law-challenge/CAIL2019/tree/master
10https://github.com/HillZhang1999/MuCGEC/tree/main/scorers/ChERRANT
10Model Parameters SFT RLHF Access BaseModel
Multilingual LLMs
MPT 7B ✗ ✗ Weights -
MPT-Instruct 7B ✓ ✗ Weights MPT-7B
LLaMA 7/13/30/65B ✗ ✗ Weights -
LLaMA-2 7/13/70B ✓ ✗ Weights -
LLaMA-2-Chat 7/13/70B ✓ ✓ Weights LLaMA-2-7/13/70B
Alpaca-v1.0 7B ✓ ✗ Weights LLaMA-7B
Vicuna-v1.3 7/13/33B ✓ ✗ Weights LLaMA-7/13/33B
WizardLM 7B ✓ ✗ Weights LLaMA-7B
StableBeluga2 70B ✓ ✗ Weights LLaMA-2-70B
ChatGPT N/A ✓ ✓ API -
GPT-4 N/A ✓ ✓ API -
Chinese-oriented LLMs
MOSS-Moon 16B ✗ ✗ Weights -
MOSS-Moon-SFT 16B ✓ ✗ Weights MOSS-Moon
TigerBot-Base 7B ✗ ✗ Weights -
TigerBot-SFT 7B ✓ ✗ Weights TigerBot-Base
GoGPT 7B ✓ ✗ Weights LLaMA-7B
ChatGLM2 6B ✓ ✗ Weights ChatGLM
Ziya-LLaMA 13B ✓ ✓ Weights LLaMA-13B
Baichuan 7/13B ✗ ✗ Weights -
Baichuan-13B-Chat 13B ✓ ✗ Weights Baichuan-13B
XVERSE 13B ✗ ✗ Weights -
InternLM 7B ✗ ✗ Weights -
InternLM-Chat 7B ✓ ✗ Weights InternLM-7B
InternLM-Chat-8K 7B ✓ ✗ Weights InternLM-7B
Qwen 7B ✗ ✗ Weights -
Qwen-Chat 7B ✓ ✗ Weights Qwen-7B
Yulan-Chat-2 13B ✓ ✗ Weights LLaMA-2-13B
BELLE-LLaMA-2 13B ✓ ✗ Weights LLaMA-2-13B
Chinese-LLaMA-2 7B ✓ ✗ Weights LLaMA-2-7B
Chinese-Alpaca-2 7B ✓ ✗ Weights LLaMA-2-7B
LLaMA-2-Chinese 7/13B ✓ ✗ Weights LLaMA-2-7/13B
Legal Specific LLMs
HanFei 7B ✓ ✗ Weights HanFei
LaWGPT-7B-beta1.0 7B ✓ ✗ Weights Chinese-LLaMA
LaWGPT-7B-beta1.1 7B ✓ ✗ Weights Chinese-alpaca-plus-7B
LexiLaw 6B ✓ ✗ Weights ChatGLM-6B
Wisdom-Interrogatory 7B ✓ ✗ Weights Baichuan-7B
Fuzi-Mingcha 6B ✓ ✗ Weights ChatGLM-6B
Lawyer-LLaMA 13B ✓ ✗ Weights LLaMA
ChatLaw 13/33B ✓ ✗ Weights Ziya-LLaMA-13B/Anima-33B
Table 2: LLMs tested on LawBench. We classify these models by their main training corpora.
Multilingual LLMs We consider 18 open-source multilingual models: MPT-7B, MPT-Instruct-7B,
LLaMA-7B / 13B / 30B / 65B, LLaMA-2-7B / 13B / 70B, LLaMA-2-Chat-7B / 13B / 70B, Alpaca-
v1.0-7B, Vicuna-v1.3-7B / 13B / 33B, WizardLM-7B, StableBeluga2. In addition, two commercial
models, ChatGPT and GPT-4, are included.
Chinese Oriented LLMs A number of Chinese-oriented LLMs are proposed to enhance Chinese
comprehension. Their typically perform better than multilingual models on Chinese NLP tasks. We
include 21 open-sourced, Chinese-oriented LLMs in our evaluation: CMOSS-Moon / Moon-SFT,
TigerBot-Base / SFT, GoGPT, ChatGLM2-6B, Ziya-LLaMA-13B, Baichuan-7B / 13B / 13B-Chat,
XVERSE-13B, InternLM-7B / Chat-7B / Chat-7B-8K, Qwen-7B / 7B-Chat, BELLE-LLaMA-2,
Chinese-LLaMA-2-7B, Chinese-Alpaca-2-7B, LLaMA-2-Chinese-7B / 13B.
Legal Specific LLMs Certain Chinese-oriented LLMs are further fine-tuned on Chinese corpus in
legal domain to improve LLMs’ understanding of Chinese laws. They are of particular interest to us;
through our benchmark, we can rigorously gauge their true advance compared to general-purpose
LLMs and identify their limitations. Here, we provide detailed descriptions of these models:
11GPT-4
ChatGPT
StableBeluga2Qwen-7B-Chat
InternLM-Chat-7B-8KInternLM-Chat-7BYulan-Chat-2-13BFuzi-MingchaChatLaw-13B
Wisdom-InterrogatoryBELLE-LLaMA-2HanFeiLexiLaw
ChatLaw-33BTigerBot-SFTLawyer-LLaMATigerBot-Base
Baichuan-13B-ChatChatGLM2-6BQwen-7B
Baichuan-13B
Ziya-LLaMA-13BInternLM-7B
MOSS-Moon-SFTBaichuan-7BMOSS-Moon
LLaMA-2-Chat-70BLLaMA-2-Chat-13BWizardLM-7B
Chinese-Alpaca-2-7BLLaMA-2-70BXVERSE-13B
Vicuna-v1.3-13BLLaMA-2-7B
Vicuna-v1.3-33BVicuna-v1.3-7BMPT-7B
MPT-Instruct-7BAlpaca-v1.0-7B
LaWGPT-7B-beta1.1LLaMA-2-13BLLaMA-7BLLaMA-30BGoGPT
Chinese-LLaMA-2-7BLLaMA-13BLLaMA-65B
LLaMA-2-Chat-7B
LLaMA-2-Chinese-13BLLaMA-2-Chinese-7BLaWGPT-7B-beta1.00510152025303540455055 Zero-shot Result(%)Multilingual LLMs
Chinese Oriented LLMs
Law Specific LLMsFigure 3: Average performance (zero-shot) of 51 LLMs evaluated on LawBench.
•ChatLaw[ 14]: ChatLaw-13B is fine-tuned based on Ziya-LLaMA-13B-v1, ChatLaw-33B is
fine-tuned based on Anima-33B.
•LaywerLLaMA[ 25]: based on Chinese-LLaMA-13B, fine-tuned with general and legal
instructions.
•FuziMingcha[ 68]: based on ChatGLM, fine-tuned with unsupervised Chinese judicial texts
along with supervised legal fine-tuning datasets.
• HanFei[22]: a fully pre-trained and fine-tuned legal model with 7 billion parameters.
•LaWGPT[ 1]: LaWGPT-7B-beta1.0 is further pre-trained on 500k Chinese judgment docu-
ments upon Chinese-LLaMA-7B and fine-tuned based on the Legal-Base-7B with instruc-
tions. LaWGPT-7B-beta1.1 is fine-tuned based on the Chinese-alpaca-plus-7B with 350k
legal Q&A datasets.
•LexiLaw[ 2]: a fine-tuned Chinese legal model based on the ChatGLM-6B with legal datasets.
•WisdomInterrogatory[ 3]: a further pre-trained and fine-tuned model built upon Baichuan-7B.
4.2 Experiment Setting
We employ OpenCompass [ 13] to perform model inference. For both ChatGPT and GPT-4, we set
the temperature at 0.7 and top pto 1. For other chat models, we tailor the prompt using prefixes and
suffixes specific to each model. Greedy decoding is performed during generation for all open-sourced
models. We set the input token length limit to 2048 and an output token length to 1024. Right
truncation is performed for input prompts exceeding the length limitation. We evaluate all models
in both zero-shot and one-shot settings. The model input in zero-shot inference is merely the task
instruction and the query (see Appendix A for individual instructions and queries). To build the
model input for one-shot inference, a single example including the query and corresponding answer
is attached after the instruction, followed by the actual query to the model.
4.3 Main Results
Figure 3 shows the overall zero-shot performance of each model. As can be seen, GPT-4 and ChatGPT
clearly lead the benchmark, substantially outperform all other models. Under the same model size
(7B-13B), Chinese oriented LLMs outperform multilingual models such as MPT and Llama by a
significant margin, confirming the effectiveness of pre-training and fine-tuning on Chinese data.
Interestingly, legal specific LLMs do not necessarily outperform general-purpose Chinese oriented
LLMs. Close inspection reveals that existing legal specific LLMs are based on rather weak foundation
models, implying that improved models may be obtained by fine-tuning a stronger foundation model
(see Section 4.4 for more detail).
12TaskMultilingual LLMs Chinese Oriented LLMs Legal Specific LLMs
GPT-4 ChatGPT Qwen-Chat InternLM-Chat-8K Fuzi-Mingcha ChatLaw-13B
0-shot 1-shot 0-shot 1-shot 0-shot 1-shot 0-shot 1-shot 0-shot 1-shot 0-shot 1-shot
1-1 15.38 17.21 15.86 16.15 18.54 17.73 15.45 15.16 25.22 20.21 14.85 15.98
1-2 55.2 54.8 36.0 37.2 34.0 28.6 40.4 40.6 7.8 12.8 28.4 29.4
2-1 12.53 18.31 9.1 13.5 22.56 25.16 22.64 21.64 4.93 2.86 12.22 13.01
2-2 41.65 46.0 32.37 40.6 27.42 27.4 35.46 36.6 19.59 2.4 2.68 9.0
2-3 69.79 69.99 51.73 54.01 31.42 32.96 28.96 30.91 28.46 17.44 42.24 30.91
2-4 44.0 44.4 41.2 41.4 35.0 31.2 35.6 33.2 18.6 8.8 27.6 26.6
2-5 56.5 64.8 53.75 61.98 48.48 46.71 54.13 54.35 97.59 93.35 39.11 41.41
2-6 76.6 79.96 69.55 74.04 37.88 57.34 17.95 26.86 44.07 42.28 54.89 60.68
2-7 37.92 40.52 33.49 40.68 36.04 42.58 27.11 30.56 54.32 31.43 38.45 42.71
2-8 61.2 59.0 36.4 37.4 24.0 26.8 36.2 30.6 8.8 11.4 18.6 20.2
2-9 78.82 76.55 66.48 67.59 44.88 50.63 62.93 63.42 16.9 21.26 31.74 40.27
2-10 65.09 65.26 39.05 40.04 18.9 21.27 20.94 20.69 7.78 7.04 14.56 17.37
3-1 52.47 53.2 29.5 30.81 44.62 52.86 34.86 38.88 25.19 3.86 33.28 25.99
3-2 27.54 33.15 31.3 34.49 33.5 34.49 19.11 28.7 22.18 32.96 31.55 33.96
3-3 41.99 41.3 35.52 34.55 40.67 39.91 41.05 42.25 55.93 43.6 27.9 12.24
3-4 82.62 83.21 78.75 77.12 76.74 78.47 63.21 67.74 77.23 78.95 76.18 74.31
3-5 81.91 82.74 76.84 73.72 77.19 73.92 67.2 71.1 75.52 79.0 73.57 73.01
3-6 48.6 49.6 27.4 31.6 26.8 26.8 34.2 36.2 7.0 13.8 28.8 26.8
3-7 77.6 77.0 61.2 66.4 42.0 44.6 43.8 44.0 47.2 38.2 41.4 42.0
3-8 19.65 19.9 17.45 17.17 19.32 20.39 13.37 12.11 16.64 13.95 17.17 16.72
A VG 52.35 53.85 42.15 44.52 37.00 38.99 35.73 37.28 33.05 28.78 32.76 32.63
%abstention 0% 25% 50% 75% 100%
Table 3: Model performance of top two performing systems from each category. qwen-chat and InternLM-chat
both have 7B parameters. Cells are colored according to model abstention rate. Further results are in Appendix
B. It can be observed that the performance of one-shot surpasses that of zero-shot.
In Table 3, we demonstrate the top 2 performing LLMs from each category on all tasks, together
with the model abstention rate.11We made the following observations. First, there is substantial
variation in the distribution of scores across tasks. The best-performing models, for example, can
achieve a score of more than 60 on tasks 3-4 and 3-5, yet no models manage to exceed 30 on tasks
1-1 and 2-1. This shows that our benchmark adequately assess model capabilities in various aspects.
Second, we notice that GPT-4/ChaGPT not only performs well on the majority of tasks, but also
has a low abstention rate, suggesting that they are excellent at following instructions and providing
responses that are more relevant to the query. Third, it can be observed that GPT models and the
Chinese oriented LLMs can successfully leverage the one-shot example and make more accurate
predictions compared to zero-shot cases. The top-performing legal specific LLMs, however, suffers a
drop in performance after seeing the one-shot example. We hypothesize that due to the fact that they
are primarily trained on legal specific instruction data, their instruction-following skills are negatively
impacted. Fourth, Fuzi-Mingcha scores 97.59 on task 2-4, whereas all other models score less than 65.
Considering its performance on other legal tasks, we suspect that there is a data contamination. This
also highlights potential caveats when evaluating LLMs [ 48]. Overall, it is promising that most LLMs
show some capability in handling legal tasks, but there’s still a substantial room for improvement.
Even the top-performing model, GPT-4, achieves only an average score of 52.35 (zero-shot) / 53.85
(one-shot), highlighting the need for additional efforts in the future.
4.4 Analysis
We find that the current legal specific LLMs do not necessarily outperform general large language
models. We analyze the effect of model size and training approach on large language models to better
understand which aspects most influence model performance.
Scaling up the model size results in better performance in one-shot case. Prior work shows
that larger models perform better in general NLP tasks [ 31]. We analyze whether this finding still
holds in legal domain. Specifically, we select representative models from different categories with
varying model sizes and calculate the overall performance and abstention rate of different tasks. The
results are shown in Figure 4. We observe that increasing the model size typically helps improve
11We present the complete results with the performance achieved by every model on all tasks in Table 24-27
(complete zero-shot results), Table 28-31 (complete one-shot results), in Appendix B.
13010203040506070510152025Score(%)
Zero-shot Score
010203040506070510152025Abstention Rate(%)
Zero-shot Abstention Rate
010203040506070510152025Score(%)
One-shot Score
010203040506070510152025Abstention Rate(%)
One-shot Abstention Rate
LLaMA
LLaMA-2
LLaMA-2-Chat
Vicuna-v1.3
05101520253035
Model size(B)0510152025303540455055Score(%)
05101520253035
Model size(B)0510152025303540455055Abstention Rate(%)
05101520253035
Model size(B)0510152025303540455055Score(%)
05101520253035
Model size(B)0510152025303540455055Abstention Rate(%)
Baichuan
LLaMA-2-Chinese
ChatLawFigure 4: Performances of models with various sizes. It can be observed that scaling up the model size usually
improves the performance, but the improvement is more consistent in the one-shot than in the zero-shot scenario.
3-4 3-5
ChatGPT (June 13)747576777879Score(%)
3-4 3-5
GPT-48283
3-4 3-5
Llama-2-70B-Chat681012141618
3-4 3-5
Qwen-7B-Chat7475767778
3-4 3-5
ChatLaw-33B54565860626466
 zero-shot
one-shot
Figure 5: Comparison between different models for 3-4 and 3-5 tasks. Most LLMs are unable of properly
utilizing legal article content information to aid in judgment prediction. Including the article content often
degrades the performance.
model performance in one-shot settings. Also, the abstention rate becomes lower, indicating that
larger models are better at following instructions. Nevertheless, we find that ChatLaw is an outlier, a
larger size results in lower performance. In zero-shot scenarios, mixed results are observed, simply
increasing the model size may not automatically lead to better performance.
Most LLMs can not efficiently leverage article content. Retrieval augmentation is a common
way to improve the accuracy of generative models [ 34;30;54]. By including the content of the
related legal articles in task 3-4 and feeding them into the LLM input, we replicate the retrieval
augmentation scenario to form task 3-5. The goal is to see if the model can successfully use this
additional knowledge to predict the proper jail sentence when supplied with the relevant article
reference that articulates the range of prison terms. We compare 5 models of various types and
visualize the comparison between task 3-4 and 3-5 in Figure 5. The results show that the vast majority
of models failto make any progress on the jail sentence prediction challenge by using the provided
article content. There is a decrease in performance across the board for most models, including
GPT-4. This suggests that simple retrieval enhancement methods may not bring further improvement,
and how to obtain LLMs that are able to effectively utilize the retrieval information is still an open
problem.
SFT may improve the performance but RLHF may not. We illustrate the performance difference
of 9 models before and after SFT in Figure 6. In most cases, models perform better through SFT.
Notably, the SFT data are collected from the general domain, yet they still substantially improve
model performance on legal tasks. The LLaMA-2 series further applies RLHF on top of SFT.
However, we do find that RLHF-trained models can refuse to answer some questions [ 46] (resulting
in a higher abstention rate and lower scores), which can lead to a drop in performance on legal
14MPT-7B Alpaca-
v1.0-7BVicuna-
v1.3-13BWizardLM-7B Moss-Moon TigerBot Baichuan-13B InternLM-7B Qwen-7B05101520253035404550556065zero-shot Result(%)SFT models vs. base models
Base Model Score
Base Model Abstention Rate
SFT Model Score
SFT Model Abstention Rate
LLaMa-2-7B LLaMa-2-13B LLaMa-2-70B05101520253035404550556065zero-shot Result(%)RLHF models vs. SFT models
Base Model Score
Base Model Abstention Rate
RLHF Model Score
RLHF Model Abstention Rate
MPT-7B Alpaca-
v1.0-7BVicuna-
v1.3-13BWizardLM-7B Moss-Moon TigerBot Baichuan-13B InternLM-7B Qwen-7B05101520253035404550556065one-shot Result(%)SFT models vs. base models
Base Model Score
Base Model Abstention Rate
SFT Model Score
SFT Model Abstention Rate
LLaMa-2-7B LLaMa-2-13B LLaMa-2-70B05101520253035404550556065one-shot Result(%)RLHF models vs. SFT models
Base Model Score
Base Model Abstention Rate
RLHF Model Score
RLHF Model Abstention RateFigure 6: Comparison of base LLMs and their SFT and RLHF variants. We put the abstention rate bar on
top of the model score bar to visualize the improvement space for (1) correctly following instructions and (2)
task-specific precision. The abstention rate score is averaged only on tasks requiring answer extraction.
tasks. This suggests that when creating models for legal tasks, stacking RLHF on top of SFT can be
counterproductive.
Legal specific fine-tuning is helpful. To assess the impact of legal specific fine-tuning, we contrast
three LLMs fine-tuned for legal tasks with their corresponding base models, as depicted in Figure 7.
It is evident that after legal specific fine-tuning, there is a consistent enhancement of model scores and
reduction of abstention rates. This underscores the efficacy of the training strategies employed. Closer
inspection of the three cognitive levels reveals that Baichuan-7B and LLaMA-13B perform very
poorly on memorizing tasks, which suggests they have not been pre-trained on large, high-quality legal
corpora. Nonetheless, fine-tuning them on legal corpora both lead to significant improvement. Even
for LLMs like Ziya-LLama-13b who have excellent memorization of legal principles, fine-tuning that
is unique to the field of law can significantly boost performance on tests requiring comprehension and
applying skills. Comparing to the LawGPT series, we found that version 1.1, which is fine-tuned only
on 350k instruction data, outperformed version 1.0 which is fine-tuned on 500k judgement documents
followed by 300k instructions. This suggests that beginning with a lower-performing model and
undertaking continuous pre-training, which is both time-consuming and resource-intensive, is inferior
to starting with a superior foundation model and fine-tuning on high-quality instruction data. Future
research may want to try fine-tuning a stronger base model (like qwen-chat or InternLM-chat) for
better performance on legal tasks.
5 Conclusion
Using LLMs to benefit the legal domain is a promising topic. However, existing benchmarks
to measure the legal knowledge of LLMs either focus on a limited subset of tasks, or are based
on American laws in English language. This paper presents LawBench : a meticulously crafted,
comprehensive evaluation benchmark to assess LLMs in performing legal-related tasks under the
Chinese civil law system. We provide structured taxonomy of the skill set required for legal-
related tasks, including 20 diverse tasks corresponding to 3 cognitive dimensions: legal knowledge
memorization, understanding and applying. We undertake a thorough examination of 51 LLMs
15Memorization Understanding Application05101520253035404550556065zero-shot Result(%)Wisdom-Interrogatory vs. Baichuan-7B
Wisdom_Interrogatory_score
Wisdom_Interrogatory_abstention_rate
Baichuan-7B_score
Baichuan-7B_abstention_rate
Memorization Understanding Application05101520253035404550556065zero-shot Result(%)ChatLaw-13B vs. ziya-llama-13B
ChatLaw_13B_score
ChatLaw_13B_abstention_rate
ziya_llama_13B_score
ziya_llama_13B_abstention_rate
Memorization Understanding Application05101520253035404550556065zero-shot Result(%)Lawyer-LLaMA vs. LLaMA-13B
Lawyer_LLaMA_score
Lawyer_LLaMA_abstention_rate
LLaMA_13B_score
LLaMA_13B_abstention_rate
Memorization Understanding Application05101520253035404550556065one-shot Result(%)Wisdom-Interrogatory vs. Baichuan-7B
Wisdom_Interrogatory_score
Wisdom_Interrogatory_abstention_rate
Baichuan-7B_score
Baichuan-7B_abstention_rate
Memorization Understanding Application05101520253035404550556065one-shot Result(%)ChatLaw-13B vs. ziya-llama-13B
ChatLaw_13B_score
ChatLaw_13B_abstention_rate
ziya_llama_13B_score
ziya_llama_13B_abstention_rate
Memorization Understanding Application05101520253035404550556065one-shot Result(%)Lawyer-LLaMA vs. LLaMA-13B
Lawyer_LLaMA_score
Lawyer_LLaMA_abstention_rate
LLaMA_13B_score
LLaMA_13B_abstention_rateFigure 7: Comparison between different legal specific LLMs and their base models. Legal specific fine-tuning
significantly improves the performance and reduces the abstention rate.
and assess their performance. The results demonstrate that current LLMs are still unable to give
meaningful judicial aid, and their scores on most tasks are often poor. While fine-tuning open-source
LLMs on legal specific language results in some advances, they still lag far below GPT-4. As the
legal field is highly professional, much of the data used in practical applications is confidential.
Developing a high-quality large language model for legal tasks necessitates collaboration among
multiple institutions. We hope the release of LawBench can serve as a foundation for future research
and we seek to encourage cooperation in order to further this effort.
Limitations
The majority of our datasets are acquired through sampling publicly available data on the internet.
Even though we have made efforts to select newest versions of datasets, there can still be risks of
test data leakage given that existing LLMs have been exhaustively trained on massive amount of
Internet data. It is possible that LLMs explicitly trained on these task formats, or even the exact test
data, can exhibit exceptionally high scores [ 48]. We will seek more principled ways to prevent data
contamination in the future.
Another notable limitation is the answer extraction methods and evaluation metrics for generative
tasks. Even though we have hand-engineered task-specific rules to extract the answer, there still
can be cases that the rule fails to match. For generative tasks, we only use Rouge-L to evaluate the
model predictions for convenience, which cannot fully reflect the human judgement about the answer
quality. Currently, there is a lack of automated methods to effectively evaluate model predictions
from legal aspects. We plan to consider training an evaluation model tailored for legal tasks in the
future, or experiment with LLM-based evaluations [39; 73].
Acknowledgments
This work is an extension which was supported by National Key R&D Program of China
(2016YFC0800803). This work is also partly supported by the National Key R&D Program of
China No.2022ZD016100 and Shanghai Postdoctoral Excellence Program (No.2022235). We appre-
16ciate Zhixin Yin for helping arrange tables and alter chart layouts. We also thank Qi Li for collecting
and organizing some public data.
References
[1] Lawgpt. https://github.com/pengxiao-song/LaWGPT , 2023.
[2] Lexilaw. https://github.com/CSHaitao/LexiLaw , 2023.
[3] Wisdominterrogatory. https://github.com/zhihaiLLM/wisdomInterrogatory , 2023.
[4]Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy.
Evaluating correctness and faithfulness of instruction-following models for question answering.
arXiv preprint arXiv:2307.16877 , 2023.
[5]Michael Bommarito II and Daniel Martin Katz. Gpt takes the bar exam. arXiv preprint
arXiv:2212.14402 , 2022.
[6]Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier
Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems
and fundamental limitations of reinforcement learning from human feedback. arXiv preprint
arXiv:2307.15217 , 2023.
[7]Ilias Chalkidis. Chatgpt may pass the bar exam soon, but has a long way to go for the lexglue
benchmark. arXiv preprint arXiv:2304.12202 , 2023.
[8]Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androut-
sopoulos. Legal-bert: The muppets straight out of law school. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages 2898–2904, 2020.
[9]Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos, Daniel
Katz, and Nikolaos Aletras. Lexglue: A benchmark dataset for legal language understanding
in english. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 4310–4330, 2022.
[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.
[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.
[12] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing
systems , 30, 2017.
[13] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation
models. https://github.com/open-compass/opencompass , 2023.
[14] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large
language model with integrated external knowledge bases. arXiv preprint arXiv:2306.16092 ,
2023.
[15] Junyun Cui, Xiaoyu Shen, Feiping Nie, Zheng Wang, Jinglong Wang, and Yulong Chen. A
survey on legal judgment prediction: Datasets, metrics, models and challenges. arXiv preprint
arXiv:2204.04859 , 2022.
[16] Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama
and alpaca. arXiv preprint arXiv:2304.08177 , 2023.
[17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.
GLM: General language model pretraining with autoregressive blank infilling. In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) , pages 320–335, Dublin, Ireland, May 2022. Association for Computational Linguistics.
17[18] Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi
Lei, Renting Rui, Jianghao Lin, Yuchen Fang, et al. Codeapex: A bilingual programming
evaluation benchmark for large language models. arXiv preprint arXiv:2309.01940 , 2023.
[19] Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought
hub: A continuous effort to measure large language models’ reasoning performance. arXiv
preprint arXiv:2305.17306 , 2023.
[20] Jidong Ge, Yunyun Huang, Xiaoyu Shen, Chuanyi Li, and Wei Hu. Learning fine-grained
fact-article correspondence in legal cases. IEEE/ACM Transactions on Audio, Speech, and
Language Processing , 29:3694–3706, 2021.
[21] Neel Guha, Julian Nyarko, Daniel E Ho, Christopher Ré, Adam Chilton, Aditya Narayana, Alex
Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel N Rockmore, et al. Legalbench: A
collaboratively built benchmark for measuring legal reasoning in large language models. arXiv
preprint arXiv:2308.11462 , 2023.
[22] Wanwei He, Jiabao Wen, Lei Zhang, Hao Cheng, Bowen Qin, Yunshui Li, Zhijian Li, Feng Jiang,
Junying Chen, Benyou Wang, and Min Yang. Hanfei. https://github.com/siat-nlp/
HanFei , 2023.
[23] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. In International
Conference on Learning Representations , 2020.
[24] Fan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential
and limitations of chatgpt in explaining implicit hate speech. In Companion Proceedings of the
ACM Web Conference 2023 , pages 294–297, 2023.
[25] Quzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen, Zirui Wu,
and Yansong Feng. Lawyer llama technical report. arXiv preprint arXiv:2305.15062 , 2023.
[26] Yunyun Huang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, and Bin Luo. Dependency learning for le-
gal judgment prediction with a unified text-to-text transformer. arXiv preprint arXiv:2112.06370 ,
2021.
[27] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng
Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval: A multi-level multi-discipline
chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322 , 2023.
[28] Wonseok Hwang, Dongjun Lee, Kyoungyeon Cho, Hanuhl Lee, and Minjoon Seo. A multi-task
benchmark for korean legal language understanding and judgement prediction. Advances in
Neural Information Processing Systems , 35:32537–32551, 2022.
[29] Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, and Xiangang Li.
Belle: Be everyone’s large language model engine. https://github.com/LianjiaTech/
BELLE , 2023.
[30] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming
Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint
arXiv:2305.06983 , 2023.
[31] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361 , 2020.
[32] David R Krathwohl. A revision of bloom’s taxonomy: An overview. Theory into practice ,
41(4):212–218, 2002.
[33] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop,
Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human
feedback with ai feedback. arXiv preprint arXiv:2309.00267 , 2023.
18[34] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented
generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing
Systems , 33:9459–9474, 2020.
[35] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and
Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese.
arXiv preprint arXiv:2306.09212 , 2023.
[36] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,
Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of
language models. arXiv preprint arXiv:2211.09110 , 2022.
[37] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out , pages 74–81, 2004.
[38] Hongcheng Liu, Yusheng Liao, Yutong Meng, and Yuhao Wang. Lawgpt: Chinese legal
dialogue large language model. https://github.com/LiuHC0428/LAW_GPT , 2023.
[39] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,
Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around
player? arXiv preprint arXiv:2307.06281 , 2023.
[40] Antoine Louis and Gerasimos Spanakis. A statutory article retrieval dataset in french. In
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 6789–6803, 2022.
[41] Antoine Louis, Gijs Van Dijck, and Gerasimos Spanakis. Finding the law: Enhancing statutory
article retrieval via graph neural networks. In Proceedings of the 17th Conference of the
European Chapter of the Association for Computational Linguistics , pages 2753–2768, 2023.
[42] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning
benchmark dataset for code understanding and generation. In Thirty-fifth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track (Round 1) , 2021.
[43] Ella Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor, and Omri Abend.
DisentQA: Disentangling parametric and contextual knowledge with counterfactual question
answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 10056–10070, Toronto, Canada, July 2023. Association
for Computational Linguistics.
[44] OpenAI. Introducing chatgpt, 2022.
[45] OpenAI. Gpt-4 technical report, 2023.
[46] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
follow instructions with human feedback. Advances in Neural Information Processing Systems ,
35:27730–27744, 2022.
[47] Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexan-
der Kranias, John J Nay, Kshitij Gupta, and Aran Komatsuzaki. Arb: Advanced reasoning
benchmark for large language models. arXiv preprint arXiv:2307.13692 , 2023.
[48] Rylan Schaeffer. Pretraining on the test set is all you need. arXiv preprint arXiv:2309.08632 ,
2023.
[49] Xiaoyu Shen, Akari Asai, Bill Byrne, and Adria De Gispert. xPQA: Cross-lingual product ques-
tion answering in 12 languages. In Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 5: Industry Track) , pages 103–115, Toronto, Canada,
July 2023. Association for Computational Linguistics.
19[50] Xiaoyu Shen, Gianni Barlacchi, Marco Del Tredici, Weiwei Cheng, Bill Byrne, and Adrià
de Gispert. Product answer generation from heterogeneous sources: A new benchmark and best
practices. In Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5) , pages
99–110, 2022.
[51] Xiaoyu Shen, Gianni Barlacchi, Marco Del Tredici, Weiwei Cheng, and Adrià de Gispert.
semipqa: A study on product question answering over semi-structured data. In Proceedings of
the Fifth Workshop on e-Commerce and NLP (ECNLP 5) , pages 111–120, 2022.
[52] Xiaoyu Shen, Youssef Oualil, Clayton Greenberg, Mittul Singh, and Dietrich Klakow. Estima-
tion of gap between current language models and human performance. 2017.
[53] Xiaoyu Shen, Svitlana Vakulenko, Marco Del Tredici, Gianni Barlacchi, Bill Byrne, and Adrià
de Gispert. Low-resource dense retrieval for open-domain question answering: A comprehensive
survey. arXiv preprint arXiv:2208.03197 , 2022.
[54] Xiaoyu Shen, Svitlana Vakulenko, Marco Del Tredici, Gianni Barlacchi, Bill Byrne, and Adrià
de Gispert. Neural ranking with weak supervision for open-domain question answering: A
survey. In Findings of the Association for Computational Linguistics: EACL 2023 , pages
1691–1705, 2023.
[55] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.
Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
Transactions on Machine Learning Research , 2023.
[56] Hui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang Yu, and Jie Zhou.
Welm: A well-read pre-trained language model for chinese. arXiv preprint arXiv:2209.10372 ,
2022.
[57] Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang
Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, et al. Moss: Training conversational language
models from synthetic data. 2023.
[58] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won
Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei.
Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint
arXiv:2210.09261 , 2022.
[59] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui,
Zhe Zhao, Jai Gupta, et al. Transformer memory as a differentiable search index. Advances in
Neural Information Processing Systems , 35:21831–21843, 2022.
[60] InternLM Team. Internlm: A multilingual language model with progressively enhanced
capabilities. https://github.com/InternLM/InternLM-techreport , 2023.
[61] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
[62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
[63] Arianna Trozze, Toby Davies, and Bennett Kleinberg. Large language models in cryptocurrency
securities cases: Can chatgpt replace lawyers? arXiv preprint arXiv:2308.06032 , 2023.
[64] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems. Advances in neural information processing systems , 32, 2019.
[65] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. In
Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP , pages 353–355, 2018.
20[66] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying
Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, et al. Cmb: A comprehensive
medical benchmark in chinese. arXiv preprint arXiv:2308.08833 , 2023.
[67] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv
preprint arXiv:2109.01652 , 2021.
[68] Shiguang Wu, Zhongkun Liu Liu, Zhen Zhang Zhang, Zheng Chen, Wentao Deng, Wenhao
Zhang, Jiyuan Yang, Zhitao Yao, Yougang Lyu, Xin Xin, Shen Gao, Pengjie Ren, Zhaochun
Ren, and Zhumin Chen. fuzi.mingcha. https://github.com/irlab-sdu/fuzi.mingcha ,
2023.
[69] Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, and Maosong Sun. Lawformer: A
pre-trained language model for chinese legal long documents. AI Open , 2:79–84, 2021.
[70] Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn
sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint
arXiv:2305.13300 , 2023.
[71] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun,
Dian Yu, Cong Yu, et al. Clue: A chinese language understanding evaluation benchmark.
InProceedings of the 28th International Conference on Computational Linguistics , pages
4762–4772, 2020.
[72] Feng Yao, Chaojun Xiao, Xiaozhi Wang, Zhiyuan Liu, Lei Hou, Cunchao Tu, Juanzi Li, Yun Liu,
Weixing Shen, and Maosong Sun. Leven: A large-scale chinese legal event detection dataset. In
Findings of the Association for Computational Linguistics: ACL 2022 , pages 183–201, 2022.
[73] Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun
Yao, Xiaohan Zhang, Hanming Li, et al. Kola: Carefully benchmarking world knowledge of
large language models. arXiv preprint arXiv:2306.09296 , 2023.
[74] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chen-
guang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language
models are strong context generators. In The Eleventh International Conference on Learning
Representations , 2023.
[75] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan
Zhou, Yao Xiao, Song Yun, Wei Lin, Xuanjing Huang, and Zhongyu Wei. Disc-lawllm:
Fine-tuning large language models for intelligent legal services, 2023.
[76] Ji Yunjie, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and
Xiangang Li. Exploring the impact of instruction data scaling on large language models: An
empirical study on real-world use cases. arXiv preprint arXiv:2303.14742 , 2023.
[77] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,
Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. In
The Eleventh International Conference on Learning Representations , 2022.
[78] Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, Wei Dai, Yujie Liao, Qianru Qin, Yifei Li,
Xingyu Liu, Zhiqiang Liu, et al. Fineval: A chinese financial domain knowledge evaluation
benchmark for large language models. arXiv preprint arXiv:2308.09975 , 2023.
[79] Qingyu Zhang, Xiaoyu Shen, Ernie Chang, Jidong Ge, and Pengke Chen. Mdia: A benchmark
for multilingual dialogue generation in 46 languages. arXiv preprint arXiv:2208.13078 , 2022.
[80] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Eval-
uating the performance of large language models on gaokao benchmark. arXiv preprint
arXiv:2305.12474 , 2023.
21[81] Yue Zhang, Zhenghua Li, Zuyi Bao, Jiacheng Li, Bo Zhang, Chen Li, Fei Huang, and Min
Zhang. Mucgec: a multi-reference multi-source evaluation dataset for chinese grammatical
error correction. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies , pages 3118–3130,
2022.
[82] XUJIANG ZHAO, JIAYING LU, CHENGYUAN DENG, CAN ZHENG, JUNXIANG WANG,
TANMOY CHOWDHURY , LI YUN, HEJIE CUI, ZHANG XUCHAO, TIANJIAO ZHAO, et al.
Domain specialization as the key to make large language models disruptive: A comprehensive
survey. arXiv preprint arXiv:2305.18703 , 2023.
[83] Henry R Zheng. China’s new civil law. The American Journal of Comparative Law , 34(4):669–
704, 1986.
[84] Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun.
Legal judgment prediction via topological learning. In Proceedings of the 2018 conference on
empirical methods in natural language processing , pages 3540–3549, 2018.
[85] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun.
Jec-qa: a legal-domain question answering dataset. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 34, pages 9701–9708, 2020.
[86] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation
models. arXiv preprint arXiv:2304.06364 , 2023.
[87] Noah Ziems, Wenhao Yu, Zhihan Zhang, and Meng Jiang. Large language models are built-in
autoregressive search engines. arXiv preprint arXiv:2305.09612 , 2023.
22A Details of Task Instruction
A.1 Legal Knowledge Memorization Tasks
INSTRUCTION :Answer the following questions, just give the content of the law directly:
QUESTION :What is the content of Article 257 of the Criminal Law?
ANSWER :Anyone who uses violence to interfere with the marriage freedom of others shall be sentenced to
fixed-term imprisonment of no more than two years or detention. Anyone who commits the crime mentioned in
the preceding paragraph and causes the death of the victim shall be sentenced to fixed-term imprisonment of no
less than two years and no more than seven years.
Table 4: The instruction and an example of Task 1-1 Article Recitation.
INSTRUCTION :Please apply your legal knowledge to select the correct answer from A, B, C, or D and write
it between [Correct Answer] and <eoa>. For example, [Correct Answer] A <eoa>. Please strictly follow this
format when answering.
QUESTION :According to the laws of our country, in the process of trial of foreign-related divorce cases
accepted by our courts, which of the following should be used as the basis for determining the validity of the
marriage? A: The law of the place where the marriage was concluded; B: The law of the parties’ home country;
C: The law of the parties’ place of residence; D: The law of the court.
ANSWER :[Correct Answer] A<eoa>
Table 5: The instruction and an example of Task 1-2 Knowledge Question Answering.
23A.2 Legal Knowledge Understanding Tasks
INSTRUCTION :Correct the spelling, redundancy, omission, and disorder errors of the words in the following
legal document sentences, and minimize modifications to the original sentences while preserving their semantics.
Only the modified sentences need to be provided in the answer, strictly following this format. Sentence:
QUERY :The above-mentioned procedure for collecting the evidence for the appeal is legal, and the content is
objective and true, which is sufficient to establish the accusation.
ANSWER :The above-mentioned procedure for collecting evidence is legal, and the content is objective and true,
which is sufficient to establish the facts of the accusation.
Table 6: The instruction and an example of Task 2-1 Document Proofread.
INSTRUCTION :Determine the category of the dispute focus contained in the sentence. Each sentence contains
only one dispute focus category. Categories include: litigation parties, rent situation, interest, principal dispute,
liability determination, liability division, loss determination and handling, whether the original judgment is
appropriate, contract effectiveness, property division, liability assumption, admissibility of appraisal conclusions,
statute of limitations, breach of contract, contract termination, hit-and-run. Write the answer between [Dispute
Focus] and <eoa>, such as [Dispute Focus] Principal Dispute <eoa>.
QUERY :Sentence: The plaintiff alleges: I have a father-son relationship with the defendant. The house located
at Unit 321, Building 7, 3 North Xiaojie, Sanlitun North, Chaoyang District, Beijing (hereinafter referred to
as the ’involved property’) was allocated to me by the Ministry of Foreign Affairs Diplomatic Service Bureau
during my time at TIME. My unit underwent housing reform, and I purchased the property at TIME. I have been
living in the involved property from TIME to the present. In TIME, the defendant started giving various reasons
for not allowing me to reside in the involved property, which raised my suspicions. In TIME, I went to ORG
to inquire about the status of the involved property and discovered that in TIME, the defendant deceived me
into signing an ’Existing Property Sale Contract’ and processed the property transfer procedures at the Housing
Management Bureau, transferring the involved property into the defendant’s name. I believe I have never sold
the involved property, and the defendant deceived me into signing the ’Existing Property Sale Contract.’ The
defendant also did not actually pay the purchase price specified in the property sale contract. Therefore, I have
filed a lawsuit to the court requesting the cancellation of the ’Property Sale Contract’ signed between me and
the defendant and the transfer of the property to my name. The defendant argues: The plaintiff and I signed
the contract voluntarily. The involved property is state-owned property, and the sale of state-owned property
requires strict approval procedures. All the documents were signed by the plaintiff himself, and I also fully paid
the purchase price. Now the plaintiff is backtracking because several other children are manipulating the elderly,
and the elderly are confused. I do not agree with the plaintiff’s lawsuit request.
ANSWER :Dispute focus category: contract effectiveness <eoa>
Table 7: The instruction and an example of Task 2-2 Dispute Focus Identification.
24INSTRUCTION :Please categorize these sentences based on tags, which include: having children after marriage,
custody of children with limited capacity, having joint property, paying child support, dividing real estate, post-
marital separation, filing for divorce for the second time, monthly payment of child support, granting divorce,
having joint debt, personal property before marriage, legal divorce, failure to fulfill family obligations, existence
of non-marital children, appropriate assistance, failure to comply with divorce agreements, compensation for
damages, separation due to emotional discord for over two years, children living with non-custodial parent,
personal property after marriage. Please write the answer between [category] and <eoa>, for example, [category]
division of real estate, joint property <eoa>. Please strictly follow this format.
QUERY :In 2014, Wang X sued and demanded a share of the down payment of 150,000 yuan on the grounds
that he and Xiao X had not divided the down payment of the building when they agreed to divorce.
ANSWER :[category] dividing real estate, having joint property <eoa>
Table 8: The instruction and an example of Task 2-3 Marital Disputes Identification.
INSTRUCTION :Please determine the category for the following consultation. Each consultation belongs to only
one category, which includes: Marriage and Family, Labor Disputes, Traffic Accidents, Debt Collection and Debt
Disputes, Criminal Defense, Contract Disputes, Real Estate Disputes, Infringement, Corporate Law, Medical
Disputes, Demolition and Resettlement, Administrative Litigation, Construction Engineering, Intellectual
Property Rights, Comprehensive Consultation, Personal Injury, Foreign-related Law, Maritime Law, Consumer
Rights, Mortgage and Guarantee. Please write the answer between [Category] and <eoa>, for example [Category]
Marriage and Family <eoa>. Please strictly follow this format when answering.
Consultation:
QUERY :It has been two years since we separated and have not been together. Will the court automatically grant
a divorce? The other party has sued for the dowry money, but the case has not been settled. Will this affect the
other party’s ability to remarry and obtain a marriage certificate?
ANSWER :Marriage and Family
Table 9: The instruction and an example of Task 2-4 Issue Topic Identification.
INSTRUCTION :Please answer the questions based on the provided paragraph as concisely as possible.
QUERY :Paragraph: Based on the valid evidence submitted by the plaintiff to the court and the plaintiff’s oral
statements in court, this court confirms the following facts in the case: The defendant Li2 and the defendant Li
x3 are mother and daughter. On September 26, 2014, the defendant Li2 borrowed 50,000 yuan from Sang x0,
and issued a promissory note stating: ’I have borrowed 50,000 yuan in cash from Sang x0 today. The loan term
is 3 months, and I will repay it on time every month with an interest of 2,500 yuan per month. If I cannot repay
it at the due date, I am willing to use all my assets to settle the debt and take full responsibility. Borrower: Si x1.
Spouse: Li2. Willing to take full responsibility.’ On November 13, 2014, with the guarantee of the defendant
Li x3, the defendant Li2 borrowed 150,000 yuan from Sang x0 and issued a promissory note stating: ’I have
borrowed 150,000 yuan in cash from Sang x0 today. The loan term is three months, and I promise to pay the
principal and interest monthly and return it on time. I hereby promise. Borrower: Li2, Guarantor: Li x3, Spouse:
Si x1.’ On the same day, Li2 issued a repayment guarantee commitment, and Si x1 signed it as a spouse. At
the same time, Li2 and Si x1 issued a mortgage commitment, promising to use the existing house, car, and any
other property in Zone 13 of Dongjiang New Village, Dongjiang Town, as collateral for this loan. However, both
parties did not complete the mortgage registration procedure. After the loan maturity date, the defendant Si x1,
besides paying 22,000 yuan in interest to the plaintiff, did not repay the remaining principal and interest. The
plaintiff then sued in this court, requesting: 1. The defendant Li2 and Si x1 to repay the principal of the loan,
which is 200,000 yuan; 2. The defendant Li2 and Si x1 to repay the loan interest at a rate of 2% per month; 3.
The defendant Li x3 to assume the guarantee liability within the range of 150,000 yuan; 4. The defendant is
to bear the litigation costs. Furthermore, it was found that the defendant Li2 and the defendant Si x1 did not
complete the marriage registration procedure. Question: Who borrowed money from the lender?
ANSWER :Answer: Si x1, Li2
Table 10: The instruction and an example of Task 2-5 Reading Comprehension.
25INSTRUCTION :Based on the provided entity types, extract entity information from the sentence. The entity
types include: suspected criminals, victims, stolen currency, item value, theft profit, stolen items, crime tools,
time, location, and organizations. List the entity information one by one.
QUERY :Sentence: After the case was solved, the public security organ lawfully returned the seized mobile
phones to the victims, Yan and Xiao.
ANSWER :Victims: Yan, Xiao; stolen items: the seized mobile phones; organizations: the public security organ
Table 11: The instruction and an example of Task 2-6 Name Entity Recognition.
INSTRUCTION :Here is a news report, please provide a summary of this report in one sentence.
QUERY :According to a report by Xinmin Evening News (Reporter Xia Yun), at about 9 am today, some
citizens reported to the journalist that a unit on Cao’an Road in Jiading District, Shanghai had a sudden fire,
with thick smoke at the scene; the fire department arrived promptly and the fire was still being put out by the
time of the report, with no casualties reported; the source is the official Weibo account of Shanghai Fire Rescue
Bureau.According to witnesses, when the fire broke out, there was a large amount of smoke above the unit, and
some drivers on the Jiamin elevated road also saw the smoke; after receiving the alarm, the police, fire and other
departments rushed to the scene for disposal; as of 9:30 am, the disposal work was still in progress; according
to the official Weibo of Shanghai Fire Rescue Bureau, at about 8:33 am on March 24, a company in Cao’an
Road, Jiading District had a fire, and the fire department arrived at the scene for disposal, with the fire now under
control and no casualties reported.
ANSWER :At around 9 a.m. this morning, a unit on Cao’an Road in Jiading District, Shanghai had a sudden fire,
with thick smoke at the scene.
Table 12: The instruction and an example of Task 2-7 Opinion Summarization.
INSTRUCTION :Based on a given prosecution point of view, please select a point of view that can form a
controversial point of view from the five defense candidate points of view A, B, C, D, and E, and write the
answer in [Correct Answer] and <eoa>. For example [correct answer]A<eoa>. Please answer strictly according
to this format.
QUERY :Sentence: The private prosecutor Yu XX alleged that at around 17:00, the defendants Bao XX, Han
XX, and Han XX came to my house and beat me on the head with a wooden stick on the grounds that I had
detained their cattle. After I was injured, I was hospitalized at the Xing’an League Mongolian Medical Hospital
for 73 days. A: The defender provided the court with a copy of Lv’s testimony, requesting the court to declare
the defendant Lv innocent and reject the victim’s incidental civil claim. B: I think I am not guilty. C: The
defendant Bao Moumou claimed that I injured the private prosecutor with a whip and agreed to compensate for
the medical expenses. D: 2. The victim’s incidental civil lawsuit should be dismissed. E: In addition, disability
compensation should not be included in the scope of judgment compensation.
ANSWER :[Correct answer]C<eoa>
Table 13: The instruction and an example of Task 2-8 Argument Mining.
26A.3 Legal Knowledge Applying Tasks
INSTRUCTION :Provide the relevant articles in the criminal code based on the following facts and charges.
Please only include the criminal code article numbers, and place your answers between [Article] and <eoa>. For
example, [Article] Article 128 of the Criminal Code, Article 341 of the Criminal Code <eoa>.
QUERY :Fact: The People’s Procuratorate of Gongzhuling City, Jilin Province, alleges that on June 18, 2014,
the defendant, Mr. Zhang, applied for a ’Da Jilin’ credit card at the China Bank Gongzhuling City Branch. As
of August 10, 2014, the outstanding balance reached 14,974.52 yuan. Despite two collection attempts by the
issuing bank’s staff, the defendant still did not repay the amount for over three months.
Charge: Fraud.
ANSWER :Article: Article 196 of the Criminal Code
Table 14: The instruction and an example of Task 3-1 Fact-based Article Prediction.
INSTRUCTION :Please provide legal justification based on specific scenarios and questions, presenting only the
relevant legal articles. Each scenario should be associated with a single legal article.
QUERY :Scenario: A company’s board of directors has decided to appoint one of its directors as a concurrent
manager to enhance the company’s management. According to which legal provision, can the board of directors
make the decision to have a board member serve concurrently as a manager?
ANSWER :According to Article 114 of the Company Law, the board of directors of a company can decide to
appoint one of its members as a concurrent manager.
Table 15: The instruction and an example of Task 3-2 Scene-based Article Prediction.
INSTRUCTION :Please simulate a judge and provide the charge based on the following facts. Only the name
of the charge is required. Place your answer between [Charge] and <eoa>. For example, [Charge] Theft;
Fraud<eoa>. Please strictly adhere to this format in your response.
QUERY :Facts: The People’s Procuratorate of Haidian District, Beijing, in the indictment, alleges the following:
On June 27, 2013, the defendant, Ren (owner of an East Wind Nissan Yida sedan with license plate number Jing
PNB057 and insured under this vehicle), in Haidian District of Beijing, fabricated a fictitious traffic accident
involving an East Wind Nissan Yida sedan (license plate number Jing PNB057) and a BMW sedan (license plate
number Jing QSU596) to deceive China People’s Property Insurance Co., Ltd., Beijing Branch, into paying
RMB 14,160.8.
On August 15, 2013, the defendant, Ren, in Haidian District of Beijing, fabricated a fictitious traffic accident
involving a Jinbei sedan (license plate number Jing KP2320) and a BMW sedan (license plate number Jing
QSU596) to deceive China People’s Property Insurance Co., Ltd., Beijing Branch, into paying RMB 10,231.2.
On September 26, 2013, the defendant, Ren, was summoned by the public security authorities. All the funds
involved in the case were refunded to the victim company after the incident.
ANSWER :Charge: Insurance fraud
Table 16: The instruction and an example of Task 3-3 Charge Prediction.
27INSTRUCTION :Based on the following facts, charges, and articles of the criminal code, predict the length of
the sentence. Only provide the sentence length in months, and place your answer between [Sentence] and <eoa>.
For example, [Sentence] 12 months <eoa>.
QUERY :Facts: The prosecuting authority alleges that on December 23, 2015, the defendant, Yang, took
advantage of his position as the former secretary of Qiansuo Street in Jiaojiang District, Taizhou City, and
misappropriated RMB 140,000 of pre-collected construction fees from Shangxu Village residents. He used this
money to cover expenses related to his eyeglass factory.
On March 20, 2016, the defendant, Yang, once again took advantage of his position and misappropriated RMB
51,765 of Shangxu Village’s funds. He used this money to repay a bank loan for his eyeglass factory.
On March 31 of the same year, the defendant, Yang, returned all the misappropriated funds to the village
collective account. On February 23, 2017, at approximately 10 o’clock, Yang was summoned to the case by the
Economic Investigation Brigade of Jiaojiang Branch, Taizhou Public Security Bureau, at his residence at No. 2
Shangxu Village, Qiansuo Street, Jiaojiang District, Taizhou City.
Charge: Embezzlement
ANSWER :Prison term: 10 months
Table 17: The instruction and an example of Task 3-4 Prison Term Prediction w.o Article.
INSTRUCTION :Based on the following facts, charges, and articles of the criminal code, predict the length of
the sentence. Only provide the sentence length in months, and place your answer between [Sentence] and <eoa>.
For example, [Sentence] 12 months <eoa>.
QUERY :Facts: The public prosecution accuses that on July 3, 2014, at around 12 o’clock, the defendant, Sun,
became very angry when he found that a fellow villager, Zhang, had not returned home after going out due to
drinking. He then climbed over the wall into Zhang’s courtyard. Inside the house, he smashed and tore apart
Zhang’s TV , stereo, clothes, and other belongings, burning some of the clothes. When Zhang returned home
and tried to stop the defendant, Sun, he was punched in the face by the defendant. According to the assessment,
the damaged property was worth 1,580 yuan. On October 10, 2014, at around 8 pm, the defendant, Sun, sent a
message to Zhang, threatening to jump into his house. Zhang hastily called his brother-in-law, and upon learning
of the situation, his brother-in-law, Sun, reported it to the police. After the incident, both parties reached a
compensation agreement. The defendant, Sun, compensated the victim, Zhang, for his losses and obtained the
victim’s forgiveness.
Charge: Illegal intrusion into a residence.
Legal Article: Article 245 of the Criminal Law.
Legal article content: Article 245 - Whoever illegally searches another person’s body or residence, or illegally
intrudes into another person’s residence, shall be sentenced to fixed-term imprisonment of not more than three
years or criminal detention. If judicial personnel abuse their power and commit the crime mentioned in the
preceding paragraph, they shall be punished more severely.
ANSWER :Prison term: 4 months
Table 18: The instruction and an example of Task 3-5 Prison Term Prediction w. Article.
INSTRUCTION :Please analyze the following case using legal knowledge and select the correct answer from A,
B, C, D, and write it between [Correct Answer] and <eoa>. For example, [Correct Answer] A <eoa>. Please
adhere strictly to this format in your response.
QUERY :During the May Day holiday, tourist A had a conflict with tourist B over the purchase of tickets to
the Potala Palace in Tibet and injured tourist B. Upon investigation, it was found that tourist A, who is of Han
ethnicity, is from Guangdong province but has been living in Fujian for several years and has some knowledge
of Tibetan. In this case, what language should the Lhasa police use to interrogate tourist A? A: Mandarin; B:
Fujianese; C: Cantonese; D: Tibetan.
ANSWER :Correct answer: D.
Table 19: The instruction and an example of Task 3-6 Case Analysis.
28INSTRUCTION :Please calculate the total amount of the crimes mentioned in the document carefully. You
don’t need to provide the calculation process; just provide the final amount between [Amount] and <eoa>. For
example, [Amount] 2000 yuan <eoa>.
QUERY :Document: Upon trial and investigation, it was ascertained that in the early hours of September 23,
2016, the defendant, Liang, drove a motorcycle to the entrance of Hutou Village in Pingma Village Committee,
Xiaojiang Street, Pubei County, Guangxi. He stole 11 castrated chickens and 12 hens from the victim, Huang 1,
as well as 2 castrated chickens from Huang 2 and 1 hen from Ning. The defendant was apprehended by Huang 1
and others while fleeing the scene and subsequently reported to the public security authorities. According to the
appraisal conducted by the Price Certification Center of Pubei County Price Bureau, the total market value of the
stolen chickens amounted to RMB 2,094.
ANSWER :The amount involved in the above-mentioned crime: 2094.0 yuan.
Table 20: The instruction and an example of Task 3-7 Crime Amount Calculation.
INSTRUCTION :Please answer the following questions, first provide the answer, and then provide the corre-
sponding legal basis:
QUERY :Is the accident determination related to the driver crossing the centerline and hitting a person, resulting
in their death despite rescue efforts? How long should it take to issue the accident determination report?
ANSWER :Answer: In the case of a traffic accident resulting in a fatality, the traffic management department of
the public security organ should issue the Road Traffic Accident Determination Report within ten days from the
date of on-site investigation. For hit-and-run traffic accident cases, the Road Traffic Accident Determination
Report should be issued within ten days after the apprehension of the hit-and-run vehicle and driver.
Legal Basis: Article 62 of the ’Provisions on the Procedures for Handling Road Traffic Accidents’ states that the
traffic management department of the public security organ should issue the Road Traffic Accident Determination
Report within ten days from the date of on-site investigation in cases of traffic accidents resulting in fatalities. In
hit-and-run cases, the Road Traffic Accident Determination Report should be issued within ten days after the
apprehension of the hit-and-run vehicle and driver. In cases requiring examination or appraisal, the Road Traffic
Accident Determination Report should be issued within five days from the date of determination based on the
examination report or appraisal opinion.
Table 21: The instruction and an example of Task 3-8 Consultation.
29A.4 Statics Information of Our Datasets
IDInstruction Input Output
Zeroshot Oneshot Max Min Avg Max Min Avg
1-1 18 71 36 13 21.742 631 16 103.742
1-2 73 141 429 32 145.41 7 7 7.0
2-1 74 133 452 20 79.614 452 19 79.604
2-2 159 342 4364 156 663.052 16 10 12.534
2-3 214 256 283 10 61.438 48 8 16.006
2-4 174 262 182 6 38.21 4 2 3.85
2-5 21 632 1419 433 734.01 149 6 25.336
2-6 81 122 293 13 69.758 248 0 57.818
2-7 26 556 369 42 323.628 393 27 144.236
2-8 101 389 943 180 382.334 12 12 12.0
2-9 159 208 314 7 55.308 23 2 5.328
2-10 95 146 281 6 62.014 27 1 4.912
3-1 75 182 22142 51 603.968 30 10 11.848
3-2 41 138 154 31 71.206 389 34 127.55
3-3 76 182 9784 26 508.15 52 5 13.082
3-4 68 214 22172 77 615.816 8 5 6.77
3-5 68 214 23421 174 938.584 8 5 6.77
3-6 80 170 858 54 212.434 7 7 7.0
3-7 72 186 1980 63 462.584 22 17 19.386
3-8 28 185 100 50 57.62 731 187 395.02
Table 22: Detailed statistical information for each task, where Instruction represents the length of
instructions, Input represents the length of input questions, and Output represents the length of
answers.
B Details of Results
Task6b 7b 13b 33b
lexilaw lawgpt-beta1.0 lawgpt-beta1.1 hanfei-1.0 fuzi-mingcha wisdom-INT chatlaw lawyer-llama chatlaw
1-1 15.47% 1.24% 3.03% 16.23% 20.21% 37.41% 15.98% 13.04% 14.36%
1-2 14.4% 0.0% 6.6% 8.6% 12.8% 15.2% 29.4% 10.6% 27.8%
2-1 4.18% 2.08% 4.7% 14.98% 2.86% 22.16% 13.01% 4.9% 4.3%
2-2 15.4% 4.0% 3.4% 9.2% 2.4% 10.0% 9.0% 19.2% 12.2%
2-3 21.49% 0.0% 4.61% 32.71% 17.44% 24.29% 30.91% 9.03% 33.49%
2-4 27.2% 0.4% 10.2% 12.6% 8.8% 13.4% 26.6% 3.0% 4.2%
2-5 41.64% 0.79% 2.26% 36.64% 93.35% 13.23% 41.41% 39.65% 38.87%
2-6 31.54% 3.19% 8.11% 52.33% 42.28% 40.1% 60.68% 36.33% 28.83%
2-7 34.57% 0.0% 4.71% 31.15% 31.43% 39.71% 42.71% 37.1% 34.2%
2-8 6.0% 0.0% 0.6% 17.0% 11.4% 0.2% 20.2% 0.4% 15.4%
2-9 19.84 0.7 16.66 25.38 21.26 15.81 40.27 33.19 26.18
2-10 8.36 0.43 0.25 4.09 7.04 4.02 17.37 6.12 15.95
3-1 15.41% 0.0% 0.49% 7.98% 3.86% 20.02% 25.99% 0.33% 4.89%
3-2 33.94% 23.75% 12.54% 34.27% 32.96% 23.33% 33.96% 27.23% 27.97%
3-3 34.03% 0.8% 13.8% 9.8% 43.6% 39.22% 12.24% 19.36% 17.54%
3-4 73.66% 0.01% 40.18% 74.34% 78.95% 81.16% 74.31% 70.99% 63.3%
3-5 70.93% 0.8% 47.3% 72.19% 79.0% 81.57% 73.01% 73.56% 53.03%
3-6 12.2% 0.0% 17.6% 10.4% 13.8% 13.0% 26.8% 6.6% 26.2%
3-7 33.2% 8.0% 26.0% 13.4% 38.2% 40.4% 42.0% 33.8% 43.2%
3-8 14.68% 0.46% 8.77% 14.99% 13.95% 20.67% 16.72% 16.02% 16.29%
%abstention 0% 25% 50% 75% 100%
Table 23: One-shot Results of legal specific LLMs
30model_name 1-1 1-2 Memorization
Multilingual LLMs
MPT-7B 3.57 0.00 1.78
MPT-Instruct-7B 2.30 1.00 1.65
LLaMA-7B 2.42 0.60 1.51
LLaMA-13B 2.64 0.80 1.72
LLaMA-30B 2.99 0.60 1.80
LLaMA-65B 1.99 0.40 1.20
LLaMA-2-7B 2.65 0.40 1.52
LLaMA-2-13B 1.90 1.80 1.85
LLaMA-2-70B 3.15 0.40 1.77
LLaMA-2-Chat-7B 0.80 6.40 3.60
LLaMA-2-Chat-13B 1.05 3.20 2.12
LLaMA-2-Chat-70B 0.87 10.00 5.43
Alpaca-v1.0-7B 1.41 0.20 0.80
Vicuna-v1.3-7B 6.40 8.80 7.60
Vicuna-v1.3-13B 7.21 18.60 12.91
Vicuna-v1.3-33B 10.45 24.40 17.43
WizardLM-7B 0.48 13.80 7.14
StableBeluga2 14.58 34.60 24.59
ChatGPT 15.86 36.00 25.93
GPT-4 15.38 55.20 35.29
Chinese Oriented LLMs
MOSS-Moon 5.04 11.60 8.32
MOSS-Moon-SFT 2.89 7.60 5.25
TigerBot-Base 15.62 3.80 9.71
TigerBot-SFT 11.18 13.60 12.39
GoGPT 6.71 17.40 12.05
ChatGLM2-6B 15.37 10.60 12.98
Ziya-LLaMA-13B 15.04 30.00 22.52
Baichuan-7B 0.03 1.80 0.92
Baichuan-13B 1.29 11.40 6.35
Baichuan-13B-Chat 12.32 34.80 23.56
XVERSE-13B 1.71 0.00 0.85
InternLM-7B 1.56 4.60 3.08
InternLM-Chat-7B 14.39 40.00 27.20
InternLM-Chat-7B-8K 15.45 40.40 27.93
Qwen-7B 0.00 0.00 0.00
Qwen-7B-Chat 18.54 34.00 26.27
Yulan-Chat-2-13B 15.03 22.40 18.72
BELLE-LLaMA-2 14.39 30.60 22.50
Chinese-LLaMA-2-7B 3.24 1.20 2.22
Chinese-Alpaca-2-7B 14.95 4.40 9.67
LLaMA-2-Chinese-7B 0.00 0.00 0.00
LLaMA-2-Chinese-13B 0.00 0.00 0.00
Law Specific LLMs
LexiLaw 16.96 21.00 18.98
Fuzi-Mingcha 25.22 7.80 16.51
HanFei 17.03 19.80 18.42
LaWGPT-7B-beta1.0 9.34 0.00 4.67
LaWGPT-7B-beta1.1 4.53 4.20 4.37
Wisdom-Interrogatory 43.05 15.40 29.23
ChatLaw-13B 14.85 28.40 21.63
Lawyer-LLaMA 12.33 23.20 17.77
ChatLaw-33B 11.74 28.60 20.17
%abstention 0% 25% 50% 75% 100%
Table 24: Zero-shot Results on Legal Knowledge Memorization Tasks
31model_name 2-1 2-2 2-3 2-4 2-5 2-6 2-7 2-8 2-9 2-10 Understanding
Multilingual LLMs
MPT-7B 0.71 7.42 12.50 2.00 3.39 6.21 14.82 2.40 11.81 0.32 6.16
MPT-Instruct-7B 0.93 4.54 11.86 10.80 4.21 17.85 4.38 3.80 9.66 0.66 6.87
LLaMA-7B 0.43 5.77 12.14 3.20 4.57 4.66 15.31 1.00 10.61 0.35 5.80
LLaMA-13B 0.38 6.39 13.00 2.20 4.89 5.59 17.12 0.20 11.90 0.35 6.20
LLaMA-30B 0.44 4.74 7.52 2.20 3.95 2.33 2.66 0.60 9.73 0.39 3.46
LLaMA-65B 0.48 5.15 5.35 1.00 5.31 2.23 3.12 0.20 12.70 0.42 3.60
LLaMA-2-7B 0.89 3.92 20.55 2.20 5.68 29.27 16.58 0.20 11.84 0.38 9.15
LLaMA-2-13B 0.73 6.19 20.90 1.80 5.30 16.36 12.05 0.60 10.13 0.72 7.48
LLaMA-2-70B 2.88 8.87 26.85 10.40 5.59 42.57 16.54 0.00 14.55 0.87 12.91
LLaMA-2-Chat-7B 0.25 5.36 16.92 5.20 4.90 5.62 1.13 6.20 14.77 2.06 6.24
LLaMA-2-Chat-13B 0.84 9.48 40.80 24.80 6.08 2.20 1.16 9.20 35.74 4.25 13.46
LLaMA-2-Chat-70B 0.74 7.22 33.38 30.40 5.22 34.72 1.25 8.40 39.75 12.61 17.37
Alpaca-v1.0-7B 0.30 4.33 6.91 3.80 15.96 4.14 3.85 0.00 5.94 1.07 4.63
Vicuna-v1.3-7B 1.00 6.60 21.56 7.40 4.54 40.92 4.00 2.60 9.62 3.38 10.16
Vicuna-v1.3-13B 1.68 3.09 24.54 11.20 21.82 5.25 3.79 11.00 17.30 4.75 10.44
Vicuna-v1.3-33B 1.76 6.39 19.30 8.20 12.51 18.38 3.35 16.40 9.75 5.88 10.19
WizardLM-7B 0.10 1.86 18.32 7.00 13.54 32.44 4.24 4.60 10.99 3.32 9.64
StableBeluga2 7.70 25.57 44.20 39.00 52.03 65.54 39.07 45.80 65.27 41.64 42.58
ChatGPT 9.10 32.37 51.73 41.20 53.75 69.55 33.49 36.40 66.48 39.05 43.31
GPT-4 12.53 41.65 69.79 44.00 56.50 76.60 37.92 61.20 78.82 65.09 54.41
Chinese Oriented LLMs
MOSS-Moon 3.00 2.68 12.69 1.80 9.24 6.17 10.72 8.20 12.07 1.42 6.80
MOSS-Moon-SFT 6.84 5.15 17.94 3.20 2.55 10.84 6.37 3.00 13.96 1.67 7.15
TigerBot-Base 12.99 5.57 10.72 22.20 24.33 27.48 29.16 13.80 18.59 13.99 17.88
TigerBot-SFT 10.43 8.04 20.88 18.60 42.10 63.60 37.10 2.80 20.35 10.57 23.45
GoGPT 2.46 1.44 22.63 9.00 15.55 26.02 10.48 0.00 1.36 0.73 8.97
ChatGLM2-6B 1.84 1.65 13.20 22.40 30.33 36.81 19.50 4.00 11.60 2.81 14.41
Ziya-LLaMA-13B 8.13 2.06 44.18 24.60 27.93 43.44 2.00 11.80 31.82 3.06 19.90
Baichuan-7B 4.65 4.95 21.96 2.80 10.07 14.82 3.69 5.40 11.91 1.04 8.13
Baichuan-13B 8.56 6.60 35.65 3.60 6.44 26.76 7.92 13.40 12.19 2.44 12.35
Baichuan-13B-Chat 4.80 12.99 32.07 2.60 21.84 55.11 1.34 0.00 1.41 2.77 13.49
XVERSE-13B 0.28 0.00 15.76 1.80 3.47 2.20 16.67 0.00 13.64 0.85 5.47
InternLM-7B 6.88 4.74 18.31 2.00 5.20 25.31 6.71 0.40 12.27 0.60 8.24
InternLM-Chat-7B 21.07 33.40 27.09 35.60 55.62 12.73 25.87 27.60 60.21 20.43 31.96
InternLM-Chat-7B-8K 22.64 35.46 28.96 35.60 54.13 17.95 27.11 36.20 62.93 20.94 34.19
Qwen-7B 5.09 4.95 7.81 24.80 31.66 38.24 1.67 7.40 26.09 8.39 15.61
Qwen-7B-Chat 22.56 27.42 31.42 35.00 48.48 37.88 36.04 24.00 44.88 18.90 32.66
Yulan-Chat-2-13B 6.99 20.00 35.20 33.80 44.75 56.18 38.06 26.00 37.82 23.13 32.19
BELLE-LLaMA-2 5.78 9.07 38.84 38.00 41.56 54.89 36.59 20.80 34.73 16.83 29.71
Chinese-LLaMA-2-7B 1.58 4.95 1.33 1.60 1.87 20.24 9.86 0.60 9.17 0.36 5.16
Chinese-Alpaca-2-7B 4.11 3.71 31.12 20.20 0.55 51.91 2.92 0.00 31.62 5.74 15.19
LLaMA-2-Chinese-7B 0.12 1.24 0.24 0.20 6.91 2.33 2.00 0.00 3.76 1.28 1.81
LLaMA-2-Chinese-13B 0.24 1.86 0.48 0.00 5.18 2.20 1.05 0.00 1.97 0.78 1.38
Law Specific LLMs
LexiLaw 6.24 3.30 15.60 22.80 45.39 48.74 33.12 21.60 15.30 11.17 22.32
Fuzi-Mingcha 4.93 19.59 28.46 18.60 97.59 44.07 54.32 8.80 16.90 7.78 30.10
HanFei 19.40 6.39 30.44 30.20 38.50 59.23 40.07 23.40 14.73 6.05 26.84
LaWGPT-7B-beta1.0 3.41 0.62 0.00 0.00 0.01 2.20 0.19 0.00 0.13 0.14 0.67
LaWGPT-7B-beta1.1 6.22 4.95 6.85 2.40 2.27 2.00 8.61 1.00 14.94 1.06 5.03
Wisdom-Interrogatory 30.97 7.84 36.72 21.00 35.56 57.06 33.34 10.60 15.98 6.24 25.53
ChatLaw-13B 12.22 2.68 42.24 27.60 39.11 54.89 38.45 18.60 31.74 14.56 28.21
Lawyer-LLaMA 4.33 8.25 15.88 4.40 34.61 41.65 38.51 9.60 29.78 2.38 18.94
ChatLaw-33B 3.67 8.04 32.08 19.80 37.16 30.14 35.47 26.40 22.14 10.56 22.55
%abstention 0% 25% 50% 75% 100%
Table 25: Zero-shot Results on Legal Knowledge Understanding Tasks
32model_name 3-1 3-2 3-3 3-4 3-5 3-6 3-7 3-8 Application
Multilingual LLMs
MPT-7B 0.69 8.20 2.71 58.41 56.09 0.00 23.20 9.67 19.87
MPT-Instruct-7B 0.66 6.67 12.19 51.40 37.39 2.00 18.60 5.67 16.82
LLaMA-7B 0.95 7.45 6.98 41.28 54.22 0.20 14.40 7.84 16.67
LLaMA-13B 0.10 5.13 4.67 33.28 37.19 0.00 21.60 7.48 13.68
LLaMA-30B 0.80 8.97 4.77 47.19 48.62 0.60 28.20 7.87 18.38
LLaMA-65B 0.33 10.35 7.55 46.09 37.74 0.00 27.20 1.18 16.31
LLaMA-2-7B 0.99 7.95 7.59 52.78 43.05 0.00 17.60 9.38 17.42
LLaMA-2-13B 0.90 12.00 5.43 52.62 32.74 0.60 11.20 1.63 14.64
LLaMA-2-70B 3.70 11.45 6.36 42.02 42.82 0.00 29.20 2.90 17.31
LLaMA-2-Chat-7B 1.87 0.64 11.57 10.13 7.21 5.60 36.20 0.36 9.20
LLaMA-2-Chat-13B 1.40 0.49 15.29 50.57 52.64 4.60 28.60 0.40 19.25
LLaMA-2-Chat-70B 6.82 0.89 14.91 17.54 12.81 9.60 42.40 4.61 13.70
Alpaca-v1.0-7B 0.69 13.11 6.09 59.61 50.93 2.00 18.60 1.56 19.07
Vicuna-v1.3-7B 0.16 19.21 2.20 33.24 23.95 10.60 11.00 13.13 14.19
Vicuna-v1.3-13B 0.15 19.28 3.57 27.01 16.69 18.80 17.80 14.34 14.70
Vicuna-v1.3-33B 0.44 23.73 4.56 10.75 10.95 23.80 9.00 11.47 11.84
WizardLM-7B 0.60 14.31 8.16 58.08 48.80 11.00 18.40 5.86 20.65
StableBeluga2 16.41 24.52 22.82 76.06 65.35 34.40 56.60 13.39 38.69
ChatGPT 29.50 31.30 35.52 78.75 76.84 27.40 61.20 17.45 44.74
GPT-4 52.47 27.54 41.99 82.62 81.91 48.60 77.60 19.65 54.05
Chinese Oriented LLMs
MOSS-Moon 23.73 7.68 13.11 69.31 53.45 19.60 9.80 15.57 26.53
MOSS-Moon-SFT 5.23 5.54 20.80 70.27 76.55 6.80 26.80 10.41 27.80
TigerBot-Base 16.61 27.98 24.81 73.24 65.64 1.00 12.00 14.58 29.48
TigerBot-SFT 3.54 25.58 21.97 76.57 71.63 12.60 30.40 16.19 32.31
GoGPT 0.00 11.61 5.91 10.71 2.10 14.20 10.60 9.47 8.07
ChatGLM2-6B 23.12 24.96 27.12 59.79 72.84 10.80 16.80 17.56 31.62
Ziya-LLaMA-13B 10.20 21.23 25.93 0.97 0.57 30.60 12.80 13.36 14.46
Baichuan-7B 10.05 8.36 22.56 75.71 76.42 5.40 17.80 2.40 27.34
Baichuan-13B 27.48 9.51 29.61 72.53 67.64 14.40 19.20 4.60 30.62
Baichuan-13B-Chat 22.79 27.23 24.21 35.61 46.11 27.00 43.00 20.29 30.78
XVERSE-13B 1.31 8.68 15.76 75.68 66.66 0.00 37.60 8.85 26.82
InternLM-7B 23.05 6.72 32.43 73.38 56.71 10.40 22.80 11.30 29.60
InternLM-Chat-7B 24.94 20.80 42.35 67.33 64.97 37.80 44.80 15.36 39.79
InternLM-Chat-7B-8K 34.86 19.11 41.05 63.21 67.20 34.20 43.80 13.37 39.60
Qwen-7B 19.88 20.80 20.16 74.59 56.77 0.40 32.80 10.07 29.43
Qwen-7B-Chat 44.62 33.50 40.67 76.74 77.19 26.80 42.00 19.32 45.10
Yulan-Chat-2-13B 7.64 30.67 38.30 76.66 75.78 23.20 46.00 17.54 39.47
BELLE-LLaMA-2 3.27 31.79 19.65 68.21 61.17 22.20 42.80 17.04 33.26
Chinese-LLaMA-2-7B 0.14 4.43 18.13 38.80 33.49 0.40 16.80 7.63 14.98
Chinese-Alpaca-2-7B 0.05 35.51 26.35 0.55 12.10 7.00 8.60 14.07 13.03
LLaMA-2-Chinese-7B 0.00 5.04 1.29 7.09 23.82 0.00 2.60 1.22 5.13
LLaMA-2-Chinese-13B 0.00 0.15 1.27 29.39 28.23 0.00 11.00 2.37 9.05
Law Specific LLMs
LexiLaw 13.15 35.78 39.99 78.08 74.92 20.80 35.80 15.82 39.29
Fuzi-Mingcha 25.19 22.18 55.93 77.23 75.52 7.00 47.20 16.64 40.86
HanFei 2.64 33.60 30.96 73.11 69.63 23.60 39.40 16.06 36.13
LaWGPT-7B-beta1.0 0.00 8.16 3.23 0.34 1.72 0.00 0.00 0.80 1.78
LaWGPT-7B-beta1.1 0.15 11.01 15.68 42.37 40.80 6.20 15.40 7.62 17.40
Wisdom-Interrogatory 32.84 32.01 35.09 80.36 81.10 15.40 17.40 20.17 39.29
ChatLaw-13B 33.28 31.55 27.90 76.18 73.57 28.80 41.40 17.17 41.23
Lawyer-LLaMA 0.60 25.94 31.30 74.19 75.52 17.80 39.20 16.94 35.19
ChatLaw-33B 5.35 26.02 12.73 67.00 53.63 34.20 41.60 16.55 32.14
%abstention 0% 25% 50% 75% 100%
Table 26: Zero-shot Results on Legal Knowledge Application Tasks
33model_name Memorization Understanding Application Overall
Multilingual LLMs
MPT-7B 1.78 6.16 19.87 11.21
MPT-Instruct-7B 1.65 6.87 16.82 10.33
LLaMA-7B 1.51 5.80 16.67 9.72
LLaMA-13B 1.72 6.20 13.68 8.74
LLaMA-30B 1.80 3.46 18.38 9.26
LLaMA-65B 1.20 3.60 16.31 8.44
LLaMA-2-7B 1.52 9.15 17.42 11.69
LLaMA-2-13B 1.85 7.48 14.64 9.78
LLaMA-2-70B 1.77 12.91 17.31 13.56
LLaMA-2-Chat-7B 3.60 6.24 9.20 7.16
LLaMA-2-Chat-13B 2.12 13.46 19.25 14.64
LLaMA-2-Chat-70B 5.43 17.37 13.70 14.71
Alpaca-v1.0-7B 0.80 4.63 19.07 10.02
Vicuna-v1.3-7B 7.60 10.16 14.19 11.52
Vicuna-v1.3-13B 12.91 10.44 14.70 12.39
Vicuna-v1.3-33B 17.43 10.19 11.84 11.57
WizardLM-7B 7.14 9.64 20.65 13.79
StableBeluga2 24.59 42.58 38.69 39.23
ChatGPT 25.93 43.31 44.74 42.15
GPT-4 35.29 54.41 54.05 52.35
Chinese Oriented LLMs
MOSS-Moon 8.32 6.80 26.53 14.84
MOSS-Moon-SFT 5.25 7.15 27.80 15.22
TigerBot-Base 9.71 17.88 29.48 21.71
TigerBot-SFT 12.39 23.45 32.31 25.89
GoGPT 12.05 8.97 8.07 8.92
ChatGLM2-6B 12.98 14.41 31.62 21.15
Ziya-LLaMA-13B 22.52 19.90 14.46 17.99
Baichuan-7B 0.92 8.13 27.34 15.09
Baichuan-13B 6.35 12.35 30.62 19.06
Baichuan-13B-Chat 23.56 13.49 30.78 21.41
XVERSE-13B 0.85 5.47 26.82 13.55
InternLM-7B 3.08 8.24 29.60 16.27
InternLM-Chat-7B 27.20 31.96 39.79 34.62
InternLM-Chat-7B-8K 27.93 34.19 39.60 35.73
Qwen-7B 0.00 15.61 29.43 19.58
Qwen-7B-Chat 26.27 32.66 45.10 37.00
Yulan-Chat-2-13B 18.72 32.19 39.47 33.76
BELLE-LLaMA-2 22.50 29.71 33.26 30.41
Chinese-LLaMA-2-7B 2.22 5.16 14.98 8.79
Chinese-Alpaca-2-7B 9.67 15.19 13.03 13.77
LLaMA-2-Chinese-7B 0.00 1.81 5.13 2.96
LLaMA-2-Chinese-13B 0.00 1.38 9.05 4.31
Law Specific LLMs
LexiLaw 18.98 22.32 39.29 28.78
Fuzi-Mingcha 16.51 30.10 40.86 33.05
HanFei 18.42 26.84 36.13 29.71
LaWGPT-7B-beta1.0 4.67 0.67 1.78 1.51
LaWGPT-7B-beta1.1 4.37 5.03 17.40 9.91
Wisdom-Interrogatory 29.23 25.53 39.29 31.41
ChatLaw-13B 21.63 28.21 41.23 32.76
Lawyer-LLaMA 17.77 18.94 35.19 25.32
ChatLaw-33B 20.17 22.55 32.14 26.14
%abstention 0% 25% 50% 75% 100%
Table 27: Zero-shot Results on Overall
34model_name 1-1 1-2 Memorization
Multilingual LLMs
MPT-7B 3.26 0.40 1.83
MPT-Instruct-7B 2.06 2.00 2.03
LLaMA-7B 3.14 1.20 2.17
LLaMA-13B 3.00 2.60 2.80
LLaMA-30B 3.15 1.00 2.07
LLaMA-65B 2.00 1.20 1.60
LLaMA-2-7B 3.42 1.80 2.61
LLaMA-2-13B 3.58 1.20 2.39
LLaMA-2-70B 3.33 0.40 1.86
LLaMA-2-Chat-7B 0.82 7.40 4.11
LLaMA-2-Chat-13B 1.62 3.80 2.71
LLaMA-2-Chat-70B 0.73 11.00 5.86
Alpaca-v1.0-7B 9.18 4.60 6.89
Vicuna-v1.3-7B 11.99 1.60 6.80
Vicuna-v1.3-13B 6.01 14.40 10.21
Vicuna-v1.3-33B 7.67 22.80 15.24
WizardLM-7B 10.81 8.60 9.71
StableBeluga2 15.03 36.00 25.51
ChatGPT 16.15 37.20 26.67
GPT-4 17.21 54.80 36.00
Chinese Oriented LLMs
MOSS-Moon 3.09 0.40 1.75
MOSS-Moon-SFT 2.07 1.40 1.74
TigerBot-Base 14.09 0.20 7.14
TigerBot-SFT 11.09 6.20 8.64
GoGPT 8.21 20.40 14.30
ChatGLM2-6B 14.67 24.80 19.74
Ziya-LLaMA-13B 0.00 28.00 14.00
Baichuan-7B 3.88 0.40 2.14
Baichuan-13B 4.05 1.00 2.53
Baichuan-13B-Chat 9.78 40.40 25.09
XVERSE-13B 2.43 0.00 1.22
InternLM-7B 2.79 1.00 1.89
InternLM-Chat-7B 13.85 39.60 26.73
InternLM-Chat-7B-8K 15.16 40.60 27.88
Qwen-7B 0.00 0.00 0.00
Qwen-7B-Chat 17.73 28.60 23.16
Yulan-Chat-2-13B 15.41 21.40 18.40
BELLE-LLaMA-2 16.58 21.40 18.99
Chinese-LLaMA-2-7B 2.77 1.20 1.98
Chinese-Alpaca-2-7B 11.84 14.00 12.92
LLaMA-2-Chinese-7B 0.00 0.40 0.20
LLaMA-2-Chinese-13B 0.00 3.20 1.60
Law Specific LLMs
LexiLaw 15.47 14.40 14.94
Fuzi-Mingcha 20.21 12.80 16.50
HanFei 16.23 8.60 12.41
LaWGPT-7B-beta1.0 1.24 0.00 0.62
LaWGPT-7B-beta1.1 3.03 6.60 4.82
Wisdom-Interrogatory 37.41 15.20 26.30
ChatLaw-13B 15.98 29.40 22.69
Lawyer-LLaMA 13.04 10.60 11.82
ChatLaw-33B 14.36 27.80 21.08
%abstention 0% 25% 50% 75% 100%
Table 28: One-shot Results on Legal Knowledge Memorization Tasks
35model_name 2-1 2-2 2-3 2-4 2-5 2-6 2-7 2-8 2-9 2-10 Understanding
Multilingual LLMs
MPT-7B 2.35 4.00 22.37 6.80 4.39 34.91 16.09 1.40 12.67 1.53 10.65
MPT-Instruct-7B 2.36 4.20 16.38 8.00 10.63 36.96 13.11 9.00 21.69 3.21 12.55
LLaMA-7B 1.16 5.00 25.45 7.40 21.93 16.74 21.70 1.60 14.43 0.57 11.60
LLaMA-13B 0.68 6.00 22.49 2.40 20.51 30.85 19.69 5.00 18.06 1.39 12.71
LLaMA-30B 0.57 9.60 26.74 3.20 23.41 8.83 10.31 1.20 27.57 2.85 11.43
LLaMA-65B 3.94 9.20 23.88 2.20 25.26 20.32 13.66 4.60 33.72 2.17 13.89
LLaMA-2-7B 1.42 6.80 19.23 6.20 23.53 34.55 30.96 4.60 25.29 1.03 15.36
LLaMA-2-13B 3.73 8.60 31.54 2.80 24.88 25.83 20.55 1.40 31.02 3.08 15.34
LLaMA-2-70B 9.93 11.00 34.15 3.80 27.27 31.71 4.66 6.20 34.30 3.84 16.69
LLaMA-2-Chat-7B 0.13 3.40 16.78 5.40 0.64 7.87 1.60 4.20 16.56 2.29 5.89
LLaMA-2-Chat-13B 0.23 6.20 31.00 17.80 0.97 3.16 1.03 5.20 26.85 2.83 9.53
LLaMA-2-Chat-70B 0.00 17.40 38.05 28.40 1.16 3.13 1.53 6.40 34.88 8.39 13.93
Alpaca-v1.0-7B 1.11 3.60 19.74 5.60 10.31 9.23 10.53 13.60 17.09 3.16 9.40
Vicuna-v1.3-7B 1.45 6.60 23.36 10.80 9.92 34.49 17.25 13.20 24.98 4.10 14.61
Vicuna-v1.3-13B 2.03 5.00 26.17 8.40 20.33 34.89 10.40 16.60 19.97 7.20 15.10
Vicuna-v1.3-33B 2.02 5.40 23.44 16.80 24.64 36.40 24.86 16.20 21.97 9.98 18.17
WizardLM-7B 0.82 2.40 17.24 1.00 13.29 27.84 7.14 12.80 14.49 2.63 9.97
StableBeluga2 8.93 15.00 41.76 38.00 53.55 64.99 45.06 37.60 65.89 40.54 41.13
ChatGPT 13.50 40.60 54.01 41.40 61.98 74.04 40.68 37.40 67.59 40.04 47.12
GPT-4 18.31 46.00 69.99 44.40 64.80 79.96 40.52 59.00 76.55 65.26 56.48
Chinese Oriented LLMs
MOSS-Moon 2.83 3.00 17.44 9.40 2.62 26.34 5.87 1.20 25.87 1.27 9.58
MOSS-Moon-SFT 5.10 3.80 21.16 3.40 2.28 17.56 6.80 1.00 24.58 2.47 8.82
TigerBot-Base 12.32 4.40 16.94 20.40 36.47 10.06 22.05 1.60 41.35 9.94 17.55
TigerBot-SFT 15.31 2.60 17.42 22.40 47.29 62.78 26.89 0.40 33.91 10.42 23.94
GoGPT 1.98 1.40 15.17 6.80 35.32 38.20 19.06 16.40 34.53 7.76 17.66
ChatGLM2-6B 3.17 10.80 10.31 4.80 38.38 36.61 15.59 12.80 26.72 5.64 16.48
Ziya-LLaMA-13B 10.27 15.40 42.75 25.20 7.64 56.84 2.75 17.00 31.13 2.65 21.16
Baichuan-7B 13.21 9.60 32.67 13.80 3.46 17.79 2.96 2.20 24.73 1.29 12.17
Baichuan-13B 14.76 11.80 43.01 7.40 3.38 12.36 9.02 1.60 40.09 2.06 14.55
Baichuan-13B-Chat 2.24 10.80 35.26 0.00 61.71 52.76 2.47 1.20 16.76 1.57 18.48
XVERSE-13B 0.09 0.80 15.79 2.00 3.69 1.71 16.39 0.00 20.09 0.41 6.10
InternLM-7B 10.73 6.40 26.72 8.40 6.04 35.56 14.56 2.80 35.04 1.58 14.78
InternLM-Chat-7B 20.35 34.20 29.72 32.80 54.44 21.93 28.76 26.20 62.68 21.14 33.22
InternLM-Chat-7B-8K 21.64 36.60 30.91 33.20 54.35 26.86 30.56 30.60 63.42 20.69 34.88
Qwen-7B 8.12 8.80 23.17 21.60 6.69 42.51 1.72 1.40 34.87 5.66 15.45
Qwen-7B-Chat 25.16 27.40 32.96 31.20 46.71 57.34 42.58 26.80 50.63 21.27 36.21
Yulan-Chat-2-13B 6.91 30.20 34.00 36.00 34.40 58.65 40.80 28.60 49.37 22.20 34.11
BELLE-LLaMA-2 5.99 13.00 41.76 30.40 43.30 65.60 41.03 9.40 46.64 20.16 31.73
Chinese-LLaMA-2-7B 7.79 3.60 7.71 6.20 4.45 18.48 14.04 8.20 21.84 0.64 9.30
Chinese-Alpaca-2-7B 7.39 12.60 29.65 20.60 37.70 32.20 5.47 16.60 37.14 4.32 20.37
LLaMA-2-Chinese-7B 0.12 5.20 11.27 1.40 11.70 31.12 2.22 7.00 22.95 4.79 9.78
LLaMA-2-Chinese-13B 0.24 10.00 24.18 0.00 19.79 43.47 1.48 10.80 30.91 12.17 15.30
Law Specific LLMs
LexiLaw 4.18 15.40 21.49 27.20 41.64 31.54 34.57 6.00 19.84 8.36 21.02
Fuzi-Mingcha 2.86 2.40 17.44 8.80 93.35 42.28 31.43 11.40 21.26 7.04 23.83
HanFei 14.98 9.20 32.71 12.60 36.64 52.33 31.15 17.00 25.38 4.09 23.61
LaWGPT-7B-beta1.0 2.08 4.00 0.00 0.40 0.79 3.19 0.00 0.00 0.70 0.43 1.16
LaWGPT-7B-beta1.1 4.70 3.40 4.61 10.20 2.26 8.11 4.71 0.60 16.66 0.25 5.55
Wisdom-Interrogatory 22.16 10.00 24.29 13.40 13.23 40.10 39.71 0.20 15.81 4.02 18.29
ChatLaw-13B 13.01 9.00 30.91 26.60 41.41 60.68 42.71 20.20 40.27 17.37 30.22
Lawyer-LLaMA 4.90 19.20 9.03 3.00 39.65 36.33 37.10 0.40 33.19 6.12 18.89
ChatLaw-33B 4.30 12.20 33.49 4.20 38.87 28.83 34.20 15.40 26.18 15.95 21.36
%abstention 0% 25% 50% 75% 100%
Table 29: One-shot Results on Legal Knowledge Understanding Tasks
36model_name 3-1 3-2 3-3 3-4 3-5 3-6 3-7 3-8 Application
Multilingual LLMs
MPT-7B 0.40 9.82 3.86 62.23 55.84 2.00 32.80 9.41 22.05
MPT-Instruct-7B 0.18 7.88 5.75 13.89 37.38 20.60 29.40 7.88 15.37
LLaMA-7B 0.38 10.16 10.45 58.73 50.47 6.80 27.40 10.51 21.86
LLaMA-13B 1.01 10.00 12.42 72.42 53.56 6.40 33.60 10.11 24.94
LLaMA-30B 4.06 10.01 12.80 65.83 50.72 7.40 49.60 11.52 26.49
LLaMA-65B 6.00 9.94 12.51 71.80 60.94 6.80 32.80 10.55 26.42
LLaMA-2-7B 1.29 10.42 12.54 73.09 57.36 8.80 32.00 10.98 25.81
LLaMA-2-13B 3.60 11.01 15.07 68.34 55.05 6.60 37.40 12.19 26.16
LLaMA-2-70B 19.50 11.86 18.50 66.05 60.36 2.80 53.00 13.38 30.68
LLaMA-2-Chat-7B 1.00 1.22 6.55 17.00 8.16 5.40 29.80 0.32 8.68
LLaMA-2-Chat-13B 0.53 13.32 16.53 33.50 38.01 2.40 35.80 1.31 17.68
LLaMA-2-Chat-70B 6.39 1.41 15.87 13.17 6.22 13.00 41.60 4.57 12.78
Alpaca-v1.0-7B 0.88 27.21 8.05 64.85 49.44 17.40 26.60 10.25 25.58
Vicuna-v1.3-7B 0.24 16.95 8.78 13.21 19.31 2.60 38.40 13.71 14.15
Vicuna-v1.3-13B 1.04 14.39 10.83 40.93 21.78 12.40 29.20 14.25 18.10
Vicuna-v1.3-33B 2.97 20.16 12.24 47.77 17.31 28.40 30.00 11.17 21.25
WizardLM-7B 0.67 28.10 5.30 37.53 37.61 16.20 30.80 14.13 21.29
StableBeluga2 16.87 32.44 23.07 75.80 63.59 33.00 56.00 16.24 39.63
ChatGPT 30.81 34.49 34.55 77.12 73.72 31.60 66.40 17.17 45.73
GPT-4 53.20 33.15 41.30 83.21 82.74 49.60 77.00 19.90 55.01
Chinese Oriented LLMs
MOSS-Moon 18.24 5.14 7.59 77.55 63.36 2.80 29.20 10.32 26.77
MOSS-Moon-SFT 18.03 4.81 11.31 70.15 75.66 2.00 29.20 9.31 27.56
TigerBot-Base 14.83 29.46 21.81 72.47 36.49 1.00 23.20 17.65 27.11
TigerBot-SFT 4.93 28.49 15.77 78.04 72.36 2.60 37.80 15.77 31.97
GoGPT 0.00 22.63 16.39 15.12 4.13 20.20 30.60 10.37 14.93
ChatGLM2-6B 14.34 34.23 32.09 38.25 61.35 16.60 42.80 14.89 31.82
Ziya-LLaMA-13B 5.83 33.63 25.98 4.51 0.55 26.00 24.20 15.53 17.03
Baichuan-7B 40.12 6.03 39.94 79.02 78.66 7.80 34.80 11.27 37.21
Baichuan-13B 55.94 6.24 43.76 80.07 77.24 14.40 42.20 10.36 41.28
Baichuan-13B-Chat 46.49 33.76 50.20 64.70 53.17 30.40 59.40 18.71 44.60
XVERSE-13B 1.24 6.93 15.52 75.53 65.00 0.00 36.60 9.88 26.34
InternLM-7B 42.50 5.92 41.43 76.70 77.10 5.60 38.00 10.44 37.21
InternLM-Chat-7B 30.88 29.10 40.91 75.57 70.40 36.00 39.00 14.73 42.07
InternLM-Chat-7B-8K 38.88 28.70 42.25 67.74 71.10 36.20 44.00 12.11 42.62
Qwen-7B 3.35 5.15 30.63 77.89 51.60 0.60 44.60 4.14 27.24
Qwen-7B-Chat 52.86 34.49 39.91 78.47 73.92 26.80 44.60 20.39 46.43
Yulan-Chat-2-13B 5.15 33.02 38.94 75.86 74.05 18.40 49.40 17.35 39.02
BELLE-LLaMA-2 2.96 33.37 19.67 63.34 57.15 26.00 42.20 15.77 32.56
Chinese-LLaMA-2-7B 0.19 5.97 27.53 25.47 43.48 7.80 32.40 11.23 19.26
Chinese-Alpaca-2-7B 2.37 27.06 32.46 5.66 15.53 22.40 35.20 14.22 19.36
LLaMA-2-Chinese-7B 0.00 30.99 8.19 16.42 24.68 1.80 23.20 9.87 14.39
LLaMA-2-Chinese-13B 0.40 19.47 9.70 62.90 25.54 22.60 33.60 11.14 23.17
Law Specific LLMs
LexiLaw 15.41 33.94 34.03 73.66 70.93 12.20 33.20 14.68 36.01
Fuzi-Mingcha 3.86 32.96 43.60 78.95 79.00 13.80 38.20 13.95 38.04
HanFei 7.98 34.27 9.80 74.34 72.19 10.40 13.40 14.99 29.67
LaWGPT-7B-beta1.0 0.00 23.75 0.80 0.01 0.80 0.00 8.00 0.46 4.23
LaWGPT-7B-beta1.1 0.49 12.54 13.80 40.18 47.30 17.60 26.00 8.77 20.84
Wisdom-Interrogatory 20.02 23.33 39.22 81.16 81.57 13.00 40.40 20.67 39.92
ChatLaw-13B 25.99 33.96 12.24 74.31 73.01 26.80 42.00 16.72 38.13
Lawyer-LLaMA 0.33 27.23 19.36 70.99 73.56 6.60 33.80 16.02 30.99
ChatLaw-33B 4.89 27.97 17.54 63.30 53.03 26.20 43.20 16.29 31.55
%abstention 0% 25% 50% 75% 100%
Table 30: One-shot Results on Legal Knowledge Application Tasks
37model_name Memorization Understanding Application Overall
Multilingual LLMs
MPT-7B 1.83 10.65 22.05 14.33
MPT-Instruct-7B 2.03 12.55 15.37 12.63
LLaMA-7B 2.17 11.60 21.86 14.76
LLaMA-13B 2.80 12.71 24.94 16.61
LLaMA-30B 2.07 11.43 26.49 16.52
LLaMA-65B 1.60 13.89 26.42 17.67
LLaMA-2-7B 2.61 15.36 25.81 18.26
LLaMA-2-13B 2.39 15.34 26.16 18.37
LLaMA-2-70B 1.86 16.69 30.68 20.80
LLaMA-2-Chat-7B 4.11 5.89 8.68 6.83
LLaMA-2-Chat-13B 2.71 9.53 17.68 12.11
LLaMA-2-Chat-70B 5.86 13.93 12.78 12.67
Alpaca-v1.0-7B 6.89 9.40 25.58 15.62
Vicuna-v1.3-7B 6.80 14.61 14.15 13.65
Vicuna-v1.3-13B 10.21 15.10 18.10 15.81
Vicuna-v1.3-33B 15.24 18.17 21.25 19.11
WizardLM-7B 9.71 9.97 21.29 14.47
StableBeluga2 25.51 41.13 39.63 38.97
ChatGPT 26.67 47.12 45.73 44.52
GPT-4 36.00 56.48 55.01 53.85
Chinese Oriented LLMs
MOSS-Moon 1.75 9.58 26.77 15.68
MOSS-Moon-SFT 1.74 8.82 27.56 15.60
TigerBot-Base 7.14 17.55 27.11 20.34
TigerBot-SFT 8.64 23.94 31.97 25.62
GoGPT 14.30 17.66 14.93 16.23
ChatGLM2-6B 19.74 16.48 31.82 22.94
Ziya-LLaMA-13B 14.00 21.16 17.03 18.79
Baichuan-7B 2.14 12.17 37.21 21.18
Baichuan-13B 2.53 14.55 41.28 24.04
Baichuan-13B-Chat 25.09 18.48 44.60 29.59
XVERSE-13B 1.22 6.10 26.34 13.71
InternLM-7B 1.89 14.78 37.21 22.47
InternLM-Chat-7B 26.73 33.22 42.07 36.11
InternLM-Chat-7B-8K 27.88 34.88 42.62 37.28
Qwen-7B 0.00 15.45 27.24 18.62
Qwen-7B-Chat 23.16 36.21 46.43 38.99
Yulan-Chat-2-13B 18.40 34.11 39.02 34.51
BELLE-LLaMA-2 18.99 31.73 32.56 30.79
Chinese-LLaMA-2-7B 1.98 9.30 19.26 12.55
Chinese-Alpaca-2-7B 12.92 20.37 19.36 19.22
LLaMA-2-Chinese-7B 0.20 9.78 14.39 10.67
LLaMA-2-Chinese-13B 1.60 15.30 23.17 17.08
Law Specific LLMs
LexiLaw 14.94 21.02 36.01 26.41
Fuzi-Mingcha 16.50 23.83 38.04 28.78
HanFei 12.41 23.61 29.67 24.91
LaWGPT-7B-beta1.0 0.62 1.16 4.23 2.33
LaWGPT-7B-beta1.1 4.82 5.55 20.84 11.59
Wisdom-Interrogatory 26.30 18.29 39.92 27.74
ChatLaw-13B 22.69 30.22 38.13 32.63
Lawyer-LLaMA 11.82 18.89 30.99 23.02
ChatLaw-33B 21.08 21.36 31.55 25.41
%abstention 0% 25% 50% 75% 100%
Table 31: One-shot Results on Overall
38