Do Text-to-Vis Benchmarks Test Real Use of Visualisations?
Hy Nguyen1∗, Xuefei He1, Andrew Reeson2, Cécile Paris2, Josiah Poon1,
and Jonathan K. Kummerfeld1
The University of Sydney1CSIRO’s Data612
nngu0448@uni.sydney.edu.au∗
Abstract
Large language models are able to generate
code for visualisations in response to simple
user requests. This is a useful application and
an appealing one for NLP research because
plots of data provide grounding for language.
However, there are relatively few benchmarks,
and those that exist may not be representa-
tive of what users do in practice. This paper
investigates whether benchmarks reflect real-
world use through an empirical study compar-
ing benchmark datasets with code from public
repositories. Our findings reveal a substantial
gap, with evaluations not testing the same dis-
tribution of chart types, attributes, and actions
as real-world examples. One dataset is repre-
sentative, but requires extensive modification
to become a practical end-to-end benchmark.
This shows that new benchmarks are needed to
support the development of systems that truly
address users’ visualisation needs. These ob-
servations will guide future data creation, high-
lighting which features hold genuine signifi-
cance for users.
1 Introduction
Text-to-Vis is the task of receiving data and a re-
quest for a visualisation expressed in human lan-
guage and generating code that will produce the
visualisation. A system with this ability would en-
able faster and more complex data analysis, but
there are relatively few benchmark datasets for the
task. Those that do exist either focus on generat-
ing a single response (Luo et al., 2021; Srinivasan
et al., 2021; Chen et al., 2021), or consider dia-
logue, but with limited flexibility in code (Shao
and Nakashole, 2020; Song et al., 2024). Most of
these datasets use generated data. The space of
code variation was defined by researchers. This
raises the question of whether these datasets are
representative of real-world use of data visualisa-
tions.In this study, we gathered publicly available
code from the Stack1to analyse human prefer-
ences when making visualisations using libraries
across four programming languages: Python, R,
Javascript, and Vega. Since each library has dif-
ferent names for the same visualisation types and
properties, we extracted key visualisation code and
developed a cross-language mapping for several
hundred functions and arguments.2
Using this aligned data, we analysed user be-
haviours when making visualisations and identified
similarities and differences between real-world and
benchmark datasets. Our analysis considered the
chart types, functions called to define properties,
and the arguments that modify how those functions
behave. We observed that (1) existing benchmarks
tend to focus on only one aspect of the Text-to-Vis
challenge, either code synthesis, data presentation,
or aesthetic attribute adjustment, and (2) only one
of the datasets is consistent with real-world use and
that dataset is limited by its lack of executability of
code outputs.
Success on existing benchmarks does not mean
systems are useful for real-world use. We need new
benchmarks that cover all aspects of the problem
and are consistent with patterns of use. Only then
will we be able to measure progress on this valuable
and challenging task.
2 Related Work
A common approach to creating Text-to-Vis
datasets involves automatic synthesis of visuali-
sations followed by human intervention for annota-
tion (Luo et al., 2021; Shao and Nakashole, 2020;
Srinivasan et al., 2021; Song et al., 2024; Chen
et al., 2021). Although this approach is straightfor-
1A 6TB collection of open source code from GitHub (Ko-
cetkov et al., 2022)
2For example, a bar plot is produced with bar() and
barh() in Python, but barplot() in R. Our data and code are
at https://github.com/giahy2507/text-to-vis-bench-assessment.arXiv:2407.19726v4  [cs.CL]  8 Oct 2024ward, datasets produced using this method often
contain inherent problems. For instance, nvBench,
the largest benchmark dataset for this task, was
synthesised from Spider (Yu et al., 2018), a text-to-
SQL dataset containing several limitations (Suhr
et al., 2020), and was only partially reviewed by
novices and experts for quality assurance, result-
ing in numerous issues (Li et al., 2024). Similarly,
ChartDialogs contains limitations as the data for
visualisation were automatically generated. These
shortcomings underscore the need for improved
methodologies in dataset creation to ensure validity
and usability.
Recent research in AI has prioritised ecologi-
cal validity to enhance benchmark dataset quality
across various domains, aiming to align them with
real-world applications (De Vries et al., 2020; Qi
et al., 2023; Lu et al., 2023). Ensuring that the data
used to train and test models accurately reflects
users’ objectives in practical scenarios is crucial.
However, prior research on Text-to-Vis has not con-
sidered this aspect of dataset creation.
3 Data Collection
Instead of examining user preferences through
chart images or relying on experts to comprehend
how visualisations are made, our approach involves
the analysis of publicly available programs specifi-
cally written for creating visualisations, e.g., line,
bar, and scatter charts. This means we consider a
wide range of samples from different programmers
and their preferences when making visualisations.
As a result, our analysis can provide a broad un-
derstanding of the essential components that are
widely used.
We used code from The Stack3to conduct
our investigation. We consider four diverse and
widely used visualisation libraries: Matplotlib4,
Graphics5,ChartJS6, and Vega-Lite7, which are
for Python, R, Javascript, and JSON, respectively.
After downloading, we selected files containing
code indicative of visualisation library usage (e.g.,
import matplotlib.pyplot as plt ). Finally,
we used abstract syntax tree (AST) parsers and
heuristics to accurately extract library-related vari-
ables, function names, arguments, and explicit val-
ues. The details are described in Appendix A, while
3https://huggingface.co/datasets/bigcode/the-stack-dedup
4https://matplotlib.org/
5https://www.rdocumentation.org/packages/graphics
6https://www.chartjs.org/docs/latest/
7https://vega.github.io/vega-lite/# samples Proportion # functions
nb-Matplotlib 385,338 35.89% 6,443,220
py-Matplotlib 464,463 3.5% 4,484,368
Graphics 6,721 17.15% 53,325
ChartJS 2,714 0.0128% 8,847
Vega-Lite 1,093 0.0013% 15,664
nvBench 7,241 40,478
ChartDialogs 3,284 14,690
PlotCoder 97,706 254,251
Table 1: Statistics of real-world and benchmark data.
“Proportion” indicates the proportion of the library’s
code in the investigated programming languages. “nb-
Matplotlib” indicates code from Jupyter notebooks,
while “py-Matplotlib” indicates code from Python files.
Benchmark Input Output Annotation note
nvBench prompt,
datacode,
visAuto-generated
based on a text-to-
SQL benchmark
ChartDialogs prompt,
data,
viscode,
visManually annotated
PlotCoder prompt,
codecode Auto-extracted
Table 2: Description of benchmark datasets.
Table 1 (upper) presents their statistics.
We examined three publicly available bench-
mark datasets: nvBench (Luo et al., 2021), Chart-
Dialogs (Shao and Nakashole, 2020), and Plot-
Coder (Chen et al., 2021). They vary in settings
and scales, as described in Table 1 and 2. While
nvBench and ChartDialogs are end-to-end Text-
to-Vis benchmarks, with queries & data as input
and code & visualisations as output, PlotCoder is
purely a code synthesis dataset, with no data or
visualisations. Appendix B shows examples from
each dataset.
4 Cross-language Mapping Table
To compare the data described in the previous sec-
tion, we constructed a cross-language mapping ta-
ble based on frequently used parameters. This in-
volved selecting the top 500 frequently used pa-
rameters, identifying categories and attributes, and
checking correctness based on the libraries’ doc-
umentation and code execution. Ultimately, the
mapping table comprises 8 categories, 62 attributes,
and around 850 parameters across the 4 visualisa-
tion languages. Figure 1 shows an example for
the attribute “x-axis title”. Details can be found in
Appendix C.For a given parameter “A | B”, A indicates the function name while B indicates the keyword argument.A called function plot(xlab=“X”) containing a parameter “plot|xlab”
Figure 1: Example of the cross-language mapping
5 Analysis & Discussion
5.1 Comparison of chart types
Figure 2 (upper) depicts the distribution of four
common plot types across real-world datasets and
nvBench8. Each dataset shows distinct preferences
for specific plot types. The distribution of nvBench,
a benchmark based on the Vega-Lite grammar, is
significantly misaligned with that of Vega-Lite,
where the bar chart dominates other types, account-
ing for over 80%, while the remaining are around
7%.
Figure 2 (lower) depicts the distribution of seven
plot types across four Python-based datasets. Gen-
erally, the distribution between Matplotlib and Plot-
Coder shows notable similarity. This trend is be-
cause both are derived from GitHub. In contrast,
ChartDialogs contains a more uniform distribution
of plot types. This is the result of its design, and
differs from what we observe in the wild. Specifi-
cally, ChartDialogs has fewer scatter plots and an
overabundance of pie charts, contours, and stream
plots.
These findings imply that nvBench and ChartDi-
alogs are not testing the same distribution of plot
types as real-world data. As suggestions for future
dataset makers, it is crucial to tailor the distribution
of chart types according to the specific needs and
domains of the intended users. At the same time,
given the imbalanced distributions in real-world
data, it is also valuable to conduct separate eval-
uations focusing specifically on rarer plot types,
acknowledging their distinct value.
5.2 Comparison of attributes
Using the cross-language mapping table and parsed
data (function names and arguments), we computed
8We categorise histograms as bar charts, while polar pie
and doughnut charts are grouped as pie charts
Figure 2: Plot type distribution over eight datasets.
Figure 3: Spearman’s rank correlation coefficient in
terms of frequent attributes
the normalised frequency for 62 attributes within
each dataset, as shown in Figure 8 located in Ap-
pendix D. We used these frequencies to determine
the Spearman’s rank correlation coefficient across
eight datasets, as illustrated in Figure 3.
The real-world datasets have a significant cor-
relation, with Spearman’s values surpassing 0.7,
except for ChartJS, which displays a moderate
correlation with coefficients around 0.5. As for
the benchmarks, ChartDialogs and nvBench show
a weak correlation with their direct counterparts,
Matplotlib and Vega-Lite, respectively. This means
many attributes that were frequently used by end
users have not been tested in these benchmarks.
These include titles, axes-scale limits, tick labels,
opacity, histogram bins, legend visibility, and mul-
tiple plots handling, as visualised in Figure 8 inAppendix D.
Conversely, PlotCoder demonstrates a strong
alignment with real-world data, with Spearman’s
values ranging from 0.7 to 0.9. The correlation
highlights PlotCoder’s potential as a resource for
crafting end-to-end Text-to-Vis benchmarks. How-
ever, it is an automatically extracted dataset. It
lacks data to plot in the input, visualisations in
the output, and information on which versions of
libraries it has as dependencies. This means the
code cannot be executed as is. Without executing
it, there is not way to confirm whether the visual
output aligns with user goals.
5.3 Comparison of attributes when permitted
Some attributes can only be activated (or permitted)
if specific preconditions are met. For instance, the
"bar thickness" attribute can only be set if the user
plots data on a bar chart and adjusts the width pa-
rameter. Consequently, these attributes may appear
infrequently in the dataset, but users often specify
their preferred values. Therefore, analysing these
attributes is crucial for a deeper understanding of
end users’ preferences.
In this analysis, we computed the frequency of
attributes for a given visualisation type or action
(e.g. plt.bar() ). This calculation is applied to
each attribute in the mapping table and visualised
as a heat map in Figure 9 located in Appendix D.
We focus solely on examining this behaviour in
Python-based datasets, including Matplotlib-nb,
Matplotlib-py, PlotCoder, and ChartDialogs. This
is because nvBench does not prioritise user inten-
tion for modifying aesthetic attributes while others
have different characteristics.
The Spearman’s coefficient calculation among
these datasets reinforces our findings in the previ-
ous section. Matplotlib-nb, Matplotlib-py, and Plot-
Coder show significant correlations, with Spear-
man’s scores above 0.8, whereas ChartDialogs
scores below 0.1. While attributes such as axes’
scales, edge colour, marker size, pie chart char-
acteristics, legend labels, and grid line attributes
receive considerable attention in ChartDialogs, end
users less frequently specify them and often rely
on the library’s defaults.
Dataset creators should consider attributes such
as histogram bins, pie precision digits, error-bar
visibility, and annotation attributes, which are fre-
quently customised by end users.No. Funcs No. Params
py-Matplotlib 6.40 10.61
nb-Matplotlib 6.19 10.54
PlotCoder 4.05 6.03
Graphics 3.20 13.82
ChartJS 6.51 12.08
Vega-Lite 10.73 19.27
nvBench 4.59 10.02
Table 3: Average number of functions and parameters
5.4 Comparison of program complexity
To compare complexity, we calculate the average
count of distinct visualisation functions and param-
eters within each code file and present the findings
in Table 3. In this section, we omit ChartDialogs
because it is a slot-filling dataset, with a fixed num-
ber of functions and parameters.
Benchmarks differ significantly from their direct
counterparts. They use far fewer functions and pa-
rameters. In most real-world data, users employ 3
to 7 functions and 10 to 14 parameters. The top 7
functions used in Matplotlib-py include plotting the
data, saving figures, assigning titles, and adjusting
legends. The higher figures in the Vega-Lite dataset
can be explained by its nature as a visualisation lan-
guage (not a library built on top of a programming
language).
6 Conclusion
In this paper, we analysed whether Text-to-Vis
benchmarks accurately reflect real-world usage
by presenting analyses of chart types, frequent at-
tributes, and program complexity. Our results show
that only one of the standard three benchmarks is
aligned with real-world use. That dataset has its
own critical limitation: it cannot be used as an end-
to-end benchmark, going from a request and data
as input to a visualisation as output. As well as cri-
tiquing current benchmarks, we provide guidance
for future benchmark development, suggesting the
evaluation of relevant attributes and challenging
charts that better reflect end users’ preferences.
Such a benchmark would guide the development
of useful and impactful systems.
Limitations
This study offers analyses of datasets and acknowl-
edges several limitations. Firstly, our examination
was restricted to only four visualisation libraries,
each corresponding to a different programminglanguage. This narrow scope may not adequately
capture the diversity of applications and use cases
within the field. Although we attempted to analyse
MatLab code files in The Stack dataset, they are
miscategorised in The Stack, processed with the
wrong extension.9Despite our efforts to clarify
this issue by reaching out to the project authors, we
have yet to receive a response. Secondly, our inves-
tigation is based on public code, mainly represent-
ing programmers with different visualisation levels,
including novices, practitioners, and experts. If the
target users are in a visualisation application like
Tableau10, our results may not be representative.
Lastly, this study concludes with an analysis and
assessment of existing benchmark datasets without
proposing solutions. Nevertheless, we believe that
the insights and recommendations provided in this
work are valuable for any dataset maker and future
studies.
Ethics Statement
The data used in this research can be found pub-
licly in the repositories of the cited papers, GitHub,
or HuggingFace. Those who want to use the pro-
cessed data in our repository will need to follow
the terms and conditions of The Stack dataset11.
Acknowledgments
This material is partially supported by the Aus-
tralian Research Council through a Discovery Early
Career Researcher Award and the Commonwealth
Scientific and Industrial Research Organisation
(CSIRO). We extend our gratitude to the anony-
mous reviewers for their constructive feedback and
valuable advice on our submissions.
References
Xinyun Chen, Linyuan Gong, Alvin Cheung, and Dawn
Song. 2021. Plotcoder: Hierarchical decoding for
synthesizing visualization code in programmatic con-
text. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and
the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 2169–2181.
Harm De Vries, Dzmitry Bahdanau, and Christopher
Manning. 2020. Towards ecologically valid re-
9https://huggingface.co/datasets/bigcode/the-stack-
dedup/blob/main/programming-languages.json
10https://www.tableau.com/
11https://huggingface.co/datasets/bigcode/the-stack-dedupsearch on language user interfaces. arXiv preprint
arXiv:2007.14435 .
Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou,
Yacine Jernite, Margaret Mitchell, Carlos Muñoz Fer-
randis, Sean Hughes, Thomas Wolf, Dzmitry Bah-
danau, et al. 2022. The stack: 3 tb of permissively li-
censed source code. Transactions on Machine Learn-
ing Research .
Guozheng Li, Xinyu Wang, Gerile Aodeng, Shunyuan
Zheng, Yu Zhang, Chuangxin Ou, Song Wang, and
Chi Harold Liu. 2024. Visualization generation
with large language models: An evaluation. arXiv
preprint arXiv:2401.11255 .
Xing Han Lu, Siva Reddy, and Harm De Vries. 2023.
The statcan dialogue dataset: Retrieving data tables
through conversations with genuine intents. In Pro-
ceedings of the 17th Conference of the European
Chapter of the Association for Computational Lin-
guistics , pages 2791–2821.
Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai,
Wenbo Li, and Xuedi Qin. 2021. Synthesizing nat-
ural language to visualization (nl2vis) benchmarks
from nl2sql benchmarks. In Proceedings of the 2021
International Conference on Management of Data ,
pages 1235–1247.
Peng Qi, Nina Du, Christopher D Manning, and Jing
Huang. 2023. Pragmaticqa: A dataset for pragmatic
question answering in conversations. In Findings of
the Association for Computational Linguistics: ACL
2023 , pages 6175–6191.
Yutong Shao and Ndapa Nakashole. 2020. ChartDi-
alogs: Plotting from Natural Language Instructions.
InProceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics , pages 3559–
3574, Online.
Yuanfeng Song, Xuefang Zhao, and Raymond Chi-
Wing Wong. 2024. Marrying dialogue systems with
data visualization: Interactive data visualization gen-
eration from natural language conversations. In Pro-
ceedings of the 30th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining , pages 2733–
2744.
Arjun Srinivasan, Nikhila Nyapathy, Bongshin Lee,
Steven M Drucker, and John Stasko. 2021. Collect-
ing and characterizing natural language utterances
for specifying data visualizations. In Proceedings
of the 2021 CHI Conference on Human Factors in
Computing Systems , pages 1–10.
Alane Suhr, Ming-Wei Chang, Peter Shaw, and Ken-
ton Lee. 2020. Exploring unexplored generalization
challenges for cross-database semantic parsing. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8372–
8388, Online.Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-
ing Yao, Shanelle Roman, Zilin Zhang, and Dragomir
Radev. 2018. Spider: A large-scale human-labeled
dataset for complex and cross-domain semantic pars-
ing and text-to-SQL task. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 3911–3921, Brussels, Bel-
gium.
A Code Parsing
After obtaining code files for Python and R, we
used abstract syntax tree (AST) parsers and heuris-
tics to accurately extract variables, function names,
arguments, and explicit values. Subsequently, we
tracked the assigned variables to correctly select the
functions used in Matplotlib while a list of Graph-
ics’ functions was used to filter for this library.
To extract ChartJS specifications, we initially
used an AST parser to extract all JSON data from
the Javascript code files. Subsequently, a heuris-
tic selection method was applied to filter JSON
containing the three essential components of this
library, namely "type," "data," and "options." This
is because ChartJS relies on the JSON format as
its foundation, serving as the input for executing
functions in Javascript.
Vega-Lite can appear in both JSON and
Javascript files, as it is a JSON schema visualisation
language. Therefore, we used the above methods
for extraction. In detail, after extracting JSON data
from code files, we exclusively extracted snippets
containing Vega-Lite schema12, which is a manda-
tory field of Vega-Lite specification.
After extracting functions, arguments, as-
signed values, and JSON specifications, tar-
geting the visualisation libraries, we trans-
formed them into a universal format to facil-
itate more accessible analysis and further pro-
cessing. For instance, a command in Python
ax.plot(x, color= 'green ', marker= 'o'),
which plots a line graph of ‘ x’, with marker
‘o’ and colour ‘ green ’, can be parsed into a
JSON as {"func_name": "plot", args: ["x"],
kargs: {"color": "green", "marker": "o"}} .
An example of translating JSON to universal for-
mat can be seen in Figure 4.
Regarding nvBench and PlotCoder, they con-
tain visualisation code in Vega-Lite and Python,
so the process was the same as described
above. When it comes to ChartDialogs, a
12Vega-Lite schema: v1, v2, v3, v4, v5,
Figure 4: The process of converting JSON to universal
format
slot-filling dataset, we converted each user’s in-
tent to a function with changed slots as key-
word parameters. For example, a user’s intent
"smaller radius, increase text size" modifying
a pie chart is transformed as universal JSON
format {'func_name ':'pie', kargs:{ 'radius:
'small ','font_size ':'large '}}.
B Benchmarks’ Examples
Figures 5, 6, and 7 illustrate examples for nvBench,
ChartDialogs, and PlotCoder, respectively.
Figure 5: A sample from the nvBench dataset.
C Cross-language Mapping Table
The procedure for making the cross-language table
is as follows. Initially, we compiled the top 100
frequent parameters from the real-world dataset
in 4 languages: Matplotlib, Graphics, ChartJS,
and Vega-Lite. Subsequently, the parameters were
grouped into different categories and attributes.
The mapping table was further expanded by in-
vestigating relevant parameters within the top 500.Figure 6: A sample in ChartDialogs dataset. This
dataset was built in a slot-filling manner. The visu-
alisation is generated by a hard-coded program.
Figure 7: A sample from the PlotCoder dataset.
If a specific language lacked relevant parameters
for a given attribute in the top 500 (resulting in a
blank cell), we persistently searched through the
remaining list until a match was found. Cells where
no relevant parameter was identified led to the an-
notation of “not found.” This identification and ver-
ification process includes understanding plotting
parameters, identifying them in API documents,
asking ChatGPT13for explanations and relevant
parameters and executing example codes.
Table 4 shows a small part of the table for con-
text. The whole table can be found in our repository
at https://github.com/giahy2507/text-to-vis-bench-
assessment.
D Calculation for heat map figures
Figure 8 shows heat maps of the most common
visualisation attributes over 7 datasets, where the
more intense green colour indicates a higher per-
centage of usage within the dataset. The calculation
for each attribute is k/n, where:
13https://chat.openai.com https://bard.google.com/chatCategory Attribute
Axesx-title, y-title, x-y-title-fontsize, x-y-title-color,
x-y-lim, x-y-ticks-labels, x-y-ticks-labels-color,
x-y-ticks-labels-rotation, x-y-scale,
x-y-ticks-fontsize, x-axis-ticks-visible,
y-axis-ticks-visible, x-y-scale-position,
invert-x-y-axis
Data Appearancefilled-color, edge-color, opacity, linewidth,
markersize, linestyle, line-capstyle, markerstyle,
bar-thickness, bar-data-stacking, hist-bins,
pie-explode, pie-label-distance,
pie-percentage-distance, pie-precision-digits,
pie-radius, errbar-cap-size, errbar-cap-thick,
errbar-color, errbar-visible
Annotationann-text/label, ann-fontsize, ann-possition,
ann-font
Main titletitle, title-fontsize, title-color, title-position,
subtitle, subtitle-fontsize
Legendlegend-title, legend-fontsize, legend-position,
legend-labels, legend-labels-color,
legend-is-display
Gridgrid-visible, grid-color, grid-linestyle,
grid-linewidth
Format size, dpi, saving-format
Otherbounding-box/border, background,
margin/padding, multiple-plots
Table 4: Categories and Attributes in the cross-language
mapping table.
•kis the number of times that attribute’s argu-
ments are specified
•nis the number of times that all arguments
are specified
As for the heat map in Figure 9, there are two
cases influencing different levels. For attributes im-
pacting the program level, such as title, x-axis title,
and x-y tick labels, the percentage is derived from
how frequently a program includes arguments for
a specific attribute. Conversely, for local attributes
affecting the function level, like filled colour, opac-
ity, and bar thickness, the percentage is calculated
based on the frequency of functions containing ar-
guments for the given attribute. The calculation is
as follows:
•kis the number of times that attribute’s argu-
ments are specified. pis the number of times
that attribute’s functions are used
•zrepresents the total number of programs in
the dataset, while gdenotes the number of
programs in which the attribute is used (any
of the attribute arguments is used).
While a figure for a given program-level attribute
isg/z, that for function-level one is k/p.multiple-plotsmargin-paddingbackgroundbounding-box-bordersaving-formatdpisizegrid-linewidthgrid-linestylegrid-colorgrid-visiblelegend-is-displaylegend-labels-colorlegend-labelslegend-positionlegend-fontsizelegend-titlesubtitle-fontsizesubtitletitle-positiontitle-colortitle-fontsizetitleann-fontann-possitionann-fontsizeann-text-labelerrbar-visibleerrbar-colorerrbar-cap-thickerrbar-cap-sizepie-radiuspie-precision-digitspie-percentage-distancepie-label-distancepie-explodehist-binsbar-data-stackingbar-thicknessmarkerstyleline-capstylelinestylemarkersizelinewidthopacityedge-colorfilled-colorinvert-x-y-axisx-y-scale-positiony-axis-ticks-visiblex-axis-ticks-visiblex-y-ticks-fontsizex-y-scalex-y-ticks-labels-rotationx-y-ticks-labels-colorx-y-ticks-labelsx-y-limx-y-title-colorx-y-title-fontsizey-titlex-titleAttribute
2.83.00.10.10.20.98.30.10.20.10.75.00.07.92.30.50.10.20.40.10.01.28.50.10.90.70.50.40.00.00.10.00.10.00.00.11.00.00.43.50.02.91.72.52.40.611.20.10.20.60.90.80.30.70.13.02.30.12.68.38.2Matplotlib-nb
2.65.10.10.30.51.65.10.10.20.20.84.50.07.32.40.50.10.20.70.10.00.86.70.21.61.00.50.50.10.00.10.00.10.00.00.00.60.10.53.60.03.31.93.22.40.711.70.10.31.01.31.00.40.50.13.43.50.22.27.26.9Matplotlib-py
2.41.70.00.20.10.37.20.00.10.10.74.40.05.81.90.20.00.20.50.00.01.111.50.00.30.20.20.10.00.00.00.00.30.00.00.12.70.00.53.00.02.22.02.32.30.610.20.00.10.40.70.40.30.90.02.41.50.12.412.712.8PlotCoder
0.00.00.00.00.00.00.06.44.95.00.00.00.03.90.00.00.00.00.00.00.00.00.00.00.00.00.01.61.82.43.02.91.42.62.51.40.40.03.11.80.02.93.06.70.03.56.17.913.40.00.00.011.40.00.00.00.00.00.00.00.0ChDialogs
1.83.00.81.21.80.52.60.00.00.00.22.00.11.53.00.80.20.10.30.00.00.55.60.72.92.83.90.30.20.00.30.00.00.00.00.00.80.20.15.30.04.92.05.90.01.215.30.01.80.60.91.10.31.30.13.35.60.00.75.65.9Graphics
0.02.90.00.00.00.00.00.21.71.83.54.30.69.51.60.80.00.00.00.10.51.05.70.10.20.10.20.00.00.00.00.00.00.00.00.00.01.20.50.20.40.82.35.50.013.921.50.10.31.61.40.81.90.40.61.36.40.50.31.81.5ChartJS
2.00.90.60.00.00.014.70.00.10.01.80.00.21.01.50.51.10.10.40.40.40.63.40.73.20.82.00.00.00.00.00.10.00.00.00.01.50.40.10.40.00.71.63.34.61.522.80.20.50.50.50.60.81.30.32.56.00.20.56.46.3Vega-Lite
0.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.90.03.10.00.00.00.04.40.00.00.00.00.00.00.00.00.00.00.00.045.845.8nvBench
Figure 8: Heat map of the most frequent aes-
thetic attributes over 7 datasets. The attributes
are classified by different categories with colours,
such as x and y axes ,data appearance ,annotation ,
title and subtitle ,legend ,grid ,figure format , and
others .multiple-plotsmargin-paddingbackgroundbounding-box-bordersaving-formatdpisizegrid-linewidthgrid-linestylegrid-colorgrid-visiblelegend-is-displaylegend-labels-colorlegend-labelslegend-positionlegend-fontsizelegend-titlesubtitle-fontsizesubtitletitle-positiontitle-colortitle-fontsizetitleann-fontann-possitionann-fontsizeann-text-labelerrbar-visibleerrbar-colorerrbar-cap-thickerrbar-cap-sizepie-radiuspie-precision-digitspie-percentage-distancepie-label-distancepie-explodehist-binsbar-data-stackingbar-thicknessmarkerstyleline-capstylelinestylemarkersizelinewidthopacityedge-colorfilled-colorinvert-x-y-axisx-y-scale-positiony-axis-ticks-visiblex-axis-ticks-visiblex-y-ticks-fontsizex-y-scalex-y-ticks-labels-rotationx-y-ticks-labels-colorx-y-ticks-labelsx-y-limx-y-title-colorx-y-title-fontsizey-titlex-titleAttribute
70.26.70.50.11.15.550.10.51.00.74.579.50.036.014.93.40.71.63.70.30.36.744.213.263.651.133.6100.011.37.424.57.874.26.34.036.372.85.140.27.60.06.39.510.88.72.333.60.00.90.00.03.32.35.60.412.47.20.87.846.046.3Matplotlib-nb
22.98.60.30.11.26.424.20.50.80.83.140.30.016.82.12.20.51.23.70.20.13.326.614.076.747.426.138.115.37.223.59.465.17.55.129.777.99.555.06.6100.07.611.714.59.12.739.30.71.58.612.02.71.82.50.48.76.90.54.730.029.9Matplotlib-py
8.40.60.00.00.10.48.70.00.10.10.98.80.016.92.50.20.10.20.50.00.01.212.56.943.836.434.9100.06.90.96.05.477.11.43.144.662.11.931.25.30.05.47.36.76.21.523.50.00.00.00.00.30.41.10.01.90.90.11.515.015.0PlotCoder
0.00.00.00.00.00.00.058.343.243.30.00.00.053.20.00.00.00.00.00.00.00.00.00.00.00.00.08.49.712.815.947.422.643.141.123.244.00.036.416.10.016.626.420.40.013.229.538.221.20.00.00.027.60.00.00.00.00.00.00.00.0ChDialogs
Figure 9: Heat map of attributes that the user often
specifies values when permitted.Category Attribute Matplotlib R ChartJS Vega-Lite ChartDialogs
x-y-axis limadd_subplot|xlim
set|xlim
axes|xlim
set_xlim|left
set_xlim|right
xlim|left
xlim|right
xlim|xmin
xlim|xmax
set_xlim|xmin
set_xlim|xmax
add_subplot|ylim
set|ylim
axes|ylim
set_ylim|bottom
set_ylim|top
ylim|bottom
ylim|top
ylim|ymin
ylim|ymax
set_ylim|ymin
set_ylim|ymaxplot|xlim
hist|xlim
barplot|xlim
plot.window|xlim
plot.default|xlim
matplot|xlim
boxplot|xlim
curve|xlim
points|xlim
lines|xlim
plot|ylim
barplot|ylim
boxplot|ylim
hist|ylim
lines|ylim
matplot|ylim
points|ylim
plot.window|ylim
plot.default|ylimoptions|scales|xAxes|ticks|beginAtZero
options|scales|xAxes|ticks|suggestedMax
options|scales|xAxes|ticks|min
options|scales|xAxes|ticks|max
options|scales|x|beginAtZero
options|scales|x|min
options|scales|x|max
options|scales|yAxes|ticks|beginAtZero
options|scales|y|beginAtZero
options|scales|yAxes|ticks|suggestedMax
options|scales|yAxes|ticks|min
options|scales|yAxes|ticks|maxencoding|x|scale|domain
encoding|x|scale|domain|selection
encoding|x|scale|domain|param
encoding|y|scale|domain
encoding|y|scale|domain|selection
encoding|y|scale|domain|paramNot found
x-y-axis x-y-scalexscale|value
set_xscale|value
yscale|valueplot|log
lines|log
boxplot|logoptions|scales|xAxes|type
options|scales|x|type
options|scales|yAxes|type
options|scales|y|typeencoding|x|scale|type
encoding|y|scale|typeplot|x_axis_scale
contour|x_axis_scale
bar|x_axis_scale
scatter|x_axis_scale
plot|y_axis_scale
contour|y_axis_scale
bar|y_axis_scale
scatter|y_axis_scale
data
appearancecolorplot|color
plot|c
scatter|color
scatter|c
plot|fmt=color
axvline|color
axhline|color
bar|color
barh|color
fill_between|color
hist|color
errorbar|color
contour|colors
set_facecolor|color
vlines|color
hlines|color
Circle|color
Line2D|color
axvspan|color
quiver|color
pie|colors
arrow|color
text|color
annotate|colorlines|col
plot|col
points|col
abline|col
barplot|col
hist|col
polygon|col
barplot|col
rect|col
segments|col
boxplot|col
image|col
curve|col
pie|col
matplot|col
contour|col
stripchart|col
text|col
mtext|coldata|datasets|backgroundColor
data|datasets|pointBackgroundColor
options|plugins|datalabels|color
data|datasets|fillColor
data|datasets|strokeColor
options|elements|line|backgroundColor
data|datasets|pointColor
options|plugins|crosshair|line|color
options|elements|point|backgroundColor
data|datasets|pointStrokeColor
options|plugins|datalabels|colorencoding|color|scale|range
encoding|color|value
encoding|color|aggregate
mark|fill
mark|color
encoding|color|scale|scheme
encoding|color|sort
config|mark|color
mark|color|stops|offset
mark|color|stops|color
encoding|color|typebar|bar_face_color
hist|bar_face_color
plot|line_color
plot|marker_face_color
scatter|marker_face_color
data
appearancemarkerstyleplot|fmt=markerstyle
plot|marker
scatter|marker
errorbar|markerpoints|pch
plot|pch
legend|pch
lines|pch
pairs|pch
matplot|pch
stripchart|pch
par|pchdata|datasets|pointStyleencoding|shape|type
encoding|shape|valueplot|marker_type
scatter|marker_type
data
appearancebar-thicknessbar|width
barh|heightbarplot|widthdata|datasets|barPercentage
data|datasets|maxBarThickness
data|datasets|barThickness
data|datasets|barWidth
options|barThicknessmark|width
mark|heightbar|bar_height
bar|bar_width
data
appearancehist-bins hist|bins hist|breaks Not foundencoding|x|bin
encoding|x|bin|step
encoding|x|bin|maxbins
encoding|y|bin|maxbins
encoding|x|bin|stephist|number_of_bins
title titletitle|label
set_title|label
set|title
set_title|titleplot|main
hist|main
title|main
barplot|main
boxplot|main
pie|main
image|main
matplot|main
pairs|main
plot.default|main
curve|mainoptions|title|display
options|title|text
options|plugins|title|display
options|plugins|title|text
options|titletitle|title
title|textNot found
Table 5: Details of the cross-language mapping table for 7 attributes over 3 categories. Each parameter in attributes
comprises two parts, function name and argument name, separated by “ |”.