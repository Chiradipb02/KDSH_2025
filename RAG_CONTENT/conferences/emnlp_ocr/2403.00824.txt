Information Flow Routes:
Automatically Interpreting Language Models at Scale
Javier Ferrando∗1Elena Voita2
Abstract
Information flows by routes inside the network
via mechanisms implemented in the model. These
routes can be represented as graphs where nodes
correspond to token representations and edges
to computations. We automatically build these
graphs in a top-down manner, for each prediction
leaving only the most important nodes and edges.
In contrast to the existing workflows relying on
activation patching, we do this through attribution:
this allows us to efficiently uncover existing cir-
cuits with just a single forward pass. Unlike with
patching, we do not need a human to carefully
design prediction templates, and we can extract
information flow routes for any prediction (not
just the ones among the allowed templates). As
a result, we can analyze model behavior in gen-
eral, for specific types of predictions, or different
domains. We experiment with Llama 2 and show
that the role of some attention heads is overall
important, e.g. previous token heads and sub-
word merging heads. Next, we find similarities
in Llama 2 behavior when handling tokens of the
same part of speech. Finally, we show that some
model components can be specialized on domains
such as coding or multilingual texts.1
1. Introduction
Current state-of-the-art LMs are built on top of the Trans-
former architecture (Vaswani et al., 2017; Brown et al., 2020;
Zhang et al., 2022; Chowdhery et al., 2022; Touvron et al.,
2023a;b). Inside the model, each representation evolves
from the current input token embedding to the final rep-
1Universitat Polit `ecnica de Catalunya2FAIR
at Meta AI. Correspondence to: Javier Ferrando
<javier.ferrando.monsonis@upc.edu >.
1The proposed method is implemented in:
https://github .com/facebookresearch/llm-
transparency-tool .
∗Work done during an internship at Meta AI.
Figure 1. The important information flow routes for a token (Mary)
prediction. GPT2-Small, τ= 0.04.
resentation used to predict the next token. This evolution
happens through additive updates coming from attention and
feed-forward blocks. The resulting stack of same-token rep-
resentations is usually referred to as “residual stream” (El-
hage et al., 2021), and the overall computation inside the
model can be viewed as a sequence of residual streams con-
nected through layer blocks. Formally, we can see it as
a graph where nodes correspond to token representations
and edges to operations inside the model (attention heads,
feed-forward layers, etc.).
While during a forward pass all the edges are present, com-
putations important for each prediction are likely to form a
small portion of the original graph (V oita et al., 2019; Wang
et al., 2023; Hanna et al., 2023, among others). We extract
this important subgraph in a top-down manner by tracing
information back through the network and, at each step,
leaving only edges that were relevant (Figure 1). To under-
stand which edges are important, we rely on an attribution
method (Ferrando et al., 2022) and refuse from activation
patching2, typical for the existing mechanistic interpretabil-
ity workflows (Wang et al., 2023; Hanna et al., 2023; Conmy
et al., 2023; Stolfo et al., 2023). Firstly, patching requires
human efforts to create templates and contrastive examples,
2Originally introduced by Vig et al. (2020) to analyze LMs
through the lens of causal mediation analysis.
1arXiv:2403.00824v2  [cs.CL]  16 Apr 2024Information Flow Routes: Automatically Interpreting Language Models at Scale
Figure 2. Full information flow graph.
thus is only applicable to a few pre-defined templates. Sec-
ondly, explaining a single prediction demands a substantial
number of interventions (patches). Given that each interven-
tion needs a forward pass, studying large models becomes
increasingly impractical. In contrast, our method is about
100 times faster.
In the experiments, we first show that our information flow
routes rely on the same task-specific attention heads found
in patching circuits (Wang et al., 2023; Hanna et al., 2023).
However, our method is more versatile and informative than
patching: it can evaluate the importance of model compo-
nents both (i) overall for a prediction, and (ii) compared
to a contrastive example (i.e., patching setting). What is
more, we argue that patching is fragile: its results can vary
depending on the choice of the contrastive template (i.e.,
human preference).
Next, we come to the settings unreachable to patching, i.e.
the broad set of predictions and general importance. We
experiment with Llama 2 and show that some attention head
functions are overall important, e.g. previous token heads
and subword merging heads. Next, we find that information
inside Llama 2 flows similarly when handling tokens of the
same part of speech. Finally, we show that some model
components are specialized on domains such as coding or
multilingual texts: they are active for these domains and not
active otherwise.
Overall, our contributions are as follows:
•we propose to explain predictions of transformer LMs
via information flow routes;
•compared to patching circuits, our method is (i) ap-
plicable to any prediction, (ii) more informative, and
(iii) around 100 times faster;
•we analyze the information flow of Llama 2 and find
model components that are (i) generally important, and
(ii) specific to domains.
Figure 3. General-case algorithm for extracting the important sub-
graph, the information flow routes, from the full information
graph (Figure 2).
2. Extracting Information Flow Routes
Figure 2 illustrates computations inside a Transformer LM
along with the intermediate representations after each block.
Information flow graph. In the graph (i) nodes corre-
spond to token representations and (ii) edges to operations
inside the network moving information across nodes. Specif-
ically, xlA
posandxl
posare representations of the token at po-
sition posafter the attention block in layer lor the entire
layer, respectively. Each xl−1
1, ...,xl−1
posis connected to xlA
pos
via attention edges (and via a residual stream edge from
xl−1
postoxlA
pos),xlA
posis connected to xl
posvia two edges: the
FFN output and the residual stream.
Extracting the important subgraph. While during a for-
ward pass all edges in Figure 2 are present, computations
relevant for each prediction are likely to form a small por-
tion of the original graph (V oita et al., 2019; Wang et al.,
2023; Hanna et al., 2023, etc). We extract this important
subgraph, the information flow routes , in a top-down manner
by tracing information back through the network (Figure 3).
We start from a single node – the representation on top of the
residual stream. Then, we go over immediately connected
lower nodes and, if important at this step, we add them to
the subgraph along with the corresponding edges. The algo-
rithm requires setting a threshold τfor the minimum edge
importance.
Importance via attribution, not patching. To complete
the proposed algorithm (Figure 3), we need to specify how
to compute the importance of an edge. While lately it has
become typical to use patching (see Appendix A for a more
formal description) (Wang et al., 2023; Hanna et al., 2023;
Conmy et al., 2023; Heimersheim & Janiak, 2023), instead
2Information Flow Routes: Automatically Interpreting Language Models at Scale
of patching, we choose to use attribution . This choice is
crucial for our work and makes our method about 100 times
faster than alternatives while (i) being able to recover previ-
ously discovered circuits, (ii) doing this in a more versatile
manner, and (iii) leading to new observations. Next, we
explain the specific attribution method we use.
2.1. Evaluating Edge Importance
For the attribution method, we adopt ideas from ALTI (Ag-
gregation of Layer-Wise Token-to-Token Interactions) (Fer-
rando et al., 2022). While ALTI propagates attributions
throughout the entire model, we only use its definition of
contributions between connected nodes. We choose this
method due to its simplicity, ease of implementation, and
demonstrated effectiveness in practical applications, e.g. de-
tecting hallucinations in neural machine translation (Dale
et al., 2023a;b; Guerreiro et al., 2023).
2.1.1. I MPORTANCE IN GENERAL CASE
In our graph (Figure 2), each node represents a sum of in-
coming vectors (edges). According to ALTI, the importance
of each vector (edge) to the overall sum (node) is propor-
tional to its proximity to the resulting sum. Formally, if
y=z1+···+zm,
importance (zj,y)=proximity (zj,y)P
kproximity (zk,y), (1)
proximity (zj,y) = max( −||zj−y||1+||y||1,0).
Here, we use negative distance as a measure of similarity:
the smaller the distance between zjandy, the more the in-
formation of zjiny. Note also that we ignore contributions
of the vectors lying beyond the l1length of y. For more
details, see Ferrando et al. (2022).
Now, we need to define vector updates corresponding to the
edges for the FFN and the attention blocks.
2.1.2. D EFINING FFN E DGES
For the FFN blocks, edge vectors are straightforward. Fol-
lowing the notation of Figure 2:
xl
pos=xlA
pos+FFNl(xlA
pos),
where the terms correspond to the edges of the residual
connection and FFN, respectively.
2.1.3. D EFINING ATTENTION EDGES
We follow previous work (Kobayashi et al., 2020; Ferrando
et al., 2022) and decompose the output of an attention block
into a sum of vectors (edges), each corresponding to a con-
nection between residual streams (Figure 4).
Figure 4. Decomposition of an update coming from an attention
head into per-input terms. Layer indices are omitted for readability.
Attention heads. Formally, for attention head h
Attnh(x≤pos) =X
j≤posαh
pos,jfh(xj), (2)
where fh(xj) =xjLWh
OV,Wh
OV=Wh
VWh
Othe values
and output combined matrix for head h,αh
pos,j are scalar
attention weights, and Lis the linearized layer normalization
(see Appendix B.1).3
Expanding further, a typical attention implementation
(1) weights corresponding value vectors within each atten-
tion head, (2) concatenates head outputs, and (3) further
multiplies by the output matrix WO. We split the output
matrix WOinto its head-specific parts Wh
O, drag these parts
inside attention heads, and combine them with the head’s
values matrix: Wh
OV=Wh
VWh
O.
Overall, Figure 4 shows that along an attention head
each representation xjis transformed linearly into
fh(xj) =xjLWh
OVand multiplied by a scalar attention
weight αh
pos,j. This gives us the “raw output” emitted by
each input vector xjwhen treating attention weights as
prediction-specific constants. In this view, information
flows through attention heads by independent channels
αh
pos,jfh(xj)that converge in the next residual stream state ,
we refer to each of this channels as a sub-edge.
Attention block. The information in an attention block
flows through all the independent channels (sub-edges) in
theHheads (Figure 5):
Attn(x≤pos) =HX
hX
j≤posαh
pos,jfh(xj). (3)
3From here onwards, we omit layer indices for readability.
3Information Flow Routes: Automatically Interpreting Language Models at Scale
Figure 5. Decomposition of an update coming from an entire at-
tention layer into per-input terms. Layer indices are omitted for
readability.
We compute the importance of each of the sub-edges in
this sum as described in Section 2.1.1 and aggregate across
heads the capacities of those sub-edges connecting the same
pair of nodes,PH
heh
pos,j. Additionally, we include the
importance of the residual connection for the current token,
eresattn
pos (Figure 5). Formally, attention edge importances are
computed as
eattn
pos,j=(PH
heh
pos,j ifj̸=posPH
heh
pos,j+eresattn
pos ifj=pos(4)
where eh
pos,j=importance (αh
pos,jfh(xj),xA
pos)and
eresattn
pos =importance (xpos,xA
pos)respectively, as defined
in equation (1).
2.2. Extracting the Important Subgraph
In Section 2.1 we explained how we compute the impor-
tances of the edges in the full information flow graph (Fig-
ure 2). Finally, when building the important information
flow subgraph (routes), we add only the edges with an impor-
tance above the specified threshold τ(Figure 3). Although
we didn’t notice significant differences, in our experiments
we first remove the sub-edges with importances eh
pos,j and
eresattn
pos,j below τ, and renormalize the rest before aggregating
across heads.
Generally, feed-forward blocks have higher importance than
attention heads. This is expected: while attention heads are
only a (small) part of the attention block, the feed-forward
block is not decomposed further. Therefore, one can set
different thresholds for retaining attention and FFN edges,
although we did not experiment with this.
3. Information Flow vs Patching Circuits
First, we compare our information routes to the circuits
found in previous work for the Indirect Object Identification
(IOI) (Wang et al., 2023) and Greater-than (Hanna et al.,2023) tasks. The IOI task consists of predicting the next
word in sentences like “ When Mary and John went to the
store, John gave a drink to ”. An initial clause features
the names of two individuals (Mary and John). Then, the
second clause depicts a person exchanging an item with the
other person. The goal is to predict the second name in the
main clause, i.e. the Indirect Object (Mary). In the Greater-
than task, the model is prompted to predict a number given
a sentence following the template: “ The [noun] lasted from
the year XXYY to the year XX ”. Here, the task is to predict
a number higher than YY .
Information flow vs patching. The circuits for these tasks
were previously found using activation patching with con-
trastive templates. These templates are designed to bring
to the surface the specific task. For example, for IOI a con-
trastive template contains three different names instead of
repeating one of them (Figure 7). One of the differences
between information flow and patching results is that our
method finds all the components that contributed to the
prediction, while patching finds what is important for the
original task but not the contrastive baseline. As we will
see, (i) our method can also be used in this manner, and (ii)
it gives more reasonable results.
3.1. Indirect Object Identification
For the IOI task, the previously found circuit contains sev-
eral attention heads with various functions, e.g. Name
Mover Heads, Duplicate Token Heads, etc. (Wang et al.,
2023). Using our method, we extract information flow
routes for both the original task and the contrastive tem-
plates, and evaluate the activation frequency of attention
heads4. When looking at overall contributions, Figure 7
(left) shows that our routes largely consist of the heads
discovered previously. There are, however, several heads
which are always part of the routes (have near 1 activa-
tion frequency) but are not in the “patching circuit” (shown
with pale circles). Interestingly, when looking at the dif-
ference with contrastive templates (Figure 7, right), these
generic heads disappear. This makes sense: information
flow routes contain all the components that were important
for prediction, and these contain (i) generic components
that are overall important, and (ii) task-specific heads. Note
also how our method goes further than patching for identi-
fying the difference between the original and the contrastive
tasks. Figure 7 shows that e.g. the previous token heads
(yellow) are important for both types of predictions and are
not specific to the IOI task.
4We consider an attention head activated if it has at least a
sub-edge in the information flow routes (important subgraph).
4Information Flow Routes: Automatically Interpreting Language Models at Scale
Figure 6. Greater-than, GPT2-Small. Attention head activation frequency ( τ= 0.03).
Figure 7. IOI, GPT2-Small. Attention head activation frequency
(τ= 0.03).
3.2. Greater-than
For the overall contributions in the greater-than task (Fig-
ure 6, left), we also see that (i) task-specific heads discov-
ered via patching are among the most important heads in the
information flow routes, and (ii) many other heads are impor-
tant. Let us now look at the differences with the contrastive
templates.
Contrastive template matters. Interestingly, when using
the same template as Hanna et al. (2023), i.e. “ ...from the
year XXYY ” with YY= 01 , instead of YY>01overall con-
tributions do not change much: all heads important for the
original task are also important for the contrastive template
(Figure 6, center). While this contradicts the original work
by Hanna et al. (2023), this is expected: predicting a number
higher than “ 01” still requires greater-than reasoning (notall numbers would fit after “ 01”, e.g. “ 00” would not). In
contrast, if we consider another template with YY= 00 ,
overall contributions change and “greater-than heads” (red
in Figure 6) become less important. This difference in the re-
sults for “ 01” vs “ 00” in the contrastive template highlights
the fragility of patching: not only it requires human-defined
contrastive templates, but also patching results are subjec-
tive since they vary depending on the chosen template .
Overall, we see that, compared to patching, information flow
routes are more versatile and informative. Indeed, they can
find the importance of model components both (i) overall for
a prediction, and (ii) compared to a contrastive example (i.e.,
patching setting), and are able to show differences between
the contrastive templates.
3.3. OPT-125m vs GPT2-Small
Additionally, we experiment with OPT-125m which has the
same number of layers and heads than GPT2-small. We
find that the information flow routes for IOI and greater-
than tasks are similar for OPT-125M and GPT2-small (Ap-
pendix C).
3.4. Hundred Times Faster than Patching
Obtaining information flow routes involves a two-step pro-
cess. First, we run a forward pass and cache internal activa-
tions. Then, we obtain edge importances to build the sub-
graph as depicted in Algorithm 3. For comparison purposes,
we contrast our approach with the ACDC algorithm (Conmy
et al., 2023), an automated circuit discovery algorithm based
on patching. According to their findings, on the IOI task
with a batch of 50 examples, it requires 8 minutes on a
single GPU to discover the circuit. In contrast, our method
accomplishes this task in 5 seconds, around 100x in time
reduction.
5Information Flow Routes: Automatically Interpreting Language Models at Scale
Figure 8. t-SNE of component importance vectors. Coloured by:
(a) input token POS Tag, (b) next token POS tag, (c) whether the
input token is the first or a later subword. Llama 2-7B.
4. General Experiments
Information flows inside the network via mechanisms im-
plemented in the model. In this section, we look at general
patterns in component importances, and try to understand
if important mechanisms depend on the type of tokens pro-
cessed. Using the full information flow graph (Figure 2) of
Llama 2-7B, we look at all the immediate connections with
each residual stream. We then record the importance values
of all the sub-edges corresponding to individual attention
heads (i.e., eh
pos,j), as well as FFN blocks. We use a subset
of 1000 sentences from the C4 dataset (Raffel et al., 2020).
4.1. Component Importance for POS
First, let us see how component importance depends on
parts of speech (POS). For this, we pack per-prediction com-
ponent importances into vectors5and apply t-SNE (van der
Maaten & Hinton, 2008). Figure 8 shows the resulting pro-
jection with datapoints colored according to either input or
next token6part of speech.7
Content vs function words. From Figure 8a we see that
for function words as inputs, component contributions are
clustered according to their part of speech tag. Roughly
speaking, for these parts of speech the model has “typical”
5Each vector corresponding to the pos-th position is defined asP
je1,1
pos,j,P
je1,2
pos,j, . . . ,P
jeL,H
pos,j, effn1pos, . . . , effnLpos
.
6Here, we take the next token from the dataset and not the one
generated by the model. While this adds some noise to the results,
we expect at least parts of speech of the reference and the predicted
tokens to be similar.
7We obtain POS tags with NLTK (universal tagset, Petrov et al.
(2012)) and assign a tag to all subwords of each word.information flow routes and, by knowing component con-
tributions, we can infer input token POS rather accurately.
Interestingly, this is not the case for content words: while
we can see verbs being somewhat separated from nouns,
overall contribution patterns for content words are mixed
together in a large cluster. Apparently, the reasoning for
these words is more complicated and is defined by a broader
context rather than simply input token POS.
First vs later subwords. Diving deeper, Figure 8c shows
that contribution patterns strongly depend on whether the
current token is the first or a later subword of some word.
Clearly, there are some model components specialized for
first or later subwords – we confirm this later in Section 4.3.
Note also that Figure 8c explains the two distinct clusters
for numbers we saw in Figure 8a (purple): they separate
number tokens into first and later digits.
Patterns wrt to current vs next token. Finally, Figure 8b
shows the same datapoint colored by the part of speech of
the next token. Comparing this to Figure 8a, we see that
contribution patterns depend more on input tokens rather
than output tokens. This might be because in the lower
part of the network, a model processes inputs in a generic
manner, while the higher network part (that prepares to
generate output) is more fine-grained and depends on more
attributes than part of speech.
4.2. Bottom-to-Top Patterns
Since we already started talking about functions of the lower
and the higher parts of the network, let us now look at
the bottom-up contribution patterns. Figure 9a shows as
average activation frequency of attention heads for a small
τ= 0.01– this gives us an estimate “of” how many times
an attention head affects a residual stream. Additionally,
we show the importance of the FFN block in each layer.
As expected, attention and feed-forward blocks are more
active at the bottom part of the network, where the model
processes input and extracts general information. Later,
when a model performs more fine-grained and specialized
operations, the activation frequency of individual attention
heads and FFN blocks goes down. Interestingly, the last-
layer FFN is highly important: apparently, this last operation
between the residual stream and the unembedding matrix
(i.e., prediction) changes representation rather significantly.
4.3. Positional and Subword Merging Heads
Some of our observations above hint at certain attention
head functions that might be overall important. First, in
Section 3.1 we showed that previous tokens heads are gener-
ally important in the IOI task, both for target examples and
the contrastive baseline – this attention head function might
be important in general. In Section 4.1 we noticed a clear
6Information Flow Routes: Automatically Interpreting Language Models at Scale
Figure 9. Attention head activation frequency ( τ= 0.01) and FFN
block importance. We show only top-50 %important heads. Llama
2-7B.
difference between component contributions for first and
later subwords – this suggests that some model components
might be responsible for that.
In what follows, we look at two attention head functions
found earlier for machine translation, positional and sub-
word merging heads, and see whether they are generally
important. Differently from previous work, we define a
head function based on token contributions within the head
and not attention weights as done previously, since attention
weights might not reflect influences properly (Bastings &
Filippova, 2020; Kobayashi et al., 2020, among others).
Positional heads. Originally, previous token heads were
found to be the most important attention heads in machine
translation encoders (V oita et al., 2019). Now, let us check
their importance for LLMs. We refer to a head as the previ-
ous token head if in at least 70 %of the cases it puts more
than half of its influence on the previous token.
Figure 9a shows previous token heads in yellow. As in the
earlier work for machine translation (V oita et al., 2019), we
also see that for LLMs, (i) there are several previous tokens
heads in the model, and (ii) almost all of them are by far the
most important attention heads in the corresponding layers.
Subword merging heads. Putting together our first-vs-
later subword observations in Section 4.1 and previous work,
we might expect our model to have subword merging heads
found for machine translation encoders (Correia et al., 2019).
We refer to a subword merging head if later subwords take
information from previous subwords of the same word but
the head is notprevious token head (see Appendix B.2
Figure 10. Examples of important information flow subgraphs
(τ= 0.01). Llama 2-7B.
for a formal definition). We see subword merging heads
in the bottom part of the network (Figure 9a, red), and
are among the most important heads in the corresponding
layers. Notably, previous work did not study their general
relevance (Correia et al., 2019). Note that these subword
merging heads are important for later subwords and not
important otherwise – this explains the clusters we saw in
Figure 8c.
We would like to highlight that for LMs, we are the first to
talk about general importance of attention heads and to find
that some of the most important heads are previous token
and subword merging heads. Future work might explore the
functions of other important heads in the model.
4.4. Peculiar Information Flow Patterns, or Periods
Acting as BOS
In Section 4.1 we talked about general contribution patterns
and saw visible clusters corresponding to the input tokens’
part of speech. Now, let us go deeper and look in detail
at one of the clusters. We choose the outlier punctuation
cluster shown in yellow in Figure 8 (to the right) – this
cluster corresponds to the first period in a text.8
Figure 9b shows the average importance of model compo-
nents for examples in this cluster. We see that for these
8823 out of 826 datapoints.
7Information Flow Routes: Automatically Interpreting Language Models at Scale
Figure 11. Average importance of attention heads and FFNs for different datasets. For non-general domains, we show only heads with
importance higher than 0.015. Llama 2-7B.
examples, the residual stream ignores all attention and FFN
blocks in all the layers except for the first and last few:
for most of the layers, contributions of all model compo-
nents are near zero. When we look at the information flow
graphs for these examples, we see that, even for a rather
small threshold τ= 0.01, bottom-to-top processing happens
largely via “untouched” residual connection (Figure 10).
In Appendix D we show that up to the last three layers,
this residual stream takes the role of the BOS token and
future tokens treat this residual stream as such. Concurrent
work (Cancedda, 2024) suggests that early FFN’s update
into a particular subspace is responsible for this behavior.
While for some examples periods acting as BOS might be
reasonable, this also happens in cases where the period does
not have an end-of-sentence role. For example, Figure 10
(right): while in a sentence For the second game
in a row, St. Thomas ... the first period does
not have the end-of-sentence meaning, the residual stream
is still acting in the same manner. In future work, it might
be valuable to explore whether this behavior might cause
incorrect generation behavior.
5. Domain-Specific Model Components
We have seen that some attention heads’ roles are gen-
erally important across predictions. Now, we analyze if
some model components are instead specialized in specific
domains. We evaluate the average importance of Llama
2-7B’s components on 1000 sentences from different do-
mains. Specifically, we consider (i) C4 (Raffel et al., 2020),
(ii) English, Russian, Italian, Spanish FLORES-200 devtest
sets (NLLB et al., 2022; Goyal et al., 2022), (iii) code
data from CodeParrot9, (iv) addition/subtraction data (Stolfo
et al., 2023).
9https://huggingface .co/datasets/
codeparrot/github-code5.1. Components are Specialized
Figure 11 shows the importance of attention heads and FFN
blocks in general and specific datasets.
Coarse grained: domains and languages. We see that
generally unimportant attention heads become highly rel-
evant for specific tasks. For example, the most important
heads for addition are among the lowest scoring heads when
looking at C4 (Figure 11 (left), large yellow blobs to the
left). Another interesting observation is that domain-specific
heads are different across domains: important heads for addi-
tion, code, and non-English are not the same heads. Overall,
we can see that model components are largely specialized.
Fine-grained: addition vs subtraction. Figure 11 (right)
shows more fine-grained results where instead of different
domains, we look at tasks within the same narrow domain:
addition vs subtraction. While the important heads for addi-
tion and subtraction largely intersect, we see several heads
that are active only for one task and not the other (bright
blue and yellow blobs). This suggests that this fine-grained
specialization might be responsible for “reasoning” inside
the model and not just domain-specific processing; future
work may validate this further. Interestingly, we did not
find similar specialization between languages and only find
non-English heads – probably, portions of training data in
these languages were not large enough to have dedicated
language-specific heads.
Finally, we can also see the difference in the importance
of the feed-forward blocks with respect to each domain.
For example, the last FFN layer is much less relevant for
non-English than other domains (Figure 11, left), and the
importance of FFN blocks for addition and subtraction falls
to zero in some layers (Figure 11, right). In future work, one
might conduct a more fine-grained analysis of the impor-
8Information Flow Routes: Automatically Interpreting Language Models at Scale
Figure 12. Top 10 tokens after projecting singular values of WOVonto the unembedding matrix. Singular value ids are shown in gray.
tance of feed-forward layers by looking at the relevance and
functions of individual neurons (along the lines of (Geva
et al., 2021; Dai et al., 2022; V oita et al., 2023) but taking
into account neuron importance for specific predictions).
5.2. Specialized Heads Output Topic-Related Concepts
In this section, we analyze what our domain-specific heads
write into the residual stream, and find that some of them
write highly interpretable and topic-related concepts.
Weight matrices analysis with SVD. As we illustrated in
Figure 4, Wh
OVtransforms representations from each of the
residual streams10into vectors that are added to the current
residual stream. To understand what kind of information
is embedded in this transformation, we use Singular Value
Decomposition (SVD). To get an intuitive explanation of
theWh
OVimpact, we can factorize it via the “thin” singular
value decomposition (Millidge & Black, 2022) as Wh
OV=
UΣVT.11Then, projecting x∈R1×dthrough Wh
OVcan
be expressed as
xWh
OV= (xUΣ)VT=rX
i=1(xuiσi)vT
i. (5)
Here, each uiσi∈Rd×1can be interpreted as a key that is
compared to the query ( x) via dot product (Molina, 2023).
Each query-key dot-product weights the right singular vec-
torvT
i. If we project these right singular vectors to the
unembedding matrix ( vT
iWU), we get an interpretation of
the attention head’s influence in terms of concepts (i.e., to-
kens) it promotes in the residual stream.
Top singular values. For some of the heads specific to
code and non-English inputs we saw in Figure 11, in Fig-
ure 12 we show the top 10 tokens that come from the de-
scribed above projection. We see that code-specific heads
promote tokens related to coding and technology in a more
general sense: e.g., tokens related to Apple, like iOS, Xcode,
iPhone, etc. Heads most active for non-English promote
tokens in multiple languages, avoiding English ones. Also,
10After applying layer normalization first.
11U∈Rd×r,Σ∈Rr×r,VT∈Rr×d;dis vector dimension-
ality in the residual stream, ris the rank of Wh
OV.we see tokens related to cities (Sydney), countries (Indones),
and currencies ( £). Overall, we looked at the functions of
attention heads from two perspectives: (i) when a head is
active, and (ii) how it updates the residual stream, and found
they are consistent. While a similar kind of analysis was
done before for neurons (V oita et al., 2023), our method
made it possible to talk about entire model components
being active/non-active for a prediction.
6. Related Work
Earlier works evaluating the importance of model compo-
nents include Bau et al. (2019), who identified important
neurons in NMT models shared across models, and V oita
et al. (2019) who looked at the relevance of entire attention
heads in the Transformer from two perspectives: attribu-
tion and pruning. More recent research focuses more on
task-specific rather than overall importance. They aim to
find subparts of LMs responsible for these tasks and rely
on activation patching methodology, firstly introduced to
analyze biases in LMs by Vig et al. (2020), and used across
several others (Heimersheim & Janiak, 2023). Initial works
discovered circuits for different tasks of GPT2, like IOI
(Wang et al., 2023) and greater-than (Hanna et al., 2023).
Patching has been also used to locate factual knowledge
in LMs (Meng et al., 2022), and to discover task vectors
behind in-context learning capabilities (Hendel et al., 2023;
Todd et al., 2023). Our work can be related to the concur-
rent line of research developing methods to approximate
activation patching (Syed et al., 2023; Kram ´ar et al., 2024;
Hanna et al., 2024).
7. Conclusions
We view computations inside the Transformer as informa-
tion flowing between token representations through model
components. Using this view, we propose to interpret lan-
guage model predictions by extracting the important part
of the overall information flow. Our method for extracting
these important information flow routes is automatic, highly
efficient, applicable to any prediction, more versatile and
informative compared to existing pipelines.
9Information Flow Routes: Automatically Interpreting Language Models at Scale
8. Acknowledgments
We would like to thank Yihong Chen, Christoforos Nalmpan-
tis, Igor Tufanov, Nicola Cancedda and Eduardo S ´anchez
for the insightful discussions.
References
Bastings, J. and Filippova, K. The elephant in the in-
terpretability room: Why use attention as explanation
when we have saliency methods? In Alishahi, A.,
Belinkov, Y ., Chrupała, G., Hupkes, D., Pinter, Y .,
and Sajjad, H. (eds.), Proceedings of the Third Black-
boxNLP Workshop on Analyzing and Interpreting Neu-
ral Networks for NLP , pp. 149–155, Online, November
2020. Association for Computational Linguistics. doi:
10.18653/v1/2020 .blackboxnlp-1 .14. URL https://
aclanthology .org/2020 .blackboxnlp-1 .14.
Bau, A., Belinkov, Y ., Sajjad, H., Durrani, N., Dalvi, F., and
Glass, J. Identifying and controlling important neurons in
neural machine translation. In International Conference
on Learning Representations , 2019. URL https://
openreview .net/forum?id=H1z-PsR5KX .
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,
Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,
Radford, A., Sutskever, I., and Amodei, D. Language
models are few-shot learners. In Larochelle, H., Ranzato,
M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.),
Advances in Neural Information Processing Systems ,
volume 33, pp. 1877–1901. Curran Associates, Inc., 2020.
URL https://proceedings .neurips .cc/
paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-
Paper .pdf.
Cancedda, N. Spectral filters, dark signals, and atten-
tion sinks, 2024. URL https://arxiv .org/abs/
2402 .09221 .
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton,
C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
S., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,
N., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,
Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,
Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,
Michalewski, H., Garcia, X., Misra, V ., Robinson, K., Fe-
dus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,
B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,
S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,
K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,
Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,
J., Petrov, S., and Fiedel, N. Palm: Scaling language
modeling with pathways, 2022.
Conmy, A., Mavor-Parker, A., Lynch, A., Heimersheim,
S., and Garriga-Alonso, A. Towards automated
circuit discovery for mechanistic interpretability.
In Oh, A., Neumann, T., Globerson, A., Saenko,
K., Hardt, M., and Levine, S. (eds.), Advances
in Neural Information Processing Systems , vol-
ume 36, pp. 16318–16352. Curran Associates,
Inc., 2023. URL https://papers .nips .cc/
paper files/paper/2023/hash/
34e1dbe95d34d7ebaf99b9bcaeb5b2be-
Abstract-Conference .html .
Correia, G. M., Niculae, V ., and Martins, A. F. T. Adap-
tively sparse transformers. In Inui, K., Jiang, J., Ng, V .,
and Wan, X. (eds.), Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP) , pp. 2174–2184,
Hong Kong, China, November 2019. Association for
Computational Linguistics. doi: 10 .18653/v1/D19-1223.
URL https://aclanthology .org/D19-1223 .
Dai, D., Dong, L., Hao, Y ., Sui, Z., Chang, B., and
Wei, F. Knowledge neurons in pretrained transform-
ers. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume
1: Long Papers) , pp. 8493–8502, Dublin, Ireland,
May 2022. Association for Computational Linguistics.
doi: 10 .18653/v1/2022 .acl-long .581. URL https:
//aclanthology .org/2022 .acl-long .581.
Dale, D., V oita, E., Barrault, L., and Costa-juss `a, M. R.
Detecting and mitigating hallucinations in machine trans-
lation: Model internal workings alone do well, sentence
similarity Even better. In Rogers, A., Boyd-Graber, J.,
and Okazaki, N. (eds.), Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pp. 36–50, Toronto, Canada,
July 2023a. Association for Computational Linguistics.
doi: 10 .18653/v1/2023 .acl-long .3. URL https://
aclanthology .org/2023 .acl-long .3.
Dale, D., V oita, E., Lam, J., Hansanti, P., Ropers, C.,
Kalbassi, E., Gao, C., Barrault, L., and Costa-juss `a,
M. R. Halomi: A manually annotated benchmark for
multilingual hallucination and omission detection in ma-
chine translation. In Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Processing ,
Singapore, December 2023b. Association for Computa-
10Information Flow Routes: Automatically Interpreting Language Models at Scale
tional Linguistics. URL https://arxiv .org/pdf/
2305 .11746 .pdf.
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph,
N., Mann, B., Askell, A., Bai, Y ., Chen, A., Conerly, T.,
DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds,
Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L.,
Ndousse, K., Amodei, D., Brown, T., Clark, J., Ka-
plan, J., McCandlish, S., and Olah, C. A mathematical
framework for transformer circuits. Transformer Cir-
cuits Thread , 2021. URL https://transformer-
circuits .pub/2021/framework/index .html .
Ferrando, J., G ´allego, G. I., and Costa-juss `a, M. R. Measur-
ing the mixing of contextual information in the trans-
former. In Goldberg, Y ., Kozareva, Z., and Zhang,
Y . (eds.), Proceedings of the 2022 Conference on Em-
pirical Methods in Natural Language Processing , pp.
8698–8714, Abu Dhabi, United Arab Emirates, Decem-
ber 2022. Association for Computational Linguistics.
doi: 10 .18653/v1/2022 .emnlp-main .595. URL https:
//aclanthology .org/2022 .emnlp-main .595.
Geiger, A., Richardson, K., and Potts, C. Neural nat-
ural language inference models partially embed theo-
ries of lexical entailment and negation. In Alishahi,
A., Belinkov, Y ., Chrupała, G., Hupkes, D., Pinter, Y .,
and Sajjad, H. (eds.), Proceedings of the Third Black-
boxNLP Workshop on Analyzing and Interpreting Neu-
ral Networks for NLP , pp. 163–173, Online, November
2020. Association for Computational Linguistics. doi:
10.18653/v1/2020 .blackboxnlp-1 .16. URL https://
aclanthology .org/2020 .blackboxnlp-1 .16.
Geva, M., Schuster, R., Berant, J., and Levy, O. Trans-
former feed-forward layers are key-value memories. In
Proceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pp. 5484–5495,
Online and Punta Cana, Dominican Republic, Novem-
ber 2021. Association for Computational Linguistics.
doi: 10 .18653/v1/2021 .emnlp-main .446. URL https:
//aclanthology .org/2021 .emnlp-main .446.
Goyal, N., Gao, C., Chaudhary, V ., Chen, P.-J., Wenzek, G.,
Ju, D., Krishnan, S., Ranzato, M., Guzm ´an, F., and Fan, A.
The Flores-101 evaluation benchmark for low-resource
and multilingual machine translation. Transactions of
the Association for Computational Linguistics , 10:522–
538, 2022. doi: 10 .1162/tacl a00474. URL https:
//aclanthology .org/2022 .tacl-1 .30.
Guerreiro, N. M., Alves, D., Waldendorf, J., Haddow, B.,
Birch, A., Colombo, P., and Martins, A. F. T. Hallucina-
tions in large multilingual translation models, 2023.
Hanna, M., Liu, O., and Variengien, A. How does GPT-2
compute greater-than?: Interpreting mathematical abili-ties in a pre-trained language model. In Oh, A., Neumann,
T., Globerson, A., Saenko, K., Hardt, M., and Levine, S.
(eds.), Advances in Neural Information Processing Sys-
tems, volume 36, pp. 76033–76060. Curran Associates,
Inc., 2023. URL https://papers .nips .cc/
paper files/paper/2023/hash/
efbba7719cc5172d175240f24be11280-
Abstract-Conference .html .
Hanna, M., Pezzelle, S., and Belinkov, Y . Have faith in
faithfulness: Going beyond circuit overlap when finding
model mechanisms, 2024.
Heimersheim, S. and Janiak, J. A circuit for python
docstrings in a 4-layer attention-only transformer,
2023. URL https://www .lesswrong .com/
posts/u6KXXmKFbXfWzoAXn/a-circuit-
for-python-docstrings-in-a-4-layer-
attention-only .
Hendel, R., Geva, M., and Globerson, A. In-context learning
creates task vectors, 2023.
Kobayashi, G., Kuribayashi, T., Yokoi, S., and Inui, K.
Attention is not only a weight: Analyzing transform-
ers with vector norms. In Webber, B., Cohn, T., He,
Y ., and Liu, Y . (eds.), Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pp. 7057–7075, Online, Novem-
ber 2020. Association for Computational Linguistics.
doi: 10 .18653/v1/2020 .emnlp-main .574. URL https:
//aclanthology .org/2020 .emnlp-main .574.
Kram ´ar, J., Lieberum, T., Shah, R., and Nanda, N. Atp*: An
efficient and scalable method for localizing llm behaviour
to components, 2024.
McGrath, T., Rahtz, M., Kramar, J., Mikulik, V ., and Legg,
S. The hydra effect: Emergent self-repair in language
model computations, 2023.
Meng, K., Bau, D., Andonian, A., and Belinkov, Y . Locating
and editing factual associations in GPT. Advances in
Neural Information Processing Systems , 36, 2022.
Millidge, B. and Black, S. The singular value
decompositions of transformer weight ma-
trices are highly interpretable, 2022. URL
https://www .lesswrong .com/posts/
mkbGjzxD8d8XqKHzA/the-singular-value-
decompositions-of-transformer-weight .
Molina, R. Traveling words: A geometric interpretation of
transformers, 2023.
NLLB, T., Costa-juss `a, M. R., Cross, J., C ¸elebi, O., Elbayad,
M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J.,
Licht, D., Maillard, J., Sun, A., Wang, S., Wenzek, G.,
11Information Flow Routes: Automatically Interpreting Language Models at Scale
Youngblood, A., Akula, B., Barrault, L., Gonzalez, G. M.,
Hansanti, P., Hoffman, J., Jarrett, S., Sadagopan, K. R.,
Rowe, D., Spruit, S., Tran, C., Andrews, P., Ayan, N. F.,
Bhosale, S., Edunov, S., Fan, A., Gao, C., Goswami,
V ., Guzm ´an, F., Koehn, P., Mourachko, A., Ropers, C.,
Saleem, S., Schwenk, H., and Wang, J. No language
left behind: Scaling human-centered machine translation,
2022.
Pearl, J. Causality . Cambridge University Press, 2 edition,
2009. doi: 10 .1017/CBO9780511803161.
Petrov, S., Das, D., and McDonald, R. A universal
part-of-speech tagset. In Calzolari, N., Choukri, K.,
Declerck, T., Do ˘gan, M. U., Maegaard, B., Mariani,
J., Moreno, A., Odijk, J., and Piperidis, S. (eds.),
Proceedings of the Eighth International Conference
on Language Resources and Evaluation (LREC’12) ,
pp. 2089–2096, Istanbul, Turkey, May 2012. European
Language Resources Association (ELRA). URL
http://www .lrec-conf .org/proceedings/
lrec2012/pdf/274 Paper .pdf.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res. , 21(1), jan 2020. ISSN
1532-4435.
Rushing, C. and Nanda, N. Explorations of self-repair in
language models, 2024.
Stolfo, A., Belinkov, Y ., and Sachan, M. Understanding
arithmetic reasoning in language models using causal
mediation analysis, 2023.
Syed, A., Rager, C., and Conmy, A. Attribution patching
outperforms automated circuit discovery, 2023.
Todd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace,
B. C., and Bau, D. Function vectors in large language
models, 2023.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E.,
Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-
ple, G. Llama: Open and efficient foundation language
models, 2023a.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,
M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,
Fuller, B., Gao, C., Goswami, V ., Goyal, N., Hartshorn,
A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,
V ., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,
Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y .,Mao, Y ., Martinet, X., Mihaylov, T., Mishra, P., Molybog,
I., Nie, Y ., Poulton, A., Reizenstein, J., Rungta, R., Saladi,
K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,
Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
Xu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan, A., Kambadur,
M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,
and Scialom, T. Llama 2: Open foundation and fine-tuned
chat models, 2023b.
van der Maaten, L. and Hinton, G. Visualizing data using t-
sne. Journal of Machine Learning Research , 9(86):2579–
2605, 2008. URL http://jmlr .org/papers/v9/
vandermaaten08a .html .
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I.
Attention is all you need. In Guyon, I., Luxburg, U. V .,
Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,
and Garnett, R. (eds.), Advances in Neural Information
Processing Systems , volume 30. Curran Associates, Inc.,
2017. URL https://proceedings .neurips .cc/
paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-
Paper .pdf.
Vig, J., Gehrmann, S., Belinkov, Y ., Qian, S., Nevo,
D., Singer, Y ., and Shieber, S. Investigating gender
bias in language models using causal mediation
analysis. In Larochelle, H., Ranzato, M., Hadsell,
R., Balcan, M., and Lin, H. (eds.), Advances in
Neural Information Processing Systems , volume 33,
pp. 12388–12401. Curran Associates, Inc., 2020.
URL https://proceedings .neurips .cc/
paper files/paper/2020/file/
92650b2e92217715fe312e6fa7b90d82-
Paper .pdf.
V oita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I.
Analyzing multi-head self-attention: Specialized heads
do the heavy lifting, the rest can be pruned. In Ko-
rhonen, A., Traum, D., and M `arquez, L. (eds.), Pro-
ceedings of the 57th Annual Meeting of the Associ-
ation for Computational Linguistics , pp. 5797–5808,
Florence, Italy, July 2019. Association for Computa-
tional Linguistics. doi: 10 .18653/v1/P19-1580. URL
https://aclanthology .org/P19-1580 .
V oita, E., Ferrando, J., and Nalmpantis, C. Neurons in large
language models: Dead, n-gram, positional, 2023.
Wang, K. R., Variengien, A., Conmy, A., Shlegeris, B.,
and Steinhardt, J. Interpretability in the wild: a circuit
for indirect object identification in GPT-2 small. In The
Eleventh International Conference on Learning Repre-
sentations , 2023. URL https://openreview .net/
forum?id=NpsVSN6o4ul .
12Information Flow Routes: Automatically Interpreting Language Models at Scale
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., Mi-
haylov, T., Ott, M., Shleifer, S., Shuster, K., Simig,
D., Koura, P. S., Sridhar, A., Wang, T., and Zettle-
moyer, L. Opt: Open pre-trained transformer language
models, 2022. URL https://arxiv .org/abs/
2205 .01068 .
13Information Flow Routes: Automatically Interpreting Language Models at Scale
A. Background on Activation Patching
Activation patching (Vig et al., 2020; Meng et al., 2022; Geiger et al., 2020; Wang et al., 2023) refers to intervening some
internal activation (intermediate representation) computed by a model component c(attention head, feedforward network) in
the forward pass ( fc(x)) with ‘base’ input x. The patched activation is taken from a forward pass fc(˜x)on a ‘source’ input
˜x. We can express this intervention using the do-operator (Pearl, 2009) as f(x|do(fc(x) =fc(˜x))). Upon intervention,
the forward pass continues and the model output is compared with the prediction with the ‘base’ input, e.g. by measuring
f(x)−f(x|do(fc(x) =fc(˜x))).
Identifying subnetworks (circuits) through activation patching has several shortcomings:
•It requires large human efforts to create the input base templates ( x) and contrastive source examples ( ˜x) for the
specific task to study, thus results vary depending on the choice of the contrastive template. Additionally, analyses are
constrained to those templates, preventing from studying models on more general types of predictions.
•For each prediction, one needs to patch every edge (or node) in the computational graph, which becomes impractical
when studying large language models.
•It has been shown that downstream components can compensate for the ablation as a form of self-repair (McGrath
et al., 2023; Rushing & Nanda, 2024), which interferes with the analysis.
In contrast, our method doesn’t require specific templates with contrastive examples which allows us to study specific tasks
and more general behaviors, computes the information flow routes graph in a single forward pass, and it’s not affected by
self-repair issues, since we make no interventions.
B. Details about the Information Flow Routes
B.1. Linearizing the Layer Normalization
Given an input representation x, the layernorm computes
LN(x) =x−µ(x)
σ(x)⊙γ+β (6)
withµandσobtaining the mean and standard deviation, and γ∈Rdandβ∈Rdrefer to learned element-wise transformation
and bias respectively. Considering σ(x)as a constant, LN can be treated as a constant affine transformation:
LN(x) =xL+β (7)
where L∈Rd×drepresents a matrix that combines centering, normalizing, and scaling operations together.
L:=1
σ(x)
γ10··· 0
0γ2··· 0
··· ··· ··· ···
0 0 ··· γn

n−1
n−1
n··· −1
n
−1
nn−1
n··· −1
n
··· ··· ··· ···
−1
n−1
n···n−1
n

The linear map on the right subtracts the mean to the input vector, x′=x−µ(x). The left matrix performs the hadamard
product with the layer normalization weights ( x′⊙γ).
B.2. Subword Merging Head definition
Formally, for a subword merging head (i) in at least 70 %of the cases, later subwords put more than half of their influence
on previous subwords of the same word, (ii) in at least 70 %of the cases, for the first subword this head’s overall influence is
no more than 0.005% .
14Information Flow Routes: Automatically Interpreting Language Models at Scale
C. Examples of Routes
Figures 13 and 14 show the information flow routes for IOI and greater-than tasks extracted for GPT2-small and OPT-125m.
Figure 13. The important information flow routes, IOI task, τ= 0.04.
Figure 14. The important information flow routes, greater-than task, τ= 0.04.
15Information Flow Routes: Automatically Interpreting Language Models at Scale
D. Period Acting as BOS
Figure 15 shows an example of an attention map for one of the heads in Llama 2-7b. We see that after the first period,
attention is spread between the BOS token and this period.
Figure 15. Llama 2 7B attention weights matrix L18H3.
16