World to Code: Multi-modal Data Generation via Self-Instructed
Compositional Captioning and Filtering
Jiacong Wang1,2*, Bohong Wu2*, Haiyong Jiang1, Xun Zhou2,
Xin Xiao2,Haoyuan Guo2,Jun Xiao1†
1School of Artificial Intelligence, University of Chinese Academy of Sciences
2ByteDance Inc
wangjiacong20@mails.ucas.ac.cn ,{haiyong.jiang,xiaojun}@ucas.ac.cn
{bohongwu,guohaoyuan,xiaoxin.ddl}@bytedance.com
Abstract
Recent advances in Vision-Language Models
(VLMs) and the scarcity of high-quality multi-
modal alignment data have inspired numer-
ous researches on synthetic VLM data gener-
ation. The conventional norm in VLM data
construction uses a mixture of specialists in
caption and OCR, or stronger VLM APIs and
expensive human annotation. In this paper, we
present World to Code ( W2C ), a meticulously
curated multi-modal data construction pipeline
that organizes the final generation output into a
Python code format. The pipeline leverages the
VLM itself to extract cross-modal information
via different prompts and filter the generated
outputs again via a consistency filtering strat-
egy. Experiments have demonstrated the high
quality of W2C by improving various existing
visual question answering and visual grounding
benchmarks across different VLMs. Further
analysis also demonstrates that the new code
parsing ability of VLMs presents better cross-
modal equivalence than the commonly used
detail caption ability. Our code is available
at https://github.com/foundation-multimodal-
models/World2Code.
1 Introduction
Fueled by the rapid development of Vision-
Language Models (VLMs) (Zhu et al., 2023;
Liu et al., 2024b; Team et al., 2023; Liu et al.,
2024a; Dong et al., 2024b) and Diffusion Models
(DMs) (Betker et al., 2023), collecting detailed and
concrete high-quality captions for each image be-
comes more and more urging. However, expensive
and tedious human labeling for high-quality image-
text pairs further incurs the necessity of a cheap and
reliable data construction pipeline without human
intervention.
Related works on image-text data curation can
be divided into two main streams. Distillation-
*These authors contributed equally to this work.
†Corresponding author.based methods leverage closed-source commer-
cial products (e.g., GPT-4V (Achiam et al., 2023))
with the state-of-the-art performance for image cap-
tion (Chen et al., 2023a; Li et al., 2024b; Chen
et al., 2024a). Another line of work curates an
image caption pipeline with existing VLMs to fil-
ter high-quality image-text for the training of bet-
ter VLMs. These methods usually combine open-
source LLMs (Touvron et al., 2023a,b; Chiang
et al., 2023) and different visual specialists (Li
et al., 2023a; Huang et al., 2023b; Zong et al., 2023;
Zhang et al., 2024a; Fang et al., 2023; Minderer
et al., 2022; Ren et al., 2024; Zhang et al., 2023b)
to endow existing VLMs with new abilities, e.g.,
pixel grounding in GLaMM (Rasheed et al., 2023).
However, the dependency on a mixture of special-
ists and human feedback in filtering noisy genera-
tions (Wang et al., 2023b) makes it difficult to scale
the generated data and automate the process.
Recent progress shows that generated results of
LLMs (Wang et al., 2022; Li et al., 2023c) and
VLMs (Zhang et al., 2024b) for prompts with simi-
lar meanings should be alike and we can help filter
out noisy generated texts and captions by consis-
tency checking among multiple prompt instructed
results. In light of the above evidence, we present
a self-instructed data construction pipeline, coined
W2C .W2C autonomously extracts and articulates
specific content from images, and enhances the
reliability of the generated image captions by em-
ploying consistency filtering by assessing the out-
puts through multiple instructed prompt consisten-
cies. The overall pipeline reduces requested spe-
cialists and frees off expensive human feedback as
shown in Figure 1. In addition, we leverage the
idea from human-machine interaction and organize
the model-generated responses into a Python code
format, following Eureka (Ma et al., 2023) and
Text2Reward (Xie et al., 2023). Experiments have
shown that our proposed W2C can improve VLMs
on various visual question-answering benchmarks.
1arXiv:2409.20424v1  [cs.CV]  30 Sep 2024Figure 1: Overview of W2C and comparison of existing data construction pipelines. W2C differs from existing
works by reducing the need for a mixture of specialists and expensive human annotations via self-instruct.
To be specific, W2C performs the best in 7 out of 9
VQA benchmarks on LLaV A-NeXT-7B, and 6 out
of 9 VQA benchmarks on LLaV A-NeXT-13B. Fur-
thermore, W2C also improves few-shot evaluations
on two widely used VQA benchmarks including
GQA and MME. Especially, on the 2-shot evalua-
tion of GQA, the method achieves over 5 accuracy
gains across different VLMs.
Our contribution is summarized in threefold:
•We present the data pipeline of W2C , which
proposes to generate and filter data all by ex-
isting VLMs themselves via self-instruct, sig-
nificantly reducing the need for a mixture of
specialists or expensive human annotations in
conventional pipelines.
•The generated data of W2C presents compa-
rable better performance on classical VQA
benchmarks and consistently better perfor-
mance on visual grounding benchmarks than
ShareGPT4V .
•Further analysis presents that the new code
parsing ability displays better cross-modality
equivalence than the commonly used detail
caption ability in presenting the details of an
image.
2 Related Work
Vision Language Models With the emergence
of LLMs (OpenAI, 2023; Achiam et al., 2023; Tou-
vron et al., 2023a; Team et al., 2023; Jiang et al.,
2024), VLMs (Zhu et al., 2023; Zhang et al., 2023a;
Team et al., 2023) have demonstrated exceptional
capabilities in visual recognition and understand-
ing, achieving remarkable results on various VLM
benchmarks (Singh et al., 2019; Tito et al., 2021;Zhang et al., 2024b; Liu et al., 2023b; Ying et al.,
2024; Fu et al., 2024). The seminal BLIP2 (Li
et al., 2023a) firstly introduces Q-Former to adapt
encoded image features as potential language to-
kens for LLM-based caption prediction. Following
works (Liu et al., 2024a; Team et al., 2023; Dong
et al., 2024c) improve the visual component by re-
placing VIT (Dosovitskiy et al., 2020) or scaling
the input image resolution, while Zhu et al. (Zhu
et al., 2023) extends BLIP2 by employing emergent
open-source LLMs (Touvron et al., 2023a; Chiang
et al., 2023), endowing current VLMs with signif-
icantly better instruction following and problem
solving abilities. LLaV A/LLaV A-1.5 (Liu et al.,
2024b, 2023a) further remove Q-Former and point
out that simple MLP projection layers present im-
pressive performance in aligning image represen-
tation with LLMs. Some works also highlight the
importance of collecting high-quality cross-modal
alignment data for improving the consistently scal-
ing VLMs (Bai et al., 2023; Wang et al., 2023b; Li
et al., 2023b).
Multi-modal Dataset Construction The
scarcity of high-quality human-labeled data
inspires the synthesis of cross-modal data (Wang
et al., 2024; Chen et al., 2023a; Rasheed et al.,
2023; Wang et al., 2023a; Li et al., 2024b; Lu
et al., 2023; Dong et al., 2024a; Chen et al.,
2024c). Among them, Wang et al. (2023b)
propose the AS-1B data generation pipeline and
open-sourced high-quality dense captions on 1B
images. GLaMM (Rasheed et al., 2023) further
extends AS-1B by introducing about 10 specialists
of different functionalities including grounding,
tagging, and in-context learning. These specialists
enable pixel-wise grounded dense captions for
2each image. However, the expensive human
annotation required in AS-1B and the complicated
construction pipeline in GLaMM have greatly
limited the potential of data scaling. In this work,
we try to answer whether synthetic data can
improve VLMs on classical VQA benchmarks (Fu
et al., 2024; Ying et al., 2024; Chen et al., 2024b)
to avoid tedious data collection.
Recent progress in synthetic data generation for
LLMs (Huang et al., 2023a; Li et al., 2023c; Wang
et al., 2022, 2023c) shed light on the possibility of
Multi-modal data construction by leveraging con-
sistency in generation to filter invalid data. Wang
et al. (2022) presents the consistent reasoning path
generation demonstrating better performance in
COT. Li et al. (2023c) uses the generator-validator
consistent data for training and can effectively im-
prove LLMs on various tasks. Zhang et al. (2024b)
further shows that the generator-validator consis-
tency in most VLMs is prone to be correct.
Code Representation for Visual Tasks Code
representations can formally encode various struc-
ture information in a scene. Eureka (Ma
et al., 2023) and Text2Reward (Xie et al., 2023)
parse a scene into Python codes and encourage
LLMs to generate programmable dense rewards.
ViStruct (Chen et al., 2023b) takes the first step in
visual code intelligence by decomposing the code-
visual representation into multiple components in-
cluding object recognition, object grounding, at-
tribute detection, relation detection, and event de-
tection. Chen et al. (2023b) further introduces a
curriculum learning approach to endow VLMs with
the aforementioned four abilities. However, the
heavy dependency on supervised human-labeled
datasets and the complicated curriculum learning
pipeline limits its potential. This work investigates
an effective data-constructing pipeline based on
code-vision representation.
3 Method
Our data construction pipeline shares some similar-
ities with GLaMM (Rasheed et al., 2023), where
both methods focus on the region-level caption of
the whole image. W2C further extend GLaMM to
support generation-validation consistency filtering
by exploring different organization formations of
the labeled elements and present how VLMs boost
themselves on basic multi-modal understanding
tasks.
To make a comprehensive and systematic exposi-tion of our W2C entire pipeline, the following will
be divided into three parts for discussion:
(1) Visual Concepts Extraction in Section 3.1,
(2) Self-Instructed Information Extraction in Sec-
tion 3.2, (3) Information Filtering via Self Con-
sistency in Section 3.3, (4) Structured formatting
in Section 3.4. The overview of our construction
pipeline is shown in Figure 2 and all the used in-
struct prompts are shown in Appendix A.1.
3.1 Visual Concepts Extraction
To build a fully covered concept list for each image
Iin images dataset Draw, we prompt VLMs to gen-
erate both general captions (for a concise overview
of the image) and detail captions (to bootstrap as
many visual concepts as possible in the caption)
using specific instruct prompts pgandpd. We use
beam search to encourage the VLMs to provide as
many visual concepts as possible to improve gen-
eration diversity. The general captions and detail
captions are obtained as follows:
og, od=fVLM(I, pg), fVLM(I, pd). (1)
Since visual concepts are mainly composed of noun
phrases, we employ the NLTK toolkit (Bird, 2006)
to extract all noun phrases denoted as Nfromog
andod.
We use Grounding DINO to map extracted noun
phrases to bounding boxes of the current image,
where part of the false positive noun phrases are fil-
tered as they fail to be mapped with corresponding
areas in the image. Here we denote the filtered vi-
sual concepts as C, and their corresponding bound-
ing boxes as B, which is formulated as follows:
B,C=fDINO(I,N). (2)
3.2 Self-Instructed Information Extraction
After obtaining visual concepts, we extract region-
level captions and OCR information for cropped
images of each concept bounding box, respectively.
Region-level Captions We crop image Ifor each
visual concept ci∈Cwith its corresponding
bounding box bi∈Bto obtain a detailed cap-
tion and prompt the VLMs to provide a general
caption centered on ci. Additionally, to encourage
the VLMs to offer more concrete details about the
properties of ci, we instruct the VLMs to include
the color and material of ciin the caption. Denote
the description prompt for the region-level caption
aspdesc(ci)and the image cropped by biasI(bi).
3Figure 2: The data construction pipeline for W2C . Our pipeline utilizes both VLM and an object detector model
to furnish structured data with region-specific awareness, detailed entity captions, and comprehensive global
information. The VLM is iteratively invoked to generate the caption and perform consistency filtering to obtain
high-quality data. The visual concepts set is obtained from the captions by the NLTK toolkit, cihere represents a
visual concept from the set. The instruction prompts are all predefined templates.
The region-level caption for each visual concept ci
is formulated as:
odesc(ci) =fVLM(pdesc(ci), I(bi)) (3)
OCR information Previous methods mainly use
OCR tools (PaddleOCR, 2023; JaidedAI, 2023) to
enhance the OCR capabilities. In contrast, W2C ac-
quire the OCR information via an instructed prompt
to guide VLMs for existing VLMs have the better
capability in reading text in complex natural sce-
narios. Given the OCR instruct prompt pocr(ci),
the OCR information in each cropped image by
bounding box area biwith concept ciis formulated
as follows:
oocr(ci) =fVLM(pocr(ci), I(bi)) (4)
3.3 Information Filtering via Self Consistency
Our consistency filtering strategy is inspired by the
similar generator-validator consistency findings in
ConBench (Zhang et al., 2024b), where different in-
struct prompts may lead to in-consistent captions of
visual concepts, and the highly consistent genera-
tions are prone to be correct ones. In this paper, we
propose to filter the visual concepts via generation-
validation consistency, where we change the region-
level captions into multiple visual question answer-
ing problems for both counting filtering and caption
reranking.
Counting Filtering via Consistency Different
from AS-1B, we introduce Grounding DINO inour construction process, which can naturally filter
part of the plausible visual concepts as these con-
cepts usually fail to find corresponding bounding
boxes in the image. However, Grounding DINO
introduces new challenges for counting problems,
as visual concepts cimight be mapped to multiple
boxes that have a large overlap due to inappropri-
ately designed hyper-parameters. To prevent the
effect by plausibly mapped (bi, ci), we group all
thecithat have the same name into ˜Cand calculate
the existing times for each ˜ci∈˜Casni. We then
merge all the boxes for each ˜ci(which might con-
tain multiple visual concepts with the same name)
into˜B, for a box ˜bi∈˜Bwe crop the image and
prompt the VLMs to check whether the group el-
ement ˜ciexistnitimes in the image via instruct
prompt p˜ci
valid-g:
ovalid-g (˜ci) =fVLM(pvalid-g (˜ci, ni), I(˜bi)).(5)
Caption Re-ranking via Consistency To pro-
vide better region-level captions for a given visual
concept, we use beam search to bootstrap mul-
tiple caption candidates. To select the best can-
didate, we again leverage the generator-validator
consistency. Specifically, for each given visual
concept ci, we get a list of caption candidate
[o1
desc(ci), o2
desc(ci), ..., ob
desc(ci)]. We use NLTK to
parse these captions and collect all the visual con-
cepts that are contained in these captions. Taking n
as the total number of extracted concepts in the cap-
tions of ci, we get a new visual concept list denoted
4Algorithm 1 Data Construction and Consistency
Filtering Pipeline
Input: Image Ifrom dataset Draw, Instruct Prompts: pg,pd,
pdesc,pocr,pvalid-g ,pvalid-c, VLM fVLM, Grounding DINO
fDINO.
1: Global Caption Generate.
og, od=fVLM(I, pg), fVLM(I, pd)
2: Visual Concepts Extraction.
N=NLTK (og, od),B,C=fDINO(I,N)
3: Region-level Captions Generate.( cifromC,bifromB)
odesc(ci) =fVLM(pdesc(ci), I(bi))
4: OCR information Extraction.
oocr(ci) =fVLM(pocr(ci), I(bi))
5: Grouping Concepts and Boxes in CandB.
˜ci∈˜C,˜bi∈˜B
6: Counting Filtering via Consistency. ( ck
i∈Crank)
ovalid-g(˜ci) =fVLM(pvalid-g(˜ci, ni), I(˜bi))
7: Caption Re-ranking via Consistency.
ovalid-c(ck
i) =fVLM(pvalid-c(ck
i), I(˜bi))
8:Rule-based Structured Formatting and Counting Filtering
to get DW2C.
Output: W2C dataset DW2C
asck
i∈Crank. Following Equation 5, we prompt
VLMs to check the existence of each extracted vi-
sual concept ck
ivia instruct prompt pvalid-c(ck
i):
ovalid-c(ck
i) =fVLM(pvalid-c(ck
i), I(˜bi)) (6)
We then manually design a scoring mechanism
based on the validation result ovalid-c(ck
i). Specif-
ically, for each caption that contains multiple ex-
tracted visual concepts, we assign each correct vi-
sual concept ovalid-c(ck
i) = "Yes" to score 1 and
each hallucinated visual concept ovalid-c(ck
i) =
"No" to -1. By accumulating the scores in each cap-
tion, we select the caption with the highest score in
one beam as the final caption odesc(ci)for the given
visual concept ci, which is supposed to be the most
diverse and correct caption.
3.4 Structured Formatting and Filtering
As shown in Figure 2, we organize the structured
information into code format to fully represent the
region-level information of an image. Inspired by
Eureka (Ma et al., 2023) and Text2Reward (Xie
et al., 2023), we organize the information as a struc-
tured representation into the Python format due to
its generality and conciseness. The organization is
achieved by the following three rules.
•One general caption ogof the whole image as
the comments of each image Class.
•Each visual concept is an attribute for the im-
age class. For each visual concept ci, weget their corresponding bounding box bi, cap-
tionodesc(ci), and OCR information oocr(ci).
Such visual concept is then organized as
{caption: odesc(ci),text:oocr(ci),bbox:bi}.
•Grouping visual concepts with the same name.
To make the representation code more concise,
we group the visual concepts with the same
name in a list ˜ci′= [c1
i, c2
i, ...].
By integrating these rules, we get the final code
representation of each image, which is then fol-
lowed by the rule-based filtering strategy that filters
out counting in-consistent samples.
In conclusion, by denoting the final dataset as
DW2C, the whole data construction pipeline is de-
picted in Algorithm 1.
4 Experiments
4.1 Experimental Setup
Datasets For the data construction pipeline, we
strictly use the images in the ShareGPT4V dataset
for our self-instructed approach validation in a
fair comparison. Since the original ShareGPT4V
dataset contains duplicate images, We remove the
repeated images in the original 102K data and get
about 87K original images. We follow the practice
of LLaV A-1.5 (Liu et al., 2023a) to adopt a two-
stage training approach consisting of prompt tuning
(PT) and instruct tuning (IT). For the experiments
on low resolution setting, we follow the LLaV A-
1.5 to use training dataset LLaV A 558kfor PT stage
andLLaV A 665kfor IT stage on LLaV A-1.5 training
stages. As the specific mixture ratio details of the
LLaV A-NeXT data were omitted, we directly uti-
lized the entire training set from each of the follow-
ing datasets in the IT stage, forming a mixture of
datasets including: LLaV A 665k(Liu et al., 2023a),
DocVQA (Tito et al., 2021), ChartQA (Masry et al.,
2022) and ShareGPT4V (Chen et al., 2023a) on
high resolution setting.
To comprehensively assess the effectiveness of
our constructed dataset, we evaluate the model
on widely adopted multi-modal benchmarks and
grouding benchmarks, including TextVQA (Singh
et al., 2019) (without providing OCR tokens),
DocVQA (Tito et al., 2021), ChartQA (Masry
et al., 2022), MME (Fu et al., 2024), MMT
Bench (Ying et al., 2024), MMStar (Chen et al.,
2024b), ScienceQA (Lu et al., 2022), POPE (Li
et al., 2023d), GQA (Hudson and Manning,
5Method GQA MME. POPE SQAIMMS. MMT. Text. Doc. Chart.
Low resolution setting
LLaV A-1.5-7B∗62.3 1468 86.2 68.2 32.4 48.6 47.6 - -
+ShareGPT4V 63.4 1507 86.0 69.0 34.3 49.3 47.9 - -
+W2C 62.8 1503 85.6 69.8 33.5 49.4 46.6 - -
LLaV A-1.5-13B∗63.7 1574 85.7 72.1 33.5 51.1 49.0 - -
+ShareGPT4V 64.0 1537 86.1 72.0 33.9 50.9 48.8 - -
+W2C 64.0 1547 85.7 72.6 36.1 51.7 48.9 - -
High resolution setting
LLaV A-NeXT-7B 64.2 1473 87.3 67.9 34.6 48.2 63.9 75.4 62.0
+ShareGPT4V∗64.0 1513 85.8 68.5 33.7 49.5 64.2 75.1 62.2
+W2C 64.2 1516 87.5 68.3 35.8 50.1 63.7 76.5 63.0
LLaV A-NeXT-13B 65.3 1545 87.1 70.1 37.2 50.6 67.6 78.1 66.2
+ShareGPT4V∗65.3 1574 87.1 70.1 37.5 50.4 67.0 78.4 63.8
+W2C 65.5 1597 87.5 70.7 37.1 51.4 65.2 79.1 65.6
Table 1: Visual Question Answering benchmarks of W2C on LLaV A1.5 and LLaV A-NeXT under different
combination of IT datasets. The best results are bold and the second results are underlined .∗: our reproduction of
LLaV A-1.5 and LLaV A-Next, which achieves comparable performance with the original papers. −: LLaV A-1.5 does
not support benchmarks that requires high input resolution. Abbreviations: SQAI(ScienceQA), MMS.(MMStar),
MMT.(MMT-Bench), Text.(TextVQA), Doc.(DocVQA), Chart.(ChartQA).
2019), RefCOCO (Kazemzadeh et al., 2014), Ref-
COCO+ (Mao et al., 2016) and RefCOCOg (Mao
et al., 2016). These benchmarks provide a com-
prehensive assessment of multiple perspectives on
multi-modal VLM performance.
Implementation Details In this paper, we em-
ploy two types of leading methods: LLaV A-
1.5 (Liu et al., 2023a) uses a CLIP-pretrained ViT-
L/14 (Radford et al., 2021) as a vision encoder,
a projector and an LLM, and LLaV A-NeXT (Liu
et al., 2024a) increases the input image resolution
by applying an adaptive image cropping strategy to
concatenate all vision tokens. To ensure a fair and
comprehensive comparison Table 1 and Table 2
present results both excluding and including the
ShareGPT4V dataset, as well as results from the
incorporation of our dataset. Table 3 We have re-
produced LLaV A-NeXT with a learning rate of ViT
to 1/10 of the base learning rate for the reason that
LLaV A-NeXT only publishes their evaluation code.
The learning rate for the PT stage is set to 1e−3
and the IT stage is set to 2e−5for both Vicuna-7B
and Vicuna-13B backbone LLM. We use 16 A100
for experiments on VLM training. We freeze the
vision encoder during training on the LLaV A-1.5
and only freeze the vision encoder on the PT stage
during training on the LLaV A-NEXT following the
original paper. We show more training details in
the Appendix C.1Data Processing Details During the data con-
struction pipeline, we employ NLTK (Bird, 2006)
tool to extract noun phrases from the captions, and
the resulting set of phrases is then post-processed
using WordNet (Miller, 1995) to remove duplicates
and filter out inaccurately named entities. The total
amount of final data after consistency filtering will
not be completely consistent for different VLMs
and we show the details in Appendix C.1. The
checkpoints of the VLM we used in our data pro-
cessing are the original checkpoints of the official
release. For LLaV A-1.5, which is not trained with
the ShareGPT4V dataset, LLaV A-NEXT is trained
with part of the ShareGPT4V dataset. The detailed
GPU hours can be found in Appendix C.2 and we
show the visualization of our W2C samples in Ap-
pendix C.3.
4.2 Main Results
Effectiveness of W2C data improve various
VLMs in Visual Question Answering bench-
marks We show a quantitative comparison re-
sults of the trained VLMs with and without the
ShareGPT4V dataset, as well as W2C for replace-
ment of the ShareGPT4V during the IT training
stage in Table 1. W2C consistently improves the
performance on different settings in both LLaV A-
1.5 and LLaV A-NeXT. Especially, in the high reso-
lution setting, our W2C presents impressive perfor-
mance improvement on multi-modal visual under-
standing benchmarks such as MMT Bench, MM-
6Method RefCOCO RefCOCO+ RefCOCOg
test-a test-b val test-a test-b val test val Avg.
Low resolution setting
LLaV A-1.5-7B 86.8 72.9 80.0 79.3 60.7 70.7 72.2 72.2 74.4
+ShareGPT4V 87.1 72.7 80.4 79.5 62.2 71.5 72.5 72.2 74.8
+W2C 88.0 75.3 81.7 81.5 63.1 73.9 75.2 75.2 76.3
LLaV A-1.5-13B 88.9 75.3 82.3 82.4 65.0 74.3 75.2 74.6 77.3
+ShareGPT4V 89.0 75.6 83.0 82.7 65.6 75.7 75.3 75.0 77.7
+W2C 89.6 77.6 84.1 85.0 67.2 77.3 76.8 76.8 79.3
High resolution setting
LLaV A-NeXT-7B 89.9 78.7 84.8 84.5 68.7 77.0 79.4 78.8 80.2
+ShareGPT4V 89.4 76.8 83.5 82.1 65.9 75.5 77.5 77.6 78.5
+W2C 90.9 81.3 86.4 85.8 70.5 79.5 80.7 80.5 82.0
LLaV A-NeXT-13B 91.7 81.9 86.3 86.2 71.2 79.5 80.9 80.8 82.3
+ShareGPT4V 91.5 80.8 86.5 86.0 71.1 79.6 79.6 79.8 81.9
+W2C 91.1 83.6 87.3 86.3 72.9 81.0 81.7 81.3 83.2
Table 2: Grounding benchmarks of W2C on LLaV A1.5 and LLaV A-NeXT under different combination of IT
datasets. The best results are bold and the second results are underlined .
Method DocVQA ChartQA POPE MMStar MMT-Bench RefCOCO valRefCOOC+ valRefCOCOg val
LLaV A-NeXT-7B∗75.4 62.0 87.3 34.6 48.2 84.8 77.0 78.8
+ShareGPT4V 75.1 62.2 85.8 33.8 49.5 83.5 77.3 77.6
+ALLaV A 76.5 63.0 87.3 33.2 50.4 85.5 77.0 79.9
+Monkey 76.4 62.5 87.5 35.7 49.6 85.0 77.4 78.2
+W2C 76.5 63.0 87.5 35.8 50.1 86.4 79.5 80.5
Table 3: Visual Question Answering benchmarks and Grouding benchmarks on LLaV A-NeXT-7B under more
combination of SOTA IT dataset methods. The best results are bold and the second results are underlined .∗: our
reproduction of LLaV A-Next, which achieves comparable performance with the original papers. To ensure a fair
comparison, we randomly selected an equal amount of corresponding data from each dataset for this analysis.
Star, and MME. Specifically, W2C can bring im-
provement in 7 out of 9 benchmarks on LLaV A-
NeXT-7B and 6 out of 9 on LLaV A-NeXT-13B.
Especially, on LLaV A-NeXT-13B, W2C improves
DocVQA by 0.7 ANLS, ChartQA by 1.8 accu-
racy, MMT Bench by 0.8 accuracy and MME by
23 points compared to the reproduction results of
LLaV A-NeXT. More benchmarks results are shown
in B.1.
W2C data show impressive performance on
Grounding benchmarks We present the perfor-
mance of the VLMs on Grounding benchmarks in
Table 2. The task of referential expression com-
prehension necessitates that the model accurately
identifies and localizes the object described. Our
models demonstrate their exceptional capability
for detailed image recognition and localization by
undergoing evaluation across various referential
expression comprehension benchmarks, including
RefCOCO, RefCOCO+, and RefCOCOg. Benefit
from the entity-enteric generation of local captions
and the presence of local bounding box informa-tion, our model achieved an average improvement
of 1.5/1.6 average IoU on LLaV A-1.5 7B/13B and
3.5/1.3 average IoU on LLaV A-NeXT-7B/13B.
Comparison results of more data generation
methods and W2C on LLaVA-NeXT-7B model
under different benchmarks. We show more
quantitative results on the LLaV A-NeXT-7B base-
line, employing more data generation methods
(ALLaV A and Monkey) that utilize the GPT API
for data annotation. To ensure a fair comparison,
we randomly selected an equal amount of corre-
sponding data from each dataset. We reported
on representative Visual Question Answering and
Grounding benchmarks and achieved the best out-
comes in 7 out of 8 benchmarks. W2C still gets
comparable results compared to ALLaV A and gets
better results on Grounding benchmarks.
4.3 Ablation Studies
Our results show advantageous performance in Ta-
ble 1 and Table 2, but our analysis of these results
shows the limitations of the base model’s OCR ca-
7Method format MMT-Bench DocVQA TextVQA RefCOCO valRefCOCO+ valRefCOCOgval
LLaV A-NeXT-7B single 49.2 75.4 63.8 85.4 78.5 79.5
LLaV A-NeXT-7B multi 48.8 72.0 61.4 82.4 73.8 76.8
LLaV A-NeXT-7B code 50.1 76.5 63.7 86.4 79.5 80.5
Table 4: Ablation study of W2C on using different data organization format. single/multi/code : constructed data are
organized in single-round conversations/multi-round conversations/python code format.
Method re-ranking counting MMT-Bench DocVQA TextVQA RefCOCO valRefCOCO+ valRefCOCOgval
LLaV A-NeXT-7B 50.3 75.5 62.7 86.6 79.0 79.7
LLaV A-NeXT-7B ✓ 49.4 76.3 63.4 86.1 78.5 80.4
LLaV A-NeXT-7B ✓ 49.4 75.3 63.2 86.5 79.2 79.7
LLaV A-NeXT-7B ✓ ✓ 50.1 76.5 63.7 86.4 79.5 80.5
Table 5: Ablation study of W2C when combined the different consistency filtering strategy. re-ranking : caption
re-ranking. counting : counting filtering.
MethodGQA MME
2-shot 4-shot 2-shot 4-shot
LLaVA-1.5-7B
detail caption 34.79 39.67 1136 1098
code parsing 41.06 43.40 1139 1169
LLaVA-1.5-13B
detail caption 34.00 40.87 1192 1170
code parsing 39.12 43.70 1199 1224
LLaVA-NeXT-7B
detail caption 34.89 40.70 1174 1105
code parsing 40.07 45.07 1154 1189
LLaVA-NeXT-13B
detail caption 31.63 40.07 1193 1127
code parsing 39.80 42.83 1151 1190
Table 6: Comparison between detail caption and code
parsing ability in few-shot evaluations on MME and
GQA without referring to the image.
pability on LLaV A-1.5. We proceed with further
ablation studies on LLaV A-Next-7B for the con-
straints on resources, which optimally demonstrate
the full benefits of our pipeline and consistency
filtering in a comprehensive manner.
Organizing data into the python code format
presents better performance We discussed in
Section 3.2 the strengths of choosing the code for-
mat for the representation of structured data. In
Table 4, we quantitatively compare our data format
with a single-round dialogue format and a multi-
round dialogue format. By using the python code
as data construction format, we observe improved
performance in both visual grounding benchmarks
and visual question answer benchmarks on LLaV A-
NeXT-7B. Especially, we improved the MMT-
Bench by 0.9/1.3 accuracy and DocVQA by 1.1/4.5ANLS compared to the single/multi data format.
Filtering introduces better downstream bench-
marks performance We show the ablation of
different consistency filtering choices in Table 5.
Similarly, the performance of LLaV A-NeXT-7B
on the both visual grounding benchmarks and vi-
sual question answering benchmarks highlights the
effectiveness and necessity of our consistency fil-
tering approaches. When two filtering strategies
are combined, we achieve the best performance
by improving DocVQA with 1.0 ANLS, TextVQA
with 1.0 accuracy, RefCOCO+ valwith 0.5 IOU and
RefCOCOg valwith 0.8 IOU. We also achieve com-
parable results on MMT-Bench and RefCOCO val
with little performance degradation.
4.4 Code Parsing Ability Evaluation
We further present better cross-modality equiva-
lence between image and text brought by the new
code parsing ability. An ideal caption of the im-
age should enable the ability to question without
referring to the image. Therefore, we compare the
quality of the code output and widely used detail
caption output in the ability to handle downstream
tasks via in-context learning on the same Large
Language Model.
Experimental Setting We conduct experiments
on both LLaV A-1.5-7B/13B and LLaV A-NeXT-
7B/13B on two widely used Visual Question An-
swering benchmarks, including GQA and the per-
ception subset of MME. Due to the support of
32k long context and satisfying performance in
the open-source community, we use Qwen-1.5-
14B (Bai et al., 2023; Team, 2024) as the problem-
solving LLM, and prompt it with few shot inputs.
8Each shot can be represented as a combination
of{description, question, answer }. For the detail
caption output, we use the models trained with both
the original dataset and the ShareGPT4V dataset to
improve their detail caption abilities. For the code
parsing output, we replace ShareGPT4V with our
proposed W2C dataset.
The code parsing ability of VLMs presents much
better few-shot performance. From Table 6, the
code parsing output shows significant improvement
when compared with using the detail caption out-
put. On the binary classification task for the visual
perception subset of MME, the code parsing abil-
ity achieves comparable or better performance in
various settings. On the free generation VQA task,
GQA, using the code parsing output can bring clear
accuracy gain across different model size and ar-
chitectures. Especially, on the 2-shot evaluation
of GQA on LLaV A-NEXT-13B, the code parsing
output by model trained with W2C achieves 8.2 ac-
curacy improvement compared to baseline, indicat-
ing that the code-parsing ability present improved
performance in presenting the details of one image.
More benchmarks results are shown in B.2.
5 Conclusion
This paper presents W2C , an enhanced data
construction pipeline that only leverages existing
VLMs themselves for detail and compositional
captions for an image, which is further organized
in Python code format. We present that existing
VLMs can improve themselves on the understand-
ing benchmarks in various scenarios, significantly
reducing the need for a mix of visual specialists
and heavy human annotations. Moreover, addi-
tional experiments show that the new code parsing
ability of VLMs presents better capability in fully
describing the image, with notable improvement in
the few-shot evaluation on downstream tasks when
the raw images are not provided. Our proposed
W2C not only enhances the original capabilities on
the widely used multi-modal understanding bench-
marks but also endows existing VLMs with detailed
and executable multi-modal parsing ability.
6 Limitation
Despite the advancements in improved multi-modal
understanding benchmarks and new code parsing
ability, W2C can be further improved in some as-
pects.•In this paper, we directly use the ShareGPT4V
dataset images for a fair comparison with
ShareGPT4V . However, it contains fewer
OCR-centric images, limiting the final perfor-
mance. Further investigation could be taken
in studying the performance of W2C on more
distribution of unlabeled datasets.
•The experiments are mainly conducted on
the SOTA open-source VLM structures, i.e.,
the LLaV A series which use MLP projectors
for multi-modal alignment. The effectiveness
ofW2C can be further investigated on other
VLM structures.
Given the promising performance of W2C on
evaluation benchmarks, we would like to explore
a more high-quality and diverse data generation
pipeline in future investigations.
Acknowledgments. This work is partially sup-
ported by the National Natural Science Founda-
tion of China (U21A20515, 62476262, 62102393,
62206263, 62271467, 2306297, 62306296),
Beijing Natural Science Foundation (4242053,
L242096), China Postdoctoral Science Founda-
tion (2022T150639) and the Fundamental Research
Funds for the Central Universities.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023. Qwen technical report. arXiv
preprint arXiv:2309.16609 .
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jian-
feng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,
Joyce Lee, Yufei Guo, et al. 2023. Improving image
generation with better captions. Computer Science.
https://cdn. openai. com/papers/dall-e-3. pdf , 2(3):8.
Steven Bird. 2006. Nltk: the natural language toolkit.
InProceedings of the COLING/ACL 2006 Interactive
Presentation Sessions , pages 69–72.
Guiming Hardy Chen, Shunian Chen, Ruifei Zhang,
Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong
Chen, Jianquan Li, Xiang Wan, and Benyou Wang.
2024a. Allava: Harnessing gpt4v-synthesized data
for a lite vision-language model. arXiv preprint
arXiv:2402.11684 .
9Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang
Zang, Zehui Chen, Haodong Duan, Jiaqi Wang,
Yu Qiao, Dahua Lin, et al. 2024b. Are we on the
right way for evaluating large vision-language mod-
els? arXiv preprint arXiv:2403.20330 .
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-
ghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. 2023a. Sharegpt4v: Improving large multi-
modal models with better captions. arXiv preprint
arXiv:2311.12793 .
Yangyi Chen, Xingyao Wang, Manling Li, Derek
Hoiem, and Heng Ji. 2023b. Vistruct: Visual struc-
tural knowledge extraction via curriculum guided
code-vision representation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 13342–13357.
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji,
and Quanquan Gu. 2024c. Self-play fine-tuning con-
verts weak language models to strong language mod-
els.arXiv preprint arXiv:2401.01335 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang,
Yuan Zhang, and Haoyuan Guo. 2024a. Bench-
marking and improving detail image caption. arXiv
preprint arXiv:2405.19092 .
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,
Bin Wang, Linke Ouyang, Xilin Wei, Songyang
Zhang, Haodong Duan, Maosong Cao, et al.
2024b. Internlm-xcomposer2: Mastering free-
form text-image composition and comprehension
in vision-language large model. arXiv preprint
arXiv:2401.16420 .
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang
Cao, Bin Wang, Linke Ouyang, Songyang Zhang,
Haodong Duan, Wenwei Zhang, Yining Li, et al.
2024c. Internlm-xcomposer2-4khd: A pioneer-
ing large vision-language model handling resolu-
tions from 336 pixels to 4k hd. arXiv preprint
arXiv:2404.06512 .
Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. 2020.
An image is worth 16x16 words: Transformers
for image recognition at scale. In International
Conference on Learning Representations .
Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang,
Xinlong Wang, and Yue Cao. 2023. Eva-02: A vi-
sual representation for neon genesis. arXiv preprint
arXiv:2303.11331 .Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,
Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.
2024. Mme: A comprehensive evaluation benchmark
for multimodal large language models. Preprint ,
arXiv:2306.13394.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,
Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.
Large language models can self-improve. arXiv
preprint arXiv:2210.11610 .
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,
Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023a.
Large language models can self-improve. In The
2023 Conference on Empirical Methods in Natural
Language Processing .
Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian,
Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo,
and Lei Zhang. 2023b. Tag2text: Guiding vision-
language model via image tagging. In The Twelfth
International Conference on Learning Representa-
tions .
Drew A Hudson and Christopher D Manning. 2019.
Gqa: A new dataset for real-world visual reasoning
and compositional question answering. In Proceed-
ings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 6700–6709.
JaidedAI. 2023. Easy-ocr [software]. https://github.
com/JaidedAI/EasyOCR.
Albert Q Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris Bam-
ford, Devendra Singh Chaplot, Diego de las Casas,
Emma Bou Hanna, Florian Bressand, et al. 2024.
Mixtral of experts. arXiv preprint arXiv:2401.04088 .
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara Berg. 2014. Referitgame: Referring to
objects in photographs of natural scenes. In Proceed-
ings of the 2014 conference on empirical methods in
natural language processing (EMNLP) , pages 787–
798.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023a. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. In International conference on ma-
chine learning , pages 19730–19742. PMLR.
Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi
Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,
Jingjing Xu, Xu Sun, et al. 2023b. M3it: A large-
scale dataset towards multi-modal multilingual in-
struction tuning. arXiv preprint arXiv:2306.04387 .
Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tat-
sunori Hashimoto, and Percy Liang. 2023c. Bench-
marking and improving generator-validator consis-
tency of language models. In The Twelfth Interna-
tional Conference on Learning Representations .
10Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng
Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and
Jiaya Jia. 2024a. Mini-gemini: Mining the potential
of multi-modality vision language models. arXiv
preprint arXiv:2403.18814 .
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,
Wayne Xin Zhao, and Ji-Rong Wen. 2023d. Eval-
uating object hallucination in large vision-language
models. arXiv preprint arXiv:2305.10355 .
Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo
Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and
Xiang Bai. 2024b. Monkey: Image resolution and
text label are important things for large multi-modal
models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages
26763–26773.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2023a. Improved baselines with visual instruc-
tion tuning. In NeurIPS 2023 Workshop on Instruc-
tion Tuning and Instruction Following .
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan
Zhang, Sheng Shen, and Yong Jae Lee. 2024a. Llava-
next: Improved reasoning, ocr, and world knowledge.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2024b. Visual instruction tuning. Advances in
neural information processing systems , 36.
Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu,
Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui
Chen, Chunyuan Li, Lianwen Jin, et al. 2023b. On
the hidden mystery of ocr in large multimodal models.
arXiv preprint arXiv:2305.07895 .
Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei
Wang, Fei Mi, Baojun Wang, Weichao Wang, Lifeng
Shang, and Qun Liu. 2023. Self: Language-driven
self-evolution for large language model. arXiv
preprint arXiv:2310.00533 .
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-
Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
Clark, and Ashwin Kalyan. 2022. Learn to explain:
Multimodal reasoning via thought chains for science
question answering. Advances in Neural Information
Processing Systems , 35:2507–2521.
Yecheng Jason Ma, William Liang, Guanzhi Wang, De-
An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke
Zhu, Linxi Fan, and Anima Anandkumar. 2023. Eu-
reka: Human-level reward design via coding large
language models. In The Twelfth International Con-
ference on Learning Representations .
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan L Yuille, and Kevin Murphy. 2016.
Generation and comprehension of unambiguous ob-
ject descriptions. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition ,
pages 11–20.Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,
and Enamul Hoque. 2022. Chartqa: A benchmark
for question answering about charts with visual and
logical reasoning. arXiv preprint arXiv:2203.10244 .
George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM , 38(11):39–41.
Matthias Minderer, Alexey Gritsenko, Austin Stone,
Maxim Neumann, Dirk Weissenborn, Alexey Doso-
vitskiy, Aravindh Mahendran, Anurag Arnab,
Mostafa Dehghani, Zhuoran Shen, et al. 2022. Sim-
ple open-vocabulary object detection. In European
Conference on Computer Vision , pages 728–755.
Springer.
OpenAI. 2023. Chatgpt. https://openai.com/blog/
chatgpt/.
Maxime Oquab, Timothée Darcet, Théo Moutakanni,
Huy V V o, Marc Szafraniec, Vasil Khalidov, Pierre
Fernandez, Daniel HAZIZA, Francisco Massa,
Alaaeldin El-Nouby, et al. 2023. Dinov2: Learning
robust visual features without supervision. Transac-
tions on Machine Learning Research .
PaddleOCR. 2023. Awesome multilingual ocr toolk-
its based on paddlepaddle. https://github.com/
PaddlePaddle/PaddleOCR.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In International confer-
ence on machine learning , pages 8748–8763. PMLR.
Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Ab-
delrahman Shaker, Salman Khan, Hisham Cholakkal,
Rao M Anwer, Erix Xing, Ming-Hsuan Yang, and
Fahad S Khan. 2023. Glamm: Pixel ground-
ing large multimodal model. arXiv preprint
arXiv:2311.03356 .
Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai
Zheng, Mu Li, Alexander J Smola, and Xu Sun. 2024.
Prompt pre-training with twenty-thousand classes for
open-vocabulary visual recognition. Advances in
Neural Information Processing Systems , 36.
Amanpreet Singh, Vivek Natarajan, Meet Shah,
Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,
and Marcus Rohrbach. 2019. Towards vqa models
that can read. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition ,
pages 8317–8326.
Gemini Team, Rohan Anil, Sebastian Borgeaud,
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Qwen Team. 2024. Introducing qwen1.5.
11Rubèn Tito, Dimosthenis Karatzas, and Ernest Val-
veny. 2021. Document collection visual question
answering. In Document Analysis and Recognition–
ICDAR 2021: 16th International Conference, Lau-
sanne, Switzerland, September 5–10, 2021, Proceed-
ings, Part II 16 , pages 778–792. Springer.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Teng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, Hao
Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan
Zhao, Ying Shan, et al. 2023a. Caption anything: In-
teractive image description with diverse multimodal
controls. arXiv preprint arXiv:2305.02677 .
Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li,
Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun
Li, Lewei Lu, Xizhou Zhu, et al. 2024. The all-seeing
project v2: Towards general relation comprehension
of the open world. arXiv preprint arXiv:2402.19474 .
Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang,
Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li,
Xizhou Zhu, Zhiguo Cao, et al. 2023b. The all-seeing
project: Towards panoptic visual recognition and un-
derstanding of the open world. In The Twelfth Inter-
national Conference on Learning Representations .
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,
Ed H Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. 2022. Self-consistency improves
chain of thought reasoning in language models. In
The Eleventh International Conference on Learning
Representations .
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023c. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 13484–13508.
Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu,
Qian Luo, Victor Zhong, Yanchao Yang, and Tao
Yu. 2023. Text2reward: Automated dense reward
function generation for reinforcement learning. arXiv
preprint arXiv:2309.11489 .
Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li,
Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi
Lin, Shuo Liu, et al. 2024. Mmt-bench: A compre-
hensive multimodal benchmark for evaluating large
vision-language models towards multitask agi. arXiv
preprint arXiv:2404.16006 .Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao
Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding,
Songyang Zhang, Haodong Duan, Hang Yan, et al.
2023a. Internlm-xcomposer: A vision-language
large model for advanced text-image comprehension
and composition. arXiv preprint arXiv:2309.15112 .
Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao,
Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping
Luo. 2023b. Gpt4roi: Instruction tuning large lan-
guage model on region-of-interest. arXiv preprint
arXiv:2307.03601 .
Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,
Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong
Luo, Yaqian Li, Shilong Liu, et al. 2024a. Recognize
anything: A strong image tagging model. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 1724–1732.
Yuan Zhang, Fei Xiao, Tao Huang, Chun-Kai Fan,
Hongyuan Dong, Jiawen Li, Jiacong Wang, Kuan
Cheng, Shanghang Zhang, and Haoyuan Guo. 2024b.
Unveiling the tapestry of consistency in large vision-
language models. arXiv preprint arXiv:2405.14156 .
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. In The Twelfth International Con-
ference on Learning Representations .
Zhuofan Zong, Guanglu Song, and Yu Liu. 2023. Detrs
with collaborative hybrid assignments training. In
Proceedings of the IEEE/CVF international confer-
ence on computer vision , pages 6748–6758.
12A Prompt Templates for W2C data
construction pipeline
A.1 Prompt Templates
W2C data construction pipeline calls the VLMs re-
peatedly by using different prompts. We guide the
VLMs to accurately answer questions by designing
universal prompt templates, thus ensuring better
compliance with instruction. All the prompts are
shown in Table 7.
B More experiments of W2C .
B.1 More Visual Question Answering
Benchmarks
We show more Visual Question Answering bench-
marks of W2C on LLaV A-NeXT-7B/13B under dif-
ferent combination of IT datasets in Table 8. The
W2C method consistently demonstrates superior
experimental results.
B.2 Code Parsing Ability Evaluation
We have added an analysis of in-context learning
for two representative datasets in Table 9: MM-
Star and RefCOCOg. It’s important to note that al-
though we report the in-context learning results on
RefCOCOg val set under the same settings, compar-
ing these two types of outputs for grounding tasks
is not practically meaningful. This is because when
we instruct the W2C -trained model to output in
detailed caption format, the captions do not usually
contain specific box information like [x1,y1,x2,y2].
This leads to a low IoU score for in-context learn-
ing with 2/4 shot detailed captions. However, when
outputting in code format, the model does predict
box information, which accounts for the significant
difference in results on RefCOCOg.
C Implementation Details for W2C
experiments
C.1 Dataset Details
All the creators or original owners of assets used
in the paper are credited properly, and the license
and terms of use are explicitly mentioned and are
respected properly. All datasets we use are from in-
ternet open-source datasets under CC-BY licenses
and are cited properly.
Data Construction Pipeline Details We incor-
porate images from the open-source ShareGPT4V
dataset, totaling approximately 87K images. Forthe VLMs in our data construction pipeline, we di-
rectly use the official release checkpoints including
LLaV A-1.5 and LLaV A-NeXT.
For the cost of our data construction pipeline,
we use about 1/1.5 day on 32 A100s GPU for
LLaV A-1.5 and about 2/3 days on 48 A100s GPU
for LLaV A-NeXT. For the data obtained by W2C
pipeline, we get 34K from LLaV A-1.5-7B, 33K
from LLaV A-1.5-13B, 37K from LLaV A-NeXT-
7B, and 29K from LLaV A-NeXT-13B. The reasons
for the inconsistency in the amount of data are mul-
tifaceted. On the one hand, a minor portion of the
data was discarded due to improper handling of
anomalous data throughout the processing stage.
On the other hand, a significant amount of data was
eliminated during the consistency filtering stage
owing to inconsistencies detected by the VLMs.
Additionally, the generative capabilities of various
VLMs vary, and the inherent randomness within
VLMs themselves also contributes to these incon-
sistencies.
Training Details During the training of VLMs,
we use different dataset combinations. We uti-
lize the original paper’s open-source dataset during
both the PT and IT training stages for LLaV A-1.5.
In contrast, for the training of LLaV A-NeXT, the
lack of disclosure regarding the specific details
of the IT stage, we trained using all training set
from LLaV A 665k(Liu et al., 2023a), DocVQA (Tito
et al., 2021), ChartQA (Masry et al., 2022) and
ShareGPT4V (Chen et al., 2023a). Furthermore, by
aligning our dataset with that of the original study,
we achieved comparable experimental results. We
use the CLIP-pretrained ViT-L/14 (Radford et al.,
2021) as a vision encoder, which input resolution
is 336×336. We freeze the vision encoder during
training on the LLaV A-1.5 and only freeze the vi-
sion encoder on the PT stage during training on the
LLaV A-NEXT following the original paper. The
experiments of VLM training are all conducted on
16 A100 GPUs.
C.2 Implementation Details of our Pipeline
We employ beam search to fully leverage the power-
ful language generation capabilities and extensive
knowledge base of VLM. This approach enables
the generation of an increased number of captions,
assisting us in acquiring a broader set of visual con-
cept candidates. Due to the limitation of GPU mem-
ory, we set the generation beam to 8 on LLaV A-1.5
and 4 on LLaV A-Next. The learning rate for the
13Stage Prompt
Prompt for Caption
Global Caption −pg Please provide a simple sentence that describes this image accurately.
Detail Caption −pd Please describe all the visual concepts in the image in detail, but use concise words
with no more than 120 words.
Prompt for Self-Instructed Concept-targeted Captions
Compositional Caption −pdesc From the image, provide one sentence that describes {e} (you should try your best to
include attributes like shape, color or material), especially, using {e} as the beginning
of your answer.
OCR Extract −pocr List all the text in the image, answer with the ocr tokens only, and answer ’No’ with
one word if there isn’t any.
Prompt for Consistency Filtering
Caption Re-ranking −pvalid-c Is ’{e}’ a valid and visible visual concept in the image? Answer yes or no with only
one single word.
Counting Group Filtering −pvalid-g Is there {parse times} or more {group key} in the image? Answer yes or no with a
single word.
Symbol Explanation
{e} means an entity in the final detected entity list of this image.
{parse times} means the number of times an entity appears in the entity list of this image.
{group key} means the entity name corresponding to parse times in the entity list of this image.
Table 7: Prompt for W2C data construction pipeline.
Method InfoVQA MMMU
LLaV A-NeXT-7B 26.3 35.8
+ShareGPT4V 27.4 35.0
+W2C 27.8 36.2
LLaV A-NeXT-13B 29.7 35.3
+ShareGPT4V 29.9 34.8
+W2C 30.4 35.3
Table 8: More Visual Question Answering benchmarks
ofW2C on LLaV A-NeXT-7B/13B under different com-
bination of IT datasets. The best results are bold .
Method MMStar RefCOCOg
2-shot 4-shot 2-shot 4-shot
LLaVA-NeXT-7B
detail caption 33.60 33.47 11.49 19.32
code parsing 35.13 36.00 49.83 51.02
Table 9: Comparison between detail caption and code
parsing ability in few-shot evaluations on MMStar and
RefCOCOg without referring to the image on LLaV A-
NeXT-7B.PT stage is set to 1e−3and the IT stage is set to
2e−5for both Vicuna-7B and Vicuna-13B back-
bone LLM. We set the warmup ratio to 0.03, the
PT stage batch size is set to 256 and the IT stage
batch size is set to 128. We use model max length
2048 on LLaV A-1.5 and 4096 on LLaV A-Next for
its high resolution setting.
C.3 Data Example
In Figure 3 and Figure 4, we present images from
the ShareGPT4V dataset alongside the correspond-
ing annotations we constructed by W2C . As shown
in these images, the annotations generated entirely
by the VLMs accurately describe both the global
captions and the detailed captions of local entities
within specific areas. Additionally, the OCR text is
also encapsulated within the corresponding frames.
For multiple entities present in the images, a dis-
play of group merging is also conducted.
14classNaturalEnv:# The image shows a large building with an American flag on top, advertising a country music event.def__init__(self):self.flag_group=[Object(type="flag", description="The flag has a star at its center, resembling the shape of the state of Tennessee, and is flown against a blue sky backdrop.", bounding_box=[0.82,0.23,0.93,0.42]),Object(type="flag", description="The flag on the pole is a flat striped flag with a stars and stripe design.", bounding_box=[0.16,0.09,0.22,0.31]),]self.banner_group=[Object(type="banner", description="This is a stylized image of a huge banner.", text=Text(text="grand ole opry, the show that made country music famous"), bounding_box=[0.28,0.38,0.58,0.64]),Object(type="banner", description="A large rectangular banner featuring the images of a man playing an acoustic guitar and four other individuals performing with microphones on a red stage setback.", bounding_box=[0.02,0.35,0.11,0.74]),Object(type="banner", description="The large banner prominently displays the sign 'OLE OPRY' in the shape of a red circle with a white border and text.", bounding_box=[0.36,0.39,0.51,0.64]),]self.bush_group=[Object(type="bush", description="This small bush is beautifully trimmed and has purple flowers adorning it.",bounding_box=[0.61,0.65,0.99,0.83]),Object(type="bush", description="Large green bush next to a white pole.", bounding_box=[0.0,0.65,0.3,0.81]),Object(type="bush", description="On the concrete walk in the foreground, there is a green bush that has been trimmed into an interesting, bushy shape.", bounding_box=[0.0,0.66,0.36,1.0]),]self.opry_house=Object(type="opry_house", description="The Grand Ole Opry house is a three-sided building with a light brown roof and orange and white odeon-style marquee.", text=Text(text="grand ole opry house, the show that made country music famous, grand ole opry"), bounding_box=[0.0,0.07,0.98,0.82])self.tree=Object(type="tree", description="The tree is tall and green, located on the side of a building next to a flower bed.", bounding_box=[0.85,0.49,1.0,0.78])self.entrance=Object(type="entrance", description="The entrance to the building with a dark wooden door and a black awning.", bounding_box=[0.36,0.64,0.5,0.83])self.country_music_musicians=Object(type="country_music_musicians", description="The poster on the wall shows a country music singer in high contrast red and blue with vibrant white highlights on his attire.", text=Text(text="Grand Ole Opry, The Show that Made Country Music Famous"), bounding_box=[0.28,0.41,0.35,0.63])Figure 3: Visualization of one W2C sample with OCR information.
15classNaturalEnv:# A herd of elephants wading through water with people.def__init__(self):self.elephant_group=[Object(type="elephant", description="The brown elephant is wadding into the water.", bounding_box=[0.45,0.42,0.77,1.0]),Object(type="elephant", description="The large elephant on the right has a muddy side and a long trunk.", bounding_box=[0.0,0.58,0.23,1.0]),Object(type="elephant", description="The elephant is a large, dark brown mammal wading in a river.", bounding_box=[0.15,0.47,0.35,0.89]),]self.people_group=[Object(type="people", description="Several people wearing green shirts and khaki pants are walking on the rocky shore.", bounding_box=[0.94,0.06,1.0,0.3]),Object(type="people", description="A group of seven men in green shirts and tan shorts standing together in a sandy area with a wooden pole.", bounding_box=[0.89,0.0,0.95,0.21]),]self.stick=Object(type="stick", description="A long, slender wooden pole with a curved shape and a shiny, smooth surface.", bounding_box=[0.81,0.55,0.88,0.66])self.water_flowing=Object(type="water_flowing", description="Rough surface of the water shows agitated movement as the elephants bathe in the murky stream.", bounding_box=[0.0,0.0,1.0,1.0])self.trunk=Object(type="trunk", description="The elephant's trunk is long and curled at the end.", bounding_box=[0.63,0.73,0.77,1.0])self.onlooker=Object(type="onlooker", description="One onlooker standing on an elevated rock with greenish-brown sandal, wearing brown cargo shorts.", bounding_box=[0.81,0.0,1.0,0.3])self.riverbank=Object(type="riverbank", description="This riverbank is sandy and rocky, with a cliff-like appearance.", bounding_box=[0.63,0.11,1.0,0.37])self.stone=Object(type="stone", description="A rectangular, weathered limestone slab by a river.", bounding_box=[0.86,0.67,1.0,0.92])self.stick=Object(type="stick", description="The brown stick the person is holding.", bounding_box=[0.04,0.24,0.07,0.45])Figure 4: Visualization of one W2C sample without OCR information.
16