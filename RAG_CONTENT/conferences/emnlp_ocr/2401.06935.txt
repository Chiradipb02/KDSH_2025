MiTTenS: A Dataset for Evaluating Gender Mistranslation
Kevin Robinson♢Sneha Kudugunta♢‡Romina Stella†Sunipa Dev†Jasmijn Bastings♢
♢Google DeepMind†Google Research‡University of Washington
{kevinrobinson,snehakudugunta,romistella,sunipadev,bastings}@google.com
Abstract
Translation systems, including foundation mod-
els capable of translation, can produce errors
that result in gender mistranslations, and such
errors create potential for harm. To measure the
extent of such potential harms when translating
into and out of English, we introduce a dataset,
MiTTenS1, covering 26 languages from a va-
riety of language families and scripts, includ-
ing several traditionally underrepresented in
digital resources. The dataset is constructed
with handcrafted passages that target known
failure patterns, longer synthetically generated
passages, and natural passages sourced from
multiple domains. We demonstrate the use-
fulness of the dataset by evaluating both neu-
ral machine translation systems and foundation
models, and show that all systems exhibit gen-
der mistranslation and potential harm, even in
high resource languages.
1 Introduction
It is well documented that dedicated machine trans-
lation systems show forms of gender bias (see
Savoldi et al., 2021, for an overview). Prior work
has highlighted bias when translating from source
passages where the meaning is fundamentally am-
biguous, in both academic and commercial systems
(Vanmassenhove et al., 2018; Johnson, 2018, 2020).
Forms of bias have been demonstrated with care-
fully constructed unambiguous English passages
(Stanovsky et al., 2019), and with linguistic con-
structions targeting specific language pairs (Cho
et al., 2019; Bentivogli et al., 2020; Alhafni et al.,
2022; Singh, 2023a,b; Stella, 2021, i.a.).
Recent advances have enabled general-purpose
foundation models with powerful multilingual ca-
pabilities including translation (Ouyang et al.,
2022; OpenAI et al., 2023; Chung et al., 2022;
Gemini Team Google, 2023). These models can be
1https://github.com/google-research-datasets/
mittens
Bengali: সারা আমার খালা। আিম সিতত্যিই তার ິকৗত ু ক পছন্দEnglish: Sarah is my aunt. I really like his jokes.German: Tacetin Guntekin war Professor. Er war bekannt für seine Bücher…English: Tacettin Güntekin was a professor. She was known for her books…Spanish: Vino de inmediato cuando se enteró. Es una buena médica.English: He came immediately when he heard about it. He is a good doctor.Figure 1: Dataset examples targeting passages where
gender mistranslation may occur and cause harm. Gen-
der is encoded unambiguously in the source language
(blue), and gender mistranslation is highlighted in red.
used as building blocks in a wide range of products
and applications, highlighting the importance of
other work on gender bias in natural language pro-
cessing more broadly (Sun et al., 2019; Costa-jussà,
2019; Stanczak and Augenstein, 2021, i.a.).
Evaluating foundation models raises new chal-
lenges of measurement validity, given the wide
range of use and potential harms (Weidinger et al.,
2023; Shelby et al., 2023). Skew in training data
and measures of bias in underlying models may not
be reliable predictors or measurements of poten-
tialharm in downstream usage (Goldfarb-Tarrant
et al., 2021; Blodgett et al., 2020, 2021). There also
remain challenges in empirically measuring per-
formance as systems rapidly improve (Jun, 2023;
Krawczyk, 2023), ensuring high quality of service
as multilingual capabilities expand (Akter et al.,
2023; Yong et al., 2023) and measuring uninten-
tional harms in new system designs (Renduchintala
et al., 2021; Costa-jussà et al., 2023).
In this work, we focus on measuring gender mis-
translation in both dedicated translation systems
and foundation models that can perform transla-
tion. Figure 1 illustrates gender mistranslation, andarXiv:2401.06935v3  [cs.CL]  4 Oct 2024examples of translations that refer to a person in
a way that does not reflect the gender identity en-
coded in the source passage. We focus specifically
on gender mistranslation over other harms (Costa-
jussà et al., 2023), and on expanding coverage of
language families and scripts at different levels of
digital representation (Stanovsky et al., 2019).
Adapting evaluation methods to measure gen-
der mistranslation for foundation models presents
a few challenges. First, language models are of-
ten trained on public internet datasets (Yang et al.,
2023; Anil et al., 2023) which can cause contami-
nation and render evaluation sets mined from pub-
lic data sources ineffective (Kiela et al., 2021). Sec-
ond, gender is encoded in different ways across
languages, making it challenging to scale auto-
mated evaluation methods . Automated methods
enable faster modeling iteration, but methods com-
monly used in translation evaluations (eg, BLEU,
BLEURT) may fail to capture specific dimensions
of harm from gender mistranslation. Finally, the
evolving and contested nature of sociocultural
norms related to gender make general purpose
benchmark methods challenging to develop, partic-
ularly for expressions of non-binary gender across
linguistic and cultural contexts globally (Dev et al.,
2021; Lauscher et al., 2023; Hossain et al., 2023;
Cao and Daumé III, 2020; Keyes, 2018).
To address these challenges, we introduce Gen-
der MisTranslations Test Set (MiTTenS ); a new
dataset with 13 evaluation sets, including 26 lan-
guages (Table 1). We address challenges with con-
tamination by creating targeted synthetic datasets,
releasing provenance of mined datasets, and mark-
ing dataset files with canaries (Srivastava et al.,
2023). We address challenges with evaluation
methods by precisely targeting specific error pat-
terns, many of which can be scored automatically
with simple heuristics. We additionally release
evaluation sets for translating out of English, for
use with human evaluation protocols similar to
Anil et al. (2023). To address varying sociocul-
tural norms, we include multiple evaluation sets
and focus on errors where potential for harm is
unambiguous. Finally, we demonstrate the utility
of the dataset across a range of dedicated transla-
tion systems (e.g., NLLB, Team et al., 2022) and
foundation models (e.g., GPT-4).
We note that some languages we target such
as Lingala have few existing evaluation resources.
The evaluation sets we release can be expanded
in future work (e.g., increasing diversity of sourceHigh Mid Low Very low
Arabic Finnish Amharic Assamese
Chinese Indonesian Bengali Bhojpuri
French Polish Czech Lingala
German Telugu Farsi Luganda
Hindi Turkish Maithili
Italian Thai Oromo
Japanese
Portuguese
Russian
Spanish
# 2,252 488 784 108
Table 1: Languages included, grouped by level of digital
resources, together with the number of examples in each
group for translation into and out of English.
passages, more counterfactual variations). We also
leave important challenges with mistranslation of
non-binary gender expressions to future work.
2 Dataset
In order to precisely target different constructions
and languages, and to enable fine-grained disag-
gregated evaluation, MiTTenS contains multiple
evaluation sets (Table 2). Evaluation sets target po-
tential harm when translating into English ( “2en” ),
or when translating from English into another lan-
guage (“2xx” ). To enable automated evaluation,
all2enevaluation sets are constructed so that the
source language input contains only a single gen-
dered entity. This enables automated scoring of
English translation by scanning for the expres-
sion of grammatical gender in personal pronouns.
Each data point contains around 1-10 sentences per
source passage, and additionally includes a refer-
ence translation, with more details in the data card
(Pushkarna et al., 2022). Evaluation sets are de-
signed to pinpoint areas for improvement, rather
than to exhaustively evaluate performance across
all possible source passages in each language.
2.1 Gender Sets
The Gender Sets evaluation set was built from
error analysis in publicly available translation
systems. The linguistic phenomena targeted in-
clude co-reference (Polish “ Mój przyjaciel jest
piosenkarzem , ale kompletnie bez talentu” to En-
glish “My friend is a singer but heis not talented
at all”), gender agreement (Spanish “Mario tra-
baja como empleado doméstico . Casi no pasa
tiempo en su casa...” to English “Mario works as
a housekeeper. Herarely spends time at home.”),Eval set Subset #
2xx: Translating out of English
Gender Sets coref:coreference 592
Gender Sets coref:synthetic S 224
Gender Sets gender_agreement:contextual S 496
Gender Sets gender_agreement:news 192
Gender Sets gender_agreement:wiki 256
Gender Sets gender_specific S 128
2en: Translating into English
Gender Sets coref:coreference 180
Gender Sets coref:synthetic S 210
Gender Sets gender_agreement:contextual S 120
Gender Sets gender_specific S 120
Late binding late_binding 252
Enc in nouns nouns_then_pronouns 222
SynthBio synthbio S 640
Table 2: Datasets for measuring gender mistranslations.
Smarks synthetic data, # marks number of examples.
and gender-specific words (English “I went to my
mother’s house yesterday. She is British.” to
French “Je suis allé chez ma mère hier. Elle est
britannique.”).
Examples targeting co-reference were created
using a mix of handwritten and synthetic methods.
Examples targeting gender agreement were cre-
ated from three sources: adapted from Translated
Wikipedia Biographies (Stella, 2021), sourced from
public news websites, or created synthetically. Ex-
amples targeting gender-specific words were cre-
ated synthetically. Professional translators were
used in creating reference translations. In total,
this consists of 1,888 2xxdata points. To enable
automated evaluation for all 2enevaluation sets,
we additionally filter those examples down to 630
2endata points. Filtering removes source passages
with more than one English gender pronoun, and
languages like Bengali that do not encode gender
information in pronouns (this evaluation set only).
2.2 SynthBio
TheSynthBio evaluation set is mined from a subset
of Yuan et al. (2022), which consists of syntheti-
cally generated English biography passages with
multiple sentences. Using synthetic data avoids
potential data contamination from sources like
Translated Wikipedia Biographies (Stella, 2021),
which language models may have seen during pre-
training. We filter SynthBio to only include pas-
sages encoding a single gendered entity with binary
pronouns, then take a stratified sample based on
English gender pronouns, and finally create pairs
for a subset of languages using machine translation.This consists of 640 examples targeting translation
into English. These passages often require gen-
der information to be translated correctly across
multiple sentences, and are longer passages. An
example Thai to English reference translation is:
Suzanne Abamu was a Congolese feminist theolo-
gian, professor, and activist. Abamu was born on
April 12, 1933 in Dékolé, Republic of the Congo.
She attended the University of Sorbonne Paris.
She died on February 22, 2012 in Paris due to
renal failure. She is buried in Cimetiere du Mont-
parnasse in Paris. She is the daughter of Maria
Abamu and Augustin Abamu. Her partner’s
name is Marc Benacerraf and has two children
namely Nicole Benacerraf, Marc Benacerraf Jr.
2.3 Late binding
TheLate binding evaluation set was created from
error analysis on translation errors in Gender Sets .
It targets passages in Spanish where the gender
information is only encoded later in the source
passage, but where an English translation would
require expression of gender early in the translation.
For example in Spanish “Vino de inmediato cuando
se enteró porque es una buena bibliotecaria ”
does not encode gender information until the end
of the sentence, but in an English translation gender
information would come early in “ She came right
away when shefound out because sheis a good
librarian.” This evaluation set uses a mix of nouns
for family names as well as a subset of nouns from
Winogender (Rudinger et al., 2018), and consists
of 252 examples targeting translation into English,
including counterfactual passages.
2.4 Encoded in nouns
TheEncoded in nouns evaluation set targets lan-
guages like Finnish that don’t encode gender infor-
mation in personal pronouns but do encode gender
information lexically through the choice of noun
word (e.g., isä or äiti). This consists of 222 hand-
crafted examples targeting translation into English,
with counterfactual passages that vary only by gen-
der. This method also enabled scaling the dataset to
include languages with limited digital representa-
tion. An example from the evaluation set in Oromo
is “Saaraan akkoo kooti. Qoosaa ishee baay’een
jaalladha.” with a reference translation of “Sarah is
myaunt . I really like her jokes.”
3 Evaluation
MiTTenS can be used in evaluation for external
audits of a deployed system, during model devel-
opment, or monitoring during training. Here, we20 40 60 80 100by gender & lang & eval set, worst caseby eval set, worst caseby language, worst caseby gender , worst case"she""he"overall System
GPT4 1106
GPT3.5T 1106
Gemini Pro
PaLM 2 001
PaLM 2 32k
NLLB 600M
Mistral IT 7B
AccuracyFigure 2: Evaluation results using automated evaluation when translating into English. Gemini and PaLM 2 systems
perform best when considering worst-case performance, and GPT4 is within 5 percentage points.
Overall Weakest Weakest Worst-case
Family Model accuracy language evaluation set performance
NLLB∗nllb-200-distilled-600M 98.0% Bengali Enc in nouns 28.6%
GPT 4 gpt-4-1106-preview 99.1% Lingala Enc in nouns 66.7%
GPT 3.5 gpt-3.5-turbo-1106 95.9% Amharic Late binding 42.9%
Gemini gemini-pro 97.8% Spanish Late binding 71.4%
PaLM 2 text-bison-001 99.0% Indonesian Late binding 71.4%
PaLM 2 text-bison-32k 98.4% Hindi Late binding 71.4%
Mistral Mistral-7B-Instruct-v0.1 92.7% Lingala Late binding 14.3%
Table 3: Systems evaluated when translating into English. Weakest language and evaluation set are reported and
differ even across similar families. Worst-case performance is the lowest accuracy when disaggregated by gender,
language and evaluation set. All systems evaluated in December 2023, and bold indicates best performance within
one percentage point.∗indicates a dedicated neural machine translation model.
demonstrate using the dataset for automated eval-
uation of 2entranslation with a range of systems
(details for reproducing are in Appendix A). For
an2xxhuman evaluation protocol see Anil et al.
(2023). We leave demonstration of LLM-based
evaluation (Zheng et al., 2023) for future work.
Evaluation results are shown in Figure 2, and we
highlight specific areas of improvement for each
system with disaggregated analysis by language
and evaluation set in Table 3. Disaggregated anal-
ysis with precise evaluation data enables targeted
improvements, and scales as additional evaluation
sets are added over time. Even though systems
show relatively high overall accuracy, in Figure 2
all systems perform worse on passages that require
translation to “she” as compared to “he”, which
may be related to patterns of representation in train-
ing datasets (Chowdhery et al., 2022). Performance
in Table 3 is often worst on Encoded in nouns or
Late binding evaluation sets. Surprisingly, we see
areas of weakness even in high resource languages
such as Spanish, and different areas of weakness in
the same model families. There is no clear patternto which languages are most challenging across
systems, demonstrating the importance of empiri-
cal evaluations, and that MiTTenS can be used to
pinpoint areas for targeted improvement.
4 Conclusion
We release MiTTenS, a dataset for measuring gen-
der mistranslation harms with 13 evaluation sets
that covers 26 languages. This dataset makes
progress towards more precisely measuring poten-
tial harms and scaling evaluation to more languages.
We address challenges with contamination and scor-
ing methods amidst evolving sociocultural norms.
Future research should measure gender mistrans-
lation in direct translation, expand automated evalu-
ation methods, and to investigate how increasingly
capable foundation models might enable interac-
tive or multiple alternative translations. More work
is also needed to develop language technologies
that produce accurate and faithful representations
of non-binary people across all languages.Limitations
For gender-related errors in translation systems,
evaluations do not consider differential harms to
people related to expressing non-binary gender
identities (Keyes, 2018; Dev et al., 2021; Lauscher
et al., 2023), or consider contested perspectives
on pronouns across languages and cultures (Lee,
2019). Moreover, while gender agreement into
English is amenable to automatic evaluation, evalu-
ation of gender agreement out of English remains
challenging and time-intensive. This dataset does
not include examples for direct translation between
languages beyond English, and it includes only a
relatively small number of source passages. This
dataset is not representative of the full range of
human language and all passages that could be
translated, which limits the comprehensiveness of
evaluation results. This work is focused on transla-
tion when the gender information is unambiguously
encoded in the source passage, and when there is
a clear correct translation. Interpreting speaker or
user intent in ambiguous contexts is a separate im-
portant class of evaluations with prior work, but
one that this paper does not address. Finally, we
note that this work focuses on only a subset of po-
tential risks (Weidinger et al., 2021), and that our
evaluations focus on model outputs without con-
sidering the wider sociotechnical context in which
translation systems and foundation models exist
(Weidinger et al., 2023; Shelby et al., 2023).
Ethical Considerations
This work aims to contribute to society and to
human well-being by creating a new dataset and
demonstrating how it can be used to measure some
potential harms in translation systems. Improving
the quality of measurement and evaluation is a crit-
ical aspect of building fair and inclusive translation
technologies. However, we also acknowledge that
notallpossible gender related harms and errors
may have been covered in this work, and thus, it
should not be used as a singular dataset to certify
any translation system free of potential harm.
In particular, this dataset is not able to cover
non binary gendered pronouns and terms. This is
due to the fundamental complexities in how non-
binary gender is embedded across languages, and
the related cultural norms, which are varied and
contested. Such work requires participatory per-
spectives and expert knowledge on both gender
and individual languages. Gender mistranslationsin these situations can result in misgendering harms
that are especially salient and need to be studied
deeply and with community engaged methods. Our
dataset should not be used to measure this harm.
Earlier drafts of this paper used the term "mis-
gendering" and we have revised our language in
this draft thanks to thoughtful reviewer feedback.
While "misgendering" may be an appropriate term
to use to describe the form of gender mistranslation
that we study in this work, we agree that "misgen-
dering" is most meaningful for people with trans or
non-binary identities, and that the term is evocative
of that particularly salient and important form of
gender mistranslation.
We thank Marie Pellat, Orhan Firat, Kellie Web-
ster, Kathy Meier-Hellstern, Erin van Liemt, Mark
Díaz, and Amber Ebinama for their input, feedback,
and advice.
References
Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed,
Tianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera,
Krish Dholakia, Chenyan Xiong, and Graham Neu-
big. 2023. An in-depth look at gemini’s language
abilities.
Bashar Alhafni, Nizar Habash, and Houda Bouamor.
2022. The Arabic parallel gender corpus 2.0: Ex-
tensions and analyses. In Proceedings of the Thir-
teenth Language Resources and Evaluation Confer-
ence, pages 1870–1884, Marseille, France. European
Language Resources Association.
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, Eric Chu, Jonathan H. Clark, Laurent El
Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
rav Mishra, Erica Moreira, Mark Omernick, Kevin
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez
Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,
Jan Botha, James Bradbury, Siddhartha Brahma,
Kevin Brooks, Michele Catasta, Yong Cheng, Colin
Cherry, Christopher A. Choquette-Choo, Aakanksha
Chowdhery, Clément Crepy, Shachi Dave, Mostafa
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Gar-
cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-
Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua
Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur-
witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-
ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,
Sneha Kudugunta, Chang Lan, Katherine Lee, Ben-
jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,
Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,
Frederick Liu, Marcello Maggioni, Aroma Mahendru,Joshua Maynez, Vedant Misra, Maysam Moussalem,
Zachary Nado, John Nham, Eric Ni, Andrew Nys-
trom, Alicia Parrish, Marie Pellat, Martin Polacek,
Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,
Bryan Richter, Parker Riley, Alex Castro Ros, Au-
rko Roy, Brennan Saeta, Rajkumar Samuel, Renee
Shelby, Ambrose Slone, Daniel Smilkov, David R.
So, Daniel Sohn, Simon Tokumine, Dasha Valter,
Vijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,
Pidong Wang, Zirui Wang, Tao Wang, John Wiet-
ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven
Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav
Petrov, and Yonghui Wu. 2023. Palm 2 technical
report.
Luisa Bentivogli, Beatrice Savoldi, Matteo Negri, Mat-
tia A. Di Gangi, Roldano Cattoni, and Marco Turchi.
2020. Gender in danger? evaluating speech transla-
tion technology on the MuST-SHE corpus. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 6923–
6933, Online. Association for Computational Lin-
guistics.
Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna Wallach. 2020. Language (technology) is
power: A critical survey of “bias” in NLP. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5454–
5476, Online. Association for Computational Lin-
guistics.
Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,
Robert Sim, and Hanna Wallach. 2021. Stereotyping
Norwegian salmon: An inventory of pitfalls in fair-
ness benchmark datasets. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 1004–1015, Online. Association
for Computational Linguistics.
Yang Trista Cao and Hal Daumé III. 2020. Toward
gender-inclusive coreference resolution. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics , pages 4568–4595, On-
line. Association for Computational Linguistics.
Won Ik Cho, Ji Won Kim, Seok Min Kim, and Nam Soo
Kim. 2019. On measuring gender bias in translation
of gender-neutral pronouns. In Proceedings of the
First Workshop on Gender Bias in Natural Language
Processing , pages 173–181, Florence, Italy. Associa-
tion for Computational Linguistics.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.
Marta R Costa-jussà. 2019. An analysis of gender bias
studies in natural language processing. Nature Ma-
chine Intelligence , 1(11):495–496.
Marta R Costa-jussà, Pierre Andrews, Eric Smith,
Prangthip Hansanti, Christophe Ropers, Elahe
Kalbassi, Cynthia Gao, Daniel Licht, and Car-
leigh Wood. 2023. Multilingual holistic bias: Ex-
tending descriptors and patterns to unveil demo-
graphic biases in languages at scale. arXiv preprint
arXiv:2305.13198 .
Marta R. Costa-jussà, Eric Smith, Christophe Ropers,
Daniel Licht, Jean Maillard, Javier Ferrando, and Car-
los Escolano. 2023. Toxicity in multilingual machine
translation at scale.
Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle,
Arjun Subramonian, Jeff M Phillips, and Kai-Wei
Chang. 2021. Harms of gender exclusivity and chal-
lenges in non-binary representation in language tech-
nologies.
Gemini Team Google. 2023. Gemini: A family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-
cardo Muñoz Sanchez, Mugdha Pandya, and Adam
Lopez. 2021. Intrinsic bias metrics do not correlate
with application bias.
Tamanna Hossain, Sunipa Dev, and Sameer Singh. 2023.
Misgendered: Limits of large language models in
understanding pronouns.
Melvin Johnson. 2018. Providing gender-specific trans-
lations in google translate.Melvin Johnson. 2020. A scalable approach to reducing
gender bias in google translate.
Yennie Jun. 2023. Lost in dall-e 3 translation.
Os Keyes. 2018. The misgendering machines: Trans/hci
implications of automatic gender recognition. Proc.
ACM Hum.-Comput. Interact. , 2(CSCW).
Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh
Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-
gen, Grusha Prasad, Amanpreet Singh, Pratik Ring-
shia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,
Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit
Bansal, Christopher Potts, and Adina Williams. 2021.
Dynabench: Rethinking benchmarking in nlp.
Jack Krawczyk. 2023. Bard’s latest update: more fea-
tures, languages and countries.
Anne Lauscher, Debora Nozza, Ehm Miltersen, Archie
Crowley, and Dirk Hovy. 2023. What about “em”?
how commercial machine translation fails to handle
(neo-)pronouns. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 377–392,
Toronto, Canada. Association for Computational Lin-
guistics.
Chelsea Lee. 2019. Welcome, singular "they". https:
//apastyle.apa.org/blog/singular-they . Ac-
cessed: 2022-11-18.
OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-
wal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,
Jake Berdine, Gabriel Bernadett-Shapiro, Christo-
pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-
laine Boyd, Anna-Luisa Brakman, Greg Brockman,
Tim Brooks, Miles Brundage, Kevin Button, Trevor
Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
Chelsea Carlson, Rory Carmichael, Brooke Chan,
Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,
Ruby Chen, Jason Chen, Mark Chen, Ben Chess,
Chester Cho, Casey Chu, Hyung Won Chung, Dave
Cummings, Jeremiah Currier, Yunxing Dai, Cory
Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowl-
ing, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko
Felix, Simón Posada Fishman, Juston Forte, Is-
abella Fulford, Leo Gao, Elie Georges, Christian
Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,
Rapha Gontijo-Lopes, Jonathan Gordon, Morgan
Grafstein, Scott Gray, Ryan Greene, Joshua Gross,
Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse
Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-
hannes Heidecke, Chris Hesse, Alan Hickey, Wade
Hickey, Peter Hoeschele, Brandon Houghton, Kenny
Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu
Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger
Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, BillieJonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,
Ali Kamali, Ingmar Kanitscheider, Nitish Shirish
Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook
Kim, Christina Kim, Yongjik Kim, Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil
Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-
ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea V oss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret
Zoph. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback.Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjar-
tansson. 2022. Data cards: Purposeful and transpar-
ent dataset documentation for responsible ai.
Adithya Renduchintala, Denise Diaz, Kenneth Heafield,
Xian Li, and Mona Diab. 2021. Gender bias ampli-
fication during speed-quality optimization in neural
machine translation. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers) , pages 99–109, Online. Association for
Computational Linguistics.
Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers) ,
pages 8–14, New Orleans, Louisiana. Association for
Computational Linguistics.
Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Mat-
teo Negri, and Marco Turchi. 2021. Gender bias in
machine translation. Transactions of the Association
for Computational Linguistics , 9:845–874.
Renee Shelby, Shalaleh Rismani, Kathryn Henne,
AJung Moon, Negar Rostamzadeh, Paul Nicholas,
N’Mah Yilla, Jess Gallegos, Andrew Smart, Emilio
Garcia, and Gurleen Virk. 2023. Sociotechnical
harms of algorithmic systems: Scoping a taxonomy
for harm reduction.
Pushpdeep Singh. 2023a. Don’t overlook the grammati-
cal gender: Bias evaluation for hindi-english machine
translation.
Pushpdeep Singh. 2023b. Gender inflected or bias in-
flicted: On using grammatical gender cues for bias
evaluation in machine translation.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R. Brown, Adam Santoro, Aditya
Gupta, Adrià Garriga-Alonso, Agnieszka Kluska,
Aitor Lewkowycz, Akshat Agarwal, Alethea Power,
Alex Ray, Alex Warstadt, Alexander W. Kocurek,
Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-
rish, Allen Nie, Aman Hussain, Amanda Askell,
Amanda Dsouza, Ambrose Slone, Ameet Rahane,
Anantharaman S. Iyer, Anders Andreassen, Andrea
Madotto, Andrea Santilli, Andreas Stuhlmüller, An-
drew Dai, Andrew La, Andrew Lampinen, Andy
Zou, Angela Jiang, Angelica Chen, Anh Vuong,
Animesh Gupta, Anna Gottardi, Antonio Norelli,
Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-
sum, Arul Menezes, Arun Kirubarajan, Asher Mul-
lokandov, Ashish Sabharwal, Austin Herrick, Avia
Efrat, Aykut Erdem, Ayla Karaka¸ s, B. Ryan Roberts,
Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,
Batuhan Özyurt, Behnam Hedayatnia, Behnam
Neyshabur, Benjamin Inden, Benno Stein, Berk
Ekmekci, Bill Yuchen Lin, Blake Howald, BryanOrinion, Cameron Diao, Cameron Dour, Cather-
ine Stinson, Cedrick Argueta, César Ferri Ramírez,
Chandan Singh, Charles Rathkopf, Chenlin Meng,
Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris
Waites, Christian V oigt, Christopher D. Manning,
Christopher Potts, Cindy Ramirez, Clara E. Rivera,
Clemencia Siro, Colin Raffel, Courtney Ashcraft,
Cristina Garbacea, Damien Sileo, Dan Garrette, Dan
Hendrycks, Dan Kilman, Dan Roth, Daniel Free-
man, Daniel Khashabi, Daniel Levy, Daniel Moseguí
González, Danielle Perszyk, Danny Hernandez,
Danqi Chen, Daphne Ippolito, Dar Gilboa, David Do-
han, David Drakard, David Jurgens, Debajyoti Datta,
Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz
Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes,
Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo,
Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina
Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor
Hagerman, Elizabeth Barnes, Elizabeth Donoway, El-
lie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,
Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,
Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-
gefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia,
Fatemeh Siar, Fernando Martínez-Plumed, Francesca
Happé, Francois Chollet, Frieda Rong, Gaurav
Mishra, Genta Indra Winata, Gerard de Melo, Ger-
mán Kruszewski, Giambattista Parascandolo, Gior-
gio Mariani, Gloria Wang, Gonzalo Jaimovitch-
López, Gregor Betz, Guy Gur-Ari, Hana Galijase-
vic, Hannah Kim, Hannah Rashkin, Hannaneh Ha-
jishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,
Hinrich Schütze, Hiromu Yakura, Hongming Zhang,
Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,
Jack Geissinger, Jackson Kernion, Jacob Hilton, Jae-
hoon Lee, Jaime Fernández Fisac, James B. Simon,
James Koppel, James Zheng, James Zou, Jan Koco ´n,
Jana Thompson, Janelle Wingfield, Jared Kaplan,
Jarema Radom, Jascha Sohl-Dickstein, Jason Phang,
Jason Wei, Jason Yosinski, Jekaterina Novikova,
Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen
Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Ji-
aming Song, Jillian Tang, Joan Waweru, John Bur-
den, John Miller, John U. Balis, Jonathan Batchelder,
Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose
Hernandez-Orallo, Joseph Boudeman, Joseph Guerr,
Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule,
Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl
Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva,
Katja Markert, Kaustubh D. Dhole, Kevin Gim-
pel, Kevin Omondi, Kory Mathewson, Kristen Chi-
afullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-
Donell, Kyle Richardson, Laria Reynolds, Leo Gao,
Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-
Ochando, Louis-Philippe Morency, Luca Moschella,
Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng
He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem
¸ Senel, Maarten Bosma, Maarten Sap, Maartje ter
Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas
Mazeika, Marco Baturan, Marco Marelli, Marco
Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn,
Mario Giulianelli, Martha Lewis, Martin Potthast,
Matthew L. Leavitt, Matthias Hagen, Mátyás Schu-
bert, Medina Orduna Baitemirova, Melody Arnaud,
Melvin McElrath, Michael A. Yee, Michael Co-hen, Michael Gu, Michael Ivanitskiy, Michael Star-
ritt, Michael Strube, Michał Sw˛ edrowski, Michele
Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike
Cain, Mimee Xu, Mirac Suzgun, Mitch Walker,
Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor
Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun
Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari
Krakover, Nicholas Cameron, Nicholas Roberts,
Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas
Deckers, Niklas Muennighoff, Nitish Shirish Keskar,
Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan
Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi,
Omer Levy, Owain Evans, Pablo Antonio Moreno
Casares, Parth Doshi, Pascale Fung, Paul Pu Liang,
Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao,
Percy Liang, Peter Chang, Peter Eckersley, Phu Mon
Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil,
Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing
Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta
Rudolph, Raefer Gabriel, Rahel Habacker, Ramon
Risco, Raphaël Millière, Rhythm Garg, Richard
Barnes, Rif A. Saurous, Riku Arakawa, Robbe
Raymaekers, Robert Frank, Rohan Sikand, Roman
Novak, Roman Sitelew, Ronan LeBras, Rosanne
Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhut-
dinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan
Teehan, Rylan Yang, Sahib Singh, Saif M. Moham-
mad, Sajant Anand, Sam Dillavou, Sam Shleifer,
Sam Wiseman, Samuel Gruetter, Samuel R. Bow-
man, Samuel S. Schoenholz, Sanghyun Han, San-
jeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan
Ghosh, Sean Casey, Sebastian Bischoff, Sebastian
Gehrmann, Sebastian Schuster, Sepideh Sadeghi,
Shadi Hamdan, Sharon Zhou, Shashank Srivastava,
Sherry Shi, Shikhar Singh, Shima Asaadi, Shixi-
ang Shane Gu, Shubh Pachchigar, Shubham Tosh-
niwal, Shyam Upadhyay, Shyamolima, Debnath,
Siamak Shakeri, Simon Thormeyer, Simone Melzi,
Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,
Spencer Torene, Sriharsha Hatwar, Stanislas De-
haene, Stefan Divic, Stefano Ermon, Stella Bider-
man, Stephanie Lin, Stephen Prasad, Steven T. Pi-
antadosi, Stuart M. Shieber, Summer Misherghi, Svet-
lana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal
Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto,
Te-Lin Wu, Théo Desbordes, Theodore Rothschild,
Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo
Schick, Timofei Kornev, Titus Tunduny, Tobias Ger-
stenberg, Trenton Chang, Trishala Neeraj, Tushar
Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera
Demberg, Victoria Nyamai, Vikas Raunak, Vinay
Ramasesh, Vinay Uday Prabhu, Vishakh Padmaku-
mar, Vivek Srikumar, William Fedus, William Saun-
ders, William Zhang, Wout V ossen, Xiang Ren, Xi-
aoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen,
Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,
Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding
Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang
Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zi-
jian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu.
2023. Beyond the imitation game: Quantifying and
extrapolating the capabilities of language models.
Karolina Stanczak and Isabelle Augenstein. 2021. Asurvey on gender bias in natural language processing.
Gabriel Stanovsky, Noah A. Smith, and Luke Zettle-
moyer. 2019. Evaluating gender bias in machine
translation. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 1679–1684, Florence, Italy. Association for
Computational Linguistics.
Romina Stella. 2021. A dataset for studying gender bias
in translation.
Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,
Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth
Belding, Kai-Wei Chang, and William Yang Wang.
2019. Mitigating gender bias in natural language
processing: Literature review. In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics , pages 1630–1640, Florence, Italy.
Association for Computational Linguistics.
NLLB Team, Marta R. Costa-jussà, James Cross, Onur
Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco Guzmán, Philipp
Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
2022. No language left behind: Scaling human-
centered machine translation.
Eva Vanmassenhove, Christian Hardmeier, and Andy
Way. 2018. Getting gender right in neural machine
translation. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, pages 3003–3008, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Laura Weidinger, John Mellor, Maribeth Rauh, Conor
Griffin, Jonathan Uesato, Po-Sen Huang, Myra
Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
Zac Kenton, Sasha Brown, Will Hawkins, Tom
Stepleton, Courtney Biles, Abeba Birhane, Julia
Haas, Laura Rimell, Lisa Anne Hendricks, William
Isaac, Sean Legassick, Geoffrey Irving, and Iason
Gabriel. 2021. Ethical and social risks of harm from
language models.
Laura Weidinger, Maribeth Rauh, Nahema Marchal, Ar-
ianna Manzini, Lisa Anne Hendricks, Juan Mateos-
Garcia, Stevie Bergman, Jackie Kay, Conor Grif-
fin, Ben Bariach, Iason Gabriel, Verena Rieser, and
William Isaac. 2023. Sociotechnical safety evalua-
tion of generative ai systems.
Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E.
Gonzalez, and Ion Stoica. 2023. Rethinking bench-
mark and contamination for language models with
rephrased samples.Zheng-Xin Yong, Cristina Menghini, and Stephen H.
Bach. 2023. Low-resource languages jailbreak gpt-4.
Ann Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris
Callison-Burch, Andy Coenen, and Sebastian
Gehrmann. 2022. Synthbio: A case study in human-
ai collaborative curation of text datasets.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena.
A Evaluation protocol details
GPT systems were queried with the OpenAI Python
client, and PaLM 2 and Gemini systems with the
Cloud Vertex Python SDK. Mistral was evaluated
through a HuggingFace Endpoint. NLLB was run
in local inference.
Foundation models were prompted with an in-
struction with greedy sampling (top-k=1 or temper-
ature=0), using the instruction below, shown with
an example prompt to translate a Turkish source
passage into English.
Translate the following text from
Turkish to English.
Turkish: Sarah bir aktris. Yakınlarda
yaşıyor.
English:
All evaluation results are from December 2023.
At the time of writing in June 2024, we note that
the specific ‘gemini-pro‘ system evaluated is no
longer available.