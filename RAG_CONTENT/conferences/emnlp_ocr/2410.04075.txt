PsFuture: A Pseudo-Future-based Zero-Shot Adaptive Policy for
Simultaneous Machine Translation
Libo Zhao1,2, Jing Li2,3*, Ziqian Zeng1*
1Shien-Ming Wu School of Intelligent Engineering, South China University of Technology
2Department of Computing, Hong Kong Polytechnic University
3Research Centre for Data Science & Artificial Intelligence, Hong Kong Polytechnic University
wilbzhao@mail.scut.edu.cn, jing-amelia.li@polyu.edu.hk, zqzeng@scut.edu.cn
Abstract
Simultaneous Machine Translation (SiMT) re-
quires target tokens to be generated in real-
time as streaming source tokens are consumed.
Traditional approaches to SiMT typically re-
quire sophisticated architectures and extensive
parameter configurations for training adaptive
read/write policies, which in turn demand con-
siderable computational power and memory.
We propose PsFuture, the first zero-shot adap-
tive read/write policy for SiMT, enabling the
translation model to independently determine
read/write actions without the necessity for ad-
ditional training. Furthermore, we introduce
a novel training strategy, Prefix-to-Full (P2F),
specifically tailored to adjust offline translation
models for SiMT applications, exploiting the
advantages of the bidirectional attention mech-
anism inherent in offline models. Experiments
across multiple benchmarks demonstrate that
our zero-shot policy attains performance on par
with strong baselines and the P2F method can
further enhance performance, achieving an out-
standing trade-off between translation quality
and latency.1
1 Introduction
Simultaneous Machine Translation (SiMT) (Gu
et al., 2017) is required to generate target tokens
concurrently as it processes incoming source to-
kens. Differing from traditional machine transla-
tion (MT) methods (Bahdanau et al., 2015; Vaswani
et al., 2017; Pang et al., 2024) that access the full
source text, SiMT necessitates a read/write (R/W)
policy to decide between emitting target tokens
or awaiting more source input, coupled with the
ability to translate from source prefixes to target
prefixes (P2P) (Ma et al., 2018). Typically, the
read/write policy is integrated with the translation
*Corresponding author.
1The code is available at https://github.com/
lbzhao970/PsFuturemechanism: either employing a fixed wait- kpol-
icy alongside a corresponding translation model
(Ma et al., 2018; Elbayad et al., 2020; Zhang et al.,
2021b), or utilizing an adaptive policy (Gu et al.,
2017; Dalvi et al., 2018; Zheng et al., 2019, 2020;
Ma et al., 2020; Zhang and Feng, 2022b; Guo et al.,
2023a; Zhao and Zeng, 2024; Chen et al., 2024)
that dynamically adjusts read/write decisions based
on the context, in conjunction with a model trained
to translate policy-defined prefixes. This adaptive
method has led to superior performance (Zhang
and Feng, 2022a, 2023), yet it demands special-
ized architectural solutions and multitask learning
frameworks for concurrent training of the closely
linked adaptive policy and translation model, com-
plicating component optimization and increasing
computational demands.
On the other hand, DaP-SiMT (Zhao et al., 2023)
introduces a novel framework that separates the
adaptive read/write policies from the translation
model, offering greater versatility in simultaneous
translation. This approach demonstrates that trans-
lation models, when directed by an effective adap-
tive read/write policy, even if initially trained on
fixed policies, can balance quality and latency well,
achieving state-of-the-art (SOTA) outcomes. How-
ever, akin to other adaptive policies, it requires
intricate designs and a significant parameter set for
training the adaptive read/write policy, often de-
manding substantial computational resources and
memory.
We introduce PsFuture, a zero-shot adaptive
read/write policy based on pseudo-future informa-
tion. This policy utilizes the inherent capabilities
of the translation model itself to make read/write
decisions without additional training. Similar to the
policy in DaP-SiMT (Zhao et al., 2023), we draw
inspiration from human simultaneous translation
(Al-Khanji et al., 2000; Liu, 2008), where inter-
preters shift from listening to translating upon antic-
ipating that further future words would not impactarXiv:2410.04075v1  [cs.CL]  5 Oct 2024+Possiblefuture information3+Possiblefuture information2+Possiblefutureinformation1 NofutureinformationÊàëÊÉ≥ÂêÉ<eos>(I)   (want)(eat)   (<eos>)ÊàëÊÉ≥ÂêÉÈ•≠(I)   (want)(eat)   (meal)ÊàëÊÉ≥ÂêÉËãπÊûú(I)   (want)(eat)   (apple)ÊàëÊÉ≥ÂêÉ(I) (want)(eat)SourceprefixesIwantIwantIwantIwantTargetprefixesPredicteddistributionofthenexttokentoaan‚Ä¶toaan‚Ä¶toaan‚Ä¶toaan‚Ä¶Figure 1: An Zh ‚ÜíEn example demonstrating an ideal timing for predicting the next token "to". Even when provided
with additional possible future information, the probability distribution of the predicted next token does not change
significantly, remaining dominated by the token "to". Therefore, based on the current source prefix " ÊàëÊÉ≥ÂêÉ" and
the current target prefix "I want," a write operation can be executed to predict the next token as "to".
their current decisions. As illustrated in Figure 1,
this behavior implies a minor divergence between
translation predictions based on partial versus more
complete source context. However, in simultaneous
translation tasks, previewing future source informa-
tion is not feasible. Our method, PsFuture, over-
comes this by utilizing pseudo-future information,
which is a token suffix in the source language that
can be fixed or dynamically predicted by language
models. By quantifying the divergence between
the predicted next target token distributions with or
without pseudo-future information, and comparing
it to a predefined threshold, a read/write decision
can be made.
The proposed PsFuture method can be directly
applied to most existing simultaneous translation
models, such as the multi-path wait- kmodel, which
demonstrates superior performance when directed
by effective adaptive policies (Zhao et al., 2023).
Additionally, we investigate the application of the
PsFuture method to offline translation models. Pre-
vious SiMT models (Elbayad et al., 2020; Zhang
and Feng, 2022a) conventionally employ a unidi-
rectional attention encoder with tailored masked-
cross-attention for prefix-to-prefix training. This
approach, while efficient, limits the model‚Äôs abil-
ity to extract features, making it less adept in
high-latency scenarios compared to offline mod-
els that utilize bidirectional attention mechanisms.
To leverage the benefits of bidirectional attention
in SiMT, we introduce a novel and effective train-
ing technique, Prefix-to-Full (P2F), designed to en-
hance the performance of offline translation models
under diverse latency conditions. Our main contri-
butions can be summarized as follows.1.We propose the first zero-shot adaptive
read/write policy in SiMT, PsFuture, which
utilizes the inherent capabilities of the transla-
tion model to make read/write decisions with-
out any additional training. To our knowledge,
PsFuture is the only adaptive method in the
current SiMT field that offers such flexibility.
2.We present an effective training technique,
Prefix-to-Full (P2F) to enhance the perfor-
mance of offline translation models under di-
verse latency conditions.
3.Experiments across multiple benchmarks
demonstrate that our zero-shot policy attains
performance on par with strong baselines and
achieves an outstanding accuracy-latency bal-
ance.
2 Related Work
SiMT policies are broadly categorized into fixed
and adaptive schemes. Fixed policies (Ma et al.,
2018; Elbayad et al., 2020; Zhang et al., 2021b)
execute read/write actions following predefined
rules, such as the wait- kpolicy (Ma et al., 2018),
which after reading ksource tokens, alternates be-
tween reading and writing one token. Conversely,
adaptive policies dynamically determine read/write
actions based on the evolving source and target
context, enhancing the balance between translation
accuracy and latency.
Adaptive approaches employ methods like re-
inforcement learning within a Neural Machine
Translation (NMT) framework (Gu et al., 2017),
incremental decoding for variable target token
output (Dalvi et al., 2018), and attention-based
methods (Arivazhagan et al., 2019; Ma et al.,SimultaneousTranslationModelSimultaneousTranslationModelx2x1y1y2y1y2Cosine Distance
x2x3x1x4compared with ùúÜNext Token Distribution1 Next Token Distribution2 Read / WriteFigure 2: An overall schematic of the PsFuture policy. Based on the current source prefixes (x1, x2), target prefixes
(y1, y2), and pseudo future information (x3, x4)(tokens highlighted in red), the simultaneous translation model can
directly perform adaptive read/write decisions.
2020). Additionally, the wait-info policy (Zhang
et al., 2022) and ITST (Zhang and Feng, 2022a)
quantify the waiting latency and information
weight respectively for adaptive policy formula-
tion. HMT (Zhang and Feng, 2023) optimizes
read/write decisions by enhancing the target se-
quence‚Äôs marginal likelihood across various transla-
tion initiation points. Kim and Cho (2023) employs
a word-level policy to enhance SiMT. Furthermore,
Ma et al. (2023) introduces a non-autoregressive
streaming Transformer (NAST) to mitigate the chal-
lenges of nonmonotonicity and source-information
leakage present in conventional autoregressive
SiMT frameworks. Guo et al. (2023b) propose
to provide a tailored reference for the improvement
of SiMT model training.
Sharing a similar inspiration with PsFuture, DaP-
SiMT (Zhao et al., 2023) autonomously generates
read/write supervisions by leveraging future infor-
mation divergence for training a decision-making
network. In contrast, our approach harnesses the
model‚Äôs inherent translation capability to attain an
immediate, zero-shot read/write policy.
3 Preliminary
3.1 Full-sentence MT and SiMT
In full sentence translation tasks, an encoder-
decoder architecture like the Transformer (Vaswani
et al., 2017) transforms a translation pair x=
(x1, x2, ..., x N)andy= (y1, y2, ..., y T)by encod-
ingxinto latent representations, followed by the au-
toregressive generation of target tokens from these
representations. Generally, the model is optimizedby minimizing the cross-entropy loss.
Lmt=‚àíXT
t=1logp(yt|x,y<t) (1)
For Simultaneous Machine Translation (SiMT),
where g(t)denotes a monotonic non-decreasing
function indicating the end timestamp of the source
prefix required to produce the t-th target token,
the objective function for SiMT can be adapted as
follows,
Lsimt=‚àíXT
t=1logp 
yt|x‚â§g(t),y<t
.(2)
3.2 Wait-k Policy and Multi-Path Wait-k
Wait- kpolicy (Ma et al., 2018), the most widely
used fixed policy, starts by reading ksource tokens
and then alternates between WRITE and READ
action. The function g(t)for the wait- kpolicy can
be formally calculated as,
g(t;k) = min {t+k‚àí1, N}. (3)
Multi-path Wait- k(Elbayad et al., 2020) is an ef-
ficient technique for wait- ktraining. It randomly
samples different kvalues between batches dur-
ing model optimization. By employing a unidi-
rectional attention encoder with a tailored upper
triangular masked cross-attention mechanism, the
multi-path wait- kmodel achieves efficient prefix-
to-prefix training. Zhao et al. (2023) demonstrates
that the multi-path wait- kmodel can attain SOTA
performance under the guidance of effective adap-
tive policies.4 Method
4.1 The Pseudo-Future-based Zero-Shot
Adaptive Policy
In simultaneous translation, skilled human transla-
tors execute read/write decisions grounded in the
evolving contexts of source and target texts. Con-
ceptualizing a well-trained translation model as an
intelligent agent like a human, our objective is to de-
lineate a zero-shot adaptive read/write policy. This
approach enables decision-making based purely on
the model‚Äôs inherent linguistic comprehension and
translation proficiency, facilitating adaptive poli-
cies without necessitating further training.
Zooming in on the details of the read/write
decision-making process, interpreters transition
from listening to translating when they have ac-
quired sufficient source context x‚â§g(t)to decide
on extending the partial translation y<twith the
next target word yt. This decision is based on the
anticipation that additional source information will
not alter their current translation choice, which im-
plies a slight divergence D 
ppart
t,pmore
t
between
the interpreters‚Äô estimation of the translation dis-
tribution with partial source context ppart
t, and the
translation distribution considering the more com-
plete source context pmore
t. Interpreters opt to wait
for more source words if this divergence becomes
substantial.
ppart
t=p(yt=¬∑|x‚â§g(t),y<t) (4)
pmore
t=p(yt=¬∑|xmore,y<t), (5)
where xmorerepresents the more complete source
context by appending additional source tokens
(xg(t)+1, xg(t)+2, ...)to the current source texts
x‚â§g(t)and the distributions can be computed by
any SiMT translation models.
However, previewing future source information
is not feasible during inferring in simultaneous
translation. Our proposed PsFuture method, as the
name implies, overcomes this by utilizing pseudo-
future information xps-suffix , which is a token suffix
in the source language. It should be noted that
pseudo-future information here does not merely
refer to the predicted next few source tokens adher-
ing to human natural language patterns, but rather
a broader concept representing additional informa-
tion beyond current source input. When such infor-
mation minimally impacts the subsequent target to-
ken prediction of the translation agent, it indicates
low ambiguity in the translating process, whichsuggests that the translation of the current source
prefix remains incomplete, thereby signaling an
appropriate moment for a WRITE operation. Con-
versely, it indicates an opportune moment for a
READ operation. In this work, we explore vari-
ous forms of pseudo-future information, including
both predefined fixed suffixes and adaptive ones
that are dynamically predicted by language models
(detailed in Section 5.2).
As shown in Equation 6 and 7, we utilize co-
sine distance, which has been validated as effec-
tive in DaP-SiMT (Zhao et al., 2023), to quantify
the divergence D
ppart
t,ppseudo
t
between the pre-
dicted next target token distributions with or with-
out pseudo-future information.
D
ppart
t,ppseudo
t
= 1‚àícos
ppart
t,ppseudo
t
(6)
ppseudo
t =p(yt=¬∑|xpseudo,y<t), (7)
where xpseudo represents the fake complete source
context by appending pseudo future information
xps-suffix to the current source texts x‚â§g(t).
By comparing the divergence value to a pre-
defined threshold Œª, a read/write decision can be
made as Equation 8. The overall schematic of the
PsFuture policy is illustrated in Figure 2.
write ifDt,g(t)< Œª, elseread (8)
Figure 3 shows an example divergence matrix
based on the PsFuture method and a highlighted
read/write path, in which we only employ a ‚Äú<eos>‚Äù
token as the pseudo-future suffix. It can be ob-
served that comparing with a suitable threshold
allows for the easy identification of a potential
read/write path.
Following (Zhao et al., 2023), we also introduce
another hyperparameter in the read/write decision-
making process to limit the maximum number of
continuous READ operations for certain languages,
thereby enhancing their performance. The infer-
ence process is summarized in Algorithm 1.
4.2 The Prefix-to-Full Training Method
Adapting Offline Models to SiMT
The PsFuture approach is versatile, compatible
with most translation models, including the of-
fline ones2(standard Transformer (Vaswani et al.,
2In this paper, we distinguish standard Transformer models,
which employ the bidirectional attention mechanism, as offline
translation models, to differentiate them easily from SiMT
models that utilize the unidirectional attention mechanism./uni00000023/uni00000023
/uni0000003a/uni00000048
/uni00000051/uni00000048/uni00000048/uni00000047
/uni00000057/uni00000052
/uni0000004c/uni00000050/uni00000053/uni00000055/uni00000052/uni00000059/uni00000048
/uni00000050/uni00000044/uni00000046/uni0000004b/uni0000004c/uni00000051/uni00000048
/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000057/uni00000048/uni00000046/uni0000004b/uni00000051/uni00000052/uni0000004f/uni00000052/uni0000004a/uni0000005c
/uni00000011/uni0000000b/uni0000005a/uni00000048/uni0000000c /uni0000000b/uni00000051/uni00000048/uni00000048/uni00000047/uni0000000c /uni0000000b/uni0000004c/uni00000050/uni00000053/uni00000055/uni00000052/uni00000059/uni00000048/uni0000000c /uni0000000b/uni00000050/uni00000044/uni00000046/uni0000004b/uni0000004c/uni00000051/uni00000048/uni0000000c /uni0000000b/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c /uni0000000b/uni00000057/uni00000048/uni00000046/uni0000004b/uni00000051/uni00000052/uni0000004f/uni00000052/uni0000004a/uni0000005c/uni0000000c /uni0000000b/uni00000011/uni0000000c
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018/uni0000001c/uni00000017 /uni00000013/uni00000011/uni00000013/uni0000001b/uni0000001b /uni00000013/uni00000011/uni00000013/uni00000016/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000016/uni0000001c /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001c /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000019/uni00000016/uni00000018 /uni00000013/uni00000011/uni00000019/uni00000016/uni00000018 /uni00000013/uni00000011/uni00000013/uni0000001b /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000018/uni00000017/uni00000017 /uni00000013/uni00000011/uni0000001c/uni00000016/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni0000001c/uni00000019/uni00000018 /uni00000013/uni00000011/uni0000001a/uni00000018/uni00000018 /uni00000013/uni00000011/uni0000001c/uni0000001c/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000015/uni0000001a /uni00000013/uni00000011/uni00000016/uni0000001b /uni00000013/uni00000011/uni00000013/uni0000001b/uni0000001c /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014
/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000016 /uni00000013/uni00000011/uni00000016/uni0000001c/uni0000001c /uni00000013/uni00000011/uni00000016/uni00000016/uni00000016 /uni00000013/uni00000011/uni0000001c/uni0000001c/uni00000016 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000017/uni00000019/uni0000001c /uni00000013/uni00000011/uni00000017/uni00000018/uni00000018 /uni00000013/uni00000011/uni0000001a/uni0000001b/uni00000016 /uni00000013/uni00000011/uni0000001b/uni00000014/uni00000019 /uni00000013/uni00000011/uni0000001c/uni00000013/uni00000016 /uni00000013/uni00000011/uni00000015/uni00000014/uni00000014 /uni00000013/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000015/uni00000019/uni00000015 /uni00000013/uni00000011/uni00000017/uni00000014/uni00000019 /uni00000013/uni00000011/uni0000001a/uni00000019/uni00000016 /uni00000013/uni00000011/uni0000001a/uni00000019/uni0000001b /uni00000013/uni00000011/uni0000001b/uni00000014/uni00000017 /uni00000013/uni00000011/uni0000001b/uni00000019/uni0000001a /uni00000013/uni00000011/uni00000013Figure 3: Example of a Zh ‚ÜíEn divergence matrix D,
where Dt,g(t)=D
ppart
t,ppseudo
t
. The red elements
in the matrix denote a potential read/write path, deter-
mined by a predefined threshold Œª(0.2 in this case).
2017)). Offline translation models have demon-
strated substantial potential for simultaneous trans-
lation, as evidenced by their efficacy in speech
translation (Papi et al., 2022). However, the lack
of Prefix-to-Prefix (P2P) training in offline models
leads to lower translation quality under low-latency
conditions compared to SiMT models (Ma et al.,
2018). On the other hand, the bidirectional atten-
tion mechanism of offline models significantly en-
hances feature extraction, surpassing the unidirec-
tional attention mechanism typically used in SiMT
models to facilitate P2P training. Thus, in high-
latency scenarios, offline models usually achieve
better translation quality, as shown in Table 1.
To harness the benefits of the bidirectional at-
tention mechanism in real-time contexts, we in-
troduces a simple yet effective training strategy
for offline translation models named Prefix-to-Full
(P2F). This method aims to preserve the model‚Äôs su-
Zh‚ÜíEn De ‚ÜíEn
Standard Transformer 20.32 32.99
Multi-path Wait- k 19.45 31.81
ITST 19.15 31.26
Table 1: Comparison of case-insensitive BLEU
in offline scenario among the standard Transformer
model(Vaswani et al., 2017), multi-path wait- k
model(Elbayad et al., 2020) and ITST(Zhang and Feng,
2022a).perior performance in high-latency scenarios while
improving its effectiveness in mid-to-low latency
situations. The training regimen not only utilizes
the conventional translation loss as Equation 1, but
also integrates an innovative loss function, Prefix-
to-Full (P2F) loss. P2F loss is designed to translate
a source prefix into a complete sentence, with the
prefix length lbeing uniformly distributed and ran-
domly chosen. The overall loss is computed as
follows.
Ltotal= (1‚àíŒ±)Lmt+Œ±Lp2f (9)
Lp2f=‚àíXT
t=1logp(yt|x‚â§l,y<t) (10)
l‚àºUniform (L) (11)
Œ±‚àºBernoulli (r), (12)
where ris a hyperparamete to control the pro-
portion of the P2F loss. Lis the candidate set
of the prefix length l, or more specifically, L=
{1,2, ...,|x|}.
The P2F loss endows offline translation mod-
els with the capability to translate prefixes. Al-
though translating prefixes into full target sentences
increases the risk of hallucinations during the si-
multaneous translation process, we posit that an
effective read/write policy can mitigate such occur-
rences. For a detailed analysis of experiments on
this, please refer to Section 6.1.
5 Experiments
5.1 Datasets
WMT2022 Zh ‚ÜíEn3. We use a subset with 25M
sentence pairs for training4, from which 1500
unique sentence pairs are extracted as the valida-
tion set. We first tokenize the Chinese and English
data using the Jieba Chinese Segmentation Tool5
and Moses6, respectively, and then apply BPE with
32000 merge operations. We employ the dev set
of 956 sentence pairs from BSTC (Zhang et al.,
2021a) as the test set.
WMT15 De ‚ÜíEn7. All 4.5M sentence pairs from
this dataset are used for training, and are tok-
enized using 32K BPE merge operations. We use
newstest2013 (3000 sentence pairs) for validation
and report results on newstest2015 (2169 sentence
pairs).
3www.statmt.org/wmt22
4The data sources include casia2015, casict2011, casict2015, datum2015,
datum2017, neu2017, News Commentary V16, ParaCrawl V9.
5https://github.com/fxsjy/jieba
6https://github.com/moses-smt
7www.statmt.org/wmt15Zh‚ÜíEn De ‚ÜíEn En ‚ÜíVi
Fixed suffix1 ‚Äú<eos>‚Äù ‚Äú<eos>‚Äù ‚Äú<eos>‚Äù
Fixed suffix2 ‚Äú<unk> <eos>‚Äù ‚Äú<unk> <eos>‚Äù ‚Äú<unk> <eos>‚Äù
Fixed suffix3 ‚Äú. . . <eos>‚Äù ‚Äú... <eos>‚Äù ‚Äú... <eos>‚Äù
Fixed suffix4 ‚Äú. . .‰ø°ÊÅØÂà∞Ê≠§‰∏≠
Êñ≠„ÄÇ<eos>‚Äù‚Äú... Die Informationen
enden hier. <eos>‚Äù‚Äú... Information inter-
rupted here. <eos>‚Äù
Random suffix ( Sampled Randomly Each Time A Read/Write Decision Occurs)
Adaptive suffix (Dynamically Generated by Language Models)
Table 2: The pseudo-future suffixes across various language pairs utilized in this paper.
2 4 6 8 10 1213151719
ALBLEU Mp Wait- k
ITST
DaP-SiMT
PsFuture-W
PsFuture-O
(a) Zh‚ÜíEn, Transformer-Big2 4 6 8 10 122426283032
ALMp Wait- k
ITST
DaP-SiMT
PsFuture-W
PsFuture-O
(b) De‚ÜíEn, Transformer-Base2 4 6 824262830
ALMp Wait- k
ITST
DaP-SiMT
PsFuture-W
PsFuture-O
(c) En‚ÜíVi, Transformer-Small
Figure 4: Comparison of BLUE vs. AL curves between multi-path (abbreviated as Mp) wait-k, ITST, DaP-SiMT,
and our proposed PsFuture approach on three language pairs. PsFuture-W and PsFuture-O denote the multi-path
wait-kmodel based PsFuture method and the offline model (P2F-enhanced) based PsFuture method, respectively.
IWSLT15 En ‚ÜíVi8. All 133K sentence pairs from
this dataset (Luong and Manning, 2015) are used
for training. We use TED tst2012 (1553 sentence
pairs) for validation and TED tst2013 (1268 sen-
tence pairs) as the test set. Following the settings
in (Ma et al., 2020), we adopt word-level tokeniza-
tion and replace rare tokens (frequency <5) with
<unk>. The vocabulary sizes are 17K for English
and 7.7K for Vietnamese, respectively.
5.2 Settings
The Pseudo-Future Suffix . In this study, we in-
vestigate various pseudo-future suffixes, denoted
asxps-suffix , as detailed in Table 2. These suffixes
can be divided into two categories: fixed and adap-
tive. A primary criterion for selecting a fixed suffix
is its richness in information. For instance, the
‚Äú<eos>‚Äù token, often encountered in training trans-
lation models, effectively indicates sentence termi-
nation. Consequently, all chosen suffixes conclude
with ‚Äú<eos>‚Äù to guarantee an essential increment
of information.
Specifically, the fixed suffixes range from the ba-
8nlp.stanford.edu/projects/nmtsic ‚Äú<eos>‚Äù token (suffix 1) to more complex struc-
tures involving special tokens (‚Äú<unk> <eos>‚Äù, suf-
fix 2) and natural sentence extensions (suffixes 3
and 4), which simulate ellipsis and ellipsis with sig-
nals of information discontinuity. We also conduct
an experiment with random suffixes to investigate
the sensitivity of the PsFuture method to suffix con-
tent. These random suffixes consist of four tokens,
each randomly selected from the top 200 most fre-
quent tokens in the vocabulary, ensuring adequate
information. Furthermore, the suffix is resampled
randomly each time a read/write decision occurs.
The adaptive suffix is dynamically generated
by large language models, based on the current
source prefix for pseudo-future information predic-
tion. For Zh ‚ÜíEn, we employ the Chinese-Llama-
2-7b model9, while the Llama-2-7b-chat (Touvron
et al., 2023) is used for De ‚ÜíEn and En ‚ÜíVi exper-
iments. In the main results (5.3), we empirically
determine the optimal suffix through performance
evaluation. Section 6.1 delves into the effects of
various suffixes on the experimental outcomes, pro-
viding a thorough assessment.
9https://github.com/LinkSoul-AI/Chinese-Llama-2-7bThe Prefix-to-Full Loss Ratio . The hyperparame-
ter P2F ratio ris employed to control the propor-
tion of the P2F loss. The most effective configura-
tions are identified as 0.5, 0.8, and 0.5 for Zh ‚ÜíEn,
De‚ÜíEn, and En ‚ÜíVi, respectively. Detailed infor-
mation on the ablation studies concerning hyperpa-
rameter ris referred to Section 6.1.
Other Settings . The proposed PsFuture policy un-
dergoes empirical experiments based on the multi-
path wait- kmodel and the P2F-enhanced offline
model as mentioned in Section 4.2, comparing its
performance with two leading models in the SiMT
domain, ITST (Zhang and Feng, 2022a) and DaP-
SiMT (Zhao et al., 2023). All our implementations
are based on the Transformer (Vaswani et al., 2017)
architecture and adapted from the Fairseq Library
(Ott et al., 2019). For the Zh ‚ÜíEn experiments, we
utilize the transformer big architecture, while the
base and small architectures are used for De ‚ÜíEn
and En ‚ÜíVi experiments respectively.
For evaluation, following ITST and DaP-SiMT,
we report case-insensitive BLEU (Papineni et al.,
2002) scores to assess translation quality and Av-
erage Lagging (AL/token) (Ma et al., 2018) to
measure latency. Regarding the maximum num-
ber of continuous read actions in our method, we
empirically select the best-performing configura-
tions, which are no constraint, 4, no constraint for
Zh‚ÜíEn, De ‚ÜíEn, En ‚ÜíVi respectively. Further-
more, to achieve more robust inference results, the
initial length of the source prefix during the real-
time translation process is set to 2.
5.3 Main Results
We compare the proposed PsFuture method against
previous approaches for three language pairs inFigure 4. PsFuture-W and PsFuture-O refer to the
multi-path wait- kmodel-based PsFuture approach
and the offline model (P2F enhanced) based PsFu-
ture method, respectively.
Firstly, the PsFuture-W experiment significantly
surpasses traditional multi-path wait- kmodels, ben-
efiting from the proposed PsFuture policy over the
fixed wait- kpolicy. Notably, the performance of
PsFuture-W often matches or exceeds the SiMT
leading model ITST, which is specifically trained
with a complicated adaptive read/write policy. This
highlights the capability of SiMT translation mod-
els to make adaptive decisions themselves. Al-
though trained with a fixed strategy, the multi-path
wait-kmodel, when coupled with the zero-shot
PsFuture policy, significantly outperforms its coun-
terparts and rivals strong SiMT baselines.
Secondly, the performance of PsFuture-O
demonstrates improvements over PsFuture-W to
varying extents across all language pairs, especially
in the Zh ‚ÜíEn experiment where it outdoes the for-
mer SiMT SOTA method, DaP-SiMT. As antici-
pated, the offline translation model, endowed with
superior feature extraction capabilities, achieves
better performance at moderate to high latencies,
while the introduction of the Prefix-to-Full loss
ensures the model maintains comparable effective-
ness at lower latencies.
6 Analysis
In this scetion, we aim to provide a detailed exami-
nation of the proposed method. Unless otherwise
noted, the PsFuture-related experiments are based
on the multi-path wait- kmodel, and the results
stem from the Zh ‚ÜíEn Transformer-Big model.
2 4 6 8 10 1213151719
ALBLEUFixed suffix1
Fixed suffix2
Fixed suffix3
Fixed suffix4
Random suffix
Adaptive suffix
Ground Truth
(a) Zh‚ÜíEn, Transformer-Big2 4 6 8 10 12222426283032
ALFixed suffix1
Fixed suffix2
Fixed suffix3
Fixed suffix4
Random suffix
Adaptive suffix
Ground Truth
(b) De‚ÜíEn, Transformer-Base2 4 6 824262830
ALFixed suffix1
Fixed suffix2
Fixed suffix3
Fixed suffix4
Random suffix
Adaptive suffix
Ground Truth
(c) En‚ÜíVi, Transformer-Samll
Figure 5: Effect of the pseudo-future suffix6.1 Effect of the pseudo-future suffix
This part investigates the influence of various
pseudo-future suffixes (Table 2) on the experiment
results. As shown in Figure 5, the majority of suf-
fixes tested can achieve a desirable equilibrium be-
tween translation quality and latency, which show-
cases the tolerance of the proposed method to the
choice of suffixes. Through the comparison of vari-
ous experimental results, it is also feasible to iden-
tify specific suffixes for particular language pairs
to optimize performance. Adaptive suffixes, gen-
erated by large language models, consistently per-
form well across various corpora. However, due to
a lack of extensive experimentation with different
adaptive suffixes, their effectiveness does not sur-
pass that of the best fixed suffixes. We believe that
a large-scale exploration of adaptive suffix experi-
ments could potentially yield superior outcomes.
Additionally, it is surprising that the random suf-
fix experiment exhibits unexpectedly strong perfor-
mance. Although there are fluctuations in specific
areas, the overall result is comparable to that of
other meticulously crafted suffixes. This indicates
that PsFuture‚Äôs effectiveness is not significantly af-
fected by suffix content. This finding indicates that
the proposed method possesses a substantial lower
bound, emphasizing its robustness and straightfor-
ward applicability. These qualities align with the
method‚Äôs key features: simple yet effective.
Furthermore, experiments with ground truth suf-
fixes are conducted to ascertain the upper bound of
the PsFuture method. The results indicate that there
remains potential for enhancement. Future efforts
will focus on incrementally approaching this upper
limit by exploring and refining suffixes.
6.2 Effect of the P2F loss
Figure 6 illustrates the impact of different Prefix-
to-Full (P2F) loss ratios on the performance of our
experiments. Setting the P2F ratio rto 0 corre-
sponds to conventional offline translation model
training. This configuration, when applied directly
to SiMT tasks, yields less than ideal results, espe-
cially at lower to medium latencies. Incorporating
any level of P2F loss markedly improves perfor-
mance, effectively tailoring the offline model for
SiMT applications. Moreover, the experimental re-
sults reveal a noticeable sensitivity to the P2F ratio
r, indicating that an optimal rcan enhance the bal-
ance between translation accuracy and latency.2 4 6 8 1013151719
ALBLEUZh‚ÜíEn
Offline BLEU
P2F Ratio 0
P2F Ratio 0.2
P2F Ratio 0.5
P2F Ratio 0.8
P2F Ratio 1.0
Figure 6: BLEU vs. AL curves comparing among
PsFuture-O experiments with varying P2F ratios.
6.3 Concerns on Hallucination
2 4 6 8 10 129%10%11%12%
ALHRWait-k
ITST
DaP-SiMT
PsFuture-W
PsFuture-O
Figure 7: Hallucination Rate (HR) vs. Average Lagging
(AL) curves comparing PsFuture-O with other methods.
In the PsFuture-O experiment, the additional in-
troduction of the Prefix-to-Full (P2F) loss aims
to enhance the model‚Äôs capability to translate a
source prefix into a full target sentence, thereby
adapting it for SiMT tasks. However, this approach
may increase the risk of hallucinations during the
translation process. A hallucination is defined as
a generated token that cannot be aligned with any
source word. To illustrate this potential issue, we
compare the hallucination rate (Chen et al., 2021)
of hypotheses generated by PsFuture-O with those
produced by other methods. The comparative re-
sults are depicted in Figure 7.
It is evident that, overall, the PsFuture-O ex-
periment achieves the lowest hallucination rate,
surpassing not only the DaP-SiMT and PsFuture-
W methods, which rely on the multi-path wait- k
model, but also outperforming the meticulously
trained ITST model. This indicates that the pro-
posed PsFuture policy effectively mitigates the oc-
currence of hallucinations during the simultaneous
translation inference process.7 Conclusion
In this paper, we propose the first zero-shot adap-
tive read/write policy for SiMT, PsFuture. It em-
powers the translation model to autonomously de-
cide on read/write actions without requiring ad-
ditional training and can attain effectiveness on
par with previously meticulously trained adaptive
policies. Moreover, we introduce a novel training
strategy, Prefix-to-Full (P2F), specifically tailored
to adjust offline translation models for SiMT appli-
cations, exploiting the benefits of the bidirectional
attention mechanism inherent in offline models.
Limitations
In this work, the proposed PsFuture policy con-
ducts two forward computations for each read/write
decision-making, which may increase the total
computational load when inferring. However, it‚Äôs
important to note that while other adaptive pol-
icy methods may require only one forward com-
putation for each decision, they also necessitate
additional computations, which are also not neg-
ligible when compared to single forward comput-
ing. Overall, despite the increased computational
requirement for inference, the PsFuture method
eliminates the need for additional learnable param-
eters and training to obtain a read/write decision
maker, which also significantly reduces computa-
tional demands during training.
Ethics Statement
After careful review, to the best of our knowledge,
we have not violated the ACL Ethics Policy.
Acknowledgements
This work was supported by the National Natu-
ral Science Foundation of China (No. 62406114),
the Guangzhou Basic and Applied Basic Research
Foundation (No. 2023A04J1687), the Fundamental
Research Funds for the Central Universities (No.
2024ZYGXZR074), the NSFC Young Scientists
Fund (No. 62006203), the Research Grants Coun-
cil of the Hong Kong Special Administrative Re-
gion (No. PolyU/25200821), the Innovation and
Technology Fund (No. PRP/047/22FX), PolyU
Research Centre on Data Science and Artificial
Intelligence (No. 1-CE1E) and a gift fund from
Microware (No. N-ZDG2).References
Raja Al-Khanji, Said El-Shiyab, and Riyadh Hussein.
2000. On the use of compensatory strategies in si-
multaneous interpretation. Journal des Traducteurs
45(3):548‚Äì577 .
Naveen Arivazhagan, Colin Cherry, Wolfgang
Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruom-
ing Pang, Wei Li, and Colin Raffel. 2019. Monotonic
infinite lookback attention for simultaneous machine
translation. arXiv preprint arXiv:1906.05218 .
Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In 3rd International
Conference on Learning Representations, ICLR
2015 .
Junkun Chen, Renjie Zheng, Atsuhito Kita, Mingbo
Ma, and Liang Huang. 2021. Improving simultane-
ous translation by incorporating pseudo-references
with fewer reorderings. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 5857‚Äì5864, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Xinjie Chen, Kai Fan, Wei Luo, Linlin Zhang, Libo
Zhao, Xinggao Liu, and Zhongqiang Huang. 2024.
Divergence-guided simultaneous speech translation.
InProceedings of the AAAI Conference on Artificial
Intelligence , pages 17799‚Äì17807.
Fahim Dalvi, Nadir Durrani, Hassan Sajjad, and Stephan
V ogel. 2018. Incremental decoding and training
methods for simultaneous translation in neural ma-
chine translation. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 2 (Short Papers) , pages
493‚Äì499, New Orleans, Louisiana. Association for
Computational Linguistics.
Maha Elbayad, Laurent Besacier, and Jakob Verbeek.
2020. Efficient wait-k models for simultaneous ma-
chine translation. arXiv preprint arXiv:2005.08595 .
Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-
tor O.K. Li. 2017. Learning to translate in real-time
with neural machine translation. In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
1, Long Papers , pages 1053‚Äì1062, Valencia, Spain.
Association for Computational Linguistics.
Shoutao Guo, Shaolei Zhang, and Yang Feng. 2023a.
Learning optimal policy for simultaneous machine
translation via binary search. arXiv preprint
arXiv:2305.12774 .
Shoutao Guo, Shaolei Zhang, and Yang Feng. 2023b.
Simultaneous machine translation with tailored refer-
ence. arXiv preprint arXiv:2310.13588 .Kang Kim and Hankyu Cho. 2023. Enhanced simulta-
neous machine translation with word-level policies.
arXiv preprint arXiv:2310.16417 .
Minhua Liu. 2008. How do experts interpret. Implica-
tions from research in interpreting studies and cogni-
tive.
Minh-Thang Luong and Christopher D Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domains. In Proceedings of the 12th
International Workshop on Spoken Language Trans-
lation: Evaluation Campaign .
Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,
Kaibo Liu, Baigong Zheng, Chuanqiang Zhang,
Zhongjun He, Hairong Liu, Xing Li, et al. 2018.
Stacl: Simultaneous translation with implicit antici-
pation and controllable latency using prefix-to-prefix
framework. arXiv preprint arXiv:1810.08398 .
Xutai Ma, Juan Miguel Pino, James Cross, Liezl Puzon,
and Jiatao Gu. 2020. Monotonic multihead attention.
InInternational Conference on Learning Representa-
tions .
Zhengrui Ma, Shaolei Zhang, Shoutao Guo, Chenze
Shao, Min Zhang, and Yang Feng. 2023. Non-
autoregressive streaming transformer for simultane-
ous translation. arXiv preprint arXiv:2310.14883 .
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for
sequence modeling. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics (Demonstrations) ,
pages 48‚Äì53, Minneapolis, Minnesota. Association
for Computational Linguistics.
Jianhui Pang, Baosong Yang*, Derek Fai Wong*,
Yu Wan, Dayiheng Liu, Lidia Sam Chao, and Jun
Xie. 2024. Rethinking the exploitation of monolin-
gual data for low-resource neural machine translation.
Computational Linguistics , 50(1):25‚Äì47.
Sara Papi, Marco Gaido, Matteo Negri, and Marco
Turchi. 2022. Does simultaneous speech transla-
tion need simultaneous models? arXiv preprint
arXiv:2204.03783 .
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. ACL ‚Äô02, page
311‚Äì318, USA. Association for Computational Lin-
guistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Ruiqing Zhang, Xiyang Wang, Chuanqiang Zhang,
Zhongjun He, Hua Wu, Zhi Li, Haifeng Wang, Ying
Chen, and Qinfei Li. 2021a. Bstc: A large-scale
chinese-english speech translation dataset. arXiv
preprint arXiv:2104.03575 .
Shaolei Zhang and Yang Feng. 2022a. Information-
transport-based policy for simultaneous translation.
InProceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , Online
and Abu Dhabi. Association for Computational Lin-
guistics.
Shaolei Zhang and Yang Feng. 2022b. Reducing
position bias in simultaneous machine translation
with length-aware framework. arXiv preprint
arXiv:2203.09053 .
Shaolei Zhang and Yang Feng. 2023. Hidden markov
transformer for simultaneous machine translation.
arXiv preprint arXiv:2303.00257 .
Shaolei Zhang, Yang Feng, and Liangyou Li. 2021b.
Future-Guided Incremental Transformer for Simulta-
neous Translation. Proceedings of the AAAI Confer-
ence on Artificial Intelligence , 35(16):14428‚Äì14436.
Shaolei Zhang, Shoutao Guo, and Yang Feng. 2022.
Wait-info Policy: Balancing Source and Target at
Information Level for Simultaneous Machine Trans-
lation.
Libo Zhao, Kai Fan, Wei Luo, Wu Jing, Shushu Wang,
Ziqian Zeng, and Zhongqiang Huang. 2023. Adap-
tive policy with wait-k model for simultaneous trans-
lation. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 4816‚Äì4832, Singapore. Association for Com-
putational Linguistics.
Libo Zhao and Ziqian Zeng. 2024. Dap-simt:
divergence-based adaptive policy for simultaneous
machine translation. International Journal of Ma-
chine Learning and Cybernetics , pages 1‚Äì20.
Baigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma,
Hairong Liu, and Liang Huang. 2020. Simultaneous
translation policies: From fixed to adaptive. arXiv
preprint arXiv:2004.13169 .
Baigong Zheng, Renjie Zheng, Mingbo Ma, and Liang
Huang. 2019. Simpler and faster learning of adaptive
policies for simultaneous translation. In Proceedings
of the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 1349‚Äì1354, Hong Kong,
China. Association for Computational Linguistics.A Effect of The Max Continuous READ
Constraint
Following DaP-SiMT (Zhao et al., 2023), we set
a constraint on the maximum consecutive reads al-
lowed during inference, necessitating a write action
once this limit is reached. Figure 8 demonstrates
the influence of this hyperparameter on various lan-
guage pairings. Consistent with DaP-SiMT, we
note that this parameter exerts little or even neg-
ative impact on the Zh ‚ÜíEn and En ‚ÜíVi experi-
ments, yet it proves substantially advantageous for
the De ‚ÜíEn pair. Thus, it is advisable to identify
the optimal maximum number of continuous reads
on the validation set before the practical implemen-
tation of this approach.
B Case Study
Here, we present specific cases to demonstrate the
effectiveness of the proposed method, as illustrated
in Figure 9 and Figure 10. It is evident that the Ps-
Future policy can effectively align the source and
target tokens. Even in instances where there is a
significant difference in word order between source
and target, the PsFuture method can still make cor-
rect decisions, waiting for more source information
to proceed with the accurate translation.
C Discussion on The Extra Cost Caused
by Bi-directional Encoders
During the decoding process, the use of a unidi-
rectional encoder allows for incremental decoding,
which reduces computational requirements. How-
ever, this is not feasible with bidirectional encoders.
Compared to unidirectional encoders, predicting
each target token necessitates the additional com-
putation of g(t)‚àí1encoder hidden states ( g(t)
represents the current number of source tokens).
While the extra computational load is affordable for
shorter texts, it becomes considerably burdensome
for longer texts, potentially imposing untenable
cost. If users cannot accommodate the substantial
computational demand, they can opt for a unidi-
rectional encoder with the PsFuture method, akin
to the PsFuture-W experiment mentioned in this
paper which also demonstrates performance com-
parable to previous top non-zero-shot read/write
policies.
D Algorithm
The inference process of PsFuture policy is sum-
marized in Algorithm 1.E Numerical Results
The numerical main results are presented in Table 3.
Algorithm 1: SiMT inference with PsFu-
ture policy
Input: streaming source tokens: X‚â§j,
threshold: Œ¥,
target idx: i‚Üê1,
source idx: j‚Üê2,
max continuous READ constraint:
rmax,
current number of continuous
READ: rc‚Üê1
Output: target tokens: Y‚Üê{<BOS>}
1while Yi‚àí1Ã∏=<EOS>do
2 calculate R/W confidence (cosine
distance) cwithYi‚àí1using the
PsFuture policy mentioned in 4.1;
3 ifc‚â§Œ¥orrc‚â•rmaxthen
4 translate yiwithX‚â§j,Y‚â§i‚àí1;
5 ifyiÃ∏=<EOS>orj‚â• |X|then
6 // execute WRITE action
7 Y.Append (yi);
8 rc‚Üê0;
9 i‚Üêi+ 1;
10 else
11 // execute READ action
12 j‚Üêj+ 1;
13 rc‚Üêrc+ 1;
14 else
15 // execute READ action
16 j‚Üêj+ 1;
17 rc‚Üêrc+ 1;
18return Y2 4 6 8 1013151719
ALBLEU
Max Continuous READ 4
Max Continuous READ 6
Max Continuous READ 8
No Constraint
(a) Zh‚ÜíEn, Transformer-Big2 4 6 8 10 12222426283032
ALMax Continuous READ 4
Max Continuous READ 6
Max Continuous READ 8
No Constraint
(b) De‚ÜíEn, Transformer-Base2 4 6 824262830
ALMax Continuous READ 4
Max Continuous READ 6
Max Continuous READ 8
No Constraint
(c) En‚ÜíVi, Transformer-Small
Figure 8: Effect of the constraint on the maximum number of continuous read
Figure 9: Case No.226 in BSTC Zh ‚ÜíEn test set, evaluated with Œª= 0.08.
Figure 10: Case No.85 in BSTC Zh ‚ÜíEn test set, evaluated with Œª= 0.08. While there is a significant difference in
word order between source and target, the PsFuture method can still make correct decisions. Specifically, at step 9,
the PsFuture Policy reads an additional six tokens in sequence to ensure the accuracy of the translation.Main Results (Figure 4)
Mp Wait- k ITST DaP-SiMT PsFuture-W PsFuture-O
Zh‚ÜíEnAL BLEU AL BLEU AL BLEU AL BLEU AL BLEU
1.31 11.7 0.7 8.91 1.18 13.07 0.06 10.99 0.31 12.12
2.23 13.46 1.46 11.92 1.85 14.67 0.77 12.62 0.92 13.49
2.96 14.37 2.16 14.35 2.8 16.7 1.52 14.06 2.08 15.26
3.87 15.15 2.76 15.55 3.72 17.25 2.03 14.78 3.2 17.1
4.76 16.34 3.5 17.06 4.54 17.73 2.98 15.76 4.31 17.88
5.63 16.98 4.27 17.72 5.06 18.14 3.88 16.77 5.22 18.5
6.45 17.61 4.79 17.95 5.85 18.19 4.72 17.69 6.48 19.22
7.27 17.87 5.74 18.07 6.83 18.76 5.83 18.17 7.59 19.45
8.09 18.05 6.82 18.63 8.36 18.88 6.38 18.42 9.16 19.93
8.82 18.54 7.66 18.58 10.71 18.9 7.21 18.59 10.1 20.13
9.56 18.45 8.74 18.61 8.08 18.92 12.29 20.1
10.26 18.55 9.96 18.75 9.18 18.71
10.9 18.55 13.68 19.15 11.9 19.28
11.46 18.76
De‚ÜíEnAL BLEU AL BLEU AL BLEU AL BLEU AL BLEU
0.47 21.08 1.57 19.2 0.49 21.65 1.06 23.41 1.49 25.3
1.45 23.97 2.17 24.71 1.3 24.51 1.76 25.88 2.24 27.76
2.12 26.21 2.77 28.26 2.17 27.12 2.36 27.24 2.95 28.83
3.12 27.15 3.31 28.85 3.25 29.19 2.99 28.26 3.74 29.62
4.1 28.53 4.01 29.55 4.31 29.97 3.73 29.03 4.52 30.54
5.05 29.16 4.82 30.35 5.87 30.84 4.57 29.94 5.54 31.27
6.03 29.72 5.66 30.52 7.65 31.29 5.55 30.53 6.59 31.68
6.97 30.16 6.65 30.91 8.98 31.52 7.69 31.02 7.34 31.78
7.9 30.69 7.7 31.05 10.53 31.6 8.27 31.18 9.11 32.12
8.78 30.86 8.73 31.08 12.53 31.79 8.97 31.25 12.27 32.51
9.7 31.11 9.79 31.2 9.91 31.45
10.57 31.2 12.6 31.32 10.65 31.52
11.42 31.41 12.18 31.75
12.24 31.41
En‚ÜíViAL BLEU AL BLEU AL BLEU AL BLEU AL BLEU
3.21 27.87 1.29 23.06 0.89 21.89 0.84 21.16 0.21 18.08
3.93 29.4 1.85 26.33 1.41 27.11 1.65 27.26 0.86 21.96
4.73 30.11 2.44 28.7 1.99 29.31 2.19 28.99 1.41 25.86
5.57 30.14 3.23 29.37 3.06 29.63 2.9 29.29 2 28.34
6.43 30.08 3.76 29.5 4.6 30.15 3.45 29.66 2.63 29.21
7.28 30.13 4.42 29.48 5.44 30.09 4.54 29.86 3.7 29.92
8.12 30.14 5.15 29.79 6.25 30.13 5.42 29.9 5.67 30.08
8.93 30.11 5.91 29.83 7.49 30.15 6.61 29.91 7.82 30.2
9.7 30.1 6.7 29.94 8.08 30.2 7.56 29.95 9.91 30.14
10.43 30.2 7.69 29.95 8.74 30.17 9.1 29.96
11.13 30.16 8.67 29.84 9.61 30.01
11.79 30.13 9.93 29.95 10.67 30.11
12.41 30.16 12.58 30.01 11.69 30.1
13.01 30.18
Table 3: Numerical results in Figure 4.