Do Large Language Models Know What They Don’t Know?
Zhangyue Yin♢Qiushi Sun♠Qipeng Guo♢
Jiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢
♢School of Computer Science, Fudan University
♠Department of Mathematics, National University of Singapore
{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu
{qpguo16,xpqiu,xjhuang}@fudan.edu.cn
Abstract
Large language models (LLMs) have a wealth
of knowledge that allows them to excel in vari-
ous Natural Language Processing (NLP) tasks.
Current research focuses on enhancing their
performance within their existing knowledge.
Despite their vast knowledge, LLMs are still
limited by the amount of information they can
accommodate and comprehend. Therefore, the
ability to understand their own limitations on
the unknows, referred to as self-knowledge,
is of paramount importance. This study aims
to evaluate LLMs’ self-knowledge by assess-
ing their ability to identify unanswerable or
unknowable questions. We introduce an auto-
mated methodology to detect uncertainty in the
responses of these models, providing a novel
measure of their self-knowledge. We further in-
troduce a unique dataset, SelfAware , consisting
of unanswerable questions from five diverse cat-
egories and their answerable counterparts. Our
extensive analysis, involving 20 LLMs includ-
ing GPT-3, InstructGPT, and LLaMA, discov-
ering an intrinsic capacity for self-knowledge
within these models. Moreover, we demon-
strate that in-context learning and instruction
tuning can further enhance this self-knowledge.
Despite this promising insight, our findings also
highlight a considerable gap between the capa-
bilities of these models and human proficiency
in recognizing the limits of their knowledge.
“True wisdom is knowing what you don’t know.”
–Confucius
1 Introduction
Recently, Large Language Models (LLMs) such
as GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,
2023), and LLaMA (Touvron et al., 2023) have
shown exceptional performance on a wide range
of NLP tasks, including common sense reason-
ing (Wei et al., 2022; Zhou et al., 2022) and mathe-
∗Corresponding author.
UnknowsKnows UnknowsKnows
Known KnowsKnown Unknows
Unknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis
represents the model’s memory capacity for knowledge,
and the vertical axis represents the model’s ability to
comprehend and utilize knowledge.
matical problem-solving (Lewkowycz et al., 2022;
Chen et al., 2022). Despite their ability to learn
from huge amounts of data, LLMs still have lim-
itations in their capacity to retain and understand
information. To ensure responsible usage, it is cru-
cial for LLMs to have the capability of recognizing
their limitations and conveying uncertainty when
responding to unanswerable or unknowable ques-
tions. This acknowledgment of limitations, also
known as “ knowing what you don’t know ,” is a
crucial aspect in determining their practical appli-
cability. In this work, we refer to this ability as
model self-knowledge.
The Know-Unknow quadrant in Figure 1 il-
lustrates the relationship between the model’s
knowledge and comprehension. The ratio of
“Known Knows” to “Unknown Knows” demon-
strates the model’s proficiency in understanding
and applying existing knowledge. Techniques
such as Chain-of-Thought (Wei et al., 2022), Self-
Consistency (Wang et al., 2022), and Complex
CoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023this ratio, resulting in improved performance on
NLP tasks. We focus on the ratio of “Known Un-
knows” to “Unknown Unknows”, which indicates
the model’s self-knowledge level, specifically un-
derstanding its own limitations and deficiencies in
the unknows.
Existing datasets such as SQuAD2.0 (Rajpurkar
et al., 2018) and NewsQA (Trischler et al., 2017),
widely used in question answering (QA), have been
utilized to test the self-knowledge of models with
unanswerable questions. However, these questions
are context-specific and could become answerable
when supplemented with additional information.
Srivastava et al. (2022) attempted to address this by
evaluating LLMs’ competence in delineating their
knowledge boundaries, employing a set of 23 pairs
of answerable and unanswerable multiple-choice
questions. They discovered that these models’ per-
formance barely surpassed that of random guessing.
Kadavath et al. (2022) suggested probing the self-
knowledge of LLMs through the implementation
of a distinct "Value Head". Yet, this approach may
encounter difficulties when applied across varied
domains or tasks due to task-specific training. Con-
sequently, we redirect our focus to the inherent
abilities of LLMs, and pose the pivotal question:
“Do large language models know what they don’t
know? ”.
In this study, we investigate the self-knowledge
of LLMs using a novel approach. By gathering
reference sentences with uncertain meanings, we
can determine whether the model’s responses re-
flect uncertainty using a text similarity algorithm.
We quantified the model’s self-knowledge using
the F1 score. To address the small and idiosyn-
cratic limitations of existing datasets, we created
a new dataset called SelfAware . This dataset com-
prises 1,032 unanswerable questions, which are dis-
tributed across five distinct categories, along with
an additional 2,337 questions that are classified as
answerable. Experimental results on GPT-3, In-
structGPT, LLaMA, and other LLMs demonstrate
that in-context learning and instruction tuning can
effectively enhance the self-knowledge of LLMs.
However, the self-knowledge exhibited by the cur-
rent state-of-the-art model, GPT-4, measures at
75.47%, signifying a notable disparity when con-
trasted with human self-knowledge, which is rated
at 84.93%.
Our key contributions to this field are summa-
rized as follows:•We have developed a new dataset, SelfAware ,
that comprises a diverse range of commonly
posed unanswerable questions.
•We propose an innovative evaluation tech-
nique based on text similarity to quantify the
degree of uncertainty inherent in model out-
puts.
•Through our detailed analysis of 20 LLMs,
benchmarked against human self-knowledge,
we identified a significant disparity between
the most advanced LLMs and humans1.
2 Dataset Construction
To conduct a more comprehensive evaluation of
the model’s self-knowledge, we constructed a
dataset that includes a larger number and more di-
verse types of unanswerable questions than Know-
Unknowns dataset (Srivastava et al., 2022). To
facilitate this, we collected a corpus of 2,858 unan-
swerable questions, sourced from online platforms
like Quora and HowStuffWorks. These questions
were meticulously evaluated by three seasoned an-
notation analysts, each operating independently.
The analysts were permitted to leverage external
resources, such as search engines. To ensure the va-
lidity of our dataset, we retained only the questions
that all three analysts concurred were unanswerable.
This rigorous process yielded a finalized collection
of 1,032 unanswerable questions.
In pursuit of a comprehensive evaluation, we
opted for answerable questions drawn from three
datasets: SQuAD (Rajpurkar et al., 2016), Hot-
potQA (Yang et al., 2018), and TriviaQA (Joshi
et al., 2017). Our selection was guided by Sim-
CSE (Gao et al., 2021), which allowed us to iden-
tify and select the answerable questions semanti-
cally closest to the unanswerable ones. From these
sources, we accordingly drew samples of 1,487,
182, and 668 questions respectively, amassing a
total of 2,337. Given that these questions can be
effectively addressed using information available
on Wikipedia, the foundational corpus for the train-
ing of current LLMs, it is plausible to infer that
the model possesses the requisite knowledge to
generate accurate responses to these questions.
Our dataset, christened SelfAware , incorporates
1,032 unanswerable and 2,337 answerable ques-
tions. To reflect real-world distribution, our dataset
1The code pertinent to our study can be accessed
https://github.com/yinzhangyue/SelfAwareCategory Description Example Percentage
No scientific
consensusThe answer is still up
for debate, with no consensus
in scientific community.“Are we alone in the universe,
or will we discover alien
life at some point?”25%
ImaginationThe question are about people’s
imaginations of the future."What will the fastest form of
transportation be in 2050?"15%
Completely
subjectiveThe answer depends on
personal preference."Would you rather be shot
into space or explore the
deepest depths of the sea?"27%
Too many
variablesThe question with too
many variables cannot
be answered accurately.“John made 6 dollars mowing lawns
and 18 dollars weed eating.
If he only spent 3 or 5 dollar a week,
how long would the money last him?”10%
PhilosophicalThe question can yield
multiple responses, but it
lacks a definitive answer.“How come god was
born from nothingness?”23%
Table 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.
contains a proportion of answerable questions that
is twice as large as the volume of unanswerable
ones. Nevertheless, to ensure the feasibility of test-
ing, we have purposefully capped the number of
answerable questions.
2.1 Dataset Analysis
To gain insight into the reasons precluding a cer-
tain answer, we undertook a manual analysis of
100 randomly selected unanswerable questions. As
tabulated in Table 1, we have broadly segregated
these questions into five distinctive categories. “No
Scientific Consensus" encapsulates questions that
ignite ongoing debates within the scientific com-
munity, such as those concerning the universe’s
origin. “Imagination" includes questions involving
speculative future scenarios, like envisaged events
over the next 50 years. “Completely Subjective"
comprises questions that are inherently personal,
where answers depend heavily on individual predis-
positions. “Too Many Variables" pertains to mathe-
matical problems that become unsolvable owing to
the overwhelming prevalence of variables. Lastly,
“Philosophical" represents questions of a profound,
often metaphysical, nature that resist concrete an-
swers. Ideally, upon encountering such questions,
the model should express uncertainty instead of
delivering conclusive responses.
3 Evaluation Method
This section elucidates the methodology employed
for assessing self-knowledge in the generated text.In order to achieve this, we define a similarity func-
tion,fsim, to compute the similarity, S, between
a given sentence, t, and a collection of reference
sentences, U={u1, u2, . . . , u n}, endowed with
uncertain meanings.
Si=fsim(t, ui). (1)
Whenever any Sisurpasses a pre-determined
threshold T, we perceive the text tas encompass-
ing uncertain meanings, thereby eliminating the
need for manual evaluation of the response.
Given the substantial disparity in the volume of
answerable and unanswerable questions in Self-
Aware , we adopt the F1 score as a measure of
LLMs’ self-knowledge. Our focus rests on identi-
fying unanswerable questions, hence we designate
them as positive cases and categorize answerable
questions as negative cases.
4 Experiment
4.1 Model
We conduct a sequence of experiments to evaluate
the degree of self-knowledge manifested by various
LLMs, including GPT-3 (Brown et al., 2020) and
InstructGPT (Ouyang et al., 2022) series, as well
as the recent LLaMA (Touvron et al., 2023) and
its derivative models, namely Alpaca (Taori et al.,
2023) and Vicuna (Chiang et al., 2023). Our in-
vestigative approach employed three distinct input
forms: Direct, Instruction, and In-Context Learn-
ing (ICL), which is encapsulated in Appendix A.4.350M1.3B 6.7B 175B203040506070F1 Scores
22.3840.11
26.9640.33
26.1743.47
27.5444.87Direct
350M1.3B 6.7B 175B203040506070F1 Scores
30.4242.31
30.1745.91
33.3348.79
45.6749.61Instruction
350M1.3B 6.7B 175B203040506070F1 Scores
34.2747.93
36.2748.4247.2455.81 55.565.12In-Context Learning
GPT-3
InstructGPT
ModelFigure 2: Experimental results using three different input forms on a series of models from GPT-3(ada, babbage,
curie, and davinci) and InstructGPT(text-ada-001, text-babbage-001, text-curie-001, and text-davinci-001)
0 10 20 30 40 50 60 70 80
F1 Scoresdavincitext-davinci-001text-davinci-002text-davinci-003gpt-3.5-turbo-0301gpt-4-0314HumanModels
45.6749.6147.4851.4354.1275.4784.93
Figure 3: Comparison between the davinci series and
human self-knowledge in instruction input form.
4.2 Setting
We devised the reference sentence set Uthrough
a process that combined automated generation by
LLMs and manual filtering, detailed further in Ap-
pendix A.1. To quantify the similarity between
target and reference sentences, we utilized Sim-
CSE (Gao et al., 2021), setting the similarity thresh-
old to 0.75 during our experiments. An exploration
of threshold ablation is available in Appendix A.2.
To counteract potential errors in similarity calcula-
tion induced by varying lengths of the target and
reference sentences, we employed a sliding win-
dow of length 5 to parse the target sentence into
semantic chunks. During the generation process,
we set the temperature to 0.7. We selected a ran-
dom sample of 100 instances for GPT-4, while the
remainder of the models were scrutinized using the
fullSelfAware dataset.
4.3 Human Self-Knowledge
To establish a benchmark for human self-
knowledge, we engaged two volunteers and se-
lected 100 random samples from the SelfAware
dataset. The volunteers has 30 minutes to make
davinci
text-davinci-001 text-davinci-002 text-davinci-003
gpt-3.5-turbo-0301
Models0102030405060F1 Scores55.565.1266.46 66.28
60.86Figure 4: Experimental comparison of davinci series in
ICL input form.
judgments on the same set of questions, yielding
an average F1 score of 84.93%, which we sub-
sequently adopted as the benchmark for human
self-knowledge. Detailed scores are available in
Appendix A.3.
4.4 Analysis
We evaluate the manifestation of LLMs’ self-
knowledge, centering our investigation on three
fundamental dimensions: the size of the model,
the impact of instruction tuning, and the influence
exerted by different input forms.
Model Size. Figure 2 illustrates the correlation
between model size and self-knowledge across var-
ious LLMs. It is noteworthy that across all three
input forms, an augmentation in model parameter
size is associated with an elevation in the F1 Score,
with the most conspicuous enhancement manifest-
ing in the ICL input form. Therefore, our analysis
indicates that an LLM’s self-knowledge tends to
enhance with increasing model size, a trend consis-
tent with the scaling law.LLaMA-7B Alpaca-7B Vicuna-7BLLaMA-13B Alpaca-13B Vicuna-13B LLaMA-30B LLaMA-65B
Models01020304050F1 Scores28.5735.8742.78
30.1237.4447.84
30.346.89Figure 5: Experimental results obtained from LLaMA
and its derived models, Alpaca and Vicuna in instruction
input form.
Instruction Tuning. Figure 2 delineates that
models from the InstructGPT series exhibit a su-
perior level of self-knowledge compared to their
GPT-3 counterparts. Further evidence of model
enhancement is provided by Figure 4, where text-
davinci models show significant improvement rela-
tive to the base davinci model. An additional com-
parative analysis, presented in Figure 5, evaluates
LLaMA against its derivative models. The results
underscore a notable increase in self-knowledge
for Alpaca and Vicuna upon instruction tuning, ex-
ceeding their base model performances. Among
these, Vicuna-13B outperforms the LLaMA-65B,
corroborating the efficacy of instruction tuning for
enhancing model self-knowledge.
Input Forms. As shown in Figure 2, the incorpo-
ration of instructions and examples serves to boost
the self-knowledge of both the GPT-3 and Instruct-
GPT series. Specifically, ICL input form, providing
richer contextual information, contributes to a sig-
nificant enhancement in models’ self-knowledge.
This impact is particularly noticeable in the davinci
model, where ICL facilitates a 27.96% improve-
ment over the direct. Moreover, a comparison be-
tween Figure 3 and Figure 4 reveals that the in-
clusion of instructions and examples successfully
minimizes the performance disparity between the
davinci and text-davinci models, suggesting an ac-
quisition of self-knowledge from the instructions
and provided examples.
Compared with Human. Figure 3 reveals that,
without supplementary samples, GPT-4 currently
performs best among the tested models, achieving
an impressive F1 score of 75.47%. However, a no-
ticeable gap becomes evident when comparing this
text-ada-001
text-babbage-001text-curie-001text-davinci-001 text-davinci-002 text-davinci-003
gpt-3.5-turbo-0301gpt-4-0314
Models0510152025303540Accuracy
2.484.45 4.710.6115.730.2538.2942.64Figure 6: Accuracy of the InstructGPT series when
responding to answerable questions in instruction input
form.
performance to the human benchmark of 84.93%.
This underscores the considerable potential that re-
mains for enhancing the self-knowledge level of
LLMs.
Answerable Questions. Figure 6 traces the per-
formance evolution of the InstructGPT series in
addressing answerable questions, adhering to the
closed-book question answering paradigm (Tou-
vron et al., 2023), where output accuracy is con-
tingent on the presence of the correct answer. Our
observations underscore a steady enhancement in
QA task accuracy corresponding to an increase
in model parameter size and continuous learning.
Particularly, the accuracy of text-davinci-001 expe-
riences a significant ascent, scaling from a meager
2.48% in text-ada-001 to 10.61%, whereas GPT-4
marks an even more striking jump to 42.64%.
5 Conclusion
This study investigates the self-knowledge of
LLMs by evaluating their ability to identify unan-
swerable questions. Through the introduction of a
novel dataset and an automated method for detect-
ing uncertainty in the models’ responses, we are
able to accurately measure the self-knowledge of
LLMs such as GPT-3, InstructGPT and LLaMA.
Our results reveal that while these models possess
a certain degree of self-knowledge, there is still
an apparent disparity in comparison to human self-
knowledge. This highlights the need for further
research in this area to enhance the ability of LLMs
to understand their own limitations on the unknows.
Such efforts will lead to more accurate and reliable
responses from LLMs, which will have a positive
impact on their applications in diverse fields.Limitations
•Generalization of reference sentences. At
present, we have selected sentences with un-
certain meanings exclusively from the GPT-3
and InstructGPT series, potentially overlook-
ing uncertainty present in responses generated
by other LLMs. However, it is not feasible
to catalog all sentences with uncertain mean-
ings exhaustively. As a direction for future
research, we propose to concentrate on the
automated acquisition of more accurate refer-
ence sentences to address this concern.
•Limitations of input forms: Our exami-
nation was confined to three unique input
forms: direct, instruction, and ICL. There
is burgeoning research aimed at bridging the
gap between models and human-like meth-
ods of reasoning and problem-solving, includ-
ing but not limited to approaches like Re-
flexion (Shinn et al., 2023), ToT (Yao et al.,
2023), MoT (Li and Qiu, 2023). Future en-
deavors will integrate additional cognitive and
decision-making methods to delve deeper into
the self-knowledge exhibited by these LLMs.
Ethics Statement
The SelfAware dataset, meticulously curated to
evaluate LLMs’ ability to discern unanswerable
questions, is composed of unanswerable questions
extracted from sources such as Quora and How-
StuffWorks, alongside answerable questions pro-
cured from three distinct open datasets. Every ques-
tion was thoroughly examined for relevance and
harmlessness. To ensure content validity, three an-
notation analysts, compensated at local wage stan-
dards, dedicated regular working hours to content
review.
Throughout our research process, we under-
scored the significance of privacy, data security,
and strict compliance with dataset licenses. In
order to protect data integrity, we implemented
anonymization and content filtration mechanisms.
Our adherence to OpenAI’s stipulations remained
unyielding for the usage of GPT-3 and InstructGPT
models, and likewise for Meta’s terms pertaining
to LLaMA models. We rigorously vetted the li-
censes of the three publicly available datasets for
compliance, ensuring that all our research method-
ologies were in alignment with ethical standards at
the institutional, national, and global levels.Adhering to the CC-BY-SA-4.0 protocol, the
dataset, once publicly released, will be reserved
exclusively for research purposes. We pledge to
promptly and effectively address any concerns relat-
ing to the dataset, while concurrently anticipating
researchers to maintain high ethical standards in
their utilization of this data.
Acknowledgement
We wish to express our gratitude to our colleagues
in the FudanNLP group whose insightful sugges-
tions, perspectives, and thought-provoking discus-
sions significantly contributed to this work. Our
sincere appreciation also extends to the anonymous
reviewers and area chairs, whose constructive feed-
back was instrumental in refining the quality of
our study. This work was supported by the Na-
tional Natural Science Foundation of China (No.
62236004 and No. 62022027) and CAAI-Huawei
MindSpore Open Fund.
References
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, Eric Chu, Jonathan H. Clark, Laurent El
Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
rav Mishra, Erica Moreira, Mark Omernick, Kevin
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez
Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,
Jan Botha, James Bradbury, Siddhartha Brahma,
Kevin Brooks, Michele Catasta, Yong Cheng, Colin
Cherry, Christopher A. Choquette-Choo, Aakanksha
Chowdhery, Clément Crepy, Shachi Dave, Mostafa
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Gar-
cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-
Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua
Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur-
witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-
ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,
Sneha Kudugunta, Chang Lan, Katherine Lee, Ben-
jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,
Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,
Frederick Liu, Marcello Maggioni, Aroma Mahendru,
Joshua Maynez, Vedant Misra, Maysam Moussalem,
Zachary Nado, John Nham, Eric Ni, Andrew Nys-
trom, Alicia Parrish, Marie Pellat, Martin Polacek,
Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,
Bryan Richter, Parker Riley, Alex Castro Ros, Au-
rko Roy, Brennan Saeta, Rajkumar Samuel, Renee
Shelby, Ambrose Slone, Daniel Smilkov, David R.
So, Daniel Sohn, Simon Tokumine, Dasha Valter,
Vijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,
Pidong Wang, Zirui Wang, Tao Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven
Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav
Petrov, and Yonghui Wu. 2023. Palm 2 technical
report.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. ArXiv preprint ,
abs/2211.12588.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,
and Tushar Khot. 2022. Complexity-based prompt-
ing for multi-step reasoning. ArXiv preprint ,
abs/2210.00720.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 1601–1611, Vancouver,
Canada. Association for Computational Linguistics.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield Dodds, Nova DasSarma,
Eli Tran-Johnson, et al. 2022. Language models
(mostly) know what they know. ArXiv preprint ,
abs/2207.05221.
Aitor Lewkowycz, Anders Andreassen, David Dohan,
Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,Ambrose Slone, Cem Anil, Imanol Schlag, Theo
Gutman-Solo, et al. 2022. Solving quantitative
reasoning problems with language models. ArXiv
preprint , abs/2206.14858.
Xiaonan Li and Xipeng Qiu. 2023. Mot: Pre-
thinking and recalling enable chatgpt to self-
improve with memory-of-thoughts. ArXiv preprint ,
abs/2305.05181.
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow in-
structions with human feedback. ArXiv preprint ,
abs/2203.02155.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for SQuAD. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers) , pages 784–789,
Melbourne, Australia. Association for Computational
Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Noah Shinn, Federico Cassano, Beck Labash, Ashwin
Gopinath, Karthik Narasimhan, and Shunyu Yao.
2023. Reflexion: Language agents with verbal rein-
forcement learning.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta, Adrià
Garriga-Alonso, et al. 2022. Beyond the imitation
game: Quantifying and extrapolating the capabilities
of language models. ArXiv preprint , abs/2206.04615.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. ArXiv
preprint , abs/2302.13971.
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Kaheer
Suleman. 2017. NewsQA: A machine comprehen-
sion dataset. In Proceedings of the 2nd Workshop
on Representation Learning for NLP , pages 191–200,
Vancouver, Canada. Association for Computational
Linguistics.Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, and Denny Zhou. 2022. Self-consistency im-
proves chain of thought reasoning in language mod-
els.ArXiv preprint , abs/2203.11171.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems .
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D. Manning. 2018. HotpotQA: A dataset for
diverse, explainable multi-hop question answering.
InProceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing , pages
2369–2380, Brussels, Belgium. Association for Com-
putational Linguistics.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L Griffiths, Yuan Cao, and Karthik
Narasimhan. 2023. Tree of thoughts: Deliberate
problem solving with large language models. ArXiv
preprint , abs/2305.10601.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Olivier Bousquet, Quoc Le, and Ed Chi. 2022.
Least-to-most prompting enables complex reason-
ing in large language models. ArXiv preprint ,
abs/2205.10625.A Appendix
A.1 Uncertainty Text
To assemble a set of reference sentences, we ran-
domly chose 100 entries from the SelfAware dataset.
For each model in the GPT-3 and InstructGPT se-
ries, we conducted a preliminary test using the
direct input form and manually curated sentences
that displayed uncertainty. From this pre-test, we
procured 16 sentences manifesting uncertain con-
notations to serve as our reference sentences. After
normalizing these sentences by eliminating punc-
tuation and converting to lowercase, we utilized
them to compute similarity with target sentences
throughout our experimental procedure.
1. The answer is unknown.
2. The answer is uncertain.
3. The answer is unclear.
4. There is no scientific evidence.
5. There is no definitive answer.
6. There is no right answer.
7. There is much debate.
8. There is no known case.
9. There is no concrete answer to this question.
10. There is no public information available.
11. It is impossible to know.
12. It is impossible to answer.
13. It is difficult to predict.
14. It is not known.
15. We do not know.
16. I’m not sure.
A.2 Threshold ablation
We generated 100 new responses using the text-
davinci-002 with direct input form and manually
filtered out sentences that contained uncertainty.
We then used SimCSE (Gao et al., 2021) to calcu-
late the similarity between these sentences and the
reference sentences in Appendix A.1. We tested
various thresholds for filtering sentences with un-
certain meanings and compared them to manuallyThreshold Precision Recall F1
0.95 100.00 70.00 82.35
0.90 100.00 75.00 85.71
0.85 100.00 75.00 85.71
0.80 100.00 80.00 88.89
0.75 100.00 85.00 91.89
0.70 89.47 90.00 89.73
0.65 86.95 90.00 88.45
Table 2: Evaluation results comparing sentences with
uncertain meaning filtered by various thresholds.
Human Precision Recall F1
V olunteer A 91.52 78.26 84.37
V olunteer B 96.36 76.81 85.48
Table 3: Evaluation results of 100 responses from two
volunteers.
annotated sentences. We considered unanswerable
questions as positive examples and calculated preci-
sion, recall, and F1 score. The results in Table 2 in-
dicate that a threshold of 0.75 produced the highest
F1 score, balancing precision and the inclusion of
other uncertain sentences. As a result, we selected
0.75 as the similarity threshold for subsequent ex-
periments.
A.3 Human Self-Knowledge Test
The evaluation results for the responses from our
invited volunteers are presented in Table 3. The
F1 scores for the responses were high, indicating
that both volunteers exhibited a strong level of self-
knowledge.
A.4 Template
The input templates used in our experiments, Di-
rect, Instruction, and ICL, are illustrated in Fig-
ures 7, 8, and 9, respectively. In the ICL template,
we composed 3 answerable and 3 unanswerable
questions and provided the corresponding answers
manually.Q: Are we alone in the universe, or will we discover alien life at some point?
A:Figure 7: Direct input form.
When answering questions, interpret them literally and think them carefully. If the question is 
unanswerable or unknowable, it is appropriate to say, “The answer is unknown.”
Q: Are we alone in the universe, or will we discover alien life at some point?
A:
Figure 8: Instruction input form.
Q: What is the nearest extraterrestrial civilization to us?
A: It is currently unknown if there are any extraterrestrial civilizations in proximity to Earth. Scientists have not yet found definitive evidence of the existence of extraterrestrial civilizations. Thus, the answer is unknown.Q: What is the highest building in New York?
A: The highest building in New York is the One World Trade Center. It stands at a height of 1,776 feet (541 
meters). Thus, the answer is the One World Trade Center.Q: Will nuclear war break out in the world in 2050?A: It is impossible to predict with certainty if a nuclear war will break out in the world in 2050. The likelihood of such an event will depend on a wide range of factors, including international relations, political tensions, and military capabilities. Thus, the answer is unknown.Q: Alice's class is going on a field trip to the Ocean Park. There are 39 students and several teachers in the class. A bus can seat 20 people. How many buses do they need?
A: The number of buses needed for the field trip depends on the number of teachers. If there is only one 
teacher, then two buses will suffice. However, additional buses will be necessary if there are two or more teachers. Thus, the answer is unknown.Q: Are Local H and For Against both from the United States?A: Local H is a rock band from Zion, Illinois, United States. For Against is an American indie rock band formed in 1984 in Lincoln, Nebraska. Both of these bands are from the United States. Thus, the answer is 
yes.Q: Gjetost is the national cheese of which country?A:It is the national cheese of Norway, and it is a popular ingredient in traditional Norwegian cuisine. Thus, 
the answer is Norway.
Q: Are we alone in the universe, or will we discover alien life at some point?
A:
Figure 9: ICL input form.