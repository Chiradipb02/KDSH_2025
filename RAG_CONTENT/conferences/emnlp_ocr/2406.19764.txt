Belief Revision: The Adaptability of
Large Language Models Reasoning
Bryan Wilie*, Samuel Cahyawijaya∗, Etsuko Ishii, Junxian He, Pascale Fung
Hong Kong University of Science and Technology
Clear Water Bay, Hong Kong
bwilie@connect.ust.hk
Abstract
The capability to reason from text is crucial
for real-world NLP applications. Real-world
scenarios often involve incomplete or evolving
data. In response, individuals update their be-
liefs and understandings accordingly. However,
most existing evaluations assume that language
models (LMs) operate with consistent informa-
tion. We introduce Belief-R1, a new dataset
designed to test LMs’ belief revision ability
when presented with new evidence. Inspired by
how humans suppress prior inferences, this task
assesses LMs within the newly proposed delta
reasoning ( ∆R) framework. Belief-R features
sequences of premises designed to simulate sce-
narios where additional information could ne-
cessitate prior conclusions drawn by LMs. We
evaluate ∼30 LMs across diverse prompting
strategies and found that LMs generally strug-
gle to appropriately revise their beliefs in re-
sponse to new information. Further, models
adept at updating often underperformed in sce-
narios without necessary updates, highlighting
a critical trade-off. These insights underscore
the importance of improving LMs’ adaptive-
ness to changing information, a step toward
more reliable AI systems.
1 Introduction
Human reasoning is characterized by its ability to
deal with partial or evolving information. When
new information becomes available, we dynami-
cally update our beliefs. We reevaluate and adjust
our initial premises or conclusions as necessary
in light of this new evidence (Łukaszewicz, 1990;
Brewka, 1991). For instance, knowing Tweety is a
bird, we conclude that it flies since birds usually
fly. Discovering Tweety is a penguin , we retract
the conclusion but not the other premises; we still
*These authors contributed equally.
1The code and dataset are available at https://github
.com/HLTCHKUST/belief-revision
Figure 1: Belief revision allows reasoners to update
their belief based on the new provided evidence. Such
ability is necessary to enable better logical reasoning on
the case of defeasible inference.
believe Tweety is a bird and that birds typically fly ,
however, we now conclude that it cannot fly since
we know that penguins cannot fly . This form of
reasoning permits new information to undermine
prior beliefs, which necessitates the ability of belief
revision (Gärdenfors, 1988, 1991; Rott, 2001).
The ability to adjust beliefs allows better adapt-
ability of AI systems by enabling them to properly
revise prior inferences as further evidence emerges,
such as in commonsense inferences (Brewka et al.,
1997; Etherington, 1986; Pfeifer and Kleiter, 2005)
and decision-making (Antoniou and Williams,
1997; Dubois et al., 2002). Despite this, recent
reasoning evaluations of state-of-the-art AI tech-
nologies, such as language models (LMs), primar-
ily focus on its ability to draw conclusions assum-
ing complete information (c.f. Bhagavatula et al.
(2020); Han et al. (2024); Kazemi et al. (2024)).
While these evaluations useful to demonstrate the
reasoning abilities of LMs, they fail to capture the
concept of belief change.arXiv:2406.19764v2  [cs.CL]  17 Oct 2024Features bAbI 15 FOLIO Proof Writer Leap of Thought αNLI BoardgameQA PropInd Belief-R
Incomplete info ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓
Contradictory info ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓
Belief revision ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓
Table 1: The comparison of Belief-R with other widely-used logical reasoning datasets. Belief-R uniquely examines
scenarios potentially necessitating belief updates. Belief-R specifically evaluates the capability of belief revision,
assessing whether prior beliefs should be adjusted or retained depending of the significance of the new information.
We introduce Belief-R, the first-of-a-kind diag-
nostic reasoning evaluation dataset designed to as-
sess inferences involving belief revision. Belief-
R is inspired by the concept of the Suppression
Task (Byrne, 1989) which enables the retraction of
previously inferred beliefs by the introduction of
new contextual premises, mimicking how humans
reassess their inferences when presented with addi-
tional context. To allow a specific and measurable
evaluation on belief revision, we introduce a new
reasoning evaluation setting dubbed as delta rea-
soning ( ∆R)framework. Within ∆R, evaluation
is done within two sequential reasoning steps. We
start by presenting LMs with two initial premises
that satisfy basic logical inference rule to assess
its basic inference ability. We expect the model
to make accurate inferences to establish the prior
beliefs. Then, we introduce another premise to
see if the model adjusts its beliefs or keeps them
unchanged, depending on the significance of the
newly introduced information to the initial beliefs.
Belief-R is specifically designed to support the
belief revision evaluation through the ∆R frame-
work. Each sample in Belief-R is equipped with
two initial premises that support basic modus
ponens or modus tollens inferences, and a new
premise that brings in new information that might
modify previously held beliefs. We synthetically
generate the premises in Belief-R leveraging on
publicly available dataset, and manually annotate
the new information significances along with the
ground truth answers through multiple human an-
notators and majority voting. As illustrated in Fig-
ure 2, Belief-R uniquely facilitates thorough evalu-
ations of belief revision capabilities.
Through Belief-R, we evaluate the belief revi-
sion ability of small and large scale LMs using
different prompting techniques. Our study shows
that these models are incapable of revising their
prior beliefs. We further reveal a critical limitation:
they confront a performance trade-off between up-
dating and maintaining their prior beliefs. Models
that perform better in the cases where an update
is needed, typically faltered on the other. Further-more, better prompting methods also fail to sig-
nificantly enhance this capability. These insights
underscore a need for strategies to enhance model’s
capability to correctly update or maintain its initial
beliefs when faced with new evidence to ensure its
reliability across evolving scenarios.
2 Related Works
Belief revision Belief revision is the process of
changing beliefs to take into account a new piece
of information. In AI systems, one of its early
implementation is through procedures by which
databases can be updated, i.e. for recording and
maintaining reasons for system beliefs (Doyle,
1979; Falappa et al., 2002; Hansson, 2022). No-
tably, Alchourrón et al. (1985) created formal
frameworks to determine how beliefs should be
updated in a rational manner. The core challenge in
belief revision is deciding rationally which prior be-
liefs to modify, retain, or discard when confronted
with new evidence (Rott, 2001). Consequently in
this paper, we look at how LMs handle belief revi-
sion. Belief in LMs can be thought of as models’
output (Li et al., 2019; Jang et al., 2022; Wang
et al., 2023a). Several works revise LMs’ beliefs
through updating its parameter directly or via fine-
tuning (De Cao et al., 2021; Dai et al., 2021; Hase
et al., 2023). However, this process is not a ratio-
nal process of the model itself (Hofweber et al.,
2024). Moreover, it relies on pre-prepared knowl-
edge, which is not ideal if we envision LMs to help
with discovering new things (Ban et al., 2023; Ma
et al., 2024). In this work, we assess LMs’ belief
revision capabilities through its response towards
queries that neccessitate judgement on whether it
needs to update its prior beliefs or keep it.
Language model reasoning evaluation Rea-
soning is one of the fundamental intelligent be-
haviors, essential for solving complex real-world
tasks (Huang and Chang, 2023). One works test
this behaviour by creating simple tasks to compre-
hensively check if a system can answer questions
by connecting facts or using basic logic (Westonet al., 2016). Others design more advanced tests
to evaluate inductive, deductive, and abductive rea-
soning (Sinha et al., 2019; Saparov et al., 2024;
Bhagavatula et al., 2020). Some benchmarks repli-
cate real-world complexities by presenting partial
or conflicting informations (Arabshahi et al., 2021;
Sprague et al., 2022; Han et al., 2024; Kazemi et al.,
2024). Belief revision focuses on the adaptability
problem: whether the model properly revises prior
beliefs as new information emerges. We extend
the reasoning evaluation by focusing on scenarios
where information evolves, presenting queries that
require dynamic updates of prior beliefs in light
of new evidence. This is distinct from other exist-
ing reasoning tasks involving incomplete or con-
tradictory information, i.e. in Talmor et al. (2020);
Bhagavatula et al. (2020); Kazemi et al. (2024),
since they assume a static environment and focus
on filling the gap or resolving the contradiction.
We further note the comparison in Table 1.
3 Belief Revision
Belief revision is the ability to adapt the reason-
ing process in response to new information. This
capability is critical as it ensures rational decision-
making in the face of incomplete and evolving na-
ture of available information (Nute, 2001; Makin-
son and Gärdenfors, 2005; Ribeiro et al., 2019). In
this section, we introduce the concept of belief re-
vision and its notation, and propose the evaluation
framework for belief revision capabilities.
3.1 Background and notation
For set of query sentences χ, it encompasses a set
of premises Γ={γ1, . . . , γ N}that could imply a set
of conclusions Φ={φ1, . . . , φ M}. In this work, we
conceptualize "belief" similarly to its usage in dia-
logue systems, where it represents what the system
currently considers true based on the context (Feng
et al., 2023; van Niekerk et al., 2020). We denote
reasoner’s belief set as a set of sentences Bto rep-
resent a contextually fixed background knowledge
ofχ. In this regard, Bis a tuple that contains set
of premises and conclusions: B=(Γ,Φ). In pres-
ence of new information γN+1, the belief revision
concept allow us to infer conclusion φM+1if it is
rational to believe φM+1after acknowledging γN+1.
Belief revision operation The belief revision op-
eration is to update belief set Bwith a new piece
of information, γN+1. Here, the result of operation
must always be that the beliefs does not contradictone another to avoid inconsistencies among them.
The significance of the new information γN+1, de-
cides whether it fits with or modifies the existing
beliefs after performing the belief revision opera-
tion. The operation should smoothly incorporate
γN+1and yield a new conclusion φM+1as long as
it does not conflict, thereby justifying the mainte-
nance of the reasoner’s prior beliefs. However, if
it conflicts, we update the initial beliefs Bappro-
priately, i.e., by retracting any prior conclusions in
Φ, to incorporate the new, conflicting information
γN+1to resolve any inconsistencies as we yield the
correct φM+1. The process to figure out what fol-
lows from the revised beliefs is then essentially to
infer the new conclusion φM+1.
3.2 Evaluating belief revision with ∆R
We introduce a novel delta reasoning ( ∆R)frame-
work, to study how LMs adapt their reasoning when
presented with new information over successive
timesteps. In this framework, we focus on under-
standing how model responds to query changes at
two essential, consecutive reasoning steps at tand
t+1. We do this by comparing responses to prior
queries at step t,χt, and the next query at step t+1,
χt+1, adding the new information γN+1.
To begin with, we need χtto minimally include
two premises, i.e. {γ1, γ2}, and at least imply con-
clusion φ1. We set χtto be basic as we expect LMs
to answer it in high accuracy to help establish the
prior belief and not be affected by the inconsisten-
cies in LMs’ behaviour (Jang et al., 2022; Kassner
et al., 2021; Hase et al., 2023). We then add the
new information γN+1as another premise γ3in
χt+1such that χt+1={γ1, γ2, γ3}. We examine the
corresponding conclusion, φM+1, to see how the
beliefs shifts according to the significance of γ3.
One way to set χtas basic, is to state them as
premises that could satisfy basic logical inference
rules of modus ponens and modus tollens (Wa-
son and Johnson-Laird, 1972; Haack, 1978; Evans,
1982). Modus ponens and modus tollens is a valid
form of inference that have been made a central
principle in many propositional and modern log-
ics (Copi, 1972; Haack, 1978). Modus ponens rule
of inference states that the premises “if pthenq” is
true and pis true ( p→q, p) satisfy modus ponens
conclusion that qmust be true ( q). Modus tollens
rule of inference states that the premises “if pthen
q” is true and qis false ( p→q,¬q) satisfy modus
tollens conclusion that pmust be false ( ¬p).
In this setup, we are able to evaluate how wellIf she has an essay to finish then she will study late in
the library
She has an essay to finish
If the library stays open then she will study late in
the library
What necessarily had to follow assuming that the above
premises were true?
(a) She will study late in the library.
(b) She will not study late in the library.
(c) She may or may not study late in the library. ✓If she has an essay to finish then she will study late in
the library
She has an essay to finish
If she has some textbooks to read then she will study
late in the library
What necessarily had to follow assuming that the above
premises were true?
(a) She will study late in the library. ✓
(b) She will not study late in the library.
(c) She may or may not study late in the library.
Figure 2: Human reasoning adapts based on new information, leading us to adjust our prior beliefs. Here, the
additional condition (left) casts doubt on prior modus ponens conclusion in (a). People may consider that certain
other conditions necessary for this conclusion to hold, i.e., the library must remain open . In contrast, the alternative
argument (right) does not affect the modus ponens inference pathway, thus prior conclusion could still hold.
the models revise its beliefs after the introduction
of new information in γ3. We measure the model’s
dynamic reasoning ability: whether it can correctly
update or maintain its initial beliefs when con-
fronted with new information that may contradict
prior beliefs. Through this approach, we can as-
sess both how accurate and how flexible different
reasoning models are in evolving scenarios.
Example Figure 2 presents a scenario where the
initial two premises at step t,γ1andγ2, adhere to a
basic inference rule, modus ponens ( p→q, p⊢q),
implying a φ1conclusion of q:She will study late
in the library . These premises: γ1,γ2, andφ1, form
the belief set B. Subsequently, we introduce the
third premise γ3, i.e., another conditional ( r→q)
“if the library stays open then she will study late in
the library”, as the new information in query χt+1
and evaluate model’s answer at step t+1. This sets
the stage to execute the belief revision operation.
Recall B= {γ1: If she has an essay to finish
then she will study late in the library., γ2: She
has an essay to finish., φ1: She will study late in
the library.}, and γ3= {If the library stays open
then she will study late in the library.}. The intro-
duction of γ3suggests that “the library being open”
is a sufficient condition for her to “study late in
the library”. However, people might consider it as
a necessary condition for φ1. This would involve
commonsense reasoning step to recognize that de-
spite the conditions set by γ1andγ2, the actual
feasibility of her studying late as concluded in φ1
might inherently depend on the library’s availabil-
ity. Thus, while γ3does not explicitly redefine the
dependency of φ1on the library’s status, it implies
a scenario where such a dependency could be rea-
sonably inferred. Consequently, we retract φ1and
infer the new conclusion φ2: “She may or may notstudy late in the library”.
4 The Belief-R Dataset
Belief-R is designed to specifically assess the be-
lief revision capability through the ∆R framework.
To account for this, we adopt a reasoning task that
has been extensively studied in cognitive science:
the suppression task (Byrne, 1989). Typically, this
task employs a trio of premises γ1, γ2, γ3that ac-
companied by three possible conclusions, i.e. as
exemplified in Figure 2 for modus ponens: (a) She
will study late in the library (q), (b) She will not
study late in the library (¬q), and (c) She may or
may not study late in the library (3q∧3¬q; here
the symbol 3expresses possibility, 3qcan be read
as “possibly q”).
At step t, we form a query χtusing the first two
premises, γ1andγ2. These two premises are the
premises that respectively satisfy the modus po-
nens or modus tollens conclusion, ( p→q, p) or
(p→q,¬q). These logical rules are basic, and
we generally expect that most reasoners can apply
them accurately. Next, at step t+1, to form the
query χt+1, we introduce a third premise γ3which
is another conditional statement r→q. The addi-
tion of γ3brings in new information that might con-
flict previously held beliefs. The new information
inrcan be seen either as adding more requirements
or providing an alternative pathway, i.e. to reach
the same modus ponens conclusion q.
For instance, if γ3states if the library stays open
then she will study late , we now view r:the library
stays open as another additional requirement on
top of p. In such cases, just knowing palone isn’t
enough to conclude q: we also need rto be true,
thus the condition now becomes p∧r→q. In this
case, we retract the prior modus ponens conclusion
q, and infer the new conclusion 3q∧3¬q. Werefer to this subset of dataset as the “Belief Up-
date” (BU) category. However, in another case, γ3
could instead states if she has textbooks then she
will study late . In this case, rstands as a separate
alternative inference path that also leads to q, thus
p∨r→q. Here, pstill directly leads to q, and the
acknowledgement of rdoesn’t affect this pathway,
enabling prior conclusion to still hold. We call this
subset as the “Belief Maintain” (BM) category.
In Belief-R, the task requires the model to per-
form multi-step reasoning to manage the relevance
of information within rand decide if it needs to
update its prior beliefs at step tor not. The model
must discern the implicit commonsense and causal
links amongst given premises to identify how pand
rare related, determining if their interaction is con-
junctive ( p∧r) or disjunctive ( p∨r). Based on the
relationships, reasoner needs to determine whether
to update its initial conclusion qif the new infor-
mation rimply an additional requirement for its
prior beliefs to hold ( p∧r), or to maintain its prior
beliefs if rsimply serves as alternatives ( p∨r).
To quantitatively measure the model’s reasoning
accuracy, we provide multiple choices and ask it to
pick the most plausible conclusion. For instance,
in examples shown in Figure 2, we would expect
LMs to choose options (c) and (a) for each scenario,
which aligns with the majority choices made in the
original study (Byrne, 1989; Byrne et al., 1999).
4.1 Dataset construction
We leverage ATOMIC (Sap et al., 2019), a publicly-
available dataset of everyday commonsense reason-
ing. It contains textual descriptions of inferential
if-then knowledge (e.g., “if X pays Y a compliment,
then Y will likely return the compliment”). In addi-
tion to the textual commonsense descriptions, the
dataset also contains detailed annotation on the type
of causal dimensions, i.e. the events, causes (i.e.,
‘xIntent ’), and effects (i.e., ‘ xEffect ’, ‘oReact ’);
with “ x” and “ o” pertain to PersonX and others.
We use ATOMIC as our seed to ensure the gold-
standard validity of our dataset. We synthetically
generate Belief-R and minimally introduce vari-
ance from the LLM by instructing it to be grounded
in the context provided by the seed and not to in-
troduce new ones. We mainly utilize GPT-4 series
model as the LLM in our data generation pipeline.
4.1.1 Dataset generation process
We prompt LLM to generate the first two premises
conditioned on the events, causes (‘ xIntent ’,SplitBasic
@tBelief
UpdateBelief
MaintainAll w/ 3
premises
Inference rule
Modus ponens 956 537 335 872
Modus tollens 956 537 335 872
Effect entities
Mental states 504 276 184 460
Events 1408 798 486 1284
Total 1912 1074 670 1744
Table 2: Statistics of Belief-R dataset.
‘xNeed ’, ‘ xAttr ’), and effects (‘ xEffect ’,
‘xReact ’, ‘xWant ’, ‘oEffect ’, ‘oReact ’, ‘oWant ’).
We exclude the static elements, as we want to
focus on the dynamic causal relationships where
change or action is involved, following the original
task (Byrne, 1989). For each event, cause, and
effect in ATOMIC, we generate the first two
premises in both modus ponens, p− →qandp, and
modus tollens, p− →qand¬q. Afterwards, we
prompt LLM to generate the third premises. We
design separately the prompt for the alternative
and additional conditions (corresponding to the
BM andBUcategories) within the context in the
first premise. For the alternative condition, we
prompt the model to generate conditions that are
not related at all to pfor the conclusions qto
happen. For the additional condition, we prompt
the model to generate conditions strongly relate to
pfor this conclusions qto surely hold. Following
the original task setup, we set the same third
statement in both cases with modus ponens and
modus tollens inferences.
In our iterations, we discovered that several enti-
ties in the ATOMIC dataset are quite abstract, such
as “wants to know what he is selling” or “to analyze
the thing in question.” To make these clearer for a
general audience and to make them less ambiguous
for our study, we prompt LLM to generate more
specific examples, changing them to “asks about
the price of a pen” or “examine the pen.” To pro-
vide more clarity on the dataset generation process,
we attach the samples of prompt and generation
process in Appendix A. Further, to decide the sig-
nificance of the third premises, whether it serves as
alternative or additional condition, we conducted
majority voting among multiple human annotators.
4.1.2 Ground-truth formulation
To further validate the implied commonsense inter-
action of the third premises, whether it serves as
alternative or additional condition, we manually an-Majority
DeBERT a-v3 (86M)
GPT2 (124M)
DeBERT a-v3 (304M)
RoBERT a (355M)
GPT2 Medium (355M)
GPT2 Large (774M)
Phi-1 (1.3B)
GPT Neo (1.3B)
Phi-1.5 (1.3B)
GPT2 XL (1.5B)
GPT Neo (2.7B)
Phi-2 (2.7B)
Phi-3 Mini (3.8B)
GPT-J (6B)
Phi-3 Small (7B)
Llama-2 Chat (7B)
Llama-2 (7B)
Llama-3 (8B)
Llama-3 Instruct (8B)
Llama-2 Chat (13B)
Llama-2 (13B)
Phi-3 Medium (14B)
Claude 3 Haikku
Command-R
Claude 3 Sonnet
Llama-3 (70B)
Command-R Plus
GPT-3.5-Turbo
Mixtral (8x22B)
GPT-4-Turbo05075100Accuracy (%)Baseline
NLI-tuned
Pre-trained
Instruction-tuned
Generation APIFigure 3: Evaluation on basic logical inference capabilities in Belief-R on various LLMs sorted by the #parameters.
Pre-trained LLMs with ≥6B parameters achieves adequate accuracy ( ≥75%), while instruction-tuned LLMs achieve
the same performance on much smaller scale with ≥2.7B parameters.
notate the final conclusions through a crowdsource
annotation task at Appen3(see Appendix B). We
cater the variability arises from different interpre-
tations from diverse human readers by asking 5
workers to annotate each problem and then take the
majority voting out of them to set the agreed op-
tions as the ground truths. Upon further inspection,
we found some annotations that logically invalid,
i.e. answering ¬qin questions with modus ponens
inferences or answering pin modus tollens infer-
ences. We view such cases as non modus ponens
(or tollens) inferences and specifically treat the an-
notation similarly with answering c) 3q∧3¬q.
In Belief-R, both cases of the logical inferences
share the same third statement. To streamline
our process, we annotate only the modus ponens
samples and then extend the insight on the third
premises’ significance to the modus tollens cases.
For modus tollens cases, if the corresponding
modus ponens sample primarily supports conclu-
sion a) q, indicating no conflict with initial beliefs,
we set the correct answer to b) ¬p. Conversely, if
on the modus ponens samples the majority vote
suggests the answer c) 3q∧3¬q, implying addi-
tional requirement for the inference, we likewise
categorize the corresponding modus tollens cases
answers to be c) 3p∧3¬p. This process maintains
the consistencies of the impact of the third premise
effectively across related inference scenarios.
4.2 Quality check
Context and logical quality checks Through-
out the data construction phase, we assign one ex-
pert to review of the logical formations to ensure
they follow the intended structure. We also furthergauge the quality of the generated data by review-
ing 100 randomly chosen samples to confirm on
the context and logical consistency. We conducted
a human evaluation via Appen2, with three native
English speakers assessing each sample’s quality.
They unanimously confirmed that the conditional
relationships in the premises were logically sound
across all samples, i.e. that qentails pandqen-
tailsrin both of the conditional premises. We also
attach the annotation guidelines in Appendix B.
Dataset filtering To enhance the quality of our
dataset for more reliable evaluation, we refine it
by focusing on consensus among annotators. For
each question, we utilize answers manually labeled
by five independent workers. We measured inter-
annotator agreement using Gwet’s AC1 (Gwet,
2008)3since it is better at handling high agree-
ment scenarios, with criteria used in (Wongpakaran
et al., 2013) to interpret the level of agreement.
We initially measured the annotation agreement
which yielded a moderate score of 0.573. Further,
we observe that some beliefs are naturally more
subjective than others. To make the evaluation
dataset more rigorous, we only take the ones with
the higher agreement, as commonly practiced in
subjective tasks such as sentiment analysis (Bo-
bicev and Sokolova, 2017). We retain only ques-
tions with strong majority agreement (at least 4 out
of 5 annotators concurred). Post-filtering, as we re-
tain∼65% of the original data, the score improved
to 0.697 and indicated a substantial agreement.
3https://pypi.org/project/irrCAC/Llama-2 (7B)
GPT-J (6B)
Llama-2 (13B)
Llama-2 Chat (7B)
Llama-3 (8B)
Llama-2 Chat (13B)
Llama-3 Instruct (8B)
Phi-2 (2.7B)
GPT-3.5-Turbo
Phi-3 Medium (14B)
Phi-3 Small (7B)
Command-R Plus
Command-R
Phi-3 Mini (3.8B)
Mixtral (8x22B)
Claude 3 Haikku
Claude 3 Sonnet
Llama-3 (70B)
GPT-4-Turbo020406080100BREU Score
020406080100
Acc@t
Acc@t ½ BM-Acc ½ BU-Acc Pre-trained Instruction-tuned Generation APIFigure 4: BREU score evaluation on belief revision capabilities in Belief-R on various models sorted by the BREU
score. While larger-scale LLMs tend to achieve higher BREU score, the performance is far below their basic logical
inferences at t(Acc@t ), showcasing limited capability of LLMs in performing belief revision.
4.3 Statistics of Belief-R
Table 2 shows the composition of our dataset,
sized optimally at around 2K entries to balance
representation and computational efficiency for
LLM inferences. The dataset includes cate-
gories such as Basic @t for basic logical infer-
ences at time t, and categories like Belief Up-
date,Belief Maintain , and All w/3 premises
for the next step queries at time t+1. Addition-
ally, the table details categories inherited from the
ATOMIC dataset for the causal relationships of
If-Event-Then-Event (e.g., “promoted to senior
manager”) and If-Event-Then-Mental-State
(e.g., “learns something new”).
5 Experiment Settings
Evaluation metrics The primary goal of our ex-
periments is to investigate whether LMs possess the
capability to perform belief revision in their reason-
ing processes. Concretely, we consider the models’
predictions on the Basic @t category in the eval-
uation dataset as the models’ initial belief. Thus,
changes in accuracy on the Belief Update andBe-
lief Maintain categories directly reflects belief re-
vision. We report accuracies in the Belief Update
(BU-Acc) and the Belief Maintain (BM-Acc) sub-
sets to indicate LMs’ capabilities in updating and
maintaining their beliefs when required. We further
introduce a novel metric, BREU (Belief Revision
Evaluation Understudy), to assess LMs’ belief re-
vision ability, by averaging BU-Acc andBM-Acc
equally. The goal of BREU is to gauge whether the
3https://appen.com/model accurately decides when to update or main-
tain its prior beliefs. We then benchmark publicly-
available LMs and design series of experiments
through ∆R framework. We perform zero-shot-
classification on series of smaller to larger scales
pre-trained and finetuned LMs, and prompt LLMs
generations through API.
Models We perform zero-shot classification us-
ing encoder-only and decoder-only LMs. For
encoder-only LMs, we employ entailment-based
inference (Yin et al., 2019) using NLI-finetuned
LMs of RoBERTa (Liu et al., 2019), DeBERTa-
v3 base (Laurer et al., 2024), and DeBERTa-v3
large (Laurer et al., 2023). For decoder-only LMs,
we follow Brown et al. (2020) using GPT (Rad-
ford et al., 2019; Black et al., 2021; Wang and Ko-
matsuzaki, 2021), Llama (Touvron et al., 2023a,b;
AI@Meta, 2024), and Phi series (Gunasekar et al.,
2023; Li et al., 2023; Abdin et al., 2024).
We also include larger-scale LLMs with ≥35B
parameters. We evaluate the belief revision ca-
pability of these larger-scale LLMs via comple-
tion API through generation-based approach. We
employ three zero-shot prompting methods, i.e.,
direct prompting (DP) , triggering the genera-
tion of chain-of-thought (CoT) (Kojima et al.,
2022), or through plan and solve (PS) prompt-
ing (Wang et al., 2023b). We employ 8 large-scale
LLMs, i.e., Llama-3 70B (AI@Meta, 2024), Mix-
tral 8x22B (Jiang et al., 2024), Command R, Com-
mand R+ (Cohere, 2024), Claude 3 Haiku, Son-
net (Anthropic, 2024), GPT-3.5 Turbo, and GPT-4
Turbo (OpenAI, 2023). Here, we follow Yao et al.
(2022) and instruct the model to output the exact0 20 40 60 80
AccuracyBM-Acc
BU-Acc
BREUModus ponens Modus tollens(a) Modus Ponens / Modus Tollens
0 20 40 60 80
AccuracyEvents Mental States (b) Events / Mental States
0 20 40 60
AccuracyDP CoT PS (c) Prompting Methods
Figure 5: Performance comparisons dissected across various aspects covering distinction on modus ponens and
modus tollens, on different effect entities, and on different prompt methods.
character of the final answer as a format. We then
retrieve the final answer and report accuracy of the
final answer as the metric. When the answer does
not follow the format instructed before, we treat it
as an instruction-following error.
6 Result and Analysis
Smaller models fail even on basic logical rea-
soning tasks. We start by examining the infer-
ences through the first two premises in Belief-R. In
Figure 3, we find as the number of parameters in
LMs increases, their ability to handle basic logical
inference improves. Smaller models, with <2B
parameters, struggle with these tasks, scoring close
to the majority baseline. Models >6B parameters
do better, surpassing 75% accuracy. Pre-trained
LMs with >6B parameters achieve ≥75% accu-
racy, while instruction-tuned LMs show an emerg-
ing ability from 2.7B parameters achieving signifi-
cantly higher performance with ≥90% accuracy.
LLMs are incapable of revising their prior be-
liefs. We group our further exploration on these
LMs that performed well ( ≥75% accuracy) in ba-
sic logical inference, and evaluate their average per-
formance in Belief Maintain (BM) and Belief Up-
date (BU) subsets. Despite being a strong reasoner
on simple logic, all larger-scale LMs under study
fail to perform well on these subsets of Belief-R. In
evaluation shown in Figure 4, most of the non-API
based models perform almost 0%in BU-Acc, indi-
cating their inability on performing belief revision.
We observe that all larger-scale both open-source
and commercial LLMs perform better on the belief
revision tasks, but their performances are still very
limited, achieving at most ∼50% on BREU.
LLMs confront a trade-off between updating
and maintaining their prior beliefs. We dis-
cover a trade-off between BU-Acc and BM-Acc:models performing well on one subset typically
faltered on the other, especially in models where
the BU-Acc is not close to 0%(see Fig 4). This in-
dicates a potential tension between enhancing spe-
cific capabilities, as improving one aspect could in-
advertently weaken another. An ideal model would
excel at belief revision by consistently making the
right decision on whether the new information con-
flicts with prior beliefs or aligns with them. This
underlines the importance of developing strategies
that refine the ability to revise beliefs accurately,
ensuring its reliability across various scenarios.
7 Discussion
Belief revision is harder in a more complex
task with modus tollens inferences. We com-
pare LLMs’ belief revision capabilities in average,
through tasks with modus ponens and modus tol-
lens rule as the basic logical inferences at step t. As
observed in Figure 5a, LLMs show reduced BREU
score in tasks with modus tollens rule. This is ex-
pected, as modus tollens is inherently more difficult
relative to modus ponens as it require backward di-
rections of reasoning and it involves reasoning with
negations (Evans, 1982, 1993; Girotto et al., 1997).
Furthermore, in tasks involving modus tollens in-
ference, we observe a notably higher BU-Acc com-
pared to a much lower BM-Acc. This disparity
suggests that executing accurate belief revision be-
comes more challenging in complex tasks: deci-
sions to update or maintain beliefs are less clear-cut
in these scenarios compared to simpler tasks.
Belief update on abstract concept is more chal-
lenging for LLMs. We examine LLMs’ belief
revision capabilities in average, when dealing with
scenarios involving causal relationships on events
and mental states effect entities and note them in
Figure 5b. While the BREU score is similar, LLMsdemonstrate tendency towards maintaining their
beliefs in mental state effects instead of updating
them. This may stem from the challenge of rec-
ognizing additional requirements implied from the
third, mental state-related, premise which is inher-
ently more abstract and less directly observable
than a concrete sequential event.
Better prompting method does not help on be-
lief revision. We explore how different prompt-
ing methods affect belief revision abilities of LLMs
on average. Figure 5c shows that CoT, which en-
courages LLMs to elicit reasoning steps, does not
significantly enhance belief revision. While this
may stem from its vulnerability to missing-step
errors (Wang et al., 2023b), attempts to correct
these errors with the PS prompting offer minimal
benefits, improving only by ∼1% of BREU. This
suggests the ability to revise beliefs could still be
absent despite elicitation of reasoning steps. We
put a more detailed analysis in Appendix C.2.
8 Conclusion
The ability to reason and adapt to changing infor-
mation is crucial for NLP applications in the real
world. Most evaluations assume static knowledge
environment, which does not prepare models for dy-
namic real-life scenarios. To address this, Belief-R
is introduced as a diagnostic dataset for evaluating
belief revision capability in LMs. Through Belief-
R and a novel evaluation framework for evaluating
reasoning in a dynamically evolving environment,
∆R, we reveal that current models struggle with
updating their beliefs in response to new informa-
tion, highlighting the need for improved adaptabil-
ity and reliability. By exposing these limitations,
our work underscores the imperative of develop-
ing AI models capable of reasoning adeptly with
evolving data, thereby propelling them closer to
real-world applicability and robustness.
9 Future Work
Observing the limited gain offered by the current
prompting strategy, we suggest that future devel-
opments aim to enhance LMs’ understanding and
handling of premise dependencies. A particularly
promising direction is to integrate LMs with deter-
ministic symbolic solvers to convert problems into
logical rules through chain-of-thought reasoning
to aim for a deeper comprehension of the intricate
dependencies among different premises. Future
work could aim to overcome current limitationsand pave the way for AI systems to be reliable
across evolving scenarios.
Acknowledgements
We thank Yejin Bang for the insightful discussions.
This work has been partially funded by the PF20-
43679 Hong Kong PhD Fellowship Scheme and
the Hong Kong Fellowship Scheme by the Hong
Kong Research Grants Council (RGC), and Care-E
Project (FS116).
Limitations
Towards understanding general belief revision
capabilities. Our study on belief revision using
the Belief-R dataset via the ∆R framework fo-
cuses on belief changes driven by logical inferences
like modus ponens and modus tollens, which may
not fully represent the complexity of real-world
belief revision that often includes a broader range
of scenarios and subtleties. To add, our method-
ology primarily considers the introduction of new
premises as the trigger for belief revision, overlook-
ing how beliefs might change through re-evaluation
of existing knowledge or shifts in perspective in
the absence of new information (i.e. in Kronemyer
and Bystritsky (2014)).
Furthermore, how to define models’ beliefs is
a debatable question and there are some works in-
specting models’ internal beliefs by probing the
hidden states (Burns et al.; Zou et al., 2023). n our
work, however, we conceptualize "belief" similarly
to its usage in dialogue systems, where it represents
what the system currently considers true based on
the context (Feng et al., 2023; van Niekerk et al.,
2020). Further exploration on examining the be-
lief through other approaches (e.g., probing hidden
states of LLMs) lies outside this paper’s scope and
we leave it as future work.
Intersection of reasoning capability and knowl-
edge capacity. The evaluation of models’ reason-
ing capabilities is intricately tied to their knowl-
edge capacity, presenting a significant challenge
in discerning pure reasoning capability from mere
knowledge recall. Current benchmarks often fail to
disentangle these aspects, as models with extensive
knowledge bases may appear to possess superior
reasoning abilities when, in fact, they might be
leveraging stored information rather than demon-
strating genuine inferential logic. This conflation
complicates the assessment of a model’s true rea-
soning faculties, as performance improvements onreasoning tasks could be attributed to enhanced
information retrieval rather than advancements in
reasoning algorithms. Similar to observations in
other reasoning datasets, we acknowledge the lim-
itation that the improved performance of models
tested on Belief-R might not only stem from their
ability to revise beliefs but could also be influenced
by superior knowledge recall (Huang and Chang,
2023). Future research could delve deeper into the
relationship between these capabilities, specifically
focusing on developing evaluation methods that
effectively distinguish between them.
Ethics statement
This research explores how well LMs can revise
their beliefs when faced with new information,
which is crucial for their use in constantly changing
real-world situations. We created a reasoning eval-
uation dataset to test whether LMs can revise their
beliefs correctly or if they stick to their initial as-
sumptions. This is important for using LMs in areas
where being accurate and up-to-date is vital, like
healthcare or legal advice. In example, being able
to revise beliefs appropriately could help prevent
LMs from repeating outdated or wrong information,
making them more reliable and trustworthy. Plus,
LMs that can refresh their understanding accord-
ing to new societal norms can avoid perpetuating
biases, contributing to the fair and ethical use of
AI. We consider this a promising and significant
area for research. We construct the dataset using
events, causes, and effects from ATOMIC and the
construction template is designed and reviewed
manually and attached in this paper. We utilized
crowd-sourced annotators who voluntarily partici-
pated through the platform Appen3, choosing tasks
they deemed fairly compensated. The annotators
were presented with multiple-choice tasks prede-
fined to avoid bias and protect privacy, ensuring an
ethical annotation process.
References
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,
Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,
Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jian-
min Bao, Harkirat Behl, Alon Benhaim, Misha
Bilenko, Johan Bjorck, Sébastien Bubeck, Qin Cai,
Martin Cai, Caio César Teodoro Mendes, Weizhu
Chen, Vishrav Chaudhary, Dong Chen, Dongdong
Chen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra,
Xiyang Dai, Allie Del Giorno, Gustavo de Rosa,
Matthew Dixon, Ronen Eldan, Victor Fragoso, DanIter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg,
Abhishek Goswami, Suriya Gunasekar, Emman
Haider, Junheng Hao, Russell J. Hewett, Jamie
Huynh, Mojan Javaheripi, Xin Jin, Piero Kauff-
mann, Nikos Karampatziakis, Dongwoo Kim, Ma-
houd Khademi, Lev Kurilenko, James R. Lee, Yin Tat
Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Li-
den, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin,
Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola,
Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon
Norick, Barun Patra, Daniel Perez-Becker, Thomas
Portet, Reid Pryzant, Heyang Qin, Marko Radmi-
lac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,
Olli Saarikivi, Amin Saied, Adil Salim, Michael San-
tacroce, Shital Shah, Ning Shang, Hiteshi Sharma,
Swadheen Shukla, Xia Song, Masahiro Tanaka, An-
drea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang,
Yu Wang, Rachel Ward, Guanhua Wang, Philipp
Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can
Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang,
Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu,
Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jian-
wen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,
Yunan Zhang, and Xiren Zhou. 2024. Phi-3 technical
report: A highly capable language model locally on
your phone. Preprint , arXiv:2404.14219.
AI@Meta. 2024. Llama 3 model card.
Carlos E Alchourrón, Peter Gärdenfors, and David
Makinson. 1985. On the logic of theory change:
Partial meet contraction and revision functions. The
journal of symbolic logic , 50(2):510–530.
Anthropic. 2024. Introducing the next generation of
claude.
Grigoris Antoniou and Mary-Anne Williams. 1997.
Nonmonotonic reasoning . Mit Press.
Forough Arabshahi, Jennifer Lee, Mikayla Gawarecki,
Kathryn Mazaitis, Amos Azaria, and Tom Mitchell.
2021. Conversational neuro-symbolic commonsense
reasoning. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 35, pages 4902–4911.
Taiyu Ban, Lyvzhou Chen, Xiangyu Wang, and Huan-
huan Chen. 2023. From query tools to causal ar-
chitects: Harnessing large language models for ad-
vanced causal discovery from data. arXiv preprint
arXiv:2306.16902 .
Chandra Bhagavatula, Ronan Le Bras, Chaitanya
Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-
nah Rashkin, Doug Downey, Wen tau Yih, and Yejin
Choi. 2020. Abductive commonsense reasoning. In
International Conference on Learning Representa-
tions .
Sid Black, Gao Leo, Phil Wang, Connor Leahy,
and Stella Biderman. 2021. GPT-Neo: Large
Scale Autoregressive Language Modeling with Mesh-
Tensorflow. If you use this software, please cite it
using these metadata.Victoria Bobicev and Marina Sokolova. 2017. Inter-
annotator agreement in sentiment analysis: machine
learning perspective. In Recent Advances in Natural
Language Processing , pages 97–102.
Gerhard Brewka. 1991. Nonmonotonic reasoning: logi-
cal foundations of commonsense , volume 12. Cam-
bridge University Press.
Gerhard Brewka, Jürgen Dix, Kurt Konolige, et al. 1997.
Nonmonotonic reasoning: an overview , volume 73.
CSLI publications Stanford.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Collin Burns, Haotian Ye, Dan Klein, and Jacob Stein-
hardt. Discovering latent knowledge in language
models without supervision. In The Eleventh Inter-
national Conference on Learning Representations .
Ruth MJ Byrne. 1989. Suppressing valid inferences
with conditionals. Cognition , 31(1):61–83.
Ruth MJ Byrne, Orlando Espino, and Carlos Santa-
maria. 1999. Counterexamples and the suppression
of inferences. Journal of Memory and Language ,
40(3):347–373.
Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga,
and William Cohen. 2022. MuRAG: Multimodal
retrieval-augmented generator for open question an-
swering over images and text. In Proceedings of
the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 5558–5570, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Cohere. 2024. Retrieval-augmented generation at pro-
duction scale.
IM Copi. 1972. Introduction to logic.
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao
Chang, and Furu Wei. 2021. Knowledge neu-
rons in pretrained transformers. arXiv preprint
arXiv:2104.08696 .
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-
ing factual knowledge in language models. arXiv
preprint arXiv:2104.08164 .
Jon Doyle. 1979. A truth maintenance system. Artificial
intelligence , 12(3):231–272.Didier Dubois, Hélène Fargier, Henri Prade, and Patrice
Perny. 2002. Qualitative decision theory: from sav-
age’s axioms to nonmonotonic reasoning. Journal of
the ACM (JACM) , 49(4):455–495.
David William Etherington. 1986. Reasoning with
incomplete information: investigations of non-
monotonic reasoning . Ph.D. thesis, University of
British Columbia.
Jonathan St BT Evans. 1982. The psychology of deduc-
tive reasoning. (No Title) .
Jonathan St BT Evans. 1993. The mental model the-
ory of conditional reasoning: Critical appraisal and
revision. Cognition , 48(1):1–20.
Marcelo A Falappa, Gabriele Kern-Isberner, and
Guillermo R Simari. 2002. Explanations, belief revi-
sion and defeasible reasoning. Artificial Intelligence ,
141(1-2):1–28.
Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, and Xiao-
Ming Wu. 2023. Towards llm-driven dialogue state
tracking. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 739–755.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen
Wang. 2023. Retrieval-augmented generation for
large language models: A survey. arXiv preprint
arXiv:2312.10997 .
Peter Gärdenfors. 1988. Knowledge in flux: Modeling
the dynamics of epistemic states. The MIT press.
Peter Gärdenfors. 1991. Belief revision and nonmono-
tonic logic: Two sides of the same coin? abstract.
InLogics in AI: European Workshop JELIA’90 Am-
sterdam, The Netherlands, September 10–14, 1990
Proceedings 2 , pages 52–54. Springer.
Vittorio Girotto, Alberto Mazzocco, and Alessandra
Tasso. 1997. The effect of premise order in condi-
tional reasoning: A test of the mental model theory.
Cognition , 63(1):1–28.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Ce-
sar Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Conti Kauffmann,
Gustavo Henrique de Rosa, Olli Saarikivi, et al. 2023.
Textbooks are all you need.
Kilem Li Gwet. 2008. Computing inter-rater reliability
and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology , 61(1):29–48.
Susan Haack. 1978. Philosophy of logics . Cambridge
University Press.
Simon Jerome Han, Keith J Ransom, Andrew Perfors,
and Charles Kemp. 2024. Inductive reasoning in hu-
mans and large language models. Cognitive Systems
Research , 83:101155.Sven Ove Hansson. 2022. Logic of Belief Revision.
In Edward N. Zalta, editor, The Stanford Encyclope-
dia of Philosophy , Spring 2022 edition. Metaphysics
Research Lab, Stanford University.
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-
nitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and
Srinivasan Iyer. 2023. Methods for measuring, up-
dating, and visualizing factual beliefs in language
models. In Proceedings of the 17th Conference of
the European Chapter of the Association for Compu-
tational Linguistics , pages 2714–2731.
Thomas Hofweber, Peter Hase, Elias Stengel-Eskin,
and Mohit Bansal. 2024. Are language models ratio-
nal? the case of coherence norms and belief revision.
arXiv preprint arXiv:2406.03442 .
Jie Huang and Kevin Chen-Chuan Chang. 2023. To-
wards reasoning in large language models: A survey.
InFindings of the Association for Computational
Linguistics: ACL 2023 , pages 1049–1065, Toronto,
Canada. Association for Computational Linguistics.
Myeongjun Jang, Deuk Sin Kwon, and Thomas
Lukasiewicz. 2022. Becel: Benchmark for consis-
tency evaluation of language models. In Proceedings
of the 29th International Conference on Computa-
tional Linguistics , pages 3680–3696.
Albert Q Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris Bam-
ford, Devendra Singh Chaplot, Diego de las Casas,
Emma Bou Hanna, Florian Bressand, et al. 2024.
Mixtral of experts. arXiv preprint arXiv:2401.04088 .
Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and
Peter Clark. 2021. Beliefbank: Adding memory to a
pre-trained language model for a systematic notion
of belief. arXiv preprint arXiv:2109.14723 .
Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung
Kim, Xin Xu, Vaiva Imbrasaite, and Deepak Ra-
machandran. 2024. Boardgameqa: A dataset for
natural language reasoning with contradictory infor-
mation. Advances in Neural Information Processing
Systems , 36.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. Advances in
neural information processing systems , 35:22199–
22213.
David Kronemyer and Alexander Bystritsky. 2014. A
non-linear dynamical approach to belief revision in
cognitive behavioral therapy. Frontiers in Computa-
tional Neuroscience , 8:55.
Moritz Laurer, Wouter van Atteveldt, Andreu Casas, and
Kasper Welbers. 2023. Building Efficient Universal
Classifiers with Natural Language Inference. arXiv
preprint . ArXiv:2312.17543 [cs].Moritz Laurer, Wouter Van Atteveldt, Andreu Casas,
and Kasper Welbers. 2024. Less annotating, more
classifying: Addressing the data scarcity issue of su-
pervised machine learning with deep transfer learning
and bert-nli. Political Analysis , 32(1):84–100.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Tao Li, Vivek Gupta, Maitrey Mehta, and Vivek Sriku-
mar. 2019. A logic-driven framework for consistency
of neural models. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) ,
pages 3924–3935, Hong Kong, China. Association
for Computational Linguistics.
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie
Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023.
Textbooks are all you need ii: phi-1.5 technical report.
arXiv preprint arXiv:2309.05463 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Witold Łukaszewicz. 1990. Non-monotonic reasoning:
formalization of commonsense reasoning.
Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo,
Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus,
Chuang Gan, and Wojciech Matusik. 2024. Llm and
simulation as bilevel optimizers: A new paradigm to
advance physical scientific discovery. arXiv preprint
arXiv:2405.09783 .
David Makinson and Peter Gärdenfors. 2005. Rela-
tions between the logic of theory change and non-
monotonic logic. In The Logic of Theory Change:
Workshop, Konstanz, FRG, October 13–15, 1989 Pro-
ceedings , pages 183–205. Springer.
Donald Nute. 2001. Defeasible logic. In International
Conference on Applications of Prolog , pages 151–
169. Springer.
OpenAI. 2023. Gpt-4 technical report. Preprint ,
arXiv:2303.08774.
Niki Pfeifer and Gernot D Kleiter. 2005. Coherence
and nonmonotonicity in human reasoning. Synthese ,
146:93–109.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.Jandson S Ribeiro, Abhaya Nayak, and Renata Wasser-
mann. 2019. Belief change and non-monotonic
reasoning sans compactness. In Proceedings of
the AAAI Conference on Artificial Intelligence , vol-
ume 33, pages 3019–3026.
Hans Rott. 2001. Change, choice and inference: A study
of belief revision and nonmonotonic reasoning . 42.
Clarendon Press.
Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-
dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,
Brendan Roof, Noah A Smith, and Yejin Choi. 2019.
Atomic: An atlas of machine commonsense for if-
then reasoning. In Proceedings of the AAAI con-
ference on artificial intelligence , volume 33, pages
3027–3035.
Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Pad-
makumar, Nitish Joshi, Mehran Kazemi, Najoung
Kim, and He He. 2024. Testing the general deductive
reasoning capacity of large language models using
ood examples. Advances in Neural Information Pro-
cessing Systems , 36.
Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle
Pineau, and William L Hamilton. 2019. Clutrr: A di-
agnostic benchmark for inductive reasoning from text.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 4506–4515.
Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, and
Greg Durrett. 2022. Natural language deduction with
incomplete information. In Proceedings of the 2022
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 8230–8258.
Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-
berg, and Jonathan Berant. 2020. Leap-of-thought:
Teaching pre-trained models to systematically rea-
son over implicit knowledge. Advances in Neural
Information Processing Systems , 33:20227–20237.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. Preprint ,
arXiv:2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Carel van Niekerk, Michael Heck, Christian Geishauser,
Hsien-Chin Lin, Nurul Lubis, Marco Moresi, and
Milica Gasic. 2020. Knowing what you know: Cali-
brating dialogue belief state distributions via ensem-
bles. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020 , pages 3096–3102.Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A
6 Billion Parameter Autoregressive Language Model.
https://github.com/kingoflolz/mesh-trans
former-jax .
Boshi Wang, Xiang Yue, and Huan Sun. 2023a. Can
chatgpt defend its belief in truth? evaluating llm
reasoning via debate. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
11865–11881.
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,
Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.
2023b. Plan-and-solve prompting: Improving zero-
shot chain-of-thought reasoning by large language
models. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 2609–2634, Toronto,
Canada. Association for Computational Linguistics.
Peter Cathcart Wason and Philip Nicholas Johnson-
Laird. 1972. Psychology of reasoning: Structure
and content , volume 86. Harvard University Press.
Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart Van Merriënboer, Armand Joulin,
and Tomas Mikolov. 2016. Towards ai-complete
question answering: A set of prerequisite toy tasks.
In4th International Conference on Learning Repre-
sentations, ICLR 2016 .
Nahathai Wongpakaran, Tinakon Wongpakaran, Danny
Wedding, and Kilem L Gwet. 2013. A comparison of
cohen’s kappa and gwet’s ac1 when calculating inter-
rater reliability coefficients: a study conducted with
personality disorder samples. BMC medical research
methodology , 13:1–7.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. In The Eleventh International Conference
on Learning Representations .
Wenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Bench-
marking zero-shot text classification: Datasets, eval-
uation and entailment approach. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 3914–3923, Hong Kong,
China. Association for Computational Linguistics.
Andy Zou, Long Phan, Sarah Chen, James Campbell,
Phillip Guo, Richard Ren, Alexander Pan, Xuwang
Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
et al. 2023. Representation engineering: A top-
down approach to ai transparency. arXiv preprint
arXiv:2310.01405 .Appendix
A Samples of Prompts
To provide more clarity on the dataset generation
process, we attach the samples of prompt in Fig-
ure A1.
B Annotation guidelines
We provide human annotators with specific guide-
lines and examples, as detailed in Figures A2 and
A3 for ground truth and quality check annotations,
respectively.
C Additional analysis
C.1 LLMs logical reasoning ability are not
robust in the presence of distractors
We analyze the performance of LLMs on basic logi-
cal inference tasks and compare it to their accuracy
on the BM subset, which differs only by including
a third premise. We selected 378 queries from the
Belief-R dataset where premises overlap between
the basic logical inference tasks at time tand the
BM subset for a fair comparison, and visualize
them in Figure A4. On most of the models, LMs’
accuracy on samples that do not require change of
conclusion (BM-Acc) is dropping compared to its
basic inference at tperformances. This indicates
that the logical reasoning ability of these models
are not robust in the presence of distractors, expos-
ing a critical problem of these models especially
on the challenges in currently adopted retrieval-
augmented-generation (RAG) pipeline to manage
noisy documents that have question-related con-
tent despite lacking substantive information (Lewis
et al., 2020; Chen et al., 2022; Gao et al., 2023).
C.2 Details on prompting methods variations
on their gain at belief revision.
We provide more details on the investigation in
the impact of varied prompting techniques on the
performance accuracy of several models, as sum-
marized previously in Figure 5c. In that figure,
the data indicates most significant performance im-
provements in the BU subset, though overall belief
revision improvements remain marginal, showing
∼1% increase in BREU. In examining the perfor-
mance across models and different prompting meth-
ods as shown in Table A1, it is clear that the influ-
ence of these methods is not uniform. For instance,
the PS prompting method notably boosted accuracy
for models like Mixtral 8x22B and Command R byModels Method BU-Acc BM-Acc BREU
Llama-3
Instruct
(70B)DP 10.99% 92.09% 51.54%
CoT 12.57% 89.40% 50.99%
PS 12.66% 88.21% 50.44%
Mixtral
(8x22B)DP 35.38% 36.57% 35.98%
CoT 27.28% 34.93% 31.11%
PS 44.04% 53.13% 48.59%
Command
RDP 12.10% 80.45% 46.28%
CoT 11.36% 81.19% 46.28%
PS 19.37% 69.85% 44.61%
Command
R+DP 13.69% 75.67% 44.68%
CoT 14.71% 77.76% 46.24%
PS 13.41% 65.07% 39.24%
Claude-3
HaikuDP 9.40% 88.66% 49.03%
CoT 13.50% 83.73% 48.62%
PS 13.22% 82.99% 48.11%
Claude-3
SonnetDP 19.65% 82.69% 51.17%
CoT 21.51% 81.19% 51.35%
PS 16.76% 83.73% 50.25%
GPT-3.5
TurboDP 14.53% 55.22% 34.88%
CoT 20.48% 65.22% 42.85%
PS 17.78% 67.91% 42.85%
GPT-4
TurboDP 16.76% 86.72% 51.74%
CoT 13.59% 87.76% 50.68%
PS 12.76% 88.66% 50.71%
Table A1: The effectiveness of various prompting tech-
niques varies across LLMs and subset of Belief-R, en-
hancing performance in some while degrading it in oth-
ers.
over 10%. Conversely, this same strategy led to per-
formance reductions in models such as Claude-3
Sonnet and GPT-4 Turbo. Similarly, utilizing CoT
and PS exhibited mixed outcomes across models.
It strengthened robustness in models like GPT-3.5
Turbo and GPT-4 Turbo, as shown by higher BM-
Acc scores, while it increased sensitivity to noise in
models like Llama-3 Instruct (70B) and Command
R, resulting in reduced BM-Acc values.Figure A1: Samples of prompts utilized in each of the Belief-R generation pipeline. Here, we take the Event:
PersonX uses PersonX’s ___ to obtain ,Cause (from PersonX): to have an advantage ,Effect (to
PersonX): pleased from ATOMIC, to generate p,q, andrfor us to form queries at step tandt+1in Belief-R and
later go through the manual annotaion process.(a) Annotation Guidelines
(b) Example of Annotation Questions
Figure A2: Details on ground truth annotation(a) Annotation Guidelines
(b) Example of Annotation Questions
Figure A3: Details on quality check annotation
Llama-3 Instruct (70B)
Mixtral (8x22B)
Command R
Command R+
Claude 3 Haiku
Claude 3 Sonnet
GPT-3.5 Turbo
GPT-4 Turbo020406080Accuracy Acc@t
BM-Acc
Figure A4: LLMs show decreased inference performance when exposed to noise from the new information in
alternative condition.