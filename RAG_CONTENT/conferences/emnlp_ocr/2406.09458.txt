Updating CLIP to Prefer Descriptions Over Captions
Amir Zur♣
amirzur@cs.stanford.eduElisa Kreiss♥
ekreiss@ucla.eduKarel D’Oosterlinck♠♦
karel.doosterlinck@ugent.be
Christopher Potts♠
cgpotts@stanford.eduAtticus Geiger♣
atticusg@gmail.com
♣Pr(Ai)2R Group♠Stanford Univeristy♥UCLA♦Ghent University – imec
Abstract
Although CLIPScore is a powerful generic met-
ric that captures the similarity between a text
and an image, it fails to distinguish between
acaption that is meant to complement the in-
formation in an image and a description that
is meant to replace an image entirely, e.g., for
accessibility. We address this shortcoming by
updating the CLIP model with the Concadia
dataset to assign higher scores to descriptions
than captions using parameter efficient fine-
tuning and a loss objective derived from work
on causal interpretability. This model correlates
with the judgements of blind and low-vision
people while preserving transfer capabilities
and has interpretable structure that sheds light
on the caption–description distinction.1
1 Introduction
The texts that accompany images online are written
with a variety of distinct purposes: to add commen-
tary, to identify entities, to enable search, and oth-
ers. One of the most important purposes is (alt-text)
description to help make the image non-visually
accessible, which is especially important for people
who are blind or low-vision (BLV) (Bigham et al.,
2006; Morris et al., 2016; Gleason et al., 2020).
The ability to automatically evaluate descriptions
of images would mark a significant step towards
making the Web accessible for everyone.
Unfortunately, present-day metrics for image-
text similarity tend to be insensitive to the text’s pur-
pose (Kreiss et al., 2022b), as they don’t distinguish
between accessibility descriptions that are intended
to replace the image from captions, which supple-
ment them (see Figure 1). Thus current metrics fall
short when it comes to making genuine progress
towards accessibility (Kreiss et al., 2022a).
The Contrastive Language-Image Pre-training
(CLIP) model of Radford et al. (2021) is an impor-
1Our code is available at https://github.com/
AmirZur/updating-clip-concadia/tree/main .
Figure 1: Visualization of a single training step on Con-
cadia, updating CLIP to prefer descriptions to captions.
Though both objectives update CLIP to be sensitive to
the description–caption distinction, IIT-DAS localizes
this distinction to a subspace of CLIP’s activations.
tant case in point. CLIP is trained to embed images
and texts with the objective of maximizing the sim-
ilarity of related image–text pairs and minimizing
the similarity of unrelated pairs, without reference
to the purpose that the text accompanying an image
plays.
The CLIPScore metric of (Hessel et al., 2021)
inherits this limitation; CLIPScore is referenceless
in that it makes no use of human-generated, ground-
truth texts, but rather depends only on image–text
pairs, and so it too is not sensitive to the purpose
of the text. Indeed, Kreiss et al. (2022a) find that
CLIPScores correlate with neither sighted nor BLV
user evaluations of image alt-descriptions. Thus,
despite high performance on many image–text clas-
sification tasks, CLIPScore is unsuitable for alt-text
evaluation.
The goal of this paper is to update CLIP to assign
higher scores to descriptions than captions when
they are both relevant to an image, while preserving
the model’s ability to select the most relevant text
for a particular image. To this end, we fine-tunearXiv:2406.09458v2  [cs.CV]  3 Oct 2024CLIP on the Concadia dataset (Kreiss et al., 2022b),
which consists of 96,918 images with correspond-
ing descriptions, captions, and textual context.
Our core goal in fine-tuning is to teach the model
to prefer descriptions over captions. To this end,
we use a contrastive loss objective, which updates
CLIP to produce a higher score for the description
than the caption of an image in Concadia. In ad-
dition, we propose an extension of this objective
that seeks to amplify the core distinction and create
more interpretable models. The guiding idea here
is to use Concadia to approximate the counterfac-
tual: What if the text for this image were a descrip-
tion instead of a caption (or vice versa)? These
counterfactuals enable us to use a novel combina-
tion of two recent ideas from causal interpretability
research: interchange intervention training (IIT;
Geiger et al. 2022) with a distributed alignment
search (DAS; Geiger et al. 2023b) to localize the
description–caption concept to an activation vector.
Our experiments lead to a few key findings that
have implications beyond the accessibility use case.
First, we find that LoRA (Hu et al., 2021) is su-
perior to standard fine-tuning at raising the CLIP-
Score assigned to descriptions compared to cap-
tions while preserving the original capabilities of
CLIP. Second, our analysis shows that improved
performance on Concadia results in stronger cor-
relations with BLV user judgements, affirming the
value of our update. Third, we find that the IIT-
DAS objective results in a more stable fine-tuning
process. Fourth, the IIT-DAS objective produces a
more interpretable model; to show this, we use me-
diated integrated gradients (Sundararajan et al.,
2017; Wu et al., 2023) to characterize how the
description–caption distinction is computed in our
fine-tuned models. The key role of the IIT-DAS
objective in these results illustrates one way that
interpretability research can lead directly to more
performant and understandable models.
2 Related Work
Image Accessibility When images can’t be seen,
visual descriptions of those images make them ac-
cessible. For images online, these descriptions can
be provided in the HTML’s alt tag, which are then
visually displayed if the image cannot be loaded
or are read out by a screen reader to, for instance,
users who are blind or low-vision (BLV). However,
alt descriptions online remain rare (Gleason et al.,
2019; Kreiss et al., 2022b).Image captioning models provide an opportu-
nity to generate such accessibility descriptions at
scale, which would promote equal access (Glea-
son et al., 2020). But the resulting models have
remained largely unsuccessful in practice (Mor-
ris et al., 2016; MacLeod et al., 2017; Gleason
et al., 2019). Kreiss et al. (2022b) argue that this
is partly due to the general approach of treating all
image-based text generation problems as the same
underlying task, and instead highlight the need for
a distinction between accessibility descriptions and
contextualizing captions. Descriptions are needed
to replace images, while the purpose of a caption is
to provide supplemental information. Kreiss et al.
(2022b) find that the language used in descriptions
and captions categorically differs and that sighted
participants tend to learn more from captions but
can visualize the image better from descriptions.
Referenceless Text-Image Evaluation Metrics
We focus on referenceless evaluation metrics (Fein-
glass and Yang, 2021; Hessel et al., 2021; Lee et al.,
2021) for text–image models. These can be applied
in diverse contexts and require no human annota-
tions, which makes them valuable tools for rapid
system assessment. Crucially, such referenceless
metrics are context-free. This brings the advantage
that they can be applied in a variety of multimodal
settings (e.g. image synthesis, description genera-
tion, zero-shot image classification). The downside
is that they are generally insensitive to variation in
context and purpose. Kreiss et al. (2022a, 2023)
report progress in incorporating context into these
metrics; our work can be seen as complementing
those efforts by focusing on textual purpose.
3 Methods
Our goal is to fine-tune a CLIP model Cθto prefer
a description over a caption when the two texts are
relevant to an image, while preserving Cθ’s abil-
ity to select the most relevant text for a particular
image. To this end, we use Concadia, a dataset
that consists of (ximage, xdescription , xcaption)triplets.
We consider two different contrastive learning ob-
jectives and two common fine-tuning methods that
minimally update Cθ’s sensitivity to the description–
caption distinction.
Behavioral Objective For each triplet
(xim, xdes, xcap), we run Cθon the image–caption
pair(xim, xcap)and on the image–description pair
(xim, xdes), and update Cθto produce a higherFine- Desc > Cap Transfer tasks (F1 Score) BLV user eval (Corr.)
Objective tuning Concadia Food101 ImageNet CIFAR100 Overall Imaginability
None None 49.4% 76.4% 53.6% 61.5% 0.08 0.10
BehavioralFull 90.1%±0.73 27.8%±24.96 14.6% ±13.47 30.9% ±24.97 0.29±0.17 0.31±0.19
LoRA 90.3%±0.72 64.4%±8.32 42.2% ±6.98 55.6%±2.53 0.36±0.10 0.38±0.12
IIT-DASFull 88.9%±0.80 35.1%±16.35 19.5% ±6.77 42.8% ±9.85 0.24±0.14 0.32±0.17
LoRA 86.6%±0.84 73.6%±3.22 45.1%±3.96 53.7% ±3.80 0.19±0.15 0.27±0.15
Table 1: The percent of Concadia examples where descriptions are scored higher than captions (column 3), F1
scores on transfer learning tasks (columns 4-6), and correlation between BLV human preferences (Kreiss et al.,
2022a) and model similarity scores (columns 7, 8). The error bounds are 95% confidence intervals from 5 random
seeds. Fine-tuning on Concadia produces models that better correlate with BLV preferences, LoRA is essential for
preserving transfer learning, and IIT-DAS sacrifices a modest amount on Desc > Cap for better transfer learning.
score for the latter by minimizing:
L=CE([Cθ(xim, xcap),Cθ(xim, xdes)],[0,1])
IIT-DAS Objective To better maintain CLIP’s
original capabilities, we propose a novel objective
called IIT-DAS, which localizes the description–
caption distinction in a linear subspace Zof an
activation vector in Cθ. For each triplet, we run Cθ
on the image-caption pair (xim, xcap), runCθon
the image-caption pair again while fixing Zto the
value it takes when Cθis run on the image descrip-
tion pair (xim, xdes), and update Cθto produce a
higher score for the latter by minimizing:
LIIT=CE([Cθ(xim, xcap),
DII(Cθ, ρθ,(xim, xcap),(xim, xdes),Z)],[0,1])
where ρθis a randomly initialized orthogonal ma-
trix used to learn the linear subspace Z. This
pushes CLIP to assign a higher score to a caption
with an intervention from a description than to a
caption on its own. Likewise, we also train on
the objective where descriptions and captions are
swapped (prefer a description over a description
with an intervention from a caption):
LIIT=CE([Cθ(xim, xdes),
DII(Cθ, ρθ,(xim, xdes),(xim, xcap),Z)],[1,0])
See Appendix A for the formal definition of DII.
Fine-tuning We consider two common fine-tuning
methods that update CLIP in accordance with the
selected objective. The first is full fine-tuning
where all the parameters of CLIP are trained with
gradient descent. The second is Low-Rank Adap-
tation (LoRA), a state-of-the-art fine-tuning tech-
nique for transfer learning (Hu et al., 2021). LoRAtraining freezes every linear layer Win a model
and learns a low-rank matrix W′that is used in
unison with the original weights (i.e. for a residual
representation x, LoRA computes Wx+W′x).
4 Experiments
4.1 Concadia and Transfer Evaluations
We use Concadia to quantify the extent to which
an image–text model is sensitive to the description–
caption distinction. The metric is simply the propor-
tion of images in Concadia where the description is
assigned a higher score than the caption. In the pre-
trained CLIP model, this is the case for ≈50% of
the triplets in Concadia (see Table 1). This means
the model assigns higher scores to descriptions only
at chance and so is not well suited for the purposes
of accessibility. However, we also need to evaluate
to what extent each fine-tuning method preserves
the original transfer capabilities of CLIP. The ideal
method will maintain high performance on the im-
age classification transfer tasks, while preferring
descriptions over captions.
We evaluate our fine-tuned models on Concadia
and three image classification tasks in disparate
domains, selected from CLIP’s original evaluation
suite (Radford et al., 2021) (see Appendix C).
Results and Discussion Table 1 shows the perfor-
mance of each CLIP model on the Concadia and
transfer evaluations. Full fine-tuning is not viable
due to its poor performance on the transfer learning
tasks. LoRA helps preserve much of the original
model’s capabilities (dropping ≈10% on Food101
and Imagenet, and ≈6% on CIFAR).
The models trained on the behavioral objec-
tive are more sensitive to the description–caption
distinction ( ≈90%) than IIT-DAS with LoRA
(≈87%). In contrast, models trained on the IIT-Group Objective Finetuning Overall Imaginability Relevance Irrelevance
BLV None None 0.08 0.10 0.09 0.09
Behavioral Full 0.24 ±0.04 0.26±0.03 0.21±0.04 0.05±0.09
LoRA 0.29 ±0.03 0.29±0.01 0.23±0.02 -0.01±0.03
IIT-DAS Full 0.29 ±0.05 0.34±0.05 0.30±0.06 0.07±0.05
LoRA 0.20 ±0.09 0.28±0.10 0.24±0.07 0.01±0.06
Sighted None None −0.01 0.06 0.00 −0.17
(no image) Behavioral Full 0.22 ±0.04 0.13±0.09 0.17±0.03 -0.03±0.03
LoRA 0.20 ±0.02 0.20±0.04 0.18±0.02 -0.13±0.05
IIT-DAS Full 0.18 ±0.06 0.21±0.03 0.12±0.08 -0.14±0.06
LoRA 0.13 ±0.05 0.18±0.04 0.11±0.05 -0.09±0.06
Sighted None None 0.14 0.11 −0.08
(with image) Behavioral Full 0.26 ±0.05 0.22±0.04 0.03±0.05
LoRA 0.25 ±0.02 0.19±0.02 -0.04±0.04
IIT-DAS Full 0.25 ±0.06 0.17±0.07 -0.04±0.07
LoRA 0.22 ±0.06 0.15±0.05 0.02±0.06
Table 2: Correlation between model similarity scores
and human preferences (Kreiss et al., 2022a).
DAS objective with LoRA achieve the best perfor-
mance on transfer tasks (preserving CLIP’s origi-
nal Food101 accuracy within its 95% confidence
interval), though sacrificing some sensitivity to the
description–caption distinction ( ≈86%). The IIT-
DAS objective is comparable to the behavioral ob-
jective; either one may be preferable depending on
the desired balance between description–caption
sensitivity and transfer capabilities.
4.2 BLV and Sighted Human Evaluations
A preference for descriptions over captions should
align CLIPScore ratings better with the quality
judgements of BLV and sighted individuals. To
evaluate this, we use data from Kreiss et al.
2022a. Kreiss et al. conducted an experiment where
sighted and BLV participants were asked to judge
a text describing an image in the context of an arti-
cle (see Appendix D for experimental details). For
our purposes, we ignore the context and isolate the
benefit of descriptions compared to captions. Par-
ticipants rated the quality of the image descriptions
along four dimensions, summarized below.
Overall The overall value of the description as an
alt-text description of the image.
Imaginability How well the participant can visual-
ize the image given the text description. This isn’t
evaluated for sighted individuals with the image,
since they are able to see the reference image.
Relevance Whether the description includes rele-
vant details from the image, given the context in
which the image appears (i.e., the preceding para-
graph in a Wikipedia article).
Irrelevance Whether the description avoids irrele-
vant details from the image, given the context of in
which the image appears.
Results and Discussion For each model evaluated,
we report correlations averaged across 5 runs. Ta-ble 1 shows the correlation between BLV individu-
als’ preferences and model similarity scores for the
overall andimaginability dimensions.
Our results show clearly that fine-tuning CLIP
on the Concadia dataset results in a CLIPScore that
is better aligned with the judgments of BLV in-
dividuals. This agrees with the finding that the
description–caption distinction is important for
BLV users (Kreiss et al., 2022a).
A broad trend is that the more a model is able
to distinguish between descriptions and captions
the more it aligns with the judgements of BLV
individuals. As such, the models trained on the
behavioral objective have the highest correlations.
Table 2 reports the correlation between fine-
tuned CLIPScores and evaluations from BLV indi-
viduals, sighted individuals without access to the
image the text describes, and sighted individuals
with access to the image.
We find that fine-tuning CLIP on the Concadia
dataset improves the model’s correlation with hu-
man judgements of text descriptions. We note that
fine-tuning CLIP does not significantly improve
the model’s correlation with human judgements
of irrelevant details in the text description. This
makes sense, because our training scheme did not
take into account to the context of the Concadia
image.
4.3 Integrated Gradients
A key benefit of localizing the description–caption
distinction in CLIP with IIT-DAS is that we can
interpret CLIP’s representation of a text’s commu-
nicative purpose (description or caption) separately
from CLIP’s similarity score. In this section, we
conduct an analysis of how CLIP distinguishes be-
tween descriptions and captions using an attribu-
tion method called integrated gradients (IG; Sun-
dararajan et al. 2017) that evaluates the contribution
of each text token to the output CLIPScore.
We are particularly curious about how tokens im-
pact the representation of the description–caption
distinction. To answer this question, we mediate
the gradient computation through the linear sub-
space learned by IIT-DAS (Wu et al., 2023). We hy-
pothesize that, since the intervention site is trained
to represent the underlying purpose of a text (i.e.
description or caption), gradient attributions that
are mediated through the linear subspace learned
by IIT-DAS will pick out tokens that highlight the
description–caption distinction.Training Mediation Concreteness Imageability
NoneNone 0.20 0.16
Through 0.14 0.07
Around - -
BehavioralNone 0.26±0.08 0 .31±0.08
Through 0.24±0.08 0 .24±0.08
Around 0.24±0.02 0 .30±0.03
IIT-DASNone 0.26±0.08 0 .23±0.08
Through 0.23±0.03 0 .23±0.03
Around 0.10±0.09 0 .03±0.09
(a) Correlation between integrated gradient
attributions and per-token human labels for
concreteness and imageability. The error
bounds are 95% confidence intervals from
runs with five random seeds.
Example Mediation Attribution
Desc.✗ablack and white photograph ofjimi hendrix
playing afender strato caster electric guitar
✓ablack and white photograph ofjimi hendrix
playing afender strato caster electric guitar
Caption✗ jimi hendrix ,fillmore east ,may 10,1968
✓ jimi hendrix ,fillmore east ,may 10,1968
Desc.✗ ablock offlats behind asetofhigh security gates
✓ ablock offlats behind asetofhigh security gates
Caption✗theexclusive block offlats inchelsea ,london
that were used astheexterior ofmark ’sflat
✓theexclusive block offlats inchelsea ,london
that were used astheexterior ofmark ’sflat
(b) Integrated gradient attributions for the IIT-DAS model run on an image and
its corresponding description and caption from the Concadia dataset. A positive
token attribution indicates a positive impact on CLIPScore (green), and negative
token attribution indicates a negative impact (magenta).
Figure 2: Mediated integrated gradient results.
Figure 2b shows an example image from Con-
cadia and the IG attributions of its corresponding
description and caption on the IIT-DAS model. We
observe that although the overall attributions are
positive for the guitarist’s name (“Jimi Hendrix”),
themediated attributions for these tokens are neg-
ative. While “Jimi Hendrix” is aligned with the
image (high overall), proper names are less likely
to appear in descriptions (low mediated).
Dataset We hypothesize that the interpretability
afforded by mediated integrated gradients will align
with the distinct purposes behind describing and
captioning images. Specifically, descriptions are
easier to visualize than captions, since their goal
is to supplant the image’s visual components as
opposed to supplement them (Kreiss et al., 2022b).
Hence, we expect that words with higher integrated
gradient attributions are easier to visualize.
We consult two collections of human ratings for
visualization-related concepts. The first dataset
consists of 5,500 words rated by imageability , or
how well a word evokes a clear mental image in
the reader’s mind (Scott et al., 2019). The second
dataset consists of over 40,000 words rated by con-
creteness , or how clearly a word corresponds to
a perceptible entity (Brysbaert et al., 2014).2We
randomly sample 100 captions and 100 descrip-
tions from the test split of the Concadia dataset
that contain at least one word within both of our
datasets, consisting of 420 unique tokens in total.
2Although imageability and concreteness are slightly dif-
ferent concepts, the imageability and concreteness ratings
have a correlation factor of 0.88 with each other.We compute the integrated gradient attributions for
all tokens in those sentences, and report their corre-
lations with imageability and concreteness ratings.
Results and Discussion Table 2a displays the cor-
relations between token-level attributions of the
LoRA model output and human ratings for image-
ability and concreteness. All fine-tuning methods
achieve a stronger correlation with imageability
and concreteness ratings than the base CLIP model.
Although all fine-tuning methods result in medi-
ated gradient attributions that correlate with image-
ability and concreteness, only the IIT-DAS attribu-
tions localize to the mediation site. The difference
in mediating through vs. around the learned site is
significant for the IIT-DAS model ( 0.23±0.03vs.
0.10±0.09for concreteness, and 0.23±0.03vs.
0.03±0.09for imageability).
Our results show that fine-tuning CLIP to prefer
descriptions over captions with IIT-DAS results
in models whose attributions correspond to the
human-interpretable concept of imageability and
concreteness. We also find that mediating inte-
grated gradients through the representation targeted
by IIT-DAS preserves this correlation and allows
for an analysis of which tokens contribute to distin-
guishing descriptions from captions.
5 Conclusion
We update the CLIP model to prefer descriptions
over captions using Concadia and produce a useful,
accessible, and interpretable model.Limitations
Our results serve as proof concept for using IIT-
DAS to update CLIP with the Concadia dataset.
This is one model and one dataset, so general con-
clusions about the use of IIT-DAS for updating a
pretrained model should not be drawn. We hope
future work will shed further light on the value of
IIT-DAS.
The Concadia dataset provides textual context
for each image-description-caption triple. We do
not use the context in our experiments, but we are
excited about future work that incorporates this
data. Whereas our work focuses on the specific
purposes of describing and captioning an image,
the context of an image can illuminate many other
purposes (e.g. search, geolocation, social commu-
nication) and models that incorporate it can enrich
our work.
Ethics Statement
We believe that modern AI is a transformative tech-
nology that should benefit all of us and accessibility
applications are an important part of this.
References
Jeffrey P Bigham, Ryan S Kaminsky, Richard E Ladner,
Oscar M Danielsson, and Gordon L Hempton. 2006.
Webinsight: making web images accessible. In Pro-
ceedings of the 8th International ACM SIGACCESS
Conference on Computers and Accessibility , pages
181–188.
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
2014. Food-101 – mining discriminative components
with random forests. In European Conference on
Computer Vision .
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2014. Concreteness ratings for 40 thousand
generally known english word lemmas. Behavior
research methods , 46:904–911.
Joshua Feinglass and Yezhou Yang. 2021. Smurf: Se-
mantic and linguistic understanding fusion for cap-
tion evaluation via typicality analysis. arXiv preprint
arXiv:2106.01444 .
Atticus Geiger, Chris Potts, and Thomas Icard. 2023a.
Causal abstraction for faithful model interpretation.
Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh
Rozner, Elisa Kreiss, Thomas Icard, Noah Good-
man, and Christopher Potts. 2022. Inducing causal
structure for interpretable neural networks. In In-
ternational Conference on Machine Learning , pages
7324–7338. PMLR.Atticus Geiger, Zhengxuan Wu, Christopher Potts,
Thomas Icard, and Noah D. Goodman. 2023b. Find-
ing alignments between interpretable causal variables
and distributed neural representations.
Cole Gleason, Patrick Carrington, Cameron Cassidy,
Meredith Ringel Morris, Kris M Kitani, and Jeffrey P
Bigham. 2019. “it’s almost like they’re trying to
hide it”: How user-provided image descriptions have
failed to make twitter accessible. In The World Wide
Web Conference , pages 549–559.
Cole Gleason, Amy Pavel, Emma McCamey, Christina
Low, Patrick Carrington, Kris M Kitani, and Jeffrey P
Bigham. 2020. Twitter a11y: A browser extension to
make twitter images accessible. In Proceedings of the
2020 chi conference on human factors in computing
systems , pages 1–12.
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le
Bras, and Yejin Choi. 2021. Clipscore: A reference-
free evaluation metric for image captioning. CoRR ,
abs/2104.08718.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand, Eric
Zelikman, Meredith Ringel Morris, and Christopher
Potts. 2022a. Context matters for image descriptions
for accessibility: Challenges for referenceless evalua-
tion metrics. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Process-
ing, pages 4685–4697.
Elisa Kreiss, Fei Fang, Noah Goodman, and Christopher
Potts. 2022b. Concadia: Towards image-based text
generation with a purpose. In Proceedings of the
2022 Conference on Empirical Methods in Natural
Language Processing , pages 4667–4684.
Elisa Kreiss, Eric Zelikman, Christopher Potts, and Nick
Haber. 2023. ContextRef: Evaluating referenceless
metrics for image description generation. Ms., Stan-
ford University.
Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learn-
ing multiple layers of features from tiny images.
Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt,
Trung Bui, and Kyomin Jung. 2021. Umic: An unref-
erenced metric for image captioning via contrastive
learning. arXiv preprint arXiv:2106.14019 .
Haley MacLeod, Cynthia L Bennett, Meredith Ringel
Morris, and Edward Cutrell. 2017. Understanding
blind people’s experiences with computer-generated
captions of social media images. In proceedings
of the 2017 CHI conference on human factors in
computing systems , pages 5988–5999.George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM , 38(11):39–41.
Meredith Ringel Morris, Annuska Zolyomi, Catherine
Yao, Sina Bahram, Jeffrey P Bigham, and Shaun K
Kane. 2016. " with most of it being pictures now, i
rarely use it" understanding twitter’s evolving accessi-
bility to blind users. In Proceedings of the 2016 CHI
conference on human factors in computing systems ,
pages 5506–5516.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021. Learn-
ing transferable visual models from natural language
supervision. In Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24
July 2021, Virtual Event , volume 139 of Proceedings
of Machine Learning Research , pages 8748–8763.
PMLR.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexan-
der C. Berg, and Li Fei-Fei. 2015. ImageNet Large
Scale Visual Recognition Challenge. International
Journal of Computer Vision (IJCV) , 115(3):211–252.
Graham G Scott, Anne Keitel, Marc Becirspahic,
Bo Yao, and Sara C Sereno. 2019. The glasgow
norms: Ratings of 5,500 words on nine scales. Be-
havior research methods , 51:1258–1270.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.
Axiomatic attribution for deep networks. In Interna-
tional conference on machine learning , pages 3319–
3328. PMLR.
Zhengxuan Wu, Karel D’Oosterlinck, Atticus Geiger,
Amir Zur, and Christopher Potts. 2023. Causal proxy
models for concept-based model explanations. In In-
ternational Conference on Machine Learning , pages
37313–37334. PMLR.A Causal Models, Interchange
Intervention Training, and Distributed
Alignment Search
This section follows Geiger et al. 2023a,b.
Causal Models can represent a variety of pro-
cesses, including deep learning models and sym-
bolic algorithms. A causal model Mconsist of
variables V, and, for each variable X∈V, a
set of values Val(X), and a structural equation
FX∶Val(V)→Val(X), which is a function
that takes in a setting of all the variables and out-
puts a value for X. The solutions of a model
M=(V,Val, F)are settings for all variables
v∈Val(V)such that the output of the causal
mechanism FX(v)is the same value that vassigns
toX, for each X∈V.
We only consider structural causal models with
a single solution that induces a directed acyclic
graphical structure such that the value for a variable
Xdepends only on the set of variables that point to
it, denoted as its parents PAX. Because of this, we
treat each causal mechanism FXas a function from
parent values in Val(PAX)to a value in Val(X).
We denote the set of variables with no parents as
Vinand those with no children Vout.
Given input∈Val(Vin) and variables X⊆V,
we define GET(M,input,X)∈Val(X)to be the
setting of Xdetermined by the given input and
model M. For example, Xcould correspond to
a hidden activation layer in a neural network, and
GET(M,input,X)then denotes the particular val-
ues that Xtakes on when the model Mprocesses
input .
Interventions simulate counterfactual states in
causal models. For a set of variables Xand a set-
ting for those variables x∈VAL(X), we define
MX←xto be the causal model identical to M, ex-
cept that the structural equations for Xare set to
constant values x. In the case of neural networks,
we overwrite the activations with xin-place so that
gradients can back-propagate through x.
Distributed interventions also simulate coun-
terfactual states in causal models, but do so by
editing the causal mechanisms rather than over-
writing them to be a constant. Given variables X
and an invertible function ρ∶VAL(X)→VAL(Y)
mapping Xinto a new variable space Y, define
ρ(M)to be the model where the variables Xare
replaced with the variables Y. For a setting of the
new variable space y∈VAL(Y), it follows that
ρ−1(ρ(M)Y←y)is the causal model identical toM, except that the causal mechanisms for Xare
edited to fix the value of Ytoy. Ifρis differen-
tiable, then gradients back-propagate through y.
Adistributed interchange intervention fixes
variables to the values they would have taken if a
different input were provided. Consider a causal
model M, an invertible function ρ∶VAL(X)→
VAL(Y), source and base inputs s,b∈Val(Vin),
and a set of intermediate variables X⊂V. A
distributed interchange intervention computes the
value Voutwhen run on b, intervening on the (dis-
tributed) intermediate variables Yto be the value
they take on when run on s. Formally, we define
DII(M, ρ,b,s,Y)=
GET(ρ−1(ρ(M)Y←GET(ρ(M),s,Y)),b,Vout)
High-Level Models Define a class ∆to con-
tain only causal models that consist of the fol-
lowing three variables. The input variable X
takes on the value of some image-text pair in
Concadia (ximage, xtext); the intermediate vari-
able P(i.e. purpose) takes on a value from
{“describe” ,“caption” }depending on the Conca-
dia label for X; and the output variable Ytakes on
a real value that represents the similarity between
ximage andxtext. The causal mechanism of Ymust
be such that for every image, the description text is
assigned a higher CLIPScore than the caption text.
If a CLIP model implements any algorithm in ∆,
then it will assign descriptions higher scores than
captions.
B Training Details
In this paper, we propose a novel combination of
LoRA fine-tuning with the IIT-DAS objective. We
apply DAS to the representation after LoRA is ap-
plied (i.e., Wx+W′x, where Wis the original
matrix weight and W′is the low-rank adaptation),
and fine-tune the LoRA parameters W′and the
rotation matrix ρθ.
We fine-tune the CLIP ViT-B/32 Transformer
model released by OpenAI3, which consists of 12
transformer layers with a hidden dimension of 512,
constituting ∼150M overall parameters. For all
fine-tuning runs, we use Adam optimization with
default parameters (Kingma and Ba, 2014) and a
batch size of 12.
3https://huggingface.co/openai/
clip-vit-base-patch32Behavioral Objective We fine-tune CLIP on the
training split of the Concadia dataset (77,534
datapoints) with early-stopping validation on the
Concadia validation split (9,693 datapoints). We
conduct a hyperparameter grid search over a
learning rate lr∈{10−3,0.5⋅10−3,10−4,0.5⋅
10−4, . . . , 10−6,0.5⋅10−6}, an L2 normalization
factor l2∈{0,0.1,0.01,0.001}. We select the con-
figuration with the highest accuracy on the Conca-
dia validation split within 5 epochs ( lr=0.5⋅10−6,
l2=0). A training run takes around 3 hours on an
RTX A6000 NVIDIA GPU.
IIT-DAS Objective We fine-tune CLIP on
100,000 triplets sampled from the train split of the
Concadia dataset (out of 77,534×77,534possible
caption–description pairs). We conduct a hyper-
parameter grid search over a learning rate lr∈
{10−5,5−6,10−6}, as well as over the intervention
site: the layer layer∈{6,8,10}, the intervention
site size intervention-size ∈{32,64,128,256}.
We select the configuration with the highest ac-
curacy on the Concadia validation split within 5
epochs ( lr=10−5,layer=10,intervention-size =
256). A training run takes around 6 hours on an
RTX A6000 NVIDIA GPU.
LoRA Fine-Tuning We perform an additional hy-
perparameter search for low-rank fine-tuning of
CLIP for both the behavioral and IIT-DAS ob-
jective. We take the best configuration for full-
finetuning, and then perform a search over the
LoRA rank rank∈{8,16,32,64,128}, the LoRA
dropout dropout∈{0,0.1,0.01}, and whether
to apply LoRA to all linear layers, all attention
weights, or only the query and value projection
matrices within attention weights. We select the
configuration with the highest accuracy on the Con-
cadia validation split with 5 epochs. For the be-
havioral objective, the configuration is rank=64,
dropout=0, with LoRA applied to all attention
weights. For the IIT-DAS objective, the configura-
tion is the same but with rank=128.
Joint Objective We note that the behavioral objec-
tive and IIT-DAS objective can complement each
other – the former teaches the model to prefer de-
scriptions to captions, and the latter teaches the
model to localize this distinction in a particular rep-
resentation. Hence, we consider a joint objective,
where we train the model to minimize
LJoint=αLIIT+(1−α)LBehavioralFigure 3: Accuracy on the Concadia test set, and
the three transfer tasks selected for transfer evaluation
(CIFAR-100, Food101, and ImageNet).
We search over an interpolation factor α∈
{0.2,0.3,0.5,0.7,0.8}. However, we find that the
joint objective neither improves upon the behav-
ioral objective nor the IIT-DAS objective.
C Transfer Evaluations
We evaluate our fine-tuned CLIP models on tasks
selected from the original suite of zero-shot eval-
uations performed on CLIP (Radford et al., 2021).
Specifically, we choose three zero-shot image clas-
sification tasks in which CLIP has strong perfor-
mance, described briefly below.
CIFAR-100 Images from 100 different categories
(Krizhevsky et al., 2009).
Food101 Food images labeled from 101 categories
(Bossard et al., 2014).
ImageNet Image-text pairs for each synonym set
in the WordNet hierarchy (Russakovsky et al.,
2015; Miller, 1995).
Pre-trained CLIP varies greatly in its ability to
generalize to each of these tasks, but it does out-
perform a supervised linear classifier trained on
ResNet-50 features (Radford et al., 2021). We re-
port the macro-averaged F1 score on zero-shot clas-
sification for each of the transfer tasks listed above,
averaged across 5 randomly seeded training runs.
During evaluation, we prefix “An image of ___” to
each label in order to improve zero-shot generaliza-
tion.
Figure 3 shows model accuracy throughout train-
ing for the 5 randomly seeded training runs of
each training objective and fine-tuning method. Al-
though the training converges quickly on the Con-
cadia dataset, training for longer seems to allow
Figure 4: Transfer score (averaged recovery percentage
over all transfer tasks) over accuracy on the Concadia
test set. A point on a seeded run yields a trade-off
between sensitivity to the caption–description distinc-
tion and preserving the capabilities of CLIP. The joint
objective refers to a training run minimizing both the
behavioral and the IIT-DAS objective (see Appendix B).
the model to recover performance on the evalu-
ated transfer tasks when using LoRA fine-tuning.
Hence, for the evaluation scores reported in Ta-
ble 1, we train models well past convergence for
10 epochs, and then select a model to balance the
trade-off between Concadia accuracy and transfer
capabilities:
•Compute a recovery percentage: divide the
model’s accuracy on the transfer task by the
accuracy of pre-trained CLIP on that task (see
Table 1).
•Compute a transfer score: average the recov-
ery percentage across the three transfer tasks.
•Compute accuracy–transfer trade-off score:
compute α⋅(Concadia accuracy) +(1−α)⋅
(transfer score).
•For each seeded run, select the training step
with the highest accuracy–transfer trade-off
score.
We manually pick a trade-off of α=0.9, mean-
ing we weigh the trade-off as 90% Concadia ac-
curacy and 10% transfer accuracy; we find that
trade-offs with a lower αresult in poor Concadia
accuracy.
We visualize other possible trade-off in Figure
4. We find that the strongest trade-off for IIT-DAS
with LoRA fine-tuning is ≈86% accuracy on theConcadia test set and ≈82% transfer score. Mean-
while, the strongest trade-off for the behavioral
objective with LoRA fine-tuning is skewed towards
stronger Concadia performance, with ≈90% accu-
racy on Concadia and ≈80% transfer score.
Figure 4 also displays the transfer–accuracy
trade-off for a joint objective that minimizes both
the behavioral and the IIT-DAS objective (see Ap-
pendix B for details). The joint objective seems to
strike some balance between the trade-off curves of
the behavioral and IIT-DAS objectives – maintain-
ing higher transfer scores around ≈85% Conca-
dia accuracy compared to the behavioral objective,
and reaching higher Concadia accuracy ( ≈90%)
compared to IIT-DAS. Nevertheless, for any given
Concadia accuracy value, one of the behavioral or
IIT-DAS objectives achieves at least as high a trans-
fer score as the joint objective. We leave strategies
to optimally combine the behavioral and IIT-DAS
objectives for further study.
D BLV and Sighted Evaluations
Kreiss et al. (2022a) recruited 16 participants via
email lists for BLV users that were unaware of the
studies purpose. Participants were totally blind
(7), nearly blind (3), light perception only (5), and
low vision (1). 15 participants reported relying on
screen readers (almost) always when browsing the
Web, and one reported using them often. In to-
tal, judgements were provided for 68 descriptions,
comprising 18 images and 17 Wikipedia articles.
E Integrated Gradients
Given a model Cθwith input xand baseline x′, the
integrated gradient attributions of xalong its ith
dimension are computed as follows.
IntegratedGrads i(x)=(xi−x′
i)⋅
∫1
α=0∂Cθ(x′+α(x−x′))
∂xi∂α. (1)
Mediated Integrated Gradients LetHbe the
activation of Cθat the intervention site when run
on input x. Our mediated integrated gradient is
∂Cθ
∂ximediated by H=∂Cθ
∂H×∂H
∂xi.
Unlike∂Cθ
∂xi, which computes the gradient of the
model output with respect to the input xi, the me-
diated gradient only flows through the interventionsiteH. We compute mediated integrated gradi-
ents by applying this gradient method within the
integral of the integrated gradients equation.