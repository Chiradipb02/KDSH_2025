Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment
Analysis
Xiaowei Zhao∗, Yong Zhou∗, Xiujuan Xu, Yu Liu†
School of Software Technology, Dalian University of Technology
xiaowei.zhao@dlut.edu.cn ,89@mail.dlut.edu.cn ,{xjxu, yuliu}@dlut.edu.cn
Abstract
Aspect-based Sentiment Analysis (ABSA) eval-
uates sentiment expressions within a text to
comprehend sentiment information. Previous
studies integrated external knowledge, such as
knowledge graphs, to enhance the semantic fea-
tures in ABSA models. Recent research has
examined the use of Graph Neural Networks
(GNNs) on dependency and constituent trees
for syntactic analysis. With the ongoing devel-
opment of ABSA, more innovative linguistic
and structural features are being incorporated
(e.g. latent graph), but this also introduces
complexity and confusion. As of now, a scal-
able framework for integrating diverse linguis-
tic and structural features into ABSA does not
exist. This paper presents the Extensible Multi-
Granularity Fusion (EMGF) network, which in-
tegrates information from dependency and con-
stituent syntactic, attention semantic , and exter-
nal knowledge graphs. EMGF, equipped with
multi-anchor triplet learning and orthogonal
projection, efficiently harnesses the combined
potential of each granularity feature and their
synergistic interactions, resulting in a cumu-
lative effect without additional computational
expenses. Experimental findings on SemEval
2014 and Twitter datasets confirm EMGF’s su-
periority over existing ABSA methods1.
1 Introduction
The primary objective of the Aspect-Based Senti-
ment Analysis(ABSA) task is to assess the senti-
ment polarity associated with specific aspects or
entities in a text, enabling a more comprehensive
understanding of the text’s sentiment information.
For example, give a laptops review " Looks nice
, but has a horribly cheap feel." and the senti-
ment polarity of the two aspects " Looks " and " feel"
are positive and negative, respectively. Therefore,
∗Equal contribution.
†Corresponding author.
1Code and datasets are available at https://github.com/
TYZY89/EMGF
Looks nice ,but  has  a  horribly  cheap feel.
VBZ JJ , CC DT .acomp
detconj
punctVBZ RB
rootccADJPVPSTOPConstituent Tree
Dependency TreeJJ NNpunct advmod amod
dobjADJPNPVPVPFigure 1: An example sentence with its dependency tree
and constituent tree. This sentence from the laptops re-
views, contains two aspects but with opposite sentiment
polarities.
ABSA accurately identifies the sentiment orienta-
tion for individual aspects, rather than assigning
a general sentiment label to a whole sentence in
sentence-level sentiment analysis. The main chal-
lenge of ABSA is to model the relationship be-
tween aspects and their associated opinions.
To this end, previous studies (Ma et al., 2018;
Zhou et al., 2020; Zhong et al., 2023) leveraged
external knowledge to enhance semantic features
in ABSA models. For example, Zhou et al. (2020)
employed words related to knowledge graphs to
build subgraphs as seed nodes. Subgraph-based
approaches yielded remarkable outcomes but may
entail complexity, particularly when dealing with a
large number of aspect terms. Zhong et al. (2023)
incorporated external knowledge graphs into low-
dimensional embeddings to efficiently represent
aspect-specific knowledge.
More recent studies (Zhang et al., 2019; Sun
et al., 2019; Chen et al., 2020; Liang et al., 2020;
Wang et al., 2020; Li et al., 2021; Liang et al.,
2022) have extensively investigated the use of
Graph Neural Networks (GNNs) on dependency
trees (Dep.Tree) and constituent trees (Con.Tree)
to explicitly leverage sentence syntactic structures.
While constituency and dependency trees sharearXiv:2402.07787v3  [cs.AI]  4 Mar 2024common sentential syntactic information, they cap-
ture syntactic details from distinct perspectives
(Dong et al., 2022).
Dependency trees (Dep.Trees) can establish con-
nections among words in a sentence (Li et al.,
2021), while constituent trees (Con.Trees) pro-
vide precise phrase segmentation and hierarchical
structures, which facilitate precise alignment of as-
pects with sentiment-indicative words (Liang et al.,
2022). We illustrate this with an example in Figure
1: (1) A dependency relation exists between the
aspect term " Looks " and the opinion term " nice";
(2) The phrase segmentation term " but" segments
"Looks nice " from " has a horribly cheap feel ".
Most of the previous work has already estab-
lished the effectiveness of single-granularity in-
formation for the ABSA task. However, single-
granularity features are insufficient to fully cap-
ture the rich information contained in the raw
data. Li et al. (2021) incorporating SynGCN and
SemGCN networks through a Mutual BiAffine
module, demonstrating the effectiveness of inte-
grating these two granularity levels for the ABSA
task.
However, most current methods use complex
and inefficient techniques to integrate diverse types
of knowledge. Currently, there is no scalable
framework capable of combining various multi-
granularity features ( e.g., syntactic, semantics, ex-
ternal knowledge graphs information, and so on) to
enhance model performance. In this context, a fun-
damental question arises: How can we ensure that
the combination of multiple granularity features
achieves a cumulative effect2and addresses the
problem of model scalability?
In this paper, we introduce a novel architecture
called the Extensible Multi-Granularity Fusion
Network model (EMGF) to address the aforemen-
tioned challenges. Firstly , we enhance the acquisi-
tion of affective representations in ABSA tasks by
integrating information from dependency syntax,
constituent syntax, semantic attention, and external
knowledge graphs. Secondly , we have developed
an Extensible Multi-Stage Fusion (EMSF) mod-
ule designed to capture profound and intricate in-
teractions among features at various granularities.
Moreover, it can integrate information of multiple
granularities at an extremely low computational
2Combining multiple features from various granularity
levels results in incremental effects. Specifically, with each
additional feature included, the effect improves compared to
the previous combination.cost, thereby achieving scalability. To elaborate,
our module comprises two stages: the "preprocess-
ing stage" and the "fusion stage." In the "prepro-
cessing stage," we employ a multi-anchor triplet
learning approach to combine dependency and con-
stituent syntactic information, enhancing their mu-
tual complementarity. We also utilize an orthogo-
nal projection layer to acquire refined syntactic and
semantic discriminative features. Finally , external
knowledge graphs offer supplementary information
support during the "fusion stage."
Our contributions are highlighted as follows:
1)For the ABSA task, we present an Extensible
Multi-Granularity Fusion Network designed to cap-
ture intricate interactions among features at various
granularities, thus achieving the cumulative effect.
2)This network can fuse an arbitrary number of
features of different granularities in an expandable
manner, at an extremely low computational cost.
3)We present multi-anchor triplet learning to
enable mutual learning between dependency syn-
tax and constituent syntax, and employ orthogonal
projection techniques to obtain refined syntactic
and semantic features.
4)Our experimental findings establish that our
EMGF model surpasses the current state-of-the-art
methods when evaluated on the SemEval 2014 and
Twitter datasets, demonstrating the effectiveness of
our EMGF model.
2 Related Work
ABSA is an entity-level and fine-grained task for
sentiment analysis (Li et al., 2021; Ma et al., 2023).
Early research in ABSA makes use of attention-
based neural models for the purpose of capturing
semantic interactions (Wang et al., 2016; Ma et al.,
2017; Xu et al., 2019).
Dependency with GNNs: Another emerging
trend is the effective incorporation of dependency
trees with Graph Neural Networks (GNNs). Xu
et al. (2020) introduce a GCN model with a hetero-
geneous graph, merging sentence and aspect nodes
via four relationship types, Liang et al. (2021)
propose a novel dependency syntactic knowledge
augmented interactive architecture with multi-task
learning, Zhang et al. (2022) enhance attention
score matrices with syntactic mask matrices for
integrating syntax and semantics, Zhao et al.
(2023) introduce RDGCN to better calculate depen-
dency importance, tackling syntactic ambiguities
in aspect-opinion analysis.Text Encoding Module
Constituent Parser
(SuPar)
Dependency Parser
(SuPar)Multi -Head 
Attention
Constituent GCN
Semantic GCN
Dependency GCNGranularity Feature Construction Module
KGE Toolkit
(OpenKE)Knowledge Graphs Embedding
ƒAverage 
PoolingSoftmaxExtensible Multi -Stage Fusion (EMSF) Module
MLP Project Layer
Eltwise MultiplicationMulti -Anchor 
Triple LearningOrthogonal 
Projection
Internal Processing 
Dropout Layer
Preprocessing Stage
Normalization
Fusion StageEMFBEMFBEMFBEMFBEMFBEMSFResidual Enhanced
Average Pooling
EMSFMulti -Anchor Triplet Learning Orthogonal Projection Layer
[CLS] Mac also … apps and programs … [SEP] apps [SEP]Language 
Model
(BERT)Raw Sentences:
Text Encoding Module
Constituent Parser
(SuPar)
Dependency Parser
(SuPar)Multi -Head 
Attention
Constituent GCN
Semantic GCN
Dependency GCNGranularity Feature Construction Module
KGE Toolkit
(OpenKE)Knowledge Graphs Embedding
ƒAverage 
PoolingSoftmaxExtensible Multi -Stage Fusion (EMSF) Module
MLP Project Layer
Eltwise MultiplicationMulti -Anchor 
Triple LearningOrthogonal 
Projection
Internal Processing 
Dropout Layer
Preprocessing Stage
Normalization
Fusion StageEMFBEMFBEMFBEMFBEMFBEMSFResidual Enhanced
Average Pooling
EMSFMulti -Anchor Triplet Learning Orthogonal Projection Layer
[CLS] Mac also … apps and programs … [SEP] apps [SEP]Language 
Model
(BERT)Raw Sentences:Figure 2: The overall architecture of our EMGF model.
Constituent with GNNs: Structural syntax
knowledge has been proven effective for seman-
tic role labeling (SRL) (Marcheggiani and Titov,
2020; Fei et al., 2021). Marcheggiani and Titov
(2020) showcases the utilization of GCNs to en-
code constituent structures in an SRL system, Fei
et al. (2021) jointly learns phrasal boundaries ex-
tracted from constituency and semantic relations
from dependency to explore the integration of di-
verse syntactic representations for SRL. For ABSA,
Liang et al. (2022) first focus on effectively har-
nessing syntactic information from the sentence’s
constituent tree to model the sentiment context of
individual aspects for learning.
3 Methodology
In this section, we provide a detailed explanation
of EMGF. The overview of the EMGF framework
is shown in Figure 2. The system comprises three
components: 1) The Text Encoding Module. 2)
The Granularity Feature Construction Module. 3)
The Extensible Multi-Stage Fusion Module.
3.1 Text Encoding Module
In the ABSA task, give a n-word sentence s=
{w1, w2, . . . , w n}, along with a specific aspect
represented as a={a1, a2, . . . , a m}, to deter-
mine its corresponding sentiment polarity class,
ca. Here, ais a sub-sequence of s, and ca∈
{Positive, Neutral, Negative }. To obtain con-
textualized representations, we utilize BERT (De-
vlin et al., 2019). In the BERT encoder, we
construct a sentence-aspect pair as input, repre-sented as x= ([CLS] s[SEP] a[SEP]) . The out-
put provides contextualized representations, de-
noted as Hbert= BERT( x). In this representation,
Hbert=
hbert
1, hbert
2,···, hbert
n
∈Rn×d, where d
represents the dimensionality of the last hidden
layer of BERT, and hbert
icorresponds to the contex-
tual representation of the i-th word.
3.2 Granularity Feature Construction Module
Dependency GCN The dependency graph con-
volutional networks (DepGCN) module takes syn-
tactic encoding as input and utilizes the probability
matrix of all dependency arcs from a dependency
parser to encode syntax information. The depen-
dency graph is embodied as an adjacency matrix
Adep∈Rn×n, which is defined as follows:
Adep
ij=1,if link (i, j) = 1
0,otherwise(1)
where link(i, j)shows that i-th and j-th words have
a dependence link. The dependency graph rep-
resentation Hdep={hdep
1, hdep
2, . . . , hdep
n}is then
obtained from the DepGCN module using the fol-
lowing formula:
hl
i=σnX
j=1AijWlhl−1
j+bl
(2)
here, Wlrepresents a weight matrix, bldenotes a
bias term, and σis an activation function, such as
ReLU.
Constituent GCN We follow the syntax struc-
ture of the Con.Tree in a bottom-up manner, in-
spired by BiSyn-GAT+ (Liang et al., 2022). TheCon.Tree is composed of multiple phrases ( phl
u)
that make up the input text, and we create corre-
sponding graphs based on these phrases phm
u.
Given the substantial depth of the constituent
tree, we choose a total of mlayers with alternating
intervals3. We make this choice because the vari-
ation in phrase hierarchical information between
adjacent layers is minimal, and excessive align-
ment would be an inefficient use of computational
resources. Additionally, the chosen value of m
aligns with the number of ConGCN layers.
The constituent graph is embodied as an adja-
cency matrix Acon∈Rlc×n×n, which is defined as
follows:
Acon(m)
i,j =1ifwiandwjare in same phm
u,
0otherwise
(3)
where mdenotes the level of the phrase
within the selected lclayers, while udenotes
the constituent label associated with the phrase,
such as S, NP, VP, and so on. Subsequently
yields the output hidden representation Hcon=
{hcon
1, hcon
2, . . . , hcon
n}is then obtained from the
ConGCN module using Eq. (2).
Semantic GCN To construct the attention score
matrix Asem, we employ the Multi-Head Attention
(MHA) mechanism on the hidden state features
Hbertderived from the BERT encoder. The MHA
computes attention scores among words, and the
formulation of the attention score matrix Asem∈
Rn×nis as follows:
Asem
ij=Softmax (MHA (hbert
i, hbert
j)) (4)
Subsequently yields the output hidden represen-
tation Hsem={hsem
1, hsem
2, . . . , hsem
n}is then ob-
tained from the SemGCN module using Eq. (2).
External Knowledge Zhong et al. (2023) syn-
ergistically combine contextual and knowledge
information to achieve more comprehensive fea-
ture representations. We introduce the external
knowledge as proposed by them, represented as
Hkge={hkge
1, hkge
2, . . . , hkge
n}.
3.3 Extensible Multi-Stage Fusion Module
In previous studies, it is common to combine only
two granularity features, so when trying to combine
additional features, the current model is no longer
3For instance, you can choose layer 1, skip one layer, pick
layer 3, and continue this pattern.applicable. This means that a model needs to be
redesigned that can be compatible with multiple
features at the same time, but this will increase the
complexity and computational cost of the model.
To address this challenge and capture intricate
interactions among features at different granular
levels while efficiently integrating diverse granu-
lar information, we introduce the extensible multi-
granularity fusion (EMGF) module. This innova-
tive approach allows for the expansion and effec-
tive exploration of interrelationships among multi-
granular features. It achieves this by cascading
multiple Extensible Multi-Stage Fusion (EMSF)
blocks, each comprising a "preprocessing stage"
and a "fusion stage." During the preprocessing
stage of EMSF, four features from different lev-
els serve as inputs, namely Hcon,Hdep,Hsem, and
Hkge.
3.3.1 Preprocessing Stage
Con.Tree and Dep.Tree share syntactic informa-
tion from different viewpoints (Dong et al., 2022).
(Ata et al., 2021; Dong et al., 2022) use multi-
view learning to study three relationship categories:
intra-node intra-view, intra-node inter-view, and
inter-node inter-view. We collectively label nodes
in these scenarios as "important nodes." However,
there is currently no research addressing how to
handle "non-important nodes," which could poten-
tially disrupt the complementary learning of "im-
portant nodes." Moreover, to handle these three
types of collaboration, it’s necessary to design three
distinct loss functions, adding complexity to the
model. To this end, we propose Multi-Anchor
Triplet Learning to address the two categories of
issues mentioned above.
Additionally, inspired by Qin et al. (2020), we
utilize orthogonal projection techniques to encour-
age the DepGCN and ConGCN networks to acquire
distinct syntactic features from the semantic fea-
tures generated by the SemGCN network. This
results in refined and more discriminative syntactic
and semantic features.granularity levels.
Within this stage, we combine Multi-Anchor
Triplet Learning and Orthogonal Projection Tech-
niques to effectively capture the complementary
and discriminative aspects of features across vari-
ous granularity levels.
Multi-Anchor Triplet Learning We choose a
node from the con-view graph as the "Anchor"
node and consider three scenarios: 1)In the con-view, all nodes connected to the Anchor are marked
as "pos" nodes, 2)Nodes in the dep-view that share
homologous4properties with the "Anchor" node
in the con-view are likewise designated as "pos"
nodes, 3)All "pos" nodes in the con-view corre-
spond to homologous nodes in the dep-view. If
these nodes are not connected to homologous nodes
corresponding to the Anchor, they are still labeled
as "pos" nodes. All other cases are labeled as "neg"
nodes. The same procedure is applied in the dep-
view when the Anchor node is located there.
It is vital to stress that nodes do not possess
equal significance. Designating all graph nodes
as Anchor nodes would undermine differentiation
and precision. Additionally, drawing inspiration
from the work of MP-GCN (Zhao et al., 2022), we
employ the Multi-Head S-Pool to select Anchor
nodes. Specifically, we use the attention matrix
Asemto conduct both average and maximum pool-
ing from two distinct perspectives, resulting in the
selection of the Top-K important nodes with the
highest scores.
Our goal is to have the "Anchor" node stay
close to the "pos" nodes to acquire complementary
knowledge, while minimizing interference from
"neg" nodes. Specifically, we accomplish this goal
by minimizing the following loss function:
Ltriplet=X
i∈AnchorσX
j∈posfa(||hz
i−hz′
j||2)
−X
j∈negfa(||hz
i−hz′
j||2) +margin(5)
Anchor =TopK (fa(Asem) +fm(Asem))(6)
where zandz′belong to the set {dep, con }, we
determine the size of the anchor set kbased on
Bourgain’s Theorem-1 (You et al., 2019). Here,
kis expressed as k=clog2n, with crepresent-
ing a constant, and ndenoting the total number of
nodes in the graph. Our approach employs func-
tions fafor average pooling and fmfor maximum
pooling. The "margin" hyperparameter in control-
ling the boundary of the triplet loss function, and
σcorresponds to the non-linear activation function
ReLU.
Orthogonal Projection Techniques Mathemat-
ically, we first project dependency syntax feature
Hdeponto semantic feature Hsem:
Hdep∗=Proj 
Hdep, Hsem(7)
4A homologous node refers to a node that corresponds to
the same entity in different data views.where Proj is a projection function.
Proj (x, y) =x·y
|y|y
|y|(8)
where xandyare vectors. Next, we perform the
projection in the orthogonal direction of the pro-
jected feature Hdepto obtain a purer classification
feature vector.
]Hdep=Proj 
Hdep,(Hdep−Hdep∗)
(9)
Correspondingly, the terms ]Hconin the formula
can be expressed as follows:
]Hcon=Proj 
Hcon,(Hcon−Hcon∗)
(10)
3.3.2 Fusion Stage
Building on the preprocessing stage, we utilize the
purified dependency syntatic]Hdep, the purified con-
stituent syntactic ]Hcon, the semantic feature Hsem,
and the extra knowledge feature Hkgeas inputs
during the fusion stage. Furthermore, inspired by
the multimodal fusion method MAMN (Xue et al.,
2023a,b), we adopt the extended multimodal fac-
torized bilinear pooling mechanism from MAMN
in fusion stage to fuse]Hdep,]Hcon,Hsem, and ex-
ternal knowledge feature Hkge. The Fusion Stage
is calculated as:
Zi
m=Norm
˜UT
dep]Hdep◦˜UT
con]Hcon
◦˜UT
semHsem◦˜UT
kgeHkge (11)
where all ˜Urepresent learnable weight parameters,
Norm denotes the normalization layer, and Zi
m
represents the outputs of the fusion stages within
thei-th EMSF block. Additionally, we have in-
troduced residual connections between different
blocks. Subsequently, we calculate the average of
the outputs from these leEMSF blocks (where le
indicates the number of EMSF blocks) to obtain
the feature rwith four distinct granularity fusions.
The specific formula is as follows:
Zi+1
m=Zi
m+ EMSF(
Zi
m,]Hdep,]Hcon, Hsem, Hkge)(12)
To obtain the final output, denoted as rfor the
EMGF, we concatenate the output features from
thelmEMSF blocks and apply average pooling.
r= Mean
Z1
m,Z2
m, . . . ,Zlmm
(13)Dataset#Positve #Negative #Neutral
Train Dev Test Train Dev Test Train Dev Test
Laptop 976 - 337 851 - 128 455 - 167
Restaurant 2164 - 727 807 - 196 637 - 196
Twitter 1507 - 172 1528 - 169 3016 - 336
MAMS 3380 403 400 2764 325 329 5042 604 607
Table 1: Satistics of four datasets.
3.4 Model Training
Softmax Classifier Subsequently, the fusion fea-
turer, obtained from the granularity fusion module,
is used to calculate the sentiment probability dis-
tribution ˆy(s,a)via a linear layer equipped with a
softmax function:
ˆy(s,a)=Softmax (Wpr+bp) (14)
where (s, a)is a sentence-aspect pair.
Our training goal is to minimize the following
overall objective function:
L(Θ) = Lc+βLtriplet (15)
where Θdenotes all the trainable model parameters,
while βare hyperparameters. The cross-entropy
lossLcfor the primary classification task is defined
as follows:
Lc=X
(s,a)∈Dy(s,a)log ˆy(s,a) (16)
where Dcontains all sentence-aspect pairs and
y(s,a)is the real distribution of sentiment.
4 Experiments
4.1 Datasets
Our model was evaluated using four bench-
mark datasets: Laptop and Restaurant from Se-
mEval2014 Task 4 (Pontiki et al., 2014), Twitter
(Dong et al., 2014), and the large-scale multi-aspect
MAMS dataset (Jiang et al., 2019). Consistent with
prior studies (Chen et al., 2017; Li et al., 2021; Tang
et al., 2022) and others, we excluded instances la-
beled as "conflict." The statistics of these datasets
are presented in Table 1.
4.2 Implementation Details
We utilized SuPar5as our parser to acquire both the
dependency and constituent tree. For constructing
our model, we employed the uncased base version
5https://github.com/yzhangcs/parserof BERT6with a dropout rate of 0.3. The training
process was conducted with a batch size of 16, uti-
lizing the Adam optimizer with a learning rate of
2e-5. For the four datasets, we set the ConGCN,
DepGCN, and SemGCN layers to (6, 3, 6, 6), (3,
3, 9, 3), and (3, 3, 1, 3), respectively, with βco-
efficients of (0.12, 0.12, 0.07, 0.12). We selected
3 layers ( lc) for the constituent tree and optimized
its performance. Additionally, we determined that
6 layers ( le) are optimal for EMSF blocks. The
hyper-parameter margin was set to 0.2. Each exper-
iment is replicated three times, with the results then
averaged for consistency. Our primary evaluation
metrics include accuracy (Acc.) and macro-f1 (F1).
4.3 Baseline Methods
We compare our EMGF with state-of-the-art base-
lines, described as follows:
1)BERT-SRC (Devlin et al., 2019) represents the
fine-tuning of BERT to incorporate aspect-specific
representations. 2) CDT (Sun et al., 2019) inves-
tigate combining dependency trees and neural net-
works for representation learning. 3) DualGCN (Li
et al., 2021) simultaneously considers the comple-
mentarity of syntax structures and semantic corre-
lations. 4) SSEGCN (Zhang et al., 2022) integrates
aspect-aware and self-attention mechanisms to en-
hance the precision of ABSA. 5) MGFN (Tang
et al., 2022) utilize a latent graph to leverage depen-
dency relation and semantic information. 6) TF-
BERT (Zhang et al., 2023) examines span-level
consistency in multi-word opinion expressions. 7)
HyCxG (Xu et al., 2023) introduce construction
grammar (CxG) to enrich language representation.
4.4 Main Results
Table 2 summarizes the primary experimental re-
sults. The EMGF model exceeds the current state-
of-the-art (SOTA) baseline, HyCxG (Xu et al.,
2023), in both the Laptop and Restaurant bench-
marks. Models that incorporate syntactic depen-
dency information tend to outperform those that
do not, but relying solely on syntactic informa-
tion may lead to subpar performance, particularly
with informal or complex sentences. Leveraging
richer syntax dependency labels and incorporating
affective semantic information, as demonstrated
by models such as (Li et al., 2021; Tang et al.,
2022), generally outperforms syntax-only models,
highlighting the effectiveness of integrating diverse
6https://github.com/huggingface/transformersModelLaptop Restaurant Twitter MAMS
Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1
BERT-SRC (Devlin et al., 2019) 78.99 75.03 84.46 76.98 73.55 72.14 82.34 81.94
CDT (Sun et al., 2019) 79.70 75.61 86.36 80.16 77.50 76.54 - -
DualGCN (Li et al., 2021) 81.80 78.10 87.13 81.16 77.40 76.02 - -
SSEGCN (Zhang et al., 2022) 81.01 77.96 87.31 81.09 77.40 76.02 - -
MGFN (Tang et al., 2022) 81.83 78.26 87.31 82.37 78.29 77.27 - -
TF-BERT (Zhang et al., 2023) 81.49 78.30 86.95 81.43 77.84 76.23 - -
HyCxG (Xu et al., 2023) 82.29 79.11 87.32 82.24 - - 85.03 84.40
Our EMGF 82.11 79.24 88.42 83.20 78.87 78.06 85.48 84.73
Table 2: Experimental results on ABSA datasets with BERT encoder. The best result on each dataset is in bold.
ModelLaptop Restaurant Twitter MAMS
Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1
Our EMGF (M4) 82.11 79.24 88.42 83.20 78.87 78.06 85.48 84.73
EMGF-M3 81.26 78.24 87.78 82.23 77.53 76.27 85.11 84.13
EMGF-M2 80.79 77.61 87.33 81.59 76.64 76.12 84.32 83.75
EMGF-M1 80.15 77.07 86.24 80.12 76.49 75.05 83.34 82.73
W/OLtriplet 80.84 76.83 86.97 81.06 76.93 75.61 83.54 83.21
W/O Orthogonal Projection 79.41 75.24 86.15 80.22 77.83 76.53 84.44 84.13
W/O Dep Project Sem 80.52 76.92 86.15 79.96 76.04 75.20 84.44 83.87
W/O Con Project Sem 80.37 76.47 85.70 79.66 76.19 74.98 83.99 83.48
Table 3: Ablation study experimental results.
feature information. Experimental results indicate
that our EMGF effectively integrates information
from four different granularities.
4.5 Ablation Study
We evaluated the extensibility of EMGF and the
effectiveness of its fusion approach by investigat-
ing how the number of granularity features affects
EMGF’s performance, the results are shown in Ta-
ble 3. M4 indicates using all granularity features,
M3 represents selecting three out of four granu-
larity features (selected through combinatorial per-
mutations) and averaging all possibilities. M2 and
M1 follow a similar pattern. As we reduced the
number of granularity features, we observed a de-
crease in performance, highlighting the extensibil-
ity of EMGF and the effectiveness of our fusion
approach, which cumulative effects. W/OLtriplet
result in reduced performance of EMGF, this shows
that multi-anchor triplet learning can gather com-
plementary knowledge from various syntactic fea-
ture information, thereby improving the model’s
performance. The expression "Dep Project Sem
(Con Project Sem)" denotes the projection of syn-
tactic features related to dependency (constituent)
onto orthogonal spaces associated with semantic
features. W/O Dep Project Sem, W/O Con Project
Sem, and W/O Orthogonal Projection Techniques,
all lead to a decrease in EMGF performance. Thisimplies that omitting the feature projections hin-
ders the model’s ability to accurately differentiate
between syntactic and semantic information, caus-
ing interference from redundant data during the
fusion stage.
4.6 Case Study
Table 4 illustrates our model through four exam-
ples. Identifying neutral sentiment is challenging
due to a lack of strong sentiment words in neutral
texts and data imbalance, with more data available
for positive and negative sentiments. In the third
sentence, MGFN incorrectly predicted the emo-
tional polarity of " chef." This can be attributed to
MGFN’s inability to capture its specific opinion
words associated with " chef," it incorrectly treated
the opinion words from " food" and " service " as
its own. The fourth sentence is particularly chal-
lenging, as MGFN, like many models, assigns pos-
itive sentiment to an aspect word without strong
emotional cues, causing three out of four EMGF
predictions to be incorrect. Drawing from our anal-
ysis, MGFN combines syntactic features derived
from the latent graph with semantic features. How-
ever, similar to other models, MGFN does not fully
capitalize on the potential offered by a variety of
granularity features. In juxtaposition, our EMGF
effectively leverages these features and their syner-
gistic effects through multi-anchor triplet learningSentence MGFN (Tang et al., 2022) EMGF(Ours)
I know real [Indian food ]negand this wasn’t it. (neu ) (neg )
Our[waiter ]poswas friendly and it is a shame that he didnt
have a supportive [staff]negto work with.(pos ,pos ) (pos ,neg )
Even when the [chef]neuis not in the house, the [food]pos
and[service ]posare right on target.(pos ,pos ,pos ) (neu ,pos ,pos )
We started with the [scallops ]neuand[asparagus ]neuand also
had the [soft shell crab ]neuas well as the [cheese plate ]neu.(pos ,pos ,pos ,neu ) (neu ,neu ,neu ,neu )
Table 4: Case study experimental results of MGFN and EMGF.
 lcon=ldep=lsem
 lcon
 ldep
 lsem
 le
1 2 3 4 5 6 7 8 9 1085.085.586.086.587.087.588.088.589.0Acc.(%)
GCN and EMSF Layers
 lcon=ldep=lsem
 lcon
 ldep
 lsem
 le
1 2 3 4 5 6 7 8 9 1077.078.079.080.081.082.083.084.085.0F1.(%)
GCN and EMSF Layers
Figure 3: The impact of the number of GCN and EMSF
block on Restaurant dataset.
and orthogonal projection.
4.7 Impact of Number of GCN and EMSF
Blocks
We varied the number of layers, lcon,ldep, and
lsem from 2 to 9 for ConGCN, DepGCN, and
SemGCN to assess their impact on the model’s
performance on Restaurant dataset. Based on ex-
perimental results, we set lcon,ldep, and lsemto 3,
3, and 9, respectively. Interestingly, maintaining
consistent layer numbers for lcon,ldep, and lsem
does not necessarily result in optimal performance.
We observed that considering the layer count sep-
arately for each of the three GCN types tends to
enhance performance. The number of cascaded
EMFB blocks (denoted as le) affects prediction ac-
curacy and F1 score. Through experiments, we
determined that the optimal number of modules is
6, as depicted in Figure 3.
4.8 Hype-parameter Analysis
We will investigate the impact of a crucial parame-
ter,k, in EMGF. This relates to selecting the num-
ber of crucial nodes in each view. We have con-
ducted experiments with various kvalues, such as
c,log2(n),log2n,n
4,n
3,n
2, where cis a constant,
andnrepresents the number of view nodes. The
value of cvaries from 1 to 5, and we calculate the
average performance. We can see from Figure 4
87.5188.42
86.0686.5286.15 86.33
82.0283.2
79.8880.5 80.5180.12
7072747678808284868890
7072747678808284868890
F1.(%)Acc.(%) Acc
 F1Figure 4: The impact of different kon Restaurant
dataset.
that the average performance reaches its peak when
kequals log2(n).
5 Conclusion
Through efficient integration of diverse granularity
features, including dependency and constituent syn-
tactic, attention semantic, and external knowledge
graphs, EMGF demonstrates superior performance
compared to existing ABSA methods. This study
has tackled the persistent challenge of fully lever-
aging the combined potential of diverse granularity
features in the ABSA framework. EMGF effec-
tively captures complex interactions among these
features by employing multi-anchor triplet learn-
ing and orthogonal projection techniques, yielding
a cumulative effect without incurring additional
computational expenses. EMGF offers a scalable
and flexible framework for integrating a variety of
multi-granularity features in ABSA, thereby en-
hancing model performance.
Limitations
Although our research has achieved commendable
results, there are limitations worth acknowledging.These limitations underscore areas for future im-
provement and exploration. In this experiment, due
to limited computational resources, we selected
the top- knodes as Anchor nodes in multi-anchor
triplet learning. However, when we attempted to set
the value of kto{log2n,n
4,n
3,n
2}magnitude, we
observed that the model training was excessively
slow, and we had to adjust the magnitude of kto a
smaller scale for experimentation. Finally, due to
constraints in computational power and time, we
were unable to explore larger model architectures
or conduct extensive hyperparameter tuning. We
hope that future research can address these limita-
tions to enhance the reliability and applicability of
the method we propose.
Ethics Statement
Our research adhered to rigorous ethical guide-
lines and principles throughout its execution. All
participants were granted informed consent, with
clear and comprehensive information regarding the
study’s objectives and procedures. We are com-
mitted to reporting the study’s findings and results
objectively and accurately, without any form of ma-
nipulation or misrepresentation. Our dedication to
upholding the highest ethical standards in research
ensures the integrity and validity of our discover-
ies. None of the authors of this article conducted
studies involving human participants or animals.
Furthermore, we affirm that none of the authors
have any known competing financial interests or
personal relationships that might potentially influ-
ence the work presented in this paper.
References
Sezin Kircali Ata, Yuan Fang, Min Wu, Jiaqi Shi,
Chee Keong Kwoh, and Xiaoli Li. 2021. Multi-
view collaborative network embedding. ACM Trans.
Knowl. Discov. Data , 15(3):39:1–39:18.
Chenhua Chen, Zhiyang Teng, and Yue Zhang. 2020.
Inducing target-specific latent structures for aspect
sentiment classification. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 5596–5607, On-
line. Association for Computational Linguistics.
Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang.
2017. Recurrent attention network on memory for
aspect sentiment analysis. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing , pages 452–461, Copenhagen,
Denmark. Association for Computational Linguis-
tics.Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li.
2022. Syntactic multi-view learning for open infor-
mation extraction. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 4072–4083, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent Twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers) , pages 49–54, Baltimore,
Maryland. Association for Computational Linguis-
tics.
Hao Fei, Shengqiong Wu, Yafeng Ren, Fei Li, and
Donghong Ji. 2021. Better combine them together!
integrating syntactic constituency and dependency
representations for semantic role labeling. In Find-
ings of the Association for Computational Linguistics:
ACL-IJCNLP 2021 , pages 549–559, Online. Associa-
tion for Computational Linguistics.
Qingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, and
Min Yang. 2019. A challenge dataset and effec-
tive models for aspect-based sentiment analysis. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 6280–
6285, Hong Kong, China. Association for Computa-
tional Linguistics.
Ruifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xi-
aojie Wang, and Eduard Hovy. 2021. Dual graph
convolutional networks for aspect-based sentiment
analysis. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 6319–6329, Online. Association for Computa-
tional Linguistics.
Bin Liang, Rongdi Yin, Lin Gui, Jiachen Du, and
Ruifeng Xu. 2020. Jointly learning aspect-focused
and inter-aspect relations with graph convolutional
networks for aspect sentiment analysis. In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics , pages 150–161, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.
Shuo Liang, Wei Wei, Xian-Ling Mao, Fei Wang, and
Zhiyong He. 2022. BiSyn-GAT+: Bi-syntax awaregraph attention network for aspect-based sentiment
analysis. In Findings of the Association for Compu-
tational Linguistics: ACL 2022 , pages 1835–1848,
Dublin, Ireland. Association for Computational Lin-
guistics.
Yunlong Liang, Fandong Meng, Jinchao Zhang, Yufeng
Chen, Jinan Xu, and Jie Zhou. 2021. A dependency
syntactic knowledge augmented interactive architec-
ture for end-to-end aspect-based sentiment analysis.
Neurocomputing , 454:291–302.
Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng
Wang. 2017. Interactive attention networks for
aspect-level sentiment classification. In Proceedings
of the Twenty-Sixth International Joint Conference
on Artificial Intelligence, IJCAI 2017, Melbourne,
Australia, August 19-25, 2017 , pages 4068–4074. ij-
cai.org.
Fukun Ma, Xuming Hu, Aiwei Liu, Yawen Yang,
Shuang Li, Philip S. Yu, and Lijie Wen. 2023. AMR-
based network for aspect-based sentiment analysis.
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 322–337, Toronto, Canada.
Association for Computational Linguistics.
Yukun Ma, Haiyun Peng, and Erik Cambria. 2018. Tar-
geted aspect-based sentiment analysis via embedding
commonsense knowledge into an attentive LSTM. In
Proceedings of the Thirty-Second AAAI Conference
on Artificial Intelligence, (AAAI-18), the 30th inno-
vative Applications of Artificial Intelligence (IAAI-
18), and the 8th AAAI Symposium on Educational
Advances in Artificial Intelligence (EAAI-18), New
Orleans, Louisiana, USA, February 2-7, 2018 , pages
5876–5883. AAAI Press.
Diego Marcheggiani and Ivan Titov. 2020. Graph con-
volutions over constituent trees for syntax-aware se-
mantic role labeling. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 3915–3928, Online. As-
sociation for Computational Linguistics.
Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-
ris Papageorgiou, Ion Androutsopoulos, and Suresh
Manandhar. 2014. SemEval-2014 task 4: Aspect
based sentiment analysis. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014) , pages 27–35, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Qi Qin, Wenpeng Hu, and Bing Liu. 2020. Feature pro-
jection for improved text classification. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics , pages 8161–8171, On-
line. Association for Computational Linguistics.
Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao,
and Xudong Liu. 2019. Aspect-level sentiment analy-
sis via convolution over dependency tree. In Proceed-
ings of the 2019 Conference on Empirical Methodsin Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 5679–5688, Hong
Kong, China. Association for Computational Linguis-
tics.
Siyu Tang, Heyan Chai, Ziyi Yao, Ye Ding, Cuiyun
Gao, Binxing Fang, and Qing Liao. 2022. Affective
knowledge enhanced multiple-graph fusion networks
for aspect-based sentiment analysis. In Proceedings
of the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 5352–5362, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan,
and Rui Wang. 2020. Relational graph attention net-
work for aspect-based sentiment analysis. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 3229–
3238, Online. Association for Computational Lin-
guistics.
Yequan Wang, Minlie Huang, Xiaoyan Zhu, and
Li Zhao. 2016. Attention-based LSTM for aspect-
level sentiment classification. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing , pages 606–615, Austin, Texas.
Association for Computational Linguistics.
Hu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019. BERT
post-training for review reading comprehension and
aspect-based sentiment analysis. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 2324–2335, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Kuanhong Xu, Hui Zhao, and Tianwen Liu. 2020.
Aspect-specific heterogeneous graph convolutional
network for aspect-based sentiment classification.
IEEE Access , 8:139346–139355.
Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Zhilin Gong,
Ming Cai, and Tianxiang Wang. 2023. Enhancing
language representation with constructional informa-
tion for natural language understanding. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 4685–4705, Toronto, Canada. Association for
Computational Linguistics.
Xiaojun Xue, Chunxia Zhang, Zhendong Niu, and Xin-
dong Wu. 2023a. Multi-level attention map net-
work for multimodal sentiment analysis. IEEE Trans.
Knowl. Data Eng. , 35(5):5105–5118.
Xiaojun Xue, Chunxia Zhang, Tianxiang Xu, and Zhen-
dong Niu. 2023b. Constrained tuple extraction with
interaction-aware network. In Proceedings of the
61st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers) ,
pages 11430–11444, Toronto, Canada. Association
for Computational Linguistics.Jiaxuan You, Rex Ying, and Jure Leskovec. 2019.
Position-aware graph neural networks. In Proceed-
ings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA , volume 97 of Proceedings of Ma-
chine Learning Research , pages 7134–7143. PMLR.
Chen Zhang, Qiuchi Li, and Dawei Song. 2019. Aspect-
based sentiment classification with aspect-specific
graph convolutional networks. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 4568–4578, Hong Kong,
China. Association for Computational Linguistics.
Mao Zhang, Yongxin Zhu, Zhen Liu, Zhimin Bao, Yun-
fei Wu, Xing Sun, and Linli Xu. 2023. Span-level
aspect-based sentiment analysis via table filling. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 9273–9284, Toronto, Canada.
Association for Computational Linguistics.
Zheng Zhang, Zili Zhou, and Yanna Wang. 2022.
SSEGCN: Syntactic and semantic enhanced graph
convolutional network for aspect-based sentiment
analysis. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 4916–4925, Seattle, United States.
Association for Computational Linguistics.
Hongyu Zhao, Jiazhi Xie, and Hongbin Wang. 2022.
Graph convolutional network based on multi-head
pooling for short text classification. IEEE Access ,
10:11947–11956.
Xusheng Zhao, Hao Peng, Qiong Dai, Xu Bai, Huail-
iang Peng, Yanbing Liu, Qinglang Guo, and Philip S.
Yu. 2023. RDGCN: reinforced dependency graph
convolutional network for aspect-based sentiment
analysis. CoRR , abs/2311.04467.
Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Hua
Jin, and Dacheng Tao. 2023. Knowledge graph aug-
mented network towards multiview representation
learning for aspect-based sentiment analysis. IEEE
Trans. Knowl. Data Eng. , 35(10):10098–10111.
Jie Zhou, Jimmy Xiangji Huang, Qinmin Vivian Hu,
and Liang He. 2020. SK-GCN: modeling syntax
and knowledge via graph convolutional network for
aspect-level sentiment classification. Knowl. Based
Syst., 205:106292.