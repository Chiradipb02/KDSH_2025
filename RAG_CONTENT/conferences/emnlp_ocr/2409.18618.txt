Model-based Preference Optimization
in Abstractive Summarization without Human Feedback
Jaepill Choi∗Kyubyung Chae∗Jiwoo Song Yohan Jo Taesup Kim†
Graduate School of Data Science, Seoul National University
{jaepill9205, kyubyung.chae, sjiwoo, yohan.jo, taesup.kim}@snu.ac.kr
Abstract
In abstractive summarization, the challenge
of producing concise and accurate summaries
arises from the vast amount of information con-
tained in the source document. Consequently,
although Large Language Models (LLMs) can
generate fluent text, they often introduce in-
accuracies by hallucinating content not found
in the original source. While supervised fine-
tuning methods that maximize likelihood con-
tribute to this issue, they do not consistently
enhance the faithfulness of the summaries.
Preference-based optimization methods, such
as Direct Preference Optimization (DPO), can
further refine the model to align with human
preferences. However, these methods still heav-
ily depend on costly human feedback. In this
work, we introduce a novel and straightforward
approach called Model-based Preference Op-
timization (MPO) to fine-tune LLMs for im-
proved summarization abilities without any hu-
man feedback. By leveraging the model’s in-
herent summarization capabilities, we create
a preference dataset that is fully generated by
the model using different decoding strategies.
Our experiments on standard summarization
datasets and various metrics demonstrate that
our proposed MPO significantly enhances the
quality of generated summaries without relying
on human feedback. The code is publicly avail-
able at https://github.com/cjaep/MPO .
1 Introduction
Large Language Models (LLMs) have demon-
strated remarkable capabilities in generating flu-
ent and plausible text (Wang and Komatsuzaki,
2021; Touvron et al., 2023a; Jiang et al., 2023).
However, despite these advancements, LLMs of-
ten produce summaries that, while plausible, con-
tain incorrect or contradictory information—a phe-
nomenon known as hallucination (Maynez et al.,
*Equal contribution.
†Corresponding author.
Figure 1: Summarized results via automated metrics.
Our method MPO, which uses the model-generated sum-
maries for preference optimization, proves to be more
effective than PPO and DPO, both of which use human
preference datasets for optimization. The results are
from using the GPT-J on the TL;DR dataset.
2020). The fundamental reason for this issue is
that LLMs are primarily trained to predict the
most likely next token based on maximum like-
lihood, which is the most common objective for
pre-training language models (King et al., 2022).
In principle, reinforcement learning based objec-
tives can circumvent these failures by choosing an
appropriate reward function (Paulus et al., 2018;
Tian et al., 2024). Recently, reinforcement learn-
ing from human feedback (RLHF) has focused on
aligning language models with human preferences,
thereby effectively enhancing the models’ summa-
rization abilities (Böhm et al., 2019; Pasunuru and
Bansal, 2018; Stiennon et al., 2020; Paulus et al.,
2018; Ramamurthy et al., 2023).
While RLHF and other preference-based opti-
mization methods (Rafailov et al., 2023) effectively
fine-tune models to align with human preferences,
human feedback is not always reliable. For exam-
ple, even though the quality of text summaries de-
pends on various factors, Hosking et al. (2024)arXiv:2409.18618v3  [cs.CL]  2 Oct 2024demonstrated that human preferences often over-
look factuality and consistency, which are crucial
in avoiding hallucination. This implies that a sum-
mary judged as favorable by humans is not nec-
essarily free from hallucination. In other words,
preference optimization with human feedback does
not guarantee improved faithfulness. Moreover, the
use of human preference faces challenges related to
the collection of human-annotated data. Although
RLHF does not require massive amounts of data to
enhance performance, sourcing high-quality human
preference data remains an expensive process (Min
et al., 2023).
To address these challenges, prior works have
aimed to conduct preference optimization without
relying on human preferences (Paulus et al., 2018;
Tian et al., 2024; Wei et al., 2024; Roit et al., 2023).
Such methods often require external metrics or
complex filtering processes to establish preference
pairs. For instance, Paulus et al. (2018) utilized lex-
ical overlap (ROUGE) to assess salience and an en-
tailment score to evaluate factual consistency. Sim-
ilarly, Tian et al. (2024) employed FactScore (Min
et al., 2023) to gauge reward signals between gen-
erated summaries. However, as stated by Good-
hart’s Law—‘ When a measure becomes a target,
it ceases to be a good measure ’—relying exces-
sively on these imperfect metrics carries the risk
of overfitting to the metrics alone (Strathern, 1997;
Ramamurthy et al., 2023).
In response, we propose Model-based Prefer-
ence Optimization (MPO), a novel and straightfor-
ward approach that leverages the model’s inherent
summarization capabilities without relying on any
human feedback or external metrics. This method
generates faithful summaries by aligning prefer-
ences between responses generated using different
decoding strategies. In particular, we utilize (1) a
deterministic decoding strategy ( e.g., beam search
decoding) to generate chosen samples and (2) a
stochastic decoding strategy ( e.g., temperature sam-
pling) to generate rejected samples. Therefore, our
approach does not require any external knowledge
or metrics to construct preference pairs.
Previous studies have shown that deterministic
decoding strategies tend to generate outputs that are
less surprising and closely aligned with the source
text, while stochastic decoding introduces random-
ness, making it more prone to hallucinations (Yang
et al., 2018; Welleck et al., 2020a; Holtzman et al.,
2020; Lee et al., 2022). Specifically, Wan et al.
(2023) presented empirical evidence indicating thatbeam search yields the most faithful summaries,
while the randomness introduced by sampling re-
duces faithfulness. Based on these findings, we
align our model’s preference toward summaries
generated via beam search rather than those ran-
domly sampled. As illustrated in Figure 1, our ap-
proach outperforms models trained with standard
supervised fine-tuning (SFT) or those optimized
with human preferences ( e.g., PPO, DPO) in terms
of faithfulness and relevance to the source text.
Our main contribution is Model-based Prefer-
ence Optimization (MPO), a simple and straightfor-
ward approach for fine-tuning language models to
improve abstractive summarization without relying
on any human feedback or external metrics. Our
experimental results demonstrate that MPO outper-
forms models optimized with human preferences,
offering superior overall performance and greater
generalizability across diverse language models
and datasets.
2 Preliminaries
2.1 Problem Setup
LetVdenote the vocabulary for both input and
output. We represent the input document as x∈ X
and the output summary as y=⟨y0, . . . , y T⟩ ∈ Y .
The sequence yconsists of T+1elements, starting
with the beginning-of-sequence token y0and ends
with the end-of-sequence token yT.
A language model (LM) is an auto-regressive
model of a sequence distribution P(y|x), where
each conditional probability is parameterized by
a neural network pθ. We assume that the model
computes the probability of the entire generated
textyusing a common left-to-right decomposition.
Thus, the distribution can be expressed as a product
of conditional probabilities:
P(y|x) =TY
t=1pθ(yt|y<t,x).
2.2 LM for Summarization
Given an input document x, the optimal summary
yfrom the set of valid strings Yis obtained using
a scoring function:
y∗= argmax
y∈Ypθ(y|x).
However, finding the optimal summary is not
tractable. Therefore, the scoring function for the
optimal string yvaries according to decoding strate-
gies to approximate the best possible output. ThereFigure 2: Model-based Preference Optimization. Our method follows a two-step process: 1) Supervised Fine-
Tuning (SFT): we fine-tune a pre-trained model ( i.e., LLM) on a given dataset. 2) Model-based Preference Opti-
mization (MPO): we build a preference dataset using different decoding strategies. In this step, the chosen samples
are derived from deterministic decoding results, while the rejected samples utilize results generated by stochastic
decoding.
are two types of decoding strategies: stochastic and
deterministic.
Stochastic Decoding The simplest approach in
decoding strategies is to sample directly from the
probabilities predicted by the model. This method
involves sampling from the conditional probability
distribution at each step, represented as:
ytemp∼P(yt|x,y<t).
However, this method exhibits high variance. To
adjust for this variance, the temperature of the soft-
max function can be modified:
P(yt|x,y<t) = softmaxpθ(yt|x,y<t)
τ
,
where τis the temperature parameter. Increasing τ
causes the model’s conditional probability distribu-
tion to approach a uniform distribution, which can
lead to the generation of random tokens that are
irrelevant to the source documents. Consequently,
this increases the risk of the model producing hal-
lucinations. For this reason, we classify samples
generated through stochastic decoding as rejected
samples in our preference dataset.
Deterministic Decoding The other strategies
are deterministic decoding algorithms. The most
straightforward algorithm, called greedy decoding,
simply selects the most probable token at each
step (Welleck et al., 2020a). This can be expressed
as:
ygreedy = argmax
y∈Vlogpθ(yt|y<t,x).
In contrast to greedy decoding, beam search de-
coding considers the top- kcandidates for token
generation. At each time step t, it tracks the kmostlikely sequence hypotheses, where kis the beam
size. The output sequence can be represented as:
ybeam= argmax
y∈VLX
t=1logpθ(yt|y<t,x),
where Lis the length of the final candidate se-
quence. These deterministic decoding strategies
tend to produce tokens that are more closely related
to the source document, resulting in more faithful
summaries than those generated by stochastic de-
coding strategies. Therefore, we align our model’s
preference toward summaries generated via the de-
terministic decoding strategies and define them as
chosen samples in our preference dataset.
3 Proposed Method
In this section, we detail our process for encourag-
ing faithfulness in abstractive summarization. We
follow the typical pipelines of preference optimiza-
tion (Rafailov et al., 2023; Ziegler et al., 2020; Sti-
ennon et al., 2020; Ouyang et al., 2022). However,
by leveraging the differences between determinis-
tic and stochastic decoding strategies, our pipeline
does not require any external knowledge ( e.g., eval-
uation metrics) or human feedback. This pipeline
is depicted in Figure 2.
3.1 Supervised Fine-Tuning (SFT)
For the summarization task, we first fine-tune a pre-
trained language model using supervised learning
on training data ( i.e., ground truth data), denoted as
Dtrain={(x,yref)}. Based on this supervised fine-
tuning (SFT) approach, the model is trained to gen-
erate a single-sentence summary from a source doc-
ument. In this work, we utilize existing SFT models
with minimal modifications or apply SFT to pre-
trained language models using QLoRA (Dettmers
et al., 2023).3.2 Preference Optimization
For preference optimization, we employ Di-
rect Preference Optimization (DPO, Rafailov
et al., 2023). DPO simplifies the process by elim-
inating the need for an explicit reward function,
making it preferable to RL-based algorithms, which
incur significant computational costs by training
multiple language models and sampling from the
policy.
Given a dataset of preference pairs D=
{(xi,yw
i,yl
i)}N
i=1, where xirepresents source doc-
uments, yw
iare chosen responses, and yl
iare re-
jected responses, the probability of observing a
preference pair is modeled using the Bradley-Terry
model (Bradley and Terry, 1952):
p(yw≻yl) =σ(r(x,yw)−r(x,yl)),
where σis the sigmoid function, and r(·,·)is a
reward function.
Rafailov et al. (2023) demonstrated that models
directly learn this policy from collected data with-
out modeling the reward function. In other words,
the 2-stage policy can be simplified into 1-stage
policy. DPO loss can be expressed as:
LDPO(πθ;πref) =
−E(x,yw,yl)∼D
logσ
βlogπθ(yw|x)
πref(yw|x)
−βlogπθ(yl|x)
πref(yl|x)
,
where πrefis the SFT model and βis a coefficient
that controls the trade-off between reward and di-
vergence. By optimizing this objective, the model
aligns with the reward function while remaining
close to the pre-trained reference model, thus mini-
mizing over-optimization (Tian et al., 2024).
3.3 Constructing Preferences Pairs without
Human Feedback
By exploiting the differences between determin-
istic and stochastic strategies, we construct a
dataset of preference pairs, denoted as Dvalid=
{(x,yw
beam,yl
temp)}. This strategy is based on the
observation that deterministic decoding typically
produces more factual summaries (Wan et al.,
2023). This significant difference in output quality
suggests that summaries generated through beam
search decoding can be used as chosen samples,
while those from temperature sampling can be des-
ignated as rejected samples. We then conduct pref-erence optimization with this generated data to re-
fine the language model, ensuring it avoids gener-
ating hallucinated or irrelevant text.
4 Experiments
4.1 Experimental Setup
Dataset We used the TL;DR dataset and the eX-
treme Summarization (XSUM) dataset (Cachola
et al., 2020; Narayan et al., 2018). The TL;DR
dataset is constructed by Reddit posts and their cor-
responding TL;DR summaries, while the XSUM
dataset consists of BBC articles and their single-
sentence summaries. Both datasets are widely used
for abstractive summarization tasks.
Models We utilized GPT-J (6B) (Wang and Ko-
matsuzaki, 2021), Mistral-7B (Jiang et al., 2023)
and LLaMA2-7B (Touvron et al., 2023b). For GPT-
J model, we used a checkpoint from Huggingface1
that was already fully fine-tuned on the train dataset.
For LLaMA2-7B and Mistral-7B models, we per-
formed Supervised Fine-Tuning (SFT) on each
training dataset using QLoRA, and then merged
the adapter into the models for further preference
optimization experiments.
Evaluation Metrics We adopt the evaluation pro-
tocol proposed by Chae et al. (2024). They catego-
rized the evaluation into three key divisions: Faith-
fulness ,Relevance (with the source), and Similarity
(with the target). For Faithfulness , we used Align-
Score (Zha et al., 2023) and FactCC (Kryscinski
et al., 2020). To measure Relevance , we employed
BARTScore (Yuan et al., 2021) and BS-FACT.
Lastly, to evaluate Similarity , we used ROUGE-
L.
Implementation Details For the SFT training,
we utilized QLoRA with a batch size of 2 and a
learning rate of 1e-4, training for one epoch in train-
ing split. After training, the SFT-trained QLoRA
was merged with the pre-trained model. For prefer-
ence optimization, we set the DPO hyperparameter
βto 0.5. The learning rate was set to 1e-4 with a
batch size of 4, and training was conducted for one
epoch on the validation split. During summary gen-
eration, the maximum number of generated tokens
was limited to 50. For beam search decoding, we
used beam size of 6. For temperature sampling, we
employed temperatures of 5.0 for GPT-J, and 1.0
for Mistral-7B and LLaMA2-7B.
1CarperAI/openai_summarize_tldr_sftDataset
(Model)MethodResponse
RatioFaithfulness Relevance Similarity
AlignScore (↑) FactCC (↑) BARTScore (↑) BS-FACT (↑) ROUGE-L (↑)
TL;DR
(GPT-J)with ground-truth data
SFT 81.2% (99.4%) 89.21 (83.54) 64.18 (53.48) -1.25 (-1.63) 91.53 (90.30) 26.74 (26.01)
SFT++ 93.8% (99.7%) 87.29 (82.30) 61.50 (57.05) -1.37 (-1.63) 91.06 (90.11) 27.47 (26.53)
with human feedback (preference dataset)
PPO 100.0% (100.0%) 83.10 (75.88) 54.40 (47.52) -1.35 (-1.80) 91.32 (89.78) 23.55 (23.28)
DPO 98.3 (99.8%) 88.12 (82.55) 61.70 (54.09) -1.33 (-1.65) 91.27 (90.22) 27.24 (26.28)
without human feedback
Preferred-FT 66.8% (99.6%) 89.90 (82.04) 76.58 (64.48) -1.39 (-1.73) 91.24 (90.09) 24.38 (24.39)
MPO (Ours) 99.9% (99.9%) 91.61∗(86.82∗)72.10∗(59.39∗)-1.10∗(-1.41∗) 92.20∗(91.20∗)26.10 (26.49)
Table 1: Results of the GPT-J model on the TL;DR dataset. We compared our Model-based Preference Optimiza-
tion (MPO) with two main baselines: supervised fine-tuning andhuman preference . All main results are based on
a beam search decoding strategy, while the results in parentheses are based on a greedy decoding strategy. MPO
showed overall better performance in terms of faithfulness andsource relevance compared to other baselines. The
SFT model is a fine-tuned model on the training split and the SFT++ model is the SFT model further fine-tuned
on the validation split. PPO and DPO are SFT models optimized on human-preference datasets. Preferred-FT is a
model fine-tuned only on the chosen samples of MPO. ∗indicates statistical significance (p-value <0.001) based
on a T-test compared to DPO.
Baselines We compared our method with two
main baselines: supervised fine-tuned models and
human preference optimized models . First, we com-
pared our approach to models fine-tuned using
ground-truth data or summaries generated via de-
terministic decoding. Second, we compared our
method to PPO and DPO models trained on human
preference pairs to demonstrate that the contrast be-
tween beam search decoding and random sampling
is more effective than human-annotated preferences
in terms of faithfulness.
SFT is a fine-tuned model on the train split of
each dataset. SFT++ is a model further trained on a
validation split from the SFT model. Preferred-FT
is fine-tuned to maximize likelihood only on the
chosen samples ( i.e.,ybeam).PPO andDPO are
optimized from SFT models on human preference
dataset provided by Stiennon et al. (2020). For PPO,
we used a Huggingface checkpoint2, already opti-
mized with the provided human preference dataset.
For DPO, we optimized in the same way as MPO
but with the human preference dataset.
4.2 Comparison with Fine-Tuned Models
In Table 1, MPO consistently outperforms fine-
tuned baselines ( i.e., SFT, SFT++, Preferred-FT).
SFT++ and Preferred-FT did not significantly im-
prove over SFT. However, MPO shows a substan-
tial increase of up to 3.28 in AlignScore, 7.92 in
FactCC, 0.22 in BARTScore, and 0.9 in BS-FACT
over SFT. These results suggest that our approach
is more effective at mitigating hallucinations than
2CarperAI/openai_summarize_tldr_ppoDataset Model Method AlignScore (↑)BARTScore (↑)ROUGE-L (↑)TL;DRGPT-JSFT 89.21 (83.54) -1.25 (-1.63) 26.74 (26.01)
SFT++ 87.29 (82.30) -1.37 (-1.63) 27.47 (26.53)
Preferred-FT 89.90 (82.04) -1.39 (-1.73) 24.38 (24.39)
MPO (Ours) 91.61 (86.82) -1.10 (-1.41) 26.10 (26.49)
MistralSFT 87.85 (82.74) -1.48 (-1.81) 25.32 (25.02)
SFT++ 86.66 (82.10) -1.44 (-1.83) 25.27 (24.66)
Preferred-FT 83.96 (79.70) -1.63 (-1.82) 22.57 (22.23)
MPO (Ours) 92.12 (89.39) -1.25 (-1.37) 24.85 (25.01)
LLaMA2SFT 84.92 (77.68) -1.65 (-2.05) 24.31 (23.33)
SFT++ 87.93 (78.03) -1.41 (-2.05) 24.79 (22.89)
Preferred-FT 81.10 (79.58) -1.74 (-1.85) 22.73 (22.47)
MPO (Ours) 85.33 (78.03) -1.64 (-2.03) 24.16 (23.29)XSUMGPT-JSFT 64.01 (52.66) -1.59 (-1.97) 25.13 (24.41)
SFT++ 62.47 (49.91) -1.62 (-2.00) 25.58 (24.66)
Preferred-FT 66.42 (40.71) -1.68 (-2.13) 17.61 (20.21)
MPO (Ours) 65.26 (54.39) -1.58 (-1.95) 25.25 (24.72)
MistralSFT 66.31 (60.00) -1.96 (-1.97) 30.65 (31.16)
SFT++ 64.99 (60.17) -1.74 (-1.96) 30.76 (30.72)
Preferred-FT 63.74 (61.14) -2.53 (-3.31) 21.17 (18.57)
MPO (Ours) 68.58 (64.57) -1.85 (-1.90) 31.11 (31.35)
LLaMA2SFT 65.80 (57.57) -1.80 (-2.06) 30.36 (27.76)
SFT++ 67.20 (57.45) -1.74 (-2.08) 29.23 (27.85)
Preferred-FT 46.96 (39.86) -2.01 (-2.24) 24.36 (23.41)
MPO (Ours) 67.31 (60.48) -1.81 (-2.02) 30.32 (28.36)
Table 2: Comparison of MPO with SFT . MPO demon-
strates generally robust results across various language
models on both the TL;DR and XSUM datasets. The
results are based on a beam search decoding strategy,
while the results in parentheses are based on a greedy
decoding strategy.
simply fine-tuning with either gold summaries or
summaries generated through deterministic decod-
ing.
In Table 2, MPO demonstrates robust and gen-
erally applicable results across various language
models on both the TL;DR and XSUM datasets.
MPO generally exhibits a lower ROUGE-L score
compared to SFT and SFT++. ROUGE-L measures
the lexical similarity between generated summaries
and the target summaries ( i.e., reference or gold)
rather than the source document. However, higher
lexical similarity does not necessarily imply betterGPT-3.5SFT (vs. MPO) DPO (vs. MPO)
Greedy Beam Greedy Beam
# of compared samples 6061 5376 5962 5332
MPO win rate (%) 51.30 59.36 50.27 47.30
Table 3: GPT-3.5 win rates on TL;DR dataset. MPO
generally outperformed the SFT, while the results varied
when compared to DPO depending on the decoding
method. Samples where both models produced identical
outputs were excluded from the win rate calculation.
Method Text
Source TITLE: [19/f] What does this guy [20/m]
actually want from me? POST: . . .became
really good friends, . . .We then somehow
from kissing gently . . .basically said he
likes me but nothing can happen because
I’m not his type... I JUST DON’T KNOW
WHAT THE BOY WANTS FROM ME.
SFT ive been friends with a guy for a while,
then we kissed, then we didn’t, then we
did again, then we didn’t, then we did
again.
DPO I don’t know what the boy wants from me,
and I don’t know what I want from the
boy.
MPO
(Ours)Became really good friends with a guy,
then we kissed, then he said he likes me
but I’m not his type. What does he want
from me?
Table 4: Example summaries of MPO model and hu-
man preference optimized model. Inconsistent words
are highlighted in red. The summary generated by the
MPO model is clearly superior to those by SFT and
DPO (w/ human pref.) models in terms of faithfulness
and source relevance.
summary quality (King et al., 2022; Schluter, 2017;
Ng and Abrecht, 2015). Thus, while ROUGE-L is
a widely used metric, it is not the primary focus of
our evaluation.
4.3 Comparison with Human Preference
Optimized Models
In Table 1 and 3, we compared MPO with human
preference optimized models ( e.g., PPO, DPO).
Based on the automatic metrics in Table 1, MPO
consistently outperforms the human preference op-
timized models. As noted in Hosking et al. (2024),
using human preference datasets can sometimes
underestimate the aspect of faithfulness.
On the other hand, as shown in Table 3, MPO did
not demonstrate dominant performance in the winGroup Selected # Samples
Group A (MPO wins)MPO 35
DPO 15
Group B (DPO wins)MPO 16
DPO 34
Table 5: Results of human evaluation. MPO achieves
an overall win rate of 51% compared to the DPO.
rate evaluation based on GPT-3.5. For details on
the win rate prompts, refer to Appendix A.1. This
discrepancy arises because summary evaluation in-
volves multiple factors (Hosking et al., 2024; Yuan
et al., 2021). While MPO excels in faithfulness and
source relevance, it may fall short in areas such
as fluency (refer to Table 4). Furthermore, human
preference optimized models were trained on sig-
nificantly more data pairs, utilizing multiple pairs
per source text, whereas MPO was optimized using
only one pair per source.
Human Evaluation To assess whether the auto-
matic score ( i.e., AlignScore) aligns with human
preference, we conducted human evaluations on
100 samples from the TL;DR dataset across two
groups. More details are provided in Appendix A.2.
•Group A: AlignScore of DPO ≤0.5 and
AlignScore of MPO > 0.5
•Group B: AlignScore of DPO > 0.5 and Align-
Score of MPO ≤0.5
In Table 5, MPO achieves an overall win rate of
51% when combining results from Groups A and
B. Notably, 70% of MPO’s summaries in Group A
were evaluated superior, while only 32% received
favorable judgments in Group B. These results
suggest that AlignScore aligns with human judg-
ment to some extent, indicating that our evaluation
method can yield results comparable to human eval-
uation.
4.4 Comparison with Decoding Strategies
Table 6 shows the results of applying MPO models
to various decoding strategies using the LLaMA2-
7B model. Despite not being specifically opti-
mized for various decoding strategies ( i.e., Nucleus
(Holtzman et al., 2020), ITI (Li et al., 2023), DoLa
(Chuang et al., 2023)), MPO models are generally
applicable to all decoding strategies and consis-
tently produces enhanced summarization results
compared to the standard SFT model in terms of
faithfulness and relevance.Decoding
StrategyMethod AlignScore (↑)BARTScore (↑)ROUGE-L (↑)
GreedySFT 77.68 -2.05 23.33
MPO 78.03 -2.03 23.29
NucleusSFT 76.25 -2.11 22.82
MPO 76.99 -2.09 22.79
ITISFT 76.95 -1.88 23.15
MPO 77.15 -1.87 23.23
DoLaSFT 82.47 -1.76 24.61
MPO 82.57 -1.75 24.55
BeamSFT 84.92 -1.65 24.31
MPO 85.33 -1.64 24.16
Table 6: Results of applying various decoding strate-
gies. MPO aligns well with different decoding strate-
gies. When combined with faithfulness-aware decoding
strategies ( i.e., ITI, DoLA), it can lead to further im-
provements. The results are from using the LLaMA2-7B
on the TL;DR dataset.
Dataset Model Method AlignScore (↑)BARTScore (↑)ROUGE-L (↑)TL;DRGPT-JBeam search 89.19 -1.24 27.00
Sampling (temp1) 57.68 -2.94 19.34
Sampling (temp5) 24.66 -6.89 8.73
MistralBeam search 87.47 -1.46 25.18
Sampling (temp1) 58.70 -3.14 18.43
Sampling (temp5) 22.96 -7.14 8.35
LLaMA2Beam search 84.72 -1.65 24.41
Sampling (temp1) 64.23 -2.71 19.69
Sampling (temp5) 23.27 -7.12 8.51XSUMGPT-JBeam search 64.55 -1.59 25.34
Sampling (temp1) 28.12 -2.99 17.77
Sampling (temp5) 14.33 -6.91 6.95
MistralBeam search 66.76 -1.96 30.57
Sampling (temp1) 43.48 -2.81 22.81
Sampling (temp5) 20.07 -7.41 6.82
LLaMA2Beam search 66.57 -1.81 30.48
Sampling (temp1) 47.65 -2.49 23.76
Sampling (temp5) 17.65 -7.41 7.39
Table 7: Evaluation reults of chosen and rejected sam-
ples. Summaries generated with deterministic decoding
(e.g., beam search) outperformed those from stochastic
decoding ( e.g., temperature-scaled sampling) across all
metrics.
5 Analysis
5.1 Evaluation of Chosen and Rejected
Samples
Our key assumption is that deterministic genera-
tion yields summaries more relevant to the source
document than stochastic generation for summa-
rization tasks. In Table 7, we compared the deter-
ministic and stochastic generated summaries used
in MPO training. The chosen samples consistently
outperformed the rejected samples across all met-
rics. Our results align with the results of recent
studies (Holtzman et al., 2020; Wan et al., 2023;
Lee et al., 2022).
However, these findings do not necessarily imply
that deterministic generation is always less halluci-
nated than stochastic generation. Thus, we adjusted
the temperature in stochastic sampling to encour-
age the generation of tokens that are unrelated toCombination AlignScore (↑)BARTScore (↑)ROUGE-L (↑)
SFT 89.21 -1.25 26.74
(yw
beam ,yl
greedy ) 51.96 -4.63 0.87
(yw
temp5 ,yl
beam ) 87.59 -1.36 27.24
(yw
greedy ,yl
temp5 ) 90.57 -1.20 26.87
(yw
beam ,yl
temp5 ) 91.61 -1.10 26.10
Table 8: MPO with different combinations of prefer-
ence pairs. The result show that using a deterministic de-
coding strategy pair significantly inhibit summarization
ability. For pairs combining deterministic and stochas-
tic decoding, setting beam search as the chosen and
temperature-based sampling as the rejected maximizes
the language model’s summarization performance. The
results are from using the GPT-J on the TL;DR dataset.
Pairs ROUGE-1 (↑)ROUGE-2 (↑)ROUGE-L (↑)
yw
beamvs.yl
greedy 47.38 35.06 43.24
yw
greedy vs.yl
temp5 12.93 0.49 9.00
yw
beamvs.yl
temp5 10.56 0.41 7.40
Table 9: ROUGE score comparison. Deterministic
decoding generated summaries exhibit high similarity,
whereas there is low similarity between summaries gen-
erated by deterministic decoding and those generated
by stochastic decoding.
the source documents.
5.2 Other Combinations for Preference Pairs
Deterministic Generation as Rejected Samples
To assess whether improving the quality of rejected
responses would enhance the model’s summariza-
tion performance, we employed greedy decoding
for the rejected responses. However, this approach
resulted in a notable decline in summarization per-
formance (see row 3 in Table 8). Examples of the
generated samples are provided in Appendix A.3.
One reason for degradation is that the chosen
and rejected samples are too similar, causing con-
fusion for the model. In Table 9, we measured the
similarity between the summaries produced by the
two decoding methods. The summaries generated
by beam search decoding and greedy decoding
achieved very high ROUGE scores. This suggests
that using overly similar summaries as chosen and
rejected responses in preference optimization can
have adverse effects (Pal et al., 2024).
Stochastic Generation as Chosen Samples In
table 8, we instead used stochastic decoding for the
chosen samples. While this approach did not result
in severe degeneration, it reduced faithfulness com-
pared to the original SFT model. This suggests that
if the chosen samples have lower source alignment
than the rejected ones, preference optimization canFigure 3: Analysis for each training iteration. The
average abstractiveness of summaries generated for the
TL;DR test set across training iterations, measured by
the MINT score, with dotted lines indicating variance.
The average extractiveness is measured by extractive
fragment coverage.
degrade the model’s existing summarization perfor-
mance in terms of faithfulness and relevance.
5.3 Faithfulness-Abstractiveness Tradeoff
from Iterative Training
Recent studies by Pang et al. (2024) and Chen et al.
(2024) have shown that iteratively constructing the
preference dataset using the trained model from the
previous iteration improves dataset quality. Build-
ing on these works, our approach extends MPO to
iterative MPO. In this experiment, we used sum-
maries generated via beam search from the pre-
vious iteration as the chosen samples, while sum-
maries generated through random sampling from
the initial SFT model were used as rejected samples.
To adapt to the continuous improvements in model
performance, we dynamically adjusted the task dif-
ficulty by progressively lowering the temperature
settings—5.0, 3.0, and 1.0—for each iteration.
We observed a notable trend where the model in-
creasingly produced more extractive summaries,
often directly incorporating sentences from the
source documents. This trend can be attributed
to the slightly extractive nature of the summaries
generated by the SFT model using beam search
decoding, which were used as the chosen sam-
ples (Ladhak et al., 2022). In other words, iterative
MPO training may suppress the model’s creativity.
Consequently, as shown in Figure 3, the model’s
faithfulness improved with increased extractiveness
over successive iterations3.
3To quantitatively assess the abstractiveness and extrac-
tiveness, we utilized the MINT (Dreyer et al., 2023) and ex-
tractive fragment coverage (Grusky et al., 2018), respectively.Method AlignScore ( ↑) BARTScore( ↑) ROUGE-L( ↑)
SFT 61.86 -1.80 36.42
MPO (yw
beam ,yl
temp) 66.42 -1.80 35.78
Lookahead (Wan et al., 2023) 67.78 -1.76 34.3
MPO* (yw
Lookahead ,yl
temp) 68.85 -1.73 34.93
Table 10: Results of experiments for the encoder-
decoder model on XSUM dataset. MPO outperforms
SFT in terms of factuality. The summarization perfor-
mance of MPO can be further improved by using en-
hanced decoding strategy ( e.g., Lookahead) instead of
beam search decoding
Qualitative study In Appendix A.3, Table 13
provides an example of summaries generated by
the SFT model and by the MPO model at different
iterations in response to a given prompt. As the
iterations progress, the summaries tend to become
more extractive for the document. Notably, the sum-
mary generated in the third iteration is quite similar
to the title.
5.4 Encoder-Decoder Model
To verify the generalizability of our method across
different model architectures, we evaluated it using
an encoder-decoder model, such as BART (Lewis
et al., 2019). As shown in Table 10, MPO out-
performs SFT in terms of AlignScore, improv-
ing from 61.86 to 66.42. These results demon-
strate that our approach can be applied to encoder-
decoder models as well. Additionally, we com-
pared MPO with another decoding strategy base-
line, Faithfulness-aware Lookahead (Wan et al.,
2023), which has shown effectiveness with encoder-
decoder models. Interestingly, by using summaries
from Faithfulness-aware Lookahead as the chosen
samples instead of the beam search summaries ( i.e.,
MPO*), MPO* increased the AlignScore by 2.43
over MPO. This suggests that incorporating more
effective decoding strategies within MPO can fur-
ther enhance summarization performance.
6 Related Work
In the realm of auto-regressive language models,
there are two primary approaches aimed to enhance
the model’s summarization capabilities: adjusting
the learning algorithm or refining the decoding
strategy (Welleck et al., 2020b). The former in-
volves updating the model’s parameters through
a learning objective, while the latter entails im-
proving the decoding algorithm during generation
while maintaining the existing pre-trained param-
eters frozen. In this paper, we will review two ap-proaches in abstractive summarization aimed at
alleviating hallucination.
Decoding Strategies Several methods have been
proposed to rectify hallucinations during gen-
eration. Inference-time intervention (ITI) shifts
activations along truth-correlated directions (Li
et al., 2023), repeating the same intervention auto-
regressively until the entire answer is generated.
Decoding by contrasting layers (DoLa) uses an
early-exit strategy by contrasting the differences in
logits obtained from projecting the later layers ver-
sus earlier layers (Chuang et al., 2023). Lastly, Wan
et al. (2023) extend the idea of lookahead (Lu et al.,
2022) to improve faithfulness in abstractive summa-
rization, showing that the deterministic decoding
strategy outperforms nucleus sampling (Holtzman
et al., 2020) in terms of faithfulness. However, it
is important to note that decoding strategies do not
change the underlying model.
Learning Algorithms To mitigate hallucinations,
naively fine-tuning with faithfulness-aware ob-
jectives might seem straightforward. FactPegasus
(Wan and Bansal, 2022) employs a tailored pre-
training setup with contrastive learning to generate
more faithful summaries. It modifies sentence selec-
tion by combining ROUGE and FactCC (Kryscin-
ski et al., 2020). However, this method risks over-
fitting to the metrics used, potentially degrading
overall summarization performance (Chae et al.,
2024).
As an alternative, RL-based objectives can be
utilized to enhance faithfulness (Böhm et al., 2019;
Roit et al., 2023; Paulus et al., 2018). RL provides
a natural path for optimizing non-differentiable ob-
jectives in LM-based generation. Ramamurthy et al.
(2023) show that RL techniques generally align
language models to human preferences better than
supervised methods. On the other hand, Direct Pref-
erence Optimization (DPO)(Rafailov et al., 2023)
simplifies the process by eliminating the need for
an explicit reward function of RL-based algorithms.
Leveraging DPO, Tian et al. (2024) have suggested
optimizing language models for factuality in long-
form text generation using FactScore (Min et al.,
2023).
In this paper, we train the underlying model to
provide summaries faithful to source documents,
based on findings from research on decoding strate-
gies. Our approach does not require external met-
rics or human feedback during the optimization pro-
cess. Furthermore, the model trained on our frame-work is versatile enough to integrate enhanced de-
coding techniques, thereby more effectively reduc-
ing hallucinations.
7 Conclusion
This study introduces Model-based Preference Op-
timization (MPO), a novel approach to improve the
faithfulness and quality of abstractive summaries
generated by Large Language Models (LLMs). Un-
like traditional methods that rely heavily on costly
human feedback, MPO leverages the model’s in-
herent summarization capabilities to create a pref-
erence dataset using different decoding strategies.
Our extensive experiments demonstrate that MPO
significantly enhances the summarization perfor-
mance, providing an efficient and scalable solution
to address the challenges of hallucination in LLM-
generated summaries.
Limitation
In our experiments, we employed QLoRA to main-
tain the performance of the SFT model. However,
this method may have limited further performance
improvements. The absence of comparative experi-
ments leaves uncertainty about actual effectiveness
of QLoRA. Additionally, due to constraints in our
experimental environment, we limited experiments
on 7B models, which raises concerns about the
scalability of our approach.
During iterative training, we observed a trend
where the model increasingly adopted an extrac-
tive approach, often replicating sentences from the
input documents directly in the summaries. This
trend poses a challenge to our goal of producing
more faithful abstractive summaries.
Ethical Concerns
We propose MPO, which leverages the outputs of
a language model as a dataset for preference opti-
mization, relying extensively on the outputs from
the SFT model. Previous researches (Sheng et al.
(2019), Nangia et al. (2020)) has shown that self-
supervised language models, which are trained on
unlabeled web-scale datasets, can unintentionally
learn and perpetuate social and ethical biases, in-
cluding racism and sexism. If such biases are in-
herent within the data, our proposed self-feedback
framework may unintentionally reinforce them. We
used the TL;DR dataset for training, derived from
Reddit posts, which may contain unmoderated andbiased expressions. The presence of offensive con-
tent in this dataset risks influencing the model’s
outputs, potentially perpetuating these biases in fur-
ther training within MPO. Moreover, as MPO pro-
gresses and the model increasingly favors extrac-
tive summarization, it may struggle to effectively
paraphrase and filter out offensive expressions.
Acknowledgements
This research was supported by the National Re-
search Foundation of Korea (NRF) grant (No.
RS-2023-00222663, RS-2024-00345809, RS-2024-
00333484, RS-2024-00414981) and the Institute of
Information & Communications Technology Plan-
ning & Evaluation (IITP) grant (under the Lead-
ing Generative AI Human Resources Development,
IITP-2024-RS-2024-00397085), both funded by
the Korea government (MSIT).
References
Florian Böhm, Yang Gao, Christian M. Meyer, Ori
Shapira, Ido Dagan, and Iryna Gurevych. 2019. Bet-
ter rewards yield better summaries: Learning to sum-
marise without references. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 3110–3120, Hong Kong,
China. Association for Computational Linguistics.
Ralph Allan Bradley and Milton E Terry. 1952. Rank
analysis of incomplete block designs: I. the method of
paired comparisons. Biometrika , 39(3/4):324–345.
Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel
Weld. 2020. TLDR: Extreme summarization of sci-
entific documents. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
4766–4777, Online. Association for Computational
Linguistics.
Kyubyung Chae, Jaepill Choi, Yohan Jo, and Taesup
Kim. 2024. Mitigating hallucination in abstractive
summarization with domain-conditional mutual in-
formation. In Findings of the Association for Compu-
tational Linguistics: NAACL 2024 , pages 1809–1820,
Mexico City, Mexico. Association for Computational
Linguistics.
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji,
and Quanquan Gu. 2024. Self-play fine-tuning con-
verts weak language models to strong language mod-
els.arXiv preprint arXiv:2401.01335 .
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon
Kim, James Glass, and Pengcheng He. 2023. Dola:
Decoding by contrasting layers improves factuality
in large language models.Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms. arXiv preprint arXiv:2305.14314 .
Markus Dreyer, Mengwen Liu, Feng Nan, Sandeep
Atluri, and Sujith Ravi. 2023. Evaluating the tradeoff
between abstractiveness and factuality in abstractive
summarization. In Findings of the Association for
Computational Linguistics: EACL 2023 , pages 2089–
2105.
Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
Newsroom: A dataset of 1.3 million summaries with
diverse extractive strategies. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers) , pages 708–719.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text
degeneration.
Tom Hosking, Phil Blunsom, and Max Bartolo. 2024.
Human feedback is not gold standard. In The Twelfth
International Conference on Learning Representa-
tions .
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, Lélio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023. Mistral 7b.
Daniel King, Zejiang Shen, Nishant Subramani,
Daniel S. Weld, Iz Beltagy, and Doug Downey. 2022.
Don’t say what you don’t know: Improving the con-
sistency of abstractive summarization by constraining
beam search. In Proceedings of the 2nd Workshop on
Natural Language Generation, Evaluation, and Met-
rics (GEM) , pages 555–571, Abu Dhabi, United Arab
Emirates (Hybrid). Association for Computational
Linguistics.
Wojciech Kryscinski, Bryan McCann, Caiming Xiong,
and Richard Socher. 2020. Evaluating the factual
consistency of abstractive text summarization. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 9332–9346, Online. Association for Computa-
tional Linguistics.
Faisal Ladhak, Esin Durmus, He He, Claire Cardie, and
Kathleen McKeown. 2022. Faithful or extractive?
on mitigating the faithfulness-abstractiveness trade-
off in abstractive summarization. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 1410–1421, Dublin, Ireland. Association for
Computational Linguistics.
Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pas-
cale N Fung, Mohammad Shoeybi, and Bryan Catan-
zaro. 2022. Factuality enhanced language models foropen-ended text generation. In Advances in Neural
Information Processing Systems , volume 35, pages
34586–34599. Curran Associates, Inc.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter
Pfister, and Martin Wattenberg. 2023. Inference-time
intervention: Eliciting truthful answers from a lan-
guage model.
Ximing Lu, Sean Welleck, Peter West, Liwei Jiang,
Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lian-
hui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith,
and Yejin Choi. 2022. NeuroLogic a*esque decoding:
Constrained text generation with lookahead heuris-
tics. In Proceedings of the 2022 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 780–799, Seattle, United States. Associa-
tion for Computational Linguistics.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906–1919, On-
line. Association for Computational Linguistics.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2023. Factscore:
Fine-grained atomic evaluation of factual precision
in long form text generation.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel Bowman. 2020. Crows-pairs: A challenge
dataset for measuring social biases in masked lan-
guage models. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP) , pages 1953–1967.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1797–1807, Brussels, Bel-
gium. Association for Computational Linguistics.
Jun-Ping Ng and Viktoria Abrecht. 2015. Better sum-
marization evaluation with word embeddings for
ROUGE. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1925–1930, Lisbon, Portugal. Association
for Computational Linguistics.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems , volume 35, pages 27730–27744.
Curran Associates, Inc.
Arka Pal, Deep Karkhanis, Samuel Dooley, Manley
Roberts, Siddartha Naidu, and Colin White. 2024.
Smaug: Fixing failure modes of preference optimisa-
tion with dpo-positive.
Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho,
He He, Sainbayar Sukhbaatar, and Jason Weston.
2024. Iterative reasoning preference optimization.
Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-
reward reinforced summarization with saliency and
entailment. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers) , pages 646–653,
New Orleans, Louisiana. Association for Computa-
tional Linguistics.
Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
marization. In International Conference on Learning
Representations .
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D. Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model.
Rajkumar Ramamurthy, Prithviraj Ammanabrolu,
Kianté Brantley, Jack Hessel, Rafet Sifa, Christian
Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.
2023. Is reinforcement learning (not) for natural lan-
guage processing: Benchmarks, baselines, and build-
ing blocks for natural language policy optimization.
InThe Eleventh International Conference on Learn-
ing Representations .
Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Ge-
offrey Cideron, Robert Dadashi, Matthieu Geist, Ser-
tan Girgin, Leonard Hussenot, Orgad Keller, Nikola
Momchev, Sabela Ramos Garea, Piotr Stanczyk,
Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan
Hassidim, Olivier Pietquin, and Idan Szpektor. 2023.
Factually consistent summarization via reinforce-
ment learning with textual entailment feedback. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 6252–6272, Toronto, Canada.
Association for Computational Linguistics.
Natalie Schluter. 2017. The limits of automatic sum-
marisation according to ROUGE. In Proceedings
of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Vol-
ume 2, Short Papers , pages 41–45, Valencia, Spain.
Association for Computational Linguistics.
Emily Sheng, Kai-Wei Chang, Prem Natarajan, and
Nanyun Peng. 2019. The woman worked as a babysit-
ter: On biases in language generation. In Proceedingsof the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 3407–3412.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 3008–3021. Curran Associates, Inc.
Marilyn Strathern. 1997. ‘improving ratings’: audit
in the british university system. European Review ,
5(3):305–321.
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-
pher D Manning, and Chelsea Finn. 2024. Fine-
tuning language models for factuality. In The Twelfth
International Conference on Learning Representa-
tions .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and fine-
tuned chat models.
Leandro von Werra, Younes Belkada, Lewis Tun-
stall, Edward Beeching, Tristan Thrush, Nathan
Lambert, and Shengyi Huang. 2020. Trl: Trans-
former reinforcement learning. https://github.
com/huggingface/trl .
David Wan and Mohit Bansal. 2022. FactPEGASUS:
Factuality-aware pre-training and fine-tuning for ab-
stractive summarization. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1010–1028, Seattle,United States. Association for Computational Lin-
guistics.
David Wan, Mengwen Liu, Kathleen McKeown, Dreyer
Markus, and Mohit Bansal. 2023. Faithfulness-aware
decoding strategies for abstractive summarization. In
Proceedings of the 17th Conference of the European
Chapter of the Association for Computational Lin-
guistics .
Ben Wang and Aran Komatsuzaki. 2021. GPT-J-
6B: A 6 Billion Parameter Autoregressive Lan-
guage Model. https://github.com/kingoflolz/
mesh-transformer-jax .
Jiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi
Guo, Andrew Estornell, and Yang Liu. 2024. Mea-
suring and reducing llm hallucination without gold-
standard answers.
Sean Welleck, Ilia Kulikov, Jaedeok Kim,
Richard Yuanzhe Pang, and Kyunghyun Cho.
2020a. Consistency of a recurrent language model
with respect to incomplete decoding. In Proceedings
of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages
5553–5568, Online. Association for Computational
Linguistics.
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2020b.
Neural text generation with unlikelihood training. In
International Conference on Learning Representa-
tions .
Yilin Yang, Liang Huang, and Mingbo Ma. 2018. Break-
ing the beam search curse: A study of (re-)scoring
methods and stopping criteria for neural machine
translation. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, pages 3054–3059, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
Bartscore: Evaluating generated text as text genera-
tion. In Advances in Neural Information Processing
Systems , volume 34, pages 27263–27277. Curran As-
sociates, Inc.
Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu.
2023. AlignScore: Evaluating factual consistency
with a unified alignment function. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 11328–11348, Toronto, Canada. Association
for Computational Linguistics.
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2020. Fine-tuning lan-
guage models from human preferences.A Appendix
A.1 GPT-3.5 Judgment Prompts
We used GPT-3.5-turbo to evaluate win rates using
prompts proposed in Rafailov et al. (2023). The or-
der of summaries or responses is randomly chosen
for each evaluation. The prompt examples we used
can be seen in Figure 4.
Figure 4: Prompt of GPT-3.5 win rate.
A.2 Human Evaluation Details
We sampled 100 instances from the TL;DR dataset
for human evaluation. Instead of randomly sam-
pling instances from the dataset, we selected in-
stances to effectively assess the reliability of Align-
Score (Zha et al., 2023) in comparison to human
evaluation. Our goal was to determine if the auto-
matic score ( i.e., AlignScore) aligns with human
judgment. To achieve this, we divided the dataset
into four groups:
•Group A: AlignScore of DPO ≤0.5 and
AlignScore of MPO > 0.5
•Group B: AlignScore of DPO > 0.5 and Align-
Score of MPO ≤0.5
•Group C: AlignScore of DPO ≤0.5 and
AlignScore of MPO ≤0.5
•Group D: AlignScore of DPO > 0.5 and Align-
Score of MPO > 0.5
To ensure fairness and align with our primary
goal, we evenly mixed Group A (MPO wins) and
Group B (DPO wins) by sampling 50 instances
from each group. We excluded instances from
Group C and Group D because the differences
between instances in those groups were minimal,
making it challenging for human annotators to as-
sess preferences based on just a few words.
We asked the annotators three questions. First,
they were asked to choose the summary they con-
sidered better between the two provided summaries(Q1). Second, they were asked to select the sum-
mary with issues based on consistency with the
source text (Q2). Finally, they were instructed to
mark the parts of the selected summary they found
problematic (Q3). For Q2, they could choose one
of four responses: Summary A, Summary B, Nei-
ther, or Both. Figure 5 illustrates the layout format
used in the survey.
Figure 5: Layout of human evaluation.
Selected # Samples
MPO (ours) 29
DPO 36
Neither 10
Both 25
Table 11: Results of human evaluation on consistency.
Based on the responses to Q2 and Q3, we con-
firmed that our method produced summaries that
were more faithful to the source text compared to
DPO (see Table 5). Interestingly, although both
methods had similar win rates, DPO performed sig-
nificantly worse in terms of consistency (see Table
11).
Participants We had 10 annotators, each of
whom reviewed 10 samples, resulting in a total
evaluation of 100 samples. All of raters were stu-
dents of Seoul National University with a STEM
focus. We appreciate the contributions of the fol-
lowing volunteers. The names are listed in random
order:
Hyunbin Jin, Gihoon Kim, Minsoo Jo, Se-
unghyun Bae, Jewon Yeom, Seoyun Yang, Ijun
Jang, Seul Lee, Junoh Park, Jinmyeong Choi
A.3 Example Cases
Table 12 shows examples of summaries with dif-
ferent combinations of preference pairs. Table 13shows examples summaries from iterative prefer-
ence optimization.
A.4 License Information of The Assets Used
in This Work
Datasets We report known license information
of the assets used in this work. The following
datasets used in this paper are under the MIT Li-
cense: XSUM (Narayan et al., 2018). The following
datasets used in this paper are under the CC BY
4.0 License: TL;DR (Cachola et al., 2020).
Models We report known license information of
the assets used in this work. The following datasets
used in this paper are under the Apache 2.0 License:
GPT-J (Wang and Komatsuzaki, 2021), Mistral-
7B (Jiang et al., 2023), BART (Lewis et al., 2019).
The following datasets used in this paper are under
the Llama2 License: LLaMA2-7B (Touvron et al.,
2023b)
Source code We use the implementation of exist-
ing baseline methods for reporting their results in
this paper. The source code utilized in this paper is
subject to the MIT License: MINT (Dreyer et al.,
2023), ITI (Li et al., 2023), AlignScore (Zha et al.,
2023), DoLa (Chuang et al., 2023), DCPMI (Chae
et al., 2024) The following source code utilized
in this paper is subject to the BSD 3-Clause
License: FactCC (Kryscinski et al., 2020) The
following source code utilized in this paper is
subject to the CC-BY-NC-4.0 License: Looka-
head (Wan et al., 2023) The following source code
utilized in this paper is subject to the Apache
2.0 License: BARTScore (Yuan et al., 2021),
trl/examples/research_projects/stack_llama_2 (von
Werra et al., 2020)
A.5 Statistics for Data
We utilized two abstractive summarization datasets,
TL;DR and XSUM. The TL;DR dataset is con-
structed by Reddit posts and their corresponding
summaries, with 117k samples in the train split,
6.45k in the validation split, and 6.55k in the test
split. The XSUM dataset consists of BBC articles
and their corresponding summaries, totaling 204k
samples in the train split, 11.3k in the validation
split, and 11.3k in the test split. Both datasets are
in English.
The train splits from each dataset were used dur-
ing the SFT phase, the validation splits during the
preference optimization phase, and the test splits
during the evaluation phase.A.6 Analysis on Error Bars
All experiments were evaluated in single run, fixing
the seed at 42. Additionally, all summary genera-
tions were conducted in the order of the provided
test dataset.
A.7 Reproducibility
We conducted our experiments using computing
clusters equipped with NVIDIA RTX 6000 (GPU
memory: 48GB) and NVIDIA RTX 3090 GPUs
(GPU memory: 24 GB), allocating a single GPU
for each experiment.
Based on NVIDIA RTX 6000, model preference
optimization typically required an average of 1 hour
and 30 minutes. When generating summaries, us-
ing GPT-J (6B) with beam search decoding took
approximately 20 hours, and with greedy decoding,
about 5 hours and 30 minutes. Using Mistral-7B
and LLaMA-7B models with beam search decod-
ing took around 5 hours, while with greedy decod-
ing, it took about 1 hour and 30 minutes.
A.8 Parameters for Package
For evaluating summaries, we loaded ROUGE and
BERTScore from the evaluate package (version:
0.4.1).Method Text
Source SUBREDDIT: r/relationships TITLE: Is she [21] playing hard to get or uninterested in
me?[22/M]POST: Hey guys first post here. So I’ll try to make this quick, I’ve been out of
the dating scene for a few years now and need advice with a girl i currently like. Her and I met
in class and have been talking for not too long a month or so. We have tons in common I have
gotten her number and we text every now and then (more on that later.) But I have really I’ve
made the most progress in the past week. So everything explained further takes place in a span
of a week. I’ve hung out with her a few times. The times we have hung out have been good we
seem to hit it off. She’s kinda touchy and takes lots of pictures of us (bff on sc if that means
anything.) She said things like I’m special for getting to see her act in her goofy way. She even
made and brought me a sandwich for when we were studying together. But ever since then she
seems less interested in me and we do not text as often. Which is weird cuz that was to me at
least the time we hit it off the most. Before We had been texting all day but now barely at all and
remember this is all in a span of less than a week. Most recently we were supposed to hangout
but she said she forgot (which is a big red flag I know) but we did reschedule for another specific
day. So is she uninterested? Playing hard to get? Or other? TL;DR:
MPO (yw
beam ,yl
greedy )\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd
\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd
MPO (yw
beam ,yl
temp5 ) Girl I like seems less interested in me after we hit it off for a week. Is she uninterested? Playing
hard to get? Or other?
MPO (yw
temp5 ,yl
beam ) ive been talking to a girl for less than a month and we seem to hit it off but now she seems less
interested.
Table 12: Example of summaries with different combinations of preference pairs. In the case of (yw
beam,yl
greedy),
the quality of the generated summaries significantly deteriorated. When there is a clear distinction between preferred
data and rejected data, as observed in the two models below, the generated summaries remain similar even if the
preferred and rejected data are swapped.
Method Text
Source SUBREDDIT: r/AskReddit TITLE: Any doctors or med students lurking? What is the process
to become a doctor like? POST: I would like to become an anesthesiologist one day. I have
thought about this a lot and would like to hear what it’s like from some insiders. I am really not
the type who can sit behind a desk all day. I’m prepared to sit at desks for studying, but I can’t
do it for 30+ years. I don’t want to wear a suit and tie. I like to be up running around. Nothing
else really interests me aside from extreme sports. I want to put my work in now and come out
making a high salary. I don’t want to climb the rungs of a corporate ladder. I’ve had friends with
dads in construction, working 80+ hours a week. My dad as an engineer and programmer worked
80+ hours a week for years and his salary still isn’t as high as an anesthesiologists (it’s great for
an engineer). I really just want to become an anesthesiologist.TL;DR:
SFT I want to become an anesthesiologist. I’m not sure if I’m going about it the right way.
MPO (iteration 1) I want to become an anesthesiologist. I want to know what it’s like from an insider. What is the
process like?
MPO (iteration 2) I want to become an anesthesiologist. I want to know what it’s like from an insider. What is the
process to become a doctor like?
MPO (iteration 3) Any doctors or med students lurking? What is the process to become a doctor like? I want
to become an anesthesiologist one day.
Table 13: Example summaries for iterative preference optimization. As the iterations progress, an increase in the
extractiveness of the summaries is observed, with summaries increasingly incorporating sentences directly from the
source. Sentences in bold indicate exact matches to the source text.