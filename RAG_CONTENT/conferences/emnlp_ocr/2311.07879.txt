Toxicity Detection is NOT all you Need:
Measuring the Gaps to Supporting Volunteer Content Moderators through
a User-Centric Method
Yang Trista Cao1Lovely-Frances Domingo1Sarah Ann Gilbert3
Michelle L. Mazurek1Katie Shilton1Hal Daumé III1,2
1University of Maryland, College Park2Microsoft Research3Cornell University
{ycao95,lovely,mmazurek,kshilton,hal3}@umd.edu
sag284@cornell.edu
Abstract
Extensive efforts in automated approaches for
content moderation have been focused on devel-
oping models to identify toxic, offensive, and
hateful content with the aim of lightening the
load for moderators. Yet, it remains uncertain
whether improvements on those tasks have truly
addressed moderators’ needs in accomplishing
their work. In this paper, we surface gaps be-
tween past research efforts that have aimed to
provide automation for aspects of content mod-
eration and the needs of volunteer content mod-
erators, regarding identifying violations of var-
ious moderation rules. To do so, we conduct
amodel review on Hugging Face to reveal the
availability of models to cover various modera-
tion rules and guidelines from three exemplar
forums. We further put state-of-the-art LLMs
to the test, evaluating how well these models
perform in flagging violations of platform rules
from one particular forum. Finally, we conduct
a user survey study with volunteer moderators
to gain insight into their perspectives on useful
moderation models. Overall, we observe a non-
trivial gap, as missing developed models and
LLMs exhibit moderate to low performance
on a significant portion of the rules. Modera-
tors’ reports provide guides for future work on
developing moderation assistant models.
1 Introduction
Content moderation guards online forums against
hostility and extremism while maintaining commu-
nity norms, ensuring the forums remain healthy
and open to all participants. While many platforms
pay for this service, others, such as Reddit, Discord,
Facebook, and Twitch, use a hybrid model, relying
on the labor of volunteers. Yet, behind the screen,
being a volunteer content moderator is time- and
emotionally-draining work. Moderators frequently
deal with abusive language, sensitive posts, and
unpleasant interactions with users (Seering et al.,
2019; Gilbert, 2020; Dosono and Semaan, 2019;
Wohn, 2019; Jiang et al., 2019), often doing thiswork in addition to full-time jobs. To support these
volunteers, efforts have been made to develop mod-
els, such as Google Perspective API1and OpenAI
undesired content detection (Markov et al., 2023),
that can automatically identify content for removal
in order to alleviate moderators’ workload.
Although these systems have shown great suc-
cess in detecting “undesired” content, they primar-
ily focus on toxic content. Yet, content moderation
encompasses more than toxicity detection,2partic-
ularly in platforms that leverage volunteer modera-
tion within smaller communities hosted by the site.
For example, Reddit is a platform consisting of var-
ious communities, known as “subreddits,” focused
on a diverse set of topics, and each subreddit has
its own moderation rules. Fiesler et al. (2018) con-
ducted a study to explore various subreddit rules,
consolidating similar ones, and arrived at 25dis-
tinct rule types. Hence, in order to support moder-
ators in detecting potential rule-violating content,
content moderation tools need to support much
more than just toxicity detection.
In this paper, we aim to assess to what extent
current natural language processing (NLP) models
can serve the wide spectrum of moderation rules
so that they can be helpful in assisting moderators.
First, to understand the functions previous auto-
mated content moderation models have focused on,
we conduct a model review on Hugging Face (HF)
with rules from three subreddits as exemplars. This
allows us to gauge the progress of past model devel-
opments in covering various moderation rules. We
use model review as opposed to the more common
literature review to gain a technical understanding
of the existing models’ functions. In addition to
examining models that are built to handle specific
tasks, we also assess so-called “general-purposed”
large language models’ (LLMs’) capability in cov-
1https://perspectiveapi.com/
2We use “toxicity detection” as an umbrella term for hate
speech detection, incivility detection, etc.arXiv:2311.07879v4  [cs.CL]  13 Nov 2024ering various moderation rules. We evaluate GPT-
4 and Llama-2 on a new evaluation dataset that
we collected, consisting of moderation decisions
from r/AskHistorians and covering a wide range
of rules. Finally, using the models’ performance
on this new dataset as an empirical grounding, we
conducted a survey study with active moderators
from r/AskHistorians (N= 11 ). Through this sur-
vey study, we aim to gain insights into model users’
perspectives on the performance requirements for
useful moderation models on different rules.
We find a substantial gap in both existing NLP
models and LLMs’ performance when to cover
the diverse set of moderation rules that subreddits
employ. Our analysis shows the majority of mod-
eration rules from the three subreddits ( ∼80%)
are unrelated to toxicity detection, with nearly 70%
lacking an huggingface model designed for their
resolution. While one might hope that general-
purpose LLMs could fill this gap, our experiments
with GPT-4 and Llama-2 show that LLMs fall short
in their ability to detect violations of many rules.
Specifically, both GPT-4 and Llama-2 exhibited
moderate to low precision and/or recall ( <70%)
for half of the rules from r/AskHistorians . Findings
from our survey study also indicate that neither
LLM has good enough performance to be useful
for6of23r/AskHistorians rules ( 26%), including
rules such as Scope ,Digression , and Sources3.
Meanwhile, our survey study also shows that mod-
erators are excited about an assistant model—they
are okay with even imperfect tools if they are well-
informed about their limitations. For different
kinds of rules, moderators have complex needs
in terms of model precision and recall. For in-
stance, they need high recall for complex rules (e.g.
Plagiarism andDigression ) and high precision
for simple rules (e.g. Current Event andJokes
and Humour ). Our study highlights the necessity of,
and provides a guide for, future content moderation
work to expand its focus beyond toxicity detection,
encompassing a broader spectrum of moderation
rules to meet the needs of moderators seeking au-
tomation assistance.
2 Background and Related Work
V olunteer moderators have been a staple of on-
line content regulation from the earliest days of
the internet, tackling issues such as spam (Seering,
3For explanation of the rules, please refer to Table 2 in
appendix.2020), trolling (Binns, 2012), and abuse (Dibbell,
1994). Currently, volunteers contribute moderation
efforts on nearly all major platforms. For example,
volunteers are responsible for moderation within
Facebook’s Groups, Discord’s servers, Twitch’s
streams, Reddit’s subreddits, and through X’s (for-
mally known as Twitter) Community Notes. These
efforts bring significant value to platforms, with
one study estimating that, at the baseline, volunteer
moderators on Reddit collectively work 466 hours
a day, amounting to a value of about 3.4 million
USD per year (Li et al., 2022). With the work-
load and continuous exposure to online harassment
(Gilbert, 2020; Wohn, 2019; Dosono and Semaan,
2019) and toxic content, such as hateful language
and violent or upsetting imagery, this moderation
work can easily result in burnout.
A common way to mitigate burnout is through
the development of tools that support moderation
labor, particularly through automation. For ex-
ample, prior study has found that automation can
help at scale, supporting moderators of large com-
munities with high levels of activity (Kiene and
Hill, 2020; Seering et al., 2018). Automation is
also helpful when communities experience unprece-
dented or unexpected growth, such in cases where
communities receive an influx of new users (Kiene
et al., 2016) and in cases where communities are
subject to sustained brigades of bad actors spam-
ming hateful content (Han et al., 2023). Mostly,
automated tools are developed and maintained by
moderators themselves. For example, the most
commonly used tool on Reddit, Automod, was
originally designed by a moderator before the com-
pany took over responsibility for its development
and maintenance (Jhaver et al., 2019) and moder-
ators on Twitch have developed bots on the fly in
response to hate raids (Han et al., 2023). While
automation can help support moderation work, it
may also add to it in different ways—like needing
to build and maintain a bot—and requires skill sets
not all moderators have (Jhaver et al., 2019).
Meanwhile, the NLP field has a long line of
work on developing automated models to detect
toxic content, which can be used to support moder-
ation work (e.g. Jigsaw, 2019; Pavlopoulos et al.,
2020; Park and Fung, 2017; Zampieri et al., 2019).
Beyond that, Lees et al. (2022) extended beyond
English content and proposed the multilingual Per-
spective API to detect offensive and hateful content
from a diverse range of languages. Moreover, somestudy delves into various types of toxic content,
trying to improve the nuance of detection models
(Markov et al., 2023; Price et al., 2020). However,
these studies primarily focus on detecting toxic
content, which is merely a subset of moderation
work. Therefore, our study uses Reddit rules as
an example to identify gaps in NLP moderation
models within a wide spectrum of rules.
Recently, Kumar et al. (2023) tested LLMs’ per-
formance on rule-based moderation with rules from
95subreddits. They evaluated GPT models (3, 3.5,
4) on a dataset consisting of removed Reddit posts
as violating and not violating content. They con-
clude that the GPT models are effective in con-
ducting moderation on subreddits like r/Movies but
struggle with other subreddits such as r/askscience
andr/AskHistorians . Our study builds on previous
results by studying gaps in both purpose-built mod-
eration models and LLMs. Rather than focusing on
the broad binary questions of removing or keeping
posts, we focus on coverage for specific modera-
tion rules. In addition, we include the perspectives
of human moderators to understand how existing
models address the needs of real-world moderation.
3 Hugging Face Model Review
One traditional approach to understanding a scien-
tific context is to perform a literature review. Be-
cause we are particularly interested in deployable
models, we opt for a model review , where instead
of reviewing papers, we review publicly available
models that can be adapted to assist content moder-
ators in making moderation decisions, with the fo-
cus on Reddit rules. We conduct the model review
via Hugging Face (HF)4, an open-source platform
that provides pre-trained models for various NLP
tasks, to understand the progress of model devel-
opment from existing research in covering various
moderation rules. To conduct the model review,
we gather rules from three subreddits and manually
investigate for each rule whether there exists an HF
model suitable for detecting such rule violations.
3.1 Method
Chandrasekharan et al. (2018) clustered 100 subred-
dits into six Meso groups plus four Micro groups
based on similarity of the removed posts. Since
Micro groups are subreddits, whose rules are more
specific to the particular subreddit, we focus on the
4https://huggingface.co/modelsMeso groups, whose rules are shared across mul-
tiple subreddits within the group. We pick three
subreddits – r/Atheism, r/Movies, r/AskHistorian ,
from three of the major Meso subreddit clusters and
use their rules as representatives for the study. We
verify the coverage of the three subreddits’ rules
by cross-referencing them with the list of 25rule
types from Fiesler et al. (2018), ensuring that all
text-based rule types are covered by at least one of
the rules from the three subreddits. See appendix
Table 1 for our list of rules and its cross-referencing
with the list of rule types.
Procedure For each rule, we first crawl its rule
description from Reddit and answer the following
two questions based on the description.
▶RULE -BASED ?: Can detection of violations on
this rule be done by rule-based approaches, e.g. by
regular expressions (yes or no)? We first mark out
the rules that can easily be handled by rule-based
approaches, such as detecting reposts. If the answer
is yes, we do not consider this rule any further; all
other rules are fully analyzed using following steps.
▶TOXIC ?: Is this rule covered by toxicity detec-
tion (yes or no)? To understand what portion of the
policies are related to toxicity detection, we mark
the rules that can be handled by toxicity detectors,
such as detecting harassment.
Then, we search on HF for a matching model
to each rule. Based on the rule descriptions and
our knowledge of previous NLP task names, we
identify keywords for model searching. For exam-
ple, for the rule Personal Attacks or Flaming :
“Keep things civil. Avoid fighting words and per-
sonal attacks. (applies to all forms of user content,
including the user’s name.)” , we use keywords
doxxing andcyberbullying .
We then search on HF by matching keywords
with model names or model cards to find the most
relevant model to each moderation rule. We tra-
verse the top 20 search results to find the most
relevant model, which we call the matching model .
If more than one model is relevant, we select the
model with the highest number of likes on HF. If
nothing relevant is found, we declare there is no
matching model for this moderation rule. We also
skip any model that does not contain a model card
(i.e., no model description). See Tables 3, 4, and 5
in the appendix for the keywords we used and the
matching models for each rule.
With the matching model, we then answer the
following questions for each moderation rule:Figure 1: Model review annotation results. Figures (a) and (b) are the results of annotation questions RULE -BASED ?
and TOXIC ?, respectively. Figures (c) and (d) are the results of question MATCH ?for rules related to toxicity
detection and not related, respectively.
▶MATCH ?: Is there an HF model that can be used
to detect violations of this rule? Possible answers
areyes;yes, but with changes ; orno. We mark yes,
but with changes when the model is intended to be
used for tasks similar to detecting violations of this
rule, but some adjustment are needed for the model
to be useful in this case. The adjustments needed
are noted in the last question.
▶PURPOSE ?: What is the original purpose of
this matching model? We note the intended use
case for the model as claimed in the model card.
▶GAP?: In order to adapt this model to detect
violations of this rule, what needs to be adjusted?
We write the changes, if needed, for the model to
be used for this rule, such as domain adaptation.
3.2 Model Review Results
As shown in Figure 1, among the 41moderation
rules, 37(90%) of the rules cannot be handled by
rule-based approaches. Among these, only 7(19%)
are toxicity-related. This reemphasizes that models
solely designed for detecting toxicity fall short in
meeting the practical needs of content moderators.
Among the 7toxicity-related rules, we see that 4
(57%) require model modifications. Most of these
modifications are customization for the rule, which
we will describe later. Even toxicity-related rules,
despite having many developed models, do not
have perfectly matching models.
Among the 37non-toxicity-related rules, only
6(20%) rules have matching HF models that can
be applied directly. To see some evaluation re-
sults of these matching models, please refer to
Appendix A in appendix. 13(43%) of the rules
require some model modifications in order for
the matching model to be adapted for the rule,
whereas 11(37%) rules have no matching model at
all. These rules include, for example, low-effortposts and proselytizing from r/Atheism , and
no homework and no “soapboxing” or loaded
questions from r/AskHistorians .
Overall, many rules lack a matching model. For
the rules that have one, most require non-trivial
model modifications to be covered. These results
indicate a substantial gap between the function-
alities of previously developed models and those
needed for Reddit moderation.
For the GAP?question, we primarily identify
four types of gaps.
1.The most common adjustment is do-
main adaptation to the rule-related topic
or to Reddit texts. For example, the
No Ambiguous/Misleading/Inaccurate
Information rule in r/Movies can be handled
with a misinformation detection model, but
the model needs to be adapted to title-like
texts and information about movie releases.
2.Some models are developed with a limited
scope of training data and thus may require ex-
tended training to be used for moderation. For
instance, we found a plagiarism model that
was trained on the Machine Paraphrase Cor-
pus (Wahle et al., 2022) consisting of ∼200k
examples of original and paraphrases using
two online paraphrasing tools. The plagiarism
cases on Reddit may be more complicated
than paraphrasing, and the potential plagia-
rism source is larger. Hence, the model needs
an extension of scope in order to be useful.
3.Models for some rules, especially toxicity-
related rules, require customization. For ex-
ample, r/Atheism has the rule Harassment or
Bigotry that prohibits harassment but permits
curse words. A toxicity detection model can
be used here but needs modification.
4.Some models can be applied to certain rules,but with a degree of stretching. For instance,
violations of the rule Off Topic can poten-
tially be caught by a topic classifier model,
but the model requires some major changes to
adapt to the subreddit and its related topics.
To see detailed annotations for each question,
please refer to Tables 3, 4, and 5 in appendix.
4 LLM Performance on Subreddit Rules
Finding a suitable NLP model for handling a mod-
eration rule, let alone executing the necessary mod-
ifications to the model, is not trivial, and some
rules simply lack a matching model to address their
specific issues. It requires familiarity with NLP
techniques and tasks, which may not be within the
scope of a content moderator’s abilities. Question-
answering-based LLM applications, on the other
hand, may be more practical for content modera-
tors to employ in assisting their jobs. Therefore,
we also evaluate LLMs’ performance in catching
rule violations. For this evaluation, we focus on the
r/AskHistorians subreddit, which has 23modera-
tion rules, and two LLMs — Llama-2 and GPT-4.
4.1 Method
Evaluation Dataset The evaluation dataset was
created and annotated by a volunteer moderator
forr/AskHistorians who has expertise in both
moderation and qualitative analysis. For each
rule, 11–13violating posts or comments were
selected, save for posts violating the Illegal
artifacts rule, which, due to the rareness of vio-
lations, was only represented 3times in the dataset.
Most content was selected from r/AskHistory , a
Reddit community that is thematically similar to
r/AskHistorians but with different moderation rules.
Some additional content was taken directly from
r/AskHistorians . We exclude all borderline cases
and only select posts and comments that clearly
violate the rules. In total, the dataset includes 101
questions and 134comments.5Because content of-
ten violates more than one rule, questions and com-
ments included in the dataset often reflect multiple
violations; however, annotations represent the most
relevant or severe violation. For example, a com-
ment containing only a joke would be annotated as
violating the rule prohibiting joke responses, even
5Inr/AskHistorians , posts are questions users have asked
and comments are responses to questions. Comments included
in the dataset are not necessarily responses to the questions
included in the dataset.though it would also violate rules on comprehen-
siveness or depth. The dataset also included 11
questions and 10comments that do not violate any
r/AskHistorians rules. We use these data as our
evaluation dataset.6
Model Testing and Prompts To assess the per-
formance of LLMs, we test GPT-4 and Llama-2-
13b models. We conduct a pilot experiment with
a small number of randomly selected examples
from our dataset to determine the prompt for model
assessment. Since moderators are mostly unfamil-
iar with prompt tuning and have limited time to
learn this skill, we aim to test the models without
extensively experimenting different wordings or
examples of the prompt, roughly as they might be
used by moderators.
First, as suggested in the prompting guidelines
(Shieh, 2023), we find that using rule descriptions
in the prompt is helpful for the model to detect
violations of the rule. Hence, we crawl rule de-
scriptions from r/askHistorians rule explanation
web page. As the descriptions from the subreddit
web page are long and include many constructive
suggestions for post-writers, we manually select
useful information for moderation to include in the
prompt. See appendix Table 2 for the list of the
rules and their descriptions.
Moreover, we experiment with both zero-shot
and few-shot settings. In the few-shot setting, we
provide the model with three examples that vio-
lated the rule and three examples that did not for
question contents. For comment contents, we pro-
vide one example for each due to the model’s input
length limit. We realize that few-shot examples are
helpful for the Llama-2 model to better understand
the task, whereas the GPT-4 model does not have
significant improvement from incorporating the ex-
amples. Therefore, for the main experiment, we
test GPT-4 with the zero-shot setting and Llama-2
with the few-shot setting, but we also report the full
set of experiments in the appendix.
Finally, following the suggestions from Shieh
(2023), we set the model’s role as a moderator
assistant and put the main question at the beginning
of the prompt. In the few-shot setting, we also
repeat the main question at the end of the prompt.
See Appendix B for the exact prompts we used.
6The dataset, along with a datasheet, are available under
a MIT licence at: https://github.com/TristaCao/into _
inclusivecoref .Figure 2: Model performance on detecting question posts that violate moderation rules with GPT-4 (top) and
Llama-2 (bottom) models. The x-axis is the moderation rules for question posts. Each bar pair is the precision (left)
and recall (right) scores on the specific rule. The ones marked grey are the model scores that moderators would not
consider useful as a moderation assistant tool. The left-most rules have good performance from both of the LLMs;
the rules in the middle have good performance from at least one of the LLMs; the right-most rules do not have good
enough performance from either of the LLMs.
4.2 Moderator Evaluation
To understand volunteer moderators’ perspectives
on the relative performance of LLMs, we also con-
ducted an evaluation survey with active moderators
from r/AskHistorians7. The survey aimed to cap-
ture 1) whether moderators would use the LLM
to help with their moderation work on each rule,
given the LLM’s performance (precision and recall)
identifying violations for this rule, and 2) for each
rule, what kind of model performance is needed to
effectively assist moderators.
To answer the first question, we asked partici-
pants to evaluate each model’s performance. We
showed participants the LLMs’ precision and recall
scores for each rule from our experiment and asked
them to choose among: 1. performance for this
rule is good enough that I would use the tool , 2.
I would use this tool for this rule if it could catch
more violations (need higher recall), 3. I would use
this tool for this rule if it could be correct more
often (need higher precision), or 4. both measures
would have to improve for me to use this tool.
For the second question, we asked participants
to cluster moderation rules based on how impor-
tant it is for the model to have high precision (or
high recall) for each rule, compared to other rules.
Options included: most important, less important,
andwould not use a model for this rule . We also
asked for participants’ comments on using LLMs
7Approved by our institutional IRB, #1704882-9.to identify violating posts in general. See appendix
Figures 7 and 8 for the exact questions we asked.
Overall, by invitation, we recruited 11out of
36total active moderators from r/AskHistorians .
We obtained their consent at the beginning of the
survey. Our participants spent an average of 25
minutes completing the survey. In return, partici-
pants received compensation in the amount of $25
USD upon completion of the survey in full.
4.3 Model Performance – Results
We tested GPT-4 and Llama-2 on the evaluation
dataset as we described above. We use the first
occurrence of “Yes” or “No” in the generated text
as the model’s output. The results are shown in
Figure 2 and Figure 3 (See appendix Figure 5 and
Figure 6 for the results of the models in both few-
shot and zero-shot settings). The results are mixed –
some rules have both high precision and high recall
from at least one model, while many of the rules
have moderate to low precision, recall, or both.
Neither of the two models is clearly better than the
other in identifying violating contents for all the
rules. The results indicate that certain rules remain
unresolved, even for state-of-the-art LLMs.
Our survey results indicate that different moder-
ators have different perceptions of what constitutes
an adequate model: for the evaluation questions,
participants exhibited only fair agreement (Fleiss’
Kappa 0.34). Mirroring policy decision-making
standards in the subreddit, we use majority voteFigure 3: Model performance on detecting comment posts that violate moderation rules with GPT-4 (top) and
Llama-2 (bottom) models. The x-axis is the moderation rules for comment posts. Each bar pair is the precision (left)
and recall (right) scores on the specific rule. The ones marked grey are the model scores that moderators would not
consider useful as a moderation assistant tool. The left-most rules have good performance from both of the LLMs;
the rules in the middle have good performance from at least one of the LLMs; the right-most rules do not have good
enough performance from either of the LLMs.
Figure 4: Clustering results from the survey study. Each
point is a moderation rule from r/AskHistorians . The
x-axis shows the precision importance score, or how
important it is for a model to have high precision for this
rule. Similarly, the y-axis shows the recall importance
score. Note that we removed rules for which at least
three participants stated they would not use a model.
to summarize participants’ evaluation of the mod-
els’ performance. The results are shown in Fig-
ure 2 and Figure 3. In most cases, moderators
would consider models with >70% precision and
recall to be useful. Among the 23rules, both mod-
els are considered useful for three question rules
and three comment rules. For some rules, such
asNSFW/Sensitive content ,No plagiarism ,No
political agendas or moralising , and Depth ,moderators can accept low recall; for some, such as
Do not just post links or quotations , mod-
erators can accept low precision. Impor-
tantly, there are six rules ( 26% of all rules)
for which neither model is considered use-
ful. These rules are Homework ,Privacy , and
Scope question rules and No partial answer or
“placeholders” ,Digression , and Sources com-
ment rules. This emphasizes that, while current
LLMs provide possibilities for supporting content
moderation work, further improvement is needed
to support useful moderation tools.
Most of the moderators were excited about the
idea of having even imperfect assistant models to
support their work. Two participants expressed
willingness to accept a model with moderate levels
of precision and recall, because “any help is bet-
ter than no help. ” One stated “A tool flags a lot
of things but isn’t right too often? Well, it’s still
flagging things for my attention. A tool that doesn’t
flag a lot but is very frequently right? Great, I don’t
have to think about whether or not to remove what
it’s found. ” This response emphasizes the impor-
tance of model transparency. Model users often can
adapt to a flawed model and make it useful as long
as they are informed of its performance limitations.
Some moderators value precision more than re-
call because a model that mis-flagged many posts
would make moderators’ workload unnecessarily
large. On the other hand, some moderators consider
recall to be more important to catch all violatingposts missed by moderators, especially for urgent
or sensitive content, such as for the rule Civility .
Moreover, some participants state that the need for
the model varies according to the rules. One partic-
ipant specified, “A tool would be useful for assess-
ing and filtering breaches of more complex rules
like plagiarism (especially Chat GPT use) or poor
sourcing, but given the time needed to assess these
claims it would need to be correct the overwhelm-
ing majority of the times it flagged something. For
simpler rules like clutter, depth, or no jokes then
I would like to see a tool which is able to flag the
majority of comments which break the rules. ” The
same participant also pointed out that they would
not use a model for rules that are inherently sub-
jective between moderators, such as Basic facts .
Another participant further indicated the need for
model explanations for complex rules.
To inform future improvement on moderation
assistant models, we asked moderators to cluster
rules according to their needs. The results, as in
Figure 4, provide quantified insight into moder-
ators’ needs for model precision and recall. We
aggregate participants’ answers by averaging their
choice of cluster for each rule. Each time a partici-
pant assigned a rule to most important , it is scored
as two points; assignments to less important are
scored as one point. Rules for which a participant
selected would not use are not included in the score
calculation but reported separately. Among the
23rules, ten require both high precision and high
recall ( >1.5). For three rules, at least three partici-
pants claimed they would not use a model — Basic
facts, Scope, andSources . We hope our results
can serve as a guide for future moderation assistant
models to better align with moderators’ needs.
5 Discussion
Overall, our findings suggest potential directions
for future research on moderation assistants. First,
there remains ample opportunity for model im-
provements. There are ten rules that moderators re-
quire both high precision and high recall, as in Fig-
ure 4. For three of them, neither GPT-4 nor Llama-
2 are considered useful by moderators: Privacy
and Homework with low recall ( 27%), and No
partial answer or “placeholders” with low
precision ( 63%). For four of the ten rules, al-
though moderators mark one model as acceptable,
there is still room for improvement in precision
or recall: Plagiarism ,NSFW/Sensitive content ,andDepth with recall 40−62%, and Do not just
post links or quotations with precision 69%.
Besides, the rule Digression , which requires high
precision, has low precision from both models
(58% and64%). Though some rules are distinct
forr/AskHistorians (e.g., Homework ), many rules
are also enforced in other subreddit communities.
For example, among the 523 subreddits studied
by Fiesler et al. (2018), 39% have rules on off-
topic content (similar to the Digression rule in
our study) and 27% have rules on NSFW content.
Hence, our findings underscore the existence of
plenty of rules from online communities beyond
toxicity detection that require exploration.
Despite the models not performing as well as
one might hope, volunteer moderators express sig-
nificant enthusiasm and flexibility about having
moderation assistants. People are used to working
with tools that may not be perfect, and they are
skilled at finding ways to make the most of them.
In our findings, moderators explained several per-
sonal approaches they take in using a model with
moderately low precision or with moderately low
recall. However, in order to exercise these strate-
gies, models’ abilities must be transparent to enable
users to adapt accordingly.
Furthermore, we observed varying requirements
among users and rules. Even for the same rule,
different moderators expressed distinct preferences
for high precision or recall. This suggests that
in situations where a trade-off between precision
and recall is necessary, moderators may lean to-
wards a model offering more user control, thus
allowing for an adjustable trade-off. This way,
they can customize the model to align with their
preferences. In addition, our participants also ex-
pressed some common preferences. For complex
rules (e.g. Plagiarism andDigression ), moder-
ators prefer higher recall and model explanations
over mere flagging of violations. Conversely, for
simpler rules (e.g. Current Event andJokes and
Humour ), they prefer higher precision.
Finally, our study showcases the importance of
involving direct stakeholders (e.g. moderators) in
the design and development loop for AI tools. They
not only ease the alignment of model construction
with users’ needs but also provide insights model
developers might otherwise miss.6 Conclusions
We identified gaps between the functionalities of
previous models on automated moderation and the
functions needed to address Reddit moderation
rules. We conducted a model review with Hug-
ging Face models to to see for how many rules an
existing model can be used to detect violations. The
findings reveal that a considerable number of rules
are not covered by existing Hugging Face models.
Even when a matching model exists, more than half
of the rules require non-trivial adjustments of the
model in order to be useful. We additionally iden-
tified four major types of necessary adjustments:
domain adaptation, model scope extension, cus-
tomization, and function shift.
Moreover, we evaluated two LLMs, GPT-4 and
Llama-2, on an evaluation dataset gathered from
r/AskHistorians , and conducted a survey study to
reveal whether the state-of-the-art general-purpose
LLMs can handle various moderation rules. The re-
sults indicate there is still a significant gap — these
models exhibit either low recall or moderate to low
precision for many of these rules. Also, moderators
are not satisfied with either of the models’ perfor-
mance for six of 23rules. This limits the usability
of these models in aiding moderators to identify
violations in real-world moderation.
Finally, our moderator survey study provides in-
sight into model-performance requirements for a
moderation assistant model. We observed modera-
tors’ excitement about the model and willingness to
be flexible to accommodate a flawed model. Mod-
erators also offered constructive feedback on how
they wish to use an assistant model differently for
different rules. Such user-centered guidance should
be useful in building improved, customized moder-
ation tools in the future.
7 Limitations
Our study has several limitations. We are primarily
studying Reddit; other platforms may have differ-
ent sets of rules. We are also focusing on English
and text-only posts; violations in other languages
or in other forms, such as images and videos, may
face different challenges. Furthermore, our survey
study is limited to volunteer content moderators
from the r/AskHistorians community. While we
anticipate that the insights gained from the moder-
ation experience might have broader applicability
across communities and for other moderators, the
extent of generalization remains uncertain withoutfurther study.
Moreover, our study and findings are grounded
in the set of Reddit moderation rules rather than
the frequency of their violation. Consequently, we
lack information regarding which rules moderators
encounter most frequently and for which rules they
want additional assistance.
In addition, our model testing is limited to one-
shot and few-shot contexts, as opposed to other
strategies, such as interactive teaching and Chain-
of-Thoughts.
Acknowledgments
This material is based upon work partially sup-
ported by the National Science Foundation under
Grant No. 2229885 and No. 2131508.
The authors are also grateful to all the reviewers
who have provided helpful suggestions to improve
this work, and thank members of the CLIP lab
at the University of Maryland for the support on
this project. We are grateful to all those who par-
ticipated in our human study, without whom this
research would not have been possible.
References
Amy Binns. 2012. Don’t feed the trolls! Managing trou-
blemakers in magazines’ online communities. Jour-
nalism practice , 6(4):547–562.
Eshwar Chandrasekharan, Mattia Samory, Shagun
Jhaver, Hunter Charvat, Amy Bruckman, Cliff
Lampe, Jacob Eisenstein, and Eric Gilbert. 2018. The
internet’s hidden rules: An empirical study of reddit
norm violations at micro, meso, and macro scales.
Proceedings of the ACM on Human-Computer Inter-
action , 2(CSCW):1–25.
Julian Dibbell. 1994. A rape in cyberspace or how an
evil clown, a haitian trickster spirit, two wizards, and
a cast of dozens turned a database into a society. Ann.
Surv. Am. L. , page 471.
Bryan Dosono and Bryan Semaan. 2019. Moderation
practices as emotional labor in sustaining online com-
munities: The case of AAPI identity work on Reddit.
InProceedings of the 2019 CHI conference on human
factors in computing systems , pages 1–13.
Casey Fiesler, Jialun Jiang, Joshua McCann, Kyle Frye,
and Jed Brubaker. 2018. Reddit rules! Characterizing
an ecosystem of governance. Proceedings of the
International AAAI Conference on Web and Social
Media , 12(11).
Sarah A Gilbert. 2020. "I run the world’s largest his-
torical outreach project and it’s on a cesspool of a
website." Moderating a public scholarship site onReddit: A case study of r/AskHistorians. Proceed-
ings of the ACM on Human-Computer Interaction ,
4(CSCW1):1–27.
Catherine Han, Joseph Seering, Deepak Kumar, Jef-
frey T Hancock, and Zakir Durumeric. 2023. Hate
raids on twitch: Echoes of the past, new modalities,
and implications for platform governance. Proceed-
ings of the ACM on Human-Computer Interaction ,
7(CSCW1):1–28.
Shagun Jhaver, Iris Birman, Eric Gilbert, and Amy
Bruckman. 2019. Human-machine collaboration for
content regulation: The case of reddit automoderator.
ACM Transactions on Computer-Human Interaction
(TOCHI) , 26(5):1–35.
Jialun Aaron Jiang, Charles Kiene, Skyler Middler,
Jed R Brubaker, and Casey Fiesler. 2019. Moderation
challenges in voice-based online communities on dis-
cord. Proceedings of the ACM on Human-Computer
Interaction , 3(CSCW):1–23.
Conversational AI Jigsaw. 2019. Jigsaw unintended
bias in toxicity classification. Kaggle .
Charles Kiene and Benjamin Mako Hill. 2020. Who
uses bots? A statistical analysis of bot usage in mod-
eration teams. In Extended abstracts of the 2020 CHI
conference on human factors in computing systems ,
pages 1–8.
Charles Kiene, Andrés Monroy-Hernández, and Ben-
jamin Mako Hill. 2016. Surviving an "eternal septem-
ber" how an online community managed a surge of
newcomers. In Proceedings of the 2016 CHI Con-
ference on Human Factors in Computing Systems ,
pages 1152–1156.
Deepak Kumar, Yousef AbuHashem, and Zakir Du-
rumeric. 2023. Watch your language: Large language
models and content moderation. (arXiv:2309.14517).
ArXiv:2309.14517 [cs].
Alyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai
Gupta, Donald Metzler, and Lucy Vasserman. 2022.
A new generation of perspective api: Efficient multi-
lingual character-level transformers. In Proceedings
of the 28th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining , KDD ’22, page
3197–3207, New York, NY , USA. Association for
Computing Machinery.
Hanlin Li, Brent Hecht, and Stevie Chancellor. 2022.
Measuring the monetary value of online volunteer
work. In Proceedings of the International AAAI Con-
ference on Web and Social Media , volume 16, pages
596–606.
Todor Markov, Chong Zhang, Sandhini Agarwal, Flo-
rentine Eloundou Nekoul, Theodore Lee, Steven
Adler, Angela Jiang, and Lilian Weng. 2023. A holis-
tic approach to undesired content detection in the
real world. Proceedings of the AAAI Conference on
Artificial Intelligence , 37(12):15009–15018.Ji Ho Park and Pascale Fung. 2017. One-step and two-
step classification for abusive language detection on
Twitter. In Proceedings of the First Workshop on
Abusive Language Online , pages 41–45, Vancouver,
BC, Canada. Association for Computational Linguis-
tics.
John Pavlopoulos, Jeffrey Scott Sorensen, Lucas Dixon,
Nithum Thain, and Ion Androutsopoulos. 2020. Toxi-
city detection: Does context really matter? In Annual
Meeting of the Association for Computational Lin-
guistics .
Ilan Price, Jordan Gifford-Moore, Jory Fleming, Saul
Musker, Maayan Roichman, Guillaume Sylvain,
Nithum Thain, Lucas Dixon, and Jeffrey Sorensen.
2020. Six attributes of unhealthy conversation.
arXiv:2010.07410 [cs] . ArXiv: 2010.07410.
Joseph Seering. 2020. Reconsidering community self-
moderation: the role of research in supporting
community-based models for online content modera-
tion. Proceedings of the ACM on Human-Computer
Interaction , 4:107.
Joseph Seering, Juan Pablo Flores, Saiph Savage, and
Jessica Hammer. 2018. The social roles of bots:
evaluating impact of bots on discussions in online
communities. Proceedings of the ACM on Human-
Computer Interaction , 2(CSCW):1–29.
Joseph Seering, Tony Wang, Jina Yoon, and Geoff Kauf-
man. 2019. Moderator engagement and community
development in the age of algorithms. New Media &
Society , 21(7):1417–1443.
Jessica Shieh. 2023. Best practices for prompt engineer-
ing with openai api. [Online; accessed Nov 2023].
Jan Philip Wahle, Terry Ruas, Tomáš Foltýnek, Nor-
man Meuschke, and Bela Gipp. 2022. Identifying
machine-paraphrased plagiarism. In Information for
a Better World: Shaping the Global Future , pages
393–413, Cham. Springer International Publishing.
Donghee Yvette Wohn. 2019. V olunteer moderators in
twitch micro communities: How they get involved,
the roles they play, and the emotional labor they ex-
perience. In Proceedings of the 2019 CHI conference
on human factors in computing systems , pages 1–13.
Marcos Zampieri, Shervin Malmasi, Preslav Nakov,
Sara Rosenthal, Noura Farra, and Ritesh Kumar.
2019. Predicting the type and target of offensive
posts in social media. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers) , pages 1415–1420, Minneapolis, Minnesota.
Association for Computational Linguistics.A Evaluation of HF Matching Models
We evaluated the HuggingFace matching models with our r/AskHistorians evaluation dataset. As indicated
in Table 5, there are four non-toxicity-related rules that have matching HF models: NSFW/Sensitive
content ,Illegal artifacts ,Basic Facts , and Jokes and humor . Among these four matching models,
the model for Illegal artifacts is not compatible with the HuggingFace model pipeline and lacks the
appropriate code for inference. Similarly, the model for Basic Facts uses a model structure that is not
defined within Huggingface module.
Consequently, we evaluated only the models for NSFW/Sensitive content and the model for Jokes
and humor using our dataset. The performance of the NSFW/Sensitive content model is: precision
57.1%and recall 92.3%. The performance of the Jokes and humor model is: precision 55.6%and recall
45.5%.
B Prompt for Testing LLMs
B.1 Prompt for Testing Llama-2 with the Few-shot Setting
<s>[INST] «SYS»
You are a moderator assistant. There is a moderation policy named {rule} that says
{rule description}. Does the following content violate the {rule} policy? Simply
give a yes or no answer.
«/SYS»
{violating example content}
Does the following content violate the {rule} policy? Simply answer yes or no.[/INST]
Yes </s><s>[INST]
{not violating example content}
Does the following content violate the {rule} policy? Simply answer yes or no.[/INST]
No </s><s>[INST]
{test content}
Does the following content violate the {rule} policy? Simply answer yes or no.[/INST]
B.2 Prompt for Testing GPT-44 with the Zero-shot Setting
"role": "system", "content": "You are a moderator assistant.",
"role": "user", "content": "There is a moderation policy named {rule} that says
{rule description}. Does the following content violate the {rule} policy? Simply
give a yes or no answer.",
"role": "user", "content": {test content},
C LLMs Performances under Zero-shot and Few-shot SettingsFigure 5: Model performance on detecting violations of moderation rules with GPT-4 model under few-shot setting
(top) and zero-shot setting (bottom). The x-axis is the moderation rules. Each bar pair is the precision (left) and
recall (right) scores on the specific rule.
Figure 6: Model performance on detecting violations of moderation rules with Llama-2 model under few-shot
setting (top) and zero-shot setting (bottom). The x-axis is the moderation rules. Each bar pair is the precision (left)
and recall (right) scores on the specific rule.D User Survey
Figure 7: The evaluation survey question for the rule Sources . The question content is similar for all the rules.Figure 8: The clustering survey question for recall importance. The same question is asked for precision importance.Categories of Rules AskHistorian Atheism Movies
Advertising & Commer-
cializationNo Spam & Self-promotion
- Same Source Posts
Consequences/ Mod era-
tion/ Enforce ment
Content/BehaviorHomework; No "what if"
questions Basic facts Depth;
No speculation; Jokes and
humorProselytizingNo Antagonism/ Flame-
wars/ Attention Whoring -
No Racial/ Sexist/ Dogwhis-
teling/ Homophobic Slurs;
No Extraneous Comic Book
Movie Submission"
Copyright No plagiarism
Doxxing/Personal InfoPrivacy; No personal anec-
dotesPersonal Attacks or Flaming
FormatNo partial answers or "place-
holders"; Do not just post
links or quotationsNo Ambiguous/ Misleading/
Inaccurate Information or
Clickbait in The Submission
Title
Harassment Civility Harassment or BigotryNo Antagonism/ Flame-
wars/ Attention Whoring -
No Racial/ Sexist/ Dogwhis-
teling/ Homophobic Slurs
Hate Speech CivilityNo Antagonism/ Flame-
wars/ Attention Whoring -
No Racial/ Sexist/ Dogwhis-
teling/ Homophobic Slurs
Images
Links & Outside Con-
tentSources Prohibited Popular Websites
Low-Quality Content Scope; Clutter Low-Effort Posts
NSFW NSFW/Sensitive content
Off-topic Digression Off TopicNo Extraneous Comic Book
Movie Submission
Personal Army Brigading No Subreddit Brigading
Personality Civility Personal Attacks or FlamingNo Antagonism/ Flame-
wars/ Attention Whoring -
No Racial/ Sexist/ Dogwhis-
teling/ Homophobic Slurs
PoliticsNo political agendas or
moralising; No "Soapbox-
ing" or loaded questions
Prescrip tive
Reddiquette
RepostingNo Repost or Discussion
Threads of New Releases
Restrictive
Spam SpamNo Spam & Self-promotion
- Same Source Posts
SpoilersNo Repost or Discussion
Threads of New Releases;
No Spoilers
Trolling No TrollingNo Ambiguous/ Misleading/
Inaccurate Information or
Clickbait in The Submission
Title
V oting "Poll-type" question
OthersCurrent event; Illegal arti-
factsDon’t complain about the
use of AA VE or slangNo Encouraging Piracy
Table 1: Subreddit rules from AskHistorian, Atheism, and Movies subreddits and their cross-referencing with
categories of rule summarized by Fiesler et al. (2018). Rules within one category are separated by semicolons.
Categories crossed out are rules that are not text-relevant or not about rule content.Rule Rule Description
Civility All users are expected to behave with courtesy and politeness at all times. We will not tolerate
racism, sexism, or any other forms of bigotry. This includes Holocaust denialism. Nor will
we accept personal insults of any kind, and do not allow minor nitpicking of grammar or
spelling.
Scope Submissions must be about a question about the human past, a META post about the state of
the subreddit, or an AMA ("Ask Me Anything") with a historical expert or panel of experts.
No Current events To discourage off-topic discussions of current events, questions, answers, and all other
comments must be confined to events that happened 20 years ago or more, inclusively (e.g.,
2003 and older).
Homework Our users aren’t here to do your homework for you, but they might be willing to help. Don’t
just give us your essay/assignment topic and ask us for ideas.
NSFW/Sensitive
contentQuestions with NSFW titles will be deleted, and we will ask you to repost it with a different
title.
No "Poll"-type
questions"Poll"-type questions aren’t appropriate here: "Who was the most influential person in
history?" or "Who was the worst general in your period?" or "Who are your Top 10 favorite
people in history?" If your question includes the words "most" or "least", or "best" or "worst"
(or can be reworded to include these words), it’s probably a "poll"-type question.
No "Soapboxing" or
Loaded QuestionsAll questions must allow a back-and-forth dialogue based on the desire to gain further
information and not be predicated on a false and loaded premise in order to push an agenda.
Example: Good Question: "People say that Nixon is the worst President of all time. Why
is this so?" Bad Question: "Nixon was the worst President of all time. Why isn’t Obama
considered the worst?" The bad question is a fishing expedition to try to start a debate about
Obama’s presidency. Most of these questions will break our 20-year rule, or try to set up a
debate about an issue using a long wall of text in the main post.
No "What If" Ques-
tionsQuestions should be about what did happen, not what could have happened.
Privacy We do not allow questions that pose possible privacy issues for living or recently deceased
persons who are not in the public eye. The cut-off for "recent" is 100 years.
Illegal artifacts It is our policy to disallow posts asking for further information on artifacts where there is a
likelihood that the acquisition or possession of the item might be illegal, unethical, and/or
run contrary to sound, historical practices.
Basic Facts Questions looking for specific, basic facts - for the purpose of this rule, seeking a name, a date
or time, a number, a location, the origin of a word, the first or last known instance/example
of an object/phenomenon/etc., or a simple list of examples or facts - are not allowed as
standalone threads.
Depth An in-depth answer provides the necessary context and complexity that the given topic calls
for, going beyond a simple cursory overview. Your answer should be giving context to the
events being discussed, not simply listing some related facts.
Sources We do not require sources to be preemptively listed in an answer here, but do expect that
respondents be familiar with relevant and reliable literature on the topic, and that answers
reflect current academic understanding or debates on the subject at hand. But sole reliance
on tertiary sources for context and analysis is not allowed, and will result in the removal of a
response.
No personal anec-
dotesPersonal anecdotes are not acceptable answers in this subreddit.
No speculation Suppositions and personal opinions are not a suitable basis for an answer here. Warning
phrases for speculation include: "I guess..." or "My guess is...", "I believe...", "I think...", "...
to my understanding", "It makes sense to me that...", "It’s only common sense."
No partial answers
or "placeholders"An answer should be full and complete in and of itself.
No political agen-
das or moralizingThis subreddit is a place for learning and open-minded discussion. As such, answers should
not be written in the interests of advancing a personal agenda, but should represent a sincere
effort to make an argument from the historical record. They should be constructed in keeping
with the principles of the historical method - that is to say, your evidence should not be
chosen selectively to support an argument that you feel is right; your argument should instead
demonstrably flow from your critical engagement with an appropriate range of evidence.
Do not just post
links or quotationsDo not just post links to other sites as an answer. This is not helpful. The expectation is that
a user is posting to this subreddit because they are looking for the type of answer dictated by
the rules in place here. Please take some time to put the links in context for the person asking
the question. Avoid only recommending a source – whether that’s another site, a book, or
large slabs of copy-pasted text. If you want to recommend a source, please provide at least a
small summary of what the source says.
No plagiarism We have a zero-tolerance policy on blatant plagiarism, such as directly copying and pasting
another person’s words and trying to pass them off as your own.
Jokes and humor A post should not consist only of a joke, a humorous remark, or a flippant comment. You
can certainly include humor as part of a full and comprehensive post, but your post should
not be made solely for the purpose of being funny.
Digression All comments should be related to the topic as outlined in the original post.
Clutter Please refrain from commenting for the sake of commenting. This includes, but is not limited
to, statements about how interesting the question is, how you would like to see an answer, to
"remember to come back later," to share a story that the question reminds you of, and so on.
Table 2: AskHistorian Rules and Rule DescriptionsRules KeywordsAnnotated
modelRULE -
BASED ?TOXIC ?MATCH ?PURPOSE ? GAP?
No Trollingtrolling;
provocativeTRAC2020
_IBEN_A_
bert-base-
multilingual-
uncasedNo Yes Yes
Personal
Attacks or
Flamingdoxxing; cy-
berbullyingTRAC2020
_IBEN_A_
bert-base-
multilingual-
uncasedNo Yes Yes
Off Topictopic classi-
fiertweet-topic-
21-multiNo NoYes, but
with
changesTwitter
topic classi-
ficationNeed to be finetuned on
this topic
Image not
submitted
correctly
Spam spambert-tiny-
finetuned-
sms-spam-
detectionNo NoYes, but
with
changesDetect mes-
sage spam1. Need to be adapted
to reddit texts rather
than message texts. 2.
Need to be adapted
to detect self-promotion
of some youtube ac-
count/website etc.
Low-Effort
Postslow effort;
low qualityNA No No No
Proselytizingproselytizing;
SoapboxNA No No No
Harassment
or Bigotryharassment;
bigotryText-
ModerationNo YesYes, but
with
changesany and all curse words
are permitted
Brigading brigadingNA - Toxic-
ityNo YesYes, but
with
changesThese models may be
able to detect some of
the brigading comments,
but there is no specific
model for brigading and
some brigading encour-
aging comments may
not be caught by the tox-
icity models
Table 3: Model review for the Atheism subreddit. The second and third columns are the keywords used for model
searching and the best-matching model from hugging face. The rest columns are annotation results. Crossed-out
rules are rules not related to text input.Rules KeywordsAnnotated
modelRULE -
BASED ?TOXIC ?MATCH ?PURPOSE ? GAP?
DoFamiliar-
ize Your self
With Our
Rules
Incivility
- No An-
tagonism/
Flamewars/
Attention
Whoring -
No Racial/
Sexist/ Dog-
whistling/
Homophobic
Slursoffensive;
hatespeechText-
ModerationNo YesYes, but
with
changesYesQuoting something of-
fensive from a movie is
okay but needs to be put
in quotation marks.
No Spam
& Self-
promotion -
Same Source
Postsspambert-tiny-
finetuned-
sms-spam-
detectionNo NoYes, but
with
changesNoDetect message spam -
1. Need to be adapted
to Reddit texts rather
than message texts. 2.
Need to be adapted
to detect self-promotion
of some YouTube ac-
count/website etc.
No Image
Posts &
Memes
No Am-
biguous/
Misleading/
Inaccurate
Information
or Clickbait
in The Sub-
mission Title
- misinforma-
tionmisinformationTwHIN-
BERT-
Misinformation-
ClassifierNo NoYes, but
with
changesNoDetect misinformation
for Twitter posts - 1.
Need to be adapted to
title-like texts. 2. Need
to be adapted to a movie-
related context
No Am-
biguous/
Misleading/
Inaccurate
Information
or Clickbait
in The Sub-
mission Title
- Clickbaitclickbaitdistilroberta-
clickbaitNo No Yes Yes
No Extrane-
ous Comic
Book Movie
SubmissionmisinformationTwHIN-
BERT-
Misinformation-
ClassifierNo NoYes, but
with
changesNoDetect misinformation
for Twitter posts - Need
to be adapted to a movie-
related context
No Repost rule-based Yes No
No Discus-
sion Threads
of New
Releasesrule-based Yes No
No Spoilers spoilerroberta-
base-
finetuned-
imdb-
spoilersNo No Yes Yes
No Encourag-
ing Piracypiracy Classification No NoYes, but
with
changesNoDetect if a website is a
piracy website - Need
to detect not only piracy
website but also com-
ments that encourage
piracyTable 4 – Continued from previous page
Rules KeywordsAnnotated
modelRULE -
BASED ?TOXIC ?MATCH ?PURPOSE ? GAP?
No Subreddit
BrigadingbrigadingNA - Toxic-
ityNo YesYes, but
with
changesNoThese models may be
able to detect some of
the brigading comments,
but there is no specific
model for brigading and
some brigading encour-
aging comments may
not be caught by the tox-
icity models
Prohibited
Popular
Websitesrule-based Yes No
Table 4: Model review for the Movies subreddit. The second and third columns are the keywords used for model
searching and the best-matching model from hugging face. The rest columns are annotation results. Crossed-out
rules are rules not related to text input.Rules KeywordsAnnotated
modelRULE -
BASED ?TOXIC ?MATCH ?PURPOSE ? GAP?
Civilitytoxicity; hate-
speechText-
ModerationNo Yes YesThe model may not be
able to catch Holocaust
denialism, which is vital
in this case.
Current eventtime/year
prediction for
event; history
event; event
extractorGoLLIE-
7BNo NoYes, but
with
changesInformation
extraction1. The model needs to
be adapted to question-
contexts and extract
event names from the
question. 2. Then by
matching the event
name with Wikipedia
page, we may get the
year or time of the
event.
Homework homework NA No No No
Scopetopic classi-
fiertweet-topic-
21-multiNo NoYes, but
with
changesTwitter
topic classi-
ficationNeed to be fine-tuned to
this topic.
NSFW/Sensitive
contentNSFW; sex-
ualNSFW_text
_classifierNo No Yes
"Poll-type"
questionpoll NA No No No
"Soapboxing"
or loaded
questionssoapboxing;
agenda
pushingNA No No No
No "what if"
questionshypothetical;
counter-
factual;
alternative
historyNA No No No
PrivacyPII; personal
privatelgpd_pii_
identifierNo NoYes, but
with
changesIdentify
sensitive
data in the
scope of
LGPD. The
goal is to
have a tool
to identify
document
numbers
like CNPJ,
CPF, peo-
ple’s names,
and other
kinds of
sensitive
data, al-
lowing
companies
to find and
anonymize
data accord-
ing to their
business
needs, and
governance
rules.Need to be adapted to
the sensitive data here
Illegal arti-
factsillegal; safetybeaver-
dam-7bNo No Yes
Basic factsbasic facts;
factoidnf-cats No No Yes
Depth depth NA No No No
Sourcesreliable
sourceNA No No NoTable 5 – Continued from previous page
Rules KeywordsAnnotated
modelRULE -
BASED ?TOXIC ?MATCH ?PURPOSE ? GAP?
No personal
anecdotespersonal
anecdotemuppet-
roberta-
base-
joke_detectorNo NoYes, but
with
changesDetect
jokes, sto-
ries, and
anecdotesThe model is trained
with 2000 jokes. Here
the detection target is
not about jokes. In addi-
tion, here the need is fo-
cusing on personal anec-
dotes.
No specula-
tionspeculation;
factual;
fact-checkverdict-
classifier-
enNo NoYes, but
with
changesFact-
checking
model for
detecting
misinfor-
mationThe model does not de-
tect speculation or per-
sonal opinion. Here the
detection target is the
speculation of history
events, which can be es-
pecially hard for such a
fact-checking model.
No partial
answers or
"placehold-
ers"partial
answerNA No No No
No political
agendas or
moralizingpolitical
agenda;
moralizingNA No No No
Do not just
post links or
quotationsrule-based NA Yes No
No plagia-
rismplagiarismlongformer-
base-
plagiarism-
detectionNo NoYes, but
with
changesDetect
machine-
paraphrased
plagiarismsThe real-world source
of plagiarism is way
larger than the training
and testing dataset size
of this model.
Jokes and hu-
morjoke; humor;
flippantmuppet-
roberta-
base-
joke_detectorNo No Yes
Digression relevanceresponse-
quality-
classifier-
largeNo NoYes, but
with
changesRate the
relevancy
and speci-
ficity of a
response
within a
dialogueNeed to be adapted to
the Reddit (question-
comment) texts.
Clutter clutter NA No No No
Table 5: Model review for the AskHistorian subreddit. The second and third columns are the keywords used for
model searching and the best-matching model from Hugging Face. The rest of the columns are annotation results.
Crossed-out rules are rules not related to text input.