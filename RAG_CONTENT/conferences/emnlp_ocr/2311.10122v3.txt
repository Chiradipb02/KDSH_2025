Video-LLaVA: Learning United Visual Representation by Alignment
Before Projection
Bin Lin1, Yang Ye1, Bin Zhu1, Jiaxi Cui4,
Munang Ning1,2,3,Peng Jin1,2,3,Li Yuan1,2,3
1Peking University Shenzhen Graduate School,2Peng Cheng Laboratory,
3AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School,
4PandaVilla Tech Limited
Correspondence: yuanli-ece@pku.edu.cn
GitHub: https://github.com/PKU-YuanGroup/Video-LLaV A
Abstract
Large Vision-Language Model (LVLM) has en-
hanced the performance of various downstream
tasks in visual-language understanding. Most
existing approaches encode images and videos
into separate feature spaces, which are then fed
as inputs to large language models. However,
due to the lack of unified tokenization for im-
ages and videos, namely misalignment before
projection, it becomes challenging for a Large
Language Model (LLM) to learn multi-modal
interactions from several poor projection layers.
In this work, we unify visual representation
into the language feature space to advance the
foundational LLM towards a unified LVLM.
As a result, we establish a simple but robust
LVLM baseline, Video-LLaVA , which learns
from a mixed dataset of images and videos,
mutually enhancing each other. As a result,
Video-LLaV A outperforms Video-ChatGPT by
5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT,
MSVD, TGIF, and ActivityNet, respectively.
Additionally, our Video-LLaV A also achieves
superior performances on a broad range of 9
image benchmarks. Notably, extensive experi-
ments demonstrate that Video-LLaV A mutually
benefits images and videos within a unified vi-
sual representation, outperforming models de-
signed specifically for images or videos. We
aim for this work to provide modest insights
into the multi-modal inputs for the LLM.
1 Introduction
Recently, LLMs have gained rapid popularity in the
AI community, such as GPT-3.5, GPT-4 (OpenAI,
2023), PaLM (Bi et al., 2020; Anil et al., 2023),
and BLOOM (Scao et al., 2022). They rely on
their powerful language comprehension abilities
to follow human-provided instructions and pro-
vide corresponding responses. Typically, LLMs
can only respond within the text input provided
by the user, which is insufficient because human
interaction with the world involves multiple chan-
nels, such as visual and textual. To this end, recent
LLM
ProjectionLLM
Pre-align
Image
EncoderVideo
EncoderShare ProjectionCache
Database
Select image   
featuresMniGPT-4 / InstructionBLIPLLM
Image
Encoder
TextProjectionLLM
Visual
EncoderProjection
ImageLLM
Video
Encoder
TextProjection
Video Image Text Video
Image
Encoder
TextVideo -ChatGPT
Image Video Text Image VideoVideoChat / Video -LLaMA
ImageBind-LLM / LLaMA-Adapter Video -LLaVA (Ours)LLM
Image
EncoderProjection
Text ImageVideo
EncoderProjection
Video
Macaw -LLM / X-LLMFigure 1: Comparing Different LVLM Paradigms.
Video-LLaV A aligns images and videos before projec-
tion, allowing LLM to learn from a unified visual rep-
resentation and endowing LLM with the ability to com-
prehend both images and videos simultaneously.
works (Ye et al., 2023; Zhu et al., 2023b; Alayrac
et al., 2022) have mapped images into text-like to-
kens, enabling LLMs to emerge with the ability to
comprehend images. Despite their effectiveness,
empowering LLMs to understand videos is more
challenging than image-only comprehension tasks.
Nevertheless, recent work (Maaz et al., 2023; Li
et al., 2023c; Zhang et al., 2023a) has made initial
strides in enabling interactions between video and
language.
However, most current LVLMs (Li et al., 2023b;
Dai et al., 2023; Luo et al., 2023; Li et al., 2023a;
Yin et al., 2023; Fu et al., 2023) can primar-
ily handle a single visual modality, either image-
language or video-language. We compare dif-
ferent LVLM paradigms as shown in Figure 1,
where VideoChat (Li et al., 2023c) and Video-
LLaMA (Zhang et al., 2023a) utilize a share visual
encoder to handle both images and videos. How-
ever, due to the inherent differences in the media
types of images and videos, it is challenging to
learn a unified representation, and the performance
falls significantly behind that of the specialized
video expert model, Video-ChatGPT. Therefore,arXiv:2311.10122v3  [cs.CV]  1 Oct 2024X-LLM (Chen et al., 2023) and Macaw-LLM (Lyu
et al., 2023) allocate a modality-specific encoder
for each modality, attempting to enable a LLM to
comprehend images or videos through several pro-
jection layers. But their performances are inferior
to dedicated video expert models such as Video-
ChatGPT (Maaz et al., 2023). We attribute this
phenomenon to the lack of alignment before pro-
jection . Because image features and video features
reside in their own spaces, this poses a challenge for
a LLM to learn their interactions from several poor
projection layers. Some similar phenomenon such
asalignment before fusion has been discussed by
ALBEF (Li et al., 2021) and ViLT (Kim et al., 2021)
in multi-model models. More recently, ImageBind-
LLM (Han et al., 2023) focuses on enabling the
LLM to simultaneously process multiple modal in-
puts by pre-aligning each modality to a common
feature space (Girdhar et al., 2023). Based on a
large image-language model, ImageBind-LLM con-
verts other modalities into the most similar image
features by retrieving from a training-free image
cached database. However, the indirect alignment
approach of ImageBind-LLM may lead to perfor-
mance degradation, and the LLM has no knowledge
of actual video data.
In this work, we introduce Video-LLaV A , a sim-
ple but powerful baseline for the LVLM simulta-
neously handling both images and videos. Specifi-
cally, As shown in Figure 1, Video-LLaV A initially
aligns the representations of images and videos to
a unified visual feature space. Since the visual rep-
resentations are already aligned prior to projection,
we employ a shared projection layer to map the uni-
fied visual representation for the LLM. To enhance
computational efficiency, Video-LLaV A undergoes
joint training of images and videos, achieving re-
markable results with 1 training epoch.
As a result, The proposed Video-LLaV A greatly
enhances the ability of the LLM to simultaneously
understand both images and videos. For image
understanding, Video-LLaV A surpasses advanced
LVLMs such as mPLUG-owl-7B and InstructBLIP-
7B in 5 image benchmarks. Additionally, utilizing
4 benchmark toolkits for a more comprehensive
evaluation, Video-LLaV A-7B even outperforms
IDEFICS-80B by 6.4% in MMBench. Moreover,
similar trends can be observed in video under-
standing, where Video-LLaV A surpasses Video-
ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% re-
spectively on the MSVD, MSRVTT, TGIF, and
ActivityNet video question-answering datasets. Ex-tensive ablation experiments demonstrate that align-
ment before projection yields greater benefits. Ad-
ditionally, joint training of images and videos can
facilitate a unified visual representation in LLM
comprehension.
We summarize our primary contributions as fol-
lows:
•We introduce Video-LLaVA , a powerful
LVLM baseline. During the training process,
Video-LLaV A binds visual signals to the lan-
guage feature space, unifying visual represen-
tations, and proposes a solution to align before
projection. We enable an LLM to perform vi-
sual reasoning capabilities on both images and
videos simultaneously.
•Extensive experiments demonstrate that a uni-
fied visual representation benefits LLMs in
learning to simultaneously handle both im-
ages and videos, validating the complemen-
tarity of modalities, showcasing significant
superiority when compared to models specifi-
cally designed for either images or videos.
2 Related Work
2.1 Large Language Models
When the well-known commercial model Chat-
GPT (OpenAI, 2023) was introduced, the The AI
community released open-source Large Language
Models (LLMs) by instruction tuning and increas-
ing model sizes. These include LLaMA (Tou-
vron et al., 2023a), Vicuna (Chiang et al., 2023),
Alpaca (Taori et al., 2023), and more recently,
LLaMA 2 (Touvron et al., 2023b). These models
are tuned with instruction sets to emulate conversa-
tions between humans and AI assistants. Further-
more, InstructGPT (Ouyang et al., 2022) is trained
based on GPT-3 (Brown et al., 2020) with 175 bil-
lion parameters through aligning with human pref-
erences. However, LLMs can only interact within
text. In this work, we introduce Video-LLaV A,
which builds upon the powerful reasoning capa-
bilities of LLM to extend modality interactions to
images and videos.
2.2 Large Vision-Language Models
When extending LLMs to multi-modal, especially
involving images and videos, the main approaches
can be categorized into two types in Table 1: i)
treating LLM as a scheduler, ii)treating LLM as a
decoder.Table 1: Comparison between different Large Vision-Language Models. For methods that treat LLMs as
scheduler, they do not require pre-alignment and joint training.
Methods Image Video Pre-aligned Joint training
LLMs as scheduler
VisualChatGPT (Wu et al., 2023) ✔ ✗ - -
HuggingGPT (Shen et al., 2023) ✔ ✗ - -
MM-REACT (Yang et al., 2023) ✔ ✔ - -
ViperGPT (Surís et al., 2023) ✔ ✔ - -
LLMs as decoder
Mini-GPT4 (Zhu et al., 2023b) ✔ ✗ - ✗
LLaV A (Liu et al., 2023b) ✔ ✗ - ✗
Video-ChatGPT (Maaz et al., 2023) ✗ ✔ - ✗
VideoChat (Li et al., 2023c) ✔ ✔ ✗ ✔
Video-LLaMA (Zhang et al., 2023a) ✔ ✔ ✗ ✔
ImageBind-LLM (Han et al., 2023) ✔ ✔ ✔ ✗
Video-LLaVA (Ours) ✔ ✔ ✔ ✔
2.2.1 LLMs as scheduler
In the scheduler-based methods, various visual
models are treated as plug-and-play modules. LLM
schedules them according to the specific visual task
requirements, like the assembly of building blocks.
Some of these methods focus on images, such as
VisualChatGPT (Wu et al., 2023) and Hugging-
GPT (Shen et al., 2023), while MM-REACT (Yang
et al., 2023) and ViperGPT (Surís et al., 2023) can
also handle videos. A key characteristic of these
scheduler-based LVLMs is that they do not require
end-to-end training, hence eliminating the need for
pre-alignment and joint training of each modality.
2.2.2 LLMs as decoder
Regarding the approach of treating LLM as a de-
coder, this is our primary focus. MiniGPT-4 (Zhu
et al., 2023b) aligns image tokens to the input of
the large language model through several linear
projection layers. However, this alignment is weak
and lacks feedback from human instructions. Sub-
sequently, mPLUG-Owl (Ye et al., 2023) adopts
a two-stage training approach. In the first stage,
images are aligned with language using an auto-
regressive pretraining style, and the second stage
involves instruction tuning through using a human
instruction dataset. With the increasing scale of
large language model backends, approaches such
as InstructBLIP (Dai et al., 2023) and LLaV A se-
ries (Liu et al., 2023b,a; Lin et al., 2024) collecte
the larger human instruction datasets to train a
larger LVLMs (13B parameters). Each answer ofinstruction datasets strictly follow to the given in-
structions. Then they undergo end-to-end train-
ing using human instruction datasets, enabling the
LLM with visual reasoning capabilities. Moreover,
Video-ChatGPT (Maaz et al., 2023) design a 100k
video instruction dataset, successfully empower-
ing LLMs to comprehend videos. VideoChat (Li
et al., 2023c) and Video-LLaMA (Zhang et al.,
2023a) achieve this by conducting joint training,
allowing LLMs to simultaneously handle images
and videos. Expanding LLMs to additional visual
modalities typically requires pre-alignment, as seen
in LLaMA-Adapter (Zhang et al., 2023b; Gao et al.,
2023) and ImageBind-LLM (Han et al., 2023).
They bind other modalities to the image space
through ImageBind’s (Girdhar et al., 2023) modal-
ity encoder. These models have demonstrated that a
unified feature space is advantageous for enhancing
LLM’s multi-modal reasoning capabilities. Distin-
guished from prior work, Video-LLaV A not only
pre-aligns image and video features but also con-
ducts joint training of images and videos, facil-
itating LLMs in learning multi-modal reasoning
capabilities from a unified visual representation.
3 Video-LLaVA
3.1 Model Structure
3.1.1 Framework Overview
As shown in Figure 2, Video-LLaV A consists of
LanguageBind encoders fV(Zhu et al., 2023a) to
extract features from the raw visual signal (im-ages or videos), a large language model fLsuch
as Vicuna, visual projection layers fPand a word
embedding layer fT. We initially obtain visual
features using LanguageBind encoders. Language-
Bind encoders are capable of mapping different
modalities into the textual feature space, thereby
providing us with a unified visual representation.
Subsequently, the unified visual representation is
encoded by shared projection layers, which is then
combined with tokenized textual queries and fed
into a large language model to generate correspond-
ing responses.
3.1.2 United Visual Representation
Our goal is to map images and videos into a shared
feature space to enable the large language model
to learn from a unified visual representation. We
assume that the same information can be conveyed
through multiple media. For example, a running
dogcan be expressed through language, a image
or a video simultaneously. Therefore, we can com-
press information from different modalities into a
common feature space, allowing the model to ex-
tract information from a dense feature space, facili-
tating modality interactions and complementarity.
Hence, we chose the modality encoders from Lan-
guageBind (Zhu et al., 2023a), which align images
and videos with the textual feature space.
3.1.3 Alignment Before Projection
Specifically, LanguageBind initializes from Open-
CLIP (Ilharco et al., 2021), naturally aligning im-
ages and language in a shared feature space. Sub-
sequently, it aligns video representations to the
language space using 3 million video-text pairs
from VIDAL-10M (Zhu et al., 2023a). By shar-
ing a language feature space, the image and video
representations ultimately converge into a unified
visual feature space, which we refer to as emergent
alignment of images and videos. Therefore, our
video encoder and image encoder are initialized
from the LanguageBind encoders zoo, pre-aligning
the inputs for LLM and reducing the gap between
representations of different visual signals. The uni-
fied visual representation is fed into LLM after
passing through a shared projection layer.
3.2 Training Pipeline
Overall, the process of generating responses by
Video-LLaV A is similar to that of a large language
model (GPT series). Given a textual input XTand
visual signals XV, the input signals are encodedinto a sequence of tokens according to Equation 1.
By maximizing the likelihood probability in Equa-
tion 2, the model ultimately achieves multi-modal
understanding capabilities.
ZT=fT(XT),ZV=fP(fV(XV)) (1)
p(XA|XV,XT) =LY
i=1pθ
X[i]
A|ZV,Z[1:i−1]
T
(2)
where Lis the length of the generated sequence
XA, and θis a trainable parameter. We dynami-
cally conduct joint training on images and videos,
wherein a single batch contains both image and
video samples simultaneously.
3.2.1 Understanding Training
At this stage, the model is required to acquire the
ability to interpret visual signals within an exten-
sive image/video-text pair dataset. Each visual
signal corresponds to a single round of conversa-
tion data (Xq,Xa), where XT=XqandXais
the ground truth. The training objective of this
stage is the original auto-regressive loss, where the
model learns the basic ability to view the vision.
We freeze the other parameters of the model during
this process.
3.2.2 Instruction Tuning
In this stage, the model is required to provide
responses corresponding to different instructions.
These instructions often involve more complex
visual comprehension tasks, rather than just de-
scribing visual signals. Note that the conversation
data 
X1
q,X1
a,···,XN
q,XN
a
consists of multiple
rounds.
Xr
T=X1
q, r = 1
Concat (Xr−1
q,Xr−1
A,Xr
q), r > 1
(3)
where rrepresents the round number. As shown in
Equation 3, when r >1we concatenate the conver-
sations from all previous rounds with the current
instruction as the input for this round. The train-
ing objective remains the same as in the previous
stage. After this stage, the model learns to generate
corresponding responses based on different instruc-
tions and requests. The LLM are also involved in
training at this stage.Large Language Model
Vicuna v1.5
Word Embedding Layer Share Projection
Encoder Zoo
LanguageBind
Image VideoV V T T T T T V V V V V𝑓𝑓𝐋𝐋
𝑓𝑓𝐏𝐏 𝑓𝑓𝐖𝐖
𝑓𝑓𝐕𝐕Image Video
Are theimage and thevideo
depicting thesame place?
Yes, theimage andthevideo aredepicting thesame place .The video shows
thestatue ofliberty from different angles ,while theimage shows aclose- up
ofthestatue .Both thevideo andtheimage capture thebeauty andgrandeur
ofthestatue ofliberty .
(a) Illustration of Video-LLaV A (b) Performance comparison
Figure 2: Training framework and performance. Video-LLaV A exhibits remarkable interactive capabilities
between images and videos, despite the absence of image-video pairs in the dataset. (a) The Video-LLaV A
framework demonstrates a data flow that generates corresponding responses based on input instructions. (b) Video-
LLaV A achieves superior performances on a broad range of 15 datasets across image and video.
LAION -CC-SBU
558kValley
702k
LLaVA -mixed
665kVideo- ChatGPT
100kStage 1:  Understanding Pretraining
Stage 2:  Instruction Tuningconcise caption
multi- turn conversations / detailed caption / reasoning
Figure 3: Data composition for training Video-
LLaVA. The dataset for stage 1 consists of single-turn
conversation, focusing on concise visual descriptions.
In stage 2, the dataset comprises multi-turn conversa-
tions, emphasizing complex visual reasoning abilities.
4 Experiments
4.1 Experimental Setup
4.1.1 Data Details
In 3, for the first stage of understanding pretrain-
ing, we use a subset of 558K LAION-CC-SBU
image-text pairs with BLIP (Li et al., 2022) cap-
tions, which is sourced from CC3M (Sharma et al.,
2018) and filtered by LLaV A (Liu et al., 2023b).
The video-text pairs are derived from a subset pro-
vided by Valley (Luo et al., 2023), and we have
access to 702k out of a total of 703k pairs, originat-
ing from WebVid (Bain et al., 2021). For the stage
of instruction tuning, We gathered instructional
datasets from two sources, including a 665k image-
text instruction dataset from LLaV A 1.5 (Liu et al.,
2023a) and a 100k video-text instruction dataset
from Video-ChatGPT (Maaz et al., 2023).4.1.2 Model Settings
We employ Vicuna-7B v1.5 as the large language
model. The visual encoders are derived from Lan-
guageBind, initialized from OpenCLIP-L/14. The
text tokenizer is sourced from LLaMA, with ap-
proximately 32,000 classes. The share projection
layers consist of 2 fully connected layers with a
GeLU (Hendrycks and Gimpel, 2016) activated
function.
4.1.3 Training Details
In the training process, we resize and crop each
image, resulting in a size of 224 ×224 for each pro-
cessed image. We uniformly sample 8 frames from
each video, and each frame undergoes image pre-
processing. The data in each batch is a random
combination of images and videos. In the first
stage, we train for one epoch with a batch size of
256, using the AdamW optimizer with a cosine
learning rate schedule. In the second stage, we re-
duce the batch size to 128. The initial learning rate
for both stages is set to 1e-3, with a warmup ratio
of 0.03. Additional hyper-parameter settings can
be found in the appendix.
4.2 Quantitative Evaluation
4.2.1 Zero-shot Video Understanding
As shown in Table 2, we conduct a quantitative
assessment of the video question-answering ca-Table 2: Comparison between different LVLMs on video reasoning benchmarks . We employ ChatGPT-
Assistant to evaluate the performance following Video-ChatGPT (Maaz et al., 2023). The version of ChatGPT is
“gpt-3.5-turbo”.
MethodsLLM MSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA
size Accuracy Score Accuracy Score Accuracy Score Accuracy Score
FrozenBiLM 1B 32.2 - 16.8 - 41.0 - 24.7 -
VideoChat 7B 56.3 2.8 45.0 2.5 34.4 2.3 - 2.2
LLaMA-Adapter 7B 54.9 3.1 43.8 2.7 - - 34.2 2.7
Video-LLaMA 7B 51.6 2.5 29.6 1.8 - - 12.4 1.1
Video-ChatGPT 7B 64.9 3.3 49.3 2.8 51.4 3.0 35.2 2.7
Chat-UniVi 7B 65.0 3.6 54.6 3.1 60.3 3.4 45.8 3.2
Video-LLaVA 7B 70.7 3.9 59.2 3.5 70.0 4.0 45.3 3.3
Table 3: Comparison between different LVLMs on image understanding benchmarks. “Res.”, “L”, “V”
respectively represent the input image resolution, LLaMA (Touvron et al., 2023a) and Vicuna (Chiang et al., 2023).
Benchmark names are abbreviated due to page limitations. VQA-v2 (Goyal et al., 2017); GQA (Hudson and
Manning, 2019); VisWiz (Gurari et al., 2018); SQAI: ScienceQA-IMG (Lu et al., 2022); VQAT: TextVQA (Singh
et al., 2019); POPE (Li et al., 2023d); MMB: MMBench (Liu et al., 2023c); LLaV AW: LLaV A-Bench (In-the-
Wild) (Liu et al., 2023b); MM-Vet (Yu et al., 2023).†donates that we reproduce LLaV A-1.5 with LanguageBind-
Image encoder to compare fairly.∗donates that there is some overlap in the training data.
Methods LLM Res.Image Question Answering Benchmark Toolkit
VQAv2GQA VisWiz SQAIVQATPOPE MMB LLaV AWMM-Vet
LLaV A-1.5 V-7B 336 - 62.0∗- - - - - - 30.5
BLIP-2 V-13B 224 41.0 41.0 19.6 61.0 42.5 85.3 - 38.1 22.4
InstructBLIP V-13B 224 - 49.5 33.4 63.1 50.7 78.9 - 58.2 25.6
IDEFICS-80B L-65B 224 60.0 45.2 36.0 - 30.9 - 54.5 - -
MiniGPT-4 L-7B 224 - 30.8 47.5 25.4 19.4 - 23.0 - 22.1
IDEFICS-9B L-7B 224 50.9 38.4 35.5 - 25.9 - 48.2 - -
mPLUG-Owl L-7B 224 - 14.0 39.0 2.8 38.8 - 46.6 - -
Otter L-7B 224 - 38.1 50.0 27.2 21.2 - 32.6 - 24.6
InstructBLIP V-7B 224 - 49.2 34.5 60.5 50.1 - 36.0 60.9 26.2
LLaV A-1.5†V-7B 224 72.3∗56.9∗47.8 67.9 49.2 83.3 59.5 63.3 25.7
Video-LLaVA V-7B 224 74.7∗60.3∗48.1 66.4 51.8 84.4 60.9 73.1 32.0
pabilities of large video-language models on four
datasets, including MSVD-QA (Chen and Dolan,
2011), MSRVTT-QA (Xu et al., 2016), TGIF-
QA (Jang et al., 2017) and ActivityNet-QA (Yu
et al., 2019). The evaluation pipeline for video
understanding follows Video-ChatGPT. We report
the accuracy and score, which is assessed using
GPT-Assistant. Video-LLaV A consistently out-
performs Video-ChatGPT in terms of question-
answering accuracy, which is an advanced large
video-language model. Moreover, Video-LLaV A
surpasses the powerful baseline of Video-ChatGPT
by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT,
MSVD, TGIF, and ActivityNet, respectively. Addi-
tionally, we conduct comparisons with the recentSOTA model, Chat-UniVi (Jin et al., 2023). De-
spite Chat-UniVi utilizing more datasets such as
MIMIC-IT (Li et al., 2023a), Video-LLaV A still
demonstrate competitive results, surpassing Chat-
UniVi on MSVD, MSRVTT, and TGIF datasets.
In summary, these results validate Video-LLaV A’s
ability to comprehend videos and provide contex-
tually appropriate responses based on instructions.
4.2.2 Zero-shot Image Question-answering
As shown in Table 3, we evaluate our approach
for image understanding on five academic image
question-answering benchmarks. Compared to
the state-of-the-art model InstructBLIP-7B, Video-
LLaV A demonstrates powerful image understand-Table 4: Zero-shot object hallucination evaluation results are reported for three POPE evaluation settings. “Yes”
indicates the proportion of positive responses to the given question.†donates that we reproduce LLaV A-1.5 with
LanguageBind-Image encoder to compare fairly.
Methods LLMAdersarial Popular Random
Accuracy F1-Score Yes Accuracy F1-Score Yes Accuracy F1-Score Yes
MiniGPT-4 V-13B 66.6 71.4 66.7 68.3 72.2 64.1 77.8 78.9 54.8
InstructBLIP V-13B 74.4 78.5 69.0 81.4 83.5 62.6 88.7 89.3 55.2
MM-GPT L-7B 50.0 66.7 100.0 50.0 66.7 100.0 50.0 66.7 100.0
mPLUG-Owl L-7B 50.7 66.8 98.7 50.9 66.9 98.6 54.0 66.4 95.6
Chat-UniVi V-7B 55.6 68.7 91.6 56.4 69.0 90.8 73.9 79.3 74.6
LLaV A-1.5†L-7B 84.3 83.2 43.5 79.8 79.4 48.0 85.7 84.8 43.0
Video-LLaVA V-7B 81.6 80.8 45.8 85.3 84.0 42.1 86.2 85.2 42.0
ing capabilities, outperforming across all five
question-answering benchmarks. Additionally,
Video-LLaV A exhibits competitive results com-
pared to several more powerful LVLMs, which
are tuned based on 13B or 65B LLM, such as sur-
passing InstructBLIP-13B by 14.7% on VisWiz,
highlighting its strong understanding ability in nat-
ural visual environments. Furthermore, to ensure a
fair comparison, we replace the image encoder in
LLaV A-1.5 with the LanguageBind-Image encoder,
called LLaV A-1.5†. This demonstrates that the per-
formance improvement observed in Video-LLaV A
is not solely attributed to a stronger image encoder.
Additional details can be found in Section 4.3.6.
Evaluation under Image Benchmark Toolkits
Additionally, we evaluate LVLMs using several
benchmark toolkits for visual instruction tuning.
These benchmark toolkits provide a detailed as-
sessment of the model’s capabilities through ro-
bust evaluation metrics. Video-LLaV A outperform
InstructBLIP-7B by 24.9%, 12.2%, and 5.8% on
MMBench, LLaV A-Bench, and MM-Vet, respec-
tively. It is worth noting that Video-LLaV A-7B
still demonstrates advanced performance compared
to larger LLM models, surpassing InstructBLIP-
13B by 6.4% on MM-Vet and IDEFICS-80B (Lau-
rençon et al., 2023) by 6.4% on MMBench. These
results demonstrate that Video-LLaV A exhibits a
strong understanding of semantic aspects of scenes,
enabling it to answer open-ended and free-form
natural language questions about images.
4.2.3 Object Hallucination Evaluation
As shown in Table 4, we report evaluation results
for zero-shot object hallucinations, utilizing a eval-
uation pipeline derived from a polling-based query
method (Li et al., 2023d). Video-LLaV A demon-strates competitive performance across three sub-
sets: random, popular, and adversarial. Specifically,
when compared to the 7B foundation model, Video-
LLaV A consistently outperforms MM-GPT (Gong
et al., 2023) across all three POPE hallucination
evaluation subsets. Furthermore, when bench-
marked against the larger 13B LLM, Video-LLaV A
even surpasses Mini-GPT4 comprehensively. The
successful performance of Video-LLaV A in object
hallucination detection validates the consistency
between unified visual representations and the gen-
eration of textual descriptions.
4.3 Ablation Results
4.3.1 Alignment Before Projection
To validate the performance degradation caused by
separated visual representation, we conduct exper-
iments to to explore the performance of the LLM
learning from different visual representations. We
define the use of LanguageBind image encoder as
unified visual representation while the MAE en-
coder (He et al., 2022) use separated visual rep-
resentation, which is a well-known and effective
image feature extractor. Additionally, since MAE
do not interact with multi-modal inputs during the
training process, we utilize CLIP-L/14, a model of
the same size. While CLIP-L/14 exhibits strong
multimodal understanding capabilities, it is not pre-
aligned with the video encoder. Consequently, this
results in a lack of uniformity in the visual features
provided to LLM. We only replace the image en-
coder of the same scale and keep the LanguageBind
video encoder.
4.3.2 For Video Understanding
Due to replacing the image encoder with the MAE
encoder, the video features and image features areTable 5: Effect of alignment before projection on image. “United” refers to the unified visual representation,
while “Separated” refers to the separated visual representation. Benchmark names are abbreviated due to page
limitations.
MethodsImage Question Answering Benchmark Toolkit
VQAv2GQA VisWiz SQAIVQATPOPE MMB LLaV AWMM-Vet
Separated-MAE 66.0 55.4 42.5 65.0 44.2 80.8 45.7 35.9 20.0
Separated-CLIP 74.6 59.9 47.8 67.3 51.5 84.4 60.2 68.9 30.6
United 74.7 60.3 48.1 66.4 51.8 84.4 60.9 73.1 32.0
∆Acc. +0.1 +0.4 +0.3 -0.9 +0.3 +0.0 +0.7 +4.2 +1.4
Table 6: Effect of joint training on video. We evaluate on four video question-answering datasets.∗denotes that
we utilized only video data in both the first and second stages.
MethodsMSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA
Accuracy Score Accuracy Score Accuracy Score Accuracy Score
Video-LLaV A∗64.8 3.2 58.3 3.4 67.8 3.4 40.7 2.0
Joint with Image 70.7 3.9 59.2 3.5 70.0 4.0 45.3 3.3
∆Acc. +5.9 +0.7 +0.9 +0.1 +2.2 +0.6 +4.6 +1.3
/uni00000030/uni00000036/uni00000039/uni00000027 /uni00000037/uni0000002a/uni0000002c/uni00000029/uni00000030/uni00000036/uni00000035/uni00000039/uni00000037/uni00000037 /uni00000024/uni00000046/uni00000057/uni0000004c/uni00000059/uni0000004c/uni00000057/uni0000005c/uni00000031/uni00000048/uni00000057
/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000054/uni00000058/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000044/uni00000051/uni00000056/uni0000005a/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000036/uni00000048/uni00000053/uni00000044/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000010/uni00000026/uni0000002f/uni0000002c/uni00000033
/uni00000036/uni00000048/uni00000053/uni00000044/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000010/uni00000030/uni00000024/uni00000028
/uni00000038/uni00000051/uni0000004c/uni00000057/uni00000048/uni00000047
/uni00000030/uni00000036/uni00000039/uni00000027/uni00000003 /uni00000037/uni0000002a/uni0000002c/uni00000029/uni00000003/uni00000030/uni00000036/uni00000035/uni00000039/uni00000037/uni00000037/uni00000003 /uni00000024/uni00000046/uni00000057/uni0000004c/uni00000059/uni0000004c/uni00000057/uni0000005c/uni00000031/uni00000048/uni00000057/uni00000003
/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000054/uni00000058/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000044/uni00000051/uni00000056/uni0000005a/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013/uni00000017/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000036/uni00000048/uni00000053/uni00000044/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000010/uni00000026/uni0000002f/uni0000002c/uni00000033
/uni00000036/uni00000048/uni00000053/uni00000044/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000010/uni00000030/uni00000024/uni00000028
/uni00000038/uni00000051/uni0000004c/uni00000057/uni00000048/uni00000047
Figure 4: Effect of alignment before projection on
video. We validate and report the accuracy and score on
four video question-answering datasets.
no longer unified during LLM’s initial learning of
visual representations. In Figure 4, compared to
separated visual representation, the united visual
representation significantly improves performance
across 4 video question-answering datasets. Sepa-
rated visual representations not only exhibit lower
accuracy in question-answering, but also demon-
strate a similar trend in answer scores. These re-
sults demonstrate that the unified visual representa-
tion can help the LLM further learn and understand
videos.
4.3.3 For Image Understanding
The unified visual representation demonstrates
strong performance, surpassing the separated vi-
sual representation comprehensively across 5 im-
age question-answering datasets and 4 benchmark
toolkits in Table 5. Additionally, we observe a
significant margin of performance improvement
in the unified visual representation on the MM-
/uni00000039/uni00000034/uni00000024/uni00000003/uni00000039/uni00000015/uni0000002a/uni00000034/uni00000024 /uni00000039/uni0000004c/uni00000056/uni0000003a/uni0000004c/uni0000005d/uni00000036/uni00000034/uni00000024/uni00000010/uni0000002c/uni00000030/uni0000002a /uni00000037/uni00000048/uni0000005b/uni00000057/uni00000039/uni00000034/uni00000024/uni00000033/uni00000032/uni00000033/uni00000028/uni00000030/uni00000030/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni0000002f/uni0000002f/uni00000044/uni00000039 /uni00000024/uni00000010/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000030/uni00000030/uni00000010/uni00000039/uni00000048/uni00000057/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni0000002f/uni0000002f/uni00000044/uni00000039/uni00000024/uni00000010/uni00000014/uni00000011/uni00000018
/uni00000039/uni0000004c/uni00000047/uni00000048/uni00000052/uni00000010/uni0000002f/uni0000002f/uni00000044/uni00000039 /uni00000024Figure 5: Effect of joint training on image.†donates
that We reproduce the results of LLaV A-1.5 at a resolu-
tion of 224 ×224 with LanguageBind-Image encoder for
a fair comparison.
Bench, LLaV A-Bench, and MM-Vet benchmark
toolkits. This highlights that the unified visual rep-
resentation not only enhances performance in im-
age question-answering but also provides benefits
in other aspects of image understanding, such as
reducing object hallucination and improving OCR
capabilities.
4.3.4 Joint Training
This subsection aims to validate the complemen-
tarity of images and videos during joint training,
which can mutually enhance the LLM’s understand-
ing of images and videos based on a unified visual
representation.
4.3.5 For Video Understanding
For comparing performance on video benchmarks,
we remove image data during the training of Video-
LLaV A, which is called Video-LLaV A∗. We com-pare with Video-LLaV A∗to assess the performance
gains from joint image training on video bench-
marks. In Table 6, we evaluate our model on
four video question-answering datasets. Com-
pared to Video-LLaV A∗without image in training,
the model trained with joint images and videos
achieves comprehensive improvements across all
four video datasets. These results demonstrate
that joint training of images and videos facilitates
LLM’s understanding of visual representations.
4.3.6 For Image Understanding
When comparing performance on image bench-
marks, it is challenging to find a image-based
LVLM with the same configuration as Video-
LLaV A. To address this, we replace the image en-
coder in LLaV A-1.5 with the LanguageBind-Image
encoder and reproduce the results at a resolution
of 224 ×224 by using the same training configura-
tion, called LLaV A-1.5†. As shown in Figure 5,
Compared to LLaV A-1.5†, which utilizes the same
image encoder configuration, we observe perfor-
mance improvements in 8 out of 9 benchmarks,
demonstrating mutual improvement in visual under-
standing. Video-LLaV A outperform LLaV A-1.5†
in POPE, indicating that joint training with videos
alleviates the object hallucination in images. The
similar trend is observed on some other benchmark
toolkits, such as LLaV A-Bench and MMBench,
where video data significantly improves LLM’s
performance in complex reasoning and image con-
versation tasks.
5 Limitation and Future Directions
5.1 Limitation
While Video-LLaV A exhibits strong competitive-
ness in both images and videos, we still observed
some limitations of Video-LLaV A. To begin with,
Video-LLaV A performs moderately in understand-
ing long videos. In Table 2, Chat-UniVi surpasses
0.5 on ActivityNet-QA because Video-LLaV A only
utilizes uniformly sampled 8 frames to comprehend
the video, which results in the loss of detailed in-
formation from long videos. Additionally, training
Video-LLaV A is computationally expensive, requir-
ing 3-4 days to complete the training process on 8
A100-80G GPUs.
5.2 Future Directions
In the future, We maybe can explore more efficient
shared projection mode that can compress tokenswhile preserving data features. This would support
Video-LLaV A in better understanding long videos.
Besides, Video-LLaV A can serve as a baseline to
extend to additional visual-related modalities, such
as depth and infrared images. Additionally, we
could explore how to incorporate timestamp embed-
dings effectively, enabling large visual-language
models to answer questions related to temporal re-
lationships.
6 Conclusion
In this work, we introduce Video-LLaV A, a simple
but powerful large visual-language baseline model.
We propose a novel framework to address the is-
sue of misalignment before projection, utilizing a
LanguageBind encoder to pre-bind visual signals
into the language feature space. To enable a LLM
to comprehend both images and videos simulta-
neously, we conduct joint training on images and
videos, allowing the LLM to learn multi-modal
interactions from a unified visual representation.
Extensive experiments demonstrate that joint train-
ing on images and videos mutually benefits per-
formance. Furthermore, we validate that aligning
visual representations before projection aids LLM
learning. Remarkably, LLM, after learning from a
unified visual representation, exhibits the remark-
able ability to simultaneously engage with both
images and videos, showcasing a powerful com-
prehension of unified visual concepts. These re-
sults collectively demonstrate the effectiveness of
the Video-LLaV A training framework. As a uni-
fied visual training framework, the performance of
Video-LLaV A even surpasses that of expert models
designed specifically for images or videos.
Acknowledgments
This work was supported in part by the Natu-
ral Science Foundation of China (No. 62202014,
62332002, 62425101), Shenzhen Basic Research
Program (No.JCYJ20220813151736001).References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in Neural
Information Processing Systems , 35:23716–23736.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305.10403 .
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zis-
serman. 2021. Frozen in time: A joint video and
image encoder for end-to-end retrieval. In Proceed-
ings of the IEEE/CVF International Conference on
Computer Vision , pages 1728–1738.
Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang,
Songfang Huang, Fei Huang, and Luo Si. 2020.
Palm: Pre-training an autoencoding&autoregressive
language model for context-conditioned generation.
arXiv preprint arXiv:2004.07159 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
David Chen and William B Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In
Proceedings of the 49th annual meeting of the associ-
ation for computational linguistics: human language
technologies , pages 190–200.
Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang
Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023. X-
llm: Bootstrapping advanced large language models
by treating multi-modalities as foreign languages.
arXiv preprint arXiv:2305.04160 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.
2023. Vicuna: An open-source chatbot impressing
gpt-4 with 90%* chatgpt quality. See https://vicuna.
lmsys. org (accessed 14 April 2023) .
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi.
2023. Instructblip: Towards general-purpose vision-
language models with instruction tuning. Preprint ,
arXiv:2305.06500.
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,
Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.
2023. Mme: A comprehensive evaluation bench-
mark for multimodal large language models. arXiv
preprint arXiv:2306.13394 .Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui
He, Xiangyu Yue, et al. 2023. Llama-adapter v2:
Parameter-efficient visual instruction model. arXiv
preprint arXiv:2304.15010 .
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-
nat Singh, Kalyan Vasudev Alwala, Armand Joulin,
and Ishan Misra. 2023. Imagebind: One embed-
ding space to bind them all. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 15180–15190.
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang,
Ping Luo, and Kai Chen. 2023. Multimodal-gpt: A
vision and language model for dialogue with humans.
arXiv preprint arXiv:2305.04790 .
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv
Batra, and Devi Parikh. 2017. Making the v in vqa
matter: Elevating the role of image understanding
in visual question answering. In Proceedings of the
IEEE conference on computer vision and pattern
recognition , pages 6904–6913.
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,
Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P
Bigham. 2018. Vizwiz grand challenge: Answering
visual questions from blind people. In Proceedings of
the IEEE conference on computer vision and pattern
recognition , pages 3608–3617.
Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao,
Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu,
Song Wen, Ziyu Guo, et al. 2023. Imagebind-llm:
Multi-modality instruction tuning. arXiv preprint
arXiv:2309.03905 .
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-
otr Dollár, and Ross Girshick. 2022. Masked autoen-
coders are scalable vision learners. In Proceedings
of the IEEE/CVF conference on computer vision and
pattern recognition , pages 16000–16009.
Dan Hendrycks and Kevin Gimpel. 2016. Gaus-
sian error linear units (gelus). arXiv preprint
arXiv:1606.08415 .
Drew A Hudson and Christopher D Manning. 2019.
Gqa: A new dataset for real-world visual reasoning
and compositional question answering. In Proceed-
ings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 6700–6709.
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman,
Cade Gordon, Nicholas Carlini, Rohan Taori, Achal
Dave, Vaishaal Shankar, Hongseok Namkoong, John
Miller, Hannaneh Hajishirzi, Ali Farhadi, and Lud-
wig Schmidt. 2021. Openclip. If you use this soft-
ware, please cite it as below.
Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim,
and Gunhee Kim. 2017. Tgif-qa: Toward spatio-
temporal reasoning in visual question answering. In
Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 2758–2766.Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun
Cao, and Li Yuan. 2023. Chat-univi: Unified vi-
sual representation empowers large language models
with image and video understanding. arXiv preprint
arXiv:2311.08046 .
Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:
Vision-and-language transformer without convolu-
tion or region supervision. In International Con-
ference on Machine Learning , pages 5583–5594.
PMLR.
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas
Bekman, Amanpreet Singh, Anton Lozhkov, Thomas
Wang, Siddharth Karamcheti, Alexander M. Rush,
Douwe Kiela, Matthieu Cord, and Victor Sanh.
2023. Obelics: An open web-scale filtered dataset
of interleaved image-text documents. Preprint ,
arXiv:2306.16527.
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. 2023a. Otter: A
multi-modal model with in-context instruction tuning.
arXiv preprint arXiv:2305.03726 .
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023b. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. arXiv preprint arXiv:2301.12597 .
Junnan Li, Dongxu Li, Caiming Xiong, and Steven
Hoi. 2022. Blip: Bootstrapping language-image pre-
training for unified vision-language understanding
and generation. In International Conference on Ma-
chine Learning , pages 12888–12900. PMLR.
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
Shafiq Joty, Caiming Xiong, and Steven Chu Hong
Hoi. 2021. Align before fuse: Vision and language
representation learning with momentum distillation.
Advances in neural information processing systems ,
34:9694–9705.
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen-
hai Wang, Ping Luo, Yali Wang, Limin Wang, and
Yu Qiao. 2023c. Videochat: Chat-centric video un-
derstanding. arXiv preprint arXiv:2305.06355 .
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,
Wayne Xin Zhao, and Ji-Rong Wen. 2023d. Eval-
uating object hallucination in large vision-language
models. arXiv preprint arXiv:2305.10355 .
Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu,
Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan.
2024. Moe-llava: Mixture of experts for large vision-
language models. arXiv preprint arXiv:2401.15947 .
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2023a. Improved baselines with visual instruc-
tion tuning. arXiv preprint arXiv:2310.03744 .
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023b. Visual instruction tuning. arXiv preprint
arXiv:2304.08485 .Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi
Wang, Conghui He, Ziwei Liu, et al. 2023c. Mm-
bench: Is your multi-modal model an all-around
player? arXiv preprint arXiv:2307.06281 .
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-
Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
Clark, and Ashwin Kalyan. 2022. Learn to explain:
Multimodal reasoning via thought chains for science
question answering. Advances in Neural Information
Processing Systems , 35:2507–2521.
Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong,
Minghui Qiu, Pengcheng Lu, Tao Wang, and
Zhongyu Wei. 2023. Valley: Video assistant with
large language model enhanced ability. arXiv
preprint arXiv:2306.07207 .
Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting
Huang, Bingshuai Liu, Zefeng Du, Shuming Shi,
and Zhaopeng Tu. 2023. Macaw-llm: Multi-modal
language modeling with image, audio, video, and
text integration. arXiv preprint arXiv:2306.09093 .
Muhammad Maaz, Hanoona Rasheed, Salman Khan,
and Fahad Shahbaz Khan. 2023. Video-chatgpt:
Towards detailed video understanding via large
vision and language models. arXiv preprint
arXiv:2306.05424 .
OpenAI. 2023. Gpt-4 technical report. Preprint ,
arXiv:2303.08774.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Piyush Sharma, Nan Ding, Sebastian Goodman, and
Radu Soricut. 2018. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic im-
age captioning. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2556–2565.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
gpt: Solving ai tasks with chatgpt and its friends in
huggingface. arXiv preprint arXiv:2303.17580 .
Amanpreet Singh, Vivek Natarajan, Meet Shah,
Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,
and Marcus Rohrbach. 2019. Towards vqa models
that can read. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition ,
pages 8317–8326.Dídac Surís, Sachit Menon, and Carl V ondrick. 2023.
Vipergpt: Visual inference via python execution for
reasoning. arXiv preprint arXiv:2303.08128 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Chenfei Wu, Shengming Yin, Weizhen Qi, Xi-
aodong Wang, Zecheng Tang, and Nan Duan.
2023. Visual chatgpt: Talking, drawing and edit-
ing with visual foundation models. arXiv preprint
arXiv:2303.04671 .
Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
video and language. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition ,
pages 5288–5296.
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin
Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,
Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-
react: Prompting chatgpt for multimodal reasoning
and action. arXiv preprint arXiv:2303.11381 .
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,
Ming Yan, Yiyang Zhou, Junyang Wang, An-
wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.
mplug-owl: Modularization empowers large lan-
guage models with multimodality. arXiv preprint
arXiv:2304.14178 .
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing
Sun, Tong Xu, and Enhong Chen. 2023. A survey on
multimodal large language models. arXiv preprint
arXiv:2306.13549 .
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan
Wang. 2023. Mm-vet: Evaluating large multimodal
models for integrated capabilities. arXiv preprint
arXiv:2308.02490 .
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yuet-
ing Zhuang, and Dacheng Tao. 2019. Activitynet-qa:
A dataset for understanding complex web videos via
question answering. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , volume 33, pages
9127–9134.Hang Zhang, Xin Li, and Lidong Bing. 2023a. Video-
llama: An instruction-tuned audio-visual language
model for video understanding. arXiv preprint
arXiv:2306.02858 .
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and
Yu Qiao. 2023b. Llama-adapter: Efficient fine-tuning
of language models with zero-init attention. arXiv
preprint arXiv:2303.16199 .
Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui,
HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu
Zhang, Zongwei Li, et al. 2023a. Languagebind: Ex-
tending video-language pretraining to n-modality by
language-based semantic alignment. arXiv preprint
arXiv:2310.01852 .
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023b. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. arXiv preprint arXiv:2304.10592 .A Example Appendix
A.1 Training Setting
We show some training settings as shown in Table 7.
video encoder and image encoder are not trained
in both stages. The projection layer consists of 2
linear layers with a GeLU (Hendrycks and Gimpel,
2016) activation function between them. Image
and video share the projection layer.
Table 7: Training setting.
Config Pretraining Instruction tuning
Video encoder LanguageBind-Video-LoRA
Image encoder LanguageBind-Image
Optimizer AdamW
Deepspeed Zero2
Epochs 1
Vision select layer -2
Weight decay 0.0
Warmup ratio 0.03
Learning rate schedule cosine decay
Learning rate 1e-3 2e-5
Batch size 256 128
A.2 Exhibition Board
We show some unselected samples here, and these
videos are sourced from Video-ChatGPT (Maaz
et al., 2023).
The video shows a man playing the drums in a room. He is seen playing the drums with 
his hands and sticks, and he continues to play while looking off into the distance.[Input Instruction] Describe the activity in the video.
The video shows a young girl performing a gymnastics routine on a balance beam. She executes several flips and tricks before dismounting and landing on a mat.[Input Instruction] Describe thevideo .
The video features a woman grooming a black horse in a stable. She brushes the horse's 
hair and cleans its hooves, and the horse is seen standing calmly throughout the process. 
The video also includes shots of the woman speaking to the camera and the horse.[Input Instruction] Briefly describe the video.
The main challenge faced by the people on the boat is the rough sea conditions. They are seen struggling to stay upright and maintain their balance as the boat sways back and forth.[Input Instruction] What is the main challenge faced by the people on the boat?
The video is about a man who is seen spinning and throwing a discus in a field. He then walks away while people measure his throw.[Input Instruction] What isthevideo about?
Sure! The man in the video demonstrates how to clean a screen window by first wiping it down with a rag. He then applies a cleaning solution to the screen and wipes it down again. Finally, he shows how to dry the screen with a towel.[Input Instruction] Can you describe theprocess forcleaning ascreen
window asdemonstrated inthevideo?
The woman and the dog are playing frisbee in the yard. The woman throws the frisbee, and the dog chases after it.[Input Instruction] What activities arethewoman andthedogengaged in
thevideo?
The video shows a person cooking a dish of noodles. They start by boiling water in a pot 
and adding noodles to it. Then, they add oil and some ingredients to the pot. After that, 
they mix the noodles and the ingredients together. Finally, they serve the dish on a plate.[Input Instruction] Step-by-step describe thecooking recipe shown inthe
video .
The video shows two men arm wrestling on a table while being watched by a group of 
people.[Input Instruction] What is the video about?
[Input Instruction] Isthere aflaginthebackground?
Yes, there is a flag in the background of the video.
The video is taken from the top of the Statue of Liberty, which is a famous landmark in 
New York City. The statue is shown from different angles, including from the water and from the top of the statue itself.[Input Instruction] Where isthisvideo taken from? What place/landmark
isshown inthevideo?
Figure 6: Samples of Video-LLaV A in video under-
standing.
The video is strange because it shows a man sitting next to a lion and a lion laying down 
next to a man. The man is also seen petting a lion in the grass.[Input Instruction] What isstrange about thisvideo?
The video isfunny because thebaby isplaying with aWiiremote, which isnotatoy
typically associated with babies. Thebaby's actions arealso humorous, such aspointing
tothecamera andmaking faces, which adds totheoverall amusement ofthevideo.[Input Instruction] Why is this video  funny?
The unusual aspect of this video is that the giraffe is seen swimming in a pool, which is 
not a common sight. The giraffe is also seen jumping off a diving board and swimming 
underwater, which adds to the uniqueness of the video.[Input Instruction] What isunusual about thisvideo?
Yes, the baby is playing with a toy truck and a toy car, which could pose a safety risk if they are not supervised. However, the baby is sitting on the floor, so there is less risk of injury compared to if they were playing with the toys on a higher surface.[Input Instruction] Canyouidentify anysafety hazards inthisvideo?
Sure, here's a short poem inspired by the video: The flowers bloom, a gentle grace, Their petals soft, their colors trace. 
Their beauty, a sight to behold, A moment of peace, a moment to hold. 
Their colors, a symphony of hues, A celebration of life, and all it can do. 
Their beauty, a reminder to cherish, 
The simple things, that bring us happiness. So let us take a moment to appreciate, 
The beauty of flowers, and all they can create.[Input Instruction] Canyouwrite ashort poem inspired from thevideo .
Once upon atime, inakingdom faraway, there lived abeautiful princess named Rose.
She was known forherkindness and beauty, and everyone loved her. One day, a
handsome prince named Leo came tovisit thekingdom .Hewas fascinated byRose's
beauty anddecided tostay inthekingdom .However, Rose's jealous sister, Lily, didnot
liketheattention Leo was giving hersister. Shetried toharm Rose, butLeo saved her
andthey fellinlove. Thekingdom celebrated their love andthey lived happily ever after.
Themoral ofthestory isthattruelove conquers allandthatjealousy canlead toharmful
consequences.[Input Instruction] Create ashort fairy talewith amoral lesson inspired
bythevideo .
The video starts with a car covered in snow, and a person is seen using a scraper to remove the snow from the car's windows. The person is seen scraping the snow off the 
windshield and the side windows of the car. The person continues to scrape the snow off the car, making sure that all the snow is removed. The video then shows the person 
wiping the snow off the car's windshield and side windows. The person is seen using a 
cloth to wipe the snow off the car's windows, making sure that the car is completely 
clean. The video ends with the person finishing the job and the car being shown off. 
Overall, the video shows a person removing snow from a car's windows and windshield, and then wiping the snow off the car's windows to make sure it is completely clean.[Input Instruction] Describe thevideo indetail.
The video shows a person preparing a dish with peppers and oranges. They slice the peppers and oranges and lay them out on a table. The person then proceeds to cut the 
peppers and oranges into strips and lays them out on a piece of wax paper. They use a 
knife to cut the peppers and oranges into strips and lay them out on a piece of wax paper. 
The person then uses a spoon to mix the peppers and oranges together. The video ends 
with the person cutting the peppers and oranges into strips and laying them out on a piece of wax paper.[Input Instruction] Describe thevideo .
The main focus of the video is the process of creating a painting of a tree using various techniques and colors. The video shows the artist's hand holding a paintbrush and using it 
to create the tree, as well as the use of different colors and techniques to create the 
painting.[Input Instruction] What isthemain focus ofthevideo .
The woman uses an ax to split the logs. She holds the ax in her hand and swings it down onto the log. She repeats this process several times, splitting the log into smaller pieces.[Input Instruction] What does thewoman usetosplit thelogs andhow does
shedoit?
In the video, we see a man walking down a track while holding a javelin. He then runs 
down the track and throws the javelin. The video then shows the man walking back to 
the starting point, and we see the javelin landing on the ground. The man then walks away from the camera. From the video, we can see that the man is a javelin thrower, and 
he is practicing his throws. The track appears to be well-maintained, and the 
surroundings seem to be quiet and peaceful. The man's form and technique are 
impressive, and it is clear that he has experience in the sport. Overall, the video provides 
a glimpse into the world of javelin throwing and the dedication and skill required to excel in the sport.[Input Instruction] Describe thevideo indetail.
Figure 7: Samples of Video-LLaV A in video under-
standing.The video is strange because it shows a man sitting next to a lion and a lion laying down 
next to a man. The man is also seen petting a lion in the grass.[Input Instruction] What isstrange about thisvideo?
The video isfunny because thebaby isplaying with aWiiremote, which isnotatoy
typically associated with babies. Thebaby's actions arealso humorous, such aspointing
tothecamera andmaking faces, which adds totheoverall amusement ofthevideo.[Input Instruction] Why is this video  funny?
The unusual aspect of this video is that the giraffe is seen swimming in a pool, which is 
not a common sight. The giraffe is also seen jumping off a diving board and swimming 
underwater, which adds to the uniqueness of the video.[Input Instruction] What isunusual about thisvideo?
Yes, the baby is playing with a toy truck and a toy car, which could pose a safety risk if they are not supervised. However, the baby is sitting on the floor, so there is less risk of injury compared to if they were playing with the toys on a higher surface.[Input Instruction] Canyouidentify anysafety hazards inthisvideo?
Sure, here's a short poem inspired by the video: The flowers bloom, a gentle grace, Their petals soft, their colors trace. 
Their beauty, a sight to behold, A moment of peace, a moment to hold. 
Their colors, a symphony of hues, A celebration of life, and all it can do. 
Their beauty, a reminder to cherish, 
The simple things, that bring us happiness. So let us take a moment to appreciate, 
The beauty of flowers, and all they can create.[Input Instruction] Canyouwrite ashort poem inspired from thevideo .
Once upon atime, inakingdom faraway, there lived abeautiful princess named Rose.
She was known forherkindness and beauty, and everyone loved her. One day, a
handsome prince named Leo came tovisit thekingdom .Hewas fascinated byRose's
beauty anddecided tostay inthekingdom .However, Rose's jealous sister, Lily, didnot
liketheattention Leo was giving hersister. Shetried toharm Rose, butLeo saved her
andthey fellinlove. Thekingdom celebrated their love andthey lived happily ever after.
Themoral ofthestory isthattruelove conquers allandthatjealousy canlead toharmful
consequences.[Input Instruction] Create ashort fairy talewith amoral lesson inspired
bythevideo .
The video starts with a car covered in snow, and a person is seen using a scraper to remove the snow from the car's windows. The person is seen scraping the snow off the 
windshield and the side windows of the car. The person continues to scrape the snow off the car, making sure that all the snow is removed. The video then shows the person 
wiping the snow off the car's windshield and side windows. The person is seen using a 
cloth to wipe the snow off the car's windows, making sure that the car is completely 
clean. The video ends with the person finishing the job and the car being shown off. 
Overall, the video shows a person removing snow from a car's windows and windshield, and then wiping the snow off the car's windows to make sure it is completely clean.[Input Instruction] Describe thevideo indetail.
The video shows a person preparing a dish with peppers and oranges. They slice the peppers and oranges and lay them out on a table. The person then proceeds to cut the 
peppers and oranges into strips and lays them out on a piece of wax paper. They use a 
knife to cut the peppers and oranges into strips and lay them out on a piece of wax paper. 
The person then uses a spoon to mix the peppers and oranges together. The video ends 
with the person cutting the peppers and oranges into strips and laying them out on a piece of wax paper.[Input Instruction] Describe thevideo .
The main focus of the video is the process of creating a painting of a tree using various techniques and colors. The video shows the artist's hand holding a paintbrush and using it 
to create the tree, as well as the use of different colors and techniques to create the 
painting.[Input Instruction] What isthemain focus ofthevideo .
The woman uses an ax to split the logs. She holds the ax in her hand and swings it down onto the log. She repeats this process several times, splitting the log into smaller pieces.[Input Instruction] What does thewoman usetosplit thelogs andhow does
shedoit?
In the video, we see a man walking down a track while holding a javelin. He then runs 
down the track and throws the javelin. The video then shows the man walking back to 
the starting point, and we see the javelin landing on the ground. The man then walks away from the camera. From the video, we can see that the man is a javelin thrower, and 
he is practicing his throws. The track appears to be well-maintained, and the 
surroundings seem to be quiet and peaceful. The man's form and technique are 
impressive, and it is clear that he has experience in the sport. Overall, the video provides 
a glimpse into the world of javelin throwing and the dedication and skill required to excel in the sport.[Input Instruction] Describe thevideo indetail.
Figure 8: Samples of Video-LLaV A in video under-
standing.
The video shows a man playing the drums in a room. He is seen playing the drums with 
his hands and sticks, and he continues to play while looking off into the distance.[Input Instruction] Describe the activity in the video.
The video shows a young girl performing a gymnastics routine on a balance beam. She executes several flips and tricks before dismounting and landing on a mat.[Input Instruction] Describe thevideo .
The video features a woman grooming a black horse in a stable. She brushes the horse's 
hair and cleans its hooves, and the horse is seen standing calmly throughout the process. 
The video also includes shots of the woman speaking to the camera and the horse.[Input Instruction] Briefly describe the video.
The main challenge faced by the people on the boat is the rough sea conditions. They are seen struggling to stay upright and maintain their balance as the boat sways back and forth.[Input Instruction] What is the main challenge faced by the people on the boat?
The video is about a man who is seen spinning and throwing a discus in a field. He then walks away while people measure his throw.[Input Instruction] What isthevideo about?
Sure! The man in the video demonstrates how to clean a screen window by first wiping it down with a rag. He then applies a cleaning solution to the screen and wipes it down again. Finally, he shows how to dry the screen with a towel.[Input Instruction] Can you describe theprocess forcleaning ascreen
window asdemonstrated inthevideo?
The woman and the dog are playing frisbee in the yard. The woman throws the frisbee, and the dog chases after it.[Input Instruction] What activities arethewoman andthedogengaged in
thevideo?
The video shows a person cooking a dish of noodles. They start by boiling water in a pot 
and adding noodles to it. Then, they add oil and some ingredients to the pot. After that, 
they mix the noodles and the ingredients together. Finally, they serve the dish on a plate.[Input Instruction] Step-by-step describe thecooking recipe shown inthe
video .
The video shows two men arm wrestling on a table while being watched by a group of 
people.[Input Instruction] What is the video about?
[Input Instruction] Isthere aflaginthebackground?
Yes, there is a flag in the background of the video.
The video is taken from the top of the Statue of Liberty, which is a famous landmark in 
New York City. The statue is shown from different angles, including from the water and from the top of the statue itself.[Input Instruction] Where isthisvideo taken from? What place/landmark
isshown inthevideo?
Figure 9: Samples of Video-LLaV A in video under-
standing.