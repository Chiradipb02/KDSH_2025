Modeling User Preferences with Automatic Metrics:
Creating a High-Quality Preference Dataset for Machine Translation
Sweta Agrawal1, José G. C. de Souza3, Ricardo Rei3, António Farinhas1,2,
Gonçalo Faria1,Patrick Fernandes1,2,5,Nuno M Guerreiro1,2,3,6,André F.T. Martins1,2,3,4
1Instituto de Telecomunicações,2Instituto Superior Técnico, Universidade de Lisboa
3Unbabel,4ELLIS Unit Lisbon,5Carnegie Mellon University
6MICS, CentraleSupélec, Université Paris-Saclay, France
swetaagrawal20@gmail.com
Abstract
Alignment with human preferences is an im-
portant step in developing accurate and safe
large language models. This is no exception
in machine translation (MT), where better han-
dling of language nuances and context-specific
variations leads to improved quality. However,
preference data based on human feedback can
be very expensive to obtain and curate at a
large scale. Automatic metrics, on the other
hand, can induce preferences, but they might
not match human expectations perfectly. In
this paper, we propose an approach that lever-
ages the best of both worlds. We first collect
sentence-level quality assessments from pro-
fessional linguists on translations generated by
multiple high-quality MT systems and evaluate
the ability of current automatic metrics to re-
cover these preferences. We then use this anal-
ysis to curate a new dataset, MT-P REF(Metric-
induced Translation PREFerence), which com-
prises 18k instances covering 18 language di-
rections, using texts sourced from multiple
domains post-2022. We show that aligning
TOWER models on MT-P REFsignificantly im-
proves translation quality on WMT23 and FLO-
RES benchmarks.1
1 Introduction
The use of large language models (LLMs) in ma-
chine translation (MT) has garnered significant at-
tention from the research community (Kocmi et al.,
2023). Unlike traditional sequence-to-sequence
MT models trained on parallel data (Koehn and
Knowles, 2017), LLM-based MT systems either
use in-context learning to elicit translation knowl-
edge acquired during pre-training (Briakou et al.,
2023) or undergo supervised finetuning (SFT) on
high-quality translations to further enhance their
translation capabilities (Li et al., 2024; Xu et al.,
2023; Alves et al., 2023, 2024).
1We release the code and datasets to reproduce
all the results at https://github.com/deep-spin/
mt-pref-alignment .The default SFT approach for LLM-based MT
is to tune systems based solely on single human
reference translations. However, this kind of super-
vision might be insufficient to push quality further:
First, because many valid translations may exist
for a given source, with some preferred over oth-
ers (Mayhew et al., 2020). Second, because the
next-token prediction objective of SFT does not
capture sentence-level semantics and quality crite-
ria (Eikema and Aziz, 2020; Liu et al., 2022). This
has motivated new approaches that go beyond SFT
to leverage translation preferences or quality feed-
back to improve learning (Yang et al., 2023; He
et al., 2024; Xu et al., 2024; Zhu et al., 2024).
A key factor in aligning LLMs toward transla-
tion preferences is ensuring the quality and diver-
sity of the datasets used for training (Gao et al.,
2024; Morimura et al., 2024; Liu et al., 2023). Un-
fortunately, existing datasets have several limita-
tions: First, they are created from translation out-
puts of one or two models, for limited language
pairs, thereby restricting their diversity and applica-
bility to novel scenarios. Second, these datasets are
either entirely automatically generated (Xu et al.,
2024) or completely based on human feedback
(Zhu et al., 2024). While automatic evaluation of-
fers efficiency, it lacks the crucial validation that the
metrics used truly align with human preferences.
On the other hand, datasets that use human feed-
back, while high-quality and reliable, pose resource
constraints and are challenging to scale.
To bridge this gap, we provide a holistic ap-
proach to balance the advantages of automated
metrics while ensuring that they lead to prefer-
ences that truly align with humans. We first col-
lect sentence-level quality assessments and prefer-
ences from human expert translators (§3)—we use
the WMT23 English-German and Chinese-English
datasets (Kocmi et al., 2023) with outputs from
five high-quality MT systems: TOWER INSTRUCT -
7B,TOWER INSTRUCT -13B(Alves et al., 2024);arXiv:2410.07779v1  [cs.CL]  10 Oct 2024ALMA-13 B-R(Xu et al., 2024); GPT-4-based
(Hendy et al., 2023) and GOOGLE TRANSLATE .
Using these assessments, we then examine the abil-
ity of automatic quality estimation (QE) metrics
to recover human preferences. Our findings show
that an ensemble of XCOMET -XLand XCOMET -
XXL (Guerreiro et al., 2023)— XCOMET -XL+XXL—
achieves the highest correlation with human judg-
ments and a high precision score in identifying the
preferred translations.
Using this analysis, we create a new MT
preference dataset, MT-P REF (Metric-induced
Translation PREFerence dataset), with source sen-
tences mined post-2022 for 10 languages (English,
German, Chinese, Russian, Portuguese, Italian,
French, Spanish, Korean and Dutch). Translations
for each source sentence are generated using di-
verse MT systems representing different architec-
tures, training data, and quality levels (§4). We
use the ensemble metric XCOMET -XL+XXL to get
the most and least preferred translations from the
set of hypotheses. Experiments on aligning MT-
specialized decoder-only models ( TOWER ) using
existing preference learning algorithms with our
MT-P REF dataset demonstrate improved transla-
tion quality on the WMT23 (Kocmi et al., 2023)
and FLORES (Costa-jussà et al., 2022) bench-
marks, with larger gains in out-of-English trans-
lation directions (§6). Further analysis shows that
the aligned models better rank translations accord-
ing to human preferences over baselines.
2 Background: Aligning MT with
Translation Preferences
Given a source text, the goal of MT is to generate
a translation that accurately reflects the informa-
tion and meaning conveyed in the source. At train-
ing time, the MT model πθgoes through SFT to
minimize the negative log-likelihood (NLL) loss
induced by source-reference pairs (x, y):
LNLL(x, y;θ) =−logπθ(y|x). (1)
A drawback of SFT is that it typically optimizes
the model towards a single reference translation. In
contrast, preference learning objectives incorporate
relative preferences between alternatives, allowing
the model to learn from subtle differences in trans-
lation quality (Zeng et al., 2023).
Different variants of preference optimization
(PO) have been proposed in the literature. Rein-
forcement learning from human feedback (RLHF)has shown to be effective in aligning model be-
havior with human values (Ouyang et al., 2022).
Rafailov et al. (2024) propose direct preference
optimization (DPO) as a simple and scalable alter-
native to RLHF. Given a preference dataset Dwith
source sentences x, preferred or chosen outputs y+
and less preferred or rejected outputs y−, the model
is trained with the following objective:
LDPO(x, y±;πθ, πref) = (2)
−logσ
βlogπθ(y+|x)
πref(y+|x)−βlogπθ(y−|x)
πref(y−|x)
,
where πθis the parameterized policy, πrefis a base
reference policy (set to the policy used to generate
the dataset for collecting preferences), and βis a
(inverse) temperature hyperparameter.
One notable limitation of the DPO objective is
that it requires both πθandπrefin memory, signifi-
cantly increasing memory requirements and com-
putation costs. To address this, Xu et al. (2024)
further approximate the DPO objective using a uni-
form reference model ( πref=U) to derive a con-
trastive preference optimization (CPO) loss:
LCPO(x, y±;πθ,U) =−logσ
βlogπθ(y+|x)
πθ(y−|x)
.
(3)
However, both losses (2)–(3)only maximize the rel-
ative difference between preferred and dispreferred
outputs. On tasks like MT where the difference in
the two outputs is small, this may lead to failure
modes where the learning objective leads to a re-
duction of the model’s likelihood of the preferred
examples, as long as the relative probability be-
tween the two classes increases (Pal et al., 2024).
Therefore, following Hejna et al. (2023), Xu et al.
(2024) introduce a behavior cloning regularizer to
ensure that the model stays close to the preferred
distribution, leading to the final CPO objective:
LCPO(x, y±;θ) = (4)
LDPO(x, y±;πθ,U) +λLNLL(x, y+;θ),
where λis a hyperparameter that controls the rela-
tive strength of the two objectives.
As the quality of the preference datasets used
for training is key for its success (Gao et al., 2024;
Morimura et al., 2024; Liu et al., 2023), we next
discuss our process of collecting a high-quality
dataset for preference learning for MT.3 Modeling User Preferences Via
Automatic Metrics
To create a high-quality preference dataset for MT,
we need human judgments on translation outputs
from strong MT systems. This helps us understand
and model human preferences among competitive
translations. Since large-scale collection of these
judgments is costly, we evaluate existing automatic
metrics to see if they effectively reflect human pref-
erences. This determines if metrics can be reli-
able proxies for human judgments when translation
quality is high and preference variance is low.
We describe the dataset, models, and task in-
structions given to the expert annotators used in
our study in §3.1. The human evaluation results
are presented in §3.2. Finally, we discuss our meta-
evaluation of automatic MT metrics in their ability
to recover human preferences in §3.3.
3.1 Data and Annotation Task
We randomly sample 200 source instances from the
WMT23 English-German ( EN-DE) and Chinese-
English ( ZH-EN) test sets and generate transla-
tions using five MT models: GOOGLE TRANS -
LATE ,GPT -4,TOWER INSTRUCT -7B,TOWER IN-
STRUCT -13B, and ALMA -13B-R (described in
Appendix B).2We employ DA+SQM (Direct As-
sessment + Scalar Quality Metric) source con-
trastive evaluation (Kocmi et al., 2022), using
the Appraise evaluation framework (Federmann,
2018).3We then ask one linguist per language
pair to read all translations for a given source and
evaluate each of them on a continuous 0-100 scale.
The scale features seven labeled tick marks indi-
cating different quality labels combining accuracy
andgrammatical correctness . Linguists can fur-
ther adjust their ratings to reflect preferences or
assign the same score to translations of similar
quality. Detailed guidelines, compensation details,
and a screenshot of the interface are provided in
Appendix A. This results in a preference dataset in-
cluding 1000 ratings each for EN-DEand ZH-EN.4
3.2 Human Evaluation Findings
We present the results from our human evaluation
in Table 1 and discuss the findings below:
2This is the only dataset that was not used in the training
of any evaluated models.
3https://github.com/AppraiseDev/Appraise.git .
4Completing the task takes approximately 10 to 11 hours
for each language pair.DA T OP-1
MODEL EN-DE ZH-EN EN-DE ZH-EN
GOOGLE TRANSLATE 86.87 79.85 62 114
GPT-4 87.98 79.12 66 108
TOWER INSTRUCT -13B86.53 69.12 53 56
ALMA-13 B-R 84.96 66.02 46 51
TOWER INSTRUCT -7B 83.32 68.66 37 63
Table 1: Human evaluation results: DA scores for all
MT systems are high, suggesting that translations are
generally of very good quality according to experts.
ALMA-R
GPT-4
Google
T ower-13b
T ower-7bALMA-R
GPT-4
Google
T ower-13b
T ower-7b0 91 120 138 158
229 0 161 204 218
220 152 0 209 223
178 115 118 0 172
171 111 107 138 0WMT23 ROW > COLUMN
050100150200
Figure 1: Pairwise Preferences between different Sys-
tems: Google and GPT-4 translations are more preferred
over open-sourced alternatives.
Overall Quality For EN-DE, DA scores range
from 83.32 to 87.98, with no significant difference
in the translation quality of different systems, ac-
cording to the Mann-Whitney test (McKnight and
Najab, 2010). On the other hand, DA and Top-1
are significantly better for GPT -4 and GOOGLE -
TRANSLATE models for ZH-EN. Further qualitative
analysis shows that for WMT23 ZH-EN, the quality
of the source sentences is often poor—up to 25%
of source sentences were marked as problematic
by the linguist. This suggests there is still room for
improvement for open-source models over close-
sourced alternatives when generating translations
for noisy source texts (Peters and Martins, 2024).
Pairwise Preferences We also report pairwise
wins for each model against the other in Fig. 1.
GOOGLE TRANSLATE andGPT -4 outputs are gen-
erally more preferred over open-sourced translation
alternatives. Further analysis shows that about 25%
and 10% pairs are tied for equal preferences for ZH-
ENand EN-DErespectively, further validating close
translation quality amongst alternatives. Taken to-
gether, these results show that all the evaluated MT
systems generate high-quality translations.EN-DE ZH-EN
METRICP S T AU PRECISION @1 P S T AU PRECISION @1
COMET KIWI-XL 0.275 0.272 0.229 47.0 0.332 0.336 0.289 42.9
COMET KIWI-XXL 0.253 0.238 0.198 43.9 0.342 0.346 0.279 46.6
XCOMET -XL 0.334 0.300 0.249 41.9 0.456 0.410 0.342 44.5
XCOMET -XXL 0.316 0.312 0.252 44.4 0.343 0.410 0.340 44.0
METRIC X-23- L 0.238 0.238 0.191 37.9 0.428 0.409 0.328 42.4
METRIC X-23- XL 0.270 0.245 0.206 39.4 0.417 0.410 0.342 45.5
XCOMET -XL+XXL 0.341 0.329 0.270 47.0 0.434 0.411 0.336 48.7
COMET KIWI-XL+XXL 0.273 0.252 0.211 43.9 0.347 0.357 0.290 41.4
XCOMET +KIWI-XXL 0.286 0.271 0.223 45.5 0.377 0.382 0.304 46.6
COMET- REF 0.331 0.286 0.234 50.5 0.243 0.211 0.169 47.1
Table 2: Correlation and Precision@1 for automatic QE metrics: XCOMET -XL+XXL results in the highest correlation
and Precision@1 across the board, outperforming reference-based metric, COMET- REF.
3.3 Evaluating Automatic Metrics
We evaluate the best-performing metrics from
the WMT23 QE Shared Task: 1) COMET KIWI
(Rei et al., 2023); 2) XCOMET (Guerreiro et al.,
2023); 3) METRIC X(Juraska et al., 2023) and
ensembles of these metrics obtained by aver-
aging the scores from the two metrics: 4)
COMET KIWI-XL+XXL 5)XCOMET -XL+XXL and
6) C OMET KIWI+XCOMET -XXL.5
3.3.1 Metrics for Meta-Evaluation
We report the following scores to assess these met-
rics in their ability to recover human preferences at
the segment level:
Correlation Following WMT evaluation cam-
paign, we report the Pearson (P), Spearman (S),
and Kendall Tau ( TAU) correlation of automatic
metrics with human judgements over all collected
judgments grouped by source.
Precision@1 for the best translation We addi-
tionally report the precision of identifying the best
hypothesis by an automatic metric as the number of
times the metric’s ranked best translation is in the
set of human-ranked best translations. Note that
as we ask linguists to provide the same scores to
mark equal preferences over different translations,
multiple translations can obtain the highest quality.
3.3.2 Findings
Our main results are summarized in Table 2. The
correlation between human judgments and met-
ric scores on these high-quality translations is
5We refer the reader to the original papers for each metric
for more details about the training and architecture.rather low, suggesting a limited ability to model
human preferences between multiple translations
for the same source. XCOMET -XL+XXL, an ensem-
ble of XCOMET -XLand XCOMET -XXL, achieves
the best Spearman and PRECISION @1across the
board, even outperforming reference-based metric
COMET (Rei et al., 2020) on this task. Hence, we
use this metric to induce preference judgments in
our dataset in §4. Designing metrics that accu-
rately reflect these quality preferences remains an
open challenge. The dataset collected in our study
can potentially be used to benchmark new metrics,
which we leave for future work.
4 MT-P REF Dataset
Building on the findings from §3, we create our
preference dataset using XCOMET -XL+XXL. We
discuss the choice of the text and models in §4.1,
followed by the method for inducing and selecting
preference pairs from the dataset in §4.2.
4.1 Data and MT Systems
We collect source segments from REDPAJAMA
(Computer, 2023) for English, German, French,
Spanish, and Italian, and use MC4(Raffel et al.,
2019) for the remaining languages: Portuguese,
Russian, Chinese, French, and Korean. Approxi-
mately 1000 segments published after July 2022
were extracted and filtered for each language us-
ing the perplexity score available in the original
REDPAJAMA and MC4collections. The perplexity
thresholds vary across languages and were defined
after manual checks on the filtered segments, avoid-
ing non-fluent segments with repetitive patternsALMA-13B TOWER-7B GPT-4 TOWER-13B Google NLLB-54B
Model0.00.10.20.30.4CountType
Chosen
Rejected
Google NLLB-54B TOWER-7B GPT-4 TOWER-13B ALMA-13B
Model0.000.250.500.751.00ScoreFigure 2: Distribution of counts and scores for the chosen ( y+) and rejected ( y−) hypotheses across models.
such as sequences of numbers, non-alphanumeric
characters, and repeated words, among others.
We generate translation outputs using greedy
decoding from six diverse models varying in archi-
tecture (encoder-decoder and decoder-only), model
sizes (7B, 13B, 54B), and output quality (see Fig-
ure 2).6Specifically, we use 1) NLLB -54B(Costa-
jussà et al., 2022), 2) ALMA -13B(Xu et al.,
2023), 3) GPT -4, 4) GOOGLE TRANSLATE , and
5)TOWER models ( TOWER INSTRUCT -13Band
TOWER INSTRUCT -7B). A detailed description of
MT systems is provided in the Appendix B. We
generate translations using all models for all direc-
tions EN⇔{DE,FR,PT,NL,KO,ZH,RU,ES,IT},
with two exceptions. For ALMA -13B, we only
generate outputs for supported language pairs ( EN
⇔{DE,ZH,RU}) and discard translations for { ZH,
KO}→ENfrom NLLB -54Bdue to inferior quality
and frequent hallucinations.
4.2 Creating Preferences
For each source sentence x, we have up to six
translation options {yj}6
j=1. Our goal is to get
preference triples of source ( x), a preferred/cho-
sen hypothesis ( y+), and a less preferred/rejected
hypothesis ( y−). We use an automatic quality es-
timation metric Mto create this dataset of pref-
erence triples D={(x, y+, y−)}and resort to a
simple criterion that obtains the maximum discrep-
ancy under M. We first measure the translation
quality scores for each pair (x, yj), resulting in the
scores s={sj}6
j=1. We then select the best and
the worst translation hypotheses from the ranked
list induced by the scores, s,i.e.y+=yarg max j(sj)
andy−=yarg min j(sj). This results in a unique
preference triplet for each source sentence.
6We did not explore alternative decoding strategies to make
it easier to scale the dataset if necessary given the large sizes
of the translation models.5 Experimental Settings
We use the MT-P REF dataset7to align MT mod-
els with translation preferences (§4) and compare
several preference learning methods detailed in §2.
Training Data TheMT-P REF dataset contains
18k instances with approximately 1k examples for
each translation direction. The counts of the chosen
and the rejected hypotheses from each model and
the distribution of metric scores are shown in Fig. 2.
The NLLB -54Bmodel accounts for most of the
rejected hypotheses ( ∼46%), whereas the chosen
hypotheses are more equally distributed across the
GPT -4,GOOGLE TRANSLATE , and the TOWER
models, illustrating consistent and higher-quality
translations generated by these models.
Evaluation We evaluate finetuned models on the
WMT23 test set ( EN↔{DE,RU,ZH}) and the
FLORES dev-test set ( EN↔DE,RU,ZH,ES,FR,
PT,NL,IT,KO) using TOWER -EVAL (Alves et al.,
2024).8We report system-level translation quality
using CHRF(Popovi ´c, 2015), COMET ,METRIC X-
XXL (Juraska et al., 2023), and XCOMET -XL. We
cluster system performance using the Wilcoxon
rank-sum test ( p <0.05) with COMET as the pri-
mary metric. Rank ranges, denoted by [l+1, n−w],
indicate the number of systems a particular system
underperforms or outperforms, where lrepresents
the number of losses, nis the total number of sys-
tems, and wis the number of systems that the sys-
tem in question significantly outperforms (Kocmi
et al., 2023). We compare the models’ accuracy ( %
ACC.) for selecting the best-over-worst hypothesis
with the model’s likelihood on the human prefer-
ences (§3) after finetuning on MT-P REF.
Model Configurations We finetune TOWER IN-
STRUCT -7Busing preference optimization methods
detailed in §2 with the following configurations:
7https://huggingface.co/datasets/sardinelab/
MT-pref
8https://github.com/deep-spin/tower-evalEN-XX XX -EN
MODEL
CHRF↑COMET↑XCOMET ↑METRIC X↓CHRF↑COMET↑XCOMET ↑METRIC X↓% A CC.
TOWER INSTRUCT -7B52.25 584.32 5 85.32 4 1.78 3 58.87 482.77 4 88.77 5 2.20 2 53.25
+ SFT 53.29 484.26 5 85.11 4 1.92 4 59.30 382.79 4 89.16 4 2.29 2 58.50
+ DPO sft 53.27 484.85 4 85.63 4 1.73 3 59.86 383.18 3 89.56 2 2.13 2 59.25
+ DPO base 49.90 684.64 5 86.14 3 1.44 2 58.34 483.05 4 89.73 1 1.87 1 59.50
+ DPO base+SFT 52.42 584.99 4 86.37 3 1.58 2 59.43 383.16 3 89.60 1 2.03 2 58.25
+ CPO 52.95 485.05 4 86.43 3 1.59 2 59.62 383.14 3 89.70 1 2.04 2 59.50
TOWER INSTRUCT -13B54.15 385.17 4 86.55 3 1.57 2 59.86 383.18 3 89.33 3 2.11 2 59.50
+ SFT 54.87 384.96 4 86.12 3 1.72 3 60.26 283.25 2 89.65 1 2.16 2 57.25
+ CPO 54.45 385.59 3 87.22 2 1.43 2 60.55 283.49 2 89.98 1 1.99 2 60.25
ALMA-13 B-R 47.57 784.95 5 87.27 2 1.30 1 58.79 483.12 4 89.43 2 2.07 2 50.00
GPT-3.5 56.38 285.56 3 86.92 3 1.54 2 60.92 283.48 2 90.00 1 1.99 2 -
GPT-4 56.94 286.01 2 87.43 2 1.48 2 61.33 283.69 2 90.34 1 1.82 1 -
GOOGLE TRANSLATE 60.43 186.44 1 87.53 1 1.55 2 62.05 184.07 1 89.83 1 2.16 2 -
Table 3: Comparing PO methods on WMT23: Both CPO and DPO base+SFT result in significant (p=0.05) improve-
ment in translation quality, closing the gap with T OWER INSTRUCT -13B.
•SFT: a baseline model supervised finetuned on
the chosen or the most preferred response.
• DPO sft: model trained with πref=SFT in Eq. 2.
•DPO base: base model directly finetuned with
DPO, i.e.πref=TOWER INSTRUCT -7B.
•DPO base+SFT: base model finetuned with a com-
bination of DPO and SFT regularization, i.e.
LDPO(x, y±;πθ, πref) +λLNLL(x, y+;θ).
•CPO: model finetuned with the objective in Eq. 4.
We also compare the aligned models against
TOWER INSTRUCT -13B(with SFT and CPO vari-
ants), GPT -4,ALMA -13B-R9and GOOGLE -
TRANSLATE models. All training details are pro-
vided in Appendix D.
6 Results
We first present the results of comparing several
PO methods (§2) in Table 3 on the WMT23 and
FLORES datasets. Scores are averaged for from-
English ( EN-XX) and to-English ( XX-EN) transla-
tion directions. Results for individual language
pairs are shown in Appendix E. We then compare
preference learning on MT-P REF against an ex-
isting preference dataset (§6.2), followed by an
ablation on the impact of the dataset size on the
final translation quality (§6.3).
9The translation outputs are directly taken from
https://github.com/fe1ixxu/ALMA/tree/master/
outputs/wmt23_outputs/ALMA-13B-R
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Epoch60
40
20
Logps
CPO
DPO-from-SFTDPO-from-Base
DPO-from-Base+SFTFigure 3: Log probabilities for chosen ( ) and rejected
() hypotheses during training across PO methods:
DPO basereduces the likelihood for both chosen and
rejected responses, resulting in reduced output quality.
6.1 Comparing PO Algorithms
SFT results in limited translation quality gains.
SFT on the chosen response from the MT-P REF
dataset improves CHRFover TOWER INSTRUCT -7B
onEN-XX(+1.04) and XX-EN(+0.43) translation
directions, with no significant difference in COMET
and XCOMET -XL inEN-XXdirection. However,
we observe a large gain (+5.25%) in % A CC., sug-
gesting that the model does acquire some ability
to distinguish high-quality translations even when
trained with best translations only.
Preference learning improves translation qual-
ity. Most PO methods improve COMET and
XCOMET -XL as well as % A CC.over TOWER IN-
STRUCT -7Bin both directions, showing that align-EN-XX XX-EN
DATASETCHRF↑COMET↑ XCOMET ↑METRIC X↓ CHRF↑COMET↑ XCOMET ↑METRIC X↓
TOWER INSTRUCT -7B 56.14 88.51 93.01 1.05 64.08 88.28 96.20 1.18
+ CPO 56.70 88.81 93.71 0.96 64.21 88.32 96.56 1.15
TOWER INSTRUCT -13B 57.17 88.89 93.85 0.93 64.80 88.50 96.44 1.12
+ CPO 57.79 89.15 94.30 0.87 64.90 88.51 96.71 1.10
Table 4: CPO finetuning using MT-P REFimproves translation quality for T OWER models on FLORES.
ing LLMs with preferences benefits MT. The trans-
lation quality gap between TOWER INSTRUCT -7B
andTOWER INSTRUCT -13Bby COMET is reduced
significantly. Optimizing TOWER INSTRUCT -13B
onMT-P REFwith CPO further improves transla-
tion quality reaching comparable quality to GPT-
3.5forEN-XXand XX-ENdirections respectively.
This illustrates that finetuning on MT-P REF can
improve translation quality even for larger models.
SFT is necessary to obtain translation qual-
ity improvements using DPO. Comparing dif-
ferent variants of DPO (DPO sft, DPO baseand
DPO base+SFT), we find that either the SFT phase
or the SFT regularization is necessary to obtain sig-
nificant COMET improvements. This also aligns
with findings from Tunstall et al. (2023) who show
that learning from chat preference datasets fails
when skipping the initial SFT stage. Interestingly,
DPO baseattains the highest % A CC.scores among
variants, showing an improved ability to discern but
not necessarily generate high-quality translations.
We find that as suggested by (Pal et al., 2024), it
is indeed because DPO baseincreases the relative
probability between the two classes by decreasing
the model’s likelihood for both chosen andrejected
translations (see Fig. 3).
Results on FLORES We report the results of
aligning TOWER INSTRUCT -7Bwith CPO on FLO-
RES in Table 4. On average, the translation qual-
ity of the base models, TOWER INSTRUCT -7Band
TOWER INSTRUCT -13B, improves with alignment
tuning across the board according to all metrics,
with TOWER INSTRUCT -7Breaching close COMET ,
XCOMET -XLandMETRIC X-XXL scores to TOW-
ERINSTRUCT -13B, despite being 2x smaller.
In gist, we show that CPO results in the best-
aligned TOWER INSTRUCT -7B, matching transla-
tion quality with TOWER INSTRUCT -13Bon both
WMT23 and FLORES benchmarks. We next com-
pare preference optimization using CPO on MT-
PREFagainst existing preference datasets.6.2 MT-P REF Vs. ALMA-R-P REF
Xu et al. (2024) use the FLORES-200 develop-
ment and test datasets to create a preference dataset,
ALMA-R-P REF. For each source sentence in
the corpus, they take the human-written reference,
and outputs from ALMA -13B-R and GPT -4 mod-
els, and induce preferences using an ensemble of
XCOMET -XXL andCOMET KIWI-XXL metrics. We
note that this metric ensemble attains similar or
lower correlation scores compared to the best indi-
vidual metrics on both language pairs as shown in
Table 2. We compare the translation quality of the
resulting models when aligned with MT-P REFand
ALMA-R-P REFpreference datasets in Table 5.
Training on ALMA-R-P REFpreference dataset
improves neural metrics but significantly hurts
CHRFcompared to the base model, TOWER IN-
STRUCT -7B.10Our analysis shows that finetuning
on the ALMA-R-P REFdataset increases the out-
put length significantly. This could be due to the
inherent bias in the dataset where the chosen re-
sponses, typically by GPT -4 (45%), are on average
longer than the rejected responses.11This has im-
portant implications for the creation and modeling
of preferences – when a model is too frequently
“preferred” in a dataset, it can lead to the distillation
of that model’s characteristics and it is unclear to
what extent humans prefer these distilled features.
TOWER INSTRUCT -7Bfinetuned on equal-sized
ALMA-R-P REF and MT-P REF datasets score
close on neural metrics ( COMET and XCOMET ),
with a difference of 1.96 points on CHRF. As our
preference dataset considers outputs from multiple
models with diverse styles, we do not distill any
such model-specific biases. Furthermore, align-
ing on preferences induced via XCOMET -XL+XXL
yields slightly better COMET score on EN-XXdirec-
tion over preferences with XCOMET +KIWI-XXL,
10A difference of 2.4 CHRFpoints is considered significant
with 87% accuracy (Kocmi et al., 2024).
11The difference in the length of chosen andrejected trans-
lations in the training dataset is also significant according to
an independent t-test with a p-value of 0.01.EN-XX XX-EN
DATASET METRIC NCHRF↑COMET ↑XCOMET ↑METRIC X↓ CHRF↑COMET ↑XCOMET ↑METRIC X↓
TOWER -7B- - 52.25 84.32 85.32 1.78 58.87 82.77 88.77 2.20
MT-P REFXCOMET -XL+XXL18k 52.95 85.05 86.43 1.59 59.62 83.14 89.70 2.04
6k 52.98 84.81 85.98 1.66 59.63 83.09 89.46 2.08
XCOMET +KIWI–XXL 18k 52.87 84.86 85.90 1.66 59.86 83.15 89.54 2.07
ALMA-R XCOMET +KIWI-XXL14k 49.87 84.89 86.35 1.42 59.63 83.24 89.47 2.02
6k 51.02 84.76 85.90 1.52 59.72 83.15 89.33 2.07
TOWER -13B 54.15 85.17 86.55 1.57 59.86 83.18 89.33 2.11
Table 5: CPO finetuning on ALMA-R-P REFandMT-P REFvariants: Preferences induced via XCOMET -XL+XXL
on all examples gives the best overall results.
0 200 400 600 800 1000
Sample Size83.083.584.084.585.0ScoresType
EN-XX
XX-EN
Figure 4: COMET with varying size of the preference
dataset: EN-XXcontinues to benefit from more samples.
further validating the importance of inducing pref-
erences using metrics guided by human knowledge.
6.3 Impact of the Size of Preference Datasets
One advantage of our approach is that we can scale
the size of preference datasets as necessary as pref-
erences are induced using an automatic QE metric.
To understand whether this is indeed beneficial, we
conduct an ablation where we vary the number of
unique source samples per language pairs as: {200,
400, 600, 800, 1000} and align TOWER INSTRUCT -
7Bon the resulting preference dataset using CPO.
Fig. 4 shows the results: while the improvement in
quality for XX-ENplateaus with just 400 samples
per language direction, COMET continues to im-
prove for EN-XXsuggesting that adding more data
might benefit translations from English to other
language pairs. This aligns with the fact that the
model is exposed to relatively fewer non-English
texts during pretraining and hence benefits more
from any additional dataset on these languages.
7 Related Work
LLMs for MT Earlier works exploring LLMs to
perform MT study prompting techniques to gener-ate translations (Hendy et al., 2023; Zhang et al.,
2023a; Vilar et al., 2023) with research focusing
on selecting high-quality and relevant examples
as demonstrations to incorporating external knowl-
edge mimicking human-like translation strategies
(He et al., 2023). More recently, several works have
proposed finetuning LLMs to improve the transla-
tion quality (Zhang et al., 2023b; Alves et al., 2023),
resulting in specialized models that attain com-
petitive performance to state-of-the-art production
level translation systems (Xu et al., 2023; Alves
et al., 2024). Across all methods, the quality of
the data used for training is paramount to the fine-
tuning methods. Therefore, in this work, we focus
on curating a high-quality translation preference
dataset using metrics that closely reflect true hu-
man translation preferences and outputs generated
from a diverse set of high-quality MT systems.
Quality Feedback for MT Using feedback from
automatic metrics for MT or human quality assess-
ment has been an active area of research through
the past decade. This quality signal is either uti-
lized during training (Shen et al., 2016; Wieting
et al., 2019; Yang et al., 2023; He et al., 2024; Gul-
cehre et al., 2023; Nguyen et al., 2017; Kreutzer
et al., 2018, 2020) or decoding (Freitag et al., 2022;
Fernandes et al., 2022; Farinhas et al., 2023) or
for modeling translation preferences in the dataset
directly (Xu et al., 2024; Zhu et al., 2024). Simi-
lar to Xu et al. (2024), we use automatic metrics
to induce preferences in the dataset but with the
additional validation that the chosen metric indeed
reflects human quality expectations and with trans-
lations generated from diverse MT systems.
8 Conclusion
We present MT-P REF, a high-quality translation
preference dataset, curated by combining thestrengths of human evaluation and automatic met-
rics. The dataset includes metric-induced prefer-
ences from strong MT models across 18 language
directions with new source sentences mined post-
2022. Aligning state-of-the-art decoder-only LLMs
on this preference dataset using existing aligning
tuning algorithms improves translation quality. Fur-
thermore, the aligned models are also better at mod-
eling human preferences of translation quality.
Limitations
We note a few limitations of our work. We evalu-
ate the translation quality of the finetuned models
primarily using automatic metrics. While we val-
idate that they can indeed provide a reasonable
signal to differentiate quality at the system level
(See Appendix C), it requires a human evaluation
to confirm whether and to what extent the aligned
models match human preferences. Furthermore,
we use existing QE metrics that can be sensitive
to the domain of the datasets (Zouhar et al., 2024).
However, as the QE metrics continue to improve,
our approach allows to substitute the preferences
with that induced by a better QE metric. Finally,
we do not handle tied preferences in translation
quality and always induce a strict preference order.
Incorporating neutral preferences between transla-
tions can help the model focus on attributes that
truly improve quality over stylistic preferences; we
leave the investigation of this phenomenon to fu-
ture work. We note that our dataset can be used to
design better QE metrics for ranking translations,
inducing preferences using new criteria, and em-
ploying better optimization methods.
Potential Risks
Large language models may carry the potential
risk of generating fluent and hallucinated content.
When the users do not know the target or the
source language, they might trust the generated
translation without further verification (Martindale
and Carpuat, 2018). And while our approach is
driven toward making the model aware of trans-
lations of varying quality during finetuning, the
coverage is limited to the supported language pairs.
Users should exercise caution and seek verification
from additional sources where possible when using
LLMs on real-world applications.Acknowledgments
We thank Miguel Ramos, Miguel Faria, and
Giuseppe Attanasio for their useful and construc-
tive comments. This work was supported by
the Portuguese Recovery and Resilience Plan
through project C645008882-00000055 (Center
for Responsible AI), by the EU’s Horizon Europe
Research and Innovation Actions (UTTER, con-
tract 101070631), by the project DECOLLAGE
(ERC-2022-CoG 101088763), and by Fundação
para a Ciência e Tecnologia through contract
UIDB/50008/2020.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Duarte Alves, Nuno Guerreiro, João Alves, José Pom-
bal, Ricardo Rei, José de Souza, Pierre Colombo,
and Andre Martins. 2023. Steering large language
models for machine translation with finetuning and
in-context learning. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
11127–11148, Singapore. Association for Computa-
tional Linguistics.
Duarte M Alves, José Pombal, Nuno M Guerreiro, Pe-
dro H Martins, João Alves, Amin Farajian, Ben Pe-
ters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal,
et al. 2024. Tower: An open multilingual large
language model for translation-related tasks. arXiv
preprint arXiv:2402.17733 .
Eleftheria Briakou, Colin Cherry, and George Foster.
2023. Searching for needles in a haystack: On the
role of incidental bilingualism in PaLM’s translation
capability. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 9432–9452, Toronto,
Canada. Association for Computational Linguistics.
Together Computer. 2023. Redpajama: an open dataset
for training large language models.
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe
Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,
et al. 2022. No language left behind: Scaling
human-centered machine translation. arXiv preprint
arXiv:2207.04672 .
Bryan Eikema and Wilker Aziz. 2020. Is MAP decoding
all you need? the inadequacy of the mode in neural
machine translation. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics ,
pages 4506–4520, Barcelona, Spain (Online). Inter-
national Committee on Computational Linguistics.António Farinhas, José de Souza, and Andre Martins.
2023. An empirical study of translation hypothesis
ensembling with large language models. In Proceed-
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 11956–11970,
Singapore. Association for Computational Linguis-
tics.
Christian Federmann. 2018. Appraise evaluation frame-
work for machine translation. In Proceedings of the
27th International Conference on Computational Lin-
guistics: System Demonstrations , pages 86–88, Santa
Fe, New Mexico. Association for Computational Lin-
guistics.
Patrick Fernandes, António Farinhas, Ricardo Rei,
José G. C. de Souza, Perez Ogayo, Graham Neubig,
and Andre Martins. 2022. Quality-aware decoding
for neural machine translation. In Proceedings of
the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1396–1412,
Seattle, United States. Association for Computational
Linguistics.
Markus Freitag, David Grangier, Qijun Tan, and Bowen
Liang. 2022. High quality rather than high model
probability: Minimum Bayes risk decoding with neu-
ral metrics. Transactions of the Association for Com-
putational Linguistics , 10:811–825.
Yang Gao, Dana Alon, and Donald Metzler. 2024. Im-
pact of preference noise on the alignment perfor-
mance of generative language models. arXiv preprint
arXiv:2404.09824 .
Nuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa
Coheur, Pierre Colombo, and André FT Martins.
2023. xcomet: Transparent machine translation eval-
uation through fine-grained error detection. arXiv
preprint arXiv:2310.10482 .
Caglar Gulcehre, Tom Le Paine, Srivatsan Srini-
vasan, Ksenia Konyushkova, Lotte Weerts, Abhishek
Sharma, Aditya Siddhant, Alex Ahern, Miaosen
Wang, Chenjie Gu, et al. 2023. Reinforced self-
training (rest) for language modeling. arXiv preprint
arXiv:2308.08998 .
Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng
Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shum-
ing Shi, and Xing Wang. 2023. Exploring human-
like translation strategy with large language models.
arXiv preprint arXiv:2305.04118 .
Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng
Zhang, Rui Wang, Shuming Shi, and Zhaopeng Tu.
2024. Improving machine translation with human
feedback: An exploration of quality estimation as a
reward model. arXiv preprint arXiv:2401.12873 .
Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea
Finn, Scott Niekum, W Bradley Knox, and Dorsa
Sadigh. 2023. Contrastive prefence learning: Learn-
ing from human feedback without rl. arXiv preprint
arXiv:2310.13639 .Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,
Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita,
Young Jin Kim, Mohamed Afify, and Hany Hassan
Awadalla. 2023. How good are gpt models at ma-
chine translation? a comprehensive evaluation. arXiv
preprint arXiv:2302.09210 .
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky.
2012. Neural networks for machine learning lecture
6a overview of mini-batch gradient descent. Cited
on, 14(8):2.
Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya
Siddhant, Mehdi Mirzazadeh, and Markus Freitag.
2023. MetricX-23: The Google submission to the
WMT 2023 metrics shared task. In Proceedings
of the Eighth Conference on Machine Translation ,
pages 756–767, Singapore. Association for Compu-
tational Linguistics.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden,
Ondˇrej Bojar, Anton Dvorkovich, Christian Fed-
ermann, Mark Fishel, Markus Freitag, Thamme
Gowda, Roman Grundkiewicz, Barry Haddow,
Philipp Koehn, Benjamin Marie, Christof Monz,
Makoto Morishita, Kenton Murray, Makoto Nagata,
Toshiaki Nakazawa, Martin Popel, Maja Popovi ´c,
and Mariya Shmatova. 2023. Findings of the 2023
conference on machine translation (WMT23): LLMs
are here but not quite there yet. In Proceedings of the
Eighth Conference on Machine Translation , pages
1–42, Singapore. Association for Computational Lin-
guistics.
Tom Kocmi, Rachel Bawden, Ond ˇrej Bojar, Anton
Dvorkovich, Christian Federmann, Mark Fishel,
Thamme Gowda, Yvette Graham, Roman Grund-
kiewicz, Barry Haddow, Rebecca Knowles, Philipp
Koehn, Christof Monz, Makoto Morishita, Masaaki
Nagata, Toshiaki Nakazawa, Michal Novák, Martin
Popel, and Maja Popovi ´c. 2022. Findings of the 2022
conference on machine translation (WMT22). In
Proceedings of the Seventh Conference on Machine
Translation (WMT) , pages 1–45, Abu Dhabi, United
Arab Emirates (Hybrid). Association for Computa-
tional Linguistics.
Tom Kocmi, Vilém Zouhar, Christian Federmann, and
Matt Post. 2024. Navigating the metrics maze: Rec-
onciling score magnitudes and accuracies. arXiv
preprint arXiv:2401.06760 .
Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Proceedings
of the First Workshop on Neural Machine Translation ,
pages 28–39, Vancouver. Association for Computa-
tional Linguistics.
Julia Kreutzer, Nathaniel Berger, and Stefan Riezler.
2020. Correct me if you can: Learning from error
corrections and markings. In Proceedings of the 22ndAnnual Conference of the European Association for
Machine Translation , pages 135–144, Lisboa, Portu-
gal. European Association for Machine Translation.
Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and
Stefan Riezler. 2018. Can neural machine translation
be improved with user feedback? In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 3 (Industry
Papers) , pages 92–105, New Orleans - Louisiana.
Association for Computational Linguistics.
Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng,
and Jiajun Chen. 2024. Eliciting the translation abil-
ity of large language models via multilingual finetun-
ing with translation instructions. Transactions of the
Association for Computational Linguistics , 12:576–
592.
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and
Junxian He. 2023. What makes good data for
alignment? a comprehensive study of automatic
data selection in instruction tuning. arXiv preprint
arXiv:2312.15685 .
Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham
Neubig. 2022. BRIO: Bringing order to abstractive
summarization. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2890–2903,
Dublin, Ireland. Association for Computational Lin-
guistics.
Marianna Martindale and Marine Carpuat. 2018. Flu-
ency over adequacy: A pilot study in measuring user
trust in imperfect mt. In Proceedings of the 13th Con-
ference of the Association for Machine Translation
in the Americas (Volume 1: Research Track) , pages
13–25.
Stephen Mayhew, Klinton Bicknell, Chris Brust, Bill
McDowell, Will Monroe, and Burr Settles. 2020. Si-
multaneous translation and paraphrase for language
education. In Proceedings of the Fourth Workshop on
Neural Generation and Translation , pages 232–243,
Online. Association for Computational Linguistics.
Patrick E McKnight and Julius Najab. 2010. Mann-
whitney u test. The Corsini encyclopedia of psychol-
ogy, pages 1–1.
Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Ken-
shi Abe, and Kaito Air. 2024. Filtered direct prefer-
ence optimization. arXiv preprint arXiv:2404.13846 .
Khanh Nguyen, Hal Daumé III, and Jordan Boyd-
Graber. 2017. Reinforcement learning for bandit
neural machine translation with simulated human
feedback. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing ,
pages 1464–1474, Copenhagen, Denmark. Associa-
tion for Computational Linguistics.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in neural in-
formation processing systems , 35:27730–27744.
Arka Pal, Deep Karkhanis, Samuel Dooley, Man-
ley Roberts, Siddartha Naidu, and Colin White.
2024. Smaug: Fixing failure modes of prefer-
ence optimisation with dpo-positive. arXiv preprint
arXiv:2402.13228 .
Ben Peters and André FT Martins. 2024. Did trans-
lation models get more robust without anyone even
noticing? arXiv preprint arXiv:2403.03923 .
Maja Popovi ´c. 2015. chrF: character n-gram F-score
for automatic MT evaluation. In Proceedings of the
Tenth Workshop on Statistical Machine Translation ,
pages 392–395, Lisbon, Portugal. Association for
Computational Linguistics.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neu-
ral Information Processing Systems , 36.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. arXiv e-prints .
Ricardo Rei, Nuno M. Guerreiro, JosÃ ©Pombal, Daan
van Stigt, Marcos Treviso, Luisa Coheur, José G.
C. de Souza, and André Martins. 2023. Scaling up
CometKiwi: Unbabel-IST 2023 submission for the
quality estimation shared task. In Proceedings of the
Eighth Conference on Machine Translation , pages
841–848, Singapore. Association for Computational
Linguistics.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 2685–2702, Online. Association
for Computational Linguistics.
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 1683–1692, Berlin, Germany. Associ-
ation for Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-
rect distillation of lm alignment. arXiv preprint
arXiv:2310.16944 .
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,
Viresh Ratnakar, and George Foster. 2023. Prompt-
ing PaLM for translation: Assessing strategies and
performance. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 15406–
15427, Toronto, Canada. Association for Computa-
tional Linguistics.
Leandro von Werra, Younes Belkada, Lewis Tun-
stall, Edward Beeching, Tristan Thrush, Nathan
Lambert, and Shengyi Huang. 2020. Trl: Trans-
former reinforcement learning. https://github.
com/huggingface/trl .
John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel,
and Graham Neubig. 2019. Beyond BLEU:training
neural machine translation with semantic similarity.
InProceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics , pages 4344–
4355, Florence, Italy. Association for Computational
Linguistics.
Haoran Xu, Young Jin Kim, Amr Sharaf, and
Hany Hassan Awadalla. 2023. A paradigm shift
in machine translation: Boosting translation perfor-
mance of large language models. arXiv preprint
arXiv:2309.11674 .
Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan,
Lingfeng Shen, Benjamin Van Durme, Kenton Mur-
ray, and Young Jin Kim. 2024. Contrastive prefer-
ence optimization: Pushing the boundaries of llm
performance in machine translation. arXiv preprint
arXiv:2401.08417 .
Guangyu Yang, Jinghong Chen, Weizhe Lin, and Bill
Byrne. 2023. Direct preference optimization for neu-
ral machine translation with minimum bayes risk
decoding. arXiv preprint arXiv:2311.08380 .
Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie
Zhou. 2023. Tim: Teaching large language mod-
els to translate with comparison. arXiv preprint
arXiv:2307.04408 .
Biao Zhang, Barry Haddow, and Alexandra Birch.
2023a. Prompting large language model for ma-
chine translation: A case study. In International Con-
ference on Machine Learning , pages 41092–41110.
PMLR.
Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp
Koehn. 2023b. Machine translation with large lan-
guage models: Prompting, few-shot learning, and
fine-tuning with QLoRA. In Proceedings of the
Eighth Conference on Machine Translation , pages
468–481, Singapore. Association for Computational
Linguistics.Dawei Zhu, Sony Trenous, Xiaoyu Shen, Dietrich
Klakow, Bill Byrne, and Eva Hasler. 2024. A
preference-driven paradigm for enhanced translation
with large language models.
Vilém Zouhar, Shuoyang Ding, Anna Currey, Tatyana
Badeka, Jenyuan Wang, and Brian Thompson. 2024.
Fine-tuned machine translation metrics struggle in
unseen domains. arXiv preprint arXiv:2306.07899 .A Annotation Guidelines and Interface
Task Overview This task involves evaluating five translations of a source text and assigning a quality
rating to each translation based on its overall quality and adherence to the source content. You will need
to consider the accuracy, fluency, and overall quality when assessing the different translations.
Annotation Scale Each translation is evaluated on a continuous scale of 0-6 with the quality levels
described as follows:
•6: Perfect Meaning and Grammar: The meaning of the translation is completely consistent with the
source and the surrounding context (if applicable). The grammar is also correct.
•4: Most Meaning Preserved and Few Grammar Mistakes: The translation retains most of the meaning
of the source. It may have some grammar mistakes or minor contextual inconsistencies.
•2: Some Meaning Preserved: The translation preserves some of the meaning of the source but misses
significant parts. The narrative is hard to follow due to fundamental errors. Grammar may be poor.
•0: Nonsense/No meaning preserved: Nearly all information is lost between the translation and source.
Grammar is irrelevant.
You can scroll up or down to see all the other translation outputs from the different systems. Figure 5
shows the interface when comparing and evaluating five translations. While each translation is evaluated
independently, these translations can also be ranked based on the difference in their absolute scores. It is
perfectly valid to give the same score to multiple translations if you believe they are of the same overall
quality.
Other Details We hired native speakers of Chinese and German for this task (both females) and they
were compensated at $20 per hour.
B MT Systems
We use the following MT systems:
1.NLLB-54B (Costa-jussà et al., 2022) is a 54B encoder-decoder multilingual translation model, based
on a sparsely gated Mixture of Experts (MoE) approach. It covers 202 languages, supporting translation
for many low-resource languages.
2.TOWER INSTRUCT -13Band TOWER INSTRUCT -7Bare 13B and 7B decoder-only LLMs, trained to
optimize quality on multiple tasks present in translation workflows. The model is continued pretrained
from LLAMA 2(Touvron et al., 2023) checkpoints on a multilingual mixture of monolingual and
parallel data, followed by finetuning on instructions relevant to translation processes. The model
is instructed with the following prompt using the chat template to generate the translation output:
Translate the following [source language] source text to [target language]:
[source language]: [source sentence]
[target language]:
3.ALMA-13 B(Xu et al., 2023) is a 13B decoder-only model specialized for MT via continued pretaining,
followed by instruction tuning on a small but high-quality parallel dataset. Unlike TOWER INSTRUCT
models, the continued pretraining phase only explores monolingual data, and the instruction tuning is
performed with an MT dataset only.
4.ALMA-13 B-R(Xu et al., 2024) is a 13B decoder-only model obtained by finetuning ALMA -13B
with ALMA-R-P REFusing CPO.
We use the prompt from the original paper to generate translations using ALMA models:
Translate this from [source language] to [target language]:
[source language]: [source sentence]
[target language]:
5.GPT-4 (Achiam et al., 2023) is prompted in a zero-shot fashion, following Hendy et al. (2023), to
generate translations using the prompt:EN-DE ZH-ENMODELCHRF COMET XCOMET -XL DA CHRF COMET XCOMET -XL DA
GOOGLE TRANSLATE 68.83(1) 0.854(1) 0.941(1) 86.87(2) 49.40(1) 0.810(1) 0.884(1) 79.85(1)
GPT-4 68.50(2) 0.848(2) 0.932(3) 87.98(1) 45.95(2) 0.799(2) 0.877(2) 79.12(2)
TOWER INSTRUCT -13B66.45(3) 0.843(3) 0.931(4) 86.53(3) 45.29(3) 0.794(3) 0.866(3) 69.12(3)
ALMA-13 B-R 59.92(5) 0.836(4) 0.935(2) 84.96(4) 44.72(4) 0.793(4) 0.858(5) 66.02(5)
TOWER INSTRUCT -7B 64.61(4) 0.830(5) 0.918(5) 83.32(5) 43.77(5) 0.790(5) 0.860(4) 68.66(4)
PAIRWISE -ACC 8/10 9/10 7/10 - 9/10 9/10 10/10 -
Table 6: Automatic Evaluation - System Level for reference-based metrics. Ranks represent the ordering based on
averaged DA scores.
Translate this sentence from [source language] to [target language]:
Source: [source sentence]
Target:
6.GOOGLE TRANSLATE is the basic version of the Translate API v2 accessed on 2024-03-04.12
C System-level Correlation
Table 6 shows the system-level translation quality scores assigned by reference-based metrics: CHRF,
COMET , and XCOMET -XLfor all five models and their induced system-level rankings. For both directions,
COMET results in 90% agreement with human judgments, confirming its accuracy in rating high-quality
systems and hence we use C OMET as the primary metric for ranking different systems.
D Training Details
Hyperparameters We finetune TOWER INSTRUCT -7BandTOWER INSTRUCT -13Bmodels (Alves et al.,
2024) using the TRL library (von Werra et al., 2020) with a batch size of 64, a maximum output length
of 256, a learning rate of 5×10−7and a warm-up ratio of 0.1. The model is finetuned using different
preference algorithms (§2) for 3 epochs with RMSProp optimizer (Hinton et al., 2012). For SFT, following
(Tunstall et al., 2023), we finetune the base model for one epoch with a learning rate of 1×10−5using
Adam optimizer (Kingma and Ba, 2014). We use greedy decoding to generate translation hypotheses using
the aligned models. All our models are trained on two Nvidia A100 GPUs. Training takes approximately
four to five hours to converge.
E Results by WMT23 Language Direction
We report results comparing preference optimization methods when trained with MT-P REFon individual
language pairs using COMET ,CHRF,XCOMET -XLandMETRIC X-XXL in Tables 7, 8, 9 and 10
respectively.
12https://translation.googleapis.com/language/translate/v2MODEL EN-DE EN-ZH EN-RU DE-EN ZH-EN RU-EN
TOWER INSTRUCT -7B 83.25 84.98 84.72 85.25 80.15 82.90
+ SFT 83.01 85.47 84.29 85.25 80.25 82.86
+ DPO sft 83.83 85.81 84.91 85.66 80.72 83.17
+ DPO base 83.73 84.64 85.55 85.25 80.60 83.30
+ DPO base+SFT 83.86 85.65 85.46 85.53 80.69 83.26
+ CPO 83.92 85.74 85.49 85.47 80.79 83.17
TOWER INSTRUCT -13B 84.02 85.97 85.52 85.60 80.71 83.23
+ SFT 83.73 86.00 85.15 85.62 81.04 83.08
+ CPO 84.53 86.32 85.91 85.72 81.25 83.49
ALMA-13 B-R 84.03 84.97 85.85 85.54 80.55 83.28
GPT-3.5 84.61 86.70 85.38 85.91 81.52 83.02
GPT-4 84.89 87.08 86.07 86.17 81.27 83.63
GOOGLE TRANSLATE 84.77 88.09 86.45 86.24 82.19 83.78
Table 7: C OMET (↑) on WMT23 dataset comparing PO methods when trained with MT-P REF.
MODEL EN-DE EN-ZH EN-RU DE-EN ZH-EN RU-EN
TOWER INSTRUCT -7B 65.74 37.34 53.66 67.80 49.91 58.89
+ SFT 65.76 40.16 53.95 67.93 50.89 59.08
+ DPO sft 66.16 39.56 54.10 68.57 51.61 59.38
+ DPO base 64.23 33.02 52.43 66.61 50.25 58.15
+ DPO base+SFT 65.90 37.59 53.78 67.97 50.89 59.42
+ CPO 66.22 38.68 53.96 68.25 51.31 59.30
TOWER INSTRUCT -13B 66.90 40.62 54.95 68.47 51.22 59.89
+ SFT 67.39 41.98 55.25 68.42 52.38 59.99
+ CPO 67.36 40.74 55.24 69.03 52.35 60.28
ALMA-13 B-R 60.38 32.14 50.19 66.30 51.28 58.79
GPT-3.5 68.38 45.25 55.50 69.21 53.78 59.77
GPT-4 69.30 45.67 55.86 69.91 53.37 60.70
GOOGLE TRANSLATE 69.08 52.99 59.21 70.28 55.15 60.72
Table 8: CHRF (↑) on WMT23 dataset comparing PO methods when trained with MT-P REF.Figure 5: Annotation Interface.MODEL EN-DE EN-ZH EN-RU DE-EN ZH-EN RU-EN
TOWER INSTRUCT -7B 84.44 83.77 87.75 89.07 85.02 92.23
+ SFT 84.48 83.67 87.19 89.24 85.75 92.48
+ DPO sft 84.98 84.13 87.78 89.62 86.22 92.84
+ DPO base 85.17 83.78 89.47 89.54 86.41 93.23
+ DPO base+SFT 85.24 84.67 89.20 89.51 86.33 92.95
+ CPO 85.33 84.98 88.97 89.51 86.70 92.88
TOWER INSTRUCT -13B 85.42 85.17 89.05 89.41 85.81 92.77
+ SFT 85.06 84.57 88.74 89.74 86.33 92.87
+ CPO 86.13 85.80 89.74 89.86 86.86 93.21
ALMA-13 B-R 86.09 84.81 90.91 89.24 86.14 92.92
GPT-3.5 86.62 85.16 88.99 89.80 87.23 92.98
GPT-4 86.72 85.59 89.98 89.92 87.43 93.68
GOOGLE TRANSLATE 85.76 86.73 90.11 89.37 86.93 93.20
Table 9: XCOMET -XL (↑) on WMT23 dataset comparing PO methods when trained with MT-P REF.
MODEL EN-DE EN-ZH EN-RU DE-EN ZH-EN RU-EN
TOWER INSTRUCT -7B 2.13 1.43 1.77 2.17 2.49 1.94
+ SFT 2.24 1.54 1.97 2.34 2.56 1.96
+ DPO sft 1.96 1.43 1.80 2.14 2.37 1.89
+ DPO base 1.64 1.26 1.42 1.83 2.08 1.70
+ DPO base+SFT 1.90 1.28 1.55 2.05 2.21 1.82
+ CPO 1.93 1.29 1.56 2.06 2.20 1.86
TOWER INSTRUCT -13B 1.87 1.28 1.55 2.05 2.44 1.84
+ SFT 2.00 1.44 1.73 2.15 2.43 1.91
+ CPO 1.64 1.22 1.44 1.98 2.21 1.78
ALMA-13 B-R 1.77 1.30 1.57 2.16 2.47 1.8
GPT-3.5 1.71 1.23 1.49 1.79 2.02 1.64
GPT-4 1.68 1.32 1.62 2.05 2.11 1.82
GOOGLE TRANSLATE 1.46 1.19 1.25 2.19 2.25 1.78
Table 10: M ETRIC X (↓) on WMT23 dataset comparing PO methods when trained with MT-P REF.