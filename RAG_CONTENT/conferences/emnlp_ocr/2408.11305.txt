UniFashion: A Unified Vision-Language Model for Multimodal Fashion
Retrieval and Generation
Xiangyu Zhao1, Yuehan Zhang2, Wenlong Zhang1,3, Xiao-Ming Wu1
1Department of Computing, The Hong Kong Polytechnic University
2Wuhan University,3Shanghai AI Laboratory
xiang-yu.zhao@connect.polyu.hk, xiao-ming.wu@polyu.edu.hk
Abstract
The fashion domain includes a range of real-
world multimodal tasks, such as multimodal
retrieval and generation. Recent advancements
in AI-generated content, particularly large lan-
guage models for text and diffusion models for
visuals, have spurred significant research in-
terest in applying these multimodal models to
fashion. However, fashion models must also
effectively handle embedding tasks, like image-
to-text and text-to-image retrieval. Moreover,
current unified fashion models often lack the
capability for image generation. In this work,
we present UniFashion, a unified framework
that tackles the challenges of multimodal gen-
eration and retrieval tasks in the fashion do-
main, by integrating image and text genera-
tion with retrieval tasks. UniFashion unifies
embedding and generative processes through
the use of a diffusion model and LLM, en-
abling controllable and high-fidelity genera-
tion. Our model significantly outperforms pre-
vious state-of-the-art models focused on single
tasks across various fashion-related challenges
and can be easily adapted to manage complex
vision-language tasks. This study highlights
the synergistic potential between multimodal
generation and retrieval, offering a promising
avenue for future research in the fashion do-
main. The source code is available at https:
//github.com/xiangyu-mm/UniFashion .
1 Introduction
The fashion domain presents a range of real-world
multimodal tasks, encompassing multimodal re-
trieval (Gao et al., 2020; Wu et al., 2021; Bai
et al., 2023; Liu et al., 2024b) and multimodal
generation (Yang et al., 2020) tasks. Such tasks
have been utilized in diverse e-commerce scenar-
ios to enhance product discoverability, seller-buyer
interaction, and customer conversion rates after
catalog browsing (Han et al., 2023; Zhuge et al.,
2021). The remarkable progress in the field of arti-ficial intelligence generated content (AIGC), par-
ticularly in technologies like large language mod-
els (LLMs) (Chiang et al., 2023; Touvron et al.,
2023; Brown et al., 2020) for text generation and
diffusion models (Rombach et al., 2022; Nichol
et al., 2022; Saharia et al., 2022) for visual genera-
tion, yielding significant advancements in numer-
ous downstream tasks (Feng et al., 2023; Zhang
et al., 2022) and sparking widespread research in-
terest in applying these multimodal models to the
fashion domain.
Instruction-tuned multimodal large language
models (Liu et al., 2023a; Dai et al., 2023; Dong
et al., 2023; Zhao et al., 2024) (MLLMs) have
emerged as a promising direction for developing a
single multi-task model (Shi et al., 2023). However,
due to the heterogeneous nature of multimodal fash-
ion tasks (Han et al., 2023), most existing MLLMs
struggle to be directly applicable in the fashion do-
main. For example, in the fashion domain, retrieval
tasks that rely on embedding ability, such as image-
to-text or text-to-image retrieval, have largely been
overlooked. Furthermore, existing MLLMs lack
the ability to solve the composed image retrieval
(CIR) (Liu et al., 2021; Baldrati et al., 2022) task,
which composes the reference image and related
caption in a joint embedding to calculate similari-
ties with candidate images and is particularly rel-
evant in recommender systems (Han et al., 2017;
Liu et al., 2022, 2024a).
Drawing inspiration from GRIT (Muennighoff
et al., 2024), which successfully combined genera-
tive and embedding tasks into a unified model for
text-centric applications and enhanced embedding
performance by incorporating a generative objec-
tive, it is evident that exploring task correlations
and integrating embedding with generative models
in the fashion domain is promising.
While previous works (Han et al., 2023; Zhuge
et al., 2021) in the fashion domain have also pro-
posed using a single model for solving multiple
1arXiv:2408.11305v2  [cs.CV]  12 Oct 2024Ivory Open Knit Anchor Dress.
Unstructured knit dress in ivory
white.Orange Orchid Beam Duchess
Dress. Structured dress in tones of
purple ...
Black Lambskin Fringe Detail
ShiftDress. Sleeveless boxy-fit
panelled leather dress in black.Champagne Crepe Deep-V  Dress.
Long sleeve crepe dress in
champagneLong sleeve shirt in white and black plaid. Button-
down spread collar . Button closure at front. Breast
pocket. Single-button barrel cuf fs. Curved hem.
Tonal stitching.
1. A yellow t-shirt with a graphic design on the front. The t-shirt has short
sleeves and a crew neckline.
2. A long-sleeved top in a soft pink or mauve color . The top features a ribbed
texture throughout. A lace or embroidered detail across the chest area. 
is green with a four
leaf clover ,
is green and has no
text
A black shirt with white letters and a white
skull on it. the shirt has a  camouflage
pattern and is buttoned up.Text-to-Image Retrieval
Black Lambskin Fringe Detail
ShiftDress. Sleeveless boxy-fit
panelled leather dress in black.
A black dress with a black belt, the dress has a
looser fit and longer sleeves, and it features a
wider v-neckline.
Image-to-T ext RetrievalText-to-Image Generation
Image-to-T ext Generation
Composed Image Retrieval Composed Caption Generation
has white letters,
has more buttons
Composed Image GenerationFigure 1: Illustration of the fashion tasks encompassed in our UniFashion framework: cross-modal retrieval,
text-guided image retrieval, fashion image captioning, and fashion image generation. Model inputs highlighted with
a light yellow background and outputs denoted by a light blue background.
tasks, they ignore image generation tasks. Besides,
for fashion tasks such as try-on (Choi et al., 2021)
and fashion design (Baldrati et al., 2023b), it is gen-
erally required to generate target images based on
multimodal input. However, previous works (Bal-
drati et al., 2023b) in fashion image generation
typically adopt the CLIP text encoder for encoding
text information. This approach may not effectively
capture the textual context due to the limitations of
the text encoder, as noted by Saharia et al. (2022).
Hence, we posit that current studies have yet to
fully explore the potential synergy between genera-
tion and retrieval.
In this work, we propose UniFashion, which
unifies retrieval and generation tasks by integrat-
ing LLMs and diffusion models, as illustrated in
Figure 2. UniFashion consists of three parts: The
Q-Former is crucial for amalgamating text and im-
age input, creating multimodal learnable queries.
These queries, once refined through task-specific
adapters, enable the LLM module to utilize them as
soft prompts for generating captions for target im-ages. Simultaneously, the diffusion module utilizes
the learnable queries as conditions to guide the la-
tent diffusion model in image synthesis and editing
tasks. To enable controllable and high-fidelity gen-
eration, we propose a two-phase training strategy.
In the first phase, we perform multimodal repre-
sentation learning on image-text pairs datasets. We
freeze Q-Former and fine-tune the LLM and diffu-
sion modules, ensuring they develop the capabil-
ity to comprehend the multimodal representations
provided by Q-Former. Subsequently, in the sec-
ond phase, we proceed to fine-tune UniFashion on
datasets with multimodal inputs, such as Fashion-
IQ, where we freeze the LLM and diffusion mod-
ules, only tuning Q-Former. This strategy ensures
that Q-Former is adept at crafting multimodal repre-
sentations that effectively integrate both reference
images and text inputs.
UniFashion holds three significant advantages
that address the challenges in multimodal fashion
retrieval and generation:
•For the first time, we conduct an in-depth
2study of the synergistic modeling of multi-
modal retrieval and generation tasks within
the fashion domain, thoroughly exploiting the
inter-task relatedness. Further, we introduce
UniFashion, a versatile, unified model that can
handle all fashion tasks.
•Secondly, our model enhances performance
via mutual task reinforcement. Specifically,
the caption generative module aids the CIR
task, while jointly training the generation and
retrieval tasks improves the multimodal en-
coder for the diffusion module.
•Thirdly, extensive experiments on diverse
fashion tasks—including cross-modal re-
trieval, composed image retrieval, and mul-
timodal generation—demonstrate that our uni-
fied model significantly surpasses previous
state-of-the-art methods.
2 Preliminaries and Related Works
2.1 Fashion Tasks
Fashion tasks encompass a range of image and
language manipulations, including cross-modal re-
trieval, composed image retrieval, fashion image
captioning and generation, etc. The representative
tasks can be briefly divided into the following two
groups.
Fashion Retrieval. It generally consists of Cross-
Modal Retrieval (CMR) (Ma et al., 2022; Ros-
tamzadeh et al., 2018) and composed image re-
trieval (CIR) tasks (Baldrati et al., 2023a; Bai et al.,
2023). CMR requests to efficiently retrieve the
most matched image/sentence from a large candi-
date pool Dgiven a text/image query. CIR is a
special type of image retrieval with a multimodal
query (a combination of a reference image and a
modifying text) matched against a set of images. It
retrieves a target image from a vast image database
based on a reference image and a text description
detailing changes to be applied to the reference im-
age. In this scenario, a query pair p={IR, t}is
provided, where IRis the reference image and tis
the text describing the desired modifications. The
challenge for this task is to accurately identify the
target image ITthat best matches the query among
all potential candidates in the image corpus D.
Fashion Generation. It consists of Fashion Im-
age Captioning (FIC) and Fashion Image Genera-
tion (FIG). FIC (Yang et al., 2020) aims to generatea descriptive caption for a product based on the
visual and/or textual information provided in the
input. FIG aims to generate images based on the
multimodal input, such as try-on (Choi et al., 2021;
Gou et al., 2023) and fashion design (Baldrati et al.,
2023b).
2.2 Multimodal Language Models
Recent research has witnessed a surge of inter-
est in multimodal LLMs, including collaborative
models (Wu et al., 2023; Yang et al., 2023b; Shen
et al., 2023) and end-to-end methods (Alayrac et al.,
2022; Zhao et al., 2024; Li et al., 2022; Bao et al.,
2021; Wang et al., 2022b,a,a). More recently, some
works also explore training LLMs with parameter-
efficient tuning (Li et al., 2023b; Zhang et al.,
2023b) and instruction tuning (Dai et al., 2023;
Liu et al., 2023a; Ye et al., 2023; Zhu et al., 2023a;
Li et al., 2023a). They only focus on generation
tasks, while our model UniFashion is designed as a
unified framework that enables both retrieval and
generation tasks.
2.3 Diffusion Models
Diffusion generative models (Rombach et al., 2022;
Ramesh et al., 2021; Nichol et al., 2022; Ruiz et al.,
2023) have achieved strong results in text condi-
tioned image generation works. Among contempo-
rary works that aim to condition pretrained latent
diffusion models, ControlNet (Zhang et al., 2023a)
proposes to extend the Stable Diffusion model with
an additional trainable copy part for conditioning
input. In this work, we focus on the fashion domain
and propose a unified framework that can leverage
latent diffusion models that directly exploit the con-
ditioning of textual sentences and other modalities
such as human body poses and garment sketches.
2.4 Problem Formulation
Existing fashion image retrieval and generation
methods are typically designed for specific tasks,
which inherently restricts their applicability to the
various task forms and input/output forms in the
fashion domain. To train a unified model that
can handle multiple fashion tasks, our approach
introduces a versatile framework capable of han-
dling multiple fashion tasks by aligning the multi-
modal representation into the LLM and the diffu-
sion model. This innovative strategy enhances the
model’s adaptability, and it can be represented as:
Iout, Tout=FTRet,TGen(Iin, Tin; Θ), (1)
3where FTrepresents the unified model parameter-
ized by Θ, it consists of retrieval module TRetand
generative module TGen.
3 Proposed Model: UniFashion
In this section, we introduce the UniFashion to
unify the fashion retrieval and generation tasks into
a single model. By combining retrieval and gener-
ative modules , the proposed UniFashion employs
atwo-stage training strategy to capture relatedness
between image and language information. Con-
sequently, it can seamlessly switch between two
operational modes for cross-modal tasks and com-
posed modal tasks.
3.1 Phase 1: Cross-modal Pre-training
In the first stage, we conduct pre-training on the
retrieval and generative modules to equip the Large
Language Model (LLM) and diffusion model with
strong cross-modal fashion representation capabili-
ties for the next phase.
3.1.1 Cross-modal Retrieval
For cross-modal retrieval tasks, given a batch of
image caption pairs p={I, C}, we first calculate
their unimodal representations using an indepen-
dent method. In particular, we adopt a lightweight
Querying Transformer, i.e., Q-Former in BLIP-
2 (Li et al., 2023b), to encode the multimodal in-
puts, as it is effective in bridging the modality gap.
To avoid information leaks, we employ a unimodal
self-attention mask (Li et al., 2023b), where the
queries and text are not allowed to see each other:
ZI=Q-Former (I, q),
ZC=Q-Former (C).(2)
where the output sequence ZIis the encoding result
of an initialized learnable query qwith the input im-
age and ZCis the encoded caption, which contains
the embedding of the output of the [CLS] token
ecls, which is a representation of the input caption
text. Since ZIcontains multiple output embed-
dings (one from each query), we first compute the
pairwise similarity between each query output and
ecls, and then select the highest one as the image-
text similarity. In our experiments, we employ 32
queries in q, with each query having a dimension of
768, which is the same as the hidden dimension of
the Q-Former. For cross-modal learning objective,
we leverage the Image-Text Contrastive Learning
(ITC) and Image-Text Matching (ITM) method.The first loss term is image-text contrastive loss,
which has been widely adopted in existing text-to-
image retrieval models. Specifically, the image-text
contrastive loss is defined as:
LITC(X, Y ) =−1
BBX
i=1logexp[λ(XT
i·Yi)]PB
j=1exp[λ(XT
i·Yj)],
(3)
where λis a learnable temperature parameter. ITM
aims to learn fine-grained alignment between im-
age and text representation. It is a binary classi-
fication task where the model is asked to predict
whether an image-text pair is positive (matched) or
negative (unmatched), it is defined as,
LITM(X, Y ) =−1
BBX
i=1logexpfθ(Xi, Yi)PB
j=1expfθ(Xj, Yi),(4)
Then, we maximize their similarities via symmetri-
cal contrastive loss:
Lcross=LITC(tc, ZI) +LITM(ZC, ZI),(5)
3.1.2 Cross-modal Generation
As depicted in Fig. 2, after the learnable queries
qpass through the multimodal encoder, they are
capable of integrating the visual information with
textual guidance. However, in Section 3.1.1, we did
not specify a learning target for q. Empirically, the
qthat has been merged with the reference image
and edited text information should be equivalent
to the encoding of the target image. This implies
that we should be able to reconstruct the target
image and its caption based on q. In this section,
we will employ generative objectives to improve
the representation of augmented q.
In the first stage, we connect the Q-Former
(equipped with a frozen image encoder) to a Large
Language Model (LLM) to harness the LLM’s
prowess in language generation, and to a diffu-
sion model to exploit its image generation capa-
bilities. Notably, we exclusively train the model
using image-text pairs throughout this process. As
depicted in Figure 2, we employ a Task Specific
Adapter (TSA) layer to linearly project the output
query embeddings qto match the dimensionality
of the embeddings used by the LLM and diffusion
model. In this stage, we freeze the parameters of
the Q-Former and fine-tune only the adapter layers,
connecting LLM and diffusion models. This ap-
proach allows us to develop a discriminative model
that can evaluate whether queries qcan generate
the target image and its corresponding caption.
4Contrastive 
Learning
Multimodal Encoder Multimodal EncoderReference 
ImageLLM
Target 
ImageU-Net Auto -
encoderImage
Generation
Learnable Queries Learnable QueriesZp Zq
Has longer sleeves 
and is  lighter in colo r
Text Guidance... ...... ...
CLIP[CLS] Has longer ...
Multimodal 
EncoderMultimodal 
Encoder
Learn able Queries......A blue Hawaiian shirt with a 
colorful design. The shirt is 
made of cotton and has a 
button -up collar. The design 
includes palm trees, parrots ...
[CLS] The image features ...Image
Generation
Caption
Generation
A blue Hawaiian shirt 
with a colorful design ...TSA
Image 
Input
LLMU-Net
AutoKL
Decoder
CLIP
Image CaptionCLIPTSAZt
Retrieval Module
Generative Module Generative Module
Phase 1 Phase 2Contrastive 
Learning
Uni-modal
MaskingZt
Bidirectional
MaskingRetrieval ModuleThe shirt is a white t -shirt 
with a short sleeve. It is a 
simple, plain design with no 
pattern or printCaption
GenerationFigure 2: Overview of the training framework of our UniFashion model. Phase 1 - Cross-modal Pre-training:
UniFashion acquires robust cross-modal fashion representation capabilities through pre-training, leveraging both
the language model and the diffusion model. Phase 2 - Composed Multimodal Fine-tuning: The model undergoes
fine-tuning to process both image and text inputs, refining its ability to learn composed modal representations. This
is achieved by aligning the multimodal encoder with the LLM and the diffusion model for enhanced performance.
Target Caption Generation. The adapter layer
is placed before the LLM to map the output of Q-
Former to the text embedding space of the LLM.
To synchronize the space of Q-Former with that of
the LLM, we propose to use the image-grounded
text generation (ITG) objective to drive the model
to generate texts based on the input image by com-
puting the auto-regressive loss:
LITG=−1
LLX
l=1logpϕ(wg
l|wg
<l, fθ(q)),(6)
where wg= (wg
1, ..., wg
L)represents the ground-
truth caption of image Iwith length L,q=
Q-Former (I, q),ϕdenotes the LLM’s parameters,
andθdenotes the text adapter layers’ parameters.
Target Image Generation. In the first stage, our
task also aims to reconstruct the image ˆITfrom q.
As in standard latent diffusion models, given an
encoded input x, the proposed denoising network
is trained to predict the noise stochastically added
tox. The corresponding objective function can be
specified as:
Lq2I=Eϵy,x0[∥ϵx−ϵx
η(xtx, fζ(q), tx)∥2],
(7)
where ηdenotes the u-net models’ parameters and
ζdenotes the image adapter layers’ parameters.
The overall loss in the first stage can be expressed:
Lph1=Lcross+LITG+Lq2T. (8)After the first training stage, we can leverage the
LLM and diffusion model as discriminators to
guide the generation of composed queries.
3.2 Phase 2: Composed Multimodal
Fine-tuning
In this phase, the inputs are reference image and
guidance text, and we fine-tune the model for com-
posed multimodal retrieval and generation tasks.
3.2.1 Composed Image Retrieval
For CIR task, the target image ITgenerally encom-
passes the removal of objects and the modification
of attributes in the reference image. To solve this
problem, as depicted in Fig. 2, the multimodal en-
coder is utilized to extract features from the ref-
erence image and the guide text. It joint embeds
the given pair p={IR, t}in a sequential output.
Specifically, a set of learnable queries qconcate-
nated with text guidance tis introduced to interact
with the features of the reference image. Finally,
the output of Q-Former is the multimodal synthetic
prompt ZR. We use a bi-directional self-attention
mask, similar to the one used in BLIP2 (Li et al.,
2023b), where all queries and texts can attend to
each other. The output query embeddings ZRthus
capture multimodal information:
ZR=Q-Former (IR, t, qR),
ZT=Q-Former (IT, qT).(9)
5Noting that the output sequence ZRconsists of
learnable queries qand encoded text guidance t,
which includes ecls, the embedding of the output
of the [CLS] token. On the other hand, the tar-
get image’s output sequence ZTconsists only of
learnable queries. Therefore, we can use ZRas a
representation that incorporates information from
the reference image and the guidance text and align
it with the features of the target image ZT. More-
over, as UniFashion acquires the ability to generate
captions for images from Sec. 3.1.2, we can gen-
erate captions for the candidate images and use
eclsto retrieve the caption ZCof the target image.
Then, the final contrastive loss for the CIR task is:
Lcir=LITC(ecls, ZT) +LITC(ecls, ZC)
+LITM(t, ZT),(10)
3.2.2 Composed Multimodal Generation
For these generation tasks, we freeze the LLM
parameters and tune the parameters of the task-
specific adapters, the diffusion model, and the Q-
Former. The loss function for the target image’s
caption generation is formulated in a way that is
similar to Eq. 6:
LITG=−1
LLX
l=1logpϕ(wg
l|wg
<l, fθ(qR)),(11)
The loss function for the target image generation is
formulated in a way that is similar to Eq. 7:
Lq2I=Eϵy,x0[∥ϵx−ϵx
η(xtx, fζ(qR), tx)∥2],
(12)
The overall loss in the second stage can be ex-
pressed as:
Lstage2 =Lcir+LITG+Lq2I. (13)
3.3 Instruction-Tuning LLMs for Different
Caption Style
Liu et al.’s work shows that LLMs have the po-
tential to handle multimodal tasks based on text
description of images. Due to the different styles
of captions in different fashion datasets, we adopt
different instructions to tune the LLM so that it can
generate captions of different styles.
We designed different instructions for different
datasets and tasks, as shown in Table 7. General
instruction template is denoted as follows:
USER: <Img><queries></Img> + Instruction. As-
sistant: <answer>.For the <image> placeholder, we substitute it
with the output of Multimodal Encoder. To avoid
overfitting to the specific task and counteract the
model’s inclination to generate excessively short
outputs, we have devised specific instructions,
which enable the LLM to produce concise re-
sponses when necessary.
4 Experiments
4.1 Experimental Setup
We initialize the multimodal encoder using
BLIP2’s Q-Former. Following the approach of
LLaV A-1.5 (Liu et al., 2023a), we initialize the
LLM from Vicuna-1.5 (Zheng et al., 2023). For
the diffusion module, we adopt the autoencoder
and denoising U-Net from Stable Diffusion v1.4,
as utilized in StableVITON. The weights of the
U-Net are initialized from Paint-by-Example. To
achieve more refined person textures, we employ
a V AE that has been fine-tuned on the VITONHD
dataset, as done in StableVITON. The statistics of
the two-stage datasets can be found in Table 6. For
cross-modal retrieval, we evaluated UniFashion on
FashionGen validation set. For the image caption-
ing task, UniFashion is evaluated in the Fashion-
Gen dataset. For the composed image retrieval
task, we evaluated the Fashion-IQ validation set.
To maintain consistency with previous work, for
the composed image generation task, we fine-tuned
UniFashion and evaluated it on the VITON-HD
and MGD datasets. More details can be found in
Appendix B.
Phase 1: For multimodal representation learning,
we follow BLIP2 and pretrain the Q-Former on
fashion image-text pairs. To adapt the model for
multimodal generation, we freeze the parameters of
Q-Former and fine-tune the MLLM and diffusion
model with their task specific adapters separately.
Due to the different styles of captions in different
fashion datasets, we adopt the approach of instruc-
tion tuning to train the LLM so that it can generate
captions of different styles. More details can be
found in Appendix 3.3.
Phase 2: In order to make UniFashion have the
composed retrieval and generation abilities, we
freeze the parameters of LLM and diffusion model,
only fine-tune the multimodal encoder.
6ModelImage to Text Text to ImageMean
R@1 R@5 R@10 R@1 R@5 R@10
FashionBERT (Li et al., 2022) 23.96 46.31 52.12 26.75 46.48 55.74 41.89
OSCAR (Alayrac et al., 2022) 23.39 44.67 52.55 25.10 49.14 56.68 41.92
KaledioBERT (Li et al., 2023b) 27.99 60.09 68.37 33.88 60.60 68.59 53.25
EI-CLIP (Li et al., 2023b) 38.70 72.20 84.25 40.06 71.99 82.90 65.02
MVLT (Dai et al., 2023) 33.10 77.20 91.10 34.60 78.00 89.50 67.25
FashionViL (Zhu et al., 2023a) 65.54 91.34 96.30 61.88 87.32 93.22 82.60
FAME-ViL (Liu et al., 2023a) 65.94 91.92 97.22 62.86 87.38 93.52 83.14
UniFashion (Ours) 71.44 93.79 97.51 71.41 93.69 97.47 87.55
Table 1: Performance comparison of UniFashion and baseline models on the FashionGen dataset for cross-modal
retrieval tasks.
ModelImage Captioning
BLEU-4 METEOR ROUGE-L CIDEr
FashionBERT 3.30 9.80 29.70 30.10
OSCAR 4.50 10.90 30.10 30.70
KaleidoBERT 5.70 12.80 32.90 32.60
FashionViL 16.18 25.60 37.23 39.30
FAME-ViL 30.73 25.04 55.83 150.4
UniFashion 35.53 29.32 54.59 169.5
Table 2: The Performance of UniFashion in the image
captioning task on the FashionGen dataset.
4.2 Datasets
We test the effectiveness of UniFashion by experi-
menting on different tasks including fashion image
captioning, cross-modal retrieval, composed image
retrieval and composed image generation.
We use the FashionGen and FshaionIQ (Lin
et al., 2014) datasets for retrieval tasks. Fashion-
Gen contains 68k fashion products accompanied
by text descriptions. Each product includes 1 - 6
images from different angles, resulting in 260.5k
image-text pairs for training and 35.5k for testing.
Fashion-IQ contains 18k training triplets (that is,
reference image, modifying text, target image) and
6k validation triplets over three categories: Dress,
Shirt, and Toptee. Each pair (reference image, tar-
get image) is manually annotated with two modify-
ing texts, which are concatenated.
For fashion image captioning tasks, we utilize
the FashionGen (Zang et al., 2021) dataset. Ad-
ditionally, to enhance our model’s capability in
the CIR task, which involves the ability to re-
trieve captions for target images, we have annotated
images from the training set of Fashion-IQ. Rec-
ognizing that manually annotating all the images
would be time-consuming and resource-intensive,
we draw inspiration from the success of recent
MLLM models such as LLaV A in text-annotation
tasks, and propose leveraging LLaV A 1.5 (13B)
to semi-automatically annotate the dataset. Moredetails can be found in Appendix C.
4.3 Evaluation Methods
We compare our models with previous state-of-the-
art methods on each task. For extensive and fair
comparisons, all prior competitors are based on
large-scale pre-trained models.
Cross-modal Retrieval Evaluation. We con-
sider both image-to-text retrieval and text-to-image
retrieval with random 100 protocols used by pre-
vious methods. 100 candidates are randomly sam-
pled from the same category to construct a retrieval
database. The goal is to locate the positive match
depicting the same garment instance from these
100 same-category negative matches. We utilize
Recall@K as the evaluation metric, which reflects
the percentage of queries whose true target ranked
within the top K candidates.
Fashion Image Captioning Evaluation. For
evaluating the performance of caption generation,
we utilize BLEU-4, METEOR, ROUGE-L, and
CIDEr as metrics.
Composed Fashion Image Retrieval Evaluation.
We compare our UniFashion with CIR methods
and the FAME-ViL model of V + L that is oriented
towards fashion in the original protocol used by
Fashion-IQ. For this task, we also utilize Recall@K
as the evaluation metric.
Composed Fashion Image Generation Evalua-
tion. We compare our UniFashion with try-on
methods on VITON-HD dataset and fashion design
works on MGD dataset. To evaluate the quality
of image generation, we use the Frechet Inception
Distance (FID) score to measure the divergence
between two multivariate normal distributions and
employ the CLIP Score (CLIP-S) provided in the
TorchMetrics library to assess the adherence of the
7ModelModalities Metrics
Text Sketch Pose Cloth FID ↓ KID↓ CLIP-S
try-on task
VITON-HD (Choi et al., 2021) ✓ ✓ 12.12 3.23 -
Paint-by-Example (Yang et al., 2023a) ✓ ✓ 11.94 3.85 -
GP-VTON (Xie et al., 2023) ✓ ✓ 13.07 4.66 -
StableVITON (Kim et al., 2024) ✓ ✓ 8.23 0.49 -
UniFashion (Ours) ✓ ✓ 8.42 0.67 -
fashion design task
SDEdit (Meng et al., 2021) ✓ ✓ ✓ 15.12 5.67 28.61
MGD (Baldrati et al., 2023b) ✓ ✓ ✓ 12.81 3.86 30.75
UniFashion (Ours) ✓ ✓ ✓ 12.43 3.74 31.29
Table 3: Performance analysis of unpaired settings on the VITON-HD and MGD datasets across different input
modalities.
ModelDress Shirt Toptee Average
R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50 Avg.
FashionVLP (Goenka et al., 2022) 32.42 60.29 31.89 58.44 38.51 68.79 34.27 62.51 48.39
CASE (Levy et al., 2023) 47.44 69.36 48.48 70.23 50.18 72.24 48.79 70.68 59.74
AMC (Zhu et al., 2023b) 31.73 59.25 30.67 59.08 36.21 66.06 32.87 61.64 47.25
CoVR-BLIP (Ventura et al., 2024) 44.55 69.03 48.43 67.42 52.60 74.31 48.53 70.25 59.39
MGUR (Chen et al., 2022) 32.61 61.34 33.23 62.55 41.40 72.51 35.75 65.47 50.61
LinCIR (Gu et al., 2024) 38.08 60.88 46.76 65.11 50.48 71.09 45.11 65.69 55.4
CMAP (Li et al., 2024) 36.44 64.25 34.83 60.06 41.79 69.12 37.64 64.42 51.03
CLIP4CIR (Baldrati et al., 2023a) 33.81 59.40 39.99 60.45 41.41 65.37 38.32 61.74 50.03
FAME-ViL (Han et al., 2023) 42.19 67.38 47.64 68.79 50.69 73.07 46.84 69.75 58.29
TG-CIR (Wen et al., 2023) 45.22 69.66 52.60 72.52 56.14 77.10 51.32 73.09 58.05
Re-ranking (Liu et al., 2023b) 48.14 71.43 50.15 71.25 55.23 76.80 51.17 73.13 62.15
SPRC (Bai et al., 2023) 49.18 72.43 55.64 73.89 59.35 78.58 54.92 74.97 64.85
UniFashion w/o cap 49.65 72.17 56.88 74.12 59.29 78.11 55.27 74.80 65.04
UniFashion w/o img 32.49 49.11 44.70 59.63 43.16 60.26 40.12 56.33 48.22
UniFashion 53.72 73.66 61.25 76.67 61.84 80.46 58.93 76.93 67.93
Table 4: Comparative evaluation of UniFashion and variants and baseline models on the Fashion-IQ dataset for
composed image retrieval task. Best and second-best results are highlighted in bold and underlined, respectively.
Model CMR CIR FIC FIG
Base 87.38 64.76 - -
Base+LLM 87.49 65.04 36.21 -
Base+LLM w/ cap 87.49 66.83 36.21 -
Base+LLM+diff. 87.55 67.93 35.53 12.43
Table 5: Ablation study and analysis of UniFash-
ion across FashionGen, Fashion-IQ, and VITON-HD
Datasets. Metrics reported include average image-to-
text and text-to-image recall for cross-modal retrieval
(CMR), average recall for composed image retrieval
(CIR), BLEU-4 for Fashion Image Captioning, and FID
for Fashion image generation (FIG).
image to the textual conditioning input (for fashion
design task).
4.4 Comparative Analysis of Baselines and
Our Method
UniFashion exhibits superior performance
across all datasets compared to baselines. Tab. 1
presents the evaluation results for each baseline
and our models in FashionGen data sets for cross-
modal retrieval. UniFashion outperforms most of
the baseline models on both the text-to-image and
image-to-text tasks. Following FAME-ViL, wealso adopt a more challenging and practical pro-
tocol that conducts retrieval on the entire product
set, which is in line with actual product retrieval
scenarios. In Tab. 2, we performed a comparison
between our UniFashion and other baselines on the
FashionGen dataset for the image captioning task.
By integrating the powerful generative ability of
the LLM, our model performed significantly better
than the traditional multimodal models in this task.
In Tab. 4, we conducted a comparison between
our UniFashion and CIR-specialist methods. Our
findings are in line with those of Tab. 1.
After fine-tuning UniFashion on image gen-
eration/editing tasks with multimodal inputs, it
exhibits outstanding performance. Tab. 3 evalu-
ates the quality of the generated image of UniFash-
ion in the VITON-HD unpaired setting. In order
to verify that our model can achieve good results
in a variety of modal inputs, we have conducted
tests, respectively, on the traditional try-on task and
the fashion design task proposed in MGD. For a
fair evaluation with baselines, all the models are
trained at a 512 ×384 resolution. To confirm the
efficacy of our approach, we assess the realism us-
8ing FID and KID score on all the tasks and using
CLIP-S score for fashion design task. As can be
seen, the proposed UniFashion model consistently
outperforms competitors in terms of realism (i.e.,
FID and KID) and coherence with input modali-
ties (i.e., CLIP-S), indicating that our method can
better encode multimodal information. Meanwhile,
although our model is slightly lower than Stable-
VITON on the try-on task, this is because we froze
the parameters of the diffusion model on the try-on
task and only fine-tuned the Q-former part, but it
can still achieve top2 results. The visual results can
be found in Appendix E.
4.5 Ablation Study
UniFashion allows for more flexible execution
of multimodal composed tasks. In Tab. 4, we
also carry out ablation studies on different retrieval
methods. Since UniFashion is capable of generat-
ing captions, for the CIR task, we initially utilize
UniFashion to generate the captions of candidate
images and then conduct the image retrieval task
(denoted as UniFashion w/o cap) and the caption
retrieval task (denoted as UniFashion w/o img).
We find that our single-task variant has already
achieved superior performance in the relevant field.
Furthermore, due to the generative ability of our
model, the pregenerated candidate library opti-
mizes the model’s performance in this task. For
specific implementation details, please refer to Ap-
pendix C.
We investigate the impact of different mod-
ules in UniFashion on various fashion tasks. In
Tab. 5, we perform an ablation study on the pro-
posed model architecture, with a focus on LLM
and diffusion models. For comparison on the cross-
modal retrieval task (CMR), we design the base
model as directly fine-tuning BLIP2 without any
new modules. The results indicate that the base
model performs relatively well on this task and
that the introduction of other modules does not
lead to significant improvements. However, in the
CIR task, the introduction of LLM and diffusion
models as supervision can lead to significant im-
provements, especially when utilizing pregenerated
captions by UniFashion to assist in retrieval, re-
sulting in greater benefits. At the same time, we
note that, after introducing the diffusion model, it
may have some negative impact on the model’s
image captioning ability, possibly due to the inher-
ent alignment differences between LLM and the
diffusion model.5 Conclusion
We have introduced UniFashion, a unified frame-
work designed to tackle challenges in multimodal
generation and retrieval within the fashion domain.
By integrating embedding and generative tasks us-
ing a diffusion model and LLM, UniFashion en-
ables controllable, high-fidelity generation, signifi-
cantly outperforming previous single-task state-of-
the-art models across various fashion tasks. Our
model’s adaptability in handling complex vision-
language tasks demonstrates its potential to en-
hance e-commerce scenarios and fashion-related
applications. This study highlights the importance
of exploring the learning synergy between multi-
modal generation and retrieval, offering a promis-
ing direction for future research in the fashion do-
main.
Limitations
In this section, we discuss limitations of our work
and offer further insights into research within the
fashion domain.
Computational Requirements. UniFashion in-
tegrates multiple complex modules, including Q-
Former, LLM, and diffusion models, which result
in higher computational complexity during training.
However, during the inference stage, the compu-
tational complexity of UniFashion is comparable
to that of current state-of-the-art models. For re-
trieval tasks, only the Q-Former module is needed
to calculate the similarity between the input image
or text and the pre-stored candidate features in the
database, eliminating the need to utilize the LLM
and diffusion model components for inference. For
composed image generation tasks, such as fashion
design, our model relies on diffusion processes,
which may take longer. In our experiments, we
tested the performance of our model on an A100
(80G) GPU. During inference, using 1000 exam-
ples from the VITON-HD dataset, UniFashion took
approximately 3.15 seconds per image generation.
We believe exploring more efficient sampling meth-
ods, such as DPM-Solver++ (Lu et al., 2022), could
improve the overall efficiency of UniFashion.
Acknowledgements
We thank the anonymous reviewers for their valu-
able feedback. This research was partially sup-
ported by the grant of HK ITF ITS/359/21FP.
9References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in Neural
Information Processing Systems , 35:23716–23736.
Yang Bai, Xinxing Xu, Yong Liu, Salman Khan, Fa-
had Khan, Wangmeng Zuo, Rick Siow Mong Goh,
and Chun-Mei Feng. 2023. Sentence-level prompts
benefit composed image retrieval. arXiv preprint
arXiv:2310.05473 .
Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and
Alberto Del Bimbo. 2022. Effective conditioned and
composed image retrieval combining clip-based fea-
tures. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages
21466–21474.
Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and
Alberto Del Bimbo. 2023a. Composed image re-
trieval using contrastive learning and task-oriented
clip-based features. ACM Transactions on Multime-
dia Computing, Communications and Applications ,
20(3):1–24.
Alberto Baldrati, Davide Morelli, Giuseppe Cartella,
Marcella Cornia, Marco Bertini, and Rita Cucchiara.
2023b. Multimodal garment designer: Human-
centric latent diffusion models for fashion image
editing. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 23393–
23402.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.
2021. Beit: Bert pre-training of image transformers.
InInternational Conference on Learning Representa-
tions .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Yiyang Chen, Zhedong Zheng, Wei Ji, Leigang Qu, and
Tat-Seng Chua. 2022. Composed image retrieval
with text feedback via multi-grained uncertainty reg-
ularization. arXiv preprint arXiv:2211.07394 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Seunghwan Choi, Sunghyun Park, Minsoo Lee, and
Jaegul Choo. 2021. Viton-hd: High-resolution vir-
tual try-on via misalignment-aware normalization. In
Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 14131–
14140.Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. 2023. In-
structblip: Towards general-purpose vision-language
models with instruction tuning.
Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi,
Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun,
Hongyu Zhou, Haoran Wei, et al. 2023. Dreamllm:
Synergistic multimodal comprehension and creation.
arXiv preprint arXiv:2309.11499 .
Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, and Xiao-
Ming Wu. 2023. Towards llm-driven dialogue state
tracking. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 739–755.
Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng
Li, Yi Wei, Yi Hu, and Hao Wang. 2020. Fashion-
bert: Text and image matching with adaptive loss for
cross-modal retrieval. In Proceedings of the 43rd
International ACM SIGIR Conference on Research
and Development in Information Retrieval , pages
2251–2260.
Sonam Goenka, Zhaoheng Zheng, Ayush Jaiswal,
Rakesh Chada, Yue Wu, Varsha Hedau, and Pradeep
Natarajan. 2022. Fashionvlp: Vision language trans-
former for fashion retrieval with feedback. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 14105–14115.
Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen
Qian, and Liqing Zhang. 2023. Taming the power
of diffusion models for high-quality virtual try-on
with appearance flow. In Proceedings of the 31st
ACM International Conference on Multimedia , pages
7599–7607.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv
Batra, and Devi Parikh. 2017. Making the v in vqa
matter: Elevating the role of image understanding
in visual question answering. In Proceedings of the
IEEE conference on computer vision and pattern
recognition , pages 6904–6913.
Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon
Kang, and Sangdoo Yun. 2024. Language-only train-
ing of zero-shot composed image retrieval. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 13225–13234.
Xiao Han, Xiatian Zhu, Licheng Yu, Li Zhang, Yi-Zhe
Song, and Tao Xiang. 2023. Fame-vil: Multi-tasking
vision-language model for heterogeneous fashion
tasks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages
2669–2680.
Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao
Zhang, Menglong Zhu, Yuan Li, Yang Zhao, and
Larry S Davis. 2017. Automatic spatially-aware fash-
ion concept discovery. In Proceedings of the IEEE
international conference on computer vision , pages
1463–1471.
10Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-
noising diffusion probabilistic models. Advances
in neural information processing systems , 33:6840–
6851.
Jeongho Kim, Guojung Gu, Minho Park, Sunghyun
Park, and Jaegul Choo. 2024. Stableviton: Learning
semantic correspondence with latent diffusion model
for virtual try-on. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition , pages 8176–8185.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
2017. Visual genome: Connecting language and vi-
sion using crowdsourced dense image annotations.
International journal of computer vision , 123:32–73.
Matan Levy, Rami Ben-Ari, Nir Darshan, and Dani
Lischinski. 2023. Data roaming and early fu-
sion for composed image retrieval. arXiv preprint
arXiv:2303.09429 .
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. 2023a. Otter: A
multi-modal model with in-context instruction tuning.
arXiv preprint arXiv:2305.03726 .
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023b. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. arXiv preprint arXiv:2301.12597 .
Junnan Li, Dongxu Li, Caiming Xiong, and Steven
Hoi. 2022. Blip: Bootstrapping language-image pre-
training for unified vision-language understanding
and generation. In International Conference on Ma-
chine Learning , pages 12888–12900. PMLR.
Shenshen Li, Xing Xu, Xun Jiang, Fumin Shen, Zhe
Sun, and Andrzej Cichocki. 2024. Cross-modal at-
tention preservation with self-contrastive learning for
composed query-based image retrieval. ACM Trans-
actions on Multimedia Computing, Communications
and Applications , 20(6):1–22.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In Computer Vision–
ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023a. Visual instruction tuning. arXiv preprint
arXiv:2304.08485 .
Qijiong Liu, Xiaoyu Dong, Jiaren Xiao, Nuo Chen,
Hengchang Hu, Jieming Zhu, Chenxu Zhu, Tetsuya
Sakai, and Xiao-Ming Wu. 2024a. Vector quantiza-
tion for recommender systems: A review and outlook.
arXiv preprint arXiv:2405.03110 .Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiao-Ming
Wu. 2022. Boosting deep ctr prediction with a plug-
and-play pre-trainer for news recommendation. In
Proceedings of the 29th International Conference on
Computational Linguistics , pages 2823–2833.
Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai,
Zhaocheng Du, Xiao-Ming Wu, Zhou Zhao, Rui
Zhang, and Zhenhua Dong. 2024b. Multimodal pre-
training, adaptation, and generation for recommen-
dation: A survey. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining , pages 6566–6576.
Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney,
and Stephen Gould. 2021. Image retrieval on real-life
images with pre-trained vision-and-language models.
in 2021 ieee. In CVF International Conference on
Computer Vision (ICCV)(2021) , pages 2105–2114.
Zheyuan Liu, Weixuan Sun, Damien Teney, and Stephen
Gould. 2023b. Candidate set re-ranking for com-
posed image retrieval with dual multi-modal encoder.
arXiv preprint arXiv:2305.16304 .
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-
uan Li, and Jun Zhu. 2022. Dpm-solver++: Fast
solver for guided sampling of diffusion probabilistic
models. arXiv preprint arXiv:2211.01095 .
Haoyu Ma, Handong Zhao, Zhe Lin, Ajinkya Kale,
Zhangyang Wang, Tong Yu, Jiuxiang Gu, Sunav
Choudhary, and Xiaohui Xie. 2022. Ei-clip: Entity-
aware interventional contrastive learning for e-
commerce cross-modal retrieval. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 18051–18061.
Chenlin Meng, Yutong He, Yang Song, Jiaming Song,
Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021.
Sdedit: Guided image synthesis and editing with
stochastic differential equations. arXiv preprint
arXiv:2108.01073 .
Niklas Muennighoff, Hongjin Su, Liang Wang, Nan
Yang, Furu Wei, Tao Yu, Amanpreet Singh, and
Douwe Kiela. 2024. Generative representational in-
struction tuning. arXiv preprint arXiv:2402.09906 .
Alexander Quinn Nichol, Prafulla Dhariwal, Aditya
Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mc-
grew, Ilya Sutskever, and Mark Chen. 2022. Glide:
Towards photorealistic image generation and edit-
ing with text-guided diffusion models. In Inter-
national Conference on Machine Learning , pages
16784–16804. PMLR.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea V oss, Alec Radford, Mark Chen, and
Ilya Sutskever. 2021. Zero-shot text-to-image gen-
eration. In International Conference on Machine
Learning , pages 8821–8831. PMLR.
Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. 2022. High-
resolution image synthesis with latent diffusion mod-
els. In Proceedings of the IEEE/CVF Conference
11on Computer Vision and Pattern Recognition , pages
10684–10695.
Negar Rostamzadeh, Seyedarian Hosseini, Thomas Bo-
quet, Wojciech Stokowiec, Ying Zhang, Christian
Jauvin, and Chris Pal. 2018. Fashion-gen: The gen-
erative fashion dataset and challenge. arXiv preprint
arXiv:1806.08317 .
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael
Pritch, Michael Rubinstein, and Kfir Aberman. 2023.
Dreambooth: Fine tuning text-to-image diffusion
models for subject-driven generation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 22500–22510.
Chitwan Saharia, William Chan, Saurabh Saxena,
Lala Li, Jay Whang, Emily L Denton, Kam-
yar Ghasemipour, Raphael Gontijo Lopes, Burcu
Karagol Ayan, Tim Salimans, et al. 2022. Photo-
realistic text-to-image diffusion models with deep
language understanding. Advances in Neural Infor-
mation Processing Systems , 35:36479–36494.
Dustin Schwenk, Apoorv Khandelwal, Christopher
Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022.
A-okvqa: A benchmark for visual question answer-
ing using world knowledge. In European Conference
on Computer Vision , pages 146–162. Springer.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
gpt: Solving ai tasks with chatgpt and its friends in
huggingface. arXiv preprint arXiv:2303.17580 .
Guangyuan Shi, Qimai Li, Wenlong Zhang, Jiaxin Chen,
and Xiao-Ming Wu. 2023. Recon: Reducing conflict-
ing gradients from the root for multi-task learning.
arXiv preprint arXiv:2302.11289 .
Jascha Sohl-Dickstein, Eric Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. 2015. Deep un-
supervised learning using nonequilibrium thermo-
dynamics. In International conference on machine
learning , pages 2256–2265. PMLR.
Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Lucas Ventura, Antoine Yang, Cordelia Schmid, and
Gül Varol. 2024. Covr: Learning composed video
retrieval from web video captions. In Proceedings
of the AAAI Conference on Artificial Intelligence ,
volume 38, pages 5270–5279.
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai
Bai, Zhikang Li, Jianxin Ma, Chang Zhou, JingrenZhou, and Hongxia Yang. 2022a. Ofa: Unifying ar-
chitectures, tasks, and modalities through a simple
sequence-to-sequence learning framework. In Inter-
national Conference on Machine Learning , pages
23318–23340. PMLR.
Wenhui Wang, Hangbo Bao, Li Dong, Johan
Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,
Owais Khan Mohammed, Saksham Singhal, Subhojit
Som, et al. 2022b. Image as a foreign language: Beit
pretraining for all vision and vision-language tasks.
arXiv preprint arXiv:2208.10442 .
Haokun Wen, Xian Zhang, Xuemeng Song, Yinwei Wei,
and Liqiang Nie. 2023. Target-guided composed
image retrieval. In Proceedings of the 31st ACM
International Conference on Multimedia , pages 915–
923.
Chenfei Wu, Shengming Yin, Weizhen Qi, Xi-
aodong Wang, Zecheng Tang, and Nan Duan.
2023. Visual chatgpt: Talking, drawing and edit-
ing with visual foundation models. arXiv preprint
arXiv:2303.04671 .
Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah,
Steven Rennie, Kristen Grauman, and Rogerio Feris.
2021. Fashion iq: A new dataset towards retrieving
images by natural language feedback. In Proceedings
of the IEEE/CVF Conference on computer vision and
pattern recognition , pages 11307–11317.
Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao,
Haoye Dong, Xijin Zhang, Feida Zhu, and Xiaodan
Liang. 2023. Gp-vton: Towards general purpose vir-
tual try-on via collaborative local-flow global-parsing
learning. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition ,
pages 23550–23559.
Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xue-
jin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen.
2023a. Paint by example: Exemplar-based image
editing with diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 18381–18391.
Xuewen Yang, Heming Zhang, Di Jin, Yingru Liu, Chi-
Hao Wu, Jianchao Tan, Dongliang Xie, Jue Wang,
and Xin Wang. 2020. Fashion captioning: Towards
generating accurate descriptions with semantic re-
wards. In Computer Vision–ECCV 2020: 16th Euro-
pean Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XIII 16 , pages 1–17. Springer.
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin
Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,
Ce Liu, Michael Zeng, and Lijuan Wang. 2023b.
Mm-react: Prompting chatgpt for multimodal rea-
soning and action. arXiv preprint arXiv:2303.11381 .
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,
Ming Yan, Yiyang Zhou, Junyang Wang, An-
wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.
mplug-owl: Modularization empowers large lan-
guage models with multimodality. arXiv preprint
arXiv:2304.14178 .
12Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song,
Hao Zhang, and Jindong Chen. 2021. Photochat: A
human-human dialogue dataset with photo sharing
behavior for joint image-text modeling. In Proceed-
ings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 6142–6152.
Haode Zhang, Haowen Liang, Yuwei Zhang, Li-Ming
Zhan, Xiao-Ming Wu, Xiaolei Lu, and Albert Lam.
2022. Fine-tuning pre-trained language models for
few-shot intent detection: Supervised pre-training
and isotropization. In Proceedings of the 2022 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies , pages 532–542.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
2023a. Adding conditional control to text-to-image
diffusion models. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages
3836–3847.
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and
Yu Qiao. 2023b. Llama-adapter: Efficient fine-tuning
of language models with zero-init attention. arXiv
preprint arXiv:2303.16199 .
Xiangyu Zhao, Bo Liu, Qijiong Liu, Guangyuan Shi,
and Xiao-Ming Wu. 2024. EasyGen: Easing mul-
timodal generation with BiDiffuser and LLMs. In
Proceedings of the 62nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 1351–1370, Bangkok, Thailand.
Association for Computational Linguistics.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing
Systems , 36:46595–46623.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023a. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. arXiv preprint arXiv:2304.10592 .
Hongguang Zhu, Yunchao Wei, Yao Zhao, Chunjie
Zhang, and Shujuan Huang. 2023b. Amc: Adaptive
multi-expert collaborative network for text-guided
image retrieval. ACM Transactions on Multime-
dia Computing, Communications and Applications ,
19(6):1–22.
Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo
Jin, Ben Chen, Haoming Zhou, Minghui Qiu, and
Ling Shao. 2021. Kaleido-bert: Vision-language
pre-training on fashion domain. In Proceedings of
the IEEE/CVF conference on computer vision and
pattern recognition , pages 12647–12657.A Basics of Diffusion Models
After the initial proposal of diffusion models
by (Sohl-Dickstein et al., 2015), they have demon-
strated remarkable capacity for generating high-
quality and diverse data. DDPM (Ho et al.,
2020) connects diffusion and score matching mod-
els through a noise prediction formulation, while
DDIM (Song et al., 2020) proposes an implicit gen-
erative model that generates deterministic samples
from latent variables.
Given a data point sampled from a real data dis-
tribution x0∈q(x), during forward diffusion, x0
is gradually “corrupted” at each step tby adding
Gaussian noise to the output of step t-1. It produces
a sequence of noisy samples x1,···,xT. Then,
diffusion models learn to reverse the process:
p(x0:T) =p(xT)TY
t=1pθ(xt−1|xt),
pθ(xt−1|xt) =N(xt−1;µt(xt, t), σ2
tI),(14)
where p(xT) =N(xT; 0,I)is the standard
Gaussian distribution and µt(·)is the parameter-
ization of the predicted mean. Diffusion models
are trained to maximize the marginal likelihood of
the data E[logpθ(x0)], and the canonical objective
is the variational lower bound of logpθ(x0).
Stable Diffusion Model. Latent diffusion models
(LDMs) operate in the latent space of a pre-trained
autoencoder achieving higher computational effi-
ciency while preserving the generation quality. Sta-
ble diffusion model is composed of an autoencoder
with an encoder Eand a decoder D, a conditional
U-Net denoising model ϵθ, and a CLIP-based text
encoder. With the fixed encoder E, an input image
xis first transformed to a lower-dimensional latent
space z0=E(x). The decoder Dperforms the op-
posite operation, decoding z0into the pixel space.
When considering a latent variable zand its noisy
counterpart zt, which is obtained by incrementally
adding noises to zovertsteps, the latent diffusion
models are designed to train the ϵθ(·)to predict the
added noise ϵusing a standard mean squared error
loss:
L:=Ez,ϵ,t[∥ϵ−ϵθ(zt, t)∥2]. (15)
Multimodal Conditional Generation. In the
context of our current work, we have a particular
focus on the pre-trained multimodal latent diffu-
sion models. For a multimodal conditional gen-
13Data types Dataset Size Stage 1 Stage 2 Metrics
CMRFashionGen (Lin et al., 2014) 260.5K"" R@K
Fashion200K (Krishna et al., 2017) 172K"% -
CIR Fashion-IQ (Liu et al., 2023a) 18K%" R@K
FICFashionGen (Liu et al., 2023a) 260.5K"" BLEU,CIDEr,METEOR,ROUGE-L
Fashion-IQ-Cap 60K"% -
FIGVITON-HD (Goyal et al., 2017) 83K%" FID, KID
MGD (Schwenk et al., 2022) 66K%" FID,KID,CLIP-S
Table 6: Description of datasets used in two stages.
Multimodal 
Encoder
Learnable QueriesText Guidance
......
CLIPTSA
yellow dillon -fit floral shirt,  
multicolor nly floral print shirt
Cloth SketchSD Encoder
(copy)
SD EncoderZero Cross -AttnSD Decoder
Zero Cross -Attn
Human feature s
Figure 3: The architecture of UniFashion for fine-tuning on the image editing task. Firstly, we supply the cloth
sketch and text guidance to the multimodal encoder. Then, the diffusion model receives the output of the multimodal
encoder, along with the cloth sketches and human features (i.e., agnostic-mask), to subsequently generate the desired
images.
eration, given a target image x0, the input condi-
tiony0could contain different constraints. The
aim is to model the conditional data distribution
q(x0|y0), where y0contains different modalities
prompts. The conditioning mechanism is imple-
mented by first encoding conditional information,
then the denoising network ϵθconditions on y0via
cross-attention. The label y0in a class-conditional
diffusion model ϵθ(xt|y0)is replaced with a null
label∅with a fixed probability during training.
B Implementation Details
LLM During the first phase, due to the flexibil-
ity brought by the modular architectural design of
BLIP-2, we are able to adapt the model to a broad
spectrum of LLMs. In order to effectively utilize
the capabilities of the existing MLLM models, we
adopted LLaV A-1.5 as the LLM module of the
model. Technically, we leverage LoRA to enable
a small subset of parameters within UniFashion to
be updated concurrently with two layers of adapter
during this phase. Specifically, the lora rank is 128
and lora alpha is 256. We utilize the AdamW opti-mizer with β0= 0.9, β1= 0.99, and weight decay
of 0. The LLMs are trained with a cosine learning
rate of 2e-5 and a warmup rate of 0.03. We use a
batch size of 32 for the tuned LLMs.
Diffusion Module We inherit the autoencoder
and the denoising U-Net of the Stable Diffusion
v1.4. The weights of the U-Net from Paint-by-
Example are used to initialize our denoising U-
Net. To achieve more refined person texture, a
V AE fine-tuned on the VITONHD dataset from
StableVITON is utilized. We train the model using
an AdamW optimizer with a fixed learning rate of
1e-4 for 360k iterations, employing a batch size of
32. For inference, we employ the pseudo linear
multi-step sampler, with the number of sampling
steps set to 50.
C Datasets
For fashion image captioning tasks, we utilize the
FashionGen (Zang et al., 2021) dataset. Addition-
ally, to enhance our model’s capability in the CIR
task, which involves the ability to retrieve captions
14Figure 4: V ocabulary of the frequent words scaled by
frequency for dresses.
for target images, we have annotated images from
the training set of Fashion-IQ. Recognizing that
manually annotating all the images would be time-
consuming and resource-intensive, we draw inspira-
tion from the success of recent MLLM models such
as LLaV A in text-annotation tasks, and propose
leveraging LLaV A 1.5 (13B) to semi-automatically
annotate the dataset. We perform word lemmati-
zation to reduce each word to its root form. Such
pre-processing stage is crucial for the Fashion-IQ
dataset, as the captions do not describe a single gar-
ment but instead express the properties to modify
in a given image to match its target. As shown in
Fig. 4, by analysis of the captions in Fashion-IQ,
we extracted key words that describe clothing in-
formation such as color, sleeve, pattern, lace, etc.,
as prompts for MLLM (LLaV A 1.5). We then in-
structed the model to generate the corresponding
captions referencing words that match the image
features, as shown in Fig. 5. After this process, we
got the captions for Fashion-IQ dataset. The trained
UniFashion from this dataset (Fashion-IQ-cap) can
generate captions for images in the evaluation set of
Fashion-IQ to assist in the CIR task. More results
can be seen in Fig. 6.
D Instruction Formats
Due to the disparity in caption styles across dif-
ferent fashion datasets, we employ diverse instruc-
tions to fine-tune the LLM, enabling it to gener-
ate captions of varying styles. Specifically, the
Fashion200K dataset inclines towards providing
brief descriptions, the FashionGen dataset is prone
to offering professional captions, and in Fashion-
IQ-cap, the captions are detailed. Consequently,
we have designed distinct instructions for different
datasets and tasks, as illustrated in Table 7.
The dress is colorful and has a flowery pattern. It is a long dress with thin 
straps and a fitted design. The dress is not revealing and has a modest 
style. The pattern is not plain, but rather a combination of different 
patterns. The dress is not crocheted and does not have a collar. It is not a 
tighter or looser dress, but rather a fitted dress. The dress is autumn 
colored, and has a vibrant and colorful design.
Please generate a  detailed  caption to describe the  {dress_type} . The 
caption describe the {dress_type}'s style,  color, pattern's style, design and 
other key points. Please select sufficient appropriate words from: 
revealing , conservative, western , eastern, sexy , modest, patterned , plain, 
frilly , simple, crochet, collar, floral , plain, elegant , casual, 
monochromatic , colorful, flowery , plain, shiny , matte, darker , lighter, 
fitted , loose, print , plain, flare, tight , loose...Image:
Prompts:
New Caption:has thin straps and different pattern,
more autumn colored and longerOriginal Caption:Figure 5: Illustration of Instruction-Following Data.
The top section displays an image alongside its original
captions from Fashion-IQ dataset. The bottom section
presents detailed captions generated by LLaV A-1.5. The
original captions are not prompts for generation but
are provided for comparison with the newly generated
caption.
E Visual Results
Figure 3 illustrates the architecture of UniFashion
for fine-tuning on the image editing task. Initially,
we input the cloth sketch and text guidance into
the multimodal encoder. The diffusion model then
receives the output from the multimodal encoder,
along with the cloth sketches and human features
(such as the agnostic mask), to generate the de-
sired images. We compare UniFashion with the
MGD (Baldrati et al., 2023b) model for this task.
In Fig. 7, we compare the images generated by
our approach with the competitor in the VITON-
HD (Choi et al., 2021) paired setting. In Fig. 8, we
show the generation effects of UniFashion in the
VITON-HD unpaired setting. Our method, unlike
the MGD method that employs a warping module
to generate input sketches, directly uses in-shop
garment sketches and is capable of generating im-
ages that align more accurately with the provided
captions and cloth sketches.
15Dataset Instruction
Fashion200K USER:<image>+Short description. Assistant:
FashionGen USER:<image>+Write a detail and professional description for the cloth. Assistant:
Fashion-IQ-cap USER:<image>+Describe the cloth’s style, color, design... and other key points. Assistant:
Table 7: Examples of task instruction templates.
The dress is a dark green, long, and fitted. It has a 
collar and is sleeveless. The dress is made of a 
shiny fabric and has a flowery pattern. The design 
is elegant and the dress is more revealing than 
conservative.
The dress is pink and has a floral pattern. It is a short dress, 
with a fitted top and a flared bottom. The dress is not 
revealing, but it is not conservative either. It is a sexy dress, 
with a collar and a belt. The dress is not crocheted, and it is 
not loose or tight. It is a colorful dress, and it is elegant.
The dress is black and white, with a checkered 
pattern. It is a short dress, revealing and sexy, with 
a fitted style. The dress has a collar and is not 
crocheted. The pattern is smaller and has a more 
floral style. The dress is elegant and casual, with a 
flowery design.
The dress is black and has a gold top half. It is a short dress 
with a fitted top and a flared bottom. The dress is sleeveless 
and has a low neckline, giving it a revealing and sexy 
appearance. The overall style of the dress is elegant and 
sophisticated, making it suitable for formal or semi -formal 
events.
The dress is black with red designs. It is a strapless dress, which is a type of revealing dress. The dress is short, which i s a 
characteristic of a more revealing dress. The dress is patterned, which adds to its visual appeal. The dress is not crocheted , and it 
does not have a collar. The dress is flowery, which is a characteristic of a more elegant dress. The dress is shiny, which ad ds to its 
visual appeal. The dress is darker, which is a characteristic of a more revealing dress. The dress is fitted, which is a char acteristic of 
a more elegant dress.
The dress is a short, leopard print dress. It is a fitted dress, 
which is designed to be revealing. The dress is made of a 
shiny material, which adds to its overall style. The animal 
print pattern on the dress is the main focus of the design, 
making it a unique and eye -catching piece.
A blue and white striped dress. The dress is long and has a fitted style. It is a one -piece dress, and the woman is wearing black heels 
with it. The dress is not particularly revealing or conservative, but it is not overly sexy or modest either. It is a simple,  patterned 
dress that is neither floral nor plain. The dress is elegant and casual, and it is made of a shiny material.
The shirt is black and has a pocket and tailored 
button tab. It is a short sleeve shirt with a collar. 
The shirt is made of a fabric that is darker than the 
pocket and button tab. The shirt is designed to be 
conservative and modest, with a simple pattern.
Figure 6: Caption generation results using our method with images from the Fashion-IQ dataset.
Model Types Task Domain Model Main Structure XMR CIRText
GenerationImage
Generation
Cross-modal RetrievalGeneral CLIP (2021) Dual-stream Transfomer "%%%
Fashion FashionBERT (2020) Single-stream Transfomer "%%%
Multimodal LLM General LLaV A (2023) CLIP, LLM %%"%
Composed Image Retrieval General SPRC (2024) CLIP, Qformer %"%%
Conditional DiffusionGeneral ControlNet (2023) Stable diffusion %%%"
Fashion StableVITON (2023) Stable diffusion %%%"
Unified ModelGeneral NExT-GPT (2023) ImageBind, LLM, Diffusion %%""
Fashion FAME-ViL (2023) Dual-stream Transfomer """%
General BLIP2 (2023) CLIP, Qformer, LLM "%"%
Unified Model (Ours) Fashion UniFashion CLIP, Qformer, LLM, Diffusion """"
Table 8: Comparison of different multimodal models. XMR : Cross-modal retrieval tasks; CIR: Compoesd image
retrieval task.
16black geo -print t -
shirt only macy, 
black plus size 
printed t -shirt only 
macy, black colour 
block t -shirt
classic tee, graphic 
tee, mid t -shirt
moss green tank 
top, green women's 
thea tank, green 
high-low trapeze 
top
high-neck blouse, 
purple mock -neck 
blouse, chlo\u00e9 
blouse
green lace -up 
jersey blouse, green 
and long sleeves, 
green long sleevesCaptionsUniFashion -
GeneratedCloth Sketch MGD -Generated Ground Truth Agnostic -mask
black long -sleeved 
lace top, black high 
neck lace, vero moda 
black high neck 
blouseFigure 7: Qualitative comparison on VITON-HD paired test set. From left to right: agnostic-mask image, caption,
cloth sketch, MGD-generated image, UniFashion (ours)-generated image and ground truth. Our method is capable
of generating images that align more accurately with the given captions and cloth sketch. For optimal viewing,
please zoom in.
17short -sleeve top only 
macy, sheer t -shirt, 
orange slub tee
black long sleeve 
eyelash lace top, 
black long -sleeved 
lace top, long sleeve 
lace
high-neck top, long -
sleeve top, silver 
high neck jersey topReference Image Agnostic -mask Cloth Sketch CaptionsUniFashion -
Generated
MGD -Generated
black petite printed 
mock -neck top only 
macy, blue floral -
print top, green ray 
floral -printed blouse
white long -sleeve 
plisse, front long 
sleeve bardot, only 
white and long 
sleeves
white petite t -shirt 
only macy, white 
perforated leather 
front tee, white 
detail teeFigure 8: Qualitative comparison on VITON-HD unpaired test set. From left to right: original image, agnostic-mask
image, captions, MGD input sketch, MGD-generated image, UniFashion input sketch and UniFashion (ours)-
generated image. Our model is capable of generating images that align more accurately with the provided captions
and cloth sketch. For optimal viewing, please zoom in.
18