Distributional Properties of Subword Regularization
Marco Cognetta
Tokyo Institute of Technology
cognetta.marco@gmail.comVilém Zouhar
ETH Zürich
vzouhar@inf.ethz.chNaoaki Okazaki
Tokyo Institute of Technology
okazaki@c.titech.ac.jp
Abstract
Subword regularization, used widely in NLP,
improves model performance by reducing the
dependency on exact tokenizations, augment-
ing the training corpus, and exposing the model
to more unique contexts during training. BPE
and MaxMatch, two popular subword tokeniza-
tion schemes, have stochastic dropout regular-
ization variants. However, there has not been
an analysis of the distributions formed by them.
We show that these stochastic variants are heav-
ily biased towards a small set of tokenizations
per word. If the benefits of subword regular-
ization are as mentioned, we hypothesize that
biasedness artificially limits the effectiveness
of these schemes. Thus, we propose an algo-
rithm to uniformly sample tokenizations that
we use as a drop-in replacement for the stochas-
tic aspects of existing tokenizers, and find that
it improves machine translation quality.
1 Introduction
Tokenization is the first stage in almost all natural
language processing pipelines, where raw text is
transformed into a format that is understood by the
model. Modern neural models use subword tok-
enization, which represents text as a sequence of
subword units drawn from a subword vocabulary
(e.g., decompositional →de composition al ).
Popular subword tokenization schemes are BPE
(Sennrich et al., 2016), MaxMatch/WordPiece (Wu
et al., 2016), and UnigramLM (Kudo, 2018). Un-
intentionally, the downstream models are thus not
conditioned on the raw text, but rather the exact
tokenization of the text. During training, subword
regularization (where static tokenizations are re-
placed with sampled tokenizations) is often used
to break the dependency on the exact tokenization.
It also serves as data augmentation, and improves
performance in a variety of downstream tasks.
There are two main types of stochastic tokeniz-
ers: those which learn a distribution from text (e.g.,BPE-Dropout p= 0.1
to ken ization 97.77%
to ke n ization 1.89%
to k en ization 0.25%
to ken iz ation 0.04%
t oken ization 0.03%
to k en iz ation 0.01%
to ke n iz ation 0.01%
to ken i z ation < 0.01%MaxMatch-Dropout p= 0.3
to ken ization 34.29%
t oken ization 14.66%
to ke n ization 10.48%
to ken iz ation 7.21%
t oke n ization 4.39%
to k en ization 3.15%
t oken iz ation 3.05%
to ke n iz ation 2.14%
Example 1: The most frequently observed tokenizations
of the word tokenization and their empirical frequen-
cies with BPE-Dropout and MaxMatch-Dropout.
UnigramLM) and those which inject randomness
by corrupting the tokenization scheme (e.g., BPE-
Dropout, Provilkov et al., 2020 and MaxMatch-
Dropout, Hiraoka, 2022). In our work, we focus on
the latter, for which no prior study of the resulting
distributions exists. BPE- and MaxMatch-Dropout
add randomness post hoc into the underlying de-
terministic tokenization, and the distributions they
produce are essentially unrelated to the text distri-
bution. We find that these distributions are heavily
biased, in that they do not produce uniform tok-
enization distributions (see Example 1).
Despite them working well in practice, there is
no reason to believe that the distributions formed
by BPE- and MaxMatch-Dropout are “good” for
training. However, there are reasons to believe that
a different strategy, uniform sampling, would be
better for training, as it would increase the amount
of regularization and augmentation injected into the
training process. We experiment with replacing the
stochastic aspects of BPE- and MaxMatch-Dropout
with one which samples uniformly at random from
all possible tokenizations, and find that it improves
modeling quality on several translation tasks.
2 Motivation
Though stochastic tokenization is known to im-
prove model quality, it remains unclear which
tokenization distribution is the best. BPE- and
MaxMatch-Dropout, which induce unlearned dis-arXiv:2408.11443v1  [cs.CL]  21 Aug 2024tributions (probabilities that are not chosen by a
learning algorithm), are a natural way of inject-
ing randomness into the underlying tokenization
algorithm post hoc . However, empirically, we see
that they both induce heavily biased distributions,
and hypothesize that an unbiased stochastic tok-
enizer would be universally better. This hypothesis
is based on three explanations for subword regular-
ization’s effectiveness:
1) Regularization Subword regularization regular-
izes the model by breaking the dependency on a
single, canonical tokenization. As shown in Exam-
ple 1, BPE- and MaxMatch-Dropout allocate most
of their probability mass to only a few tokeniza-
tions for a given input. A tokenizer that uniformly
samples from the distribution will expose the model
to a greater variety of unique tokenizations of the
same input text during training.
2) Augmentation Subword regularization acts as
data augmentation by increasing the number of
unique inputs that are seen during model train-
ing. An unbiased tokenization sampler will pro-
duce more unique tokenizations of the same input
than a biased sampler.
3) Efficiency Subword regularization increases the
tokenizer’s efficiency in the information-theoretic
sense,1which is a quality shown to be well
correlated with downstream task performance
(Gutierrez-Vasques et al., 2021; Zouhar et al.,
2023). A tokenizer with unbiased sampling will
generally have higher efficiency than a biased one.
3 Subword Tokenization
Subword tokenizers are typically deterministic in
that the same character sequence will result in the
same tokenized output sequence. Stochastic vari-
ants were developed to allow for sampling tokeniza-
tions, which has been shown to improve model
quality and robustness in a variety of NLP tasks.
We briefly introduce three common subword to-
kenization schemes and their stochastic variants,
which all share the same formalization.
Formalization. Deterministic tokenization maps
words w∈Σ+to a sequence of subwords from a
finite vocabulary Σ⊆ V ⊂ Σ∗ast(w)∈ V+.
An important part of tokenization is that it is
lossless—a tokenization of an input can be in-
verted to recover the original word. For example,
1Rényi efficiency is defined as Hα(pV)/log(|V|), where
Hαis Rényi entropy, Vis a subword vocabulary, and pV(w)
is the unigram probability of subword in the tokenized corpus.Inputs : Word w∈Σ+, (Ordered) Merges µ
Output : Tokenized sequence t∈ V+
1:φ← ⟨(wi, wi+1)|(wi, wi+1)∈µ∧RAND()> p⟩
2:forφ̸=∅do
3: (x, y)←arg maxµφ ▷ Ordered by µ
4: w←REPLACE ((x, y)→xy, w )
5: φ← ⟨(wi, wi+1)|(wi, wi+1)∈µ∧RAND()> p⟩
6:return w
Algorithm 1: BPE Inference ( with dropout)
Inputs : Word w∈Σ+, V ocabulary V
Output : Tokenized sequence t∈ V+
1:t←[ ], i←1
2:while i≤ |w|do
3: z←wi
4: forj∈1. . .max v∈V|v|do
5: ifwi ... i +j∈ V∧RAND()> pthen
6: z←wi ... i +j
7: tappend← − − − z, i←i+|z|
8:return t
Algorithm 2: MaxMatch Inference ( with dropout)
t(tokenization )=to ken ization but impor-
tantly t−1(to ken ization )=tokenization .
In contrast, stochastic tokenization is not a one-
to-one mapping, but rather a probability distribu-
tion function Twfor each word w. This assigns
each tokenization ¯wa probability Tw( ¯w)∈[0,1].
Continuing Example 1, Tw(to ken ization )=
0.98 and Tw(to ke n ization )= 0.02. During
application of the tokenizer, the specific tokeniza-
tion of wis sampled from the distribution Tw.
BPE and Dropout. BPE forms a vocabulary by
iteratively merging the most frequently cooccurring
pair of tokens in the corpus2(Sennrich et al., 2016).
During inference, the sequence of learned merges
is applied greedily to new text (Algorithm 1). To
implement dropout, a probability pis introduced,
and the highlighted statement randomly removes
candidate merges (Provilkov et al., 2020).
MaxMatch and Dropout. Given a subword vo-
cabulary,3MaxMatch tokenizes text from left to
right by iteratively selecting the longest match-
ing subword, shown in Algorithm 2. MaxMatch-
Dropout randomly discards matching subwords and
falls back to shorter ones via thecondition on Line
5 (Hiraoka, 2022).
UnigramLM. UnigramLM (Kudo, 2018) intro-
duced the concept of subword regularization. It
learns a vocabulary and unigram probabilities for
each token in the vocabulary according to some
2Appendix B, Algorithm 4.
3We use the standard WordPiece training algorithm as
described by (Schuster and Nakajima, 2012).loss function over the training corpus. The tok-
enization of an input text is sampled from the prob-
ability distribution induced by the model and some
temperature α.4We do not explore the distribu-
tional properties of UnigramLM here, since they
are highly corpus dependant.
Dropout Distributions. Example 1 and Ap-
pendix E show empirical probabilities for cer-
tain values of pin BPE- and MaxMatch-Dropout.
While BPE- and MaxMatch-Dropout were not de-
signed to form (or even claimed to be) unbiased
distributions, here we concretely show that their
distributions are biased, under mild conditions.
Lemma 3.1. LetB= (V, µ)be a BPE tokenizer
such that there exists (a, b),(b, b),(b, c)∈µwith
(a, b)>µ(b, b)>µ(b, c)andabb, bbc, abbc / ∈ V.
Then, there exists a word w∈Σ+for which the
distribution of the dropout tokenizer B′(w)is non-
uniform for any p.
Lemma 3.2. LetMbe a MaxMatch tokenizer over
vocabulary V, such that Σ⊂ V and there exists a
token v∈ V \ Σwhich is a proper prefix of some
other token z=vy∈ V. Then, there exists a word
w∈Σ+for which the distribution of the dropout
tokenizer M′(w)is non-uniform for any p.
0 1a2b3a4b5c
(a) An automaton Arepresenting ababc .
3
5c:#bc
4b:#ab
0 2a:a
b:b
c:c
1a:εb:ε
a:ε
a:#a
b:#b
c:#cb:abb:ε
a:εa:#a
b:#b
c:#c
(b) A transducer Tfor the subword vocabulary
{a, b, c, ab, #a, #b, #c, #ab, #bc} .
0
2ab1a
5#ab
4#a3#b
#ab
#a
6#c#b
#bc
(c) A lattice, A ◦ T , of all possible tokenizations of ababc .
Figure 1: Uniformly sampling tokenizations from A◦T .
4Note that α= 0yields the uniform distribution, but this
would sample a tokenization of the entire sentence, rather than
word-by-word, which harms model quality.Inputs : Sentence s, Tokenizer T, Probability p
Output : Tokenized sequence t∈ V+
1:t←[ ]
2:forw∈sdo
3: ifRAND()< pthen
4: textend← − − − UNIFORM SAMPLE (w)
5: else
6: textend← − − − T(w)
7:return t
Algorithm 3: Uniform Sampling Tokenization.
4 Uniformly Sampling Tokenizations
Given a subword vocabulary, we first produce a
character-to-subword finite-state transducer repre-
senting it. Encoding an input word as a linear finite-
state automaton and composing it with this trans-
ducer produces a lattice which encodes all possible
tokenizations. Since the word length is finite, this
lattice must be acyclic, and we can sample paths
from it using Algorithm 6 (Lavrov, 2018). An ex-
ample of the transducer construction, composition,
and sampling is shown in Figure 1.
Given a baseline BPE or MaxMatch tokenizer,
we implement our uniform sampling tokenizer by
constructing a subword transducer from its sub-
word vocabulary and selecting a dropout proba-
bility p. During training, a word is tokenized via
uniform sampling with probability pand via the
deterministic tokenizer with probability 1−p, as
shown in Algorithm 3.
One of the reasons for the success of subword
regularization is that they expose the model to a
more diverse set of tokenizations (Section 2). Fig-
ure 2 shows that across any choice of p, even with
far fewer samples, a much more diverse set of tok-
enizations for a given word is observed when using
Uniform Sampling compared to Dropout, indicat-
ing that a model will be exposed to a far greater
number of unique contexts during training.
10
100
1000
10000SamplesBPE-Dropout
 MaxMatch-Dropout
0 1Dropout p10
100
1000
10000SamplesBPE-Uniform
0 1Dropout pMaxMatch-Uniform
One tokenization observed All tokenizations observed
Figure 2: The number of unique, observed tokenizations
of a word with Nsamples and dropout p.5 Experiments
We use English ↔German, English ↔Romanian,
and English ↔French as our translation tasks. For
each language pair, we train a baseline BPE and
MaxMatch tokenizer with the same vocabulary size
and use them to build Dropout and Uniform Sam-
pler variants so that the vocabulary between a base-
line tokenizer and its stochastic variants is exactly
the same and only the tokenization distribution is
different. We include a UnigramLM tokenizer with
the same vocabulary size as a learned-distribution
baseline.We use the same underlying transformer
model (Appendix A) for each language pair, and
only change the embedding and decoding layers,
according to the choice of tokenizer. We compare
tokenizer efficiency (via tokenization-scorer ),
BLEU (Papineni et al., 2002; Post, 2018), CHRF
(Popovi ´c, 2015), and COMET DA-22 (Rei et al.,
2022) by averaging the results of three experimen-
tal runs per model. We use p= 0.1for BPE-
Dropout, p= 0.3for MaxMatch-Dropout, and
α= 0.3for UnigramLM. For Uniform Sampling,
we use p= 0.1and0.25, which were chosen as an
estimate of the frequency that a non-canonical tok-
enization of word in BPE- and MaxMatch-Dropout
was sampled, respectively. Thus, we should ex-
pect Uniform Samplers to have roughly the same
amount of non-canonically-tokenized-words in a
corpus as BPE- and MaxMatch-Dropout, so the
salient difference is the variety of tokenizations.
The results are shown in Table 1. In nearly ev-
ery translation metric, Uniform Sampling outper-
forms BPE- and MaxMatch-Dropout. However,
curiously, Uniform Sampling does not always have
higher efficiency than BPE- or MaxMatch-Dropout
(but is always higher than the baseline), as Uni-
form Sampling guarantees maximum entropy at
theword -tokenization level, which does not neces-
sarily translate to the global -tokenization entropy.
There is only one metric (EN →DE, BPE, CHRF)
where a Uniform Sampling model is not the best.
However, in that same case, the Uniform Sam-
pler improved upon the BPE-Dropout model by
0.8 BLEU, which is nearly as much as the BPE-
Dropout improved upon the BPE baseline. In ad-
dition, the +0.61 increase in COMET DA-22 corre-
sponds to a 82% agreement accuracy with humans
(Kocmi et al., 2024). In the EN →RO pair, Uniform
Sampling models were the best across all metrics
and underlying tokenizers. Further, Unigram Sam-
pling consistently outperforms UnigramLM bothin terms of raw translation quality metrics and im-
provement over the deterministic baseline. These
results extend to our full experimental results (Ap-
pendix D) and support our hypothesis that an unbi-
ased tokenizer should generally outperform biased
dropout tokenizers.
Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4636 28.44 55.80 76.22
BPE + Dropout ( p=0.1) 0.4747 29.37 56.63 77.51
BPE + Uniform ( p=0.1) 0.4731 30.05 56.37 78.12
BPE + Uniform ( p=0.25) 0.4719 30.16 56.47 78.08
MaxMatch 0.4584 28.41 55.97 76.57
MaxMatch + Dropout ( p=0.3) 0.4530 29.13 56.43 77.38
MaxMatch + Uniform ( p=0.1) 0.4657 29.18 56.43 77.76
MaxMatch + Uniform ( p=0.25) 0.4633 29.43 56.57 77.62
Unigram ( α=1) 0.4452 28.40 55.93 76.66
Unigram ( α=0.3) 0.3796 28.97 56.33 77.44
(a) English →German (source+target dropout)
Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4524 23.56 53.20 81.03
BPE + Dropout ( p=0.1) 0.4614 23.98 53.70 81.90
BPE + Uniform ( p=0.1) 0.4594 23.83 53.67 82.00
BPE + Uniform ( p=0.25) 0.4647 24.13 53.73 82.20
MaxMatch 0.4476 23.52 53.23 81.17
MaxMatch + Dropout ( p=0.3) 0.4578 23.95 53.70 81.98
MaxMatch + Uniform ( p=0.1) 0.4528 24.32 53.90 82.11
MaxMatch + Uniform ( p=0.25) 0.4563 24.10 53.87 82.06
Unigram ( α=1) 0.4338 23.68 53.37 81.28
Unigram ( α=0.3) 0.4284 24.17 53.87 82.00
(b) English →Romanian (source only dropout)
Table 1: Experimental results for EN →DE and
EN→RO. In each block, we compare a baseline tok-
enizer with its dropout and uniform sampling variants.
Each group has the same vocabulary and differs only
in the tokenization distribution. The best performing
model for each baseline and metric is bolded . The full
results for all languages is in Appendix D.
6 Conclusion
We investigate the distributions induced by BPE-
and MaxMatch-Dropout, two popular subword reg-
ularization schemes. We hypothesize and show
that BPE- and MaxMatch-Dropout are subopti-
mal in that they form heavily biased distributions.
We introduce a Uniform Sampler tokenizer, which
guaranteesuniform distributions and consistently
outperforms BPE- and MaxMatch-Dropout on ma-
chine translation tasks.
Future work. Uniform Sampling is uniform at the
word level, but past research suggests that unifor-
mity at the global unigram level is desired. There-
fore, algorithms could be designed to directly op-
timize global uniformity. Further investigations
should reconcile how both Uniform Sampling and
UnigramLM improve performance despite their op-
posing motivations (higher/lower entropy).Limitations
We did not establish statistical significance for our
results, but note that the trend hold across language
pairs, tokenizers, and metrics. We did not do sub-
stantial hyperparameter searching for vocabulary
size or dropout rates, but rather used values that
commonly appear in the literature. It is possible
that some trends in our results may change with
different choices of tokenization hyperparameters.
We also did not experiment with extremely-low
resource settings (our smallest setting, EN ↔DE
has 150k sentence pairs), or very large settings (our
largest, EN ↔FR, has 2M sentence pairs). Addi-
tionally, in our largest case, the improvement seen
by Uniform Sampling are less consistent and less
significant. However, this is in line with prior re-
search that shows the diminishing effectiveness of
subword regularization as the corpus size increases.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In Implementation and Application of Au-
tomata , pages 11–23, Berlin, Heidelberg. Springer
Berlin Heidelberg.
Ond rej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck, An-
tonio Jimeno Yepes, Philipp Koehn, Varvara Lo-
gacheva, Christof Monz, Matteo Negri, Aurelie
Neveol, Mariana Neves, Martin Popel, Matt Post,
Raphael Rubino, Carolina Scarton, Lucia Specia,
Marco Turchi, Karin Verspoor, and Marcos Zampieri.
2016. Findings of the 2016 conference on machine
translation. In Proceedings of the First Conference
on Machine Translation , pages 131–198, Berlin, Ger-
many. Association for Computational Linguistics.
Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa
Bentivogli, and Marcello Federico. Report on the
11th IWSLT evaluation campaign. In Proceedings
of the 11th International Workshop on Spoken Lan-
guage Translation: Evaluation Campaign .
Ximena Gutierrez-Vasques, Christian Bentz, Olga Sozi-
nova, and Tanja Samardzic. 2021. From characters
to words: the turning point of BPE merges. In Pro-
ceedings of the 16th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Main Volume , pages 3454–3468, Online.
Association for Computational Linguistics.
Tatsuya Hiraoka. 2022. MaxMatch-dropout: Subword
regularization for WordPiece. In Proceedings of the
29th International Conference on Computational Lin-
guistics , pages 4864–4872, Gyeongju, Republic ofKorea. International Committee on Computational
Linguistics.
Tom Kocmi, Vilém Zouhar, Christian Federmann, and
Matt Post. 2024. Navigating the metrics maze: Rec-
onciling score magnitudes and accuracies. Preprint ,
arXiv:2401.06760.
Philipp Koehn. Europarl: A parallel corpus for statisti-
cal machine translation. In Proceedings of Machine
Translation Summit X: Papers .
Taku Kudo. 2018. Subword regularization: Improv-
ing neural network translation models with multiple
subword candidates. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 66–75,
Melbourne, Australia. Association for Computational
Linguistics.
Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.
Misha Lavrov. 2018. A procedure for sampling paths
in a directed acyclic graph. Mathematics Stack Ex-
change.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for
sequence modeling. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics (Demonstrations) ,
pages 48–53, Minneapolis, Minnesota. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Maja Popovi ´c. 2015. chrF: Character n-gram F-score
for automatic MT evaluation. In Proceedings of the
Tenth Workshop on Statistical Machine Translation ,
pages 392–395, Lisbon, Portugal. Association for
Computational Linguistics.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers , pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Ivan Provilkov, Dmitrii Emelianenko, and Elena V oita.
2020. BPE-dropout: Simple and effective subword
regularization. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 1882–1892, Online. Association for
Computational Linguistics.Ricardo Rei, José G. C. de Souza, Duarte Alves,
Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova,
Alon Lavie, Luisa Coheur, and André F. T. Martins.
2022. COMET-22: Unbabel-IST 2022 submission
for the metrics shared task. In Proceedings of the
Seventh Conference on Machine Translation (WMT) ,
pages 578–585, Abu Dhabi, United Arab Emirates
(Hybrid). Association for Computational Linguistics.
Mike Schuster and Kaisuke Nakajima. 2012. Japanese
and korean voice search. In 2012 IEEE International
Conference on Acoustics, Speech and Signal Process-
ing, ICASSP 2012, Kyoto, Japan, March 25-30, 2012 ,
pages 5149–5152. IEEE.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1715–1725,
Berlin, Germany. Association for Computational Lin-
guistics.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,
Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
Klingner, Apurva Shah, Melvin Johnson, Xiaobing
Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,
Taku Kudo, Hideto Kazawa, Keith Stevens, George
Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason
Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
Greg Corrado, Macduff Hughes, and Jeffrey Dean.
2016. Google’s neural machine translation system:
Bridging the gap between human and machine trans-
lation. Preprint , arXiv:1609.08144.
Vilém Zouhar, Clara Meister, Juan Gastaldi, Li Du,
Mrinmaya Sachan, and Ryan Cotterell. 2023. To-
kenization and the noiseless channel. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 5184–5207, Toronto, Canada. Association for
Computational Linguistics.A Training Details
We use fairseq (Ott et al., 2019) for language
modeling, HuggingFace’s tokenizers library for
our underlying BPE and MaxMatch tokenizers, and
OpenFst (Allauzen et al., 2007) for the subword
lattice construction. For UnigramLM, we used Sen-
tencePiece (Kudo and Richardson, 2018). We used
fairseq ’stransformer_iwslt_de_en architec-
ture for EN ↔DE, and the baseline transformer
architecture for EN ↔RO and EN ↔FR. The hyper-
parameters and optimizer configuration are given
in Tables 2, 3, and 4. Our datasets were:
• EN↔DE: 160k, IWSLT14 (Cettolo et al.)
• EN↔RO: 600k, WMT16 (Bojar et al., 2016)
• EN↔FR: 2M, Europarl (Koehn)
V ocabulary Sizes (src, tgt) EN↔DE: (10k, 10k)
Embedding Dimension 512
FFN Dimension 1024
Number of Heads 4
Number of Layers 6
Dropout 0.3
Table 2: The transformer_iwslt_en_de architecture,
used for the English ↔German task.
V ocabulary Sizes (src, tgt)EN↔RO: (14k, 14k)
EN↔FR: (30k, 30k)
Embedding Dimension 512
FFN Dimension 2048
Number of Heads 6
Number of Layers 8
Dropout 0.1
Table 3: The transformer architecture, used for the
English ↔Romainan and English ↔French tasks.
Optimizer ADAM
β1, β2 (0.9,0.98)
Learning Rate 5×10−4
Warmup 4000 steps
Scheduler Inverse Square Root
Tokens-per-batch 8192
PatienceEN↔DE: 8
EN↔RO: 10
EN↔FR: 5
Table 4: The optimizer parameters, used for all tasks.B Algorithms
Inputs : Corpus C, Alphabet Σ, Target size n,
Outputs : V ocabulary V, Merges µ
1:V ← Σ
2:fori∈1. . . n do
3: (x, y)←arg max
a,b∈VCOUNT ((a, b),C)
4: V ← V ∪ { xy}
5: µ←µ∪ ⟨(x, y)⟩
6: C ← REPLACE ((x, y)→xy,C)
7:return V, µ
Algorithm 4: BPE Training.
Inputs : Directed Acyclic Graph D,
Outputs : Path π, Path-probability p
1:π←[ ]
2:p←1
3:CUR←qstart
4:while CUR is not final do
5: (w, q)∼UNIFORM (ADJ(CUR))
6: A PPEND (π,(CUR, w, q))
7: p←p×1
DEG(CUR)
8: CUR←q
9:return π, p
Algorithm 5: Biased DAG Sampling.
Inputs : Directed Acyclic Graph D,
Output : Path π
1:pmin=Q
q∈D1
DEG(q)
2:(π, p)∼BIASED SAMPLE (D)
3:while RAND()>pmin
pdo
4: (π, p)∼BIASED SAMPLE (D)
5:return π
Algorithm 6: Unbiased DAG Sampling.C Proofs
Lemma 3.1. LetB= (V, µ)be a BPE tokenizer
such that there exists (a, b),(b, b),(b, c)∈µwith
(a, b)>µ(b, b)>µ(b, c)andabb, bbc, abbc / ∈ V.
Then, there exists a word w∈Σ+for which the
distribution of the dropout tokenizer B′(w)is non-
uniform for any p.
Proof. Since abb, bbc, abbc / ∈ V , then
(ab, b),(a, bb),(b, bc)/∈µ. Consider the
word abbc. There are 5 possible tokenizations
[a, b, b, c ],[a, b, bc ],[a, bb, c ],[ab, b, c ],[ab, bc ].
We proceed by case analysis and compute the
probability of each, given a dropout probability p.
•[a, b, b, c ]→p3
–Merges (a, b),(b, b), and (b, c)must be
dropped which has probability p×p×p=p3.
•[a, b, bc ]→p2(1−p)
–Merges (a, b), and (b, b)must be dropped,
but not (b, c).
•[a, bb, c ]→p(1−p)
–Merge (a, b)must be dropped but not (b, b).
The merge (b, c)is irrelevant as (b, b)>µ
(b, c).
•[ab, b, c ]→(1−p)p
•[ab, bc ]→(1−p)2
To be uniform, p3=p2(1−p) =p(1−p) =
(1−p)2=1
5, which does not exist. Hence, there
is nopsuch that B′(abbc)is uniform.
Lemma 3.2. LetMbe a MaxMatch tokenizer over
vocabulary V, such that Σ⊂ V and there exists a
token v∈ V \ Σwhich is a proper prefix of some
other token z=vy∈ V. Then, there exists a word
w∈Σ+for which the distribution of the dropout
tokenizer M′(w)is non-uniform for any p.
Proof. Consider the distribution of tokenizations
M′(z), under which the probability of zbeing the
final tokenization is (1−p). Let the total num-
ber of tokenizations be n, and assume the distri-
bution is unbiased. The probability of the tok-
enization [v, y 1, y2, . . . , y k]is(1−p)pk. Thus, the
distribution is only unbiased if (1−p) = (1 −
p)pk=1
n. Since there are at least 3tokenizations
[z],[z1, z2, . . . , z n],and[v, y 1, y2, . . . , y k], this is a
contradiction.
0.00.1 0.91.0 Dropout p0.00.20.40.60.81.0Shannon EfficiencyBPE Dropout
MaxMatch Dropout
BPE/MaxMatch UniformFigure 3: Distribution uniformity measured by Shannon
Efficiency (higher=more uniform; excludes the canoni-
cal form, which usually takes up most of the probability
mass). Our Uniform Sampling versions (both for BPE
and MaxMatch) guarantee balanced sampling of tok-
enizations, which is not true for the standard Dropout
versions whose balance depends non-linearly on the
dropout rate p.D Full Experiments
Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4636 33.66 57.10 79.30
BPE + Dropout ( p=0.1) 0.4747 35.06 58.07 80.51
BPE + Uniform ( p=0.1) 0.4731 35.03 57.97 80.46
BPE + Uniform ( p=0.25) 0.4719 35.22 58.13 80.57
MaxMatch 0.4584 33.85 57.17 79.48
MaxMatch + Dropout ( p=0.3) 0.4530 34.92 57.87 80.37
MaxMatch + Uniform ( p=0.1) 0.4657 35.17 58.10 80.60
MaxMatch + Uniform ( p=0.25) 0.4633 35.32 58.13 80.71
Unigram ( α=1) 0.4452 33.37 56.77 79.43
Unigram ( α=0.3) 0.3796 34.24 57.70 80.31
(a) German →English (source+target dropout)Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4636 28.44 55.80 76.22
BPE + Dropout ( p=0.1) 0.4747 29.37 56.63 77.51
BPE + Uniform ( p=0.1) 0.4731 30.05 56.37 78.12
BPE + Uniform ( p=0.25) 0.4719 30.16 56.47 78.08
MaxMatch 0.4584 28.41 55.97 76.57
MaxMatch + Dropout ( p=0.3) 0.4530 29.13 56.43 77.38
MaxMatch + Uniform ( p=0.1) 0.4657 29.18 56.43 77.76
MaxMatch + Uniform ( p=0.25) 0.4633 29.43 56.57 77.62
Unigram ( α=1) 0.4452 28.40 55.93 76.66
Unigram ( α=0.3) 0.3796 28.97 56.33 77.44
(b) English →German (source+target dropout)
Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4034 40.86 64.57 86.39
BPE + Dropout ( p=0.1) 0.4137 40.96 64.57 86.50
BPE + Uniform ( p=0.1) 0.4139 41.10 64.70 86.52
BPE + Uniform ( p=0.25) 0.4259 40.86 64.57 86.36
MaxMatch 0.4003 41.02 64.70 86.48
MaxMatch + Dropout ( p=0.3) 0.4186 40.88 64.57 86.47
MaxMatch + Uniform ( p=0.1) 0.4094 41.04 64.70 86.54
MaxMatch + Uniform ( p=0.25) 0.4194 40.80 64.50 86.38
Unigram ( α=1) 0.3801 40.59 64.43 86.27
Unigram ( α=0.3) 0.3773 40.71 64.53 86.36
(c) French →English (source only dropout)Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4034 41.27 65.77 86.86
BPE + Dropout ( p=0.1) 0.4137 41.45 65.90 87.08
BPE + Uniform ( p=0.1) 0.4139 41.54 65.93 87.04
BPE + Uniform ( p=0.25) 0.4259 41.35 65.83 87.03
MaxMatch 0.4003 41.38 65.87 87.00
MaxMatch + Dropout ( p=0.3) 0.4186 41.24 65.80 86.95
MaxMatch + Uniform ( p=0.1) 0.4094 41.44 65.93 87.07
MaxMatch + Uniform ( p=0.25) 0.4194 41.22 65.77 86.93
Unigram ( α=1) 0.3801 40.47 65.27 86.36
Unigram ( α=0.3) 0.3773 40.15 65.00 86.08
(d) English →French (source only dropout)
Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4524 30.81 57.30 77.77
BPE + Dropout ( p=0.1) 0.4614 32.13 58.17 79.50
BPE + Uniform ( p=0.1) 0.4594 31.92 58.13 79.64
BPE + Uniform ( p=0.25) 0.4647 31.85 58.23 79.54
MaxMatch 0.4476 31.01 57.23 78.03
MaxMatch + Dropout ( p=0.3) 0.4578 31.90 58.13 79.63
MaxMatch + Uniform ( p=0.1) 0.4528 32.02 58.37 79.81
MaxMatch + Uniform ( p=0.25) 0.4563 31.83 58.33 79.74
Unigram ( α=1) 0.4338 30.34 56.97 77.74
Unigram ( α=0.3) 0.4284 31.53 58.07 79.40
(e) Romanian →English (source only dropout)Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4524 23.56 53.20 81.03
BPE + Dropout ( p=0.1) 0.4614 23.98 53.70 81.90
BPE + Uniform ( p=0.1) 0.4594 23.83 53.67 82.00
BPE + Uniform ( p=0.25) 0.4647 24.13 53.73 82.20
MaxMatch 0.4476 23.52 53.23 81.17
MaxMatch + Dropout ( p=0.3) 0.4578 23.95 53.70 81.98
MaxMatch + Uniform ( p=0.1) 0.4528 24.32 53.90 82.11
MaxMatch + Uniform ( p=0.25) 0.4563 24.10 53.87 82.06
Unigram ( α=1) 0.4338 23.68 53.37 81.28
Unigram ( α=0.3) 0.4284 24.17 53.87 82.00
(f) English →Romanian (source only dropout)
Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4524 30.81 57.30 77.77
BPE + Dropout ( p=0.1) 0.4672 32.48 58.87 80.29
BPE + Uniform ( p=0.1) 0.4615 32.46 58.87 80.26
BPE + Uniform ( p=0.25) 0.4562 32.83 59.07 80.71
MaxMatch 0.4476 31.01 57.23 78.03
MaxMatch + Dropout ( p=0.3) 0.4465 32.89 59.03 80.69
MaxMatch + Uniform ( p=0.1) 0.4544 32.83 58.97 80.36
MaxMatch + Uniform ( p=0.25) 0.4484 33.03 59.13 80.65
Unigram ( α=1) 0.4338 30.29 57.07 0.7774
Unigram ( α=0.3) 0.4061 32.30 58.70 0.8011
(g) Romanian →English (source+target dropout)Tokenizer Efficiency BLEU CHRF COMET
BPE 0.4524 23.56 53.20 81.03
BPE + Dropout ( p=0.1) 0.4672 24.85 54.30 82.71
BPE + Uniform ( p=0.1) 0.4615 24.78 54.30 83.03
BPE + Uniform ( p=0.25) 0.4562 24.77 54.00 82.67
MaxMatch 0.4476 23.52 53.23 81.17
MaxMatch + Dropout ( p=0.3) 0.4465 25.02 54.20 82.67
MaxMatch + Uniform ( p=0.1) 0.4544 24.77 54.40 83.08
MaxMatch + Uniform ( p=0.25) 0.4484 25.16 54.33 83.00
Unigram ( α=1) 0.4338 23.61 53.23 0.8114
Unigram ( α=0.3) 0.4061 24.61 54.13 0.8274
(h) English →Romanian (source+target dropout)
Table 5: The main results of machine translation performance (average across 3 seeds). In almost all cases the
Uniform sampling yields the best results.E Examples of Distributions from Data
BPE-Dropout p=0.1
something 96.50%
some thing 1.60%
so met hing 1.52%
so m et hing 0.16%
so me thing 0.09%
som eth ing 0.03%
s ome thing 0.03%
so m eth ing 0.03%
somet hing 0.01%
some th ing 0.00%MaxMatch-Dropout p=0.3
something 69.85%
somet hing 14.79%
somet hi n g 4.54%
some thing 4.35%
somet h ing 1.31%
some th ing 0.93%
som eth ing 0.91%
some t hing 0.42%
somet h in g 0.38%
some th in g 0.29%BPE-Dropout p=0.1
started 97.56%
star ted 2.15%
star te d 0.11%
start ed 0.07%
st ar ted 0.05%
st art ed 0.02%
s ta r ted 0.02%
star t ed 0.01%
s ta r t ed 0.00%
s t art ed 0.00%MaxMatch-Dropout p=0.3
started 69.97%
start ed 14.88%
start e d 6.21%
star ted 4.37%
star te d 1.33%
st art ed 0.91%
star t ed 0.42%
s ta r ted 0.39%
st art e d 0.39%
st ar ted 0.26%
BPE-Dropout p=0.1
percent 73.54%
per c ent 8.83%
per cent 8.12%
perce nt 7.84%
per ce nt 0.88%
p er c ent 0.18%
p er ce n t 0.12%
p er cent 0.11%
pe r cent 0.10%
per ce n t 0.09%MaxMatch-Dropout p=0.3
percent 69.93%
perce nt 14.61%
perce n t 6.40%
per cent 4.53%
pe r cent 1.33%
per ce nt 0.90%
per ce n t 0.39%
p er cent 0.38%
per c ent 0.37%
pe r ce nt 0.26%BPE-Dropout p=0.1
different 82.44%
dif fe rent 8.74%
diff ere nt 8.05%
differ ent 0.29%
dif fe re nt 0.20%
dif f ere nt 0.13%
dif fer ent 0.07%
d iff ere nt 0.03%
d if fer ent 0.03%
di ff er ent 0.01%MaxMatch-Dropout p=0.3
different 69.95%
differ ent 14.74%
differ en t 4.44%
diff ere nt 3.05%
differ e nt 1.36%
diff ere n t 1.27%
dif fer ent 0.94%
diff er ent 0.94%
differ e n t 0.56%
diff e rent 0.41%
BPE-Dropout p=0.1
together 88.88%
to ge ther 9.94%
to get her 0.95%
tog ether 0.08%
to ge t her 0.07%
to g ether 0.03%
to ge th er 0.03%
tog e ther 0.00%
t og ether 0.00%
tog eth er 0.00%MaxMatch-Dropout p=0.3
together 69.99%
tog ether 14.68%
tog eth er 3.12%
to get her 3.05%
t og ether 1.35%
tog eth e r 1.33%
to ge ther 0.95%
tog et her 0.94%
to get he r 0.90%
t o get her 0.41%BPE-Dropout p=0.1
happening 95.11%
happen ing 1.93%
ha pp ening 1.71%
happ ening 0.91%
ha pp en ing 0.16%
happ en ing 0.08%
h app en ing 0.03%
happ e ning 0.03%
ha pp e ning 0.02%
h app ening 0.01%MaxMatch-Dropout p=0.3
happening 70.05%
happen ing 14.80%
happen in g 4.46%
happ ening 4.26%
happen i n g 1.83%
ha pp ening 0.94%
happ en ing 0.89%
h app ening 0.44%
happ e ning 0.40%
ha p pe ning 0.27%
Example 2: Frequencies of tokenizations of several words sampled from BPE-Dropout (with p= 0.1) and
MaxMatch-Dropout (with p= 0.3). The top row in each is the canonical tokenization.